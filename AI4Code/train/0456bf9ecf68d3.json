{"cell_type":{"0e03943b":"code","6f620a9f":"code","c6a9bdee":"code","0c974ec1":"code","8cabf766":"code","4751ef94":"code","54987705":"code","dfe487cb":"code","1082415c":"code","36aa5e8b":"code","96d160ad":"code","3d7c739c":"code","d14a22b5":"code","33a9eb6c":"code","de9bb0c9":"code","7ae01d02":"code","e162a25f":"code","ef96fad7":"code","9d2b437a":"code","bfbe69ab":"code","769eb59e":"code","189be90b":"code","a5f68715":"code","37835e2a":"code","b82d072c":"code","ab6c6999":"code","0b1c6da4":"code","9a1603a2":"code","de2b72c9":"code","50d88ab5":"code","2783a439":"code","9476418f":"code","811aa6d0":"code","af9f0f08":"code","aef3f490":"code","068d61a8":"code","0c5a6c42":"code","59c0fc79":"code","139d096b":"code","f9991c8b":"code","2342223b":"code","324c3337":"code","ded5da69":"code","7148978e":"code","bbef0977":"code","c18a441b":"code","18fb0635":"code","6315837e":"code","7f1acece":"code","93257315":"code","49fa0396":"code","14711b69":"code","bc964874":"code","b0b2e118":"code","0148756e":"code","d5135521":"code","804c35de":"code","2093f796":"code","99ec1a2e":"code","4eded1d5":"code","4e9f8e2d":"code","d0516b3b":"code","466e8093":"code","9e405fef":"code","e8773954":"code","92218e43":"code","9c2fb977":"code","bc3884c7":"code","f728d04c":"code","0f6f1524":"code","e2311055":"code","84f57f49":"code","e3ec5a93":"markdown","0a3e42f6":"markdown","e8c21aad":"markdown","227e4567":"markdown","ff150591":"markdown","87ee79c7":"markdown","5ec2670f":"markdown","0e8d23fa":"markdown","1e6d380a":"markdown","0d025ede":"markdown","5b8f6ab1":"markdown","d65d39c3":"markdown","954f6750":"markdown","e0f7c406":"markdown","b7f90e19":"markdown","ecdaafa7":"markdown","921b3dc3":"markdown","3d7a9dc3":"markdown","ebb4dd43":"markdown","7e956d7a":"markdown","0b3a9e91":"markdown","87f894ac":"markdown","33f3d99a":"markdown","fd5ca8b8":"markdown","79314cf3":"markdown","5c3dc318":"markdown","b8f8358d":"markdown","5da1f9d4":"markdown","5a2a1fba":"markdown","0ebdc4c2":"markdown","45cac72f":"markdown","6904bce7":"markdown","252d6c70":"markdown","950768c9":"markdown","31c69b0e":"markdown","32093776":"markdown","891dc322":"markdown","a48a8e7c":"markdown","38406f81":"markdown","321262c5":"markdown","940f7278":"markdown","f2e8ac8b":"markdown"},"source":{"0e03943b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","6f620a9f":"#  For using autocomplete in kaggle notebooks\n%config Completer.use_jedi = False","c6a9bdee":"df = pd.read_csv('..\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","0c974ec1":"df ","8cabf766":"df.info()","4751ef94":"df.describe()","54987705":"sns.countplot(df.output)\nprint(df.output.value_counts()\/len(df)*100)","dfe487cb":"sns.histplot(df.age)","1082415c":"sns.countplot(df.sex)\nprint(df.sex.value_counts()\/len(df)*100)","36aa5e8b":"sns.histplot(df.thalachh)","96d160ad":"sns.histplot(df.chol)","3d7c739c":"print('Before droping we have dataset of size', df.shape)\ndf.drop(index=df[(df.chol > 369.75) | (df.chol < 115.75)].index, inplace=True)\nprint('After droping we have dataset of size', df.shape)","d14a22b5":"sns.countplot(df.cp)","33a9eb6c":"columns = ['fbs','restecg','exng','slp','caa','thall']\nfor col in columns:\n    sns.countplot(df[col])\n    plt.show()","de9bb0c9":"# Create bins for age and group them\n# Aggregate by sum()\ndf.groupby(pd.cut(df.age,[25,35,45,55,65,75,85])).sum().sort_values('output', ascending=False)","7ae01d02":"sns.kdeplot(df.loc[df['output'] == 0, 'age'], label = 'output 0')\nsns.kdeplot(df.loc[df['output'] == 1, 'age'], label = 'output 1')\nplt.legend()","e162a25f":"# Aggregate by Count()\ndf.groupby(pd.cut(df.age,[25,35,45,55,65,75,85])).count().sort_values('output', ascending=False)","ef96fad7":"df_age = df.groupby(pd.cut(df.age,[25,35,45,55,65,75,85])).sum().sort_values('output', ascending=False)\ndf_age['total_count'] = df.groupby(pd.cut(df.age,[25,35,45,55,65,75,85])).count().sort_values('output', ascending=False)['output']\ndf_age['percentage'] = df_age['output'] \/ df_age['total_count'] * 100\ndf_age = df_age[['output','total_count','percentage']]\ndf_age","9d2b437a":"sns.histplot(df.age,bins=6)","bfbe69ab":"df.sex.value_counts()\/len(df)*100","769eb59e":"df.groupby('sex').sum()","189be90b":"df.groupby('sex').mean()","a5f68715":"sns.kdeplot(df.loc[df['output'] == 0, 'sex'], label = 'output 0')\nsns.kdeplot(df.loc[df['output'] == 1, 'sex'], label = 'output 1')\nplt.legend()","37835e2a":"# copy dataframe to another varible\ndf_se_ag = df.copy()\n\n# create seperate variable for both sex\ndf_se_ag['female'] = np.where(df_se_ag.sex == 0,1,0)\ndf_se_ag['male'] = np.where(df_se_ag.sex == 1,1,0)\n\n# seperate output variable base on sex\ndf_se_ag['out_female'] = np.where(df_se_ag.sex == 0,df_se_ag.output,0)\ndf_se_ag['out_male'] = np.where(df_se_ag.sex == 1,df_se_ag.output,0)","b82d072c":"# apply groupby function by age\n# take only necessary columns\nma_fe_df = df_se_ag.groupby(pd.cut(df_se_ag.age,[25,35,45,55,65,75,85])).sum()[['female','male','out_female','out_male']]\nma_fe_df","ab6c6999":"# find out percentage \nma_fe_df['per_female'] = ma_fe_df['out_female'] \/ ma_fe_df['female'] *100\nma_fe_df['per_male'] = ma_fe_df['out_male'] \/ ma_fe_df['male']*100\n\nma_fe_df","0b1c6da4":"df.groupby('chol').sum()","9a1603a2":"sns.kdeplot(df.loc[df['output'] == 0, 'chol'], label = 'output 0')\nsns.kdeplot(df.loc[df['output'] == 1, 'chol'], label = 'output 1')\nplt.legend()","de2b72c9":"df_chol = df.copy()\ndf_chol['c_grp'] = pd.cut(df['chol'],14)\ndf_chol = df_chol.groupby('c_grp').sum()\ndf_chol['chol_count'] = df.groupby(pd.cut(df['chol'],14)).count()['chol']\ndf_chol['percent'] = df_chol['output'] \/ df_chol['chol_count']*100\ndf_chol","50d88ab5":"def kdplot(col):\n    '''\n    Return\n    ----------\n    plot kdeplot for output variable `col`\n    '''\n    sns.kdeplot(df.loc[df['output'] == 0, col], label = 'output 0')\n    sns.kdeplot(df.loc[df['output'] == 1, col], label = 'output 1')\n    plt.legend()\n    plt.show","2783a439":"def show_per_cat(df,col):\n    '''\n    Function for category\n    \n    Return\n    --------\n    df_temp(Dataframe):\n        Contain count of attacks , total points and percentage of attack\n    '''\n    df_temp = df.copy()\n    df_temp = df_temp.groupby(col).sum()\n    df_temp['grp_count'] = df.groupby(col).count()['output']\n    df_temp['percent'] = df_temp['output'] \/ df_temp['grp_count']*100\n    \n    return df_temp[['output','grp_count','percent']]","9476418f":"show_per_cat(df,'exng')","811aa6d0":"kdplot('exng')","af9f0f08":"show_per_cat(df,'cp')","aef3f490":"kdplot('cp')","068d61a8":"def show_per_cont(df,col,quant=5):\n    '''\n    Function for continuous features\n    \n    quant (int):\n        It divide number of unique values in that feature and create those amount of bins\n        we can change that if feature has many unique features \n    \n    Return\n    --------\n    df_temp(Dataframe):\n        Contain count of attacks , total points and percentage of attack\n    '''\n    df_chol = df.copy()\n    df_chol = df_chol.groupby(pd.cut(df[col],int(df[col].nunique()\/quant))).sum()\n    df_chol['grp_count'] = df.groupby(pd.cut(df[col],int(df[col].nunique()\/quant))).count()['output']\n    df_chol['percent'] = df_chol['output'] \/ df_chol['grp_count']*100\n    \n    return df_chol[['output','grp_count','percent']]","0c5a6c42":"show_per_cont(df,'trtbps')","59c0fc79":"kdplot('trtbps')","139d096b":"show_per_cat(df,'fbs')","f9991c8b":"kdplot('fbs')","2342223b":"show_per_cat(df,'restecg')","324c3337":"kdplot('restecg')","ded5da69":"show_per_cat(df,'thall')","7148978e":"kdplot('thall')","bbef0977":"# Convert features data type to category\ncat_col = ['sex','cp','fbs','restecg', 'exng', 'slp', 'caa', 'thall']\ndf[cat_col] = df[cat_col].astype('category')","c18a441b":"# Performing One-Hot-Encoding\ndf_process = pd.get_dummies(df, columns=cat_col, drop_first=True)","18fb0635":"df_process","6315837e":"# Seperate output feature\nX = df_process.drop(columns='output')\ny = df_process.output","7f1acece":"# Standardise data to improve accuracy of Linear models\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\ndf_scale = scaler.transform(X)","93257315":"# Dividing data into train and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","49fa0396":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","14711b69":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import classification_report\ntarget_names = ['class 0', 'class 1']","bc964874":"lr = LogisticRegression(solver='liblinear')\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","b0b2e118":"sv = SVC()\nsv.fit(X_train,y_train)\ny_pred = sv.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","0148756e":"dt = DecisionTreeClassifier()\ndt.fit(X_train,y_train)\ny_pred = dt.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","d5135521":"rf = RandomForestClassifier()\nrf.fit(X_train,y_train)\ny_pred = rf.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","804c35de":"gb = GradientBoostingClassifier(n_estimators=20)\ngb.fit(X_train,y_train)\ny_pred = gb.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","2093f796":"lr = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l1','l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n\ngrid_search = GridSearchCV(estimator=lr, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","99ec1a2e":"lr = LogisticRegression(C=10, penalty='l2',solver='newton-cg')\nlr.fit(X_train,y_train)\ny_pred = lr.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","4eded1d5":"model = SVC()\nkernel = ['poly', 'rbf', 'sigmoid']\nC = [50, 10, 5, 1.0, 0.1, 0.01]\ngamma = ['scale']\n# define grid search\ngrid = dict(kernel=kernel,C=C,gamma=gamma)\ncv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","4e9f8e2d":"model = RandomForestClassifier()\nn_estimators = [10, 100, 1000]\nmax_features = ['sqrt', 'log2']\n# define grid search\ngrid = dict(n_estimators=n_estimators,max_features=max_features)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X, y)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))","d0516b3b":"# model = GradientBoostingClassifier()\n# n_estimators = [10, 30, 70, 100, 1000]\n# learning_rate = [0.001, 0.01, 0.1]\n# subsample = [0.5, 0.7, 1.0]\n# max_depth = [2, 4, 6, 8, 10]\n# # define grid search\n# grid = dict(learning_rate=learning_rate, n_estimators=n_estimators, subsample=subsample, max_depth=max_depth)\n# cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\n# grid_result = grid_search.fit(X, y)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n# >>>> Best: 0.827759 using {'learning_rate': 0.01, 'max_depth': 2, 'n_estimators': 1000, 'subsample': 0.5}","466e8093":"gb = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1000, subsample=0.5)\ngb.fit(X_train,y_train)\ny_pred = gb.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","9e405fef":"import xgboost as xgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import f1_score, make_scorer\nimport xgboost","e8773954":"#  It takes half hour to run\n# model = xgboost.XGBClassifier()\n# params = {\"learning_rate\"    : [0.05, 0.10, 0.15],\n#          \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n#           \"min_child_weight\" : [ 1, 3, 5, 7 ],\n#           \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n#           \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ] }\n\n# cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=1)\n# grid_search = RandomizedSearchCV(estimator=model, param_distributions=params, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0,n_iter=20)\n# grid_result = grid_search.fit(X, y)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n# # >>> Best: 0.813143 using {'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.3, 'colsample_bytree': 0.3}","92218e43":"xgbt = xgboost.XGBClassifier(min_child_weight=1, max_depth=3, learning_rate=0.1, gamma=0.3, colsample_bytree=0.3)\nxgbt.fit(X_train,y_train)\ny_pred = xgbt.predict(X_test)\nprint(classification_report(y_test, y_pred, target_names=target_names))","9c2fb977":"param = {'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'gamma': 0.3, 'colsample_bytree': 0.3}\nnum_round = 20","bc3884c7":"txgb = xgboost.XGBClassifier(min_child_weight=1, max_depth=3, learning_rate=0.1, gamma=0.3, colsample_bytree=0.3)\ndtrain = xgb.DMatrix(df.drop('output',axis=1), label=df.output, enable_categorical=True)\nbst = xgb.train(param, dtrain, num_round)\n# txgb.fit(dtrain)\n# y_pred = txgb.predict(X_test)\n# print(classification_report(y_test, y_pred, target_names=target_names))","f728d04c":"# For features importance\ngb = GradientBoostingClassifier(learning_rate=0.01, max_depth=2, n_estimators=1000, subsample=0.5)\ngb.fit(df.drop('output',axis=1),df.output)","0f6f1524":"def plot_feature_importance(importance,names,model_type):\n    #Create arrays from feature importance and feature names\n    feature_importance = np.array(importance)\n    feature_names = np.array(names)\n\n    #Create a DataFrame using a Dictionary\n    data={'feature_names':feature_names,'feature_importance':feature_importance}\n    fi_df = pd.DataFrame(data)\n\n    #Sort the DataFrame in order decreasing feature importance\n    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n\n    #Define size of bar plot\n    plt.figure(figsize=(10,8))\n    #Plot Searborn bar chart\n    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n    #Add chart labels\n    plt.title(model_type + 'FEATURE IMPORTANCE')\n    plt.xlabel('FEATURE IMPORTANCE')\n    plt.ylabel('FEATURE NAMES')\n","e2311055":"plot_feature_importance(gb.feature_importances_,df.drop('output',axis=1).columns,'GRADIENT BOOST')","84f57f49":"# we plot feature importance\npd.Series(gb.feature_importances_,index=df.drop('output',axis=1).columns).sort_values(ascending=False)","e3ec5a93":"`out_female` and `out_male` feature is show total number of heart attack paitent ","0a3e42f6":"# I hope this is HELPFUL\u2764\ufe0f !!!","e8c21aad":"Data is imbalance we cannot trust ACCURACY we use F1 score","227e4567":"* We can say Female has more probability of attacks by seeing kdeplot","ff150591":"`Pandas cut()` function is used to separate the array elements into different bins . The cut function is mainly used to perform statistical analysis on scalar data. \n\n`cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False, duplicates=\u201draise\u201d,)`\n\nParameters:\n\nx: The input array to be binned. Must be 1-dimensional.\n\nbins: defines the bin edges for the segmentation.","87ee79c7":"because male has more data points mean value decrese\n\n\nand female has less data points their mean value increase\n\nImbalance dataset","5ec2670f":"# Sex","0e8d23fa":"We see here Chest pain has maximium contribution to predictiong more chances of heart attack.","1e6d380a":"# Cholestrol","0d025ede":"More thall less probability to heart diseases","5b8f6ab1":"By above observation we don't able to say 35 - 65 have more chances of heart attacks because,\n\nWe have very less data for saying 25-35 and 65-85 has less heart attacks\n\nSo, we can't direct jump on colnclusion, we need more data","d65d39c3":"We have inbalance dataset","954f6750":"Definitely chest pain is bad sign for heart","e0f7c406":"There is no usefulness insight from trtbps means resting blood pressure","b7f90e19":"## Male Female who is more likely to have heart problem ?","ecdaafa7":"because values are between 1 and 0 when we add them we only get count of 1","921b3dc3":"* 0 : Female\n* 1 : Male","3d7a9dc3":"exercise induced angina (1 = yes; 0 = no)\n\n\n0 is more probable to heart disease","ebb4dd43":"** At first glance anybody can say cholestrol above 192 is more chances of heart diseases **\n\n\nThere is BIG BUT..","7e956d7a":"# Multivariant analysis","0b3a9e91":"* Data is almost abalance","87f894ac":"# Chest Pain","33f3d99a":"For training we have 238 data points and for testing we have 60","fd5ca8b8":"we can clearly see here cholestrol is not a good factor in Heart Disease\n\nBecause the guy who has 125 cholestrol has 66% probability and guy has more than 340 also same probability","79314cf3":"* 0 are females\n* 1 is Male\n\n\nIt might be happen our model is more bias towards Males because it has more data","5c3dc318":"* thalachh = maximum heart rate achieved","b8f8358d":"### How many times we write same code lets make function for it","5da1f9d4":"# Import Libraries","5a2a1fba":"Now we have 23 features","0ebdc4c2":"cp : Chest Pain type chest pain type","45cac72f":"Not any impactful insight","6904bce7":"* chol : cholestoral in mg\/dl fetched via BMI sensor\n","252d6c70":"also important to write again and again to understand it","950768c9":"By sum we can easily see Male get more heart attacks But...","31c69b0e":"Again we confirm our previous statement by seeing agewise male female\n\nFemale has more probably of heart attacks as compare to Male","32093776":"# Hyperparameter Tunning","891dc322":"## About Data\n\n`age` - Age of the patient\n\n`sex` - Sex of the patient\n\n`cp` - Chest pain type ~ 0 = Typical Angina, 1 = Atypical Angina, 2 = Non-anginal Pain, 3 = Asymptomatic\n\n`trtbps` - Resting blood pressure (in mm Hg)\n\n`chol` - Cholestoral in mg\/dl fetched via BMI sensor\n\n`fbs` - (fasting blood sugar > 120 mg\/dl) ~ 1 = True, 0 = False\n\n`restecg` - Resting electrocardiographic results ~ 0 = Normal, 1 = ST-T wave normality, 2 = Left ventricular hypertrophy\n\n`thalachh` - Maximum heart rate achieved\n\n`oldpeak` - Previous peak\n\n`slp` - Slope\n\n`caa` - Number of major vessels\n\n`thall` - Thalium Stress Test result ~ (0,3)\n\n`exng` - Exercise induced angina ~ 1 = Yes, 0 = No\n\n`output` - Target variable","a48a8e7c":"## What next\n* We can create webapp that give you advice for healthy heart from this dataset using `Flask` or `Stramlit`  ","38406f81":"Mean show completely different picture here, they show female gets more heart attack, HOW??","321262c5":"# Modeling","940f7278":"![heart](https:\/\/www.healtheuropa.eu\/wp-content\/uploads\/2020\/12\/Cardiovascular-disease-related-to-Type-2-diabetes-can-be-vastly-reduced-696x378.jpg)\n\nGraphs gives us better idea of data but when we need exact Number we use aggregation and groupby function to get more insight from data","f2e8ac8b":"## Age"}}