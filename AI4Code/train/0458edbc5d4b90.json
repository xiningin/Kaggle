{"cell_type":{"6c5ac144":"code","5af51542":"code","c1933297":"code","95d5a4ea":"code","b2d4768f":"code","6508cf21":"code","46f77aea":"code","8c39c3af":"code","e0c02350":"code","3a86bea0":"code","2ccbd61d":"code","64aa4503":"code","875d36a4":"code","5bfe1d88":"code","e6fd8188":"code","3207ae22":"code","23f767b0":"code","ecaf651a":"code","5990c21e":"code","913a8fce":"code","78bda725":"code","543def2c":"code","af571dd8":"code","d6defabb":"code","ab19e451":"code","8f29734d":"code","98d64bca":"code","a8bf5a14":"code","f1dcb314":"code","c1cc3b04":"code","39c933cd":"code","4198d41d":"code","9f553de7":"code","a310fa0a":"code","fe671430":"code","980545a0":"code","4afe84c3":"code","b38392c4":"code","f550c9fa":"code","04b3316c":"code","1a92070b":"code","63041641":"code","63db9698":"code","c88309d3":"code","8f6f82b8":"code","ffbf8bfe":"code","565fe4eb":"code","83227848":"code","cdecdee1":"code","ea285b75":"code","0972fef2":"code","30a36bf6":"code","55b192b4":"code","e61131b4":"code","f2d199ce":"code","fb4f3c18":"markdown","264b0344":"markdown","6bc57de7":"markdown","77a6740e":"markdown","fb5908b3":"markdown","b6e89040":"markdown","6627b784":"markdown","e59106b4":"markdown","7affd23a":"markdown","67af16ef":"markdown","b9568e6e":"markdown","0a2a857b":"markdown","0205b4dd":"markdown","4bbf42cc":"markdown"},"source":{"6c5ac144":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5af51542":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport numpy as np","c1933297":"train_data = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/cap-4611-2021-fall-assignment-3\/eval.csv\")","95d5a4ea":"train_data.head()","b2d4768f":"train_data.info()","6508cf21":"train_data.describe()","46f77aea":"train_data.dtypes","8c39c3af":"missing_vals = train_data[train_data.isna().all(axis=1)]","e0c02350":"missing_vals.info()","3a86bea0":"missing_vals.head()","2ccbd61d":"test_data.head()","64aa4503":"test_data.info()","875d36a4":"test_data.dtypes","5bfe1d88":"test_data.describe()","e6fd8188":"missing_vals = test_data[test_data.isna().all(axis=1)]","3207ae22":"missing_vals.info()","23f767b0":"missing_vals.head()","ecaf651a":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split","5990c21e":"X = train_data.drop(\"Eat\", axis=1)\ny = train_data[\"Eat\"]\n\nX.shape","913a8fce":"es = EarlyStopping(monitor='val_loss', patience=4)","78bda725":"def fitAndScore(model, X_train, X_test, y_train, y_test):\n    \n    model.fit(X_train, y_train, epochs=20000, batch_size=325, validation_split=0.3, callbacks=[es])\n    \n    score = model.evaluate(X_test, y_test)\n    \n    return score","543def2c":"def generateDistributions(model, X, y, n):\n    model_scores = []\n    \n    for i in range(n):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n        \n        X_train = tf.convert_to_tensor(X_train)\n        X_test = tf.convert_to_tensor(X_test)\n        y_train = tf.convert_to_tensor(y_train)\n        y_test = tf.convert_to_tensor(y_test)\n        \n        optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n        model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n        model_scores.append(fitAndScore(model, X_train, X_test, y_train, y_test))\n        \n        \n    return model_scores","af571dd8":"n = 30","d6defabb":"model_scores = []\n    \nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n        \n    X_train = tf.convert_to_tensor(X_train)\n    X_test = tf.convert_to_tensor(X_test)\n    y_train = tf.convert_to_tensor(y_train)\n    y_test = tf.convert_to_tensor(y_test)\n    \n    model_1 = Sequential()\n\n    model_1.add(Dense(100, input_shape=(1277, )))\n    model_1.add(Dense(100))\n    model_1.add(Dense(1))\n        \n    optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n    model_1.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    model_scores.append(fitAndScore(model_1, X_train, X_test, y_train, y_test))","ab19e451":"model_scores = np.sqrt(model_scores)\nnp.mean(model_scores)","8f29734d":"plt.hist(model_scores, bins=30)\nplt.show()","98d64bca":"pd.DataFrame(model_scores).describe()","a8bf5a14":"model_scores = []\n    \nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n        \n    X_train = tf.convert_to_tensor(X_train)\n    X_test = tf.convert_to_tensor(X_test)\n    y_train = tf.convert_to_tensor(y_train)\n    y_test = tf.convert_to_tensor(y_test)\n    \n    model_2 = Sequential()\n\n    model_2.add(Dense(100, input_shape=(1277, )))\n    model_2.add(Dense(50))\n    model_2.add(Dense(50))\n    model_2.add(Dense(50))\n    model_2.add(Dense(1))\n    \n    optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n    model_2.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    model_scores.append(fitAndScore(model_2, X_train, X_test, y_train, y_test))","f1dcb314":"model_scores = np.sqrt(model_scores)\nnp.mean(model_scores)","c1cc3b04":"plt.hist(model_scores)\nplt.show()","39c933cd":"pd.DataFrame(model_scores).describe()","4198d41d":"model_scores = []\n    \nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n        \n    X_train = tf.convert_to_tensor(X_train)\n    X_test = tf.convert_to_tensor(X_test)\n    y_train = tf.convert_to_tensor(y_train)\n    y_test = tf.convert_to_tensor(y_test)\n    \n    model_3 = Sequential()\n\n    model_3.add(Dense(500, input_shape=(1277, )))\n    model_3.add(Dense(300))\n    model_3.add(Dense(200))\n    model_3.add(Dense(150))\n    model_3.add(Dense(100))\n    model_3.add(Dense(50))\n    model_3.add(Dense(10))\n    model_3.add(Dense(1))\n        \n    optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n    model_3.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    model_scores.append(fitAndScore(model_3, X_train, X_test, y_train, y_test))","9f553de7":"model_scores = np.sqrt(model_scores)\nnp.mean(model_scores)","a310fa0a":"plt.hist(model_scores)\nplt.show()","fe671430":"pd.DataFrame(model_scores).describe()","980545a0":"model_scores = []\n    \nfor i in range(n):\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n        \n    X_train = tf.convert_to_tensor(X_train)\n    X_test = tf.convert_to_tensor(X_test)\n    y_train = tf.convert_to_tensor(y_train)\n    y_test = tf.convert_to_tensor(y_test)\n    \n    model_4 = Sequential()\n\n    model_4.add(Dense(638, activation='sigmoid', input_shape=(1277, )))\n    model_4.add(Dense(1))\n        \n    optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n    model_4.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    model_scores.append(fitAndScore(model_4, X_train, X_test, y_train, y_test))","4afe84c3":"model_4_scores = np.sqrt(model_scores)\nnp.mean(model_scores)","b38392c4":"plt.hist(model_scores)\nplt.show()","f550c9fa":"pd.DataFrame(model_scores).describe()","04b3316c":"# def build_model(n_hidden=1, n_neurons=638, learning_rate=0.003, input_shape=(1277,), beta_1=0.9, beta_2=0.999): \n   \n#     model = Sequential()\n#     options = {'input_shape': input_shape}\n    \n#     for layer in range(n_hidden):\n#         keras.layers.BatchNormalization()\n#         model.add(Dense(n_neurons, activation='sigmoid', **options))\n#         keras.layers.BatchNormalization()\n#         options = {}\n   \n#     model.add(keras.layers.Dense(1, **options))\n    \n#     optimizer = keras.optimizers.Adam(learning_rate, )     \n#     model.compile(loss='mean_squared_error', optimizer=optimizer) \n    \n#     return model","1a92070b":"# keras_reg = keras.wrappers.scikit_learn.KerasRegressor(build_model)","63041641":"# from scipy.stats import reciprocal\n# from sklearn.model_selection import RandomizedSearchCV\n\n# param_distributions = {\n#     'n_hidden' : [1, 2, 3],\n#     'n_neurons': np.arange(100, 1000),\n#     'learning_rate': reciprocal(3e-6, 3e-1),\n#     'beta_1': np.arange(1e-7, 0.5),\n#     'beta_2': np.arange(1e-7, 0.5),\n# }","63db9698":"# rcv = RandomizedSearchCV(keras_reg, param_distributions, n_iter=60, cv=3, scoring='neg_mean_squared_error')\n# rcv.fit(X, y, epochs=20000, batch_size=325, validation_split=0.3, callbacks=[es])","c88309d3":"# rcv.best_params_","8f6f82b8":"# rcv.best_score_","ffbf8bfe":"final_model_scores = []   \n    \nfor i in range(20):\n\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state = i)\n\n    X_train = tf.convert_to_tensor(X_train)\n    X_test = tf.convert_to_tensor(X_test)\n    y_train = tf.convert_to_tensor(y_train)\n    y_test = tf.convert_to_tensor(y_test)\n\n    final_model = Sequential()\n\n    keras.layers.BatchNormalization()\n    final_model.add(Dense(342, activation='sigmoid', input_shape=(1277,)))\n    keras.layers.BatchNormalization()\n    final_model.add(Dense(342, activation='sigmoid'))\n    keras.layers.BatchNormalization()\n    final_model.add(Dense(1))\n\n\n    optimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \n    final_model.compile(optimizer=optimizer, loss='mean_squared_error')\n\n    final_model_scores.append(fitAndScore(final_model, X_train, X_test, y_train, y_test))\n","565fe4eb":"final_model_scores = np.sqrt(final_model_scores)\nnp.mean(final_model_scores)","83227848":"plt.hist(final_model_scores)\nplt.show()","cdecdee1":"pd.DataFrame(final_model_scores).describe()","ea285b75":"final_model = Sequential()\n\nkeras.layers.BatchNormalization()\nfinal_model.add(Dense(342, activation='sigmoid', input_shape=(1277,)))\nkeras.layers.BatchNormalization()\nfinal_model.add(Dense(342, activation='sigmoid'))\nkeras.layers.BatchNormalization()\nfinal_model.add(Dense(1))\n\n\noptimizer = keras.optimizers.Adam(learning_rate=2.5378811106222304e-05, beta_1=1e-07, beta_2=1e-07) \nfinal_model.compile(optimizer=optimizer, loss='mean_squared_error')","0972fef2":"final_model.fit(X, y, epochs=20000, batch_size=325, validation_split=0.3, callbacks=[es])","30a36bf6":"pred = final_model.predict(test_data)\npred = pred.ravel()","55b192b4":"pred.shape","e61131b4":"output = pd.DataFrame({'id': test_data.id, 'Eat': pred})\noutput.to_csv('submission.csv', index=False)","f2d199ce":"print(output.to_string())","fb4f3c18":"# Why this Model Works\n\nThis model works because of how I tweaked the optimizer's parameters, quite simply. With two layers and 342 nodes each, there is fair enough complexity for the model to tackle this problem. \nThe secret is in the learning rate and Adam's \"beta_1\/2\" parameters. In setting these values smaller, we achieve a slower rate of change when performing backpropogation. Early stopping is implemented to prevent excessive epochs when there is no improvement in sight. So with this model we sacrifice training time for model performance on the training set.\n\n# Preparing a Submission\n\nRecreating the model to fit to the whole training-set\n","264b0344":"# Test Data EDA","6bc57de7":"# Model 4\n\nImplementing an algorithm described in https:\/\/stats.stackexchange.com\/questions\/181\/how-to-choose-the-number-of-hidden-layers-and-nodes-in-a-feedforward-neural-netw\n\nBasically, using one hidden layer with the mean of (input_nodes + output_nodes) so in this case ~638\n\nSo far the best model! Just from nodes alone. Adding a tanh activation function also improved the accuracy of the model further, however sigmoid is outperforming tanh","77a6740e":"# Reading in Data from CSV","fb5908b3":"# Building Models","b6e89040":"# Checking for Missing Values in Testing Data","6627b784":"# Automating the Tuning Process\n\nI am going to implement the method given by Aur\u00e9lien G\u00e9ron in HOML.\n\nFrom there I am going to tune the number of layers and nodes per layer through RandomSearchCV, as well as a few other parameters\n\nWhat follows is the code I used to do that. The actual results are implemented in the following model.","e59106b4":"# Building a Distribution with the Best Found Parameters","7affd23a":"# Checking for Missing Values in Training Data","67af16ef":"# Reading in Data from CSV Files","b9568e6e":"# Model 1","0a2a857b":"# Baisc EDA\n\nSince the data is not very intuitive, I elected to not do very in depth analysis on the data. Condsidering that it is all based on molecular features, and a pubchem_id (which on hindsight, I wish I elected to drop that and id before training).\n\n\nThe rest of the dataset looks fairly pure on a first look","0205b4dd":"# Model 2","4bbf42cc":"# Model 3\n\nMore layers is a good thing?"}}