{"cell_type":{"63be849e":"code","11dc799a":"code","88288dc9":"code","ec030b08":"code","5f7afcaa":"code","b1445696":"code","64df33d1":"code","945be204":"code","0510729b":"code","ac3b0a99":"code","ca4ce168":"code","42331eb4":"code","6dc9476d":"code","d84cbe66":"code","458eb789":"code","c3baa1c3":"code","82563f8c":"code","2b0f1c82":"code","1f2e7892":"code","1846fe3a":"code","f11f524d":"code","14ad2f0f":"code","31873fdc":"code","046a8153":"code","61dd634b":"code","928b7642":"markdown","1ac85b80":"markdown","c0702be6":"markdown","8ae6d179":"markdown","4453475c":"markdown","0f89db17":"markdown","53c18954":"markdown","9afa5165":"markdown","5eed2112":"markdown","225c8703":"markdown","8366c5bf":"markdown","0ab9314c":"markdown","bd3398b9":"markdown","545edb17":"markdown","b70e67d9":"markdown","3c46ac88":"markdown","c8032335":"markdown"},"source":{"63be849e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\n\npd.options.display.max_columns = 25\nsns.set_style('darkgrid')","11dc799a":"from sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import roc_auc_score","88288dc9":"from tensorflow.keras import layers\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.models import Model, Sequential\nfrom tensorflow.keras import callbacks\nfrom tensorflow.keras import utils","ec030b08":"train = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\")\nprint(train.shape)\ntrain.head()","5f7afcaa":"test = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")\nsample = pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")","b1445696":"test[\"target\"] = -1\ndata = pd.concat([train, test]).reset_index(drop=True)\nfeatures = [x for x in train.columns if x not in [\"id\", \"target\"]]\n\nfor feat in features:\n    lbl_enc = LabelEncoder()\n    data[feat] = lbl_enc.fit_transform(data[feat].values)","64df33d1":"def viz_high_cardinality(col):\n    tmp = pd.crosstab(train[col], train['target'])\n    tmp = tmp.reset_index()\n\n    fig, ax = plt.subplots(figsize=(20, 6))\n    N = len(tmp)\n    ind = np.arange(N)\n    width = 0.35\n\n    p1 = plt.bar(ind, tmp[0].values, width)\n    p2 = plt.bar(ind, tmp[1].values, width, bottom=tmp[0].values)\n\n    plt.xticks(ind, tmp[col].values)\n    plt.legend((p1[0], p2[0]), ('0', '1'))\n    plt.title(col, fontsize=18)\n    plt.show()\n    \ndef viz_huge_cardinality(col):\n    fig, ax = plt.subplots(figsize=(20, 6))\n    sns.distplot(train[col].value_counts().reset_index()[col].values)\n    ax.set_title(col, fontsize=18)    \n    plt.show()","945be204":"fig = plt.figure(figsize=(20, 6))\n\nax1 = plt.subplot2grid((1, 3), (0, 0))\ntrain['target'].value_counts().plot.pie(legend=True, autopct='%1.0f%%', ax=ax1)\n\nax2 = plt.subplot2grid((1, 3), (0, 1), colspan=2)\ntrain['target'].value_counts().plot.bar()\nax2.grid()\nx_offset = -0.03\ny_offset = 0.05\nfor p in ax2.patches:\n    b = p.get_bbox()\n    val = \"{:.2f}\".format(b.y1 + b.y0)        \n    ax2.annotate(val, ((b.x0 + b.x1)\/2 + x_offset, b.y1 + y_offset))","0510729b":"fig = plt.figure(figsize=(20, 20))\nax = []\nfor i in range(5):\n    ax.append(plt.subplot2grid((5, 3), (i, 0)))\n    ax.append(plt.subplot2grid((5, 3), (i, 1), colspan=2))\n\nfor i in range(0, 10, 2)    :\n    train[f'bin_{i\/\/2}'].value_counts().plot.pie(ax=ax[i], autopct=\"%1.1f%%\")\n\nx_offset = -0.03\ny_offset = 0.05    \n    \nfor i in range(1, 10, 2):\n    sns.countplot(x=f'bin_{i\/\/2}', data=train, hue='target', ax=ax[i])\n    for p in ax[i].patches:\n        b = p.get_bbox()\n        val = \"{:.2f}%\".format((b.y1 + b.y0)\/len(train)*100)        \n        ax[i].annotate(val, ((b.x0 + b.x1)\/2 + x_offset, b.y1 + y_offset))","ac3b0a99":"fig = plt.figure(figsize=(20, 20))\nax = []\nfor i in range(5):\n    ax.append(plt.subplot2grid((5, 3), (i, 0)))\n    ax.append(plt.subplot2grid((5, 3), (i, 1), colspan=2))\n\nfor i in range(0, 10, 2)    :\n    train[f'nom_{i\/\/2}'].value_counts().plot.pie(ax=ax[i], autopct=\"%1.1f%%\")\n\nx_offset = -0.03\ny_offset = 0.05    \n    \nfor i in range(1, 10, 2):\n    sns.countplot(x=f'nom_{i\/\/2}', data=train, hue='target', ax=ax[i])\n    for p in ax[i].patches:\n        b = p.get_bbox()\n        val = \"{:.2f}%\".format((b.y1 + b.y0)\/len(train)*100)        \n        ax[i].annotate(val, ((b.x0 + b.x1)\/2 + x_offset, b.y1 + y_offset))","ca4ce168":"for i in range(5, 10):\n    viz_huge_cardinality(f'nom_{i}')","42331eb4":"fig = plt.figure(figsize=(20, 12))\nax = []\nfor i in range(3):\n    ax.append(plt.subplot2grid((3, 3), (i, 0)))\n    ax.append(plt.subplot2grid((3, 3), (i, 1), colspan=2))\n\nfor i in range(0, 6, 2)    :\n    train[f'ord_{i\/\/2}'].value_counts().plot.pie(ax=ax[i], autopct=\"%1.1f%%\")\n\nx_offset = -0.03\ny_offset = 0.05    \n    \nfor i in range(1, 6, 2):\n    sns.countplot(x=f'ord_{i\/\/2}', data=train, hue='target', ax=ax[i])\n    for p in ax[i].patches:\n        b = p.get_bbox()\n        val = \"{:.2f}%\".format((b.y1 + b.y0)\/len(train)*100)        \n        ax[i].annotate(val, ((b.x0 + b.x1)\/2 + x_offset, b.y1 + y_offset))","6dc9476d":"viz_high_cardinality('ord_3')\nviz_high_cardinality('ord_4')\nviz_huge_cardinality('ord_5')  ","d84cbe66":"viz_high_cardinality('day')\nviz_high_cardinality('month')","458eb789":"train = data[data.target != -1].reset_index(drop=True)\ntest = data[data.target == -1].reset_index(drop=True)\ntest_data = [test.iloc[:, k+1].values for k in range(len(features))]\n\nX = [train.iloc[:, k+1] for k in range(len(features))]\ny = utils.to_categorical(train.target.values)","c3baa1c3":"def auroc(y_true, y_pred):\n    def fallback_auc(y_true, y_pred):\n        try:\n            return roc_auc_score(y_true, y_pred)\n        except:\n            return 0.5\n    return tf.py_function(fallback_auc, (y_true, y_pred), tf.double)","82563f8c":"%%time\n\nX_ = OneHotEncoder().fit_transform(train[features])\ny_ = train.target\n\nprint('Baseline Score:', np.mean(cross_val_score(LogisticRegression(max_iter=100_000), X_, y_, scoring='roc_auc')))","2b0f1c82":"es = callbacks.EarlyStopping(\n    monitor='val_auroc', min_delta=0.001, patience=7, verbose=1, mode='max', baseline=None,\n    restore_best_weights=True\n)\n\nrlp = callbacks.ReduceLROnPlateau(\n    monitor='val_auroc', factor=0.5, patience=3, min_lr=1e-6, mode='max', verbose=1\n)\n\nclass config():\n    EMBEDDING_DIM = 64\n    SPATIAL_DROPOUT = 0.3\n    HIDDEN_LAYERS = [(300, 'relu', 0.3), (300, 'relu', 0.3)]\n    OUTPUT_CELLS = 2\n    OUTPUT_ACTIVATION = 'softmax'\n    LOSS = 'binary_crossentropy'\n    OPT = 'adam'\n    METRICS = [auroc]\n    BATCH_SIZE = 1024\n    MAX_EPOCHS = 100\n    CALL_BACKS = [es, rlp]","1f2e7892":"def create_model(data, catcols):    \n    inputs = []\n    outputs = []\n    for c in catcols:\n        num_unique_values = int(data[c].nunique())\n        inp = layers.Input(shape=(1,))\n        out = layers.Embedding(num_unique_values + 1, config.EMBEDDING_DIM, name=c)(inp)\n        out = layers.SpatialDropout1D(config.SPATIAL_DROPOUT)(out)\n        out = layers.Reshape(target_shape=(config.EMBEDDING_DIM, ))(out)\n        inputs.append(inp)\n        outputs.append(out)\n    \n    x = layers.Concatenate()(outputs)\n    x = layers.BatchNormalization()(x)\n    \n    for n_cells, act_fn, dropout in config.HIDDEN_LAYERS:\n        x = layers.Dense(n_cells, activation=act_fn)(x)\n        x = layers.Dropout(dropout)(x)\n        x = layers.BatchNormalization()(x)\n    \n    y = layers.Dense(config.OUTPUT_CELLS, activation=config.OUTPUT_ACTIVATION)(x)\n\n    model = Model(inputs=inputs, outputs=y)\n    return model","1846fe3a":"model = create_model(data, features)\nmodel.compile(loss=config.LOSS, optimizer=config.OPT, metrics=config.METRICS)\nmodel.summary()\nutils.plot_model(model, show_shapes=True)","f11f524d":"history =  model.fit(\n    X, y, validation_split=0.1, \n    batch_size=config.BATCH_SIZE, callbacks=config.CALL_BACKS, epochs=config.MAX_EPOCHS\n)","14ad2f0f":"fig, ax = plt.subplots(2, 1, figsize=(20, 8))\ndf = pd.DataFrame(history.history)\ndf[['auroc', 'val_auroc']].plot(ax=ax[0])\ndf[['loss', 'val_loss']].plot(ax=ax[1])\nax[0].set_title('Model AUROC', fontsize=12)\nax[1].set_title('Model Loss', fontsize=12)\nfig.suptitle('Model Metrics', fontsize=18);","31873fdc":"pd.DataFrame({\n    'id': test.id.values,\n    'target': model.predict(test_data)[:, 1]\n}).to_csv(\"submission.csv\", index=False)","046a8153":"encoder = Model(inputs=model.input, outputs=model.layers[-8].output)\nembeddings_train = encoder.predict(X)\nembeddings_test = encoder.predict(test_data)","61dd634b":"%%time\n\nestimator = CatBoostClassifier(task_type=\"GPU\", silent=True).fit(embeddings_train, y[:, 1])\n\npd.DataFrame({\n    'id': test.id.values,\n    'target': estimator.predict_proba(embeddings_test)[:, 1]\n}).to_csv(f\"submission_hybrid.csv\", index=False)","928b7642":"## Day and Month","1ac85b80":"## Baseline Model\nOne Hot Encoding -> LogisticRegression","c0702be6":"We can't see any clear pattern to positive values in target.\n\n* It's interesting to see that in bin_3 the ratio of target true have the same size in both values; (~15%)\n* In the other binary features the pattern is very similar.","8ae6d179":"We can see clear different patterns between the nominal category values.\n\nSome summary of these features:\n\n* NOM_0 - Red (~35%) value have the highest % of positive values in the target;\n* NOM_1 - Triangle(~36%) value have the highest % of positive values in the target;\n* NOM_2 - Hamster(~36%) value have the highest % of positive values in the target;\n* NOM_3 - India(~36%) value have the highest % of positive values in the target;\n* NOM_4 - Theremin(~36%) value have the highest % of positive values in the target;\n* All the values with highest % of True values on target, are the category's with lowest frequency on the nominal categories","4453475c":"# EDA","0f89db17":"## Entity Embeddings","53c18954":"# References\n[Entity Embeddings of Categorical Variables](https:\/\/arxiv.org\/pdf\/1604.06737v1.pdf)","9afa5165":"Curiously, the data have two values that have few entries;\n\n* In the day column, the value 6 (maybe saturday?!) have less entries;\n* In the month column, the value 6 (maybe the holidays?!) have less entries;","5eed2112":"## Binary Features","225c8703":"## Ordinal Features","8366c5bf":"**Score:** 0.80221","0ab9314c":"**Score:** 0.79553","bd3398b9":"# Modelling","545edb17":"## Target Class Distribution","b70e67d9":"## Nominal Features","3c46ac88":"## Hybrid Model\n\nEntity Embeddings -> Catboost","c8032335":"# Dataset"}}