{"cell_type":{"9536e869":"code","19924acf":"code","45a496e2":"code","d609dcea":"code","28c753df":"code","e0bfdd59":"code","40210f7c":"code","acbee581":"code","a39f1b3f":"code","c41e9394":"code","cf68fc5e":"code","ef689782":"code","19a8c0d6":"code","61d560f6":"code","458dffad":"code","27dc07e8":"code","4bbc9ea2":"code","c7b4df18":"code","2cec850b":"code","e6ed6f2e":"code","4576b23c":"code","85cef9bf":"code","33ecd477":"code","4eda54f2":"code","7d123ca2":"code","3dbedf21":"code","f9d917ae":"code","aa0e7b82":"code","f49ad84f":"code","ab9e1a5f":"code","9b456007":"code","af4e2971":"code","4be49c7a":"code","7276e621":"code","d8e285cc":"code","354f99b8":"code","f790603a":"code","9ce8415c":"code","e5d413b0":"code","1b317923":"code","28e57f63":"code","b6037340":"code","0d5d1393":"code","491a991e":"code","809c4f62":"code","f23e739f":"code","c2da9af1":"code","7c159d51":"code","74582762":"code","e4d239ee":"code","d9efc8d7":"code","e6cf34b2":"code","87f01d2d":"code","512a33b4":"code","97bc9abf":"code","40c26841":"code","ef6426d2":"code","5b4a5c1c":"code","f86caa84":"code","27dbe5e0":"code","09cfcc9c":"code","47f473d0":"markdown","2f794e68":"markdown","b75a5f31":"markdown","03e9ea63":"markdown","753eaab9":"markdown","4f786acf":"markdown","51041cb1":"markdown","e9eaba85":"markdown","4c257176":"markdown","cddaefa3":"markdown","06d4121a":"markdown"},"source":{"9536e869":"import pandas as pd\nimport numpy as np\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","19924acf":"train_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","45a496e2":"train_df.head()","d609dcea":"test_data = test_df.copy()\ntest_data.head()","28c753df":"train_df.shape","e0bfdd59":"train_df.describe()\n","40210f7c":"with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n    print(train_df.isnull().sum())","acbee581":"with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n    print(train_df.info())","a39f1b3f":"#MiscFeature, PoolQC, Fence, FireplaceQu and Alley have lots of missing values, lets see in the test data as well\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None): \n    print(test_data.info())","c41e9394":"#Dropping the Id column\ntrain_df.drop(['Id'], axis=1, inplace=True)\ntest_data.drop(['Id'], axis=1, inplace=True)\ntrain_df.head(5)","cf68fc5e":"#dropping features with lots of missing values\ntrain_df.drop(['MiscFeature', 'PoolQC', 'Fence', 'Alley', 'FireplaceQu'], axis=1, inplace=True)\ntest_data.drop(['MiscFeature', 'PoolQC', 'Fence', 'Alley', 'FireplaceQu'], axis=1, inplace=True)\ntrain_df.shape","ef689782":"test_data.shape","19a8c0d6":"#column LotFrontage\nimport seaborn as sns\nsns.jointplot(data=train_df, x=\"LotFrontage\", y=\"SalePrice\", height=6)","61d560f6":"sns.countplot(x='BsmtQual', data=train_df)","458dffad":"#column Electrical\nsns.countplot(x='Electrical', data=train_df)","27dc07e8":"#For Electrical we can fill in the 1 missing value with the mode, SBrkr\ntrain_df['Electrical'].fillna(\"SBrkr\", inplace=True)","4bbc9ea2":"#MasVnrArea column\ntrain_df['MasVnrArea'].value_counts()","c7b4df18":"#relationship between MasVnrArea and MasVnrType\nsns.stripplot(x='MasVnrType', y='MasVnrArea', data=train_df)","2cec850b":"#MasVnrArea distribution\nsns.displot(data=train_df, x=\"MasVnrArea\", kde=True)","e6ed6f2e":"#distribution of garage year built\nsns.histplot(data=train_df, x=\"GarageYrBlt\")","4576b23c":"#relationship between year house built and year garage built\nsns.pairplot(\n    train_df,\n    x_vars=[\"GarageYrBlt\"],\n    y_vars=[\"YearBuilt\"],\n    height=4,\n)","85cef9bf":"#the year the garage was built cannot be before the year the house was built from the plot\n#safe assumption for missing values is to equate to the year the house was built\ntrain_df['GarageYrBlt'].fillna(train_df['YearBuilt'], inplace=True)\ntest_data['GarageYrBlt'].fillna(train_df['YearBuilt'], inplace=True)","33ecd477":"#relationship between select basement features\n#a plot of Basement Quality vs total basement floor area\nsns.catplot(x=\"BsmtQual\", y=\"BsmtFinSF1\", data=train_df)","4eda54f2":"#relationship between BsmtQual and BsmtCond\nsns.countplot(x=\"BsmtCond\", hue=\"BsmtQual\", data=train_df)","7d123ca2":"sns.countplot(x=\"BsmtQual\", hue=\"BsmtCond\", data=train_df)","3dbedf21":"#relationship between garage type and sale price\nsns.stripplot(x=\"GarageType\", y=\"SalePrice\", data=train_df)","f9d917ae":"#lets fill in missing values for LotFrontage and MsnVarArea columns using Median\ntrain_df['LotFrontage'].fillna(train_df['LotFrontage'].median(), inplace=True)\ntrain_df['MasVnrArea'].fillna(train_df['MasVnrArea'].median(), inplace=True)\n\ntest_data['LotFrontage'].fillna(train_df['LotFrontage'].median(), inplace=True)\ntest_data['MasVnrArea'].fillna(train_df['MasVnrArea'].median(), inplace=True)\n\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None): \n    print(train_df.isnull().sum())","aa0e7b82":"print(train_df.shape)\nprint(test_data.shape)","f49ad84f":"train_df['Age_On_Sale'] = train_df['YrSold'] - train_df['YearBuilt']\ntrain_df['Grg_Age_On_Sale'] = train_df['YrSold'] - train_df['GarageYrBlt']\n\ntest_data['Age_On_Sale'] = test_data['YrSold'] - test_data['YearBuilt']\ntest_data['Grg_Age_On_Sale'] = test_data['YrSold'] - test_data['GarageYrBlt']","ab9e1a5f":"#AgeAfterRemod -> YrSold - YearRemodAdd\ntrain_df['AgeAfterRemod'] = train_df['YrSold'] - train_df['YearRemodAdd']\ntest_data['AgeAfterRemod'] = test_data['YrSold'] - test_data['YearRemodAdd']","9b456007":"print(train_df.shape)\nprint(test_data.shape)","af4e2971":"#dropping features after modifications\ntrain_df.drop(['YrSold', 'YearBuilt', 'GarageYrBlt', 'YearRemodAdd'], axis=1, inplace=True)\ntest_data.drop(['YrSold', 'YearBuilt', 'GarageYrBlt', 'YearRemodAdd'], axis=1, inplace=True)\n","4be49c7a":"train_df['OverallQC'] = (train_df['OverallQual'] * train_df['OverallCond'] \/ 100)\ntest_data['OverallQC'] = (test_data['OverallQual'] * test_data['OverallCond'] \/ 100)","7276e621":"train_df['BsmtFin'] = train_df['BsmtFinSF1'] + train_df['BsmtFinSF2']\ntest_data['BsmtFin'] = test_data['BsmtFinSF1'] + test_data['BsmtFinSF2']","d8e285cc":"train_df['FloorFt'] = train_df['1stFlrSF'] + train_df['2ndFlrSF']\ntest_data['FloorFt'] = test_data['1stFlrSF'] + test_data['2ndFlrSF']","354f99b8":"train_df.drop(['OverallQual', 'OverallCond', 'BsmtFinSF1', 'BsmtFinSF2', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\ntest_data.drop(['OverallQual', 'OverallCond', 'BsmtFinSF1', 'BsmtFinSF2', '1stFlrSF', '2ndFlrSF'], axis=1, inplace=True)\nprint(train_df.shape)\nprint(test_data.shape)","f790603a":"with pd.option_context('display.max_rows', None, 'display.max_columns', None): \n    print(train_df.info())","9ce8415c":"#OHE of categorical features\ntrain_df = pd.get_dummies(train_df, columns = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n                                              'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n                                              'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n                                              'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n                                              'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'Fireplaces',\n                                              'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'])","e5d413b0":"test_data = pd.get_dummies(test_data, columns = ['MSZoning', 'Street', 'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n                                              'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n                                              'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'ExterCond', 'Foundation',\n                                              'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'Heating',\n                                              'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual', 'Functional', 'Fireplaces',\n                                              'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition'])","1b317923":"print(train_df.shape)\nprint(test_data.shape)","28e57f63":"from sklearn.impute import KNNImputer\n\nimputer = KNNImputer(n_neighbors=5)\ntrain_df = pd.DataFrame(imputer.fit_transform(train_df), columns = train_df.columns)\ntest_data = pd.DataFrame(imputer.fit_transform(test_data), columns = test_data.columns)","b6037340":"import matplotlib.pyplot as plt\n%matplotlib inline","0d5d1393":"#plotting correlation between the top features\n\ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    au_corr = train_df.corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df)\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]\n\nprint(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_df, 20))\n","491a991e":"train_df.drop(['Street_Pave', 'CentralAir_Y', 'Exterior2nd_CBlock', 'Utilities_NoSeWa', 'FloorFt', 'SaleCondition_Partial',\n'Exterior2nd_VinylSd', 'Exterior2nd_CmentBd', 'Exterior2nd_MetalSd', 'RoofStyle_Hip', 'LotShape_Reg', 'LandSlope_Mod',\n'ExterQual_TA', 'ExterCond_TA', 'Exterior2nd_HdBoard', 'GarageArea', 'Neighborhood_Somerst', 'Exterior2nd_Wd Sdng', \n'Electrical_SBrkr', 'PavedDrive_Y'], axis=1, inplace=True)\n\ntrain_df.shape","809c4f62":"test_data.drop(['Street_Pave', 'CentralAir_Y', 'Exterior2nd_CBlock', 'FloorFt', 'SaleCondition_Partial',\n'Exterior2nd_VinylSd', 'Exterior2nd_CmentBd', 'Exterior2nd_MetalSd', 'RoofStyle_Hip', 'LotShape_Reg', 'LandSlope_Mod',\n'ExterQual_TA', 'ExterCond_TA', 'Exterior2nd_HdBoard', 'GarageArea', 'Neighborhood_Somerst', 'Exterior2nd_Wd Sdng', \n'Electrical_SBrkr', 'PavedDrive_Y'], axis=1, inplace=True)","f23e739f":"#we inspect the remaining correlations again\nprint(get_top_abs_correlations(train_df, 20))","c2da9af1":"#lets split the features and the labels\nX = train_df.drop(['SalePrice'], axis=1)\nX.head(10)","7c159d51":"y = train_df.SalePrice\ny.head(10)","74582762":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e4d239ee":"from sklearn.linear_model import LinearRegression\n\nlinreg_model = LinearRegression()\n\nlinreg_model.fit(X_train, y_train)","d9efc8d7":"import xgboost as xgb\n\nxgb_model = xgb.XGBRegressor()\n\nxgb_model.fit(X_train, y_train)","e6cf34b2":"from sklearn import svm\n\nsvm_model = svm.SVR()\n\nsvm_model.fit(X_train, y_train)","87f01d2d":"from sklearn.ensemble import GradientBoostingRegressor\n\ngbr_model = GradientBoostingRegressor()\n\ngbr_model.fit(X_train, y_train)","512a33b4":"from sklearn.tree import DecisionTreeRegressor\n\ntree_model = DecisionTreeRegressor()\n\ntree_model.fit(X_train, y_train)","97bc9abf":"from sklearn.metrics import mean_absolute_error\n\nlin_mae = mean_absolute_error(linreg_model.predict(X_test), y_test)\nprint(\"linreg_model: {}\".format(lin_mae) + \"\\n\")\n\nxgb_mae = mean_absolute_error(xgb_model.predict(X_test), y_test)\nprint(\"xgb_model: {}\".format(xgb_mae) + \"\\n\")\n\nsvm_mae = mean_absolute_error(svm_model.predict(X_test), y_test)\nprint(\"svm_model: {}\".format(svm_mae) + \"\\n\")\n\ngbr_mae = mean_absolute_error(gbr_model.predict(X_test), y_test)\nprint(\"gbr_model: {}\".format(gbr_mae)  + \"\\n\")\n\ntree_mae = mean_absolute_error(tree_model.predict(X_test), y_test)\nprint(\"tree_model: {}\".format(tree_mae))","40c26841":"#1.Tuning the XGBoost Regressor, xgb_model using RandomizedCVSearch\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#defining the hyperparameter grid to search\nhyperparameter_grid = {\n    'n_estimators': [200, 400, 800],\n    'max_depth': [6, 9, 12],\n    'learning_rate': [0.05, 0.1, 0.20],\n    'min_child_weight': [1, 10, 100]\n    }\n\n# Set up the random search with cross validation\nrandom_cv = RandomizedSearchCV(estimator=xgb_model,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 1,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)\n\nrandom_cv.fit(X_train,y_train)","ef6426d2":"random_cv.best_estimator_","5b4a5c1c":"X['Fireplaces_4'] = 0","f86caa84":"#Fitting the XGBoost with best parameters to our data\nbest_xgb = xgb.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, enable_categorical=False,\n             gamma=0, gpu_id=-1, importance_type=None,\n             interaction_constraints='', learning_rate=0.05, max_delta_step=0,\n             max_depth=6, min_child_weight=10,monotone_constraints='()', n_estimators=400, n_jobs=4,\n             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1, verbosity=None)\n\nbest_xgb.fit(X[test_data.columns], y)","27dbe5e0":"y_preds = best_xgb.predict(test_data)","09cfcc9c":"# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'Id': test_df.Id,\n                       'SalePrice': y_preds})\noutput.to_csv('submission.csv', index=False)","47f473d0":"# Feature Engineering\n* combine Year Built and Year Sold -> Age_On_Sale\n* combine Year Built and GarageYrBlt -> Grg_Age_On_Sale","2f794e68":"To match the training set shape,we use columns in the test data dataframe to train our models","b75a5f31":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","03e9ea63":"# Train-Test Split\nSplitting the dataset into a 20% train set to train our model","753eaab9":"# Model Training\npredicting the SalePrice is a Regression task. We will train a few regression models and choose the best one to take:\n* Linear Regressor\n* XGBoost Regressor\n* Support Vector Machine\n* Gradient Boosting Regressor\n* Decision Tree Regressor","4f786acf":"# Exploratory Data Analysis\nWe are going to perform some EDA to better understand the data and handle some missing values","51041cb1":"# Missing Values\nfor the train set we have missing values in the following columns:\n* LotFrontage\n* Alley\n* BsmtQual\n* BsmtCond\n* BsmtExposure\n* BsmtFinType1\n* BsmtFinType2\n* Electrical\n* FireplaceQu       \n* GarageType         \n* GarageYrBlt        \n* GarageFinish\n* GarageQual\n* GarageCond\n* PoolQC\n* Fence\n* MiscFeature\n* MasVnrType\n* MasVnrArea","e9eaba85":"None remaining with a correlation above 0.85","4c257176":"# Model Evaluation","cddaefa3":"# Hyperparameter tuning\nwe choose the 2 best models so far:\n1. xgb_model\n2. gbr_model\n\nand we tune the hyperparameters to see if we can improve the resultant MAE","06d4121a":"We are going to drop one of the columns with a correlation greater than 0.85\n"}}