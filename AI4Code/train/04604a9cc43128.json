{"cell_type":{"cadfc069":"code","29b1445c":"code","e2133a5d":"code","935b5e14":"code","1d68a890":"code","bf3ac867":"code","8a800cdf":"code","2cbeb006":"code","87265ffe":"code","b2b5da8e":"code","c4712ae3":"code","cee005ae":"code","c476f995":"code","6d62955c":"code","ec49e66f":"code","5070bf79":"code","bf5b6f94":"code","c760431a":"code","77795cca":"code","f0b7415e":"code","15e5e400":"code","3ec54cb3":"code","74312141":"code","11b2744e":"code","3b1381a5":"markdown","6c6d50e4":"markdown","75f390a6":"markdown","3ef18084":"markdown","900cf7d5":"markdown","1557ac95":"markdown","250a219c":"markdown","2e480492":"markdown","cac361a1":"markdown","07a9e17c":"markdown","b913606f":"markdown","a4aa57a3":"markdown","3163e394":"markdown","e1c29b76":"markdown","9389cbfa":"markdown","c28c2bcc":"markdown","3444011b":"markdown","a6a38a52":"markdown","fe54e1c1":"markdown","d7668f2a":"markdown","f6465460":"markdown","6ad63dee":"markdown","d7e3b8aa":"markdown","47441f42":"markdown","5da3e2e3":"markdown"},"source":{"cadfc069":"#import dependencies\n\nimport warnings \nwarnings.filterwarnings('ignore')\n\n#Basic packages\nimport os\nimport pandas as pd\nimport numpy as np\nimport datetime\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Time Series\nfrom statsmodels.tsa.arima_model import ARIMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom pandas.plotting import autocorrelation_plot\nfrom statsmodels.tsa.stattools import adfuller, acf, pacf,arma_order_select_ic\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\n\ncolor = sns.color_palette()\nsns.set_style('darkgrid')","29b1445c":"#Read files in the directory\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e2133a5d":"train_df = pd.read_csv(\"\/kaggle\/input\/demand-forecasting-kernels-only\/train.csv\")\ntrain_df['date'] = pd.to_datetime(train_df['date'], format=\"%Y-%m-%d\")\ntrain_df.head()","935b5e14":"train_df.info()\n# Here we can see that there are no missing values in our dataframe","1d68a890":"# Expand dataframe with more useful columns\ndef expand_df(df):\n    data = df.copy()\n    \n    data['day'] = data.date.dt.day\n    data['month'] = data.date.dt.month\n    data['year'] = data.date.dt.year\n    data['dayofweek'] = data.date.dt.dayofweek\n    return data\n\ntrain_df = expand_df(train_df)\ndisplay(train_df)\n\ngrand_avg = train_df.sales.mean()\nprint(f\"The grand average of sales in this dataset is {grand_avg:.4f}\")","bf3ac867":"train_df = train_df.set_index('date')\ntrain_df.head()","8a800cdf":"train_df.tail()","2cbeb006":"#Sales by year\n\nagg_year_item = pd.pivot_table(train_df, index='year', columns='item',\n                               values='sales', aggfunc=np.mean).values\nagg_year_store = pd.pivot_table(train_df, index='year', columns='store',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_year_item \/ agg_year_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_year_store \/ agg_year_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Year\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","87265ffe":"# Now lets check the sales w.r.t. the month\n\nagg_month_item = pd.pivot_table(train_df, index='month', columns='item',\n                               values='sales', aggfunc=np.mean).values\nagg_month_store = pd.pivot_table(train_df, index='month', columns='store',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_month_item \/ agg_month_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_month_store \/ agg_month_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Month\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","b2b5da8e":"#Sales by days of week\n\nagg_weekly_item = pd.pivot_table(train_df, index='dayofweek', columns='item',\n                               values='sales', aggfunc=np.mean).values\nagg_weekly_store = pd.pivot_table(train_df, index='dayofweek', columns='store',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(12, 5))\nplt.subplot(121)\nplt.plot(agg_weekly_item \/ agg_weekly_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Days of week\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_weekly_store \/ agg_weekly_store.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Days of week\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","c4712ae3":"#Also now lets check for item store relationship\n\nagg_store_item = pd.pivot_table(train_df, index='store', columns='item',\n                                values='sales', aggfunc=np.mean).values\n\nplt.figure(figsize=(14, 5))\nplt.subplot(121)\nplt.plot(agg_store_item \/ agg_store_item.mean(0)[np.newaxis])\nplt.title(\"Items\")\nplt.xlabel(\"Store\")\nplt.ylabel(\"Relative Sales\")\nplt.subplot(122)\nplt.plot(agg_store_item.T \/ agg_store_item.T.mean(0)[np.newaxis])\nplt.title(\"Stores\")\nplt.xlabel(\"Item\")\nplt.ylabel(\"Relative Sales\")\nplt.show()","cee005ae":"sns.lineplot(x=\"date\",y=\"sales\",legend=\"full\",data=train_df)","c476f995":"# Lets decompose for data of smaller size. Here I will take data having item and store equal to 1.\n\ntrain_item1 = train_df[train_df['item']==1]\ntrain_final = train_item1[train_item1['store']==1]\n\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nresult = seasonal_decompose(train_final['sales'], model='additive', freq=365)\n\nfig = plt.figure()  \nfig = result.plot()  \nfig.set_size_inches(14, 12)","6d62955c":"# Rolling Mean Analysis\n\ndef roll_stats(timeseries, window = 12, cutoff = 0.01):\n    \n    rolmean = timeseries.rolling(window).mean()\n    rolstd = timeseries.rolling(window).std()\n\n    #Plot rolling statistics:\n    fig = plt.figure(figsize=(16, 4))\n    orig = plt.plot(timeseries, color='blue',label='Original')\n    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n    plt.legend(loc='best')\n   # plt.rcParams['agg.path.chunksize'] = 50000\n    plt.title('Rolling Mean & Standard Deviation')\n    plt.show()\n    \nroll_stats(train_final['sales'])    ","ec49e66f":"# Perform Dickey-Fuller test\ndef dickey_fuller_test(timeseries, window = 12, cutoff = 0.01):\n    dftest = adfuller(timeseries, autolag='AIC', maxlag = 20 )\n    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n    for key,value in dftest[4].items():\n        dfoutput['Critical Value (%s)'%key] = value\n    pvalue = dftest[1]\n    if pvalue < cutoff:\n        print('p-value = %.4f. The series is likely stationary.' % pvalue)\n    else:\n        print('p-value = %.4f. The series is likely non-stationary.' % pvalue)\n    \n    print(dfoutput)\n    \ndickey_fuller_test(train_final['sales'])    ","5070bf79":"# Make the time series stationary\n\nfirst_diff = train_final.sales - train_final.sales.shift(1)\nfirst_diff = first_diff.dropna(inplace = False)\nprint(first_diff.head())\nroll_stats(first_diff,window = 12, cutoff = 0.01)\ndickey_fuller_test(first_diff, window = 12)","bf5b6f94":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(train_final.sales, lags=40, ax=ax1) # \nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(train_final.sales, lags=40, ax=ax2)# , lags=40","c760431a":"fig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(first_diff, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(first_diff, lags=40, ax=ax2)","77795cca":"arima_model = sm.tsa.ARIMA(train_final.sales, (6,1,0)).fit(disp=False)\nprint(arima_model.summary())","f0b7415e":"from scipy import stats\nfrom scipy.stats import normaltest\n\nresid = arima_model.resid\nprint(normaltest(resid))\n# returns a 2-tuple of the chi-squared statistic, and the associated p-value. the p-value is very small, meaning\n# the residual is not a normal distribution\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(arima_model.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(arima_model.resid, lags=40, ax=ax2)","15e5e400":"start_index = 1726\nend_index = 1826\ntrain_df['forecast'] = arima_model.predict(start = start_index, end= end_index, dynamic= True)  \ntrain_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","3ec54cb3":"# Now we will use SARIMAX\n\nsarima_model = sm.tsa.statespace.SARIMAX(train_final.sales, trend='n', order=(6,1,0)).fit()\nprint(sarima_model.summary())","74312141":"resid = sarima_model.resid\nprint(normaltest(resid))\n\nfig = plt.figure(figsize=(12,8))\nax0 = fig.add_subplot(111)\n\nsns.distplot(resid ,fit = stats.norm, ax = ax0) # need to import scipy.stats\n\n# Get the fitted parameters used by the function\n(mu, sigma) = stats.norm.fit(resid)\n\n#Now plot the distribution using \nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\nplt.ylabel('Frequency')\nplt.title('Residual distribution')\n\n# ACF and PACF\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(sarima_model.resid, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(sarima_model.resid, lags=40, ax=ax2)","11b2744e":"start_index = 1726\nend_index = 1826\ntrain_df['forecast'] = sarima_model.predict(start = start_index, end= end_index, dynamic= True)  \ntrain_df[start_index:end_index][['sales', 'forecast']].plot(figsize=(12, 8))","3b1381a5":"As we can see that the time series is stationary now.","6c6d50e4":"# **Lets decompose the time series:** \n\nTime series data can exhibit a variety of patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. When we decompose a time series into components, we usually combine the trend and cycle into a single trend-cycle component (sometimes called the trend for simplicity). Thus we think of a time series comprising three components: a trend-cycle component, a seasonal component, and a remainder component (containing anything else in the time series).\n\n**The Seasonal component:** A seasonal pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. Seasonality is always of a fixed and known frequency. A time series can contain multiple superimposed seasonal periods.\n\n**The Trend component:** A trend exists when there is a long-term increase or decrease in the data. It does not have to be linear. Sometimes a trend is referred to as \u201cchanging direction\u201d when it might go from an increasing trend to a decreasing trend.\n\n**The Cyclical component:** The cyclical component represents phenomena that happen across seasonal periods. Cyclical patterns do not have a fixed period like seasonal patterns do. The cyclical component is hard to isolate and it's often \u2018left alone\u2019 by combining it with the trend component.\n\n**The Noise component:** The noise or the random component is what remains behind when you separate out seasonality and trend from the time series. Noise is the effect of factors that you do not know, or which you cannot measure. It is the effect of the known unknowns, or the unknown unknowns.\n\nhttps:\/\/towardsdatascience.com\/what-is-time-series-decomposition-and-how-does-it-work-9b67e007ae90","75f390a6":"Now we have checked the relationships between different variables. \n\nIt's time for us to decompose the time series","3ef18084":"**Sales by month has also shown similar growth pattern for both items and sales.**","900cf7d5":"# References:\n\nhttps:\/\/www.kaggle.com\/ashishpatel26\/keeping-it-simple-by-xyzt\n\nhttps:\/\/www.kaggle.com\/sumi25\/understand-arima-and-tune-p-d-q\n\nhttps:\/\/medium.com\/analytics-vidhya\/an-introduction-to-time-series-analysis-2a12d3702299 ","1557ac95":"# EDA","250a219c":"**We can see that the Items and Stores have grown similarly over the time.**","2e480492":"# Lets check the stationarity\n\nHere we are going to check the stationarity using 2 methods:\n\n**1. Rolling Mean:** Plot the moving average or moving standard deviation to see if it varies with time.\n\n**2. ADCF Test \u2014 Augmented Dickey\u2013Fuller test:** This is used to gives us various values that can help in identifying stationarity. *The Null hypothesis says that a Time-series is non-stationary.* It comprises of a Test Statistics & some critical values for some confidence levels. If the Test statistics is less than the critical values, we can reject the null hypothesis & say that the series is stationary. The ADCF test also gives us a p-value. According to the null hypothesis, lower values of p is better.","cac361a1":"Clearly we can see that there is some problem with this model. So let's try with some different model. ","07a9e17c":"pivot_table: https:\/\/www.dataquest.io\/blog\/pandas-pivot-table\/","b913606f":"# Motivation: Given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.","a4aa57a3":"**Now let\u2019s see what is an AR and MA time series process:**\n\n**Auto regressive (AR)** process , a time series is said to be AR when present value of the time series can be obtained using previous values of the same time series i.e the present value is weighted average of its past values. Stock prices and global temperature rise can be thought of as an AR processes.\nThe AR process of an order p can be written as,\n\n![](https:\/\/miro.medium.com\/max\/463\/1*rPyGlYZdKKJ-Ozc3NkEa9g.jpeg)\n\nWhere \u03f5t is a white noise and y\u2019t-\u2081 and y\u2019t-\u2082 are the lags. Order p is the lag value after which PACF plot crosses the upper confidence interval for the first time. These p lags will act as our features while forecasting the AR time series. We cannot use the ACF plot here because it will show good correlations even for the lags which are far in the past.  If we consider those many features, we will have multicollinearity issues.This is not a problem with PACF plot as it removes components already explained by earlier lags, so we only get the lags which have the correlation with the residual i.e the component not explained by earlier lags.","3163e394":"# Stationarity\n\nBefore applying any statistical model on a Time Series, the series has to be stationary or time invariant, which means that, over different time periods, **it should have constant means, constant variance and constant covariance.** It means that the data should have constant mean throughout, scattered consistently and should have same frequency throughout. So, if our data mean, variance and covariance is varied with time then our data is non-stationary and we have to make it stationary before applying any method. This is necessary because if our data has some regular pattern then there\u2019s a high probability that over a different interval, it will have same behavior and can cause problem in accuracy of model. And also, mathematical computation for stationary data is easier as compared to that of non-stationary data.\n\n![](https:\/\/miro.medium.com\/max\/1050\/1*xdblkZyg6YmmReAkZHUksw.png)","e1c29b76":"![](https:\/\/miro.medium.com\/max\/2774\/1*2s3HRji-pT6flEiYsIZXEw.png) ","9389cbfa":"**How to find the order of the AR term (p)**\n\nYou can find out the required number of AR terms by inspecting the Partial Autocorrelation (PACF) plot. Any autocorrelation in a stationarized series can be rectified by adding enough AR terms. So, we initially take the order of AR term to be equal to as many lags that crosses the significance limit in the PACF plot.","c28c2bcc":"**ARIMA,** short for \u2018Auto Regressive Integrated Moving Average\u2019 is actually a class of models that \u2018explains\u2019 a given time series based on its own past values, that is, its own lags and the lagged forecast errors, so that equation can be used to forecast future values.\n\nAny \u2018non-seasonal\u2019 time series that exhibits patterns and is not a random white noise can be modeled with ARIMA models.\n\nAn ARIMA model is characterized by 3 terms: p, d, q\n\nwhere,\n\np: number of autoregressive terms (AR order)\n\nd: number of nonseasonal differences (differencing order)\n\nq: number of moving-average terms (MA order)\n\nReference: https:\/\/www.machinelearningplus.com\/time-series\/arima-model-time-series-forecasting-python\/","3444011b":"What information do we have right now? \n\nThe dataset is privided with 4 columns: \n\n1. date: The date of sale\n2. store: This is the store number\n3. item: This is the item number\n4. sales: Sales made on that particular day","a6a38a52":"# Prediction using ARIMA Model","fe54e1c1":"**Lets start with ACF and PACF:**\n\n**ACF** is an (complete) auto-correlation function which gives us values of auto-correlation of any series with its lagged values. We plot these values along with the confidence band and tada! We have an ACF plot. In simple terms, it describes how well the present value of the series is related with its past values. A time series can have components like trend, seasonality, cyclic and residual. ACF considers all these components while finding correlations hence it\u2019s a \u2018complete auto-correlation plot\u2019.\n\n**PACF** is a partial auto-correlation function. Basically instead of finding correlations of present with lags like ACF, it finds correlation of the residuals (which remains after removing the effects which are already explained by the earlier lag(s)) with the next lag value hence \u2018partial\u2019 and not \u2018complete\u2019 as we remove already found variations before we find the next correlation. So if there is any hidden information in the residual which can be modeled by the next lag, we might get a good correlation and we will keep that next lag as a feature while modeling. Remember while modeling we don\u2019t want to keep too many features which are correlated as that can create multicollinearity issues. Hence we need to retain only the relevant features.","d7668f2a":"**Moving average (MA)** process, a process where the present value of series is defined as a linear combination of past errors. We assume the errors to be independently distributed with the normal distribution. The MA process of order q is defined as ,\n\n![](https:\/\/miro.medium.com\/max\/485\/1*Kj9au6g7FEh4rczVsO5MrA.jpeg)\n\nHere \u03f5t is a white noise. To get intuition of MA process lets consider order 1 MA process which will look like,\n\n![](https:\/\/miro.medium.com\/max\/215\/1*dfjgyPr40013a_4eFEJaLg.jpeg)\n\nOrder q of the MA process is obtained from the ACF plot, this is the lag after which ACF crosses the upper confidence interval for the first time.\n\nReference: https:\/\/towardsdatascience.com\/significance-of-acf-and-pacf-plots-in-time-series-analysis-2fa11a5d10a8","f6465460":"Approach: Here I will try to use the basic ARIMA model, below are the steps to start with the solution: \n\n1. Loading and Handling Time Series in Pandas\n2. How to Check Stationarity of a Time Series?\n3. How to make a Time Series Stationary?\n4. Forecasting a Time Series","6ad63dee":"**To be continued....**\n\nThis is just a rough prediction. I will be updating it in the upcoming versions by trying different values for p,d & q. Also will try different models. ","d7e3b8aa":"We can see that this model is better than simple ARIMA model. ","47441f42":"# Prediction using SARIMAX","5da3e2e3":"# How to determine additive or multiplicative model for decomposition?\n\nWe use multiplicative models when the magnitude of the seasonal pattern in the data depends on the magnitude of the data. On other hand, in the additive model, the magnitude of seasonality does not change in relation to time.\n\nDepending on whether the composition is multiplicative or additive, we\u2019ll need to divide or subtract the trend component from the original time series to retrieve the seasonal and noise components.\n\nReferences:\n\nhttps:\/\/sigmundojr.medium.com\/seasonality-in-python-additive-or-multiplicative-model-d4b9cf1f48a7\n\nhttps:\/\/towardsdatascience.com\/what-is-time-series-decomposition-and-how-does-it-work-9b67e007ae90 "}}