{"cell_type":{"bcbf08c2":"code","8d276897":"code","32b2f8d2":"code","6ea91547":"code","c4bab1a4":"code","40429889":"code","d1d250f4":"code","0ee4ee7d":"code","6cba20c7":"code","af658daa":"code","d6bd2cdc":"code","8b5b136f":"code","69ed9018":"code","97421fcf":"markdown","ad199fb9":"markdown"},"source":{"bcbf08c2":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nfrom scipy.special import expit","8d276897":"# x: (m x n), theta: (n, 1), y: (m, 1)\n\ndef sigmoid(X, theta):\n    theta_x = X.dot(theta)\n    return expit(theta_x)\n\n\ndef cost_function(y, theta, h):\n    assert h.shape == y.shape, \"h and y must be of the same shape, h (%s %s)\" % h.shape\n    m = y.shape[0]\n\n    y_0 = -y * np.log(h)\n    y_1 = - (1 - y) * np.log(1 - h)\n    return 1\/m * np.sum(y_0 + y_1)\n\ndef logisitcRegression(X, y, theta, alpha=0.05, n_iters=500):\n    assert X.shape[0] == y.shape[0]\n    assert X.shape[1] == theta.shape[0] \n\n    m = y.shape[0]\n    costs = np.zeros(n_iters)\n    \n    for i in range(n_iters):\n        h = sigmoid(X, theta)\n        gradient = X.T.dot(h - y)\/ m\n        \n        assert gradient.shape == theta.shape, \"Theta and gradient must be of the same shape. G (%s %s)\" % gradient\n        theta = theta - gradient\n        \n        costs[i] = cost_function(y, theta, h)\n    \n    return theta, costs\n\ndef predict(theta, x_test):\n    m = x_test.shape[0]\n    result = x_test.dot(theta)\n    outcome = np.zeros(m)\n    for i in range(m):\n        if result[i] > 0.5:\n            outcome[i] =1\n        else:\n            outcome[i] = 0\n    return outcome\n        \n    ","32b2f8d2":"data = pd.read_csv(\"..\/input\/ex2data1.txt\", names=[\"x1\",\"x2\",\"y\"])\ndata.head()","6ea91547":"y_data = data.filter(\"y\")\nx_data = data.drop(\"y\", axis=1)\nprint(y_data.head())\nprint(x_data.head())","c4bab1a4":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nx = sc.fit_transform(x_data)\nx[:2]\ny = y_data.values","40429889":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=10)\nx_train.shape, y_test.shape","d1d250f4":"intercept = np.ones((x_train.shape[0], 1))\nX_train = np.concatenate((intercept, x_train), axis=1)\nn = X.shape[1]\n\ntheta = np.zeros((n, 1))\n\nt, costs = logisitcRegression(X_train, y_train, theta)\nt","0ee4ee7d":"plt.plot(costs)","6cba20c7":"y_train_pred = predict(t, X_train)\nplt.figure(figsize=(20,4))\nplt.plot(y_train, \"bo\", ms=9)\nplt.plot(y_train_pred, \"ro\")","af658daa":"X_test = np.concatenate((np.ones((x_test.shape[0], 1)), x_test), axis=1)\n\ny_test_pred = predict(t, X_test)\n\nl = list(range(y_test.shape[0]))\nplt.plot(l, y_test, \"bo\", ms=\"8\")\nplt.plot(l, y_test_pred, \"ro\")","d6bd2cdc":"from sklearn.linear_model import LogisticRegression\n\n\nreg = LogisticRegression(solver=\"liblinear\")\n\nreg.fit(x_train, y_train)\n\nprint(\"Coeff\", reg.coef_)\nprint(\"Intercept\", reg.intercept_)","8b5b136f":"y_test_pred = reg.predict(x_test)\n\nl = list(range(y_test.shape[0]))\nplt.plot(l, y_test, \"bo\", ms=\"8\")\nplt.plot(l, y_test_pred, \"ro\")","69ed9018":"y_train_pred = reg.predict(x_train)\n\nplt.figure(figsize=(20, 4))\n\nplt.plot(y_train, \"bo\", ms=\"8\")\nplt.plot(y_train_pred, \"ro\")","97421fcf":"### Importing and cleaning Data","ad199fb9":"## Logisitc Regression\n\n$$H_\\theta(x) = \\frac{1}{1 +e^{-\\theta^Tx}}$$ . \n\n### Cost function (J)\n\n$$J = \\frac{1}{m}\\sum^m_{i=1} Cost(H_\\theta(x), y)$$  \n\n\n$Cost(h_\\theta(x^{(i)}),y^{(i)} = -log(h_\\theta(x))$, if y = 1, and  \n$Cost(h_\\theta(x^{(i)}),y^{(i)} = -log(1 - h_\\theta(x))$, if y = 0,\n\nA simplified cost is given as\n\n$$ -ylog(h_\\theta(x)) - (1 - y)log(1 - h_\\theta(x))$$\n\nPluggin this in, we have   \n\n$$J = \\frac{1}{m}\\sum^m_{i=1} [-ylog(h_\\theta(x)) - (1 - y)log(1 - h_\\theta(x))]$$   \n\n$$J = -\\frac{1}{m}\\sum^m_{i=1} [ylog(h_\\theta(x)) + (1 - y)log(1 - h_\\theta(x))]$$  \n\n### Gradient Descent\n\n{\n\n$$ \\theta_j := \\theta_j - \\alpha\\frac{\\partial}{\\partial x}J(\\theta) $$\n}\n\nSubstituting for J, and getting the partial derivative we have\n\n\n$$\\theta_j := \\theta_j - \\frac{\\alpha}{m}\\sum_{i=1}^m(h_\\theta(x^{(i)}) - y^{(i)}). x_j^{(i)}$$\n\n"}}