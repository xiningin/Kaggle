{"cell_type":{"f9607aca":"code","c7d322c3":"code","e1fb915c":"code","42ad81d9":"code","17b4e01c":"code","c4ffdfa0":"code","8819d94f":"code","1a268bc6":"code","10ecc26f":"code","13a2fdfd":"code","338af1cf":"code","9c46ebd9":"code","1d1c287b":"code","0df4af58":"code","52fd6c4b":"code","2cbc3db7":"code","9bb34f9e":"code","50e1aedd":"code","707866c3":"code","01203bc5":"code","7df01219":"code","dc637823":"code","aa8d661d":"code","953fec8c":"code","615132ea":"code","749b83fb":"code","f317e45b":"code","0915af81":"code","a8fb1442":"code","d8f7ca5c":"code","5c1d51a8":"markdown","5581a501":"markdown","f92a80fb":"markdown","983cb8b3":"markdown","03c06f9f":"markdown","d65528dd":"markdown","cf41cd00":"markdown","80318401":"markdown","be98ee80":"markdown","096fb3f1":"markdown","dbd659d4":"markdown","c0bc551a":"markdown","f4c62bf0":"markdown","282534d3":"markdown","b0f553d1":"markdown","500bf5bf":"markdown","d926c5f7":"markdown","ea39dc21":"markdown","958448e2":"markdown","a14d3e11":"markdown","f70b4165":"markdown","5ff8758a":"markdown","3ec2a899":"markdown","0a449655":"markdown","a5f59afc":"markdown","0aca2bf0":"markdown","d4d117ce":"markdown","0cfee68e":"markdown","484fd4de":"markdown"},"source":{"f9607aca":"import numpy as np\nimport pandas as pd\nimport re\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\n\nimport string\nfrom wordcloud import WordCloud\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\n\nfrom sklearn.metrics import confusion_matrix, f1_score\nfrom tqdm import tqdm","c7d322c3":"df_train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\nprint (df_train.shape)\ndf_train.head()","e1fb915c":"plt.figure(figsize=(8,5))\ntemp = df_train['target'].value_counts()\nsns.barplot(x=temp.index, y=temp.values)\nplt.xlabel(\"Target\", weight='bold', fontsize=15)\nplt.ylabel(\"Number of Tweets\", weight='bold', fontsize=15)\nplt.show()","42ad81d9":"df_test = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nprint (df_test.shape)\ndf_test.head()","17b4e01c":"from sklearn.model_selection import train_test_split\n\nX = df_train.text.values\ny = df_train.target.values\n\nX_train, X_val, y_train, y_val =\\\n    train_test_split(X, y, test_size=0.1, random_state=2020)","c4ffdfa0":"# Keep important columns\ntest_data = df_test[['id', 'text']]\n\n# Display 5 samples from the test data\ntest_data.sample(5)","8819d94f":"import torch\n\nif torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")","1a268bc6":"import nltk\n# Uncomment to download \"stopwords\"\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\ndef text_preprocessing(s):\n    \"\"\"\n    - Lowercase the sentence\n    - Change \"'t\" to \"not\"\n    - Remove \"@name\"\n    - Isolate and remove punctuations except \"?\"\n    - Remove other special characters\n    - Remove stop words except \"not\" and \"can\"\n    - Remove trailing whitespace\n    \"\"\"\n    s = s.lower()\n    # Change 't to 'not'\n    s = re.sub(r\"\\'t\", \" not\", s)\n    # Remove @name\n    s = re.sub(r'(@.*?)[\\s]', ' ', s)\n    # Isolate and remove punctuations except '?'\n    s = re.sub(r'([\\'\\\"\\.\\(\\)\\!\\?\\\\\\\/\\,])', r' \\1 ', s)\n    s = re.sub(r'[^\\w\\s\\?]', ' ', s)\n    # Remove some special characters\n    s = re.sub(r'([\\;\\:\\|\u2022\u00ab\\n])', ' ', s)\n    # Remove stopwords except 'not' and 'can'\n    s = \" \".join([word for word in s.split()\n                  if word not in stopwords.words('english')\n                  or word in ['not', 'can']])\n    # Remove trailing whitespace\n    s = re.sub(r'\\s+', ' ', s).strip()\n    \n    return s","10ecc26f":"%%time\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Preprocess text\nX_train_preprocessed = np.array([text_preprocessing(text) for text in X_train])\nX_val_preprocessed = np.array([text_preprocessing(text) for text in X_val])\n\n# Calculate TF-IDF\ntf_idf = TfidfVectorizer(ngram_range=(1, 3),\n                         binary=True,\n                         smooth_idf=False)\nX_train_tfidf = tf_idf.fit_transform(X_train_preprocessed)\nX_val_tfidf = tf_idf.transform(X_val_preprocessed)","13a2fdfd":"from sklearn.model_selection import StratifiedKFold, cross_val_score\n\ndef get_auc_CV(model):\n    \"\"\"\n    Return the average AUC score from cross-validation.\n    \"\"\"\n    # Set KFold to shuffle data before the split\n    kf = StratifiedKFold(5, shuffle=True, random_state=1)\n\n    # Get AUC scores\n    auc = cross_val_score(\n        model, X_train_tfidf, y_train, scoring=\"roc_auc\", cv=kf)\n\n    return auc.mean()","338af1cf":"from sklearn.naive_bayes import MultinomialNB\n\nres = pd.Series([get_auc_CV(MultinomialNB(i))\n                 for i in np.arange(1, 10, 0.1)],\n                index=np.arange(1, 10, 0.1))\n\nbest_alpha = np.round(res.idxmax(), 2)\nprint('Best alpha: ', best_alpha)\n\nplt.plot(res)\nplt.title('AUC vs. Alpha')\nplt.xlabel('Alpha')\nplt.ylabel('AUC')\nplt.show()","9c46ebd9":"from sklearn.metrics import accuracy_score, roc_curve, auc\n\ndef evaluate_roc(probs, y_true):\n    \"\"\"\n    - Print AUC and accuracy on the test set\n    - Plot ROC\n    @params    probs (np.array): an array of predicted probabilities with shape (len(y_true), 2)\n    @params    y_true (np.array): an array of the true values with shape (len(y_true),)\n    \"\"\"\n    preds = probs[:, 1]\n    fpr, tpr, threshold = roc_curve(y_true, preds)\n    roc_auc = auc(fpr, tpr)\n    print(f'AUC: {roc_auc:.4f}')\n       \n    # Get accuracy over the test set\n    y_pred = np.where(preds >= 0.5, 1, 0)\n    accuracy = accuracy_score(y_true, y_pred)\n    print(f'Accuracy: {accuracy*100:.2f}%')\n    \n    # Plot ROC AUC\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.xlim([0, 1])\n    plt.ylim([0, 1])\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.show()","1d1c287b":"# Compute predicted probabilities\nnb_model = MultinomialNB(alpha=1.8)\nnb_model.fit(X_train_tfidf, y_train)\nprobs = nb_model.predict_proba(X_val_tfidf)\n\n# Evaluate the classifier\nevaluate_roc(probs, y_val)","0df4af58":"!pip install transformers","52fd6c4b":"def text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text","2cbc3db7":"# Print sentence 0\nprint('Original: ', X[0])\nprint('Processed: ', text_preprocessing(X[0]))","9bb34f9e":"from transformers import BertTokenizer\n\n# Load the BERT tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n\n# Create a function to tokenize a set of texts\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # Create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n\n    # For every sentence...\n    for sent in data:\n        # `encode_plus` will:\n        #    (1) Tokenize the sentence\n        #    (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        #    (3) Truncate\/Pad sentence to max length\n        #    (4) Map tokens to their IDs\n        #    (5) Create attention mask\n        #    (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text=text_preprocessing(sent),  # Preprocess sentence\n            add_special_tokens=True,        # Add `[CLS]` and `[SEP]`\n            max_length=MAX_LEN,                  # Max length to truncate\/pad\n            pad_to_max_length=True,         # Pad sentence to max length\n            #return_tensors='pt',           # Return PyTorch tensor\n            return_attention_mask=True      # Return attention mask\n            )\n        \n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n\n    # Convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n\n    return input_ids, attention_masks","50e1aedd":"# Concatenate train data and test data\nall_tweets = np.concatenate([df_train.text.values, df_test.text.values])\n\n# Encode our concatenated data\nencoded_tweets = [tokenizer.encode(sent, add_special_tokens=True) for sent in all_tweets]\n\n# Find the maximum length\nmax_len = max([len(sent) for sent in encoded_tweets])\nprint('Max length: ', max_len)","707866c3":"MAX_LEN = 64\n\n# Print sentence 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ', X[0])\nprint('Token IDs: ', token_ids)\n\n# Run function `preprocessing_for_bert` on the train set and the validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","01203bc5":"from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\n# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n# For fine-tuning BERT, the authors recommend a batch size of 16 or 32.\nbatch_size = 32\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs, train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","7df01219":"%%time\nimport torch\nimport torch.nn as nn\nfrom transformers import BertModel\n\n# Create the BertClassfier class\nclass BertClassifier(nn.Module):\n    \"\"\"Bert Model for Classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param    bert: a BertModel object\n        @param    classifier: a torch.nn.Module classifier\n        @param    freeze_bert (bool): Set `False` to fine-tune the BERT model\n        \"\"\"\n        super(BertClassifier, self).__init__()\n        # Specify hidden size of BERT, hidden size of our classifier, and number of labels\n        D_in, H, D_out = 768, 50, 2\n\n        # Instantiate BERT model\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n\n        # Instantiate an one-layer feed-forward classifier\n        self.classifier = nn.Sequential(\n            nn.Linear(D_in, H),\n            nn.ReLU(),\n            #nn.Dropout(0.5),\n            nn.Linear(H, D_out)\n        )\n\n        # Freeze the BERT model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n        \n    def forward(self, input_ids, attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        # Feed input to BERT\n        outputs = self.bert(input_ids=input_ids,\n                            attention_mask=attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:, 0, :]\n\n        # Feed input to classifier to compute logits\n        logits = self.classifier(last_hidden_state_cls)\n\n        return logits\n","dc637823":"from transformers import AdamW, get_linear_schedule_with_warmup\n\ndef initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n\n    # Tell PyTorch to run the model on GPU\n    bert_classifier.to(device)\n\n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                      lr=5e-5,    # Default learning rate\n                      eps=1e-8    # Default epsilon value\n                      )\n\n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n\n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=0, # Default value\n                                                num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","aa8d661d":"import random\nimport time\n\n# Specify loss function\nloss_fn = nn.CrossEntropyLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels)\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20 batches\n            if (step % 20 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss \/ batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels)\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        preds = torch.argmax(logits, dim=1).flatten()\n\n        # Calculate the accuracy rate\n        accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy","953fec8c":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=2, evaluation=True)","615132ea":"import torch.nn.functional as F\n\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    probs = F.softmax(all_logits, dim=1).cpu().numpy()\n\n    return probs","749b83fb":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, val_dataloader)\n\n# Evaluate the Bert classifier\nevaluate_roc(probs, y_val)","f317e45b":"# Concatenate the train set and the validation set\nfull_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\nfull_train_sampler = RandomSampler(full_train_data)\nfull_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=32)\n\n# Train the Bert Classifier on the entire training data\nset_seed(42)\nbert_classifier, optimizer, scheduler = initialize_model(epochs=2)\ntrain(bert_classifier, full_train_dataloader, epochs=2)","0915af81":"# Run `preprocessing_for_bert` on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(df_test.text)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\ntest_sampler = SequentialSampler(test_dataset)\ntest_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)","a8fb1442":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\nthreshold = 0.9\npreds = np.where(probs[:, 1] > threshold, 1, 0)\n\n# Number of tweets predicted non-negative\nprint(\"Number of tweets predicted non-negative: \", preds.sum())","d8f7ca5c":"output = test_data[preds==1]\nlist(output.sample(20).text)","5c1d51a8":"BERT-base consists of 12 transformer layers, each transformer layer takes in a list of token embeddings, and produces the same number of embeddings with the same hidden size (or dimensions) on the output. The output of the final transformer layer of the [CLS] token is used as the features of the sequence to feed a classifier.\n\nThe transformers library has the BertForSequenceClassification class which is designed for classification tasks. However, we will create a new class so we can specify our own choice of classifiers.\n\nBelow we will create a BertClassifier class with a BERT model to extract the last hidden layer of the [CLS] token and a single-hidden-layer feed-forward neural network as our classifier","5581a501":"![](https:\/\/www.researchgate.net\/publication\/340223686\/figure\/fig1\/AS:873545626308609@1585280915468\/BERT-Original-sentence-how-are-you-doing-today.ppm)","f92a80fb":"**2.2. Create PyTorch DataLoader**","983cb8b3":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nTest Data\n<\/h1>\n<\/div>","03c06f9f":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nLibrary Loading\n<\/h1>\n<\/div>","d65528dd":"**3.4. Evaluation on Validation Set**","cf41cd00":"**3.3. Training Loop**","80318401":"**2. Train Naive Bayes Classifier**","be98ee80":"## Real vs Fake","096fb3f1":"**1.2. TF-IDF Vectorizer**","dbd659d4":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nD - Fine-tuning BERT\n<\/h1>\n<\/div>","c0bc551a":"**2. Tokenization and Input Formatting**","f4c62bf0":"Before tokenizing our text, we will perform some slight processing on our text including removing entity mentions (eg. @united) and some special character. The level of processing here is much less than in previous approachs because BERT was trained with the entire sentences.","282534d3":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nTrain Data\n<\/h1>\n<\/div>","b0f553d1":"In information retrieval, TF-IDF, short for term frequency\u2013inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. We will use TF-IDF to vectorize our text data before feeding them to machine learning algorithms.","500bf5bf":"**3.2. Optimizer & Learning Rate Scheduler**","d926c5f7":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nTraining our BertClassifier!\n<\/h1>\n<\/div>\n","ea39dc21":"**4. Predictions on Test Set**","958448e2":"The transformer library of Hugging Face contains PyTorch implementation of state-of-the-art NLP models including BERT (from Google), GPT (from OpenAI) ... and pre-trained model weights.","a14d3e11":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nPredictions\n<\/h1>\n<\/div>\n\n","f70b4165":"**3. Train Our Model**","5ff8758a":"**1. Install the Hugging Face Library**","3ec2a899":"**3.5. Train Our Model on the Entire Training Data**","0a449655":"**1. Data Preparation**","a5f59afc":"**3.1. Create BertClassifier**","0aca2bf0":"**2.1. BERT Tokenizer**","d4d117ce":"**2.2. Evaluation on Validation Set**","0cfee68e":"2.1. Hyperparameter Tuning","484fd4de":"<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#ffc299;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<h1 style=\"text-align: center;\n           padding: 10px;\n           color:white;\">\n\nC - Baseline: TF-IDF + Naive Bayes Classifier\n<\/h1>\n<\/div>\n"}}