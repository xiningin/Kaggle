{"cell_type":{"5ee05a04":"code","91b00faf":"code","d5d069a0":"code","1f8b6bcc":"code","57f1761d":"code","59179d8c":"code","51de059b":"code","1f9f468d":"code","4a842a7a":"code","0cada976":"code","411f58f4":"code","64998e7b":"code","08507200":"code","9ba38e15":"code","3ffd0b95":"code","bfe73cf1":"code","944bdf8a":"code","5a3d29ae":"code","707c649d":"code","8482e55a":"code","d452c325":"code","20a48856":"code","602afd02":"code","a47a5038":"code","dd0a1967":"code","dcb72d9c":"code","aa2796ae":"code","4b3337b0":"code","01c25e2c":"code","5eb23f79":"code","7b21d5b5":"code","cc4e1e98":"code","cb947ba6":"code","c2df7fb7":"code","d17cbe16":"markdown","ccc47a17":"markdown","7ef53744":"markdown","c0e85230":"markdown","9b24178d":"markdown","4187c8c4":"markdown","2cbb7b7e":"markdown","41460021":"markdown","d2c3dc97":"markdown","3701b8e9":"markdown","281b3041":"markdown","3f3680d5":"markdown","e1f87184":"markdown","85106b09":"markdown","091045de":"markdown","7aaa0591":"markdown","36e1fbe5":"markdown","f4f37916":"markdown","91822b4e":"markdown","cedb8aee":"markdown","75e9b566":"markdown","26497742":"markdown","757cb4bd":"markdown","4df443eb":"markdown","e054a823":"markdown","f5a72f4d":"markdown","12ce83ea":"markdown","48efff38":"markdown","43d08d90":"markdown","2507252f":"markdown","5a05ce02":"markdown","397c064e":"markdown","c6cda417":"markdown","03083e92":"markdown","8a46ba70":"markdown","f62f60c8":"markdown","eea1785d":"markdown","dabfdf92":"markdown"},"source":{"5ee05a04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport matplotlib\nplt.style.use(\"seaborn-whitegrid\")\n\nimport seaborn as sns\nfrom collections import OrderedDict\ncmaps = OrderedDict()\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91b00faf":"#load data\ndf = pd.read_csv(\"\/kaggle\/input\/customer-analytics\/Train.csv\")\ndf.head()","d5d069a0":"df.info()","1f8b6bcc":"df.describe()","57f1761d":"df.columns","59179d8c":"def categorical_vis(variable):\n    \n    var = df[variable].value_counts()\n    \n    x = var.index\n    y = var\n    if variable == \"Reached.on.Time_Y.N\":\n        plt.bar(x,y,edgecolor=\"#1F1F1F\",color='#636363')\n        plt.title(variable,fontsize=12,fontweight=\"bold\",color=\"#1F1F1F\")\n        plt.ylabel(\"Frequency\")\n        plt.xticks([0,1])\n        plt.show()\n        print(\"{}: \\n {}\".format(variable,var))\n    else:\n        plt.bar(x,y,edgecolor=\"#1F1F1F\",color='#636363')\n        plt.title(variable,fontsize=12,fontweight=\"bold\",color=\"#1F1F1F\")\n        plt.ylabel(\"Frequency\")\n        plt.show()\n        print(\"{}: \\n {}\".format(variable,var))","51de059b":"categorical = [\"Warehouse_block\",\"Customer_rating\",\"Mode_of_Shipment\",\"Product_importance\",\"Gender\",\"Reached.on.Time_Y.N\"]\nfor cat in categorical:\n    categorical_vis(cat)","1f9f468d":"def numerical_vis(variable):\n    plt.figure(figsize = (9,3))\n    plt.hist(df[variable],bins=50,color='#636363')\n    plt.xlabel(variable)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(variable),fontsize=12,fontweight=\"bold\",color=\"#1F1F1F\")\n    plt.show()","4a842a7a":"numerical = [\"Customer_care_calls\",\"Cost_of_the_Product\",\"Prior_purchases\",\"Discount_offered\",\"Weight_in_gms\"]\nfor num in numerical:\n    numerical_vis(num)","0cada976":"colors_blue = [\"#132C33\", \"#264D58\", '#17869E', '#51C4D3', '#B4DBE9']\ncolors_dark = [\"#1F1F1F\", \"#313131\", '#636363', '#AEAEAE', '#DADADA']\ncolors_red = [\"#331313\", \"#582626\", '#9E1717', '#D35151', '#E9B4B4']\ncolors_mix = [\"#17869E\", '#264D58', '#179E66', '#D35151', '#E9DAB4', '#E9B4B4', '#D3B651', '#6351D3']\ncolors_div = [\"#132C33\", '#17869E', '#DADADA', '#D35151', '#331313']\nsns.set_palette(\"mako\")\n\nsns.palplot(colors_blue)\nsns.palplot(colors_dark)\nsns.palplot(colors_red)\nsns.palplot(colors_mix)\nsns.palplot(colors_div)","411f58f4":"features = [\"Warehouse_block\",\"Mode_of_Shipment\",\"Customer_rating\",\"Product_importance\",\"Gender\",\"Customer_care_calls\",\"Prior_purchases\"]\n\nfor f in features:\n    fig, ax = plt.subplots(figsize=(9, 5))\n    sns.countplot(x=f,data=df,hue=\"Reached.on.Time_Y.N\",edgecolor=colors_dark[0])\n    ax.legend([\"Delivered on time\",\"Not delivered on time\"],loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=2, borderpad=1, frameon=True, fontsize=10)\n    plt.ylabel(\"Count\",fontsize=12, fontweight='light', color=colors_dark[0])\n    plt.xlabel(f\"{f}\",fontsize=12, fontweight='light', color=colors_dark[0])\n    plt.title(\"{} - Delivered on time\".format(f), fontsize=18, fontweight='bold', color=colors_dark[0])\n    plt.show()","64998e7b":"fig, ax = plt.subplots(figsize=(9,5))\nsns.scatterplot(x=\"Cost_of_the_Product\",y=\"Discount_offered\",hue=\"Reached.on.Time_Y.N\",data=df,edgecolor=colors_dark[0],palette=\"deep\")\nax.legend([\"Not delivered on time\",\"Delivered on time\"],loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=2, borderpad=1, frameon=True, fontsize=10)\nplt.ylabel(\"discount\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.xlabel(\"cost\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.title(\"Cost_of_the_Product - Discount_offered\", fontsize=18, fontweight='bold', color=colors_dark[0])\nplt.show()","08507200":"fig, ax = plt.subplots(figsize=(9, 5))\nsns.scatterplot(x=\"Weight_in_gms\",y=\"Cost_of_the_Product\",hue=\"Reached.on.Time_Y.N\",data=df,edgecolor=colors_dark[0],palette=\"deep\")\nax.legend([\"Not delivered on time\",\"Delivered on time\"],loc='upper center', bbox_to_anchor=(0.5, -0.2), ncol=2, borderpad=1, frameon=True, fontsize=10)\nplt.ylabel(\"Cost_of_the_Product\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.xlabel(\"Weight_in_gms\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.title(\"Weight_in_gms - Cost_of_the_Product\", fontsize=18, fontweight='bold', color=colors_dark[0])\nplt.show()","9ba38e15":"rate_mean = []\navg = df[\"Customer_rating\"].mean()\nfor i in range(2,8):\n    rate_mean.append(float(df[df[\"Customer_care_calls\"]==i][\"Customer_rating\"].mean()))\n    \ncalls=df.Customer_care_calls.sort_values(ascending=True).unique()","3ffd0b95":"df[df[\"Customer_care_calls\"]==i][\"Customer_rating\"].mean()\nfig, ax = plt.subplots(figsize=(9, 5))\nsns.barplot(x=calls,y=rate_mean,edgecolor=colors_dark[0],palette=\"deep\")\nax.axhline(avg, linestyle='--', color=\"black\")\nxmin, xmax = ax.get_xlim()\nymin, ymax = ax.get_ylim()\nax.text(\n    s=\"Avarage\\nRating: {:.2f}\".format(avg),\n    x=xmax*1.02,\n    y=avg,\n    backgroundcolor=colors_dark[2],\n    fontsize=10,\n    fontweight='bold',\n    color='white'\n)\nplt.ylabel(\"Average customer rating\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.xlabel(\"Customer care calls\",fontsize=12, fontweight='light', color=colors_dark[0])\nplt.title(\"Customer care calls - Average customer rating\", fontsize=18, fontweight='bold', color=colors_dark[0])\nplt.show()","bfe73cf1":"plt.figure(figsize=(9,5))\nsns.heatmap(df.corr(),annot=True)","944bdf8a":"boxes = [\"Cost_of_the_Product\",\"Discount_offered\",\"Weight_in_gms\"]\nfor bx in boxes:\n    g = sns.FacetGrid(df)\n    g.map(sns.boxplot, bx,palette=\"deep\")\n    plt.show()","5a3d29ae":"#import\nfrom scipy.stats import zscore\n\nfor i in boxes:\n    print(i)\n    z_scores = zscore(df[i])\n    print(\"*\"*50)\n    for threshold in np.arange(1.5,5,0.5):\n        print(\"Treshold: {}\".format(threshold))\n        print(\"Amount of Outliers: {}\".format(len((np.where(z_scores > threshold)[0]))))\n        print('------')","707c649d":"for i in boxes:\n    z_scores = zscore(df[i])\n    print(z_scores)","8482e55a":"for i in boxes:\n    z_scores = zscore(df[i])\n    df[\"z_scores_{}\".format(i)] = z_scores\ndf.head()","d452c325":"indeces = df[(df[\"z_scores_Cost_of_the_Product\"]>=2) | (df[\"z_scores_Discount_offered\"]>=2) | (df[\"z_scores_Weight_in_gms\"]>=2)].index.to_list()\ncleaned_data = df\n\nfor i in indeces:\n    cleaned_data = cleaned_data.drop(index=i,axis=0)\n\ncleaned_data[(cleaned_data[\"z_scores_Cost_of_the_Product\"]>=2) | (cleaned_data[\"z_scores_Discount_offered\"]>=2) | (cleaned_data[\"z_scores_Weight_in_gms\"]>=2)]","20a48856":"#Dropping z_score columns\ndf.drop(columns=[\"z_scores_Cost_of_the_Product\",\"z_scores_Discount_offered\",\"z_scores_Weight_in_gms\"],axis=1,inplace=True)\ncleaned_data.drop(columns=[\"z_scores_Cost_of_the_Product\",\"z_scores_Discount_offered\",\"z_scores_Weight_in_gms\"],axis=1,inplace=True)","602afd02":"#import\nfrom scipy.stats.mstats import winsorize\n\nwinsorized= winsorize(df[\"Discount_offered\"],(0,0.10))\n\n#Visualization\nplt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nplt.boxplot(winsorized)\nplt.title(\"Winsorized Data (Discount offered)\",fontsize=12,fontweight=\"bold\")\nplt.subplot(1,2,2)\nplt.boxplot(df[\"Discount_offered\"].values)\nplt.title(\"Normal Data (Discount offered)\",fontsize=12,fontweight=\"bold\")\nplt.show()","a47a5038":"winsorized_data = df\nwinsorized_data[\"Discount_offered\"] = winsorized\nwinsorized_data.head()","dd0a1967":"# Feature extraction - Discount Rate\nnew = winsorized_data[\"Cost_of_the_Product\"]\/winsorized_data[\"Weight_in_gms\"]\nwinsorized_data[\"discount_rate\"] = new.values","dcb72d9c":"#one hot encoding\ncleaned_data = pd.get_dummies(cleaned_data, columns=[\"Warehouse_block\",\"Mode_of_Shipment\",\"Gender\"])\n#label encoding\ncleaned_data[\"Product_importance\"].replace({\"low\":0,\"medium\":1,\"high\":2},inplace=True)\ncleaned_data[\"Reached.on.Time_Y.N\"].replace({1:0,0:1},inplace=True)\n#Drop ID column\ncleaned_data.drop(columns=[\"ID\"],axis=1,inplace=True)\n\n#one hot encoding\nwinsorized_data = pd.get_dummies(winsorized_data, columns=[\"Warehouse_block\",\"Mode_of_Shipment\",\"Gender\"])\n#label encoding\nwinsorized_data[\"Product_importance\"].replace({\"low\":0,\"medium\":1,\"high\":2},inplace=True)\nwinsorized_data[\"Reached.on.Time_Y.N\"].replace({1:0,0:1},inplace=True)\n#Drop ID column\nwinsorized_data.drop(columns=[\"ID\"],axis=1,inplace=True)\n\nwinsorized_data.reset_index(drop=True,inplace=True)\ncleaned_data.reset_index(drop=True,inplace=True)","aa2796ae":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\n\ncon_data= cleaned_data[[\"Customer_care_calls\",\"Cost_of_the_Product\",\"Discount_offered\",\"Weight_in_gms\"]]\ncon_data2= winsorized_data[[\"Customer_care_calls\",\"Cost_of_the_Product\",\"Discount_offered\",\"Weight_in_gms\"]]\n\nX = StandardScaler().fit_transform(con_data)\nX2 = StandardScaler().fit_transform(con_data2)\n\nsklearn_pca = PCA(n_components=2)\nY_sklearn = sklearn_pca.fit_transform(X)\n\nsklearn_pca2 = PCA(n_components=2)\nY_sklearn2 = sklearn_pca2.fit_transform(X2)\nprint(\"Outliers Deleted Data\")\nprint(sklearn_pca.explained_variance_ratio_)\nprint(\"Cumulative:\")\nvalue = 0\nfor i in sklearn_pca.explained_variance_ratio_:\n    value += i\n    print(value)\n\nprint(\"\\n\")\nprint(\"Winsorized Data\")  \nprint(sklearn_pca2.explained_variance_ratio_)\nprint(\"Cumulative:\")\nvalue = 0\nfor i in sklearn_pca2.explained_variance_ratio_:\n    value += i\n    print(value)","4b3337b0":"pca_new = pd.DataFrame(Y_sklearn,columns=[\"PC1\",\"PC2\"])\npca_new2 = pd.DataFrame(Y_sklearn2,columns=[\"PC1\",\"PC2\"])\n\nclean_pca = pd.concat([cleaned_data,pca_new],axis=1)\nclean_pca = clean_pca.drop(columns=[\"Customer_care_calls\",\"Cost_of_the_Product\",\"Discount_offered\",\"Weight_in_gms\"])\n\nwin_pca = pd.concat([winsorized_data,pca_new2],axis=1)\nwin_pca = win_pca.drop(columns=[\"Customer_care_calls\",\"Cost_of_the_Product\",\"Discount_offered\",\"Weight_in_gms\"])","01c25e2c":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#non-pca and outliers deleted data\nnon_cl_y = cleaned_data[\"Reached.on.Time_Y.N\"]\nnon_cl_X = cleaned_data.drop([\"Reached.on.Time_Y.N\"],axis=1)\n\nX_train1, X_test1, y_train1, y_test1 = train_test_split(non_cl_X,non_cl_y,test_size=0.33,random_state=123)\n\n#non-pca and winsorized data\nnon_win_y = winsorized_data[\"Reached.on.Time_Y.N\"]\nnon_win_X = winsorized_data.drop([\"Reached.on.Time_Y.N\"],axis=1)\n\nX_train2, X_test2, y_train2, y_test2 = train_test_split(non_win_X,non_win_y,test_size=0.33,random_state=123)\n\n#pca and outliers deleted data\ncl_y = clean_pca[\"Reached.on.Time_Y.N\"]\ncl_X = clean_pca.drop([\"Reached.on.Time_Y.N\"],axis=1)\n\nX_train3, X_test3, y_train3, y_test3 = train_test_split(cl_X,cl_y,test_size=0.33,random_state=123)\n\n#pca and winsorized data\nwin_y = win_pca[\"Reached.on.Time_Y.N\"]\nwin_X = win_pca.drop([\"Reached.on.Time_Y.N\"],axis=1)\n\nX_train4, X_test4, y_train4, y_test4 = train_test_split(win_X,win_y,test_size=0.33,random_state=123)\n","5eb23f79":"rf= RandomForestClassifier(max_depth = 7,min_samples_leaf=1,min_samples_split=4,criterion=\"entropy\",random_state=123)\n\nprint(\"non-pca and outliers deleted data\")\nrf.fit(X_train1,y_train1)\ny_pred1 = rf.predict(X_test1)\n\nprint(confusion_matrix(y_test1,y_pred1))\nprint(accuracy_score(y_test1,y_pred1))\nprint(\"*\"*30)\n\nprint(\"non-pca and winsorized data\")\nrf.fit(X_train2,y_train2)\ny_pred2 = rf.predict(X_test2)\n\nprint(confusion_matrix(y_test2,y_pred2))\nprint(accuracy_score(y_test2,y_pred2))\nprint(\"*\"*30)\n\nprint(\"pca and outliers deleted data\")\nrf.fit(X_train3,y_train3)\ny_pred3 = rf.predict(X_test3)\n\nprint(confusion_matrix(y_test3,y_pred3))\nprint(accuracy_score(y_test3,y_pred3))\nprint(\"*\"*30)\n\nprint(\"pca and winsorized data\")\nrf.fit(X_train4,y_train4)\ny_pred4 = rf.predict(X_test4)\n\nprint(confusion_matrix(y_test4,y_pred4))\nprint(accuracy_score(y_test4,y_pred4))","7b21d5b5":"knn= KNeighborsClassifier(n_neighbors=5)\n\nprint(\"non-pca and outliers deleted data\")\nknn.fit(X_train1,y_train1)\ny_pred1 = knn.predict(X_test1)\n\nprint(confusion_matrix(y_test1,y_pred1))\nprint(accuracy_score(y_test1,y_pred1))\nprint(\"*\"*30)\n\nprint(\"non-pca and winsorized data\")\nknn.fit(X_train2,y_train2)\ny_pred2 = knn.predict(X_test2)\n\nprint(confusion_matrix(y_test2,y_pred2))\nprint(accuracy_score(y_test2,y_pred2))\nprint(\"*\"*30)\n\nprint(\"pca and outliers deleted data\")\nknn.fit(X_train3,y_train3)\ny_pred3 = knn.predict(X_test3)\n\nprint(confusion_matrix(y_test3,y_pred3))\nprint(accuracy_score(y_test3,y_pred3))\nprint(\"*\"*30)\n\nprint(\"pca and winsorized data\")\nknn.fit(X_train4,y_train4)\ny_pred4 = knn.predict(X_test4)\n\nprint(confusion_matrix(y_test4,y_pred4))\nprint(accuracy_score(y_test4,y_pred4))","cc4e1e98":"from sklearn.ensemble import GradientBoostingClassifier\n\nGBC = GradientBoostingClassifier(loss=\"exponential\",n_estimators=90,criterion=\"mse\",random_state=123, )\n\nprint(\"non-pca and outliers deleted data\")\nGBC.fit(X_train1,y_train1)\ny_pred1 = GBC.predict(X_test1)\n\nprint(confusion_matrix(y_test1,y_pred1))\nprint(accuracy_score(y_test1,y_pred1))\nprint(\"*\"*30)\n\nprint(\"non-pca and winsorized data\")\nGBC.fit(X_train2,y_train2)\ny_pred2 = GBC.predict(X_test2)\n\nprint(confusion_matrix(y_test2,y_pred2))\nprint(accuracy_score(y_test2,y_pred2))\nprint(\"*\"*30)\n\nprint(\"pca and outliers deleted data\")\nGBC.fit(X_train3,y_train3)\ny_pred3 = GBC.predict(X_test3)\n\nprint(confusion_matrix(y_test3,y_pred3))\nprint(accuracy_score(y_test3,y_pred3))\nprint(\"*\"*30)\n\nprint(\"pca and winsorized data\")\nGBC.fit(X_train4,y_train4)\ny_pred4 = GBC.predict(X_test4)\n\nprint(confusion_matrix(y_test4,y_pred4))\nprint(accuracy_score(y_test4,y_pred4))","cb947ba6":"rf.get_params()\nparams = {\"max_depth\":[x for x in range (3,15)],\n         \"min_samples_split\":[x for x in range (2,5)],\n         \"min_samples_leaf\":[x for x in range (1,3)],\n         }","c2df7fb7":"from sklearn.model_selection import GridSearchCV\ngrid_cv = GridSearchCV(estimator=rf,\n                       param_grid = params,\n                       cv = 10\n                      )\ngrid_cv.fit(non_win_X, non_win_y)\n\nprint(\"Best Parameters : \", grid_cv.best_params_)","d17cbe16":"Customers who received a discount of more than 10 dollars did not receive on time delivery. This feature will be important for us to prediction.","ccc47a17":"<a id=\"1\"><\/a><br>\n# Load and Check Data","7ef53744":"<a id=\"2\"><\/a>\n# Dataset Description","c0e85230":"I made one hot encdoing for categorical features and label encoding for \"Product_importance\" to prepare our data for prediction. Also, as you know for delivered on time 1 was indicating \"NO\", and 0 was \"YES so that makes confisuon so I changed it as well. 0 is \"NO\" and 1 is \"YES\" from now on.","9b24178d":"'max_depth': 7, 'min_samples_leaf': 1, 'min_samples_split': 4","4187c8c4":"* Correllation matrix shows relationships among features. We interpreted that if we give more than 10 dollar discount to our customer, this order probably will not be on time. As you can see there is a correalation between reached on time and discount offered. When discount offered is going up, reaching on time going down. That might be seems wrong to you but as you know 1: NOT reached on time, 0: Reached on time.\n* Cost of product and customer care calls have more strong relationship compared to other relationships. That means if our costumer pays more money for product, they have tendecy to have more customer care callls.","2cbb7b7e":"<a id=\"10\"><\/a>\n### Dropping Outliers","41460021":"As we thought, the most number of outliers in \"discount_offered\" column. I will use 2 method to handle with outliers;\nFirst, I will drop them from our data. Also, I will try to solve that problem with winsorize so we can compare at the end which data set giving better results for prediction.\n","d2c3dc97":"<a id=\"7\"><\/a>\n## Basic Data Analysis","3701b8e9":"In this section, we will check if we have any outlier value and if we have, how we will handle with them.","281b3041":"<a id=\"6\"><\/a>\n## Numerical Variable Analysis","3f3680d5":"<a id=\"15\"><\/a>\n# Conclusion","e1f87184":"I will use 3 algorithms such as RandomForestClassifier, KNN, GradientBoostingClassifier for our 4 data set and will compare them each other.","85106b09":"We have 12 columns in this data set. We will check which columns means what in variable description section. Let's continue!","091045de":"* Customer who had 6 or 7 times customer care calls has more delivery on time probability.\n* Customer who had 4-6 times prior purchases has more delivery on time probability.","7aaa0591":"<a id=\"11\"><\/a>\n### Winsorization","36e1fbe5":"I made 4 data set which is outliers cleaned, winsorized and pca version both of them. When we check result better accuracy and precision scores came with RandomForest and non-pca winsorized data. I found best parameters for RandomForest with hyperparamter tuning method - GridSearchCV. So as a result, we approximately got 69.5 accuracy score with no-pca winsorized data and RandomForestClassifier.","f4f37916":"* 2000 - 4000 and 6000+ gram products are not delivered on time every time. \n* Delivered on time products' weights changing between 1000-2000 and 4000-6000 grams.","91822b4e":"We can quickly reach some statistical information with describe method() like mean, std, first and second quartile medians so on.","cedb8aee":"We got better scores with random forest and gradien boosting for \"non-pca and winsorized data\". As you can see, we just lost about %1 percent of accuracy with PCA. I will continue with random forest and \"non-pca and winsorized data\" because it gave us better accuracy and precision score. I will try to improve it with hyperparameter tuning.","75e9b566":"<a id=\"3\"><\/a>\n## Variable Description\n\nThe dataset used for model building contained 10999 observations of 12 variables.\nThe data contains the following information:\n\n1. ID: ID Number of Customers.\n2. Warehouse block: The Company have big Warehouse which is divided in to block such as A,B,C,D,E.\n3. Mode of shipment:The Company Ships the products in multiple way such as Ship, Flight and Road.\n4. Customer care calls: The number of calls made from enquiry for enquiry of the shipment.\n5. Customer rating: The company has rated from every customer. 1 is the lowest (Worst), 5 is the highest (Best).\n6. Cost of the product: Cost of the Product in US Dollars.\n7. Prior purchases: The Number of Prior Purchase.\n8. Product importance: The company has categorized the product in the various parameter such as low, medium, high.\n9. Gender: Male and Female.\n10. Discount offered: Discount offered on that specific product.\n11. Weight in gms: It is the weight in grams.\n12. Reached on time: It is the target variable, where 1 Indicates that the product has NOT reached on time and 0 indicates it has reached on time.","26497742":"<a id=\"4\"><\/a>\n# Univaraite Variable Analysis\n* Categorical variable: Warehouse block, Customer rating, Mode of shipment, Product importance, Gender, Reached on time(Target Value)\n* Numerical Variable: ID, Customer care calls, Cost of the product, Prior purchases, Discount offered, Weight in gms \n\nWe have 6 categorical variable and 6 numerical variable.\n","757cb4bd":"I used PCA for each data set we have (cleaned and winsorized). I put columns which have continous variables so we just have 4 column like this. I will convert these 4 column to 2 column. So now, we are ready for prediction. Are you excited about results? LET'S GO!!","4df443eb":"Let's look from surface to our data with info() method. We can see what are our variables' types and if there is null data.","e054a823":"I will make one sided winsorization because when we checked box plots that shows us we just have outliers at high values. I will winsorize them to 0.90. Let's start!","f5a72f4d":"<a id=\"14\"><\/a>\n## Hyperparamter Tuning - GridSearch CV","12ce83ea":"<a id=\"12\"><\/a>\n## Feature Engineering","48efff38":"Our outliers visibly decreased after winsorization. I will save them with new data frame which is will be called winsorized_data.","43d08d90":"<a id=\"9\"><\/a>\n## Z_score","2507252f":"<a id=\"13\"><\/a>\n# Prediction","5a05ce02":"I gave a new name to our data after drop outliers (cleaned_data). As you can see when we check if there is any row that has higher z_score than 2, we didn't get any so that means we got rid of them. Yuppi!","397c064e":"It seems like we have a lot of outliers for discount offered but we might have some outliers for other features as well. Let's find them with z_score!","c6cda417":"# Introduction\n\nAn international e-commerce company based wants to discover key insights from their customer database. They want to use some of the most advanced machine learning techniques to study their customers. The company sells electronic products.\n\n<font color=\"blue\">\nContent:\n\n1. [Load and Check Data](#1)\n1. [Dataset Description](#2)\n    * [Variable Description](#3)\n    * [Univeraite Variable Analysis](#4)\n        * [Categorical Variable Analysis](#5)\n        * [Numerical Variable Analysis](#6)\n   \n1. [Basic Data Analysis](#7)\n1. [Outlier Detection](#8)\n    * [Z_Score](#9)\n    * [Dropping Outliers](#10)\n    * [Winsorization](#11)\n1. [Feature Engineering](#12)\n1. [Prediction](#13)\n1. [Hyperparamater tuning - GridSearchCV](#14)\n1. [Conclusion](#15)","03083e92":"We can see that most of our shipments is not on time (on time: 4436, not on time: 6563). In mode of shipment, ship is dominating that category so we can interpret this probably most of our not on time shipment is coming from \"ship\" because they are slower than other methods.","8a46ba70":"There is no meaningful difference between them. As you can see, we are not getting better rating score if we call our customer more.","f62f60c8":"I will use z_score to see how many outlier we have. Also we can use Tukey Method for it but I just used it in my previous project (Titanic EDA) so I will continue with z_score this time.","eea1785d":"<a id=\"5\"><\/a>\n## Categorical Variable Analysis","dabfdf92":"<a id=\"8\"><\/a>\n# Outlier Detection"}}