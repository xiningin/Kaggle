{"cell_type":{"5316fa60":"code","91dbcee1":"code","d3fa58be":"code","1a39326d":"code","abc72e7b":"code","edf4e54f":"code","6cba62d9":"code","90dd13be":"code","e401f56d":"code","c3676540":"code","16836ba6":"code","095789b6":"code","70e4d8c9":"code","06a62465":"code","1671fbda":"code","53809b34":"code","d641ad5c":"code","4963c883":"code","3433be8b":"code","8a38be7c":"code","9d85fde3":"code","ce1b359c":"code","43b65179":"code","542f13b9":"code","7c06663c":"code","965f1a96":"code","663876c7":"code","f59e494b":"code","43376125":"code","94a9ef40":"code","c0474a5a":"code","a8139194":"code","ac812e1c":"code","500e5d96":"code","fd4c4054":"code","36c5e9cc":"code","3722ea3c":"code","cb3bf161":"code","46910212":"code","5b658d97":"code","35307ec3":"code","0f99c5f5":"code","61cfa0c9":"code","b2a86255":"code","f040802b":"code","c38d4b0b":"code","c0795d00":"code","4e71111e":"code","4f15fa7e":"code","22485230":"code","eebc22e5":"code","42889dd0":"code","21d344a5":"code","f1cdc76d":"code","0076c4b0":"code","1a93dca0":"code","ff3da14a":"code","1fc328fc":"code","702169d3":"code","84bb59a3":"code","679d5ad1":"code","2ab8e636":"code","989bbd50":"code","562e7f31":"code","ffd3d932":"code","8cd662a7":"code","38c509af":"code","4db7857d":"code","7a1a7276":"code","3e469dcb":"code","fd4c6c46":"code","848e10c8":"code","2c5f2c84":"code","27a89061":"code","378133c4":"code","82461792":"code","dc202b99":"code","dd48f2fa":"code","c1262c41":"code","ce1389d7":"code","c9294b2a":"code","39a03166":"code","b4583c0d":"code","89f070b9":"code","4742ccf4":"code","33759080":"code","3be69e7e":"code","45d51c72":"code","99efe1f2":"code","05916ec2":"code","2ea3d8ce":"code","8ff532f1":"code","ed3531e1":"code","e304dd75":"code","b6fa0bff":"code","f84bfb1b":"code","c90c04ec":"code","c7c61c90":"code","a3675fd8":"code","67eae7de":"code","7b504aa8":"code","2d3aa486":"code","53dcef6e":"code","3938fd11":"code","d5fc5f8d":"code","592fc767":"code","63d1b7f2":"code","18581546":"code","6e28d21f":"code","5b77b05f":"code","840383aa":"code","043da97e":"code","dc689f9c":"code","d5d8477d":"code","100cfdac":"code","d58c4da6":"code","2c661d70":"code","36ee796d":"code","1891c0c7":"code","b7fa1114":"code","cbf601f5":"code","d4ea9cda":"code","dcb3f2ad":"code","91e342e9":"code","0f7ec939":"code","aa1b38c8":"code","587550d5":"code","133ba11a":"code","b0a93bd1":"code","692edc3a":"code","b731e6c1":"code","5dd4db5e":"code","5911b380":"code","70afdec8":"code","1a3d11e1":"code","d4b0e424":"code","a2c866ca":"code","f9d81b22":"code","d89ba155":"code","96037908":"code","b94efed9":"code","d3515e4b":"code","ed8d882e":"code","83a40092":"code","93cf40a9":"code","6e168e9a":"code","dede19d5":"code","fe7cb443":"code","cb640b60":"code","d763c580":"code","f2c25567":"code","82ceaeca":"code","00021d76":"code","29c8f9b3":"code","7cc02a81":"code","d36cc45e":"code","2b3b3bfe":"code","481caebc":"code","f22a7f87":"code","6e03c150":"code","1418702c":"code","c2edd92c":"code","2e85b56d":"markdown","357239c9":"markdown","71ba070b":"markdown","68528a7d":"markdown","bb4ccb7d":"markdown","97513d37":"markdown","3b299fca":"markdown","da33c954":"markdown","a80be7f8":"markdown","2e461bb9":"markdown","e83cac43":"markdown","64424cf9":"markdown","0d33e280":"markdown","092f6796":"markdown","450f0496":"markdown","82436184":"markdown","5c334c68":"markdown","b8190565":"markdown","38041aa1":"markdown","50c5aa27":"markdown","f6811a9f":"markdown","1c2f00cd":"markdown","c726fb02":"markdown","750e8f15":"markdown","2fedd063":"markdown","68237088":"markdown","bef0d565":"markdown","b6904978":"markdown","0d121b70":"markdown","1fac17de":"markdown","c32b0fed":"markdown","66f430d7":"markdown","c3ac3c49":"markdown","8ee38757":"markdown","9b47992a":"markdown","adc7747d":"markdown","4b3499a8":"markdown","7a863517":"markdown","064f4b4b":"markdown"},"source":{"5316fa60":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","91dbcee1":"pd.set_option('display.max_columns', 500)","d3fa58be":"#COMMENT THIS SECTION INCASE RUNNING THIS NOTEBOOK LOCALLY\n\n#Checking the kaggle paths for the uploaded datasets\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1a39326d":"#INCASE RUNNING THIS LOCALLY, PASS THE RELATIVE PATH OF THE CSV FILES BELOW\n#(e.g. if files are in same folder as notebook, simple write \"train.csv\" as path)\n\ndf = pd.read_csv(\"..\/input\/mlc32telecomchurncasestudy\/train.csv\")\nunseen = pd.read_csv(\"..\/input\/mlc32telecomchurncasestudy\/test.csv\")\nsample = pd.read_csv(\"..\/input\/mlc32telecomchurncasestudy\/sample.csv\")\ndata_dict = pd.read_csv(\"..\/input\/mlc32telecomchurncasestudy\/data_dictionary.csv\")\n\nprint(df.shape)\nprint(unseen.shape)\nprint(sample.shape)\nprint(data_dict.shape)","abc72e7b":"df.head()","edf4e54f":"df.shape","6cba62d9":"df.info()","90dd13be":"df.describe()","e401f56d":"# Cheking percent of missing values in columns\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","c3676540":"# List the columns having more than 30% missing values\ncol_list_missing_30 = list(df_missing_columns.index[df_missing_columns['null'] > 30])","16836ba6":"# Delete the columns having more than 30% missing values\ndf = df.drop(col_list_missing_30, axis=1)","095789b6":"df.shape","70e4d8c9":"# List the date columns\ndate_cols = [k for k in df.columns.to_list() if 'date' in k]\nprint(date_cols) ","06a62465":"# Dropping date columns\ndf = df.drop(date_cols, axis=1)","1671fbda":"# Drop circle_id column\ndf = df.drop('circle_id', axis=1)","53809b34":"df.shape","d641ad5c":"df['avg_rech_amt_6_7'] = (df['total_rech_amt_6'] + df['total_rech_amt_7'])\/2","4963c883":"X = df['avg_rech_amt_6_7'].quantile(0.7)\nX","3433be8b":"df = df[df['avg_rech_amt_6_7'] >= X]\ndf.head()","8a38be7c":"df.shape","9d85fde3":"# Count the rows having more than 50% missing values\ndf_missing_rows_50 = df[(df.isnull().sum(axis=1)) > (len(df.columns)\/\/2)]\ndf_missing_rows_50.shape","ce1b359c":"# Deleting the rows having more than 50% missing values\ndf = df.drop(df_missing_rows_50.index)\ndf.shape","43b65179":"# Checking the missing values in columns again\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","542f13b9":"# Listing the columns of MOU Aug(8)\nprint(((df_missing_columns[df_missing_columns['null'] == 2.75]).index).to_list())","7c06663c":"# Creating a dataframe with the condition, in which MOU for Aug(8) are null\ndf_null_mou_8 = df[(df['loc_og_t2m_mou_8'].isnull()) & (df['loc_ic_t2f_mou_8'].isnull()) & (df['roam_og_mou_8'].isnull()) & (df['std_ic_t2m_mou_8'].isnull()) &\n  (df['loc_og_t2t_mou_8'].isnull()) & (df['std_ic_t2t_mou_8'].isnull()) & (df['loc_og_t2f_mou_8'].isnull()) & (df['loc_ic_mou_8'].isnull()) &\n  (df['loc_og_t2c_mou_8'].isnull()) & (df['loc_og_mou_8'].isnull()) & (df['std_og_t2t_mou_8'].isnull()) & (df['roam_ic_mou_8'].isnull()) &\n  (df['loc_ic_t2m_mou_8'].isnull()) & (df['std_og_t2m_mou_8'].isnull()) & (df['loc_ic_t2t_mou_8'].isnull()) & (df['std_og_t2f_mou_8'].isnull()) & \n  (df['std_og_t2c_mou_8'].isnull()) & (df['og_others_8'].isnull()) & (df['std_og_mou_8'].isnull()) & (df['spl_og_mou_8'].isnull()) & \n  (df['std_ic_t2f_mou_8'].isnull()) & (df['isd_og_mou_8'].isnull()) & (df['std_ic_mou_8'].isnull()) & (df['offnet_mou_8'].isnull()) & \n  (df['isd_ic_mou_8'].isnull()) & (df['ic_others_8'].isnull()) & (df['std_ic_t2o_mou_8'].isnull()) & (df['onnet_mou_8'].isnull()) & \n  (df['spl_ic_mou_8'].isnull())]\n\ndf_null_mou_8.head()","965f1a96":"df_null_mou_8.shape","663876c7":"# Deleting the records for which MOU for Aug(8) are null\ndf = df.drop(df_null_mou_8.index)","f59e494b":"# Again Cheking percent of missing values in columns\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","43376125":"# Listing the columns of MOU Jul(7)\nprint(((df_missing_columns[df_missing_columns['null'] == 0.62]).index).to_list())","94a9ef40":"# Creating a dataframe with the condition, in which MOU for Aug(8) are null\ndf_null_mou_7 = df[(df['loc_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2f_mou_7'].isnull()) & (df['roam_og_mou_7'].isnull()) & (df['std_ic_t2m_mou_7'].isnull()) &\n  (df['loc_og_t2t_mou_7'].isnull()) & (df['std_ic_t2t_mou_7'].isnull()) & (df['loc_og_t2f_mou_7'].isnull()) & (df['loc_ic_mou_7'].isnull()) &\n  (df['loc_og_t2c_mou_7'].isnull()) & (df['loc_og_mou_7'].isnull()) & (df['std_og_t2t_mou_7'].isnull()) & (df['roam_ic_mou_7'].isnull()) &\n  (df['loc_ic_t2m_mou_7'].isnull()) & (df['std_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2t_mou_7'].isnull()) & (df['std_og_t2f_mou_7'].isnull()) & \n  (df['std_og_t2c_mou_7'].isnull()) & (df['og_others_7'].isnull()) & (df['std_og_mou_7'].isnull()) & (df['spl_og_mou_7'].isnull()) & \n  (df['std_ic_t2f_mou_7'].isnull()) & (df['isd_og_mou_7'].isnull()) & (df['std_ic_mou_7'].isnull()) & (df['offnet_mou_7'].isnull()) & \n  (df['isd_ic_mou_7'].isnull()) & (df['ic_others_7'].isnull()) & (df['std_ic_t2o_mou_7'].isnull()) & (df['onnet_mou_7'].isnull()) & \n  (df['spl_ic_mou_7'].isnull())]\n\ndf_null_mou_7.head()","c0474a5a":"# Deleting the records for which MOU for Jul(7) are null\ndf = df.drop(df_null_mou_7.index)","a8139194":"# Again cheking percent of missing values in columns\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","ac812e1c":"# Listing the columns of MOU Jun(6)\nprint(((df_missing_columns[df_missing_columns['null'] == 0.56]).index).to_list())","500e5d96":"# Creating a dataframe with the condition, in which MOU for Jun(6) are null\ndf_null_mou_6 = df[(df['loc_og_t2m_mou_6'].isnull()) & (df['loc_ic_t2f_mou_6'].isnull()) & (df['roam_og_mou_6'].isnull()) & (df['std_ic_t2m_mou_6'].isnull()) &\n  (df['loc_og_t2t_mou_6'].isnull()) & (df['std_ic_t2t_mou_6'].isnull()) & (df['loc_og_t2f_mou_6'].isnull()) & (df['loc_ic_mou_6'].isnull()) &\n  (df['loc_og_t2c_mou_6'].isnull()) & (df['loc_og_mou_6'].isnull()) & (df['std_og_t2t_mou_6'].isnull()) & (df['roam_ic_mou_6'].isnull()) &\n  (df['loc_ic_t2m_mou_6'].isnull()) & (df['std_og_t2m_mou_6'].isnull()) & (df['loc_ic_t2t_mou_6'].isnull()) & (df['std_og_t2f_mou_6'].isnull()) & \n  (df['std_og_t2c_mou_6'].isnull()) & (df['og_others_6'].isnull()) & (df['std_og_mou_6'].isnull()) & (df['spl_og_mou_6'].isnull()) & \n  (df['std_ic_t2f_mou_6'].isnull()) & (df['isd_og_mou_6'].isnull()) & (df['std_ic_mou_6'].isnull()) & (df['offnet_mou_6'].isnull()) & \n  (df['isd_ic_mou_6'].isnull()) & (df['ic_others_6'].isnull()) & (df['std_ic_t2o_mou_6'].isnull()) & (df['onnet_mou_6'].isnull()) & \n  (df['spl_ic_mou_6'].isnull())]\n\ndf_null_mou_6.head()","fd4c4054":"# Deleting the records for which MOU for Jun(6) are null\ndf = df.drop(df_null_mou_6.index)","36c5e9cc":"# Again cheking percent of missing values in columns\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","3722ea3c":"# Listing the columns of MOU Jul(7)\nprint(((df_missing_columns[df_missing_columns['null'] == 0.0]).index).to_list())","cb3bf161":"# Creating a dataframe with the condition, in which MOU for Jul(7) are null\ndf_null_mou_7 = df[(df['loc_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2f_mou_7'].isnull()) & (df['roam_og_mou_7'].isnull()) & (df['std_ic_t2m_mou_7'].isnull()) &\n  (df['loc_og_t2t_mou_7'].isnull()) & (df['std_ic_t2t_mou_7'].isnull()) & (df['loc_og_t2f_mou_7'].isnull()) & (df['loc_ic_mou_7'].isnull()) &\n  (df['loc_og_t2c_mou_7'].isnull()) & (df['loc_og_mou_7'].isnull()) & (df['std_og_t2t_mou_7'].isnull()) & (df['roam_ic_mou_7'].isnull()) &\n  (df['loc_ic_t2m_mou_7'].isnull()) & (df['std_og_t2m_mou_7'].isnull()) & (df['loc_ic_t2t_mou_7'].isnull()) & (df['std_og_t2f_mou_7'].isnull()) & \n  (df['std_og_t2c_mou_7'].isnull()) & (df['og_others_7'].isnull()) & (df['std_og_mou_7'].isnull()) & (df['spl_og_mou_7'].isnull()) & \n  (df['std_ic_t2f_mou_7'].isnull()) & (df['isd_og_mou_7'].isnull()) & (df['std_ic_mou_7'].isnull()) & (df['offnet_mou_7'].isnull()) & \n  (df['isd_ic_mou_7'].isnull()) & (df['ic_others_7'].isnull()) & (df['std_ic_t2o_mou_7'].isnull()) & (df['onnet_mou_7'].isnull()) & \n  (df['spl_ic_mou_7'].isnull())]\n\ndf_null_mou_7.head()","46910212":"# Deleting the records for which MOU for Jul(7) are null\ndf = df.drop(df_null_mou_7.index)","5b658d97":"# Again cheking percent of missing values in columns\ndf_missing_columns = (round(((df.isnull().sum()\/len(df.index))*100),2).to_frame('null')).sort_values('null', ascending=False)\ndf_missing_columns","35307ec3":"df.shape","0f99c5f5":"# Checking percentage of rows we have lost while handling the missing values\nround((1- (len(df.index)\/30011)),2)","61cfa0c9":"df['churn'] = np.where((df['total_ic_mou_8']==0) & (df['total_og_mou_8']==0) & (df['vol_2g_mb_8']==0) & (df['vol_3g_mb_8']==0), 1, 0)","b2a86255":"df.head()","f040802b":"# List the columns for churn month(9)\ncol_8 = [col for col in df.columns.to_list() if '_8' in col]\nprint(col_8)","c38d4b0b":"# Deleting the churn month columns\ndf = df.drop(col_8, axis=1)","c0795d00":"# Dropping aug_vbc_3g column\ndf = df.drop('aug_vbc_3g', axis=1)","4e71111e":"round(100*(df['churn'].mean()),2)","4f15fa7e":"df['id'] = df['id'].astype(object)\ndf['churn'] = df['churn'].astype(object)","22485230":"df.info()","eebc22e5":"# List only the numeric columns\nnumeric_cols = df.select_dtypes(exclude=['object']).columns\nprint(numeric_cols)","42889dd0":"# Removing outliers below 10th and above 90th percentile\nfor col in numeric_cols: \n    q1 = df[col].quantile(0.10)\n    q3 = df[col].quantile(0.90)\n    iqr = q3-q1\n    range_low  = q1-1.5*iqr\n    range_high = q3+1.5*iqr\n    # Assigning the filtered dataset into data\n    data = df.loc[(df[col] > range_low) & (df[col] < range_high)]\n\ndata.shape","21d344a5":"# List the columns of total mou, rech_num and rech_amt\n[total for total in data.columns.to_list() if 'total' in total]","f1cdc76d":"# Total mou at good phase incoming and outgoing\ndata['total_mou_good'] = (data['total_og_mou_6'] + data['total_ic_mou_6'])# Avg. mou at action phase\n# We are taking average because there are two months(7 and 8) in action phase\ndata['avg_mou_action'] = (data['total_og_mou_6'] + data['total_og_mou_7'] + data['total_ic_mou_6'] + data['total_ic_mou_7'])\/2","0076c4b0":"# Difference avg_mou_good and avg_mou_action\ndata['diff_mou'] = data['avg_mou_action'] - data['total_mou_good']","1a93dca0":"# Checking whether the mou has decreased in action phase\ndata['decrease_mou_action'] = np.where((data['diff_mou'] < 0), 1, 0)","ff3da14a":"data.head()","1fc328fc":"# Avg rech number at action phase\ndata['avg_rech_num_action'] = (data['total_rech_num_6'] + data['total_rech_num_7'])\/2","702169d3":"# Difference total_rech_num_6 and avg_rech_action\ndata['diff_rech_num'] = data['avg_rech_num_action'] - data['total_rech_num_6']","84bb59a3":"# Checking if rech_num has decreased in action phase\ndata['decrease_rech_num_action'] = np.where((data['diff_rech_num'] < 0), 1, 0)","679d5ad1":"data.head()","2ab8e636":"# Avg rech_amt in action phase\ndata['avg_rech_amt_action'] = (data['total_rech_amt_6'] + data['total_rech_amt_7'])\/2","989bbd50":"# Difference of action phase rech amt and good phase rech amt\ndata['diff_rech_amt'] = data['avg_rech_amt_action'] - data['total_rech_amt_6']","562e7f31":"# Checking if rech_amt has decreased in action phase\ndata['decrease_rech_amt_action'] = np.where((data['diff_rech_amt'] < 0), 1, 0) ","ffd3d932":"data.head()","8cd662a7":"# ARUP in action phase\ndata['avg_arpu_action'] = (data['arpu_6'] + data['arpu_7'])\/2","38c509af":"# Difference of good and action phase ARPU\ndata['diff_arpu'] = data['avg_arpu_action'] - data['arpu_6']","4db7857d":"# Checking whether the arpu has decreased on the action month\ndata['decrease_arpu_action'] = np.where(data['diff_arpu'] < 0, 1, 0)","7a1a7276":"data.head()","3e469dcb":"# VBC in action phase\ndata['avg_vbc_3g_action'] = (data['jun_vbc_3g'] + data['jul_vbc_3g'])\/2","fd4c6c46":"# Difference of good and action phase VBC\ndata['diff_vbc'] = data['avg_vbc_3g_action'] - data['jun_vbc_3g']","848e10c8":"# Checking whether the VBC has decreased on the action month\ndata['decrease_vbc_action'] = np.where(data['diff_vbc'] < 0 , 1, 0)","2c5f2c84":"data.head()","27a89061":"# Converting churn column to int in order to do aggfunc in the pivot table\ndata['churn'] = data['churn'].astype('int64')","378133c4":"data.pivot_table(values='churn', index='decrease_mou_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","82461792":"data.pivot_table(values='churn', index='decrease_rech_num_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","dc202b99":"data.pivot_table(values='churn', index='decrease_rech_amt_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","dd48f2fa":"data.pivot_table(values='churn', index='decrease_vbc_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","c1262c41":"# Creating churn dataframe\ndata_churn = data[data['churn'] == 1]\n# Creating not churn dataframe\ndata_non_churn = data[data['churn'] == 0]","ce1389d7":"# Distribution plot\nax = sns.distplot(data_churn['avg_arpu_action'],label='churn',hist=False)\nax = sns.distplot(data_non_churn['avg_arpu_action'],label='not churn',hist=False)\nax.set(xlabel='Action phase ARPU')","c9294b2a":"# Distribution plot\nax = sns.distplot(data_churn['total_mou_good'],label='churn',hist=False)\nax = sns.distplot(data_non_churn['total_mou_good'],label='non churn',hist=False)\nax.set(xlabel='Action phase MOU')","39a03166":"data.pivot_table(values='churn', index='decrease_rech_amt_action', columns='decrease_rech_num_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","b4583c0d":"data.pivot_table(values='churn', index='decrease_rech_amt_action', columns='decrease_vbc_action', aggfunc='mean').plot.bar()\nplt.ylabel('churn rate')\nplt.show()","89f070b9":"plt.figure(figsize=(10,6))\nax = sns.scatterplot('avg_rech_num_action','avg_rech_amt_action', hue='churn', data=data)","4742ccf4":"data = data.drop(['total_mou_good','avg_mou_action','diff_mou','avg_rech_num_action','diff_rech_num','avg_rech_amt_action',\n                 'diff_rech_amt','avg_arpu_action','diff_arpu','avg_vbc_3g_action','diff_vbc','avg_rech_amt_6_7'], axis=1)","33759080":"# Import library\nfrom sklearn.model_selection import train_test_split","3be69e7e":"# Putting feature variables into X\nX = data.drop(['id','churn'], axis=1)","45d51c72":"# Putting target variable to y\ny = data['churn']","99efe1f2":"# Splitting data into train and test set 80:20\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, test_size=0.2, random_state=100)","05916ec2":"# Imporing SMOTE\nfrom imblearn.over_sampling import SMOTE","2ea3d8ce":"# Instantiate SMOTE\nsm = SMOTE(random_state=27)","8ff532f1":"# Fittign SMOTE to the train set\nX_train, y_train = sm.fit_resample(X_train, y_train)","ed3531e1":"# Standardization method\nfrom sklearn.preprocessing import StandardScaler","e304dd75":"# Instantiate the Scaler\nscaler = StandardScaler()","b6fa0bff":"# List of the numeric columns\ncols_scale = X_train.columns.to_list()\n# Removing the derived binary columns \ncols_scale.remove('decrease_mou_action')\ncols_scale.remove('decrease_rech_num_action')\ncols_scale.remove('decrease_rech_amt_action')\ncols_scale.remove('decrease_arpu_action')\ncols_scale.remove('decrease_vbc_action')","f84bfb1b":"# Fit the data into scaler and transform\nX_train[cols_scale] = scaler.fit_transform(X_train[cols_scale])","c90c04ec":"X_train.head()","c7c61c90":"# Transform the test set\nX_test[cols_scale] = scaler.transform(X_test[cols_scale])\nX_test.head()","a3675fd8":"#Import PCA\nfrom sklearn.decomposition import PCA","67eae7de":"# Instantiate PCA\npca = PCA(random_state=42)","7b504aa8":"# Fit train set on PCA\npca.fit(X_train)","2d3aa486":"# Principal components\npca.components_","53dcef6e":"# Cumuliative varinace of the PCs\nvariance_cumu = np.cumsum(pca.explained_variance_ratio_)\nprint(variance_cumu)","3938fd11":"# Plotting scree plot\nfig = plt.figure(figsize = (10,6))\nplt.plot(variance_cumu)\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Variance')","d5fc5f8d":"# Importing incremental PCA\nfrom sklearn.decomposition import IncrementalPCA","592fc767":"# Instantiate PCA with 60 components\npca_final = IncrementalPCA(n_components=60)","63d1b7f2":"# Fit and transform the X_train\nX_train_pca = pca_final.fit_transform(X_train)","18581546":"X_test_pca = pca_final.transform(X_test)","6e28d21f":"# Importing scikit logistic regression module\nfrom sklearn.linear_model import LogisticRegression","5b77b05f":"# Impoting metrics\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix","840383aa":"# Importing libraries for cross validation\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV","043da97e":"# Creating KFold object with 5 splits\nfolds = KFold(n_splits=5, shuffle=True, random_state=4)\n\n# Specify params\nparams = {\"C\": [0.01, 0.1, 1, 10, 100, 1000]}\n\n# Specifing score as recall as we are more focused on acheiving the higher sensitivity than the accuracy\nmodel_cv = GridSearchCV(estimator = LogisticRegression(),\n                        param_grid = params, \n                        scoring= 'recall', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True) \n\n# Fit the model\nmodel_cv.fit(X_train_pca, y_train)","dc689f9c":"# results of grid search CV\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","d5d8477d":"# plot of C versus train and validation scores\n\nplt.figure(figsize=(8, 6))\nplt.plot(cv_results['param_C'], cv_results['mean_test_score'])\nplt.plot(cv_results['param_C'], cv_results['mean_train_score'])\nplt.xlabel('C')\nplt.ylabel('sensitivity')\nplt.legend(['test result', 'train result'], loc='upper left')\nplt.xscale('log')","100cfdac":"# Best score with best C\nbest_score = model_cv.best_score_\nbest_C = model_cv.best_params_['C']\n\nprint(\" The highest test sensitivity is {0} at C = {1}\".format(best_score, best_C))","d58c4da6":"# Instantiate the model with best C\nlogistic_pca = LogisticRegression(C=best_C)","2c661d70":"# Fit the model on the train set\nlog_pca_model = logistic_pca.fit(X_train_pca, y_train)","36ee796d":"# Predictions on the train set\ny_train_pred = log_pca_model.predict(X_train_pca)","1891c0c7":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_train, y_train_pred)\nprint(confusion)","b7fa1114":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","cbf601f5":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","d4ea9cda":"# Importing SVC\nfrom sklearn.svm import SVC","dcb3f2ad":"# specify range of hyperparameters\n\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n\n# specify model with RBF kernel\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = 3, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train_pca, y_train) ","91e342e9":"# cv results\ncv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","0f7ec939":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.80, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='upper left')\nplt.xscale('log')","aa1b38c8":"# Printing the best score \nbest_score = model_cv.best_score_\nbest_hyperparams = model_cv.best_params_\n\nprint(\"The best test score is {0} corresponding to hyperparameters {1}\".format(best_score, best_hyperparams))","587550d5":"# Building the model with optimal hyperparameters\nsvm_pca_model = SVC(C=100, gamma=0.0001, kernel=\"rbf\")\n\nsvm_pca_model.fit(X_train_pca, y_train)","133ba11a":"# Predictions on the train set\ny_train_pred = svm_pca_model.predict(X_train_pca)","b0a93bd1":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_train, y_train_pred)\nprint(confusion)","692edc3a":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","b731e6c1":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","5dd4db5e":"# Prediction on the test set\ny_test_pred = svm_pca_model.predict(X_test_pca)","5911b380":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_test, y_test_pred)\nprint(confusion)","70afdec8":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","1a3d11e1":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","d4b0e424":"# Importing decision tree classifier\nfrom sklearn.tree import DecisionTreeClassifier","a2c866ca":"# Create the parameter grid \nparam_grid = {\n    'max_depth': range(5, 15, 5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n}\n\n\n# Instantiate the grid search model\ndtree = DecisionTreeClassifier()\n\ngrid_search = GridSearchCV(estimator = dtree, \n                           param_grid = param_grid, \n                           scoring= 'recall',\n                           cv = 5, \n                           verbose = 1)\n\n# Fit the grid search to the data\ngrid_search.fit(X_train_pca,y_train)","f9d81b22":"# cv results\ncv_results = pd.DataFrame(grid_search.cv_results_)\ncv_results","d89ba155":"# Printing the optimal sensitivity score and hyperparameters\nprint(\"Best sensitivity:-\", grid_search.best_score_)\nprint(grid_search.best_estimator_)","96037908":"# Model with optimal hyperparameters\ndt_pca_model = DecisionTreeClassifier(criterion = \"gini\", \n                                  random_state = 100,\n                                  max_depth=10, \n                                  min_samples_leaf=50,\n                                  min_samples_split=50)\n\ndt_pca_model.fit(X_train_pca, y_train)","b94efed9":"# Predictions on the train set\ny_train_pred = dt_pca_model.predict(X_train_pca)","d3515e4b":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_train, y_train_pred)\nprint(confusion)","ed8d882e":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","83a40092":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","93cf40a9":"# Prediction on the test set\ny_test_pred = dt_pca_model.predict(X_test_pca)","6e168e9a":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_test, y_test_pred)\nprint(confusion)","dede19d5":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","fe7cb443":"# Importing random forest classifier\nfrom sklearn.ensemble import RandomForestClassifier","cb640b60":"param_grid = {\n    'max_depth': range(5,10,5),\n    'min_samples_leaf': range(50, 150, 50),\n    'min_samples_split': range(50, 150, 50),\n    'n_estimators': [100,200,300], \n    'max_features': [10, 20]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = param_grid, \n                           cv = 3,\n                           n_jobs = -1,\n                           verbose = 1, \n                           return_train_score=True)\n\n# Fit the model\ngrid_search.fit(X_train_pca, y_train)","d763c580":"# printing the optimal accuracy score and hyperparameters\nprint('We can get accuracy of',grid_search.best_score_,'using',grid_search.best_params_)","f2c25567":"# model with the best hyperparameters\n\nrfc_model = RandomForestClassifier(bootstrap=True,\n                             max_depth=5,\n                             min_samples_leaf=50, \n                             min_samples_split=100,\n                             max_features=20,\n                             n_estimators=300)","82ceaeca":"# Fit the model\nrfc_model.fit(X_train_pca, y_train)","00021d76":"# Predictions on the train set\ny_train_pred = rfc_model.predict(X_train_pca)# Confusion matrix\nconfusion = metrics.confusion_matrix(y_train, y_train_pred)\nprint(confusion)","29c8f9b3":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","7cc02a81":"# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_train, y_train_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","d36cc45e":"# Prediction on the test set\ny_test_pred = rfc_model.predict(X_test_pca)","2b3b3bfe":"# Confusion matrix\nconfusion = metrics.confusion_matrix(y_test, y_test_pred)\nprint(confusion)","481caebc":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives# Accuracy\nprint(\"Accuracy:-\",metrics.accuracy_score(y_test, y_test_pred))\n\n# Sensitivity\nprint(\"Sensitivity:-\",TP \/ float(TP+FN))\n\n# Specificity\nprint(\"Specificity:-\", TN \/ float(TN+FP))","f22a7f87":"##### Importing stats model\nimport statsmodels.api as sm","6e03c150":"# Instantiate the model\n# Adding the constant to X_train\nlog_no_pca = sm.GLM(y_train,(sm.add_constant(X_train)), family=sm.families.Binomial())","1418702c":"# Fit the model\nlog_no_pca = log_no_pca.fit().summary()","c2edd92c":"# Summary\nlog_no_pca","2e85b56d":"## Logistic regression with No PCA","357239c9":"##### Hyperparameter tuning","71ba070b":"**Feature Scaling**","68528a7d":"# 1. Loading dependencies & datasets\n\nLets start by loading our dependencies. We can keep adding any imports to this cell block, as we write mode and mode code.","bb4ccb7d":"##### Prediction on the train set","97513d37":"##### Hyperparameter tuning","3b299fca":"# Without PCA","da33c954":"**Train-Test Split**","a80be7f8":"**Build the model with optimal hyperparameters**","2e461bb9":"**Random forest with PCA**","e83cac43":"# Telecom Churn Prediction - Starter Notebook\n\n**Author:** Vinay Shivaram","64424cf9":"**Prediction on the train set**","0d33e280":"##### Prediction on the train set","092f6796":"Next, we load our datasets and the data dictionary file.\n\nThe **train.csv** file contains both dependent and independent features, while the **test.csv** contains only the independent variables. \n\nSo, for model selection, I will create our own train\/test dataset from the **train.csv** and use the model to predict the solution using the features in unseen test.csv data for submission.","450f0496":"Dropping circle_id column as this column has only one unique value. Hence there will be no impact of this column on the data analysis.","82436184":"##### Prediction on the test set","5c334c68":"**Applying transformation on the test set**\nWe are only doing Transform in the test set not the Fit-Transform. Because the Fitting is already done on the train set. So, we just have to do the transformation with the already fitted data on the train set.","b8190565":"**Model with PCA**","38041aa1":"**Dealing with data imbalance**\nWe are creating synthetic samples by doing upsampling using SMOTE(Synthetic Minority Oversampling Technique).","50c5aa27":"##### Prediction on the train set","f6811a9f":"**Performing PCA with 60 components**","1c2f00cd":"## Decision tree with PCA","c726fb02":"# 0. Problem statement\n\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\nFor many incumbent operators, retaining high profitable customers is the number one business\ngoal. To reduce customer churn, telecom companies need to predict which customers are at high risk of churn. In this project, you will analyze customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn, and identify the main indicators of churn.\n\nIn this competition, your goal is *to build a machine learning model that is able to predict churning customers based on the features provided for their usage.*\n\n**Customer behaviour during churn:**\n\nCustomers usually do not decide to switch to another competitor instantly, but rather over a\nperiod of time (this is especially applicable to high-value customers). In churn prediction, we\nassume that there are three phases of customer lifecycle :\n\n1. <u>The \u2018good\u2019 phase:<\/u> In this phase, the customer is happy with the service and behaves as usual.\n\n2. <u>The \u2018action\u2019 phase:<\/u> The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. It is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\n3. <u>The \u2018churn\u2019 phase:<\/u> In this phase, the customer is said to have churned. In this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month (September) is the \u2018churn\u2019 phase.","750e8f15":"Handling missing values\nand  Handling missing values in columns","2fedd063":"### Feature Selection Using RFE","68237088":"##### Model with optimal hyperparameters","bef0d565":"##### Prediction on the test set","b6904978":"Deleting the date columns as the date columns are not required in our analysis","0d121b70":"**Support Vector Machine(SVM) with PCA**","1fac17de":"From the above plot, we can see that higher value of gamma leads to overfitting the model. With the lowest value of gamma (0.0001) we have train and test accuracy almost same.\n\nAlso, at C=100 we have a good accuracy and the train and test scores are comparable.\n\nThough sklearn suggests the optimal scores mentioned above (gamma=0.01, C=1000), one could argue that it is better to choose a simpler, more non-linear model with gamma=0.0001. This is because the optimal values mentioned here are calculated based on the average test accuracy (but not considering subjective parameters such as model complexity).\n\nWe can achieve comparable average test accuracy (~90%) with gamma=0.0001 as well, though we'll have to increase the cost C for that. So to achieve high accuracy, there's a tradeoff between:\n- High gamma (i.e. high non-linearity) and average value of C\n- Low gamma (i.e. less non-linearity) and high value of C\n\nWe argue that the model will be simpler if it has as less non-linearity as possible, so we choose gamma=0.0001 and a high C=100.","c32b0fed":"#### Top predictors\n\nBelow are few top variables selected in the logistic regression model.\n\n| Variables   | Coefficients |\n|---------------------|--------------|\n|loc_ic_mou_8|-3.3287|\n|og_others_7|-2.4711|\n|ic_others_8|-1.5131|\n|isd_og_mou_8|-1.3811|\n|decrease_vbc_action|-1.3293|\n|monthly_3g_8|-1.0943|\n|std_ic_t2f_mou_8|-0.9503|\n|monthly_2g_8|-0.9279|\n|loc_ic_t2f_mou_8|-0.7102|\n|roam_og_mou_8|0.7135|\n\nWe can see most of the top variables have negative coefficients. That means, the variables are inversely correlated with the churn probablity.\n\nE.g.:- \n\nIf the local incoming minutes of usage (loc_ic_mou_8) is lesser in the month of August than any other month, then there is a higher chance that the customer is likely to churn.\n\n***Recomendations***\n\n1. Target the customers, whose minutes of usage of the incoming local calls and outgoing ISD calls are less in the action phase (mostly in the month of August).\n2. Target the customers, whose outgoing others charge in July and incoming others on August are less.\n3. Also, the customers having value based cost in the action phase increased are more likely to churn than the other customers. Hence, these customers may be a good target to provide offer.\n4. Cutomers, whose monthly 3G recharge in August is more, are likely to be churned. \n5. Customers having decreasing STD incoming minutes of usage for operators T to fixed lines of T for the month of August are more likely to churn.\n6. Cutomers decreasing monthly 2g usage for August are most probable to churn.\n7. Customers having decreasing incoming minutes of usage for operators T to fixed lines of T for August are more likely to churn.\n8. roam_og_mou_8 variables have positive coefficients (0.7135). That means for the customers, whose roaming outgoing minutes of usage is increasing are more likely to churn.\n","66f430d7":"##### Prediction on the test set","c3ac3c49":"***Model analysis***\n1. We can see that there are few features have positive coefficients and few have negative.\n2. Many features have higher p-values and hence became insignificant in the model.\n\n***Coarse tuning (Auto+Manual)***\n\nWe'll first eliminate a few features using Recursive Feature Elimination (RFE), and once we have reached a small set of variables to work with, we can then use manual feature elimination (i.e. manually eliminating features based on observing the p-values and VIFs).","8ee38757":"Reading and understanding the data","9b47992a":"**Logistic regression with optimal C**","adc7747d":"The goal of this notebook is to provide an overview of how write a notebook and create a submission file that successfully solves the churn prediction problem. Please download the datasets, unzip and place them in the same folder as this notebook.\n\nWe are going to follow the process called CRISP-DM.\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/b\/b9\/CRISP-DM_Process_Diagram.png\/639px-CRISP-DM_Process_Diagram.png\" style=\"height: 400px; width:400px;\"\/>\n\nAfter Business and Data Understanding via EDA, we want to prepare data for modelling. Then evaluate and submit our predictions.","4b3499a8":"**Plotting the accuracy with various C and gamma values**","7a863517":"**Logistic regression with PCA**","064f4b4b":"##### Model with optimal hyperparameters"}}