{"cell_type":{"f69bdd23":"code","5ae9e7c2":"code","024c32db":"code","80786f07":"code","bdef86e4":"code","a1542670":"code","1942a52b":"code","fb7ca04f":"code","c4104d58":"code","2acffef1":"code","436ef5f2":"code","c7393ce2":"code","b2840f1f":"code","5b0b7a61":"code","cb6aab77":"code","b28852c4":"code","54a6143e":"code","5e1586ca":"code","97201cf2":"code","bc1b1116":"code","3224d9eb":"code","f287d81a":"code","9f59ac4a":"code","6b7e9604":"code","46713ce5":"code","cc73da57":"code","e1f09e50":"code","f44767c6":"code","0de94037":"code","089f41d4":"code","58e8e7d4":"code","bed13ed2":"code","3461ae64":"code","be1e298d":"code","3bee211a":"code","d24cadb7":"code","d887422a":"code","dee53461":"code","032d4269":"markdown","4b0f4178":"markdown","be9fe996":"markdown","5856571c":"markdown","e3058744":"markdown","cce5a807":"markdown","10076349":"markdown","5b3ebc36":"markdown","f131761b":"markdown","5aa126f1":"markdown","737da517":"markdown","039ae36d":"markdown","b14f0bb4":"markdown","97a515b9":"markdown","a5891060":"markdown","63364371":"markdown","b9e16d2f":"markdown","39e64ccd":"markdown","b21260de":"markdown","ea60904a":"markdown"},"source":{"f69bdd23":"# Load the necessary libraries\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5ae9e7c2":"# Read all data\ncountry = pd.read_csv('..\/input\/world-development-indicators\/Country.csv')\nindicators = pd.read_csv('..\/input\/world-development-indicators\/Indicators.csv')\nseries = pd.read_csv('..\/input\/world-development-indicators\/Series.csv')","024c32db":"# Inspect the data\ncountry.columns","80786f07":"indicators.columns","bdef86e4":"series.columns","a1542670":"# Let's dive into the given data: List all vailable Topics\nseries.Topic.unique()[:10]","1942a52b":"series[series.Topic == 'Health: Population: Structure'].IndicatorName.unique()","fb7ca04f":"SeriesCodeSelect = str(series[series.IndicatorName == 'Population, total'].SeriesCode)\nprint(SeriesCodeSelect)","c4104d58":"plt_data = indicators[indicators['IndicatorName'] == 'Population, total']\nplt_data.head()","2acffef1":"# Select a two countries for starters\nplt_data_select = plt_data[plt_data['CountryCode'].isin(['CHN', 'IND'])]","436ef5f2":"sns.lineplot('Year', 'Value', hue = 'CountryCode', data = plt_data_select)","c7393ce2":"# Select single year and compare over largest countries -> take care! the list also features regions\nplt_data_select = plt_data[plt_data['CountryCode'].isin(['CHN', 'IND', 'USA', 'BRA', 'RUS']) & (plt_data['Year'] == 1977)]\n# plt_data_select.head()\nsns.barplot('CountryCode', 'Value', data = plt_data_select)","b2840f1f":"# Select region\nprint(country.Region.unique())\nplt_data_select = plt_data[plt_data['CountryCode'].isin(country[country['Region'] == 'South Asia'].CountryCode) & (plt_data['Year'] == 1977)]\n# Sort data\nplt_data_select.sort_values('Value', ascending = False, inplace = True)\nsns.barplot('CountryCode', 'Value', data = plt_data_select)","5b0b7a61":"# Select Topic\nSeriesCode_list = series[series.Topic.str.contains(r'Financial Sector', na = True)]\nSeriesCode_list = list(SeriesCode_list.SeriesCode)\n\n# Subset data\ndata = indicators[indicators.IndicatorCode.isin(SeriesCode_list)]\n\n# Define a function as we might be calling this several times\ndef checkDQ(data):\n\n    data = data[['IndicatorCode', 'Value']]\n\n    # Calculate summary statistics by IndicatorCode\n    data_agg = data.groupby(['IndicatorCode']).agg(\n        [\n            ('NA_Number', lambda x: sum(x.isnull())),   \n            ('NA_Percent', lambda x: 100 * (sum(x.isnull()) \/ len(x))),\n            ('Zero_number', lambda x: sum(x == 0)),\n            ('Zero_Percent', lambda x: 100 * np.mean(x == 0)),#,\n            ('Min', 'min'),\n            ('Max', 'max'),\n            ('Mean', 'mean'),\n            ('Median', 'median'),\n            ('Std', 'std')\n        ]\n        )\n\n    # Remove multindex colum structure\n    data_agg.columns = data_agg.columns.levels[1]\n    \n    # Transform SeriesCode to IndicatorName\n    return(\n        pd.merge(data_agg, series[['SeriesCode', 'Topic', 'IndicatorName']], left_on = ['IndicatorCode'], right_on = ['SeriesCode'], how = 'left')\n    )\n\n\ncheckDQ(data)\n","cb6aab77":"# Go from long to wide format\ndata = indicators[indicators.IndicatorCode.isin(SeriesCode_list)]\ndata = data[['IndicatorCode', 'CountryCode', 'Year', 'Value']]\n\ndata_wide = pd.pivot_table(data, index = ['CountryCode', 'Year'], columns = 'IndicatorCode', values = 'Value')\ndata_wide.reset_index(inplace = True) # The MultiIndex created by pivot_table is very inconvenient for this procedure.\n\n# And now go back to long format\ndata_long = pd.melt(data_wide, id_vars = ['CountryCode', 'Year'], value_vars = SeriesCode_list, value_name = \"Value\")\n#data_long.reset_index(inplace = True)\n\n\n# Check dimensions before and after data expansion\nprint('Number of rows INITITALLY: {}'.format(len(data.index)))\nprint('Number of rows AFTER expansion: {}'.format(len(data_long.index)))\n\n# That is an increase by factor 4!\n","b28852c4":"# Check data quality again in wide format!\ncheckDQ(data_long)\n","54a6143e":"# Check data quality over time! and by country\ngroup_cols = ['IndicatorCode', 'Year'] #, 'CountryCode'\nselect_cols = group_cols.copy()\nselect_cols.append('Value')\n\ndata_long_NA = data_long[select_cols].groupby(group_cols).agg([(\"NA_percent\", lambda x: 100 * (sum(x.isnull()) \/ len(x)))])\ndata_long_NA.columns = data_long_NA.columns.levels[1]\ndata_long_NA.reset_index(inplace = True)\n\n# Number of NA over time\nna_plot = sns.boxplot(x = 'Year', y = 'NA_percent', data = data_long_NA)\nplt.xticks(rotation = 90, fontsize = 6)\nplt.show()\n\n# There appears to be an increased availability of data starting in the year 2000\n\n","5e1586ca":"# Hypothesis observed -> Subset the data to Year >= 2001\ndata_long = data_long[data_long['Year'] >= 2001]\nprint(data_long.head())\n","97201cf2":"# Check missing values by country\ngroup_cols = ['IndicatorCode', 'CountryCode'] #, 'CountryCode'\nselect_cols = group_cols.copy()\nselect_cols.append('Value')\n\ndata_long_NA = data_long[select_cols].groupby(group_cols).agg([(\"NA_percent\", lambda x: 100 * (sum(x.isnull()) \/ len(x)))])\ndata_long_NA.columns = data_long_NA.columns.levels[1]\ndata_long_NA.reset_index(inplace = True)\n\n# Which countries have good data coverage, and which do not?\n# Can we find like 50 or 100 countries that have somehow good data coverage?\n\n# Calculate the median number of missing value percent for each country\ndata_long_NA_by_country = data_long_NA.groupby('CountryCode').agg([('Median', np.median)])\ndata_long_NA_by_country.columns = data_long_NA_by_country.columns.levels[1]\ndata_long_NA_by_country.reset_index(inplace = True)\n\nsns.barplot(data = data_long_NA_by_country.sort_values('Median'), x = \"CountryCode\", y = \"Median\")\nplt.show()\n\nprint(\"Number of countries with less than 20% missing (median-wise): \", len(data_long_NA_by_country[data_long_NA_by_country.Median <= 20].index))\nselect_CountryCode = list(data_long_NA_by_country.loc[data_long_NA_by_country.Median <= 20, \"CountryCode\"])\nprint(select_CountryCode)","bc1b1116":"# Check for Germany (DEU) which Indicators are available.\nCountryCode_sel = \"DEU\"\n\ndata_long_NA_sel = data_long_NA[(data_long_NA[\"CountryCode\"] == CountryCode_sel) & (data_long_NA[\"NA_percent\"] >= 50)]\n\ndata_long_NA_sel = pd.merge(data_long_NA_sel, series[['SeriesCode', 'IndicatorName']], left_on = ['IndicatorCode'], right_on = ['SeriesCode'], how = 'left')\n\nprint(list(data_long_NA_sel.IndicatorName))\n\n\n","3224d9eb":"CountryCode_sel = [\"DEU\", \"USA\", \"SGP\", \"GBR\", \"ZAR\"]\n\ndata_long_NA_sel = data_long_NA[data_long_NA.CountryCode.isin(CountryCode_sel)]\n\n# Spread to wide format to inspect\ndata_long_NA_sel_wide = pd.pivot_table(data_long_NA_sel, index = 'IndicatorCode', columns = 'CountryCode', values = 'NA_percent')\ndata_long_NA_sel_wide.reset_index(inplace = True)\n\n# Check which indiactors apply to DEU und which to ZAR. It seems some indicators are shared and some are not!\ndata_long_NA_sel_wide_sel = data_long_NA_sel_wide[[\"IndicatorCode\", \"DEU\", \"ZAR\"]][((data_long_NA_sel_wide.DEU < 10) & (data_long_NA_sel_wide.ZAR > 50)) | ((data_long_NA_sel_wide.DEU > 50) & (data_long_NA_sel_wide.ZAR < 10))]\n\npd.merge(data_long_NA_sel_wide_sel, series[['SeriesCode', 'IndicatorName']], left_on = ['IndicatorCode'], right_on = ['SeriesCode'], how = 'left')\n","f287d81a":"# Let's check which lines are available for both countries. And if that will be enough for an analysis\ndata_long_NA_sel_wide_sel = data_long_NA_sel_wide[[\"IndicatorCode\", \"DEU\", \"ZAR\"]][((data_long_NA_sel_wide.DEU < 20) & (data_long_NA_sel_wide.ZAR < 20))]\npd.merge(data_long_NA_sel_wide_sel, series[['SeriesCode', 'IndicatorName']], left_on = ['IndicatorCode'], right_on = ['SeriesCode'], how = 'left')\n\n# Nice. 5 Series. That is a bit disappointing.","9f59ac4a":"# Something is awfully weird.\nseries.loc[series.IndicatorName.str.contains(r\"Central\", na = False)]","6b7e9604":"# Starting point is a list of countries which we want to have in our analysis.\nCountryCode_sel = [\"DEU\", \"USA\", \"SGP\", \"GBR\", \"RUS\", \"JPN\", \"KOR\"]\n\n# Then we will check which series are available for these countries, i.e. NA_Percent < 10%.\ndata_long_sel = data_long.loc[data_long.CountryCode.isin(CountryCode_sel)]\n\ngroup_cols = ['IndicatorCode', 'CountryCode'] #, 'CountryCode'\nselect_cols = group_cols.copy()\nselect_cols.append('Value')\n\ndata_long_sel_NA = data_long_sel[select_cols].groupby(group_cols).agg([(\"NA_percent\", lambda x: 100 * (sum(x.isnull()) \/ len(x)))])\ndata_long_sel_NA.columns = data_long_sel_NA.columns.levels[1]\ndata_long_sel_NA.reset_index(inplace = True)\n\n\n# Now let's check which IndicatorCodes do not have any CountryCode with NA_percent > 10!\ndata_long_sel_NA = data_long_sel_NA.groupby('IndicatorCode').agg([(\"NA_percent\", max)])\ndata_long_sel_NA.columns = data_long_sel_NA.columns.levels[0]\ndata_long_sel_NA.reset_index(inplace = True)\n\ndata_long_sel_NA = data_long_sel_NA.loc[data_long_sel_NA.NA_percent < 50]\n\nIndicatorCode_sel = data_long_sel_NA.IndicatorCode.unique()\n\nprint(IndicatorCode_sel)\nprint(len(IndicatorCode_sel))\nprint(len(IndicatorCode_sel)\/len(data.IndicatorCode.unique()))\n\n# That is a VERY short list. About 20% of our available series.","46713ce5":"# Now we will check for how many other countries this applies.\ndata_long_sel = data_long.loc[data_long.IndicatorCode.isin(IndicatorCode_sel)]\n\ngroup_cols = ['IndicatorCode', 'CountryCode'] #, 'CountryCode'\nselect_cols = group_cols.copy()\nselect_cols.append('Value')\n\ndata_long_sel_NA = data_long_sel[select_cols].groupby(group_cols).agg([(\"NA_percent\", lambda x: 100 * (sum(x.isnull()) \/ len(x)))])\ndata_long_sel_NA.columns = data_long_sel_NA.columns.levels[1]\ndata_long_sel_NA.reset_index(inplace = True)\n\n# Get the worst performing indicator for each CountryCode\ndata_long_sel_NA = data_long_sel_NA.groupby('CountryCode').agg([(\"NA_percent\", max)])\ndata_long_sel_NA.columns = data_long_sel_NA.columns.levels[0]\ndata_long_sel_NA.reset_index(inplace = True)\n\n# print(data_long_sel_NA.loc[data_long_sel_NA.CountryCode == \"DEU\"])\n\n# # Now let's check which CountryCodes do have any NA_percent > 10!\ndata_long_sel_NA = data_long_sel_NA.loc[data_long_sel_NA.NA_percent < 50]\n\n# # print(data_long_sel_NA.loc[data_long_sel_NA.IndicatorCode == 'FS.LBL.LIQU.GD.ZS'])\n\nCountryCode_sel = data_long_sel_NA.CountryCode.unique()\n# # data_long_sel_NA = data_long_sel_NA.loc[data_long_sel_NA.NA_percent < 10]\nprint(CountryCode_sel)\nprint(len(CountryCode_sel))\n\n# This is an okay list. About 25% of all countries of the data set can be used.\n\n","cc73da57":"# Create data for further analysis\ndata_sel = data_long.loc[data_long.IndicatorCode.isin(IndicatorCode_sel)]\ndata_sel = data_sel.loc[data_sel.CountryCode.isin(CountryCode_sel)]\nprint(data_sel.describe())","e1f09e50":"# Check missing values\ncheckDQ(data_sel).sort_values(\"NA_Percent\")","f44767c6":"data_sel_clean = data_sel.set_index('Year')\ndata_sel_clean = data_sel_clean.fillna(method = 'ffill')\n\n# Check number of NAs\ncheckDQ(data_sel_clean).sort_values(\"NA_Percent\")\n\n# Where do these 3 NAs come from?\n","0de94037":"# It comes from the problem, that when the value is missing in the first year (2001), then there is no observation to carry forward!\ndata_sel_clean[data_sel_clean.Value.isnull()]","089f41d4":"# Take the last known observations and apply it to all former observations!\ndata_sel_clean = data_sel_clean.fillna(method = 'bfill')\n\n# And there we go!\ncheckDQ(data_sel_clean).sort_values(\"NA_Percent\")","58e8e7d4":"# For this to work, we need to have each SeriesCode as column, i.e. go from long to wide!\ndata_sel_clean_wide = pd.pivot_table(data_sel_clean, index = ['CountryCode', 'Year'], columns = 'IndicatorCode', values = 'Value')\ndata_sel_clean_wide.reset_index(inplace = True) # The MultiIndex created by pivot_table is very inconvenient for this procedure.\n\n# data_sel_clean_wide.head()\ncorr_M = data_sel_clean_wide.corr()\nsns.heatmap(corr_M, annot = True, fmt = '.1f')\nplt.show()","bed13ed2":"# How does this look like for selected countries?\ncorr_M = data_sel_clean_wide.loc[data_sel_clean_wide.CountryCode == \"USA\"].corr()\nsns.heatmap(corr_M, annot = True, fmt = '.1f')\nplt.show()\n","3461ae64":"# And for a specific year?\ncorr_M = data_sel_clean_wide.loc[data_sel_clean_wide.Year == 2010].corr()\nsns.heatmap(corr_M, annot = True, fmt = '.1f')\nplt.show()","be1e298d":"from sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.decomposition import PCA\n\nscaler = StandardScaler()\n\nx = data_sel_clean_wide.loc[:,data_sel_clean_wide.columns.isin([\"Year\", \"CountryCode\"])==False]\n\nx_scaled = scaler.fit_transform(x)\n\npca = PCA(n_components = .95) # We want 95% of the variance explained\npca.fit(x_scaled)\nreduced = pca.transform(x_scaled)\n\n# Check explained variance\npd.DataFrame(pca.explained_variance_ratio_).plot.bar()\nplt.legend('')\nplt.xlabel('Principal Components')\nplt.ylabel('Explained Variance')\nplt.axhline(y = 0.1, color = \"r\", linestyle = \"-\") # Guess.\nplt.show()\n\n# Let's take the first 4 components!\n","3bee211a":"# Calculate the weights, i.e. the \"normalised squared factor loadings\"\npca = PCA(n_components = 4) # As decided further up!\npca.fit(x_scaled)\nreduced = pca.transform(x_scaled)\n\n# The matrix has the following dimensions: columns = n_components, rows = number of variables in x_scaled\nfactor_loadings = pca.components_.T * np.sqrt(pca.explained_variance_)\n\n# Square each element of factor_loadings\nfactor_loadings = factor_loadings ** 2\n\n# Divide each column by its sum to normalize\nfactor_loadings = factor_loadings \/ factor_loadings.sum(axis = 0)\n\n# Calculate intermediate weights\nw_row_max = factor_loadings.max(axis = 1)\n\n# Create matrix with dimensions: n_vars * n_components, where rowwise we only keep the max, everything else is 0\nw_factor_loadings_max = factor_loadings.copy()\n\ni_r, j_r = w_factor_loadings_max.shape\n\nfor i in range(0, i_r):\n    for j in range(0, j_r):\n        if (w_factor_loadings_max[i, j] != w_row_max[i]):\n            w_factor_loadings_max[i, j] = 0\n\n# Weight with the explained variance -> rescale to 0 : 1 first!\nvar_explained = pca.explained_variance_\nvar_explained = var_explained \/ sum(var_explained)\n\n# Apply explained variance weights to weight matrix to create final weights\nw_final = np.matmul(w_factor_loadings_max, var_explained.T)\n\n# Now, apply the weights to our normalized variables to create an index per year \/ CountryCode\nindex = x_scaled * w_final\nindex = index.sum(axis = 1)\n\nindex_scaled = scaler.fit_transform(index.reshape(-1, 1))\n\n# Create pd.DataFrame with CountryCode labels!\nindex_df = data_sel_clean_wide.copy()\nindex_df[\"index\"] = index_scaled\nindex_df.describe()\n","d24cadb7":"# Inspect the index. Select a year\nyear_sel = 2013\n\n# As there are too many countries for a nice plot, let's select the top and bottom 5\nindex_df_plot = index_df.loc[index_df.Year == year_sel].sort_values(\"index\")\n\nsns.barplot(\n    data = index_df_plot.head(5).append(index_df_plot.tail(5)),\n    y = \"CountryCode\",\n    x = \"index\",\n    orient = \"h\"\n)\n\n# Looking good! Maybe we need to rescale and invert the index though!\n","d887422a":"# Use sklearns MinMaxScaler for this\nindex_df[\"index_fin\"] = -index_df[\"index\"]\nscaler_minmax = MinMaxScaler(feature_range = (-1, 1))\n\nindex_df[\"index_fin\"] = scaler_minmax.fit_transform(index_df[\"index_fin\"].values.reshape(-1, 1))\n\n# Inspect the index. Select a year\nyear_sel = 2013\n\n# As there are too many countries for a nice plot, let's select the top and bottom 5\nindex_df_plot = index_df.loc[index_df.Year == year_sel].sort_values(\"index_fin\")\n\nsns.barplot(\n    data = index_df_plot.head(5).append(index_df_plot.tail(5)),\n    y = \"CountryCode\",\n    x = \"index_fin\",\n    orient = \"h\"\n)\n\n","dee53461":"# Select a set of countries\nCountryCode_sel_plot = [\"CHE\", \"TUR\", \"USA\", \"RUS\", \"SGP\"]\n\n# As there are too many countries for a nice plot, let's select the top and bottom 5\nindex_df_plot = index_df.loc[index_df.CountryCode.isin(CountryCode_sel_plot)]\n\nsns.lineplot(\n    data = index_df_plot,\n    hue = \"CountryCode\",\n    x = \"Year\",\n    y = \"index_fin\"\n)","032d4269":"# EDA and PCA to create data-driven Index\n\n\nThe world development indicators provide an interesting set of indicators which allow a great set of analysis. You could take a subset of Indicators and try to predict another subset, like the GDP per country. The indicators could serve as a means to create country clusters. Or you could let the data itself speak and extract the information from the given variation. Which is what we will do in this very notebook.\n\nI will perform a detailed EDA to guide you through my exploration process of the WDI dataset. To give a realistic feeling of EDA, I did not delete any steps nor did I delete dead-ends. This notebooks represents both failures and successes.\n\nFollowing the EDA, I will guide you through the construction of an Index by a weighting algorithm performed on the results of a PCA. The Index will allow not only to rank countries, but also to tell how these ranks developed over time and how are apart each rank is.\n\nEnough for the introuctino. Let's start!\n\n","4b0f4178":"Let's dig deeper by selecting a single Topic. With economic data, I always like to pick population statistics first when available. I know what to expect, which makes it easy.","be9fe996":"Now, let's create our index using 4 components. For the methodology I roughly follow:\n\nNicoletti, G., S. Scarpetta and O. Boylaud (2000), \"Summary Indicators of Product Market Regulation with an Extension to Employment Protection Legislation\", OECD Economics Department Working Papers, No. 226, OECD Publishing, Paris, https:\/\/doi.org\/10.1787\/215182844604. \n\nYou can find the working paper here: \nhttps:\/\/www.oecd-ilibrary.org\/economics\/summary-indicators-of-product-market-regulation-with-an-extension-to-employment-protection-legislation_215182844604\n","5856571c":"It appears that Germany has all the stock data - which ZAR is missing.\n\n\nWhile ZAR has all the money data, which is missing for Germany.\nThat is a bit unfortunate.","e3058744":"The data appears to be very clean. There is not a single NA-value. And also, not too many 0s, which might have been a placeholder for NA values.\n\nBut does that make any sense? A dataset spanning several decades and all countries in the world without a single NA? Short answer: It does not make sense.\n\nA common technique to compress data in long-format is to drop missing observations. Listing a data point as missing or just having that data point missing has the same amount of information.\n\nTherefore, to inspect the real magnitude of missing value problem, the data needs to be filled. An easy way is to transform from long to wide format and then back to the long format.","cce5a807":"Well, that looks a bit different now. It seems like we are missing about 80% of the data.\n\nHypothesis 1: Data is missing especially in older periods. The data collection process should have improved over time and should be quite okay in the 2000s.","10076349":"### Missing value imputation\n\nAs our data is time series, we have the following option:\n* LOC (Last observation carry foward - ffill in fillna)\n* Linear Interpolation\n* A mix of both methods\n\nFor simplicity, I will use LOC.","5b3ebc36":"## Conclusion\n\nIn this notebook we performed a detailed EDA to create a feeling of the world development indicator dataset. We discovered a substantial amount of missing values. Further, we also discovered that many indicators are available only for a specific group of countries.\n\nBuilding on these observations, we created a data-driven index to capture the financial changes over time for a set of countries.\n\nThis was all a starting point for a very detailed and interesting in-depth analysis. There are a lot of further topics to be inquired:\n\n\n* Create some plausibility checks to check the index\n* Check how other features vary with the created Index. Can it be used to predict certain measures or create Country Development clusters?\n* Can this procedure be applied to other subsetes of the data? E.g. to create an Environemntal Development Index?","f131761b":"There is no Indicator related to Central Banks. That seems a bit weird for a financial data topic.\n\n\n\n**Solution to the problem:** \nThe interesting Indicators for a financial analysis are not in the WDI. There are in the World bank Database: \"Global Financial Development\".","5aa126f1":"## Inspect the Data\n\nAs we have several data sources, it is very important to inspect the dimensions of all our data. To get a quick overview, we will have a look at all available column names.","737da517":"Now that we have an overview, let's look at some of the category-columns that could be of interest.","039ae36d":"NY.GDP.DEFL.KD.ZG and FP.CPI.TOTL.ZG seem to be highly correlated.\n\nWell, a deflator and consumer price index are likely to be correlated as they are defined very similiarly. \n\nBoth measure the current level of prices. One measures the prices of all goods and services produced by an economy, the other measures the prices of consumer goods.","b14f0bb4":"Well, that is not the list of countries I expected. Where are Germany (DEU), France (FRA), the United States (USA)?\nIs it possible that some Series Codes are very specific to developing countries?","97a515b9":"### Correlation Analysis\n\nNow that we have no more missing values, we can perform correlation analysis.\n\nIn this context this has two purposes:\n* Detect possible multicollinearity\n* Check if we have a nice mix of positive and negative correlation. If we don't, our index won't be able to properly distinguish.","a5891060":"Now that we checked the time dimension, we look at countries.","63364371":"Looks good! Time for some Econometrics!","b9e16d2f":"## Data-Driven Index\n\nFrom our EDA we learned that there is a large amount of missing data. Also, Indicators available for one group might not be available for another group of countries.\n\nThis will reduce the number of available data series tremendously. But there is no way I am going to fill data series with > 50% missing data.\n\nTo create the data-driven index I will therefore reduce the available countries and indicators to a acceptable subset.","39e64ccd":"How can these values be missing?\n\nI checked with the databank at worldbank. These values are actually missing for Germany for the period 2001 - 2020. Why is that? Are there two similar SeriesCodes for the same measurement? Are monetary units measured at a supranational level (European Union)? There are many open questions to be answered.\n\nThose are methodological questions unrelated to the purpose of this notebook.\n\nTherefore, let's take a different approach. We define a list of countries that we want to have for sure. For these selected countries, we check which indicators have enough available data. Then we check how many other countries have enough data for these indicators as well.","b21260de":"## Data quality and summary statistics\n\nNow that we feel comfortable with the data and its structure, we can go one step further and inspect the data with a more structured approach to get an even better understanding of what we are dealing with.\n\nOur data has several dimensions:\n* Country\n* Year\n* Indicator\n\nAs we have a large number of available years and countries, it is very likely that we also have a lot of missing values. Especially developing countries are very likely not to track all of these indicators for a long time - if at all.\n\nIn a first step, therefore, I would like to know the summary statistics for each Indicator. This will give me a rough guidance on the data quality.\n\nI like to look at:\n* General summary statistics (min, max, mean, median, std)\n* Missing values\n* Number of Zeros\n\nThere are two reasons to look at the number of zeros:\n* The data provider could fill missing values with zeros.\n* When the proportion of zeros is very large, then some models do not work properly.\n","ea60904a":"### PCA\n\nNow that we cleaned the data and checked the correlation, it is time to perform the PCA.\n\nBut what exactly do I want to measure with the PCA?\n\nThe given data are the world **development** indicators. There should be enough information in the data to measure a countries development in comparison to other countries. In general, this is something very difficult to measure. When it is done, there usually is a lot of qualitative input as well, which is very difficult to quantify. \n\nAlso, country development are usually reported as ranks, i.e. Country A is rank 1 and Country B is rank 2. Okay, so country A is more developed than country B. And that is all the information you can get from a rank.\n\nYou cannot determine how far these countries' developments lie apart. Maybe the difference is only marginal, maybe it is substantial. \n\nFurther, you cannot gain interesting information from tracking these ranks over time. Country A might always be on rank 1. If Country B remains on rank 2, we can infer from a ranking that Country B lags Country A over time. This might give the false impression that Country A is doing top and Country B's measure does not change over time. But that is wrong. From a ranking we do not know how Country A's actual measure changes over time. It might increase or decrease. \n"}}