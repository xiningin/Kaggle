{"cell_type":{"5027cb2d":"code","6eb85604":"code","731b7efc":"code","025f49b6":"code","86cec3ca":"code","8f71b1b6":"code","d9bd41a4":"code","5306b1e4":"code","babbc5d1":"code","9e7aa50b":"code","881e57ed":"code","33cb74ab":"code","654b52e8":"code","bcde83e7":"code","7db64eb2":"code","065a8811":"code","20f6c857":"code","667207b7":"code","17d5f9ae":"code","70464658":"code","292502c9":"code","39e73e94":"code","3e07f66f":"code","dd897a3b":"code","5eb496ce":"code","63f62b69":"code","b8111aa7":"code","ff7a5015":"code","0760e4b1":"code","f1b0dbfb":"code","f7332e2d":"code","ca0d2121":"markdown","7594b4f2":"markdown","28a62555":"markdown","57fbb4dd":"markdown","e213f3e1":"markdown","132276c9":"markdown","af2dc405":"markdown","3a799565":"markdown","ed0f27b4":"markdown","08cec5ac":"markdown","8549bfa6":"markdown","7f30b797":"markdown","0519022c":"markdown","b082b690":"markdown","faf054e4":"markdown","5643d5c6":"markdown","414c3df7":"markdown","c10ab560":"markdown","a3dd58bf":"markdown","396dedb8":"markdown","85ae50de":"markdown","69231e25":"markdown","3a7f0516":"markdown"},"source":{"5027cb2d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","6eb85604":"# loading core libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom itertools import combinations\nimport math\nimport statistics\nimport time\nfrom datetime import datetime\nimport matplotlib.dates as mdates\nimport dateutil.easter as easter\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV, TimeSeriesSplit\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge\n\n# Models\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\n\n# Tensorflow\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import callbacks","731b7efc":"### Loading initial data\ntrain_data=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/train.csv', index_col='row_id')\ntest_data=pd.read_csv('..\/input\/tabular-playground-series-jan-2022\/test.csv', index_col='row_id')\n\n# Shape and preview\nprint('Training data df shape:',train_data.shape)\nprint('Test data df shape:',test_data.shape)\ntrain_data.head()","025f49b6":"# loading the test data, as we can see below num_sold column is missing which is objective. \n# We need to predict num_sold in future time series.\ntest_data.head()","86cec3ca":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Convert date to datetime\n\ntrain_data.date=pd.to_datetime(train_data.date)\ntest_data.date=pd.to_datetime(test_data.date)\n\n# drop 29th Feb\ntrain_data.drop(train_data[(train_data.date.dt.month==2) & (train_data.date.dt.day==29)].index, axis=0, inplace=True)","8f71b1b6":"#lets visualize the data\ntrain_data.head()","d9bd41a4":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Figure\nplt.figure(figsize=(12,5))\n\n# Groupby\naa=train_data.groupby(['date','store']).agg(num_sold=('num_sold','sum'))\n\n# Lineplot\nsns.lineplot(data=aa, x='date', y='num_sold', hue='store')\n\n# Aesthetics\nplt.title('num_sold by store')","5306b1e4":"# plotting number of iteams sold indepenpently \n#ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\nKR=train_data[train_data.store=='KaggleRama']\nKM=train_data[train_data.store=='KaggleMart']\nbb=KR.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\ncc=KM.groupby(['date','product']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=bb, x='date', y='num_sold', hue='product')\nax2=sns.lineplot(ax=axes[1], data=cc, x='date', y='num_sold', hue='product')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","babbc5d1":"#visualizing the number of ideas sold per country\n# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Subplots\nfig, axes = plt.subplots(2, 1, figsize=(12, 10))\n\n# Groupby\ndd=KR.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\nee=KM.groupby(['date','country']).agg(num_sold=('num_sold','sum'))\n\n# Lineplots\nax1=sns.lineplot(ax=axes[0], data=dd, x='date', y='num_sold', hue='country')\nax2=sns.lineplot(ax=axes[1], data=ee, x='date', y='num_sold', hue='country')\n\n# Aesthetics\nax1.title.set_text('KaggleRama')\nax2.title.set_text('KaggleMart')","9e7aa50b":"# Loading the column which needs to predicted to y and rest of the data to X\n# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Labels\ny=train_data.num_sold\n\n# Features\nX=train_data.drop('num_sold', axis=1)","881e57ed":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# From https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298990\ndef unofficial_hol(df):\n    countries = {'Finland': 1, 'Norway': 2, 'Sweden': 3}\n    stores = {'KaggleMart': 1, 'KaggleRama': 2}\n    products = {'Kaggle Mug': 1,'Kaggle Hat': 2, 'Kaggle Sticker': 3}\n    \n    # load holiday info.\n    hol_path = '..\/input\/publicandunofficialholidaysnorfinswe201519\/holidays.csv'\n    holiday = pd.read_csv(hol_path)\n    \n    fin_holiday = holiday.loc[holiday.country == 'Finland']\n    swe_holiday = holiday.loc[holiday.country == 'Sweden']\n    nor_holiday = holiday.loc[holiday.country == 'Norway']\n    df['fin holiday'] = df.date.isin(fin_holiday.date).astype(int)\n    df['swe holiday'] = df.date.isin(swe_holiday.date).astype(int)\n    df['nor holiday'] = df.date.isin(nor_holiday.date).astype(int)\n    df['holiday'] = np.zeros(df.shape[0]).astype(int)\n    df.loc[df.country == 'Finland', 'holiday'] = df.loc[df.country == 'Finland', 'fin holiday']\n    df.loc[df.country == 'Sweden', 'holiday'] = df.loc[df.country == 'Sweden', 'swe holiday']\n    df.loc[df.country == 'Norway', 'holiday'] = df.loc[df.country == 'Norway', 'nor holiday']\n    df.drop(['fin holiday', 'swe holiday', 'nor holiday'], axis=1, inplace=True)\n    \n    return df","33cb74ab":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef get_holidays(df):\n    # End of year\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"n-dec{d}\":\n                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(24, 32)}),\n        pd.DataFrame({f\"f-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                      for d in range(1, 14)}),\n        pd.DataFrame({f\"jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in range(1, 10)}),\n        pd.DataFrame({f\"s-jan{d}\":\n                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                      for d in range(1, 15)})], axis=1)\n    \n    # May\n    df = pd.concat([df, pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                      for d in list(range(1, 10))}),\n        pd.DataFrame({f\"may{d}\":\n                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                      for d in list(range(19, 26))})], axis=1)\n    \n    # June and July\n    df = pd.concat([df, pd.DataFrame({f\"june{d}\":\n                   (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                   for d in list(range(8, 14))})], axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"wed_june{d}\": \n                   (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(-4, 6))})], axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    \n    df = pd.concat([df, pd.DataFrame({f\"sun_nov{d}\": \n                   (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                   for d in list(range(0, 9))})], axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    df = pd.concat([df, pd.DataFrame({f\"dec{d}\":\n                   (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                   for d in list(range(6, 14))})], axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df = pd.concat([df, pd.DataFrame({f\"easter{d}\":\n                   (df.date - easter_date == np.timedelta64(d, \"D\"))\n                   for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})], axis=1)\n    \n    return df","654b52e8":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef date_feat_eng_X1(df):\n    df['year']=df['date'].dt.year                   # 2015 to 2019\n    return df\n\ndef date_feat_eng_X2(df):\n    df['day_of_week']=df['date'].dt.dayofweek       # 0 to 6\n    df['day_of_month']=df['date'].dt.day            # 1 to 31\n    df['dayofyear'] = df['date'].dt.dayofyear       # 1 to 366\n    df.loc[(df.date.dt.year==2016) & (df.dayofyear>60), 'dayofyear'] -= 1   # 1 to 365\n    df['week']=df['date'].dt.isocalendar().week     # 1 to 53\n    df['week']=df['week'].astype('int')             # int64\n    df['month']=df['date'].dt.month                 # 1 to 12\n    return df","bcde83e7":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef get_GDP(df):\n\n    # Load data\n    GDP_data = pd.read_csv('..\/input\/gdp20152019finlandnorwayandsweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv',index_col=\"year\")\n\n    # Rename the columns in GDP df \n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n\n    # Create a dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n    \n    df['GDP']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)\n    \n    # Split GDP by country (for linear model)\n    df['GDP_Finland']=df['GDP'] * (df['country']=='Finland')\n    df['GDP_Norway']=df['GDP'] * (df['country']=='Norway')\n    df['GDP_Sweden']=df['GDP'] * (df['country']=='Sweden')\n    \n    df=df.drop('GDP',axis=1)\n    \n    # Suggested by AmbrosM\n    #df['GDP_1212']=df.set_index(['country', 'year']).index.map(GDP_dictionary.get)**1.212\n\n    return df","7db64eb2":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef GDP_PC(df):\n    # Load data\n    GDP_data = pd.read_csv('..\/input\/gdp-data-2015-to-2019-finland-norway-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv',index_col=\"year\")\n\n    # Rename the columns in GDP df\n    GDP_data.columns = ['Finland', 'Norway', 'Sweden']\n\n    # Create a dictionary\n    GDP_dictionary = GDP_data.unstack().to_dict()\n\n    # Create new GDP_PC column\n    df['GDP_PC'] = df.set_index(['country', 'year']).index.map(GDP_PC_dictionary.get)\n    \n    return df","065a8811":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# From https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model#Simple-feature-engineering-(without-holidays)\ndef FourierFeatures(df):\n    # temporary one hot encoding\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        df[product] = df['product'] == product\n    \n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 2):\n        df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        df[f'mug_sin{k}'] = df[f'sin{k}'] * df['Kaggle Mug']\n        df[f'mug_cos{k}'] = df[f'cos{k}'] * df['Kaggle Mug']\n        df[f'hat_sin{k}'] = df[f'sin{k}'] * df['Kaggle Hat']\n        df[f'hat_cos{k}'] = df[f'cos{k}'] * df['Kaggle Hat']\n        df=df.drop([f'sin{k}', f'cos{k}'], axis=1)\n    \n    # drop temporary one hot encoding\n    df=df.drop(['Kaggle Mug', 'Kaggle Hat'], axis=1)\n    \n    return df","20f6c857":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Help linear model find the right height of trends for each combination of features\ndef get_interactions(df):\n    df['KR_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    df['KR_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleRama')\n    df['KR_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleRama')\n    df['KR_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleRama')\n    \n    df['KM_Sweden_Mug']=(df.country=='Sweden')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Sweden_Hat']=(df.country=='Sweden')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Sweden_Sticker']=(df.country=='Sweden')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Norway_Mug']=(df.country=='Norway')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Norway_Hat']=(df.country=='Norway')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Norway_Sticker']=(df.country=='Norway')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    df['KM_Finland_Mug']=(df.country=='Finland')*(df['product']=='Kaggle Mug')*(df.store=='KaggleMart')\n    df['KM_Finland_Hat']=(df.country=='Finland')*(df['product']=='Kaggle Hat')*(df.store=='KaggleMart')\n    df['KM_Finland_Sticker']=(df.country=='Finland')*(df['product']=='Kaggle Sticker')*(df.store=='KaggleMart')\n    \n    return df","667207b7":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef dropdate(df):\n    df=df.drop('date',axis=1)\n    return df\n# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\ndef onehot(df,columns):\n    df=pd.get_dummies(df, columns)\n    return df","17d5f9ae":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Feature set for trend model\ndef FeatEng_X1(df):\n    df=date_feat_eng_X1(df)\n    df=get_GDP(df)\n    df=FourierFeatures(df)\n    df=get_interactions(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n# Feature set for interactions model\ndef FeatEng_X2(df):\n    df=date_feat_eng_X2(df)\n    df=get_holidays(df)\n    df=unofficial_hol(df)\n    df=dropdate(df)\n    df=onehot(df,['store', 'product', 'country'])\n    return df\n\n","70464658":"# ref: https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model\n# Apply feature engineering\nX_train_1=FeatEng_X1(X)\nX_train_2=FeatEng_X2(X)\nX_test_1=FeatEng_X1(test_data)\nX_test_2=FeatEng_X2(test_data)","292502c9":"# as you can see below, GDP information, sign thera information in included which was not present before. \n# These are the features whcih we have included addtionally\n# another thing to note is insteady providing date informaiton we have provided date and sign theta value\nX_train_1.head()","39e73e94":"# Simlar to previous step, instead of providing date informaiton we have provided date and day of year, day of the month .... informaiton like that\nX_train_2","3e07f66f":"X_test_1","dd897a3b":"# visualizing our target\ny.head()","5eb496ce":"\n# lets start with random forest model, which is very easy to apply. \n#from sklearn.ensemble import RandomForestRegressor\n#clf=RandomForestRegressor(n_estimators=107,\n                          #random_state=1337,\n                          #max_depth=20,\n                          #min_samples_leaf=2,\n                          #verbose=2)\n#clf.fit(X_train_2,y)      \n#predicting the future sales based on the model\n#my_pred=clf.predict(X_test_2)\n# From https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\n#def geometric_round(arr):\n#    result_array = arr\n#    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n#    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n#    return result_array\n#_submit=geometric_round(my_pred)\n# Save predictions to file\n#output = pd.DataFrame({'row_id': X_test_2.index, 'num_sold': _submit})\n\n# Check format\n#output.head()\n#output.to_csv('submission.csv', index=False)","63f62b69":"#from sklearn.preprocessing import StandardScaler\n#scaler = StandardScaler()\n#X_train_2_scaled = scaler.fit_transform(X_train_2)\n#X_test_2_scaled = scaler.fit_transform(X_test_2)\n#X_train_1_scaled = scaler.fit_transform(X_train_1)\n#X_test_1_scaled = scaler.fit_transform(X_test_1)\n#print(f'\\nDimensions before scaling: \\ntrain_set: {X_train_2.shape}\\ntest_set: {X_test_2.shape}')\n#print(f'\\nDimensions after scaling: \\ntrain_set: {X_train_2_scaled.shape}\\ntest_set: {X_test_2_scaled.shape}')","b8111aa7":"# A class is a collection of properties and methods (like models from Sklearn)\nclass HybridModel:\n    def __init__(self, model_1, model_2, grid=None):\n        self.model_1 = model_1\n        self.model_2 = model_2\n        self.grid=grid\n        \n    def fit(self, X_train_1, X_train_2, y):\n        # Train model 1\n        self.model_1.fit(X_train_1, y)\n        \n        # Predictions from model 1 (trend)\n        y_trend = self.model_1.predict(X_train_1)\n\n        if self.grid:\n            # Grid search\n            tscv = TimeSeriesSplit(n_splits=3)\n            grid_model = GridSearchCV(estimator=self.model_2, cv=tscv, param_grid=self.grid)\n        \n            # Train model 2 on detrended series\n            grid_model.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = grid_model.predict(X_train_2)\n            \n            # Save model\n            self.grid_model=grid_model\n        else:\n            # Train model 2 on residuals\n            self.model_2.fit(X_train_2, y-y_trend)\n            \n            # Model 2 preditions (for residual analysis)\n            y_resid = self.model_2.predict(X_train_2)\n        \n        # Save data\n        self.y_train_trend = y_trend\n        self.y_train_resid = y_resid\n        \n    def predict(self, X_test_1, X_test_2):\n        # Predict trend using model 1\n        y_trend = self.model_1.predict(X_test_1)\n        \n        if self.grid:\n            # Grid model predictions\n            y_resid = self.grid_model.predict(X_test_2)\n        else:\n            # Model 2 predictions\n            y_resid = self.model_2.predict(X_test_2)\n        \n        # Add predictions together\n        y_pred = y_trend + y_resid\n        \n        # Save data\n        self.y_test_trend = y_trend\n        self.y_test_resid = y_resid\n        \n        return y_pred","ff7a5015":"\n# Choose models\nfrom sklearn.ensemble import RandomForestRegressor\n#model_1=RandomForestRegressor(n_estimators=107,\n                          #random_state=1337,\n                          #max_depth=20,\n                          #min_samples_leaf=2,\n                          #verbose=2)\n#models_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\n\n# Parameter grid\n#param_grid = {'n_estimators': [100],\n       # 'max_depth': [4, 6,],\n       # 'learning_rate': [0.1]}\n\n# Initialise output vectors\n#y_pred=np.zeros(len(test_data))\n#train_preds=np.zeros(len(y))\n\n# Ensemble predictions\n#for model_2 in models_2:\n    # Start timer\n#    start = time.time()\n    \n    # Construct hybrid model\n#    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n#    model.fit(X_train_1, X_train_2, np.log(y))\n\n    # Save predictions\n#    y_pred += np.exp(model.predict(X_test_1,X_test_2))\n    \n    # Training set predictions (for residual analysis)\n#    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n#    stop = time.time()\n    \n#    print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n#    if model.grid:\n#        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\n#y_pred = y_pred\/len(models_2)\n#train_preds = train_preds\/len(models_2)","0760e4b1":"# From https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\n#def geometric_round(arr):\n    #result_array = arr\n    #result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    #result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    #return result_array\n\n#y_pred=geometric_round(y_pred)\n# Save predictions to file\n#output = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred})\n\n# Check format\n#output.head()\n#output.to_csv('submission.csv', index=False)","f1b0dbfb":"# Choose models\n#model_1=RandomForestRegressor(n_estimators=107,\n                          #random_state=1337,\n                          #max_depth=20,\n                          #min_samples_leaf=2,\n                          #verbose=2)\n#models_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\n\n# Parameter grid\n#param_grid = {'n_estimators': [100],\n        #'max_depth': [4, 6,],\n        #'learning_rate': [0.1]}\n\n# Initialise output vectors\n#y_pred=np.zeros(len(test_data))\n#train_preds=np.zeros(len(y))\n\n# Ensemble predictions\n#for model_2 in models_2:\n    # Start timer\n    #start = time.time()\n    \n    # Construct hybrid model\n    #model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    #model.fit(X_train_1_scaled, X_train_2_scaled, np.log(y))\n\n    # Save predictions\n    #y_pred += np.exp(model.predict(X_test_1_scaled,X_test_2_scaled))\n    \n    # Training set predictions (for residual analysis)\n    #train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    #stop = time.time()\n    \n    #print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n    #if model.grid:\n    #    print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\n#y_pred_scaled = y_pred\/len(models_2)\n#train_preds_scaled = train_preds\/len(models_2)\n#y_pred_scaled=geometric_round(y_pred_scaled)\n# Save predictions to file\n#output = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred_scaled})\n\n# Check format\n#output.head()\n#output.to_csv('submission.csv', index=False)","f7332e2d":"# Choose models\nmodel_1=LinearRegression()\n#models_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0)]\nmodels_2=[LGBMRegressor(random_state=0), CatBoostRegressor(random_state=0, verbose=False), XGBRegressor(random_state=0),RandomForestRegressor(random_state=1337)]\n\n# Parameter grid\nparam_grid = {'n_estimators': [100],\n        'max_depth': [4, 6,]}\n\n# Initialise output vectors\ny_pred=np.zeros(len(test_data))\ntrain_preds=np.zeros(len(y))\n\n# Ensemble predictions\nfor model_2 in models_2:\n    # Start timer\n    start = time.time()\n    \n    # Construct hybrid model\n    model = HybridModel(model_1, model_2, grid=param_grid)\n\n    # Train model\n    model.fit(X_train_1, X_train_2, np.log(y))\n\n    # Save predictions\n    y_pred += np.exp(model.predict(X_test_1,X_test_2))\n    \n    # Training set predictions (for residual analysis)\n    train_preds += np.exp(model.y_train_trend+model.y_train_resid)\n    \n    # Stop timer\n    stop = time.time()\n    \n    print(f'Model_2:{model_2} -- time:{round((stop-start)\/60,2)} mins')\n    \n    if model.grid:\n        print('Best parameters:',model.grid_model.best_params_,'\\n')\n    \n# Scale\ny_pred_RF = y_pred\/len(models_2)\ntrain_preds_RF = train_preds\/len(models_2)\n# From https:\/\/www.kaggle.com\/fergusfindley\/ensembling-and-rounding-techniques-comparison\ndef geometric_round(arr):\n    result_array = arr\n    result_array = np.where(result_array < np.sqrt(np.floor(arr)*np.ceil(arr)), np.floor(arr), result_array)\n    result_array = np.where(result_array >= np.sqrt(np.floor(arr)*np.ceil(arr)), np.ceil(arr), result_array)\n    return result_array\ny_pred_RF=geometric_round(y_pred_RF)\n# Save predictions to file\noutput = pd.DataFrame({'row_id': test_data.index, 'num_sold': y_pred_RF})\n\n# Check format\noutput.head()\noutput.to_csv('submission.csv', index=False)","ca0d2121":"### Removing date column and  get_dummies() function is used to convert categorical variable into dummy\/indicator variables","7594b4f2":"## I have tried to several options, I will shown one by one. \n## Try 1: Random Forest regressor","28a62555":"## Lets visualize our data before we start building our model\n","57fbb4dd":"# A qucik view of the GDP data per country\n![image.png](attachment:7364101c-4ab3-48c9-8fa9-afeb24396cda.png)","e213f3e1":"## Try4: Now trying linear regression with Randomforest+LGBM+Catboost+XGB)","132276c9":"# ** TPS Jan 2022- Random Forest based ensemble approach with explanation ( score 4.63143) **\n","af2dc405":"### So far Notebook has not recorgnized the date column is acutally time in date format. We need to assign accordingly","3a799565":"![image.png](attachment:ce44b9eb-efd3-4cfa-b87c-54d5efdc0d80.png)","ed0f27b4":"![image.png](attachment:217fcde6-25b4-4db7-915b-c0970f1b8571.png)","08cec5ac":"### I have used gridsearch algorithm to fine tune the hyperparameters of Randomforestregressor. **The Score was 5**\n### Then I wanted to try ensemble approad. I started with scaling the data using standrdscaler whcih I will use later","8549bfa6":"# Score was 5.20, not an improvement from previous models","7f30b797":"## Try3: This time lets use scaled data and see does it improve any performance?","0519022c":"![image.png](attachment:4e66abb6-0177-4572-a166-3be96cf57ad2.png)","b082b690":"## lessons learned \n## * A single algorithum might not work. Better to go for Ensemble\n## * Fine tuning hyperparameters helps but its not game changer \n## * Focus on feature engineering ","faf054e4":"## The sales are effected by the holidays, so we need to take into account. we will be uploading the following csv file to our note book, a CSV file with such informaiton is loaded into the notebook","5643d5c6":"## As we have seen earlier and below image each country have a differnt range of sales. GDP information can be useful. Below is the data which gives intution on the data","414c3df7":"## Try 2: Apply Ensemble approach with combination of Randomforestregressor with other boosting regressors","c10ab560":"### this notebook is modified from excellent work of https:\/\/www.kaggle.com\/samuelcortinhas\/tps-jan-22-quick-eda-hybrid-model","a3dd58bf":"## wow the Score is 4.63143. This is the best so far. ","396dedb8":"## WE have noticed that the sales are following a typical sin theta wave as shown below. We will use fourier concept to extract some of the features\n![image.png](attachment:37987a01-174f-4e7e-b912-49face021d20.png)![image.png](attachment:828c6698-a168-4063-a3b4-c2068957fde0.png)","85ae50de":"## Official holidays must be treated seperately. Note that we spend more money during christmas than regular sunday. ","69231e25":"### Bit of feature engineering so we can convert the date to numerical values.","3a7f0516":"## **Visalizing the data which has been loaded sofar.**"}}