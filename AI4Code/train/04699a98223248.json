{"cell_type":{"5c8249df":"code","f2627d40":"code","326afd38":"code","9fc072b0":"code","31b786c1":"code","fa882366":"code","b587f8ad":"code","4080ab78":"code","e3c5750c":"code","bb893d18":"code","5f02a523":"code","c3894c80":"code","a736b1aa":"code","c75ce1db":"code","af9dedb7":"code","63311bce":"code","dadc535f":"code","6044fc8c":"code","0f59e1ce":"code","75dd8e06":"code","5d36d53a":"code","5c6f912a":"code","8163daaf":"code","3807b294":"code","5a40cf2f":"code","8833fb21":"code","d5f947d0":"code","e6520fa6":"code","db351df3":"code","c1f0cc36":"code","19a517d3":"code","2d009702":"code","7ef5c7ea":"code","67131495":"code","9766e2dc":"code","8b0fda83":"code","82d00674":"code","c282b455":"code","49fb14d8":"code","ac7e332f":"code","a68cb1ed":"code","5ea2a2e1":"code","e87a864f":"code","b7be6ac3":"code","cd3fc9c6":"code","61fe1f26":"code","d19bd94b":"code","45f1a01f":"code","be5025de":"code","f24b3954":"code","9dbf9844":"code","f1d0e3ff":"code","6303423d":"code","e44b37cb":"code","562ce82c":"code","db549fc2":"code","13c6ae3a":"code","e1b88a4d":"code","e2655b04":"code","e485a79b":"code","25c66796":"code","22fa0962":"code","b7c7ca86":"code","23b8e73e":"code","062e6927":"code","d964581a":"code","bb25467e":"code","5010e6e2":"code","ee6ea828":"code","87da1587":"code","1475754a":"code","6d858c94":"code","4688cc6c":"code","5c0bd8d3":"code","f99d5d7f":"code","7787f600":"code","9a97ac69":"code","99572ed9":"markdown","b634ad15":"markdown","ff8490b0":"markdown","65389c25":"markdown","ce5e9029":"markdown","0766de57":"markdown","ac5e43e5":"markdown","09a3c102":"markdown","f2fdc0ef":"markdown","9c5a0074":"markdown","0ce5ee64":"markdown","855e169f":"markdown","1ab2bad6":"markdown","0399986e":"markdown","b7d73360":"markdown","67186d1c":"markdown","71415f8f":"markdown","f46730e5":"markdown","cb820d35":"markdown","44f70ea3":"markdown","c61ab710":"markdown","b3cdc142":"markdown","ff044e55":"markdown","1d5acf9d":"markdown","d70ac466":"markdown","d9b42f17":"markdown","952e22dc":"markdown","a6fbd830":"markdown","e83c0c0f":"markdown","a8f82239":"markdown","ada765f2":"markdown"},"source":{"5c8249df":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2627d40":"pip install PrettyTable","326afd38":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\n\nfrom ast import literal_eval\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import (StandardScaler, MinMaxScaler)\nfrom sklearn.model_selection import (train_test_split, KFold, StratifiedKFold)\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, StackingRegressor)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom catboost import CatBoostRegressor\nimport shap\nfrom xgboost import XGBRegressor\nfrom prettytable import PrettyTable\n\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import (mean_squared_error, mean_absolute_error)\nfrom sklearn.metrics import mean_absolute_percentage_error","9fc072b0":"# \u0437\u0430\u0444\u0438\u043a\u0441\u0438\u0440\u0443\u044e \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 RANDOM_SEED, \u0447\u0442\u043e\u0431\u044b \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u044b \u0431\u044b\u043b\u0438 \u0432\u043e\u0441\u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u043c\u044b\u043c\u0438:\n\nRANDOM_SEED = 42","31b786c1":"# \u0417\u0430\u0433\u0440\u0443\u0436\u0430\u044e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\n\ndata = pd.read_csv('\/kaggle\/input\/diplomdata\/data.csv')\ndata.head()","fa882366":"data.columns","b587f8ad":"data.info()","4080ab78":"data['target'].value_counts()","e3c5750c":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u044e \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e \u0434\u043b\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u0432 \u043c\u043e\u0434\u0435\u043b\u0438:\n\ndata['target'] = data['target'].apply(lambda x: str(x).replace('$',''))\ndata['target'] = data['target'].apply(lambda x: str(x).replace('+',''))\ndata['target'] = data['target'].apply(lambda x: str(x).replace(',',''))\ndata['target'] = data['target'].apply(lambda x: str(x).replace('\/mo','000'))\ndata['target'] = data['target'].apply(lambda x: str(x).replace('1,215 - 1,437','1300000'))\ndata['target'] = data['target'].apply(lambda x: str(x).replace('nan','225000'))\ndata['target'] = data['target'].apply(lambda x: str(x).replace('1215 - 1437000','1437000'))","bb893d18":"X2 = data.drop('target',axis = 1).fillna('No info')\ny2 = data['target']\n\nskf = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\ncat_boost_mape_values = []\nmae_values = []\nrmse_values = []\n\nfor train_index, test_index in skf.split(X2, y2):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train2, X_test2 = X2[X2.index.isin(train_index)], X2[X2.index.isin(test_index)]\n    y_train2, y_test2 = y2[y2.index.isin(train_index)], y2[y2.index.isin(test_index)]\n    \n\n    model = CatBoostRegressor(iterations=300)\n    model.fit(X_train2, y_train2,eval_set=(X_test2, y_test2), verbose=100, cat_features=['status', 'private pool', 'propertyType', 'street', 'baths', 'homeFacts', 'fireplace', 'city', 'schools', 'sqft', 'zipcode', 'beds', 'state', 'stories', 'mls-id', 'PrivatePool', 'MlsId'])\n\n\n\n    y_pred2 = model.predict(X_test2)\n\n    print(np.sqrt(mean_squared_error(y_test2, y_pred2)))\n\n    cat_boost_mape_value = (mean_absolute_percentage_error(y_test2, y_pred2))\n    cat_boost_mape_values.append(cat_boost_mape_value)\n    print(cat_boost_mape_value)\n    mae_value = (mean_absolute_error(y_test2, y_pred2))\n    mae_values.append(mae_value)\n    rmse_value = np.sqrt(mean_squared_error(y_test2, y_pred2))\n    rmse_values.append(rmse_value)\n    \nprint(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values)):0.2f}%.\")\nprint(f\"The MAE mertic for default CatBoost model: {(np.mean(mae_values)):0.2f}\")\nprint(f\"The RMSE default CatBoost model: {(np.mean(rmse_values)):0.2f}\")","5f02a523":"# \u041f\u043e\u0441\u0442\u0440\u043e\u044e \u0433\u0440\u0430\u0444\u0438\u043a \u0432\u0430\u0436\u043d\u043e\u0441\u0442\u0438 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0434\u043b\u044f \u043c\u043e\u0434\u0435\u043b\u0438\n\nexplainer = shap.TreeExplainer(model)\nshap_values = explainer.shap_values(X2)","c3894c80":"shap.summary_plot(shap_values, X2)","a736b1aa":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u044e \u043d\u0430 \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u043f\u0443\u0441\u0442\u044b\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u0445\n\nnan_count=100*data.isna().sum().sort_values(ascending=False)\/data.shape[0]\nfig=px.bar(x=nan_count.index,y=nan_count.values, labels={\"y\": \"Nan ammount (%)\",\"x\": \"Feature\"})\nfig.show()","c75ce1db":"# \u0421\u0442\u043e\u043b\u0431\u0446\u044b 'privat pool' \u0438 'mis-id' \u0438\u043c\u0435\u044e\u0442 \u0431\u043e\u043b\u0435\u0435 \u0447\u0435\u043c 80% \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432 \u0438 \u0432\u043d\u043e\u0441\u044f\u0442 \u043d\u0430\u0438\u043c\u0435\u043d\u044c\u0448\u0438\u0439 \u0432\u043a\u043b\u0430\u0434 \u0432 \u043c\u043e\u0434\u0435\u043b\u044c. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u0438\u0445 \u0443\u0434\u0430\u043b\u0438\u0442\u044c\n\ndata.drop('private pool', axis=1, inplace=True)","af9dedb7":"X3 = data.drop('target',axis = 1).fillna('No info')\ny3 = data['target']\n\nskf = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\ncat_boost_mape_values = []\nmae_values = []\nrmse_values = []\n\nfor train_index, test_index in skf.split(X3, y3):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train3, X_test3 = X3[X3.index.isin(train_index)], X3[X3.index.isin(test_index)]\n    y_train3, y_test3 = y3[y3.index.isin(train_index)], y3[y3.index.isin(test_index)]\n    \n\n    model2 = CatBoostRegressor(iterations=300)\n    model2.fit(X_train3, y_train3,eval_set=(X_test3, y_test3), verbose=100, cat_features=['mls-id', 'status', 'propertyType', 'street', 'baths', 'homeFacts', 'fireplace', 'city', 'schools', 'sqft', 'zipcode', 'beds', 'state', 'stories', 'PrivatePool', 'MlsId'])\n\n\n\n    y_pred3 = model2.predict(X_test3)\n\n    print(np.sqrt(mean_squared_error(y_test3, y_pred3)))\n\n    cat_boost_mape_value = (mean_absolute_percentage_error(y_test3, y_pred3))\n    cat_boost_mape_values.append(cat_boost_mape_value)\n    print(cat_boost_mape_value)\n    mae_value = (mean_absolute_error(y_test3, y_pred3))\n    mae_values.append(mae_value)\n    rmse_value = np.sqrt(mean_squared_error(y_test3, y_pred3))\n    rmse_values.append(rmse_value)\n    \nprint(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values)):0.2f}%.\")\nprint(f\"The MAE mertic for default CatBoost model: {(np.mean(mae_values)):0.2f}\")\nprint(f\"The RMSE default CatBoost model: {(np.mean(rmse_values)):0.2f}\")\n","63311bce":"# data.drop('mls-id', axis=1, inplace=True)","dadc535f":"'''\nX3 = data.drop('target',axis = 1).fillna('No info')\ny3 = data['target']\n\nskf = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\ncat_boost_mape_values = []\nmae_values = []\nrmse_values = []\n\nfor train_index, test_index in skf.split(X3, y3):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train3, X_test3 = X3[X3.index.isin(train_index)], X3[X3.index.isin(test_index)]\n    y_train3, y_test3 = y3[y3.index.isin(train_index)], y3[y3.index.isin(test_index)]\n    \n\n    model2 = CatBoostRegressor(iterations=300)\n    model2.fit(X_train3, y_train3,eval_set=(X_test3, y_test3), verbose=100, cat_features=['status', 'propertyType', 'street', 'baths', 'homeFacts', 'fireplace', 'city', 'schools', 'sqft', 'zipcode', 'beds', 'state', 'stories', 'PrivatePool', 'MlsId'])\n\n\n    y_pred3 = model2.predict(X_test3)\n\n    print(np.sqrt(mean_squared_error(y_test3, y_pred3)))\n\n    cat_boost_mape_value = (mean_absolute_percentage_error(y_test3, y_pred3))\n    cat_boost_mape_values.append(cat_boost_mape_value)\n    print(cat_boost_mape_value)\n    mae_value = (mean_absolute_error(y_test3, y_pred3))\n    mae_values.append(mae_value)\n    rmse_value = np.sqrt(mean_squared_error(y_test3, y_pred3))\n    rmse_values.append(rmse_value)\n    \nprint(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values)):0.2f}%.\")\nprint(f\"The MAE mertic for default CatBoost model: {(np.mean(mae_values)):0.2f}\")\nprint(f\"The RMSE default CatBoost model: {(np.mean(rmse_values)):0.2f}\")\n'''","6044fc8c":"data.info()","0f59e1ce":"data['homeFacts']","75dd8e06":"# \u0414\u0430\u043d\u043d\u044b\u0439 \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u0441\u043e\u0434\u0435\u0440\u0436\u0438\u0442 7 \u043e\u0442\u0434\u0435\u043b\u044c\u043d\u044b\u0445 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432. \u0418\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u043e \u043f\u043b\u043e\u0449\u0430\u0434\u0438 \u0438 \u0446\u0435\u043d\u0435 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0435 \u0443\u0436\u0435 \u0435\u0441\u0442\u044c. \n# \u0414\u043e\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0432 \u0434\u0430\u0442\u0430\u0441\u0435\u0442 'Remodeled year', 'Heating' \u0438 'Cooling' \u0443\u0445\u0443\u0434\u0448\u0430\u0435\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0443,  'Parking' \u0438 'Year built' \u0434\u0435\u043b\u0430\u0435\u0442 \u043b\u0443\u0447\u0448\u0435. \n\ndata['homeFacts'][1]","5d36d53a":"ser = pd.Series(data['homeFacts']) \n\nser.head(10) ","5c6f912a":"ser[0][35:39]","8163daaf":"year = []\nfor j in range(0,377185):\n    year.append(ser[j][35:39])","3807b294":"year[:10]","5a40cf2f":"data['year'] = year","8833fb21":"data['year'].value_counts().head(20)","d5f947d0":"data['year'].unique()","e6520fa6":"def year_cod(status):\n    if \"', '\" in status: return '0'\n    elif 'one,' in status: return '0'\n    elif 'No D' in status: return '0'\n    elif \"1', \" in status: return '0'\n    else: \n        return status\n    \ndata['year'] = data['year'].apply(lambda x: year_cod(x))\ndata['year'].value_counts()","db351df3":"from ast import literal_eval\ndef funk(x):\n    homefact_dict = literal_eval(x)\n    homefact_lst = homefact_dict['atAGlanceFacts'][4].get('factValue')\n    if homefact_lst:\n        return str(homefact_lst)\n    else:\n        return np.nan\nParking = list(data[:].homeFacts.apply(funk))\nParking[:10]","c1f0cc36":"data['Parking'] = Parking  ","19a517d3":"data['Parking'] = data['Parking'].apply(lambda x: str(x).lower()) # \u043f\u0440\u0438\u0432\u0435\u0434\u0443 \u0434\u0430\u043d\u043d\u044b\u0435 \u043a \u043d\u0438\u0436\u043d\u0435\u043c\u0443 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\u0443","2d009702":"data['Parking'].value_counts().head(30)","7ef5c7ea":"def parking_cod(status):\n    if '1' in status: return 'one'\n    elif '2' in status: return 'two'\n    elif '3' in status: return 'three'\n    elif '4' in status: return 'four'\n    elif 'attached' in status: return 'attached'\n    elif 'detached' in status: return 'detached'\n    elif 'carport' in status: return 'carport'\n    elif 'no data' in status: return 'no info'\n    elif '5' in status: return 'five'\n    elif 'parking' in status: return 'parking'\n    elif 'on street' in status: return 'on street'\n    elif 'off street' in status: return 'off street'\n    elif 'none' in status: return 'zero'\n    elif '0' in status: return 'zero'\n    elif 'garage' in status: return 'one'\n    elif 'nan' in status: return 'no info'\n    else: \n        return 'other'\n    \ndata['Parking'] = data['Parking'].apply(lambda x: parking_cod(x))\ndata['Parking'].value_counts()","67131495":"data.drop('homeFacts', axis=1, inplace=True)","9766e2dc":"data.info()","8b0fda83":"X3 = data.drop('target',axis = 1).fillna('No info')\ny3 = data['target']\n\nskf = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\ncat_boost_mape_values = []\nmae_values = []\nrmse_values = []\n\nfor train_index, test_index in skf.split(X3, y3):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train3, X_test3 = X3[X3.index.isin(train_index)], X3[X3.index.isin(test_index)]\n    y_train3, y_test3 = y3[y3.index.isin(train_index)], y3[y3.index.isin(test_index)]\n    \n\n    model2 = CatBoostRegressor(iterations=300)\n    model2.fit(X_train3, y_train3,eval_set=(X_test3, y_test3), verbose=100, cat_features=['Parking',  'mls-id', 'year','status','propertyType', 'street', 'baths',  'fireplace', 'city', 'schools', 'sqft', 'zipcode', 'beds', 'state', 'stories', 'PrivatePool', 'MlsId'])\n\n\n    y_pred3 = model2.predict(X_test3)\n\n    print(np.sqrt(mean_squared_error(y_test3, y_pred3)))\n\n    cat_boost_mape_value = (mean_absolute_percentage_error(y_test3, y_pred3))\n    cat_boost_mape_values.append(cat_boost_mape_value)\n    print(cat_boost_mape_value)\n    mae_value = (mean_absolute_error(y_test3, y_pred3))\n    mae_values.append(mae_value)\n    rmse_value = np.sqrt(mean_squared_error(y_test3, y_pred3))\n    rmse_values.append(rmse_value)\n    \nprint(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values)):0.2f}%.\")\nprint(f\"The MAE mertic for default CatBoost model: {(np.mean(mae_values)):0.2f}\")\nprint(f\"The RMSE default CatBoost model: {(np.mean(rmse_values)):0.2f}\")\n","82d00674":"data['schools']","c282b455":"type(data['schools'])","49fb14d8":"# \u0437\u0434\u0435\u0441\u044c \u043d\u0430\u0438\u0431\u043e\u043b\u0435\u0435 \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u044b\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043c\u043d\u0435 \u043a\u0430\u0436\u0443\u0442\u0441\u044f \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0448\u043a\u043e\u043b\u044b \u0438 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435 \u0434\u043e \u0448\u043a\u043e\u043b\u044b. \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u0432\u044b\u0447\u0438\u0441\u043b\u0438\u0442\u044c \u0441\u0440\u0435\u0434\u043d\u0438\u0439 \u0440\u0435\u0439\u0442\u0438\u043d\u0433 \u0438 \u0441\u0440\u0435\u0434\u043d\u0435\u0435 \u0440\u0430\u0441\u0441\u0442\u043e\u044f\u043d\u0438\u0435.\n\ndata['schools'][1]","ac7e332f":"def funk(x):\n    Distance = literal_eval(x)\n    Distance_list = Distance[0]['data'].get('Distance')\n    if Distance_list:\n        return Distance_list\n    else:\n        return '0'\n    \nDistance = list(data[:].schools.apply(funk))\nDistance[:10]","a68cb1ed":"data['Distance'] = Distance","5ea2a2e1":"data['Distance'][:10]","e87a864f":"average_distance_school = []","b7be6ac3":"for x in range(0,377185):\n   \n    l = data['Distance'][x]\n    l1=[]\n    for x in l:\n        x =str(x).replace('mi','').replace('[','').replace(']','')\n        l1.append(x)\n    floatlist = [float(elem) for elem in l1]\n    average_distance_school.append(sum(floatlist)\/len(floatlist))\n    \n       ","cd3fc9c6":"average_distance_school[:10]","61fe1f26":"len(average_distance_school)","d19bd94b":"data['average_distance_school'] = average_distance_school\ndata['average_distance_school'] = data['average_distance_school'].apply(lambda x: round(x,2))","45f1a01f":"data['average_distance_school'].value_counts().head(30)","be5025de":"data.average_distance_school.isna().sum()","f24b3954":"def funk(x):\n    rating = literal_eval(x)\n    rating_list = rating[0].get('rating')\n    if rating_list:\n        return rating_list\n    else:\n        return '0'\n    \nrating_school = list(data[:].schools.apply(funk))\nrating_school[:10]","9dbf9844":"data['rating_school'] = rating_school","f1d0e3ff":"data['rating_school'][:10]","6303423d":" average_rating_school = []","e44b37cb":"for x in range(0,377185):\n   \n    m = data['rating_school'][x]\n    m1=[]\n    for x in m:\n        x =str(x).replace('1\/10','1').replace('2\/10','2').replace('4\/10','4').replace('5\/10','5').replace('6\/10','6').replace('7\/10','7').replace('8\/10','8').replace('9\/10','9').replace('10\/10','10').replace('None\/10','0').replace('NA','0').replace('NR','0').replace('3\/10','3').replace('[','').replace(']','').replace('','0')\n        m1.append(x)\n    floatlist = [float(elem) for elem in m1]\n    average_rating_school.append((sum(floatlist)\/len(floatlist))\/10)","562ce82c":"data['average_rating_school'] = average_rating_school\ndata['average_rating_school'] = data['average_rating_school'].apply(lambda x: round(x,2))","db549fc2":"data['average_rating_school'].value_counts()","13c6ae3a":"data.info()","e1b88a4d":"data.drop('schools', axis=1, inplace=True)\ndata.drop('Distance', axis=1, inplace=True)\ndata.drop('rating_school', axis=1, inplace=True)","e2655b04":"X3 = data.drop('target',axis = 1).fillna('No info')\ny3 = data['target']\n\nskf = StratifiedKFold(n_splits=4, random_state=42, shuffle=True)\ncat_boost_mape_values = []\nmae_values = []\nrmse_values = []\n\nfor train_index, test_index in skf.split(X3, y3):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train3, X_test3 = X3[X3.index.isin(train_index)], X3[X3.index.isin(test_index)]\n    y_train3, y_test3 = y3[y3.index.isin(train_index)], y3[y3.index.isin(test_index)]\n    \n    model2 = CatBoostRegressor(iterations=300)\n    model2.fit(X_train3, y_train3,eval_set=(X_test3, y_test3), verbose=100, cat_features=['mls-id', 'year','Parking','status','propertyType', 'street', 'baths',  'fireplace', 'city', 'sqft', 'zipcode', 'beds', 'state', 'stories', 'PrivatePool', 'MlsId'])\n\n\n    y_pred3 = model2.predict(X_test3)\n\n    print(np.sqrt(mean_squared_error(y_test3, y_pred3)))\n\n    cat_boost_mape_value = (mean_absolute_percentage_error(y_test3, y_pred3))\n    cat_boost_mape_values.append(cat_boost_mape_value)\n    print(cat_boost_mape_value)\n    mae_value = (mean_absolute_error(y_test3, y_pred3))\n    mae_values.append(mae_value)\n    rmse_value = np.sqrt(mean_squared_error(y_test3, y_pred3))\n    rmse_values.append(rmse_value)\n    \nprint(f\"The MAPE mertic for the default CatBoost model using 4-fold CV is: {(np.mean(cat_boost_mape_values)):0.2f}%.\")\nprint(f\"The MAE mertic for default CatBoost model: {(np.mean(mae_values)):0.2f}\")\nprint(f\"The RMSE default CatBoost model: {(np.mean(rmse_values)):0.2f}\")\n","e485a79b":"data.head()","25c66796":"df = data.copy()","22fa0962":"# \u041d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u0432 \u043c\u043e\u0436\u043d\u043e \u043f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0438\u0437 object \u0432 int\n\ndf['average_rating_school'] = df['average_rating_school'].astype(int)\ndf['year'] = df['year'].astype(int)\ndf['target'] = df['target'].astype(int)","b7c7ca86":"df.info()","23b8e73e":"from sklearn import preprocessing\n# \u041a\u043e\u0434\u0438\u0440\u0443\u044e \u043e\u0441\u0442\u0430\u0432\u0448\u0438\u0435\u0441\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f. \u0415\u0441\u043b\u0438 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c One Hot Encoding, \n# \u043d\u043e\u0443\u0442\u0431\u0443\u043a \u0432\u044b\u043b\u0435\u0442\u0430\u0435\u0442 \u0438\u0437-\u0437\u0430 \u043f\u0440\u0435\u0432\u044b\u0448\u0435\u043d\u0438\u044f \u0440\u0430\u0437\u043c\u0435\u0440\u0430 \u043f\u0430\u043c\u044f\u0442\u0438. \n\nlabel_encoder = preprocessing.LabelEncoder()\n\ndf['status']= label_encoder.fit_transform(df['status']) \ndf['propertyType']= label_encoder.fit_transform(df['propertyType']) \ndf['street']= label_encoder.fit_transform(df['street'])\ndf['baths']= label_encoder.fit_transform(df['baths']) \ndf['fireplace']= label_encoder.fit_transform(df['fireplace']) \ndf['city']= label_encoder.fit_transform(df['city']) \ndf['beds']= label_encoder.fit_transform(df['beds']) \ndf['state']= label_encoder.fit_transform(df['state']) \ndf['PrivatePool']= label_encoder.fit_transform(df['PrivatePool']) \ndf['MlsId']= label_encoder.fit_transform(df['MlsId']) \ndf['stories']= label_encoder.fit_transform(df['stories']) \ndf['Parking']= label_encoder.fit_transform(df['Parking']) \ndf['zipcode']= label_encoder.fit_transform(df['zipcode']) \ndf['sqft']= label_encoder.fit_transform(df['sqft']) \ndf['mls-id']= label_encoder.fit_transform(df['mls-id'])\n\nprint(df.head())\n","062e6927":"df.head()","d964581a":"df.info()","bb25467e":"#\u0412\u044b\u0433\u0440\u0443\u0437\u0438\u043c \u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0440\u0430\u0431\u043e\u0442\u044b \u043d\u0430 \u043b\u043e\u043a\u0430\u043b\u044c\u043d\u043e\u0439 \u043c\u0430\u0448\u0438\u043d\u0435 \ndf.to_csv(\"\/kaggle\/working\/df.csv\")","5010e6e2":"X = df.drop('target',axis = 1)\ny = df['target']","ee6ea828":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","87da1587":"\nnaiv_model = LinearRegression()\nnaiv_model.fit(X_train, y_train)\nnaiv_predict = naiv_model.predict(X_test)\n \n\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 MAE: {(mean_absolute_error(y_test, naiv_predict)):0.2f}\")\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 MAPE: {(mean_absolute_percentage_error(y_test, naiv_predict)):0.2f}%\")\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 RMSE: {(np.sqrt(mean_squared_error(y_test, naiv_predict))):0.2f}\")\n\n# # \u043f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u043d\u0438\u0435 \nnaiv_model_log = LinearRegression()\nnaiv_model_log.fit(X_train, np.log(y_train))\nnaiv_predict_log = np.exp(naiv_model_log.predict(X_test))\n\n\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 log MAE: {(mean_absolute_error(y_test, naiv_predict_log)):0.2f}\")\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 log MAPE: {(mean_absolute_percentage_error(y_test, naiv_predict_log)):0.2f}%\")\nprint(f\"\u0422\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u043c\u0435\u0442\u0440\u0438\u043a\u0435 log RMSE: {(np.sqrt(mean_squared_error(y_test, naiv_predict_log))):0.2f}\")","1475754a":"rf_log = RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\nrf_log.fit(X_train, np.log(y_train))\npredict_rf_log = np.exp(rf_log.predict(X_test))\npredict_rf_x = np.exp(rf_log.predict(X_train))\n\nprint(f\"The MAE train Random Forest model is : {(mean_absolute_error(y_train, predict_rf_x)):0.2f}.\")\nprint(f\"The MAE test  Random Forest model is : {(mean_absolute_error(y_test, predict_rf_log)):0.2f}.\")\n\nprint(f\"The MAPE train Random Forest model is : {(mean_absolute_percentage_error(y_train, predict_rf_x)):0.2f}%.\")\nprint(f\"The MAPE test  Random Forest model is : {(mean_absolute_percentage_error(y_test, predict_rf_log)):0.2f}%.\")\n\nprint(f\"The RMSE train Random Forest model is : {(np.sqrt(mean_squared_error(y_train, predict_rf_x))):0.2f}.\")\nprint(f\"The RMSE test Random Forest model is : {(np.sqrt(mean_squared_error(y_test, predict_rf_log))):0.2f}.\")","6d858c94":"etr_log = ExtraTreesRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\netr_log.fit(X_train, np.log(y_train))\npredict_etr_log = np.exp(etr_log.predict(X_test))\npredict_etr_x = np.exp(etr_log.predict(X_train))\n\nprint(f\"The MAE train ExtraTrees model is : {(mean_absolute_error(y_train, predict_etr_x)):0.2f}.\")\nprint(f\"The MAE test ExtraTrees model is : {(mean_absolute_error(y_test, predict_etr_log)):0.2f}.\")\n\nprint(f\"The MAPE train ExtraTrees model is : {(mean_absolute_percentage_error(y_train, predict_etr_x)):0.2f}%.\")\nprint(f\"The MAPE test ExtraTrees model is : {(mean_absolute_percentage_error(y_test, predict_etr_log)):0.2f}%.\")\n\nprint(f\"The RMSE train ExtraTrees model is : {(np.sqrt(mean_squared_error(y_train, predict_etr_x))):0.2f}.\")\nprint(f\"The RMSE test ExtraTrees model is : {(np.sqrt(mean_squared_error(y_test, predict_etr_log))):0.2f}.\")","4688cc6c":"log_cat_boost = CatBoostRegressor(iterations = 10000,\n                          random_seed = RANDOM_SEED,\n                          eval_metric='MAPE',\n                          custom_metric=['R2', 'MAE'],\n                          silent=True,\n                         )\nlog_cat_boost.fit(X_train, np.log(y_train),\n          eval_set=(X_test, np.log(y_test)),\n          verbose_eval=0,\n          use_best_model=True,\n          )\n\nlog_cat_boost.save_model('catboost_single_model_baseline.model')\n\n\nlog_predict_cat_boost = log_cat_boost.predict(X_test)\npredict_cat_boost_x = np.exp(log_cat_boost.predict(X_train))\n\nprint(f\"The MAE train CatBoost model: {(mean_absolute_error(y_train, predict_cat_boost_x)):0.2f}.\")\nprint(f\"The MAE test CatBoost model: {(mean_absolute_error(y_test, log_predict_cat_boost)):0.2f}\")\n\nprint(f\"The MAPE train CatBoost model: {(mean_absolute_percentage_error(y_train, predict_cat_boost_x)):0.2f}%.\")\nprint(f\"The MAPE test CatBoost model: {(mean_absolute_percentage_error(y_test, log_predict_cat_boost)):0.2f}%\")\n\nprint(f\"The RMSE train CatBoost model: {(np.sqrt(mean_squared_error(y_train, predict_cat_boost_x))):0.2f}.\")\nprint(f\"The RMSE test CatBoost model: {(np.sqrt(mean_squared_error(y_test, log_predict_cat_boost))):0.2f}\")","5c0bd8d3":"gb = GradientBoostingRegressor(random_state=42)\ngb.fit(X_train, np.log(y_train))\npredict_gb = gb.predict(X_test)\npredict_gb_x = gb.predict(X_train)\n\nprint(f\"The MAE train GradientBoosting model: {(mean_absolute_error(y_train, predict_gb_x)):0.2f}.\")\nprint(f\"The MAE test GradientBoosting model: {(mean_absolute_error(y_test, predict_gb)):0.2f}\")\n\nprint(f\"The MAPE train GradientBoosting model: {(mean_absolute_percentage_error(y_train, predict_gb_x)):0.2f}%.\")\nprint(f\"The MAPE test GradientBoosting model: {(mean_absolute_percentage_error(y_test, predict_gb)):0.2f}%\")\n\nprint(f\"The RMSE train GradientBoosting model: {(np.sqrt(mean_squared_error(y_train, predict_gb_x))):0.2f}.\")\nprint(f\"The RMSE test GradientBoosting model: {(np.sqrt(mean_squared_error(y_test, predict_gb))):0.2f}\")","f99d5d7f":"estimators = [\n     ('cat_boost', CatBoostRegressor(iterations = 10000,\n                          random_seed = RANDOM_SEED,\n                          eval_metric='MAPE',\n                          custom_metric=['R2', 'MAE'],\n                          silent=True\n                         )),\n     ('gb', GradientBoostingRegressor(random_state=RANDOM_SEED))\n]\n\nsr_log = StackingRegressor(\n    estimators=estimators,\n    final_estimator=RandomForestRegressor(random_state=RANDOM_SEED, n_jobs=-1, verbose=1)\n)\n\nsr_log.fit(X_train, np.log(y_train))\n\ny_pred = np.exp(sr_log.predict(X_test))\n\nprint(f\"The MAE mertic for StackingRegressor model: {(mean_absolute_error(y_test, y_pred)):0.2f}\")\nprint(f\"The MAPE mertic for StackingRegressor model: {(mean_absolute_percentage_error(y_test, y_pred)):0.2f}%\")\nprint(f\"The RMSE mertic for StackingRegressor model: {(np.sqrt(mean_squared_error(y_test, y_pred))):0.2f}\")","7787f600":"x = PrettyTable()\nx.field_names = ['metrics', \"metrics_naive\", \"Random Forest\",  'CatBoost', 'GradientBoosting', 'Stacking' ]\nx.add_row([\"MAPE:\", '6.74%', '2.20%',  '1.00%', '1.00%', '1.54%'])\nx.add_row([\"MAE\", '457497.40', '224498.49',  '644162.03', '644162.03', '257812.28'])\nx.add_row([\"RMSE\", '1949954.81', '1527149.66',  '2050654.90', '2050655.03', '1515244.42'])","9a97ac69":"print(x)","99572ed9":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u043e\u0432 \u0443\u0445\u0443\u0434\u0448\u0430\u044e\u0442 \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438. \u0418\u0441\u043a\u043b\u044e\u0447\u0435\u043d\u0438\u0435 - 'homeFacts' \u0438 'schools'","b634ad15":"\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u044e \u043d\u0430 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u0443\u044e","ff8490b0":"\u041c\u0435\u0442\u0440\u0438\u043a\u0430 \u0443\u043b\u0443\u0447\u0448\u0438\u043b\u0430\u0441\u044c, \u043f\u0440\u0438\u0437\u043d\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0443\u0434\u0430\u043b\u0438\u0442\u044c","65389c25":"\u0412 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0435 \u043d\u0438\u0432\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u0431\u0443\u0434\u0443 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u043b\u0438\u043d\u0435\u0439\u043d\u0443\u044e \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044e","ce5e9029":"\u0417\u0430\u0434\u0430\u0447\u0430: \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u0435\u0440\u0432\u0438\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0434\u043e\u043c\u043e\u0432, \u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u044f\u0441\u044c \u043d\u0430 \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439.","0766de57":"# Model 5: GradientBoosting","ac5e43e5":"The MAE mertic for the default StackingRegressor model: 257812.28\n\nThe MAPE mertic for the default StackingRegressor model: 1.54%\n\nThe RMSE mertic for the default StackingRegressor model: 1515244.42","09a3c102":"## Model 1:  \"\u041d\u0430\u0438\u0432\u043d\u0430\u044f\" \u043c\u043e\u0434\u0435\u043b\u044c","f2fdc0ef":"The MAPE mertic for the default CatBoost model using 4-fold CV is: 12.37%.\n","9c5a0074":" \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0431\u0435\u0439\u0437\u043b\u0430\u0439\u043d\u0430.\n \n \u0411\u0443\u0434\u0443 \u043e\u0431\u0443\u0447\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u044c CatBoost \u0438 \u0438\u0437\u043c\u0435\u0440\u044f\u0442\u044c \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c \u043d\u0430 k-fold \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u043f\u043e\u0441\u043b\u0435 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u0440\u0438\u043d\u0430\u043a\u043e\u0432, \u0447\u0442\u043e\u0431\u044b \u043f\u043e\u043d\u044f\u0442\u044c, \u043a\u0430\u043a\u0438\u0435 \u0438\u0437 \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0439 \u0434\u0435\u0439\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u043e \u0443\u043b\u0443\u0447\u0448\u0430\u044e\u0442 \u043c\u043e\u0434\u0435\u043b\u044c. ","0ce5ee64":"The MAE mertic for the log ExtraTrees model is : 222826.93\n\nThe MAPE mertic for the log ExtraTrees model is : 3.22%.\n\nThe RMSE mertic for the log ExtraTrees model is : 1485521.77","855e169f":"The MAPE mertic for the default CatBoost model using 4-fold CV is: 15.35%.\n\nThe MAE mertic for default CatBoost model: 298805.77\n\nThe RMSE default CatBoost model: 1290314.38","1ab2bad6":"# \u0418\u0442\u043e\u0433\n\n\u0417\u0430\u0434\u0430\u0447\u0430: \u0440\u0430\u0437\u0440\u0430\u0431\u043e\u0442\u0430\u0442\u044c \u0441\u0435\u0440\u0432\u0438\u0441, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0431\u0443\u0434\u0435\u0442 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u0441\u0442\u043e\u0438\u043c\u043e\u0441\u0442\u044c \u0434\u043e\u043c\u043e\u0432, \u043e\u0441\u043d\u043e\u0432\u044b\u0432\u0430\u044f\u0441\u044c \u043d\u0430 \u0438\u0441\u0442\u043e\u0440\u0438\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0439.\n\n\u0412 \u0445\u043e\u0434\u0435 \u0432\u044b\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u044f \u043f\u0440\u043e\u0435\u043a\u0442\u0430:\n\n\u0417\u0430\u0433\u0440\u0443\u0436\u0435\u043d\u044b \u0434\u0430\u043d\u043d\u044b\u0435, \u043f\u0440\u043e\u0438\u0437\u0432\u0435\u0434\u0435\u043d\u0430 \u0438\u0445 \u043e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430;\n\n\u0421\u043e\u0437\u0434\u0430\u043d\u044b \u043d\u043e\u0432\u044b\u0435 \u043f\u0440\u0438\u0437\u043d\u0430\u043a\u0438 \u043d\u0430 \u043e\u0441\u043d\u043e\u0432\u0435 \u0438\u043c\u0435\u044e\u0449\u0438\u0445\u0441\u044f;\n\n\u041f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u044b \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043c\u043e\u0434\u0435\u043b\u0438 \u043a\u043b\u0430\u0441\u0441\u0438\u0447\u0435\u0441\u043a\u043e\u0433\u043e ML \u0434\u043b\u044f \u0440\u0435\u0448\u0435\u043d\u0438\u044f \u0437\u0430\u0434\u0430\u0447\u0438 \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u0438;\n\n\u0414\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0439 \u043f\u043e\u0441\u0442\u0440\u043e\u0435\u043d\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e\u043b\u0443\u0447\u0435\u043d\u044b \u0441\u043e\u043e\u0442\u0432\u0435\u0442\u0441\u0442\u0432\u0443\u044e\u0449\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438.\n\n\u0415\u0441\u043b\u0438 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c \u043c\u043e\u0434\u0435\u043b\u0438 \u043f\u043e \u0441\u043e\u0432\u043e\u043a\u0443\u043f\u043d\u043e\u0441\u0442\u0438 \u043c\u0435\u0442\u0440\u0438\u043a, \u044f \u0432\u044b\u0431\u0438\u0440\u0430\u044e StackingRegressor. \u0421\u0443\u0434\u044f \u043f\u043e RMSE \u0432 \u043d\u0435\u0439 \u043c\u0435\u043d\u044c\u0448\u0435 \u0440\u0430\u0441\u0445\u043e\u0436\u0434\u0435\u043d\u0438\u0435 \u043c\u0435\u0436\u0434\u0443 \u043f\u0440\u043e\u0433\u043d\u043e\u0437\u043e\u043c \u0438 \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439, \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f MAE \u0438 RMSE \u043d\u0438\u0436\u0435, \u0447\u0435\u043c \u0432 \u043d\u0430\u0438\u0432\u043d\u043e\u0439 \u043b\u0438\u043d\u0435\u0439\u043d\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438.\n","0399986e":"The MAE mertic for the CatBoost model: 644162.03\n\nThe MAPE mertic for the CatBoost model: 1.00%\n\nThe RMSE mertic for the CatBoost model: 2050654.90","b7d73360":"The MAE mertic for the log Random Forest model is : 224498.49\n\nThe MAPE mertic for the log Random Forest model is : 2.20%.\n\nThe RMSE mertic for the log Random Forest model is : 1527149.66","67186d1c":"# schools","71415f8f":"\u041f\u043e\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u0443\u0445\u0443\u0434\u0448\u0438\u043b\u043e\u0441\u044c, \u0441\u0442\u043e\u043b\u0431\u0435\u0446 \u043d\u0443\u0436\u043d\u043e \u043e\u0441\u0442\u0430\u0432\u0438\u0442\u044c","f46730e5":"The MAPE mertic for the default CatBoost model using 4-fold CV is: 14.60%.\n\nThe MAE mertic for default CatBoost model: 299712.70\n\nThe RMSE default CatBoost model: 1291909.98","cb820d35":"\u0412 \u0434\u0430\u043b\u044c\u043d\u0435\u0439\u0448\u0435\u043c, \u043c\u043e\u0434\u0435\u043b\u0438 \u0441 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435\u043c \u0446\u0435\u043b\u0435\u0432\u043e\u0439 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u043e\u0439 \u043f\u043e\u043a\u0430\u0437\u0430\u043b\u0438 \u043b\u0443\u0447\u0448\u0438\u0439 \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442, \u043f\u043e\u044d\u0442\u043e\u043c\u0443 out \u0431\u0435\u0437 \u043b\u043e\u0433\u0430\u0440\u0438\u0444\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u0442\u044c \u043d\u0435 \u0431\u0443\u0434\u0443","44f70ea3":"# Model 4 : CatBoost","c61ab710":"# homeFacts","b3cdc142":"# Model 2 : Random Forest","ff044e55":"The MAPE mertic for the default CatBoost model using 4-fold CV is: 15.33%.\n\nThe MAE mertic for default CatBoost model: 300495.67\n\nThe RMSE default CatBoost model: 1298697.18","1d5acf9d":"\u041f\u043e\u043f\u0440\u043e\u0431\u0443\u044e \u043a\u043e\u043c\u0431\u0438\u043d\u0430\u0446\u0438\u044e \u0438\u0437 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043f\u043e\u043a\u0430\u0437\u0430\u0432\u0448\u0438\u0445 \u043b\u0443\u0447\u0448\u0438\u0435 \u043c\u0435\u0442\u0440\u0438\u043a\u0438","d70ac466":"The MAPE mertic for the default CatBoost model using 4-fold CV is: 12.76%.\n\nThe MAE mertic for default CatBoost model: 300974.52\n\nThe RMSE default CatBoost model: 1292707.73","d9b42f17":"# Model 6 : StackingRegressor","952e22dc":"The MAE mertic for GradientBoosting model: 644162.03\n\nThe MAPE mertic for GradientBoosting model: 1.00%\n\nThe RMSE mertic for GradientBoosting model: 2050655.03","a6fbd830":"\u0422\u0435\u043f\u0435\u0440\u044c \u043f\u0440\u043e\u0430\u043d\u0430\u043b\u0438\u0437\u0438\u0440\u0443\u044e  \u0434\u0430\u043d\u043d\u044b\u0435 \u043e \u043f\u0430\u0440\u043a\u043e\u0432\u043a\u0435","e83c0c0f":"\u0420\u0430\u0437\u0431\u0438\u0432\u0430\u044e \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0430 test \u0438 train","a8f82239":"# Model 3 : ExtraTreesRegressor","ada765f2":"# ML models"}}