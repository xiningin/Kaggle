{"cell_type":{"439e4ab9":"code","f7443958":"code","ecf168d9":"code","c860169b":"code","80dbf690":"code","4a27b925":"code","e973324a":"code","f16ea3b0":"code","56ed1be6":"code","c581a098":"code","c87dd8b0":"code","737af7fe":"code","480b0de5":"code","0509a22d":"code","744f79bb":"code","2ce5ed90":"code","f4c9997c":"code","1870b6b0":"code","de77ac35":"code","e58f1923":"code","78c7d31d":"code","b15621f2":"code","adfe2f0c":"markdown"},"source":{"439e4ab9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom sklearn import preprocessing\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.metrics import confusion_matrix, classification_report,accuracy_score\nfrom scipy.spatial.distance import mahalanobis\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f7443958":"df = pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","ecf168d9":"df.describe()","c860169b":"print(df.shape)\ndata= df.sample(frac = 0.2,random_state=1)\nprint(data.shape)","80dbf690":"df.isnull().values.any()","4a27b925":"num_classes = pd.value_counts(df['Class'], sort = True)\nnum_classes.plot(kind = 'bar')\nplt.title(\"Transaction Class Distribution\")\nplt.xticks(range(2), [\"Normal\", \"Fraud\"])\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");","e973324a":"fraud = df[df['Class'] == 1]\nnormal = df[df['Class'] == 0]\n\nprint(fraud.shape, normal.shape)","f16ea3b0":"fraud.describe()","56ed1be6":"normal.describe()","c581a098":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Time of transaction v\/s Amount by Class type')\nbins = 10\nax1.scatter(fraud.Time, fraud.Amount)\nax1.set_title('Fraud')\nax2.scatter(normal.Time, normal.Amount)\nax2.set_title('Normal')\nplt.xlabel('Time (in secs)')\nplt.ylabel('Amount')\nplt.xlim((0, 20000))\n\nplt.show()","c87dd8b0":"f, (ax1, ax2) = plt.subplots(2, 1, sharex=True)\nf.suptitle('Amount per transaction by class')\nbins = 10\nax1.hist(fraud.Amount, bins = bins)\nax1.set_title('Fraud')\nax2.hist(normal.Amount, bins = bins)\nax2.set_title('Normal')\nplt.xlabel('Amount ($)')\nplt.ylabel('Number of Transactions')\nplt.xlim((0, 20000))\nplt.yscale('log')\n\nplt.show()","737af7fe":"fraud = data[data['Class']==1]\nnormal = data[data['Class']==0]\noutlier_fraction = len(fraud)\/float(len(normal))\n\nanomaly_fraction = len(fraud)\/float(len(normal))\nprint(anomaly_fraction)\n\nprint(\"Fraud Cases: \" + str(len(fraud)))\nprint(\"Normal Cases: \" + str(len(normal)))","480b0de5":"data.hist(figsize=(15,15), bins = 64)\nplt.show()","0509a22d":"data.drop(['Time', 'V1', 'V24'], axis=1, inplace=True)\ncolumns = data.columns.tolist()\n\ntarget=columns[-1]\ncolumns = columns[:-1]\n\nX_train = data.iloc[:45000, :-1]\ny_train = data.iloc[:45000, -1]\n\nX_test = data.iloc[45000:, :-1]\ny_test = data.iloc[45000:, -1]\n\nprint(X_train.shape, X_test.shape)\nprint(y_train.shape, y_test.shape)","744f79bb":"columns = data.columns.tolist()\ncolumns = [c for c in columns if c not in [\"Class\"]]\ntarget = \"Class\"\n\n# state = np.random.RandomState(0)\nX = data[columns]\nY = data[target]\n# X_outliers = state.uniform(low=0, high=1, size=(X.shape[0], X.shape[1]))\n\nprint(X.shape)\nprint(Y.shape)","2ce5ed90":"# model = LocalOutlierFactor(algorithm='auto', metric='mahalanobis', metric_params={'V': np.cov(X)})\nmodel = LocalOutlierFactor(contamination=anomaly_fraction)","f4c9997c":"y_train_pred = model.fit_predict(X_train)\n# print(y_train[:5], y_train_pred[:5])\ny_train_pred[y_train_pred == 1] = 0\ny_train_pred[y_train_pred == -1] = 1\n\ny_test_pred = model.fit_predict(X_test)\n# print(y_train[:5], y_train_pred[:5])\ny_test_pred[y_test_pred == 1] = 0\ny_test_pred[y_test_pred == -1] = 1","1870b6b0":"import itertools\nclasses = np.array(['0','1'])\n\ndef plot_confusion_matrix(cm, classes,title='Confusion matrix', cmap=plt.cm.Blues):\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = 'd' \n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","de77ac35":"cm_train = confusion_matrix(y_train, y_train_pred)\nplot_confusion_matrix(cm_train,[\"Normal\", \"Fraud\"])","e58f1923":"cm_test = confusion_matrix(y_test_pred, y_test)\nplot_confusion_matrix(cm_test,[\"Normal\", \"Fraud\"])","78c7d31d":"print('Total fraudulent transactions detected in training set: ' + str(cm_train[1][1]) + ' \/ ' + str(cm_train[1][1]+cm_train[1][0]))\nprint('Total non-fraudulent transactions detected in training set: ' + str(cm_train[0][0]) + ' \/ ' + str(cm_train[0][1]+cm_train[0][0]))\n\nprint('Probability to detect a fraudulent transaction in the training set: ' + str(cm_train[1][1]\/(cm_train[1][1]+cm_train[1][0])))\nprint('Probability to detect a non-fraudulent transaction in the training set: ' + str(cm_train[0][0]\/(cm_train[0][1]+cm_train[0][0])))\n\nprint(\"Accuracy of unsupervised anomaly detection model on the training set: \"+str(100*(cm_train[0][0]+cm_train[1][1]) \/ (sum(cm_train[0]) + sum(cm_train[1]))) + \"%\")","b15621f2":"print('Total fraudulent transactions detected in test set: ' + str(cm_test[1][1]) + ' \/ ' + str(cm_test[1][1]+cm_test[1][0]))\nprint('Total non-fraudulent transactions detected in test set: ' + str(cm_test[0][0]) + ' \/ ' + str(cm_test[0][1]+cm_test[0][0]))\n\nprint('Probability to detect a fraudulent transaction in the test set: ' + str(cm_test[1][1]\/(cm_test[1][1]+cm_test[1][0])))\nprint('Probability to detect a non-fraudulent transaction in the test set: ' + str(cm_test[0][0]\/(cm_test[0][1]+cm_test[0][0])))\n\nprint(\"Accuracy of unsupervised anomaly detection model on the test set: \"+str(100*(cm_test[0][0]+cm_test[1][1]) \/ (sum(cm_test[0]) + sum(cm_test[1]))) + \"%\")","adfe2f0c":"The results we've got through this model are far from ideal. We have not been able to classify fraudulent transactions efficiently despite having a high accuracy (which is not a good metric to measure performance on a skewed dataset anyways). \nSupervised learning for anomaly detection is the move fot this dataset since we have the labels. One reason why unsupervised learning did not perform well enough is because most of the fraudulent transactions did not have much unusual characteristics regarding them which can be well separated from normal transactions and I feel that's the main reason they provided us with a labelled dataset. \nAnyways, this notebook represents how unsupervised learning captures anomalies. The accuracy of detecting anomalies on the test set is 25%, which is way better than a random guess (the fraction of anomalies in the dataset is < 0.1%). I have also implemented the supervised learning model for this dataset, which works extremely well. Check it out [here](https:\/\/www.kaggle.com\/vardaanbajaj\/credit-card-fraud-svm)."}}