{"cell_type":{"c80975ac":"code","74964e82":"code","8fde0859":"code","f5908035":"code","1bab2ba5":"code","a5a65a03":"code","09efbda6":"code","f5b2360f":"code","70985d6f":"code","a75655d1":"code","9056a282":"code","e4cb45f8":"code","1ae8c98e":"code","32c2f162":"code","dba3b798":"code","723c7b36":"code","931f222c":"code","527d5f39":"code","5d219711":"code","58f85b0b":"code","943529ee":"code","d9b0361c":"code","2bf3e3d4":"code","4dc78bbe":"code","a23c2f3a":"code","c2680a93":"code","08af1348":"code","90742584":"code","4cb8b136":"code","760e5c2e":"code","88401459":"code","e6a403c6":"code","d6436acb":"code","646813dd":"code","a3cfe6cc":"code","d68e5b3f":"code","7674ae85":"code","b9e77bbd":"code","ab7e5671":"code","9bb57532":"code","4bb882d9":"code","e169a48e":"code","d7777f9a":"code","37423670":"code","4d38cc21":"code","d71fefc2":"code","2edd8008":"code","f51867d5":"code","ab5e41d7":"code","363410ab":"code","2e8f4f51":"code","e0c19248":"code","25c2ff7a":"code","44a86872":"code","f9c9edc1":"code","86fff124":"code","9db1c26d":"code","85134697":"code","ff1518c3":"code","ae381fe0":"code","a6a1875c":"code","bd7eb873":"code","f2266201":"code","e92f984f":"code","8530cc23":"code","c848d379":"code","bea1be3b":"code","84aa265f":"code","f5d5d440":"code","a7f74e53":"code","8bdfa379":"code","e8db667f":"code","33ca4d8f":"code","c7128381":"code","dd91082a":"code","de147cc8":"code","25e105bc":"code","6e6e9c70":"code","16c4de16":"code","f4628e18":"code","f3127990":"code","be020c05":"code","92c1b31d":"code","f7140220":"markdown","fea00edc":"markdown","1f8a8bcc":"markdown","53551f1e":"markdown","563c5d79":"markdown","a76aa3cc":"markdown","f5bf78f7":"markdown","64d3d6b8":"markdown","4077563e":"markdown","71b035a9":"markdown","14d60267":"markdown","e13cc1ff":"markdown","5fefdb55":"markdown","7015e4eb":"markdown","8be35b07":"markdown","d1dfe3d6":"markdown","d1524aa5":"markdown","ae7bc5a7":"markdown","f8245414":"markdown","eaffee7f":"markdown","f7037ce5":"markdown","2292e451":"markdown","a6fad6e5":"markdown","32fe8bbc":"markdown","0a6f60f4":"markdown","0ec41051":"markdown","a4285ace":"markdown","90cfa3a2":"markdown","6913fa29":"markdown","540374cd":"markdown","0b817e0f":"markdown","c745a0c1":"markdown","4537d3aa":"markdown","c210f332":"markdown","e20f9971":"markdown","04c13739":"markdown","ee94c97d":"markdown","1d630057":"markdown","2c48cb87":"markdown","af8d1681":"markdown","5cfb31e7":"markdown","b3fafec6":"markdown","1b13f74a":"markdown","2137bc19":"markdown","af79fe7d":"markdown","0f3fdacc":"markdown","38a9bcf9":"markdown","58e4837a":"markdown","ed8ccc5c":"markdown","4945ef0e":"markdown","559245d9":"markdown","ff27c484":"markdown","0740b48b":"markdown","041a5f73":"markdown","b14abeb7":"markdown","11a72a4c":"markdown","2db68a26":"markdown","89dfb69e":"markdown","5bbed717":"markdown","9d8c5d48":"markdown","c00fa36e":"markdown","bf40ac7d":"markdown","a8189bfd":"markdown","39700cec":"markdown","6606a83c":"markdown","42c5592b":"markdown","99aa84ed":"markdown","0bae20ef":"markdown","e327d422":"markdown","95f33443":"markdown"},"source":{"c80975ac":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","74964e82":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avanc\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","8fde0859":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')","f5908035":"df.head(10)","1bab2ba5":"df.columns","a5a65a03":"from IPython.core.display import HTML # permet d'afficher du code html dans jupyter\ndisplay(HTML(df.head(10).to_html()))","09efbda6":"df.shape","f5b2360f":"df.describe()","70985d6f":"df.columns","a75655d1":"df = df.drop(['Unnamed: 32'], axis=1)","9056a282":"df.diagnosis.value_counts()","e4cb45f8":"malin = df.diagnosis == 'M'\nbenin = df.diagnosis == 'B'","1ae8c98e":"sns.jointplot(\"perimeter_worst\", \"area_worst\", df, kind='kde');","32c2f162":"plt.figure(figsize=(12,12))\nsns.kdeplot(df.perimeter_worst, df.area_worst,  shade=True)","dba3b798":"plt.figure(figsize=(12,12))\nsns.kdeplot(df[malin].perimeter_worst, df[malin].area_worst, cmap=\"Reds\",  shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[benin].perimeter_worst, df[benin].area_worst, cmap=\"Greens\", shade=True, alpha=0.3, shade_lowest=False)","723c7b36":"sns.boxplot(x=\"diagnosis\", y=\"perimeter_worst\", data=df)","931f222c":"sns.violinplot(x=\"diagnosis\", y=\"perimeter_worst\", data=df)","527d5f39":"fig = sns.FacetGrid(df, hue=\"diagnosis\", aspect=3, palette=\"Set2\") # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"perimeter_worst\", shade=True)\nfig.add_legend()","5d219711":"sns.lmplot(x=\"radius_mean\", y=\"texture_mean\", data=df, fit_reg=False, hue='diagnosis')","58f85b0b":"#sns.pairplot(df, hue=\"diagnosis\")","943529ee":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)     # le reste des donn\u00e9es pour le test","d9b0361c":"X_train = data_train.drop(['diagnosis'], axis=1)\ny_train = data_train['diagnosis']\nX_test = data_test.drop(['diagnosis'], axis=1)\ny_test = data_test['diagnosis']","2bf3e3d4":"plt.figure(figsize=(9,9))\n\nlogistique = lambda x: np.exp(x)\/(1+np.exp(x))   \n\nx_range = np.linspace(-10,10,50)       \ny_values = logistique(x_range)\n\nplt.plot(x_range, y_values, color=\"red\")","4dc78bbe":"from sklearn.linear_model import LogisticRegression","a23c2f3a":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","c2680a93":"y_lr = lr.predict(X_test)","08af1348":"from sklearn.metrics import accuracy_score, confusion_matrix","90742584":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","4cb8b136":"# Matrice de confusion\ncm = confusion_matrix(y_test, y_lr)\nprint(cm)","760e5c2e":"pd.crosstab(y_test, y_lr, rownames=['Reel'], colnames=['Prediction'], margins=True)","88401459":"df.columns","e6a403c6":"df = df.drop(['id'], axis=1)","d6436acb":"df.head()","646813dd":"data_train = df.sample(frac=0.8, random_state=2)          # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)     # le reste des donn\u00e9es pour le test","a3cfe6cc":"X_train = data_train.drop(['diagnosis'], axis=1)\ny_train = data_train['diagnosis']\nX_test = data_test.drop(['diagnosis'], axis=1)\ny_test = data_test['diagnosis']","d68e5b3f":"lr = LogisticRegression()\nlr.fit(X_train,y_train)","7674ae85":"y_lr = lr.predict(X_test)","b9e77bbd":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","ab7e5671":"pd.crosstab(y_test, y_lr, rownames=['Reel'], colnames=['Prediction'], margins=True)","9bb57532":"fig = sns.FacetGrid(df, hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"perimeter_worst\", shade=True)\nfig.add_legend()","4bb882d9":"fig = sns.FacetGrid(df[df.perimeter_worst>110], hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"texture_mean\", shade=True)\nfig.add_legend()","e169a48e":"fig = sns.FacetGrid(df[(df.perimeter_worst>110) & (df.texture_mean>17)], hue=\"diagnosis\", aspect=3) # aspect=3 permet d'allonger le graphique\nfig.map(sns.kdeplot, \"concave points_mean\", shade=True)\nfig.add_legend()","d7777f9a":"from sklearn import tree\ndtc = tree.DecisionTreeClassifier()\ndtc.fit(X_train,y_train)\ny_dtc = dtc.predict(X_test)\nprint(accuracy_score(y_test, y_dtc))","37423670":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","4d38cc21":"dtc1 = tree.DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 20)\ndtc1.fit(X_train,y_train)","d71fefc2":"plt.figure(figsize=(30,30))\ntree.plot_tree(dtc1, feature_names=X_train.columns, class_names=['benin','malin'], fontsize=14, filled=True)  ","2edd8008":"y_dtc1 = dtc1.predict(X_test)\nprint(accuracy_score(y_test, y_dtc1))","f51867d5":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)\ny_rf = rf.predict(X_test)","ab5e41d7":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","363410ab":"pd.crosstab(y_test, y_rf, rownames=['Reel'], colnames=['Prediction'], margins=True)","2e8f4f51":"importances = rf.feature_importances_\nindices = np.argsort(importances)","e0c19248":"plt.figure(figsize=(12,8))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), df.columns[indices])\nplt.title('Importance des caracteristiques')","25c2ff7a":"# Directive pour afficher les graphiques dans Jupyter\n%matplotlib inline","44a86872":"# Pandas : librairie de manipulation de donn\u00e9es\n# NumPy : librairie de calcul scientifique\n# MatPlotLib : librairie de visualisation et graphiques\n# SeaBorn : librairie de graphiques avanc\u00e9s\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns","f9c9edc1":"from sklearn.linear_model import LogisticRegression","86fff124":"df = sns.load_dataset('iris')","9db1c26d":"df.head()","85134697":"setosa = df.species == 'setosa'\nvirginica = df.species == 'virginica'\nversicolor = df.species == 'versicolor'","ff1518c3":"sns.pairplot(df, hue=\"species\")","ae381fe0":"sns.jointplot(\"sepal_length\", \"sepal_width\", df, kind='kde');","a6a1875c":"sns.jointplot(\"petal_length\", \"petal_width\", df, kind='kde');","bd7eb873":"plt.figure(figsize = (6,6))\nsns.kdeplot(df[setosa].sepal_length, df[setosa].sepal_width, cmap = \"Reds\",  shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[virginica].sepal_length, df[virginica].sepal_width, cmap = \"Greens\", shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[versicolor].sepal_length, df[versicolor].sepal_width, cmap = \"Blues\", shade=True, alpha=0.3, shade_lowest=False)","f2266201":"plt.figure(figsize = (6,6))\nsns.kdeplot(df[setosa].petal_length, df[setosa].petal_width, cmap = \"Reds\",  shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[virginica].petal_length, df[virginica].petal_width, cmap = \"Greens\", shade=True, alpha=0.3, shade_lowest=False)\nsns.kdeplot(df[versicolor].petal_length, df[versicolor].petal_width, cmap = \"Blues\", shade=True, alpha=0.3, shade_lowest=False)","e92f984f":"sns.boxplot(x=\"species\", y=\"sepal_length\", data=df)","8530cc23":"sns.boxplot(x=\"species\", y=\"sepal_width\", data=df)","c848d379":"sns.boxplot(x=\"species\", y=\"petal_length\", data=df)","bea1be3b":"sns.boxplot(x=\"species\", y=\"petal_width\", data=df)","84aa265f":"sns.violinplot(x=\"species\", y=\"sepal_length\", data=df)","f5d5d440":"sns.violinplot(x=\"species\", y=\"sepal_width\", data=df)","a7f74e53":"sns.violinplot(x=\"species\", y=\"petal_length\", data=df)","8bdfa379":"sns.violinplot(x=\"species\", y=\"petal_width\", data=df)","e8db667f":"data_train = df.sample(frac=0.8, random_state=1)          # 80% des donn\u00e9es avec frac=0.8\ndata_test = df.drop(data_train.index)                     # le reste des donn\u00e9es pour le test","33ca4d8f":"X_train = data_train.drop(['species'], axis=1)\ny_train = data_train['species']\nX_test = data_test.drop(['species'], axis=1)\ny_test = data_test['species']","c7128381":"lr = LogisticRegression(max_iter = 200)\nlr.fit(X_train,y_train)","dd91082a":"y_lr = lr.predict(X_test)","de147cc8":"lr_score = accuracy_score(y_test, y_lr)\nprint(lr_score)","25e105bc":"pd.crosstab(y_test, y_lr, rownames=['Reel'], colnames=['Prediction'], margins=True)","6e6e9c70":"from sklearn import ensemble\nrf = ensemble.RandomForestClassifier()\nrf.fit(X_train, y_train)","16c4de16":"y_rf = rf.predict(X_test)","f4628e18":"rf_score = accuracy_score(y_test, y_rf)\nprint(rf_score)","f3127990":"pd.crosstab(y_test, y_rf, rownames=['Reel'], colnames=['Prediction'], margins=True)","be020c05":"importances = rf.feature_importances_\nindices = np.argsort(importances)","92c1b31d":"plt.figure(figsize=(12,8))\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), df.columns[indices])\nplt.title('Importance des caracteristiques')","f7140220":"On s\u00e9pare les donn\u00e9es d'apprentissage (*X_train*) et la cible (*y_train*, la colonnes des donn\u00e9es *classe*)","fea00edc":"## Librairies utiles","1f8a8bcc":"*jointplot* permet de visualiser dans un plan les distributions d'un couple de param\u00e8tres :","53551f1e":"On a les informations suivantes :\n- longueur du s\u00e9pale (en cm)\n- largeur du s\u00e9pale\n- longueur du p\u00e9tale\n- largeur du p\u00e9tale\n- esp\u00e8ce : Virginica, Setosa ou Versicolor","563c5d79":"La r\u00e9gression logistique consiste \u00e0 trouver une fonction lin\u00e9aire C(X) qui permette d'estimer la probabilit\u00e9 de $Y=1$ connaissant $X$ :\n$$p(Y=1|X) = \\frac{e^{C(X)}}{1+e^{C(X)}}$$","a76aa3cc":"## R\u00e9gression logistique","f5bf78f7":"***Score et matrice de confusion***","64d3d6b8":"On entra\u00eene le mod\u00e8le de r\u00e9gression logistique avec *fit* :","4077563e":"## Arbres de d\u00e9cision","71b035a9":"En utilisant le *jointplot* , nous pouvons visualiser la tendance de la forme des sepals et des pedals :","14d60267":"Observez les violins plots suivants :","e13cc1ff":"On veut tracer un nuage de points selon le rayon et la texture de la tumeur, en diff\u00e9renciant la couleur des points selon le diagnostic :","5fefdb55":"# *Machine Learning*","7015e4eb":"cf par exemple :  \nhttps:\/\/fr.wikipedia.org\/wiki\/For%C3%AAt_d%27arbres_d%C3%A9cisionnels  \nhttps:\/\/www.biostars.org\/p\/86981\/  \nhttps:\/\/infinitescript.com\/2016\/08\/random-forest\/","8be35b07":"Le dataset des iris est pr\u00e9d\u00e9fini dans seaborn :","d1dfe3d6":"*Observez les diagrammes en bo\u00eete suivants :*","d1524aa5":"## Score et matrice de confusion","ae7bc5a7":"On obtient un arbre un peu diff\u00e9rent :","f8245414":"# Logistic Regression","eaffee7f":"<img src=\"https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcQM3aH4Q3AplfE1MR3ROAp9Ok35fafmNT59ddXkdEvNdMkT8X6E\">","f7037ce5":"L'attribut *feature_importances_* renvoie un tableau du poids de chaque caract\u00e9ristique dans la d\u00e9cision :","2292e451":"### Exercice  \nLe r\u00e9sultat est-il satisfaisant ?  \nQuel pourrait \u00eatre le probl\u00e8me ?","a6fad6e5":"Le dataset est accessible sur :  \nhttps:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data  \nhttp:\/\/archive.ics.uci.edu\/ml\/datasets\/breast+cancer+wisconsin+%28diagnostic%29  \n(on peut utiliser pd.read_table pour lire un fichier .dat)","32fe8bbc":"# *Visualisations*","0a6f60f4":"On entra\u00eene le mod\u00e8le de r\u00e9gression logistique avec fit :","0ec41051":"### Exercice : les iris","a4285ace":"La mesure de pertinence compte le nombre de fois o\u00f9 l'algorithme a fait une bonne pr\u00e9diction (en pourcentage) :","90cfa3a2":"On trace ce type de graphique avec des couleurs :","6913fa29":"## Random forests","540374cd":"On peut tracer ce type de graphique avec des couleurs :","0b817e0f":"Pour plus de d\u00e9tails, cf par exemple :  \nhttp:\/\/eric.univ-lyon2.fr\/~ricco\/cours\/cours\/pratique_regression_logistique.pdf","c745a0c1":"Les **violins plots** sont similaires aux box plots, except\u00e9 qu\u2019ils permettent de montrer la courbe de densit\u00e9 de probabilit\u00e9 des diff\u00e9rentes valeurs. Typiquement, les violins plots pr\u00e9sentent un marqueur pour la m\u00e9diane des donn\u00e9es et l\u2019\u00e9cart interquartile, comme dans un box plot standard.","4537d3aa":"Une mesure plus fine consiste \u00e0 compter le nombre de **faux positif** (valeur pr\u00e9dite 1 et r\u00e9elle 0) et de **vrai n\u00e9gatif** (valeur pr\u00e9dite 0 et r\u00e9elle 1). On utilise une **matrice de confusion** :","c210f332":"On peut visualiser ces degr\u00e9s d'importance avec un graphique \u00e0 barres par exemple :","e20f9971":"Les **diagrammes en bo\u00eete** (ou **bo\u00eetes \u00e0 moustaches** ou **box plot**) r\u00e9sument quelques caract\u00e9ristiques de position du caract\u00e8re \u00e9tudi\u00e9 (m\u00e9diane, quartiles, minimum, maximum ou d\u00e9ciles). Ce diagramme est utilis\u00e9 principalement pour comparer un m\u00eame caract\u00e8re dans deux populations de tailles diff\u00e9rentes. Il s'agit de tracer un rectangle allant du premier quartile au troisi\u00e8me quartile et coup\u00e9 par la m\u00e9diane. On ajoute alors des segments aux extr\u00e9mit\u00e9s menant jusqu'aux valeurs extr\u00eames.  \nPar exemple pour la r\u00e9partion des esp\u00e8ces selon la longueur du s\u00e9pale :","04c13739":"## Donn\u00e9es","ee94c97d":"La colonne **'Unnamed: 32'** est vide : on va la supprimer ","1d630057":"On s\u00e9pare le dataset en deux parties :\n- un ensemble d'apprentissage (entre 70% et 90% des donn\u00e9es), qui va permettre d'entra\u00eener le mod\u00e8le\n- un ensemble de test (entre 10% et 30% des donn\u00e9es), qui va permettre d'estimer la pertinence de la pr\u00e9diction","2c48cb87":"Importance des caract\u00e9ristiques","af8d1681":"*FacetGrid* permet de superposer des graphiques selon une ou plusieurs caract\u00e9ristiques. On cr\u00e9e une structure avec *FacetGrid*, et on trace ensuite les graphiques avec *map*","5cfb31e7":"On peut modifier certains param\u00e8tres :  Le param\u00e8tre *max_depth* est un seuil sur la profondeur maximale de l\u2019arbre. Le param\u00e8tre *min_samples_leaf* donne le nombre minimal d\u2019\u00e9chantillons dans un noeud feuille.","b3fafec6":"***Score et matrice de confusion***","1b13f74a":"*Tous les imports*","2137bc19":"## Visualisations","af79fe7d":"*Les resultats \u00e9taient parfaits!!!*","0f3fdacc":"Pour plus de d\u00e9tails sur les arbres de d\u00e9cision :  \nhttps:\/\/zestedesavoir.com\/tutoriels\/962\/les-arbres-de-decisions\/comprendre-le-concept\/#1-les-origines  \nhttp:\/\/cedric.cnam.fr\/vertigo\/Cours\/ml2\/tpArbresDecision.html  \nhttp:\/\/perso.mines-paristech.fr\/fabien.moutarde\/ES_MachineLearning\/Slides\/coursFM_AD-RF.pdf  ","38a9bcf9":"L'indice *GINI* mesure avec quelle fr\u00e9quence un \u00e9l\u00e9ment al\u00e9atoire de l'ensemble serait mal class\u00e9 si son \u00e9tiquette \u00e9tait s\u00e9lectionn\u00e9e al\u00e9atoirement depuis la distribution des \u00e9tiquettes dans le sous-ensemble.","58e4837a":"On peut pr\u00e9dire les valeurs sur l'ensemble de test avec le mod\u00e8le entra\u00een\u00e9 :","ed8ccc5c":"On a un *faux positif* et deux *faux n\u00e9gatifs*","4945ef0e":"*pairplot* affiche les nuages de points associ\u00e9s \u00e0 tous les couples de param\u00e8tres :","559245d9":"On peut tracer la courbe de r\u00e9gression logistique pour pr\u00e9dire l'esp\u00e8ce Virginica \u00e0 partir de la longueur du s\u00e9pale avec la fonction *lmplot* :","ff27c484":"Un arbre de d\u00e9cision permet de faire \u00e0 chaque \u00e9tape un choix entre deux possibilit\u00e9s, pour arriver \u00e0 une r\u00e9ponse sur les feuilles (cf. Akinator)  \n<img src=\"https:\/\/fr.akinator.com\/bundles\/elokencesite\/images\/akitudes_670x1096\/defi.png?v95\">","0740b48b":"# Random Forest","041a5f73":"On a les informations suivantes :\n1) ID number 2) Diagnosis (M = malignant, B = benign) 3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)  \nb) texture (standard deviation of gray-scale values)  \nc) perimeter  \nd) area  \ne) smoothness (local variation in radius lengths)  \nf) compactness (perimeter^2 \/ area - 1.0)  \ng) concavity (severity of concave portions of the contour)  \nh) concave points (number of concave portions of the contour)  \ni) symmetry  \nj) fractal dimension (\"coastline approximation\" - 1)  \n  \nThe mean, standard error and \"worst\" or largest (mean of the three largest values) of these features were computed for each image, resulting in 30 features. For instance, field 3 is Mean Radius, field 13 is Radius SE, field 23 is Worst Radius.  \n  \nAll feature values are recoded with four significant digits.","b14abeb7":"## Machine learning","11a72a4c":"On peut afficher les 10 premi\u00e8res lignes du dataset :","2db68a26":"Pour avoir l'ensemble du tableau, on peut utiliser un affichage au format HTML :","89dfb69e":"On veut pr\u00e9dire une variable al\u00e9atoire $Y$ \u00e0 partir d'un vecteur de variables explicatives $X=(X_1,...,X_n)$\nOn \n","5bbed717":"### Importance des caract\u00e9ristiques","9d8c5d48":"Autrement dit, cela revient \u00e0 trouver une s\u00e9paration lin\u00e9aire des caract\u00e9ristiques qui minimise un crit\u00e8re d'erreur.","c00fa36e":"## Le dataset Breast Cancer Wisconsin","bf40ac7d":"On peut aussi utiliser la m\u00e9thode **crosstab** de **Pandas** (plut\u00f4t que la m\u00e9thode confusion_matrix de sklearn) pour afficher la matrice de confusion :","a8189bfd":"Pour construire un arbre de d\u00e9cision \u00e0 partir d'un ensemble d'apprentissage, on va choisir une variable qui s\u00e9pare l'ensemble en deux parties les plus distinctes en fonction d'un crit\u00e8re. Sur les iris par exemple, on peut utiliser la largeur du p\u00e9tale pour s\u00e9parer l'esp\u00e8ce Setosa des autres.","39700cec":"Et la matrice de confusion :","6606a83c":"<img src=\"https:\/\/infinitescript.com\/wordpress\/wp-content\/uploads\/2016\/08\/Random-Forest-Example.jpg\">","42c5592b":"On veut maintenant pr\u00e9dire l'esp\u00e8ce \u00e0 partir de toutes les caract\u00e9ristiques, et \u00e9valuer la qualit\u00e9 de cette pr\u00e9diction en utilisant la r\u00e9gression logistique d\u00e9finie dans la librairie *sklearn* :","99aa84ed":"La fonction logistique $\\frac{e^{x}}{1+e^{x}}$ varie entre $-\\infty$ et $+\\infty$ pour $x$ variant entre $0$ et $1$.  \nElle est souvent utilis\u00e9e pour \"mapper\" une probabilit\u00e9 et un espace r\u00e9el","0bae20ef":"# Breast cancer Wisconsin","e327d422":"<img src=\"https:\/\/i.stack.imgur.com\/gKyb9.png\">","95f33443":"*On utiliser la m\u00e9thode crosstab de Pandas (plut\u00f4t que la m\u00e9thode confusion_matrix de sklearn) pour afficher la matrice de confusion :*"}}