{"cell_type":{"087db378":"code","fd8cdc8c":"code","a315c8d4":"code","54d1dabc":"code","6df936bd":"code","f2fc69bd":"code","ed916761":"code","7b374bc3":"code","b6a0e9d7":"code","5ee00719":"code","f9b15cd7":"code","5b651551":"code","ed35e70f":"code","9b054d38":"code","4d832889":"code","66484e2a":"code","f8a98929":"code","ece30a32":"markdown","a5735fe1":"markdown","2de91834":"markdown","4567dc8e":"markdown"},"source":{"087db378":"import numpy as np \nimport gc\nimport pandas as pd\npd.set_option(\"display.max_columns\", 999)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm_notebook\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.model_selection import cross_val_score, cross_validate\nfrom sklearn.model_selection import KFold, ShuffleSplit, StratifiedShuffleSplit, train_test_split, StratifiedKFold, TimeSeriesSplit\n\nfrom time import time, ctime\n\nimport catboost\nfrom catboost import CatBoostClassifier, Pool\n\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, roc_auc_score\n\nRANDOM_STATE = 12061985\nnp.random.seed(RANDOM_STATE)\n\n# HPO\nfrom skopt.space import Integer, Categorical, Real\nfrom skopt.utils import use_named_args\nfrom skopt import gp_minimize, gbrt_minimize, forest_minimize, dummy_minimize\nfrom skopt.plots import plot_convergence\nfrom skopt.callbacks import DeltaXStopper, DeadlineStopper, DeltaYStopper, EarlyStopper","fd8cdc8c":"def get_params_SKopt(model, X, Y, space, cv_search, alg = 'catboost', cat_features = None, eval_dataset = None, UBM = False, opt_method = 'gbrt_minimize', \n                     verbose = True,  multi = False, scoring = 'neg_mean_squared_error', n_best = 50, total_time = 7200):\n    \n    if alg == 'catboost':\n        fitparam = { 'eval_set' : eval_dataset,\n                     'use_best_model' : UBM,\n                     'cat_features' : cat_features,\n                     'early_stopping_rounds': 10 }\n    else:\n        fitparam = {}\n        \n    @use_named_args(space)\n    def objective(**params):\n        model.set_params(**params)\n        return -np.mean(cross_val_score(model, \n                                        X, Y, \n                                        cv=cv_search, \n                                        scoring= scoring,\n                                        fit_params=fitparam))\n    \n    if opt_method == 'gbrt_minimize':\n        \n        HPO_PARAMS = {'n_calls':1000,\n                      'n_random_starts':20,\n                      'acq_func':'EI',}\n        \n        reg_gp = gbrt_minimize(objective, \n                               space, \n                               n_jobs = -1,\n                               verbose = verbose,\n                               callback = [DeltaYStopper(delta = 0.01, n_best = 5), RepeatedMinStopper(n_best = n_best), DeadlineStopper(total_time = total_time)],\n                               **HPO_PARAMS,\n                               random_state = RANDOM_STATE)\n        \n\n    elif opt_method == 'forest_minimize':\n        \n        HPO_PARAMS = {'n_calls':1000,\n                      'n_random_starts':20,\n                      'acq_func':'EI',}\n        \n        reg_gp = forest_minimize(objective, \n                               space, \n                               n_jobs = -1,\n                               verbose = verbose,\n                               callback = [RepeatedMinStopper(n_best = n_best), DeadlineStopper(total_time = total_time)],\n                               **HPO_PARAMS,\n                               random_state = RANDOM_STATE)\n        \n    elif opt_method == 'gp_minimize':\n        \n        HPO_PARAMS = {'n_calls':1000,\n                      'n_random_starts':20,\n                      'acq_func':'gp_hedge',}        \n        \n        reg_gp = gp_minimize(objective, \n                               space, \n                               n_jobs = -1,\n                               verbose = verbose,\n                               callback = [RepeatedMinStopper(n_best = n_best), DeadlineStopper(total_time = total_time)],\n                               **HPO_PARAMS,\n                               random_state = RANDOM_STATE)\n    \n    TUNED_PARAMS = {} \n    for i, item in enumerate(space):\n        if multi:\n            TUNED_PARAMS[item.name.split('__')[1]] = reg_gp.x[i]\n        else:\n            TUNED_PARAMS[item.name] = reg_gp.x[i]\n    \n    return [TUNED_PARAMS,reg_gp]\n\nclass RepeatedMinStopper(EarlyStopper):\n    \"\"\"Stop the optimization when there is no improvement in the minimum.\n    Stop the optimization when there is no improvement in the minimum\n    achieved function evaluation after `n_best` iterations.\n    \"\"\"\n    def __init__(self, n_best=50):\n        super(EarlyStopper, self).__init__()\n        self.n_best = n_best\n        self.count = 0\n        self.minimum = np.finfo(np.float).max\n\n    def _criterion(self, result):\n        if result.fun < self.minimum:\n            self.minimum = result.fun\n            self.count = 0\n        elif result.fun > self.minimum:\n            self.count = 0\n        else:\n            self.count += 1\n\n        return self.count >= self.n_best","a315c8d4":"train_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')\ntest_transaction = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_transaction.csv')\ntest_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/test_identity.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/sample_submission.csv')\n\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')","54d1dabc":"del train_identity, train_transaction, test_identity, test_transaction","6df936bd":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","f2fc69bd":"# From https:\/\/www.kaggle.com\/artgor\/eda-and-models\ntrain['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\ntrain['D15_to_mean_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('mean')\ntrain['D15_to_mean_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('mean')\ntrain['D15_to_std_addr1'] = train['D15'] \/ train.groupby(['addr1'])['D15'].transform('std')\ntrain['D15_to_std_addr2'] = train['D15'] \/ train.groupby(['addr2'])['D15'].transform('std')\n\ntest['D15_to_mean_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('mean')\ntest['D15_to_mean_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('mean')\ntest['D15_to_std_addr1'] = test['D15'] \/ test.groupby(['addr1'])['D15'].transform('std')\ntest['D15_to_std_addr2'] = test['D15'] \/ test.groupby(['addr2'])['D15'].transform('std')","ed916761":"train[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = train['P_emaildomain'].str.split('.', expand=True)\ntrain[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = train['R_emaildomain'].str.split('.', expand=True)\ntest[['P_emaildomain_1', 'P_emaildomain_2', 'P_emaildomain_3']] = test['P_emaildomain'].str.split('.', expand=True)\ntest[['R_emaildomain_1', 'R_emaildomain_2', 'R_emaildomain_3']] = test['R_emaildomain'].str.split('.', expand=True)","7b374bc3":"# from https:\/\/www.kaggle.com\/nroman\/lgb-single-model-lb-0-9419\n\n# New feature - decimal part of the transaction amount\ntrain['TransactionAmt_decimal'] = ((train['TransactionAmt'] - train['TransactionAmt'].astype(int)) * 1000).astype(int)\ntest['TransactionAmt_decimal'] = ((test['TransactionAmt'] - test['TransactionAmt'].astype(int)) * 1000).astype(int)\n\n# Count encoding for card1 feature. \n# Explained in this kernel: https:\/\/www.kaggle.com\/nroman\/eda-for-cis-fraud-detection\ntrain['card1_count_full'] = train['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\ntest['card1_count_full'] = test['card1'].map(pd.concat([train['card1'], test['card1']], ignore_index=True).value_counts(dropna=False))\n\n# https:\/\/www.kaggle.com\/fchmiel\/day-and-time-powerful-predictive-feature\ntrain['Transaction_day_of_week'] = np.floor((train['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntest['Transaction_day_of_week'] = np.floor((test['TransactionDT'] \/ (3600 * 24) - 1) % 7)\ntrain['Transaction_hour'] = np.floor(train['TransactionDT'] \/ 3600) % 24\ntest['Transaction_hour'] = np.floor(test['TransactionDT'] \/ 3600) % 24\n\n# Some arbitrary features interaction\nfor feature in ['id_02__id_20', 'id_02__D8', 'D11__DeviceInfo', 'DeviceInfo__P_emaildomain', 'P_emaildomain__C2', \n                'card2__dist1', 'card1__card5', 'card2__id_20', 'card5__P_emaildomain', 'addr1__card1']:\n\n    f1, f2 = feature.split('__')\n    train[feature] = train[f1].astype(str) + '_' + train[f2].astype(str)\n    test[feature] = test[f1].astype(str) + '_' + test[f2].astype(str)\n    \nfor feature in ['id_34', 'id_36']:\n        # Count encoded for both train and test\n        train[feature + '_count_full'] = train[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        test[feature + '_count_full'] = test[feature].map(pd.concat([train[feature], test[feature]], ignore_index=True).value_counts(dropna=False))\n        \nfor feature in ['id_01', 'id_31', 'id_33', 'id_35', 'id_36']:\n        # Count encoded separately for train and test\n        train[feature + '_count_dist'] = train[feature].map(train[feature].value_counts(dropna=False))\n        test[feature + '_count_dist'] = test[feature].map(test[feature].value_counts(dropna=False))","b6a0e9d7":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() \/ train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() \/ test.shape[0] > 0.9]\n\nbig_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\n\ncols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\ncols_to_drop.remove('isFraud')\nprint(len(cols_to_drop))\n\ntrain = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","5ee00719":"cat_cols = []\nfor col in tqdm_notebook(train.columns):\n    if train[col].dtype == 'object':\n        cat_cols.append(col)\n        le = LabelEncoder()\n        le.fit(list(train[col].astype(str).values) + list(test[col].astype(str).values))\n        train[col] = le.transform(list(train[col].astype(str).values))\n        test[col] = le.transform(list(test[col].astype(str).values))   ","f9b15cd7":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\ntest = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)","5b651551":"del train\ngc.collect()","ed35e70f":"# by https:\/\/www.kaggle.com\/dimartinot\ndef clean_inf_nan(df):\n    return df.replace([np.inf, -np.inf], np.nan)   \n\n# Cleaning infinite values to NaN\nX = clean_inf_nan(X)\ntest = clean_inf_nan(test )","9b054d38":"X.fillna(0, inplace=True)\ntest.fillna(0, inplace=True)","4d832889":"print(X.shape, test.shape)","66484e2a":"%%time\n\nacc, auc, F1 = [], [], []\n\nY_TEST_preds = pd.DataFrame({'ind': list(test.index), \n                         'prediction': [0.0] * len(test)}) \n\nX_t = X.sample(frac = 1., random_state = RANDOM_STATE)\ny_t = y.loc[X_t.index]\n\n# The commented code below you can use to perform the catboost parameters tuning \n# STATIC_PARAMS = {'eval_metric': 'AUC',\n#                  'random_seed': RANDOM_STATE,\n#                  'scale_pos_weight': y_t.value_counts()[0]\/y_t.value_counts()[1],\n#                  'thread_count': -1,\n#                   'task_type' : \"GPU\",\n#                   'devices' : '0',\n#                 }       \n\n# space_SKopt = [\n#          Real(0, 10, name='l2_leaf_reg'),  \n#          Real(0, 10, name='bagging_temperature'),\n#          Integer(1, 16, name='depth'),\n#          Real(0.001, .5, name='learning_rate'),\n\n\n# X_train, X_test, y_train, y_test = train_test_split(X_t, y_t, test_size=0.25, random_state=42, shuffle = True)\n\n# eval_dataset = Pool(data=X_test,\n#                 label=y_test,\n#                 cat_features=cat_cols)\n\n# n_fold = 3\n# cv_tune = StratifiedKFold(n_splits=n_fold, random_state=RANDOM_STATE, shuffle=True)\n\n# start_time = time()\n# [TUNED_PARAMS,reg_gp] = get_params_SKopt(CatBoostClassifier(**STATIC_PARAMS, \n#                                                          iterations = 500,\n#                                                          verbose = False,     \n#                                                          gpu_cat_features_storage = 'CpuPinnedMemory',\n#                                                          border_count = 10,  # 128\n#                                                          max_ctr_complexity = 1, # 4\n#                                                          cat_features=cat_cols,), \n#                                                          X_train, y_train, \n#                                                          space_SKopt, \n#                                                          cv_tune,\n#                                                          alg = 'catboost',\n#                                                          cat_features = cat_cols,\n#                                                          eval_dataset = eval_dataset,\n#                                                          UBM = True,\n#                                                          opt_method = 'forest_minimize',\n#                                                          verbose = True,\n#                                                          multi = False, \n#                                                          scoring = 'roc_auc',\n#                                                          n_best = 10,\n#                                                          total_time = 15000)\n\n# print('\\nTime for tuning: {0:.2f} minutes'.format((time() - start_time)\/60))\n# NEW_PARAMS = {**STATIC_PARAMS, **TUNED_PARAMS}\n# best_model = CatBoostClassifier(**NEW_PARAMS)    \n# best_model.set_params(iterations = 10000,  learning_rate = TUNED_PARAMS['learning_rate']\/10)\n# print(best_model.get_params())\n\nPARAMS = {'learning_rate': 0.03805602919544498,\n          'depth': 4,\n          'l2_leaf_reg': 2.2556764893424943,\n          'random_seed': 12061985,\n          'eval_metric': 'AUC',\n          'bagging_temperature': 0.22312590985798633,\n#           'task_type': 'GPU', \n#           'devices': '0',\n          'scale_pos_weight': 27.579586700866283,\n          'iterations': 10000}\n\nbest_model = CatBoostClassifier(**PARAMS)  \n\nn_fold = 3\ncv = StratifiedKFold(n_splits=n_fold, random_state=RANDOM_STATE, shuffle=True)\n\nfor fold_n, (train_index, valid_index) in enumerate(cv.split(X_t, y_t)):\n    print('\\nFold', fold_n, 'started at', ctime())\n\n    X_train = X_t.iloc[train_index,:]\n    X_valid = X_t.iloc[valid_index,:]\n\n    y_train = y_t.iloc[train_index]\n    y_valid = y_t.iloc[valid_index]      \n    \n    train_dataset = Pool(data=X_train,\n                     label=y_train,\n                     cat_features=cat_cols)\n    \n    eval_dataset = Pool(data=X_valid,\n                    label=y_valid,\n                    cat_features=cat_cols)\n    \n    best_model.fit(train_dataset,\n              use_best_model=True,\n              verbose = False,\n              plot = True,\n              eval_set=eval_dataset,\n              early_stopping_rounds=50)\n    \n    y_pred = best_model.predict(Pool(data=X_valid, cat_features=cat_cols))\n\n    acc.append(metrics.accuracy_score(y_valid, y_pred))\n    auc.append(metrics.roc_auc_score(y_valid, y_pred))\n    F1.append(metrics.f1_score(y_valid, y_pred))\n\n    print('Best score', best_model.best_score_) \n    print('Best iteration', best_model.best_iteration_)  \n\n    Y_TEST_preds.loc[:, 'prediction'] += best_model.predict_proba(Pool(data=test, cat_features=cat_cols))[:,1]\n\nY_TEST_preds.loc[:, 'prediction'] \/= n_fold            \n\nprint('='*45)           \nprint('CV mean accuarcy: {0:.4f}, std: {1:.4f}.'.format(np.mean(acc), np.std(acc)))\nprint('CV mean AUC: {0:.4f}, std: {1:.4f}.'.format(np.mean(auc), np.std(auc)))\nprint('CV mean F1: {0:.4f}, std: {1:.4f}.'.format(np.mean(F1), np.std(F1)))","f8a98929":"submission['isFraud'] = Y_TEST_preds['prediction'] \nsubmission.to_csv('cat_skopt.csv', index=False)\nprint(sub.head())","ece30a32":"## References\n1. https:\/\/www.kaggle.com\/artgor\/eda-and-models\n2. https:\/\/www.kaggle.com\/nroman\/lgb-single-model-lb-0-9419","a5735fe1":"## Data loading ","2de91834":"## CatBoost","4567dc8e":"## Feature engineering"}}