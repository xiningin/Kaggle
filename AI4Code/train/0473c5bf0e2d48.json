{"cell_type":{"eeff9224":"code","ddf633b8":"code","c03e554e":"code","3f2a78b8":"code","4bbad607":"code","55397236":"code","8222bd2d":"code","6d2badbc":"code","726fc06e":"code","68b85159":"code","0004f8f1":"code","87ee5d25":"code","c7866c32":"code","6a858e1f":"code","7c66b38c":"code","65e05261":"code","960e4783":"code","09bb1153":"code","0a6c9319":"code","91432334":"code","d7c699ca":"code","832ae0ca":"code","f39ea630":"code","8a2603e1":"code","a1c18065":"code","61174f0d":"code","8c014150":"code","998a20d5":"code","8edc2828":"code","59aaadf1":"code","0e3c98d0":"code","955b02e5":"markdown","27ea70d4":"markdown","cf9e56b4":"markdown","ec29bead":"markdown","75758103":"markdown","98468fe1":"markdown","0cd24452":"markdown","2553f972":"markdown","281a6446":"markdown","17e90023":"markdown","f5fa3bd1":"markdown","602bc597":"markdown","54ce6f72":"markdown","3716ae66":"markdown","ecb5d0b0":"markdown","99ebe419":"markdown","2a96328d":"markdown","9a65df33":"markdown","d30a484c":"markdown","9eb687c8":"markdown"},"source":{"eeff9224":"!pip install vaderSentiment\n!pip install afinn","ddf633b8":"import numpy as np \nimport pandas as pd\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom string import punctuation\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.treebank import TreebankWordDetokenizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom textblob import TextBlob\nfrom afinn import Afinn\nfrom nltk.sentiment import SentimentAnalyzer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom gensim.test.utils import common_texts\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nimport warnings\nwarnings.simplefilter(\"ignore\")","c03e554e":"tweets_df = pd.read_csv('..\/input\/kenyan-political-tweets\/kenya_political_tweets.csv')","3f2a78b8":"def cleaning_data(data):\n    data['modi']=data['text'].str.lower()\n    i=0\n    for x in data['modi']:\n        for y in x:\n            if y in punctuation:\n                data['modi'][i]=data['modi'][i].replace(y,'')\n        data['modi'][i]=data['modi'][i].replace('br','')\n        i+=1\n    data = pd.concat([data.drop(['modi'], axis=1), data['modi'].apply(pd.Series)], axis=1)","4bbad607":"def preprocess_text(text):\n    tokens = word_tokenize(text)\n    stop_words = set(stopwords.words('english'))\n    tokens = [w for w in tokens if not w in stop_words]\n    porter = PorterStemmer()\n    stems = []\n    for t in tokens:    \n        stems.append(porter.stem(t))\n    return stems","55397236":"cleaning_data(tweets_df)\ntweets_df['modi']= tweets_df['modi'].apply(lambda x : preprocess_text(x))\ntweets_df['modi']=tweets_df['modi'].apply(lambda x: TreebankWordDetokenizer().detokenize(x))","8222bd2d":"def find_sentiment_polarity_textblob(text):\n    blob = TextBlob(text)\n    polarity = 0\n    for sentence in blob.sentences:\n        polarity += sentence.sentiment.polarity\n    return polarity\n\ndef find_sentiment_subjectivity_textblob(text):\n    blob = TextBlob(text)\n    subjectivity = 0\n    for sentence in blob.sentences:\n        subjectivity += sentence.sentiment.subjectivity\n    return subjectivity","6d2badbc":"def sentiment_scores(data):\n    analyser = SentimentIntensityAnalyzer()\n    afin = Afinn(emoticons=True)\n    data['vader']=data['text'].apply(lambda x : analyser.polarity_scores(x))\n    data['afinn']=data['text'].apply(lambda x : afin.score(x))\n    data = pd.concat([data.drop(['vader'], axis=1), data['vader'].apply(pd.Series)], axis=1)\n    data[\"tb_pol\"] = data[\"text\"].apply(lambda x: find_sentiment_polarity_textblob(x))\n    data[\"tb_sub\"] = data[\"text\"].apply(lambda x: find_sentiment_subjectivity_textblob(x))\n    data['blob']=data['text'].apply(lambda x : TextBlob(x).sentiment)\n    data = pd.concat([data.drop(['blob'], axis=1), data['blob'].apply(pd.Series)], axis=1)\n    data['lenght'] = data['text'].apply(lambda x : len(x))\n    data['words'] = data['text'].apply(lambda x :len(x.split(\" \")))\n    return data","726fc06e":"def doc2vec(data):\n    documents = [TaggedDocument(doc, [i]) for i, doc in enumerate(data[\"modi\"].apply(lambda x: x.split(\" \")))]\n    # train a Doc2Vec model with our text data\n    model = Doc2Vec(documents, workers=4,vector_size=50,dm=0,min_count = 1)\n\n    # transform each document into a vector data\n    doc2vec_df = data[\"modi\"].apply(lambda x: model.infer_vector(x.split(\" \"))).apply(pd.Series)\n    doc2vec_df.columns = [\"d2v_\" + str(x) for x in doc2vec_df.columns]\n    data = pd.concat([data, doc2vec_df], axis=1)\n     \n    return data","68b85159":"def make_features(data):\n    data = sentiment_scores(data)\n    data = doc2vec(data)\n    \n    tfidf = TfidfVectorizer(min_df=10,max_df=0.6,ngram_range=(1,3))\n    tfidf.fit(data[\"modi\"])\n    tfidf_result = tfidf.transform(data[\"modi\"]).toarray()\n    tfidf_df = pd.DataFrame(tfidf_result, columns = tfidf.get_feature_names())\n    tfidf_df.columns = [\"word_\" + str(x) for x in tfidf_df.columns]\n    tfidf_df.index = data.index\n    data = pd.concat([data, tfidf_df], axis=1)\n\n    data.fillna(0,inplace=True)\n    return data","0004f8f1":"x = make_features(tweets_df)","87ee5d25":"selected_columns = ['text', 'modi', 'afinn', 'neg', 'neu', 'pos', 'compound', 'tb_pol', 'tb_sub', 'lenght', 'words']\nx[selected_columns].head()","c7866c32":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns ","6a858e1f":"selected = [\"afinn\", \"compound\",\"neg\", \"neu\", \"pos\",\"tb_pol\", \"tb_sub\"]\nsns.pairplot(x[selected].sample(500), diag_kind=\"kde\", corner=True)","7c66b38c":"def label_based_on_compound_value(compound):\n    if compound > 0:\n        return 1 # positive\n    else:\n        return 0 # negative","65e05261":"x['label'] = x['compound'].apply(lambda x: label_based_on_compound_value(x))","960e4783":"ignore_columns = ['id', 'user_name', 'user_location', 'user_description', 'user_created', 'user_followers', 'user_friends', \n                  'user_favourites', 'user_verified', 'date', 'text', 'hashtags', 'source', 'retweets', 'favorites', 'is_retweet', \n                  'modi', 'afinn', 'neg', 'neu', 'pos', 'compound', 'tb_pol', 'tb_sub', 0, 1, 'lenght', 'words', 'label']","09bb1153":"from sklearn.model_selection import train_test_split\nfeatures = [c for c in list(x.columns) if c not in ignore_columns]\ntrain_labels = x['label']\nX_train, X_test, y_train, y_test = train_test_split(x[features], train_labels, test_size=0.1, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)","0a6c9319":"print(f\"train: {X_train.shape}, test: {X_test.shape}, valid: {X_val.shape}\")","91432334":"from keras.layers import Dense\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Activation\nfrom keras.models import Sequential\nfrom keras.layers import Dropout\nfrom keras.layers import Bidirectional\nfrom keras.callbacks import ModelCheckpoint, Callback, EarlyStopping, ReduceLROnPlateau, LearningRateScheduler\nfrom keras import optimizers\nimport keras\nfrom sklearn import metrics","d7c699ca":"model = Sequential([\n    Dense(32, input_shape=(x[features].shape[1],)),\n    Activation('relu'),\n    Dropout(0.85),\n    Dense(16),\n    Activation('relu'),\n    Dropout(0.55),\n    Dense(1),\n    Activation('sigmoid'),\n])","832ae0ca":"model.summary()","f39ea630":"model.compile(optimizer=keras.optimizers.Adam(), loss='binary_crossentropy', metrics=['accuracy'])","8a2603e1":"no_epoch = 50\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nan = LearningRateScheduler(lambda x: 1e-3 * 0.991 ** (x+no_epoch))\nck = ModelCheckpoint('best_model.h5',\n                                monitor='val_loss',\n                                verbose=0,\n                                save_best_only=True,\n                                save_weights_only=True)\n","a1c18065":"model_history = model.fit(X_train,y_train,validation_data=(X_val, y_val), epochs=no_epoch, batch_size=64,verbose = 1, callbacks = [es, an, ck])","61174f0d":"print(\"Test:\", model.evaluate(X_test,y_test))\nprint(\"Valid:\", model.evaluate(X_val,y_val))\nprint(\"Train:\", model.evaluate(X_train,y_train))","8c014150":"import plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools, subplots\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ndef create_trace(x,y,ylabel,color):\n        trace = go.Scatter(\n            x = x,y = y,\n            name=ylabel,\n            marker=dict(color=color),\n            mode = \"markers+lines\",\n            text=x\n        )\n        return trace\n    \ndef plot_accuracy_and_loss(train_model):\n    hist = train_model.history\n    acc = hist['accuracy']\n    val_acc = hist['val_accuracy']\n    loss = hist['loss']\n    val_loss = hist['val_loss']\n    epochs = list(range(1,len(acc)+1))\n    #define the traces\n    trace_ta = create_trace(epochs,acc,\"Training accuracy\", \"Green\")\n    trace_va = create_trace(epochs,val_acc,\"Validation accuracy\", \"Red\")\n    trace_tl = create_trace(epochs,loss,\"Training loss\", \"Blue\")\n    trace_vl = create_trace(epochs,val_loss,\"Validation loss\", \"Magenta\")\n    fig = subplots.make_subplots(rows=1,cols=2, subplot_titles=('Training and validation accuracy',\n                                                             'Training and validation loss'))\n    #add traces to the figure\n    fig.append_trace(trace_ta,1,1)\n    fig.append_trace(trace_va,1,1)\n    fig.append_trace(trace_tl,1,2)\n    fig.append_trace(trace_vl,1,2)\n    #set the layout for the figure\n    fig['layout']['xaxis'].update(title = 'Epoch')\n    fig['layout']['xaxis2'].update(title = 'Epoch')\n    fig['layout']['yaxis'].update(title = 'Accuracy', range=[0,1])\n    fig['layout']['yaxis2'].update(title = 'Loss', range=[0,1])\n    #plot\n    iplot(fig, filename='accuracy-loss')\n\nplot_accuracy_and_loss(model_history)","998a20d5":"def test_accuracy_report(model, X, y, title=\"\"):\n    print(title)\n    predicted = model.predict(X)\n    predicted = np.round(predicted)\n    truth = y.values\n    print(metrics.classification_report(truth, predicted, target_names=[\"Negative\", \"Positive\"])) \n    res = model.evaluate(X, y.values, verbose=0)\n    print('Loss function: %s, accuracy:' % res[0], res[1])","8edc2828":"test_accuracy_report(model, X_test, y_test, \"Test set (how well model generalize for fresh data)\")","59aaadf1":"test_accuracy_report(model, X_val, y_val, \"Validation set (how well model perform with validation data)\")","0e3c98d0":"test_accuracy_report(model, X_train, y_train, \"Train set (how well model learns its own data)\")","955b02e5":"## <a id=\"3\">Sentiment analysis - a data exploration<\/a>","27ea70d4":"Let's look to the original text, the pre-processed text (`modi`),  extracted sentiment analysis features and few more features extracted (length of initial text, number of words).","cf9e56b4":"## <a id=\"1\">Analysis preparation<\/a>\n\nWe install some missing packages:\n* vaderSentiment\n* afinn","ec29bead":"Following, we load the data.","75758103":"Next, we introduce few useful functions for preparing the additional features, as following:\n* data cleaning; the data filtered is not presented to the sentiment analysis extraction (we need the full context, including punctuation, for the sentiment analysis tasks).\n* text preprocessing - further text transformation: tokenization, eliminate stop words and stemming (same as before, the transformed text will not be presented to the sentiment analysis task;\n* further text transformation, using TreebankWordDetokenizer.","98468fe1":"<h1>Kenya Political Tweets<\/h1>\n<h2> Sentiment analysis <\/h2>\n\n<a id=\"0\"><\/a>\n\n### Content  \n\n* <a href='#1'>Analysis preparation<\/a>  \n* <a href='#2'>Feature engineering<\/a>  \n* <a href='#3'>Sentiment analysis - a data exploration<\/a>  \n* <a href='#4'>Use sentiment polarity features for labeling<\/a>    \n* <a href='#5'>Train a supervised model and validate it<\/a>    \n* <a href='#6'>Conclusions and future work<\/a>  \n\n\n\n","0cd24452":"We import the libraries for the model.","2553f972":"Apply text cleaning (filtering), text preprocessing and further text transformation.","281a6446":"## <a id=\"4\">Use sentiment polarity features for labeling<\/a>\n\n\nWe will use `compound` feature for labeling, as following:\n* if the feature is smaller than 0, we label as `negative` (0) the tweet;\n* if the feature is larger than 0, we label as `positive` (1) the tweet.","17e90023":"We create a train, validation and test set. We select 10% of the data for test set. From the remaining 90%, 90% (or 81% from initial dataset) we use as train and the rest of 10% as validation data.","f5fa3bd1":"Let's verify accuracy of inference using the trained model for: train, validation and test data.","602bc597":"We prepare now to fit the model. We will use as well validation data during model training. We train for 100 epochs.","54ce6f72":"## <a id=\"5\">Train a supervised model and validate it<\/a>\n\nWe select the columns to include into the model.","3716ae66":"## <a id=\"6\">Conclusions and future work<\/a>\n\n\n### Conclusions\n\nWe used sentiment analysis libraries to calculate negtive, positive, neutral, respective polarity and subjectivity of sentiment. We used the compound factor from {postive, negative, neutral} sentiment score to label data.\nWe split available data in train, validation and test data.\nWe used train and validation data to train and validate a model.\n\nThen we checked the accuracy of prediction on test data (not used during training or validation) with the trained model.\nWe obtained a **84%** accuracy of the sentiment polarity score prediction.\n\n### Future work\n\nWe will manually label data for better performance of the training. We will train a model with the manual labeled data and apply the model to predict sentiment score for the new data, not used in the train. We will analyze next the correlation between the sentiment score obtained using pre-trained models from libraries with the sentiment obtained by applying the model.\nThe model itself can be further much improve by using the following techniques:\n- Use a DL model with word embeddings Bidirectional LSTM layers;\n- Use a transformed type model;","ecb5d0b0":"### Sentiment analysis\n\nFirst, we define the functions for the sentiment analysis:\n- using TextBlob, and obtain sentiment polarity and sentiment subjectivity;\n- using vader by nltk SentimentIntensityAnalyzer;\n- using Affin.\n\nThen we calculate doc2vec transformation using gensim Doc2Vec and also calculate TF-IDF on the modified form of the text data (text that was subjcted to text cleaning and pre-processing).\n","99ebe419":"We then include the packages used for the analysis.","2a96328d":"Let's glimpse the model structure.","9a65df33":"## <a id=\"2\">Feature engineering<\/a>\n\n\n### Text cleaning and preprocessing","d30a484c":"### End to end feature engineering\n\nWe apply in the make_features function (see below) the transformations (to calculate sentiment scores and to form the vectorized features from modified text data).","9eb687c8":"We define the model. Will use (for this baseline) few Dense layers and few Dropouts  layers (to reduce overfitting)."}}