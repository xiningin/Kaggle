{"cell_type":{"e739f937":"code","289fc3c4":"code","539cd95d":"code","8721c0d8":"code","73fb7ee5":"code","1a3b4664":"code","d06d1ab9":"code","cc5a45ae":"code","629c7692":"code","2ad9a2e5":"code","c241db93":"code","d8654557":"code","858495f3":"code","f88f84ac":"code","e8abb1e7":"code","207e62ae":"code","cfe42ee0":"code","ca00c05d":"code","1bed93db":"markdown"},"source":{"e739f937":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","289fc3c4":"# Importing necessary libraries\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","539cd95d":"# Checking the training data\nfile_train = \"\/kaggle\/input\/sign-language-mnist\/sign_mnist_train.csv\"\ndata_train = pd.read_csv(file_train)\n# data length\nprint(data_train.shape)\n# data.describe\n#print(data_train.describe())\n","8721c0d8":"# Checking the test data\nfile_test = \"\/kaggle\/input\/sign-language-mnist\/sign_mnist_test.csv\"\ndata_test = pd.read_csv(file_test)\n#data shape\nprint(data_test.shape)","73fb7ee5":"# Preprocess Function\n  # Convert the labels into categorical forms i.e telling when a particular label has occured or not\n  # Selecting the image pixel values and normalizing them and reshapping them\n  # Returning the labels , images\ndef preprocess(data):\n    num_classes = 25\n    num_images = data.shape[0]\n    target = tf.keras.utils.to_categorical(data.label,num_classes)\n    images_p = (data.values[:,1:])\n    images_p1 = images_p.reshape(num_images,28,28,1)\n    return images_p1,target\n","1a3b4664":"train_X,train_y = preprocess(data_train)\ntest_X,test_y   = preprocess(data_test)","d06d1ab9":"plt.figure(figsize = (7,7))\nrand = random.randrange(0,data_train.shape[0])\nplt.imshow(train_X[rand][:,:,-1],cmap=\"gray\")\nplt.show()","cc5a45ae":"plt.figure(figsize=(7,7))\nrand = random.randrange(0,test_X.shape[0])\nplt.imshow(test_X[rand][:,:,0],cmap=\"inferno\")\nplt.show()","629c7692":"# Building a model  using Convolutional layers whch will use features to find features \n# Max Pooling layers to compress the image by making sure that the features remain intact\n# Dropout -- to reduce overfitting by randomly making a couple weight values as zero\nnum_classes = 25\nmodel_classifier = tf.keras.models.Sequential([\n                   tf.keras.layers.Conv2D(64,kernel_size=(3,3),activation=\"relu\",input_shape=(28,28,1)),\n                   tf.keras.layers.MaxPooling2D(2,2),\n                   tf.keras.layers.Conv2D(32,kernel_size=(3,3),activation=\"relu\"),\n                   tf.keras.layers.MaxPooling2D(2,2),\n                   tf.keras.layers.Conv2D(32,kernel_size=(3,3),activation=\"relu\"),\n                   tf.keras.layers.MaxPooling2D(2,2),\n                   tf.keras.layers.Dropout(0.2),\n                   tf.keras.layers.Flatten(),\n                   tf.keras.layers.Dense(512,activation=\"relu\"),\n                   tf.keras.layers.Dense(num_classes,activation=\"softmax\")\n    \n])\n# The journey of the image through the model for classification purpose\nmodel_classifier.summary()","2ad9a2e5":"model_classifier.compile(optimizer = tf.keras.optimizers.RMSprop(lr=0.001),loss = \"categorical_crossentropy\" ,\n                         metrics =['acc'])\nhistory1 = model_classifier.fit(train_X,train_y,epochs=10, steps_per_epoch = train_X.shape[0]\/16, validation_split=0.3)\nmodel_classifier.evaluate(test_X,test_y)","c241db93":"model_classifier.compile(optimizer=\"adam\" , loss = \"categorical_crossentropy\" , metrics = ['acc'])\nhistory2 = model_classifier.fit(train_X,train_y,epochs = 10,validation_split=0.2)\nmodel_classifier.evaluate(test_X,test_y)","d8654557":"# the training and testing accuracy and respective losses\nplt.figure()\nacc      = history1.history['acc' ]\nval_acc  = history1.history['val_acc']\nloss     = history1.history['loss']\nval_loss = history1.history['val_loss']\nsns.lineplot(x = [i for i in range(1,len(acc)+1)],y = acc)\nsns.lineplot(x = [i for i in range(1,len(val_acc)+1)],y=val_acc)\n\n","858495f3":"plt.figure()\nplt.plot(history1.history['loss'])\nplt.plot(history1.history['val_loss'])","f88f84ac":"# the testing accuracy , val_acc , loss, val_loss\nplt.figure()\nacc = history2.history['acc']\nval_acc = history2.history['val_acc']\nplt.plot(acc)\nplt.plot(val_acc)\n","e8abb1e7":"plt.figure()\nplt.plot(history2.history['loss'])\nplt.plot(history2.history['val_loss'])","207e62ae":"# Using Image Data Generator\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\n\ntrain_gen = ImageDataGenerator(rescale = 1.\/255,\n                              rotation_range=40,\n                              shear_range=0.2,\n                              zoom_range=0.2,\n                              horizontal_flip=True,\n                              fill_mode=\"nearest\")\n\nval_gen = ImageDataGenerator(rescale=1.\/255)","cfe42ee0":"history3 = model_classifier.fit(train_gen.flow(train_X,train_y),\n            epochs = 10,\n           validation_data=val_gen.flow(test_X,test_y))","ca00c05d":"plt.figure()\nplt.plot(history3.history['acc'])\nplt.plot(history3.history['val_acc'])\nplt.figure()\nplt.plot(history3.history['loss'])\nplt.plot(history3.history['val_loss'])","1bed93db":"Compile Method will help the model have its own optimizing process , a loss function to tell how much wrong the model is about its prediction and the actual answer and metrics is the measure of accuracy.\n\nFit will help the model to find patterns and make rules when given the training labels and images. Here rules are formed by the values of weights and biases\n\nEvaluate function is a test for the model on how correct the rules are when it sees unknown data(testing labels, testing images)"}}