{"cell_type":{"abdf7881":"code","daecf44e":"code","e34d0121":"code","b2519f88":"code","d5089bd9":"code","2b4c20a6":"code","8b9605cc":"code","0c912b44":"code","1001bb4b":"code","c1c06221":"code","ccf81985":"code","01ae7c38":"code","13a677d9":"code","92adff59":"code","ff6eeea7":"code","ad43efb6":"code","f8f62792":"code","a394fe7e":"code","038fdf01":"code","1d3f4357":"code","403df764":"code","929d45f2":"code","c16187d5":"code","d626e051":"code","7e4e715e":"code","cbb45e81":"code","21b22ca9":"code","294fd4c0":"code","cb39dd7b":"code","2f9b7aa6":"code","212bbc70":"code","fe8354ce":"code","add8e734":"code","296e764a":"code","a392cfb5":"code","0fecb25f":"code","829fc4f9":"code","81be3313":"code","b7558d5e":"code","1a3bc918":"code","44e99ccb":"code","fc8b3f88":"code","7da0d670":"code","81de307d":"code","bb03d3e5":"code","5f64b35b":"code","e2230dd4":"code","ba85a103":"code","bed27064":"code","c1b164d7":"code","89031a02":"code","d8f4ae76":"code","ab90e896":"code","23ba954c":"code","2bc71c42":"code","19ef6127":"code","18fde761":"code","c8725e6f":"code","f6b0fe32":"code","05f27927":"code","c86a706d":"code","ab7f5d11":"code","39fed0fe":"code","8a24b24c":"code","d4442352":"code","8ceec337":"markdown","f35b2ee1":"markdown"},"source":{"abdf7881":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport copy\nfrom sklearn.preprocessing import LabelEncoder\nimport seaborn\nimport plotly.express as px\nimport os\nfrom sklearn import metrics\nfrom sklearn.metrics import recall_score\nfrom sklearn.ensemble import RandomForestClassifier\n","daecf44e":"original_data=pd.read_csv('..\/input\/credit-card-customers\/BankChurners.csv')\ndf=copy.deepcopy(original_data)","e34d0121":"# delete CLIENTUNM and Naive-- \ndf=df.iloc[:,:-2]\ndf=df.iloc[:,1:]","b2519f88":"df.head()","d5089bd9":"# Statistic on numerical data\ndf.describe(exclude = ['object'])","2b4c20a6":"# Statistic on categorical data\ndf.describe(exclude = ['float', 'int64'])","8b9605cc":"df.shape","0c912b44":"df.isnull().sum()","1001bb4b":"df['Attrition_Flag'].value_counts()","c1c06221":"df['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer':0, 'Attrited Customer':1})\n","ccf81985":"## Lets analyse the continuous values by creating histograms to understand the distribution\ncontinuous_feature = [feature for feature in df.columns]\nColors=['#636EFA']\nfor feature in continuous_feature:\n    fig = px.histogram(df, x= feature,title= 'Distribution of '+feature,color_discrete_sequence= Colors)\n    fig.show()","01ae7c38":"## Compare two distribution by Attrition_Flag\n\nfor feature in continuous_feature:\n    fig = px.histogram(df, x= feature,title= 'Distribution of '+feature,color=\"Attrition_Flag\",pattern_shape=\"Attrition_Flag\",\n                   template='plotly_white', barmode='group',color_discrete_sequence={\n                0: \"#f0f87f\",\n                1: \"#4696f9\"})\n    fig.show()\n","13a677d9":"# Encode binary variables\ndf['Gender'] = df['Gender'].map({'M':1, 'F':0})\n","92adff59":"# Finding all the categorical columns from the data\ncategorical_columns = df.select_dtypes(exclude=['int64','float64']).columns\nnumerical_columns = df.select_dtypes(include=['int64','float64']).columns\ncategorical_columns","ff6eeea7":"# One hot encoding for logistic regression\ndef encode_and_bind(original_dataframe, feature_to_encode):\n    dummies = pd.get_dummies(original_dataframe[[feature_to_encode]])\n    res = pd.concat([original_dataframe, dummies], axis=1)\n    res = res.drop([feature_to_encode], axis=1)\n    return(res) \n","ad43efb6":"for feature in categorical_columns:\n    df = encode_and_bind(df, feature)\n\ndf.head()","f8f62792":"X=df.drop('Attrition_Flag', axis=1)\ny=df[['Attrition_Flag']]","a394fe7e":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","038fdf01":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import StandardScaler\n\nclf1=LogisticRegression(C=100, max_iter=50000) # C : L_2 inverse regularization weight\nclf1.fit(x_train, np.ravel(y_train))\n\n# Predicting the test set\ny_pred = clf1.predict(x_test)\nrec=recall_score(np.ravel(y_test), y_pred)\n\nprint('Logistic Regression:')\nprint('Traning Model accruracy: {:.2%}'.format(clf1.score(x_train,np.ravel(y_train))))\nprint('Test Model accruracy: {:.2%}'.format(clf1.score(x_test,np.ravel(y_test))))\nprint('Recall for test data :',rec)","1d3f4357":"# Label encoding for tree-based method\nfrom sklearn.preprocessing import LabelEncoder","403df764":"labelencoder = LabelEncoder()","929d45f2":"df=copy.deepcopy(original_data)","c16187d5":"# delete CLIENTUNM and Naive-- \ndf=df.iloc[:,:-2]\ndf=df.iloc[:,1:]\ndf['Attrition_Flag'] = df['Attrition_Flag'].map({'Existing Customer':0, 'Attrited Customer':1})","d626e051":"# Finding all the categorical columns from the data\ncategorical_columns = df.select_dtypes(exclude=['int64','float64']).columns\n\ncategorical_columns","7e4e715e":"for col in categorical_columns:\n    df[col]=labelencoder.fit_transform(df[col])\ndf","cbb45e81":"X=df.drop(\"Attrition_Flag\",axis=1)\ny=df[[\"Attrition_Flag\"]]","21b22ca9":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","294fd4c0":"# XGB\nfrom xgboost import XGBClassifier\n\nclassifier = XGBClassifier(random_state=42, use_label_encoder=False)\nclassifier.fit(x_train,np.ravel(y_train))\n# XGB\n# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\nprint('XGBoost:')\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('accuracy:',accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint('AUC:',auc)\n\nrec=recall_score(y_test, y_pred)\nprint('recall:',rec)","cb39dd7b":"#RF\nrf_clf = RandomForestClassifier(random_state=1234)\nrf_clf.fit(x_train, np.ravel(y_train))\ny_pred = rf_clf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nprint('Random Forest:')\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint('accuracy:',accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint('AUC:',auc)\n\nrec=recall_score(y_test, y_pred)\nprint('recall:',rec)\n\nconfusion_matrix(y_test, y_pred)","2f9b7aa6":"# RF for feature importance\n\nrf_clf = RandomForestClassifier(random_state=1234)\nrf_clf.fit(X, np.ravel(y))","212bbc70":"rf_clf.feature_importances_","fe8354ce":"feature_names=list(X.columns)","add8e734":"features_to_plot = 19\n\nimportances = rf_clf.feature_importances_\nindices = np.argsort(importances)\n\nbest_vars = np.array(feature_names)[indices][-features_to_plot:]\nvalues = importances[indices][-features_to_plot:]\nbest_vars","296e764a":"y_ticks = np.arange(0, features_to_plot)\nfig, ax = plt.subplots()\nax.barh(y_ticks, values)\nax.set_yticks(y_ticks)\nax.set_yticklabels(best_vars)\nax.set_title(\"Random Forest Feature Importances\")\nfig.tight_layout()\nplt.show()\nfig.savefig('fig1.png', dpi=300)","a392cfb5":"# Delete Gender, Card_Category features\nvari = best_vars[-17:]\nX=X[vari]","0fecb25f":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=42)","829fc4f9":"# XGB\nfrom xgboost import XGBClassifier\n\nclassifier = XGBClassifier(random_state=42, use_label_encoder=False)\nclassifier.fit(x_train,np.ravel(y_train))","81be3313":"# XGB\n# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Making the confusion matrix and calculating accuracy score\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(y_test, y_pred)\nprint(rec)","b7558d5e":"#RF\nrf_clf = RandomForestClassifier(random_state=1234)\nrf_clf.fit(x_train, np.ravel(y_train))\ny_pred = rf_clf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\naccuracy = metrics.accuracy_score(np.ravel(y_test), y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(np.ravel(y_test), y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(np.ravel(y_test), y_pred)\nprint(rec)\n\nconfusion_matrix(y_test, y_pred)","1a3bc918":"from sklearn.decomposition import PCA\npca = PCA(n_components=2) # Two principal components\nprintcipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data=printcipalComponents)\n# Make principal components DataFrame","44e99ccb":"principalDf.head()","fc8b3f88":"sum(pca.explained_variance_ratio_)","7da0d670":"X_new= copy.deepcopy(X)","81de307d":"# Add principal components to Data\nX_new['PC1']=principalDf.iloc[:,0]\n\nX_new['PC2']=principalDf.iloc[:,1]","bb03d3e5":"# Splitting the dataset into training set and test set\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(X_new, y, test_size = 0.3, random_state=42)","5f64b35b":"from xgboost import XGBClassifier\n\nclassifier = XGBClassifier(random_state=42, use_label_encoder=False)\nclassifier.fit(x_train,np.ravel(y_train))\n\n# XGB\n# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Result\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(y_test, y_pred)\nprint(rec)","e2230dd4":"#RF_PCA\nrf_clf = RandomForestClassifier(random_state=1234)\nrf_clf.fit(x_train, np.ravel(y_train))\ny_pred = rf_clf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)\naccuracy = metrics.accuracy_score(np.ravel(y_test), y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(np.ravel(y_test), y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(np.ravel(y_test), y_pred)\nprint(rec)","ba85a103":"confusion_matrix(y_test, y_pred)","bed27064":"# Importing packages for SMOTE\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.over_sampling import BorderlineSMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nfrom imblearn.pipeline import Pipeline\n\nfrom collections import Counter","c1b164d7":"sm = SMOTE(sampling_strategy='auto', random_state=1234)\nx_sm, y_sm = sm.fit_resample(x_train, y_train)","89031a02":"print(Counter(np.ravel(y_train)))\nprint(Counter(np.ravel(y_sm)))","d8f4ae76":"over = BorderlineSMOTE(sampling_strategy=0.3)\nunder = RandomUnderSampler(sampling_strategy=0.6)\n\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\n\nx_sm_us, y_sm_us = pipeline.fit_resample(x_train, y_train)\n\nprint(Counter(np.ravel(y_train)))\nprint(Counter(np.ravel(y_sm_us)))","ab90e896":"#XGB\nfrom xgboost import XGBClassifier\n\nclassifier = XGBClassifier(random_state=42, use_label_encoder=False,n_estimators=1000)\nclassifier.fit(x_sm_us,np.ravel(y_sm_us))\n# Predicting the test set\ny_pred = classifier.predict(x_test)\n\n# Result\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(np.ravel(y_test), y_pred)\nprint(rec)","23ba954c":"#RF\nrf_clf = RandomForestClassifier(n_estimators=500,max_depth=12,random_state=1234)\nrf_clf.fit(x_sm_us, np.ravel(y_sm_us))\ny_pred = rf_clf.predict(x_test)\n\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test, y_pred)\naccuracy = metrics.accuracy_score(y_test, y_pred)\nprint(accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\nrec=recall_score(np.ravel(y_test), y_pred)\nprint(rec)","2bc71c42":"# Hyperparameter tuning by Bayesian optimization\n\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.model_selection import cross_val_score\n","19ef6127":"def XGB_cv(max_depth,learning_rate, n_estimators, gamma\n           ,min_child_weight, max_delta_step, subsample\n           ,colsample_bytree, silent=True, nthread=-1):\n    model = XGBClassifier(max_depth=int(max_depth),\n                              learning_rate=learning_rate,\n                              n_estimators=int(n_estimators),\n                              silent=True,\n                              nthread=nthread,\n                              gamma=gamma,\n                              min_child_weight=min_child_weight,\n                              max_delta_step=max_delta_step,\n                              subsample=subsample,\n                              colsample_bytree=colsample_bytree,\n                         use_label_encoder=False)\n    model  = model.fit(x_sm_us,np.ravel(y_sm_us))\n    y_pred = model.predict(x_test)\n    res=recall_score(y_test, y_pred)\n    return res","18fde761":"pbounds = {'max_depth': (1, 100),\n          'learning_rate': (0.01, 1),\n          'n_estimators': (500, 1000),\n          'gamma': (0.01, 1),\n          'min_child_weight': (1, 10),\n          'max_delta_step': (0, 1),\n          'subsample': (0.3, 0.9),\n          'colsample_bytree' :(0.5, 0.99)\n          }","c8725e6f":"import xgboost as xgb\n\nxgb.set_config(verbosity=0)","f6b0fe32":"optimizer = BayesianOptimization(f=XGB_cv, pbounds=pbounds, random_state=1)\noptimizer.maximize(init_points=3, n_iter=30)\n","05f27927":"fit_xgb = XGBClassifier(max_depth= int( optimizer.max['params']['max_depth'] ),\n                             learning_rate=optimizer.max['params']['learning_rate'],\n                             n_estimators=int(optimizer.max['params']['n_estimators']),\n                             gamma= optimizer.max['params']['gamma'],\n                             min_child_weight=optimizer.max['params']['min_child_weight'],\n                             max_delta_step=optimizer.max['params']['max_delta_step'],\n                             subsample=optimizer.max['params']['subsample'],\n                             colsample_bytree=optimizer.max['params']['colsample_bytree'],use_label_encoder=False)\n\nmodel  = fit_xgb.fit(x_sm_us,np.ravel(y_sm_us))\n# Predicting the test set\ny_pred = model.predict(x_test)\n\n# Final result for XGBoost\nprint('XGBoost:')\naccuracy = metrics.accuracy_score(np.ravel(y_test), y_pred)\nprint('accuracy:',accuracy)\n\nfpr, tpr, thresholds = metrics.roc_curve(np.ravel(y_test), y_pred)\nauc = metrics.auc(fpr, tpr)\nprint('AUC:',auc)\n\nrec=recall_score(y_test, y_pred)\nprint('recall:',rec)","c86a706d":"# Hyperparameter tuning by grid search on Random Forest\n\nx, test_res, train_res = [], [], []\n\nfor i in range(3,25):\n    clf = RandomForestClassifier(max_depth = i,random_state=1234)\n    clf.fit(x_sm_us, np.ravel(y_sm_us))\n    \n    train_res += [accuracy_score(np.ravel(y_sm_us), clf.predict(x_sm_us))]\n    test_res += [accuracy_score(np.ravel(y_test), clf.predict(x_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'accuracy on train data')\nplt.plot(x, test_res, label = 'accuracy on test data')\nplt.legend()","ab7f5d11":"x, test_res, train_res = [], [], []\n\nfor i in range(1,10):\n    clf = RandomForestClassifier(max_depth = 10,class_weight = {0 : i, 1: 10-i},random_state=1234)\n    clf.fit(x_sm_us, np.ravel(y_sm_us))\n    \n    train_res += [accuracy_score(np.ravel(y_sm_us), clf.predict(x_sm_us))]\n    test_res += [accuracy_score(np.ravel(y_test), clf.predict(x_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'accuracy on train data')\nplt.plot(x, test_res, label = 'accuracy on test data')\nplt.legend()","39fed0fe":"x, test_res, train_res = [], [], []\n\nfor i in range(1,10):\n    clf = RandomForestClassifier(max_depth = 10,class_weight = {0 : i, 1: 10-i},random_state=1234)\n    clf.fit(x_sm_us, np.ravel(y_sm_us))\n    \n    train_res += [recall_score(np.ravel(y_sm_us), clf.predict(x_sm_us))]\n    test_res += [recall_score(np.ravel(y_test), clf.predict(x_test))]\n    x += [i]\n    \n    \nplt.figure(figsize = (12, 4))\nplt.plot(x, train_res, label = 'recall on train data')\nplt.plot(x, test_res, label = 'recall on test data')\nplt.legend()","8a24b24c":"clf = RandomForestClassifier(max_depth = 10, class_weight = {0 : 1, 1: 9},random_state=1234)\nclf.fit(x_sm_us, np.ravel(y_sm_us))\ny_pred = clf.predict(x_test)\n\nfpr, tpr, thresholds = metrics.roc_curve(np.ravel(y_test), y_pred)\nauc = metrics.auc(fpr, tpr)\nprint('Random Forest:')\nprint('accuracy:', accuracy_score(y_test, y_pred))\nprint('AUC:',auc)\nprint('recall:', recall_score(y_test, y_pred))\n","d4442352":"confusion_matrix(y_test, y_pred)","8ceec337":"###### ","f35b2ee1":"I utilized codes on references and added data preprocessing and bayesian optimization.\n\nReferences\nhttps:\/\/www.kaggle.com\/andreshg\/churn-prediction-0-99-auc-h2o-sklearn-smote https:\/\/www.kaggle.com\/sonalisingh1411\/customer-churn-eda-top-5-models-comparion-95 https:\/\/www.kaggle.com\/olenasemyvolos\/99-recall-with-randomforest-graph-explanation\nhttps:\/\/www.kaggle.com\/stuartday274\/logistic-regression-and-decision-trees"}}