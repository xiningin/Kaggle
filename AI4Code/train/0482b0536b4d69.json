{"cell_type":{"52300d3a":"code","d5dabd65":"code","e022cb5e":"code","e20721c6":"code","577964b7":"code","6a306ff8":"code","2fa680b0":"code","7d93587a":"code","7f245da7":"code","88b7ac99":"code","2ee7c7de":"code","e2f5f15e":"code","8e1acdea":"code","95964879":"code","854e3935":"code","8fd31918":"code","b75034cc":"code","85bfc701":"code","eee6a29f":"code","c68c6129":"code","d41dcfbd":"code","90a24946":"code","938f485d":"code","649ae92c":"code","7e2be094":"code","5e78a1bd":"code","3ce1dcbf":"code","2c3eed72":"code","e2fbd90c":"code","6ee200de":"code","959edd85":"code","1d0f3785":"code","549e3e90":"code","924540c5":"code","0a3e1642":"code","71de5d78":"code","018046cc":"code","88c5ace8":"code","22036a78":"code","05925478":"code","c5633986":"code","8693f360":"code","a4218ca8":"code","34eb75cf":"code","ee4037cc":"code","f81a7e62":"code","75d2c697":"code","b189c839":"code","e8514286":"code","a9169b08":"code","c5312ed4":"code","4cecd4a3":"code","bb9e5b99":"markdown","46b0bc3d":"markdown","098cfc09":"markdown","a5fd302e":"markdown","9c4e980a":"markdown","53f2ad7a":"markdown","bce395f4":"markdown","691f0616":"markdown","6d0729b3":"markdown","3991443e":"markdown","3024a0e5":"markdown","f3d729c2":"markdown","44fb0c3e":"markdown","d29b3027":"markdown","60dd15de":"markdown","0380466a":"markdown","22b72721":"markdown","19b944a1":"markdown","cf611fd8":"markdown","f835945b":"markdown","6f411688":"markdown","cffc073c":"markdown","bcd38679":"markdown","d7146166":"markdown","c4818cb0":"markdown","536663d5":"markdown","7a2ac618":"markdown","cbbaf49d":"markdown","05a42e2b":"markdown","a1d4652a":"markdown","7e1b832d":"markdown","c2b09668":"markdown","dea83de9":"markdown","d3731c1a":"markdown","cf5aea91":"markdown","e19d454b":"markdown","b9788c6c":"markdown","a7647e99":"markdown","ce58c802":"markdown","02ed1b96":"markdown","cbef4f6b":"markdown","94a4905d":"markdown","2f26a682":"markdown","8fe55c79":"markdown","b15e4610":"markdown","29a420c6":"markdown","7723c9a1":"markdown","80154b11":"markdown","22d422af":"markdown","308de8f7":"markdown","23b756d7":"markdown","5c40e3fe":"markdown","c0165497":"markdown","a1d5b81a":"markdown","763b1022":"markdown","6c87f401":"markdown","a5735dd4":"markdown","9c1d9175":"markdown","96620143":"markdown","0ce9a5f2":"markdown","58af76ef":"markdown","66df10ef":"markdown","213e3529":"markdown","fec0b943":"markdown","f4f01f41":"markdown","7226f8d4":"markdown","03f4777b":"markdown","151b2c04":"markdown","245c1da0":"markdown","d26170df":"markdown","3d96bbf9":"markdown"},"source":{"52300d3a":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","d5dabd65":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import KNNImputer\n\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn import tree\n\nfrom xgboost import XGBClassifier\nfrom skopt import BayesSearchCV\nfrom skopt.space import Integer, Real\n\nfrom sklearn.neural_network import MLPClassifier\nfrom scipy.stats import loguniform as sp_loguniform\nfrom sklearn.model_selection import RandomizedSearchCV","e022cb5e":"adult_train = pd.read_csv('..\/input\/adult-pmr3508\/train_data.csv', na_values='?')\nadult_train.set_index('Id',inplace=True)\n\nadult_test = pd.read_csv('..\/input\/adult-pmr3508\/test_data.csv', na_values='?')\nadult_test.set_index('Id', inplace=True)\n\nnewnames = {\n    \"education.num\" : \"education_num\",\n    \"marital.status\" : \"marital_status\",\n    \"capital.gain\" : \"capital_gain\",\n    \"capital.loss\" : \"capital_loss\",\n    \"hours.per.week\" : \"hours_per_week\",\n    \"native.country\" : \"native_country\"\n}\n\nadult_train = adult_train.rename(columns = newnames)\nadult_test = adult_test.rename(columns = newnames)\n\nprint(adult_train.shape)\nprint(adult_test.shape)","e20721c6":"adult_train.head()","577964b7":"adult_test.head()","6a306ff8":"adult_train.isnull().sum().sort_values(ascending = False)","2fa680b0":"adult_train = adult_train.apply(lambda x:x.fillna(x.value_counts().index[0]))\nadult_train.isnull().sum().sort_values(ascending = False)","7d93587a":"adult_test.isnull().sum().sort_values(ascending = False)","7f245da7":"adult_test = adult_test.apply(lambda x:x.fillna(x.value_counts().index[0]))\nadult_test.isnull().sum().sort_values(ascending = False)","88b7ac99":"print(adult_train.shape)\nprint(adult_test.shape)","2ee7c7de":"adult_train.sample(5)","e2f5f15e":"adult_train.info()","8e1acdea":"adult_train.describe()","95964879":"adult_train['native_country'].value_counts()","854e3935":"is_us = adult_train['native_country'].map(lambda x: x=='United-States')\nis_us.value_counts().plot(kind='pie',autopct='%1.0f%%')","8fd31918":"adult_train['capital_gain'].value_counts()","b75034cc":"adult_train['capital_loss'].value_counts()","85bfc701":"cg0 = adult_train['capital_gain'].map(lambda x: x==0)\ncg0.value_counts().plot(kind='pie',autopct='%1.0f%%')","eee6a29f":"cl0 = adult_train['capital_loss'].map(lambda x: x==0)\ncl0.value_counts().plot(kind='pie',autopct='%1.0f%%')","c68c6129":"sns.violinplot(x='sex', y='age', hue='income', data=adult_train, split=True)","d41dcfbd":"sns.barplot(x='income', y='hours_per_week', data=adult_train)","90a24946":"sns.barplot(x='income', y='education_num', data=adult_train, hue='sex')","938f485d":"sns.barplot(x='income', y='education_num', data=adult_train)","649ae92c":"def barplot_percent(x_axis,hue):\n  x,y = hue, x_axis\n\n  df1 = adult_train.groupby(x)[y].value_counts(normalize=True)\n  df1 = df1.mul(100)\n  df1 = df1.rename('percentage').reset_index()\n\n  g = sns.catplot(x=y,y='percentage',hue=x,kind='bar',data=df1)\n  g.ax.set_ylim(0,100)","7e2be094":"barplot_percent('income','sex')","5e78a1bd":"barplot_percent('income', 'race')","3ce1dcbf":"barplot_percent('income', 'occupation')","2c3eed72":"barplot_percent('income','workclass')","e2fbd90c":"adult_test.sample(5)","6ee200de":"adult_test.info()","959edd85":"adult_test.describe()","1d0f3785":"x = adult_train[['age','education_num','capital_gain', 'capital_loss', 'hours_per_week']]\ny = adult_train['income']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25,\n                                                         stratify = y)","549e3e90":"knn = KNeighborsClassifier(n_neighbors=3)\n\nscores = cross_val_score(knn, xtrain, ytrain, cv=10)\nscores.mean()","924540c5":"knn.fit(xtrain,ytrain)\npredict = knn.predict(xtest)\npredict","0a3e1642":"accuracy_score(ytest,predict)","71de5d78":"numadult_train = adult_train[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex']].apply(preprocessing.LabelEncoder().fit_transform)\nnumadult_train","018046cc":"def predict_model(n):\n  x = numadult_train\n  y = adult_train['income']\n\n  SEED = 158020\n  np.random.seed(SEED)\n  xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25,\n                                                          stratify = y)\n  \n  scaler = StandardScaler()\n  scaler.fit(xtrain)\n  xtrain = scaler.transform(xtrain)\n  xtest = scaler.transform(xtest)\n\n  knn = KNeighborsClassifier(n_neighbors=n)\n  knn.fit(xtrain,ytrain)\n  scores = cross_val_score(knn, xtrain, ytrain, cv=10)\n  return scores.mean()","88c5ace8":"x=[]\nfor i in range(1,40):\n  x.append(predict_model(i))","22036a78":"plt.plot(x)","05925478":"predict_model(18)","c5633986":"placeholder = adult_train[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex','income']].apply(preprocessing.LabelEncoder().fit_transform)","8693f360":"x = placeholder[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex']]\ny = placeholder['income']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25,\n                                                          stratify = y)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nLR = LinearRegression()\nLR.fit(xtrain,ytrain)\nscores = cross_val_score(LR, xtrain, ytrain, cv=10).mean()\nprint(scores)","a4218ca8":"x = placeholder[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex']]\ny = placeholder['income']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n  \nscaler = StandardScaler()\nscaler.fit(xtrain)\n\nxtrain = scaler.transform(xtrain)\nxtest = scaler.transform(xtest)\n\nmodel = tree.DecisionTreeClassifier()\nmodel.fit(xtrain,ytrain)\n\ncross_val_score(model, xtrain, ytrain, cv=10).mean()","34eb75cf":"xgb = XGBClassifier(random_state=42)\n\nxgb_search_cv = BayesSearchCV(estimator = xgb,\n                              search_spaces = {'n_estimators': Integer(10, 500),\n                                               'learning_rate': Real(1e-3, 1),\n                                               'max_depth': Integer(1, 20),\n                                               'reg_alpha': Real(1e-14, 1e1, prior = 'log-uniform'),\n                                               'reg_lambda': Real(1e-14, 1e1, prior = 'log-uniform'),},\n                              cv = 5,\n                              n_iter = 75, n_jobs=-1, random_state=42)\n\nx = placeholder[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex']]\ny = placeholder['income']\n\nSEED = 158020\nnp.random.seed(SEED)\nxtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size = 0.25)\n\n\n%timeit -n 1 -r 1 xgb_search_cv.fit(xtrain, ytrain)\n\nprint('{}'.format(round(xgb_search_cv.best_score_,5)))","ee4037cc":"mlp = MLPClassifier(random_state=42, early_stopping=True)\n\n\nhyperparams = {'hidden_layer_sizes': [(2 ** i, 2 ** j) for j in np.arange(5, 8) for i in np.arange(4, 7)],\n               'alpha': sp_loguniform(1e-10, 1e-1),\n               'learning_rate': ['constant','adaptive']}\n\n\nmlp_search_cv = RandomizedSearchCV(mlp, hyperparams, scoring='accuracy', n_iter=25, cv=3, n_jobs=-1, random_state=42)\n%timeit -n 1 -r 1 mlp_search_cv.fit(xtrain, ytrain)\n\nprint('{}'.format(round(mlp_search_cv.best_score_,5)))","f81a7e62":"adult_test.sample(10)","75d2c697":"numadult_test = adult_test[['age','workclass','education_num','capital_gain',\n                      'capital_loss', 'hours_per_week','marital_status',\n                      'occupation','relationship','race','sex']].apply(preprocessing.LabelEncoder().fit_transform)\nnumadult_test","b189c839":"scaler = StandardScaler()\nscaler.fit(numadult_test)\nxadult_test = scaler.transform(numadult_test)","e8514286":"predict = xgb_search_cv.predict(numadult_test)\nlen(predict)","a9169b08":"submission = pd.DataFrame()\nsubmission[0] = adult_test.index\nsubmission[1] = predict\nsubmission.columns = ['Id','income']\nsubmission['income'].value_counts()","c5312ed4":"submission['income'] = submission['income'].replace(0,'<=50K')\nsubmission['income'] = submission['income'].replace(1,'>50K')\nsubmission","4cecd4a3":"submission.to_csv('my_submission.csv',index = False)","bb9e5b99":"## 3.Treating missing data\nNext, we'll be treating any missing data from our dataset","46b0bc3d":"This number is really close to what we got through the cross validation method","098cfc09":"# PMR3508 - Aprendizado de M\u00e1quina e Reconhecimento de Padr\u00f5es\n\nAuthor: Gabriel Pinheiro Date: 10 \/ 09 \/ 2020","a5fd302e":"We are only going to work with the numerical columns for this first model","9c4e980a":"Knowing that, we are going to use the \"most common\" method to fill these null values","53f2ad7a":"And finally, we submit our newly created DataFrame as \"submission.csv\"","bce395f4":"By what we can observe from the violin plot, both \"Male\" and \"Female\" subjects start to earn an income grater than 50K when they get closer to 40 years of age. On the 20 years mark, it seems that the vast majority earns less than 50K. ","691f0616":"The \"predict\" is an array filled with \">50K\" and \"<=50K\". Let's see it's accuracy","6d0729b3":"For this next analysis, we are going to see the impact of age to the income result","3991443e":"With this method we can achieve 81.7% accuracy, which is better than the Linear regression one, but doesn't surpass the knn model.","3024a0e5":"As we can see, the accuracy of this model doesn't surpass the boosted model.","f3d729c2":"As we can see on the graph, \"Female\" subjects tend to have a higher education on their income group if compared to the \"Male\" subjects, this evidenciates the inequality of payment between both sex groups.","44fb0c3e":"For the following plots, we are going to bulid a function that shows relations by percentage","d29b3027":"The plot shows us that around 90% of \"Female\" subjects and 70% of \"Male\" subjects earn less than 50K, highlighting the impact of sex on income","60dd15de":"## 1. Importing Libraries\nThe following libraries will be used for the data analysis section of the notebook","0380466a":"### 4.1 \"Train\" dataset\nBecause we are going to base our model on the \"adult_train\" dataset, it's going to receive a more robust analysis compared to the \"adult_test\" dataset","22b72721":"By what we saw on the column \"native_country\", it seems that there is a predominance of individuals from the USA.","19b944a1":"### 5.6 Booster model\n\nNow let's use a booster model","cf611fd8":"### 5.3 Linear regression model\n\nWe'll now try to build a Linear Regression model for our database","f835945b":"The last conclusion is corroborated by the previous graph. As we can see, 91% of the people on this dataset are american citizens","6f411688":"First, let's see how many null values there are in each column of the \"adult_train\" dataset","cffc073c":"First we are going to create a placeholder matrix to make things easier","bcd38679":"Next, let's create a new DataFrame to populate with our predictions","d7146166":"As we can see, we filled all null values from \"adult_train\". Now let's do the same thing with the \"adult_test\" dataset","c4818cb0":"As a last step, we fix this new dataset's formatting","536663d5":"After all that, we can see that the shape of both datasets remain untouched","7a2ac618":"Creating the model for k = 3, and testing it with cross_val_score, also from sklearn","cbbaf49d":"In this notebook we'll be analysing the Adult dataset. With this we'll try to understand the dataset and how it represents the world around us. Then we'll use that information to build models that are able to predict the income of any given individual, so we can choose the best one for this application.","05a42e2b":"Next, we'll analyze the impact of race on income","a1d4652a":"Then we create our predictions","7e1b832d":"Now, we're going to analyse how the income is affected by sex and education","c2b09668":"As expected, people with a higher education generally earn more than 50K","dea83de9":"Now, let's observe the impact of occupation on the income","d3731c1a":"On this case, \"Asian-Pac-Islander\" and \"White\" subjects have a similar distribution between income, around 30% of them earn more than 50K. In comparison, \"Black\", \"Amer-Indian-Eskimo\" and \"Other\" subjects are highly condensed on the \"<=50K\" group","cf5aea91":"Firstly, we are going to build a really simple model, just to get a primitive result","e19d454b":"Next, we are going to create models with different K values, with this we will discover what's the optimal number of nearest neighbours","b9788c6c":"In the function \"predict_model\" we use the preprocessing function \"StandadScaler\" so we can create a more accurate model","a7647e99":"Next, we are going to take a look on the \"capital_gain\" and \"capital_loss\" columns, because it seems that there is a grater appearance of \"0\" values for both columns","ce58c802":"First of all, let's get a random sample from the \"adult_train\" dataset, just so we can remember it's rows and columns","02ed1b96":"This model has the best accuracy yet, with 87.2%","cbef4f6b":"Also, we are going to use \".describe()\" some important statistical information about the numerical columns","94a4905d":"And these are the libraries that will be used to build the prediction model","2f26a682":"By what we can observe, the \"adult_test\" dataset has similar information to the \"adult_train\" dataset, meaning that our prediction model will be appropriated for our final objective","8fe55c79":"There are 6 numerical columns: \"age\", \"fnlwgt\", \"education_num\", \"capital_gain\", \"capital_loss\" and \"hours_per_week\". The rest are filled with labels for \"native_country\", \"sex\", \"race\", etc","b15e4610":"As we can see, both datasets have the same columns, except for one: \"income\". This column will be built by our prediction model later on.","29a420c6":"Next, we'll analyze the effect of education on the income","7723c9a1":"### 5.1 Primitive model training","80154b11":"### 5.2 Bettering the model\nNow, to build a more robust model, we first need to make all the columns we are going to use, in numerical columns","22d422af":"With the function \"barplot_percent\" defined, we can take a look on how the groups \"Female\" and \"Male\" are distributed by income","308de8f7":"### 5.7 Neural network\n\nAs a last model, we are going to create a neural network","23b756d7":"Now we take a look on the 5 first rows of the \"adult_test\" dataset","5c40e3fe":"The previous graph shows us that \"Exec-managerial\" and \"Prof-specialty\" stand out by having more than 40% of it's individuals earning more than 50K","c0165497":"As we already know, only the \"income\" column is missing, but let's see if this dataset has the same general information","a1d5b81a":"On average, people that earn less than 50K work 40 hours per week, while people that earn more than 50K work around 46 hours per week. It seems that people earning less than 50K are employed on regular jobs (9 to 5 jobs) whilst the other group have more fluid work hours.","763b1022":"With k = 18 we get around 85.2% of accuracy, this is a 4.4% increase from the primitive model.","6c87f401":"## 6. Submission\nNow for our submission. First we need to transform all the columns in the dataset \"adult_test\" in numerical ones","a5735dd4":"As a conclusion for this section, we can clearly see that the model that better fits with our database is the booster model.","9c1d9175":"Our suspection is confirmed by these last graphs","96620143":"To conclude this section, let's take a look on how income and workclass relate to each other","0ce9a5f2":"Then we'll use \".info()\" to get some general information","58af76ef":"## 5. Model training\nAfter understanding our datasets, we can finally build our prediction model. For this we'll be using the KNN method. We'll be using KNeighborsClassifier() from sklearn to build our models","66df10ef":"As we can see, government workers and people in the \"Self-emp-inc\" group tend to earn more than 50K","213e3529":"## 4. Data analysis","fec0b943":"The mean from the cross validation method gives us around 80.8% accuracy","f4f01f41":"This model gave us 30.5% accuracy, which is less than our KNN model.","7226f8d4":"After that we have to decide which informations are necessary to our model, and which arent. Through many tests, we discovered that the columns \"fnlwgt\", \"education\" and \"native_country\" don't help making the model more accurate.\n\nFinally, we'll define a function that uses: \"age\", \"workclass\", \"education_num\", \"capital_gain\", \"capital_loss\", \"hours_per_week\", \"marital_status\", \"occupation\", \"relationship\", \"race\" and \"sex\" to predict the \"income\".","03f4777b":"Next, we'll see how hours worked per week impact on the income","151b2c04":"### 4.2 \"Test\" dataset\nNow, let's take a observe how the \"adult_test\" dataset is built","245c1da0":"## 2.Data import\nIn this next section both datasets are going to be imported, \"adult_train\" represents \"train_data.csv\" and \"adult_test\" represents \"test_data.csv\". Notice that the \"adult_train\" dataset has both more columns and more rows than the \"adult_test\".","d26170df":"### 5.5 Decision Tree model\n\nNext we'll text a Decision tree model","3d96bbf9":"Now let's take a look on the first 5 rows of the \"adult_train\" dataset."}}