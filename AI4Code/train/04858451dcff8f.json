{"cell_type":{"2bce770d":"code","a6cf4dc6":"code","d7caf469":"code","81debdee":"code","8993e491":"code","0ee72829":"code","6be05bc7":"code","759fccc1":"code","2e1b4626":"code","ef345a71":"code","35f2999a":"code","05c867d9":"code","ebef1133":"code","43530e65":"code","9dcbfffe":"code","d53e6630":"code","23cb41c5":"code","7d0b3a2f":"code","dc1849ed":"code","62e213ec":"code","7a4d43bd":"code","2b4af698":"code","0eb6cf27":"code","766a6e4e":"code","1c71590f":"code","43caffa2":"code","de4c1ef7":"code","4885fef4":"code","272454d4":"code","d50a099b":"code","0b2a7b25":"code","a36d76dc":"code","39531c9c":"code","ca5be27d":"code","8345a415":"code","49a637b8":"code","4f40d148":"code","ab59081f":"code","633e0053":"code","aa7e4d65":"code","3dc4dd18":"code","bf90cf2a":"code","1758078e":"code","9f53561b":"code","b5402269":"code","b9210dc2":"code","5926198f":"code","2195d02a":"code","4fe4252b":"code","8c359f6e":"code","345fe519":"code","90eea375":"code","2e692d79":"code","c3fc5bd1":"code","379d7362":"code","bc3b5122":"code","335be055":"code","ce55e31b":"code","c13b5759":"code","9adf8512":"code","91ec9c46":"code","d967ad93":"code","51ff827e":"code","11a56b0c":"code","4bdc79c8":"code","9f24c4e9":"code","4fd710d5":"code","45aa1dd1":"code","330cda5d":"code","e0fc8222":"code","2b3f30d6":"code","afb05656":"code","10d0e4fd":"code","06e8ddb3":"code","c3bbd633":"code","7a1bb658":"code","3e8e5afd":"code","f35946ac":"code","ec98a4c9":"code","2466ee5b":"code","01e80315":"code","5a5b0469":"code","0ebf2740":"code","fd0f82d8":"code","f43fb416":"code","5330478c":"code","e51df167":"code","8f15819b":"code","09679900":"code","2f886276":"code","3ba31806":"code","8d7e50da":"code","b06a575f":"code","d4530b3c":"code","7010fe49":"code","e8d5ec84":"code","e5c3592a":"code","97002949":"code","14553d3f":"code","a43a694c":"code","1157dec7":"code","f111db7d":"code","4e6806a3":"code","2d98189f":"code","485f9d01":"code","34168c22":"code","84c97457":"code","442255e5":"code","aa5b8302":"code","133bb916":"code","ab866439":"code","b4a60f99":"code","de46d362":"code","29eb363f":"code","4f106ab8":"code","783412e2":"code","956c2c6d":"code","cfe023f0":"code","da025d2b":"code","6a36ff61":"code","e6c08b5a":"code","7985d7a1":"code","bb9b6ad1":"code","94c66f46":"code","a639fbf4":"code","f6054f9d":"code","76c90ebd":"code","87004b12":"code","bfb83dea":"code","1b318a55":"code","955dcbe7":"code","8fe533a4":"code","2bf4a65d":"code","e7bfb13f":"code","ea73da2e":"code","a677b0fe":"code","9c06f8b4":"code","49fdab6e":"code","0794fff6":"code","fad8fc31":"code","7d2892ef":"code","e9ade123":"code","54007d86":"code","2f423769":"code","47ebb327":"code","ef564230":"code","d21195ab":"code","1f1e0acb":"code","73900a07":"code","256d39eb":"code","3eb431ae":"code","5065f764":"code","9a9e855a":"code","603f0d81":"code","c95df424":"code","36a10be0":"code","d0b24321":"code","76aad421":"code","7030527c":"code","5d08e935":"code","3f60ad0f":"code","9786014e":"code","62aab172":"code","4f653bd5":"code","b409bf94":"code","981da86a":"code","63046b07":"code","37212376":"code","0b75c2a0":"code","4279426e":"code","8197e92f":"code","8a1e2708":"code","1061cc2a":"code","992228fd":"code","3e908b56":"code","830603f5":"code","587a1a4b":"code","12c3f600":"code","cfbbc428":"code","be85ea39":"code","38ef2eb9":"code","21346703":"code","4274e116":"code","4c396136":"code","037aec83":"code","a9abafe7":"code","fea4a074":"code","b5ed1564":"code","32e1fc5a":"code","07e5a694":"code","12b8a40d":"code","a9714906":"code","186bd575":"code","8b259f44":"code","3752f731":"code","97902c9f":"code","f226c917":"code","0089ed18":"code","64fb9d16":"code","61aa0606":"code","d2da6ad9":"code","8d3e3354":"code","e81f0a42":"code","a3d65178":"code","639d88db":"code","239b4771":"code","bfae58f3":"code","69c719eb":"code","85684eac":"code","c790032b":"code","23367927":"code","c4c743ef":"code","8605de6c":"code","149083a9":"code","43536e47":"code","f6f8c7ed":"code","20bc0c51":"code","29d40210":"code","ef31caac":"code","a9e431a4":"code","77c80132":"code","79f82eff":"code","020b2b43":"code","6f8b1244":"code","0c810e97":"code","fb64ed20":"code","125c1f2f":"code","84499f12":"code","e9aed6b9":"code","6945e660":"code","141223a5":"code","109035e0":"code","9ae0577d":"code","1f4fc598":"code","27ff3c1c":"code","f8f92257":"code","55ea575b":"code","dae694dd":"code","904ce25f":"code","15b6a862":"code","9cc72e17":"code","75d02ae5":"code","e0908eaf":"code","8fbcc62f":"code","b02447a5":"code","f6c45f09":"code","fcdf1c08":"code","de7cf724":"code","1f45c2c8":"code","d37c14cd":"code","2161b3ca":"code","1a888af1":"code","1452661d":"code","e2bb6c6f":"code","501b4f74":"code","18ab643c":"code","51c6d176":"code","e4febb6b":"code","47ce75e8":"code","a7c2a896":"code","b8c74b97":"code","557c8565":"code","4b283f6b":"code","fa134f20":"code","30b0424b":"code","1c13f34e":"code","0c36e0e8":"code","e9d44026":"code","0a9406c9":"code","b9421279":"code","8d06aafb":"code","ffb09cdd":"code","ccd97af9":"code","a1a6a173":"code","f94c429e":"code","860317ae":"code","c2f6153c":"code","75527d08":"code","1415bc8b":"code","e2bbf159":"code","b17bc705":"code","b9e63191":"code","1e8c8271":"code","2a73d21a":"code","84c38636":"code","12c8f4ca":"code","b9900db7":"code","56394818":"code","a7179bfd":"code","ae944e52":"code","2f247b6b":"code","3484c0a8":"code","282366c4":"code","c5acbb67":"code","abc14d25":"code","b4005c9e":"code","2b67163b":"code","aa95f028":"code","738aff03":"code","6742343f":"code","a92c0285":"code","98da69e8":"code","79c8c9d5":"code","58f91848":"code","1444db0f":"code","3444dbab":"code","66626175":"code","74f88af5":"code","44749941":"code","43d5459f":"code","f53be917":"code","41be1f89":"code","ca0744b7":"code","0853f06a":"code","157e2330":"code","7b26f952":"code","e3babffc":"code","598f0185":"code","470e7780":"code","b55ee31c":"code","7e56f215":"code","fc10780b":"code","3d5f4dee":"code","b676cb4c":"code","d14305e4":"code","be2aee84":"code","db50455b":"code","60b7d529":"code","8e0845c4":"code","5778b2c6":"code","1506e037":"code","bcf5c18b":"code","67e52b9b":"code","65928c60":"markdown","7e0d15ef":"markdown","72d531fd":"markdown","92516198":"markdown","d54cdb45":"markdown","c38fc0f2":"markdown","8a5f6743":"markdown","afd43bd6":"markdown","818b4927":"markdown","c9b81708":"markdown","de05b20d":"markdown","77ce496b":"markdown","153f2bea":"markdown","ad031728":"markdown","9eb2deed":"markdown","045c3e4d":"markdown","f1a618fb":"markdown","9a283a6a":"markdown","d911b6a5":"markdown","a7370356":"markdown","413b9902":"markdown","e9104924":"markdown","f5b0e47c":"markdown","aed10fec":"markdown","9355746b":"markdown"},"source":{"2bce770d":"##https:\/\/putusan3.mahkamahagung.go.id\/direktori\/index\/kategori\/merek.html\n####https:\/\/putusan3.mahkamahagung.go.id\/direktori\/index\/pengadilan\/mahkamah-agung\/kategori\/merek.html\n####https:\/\/pdki-indonesia.dgip.go.id\/search?type=trademark&keyword=payfazz&id=D002015015616\n##https:\/\/machinelearningmastery.com\/clustering-algorithms-with-python\/\n##https:\/\/github.com\/zll17\/Neural_Topic_Models\n###https:\/\/paperswithcode.com\/paper\/neural-topic-modeling-with-bidirectional#code\n###https:\/\/www.hindawi.com\/journals\/mpe\/2017\/8310934\/","a6cf4dc6":"pip install selenium","d7caf469":"pip install requests_html","81debdee":"from requests_html import AsyncHTMLSession\nsession = AsyncHTMLSession()\n\nr = session.get('https:\/\/pdki-indonesia.dgip.go.id\/search?type=trademark&keyword=payfazz&id=D002015015616')\n\nr.html.render()\n\nr.html.search('Ditolak')","8993e491":"import asyncio\nfrom requests_html import AsyncHTMLSession\nasession = AsyncHTMLSession()\n\nr = await asession.get('https:\/\/pdki-indonesia.dgip.go.id\/search?type=trademark&keyword=payfazz&id=D002015015616')\nr.html.render()\nr.html.search('Ditolak')","0ee72829":"pip install scrapy","6be05bc7":"import scrapy\n\n\nclass MySpider(scrapy.Spider):\n    name = 'my_spider'\n    start_urls = ['https:\/\/pdki-indonesia.dgip.go.id\/search?type=trademark&keyword=payfazz&id=D0020150156']\n\n    def start_requests(self):\n        yield scrapy.Request(url=self.start_urls[0])\n\n\n    def parse(self, response):\n\n        # do stuff with results, scrape items etc.\n        # now were just checking everything worked\n\n        print(response.body)","759fccc1":"pip install Sastrawi","2e1b4626":"import os\n\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n#from mlxtend.frequent_patterns import fpgrowth\nfrom nltk.tokenize import word_tokenize\nimport pandas as pd\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport os\n\n#from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\nimport warnings\nfrom wordcloud import WordCloud\nimport numpy as np\nimport pandas as pd\nimport nltk\nimport re\nimport os\nimport codecs\nfrom sklearn import feature_extraction\n#import mpld3\nimport re\nfrom os import system, name\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans","ef345a71":"import pandas as pd\ndf = pd.read_csv ('..\/input\/dataitelengkap\/fileakhir2.csv')\nprint(df)","35f2999a":"df['pertimbangan']","05c867d9":"df['putusan']","ebef1133":"indexNames = df[ df['putusan'] == ' ' ].index","43530e65":"df.drop(indexNames , inplace=True)","9dcbfffe":"titles = df['namafile'].values.tolist()","d53e6630":"titles2 = df['namafile'].values.tolist()","23cb41c5":"titles","7d0b3a2f":"synopses= df['pertimbangan'].values.tolist()","dc1849ed":"synopses2= df['putusan'].values.tolist()","62e213ec":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2,\n                                 use_idf=True, ngram_range=(1,3))\n\n%time tfidf_matrix = tfidf_vectorizer.fit_transform(synopses) #fit the vectorizer to synopses\n\nprint(tfidf_matrix.shape)","7a4d43bd":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n#define vectorizer parameters\ntfidf_vectorizer2 = TfidfVectorizer(max_df=0.8, max_features=200000,\n                                 min_df=0.2,\n                                 use_idf=True, ngram_range=(1,3))\n\n%time tfidf_matrix2 = tfidf_vectorizer2.fit_transform(synopses2) #fit the vectorizer to synopses\n\nprint(tfidf_matrix2.shape)","2b4af698":"range_n_clusters = [2, 3, 4, 5, 6, 7, 8,9,10,11,12]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(tfidf_matrix)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n\n\n\nprint()","0eb6cf27":"range_n_clusters = [2, 3, 4, 5, 6, 7, 8,9,10,11,12]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(tfidf_matrix)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(tfidf_matrix2, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n\n\n\nprint()","766a6e4e":"sum_of_squared_distances = []\nK = range(1,9)\nfor k in K:\n    k_means = KMeans(n_clusters=k)\n    model = k_means.fit(tfidf_matrix)\n    sum_of_squared_distances.append(k_means.inertia_)","1c71590f":"print(sum_of_squared_distances)","43caffa2":"plt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('sum_of_squared_distances')\nplt.title('elbow method for optimal k')\nplt.show()","de4c1ef7":"sum_of_squared_distances = []\nK = range(1,9)\nfor k in K:\n    k_means = KMeans(n_clusters=k)\n    model = k_means.fit(tfidf_matrix2)\n    sum_of_squared_distances.append(k_means.inertia_)","4885fef4":"plt.plot(K, sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('sum_of_squared_distances')\nplt.title('elbow method for optimal k')\nplt.show()","272454d4":"# calculate the feature matrix\nfeature_matrix = tfidf_vectorizer.fit_transform(synopses)\n\n# display the feature matrix shape\ndisplay(feature_matrix.shape)","d50a099b":"# calculate the feature matrix\nfeature_matrix2 = tfidf_vectorizer.fit_transform(synopses2)\n\n# display the feature matrix shape\ndisplay(feature_matrix2.shape)","0b2a7b25":"from sklearn.cluster import DBSCAN\nDBS_clf = DBSCAN(eps=1, min_samples=4)\nDBS_clf.fit(tfidf_matrix)\nprint(DBS_clf.labels_)","a36d76dc":"# === Define the function of classify the original corpus according to the labels === #\ndef labels_to_original(labels, original_corpus):\n    assert len(labels) == len(original_corpus)\n    max_label = max(labels)\n    number_label = [i for i in range(0, max_label + 1, 1)]\n    number_label.append(-1)\n    result = [[] for i in range(len(number_label))]\n    for i in range(len(labels)):\n        index = number_label.index(labels[i])\n        result[index].append(original_corpus[i])\n    return result","39531c9c":"labels_original = labels_to_original(DBS_clf.labels_, titles)\nfor i in range(5):\n\tprint(labels_original[i])\n # Clustering results show (part)","ca5be27d":"from scipy.cluster.vq import kmeans, vq\nimport seaborn as sns\n\ndef kmeans_cluster_terms(num_clusters, top_n):\n    \"\"\"Performs K-means clustering and returns top_n features in each cluster.\n\n    Args:\n        num_cluster: k in k-means.\n        top_n: top n features closest to the centroid of each cluster.\n\n    Returns:\n        cluster_centers: centroids of each cluster.\n        distortion: sum of squares within each cluster.\n        key_terms: list of top_n features closest to each centroid.\n        labels: cluster assignments\n    \"\"\"\n    # Generate cluster centers through the kmeans function\n    cluster_centers, distortion = kmeans(feature_matrix.todense(), num_clusters)\n\n    # Generate terms from the tfidf_vectorizer object\n    terms = tfidf_vectorizer.get_feature_names()\n\n    # Display the top_n terms in that cluster\n    key_terms = []\n    for i in range(num_clusters):\n        # Sort the terms and print top_n terms\n        center_terms = dict(zip(terms, list(cluster_centers[i])))\n        sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n        key_terms.append(sorted_terms[:top_n])\n\n    # label the clusters\n    labels, _ = vq(feature_matrix.todense(), cluster_centers, check_finite=True)\n\n    return cluster_centers, distortion, key_terms, labels","8345a415":"# vary k from 2,10\ndistortions = []\ncentroids = []\ntop_10 = []\ncluster_labels = []\n\nnum_clusters = range(2, 10)\n\nfor i in num_clusters:\n    cluster_centers, distortion, key_terms, labels = kmeans_cluster_terms(i, 10)\n\n    centroids.append(cluster_centers)\n    distortions.append(distortion)\n    top_10.append(key_terms)\n    cluster_labels.append(labels)\n\n# plot the elbow plot\nelbow_plot_data = pd.DataFrame({'num_clusters': num_clusters,\n                               'distortions': distortions})\n\nsns.lineplot(x='num_clusters', y='distortions', data = elbow_plot_data)\nplt.show()","49a637b8":"from sklearn.decomposition import PCA\npca = PCA()\ncomponents = pca.fit_transform(feature_matrix.todense())\n\nxs, ys = components[:, 0], components[:, 1]","4f40d148":"from sklearn.decomposition import PCA\npca = PCA()\ncomponents2 = pca.fit_transform(feature_matrix2.todense())\n\nxs2, ys2 = components2[:, 0], components2[:, 1]","ab59081f":"#set up colors per clusters using a dict\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a'}  #, 4: '#66a61e'}\n\n#set up cluster names using a dict\ncluster_names = {0: ', '.join(top_10[2][0]),\n                 1: ', '.join(top_10[2][1]),\n                 2: ', '.join(top_10[2][2]),\n                 3: ', '.join(top_10[2][3])}\n\n# get the cluster labels\nlabels_four = list(cluster_labels[2])","633e0053":"from sklearn.metrics import silhouette_samples, silhouette_score\n\ndef plot_kmeans(k, xs, ys, labels, cluster_names, cluster_colors, fig, ax, num_points=len(xs)):\n    \"\"\"Plots k-means clusters with first two principal components\n    Args:\n        k: number of clusters\n        xs: first principal component\n        ys: second principal component\n        labels: cluster labels assigned by k-means algorithm\n        cluster_names: top_10 features around the centroid\n        cluster_colors: dictionary of pre-established colors\n        num_points: number of observations you want displayed.\n    \"\"\"\n\n    #create data frame that has the result of the PCA plus the cluster numbers\n    df = pd.DataFrame(dict(x=xs[:num_points], y=ys[:num_points], label=labels[:num_points],\n                           title=labels[:num_points]))\n\n    #group by cluster\n    groups = df.groupby('label')\n\n    # set up plot\n    #fig, ax = plt.subplots(figsize=(17, 9)) # set size\n    ax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\n\n    #iterate through groups to layer the plot\n    #note that I use the cluster_name and cluster_color dicts\n    #with the 'name' lookup to return the appropriate color\/label\n    for name, group in groups:\n        ax.plot(group.x, group.y, marker='o', linestyle='', ms=12,\n                label=cluster_names[name], color=cluster_colors[name],\n                mec='none')\n        ax.set_aspect('auto')\n        ax.tick_params(\\\n            axis= 'x',          # changes apply to the x-axis\n            which='both',      # both major and minor ticks are affected\n            bottom='off',      # ticks along the bottom edge are off\n            top='off',         # ticks along the top edge are off\n            labelbottom='off')\n        ax.tick_params(\\\n            axis= 'y',         # changes apply to the y-axis\n            which='both',      # both major and minor ticks are affected\n            left='off',      # ticks along the bottom edge are off\n            top='off',         # ticks along the top edge are off\n            labelleft='off')\n\n    ax.legend(numpoints=1)  #show legend with only 1 point\n\n    #add label in x,y position with the label as the film title\n    #for i in range(len(df)):\n        #ax.text(df.ix[i]['x'], df.ix[i]['y'], df.ix[i]['title'], size=10)\n\n\n    ax.set_title(\"K-means with \" + str(k) + \" clusters showing \" \\\n                 + str(num_points) + \" movie reviews\")\n    ax.set_xlabel(\"first principal component\")\n    ax.set_ylabel(\"second principal component\")\n\n    return ax\n\ndef plot_silhouette(n_clusters, X, labels, cluster_colors, fig, ax):\n    \"\"\"Plots the silhouette plot\n    Args:\n        n_clusters: number of clusters\n        X: dense tf-idf feature matrix, the same one used to fit the model\n        labels: clusters guessed by the algorithm\n        cluster_colors: dict of cluster colors - same colors used in scatterplot.\n        fig: figure object to display in a single row\n        ax: axes object to display in a single row\n\n    Returns:\n        ax: axes object\n    \"\"\"\n\n    # set up plot\n    # Create a subplot with 1 row and 2 columns\n    #fig, ax = plt.subplots(figsize=(15, 9)) # set size\n\n    # may need to set ax.set_xlim ax.set_ylim\n\n    # get predicted cluster labels for k\n    # convert to ndarray as fillcolor needs it that way.\n    cluster_labels = np.asarray(labels)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n            \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    color = tuple(cluster_colors.values())\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        #print(ith_cluster_silhouette_values)\n\n        #color = cm.nipy_spectral(float(i) \/ n_clusters)\n        #color = tuple(cluster_colors.values())\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                            0, ith_cluster_silhouette_values,\n                            facecolor=color[i], edgecolor=color[i], alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.02, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax.set_title(\"The silhouette plot for the various clusters.\")\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([])\n\n    return ax\n\n\n\nimport matplotlib.cm as cm\ndef plot_silhouette_hierarchical(n_clusters, X, labels, fig, ax):\n    \"\"\"Plots the silhouette plot\n    Args:\n        n_clusters: number of clusters\n        X: dense tf-idf feature matrix, the same one used to fit the model\n        labels: clusters guessed by the algorithm\n        fig: figure object to display in a single row\n        ax: axes object to display in a single row\n\n    Returns:\n        ax: axes object\n    \"\"\"\n\n    # set up plot\n    # Create a subplot with 1 row and 2 columns\n    #fig, ax = plt.subplots(figsize=(15, 9)) # set size\n\n    # may need to set ax.set_xlim ax.set_ylim\n\n    # get predicted cluster labels for k\n    # convert to ndarray as fillcolor needs it that way.\n    cluster_labels = np.asarray(labels)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X, cluster_labels)\n    print(\"For n_clusters =\", n_clusters,\n            \"The average silhouette_score is :\", silhouette_avg)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        #print(ith_cluster_silhouette_values)\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        #color = tuple(cluster_colors.values())\n        ax.fill_betweenx(np.arange(y_lower, y_upper),\n                            0, ith_cluster_silhouette_values)\n                            #facecolor=color[i], edgecolor=color[i], alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax.set_title(\"The silhouette plot for the various clusters.\")\n    ax.set_xlabel(\"The silhouette coefficient values\")\n    ax.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax.set_xticks([])\n\n    return ax","aa7e4d65":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\nax1 = plot_kmeans(4, xs, ys, labels_four, cluster_names, cluster_colors, fig, ax1, 250)\nax2 = plot_silhouette(4, feature_matrix.todense(), labels_four, cluster_colors, fig, ax2)\n\nplt.suptitle(\"Clustering user movie reviews using K-means with K=4\", fontsize=14, fontweight='bold')\nplt.show()","3dc4dd18":"#set up colors per clusters using a dict\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3' }#, 3: '#e7298a'}  #, 4: '#66a61e'}\n\n#set up cluster names using a dict\ncluster_names = {0: ', '.join(top_10[1][0]),\n                 1: ', '.join(top_10[1][1]),\n                 2: ', '.join(top_10[1][2])}\n\n# get the cluster labels\nlabels_three = list(cluster_labels[1])","bf90cf2a":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\nax1 = plot_kmeans(3, xs, ys, labels_three, cluster_names, cluster_colors, fig, ax1)\nax2 = plot_silhouette(3, feature_matrix.todense(), labels_three, cluster_colors, fig, ax2)\n\nplt.suptitle(\"Clustering user movie reviews using K-means with K=3\", fontsize=14, fontweight='bold')\nplt.show()","1758078e":"#set up colors per clusters using a dict\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n\n#set up cluster names using a dict\ncluster_names = {0: ', '.join(top_10[3][0]),\n                 1: ', '.join(top_10[3][1]),\n                 2: ', '.join(top_10[3][2]),\n                 3: ', '.join(top_10[3][3]),\n                 4: ', '.join(top_10[3][4])}\n\n# get the cluster labels\nlabels_five = list(cluster_labels[3])","9f53561b":"fig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\nax1 = plot_kmeans(5, xs, ys, labels_four, cluster_names, cluster_colors, fig, ax1, 250)\nax2 = plot_silhouette(5, feature_matrix.todense(), labels_four, cluster_colors, fig, ax2)\n\nplt.suptitle(\"Clustering user movie reviews using K-means with K=4\", fontsize=14, fontweight='bold')\nplt.show()","b5402269":"# Import cosine_similarity to calculate similarity of movie plots\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate the similarity distance\nsimilarity_distance = 1 - cosine_similarity(feature_matrix)\n\n# Import modules necessary to plot dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Create mergings matrix\nmergings = linkage(similarity_distance, method='complete')","b9210dc2":"# Import cosine_similarity to calculate similarity of movie plots\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n# Calculate the similarity distance\nsimilarity_distance2 = 1 - cosine_similarity(feature_matrix2)\n\n# Import modules necessary to plot dendrogram\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Create mergings matrix\nmergings2 = linkage(similarity_distance2, method='complete')","5926198f":"from scipy.cluster.hierarchy import ward, dendrogram\n\nlinkage_matrix = ward(similarity_distance) #define the linkage_matrix using ward clustering pre-computed distances\n\nfig, ax = plt.subplots(figsize=(15, 20)) # set size\nax = dendrogram(linkage_matrix, orientation=\"left\", labels=titles);\n\nplt.tick_params(\\\n    axis= 'x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    top='off',         # ticks along the top edge are off\n    labelbottom='off')\n\nplt.tight_layout() #show plot with tight layout\n\n#uncomment below to save figure\nplt.savefig('ward_clusters.png', dpi=200) #save figure as ward_clusters","2195d02a":"from scipy.cluster.hierarchy import ward, dendrogram\n\nlinkage_matrix2 = ward(similarity_distance2) #define the linkage_matrix using ward clustering pre-computed distances\n\nfig, ax = plt.subplots(figsize=(15, 20)) # set size\nax = dendrogram(linkage_matrix2, orientation=\"left\", labels=titles);\n\nplt.tick_params(\\\n    axis= 'x',          # changes apply to the x-axis\n    which='both',      # both major and minor ticks are affected\n    bottom='off',      # ticks along the bottom edge are off\n    top='off',         # ticks along the top edge are off\n    labelbottom='off')\n\nplt.tight_layout() #show plot with tight layout\n\n#uncomment below to save figure\nplt.savefig('ward_clusters2.png', dpi=200) #save figure as ward_clusters","4fe4252b":"from scipy.cluster.hierarchy import fcluster\n\nk = 5\nh_link_cluster_labels = fcluster(mergings, k, criterion='maxclust')\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\n#fig, ax = plt.subplots(figsize=(17, 9))\n\nax2 = sns.scatterplot(x = xs, y=ys, hue=h_link_cluster_labels, palette=\"Set2\",\n                      alpha=0.8, legend=\"full\")\nax2.set_title(\"Hierarchical clustering using complete linkage with 9 clusters\")\nax2.set_xlabel(\"First principal component\")\nax2.set_ylabel(\"Second principal component\")\n\nax1 = plot_silhouette_hierarchical(9, feature_matrix.todense(), h_link_cluster_labels, fig, ax1)\n\nplt.suptitle(\"Clustering user movie reviews using AGNES complete linkage with 5 clusters\",\n             fontsize=14, fontweight='bold')\nplt.show()","8c359f6e":"from scipy.cluster.hierarchy import fcluster\n\nk = 5\nh_link_cluster_labels = fcluster(mergings, k, criterion='maxclust')\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\n#fig, ax = plt.subplots(figsize=(17, 9))\n\nax2 = sns.scatterplot(x = xs2, y=ys2, hue=h_link_cluster_labels, palette=\"Set2\",\n                      alpha=0.8, legend=\"full\")\nax2.set_title(\"Hierarchical clustering using complete linkage with 9 clusters\")\nax2.set_xlabel(\"First principal component\")\nax2.set_ylabel(\"Second principal component\")\n\nax1 = plot_silhouette_hierarchical(9, feature_matrix2.todense(), h_link_cluster_labels, fig, ax1)\n\nplt.suptitle(\"Clustering user movie reviews using AGNES complete linkage with 5 clusters\",\n             fontsize=14, fontweight='bold')\nplt.show()","345fe519":"from scipy.cluster.hierarchy import ward\n\nlinkage_matrix = ward(similarity_distance)\n\n# Plot the dendrogram, using title as label column\ndendrogram_ = dendrogram(linkage_matrix,\n               labels=titles,\n               orientation=\"left\",\n               leaf_font_size=16,\n)\n\n# Adjust the plot\nfig = plt.gcf()\n_ = [lbl.set_color('r') for lbl in plt.gca().get_xmajorticklabels()]\nfig.set_size_inches(108, 21)\n\nplt.show()","90eea375":"from scipy.cluster.hierarchy import ward\n\nlinkage_matrix = ward(similarity_distance)\nk = 3\nh_ward_cluster_labels = fcluster(linkage_matrix, k, criterion='maxclust')\n\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18, 7)\n\n#fig, ax = plt.subplots(figsize=(17, 9))\n\nax2 = sns.scatterplot(x = xs, y=ys, hue=h_ward_cluster_labels, palette=\"Set2\",\n                      alpha=0.8, legend=\"full\")\nax2.set_title(\"Hierarchical clustering using ward linkage with 3 clusters\")\nax2.set_xlabel(\"First principal component\")\nax2.set_ylabel(\"Second principal component\")\n\nax1 = plot_silhouette_hierarchical(3, feature_matrix.todense(), h_ward_cluster_labels, fig, ax1)\n\nplt.suptitle(\"Clustering user movie reviews using AGNES ward linkage with 3 clusters\",\n             fontsize=14, fontweight='bold')\nplt.show()","2e692d79":"from sklearn import metrics\nk_means_5 = KMeans(n_clusters=5)\nmodel = k_means_5.fit(tfidf_matrix.toarray())\ny_hat_5 = k_means_5.predict(tfidf_matrix.toarray())\nlabels_5 = k_means_5.labels_\nmetrics.silhouette_score(tfidf_matrix, labels_5, metric = 'euclidean')\nmetrics.calinski_harabasz_score(tfidf_matrix.toarray(), labels_5)","c3fc5bd1":"# import neccessaries librariesimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import datasets\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples,silhouette_score","379d7362":"terms = tfidf_vectorizer.get_feature_names()","bc3b5122":"terms","335be055":"from sklearn.metrics.pairwise import cosine_similarity\ndist = 1 - cosine_similarity(tfidf_matrix)\nprint\nprint","ce55e31b":"\nfrom sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\n%time km.fit(tfidf_matrix)\n\nclusters = km.labels_.tolist()","c13b5759":"from sklearn.cluster import KMeans\nlines_for_predicting = [\"pencemaran nama baik!\", \"telanjang\"]\nkm.predict(tfidf_vectorizer.transform(lines_for_predicting))\n","9adf8512":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\n\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE","91ec9c46":"tfidf = TfidfVectorizer(\n    min_df = 5,\n    max_df = 0.95,\n    max_features = 8000\n)\ntfidf.fit(synopses)\ntext = tfidf.transform(synopses)","d967ad93":"def find_optimal_clusters(data, max_k):\n    iters = range(2, max_k+1, 1)\n    \n    sse = []\n    for k in iters:\n        sse.append(MiniBatchKMeans(n_clusters=k, init_size=1024, batch_size=2048, random_state=20).fit(data).inertia_)\n        print('Fit {} clusters'.format(k))\n        \n    f, ax = plt.subplots(1, 1)\n    ax.plot(iters, sse, marker='o')\n    ax.set_xlabel('Cluster Centers')\n    ax.set_xticks(iters)\n    ax.set_xticklabels(iters)\n    ax.set_ylabel('SSE')\n    ax.set_title('SSE by Cluster Center Plot')\n    \nfind_optimal_clusters(text,8)","51ff827e":"clusters = MiniBatchKMeans(n_clusters=5, init_size=1024, batch_size=2048, random_state=20).fit_predict(text)","11a56b0c":"def plot_tsne_pca(data, labels):\n    max_label = max(labels)\n    max_items = np.random.choice(range(data.shape[0]), size=3000, replace=True)\n    \n    pca = PCA(n_components=2).fit_transform(data[max_items,:].todense())\n    tsne = TSNE().fit_transform(PCA(n_components=50).fit_transform(data[max_items,:].todense()))\n    \n    \n    idx = np.random.choice(range(pca.shape[0]), size=300, replace=True)\n    label_subset = labels[max_items]\n    label_subset = [cm.hsv(i\/max_label) for i in label_subset[idx]]\n    \n    f, ax = plt.subplots(1, 2, figsize=(14, 6))\n    \n    ax[0].scatter(pca[idx, 0], pca[idx, 1], c=label_subset)\n    ax[0].set_title('PCA Cluster Plot')\n    \n    ax[1].scatter(tsne[idx, 0], tsne[idx, 1], c=label_subset)\n    ax[1].set_title('TSNE Cluster Plot')\n    \nplot_tsne_pca(text, clusters)\n    ","4bdc79c8":"def get_top_keywords(data, clusters, labels, n_terms):\n    df = pd.DataFrame(data.todense()).groupby(clusters).mean()\n    \n    for i,r in df.iterrows():\n        print('\\nCluster {}'.format(i))\n        print(','.join([labels[t] for t in np.argsort(r)[-n_terms:]]))\n            \nget_top_keywords(text, clusters, tfidf.get_feature_names(), 50)","9f24c4e9":"from sklearn.cluster import KMeans\n\nnum_clusters = 5\n\nkm2 = KMeans(n_clusters=num_clusters)\n\n%time km2.fit(tfidf_matrix2)\n\nclusters2 = km2.labels_.tolist()","4fd710d5":"#model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n#model.fit(X)\ntrue_k = 5\nprint(\"Top terms per cluster pertimbangan:\")\norder_centroids = km.cluster_centers_.argsort()[:, ::-1]\nterms = tfidf_vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :30]:\n        print(' %s' % terms[ind]),\n    print","45aa1dd1":"films = { 'title': titles,'synopsis': synopses, 'cluster': clusters }\n\nframe = pd.DataFrame(films, index = [clusters] , columns = [ 'title', 'cluster'])\nresult=pd.DataFrame(films, index = [clusters] , columns = [ 'title', 'cluster'])","330cda5d":"for k in range(0,5):\n   s=result[result.cluster==k]\n   text=synopses\n   #text=text.lower()\n   text=' '.join([word for word in text.split()])\n   wordcloud = WordCloud(max_font_size=50, max_words=100, background_color=\"white\").generate(text)\n   print('Cluster: {}'.format(k))\n   print('Titles')\n   titles=wiki_cl[wiki_cl.cluster==k]['title']         \n   print(titles.to_string(index=False))\n   plt.figure()\n   plt.imshow(wordcloud, interpolation=\"bilinear\")\n   plt.axis(\"off\")\n   plt.show()","e0fc8222":"#model = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\n#model.fit(X)\ntrue_k = 5\nprint(\"Top terms per cluster putusan:\")\norder_centroids2 = km2.cluster_centers_.argsort()[:, ::-1]\nterms2 = tfidf_vectorizer2.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids2[i, :30]:\n        print(' %s' % terms2[ind]),\n    print","2b3f30d6":"\nimport joblib\n\n#uncomment the below to save your model \n#since I've already run my model I am loading from the pickle\n\njoblib.dump(km,  'doc_cluster.pkl')\n\nkm = joblib.load('doc_cluster.pkl')\nclusters = km.labels_.tolist()","afb05656":"import joblib\n\n#uncomment the below to save your model \n#since I've already run my model I am loading from the pickle\n\njoblib.dump(km2,  'doc_cluster.pkl')\n\nkm2 = joblib.load('doc_cluster.pkl')\nclusters2 = km2.labels_.tolist()","10d0e4fd":"films = { 'title': titles,'synopsis': synopses, 'cluster': clusters }\n\nframe = pd.DataFrame(films, index = [clusters] , columns = [ 'title', 'cluster'])\nresult=pd.DataFrame(films, index = [clusters] , columns = [ 'title', 'cluster'])","06e8ddb3":"films2 = { 'title': titles,'synopsis': synopses2, 'cluster': clusters2 }\n\nframe2 = pd.DataFrame(films2, index = [clusters2] , columns = [ 'title', 'cluster'])","c3bbd633":"frame['title']","7a1bb658":"frame['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)","3e8e5afd":"frame2['cluster'].value_counts() #number of films per cluster (clusters from 0 to 4)","f35946ac":"import os  # for os.path.basename\n\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\n\nfrom sklearn.manifold import MDS\n\nMDS()\n\n# convert two components as we're plotting points in a two-dimensional plane\n# \"precomputed\" because we provide a distance matrix\n# we will also specify `random_state` so the plot is reproducible.\nmds = MDS(n_components=2, dissimilarity=\"precomputed\", random_state=1)\n\npos = mds.fit_transform(dist)  # shape (n_components, n_samples)\n\nxs, ys = pos[:, 0], pos[:, 1]\nprint()\nprint()","ec98a4c9":"#set up colors per clusters using a dict\ncluster_colors = {0: '#1b9e77', 1: '#d95f02', 2: '#7570b3', 3: '#e7298a', 4: '#66a61e'}\n\n#set up cluster names using a dict\ncluster_names = {0: 'Family, home, war', \n                 1: 'Police, killed, murders', \n                 2: 'Father, New York, brothers', \n                 3: 'Dance, singing, love', \n                 4: 'Killed, soldiers, captain'}","2466ee5b":"#some ipython magic to show the matplotlib plots inline\n%matplotlib inline \n\n#create data frame that has the result of the MDS plus the cluster numbers and titles\ndf = pd.DataFrame(dict(x=xs, y=ys, label=clusters, title=titles)) \n\n#group by cluster\ngroups = df.groupby('label')\n\n\n# set up plot\nfig, ax = plt.subplots(figsize=(17, 9)) # set size\nax.margins(0.05) # Optional, just adds 5% padding to the autoscaling\n\n#iterate through groups to layer the plot\n#note that I use the cluster_name and cluster_color dicts with the 'name' lookup to return the appropriate color\/label\nfor name, group in groups:\n    ax.plot(group.x, group.y, marker='o', linestyle='', ms=12, \n            label=cluster_names[name], color=cluster_colors[name], \n            mec='none')\n    ax.set_aspect('auto')\n    ax.tick_params(\\\n        axis= 'x',          # changes apply to the x-axis\n        which='both',      # both major and minor ticks are affected\n        bottom='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelbottom='off')\n    ax.tick_params(\\\n        axis= 'y',         # changes apply to the y-axis\n        which='both',      # both major and minor ticks are affected\n        left='off',      # ticks along the bottom edge are off\n        top='off',         # ticks along the top edge are off\n        labelleft='off')\n    \nax.legend(numpoints=1)  #show legend with only 1 point\n\n#add label in x,y position with the label as the film title\nfor i in range(len(df)):\n    ax.text(df.loc[i]['x'], df.loc[i]['y'], df.loc[i]['title'], size=8)  \n\n    \n    \nplt.show() #show the plot\n\n#uncomment the below to save the plot if need be\n#plt.savefig('clusters_small_noaxes.png', dpi=200)","01e80315":"from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n    \n   \n    \n    print(\"Cluster %d titles:\" % i, end='')\n    for title in frame.loc[i]['title'].values.tolist():\n        print(' %s,' % title, end='')\n    print() #add whitespace\n    print() #add whitespace\n    \nprint()\nprint()","5a5b0469":"from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km2.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n    \n   \n    \n    print(\"Cluster %d titles:\" % i, end='')\n    for title in frame2.loc[i]['title'].values.tolist():\n        print(' %s,' % title, end='')\n    print() #add whitespace\n    print() #add whitespace\n    \nprint()\nprint()","0ebf2740":"from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n     \n    \n    print(\"Cluster %d titles:\" % i, end='')\n    for title in frame2.loc[i]['title'].values.tolist():\n        print(' %s,' % title, end='')\n    print() #add whitespace\n    print() #add whitespace\n    \nprint()\nprint()","fd0f82d8":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n# Driver Code \nlst1 = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst1b = ['filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst2b = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt']\n\nlst3b = ['filepdf100107_NoRestriction.txt','filepdf108230_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf222162_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst4b = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst5b = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\nprint(intersection(lst1, lst1b)) \nprint(intersection(lst1, lst2b)) \nprint(intersection(lst1, lst3b)) \nprint(intersection(lst1, lst4b)) \nprint(intersection(lst1, lst5b)) ","f43fb416":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n  \n# Driver Code \nlst2 = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst1b = ['filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst2b = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt']\n\nlst3b = ['filepdf100107_NoRestriction.txt','filepdf108230_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf222162_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst4b = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst5b = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\nprint(intersection(lst2, lst1b)) \nprint(intersection(lst2, lst2b)) \nprint(intersection(lst2, lst3b)) \nprint(intersection(lst2, lst4b)) \nprint(intersection(lst2, lst5b)) ","5330478c":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n  \n# Driver Code \nlst3 = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\n\nlst1b = ['filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst2b = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt']\n\nlst3b = ['filepdf100107_NoRestriction.txt','filepdf108230_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf222162_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst4b = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst5b = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\nprint(intersection(lst3, lst1b)) \nprint(intersection(lst3, lst2b)) \nprint(intersection(lst3, lst3b)) \nprint(intersection(lst3, lst4b)) \nprint(intersection(lst3, lst5b)) ","e51df167":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n  \n# Driver Code \nlst4 = ['filepdf100107_NoRestriction.txt','filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst1b = ['filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst2b = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt']\n\nlst3b = ['filepdf100107_NoRestriction.txt','filepdf108230_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf222162_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst4b = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst5b = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\nprint(intersection(lst4, lst1b)) \nprint(intersection(lst4, lst2b)) \nprint(intersection(lst4, lst3b)) \nprint(intersection(lst4, lst4b)) \nprint(intersection(lst4, lst5b)) ","8f15819b":"def intersection(lst1, lst2): \n    return list(set(lst1) & set(lst2)) \n  \n# Driver Code \nlst5 = ['filepdf108230_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf222162_NoRestriction.txt']\n\nlst1b = ['filepdf101587_NoRestriction.txt','filepdf102324_NoRestriction.txt','filepdf106015_NoRestriction.txt','filepdf106752_NoRestriction.txt','filepdf117162_NoRestriction.txt','filepdf120845_NoRestriction.txt','filepdf123794_NoRestriction.txt','filepdf130429_NoRestriction.txt','filepdf13063_NoRestriction.txt','filepdf131903_NoRestriction.txt','filepdf132640_NoRestriction.txt','filepdf133377_NoRestriction.txt','filepdf134851_NoRestriction.txt','filepdf137822_NoRestriction.txt','filepdf14538_NoRestriction.txt','filepdf148896_NoRestriction.txt','filepdf151130_NoRestriction.txt','filepdf152604_NoRestriction.txt','filepdf15275_NoRestriction.txt','filepdf155555_NoRestriction.txt','filepdf158526_NoRestriction.txt','filepdf162953_NoRestriction.txt','filepdf165174_NoRestriction.txt','filepdf171819_NoRestriction.txt','filepdf174037_NoRestriction.txt','filepdf175511_NoRestriction.txt','filepdf176248_NoRestriction.txt','filepdf190384_NoRestriction.txt','filepdf191128_NoRestriction.txt','filepdf19698_NoRestriction.txt','filepdf1974_NoRestriction.txt','filepdf199264_NoRestriction.txt','filepdf200738_NoRestriction.txt','filepdf202212_NoRestriction.txt','filepdf202949_NoRestriction.txt','filepdf203692_NoRestriction.txt','filepdf20435_NoRestriction.txt','filepdf207389_NoRestriction.txt','filepdf208135_NoRestriction.txt','filepdf21172_NoRestriction.txt','filepdf211819_NoRestriction.txt','filepdf225116_NoRestriction.txt','filepdf228063_NoRestriction.txt']\n\nlst2b = ['filepdf103061_NoRestriction.txt','filepdf104535_NoRestriction.txt','filepdf110482_NoRestriction.txt','filepdf117899_NoRestriction.txt','filepdf121582_NoRestriction.txt','filepdf122319_NoRestriction.txt','filepdf1237_NoRestriction.txt','filepdf124531_NoRestriction.txt','filepdf125268_NoRestriction.txt','filepdf134114_NoRestriction.txt','filepdf135588_NoRestriction.txt','filepdf141512_NoRestriction.txt','filepdf142249_NoRestriction.txt','filepdf142986_NoRestriction.txt','filepdf151867_NoRestriction.txt','filepdf154080_NoRestriction.txt','filepdf157780_NoRestriction.txt','filepdf159263_NoRestriction.txt','filepdf160006_NoRestriction.txt','filepdf165911_NoRestriction.txt','filepdf166648_NoRestriction.txt','filepdf16748_NoRestriction.txt','filepdf170345_NoRestriction.txt','filepdf171082_NoRestriction.txt','filepdf17485_NoRestriction.txt','filepdf179195_NoRestriction.txt','filepdf181406_NoRestriction.txt','filepdf182880_NoRestriction.txt','filepdf192613_NoRestriction.txt','filepdf201475_NoRestriction.txt','filepdf204429_NoRestriction.txt','filepdf220683_NoRestriction.txt','filepdf221420_NoRestriction.txt']\n\nlst3b = ['filepdf100107_NoRestriction.txt','filepdf108230_NoRestriction.txt','filepdf10852_NoRestriction.txt','filepdf108985_NoRestriction.txt','filepdf11589_NoRestriction.txt','filepdf116425_NoRestriction.txt','filepdf118636_NoRestriction.txt','filepdf120108_NoRestriction.txt','filepdf123056_NoRestriction.txt','filepdf12326_NoRestriction.txt','filepdf126005_NoRestriction.txt','filepdf126747_NoRestriction.txt','filepdf127479_NoRestriction.txt','filepdf131166_NoRestriction.txt','filepdf137074_NoRestriction.txt','filepdf13800_NoRestriction.txt','filepdf138559_NoRestriction.txt','filepdf139296_NoRestriction.txt','filepdf143723_NoRestriction.txt','filepdf147402_NoRestriction.txt','filepdf148139_NoRestriction.txt','filepdf150393_NoRestriction.txt','filepdf153343_NoRestriction.txt','filepdf156298_NoRestriction.txt','filepdf164434_NoRestriction.txt','filepdf167385_NoRestriction.txt','filepdf168122_NoRestriction.txt','filepdf168867_NoRestriction.txt','filepdf169605_NoRestriction.txt','filepdf180669_NoRestriction.txt','filepdf182143_NoRestriction.txt','filepdf18222_NoRestriction.txt','filepdf184372_NoRestriction.txt','filepdf185109_NoRestriction.txt','filepdf186631_NoRestriction.txt','filepdf188117_NoRestriction.txt','filepdf191876_NoRestriction.txt','filepdf195567_NoRestriction.txt','filepdf196304_NoRestriction.txt','filepdf197046_NoRestriction.txt','filepdf197790_NoRestriction.txt','filepdf198527_NoRestriction.txt','filepdf200001_NoRestriction.txt','filepdf206652_NoRestriction.txt','filepdf208872_NoRestriction.txt','filepdf212556_NoRestriction.txt','filepdf213293_NoRestriction.txt','filepdf214775_NoRestriction.txt','filepdf215505_NoRestriction.txt','filepdf216249_NoRestriction.txt','filepdf218472_NoRestriction.txt','filepdf219209_NoRestriction.txt','filepdf219946_NoRestriction.txt','filepdf222162_NoRestriction.txt','filepdf224379_NoRestriction.txt']\n\nlst4b = ['filepdf113446_NoRestriction.txt','filepdf129692_NoRestriction.txt','filepdf145933_NoRestriction.txt','filepdf162216_NoRestriction.txt','filepdf178458_NoRestriction.txt','filepdf194823_NoRestriction.txt','filepdf211082_NoRestriction.txt','filepdf227326_NoRestriction.txt']\n\nlst5b = ['filepdf111972_NoRestriction.txt','filepdf128218_NoRestriction.txt','filepdf144459_NoRestriction.txt','filepdf160742_NoRestriction.txt','filepdf176984_NoRestriction.txt','filepdf183617_NoRestriction.txt','filepdf193349_NoRestriction.txt','filepdf209608_NoRestriction.txt','filepdf225852_NoRestriction.txt']\nprint(intersection(lst5, lst1b)) \nprint(intersection(lst5, lst2b)) \nprint(intersection(lst5, lst3b)) \nprint(intersection(lst5, lst4b)) \nprint(intersection(lst5, lst5b)) ","09679900":"lst1 = ['filepdf113446_NoRestriction.txt',' filepdf129692_NoRestriction.txt',' filepdf145933_NoRestriction.txt',' filepdf162216_NoRestriction.txt',' filepdf178458_NoRestriction.txt',' filepdf194823_NoRestriction.txt',' filepdf211082_NoRestriction.txt',' filepdf227326_NoRestriction.txt'] \nprint(lst1)","2f886276":"from __future__ import print_function\n\nprint(\"Top terms per cluster:\")\nprint()\n#sort cluster centers by proximity to centroid\norder_centroids = km.cluster_centers_.argsort()[:, ::-1] \n\nfor i in range(num_clusters):\n    print(\"Cluster %d words:\" % i, end='')\n    \n    \n    \n    print(\"Cluster %d titles:\" % i, end='')\n    \n    \nprint()\nprint()","3ba31806":"#define vectorizer parameters yang pertimbangan\ntfidf_vectorizer = TfidfVectorizer(max_df=0.8, max_features=20000,\n                                 min_df=0.2, use_idf=True,  ngram_range=(1,3))\n\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['pertimbangan']) #fit the vectorizer to synopses\n\n","8d7e50da":"\n\n\nprint(tfidf_matrix.shape)\nnum_clusters = 5\n\nkm = KMeans(n_clusters=num_clusters)\n\nkm.fit(tfidf_matrix)\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(tfidf_matrix)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(tfidf_matrix, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))\n\n\n\nprint()","b06a575f":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\nfeatures = tfidf.fit_transform(df['pertimbangan']).toarray()\n","d4530b3c":"for data in df['pertimbangan']:\n    hasil=data.find(\"nama baik\")\n    if (hasil>=0):\n        print(hasil)","7010fe49":"df['category_id'] = df['label'].factorize()[0]\ncategory_id_df = df[['label', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id = dict(category_id_df.values)\nid_to_category = dict(category_id_df[['category_id', 'label']].values)\ndf.head()\n","e8d5ec84":"df","e5c3592a":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ndf.groupby('label').pertimbangan.count().plot.bar(ylim=0)\nplt.show()","97002949":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\nfeatures = tfidf.fit_transform(df.pertimbangan).toarray()\nlabels = df.category_id\nfeatures.shape","14553d3f":"from sklearn.feature_selection import chi2\nimport numpy as np\nN = 2\nfor label, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"# '{}':\".format(label))\n  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","a43a694c":"##error code\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.naive_bayes import MultinomialNB\nX_train, X_test, y_train, y_test = train_test_split(df['pertimbangan'], df['label'], random_state = 0)\ncount_vect = CountVectorizer()\nX_train_counts = count_vect.fit_transform(X_train)\ntfidf_transformer = TfidfTransformer()\nX_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\nclf = MultinomialNB().fit(X_train_tfidf, y_train)\n\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\n\n","1157dec7":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n\nX= df['pertimbangan']\ny = df['label']  \ncv = CountVectorizer()\nX = cv.fit_transform(X) # Fit the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=40)\n#Naive Bayes Classifier\nclf = MultinomialNB()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","f111db7d":"from sklearn.metrics import confusion_matrix\nX= df['pertimbangan']\ny = df['label']  \ncv = CountVectorizer()\nX = cv.fit_transform(X) # Fit the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n#Naive Bayes Classifier\nclf = MultinomialNB()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pred = clf.predict(X_test)\n\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\nplt.savefig('multibayes.png')","4e6806a3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=40),\n    LinearSVC(random_state=40),\n    MultinomialNB(),\n    LogisticRegression(random_state=40),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","2d98189f":"cv_df.groupby('model_name').accuracy.mean()","485f9d01":"model = LinearSVC()\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.30, random_state=5)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()\nplt.savefig('svm.png')","34168c22":"#Naive Bayes Classifier\nclf = LinearSVC()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))","84c97457":"from IPython.display import display\nfor predicted in category_id_df.category_id:\n  for actual in category_id_df.category_id:\n    if predicted != actual and conf_mat[actual, predicted] >= 10:\n      print(\"'{}' predicted as '{}' : {} examples.\".format(id_to_category[actual], id_to_category[predicted], conf_mat[actual, predicted]))\n      display(df.loc[indices_test[(y_test == actual) & (y_pred == predicted)]][['label', 'pertimbangan']])\n      print('')","442255e5":"model.fit(features, labels)\nN = 2\nfor label, category_id in sorted(category_to_id.items()):\n  indices = np.argsort(model.coef_[category_id])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 1][:N]\n  bigrams = [v for v in reversed(feature_names) if len(v.split(' ')) == 2][:N]\n  print(\"# '{}':\".format(label))\n  print(\"  . Top unigrams:\\n       . {}\".format('\\n       . '.join(unigrams)))\n  print(\"  . Top bigrams:\\n       . {}\".format('\\n       . '.join(bigrams)))","aa5b8302":"from sklearn import metrics\nprint(metrics.classification_report(y_test, y_pred, target_names=df['label'].unique()))","133bb916":"df2=df.loc[df['label'] == 'kesusilaan']","ab866439":"df2=df2.append(df.loc[df['label'] == 'nama baik'])","b4a60f99":"df2=df2.append(df.loc[df['label'] == 'sara'])","de46d362":"print(df2)","29eb363f":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize=(8,6))\ndf2.groupby('label').pertimbangan.count().plot.bar(ylim=0)\nplt.show()","4f106ab8":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2))\nfeatures = tfidf.fit_transform(df2.pertimbangan).toarray()\nlabels = df2.category_id\nfeatures.shape","783412e2":"from sklearn.feature_selection import chi2\nimport numpy as np\nN = 2\nfor label, category_id in sorted(category_to_id.items()):\n  features_chi2 = chi2(features, labels == category_id)\n  indices = np.argsort(features_chi2[0])\n  feature_names = np.array(tfidf.get_feature_names())[indices]\n  unigrams = [v for v in feature_names if len(v.split(' ')) == 1]\n  bigrams = [v for v in feature_names if len(v.split(' ')) == 2]\n  print(\"# '{}':\".format(label))\n  print(\"  . Most correlated unigrams:\\n. {}\".format('\\n. '.join(unigrams[-N:])))\n  print(\"  . Most correlated bigrams:\\n. {}\".format('\\n. '.join(bigrams[-N:])))","956c2c6d":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import classification_report\n\n\nX= df2['pertimbangan']\ny = df2['label']  \ncv = CountVectorizer()\nX = cv.fit_transform(X) # Fit the Data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n#Naive Bayes Classifier\nclf = MultinomialNB()\nclf.fit(X_train,y_train)\nclf.score(X_test,y_test)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_test, y_pred))\n","cfe023f0":"conf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df2.label.values, yticklabels=category_id_df2.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","da025d2b":"model = LinearSVC()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df.label.values, yticklabels=category_id_df.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","6a36ff61":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nmodels = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n  model_name = model.__class__.__name__\n  accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n  for fold_idx, accuracy in enumerate(accuracies):\n    entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\nimport seaborn as sns\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","e6c08b5a":"cv_df.groupby('model_name').accuracy.mean()","7985d7a1":"category_id_df2 = df[['label', 'category_id']].drop_duplicates().sort_values('category_id')\ncategory_to_id2 = dict(category_id_df2.values)\nid_to_category2 = dict(category_id_df[['category_id', 'label']].values)\ndf.head()","bb9b6ad1":"model = LinearSVC()\nX_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df2.index, test_size=0.33, random_state=0)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(conf_mat, annot=True, fmt='d',\n            xticklabels=category_id_df2.label.values, yticklabels=category_id_df2.label.values)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","94c66f46":"Cluster0putusan=[ 'filepdf100107_NoRestriction.pdf', 'filepdf101587_NoRestriction.pdf', 'filepdf102324.pdf', 'filepdf102324_NoRestriction.pdf', 'filepdf103061.pdf', 'filepdf103061_NoRestriction.pdf', 'filepdf104535.pdf', 'filepdf104535_NoRestriction.pdf', 'filepdf106015.pdf', 'filepdf106015_NoRestriction.pdf', 'filepdf106752_NoRestriction.pdf', 'filepdf108230.pdf', 'filepdf108230_NoRestriction.pdf', 'filepdf10852.pdf', 'filepdf10852_NoRestriction.pdf', 'filepdf108985.pdf', 'filepdf108985_NoRestriction.pdf', 'filepdf110482.pdf', 'filepdf110482_NoRestriction.pdf', 'filepdf111972.pdf', 'filepdf111972_NoRestriction.pdf', 'filepdf113446.pdf', 'filepdf113446_NoRestriction.pdf', 'filepdf11589.pdf', 'filepdf11589_NoRestriction.pdf', 'filepdf116425.pdf', 'filepdf116425_NoRestriction.pdf', 'filepdf117162.pdf', 'filepdf117162_NoRestriction.pdf', 'filepdf117899.pdf', 'filepdf117899_NoRestriction.pdf', 'filepdf118636.pdf', 'filepdf118636_NoRestriction.pdf', 'filepdf120108.pdf', 'filepdf120108_NoRestriction.pdf', 'filepdf120845.pdf', 'filepdf120845_NoRestriction.pdf', 'filepdf121577.pdf', 'filepdf121582.pdf', 'filepdf121582_NoRestriction.pdf', 'filepdf122314.pdf', 'filepdf122319.pdf', 'filepdf122319_NoRestriction.pdf', 'filepdf123056_NoRestriction.pdf', 'filepdf1232.pdf', 'filepdf12321.pdf', 'filepdf12326.pdf', 'filepdf12326_NoRestriction.pdf', 'filepdf1237.pdf', 'filepdf123789.pdf', 'filepdf123794.pdf', 'filepdf123794_NoRestriction.pdf', 'filepdf1237_NoRestriction.pdf', 'filepdf124526.pdf', 'filepdf124531_NoRestriction.pdf', 'filepdf125263.pdf', 'filepdf125268.pdf', 'filepdf125268_NoRestriction.pdf', 'filepdf126000.pdf', 'filepdf126005.pdf', 'filepdf126005_NoRestriction.pdf', 'filepdf126747.pdf', 'filepdf126747_NoRestriction.pdf', 'filepdf127474.pdf', 'filepdf127479.pdf', 'filepdf127479_NoRestriction.pdf', 'filepdf128213.pdf', 'filepdf128218.pdf', 'filepdf128218_NoRestriction.pdf', 'filepdf129687.pdf', 'filepdf129692.pdf', 'filepdf129692_NoRestriction.pdf', 'filepdf130424.pdf', 'filepdf130429.pdf', 'filepdf130429_NoRestriction.pdf', 'filepdf13058.pdf', 'filepdf13063.pdf', 'filepdf13063_NoRestriction.pdf', 'filepdf131161.pdf', 'filepdf131166.pdf', 'filepdf131166_NoRestriction.pdf', 'filepdf131898.pdf', 'filepdf131903.pdf', 'filepdf131903_NoRestriction.pdf', 'filepdf132635.pdf', 'filepdf132640.pdf', 'filepdf132640_NoRestriction.pdf', 'filepdf133372.pdf', 'filepdf133377.pdf', 'filepdf133377_NoRestriction.pdf', 'filepdf134109.pdf', 'filepdf134114.pdf', 'filepdf134114_NoRestriction.pdf', 'filepdf134846.pdf', 'filepdf134851.pdf', 'filepdf134851_NoRestriction.pdf', 'filepdf135583.pdf', 'filepdf135588.pdf', 'filepdf135588_NoRestriction.pdf', 'filepdf137074.pdf', 'filepdf137074_NoRestriction.pdf', 'filepdf137817.pdf', 'filepdf137822.pdf', 'filepdf137822_NoRestriction.pdf', 'filepdf13795.pdf', 'filepdf13800.pdf', 'filepdf13800_NoRestriction.pdf', 'filepdf138554.pdf', 'filepdf138559.pdf', 'filepdf138559_NoRestriction.pdf', 'filepdf139291.pdf', 'filepdf139296.pdf', 'filepdf139296_NoRestriction.pdf', 'filepdf141507.pdf', 'filepdf141512.pdf', 'filepdf141512_NoRestriction.pdf', 'filepdf142244.pdf', 'filepdf142249.pdf', 'filepdf142249_NoRestriction.pdf', 'filepdf142981.pdf', 'filepdf142986.pdf', 'filepdf142986_NoRestriction.pdf', 'filepdf143718.pdf', 'filepdf143723.pdf', 'filepdf143723_NoRestriction.pdf', 'filepdf144454.pdf', 'filepdf144459.pdf', 'filepdf144459_NoRestriction.pdf', 'filepdf14533.pdf', 'filepdf14538.pdf', 'filepdf14538_NoRestriction.pdf', 'filepdf145928.pdf', 'filepdf145933.pdf', 'filepdf145933_NoRestriction.pdf', 'filepdf147397.pdf', 'filepdf147402.pdf', 'filepdf147402_NoRestriction.pdf', 'filepdf148134.pdf', 'filepdf148139.pdf', 'filepdf148139_NoRestriction.pdf', 'filepdf148896.pdf', 'filepdf148896_NoRestriction.pdf', 'filepdf150393.pdf', 'filepdf150393_NoRestriction.pdf', 'filepdf151130.pdf', 'filepdf151130_NoRestriction.pdf', 'filepdf151862.pdf', 'filepdf151867.pdf', 'filepdf151867_NoRestriction.pdf', 'filepdf152599.pdf', 'filepdf152604.pdf', 'filepdf152604_NoRestriction.pdf', 'filepdf15270.pdf', 'filepdf15275.pdf', 'filepdf15275_NoRestriction.pdf', 'filepdf153338.pdf', 'filepdf153343.pdf', 'filepdf153343_NoRestriction.pdf', 'filepdf154075.pdf', 'filepdf154080.pdf', 'filepdf154080_NoRestriction.pdf', 'filepdf155550.pdf', 'filepdf155555.pdf', 'filepdf155555_NoRestriction.pdf', 'filepdf156298.pdf', 'filepdf156298_NoRestriction.pdf', 'filepdf157775.pdf', 'filepdf157780.pdf', 'filepdf157780_NoRestriction.pdf', 'filepdf158521.pdf', 'filepdf158526.pdf', 'filepdf158526_NoRestriction.pdf', 'filepdf159263_NoRestriction.pdf', 'filepdf160006_NoRestriction.pdf', 'filepdf160737.pdf', 'filepdf160742.pdf', 'filepdf160742_NoRestriction.pdf', 'filepdf162211.pdf', 'filepdf162216.pdf', 'filepdf162216_NoRestriction.pdf', 'filepdf162948.pdf', 'filepdf162953.pdf', 'filepdf162953_NoRestriction.pdf', 'filepdf164429.pdf', 'filepdf164434.pdf', 'filepdf164434_NoRestriction.pdf', 'filepdf165169.pdf', 'filepdf165174.pdf', 'filepdf165174_NoRestriction.pdf', 'filepdf165906.pdf', 'filepdf165911.pdf', 'filepdf165911_NoRestriction.pdf', 'filepdf166643.pdf', 'filepdf166648.pdf', 'filepdf166648_NoRestriction.pdf', 'filepdf167385_NoRestriction.pdf', 'filepdf16748_NoRestriction.pdf', 'filepdf168117.pdf', 'filepdf168122.pdf', 'filepdf168122_NoRestriction.pdf', 'filepdf168862.pdf', 'filepdf168867.pdf', 'filepdf168867_NoRestriction.pdf', 'filepdf169600.pdf', 'filepdf169605_NoRestriction.pdf', 'filepdf170340.pdf', 'filepdf170345.pdf', 'filepdf170345_NoRestriction.pdf', 'filepdf171077.pdf', 'filepdf171082.pdf', 'filepdf171082_NoRestriction.pdf', 'filepdf171814.pdf', 'filepdf171819_NoRestriction.pdf', 'filepdf173300_NoRestriction.pdf', 'filepdf173300_NoRestriction.txt', 'filepdf174032.pdf', 'filepdf174037_NoRestriction.pdf', 'filepdf17480.pdf', 'filepdf17485.pdf', 'filepdf17485_NoRestriction.pdf', 'filepdf175506.pdf', 'filepdf175511.pdf', 'filepdf175511_NoRestriction.pdf', 'filepdf176243.pdf', 'filepdf176248.pdf', 'filepdf176248_NoRestriction.pdf', 'filepdf176979.pdf', 'filepdf176984.pdf', 'filepdf176984_NoRestriction.pdf', 'filepdf178453.pdf', 'filepdf178458.pdf', 'filepdf178458_NoRestriction.pdf', 'filepdf179190.pdf', 'filepdf179195.pdf', 'filepdf179195_NoRestriction.pdf', 'filepdf180664.pdf', 'filepdf180669.pdf', 'filepdf180669_NoRestriction.pdf', 'filepdf181401.pdf', 'filepdf181406_NoRestriction.pdf', 'filepdf182138.pdf', 'filepdf182143.pdf', 'filepdf182143_NoRestriction.pdf', 'filepdf18217.pdf', 'filepdf18222.pdf', 'filepdf18222_NoRestriction.pdf', 'filepdf182875.pdf', 'filepdf182880.pdf', 'filepdf182880_NoRestriction.pdf', 'filepdf183612.pdf', 'filepdf183617.pdf', 'filepdf183617_NoRestriction.pdf', 'filepdf184367.pdf', 'filepdf184372.pdf', 'filepdf184372_NoRestriction.pdf', 'filepdf185109.pdf', 'filepdf185109_NoRestriction.pdf', 'filepdf186626.pdf', 'filepdf186631.pdf', 'filepdf186631_NoRestriction.pdf', 'filepdf188117.pdf', 'filepdf188117_NoRestriction.pdf', 'filepdf190379.pdf', 'filepdf190384.pdf', 'filepdf190384_NoRestriction.pdf', 'filepdf191123.pdf', 'filepdf191128.pdf', 'filepdf191128_NoRestriction.pdf', 'filepdf191871.pdf', 'filepdf191876.pdf', 'filepdf191876_NoRestriction.pdf', 'filepdf192608.pdf', 'filepdf192613.pdf', 'filepdf192613_NoRestriction.pdf', 'filepdf193344.pdf', 'filepdf193349.pdf', 'filepdf193349_NoRestriction.pdf', 'filepdf194818.pdf', 'filepdf194823.pdf', 'filepdf194823_NoRestriction.pdf', 'filepdf195562.pdf', 'filepdf195567.pdf', 'filepdf195567_NoRestriction.pdf', 'filepdf196299.pdf', 'filepdf196304.pdf', 'filepdf196304_NoRestriction.pdf', 'filepdf19698_NoRestriction.pdf', 'filepdf197041.pdf', 'filepdf197046.pdf', 'filepdf197046_NoRestriction.pdf', 'filepdf1974.pdf', 'filepdf1974_NoRestriction.pdf', 'filepdf197790_NoRestriction.pdf', 'filepdf198527.pdf', 'filepdf198527_NoRestriction.pdf', 'filepdf199259.pdf', 'filepdf199264.pdf', 'filepdf199264_NoRestriction.pdf', 'filepdf199996.pdf', 'filepdf200001.pdf', 'filepdf200001_NoRestriction.pdf', 'filepdf200733.pdf', 'filepdf200738.pdf', 'filepdf200738_NoRestriction.pdf', 'filepdf201470.pdf', 'filepdf201475.pdf', 'filepdf201475_NoRestriction.pdf', 'filepdf202207.pdf', 'filepdf202212.pdf', 'filepdf202212_NoRestriction.pdf', 'filepdf202944.pdf', 'filepdf202949.pdf', 'filepdf202949_NoRestriction.pdf', 'filepdf203692.pdf', 'filepdf203692_NoRestriction.pdf', 'filepdf20430.pdf', 'filepdf20435.pdf', 'filepdf20435_NoRestriction.pdf', 'filepdf204429_NoRestriction.pdf', 'filepdf206652.pdf', 'filepdf206652_NoRestriction.pdf', 'filepdf207384.pdf', 'filepdf207389.pdf', 'filepdf207389_NoRestriction.pdf', 'filepdf208130.pdf', 'filepdf208135.pdf', 'filepdf208135_NoRestriction.pdf', 'filepdf208867.pdf', 'filepdf208872.pdf', 'filepdf208872_NoRestriction.pdf', 'filepdf209603.pdf', 'filepdf209608.pdf', 'filepdf209608_NoRestriction.pdf', 'filepdf211077.pdf', 'filepdf211082.pdf', 'filepdf211082_NoRestriction.pdf', 'filepdf21167.pdf', 'filepdf21172.pdf', 'filepdf21172_NoRestriction.pdf', 'filepdf211819_NoRestriction.pdf', 'filepdf212551.pdf', 'filepdf212556.pdf', 'filepdf212556_NoRestriction.pdf', 'filepdf213288.pdf', 'filepdf213293.pdf', 'filepdf213293_NoRestriction.pdf', 'filepdf214770.pdf', 'filepdf214775.pdf', 'filepdf214775_NoRestriction.pdf', 'filepdf215505_NoRestriction.pdf', 'filepdf216249_NoRestriction.pdf', 'filepdf218467.pdf', 'filepdf218472.pdf', 'filepdf218472_NoRestriction.pdf', 'filepdf219204.pdf', 'filepdf219209.pdf', 'filepdf219209_NoRestriction.pdf', 'filepdf219946_NoRestriction.pdf', 'filepdf220678.pdf', 'filepdf220683.pdf', 'filepdf220683_NoRestriction.pdf']\n\nCluster1putusan=[ 'filepdf100107_NoRestriction.txt', 'filepdf108230_NoRestriction.txt', 'filepdf108985_NoRestriction.txt', 'filepdf116425_NoRestriction.txt', 'filepdf126747_NoRestriction.txt', 'filepdf127479_NoRestriction.txt', 'filepdf134851_NoRestriction.txt', 'filepdf137074_NoRestriction.txt', 'filepdf137822_NoRestriction.txt', 'filepdf148896_NoRestriction.txt', 'filepdf156298_NoRestriction.txt', 'filepdf160006_NoRestriction.txt', 'filepdf168867_NoRestriction.txt', 'filepdf184372_NoRestriction.txt', 'filepdf185109_NoRestriction.txt', 'filepdf186631_NoRestriction.txt', 'filepdf188117_NoRestriction.txt', 'filepdf191128_NoRestriction.txt', 'filepdf195567_NoRestriction.txt', 'filepdf197046_NoRestriction.txt', 'filepdf197790_NoRestriction.txt', 'filepdf203692_NoRestriction.txt', 'filepdf206652_NoRestriction.txt', 'filepdf208135_NoRestriction.txt', 'filepdf218472_NoRestriction.txt']\n\nCluster2putusan=[ 'filepdf101587_NoRestriction.txt', 'filepdf117162_NoRestriction.txt', 'filepdf123794_NoRestriction.txt', 'filepdf130429_NoRestriction.txt', 'filepdf14538_NoRestriction.txt', 'filepdf155555_NoRestriction.txt', 'filepdf165174_NoRestriction.txt', 'filepdf175511_NoRestriction.txt', 'filepdf176248_NoRestriction.txt', 'filepdf190384_NoRestriction.txt', 'filepdf1974_NoRestriction.txt', 'filepdf202212_NoRestriction.txt', 'filepdf202949_NoRestriction.txt', 'filepdf20435_NoRestriction.txt', 'filepdf211819_NoRestriction.txt']\n\nCluster3putusan=[ 'filepdf102324_NoRestriction.txt', 'filepdf104535_NoRestriction.txt', 'filepdf106015_NoRestriction.txt', 'filepdf10852_NoRestriction.txt', 'filepdf110482_NoRestriction.txt', 'filepdf11589_NoRestriction.txt', 'filepdf12326_NoRestriction.txt', 'filepdf125268_NoRestriction.txt', 'filepdf126005_NoRestriction.txt', 'filepdf13063_NoRestriction.txt', 'filepdf131166_NoRestriction.txt', 'filepdf131903_NoRestriction.txt', 'filepdf134114_NoRestriction.txt', 'filepdf13800_NoRestriction.txt', 'filepdf147402_NoRestriction.txt', 'filepdf148139_NoRestriction.txt', 'filepdf150393_NoRestriction.txt', 'filepdf15275_NoRestriction.txt', 'filepdf153343_NoRestriction.txt', 'filepdf162953_NoRestriction.txt', 'filepdf164434_NoRestriction.txt', 'filepdf165911_NoRestriction.txt', 'filepdf167385_NoRestriction.txt', 'filepdf180669_NoRestriction.txt', 'filepdf182143_NoRestriction.txt', 'filepdf18222_NoRestriction.txt', 'filepdf183617_NoRestriction.txt', 'filepdf191876_NoRestriction.txt', 'filepdf192613_NoRestriction.txt', 'filepdf196304_NoRestriction.txt', 'filepdf198527_NoRestriction.txt', 'filepdf199264_NoRestriction.txt', 'filepdf200001_NoRestriction.txt', 'filepdf201475_NoRestriction.txt', 'filepdf207389_NoRestriction.txt', 'filepdf21172_NoRestriction.txt', 'filepdf212556_NoRestriction.txt', 'filepdf215505_NoRestriction.txt', 'filepdf219209_NoRestriction.txt']\n\n\nCluster4putusan=[ 'filepdf103061_NoRestriction.txt', 'filepdf106752_NoRestriction.txt', 'filepdf117899_NoRestriction.txt', 'filepdf118636_NoRestriction.txt', 'filepdf120108_NoRestriction.txt', 'filepdf120845_NoRestriction.txt', 'filepdf121582_NoRestriction.txt', 'filepdf122319_NoRestriction.txt', 'filepdf123056_NoRestriction.txt', 'filepdf1237_NoRestriction.txt', 'filepdf124531_NoRestriction.txt', 'filepdf132640_NoRestriction.txt', 'filepdf133377_NoRestriction.txt', 'filepdf135588_NoRestriction.txt', 'filepdf138559_NoRestriction.txt', 'filepdf139296_NoRestriction.txt', 'filepdf141512_NoRestriction.txt', 'filepdf142249_NoRestriction.txt', 'filepdf142986_NoRestriction.txt', 'filepdf143723_NoRestriction.txt', 'filepdf151130_NoRestriction.txt', 'filepdf151867_NoRestriction.txt', 'filepdf152604_NoRestriction.txt', 'filepdf154080_NoRestriction.txt', 'filepdf157780_NoRestriction.txt', 'filepdf158526_NoRestriction.txt', 'filepdf159263_NoRestriction.txt', 'filepdf166648_NoRestriction.txt', 'filepdf16748_NoRestriction.txt', 'filepdf168122_NoRestriction.txt', 'filepdf169605_NoRestriction.txt', 'filepdf170345_NoRestriction.txt', 'filepdf171082_NoRestriction.txt', 'filepdf171819_NoRestriction.txt', 'filepdf174037_NoRestriction.txt', 'filepdf17485_NoRestriction.txt', 'filepdf179195_NoRestriction.txt', 'filepdf181406_NoRestriction.txt', 'filepdf182880_NoRestriction.txt', 'filepdf19698_NoRestriction.txt', 'filepdf200738_NoRestriction.txt', 'filepdf204429_NoRestriction.txt', 'filepdf208872_NoRestriction.txt', 'filepdf213293_NoRestriction.txt', 'filepdf214775_NoRestriction.txt', 'filepdf216249_NoRestriction.txt', 'filepdf219946_NoRestriction.txt']\n\n\nCluster5putusan=[ 'filepdf111972_NoRestriction.txt', 'filepdf128218_NoRestriction.txt', 'filepdf144459_NoRestriction.txt', 'filepdf160742_NoRestriction.txt', 'filepdf176984_NoRestriction.txt', 'filepdf193349_NoRestriction.txt', 'filepdf209608_NoRestriction.txt']\n\nCluster6putusan=[ 'filepdf113446_NoRestriction.txt', 'filepdf129692_NoRestriction.txt', 'filepdf145933_NoRestriction.txt', 'filepdf162216_NoRestriction.txt', 'filepdf178458_NoRestriction.txt', 'filepdf194823_NoRestriction.txt', 'filepdf211082_NoRestriction.txt']","a639fbf4":"Cluster0timbang=[ 'filepdf100107_NoRestriction.pdf', 'filepdf101587_NoRestriction.pdf', 'filepdf102324.pdf', 'filepdf102324_NoRestriction.pdf', 'filepdf103061.pdf', 'filepdf103061_NoRestriction.pdf', 'filepdf104535.pdf', 'filepdf104535_NoRestriction.pdf', 'filepdf106015.pdf', 'filepdf106015_NoRestriction.pdf', 'filepdf106752_NoRestriction.pdf', 'filepdf108230.pdf', 'filepdf108230_NoRestriction.pdf', 'filepdf10852.pdf', 'filepdf10852_NoRestriction.pdf', 'filepdf108985.pdf', 'filepdf108985_NoRestriction.pdf', 'filepdf110482.pdf', 'filepdf110482_NoRestriction.pdf', 'filepdf111972.pdf', 'filepdf111972_NoRestriction.pdf', 'filepdf113446.pdf', 'filepdf113446_NoRestriction.pdf', 'filepdf11589.pdf', 'filepdf11589_NoRestriction.pdf', 'filepdf116425.pdf', 'filepdf116425_NoRestriction.pdf', 'filepdf117162.pdf', 'filepdf117162_NoRestriction.pdf', 'filepdf117899.pdf', 'filepdf117899_NoRestriction.pdf', 'filepdf118636.pdf', 'filepdf118636_NoRestriction.pdf', 'filepdf120108.pdf', 'filepdf120108_NoRestriction.pdf', 'filepdf120845.pdf', 'filepdf120845_NoRestriction.pdf', 'filepdf121577.pdf', 'filepdf121582.pdf', 'filepdf121582_NoRestriction.pdf', 'filepdf122314.pdf', 'filepdf122319.pdf', 'filepdf122319_NoRestriction.pdf', 'filepdf123056_NoRestriction.pdf', 'filepdf1232.pdf', 'filepdf12321.pdf', 'filepdf12326.pdf', 'filepdf12326_NoRestriction.pdf', 'filepdf1237.pdf', 'filepdf123789.pdf', 'filepdf123794.pdf', 'filepdf123794_NoRestriction.pdf', 'filepdf1237_NoRestriction.pdf', 'filepdf124526.pdf', 'filepdf124531_NoRestriction.pdf', 'filepdf125263.pdf', 'filepdf125268.pdf', 'filepdf125268_NoRestriction.pdf', 'filepdf126000.pdf', 'filepdf126005.pdf', 'filepdf126005_NoRestriction.pdf', 'filepdf126747.pdf', 'filepdf126747_NoRestriction.pdf', 'filepdf127474.pdf', 'filepdf127479.pdf', 'filepdf127479_NoRestriction.pdf', 'filepdf128213.pdf', 'filepdf128218.pdf', 'filepdf128218_NoRestriction.pdf', 'filepdf129687.pdf', 'filepdf129692.pdf', 'filepdf129692_NoRestriction.pdf', 'filepdf130424.pdf', 'filepdf130429.pdf', 'filepdf130429_NoRestriction.pdf', 'filepdf13058.pdf', 'filepdf13063.pdf', 'filepdf13063_NoRestriction.pdf', 'filepdf131161.pdf', 'filepdf131166.pdf', 'filepdf131166_NoRestriction.pdf', 'filepdf131898.pdf', 'filepdf131903.pdf', 'filepdf131903_NoRestriction.pdf', 'filepdf132635.pdf', 'filepdf132640.pdf', 'filepdf132640_NoRestriction.pdf', 'filepdf133372.pdf', 'filepdf133377.pdf', 'filepdf133377_NoRestriction.pdf', 'filepdf134109.pdf', 'filepdf134114.pdf', 'filepdf134114_NoRestriction.pdf', 'filepdf134846.pdf', 'filepdf134851.pdf', 'filepdf134851_NoRestriction.pdf', 'filepdf135583.pdf', 'filepdf135588.pdf', 'filepdf135588_NoRestriction.pdf', 'filepdf137074.pdf', 'filepdf137074_NoRestriction.pdf', 'filepdf137817.pdf', 'filepdf137822.pdf', 'filepdf137822_NoRestriction.pdf', 'filepdf13795.pdf', 'filepdf13800.pdf', 'filepdf13800_NoRestriction.pdf', 'filepdf138554.pdf', 'filepdf138559.pdf', 'filepdf138559_NoRestriction.pdf', 'filepdf139291.pdf', 'filepdf139296.pdf', 'filepdf139296_NoRestriction.pdf', 'filepdf141507.pdf', 'filepdf141512.pdf', 'filepdf141512_NoRestriction.pdf', 'filepdf142244.pdf', 'filepdf142249.pdf', 'filepdf142249_NoRestriction.pdf', 'filepdf142981.pdf', 'filepdf142986.pdf', 'filepdf142986_NoRestriction.pdf', 'filepdf143718.pdf', 'filepdf143723.pdf', 'filepdf143723_NoRestriction.pdf', 'filepdf144454.pdf', 'filepdf144459.pdf', 'filepdf144459_NoRestriction.pdf', 'filepdf14533.pdf', 'filepdf14538.pdf', 'filepdf14538_NoRestriction.pdf', 'filepdf145928.pdf', 'filepdf145933.pdf', 'filepdf145933_NoRestriction.pdf', 'filepdf147397.pdf', 'filepdf147402.pdf', 'filepdf147402_NoRestriction.pdf', 'filepdf148134.pdf', 'filepdf148139.pdf', 'filepdf148139_NoRestriction.pdf', 'filepdf148896.pdf', 'filepdf148896_NoRestriction.pdf', 'filepdf150393.pdf', 'filepdf150393_NoRestriction.pdf', 'filepdf151130.pdf', 'filepdf151130_NoRestriction.pdf', 'filepdf151862.pdf', 'filepdf151867.pdf', 'filepdf151867_NoRestriction.pdf', 'filepdf152599.pdf', 'filepdf152604.pdf', 'filepdf152604_NoRestriction.pdf', 'filepdf15270.pdf', 'filepdf15275.pdf', 'filepdf15275_NoRestriction.pdf', 'filepdf153338.pdf', 'filepdf153343.pdf', 'filepdf153343_NoRestriction.pdf', 'filepdf154075.pdf', 'filepdf154080.pdf', 'filepdf154080_NoRestriction.pdf', 'filepdf155550.pdf', 'filepdf155555.pdf', 'filepdf155555_NoRestriction.pdf', 'filepdf156298.pdf', 'filepdf156298_NoRestriction.pdf', 'filepdf157775.pdf', 'filepdf157780.pdf', 'filepdf157780_NoRestriction.pdf', 'filepdf158521.pdf', 'filepdf158526.pdf', 'filepdf158526_NoRestriction.pdf', 'filepdf159263_NoRestriction.pdf', 'filepdf160006_NoRestriction.pdf', 'filepdf160737.pdf', 'filepdf160742.pdf', 'filepdf160742_NoRestriction.pdf', 'filepdf162211.pdf', 'filepdf162216.pdf', 'filepdf162216_NoRestriction.pdf', 'filepdf162948.pdf', 'filepdf162953.pdf', 'filepdf162953_NoRestriction.pdf', 'filepdf164429.pdf', 'filepdf164434.pdf', 'filepdf164434_NoRestriction.pdf', 'filepdf165169.pdf', 'filepdf165174.pdf', 'filepdf165174_NoRestriction.pdf', 'filepdf165906.pdf', 'filepdf165911.pdf', 'filepdf165911_NoRestriction.pdf', 'filepdf166643.pdf', 'filepdf166648.pdf', 'filepdf166648_NoRestriction.pdf', 'filepdf167385_NoRestriction.pdf', 'filepdf16748_NoRestriction.pdf', 'filepdf168117.pdf', 'filepdf168122.pdf', 'filepdf168122_NoRestriction.pdf', 'filepdf168862.pdf', 'filepdf168867.pdf', 'filepdf168867_NoRestriction.pdf', 'filepdf169600.pdf', 'filepdf169605_NoRestriction.pdf', 'filepdf170340.pdf', 'filepdf170345.pdf', 'filepdf170345_NoRestriction.pdf', 'filepdf171077.pdf', 'filepdf171082.pdf', 'filepdf171082_NoRestriction.pdf', 'filepdf171814.pdf', 'filepdf171819_NoRestriction.pdf', 'filepdf173300_NoRestriction.pdf', 'filepdf173300_NoRestriction.txt', 'filepdf174032.pdf', 'filepdf174037_NoRestriction.pdf', 'filepdf17480.pdf', 'filepdf17485.pdf', 'filepdf17485_NoRestriction.pdf', 'filepdf175506.pdf', 'filepdf175511.pdf', 'filepdf175511_NoRestriction.pdf', 'filepdf176243.pdf', 'filepdf176248.pdf', 'filepdf176248_NoRestriction.pdf', 'filepdf176979.pdf', 'filepdf176984.pdf', 'filepdf176984_NoRestriction.pdf', 'filepdf178453.pdf', 'filepdf178458.pdf', 'filepdf178458_NoRestriction.pdf', 'filepdf179190.pdf', 'filepdf179195.pdf', 'filepdf179195_NoRestriction.pdf', 'filepdf180664.pdf', 'filepdf180669.pdf', 'filepdf180669_NoRestriction.pdf', 'filepdf181401.pdf', 'filepdf181406_NoRestriction.pdf', 'filepdf182138.pdf', 'filepdf182143.pdf', 'filepdf182143_NoRestriction.pdf', 'filepdf18217.pdf', 'filepdf18222.pdf', 'filepdf18222_NoRestriction.pdf', 'filepdf182875.pdf', 'filepdf182880.pdf', 'filepdf182880_NoRestriction.pdf', 'filepdf183612.pdf', 'filepdf183617.pdf', 'filepdf183617_NoRestriction.pdf', 'filepdf184367.pdf', 'filepdf184372.pdf', 'filepdf184372_NoRestriction.pdf', 'filepdf185109.pdf', 'filepdf185109_NoRestriction.pdf', 'filepdf186626.pdf', 'filepdf186631.pdf', 'filepdf186631_NoRestriction.pdf', 'filepdf188117.pdf', 'filepdf188117_NoRestriction.pdf', 'filepdf190379.pdf', 'filepdf190384.pdf', 'filepdf190384_NoRestriction.pdf', 'filepdf191123.pdf', 'filepdf191128.pdf', 'filepdf191128_NoRestriction.pdf', 'filepdf191871.pdf', 'filepdf191876.pdf', 'filepdf191876_NoRestriction.pdf', 'filepdf192608.pdf', 'filepdf192613.pdf', 'filepdf192613_NoRestriction.pdf', 'filepdf193344.pdf', 'filepdf193349.pdf', 'filepdf193349_NoRestriction.pdf', 'filepdf194818.pdf', 'filepdf194823.pdf', 'filepdf194823_NoRestriction.pdf', 'filepdf195562.pdf', 'filepdf195567.pdf', 'filepdf195567_NoRestriction.pdf', 'filepdf196299.pdf', 'filepdf196304.pdf', 'filepdf196304_NoRestriction.pdf', 'filepdf19698_NoRestriction.pdf', 'filepdf197041.pdf', 'filepdf197046.pdf', 'filepdf197046_NoRestriction.pdf', 'filepdf1974.pdf', 'filepdf1974_NoRestriction.pdf', 'filepdf197790_NoRestriction.pdf', 'filepdf198527.pdf', 'filepdf198527_NoRestriction.pdf', 'filepdf199259.pdf', 'filepdf199264.pdf', 'filepdf199264_NoRestriction.pdf', 'filepdf199996.pdf', 'filepdf200001.pdf', 'filepdf200001_NoRestriction.pdf', 'filepdf200733.pdf', 'filepdf200738.pdf', 'filepdf200738_NoRestriction.pdf', 'filepdf201470.pdf', 'filepdf201475.pdf', 'filepdf201475_NoRestriction.pdf', 'filepdf202207.pdf', 'filepdf202212.pdf', 'filepdf202212_NoRestriction.pdf', 'filepdf202944.pdf', 'filepdf202949.pdf', 'filepdf202949_NoRestriction.pdf', 'filepdf203692.pdf', 'filepdf203692_NoRestriction.pdf', 'filepdf20430.pdf', 'filepdf20435.pdf', 'filepdf20435_NoRestriction.pdf', 'filepdf204429_NoRestriction.pdf', 'filepdf206652.pdf', 'filepdf206652_NoRestriction.pdf', 'filepdf207384.pdf', 'filepdf207389.pdf', 'filepdf207389_NoRestriction.pdf', 'filepdf208130.pdf', 'filepdf208135.pdf', 'filepdf208135_NoRestriction.pdf', 'filepdf208867.pdf', 'filepdf208872.pdf', 'filepdf208872_NoRestriction.pdf', 'filepdf209603.pdf', 'filepdf209608.pdf', 'filepdf209608_NoRestriction.pdf', 'filepdf211077.pdf', 'filepdf211082.pdf', 'filepdf211082_NoRestriction.pdf', 'filepdf21167.pdf', 'filepdf21172.pdf', 'filepdf21172_NoRestriction.pdf', 'filepdf211819_NoRestriction.pdf', 'filepdf212551.pdf', 'filepdf212556.pdf', 'filepdf212556_NoRestriction.pdf', 'filepdf213288.pdf', 'filepdf213293.pdf', 'filepdf213293_NoRestriction.pdf', 'filepdf214770.pdf', 'filepdf214775.pdf', 'filepdf214775_NoRestriction.pdf', 'filepdf215505_NoRestriction.pdf', 'filepdf216249_NoRestriction.pdf', 'filepdf218467.pdf', 'filepdf218472.pdf', 'filepdf218472_NoRestriction.pdf', 'filepdf219204.pdf', 'filepdf219209.pdf', 'filepdf219209_NoRestriction.pdf', 'filepdf219946_NoRestriction.pdf', 'filepdf220678.pdf', 'filepdf220683.pdf', 'filepdf220683_NoRestriction.pdf']\n\nCluster1timbang=[ 'filepdf103061_NoRestriction.txt', 'filepdf104535_NoRestriction.txt', 'filepdf108985_NoRestriction.txt', 'filepdf110482_NoRestriction.txt', 'filepdf117899_NoRestriction.txt', 'filepdf121582_NoRestriction.txt', 'filepdf122319_NoRestriction.txt', 'filepdf123056_NoRestriction.txt', 'filepdf12326_NoRestriction.txt', 'filepdf1237_NoRestriction.txt', 'filepdf124531_NoRestriction.txt', 'filepdf13063_NoRestriction.txt', 'filepdf132640_NoRestriction.txt', 'filepdf133377_NoRestriction.txt', 'filepdf134114_NoRestriction.txt', 'filepdf135588_NoRestriction.txt', 'filepdf141512_NoRestriction.txt', 'filepdf142249_NoRestriction.txt', 'filepdf142986_NoRestriction.txt', 'filepdf150393_NoRestriction.txt', 'filepdf151867_NoRestriction.txt', 'filepdf153343_NoRestriction.txt', 'filepdf154080_NoRestriction.txt', 'filepdf157780_NoRestriction.txt', 'filepdf158526_NoRestriction.txt', 'filepdf159263_NoRestriction.txt', 'filepdf162953_NoRestriction.txt', 'filepdf165911_NoRestriction.txt', 'filepdf166648_NoRestriction.txt', 'filepdf16748_NoRestriction.txt', 'filepdf168122_NoRestriction.txt', 'filepdf170345_NoRestriction.txt', 'filepdf171082_NoRestriction.txt', 'filepdf171819_NoRestriction.txt', 'filepdf17485_NoRestriction.txt', 'filepdf179195_NoRestriction.txt', 'filepdf181406_NoRestriction.txt', 'filepdf182143_NoRestriction.txt', 'filepdf18222_NoRestriction.txt', 'filepdf182880_NoRestriction.txt', 'filepdf186631_NoRestriction.txt', 'filepdf192613_NoRestriction.txt', 'filepdf196304_NoRestriction.txt', 'filepdf200001_NoRestriction.txt', 'filepdf201475_NoRestriction.txt', 'filepdf204429_NoRestriction.txt', 'filepdf212556_NoRestriction.txt', 'filepdf213293_NoRestriction.txt', 'filepdf214775_NoRestriction.txt']\n\nCluster2timbang=[ 'filepdf116425_NoRestriction.txt', 'filepdf126747_NoRestriction.txt', 'filepdf137074_NoRestriction.txt', 'filepdf156298_NoRestriction.txt', 'filepdf160006_NoRestriction.txt', 'filepdf168867_NoRestriction.txt', 'filepdf184372_NoRestriction.txt', 'filepdf185109_NoRestriction.txt', 'filepdf188117_NoRestriction.txt', 'filepdf190384_NoRestriction.txt', 'filepdf197046_NoRestriction.txt', 'filepdf197790_NoRestriction.txt', 'filepdf203692_NoRestriction.txt', 'filepdf206652_NoRestriction.txt', 'filepdf218472_NoRestriction.txt']\n\nCluster3timbang=[ 'filepdf113446_NoRestriction.txt', 'filepdf129692_NoRestriction.txt', 'filepdf145933_NoRestriction.txt', 'filepdf162216_NoRestriction.txt', 'filepdf178458_NoRestriction.txt', 'filepdf194823_NoRestriction.txt', 'filepdf211082_NoRestriction.txt']\n\nCluster4timbang=[ 'filepdf101587_NoRestriction.txt', 'filepdf106015_NoRestriction.txt', 'filepdf106752_NoRestriction.txt', 'filepdf11589_NoRestriction.txt', 'filepdf117162_NoRestriction.txt', 'filepdf118636_NoRestriction.txt', 'filepdf123794_NoRestriction.txt', 'filepdf130429_NoRestriction.txt', 'filepdf131166_NoRestriction.txt', 'filepdf134851_NoRestriction.txt', 'filepdf137822_NoRestriction.txt', 'filepdf14538_NoRestriction.txt', 'filepdf147402_NoRestriction.txt', 'filepdf148139_NoRestriction.txt', 'filepdf148896_NoRestriction.txt', 'filepdf151130_NoRestriction.txt', 'filepdf15275_NoRestriction.txt', 'filepdf155555_NoRestriction.txt', 'filepdf165174_NoRestriction.txt', 'filepdf175511_NoRestriction.txt', 'filepdf176248_NoRestriction.txt', 'filepdf180669_NoRestriction.txt', 'filepdf183617_NoRestriction.txt', 'filepdf195567_NoRestriction.txt', 'filepdf1974_NoRestriction.txt', 'filepdf199264_NoRestriction.txt', 'filepdf202212_NoRestriction.txt', 'filepdf202949_NoRestriction.txt', 'filepdf207389_NoRestriction.txt', 'filepdf211819_NoRestriction.txt', 'filepdf219946_NoRestriction.txt']\n\nCluster5timbang=[ 'filepdf100107_NoRestriction.txt', 'filepdf102324_NoRestriction.txt', 'filepdf108230_NoRestriction.txt', 'filepdf10852_NoRestriction.txt', 'filepdf120108_NoRestriction.txt', 'filepdf120845_NoRestriction.txt', 'filepdf125268_NoRestriction.txt', 'filepdf126005_NoRestriction.txt', 'filepdf127479_NoRestriction.txt', 'filepdf131903_NoRestriction.txt', 'filepdf13800_NoRestriction.txt', 'filepdf138559_NoRestriction.txt', 'filepdf139296_NoRestriction.txt', 'filepdf143723_NoRestriction.txt', 'filepdf152604_NoRestriction.txt', 'filepdf164434_NoRestriction.txt', 'filepdf167385_NoRestriction.txt', 'filepdf169605_NoRestriction.txt', 'filepdf174037_NoRestriction.txt', 'filepdf191128_NoRestriction.txt', 'filepdf191876_NoRestriction.txt', 'filepdf19698_NoRestriction.txt', 'filepdf198527_NoRestriction.txt', 'filepdf200738_NoRestriction.txt', 'filepdf20435_NoRestriction.txt', 'filepdf208135_NoRestriction.txt', 'filepdf208872_NoRestriction.txt', 'filepdf21172_NoRestriction.txt', 'filepdf215505_NoRestriction.txt', 'filepdf216249_NoRestriction.txt', 'filepdf219209_NoRestriction.txt']\n\nCluster6timbang=[ 'filepdf111972_NoRestriction.txt', 'filepdf128218_NoRestriction.txt', 'filepdf144459_NoRestriction.txt', 'filepdf160742_NoRestriction.txt', 'filepdf176984_NoRestriction.txt', 'filepdf193349_NoRestriction.txt', 'filepdf209608_NoRestriction.txt']\n\n","f6054f9d":"intersection_set = set.intersection(set(Cluster0putusan), set(Cluster0timbang))\n\n\nintersection_list = list(intersection_set)\n\nprint(intersection_list)\n","76c90ebd":"print(len(intersection_list))","87004b12":"intersection_set = set.intersection(set(Cluster4putusan), set(Cluster1timbang))\n\n\nintersection_list = list(intersection_set)\n\nprint(intersection_list)","bfb83dea":"print(len(intersection_list))","1b318a55":"for i in range(7):\n    for j in range(7):\n        timbang='Cluster'+str(i)+'timbang'\n        putusan='Cluster'+str(j)+'putusan'\n        print(timbang)\n        print(putusan)\n        intersection_set = set.intersection(set(eval(timbang)), set(eval(putusan)))\n        intersection_list = list(intersection_set)\n        print()\n        print(intersection_list)\n        print()\n        print()\n        print(len(intersection_list))\n        print()\n        print()","955dcbe7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8fe533a4":"pip install Sastrawi","2bf4a65d":"# import StemmerFactory class\nfrom Sastrawi.Stemmer.StemmerFactory import StemmerFactory# create stemmer\nfactory = StemmerFactory()\nstemmer = factory.create_stemmer()# stemming process\nsentence = 'Perekonomian Indonesia sedang dalam pertumbuhan yang membanggakan'\noutput   = stemmer.stem(sentence)\nprint(output)\n# ekonomi indonesia sedang dalam tumbuh yang banggaprint(stemmer.stem('Mereka meniru-nirukannya'))\n# mereka tiru\n","e7bfb13f":"kalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nlower_case = kalimat.lower()\nprint(lower_case)# output\n# berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah korea selatan, jepang, singapura, hong kong, dan finlandia.\n","ea73da2e":"import re # impor modul regular expression\nkalimat = \"Berikut ini adalah 5 negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\"\nhasil = re.sub(r\"\\d+\", \"\", kalimat)\nprint(hasil)# ouput\n# Berikut ini adalah negara dengan pendidikan terbaik di dunia adalah Korea Selatan, Jepang, Singapura, Hong Kong, dan Finlandia.\n","a677b0fe":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","9c06f8b4":"preprocess_text(kalimat)","49fdab6e":"kalimat = \"rumah idaman adalah rumah yang bersih\"\npisah = kalimat.split()\nprint(pisah)# output\n# ['rumah', 'idaman', 'adalah', 'rumah', 'yang', 'bersih']\n","0794fff6":"pip install nltk","fad8fc31":"# impor word_tokenize dari modul nltk\nimport nltk\nfrom nltk.tokenize import word_tokenize \n \nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online.\"\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nprint(tokens)# ouput \n# ['Andi', 'kerap', 'melakukan', 'transaksi', 'rutin', 'secara', 'daring', 'atau', 'online', '.']\n","7d2892ef":"from nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\nkalimat = \"Andi kerap melakukan transaksi rutin secara daring atau online. Menurut Andi belanja online lebih praktis & murah.\"\nkalimat = kalimat.translate(str.maketrans('','')).lower()\n \ntokens = nltk.tokenize.word_tokenize(kalimat)\nkemunculan = nltk.FreqDist(tokens)\nprint(kemunculan.most_common())# \n","e9ade123":"import matplotlib.pyplot as plt\nkemunculan.plot(30,cumulative=False)\nplt.show()\n","54007d86":"# TfidfVectorizer \n# CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd\n# set of documents\ntrain = ['The sky is blue. water is clean The sky is blue','The sun is bright. the moon is darak']\ntest = ['The sun in the sky is bright', 'We can see the shining sun, the bright sun.the moon is blue']\n# instantiate the vectorizer object\ncountvectorizer = CountVectorizer(analyzer= 'word', stop_words='english')\ntfidfvectorizer = TfidfVectorizer(analyzer='word',stop_words= 'english')\n# convert th documents into a matrix\ncount_wm = countvectorizer.fit_transform(train)\ntfidf_wm = tfidfvectorizer.fit_transform(train)\n#retrieve the terms found in the corpora\n# if we take same parameters on both Classes(CountVectorizer and TfidfVectorizer) , it will give same output of get_feature_names() methods)\n#count_tokens = tfidfvectorizer.get_feature_names() # no difference\ncount_tokens = countvectorizer.get_feature_names()\ntfidf_tokens = tfidfvectorizer.get_feature_names()\ndf_countvect = pd.DataFrame(data = count_wm.toarray(),index = ['Doc1','Doc2'],columns = count_tokens)\ndf_tfidfvect = pd.DataFrame(data = tfidf_wm.toarray(),index = ['Doc1','Doc2'],columns = tfidf_tokens)\nprint(\"Count Vectorizer\\n\")\nprint(df_countvect)\nprint(\"\\nTD-IDF Vectorizer\\n\")\nprint(df_tfidfvect)\n","2f423769":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","47ebb327":"training_data['label'] = training_data['class'].apply(lambda x: 0 if x=='stmt' else 1)\ntraining_data","ef564230":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(training_data['sent'], training_data['label'], test_size = 0.3,random_state=0)","d21195ab":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","1f1e0acb":"from sklearn.preprocessing import StandardScaler \nscaler = StandardScaler() \nscaler.fit(X_train_cv)\nX_train_cv = scaler.transform(X_train_cv) \nX_test_cv = scaler.transform(X_test_cv)","73900a07":"word_freq = pd.DataFrame(X_train_cv.toarray(), columns=cv.get_feature_names())\ntop_words_df = pd.DataFrame(word_freq.sum()).sort_values(0, ascending=False)","256d39eb":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions = naive_bayes.predict(X_test_cv)","3eb431ae":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions))\nprint('Precision score: ', precision_score(y_test, predictions))\nprint('Recall score: ', recall_score(y_test, predictions))","5065f764":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test, predictions)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['question', 'stmt'], yticklabels=['question', 'stmt'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","9a9e855a":"from sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=5) \nclassifier.fit(X_train_cv, y_train)","603f0d81":"y_pred = classifier.predict(X_test_cv)","c95df424":"from sklearn.metrics import classification_report, confusion_matrix \nprint(confusion_matrix(y_test, y_pred)) \nprint(classification_report(y_test, y_pred))","36a10be0":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train_cv,y_train)\n\nY_pred_lr = lr.predict(X_test_cv)\n","d0b24321":"\nscore_lr = round(accuracy_score(Y_pred_lr,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Logistic Regression is: \"+str(score_lr)+\" %\")","76aad421":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train_cv, y_train)","7030527c":"Y_pred_svm = sv.predict(X_test_cv)\nscore_svm = round(accuracy_score(Y_pred_svm,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Linear SVM is: \"+str(score_svm)+\" %\")","5d08e935":"Y_pred_svm.shape","3f60ad0f":"from sklearn.tree import DecisionTreeClassifier\n\nmax_accuracy = 0\n\n\nfor x in range(20):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cv,y_train)\n    Y_pred_dt = dt.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\n\ndt = DecisionTreeClassifier(random_state=best_x)\ndt.fit(X_train_cv,y_train)\nY_pred_dt = dt.predict(X_test_cv)","9786014e":"score_dt = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_dt)+\" %\")","62aab172":"from sklearn.ensemble import RandomForestClassifier\nmax_accuracy = 0\nfor x in range(20):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train_cv,y_train)\n    Y_pred_rf = rf.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)\n\nrf = RandomForestClassifier(random_state=best_x)\nrf.fit(X_train_cv,y_train)\nY_pred_rf = rf.predict(X_test_cv)","4f653bd5":"score_rf = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n\nprint(\"The accuracy score achieved using Decision Tree is: \"+str(score_rf)+\" %\")","b409bf94":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train_cv, y_train)\n\nY_pred_xgb = xgb_model.predict(X_test_cv)","981da86a":"score_xgb = round(accuracy_score(Y_pred_xgb,y_test)*100,2)\n\nprint(\"The accuracy score achieved using XGBoost is: \"+str(score_xgb)+\" %\")","63046b07":"scores = [score_lr,score_nb,score_svm,score_knn,score_dt,score_rf,score_xgb]\nalgorithms = [\"Logistic Regression\",\"Naive Bayes\",\"Support Vector Machine\",\"K-Nearest Neighbors\",\"Decision Tree\",\"Random Forest\",\"XGBoost\"]    \n\nfor i in range(len(algorithms)):\n    print(\"The accuracy score achieved using \"+algorithms[i]+\" is: \"+str(scores[i])+\" %\")","37212376":"sns.set(rc={'figure.figsize':(15,7)})\nplt.xlabel(\"Algorithms\")\nplt.ylabel(\"Accuracy score\")","0b75c2a0":"import numpy as np\nimport pandas as pd\n    \n# for performing text clustering    \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\n# for providing the path\nimport os\nprint(os.listdir('..\/input\/'))\n\n# for visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')","4279426e":"import pandas as pd\n\ncolumns = ['sent', 'class']\nrows = []\n\nrows = [['This is my book', 'stmt'], \n        ['They are novels', 'stmt'],\n        ['have you read this book', 'question'],\n        ['who is the author', 'question'],\n        ['what are the characters', 'question'],\n        ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question'],\n       ['This is how I bought the book', 'stmt'],\n        ['I like fictions', 'stmt'],\n        ['what is your favorite book', 'question']]\n\ntraining_data = pd.DataFrame(rows, columns=columns)\ntraining_data","8197e92f":"training_data['length'] = training_data['sent'].apply(len)","8a1e2708":"training_data.head()","1061cc2a":"plt.rcParams['figure.figsize'] = (15, 7)\nsns.distplot(training_data['length'], color = 'purple')\nplt.title('The Distribution of Length over the Texts', fontsize = 20)","992228fd":"# wordcloud\n\nfrom wordcloud import WordCloud\n\nwordcloud = WordCloud(background_color = 'lightcyan',\n                      width = 1200,\n                      height = 700).generate(str(training_data['sent']))\n\nplt.figure(figsize = (15, 10))\nplt.imshow(wordcloud)\nplt.title(\"WordCloud \", fontsize = 20)","3e908b56":"from sklearn.feature_extraction.text import CountVectorizer\n\n\ncv = CountVectorizer()\nwords = cv.fit_transform(training_data['sent'])\nsum_words = words.sum(axis=0)\n\n\nwords_freq = [(word, sum_words[0, idx]) for word, idx in cv.vocabulary_.items()]\nwords_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\nfrequency = pd.DataFrame(words_freq, columns=['word', 'freq'])\n\ncolor = plt.cm.twilight(np.linspace(0, 1, 20))\nfrequency.head(20).plot(x='word', y='freq', kind='bar', figsize=(15, 7), color = color)\nplt.title(\"Most Frequently Occuring Words - Top 20\")","830603f5":"print(\"Shape of X :\", words.shape)","587a1a4b":"true_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(words)","12c3f600":"print(\"Top terms per cluster:\")\n\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = cv.get_feature_names()\n\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\nY = cv.transform([\"you read this book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)\nY = cv.transform([\"what is favorite book\"])\nprediction = model.predict(Y)\nprint(\"Cluster number :\", prediction)","cfbbc428":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX = vectorizer.fit_transform(documents)\n\ntrue_k = 2\nmodel = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1)\nmodel.fit(X)\n\nprint(\"Top terms per cluster:\")\norder_centroids = model.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(true_k):\n    print(\"Cluster %d:\" % i),\n    for ind in order_centroids[i, :10]:\n        print(' %s' % terms[ind]),\n    print\n\nprint(\"\\n\")\nprint(\"Prediction\")\n\nY = vectorizer.transform([\"chrome browser to open.\"])\nprediction = model.predict(Y)\nprint(prediction)\n\nY = vectorizer.transform([\"My cat is hungry.\"])\nprediction = model.predict(Y)\nprint(prediction)","be85ea39":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import adjusted_rand_score\n\nimport re\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom scipy.spatial.distance import pdist\nfrom scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n\n\n\ndocuments = [\"This little kitty came to play when I was eating at a restaurant.\",\n             \"Merley has the best squooshy kitten belly.\",\n             \"Google Translate app is incredible.\",\n             \"If you open 100 tab in google you get a smiley face.\",\n             \"Best cat photo I've ever taken.\",\n             \"Climbing ninja cat.\",\n             \"Impressed with google map feedback.\",\n             \"Key promoter extension for Google Chrome.\"]\n\ntfidf = TfidfVectorizer(stop_words='english')\nX = tfidf.fit_transform(documents).todense()\n# transform the data matrix into pairwise distances list\ndist_array = pdist(X)\n# calculate hierarchy\n#Z = linkage(dist_array, 'ward')\n#plt.title(\"Ward\")\n#dendrogram(Z, labels=labels)\n\nimport scipy.cluster.hierarchy as sch\ndendrogram = sch.dendrogram(sch.linkage(X, method = 'ward'))\nplt.title('Dendrogram')\nplt.xlabel('Nomor Dokumen')\nplt.ylabel('Jarak Euclidean')\nplt.show()\n\n\nfrom sklearn.cluster import AgglomerativeClustering\n\ncluster = AgglomerativeClustering(n_clusters=2, affinity='euclidean', linkage='ward')  \ncluster.fit_predict(X) \nprint(cluster.labels_)  ","38ef2eb9":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\n%matplotlib inline","21346703":"sns.countplot(training_data.label)\nplt.xlabel('Label')\nplt.title('Number of statement and question messages')\ndf=training_data","4274e116":"X = df.sent\nY = df.label\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","4c396136":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","037aec83":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","a9abafe7":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","fea4a074":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","b5ed1564":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","32e1fc5a":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","07e5a694":"accr = model.evaluate(test_sequences_matrix,Y_test)","12b8a40d":"print('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","a9714906":"# Importing libraries\nfrom keras.datasets import imdb\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import Flatten\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom keras.preprocessing import sequence# Our dictionary will contain only of the top 7000 words appearing most frequently\ntop_words = 7000# Now we split our data-set into training and test data\n(X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words)# Looking at the nature of training data\nprint(X_train[0])\nprint(y_train[0])\nprint('Shape of training data: ')\nprint(X_train.shape)\nprint(y_train.shape)\nprint('Shape of test data: ')\nprint(X_test.shape)\nprint(y_test.shape)","186bd575":"# Looking at the nature of training data\nprint(X_train[0])\nprint(y_train[0])\nprint('Shape of training data: ')\nprint(X_train.shape)\nprint(y_train.shape)\nprint('Shape of test data: ')\nprint(X_test.shape)\nprint(y_test.shape)","8b259f44":"# Padding the data samples to a maximum review length in words\nmax_words = 450\nX_train = sequence.pad_sequences(X_train, maxlen=max_words)\nX_test = sequence.pad_sequences(X_test, maxlen=max_words)# Building the CNN Model\nmodel = Sequential()      # initilaizing the Sequential nature for CNN model# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\nmodel.add(Embedding(top_words, 32, input_length=max_words))\nmodel.add(Conv1D(32, 3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","3752f731":"model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","97902c9f":"# Fitting the data onto model\nmodel.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=128, verbose=2)# Getting score metrics from our model\nscores = model.evaluate(X_test, y_test, verbose=0)# Displays the accuracy of correct sentiment prediction over test data\nprint(\"Accuracy: %.2f%%\" % (scores[1]*100))","f226c917":"# libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nnp.random.seed(32)\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.manifold import TSNE\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import LSTM, Conv1D, MaxPooling1D, Dropout\nfrom keras.utils.np_utils import to_categorical\n","0089ed18":"MAX_NB_WORDS = 20000\n\n# get the raw text data\ntexts_train = X_train.astype(str)\ntexts_test = X_test.astype(str)\n\n# finally, vectorize the text samples into a 2D integer tensor\ntokenizer = Tokenizer(num_words=MAX_NB_WORDS, char_level=False)\ntokenizer.fit_on_texts(texts_train)\nsequences = tokenizer.texts_to_sequences(texts_train)\nsequences_test = tokenizer.texts_to_sequences(texts_test)\n\nword_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))","64fb9d16":"type(tokenizer.word_index), len(tokenizer.word_index)","61aa0606":"index_to_word = dict((i, w) for w, i in tokenizer.word_index.items())","d2da6ad9":"\" \".join([index_to_word[i] for i in sequences[0]])","8d3e3354":"seq_lens = [len(s) for s in sequences]\nprint(\"average length: %0.1f\" % np.mean(seq_lens))\nprint(\"max length: %d\" % max(seq_lens))","e81f0a42":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nplt.hist(seq_lens, bins=50);","a3d65178":"plt.hist([l for l in seq_lens if l < 200], bins=50);","639d88db":"MAX_SEQUENCE_LENGTH = 150\n\n# pad sequences with 0s\nx_train = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(sequences_test, maxlen=MAX_SEQUENCE_LENGTH)\nprint('Shape of data tensor:', x_train.shape)\nprint('Shape of data test tensor:', x_test.shape)","239b4771":"# Building the CNN Model\nmodel = Sequential()      # initilaizing the Sequential nature for CNN model# Adding the embedding layer which will take in maximum of 450 words as input and provide a 32 dimensional output of those words which belong in the top_words dictionary\nmodel.add(Embedding(top_words, 32, input_length=max_words))\nmodel.add(Conv1D(32, 3, padding='same', activation='relu'))\nmodel.add(MaxPooling1D())\nmodel.add(Flatten())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))","bfae58f3":"#y_train = train_y\n#y_test = test_y\n\ny_train = to_categorical(np.asarray(y_train))\nprint('Shape of label tensor:', y_train.shape)","69c719eb":"from keras.layers import Dense, Input, Flatten\nfrom keras.layers import GlobalAveragePooling1D, Embedding\nfrom keras.models import Model\n\nEMBEDDING_DIM = 50\nN_CLASSES = 2\n\n# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)\nembedded_sequences = embedding_layer(sequence_input)\n\naverage = GlobalAveragePooling1D()(embedded_sequences)\npredictions = Dense(N_CLASSES, activation='softmax')(average)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['acc'])","85684eac":"x_train.shape","c790032b":"y_train.shape","23367927":"model.fit(X_train, y_train, validation_split=0.1,\n          epochs=10, batch_size=128)","c4c743ef":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","8605de6c":"from keras.layers import Dense, Input, Flatten\nfrom keras.layers import GlobalAveragePooling1D, Embedding\nfrom keras.models import Model\n\nEMBEDDING_DIM = 50\nN_CLASSES = 2\n\n# input: a sequence of MAX_SEQUENCE_LENGTH integers\nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n\nembedding_layer = Embedding(MAX_NB_WORDS, EMBEDDING_DIM,\n                            input_length=MAX_SEQUENCE_LENGTH,\n                            trainable=True)\nembedded_sequences = embedding_layer(sequence_input)\n\naverage = GlobalAveragePooling1D()(embedded_sequences)\npredictions = Dense(N_CLASSES, activation='softmax')(average)\n\nmodel = Model(sequence_input, predictions)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer='adam', metrics=['acc'])","149083a9":"# Silhouette score analysis to find the ideal number of clusters for K-means clustering\n\nrange_n_clusters = [2, 3, 4, 5, 6, 7, 8]\n\nfor num_clusters in range_n_clusters:\n    \n    # intialise kmeans\n    kmeans = KMeans(n_clusters=num_clusters, max_iter=50,random_state= 100)\n    kmeans.fit(df_pca_final_data)\n    \n    cluster_labels = kmeans.labels_\n    \n    # silhouette score\n    silhouette_avg = silhouette_score(df_pca_final_data, cluster_labels)\n    print(\"For n_clusters={0}, the silhouette score is {1}\".format(num_clusters, silhouette_avg))","43536e47":"error = []\nfor i in range(1, 4): \n knn = KNeighborsClassifier(n_neighbors=i)\n knn.fit(X_train_cv, y_train)\n pred_i = knn.predict(X_test_cv)\n #error.append(np.mean(pred_i != y_test))","f6f8c7ed":"from keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\nfrom gensim.models import Word2Vec\nimport numpy as np\nimport gc\n\n\ndef train_word2vec(documents, embedding_dim):\n    \"\"\"\n    train word2vector over traning documents\n    Args:\n        documents (list): list of document\n        embedding_dim (int): outpu wordvector size\n    Returns:\n        word_vectors(dict): dict containing words and their respective vectors\n    \"\"\"\n    model = Word2Vec(documents, min_count=1, size=embedding_dim)\n    word_vectors = model.wv\n    del model\n    return word_vectors\n\n\ndef create_embedding_matrix(tokenizer, word_vectors, embedding_dim):\n    \"\"\"\n    Create embedding matrix containing word indexes and respective vectors from word vectors\n    Args:\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object containing word indexes\n        word_vectors (dict): dict containing word and their respective vectors\n        embedding_dim (int): dimention of word vector\n\n    Returns:\n\n    \"\"\"\n    nb_words = len(tokenizer.word_index) + 1\n    word_index = tokenizer.word_index\n    embedding_matrix = np.zeros((nb_words, embedding_dim))\n    print(\"Embedding matrix shape: %s\" % str(embedding_matrix.shape))\n    for word, i in word_index.items():\n        try:\n            embedding_vector = word_vectors[word]\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n        except KeyError:\n            print(\"vector not found for word - %s\" % word)\n    print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n    return embedding_matrix\n\n\ndef word_embed_meta_data(documents, embedding_dim):\n    \"\"\"\n    Load tokenizer object for given vocabs list\n    Args:\n        documents (list): list of document\n        embedding_dim (int): embedding dimension\n    Returns:\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n        embedding_matrix (dict): dict with word_index and vector mapping\n    \"\"\"\n    documents = [x.lower().split() for x in documents]\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(documents)\n    word_vector = train_word2vec(documents, embedding_dim)\n    embedding_matrix = create_embedding_matrix(tokenizer, word_vector, embedding_dim)\n    del word_vector\n    gc.collect()\n    return tokenizer, embedding_matrix\n\n\ndef create_train_dev_set(tokenizer, sentences_pair, is_similar, max_sequence_length, validation_split_ratio):\n    \"\"\"\n    Create training and validation dataset\n    Args:\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n        sentences_pair (list): list of tuple of sentences pairs\n        is_similar (list): list containing labels if respective sentences in sentence1 and sentence2\n                           are same or not (1 if same else 0)\n        max_sequence_length (int): max sequence length of sentences to apply padding\n        validation_split_ratio (float): contain ratio to split training data into validation data\n\n    Returns:\n        train_data_1 (list): list of input features for training set from sentences1\n        train_data_2 (list): list of input features for training set from sentences2\n        labels_train (np.array): array containing similarity score for training data\n        leaks_train(np.array): array of training leaks features\n\n        val_data_1 (list): list of input features for validation set from sentences1\n        val_data_2 (list): list of input features for validation set from sentences1\n        labels_val (np.array): array containing similarity score for validation data\n        leaks_val (np.array): array of validation leaks features\n    \"\"\"\n    sentences1 = [x[0].lower() for x in sentences_pair]\n    sentences2 = [x[1].lower() for x in sentences_pair]\n    train_sequences_1 = tokenizer.texts_to_sequences(sentences1)\n    train_sequences_2 = tokenizer.texts_to_sequences(sentences2)\n    leaks = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n             for x1, x2 in zip(train_sequences_1, train_sequences_2)]\n\n    train_padded_data_1 = pad_sequences(train_sequences_1, maxlen=max_sequence_length)\n    train_padded_data_2 = pad_sequences(train_sequences_2, maxlen=max_sequence_length)\n    train_labels = np.array(is_similar)\n    leaks = np.array(leaks)\n\n    shuffle_indices = np.random.permutation(np.arange(len(train_labels)))\n    train_data_1_shuffled = train_padded_data_1[shuffle_indices]\n    train_data_2_shuffled = train_padded_data_2[shuffle_indices]\n    train_labels_shuffled = train_labels[shuffle_indices]\n    leaks_shuffled = leaks[shuffle_indices]\n\n    dev_idx = max(1, int(len(train_labels_shuffled) * validation_split_ratio))\n\n    del train_padded_data_1\n    del train_padded_data_2\n    gc.collect()\n\n    train_data_1, val_data_1 = train_data_1_shuffled[:-dev_idx], train_data_1_shuffled[-dev_idx:]\n    train_data_2, val_data_2 = train_data_2_shuffled[:-dev_idx], train_data_2_shuffled[-dev_idx:]\n    labels_train, labels_val = train_labels_shuffled[:-dev_idx], train_labels_shuffled[-dev_idx:]\n    leaks_train, leaks_val = leaks_shuffled[:-dev_idx], leaks_shuffled[-dev_idx:]\n\n    return train_data_1, train_data_2, labels_train, leaks_train, val_data_1, val_data_2, labels_val, leaks_val\n\n\ndef create_test_data(tokenizer, test_sentences_pair, max_sequence_length):\n    \"\"\"\n    Create training and validation dataset\n    Args:\n        tokenizer (keras.preprocessing.text.Tokenizer): keras tokenizer object\n        test_sentences_pair (list): list of tuple of sentences pairs\n        max_sequence_length (int): max sequence length of sentences to apply padding\n\n    Returns:\n        test_data_1 (list): list of input features for training set from sentences1\n        test_data_2 (list): list of input features for training set from sentences2\n    \"\"\"\n    test_sentences1 = [x[0].lower() for x in test_sentences_pair]\n    test_sentences2 = [x[1].lower() for x in test_sentences_pair]\n\n    test_sequences_1 = tokenizer.texts_to_sequences(test_sentences1)\n    test_sequences_2 = tokenizer.texts_to_sequences(test_sentences2)\n    leaks_test = [[len(set(x1)), len(set(x2)), len(set(x1).intersection(x2))]\n                  for x1, x2 in zip(test_sequences_1, test_sequences_2)]\n\n    leaks_test = np.array(leaks_test)\n    test_data_1 = pad_sequences(test_sequences_1, maxlen=max_sequence_length)\n    test_data_2 = pad_sequences(test_sequences_2, maxlen=max_sequence_length)\n\n    return test_data_1, test_data_2, leaks_test","20bc0c51":"# keras imports\nfrom keras.layers import Dense, Input, LSTM, Dropout, Bidirectional\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.embeddings import Embedding\nfrom keras.layers.merge import concatenate\nfrom keras.callbacks import TensorBoard\nfrom keras.models import load_model\nfrom keras.models import Model\n\n# std imports\nimport time\nimport gc\nimport os\n\n#from inputHandler import create_train_dev_set\n\n\n\nclass SiameseBiLSTM:\n    def __init__(self, embedding_dim, max_sequence_length, number_lstm, number_dense, rate_drop_lstm, \n                 rate_drop_dense, hidden_activation, validation_split_ratio):\n        self.embedding_dim = embedding_dim\n        self.max_sequence_length = max_sequence_length\n        self.number_lstm_units = number_lstm\n        self.rate_drop_lstm = rate_drop_lstm\n        self.number_dense_units = number_dense\n        self.activation_function = hidden_activation\n        self.rate_drop_dense = rate_drop_dense\n        self.validation_split_ratio = validation_split_ratio\n\n    def train_model(self, sentences_pair, is_similar, embedding_meta_data, model_save_directory='.\/'):\n        \"\"\"\n        Train Siamese network to find similarity between sentences in `sentences_pair`\n            Steps Involved:\n                1. Pass the each from sentences_pairs  to bidirectional LSTM encoder.\n                2. Merge the vectors from LSTM encodes and passed to dense layer.\n                3. Pass the  dense layer vectors to sigmoid output layer.\n                4. Use cross entropy loss to train weights\n        Args:\n            sentences_pair (list): list of tuple of sentence pairs\n            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n            model_save_directory (str): working directory for where to save models\n        Returns:\n            return (best_model_path):  path of best model\n        \"\"\"\n        tokenizer, embedding_matrix = embedding_meta_data['tokenizer'], embedding_meta_data['embedding_matrix']\n\n        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, sentences_pair,\n                                                                               is_similar, self.max_sequence_length,\n                                                                               self.validation_split_ratio)\n\n        if train_data_x1 is None:\n            print(\"++++ !! Failure: Unable to train model ++++\")\n            return None\n\n        nb_words = len(tokenizer.word_index) + 1\n\n        # Creating word embedding layer\n        embedding_layer = Embedding(nb_words, self.embedding_dim, weights=[embedding_matrix],\n                                    input_length=self.max_sequence_length, trainable=False)\n\n        # Creating LSTM Encoder\n        lstm_layer = Bidirectional(LSTM(self.number_lstm_units, dropout=self.rate_drop_lstm, recurrent_dropout=self.rate_drop_lstm))\n\n        # Creating LSTM Encoder layer for First Sentence\n        sequence_1_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n        embedded_sequences_1 = embedding_layer(sequence_1_input)\n        x1 = lstm_layer(embedded_sequences_1)\n\n        # Creating LSTM Encoder layer for Second Sentence\n        sequence_2_input = Input(shape=(self.max_sequence_length,), dtype='int32')\n        embedded_sequences_2 = embedding_layer(sequence_2_input)\n        x2 = lstm_layer(embedded_sequences_2)\n\n        # Creating leaks input\n        leaks_input = Input(shape=(leaks_train.shape[1],))\n        leaks_dense = Dense(int(self.number_dense_units\/2), activation=self.activation_function)(leaks_input)\n\n        # Merging two LSTM encodes vectors from sentences to\n        # pass it to dense layer applying dropout and batch normalisation\n        merged = concatenate([x1, x2, leaks_dense])\n        merged = BatchNormalization()(merged)\n        merged = Dropout(self.rate_drop_dense)(merged)\n        merged = Dense(self.number_dense_units, activation=self.activation_function)(merged)\n        merged = BatchNormalization()(merged)\n        merged = Dropout(self.rate_drop_dense)(merged)\n        preds = Dense(1, activation='sigmoid')(merged)\n\n        model = Model(inputs=[sequence_1_input, sequence_2_input, leaks_input], outputs=preds)\n        model.compile(loss='binary_crossentropy', optimizer='nadam', metrics=['acc'])\n\n        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n\n        STAMP = 'lstm_%d_%d_%.2f_%.2f' % (self.number_lstm_units, self.number_dense_units, self.rate_drop_lstm, self.rate_drop_dense)\n\n        checkpoint_dir = model_save_directory + 'checkpoints\/' + str(int(time.time())) + '\/'\n\n        if not os.path.exists(checkpoint_dir):\n            os.makedirs(checkpoint_dir)\n\n        bst_model_path = checkpoint_dir + STAMP + '.h5'\n\n        model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=False)\n\n        tensorboard = TensorBoard(log_dir=checkpoint_dir + \"logs\/{}\".format(time.time()))\n\n        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n                  epochs=200, batch_size=64, shuffle=True,\n                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n\n        return bst_model_path\n\n\n    def update_model(self, saved_model_path, new_sentences_pair, is_similar, embedding_meta_data):\n        \"\"\"\n        Update trained siamese model for given new sentences pairs \n            Steps Involved:\n                1. Pass the each from sentences from new_sentences_pair to bidirectional LSTM encoder.\n                2. Merge the vectors from LSTM encodes and passed to dense layer.\n                3. Pass the  dense layer vectors to sigmoid output layer.\n                4. Use cross entropy loss to train weights\n        Args:\n            model_path (str): model path of already trained siamese model\n            new_sentences_pair (list): list of tuple of new sentences pairs\n            is_similar (list): target value 1 if same sentences pair are similar otherwise 0\n            embedding_meta_data (dict): dict containing tokenizer and word embedding matrix\n        Returns:\n            return (best_model_path):  path of best model\n        \"\"\"\n        tokenizer = embedding_meta_data['tokenizer']\n        train_data_x1, train_data_x2, train_labels, leaks_train, \\\n        val_data_x1, val_data_x2, val_labels, leaks_val = create_train_dev_set(tokenizer, new_sentences_pair,\n                                                                               is_similar, self.max_sequence_length,\n                                                                               self.validation_split_ratio)\n        model = load_model(saved_model_path)\n        model_file_name = saved_model_path.split('\/')[-1]\n        new_model_checkpoint_path  = saved_model_path.split('\/')[:-2] + str(int(time.time())) + '\/' \n\n        new_model_path = new_model_checkpoint_path + model_file_name\n        model_checkpoint = ModelCheckpoint(new_model_checkpoint_path + model_file_name,\n                                           save_best_only=True, save_weights_only=False)\n\n        early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n\n        tensorboard = TensorBoard(log_dir=new_model_checkpoint_path + \"logs\/{}\".format(time.time()))\n\n        model.fit([train_data_x1, train_data_x2, leaks_train], train_labels,\n                  validation_data=([val_data_x1, val_data_x2, leaks_val], val_labels),\n                  epochs=50, batch_size=3, shuffle=True,\n                  callbacks=[early_stopping, model_checkpoint, tensorboard])\n\n        return new_model_path","29d40210":"EMBEDDING_DIM = 50\n\nMAX_SEQUENCE_LENGTH = 10\nVALIDATION_SPLIT = 0.1\n\n\nRATE_DROP_LSTM = 0.17\nRATE_DROP_DENSE = 0.25\nNUMBER_LSTM = 50\nNUMBER_DENSE_UNITS = 50\nACTIVATION_FUNCTION = 'relu'\n\n\nsiamese_config = {\n\t'EMBEDDING_DIM': EMBEDDING_DIM,\n\t'MAX_SEQUENCE_LENGTH' : MAX_SEQUENCE_LENGTH,\n\t'VALIDATION_SPLIT': VALIDATION_SPLIT,\n\t'RATE_DROP_LSTM': RATE_DROP_LSTM,\n\t'RATE_DROP_DENSE': RATE_DROP_DENSE,\n\t'NUMBER_LSTM': NUMBER_LSTM,\n\t'NUMBER_DENSE_UNITS': NUMBER_DENSE_UNITS,\n\t'ACTIVATION_FUNCTION': ACTIVATION_FUNCTION\n}","ef31caac":"#from model import SiameseBiLSTM\n#from inputHandler import word_embed_meta_data, create_test_data\n#from config import siamese_config\nimport pandas as pd\n\n############ Data Preperation ##########\n\ndf = pd.read_csv('..\/input\/datasetsnn\/lstm-siamese-text-similarity-master\/sample_data.csv')\n\nsentences1 = list(df['sentences1'])\nsentences2 = list(df['sentences2'])\nis_similar = list(df['is_similar'])\ndel df\n\n######## Word Embedding ############\n\ntokenizer, embedding_matrix = word_embed_meta_data(sentences1 + sentences2,  siamese_config['EMBEDDING_DIM'])\n\nembedding_meta_data = {\n\t'tokenizer': tokenizer,\n\t'embedding_matrix': embedding_matrix\n}\n\n## creating sentence pairs\nsentences_pair = [(x1, x2) for x1, x2 in zip(sentences1, sentences2)]\ndel sentences1\ndel sentences2\n\n######## Training ########\n\nclass Configuration(object):\n    \"\"\"Dump stuff here\"\"\"\n\nCONFIG = Configuration()\n\nCONFIG.embedding_dim = siamese_config['EMBEDDING_DIM']\nCONFIG.max_sequence_length = siamese_config['MAX_SEQUENCE_LENGTH']\nCONFIG.number_lstm_units = siamese_config['NUMBER_LSTM']\nCONFIG.rate_drop_lstm = siamese_config['RATE_DROP_LSTM']\nCONFIG.number_dense_units = siamese_config['NUMBER_DENSE_UNITS']\nCONFIG.activation_function = siamese_config['ACTIVATION_FUNCTION']\nCONFIG.rate_drop_dense = siamese_config['RATE_DROP_DENSE']\nCONFIG.validation_split_ratio = siamese_config['VALIDATION_SPLIT']\n\nsiamese = SiameseBiLSTM(CONFIG.embedding_dim , CONFIG.max_sequence_length, CONFIG.number_lstm_units , CONFIG.number_dense_units, CONFIG.rate_drop_lstm, CONFIG.rate_drop_dense, CONFIG.activation_function, CONFIG.validation_split_ratio)\n\nbest_model_path = siamese.train_model(sentences_pair, is_similar, embedding_meta_data, model_save_directory='.\/')","a9e431a4":"###Testing\nfrom operator import itemgetter\nfrom keras.models import load_model\n\nmodel = load_model(best_model_path)\n\ntest_sentence_pairs = [('What can make Physics easy to learn?','How can you make physics easy to learn?'),('How many times a day do a clocks hands overlap?','What does it mean that every time I look at the clock the numbers are the same?')]\n\ntest_data_x1, test_data_x2, leaks_test = create_test_data(tokenizer,test_sentence_pairs,  siamese_config['MAX_SEQUENCE_LENGTH'])\n\npreds = list(model.predict([test_data_x1, test_data_x2, leaks_test], verbose=1).ravel())\nresults = [(x, y, z) for (x, y), z in zip(test_sentence_pairs, preds)]\nresults.sort(key=itemgetter(2), reverse=True)\nprint (results)","77c80132":"import nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","79f82eff":"df=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')\ndf=df[['v1', 'v2']]\ndf.head()","020b2b43":"df['Spam']=pd.get_dummies(df['v1'], drop_first=True)","6f8b1244":"df=df.drop('v1', axis=1)\ndf.head()","0c810e97":"sns.countplot(x='Spam',data=df)","fb64ed20":"sns.countplot(x='Spam',data=df)","125c1f2f":"import re\nimport string\nfrom string import punctuation\nfrom nltk.corpus import stopwords","84499f12":"def text_processing(message):\n    \n    Stopwords = stopwords.words('english')\n    # Check characters to see if they are in punctuation\n    no_punctuation = [char for char in message if char not in string.punctuation]\n\n    # Join the characters again to form the string.\n    no_punctuation = ''.join(no_punctuation)\n    \n    # Now just remove any stopwords\n    return ' '.join([word for word in no_punctuation.split() if word.lower() not in Stopwords])","e9aed6b9":"df['text'] = df['v2'].apply(text_processing)\ndf.head()","6945e660":"from sklearn.model_selection import train_test_split\nX=df['text'].values\ny=df['Spam'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","141223a5":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#Vectorization\nbow = CountVectorizer()\nX_train = bow.fit_transform(X_train)\nX_test = bow.transform(X_test)","109035e0":"\n#Term Frequency, Inverse Document Frequency\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)\nX_train=X_train.toarray()\nX_test=X_test.toarray()","9ae0577d":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","1f4fc598":"X_train.shape","27ff3c1c":"model = Sequential()\nmodel.add(Dense(units=8270,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=4000,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1000,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=400,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","f8f92257":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","55ea575b":"model.fit(x=X_train, \n          y=y_train, \n          epochs=40,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","dae694dd":"import pandas as pd\nimport numpy as np\nimport nltk\nimport sklearn","904ce25f":"\n\"\"\"\n    News headline get from \n    \n    https:\/\/www.reuters.com\/article\/us-musk-tunnel\/elon-musks-boring-co-to-build-high-speed-airport-link-in-chicago-idUSKBN1JA224\n    http:\/\/money.cnn.com\/2018\/06\/14\/technology\/elon-musk-boring-company-chicago\/index.html\n    https:\/\/www.theverge.com\/2018\/6\/13\/17462496\/elon-musk-boring-company-approved-tunnel-chicago\n\n\"\"\"\n\nnews_headline1 = \"Elon Musk's Boring Co to build high-speed airport link in Chicago\"\nnews_headline2 = \"Elon Musk's Boring Company to build high-speed Chicago airport link\"\nnews_headline3 = \"Elon Musk\u2019s Boring Company approved to build high-speed transit between downtown Chicago and O\u2019Hare Airport\"\nnews_headline4 = \"Both apple and orange are fruit\"\n\nnews_headlines = [news_headline1, news_headline2, news_headline3, news_headline4]","15b6a862":"\nnews_headline1_tokens = nltk.word_tokenize(news_headline1)\nnews_headline2_tokens = nltk.word_tokenize(news_headline2)\nnews_headline3_tokens = nltk.word_tokenize(news_headline3)\nnews_headline4_tokens = nltk.word_tokenize(news_headline4)\n\nfor words in [news_headline1_tokens, news_headline2_tokens, news_headline3_tokens, news_headline4_tokens]:\n    print('First 7 tokens from news headlines: ', words[:7])","9cc72e17":"from numpy import argmax\n\ndef transform(headlines):\n    tokens = [w for s in headlines for w in s ]\n    print()\n    print('All Tokens:')\n    print(tokens)\n\n    results = []\n    label_enc = sklearn.preprocessing.LabelEncoder()\n    onehot_enc = sklearn.preprocessing.OneHotEncoder()\n    \n    encoded_all_tokens = label_enc.fit_transform(list(set(tokens)))\n    encoded_all_tokens = encoded_all_tokens.reshape(len(encoded_all_tokens), 1)\n    \n    onehot_enc.fit(encoded_all_tokens)\n    \n    for headline_tokens in headlines:\n        print()\n        print('Original Input:', headline_tokens)\n        \n        encoded_words = label_enc.transform(headline_tokens)\n        print('Encoded by Label Encoder:', encoded_words)\n        \n        encoded_words = onehot_enc.transform(encoded_words.reshape(len(encoded_words), 1))\n        print('Encoded by OneHot Encoder:')\n        print(encoded_words)\n\n        results.append(np.sum(encoded_words.toarray(), axis=0))\n    \n    return results\n\ntransformed_results = transform([\n    news_headline1_tokens, news_headline2_tokens, news_headline3_tokens, news_headline4_tokens])","75d02ae5":"print('Master Sentence: %s' % news_headlines[0])\nfor i, news_headline in enumerate(news_headlines):\n    score = sklearn.metrics.pairwise.euclidean_distances([transformed_results[i]], [transformed_results[0]])[0][0]\n    print('-----')\n    print('Score: %.2f, Comparing Sentence: %s' % (score, news_headline))","e0908eaf":"print('Master Sentence: %s' % news_headlines[0])\nfor i, news_headline in enumerate(news_headlines):\n    score = sklearn.metrics.pairwise.cosine_similarity([transfaormed_results[i]], [transformed_results[0]])[0][0]\n    print('-----')","8fbcc62f":"\"\"\"\n    Finding the posistion (from lookup table) of word instead of using 1 or 0\n    to prevent misleading of the meaning of \"common\" word\n\"\"\"\n\ndef calculate_position(values):\n    x = []\n    for pos, matrix in enumerate(values):\n        if matrix > 0:\n            x.append(pos)\n    return x\n\n\"\"\"\n    Since scikit-learn can only compare same number of dimension of input. \n    Add padding to the shortest sentence.\n\"\"\"\ndef padding(sentence1, sentence2):\n    x1 = sentence1.copy()\n    x2 = sentence2.copy()\n    \n    diff = len(x1) - len(x2)\n    \n    if diff > 0:\n        for i in range(0, diff):\n            x2.append(-1)\n    elif diff < 0:\n        for i in range(0, abs(diff)):\n            x1.append(-1)\n    \n    return x1, x2    \n\ny_actual = calculate_position(transformed_results[0])\n\nprint('Master Sentence: %s' % news_headlines[0])\nfor i, news_headline in enumerate(news_headlines):\n    y_compare = calculate_position(transformed_results[i])\n    x1, x2 = padding(y_actual, y_compare)\n    score = sklearn.metrics.jaccard_similarity_score(x1, x2)\n    print('-----')\n    print('Score: %.2f, Comparing Sentence: %s' % (score, news_headline))","b02447a5":"\"\"\"\n    News headline get from \n    \n    https:\/\/www.reuters.com\/article\/us-musk-tunnel\/elon-musks-boring-co-to-build-high-speed-airport-link-in-chicago-idUSKBN1JA224\n    http:\/\/money.cnn.com\/2018\/06\/14\/technology\/elon-musk-boring-company-chicago\/index.html\n    https:\/\/www.theverge.com\/2018\/6\/13\/17462496\/elon-musk-boring-company-approved-tunnel-chicago\n\n\"\"\"\n\nnews_headline1 = \"Elon Musk's Boring Co to build high-speed airport link in Chicago\"\nnews_headline2 = \"Elon Musk's Boring Company to build high-speed Chicago airport link\"\nnews_headline3 = \"Elon Musk\u2019s Boring Company approved to build high-speed transit between downtown Chicago and O\u2019Hare Airport\"\nnews_headline4 = \"Both apple and orange are fruit\"\n\nnews_headlines = [news_headline1, news_headline2, news_headline3, news_headline4]","f6c45f09":"# Load Word Embedding Model\nimport gensim\nprint('gensim version: %s' % gensim.__version__)\nglove_model = gensim.models.KeyedVectors.load_word2vec_format('..\/model\/text\/stanford\/glove\/glove.6B.50d.vec')","fcdf1c08":"# Remove stopwords\nimport spacy\nspacy_nlp = spacy.load('en')\n\nheadline_tokens = []\nfor news_headline in news_headlines:\n    headline_tokens.append([token.text.lower() for token in spacy_nlp(news_headline) if not token.is_stop])\n\nprint(headline_tokens)","de7cf724":"subject_headline = news_headlines[0]\nsubject_token = headline_tokens[0]\n\nprint('Headline: ', subject_headline)\nprint('=' * 50)\nprint()\n\nfor token, headline in zip(headline_tokens, news_headlines):\n    print('-' * 50)\n    print('Comparing to:', headline)\n    distance = glove_model.wmdistance(subject_token, token)\n    print('distance = %.4f' % distance)","1f45c2c8":"# Confusion matrix with sklearn\nimport numpy as np\nfrom sklearn.metrics import confusion_matrix\nactuals = np.array([1, 1, 1, 1, 1, 0, 0, 1, 1, 1])\npredictions = np.array([1, 0, 1, 0, 1, 0, 1, 1, 1, 1])\n\nprint(\"Confusion matrix \\n\", confusion_matrix(actuals, predictions))\n","d37c14cd":"# Creating the confusion matrix graphs\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\ncf_train_matrix = confusion_matrix(actuals, predictions)\nplt.figure(figsize=(10,8))\nsns.heatmap(cf_train_matrix, annot=True, fmt='d')","2161b3ca":"import pandas as pd\n\n############ Data Preperation ##########\n\ndf = pd.read_csv('..\/input\/dataite\/DataITE.csv',encoding='latin')","1a888af1":"df.head","1452661d":"import pandas as pd\n\ndf = pd.read_excel (r'..\/input\/dataiteexcel\/DataITE.xlsx')\nprint (df)","e2bb6c6f":"df['labelx'] = df['status'].apply(lambda x: 1 if x==1 else 0)\ndf","501b4f74":"import re\ndef preprocess_text(document):\n        # Remove all the special characters\n        document = re.sub(r'\\W', ' ', str(document))\n\n        # remove all single characters\n        document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n\n        # Remove single characters from the start\n        document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document)\n\n        # Substituting multiple spaces with single space\n        document = re.sub(r'\\s+', ' ', document, flags=re.I)\n\n        # Removing prefixed 'b'\n        document = re.sub(r'^b\\s+', '', document)\n\n        # Converting to Lowercase\n        document = document.lower()\n\n        \n        return document","18ab643c":"datadocs=[]\nfor document in df['text']:\n    print(preprocess_text(document))\n    datadocs.append(preprocess_text(document))","51c6d176":"pip install Sastrawi","e4febb6b":"from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\ndataakhir=[]\nfactory = StopWordRemoverFactory()\nstopword = factory.create_stop_word_remover()\nfor datadoc in datadocs:\n     print(stopword.remove(datadoc))\n     dataakhir.append(stopword.remove(datadoc)) ","47ce75e8":"dataakhir","a7c2a896":"columns = ['dataclean']","b8c74b97":"df['dataclean'] = dataakhir\ndf.head","557c8565":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df['dataclean'], df['labelx'], test_size = 0.3,random_state=0)","4b283f6b":"X_train.head","fa134f20":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer()\nX_train_cv = cv.fit_transform(X_train)\nX_test_cv = cv.transform(X_test)","30b0424b":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cv, y_train)\npredictions_nb = naive_bayes.predict(X_test_cv)","1c13f34e":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions_nb))\nprint('Precision score: ', precision_score(y_test, predictions_nb))\nprint('Recall score: ', recall_score(y_test, predictions_nb))","0c36e0e8":"from sklearn.metrics import confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncm = confusion_matrix(y_test, predictions_nb)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","e9d44026":"from sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=5) \nclassifier.fit(X_train_cv, y_train)","0a9406c9":"predictions_knn = classifier.predict(X_test_cv)","b9421279":"print('Accuracy score: ', accuracy_score(y_test, predictions_knn))\nprint('Precision score: ', precision_score(y_test, predictions_knn))\nprint('Recall score: ', recall_score(y_test, predictions_knn))","8d06aafb":"cm = confusion_matrix(y_test, predictions_knn)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","ffb09cdd":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train_cv,y_train)\n\npredictions_lr = lr.predict(X_test_cv)","ccd97af9":"print('Accuracy score: ', accuracy_score(y_test, predictions_lr))\nprint('Precision score: ', precision_score(y_test, predictions_lr))\nprint('Recall score: ', recall_score(y_test, predictions_lr))","a1a6a173":"cm = confusion_matrix(y_test, predictions_lr)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","f94c429e":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train_cv, y_train)\npredictions_svm = sv.predict(X_test_cv)","860317ae":"print('Accuracy score: ', accuracy_score(y_test, predictions_svm))\nprint('Precision score: ', precision_score(y_test, predictions_svm))\nprint('Recall score: ', recall_score(y_test, predictions_svm))","c2f6153c":"cm = confusion_matrix(y_test, predictions_svm)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","75527d08":"from sklearn.tree import DecisionTreeClassifier\nmax_accuracy = 0\npredictions_dt=0","1415bc8b":"for x in range(20):\n    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cv,y_train)\n    Y_pred_dt = dt.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        predictions_dt= Y_pred_dt\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)","e2bbf159":"print('Accuracy score: ', accuracy_score(y_test, predictions_dt))\nprint('Precision score: ', precision_score(y_test, predictions_dt))\nprint('Recall score: ', recall_score(y_test, predictions_dt))","b17bc705":"cm = confusion_matrix(y_test, predictions_dt)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","b9e63191":"from sklearn.ensemble import RandomForestClassifier\nmax_accuracy = 0\npredictions_rf=0\nfor x in range(20):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train_cv,y_train)\n    Y_pred_rf = rf.predict(X_test_cv)\n    current_accuracy = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        predictions_rf=Y_pred_rf\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)","1e8c8271":"cm = confusion_matrix(y_test, predictions_rf)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","2a73d21a":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(objective=\"binary:logistic\", random_state=42)\nxgb_model.fit(X_train_cv, y_train)\n\npredictions_xgb = xgb_model.predict(X_test_cv)","84c38636":"print('Accuracy score: ', accuracy_score(y_test, predictions_xgb))\nprint('Precision score: ', precision_score(y_test, predictions_xgb))\nprint('Recall score: ', recall_score(y_test, predictions_xgb))","12c8f4ca":"cm = confusion_matrix(y_test, predictions_xgb)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","b9900db7":"# CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\nimport pandas as pd","56394818":"tfidfvectorizer = TfidfVectorizer()","a7179bfd":"X_train_cvtf = tfidfvectorizer.fit_transform(X_train)\nX_test_cvtf = tfidfvectorizer.transform(X_test)","ae944e52":"from sklearn.naive_bayes import MultinomialNB\nnaive_bayes = MultinomialNB()\nnaive_bayes.fit(X_train_cvtf, y_train)\npredictions_nb = naive_bayes.predict(X_test_cvtf)","2f247b6b":"from sklearn.metrics import accuracy_score, precision_score, recall_score\nprint('Accuracy score: ', accuracy_score(y_test, predictions_nb))\nprint('Precision score: ', precision_score(y_test, predictions_nb))\nprint('Recall score: ', recall_score(y_test, predictions_nb))","3484c0a8":"cm = confusion_matrix(y_test, predictions_nb)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","282366c4":"from sklearn.neighbors import KNeighborsClassifier \nclassifier = KNeighborsClassifier(n_neighbors=5) \nclassifier.fit(X_train_cvtf, y_train)","c5acbb67":"predictions_knn = classifier.predict(X_test_cvtf)","abc14d25":"print('Accuracy score: ', accuracy_score(y_test, predictions_knn))\nprint('Precision score: ', precision_score(y_test, predictions_knn))\nprint('Recall score: ', recall_score(y_test, predictions_knn))","b4005c9e":"sns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","2b67163b":"from sklearn.linear_model import LogisticRegression\n\nlr = LogisticRegression()\n\nlr.fit(X_train_cvtf,y_train)\n\npredictions_lr = lr.predict(X_test_cvtf)","aa95f028":"print('Accuracy score: ', accuracy_score(y_test, predictions_lr))\nprint('Precision score: ', precision_score(y_test, predictions_lr))\nprint('Recall score: ', recall_score(y_test, predictions_lr))","738aff03":"cm = confusion_matrix(y_test, predictions_lr)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","6742343f":"from sklearn import svm\n\nsv = svm.SVC(kernel='linear')\n\nsv.fit(X_train_cvtf, y_train)\npredictions_svm = sv.predict(X_test_cvtf)","a92c0285":"print('Accuracy score: ', accuracy_score(y_test, predictions_svm))\nprint('Precision score: ', precision_score(y_test, predictions_svm))\nprint('Recall score: ', recall_score(y_test, predictions_svm))","98da69e8":"cm = confusion_matrix(y_test, predictions_svm)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","79c8c9d5":"from sklearn.tree import DecisionTreeClassifier\nmax_accuracy = 0\npredictions_dt=0","58f91848":"    dt = DecisionTreeClassifier(random_state=x)\n    dt.fit(X_train_cvtf,y_train)\n    Y_pred_dt = dt.predict(X_test_cvtf)\n    current_accuracy = round(accuracy_score(Y_pred_dt,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        predictions_dt= Y_pred_dt\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)","1444db0f":"print('Accuracy score: ', accuracy_score(y_test, predictions_dt))\nprint('Precision score: ', precision_score(y_test, predictions_dt))\nprint('Recall score: ', recall_score(y_test, predictions_dt))","3444dbab":"cm = confusion_matrix(y_test, predictions_dt)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","66626175":"from sklearn.ensemble import RandomForestClassifier\nmax_accuracy = 0\npredictions_rf=0\nfor x in range(20):\n    rf = RandomForestClassifier(random_state=x)\n    rf.fit(X_train_cvtf,y_train)\n    Y_pred_rf = rf.predict(X_test_cvtf)\n    current_accuracy = round(accuracy_score(Y_pred_rf,y_test)*100,2)\n    if(current_accuracy>max_accuracy):\n        max_accuracy = current_accuracy\n        predictions_rf=Y_pred_rf\n        best_x = x\n        \nprint(max_accuracy)\nprint(best_x)","74f88af5":"print('Accuracy score: ', accuracy_score(y_test, predictions_rf))\nprint('Precision score: ', precision_score(y_test, predictions_rf))\nprint('Recall score: ', recall_score(y_test, predictions_rf))","44749941":"cm = confusion_matrix(y_test, predictions_rf)\nsns.heatmap(cm, square=True, annot=True, cmap='RdBu', cbar=False,\nxticklabels=['kabul', 'tolak'], yticklabels=['kabul', 'tolak'])\nplt.xlabel('true label')\nplt.ylabel('predicted label')","43d5459f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping","f53be917":"df.head","41be1f89":"sns.countplot(df.status)\nplt.xlabel('Label')\nplt.title('Number of kabul dan tolak')\n","ca0744b7":"X = df.dataclean\nY = df.status\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY = Y.reshape(-1,1)","0853f06a":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","157e2330":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(X_train)\nsequences = tok.texts_to_sequences(X_train)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","7b26f952":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","e3babffc":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","598f0185":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10, validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","470e7780":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","b55ee31c":"accr = model.evaluate(test_sequences_matrix,Y_test)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","7e56f215":"import nltk\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","fc10780b":"from sklearn.model_selection import train_test_split\nX=df['dataclean'].values\ny=df['status'].values\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","3d5f4dee":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n#Vectorization\nbow = CountVectorizer()\nX_train = bow.fit_transform(X_train)\nX_test = bow.transform(X_test)","b676cb4c":"#Term Frequency, Inverse Document Frequency\nfrom sklearn.feature_extraction.text import TfidfTransformer\ntfidf = TfidfTransformer()\nX_train = tfidf.fit_transform(X_train)\nX_test = tfidf.transform(X_test)\nX_train=X_train.toarray()\nX_test=X_test.toarray()","d14305e4":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout","be2aee84":"X_train.shape","db50455b":"model = Sequential()\nmodel.add(Dense(units=8270,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=4000,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1000,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=400,activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(units=1,activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam')","60b7d529":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)","8e0845c4":"model.fit(x=X_train, \n          y=y_train, \n          epochs=40,\n          validation_data=(X_test, y_test), verbose=1,\n          callbacks=[early_stop]\n          )","5778b2c6":"model.summary()","1506e037":"predictions = model.predict_classes(X_test)","bcf5c18b":"from sklearn.metrics import classification_report,confusion_matrix","67e52b9b":"print(confusion_matrix(y_test,predictions))\nprint(classification_report(y_test,predictions))","65928c60":"ann","7e0d15ef":"https:\/\/github.com\/adsieg","72d531fd":"penelitian.....klasifikasi","92516198":"https:\/\/github.com\/theAIGuysCode\nhttps:\/\/github.com\/theAIGuysCode\/yolov4-custom-functions\nhttps:\/\/github.com\/nicknochnack","d54cdb45":"https:\/\/github.com\/likejazz\/Siamese-LSTM","c38fc0f2":"pkai tf idf","8a5f6743":"https:\/\/dataaspirant.com\/build-email-spam-classification-model-spacy-python\/","afd43bd6":"https:\/\/www.kaggle.com\/eliotbarr\/text-classification-using-neural-networks\nX_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.15)","818b4927":"snn\nhttps:\/\/github.com\/amansrivastava17\/lstm-siamese-text-similarity","c9b81708":"https:\/\/medium.com\/analytics-vidhya\/spam-classification-with-tensorflow-keras-7e9fb8ace263","de05b20d":"terbukti secara sah dan meyakinkan menurut hukum bersalah\n(501, 232)\n0    361\n1     49\n5     31\n4     31\n2     15\n6      7\n3      7","77ce496b":"(501, 424)\n0    361\n4     47\n3     39\n1     25\n2     15\n6      7\n5      7\n\n(501, 232) 0 361 1 49 5 31 4 31 2 15 6 7 3 7","153f2bea":"salah yang dibawah","ad031728":"https:\/\/github.com\/Harshita9511\/Spam-Classification-with-Tensorflow-Keras\/blob\/master\/Spam_Classification.ipynb","9eb2deed":"ann\nhttps:\/\/www.kaggle.com\/kredy10\/simple-lstm-for-text-classification","045c3e4d":"https:\/\/medium.com\/voice-tech-podcast\/text-classification-using-cnn-9ade8155dfb9","f1a618fb":"klastering","9a283a6a":"ini mulai klasifikasi","d911b6a5":"salah jangan dipakai","a7370356":"model.fit(sequences_matrix,Y_train,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","413b9902":"ini juga lain","e9104924":"https:\/\/github.com\/rsreetech\/TextClassificationWithSpacy","f5b0e47c":"https:\/\/www.analyticsvidhya.com\/blog\/2020\/04\/beginners-guide-exploratory-data-analysis-text-data\/\nhttps:\/\/towardsdatascience.com\/nlp-part-3-exploratory-data-analysis-of-text-data-1caa8ab3f79d\nhttps:\/\/towardsdatascience.com\/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a","aed10fec":"https:\/\/github.com\/makcedward\/nlp\/blob\/master\/sample\/nlp-3_basic_distance_measurement_in_text_mining.ipynb","9355746b":"natural language tools kit\n"}}