{"cell_type":{"b27bb414":"code","eeb7ce7a":"code","ae7bb0f2":"code","5f61f536":"code","2fd8acc4":"code","47b7a295":"code","f00c1247":"code","2499c843":"code","e70552c3":"code","37a95dd9":"code","be97e51c":"code","a6020b2f":"code","82edd16b":"code","7d4760bc":"code","8d4df7fc":"code","fa5214e6":"code","109e8c49":"code","3820b669":"code","ce4b82e0":"code","86282bf5":"code","4bed6b8a":"code","21184b8f":"code","047dcc3c":"code","cc531302":"code","0b3cd526":"code","566e99dd":"code","555e86d1":"code","e9c3826d":"code","43b60be0":"code","88697789":"code","42db34d1":"code","b2f8929a":"code","5dff5ca4":"code","c9b4982a":"code","209c6575":"code","348c9a64":"code","9e0a3205":"code","7829298b":"code","b9193d32":"code","f2e8680b":"code","dd7d5127":"code","46034195":"code","ad441a01":"code","3a5321e6":"code","74efc74c":"code","f76542b3":"code","bd957522":"code","d54764ea":"code","0d98b61d":"code","9aca7f00":"code","cceb7fe0":"code","52b39a3c":"code","df94b54f":"code","b6e043c6":"code","8896f61c":"code","3f54b06c":"code","7a8bf9ca":"code","1ab140cb":"code","e10f290d":"code","eaebc990":"code","3a2ddf00":"code","a818fbc9":"code","787abc7c":"code","cb79d651":"code","3e3c7763":"code","e92eea11":"code","699a65fc":"code","497a431a":"code","13fbf274":"markdown","fdee7374":"markdown","0a4760f2":"markdown","78dd833d":"markdown","2202439a":"markdown","e33e169b":"markdown","30b5bb61":"markdown","426376ba":"markdown","2193bba2":"markdown","92a57914":"markdown","46b85ad9":"markdown","27d49517":"markdown","c2bab4ea":"markdown","690fab87":"markdown","4ff46239":"markdown","dbd39898":"markdown","e2c57e0d":"markdown","10e78071":"markdown","5275a2f6":"markdown","2d373572":"markdown","9bbe99e7":"markdown","50c11b69":"markdown","7b0ed191":"markdown","3c3c471f":"markdown","674fc753":"markdown","90f6672e":"markdown","e6891313":"markdown","5b2a7455":"markdown","adea4f28":"markdown","9c856ff3":"markdown","fdb332f3":"markdown","3e3babb2":"markdown","9e21c257":"markdown"},"source":{"b27bb414":"#general packages for data manipulation\nimport os\nimport pandas as pd\nimport numpy as np\n#visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#consistent sized plot \nfrom pylab import rcParams\nrcParams['figure.figsize']=12,5\nrcParams['axes.labelsize']=12\nrcParams['xtick.labelsize']=12\nrcParams['ytick.labelsize']=12\n#handle the warnings in the code\nimport warnings\nwarnings.filterwarnings(action='ignore',category=DeprecationWarning)\nwarnings.filterwarnings(action='ignore',category=FutureWarning)\n#text preprocessing libraries\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.stem import PorterStemmer\n#import texthero\n#import texthero as hero\n#regular expressions\nimport re\n#display pandas dataframe columns \npd.options.display.max_columns = None","eeb7ce7a":"#load the csv file as a pandas dataframe\n#ISO-8859-1\ntweet = pd.read_csv('\/kaggle\/input\/twitter-hate-speech\/TwitterHate.csv',delimiter=',',engine='python',encoding='utf-8-sig')\ntweet.head()","ae7bb0f2":"#get rid of the identifier number of the tweet\ntweet.drop('id',axis=1,inplace=True)","5f61f536":"#view one of the tweets randomly \nrandom = np.random.randint(0,len(tweet))\nprint(random)\ntweet.iloc[random]['tweet']","2fd8acc4":"#create a copy of the original data to work with \ndf = tweet.copy()","47b7a295":"def simplify(text):\n    '''Function to handle the diacritics in the text'''\n    import unicodedata\n    try:\n        text = unicode(text, 'utf-8')\n    except NameError:\n        pass\n    text = unicodedata.normalize('NFD', text).encode('ascii', 'ignore').decode(\"utf-8\")\n    return str(text)","f00c1247":"df['tweet'] = df['tweet'].apply(simplify)","2499c843":"#test on a sample string\nsample = \"and @user1 i would like you to discuss with @user2 and then with @username3\"\npattern = re.compile(r'@\\w+')\nre.findall(pattern,sample)","e70552c3":"#remove all the user handles --> strings starting with @\ndf['tweet'].replace(r'@\\w+','',regex=True,inplace=True)","37a95dd9":"#test on a sample \nsample = \"https:\/\/www.machinelearing.com prakhar and https:\/\/www.simple.com\"\npattern = re.compile(r'http\\S+')\nre.findall(pattern,sample)","be97e51c":"df['tweet'].replace(r'http\\S+','',regex=True,inplace=True)","a6020b2f":"#test on a sample text\nsample = 'wonderfl :-)  when are you coming for #party'\ntweet_tokenize = TweetTokenizer(preserve_case=True)\ntweet_tokenize.tokenize(sample)","82edd16b":"#tokenize the tweets in the dataframe using TweetTokenizer\ntokenizer = TweetTokenizer(preserve_case=True)\ndf['tweet'] = df['tweet'].apply(tokenizer.tokenize)","7d4760bc":"#view the tokenized tweets\ndf.head(3)","8d4df7fc":"stop_words = stopwords.words('english')\n\n#add additional stop words to be removed from the text\nadditional_list = ['amp','rt','u',\"can't\",'ur']\n\nfor words in additional_list:\n    stop_words.append(words)","fa5214e6":"stop_words[-10:]","109e8c49":"#remove stop words\ndef remove_stopwords(text):\n    '''Function to remove the stop words from the text corpus'''\n    clean_text = [word for word in text if not word in stop_words]\n    return clean_text    ","3820b669":"#remove the stop words from the tweets\ndf['tweet'] = df['tweet'].apply(remove_stopwords)","ce4b82e0":"df['tweet'].head()","86282bf5":"#apply spelling correction on a sample text\nfrom textblob import TextBlob\nsample = 'amazng man you did it finallyy'\ntxtblob = TextBlob(sample)\ncorrected_text = txtblob.correct()\nprint(corrected_text)","4bed6b8a":"#textblob expect a string to be passed and not a list of strings\nfrom textblob import TextBlob\n\ndef spell_check(text):\n    '''Function to do spelling correction using '''\n    txtblob = TextBlob(text)\n    corrected_text = txtblob.correct()\n    return corrected_text\n    ","21184b8f":"#try tremoving # symbols from a sample text\nsample = '#winner #machine i am learning'\npattern = re.compile(r'#')\nre.sub(pattern,'',sample)","047dcc3c":"def remove_hashsymbols(text):\n    '''Function to remove the hashtag symbol from the text'''\n    pattern = re.compile(r'#')\n    text = ' '.join(text)\n    clean_text = re.sub(pattern,'',text)\n    return tokenizer.tokenize(clean_text)    ","cc531302":"df['tweet'] = df['tweet'].apply(remove_hashsymbols)","0b3cd526":"df.head(3)","566e99dd":"def rem_shortwords(text):\n    '''Function to remove the short words of length 1 and 2 characters'''\n    '''Arguments: \n       text: string\n       returns: string without containing words of length 1 and 2'''\n    lengths = [1,2]\n    new_text = ' '.join(text)\n    for word in text:\n        text = [word for word in tokenizer.tokenize(new_text) if not len(word) in lengths]\n        \n    return new_text       \n    ","555e86d1":"df['tweet'] = df['tweet'].apply(rem_shortwords)","e9c3826d":"df.head(2)","43b60be0":"df['tweet'] = df['tweet'].apply(tokenizer.tokenize)","88697789":"df.head(3)","42db34d1":"def rem_digits(text):\n    '''Function to remove the digits from the list of strings'''\n    no_digits = []\n    for word in text:\n        no_digits.append(re.sub(r'\\d','',word))\n    return ' '.join(no_digits)   ","b2f8929a":"df['tweet'] = df['tweet'].apply(rem_digits)","5dff5ca4":"df['tweet'] = df['tweet'].apply(tokenizer.tokenize)","c9b4982a":"df.head()","209c6575":"def rem_nonalpha(text):\n    '''Function to remove the non-alphanumeric characters from the text'''\n    text = [word for word in text if word.isalpha()]\n    return text","348c9a64":"#remove the non alpha numeric characters from the tweet tokens\ndf['tweet'] = df['tweet'].apply(rem_nonalpha)","9e0a3205":"#plot of the count of hate and non hate tweet\nsns.countplot(df['label'])\nplt.title('Count of Hate vs Non Hate Tweet')\nplt.grid()\nplt.show()","7829298b":"from collections import Counter\nresults = Counter()\ndf['tweet'].apply(results.update)\n#print the top 10 most common terms in the tweet \nprint(results.most_common(10))","b9193d32":"#plot the cumulative frequency of the top 10 most common tokens \nfrequency = nltk.FreqDist(results)\nplt.title('Top 10 Most Common Terms')\nfrequency.plot(10,cumulative=True)\nplt.show()","f2e8680b":"#plot the frequency of the top 10 most common tokens \nfrequency = nltk.FreqDist(results)\nplt.title('Top 10 Most Common Terms')\nfrequency.plot(10,cumulative=False)\nplt.show()","dd7d5127":"df.head()","46034195":"#check for the null values\ndf.isnull().sum()","ad441a01":"#join the tokens back to form the string\ndf['tweet'] = df['tweet'].apply(lambda x: ' '.join(x))","3a5321e6":"#check the top rows\ndf.head(3)","74efc74c":"#split the data into input X and output y\nX = df['tweet']\ny = df['label']","f76542b3":"#split the data \nfrom sklearn.model_selection import train_test_split\nseed = 51\ntest_size = 0.2 #20% of the data in the \nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=seed,stratify=df['label'])\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","bd957522":"#import tfidf vectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer","d54764ea":"#instantiate the vectorizer \nvectorizer = TfidfVectorizer(max_features=5000)","0d98b61d":"#fit on the training data\nX_train = vectorizer.fit_transform(X_train)\n#transform the test data\nX_test = vectorizer.transform(X_test)","9aca7f00":"#check the shape\nX_train.shape, X_test.shape","cceb7fe0":"#import the models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB","52b39a3c":"#instantiate the models with default hyper-parameters\nclf = LogisticRegression()\nclf.fit(X_train,y_train)\ntrain_predictions = clf.predict(X_train)\ntest_predictions = clf.predict(X_test)","df94b54f":"#import the metrics\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","b6e043c6":"#get the model accuracy on the training and the test set\nprint('Accuracy Score on training set %.5f' %accuracy_score(y_train,train_predictions))\nprint('Accuracy Score on test set %.5f' %accuracy_score(y_test,test_predictions))","8896f61c":"print('Classification Report Training set')\nprint('\\n')\nprint(classification_report(y_train,train_predictions))","3f54b06c":"print('Classification Report Testing set')\nprint('\\n')\nprint(classification_report(y_test,test_predictions))","7a8bf9ca":"df['label'].value_counts()","1ab140cb":"#define the weight of the class labels using inverse ratio\nweights = {0:1.0,1:13.0}\n\n#instantiate the logistic regression model and account for the weights to be applied for model coefficients update magnitude\nclf = LogisticRegression(solver='lbfgs',class_weight=weights)\n\n#fit and predict\nclf.fit(X_train,y_train)\ntrain_predictions = clf.predict(X_train)\ntest_predictions = clf.predict(X_test)\n\n#classification report\nprint('Classification Report Training set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_train,train_predictions))\nprint('\\n')\n\nprint('Classification Report Testing set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_test,test_predictions))","e10f290d":"#import the required libraries for grid search\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import cross_val_score","eaebc990":"# define search space\nfrom scipy.stats import loguniform\nspace = dict()\nspace['solver'] = ['newton-cg', 'lbfgs', 'liblinear']\nspace['penalty'] = ['l1', 'l2', 'elasticnet']\nspace['C'] = loguniform(1e-5, 100)","3a2ddf00":"#check the search space \nprint(space)","a818fbc9":"#define the model with balanced class weights\nweights = {0:1.0,1:1.0}\nclf = LogisticRegression(class_weight=weights)\n#define the number of folds \nfolds = StratifiedKFold(n_splits=4,random_state=seed)\n# define search\ngrid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n                            n_jobs=-1, cv=folds, random_state=seed)\n#fit grid search on the train data\ngrid_result = grid_search.fit(X_train,y_train)","787abc7c":"#retrieve the best model \ngrid_result.best_estimator_","cb79d651":"#instantiate the best model\nclf = LogisticRegression(C=23.871926754399514,penalty='l1',solver='liblinear',class_weight=weights)","3e3c7763":"#fit and predict\nclf.fit(X_train,y_train)\ntrain_predictions = clf.predict(X_train)\ntest_predictions = clf.predict(X_test)\n\n#classification report\nprint('Classification Report Training set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_train,train_predictions))\nprint('\\n')\n\nprint('Classification Report Testing set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_test,test_predictions))","e92eea11":"#use the class weights to handle the imbalance in the labels\nweights = {0:1.0,1:13}\n\nclf = LogisticRegression(class_weight=weights)\n#define the number of folds \nfolds = StratifiedKFold(n_splits=4,random_state=seed)\n# define search\ngrid_search = RandomizedSearchCV(estimator=clf,param_distributions=space, n_iter=100, scoring='recall',\n                            n_jobs=-1, cv=folds, random_state=seed)\n#fit grid search on the train data\ngrid_result = grid_search.fit(X_train,y_train)\n\n#retrieve the best model \ngrid_result.best_estimator_","699a65fc":"#instantiate the best model\nclf = LogisticRegression(C=0.16731783677034165,penalty='l2',solver='liblinear',class_weight=weights)\n\n#fit and predict\nclf.fit(X_train,y_train)\ntrain_predictions = clf.predict(X_train)\ntest_predictions = clf.predict(X_test)\n\n#classification report\nprint('Classification Report Training set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_train,train_predictions))\nprint('\\n')\n\nprint('Classification Report Testing set')\nprint('------------------------------------')\nprint('\\n')\nprint(classification_report(y_test,test_predictions))\n","497a431a":"from sklearn.metrics import plot_confusion_matrix\nplot_confusion_matrix(clf,X_test,y_test,cmap='summer')\nplt.title('Confusion Matrix Test Set')\nplt.show()","13fbf274":"<a id='2-2'><\/a>\n### _Remove user handles_","fdee7374":"### _Data Formatting for Predictive Modeling_","0a4760f2":"<a id='3'><\/a>\n## _Exploratory Data Analysis - Broad Approach_","78dd833d":"<a name='2-1'><\/a>\n### _Handle Diacritics using text normalization_","2202439a":"_The model's f1-score is low for label 1 which indicates the hate text in the twitter_","e33e169b":"<a id='3-1'><\/a>\n### _Check for data balance_","30b5bb61":"<a id='1'><\/a>\n## _Import Libraries and Load Data_","426376ba":"_There are more non hatespeeches than the hatespeech in the dataset_","2193bba2":"<a id='4'><\/a>\n## _Predictive Modeling_","92a57914":"<a id='4-7'><\/a>\n### _Fine tuned Model with Balanced Class Weights_","46b85ad9":"<a id='2-7'><\/a>\n### _Remove # symbols while retaining the text_","27d49517":"<a id='2-4'><\/a>\n### _Tokenize using tweet tokenizer_","c2bab4ea":"<a id='2-6'><\/a>\n### _Spelling corrections_","690fab87":"<a id='5'><\/a>\n## _Summary_\n\n- Logistic Regression with default paramaters recall = 29%\n- Logistic Regression with class weights in proportion to the data imbalance recall = 75%\n- Logistic Regression fine tuned with grid search and balanced class weights recall = 56%\n- Logistic Regression fine tuned with grid search and class weights in proportion to data imbalance recall = 77%\n","4ff46239":"## Table of Contents\n\n- [1 - Import Libraries and Load Data](#1)\n- [2 - Text Cleaning](#2)\n    - [2.1 - Handle Diacritics using Text Normalization](#2-1)\n    - [2.1 - Remove user handles](#2-2)\n    - [2.2 - Remove the URLs](#2-3)\n    - [2.3 - Tokenize using TweetTokenizer](#2-4)\n    - [2.4 - Remove Stopwords](#2-5)\n    - [2.5 - Spelling Corrections](#2-6)\n    - [2.6 - Remove #symbols while retaining the text](#2-7)\n    - [2.7 - Remove single and double character length tokens ](#2-8)\n    - [2.8 - Remove digits](#2-9)\n    - [2.9 - Remove non alpha numeric characters ](#2-10)\n\n    \n- [3 - Exploratory Data Analysis](#3)\n    - [3.1 - Check for data imbalance](#3-1)\n    - [3.2 - Check top terms in the tweet](#3-2)\n    \n- [ 4 - Predictive Modeling](#4)\n    - [4.1 - Data Formatting for Predidictive Modeling](#4-1)\n    - [4.2 - Using tf-idf vectorizer to generate the feature vectors](#4-2)\n    - [4-3 - Model using Ordinary Logistic Regression with Default Parameters](#4-3)\n    - [4-4 - Model Evaluation](#4-4)\n    - [4-5 - Model using Weighted Logistic Regression to handle data imbalance](#4-5)\n    - [4-6 - Model Fine Tuning using Randomized Grid Search](#4-6)\n    - [4-7 - Fine Tuned Model Prediction & Evaluation with balanced class weights](#4-7)\n    - [4-8 - Fine Tuned Model Prediction & Evaluation with imbalanced class weights](#4-8)\n- [5 - Summary](#5)\n    ","dbd39898":"<a id='2-8'><\/a>\n### _Remove single and double length characters_","e2c57e0d":"<a id='3-2'><\/a>\n### _Check out the top terms in the tweets_","10e78071":"<a id='4-4'><\/a>\n### _Model evaluation_\n\n","5275a2f6":"<a id='2'><\/a>\n## _Text Cleaning_","2d373572":"<a id='2-9'><\/a>\n### _Remove digits_","9bbe99e7":"<a id='2-10'><\/a>\n### _Remove special characters_\n","50c11b69":"<a id='2-5'><\/a>\n### _Remove Stopwords_\n_Append more words to be removed from the text - example rt and amp which occur very frequently_","7b0ed191":"<a id='4-8'><\/a>\n### _Fine tuned model with class weights proportional to the class imbalance_","3c3c471f":"_Accuracy is never a good metric for an imbalanced dataset as in this case. This can be highighted using the f1 score. A low f1-score for a label indicate poor performance of the model._","674fc753":"<a id='4-3'><\/a>\n### _Model building: Ordinary Logistic Regression_","90f6672e":"## _Natural Language Processing_ \n### _Help Twitter Combat Hate Speech Using NLP and Machine Learning_\n***\n<b>DESCRIPTION<\/b>\n\nUsing NLP and ML, make a model to identify hate speech (racist or sexist tweets) in Twitter.\n\n<b>Problem Statement:<\/b>\n***\n\nTwitter is the biggest platform where anybody and everybody can have their views heard. Some of these voices spread hate and negativity. Twitter is wary of its platform being used as a medium  to spread hate. \n\nYou are a data scientist at Twitter, and you will help Twitter in identifying the tweets with hate speech and removing them from the platform. You will use NLP techniques, perform specific cleanup for tweets data, and make a robust model.\n\nDomain: Social Media\n\n<b>Analysis to be done:<\/b> Clean up tweets and build a classification model by using NLP techniques, cleanup specific for tweets data, regularization and hyperparameter tuning using stratified k-fold and cross validation to get the best model.\n\n<b>Content: <\/b>\n\nid: identifier number of the tweet\n\nLabel: 0 (non-hate) \/1 (hate)\n\nTweet: the text in the tweet\n***\n\n<b>Tasks: <\/b>\n\nLoad the tweets file using read_csv function from Pandas package. \n\nGet the tweets into a list for easy text cleanup and manipulation.\n\n<b>To cleanup: <\/b>\n\n- Normalize the casing.\n- Using regular expressions, remove user handles. These begin with '@\u2019.\n- Using regular expressions, remove URLs.\n- Using TweetTokenizer from NLTK, tokenize the tweets into individual terms.\n- Remove stop words.\n- Remove redundant terms like \u2018amp\u2019, \u2018rt\u2019, etc.\n- Remove \u2018#\u2019 symbols from the tweet while retaining the term.\n- Extra cleanup by removing terms with a length of 1.\n\n<b>Check out the top terms in the tweets:<\/b>\n- First, get all the tokenized terms into one large list.\n- Use the counter and find the 10 most common terms.\n\n<b>Data formatting for predictive modeling:<\/b>\n- Join the tokens back to form strings. This will be required for the vectorizers.\n- Assign x and y.\n- Perform train_test_split using sklearn.\n\n<b>We\u2019ll use TF-IDF values for the terms as a feature to get into a vector space model.<\/b>\n- Import TF-IDF  vectorizer from sklearn.\n- Instantiate with a maximum of 5000 terms in your vocabulary.\n- Fit and apply on the train set.\n- Apply on the test set.\n\n<\/b>Model building: Ordinary Logistic Regression<\/b>\n- Instantiate Logistic Regression from sklearn with default parameters.\n- Fit into  the train data.\n- Make predictions for the train and the test set.\n\n<b>Model evaluation: Accuracy, recall, and f_1 score.<\/b>\n- Report the accuracy on the train set.\n- Report the recall on the train set: decent, high, or low.\n- Get the f1 score on the train set.\n\n<b>Looks like you need to adjust the class imbalance, as the model seems to focus on the 0s.<\/b>\n- Adjust the appropriate class in the LogisticRegression model.\n\n<b>Train again with the adjustment and evaluate.<\/b>\n- Train the model on the train set.\n- Evaluate the predictions on the train set: accuracy, recall, and f_1 score.\n\n<b>Regularization and Hyperparameter tuning:<\/b>\n- Import GridSearch and StratifiedKFold because of class imbalance.\n- Provide the parameter grid to choose for \u2018C\u2019 and \u2018penalty\u2019 parameters.\n- Use a balanced class weight while instantiating the logistic regression.\n\n<b>Find the parameters with the best recall in cross-validation.<\/b>\n- Choose \u2018recall\u2019 as the metric for scoring.\n- Choose a stratified 4 fold cross-validation scheme.\n- Fit into  the train set.\n\n<b>What are the best parameters?<\/b>\n\n<b>Predict and evaluate using the best estimator.<\/b>\n- Use the best estimator from the grid search to make predictions on the test set.\n- What is the recall on the test set for the toxic comments?\n- What is the f_1 score?\n","e6891313":"<a id='4-5'><\/a>\n### _Weighted Logistic Regression Or Cost Sensitive Logistic Regression_\n","5b2a7455":"<a id='4-2'><\/a>\n### _Use tf-idf as a feature to get into the vector space model_\n","adea4f28":"_The minority to majority class ratio is 1:13_ ","9c856ff3":"_Love is the most frequently used word followed by day, happy etc. This is expected as there are more non hate tweets than hate tweets in the dataset_","fdb332f3":"<a id='2-3'><\/a>\n### _Remove the urls_","3e3babb2":"_The f1 score of both the training and testing set has improved compared to the plain vanilla Logistic Regression model. There is still more opportunity to improve the score using better models or even handling the data imbalance by adding synthetic data_","9e21c257":"<a id='4-6'><\/a>\n### _Regularization and Hyperparameter tuning:_"}}