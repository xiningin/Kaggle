{"cell_type":{"3846f73f":"code","c6d58f0a":"code","33aae707":"code","d4df633e":"code","7d285dac":"code","8c0fade0":"code","52344a61":"code","8196f159":"code","c69fec9b":"code","49067167":"code","b005b3db":"code","ba538d1f":"code","cadbcbd4":"code","f3def257":"code","ddb1fa51":"code","b0304f96":"code","a658a6e7":"code","3c002bd3":"code","80b21b2c":"code","fa2116e6":"markdown","41314fd6":"markdown","16459ce6":"markdown"},"source":{"3846f73f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport os, time\nimport tensorflow as tf\nimport math\nfrom transformers import TFXLMRobertaModel\nfrom tensorflow.keras.optimizers import Adam\nimport os\nfrom kaggle_datasets import KaggleDatasets\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6d58f0a":"#set TPU coniguration\nAUTO = tf.data.experimental.AUTOTUNE\n\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\nprint(tpu_strategy.num_replicas_in_sync)","33aae707":" MY_GCS_PATH=KaggleDatasets().get_gcs_path('dataencoding')","d4df633e":"data_path = \"..\/input\/dataencoding\/\"\n\n\ntrain_ids = np.load(os.path.join(data_path, \"ids.npy\"))\ntrain_labels = np.load(os.path.join(data_path, \"labels.npy\")).astype(int)\nval_ids = np.load(os.path.join(data_path, \"val_ids.npy\"))\nval_labels = np.load(os.path.join(data_path, \"val_labels.npy\"))","7d285dac":"MODEL = \"jplu\/tf-xlm-roberta-large\"\nSEQUENCE_LENGTH = 192\nBATCH_SIZE =  16 * tpu_strategy.num_replicas_in_sync","8c0fade0":"pos = train_ids[np.where(train_labels == 1)[0]]\nneg = train_ids[np.where(train_labels == 0)[0]]\n\npos_labels = train_labels[np.where(train_labels==1)[0]]\nneg_labels = train_labels[np.where(train_labels==0)[0]]","52344a61":"def make_ds(features, labels):\n    ds = tf.data.Dataset.from_tensor_slices((features, labels))\n    ds = ds.shuffle(1500000).repeat()\n    return ds","8196f159":"del train_ids\ndel train_labels","c69fec9b":"pos_ds = make_ds(pos, pos_labels)\nneg_ds = make_ds(neg, neg_labels)\n\nresampled_ds = tf.data.experimental.sample_from_datasets([pos_ds, neg_ds], weights=[0.5, 0.5])\nresampled_ds = resampled_ds.batch(BATCH_SIZE).prefetch(AUTO)\n","49067167":"del pos\ndel neg","b005b3db":"val_dataset = (tf.data.Dataset.from_tensor_slices((val_ids, val_labels))\n               .shuffle(len(val_ids))\n               .repeat()\n               .batch(BATCH_SIZE)\n               .prefetch(AUTO)\n              )","ba538d1f":"def make_model(embed_model):\n    \n    \n    input_ids = tf.keras.layers.Input(shape=(SEQUENCE_LENGTH,), name='input_token', dtype='int32')\n\n    embed_layer = embed_model([input_ids])[0]\n    avg_pool = tf.reduce_mean(embed_layer, axis=1)\n    max_pool = tf.reduce_max(embed_layer, axis=1)\n    X = tf.concat([avg_pool, max_pool], axis=1)\n    X = tf.keras.layers.Dropout(0.3)(X)\n    X = tf.keras.layers.Dense(1, activation=\"sigmoid\")(X)\n    model = tf.keras.Model(inputs=input_ids, outputs = X)\n    return model","cadbcbd4":"with tpu_strategy.scope():\n    xlm_roberta = TFXLMRobertaModel.from_pretrained(MODEL)\n    xr_model = make_model(xlm_roberta)\n    xr_model.summary()\n    xr_model.compile(optimizer=tf.keras.optimizers.Adam(lr=3e-5), loss=tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1), metrics=[tf.keras.metrics.AUC()])","f3def257":"N_STEPS = 1200000\/\/BATCH_SIZE\nN_STEPS\n\nVAL_STEPS = val_ids.shape[0]\/\/BATCH_SIZE","ddb1fa51":"from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n\ndef build_lrfn(lr_start=0.000001, lr_max=0.000002, \n               lr_min=0.0000001, lr_rampup_epochs=7, \n               lr_sustain_epochs=0, lr_exp_decay=.87):\n    lr_max = lr_max * tpu_strategy.num_replicas_in_sync\n\n    def lrfn(epoch):\n        if epoch < lr_rampup_epochs:\n            lr = (lr_max - lr_start) \/ lr_rampup_epochs * epoch + lr_start\n        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n            lr = lr_max\n        else:\n            lr = (lr_max - lr_min) * lr_exp_decay**(epoch - lr_rampup_epochs - lr_sustain_epochs) + lr_min\n        return lr\n    \n    return lrfn\n\nlrfn = build_lrfn()\n\n\nes = EarlyStopping(monitor='val_accuracy', mode='max', patience=2, \n                   restore_best_weights=True, verbose=1)\nlr_callback = LearningRateScheduler(lrfn, verbose=1)\ncallbacks = [es, lr_callback]","b0304f96":"%%time\nhistory = xr_model.fit(resampled_ds,\n                       validation_data=val_dataset,\n                       epochs=2,\n                       steps_per_epoch=N_STEPS,\n                       validation_steps = VAL_STEPS,\n                       callbacks=callbacks\n                      )","a658a6e7":"val_history = xr_model.fit(val_dataset,\n                       epochs=2,\n                       steps_per_epoch=VAL_STEPS,\n                       callbacks=callbacks\n                      )","3c002bd3":"xr_model.save_weights(\"weights.h5\")","80b21b2c":"sub = pd.read_csv(os.path.join('..\/input\/jigsaw-multilingual-toxic-comment-classification\/','sample_submission.csv'))\ntest_ids = np.load(\"..\/input\/dataencoding\/test_ids.npy\", allow_pickle=True)\nsub['toxic'] = xr_model.predict(test_ids, verbose=1)\nsub.to_csv('submission.csv', index=False)","fa2116e6":"There I will build model upon xlm-roberta using techniques to fight unbalanced classes, as I've shown there:\nhttps:\/\/www.kaggle.com\/vgodie\/class-balancing\n\nUsing preprocessed data from this my notebook\n\nhttps:\/\/www.kaggle.com\/vgodie\/data-encoding","41314fd6":"Making model taking sentence emdedding as concatenation of max and average pooling","16459ce6":"Make resampled dataset to ensure that classes are balanced in training batches"}}