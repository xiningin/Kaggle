{"cell_type":{"c42594a8":"code","5dbb1db0":"code","ecd43d32":"code","8116a433":"code","ddf49c87":"code","15d2a01e":"code","6c3f878e":"code","17b0c72d":"code","4cc5be43":"code","70dd9ee7":"code","85c20115":"code","7cf24635":"code","d56ff858":"code","b2d32262":"code","62d60ec0":"code","0d91db96":"code","a5653ae5":"code","2caa830f":"code","dd7636a0":"code","c4b64eec":"code","e302dc28":"code","f76b57bc":"code","0e34c2b6":"code","495741cf":"code","1cf3fee6":"code","68bed580":"code","cbba4426":"code","6fe8c516":"code","51fb6c4b":"code","194e1b78":"code","b6ffef69":"code","8307afbd":"markdown","cff7e213":"markdown","427fc985":"markdown","1e60e13a":"markdown","9dac92e8":"markdown","ac2c4223":"markdown","9b7e640a":"markdown","c0ca435d":"markdown","8807c14a":"markdown","4a54ba78":"markdown","a1c192c6":"markdown","5ed1c643":"markdown","08972b6c":"markdown","2ba3d767":"markdown","50653fe2":"markdown"},"source":{"c42594a8":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\n\nplt.rcParams['figure.figsize'] = (16, 8)\nplt.style.use('fivethirtyeight')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction import DictVectorizer\nfrom sklearn.tree import DecisionTreeRegressor, export_text, plot_tree\nfrom sklearn.ensemble import RandomForestRegressor\n\nfrom sklearn import metrics\nimport xgboost as xgb","5dbb1db0":"columns = [\n    'neighbourhood_group', 'room_type', 'latitude', 'longitude',\n    'minimum_nights', 'number_of_reviews','reviews_per_month',\n    'calculated_host_listings_count', 'availability_365',\n    'price'\n]\n\ndf = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv', usecols=columns)\ndf.reviews_per_month = df.reviews_per_month.fillna(0)","ecd43d32":"\"\"\"\nLook at the price variable. Does it have a long tail?\n\"\"\"\nfig, axes = plt.subplots(1, 2)\n      \naxes[0].hist(df['price'], density= True, bins = 50)\naxes[0].set_title(\"Normal scale Price\")\n\naxes[1].hist(np.log1p(df['price']), density= True, bins = 50)\naxes[1].set_title(\"Logarithmic scale Price\");","8116a433":"\ndf_full_train, df_test = train_test_split(df, test_size=0.2, random_state=1)\ndf_train, df_val = train_test_split(df_full_train, test_size=0.25, random_state=1)\nlen(df_train), len(df_val), len(df_test)\n\ndf_train = df_train.reset_index(drop=True)\ndf_val = df_val.reset_index(drop=True)\ndf_test = df_test.reset_index(drop=True)","ddf49c87":"\"\"\"Apply the log transformation to the price variable using the np.log1p() function.\"\"\"\ny_train = np.log1p(df_train['price'].values)\ny_val = np.log1p(df_val['price'].values)\ny_test = np.log1p(df_test['price'].values)\n\n\"\"\"Make sure that the target value ('price') is not in your dataframe.\"\"\"\ndel df_train['price']\ndel df_val['price']\ndel df_test['price']","15d2a01e":"\"\"\"Now, use DictVectorizer to turn train and validation into matrices:\"\"\"\ndv = DictVectorizer(sparse=False)\n\ntrain_dict = df_train.to_dict(orient='records')\nval_dict = df_val.to_dict(orient='records')\n\nX_train = dv.fit_transform(train_dict)\nX_val = dv.transform(val_dict)\n\ndv_features = dv.get_feature_names()","6c3f878e":"\ndt_r = DecisionTreeRegressor(max_depth=1)\ndt_r.fit(X_train, y_train)\n","17b0c72d":"\nprint(export_text(dt_r, feature_names=dv_features))\nprint('\\n')\nplot_tree(dt_r , filled = True , feature_names = dv_features);","4cc5be43":"rf_r = RandomForestRegressor(n_estimators=10, random_state=1, n_jobs=-1)\nrf_r.fit(X_train, y_train)","70dd9ee7":"y_pred = rf_r.predict(X_val)\nrmse_val = metrics.mean_squared_error(y_val, y_pred, squared = False)\nrmse_val","85c20115":"preds = []\nrmse = {}\nfor est in range(10,201,10):\n    rf = RandomForestRegressor(n_estimators=est, random_state=1)\n    rf.fit(X_train, y_train)\n    y_pred = rf.predict(X_val)\n    preds.append(y_pred)\n    rmse_val = metrics.mean_squared_error(y_val, y_pred, squared = False)\n    rmse[est] = rmse_val\n    print(f\"n_estimators = {est} RMSE: {rmse[est]}\")","7cf24635":"\nplt.plot(rmse.keys(), rmse.values())\nplt.xlabel('n_estimators')\nplt.ylabel('RMSE')\nplt.xticks(list(rmse.keys()))\nplt.title('RandomForest RMSE over various n_estimators');","d56ff858":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10,201,10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                    max_depth=d,\n                                    random_state=1)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict(X_val)\n        rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n        print(f\"max_depth = {d}, n_estimators = {n}, RMSE: {rmse}\")\n        scores.append((d, n, rmse))","b2d32262":"columns = ['max_depth', 'n_estimators', 'rmse']\ndf_scores = pd.DataFrame(scores, columns=columns)","62d60ec0":"for d in [10, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend()","0d91db96":"print(f\"Best value for max_depth: {int(df_scores.max_depth[df_scores.rmse == min(df_scores.rmse)].values)}\")","a5653ae5":"scores = []\n\nfor d in [10, 15, 20, 25]:\n    for n in range(10,201,10):\n        rf = RandomForestRegressor(n_estimators=n,\n                                    max_depth=d,\n                                    random_state=7)\n        rf.fit(X_train, y_train)\n\n        y_pred = rf.predict(X_val)\n        rmse = metrics.mean_squared_error(y_val, y_pred, squared = False)\n        print(f\"max_depth = {d}, n_estimators = {n}, RMSE: {rmse}\")\n        scores.append((d, n, rmse))\n        \ncolumns = ['max_depth', 'n_estimators', 'rmse']\ndf_scores = pd.DataFrame(scores, columns=columns) \n\n\nfor d in [10, 15, 20, 25]:\n    df_subset = df_scores[df_scores.max_depth == d]\n    \n    plt.plot(df_subset.n_estimators, df_subset.rmse,\n             label='max_depth=%d' % d)\n\nplt.legend()","2caa830f":"rf = RandomForestRegressor(n_estimators=10,\n    max_depth=20,\n    random_state=1,\n    n_jobs=-1)\nrf.fit(X_train, y_train)\nfeature_importance = rf.feature_importances_","dd7636a0":"list_feat_imp = [(feat,imp) for feat, imp in zip(dv_features, feature_importance)]\ncolumns = ['features', 'feature_importance']\ndf_feat_importance = pd.DataFrame(list_feat_imp, columns=columns) \ndf_feat_importance.sort_values(by='feature_importance', ascending=False, inplace=True)\n\n\nplt.figure(figsize= (16,10))\nax= sns.barplot( df_feat_importance['feature_importance'], df_feat_importance['features'],palette='copper' )\nplt.title(\"Feature Importance\",fontsize = 20)\nplt.xticks(fontsize = 15)\nplt.show();","c4b64eec":"def parse_xgb_output(output):\n    results = []\n\n    for line in output.stdout.strip().split('\\n'):\n        it_line, train_line, val_line = line.split('\\t')\n\n        it = int(it_line.strip('[]'))\n        train = float(train_line.split(':')[1])\n        val = float(val_line.split(':')[1])\n\n        results.append((it, train, val))\n    \n    columns = ['num_iter', 'train_rmse', 'val_rmse']\n    df_results = pd.DataFrame(results, columns=columns)\n    return df_results","e302dc28":"dtrain = xgb.DMatrix(X_train, label=y_train, feature_names=dv_features)\ndval = xgb.DMatrix(X_val, label=y_val, feature_names=dv_features)\nwatchlist = [(dtrain, 'train'), (dval, 'val')]\n\nxgb_params = {\n    'eta':0.3,\n    'max_depth': 6,\n    'min_child_weight': 1,\n    'objective': 'reg:squarederror',\n    'nthread': 8,\n    'seed': 1,\n    'verbosity': 1,\n}\n\nscores = dict()","f76b57bc":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","0e34c2b6":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","495741cf":"xgb_params[\"eta\"] = 0.1","1cf3fee6":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","68bed580":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","cbba4426":"xgb_params[\"eta\"] = 0.01","6fe8c516":"%%capture output\nmodel = xgb.train(xgb_params, dtrain, num_boost_round=100,\n                  verbose_eval=5,\n                  evals=watchlist)","51fb6c4b":"key = f'eta={xgb_params[\"eta\"]}'\nscores[key] = parse_xgb_output(output)","194e1b78":"for eta, df_score in scores.items():\n    plt.plot(df_score.num_iter, df_score.val_rmse, label=eta)\n\n    plt.xlabel('n_rounds')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.title('Full graph')\n\nplt.suptitle(f'XGBoost [eta] hypertuning');","b6ffef69":"for eta, df_score in scores.items():\n    plt.plot(df_score.num_iter, df_score.val_rmse, label=eta)\n\n    plt.xlabel('n_rounds')\n    plt.ylabel('RMSE')\n    plt.legend()\n    plt.title('Full graph')\n    plt.ylim(0.43,0.45)\n\nplt.suptitle(f'XGBoost [eta] hypertuning');","8307afbd":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-6: <b>0.1<\/b><\/div>","cff7e213":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-5: <b>room_type=Entire home\/apt<\/b><\/div>","427fc985":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 6:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's train an XGBoost model! For this question, we'll tune the eta parameter\n\nInstall XGBoost\n    \nCreate DMatrix for train and validation\n    \nCreate a watchlist\n    \nTrain a model with these parameters for 100 rounds:\n\nxgb_params = {'eta': 0.3, \n'max_depth': 6,\n'min_child_weight': 1,\n\n'objective': 'reg:squarederror',\n'nthread': 8,\n\n'seed': 1,\n'verbosity': 1,\n}\n    \nNow change eta first to 0.1 and then to 0.01\n\nWhich eta leads to the best RMSE score on the validation dataset?\n<\/ul>\n<\/div>","1e60e13a":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-4: <b>15<\/b><\/div>","9dac92e8":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-2: <b>0.459<\/b><\/div>","ac2c4223":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-Bonus: <b>No change still 15<\/b><\/div>","9b7e640a":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 3:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's experiment with the n_estimators parameter\n    \nTry different values of this parameter from 10 to 200 with step 10\n\nSet random_state to 1\n    \nEvaluate the model on the validation dataset\n    \nAfter which value of n_estimators does RMSE stop improving?\n<\/ul>\n<\/div>","c0ca435d":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 4:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nLet's select the best max_depth:\n\nTry different values of max_depth: [10, 15, 20, 25]\n    \nFor each of these values, try different values of n_estimators from 10 till 200 (with step 10)\n    \nFix the random seed: random_state=1\n    \n    \nWhat's the best max_depth?\n<\/ul>\n<\/div>\n","8807c14a":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-1: <b>room_type<\/b><\/div>","4a54ba78":"<div style=\"background:#2b6684;color:white; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:30px;color:white\"><u>This Week Questions<\/u>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 1:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nLet's train a decision tree regressor to predict the price variable.\n\nTrain a model with max_depth=1\n<\/ul>   \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 2:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nTrain a random forest model with these parameters:n_estimators=10 random_state=1 n_jobs=-1 (optional - to make training faster)\n    \nWhat's the RMSE of this model on validation?\n<\/ul>     \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 3:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's experiment with the n_estimators parameter\n    \nTry different values of this parameter from 10 to 200 with step 10\n\nSet random_state to 1\n    \nEvaluate the model on the validation dataset\n    \nAfter which value of n_estimators does RMSE stop improving?\n<\/ul> \n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 4:<\/u><\/p>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nLet's select the best max_depth:\n\nTry different values of max_depth: [10, 15, 20, 25]\n    \nFor each of these values, try different values of n_estimators from 10 till 200 (with step 10)\n    \nFix the random seed: random_state=1\n    \n\nAfter which value of n_estimators does RMSE stop improving?\n<\/ul>\n    \n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Bonus Question: Not Graded<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nWill the answer be different if we change the seed for the model?\n    \n<\/ul>   \n    \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 5:<\/u><\/p>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nWe can extract feature importance information from tree-based models.\n\nAt each step of the decision tree learning algorith, it finds the best split. When doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. This gain is quite useful in understanding what are the imporatant features for tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the feature_importances_ field.\n\nFor this homework question, we'll find the most important feature:\n\nTrain the model with these parametes:\nn_estimators=10,\nmax_depth=20,\nrandom_state=1,\nn_jobs=-1 (optional)\n    \nGet the feature importance information from this model\n    \n\nWhat's the most important feature?\n<\/ul>\n      \n\n<p style=\"font-family:cursive;font-size:15px;color:  yellow\"><u>Question 6:<\/u><\/p>\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nNow let's train an XGBoost model! For this question, we'll tune the eta parameter\n\nInstall XGBoost\n    \nCreate DMatrix for train and validation\n    \nCreate a watchlist\n    \nTrain a model with these parameters for 100 rounds:\n\nxgb_params = {'eta': 0.3, \n'max_depth': 6,\n'min_child_weight': 1,\n\n'objective': 'reg:squarederror',\n'nthread': 8,\n\n'seed': 1,\n'verbosity': 1,\n}\n    \nNow change eta first to 0.1 and then to 0.01\n    \n\nWhich eta leads to the best RMSE score on the validation dataset?    \n<\/ul>\n\n<\/div>\n","a1c192c6":"<div style=\"background:#efddc2;color:#2b6684; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\">Answer-3: <b>120<\/b><\/div>","5ed1c643":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Bonus question (not graded):<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nWill the answer be different if we change the seed for the model?\n    \n<\/ul>\n<\/div>\n","08972b6c":"<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 2:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nTrain a random forest model with these parameters:n_estimators=10 random_state=1 n_jobs=-1 (optional - to make training faster)\n    \nWhat's the RMSE of this model on validation?\n<\/ul>\n<\/div>","2ba3d767":"\n<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 1:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\n\nLet's train a decision tree regressor to predict the price variable.\n\nTrain a model with max_depth=1\n\n\n<\/ul>\n<\/div>","50653fe2":"\n<div style=\"background:#2b6684;color:yellow; font-family:'Goudy Old Style';padding:0.5em;border-radius:0.2em;font-size:25px\"><b>Question 5:<\/b>\n\n<ul style=\"font-family:cursive;font-size:15px;color:  white\">\nWe can extract feature importance information from tree-based models.\n\nAt each step of the decision tree learning algorith, it finds the best split. When doint it, we can calculate \"gain\" - the reduction in impurity before and after the split. This gain is quite useful in understanding what are the imporatant features for tree-based models.\n\nIn Scikit-Learn, tree-based models contain this information in the feature_importances_ field.\n\nFor this homework question, we'll find the most important feature:\n\nTrain the model with these parametes:\nn_estimators=10,\nmax_depth=20,\nrandom_state=1,\nn_jobs=-1 (optional)\nGet the feature importance information from this model\n    \n    \nWhat's the most important feature?\n<\/ul>\n<\/div>"}}