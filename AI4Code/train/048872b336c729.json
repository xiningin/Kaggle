{"cell_type":{"f9e3a8bb":"code","b5f29ec0":"code","5b58d3b5":"code","96f94f8e":"code","c4c3050d":"code","c1d951a5":"code","dad7f966":"code","4cf47c22":"code","219e044b":"code","2dbafc9f":"code","bc4b00ed":"code","36588639":"code","d5d623ad":"code","b6a59d29":"code","43fa10a2":"code","c348a360":"code","f1d5cdee":"code","e22691c7":"code","7c65b1e1":"code","5b081d20":"code","7dca729b":"code","560f0349":"code","719fd500":"code","fb7b3b74":"code","3bee8053":"code","d06b89ff":"code","9be0d976":"code","5616d028":"code","fc20abc4":"code","7c711bf0":"code","b7756d38":"code","e63bc67b":"code","f2e88c2e":"code","030f76e6":"code","71c16f04":"code","acbb4d7b":"code","ef0749a4":"code","9127b7ca":"code","b1a14d6f":"code","d381c5d2":"code","6e93ba9b":"code","cc4ff6e1":"code","71b13656":"code","e6e098cf":"code","31fa88f3":"code","0c9deee5":"code","e7831b14":"code","e8e9836d":"code","d0e15d26":"code","f4547f19":"code","a5afdc6a":"code","c3bb3d36":"code","cc638201":"code","f8fa7b15":"code","102d9451":"code","b8dca436":"code","06e70dc9":"markdown","67df0bd6":"markdown","cfaabbcc":"markdown","b7a80fbb":"markdown","8f0b12e5":"markdown","4c7c3694":"markdown","c8b4de29":"markdown","6305f0c7":"markdown","e62be3f9":"markdown"},"source":{"f9e3a8bb":"import numpy as np\nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt \n\nimport tensorflow as tf \nimport seaborn as sns ","b5f29ec0":"# import MNIST dataset from keras  \nfrom tensorflow.keras.datasets import mnist\n\n(x_train,y_train), (x_test,y_test) = mnist.load_data()\n","5b58d3b5":"# plot image\nplt.imshow(x_train[1])","96f94f8e":"# Normalization Data\nx_train = x_train \/ 255.0\nx_test = x_test \/ 255.0","c4c3050d":"# Create Logistic Model\nnum_class = 10\nmodel = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n                                 tf.keras.layers.Dense(num_class, activation=tf.nn.softmax)])\n\nmodel.compile(optimizer=\"sgd\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])","c1d951a5":"history = model.fit(x_train,y_train, epochs=10)","dad7f966":"# Plot training accuracy values\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","4cf47c22":"# Plot training loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","219e044b":"model.evaluate(x_test,y_test)","2dbafc9f":"#Calculate metrics \n\npredictions = model.predict(x_test)\nprediction = [np.argmax(p) for p in predictions]\nprediction = np.asarray(prediction)\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n\nprint(classification_report(y_test,prediction))\n\n\n# Plot Confusion Matrix for Logistic Regression\nconfusion_mtx = confusion_matrix(y_test, prediction)\n\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor='gray', fmt='.1f', ax=ax)\nplt.show()","bc4b00ed":"#import MNIST dataset\nfrom tensorflow.keras.datasets import mnist \n(x_train,y_train), (x_test,y_test) = mnist.load_data()\n\nplt.imshow(x_train[5])","36588639":"#Reshape Dataset from 28*28 -> 784\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples, nx*ny))\nprint(x_train.shape)\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples, nx*ny))\nprint(x_test.shape)\n","d5d623ad":"# Standartize Data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()               #Normal Gauss Distribution\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","b6a59d29":"# Crate PCA Model \nfrom sklearn.decomposition import PCA \n\npca = PCA(.90)  # .90 represents decomposition ratio\npca_dimentional_data = pca.fit_transform(x_train)\nprint(pca.explained_variance_)\nprint(\"n_components : \", pca_dimentional_data.shape[1])\n","43fa10a2":"# Inverse Transfrom from 64 -> 784\n\napprx = pca.inverse_transform(pca_dimentional_data)\nplt.imshow(apprx[5].reshape(28,28))","c348a360":"from sklearn.decomposition import PCA \n\npca = PCA(n_components=50)   # Max Componenet values = 728\npca_dimentional_data = pca.fit_transform(x_train)\nprint(pca.explained_variance_)\nprint(pca_dimentional_data.shape, type(pca_dimentional_data))\n","f1d5cdee":"# Inverse Transfrom from 64 -> 784\n\napprx = pca.inverse_transform(pca_dimentional_data)\nplt.imshow(apprx[5].reshape(28,28))","e22691c7":"from tensorflow.keras.datasets import mnist \n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train_pca = apprx #pca dataset \n","7c65b1e1":"#Reshape Dataset from 28*28 -> 784\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples, nx*ny))\nprint(x_train.shape)\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples, nx*ny))\nprint(x_test.shape)\n\n# Standartize Data \nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()               #Normal Gauss Distribution\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\n\n\nnsamples, nxy = x_train_pca.shape\nx_train_pca = x_train_pca.reshape((nsamples, int(nxy**(1\/2)), int(nxy**(1\/2))))\nnsamples, nxy = x_test.shape\nx_test = x_test.reshape((nsamples, int(nxy**(1\/2)), int(nxy**(1\/2))))\n","5b081d20":"\nimport tensorflow as tf \n\nnum_class = 10\nmodel_with_pca = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n                                 tf.keras.layers.Dense(num_class, activation=tf.nn.softmax)])\n\nmodel_with_pca.compile(optimizer=\"sgd\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])","7dca729b":"%%time\nhistory = model_with_pca.fit(x_train_pca, y_train, epochs=10)","560f0349":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","719fd500":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","fb7b3b74":"model_with_pca.evaluate(x_test,y_test)","3bee8053":"#Calculate metrics \n\npredictions = model_with_pca.predict(x_test)\nprediction = [np.argmax(p) for p in predictions]\nprediction = np.asarray(prediction)\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n\nprint(classification_report(y_test,prediction))\n\nconfusion_mtx = confusion_matrix(y_test, prediction)\n\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor='gray', fmt='.1f', ax=ax)\nplt.show()","d06b89ff":"from tensorflow.keras.datasets import mnist \n(x_train, y_train), (x_test,y_test) = mnist.load_data()\nplt.imshow(x_train[1])","9be0d976":"#Reshape Dataset from 28*28 -> 784\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples, nx*ny))\nprint(x_train.shape)\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples, nx*ny))\nprint(x_test.shape)","5616d028":"# Standartize Data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()               #Normal Gauss Distribution\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","fc20abc4":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n\nlda = LinearDiscriminantAnalysis()\n\nresult_lda = lda.fit(x_train,y_train).transform(x_train)","7c711bf0":"from sklearn.utils.validation import check_is_fitted\nfrom sklearn.utils import check_array, check_X_y\n\ndef inverse_transform(lda, x):\n    if lda.solver == 'lsqr':\n        raise NotImplementedError(\"(inverse) transform not implemented for 'lsqr' \"\n                                  \"solver (use 'svd' or 'eigen').\")\n    check_is_fitted(lda, ['xbar_', 'scalings_'], all_or_any=any)\n    \n    inv = np.linalg.pinv(lda.scalings_)\n   \n    print(inv.shape)\n    x = check_array(x)\n    print(x.shape)\n    if lda.solver == 'svd':\n        x_back = np.dot(x, inv) + lda.xbar_\n    elif lda.solver == 'eigen':\n        x_back = np.dot(x, inv)\n\n    return x_back\n\napprx_lda = inverse_transform(lda,result_lda)\nplt.imshow(apprx_lda[5].reshape(28,28))","b7756d38":"from tensorflow.keras.datasets import mnist\nfrom sklearn.preprocessing import StandardScaler\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\nx_train_lda = apprx_lda\n","e63bc67b":"#28*28 -> 784\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples,nx*ny))\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples,nx*ny))\n\n# Data Standart\nscaler = StandardScaler()          # Mean = 0, Varience = 1 actualy is Normal Gauss Distrubution \nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)\n\n#784 -> 28*28\nnsamples, nxy = x_train_lda.shape\nx_train_lda = x_train_lda.reshape((nsamples,int(nxy**(1\/2)),int(nxy**(1\/2))))\nnsamples,nxy = x_test.shape\nx_test = x_test.reshape((nsamples,int(nxy**(1\/2)),int(nxy**(1\/2))))\nprint(x_test.shape)","f2e88c2e":"import tensorflow as tf \nmodel_with_lda = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n                                    tf.keras.layers.Dense(10, activation=tf.nn.softmax)])\n\nmodel_with_lda.compile(optimizer='sgd',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","030f76e6":"%%time\nhistory = model_with_lda.fit(x_train_lda,y_train, epochs=10)","71c16f04":"# Plot training & validation accuracy values\nplt.plot(history.history['accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","acbb4d7b":"# Plot training & validation loss values\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","ef0749a4":"model_with_lda.evaluate(x_test,y_test)","9127b7ca":"#Calculate metrics \n\npredictions = model_with_lda.predict(x_test)\nprediction = [np.argmax(p) for p in predictions]\nprediction = np.asarray(prediction)\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n\nprint(classification_report(y_test,prediction))\n\nconfusion_mtx = confusion_matrix(y_test, prediction)\n\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor='gray', fmt='.1f', ax=ax)\nplt.show()","b1a14d6f":"#import dataset\nfrom tensorflow.keras.datasets import mnist \n(x_train,y_train), (x_test,y_test) = mnist.load_data()\n\nplt.imshow(x_train[1])","d381c5d2":"#Reshape Dataset from 28*28 -> 784\nnsamples, nx, ny = x_train.shape\nx_train = x_train.reshape((nsamples, nx*ny))\nprint(x_train.shape)\n\nnsamples, nx, ny = x_test.shape\nx_test = x_test.reshape((nsamples, nx*ny))\nprint(x_test.shape)\n","6e93ba9b":"# Standartize Data \nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()               #Normal Gauss Distribution\nscaler.fit(x_train)\nx_train = scaler.transform(x_train)\nx_test = scaler.transform(x_test)","cc4ff6e1":"# Crate PCA Model \n\"\"\"\n\nfrom sklearn.decomposition import KernelPCA \n\nk_pca = KernelPCA(.20)\nk_pca_dimentional_data = k_pca.fit_transform(x_train)\nprint(k_pca_dimentional_data.shape, type(k_pca_dimentional_data))\n\"\"\"","71b13656":"\"\"\"\n# Inverse Transfrom from 64 -> 784\napprx = k_pca.inverse_transform(k_pca_dimentional_data)\nplt.imshow(apprx[1].reshape(28,28))\n\"\"\"","e6e098cf":"\"\"\"\nx_train_pca = apprx #kernel pca dataset ","31fa88f3":"\"\"\"\"\n# 784 -> 28*28\nnsamples, nxy = x_train_pca.shape\nx_train_pca = x_train_pca.reshape((nsamples, int(nxy**(1\/2)), int(nxy**(1\/2))))\nnsamples, nxy = x_test.shape\nx_test = x_test.reshape((nsamples, int(nxy**(1\/2)), int(nxy**(1\/2))))","0c9deee5":"\"\"\"\"\nimport tensorflow as tf \n\nnum_class = 10\nmodel_with_k_pca = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n                                 tf.keras.layers.Dense(num_class, activation=tf.nn.softmax)])\n\nmodel_with_k_pca.compile(optimizer=\"sgd\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])","e7831b14":"\n\"\"\"\"history = model_with_k_pca.fit(x_train_pca, y_train, epochs=10)","e8e9836d":"# Plot training & validation accuracy values\n\"\"\"\"\nplt.plot(history.history['acc'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","d0e15d26":"# Plot training & validation loss values\n\"\"\"\"\nplt.plot(history.history['loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()","f4547f19":"\"\"\"\"\nmodel_with_k_pca.evaluate(x_test,y_test)","a5afdc6a":"#Calculate metrics \n\"\"\"\"\npredictions = model_with_k_pca.predict(x_test)\nprediction = [np.argmax(p) for p in predictions]\nprediction = np.asarray(prediction)\n\nfrom sklearn.metrics import precision_recall_fscore_support, confusion_matrix, classification_report\n\nprint(classification_report(y_test,prediction))\n\nconfusion_mtx = confusion_matrix(y_test, prediction)\n\nf,ax = plt.subplots(figsize=(10,10))\nsns.heatmap(confusion_mtx, annot=True, linewidths=0.01, cmap=\"Greens\", linecolor='gray', fmt='.1f', ax=ax)\nplt.show()","c3bb3d36":"from numpy.linalg import inv,pinv\nimport numpy as np\nfrom numpy.linalg import eig\nimport numpy as np\nimport tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import axes3d\n\nclass DataSet:\n  def __init__(self, data, targets, valid_classes=None):\n    if valid_classes is None:\n      self.valid_classes = np.unique(targets)\n    else:\n      self.valid_classes = valid_classes\n    #print(self.valid_classes)\n    self.number_of_classes = len(self.valid_classes)\n    self.data = self.to_dict(data, targets)\n\n  def to_dict(self, data, targets):\n    data_dict = {}\n    for x, y in zip(data, targets):\n      if y in self.valid_classes:\n        if y not in data_dict:\n          data_dict[y] = [x.flatten()]\n        else:\n          data_dict[y].append(x.flatten())\n\n    for i in self.valid_classes:\n      data_dict[i] = np.asarray(data_dict[i])\n\n    return data_dict\n\n  def get_data_by_class(self, class_id):\n    if class_id in self.valid_classes:\n      return self.data[class_id]\n    else:\n      raise (\"Class not found.\")\n\n  def get_data_as_dict(self):\n    return self.data\n\n  def get_all_data(self):\n    data = []\n    labels = []\n    for label, class_i_data in self.data.items():\n      data.extend(class_i_data)\n      labels.extend(class_i_data.shape[0] * [label])\n    data = np.asarray(data)\n    labels = np.asarray(labels)\n    return data, labels\n\nclass FDAClassifier:\n  \n  def __init__(self, projection_dim):\n    self.projection_dim = projection_dim\n    self.W = None # weights\n    self.g_means, self.g_covariance, self.priors = None, None, None\n\n  def fit(self,X):\n    means_k = self.__compute_means(X)\n\n    Sks = []\n    for class_i, m in means_k.items():\n      sub = np.subtract(X[class_i], m)\n      Sks.append(np.dot(np.transpose(sub), sub))\n\n    Sks = np.asarray(Sks)\n    Sw = np.sum(Sks, axis=0) # shape = (D,D)\n\n    Nk = {}\n    sum_ = 0\n    for class_id, data in X.items():\n      Nk[class_id] = data.shape[0]\n      sum_ += np.sum(data, axis=0)\n\n    self.N = sum(list(Nk.values()))\n\n    # m is the mean of the total data set\n    m = sum_ \/ self.N\n\n    SB = []\n    for class_id, mean_class_i in means_k.items():\n      sub_ = mean_class_i - m\n      SB.append(np.multiply(Nk[class_id], np.outer(sub_, sub_.T)))\n    \n    # between class covariance matrix shape = (D,D). D = input vector dimensions\n    SB = np.sum(SB, axis=0)  # sum of K (# of classes) matrices\n    \n    matrix = np.dot(pinv(Sw), SB)\n    \n    # find eigen values and eigen-vectors pairs for np.dot(pinv(SW),SB)\n    eigen_values, eigen_vectors = eig(matrix)\n    #print(\"eigen_values:\", eigen_values.shape)\n    #print(\"eigen_vectors:\", eigen_vectors.shape)\n\n    eiglist = [(eigen_values[i], eigen_vectors[:, i]) for i in range(len(eigen_values))]\n\n    # sort the eigvals in decreasing order\n    eiglist = sorted(eiglist, key=lambda x: x[0], reverse=True)\n\n    # take the first num_dims eigvectors\n    self.W = np.array([eiglist[i][1] for i in range(self.projection_dim)])\n    self.W = np.asarray(self.W).T\n\n    # get parameter of the Gaussian distribution\n    self.g_means, self.g_covariance, self.priors = self.gaussian(X)\n    return \n\n  # Returns the parameters of the Gaussian distributions\n  def gaussian(self, X):\n    means = {}\n    covariance = {}\n    priors = {}  # p(Ck)\n    for class_id, values in X.items():\n      proj = np.dot(values, self.W)\n      means[class_id] = np.mean(proj, axis=0)\n      covariance[class_id] = np.cov(proj, rowvar=False)\n      priors[class_id] = values.shape[0] \/ self.N\n    return means, covariance, priors\n  \n  # model a multi-variate Gaussian distribution for each class\u2019 likelihood distribution P(x|Ck)\n  def gaussian_distribution(self, x, u, cov):\n    scalar = (1. \/ ((2 * np.pi) ** (x.shape[0] \/ 2.))) * (1 \/ np.sqrt(np.linalg.det(cov)))\n    x_sub_u = np.subtract(x, u)\n    return scalar * np.exp(-np.dot(np.dot(x_sub_u, inv(cov)), x_sub_u.T) \/ 2.)\n\n  def score(self,X,y):\n    proj = self.project(X)\n    gaussian_likelihoods = []\n    classes = sorted(list(self.g_means.keys()))\n    for x in proj:\n      row = []\n      for c in classes:  # number of classes\n        res = self.priors[c] * self.gaussian_distribution(x, self.g_means[c], self.g_covariance[c])  # Compute the posterios P(Ck|x) prob of a class k given a point x\n        row.append(res)\n\n      gaussian_likelihoods.append(row)\n\n    gaussian_likelihoods = np.asarray(gaussian_likelihoods)\n    \n    # assign x to the class with the largest posterior probability\n    predictions = np.argmax(gaussian_likelihoods, axis=1)\n    return gaussian_likelihoods, np.sum(predictions == y) \/ len(y), predictions, proj\n\n  def project(self,X):\n    return np.dot(X, self.W)\n\n  def __compute_means(self, X):\n    # Compute the means for each class k=1,2,3...K\n    # If the dataset has K classes, then, self.means_k.shape = [# of records, K]\n    means_k = {}\n    for class_i, input_vectors in X.items():\n      means_k[class_i] = np.mean(input_vectors, axis=0)\n    return means_k","cc638201":"from tensorflow.keras.datasets import mnist\n\n(x_train, y_train),(x_test, y_test) = mnist.load_data()\nx_train, x_test = x_train \/ 255.0, x_test \/ 255.0\n\ntrain_dataset = DataSet(x_train, y_train)","f8fa7b15":"inputs, targets = train_dataset.get_all_data()\nplt.imshow(inputs[20000].reshape(28,28))","102d9451":"fish = FDAClassifier(projection_dim=9)\nfish.fit(train_dataset.get_data_as_dict())\n","b8dca436":"gauslk, acc, predictions, proj = fish.score(inputs,targets)\nprint(\"Acc:\", acc)\nprint(\"Output shape :\", proj.shape)","06e70dc9":"## PCA","67df0bd6":"## LDA ","cfaabbcc":"## Fisher Discriminant Analysis","b7a80fbb":"## PCA with Logistic Regression","8f0b12e5":"#### Define Lib. ","4c7c3694":"## PCA and LDA with Logistic Regression on MNIST\n### This kernel shows how to apply pca, lda, and inverse transform, fisher, kernel pca with logistic regression on the MNIST","c8b4de29":"## LDA + Logistic Regression","6305f0c7":"## 1-) Logistic Regression ","e62be3f9":"## Kernel PCA !!! Memmory Error (Use Small Dataset) "}}