{"cell_type":{"af7c680b":"code","3d707694":"code","1083f77a":"code","dc8e2dd5":"code","cbaa09b0":"code","8cb60c48":"code","e71f0b54":"code","45d40530":"code","fb598e4f":"code","a9bc4d7c":"code","c40784fd":"code","13eaf767":"code","2991d6d4":"code","1aa1ecce":"code","0fd5e0b7":"code","7169a1b0":"code","e34efe8b":"code","7065c8d8":"code","f3075406":"code","f20ea96b":"code","e5687a71":"code","e2a1259f":"code","49a05257":"markdown","4fe3e1fb":"markdown","b6b3a164":"markdown","1fc3716a":"markdown","d82405ba":"markdown","4520fcaa":"markdown","46d06b7e":"markdown","8ef2e8eb":"markdown","11906da4":"markdown","1b3e7f14":"markdown","679786ad":"markdown","3af84052":"markdown"},"source":{"af7c680b":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom keras import layers, Input, Model\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","3d707694":"data_dir = '..\/input\/'\n!ls {data_dir}","1083f77a":"train_raw = pd.read_csv(f'{data_dir}train.csv')\ntrain_raw.head()","dc8e2dd5":"test_raw = pd.read_csv(f'{data_dir}test.csv')\ntest_raw.head()","cbaa09b0":"train_raw.shape, test_raw.shape","8cb60c48":"train_raw.isnull().sum().sum(), test_raw.isnull().sum().sum()","e71f0b54":"sns.countplot(train_raw.target)\nplt.show()","45d40530":"train_raw.target.value_counts()","fb598e4f":"trn_x, valid_x, trn_y, valid_y = train_test_split(train_raw.drop(['id', 'target'], axis=1), train_raw.target, random_state=33, test_size=0.15)\ntrn_x.shape, valid_x.shape, trn_y.shape, valid_y.shape","a9bc4d7c":"trn_wheezy = pd.get_dummies(trn_x['wheezy-copper-turtle-magic'])\nvalid_wheezy = pd.get_dummies(valid_x['wheezy-copper-turtle-magic'])\ntest_wheezy = pd.get_dummies(test_raw['wheezy-copper-turtle-magic'])\n\ntrn_wheezy.shape, valid_wheezy.shape, test_wheezy.shape","c40784fd":"trn_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\nvalid_x.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)\ntest_raw.drop('wheezy-copper-turtle-magic', axis=1, inplace=True)","13eaf767":"sc = StandardScaler()\ntrn_x = sc.fit_transform(trn_x)\n\nvalid_x = sc.transform(valid_x)\ntest_x = sc.transform(test_raw.drop('id', axis=1))","2991d6d4":"trn_x = np.concatenate([trn_x, trn_wheezy.values], axis=1)\nvalid_x = np.concatenate([valid_x, valid_wheezy.values], axis=1)\ntest_x = np.concatenate([test_x, test_wheezy.values], axis=1)","1aa1ecce":"def build_model():\n    inp = Input(shape=(trn_x.shape[1],), name='input')\n    x = layers.Dense(1000, activation='relu')(inp)\n    x = layers.Dropout(0.65)(x)\n    x = layers.Dense(750, activation='relu')(x)\n    x = layers.Dropout(0.65)(x)\n    x = layers.Dense(500, activation='relu')(x)\n    x = layers.Dropout(0.6)(x)\n    x = layers.Dense(1, activation='sigmoid')(x)\n    \n    model = Model(inp, x)\n    model.compile(optimizer='adam',\n                 loss='binary_crossentropy', metrics=['acc'])\n    \n    return model\n\nmodel = build_model()\nmodel.summary()","0fd5e0b7":"weights_path = f'weights.best.hdf5'\nval_loss_checkpoint = ModelCheckpoint(weights_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nreduceLR = ReduceLROnPlateau(monitor='val_loss', patience=5, verbose=1, mode='min', min_lr=1e-6)","7169a1b0":"model.fit(trn_x, trn_y, epochs=80, validation_data=(valid_x, valid_y),\n         callbacks=[val_loss_checkpoint, reduceLR], batch_size=512, verbose=1)","e34efe8b":"model.load_weights(weights_path)","7065c8d8":"val_preds = model.predict(valid_x, batch_size=2048, verbose=1)","f3075406":"roc_auc_score(valid_y.values, val_preds.reshape(-1))","f20ea96b":"test_preds = model.predict(test_x, batch_size=2048, verbose=1)","e5687a71":"sub_df = pd.read_csv(f'{data_dir}sample_submission.csv')\nsub_df.target = test_preds.reshape(-1)\nsub_df.head()","e2a1259f":"sub_df.to_csv('solution.csv', index=False)","49a05257":"### Loading data","4fe3e1fb":"### Categorical Feature","b6b3a164":"So there are no missing values in either training or test set.","1fc3716a":"### Train-validation split\n\nLet's split our training set such that 15% of data are used for validation -","d82405ba":"### Imports","4520fcaa":"### Training","46d06b7e":"### Normalize features","8ef2e8eb":"### Prediction on test data","11906da4":"### Target distribution","1b3e7f14":"### roc_auc_score on validation data","679786ad":"Looks like class labels are uniformly distributed in training data.","3af84052":"### Model"}}