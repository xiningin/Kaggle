{"cell_type":{"cae5558f":"code","4749906c":"code","301a6374":"code","9bbefb3e":"code","6e720f4a":"code","93efe59e":"code","aab5ad34":"code","e5cd016b":"code","7dae10ce":"code","f0f7e12f":"code","300513d6":"code","e2838ee1":"code","70223eb7":"code","4f71efd3":"code","35ca1f57":"code","21c57ca6":"code","5b95d9c4":"code","2f74a9c3":"code","93a71917":"code","67a58ad9":"code","9fe5eca7":"code","ecc477cc":"code","087dc6e2":"code","5a44ff03":"code","a974db21":"code","e1754998":"code","bd8b79fe":"code","d827964f":"code","3db37811":"code","3850e19a":"code","7c8cbfce":"code","6c317f39":"code","a11af86a":"code","9c309769":"code","e8033b02":"code","a45b09af":"code","528e271a":"code","da9a4a23":"code","f8b35d78":"code","08d92295":"code","f0659694":"code","607f8f50":"code","ee6f8520":"code","9aa8a6e6":"code","9a811f4e":"code","53868909":"code","9c21c48f":"code","45b67a4b":"code","596cfa29":"code","51b988b6":"code","192a31d8":"markdown","28f1781d":"markdown","7de76f6b":"markdown","ea051410":"markdown","ea54a553":"markdown","8f26a7e9":"markdown","fc41caec":"markdown","04bab21d":"markdown","0a90802c":"markdown","1685d49c":"markdown","914581ff":"markdown","35879d82":"markdown","cd769c9f":"markdown","2733a252":"markdown","3cab143a":"markdown","525998db":"markdown","893e8a08":"markdown","faff818e":"markdown","51520144":"markdown"},"source":{"cae5558f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport keras\nprint(keras.__version__)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom math import nan\nfrom keras.callbacks import ModelCheckpoint\n\n!pip install git+https:\/\/www.github.com\/keras-team\/keras-contrib.git\nfrom keras_contrib.layers import CRF\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","4749906c":"dframe = pd.read_csv(\"..\/input\/entity-annotated-corpus\/ner.csv\", encoding = \"ISO-8859-1\", error_bad_lines=False)","301a6374":"dframe","9bbefb3e":"dframe.columns","6e720f4a":"dataset=dframe.drop(['Unnamed: 0', 'lemma', 'next-lemma', 'next-next-lemma', 'next-next-pos',\n       'next-next-shape', 'next-next-word', 'next-pos', 'next-shape',\n       'next-word', 'prev-iob', 'prev-lemma', 'prev-pos',\n       'prev-prev-iob', 'prev-prev-lemma', 'prev-prev-pos', 'prev-prev-shape',\n       'prev-prev-word', 'prev-shape', 'prev-word',\"pos\"],axis=1)","93efe59e":"dataset.info()","aab5ad34":"dataset.head()","e5cd016b":"dataset=dataset.drop(['shape'],axis=1)","7dae10ce":"dataset.head()","f0f7e12f":"class SentenceGetter(object):\n    \n    def __init__(self, dataset):\n        self.n_sent = 1\n        self.dataset = dataset\n        self.empty = False\n        agg_func = lambda s: [(w, t) for w,t in zip(s[\"word\"].values.tolist(),\n                                                        s[\"tag\"].values.tolist())]\n        self.grouped = self.dataset.groupby(\"sentence_idx\").apply(agg_func)\n        self.sentences = [s for s in self.grouped]\n    \n    def get_next(self):\n        try:\n            s = self.grouped[\"Sentence: {}\".format(self.n_sent)]\n            self.n_sent += 1\n            return s\n        except:\n            return None","300513d6":"getter = SentenceGetter(dataset)","e2838ee1":"sentences = getter.sentences","70223eb7":"print(sentences[5])","4f71efd3":"maxlen = max([len(s) for s in sentences])\nprint ('Maximum sequence length:', maxlen)","35ca1f57":"# Check how long sentences are so that we can pad them\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.style.use(\"ggplot\")","21c57ca6":"plt.hist([len(s) for s in sentences], bins=50)\nplt.show()","5b95d9c4":"words = list(set(dataset[\"word\"].values))\nwords.append(\"ENDPAD\")","2f74a9c3":"n_words = len(words); n_words","93a71917":"tags = []\nfor tag in set(dataset[\"tag\"].values):\n    if tag is nan or isinstance(tag, float):\n        tags.append('unk')\n    else:\n        tags.append(tag)\nprint(tags)","67a58ad9":"n_tags = len(tags); n_tags","9fe5eca7":"from future.utils import iteritems\nword2idx = {w: i for i, w in enumerate(words)}\ntag2idx = {t: i for i, t in enumerate(tags)}\nidx2tag = {v: k for k, v in iteritems(tag2idx)}","ecc477cc":"word2idx['Obama']","087dc6e2":"tag2idx[\"O\"]","5a44ff03":"tag2idx","a974db21":"idx2tag[5]","e1754998":"idx2tag","bd8b79fe":"from keras.preprocessing.sequence import pad_sequences\nX = [[word2idx[w[0]] for w in s] for s in sentences]","d827964f":"np.array(X).shape","3db37811":"X = pad_sequences(maxlen=140, sequences=X, padding=\"post\",value=n_words - 1)","3850e19a":"y_idx = [[tag2idx[w[1]] for w in s] for s in sentences]\nprint(sentences[100])\nprint(y_idx[100])","7c8cbfce":"y = pad_sequences(maxlen=140, sequences=y_idx, padding=\"post\", value=tag2idx[\"O\"])\nprint(y_idx[100])","6c317f39":"from keras.utils import to_categorical\ny = [to_categorical(i, num_classes=n_tags) for i in y]","a11af86a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","9c309769":"from keras.models import Model, Input\nfrom keras.layers import LSTM, Embedding, Dense, TimeDistributed, Dropout, Bidirectional\nimport keras as k","e8033b02":"print(k.__version__)","a45b09af":"input = Input(shape=(140,))\nword_embedding_size = 300\nmodel = Embedding(input_dim=n_words, output_dim=word_embedding_size, input_length=140)(input)\nmodel = Bidirectional(LSTM(units=word_embedding_size, \n                           return_sequences=True, \n                           dropout=0.5, \n                           recurrent_dropout=0.5, \n                           kernel_initializer=k.initializers.he_normal()))(model)\nmodel = LSTM(units=word_embedding_size * 2, \n             return_sequences=True, \n             dropout=0.5, \n             recurrent_dropout=0.5, \n             kernel_initializer=k.initializers.he_normal())(model)\nmodel = TimeDistributed(Dense(n_tags, activation=\"relu\"))(model)  # previously softmax output layer\n\ncrf = CRF(n_tags)  # CRF layer\nout = crf(model)  # output","528e271a":"model = Model(input, out)","da9a4a23":"adam = k.optimizers.Adam(lr=0.0005, beta_1=0.9, beta_2=0.999)\n#model.compile(optimizer=adam, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.compile(optimizer=adam, loss=crf.loss_function, metrics=[crf.accuracy, 'accuracy'])","f8b35d78":"model.summary()","08d92295":"# Saving the best only\nfilepath=\"ner-bi-lstm-td-model-{val_acc:.2f}.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\ncallbacks_list = [checkpoint]","f0659694":"history = model.fit(X_train, np.array(y_train), batch_size=256, epochs=20, validation_split=0.2, verbose=1, callbacks=callbacks_list)","607f8f50":"TP = {}\nTN = {}\nFP = {}\nFN = {}\nfor tag in tag2idx.keys():\n    TP[tag] = 0\n    TN[tag] = 0    \n    FP[tag] = 0    \n    FN[tag] = 0    \n\ndef accumulate_score_by_tag(gt, pred):\n    \"\"\"\n    For each tag keep stats\n    \"\"\"\n    if gt == pred:\n        TP[gt] += 1\n    elif gt != 'O' and pred == 'O':\n        FN[gt] +=1\n    elif gt == 'O' and pred != 'O':\n        FP[gt] += 1\n    else:\n        TN[gt] += 1\n","ee6f8520":"i = 357\np = model.predict(np.array([X_test[i]]))\np = np.argmax(p, axis=-1)\ngt = np.argmax(y_test[i], axis=-1)\nprint(gt)\nprint(\"{:14}: ({:5}): {}\".format(\"Word\", \"True\", \"Pred\"))\nfor idx, (w,pred) in enumerate(zip(X_test[i],p[0])):\n    #\n    print(\"{:14}: ({:5}): {}\".format(words[w],idx2tag[gt[idx]],tags[pred]))","9aa8a6e6":"p = model.predict(np.array(X_test))  ","9a811f4e":"p.shape","53868909":"from sklearn.metrics import classification_report","9c21c48f":"np.argmax(p, axis=2)[0]","45b67a4b":"print(classification_report(np.argmax(y_test, 2).ravel(), np.argmax(p, axis=2).ravel(),labels=list(idx2tag.keys()), target_names=list(idx2tag.values())))","596cfa29":"for i, sentence in enumerate(X_test):\n    y_hat = np.argmax(p[i], axis=-1)\n    gt = np.argmax(y_test[i], axis=-1)\n    for idx, (w,pred) in enumerate(zip(sentence,y_hat)):\n        accumulate_score_by_tag(idx2tag[gt[idx]],tags[pred])","51b988b6":"for tag in tag2idx.keys():\n    print(f'tag:{tag}')    \n    print('\\t TN:{:10}\\tFP:{:10}'.format(TN[tag],FP[tag]))\n    print('\\t FN:{:10}\\tTP:{:10}'.format(FN[tag],TP[tag]))    ","192a31d8":"## Standard Classification Report","28f1781d":"## Accumulate the scores by tag","7de76f6b":"## Save the model after each epoch if validation is better","ea051410":"**Converting words to numbers and numbers to words**","ea54a553":"## Keras version","8f26a7e9":"## Fit","fc41caec":"## Fix the tags","04bab21d":"## Predict everything at once","0a90802c":"## Create list of list of tuples to differentiate each sentence from each other","1685d49c":"## The output is 3d: sent x word x tag prob (softmax)","914581ff":"## Model\n **Pay attention to the word embedding size","35879d82":"## How did Classification perform for each tag","cd769c9f":"## Import Keras","2733a252":"## Data preprocessing","3cab143a":"## Accumulate metrics by tag ","525998db":"Grab the 3d dimension and return the index of the highest probability ... the index matches the tag value\nnp.argmax(p, axis=2)","893e8a08":"\n## Importing the dataset for named entity recognition model","faff818e":"## We want word, pos, sentence_idx and tag as an input ","51520144":"## Single prediction and verbose results"}}