{"cell_type":{"20dae9ff":"code","72951705":"code","f2db8fff":"code","a969fa38":"code","d75b2580":"code","c05c2c31":"code","87e9348f":"code","7e238965":"code","ccabc353":"code","1aba8d1c":"code","b025d993":"code","3cbd2c35":"code","84fe6b9e":"code","fb83d52c":"code","198199f7":"code","c4ae5507":"code","f01794c2":"code","30882b0f":"code","6bce1bac":"code","eaa807a5":"code","abb794fa":"code","db7f3c48":"code","b5d5070c":"code","036132b0":"code","f12dd947":"code","cf5485e6":"code","53b32736":"code","5e8d3c74":"code","b49b69dc":"code","49b754f6":"code","785df9de":"code","f3907701":"code","6d637d5a":"code","1201dc0d":"code","ec35e04f":"code","875743be":"code","b433d696":"code","aff21545":"code","c3b74f27":"code","247e1c37":"code","ac93f5dc":"code","ff28fa14":"code","b063c5a0":"code","c695979f":"code","b6edbf9b":"code","e621ac9b":"code","759be618":"code","1d1dd900":"code","0626fea3":"code","37681261":"code","f511f11e":"code","db7ebcaa":"code","455655c9":"code","7a57315b":"code","23054bd7":"code","20683c85":"code","cfed4b01":"code","8ece0bdf":"code","b53e4cad":"code","7f7de467":"code","e661fb09":"code","a6dce811":"code","47b49038":"code","002bb288":"code","4509db75":"code","409e091a":"code","37d684fa":"code","65df93ef":"code","ace001c0":"code","3207d132":"code","b78f0f5a":"code","4be35c7a":"code","3ee33872":"code","c572a83c":"code","e5bc462b":"markdown","64a66375":"markdown","5b1a0a49":"markdown","4856c378":"markdown","57d12c53":"markdown","cbd9422a":"markdown","4858ee95":"markdown","f6f76ae1":"markdown","3c247365":"markdown","82600039":"markdown","39977b41":"markdown","c6737a8f":"markdown"},"source":{"20dae9ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport scipy.stats\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","72951705":"#Train data\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv').set_index('Id')\n#Test data\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv').set_index('Id')","f2db8fff":"train","a969fa38":"preprocessing_pipeline = []\ntest_preprocessing_pipeline = []\n\ndrop_cols = []\nhave_null_value_cols = [] #Columns to estimate none value\n\nfor col in  train.columns:\n    col_to_allRow = train[col].isnull().values.sum()\/len(train)\n    print(f'Ratio between non-null {col} rows to all rows {col_to_allRow}')\n    # if whose ratio higher than 40% (arbitrarily set), I would drop those.\n    if col_to_allRow > 0.4:\n        drop_cols.append(col)\n    elif col_to_allRow > 0:\n        have_null_value_cols.append(col)\n        ","d75b2580":"def drop_unused_col_by_ratio(dataframe: pd.DataFrame):\n    return dataframe.drop(drop_cols, axis=1)\n\npreprocessing_pipeline.append(drop_unused_col_by_ratio)\ntest_preprocessing_pipeline.append(drop_unused_col_by_ratio)\n","c05c2c31":"train_1st_stage = drop_unused_col_by_ratio(train)","87e9348f":"train_1st_stage","7e238965":"def get_condition(combine, feature, observation):\n    '''\n    Function for returning a resulting dataframe\n    '''\n    new_df = (combine[[feature, observation]].groupby([feature], as_index=False).\n             mean().\n             sort_values(by=observation, ascending=False))\n    new_df.columns = [feature, f'{observation}_mean']\n    std_df = (combine[[feature, observation]].groupby([feature], as_index=False).\n             std().\n             sort_values(by=observation, ascending=False))\n    new_df[f'{observation}_std'] = std_df[observation] \n    return new_df\ncondition = get_condition(train_1st_stage ,'MSSubClass', 'SalePrice')\ncondition['SalePrice_mean']","ccabc353":"overall_mean = train_1st_stage['SalePrice'].mean()\ntrain_1st_stage['overall_mean'] = overall_mean\nSalePrice = train_1st_stage['SalePrice']\noverall_mean = train_1st_stage['overall_mean']\nSST = sum((SalePrice - overall_mean)**2)\n\nprint(f'SST: {SST}')\n","1aba8d1c":"col = 'MSSubClass'\ngroup_means = train_1st_stage[[col, 'SalePrice']].groupby(col).mean()\ngroup_means = group_means.rename(columns = {'SalePrice':'group_mean'})\n","b025d993":"group_means","3cbd2c35":"train_1st_stage = train_1st_stage.merge(group_means, left_on= col, right_index= True)","84fe6b9e":"group_mean = train_1st_stage['group_mean']\nSSW = sum((SalePrice - group_mean)**2)\nprint(f'SSW: {SSW}')\n\n","fb83d52c":"SSW","198199f7":"SSB = sum((train_1st_stage['overall_mean'] - train_1st_stage['group_mean'])**2)\nSSB","c4ae5507":"SSB_prove = 0.0\nk = len(group_means)\nfor j in range(k):\n    n_j = len(train_1st_stage[train_1st_stage[col] == group_means.index[j]])\n    X_j = group_means.iloc[j][0]\n    X_t = overall_mean.iloc[0]\n    SSB_prove += n_j*(X_j - X_t)**2\nSSB_prove","f01794c2":"int(SSB_prove) == int(SSB)","30882b0f":"k = group_means.shape[0]\nn = train_1st_stage.shape[0]\ndf_between_group = k - 1\ndf_within_group = n - k\ndf_total = n - 1","6bce1bac":"MSB = SSB\/df_between_group\nMSW = SSW\/df_within_group","eaa807a5":"print(f'MSB: {MSB}\\nMSW: {MSW}')","abb794fa":"F = MSB\/MSW\nF","db7f3c48":"p_value = 1- scipy.stats.f.cdf(F, df_between_group, df_within_group)","b5d5070c":"print(f'P_value of {col}: {p_value}')","036132b0":"n = train_1st_stage.shape[0]\nn","f12dd947":"set(train_1st_stage[col])\n","cf5485e6":"# for _x_ in set(train_1st_stage[col]):\n#    print(f\"train_1st_stage[train_1st_stage[col] == {_x_}]['SalePrice'],\")","53b32736":"def one_way_ANOVA(col, debug = False):\n    #Cal SST\n    overall_mean = train_1st_stage['SalePrice'].mean()\n    train_1st_stage['overall_mean'] = overall_mean\n    SalePrice = train_1st_stage['SalePrice']\n    overall_mean = train_1st_stage['overall_mean']\n    SST = sum((SalePrice - overall_mean)**2)\n    if debug: print(f'SST: {SST}')\n    \n    #Cal SSW\n    group_means = train_1st_stage[[col, 'SalePrice']].groupby(col).mean()\n    group_means = group_means.rename(columns = {'SalePrice':'group_mean'})\n    group_mean = train_1st_stage['group_mean']\n    SSW = sum((SalePrice - group_mean)**2)\n    if debug: print(f'SSW: {SSW}')\n    \n    #Cal SSB\n    SSB = sum((train_1st_stage['overall_mean'] - train_1st_stage['group_mean'])**2)\n    if debug: print(f'SSB: {SSB}')\n    \n    #Cal Degree of freedom\n    k = group_means.shape[0]\n    n = train_1st_stage.shape[0]\n    df_between_group = k - 1\n    df_within_group = n - k\n    df_total = n - 1\n    \n    #Cal Mean square\n    MSB = SSB\/df_between_group\n    MSW = SSW\/df_within_group\n    \n    #Cal F ratio\n    F = MSB\/MSW\n    \n    #Cal P value\n    p_value = 1- scipy.stats.f.cdf(F, df_between_group, df_within_group)\n    \n    return p_value\n\none_way_ANOVA('MSSubClass')","5e8d3c74":"continuous_data = []\nfor col in train_1st_stage.columns:\n    if len(set(train_1st_stage[col])) < 20:\n        try:\n            print(f'Column {col}: {one_way_ANOVA(col)}')\n        except:\n            print(f'Column {col}: {None}')\n    else:\n        continuous_data.append(col)","b49b69dc":"continuous_data","49b754f6":"corr_table = train_1st_stage[continuous_data].corr(method ='pearson')","785df9de":"corr_table[np.abs(corr_table['SalePrice']) < 0.05].index","f3907701":"import matplotlib.pyplot as plt\n\n\nf, axs = plt.subplots(2,2,figsize=(15,15))\n\nfor i,feature in enumerate(corr_table[np.abs(corr_table['SalePrice']) < 0.05].index):\n    row = int(i\/2)\n    col = i % 2\n\n    axs[row][col].plot(train_1st_stage[feature], train_1st_stage['SalePrice'], '*')\n    axs[row][col].title.set_text(feature)","6d637d5a":"corr_table[np.abs(corr_table['SalePrice']) < 0.05].index","1201dc0d":"def drop_unrelated_columns(df):\n    drop_columns = ['BsmtFinSF2', 'LowQualFinSF', '3SsnPorch', 'MiscVal']\n    return df.drop(drop_columns, axis = 1)\npreprocessing_pipeline.append(drop_unrelated_columns)\ntest_preprocessing_pipeline.append(drop_unrelated_columns)\ntrain_2nd_stage = drop_unrelated_columns(train_1st_stage)","ec35e04f":"def drop_null_lot_Frontage(df):\n    return df.loc[df['LotFrontage'].notnull()]\npreprocessing_pipeline.append(drop_null_lot_Frontage)\ntrain_3rd_stage = train_2nd_stage.loc[train_2nd_stage['LotFrontage'].notnull()]","875743be":"train_3rd_stage\n","b433d696":"still_have_null_value = []\nfor col in  train_3rd_stage.columns:\n    col_to_allRow = train_3rd_stage[col].isnull().values.sum()\/len(train)\n    if col_to_allRow > 0:\n        print(col)\n        still_have_null_value.append(col)","aff21545":"for col in train_3rd_stage[still_have_null_value].columns:\n    if len(set(train_3rd_stage[col]) ) > 7:\n        print(col)","c3b74f27":"def drop_MasVnrArea_and_GarageYrBlt(df):\n    return df[df['MasVnrArea'].notnull() & df['GarageYrBlt'].notnull()]\ntrain_4th_stage = drop_MasVnrArea_and_GarageYrBlt(train_3rd_stage)\npreprocessing_pipeline.append(drop_MasVnrArea_and_GarageYrBlt)","247e1c37":"def drop_class_null(df):\n    return df[df['BsmtQual'].notnull() &\n              df['BsmtExposure'].notnull() & \n              df['BsmtFinType2'].notnull() &\n              df['Electrical'].notnull()]\npreprocessing_pipeline.append(drop_class_null)\ntrain_5th_stage = drop_class_null(train_4th_stage)","ac93f5dc":"train_5th_stage","ff28fa14":"still_have_null_value = []\nfor col in  train_5th_stage.columns:\n    col_to_allRow = train_5th_stage[col].isnull().values.sum()\/len(train)\n    if col_to_allRow > 0:\n        print(col)\n        still_have_null_value.append(col)\nif still_have_null_value == []:\n    print('No null')","b063c5a0":"# for list(set(train_5th_stage['MSSubClass']))","c695979f":"list(set(train_5th_stage['MSSubClass'])).sort()","b6edbf9b":"temp = train\nfor func in preprocessing_pipeline:\n    temp = func(temp)","e621ac9b":"col_mapper = {}\nfor col in temp:\n    if temp[col].dtype == 'O':\n        print(col)\n        col_map = {}\n        for v,value in enumerate(set(temp[col])):\n            col_map[value] = v\n        col_mapper[col] = col_map\n        print('-'*13)","759be618":"for col, mapper in col_mapper.items():\n    temp[col] = temp[col].map(mapper)","1d1dd900":"temp","0626fea3":"def parse_data(df):\n    for col, mapper in col_mapper.items():\n        df[col] = df[col].map(mapper)\n        \n    for col in df.columns:\n        if len(set(df[col])) <= 1:\n            df = df.drop(col, axis = 1)\n    return df\npreprocessing_pipeline.append(parse_data)\ntest_preprocessing_pipeline.append(parse_data)","37681261":"temp = train\nfor func in preprocessing_pipeline:\n    temp = func(temp)\ntemp","f511f11e":"split_ratio = 0.75\nsplit_index = int(split_ratio*temp.shape[0])\nTrain = temp.iloc[:split_index]\nTest  = temp.iloc[split_index :]","db7ebcaa":"Train","455655c9":"X_Train = Train.drop('SalePrice', axis = 1)\nY_Train = Train[['SalePrice']]\n\nX_Test = Test.drop('SalePrice', axis = 1)\nY_Test = Test[['SalePrice']]","7a57315b":"from sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import metrics\n\nimport tensorflow as tf\nfrom keras.regularizers import l1, l2\n\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers.experimental import preprocessing\nfrom tensorflow_addons.layers import WeightNormalization\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\ndef getModel(model = 'linear'):\n    if model == 'linear':\n        return make_pipeline(StandardScaler(), Lasso(alpha = 2))\n    elif model == 'nn':\n        horsepower = np.array(X_Train)\n        horsepower_normalizer = preprocessing.Normalization(input_shape=[X_Train.shape[1],])\n        horsepower_normalizer.adapt(horsepower)\n        \n        horsepower_model = tf.keras.Sequential([\n            horsepower_normalizer,\n            WeightNormalization(layers.Dense(units=8 , activation='relu')),\n            layers.BatchNormalization(),\n            layers.Dense(units=1)\n        ])\n        \n        horsepower_model.summary()\n        \n        METRICS = [\n            metrics.MeanSquaredError(),\n\n        ]\n        \n        horsepower_model.compile(\n                            optimizer=tf.optimizers.Adam(learning_rate=0.1),\n                            loss='mae',\n                            metrics=METRICS)\n        return horsepower_model\n        \nreg = getModel('nn')\n\nfilepath=\"best_weights.hdf5\"\nMonitor = 'loss'\ncheckpoint = ModelCheckpoint(filepath, monitor=Monitor, verbose=1, save_best_only=True, mode='min')\nreg.fit(X_Train, Y_Train, epochs = 3000,callbacks = [checkpoint])\nfrom keras.models import load_model\nreg = load_model('best_weights.hdf5')","23054bd7":"np.array(X_Train)","20683c85":"plt.plot(X_Train['LotArea'],Y_Train, '*')\nplt.plot(X_Train['LotArea'],reg.predict(X_Train).flatten(), '*')","cfed4b01":"try:\n    reg.score(X_Train, Y_Train)\nexcept:\n    pass","8ece0bdf":"to_be_predict_data = test\n\nfor func in test_preprocessing_pipeline:\n    to_be_predict_data = func(to_be_predict_data)\nto_be_predict_data","b53e4cad":"SUM = 0\nnull_col = []\n\nfor col in to_be_predict_data:\n    if any(to_be_predict_data[col].isnull()):\n        print(col)\n        null_col.append((col, to_be_predict_data[col][to_be_predict_data[col].isnull()].index))\n        print(to_be_predict_data[col][to_be_predict_data[col].isnull()])\n","7f7de467":"to_be_predict_data['RoofStyle']","e661fb09":"to_be_predict_data.corr()['RoofStyle'].sort_values(ascending=False, key = lambda x: np.abs(x)).index[1:6]","a6dce811":"from sklearn.metrics.pairwise import cosine_similarity\nsim = cosine_similarity(to_be_predict_data.fillna(0)[['MasVnrArea', '1stFlrSF', 'TotalBsmtSF', 'BsmtQual', 'BsmtFinSF1']])","47b49038":"sim_df = pd.DataFrame(sim, index = to_be_predict_data.index, columns = to_be_predict_data.index)\nsim_df","002bb288":"sim_df[2514].sort_values().iloc[-2:]","4509db75":"to_be_predict_data.loc[2514,'RoofStyle'] = to_be_predict_data.loc[2173]['RoofStyle']","409e091a":"most_sim_index = sim_df[2611].sort_values().iloc[-2:].index[0]\nto_be_predict_data.loc[2611,'RoofStyle'] = to_be_predict_data.loc[most_sim_index]['RoofStyle']","37d684fa":"most_sim_index = sim_df[2807].sort_values().iloc[-2:].index[0]\nto_be_predict_data.loc[2807,'RoofStyle'] = to_be_predict_data.loc[most_sim_index]['RoofStyle']\n","65df93ef":"to_be_predict_data.loc[2807,'RoofStyle']","ace001c0":"null_col","3207d132":"for col, indices in null_col:\n    for index in indices:\n        print(f'Col: {col} index: {index}')\n        most_releted_cols = to_be_predict_data.corr()[col].sort_values(ascending=False, key = lambda x: np.abs(x)).index[1:6]\n        sim = cosine_similarity(to_be_predict_data.fillna(0)[most_releted_cols])\n        sim_df = pd.DataFrame(sim, index = to_be_predict_data.index, columns = to_be_predict_data.index)\n        start = -2\n        for m_sim in sim_df[index].sort_values().iloc:\n            most_sim_index = sim_df[index].sort_values().iloc[start:].index[0]\n            if np.isnan(to_be_predict_data.loc[most_sim_index][col]):\n                start -= 1\n            else:\n                break\n        to_be_predict_data.loc[index,col] = to_be_predict_data.loc[most_sim_index][col]","b78f0f5a":"np.isnan(to_be_predict_data.loc[most_sim_index][col])","4be35c7a":"prediction = pd.DataFrame(reg.predict(to_be_predict_data.drop(['Utilities'], axis =1 )), index = to_be_predict_data.index, columns = ['SalePrice'])","3ee33872":"prediction.to_csv('submission.csv')","c572a83c":"prediction","e5bc462b":"# Preprocessing\n## Check which columns to be dropped","64a66375":"# Find condition\n## Using one-way ANOVA\nref: https:\/\/towardsdatascience.com\/1-way-anova-from-scratch-dissecting-the-anova-table-with-a-worked-example-170f4f2e58ad\n\nhttp:\/\/www2.it.kmutnb.ac.th\/teacher\/FileDL\/AsstProfDrNalinpat183256000043.pdf","5b1a0a49":"# Corelation","4856c378":"# Analytics","57d12c53":"## Degrees of freedom","cbd9422a":"# Importing","4858ee95":"## Sum of Squares Between group","f6f76ae1":"## Mean squares\n\n$$\nMS_{\\text{Between group}} = \\frac{SSB}{dfB}\\\\\nMS_{\\text{within group}} = \\frac{SSW}{dfW}\\\\\n$$","3c247365":"## Getting Sum of Squares Total (SST)","82600039":"## F-ratio","39977b41":"## Sum of Squares Residual (within group)","c6737a8f":"# import data"}}