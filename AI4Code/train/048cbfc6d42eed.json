{"cell_type":{"1877aa4f":"code","2b2600dd":"code","57df9bdb":"code","01674a92":"code","489867df":"code","8be7a256":"code","718e3733":"code","6b317d88":"code","612c01cf":"code","e2d9871f":"code","d3e31ab9":"code","01d6d4be":"code","13839e9a":"code","bf27705e":"code","cf1a39fc":"code","a2eeafe2":"code","f3de9620":"markdown","3f342b5a":"markdown","4b1dd207":"markdown","22c5f2e9":"markdown","0dfbff54":"markdown","443b5a3f":"markdown","e5ecb898":"markdown","b01d11a7":"markdown","382dd258":"markdown","1c82a5b0":"markdown","b035e6c9":"markdown","59399a59":"markdown"},"source":{"1877aa4f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt #Data Visualization \nimport seaborn as sns  #Python library for Vidualization\nimport os\nnp.random.seed(10)\n\nfrom sklearn import cluster, datasets, mixture\nX1,Y1 = datasets.make_moons(n_samples=2000, noise=.09,random_state=10)\n#plt.scatter(X1[:, 0], X1[:, 1], marker='o', c=Y1,s=25, edgecolor='r')\nprint(X1.shape)\nprint(Y1.shape)\nplt.scatter(X1[:, 0], X1[:, 1], s=10, c=Y1)\nplt.title('DATASET 1')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\n#plt.savefig('Dataset1')\n\nplt.show()\n\n\nfrom sklearn.datasets import make_blobs\nX3,Y3  = make_blobs(n_samples=2000,cluster_std=3.5,centers=2, n_features=2,random_state=10)\nprint(X3.shape)\nprint(Y3.shape)\nplt.title('DATASET 2')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.scatter(X3[:, 0], X3[:, 1], s=10, c=Y3)\n#plt.savefig('Dataset2')\nplt.show()","2b2600dd":"from sklearn.cluster import KMeans\nfrom sklearn.cluster import Birch\nfrom sklearn.cluster import AgglomerativeClustering\n","57df9bdb":"\n#Model Build dataset1\nkmeansmodel = KMeans(n_clusters= 2, init='k-means++',max_iter=1000,random_state=10)\ny_kmeans= kmeansmodel.fit_predict(X1)\n\nbirchmodel=Birch(n_clusters=2,threshold=0.5,branching_factor=100)\ny_birch=birchmodel.fit_predict(X1)\nprint(y_birch.shape)\n\nagnesmodel = AgglomerativeClustering(n_clusters=2)\ny_agnes=birchmodel.fit_predict(X1)\nprint(y_agnes.shape)\n\n\n\n#Model Build dataset 2\nkmeansmodel2 = KMeans(n_clusters= 2, init='k-means++',max_iter=1000,random_state=10)\ny_kmeans2= kmeansmodel2.fit_predict(X3)\n\nbirchmodel2=Birch(n_clusters=2,threshold=0.1,branching_factor=100)\ny_birch2=birchmodel2.fit_predict(X3)\nprint(y_birch2.shape)\n\nagnesmodel2 = AgglomerativeClustering(n_clusters=2)\ny_agnes2=agnesmodel2.fit_predict(X3)\nprint(y_agnes2.shape)\n","01674a92":"#print(y_kmeans)","489867df":"plt.figure(figsize=(15,5))\n#plt.subplot(1,2,1)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_kmeans)\nplt.title('Kmeans Dataset1')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1',dpi=300)\nplt.show()\n\n#plt.subplot(1,2,2)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_kmeans2)\nplt.title('Kmeans Dataset2')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1d2',dpi=300)\nplt.show()","8be7a256":"## K mediod Implementation\ndef euclideanDistance(x, y):\n    '''\n    Euclidean distance between x, y\n    --------\n    Return\n    d: float\n    '''\n    squared_d = 0\n    for i in range(len(x)):\n        squared_d += (x[i] - y[i])**2\n    d = np.sqrt(squared_d)\n    return d\n\ndef visualize(X, label):\n    K = np.amax(label) + 1\n    X0 = X[label == 0, :]\n    X1 = X[label == 1, :]\n    #X2 = X[label == 2, :]\n    \n    #you can fix this dpi \n    #plt.figure(dpi=120)\n    \n    plt.plot(X0[:, 0], X0[:, 1], 'b^')\n    plt.plot(X1[:, 0], X1[:, 1], 'go')\n    #plt.plot(X2[:, 0], X2[:, 1], 'rs', markersize = 4, alpha = .8)\n\n    #plt.axis('equal')\n    #plt.plot()\n    #plt.show()\n    \n#visualize(X, original_label)\nclass k_medoids:\n    def __init__(self, k = 2, max_iter = 300, has_converged = False):\n        self.k = k\n        self.max_iter = max_iter\n        self.has_converged = has_converged\n        self.medoids_cost = []\n        \n    def initMedoids(self, X):\n        ''' \n        Parameters\n        ----------\n        X: input data. \n        '''\n        self.medoids = []\n        \n        #Starting medoids will be random members from data set X\n        indexes = np.random.randint(0, len(X)-1,self.k)\n        self.medoids = X[indexes]\n        \n        for i in range(0,self.k):\n            self.medoids_cost.append(0)\n        \n    def isConverged(self, new_medoids):\n        return set([tuple(x) for x in self.medoids]) == set([tuple(x) for x in new_medoids])\n        \n    def updateMedoids(self, X, labels):\n        self.has_converged = True\n        \n        #Store data points to the current cluster they belong to\n        clusters = []\n        for i in range(0,self.k):\n            cluster = []\n            for j in range(len(X)):\n                if (labels[j] == i):\n                    cluster.append(X[j])\n            clusters.append(cluster)\n        \n        #Calculate the new medoids\n        new_medoids = []\n        for i in range(0, self.k):\n            new_medoid = self.medoids[i]\n            old_medoids_cost = self.medoids_cost[i]\n            for j in range(len(clusters[i])):\n                \n                #Cost of the current data points to be compared with the current optimal cost\n                cur_medoids_cost = 0\n                for dpoint_index in range(len(clusters[i])):\n                    cur_medoids_cost += euclideanDistance(clusters[i][j], clusters[i][dpoint_index])\n                \n                #If current cost is less than current optimal cost,\n                #make the current data point new medoid of the cluster\n                if cur_medoids_cost < old_medoids_cost:\n                    new_medoid = clusters[i][j]\n                    old_medoids_cost = cur_medoids_cost\n            \n            #Now we have the optimal medoid of the current cluster\n            new_medoids.append(new_medoid)\n        \n        #If not converged yet, accept the new medoids\n        if not self.isConverged(new_medoids):\n            self.medoids = new_medoids\n            self.has_converged = False\n    \n    def fit(self, X):\n        self.initMedoids(X)\n        \n        for i in range(self.max_iter):\n            #Labels for this iteration\n            cur_labels = []\n            for medoid in range(0,self.k):\n                #Dissimilarity cost of the current cluster\n                self.medoids_cost[medoid] = 0\n                for k in range(len(X)):\n                    #Distances from a data point to each of the medoids\n                    d_list = []                    \n                    for j in range(0,self.k):\n                        d_list.append(euclideanDistance(self.medoids[j], X[k]))\n                    #Data points' label is the medoid which has minimal distance to it\n                    cur_labels.append(d_list.index(min(d_list)))\n                    \n                    self.medoids_cost[medoid] += min(d_list)\n                                \n            self.updateMedoids(X, cur_labels)\n            \n            if self.has_converged:\n                break\n\n        return np.array(self.medoids)\n\n        \n    def predict(self,data):\n        pred = []\n        for i in range(len(data)):\n            #Distances from a data point to each of the medoids\n            d_list = []\n            for j in range(len(self.medoids)):\n                d_list.append(euclideanDistance(self.medoids[j],data[i]))\n                \n            pred.append(d_list.index(min(d_list)))\n            \n        return np.array(pred)\n\n","718e3733":"point,lb = datasets.make_moons(n_samples=2000, noise=.08,random_state=10)\nmodel=k_medoids(k=2)\nprint('Centers found by your model(dataset1):')\nprint(model.fit(point))\nplt.figure(figsize=(15,5))\n\npred = model.predict(point)\nplt.subplot(1, 2, 1)\nvisualize(point,pred)\n\n\nfrom sklearn.datasets import make_blobs\npoint1,lb1  = make_blobs(n_samples=2000,cluster_std=3.5, centers=2, n_features=2,random_state=10)\nmodel=k_medoids(k=2)\nprint('Centers found by your model(dataset2):')\nprint(model.fit(point1))\n\npred1 = model.predict(point1)\nplt.subplot(1, 2, 2)\nvisualize(point1,pred1)\n\n#plt.savefig('Kmediodd1d2',dpi=300)\n\n#plt.show()\n\n#print(pred)","6b317d88":"plt.figure(figsize=(15,5))\n#plt.subplot(1,2,1)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_agnes)\nplt.title('Agnes Dataset1')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1',dpi=300)\nplt.show()\n\n#plt.subplot(1,2,2)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_agnes2)\nplt.title('Agnes Dataset2')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1d2',dpi=300)\nplt.show()","612c01cf":"#plt.figure(figsize=(15,5))\nplt.subplot(1,2,1)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_birch)\nplt.title('Birch Dataset1')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1',dpi=300)\nplt.show()\n\n#plt.subplot(1,2,2)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_birch2)\nplt.title('Birch Dataset2')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('birchd1d2',dpi=300)\nplt.show()","e2d9871f":"def MyDBSCAN(D, eps, MinPts):  \n    labels = [0]*len(D)\n    C = 0\n    for P in range(0, len(D)):\n        if not (labels[P] == 0):\n           continue\n        NeighborPts = regionQuery(D, P, eps)\n        if len(NeighborPts) < MinPts:\n            labels[P] = -1    \n        else: \n           C += 1\n           growCluster(D, labels, P, NeighborPts, C, eps, MinPts)\n\n    return labels\n\n\ndef growCluster(D, labels, P, NeighborPts, C, eps, MinPts):\n    labels[P] = C\n    i = 0\n    while i < len(NeighborPts):    \n        Pn = NeighborPts[i]\n        if labels[Pn] == -1:\n           labels[Pn] = C\n        elif labels[Pn] == 0:\n            labels[Pn] = C\n            PnNeighborPts = regionQuery(D, Pn, eps)\n            if len(PnNeighborPts) >= MinPts:\n                NeighborPts = NeighborPts + PnNeighborPts\n        i += 1        \n\n\ndef regionQuery(D, P, eps):\n\n    neighbors = []\n    \n    for Pn in range(0, len(D)):\n        \n        if np.linalg.norm(D[P] - D[Pn]) < eps:\n           neighbors.append(Pn)\n            \n    return neighbors\n\n","d3e31ab9":"\ndbscan_labels1=MyDBSCAN(X1, .2, 70)\n#plt.figure(figsize=(10,8))\n#plt.subplot(1, 2, 5)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=dbscan_labels1)\nplt.show()\ninti_point = np.random.randint(0, len(X1)-1, 2 )\nmedoids=X1[inti_point]\n","01d6d4be":"dbscan_labels2=MyDBSCAN(X3,1,10)\n#plt.figure(figsize=(10,8))\n#plt.subplot(1, 2, 5)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=dbscan_labels2)\nplt.show()\ninti_point = np.random.randint(0, len(X3)-1, 2 )\nmedoids=X3[inti_point]","13839e9a":"plt.figure(figsize=(15,5))\n#plt.subplot(1,2,1)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=dbscan_labels1)\nplt.title('DBSCAN Dataset1')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('Kmeansd1',dpi=300)\n#plt.show()\n\n#plt.subplot(1,2,2)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=dbscan_labels2)\nplt.title('DBSCAN Dataset2')\nplt.xlabel('X axis')\nplt.ylabel('Y axis')\nplt.legend()\n#plt.savefig('dbscand1d2',dpi=300)\nplt.show()","bf27705e":"#ARI\n#NMI\n#Silhouette Coefficient\nfrom sklearn.metrics.cluster import adjusted_rand_score\nfrom sklearn.metrics.cluster import normalized_mutual_info_score\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\n\n\nari_kmeans=adjusted_rand_score(Y1,y_kmeans)\nari_kmediod=adjusted_rand_score(lb,pred)\nari_birch=adjusted_rand_score(Y1,y_birch)\nari_dbscan=adjusted_rand_score(Y1,dbscan_labels1)\nari_agnes=adjusted_rand_score(Y1,y_agnes)\n\nprint(\"DATASET1:\")\nprint(\"ARI of Kmeans: \"+ str(ari_kmeans))\nprint(\"ARI of Kmediod: \"+ str(ari_kmediod))\nprint(\"ARI of Birch :\"+ str(ari_birch))\nprint(\"ARI of Dbscan: \"+ str(ari_dbscan))\nprint(\"ARI of Agnes: \"+ str(ari_agnes))\n\n\nari_kmeans=adjusted_rand_score(Y3,y_kmeans2)\nari_kmediod=adjusted_rand_score(lb1,pred1)\nari_birch=adjusted_rand_score(Y3,y_birch2)\nari_dbscan=adjusted_rand_score(Y3,dbscan_labels2)\nari_agnes=adjusted_rand_score(Y3,y_agnes2)\n\nprint(\"DATASET2:\")\nprint(\"ARI of Kmeans: \"+ str(ari_kmeans))\nprint(\"ARI of Kmediod: \"+ str(ari_kmediod))\nprint(\"ARI of Birch :\"+ str(ari_birch))\nprint(\"ARI of Dbscan: \"+ str(ari_dbscan))\nprint(\"ARI of Agnes: \"+ str(ari_agnes))\n\n\n\n\nnmi_kmeans=normalized_mutual_info_score(Y1,y_kmeans)\nnmi_kmediod=normalized_mutual_info_score(lb,pred)\nnmi_birch=normalized_mutual_info_score(Y1,y_birch)\nnmi_dbscan=normalized_mutual_info_score(Y1,dbscan_labels1)\nnmi_agnes=normalized_mutual_info_score(Y1,y_agnes)\nprint(\"DATASET1:\")\nprint(\"NMI of Kmeans: \"+ str(nmi_kmeans))\nprint(\"NMI of Kmediod: \"+ str(nmi_kmediod))\nprint(\"NMI of Birch :\"+ str(nmi_birch))\nprint(\"NMI of Dbscan: \"+ str(nmi_dbscan))\nprint(\"NMI of Agnes: \"+ str(nmi_agnes))\n\n\nnmi_kmeans=normalized_mutual_info_score(Y3,y_kmeans2)\nnmi_kmediod=normalized_mutual_info_score(lb1,pred1)\nnmi_birch=normalized_mutual_info_score(Y3,y_birch2)\nnmi_dbscan=normalized_mutual_info_score(Y3,dbscan_labels2)\nnmi_agnes=normalized_mutual_info_score(Y3,y_agnes2)\nprint(\"DATASET2:\")\nprint(\"NMI of Kmeans: \"+ str(nmi_kmeans))\nprint(\"NMI of Kmediod: \"+ str(nmi_kmediod))\nprint(\"NMI of Birch :\"+ str(nmi_birch))\nprint(\"NMI of Dbscan: \"+ str(nmi_dbscan))\nprint(\"NMI of Agnes: \"+ str(nmi_agnes))\n\n\n\n\n\nsil_kmeans=silhouette_score(X1,y_kmeans)\nsil_kmediod=silhouette_score(point,pred)\nsil_birch=silhouette_score(X1,y_birch)\nsil_dbscan=silhouette_score(X1,dbscan_labels1)\nsil_agnes=silhouette_score(X1,y_agnes)\n\nprint(\"Dataset1:\")\nprint(\"Silhouette Coefficient with Kmeans: \"+ str(sil_kmeans))\nprint(\"Silhouette Coefficient with Kmediod: \"+ str(sil_kmediod))\nprint(\"Silhouette Coefficient with Birch :\"+ str(sil_birch))\nprint(\"Silhouette Coefficient with Dbscan : \"+ str(sil_dbscan))\nprint(\"Silhouette Coefficient with Agnes : \"+ str(sil_agnes))\n\nprint(\"Dataset2:\")\nsil_kmeans=silhouette_score(X3,y_kmeans2)\nsil_kmediod=silhouette_score(point1,pred1)\nsil_birch=silhouette_score(X3,y_birch2)\nsil_dbscan=silhouette_score(X3,dbscan_labels2)\nsil_agnes=silhouette_score(X3,y_agnes2)\n\nprint(\"Silhouette Coefficient with Kmeans: \"+ str(sil_kmeans))\nprint(\"Silhouette Coefficient with Kmediod: \"+ str(sil_kmediod))\nprint(\"Silhouette Coefficient with Birch :\"+ str(sil_birch))\nprint(\"Silhouette Coefficient with Dbscan : \"+ str(sil_dbscan))\nprint(\"Silhouette Coefficient with Agnes : \"+ str(sil_agnes))\n\n\n\n","cf1a39fc":"plt.figure(figsize=(25,10))\n\nplt.subplot(1,6,1)\nplt.scatter(X1[:, 0], X1[:, 1], s=100,c=Y1)\nplt.title('DATASET1')\n\nplt.subplot(1,6,2)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_kmeans)\nplt.title('KMEANS)')\n\n\n\n#Visualizing all the clusters of birch\nplt.subplot(1,6,3)\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_birch)\nplt.title('BIRCH')\n\n\nplt.subplot(1,6,4)\nplt.title('DBSCAN')\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=dbscan_labels1)\n\nplt.subplot(1,6,5)\nplt.title(\"AGNES\")\nplt.scatter(X1[:, 0], X1[:, 1], s=100, c=y_agnes)\n\nplt.subplot(1,6,6)\nplt.title('KMEDIOD')\nvisualize(point,pred)\n\nplt.savefig('alld1',dpi=300)\n\nplt.show()\n","a2eeafe2":"plt.figure(figsize=(20,10))\nplt.subplot(1,6,1)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=Y3)\nplt.title('DATASET2')\n\nplt.subplot(1,6,2)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_kmeans2)\n#plt.show()\nplt.title(\"KMEANS\")\n\nplt.subplot(1,6,3)\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_birch2)\nplt.title(\"BIRCH\")\n\nplt.subplot(1,6,4)\nplt.title(\"DBSCAN\")\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=dbscan_labels2)\n\nplt.subplot(1,6,5)\nplt.title(\"AGNES\")\nplt.scatter(X3[:, 0], X3[:, 1], s=100, c=y_agnes2)\n\nplt.subplot(1,6,6)\nplt.title('KMEDIOD')\nvisualize(point1,pred1)\n\nplt.savefig('alld2',dpi=300)\nplt.show()\n","f3de9620":"# All Result Compilation & Discussion:\n\n\n| **Scoring Method** | **Algorithm** | **Dataset1** | **Dataset2** |\n| --- | --- | --- | --- |\n| ARI | kmeans | 0.2476 | 0.9234 |\n| NMI | kmeans | 0.18714 | 0.8613 |\n| Silhouette Coefficient | kmeans | 0.48667 | 0.5907 |\n| ARI | kmediod | 0.3302 | 0.9254 |\n| NMI | kmediod | 0.2540 | 0.8642 |\n| Silhouette Coefficient | kmediod | 0.4863 | 0.5907 |\n| ARI | AGNES | 0.3767 | 0.9081 |\n| NMI | AGNES | 0.3413 | 0.8427 |\n| Silhouette Coefficient | AGNES | 0.4583 | 0.5878 |\n| ARI | Birch | 0.3767 | 0.8722 |\n| NMI | Birch | 0.3413 | 0.8102 |\n| Silhouette Coefficient | Birch | 0.4583 | 0.5760 |\n| ARI | DBSCAN | 0.9920 | -0.0001 |\n| NMI | DBSCAN | 0.9787 | 0.0022 |\n| Silhouette Coefficient | DBSCAN | 0.3010 | -0.1626 |\n\n                     Table 6: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2 on all algorithms.\n\nAccording to all the visualizations and the score from the NMI, ARI, Silhouette coefficient, we have seen that from dataset to dataset different clustering algorithm works differently. Some algorithms work better in one dataset and performs bad in other. So we need to know the dataset before applying algorithms.\n\nFrom our result we can see that in our dataset1 which is non convex shaped points, another dataset2 is convex shaped.\n\nFor partition based algorithms like kmeans, kmediod works good with respect to any dataset.but they works better with convex shaped clustered datasets.They are computationaly cheaper than others.\n\nFor hierarchical clustering like AGNES ,BIRCH ,we can see that they are very costly to computational cost. Don&#39;t to better in the case of huge datasets. It&#39;s only advantage is easy to implement. It performs good but it is not better than partition based clustering.\n\nFor density based clustering ,DBSCAN works well with data set separated data clusters with different density in the dataset. Density based DBSCAN is very sensitive to hyper paramemeter. With dataset2 we can see it performs very bad as it is very noisy data and density difference is very equivalent. But in dataset1 we can see that with different density of clusters, it works well.","3f342b5a":"# 5.DBSCAN\nDBSCAN means Density-based spatial clustering of applications with noise (DBSCAN).DBSCAN groups together points that are close to each other based on a distance measurement (usually Euclidean distance) and a minimum number of points. It also marks as outliers the points that are in low-density regions.\n                       \n                       Scores of ARI, NMI and Silhouette coefficient for dataset1 and dataset 2:\n\n| Scoring Method | Dataset1 | Dataset2 |\n| --- | --- | --- |\n| ARI | 0.99201 | -0.0001 |\n| NMI | 0.9787 | 0.0022 |\n| Silhouette Coefficient | 0.3010 | -0.16260 |\n\n                     Table 5: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2\n\nAs we can see from the visualization and the scoring values from the table,we can say that DBSCAN performed well on dataset1 and very bad on dataset.\n\nIt is because in the border region of the dataset and its density. In dataset 2 the border regions are very dense so DBSCAN fails to find the clusters but in dataset1,the border region have less noise and difference of density is prominent. That&#39;s why DBSCAN works well in dataset2.\n\nDBSCAN is very sensitive to it&#39;s hyperparameters. I had to do very fine tuing on epsilon. With slight wrong values, DBSCAN performs very poorly.\n\nBut this algorithm can detect outliers and it doesn&#39;t require any kind of cluster number to calculate.","4b1dd207":"# Datasets\n* For this experiment, I have taken two dataset for my work. This datasets are created with pythons API sklearn.\n* For dataset 1, I have used sklearn  make_moon() function. My dataset have 2000 points. In data point have two feature. The dataset is labeled and all points are randomly selected. I have added 0.09 as noise factor for the dataset 1.\n* For dataset 2, I have also used sklearn make_blob() function. This dataset is also 2000 data point.All the data points are labeled. There is two cluster in the dataset and each instance has about two input feature. For the clusters standard deviation is used 3.5. All the data are created randomly.\n\n","22c5f2e9":"# 4.BIRCH\n\nBirch stands for Balanced Iterative Reducing and Clustering. Regular clustering algorithms do not scale well in terms of running time and quality as the size of the dataset increases. This is where BIRCH clustering comes in. I have tuned the hyper parameters as threshold, branching factor and number of clusters for best result.\n\n                            Scores of ARI, NMI and Silhouette coefficient for dataset1 and dataset 2:\n\n| Scoring Method | Dataset1 | Dataset2 |\n| --- | --- | --- |\n| ARI | 0.3767 | 0.8722 |\n| NMI | 0.3413 | 0.8102 |\n| Silhouette Coefficient | 0.4583 | 0.5760 |\n\n                           Table 4: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2\n\nAs we can see from the visualization and the table of scoring, it is prevalent that Birch works like AGNES. Both of them are hierarchical clustering.\n\nIn my dataset 1 and dataset 2 BIRCH performs as like as agnes.\n\nIt is also very time consuming like Agnes.\n","0dfbff54":"# Conclusion: \nSo we can say from the analysis, different dataset works differently with clustering algorithms. With the nature of dataset, we should choose algorithm for clustering. There are many different clustering algorithms, and no single best method for all datasets.","443b5a3f":"# Comparative Analysis of Clustering Algorithms:\n* In the dataset1 and dataset2, I have applied the following clustering algorithms:\n1. KMEANS\n2. KMEDIOD\n3. BIRCH\n4. AGNES\n5. DBSCAN\n\n* All the hyper parameters are tuned to maximum efficiency by random search and repeatedly tested with the datasets. Now I will discuss each algorithm I have used and show it\u2019s cluster visualization with ARI, NMI and Silhouette coefficient.\n\n\n* Now i will build the alogrithms that are available in Sk learn here:\n","e5ecb898":"# 1. KMEANS:\n** Kmeans clustering Algorithm is based on partion. I applied kmeans clustering from the python sklearn library. But I have tuned the hyper parameters.\n\nScores of ARI, NMI and Silhouette coefficient for dataset1 and dataset 2:\n\n| Scoring Method | Dataset1 | Dataset 2 |\n| --- | --- | --- |\n| ARI | 0.2476 | 0.9234 |\n| NMI | 0.1871 | 0.8613 |\n| Silhouette Coefficient | 0.48667 | 0.590 |\n\n                                         Table 1: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2\n\nAs the scoring and the visualization indicates, kmeans clustering worked well in the dataset2. Kmeans need convex shaped dataset to work better. It works better on the convex shapes dataset. That is the major drawback of the kmeans clustering. As our dataset is non convex in dataset1 and kind of convex in dataset2, the kmeans performs well in the dataset2. It is the expected result.\n\nThough kmeans suffers when there is noise, that&#39;s the reason for not achieving state of the art score in our dataset 2. We would have got score close to 1 in ARI in dataset2 if we have less noise in dataset2.\n\nKmeans can&#39;t work to find the outliers.As we can see there is no outlier detected in our datasets.\n","b01d11a7":"* In the current age of data mining, there is a lot of unlabeled data available. This data are totally unstructured and maximum meaningless to naked eye. With new technologies, and the data increasing day by day, we can use clustering algorithms to cluster the data for classification. With huge size of data available totally unstructured unclassified, clustering is the best way to have first look in it.\n\n* In this report, we will be discussing the most popular algorithms for clustering algorithms and how to implement them in python. We will discuss about the ARI, NMI and Silhouette coefficient scores for the datasets and discuss about their clustering performances in visual manner.\n\n\n![](https:\/\/d2h0cx97tjks2p.cloudfront.net\/blogs\/wp-content\/uploads\/sites\/2\/2019\/08\/introduction-to-clustering.png)","382dd258":"# 3. AGNES\nFor agnes, I have used sklearn API. It is the most common type of hierarchical clustering used to group objects in clusters based on their similarity. I have tuned the hyper parameters with random search. It works in a &quot;Bottom Up&quot; manner.\n\n                      Scores of ARI, NMI and Silhouette coefficient for dataset1 and dataset 2:\n\n| Scoring Method | Dataset1 | Dataset2 |\n| --- | --- | --- |\n| ARI | 0.3767 | 0.9081 |\n| NMI | 0.3413 | 0.8427 |\n| Silhouette Coefficient | 0.4583 | 0.5878 |\n\n                      Table 3: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2\n\nAs we can see from the visualization and table, the Agnes performs less better than the partition clustering in kmeans and kmediod.\n\nSpecially in the dataset1,where the data is non convex, agnes fails to detect the right clusters. In dataset2, agnes performs well\n\nAgnes can&#39;t undo the previous steps, so if the connection is not good then it gives bad result.\n\nAgnes is very slow, with a large dataset it works very poor","1c82a5b0":"\n# References :\n\nT. Kanungo, D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman and A. Y. Wu, \"An efficient k-means clustering algorithm: analysis and implementation,\" in IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 24, no. 7, pp. 881-892, July 2002, doi: 10.1109\/TPAMI.2002.1017616.\n\nDanyang Cao and Bingru Yang, \"An improved k-medoids clustering algorithm,\" 2010 The 2nd International Conference on Computer and Automation Engineering (ICCAE), Singapore, 2010, pp. 132-135, doi: 10.1109\/ICCAE.2010.5452085.\n\nG. H. Shah, \"An improved DBSCAN, a density based clustering algorithm with parameter selection for high dimensional data sets,\" 2012 Nirma University International Conference on Engineering (NUiCONE), Ahmedabad, India, 2012, pp. 1-6, doi: 10.1109\/NUICONE.2012.6493211.","b035e6c9":"# Introduction\n","59399a59":"# 2. KMEDIOD:\nFor kmediod, I have implemented in from scratch. It is also a partition based clustering algorithm. I have tuned the hyper parameters with random search.\n\n                       Scores of ARI, NMI and Silhouette coefficient for dataset1 and dataset 2:\n\n| Scoring Method | Dataset1 | Dataset2 |\n| --- | --- | --- |\n| ARI | 0.3302 | 0.9254 |\n| NMI | 0.2540 | 0.8642 |\n| Silhouette Coefficient | 0.4863 | 0.5907 |\n\n                       Table 2: ARI , NMI, Silhouette Coefficient values for dataset1 and dataset2\n\nAs the visualization and scoring values indicate, kmediod performs well on the convex shaped dataset better than the non convex ones. In our datasets, kmediod works as like as kmeans clustering. It is basically the same algorithm as kmeans but the distance calculation is different. I used Eucleadean distance measurement method.\n\nBut in the matter of time complexity, kmediod takes a longer time than kmeans. It is a costly algorithm.\n\nAs kmediod is built on PAM,it is not suitable for clustering non-spherical (arbitrary shaped) groups of objects like kmeans.\n\nIt also cannot detect any outlier in the datasets.And we have to declare the cluster numbers before running the algorithm.\n\nSo the result we can see is expected characteristics of kmediod algorithm."}}