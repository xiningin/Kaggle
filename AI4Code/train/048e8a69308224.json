{"cell_type":{"81946dd6":"code","9227dedf":"code","e6c0d254":"code","cbd78529":"code","745cc6a9":"code","9435ceff":"code","e987ad81":"code","5541fa80":"code","92eca7c6":"code","d3af39fc":"code","4f484168":"code","00eeca5a":"code","d9b0c6a1":"code","e8ef4211":"code","84932e82":"code","1e275da4":"code","f723b24c":"code","d2fedd4c":"code","d100be48":"code","e4bb66d0":"code","a08b5500":"code","ba70d52d":"code","3b00e9ad":"code","71e793b3":"code","444073fe":"code","db9d6393":"code","b57bc7ac":"code","3b8d74c3":"code","e84c1381":"code","ef204adf":"code","19971bfa":"code","39c5c194":"code","b3961631":"code","9d2ac3f8":"code","8d8fea4e":"code","56ce770a":"code","23690668":"code","8406a5ef":"code","8c292b78":"code","08b23592":"code","e984106e":"code","e4221518":"code","b8fd1979":"code","b60f44d8":"code","1498b41e":"code","5e25fe12":"code","08be5be2":"code","84080e1f":"code","cf865a25":"code","3bb0d69d":"code","4b98f3ee":"code","cb865353":"code","d2001bd3":"code","47ae424f":"code","18eadd78":"code","4891b226":"code","17a950f8":"code","e93021fc":"code","a5211224":"code","60da31ba":"code","76816a69":"code","35c0961e":"code","f8b12516":"code","dc3b3f91":"code","af1a923a":"code","c324f23d":"code","2a5218b3":"code","479a92ed":"code","e0e0f118":"code","993163b7":"code","68315257":"code","0633f0d8":"code","90cd4ae1":"code","884c1272":"code","33b7168b":"code","621273e4":"code","d0572b8d":"code","55fb8a9a":"code","54c722ee":"code","baef2c5b":"code","110e0e9b":"code","0d6297ed":"code","22a426dd":"code","1d1f616e":"code","44ec61a0":"code","6f02f9f4":"code","28c15438":"code","97347feb":"code","c545b0c8":"code","73fa5237":"code","97cb4020":"code","1aff572b":"code","bb04067e":"code","1748f082":"code","b5a4481f":"code","d82dd2d7":"code","315f4590":"code","a9e085a3":"code","235d92a3":"code","7b3b90c5":"code","c7bf5148":"code","21045bdb":"code","c2552946":"code","e7ebf739":"code","2c6f6daa":"code","e0174de3":"code","48b60f55":"code","31cb7f0e":"code","316188fc":"code","45034457":"code","b84ba270":"code","6122bd29":"code","3570216b":"code","0c167919":"code","b3d82faa":"code","cc9b3eff":"code","dd245134":"code","082e0dd2":"code","5da97e6e":"code","12ed3416":"code","7bd3ac10":"code","d129a513":"code","9d912560":"code","b2d16bd2":"code","45932eaa":"code","6c3b75b0":"code","99825e68":"code","195f96f7":"code","6adf1190":"code","4c4b749e":"code","908a76d3":"code","caeebccd":"code","3c9f568d":"code","56e1424e":"code","f2d5b41d":"code","b6483899":"code","74934ddf":"code","c7bcf2b3":"code","2a07af72":"code","ddcad7f5":"code","ecbce0ad":"code","250b98ee":"code","1c31df40":"code","d76abe78":"code","9837fac8":"code","c97de48f":"code","ee643d7e":"code","31bd1e01":"code","334fd594":"code","2351a498":"code","91fb7374":"code","3a9a40d5":"code","f002aeb9":"code","301b92a7":"code","99864640":"code","bca279a7":"code","fb16809c":"code","4bd634fb":"code","7c554744":"code","c0e0bc79":"code","ae38da90":"code","90477d85":"code","3f3299df":"code","196fde64":"code","10e654fa":"code","c44cdbf9":"code","893cf7ea":"code","9663bac0":"code","98143ae5":"code","b9bc55f1":"code","117d9852":"code","b3dbc4a7":"code","8370e64d":"code","e94d33e0":"code","c156a896":"code","0e387d26":"code","79c6ae51":"code","b9ce93cd":"code","124a72d8":"markdown","c93bbf7f":"markdown","074d4e5c":"markdown","8f160590":"markdown","af8ad4a5":"markdown","cc400cfc":"markdown","f26c17b8":"markdown","a2440eb2":"markdown","ca6d4b90":"markdown","01845561":"markdown","9a6b5944":"markdown","dfbd6897":"markdown","8285f1ee":"markdown","085eb166":"markdown","fc03654b":"markdown","1d55f1bc":"markdown","6be086d9":"markdown","8972a482":"markdown","fa5966bc":"markdown","9a2189dd":"markdown","7ee403b3":"markdown","33ba872d":"markdown","b6833fe3":"markdown","0ed47b88":"markdown","8d3098c7":"markdown","6e44bbfc":"markdown","c99990d9":"markdown","9160feba":"markdown","c71b2be6":"markdown","38e84610":"markdown","93c46ab9":"markdown","b9ab8fa8":"markdown","9fb8850d":"markdown","f98df6af":"markdown","8889f0a9":"markdown","11b82c76":"markdown","e95ddf8f":"markdown","7433f4fb":"markdown","f603308e":"markdown","61bca085":"markdown","fb401b93":"markdown","4a7bd157":"markdown","b77e41ab":"markdown","80d098ef":"markdown","6a8c4fcb":"markdown","1963d214":"markdown","c19ee60f":"markdown","809fd7c2":"markdown","3033f988":"markdown","075ee133":"markdown","b0677815":"markdown","75b3470f":"markdown","8069adf0":"markdown","f3e4a9bc":"markdown","23f57232":"markdown","f644c3eb":"markdown","ed20c062":"markdown","4781c6d8":"markdown","ee50feea":"markdown","a01b0bf0":"markdown","57ce1cb5":"markdown","eabc914e":"markdown","39c5ca9d":"markdown","6e2040a5":"markdown","19a66125":"markdown","56247dbe":"markdown","bc494259":"markdown","a5c1b5b6":"markdown","b93175b5":"markdown","845caa9a":"markdown","bbdea863":"markdown","2bae666b":"markdown","ad2aa8ad":"markdown","a950cfee":"markdown","c1b50ac3":"markdown","18288dc5":"markdown","b0d241a3":"markdown","741dc7c1":"markdown","119a43d9":"markdown","b0a0b3d4":"markdown","df152e84":"markdown","080a8ba0":"markdown","ea05983e":"markdown","a03e88d1":"markdown","e66b9249":"markdown","195e679a":"markdown","9347b97d":"markdown","db6f8b39":"markdown","f9fe0f49":"markdown","50a0b0b4":"markdown","9b531d53":"markdown","600e54e8":"markdown","b3d35274":"markdown","07a569a9":"markdown","a0ac9aa4":"markdown","5a7b99b0":"markdown","30d50122":"markdown","9dfbb4e0":"markdown","27404953":"markdown","917d622a":"markdown","91a4cb8e":"markdown","1d0b4d77":"markdown","a884193a":"markdown","6c809c1c":"markdown","23a550dc":"markdown","9c587025":"markdown","6d27276a":"markdown","80bf8074":"markdown","75fc0a6c":"markdown","c027618a":"markdown","63f450da":"markdown","7677a072":"markdown","4aaf00a2":"markdown","860092a0":"markdown","23e713c5":"markdown","a2047cff":"markdown","8a707990":"markdown","e4ced2f9":"markdown","6e9f3bef":"markdown","c0722e52":"markdown","c6e9067f":"markdown","6d3ef3fa":"markdown","975e61f3":"markdown","b1bb381b":"markdown","c7fe5eb1":"markdown","a8c86533":"markdown","50a84c45":"markdown","795db49f":"markdown","c4782ca6":"markdown","b8129343":"markdown","9c1a148b":"markdown","9bf11555":"markdown","0ab01c88":"markdown","606e2a25":"markdown","27e6acd9":"markdown","687dac08":"markdown","75390197":"markdown","2f49840b":"markdown","cf277180":"markdown","c9e4d41a":"markdown","eb722e70":"markdown","a6dc0765":"markdown","378839d4":"markdown","ccd722f5":"markdown","51c17584":"markdown","80570907":"markdown","86d5e486":"markdown","262c5ed6":"markdown","01b8cbea":"markdown","16a1c556":"markdown","bdfdf7fa":"markdown","88d39251":"markdown","59c0a59f":"markdown","407de3f2":"markdown","25dc9a16":"markdown","69aab427":"markdown","5423ad8f":"markdown","c00474b8":"markdown","f5ca2238":"markdown","81ed33b3":"markdown","16ec3cce":"markdown","057dca5d":"markdown","8ab15cdc":"markdown","6f663f21":"markdown","9fd5aa4e":"markdown","6b0b7f50":"markdown","c4be2d72":"markdown","92e07adb":"markdown","7a96c7d2":"markdown","d8f11d8f":"markdown","70558a24":"markdown","4f60cd25":"markdown"},"source":{"81946dd6":"import numpy as np\nnp.random.seed(7)","9227dedf":"import tensorflow as tf\ntf.__version__","e6c0d254":"import pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\n\nimport random, re\nimport time\n\n# used to supress display of warnings\nimport warnings\n\nimport missingno as mno\n\n# nlp libraries\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\n\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport holoviews as hv\nfrom holoviews import opts\n\nimport os;\nfrom os import makedirs\n\n# sampling methods\nfrom sklearn.utils import resample\nfrom imblearn.over_sampling import SMOTE\n\n# import zscore for scaling the data\nfrom scipy.stats import zscore\n\nfrom scipy.stats import randint as sp_randint\n\n# save models\nimport pickle\n\n# pre-processing methods\nfrom sklearn.model_selection import train_test_split\n\n# the classification models \nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import RidgeClassifier\nfrom sklearn.linear_model import Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\n# ensemble models\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\n\n# methods and classes for evaluation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score, roc_auc_score\n\n# cross-validation methods\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\n# feature selection methods\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import f_classif\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\n\n# pre-processing methods\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import LabelEncoder\n\n# Deep learning libraries\nfrom keras.utils import np_utils\nfrom keras.utils import plot_model\nfrom keras.layers import Input\nfrom keras.layers.merge import Concatenate\nfrom keras.optimizers import SGD\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras import optimizers\nfrom keras.models import Model\nfrom tensorflow.keras.layers import Flatten, Activation, Dense, LSTM, BatchNormalization, Embedding, Dropout, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import model_from_json\nfrom keras.regularizers import l1, l2, l1_l2\nfrom keras.constraints import maxnorm, min_max_norm\nfrom keras.constraints import unit_norm\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ModelCheckpoint\nfrom keras.models import model_from_json\n\nfrom keras.models import load_model\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\n# Keras pre-processing\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","cbd78529":"os.environ['HV_DOC_HTML'] = 'true'\n\ndef _render(self, **kw):\n  hv.extension('bokeh')\n  return hv.Store.render(self)\nhv.core.Dimensioned._repr_mimebundle_ = _render","745cc6a9":"os.environ['PYTHONHASHSEED']=str(7)\n\n# Reproduce the results\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(7)\n   #np.random.seed(7)\n   #random.seed(7)\n   tf.random.set_seed(7)\n\n#random_state = 42\n#np.random.seed(random_state)\n#tf.random.set_seed(random_state)\n\n!rm -R log\/","9435ceff":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","e987ad81":"# Read IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv file\nindustry_df = pd.read_csv(\"..\/input\/industrial-safety-and-health-analytics-database\/IHMStefanini_industrial_safety_and_health_database_with_accidents_description.csv\")","5541fa80":"# Get the top 5 rows\ndisplay(industry_df.head())","92eca7c6":"print(\"Number of rows = {0} and Number of Columns = {1} in the Data frame\".format(industry_df.shape[0], industry_df.shape[1]))","d3af39fc":"# Check datatypes\nindustry_df.dtypes","4f484168":"# Check Data frame info\nindustry_df.info()","00eeca5a":"# Column names of Data frame\nindustry_df.columns","d9b0c6a1":"# Remove 'Unnamed: 0' column from Data frame\nindustry_df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n\n# Rename 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns in Data frame\nindustry_df.rename(columns={'Data':'Date', 'Countries':'Country', 'Genre':'Gender', 'Employee or Third Party':'Employee type'}, inplace=True)\n\n# Get the top 2 rows\nindustry_df.head(2)","e8ef4211":"# Check duplicates in a data frame\nindustry_df.duplicated().sum()","84932e82":"# View the duplicate records\nduplicates = industry_df.duplicated()\n\nindustry_df[duplicates]","1e275da4":"# Delete duplicate rows\nindustry_df.drop_duplicates(inplace=True)","f723b24c":"# Get the shape of Industry data\nindustry_df.shape","d2fedd4c":"print(\"Number of rows = {0} and Number of Columns = {1} in the Data frame after removing the duplicates.\".format(industry_df.shape[0], industry_df.shape[1]))","d100be48":"# Check unique values of all columns except 'Description' column\nfor x in industry_df.columns:\n    if x != 'Description':\n      print('--'*30); print(f'Unique values of \"{x}\" column'); print('--'*30)\n      print(industry_df[x].unique())\n      print('\\n')","e4bb66d0":"# Check the presence of missing values\nindustry_df.isnull().sum()","a08b5500":"# Visualize missing values\nmno.matrix(industry_df, figsize = (20, 6));","ba70d52d":"industry_df['Date'] = pd.to_datetime(industry_df['Date'])\n\nindustry_df['Year'] = industry_df.Date.apply(lambda x : x.year)\nindustry_df['Month'] = industry_df.Date.apply(lambda x : x.month)\nindustry_df['Day'] = industry_df.Date.apply(lambda x : x.day)\nindustry_df['Weekday'] = industry_df.Date.apply(lambda x : x.day_name())\nindustry_df['WeekofYear'] = industry_df.Date.apply(lambda x : x.weekofyear)\n\nindustry_df.head()","3b00e9ad":"# function to create month variable into seasons\ndef month2seasons(x):\n    if x in [9, 10, 11]:\n        season = 'Spring'\n    elif x in [12, 1, 2]:\n        season = 'Summer'\n    elif x in [3, 4, 5]:\n        season = 'Autumn'\n    elif x in [6, 7, 8]:\n        season = 'Winter'\n    return season","71e793b3":"industry_df['Season'] = industry_df['Month'].apply(month2seasons)\nindustry_df.head(3)","444073fe":"import holidays\n\nbrazil_holidays = []\n\nprint('--'*40); print('List of Brazil holidays in 2016'); print('--'*40)\nfor date in holidays.Brazil(years = 2016).items():\n    brazil_holidays.append(str(date[0]))\n    print(date)\n\nprint('--'*40); print('List of Brazil holidays in 2017'); print('--'*40)\nfor date in holidays.Brazil(years = 2017).items():\n    brazil_holidays.append(str(date[0]))\n    print(date)","db9d6393":"industry_df['Is_Holiday'] = [1 if str(val).split()[0] in brazil_holidays else 0 for val in industry_df['Date']]\nindustry_df.head(3)","b57bc7ac":"print('--'*30); print('Value Counts for `Country` label'); print('--'*30)\n\ntotal_row_cnt = industry_df.shape[0]\ncountry_01_cnt = industry_df[industry_df.Country == 'Country_01'].shape[0]\ncountry_02_cnt = industry_df[industry_df.Country == 'Country_02'].shape[0]\ncountry_03_cnt = industry_df[industry_df.Country == 'Country_03'].shape[0]\n\nprint(f'Country_01 count: {country_01_cnt} i.e. {round(country_01_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Country_02 count: {country_02_cnt} i.e. {round(country_02_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Country_03 count: {country_03_cnt} i.e. {round(country_03_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Country` label'); print('--'*30)\n_ = industry_df['Country'].value_counts().plot(kind = 'pie', autopct = '%.0f%%', labels = ['Country_01', 'Country_02', 'Country_03'], figsize = (10, 6))","3b8d74c3":"local_cnt = np.round(industry_df['Local'].value_counts(normalize=True) * 100)\n\nhv.extension('bokeh')\nhv.Bars(local_cnt).opts(title=\"Local Count\", color=\"#8888ff\", xlabel=\"Locals\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=700, height=300,tools=['hover'],show_grid=True))","e84c1381":"print('--'*30); print('Value Counts for `Industry Sector` label'); print('--'*30)\n\nMining_cnt = industry_df[industry_df['Industry Sector'] == 'Mining'].shape[0]\nMetals_cnt = industry_df[industry_df['Industry Sector'] == 'Metals'].shape[0]\nOthers_cnt = industry_df[industry_df['Industry Sector'] == 'Others'].shape[0]\n\nprint(f'Mining count: {Mining_cnt} i.e. {round(Mining_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Metals count: {Metals_cnt} i.e. {round(Metals_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Others count: {Others_cnt} i.e. {round(Others_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Industry Sector` label'); print('--'*30)\n\nsector_cnt = np.round(industry_df['Industry Sector'].value_counts(normalize=True) * 100)\n\nhv.Bars(sector_cnt).opts(title=\"Industry Sector Count\", color=\"#8888ff\", xlabel=\"Sectors\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))\\\n                * hv.Text('Mining', 15, f\"{int(sector_cnt.loc['Mining'])}%\")\\\n                * hv.Text('Metals', 15, f\"{int(sector_cnt.loc['Metals'])}%\")\\\n                * hv.Text('Others', 15, f\"{int(sector_cnt.loc['Others'])}%\")","ef204adf":"print('--'*30); print('Value Counts for `Accident Level` label'); print('--'*40)\n\nI_acc_cnt = industry_df[industry_df['Accident Level'] == 'I'].shape[0]\nII_acc_cnt = industry_df[industry_df['Accident Level'] == 'II'].shape[0]\nIII_acc_cnt = industry_df[industry_df['Accident Level'] == 'III'].shape[0]\nIV_acc_cnt = industry_df[industry_df['Accident Level'] == 'IV'].shape[0]\nV_acc_cnt = industry_df[industry_df['Accident Level'] == 'V'].shape[0]\nVI_acc_cnt = industry_df[industry_df['Accident Level'] == 'VI'].shape[0]\n\nprint(f'Accident Level - I count: {I_acc_cnt} i.e. {round(I_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - II count: {II_acc_cnt} i.e. {round(II_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - III count: {III_acc_cnt} i.e. {round(III_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - IV count: {IV_acc_cnt} i.e. {round(IV_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - V count: {V_acc_cnt} i.e. {round(V_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Accident Level - VI count: {VI_acc_cnt} i.e. {round(VI_acc_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Value Counts for `Potential Accident Level'); print('--'*40)\n\nI_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'I'].shape[0]\nII_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'II'].shape[0]\nIII_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'III'].shape[0]\nIV_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'IV'].shape[0]\nV_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'V'].shape[0]\nVI_pot_acc_cnt = industry_df[industry_df['Potential Accident Level'] == 'VI'].shape[0]\n\nprint(f'Potential Accident Level - I count: {I_pot_acc_cnt} i.e. {round(I_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - II count: {II_pot_acc_cnt} i.e. {round(II_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - III count: {III_pot_acc_cnt} i.e. {round(III_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - IV count: {IV_pot_acc_cnt} i.e. {round(IV_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - V count: {V_pot_acc_cnt} i.e. {round(V_pot_acc_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Potential Accident Level - VI count: {VI_pot_acc_cnt} i.e. {round(VI_pot_acc_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Accident Level` & `Potential Accident Level` label'); print('--'*40)\n\nac_level_cnt = np.round(industry_df['Accident Level'].value_counts(normalize=True) * 100)\npot_ac_level_cnt = np.round(industry_df['Potential Accident Level'].value_counts(normalize=True) * 100, decimals=1)\nac_pot = pd.concat([ac_level_cnt, pot_ac_level_cnt], axis=1,sort=False).fillna(0).rename(columns={'Accident Level':'Accident', 'Potential Accident Level':'Potential'})\nac_pot = pd.melt(ac_pot.reset_index(), ['index']).rename(columns={'index':'Severity', 'variable':'Levels'})\n\nhv.Bars(ac_pot, ['Severity', 'Levels'], 'value').opts(opts.Bars(title=\"Accident Levels Count\", width=700, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=45, ylabel=\"Percentage\", yformatter='%d%%'))","19971bfa":"print('--'*30); print('Value Counts for `Gender` label'); print('--'*30)\n\nMale_cnt = industry_df[industry_df['Gender'] == 'Male'].shape[0]\nFemale_cnt = industry_df[industry_df['Gender'] == 'Female'].shape[0]\n\nprint(f'Male count: {Male_cnt} i.e. {round(Male_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Female count: {Female_cnt} i.e. {round(Female_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Gender` label'); print('--'*30)\n\ngender_cnt = np.round(industry_df['Gender'].value_counts(normalize=True) * 100)\n\nhv.Bars(gender_cnt).opts(title=\"Gender Count\", color=\"#8888ff\", xlabel=\"Gender\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","39c5c194":"print('--'*30); print('Value Counts for `Employee type` label'); print('--'*30)\n\nthird_party_cnt = industry_df[industry_df['Employee type'] == 'Third Party'].shape[0]\nemp_cnt = industry_df[industry_df['Employee type'] == 'Employee'].shape[0]\nthird_rem_cnt = industry_df[industry_df['Employee type'] == 'Third Party (Remote)'].shape[0]\n\nprint(f'Third Party count: {third_party_cnt} i.e. {round(third_party_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Employee count: {emp_cnt} i.e. {round(emp_cnt\/total_row_cnt*100, 0)}%')\nprint(f'Third Party (Remote) count: {third_rem_cnt} i.e. {round(third_rem_cnt\/total_row_cnt*100, 0)}%')\n\nprint('--'*30); print('Distributon of `Employee type` label'); print('--'*30)\n\nemp_type_cnt = np.round(industry_df['Employee type'].value_counts(normalize=True) * 100)\n\nhv.Bars(emp_type_cnt).opts(title=\"Employee type Count\", color=\"#8888ff\", xlabel=\"Employee Type\", ylabel=\"Percentage\", yformatter='%d%%')\\\n                .opts(opts.Bars(width=500, height=300,tools=['hover'],show_grid=True))","b3961631":"cr_risk_cnt = np.round(industry_df['Critical Risk'].value_counts(normalize=True) * 100)\n\nhv.Bars(cr_risk_cnt[::-1]).opts(title=\"Critical Risk Count\", color=\"#8888ff\", xlabel=\"Critical Risks\", ylabel=\"Percentage\", xformatter='%d%%')\\\n                .opts(opts.Bars(width=600, height=600,tools=['hover'],show_grid=True,invert_axes=True))","9d2ac3f8":"year_cnt = np.round(industry_df['Year'].value_counts(normalize=True,sort=False) * 100)\nyear = hv.Bars(year_cnt).opts(title=\"Year Count\", color=\"yellow\", xlabel=\"Years\")\n\nmonth_cnt = np.round(industry_df['Month'].value_counts(normalize=True,sort=False) * 100)\nmonth = hv.Bars(month_cnt).opts(title=\"Month Count\", color=\"#8888ff\", xlabel=\"Months\") * hv.Curve(month_cnt).opts(color='red', line_width=3)\n\n(year + month).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(2)","8d8fea4e":"day_cnt = np.round(industry_df['Day'].value_counts(normalize=True,sort=False) * 100)\nhv.Bars(day_cnt).opts(title=\"Day Count\", color=\"#8888ff\", xlabel=\"Days\") * hv.Curve(day_cnt).opts(width=500, height=300, color='red', line_width=3)","56ce770a":"weekday_cnt = pd.DataFrame(np.round(industry_df['Weekday'].value_counts(normalize=True,sort=False) * 100))\nweekday_cnt['week_num'] = [['aMonday', 'bTuesday', 'cWednesday', 'dThursday', 'eFriday', 'fSaturday', 'gSunday'].index(i) for i in weekday_cnt.index]\nweekday_cnt.sort_values('week_num', inplace=True)\n\nhv.Bars((weekday_cnt.index, weekday_cnt.Weekday)).opts(title=\"Weekday Count\", color=\"#8888ff\", xlabel=\"Weekdays\") * hv.Curve(weekday_cnt['Weekday']).opts(width=500, height=300, color='red', line_width=3)\n#(day + weekday).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(2)","23690668":"# Check the proportion of Industry sector in different countries\nindsec_cntry_table = pd.crosstab(index = industry_df['Industry Sector'], columns = industry_df['Country'])\nindsec_cntry_table.plot(kind = 'bar', figsize=(8,8), stacked = True)\nplt.title(\"Proportion of Industry Sector in different countries\")\nplt.show()","8406a5ef":"mining_country1 = industry_df[industry_df['Industry Sector'] == 'Mining']['Country'].value_counts()[0]\nmining_country2 = industry_df[industry_df['Industry Sector'] == 'Mining']['Country'].value_counts()[1]\n\nmetals_country1 = industry_df[industry_df['Industry Sector'] == 'Metals']['Country'].value_counts()[1]\nmetals_country2 = industry_df[industry_df['Industry Sector'] == 'Metals']['Country'].value_counts()[0]\n\nothers_country1 = industry_df[industry_df['Industry Sector'] == 'Others']['Country'].value_counts()[2]\nothers_country2 = industry_df[industry_df['Industry Sector'] == 'Others']['Country'].value_counts()[1]\nothers_country3 = industry_df[industry_df['Industry Sector'] == 'Others']['Country'].value_counts()[0]\n\nprint([mining_country1, metals_country1, others_country1], [country_01_cnt])\nprint(f'Proportions of mining, metals, others in country_01 = {round(200\/248,2)}%, {round(46\/248,2)}%, {round(2\/248,2)}% respectively')","8c292b78":"from statsmodels.stats.proportion import proportions_ztest\n\n# Z-test proportions: More than 2 samples not implemented yet, hence I am passing two elements\nt_statistic, p_value = proportions_ztest([mining_country1, metals_country1], [country_01_cnt])\n\nprint(\"Mining and Metals t_statistic\", t_statistic)\nprint(\"Mining and Metals p_value\", p_value)\n\nt_statistic, p_value = proportions_ztest([mining_country1, others_country1], [country_01_cnt])\n\nprint(\"Mining and Others t_statistic\", t_statistic)\nprint(\"Mining and Others p_value\", p_value)","08b23592":"reject_null = False\nif p_value < 0.05:\n    reject_null = True \nelse: \n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","e984106e":"f = lambda x : np.round(x\/x.sum() * 100)\nem_gen = industry_df.groupby(['Gender','Employee type'])['Employee type'].count().unstack().apply(f, axis=1)\n\nhv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Employee type'], 'value').opts(opts.Bars(title=\"Employee type by Gender Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","e4221518":"male_emp = industry_df[industry_df['Employee type'] == 'Employee'].Gender.value_counts()[0]\nfemale_emp = industry_df[industry_df['Employee type'] == 'Employee'].Gender.value_counts()[1]\n\nprint([male_emp, female_emp], [Male_cnt, Female_cnt])\nprint(f'Proportion of own employee types in male, female = {round(170\/396,2)}%, {round(8\/22,2)}% respectively')","b8fd1979":"t_statistic, p_value = proportions_ztest([male_emp, female_emp], [Male_cnt, Female_cnt])\n\nprint(\"t_statistic\", t_statistic)\nprint(\"p_value\", p_value)","b60f44d8":"reject_null = False\nif p_value < 0.05:\n    reject_null = True \nelse: \n    reject_null = False\n    \nprint(\"reject null? : \" + str(reject_null))","1498b41e":"f = lambda x : np.round(x\/x.sum() * 100)\nem_gen = industry_df.groupby(['Gender','Industry Sector'])['Industry Sector'].count().unstack().apply(f, axis=1)\n\nhv.Bars(pd.melt(em_gen.reset_index(), ['Gender']), ['Gender','Industry Sector'], 'value').opts(opts.Bars(title=\"Industry Sector by Gender Count\", width=800, height=300,tools=['hover'],\\\n                                                                show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","5e25fe12":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_gen = industry_df.groupby(['Gender','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_gen.reset_index(), ['Gender']), ['Gender','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Gender Count\"))\n\npot_ac_gen = industry_df.groupby(['Gender','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_gen.reset_index(), ['Gender']), ['Gender','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Gender Count\"))\n\n(ac + pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%'))","08be5be2":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_em = industry_df.groupby(['Employee type','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1)\nac = hv.Bars(pd.melt(ac_em.reset_index(), ['Employee type']), ['Employee type','Accident Level'], 'value').opts(opts.Bars(title=\"Accident Level by Employee type Count\"))\n\npot_ac_em = industry_df.groupby(['Employee type','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1)\npot_ac = hv.Bars(pd.melt(pot_ac_em.reset_index(), ['Employee type']), ['Employee type','Potential Accident Level'], 'value').opts(opts.Bars(title=\"Potential Accident Level by Employee type Count\"))\n\n(ac + pot_ac).opts(opts.Bars(width=400, height=300,tools=['hover'],show_grid=True,xrotation=0, ylabel=\"Percentage\", yformatter='%d%%',fontsize={'title':9}))","84080e1f":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_mo = industry_df.groupby(['Month','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac = hv.Curve(ac_mo['I'], label='I') * hv.Curve(ac_mo['II'], label='II') * hv.Curve(ac_mo['III'], label='III') * hv.Curve(ac_mo['IV'], label='IV') * hv.Curve(ac_mo['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Month Count\"))\n\npot_ac_mo = industry_df.groupby(['Month','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\npot_ac = hv.Curve(pot_ac_mo['I'], label='I') * hv.Curve(pot_ac_mo['II'], label='II') * hv.Curve(pot_ac_mo['III'], label='III') * hv.Curve(pot_ac_mo['IV'], label='IV')\\\n        * hv.Curve(pot_ac_mo['V'], label='V') * hv.Curve(pot_ac_mo['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Month Count\"))\n        \n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","cf865a25":"f = lambda x : np.round(x\/x.sum() * 100)\n\nac_weekday = industry_df.groupby(['Weekday','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac_weekday['week_num'] = [['aMonday', 'bTuesday', 'cWednesday', 'dThursday', 'eFriday', 'fSaturday', 'gSunday'].index(i) for i in ac_weekday.index]\nac_weekday.sort_values('week_num', inplace=True)\nac_weekday.drop('week_num', axis=1, inplace=True)\nac = hv.Curve(ac_weekday['I'], label='I') * hv.Curve(ac_weekday['II'], label='II') * hv.Curve(ac_weekday['III'], label='III') * hv.Curve(ac_weekday['IV'], label='IV') * hv.Curve(ac_weekday['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Weekday Count\"))\n\npot_ac_weekday = industry_df.groupby(['Weekday','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=0).fillna(0)\npot_ac_weekday['week_num'] = [['aMonday', 'bTuesday', 'cWednesday', 'dThursday', 'eFriday', 'fSaturday', 'gSunday'].index(i) for i in pot_ac_weekday.index]\npot_ac_weekday.sort_values('week_num', inplace=True)\npot_ac_weekday.drop('week_num', axis=1, inplace=True)\npot_ac = hv.Curve(pot_ac_weekday['I'], label='I') * hv.Curve(pot_ac_weekday['II'], label='II') * hv.Curve(pot_ac_weekday['III'], label='III') * hv.Curve(pot_ac_weekday['IV'], label='IV')\\\n        * hv.Curve(pot_ac_weekday['V'], label='V') * hv.Curve(pot_ac_weekday['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Weekday Count\"))\n\n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","3bb0d69d":"f = lambda x : np.round(x\/x.sum() * 100)\nac_season = industry_df.groupby(['Season','Accident Level'])['Accident Level'].count().unstack().apply(f, axis=1).fillna(0)\nac_season['season_num'] = [['dSpring', 'aSummer', 'bAutumn', 'cWinter'].index(i) for i in ac_season.index]\nac_season.sort_values('season_num', inplace=True)\nac_season.drop('season_num', axis=1, inplace=True)\nac = hv.Curve(ac_season['I'], label='I') * hv.Curve(ac_season['II'], label='II') * hv.Curve(ac_season['III'], label='III') * hv.Curve(ac_season['IV'], label='IV') * hv.Curve(ac_season['V'], label='V')\\\n        .opts(opts.Curve(title=\"Accident Level by Season Count\"))\n\npot_ac_season = industry_df.groupby(['Season','Potential Accident Level'])['Potential Accident Level'].count().unstack().apply(f, axis=0).fillna(0)\npot_ac_season['season_num'] = [['dSpring', 'aSummer', 'bAutumn', 'cWinter'].index(i) for i in pot_ac_season.index]\npot_ac_season.sort_values('season_num', inplace=True)\npot_ac_season.drop('season_num', axis=1, inplace=True)\npot_ac = hv.Curve(pot_ac_season['I'], label='I') * hv.Curve(pot_ac_season['II'], label='II') * hv.Curve(pot_ac_season['III'], label='III') * hv.Curve(pot_ac_season['IV'], label='IV')\\\n        * hv.Curve(pot_ac_season['V'], label='V') * hv.Curve(pot_ac_season['VI'], label='VI').opts(opts.Curve(title=\"Potential Accident Level by Season Count\"))\n\n(ac+pot_ac).opts(opts.Curve(width=800, height=300,tools=['hover'],show_grid=True, ylabel=\"Percentage\", yformatter='%d%%')).cols(1)","4b98f3ee":"# Summary statistics\nindustry_df.drop(columns='Description').describe(exclude=[np.number]).T","cb865353":"# Check the Correlation\nindustry_df.corr()","d2001bd3":"# Checking 5 random Description and accident_levels from the data\nprint('--'*35); print('Checking 5 random Descriptions and accident_levels from the data'); print('--'*35)\nrands = random.sample(range(1, industry_df.shape[0]), 5)\ndescriptions, accident_levels = list(industry_df.loc[rands, 'Description']), list(industry_df.loc[rands, 'Accident Level'])\n\n_ = [print(f'Description: {description}\\naccident_level: {acclevel}\\n') for description, acclevel in zip(descriptions, accident_levels)]","47ae424f":"# Checking 5 random Descriptions and accident_levels from the data where the length of headline is > 100\nprint('--'*55); print('Checking 5 random Descriptions and accident_levels from the data where the length of Description is > 100'); print('--'*55)\nindexes = list(industry_df.loc[industry_df['Description'].str.len() > 100, 'Description'].index)\nrands = random.sample(indexes, 5)\ndescriptions, accident_levels = list(industry_df.loc[rands, 'Description']), list(industry_df.loc[rands, 'Accident Level'])\n\n_ = [print(f'Description: {description}\\naccident_level: {acclevel}\\n') for description, acclevel in zip(descriptions, accident_levels)]\n\nprint('--'*40); print('Distributon of accident_level where the length of Description is > 100'); print('--'*40)\n_ = industry_df.loc[indexes, 'Accident Level'].value_counts().plot(kind = 'pie', autopct = '%.0f%%', labels = ['I', 'II', 'III', 'IV', 'V'], figsize = (10, 6))","18eadd78":"# Checking 5 random Descriptions and pot_accident_levels from the data where the length of headline is > 100\nprint('--'*60); print('Checking 5 random Descriptions and pot_accident_levels from the data where the length of Description is > 100'); print('--'*60)\nindexes = list(industry_df.loc[industry_df['Description'].str.len() > 100, 'Description'].index)\nrands = random.sample(indexes, 5)\ndescriptions, pot_accident_levels = list(industry_df.loc[rands, 'Description']), list(industry_df.loc[rands, 'Potential Accident Level'])\n\n_ = [print(f'Description: {descriptin}\\npot_accident_level: {pot_acclevel}\\n') for descriptin, pot_acclevel in zip(descriptions, pot_accident_levels)]\n\nprint('--'*40); print('Distributon of pot_accident_level where the length of Description is > 100'); print('--'*40)\n_ = industry_df.loc[indexes, 'Potential Accident Level'].value_counts().plot(kind = 'pie', autopct = '%.0f%%', labels = ['IV', 'III', 'II', 'I', 'V', 'VI'], figsize = (10, 6))","4891b226":"# Text preprocessing and stopwords\nfrom text_preprocess_py import * #(custom module)","17a950f8":"print('--'*30); print('Converting description to lower case')\nindustry_df['Cleaned_Description'] = industry_df['Description'].apply(lambda x : x.lower())\n\nprint('Replacing apostrophes to the standard lexicons')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x : replace_words(x))\n\nprint('Removing punctuations')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: remove_punctuation(x))\n\nprint('Applying Lemmatizer')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: lemmatize(x))\n\nprint('Removing multiple spaces between words')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: re.sub(' +', ' ', x))\n\nprint('Removing stop words')\nindustry_df['Cleaned_Description'] = industry_df['Cleaned_Description'].apply(lambda x: remove_stopwords(x))\n\nprint('--'*30)","e93021fc":"print('--'*45); print('Get the length of each line, find the maximum length and print the maximum length line'); \nprint('Length of line ranges from 64 to 672.'); print('--'*45)\n\n# Get length of each line\nindustry_df['line_length'] = industry_df['Cleaned_Description'].str.len()\n\nprint('Minimum line length: {}'.format(industry_df['line_length'].min()))\nprint('Maximum line length: {}'.format(industry_df['line_length'].max()))\nprint('Line with maximum length: {}'.format(industry_df[industry_df['line_length'] == industry_df['line_length'].max()]['Cleaned_Description'].values[0]))","a5211224":"print('--'*45); print('Get the number of words, find the maximum number of words and print the maximum number of words'); \nprint('Number of words ranges from 10 to 98.'); print('--'*45)\n\n# Get length of each line\nindustry_df['nb_words'] = industry_df['Cleaned_Description'].apply(lambda x: len(x.split(' ')))\n\nprint('Minimum number of words: {}'.format(industry_df['nb_words'].min()))\nprint('Maximum number of words: {}'.format(industry_df['nb_words'].max()))\nprint('Line with maximum number of words: {}'.format(industry_df[industry_df['nb_words'] == industry_df['nb_words'].max()]['Cleaned_Description'].values[0]))","60da31ba":"wordcloud = WordCloud(width = 1500, height = 800, random_state=0, background_color='black', colormap='rainbow', \\\n                      min_font_size=5, max_words=300, collocations=False).generate(\" \".join(industry_df['Cleaned_Description'].values))\nplt.figure(figsize=(15,10))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","76816a69":"print('--'*30); print('Five point summary for number of words')\ndisplay(industry_df['nb_words'].describe().round(0).astype(int)); \n\nprint('99% quantilie: {}'.format(industry_df['nb_words'].quantile(0.99)));print('--'*30)","35c0961e":"from gensim.models import Word2Vec\n# define training data\nsentences = industry_df['Cleaned_Description']\n\n# train model\nmodel = Word2Vec(sentences, min_count=1)\n\n# summarize the loaded model\nprint(model)\n\n# summarize vocabulary\nwords = list(model.wv.index_to_key)\nprint(words)\n\n# save model\nmodel.save('model.bin')\n\n# load model\nnew_model = Word2Vec.load('model.bin')\nprint(new_model)","f8b12516":"embeddings_index = {}\nEMBEDDING_FILE = '..\/input\/glove6b200d\/glove.6B.200d.txt'\nf = open(EMBEDDING_FILE)\nfor line in tqdm(f):\n    values = line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","dc3b3f91":"# this function creates a normalized vector for the whole sentence\ndef sent2vec(s):\n    words = str(s).lower()\n    words = word_tokenize(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    M = []\n    for w in words:\n        try:\n            M.append(embeddings_index[w])\n        except:\n            continue\n    M = np.array(M)\n    v = M.sum(axis=0)\n    if type(v) != np.ndarray:\n        return np.zeros(300)\n    return v \/ np.sqrt((v ** 2).sum())","af1a923a":"# create sentence GLOVE embeddings vectors using the above function for training and validation set\nind_glove_df = [sent2vec(x) for x in tqdm(industry_df['Cleaned_Description'])]","c324f23d":"ind_glove_df[0]","2a5218b3":"ind_tfidf_df = pd.DataFrame()\nfor i in [1,2,3]:\n    vec_tfidf = TfidfVectorizer(max_features=10, norm='l2', stop_words='english', lowercase=True, use_idf=True, ngram_range=(i,i))\n    X = vec_tfidf.fit_transform(industry_df['Cleaned_Description']).toarray()\n    tfs = pd.DataFrame(X, columns=[\"TFIDF_\" + n for n in vec_tfidf.get_feature_names()])\n    ind_tfidf_df = pd.concat([ind_tfidf_df.reset_index(drop=True), tfs.reset_index(drop=True)], axis=1)\n\nind_tfidf_df.head(3)","479a92ed":"# To replace white space everywhere in Employee type\nindustry_df['Employee type'] = industry_df['Employee type'].str.replace(' ', '_')\nindustry_df['Employee type'].value_counts()","e0e0f118":"# To replace white space everywhere in Critical Risk\nindustry_df['Critical Risk'] = industry_df['Critical Risk'].str.replace('\\n', '').str.replace(' ', '_')\nindustry_df['Critical Risk'].value_counts().head()","993163b7":"# Create Industry DataFrame\nind_featenc_df = pd.DataFrame()\n\n# Label encoding\nindustry_df['Season'] = industry_df['Season'].replace('Summer', 'aSummer').replace('Autumn', 'bAutumn').replace('Winter', 'cWinter').replace('Spring', 'dSpring')\nind_featenc_df['Season'] = LabelEncoder().fit_transform(industry_df['Season']).astype(np.int8)\n\nindustry_df['Weekday'] = industry_df['Weekday'].replace('Monday', 'aMonday').replace('Tuesday', 'bTuesday').replace('Wednesday', 'cWednesday').replace('Thursday', 'dThursday').replace('Friday', 'eFriday').replace('Saturday', 'fSaturday').replace('Sunday', 'gSunday')\nind_featenc_df['Weekday'] = LabelEncoder().fit_transform(industry_df['Weekday']).astype(np.int8)\n\nind_featenc_df['Accident Level'] = LabelEncoder().fit_transform(industry_df['Accident Level']).astype(np.int8)\nind_featenc_df['Potential Accident Level'] = LabelEncoder().fit_transform(industry_df['Potential Accident Level']).astype(np.int8)","68315257":"# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(ind_featenc_df['Accident Level'])\ndummy_y","0633f0d8":"# Dummy variables encoding\nCountry_dummies = pd.get_dummies(industry_df['Country'], columns=[\"Country\"], drop_first=True)\nLocal_dummies = pd.get_dummies(industry_df['Local'], columns=[\"Local\"], drop_first=True)\nGender_dummies = pd.get_dummies(industry_df['Gender'], columns=[\"Gender\"], drop_first=True)\nIS_dummies = pd.get_dummies(industry_df['Industry Sector'], columns=['Industry Sector'], prefix='IS', drop_first=True)\nEmpType_dummies = pd.get_dummies(industry_df['Employee type'], columns=['Employee type'], prefix='EmpType', drop_first=True)\nCR_dummies = pd.get_dummies(industry_df['Critical Risk'], columns=['Critical Risk'], prefix='CR', drop_first=True)\n\n# Merge the above dataframe with the original dataframe ind_feat_df\nind_featenc_df = ind_featenc_df.join(Country_dummies.reset_index(drop=True)).join(Local_dummies.reset_index(drop=True)).join(Gender_dummies.reset_index(drop=True)).join(IS_dummies.reset_index(drop=True)).join(EmpType_dummies.reset_index(drop=True)).join(CR_dummies.reset_index(drop=True))\n\nind_featenc_df = industry_df[['Year','Month','Day','WeekofYear']].reset_index(drop=True).join(ind_featenc_df.reset_index(drop=True))\n\nind_featenc_df.head(3)","90cd4ae1":"# Check NaN values\nnp.any(np.isnan(ind_featenc_df))","884c1272":"# Consider only top 30 GLOVE features\nind_feat_df = ind_featenc_df.join(pd.DataFrame(ind_glove_df).iloc[:,0:30].reset_index(drop=True))","33b7168b":"ind_feat_df.head(3)","621273e4":"# Consider only top 30 GLOVE features\nind_feat_df = ind_featenc_df.join(ind_tfidf_df.reset_index(drop=True))","d0572b8d":"ind_feat_df.head(3)","55fb8a9a":"X = ind_feat_df.drop(['Accident Level','Potential Accident Level'], axis = 1) # Considering all Predictors\ny = ind_feat_df['Accident Level']","54c722ee":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 1, stratify = y)","baef2c5b":"X_train, X_test, y_train_dummy, y_test_dummy = train_test_split(X, dummy_y, test_size = 0.20, random_state = 1, stratify = y)","110e0e9b":"print('X_train shape : ({0},{1})'.format(X_train.shape[0], X_train.shape[1]))\nprint('y_train shape : ({0},)'.format(y_train.shape[0]))\nprint('X_test shape : ({0},{1})'.format(X_test.shape[0], X_test.shape[1]))\nprint('y_test shape : ({0},)'.format(y_test.shape[0]))","0d6297ed":"# Display old accident level counts\nind_feat_df['Accident Level'].value_counts()","22a426dd":"# Concatenate our training data back together\nX_up = pd.concat([X_train, y_train], axis=1)\n\n# Get the majority and minority class\nacclevel_0_majority = X_up[X_up['Accident Level'] == 0]\nacclevel_1_minority = X_up[X_up['Accident Level'] == 1]\nacclevel_2_minority = X_up[X_up['Accident Level'] == 2]\nacclevel_3_minority = X_up[X_up['Accident Level'] == 3]\nacclevel_4_minority = X_up[X_up['Accident Level'] == 4]\n\n# Upsample Level1 minority class\nacclevel_1_minority_upsampled = resample(acclevel_1_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level2 minority class\nacclevel_2_minority_upsampled = resample(acclevel_2_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level3 minority class\nacclevel_3_minority_upsampled = resample(acclevel_3_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)\n\n# Upsample Level4 minority class\nacclevel_4_minority_upsampled = resample(acclevel_4_minority,\n                                replace = True, # sample with replacement\n                                n_samples = len(acclevel_0_majority), # to match majority class\n                                random_state = 1)","1d1f616e":"# Combine majority class with upsampled minority classes\ndf_upsampled = pd.concat([acclevel_0_majority, acclevel_1_minority_upsampled, acclevel_2_minority_upsampled, acclevel_3_minority_upsampled, \n                          acclevel_4_minority_upsampled])","44ec61a0":"# Display new accident level counts\ndf_upsampled['Accident Level'].value_counts()","6f02f9f4":"# Separate input features and target\nX_train_up = df_upsampled.drop(['Accident Level'], axis = 1) # Considering all Predictors\ny_train_up = df_upsampled['Accident Level']","28c15438":"sm = SMOTE(random_state=1)\nX_train_smote, y_train_smote = sm.fit_resample(X_train, y_train)\ndf_smote = pd.concat([pd.DataFrame(X_train_smote), pd.DataFrame(y_train_smote)], axis=1)\ndf_smote.columns = ['Year', 'Month', 'Day',\n        'WeekofYear', 'Season', 'Weekday',\n       'Country_02', 'Country_03', 'Local_02', 'Local_03', 'Local_04',\n       'Local_05', 'Local_06', 'Local_07', 'Local_08', 'Local_09', 'Local_10',\n       'Local_11', 'Local_12', 'Male', 'IS_Mining', 'IS_Others',\n       'EmpType_Third_Party', 'EmpType_Third_Party_(Remote)',\n       'CR_Blocking_and_isolation_of_energies', 'CR_Burn',\n       'CR_Chemical_substances', 'CR_Confined_space', 'CR_Cut',\n       'CR_Electrical_Shock', 'CR_Electrical_installation', 'CR_Fall',\n       'CR_Fall_prevention', 'CR_Fall_prevention_(same_level)',\n       'CR_Individual_protection_equipment', 'CR_Liquid_Metal',\n       'CR_Machine_Protection', 'CR_Manual_Tools', 'CR_Not_applicable',\n       'CR_Others', 'CR_Plates', 'CR_Poll', 'CR_Power_lock', 'CR_Pressed',\n       'CR_Pressurized_Systems',\n       'CR_Pressurized_Systems_\/_Chemical_Substances', 'CR_Projection',\n       'CR_Projection\/Burning', 'CR_Projection\/Choco',\n       'CR_Projection\/Manual_Tools', 'CR_Projection_of_fragments',\n       'CR_Suspended_Loads', 'CR_Traffic', 'CR_Vehicles_and_Mobile_Equipment',\n       'CR_Venomous_Animals', 'CR_remains_of_choco', 'TFIDF_activity', 'TFIDF_area',\n       'TFIDF_causing', 'TFIDF_employee', 'TFIDF_hand', 'TFIDF_injury',\n       'TFIDF_left', 'TFIDF_operator', 'TFIDF_right', 'TFIDF_time',\n       'TFIDF_causing injury', 'TFIDF_described injury',\n       'TFIDF_employee reports', 'TFIDF_finger left', 'TFIDF_injury described',\n       'TFIDF_left foot', 'TFIDF_left hand', 'TFIDF_medical center',\n       'TFIDF_right hand', 'TFIDF_time accident',\n       'TFIDF_causing injury described', 'TFIDF_described time accident',\n       'TFIDF_finger left hand', 'TFIDF_finger right hand',\n       'TFIDF_generating described injury', 'TFIDF_hand causing injury',\n       'TFIDF_injury time accident', 'TFIDF_left hand causing',\n       'TFIDF_right hand causing', 'TFIDF_time accident employee', 'Accident Level']","97347feb":"# Separate input features and target\nX_train_smote = df_smote.iloc[:,:-1] # Considering all Predictors\ny_train_smote = df_smote.iloc[:,-1:]","c545b0c8":"X_train_smote.head(1)","73fa5237":"# Display new accident level counts\ny_train_smote['Accident Level'].value_counts()","97cb4020":"# convert integers to dummy variables (i.e. one hot encoded)\ny_train_smote_dummy = np_utils.to_categorical(y_train_smote['Accident Level'])\ny_train_smote_dummy","1aff572b":"# Transform independent features\nscaler_X = StandardScaler()#StandardScaler()\npipeline = Pipeline(steps=[('s', scaler_X)])\nX_train.iloc[:,:6] = pipeline.fit_transform(X_train.iloc[:,:6]) # Scaling only first 6 feautres\n\nX_test.iloc[:,:6] = pipeline.fit_transform(X_test.iloc[:,:6]) # Scaling only first 6 feautres","bb04067e":"X_train.head(3)","1748f082":"# generating the covariance matrix and the eigen values for the PCA analysis\ncov_matrix = np.cov(X_train.T) # the relevanat covariance matrix\nprint('Covariance Matrix \\n%s', cov_matrix)\n\n#generating the eigen values and the eigen vectors\ne_vals, e_vecs = np.linalg.eig(cov_matrix)\nprint('Eigenvectors \\n%s' %e_vecs)\nprint('\\nEigenvalues \\n%s' %e_vals)","b5a4481f":"# the \"cumulative variance explained\" analysis \ntot = sum(e_vals)\nvar_exp = [( i \/tot ) * 100 for i in sorted(e_vals, reverse=True)]\ncum_var_exp = np.cumsum(var_exp)\nprint(\"Cumulative Variance Explained\", cum_var_exp)","d82dd2d7":"# Plotting the variance expalained by the principal components and the cumulative variance explained.\nplt.figure(figsize=(20 , 5))\nplt.bar(range(1, e_vals.size + 1), var_exp, alpha = 0.5, align = 'center', label = 'Individual explained variance')\nplt.step(range(1, e_vals.size + 1), cum_var_exp, where='mid', label = 'Cumulative explained variance')\nplt.ylabel('Explained Variance Ratio')\nplt.xlabel('Principal Components')\nplt.legend(loc = 'best')\nplt.tight_layout()\nplt.show()","315f4590":"# Capturing 90% variance of the data\npca = PCA(n_components = 0.90)\nX_train_reduced = pca.fit_transform(X_train)\nX_test_reduced = pca.transform(X_test)","a9e085a3":"print(X_train_reduced.shape)\nprint(X_test_reduced.shape)","235d92a3":"# DummyClassifier to predict all Accident levels\ndummy = DummyClassifier(strategy='stratified').fit(X_train, y_train)\ndummy_pred = dummy.predict(X_test)\n\n# checking unique labels\nprint('Unique predicted labels: ', (np.unique(dummy_pred)))\n\n# checking accuracy\nprint('Test score: ', accuracy_score(y_test, dummy_pred))","7b3b90c5":"# Checking unique values\npredictions = pd.DataFrame(dummy_pred)\npredictions[0].value_counts()","c7bf5148":"def multiclass_logloss(actual, predicted, eps=1e-15):\n    \"\"\"Multi class version of Logarithmic Loss metric.\n    :param actual: Array containing the actual target classes\n    :param predicted: Matrix with class predictions, one probability per class\n    \"\"\"\n    # Convert 'actual' to a binary array if it's not already:\n    if len(actual.shape) == 1:\n        actual2 = np.zeros((actual.shape[0], predicted.shape[1]))\n        for i, val in enumerate(actual):\n            actual2[i, val] = 1\n        actual = actual2\n\n    clip = np.clip(predicted, eps, 1 - eps)\n    rows = actual.shape[0]\n    vsota = np.sum(actual * np.log(clip))\n    return -1.0 \/ rows * vsota","21045bdb":"def train_test_model(model, method, X_train, X_test, y_train, y_test, of_type, index, scale, report, save_model):\n    \n    if report == \"yes\":\n        print (model)\n        print (\"***************************************************************************\")\n\n    if method == 'CatBoostClassifier' or method == 'LGBMClassifier':\n\n      model.fit(X_train, y_train) # Fit the model on Training set\n    else:\n      model.fit(X_train, y_train) # Fit the model on Training set\n\n    from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score, precision_score\n    \n    if of_type == \"coef\":\n        # Intercept and Coefficients\n        print(\"The intercept for our model is {}\".format(model.intercept_), \"\\n\")\n        \n        for idx, col_name in enumerate(X_train.columns):\n            print(\"The coefficient for {} is {}\".format(col_name, model.coef_.ravel()[idx]))\n\n    y_pred = model.predict(X_test) # Predict on Test set\n\n    # Initialise mc_logloss\n    mc_logloss = 1.00\n    if method != 'RidgeClassifier':\n      y_predictions = model.predict_proba(X_test)\n\n    train_accuracy_score = model.score(X_train, y_train)\n    test_accuracy_score = model.score(X_test, y_test)\n\n    precision_score = precision_score(y_test, y_pred, average='weighted')\n    recall_score = recall_score(y_test, y_pred, average='weighted')\n    f1_score = f1_score(y_test, y_pred, average='weighted')\n\n    if method != 'RidgeClassifier':\n      mc_logloss = multiclass_logloss(y_test, y_predictions, eps=1e-15)\n\n    if report == \"yes\":\n      # Model - Confusion matrix\n      model_cm = confusion_matrix(y_test, y_pred)\n\n      sns.heatmap(model_cm, annot=True,  fmt='.2f', xticklabels = [\"I\", \"II\", \"III\", \"IV\", \"V\"] , yticklabels = [\"I\", \"II\", \"III\", \"IV\", \"V\"] )\n      plt.ylabel('Actual')\n      plt.xlabel('Predicted')\n      plt.show()\n\n      # Model - Classification report\n      model_cr = classification_report(y_test, y_pred)\n      print(model_cr)\n\n    # Store the accuracy results for each model in a dataframe for final comparison\n    resultsDf = pd.DataFrame({'Method': method, 'Train Accuracy': train_accuracy_score, 'Test Accuracy': test_accuracy_score, \n                              'Precision': precision_score, 'Recall': recall_score, 'F1-Score': f1_score, \n                              'Multi-Class Logloss': mc_logloss}, index=[index])\n    \n    # Save the model\n    if save_model == \"yes\":\n      filename = 'finalised_model.sav'\n      pickle.dump(model, open(filename, 'wb'))\n      \n    return resultsDf  # return all the metrics along with predictions","c2552946":"import lightgbm as lgb\n\ndef train_test_allmodels(X_train_common, X_test_common, y_train, y_test, scale):\n\n    # define classification models\n    models=[['LogisticRegression',LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)],\n        ['RidgeClassifier',RidgeClassifier(random_state = 1)],\n        #['Lasso',Lasso(random_state = 1)],\n        ['KNeighborsClassifier',KNeighborsClassifier(n_neighbors = 3)],\n        ['SVC',SVC(kernel = 'rbf', probability=True)],\n        ['DecisionTreeClassifier',DecisionTreeClassifier(criterion = 'gini', random_state=1)],\n        ['RandomForestClassifier',RandomForestClassifier(n_estimators=10, random_state=1)],\n        ['BaggingClassifier',BaggingClassifier(n_estimators=30, max_samples=0.75, random_state=1, oob_score=True)],\n        ['ExtraTreesClassifier',ExtraTreesClassifier(n_estimators = 50, criterion='entropy', max_features='auto', min_samples_split=2, \n                                 bootstrap=True, oob_score=True)],\n        ['AdaBoostClassifier',AdaBoostClassifier(n_estimators=100, learning_rate=0.25, random_state=1)],\n        ['GradientBoostingClassifier',GradientBoostingClassifier(loss='deviance', n_estimators=50, learning_rate=0.1, validation_fraction=0.2, \n                                       random_state=1)],\n        ['CatBoostClassifier',CatBoostClassifier(task_type= 'GPU', loss_function=\"MultiClass\", random_state=1, verbose=0)],\n                                                #early_stopping_rounds = 30)],\n        ['LGBMClassifier',LGBMClassifier(random_state=1, metric = \"multi_logloss\", objective=\"multiclass\")],\n                                         #early_stopping_rounds = 30)],\n        ['XGBClassifier',XGBClassifier(min_child_weight = 7, max_depth = 6, objective=\"multi:softmax\", learning_rate = 0.1, gamma = 0.4, \n                                       colsample_bytree = 0.5)]\n    ]\n\n    resultsDf_common = pd.DataFrame()\n    i = 1\n    for name, classifier in models:\n        # Train and Test the model\n        reg_resultsDf = train_test_model(classifier, name, X_train_common, X_test_common, y_train, y_test, 'none', i, scale, 'no', 'no')\n\n        # Store the accuracy results for each model in a dataframe for final comparison\n        resultsDf_common = pd.concat([resultsDf_common, reg_resultsDf])\n        i = i+1\n\n    return resultsDf_common","e7ebf739":"def hyperparameterstune_model(name, model, X_train, y_train, param_grid):\n    \n    start = time.time()  # note the start time \n    \n    # Before starting with grid search we need to create a scoring function. This is accomplished using the make_scorer function of scikit-learn.\n    mll_scorer = metrics.make_scorer(multiclass_logloss, greater_is_better=False, needs_proba=True)\n\n    # define grid search\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    if name == 'LGBMClassifier':\n      grid_search = RandomizedSearchCV(estimator=model, param_distributions=param_grid, n_iter=100, n_jobs=-1, cv=cv, \n                                       scoring = mll_scorer, error_score=0)\n    else:\n      grid_search = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=cv, \n                                 scoring = mll_scorer, error_score=0)\n      \n    model_grid_result = grid_search.fit(X_train, y_train)\n\n    # summarize results\n    print(\"Best F1_Score: %f using %s\" % (model_grid_result.best_score_, model_grid_result.best_params_))\n    means = model_grid_result.cv_results_['mean_test_score']\n    stds = model_grid_result.cv_results_['std_test_score']\n    params = model_grid_result.cv_results_['params']\n    for mean, stdev, param in zip(means, stds, params):\n      if param == model_grid_result.best_params_:\n        print(\"%f (%f) with: %r\" % (mean, stdev, param))\n        print(\"95% Confidence interval range: ({0:.4f} %, {1:.4f} %)\".format(mean-(2*stdev), mean+(2*stdev)))\n\n    end = time.time()  # note the end time\n    duration = end - start  # calculate the total duration\n    print(\"Total duration\" , duration, \"\\n\")\n    \n    return model_grid_result.best_estimator_","2c6f6daa":"# For multiclass problems, only 'newton-cg', 'sag', 'saga' and 'lbfgs' handle multinomial loss; 'liblinear' is limited to one-versus-rest schemes.\n\nresultsDf = pd.DataFrame()\n\n# Building a Linear Regression model\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)\n                                                     \n# Train and Test the model\nresultsDf = train_test_model(lr, 'Logistic Regression without Sampling', X_train, X_test, y_train, y_test, 'none', 1, 'no', 'yes', 'no')\n\n# Store the accuracy results for each model in a dataframe for final comparison\nresultsDf","e0174de3":"# Building a Random Forest Classifier on Training set\nrfc_model = RandomForestClassifier(n_estimators=10, random_state=1)\n\n# Train and Test the model\nrf_df = train_test_model(rfc_model, 'Random Forest with original data', X_train, X_test, y_train, y_test, 'none', 2, 'no', 'yes', 'no')\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,rf_df])\nresultsDf","48b60f55":"# Building a Linear Regression model\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)\n                                                     \n# Train and Test the model\nlr_df = train_test_model(lr, 'Logistic Regression with Sampling', X_train_up, X_test, y_train_up, y_test, 'none', 3, 'no', 'yes', 'no')\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,lr_df])\nresultsDf","31cb7f0e":"# Building a Linear Regression model\nlr = LogisticRegression(solver='lbfgs', multi_class='multinomial', random_state = 1)\n                                                     \n# Train and Test the model\nlr_smote_df = train_test_model(lr, 'Logistic Regression with SMOTE', X_train_smote, X_test, y_train_smote, y_test, 'none', 4, 'no', 'yes', 'no')\n\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.concat([resultsDf,lr_smote_df])\nresultsDf","316188fc":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train, X_test, y_train, y_test, 'no')","45034457":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_up, X_test, y_train_up, y_test, 'no')","b84ba270":"# Train and Test all models with Lasso interaction terms\ntrain_test_allmodels(X_train_smote, X_test, y_train_smote, y_test, 'no')","6122bd29":"# define regressor models\nmodels=[['LogisticRegression',LogisticRegression()],\n    ['Ridge',RidgeClassifier()],\n    ['KNeighborsClassifier',KNeighborsClassifier()],\n    ['SVC',SVC()]\n    ['RandomForestClassifier',RandomForestClassifier()],\n    ['BaggingClassifier',BaggingClassifier()],\n    ['ExtraTreesClassifier',ExtraTreesClassifier()],\n    ['AdaBoostClassifier',AdaBoostClassifier()],\n    ['GradientBoostingClassifier',GradientBoostingClassifier()],\n    ['CatBoostClassifier',CatBoostClassifier(verbose=False)],\n    ['LGBMClassifier',LGBMClassifier(verbose=False)],\n    ['XGBClassifier',XGBClassifier()]\n]\n\n# define model parameters\n\nlr_param_grid = {'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n                 'penalty': ['l2'],\n                 #'penalty': ['none', 'l1', 'l2', 'elasticnet'],\n                 'C': [100, 10, 1.0, 0.1, 0.01]}\n                 #'class_weight':['none','balanced'],\n                 #'multi_class':['ovr', 'multinomial']}\nridge_param_grid = {'alpha': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0],\n                    'class_weight':['none','balanced'],\n                    'solver': ['svd', 'cholesky', 'lsqr', 'sparse_cg', 'sag', 'saga']}\n#lasso_param_grid = {'alpha': [0.02, 0.024, 0.025, 0.026, 0.03]}\nknn_param_grid = {'n_neighbors': range(3, 21, 2),\n                 'weights': ['uniform', 'distance'],\n                 'metric': ['euclidean', 'manhattan', 'minkowski']}\nsvc_param_grid = {'kernel': ['poly', 'rbf', 'sigmoid'],\n                 'C': [50, 10, 1.0, 0.1, 0.01],\n                 'gamma': ['scale'],\n                 'decision_function_shape': ['ovo', 'ovr']}\nrf_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_features': ['auto', 'sqrt', 'log2']}              \n                 #'class_weight':['balanced','balanced_subsample','none']}\nbag_param_grid = {'n_estimators': [10, 100, 1000],\n                 'max_samples': np.arange(0.7, 0.8, 0.05)}\net_param_grid = {'n_estimators': np.arange(10,100,10),\n                 'max_features': ['auto', 'sqrt', 'log2'],\n                 'min_samples_split': np.arange(2,15,1)}\n                 #'class_weight':['balanced','balanced_subsample','none']}\nadb_param_grid = {'n_estimators': np.arange(30,100,10),\n                 'learning_rate': np.arange(0.1,1,0.5)}\ngb_param_grid = {'n_estimators': [10, 50, 100, 500],\n                 'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n                 'subsample':[0.5, 0.7, 1.0],\n                 'max_depth': [3, 7, 9]}\ncatb_param_grid = {'task_type': 'GPU','depth': [4, 7, 10],\n                  'learning_rate' : [0.03, 0.1, 0.15],\n                  'l2_leaf_reg': [1,4,9],\n                  'early_stopping_rounds':[50],\n                  'iterations': [300],\n                  'loss_function':['MultiClass']}\nlightgbm_param_grid = {'learning_rate': [0.0001, 0.001, 0.01, 0.1, 1.0],\n                  'n_estimators': [10, 50, 100, 500, 1000, 5000],\n                  'min_child_samples': sp_randint(100, 500), \n                  'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n                  'boosting_type':['gbdt', 'dart', 'goss'],\n                  'bagging_fraction': (0.5, 1),\n                  'bagging_frequency' : (5, 8),\n                  'feature_fraction': (0.5, 0.8),\n                  'max_depth': (10, 13),\n                  'min_data_in_leaf': (90, 120),\n                  'num_leaves':(1200, 1550),\n                  'metric': ['multi_logloss'],\n                  'objective': ['multiclass'],\n                  'num_class': [5],\n                  'early_stopping_rounds':[50],\n                  'verbosity':[1]}\nxgb_param_grid = {'learning_rate': [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ],\n                  'max_depth' : [3, 4, 5, 6, 8, 10, 12, 15],\n                  'min_child_weight': [ 1, 3, 5, 7],\n                  'gamma': [0.0, 0.1, 0.2 , 0.3, 0.4],\n                  'colsample_bytree': [ 0.3, 0.4, 0.5 , 0.7],\n                  'objective': ['multi:softmax'],\n                  'eval_metric': ['mlogloss'],\n                  'num_class': [5]}\n\nfor name, classifier in models:\n    if name == 'LogisticRegression':\n        lr_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, lr_param_grid)\n    elif name == 'Ridge':\n        ridge_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, lasso_param_grid)\n    elif name == 'KNeighborsClassifier':\n        knn_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, knn_param_grid)\n    elif name == 'SVC':\n        svc_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, svc_param_grid)\n    elif name == 'RandomForestClassifier':\n        rf_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, rf_param_grid)\n    elif name == 'BaggingClassifier':\n        bag_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, bag_param_grid)\n    elif name == 'ExtraTreesClassifier':\n        et_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, et_param_grid)\n    elif name == 'AdaBoostClassifier':\n        adb_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, adb_param_grid)\n    elif name == 'GradientBoostingClassifier':\n        gb_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, gb_param_grid)\n    elif name == 'CatBoostClassifier':\n        catb_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, catb_param_grid)\n    elif name == 'LGBMClassifier':\n        lightgbm_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, lightgbm_param_grid)\n    elif name == 'XGBClassifier':\n        xgb_best_estimator = hyperparameterstune_model(name, classifier, X_train, y_train, xgb_param_grid)","3570216b":"ind_feat_df.drop(['Accident Level','Potential Accident Level'], axis = 1, inplace=True) # Considering all Predictors\n\n# Consider only top 30 GLOVE features\nind_feat_df = ind_feat_df.join(y.reset_index(drop=True))\nind_feat_df.head(2)","0c167919":"values = ind_feat_df.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(ind_feat_df) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nrf_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n     # fit model\n    rfTree = RandomForestClassifier(n_estimators=100)\n\n    # fit against independent variables and corresponding target values\n    rfTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = rfTree.score(test[:, :-1] , y_bs_test)\n    predictions = rfTree.predict(test[:, :-1]) \n\n    rf_stats.append(score)","b3d82faa":"# plot scores\nplt.hist(rf_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(rf_stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(rf_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","cc9b3eff":"values = ind_feat_df.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(ind_feat_df) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nadab_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n     # fit model\n    adabTree = AdaBoostClassifier(n_estimators=100, learning_rate=0.25, random_state=1)\n\n    # fit against independent variables and corresponding target values\n    adabTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = adabTree.score(test[:, :-1] , y_bs_test)\n    predictions = adabTree.predict(test[:, :-1]) \n\n    adab_stats.append(score)","dd245134":"# plot scores\nplt.hist(adab_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(adab_stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(adab_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","082e0dd2":"values = ind_feat_df.values\n\n# Number of bootstrap samples to create\nn_iterations = 1000        \n\n# size of a bootstrap sample\nn_size = int(len(ind_feat_df) * 1)    \n\n# run bootstrap\n# empty list that will hold the scores for each bootstrap iteration\nxgb_stats = list()   \nfor i in range(n_iterations):\n    # prepare train and test sets\n    train = resample(values, n_samples=n_size)  # Sampling with replacement \n    test = np.array([x for x in values if x.tolist() not in train.tolist()])  # picking rest of the data not considered in sample\n    \n     # fit model\n    xgbTree = XGBClassifier(max_depth = 6, objective=\"multi:softmax\", learning_rate = 0.1, gamma = 0.4)\n\n    # fit against independent variables and corresponding target values\n    xgbTree.fit(train[:,:-1], train[:,-1]) \n\n    # Take the target column for all rows in test set\n    y_bs_test = test[:,-1]  \n\n    # evaluate model\n    # predict based on independent variables in the test data\n    score = xgbTree.score(test[:, :-1] , y_bs_test)\n    predictions = xgbTree.predict(test[:, :-1]) \n\n    xgb_stats.append(score)","5da97e6e":"# plot scores\nplt.hist(xgb_stats)\nplt.show()\n\n# confidence intervals\nalpha = 0.95                             # for 95% confidence \np = ((1.0-alpha)\/2.0) * 100              # tail regions on right and left .25 on each side indicated by P value (border)\nlower = max(0.0, np.percentile(xgb_stats, p))  \np = (alpha+((1.0-alpha)\/2.0)) * 100\nupper = min(1.0, np.percentile(xgb_stats, p))\nprint('%.1f confidence interval %.1f%% and %.1f%%' % (alpha*100, lower*100, upper*100))","12ed3416":"# disable keras warnings\ntf.get_logger().setLevel('ERROR')","7bd3ac10":"# get the accuracy, precision, recall, f1 score from model\ndef get_classification_metrics(model, X_test, y_test, target_type):\n  \n  # predict probabilities for test set\n  yhat_probs = model.predict(X_test, verbose=0) # Multiclass\n\n  # predict crisp classes for test set\n  if target_type == 'multi_class':\n    yhat_classes = model.predict_classes(X_test, verbose=0) # Multiclass\n  else:\n    yhat_classes = (np.asarray(model.predict(X_test))).round() # Multilabel\n\n  # reduce to 1d array\n  yhat_probs = yhat_probs[:, 0]\n\n  # accuracy: (tp + tn) \/ (p + n)\n  accuracy = accuracy_score(y_test, yhat_classes)\n\n  # precision tp \/ (tp + fp)\n  precision = precision_score(y_test, yhat_classes, average='micro')\n\n  # recall: tp \/ (tp + fn)\n  recall = recall_score(y_test, yhat_classes, average='micro')\n\n  # f1: 2 tp \/ (2 tp + fp + fn)\n  f1 = f1_score(y_test, yhat_classes, average='micro')\n\n  return accuracy, precision, recall, f1","d129a513":"class Metrics(tf.keras.callbacks.Callback):\n\n    def __init__(self, validation_data=()):\n        super().__init__()\n        self.validation_data = validation_data\n\n    def on_train_begin(self, logs={}):\n        self.val_f1s = []\n        self.val_recalls = []\n        self.val_precisions = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        xVal, yVal, target_type = self.validation_data\n        if target_type == 'multi_class':\n          val_predict_classes = model.predict_classes(xVal, verbose=0) # Multiclass\n        else:\n          val_predict_classes = (np.asarray(self.model.predict(xVal))).round() # Multilabel\n        \n        \n        val_targ = yVal\n\n        _val_f1 = f1_score(val_targ, val_predict_classes, average='micro')\n        _val_recall = recall_score(val_targ, val_predict_classes, average='micro')\n        _val_precision = precision_score(val_targ, val_predict_classes, average='micro')\n        self.val_f1s.append(_val_f1)\n        self.val_recalls.append(_val_recall)\n        self.val_precisions.append(_val_precision)\n        #print(\"\u2014 train_f1: %f \u2014 train_precision: %f \u2014 train_recall %f\" % (_val_f1, _val_precision, _val_recall))\n        return","9d912560":"# fix random seed for reproducibility\nseed = 7\nnp.random.seed(seed)\ntf.random.set_seed(seed)\n\n# define the model\nmodel = Sequential()\nmodel.add(Dense(50, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(100, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(150, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(40, activation='relu', kernel_initializer='he_uniform'))\nmodel.add(Dense(1, activation='linear'))\n\n# compile the keras model\n#opt = optimizers.Adam(lr=1e-3)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='mse', optimizer=opt, metrics=['accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train, y_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_test, y_test), callbacks=[rlrp])","b2d16bd2":"model.summary()","45932eaa":"# evaluate the keras model\n_, accuracy = model.evaluate(X_test, y_test, batch_size=8, verbose=0)\nprint('Test accuracy: %.2f' % (accuracy*100))","6c3b75b0":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot  (epochs, training_history.history['loss'], label = 'train')\nplt.plot  (epochs, training_history.history['val_loss'], label = 'val')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","99825e68":"# fix random seed for reproducibility\nreset_random_seeds()\n#param = 1e-9\nparam = 1e-4\n\n# define the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_dim=X_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(5, activation='softmax', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm())) # Multilabel\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_train, y_train_dummy, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train, y_train_dummy, epochs=100, batch_size=8, verbose=1, validation_data=(X_test, y_test_dummy), callbacks=[rlrp, metrics])","195f96f7":"model.summary()","6adf1190":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_train, y_train_dummy, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_test, y_test_dummy, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","4c4b749e":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_test, y_test_dummy, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)","908a76d3":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","caeebccd":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['categorical_accuracy'], label = 'train')\nplt.plot(epochs, training_history.history['val_categorical_accuracy'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","3c9f568d":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","56e1424e":"# fix random seed for reproducibility\nreset_random_seeds()\n#param = 1e-9\nparam = 1e-4\n\n# define the model\nmodel = Sequential()\n\nmodel.add(Dense(10, input_dim=X_train_smote.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.2))\nmodel.add(BatchNormalization())\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm()))\nmodel.add(Dropout(0.5))\nmodel.add(BatchNormalization())\nmodel.add(Dense(5, activation='softmax', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm())) # Multilabel\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['categorical_accuracy'])\n\n# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_train_smote, y_train_smote_dummy, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_train_smote, y_train_smote_dummy, epochs=100, batch_size=8, verbose=1, validation_data=(X_test, y_test_dummy), callbacks=[rlrp, metrics])","f2d5b41d":"model.summary()","b6483899":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_train_smote, y_train_smote_dummy, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_test, y_test_dummy, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","74934ddf":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_test, y_test_dummy, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)","c7bcf2b3":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","2a07af72":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['categorical_accuracy'], label = 'train')\nplt.plot(epochs, training_history.history['val_categorical_accuracy'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","ddcad7f5":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","ecbce0ad":"# Select input and output features\nX_text = industry_df['Cleaned_Description']\ny_text = industry_df['Accident Level']","250b98ee":"# Encode labels in column 'Accident Level'.\ny_text = LabelEncoder().fit_transform(y_text)","1c31df40":"# Divide our data into testing and training sets:\nX_text_train, X_text_test, y_text_train, y_text_test = train_test_split(X_text, y_text, test_size = 0.20, random_state = 1, stratify = y_text)\n\nprint('X_text_train shape : ({0})'.format(X_text_train.shape[0]))\nprint('y_text_train shape : ({0},)'.format(y_text_train.shape[0]))\nprint('X_text_test shape : ({0})'.format(X_text_test.shape[0]))\nprint('y_text_test shape : ({0},)'.format(y_text_test.shape[0]))","d76abe78":"# Convert both the training and test labels into one-hot encoded vectors:\ny_text_train = np_utils.to_categorical(y_text_train)\ny_text_test = np_utils.to_categorical(y_text_test)","9837fac8":"# The first step in word embeddings is to convert the words into thier corresponding numeric indexes.\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(X_text_train)\n\nX_text_train = tokenizer.texts_to_sequences(X_text_train)\nX_text_test = tokenizer.texts_to_sequences(X_text_test)","c97de48f":"# Sentences can have different lengths, and therefore the sequences returned by the Tokenizer class also consist of variable lengths.\n# We need to pad the our sequences using the max length.\nvocab_size = len(tokenizer.word_index) + 1\nprint(\"vocab_size:\", vocab_size)\n\nmaxlen = 100\n\nX_text_train = pad_sequences(X_text_train, padding='post', maxlen=maxlen)\nX_text_test = pad_sequences(X_text_test, padding='post', maxlen=maxlen)","ee643d7e":"# We need to load the built-in GloVe word embeddings\nembedding_size = 200\nembeddings_dictionary = dict()\n\nglove_file = open('..\/input\/glove6b200d\/glove.6B.200d.txt', encoding=\"utf8\")\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary[word] = vector_dimensions\n\nglove_file.close()\n\nembedding_matrix = np.zeros((vocab_size, embedding_size))\n\nfor word, index in tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector\n\nlen(embeddings_dictionary.values())","31bd1e01":"reset_random_seeds()\n\n# Build a LSTM Neural Network\ndeep_inputs = Input(shape=(maxlen,))\nembedding_layer = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(deep_inputs)\n\nLSTM_Layer_1 = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\nmax_pool_layer_1 = GlobalMaxPool1D()(LSTM_Layer_1)\ndrop_out_layer_1 = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)\ndense_layer_1 = Dense(128, activation = 'relu')(drop_out_layer_1)\ndrop_out_layer_2 = Dropout(0.5, input_shape = (128,))(dense_layer_1)\ndense_layer_2 = Dense(64, activation = 'relu')(drop_out_layer_2)\ndrop_out_layer_3 = Dropout(0.5, input_shape = (64,))(dense_layer_2)\n\ndense_layer_3 = Dense(32, activation = 'relu')(drop_out_layer_3)\ndrop_out_layer_4 = Dropout(0.5, input_shape = (32,))(dense_layer_3)\n\ndense_layer_4 = Dense(10, activation = 'relu')(drop_out_layer_4)\ndrop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)\n\ndense_layer_5 = Dense(5, activation='softmax')(drop_out_layer_5)\n#dense_layer_3 = Dense(5, activation='softmax')(drop_out_layer_3)\n\n# LSTM_Layer_1 = LSTM(128)(embedding_layer)\n# dense_layer_1 = Dense(5, activation='softmax')(LSTM_Layer_1)\n# model = Model(inputs=deep_inputs, outputs=dense_layer_1)\n\nmodel = Model(inputs=deep_inputs, outputs=dense_layer_5)\n#model = Model(inputs=deep_inputs, outputs=dense_layer_3)\n\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])","334fd594":"print(model.summary())","2351a498":"plot_model(model, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)","91fb7374":"# Use earlystopping\n# callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=5, min_delta=0.001)\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_text_train, y_text_train, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_text_train, y_text_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_text_test, y_text_test), callbacks=[rlrp, metrics])","3a9a40d5":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_text_train, y_text_train, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_text_test, y_text_test, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","f002aeb9":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_text_test, y_text_test, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)","301b92a7":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","99864640":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['acc'], label = 'train')\nplt.plot(epochs, training_history.history['val_acc'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","bca279a7":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","fb16809c":"# Select input and output features\nX_cat = ind_featenc_df.drop(['Accident Level','Potential Accident Level'], axis = 1)\ny_cat = industry_df['Accident Level']","4bd634fb":"# Encode labels in column 'Accident Level'.\ny_cat = LabelEncoder().fit_transform(y_cat)","7c554744":"# Divide our data into testing and training sets:\nX_cat_train, X_cat_test, y_cat_train, y_cat_test = train_test_split(X_cat, y_cat, test_size = 0.20, random_state = 1, stratify = y_cat)\n\nprint('X_cat_train shape : ({0})'.format(X_cat_train.shape[0]))\nprint('y_cat_train shape : ({0},)'.format(y_cat_train.shape[0]))\nprint('X_cat_test shape : ({0})'.format(X_cat_test.shape[0]))\nprint('y_cat_test shape : ({0},)'.format(y_cat_test.shape[0]))","c0e0bc79":"# Convert both the training and test labels into one-hot encoded vectors:\ny_cat_train = np_utils.to_categorical(y_cat_train)\ny_cat_test = np_utils.to_categorical(y_cat_test)","ae38da90":"# Variable transformation using StandardScaler\nscaler_X = StandardScaler()#StandardScaler()\nX_cat_train.iloc[:,:6] = scaler_X.fit_transform(X_cat_train.iloc[:,:6]) # Scaling only first 6 feautres\n\nX_cat_test.iloc[:,:6] = scaler_X.fit_transform(X_cat_test.iloc[:,:6]) # Scaling only first 6 feautres","90477d85":"# fix random seed for reproducibility\nreset_random_seeds()\n\n#param = 1e-9\nparam = 1e-4\n\ninput2 = Input(shape=(X_cat_train.shape[1],))\ndense_layer_1 = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                kernel_constraint=unit_norm())(input2)\ndrop_out_layer_1 = Dropout(0.2)(dense_layer_1)\nbatch_norm_layer_1 = BatchNormalization()(drop_out_layer_1)\ndense_layer_2 = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                kernel_constraint=unit_norm())(batch_norm_layer_1)\ndrop_out_layer_2 = Dropout(0.5)(dense_layer_2)\nbatch_norm_layer_2 = BatchNormalization()(drop_out_layer_2)\ndense_layer_3 = Dense(5, activation='softmax', kernel_regularizer=l2(param), kernel_constraint=unit_norm())(batch_norm_layer_2)\n\nmodel = Model(inputs=input2, outputs=dense_layer_3)\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])","3f3299df":"print(model.summary())","196fde64":"plot_model(model, to_file='model_plot1.png', show_shapes=True, show_dtype=True, show_layer_names=True)","10e654fa":"# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=(X_cat_train, y_cat_train, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit(X_cat_train, y_cat_train, epochs=100, batch_size=8, verbose=1, validation_data=(X_cat_test, y_cat_test), callbacks=[rlrp, metrics])","c44cdbf9":"# evaluate the keras model\n_, train_accuracy = model.evaluate(X_cat_train, y_cat_train, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate(X_cat_test, y_cat_test, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","893cf7ea":"accuracy, precision, recall, f1 = get_classification_metrics(model, X_cat_test, y_cat_test, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)","9663bac0":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","98143ae5":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['acc'], label = 'train')\nplt.plot(epochs, training_history.history['val_acc'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","b9bc55f1":"# serialize model to JSON\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)\n    \n# serialize weights to HDF5\nmodel.save_weights(\"model.h5\")\nprint(\"Saved model weights to disk\")\n\n# Save the model in h5 format \nmodel.save(\"finalized_keras_model.h5\")\nprint(\"Saved model to disk\")","117d9852":"# fix random seed for reproducibility\nreset_random_seeds()\n\ninput_1 = Input(shape=(maxlen,))\nembedding_layer   = Embedding(vocab_size, embedding_size, weights=[embedding_matrix], trainable=False)(input_1)\nLSTM_Layer_1      = Bidirectional(LSTM(128, return_sequences = True))(embedding_layer)\nmax_pool_layer_1  = GlobalMaxPool1D()(LSTM_Layer_1)\ndrop_out_layer_1  = Dropout(0.5, input_shape = (256,))(max_pool_layer_1)\ndense_layer_1     = Dense(128, activation = 'relu')(drop_out_layer_1)\ndrop_out_layer_2  = Dropout(0.5, input_shape = (128,))(dense_layer_1)\ndense_layer_2     = Dense(64, activation = 'relu')(drop_out_layer_2)\ndrop_out_layer_3  = Dropout(0.5, input_shape = (64,))(dense_layer_2)\n\ndense_layer_3     = Dense(32, activation = 'relu')(drop_out_layer_3)\ndrop_out_layer_4  = Dropout(0.5, input_shape = (32,))(dense_layer_3)\n\ndense_layer_4     = Dense(10, activation = 'relu')(drop_out_layer_4)\ndrop_out_layer_5 = Dropout(0.5, input_shape = (10,))(dense_layer_4)\n\n#-------------------------------------------------------------------------------\nparam = 1e-4\n\ninput_2 = Input(shape=(X_cat_train.shape[1],))\ndense_layer_5       = Dense(10, input_dim=X_cat_train.shape[1], activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param),\n                      kernel_constraint=unit_norm())(input_2)\ndrop_out_layer_6    = Dropout(0.2)(dense_layer_5)\nbatch_norm_layer_1  = BatchNormalization()(drop_out_layer_6)\ndense_layer_6       = Dense(10, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=l2(param), \n                            kernel_constraint=unit_norm())(batch_norm_layer_1)\ndrop_out_layer_7   = Dropout(0.5)(dense_layer_6)\nbatch_norm_layer_2 = BatchNormalization()(drop_out_layer_7)\n\nconcat_layer        = Concatenate()([drop_out_layer_5, batch_norm_layer_2])\ndense_layer_7       = Dense(10, activation='relu')(concat_layer)\noutput  = Dense(5, activation='softmax')(dense_layer_7)\nmodel   = Model(inputs=[input_1, input_2], outputs=output)\n\n# compile the keras model\n#opt = optimizers.Adamax(lr=0.01)\nopt = SGD(lr=0.001, momentum=0.9)\nmodel.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['acc'])","b3dbc4a7":"print(model.summary())","8370e64d":"plot_model(model, to_file='model_plot3.png', show_shapes=True, show_layer_names=True)","e94d33e0":"# Use earlystopping\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=7, min_delta=1E-3)\nrlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.0001, patience=5, min_delta=1E-4)\n\ntarget_type = 'multi_label'\nmetrics = Metrics(validation_data=([X_text_train, X_cat_train], y_cat_train, target_type))\n\n# fit the keras model on the dataset\ntraining_history = model.fit([X_text_train, X_cat_train], y_cat_train, epochs=100, batch_size=8, verbose=1, validation_data=([X_text_test, X_cat_test], y_cat_test), callbacks=[rlrp, metrics])","c156a896":"# evaluate the keras model\n_, train_accuracy = model.evaluate([X_text_train, X_cat_train], y_cat_train, batch_size=8, verbose=0)\n_, test_accuracy = model.evaluate([X_text_test, X_cat_test], y_cat_test, batch_size=8, verbose=0)\n\nprint('Train accuracy: %.2f' % (train_accuracy*100))\nprint('Test accuracy: %.2f' % (test_accuracy*100))","0e387d26":"accuracy, precision, recall, f1 = get_classification_metrics(model, [X_text_test, X_cat_test], y_cat_test, target_type)\nprint('Accuracy: %f' % accuracy)\nprint('Precision: %f' % precision)\nprint('Recall: %f' % recall)\nprint('F1 score: %f' % f1)","79c6ae51":"epochs = range(len(training_history.history['loss'])) # Get number of epochs\n\n# plot loss learning curves\nplt.plot(epochs, training_history.history['loss'], label = 'train')\nplt.plot(epochs, training_history.history['val_loss'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation loss')","b9ce93cd":"# plot accuracy learning curves\nplt.plot(epochs, training_history.history['acc'], label = 'train')\nplt.plot(epochs, training_history.history['val_acc'], label = 'test')\nplt.legend(loc = 'upper right')\nplt.title ('Training and validation accuracy')","124a72d8":"#### 3. Modelling - Logistic Regression - Oversampling","c93bbf7f":"#### Model with Hyperparameter Tuning","074d4e5c":"* To better understand the data, I am extracting the day, month and year from Date column and creating new features such as weekday, weekofyear.","8f160590":"#### Study Summary Statistics","af8ad4a5":"##### e. Accident Levels by Employee type - Is the distribution of accident levels and potential accident levels differ significantly in different employee types?","cc400cfc":"* **Target variable:** 'Accident Level', 'Potential Accident Level'\n* **Predictors (Input varibles):** 'Date', 'Country', 'Local', 'Industry Sector', 'Gender', 'Employee type', 'Critical Risk', 'Description'","f26c17b8":"[Table of Contents](#table-of-contents)","a2440eb2":"1. Logistic Regression\n\n  Best F1_Score: 0.629247 using {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n  0.629247 (0.026945) with: {'C': 0.1, 'penalty': 'l1', 'solver': 'liblinear'}\n  95% Confidence interval range: (0.5754 %, 0.6831 %)\n  Total duration 428.40893173217773 \n\n2. Ridge\n\n  Best F1_Score: 0.616087 using {'alpha': 0.02}\n  0.616087 (0.023211) with: {'alpha': 0.02}\n  95% Confidence interval range: (0.5697 %, 0.6625 %)\n  Total duration 2.5354909896850586 \n\n3. KNeighborsClassifier\n\n  Best F1_Score: 0.629247 using {'metric': 'euclidean', 'n_neighbors': 13, 'weights': 'uniform'}\n  0.629247 (0.026945) with: {'metric': 'euclidean', 'n_neighbors': 13, 'weights': 'uniform'}\n  95% Confidence interval range: (0.5754 %, 0.6831 %)\n  Total duration 12.201840877532959 \n\n4. SVC\n\n  Best F1_Score: 0.629247 using {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n  0.629247 (0.026945) with: {'C': 1.0, 'gamma': 'scale', 'kernel': 'rbf'}\n  95% Confidence interval range: (0.5754 %, 0.6831 %)\n  Total duration 7.190079927444458 \n\n5. RandomForestClassifier\n\n  Best F1_Score: 0.622235 using {'max_features': 'sqrt', 'n_estimators': 1000}\n  0.622235 (0.028932) with: {'max_features': 'sqrt', 'n_estimators': 1000}\n  95% Confidence interval range: (0.5644 %, 0.6801 %)\n  Total duration 176.1380627155304 \n\n6. BaggingClassifier\n\n  Best F1_Score: 0.620604 using {'max_samples': 0.75, 'n_estimators': 10}\n  0.620604 (0.034210) with: {'max_samples': 0.75, 'n_estimators': 10}\n  95% Confidence interval range: (0.5522 %, 0.6890 %)\n  Total duration 305.1015794277191 \n\n7. ExtraTreesClassifier\n\n  Best F1_Score: 0.628276 using {'max_features': 'log2', 'min_samples_split': 13, 'n_estimators': 90}\n  0.628276 (0.028086) with: {'max_features': 'log2', 'min_samples_split': 13, 'n_estimators': 90}\n  95% Confidence interval range: (0.5721 %, 0.6844 %)\n  Total duration 688.0113415718079 \n\n8. AdaBoostClassifier\n\n  Best F1_Score: 0.630245 using {'learning_rate': 0.1, 'n_estimators': 70}\n  0.630245 (0.030167) with: {'learning_rate': 0.1, 'n_estimators': 70}\n  95% Confidence interval range: (0.5699 %, 0.6906 %)\n  Total duration 51.544673681259155 \n\n9. GradientBoostingClassifier\n\n  Best F1_Score: 0.613227 using {'learning_rate': 0.1, 'n_estimators': 30}\n  0.613227 (0.033099) with: {'learning_rate': 0.1, 'n_estimators': 30}\n  95% Confidence interval range: (0.5470 %, 0.6794 %)\n  Total duration 165.68569326400757 \n\n10. CatBoostClassifier\n\n  Best F1_Score: 0.629247 using {'depth': 4, 'iterations': 300, 'l2_leaf_reg': 4, 'learning_rate': 0.03}\n  0.629247 (0.026945) with: {'depth': 4, 'iterations': 300, 'l2_leaf_reg': 4, 'learning_rate': 0.03}\n  95% Confidence interval range: (0.5754 %, 0.6831 %)\n  Total duration 6367.161168336868 \n\n11. LGBMClassifier\n\n  Best F1_Score: 0.629247 using {'bagging_fraction': 0.5, 'bagging_frequency': 8, 'boosting_type': 'dart', 'early_stopping_rounds': 200, 'feature_fraction': 0.8, 'learning_rate': 0.0001, 'max_depth': 10, 'metric': 'multi_logloss', 'min_child_samples': 486, 'min_child_weight': 0.01, 'min_data_in_leaf': 90, 'n_estimators': 1000, 'num_class': 5, 'num_leaves': 1550, 'objective': 'multiclass', 'verbosity': 1}\n  0.629247 (0.026945) with: {'bagging_fraction': 0.5, 'bagging_frequency': 8, 'boosting_type': 'dart', 'early_stopping_rounds': 200, 'feature_fraction': 0.8, 'learning_rate': 0.0001, 'max_depth': 10, 'metric': 'multi_logloss', 'min_child_samples': 486, 'min_child_weight': 0.01, 'min_data_in_leaf': 90, 'n_estimators': 1000, 'num_class': 5, 'num_leaves': 1550, 'objective': 'multiclass', 'verbosity': 1}\n  95% Confidence interval range: (0.5754 %, 0.6831 %)\n  Total duration 2430.5182700157166 ","ca6d4b90":"* We observed that there are records of accidents from 1st Jan 2016 to 9th July 2017 in every month. So there are no outliers in the 'Date' column.\n\n* There are only three country types so there are no outliers in 'Country' column.\n\n* There are 12 Local cities where manufacturing plant is located and it's types are in sequence so there are no outliers in 'Local' column.\n\n* There are only three Industry Sector types which are in sequence so there are no outliers in 'Industry Sector' column.\n\n* There are only five Accident Level types which are in sequence so there are no outliers in 'Accident Level' column.\n\n* There are only six Potential Accident Level types which are in sequence so there are no outliers in 'Potential Accident Level' column.\n\n* There are only two Gender types in the provided data so there are no outliers in 'Gender' column.\n\n* There are only three Employee types in the provided data so there are no outliers in 'Gender' column.\n\n* There are quite a lot of Critical risk descriptions and we don't see any outliers but with the help of SME we can decide whether this column has outliers or not.","01845561":"1. There are about 425 rows and 11 columns in the dataset.\n2. We noticed that except a 'date' column all other columns are categorical columns.","9a6b5944":"### Domain - Industrial safety. NLP based Chatbot.","dfbd6897":"\n\n**Local**\n* Highest manufacturing plants are located in Local_03 city and lowest in Local_09 city.\n\n**Country**\n* Percentage(%) of accidents occurred in respective countries: 59% in Country_01, 31% in Country_02 and 10% in Country_03.\n\n**Industry Sector**\n* Percentage(%) of manufacturing plants belongs to respective sectors: 57% to Mining sector, 32% to Metals sector and 11% to Others sector.\n\n**Country + Industry Sector**\n* Metals and Mining industry sector plants are not available in Country_03.\n* Distribution of industry sector differ significantly in each country.\n\n**Accident Levels**\n* The number of accidents decreases as the Accident Level increases and increases as the Potential Accident Level increases.\n\n**Gender**\n* There are more men working in this industry as compared to women.\n\n**Employee type**\n* 44% Third party empoyees, 43% own empoyees and 13% Third party(Remote) empoyees working in this industry.\n\n**Gender + Employee type**\n* Proportion of third party employees in each gender is equal, third party(remote) employees in each gender is not equal and \nown employees in each gender is not equal.\n\n**Gender + Industry Sector**\n* Proportion of Metals, Mining and Others sector employees in each gender is not equal\n\n**Gender + Accident Levels**\n* Males have a higher accident levels than females.\n* There are many low risks at general accident level, but many high risks at potential accident level.\n\n**Accident Levels + Employee type**\n* For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be \nslightly higher at high accident levels.\n\n**Accident Levels + Calendar**\n* Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month, there are high number of accidents in 2016 and less in 2017.\n* Number of accidents are high in beginning of the year and it keeps decreasing later.\n* Number of accidents are very high in particular days like 4, 8 and 16 in every month.\n* Number of accidents increased during the middle of the week and declined since the middle of th week.\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, \nand some of these levels increased slightly in the second half of the year.\n* Both of the two accident level is thought that non-severe levels decreased in the first and the last of the week, but severe levels did not \nchanged much.\n\n**Critical Risk**\n* Most of the critical risks are classified as Others.","8285f1ee":"#### Remove 'Unnamed: 0' and Rename - 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns","085eb166":"**Calendar**","fc03654b":"#### Study Correlation","1d55f1bc":"#### Resampling Techniques \u2014 Oversample minority class","6be086d9":"As we know, this database comes from one of the biggest industry in Brazil which has four climatological seasos as below.\n\nhttps:\/\/seasonsyear.com\/Brazil\n\n* Spring : September to November\n* Summer : December to February\n* Autumn : March to May\n* Winter : June to August\n\nWe can create seasonal variable based on month variable.","8972a482":"#### Variable Creation - Word2Vec Embeddings","fa5966bc":"[Table of Contents](#table-of-contents)","9a2189dd":"#### Define MultiClass-Logloss","7ee403b3":"<a id=\"ml-models-hyperparam-tuning\"><\/a>\n#### Hyperparameter tuning with original features","33ba872d":"*\tGood EDA Notebook\n\n  https:\/\/www.kaggle.com\/koheimuramatsu\/industrial-accident-causal-analysis\n\n*\tHoloviews plot tips\n\n  http:\/\/holoviews.org\/user_guide\/Customizing_Plots.html\n\n*\tScikit-Learn API\n\n  https:\/\/scikit-learn.org\/stable\/\n\n*\tKeras API\n\n  https:\/\/keras.io\/api\/\n\n*\tStreamlit API\n\n  https:\/\/docs.streamlit.io\/en\/stable\/api.html\n  https:\/\/share.streamlit.io\/daniellewisdl\/streamlit-cheat-sheet\/app.py\n\n*\tHeroku with Python\n\n  https:\/\/devcenter.heroku.com\/articles\/getting-started-with-python\n","b6833fe3":"#### Multiclass classification - Target variable - One hot encoded with SMOTE data\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and one-hot encoded target variable. We can use simple densely connected neural networks to make predictions.","0ed47b88":"##### b. Employee type by Gender - Is the distribution of employee type differ significantly in different genders?","8d3098c7":"**Observations**\n\n* Proportion of third party employees in each gender is equal.\n* Proportion of third party(remote) employees in each gender is not equal.\n* Proportion of own employees in each gender is not equal. But let's check is that difference is statistically significant?","6e44bbfc":"**Observations**\n\n* Both of the two accident level is thought that non-severe levels decreased in the first and the last of the week, but severe levels did not changed much.","c99990d9":"<a id=\"nlp-models\"><\/a>\n## 12. Design, train and test RNN or LSTM classifiers","9160feba":"<a id=\"nlp-pre-processing\"><\/a>\n## 8. NLP Pre-processing\n\nFew of the NLP pre-processing steps taken before applying model on the data\n\n- Converting to lower case, avoid any capital cases\n- Converting apostrophe to the standard lexicons\n- Removing punctuations\n- Lemmatization\n- Removing stop words","c71b2be6":"* 44% Third party empoyees working in this industry.\n* 43% own empoyees working in this industry.\n* 13% Third party(Remote) empoyees working in this industry.","38e84610":"**Observations**\n\n* For both accident levels, the incidence of Employee is higher at low accident levels, but the incidence of Third parties seems to be slightly higher at high accident levels.","93c46ab9":"###### 1. State the H0 and Ha\n\n###### Ho = The proportions of industry sector is not differ in different countries\n###### Ha = The proportions of industry sector is differ in different countries\n\n###### 2. Decide the significance level: alpha = 0.05\n\n###### 3. Identify the test-statistic: Z-test of proportions\n\n###### 4. Calculate the p_value using test-statistic","b9ab8fa8":"#### Shape of the data","9fb8850d":"\n**Observations**\n\n- 74% of data where accident description > 100 is captured in low accident level. \n- Based on some random headlines seen above, it appears that the data is mostly lower-cased. Pre-processing such as removing punctuations and lemmatization can be used.\n- There are few alphanumeric characters like 042-TC-06, Nv. 3370, CX 212 captured in description where removing these characters might help.\n- There are digits in the description for e.g. level 326, Dumper 01 where removing the digits wouldn't help.","f98df6af":"#### SMOTE - Generate synthetic samples - upsample smaller class","8889f0a9":"#### Bootstrap Sampling - AdaBoostClassifier","11b82c76":"<a id=\"data-cleansing\"><\/a>\n## 4. Data Cleansing","e95ddf8f":"#### Data type of each attribute","7433f4fb":"* The number of accidents decreases as the Accident Level increases.\n* The number of accidents increases as the Potential Accident Level increases.","f603308e":"#### Combine Glove and Encoded Features","61bca085":"We could see it accuracy continually rise during training. As expected, we see the learning curves for accuracy on the test dataset plateau, indicating that the model has no longer overfit the training dataset and it is generalized model.","fb401b93":"**Gender**","4a7bd157":"**Country**","b77e41ab":"<a id=\"data-pre-processing\"><\/a>\n## 5. Data Pre-processing","80d098ef":"<a id=\"references\"><\/a>\n## 17. References","6a8c4fcb":"##### d. Accident Levels by Gender - Is the distribution of accident levels and potential accident levels differ significantly in different genders?","1963d214":"#### 2. Decision Tree - Random Forest Classifier\n\n* While in every machine learning problem, it\u2019s a good rule of thumb to try a variety of algorithms, it can be especially beneficial with imbalanced datasets. Decision trees frequently perform well on imbalanced data. They work by learning a hierarchy of if\/else questions and this can force both classes to be addressed.","c19ee60f":"<a id=\"conclusion\"><\/a>\n## 13. Conclusion","809fd7c2":"[Table of Contents](#table-of-contents)","3033f988":"<a id=\"table-of-contents\"><\/a>\n## Table of Contents\n\n1. [Overview](#overview)\n2. [Import the necessary libraries](#import-libraries)\n3. [Data Collection](#data-collection)\n4. [Data Cleansing](#data-cleansing)\n5. [Data Pre-processing](#data-preprocessing)\n6. [EDA (Data Analysis and Preparation)](#eda)\n    - [Univariate Analysis](#univariate-analysis)\n    - [Bivariate Analysis and Hypothesis testing](#bivariate-analysis)\n7. [NLP Analysis](#nlp-analysis)\n8. [NLP Pre-processing](#nlp-pre-processing)\n    - [Word Cloud](#wordcloud)\n9. [Feature Engineering](#feature-engineering)\n10. [Design, train and test machine learning classifiers](#ml-models)\n    - [All models - Original data](#ml-models-original-data)\n    - [All models - Oversampling data](#ml-models-oversampling-data)\n    - [All models - SMOTE data](#ml-models-smote-data)\n    - [Hyperparameter tuning with original features](#ml-models-hyperparam-tuning)\n    - [Bootstrap Sampling](#ml-models-bootstrap-sampling)\n11. [Design, train and test Neural networks classifiers](#ann-models)\n    - [Convert Classification to Numeric problem](#ann-models-clas-to-num-problem)\n    - [Multiclass classification - Target variable - One hot encoded](#ann-models-multi-class)\n12. [Design, train and test RNN or LSTM classifiers](#nlp-models)\n    - [Creating a Model with Text Inputs Only](#nlp-models-text-input)\n    - [Creating a Model with Categorical features Only](#nlp-models-cat-features)\n    - [Creating a Model with Multiple Inputs](#nlp-models-multiple-input)\n13. [Conclusion](#conclusion)\n14. [Recommendations](#recommendations)\n15. [Limitations](#limitations)\n16. [Closing Reflections](#closing-reflections)\n17. [References](#references)","075ee133":"<a id=\"import-libraries\"><\/a>\n## 2. Import the necessary libraries","b0677815":"#### Train and test all models","75b3470f":"[Table of Contents](#table-of-contents)","8069adf0":"[Table of Contents](#table-of-contents)","f3e4a9bc":"<a id=\"nlp-models-cat-features\"><\/a>\n##### 2. Creating a Model with Categorical features Only\n\nIn this section, we will create a classification model that uses categorical columns alone. Since the data for these columns is well structured and doesn't contain any sequential or spatial pattern, we can use simple densely connected neural networks to make predictions.","23f57232":"##### g. Accident Levels by Weekday - Is the distribution of accident levels and potential accident levels differ significantly in different weekday?","f644c3eb":"<a id=\"ann-models\"><\/a>\n## 11. Design, train and test Neural networks classifiers","ed20c062":"[Table of Contents](#table-of-contents)","4781c6d8":"* Accidents are recorded from 1st Jan 2016 to 9th July 2017 in every month, there are high number of accidents in 2016 and less in 2017.\n* Number of accidents are high in beginning of the year and it keeps decreasing later.","ee50feea":"By comparing the results from all above methods, we can select best method as Ridge classifier with f1-score 62.68% and all other methods are over fitting the training data.","a01b0bf0":"* Number of accidents increased during the middle of the week and declined since the middle of th week.","57ce1cb5":"<a id=\"wordcloud\"><\/a>\n#### WordCloud","eabc914e":"#### Use PCA - Extract Principal Components that capture about 95% of the variance in the data","39c5ca9d":"###### 5. Decide to Reject or Accept Null Hypothesis","6e2040a5":"**Observations**\n\nThere are many body-related, employee related, movement-related, equipment-related and accident-related words.\n\n* Body-related: left, right, hand, finger, face, foot and glove\n* Employee-related: employee, operator, collaborator, assistant, worker and mechanic\n* Movement-related: fall, hit, lift and slip\n* Equipment-related: equipment, pump, meter, drill, truck and tube\n* Accident-related: accident, activity, safety, injury, causing\n","19a66125":"## EDA Summary:","56247dbe":"[Table of Contents](#table-of-contents)","bc494259":"We could see it accuracy continually rise during training. As expected, we see the learning curves for accuracy on the test dataset plateau, indicating that the model has no longer overfit the training dataset and it is generalized model.","a5c1b5b6":"#### Variable Creation - TFIDF Features","b93175b5":"Firstly, let's select TensorFlow version 2.x in colab","845caa9a":"##### c. Industry Sector by Gender - Is the distribution of industry sector differ significantly in different genders?","bbdea863":"We can create holidays variable based on Brazil holidays list from 2016 and 2017.\n\nAnother national holidays are election days. There are a plenty of unofficial ethnic and religious holidays in Brazil. Octoberfest, Brazilian Carnival, Kinderfest, Fenaostra, Fenachopp, Musikfest, Schutzenfest, Kegelfest, Cavalhadas, Oberlandfest, Tirolerfest, Marejada are among them.\n\n**Note**: Considering official holidays only.","2bae666b":"#### Architecture\n\n1. Create a model with Text inputs only.\n2. Create a model with Categorical inputs only.\n3. Create a model with Multiple inputs.","ad2aa8ad":"- 74% of data where accident description > 100 is captured in low accident level.\n- 34% of data where accident description > 100 is captured in high medium potential accident level.\n- 25% of data where accident description > 100 is captured in medium potential accident level.\n- 23% of data where accident description > 100 is captured in low potential accident level. \n- Few of the NLP pre-processing steps taken before applying model on the data\n\n  - Converting to lower case, avoid any capital cases\n  - Converting apostrophe to the standard lexicons\n  - Removing punctuations\n  - Lemmatization\n  - Removing stop words\n\n- After pre-processing steps:\n  - Minimum line length: 64\n  - Maximum line length: 672\n  - Minimum number of words: 10\n  - Maximum number of words: 98","a950cfee":"## Data Cleansing Summary:","c1b50ac3":"<a id=\"ml-models-bootstrap-sampling\"><\/a>\n#### Bootstrap Sampling - RandomForestClassifier","18288dc5":"**Critical Risk**","b0d241a3":"Above one is good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. The loss of the model will almost always be lower on the training dataset than the validation dataset.","741dc7c1":"By comparing the results from all above methods, we can select the best method as AdaBoost classifier with f1-score 65.38%","119a43d9":"* **From the above output, we see that except first column all other columns datatype is object.**\n\n* **Categorical columns - 'Countries', 'Local', 'Industry Sector', 'Accident Level', 'Potential Accident Level', 'Genre', 'Employee or Third Party', 'Critical Risk', 'Description'**\n\n* **Date column - 'Data'**","b0a0b3d4":"* Because most part of the Critical Risks are classified as 'Others', it is thought that there are too many risks to classify precisely.\n\n* And it is also thought that it takes so much time to analyze risks and reasons why the accidents occur.","df152e84":"*\tWe have less number of observations to analyse the cause of accidents correctly and rather we should collect more number of observations to get better results.\n*\tLess number of features available in dataset.\n*\tLack of access to quality data.\n\n**Where does our model fall short in the real world?**\n\n*\tOnce we deploy the finalised model in Production, we might get less f1-score as compared to productionalized model results.\n\n*\tSince we are predicting the accident level, we need to be 100% sure or at least close to 100% so that we can prevent the lot of accidents in industry.\n\n**What can you do to enhance the solution?** -- Need to work on limitations.\n","080a8ba0":"[Table of Contents](#table-of-contents)","ea05983e":"<a id=\"ann-models-multi-class\"><\/a>\n#### Multiclass classification - Target variable - One hot encoded\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and one-hot encoded target variable. We can use simple densely connected neural networks to make predictions.","a03e88d1":"*\tIn this project, we discovered that the main causes of accidents are mistakes in hand-operation and time-related factor.\n\n*\tTo reduce the occurrences of accidents, more stringent safety standards in hand-operation will be needed in period when many accidents occur.\n\n*\tI realized that the detail information of accidents like 'Description' is so useful to analyze the cause.\n\n*\tWith more detailed information such as machining data (ex. CNC, Current, Voltage) in plants, weather information, employee's personal data (ex. age, experience in the industry sector, work performance ), we can clarify the cause of accidents more correctly.\n\n*\tWith more number of observations than current number of records = 425 so that we can feed more data into ML\/ANN\/NLP models to train, evaluate the performance of those models and get the better results.\n\n*\tThere are quite a lot of Critical risk descriptions, but with the help of SME we can decide whether this column has outliers or not and also SME can help us in understanding the data better.\n","e66b9249":"<a id=\"nlp-models-multiple-input\"><\/a>\n##### 3. Creating a Model with Multiple Inputs\n\nThe first submodel will accept textual input in the form of accident description. This submodel will consist of an input shape layer, an embedding layer, and bidirectional LSTM layer of 128 neurons followed by max pool layer, drop out and dense layers. The second submodel will accept input in the form of meta information which consists of dense, batch norm and drop out layers.\n\nThe output from the dropout layer of the first submodel and the output from the batch norm layer of the second submodel will be concatenated together and will be used as concatenated input to another dense layer with 10 neurons. Finally, the output dense layer will have five neuorns corresponding to each accident level.\n","195e679a":"[Table of Contents](#table-of-contents)","9347b97d":"#### NLP text summary statistics","db6f8b39":"<a id=\"ml-models-oversampling-data\"><\/a>\n#### All models - Oversampling data","f9fe0f49":"###### 1. State the H0 and Ha\n\n###### Ho = The proportions of own employees in each gender is equal.\n###### Ha = The proportions of own employees in each gender is not equal.\n\n###### 2. Decide the significance level: alpha = 0.05\n\n###### 3. Identify the test-statistic: Z-test of proportions\n\n###### 4. Calculate the p_value using test-statistic","50a0b0b4":"[Table of Contents](#table-of-contents)","9b531d53":"**Observations**\n\n* Proportion of accident levels in each gender is not equal and males have a higher accident levels than females.\n* There are many low risks at general accident level, but many high risks at potential accident level.","600e54e8":"Above one is underfit model, it can be identified from the learning curve of the training loss only. It is showing noisy values of relatively high loss, indicating that the model was unable to learn the training dataset at all and model does not have a suitable capacity for the complexity of the dataset.","b3d35274":"<a id=\"nlp-models-text-input\"><\/a>\n##### 1. Creating a Model with Text Inputs Only\n\nIn this section, we will create a classification model that uses accident description column alone.","07a569a9":"<a id=\"recommendations\"><\/a>\n## 14. Recommendations","a0ac9aa4":"### Context:\n\nThe database comes from one of the biggest industry in Brazil and in the world. It is an urgent need for industries\/companies around the\nglobe to understand why employees still suffer some injuries\/accidents in plants. Sometimes they also die in such environment.","5a7b99b0":"**Observations**\n\n* WeekofYear featuer is having very high positive correlation with Month feature.","30d50122":"<a id=\"feature-engineering\"><\/a>\n## 9. Feature Engineering","9dfbb4e0":"[Table of Contents](#table-of-contents)","27404953":"#### Here we can use the DummyClassifier to predict all accident levels just to show how misleading accuracy can be.","917d622a":"**What did we learned from the process?**\n\n*\tHow to work on Data Science project to end-to-end.\n*\tHow to handle class imbalance data set.\n*\tHow to build different ANN model architectures for handling multi-class classification problems.\n*\tHow to build different NLP architectures for handling both numerical and text data.\n\n**What I do differently next time?**\n\n*\tPerhaps I will explore more feature engineering and feature selection techniques.\n*\tI will build the real chatbot using Streamlit or some other applications.\n","91a4cb8e":"[Table of Contents](#table-of-contents)","1d0b4d77":"#### Variable Identification","a884193a":"#### Get the Length of each line and find the maximum length\n\nAs different lines are of different length. We need to pad the our sequences using the max length.","6c809c1c":"* 59% accidents occurred in Country_01\n* 31% accidents occurred in Country_02\n* 10% accidents occurred in Country_03\n\n","23a550dc":"Above one is good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. The loss of the model will almost always be lower on the training dataset than the validation dataset.","9c587025":"<a id=\"univariate-analysis\"><\/a>\n#### Univariate Analysis","6d27276a":"Above one is good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. The loss of the model will almost always be lower on the training dataset than the validation dataset.","80bf8074":"#### Variable Creation - Glove Word Embeddings","75fc0a6c":"#### Variable Creation - Label Encoding","c027618a":"[Table of Contents](#table-of-contents)","63f450da":"#### Drop Duplicates","7677a072":"#### 1. Modelling - Logistic Regression","4aaf00a2":"#### Bootstrap Sampling - XGBClassifier","860092a0":"#### Varible Tansformation (Normalization and Scaling)","23e713c5":"Hence we fail to reject Null Hypothesis, we have enough (95%) evidence to prove that, the proportion of own employees in each gender is equal.","a2047cff":"**Accident Levels**","8a707990":"#### 4. Modelling - Logistic Regression - SMOTE","e4ced2f9":"**Observations**\n\n* Metals and Mining industry sector plants are not available in Country_03.\n* Distribution of industry sector differ significantly in each country. But let's check the proportion of metals, mining and others sector in Country_01 and is that difference is statistically significant?","6e9f3bef":"\n**Observations**\n\n- 34% of data where accident description > 100 is captured in high medium potential accident level.\n- 25% of data where accident description > 100 is captured in medium potential accident level.\n- 23% of data where accident description > 100 is captured in low potential accident level. \n- Based on some random headlines seen above, it appears that the data is mostly lower-cased. Pre-processing such as removing punctuations and lemmatization can be used.\n- There are few alphanumeric characters like AFO-755 captured in description where removing these characters might help.\n- There are digits in the description for e.g. ditch 3570, 0.50 cm deep, 30 kg where removing the digits wouldn't help.","c0722e52":"[Table of Contents](#table-of-contents)","c6e9067f":"<a id=\"ml-models-smote-data\"><\/a>\n#### All models - SMOTE data","6d3ef3fa":"Above one is overfit model, it can be identified from the learning curve of the training and validation loss only.","975e61f3":"* Highest manufacturing plants are located in Local_03 city.\n* Lowest manufacturing plants are located in Local_09 city.","b1bb381b":"[Table of Contents](#table-of-contents)","c7fe5eb1":"<a id=\"bivariate-analysis\"><\/a>\n#### Bivariate Analysis and Hypothesis testing","a8c86533":"<a id=\"ann-models-clas-to-num-problem\"><\/a>\n#### Convert Classification to Numeric problem\n\nIn this section, we will create a classification model that uses categorical columns and tf-idf features from accident description and label encoded target variable. We can use simple densely connected neural networks to make predictions.\n\nSince we have ordinal relationship between each category in target variable, I have considered this one as numerical\/regression problem and try to observe the ANN behaviour.\n","50a84c45":"#### Check Outliers\n\nAs we know, there is no concept of outliers detection in categorical variables(nominal and ordinal), as each value is count as labels. Let's check the unique and frequency(mode) of each variable.","795db49f":"#### Train and test model","c4782ca6":"##### h. Accident Levels by Seasons - Is the distribution of accident levels and potential accident levels differ significantly in different seasons?","b8129343":"<a id=\"data-collection\"><\/a>\n## 3. Data Collection","9c1a148b":"<a id=\"ml-models\"><\/a>\n## 10. Design, train and test machine learning classifiers","9bf11555":"* There are more men working in this industry as compared to women.","0ab01c88":"* 57% manufacturing plants belongs to Mining sector.\n* 32% manufacturing plants belongs to Metals sector.\n* 11% manufacturing plants belongs to Others sector.","606e2a25":"**Employee type**","27e6acd9":"* There is no need to worry about preserving the data; it is already a part of the industry dataset and we can merely remove or drop these rows from your cleaned data","687dac08":"* Number of accidents are very high in particular days like 4, 8 and 16 in every month.","75390197":"Above one is good fit, it is identified by a training and validation loss that decreases to a point of stability with a minimal gap between the two final loss values. The loss of the model will almost always be lower on the training dataset than the validation dataset.","2f49840b":"<a id=\"nlp-analysis\"><\/a>\n## 7. NLP Analysis","cf277180":"**Observations**\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, and some of these levels increased slightly in the second half of the year.","c9e4d41a":"[Table of Contents](#table-of-contents)","eb722e70":"We could see it accuracy continually rise during training. As expected, we see the learning curves for accuracy on the test dataset plateau, indicating that the model has no longer overfit the training dataset and it is generalized model.","a6dc0765":"[Table of Contents](#table-of-contents)","378839d4":"#### Get ANN Multiclass Classification Metrics","ccd722f5":"**Observations**\n\n* Proportion of Metals sector employees in each gender is not equal.\n* Proportion of Mining sector employees in each gender is not equal.\n* Proportion of Others sector employees in each gender is not equal.","51c17584":"1. Removed 'Unnamed: 0' column and renamed - 'Data', 'Countries', 'Genre', 'Employee or Third Party' columns in the dataset.\n2. We had 7 duplicate instances in the dataset and dropped those duplicates.\n3. There are no outliers in the dataset.\n4. No missing values in dataset.\n5. We are left with 418 rows and 10 columns after data cleansing.","80570907":"[Table of Contents](#table-of-contents)","86d5e486":"<a id=\"eda\"><\/a>\n## 6. EDA (Data Analysis and Preparation)","262c5ed6":"* Hence we reject Null Hypothesis, we have enough (95%) evidence to prove that, the mining sector in country 1 is differ from metals sector)\n\n* Hence we reject Null Hypothesis, we have enough (95%) evidence to prove that, the mining sector in country 1 is differ from others sector)","01b8cbea":"[Table of Contents](#table-of-contents)","16a1c556":"## NLP Pre-processing Summary:","bdfdf7fa":"**Industry Sector**","88d39251":"1.\tAble to predict the accident level with a test accuracy of 73.81% and f1-score of 73.81%\n2.\tWe have seven duplicate values in this dataset and dropped those duplicate values.\n3.\tWe have no outliers in this dataset.\n4.\tWe have no missing values in this dataset.\n5.\tExtracted the day, month and year from Date column and created new features such as weekday, weekofyear and seasons.\n6.\tTarget variable \u2013 \u2018Accident Level\u2019 distribution is not equal (I: 309, II: 40, III: 31, IV: 30, V: 8).\n7.\tClass imbalance issue is handled using below methods and found out that, for this particular dataset, with original data we have achieved the better results.\n\n  a.\tResampling techniques: Oversampling minority class\n\n  b.\tSMOTE: Generate synthetic samples\n8.\tBy comparing the results from all ML methods with original data, we can select the best method as AdaBoost classifier with f1-score 65.38% with original data.\n\n9. **Bootstrap sampling with RandomForestClassifier model with an accuracy of 66.7% - 77.6% is our best model.**\n\n10. Bootstrap sampling with AdaBoostClassifier model with an accuracy of 51.7% - 76.8% is our best model.\n\n11. Bootstrap sampling with XGBClassifier model with an accuracy of 63.5% - 74.8% is our best model.\n\n12.\tExplored below options in Neural Networks.\n\n  a.\tConvert Classification to Numerical problem: achieved a test accuracy of 53.57% which is a bad result.\n\n  b.\tMulticlass classification - Target variable - One hot encoded: achieved a test accuracy of 73.81% and f1-score of 73.81% with original data + TF-IDF features from accident description column.\n\n  c.\tCreate a model with Text inputs (accident description alone) only: surprisingly achieved a test accuracy of 73.81% and f1-score of 73.81% with original data.\n\n  d.\tCreate a model with Categorical features only: achieved a test accuracy of 73.81% and f1-score of 72.28% with original data.\n\n  e.\tCreate a model with Multiple Inputs (concatenated the layers from text input model and categorical features input model): surprisingly achieved a test accuracy of 73.81% and f1-score of 73.81% with original data.\n\n13.\t**Finally bidirectional LSTM model can be considered to productionalized the model and predict the accident level.**\n","59c0a59f":"##### a. Industry Sector by Countries - Is the distribution of industry sector different significantly in differ countries or not?","407de3f2":"#### Check Missing Values","25dc9a16":"[Table of Contents](#table-of-contents)","69aab427":"###### 5. Decide to Reject or Accept Null Hypothesis","5423ad8f":"<a id=\"limitations\"><\/a>\n## 15. Limitations","c00474b8":"### Data Description:\n\nThis The database is basically records of accidents from 12 different plants in 03 different countries which every line in the data is an\noccurrence of an accident.\n\n**Columns description:**\n\n- **Data**: timestamp or time\/date information\n- **Countries**: which country the accident occurred (anonymised)\n- **Local**: the city where the manufacturing plant is located (anonymised)\n- **Industry sector**: which sector the plant belongs to\n- **Accident level**: from I to VI, it registers how severe was the accident (I means not severe but VI means very severe)\n- **Potential Accident Level**: Depending on the Accident Level, the database also registers how severe the accident could have been (due to other factors\ninvolved in the accident)\n- **Genre**: if the person is male of female\n- **Employee or Third Party**: if the injured person is an employee or a third party\n- **Critical Risk**: some description of the risk involved in the accident\n- **Description**: Detailed description of how the accident happened.\n\nLink to download the dataset: https:\/\/www.kaggle.com\/ihmstefanini\/industrial-safety-and-health-analytics-database","f5ca2238":"##### f. Accident Levels by Month - Is the distribution of accident levels and potential accident levels differ significantly in different months?","81ed33b3":"**Observations**\n\n* Both of the two accident level have the tendency that non-severe levels decreased throughout the year, but severe levels did not changed much, and some of these levels increased slightly in the second half of the year.","16ec3cce":"**Local**","057dca5d":"<a id=\"closing-reflections\"><\/a>\n## 16. Closing Reflections","8ab15cdc":"## Data Collection Summary:","6f663f21":"<a id=\"ml-models-original-data\"><\/a>\n#### All models - Original data","9fd5aa4e":"#### Combine TFIDF and Encoded Features","6b0b7f50":"We could see it accuracy continually rise during training. As expected, we see the learning curves for accuracy on the test dataset plateau, indicating that the model has no longer overfit the training dataset and it is generalized model.\n\n**Note: Surprisingly we observe that same f1-score = 73.89 % with accident description alone.**\n","c4be2d72":"# AIML Capstone Project - Industrial Safety and Health Analytics Database","92e07adb":"#### Check Duplicates","7a96c7d2":"By comparing the results from all above methods, all are over fitting the training data.","d8f11d8f":"<a id=\"overview\"><\/a>\n## 1. Overview","70558a24":"#### Sampling Techniques - Create Training and Test Set","4f60cd25":"#### Setting Options"}}