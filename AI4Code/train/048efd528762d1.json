{"cell_type":{"5c72ed6c":"code","e788eb40":"code","0b4fb252":"code","6fe64d1e":"code","40c7cacb":"code","05aafa1d":"code","455f0c81":"code","259eab38":"code","f00a979a":"code","5e8e0871":"code","1323439a":"code","3639fe1c":"code","8ab6f007":"code","e4cbdecf":"code","ca5c9489":"code","b268c4e3":"code","29fa769a":"markdown","e64b4757":"markdown","241b2407":"markdown","14eb8fc9":"markdown","f12cb7c1":"markdown","afef106f":"markdown","7c295b9b":"markdown","3f847bd1":"markdown","79608bf0":"markdown"},"source":{"5c72ed6c":"!wget -P \/kaggle\/working\/data\/ http:\/\/nlp.stanford.edu\/data\/glove.6B.zip\n!unzip \/kaggle\/working\/data\/glove.6B.zip -d \/kaggle\/working\/data\/\n!head .\/data\/glove.6B.50d.txt","e788eb40":"import re\n\nimport tensorflow as tf\nimport keras\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm","0b4fb252":"EPOCHS = 2\nBATCH_SIZE = 128\n\nMAX_LEN = 128\nNUM_WORDS = 10000\n\nEMBEDDING_DIM = 50\nH1 = 32\n\nTHRESH = 0.5","6fe64d1e":"train_data=pd.read_csv('\/kaggle\/input\/fake-news\/train.csv')\ntest_data=pd.read_csv('\/kaggle\/input\/fake-news\/test.csv')\ntrain_data.loc[train_data[\"text\"].isnull(), \"text\"] = \"\"\ntest_data.loc[test_data[\"text\"].isnull(), \"text\"] = \"\"\nprint(train_data.shape, test_data.shape)\ntrain_data.head()","40c7cacb":"train_data[\"text\"] = train_data[\"text\"].map(lambda x: re.sub(\"([^a-zA-Z0-9\\s])\", r' \\1 ', x))\ntrain_data[\"text\"] = train_data[\"text\"].map(lambda x: re.sub(\"\\s+\", r' ', x))\ntest_data[\"text\"] = test_data[\"text\"].map(lambda x: re.sub(\"([^a-zA-Z0-9\\s])\", r' \\1 ', x))\ntest_data[\"text\"] = test_data[\"text\"].map(lambda x: re.sub(\"\\s+\", r' ', x))\ntrain_data.head()","05aafa1d":"word_vec = pd.read_table(\".\/data\/glove.6B.50d.txt\", sep=r\"\\s\", header=None)\nword_vec.set_index(0, inplace=True)\nword_vec.head()","455f0c81":"tokenizer = tf.keras.preprocessing.text.Tokenizer(\n    filters=\"\",\n    num_words=10000,\n    lower=True,\n    document_count=5,\n)\ntokenizer.fit_on_texts(train_data[\"text\"].values)","259eab38":"train_tokens = tokenizer.texts_to_sequences(train_data[\"text\"])\ntest_tokens = tokenizer.texts_to_sequences(test_data[\"text\"])","f00a979a":"tokenizer.index_word[9999], min(tokenizer.index_word.keys())","5e8e0871":"words_used = [tokenizer.index_word[i] for i in range(1, 10000)]\nmissing_words = set(words_used) - set(word_vec.index.values)\nprint(len(missing_words))\nmissing_word_index = [tokenizer.word_index[word] for word in missing_words]","1323439a":"train_tokens = [[word for word in sentence if word not in missing_word_index] for sentence in train_tokens]\ntest_tokens = [[word for word in sentence if word not in missing_word_index] for sentence in test_tokens]","3639fe1c":"train_X = keras.preprocessing.sequence.pad_sequences(train_tokens, maxlen = MAX_LEN)\ntest_X = keras.preprocessing.sequence.pad_sequences(test_tokens, maxlen = MAX_LEN)\ntrain_Y = train_data[\"label\"].values","8ab6f007":"embedding_weights = np.zeros((10000, 50))\nindex_n_word = [(i, tokenizer.index_word[i]) for i in range(1, len(embedding_weights)) if tokenizer.index_word[i] in word_vec.index]\nidx, word = zip(*index_n_word)\nembedding_weights[idx, :] = word_vec.loc[word,:].values","e4cbdecf":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(tokenizer.num_words, \n                                 EMBEDDING_DIM, \n                                 weights=[embedding_weights],\n                                 trainable=False\n                                )) # , batch_size=batch_size\nmodel.add(keras.layers.LSTM(H1))\nmodel.add(keras.layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.summary()","ca5c9489":"model.fit(train_X, train_Y, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_split=0.1)","b268c4e3":"test_y = model.predict(test_X) > THRESH\ntest_data[\"label\"] = test_y.astype(np.int)\ntest_data[[\"id\", \"label\"]].to_csv(\"submission.csv\", index=False)","29fa769a":"Get the missing words in our glove vectors.","e64b4757":"We are required to convert words to numbers. We do this by creating a mapping of word to integer. eg. `{\"the\": 1, \"I\": 2, \"am\": 3, ...}`. `tf.keras.preprocessing.text.Tokenizer` does this for us.","241b2407":"## Model\nConsidering all the tokenized numbers above are effectively categories, we need to pass this through an embedding layer to get the embedding. In this case each word is represented by `EMBEDDING_DIM` numbers. This is then passed through an RNN layer before passing through a final feed forward layer to calculate the probability.\n\nIf you wish to understand LSTMs at a mathematical level this is an amazing blog post: https:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/","14eb8fc9":"Note that `fit_on_texts` only creates the mapping while `texts_to_sequences` below creates the actual tokens.","f12cb7c1":"## Shameless Self Promotion\nSee here for [my course](https:\/\/www.udemy.com\/course\/machine-learning-and-data-science-2021\/?referralCode=E79228C7436D74315787) on Machine Learning and Deep Learning (Use code DEEPSCHOOL-MARCH to 85% off).","afef106f":"## Submission","7c295b9b":"Delete any of the above 'missing words'","3f847bd1":"We cannot send in different shaped sequences in when we are doing batch processing. Therefore the data needs to be padded with zeros so that all sequences are of length `MAX_LEN`.","79608bf0":"## Data\nThis section talks about grabbing and pre processing the dataset."}}