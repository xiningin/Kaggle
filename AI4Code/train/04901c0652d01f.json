{"cell_type":{"b99785e8":"code","49238f50":"code","d5959165":"code","84a161d1":"code","99b620a4":"code","cb2049cf":"code","a9de42f1":"code","4faa5670":"code","e5cf0543":"code","37952477":"code","dc83f66f":"code","620b7dc5":"code","6ba7a174":"code","9bf8b3fb":"code","3cf5c320":"code","0f4e7b09":"code","49e142a5":"code","11d8bc04":"code","81ef54f9":"code","ac6f926a":"code","0f3e1424":"code","c143fc24":"code","1c128efb":"code","48c4619c":"code","7fb12c7e":"code","72719b36":"code","23d80dc6":"code","00270fd4":"code","8a9c08b9":"code","bb0e8926":"code","d88fe078":"code","185341be":"code","d6d27f8b":"code","9a9986cb":"code","28e12d13":"code","d25f0b8a":"code","5159bc02":"code","6f71b643":"code","16d92346":"code","b52fe241":"code","72a4a19e":"code","e49216b4":"code","f91e9c3a":"code","b12aa7ce":"code","ad484041":"code","4de58580":"code","4d356e5f":"markdown","d87d8631":"markdown","7720a30a":"markdown","6771938d":"markdown","9ca40947":"markdown","f86a93b3":"markdown","e5c2f871":"markdown","35e6c9f5":"markdown","b85c7603":"markdown","c3e3c9df":"markdown","531863f2":"markdown","02e63f28":"markdown","4d8628f0":"markdown","06cf9f76":"markdown","4e436ef8":"markdown","2168341d":"markdown","268246e7":"markdown","e3932beb":"markdown","5adbcee1":"markdown","840b13a6":"markdown","b6349468":"markdown","b82c52a0":"markdown","bf0776ab":"markdown"},"source":{"b99785e8":"pip install pyspark","49238f50":"pip install findspark","d5959165":"import findspark\nfindspark.init","84a161d1":"from pyspark.sql import SparkSession # required to created a dataframe\nspark=SparkSession.builder.appName(\"Basics\").getOrCreate() \n\nimport pyspark.sql.functions","99b620a4":"df=spark.read.csv(\"..\/input\/iris\/Iris.csv\",header=True)  \ndf.show(10)","cb2049cf":"df.printSchema()","a9de42f1":"from pyspark.sql.types import IntegerType\ndf = df.withColumn(\"SepalLengthCm\", df[\"SepalLengthCm\"].cast(IntegerType()))\ndf = df.withColumn(\"SepalWidthCm\", df[\"SepalWidthCm\"].cast(IntegerType()))\ndf = df.withColumn(\"PetalLengthCm\", df[\"PetalLengthCm\"].cast(IntegerType()))\ndf = df.withColumn(\"PetalWidthCm\", df[\"PetalWidthCm\"].cast(IntegerType()))","4faa5670":"df.printSchema()","e5cf0543":"df.groupBy(\"Species\").mean(\"SepalWidthCm\").show()","37952477":"df.groupBy(\"Species\").avg(\"SepalWidthCm\").show()","dc83f66f":"df.groupBy(\"Species\").max(\"SepalWidthCm\").show()","620b7dc5":"df.groupBy(\"Species\").min(\"SepalWidthCm\").show()","6ba7a174":"df.groupBy(\"Species\").count().show()","9bf8b3fb":"df.show(5)","3cf5c320":"pivotDF = df.groupBy(\"Species\").pivot(\"SepalWidthCm\").sum(\"SepalLengthCm\")\npivotDF.show()\n\n#group by one column, pivot or transpose next column and the values will be the agg of the specified column","0f4e7b09":"#method 1\nrdd = spark.sparkContext.textFile(\"Iris.csv\",5).map(lambda line: line.split(\",\"))\n","49e142a5":"#method 2 :parallelize is use only when data is present in the memory\ndata = [1,2,3,4,5,6,7,8,9,10,11,12]\nrdd1=spark.sparkContext.parallelize(data)","11d8bc04":"re_rdd=rdd.repartition(5)\n","81ef54f9":"co_rdd=rdd.coalesce(2)\n","ac6f926a":"flatmap_rdd=rdd.flatMap(lambda x: x.split(\"-\"))","0f3e1424":"map_rdd=rdd.map(lambda x,y: x+y)","c143fc24":"map_rdd2=rdd.map(lambda x:(x,2))","1c128efb":"map_rdd3=rdd.map(lambda x: x**3)","48c4619c":"filter_rdd = rdd.filter(lambda x : 'an' in x[1])\n","7fb12c7e":"a=10\nbroadcastVar = spark.sparkContext.broadcast(a)\nbroadcastVar.value","72719b36":"IN=\"INDIA\"\nbcv=spark.sparkContext.broadcast(IN)\nbcv.value","23d80dc6":"#can be used on filters. Just an example of the syntax\nfilteDf= df.where((df['Species'].isin(broadcastVar.value)))\nfilteDf.show()","00270fd4":"#Example 1:\ndf.createOrReplaceTempView(\"flowers\")\ndf.show(3)","8a9c08b9":"df2 = spark.sql(\"SELECT * from flowers where Species='Iris-setosa'\")\ndf2.show(5)","bb0e8926":"df.printSchema()","d88fe078":"#1\ndf.select(\"ID\").show(2)","185341be":"#2.\ndf.select(\"SepalLengthCm\",\"SepalWidthCm\").show(2)","d6d27f8b":"#3.\ndf.select(df.columns[2:4]).show(3)\n","9a9986cb":"data_collect=df.collect()\n","28e12d13":"#1. Casting\ndf=df.withColumn(\"ID\",df[\"ID\"].cast(\"Integer\"))\ndf.printSchema()","d25f0b8a":"#2. update value of existing column\ndf=df.withColumn(\"ID\",df[\"ID\"]*10)\ndf.show(5)","5159bc02":"#3. New column from an existing column\ndf=df.withColumn(\"New Column\",df[\"ID\"]-5)\ndf.show(5)","6f71b643":"#4.\ndf.withColumnRenamed(\"ID\",\"No\").show(5)","16d92346":"df=df.drop(\"New Column\")\ndf.show(1)","b52fe241":"distinctdf = df.distinct()\nprint(\"Distinct count: \"+str(distinctdf.count()))\ndistinctdf.show(5)","72a4a19e":"#1\ndf11=df.dropDuplicates()\nprint(\"Distinct count:\" +str(df11.count()))\nprint(\"\\n\")\ndf11.show(5)","e49216b4":"#2\ndf12=df.dropDuplicates([\"Species\"]) #can take more than one column\nprint(\"Distinct count: \" +str(df12.count()))\ndf12.show()","f91e9c3a":"#1\ndf.sort(\"Species\",\"SepalLengthCm\").show(5)","b12aa7ce":"#2\ndf.sort(df.Species.asc(),df.SepalLengthCm.desc()).show(5)","ad484041":"#1\ndf.orderBy(\"Species\",\"SepalLengthCm\").show(5)","4de58580":"#2\ndf.orderBy(df.Species.asc(),df.SepalLengthCm.desc()).show(5)","4d356e5f":"## SELECT () function","d87d8631":"## Drop Column","7720a30a":"### SQL module","6771938d":"## Repartition methods","9ca40947":"#### Groupby with Pivot","f86a93b3":"## WITHCOLUMN()","e5c2f871":"## Import Spark Session and required libraries","35e6c9f5":"#### I have taken a sample data set to understand the syntax","b85c7603":"## change the column types to integer so that we can perform operations","c3e3c9df":"## COLLECT () function","531863f2":"##  GROUP BY with aggregate functions","02e63f28":"### 2. map()","4d8628f0":"### 1. flatMap()","06cf9f76":"## OrderBy()","4e436ef8":"## Transformations","2168341d":"## sort()","268246e7":"### Create tables","e3932beb":"## Create an RDD","5adbcee1":"## dropDuplicates()","840b13a6":"## Broadcast variable","b6349468":"### 3. filter()","b82c52a0":"## Read the input file and store in a dataframe","bf0776ab":"## Distinct ()"}}