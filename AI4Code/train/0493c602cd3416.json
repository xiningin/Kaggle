{"cell_type":{"5552da29":"code","1333d5ba":"code","35799522":"code","ec28c06e":"code","8e6e3dd0":"code","433fa2e0":"code","ed98b8dc":"code","24bc2ac8":"code","ad89725f":"code","371af331":"code","5ea119d6":"code","8684db92":"code","7bdbcf2d":"code","7a8b55ad":"code","8d852e57":"code","62fba237":"code","190935c5":"code","7c915d6e":"code","938f6b77":"code","4700fc72":"code","4e5878c5":"code","d9b68020":"code","baa3f4d6":"code","a94ff1bd":"code","105ea983":"code","e29d6307":"code","580a670f":"code","c7994ae2":"code","894981a3":"code","9e0c179a":"code","3e6d5395":"code","b0d4b70a":"code","5ae4cf93":"code","703f427a":"code","2ddc770f":"code","131fc50a":"code","0d761a22":"code","a6c45762":"code","9c2ecd92":"code","d1089078":"code","a2249ef8":"code","6ff73cd2":"code","6471827f":"code","528b62e7":"code","2eae776b":"code","079c540e":"code","2e26fc3d":"code","4cda0bf1":"code","bdd6ea80":"code","0a215faf":"code","fa67fe6c":"code","217ec3a3":"code","03ea7539":"code","90975c36":"code","4a7f2184":"code","bba54c9e":"code","2922865c":"code","374347fd":"markdown","fd0f9bf9":"markdown","3d675d40":"markdown","9d0f7314":"markdown","011706e9":"markdown","9d4a427b":"markdown","6e527398":"markdown","989521a2":"markdown","a4c9c712":"markdown","a91d4a72":"markdown","e08c2b00":"markdown","42058e1b":"markdown","c9c30e40":"markdown","72576be8":"markdown","53ca129f":"markdown","2383a7cb":"markdown","5b65f6ce":"markdown","9c1345ce":"markdown","8aadec0c":"markdown","a7b62985":"markdown","2ad77893":"markdown","a23269fa":"markdown","37d7dca8":"markdown","e37feeec":"markdown","7c11b264":"markdown","9afca81a":"markdown","b26d70ab":"markdown","4c97696d":"markdown","2f4901dd":"markdown","d2b9a421":"markdown","d480d473":"markdown","369749ec":"markdown","68236636":"markdown","563804fd":"markdown","be9704fa":"markdown"},"source":{"5552da29":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\npd.set_option('display.max_columns', 40)\npd.set_option('display.max_colwidth', 120)","1333d5ba":"df = pd.read_csv('..\/input\/organics.csv')","35799522":"df.shape","ec28c06e":"df.head()","8e6e3dd0":"df.columns","433fa2e0":"df.describe()","ed98b8dc":"df.dtypes","24bc2ac8":"df.groupby('Gender').size()","ad89725f":"print('Gender',df['Gender'].unique())\nprint('Geographic Region',df['Geographic Region'].unique())\nprint('Loyalty Status',df['Loyalty Status'].unique())\nprint('Neighborhood Cluster-7 Level',df['Neighborhood Cluster-7 Level'].unique())\nprint('Television Region',df['Television Region'].unique())\nprint('Affluence Grade',df['Affluence Grade'].unique())\nprint('Age',df['Age'].unique())\nprint('Frequency Percent',df['Frequency Percent'].unique())\nprint('Loyalty Card Tenure',df['Loyalty Card Tenure'].unique())","371af331":"i = 0\nfor x in df['Gender']:\n    if x == 'U':\n        df.iloc[i, df.columns.get_loc('Gender')] = np.NaN\n    i = i + 1","5ea119d6":"i = 0\nfor x in df['Affluence Grade']:\n    try:\n        df.iloc[i, df.columns.get_loc('Affluence Grade')] = int(x)\n    except:\n        df.iloc[i, df.columns.get_loc('Affluence Grade')] = np.NaN\n    finally:\n        i = i + 1","8684db92":"i = 0\nfor x in df['Age']:\n    try:\n        df.iloc[i, df.columns.get_loc('Age')] = int(x)\n    except:\n        df.iloc[i, df.columns.get_loc('Age')] = np.NaN\n    finally:\n        i = i + 1","7bdbcf2d":"i = 0\nfor x in df['Loyalty Card Tenure']:\n    try:\n        df.iloc[i, df.columns.get_loc('Loyalty Card Tenure')] = int(x)\n    except:\n        df.iloc[i, df.columns.get_loc('Loyalty Card Tenure')] = np.NaN\n    finally:\n        i = i + 1","7a8b55ad":"print('Gender',df['Gender'].unique())\nprint('Geographic Region',df['Geographic Region'].unique())\nprint('Loyalty Status',df['Loyalty Status'].unique())\nprint('Neighborhood Cluster-7 Level',df['Neighborhood Cluster-7 Level'].unique())\nprint('Television Region',df['Television Region'].unique())\nprint('Affluence Grade',df['Affluence Grade'].unique())\nprint('Age',df['Age'].unique())\nprint('Frequency Percent',df['Frequency Percent'].unique())\nprint('Loyalty Card Tenure',df['Loyalty Card Tenure'].unique())","8d852e57":"df.isnull().any()\ndf.isnull().sum(axis=0)","62fba237":"df.dropna(subset=['Gender','Age','Affluence Grade'],inplace=True)","190935c5":"df.isnull().any()\ndf.isnull().sum(axis=0)","7c915d6e":"df.shape","938f6b77":"col_name = df.columns\ncol_name","4700fc72":"from sklearn.impute import SimpleImputer\nimp = SimpleImputer(strategy=\"most_frequent\")\ntemp_arr = imp.fit_transform(df)","4e5878c5":"df2 = pd.DataFrame.from_records(data=temp_arr,columns=col_name)","d9b68020":"df2.sample(10)","baa3f4d6":"df2.isnull().any()\ndf2.isnull().sum(axis=0)","a94ff1bd":"print('Gender',df2['Gender'].unique())\nprint('Geographic Region',df2['Geographic Region'].unique())\nprint('Loyalty Status',df2['Loyalty Status'].unique())\nprint('Neighborhood Cluster-7 Level',df2['Neighborhood Cluster-7 Level'].unique())\nprint('Television Region',df2['Television Region'].unique())\nprint('Affluence Grade',df2['Affluence Grade'].unique())\nprint('Age',df2['Age'].unique())\nprint('Frequency Percent',df2['Frequency Percent'].unique())\nprint('Loyalty Card Tenure',df2['Loyalty Card Tenure'].unique())","105ea983":"df2.rename(inplace=True, columns={'Organics Purchase Indicator': 'ORGANICS'}) \ndf2.columns","e29d6307":"df2.groupby('ORGANICS').hist(figsize=(20,20))\nplt.show()\nplt.close()","580a670f":"df2.drop(['Customer Loyalty ID','Organics Purchase Count'], axis=1, inplace=True)\ndf2.head()","c7994ae2":"df2.drop(['Frequency','Frequency Percent'], axis=1, inplace=True)\ndf2.head()","894981a3":"df2.dtypes","9e0c179a":"df3 = pd.get_dummies(data=df2, columns=['Gender','Geographic Region','Loyalty Status',\n                                        'Neighborhood Cluster-7 Level','Television Region'])","3e6d5395":"df3.shape","b0d4b70a":"df3.head()","5ae4cf93":"df3.columns","703f427a":"df3 = df3[['Neigborhood Cluster-55 Level', 'Affluence Grade', 'Age',\n       'Loyalty Card Tenure', 'Total Spend', 'Gender_F',\n       'Gender_M', 'Geographic Region_Midlands', 'Geographic Region_North',\n       'Geographic Region_Scottish', 'Geographic Region_South East',\n       'Geographic Region_South West', 'Loyalty Status_Gold',\n       'Loyalty Status_Platinum', 'Loyalty Status_Silver',\n       'Loyalty Status_Tin', 'Neighborhood Cluster-7 Level_A',\n       'Neighborhood Cluster-7 Level_B', 'Neighborhood Cluster-7 Level_C',\n       'Neighborhood Cluster-7 Level_D', 'Neighborhood Cluster-7 Level_E',\n       'Neighborhood Cluster-7 Level_F', 'Neighborhood Cluster-7 Level_U',\n       'Television Region_Border', 'Television Region_C Scotland',\n       'Television Region_East', 'Television Region_London',\n       'Television Region_Midlands', 'Television Region_N East',\n       'Television Region_N Scot', 'Television Region_N West',\n       'Television Region_S & S East', 'Television Region_S West',\n       'Television Region_Ulster', 'Television Region_Wales & West',\n       'Television Region_Yorkshire','ORGANICS']]","2ddc770f":"df3.head()","131fc50a":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.linear_model import Ridge\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\n# Build a list with all models\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC()))\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('DT', DecisionTreeClassifier()))\nmodels.append(('GNB', GaussianNB()))\nmodels.append(('RF', RandomForestClassifier()))\nmodels.append(('GB', GradientBoostingClassifier()))","0d761a22":"features = df3.columns[:-1]\nx_m = df3[features]\ny_m = df3.ORGANICS","a6c45762":"from sklearn.model_selection import train_test_split\n\nX_F_TRAIN, X_F_TEST, Y_F_TRAIN, Y_F_TEST = train_test_split(\n    x_m, y_m, test_size=0.2, stratify = y_m, random_state=42)","9c2ecd92":"main_df = pd.DataFrame(columns=['Num_Of_Feature','Features_Sel','KNN','SVM','LR','DT','GNB','RF','GB'])\nmodel_feat = LogisticRegression()\n\ns_highest = 0\n\nfor n in range(3,38):\n    print(\"Running selecting \", n ,\" features to run on all models...\" )\n    rfe = RFE(model_feat, n)\n    fit = rfe.fit(X_F_TRAIN, Y_F_TRAIN)\n    \n    features_sel = []\n    for sel, col in zip((fit.support_),features):\n        if sel == True:\n            features_sel.append(col)\n    \n    x = X_F_TRAIN[(features_sel)]\n    y = Y_F_TRAIN\n\n    x_train, x_test, y_train, y_test = train_test_split(\n    x, y, stratify = y, random_state=42)\n    \n    names = []\n    scores = []\n    for name, model in models:\n        model.fit(x_train, y_train)\n        y_pred = model.predict(x_test)\n        score = accuracy_score(y_test, y_pred)\n        scores.append(score)\n        names.append(name)\n        if score > s_highest:\n            s_highest = score\n            f_highest = features_sel\n            n_highest = name\n            m_highest = model\n            \n    main_df = main_df.append({'Num_Of_Feature':n,\n                              'Features_Sel':(\", \".join(features_sel)),\n                              names[0]:scores[0],\n                              names[1]:scores[1],\n                              names[2]:scores[2],\n                              names[3]:scores[3],\n                              names[4]:scores[4],\n                              names[5]:scores[5],\n                              names[6]:scores[6]},\n                             ignore_index=True)\n\n#print('The highest score is',s_highest,'with these features',f_highest,'on model',n_highest)","d1089078":"fig = plt.figure(figsize=(20,10))\nax = fig.add_subplot(111)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='KNN',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='SVM',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='LR',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='DT',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='GNB',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='RF',ax=ax)\nmain_df.plot(kind='line',x='Num_Of_Feature',y='GB',ax=ax)\n\nax.set_xticks(np.arange(3, 38, step=1.0))\nax.set_yticks(np.arange(0.70, 0.83, step=0.01))\n\nplt.show()\nplt.close()","a2249ef8":"main_df","6ff73cd2":"strlist = list(main_df[main_df['Num_Of_Feature'] == 20]['Features_Sel'])","6471827f":"str1 = str(strlist)\nstr2 = str1[2:-2]\nf_selected = str2.split(\", \")\nf_selected","528b62e7":"x = X_F_TRAIN[(f_selected)]\ny = Y_F_TRAIN\n\nx_train, x_test, y_train, y_test = train_test_split(\nx, y, stratify = y, random_state=42)","2eae776b":"model = m_highest\nmodel","079c540e":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparameters = {\n    \"loss\":[\"deviance\"],\n    \"learning_rate\": [0.01, 0.1, 0.2],\n    \"max_depth\":[3,5,8],\n    \"criterion\": [\"friedman_mse\"],\n    \"subsample\":[0.5, 0.8, 1.0],\n    \"n_estimators\":[100]\n    }\n\nclf = GridSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_jobs=-1)\n# clf = RandomizedSearchCV(GradientBoostingClassifier(), parameters, cv=10, n_iter=100, n_jobs=-1)","2e26fc3d":"import datetime\n#Starting time\nprint(\"Start time is\",datetime.datetime.now())\n\n#Beware: This line of code can takes hours to run depend of the parameters setting above \nclf.fit(x, y)\n\n#Stop time\nprint(\"Stop time is\",datetime.datetime.now())","4cda0bf1":"print(clf.best_params_)","bdd6ea80":"print(clf.best_estimator_)","0a215faf":"final_score = cross_val_score(clf.best_estimator_, x, y, \n                              cv=10, scoring='accuracy').mean()\nprint(\"Final accuracy : {} \".format(final_score))","fa67fe6c":"model = clf.best_estimator_\nx_train = X_F_TRAIN[(f_selected)]\ny_train = Y_F_TRAIN\n\nx_test = X_F_TEST[(f_selected)]\ny_test = Y_F_TEST","217ec3a3":"model.fit(x_train, y_train)\ny_pred = model.predict(x_test)\nscore = accuracy_score(y_test, y_pred)","03ea7539":"print(\"The accuracy for unseen data is\",score)","90975c36":"df4 = df2[df2['ORGANICS']==1]\n\ntemp_list = [x for x in df4['Gender'] if x == 'M']\nMA = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['Gender'] if x == 'F']\nFE = len(temp_list)\/len(df4)\n\ndata = {'Male': [MA], 'Female': [FE]}\ndf5 = pd.DataFrame.from_dict(data)\n\ndf5.plot.bar(stacked=True, title ='Gender %',figsize=(10,6))\nplt.show()\nplt.close()\n\ndf5.rename(index={0: 'Gender'})","4a7f2184":"df4['AGE GRP'] = pd.cut(df4['Age'], [0, 31, 41, 51, 61, 101], labels=['Below 30', '31-40', '41-50', '51-60', 'Above 61'])\n\ntemp_list = [x for x in df4['AGE GRP'] if x == 'Below 30']\nGP1 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AGE GRP'] if x == '31-40']\nGP2 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AGE GRP'] if x == '41-50']\nGP3 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AGE GRP'] if x == '51-60']\nGP4 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AGE GRP'] if x == 'Above 61']\nGP5 = len(temp_list)\/len(df4)\n\ndata = {'Below 30': [GP1], '31-40': [GP2], '41-50':[GP3], '51-60':[GP4], 'Above 61':[GP5]}\ndf5 = pd.DataFrame.from_dict(data)\n\ndf5.plot.bar(stacked=True, title ='AGE GROUP %',figsize=(10,6))\nplt.show()\nplt.close()\n\ndf5.rename(index={0: 'AGE GROUP'})","bba54c9e":"df4['AFF GRP'] = pd.cut(df4['Affluence Grade'], [0, 11, 16, 21, 26, 40], labels=['Below 10', '11-15', '16-20', '21-25', 'Above 26'])\n\ntemp_list = [x for x in df4['AFF GRP'] if x == 'Below 10']\nGP1 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AFF GRP'] if x == '11-15']\nGP2 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AFF GRP'] if x == '16-20']\nGP3 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AFF GRP'] if x == '21-25']\nGP4 = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['AFF GRP'] if x == 'Above 26']\nGP5 = len(temp_list)\/len(df4)\n\ndata = {'Below 10': [GP1], '11-15': [GP2], '16-20':[GP3], '21-25':[GP4], 'Above 26':[GP5]}\ndf5 = pd.DataFrame.from_dict(data)\n\ndf5.plot.bar(stacked=True, title ='Affluence Grade %',figsize=(10,6))\nplt.show()\nplt.close()\n\ndf5.rename(index={0: 'AFFLUENCE GROUP'})","2922865c":"temp_list = [x for x in df4['Loyalty Status'] if x == 'Gold']\nGO = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['Loyalty Status'] if x == 'Silver']\nSI = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['Loyalty Status'] if x == 'Tin']\nTI = len(temp_list)\/len(df4)\ntemp_list = [x for x in df4['Loyalty Status'] if x == 'Platinum']\nPL = len(temp_list)\/len(df4)\n\n\ndata = {'Gold': [GO], 'Silver': [SI], 'Tin': [TI], 'Platinum': [PL]}\ndf5 = pd.DataFrame.from_dict(data)\n\ndf5.plot.bar(stacked=True, title ='Loyalty Status %',figsize=(10,6))\nplt.show()\nplt.close()\n\ndf5.rename(index={0: 'Loyalty Status'})","374347fd":"### Rename target column to 'ORGANICS'","fd0f9bf9":"### Converting object to integer value if failed then converting to Nan","3d675d40":"### Converting unknown 'U' values to Nan","9d0f7314":"### Now we look into data for a few features","011706e9":"### Reading CSV file into Dataframe","9d4a427b":"### Will use number of features selected equal to 20 since after that it did not change much","6e527398":"### Convert back to Dataframe","989521a2":"### This section is doing features selection using LogisticRegression() and with selected features then pump into different models to get best result","a4c9c712":"### Move the 'ORGANICS' to the last column","a91d4a72":"### Check number of Nan in all variables","e08c2b00":"### Checking unique values of some varibles","42058e1b":"### Drop columns with only 1 value","c9c30e40":"### Quick checks on the dataset","72576be8":"### Check for Nan value again","53ca129f":"### Drop customer ID column and reject column","2383a7cb":"### Accuracy for unseen data","5b65f6ce":"### Bin into groups for features AGE and Affluence Grade","9c1345ce":"### Below is the final score with the best model and parameters selected","8aadec0c":"### Split dataset for training and testing with Cross Validation","a7b62985":"### Replace the rest of missing value with 'most_frequent' in SimpleImputer","2ad77893":"## Conclusion \n\n#### As we can see 'Gender', 'Age', 'Affluence Grade' and 'Loyalty Status' are important features in this dataset. And GradientBoosting GB is the best model as compare with the other models in the list.\n\n#### Age between 31 and 40 year old, female, Affluence Grade below 10 and Tin Loyalty Status are the majority contribution to the contributer.\n\n#### Suggest to focus on this group of customer and bundle to those products that this group of customer used to buy. And also displaying organics products with those products that this group of customer used to buy.","a23269fa":"## Business Objective\n\nA supermarket is offering a new line of organic products. The supermarket's management wants to determine which customers are likely to purchase these products.\n\nThe supermarket has a customer loyalty program. As an initial buyer incentive plan, the supermarket provided coupons for the organic products to all of the loyalty program participants and collected data that includes whether these customers purchased any of the organic products.\n\nAlthough two target variables are listed, these exercises concentrate on the binary variable TargetBuy.","37d7dca8":"### Plot a graph with all models with the range of feature selected","e37feeec":"### Convert Dataframe to X and Y data for modeling","7c11b264":"### Below show the best parameters selected and best estimator","9afca81a":"### Get dummy variables for all categorical columns","b26d70ab":"### Take a quick look into the dataset using charts","4c97696d":"### Remove rows with Nan values from 'Gender','Age' and 'Affluence Grade'","2f4901dd":"### Split dataset for modeling and final testing","d2b9a421":"### Perform tuning on selected model","d480d473":"### Take a look at new format dataset","369749ec":"### Without imputing 'Gender','Age' and 'Affluence Grade' is to prevent data bias to certain value","68236636":"### Importing Libraries","563804fd":"### Check again to see all unknown values converted to Nan","be9704fa":"### Now we want to bring in unseen data to the model and test"}}