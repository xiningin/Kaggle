{"cell_type":{"e15134a5":"code","b8a2b614":"code","db2a1666":"code","d05f4e74":"code","5a82384b":"code","eab421c8":"code","a302c548":"code","4b2fc5b8":"code","4b8e59c0":"code","8c98dc7f":"code","56b14879":"code","6c5a2aad":"code","53c97477":"code","f271b5dc":"code","d2ef9996":"code","891cf2ac":"code","25317c8d":"code","23f25c10":"code","aef585ed":"code","d9076a42":"code","5b251344":"code","7bc849f9":"code","d7352f93":"code","0422cdd9":"code","f1451ea9":"code","a058f67e":"code","12e25901":"code","0a2c1264":"code","1d776572":"code","b53b13f8":"code","89d1a91d":"code","8ab50bb5":"code","255946bd":"code","71458623":"code","f57effaa":"code","4bba14c9":"code","59081d16":"code","a90c5eca":"code","18bbc081":"code","2e4c7e3e":"code","294611ce":"markdown","cf3c20ed":"markdown","56bd8a4d":"markdown","06a0c0f5":"markdown","72450b88":"markdown","0711b551":"markdown","6de2f042":"markdown","cefaf66e":"markdown","4ba73b48":"markdown","97311746":"markdown","39773c84":"markdown","2a705e89":"markdown"},"source":{"e15134a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport sklearn\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score\n\nimport matplotlib\nimport matplotlib.pyplot as plt\nprint(\"matplotlib version: {}\". format(matplotlib.__version__))\n\nimport plotly\nimport plotly.express as px\nimport plotly.graph_objects as go\nprint(\"plotly version: {}\". format(plotly.__version__))\n\n\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\nprint(\"xgboost version: {}\". format(xgb.__version__))\n\nimport optuna\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b8a2b614":"# read input files\ndf_train = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-jul-2021\/sample_submission.csv\")\ntarget_cols = [\"target_carbon_monoxide\",\"target_benzene\",\"target_nitrogen_oxides\"]\ndf_test = df_test.loc[1:] # remove the row that is duplicated in train and test, messes up lag, pct features. Is added again later.\ndf_all = df_train.drop(columns=target_cols).append(df_test) # makes preprocessing easier","db2a1666":"# store target columns\ny_cm = df_train[\"target_carbon_monoxide\"]\ny_b = df_train[\"target_benzene\"]\ny_no = df_train[\"target_nitrogen_oxides\"]\n# store known target values\nknown_target = df_train.iloc[7110][target_cols]","d05f4e74":"\n# consider daily sums here or 6h sums\n\n# lags features\ndf_all = df_train.drop(columns=target_cols).append(df_test)\nlag_columns = ['sensor_1', 'sensor_2', 'sensor_3', 'sensor_4', 'sensor_5'] # define which for which columns shifts\/lags shall be created\nshift_range = [3,6,9,12] # define lag range\n\n# create lags\nfor column in lag_columns:\n    for i in shift_range:\n        col_name = (column +\"_lag_\" + str(i))\n        df_all[col_name] = df_all[column].shift(periods = i)\n    \n#df_all.head(10)\n","5a82384b":"# percentage change features, calculate change compared to previous hour, removed for now, did not improve result\n#pct_columns = lag_columns\n#for column in lag_columns:\n#    col_name = column + \"_pct\"\n#    df_all[col_name] = df_all[column].pct_change()*100\n#df_all.head()","eab421c8":"df_all['date_time'] = pd.to_datetime(df_all['date_time'])\ndf_all['year'] = df_all['date_time'].dt.year\ndf_all['month'] = df_all['date_time'].dt.month\ndf_all['day'] = df_all['date_time'].dt.day\ndf_all['hour'] = df_all['date_time'].dt.hour\ndf_all['dayofweek'] = df_all['date_time'].dt.dayofweek\ndf_all['weekend'] = df_all['dayofweek'].apply(lambda x: 1 if (x>4)  else 0) # Sat, Sun are counted as weekend\ndf_all['rushhour'] = df_all['hour'].apply(lambda x: 1 if x in [8,9,18,19,20]  else 0) # define those hour as rush hour\ndf_all['time'] = df_all['date_time'].astype(np.int64)\/\/10**9 # Unix time\n#df_all.head()","a302c548":"# check the rows where the test set starts \ndf_all.iloc[7109:7113]","4b2fc5b8":"# split again in train and test\ndf_train =  df_all.iloc[0:df_train.shape[0]]\ndf_test = df_all.iloc[df_train.shape[0]-1:]\n# remove rows containing Nan due to the lags\ndf_train = df_train.loc[max(shift_range):]\ny_no = y_no.loc[max(shift_range):]\ny_cm = y_cm.loc[max(shift_range):]\ny_b = y_b.loc[max(shift_range):]\n#df_test.head()","4b8e59c0":"kfolds = KFold(n_splits=5, shuffle=True, random_state=7)","8c98dc7f":"def tune(objective):\n    study = optuna.create_study(direction=\"minimize\")\n    study.optimize(objective, n_trials=100) \n\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score} \\nOptimized parameters: {params}\")\n    return params","56b14879":"def xgb_objective(trial):\n    _n_estimators = trial.suggest_int(\"n_estimators\", 50, 1000) \n    _max_depth = trial.suggest_int(\"max_depth\", 2, 10) \n    _learning_rate = trial.suggest_float(\"learning_rate\", 1e-6, 0.3, log=True)\n    _gamma = trial.suggest_float(\"gamma\", 0.01, 1)\n    _min_child_weight = trial.suggest_int(\"min_child_weight\", 1, 10)\n    _subsample = trial.suggest_float('subsample', 0.5, 1)\n    _colsample_bytree = trial.suggest_float('colsample_bytree', 0.7, 1)\n    _reg_lambda = trial.suggest_float('reg_lambda', 0.01, 10, log=True)\n\n    \n    model = xgb.XGBRegressor(\n        objective = 'reg:squarederror', #'reg:sqarederror' is default and gives better rsmle than 'reg:squaredlogerror'. why?\n        n_estimators=_n_estimators,\n        max_depth=_max_depth, \n        learning_rate=_learning_rate,\n        gamma=_gamma,\n        min_child_weight=_min_child_weight,\n        subsample=_subsample,\n        reg_lambda=_reg_lambda,\n        random_state=7,\n    )\n    \n\n    val_rmsle = []\n    for train_index, test_index in kfolds.split(X):\n \n        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n        model.fit(\n            X_train, \n            y_train, \n            eval_metric=\"rmsle\", \n            eval_set=[(X_train, y_train), (X_val, y_val)], \n            verbose=False, \n            early_stopping_rounds = 50)\n    \n        val_rmsle.append(model.evals_result()['validation_1']['rmsle'][-1])\n    \n   \n    score = sum(val_rmsle)\/len(val_rmsle)\n    \n    return score\n","6c5a2aad":"X = df_train.drop(columns=\"date_time\")\ny = y_b\n#xgb_params_y_b = tune(xgb_objective) # uncomment to run optuna tuning again, takes time....\n\n# Optuna Result: Best score: 0.0863592 \nxgb_params_y_b = {'n_estimators': 365,\n 'max_depth': 10,\n 'learning_rate': 0.02965812395223671,\n 'gamma': 0.3086304942573926,\n 'min_child_weight': 9.03951830237063,\n 'subsample': 0.5054451890047076,\n 'colsample_bytree': 0.9986600565238701,\n 'reg_lambda': 1.5966168376624417}","53c97477":"X = df_train.drop(columns=\"date_time\")\ny = y_cm\n#xgb_params_y_cm = tune(xgb_objective) # uncomment to run optuna tuning again, takes time....\n\n# Optuna Result: Best score: 0.09788859999999999 \nxgb_params_y_cm = {'n_estimators': 418,\n 'max_depth': 5,\n 'learning_rate': 0.07432650387394699,\n 'gamma': 0.0676479407897746,\n 'min_child_weight': 1.3220415658077735,\n 'subsample': 0.8146019010378812,\n 'colsample_bytree': 0.8918164888978178,\n 'reg_lambda': 5.394004764094763}","f271b5dc":"X = df_train.drop(columns=\"date_time\")\ny = y_no\n#xgb_params_y_no = tune(xgb_objective) # uncomment to run optuna tuning again, takes time....\n\n#[W 2021-07-16 12:08:03,325] Trial 0 failed, because the objective function returned nan. --- todo: check what went wrong\n# Optuna Result: Best score: 0.20224699999999998 \nxgb_params_y_no = {'n_estimators': 434,\n 'max_depth': 10,\n 'learning_rate': 0.042338664081773426,\n 'gamma': 0.43939165464469737,\n 'min_child_weight': 2.6282895129902637,\n 'subsample': 0.835280405845509,\n 'colsample_bytree': 0.8120530400759991,\n 'reg_lambda': 5.228573656907881}","d2ef9996":"# use all except 2 month as training set\n# use the last 2 month weeks as validation set (methodisch nicht ganz sauber da diese Daten benutzt wurden um die Hyperparamter zu lernen, potential leakage)\nX_train = df_train[(df_train.year == 2010) & (df_train.month.isin(np.arange(3, 11, 1)))].drop(columns=\"date_time\")\nlen_train = X_train.shape[0] + max(shift_range) -1\ny_train_no = y_no.loc[:len_train]\ny_train_cm = y_cm.loc[:len_train]\ny_train_b = y_b.loc[:len_train]\n\nX_val = df_train.iloc[X_train.shape[0]:]\nval_time = X_val.date_time\nX_val = X_val.drop(columns=\"date_time\")\ny_val_no = y_no.loc[len_train+1:]\ny_val_cm = y_cm.loc[len_train+1:]\ny_val_b = y_b.loc[len_train+1:]","891cf2ac":"# target_benzene\nmodel = XGBRegressor(\n    objective = 'reg:squarederror', # 'reg:squaredlogerror' gives worse rsmle results, is not able to fit to the peaks, change and recalculate viz to see that\n    ** xgb_params_y_b,\n    seed=7)\n\nmodel.fit(\n            X_train, \n            y_train_b, \n            eval_metric=\"rmsle\", \n            eval_set=[(X_train, y_train_b), (X_val, y_val_b)], \n            verbose=10, \n            early_stopping_rounds = 10)\npredictions_y_b = model.predict(X_val)\n\n#[158]\tvalidation_0-rmsle:0.05487\tvalidation_1-rmsle:0.09711","25317c8d":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=y_val_b, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Benzene Ground Truth\")\n)\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=predictions_y_b, \n                 mode = 'lines', \n                 line = {'color':'black', 'width' : 1},\n                 name=\"Benzene Predicted\")\n)\n\nfig.update_layout(\n    title=\"Benzene Predicted (val data = last 2 month) vs. Ground Truth (train data)\"\n)\nfig.show()","23f25c10":"# target_carbon_monoxides\nmodel = XGBRegressor(\n    objective = 'reg:squarederror', \n    ** xgb_params_y_cm,\n    seed=7)\n\nmodel.fit(\n            X_train, \n            y_train_cm, \n            eval_metric=\"rmsle\", \n            eval_set=[(X_train, y_train_cm), (X_val, y_val_cm)], \n            verbose=10, \n            early_stopping_rounds = 10)\npredictions_y_cm = model.predict(X_val)\n\n#[148]\tvalidation_0-rmsle:0.06803\tvalidation_1-rmsle:0.22248","aef585ed":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=y_val_cm, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Carbon Monoxide Ground Truth\")\n)\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=predictions_y_cm, \n                 mode = 'lines', \n                 line = {'color':'black', 'width' : 1},\n                 name=\"Carbon Monoxide Predicted\")\n)\n\nfig.update_layout(\n    title=\"Carbon Monoxide Predicted (val data = last 2 month) vs. Ground Truth (train data)\"\n)\nfig.show()","d9076a42":"# target nitrogen_oxides\nmodel = XGBRegressor(\n    objective = 'reg:squarederror', # 'reg:squaredlogerror' gives worse rsmle results, is not able to fit to the data, change and recalculate viz to see that\n    ** xgb_params_y_no,\n    seed=7)\n\nmodel.fit(\n            X_train, \n            y_train_no, \n            eval_metric=\"rmsle\", \n            eval_set=[(X_train, y_train_no), (X_val, y_val_no)], \n            verbose=30, \n            early_stopping_rounds = 10)\npredictions_y_no = model.predict(X_val)\n\n#[433]\tvalidation_0-rmsle:0.04537\tvalidation_1-rmsle:0.54051","5b251344":"fig = go.Figure()\n\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=y_val_no, \n                 mode = 'lines', \n                 line = {'color':'darkgoldenrod', 'width' : 1},\n                 name=\"Nitrogen Oxides Ground Truth\")\n)\nfig.add_trace(\n      go.Scatter(x=val_time, \n                 y=predictions_y_no, \n                 mode = 'lines', \n                 line = {'color':'black', 'width' : 1},\n                 name=\"Nitrogen Oxides Predicted\")\n)\n\nfig.update_layout(\n    title=\"Nitrogen Oxides Predicted (val data = last 2 month) vs. Ground Truth (train data)\"\n)\nfig.show()","7bc849f9":"def plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)","d7352f93":"X_test = df_test.drop(columns=\"date_time\")","0422cdd9":"# target_benzene\nX = df_train.drop(columns=\"date_time\")\ny = y_b\n\nxgb_params_y_b = {'n_estimators': 365,\n 'max_depth': 10,\n 'learning_rate': 0.02965812395223671,\n 'gamma': 0.3086304942573926,\n 'min_child_weight': 9.03951830237063,\n 'subsample': 0.5054451890047076,\n 'colsample_bytree': 0.9986600565238701,\n 'reg_lambda': 1.5966168376624417}\n\nmodel = XGBRegressor(\n    objective = 'reg:squarederror', # 'reg:squaredlogerror' gives worse rsmle results\n    ** xgb_params_y_b,\n    seed=7)\n\nmodel.fit(X,y)\npredictions_y_b = model.predict(X_test)\n\nplot_features(model,(10,12) )\nplt.show()","f1451ea9":"# target_carbon_monoxide\nX = df_train.drop(columns=\"date_time\")\ny = y_cm\n\nxgb_params_y_cm = {'n_estimators': 418,\n 'max_depth': 5,\n 'learning_rate': 0.07432650387394699,\n 'gamma': 0.0676479407897746,\n 'min_child_weight': 1.3220415658077735,\n 'subsample': 0.8146019010378812,\n 'colsample_bytree': 0.8918164888978178,\n 'reg_lambda': 5.394004764094763}\n\nmodel = XGBRegressor(\n    objective = 'reg:squarederror',\n    ** xgb_params_y_cm,\n    seed=7)\n\nmodel.fit(X,y)\npredictions_y_cm = model.predict(X_test)\n\nplot_features(model,(10,12) )\nplt.show()","a058f67e":"# target_nitrogen_oxides\nX = df_train.drop(columns=\"date_time\")\ny = y_no\n\nxgb_params_y_no = {'n_estimators': 434,\n 'max_depth': 10,\n 'learning_rate': 0.042338664081773426,\n 'gamma': 0.43939165464469737,\n 'min_child_weight': 2.6282895129902637,\n 'subsample': 0.835280405845509,\n 'colsample_bytree': 0.8120530400759991,\n 'reg_lambda': 5.228573656907881}\n\nmodel = XGBRegressor(\n    objective = 'reg:squarederror', \n    ** xgb_params_y_no,\n    seed=7)\n\nmodel.fit(X,y)\npredictions_y_no = model.predict(X_test)\n\nplot_features(model,(10,12) )\nplt.show()","12e25901":"# generate submission file (Public Score 0.20824)\nsubmission_xgb = pd.DataFrame(data={\"date_time\" : sample_submission.date_time,\n                               \"target_carbon_monoxide\" : predictions_y_cm,\n                               \"target_benzene\" : predictions_y_b,\n                               \"target_nitrogen_oxides\" : predictions_y_no})\nsubmission_xgb.loc[0,target_cols] = known_target # replace the first row with the known values as timestamp is identical\nsubmission_xgb.to_csv('submission_xgb.csv', index=False)\nsubmission_xgb.head()","0a2c1264":"# generate submission file #  very low factors (Public Score: 0.20777, improvement!)\nsubmission_xgb = pd.DataFrame(data={\"date_time\" : sample_submission.date_time,\n                               \"target_carbon_monoxide\" : predictions_y_cm*1.001,\n                               \"target_benzene\" : predictions_y_b*1.001,\n                               \"target_nitrogen_oxides\" : predictions_y_no*1.01})\nsubmission_xgb.loc[0,target_cols] = known_target # replace the first row with the known values as timestamp is identical\nsubmission_xgb.to_csv('submission_xgb_verylowfactors.csv', index=False)\nsubmission_xgb.head()","1d776572":"# generate submission file # low factors (gives Public score of 0.21209)\nsubmission_xgb = pd.DataFrame(data={\"date_time\" : sample_submission.date_time,\n                               \"target_carbon_monoxide\" : predictions_y_cm*1.05,\n                               \"target_benzene\" : predictions_y_b*1.01,\n                               \"target_nitrogen_oxides\" : predictions_y_no*1.1})\nsubmission_xgb.loc[0,target_cols] = known_target # replace the first row with the known values as timestamp is identical\nsubmission_xgb.to_csv('submission_xgb_low_factor.csv', index=False)\nsubmission_xgb.head()","b53b13f8":"# generate submission file # min factors (gives Public score of 0.20802)\nsubmission_xgb = pd.DataFrame(data={\"date_time\" : sample_submission.date_time,\n                               \"target_carbon_monoxide\" : predictions_y_cm*1.0007,\n                               \"target_benzene\" : predictions_y_b*1.0007,\n                               \"target_nitrogen_oxides\" : predictions_y_no*1.005})\nsubmission_xgb.loc[0,target_cols] = known_target # replace the first row with the known values as timestamp is identical\nsubmission_xgb.to_csv('submission_xgb_minfactor.csv', index=False)\nsubmission_xgb.head()","89d1a91d":"# generate submission file # new factors \nsubmission_xgb = pd.DataFrame(data={\"date_time\" : sample_submission.date_time,\n                               \"target_carbon_monoxide\" : predictions_y_cm*1.001,\n                               \"target_benzene\" : predictions_y_b*1.001,\n                               \"target_nitrogen_oxides\" : predictions_y_no*1.005})\nsubmission_xgb.loc[0,target_cols] = known_target # replace the first row with the known values as timestamp is identical\nsubmission_xgb.to_csv('submission_xgb_newfactor.csv', index=False)","8ab50bb5":"# public score = 0.77258\nsample_submission.head()","255946bd":"p = 0.1\na = 0.2\n((np.log(p+1) - np.log(a+1))**2)**0.5","71458623":"p = np.array(list(range(0,50,1)))\/10 # predicted\na = np.array([1] * 50)            # actual\ny=((np.log(p+1) - np.log(a+1))**2)**0.5 #rmsle\nmse = (a-p)**2\nrmse = ((a-p)**2)**0.5\n#display(p)\n#display(y)\n#display(mse)\n#display(rmse)","f57effaa":"import matplotlib\nimport matplotlib.pyplot as plt","4bba14c9":"plt.figure(figsize = (12,6))\nplt.plot(p,y, color = 'blue')\nplt.vlines(1,-0.2,1.5, colors='green', label='actual value')\n#plt.plot(p,mse, color = 'red')\nplt.plot(p,rmse, color = 'black')\nplt.xlabel(\"prediction\")\nplt.ylabel(\"rmsle\")\nplt.show()","59081d16":"from sklearn.metrics import mean_squared_log_error \nfrom sklearn.metrics import mean_squared_error","a90c5eca":"y_true = [3, 2, 4]\ny_pred = 1.1 * np.array(y_true)\nprint(\"MSLE:  \", mean_squared_log_error(y_true, y_pred))\nprint(\"RMLSE: \", np.sqrt(mean_squared_log_error(y_true, y_pred)))\nprint(\"RMLSE: \", mean_squared_error(np.log1p(y_true), np.log1p(y_pred), squared = False))","18bbc081":"p = np.array(y_pred)\na = np.array(y_true)\ny=((np.log(p+1) - np.log(a+1))**2)\nsum(y) \/ len(y)\nprint(\"MSLE:  \", sum(y) \/ len(y))\nprint(\"RMLSE: \", (sum(y) \/ len(y))**0.5)","2e4c7e3e":"x = np.array(list(range(0,100)))\/10\ny = np.log(x+1)   # equivalent: y = np.log1p(x)\n\nplt.figure(figsize = (12,6))\nplt.plot(x,y)\nplt.show()","294611ce":"I generate multiple submission files using a factor to uplift my predictions. This idea is based on the graphic analysis using the validation set (predictions were often below ground truth) and the fact that RMSLE penalizes underpredictions harder than overpredictions.\n\nAfter some unsucessfull trials with too strong factors, I found factors that actually improved my Public Score!","cf3c20ed":"## Understanding the error metric","56bd8a4d":"## Tune Model\n(Changed in Version 16.) Use 3 models to predict the 3 target variables. Use Optuna to tune the models. (No chained prediction for the moment any more.) Use k-fold cross validation. This is just a guess, seasonality captured in the features could be enough. And we don't have enough training data to make good use of seasonality.\n\n**Step 1**: Setup Optuna Tuner","06a0c0f5":"## Visualize Model Performance with Validation Set","72450b88":"# Tabular Playground Series 7 - Predict Air Pollution\n\nThe dataset deals with predicting air pollution in a city via various input sensor values. The task is to predict, based on the sensor, values **three** target variables: target_carbon_monoxide,target_benzene and target_nitrogen_oxides. Submissions are evaluated using the [mean column-wise root mean squared logarithmic error](http:\/\/www.kaggle.com\/c\/tabular-playground-series-jul-2021\/overview\/evaluation).\n\nThe model is created based on my [EDA](https:\/\/www.kaggle.com\/melanie7744\/tps7-interactive-eda-with-plotly\/edit).\n\nSteps:\n* feature engineering\n* model tuning with Optuna\n* visual checks on model performance\n* predict & submit\n\nAt the end of the notebook there is a section about understanding the error metric.\n\nFeedback is highly appreciated!\n\n\n### I decided not to use the leak!\n","0711b551":"**Step 2:** Find hyperparameters for all three target variables seperately.","6de2f042":"## Predict","cefaf66e":"Predictions lower than the ground truth are penalized harder than predictions above ground truth.\n\nCompared to MSE and RMSE the RMSLE penalized predictions that are far off, less.","4ba73b48":"## Adding features","97311746":"## Submission","39773c84":"As already indicated by the RMSLE, the predictions for Nitrogen Oxides are worse than for the other 2 variables. The model is not able to capture the peeks well. It can be seen that in the timeframe from Dec 14th - Dec 17th where the sensor behave strange (see [EDA](https:\/\/www.kaggle.com\/melanie7744\/tps7-interactive-eda-with-plotly\/edit) ), the predictions are worst. ","2a705e89":"Predictions are made for all three target variables seperately. Feature importance as seen by XGBoost is visualized."}}