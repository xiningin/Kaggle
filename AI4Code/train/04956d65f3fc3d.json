{"cell_type":{"f33ad97c":"code","610ba3fb":"code","52b19fef":"code","efad942d":"code","b4ad5127":"code","29111f28":"code","15b7c1c6":"code","56484496":"code","89afe57f":"code","f5413b81":"code","6f6294a9":"code","054ed49d":"code","716d7fce":"code","d29fdce0":"code","a7f429c3":"code","e0dca5fd":"code","115112e9":"code","d3f76978":"code","63b8cc9c":"code","f3d9d354":"code","aff10511":"code","b1c49524":"code","28eeb8b2":"code","99fe5889":"code","f62463b5":"code","a9aac132":"code","66659b13":"code","1d64be04":"markdown","4a5e6fcc":"markdown"},"source":{"f33ad97c":"!pip install dataprep by","610ba3fb":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom dataprep.eda import *\nfrom dataprep.eda import plot\nfrom dataprep.eda import plot_correlation\nfrom dataprep.eda import plot_missing\nimport matplotlib.pyplot as plt \nimport seaborn as sns; sns.set()\nimport warnings\nwarnings.filterwarnings(\"ignore\")","52b19fef":"train = pd.read_csv('..\/input\/dont-overfit-ii\/train.csv')\ntest = pd.read_csv('..\/input\/dont-overfit-ii\/test.csv')\nsubmission = pd.read_csv('..\/input\/dont-overfit-ii\/sample_submission.csv')","efad942d":"print(train.head())\nprint('_'*80)\nprint(test.head())\nprint('_'*80)\nprint(submission.head())","b4ad5127":"print(train.info())\nprint('_'*50)\nprint(test.info())\nprint('_'*50)\nprint(submission.info())","29111f28":"print(train.describe().T)\nprint('_'*50)\nprint(test.describe().T)\nprint('_'*50)\nprint(submission.describe().T)","15b7c1c6":"print(train.isna())\nprint('_'*50)\nprint(test.isna())\nprint('_'*50)\nprint(submission.isna())","56484496":"#sns.pairplot(train, height=3.5, aspect=1.3)","89afe57f":"# plots the distribution of each column and calculates dataset statistics\nplot(train)","f5413b81":"plot(test)","6f6294a9":"sns.heatmap(train.corr())","054ed49d":"sns.heatmap(test.corr())","716d7fce":"#import pandas_profiling as pp\n#pp.ProfileReport(train)","d29fdce0":"#import pandas_profiling as pp\n#pp.ProfileReport(test)","a7f429c3":"X_train = train.drop(['id', 'target'], axis=1).values\ny_train = train['target'].values\n\nX_test = test.drop(['id'], 1).values","e0dca5fd":"from sklearn.preprocessing import QuantileTransformer\nscaler = QuantileTransformer()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","115112e9":"X_train.shape, y_train.shape, X_test.shape","d3f76978":"from sklearn.svm import SVC\nsvm = SVC(C=100, kernel='linear', max_iter=100, gamma='auto', probability=True, random_state=0)\nsvm.fit(X_train, y_train)","63b8cc9c":"from sklearn.model_selection import cross_val_score\n\nscore = cross_val_score(svm, X_train, y_train, cv=20, scoring='roc_auc')\nprint(score)\nprint('-' * 60)\nprint(score.max())","f3d9d354":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nxgb_model = xgb.XGBClassifier().fit(X_train, y_train)\n\n#brute force scan for all parameters, here are the tricks\n#usually max_depth is 6,7,8\n#learning rate is around 0.05, but small changes may make big diff\n#tuning min_child_weight subsample colsample_bytree can have \n#much fun of fighting against overfit \n#n_estimators is how many round of boosting\n#finally, ensemble xgboost with multiple seeds may reduce variance\nparameters = {'nthread':[4], #when use hyperthread, xgboost may become slower\n              'objective':['binary:logistic'],\n              'learning_rate': [0.05], #so called `eta` value\n              'max_depth': [6],\n              'min_child_weight': [11],\n              'silent': [1],\n              'subsample': [0.8],\n              'colsample_bytree': [0.7],\n              'n_estimators': [5], #number of trees, change it to 1000 for better results\n              'missing':[-999],\n              'seed': [1337]}\n\n\ngrid_search = GridSearchCV(xgb_model, param_grid=parameters, n_jobs=5)\ngrid_search.fit(X_train, y_train)\nprint('Best score: {}'.format(grid_search.best_score_))\nprint('Best parameters: {}'.format(grid_search.best_params_))","aff10511":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=30)\nknn.fit(X_train,y_train)\ny_pred_KNN =knn.predict(X_test)\nscore = cross_val_score(knn, X_train, y_train, cv=20, scoring='roc_auc')\nprint(score)\nprint('-' * 60)\nprint(score.max())","b1c49524":"svm_pred = svm.predict_proba(X_test)[:, 1]\nxgb_pred = xgb_model.predict_proba(X_test)[:, 1]\nknn_pred = knn.predict_proba(X_test)[:, 1]\nav_pred = (svm_pred + xgb_pred+knn_pred) \/ 3\nav_pred","28eeb8b2":"submission['target'] = svm_pred\nsubmission.to_csv('svm_submission.csv', index=False)","99fe5889":"submission['target'] = xgb_pred\nsubmission.to_csv('xgb_submission.csv', index=False)","f62463b5":"submission['target'] = knn_pred\nsubmission.to_csv('knn_submission.csv', index=False)","a9aac132":"submission['target'] = av_pred\nsubmission.to_csv('submission.csv', index=False)","66659b13":"submission.head()","1d64be04":"## Don't Overfit!EDA+SVM+KNN+XGB\n\n### We used EDA and 3 algorithms Classifier \n\n #### Exploratory Data Analysis(EDA)\n\n\nExploratory Data Analysis - does this for Machine Learning enthusiast. It is a way of visualizing, summarizing and interpreting the information that is hidden in rows and column format.\n\n<img src=\"https:\/\/csml.princeton.edu\/sites\/csml\/files\/events\/data-analysis-blog.jpg\" width=\"800px\">\n\n\n\n\n#### 1- Support Vector Machines\n\nSupport Vector Machines (SVMs), also known as support vector networks, are a family of extremely powerful models which use method based learning and can be used in classification and regression problems. They aim at finding decision boundaries that separate observations with differing class memberships. In other words, SVM is a discriminative classifier formally defined by a separating hyperplane\n\n<img src=\"https:\/\/d2o2utebsixu4k.cloudfront.net\/media\/images\/f64026c1-4f3d-42f7-98b9-0ee5fe46ef92.jpg\" width=\"800px\">\n\n#### 2- XGboost\n\nXGboost is the most widely used algorithm in machine learning, whether the problem is a classification or a regression problem. It is known for its good performance as compared to all other machine learning algorithms. \n\n<img src=\"https:\/\/www.analyticssteps.com\/backend\/media\/thumbnail\/3327098\/5525447_1593423035_XG.jpg\" width=\"800px\">\n\n #### 3- K-Nearest Neighbor\n\nK-Nearest Neighbor classifier is one of the introductory supervised classifiers, which every data science learner should be aware of. This algorithm was first used for a pattern classification task which was first used by Fix & Hodges in 1951. To be similar the name was given as KNN classifier. KNN aims for pattern recognition tasks.\n\n<img src=\"https:\/\/www.analyticssteps.com\/backend\/media\/thumbnail\/8694049\/8459793_1587615148_KNN.jpg\" width=\"800px\">\n\n","4a5e6fcc":"\n### What am I predicting?\nYou are predicting the binary target associated with each row, without overfitting to the minimal set of training examples provided.\n\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/1\/19\/Overfitting.svg\/1200px-Overfitting.svg.png\" width=\"500px\">\n\n\n#### Files\n* train.csv - the training set. 250 rows.\n* test.csv - the test set. 19,750 rows.\n* sample_submission.csv - a sample submission file in the correct format\n\n\n#### Columns\n* id- sample id\n* target- a binary target of mysterious origin.\n* 0-299- continuous variables.\n\n#### Dataset Link\n\n[Here](https:\/\/www.kaggle.com\/c\/dont-overfit-ii\/data)"}}