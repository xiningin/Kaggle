{"cell_type":{"d5a608a5":"code","1c9d0c9e":"code","09d1806a":"code","4a6542b1":"code","bd709029":"code","fc666f5e":"code","7cdd3150":"code","eb62c644":"code","e5c080bf":"code","c60cdeff":"code","312a3af4":"code","161d4a95":"code","9549210c":"code","6add50fc":"code","12ed1b3d":"code","35580830":"code","e7f0ae3c":"code","836685fa":"code","4ce486ae":"code","804edc1b":"code","20e855bf":"code","574fcdb5":"code","cfec8e76":"code","670442ea":"code","d02707f1":"code","26ee2739":"code","b6bc2f8f":"code","9377bae5":"code","1e815b81":"code","1e96f26c":"code","21c1fca6":"code","5dde46a8":"markdown","fc43f76c":"markdown","0b22af7c":"markdown","a726fcee":"markdown","3e1dd3be":"markdown","041ff772":"markdown"},"source":{"d5a608a5":"from fastai.vision import *\nimport os\nprint(os.listdir(\"..\/input\/samples\/samples\")[:10])","1c9d0c9e":"path = Path(r'..\/input\/samples\/')","09d1806a":"from IPython.display import Image\nImage(filename='..\/input\/samples\/samples\/bny23.png')","4a6542b1":"def label_from_filename(path):\n    label = [char for char in path.name[:-4]]\n    return label","bd709029":"data = (ImageList.from_folder(path)\n        .split_by_rand_pct(0.2)\n        .label_from_func(label_from_filename)\n        .transform(get_transforms(do_flip=False))\n        .databunch()\n        .normalize()\n       )\ndata.show_batch(3)","fc666f5e":"acc_02 = partial(accuracy_thresh, thresh=0.2)","7cdd3150":"learn = learn = cnn_learner(data, models.resnet18, model_dir='\/tmp', metrics=acc_02)\nlr_find(learn)\nlearn.recorder.plot()","eb62c644":"lr = 5e-2\nlearn.fit_one_cycle(5, lr)","e5c080bf":"learn.unfreeze()\nlr_find(learn)\nlearn.recorder.plot()","c60cdeff":"learn.fit_one_cycle(15, slice(1e-3, lr\/5))","312a3af4":"def show_extremes(learn, k=3, thresh=0.2, show_losses=True):\n    preds, y, losses = learn.get_preds(with_loss=True)\n    losses = losses.view(preds.shape).sum(dim=1)\n    \n    if show_losses: sort_idx = np.argsort(losses.numpy())[::-1]\n    else: sort_idx = np.argsort(losses.numpy())\n        \n    imgs = learn.data.valid_ds\n    \n    fig, ax = plt.subplots(ncols=k, nrows=k, figsize=(10,10))\n\n    for i,axis in zip(sort_idx, ax.flatten()):\n        img, actual_label = imgs[i]\n        actual_label = ''.join(set(str(actual_label).split(';')))\n\n        pred = preds[i]\n        show_image(img, ax=axis)\n        pred_label = [c for p,c in zip(pred>=thresh, learn.data.classes) if p]\n        loss = losses[i]\n\n        title = f'{\"\".join(pred_label)} | {str(actual_label)} | {loss.item():.4f}'\n        axis.set_title(title)\n    fig.suptitle('prediction | actual | loss', fontsize=16)\n    plt.show()","161d4a95":"show_extremes(learn, show_losses=True)","9549210c":"show_extremes(learn, show_losses=False)","6add50fc":"def char_from_path(path): return path.name[2]","12ed1b3d":"data = (ImageList.from_folder(path)\n        .split_by_rand_pct(0.2)\n        .label_from_func(char_from_path)\n        .transform(get_transforms(do_flip=False))\n        .databunch()\n        .normalize()\n       )\ndata.show_batch(3)","35580830":"learn = learn = cnn_learner(data, models.resnet18, model_dir='\/tmp', metrics=accuracy, ps=.2)\n#lr_find(learn)\n#learn.recorder.plot()","e7f0ae3c":"lr = 1e-2\nlearn.fit_one_cycle(10, lr)","836685fa":"learn.unfreeze()\nlr_find(learn)\nlearn.recorder.plot()","4ce486ae":"learn.fit_one_cycle(15, slice(5e-4, lr\/5))","804edc1b":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10))","20e855bf":"#convert label\nlabels = [[char for char in code.name[:-4]] for code in (path\/'samples').glob('*.png')]\nlabels = set([letter for label in labels for letter in label])\nprint(len(labels), 'different labels were found')\n\nencoding_dict = {l:e for e,l in enumerate(labels)}\ndecoding_dict = {e:l for l,e in encoding_dict.items()}\n\ncode_dimension = len(labels)\ncaptcha_dimension = 5\n\ndef to_onehot(filename):\n    code = filename.name[:-4]\n    onehot = np.zeros((code_dimension, captcha_dimension))\n    for column, letter in enumerate(code):\n        onehot[encoding_dict[letter], column] = 1\n    return onehot.reshape(-1)\n\ndef decode(onehot):\n    onehot = onehot.reshape(code_dimension, captcha_dimension)\n    idx = np.argmax(onehot, axis=0)\n    return [decoding_dict[i.item()] for i in idx]\n\ndef label_accuracy(preds, actuals):\n    pred = torch.unbind(preds)\n    act = torch.unbind(actuals)\n    \n    valid = 0\n    total = 0\n    \n    for left,right in zip(pred,act):\n        total+=1\n        p = decode(left)\n        a = decode(right)\n        if p==a: valid += 1\n\n    return torch.tensor(valid\/total).cuda()\n\ndef char_accuracy(n):\n    def c_acc(preds, actuals):\n        pred = torch.unbind(preds)\n        act = torch.unbind(actuals)\n\n        valid = 0\n        total = 0\n\n        for left,right in zip(pred,act):\n            total+=1\n            p = decode(left)\n            a = decode(right)\n            if p[n]==a[n]: valid += 1\n\n        return torch.tensor(valid\/total).cuda()\n    return c_acc","574fcdb5":"#The captchas are already the result of some sort of transformation, so I'll try not using any additional ones\ndata = (ImageList.from_folder(path)\n        .split_by_rand_pct(0.2)\n        .label_from_func(to_onehot, label_cls = FloatList)\n        .transform(get_transforms(do_flip=False))\n        .databunch()\n        .normalize()\n       )","cfec8e76":"#the labels are already in onehot-encoded form at this point\ndata.show_batch(3)","670442ea":"learn = cnn_learner(data, models.resnet50, model_dir='\/tmp',\n                    metrics=[label_accuracy, char_accuracy(0),char_accuracy(1),char_accuracy(2),char_accuracy(3),char_accuracy(4)],\n                   ps=0.)","d02707f1":"lr_find(learn)\nlearn.recorder.plot()","26ee2739":"lr = 5e-2\nlearn.fit_one_cycle(5, lr)","b6bc2f8f":"learn.save('pretrained')","9377bae5":"learn.unfreeze()\n#lr_find(learn)\n#learn.recorder.plot()","1e815b81":"learn.fit_one_cycle(20, slice(1e-3, 1e-2))","1e96f26c":"dat, lbl = learn.data.valid_ds[10]\ndat.show??","21c1fca6":"def show_preds(learn, k=3)\nfor i in range(10):\n    dat, lbl = learn.data.valid_ds[10+i]\n    dat.show()\n    plt.title(' '.join(decode(learn.predict(dat)[0].data)))\n    plt.show()","5dde46a8":"## Multilabel Classification","fc43f76c":"# Captchas","0b22af7c":"# Single char classification","a726fcee":"The validation loss is sinking, but not a single captcha is correctly (completely) classified at this point.","3e1dd3be":"# Helpers\n\nRecognition of the 5 characters of each captcha is done as a regression task. There are 5 characters and 19 possibilities for each characters. Each captcha can be represented by a 19x5 Matrix (concatenation of 5 one-hot encoded vectors). This matrix is flattened to a 95 dimensional vector. This vector is the target for the regression.\nThe following methods handle\n- The encoding from each possible character to the onehot-encoded vector\n- The encoding of each captcha to the (flattened) onehot-encoded vector\n- The decoding of the flattened vector to readable captcha characters\n\nThe loss function for the regression is MSE. While usefull for seeing if we are moving towards the right direction, it's not as useful for judging if it's solving the captcha recognition task. To make the progress towards the goal visible, we take each prediction vector from the validation set, transform it into the original onehot-matrix, convert the columns to characters and then compare this character list to the actual label.\n\nThe ratio of correctly classified captchas over the total amount of captchas is calculated in 'label_accuracy'.","041ff772":"### Misclassifications"}}