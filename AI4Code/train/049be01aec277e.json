{"cell_type":{"7f080b85":"code","26da82b2":"code","db9b7e9b":"code","8cb4dadf":"code","e44b993c":"code","c1fcc613":"code","2e39f4da":"code","76ed56b2":"code","a7dbb049":"code","3ac4d622":"code","bc4d2dc2":"code","5e9fdecf":"code","0494cecf":"code","437877a6":"code","49203241":"code","e114c580":"code","c49dfb14":"code","71b76e53":"code","40c0a6a0":"code","9a4847c9":"code","5c149e32":"code","c2336445":"code","ce142fd7":"code","c0f0ad8d":"code","218d5c1e":"code","4f561e91":"code","d0b06605":"code","e25c3905":"code","eb40d8d5":"code","4be9d6cd":"code","8324fb11":"code","b50c36d7":"code","a43c5567":"code","1f6f1a49":"code","b2556202":"code","a8a60f50":"code","d7912368":"code","f0703fb9":"code","ce12e3c7":"code","0d008f1d":"code","fdad6f95":"code","728cf6b0":"code","43d5351b":"code","699d01a8":"code","37c199c2":"markdown","9f0c225c":"markdown","213065ce":"markdown","c74fa225":"markdown","30721078":"markdown","e831bef3":"markdown"},"source":{"7f080b85":"#Importing libraries\nimport nltk, re, pprint\nimport numpy as np\nimport pandas as pd\nimport requests\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pprint, time\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize","26da82b2":"# reading the Treebank tagged sentences\nwsj = list(nltk.corpus.treebank.tagged_sents())","db9b7e9b":"# first few tagged sentences\nprint(wsj[:40])","8cb4dadf":"# Splitting into train and test\nrandom.seed(1234)\ntrain_set, test_set = train_test_split(wsj,test_size=0.3)\n\nprint(len(train_set))\nprint(len(test_set))\nprint(train_set[:40])","e44b993c":"# Getting list of tagged words\ntrain_tagged_words = [tup for sent in train_set for tup in sent]\nlen(train_tagged_words)","c1fcc613":"# tokens \ntokens = [pair[0] for pair in train_tagged_words]\ntokens[:10]","2e39f4da":"# vocabulary\nV = set(tokens)\nprint(len(V))","76ed56b2":"# number of tags\nT = set([pair[1] for pair in train_tagged_words])\nlen(T)","a7dbb049":"print(T)","3ac4d622":"# computing P(w\/t) and storing in T x V matrix\nt = len(T)\nv = len(V)\nw_given_t = np.zeros((t, v))","bc4d2dc2":"print(w_given_t)","5e9fdecf":"# compute word given tag: Emission Probability\ndef word_given_tag(word, tag, train_bag = train_tagged_words):\n    tag_list = [pair for pair in train_bag if pair[1]==tag]\n    count_tag = len(tag_list)\n    w_given_tag_list = [pair[0] for pair in tag_list if pair[0]==word]\n    count_w_given_tag = len(w_given_tag_list)\n    \n    return (count_w_given_tag, count_tag)","0494cecf":"# examples\n\n# large\nprint(\"\\n\", \"large\")\nprint(word_given_tag('large', 'JJ'))\nprint(word_given_tag('Android', 'NN'))\nprint(word_given_tag('large', 'VB'))\nprint(word_given_tag('large', 'NN'), \"\\n\")\n\n# will\nprint(\"\\n\", \"will\")\nprint(word_given_tag('will', 'MD'))\nprint(word_given_tag('will', 'NN'))\nprint(word_given_tag('will', 'VB'))\n\n# book\nprint(\"\\n\", \"book\")\nprint(word_given_tag('book', 'NN'))\nprint(word_given_tag('book', 'VB'))","437877a6":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1 ,(count_t2_t1\/count_t1)*100)","49203241":"# examples\nprint(t2_given_t1(t2='NNP', t1='JJ'))\nprint(t2_given_t1('NN', 'JJ'))\nprint(t2_given_t1('NN', 'DT'))\nprint(t2_given_t1('NNP', 'VB'))\nprint(t2_given_t1(',', 'NNP'))\nprint(t2_given_t1('PRP', 'PRP'))\nprint(t2_given_t1('VBG', 'NNP'))\nprint(t2_given_t1('VB', 'MD'))","e114c580":"#Please note P(tag|start) is same as P(tag|'.')\nprint(t2_given_t1('DT', '.'))\nprint(t2_given_t1('VBG', '.'))\nprint(t2_given_t1('NN', '.'))\nprint(t2_given_t1('NNP', '.'))\n","c49dfb14":"# creating t x t transition matrix of tags\n# each column is t2, each row is t1\n# thus M(i, j) represents P(tj given ti)\n\ntags_matrix = np.zeros((len(T), len(T)), dtype='float32')\nfor i, t1 in enumerate(list(T)):\n    for j, t2 in enumerate(list(T)): \n        tags_matrix[i, j] = t2_given_t1(t2, t1)[0]\/t2_given_t1(t2, t1)[1]","71b76e53":"tags_matrix","40c0a6a0":"# convert the matrix to a df for better readability\ntags_df = pd.DataFrame(tags_matrix, columns = list(T), index=list(T))","9a4847c9":"tags_df","5c149e32":"tags_df.loc['.', :]","c2336445":"# heatmap of tags matrix\n# T(i, j) means P(tag j given tag i)\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_df)\nplt.show()\n","ce142fd7":"# frequent tags\n# filter the df to get P(t2, t1) > 0.5\ntags_frequent = tags_df[tags_df>0.5]\nplt.figure(figsize=(18, 12))\nsns.heatmap(tags_frequent)\nplt.show()","c0f0ad8d":"len(train_tagged_words)","218d5c1e":"# Viterbi Heuristic\ndef Viterbi(words, train_bag = train_tagged_words):\n    state = []\n    T = list(set([pair[1] for pair in train_bag]))\n    \n    for key, word in enumerate(words):\n        #initialise list of probability column for a given observation\n        p = [] \n        for tag in T:\n            if key == 0:\n                transition_p = tags_df.loc['.', tag]\n            else:\n                transition_p = tags_df.loc[state[-1], tag]\n                \n            # compute emission and state probabilities\n            emission_p = word_given_tag(words[key], tag)[0]\/word_given_tag(words[key], tag)[1]\n            state_probability = emission_p * transition_p    \n            p.append(state_probability)\n            \n        pmax = max(p)\n        # getting state for which probability is maximum\n        state_max = T[p.index(pmax)] \n        state.append(state_max)\n    return list(zip(words, state))\n\n","4f561e91":"# Running on entire test dataset would take more than 3-4hrs. \n# Let's test our Viterbi algorithm on a few sample sentences of test dataset\n\nrandom.seed(1234)\n\n# choose random 5 sents\nrndom = [random.randint(1,len(test_set)) for x in range(5)]\n\n# list of sents\ntest_run = [test_set[i] for i in rndom]\n\n# list of tagged words\ntest_run_base = [tup for sent in test_run for tup in sent]\n\n# list of untagged words\ntest_tagged_words = [tup[0] for sent in test_run for tup in sent]\ntest_run","d0b06605":"# tagging the test sentences\nstart = time.time()\ntagged_seq = Viterbi(test_tagged_words)\nend = time.time()\ndifference = end-start","e25c3905":"print(\"Time taken in seconds: \", difference)\nprint(tagged_seq)\n#print(test_run_base)","eb40d8d5":"# accuracy\ncheck = [i for i, j in zip(tagged_seq, test_run_base) if i == j] ","4be9d6cd":"accuracy = len(check)\/len(tagged_seq)","8324fb11":"accuracy","b50c36d7":"incorrect_tagged_cases = [[test_run_base[i-1],j] for i, j in enumerate(zip(tagged_seq, test_run_base)) if j[0]!=j[1]]","a43c5567":"incorrect_tagged_cases","1f6f1a49":"## Testing\nsentence_test = 'Twitter is the best networking social site. Man is a social animal. Data science is an emerging field. Data science jobs are high in demand.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","b2556202":"print(tagged_seq)\nprint(difference)","a8a60f50":"## Testing\nsentence_test = 'Donald Trump is the current President of US. Before entering politics, he was a domineering businessman and television personality.'\nwords = word_tokenize(sentence_test)\n\nstart = time.time()\ntagged_seq = Viterbi(words)\nend = time.time()\ndifference = end-start","d7912368":"print(tagged_seq)\nprint(difference)","f0703fb9":"#Consider the following text with POS tags\n\n#Donald\/NN Trump\/NN is\/VB the\/DT current\/JJ President\/NN of\/IN US\/NN. Before\/IN entering\/VB into\/IN dirty\/JJ politics\/NN, he\/PRP was\/VB a\/DT domineering\/JJ businessman\/NN and\/CC television\/NN personality\/NN. Trump\/NN entered\/VB the\/DT 2016\/CD presidential\/JJ race\/NN as\/IN a\/DT Republican\/NN and\/CC defeated\/VBD 16\/CD opponents\/NN.\n\n#Calculate the transition probability of JJ followed by NN, i.e. NN appearing after JJ.","ce12e3c7":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef t2_given_t1(t2, t1, train_bag = train_tagged_words):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1 ,(count_t2_t1\/count_t1)*100)","0d008f1d":"train_tagged_words","fdad6f95":"train_tagged_words1 = [('Donald','NN'),\n('Trump','NN'),\n('is','VB'),\n('the','DT'),\n('current','JJ'),\n('President','NN'),\n('of','IN'),\n('US','NN'),\n('.','.'),\n('Before','IN'),\n('entering','VB'),\n('into','IN'),\n('dirty','JJ'),\n('politics','NN'),\n(','','),\n('he','PRP'),\n('was','VB'),\n('a','DT'),\n('domineering','JJ'),\n('businessman','NN'),\n('and','CC'),\n('television','NN'),\n('personality','NN'),\n('.','.'),\n('Trump','NN'),\n('entered','VB'),\n('the','DT'),\n('2016','CD'),\n('presidential','JJ'),\n('race','NN'),\n('as','IN'),\n('a','DT'),\n('Republican','NN'),\n('and','CC'),\n('defeated','VBD'),\n('16','CD'),\n('opponents','NN'),\n('.','.')]\n","728cf6b0":"train_tagged_words1","43d5351b":"# compute tag given tag: tag2(t2) given tag1 (t1), i.e. Transition Probability\n\ndef P_t2_given_t1(t2, t1, train_bag = train_tagged_words1):\n    tags = [pair[1] for pair in train_bag]\n    count_t1 = len([t for t in tags if t==t1])\n    count_t2_t1 = 0\n    for index in range(len(tags)-1):\n        if tags[index]==t1 and tags[index+1] == t2:\n            count_t2_t1 += 1\n    return (count_t2_t1, count_t1 ,(count_t2_t1\/count_t1)*100)","699d01a8":"# Calculate the transition probability of JJ followed by NN, i.e. NN appearing after JJ.\nprint(P_t2_given_t1(t2='NN', t1='JJ'))","37c199c2":"## POS Tagging, HMMs, Viterbi\n\nLet's learn how to do POS tagging by Viterbi Heuristic using tagged Treebank corpus. Before going through the code, let's first understand the pseudo-code for the same. \n\n1. Tagged Treebank corpus is available (Sample data to training and test data set)\n   - Basic text and structure exploration\n2. Creating HMM model on the tagged data set.\n   - Calculating Emission Probabaility: P(observation|state)\n   - Calculating Transition Probability: P(state2|state1)\n3. Developing algorithm for Viterbi Heuristic\n4. Checking accuracy on the test data set\n\n\n## 1. Exploring Treebank Tagged Corpus","9f0c225c":"## 2. POS Tagging Algorithm - HMM\n\nWe'll use the HMM algorithm to tag the words. Given a sequence of words to be tagged, the task is to assign the most probable tag to the word. \n\nIn other words, to every word w, assign the tag t that maximises the likelihood P(t\/w). Since P(t\/w) = P(w\/t). P(t) \/ P(w), after ignoring P(w), we have to compute P(w\/t) and P(t).\n\n\nP(w\/t) is basically the probability that given a tag (say NN), what is the probability of it being w (say 'building'). This can be computed by computing the fraction of all NNs which are equal to w, i.e. \n\nP(w\/t) = count(w, t) \/ count(t). \n\n\nThe term P(t) is the probability of tag t, and in a tagging task, we assume that a tag will depend only on the previous tag. In other words, the probability of a tag being NN will depend only on the previous tag t(n-1). So for e.g. if t(n-1) is a JJ, then t(n) is likely to be an NN since adjectives often precede a noun (blue coat, tall building etc.).\n\n\nGiven the penn treebank tagged dataset, we can compute the two terms P(w\/t) and P(t) and store them in two large matrices. The matrix of P(w\/t) will be sparse, since each word will not be seen with most tags ever, and those terms will thus be zero. \n","213065ce":"### Transition Probabilities","c74fa225":"## 4. Evaluating on Test Set","30721078":"### Emission Probabilities","e831bef3":"## 3. Viterbi Algorithm\n\nLet's now use the computed probabilities P(w, tag) and P(t2, t1) to assign tags to each word in the document. We'll run through each word w and compute P(tag\/w)=P(w\/tag).P(tag) for each tag in the tag set, and then assign the tag having the max P(tag\/w).\n\nWe'll store the assigned tags in a list of tuples, similar to the list 'train_tagged_words'. Each tuple will be a (token, assigned_tag). As we progress further in the list, each tag to be assigned will use the tag of the previous token.\n\nNote: P(tag|start) = P(tag|'.') "}}