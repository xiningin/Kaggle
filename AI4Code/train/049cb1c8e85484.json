{"cell_type":{"25ff23b0":"code","a25d09c3":"code","17c2ca38":"code","88c0d286":"code","49837b56":"code","2e64e174":"code","bcbca3b7":"code","c2b093db":"code","72441900":"code","8531c16f":"code","ebd934fe":"code","62453fed":"code","46e37a37":"code","77ae8dea":"code","f09e5933":"code","163c41df":"code","a2ac794f":"code","b23e90ea":"code","a7d446a4":"code","3ab5aece":"code","850ff991":"code","59f0a087":"code","1eb8fed5":"code","8c865c9b":"code","40556edf":"markdown","431ac116":"markdown","9b8be860":"markdown","5601ef36":"markdown","ed37d0d4":"markdown","cd9261c9":"markdown","f64662ff":"markdown","67c178b9":"markdown","4b4fcf9d":"markdown","090a9d59":"markdown","6ceac40d":"markdown","9c09a126":"markdown","8971ac7a":"markdown","9b8e5821":"markdown","75389b39":"markdown","37208ec6":"markdown","bf03cb09":"markdown","21ae59b7":"markdown","23fd87bd":"markdown","bc933e15":"markdown","0f027a5a":"markdown","6bd6c23b":"markdown","d37848f3":"markdown","7f463d8a":"markdown","8f299313":"markdown","48b26e2d":"markdown","bbe86b8a":"markdown","4509ac01":"markdown","5ffd7b19":"markdown","f63edb8d":"markdown","011733ac":"markdown"},"source":{"25ff23b0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom scipy import stats\n\n# Prevent Pandas from truncating displayed dataframes\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\n\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]","a25d09c3":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")","17c2ca38":"train.head()","88c0d286":"submission.head()","49837b56":"print(\"Training Size: {} observations, {} features\\nTest Size: {} observations, {} features\\n\".format(train.shape[0], train.shape[1], test.shape[0], test.shape[1]))","2e64e174":"set(train.columns) - set(test.columns)","bcbca3b7":"train.describe()","c2b093db":"numeric_features = train.select_dtypes(include=[np.number])\nnumeric_features.columns","72441900":"categorical_features = train.select_dtypes(include=[np.object])\ncategorical_features.columns","8531c16f":"unique_categories = pd.DataFrame(index=categorical_features.columns, columns=[\"TrainCount\", \"TestCount\"])\nfor c in categorical_features.columns:\n    unique_categories.loc[c, \"TrainCount\"] = len(train[c].value_counts())\n    unique_categories.loc[c, \"TestCount\"] = len(test[c].value_counts())\n    \nunique_categories = unique_categories.sort_values(by=\"TrainCount\", ascending=False)\nunique_categories.head()","ebd934fe":"temp = pd.melt(unique_categories.reset_index(), id_vars=\"index\")\ng = sns.catplot(y=\"index\", x=\"value\", hue=\"variable\", data=temp, kind=\"bar\", height=9)\ng.set_ylabels(\"Count\")\ng.set_xlabels(\"Categorical Variable\")\ng.set_xticklabels(rotation=90)\nplt.title(\"Number of Unique Categories by Feature\")\nplt.show()","62453fed":"nulls = train.isnull().sum()[train.isnull().sum() > 0].sort_values(ascending=False).to_frame().rename(columns={0: \"MissingVals\"})\nnulls[\"MissingValsPct\"] = nulls[\"MissingVals\"] \/ len(train)\nnulls","46e37a37":"sns.barplot(y=nulls.index, x=nulls[\"MissingValsPct\"], orient=\"h\")\nplt.title(\"% of Values Missing by Feature\")\nplt.show()","77ae8dea":"msno.matrix(train, labels=True)\nplt.show()","f09e5933":"z_threshold = 3\nz = pd.DataFrame(np.abs(stats.zscore(train[numeric_features.columns])))\noutlier_rows = z[z[z > z_threshold].any(axis=1)] # Rows with outliers\nprint(\"# Rows with potential outliers: {}\".format(len(outlier_rows)))\noutlier_rows.head()","163c41df":"fig, ax = plt.subplots(1,3, figsize=(15,5))\nsns.distplot(train[\"SalePrice\"], ax=ax[0], fit=stats.norm)\nsns.boxplot(train[\"SalePrice\"], orient='v', ax=ax[1])\nstats.probplot(train[\"SalePrice\"], plot=plt)\n\nax[0].set_title(\"SalePrice Distribution vs. Normal Distribution\")\nax[1].set_title(\"Boxplot of SalePrice\")\nax[2].set_title(\"Q-Q Plot of SalePrice\")\nax[0].set_ylabel(\"SalePrice\")\nax[1].set_xlabel(\"All Homes\")\n\nfor a in ax:\n    for label in a.get_xticklabels():\n        label.set_rotation(90)\nplt.tight_layout()\nplt.show()","a2ac794f":"log_SalePrice = np.log1p(train[\"SalePrice\"]) # Applies log(1+x) to all elements of column\nfig, ax = plt.subplots(1,3, figsize=(15,5))\nsns.distplot(log_SalePrice, ax=ax[0], fit=stats.norm)\nsns.boxplot(log_SalePrice, orient='v', ax=ax[1])\nstats.probplot(log_SalePrice, plot=plt)\n\nax[0].set_title(\"Log(SalePrice + 1) vs. Normal Distribution\")\nax[1].set_title(\"Boxplot of Log(SalePrice + 1)\")\nax[2].set_title(\"Q-Q Plot of Log(SalePrice + 1)\")\nax[0].set_ylabel(\"SalePrice\")\nax[1].set_xlabel(\"All Homes\")\n\n\nplt.tight_layout()\nplt.show()","b23e90ea":"fig = plt.figure(figsize=(15,12))\ncorr_matrix = train.corr()\nsns.heatmap(corr_matrix, square=True)\nplt.title(\"Heatmap of All Numerical Features\")\nplt.show()","a7d446a4":"fig = plt.figure(figsize=(15,12))\nsns.heatmap(corr_matrix[(corr_matrix > 0.5) | (corr_matrix < -0.5)], annot=True, annot_kws={\"size\": 9}, linewidths=0.1, square=True)\nplt.title(\"Heatmap of Highest Correlated Features\")\nplt.show()","3ab5aece":"k = 11 #number of variables for heatmap (including SalePrice)\ncols_positive = corr_matrix.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols_positive].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols_positive.values, xticklabels=cols_positive.values)\nplt.show()","850ff991":"k = 10 #number of variables for heatmap\ncols_negative = np.append(['SalePrice'], corr_matrix.nsmallest(k, 'SalePrice')['SalePrice'].index.values)\ncm = np.corrcoef(train[cols_negative].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols_negative, xticklabels=cols_negative)\nplt.show()","59f0a087":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageArea', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd', 'YearBuilt', 'YearRemodAdd']\nsns.pairplot(train[cols_positive], diag_kind='kde')\nplt.show()","1eb8fed5":"x = train.copy()\nfor c in categorical_features.columns:\n    x[c] = x[c].astype('category')\n    if x[c].isnull().any():\n        x[c] = x[c].cat.add_categories(['Missing'])\n        x[c] = x[c].fillna('Missing')\nx[\"SalePrice\"] = train[\"SalePrice\"]\nx.head()","8c865c9b":"def boxplot_custom(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x = plt.xticks(rotation=90)\n\ndf = pd.melt(x, id_vars=[\"SalePrice\"], value_vars=categorical_features)\ng = sns.FacetGrid(df, col=\"variable\", col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(boxplot_custom, \"value\", \"SalePrice\")\nplt.show()","40556edf":"Notice the significant correlation between `TotalBsmtSF`\/`1stFlrSF` and `GarageCars`\/`GarageArea`. This indicates that these features provide almost the same information; we may choose to use just one or the other. Let's isolate the features with highest correlation so their interrelationships are easier to see.","431ac116":"To accomplish this task, must train a model using the train dataset and predict values for each observation in the test dataset. What are the sizes of each of these datasets?","9b8be860":"Because there are so many categorical variables, we will likely need to identiy and utilize only the categorical variables that have the strongest correlation with the target. Alternatively, we might choose to include all variables and use penalty-based automated variable selection, such as lasso or ridge regression, to prioritize the most salient features. Topics for future notebooks :)","5601ef36":"**Check for Outliers**\n\nOne way to check for outliers is by calculating the Z-score, which represents the number of standard deviations away from the observed mean value. Typically, if $|Z-score| > 3$, the data point is considered an outlier. We may choose to remove these rows prior to training, however we will need to weigh the cost of deleting datapoints for our model.","ed37d0d4":"We can also observe the co-occurrence of nulls across features. This exhibit confirms our theory that `Bsmt_` and `Garage_` nulls tend to occur within the same observations (see the bands of horizontal white lines).","cd9261c9":"It's important to understand the variable that we are trying to predict. Below, we see that the distribution of `SalePrice` is right skewed, indicating the presence of outliers (unusually high-priced homes); we can also observe these outliers in a box plot. Lastly, we create a Q-Q to confirm that the `SalePrice` distribution does not follow a normal distribution. Since linear models generally work best with normally distributed data, we will need to manipulate `SalePrice`, for example by taking a log-transformation.","f64662ff":"After apply a log-transformation, the `SalePrice` distribution is no longer skewed and our outliers are more evenly distributed. Likewise, our Q-Q plot indicates the data is close to normally distributed.","67c178b9":"We can also \"zoom in\" to examine variables with the highest correlation; below we see the 10 variables that are most positively correlated with `SalePrice`, including `OverallQual` and `GrLivArea`.\n\nFinally, on a final feature selection note, `Id` can probably be safely deleted as it is simply a auto-incrementing label for each observation.","4b4fcf9d":"**Observe Numerical Feature Correlation with a Heatmap**\n\nDiving into the numerical features, we use a heatmap to quickly visualize which variables are correlated (move together).","090a9d59":"Does it make sense that some of these variables are highly correlated?\n\n* `GarageCars` and `GarageArea`: Since the number of cars that can fit in a garage is a byproduct of the garage's area, we expect these features to be correlated.\n* `YearBuilt` and `GarageYrBlt`: This one is more a by-product of the fact that the garage must have been built the same year or sometime after the house was built.\n* `TotalBsmtSF` and `1stFlrSF`: We would expect that the total basement square footage and 1st floor square footage are related.\n* `GrLiveArea` and `TotRmsAbvGrd`: Again, this makes sense. More rooms, more living space.","6ceac40d":"As expected, the test set is missing the `SalePrice` column. This is the target variable that we are tasked to predict.\n\nThe `.describe()` method allows us to quickly eyeball various statistics for each feature, including min, median and max.","9c09a126":"**Preview the Data**","8971ac7a":"`PoolQC`, `MiscFeature` and `Alley` are all missing a significant number of values (>90%) - these features are candidates for deletion as there is likely little information contained within the remaining entries.\n\nNotice how the `Garage_` and `Bsmt_` variables have similar numbers of missing values. One possibility is that these missing values originate from the same set of observations.\n\nIt can be helpful to view the percentage of missing values in a bar chart, to get a sense of the relativities.","9b8e5821":"Thank you very much for reading - I hope you learned a trick or two. \n\nSuggestions? Comments? Please leave me a note below.\n\nUntil next time, happy coding :)","75389b39":"**Observe the Data Structure & Expected Output**","37208ec6":"What numeric features do we have?","bf03cb09":"Now we can generate box plots. Note that by creating a category for missing values, we are able to view the distribution of sale price for those observations.","21ae59b7":"And likewise, the 10 variables which are most negatively correlated with SalePrice.","23fd87bd":"What about the categorical features? Let's use box plots to see which features might be helpful in predicting `SalePrice`.\n\nFirst, we'll need to create a new category for missing values.","bc933e15":"# House Prices: Exploratory Data Analysis\n\nGreetings! In this notebook we will explore the Ames house price data, identifying important characteristics and noting where pre-processing will be required. The purpose of this work is to lay the groundwork for later data cleaning and transformation, with the end goal of applying various machine learning strategies.\n\nThank you for reading and please leave a comment below if you have any suggestions. I am always learning and appreciate your feedback.","0f027a5a":"We can see that there the training and test sets are roughly the same size (around 1460 rows). We have a large number of features (80), some of which might be candidates for deletion.","6bd6c23b":"A quick plot of the number of unique categories per feature indicates that different categories are indeed present in train vs. test. Our model will need to handle the case when it encounters a new category that it wasn't originally trained on.","d37848f3":"**Observe the Target Distribution**","7f463d8a":"**Observe Categorical Features with Box Plots**","8f299313":"Let's create a pairplot using the positively correlated features that we identified. Take a moment to look at some of the plots to see if you can explain the relationship between various features.","48b26e2d":"Let's look at the sample submission. This shows us what our model will need to output: a sale price for each `Id` in the test set.","bbe86b8a":"**Check for Nulls**\n\nMissing values introduce bias into our dataset and can lead to biased conclusions or predictions. There are a variety of methods that can be used to deal with missing values, but first, let's just see which features contain N\/As.","4509ac01":"And what categorical features do we have?","5ffd7b19":"How many different categories are there for each of the categorical variables? Do we have categories that don't appear in both train and test?","f63edb8d":"**Conclusion**","011733ac":"**Load the Data**"}}