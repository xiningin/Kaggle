{"cell_type":{"adeccf38":"code","cc643eff":"code","885592dc":"code","cbd35ff5":"code","492a5b6c":"code","31322d21":"code","f87828a3":"code","1e9afe6b":"code","a28c6b57":"code","73b11170":"code","eebec05d":"code","acc8e0b2":"code","32f21b54":"code","ffdcdf25":"code","37739b59":"code","abb12db4":"code","ed7bcfa9":"code","daa60d34":"code","0a66244a":"code","7d7f9620":"code","c655f6ab":"code","af8a761e":"code","555b59b0":"code","dcd7beb2":"code","9c919d5b":"code","fcc8dcf0":"code","457ae79b":"code","79a5b8d5":"code","37f9dddf":"code","38c850e2":"code","94766d9c":"code","c6c3f5a2":"code","48e16885":"code","d0bcb9c0":"code","ad412982":"code","6f70f5bb":"code","5bb4f14d":"code","10e3318a":"code","e7a968ae":"code","88006996":"code","c1a1c380":"code","a5eaaaa4":"code","42d5317d":"code","ecd62df0":"code","f6fd0582":"code","04aecead":"code","644a253b":"code","f146481f":"code","d64501df":"code","8c1ca1e7":"code","1250a6f8":"code","84343e31":"code","bd6d5126":"code","99cf8563":"code","e19e93b8":"code","40797ce8":"code","c676c819":"code","444081e7":"code","34773112":"code","c426ef34":"code","02097522":"code","cc5eb980":"code","66175026":"code","dbe2741c":"code","255b1f7d":"code","696e6f23":"code","0e61fafc":"code","44e63b3d":"code","d29bd5e4":"code","49bad750":"code","8be428fc":"code","9571fdec":"code","dd54318c":"code","4e6fa5ad":"code","9d6931c5":"code","fbc36c2a":"code","458fa18a":"code","ed0aa4c7":"code","5dd9cc8d":"code","a38394ed":"code","644b8a77":"code","c70fc225":"code","52409d40":"code","3351aefd":"code","f61b242f":"markdown","aa5c5114":"markdown","c7726980":"markdown","6f828206":"markdown","0236a9e0":"markdown","c42efe2a":"markdown","8d62cd97":"markdown","56f4d9c7":"markdown","f07829c4":"markdown","05c16867":"markdown","a0d6184c":"markdown","57feeeec":"markdown","f31323cf":"markdown","7dc4baef":"markdown","6b205464":"markdown","978a17ae":"markdown","df6e856c":"markdown","79bf64a5":"markdown","166ada5b":"markdown","33d4a112":"markdown","ef15073a":"markdown","8cebb9a3":"markdown","d709d28f":"markdown","5158b55e":"markdown","a5c5dfdf":"markdown","2517e5d3":"markdown","0c9ba319":"markdown","1860eb7c":"markdown","3550e728":"markdown","79921445":"markdown","5ec77a26":"markdown","770b8878":"markdown","5e00a053":"markdown","34e8bd1e":"markdown","20351fd7":"markdown","52f50cba":"markdown","c60ffa23":"markdown"},"source":{"adeccf38":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n","cc643eff":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom collections import defaultdict\nfrom collections import Counter\nplt.style.use('ggplot')\nstop = set(stopwords.words('english'))\nimport re\nfrom nltk.tokenize import word_tokenize\nimport gensim\nimport string\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM, Dense, SpatialDropout1D\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam","885592dc":"import os\n#os.listdir('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt')","cbd35ff5":"import os\n","492a5b6c":"tweet= pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntweet.head(3)","31322d21":"print('There are {} rows and {} columns in train'.format(tweet.shape[0],tweet.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","f87828a3":"print('{}row and {} columns in train'.format(tweet.shape[0], tweet.shape[1]))\nprint('{}row and {} columns in test'.format(test.shape[0], tweet.shape[1]))","1e9afe6b":"x=tweet.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","a28c6b57":"tweet.target.value_counts().plot('bar')\nplt.gca().set_ylabel('samples')","73b11170":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()\n","eebec05d":"fig, ax = plt.subplots(1,2,figsize=(10,5))\ntweet_len1 = tweet[tweet.target == 1] ['text'].str.len()\ntweet_len2 = tweet[tweet.target == 0] ['text'].str.len()\n\nax[0].hist(tweet_len1, color = 'red')\nax[1].hist(tweet_len2, color = 'green')\n\nax[0].set_title('not disaster tweets')\nax[1].set_title('disaster tweets')\nfig.suptitle('Characters in tweets')\nplt.show()","acc8e0b2":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntweet_len=tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len,color='red')\nax1.set_title('disaster tweets')\ntweet_len=tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='green')\nax2.set_title('Not disaster tweets')\nfig.suptitle('Words in a tweet')\nplt.show()\n","32f21b54":"fig, (ax1,ax2) = plt.subplots(2,1, figsize = (10,5))\ntweet_len1 = tweet[tweet.target == 0]['text'].str.split().map(lambda x : len(x))\ntweet_len2 = tweet[tweet.target == 1]['text'].str.split().map(lambda x : len(x))\n\nax1.hist(tweet_len1, color='red')\nax2.hist(tweet_len2, color='green')\n\nax1.set_title('not disaster')\nax2.set_title('disaster')\n\nfig.suptitle('words in tweet')\nplt.show()","ffdcdf25":"fig,(ax1,ax2, ax3, ax4)=plt.subplots(1,4,figsize=(10,5))\nword=tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('disaster')\nsns.distplot(word.map(lambda x : np.mean(x)),kde = False, ax=ax2, color = 'blue')\nword=tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax3,color='green')\nax3.set_title('Not disaster')\nsns.distplot(word.map(lambda x : np.mean(x)),kde =False ,ax = ax4, color = 'yellow')\nfig.suptitle('Average word length in each tweet')","37739b59":"tweet[tweet.target ==1]['text'].str.split()","abb12db4":"word = tweet[tweet.target == 0]['text'].str.split().map(lambda x : [len(i) for i in x])\nprint(word)","ed7bcfa9":"# map\uc774\ub791 apply \ub458\ub2e4 \uc0ac\uc6a9\ud574\ub3c4 \ub428. \ub2e4\ub9cc lambda \uc2dd\uc5d0\uc11c \ub300\uad04\ud638\uc788\ub294\uac70 \ucc38\uc870.\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize = (10,5))\nword = tweet[tweet.target == 1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.apply(lambda x : np.mean(x)), color = 'red', ax=ax1)\nax1.set_title('disaster')\n\nword  = tweet[tweet.target == 0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.apply(lambda x : np.mean(x)), ax=ax2, color='green')\nax2.set_title('no disaster')\n\nplt.show()","daa60d34":"def create_corpus(target):\n    corpus=[]\n    \n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","0a66244a":"def create_corpus(target):\n    corpus = []\n    \n    for x in tweet[tweet.target == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","7d7f9620":"tweet.head()","c655f6ab":"corpus = create_corpus(0)\n\ndict = defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dict[word] +=1\ntop = sorted(dict.items(), key = lambda x: x[1], reverse = True)[:10]\nprint(top)\n","af8a761e":"# defaultdict \ub791 stop \uc740 \ub178\ud2b8\ubd81 \uccab\ubd80\ubd84\uc5d0\uc11c import \ubc0f \uc815\uc758\ud568. \n# stop \uc740 \ubd88\uc6a9\uc5b4\ub4e4\uc744 \ubaa8\uc544 \ub193\uc740 \uac83. \ubd88\uc6a9\uc5b4\ub780 i, my \ub4f1 \ubd84\uc11d\uc5d0 \ud070 \uc758\ubbf8\uc5c6\ub294 \ub2e8\uc5b4\ub4e4.\n# defaultdict \ub294 \ube48 \ub515\uc154\ub108\ub9ac\ub97c \ub9cc\ub4dc\ub294 \ub370, int, list \ub4f1\uc73c\ub85c \ucd08\uae30\uac12\uc744 \uc124\uc815\ud560 \uc218 \uc788\ub294 \uac83.\n\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nprint(dic)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n        \n\nprint(dic.items())\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \nprint('\\n',top)\n","555b59b0":"print(dic)","dcd7beb2":"x,y=zip(*top)\nplt.bar(x,y)","9c919d5b":"key, value = zip(*top)\nplt.bar(key, value)","fcc8dcf0":"corpus=create_corpus(1)\n\ndic=defaultdict(int)\nfor word in corpus:\n    if word in stop:\n        dic[word]+=1\n\ntop=sorted(dic.items(), key=lambda x:x[1],reverse=True)[:10] \n    \n\n\nx,y=zip(*top)\nplt.bar(x,y)","457ae79b":"#string.punctaution \uc740 \uc77c\ub828\uc758 \ud2b9\uc218\uae30\ud638\ub97c \ub098\uc5f4\ud55c str \ud0c0\uc785\uc758 \uac1d\uccb4.\n\nplt.figure(figsize=(10,5))\ncorpus=create_corpus(1)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n\n# \ubc11\uc5d0\uc11c \ubcfc \uc218 \uc788\ub4ef\uc774 dict.items() \uac1d\uccb4\ub294 \ud55c \ub9ac\uc2a4\ud2b8 \uc548\uc5d0 key\uc640 value\uac00 \ubb36\uc5ec\uc838 \uc788\ub294 \ud615\ud0dc\ub85c, asteroid(*)\ub97c \ud1b5\ud574 \ub9ac\uc2a4\ud2b8\ub97c \ubc97\uaca8\ub0b4\uace0\n# zip \ud568\uc218\ub97c \ud1b5\ud574 key\ub294 key \ub07c\ub9ac, value\ub294 value\ub07c\ub9ac \ubb36\uc5b4\uc900\ub2e4.\nx,y=zip(*dic.items())\nplt.bar(x,y)","79a5b8d5":"type(special)\nprint(special)","37f9dddf":"print((dict.items()))\nprint('\\n', len(dict.items()))","38c850e2":"plt.figure(figsize = (10,5) )\ncorpus = create_corpus(0)\ndict = defaultdict(int)\nfor word in corpus:\n    if word in string.punctuation:\n        dict[word] +=1\nkey, value = zip(*dict.items())\nplt.bar(key, value)","94766d9c":"plt.figure(figsize=(10,5))\ncorpus=create_corpus(0)\n\ndic=defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\n        \nx,y=zip(*dic.items())\nplt.bar(x,y,color='green')","c6c3f5a2":"corpus = create_corpus(0)\n\ndic = defaultdict(int)\nfor word in corpus:\n    if word in string.punctuation:\n        dic[word] +=1\nplt.figure(figsize = (10,5))\nkey, value = zip(*dic.items())\nplt.bar(key,value, color = 'green')","48e16885":"# Counter \ud568\uc218\ub294 \uc785\ub825\uc5d0 \ub300\ud574\uc11c \uadf8 \uc785\ub825\uc744 \uad6c\uc131\ud558\ub294 \uac83\ub4e4\uc758 \uac1c\uc218\ub97c \uad6c\ud574\uc11c \ub515\uc154\ub108\ub9ac\ud615\ud0dc\ub85c \ubc18\ud658\ud55c\ub2e4.\ncounter=Counter(corpus)\nprint(sorted(counter.items(), key = lambda x: x[1], reverse =True)[:10])\n\n# most_commom() \uc740 \uadf8\ub0e5 \uc704\uc640 \uac19\uc740 \ubc29\ubc95\uc73c\ub85c sorted \ud55c \uac83.\nmost=counter.most_common()\n\nx=[]\ny=[]\n\nfor word,count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)","d0bcb9c0":"# \ucc38\uace0\ub85c stop \uc5d0 \ud3ec\ud568\ub41c \ubd88\uc6a9\uc5b4\ub294 'the' \uc600\uace0, 'The'\ub294 \ud3ec\ud568\uc548\ub428.\nsns.barplot(x=y,y=x)","ad412982":"corpus = create_corpus(0)\ncounter = Counter(corpus)\n\nx= []\ny= []\n\nmost = counter.most_common()\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\n\n        ","6f70f5bb":"sns.barplot(x = y, y = x)","5bb4f14d":"# ngram_range = (a, b) \uc774\uba74 a\ubd80\ud130 b\uae4c\uc9c0\uc758 \ub2e8\uc5b4 \ubb36\uc74c\uc744 \uc2e4\uc2dc\ud55c\ub2e4. a = b\uc774\uba74 a\uac1c\uc758 \ub2e8\uc5b4\ubb36\uc74c\ub9cc \uc2e4\uc2dc.\n# \uc6d0\ub798\ub294 vector\ud654\ud558\uba74 (\ubb38\uc7a5\uac1c\uc218, \ub2e8\uc5b4\uc885\ub958)\uc758 \uc5b4\ub9c8\uc5b4\ub9c8\ud558\uac8c \ud070 \ud76c\uc18c\ud589\ub82c\uc774 \ub9cc\ub4e4\uc5b4\uc9c0\uc9c0\ub9cc, CSR \ubc29\uc2dd\uc73c\ub85c \uc800\uc7a5\ud558\uba74 \uc6a9\ub7c9\uc744 \ub9ce\uc774 \uc904\uc77c \uc218 \uc788\ub2e4.\n# \ubc11\uc5d0\uc11c bag_of_words\ub294 CSR \ud615\uc2dd\uc774\uba70, print \ud588\uc744 \ub54c\ub294 ((\ubb38\uc7a5\uc758\uc21c\uc11c, \uba87\ubc88\uc9f8 \ub2e8\uc5b4\uc778\uc9c0), \ub2e8\uc5b4\ub4f1\uc7a5 \uac1c\uc218) \uac00 \ubc18\ud658\ub41c\ub2e4. \n# sum_words \ub294 \ud76c\uc18c\ud589\ub82c\ub2e8\uacc4\uc758 BOW\uc5d0\uc11c axis=0\uc774\ubbc0\ub85c \uac01 \ub2e8\uc5b4\uc758 \ucd1d \ub4f1\uc7a5\ud69f\uc218\uc774\ub2e4. \n# vec.vocabulary_.items() \ub294 2-gram \uc9dc\ub9ac \ub2e8\uc5b4\uc640 \uadf8 \ub2e8\uc5b4\uc758 \uc21c\uc11c(index)\ub85c \uc774\ub8e8\uc5b4\uc9c4 \ub515\uc154\ub108\ub9ac.\n\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    print(vec,'\\n')\n    bag_of_words = vec.transform(corpus)\n    print(type(bag_of_words),'\\n',bag_of_words.shape)\n    print(bag_of_words,'\\n')\n    sum_words = bag_of_words.sum(axis=0) \n    print(sum_words,'\\n', print('\ubaa8\uc591:',sum_words.shape))\n    print(vec.vocabulary_.items())\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]","10e3318a":"plt.figure(figsize=(10,5))\ntop_tweet_bigrams=get_top_tweet_bigrams(tweet['text'])[:10]\n\n# \ud0a4\ub300\ub85c, value\ub300\ub85c zip\ud558\uace0 \uac01\uac01\uc744 list\ud654\nx,y=map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x)","e7a968ae":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range = (2,2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return words_freq[:n]\n    ","88006996":"plt.figure(figsize = (10,5))\na = get_top_tweet_bigrams(tweet['text'], n=10)\nx, y = map(list, zip(*a))\nsns.barplot(y,x)","c1a1c380":"df=pd.concat([tweet,test])\ndf.shape","a5eaaaa4":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","42d5317d":"# sub\ub294 (\ub300\uccb4\ud560 \uac83, \ub300\uc0c1text) \ub97c \uc778\uc218\ub85c \ud55c\ub2e4. \ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","ecd62df0":"df['text']=df['text'].apply(lambda x : remove_URL(x))","f6fd0582":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'-',text)\n\nremove_URL(example)","04aecead":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","644a253b":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\nprint(remove_html(example))","f146481f":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\nprint(remove_html(example))","d64501df":"df['text']=df['text'].apply(lambda x : remove_html(x))","8c1ca1e7":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","1250a6f8":"df['text']=df['text'].apply(lambda x: remove_emoji(x))\n","84343e31":"# maketrans \ub294 input \ubb38\uc790\uc5f4 \uac01\uac01\uc744 output \ubb38\uc790\uc5f4\ub85c \ubc14\uafb8\uc5b4\uc900\ub2e4. \ub530\ub77c\uc11c \ub458 \uac04\uc758 \uae38\uc774\uac00 \uac19\uc544\uc57c\ud568. \n# translate \uc740 \uc2e4\uc81c\ub85c \uadf8\ub807\uac8c \ubc30\uc6b4 \uaddc\uce59\uc744 text\uc5d0 \uc801\uc6a9\uc2dc\ud0a4\ub294 \uac83. \n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","bd6d5126":"print(len(string.punctuation))","99cf8563":"def remove_punct(text):\n    table = str.maketrans(string.punctuation, '                                ')\n    return text.translate(table)\nprint(remove_punct(example))\n    ","e19e93b8":"df['text']=df['text'].apply(lambda x : remove_punct(x))","40797ce8":"!pip install pyspellchecker","c676c819":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)\n        \ntext = \"corect me plese\"\ncorrect_spellings(text)","444081e7":"from spellchecker import SpellChecker\n\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split())\n    for word in text.split():\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return ' '.join(corrected_text)\ntext = 'corect me plese'\ncorrect_spellings(text)","34773112":"df['text']=df['text'].apply(lambda x : correct_spellings(x))","c426ef34":"# tqdm \ub77c\uc774\ube0c\ub7ec\ub9ac\ub294 \uc9c4\ud589\uc0c1\ud669\uc744 \ud45c\uc2dc\ud574\uc90c. for \ubb38 \uc548\uc5d0 \uae30\uc874 \uc791\uc5c5\ud560 \uac83(?)\uc744 tqdm()\uc5d0 \ub123\uc5b4\uc8fc\uba74 \ub428. \ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet)if ((word.isalpha ==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","02097522":"\ndef create_corpus(df):\n    corpus=[]\n    for tweet in tqdm(df['text']):\n        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n        corpus.append(words)\n    return corpus\n        \n        ","cc5eb980":"corpus=create_corpus(df)","66175026":"#\ubc11\uc5d0 \uac00\uc838\uc624\ub294 \ud30c\uc77c\uc740 \ud55c line.\uc774 (word, 100\ucc28\uc6d0 embedding \ubca1\ud130)\ub85c \uad6c\uc131\ub41c text \ud30c\uc77c\nembedding_dict={}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f:\n    for line in f:\n        values=line.split()\n        word=values[0]\n        vectors=np.asarray(values[1:],'float32')\n        embedding_dict[word]=vectors\nf.close()","dbe2741c":"embedding_dict = {}\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt', 'r') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1], 'float32')\n        embedding_dict[word] = vectors\nf.close()","255b1f7d":"print(type(word))","696e6f23":"# texts_to_sequences() \ub294 \ubb38\uc7a5\uc744 index\uc758 \ub098\uc5f4\ub85c \ubc14\uafb8\uc5b4\uc900\ub2e4. \n# https:\/\/codetorial.net\/tensorflow\/natural_language_processing_in_tensorflow_01.html \ucc38\uc870\n# https:\/\/wikidocs.net\/32105 \ucc38\uc870\n\nMAX_LEN=50\ntokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences=tokenizer_obj.texts_to_sequences(corpus)\n\n# pad_sequences() \ub294 \uc11c\ub85c \uae38\uc774\uac00 \ub2e4\ub978 \uc2dc\ud000\uc2a4\ub4e4\uc744 max_len\uc5d0 \ub9de\uac8c \uc790\ub974\uac70\ub098 padding \ud558\ub294 \ud568\uc218\n# post \ub294 \ub4a4\uc5d0\uaebc\ub97c \uc790\ub978\ub2e4\ub294 \uc758\ubbf8\uc778\ub4ef. \ntweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')","0e61fafc":"print(len(corpus))","44e63b3d":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences ,maxlen = MAX_LEN, truncating = 'post', padding='post')\n","d29bd5e4":"print(tokenizer_obj)\nword_index = tokenizer_obj.word_index\nprint('num of unique words:', len(word_index))\nprint(word_index)","49bad750":"word_index=tokenizer_obj.word_index\nprint('Number of unique words:',len(word_index))","8be428fc":"# +1 \uc744 \ud574\uc8fc\ub294 \uc774\uc720\ub294 \ub4a4\uc5d0 i > num_words \ub97c \ud558\ub824\ub294 \uc758\ub3c4\nnum_words=len(word_index)+1\nembedding_matrix=np.zeros((num_words,100))\n\n# i \uac00 num_words \ubcf4\ub2e4 \ud074 \uc218\ub294 \uc5c6\uc73c\ubbc0\ub85c, \uc624\ub958\ubc29\uc9c0(?)\ucc28\uc6d0\uc5d0\uc11c \uc801\uc740 \ub4ef.\nfor word,i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n# get \uc744 \ud1b5\ud574 \ub515\uc154\ub108\ub9ac\uc5d0\uc11c value\ub97c get\ud560 \uc218 \uc788\ub2e4.     \n    emb_vec=embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i]=emb_vec\n            ","9571fdec":"num_words = len(word_index) +1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","dd54318c":"# \ubcf4\ub2e4\uc2dc\ud53c num_words\uc5d0 +1\uc744 \ud588\uc73c\ubbc0\ub85c \ub9c8\uc9c0\ub9c9 \ud589\uc740 for \ubb38\uc5d0 \uc758\ud574 \ucd94\uac00\ub418\uc9c0 \uc54a\uc73c\ubbc0\ub85c \uadf8\ub0e5 \uae30\uc874 0\uc774\ub2e4. \nprint(embedding_matrix[-1])","4e6fa5ad":"model=Sequential()\n\n# \uae30\uc874\uc5d0 \ub2e4\uc6b4\ubc1b\uc740 embedding_matrix\ub97c \ud6c8\ub828\ud558\uc9c0 \uc54a\uace0 \uadf8\ub300\ub85c \ud65c\uc6a9\ud560 \uc0dd\uac01\nembedding=Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(1, activation='sigmoid'))\n\n\noptimzer=Adam(learning_rate=1e-5)\n\nmodel.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])\n\n","9d6931c5":"model = Sequential()\nembedding = Embedding(num_words, 100, embeddings_initializer = Constant(embedding_matrix))\n\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(64, dropout = 0.2, recurrent_dropout = 0.2))\nmodel.add(Dense(1, activation = 'sigmoid'))\n\noptimizer = Adam(learning_rate = 1e-5)\nmodel.compile(loss = 'binary_crossentropy', optimizer = optimizer, metrics = ['accuracy'])","fbc36c2a":"model.summary()","458fa18a":"print(tweet_pad.shape)\ntrain=tweet_pad[:tweet.shape[0]]\ntest=tweet_pad[tweet.shape[0]:]\nprint(test.shape, train.shape)\nprint(tweet.shape)","ed0aa4c7":"X_train,X_test,y_train,y_test=train_test_split(train,tweet['target'].values,test_size=0.15)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","5dd9cc8d":"history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","a38394ed":"tweet_pad","644b8a77":"sample_sub=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\nsample_sub.head()","c70fc225":"y_pre = model.predict(test)\ny_pre = np.round(y_pre).astype(int).reshape(3263)\nsub = pd.DataFrame({'id': sample_sub['id'], 'target':y_pre})\nsub.to_csv('submission.csv', index = False)\nsub.head()","52409d40":"y_pre=model.predict(test)\nprint(y_pre.shape)\ny_pre=np.round(y_pre).astype(int).reshape(3263)\nsub=pd.DataFrame({'id':sample_sub['id'].values.tolist(),'target':y_pre})\nsub.to_csv('submission.csv',index=False)\nsub.head()\n","3351aefd":"sub.head()","f61b242f":"## Making our submission","aa5c5114":"### Removing HTML tags","c7726980":"### Importing required Libraries.","6f828206":"### Common stopwords in tweets","0236a9e0":"Even if I'm not good at spelling I can correct it with python :) I will use `pyspellcheker` to do that.","c42efe2a":"<font size='5' color='red'>  if you like this kernel,please do an upvote.<\/font>","8d62cd97":"### Romoving Emojis","56f4d9c7":"We will need lot of cleaning here..","f07829c4":"Lot of cleaning needed !","05c16867":"### Removing urls","a0d6184c":"In both of them,\"the\" dominates which is followed by \"a\" in class 0 and \"in\" in class 1.","57feeeec":"### Number of characters in tweets","f31323cf":"### Common words ?","7dc4baef":"Now,we will move on to class 0.","6b205464":"###  Average word length in a tweet","978a17ae":"### Spelling Correction\n","df6e856c":"Before we begin with anything else,let's check the class distribution.There are only two classes 0 and 1.","79bf64a5":"First,we will do very basic analysis,that is character level,word level and sentence level analysis.","166ada5b":"### Ngram analysis","33d4a112":"## GloVe for Vectorization","ef15073a":"## Loading the data and getting basic idea ","8cebb9a3":"### Analyzing punctuations.","d709d28f":"## What's in this kernel?\n- Basic EDA\n- Data Cleaning\n- Baseline Model","5158b55e":"## Baseline Model","a5c5dfdf":"### Basic Intro \n\nIn this competition, you\u2019re challenged to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t.\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTigQWzoYCNiDyrz1BN4WTf2X2k9OZ_yvW-FsmcIMsdS9fppNmh)","2517e5d3":"Here we will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.We will try 100 D here.","0c9ba319":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","1860eb7c":"### Removing punctuations","3550e728":"## Class distribution","79921445":"First let's check tweets indicating real disaster.","5ec77a26":"## Exploratory Data Analysis of tweets","770b8878":"First we  will analyze tweets with class 0.","5e00a053":"Now,we will analyze tweets with class 1.","34e8bd1e":"ohh,as expected ! There is a class distribution.There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","20351fd7":"## Data Cleaning\nAs we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","52f50cba":"we will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.","c60ffa23":"### Number of words in a tweet"}}