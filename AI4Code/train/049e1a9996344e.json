{"cell_type":{"31534c96":"code","f918429a":"code","17a2f5a1":"code","392ee82b":"code","653ac38a":"code","67b11835":"code","8a10e69a":"code","b1da2db1":"code","9b6a9356":"code","a82a641b":"code","c1bf6ee9":"code","190fdea3":"code","46c4e71f":"code","738a08d7":"code","2599c99f":"code","3b434312":"code","80547579":"code","d4c96ef6":"code","9531b2fb":"code","8a02eef6":"code","520b6daf":"code","1bb840c8":"code","6803691f":"code","c8c1521e":"code","9c8ce3e8":"code","9184146b":"code","784f67bf":"code","c4754923":"code","2e84383f":"code","952f0bb6":"code","625bdc91":"code","88e85d26":"code","c73aab32":"code","c8109c8d":"code","4da737c6":"code","f323333e":"code","80ae6088":"code","5b855c9e":"code","851f27c8":"code","30a930ed":"code","5b5a123e":"code","58eb4b7c":"code","8c3f4f62":"code","4307bb93":"code","212638a8":"code","a214eed5":"code","bc80dee1":"code","a73af234":"code","5278f106":"code","c0d6d7c5":"code","61598113":"code","68ef6bf3":"code","89d55b05":"code","aed2323b":"code","52bddeb2":"code","3c446b61":"code","b8f7ffb9":"markdown","01c20e1a":"markdown","a4b8e52d":"markdown","6761f0db":"markdown","e83c97f1":"markdown","348c17f1":"markdown","129dd354":"markdown","92f55ab2":"markdown","e5484f81":"markdown","7389a004":"markdown","7238923a":"markdown","2504fed4":"markdown","3223729f":"markdown","da4001bc":"markdown","27fabce6":"markdown","4af3eca0":"markdown","f88ddbeb":"markdown","6afb6c26":"markdown"},"source":{"31534c96":"import numpy as np # Multi-dimensional array object\nimport pandas as pd # Data Manipulation\nimport matplotlib.pyplot as plt # Data Visualization\nimport seaborn as sns # Data Visualization\nimport plotly.express as px # Interactive Data Visualization\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot # Offline version of the Plotly modules.\nimport cufflinks as cf # Works as a connector between the pandas library and plotly\ncf.go_offline() \n\n","f918429a":"#load data on dataframe\ntelco_df = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n#display dataframe\ntelco_df.head()\n","17a2f5a1":"#count of rows and columns\ntelco_df.shape","392ee82b":"# Get data frame info\ntelco_df.info()","653ac38a":"# Change TotalCharges column type to numeric\ntelco_df.TotalCharges = pd.to_numeric(telco_df.TotalCharges, errors='coerce')\ntelco_df.info()","67b11835":"# Get the statistics of the data frame\ntelco_df.describe()","8a10e69a":"# There are 11 missing values out of 7043 values in TotalCharges column. I prefer deleting rows with null values \ntelco_df.dropna(inplace = True)","b1da2db1":"# We don't need customerID column for analyzing\ntelco_df.drop(['customerID'], axis =1, inplace =True)","9b6a9356":"telco_df.gender = [1 if each == \"Male\" else 0 for each in telco_df.gender]\n\ncolumns_to_convert = ['Partner', \n                      'Dependents', \n                      'PhoneService', \n                      'OnlineSecurity',\n                      'OnlineBackup',\n                      'DeviceProtection',\n                      'TechSupport',\n                      'StreamingTV',\n                      'StreamingMovies',\n                      'PaperlessBilling', \n                      'Churn',\n                      'MultipleLines']\n\nfor item in columns_to_convert:\n    telco_df[item] = [1 if each == \"Yes\" else 0 if each == \"No\" else -1 for each in telco_df[item]]\n\ntelco_df.head()","a82a641b":"telco_df.info()","c1bf6ee9":"sns.countplot(x=\"Churn\",data=telco_df);","190fdea3":"counts = telco_df['Churn'].value_counts()\ncounts","46c4e71f":"fig, ax = plt.subplots()\nax.pie(counts, autopct='%1.1f%%')\nax.legend(labels=['1(Stay)', '0(Left)'], title='Customer Churn',loc='lower right')\nax.set_title(\"Customer Churn Percentage\")\nplt.show()","738a08d7":"sns.pairplot(telco_df,vars = ['TotalCharges','MonthlyCharges','tenure'], hue=\"Churn\")","2599c99f":"counts = telco_df['InternetService'].value_counts()\ncounts","3b434312":"sns.set(style=\"whitegrid\")\nsns.countplot(x=\"InternetService\", data = telco_df)","80547579":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"InternetService\", y=\"Churn\", data=telco_df,kind=\"bar\")\ng1.set(xlabel='InternerService', ylabel = 'Churn Probability')\nplt.show()","d4c96ef6":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"Contract\", y=\"Churn\", data=telco_df,kind=\"bar\")\ng1.set(xlabel='Contract', ylabel = 'Churn Probability')\nplt.show()","9531b2fb":"sns.set(style=\"whitegrid\")\ng1=sns.catplot(x=\"PaymentMethod\", y=\"Churn\", data=telco_df,kind=\"bar\")\ng1.set(xlabel='PaymentMethod', ylabel = 'Churn Probability')\nplt.show()","8a02eef6":"# Let's create new columns by using pandas get_dummies function.\ntelco_df = pd.get_dummies(data=telco_df)\ntelco_df.head()","520b6daf":"# Now see the correlation between churn and all of the columns.\ntelco_df.corr()['Churn'].sort_values()","1bb840c8":"telco_df.info()","6803691f":"# Assigning y as input and X as output\ny = telco_df['Churn']\nnew_data = telco_df.drop(['Churn'], axis=1)\nX =  (new_data-np.min(new_data))\/(np.max(new_data)-np.min(new_data)).values","c8c1521e":"X[0:5]","9c8ce3e8":"y[0:5]","9184146b":"#Split data into Train and Test \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state =1)","784f67bf":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_rf=RandomForestClassifier(n_estimators = 5, random_state = 1)\nmodel_rf.fit(X_train,y_train.values.ravel())","c4754923":"y_predict1=model_rf.predict(X_test)","2e84383f":"from sklearn.metrics import classification_report, confusion_matrix\nprint(classification_report(y_test,y_predict1))","952f0bb6":"# Find the best number for n_estimators\nscore_array = []\nfor number in range(1,50):\n    rf_loop = RandomForestClassifier(n_estimators = number, random_state = 1) #set K neighbor as 3\n    rf_loop.fit(X_train,y_train)\n    score_array.append(rf_loop.score(X_test,y_test))\n    \nplt.plot(range(1,50),score_array)\nplt.xlabel(\"Range\")\nplt.ylabel(\"Score\")\nplt.show()","625bdc91":"# Let's use the n_estimators = 33\n\nmodel_rf=RandomForestClassifier(n_estimators = 33, random_state = 1)\nmodel_rf.fit(X_train,y_train.values.ravel())\nprint(\"Random Forest accuracy for 5 trees is :\",model_rf.score(X_test,y_test))","88e85d26":"# Another way of looking at the accuracy of the classifier\ncm=confusion_matrix(y_test,y_predict1)\nsns.heatmap(cm,annot=True)","c73aab32":"from sklearn.calibration import CalibratedClassifierCV # For probability score output\nfrom sklearn.svm import LinearSVC\n\nmodel_svm=LinearSVC(max_iter=10000)\nmodel_svm=CalibratedClassifierCV(model_svm)\nmodel_svm.fit(X_train,y_train)\n","c8109c8d":"y_predict2=model_svm.predict(X_test)","4da737c6":"print(classification_report(y_test,y_predict2))","f323333e":"cm=confusion_matrix(y_test,y_predict2)\nsns.heatmap(cm, annot=True)\n\n","80ae6088":"from sklearn.neighbors import KNeighborsClassifier\n\nmodel_knn=KNeighborsClassifier()\nmodel_knn.fit(X_train,y_train)","5b855c9e":"y_predict3=model_knn.predict(X_test)","851f27c8":"print(classification_report(y_test,y_predict3))","30a930ed":"cm=confusion_matrix(y_test,y_predict3)\nsns.heatmap(cm,annot=True)","5b5a123e":"from sklearn.naive_bayes import GaussianNB","58eb4b7c":"model_gnb=GaussianNB()\nmodel_gnb.fit(X_train, y_train)","8c3f4f62":"y_predict4=model_gnb.predict(X_test)","4307bb93":"print(classification_report(y_test, y_predict4))","212638a8":"cm = confusion_matrix(y_test, y_predict4)\nsns.heatmap(cm, annot = True)","a214eed5":"# %%Decision Tree Classification\nfrom sklearn.tree import DecisionTreeClassifier\nmodel_dt = DecisionTreeClassifier()\nmodel_dt.fit(X_train,y_train)","bc80dee1":"y_predict5=model_dt.predict(X_test)","a73af234":"print(classification_report(y_test, y_predict5))","5278f106":"cm = confusion_matrix(y_test, y_predict5)\nsns.heatmap(cm, annot = True)","c0d6d7c5":"# Print all results of each algorithm\nprint(\"Random Forest Classifier accuracy :\", model_rf.score(X_test,y_test))\nprint(\"SVM accuracy :\", model_svm.score(X_test,y_test))\nprint(\"KNN accuracy :\", model_svm.score(X_test,y_test))\nprint(\"Naive Bayes accuracy :\", model_svm.score(X_test,y_test))\nprint(\"Decision Tree accuracy :\", model_dt.score(X_test,y_test))","61598113":"# Compare the Classication_report for each algorithm\nprint(\"Random Forest Classifier(RFC):\")\nprint(\"RFC:\",classification_report(y_test, y_predict1))\nprint()\nprint(\"SVM:\", classification_report(y_test, y_predict2))\nprint()\nprint(\"KNN:\")\nprint(classification_report(y_test, y_predict3))\nprint()\nprint(\"Naive Bayes:\")\nprint(classification_report(y_test, y_predict4))\nprint()\nprint(\"Decision Tree:\")\nprint(classification_report(y_test, y_predict5))\n","68ef6bf3":"model_svm.predict_proba(X_test)","89d55b05":"model_svm.predict_proba(X_test)[:, 1]","aed2323b":"# ROC curve\nfrom sklearn.metrics import roc_curve\n\nfpr1, tpr1, thresh1 = roc_curve(y_test, model_svm.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr2, tpr2, thresh2 = roc_curve(y_test, model_dt.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr3, tpr3, thresh3 = roc_curve(y_test, model_gnb.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr4, tpr4, thresh4 = roc_curve(y_test, model_knn.predict_proba(X_test)[:, 1], pos_label = 1)\nfpr5, tpr5, thresh5 = roc_curve(y_test, model_rf.predict_proba(X_test)[:, 1], pos_label = 1)\n","52bddeb2":"# AUC score\n\nfrom sklearn.metrics import roc_auc_score\nauc_score1 = roc_auc_score(y_test, model_svm.predict_proba(X_test)[:, 1])\nauc_score2 = roc_auc_score(y_test, model_dt.predict_proba(X_test)[:, 1])\nauc_score3 = roc_auc_score(y_test, model_gnb.predict_proba(X_test)[:, 1])\nauc_score4 = roc_auc_score(y_test, model_knn.predict_proba(X_test)[:, 1])\nauc_score5 = roc_auc_score(y_test, model_rf.predict_proba(X_test)[:, 1])\n\n\n\nprint(\"Support Vector Machine: \", auc_score1) # Support Vector Machine\nprint(\"Decision Tree: \", auc_score2) # Decision Tree\nprint(\"Naive Bayes: \", auc_score3) # Naive Bayes\nprint(\"K-Nearest Neighbors: \", auc_score4) # K-Nearest Neighbors\nprint(\"Random Forest: \", auc_score5) # Random Forest","3c446b61":"plt.plot(fpr1, tpr1, linestyle = \"--\", color = \"black\", label = \"SVM\")\nplt.plot(fpr2, tpr2, linestyle = \"--\", color = \"red\", label = \"Decision Tree\")\nplt.plot(fpr3, tpr3, linestyle = \"--\", color = \"green\", label = \"Naive bayes\")\nplt.plot(fpr4, tpr4, linestyle = \"--\", color = \"purple\", label = \"KNN\")\nplt.plot(fpr5, tpr5, linestyle = \"--\", color = \"orange\", label = \"Random Forest\")\n\n\n#plt.title(\"Receiver Operator Characteristics ROC\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive rate\")\n\nplt.legend(loc = 'best')\nplt.savefig('ROC', dpi = 300)\nplt.show()","b8f7ffb9":"### RandomForestClassifier","01c20e1a":"* **SVM** performed the highest F1 Score among all algorithms. F1 score is the harmonic average of the precision and recall. The nearer the F1 score to the value 1 the better the precision and recall.\n* **Precision** is a measure of the accuracy provided that a class label has been predicted where precision = TP \/ (TP + FP)\n* **Recall** is the true positive rate where recall = TP \/ (TP + FN)","a4b8e52d":"* The graph shows that  Support Vector Machine algorithm produced the best AUC score. \n* Support Vector Machine algorithm also shows the highest F1  Score, so it is the best model\n* The Decision Tree algorithm shows the lowest AUC score\n* Customers having fiber optic internet service, using electronic payment and having month-to-month contract are tend to churn more\n* Custmers having two-year contract and having no internet service are tend to not churn\n ","6761f0db":"### NAIVE BAYES CLASSIFIER","e83c97f1":"From the above graph, we can see that people who have lower tenure and higher monthly charges are tend to churn more.","348c17f1":"### CONCLUSION","129dd354":"## APPLY MACHINE LEARNING ALGORITHMS","92f55ab2":"ROC Curve is a metric that assesses the model ability to distinguish between binary (0 or 1) classes. The ROC curve is created by plotting the true positive rate(TPR) against the false positive rate (FPR) at various threshold setting. Points above the diagonal line represent good classification (better than random).","e5484f81":"Customers having fiber optic internet service, using electronic payment and having month-to-month contract are tend to churn more. However customers having two-year contract and having no internet service are tend to not churn.","7389a004":"## PREPARE THE DATA BEFORE TRAINING","7238923a":"Churn - dependent feature ('Yes' denotes customers left, 'No' denotes customer stay here)","2504fed4":"### PLOT ROC CURVES FOR THE 5 MODELS AND FIND AUC SCORES\u00b6","3223729f":"As we can see the highest accuracy is at n_estimators = 33","da4001bc":"# Machine Learning in Customer Churn Prediction\nThis work is to apply various machine learning method on Telco customers churn dataset (WA_Fn-UseC_-Telco-Customer-Churn.csv)\n\n* Random Forest Classifier\n* Support Vector Machine Classifier\n* K-Nearest Neighbour(KNN)\n* Naive Bayes\n* Decision Tree Classification","27fabce6":"### KNN","4af3eca0":"### Decision Tree Classification","f88ddbeb":"##### Let's do the \n* Random Forest Classifier\n* Support Vector Machine Classifier\n* K-Nearest Neighbour(KNN)\n* Naive Bayes\n* Decision Tree Classification\nand find the accuracy of each","6afb6c26":"### SUPPORT VECTOR MACHINE CLASSIFICATION"}}