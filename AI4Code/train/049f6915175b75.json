{"cell_type":{"cc59b477":"code","08f3a623":"code","ed2a4c08":"code","4648991b":"code","7607093c":"code","c9102adf":"code","5b74555d":"code","2816be1d":"code","ddc6c260":"code","242d9623":"code","dd5adfe0":"code","92147bc4":"code","6b369401":"code","e2080a35":"code","2e3549c6":"code","fa2eedd5":"code","95d686ad":"code","693a70c6":"markdown","a0b2057d":"markdown","3fab892e":"markdown","2d569b67":"markdown","eca4ce16":"markdown","b440f823":"markdown","c8254704":"markdown","63b27cf3":"markdown","0fe26a90":"markdown","ded22d2b":"markdown","d57b84e7":"markdown","565513d4":"markdown","ac923397":"markdown","2500b06f":"markdown","ee95ca86":"markdown","f86ba994":"markdown","093d8e23":"markdown","e926feb2":"markdown","d23c845d":"markdown"},"source":{"cc59b477":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import ListedColormap\nimport itertools","08f3a623":"X1=[]\nX2=[]\nY1=[]\n\nfor i,j in itertools.product(range(50),range(50)):\n    if abs(i-j)>5 and abs(i-j)<40 and np.random.randint(5,size=1) >0:\n        X1=X1+[i\/2]\n        X2=X2+[j\/2]\n        if (i>j):\n            Y1=Y1+[1]\n        else:\n            Y1=Y1+[0]\n            \nX=np.array([X1,X2]).T\nY=np.array([Y1]).T","ed2a4c08":"cmap = ListedColormap(['blue', 'red'])                    \nplt.scatter(X1,X2, c=Y1,marker='.', cmap=cmap)\nplt.show()","4648991b":"SMean=np.min(X,axis=0)    #using Min-Max Normalization\nSDev=np.max(X,axis=0)\ndef NormalizeInput(X,SMean,SDev):   \n    XNorm=(X-SMean)\/SDev\n    return XNorm","7607093c":"XNorm=NormalizeInput(X,SMean,SDev)","c9102adf":"def mapFeature(X,degree):\n    \n    sz=X.shape[1]\n    if (sz==2):\n        sz=(degree+1)*(degree+2)\/2\n        sz=int(sz)\n    else:\n         sz=degree+1\n    out=np.ones((X.shape[0],sz))     #Adding Bias W0\n\n    sz=X.shape[1]\n    if (sz==2):\n        X1=X[:, 0:1]\n        X2=X[:, 1:2]\n        col=1\n        for i in range(1,degree+1):        \n            for j in range(0,i+1):\n                out[:,col:col+1]= np.multiply(np.power(X1,i-j),np.power(X2,j))    \n                col+=1\n        return out\n    else:\n        for i in range(1,degree+1):        \n            out[:,i:i+1]= np.power(X,i)\n    \n    return out","5b74555d":"degree=2\ninputX=mapFeature(XNorm,degree) ","2816be1d":"def sigmoid(z):\n    return 1\/(1 + np.exp(-z))","ddc6c260":"def computeCost(weights,X,Y):\n    n = X.shape[0]\n    fx=np.matmul( X,weights)                      #Hypothesis\n    hx=sigmoid(fx)\n    term1=np.sum(np.multiply(Y,np.log(hx)))\n    term2=np.sum(np.multiply(np.subtract(1,Y),np.log(1-hx)))    \n    J=(-1\/n)*(term1+term2)\n    return J","242d9623":"batchSize=len(Y)         #no of Examples\niterations = 10000\nalpha = 0.9\nbeta1=0.99\nbeta2=0.999\nlearningDecayRate=0.999998\nepsilon=0.0000000001\nfeatureCount=inputX.shape[1] \nweights=np.zeros((featureCount, 1)) #initialize Weight Paramters\nvDW=np.zeros((featureCount, 1))\nsDW=np.zeros((featureCount, 1))\nlossList=np.zeros((iterations,1),dtype=float)  #for plotting loss curve","dd5adfe0":"\nfor k in range(iterations):\n    #nth iteration\n    t=k+1\n    \n    #Hypothesis\n    fx=np.matmul( inputX,weights)           \n    \n    hx=sigmoid(fx)\n    \n    #Loss\n    loss=hx-Y  \n    \n    \n    #derivative\n    dW=np.matmul(inputX.T,loss)  #Derivative\n   \n    #learning Rate decrease as training progresses \n    alpha=alpha*learningDecayRate\n    \n    #Moment Update\n    vDW = (beta1) *vDW+ (1-beta1) *dW        #Momentum  \n    sDW = (beta2) *sDW+ (1-beta2) *(dW**2)   #RMSProp\n    \n    #Bias Correction\n    vDWc =vDW\/(1-beta1**t)       \n    sDWc =sDW\/(1-beta2**t)\n    \n    #gradient Update\n    #weights=weights - (alpha\/batchSize)*dW                           #Simple\n    weights=weights - (alpha\/batchSize)*vDW                          #Momentum   \n    #weights=weights - (alpha\/batchSize)*dW\/np.sqrt(csDW+epsilon)     #RMSProp \n    #weights=weights - (alpha\/batchSize)*(vDWc\/(np.sqrt(sDWc)+epsilon)) #Adam          \n    \n    \n    #Compute Loss for Plotting\n    lossList[k]=computeCost(weights,inputX,Y)\n\nprint(\"{0:.15f}\".format(lossList[iterations-1][0]))\n","92147bc4":"plt.plot(lossList,color='r')\nplt.show","6b369401":"def predict(X,weights,SMean,SDev,degree):\n    XNorm=NormalizeInput(X,SMean,SDev)\n    inputX=mapFeature(XNorm,degree)\n    fx=np.matmul(inputX, weights)\n    hx=sigmoid(fx)\n    PY=np.round(hx) \n    return PY\n","e2080a35":"def accurracy(Y1,Y2):\n    m=np.mean(np.where(Y1==Y2,1,0))    \n    return m*100","2e3549c6":"pY=predict(X, weights,SMean,SDev,degree) \nprint(accurracy(Y, pY))","fa2eedd5":"plt.figure(figsize=(12,8))\nplt.scatter(X[:,0],X[:,1], c=Y[:,0], cmap=cmap) \n###########################################################################\n#Predict for each X1 and X2 in Grid \nx_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\ny_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\nu = np.linspace(x_min, x_max, 50) \nv = np.linspace(y_min, y_max, 50) \n\nU,V=np.meshgrid(u,v)\nUV=np.column_stack((U.flatten(),V.flatten())) \nW=predict(UV, weights,SMean,SDev,degree) \nplt.scatter(U.flatten(), V.flatten(),  c=W.flatten(), cmap=cmap,marker='.', alpha=0.1)\n\n###########################################################################\n#Exact Decision Boundry can be plot with contour\nz = np.zeros(( len(u), len(v) )) \nfor i in range(len(u)): \n    for j in range(len(v)): \n        uv= np.column_stack((np.array([[u[i]]]),np.array([[v[j]]])))               \n        z[i,j] =predict(uv, weights,SMean,SDev,degree) \nz = np.transpose(z) \nplt.contour(u, v, z)\n###########################################################################\nplt.show()","95d686ad":"XNorm=NormalizeInput(X,SMean,SDev)\ninputX=mapFeature(XNorm,degree)\nfx=np.matmul(inputX, weights)\nhx=sigmoid(fx)\nplt.figure(figsize=(12,8))\nplt.scatter(fx,hx,c=np.round(hx), cmap=cmap)\n\nx = np.arange(-18, 18, 0.1)\ng = sigmoid(x)\nplt.plot(x, g,color='g' ,linewidth=2,alpha=1)\nplt.plot(x, x*0,color='k',linewidth=1,alpha=0.2)\nplt.plot([-2,0,2], [0.5,0.5,0.5],color='r',alpha=0.8)\nplt.plot([0,0], [-0.1,1],color='k',linewidth=1,alpha=0.2)\nplt.xlabel('x')\nplt.ylabel('$\\sigma(z)$')\nplt.show()","693a70c6":"<H1>Data Generate","a0b2057d":"<h5>Accurracy on Training Data","3fab892e":"<h3>Cost\/Loss Function(logistics)<\/h3>\n<p>\n<p>$f(x) =W^T X$  where $W^T X=w_0 + w_1 x+ w_2 y + w_3 x^2 + w_4 x y + w_5 y^2$\n<p>$g(z) =\\displaystyle\\frac{1}{1+e^{-z}}$\n<p>$\\implies h(X)=g(f(X))=g(W^T X)=\\displaystyle\\frac{1}{1+e^{-W^T X}}$\n","2d569b67":"<h3> Visualize Sigmoid for given Data points","eca4ce16":"<h5>Normalize Input   ","b440f823":"<h5> Initialization","c8254704":"<h5>Add Polynomial Features","63b27cf3":"<h1>Plotting Hypothesis","0fe26a90":"<h1>Training","ded22d2b":"<h1> Prediction\/Accuracy Evaluation","d57b84e7":" <p>We calculate loss, \n      \n   <p>$Loss= \\begin{cases} \n              -\\log(h(X)) & Y=1 \\\\\n              -\\log(1- h(X)) & Y=0\n               \\end{cases}$\n <p> Therefore we can simplify above discrete funciton into following loss function     \n <p>$L(W)=\\frac{-1}{n} \\displaystyle \\sum_{i=1}^n[Ylog(h(X)) +(1-Y)log(1-h(X))]$","565513d4":"<h5> Visualize Data","ac923397":"<h1>Logistics Regression (Classification) <\/h1>\n<p>\nTrying to sparate the classes by line or Curve using Gradient Descent Algorithm","2500b06f":"<h1>Plot Loss","ee95ca86":"<h3> Gradient Descent Algorithm (Logistics)<\/h3>\n<p>\nWe start with assumpution equation (Called hypothesis) which can separte above data in two classes. \n    <img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/8\/88\/Logistic-curve.svg\" width=200 align=\"right\" \/>\n<p>\n \n $h(x) =g(w_0 + w_1 x_1+ w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2 + w_5 x_2^2)$\n\n OR\n  \n <p>\n $h(x) =g(f(x))$  where $f(x) =w_0 + w_1 x_1+ w_2 x_2 + w_3 x_1^2 + w_4 x_1 x_2 + w_5 x_2^2$\n\n where $g$ is called \"sigmoid\" or \"logistics\" function\n $g(z) =\\displaystyle\\frac{1}{1+e^{-z}}$\n\n<\/p>\nThe coefficients with initial guess (i.e. $w_0$, $w_1$...) of $h(x)$ will be fed into the algorithm.\nThen Program will start from initial guess and then iterate steps to find the best fit.\n<p> We predict $\\hat{Y}= 1$ if\n    $h(x)>=0.5$ i.e.  $f(x)>=0$ \n<p> We predict $\\hat{Y}= 0$  if\n    $h(x)<0.5$ i.e. $f(x)<0$\n<p>\n Our objective is to minimize Error in predicted values.\n    <p>\n $ Error=   \\hat{Y}-Y$  Where  $\\hat{Y}=h(X)$\n <\/p>\nSince Loss involve propablities between 0 and 1. we define loss function differently. we define Loss\/Cost function as follows","f86ba994":"<h3>Derivative of Cost\/Loss Function(logistics)<\/h3>\n  <p>Now, \n      \n   <p>$\\log(h(X))=\\displaystyle\\log(\\frac{1}{1+e^{-W^T X} })$  \n    <p> $\\hspace{20mm}  =  -\\log ( 1+e^{-W^T X} ) $\n    \n <p> $\\log(1- h(X))=\\displaystyle\\log(1-\\frac{1}{1+e^{-W^T X}})$\n  <p>  $\\hspace{25mm}=\\displaystyle\\log(\\frac{e^{-W^T X}}{1+e^{-W^T X}})$\n  <p>  $\\hspace{25mm}=\\log (e^{-W^T X} )-\\log ( 1+e^{-W^T X} )$\n  <p>  $\\hspace{25mm}=e^{-W^T X}-\\log ( 1+e^{-W^T X} )$\n    <p>$L(W)=\\frac{-1}{n} \\displaystyle \\sum_{i=1}^n[Ylog(h(X)) +(1-Y)log(1-h(X))]$\n    <p> $\\hspace{15mm}=\\frac{-1}{n}\\displaystyle\\sum_{i=1}^n \\left[Y(\\log ( 1+e^{-W^T X})) + (1-Y)(-W^T X-\\log ( 1+e^{-W^T X} ))\\right]$    \n <p>$\\hspace{15mm}=\\frac{-1}{n}\\displaystyle\\sum_{i=1}^n \\left[YW^T X-W^T X-\\log(1+e^{-W^T X})\\right]$    \n <p>$\\hspace{15mm}=\\frac{-1}{n}\\displaystyle\\sum_{i=1}^n \\left[YW^T X-\\log e^{W^T X}- \\log(1+e^{-W^T X})\\right]$  \n      $\\hspace{15mm}\\text{using}\\hspace{15mm} \\log(e^{W^T X})  = W^T X $\n <p>$\\hspace{15mm}=\\frac{-1}{n}\\displaystyle\\sum_{i=1}^n \\left[YW^T X-\\log(1+e^{W^T X})\\right]$ \n    $\\hspace{15mm}\\text{using}\\hspace{15mm} \\log(X) + \\log(Y) = log(X Y) $  \n\n\n<p>$\\frac{\\partial}{\\partial W} L(W)$\n    $=\\frac{\\partial}{\\partial W}(-YW^T X +\\displaystyle\\log(1+e^{W^T X}))$\n   <p> $\\hspace{20mm}=\\frac{\\partial}{\\partial W}(-YW^T X) +\\frac{\\partial}{\\partial W}(\\log(1+e^{W^T X})))$\n   <p> $\\hspace{20mm}=-YX+\\displaystyle\\frac{e^{W^T X}}{1+e^{W^T X}} X$\n    <p> $\\hspace{20mm}=(-Y+\\displaystyle\\frac{e^{W^T X}}{1+e^{W^T X }}) X$\n    <p> $\\hspace{20mm}=(-Y+\\displaystyle\\frac{1}{1+e^{-W^T X }}) X$\n    <p> $\\hspace{20mm}=(-Y+h(X)) X$\n   <p>$ Finally$\n    <p> $\\implies\\frac{\\partial}{\\partial W} L(W)=(h(X)-Y) X$\n    ","093d8e23":"<h5> Gradient Descent Updates","e926feb2":"<h1> Derivation of Logistics Loss Function and Gradient Updates","d23c845d":"<h3>Cost\/Loss Function<\/h3>\nLoss funnction is defined as\n<p>$L(W) = \\dfrac {-1}{n} \\displaystyle \\sum _{i=1}^n \\left [ Y_{i} log(\\hat{Y}_{i})+ (1-Y_{i}) log(1-\\hat{Y}_{i}) \\right]$\n<p> and gradient update is same as it was in case of linear. \n    <p>$W :=  W - \\alpha \\frac{1}{n} \\sum\\limits_{i=1}^{n}(h(X) - Y)X$ \n    \n"}}