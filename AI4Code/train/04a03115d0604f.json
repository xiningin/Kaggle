{"cell_type":{"1902920a":"code","0924a082":"code","cfebc0a9":"code","f7d85c6b":"code","b1d3e749":"code","fac8add5":"code","600873c0":"code","9e1c1d08":"code","e4d69fcb":"code","52d7ea0b":"code","cdae1d25":"code","8c13d72f":"code","b4092af7":"code","883f2d09":"code","1d62a13c":"code","bc3fb971":"code","f96f665f":"code","c847ee66":"code","b1dfcb5c":"code","3da8c15d":"code","769c5f5d":"code","9b3db719":"code","ce7792e8":"code","b50ad3a2":"code","145f12c0":"code","ec3693e2":"code","e34818f3":"code","98a7e218":"code","26350ee1":"code","71d08d4c":"code","c3e31aec":"code","87f20948":"code","58235f24":"markdown","c7ded660":"markdown","68ba4812":"markdown"},"source":{"1902920a":"# Load the required libraries\nimport re\nimport os\n\nimport requests\nfrom bs4 import BeautifulSoup\n#import datefinder\n\nimport pickle\nimport time\nimport pandas as pd ","0924a082":"import io, requests\nfrom io import StringIO\n\nurl = \"https:\/\/github.com\/owid\/covid-19-data\/blob\/master\/public\/data\/testing\/covid-testing-all-observations.csv\"\ncontent = requests.get(url).content.decode('utf-8')\n\nwith open('covid-testing-all-observations.csv', 'w+') as f:\n    f.write(content)","cfebc0a9":"# tests_df = pd.read_csv('covid-testing-all-observations.csv', index_col=0)\n\n# print(tests_df.shape)\n# tests_df.head()","f7d85c6b":"#! git clone https:\/\/github.com\/owid\/covid-19-data # <- Uncomment if you want to clone the github repo\n#!ls covid-19-data\/public","b1d3e749":"# # Define URL for webpage <<- This URL doesn't work so have commented up the old code # Date: 9th April 2020\n# url = 'https:\/\/ourworldindata.org\/coronavirus-testing-source-data#population-estimates-to-calculate-tests-per-million-people'\n\n# try:\n#     resp = requests.get(url)\n#     soup = BeautifulSoup(resp.text, 'html.parser')\n# except:\n#     print(\"Error: In acquiring page data\")\n    \n# # Find table\n# allTable = soup.find_all('table')\n\n# First row has the table header.\n#headings = [cell.get_text().strip() for cell in allTable[0].find(\"tr\").find_all('td')]\n\n# # Get the remaining rows \n# datasets = list() \n# for row in allTable[0].find_all(\"tr\")[1:]:\n#     dataset = dict(zip(headings, (td.get_text().strip() for td in row.find_all(\"td\"))))\n# #     datasets.append(dataset)\n\n# # Convert to a dataframe\n# dataset_df = pd.DataFrame(datasets)\n\n# # Remove commas from the number of tests 30,098 => 30098\n# dataset_df['Total tests'] = dataset_df['Total tests'].apply(lambda x: x.replace(\",\", \"\"))\n\n# dataset_df.dtypes\n# dataset_df.to_excel(\"Tests_Conducted.xlsx\", index=False)","fac8add5":"url = 'https:\/\/en.wikipedia.org\/wiki\/COVID-19_testing' # define url to be scraped","600873c0":"# Parse the html page\ntry:\n    resp = requests.get(url)\n    soup = BeautifulSoup(resp.text, 'html.parser')\nexcept:\n    print(\"Error: In acquiring page data\")","9e1c1d08":"# Find table\nallTable = soup.find_all('table') # allTable[2] has the data of interest","e4d69fcb":"# find the idx to the required table\nfor idx in range(len(allTable)):\n    print(allTable[idx]['class'])\n    headings = [cell.get_text().strip() for cell in allTable[idx].find(\"tr\").find_all('th')]\n    print(headings)\n    try:\n        if headings[0].__contains__('Country'):\n            print(idx)\n            break\n    except:\n        continue\n    print(\"*************************************************\")\n#     if allTable[idx]['class'][0].find('covid19-testing') == 0:\n#         print(allTable[idx]['class'])\n#         break","52d7ea0b":"# Get the headings & make some changes to their names\n\nheadings = [cell.get_text().strip() for cell in allTable[idx].find(\"tr\").find_all('th')]\nheadings","cdae1d25":"allRows = allTable[idx].find_all(\"tbody\")[0].find_all('tr')\nlen(allRows)","8c13d72f":"# Main loop for capturing the information\n\ndatasets = list() \nfor row in allRows[1:]:\n    cellValue = row.find_all(\"td\")\n    \n    dataset = dict(zip(headings[1:], (td.get_text().strip() for td in cellValue)))\n    \n    try:\n        dataset['Country'] = row.find('th').get_text().strip() # Get country or region info\n    except:\n        print(len(datasets))\n        break\n    \n    getA = cellValue[-1].find_all('a', href=True) # Get citation values\n    dataset['Ref.'] = \" \"\n    \n    for idx in range(len(getA)):\n         dataset['Ref.'] = dataset['Ref.'] + getA[idx]['href'].replace(\"#\", \"\") + \" \"\n    \n    datasets.append(dataset)","b4092af7":"dataset_df = pd.DataFrame(datasets)\ndataset_df.columns","883f2d09":"dataset_df.rename(columns={'Date[a]':'Date', 'Units[b]': 'Units', 'Confirmed(cases)': 'Positive', \n                          'Confirmed\u2009\/millionpeople':'Positive\u2009\/millionpeople'}, inplace=True)\n\ndataset_df = dataset_df[['Country', \"Date\",'Tested', 'Units', 'Positive', \"%\", 'Tested\u2009\/millionpeople', 'Positive\u2009\/millionpeople', 'Ref.' ] ]\nprint(dataset_df.columns)\nprint(dataset_df.shape)\ndataset_df.head(10)","1d62a13c":"# Get reference section of the page\nrefSoup = soup.find_all('ol')\nlen(refSoup)","bc3fb971":"\nallRefId = refSoup[-1].find_all('li')\nlen(allRefId)","f96f665f":"# parse the reference list \nrefList = list()\nfor refId in allRefId:\n    refDict = dict()\n    refDict['Ref.'] = refId.get('id')   #getA[0].get('href').strip().replace(\"ref\", \"note\")\n    \n    getA = refId.find_all('a')\n    for i in range(len(getA)):\n        href = getA[i].get('href').strip()\n        if \"http\" in href: \n            refDict['Source'] = href\n    \n    refList.append(refDict)","c847ee66":"ref_df = pd.DataFrame(refList)\nprint(ref_df.shape)\nref_df.head()","b1dfcb5c":"# Merge the two dataframes, to get source URL\n# This is kind of round-about way of doing it, Iff someone can suggest a better method. Pls do it\n\nsource1 = list()\nsource2 = list()\n\nfor idx, row in dataset_df.iterrows():\n    refs = row['Ref.'].split()\n    try:\n        source1.append(ref_df[ref_df['Ref.']==refs[0]]['Source'].values[0])\n    \n        if len(refs) > 1:\n            source2.append(ref_df[ref_df['Ref.']==refs[1]]['Source'].values[0])\n        else:\n            source2.append(\" \")\n    except:\n        source1.append(\" \")\n        source2.append(\" \")\n        \nassert len(source1) == len(source2)","3da8c15d":"dataset_df.loc[:,('Source_1')] = source1\ndataset_df.loc[:,('Source_2')] = source2\ndataset_df.head()","769c5f5d":"dataset_df.dtypes","9b3db719":"# Replace the commas >>> 2,183 --> 2183\ndataset_df['Tested'] = dataset_df['Tested'].apply(lambda x: x.replace(',','').strip()) \ndataset_df.loc[:, ('Positive')] = dataset_df['Positive'].apply(lambda x: x.replace(',','').strip())\ndataset_df.loc[:, ('Tested\u2009\/millionpeople')] = dataset_df['Tested\u2009\/millionpeople'].apply(lambda x: x.replace(',','').strip())\ndataset_df.loc[:, ('Positive\u2009\/millionpeople')] = dataset_df['Positive\u2009\/millionpeople'].apply(lambda x: x.replace(',','').strip())","ce7792e8":"#dataset_df.loc[dataset_df['Tests'] == '83800*', 'Tests'] = 83800 # For US-California\n#dataset_df['Tests'] = dataset_df['Tests'].astype('int')\ndataset_df.loc[:, ('Tested')] = pd.to_numeric(dataset_df['Tested'], errors='coerce')","b50ad3a2":"dataset_df.loc[:, ('Positive')] = pd.to_numeric(dataset_df['Positive'], errors='coerce')\ndataset_df.loc[:, ('%')] = pd.to_numeric(dataset_df['%'], errors='coerce')\n\ndataset_df.loc[:, ('Tested\u2009\/millionpeople')] = pd.to_numeric(dataset_df['Tested\u2009\/millionpeople'], errors='coerce')\ndataset_df.loc[:, ('Positive\u2009\/millionpeople')] = pd.to_numeric(dataset_df['Positive\u2009\/millionpeople'], errors='coerce')","145f12c0":"dataset_df.dtypes","ec3693e2":"dataset_df.head(15)","e34818f3":"dataset_df.to_csv('Tests_conducted_13July2020.csv', index=False)","98a7e218":"allData = pd.read_csv('\/kaggle\/input\/covid19-tests-conducted-by-country\/TestsConducted_AllDates_09June2020.csv')\nprint(allData.shape)\nallData.head()","26350ee1":"dataset_df.columns","71d08d4c":"dataset_df = dataset_df[['Country', 'Date', 'Tested', 'Units', 'Positive', '%', 'Source_1','Source_2' ]]\ndataset_df.rename(columns={'%': 'Positive\/Tested %'}, inplace=True)\ndataset_df['FileDate'] = '13-July-2020'","c3e31aec":"allData = pd.concat([dataset_df, allData], axis=0, sort=False)\nprint(allData.shape)\nprint(allData.columns)\nallData.head()","87f20948":"allData.to_csv('TestsConducted_AllDates_13July2020.csv', index=False)","58235f24":"**Now we will try to compress the data & convert it to numerics, so that we can directly use it in our output**","c7ded660":"## **Updated Notebook**\n\n\n## Change log\n\n*Date: 11th May 2020*\n\nNotebook updated to include the concatenated dataset. This includes all files from previous runs. Check [kernel](https:\/\/www.kaggle.com\/skylord\/kernel-for-concatenating-all-files) here.\n\nTwo files are created: 1# Original raw data 2# Concatenated file\n\n*Date: 8th May 2020*\n\nUpdated\n\n*Date: 5th May 2020*\n\nUpdated\n\n*Date: 1st May 2020*\n\nUpdated till date\nMinor changes in column names\n\n```\nTests -> Tested\nTests \/millionpeople -> Tested \/millionpeople\n\n```\n\nNew Column `%` added\n\n*Date:  26th April 2020*\n\nThis was long delayed!  \n\n*Date:  15th April 2020*\n\nChanges in table structure: Added columns: Date (change in column name), Positive \/millionpeople (instead of Positive \/ thousands) and other changes\nCommented out extraction from OurWorldInData. Its now available from their github repo\n\n*Date:  9th April 2020*\n\nChanges in table structure\n\n*Created Date: 31st March 2020*\n\nThe notebook has been updated to include web scraping of the new source of information.The updated notebook is appended at the end of the file. The new file takes information from the following source: \n\n[Wiki link for Covid-19 Testing](https:\/\/en.wikipedia.org\/wiki\/COVID-19_testing)\n\nNew columns include: positive\/confirmed cases, tests conducted per million of population, positive cases per million & source information!\n","68ba4812":"******************************************************************\n\n# Updated notebook starts from here \n# Date: 31st March 2020\n******************************************************************\n"}}