{"cell_type":{"e72edf87":"code","6214291e":"code","4cf270fb":"code","67c0e064":"code","9219a157":"code","e5060cae":"code","184371cb":"code","c795a646":"code","ae2b9cf1":"code","a36f22b0":"code","cafaccd9":"code","acd43712":"code","ef892688":"code","d6758fda":"code","149adcc1":"code","9cac7cd6":"code","130342ab":"code","e1bbf1fe":"code","f56c35da":"code","c0846166":"code","afcfdaec":"code","c9b2ca3e":"code","5535a5a8":"code","2a219387":"code","aac7b057":"code","35d86f6a":"code","febed4c4":"code","829ab2e9":"code","a48a107c":"code","ee121fb2":"code","7b4a316f":"code","d3f01aa5":"code","1cd6b65a":"code","27a59679":"code","83716279":"code","402fd407":"code","47c939a3":"code","44aa03f0":"code","1b970e4c":"code","1bfc62d3":"code","859090fd":"code","df62dfc9":"code","cea59035":"code","a0a1724b":"code","7d5a33c5":"markdown","cd830909":"markdown","226fe3dd":"markdown","de0c6df0":"markdown","a45f9ca6":"markdown","349376eb":"markdown","04ab3292":"markdown","39d57ece":"markdown","7b43613f":"markdown","24f9c36a":"markdown","3ae5a397":"markdown","699ba7d5":"markdown","ca298841":"markdown"},"source":{"e72edf87":"#pip install scikit-learn  -U","6214291e":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport matplotlib.dates as mdates\nimport seaborn as sns\n\n#import lightgbm as lgb \n#import xgboost as xgb\n#from catboost import CatBoostRegressor\n\nfrom sklearn.linear_model import LinearRegression,HuberRegressor,Ridge,TweedieRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler\n\nimport dateutil.easter as easter\n\nimport optuna\nimport math","4cf270fb":"#Holidays\nHOLIDAYS = False     \nNEXT_HOLIDAY = False  \n\nPOST_PROCESSING = False\nMODEL_TYPE = \"Ridge Regression\"\n\nVAL_SPLIT = \"2017-12-31\" #\"2018-05-31\"","67c0e064":"EPOCHS = 10000    \nEARLY_STOPPING = 30\nDEVICE = \"cpu\"\n\nSCALER_NAME = \"MinMax\"  #None MinMax Standard\nSCALER = MinMaxScaler()  #MinMaxScaler StandardScaler","9219a157":"train_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/train.csv\",index_col = 0)\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ngdp_df = pd.read_csv('..\/input\/gdp-20152019-finland-norway-and-sweden\/GDP_data_2015_to_2019_Finland_Norway_Sweden.csv')\ngdp_df.set_index('year', inplace=True)\n\nif HOLIDAYS:\n    holidays = pd.read_csv(\"..\/input\/holidays-finland-norway-sweden-20152019\/Holidays_Finland_Norway_Sweden_2015-2019.csv\",usecols = [\"Date\",\"Country\",\"Name\"]                      )\n    holidays.rename(columns = {\"Date\":\"date\",\"Country\":\"country\",\"Name\":\"holiday\"},inplace= True)\n    holidays[\"holiday\"]= 1\n    holidays[\"holiday\"]= holidays[\"holiday\"].astype(\"int32\")\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])","e5060cae":"#Make date\ntrain_df[\"date\"] = pd.to_datetime(train_df[\"date\"])\ntest_df[\"date\"] = pd.to_datetime(test_df[\"date\"])","184371cb":"train_df.head()","c795a646":"# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    \n    def get_gdp(row):\n        country = 'GDP_' + row.country\n        return gdp_df.loc[row.date.year, country]\n        \n    new_df = pd.DataFrame({'gdp': np.log(df.apply(get_gdp, axis=1)),\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                          })\n\n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Hat']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 3):\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'hat_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Hat']\n        new_df[f'hat_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Hat']\n\n    return new_df","ae2b9cf1":"# Feature engineering for holidays\ndef engineer_more(df):\n    \"\"\"Return a new dataframe with more engineered features\"\"\"\n    new_df = engineer(df)\n\n    # End of year\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d)\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"n-dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(24, 32)}),\n                        pd.DataFrame({f\"f-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in range(1, 14)}),\n                        pd.DataFrame({f\"jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in range(1, 10)}),\n                        pd.DataFrame({f\"s-jan{d}\":\n                                      (df.date.dt.month == 1) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in range(1, 15)})],\n                       axis=1)\n    \n    # May\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) \n                                      for d in list(range(1, 10))}), #  + list(range(17, 25))\n                        pd.DataFrame({f\"may{d}\":\n                                      (df.date.dt.month == 5) & (df.date.dt.day == d) & (df.country == 'Norway')\n                                      for d in list(range(19, 26))})],\n                       axis=1)\n    \n    # June and July\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"june{d}\":\n                                      (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Sweden')\n                                      for d in list(range(8, 14))}),\n                        #pd.DataFrame({f\"june{d}\":\n                        #              (df.date.dt.month == 6) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(22, 31))}),\n                        #pd.DataFrame({f\"july{d}\":\n                        #              (df.date.dt.month == 7) & (df.date.dt.day == d) & (df.country == 'Norway')\n                        #              for d in list(range(1, 3))})],\n                       ],\n                       axis=1)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"wed_june{d}\": \n                                      (df.date - wed_june_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(-4, 6))})],\n                       axis=1)\n    \n    # First Sunday of November\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"sun_nov{d}\": \n                                      (df.date - sun_nov_date == np.timedelta64(d, \"D\")) & (df.country != 'Norway')\n                                      for d in list(range(0, 9))})],\n                       axis=1)\n    \n    # First half of December (Independence Day of Finland, 6th of December)\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"dec{d}\":\n                                      (df.date.dt.month == 12) & (df.date.dt.day == d) & (df.country == 'Finland')\n                                      for d in list(range(6, 14))})],\n                       axis=1)\n\n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    new_df = pd.concat([new_df,\n                        pd.DataFrame({f\"easter{d}\": \n                                      (df.date - easter_date == np.timedelta64(d, \"D\"))\n                                      for d in list(range(-2, 11)) + list(range(40, 48)) + list(range(50, 59))})],\n                       axis=1)\n    \n    return new_df.astype(np.float32)\n\ntrain = engineer_more(train_df)\n\ntrain['num_sold'] = train_df.num_sold.astype(np.float32)\ntest = engineer_more(test_df)\n\nfeatures = list(test.columns)\nprint(features)\ntest['date'] = test_df.date\ntrain['date'] = train_df.date","a36f22b0":"train[[\"store\",\"product\",\"country\"]]= train_df[[\"store\",\"product\",\"country\"]]\ntest[[\"store\",\"product\",\"country\"]]= test_df[[\"store\",\"product\",\"country\"]]","cafaccd9":"def public_hols(df):\n    df = pd.merge(df, holidays, how='left', on=['date', 'country'])\n    df.fillna(value = 0,inplace=True)\n    return df","acd43712":"if HOLIDAYS:\n    train = public_hols(train)\n    test = public_hols(test)","ef892688":"def next_holiday(x):\n    i=1\n    while sum(holidays[\"date\"] == pd.Timestamp(x) + pd.DateOffset(days=i)) ==0:\n        i+=1\n        if i >200:\n            i=0\n            break\n            break\n    return i\n\nif NEXT_HOLIDAY:\n    holidays[\"date\"] = pd.to_datetime(holidays[\"date\"])\n    train[\"to_holiday\"] = train[\"date\"].apply(lambda x : next_holiday(x))\n    test[\"to_holiday\"] = test[\"date\"].apply(lambda x : next_holiday(x))","d6758fda":"def SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)","149adcc1":"prior_2017 = train[train[\"date\"]<=VAL_SPLIT].index\nafter_2017 = train[train[\"date\"]>VAL_SPLIT].index","9cac7cd6":"X = train[features]\ny= train[\"num_sold\"]\n\nX_train = train[features].iloc[prior_2017,:]\nX_test = train[features].iloc[after_2017,:]\ny_train= train.iloc[prior_2017,:][\"num_sold\"]\ny_test= train.iloc[after_2017,:][\"num_sold\"]","130342ab":"def scale_data(X_train, test, X_test= None):\n     \n    scaler= SCALER\n    \n    #this can be X or X_train \n    X_train_s = scaler.fit_transform(X_train)\n    test_s = scaler.transform(test)\n    \n   \n    if X_test is not None:\n        X_test_s = scaler.transform(X_test)\n        return X_train_s, test_s , X_test_s\n    \n    else:\n        return X_train_s, test_s ","e1bbf1fe":"params = {\"power\":1.0,\n    \"alpha\":0.0,\n    \"fit_intercept\":False,\n    \"link\":'log', #\u2018auto\u2019, \u2018identity\u2019, \u2018log\u2019}\n    \"tol\":0.00000000001,\n    \"warm_start\":False\n         }","f56c35da":"def fit_model(X,y,test, X_test = None,y_test= None):\n    \n    #X_train_s, test_s , X_test_s = scale_data(X_train, test, X_test)\n    model = Ridge(max_iter=EPOCHS)\n\n    if X_test is not None: \n        X_train_s, test_s , X_test_s = scale_data(X, test, X_test)\n        model.fit(X_train_s,np.log1p(y))\n        preds = np.expm1(model.predict(X_test_s))\n        \n        smape = SMAPE(y_test,preds)\n        print(\"SMAPE:\",smape )\n        \n        return preds, model, smape\n        \n    else:\n        X_s, test_s = scale_data(X, test)\n        \n        model.fit(X_s,np.log1p(y))\n        preds = np.exp(model.predict(test_s))\n        \n        return preds, model","c0846166":"val_predictions , model ,smape = fit_model(X_train,y_train,test[features] , X_test,y_test)","afcfdaec":"print(\"SMAPE :\",smape )\nprint(f\"\\n EPOCHS: {EPOCHS}\")\nprint(f\"\\n SCALER: {SCALER_NAME}\")\nprint(f\"\\n POST_PROCESSING: {POST_PROCESSING}\")","c9b2ca3e":"# fit on full dataset\nonesplit_preds , model = fit_model(X,y,test[features])","5535a5a8":"onesplit_preds","2a219387":"import pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'","aac7b057":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","35d86f6a":"def split_models(split_on, sub_df):    \n\n    split_smape=0\n    \n    # split training on product\/ store\/ country\n    for split in train[split_on].unique():\n        print(f\"\\nPredicting for {split_on} {split}\")\n\n        train_split= train[train[split_on] ==split]\n        test_split =test[test[split_on] ==split]\n\n        train_split.drop([\"store\",\"product\",\"country\"],axis =1 ,inplace=True)\n        test_split.drop([\"store\",\"product\",\"country\", \"date\"],axis =1 ,inplace=True)\n\n        X_train = train_split[train_split[\"date\"]<=VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        X_test = train_split[train_split[\"date\"]>VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        y_train= train_split[train_split[\"date\"]<=VAL_SPLIT][\"num_sold\"]\n        y_test= train_split[train_split[\"date\"]>VAL_SPLIT][\"num_sold\"]\n\n\n        #run model for each split type\n        val_predictions , model ,smape = fit_model(X_train,y_train,test_split, X_test,y_test)\n        print(f\"\\n{split_on} smape:\",smape)\n        split_smape += smape\/train[split_on].nunique()\n        \n        #train on Full dataset\n        final_predictions , model = fit_model(train_split.drop([\"num_sold\", \"date\"],axis =1),train_split[\"num_sold\"],test_split)\n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions.round()\n\n    print(f\"\\n Final smape:\",split_smape)\n    \n    return split_smape, sub_df, model","febed4c4":"store_smape, sub_store, model = split_models(\"store\", sub.copy(deep=True))","829ab2e9":"sub_store","a48a107c":"store_smape, sub_product, model = split_models(\"product\", sub.copy(deep=True))","ee121fb2":"sub_product","7b4a316f":"store_smape, sub_country,model = split_models(\"country\", sub.copy(deep=True))","d3f01aa5":"def split_models_recursive(split_on, sub_df):    \n\n    split_smape=0\n    \n    # split training on product\/ store\/ country\n    for idx, split in enumerate(train[split_on].unique()):\n        print(f\"\\nPredicting for {split_on} {split}\")\n        \n        #apply previous preds to train and test\n        if idx>0:\n            test.loc[test[test[split_on] ==split].index ,f'{idx}_{split}'] = final_predictions.round()\n            test.loc[test_split.index,f'{idx}_{split}'] = final_predictions.round()\n            test.fillna(0,inplace=True)\n\n        train_split= train[train[split_on] ==split]\n        test_split =test[test[split_on] ==split]\n        \n        print(train_split.shape)\n        print(test_split.shape)\n\n        train_split.drop([\"store\",\"product\",\"country\"],axis =1 ,inplace=True)\n        test_split.drop([\"store\",\"product\",\"country\", \"date\"],axis =1 ,inplace=True)\n\n        X_train = train_split[train_split[\"date\"]<=VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        X_test = train_split[train_split[\"date\"]>VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        y_train= train_split[train_split[\"date\"]<=VAL_SPLIT][\"num_sold\"]\n        y_test= train_split[train_split[\"date\"]>VAL_SPLIT][\"num_sold\"]\n\n        #run model for each split type\n        val_predictions , model ,smape = fit_model(X_train,y_train,test_split, X_test,y_test)\n        print(f\"\\n{split_on} smape:\",smape)\n        split_smape += smape\/train[split_on].nunique()\n        \n        #train on Full dataset\n        final_predictions , model = fit_model(train_split.drop([\"num_sold\", \"date\"],axis =1),train_split[\"num_sold\"],test_split)\n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions.round()\n        \n        #add train \n        num_store = train[train[\"store\"] ==split][\"num_sold\"]\n        train.loc[num_store.index ,f'{idx}_{split}']  = num_store\n        train.fillna(0,inplace=True)\n\n\n    print(f\"\\n Final smape:\",split_smape)\n    \n    return split_smape, sub_df, model\n\n#store_smape, sub_country,model = split_models_recursive(\"store\", sub.copy(deep=True))","1cd6b65a":"import itertools\nall_splits = list(itertools.product(['KaggleMart', 'KaggleRama'],['Kaggle Mug', 'Kaggle Hat', 'Kaggle Sticker'],['Finland', 'Norway', 'Sweden']))","27a59679":"def split_models_ALL(split_on, sub_df):    \n\n    split_smape=0\n    split_dict = {}\n\n    # split training on product\/ store\/ country\n    for idx ,split in enumerate(split_on):\n        print(f\"\\nPredicting for store: {split[0]}, product: {split[1]}, country: {split[2]} \")\n\n        train_split= train[ (train[\"store\"] == split[0]) & (train[\"product\"] == split[1]) & (train[\"country\"] == split[2])]\n        test_split =test[ (test[\"store\"] == split[0]) & (test[\"product\"] == split[1]) & (test[\"country\"] == split[2])]\n\n        \n        train_split.drop([\"store\",\"product\",\"country\"],axis =1 ,inplace=True)\n        test_split.drop([\"store\",\"product\",\"country\", \"date\"],axis =1 ,inplace=True)\n        \n        X_train = train_split[train_split[\"date\"]<=VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        X_test = train_split[train_split[\"date\"]>VAL_SPLIT].drop([\"num_sold\", \"date\"], axis=1)\n        y_train= train_split[train_split[\"date\"]<=VAL_SPLIT][\"num_sold\"]\n        y_test= train_split[train_split[\"date\"]>VAL_SPLIT][\"num_sold\"]\n\n\n        #run model for each split type\n        val_predictions , model ,smape = fit_model(X_train,y_train,test_split, X_test,y_test)\n\n        split_smape += smape\/len(all_splits)\n        split_dict[split] = smape\n\n        #train on Full dataset\n        final_predictions , model = fit_model(train_split.drop([\"num_sold\", \"date\"],axis =1),train_split[\"num_sold\"],test_split)\n        sub_df.loc[test_split.index,\"num_sold\"] = final_predictions.round()\n        \n\n    print(f\"\\n final all_split smape:\",split_smape)\n    \n    return split_smape, sub_df , split_dict","83716279":"smape_all, sub_all, split_dict = split_models_ALL(all_splits, sub.copy(deep=True))","402fd407":"split_dict","47c939a3":"sub_all","44aa03f0":"sub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv\",index_col = 0)","1b970e4c":"sub[\"num_sold\"] = onesplit_preds.round()","1bfc62d3":"sub.to_csv(\"submission_noSplit\")\nsub_store.to_csv(\"submission_store.csv\")\nsub_product.to_csv(\"submission_product.csv\")\nsub_country.to_csv(\"submission_country.csv\")\nsub_all.to_csv(\"submission_all.csv\")","859090fd":"sns.lineplot(data = sub_country, x= test[\"date\"] , y = \"num_sold\", label =\"Country split prediction\" ,ci=None)","df62dfc9":"plt.figure(figsize=(20,10))\n\nsns.lineplot(data= train, x= \"date\", y= \"num_sold\" ,label=\"Train Actual\",ci=None)\nsns.lineplot(data = sub_store,x = test[\"date\"] , y = \"num_sold\", label =\"Store split prediction\" ,ci=None)\nsns.lineplot(data = sub_product, x= test[\"date\"] , y = \"num_sold\", label =\"Product split prediction\" ,ci=None)\nsns.lineplot(data = sub_country, x= test[\"date\"] , y = \"num_sold\", label =\"Country split prediction\" ,ci=None)\nsns.lineplot(data = sub_all, x= test[\"date\"] , y = \"num_sold\", label =\"All  split prediction\" ,ci=None)\n\nplt.show()","cea59035":"'''#for visual only\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2022\/test.csv\",index_col = 0)\ntest[\"date\"] = pd.to_datetime(test[\"date\"])\n\nfig,ax = plt.subplots(2,1, figsize=(25,20),sharey= True)\n\ndiff = y_test - val_predictions\nsns.lineplot(ax=ax[0], y= y_test, x= y_test_index, label=\"Train Actual\",ci=None)\nsns.lineplot(ax=ax[0], x = y_test_index , y = val_predictions, label =\"Validation Prediction\" ,ci=None)\nsns.lineplot(ax=ax[0],data =sub,x = test_df[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \n\nax[0].set_title(f\"Actual and Predicted Sales for {MODEL_TYPE}\")\n\nsns.lineplot(ax=ax[1], data = diff, label =\"Residuals\" )\nax[1].set_title(f\"Residuals for {MODEL_TYPE} for 2018\")\n\nplt.show()'''","a0a1724b":"plt.figure(figsize=(25,10))\n\nsns.lineplot(x =train_df[\"date\"], y= train_df[\"num_sold\"] ,label=\"Train Actual\",ci=None)\nsns.lineplot(data =sub,x = test_df[\"date\"], y = \"num_sold\",label=\"Final Prediction\" ,ci=None) \nplt.title(\"Actual and Predicted Sales\")\n\nplt.show()","7d5a33c5":"# Functions ","cd830909":"# Split and Scale","226fe3dd":"# Libraries","de0c6df0":"# Training Visualization","a45f9ca6":"obj is the objective function of the algorithm, i.e. what it's trying to maximize or minimize, e.g. \"regression\" means it's minimizing squared residuals.\n\nMetric and eval are essentially the same. They are used for Early stopping ","349376eb":"# All Split ","04ab3292":"# Each prediction added to next model ","39d57ece":"Thanks to [ambrosm](https:\/\/www.kaggle.com\/anirudhg15)\n\nhttps:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\/notebook#More-feature-engineering-(advanced-model)","7b43613f":"#  Multi Split","24f9c36a":"# No-Split Train on full Dataset","3ae5a397":"# Run model","699ba7d5":"# Post Processing & Submission ","ca298841":"# Load Data"}}