{"cell_type":{"1219c548":"code","76b939bf":"code","ccf16635":"code","c02ece63":"code","3c81a654":"code","de2e90eb":"code","57dd7188":"code","04da4e14":"markdown","b3f49a55":"markdown","cd349d88":"markdown","54b452fe":"markdown","bfc2a08d":"markdown","f4d6fab7":"markdown","996c5608":"markdown","cf351f92":"markdown","6ff25f1b":"markdown","4025d48d":"markdown"},"source":{"1219c548":"from IPython.display import display, Markdown\nimport numpy as np\nimport pandas as pd\nfrom sklearn.svm import SVC \nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nenc = OneHotEncoder(handle_unknown='ignore')\nsts = StandardScaler()\ndf = pd.read_csv('..\/input\/titanic\/train.csv')\nfeature_scale = ['Fare','Age']\ndf[feature_scale] = sts.fit_transform(df[feature_scale])\ndf.dropna()\n#print(df['Sex'])\n# the test data which is to be predicted\ntes = pd.read_csv('..\/input\/titanic\/test.csv')\ntes[feature_scale] = sts.fit_transform(tes[feature_scale])\ndisplay(tes.head(5))\ntes = tes.loc[:,['Pclass','Sex', 'Age','SibSp','Parch','Fare','Embarked']]\nprede = tes.to_numpy()\ndisplay(tes.head(5))","76b939bf":"display(df.head(5))","ccf16635":"display(df.Survived.mean())","c02ece63":"display(df.Embarked.value_counts())\n\n#display(df.Fare.value_counts())\n\ndisplay(df.Age.mean())","3c81a654":"labels = tuple(df['Survived'])\n# this the traning data label which will be used to train the algorithm.\n#feature_matrix = []\ntrainy = np.array(labels)\n\n\n\n\n# the relevant indexes for our ml purpose is pclass, sex , age , sibsp, parch, Fare, Embarked and\ndf = df.loc[:, ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch','Fare','Embarked']]\nX = df.to_numpy()\n#print(X)\nindex_letter = (X == 'S')\nX[index_letter] = 1\nindex_letter = (X == 'C')\nX[index_letter] = 2\nindex_letter = (X == 'Q')\nX[index_letter] = 3\nindex_male = (X == 'male')\nX[index_male] = 1\nindex_female = (X == 'female')\nX[index_female] = 0\nX = np.array(X, dtype=[('O', np.float)]).astype(np.float)\nindex_nan = np.isnan(X)\nX[index_nan] = 2.388378943731429e-16\n# Some preprocessing will be added to the data before training the algorithm.\n\ndisplay(X[:5])","de2e90eb":"the = SVC(C = 100, kernel='rbf')\nindex_letter = (prede == 'S')\nprede[index_letter] = 1\nindex_letter = (prede == 'C')\nprede[index_letter] = 2\nindex_letter = (prede == 'Q')\nprede[index_letter] = 3\nindex_male = (prede == 'male')\nprede[index_male] = 1\nindex_female = (prede == 'female')\nprede[index_female] = 0\nprede = np.array(prede, dtype=[('O', np.float)]).astype(np.float)\nindex_nan = np.isnan(prede)\nprede[index_nan] = 2.388378943731429e-16\nthe.fit(X , trainy)\n\npred = the.predict(prede)\n\nprint(len(pred))","57dd7188":"resu = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nresu.head(5)\nresul = pd.DataFrame(data = np.transpose([np.array(range(418))+892, pred]), columns=['PassengerId','Survived'])\nresul.head(5)\nresul.to_csv('PREDICTIONSfinalpreprocessed.csv', index=False)\nprint(resul)","04da4e14":"#### The above is the training data, further code will reveal some interesting insights about the data.","b3f49a55":"#### In a previous run of the algorithm, using SVC, I took less features(4) and the algorithm predicted that everybody died, after considering more features like the fare and their place of boarding the accuracy drastically increased\nSome things I picked up from this project are:\n> importance of taking more and more feature<br> preprocessing of the data is important so there is no unnecessary weight given to a following feature.\n\nthis has been a great learning experience so far.","cd349d88":"### The above is the test data, notice that very less people survived in the incident, so dont be all confused if the output suggests everybody died, as we will see in the traning data.","54b452fe":"#### When you factor in the data regarding the fare paid by the customer, the station from which they embarked the picture becomes clearer regarding the prediction of the data, after this the results of the data will be summarised.","bfc2a08d":"#### This scored a respectable 0.7546 in the competition, ","f4d6fab7":"#### The following provides detail regarding mean age, various embarked stations across all the passengers and the fare paid by all. \nAn undocumented attempt has been comitted by me, in that project but I took less parameters.","996c5608":"#### In my previous attempts to the problem, I took only four parameters and it created a problem\n> the first time I did this project the parameters I took were only gender, age, siblings and the parents of the passanger, but the result of the predictions suggested everybody died, so I incresed the parameters, i.e. added the fare and embarked point of the data, and now the results are inline with the 38 percent survival rate of the initial data.","cf351f92":"#### Survival ratio is:","6ff25f1b":"# Titanic project: a first on kaggle\n\nThe following is the my first independent attempt at machine learning to find solutions to a problem, I have used packages such as pandas, sklearn and numpy to solve some of the problems. ","4025d48d":"#### The following methods are used to clean the data and convert them into numeric types, this will make the data easily digestable by the algorithms in the sklearn package."}}