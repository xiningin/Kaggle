{"cell_type":{"d788ece7":"code","30b39354":"code","79ad705a":"code","03b4aba4":"code","369fdb23":"code","7da820f9":"code","ce7e33b2":"code","a947fe7a":"code","e02136db":"code","350afde8":"code","6abac94d":"code","1d408c57":"code","16c6d50e":"code","f9ef18b4":"code","34008553":"code","d03fc8ae":"code","d9261238":"code","5c3d3f95":"code","193c3dda":"code","61da7254":"code","a0d7238f":"code","997c8290":"code","87528418":"code","0e54f868":"code","5a142bc0":"code","e55420b0":"code","5602a3ac":"code","3c20ed6f":"code","846cc317":"code","f17d089c":"code","b55dd302":"code","cdbb61cc":"code","0a8e50ce":"code","a5394e95":"code","4d337f2c":"code","3cf2c90c":"code","c350e32a":"code","2c0e70d1":"code","01e918f3":"code","8dbcc60e":"code","e06f58ec":"markdown","7d0b3af3":"markdown","2839350a":"markdown","3e76d59a":"markdown","fb5b4dfb":"markdown","5d74b652":"markdown","24558221":"markdown","e5c01404":"markdown","be3901fa":"markdown","a14815f0":"markdown","5bb9f7af":"markdown","bf13efc5":"markdown","1bec0226":"markdown","dd0c9b59":"markdown","54a97afb":"markdown","5bb8a0f2":"markdown","d5ad748f":"markdown","82929ea8":"markdown","8d0024c8":"markdown","82728cd6":"markdown","624ba297":"markdown","92e598b8":"markdown","069719a1":"markdown","5f094edc":"markdown","e64d96b3":"markdown","86034dd2":"markdown","9f8a7145":"markdown","616d0533":"markdown","742e5430":"markdown","e67dd0d0":"markdown","905d343c":"markdown","be4b280e":"markdown","f1b5a646":"markdown","b6b2c94b":"markdown","aea924d9":"markdown","a7149e1c":"markdown","a1998671":"markdown"},"source":{"d788ece7":"import numpy as np\nimport matplotlib.pyplot as plt\n\nimport pickle as pkl\nimport os\n\nimport tensorflow\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras import losses, optimizers\nfrom tensorflow.keras import activations\nfrom tensorflow.keras.callbacks import TensorBoard\nfrom tensorflow.keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import plot_model\nfrom IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\n%matplotlib inline","30b39354":"num_classes = 10\nclass_names = ['T-shirt\/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n# input image dimensions\nimg_rows, img_cols = 28, 28","79ad705a":"from tensorflow.keras.datasets import fashion_mnist\n\n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data()","03b4aba4":"if K.image_data_format() == 'channels_first':\n    train_images = train_images.reshape(train_images.shape[0], 1, img_rows, img_cols)\n    test_images = test_images.reshape(test_images.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    train_images = train_images.reshape(train_images.shape[0], img_rows, img_cols, 1)\n    test_images = test_images.reshape(test_images.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\ntrain_images = train_images.astype('float32')\ntest_images = test_images.astype('float32')\n\nprint('train_images shape:', train_images.shape)\nprint('test_images shape:', test_images.shape)\nprint('train_labels shape:', train_labels.shape)\nprint('test_labels shape:', test_labels.shape)\n\nprint(train_images.shape[0], 'train samples')\nprint(test_images.shape[0], 'test samples')","369fdb23":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\ny_counts = pd.DataFrame({\n    'data': np.array(['train'] * num_classes + ['test'] * num_classes),\n    'class': np.tile(np.arange(num_classes), 2),\n    'prop': np.hstack([np.bincount(train_labels) \/ train_labels.shape[0], \n                         np.bincount(test_labels) \/ test_labels.shape[0]])\n})\n\nfig, ax = plt.subplots(figsize=(8, 4))\nsns.barplot(x='class', y='prop', hue='data', data=y_counts, ax=ax)","7da820f9":"y_counts.head()","ce7e33b2":"plt.figure(figsize=(10,10))\nfor i in range(25):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(train_images[i].reshape(28,28), cmap=plt.cm.binary)\n    plt.xlabel(class_names[train_labels[i]])\nplt.show()","a947fe7a":"print(np.array2string(train_images[0].astype(np.int).reshape(28, 28), \n                      max_line_width=150))","e02136db":"plt.figure()\nplt.imshow(train_images[0].reshape(28,28))\nplt.colorbar()\nplt.grid(False)\nplt.show()","350afde8":"train_images = train_images \/ train_images.max()\ntest_images = test_images \/ test_images.max()","6abac94d":"plt.figure()\nplt.imshow(train_images[0].reshape(28,28))\nplt.colorbar()\nplt.grid(False)\nplt.show()","1d408c57":"# convert class vectors to binary class matrices\ntrain_labels = to_categorical(train_labels, num_classes)\ntest_labels = to_categorical(test_labels, num_classes)\ntrain_labels[:10]","16c6d50e":"train_labels.shape","f9ef18b4":"test_labels.shape","34008553":"train_images, valid_images, train_labels, valid_labels = train_test_split(train_images, train_labels, test_size=0.2, random_state=2020)","d03fc8ae":"print(\"Fashion MNIST train -  rows:\",train_images.shape[0],\" columns:\", train_images.shape[1:4])\nprint(\"Fashion MNIST valid -  rows:\",valid_images.shape[0],\" columns:\", valid_images.shape[1:4])\nprint(\"Fashion MNIST test -  rows:\",test_images.shape[0],\" columns:\", test_images.shape[1:4])","d9261238":"valid_set= (valid_images,valid_labels)","5c3d3f95":"model_cnn = Sequential()\nmodel_cnn.add(Conv2D(64, kernel_size=(5, 5), activation='relu', input_shape=input_shape, name='conv2d_1'))\nmodel_cnn.add(Conv2D(128, kernel_size=(5, 5), activation='relu', name='conv2d_2'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_1'))\nmodel_cnn.add(Dropout(0.25, name='dropout_1'))\nmodel_cnn.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv2d_3'))\nmodel_cnn.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_2'))\nmodel_cnn.add(Dropout(0.25, name='dropout_2'))\nmodel_cnn.add(Flatten(name='flatten'))\nmodel_cnn.add(Dense(256, activation='relu', name='dense'))\nmodel_cnn.add(Dense(num_classes, activation='softmax', name='output'))\n                  \nmodel_cnn.compile(loss=losses.categorical_crossentropy,\n                  optimizer=optimizers.Adadelta(),\n                  metrics=['accuracy'])\n\nmodel_cnn.summary()","193c3dda":"#run the model\nbatch_size = 32\nepochs = 20\n\n# Run the train\nhistory_cnn = model_cnn.fit(train_images, train_labels,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=valid_set)\nscore_cnn = model_cnn.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score_cnn[0])\nprint('Test accuracy:', score_cnn[1])","61da7254":"model_cnn2 = Sequential()\nmodel_cnn2.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape, name='conv2d_1'))\nmodel_cnn2.add(Conv2D(128, kernel_size=(3, 3), activation='relu', name='conv2d_2'))\nmodel_cnn2.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_1'))\nmodel_cnn2.add(Dropout(0.25, name='dropout_1'))\nmodel_cnn2.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv2d_3'))\nmodel_cnn2.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_2'))\nmodel_cnn2.add(Dropout(0.25, name='dropout_2'))\nmodel_cnn2.add(Flatten(name='flatten'))\nmodel_cnn2.add(Dense(256, activation='relu', name='dense'))\nmodel_cnn2.add(Dense(num_classes, activation='softmax', name='output'))\n                    \nmodel_cnn2.compile(loss=losses.categorical_crossentropy,\n                  optimizer=optimizers.Adadelta(),\n                  metrics=['accuracy'])\n\nmodel_cnn2.summary()\n\n\n","a0d7238f":"#run the model\nbatch_size = 32\nepochs = 20\n\n# Run the train\nhistory_cnn2 = model_cnn2.fit(train_images, train_labels,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=0,\n                            validation_data=valid_set)\nscore_cnn2 = model_cnn2.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score_cnn2[0])\nprint('Test accuracy:', score_cnn2[1])","997c8290":"model_cnn3 = Sequential()\nmodel_cnn3.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape, name='conv2d_1'))\nmodel_cnn3.add(Conv2D(128, kernel_size=(3, 3), activation='relu', name='conv2d_2'))\nmodel_cnn3.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_1'))\nmodel_cnn3.add(Dropout(0.25, name='dropout_1'))\nmodel_cnn3.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv2d_3'))\nmodel_cnn3.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_2'))\nmodel_cnn3.add(Dropout(0.25, name='dropout_2'))\nmodel_cnn3.add(Flatten(name='flatten'))\nmodel_cnn3.add(Dense(256, activation='relu', name='dense'))\nmodel_cnn3.add(Dense(num_classes, activation='softmax', name='output'))\n                    \nmodel_cnn3.compile(loss=losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n","87528418":"#run the model\nbatch_size = 32\nepochs = 20\n\n# Run the train\nhistory_cnn3 = model_cnn3.fit(train_images, train_labels,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=0,\n                            validation_data=valid_set)\nscore_cnn3 = model_cnn3.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score_cnn3[0])\nprint('Test accuracy:', score_cnn3[1])","0e54f868":"model_cnn4 = Sequential()\nmodel_cnn4.add(Conv2D(64, kernel_size=(3, 3), activation='relu', input_shape=input_shape, name='conv2d_1'))\nmodel_cnn4.add(Conv2D(128, kernel_size=(3, 3), activation='relu', name='conv2d_2'))\nmodel_cnn4.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_1'))\nmodel_cnn4.add(Dropout(0.5, name='dropout_1'))\nmodel_cnn4.add(Conv2D(256, kernel_size=(3, 3), activation='relu', name='conv2d_3'))\nmodel_cnn4.add(MaxPooling2D(pool_size=(2, 2), name='max_pool_2'))\nmodel_cnn4.add(Dropout(0.5, name='dropout_2'))\nmodel_cnn4.add(Flatten(name='flatten'))\nmodel_cnn4.add(Dense(256, activation='relu', name='dense'))\nmodel_cnn4.add(Dense(num_classes, activation='softmax', name='output'))\n                    \nmodel_cnn4.compile(loss=losses.categorical_crossentropy,\n                  optimizer='adam',\n                  metrics=['accuracy'])","5a142bc0":"from tensorflow.keras.callbacks import EarlyStopping","e55420b0":"#run the model\nbatch_size = 32\nepochs = 20\n\n# Run the train\nhistory_cnn4 = model_cnn4.fit(train_images, train_labels,\n                            batch_size=batch_size,\n                            epochs=epochs,\n                            verbose=1,\n                            validation_data=valid_set,\n                            callbacks=[EarlyStopping(monitor='val_loss', patience=5)])\nscore_cnn4 = model_cnn4.evaluate(test_images, test_labels, verbose=0)\nprint('Test loss:', score_cnn4[0])\nprint('Test accuracy:', score_cnn4[1])","5602a3ac":"# summarize history for accuracy\n\ndef history_accuracy(history_model):\n  history = history_model\n  plt.plot(history.history['accuracy'])\n  plt.plot(history.history['val_accuracy'])\n  plt.title('model accuracy')\n  plt.ylabel('accuracy')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()\n# summarize history for loss\n\ndef history_loss(history_model):\n  history = history_model\n  plt.plot(history.history['loss'])\n  plt.plot(history.history['val_loss'])\n  plt.title('model loss')\n  plt.ylabel('loss')\n  plt.xlabel('epoch')\n  plt.legend(['train', 'test'], loc='upper left')\n  plt.show()\n\n","3c20ed6f":"#Loss\nprint(\"FIRST CNN MODEL:\" )\nhistory_loss(history_cnn)\nprint(\"2ND CNN MODEL :\")\nhistory_loss(history_cnn2)\nprint(\"3RD CNN MODEL:\")\nhistory_loss(history_cnn3)\nprint(\"4TH CNN MODEL: \")\nhistory_loss(history_cnn4)","846cc317":"# Accuracy\nprint(\"FIRST CNN MODEL:\" )\nhistory_accuracy(history_cnn)\nprint(\"2ND CNN MODEL :\")\nhistory_accuracy(history_cnn2)\nprint(\"3RD CNN MODEL:\")\nhistory_accuracy(history_cnn3)\nprint(\"4TH CNN MODEL: \")\nhistory_accuracy(history_cnn4)\n\n","f17d089c":"fig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nCNN = ['CNN 1', 'CNN 2', 'CNN 3', 'CNN 4']\nTest_Accuracy = [score_cnn[1],score_cnn2[1],score_cnn3[1],score_cnn4[1]]\nax.bar(CNN,Test_Accuracy)\nplt.show()","b55dd302":"def plot_image(i, predictions_array, true_label, img):\n  true_label, img = true_label[i], img[i]\n  plt.grid(False)\n  plt.xticks([])\n  plt.yticks([])\n\n  plt.imshow(img.reshape(28,28), cmap=plt.cm.binary)\n\n  predicted_label = np.argmax(predictions_array)\n  true_label2=np.argmax(true_label)\n  if predicted_label == np.argmax(true_label):\n    color = 'blue'\n  else:\n    color = 'red'\n\n  plt.xlabel(\"{} {:2.0f}% ({})\".format(class_names[predicted_label],\n                                100*np.max(predictions_array),\n                                class_names[true_label2]),\n                                color=color)\n\ndef plot_value_array(i, predictions_array, true_label):\n  true_label = true_label[i]\n  plt.grid(False)\n  plt.xticks(range(10))\n  plt.yticks([])\n  thisplot = plt.bar(range(10), predictions_array, color=\"#777777\")\n  plt.ylim([0, 1])\n  predicted_label = np.argmax(predictions_array)\n  true_label2=np.argmax(true_label)\n\n  thisplot[predicted_label].set_color('red')\n  thisplot[true_label2].set_color('blue')","cdbb61cc":"# Plot the first X test images, their predicted labels, and the true labels.\n# Color correct predictions in blue and incorrect predictions in red.\ndef plot_prediction_images(prediction):\n  num_rows = 5\n  num_cols = 3\n  num_images = num_rows*num_cols\n  plt.figure(figsize=(2*2*num_cols, 2*num_rows))\n  for i in range (num_images):\n    plt.subplot(num_rows, 2*num_cols, 2*i+1)\n    plot_image(i, prediction[i], test_labels, test_images)\n    plt.subplot(num_rows, 2*num_cols, 2*i+2)\n    plot_value_array(i, prediction[i], test_labels)\n  plt.tight_layout()\n  plt.show()\n","0a8e50ce":"probability_model = tensorflow.keras.Sequential([model_cnn, \n                                         tensorflow.keras.layers.Softmax()])\npredictions = probability_model.predict(test_images)\n\n","a5394e95":"plot_prediction_images(predictions)","4d337f2c":"probability_model = tensorflow.keras.Sequential([model_cnn4, \n                                         tensorflow.keras.layers.Softmax()])\npredictions4 = probability_model.predict(test_images)\nplot_prediction_images(predictions4)","3cf2c90c":"def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix', cmap=plt.cm.Blues):\n  plt.imshow(cm, interpolation='nearest', cmap=cmap)\n  plt.title(title)\n  plt.colorbar()\n  tick_marks = np.arange(len(classes))\n  plt.xticks(tick_marks, classes, rotation=90)\n  plt.yticks(tick_marks, classes)\n\n  if normalize:  cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n  thresh = cm.max() \/ 2.\n  for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n    plt.text(j, i, cm[i, j], horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","c350e32a":"from sklearn.metrics import confusion_matrix, classification_report\nimport itertools","2c0e70d1":"# Predict the values from the validation dataset\nY_pred = model_cnn.predict(test_images)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(test_labels,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])\n","01e918f3":"# Predict the values from the validation dataset\nY_pred = model_cnn4.predict(test_images)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(test_labels,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])\n","8dbcc60e":"i = 4\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(977, predictions4[977], test_labels, test_images)\nplt.subplot(1,2,2)\nplot_image(905, predictions4[905], test_labels, test_images)\nplt.show()\n\nplt.figure(figsize=(6,3))\nplt.subplot(1,2,1)\nplot_image(830, predictions4[830], test_labels, test_images)\nplt.subplot(1,2,2)\nplot_image(860, predictions4[860], test_labels, test_images)\nplt.show()\n","e06f58ec":"In the next cell, we illustrate the first elements of the training data: pixels grayscale of the clothes and their corresponding label.","7d0b3af3":"## **II. Analysis of the results**\n\n","2839350a":"The first and second models achieve lower accuracy level. However, the train and test accuracy levels are close enough to conclude that there is no overfitting. \nOverfitting is clearly observed in the 3rd model, and largely reduced in the 4th one. ","3e76d59a":"There are 60 000 training images and 10 000 testing images. All the images are 28x28 pixels. ","fb5b4dfb":"T-Shirt and Shirt have a very close shape. Hence, the CNN detects a similar pattern of grey pixels. \nThe fact that T-Shirt and Shirt can have long or short sleeves make it even more complicated for the CNN for correctly learn the pattern associated to one or the other of those two categories.","5d74b652":"### **3rd version of the CNN: Changing the optimizer**","24558221":"The lower the loss, the better a model. The loss is calculated on training and validation and its interperation is how well the model is doing for these two sets. Unlike accuracy, loss is not a percentage. It is a summation of the errors made for each example in training or validation sets. In the case of neural networks, the loss is usually negative log-likelihood and residual sum of squares for classification and regression respectively. \n\nBy increasing the number of epochs, the loss would have probably continue to decrease for the 1st nd 2nd CNN. However, it stays at 1st a higher lever for those models than the last one.\nWe note that the 3rd CNN's loss function of the test set diverges from the loss function of the train set. One would expect the reduction of loss after each, or several, iterations, which is clearly not the case for this model's test set.","e5c01404":"Data cleaning is the first step in every machine learning exercise. \nHowever, this is a widely used dataset of images for praticing so the dataset is -almost- ready to be used. \nWe still need to normalize the data.\n","be3901fa":"The training dataset has the same proportion of each class, which is optimal to train a model so it can be trained with the same occurence of each class. ","a14815f0":"\n\n---\n\n","5bb9f7af":"### **Second version of the CNN: changing the kernel size**","bf13efc5":"## **Conclusion**\n\nOur best model achieve an accuracy level of 92% on the testing data set. \nReducing the size of the kernel to 3x3 in each convolutional layers, changing for 'adam' optimizer and setting an earlystopping method were valuable to train our model.\n\nAfter analysing the predictions of the best model, we realised that some categories were harder to classify than others. In general, upper-body clothes (T-Shirt, Shirt, Coat...) are difficult to identify because of they similar shape (i.e similar pixels patterns).\n\nLast but not least, we could wonder if the model we just trained on the Fashion MNIST dataset would be directly applicable to images outside the Fashion MNIST dataset. \nFor the answer to be yes, a lot of work would be needed on the newly used clothes images. Indeed, MNIST dataset images have already been processed (Converted to grayscale, Segmented,Resized)\nFor real-world fashion and clothing images, you would have to preprocess your data in the same manner as the Fashion MNIST dataset.\nEven with this precaution, there's a high probability that the model doesn't accurately classify new images as they would be too different from the standardized mnist fashion one. ","1bec0226":"### **Creation of a validation set**\n\nWe further split the train set into a train and validation set. The validation set will be 20% from the original train set, therefore the split will be train\/validation of 0.8\/0.2.\n\nThe actual training set will be divided into two groups:\n* the training set \n* the validation set\n\nThe latter is going to be used during the learning process of the model. Before training begin, we take a subset of the training set and label it has \"validation data\". Then, during the traiing process, the model will train on the training data and validate on the separated validation data. The model will, in addition in each epoch, predict on the validation set. The validation set is for us to see how general the model is on data not included on the training data. It will also help us to see if overfittig problem occurs.  \n\nWe could notice two things:\n* We could have used the \"validation_split\" argument of the fit function which directly set apart a fraction of the training data. The model will not train on it, and will evaluate the loss and any model metrics on this data at the end of each epoch.\n\n* The \"Mnist_fashion\" dataset is pre shuffled but it is important to keep in mind that shuffling data serves the purpose of reducing variance and making sure that models remain general and overfit less.\nShuffling makes sure that the training\/test\/validation sets are representative of the overall distribution of the data.","dd0c9b59":"# **Introduction to machine learning: Image classification with deep learning** ","54a97afb":"The second CNN achieve a slightly better accuracy level, both during training and on testing set. Hence, we can conclude that the 3X3 kernel size is more adapted to the fashion mnist images. \n","5bb8a0f2":"### **First CNN**","d5ad748f":"Each image can be visualized as a 28x28 matrix where each pixels is translated into a number from 0 to 255, corresponding to a shade of grey. The scale of grey goes from 0 (which correspond to white) to 255 (corresponds to black).","82929ea8":"*Remarks:*\n\nI ran the same models with different \"batch_size\" (10, 64, 128) and noticed that:\n* for the firsts CNNs, a small batch_size led to better accuracy levels. \n* However, for the last ones (which have been otpimized from the previous ones), changing the batch size didn't have a significant effect on the accuracy level.\n","8d0024c8":"### 4th CNN predictions","82728cd6":"Analysing the confusing matrixs, we conclude that shirt is the hardest type of clothe to classify. Our 4th model classifies \"Shirt\" more than 6 times better than the first CNN. It is still quite low compared to the other categories. \nThe misclassification between 'T-Shirt\/Top' and 'Shirt' often occurs; one is often classifies as the other.\n\nOn the other hand, 'Bag', 'Trouser' and 'Sandal' are the best classified categories in the 4th CNN. This can be explained by their very particular shapes, very different from the other items.","624ba297":"\n**Epoch:**\nOne Epoch is when an entire dataset is passed forward and backward through the neural network only once. Since one epoch is too big to feed to the computer at once we divide it in several smaller batches.\nWe need to pass the full dataset multiple times to the same neural network, this is why epoch isn't equal to 1.\nOne of the critical issues while training a neural network on the sample data is Overfitting. It tends to happen when the number of epochs used to train a neural network model is \"too high\". We start with epochs=20 which generally is enough for the model to converge before overfitting.\n\n\n**Batch Size**\nIt is a hyperparameter to tune too. The batch size is the number of samples that are passed to the network at once. We start with batch_size=32.\n\n**Optimizer**\nAn optimizer update the weight parameters to minimize the loss function. We strat by using Adadelta.\n","92e598b8":"Deep learning is a type of artificial intelligence derived from machine learning where the machine is capable of \"learning by itself\". This homework illustrates the use of Convolutional Neural Network \u2014 a pillar algorithm of deep learning - which has been one of the most influential innovations in the field of computer vision.\nHere, we are going to use these neural networks in order to classify images. This homework uses the Fashion MNIST dataset, from the Application Programming Interface \"Keras\". This dataset contains 70,000 grayscale images associated with a label from 10 classes.\n\nBefore getting any further, let's have a look at the data. \n\n\n\n\n","069719a1":"Now that we are familiar with the data, let's dive into the real work: building a neural network that will be able to classify accurately new clothes images.\n\nIn this section, we are going to try to create the most efficient CNN by modifying the architecture of the model but also hyperparameters. \n","5f094edc":"Both bias and variance are forms of prediction error in machine learning. Typically, we can reduce error from bias but might increase error from variance as a result, or vice versa. This trade-off between too simple (high bias) vs. too complex (high variance) is a key concept in statistics and machine learning, and one that affects all supervised learning algorithms. \nHere, we observe after 20 epochs that there is an increasing gap between the accuracy level on the training set and the validation\/testing one. The accuracy level on the training set tends to increase to 99% while the accuracy on the validation set converges around 93%. \nTypically, the Dropout is a technique used to prevent a model from overfitting. Dropout works by randomly setting the outgoing edges of hidden units (neurons that make up hidden layers) to 0 at each update of the training phase. \nWe tried to build a CNN without dropout layers and observed a large overfitting. \nWe increased the dropout rate to 0.5. In passing 0.5, every hidden unit (neuron) is set to 0 with a probability of 0.5. In other words, there\u2019s a 50% change that the output of a given neuron will be forced to 0. \nWe try this method and the aftermaths were:\n* Almost no more overfitting\n* A slightly lower accuracy level on the testing set (91.3%)\n\nWe can use EarlyStopping, which is also a useful method to prevent overfitting. \nThe **patience** argument corresponds to the number of epochs with no improvement after which training will be stopped. We decide to set it to 5.\n","e64d96b3":"### II. B. Which clothes are the harder to classify ?","86034dd2":"\n\n**Let's explore the data:**","9f8a7145":"### Pre processing the data:","616d0533":"\n\n---\n\n","742e5430":"### **4th version of the CNN: Preventing overfitting**","e67dd0d0":"Instead of using Adadelta optimizer, we are trying to assess if other optimizers would build better predictive model. We are now using the 'Adam' optimizer. Adam: Adaptive Moment Estimation (Adam) is a method that computes adaptive learning rates for each parameter. In addition to storing an exponentially decaying average of past squared gradients like Adadelta, Adam also keeps an exponentially decaying average of past gradients, similar to momentum.","905d343c":"**Comment on the first neural network**\n\nThe accuracy level on the training set is quite low (74%). With epochs=20, we see that the level of accuracy still increases. However, increasing the epochs hyperparameter is time-consuming for only small improvement in accuracy.\n\nInstead, we are going to focus first on the kernel size used. Here, we used a 5x5 kernel size. The question raised is *how to choose a kernel size ?*\n\nMost of the useful features in an image are usually local and it makes sense to take few local pixels at a time to apply convolutions.\nMost of these useful features may be found in more than one place in an image. So, it makes sense to slide a single kernel all over the image in the hope of extracting that feature in different parts of the image using the same kernel. Let's notice that even kernel size are skipped for implementation simplicity. Moreover, out input images are quite small (28x28).\nHence, we should try using a 3x3 kernel (which is a popular choice).","be4b280e":"## **I. Builing the CNN models**","f1b5a646":"### II. A. Comparaison of accuracy and loss ","b6b2c94b":"We normalize the data:","aea924d9":"The scale of grey is now from 0 to 1.\n\n\n\n\n","a7149e1c":"### 1st CNN predictions","a1998671":"Adam optimizer increased the accuracy level on the test set by 17% ! This is a major improvement. However, the gap between the accuracy on the training (98%) and the testing set illustrates the \"over-fitting\" phenomenon."}}