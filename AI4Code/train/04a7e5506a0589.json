{"cell_type":{"d913e0d3":"code","74cd7003":"code","84e237ac":"code","21aa113f":"code","cbde482b":"code","78c4727f":"code","f59bec98":"code","c51f8cc1":"code","ad76064c":"code","38f85ce6":"code","beb5ecfb":"code","7c15506d":"code","d3e3a002":"code","8d00a458":"code","6ca3674d":"code","5b496729":"code","d47ccab9":"code","27cfad90":"code","4f726697":"code","3005a1a8":"code","0b04d190":"code","c469b228":"code","78f2050c":"markdown"},"source":{"d913e0d3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","74cd7003":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nsns.set_style('darkgrid')","84e237ac":"data = pd.read_csv(\"\/kaggle\/input\/water-potability\/water_potability.csv\", sep = \",\")\ndf = pd.DataFrame(data)\ndf.head()","21aa113f":"df.info()","cbde482b":"df.isnull().sum()","78c4727f":"df = df.fillna(df.mean())","f59bec98":"df.isnull().sum()","c51f8cc1":"sns.countplot(x =df['Potability'], data = df)","ad76064c":"i=1\nplt.figure(figsize=(15,19))\nfor col in df.columns:\n    plt.subplot(4,3,i)\n    sns.boxplot(x = 'Potability', y = col,  data =df)\n    plt.title(col)\n    i+=1\n    ","38f85ce6":"corr = df.corr()\nplt.figure(figsize=(15,10))\nsns.heatmap(corr, linewidth = 0.5, cmap = 'coolwarm', annot =True)","beb5ecfb":"sns.pairplot(df, hue = 'Potability')","7c15506d":"X = df.drop(['Potability'],1)\ny = df.Potability","d3e3a002":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 43)","8d00a458":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train, y_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)","6ca3674d":"from sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import roc_auc_score , plot_roc_curve\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(learning_rate = 0.03, max_depth = 8, n_estimators = 1000, \n                    verbosity = 1, random_state = 44, use_label_encoder=False)\n\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error\nrf = RandomForestClassifier(random_state=43)\nad = AdaBoostClassifier(base_estimator =rf)\ndt = DecisionTreeClassifier()\nkn = KNeighborsClassifier()\nrbf = RBF()\ngp = GaussianProcessClassifier(1.0 * RBF(1.0))\nmlp = MLPClassifier(alpha=1, max_iter=1000)\ngnb = GaussianNB()\nsvc = SVC(random_state = 43, C = 10, gamma = 0.1, kernel ='rbf')\n\nmodels = [rf,ad, dt, kn, svc, mlp,xgb ]\nfor model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    scores = cross_val_score(model, X, y, cv=5).mean().round(3)\n    accuracy = metrics.classification_report(y_test, y_pred)\n    #f1score = metrics.f1_score(y_test, y_pred).round(3)\n    print(model, '\\n', 'REPORT:','\\n', accuracy,'\\n', 'mean_score:',scores, '\\n' )","5b496729":"from sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\n#max_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\n#min_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nrandom_state  = [int(x) for x in np.linspace(10, 100,num= 10)]\n# Method of selecting samples for training each tree\nlearning_rate = [0.03,0.05,0.07,0.1,0.2]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_depth': max_depth,\n               'random_state': random_state,\n               'learning_rate': learning_rate}\nprint(random_grid)","d47ccab9":"# Use the random grid to search for best hyperparameters\n# First create the base model to tune\n#xgb = XGBClassifier()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\n#xgb_random = RandomizedSearchCV(estimator = xgb, param_distributions = random_grid, n_iter = 70,\n#                                cv = 3, verbose=2, n_jobs = -1)\n# Fit the random search model\n#xgb_random.fit(X_train, y_train)","27cfad90":"#xgb_random.best_params_","4f726697":"xgb =  XGBClassifier(random_state = 90, n_estimators = 200, max_depth = 200, learning_rate = 0.07)\nmodel = xgb.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nscores = cross_val_score(model, X, y, cv=3).mean().round(3)\naccuracy = metrics.classification_report(y_test, y_pred)\n#f1score = metrics.f1_score(y_test, y_pred).round(3)\nprint(model, '\\n', 'REPORT:','\\n', accuracy,'\\n', 'mean_score:',scores, '\\n' )","3005a1a8":"from yellowbrick.classifier import confusion_matrix\nfrom yellowbrick.classifier import ClassificationReport\nclasses = [0,1]\nvisualizer = ClassificationReport(xgb, classes=classes, support=True)\n\nvisualizer.fit(X_train, y_train)        # Fit the visualizer and the model\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and show the figure","0b04d190":"#confusion_matrix(XGbClassifier(), X_train, y_train, X_test, y_test, classes=[0, 1])\n#plt.tight_layout()\nfrom yellowbrick.classifier import ConfusionMatrix\n\ncm = ConfusionMatrix(xgb, classes=[0,1], label_encoder={0: 'Potable', 1: 'Not_Potable'})\n\ncm.fit(X_train, y_train)\ncm.score(X_test, y_test)\ncm.show()","c469b228":"from yellowbrick.classifier import ROCAUC\nvisualizer = ROCAUC(xgb, classes=[\"Potable\", \"Not_Potable\"])\n\nvisualizer.fit(X_train, y_train)        # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)        # Evaluate the model on the test data\nvisualizer.show()                       # Finalize and show the figure","78f2050c":"# Boxplot of parameters"}}