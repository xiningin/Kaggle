{"cell_type":{"3d2b6427":"code","2422f2a6":"code","315a2393":"code","5c0a6cfd":"code","9aaa6d99":"code","4d37c343":"code","7cd16adb":"code","96512fb7":"code","a7f07f17":"code","499b8fd1":"code","69d27920":"code","a8d00ff0":"markdown","719d1bda":"markdown","0a5cd3fb":"markdown"},"source":{"3d2b6427":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom scipy import optimize\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2422f2a6":"data = pd.read_csv('..\/input\/sq_data.csv')\nprint (data.shape)\ndata.head()","315a2393":"data['install_date'] = pd.to_datetime(data['install_date'],dayfirst = True)\ndata['pay_date'] = pd.to_datetime(data['pay_date'], dayfirst = True)\ndata = data.sort_values('pay_date')\ndef get_cum_sum(date):\n    return data['sum'].where(data['pay_date']<=date).sum()\n\ndata['cum_sum'] = data['pay_date'].map(lambda x: get_cum_sum(x))\ndef get_users_utd(date):\n    return data['user'].where(data['install_date'] <=date).count()\n\ndata['users_n_utd'] = data['pay_date'].map(lambda x: get_users_utd(x))\ndata['ltv'] = data['cum_sum']\/data['users_n_utd'].astype(float)\ndata['day'] = pd.to_timedelta(data['pay_date'] - data['install_date'].min()).dt.days + 1\ndata['day'] = data['day'].astype(int)\ndata.head(10)","5c0a6cfd":"plt.scatter(data['day'], data['ltv'], label ='Days')\nplt.scatter(data['users_n_utd'], data['ltv'], color='r', label='Players')\nplt.scatter(data['cum_sum'], data['ltv'], color='g', label='Revenue')\nplt.scatter(data['sum'], data['ltv'], color='b', label='Daily sales')\nplt.xlabel('Days, Players, Revenue, Daily sales')\nplt.ylabel('LTV')\nplt.legend()","9aaa6d99":"sns.pairplot(data)","4d37c343":"ltv_data = data[['day','users_n_utd', 'cum_sum', 'ltv']]\nX = ltv_data[['day', 'users_n_utd','cum_sum']]\ny = ltv_data['ltv']","7cd16adb":"plt.scatter(data['day'], data['ltv'])\nplt.xlabel('Days')\nplt.ylabel('LTV')","96512fb7":"X_l = X['day'].values \nY_l = y.values\n\ncoefs_l, cov = optimize.curve_fit(lambda t,a,b: a+b*np.log(t),  X_l,  Y_l)\n\nprint (coefs_l)\n\ndef ltv_func(param):\n    result = coefs_l[0] + coefs_l[1]*np.log(param)\n    return result\n    \n\nltv_90_180 = ltv_func([90., 180.])\nltv90 = round(ltv_90_180[0],2)\nltv180 = round(ltv_90_180[1],2)\n\nprint (\"LTV by day 90: \" + str(ltv90))\nprint (\"LTV by day 180: \" + str(ltv180))\n","a7f07f17":"days = np.hstack([X_l, [90, 180]])\nplt.scatter(X_l,y.values)\nplt.plot(days,ltv_func(days.reshape(-1, 1)))\nplt.xlabel('Days')\nplt.ylabel('LTV')\nplt.legend(['LTV forecast', 'LTV'])","499b8fd1":"k_days = [1, 3, 7, 30]\nks= []\nfor k in k_days:\n    k_revenue = ltv_data['cum_sum'].loc[ltv_data['day'] == k].values[0] \n    coeff = ltv180\/k_revenue\n    ks.append(coeff)\n    print (\"K\" + str(k) + \": \" + str(round(coeff, 2)))\nk180 = ltv180\/(ltv180 * ltv_data['users_n_utd'].max())","69d27920":"plt.title('Cumulative sales elbow-curve.')\nplt.plot(k_days,ks)\nplt.xlabel('Days')\nplt.ylabel('K')","a8d00ff0":"This is my first experience with submission on Kaggle. So, my apoligies if something does not work well here with this  first attempt :).\n\nThis is a simple way for calculating and projecting LTV ( Life Time Value) for free-to-play platform\/game, using a small portion of historical data.\nAfter reading and analyzing the data sample where we can see that our data contains: useer ID, game installation date, day of payment and amount of payment by user on a payment day.\n\nThe objective here is to:\n\na) Perform a data exploratory analysis\n\nb) Identify the best function for LTV projection. Here we can go with explicit choice of the function to fit (like I did) or use some alternative approach with preliminary feature engineering.\n\nc) Project values for LTV90 and LTV180 where LTV{N} forecast of LTV value for the day N\n\nd) Considering that our player's lifetime is 180 days, we can calculate the coefficients K1, K3, K7, K30 and K180, where K{N} is the ratio between revenue by day N and LTV180, identifyig the bending point at which cumulative revenue generation slows down and we either need to attract more users or stimulate existing users to make additional purchases.","719d1bda":"LTV in our case simply depends on two variables - cumulative sales and number of users - all that by days.\nSo, here is an approach:\nLTV by day has somewhat like a log-fnction shape and we can predict it via optimizing the fit of the LTV value to the log-like function curve. This can be done with 2 assumptions: number of users and cumulative sales are reaching saturation, which is somewhat true for the users once we look at the users over days plot and not so evident for the daily and thus cumulative sales. However, we can assume that the normal behaviour of the user on the platform shows that after certain amount of days user does not pay anymore or churns. Then we can easily apply here the LTV prediction using optimize.curve_fit method.","0a5cd3fb":"Analyzin the chart above we can see that probably around days 3-7 we need to simulate our users with additional offers pushing them to perform more purchases on the platform.\n"}}