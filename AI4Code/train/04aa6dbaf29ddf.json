{"cell_type":{"6512c36e":"code","9a648e88":"code","4102093a":"code","01095b41":"code","60c95a3b":"code","d72b40e5":"code","0f71f31a":"code","73cc9b0b":"code","38eac092":"code","bd91954f":"code","5837577b":"code","ab2bc9fd":"code","ab1e23e4":"code","bdd1aa5b":"code","7b5352b3":"code","21c6bd51":"code","9f26191f":"code","1e8d8280":"code","87461770":"code","79f3a9bb":"code","371da956":"code","1b3d641b":"code","a7b8af40":"code","a30936f9":"code","b11d4b86":"code","b423e1c6":"code","2c01af99":"code","f0ed583e":"code","7fad79d4":"code","4f2d68e9":"code","6d40d6e8":"code","7f669166":"code","6a3a1d07":"code","ae849774":"code","f3200b8d":"code","295a582e":"code","35fb00a2":"code","998b245e":"code","fe6f065f":"code","36dc5d9b":"code","5916cb05":"code","6ae2fab1":"code","f3c13fdf":"code","8c5acbf5":"code","19937f59":"code","d92f126c":"code","8210dcd6":"code","05b30411":"code","41ece1c4":"code","2c80920e":"code","c4e5fa81":"markdown","6dced467":"markdown","3d9bd263":"markdown","e94eff34":"markdown","467aa858":"markdown","92ec16e9":"markdown","dc8e522e":"markdown","341acbf5":"markdown","10a6f577":"markdown","310a4d7e":"markdown","1d1c4412":"markdown","6a778b20":"markdown","fb9e39b8":"markdown","90a066b9":"markdown","dd1df7e3":"markdown","b68bc2f4":"markdown","93f8e9a8":"markdown","e6e69012":"markdown","f2523925":"markdown","29f77364":"markdown","4e4e25ae":"markdown"},"source":{"6512c36e":"!pip install scispacy\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_ner_bc5cdr_md-0.2.4.tar.gz","9a648e88":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport re\nimport json\nimport gc\nfrom glob import glob\n\nimport scispacy\nimport spacy\nimport en_ner_bc5cdr_md\n\nfrom sklearn.metrics.pairwise import cosine_similarity\n\npd.set_option('max_columns', 100)\nfrom IPython.core.display import display, HTML\ndisplay(HTML('<style>.container { width:100% !important; }<\/style>'))","4102093a":"def format_ref(r):\n    return '\\n'.join(['{0}-{1}:{2}'.format(x['start'],x['end'],x['ref_id']) for x in r])\n\ndef format_author(author):    \n    return \" \".join([author['first'], \" \".join(author['middle']), author['last']])\n\ndef json_reader(file):\n    #takes a json file, processes the body, ref, and bib data into a dataframe\n    with open(file) as f:\n        j = json.load(f)\n        \n    #format the body text so the sections are clear, but it's easy to view the whole thing\n    body_text = '\\n\\n'.join(['<section {}> '.format(n) + x['section'] + '\\n\\n' + x['text'] for n,x in enumerate(j['body_text'])])\n    ref_spans = '\\n\\n'.join(['<section {}> '.format(n) + x['section'] + '\\n\\n' + format_ref(x['ref_spans']) for n,x in enumerate(j['body_text'])])\n    cite_spans = '\\n\\n'.join(['<section {}> '.format(n) + x['section'] + '\\n\\n' + format_ref(x['cite_spans']) for n,x in enumerate(j['body_text'])])\n    \n    #format references in a similar way\n    ref_data = '\\n\\n'.join([k + '\\n\\n' + v['text'] + '\\n\\nlatex- {}'.format(v['latex']) for k,v in j['ref_entries'].items()])\n\n    #put the bibliography together, and format the authors\n    for k in j['bib_entries']:\n        j['bib_entries'][k]['author_list'] = ', '.join([format_author(a) for a in (j['bib_entries'][k]['authors'])])\n\n    bib_keys = ['ref_id', 'title', 'author_list', 'year', 'venue', 'volume', 'issn', 'pages', 'other_ids']\n    bib_data = '\\n\\n'.join([', '.join([str(x[k]) for k in bib_keys]) for _,x in j['bib_entries'].items()])\n\n    df = pd.DataFrame(index=[0], data={'body_text':body_text, \n                                            'cite_spans':cite_spans, \n                                            'ref_spans':ref_spans,\n                                            'ref_data': ref_data,\n                                            'bib_data': bib_data,\n                                            'paper_id': j['paper_id']})\n    \n    return df\n\n\ndef parse_folder(data_folder):\n    filelist = glob('\/kaggle\/input\/CORD-19-research-challenge\/{0}\/{0}\/*'.format(data_folder))\n    filelist.sort()\n    print('{} has {} files'.format(data_folder, len(filelist)))\n\n    df_ls=[]\n    for n,file in enumerate(filelist):\n        if n%1000==0:\n            print(n,file[-46:])\n        df = json_reader(file)\n        df_ls.append(df)\n    return pd.concat(df_ls)\n","01095b41":"#go through each of the four folders of json files and put everything into one dataframe\n#takes around 3-4min to complete\ndf_ls = []\nfor folder in ['comm_use_subset', 'noncomm_use_subset', 'custom_license', 'biorxiv_medrxiv']:\n    t = parse_folder(folder)\n    df_ls.append(t)\ndf = pd.concat(df_ls)","60c95a3b":"meta = pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv')\nmeta.rename(columns={'sha':'paper_id'}, inplace=True)\ndf = meta.merge(df, on='paper_id', how='left')","d72b40e5":"df.shape","0f71f31a":"df.head(3)","73cc9b0b":"df.shape","38eac092":"df.drop_duplicates(['abstract', 'body_text', 'ref_data'], inplace=True)\ndf.shape","bd91954f":"df.isna().sum(axis=0).sort_values(ascending=False) \/ df.shape[0]","5837577b":"df.source_x.value_counts(normalize=True)","ab2bc9fd":"df[df['paper_id'].notna()].source_x.value_counts(normalize=True)","ab1e23e4":"df.license.value_counts(normalize=True)","bdd1aa5b":"df.journal.value_counts(normalize=True).head(10)","7b5352b3":"df.publish_time.value_counts(normalize=True).head(10)","21c6bd51":"pub_year = df['publish_time'].dropna().apply(lambda x: x[:4]).value_counts().sort_index()\nplt.bar(pub_year.index, np.log10(pub_year.values))\nplt.xticks([i for i in range(pub_year.shape[0]) if i%8==0]);\nplt.title('Log Plot of All Publications Per Year');\nplt.ylabel('Log10');","9f26191f":"pub_year = df[df['source_x'] == 'biorxiv']['publish_time'].dropna().apply(lambda x: x[:4]).value_counts().sort_index()\nplt.bar(pub_year.index, np.log10(pub_year.values))\n\nplt.xticks([i for i in range(pub_year.shape[0]) if i%1==0]);\nplt.title('BIORXIV Log Plot of Publications Per Year');\nplt.ylabel('Log10');","1e8d8280":"pub_year = df[df['source_x'] == 'CZI']['publish_time'].dropna().apply(lambda x: x[:4]).value_counts().sort_index()\nplt.bar(pub_year.index, np.log10(pub_year.values))\n\nplt.xticks([i for i in range(pub_year.shape[0]) if i%1==0]);\nplt.title('BIORXIV Log Plot of Publications Per Year');\nplt.ylabel('Log10');","87461770":"#All MEDRXIV PAPERS ARE MISSING MOST PUBLISH TIMES\ndf[df['source_x'] == 'medrxiv']['publish_time'].notna().sum() \/ df[df['source_x'] =='medrxiv'].shape[0]","79f3a9bb":"((df['abstract'].dropna().str.contains('corona', case=False)) | (df['abstract'].dropna().str.contains('COVID', case=False))).sum() \/ df.shape[0]","371da956":"df[df['abstract'].fillna('').str.contains('chloroquine', case=False)].shape[0]","1b3d641b":"df[df['abstract'].fillna('').str.contains('favipiravir', case=False)].shape[0]","a7b8af40":"df[df['abstract'].fillna('').str.contains('lopinavir', case=False)].shape[0]","a30936f9":"df[df['abstract'].fillna('').str.contains('ritonavir', case=False)].shape[0]","b11d4b86":"df[df['abstract'].fillna('').str.contains('convalescent plasma', case=False)].shape[0]","b423e1c6":"df[df['abstract'].fillna('').str.contains('passive antibody', case=False)].shape[0]","2c01af99":"df['abstract'].dropna().apply(lambda x: len(x.split(' '))).describe()","f0ed583e":"df['body_text'].dropna().apply(lambda x: len(x.split(' '))).describe()","7fad79d4":"long_idx = df[df['abstract'].apply(lambda x: len(x.split(' ')) if isinstance(x,str) else 0) > 10000]['paper_id'].values[0]","4f2d68e9":"df[df['paper_id'] == long_idx][['paper_id', 'source_x','title', 'license','abstract','publish_time', 'authors','journal','has_full_text']]","6d40d6e8":"' '.join(df[df['paper_id'] == long_idx]['abstract'].values[0].split(' ')[:100])","7f669166":"' '.join(df[df['paper_id'] == long_idx]['abstract'].values[0].split(' ')[-100:])","6a3a1d07":"df.dropna(subset=['abstract'], inplace=True)\ndf.drop(columns='body_text', inplace=True)","ae849774":"df.shape","f3200b8d":"gc.collect()","295a582e":"#load the scispacy model relevant to diseases\nnlp = spacy.load('en_ner_bc5cdr_md')","35fb00a2":"nlp.vocab.length","998b245e":"#each word comes with an embedding\nnlp(df.iloc[0]['abstract'])[0].vector[:10]","fe6f065f":"def get_doc_vec(tokens):\n    #combine word embeddings from a document into a single document vector\n    #filter out any stop words like 'the', and remove any punction\/numbers\n    w_all = np.zeros(tokens[0].vector.shape)\n    n=0\n    for w in tokens:\n        if (not w.is_stop) and (len(w)>1) and (not w.is_punct) and (not w.is_digit):\n            w_all += w.vector\n            n+=1\n    return (w_all \/ n) if n>0 else np.zeros(tokens[0].vector.shape)","36dc5d9b":"#takes a long time, load from file\nvector_dict={}\nfor n,row in df.iterrows():\n    if n%500==0:\n        print(n)\n    if len(row['abstract']) > 0:\n        vector_dict[row['paper_id']] = get_doc_vec(nlp(row['abstract']))","5916cb05":"vec_vals = list(vector_dict.values())\nvec_vals = [v for v in vec_vals if all(v==0)==False]","6ae2fab1":"pd.to_pickle(vec_vals, 'vec_vals.pkl')","f3c13fdf":"len(vec_vals)","8c5acbf5":"q_vec = [get_doc_vec(nlp('What do we know about COVID-19 risk factors?'))]","19937f59":"target_sims = cosine_similarity(vec_vals, q_vec)","d92f126c":"target_sims.shape","8210dcd6":"q_series = pd.Series(dict(zip(vector_dict.keys(), target_sims)))","05b30411":"closet_papers = q_series.sort_values(ascending=False).head(10).index.tolist()","41ece1c4":"pd.set_option('max_colwidth',200)","2c80920e":"df[df['paper_id'].isin(closet_papers)][['title','abstract']]","c4e5fa81":"# Covid References","6dced467":"### Journal Distribution","3d9bd263":"### Long Abstract Example","e94eff34":"### Some body text is less than 1000 words","467aa858":"The goal of this notebook is do two things:\n1. Some general, basic exploration of the dataset to become familiar with columns and potential issues. I include functions for loading the data as well\n2. Use cosine similarity with the scispacy package to find articles related to the task questions","92ec16e9":"#  Basic Stats","dc8e522e":"# Publish Time","341acbf5":"# Load the datasets and combine them.\n## Heavily based off the work by xhulu","10a6f577":"### License Distribution","310a4d7e":"Not bad!\n\nSome article titles like '**Super-spreaders and the rate of transmission of the SARS virus**', '**Bioethical Implications of Globalization: An International Consortium Project of the European Commission**', and '**Infectious Disease Prevalence and Factors Associated with Upper Respiratory Infection in Cats Following Relocation**' all sound promising. \n\nThough I guess it's debatable how relevant the cat article really is.","1d1c4412":"### Only ~20% of papers are about Covid-19!","6a778b20":"### CZI and Biorxiv papers are only available for recent publications","fb9e39b8":"### References to Promising Drugs","90a066b9":"### Some abstracts have more than 10k words!","dd1df7e3":"### Pubmed makes up more of the abstracts\n### But Elsevier makes up more of the text","b68bc2f4":"### Paper text is missing ~28%\n### Abstract, Title, and Author mostly complete","93f8e9a8":"# Word counts","e6e69012":"# Data Cleaning\n### Duplicate papers","f2523925":"###  Medrxiv papers don't have publish times","29f77364":"# Basic Text Analysis","4e4e25ae":"### Papers date back to 1950s"}}