{"cell_type":{"d2769800":"code","25a55e6c":"code","ef5a0156":"code","47a7dfc6":"code","15d9c388":"code","6fbe5898":"code","13f2228b":"code","f279045c":"code","c732fccc":"code","b9e7f96f":"code","9086856c":"code","bcb29b5c":"code","1b13898f":"code","a8aab116":"code","0ea96313":"code","d8a4fcd2":"code","9142c059":"code","17bd7548":"code","238d90a8":"code","5966575c":"code","54339bba":"code","46f98956":"code","689c523b":"code","2c8abdd6":"code","d89dda97":"code","c284a9a1":"code","ee44bd6d":"code","17ba6eec":"markdown","6cac40de":"markdown","e4b7182d":"markdown","7f1edb05":"markdown","a4f45398":"markdown","538480da":"markdown","108e2cac":"markdown","352b66fd":"markdown","da0ec701":"markdown","2a2ebb99":"markdown","d5072e0d":"markdown","50b6ae01":"markdown","42971bbe":"markdown","0c9095e9":"markdown","fbb11334":"markdown","e0fa7010":"markdown","01cfc600":"markdown","50519c0e":"markdown","843c20fd":"markdown","9e415be0":"markdown","46954141":"markdown","153695e3":"markdown","ef600bdd":"markdown","1b0d2716":"markdown","51c27174":"markdown","762a39dd":"markdown","53a42b9e":"markdown","6eccdeae":"markdown","e7f278dd":"markdown","3e71f312":"markdown","15863a2a":"markdown","4711bc9c":"markdown","f35a32cd":"markdown","ce219d70":"markdown"},"source":{"d2769800":"# Imports.\n# Basic libraries\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport matplotlib.patches as patches\nimport matplotlib.colors\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\n# Sklearn\nfrom sklearn.metrics import roc_curve, auc, confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold, \\\n    GridSearchCV, cross_val_predict\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.preprocessing import MinMaxScaler, power_transform\nimport warnings","25a55e6c":"warnings.filterwarnings(action='ignore')\npd.options.display.max_columns = None","ef5a0156":"# Configure Matplotlib and seaborn\nplt.style.use('seaborn-muted')\nsns.set_palette(\"muted\")\nplt.rcParams['figure.figsize'] = (16,5);\nplt.rcParams['figure.facecolor'] = '#daf2e1'\nplt.rcParams['axes.facecolor'] = '#daf2e1'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['lines.linewidth'] = 5\nplt.rcParams['figure.titlesize'] = 30\nplt.rcParams['axes.titlesize'] = 25\nplt.rcParams['image.cmap']=cm.tab10\nplt.rcParams['font.family'] = 'serif'\nplt.rcParams['xtick.labelsize']=14\nplt.rcParams['ytick.labelsize']=14\ncmap = cm.tab10\nblue = cmap.colors[0]\nblue_hex = matplotlib.colors.to_hex(blue)\norange = cmap.colors[1]\norange_hex = matplotlib.colors.to_hex(orange)\ngreen = cmap.colors[2]\ngreen_hex = matplotlib.colors.to_hex(green)","47a7dfc6":"cmap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [blue, orange,\n                                                             green])\nmatplotlib.cm.register_cmap(\"mycolormap\", cmap)\ncpal = sns.color_palette(\"mycolormap\", n_colors=60)","15d9c388":"df = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv',\n                 index_col='id')","6fbe5898":"cmap2 = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", [orange, blue],\n                                                 N=2)\ndf_t = df.describe().T\nselected_features = ['area_worst', 'perimeter_worst', 'texture_worst',\n                     'radius_worst', 'fractal_dimension_worst',\n                     'symmetry_worst', 'Unnamed: 32']\nselected_columns =  ['count', 'mean', 'max']\ndf_stats = df_t.loc[selected_features, selected_columns]\ndf_stats.sort_values('mean', inplace=True, ascending=False)\ndf_stats.loc['diagnosis'] = [df['diagnosis'].count(), np.nan, np.nan]\ndf_stats['dtype'] = df[selected_features + ['diagnosis']].dtypes.astype('str')\ndef background_color(s):\n    is_special = s.isin([0, 'object'])\n    return [f'background-color: {orange_hex}' if v else '' for v in is_special]\nformat_dict = {selected_column: \"{:10,.2f}\" for selected_column in selected_columns}\ndf_stats.style.apply(background_color).format(format_dict)\\\n                 .bar(subset=[\"mean\"], color=green_hex)\\\n                 .bar(subset=[\"max\"], color=blue_hex)","13f2228b":"df.drop(['Unnamed: 32'], axis=1, inplace=True)\ndf.sort_index(axis=1, inplace=True)\n\nencoder = LabelEncoder()\ndf['diagnosis'] = encoder.fit_transform(df['diagnosis'])\nencoder_mapping = dict(zip(encoder.classes_,\n                           encoder.transform(encoder.classes_)))","f279045c":"def feature_distibutions(df, log=False):\n    columns = ['area_worst', 'concave points_worst', 'concavity_worst',\n               'perimeter_worst', 'radius_worst']\n    plot_columns = 2\n    plot_rows = int(np.ceil(len(columns) \/ plot_columns))\n    fig, axes = plt.subplots(plot_rows, plot_columns, figsize=(16, 8))\n    plot_axes = df[columns].hist(ax=axes.ravel()[:len(columns)], legend=True)\n    for ax in plot_axes.ravel():\n        ax.legend(prop={'size': 15})\n    df[columns].plot(kind='kde', secondary_y=True, subplots=True,\n                     ax=axes.ravel()[:len(columns)], color=orange, legend=False)\n    last_ax = axes.ravel()[-1]\n    last_ax.axis('off')\n    if log:\n        text = 'Feature distributions seem\\na bit closer to normal.'\n    else:\n        text = 'Feature distributions are\\nnot normal.'\n    last_ax.text(0.1, 0.4, text,\n           transform = last_ax.transAxes, size=25, fontweight='bold')\n    plt.suptitle('Feature distributions')\n    plt.tight_layout();\nfeature_distibutions(df)","c732fccc":"columns = ['area_worst', 'concave points_worst', 'concavity_worst',\n           'perimeter_worst', 'radius_worst']\nmalignant = df[df['diagnosis'] == 1][columns]\nbenign = df[df['diagnosis'] == 0][columns]\nplot_columns = 2\nplot_rows = int(np.ceil(len(columns) \/ plot_columns))\nfig, axes = plt.subplots(plot_rows, plot_columns, figsize=(16, 8))\nfor column, ax in zip(columns, axes.ravel()):\n    ax.hist(malignant[column], label='Malignant', density=True, alpha = 0.5,\n            color=blue)\n    ax.hist(benign[column], label='Benign', density=True, alpha = 0.5,\n            color=orange)\n    ax.legend(fontsize=14)\n    ax.set_title(column)\nplt.suptitle('Feature distributions of classes')\nlast_ax = axes.ravel()[-1]\nlast_ax.axis('off')\nlast_ax.text(0.1, 0.4, 'Features can separate classes \\nas there is little'\n                       ' overlap in\\nfeature distributions.',\n       transform=last_ax.transAxes, size=25, fontweight='bold')\nplt.tight_layout();","b9e7f96f":"def autolabel(rects, ax, decimals=2):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        value = round(height, decimals)\n        ax.annotate('{}'.format(value),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  size=18,# 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\ndiagnosis_count = df['diagnosis'].value_counts()\ndiagnosis_percent = \\\n    df['diagnosis'].value_counts(normalize=True).apply(round, args=(2,))\nfig, ax = plt.subplots(figsize=(16, 7))\nfig.suptitle('Dependent variable distribution')\nrects = ax.bar(diagnosis_count.index, diagnosis_count.values,\n               color=[blue, orange], zorder=3)\nax.set_xticks([0, 1])\nax.set_ylim(0, 400)\nax.set_xticklabels(['Benign', 'Malignant'])\nax.set_title(f'Benign: {diagnosis_percent.loc[0] * 100}%\\n'\n              f'Malignant: {diagnosis_percent.loc[1] * 100}%\\n', color=orange)\nax.text(0.65, 0.70, f'Imbalanced classes\\n',\n             transform = ax.transAxes, size=18, fontweight='bold')\nautolabel(rects, ax, decimals=2)\nplt.tight_layout();","9086856c":"def update_scores(score_df, modelname, score):\n    if modelname not in score_df['ModelName'].values:\n        model_dict = dict()\n        model_dict['ModelName'] = modelname\n        model_dict['Score'] = score\n        score_df = score_df.append(model_dict, ignore_index=True)\n    return score_df\n","bcb29b5c":"corr = df.corr()\nsorted_index = \\\n    corr['diagnosis'].apply(np.abs).sort_values(ascending=False).index\nsorted_corr = corr[sorted_index].reindex(sorted_index)\n\nmost_correlated_index = sorted_corr[(sorted_corr.iloc[:, 0] >= 0.7) |\n                                    (sorted_corr.iloc[:, 0] <= -0.7)].index\nsorted_corr = sorted_corr.loc[most_correlated_index, most_correlated_index]","1b13898f":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.heatmap(sorted_corr, ax=ax, annot=True, fmt='.2f',\n            cmap='YlGnBu',\n            cbar=False,\n            square=True)\nfig.suptitle('Most correlated features.')\nrect = patches.Rectangle((4, 4), 4.95, 4.95, linewidth=5, edgecolor=orange,\n                         facecolor='none')\nax.add_patch(rect)\nax.text(-0.4, -0.2, 'High correlation\\nwith dependent\\nvariable',\n       transform = ax.transAxes, size=15, fontweight='bold')\nrect = patches.Rectangle((0.05, 1), 1, 7.95, linewidth=5, edgecolor=orange,\n                         facecolor='none')\nax.add_patch(rect)\nax.text(1.05, 0.25, 'Features are\\nhighly correlated\\nwith each other',\n       transform = ax.transAxes, size=15, fontweight='bold')\nplt.tight_layout();","a8aab116":"columns = ['radius_worst', 'area_worst', 'concave points_worst',\n          'perimeter_worst', 'diagnosis']","0ea96313":"with sns.plotting_context(\"notebook\", font_scale=1.2):\n    axes = sns.pairplot(df[columns], hue='diagnosis', )\naxes.fig.suptitle('Pairplots for most correlated features')\naxes._legend.remove()\nplt.tight_layout();","d8a4fcd2":"X = df.drop('diagnosis', axis=1)\ny = df['diagnosis']\n\n# Standardization before using of PCA\nscaler = StandardScaler()\nX_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns,\n                        index=X.index)\npca = PCA(n_components=2)\nX_tr = pca.fit_transform(X_scaled, y)","9142c059":"fig, ax = plt.subplots(figsize=(16, 8))\nsns.scatterplot(X_tr[:, 0], X_tr[:, 1], hue=y, ax=ax, style=y, s=100)\nax.set_title('Two component PCA ')\nax.plot([-5, 10], [-7.4, 12.5], color=green)\nax.legend(['Example line', 'Malignant', 'Benign'], prop={'size': 20})\nax.set_ylim((-5, 10))\nax.set_xlim((-5, 10))\nax.text(0.75, 0.7, 'Classes are mostly\\nlinearly separable',\n       transform = ax.transAxes, size=20, fontweight='bold');","17bd7548":"def score_model(model, X_scored, y, scoring):\n\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    score = cross_val_score(model, X_scored, y, scoring=scoring,\n                            cv=skf)\n    return score\n\ndef compare_scores(score_1, score_2, score_name_1, score_name_2, description):\n    score_range = range(1, 6)\n    fig, ax = plt.subplots(figsize=(16, 8))\n    ax.plot(score_range, score_1, label=f'{score_name_1} CV')\n    ax.plot(score_range, score_2, label=f'{score_name_2} CV')\n    plt.suptitle('Logistic regression cross-validation recall')\n    ax.set_title('Metric: Recall', color=orange, size=25)\n    ax.set_ylim([0.85,1.05])\n    ax.set_xlim([0.80,5.5])\n    ax.set_xlabel('Results')\n    ax.set_ylabel('Score')\n    ax.set_xticks(score_range)\n    score_1_mean = score_1.mean()\n    ax.axhline(y=score_1_mean, linestyle='--', label=f'{score_name_1} mean',\n               alpha=0.5)\n    ax.annotate(format(score_1_mean, '.3f'), xy=(5, score_1_mean), size=20)\n    score_2_mean = score_2.mean()\n    ax.axhline(y=score_2_mean, linestyle='--', label=f'{score_name_2} mean',\n               color=orange, alpha=0.5)\n    ax.annotate(format(score_2_mean, '.3f'), xy=(5,score_2_mean), size=20)\n    ax.legend(loc='best', prop={'size': 20})\n    ax.text(0.3, 0.85, description,\n       transform = ax.transAxes, size=20, fontweight='bold')\n    ax.set_xlabel('Results', fontsize=18)\n    ax.set_ylabel('Score', fontsize=18)\n    plt.tight_layout();\n\nunscaled_recall = score_model(LogisticRegression(random_state=0), X, y,\n                              'recall')\nscaled_recall = score_model(LogisticRegression(random_state=0), X_scaled, y,\n                            'recall')\ndescription = 'Scaling improves model score'\ncompare_scores(unscaled_recall, scaled_recall, 'Unscaled', 'Scaled', description)","238d90a8":"# Create a DataFrame with scores\nscore_df = pd.DataFrame(columns=['ModelName', 'Score'])\nscore_df = update_scores(score_df, 'Baseline', unscaled_recall.mean())\nscore_df = update_scores(score_df, 'Scaled features', scaled_recall.mean())","5966575c":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\nrfecv = RFECV(LogisticRegression(random_state=0), cv=skf, scoring='recall').fit\\\n    (X_scaled, y)\nused_features = rfecv.get_support()\nX_scaled_rfe = X_scaled.loc[:, used_features]\nselected_recall = \\\n    score_model(LogisticRegression(random_state=0), X_scaled_rfe, y, 'recall')\ndescription = 'Feature selection\\nimproves model score'\ncompare_scores(scaled_recall, selected_recall, 'Scaled', 'Selected', description)\nscore_df = update_scores(score_df, 'Feature selection + Scaling',\n                       selected_recall.mean())","54339bba":"features = \\\n    pd.Series(rfecv.estimator_.coef_[0], index=X_scaled.columns[used_features])\nfeatures = features.apply(np.abs).sort_values(ascending=False)[:7]\n\nfig, ax = plt.subplots(figsize=(16, 7))\nrects = ax.bar(x=features.index, height=features,\n               zorder=2, color=[blue, orange, green])\nautolabel(rects, ax, decimals=2)\nplt.suptitle('Top 7 feature importances (absolute)')\nax.set_title(f'Before feature selection: {len(X_scaled.columns)} features\\n'\n             f'After feature selection: {len(X_scaled_rfe.columns)} '\n             f'features\\n',\n             color=orange)\nax.set_ylim(0.0, 1.40)\nax.tick_params(axis='both', which='major', labelsize=16)\nplt.tight_layout();","46f98956":"X_power_transform \\\n    = pd.DataFrame(power_transform(X_scaled_rfe, standardize=True),\n                   columns=X_scaled_rfe.columns, index=X_scaled_rfe.index)\nfeature_distibutions(X_power_transform, log=True)","689c523b":"power_transform_recall = \\\n    score_model(LogisticRegression(random_state=0), X_power_transform, y, 'recall')\ndescription = 'Power transform does\\nnot improve model score'\ncompare_scores(selected_recall, power_transform_recall, 'Selected',\n               'Power transform', description);\nscore_df = update_scores(score_df, 'Power transform', power_transform_recall\n                         .mean())","2c8abdd6":"grid = {'penalty': ['l2', 'l1'],\n        'solver': ['lbfgs', 'liblinear', 'sag', 'saga'],\n        'C': [0.0001, 0.01, 0.01, 0.1, 0.8, 1, 1.2, 10, 100, 1000],\n        }\n\ngrid_search =\\\n    GridSearchCV(estimator=LogisticRegression(random_state=0, n_jobs=-1),\n                 param_grid=grid, scoring='recall', n_jobs=-1,\n                 cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=0))\ngrid_search.fit(X_scaled_rfe, y)\nbest_params = grid_search.best_params_\ngrid_search_score = \\\n    score_model(LogisticRegression(n_jobs=-1, random_state=0, **best_params),\n                X_scaled_rfe, y, scoring='recall')\ndescription = 'Grid search shows the same results\\n'\ncompare_scores(selected_recall, grid_search_score, 'Selected',\n               'Power transform', description);\ntransformed_dict = {'ModelName': 'Grid search',\n               'Score': grid_search.best_score_}","d89dda97":"def results_plot(model, X, y):\n    fig = plt.figure(figsize=(16, 8))\n    layout = (1, 2)\n    cm_ax = plt.subplot2grid(layout, (0, 0))\n    roc_ax = plt.subplot2grid(layout, (0, 1))\n\n    scoring_metrics = ['accuracy', 'recall', 'precision']\n    title = ''\n    for metric in scoring_metrics:\n        score = score_model(model, X_scaled_rfe, y, scoring=metric).mean()\n        title += f'{metric.capitalize()}: {round(score, 4)}   '\n    plt.suptitle('Results\\n\\n')\n    fig.text(0.25, 0.85, title,\n             transform=fig.transFigure, size=20, fontweight='bold', color=orange)\n    predictions = cross_val_predict(model, X, y, cv=skf)\n    conf = confusion_matrix(y, predictions)\n    sns.heatmap(conf, annot=True, fmt='.0f', ax=cm_ax,\n                cmap=cpal,\n                xticklabels=['Benign', 'Malignant'],\n                annot_kws={'fontsize': 20})\n    cm_ax.set_yticklabels(['Benign', 'Malignant'], va='center')\n    cm_ax.set_xlabel('Predicted', size=20)\n    cm_ax.set_ylabel('Actual', size=20)\n    cm_ax.set_title('Confusion matrix')\n\n    y_probs = cross_val_predict(LogisticRegression(random_state=0), X_scaled_rfe, y,\n                          method='predict_proba', cv=skf)[:, 1]\n    fpr, tpr, thresholds = roc_curve(y, y_probs)\n    roc_auc = auc(fpr, tpr)\n    roc_ax.plot(fpr, tpr, label=f'Logistic regression (AUC: {round(roc_auc, 3)})')\n    roc_ax.plot([0, 1], [0, 1], label='Guessing')\n    roc_ax.set_title('ROC Curve')\n    roc_ax.set_xlabel('False Positive Rate', size=20)\n    roc_ax.set_ylabel('True Positive Rate', size=20)\n    roc_ax.legend(loc='best', prop={'size': 15})\n    plt.tight_layout();\n\nmodel = LogisticRegression(random_state=0)\nresults_plot(model, X_scaled_rfe, y)","c284a9a1":"skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\ny_pred = cross_val_predict(model, X_scaled_rfe, y, cv=skf)\ntrue_positive_mask = ((y == y_pred) & (y.isin([1])))\ntrue_negative_mask = ((y == y_pred) & (y.isin([0])))\nfalse_positive_mask = ((y != y_pred) & (y.isin([0])))\nfalse_negative_mask = ((y != y_pred) & (y.isin([1])))\n\nfig, ax = plt.subplots(figsize=(16, 8))\npca = PCA(n_components=2)\nX_pca = pd.DataFrame(pca.fit_transform(X_scaled_rfe, y),\n                    columns=['PCA1', 'PCA2'], index=X_scaled_rfe.index)\n\nax.scatter(X_pca.loc[true_positive_mask, 'PCA1'],\n           X_pca.loc[true_positive_mask, 'PCA2'],\n           color=blue, marker='.', label='True positives', s=120)\nax.scatter(X_pca.loc[true_negative_mask, 'PCA1'],\n           X_pca.loc[true_negative_mask, 'PCA2'],\n           color=orange, marker='.', label='True negatives', s=120)\nax.scatter(X_pca.loc[false_positive_mask, 'PCA1'],\n           X_pca.loc[false_positive_mask, 'PCA2'],\n           color=green, marker='X', label='False positives', s=200)\nax.scatter(X_pca.loc[false_negative_mask, 'PCA1'],\n           X_pca.loc[false_negative_mask, 'PCA2'],\n           c='black', marker='X', label='False negatives', s=200)\nax.text(0.05, 0.10, 'Most of the false negatives\\n'\n                    'are close to the true\\n'\n                    'negatives population',\n        transform = ax.transAxes, size=15, fontweight='bold')\nax.set_title('PCA Prediction analysis')\nax.set_xlim((-5, 10))\nax.set_xlabel('PCA 1', fontsize=18)\nax.set_ylabel('PCA 2', fontsize=18)\nax.legend(prop={'size': 15});\n","ee44bd6d":"fig, ax = plt.subplots(figsize=(16, 7))\nscore_df.sort_values(['Score'], inplace=True, ascending=False)\nrects = ax.bar(x=score_df['ModelName'], height=score_df['Score'],\n               zorder=2, color=[ blue, orange, green])\nautolabel(rects, ax, decimals=4)\nplt.suptitle('Scores')\nax.set_title('Metric: Recall', color=orange)\nax.set_ylim(0.0, 1.40)\nax.text(0.03, 0.8, 'Best feature engineering',\n        transform = ax.transAxes, size=17, fontweight='bold')\nax.tick_params(axis='both', which='major', labelsize=16)\nplt.tight_layout();","17ba6eec":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 6 results:<\/h2>\n\n### 1. **Logistic Regression shows pretty good results**: we get over 91% of the Malignant cases right with raw features.\n### 2. **Model fitted with scaled features performs better** than model with raw  features. Recall raises from 91,5% to 96%.","6cac40de":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Get overall statistics of DataFrame (selected features).<\/h3>","e4b7182d":"<h1 style=\"text-align: center; color:#01872A; font-size: 80px;\nbackground:#daf2e1; border-radius: 20px;\n\">Simple <br>  Breast Cancer analysis.<\/h1>\n<h2 style=\"padding: 10px; text-align: center; color:#01872A; font-size: 40px;\nbackground:#daf2e1; border-radius: 20px;\n\">Contents<\/h2>\n\n## 1. [Overview of data.](#step1)\n## 2. [Univariate analysis of features](#Step2)\n## 3. [Bivariate analysis](#Step3)\n## 4. [Model selection.](#Step4)\n## 5. [Choosing metric.](#Step5)\n## 6. [Fitting model.](#Step6)\n## 7. [Feature engineering.](#Step7)\n## 8. [Hyperparameter tuning.](#Step8)\n## 9. [Metrics analysis.](#Step9)\n## 10. [Conclusion.](#Step10)","7f1edb05":"<div id=\"Step7\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 7. Improving performance with feature engineering.<\/h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">To improve performance we can try following ideas:<\/h3>\n\n### 1. Feature selection techniques.\n### 2. Power transform the data to make distributions more Gaussian.\n","a4f45398":"<div id=\"Step4\">\n<\/div>\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 4. Model selection.<\/h2>\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">To choose between linear and non-liner models perform a check if the\nproblem may be solved with a linear model with PCA with 2 components.<\/h3>\n","538480da":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 8 results:<\/h2>\n\n### 1. **GridSearch CV doesn't make the results better** with current model.","108e2cac":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 9 results:<\/h2>\n\n### 1. **Most of false negative predictions seem to be very close to negative value**  population, so it seems to be impossible to improve recall with current feature set and current model.\n### 2. **Overall results are good enough with current model**.","352b66fd":"### 1. Logistic regression shows good results on this dataset even if used on raw features.\n### 2. Feature selection + feature scaling allows to create a prediction which is sufficiently better than a baseline.","da0ec701":"<div id=\"Step10\">\n<\/div>","2a2ebb99":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 1 results:<\/h2>\n\n### 1. **Feature 'Unnamed: 32' should be removed** as it contains only empty values.\n### 2. **Dependent variable is a categorical one (type: object)**, so we need to encode it. The M(malignant) label should be 1 and B(benign) should be 0.\n### 3. There are features that have different magnitude (mean values differ, for example, from 0.08 to 880), so we will **need to do scaling for linear models**.\n### 4. There are no missing values, so **no imputing needed**.\n### 5. The dataset is very small, so no need to divide into the train\/validation\/test sets, for scoring **standard 5 fold cross-validation** will be used.","d5072e0d":"<div id=\"Step3\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 3. Bivariate analysis <br> (analysis of pairs of feature).<\/h2>\n","50b6ae01":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Feature selection based on feature importance.","42971bbe":"<div id=\"Step2\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 2. Univariate analysis <br> (Individual feature statistics).<\/h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Histogram plots of selected features.<\/h3>","0c9095e9":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1;border-radius: 20px;\n\">Step 4 results:<\/h2>\n\n### 1. Data is **mostly linearly separable**, so linear models could be used. To  keep it simple, basic linear model will be used: LogisticRegression.\n### 2. Linear models can probably achieve **good results**.","fbb11334":"<div id=\"step1\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 10. Conclusion.<\/h2>\n","e0fa7010":"<div id=\"step1\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 1. Overview of data.<\/h2>\n","01cfc600":"<div id=\"Step9\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 9. Metrics analysis.<\/h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Basic statistics, confusion matrix, ROC Curve.<\/h3>","50519c0e":"<div id=\"Step8\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 8. Improving performance with hyperparameter tuning.<\/h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Define the grid and use GridSearch to find the best parameters.<\/h3>\n","843c20fd":"<div id=\"Step5\">\n<\/div>\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 5. Choosing metric.<\/h2>","9e415be0":"<div id=\"Step6\">\n<\/div>\n\n<h2 style=\"padding: 10px; background:#daf2e1; border-radius: 20px;\ntext-align: center; color:#01872A; font-size: 40px;\">\nStep 6. Fitting model.<\/h2>\n\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Create model and check its results with both scaled and unscaled data.<\/h3>","46954141":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Pairplots plots for most correlated features.<\/h3>\n","153695e3":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Compare feature distributions of both classes.<\/h3>\n","ef600bdd":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 7 results:<\/h2>\n\n### 1. **Feature selection improves score** from 96% to 97% and reduces dataset from  30 features to 23 features.\n### 2. **Power transform doesn't give any score improvements**.\n","1b0d2716":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Power transform the data<\/h3>\n","51c27174":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Check dependent variable distributions.<\/h3>","762a39dd":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Further analysis.<\/h3>\n\n### 1. **Use non-linear models**: K-Nearest Neighbours, Random Forest, SVM, Gradient Boosting.\n### 2. **Use wider feature engineering** to distinguish False Negatives.","53a42b9e":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Prediction analysis.<\/h3>","6eccdeae":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Create the correlation DataFrame and sort it according to the 'Diagnosis'\ncorrelation by absolute values. Choose only correlations with dependent variable with 0.7 or higher or with\n -0.7 or lower.<\/h3>","e7f278dd":"<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">As a metric I will use recall, where:<\/h3>\n$\n\\ \\huge{Recall} = \\frac {True\\ Positives} {True\\ Positives\\ +\\ False\\ Negatives}\n$\n<h3 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 20px;\n\">Recall shows how good we can identify positive (M(malignant)) cases and immediately start responding to such serious diagnosis.<\/h3>\n","3e71f312":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 5 results:<\/h2>\n\n## 1. **Appropriate** metric selected for model evaluation (Recall).","15863a2a":"<h2 style=\"padding: 30px; text-align: center; color:#01872A; font-size: 40px;\nbackground:#daf2e1; border-radius: 20px;\">\nThank you for reading.<br> Any feedback is highly appreciated. <\/h2>","4711bc9c":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 2 results:<\/h2>\n\n### 1. **The features can distinguish classes**, e.g. 'radius_worst' feature  has very little overlap between classes.\n### 2. **We got some skewed distributions**, so it is possible to try to change distributions to improve scoring (power transform).\n### 3. **The dependent variable distribution is imbalanced**, so stratification will be useful during cross-validation to balance classes.","f35a32cd":"<p align=\"center\">\n<img align=\"center\" src=\"https:\/\/image.freepik.com\/free-photo\/physician-noting-down-symptoms-patient_53876-63308.jpg\">\n\n<\/p>\n\n[Image source](https:\/\/www.freepik.com\/photos\/medical)","ce219d70":"<h2 style=\"padding: 10px; text-align: left; color:#01872A; font-size: 30px;\nfont-style: italic; background:#daf2e1; border-radius: 20px;\n\">Step 3 results:<\/h2>\n\n### 1. **There are some strong feature correlations with dependent variable**. At the same time those features are correlated with each other, e.g. 'concave_points_mean'  and 'concave_points_worst', so **feature selection algorithms can be used**.\n### 2. **All the top correlated features are positively correlated** with dependent  variable. The higher value of independent features, more likely the dependent  feature will be 1."}}