{"cell_type":{"50640660":"code","34fb350d":"code","e998104a":"code","af6861b9":"code","88296ce3":"code","5da7035a":"code","006162d3":"code","1105e876":"code","5ad30e3c":"code","209ca2c3":"code","cf666cb5":"code","8506890e":"code","291a0666":"code","47072b44":"code","c3ffe020":"code","d98970bf":"code","09bca96a":"code","6d0cb353":"code","6ab130cf":"code","f766c3f6":"code","d0f6f9a9":"code","ec5fffed":"code","497b8ec1":"code","d7126862":"code","596ca448":"code","da2eef2a":"code","8cbb41dd":"code","dacc7749":"code","1f5bac9f":"code","76526b5b":"code","caeb7d20":"code","3a16fc0b":"markdown","ab178740":"markdown","ee343d0e":"markdown","e73c0987":"markdown","49a78d58":"markdown","2a9d8a5d":"markdown","80e360d0":"markdown","274e4a84":"markdown","6b2c25a7":"markdown"},"source":{"50640660":"#import our read and manipulation library \nimport pandas as pd ","34fb350d":"df = pd.read_csv(\"..\/input\/coleridgeinitiative-show-us-the-data\/train.csv\")\ndf.head()","e998104a":"#inspect the dtypes\ndf.info()","af6861b9":"#feature engineering \ndf['pub_title_count'] = df['pub_title'].apply(\n    lambda x: len(x.strip().split()))  # word count in abstract\ndf.head()","88296ce3":"#remove possible duplicates\ndf.drop_duplicates(['pub_title'], inplace=True)\ndf['pub_title'].describe(include='all')","5da7035a":"#examine data\ndf.head(10)","006162d3":"#drop NA's \ndf.dropna(subset=['pub_title'],inplace=True)\ndf.info()","1105e876":"df.isnull().sum()","5ad30e3c":"#subset and check the head one last time\ndf = df[['pub_title','pub_title_count']]\ndf.head()","209ca2c3":"#vectorize the data\nfrom sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text,maxx_features):\n    vectorizer = TfidfVectorizer(max_features = maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X ","cf666cb5":"#ensure search is converted to string, no punctuation and lower case \ndf['pub_title']=df['pub_title'].astype(str)\ndf[\"pub_title\"] = df['pub_title'].str.replace('[^\\w\\s]','')\ndf[\"pub_title\"] = df['pub_title'].str.lower()\ndf.head(10)","8506890e":"#tfid vectorize the column \ntext = df['pub_title'].values\nX = vectorize(text,2**12)\nX.shape","291a0666":"#PCA to reduce dimensions and k-means clustering \nfrom sklearn.decomposition import PCA \n\npca = PCA(n_components=.95,random_state=42)\nX_reduced = pca.fit_transform(X.toarray())\nX_reduced.shape","47072b44":"from sklearn.cluster import KMeans","c3ffe020":"from sklearn import metrics\nimport numpy as np\nfrom scipy.spatial.distance import cdist \n\ndistortions = []\nK = range(2,20)\n\nfor k in K: \n    k_means = KMeans(n_clusters=k,random_state=42).fit(X_reduced)\n    k_means.fit(X_reduced)\n    distortions.append(sum(np.min(cdist(X_reduced,k_means.cluster_centers_,'euclidean'),axis=1))\/X.shape[0])","d98970bf":"import matplotlib.pyplot as plt \nX_line = [K[0],K[-1]]\nY_line = [distortions[0],distortions[-1]]\n\n#plot the elbow \nplt.plot(K,distortions,'b-')\nplt.plot(X_line,Y_line,'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method Showing Optimal K')\nplt.show()","09bca96a":"k = 8\nkmeans = KMeans(n_clusters = k, random_state=42)\ny_pred = kmeans.fit_predict(X_reduced)\ndf['y'] = y_pred","6d0cb353":"from sklearn.manifold import TSNE \n\ntsne = TSNE(verbose=1,perplexity=100,random_state=42)\nX_embedded = tsne.fit_transform(X.toarray())","6ab130cf":"from matplotlib import pyplot as plt \nimport seaborn as sns \n\n#sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n#colors\npalette = sns.color_palette(\"bright\",1)\n\n#plot\nsns.scatterplot(X_embedded[:,0],X_embedded[:,1],palette=palette)\nplt.title('t-SNE with no labels')\nplt.savefig('t-sne_kmeans_academic')\nplt.show()","f766c3f6":"from matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(15,15)})\n\n# colors\npalette = sns.hls_palette(k, l=.4, s=.9)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=y_pred, legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"improved_cluster_tsne.png\")\nplt.show()","d0f6f9a9":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer ","ec5fffed":"vectorizers = []\n\nfor i in range(0,k):\n    vectorizers.append(CountVectorizer(min_df=5,max_df=.9,stop_words='english',lowercase=True))","497b8ec1":"vectorized_data = []\n\nfor current_cluster,cvec in enumerate(vectorizers):\n    try:\n        vectorized_data.append(cvec.fit_transform(df.loc[df['y']==current_cluster,'pub_title']))\n    except Exception as e:\n        print(\"Not enough instances in cluster: \" + str(current_cluster))\n        vectorized_data.append(None)","d7126862":"len(vectorized_data)","596ca448":"num_topics_per_cluster = k \n\nlda_models = []\nfor i in range(0,k):\n    lda = LatentDirichletAllocation(n_components = num_topics_per_cluster, max_iter = 10, learning_method = 'online',verbose=False,random_state=1)\n    lda_models.append(lda)\n    \nlda_models[0]","da2eef2a":"clusters_lda_data = []\n\nfor current_cluster, lda in enumerate(lda_models):\n    if vectorized_data[current_cluster] != None:\n        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))","8cbb41dd":"def selected_topics(model,vectorizer,top_n=3):\n    current_words = []\n    keywords = []\n    \n    for idx, topic in enumerate(model.components_):\n        words = [(vectorizer.get_feature_names()[i],topic[i]) for i in topic.argsort()[:-top_n - 1:1]]\n        for word in words:\n            if word[0] not in current_words:\n                keywords.append(word)\n                current_words.append(word[0])\n                \n    keywords.sort(key = lambda x:x[1])\n    keywords.reverse()\n    return_values = []\n    for i in keywords:\n        return_values.append(i[0])\n    return return_values ","dacc7749":"all_keywords = []\n\nfor current_vectorizer,lda in enumerate(lda_models):\n    if vectorized_data[current_vectorizer] != None:\n        all_keywords.append(selected_topics(lda,vectorizers[current_vectorizer]))","1f5bac9f":"all_keywords","76526b5b":"df['kmeans'] = y_pred","caeb7d20":"df.to_csv(\"train_w_clusters.csv\")","3a16fc0b":"## Detect Topics for each Cluster","ab178740":"### Add Clusters to Data ","ee343d0e":"This notebook starts by reading in the data and quickly looking at the data quality. After, we vectorize the titles of the paper and run k-means clustering. The visual below is a t-SNE representation colored by each cluster. ","e73c0987":"![image.png](attachment:image.png)","49a78d58":"### K-Means ","2a9d8a5d":"## Read the Data","80e360d0":"## Cluster Analysis","274e4a84":"# Clustering Titles with Theme Detection ","6b2c25a7":"Last, we detect common themes in the clusters and save the file. "}}