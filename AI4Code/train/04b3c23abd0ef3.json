{"cell_type":{"aac4a4c3":"code","f41e6ef9":"code","e89da6c5":"code","d1946cdc":"code","46432ae0":"code","96e7fdc2":"code","b906e942":"code","208eb6cf":"code","84d17151":"code","a799e1cb":"code","696a8194":"code","a8f343c6":"code","f42a266f":"code","76afd638":"code","233bc860":"code","f7dd6acb":"code","a03c84c0":"code","20db2864":"code","6170234e":"code","b3cabb3b":"code","df23c8e1":"code","b85f8af1":"code","049b8e25":"code","c5fed490":"code","61347a23":"code","5cc1e3b3":"code","9ad62fad":"code","a6d9beca":"code","dc358a16":"code","ed323655":"code","b05cce8b":"code","57eb62e6":"markdown","98ddd7f6":"markdown","8cb8e406":"markdown","7978c214":"markdown","d60aef79":"markdown","51270147":"markdown","60579ee8":"markdown","94c102a0":"markdown","300d70fa":"markdown","0775f9f7":"markdown","9d3e760b":"markdown","aefc41ba":"markdown","5b9c1727":"markdown","56c1a7c5":"markdown","cd91e973":"markdown","def7457f":"markdown","8f407491":"markdown","914e3653":"markdown","c41a66fa":"markdown","b43d3874":"markdown","8a07f1a9":"markdown"},"source":{"aac4a4c3":"%%time\n\nimport numpy as np\n\nimages = np.load(\"\/kaggle\/input\/brain-tumor\/brain_tumor_dataset\/images.npy\", allow_pickle=True)\nmasks = np.load(\"\/kaggle\/input\/brain-tumor\/brain_tumor_dataset\/masks.npy\", allow_pickle=True)\nlabels = np.load(\"\/kaggle\/input\/brain-tumor\/brain_tumor_dataset\/labels.npy\")\ninteger_to_class = {1: 'meningioma', 2: 'glioma', 3: 'pituitary tumor'}\n\nprint(f\"images:{images.shape}, \\\nmasks:{masks.shape}, \\\nlabels:{labels.shape}\")","f41e6ef9":"data = np.column_stack((images, masks, labels))\ndata.shape","e89da6c5":"from sklearn.model_selection import train_test_split\n\ntrain_data, val_data = train_test_split(data, test_size=0.08, random_state=42)\ntrain_data, test_data = train_test_split(train_data, test_size=0.12, random_state=42)\n\nprint(\"Train:\", train_data.shape,\n      \"\\nVal:\", val_data.shape, \n      \"\\nTest:\", test_data.shape,)","d1946cdc":"import matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")\n\n# https:\/\/www.kaggle.com\/awsaf49\/brain-tumor-visualization\/data\n\nlabels, counts = np.unique(data[:,2], return_counts=True)\n\nplt.figure(figsize=(10,6))\nplt.bar(labels, counts, color=[\"aqua\", \"violet\", \"greenyellow\"],\n        tick_label=['Meningioma(1)', 'Glioma(2)', 'Pituitary Tumor(3)'])\n\n\n# Annotate\nfor row, value in zip(labels,counts):\n    plt.annotate(int(value), xy=(row, value-150), \n                rotation=0, color=\"black\", \n                ha=\"center\", verticalalignment='bottom', \n                fontsize=15, fontweight=\"bold\")","46432ae0":"import cv2\n\ndef data_to_viz(data, label, n=5):\n    \n    # logical slice for receiving data with the expected label\n    expected_index = np.where(data[:,2] == label)\n    expected_data = data[expected_index]\n    \n    # n random samples\n    index = np.random.choice(expected_data.shape[0], n, replace=False)\n    data_to_viz = expected_data[index]\n    \n    imgs = []\n    masks = []\n    labels = []\n    for data_i in data_to_viz:\n        \n        # img\n        imgs.append(cv2.resize(data_i[0], (512, 512)))\n\n        # mask\n        masks.append(cv2.resize(data_i[1].astype(\"uint8\"), \n                                (512, 512)))\n\n        # label\n        labels.append(data_i[2])\n\n    return np.hstack(imgs), np.hstack(masks), labels","96e7fdc2":"meningiomas_imgs, meningiomas_masks, meningiomas_labels = data_to_viz(data, label=1, n=5)\nglioma_imgs, glioma_masks, glioma_labels  = data_to_viz(data, label=2, n=5)\ntumor_imgs, tumor_masks, tumor_labels = data_to_viz(data, label=3, n=5)\n\nprint(\"Meningiomas:\",\n      meningiomas_imgs.shape, meningiomas_masks.shape, meningiomas_labels)\nprint(\"Glioma:\",\n      glioma_imgs.shape, glioma_masks.shape, glioma_labels)\nprint(\"Pituitary Tumor:\",\n      tumor_imgs.shape, tumor_masks.shape, tumor_labels)","b906e942":"# Data to visualization\nfrom mpl_toolkits.axes_grid1 import ImageGrid\n\n# Plot\nfig = plt.figure(figsize=(25., 25.))\ngrid = ImageGrid(fig, 111,  # similar to subplot(111)\n                 nrows_ncols=(3, 1),  # creates 1x4 grid of axes\n                 axes_pad=0.1,  # pad between axes in inch.\n                 )\n\n\ngrid[0].imshow(meningiomas_imgs, cmap=\"bone\")\ngrid[0].imshow(np.ma.masked_where(meningiomas_masks == False, \n                                  meningiomas_masks), cmap='rainbow', alpha=0.3)\n\ngrid[0].set_title(\"Meningiomas\", fontsize=20)\ngrid[0].axis(\"off\")\n\ngrid[1].imshow(glioma_imgs, cmap=\"bone\")\ngrid[1].imshow(np.ma.masked_where(glioma_masks == False,\n                                  glioma_masks), cmap='rainbow', alpha=0.3)\ngrid[1].set_title(\"Glioma\", fontsize=20)\ngrid[1].axis(\"off\")\n\ngrid[2].imshow(tumor_imgs, cmap=\"bone\")\ngrid[2].imshow(np.ma.masked_where(tumor_masks == False,\n                                  tumor_masks), cmap='rainbow', alpha=0.3)\n\ngrid[2].set_title(\"Pituitary Tumor\", fontsize=20)\ngrid[2].axis(\"off\")\n\n\n# annotations\nplt.suptitle(\"Brain MRI Images for Brain Tumor Detection\\nBrainTumorRetrieval Dataset\",\n             y=.80, fontsize=30, weight=\"bold\")\n\n# save and show\nplt.savefig(\"dataset.png\", pad_inches=0.2, transparent=True)\nplt.show()","208eb6cf":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensor\n\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","84d17151":"class BrainMriDataset(Dataset):\n    def __init__(self, data, transforms, n_classes=3):\n        \n        self.data = data\n        self.transforms = transforms\n        self.n_classes = n_classes\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n\n        image = self.data[idx][0].astype(\"float32\")\n\n        # global standardization of pixels\n        mean, std = image.mean(), image.std()\n        image = (image - mean) \/ std\n        \n        # convert to rgb\n        image_rgb = np.stack([image]*3).transpose(1,2,0)\n        \n        # create target masks\n        label = self.data[idx][2] -1\n        mask = np.expand_dims(self.data[idx][1], -1)\n        \n        target_mask = np.zeros((mask.shape[0], mask.shape[1], \n                                self.n_classes))\n        target_mask[:,:, label : label + 1] = mask.astype(\"uint8\")\n        \n        #  binary mask\n        target_mask = np.clip(target_mask, 0, 1).astype(\"float32\")\n        \n        # augmentations\n        augmented = self.transforms(image=image_rgb, \n                                    mask=target_mask)\n        image = augmented['image']\n        mask = augmented['mask']\n        \n        return image, mask","a799e1cb":"transforms = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(scale_limit=0.5, rotate_limit=0, shift_limit=0.1, p=0.5, \n                       border_mode=0),\n                        \n    A.GridDistortion(p=0.5),\n    A.OpticalDistortion(p=0.5, distort_limit=2, shift_limit=0.5),\n    A.Resize(156, 156, p=1.),\n    A.RandomCrop(128, 128, p=1.)\n    ])","696a8194":"# train\ntrain_dataset = BrainMriDataset(data=train_data, transforms=transforms)\ntrain_dataloader = DataLoader(train_dataset, batch_size=16, num_workers=4, \n                              shuffle=True)\n\n# validation\nval_dataset = BrainMriDataset(data=val_data, transforms=transforms)\nval_dataloader = DataLoader(val_dataset, batch_size=16, num_workers=4, \n                            shuffle=True)\n\n# test\ntest_dataset = BrainMriDataset(data=test_data, transforms=transforms)\ntest_dataloader = DataLoader(test_dataset, batch_size=16, num_workers=4, \n                             shuffle=True)","a8f343c6":"def show_aug(inputs, nrows=3, ncols=5, image=True):\n    plt.figure(figsize=(10, 10))\n    plt.subplots_adjust(wspace=0., hspace=0.)\n    i_ = 0\n    \n    if len(inputs) > 15:\n        inputs = inputs[:15]\n        \n    for idx in range(len(inputs)):\n    \n        # normalization\n        if image is True:           \n            img = inputs[idx].numpy()#.transpose(1,2,0)\n            #mean = [0.485, 0.456, 0.406]\n            #std = [0.229, 0.224, 0.225] \n            #img = (img*std+mean).astype(np.float32)\n            #img = np.clip(img, 0,1)\n        else:\n            img = inputs[idx].numpy().astype(np.float32)\n            img = img[0,:,:]\n        \n        #plot\n        #print(img.max(), len(np.unique(img)), img.mean())\n        plt.subplot(nrows, ncols, i_+1)\n        plt.imshow(img); \n        plt.axis('off')\n \n        i_ += 1\n        \n    return plt.show()\n\n    \nimages, masks = next(iter(train_dataloader))\nprint(images.shape, masks.shape)\n\nshow_aug(images)\nshow_aug(masks)\n","f42a266f":"from torchvision.models import resnext50_32x4d\n\nclass ConvRelu(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel, padding):\n        super().__init__()\n\n        self.convrelu = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n            nn.ReLU(inplace=True)\n        )\n\n    def forward(self, x):\n        x = self.convrelu(x)\n        return x\n\nclass DecoderBlock(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super().__init__()\n        \n        self.conv1 = ConvRelu(in_channels, in_channels \/\/ 4, 1, 0)\n        \n        self.deconv = nn.ConvTranspose2d(in_channels \/\/ 4, in_channels \/\/ 4, kernel_size=4,\n                                          stride=2, padding=1, output_padding=0)\n        \n        self.conv2 = ConvRelu(in_channels \/\/ 4, out_channels, 1, 0)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.deconv(x)\n        x = self.conv2(x)\n\n        return x","76afd638":"class ResNeXtUNet(nn.Module):\n\n    def __init__(self, n_classes):\n        super().__init__()\n        \n        self.base_model = resnext50_32x4d(pretrained=True)\n        self.base_layers = list(self.base_model.children())\n        filters = [4*64, 4*128, 4*256, 4*512]\n        \n        # Down\n        self.encoder0 = nn.Sequential(*self.base_layers[:3])\n        self.encoder1 = nn.Sequential(*self.base_layers[4])\n        self.encoder2 = nn.Sequential(*self.base_layers[5])\n        self.encoder3 = nn.Sequential(*self.base_layers[6])\n        self.encoder4 = nn.Sequential(*self.base_layers[7])\n\n        # Up\n        self.decoder4 = DecoderBlock(filters[3], filters[2])\n        self.decoder3 = DecoderBlock(filters[2], filters[1])\n        self.decoder2 = DecoderBlock(filters[1], filters[0])\n        self.decoder1 = DecoderBlock(filters[0], filters[0])\n\n        # Final Classifier\n        self.last_conv0 = ConvRelu(256, 128, 3, 1)\n        self.last_conv1 = nn.Conv2d(128, n_classes, 3, padding=1)\n                       \n        \n    def forward(self, x):\n        # Down\n        x = self.encoder0(x)\n        e1 = self.encoder1(x)\n        e2 = self.encoder2(e1)\n        e3 = self.encoder3(e2)\n        e4 = self.encoder4(e3)\n\n        # Up + sc\n        d4 = self.decoder4(e4) + e3\n        d3 = self.decoder3(d4) + e2\n        d2 = self.decoder2(d3) + e1\n        d1 = self.decoder1(d2)\n        #print(d1.shape)\n\n        # final classifier\n        out = self.last_conv0(d1)\n        out = self.last_conv1(out)\n        out = torch.sigmoid(out)\n        \n        return out\n","233bc860":"def dice_coef_metric(inputs, target):\n    intersection = 2.0 * (target * inputs).sum()\n    union = target.sum() + inputs.sum()\n    if target.sum() == 0 and inputs.sum() == 0:\n        return 1.0\n\n    return intersection \/ union\n\ndef dice_coef_loss(inputs, target):\n    num = target.size(0)\n    inputs = inputs.reshape(num, -1)\n    target = target.reshape(num, -1)\n    smooth = 1.0\n    intersection = (inputs * target)\n    dice = (2. * intersection.sum(1) + smooth) \/ (inputs.sum(1) + target.sum(1) + smooth)\n    dice = 1 - dice.sum() \/ num\n    return dice\n\ndef bce_dice_loss(inputs, target):\n    dicescore = dice_coef_loss(inputs, target)\n    bcescore = nn.BCELoss()\n    bceloss = bcescore(inputs, target)\n\n    return bceloss + dicescore","f7dd6acb":"model = ResNeXtUNet(n_classes=3).to(device)\nadam = torch.optim.Adam(model.parameters(), lr=1e-3)\nscheduler = torch.optim.lr_scheduler.StepLR(adam, step_size=10, gamma=0.1)","a03c84c0":"def train_one_epoch(model, optimizer, lr_scheduler, data_loader, epoch):\n    print(\"Start Train ...\")\n    model.train()\n\n    losses = []\n    accur = []\n\n    for data, target in data_loader:\n\n        data = data.permute(0,3,1,2).to(device)\n        targets = target.permute(0,3,1,2).to(device)\n\n        outputs = model(data)\n\n        out_cut = np.copy(outputs.data.cpu().numpy())\n        out_cut[np.nonzero(out_cut < 0.5)] = 0.0\n        out_cut[np.nonzero(out_cut >= 0.5)] = 1.0\n\n        train_dice = dice_coef_metric(out_cut, targets.data.cpu().numpy())\n\n        loss = bce_dice_loss(outputs, targets)\n\n        losses.append(loss.item())\n        accur.append(train_dice)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n    if lr_scheduler is not None:\n        lr_scheduler.step()\n\n    print(\"Epoch [%d]\" % (epoch))\n    print(\"Mean loss on train:\", np.array(losses).mean(), \"Mean DICE on train:\", np.array(accur).mean())\n\n    return np.array(losses).mean(), np.array(accur).mean()","20db2864":"def val_epoch(model, data_loader_valid, epoch, threshold=0.3):\n    if epoch is None:\n        print(\"Test Start...\")\n    else:\n        print(\"Start Validation ...\")\n\n    model.eval()\n    val_acc = []\n\n    with torch.no_grad():\n        for data, targets in data_loader_valid:\n\n            data = data.permute(0,3,1,2).to(device)\n            targets = targets.permute(0,3,1,2).to(device)\n\n            outputs = model(data)\n\n            out_cut = np.copy(outputs.data.cpu().numpy())\n            out_cut[np.nonzero(out_cut < threshold)] = 0.0\n            out_cut[np.nonzero(out_cut >= threshold)] = 1.0\n\n            val_dice = dice_coef_metric(out_cut, targets.data.cpu().numpy())\n            val_acc.append(val_dice)\n\n        print(\"Epoch:  \" + str(epoch) + \"  Threshold:  \" + str(threshold)\\\n              + \" Mean Validation DICE Score:\", np.array(val_acc).mean())\n        \n        return np.array(val_acc).mean()","6170234e":"from tqdm import trange\nimport os\nimport glob\n\nweights_dir = \"weights\"\nif os.path.exists(weights_dir) == False:\n    os.mkdir(weights_dir)\n\nnum_epochs = 30\nloss_history = []\ntrain_dice_history = []\nval_dice_history = []\n\nfor epoch in trange(num_epochs):\n    loss, train_dice = train_one_epoch(model, adam, scheduler, \n                                       train_dataloader, epoch)\n    \n    val_dice = valscore = val_epoch(model, val_dataloader, epoch)\n\n    # train history\n    loss_history.append(loss)\n    train_dice_history.append(train_dice)\n    val_dice_history.append(val_dice)\n\n    # save best weights\n    best_dice = max(val_dice_history)\n    if val_dice >= best_dice:\n        torch.save({'state_dict': model.state_dict()},\n                   os.path.join(weights_dir, f\"{val_dice:0.5f}_.pth\"))\n","b3cabb3b":" # Dirty tricks\n\"\"\" with torch.no_grad():\n        for data, targets in data_loader_valid:\n            data = data.permute(0,3,1,2).to(device)\n            outputs = model(data)\n\n\nmodel.eval()\nfor m in model.modules():\n    if isinstance(m, nn.BatchNorm2d):\n     m.track_runing_stats=False\"\"\";","df23c8e1":"# Load the best weights\nbest_weights =  sorted(glob.glob(weights_dir + \"\/*\"),\n                       key= lambda x: x[8:-5])[-1]\ncheckpoint = torch.load(best_weights)\nmodel.load_state_dict(checkpoint['state_dict'])\n\nprint(f'Loaded model: {best_weights.split(\"\/\")[1]}')","b85f8af1":"def plot_model_history(train_history,\n                       val_history,\n                       loss_history ,\n                       num_epochs):\n    \n    x = np.arange(num_epochs)\n\n    fig = plt.figure(figsize=(10, 6))\n    plt.plot(x, train_history, label='train dice', lw=3, c=\"springgreen\")\n    plt.plot(x, val_history, label='validation dice', lw=3, c=\"deeppink\")\n    plt.plot(x, loss_history, label='dice + bce', lw=3)\n\n    plt.xlabel(\"Epoch\", fontsize=15)\n    plt.ylabel(\"DICE\", fontsize=15)\n    plt.legend()\n\n    return plt.show()","049b8e25":"plot_model_history(train_dice_history, val_dice_history, loss_history, num_epochs)","c5fed490":"test_iou = val_epoch(model, test_dataloader, epoch=None, threshold=0.5)\nprint(f\"\"\"Mean IoU of the test images - {np.around(test_iou, 2)*100}%\"\"\")","61347a23":"dices = []\nthresholds = [0.1, 0.2, 0.33, 0.4, 0.5, 0.6, 0.7, 0.8, 0.85, 0.88]\nfor i in thresholds:\n    test_dice = val_epoch(model, test_dataloader,threshold=i, epoch=None)\n    dices.append(test_dice)","5cc1e3b3":"import random\nimport matplotlib.colors as mcolors\n\ncolors = random.choices(list(mcolors.CSS4_COLORS.keys()),k=len(thresholds))\n\nplt.figure(figsize=(10,6))\nplt.bar(thresholds, dices, width=0.05, color=colors)\nplt.ylabel(\"Dice\", fontsize=15)\nplt.xlabel(\"Threshold values\", fontsize=15)\nplt.title(\"Global IoU with different thresholds\", fontsize=15)\n\n\n# Annotate\nfor row, value in zip(thresholds, dices):\n    plt.annotate(f\"{value*100:0.2f}%\", xy=(row, value), \n                 rotation=0, color=\"white\", \n                 ha=\"center\", verticalalignment='bottom', \n                 fontsize=10, fontweight=\"bold\")","9ad62fad":"test_predictions = []\ntest_ground_truths = []\nfor data, target in test_dataloader:\n    with torch.no_grad():\n        data = data.permute(0,3,1,2).to(device)\n        target = target.permute(0,3,1,2)\n        prediction = model(data)\n        test_predictions.append(prediction.detach().cpu())\n        test_ground_truths.append(target)","a6d9beca":"test_predictions = torch.cat(test_predictions)\ntest_ground_truths = torch.cat(test_ground_truths)\n#test_predictions = test_predictions.reshape(test_predictions.shape[0], -1)\n#test_ground_truths = test_ground_truths.reshape(test_ground_truths.shape[0], -1)\n\nprint(test_predictions.shape, test_ground_truths.shape)","dc358a16":"# data\ndice1 = dice_coef_metric(test_predictions[:,0,:,:], test_ground_truths[:,0,:,:])\ndice2 = dice_coef_metric(test_predictions[:,1,:,:], test_ground_truths[:,1,:,:])\ndice3 = dice_coef_metric(test_predictions[:,2,:,:], test_ground_truths[:,2,:,:])\ndices = [dice1, dice2, dice3]\n\n# x, y\nx = np.arange(3)\ndices = [dice1, dice2, dice3]\n\n# plot\nplt.figure(figsize=(10, 6))\nplt.bar(x, dices, \n        color=[\"aqua\", \"violet\", \"greenyellow\"], width=0.5)\n\n                                        \nplt.xticks(x, ['Meningioma(1)', 'Glioma(2)', 'Pituitary Tumor(3)'], fontsize=15)\nplt.ylabel(\"Dice\", fontsize=15)\nplt.title(\"Dice for each class\", fontsize=15)\n\n\n# Annotate\nfor row, value in zip(x, dices):\n    plt.annotate(f\"{value*100:0.3f}%\", xy=(row, value), \n                 rotation=0, color=\"white\", \n                 ha=\"center\", verticalalignment='bottom', \n                 fontsize=10, fontweight=\"bold\")\n    \nplt.show()","ed323655":"index = np.random.choice(test_data.shape[0], 1, replace=False)\n\n# image\nimage = test_data[index][0][0]\n\n# global standardization of pixels\nmean, std = image.mean(), image.std()\nimage = (image - mean) \/ std  \nimage = cv2.resize(image, (128, 128))\n# convert to rgb\nimage = np.stack([image]*3).transpose(1,2,0)\n\n# mask\nmask = test_data[index][0][1]\n\n# label\nlabel = test_data[index][0][2]\n\nprint(image.shape, mask.shape, label)","b05cce8b":"#----------- Data -------------#\n\n# predictions\npreds = torch.tensor(image.astype(np.float32)).unsqueeze(0).permute(0,3,1,2)\npreds = model(preds.to(device))\npreds = preds.detach().cpu().numpy()\n\n# threshold\npreds[np.nonzero(preds < 0.4)] = 0.0\npreds[np.nonzero(preds >= 0.4)] = 255.#1.0\npreds = preds.astype(\"uint8\")\n\npred_1 = preds[:,0,:,:]\npred_2 = preds[:,1,:,:]\npred_3 = preds[:,2,:,:]\n\n\n#------------ Plot ------------#\n\n# data plot\nfig, ax = plt.subplots(nrows=1,  ncols=2, figsize=(10, 10))\n\nax[0].imshow(image)\nax[0].set_title(\"Image\")\nax[1].imshow(mask)\nax[1].set_title(f'Ground Truth with label \"{integer_to_class[label].capitalize()}\"')\n#ax[1].imshow(preds[0,:,:,:])\n#ax[0].set_title(\"Preiction\")\nplt.suptitle(\"Random Test Sample\",\n             y=.75, fontsize=20, weight=\"bold\")\n\n# prediction plot\nfig, ax = plt.subplots(nrows=1,  ncols=3, figsize=(10, 10))\n\nax[0].imshow(pred_1[0,:,:])\nax[0].set_title(f'{integer_to_class[1].capitalize()}')\nax[1].imshow(pred_2[0,:,:])\nax[1].set_title(f'{integer_to_class[2].capitalize()}')\nax[2].imshow(pred_3[0,:,:])\nax[2].set_title(f'{integer_to_class[3].capitalize()}')","57eb62e6":"### Train history","98ddd7f6":"# Train Model","8cb8e406":"Split data on train val test","7978c214":"# What does the data look like?","d60aef79":"### Global IoU with different thresholds","51270147":"# Datataset and DataGenerator","60579ee8":"### Data Generators","94c102a0":"Plot","300d70fa":"# Test prediction","0775f9f7":"# Metric and Loss","9d3e760b":"Since the net did not reach a plateau, batch norm layers did not accumulate stable statistics; therefore, saved model weights in the early steps of the train loop - shows worse results, how to fix it? reach a plateau or go forward several epochs with ```torch.no_grad_():``` (dirty tricks) before saving weights or before switching the model to eval mode ```model.eval()``` for the weights that are.","aefc41ba":"# UNet","5b9c1727":"### IoU for each class\n","56c1a7c5":"### Samples of images of each class","cd91e973":"### This kernel is fork of [this](https:\/\/www.kaggle.com\/bonhart\/brain-mri-data-visualization-unet-fpn#DataGenerator-and-Data-Augmentation) kernel.\n\n### Steps:\n+ Data Preparation\n+ Visualization data\n+ Datataset and DataGenerator\n+ UNet\n+ Train model\n+ Test predictions","def7457f":"### Class distribution","8f407491":"### Data Transformation","914e3653":"# Data","c41a66fa":"Data","b43d3874":"### Random test sample","8a07f1a9":"Stacking rows as a data frame."}}