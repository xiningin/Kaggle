{"cell_type":{"593f06fb":"code","0e77e329":"code","5c3f67de":"code","5d67c0d7":"code","20a1e011":"code","0cbfc3a2":"code","2e10592b":"code","58f9ba9c":"code","ce120738":"code","89346b4a":"code","1a1b2159":"code","9bd41f88":"code","87fa2032":"markdown","017706b2":"markdown","ae65aa9b":"markdown","fbde362f":"markdown","8302714b":"markdown","3f1f34ae":"markdown","73c0941a":"markdown","3765a65d":"markdown","6c3da634":"markdown","be57934d":"markdown","d89099eb":"markdown","6c7a3698":"markdown"},"source":{"593f06fb":"# Import libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nimport warnings\nwarnings.filterwarnings('ignore')","0e77e329":"# Load dataset\n# we don't need the header because it contains meaningless names\ncc_apps = pd.read_csv('..\/input\/credit-card-dataset\/cc_approvals.data', header=None)\n\n# Inspect data\ncc_apps.head()","5c3f67de":"# Print summary statistics\ncc_apps_description = cc_apps.describe()\nprint(cc_apps_description)\n\nprint(\"\\n\")\n\n# Print DataFrame information\ncc_apps_info = cc_apps.info()\nprint(cc_apps_info)\n\nprint(\"\\n\")\n\n# Inspect missing values in the dataset\ncc_apps.tail(n=17)","5d67c0d7":"# Inspect missing values in the dataset\n# cc_apps[(cc_apps == '?').any(axis=1)]\nprint(cc_apps.tail(n=17))\n\n# Replace the '?'s with NaN\ncc_apps = cc_apps.replace({'?': np.nan})\n\n# Inspect the missing values again\nprint(cc_apps.tail(n=17))","20a1e011":"# Impute the missing values with mean imputation\ncc_apps.fillna(cc_apps.mean(), inplace=True)\n\n# Count the number of NaNs in the dataset to verify\ncc_apps.isnull().sum()","0cbfc3a2":"# Iterate over each column of cc_apps\nfor col in cc_apps:\n    # Check if the column is of object type\n    if cc_apps[col].dtype == 'object':\n        # Impute with the most frequent value\n        cc_apps = cc_apps.fillna(cc_apps[col].value_counts().index[0])\n\n# Count the number of NaNs in the dataset and print the counts to verify\ncc_apps.isnull().sum()","2e10592b":"# Instantiate LabelEncoder\nle = LabelEncoder()\n\n# Iterate over all the values of each column and extract their dtypes\nfor col in cc_apps:\n    # Compare if the dtype is object\n    if cc_apps[col].dtype=='object':\n    # Use LabelEncoder to do the numeric transformation\n        cc_apps[col]=le.fit_transform(cc_apps[col])","58f9ba9c":"# Drop the features 11 and 13 and convert the DataFrame to a NumPy array\ncc_apps = cc_apps.drop([11, 13], axis=1)\ncc_apps = cc_apps.to_numpy()\n\n# Segregate features and labels into separate variables\nX,y = cc_apps[:,0:13] , cc_apps[:,13]\n\n# Split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","ce120738":"# Import MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Instantiate MinMaxScaler and use it to rescale X_train and X_test\nscaler = MinMaxScaler(feature_range=(0, 1))\nrescaledX_train = scaler.fit_transform(X_train)\nrescaledX_test = scaler.transform(X_test)","89346b4a":"# Instantiate a LogisticRegression classifier with default parameter values\nlogreg = LogisticRegression()\n\n# Fit logreg to the train set\nlogreg.fit(rescaledX_train, y_train)","1a1b2159":"# Use logreg to predict instances from the test set and store it\ny_pred = logreg.predict(rescaledX_test)\n\n# Get the accuracy score of logreg model and print it\nprint(\"Accuracy of logistic regression classifier: \", logreg.score(rescaledX_test, y_test))\n\n# Print the confusion matrix of the logreg model\nprint(\"Confusion Matrix: \\n\", confusion_matrix(y_test, y_pred))","9bd41f88":"# Define the grid of values for tol and max_iter\ntol = [0.01, 0.001, 0.0001]\nmax_iter = [100, 150, 200]\n\n# Create a dictionary where tol and max_iter are keys and the lists of their values are corresponding values\nparam_grid = dict(tol=tol, max_iter=max_iter)\n\n# Instantiate GridSearchCV with the required parameters\ngrid_model = GridSearchCV(estimator=logreg, param_grid=param_grid, cv=5)\n\n# Use scaler to rescale X and assign it to rescaledX\nrescaledX = scaler.fit_transform(X)\n\n# Fit data to grid_model\ngrid_model_result = grid_model.fit(rescaledX, y)\n\n# Summarize results\nbest_score, best_params = grid_model_result.best_score_, grid_model_result.best_params_\nprint(\"Best: %f using %s\" % (best_score, best_params))","87fa2032":"We can see column 0,1,3,4,5,6,13 still have missing values and they all are non numeric values. Mean imputation don't apply on that so we need a different fix for these columns. These columns are categorical so we will use most frequent values in replace of missing.","017706b2":"We gave so much importance to missing values because it can lead to machine learning model heavily. We will use mean imputation to resolve missing values.","ae65aa9b":"# Introduction\nCommerical banks receive a lot of application for credit card and many of them rejected because of reasons like high loan balances and low income. If we analyze these applications manually this can lead to error-prone and also time consuming, So with the power of machine learning we can automate these tasks and most of the banks does now a days.\n\nWe will develop an credit approval predicator using machine learning techniques, just like the banks do","fbde362f":"Now we converted non numerical to numerical, we will split our data into training and testing which is essential for machine learning modeling. We don't need features like DriversLicense and ZipCode because they are not useful to predict credit card approvals. We only include best features to design our machine learning model and drop other. In Data Science theory this is referred as feature selection.","8302714b":"# Clean the data\nWe have 4 columns consist of numerical data ranges differently (2, 7, 10, 14). \n1. fix ? label and change to NaN.\n2. Use mean imputation for numerical columns\n3. Use most often values in categorical columns","3f1f34ae":"# Building a logistic regression model\nTo predict application will be approved or disapproved is a type of classification problem.","73c0941a":"# Explore the data\nThe output of data is very confusing, to understand the data we can read this [blog](http:\/\/rstudio-pubs-static.s3.amazonaws.com\/73039_9946de135c0a49daa7a0a9eda4a67a72.html) which give us a overview of features. The typical features include in the credit card data are Gender, Age, Debt, Married, BankCustomer, EducationLevel, Ethnicity, YearsEmployed, PriorDefault, Employed, CreditScore, DriversLicense, Citizen, ZipCode, Income and ApprovalStatus. Now we can map these features to our columns in the output. By looking at the dataset it gives numerical and also non-numerical values, so we need to learn more about data to resolve issue in our dataset.","3765a65d":"After spliting data into training and testing sets, we will scale our features before fitting into machine learning model. We will scale our features between the range of 0 and 1 and 1 is highest trustworthy person to be considered for credit card approval.","6c3da634":"# Preprocessing the data\nThe missing values are successful resolved but still need for data preprocessing, so we proceed towards our machine learning model. There are 3 main tasks in preprocessing,\n1. Convert non-numerical data to numeric\n2. Split the data into train and testing sets\n3. Use scaling to uniform feature values\n\nWe can convert our non-numeric to numeri using a technique called label encoding","be57934d":"# Make model more efficent\nOur model predict 84% correctly. In confusion matrix, the first element of first row gives 94 TP (True-Positive) means our model predicts approved application correctly and second element of second row gives 98 TN (True-Negative) means our model predicts denied application also correctly. For making our model more efficent, we use GridSearch to tune hyperparameters.  ","d89099eb":"# Prediction and Evaluating\nNow we will predict our data and also ensure how well it performed. We will measure it's accuracy using classification accuracy and confusion matrix. It tells you what your algorithm did right and what it did wrong.","6c7a3698":"# Import the dataset\nWe'll use the Credit Card Approval dataset from the [UCI Machine Learning Repository](http:\/\/archive.ics.uci.edu\/ml\/datasets\/credit+approval). Import and view the dataset. The contributor changes all attribute names and values to meaningless symbols to protect confidentiality of the data."}}