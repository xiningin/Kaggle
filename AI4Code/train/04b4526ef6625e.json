{"cell_type":{"96692db2":"code","84bb9733":"code","871247fc":"code","f67156aa":"code","ec69ad30":"code","1359a226":"code","81932046":"code","3f184b0a":"code","56db15f1":"code","d083956b":"code","acdccaca":"code","651ee911":"code","29ed5b1d":"code","635cab99":"markdown","9a6b7b84":"markdown","9b2d41f3":"markdown","329dd324":"markdown","54b7eff2":"markdown","f92c6d05":"markdown"},"source":{"96692db2":"# Carga de librer\u00edas\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Carga de datos\n\ndata_train = pd.read_csv('..\/input\/titanic-solution-a-beginners-guide\/train.csv')\ndata_test = pd.read_csv('..\/input\/titanic-solution-a-beginners-guide\/test.csv')\n\nprint(len(data_train))\n\ndata_train.sample(3)","84bb9733":"# Visualizaci\u00f3n de datos\n\n# Es crucial para reconocer los patrones subyacentes que hay que explotar en el modelo.\n# Gr\u00e1fico de barras donde seg\u00fan clase de embarque y sexo se muestra % supervivencia\n\nsns.barplot(x=\"Embarked\", y=\"Survived\", hue=\"Sex\", data=data_train);","871247fc":"sns.pointplot(x=\"Pclass\", y=\"Survived\", hue=\"Sex\", data=data_train,\n              palette={\"male\": \"blue\", \"female\": \"pink\"},\n              markers=[\"*\", \"o\"], linestyles=[\"-\", \"--\"]);","f67156aa":"def simplify_ages(df):\n    df.Age = df.Age.fillna(-0.5)\n    bins = (-1, 0, 5, 12, 18, 25, 35, 60, 120)\n    group_names = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\n    categories = pd.cut(df.Age, bins, labels=group_names)\n    df.Age = categories\n    return df\n\ndef simplify_cabins(df):\n    df.Cabin = df.Cabin.fillna('N')\n    df.Cabin = df.Cabin.apply(lambda x: x[0])\n    return df\n\ndef simplify_fares(df):\n    df.Fare = df.Fare.fillna(-0.5)\n    bins = (-1, 0, 8, 15, 31, 1000)\n    group_names = ['Unknown', '1_quartile', '2_quartile', '3_quartile', '4_quartile']\n    categories = pd.cut(df.Fare, bins, labels=group_names)\n    df.Fare = categories\n    return df\n\ndef format_name(df):\n    df['Lname'] = df.Name.apply(lambda x: x.split(' ')[0])\n    df['NamePrefix'] = df.Name.apply(lambda x: x.split(' ')[1])\n    return df    \n    \ndef drop_features(df):\n    return df.drop(['Ticket', 'Name', 'Embarked'], axis=1)\n\ndef transform_features(df):\n    df = simplify_ages(df)\n    df = simplify_cabins(df)\n    df = simplify_fares(df)\n    df = format_name(df)\n    df = drop_features(df)\n    return df\n\ndata_train = transform_features(data_train)\ndata_test = transform_features(data_test)\n\ndata_train.head()","ec69ad30":"sns.barplot(x=\"Age\", y=\"Survived\", hue=\"Sex\", data=data_train);","1359a226":"sns.barplot(x=\"Cabin\", y=\"Survived\", hue=\"Sex\", data=data_train);","81932046":"sns.barplot(x=\"Fare\", y=\"Survived\", hue=\"Sex\", data=data_train);","3f184b0a":"from sklearn import preprocessing\n\ndef encode_features(df_train, df_test):\n    features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n    df_combined = pd.concat([df_train[features], df_test[features]])\n    for feature in features:\n        le = preprocessing.LabelEncoder()\n        le = le.fit(df_combined[feature])\n        df_train[feature] = le.transform(df_train[feature])\n        df_test[feature] = le.transform(df_test[feature])\n    return df_train, df_test\n    \ndata_train, data_test = encode_features(data_train, data_test)\ndata_train.head()","56db15f1":"from sklearn.model_selection import train_test_split\n\nX_all = data_train.drop(['Survived', 'PassengerId'], axis=1)\ny_all = data_train['Survived']\n\nnum_test = 0.20\nX_train, X_test, y_train, y_test = train_test_split(X_all, y_all, test_size=num_test, random_state=23)","d083956b":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\n\n# Choose the type of classifier. \nclf = RandomForestClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)\ngrid_obj = grid_obj.fit(X_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(X_train, y_train)","acdccaca":"predictions = clf.predict(X_test)\n\nprint(accuracy_score(y_test, predictions))","651ee911":"\nfrom sklearn.model_selection import KFold\n\ndef run_kfold(clf):\n    kf = KFold(n_splits=10)\n    outcomes = []\n    fold = 0\n    for train_index, test_index in kf.split(X_all):\n        fold += 1\n        X_train, X_test = X_all.values[train_index], X_all.values[test_index]\n        y_train, y_test = y_all.values[train_index], y_all.values[test_index]\n        clf.fit(X_train, y_train)\n        predictions = clf.predict(X_test)\n        accuracy = accuracy_score(y_test, predictions)\n        outcomes.append(accuracy)\n        print(\"Fold {0} accuracy: {1}\".format(fold, accuracy))\n    mean_outcome = np.mean(outcomes)\n    print(\"Mean Accuracy: {0}\".format(mean_outcome))\n\nrun_kfold(clf)\n","29ed5b1d":"# Predecir los datos reales de la prueba\n\n# Y ahora llega el momento de la verdad. Haz las predicciones,\n\nids = data_test['PassengerId']\npredictions = clf.predict(data_test.drop('PassengerId', axis=1))\n\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\n# output.to_csv('titanic-predictions.csv', index = False)\n\noutput.head()","635cab99":"**Validar con KFold**\n\n\u00bfEs realmente bueno este modelo? Es \u00fatil verificar la eficacia del algoritmo utilizando KFold. Para ello, dividiremos nuestros datos en 10 cubos y ejecutaremos el algoritmo utilizando un cubo diferente como conjunto de prueba para cada iteraci\u00f3n.","9a6b7b84":"**Caracter\u00edsticas de transformaci\u00f3n**\n\n* Adem\u00e1s del \"Sexo\", la caracter\u00edstica \"Edad\" es la segunda en importancia. Para evitar el sobreajuste, agrupo a las personas en grupos l\u00f3gicos de edad humana.\n\n* Cada cabina comienza con una letra. Apuesto a que esta letra es mucho m\u00e1s importante que el n\u00famero que le sigue, vamos a cortarla.\n\n* La tarifa es otro valor continuo que debe ser simplificado. Ejecut\u00e9 data_train.Fare.describe() para obtener la distribuci\u00f3n de la caracter\u00edstica, y luego los coloqu\u00e9 en bines de cuartiles en consecuencia.\n\n\n* Extraiga la informaci\u00f3n de la caracter\u00edstica 'Nombre'. En lugar de utilizar el nombre completo, extraje el apellido y el prefijo del nombre (Sr., Sra., etc.) y los a\u00f1ad\u00ed como caracter\u00edsticas propias.\n\n* Por \u00faltimo, elimine las caracter\u00edsticas in\u00fatiles. (Billete y Nombre)\n","9b2d41f3":"**Dividir los datos de entrenamiento**\n\nHa llegado el momento del aprendizaje autom\u00e1tico. En primer lugar, separamos las caracter\u00edsticas (X) de las etiquetas (y).\n\n* X_all: Todas las caracter\u00edsticas menos el valor que queremos predecir (Sobrevivido).\n\n* y_all: S\u00f3lo el valor que queremos predecir.\n\nEn segundo lugar, utilizar Scikit-learn para barajar aleatoriamente estos datos en cuatro variables. En este caso, estoy entrenando el 80% de los datos, y luego probando contra el otro 20%. M\u00e1s tarde, estos datos se reorganizar\u00e1n en un patr\u00f3n KFold para validar la eficacia de un algoritmo entrenado.\n","329dd324":"**Algunas codificaciones finales**\n\nLa \u00faltima parte de la fase de preprocesamiento consiste en normalizar las etiquetas. El **LabelEncoder **en Scikit-learn convertir\u00e1 cada valor de cadena \u00fanico en un n\u00famero, haciendo que nuestros datos sean m\u00e1s flexibles para varios algoritmos.\n\nEl resultado es una tabla de n\u00fameros que da miedo a los humanos, pero que es hermosa para las m\u00e1quinas.","54b7eff2":"# Aprendizaje autom\u00e1tico con Scikit-Learn - Caso Titanic\n\nOriginal en https:\/\/www.kaggle.com\/jeffd23\/scikit-learn-ml-from-start-to-finish\/notebook\n\nSe pasa de trabajar con datos en bruto a una precisi\u00f3n de al menos el 78% en el conjunto de datos de los supervivientes del Titanic.\n\n**Pasos cubiertos**\n\n* Importaci\u00f3n de datos a un dataframe\n* Visualizaci\u00f3n de datos\n* Lipieza y trannsformaci\u00f3n de datos\n* Codificaci\u00f3n de datos\n* Armado de grupo de entrenamiento y prueba\n* Afinaci\u00f3n del algoritmo\n* Validaci\u00f3n cruzada con h KFold\n","f92c6d05":"**Ajustar y afinar un algoritmo**\n\nAhora es el momento de averiguar qu\u00e9 algoritmo va a ofrecer el mejor modelo. Yo voy con el RandomForestClassifier, pero puedes dejar cualquier otro clasificador aqu\u00ed, como Support Vector Machines o Naive Bayes."}}