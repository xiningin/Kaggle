{"cell_type":{"a9b185ee":"code","31384785":"code","745a78f1":"code","709f55c8":"code","bebd5a8f":"code","d74e297a":"code","25972b15":"code","31af80d0":"code","0ff27ea1":"code","8582ddd2":"code","29b2476a":"code","69318176":"code","8cda8fce":"code","a4d53ad7":"code","fd1b17fd":"code","af522d7f":"code","5cac1ca5":"code","b555b5ab":"code","45fd7e03":"code","5bd2ba8a":"code","64dea54e":"code","0f97c703":"code","aba7399c":"code","e9e59208":"code","a6a60cc3":"code","00574422":"code","f094a1f4":"code","cd4f19b2":"markdown","f992cc98":"markdown"},"source":{"a9b185ee":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","31384785":"!pip install --upgrade pytorch_lightning","745a78f1":"import argparse\nimport glob\nimport os\nimport json\nimport time\nimport logging\nimport random\nimport re\nfrom sklearn.model_selection import train_test_split\nimport shutil\nfrom string import punctuation\n\nimport pandas as pd\nimport numpy as np\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nimport pytorch_lightning as pl\nfrom transformers import AdamW, T5ForConditionalGeneration, T5Tokenizer, get_linear_schedule_with_warmup","709f55c8":"print(\"is GPU available: \", torch.cuda.is_available())\nprint(pl.__version__)","bebd5a8f":"with open('..\/input\/news-data\/news_with_ner_predictions.json', encoding='utf-8') as f:\n    news_data = json.load(f)\n\nnews_items_data = []\ndoc_cnt = 0\nneg_doc_cnt = 0\n# Interested only in Organizations & Persons\nvalid_ner_tags = ['I-ORG', 'B-ORG', 'I-PER', 'B-PER']\n\nfor item in news_data:\n    temp_item = {}\n    temp_item[\"text\"] = item[\"text\"]\n    temp_item[\"labels\"] = item[\"labels\"]\n    temp_item[\"token_size\"] = item[\"token_size\"]\n    temp_item[\"predicted_ner\"] = []\n    \n    for ner_item in item['predicted_ner']:\n        if (ner_item[1] in valid_ner_tags) and \\\n        (ner_item[0].startswith(\"##\") == False) and \\\n        (len(ner_item[0]) > 1):\n            \n            dup_flag = 0\n            for val, _ in temp_item[\"predicted_ner\"]:\n                if val.strip() == ner_item[0].strip():\n                    dup_flag = 1\n                    break\n    \n            if dup_flag == 0:\n                temp_item[\"predicted_ner\"].append(ner_item)\n            \n    if len(temp_item[\"predicted_ner\"]) > 0:\n        doc_cnt += 1\n        if len(temp_item[\"labels\"]) > 0:\n            neg_doc_cnt += 1\n        news_items_data.append(temp_item)\n        \nprint(\"Total paragraphs which has valid NER tags: \", doc_cnt)\nprint(\"Total paragraphs which has negative labels: \", neg_doc_cnt)\n\n# Setting random seed and undersample the unrelated class\nnp.random.seed(99)\nfinal_news_data = []\nfor item in news_items_data:\n    if len(item[\"labels\"]) > 0 or (np.random.rand() > 0.72):\n        final_news_data.append(item)\nprint(\"Number of samples used for modelling: \", len(final_news_data))\nfinal_news = pd.DataFrame(final_news_data)\nfinal_news.head()","d74e297a":"final_news[\"label_type\"] = final_news[\"labels\"].map(lambda x: len(x) > 0).astype(int)\nneg_idx_list = final_news[final_news[\"label_type\"] == 1].index.tolist()\npos_idx_list = final_news[final_news[\"label_type\"] == 0].index.tolist()\n\nnp.random.seed(99)\nnp.random.shuffle(neg_idx_list)\nnp.random.shuffle(pos_idx_list)\n\nval_idx_list = neg_idx_list[-1 * round(len(neg_idx_list) * 0.04):] + \\\n            pos_idx_list[-1 * round(len(pos_idx_list) * 0.05):]\nfinal_news[\"split\"] = ['val' if i in val_idx_list else 'train' for i in range(final_news.shape[0])]\n\n#!ls -l ..\/input\n#!rmdir final_news_data\n!mkdir final_news_data\n#final_news[final_news[\"split\"] == 'train'].to_csv(\".\/final_news_data\/train.json\", index=False)\n#final_news[final_news[\"split\"] == 'val'].to_csv(\".\/final_news_data\/val.json\", index=False)\nfinal_news[final_news[\"split\"] == 'train'].to_json(\".\/final_news_data\/train.json\", orient='records')\nfinal_news[final_news[\"split\"] == 'val'].to_json(\".\/final_news_data\/val.json\", orient='records')","25972b15":"class T5FineTuner(pl.LightningModule):\n    \n    def __init__(self, hparams):\n        \n        super(T5FineTuner, self).__init__()\n        self.hparams = hparams\n        self.model = T5ForConditionalGeneration.from_pretrained(hparams.model_name_or_path)\n        self.tokenizer = T5Tokenizer.from_pretrained(hparams.tokenizer_name_or_path)\n  \n    def is_logger(self):\n        return self.trainer.proc_rank <= 0\n  \n    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, \\\n                  decoder_attention_mask=None, lm_labels=None):\n        \n        return self.model(input_ids,attention_mask=attention_mask, \\\n                          decoder_input_ids=decoder_input_ids, \\\n                          decoder_attention_mask=decoder_attention_mask, \\\n                          lm_labels=lm_labels)\n    \n    def _step(self, batch):\n        lm_labels = batch[\"target_ids\"]\n        lm_labels[lm_labels[:, :] == self.tokenizer.pad_token_id] = -100\n\n        outputs = self(input_ids=batch[\"source_ids\"], attention_mask=batch[\"source_mask\"], \\\n                       lm_labels=lm_labels, decoder_attention_mask=batch['target_mask'])\n        loss = outputs[0]\n        return loss\n\n    def training_step(self, batch, batch_idx):\n        loss = self._step(batch)\n\n        tensorboard_logs = {\"train_loss\": loss}\n        return {\"loss\": loss, \"log\": tensorboard_logs}\n    \n    def training_epoch_end(self, outputs):\n        avg_train_loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"avg_train_loss\": avg_train_loss}\n        return {\"avg_train_loss\": avg_train_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def validation_step(self, batch, batch_idx):\n        loss = self._step(batch)\n        return {\"val_loss\": loss}\n  \n    def validation_epoch_end(self, outputs):\n        avg_loss = torch.stack([x[\"val_loss\"] for x in outputs]).mean()\n        tensorboard_logs = {\"val_loss\": avg_loss}\n        return {\"avg_val_loss\": avg_loss, \"log\": tensorboard_logs, 'progress_bar': tensorboard_logs}\n\n    def configure_optimizers(self):\n        \"Prepare optimizer and schedule (linear warmup and decay)\"\n\n        model = self.model\n        no_decay = [\"bias\", \"LayerNorm.weight\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n                \"weight_decay\": self.hparams.weight_decay,\n            },\n            {\n                \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n                \"weight_decay\": 0.0,\n            }]\n        \n        optimizer = AdamW(optimizer_grouped_parameters, \\\n                          lr=self.hparams.learning_rate, \\\n                          eps=self.hparams.adam_epsilon)\n        self.opt = optimizer\n        return [optimizer]\n    \n    def optimizer_step(self, current_epoch, batch_idx, optimizer, optimizer_idx, \\\n                       second_order_closure=None, on_tpu=False, using_native_amp=False, \\\n                       using_lbfgs=False):\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        self.lr_scheduler.step()\n        \n  \n    def get_tqdm_dict(self):\n        tqdm_dict = {\"loss\": \"{:.3f}\".format(self.trainer.avg_loss), \"lr\": self.lr_scheduler.get_last_lr()[-1]}\n        return tqdm_dict\n\n    def train_dataloader(self):\n        train_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"train\", args=self.hparams)\n        dataloader = DataLoader(train_dataset, batch_size=self.hparams.train_batch_size, \\\n                                drop_last=True, shuffle=True, num_workers=4)\n        t_total = (\n            (len(dataloader.dataset) \/\/ (self.hparams.train_batch_size * max(1, self.hparams.n_gpu)))\n            \/\/ self.hparams.gradient_accumulation_steps\n            * float(self.hparams.num_train_epochs))\n        \n        print(\"Total training steps: \", t_total)\n        \n        scheduler = get_linear_schedule_with_warmup(\n                self.opt, num_warmup_steps=self.hparams.warmup_steps, num_training_steps=t_total)\n        self.lr_scheduler = scheduler\n        \n        return dataloader\n\n    def val_dataloader(self):\n        val_dataset = get_dataset(tokenizer=self.tokenizer, type_path=\"val\", args=self.hparams)\n        return DataLoader(val_dataset, batch_size=self.hparams.eval_batch_size, num_workers=4)","31af80d0":"logger = logging.getLogger(__name__)\n\nclass LoggingCallback(pl.Callback):\n    def on_validation_end(self, trainer, pl_module):\n        logger.info(\"***** Validation results *****\")\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n            # Log results\n            for key in sorted(metrics):\n                if key not in [\"log\", \"progress_bar\"]:\n                    logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n\n    def on_test_end(self, trainer, pl_module):\n        logger.info(\"***** Test results *****\")\n\n        if pl_module.is_logger():\n            metrics = trainer.callback_metrics\n\n            # Log and save results to file\n            output_test_results_file = os.path.join(pl_module.hparams.output_dir, \"test_results.txt\")\n            with open(output_test_results_file, \"w\") as writer:\n                for key in sorted(metrics):\n                    if key not in [\"log\", \"progress_bar\"]:\n                        logger.info(\"{} = {}\\n\".format(key, str(metrics[key])))\n                        writer.write(\"{} = {}\\n\".format(key, str(metrics[key])))","0ff27ea1":"args_dict = dict(\n    data_dir=\"\", # path for data files\n    output_dir=\"\", # path to save the checkpoints\n    model_name_or_path='t5-base',\n    tokenizer_name_or_path='t5-base',\n    max_seq_length=512,\n    learning_rate=3e-4,\n    weight_decay=0.0,\n    adam_epsilon=1e-8,\n    warmup_steps=0,\n    train_batch_size=8,\n    eval_batch_size=8,\n    num_train_epochs=2,\n    gradient_accumulation_steps=16,\n    n_gpu=1,\n    early_stop_callback=False,\n    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n    opt_level='O1', # you can find out more on optimisation levels here https:\/\/nvidia.github.io\/apex\/amp.html#opt-levels-and-properties\n    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n    seed=100)","8582ddd2":"class SwagDataset(Dataset):\n    \n    def __init__(self, tokenizer, data_dir, type_path,  max_len=512):\n        \n        self.data_dir = data_dir\n        self.type_path = type_path\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.examples = pd.read_json(os.path.join(self.data_dir, self.type_path + \".json\"))\n  \n    def __getitem__(self, index):\n        \n        \n        inputs, targets = self._create_features(self.examples.iloc[index])\n        source_ids = inputs[\"input_ids\"].squeeze()\n        target_ids = targets[\"input_ids\"].squeeze()\n\n        src_mask    = inputs[\"attention_mask\"].squeeze()  # might need to squeeze\n        target_mask = targets[\"attention_mask\"].squeeze()  # might need to squeeze\n\n        return {\"source_ids\": source_ids, \"source_mask\": src_mask, \\\n                \"target_ids\": target_ids, \"target_mask\": target_mask}\n  \n    def __len__(self):\n        return len(self.examples)\n  \n    def _build(self):\n        #examples = pd.read_csv(os.path.join(self.data_dir, self.type_path + \".csv\"))\n        #for idx, row in examples.iterrows():\n        #    self._create_features(row)\n        pass\n  \n    def _create_features(self, p_row):\n        \n        src_text = p_row[\"text\"]\n        src_content = \"context: \" + src_text\n        rnd_ner = p_row[\"predicted_ner\"][np.random.randint(0, len(p_row[\"predicted_ner\"]))][0]\n\n        if len(p_row[\"labels\"]) > 0:\n\n            match_list = [rnd_ner for label in p_row[\"labels\"] \\\n                          if label[\"mapped_para_text\"].strip().lower() == rnd_ner.strip().lower()]\n\n            if len(match_list) > 0:\n                # Create negative label\n                src_content = src_content + \" \" + \"The review on entity \" + rnd_ner.strip() + \\\n                                \" is options: 1: negative. 2: non-negative. <\/s>\"\n                trg = \"1 <\/s>\"\n            else:\n                # Create non-negative label\n                src_content = src_content + \" \" + \"The review on entity \" + rnd_ner.strip() + \\\n                                \" is options: 1: negative. 2: non-negative. <\/s>\"\n                trg = \"2 <\/s>\"\n        else:\n            # Create unrelated label\n            src_content = src_content + \" \" + \"The review on entity \" + rnd_ner.strip() + \\\n                            \" is options: 1: negative. 2: non-negative. <\/s>\"\n            trg = \"2 <\/s>\"\n        \n\n        # tokenize inputs\n        tokenized_inputs = self.tokenizer.batch_encode_plus([src_content], max_length=self.max_len, \\\n                                                            pad_to_max_length=True, return_tensors=\"pt\")\n        # tokenize targets\n        tokenized_targets = self.tokenizer.batch_encode_plus([trg], max_length=2, \\\n                                                             pad_to_max_length=True, return_tensors=\"pt\")\n        \n        return tokenized_inputs, tokenized_targets","29b2476a":"!mkdir -p t5_swag","69318176":"data_dir = 'final_news_data'\nout_dir = 't5_swag'\nnum_epochs = 10\n\ntokenizer = T5Tokenizer.from_pretrained('t5-base')\ndataset = SwagDataset(tokenizer, data_dir=data_dir, type_path='val')\nprint(\"len of val dataset: \", len(dataset))\n\n''' update args '''\nargs_dict.update({'data_dir': data_dir, 'output_dir': out_dir, 'num_train_epochs':num_epochs})\nargs = argparse.Namespace(**args_dict)\nargs.max_grad_norm","8cda8fce":"#next(iter(dataset))\n#pd.read_(\".\/final_news_data\/val.csv\")[\"labels\"].iloc[6]","a4d53ad7":"checkpoint_callback = pl.callbacks.ModelCheckpoint(filepath=args.output_dir, prefix=\"checkpoint\", \\\n                                                   monitor=\"val_loss\", mode=\"min\", save_top_k=5)\n\n''' checkpoint_callback should be given as below argument to persist the model'''\ntrain_params = dict(\n    accumulate_grad_batches=args.gradient_accumulation_steps,\n    gpus=args.n_gpu,\n    max_epochs=args.num_train_epochs,\n    early_stop_callback=False,\n    precision= 16 if args.fp_16 else 32,\n    amp_level=args.opt_level,\n    gradient_clip_val=args.max_grad_norm,\n    checkpoint_callback=None,\n    callbacks=[LoggingCallback()])","fd1b17fd":"def get_dataset(tokenizer, type_path, args):\n    return SwagDataset(tokenizer=tokenizer, data_dir=args.data_dir, type_path=type_path,  \\\n                       max_len=args.max_seq_length)","af522d7f":"model = T5FineTuner(args)","5cac1ca5":"trainer = pl.Trainer(**train_params)","b555b5ab":"trainer.fit(model)","45fd7e03":"!ls -l \"\/kaggle\/working\/\"\n#!rm -r \"\/kaggle\/working\/lightning_logs\"","5bd2ba8a":"import textwrap\nfrom tqdm.auto import tqdm\nfrom sklearn import metrics","64dea54e":"#torch.cuda.device_count()\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","0f97c703":"dataset =  SwagDataset(tokenizer, data_dir=data_dir, type_path='val')\nloader = DataLoader(dataset, batch_size=8, num_workers=4, shuffle=True)\n\n#dataset = ImdbDataset(tokenizer, 'aclImdb', 'test',  max_len=512)\n#loader = DataLoader(dataset, batch_size=32, num_workers=4)\n\n''' Put on GPU, not required if we are checkpointing'''\nmodel.to(device)\nmodel.model.eval()\noutputs = []\ntargets = []\nfor batch in tqdm(loader):\n    outs = model.model.generate(input_ids=batch['source_ids'].to(device), \\\n                                attention_mask=batch['source_mask'].to(device), \\\n                                max_length=2)\n\n    dec = [tokenizer.decode(ids) for ids in outs]\n    target = [tokenizer.decode(ids) for ids in batch[\"target_ids\"]]\n  \n    outputs.extend(dec)\n    targets.extend(target)\n#loader = DataLoader(dataset, batch_size=32, shuffle=True)","aba7399c":"metrics.accuracy_score(targets, outputs)","e9e59208":"print(metrics.classification_report(targets, outputs))","a6a60cc3":"''' Test the results'''\nit = iter(loader)\nbatch = next(it)\nbatch[\"source_ids\"].shape","00574422":"outs = model.model.generate(input_ids=batch['source_ids'].to(device), \n                              attention_mask=batch['source_mask'].to(device), \n                              max_length=2)\n\ndec = [tokenizer.decode(ids) for ids in outs]\n\ntexts = [tokenizer.decode(ids) for ids in batch['source_ids']]\ntargets = [tokenizer.decode(ids) for ids in batch['target_ids']]","f094a1f4":"for i in range(batch[\"source_ids\"].shape[0]):\n    c = texts[i]\n    lines = textwrap.wrap(\"text:\\n%s\\n\" % c, width=100)\n    print(\"\\n\".join(lines))\n    print(\"\\nActual sentiment: %s\" % targets[i])\n    print(\"predicted sentiment: %s\" % dec[i])\n    print(\"=====================================================================\\n\")","cd4f19b2":"###### Load news data & Create samples","f992cc98":"###### Create Train\/Valid datasets"}}