{"cell_type":{"eca23f1b":"code","0579e6e1":"code","dc607b8c":"code","1b437d34":"code","05af8f52":"code","2992e70a":"code","160701ee":"code","c8b144b7":"code","1872e8eb":"code","ca1de3ac":"code","798f2331":"code","7123e69b":"code","3e4f3fa8":"code","3b50e270":"code","69b0ff80":"code","097733ee":"code","34408244":"code","44842b23":"code","d6c5be4f":"code","fe8d1654":"code","8004c412":"code","584c05f9":"code","3ed33a48":"code","701d6cbc":"code","e0b70ab2":"code","90887e6d":"code","038b19b7":"markdown","e762938f":"markdown","4694824c":"markdown","cc0218b0":"markdown","8ac3705b":"markdown","9a0e622c":"markdown","3442f52c":"markdown","24094c3e":"markdown","126fc4d2":"markdown","30b2154c":"markdown","89762b92":"markdown","b65952cf":"markdown","96bca1d3":"markdown","1f342d6d":"markdown","586922e1":"markdown"},"source":{"eca23f1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0579e6e1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\nfrom sklearn.utils import shuffle\n\nimport re\nimport nltk\nnltk.download('stopwords')\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n","dc607b8c":"dataset_cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndata = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=dataset_cols)","1b437d34":"data.shape","05af8f52":"data.head()","2992e70a":"df = shuffle(data,random_state=10)\ndf = df[1:100000]","160701ee":"df.shape","c8b144b7":"sns.countplot(df[\"target\"])","1872e8eb":"df[\"text\"].iloc[1]","ca1de3ac":"stop_words = stopwords.words(\"english\")\nstemmer = SnowballStemmer(\"english\")\n\ncleaning = \"@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]+\"\n\ndef preprocess(text, stem=False):\n    # Remove link,user and special characters\n    text = re.sub(cleaning, ' ', str(text).lower()).strip()\n    tokens = []\n    for token in text.split():\n        if token not in stop_words:\n            if stem:\n                tokens.append(stemmer.stem(token))\n            else:\n                tokens.append(token)\n    return \" \".join(tokens)","798f2331":"df.text = df.text.apply(lambda x: preprocess(x, stem = True))","7123e69b":"from unicodedata import normalize\ndf[\"text\"] = df[\"text\"].apply(lambda text: normalize(\"NFKD\", str(text)).encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\"))","3e4f3fa8":"df[\"target\"] = df[\"target\"].map({0: \"negative\", 4: \"positive\"})","3b50e270":"y = df['target']\nle = LabelEncoder()\ny = le.fit_transform(y)\n","69b0ff80":"X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size = 0.20, random_state = 0)","097733ee":"tfidf = TfidfVectorizer(max_features = 600)\nX_train_tf = tfidf.fit_transform(X_train).toarray() \nX_test_tf = tfidf.transform(X_test).toarray()","34408244":"X_train_tf.shape, X_test_tf.shape, y_train.shape, y_test.shape","44842b23":"X_train_tf","d6c5be4f":"lr = LogisticRegression(random_state = 0)\nlr.fit(X_train_tf, y_train) ","fe8d1654":"y_pred_lr = lr.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_lr))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))","8004c412":"nb = MultinomialNB()\nnb.fit(X_train_tf,y_train)","584c05f9":"y_pred_nb = nb.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))","3ed33a48":"from catboost import CatBoostClassifier\ncatb = CatBoostClassifier(verbose=0, n_estimators=100)\nmodel= catb.fit(X_train_tf, y_train)\n","701d6cbc":"y_pred_catb = model.predict(X_test_tf)\ny_pred_catb[y_pred_catb > 0.5] = 1\ny_pred_catb[y_pred_catb <= 0.5] = 0\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_catb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_catb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_catb))","e0b70ab2":"rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nstart_time = time.time()\nrf.fit(X_train_tf, y_train)\nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","90887e6d":"y_pred_rf = rf.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_rf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))","038b19b7":"Countplot of the target varible","e762938f":"Using different ML Model for prediction ","4694824c":"Naive Bayes","cc0218b0":"Read the data-set","8ac3705b":"Data Pre-Processing steps regax,stopword removal, tokenization","9a0e622c":"Convert the target to positive and nagative after that apply label encoding to make 0,1","3442f52c":"Importing libraries","24094c3e":"Creat TfiDF Vector","126fc4d2":"Make train test split","30b2154c":"Remove the accent from the tweet ","89762b92":"**Conclusion\nThe accuracy score of various classification algorithms on Twitter Sentiment Data is as follows:\n1. Logistic Regression: 73.54%\n2. Naive Bayes: 73.045%\n3. CatBoost: 73.11%\n4. Random Forest: 71.35%**\n","b65952cf":"Random Forest ","96bca1d3":"Catboost Classifier ","1f342d6d":"Creating a sample dataset for performing","586922e1":"Logistic Regression"}}