{"cell_type":{"b2d369c0":"code","682b7bff":"code","333c1e84":"code","32f5ee49":"code","ffffc9c5":"code","5d91eb32":"code","c80ed567":"code","e2071b23":"code","dbacff8a":"code","bfee8b51":"code","dad27244":"code","69192ca0":"code","4ed206ec":"code","fd813648":"code","3f6824b0":"code","c45a524c":"code","e34ceab7":"code","77ddf72e":"code","8af14efe":"code","83da61d5":"code","90c47b3e":"code","93d4ba2c":"code","c4cadbe0":"code","9926885a":"code","dcd6ef27":"code","690f6484":"code","1c38b943":"code","b07596a2":"code","e0ae3e57":"code","6c3a3a05":"code","0ccd493e":"code","4728e3cd":"code","bb043375":"code","79fa4330":"code","ba149cdf":"code","fff6279f":"code","d821b076":"code","71e7ffb4":"code","39aaef6d":"code","ac5199f5":"code","1ef2bcc2":"code","4fa313cd":"code","fac5338c":"code","9216a637":"code","dc63b8fd":"code","5e87ee3b":"code","6f809d93":"code","a13b6ba5":"code","2f6f50cc":"code","0c509129":"code","3f4c883e":"code","831a8d6e":"code","36a8c786":"markdown","c40eead7":"markdown","5ec1e987":"markdown","ff38c16c":"markdown","e88294f3":"markdown","040eb568":"markdown","fd467638":"markdown","5b25f420":"markdown","f6d09987":"markdown","bd098064":"markdown","fb456d06":"markdown","06671247":"markdown"},"source":{"b2d369c0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nimport matplotlib as mp\nimport scipy\nimport scipy.stats\nimport tensorflow as tf\n#import tensorflow_hub as hub\nimport json\nimport pickle\nimport urllib\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer\n\nprint(tf.__version__)\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\nfrom wordcloud import WordCloud, STOPWORDS","682b7bff":"path = os.path.join(dirname, filename)\ndf = pd.read_csv(path) ","333c1e84":"df.columns","32f5ee49":"data=df.tweet","ffffc9c5":"df.head()","5d91eb32":"wc=WordCloud(width=200,height=100,background_color='black',stopwords=STOPWORDS\n            ).generate(str(data))","c80ed567":"fig=plt.figure(figsize=(10,10),facecolor='k',edgecolor='w')\nplt.imshow(wc,interpolation='bicubic')\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()","e2071b23":"fig=plt.figure(figsize=(15,15),facecolor='k',edgecolor='w')\nplt.imshow(wc,interpolation='bicubic')\nplt.axis(\"off\")\nplt.tight_layout()\nplt.show()","dbacff8a":"from nltk.corpus import stopwords\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nimport string\nimport gensim\nfrom gensim import corpora","bfee8b51":"stop = set(stopwords.words('english'))\n\nexclude = set(string.punctuation)\n\nlemmma= WordNetLemmatizer() #base word conversion for bbetter tuning and performance\n\ndef clean(doc):\n    stop_free=\" \".join([i for i in doc.lower().split() if i not in stop])\n    punc_free=\"\".join([char for char in stop_free if char not in exclude])\n    normalisation = \" \".join(lemmma.lemmatize(word) for word in punc_free.split(' '))\n    return normalisation\n\ndocument=df.tweet.to_list()\n\n\ndoc_clean=[clean(docu).split() for docu in document ]\n\ndoc_clean[:10]","dad27244":"df['tweet_clean']=pd.Series(doc_clean)\n","69192ca0":"df.head()","4ed206ec":"dictionary=corpora.Dictionary(doc_clean)\n\nprint(dictionary)","fd813648":"doc_word_freqcies=[dictionary.doc2bow(term) for term in doc_clean]\ndoc_word_freqcies[:30]","3f6824b0":"from gensim.models import LdaModel","c45a524c":"\nmodel=LdaModel(doc_word_freqcies,num_topics=9,id2word=dictionary,passes=400) ","e34ceab7":"types= model.show_topics()\nfor t in types:\n    print(t)\n    print('----------------')","77ddf72e":"diction={}\nfor i in range(6):\n    words=model.show_topic(i,topn=20)\n    #print(words)\n    diction[\"Topic number\" + \"{}\".format(i)]=[i[0] for i in words]\n    \n    \npd.DataFrame(diction)","8af14efe":"import pyLDAvis.gensim","83da61d5":"Vis=pyLDAvis.gensim.prepare(model,doc_word_freqcies,dictionary,sort_topics=False)","90c47b3e":"pyLDAvis.display(Vis)","93d4ba2c":"# Create Corpus: Term Document Frequency\ncorpus = [dictionary.doc2bow(text) for text in doc_clean]\n","c4cadbe0":"\n\ndef format_topics_sentences(ldamodel=None, corpus=corpus, texts=data):\n    # Init output\n    sent_topics_df = pd.DataFrame()\n\n    # Get main topic in each document\n    for i, row_list in enumerate(ldamodel[corpus]):\n        row = row_list[0] if ldamodel.per_word_topics else row_list            \n        # print(row)\n        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n        # Get the Dominant topic, Perc Contribution and Keywords for each document\n        for j, (topic_num, prop_topic) in enumerate(row):\n            if j == 0:  # => dominant topic\n                wp = ldamodel.show_topic(topic_num)\n                topic_keywords = \", \".join([word for word, prop in wp])\n                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n            else:\n                break\n    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n\n    # Add original text to the end of the output\n    contents = pd.Series(texts)\n    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n    return(sent_topics_df)\n\n\ndf_topic_sents_keywords = format_topics_sentences(ldamodel=model, corpus=corpus, texts=doc_clean)\n\n# Format\ndf_dominant_topic = df_topic_sents_keywords.reset_index()\ndf_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\ndf_dominant_topic.head(10)","9926885a":"vc=df_dominant_topic.Dominant_Topic.value_counts()\nvc","dcd6ef27":"#topic 2 and 4  is \n#topic 3 is\n#topic 5 IS \n#topic 1 is\ndic={1.0:\"2\",2.0:\"3\",3.0:\"4\",4.0:\"5\",5.0:\"6\",6.0:'7',0.0:\"1\",8.0:'9',7.0:'8'}\nvc=df_dominant_topic.Dominant_Topic.value_counts()","690f6484":"dt=df_dominant_topic[[\"Dominant_Topic\"]]","1c38b943":"dt","b07596a2":"dt.Dominant_Topic=dt.Dominant_Topic.apply(lambda row: dic[row])","e0ae3e57":"pd.DataFrame(dt.Dominant_Topic.value_counts())","6c3a3a05":"# lda is assumed to be the variable holding the LdaModel object\nimport matplotlib.pyplot as plt\nfor t in range(model.num_topics):\n    plt.figure()\n#   plt.imshow(WordCloud().fit_words(model.show_topic(t, 200)))\n    plt.imshow(WordCloud().fit_words(dict(model.show_topic(t, 200))))\n    plt.axis(\"off\")\n    plt.title(\"Topic #\" + str(t))\n    plt.show()","0ccd493e":"from collections import OrderedDict\n\ndata_lda = {i: OrderedDict(model.show_topic(i,25)) for i in range(6)}\n#data_lda","4728e3cd":"import pandas as pd\n\ndf_lda = pd.DataFrame(data_lda)\nprint(df_lda.shape)\ndf_lda = df_lda.fillna(0).T\nprint(df_lda.shape)","bb043375":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\ng=sns.clustermap(df_lda.corr(), center=0, cmap=\"RdBu\", metric='cosine'\n                 , linewidths=.75, figsize=(10, 10))\nplt.setp(g.ax_heatmap.yaxis.get_majorticklabels(), rotation=0)\nplt.show()\n#plt.setp(ax_heatmap.get_yticklabels(), rotation=0)  # For y axis\n\n","79fa4330":"from textblob import TextBlob\ndf['pop']=df.tweet_clean.apply(lambda tw: TextBlob(' '.join(tw)).sentiment.polarity)\ndf","ba149cdf":"df['sent']=df['pop'].apply(lambda p: 'positive' if p>0 else  \n                           ( 'negative' if p<0  else 'neutral'))","fff6279f":"df","d821b076":"plt.figure(figsize=(10, 10))\ndf['Time']=pd.to_datetime(df.timestamp)\nsns.scatterplot(x=df['Time'], y=df['pop'],data=df,hue=df['sent']);\n\n\n","71e7ffb4":"df['year']=df.Time.apply(lambda x:x.year)\ndf.groupby('year')['pop'].describe()['mean'].plot(figsize=(10,10))","39aaef6d":"print(df.sent.value_counts())\nsns.countplot(x='sent', data = df);","ac5199f5":"plt.figure(figsize=(10,6))\nsns.distplot(df['pop'], bins=30)\nplt.title('Sentiment Distribution',size = 10)\nplt.xlabel('Polarity',size = 10)\nplt.ylabel('Frequency',size = 10)\nplt.show();","1ef2bcc2":"count = df.groupby(['year','sent'])['tweet_clean'].count().reset_index().rename(\n    columns={'tweet_clean':'count'})\ncount","4fa313cd":"times = count.year.unique()\nneut=[]\nfor year in count.year.unique():\n    a=count[count['year']==year]\n    if len(list(a[a['sent']=='neutral'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='neutral']['count'])[0]\n        neut.append(c)\n    else:\n        neut.append(0)\npos = count.loc[count['sent'] == 'positive']['count'].reset_index(drop = True)\n\nneg=[]\nfor year in count.year.unique():\n    a=count[count['year']==year]\n    if len(list(a[a['sent']=='negative'].sent))>0:#list(a['sent'].unique()):\n        c=list(a[a['sent']=='negative']['count'])[0]\n        neg.append(c)\n    else:\n        neg.append(0)                        \nplt.figure(figsize=(10,10))\nplt.xticks(rotation='45')\nlin1=plt.plot(times, pos, 'ro-', label='positive')\nlin2=plt.plot(times, neut, 'g^-', label='neutral')\nlin3=plt.plot(times, neg, 'b--', label='negative')\nplt.legend()\nplt.show","fac5338c":"df_dominant_topic.Dominant_Topic.shape","9216a637":"df['topic']=df_dominant_topic.Dominant_Topic","dc63b8fd":"df.columns","5e87ee3b":"df_nlp=df[['id','Time','year','topic','pop','sent']]","6f809d93":"print(df_nlp.topic.value_counts())\nsns.countplot(x='topic', data = df_nlp);","a13b6ba5":"import plotly.graph_objs as go\nlabels =['positive stuff','campaign','trump','racism','trump','gun','education','campaign','health care','biden']\nouter_values = df_nlp.topic.value_counts()\ninner_values=pd.Series()\nfor top in df_nlp.topic.unique():\n    df_top=df_nlp[df_nlp['topic']==top]\n    inner_values=inner_values.append(df_top[df_top['sent']=='positive'].value_counts())\n    inner_values=inner_values.append(df_top[df_top['sent']=='negative'].value_counts())\n    inner_values=inner_values.append(df_top[df_top['sent']=='neutral'].value_counts())\ntrace1 = go.Pie(\n    hole=0.5,\n    sort=False,\n    direction='clockwise',\n    domain={'x': [0.15, 0.85], 'y': [0.15, 0.85]},\n    values=inner_values,\n    textinfo='label',\n    textposition='inside',\n    marker={'line': {'color': 'white', 'width': 1}}\n)\ntrace2 = go.Pie(\n    hole=0.7,\n    sort=False,\n    direction='clockwise',\n    values=outer_values,\n    labels=labels,\n    textinfo='label',\n    textposition='outside',\n    marker={'colors': ['green', 'red', 'blue'],\n            'line': {'color': 'white', 'width': 1}}\n)\nfig = go.FigureWidget(data=[trace1, trace2])\nfig\n\n","2f6f50cc":"df_nlp.head()","0c509129":"df_nlp['hour']=df_nlp.Time.apply(lambda x:x.hour)\n\n#What time he posts relates with the pop score?\n\ndf_nlp.groupby('hour')['pop'].describe()['mean'].plot(figsize=(10,10))","3f4c883e":"sns.heatmap(df_nlp.corr())","831a8d6e":"df_nlp.hour.value_counts()\n","36a8c786":"## Main wordcloud","c40eead7":"## Biden ain't timewise-moody hmmm :-(\n## But hey too many posts between 11pm-1am","5ec1e987":"## Clean and Lemmatize into a document","ff38c16c":"## Now we have dived real deep. What else can we check here?","e88294f3":"## Time of tweet and topic\/sentiment related? i.e is Biden timewise-moody?","040eb568":"## Make new nlp subset","fd467638":"## Sentiment analysis","5b25f420":"## Visualize topics","f6d09987":"## Topic word cloud","bd098064":"## Make LDA Model","fb456d06":"## Word heatmap","06671247":"## topic classification topic number and its meaning\n\n## 4 racism\n## 6 education\n## 5 gun\n## 8 health care\n## 7 and 2 campaign\n## 3  trump\n## 9 biden\n## 1 positive stuff\n\n\n\n\n## He does talk about stuff related to \n## jobs  too, \n## depending on number of passes \n## and number of topics \n## we might get different results\n## and classifications"}}