{"cell_type":{"8be048c5":"code","7619fdb5":"code","a042a578":"code","ca5a78a5":"code","04c3a4a5":"code","61558445":"code","275a1e8a":"code","ecd91110":"code","5a39cca3":"code","79f23214":"code","3df6aedf":"code","f2a3367f":"code","09f69049":"code","4ea624cc":"code","421b687d":"code","6dc57a05":"code","7f254b8f":"code","928e04db":"code","6d94aebf":"code","19aedd01":"code","94b93d15":"code","40ae02df":"code","81c5fda1":"code","8c59ffb6":"code","6f4b6ff8":"code","c3bc5ec6":"code","604f4f9e":"code","6b4893eb":"code","3f7c6657":"code","5c560b7c":"code","4ee5ff81":"code","4b8d54ce":"code","00e2aa44":"code","68815b64":"code","9944378a":"code","48fa735e":"code","bdc2779a":"code","81d3634b":"code","b2f8320e":"code","471a4c0e":"code","7014a558":"markdown","477108fb":"markdown","a8995169":"markdown","d7509dc7":"markdown","af7c2e5b":"markdown","48045250":"markdown","e573bdd7":"markdown","7e5dfdfe":"markdown","89455eae":"markdown","59340f48":"markdown","bed7266d":"markdown","2250b67b":"markdown","feb5f800":"markdown","a2cfbf14":"markdown","8ee2b45a":"markdown","a0a48453":"markdown","a06a8635":"markdown","b59dfb12":"markdown","430e5efe":"markdown","11b20370":"markdown","00081f19":"markdown","c922adaa":"markdown","216c4e97":"markdown","a2076637":"markdown","dd0d5111":"markdown","6665e040":"markdown","4f789bf8":"markdown","28289fd7":"markdown","8bd128ec":"markdown","3e93050a":"markdown","9cb05d54":"markdown","3638a81d":"markdown","66bbf792":"markdown","f2748d86":"markdown","55d8641b":"markdown"},"source":{"8be048c5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualization\nimport seaborn as sns # statistical data visualization\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7619fdb5":"import warnings\n\nwarnings.filterwarnings('ignore')","a042a578":"data = '..\/input\/car-evaluation-data-set\/car_evaluation.csv'\ndf = pd.read_csv(data, header=None)","ca5a78a5":"df.head()","04c3a4a5":"df.shape ","61558445":"col_names =['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\ndf.columns = col_names\ncol_names","275a1e8a":"df.head()","ecd91110":"df.info()","5a39cca3":"df.value_counts().sum()","79f23214":"col_names = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n\nfor col in col_names:\n    print(df[col].value_counts())  ","3df6aedf":"df['class'].value_counts()","f2a3367f":"df.isnull().sum()","09f69049":"X = df.drop(['class'], axis=1)\n\ny = df['class']","4ea624cc":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.33, random_state=42)","421b687d":"print(X_train.shape)\nprint(X_test.shape)","6dc57a05":"# Check the data type of X_train\nX_train.dtypes","7f254b8f":"X_train.head()","928e04db":"# import category encoders\n\nimport category_encoders as ce","6d94aebf":"# encode variables with ordinal encoding\n\nencoder = ce.OrdinalEncoder(cols=['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety'])\n\n\nX_train = encoder.fit_transform(X_train)\n\nX_test = encoder.transform(X_test)","19aedd01":"X_train.head()","94b93d15":"X_test.head()","40ae02df":"# import DecisionTreeClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier","81c5fda1":"# instantiate the DecisionTreeClassifier model with criterion gini index\n\nclf_gini = DecisionTreeClassifier(criterion='gini', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_gini.fit(X_train, y_train)","8c59ffb6":"y_pred_gini = clf_gini.predict(X_test)\ny_pred_gini[0:5]","6f4b6ff8":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_gini)))","c3bc5ec6":"y_pred_train_gini = clf_gini.predict(X_train)\n\ny_pred_train_gini","604f4f9e":"print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_gini)))","6b4893eb":"print('Training set score: {:.4f}'.format(clf_gini.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(clf_gini.score(X_test, y_test)))","3f7c6657":"plt.figure(figsize=(12,8))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_gini.fit(X_train, y_train)) ","5c560b7c":"#Visualize decision-trees with graphviz\nimport graphviz \ndot_data = tree.export_graphviz(clf_gini, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","4ee5ff81":"clf_en = DecisionTreeClassifier(criterion='entropy', max_depth=3, random_state=0)\n\n\n# fit the model\nclf_en.fit(X_train, y_train)","4b8d54ce":"y_pred_en = clf_en.predict(X_test)\ny_pred_en[0:5]","00e2aa44":"from sklearn.metrics import accuracy_score\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))","68815b64":"y_pred_train_en = clf_en.predict(X_train)\ny_pred_train_en[0:5]","9944378a":"from sklearn.metrics import accuracy_score\n\nprint('Training-Set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))","48fa735e":"# Print the score accuracy for both train and test set.\nprint('Training-Set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train_en)))\n\nprint('Model accuracy score with criterion gini index: {0:0.4f}'. format(accuracy_score(y_test, y_pred_en)))","bdc2779a":"plt.figure(figsize=(12,8))\n\nfrom sklearn import tree\n\ntree.plot_tree(clf_en.fit(X_train, y_train)) ","81d3634b":"#Visualize decision-trees with graphviz\nimport graphviz \ndot_data = tree.export_graphviz(clf_en, out_file=None, \n                              feature_names=X_train.columns,  \n                              class_names=y_train,  \n                              filled=True, rounded=True,  \n                              special_characters=True)\n\ngraph = graphviz.Source(dot_data) \n\ngraph ","b2f8320e":"# Print the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_en)\n\nprint('Confusion matrix\\n\\n', cm)","471a4c0e":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_en))","7014a558":"### Feature Engineering\n*  process of transforming raw data into useful features that help us to understand our model better and increase its predictive power. I will carry out feature engineering on different types of variables.","477108fb":"The class target variable is ordinal in nature","a8995169":"### Exploring the class variable","d7509dc7":"## Visualize decision-trees","af7c2e5b":"We can see that the training-set score and test-set score is same as above. The training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting.","48045250":"## Check for overfitting and underfitting","e573bdd7":"### Renaming the columns","7e5dfdfe":"We can see that there is no missing value in the dataset","89455eae":"## Compare the train-set and test-set accuracy","59340f48":"## Classificataion Report\n* Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model.","bed7266d":"### Compare the train-set and test-set accuracy\n* Now, I will compare the train-set and test-set accuracy to check for overfitting.","2250b67b":"### Lets split the data into training and testing set","feb5f800":"Here, y_test are the true class labels and y_pred_gini are the predicted class labels in the test-set.","a2cfbf14":"### Declare the feature vector and target variable","8ee2b45a":"### Checking for the accuracy score with criterion entropy","a0a48453":"### Checking for overfitting and Underfitting","a06a8635":"### Predict the Test result with criterion entropy","b59dfb12":"We now have the training and testing data ready for model.","430e5efe":"### Check the shape of X_train and X_test","11b20370":"### Summary of the variables\n* There are 7 variables in the dataset, all of them are categorical\n* Class is the target variable","00081f19":"### Exploratory data analysis","c922adaa":"## Decision Tree Classifier with criterion entropy","216c4e97":"### Importing the dataset","a2076637":"## Visualize decision-trees","dd0d5111":"We can see that all the variables are ordinal categorical data type.","6665e040":"## Confusion Matrix\n* It helps to summarize the perfomance of a classification algorithm.\n* it gives the summary of the correct and incorrect predictions broken down by category in tabluar form.\n* Four types of outcomes are possible when evaluating a classification model perfomance; TP,TN,FP,FN","4f789bf8":"Here, the training-set accuracy score is 0.7865 while the test-set accuracy to be 0.8021. These two values are quite comparable. So, there is no sign of overfitting.","28289fd7":"## Result and Conclusions\n* I build decisio tree classifier model to predict the safety of a car\n* One model was with gini Impurity, the other was with entropy.\n* For both models, there is no overfitting because the accuracy of the training and testing set are comparable, this might be due to small dataset.\n* The confusion matrix and classification report yield a very good model performance.","8bd128ec":"### Checking for missing values","3e93050a":"### Check accuracy score with criterion gini","9cb05d54":"## Decision Tree Classifier with criterion GINI index","3638a81d":"## Reference \n* https:\/\/www.kaggle.com\/prashant111\/decision-tree-classifier-tutorial\/notebook\n* https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.confusion_matrix.html\n* https:\/\/benslack19.github.io\/data%20science\/statistics\/f-statistic\/","66bbf792":"Our classification model accuracy is very good. Our model is doing a very good job in terms of predicting the class labels.\n\nBut, it does not give the underlying distribution of values. Also, it does not tell anything about the type of errors our classifer is making.\n\nWe have another tool called Confusion matrix that comes to our rescue.","f2748d86":"### Frequency distribution of values in variables\nTo check the frequency counts of categorical variables.","55d8641b":"### Predict the test result with criterion Gini Test"}}