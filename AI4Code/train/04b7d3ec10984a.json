{"cell_type":{"1c1d114d":"code","c560efdd":"code","aa246805":"code","a53cbb82":"code","e5538bfd":"code","275657b0":"code","bc8ec917":"code","d8663e3b":"code","f9deffce":"code","81ed6b0c":"markdown","848fa1ee":"markdown","872ff537":"markdown","cf532531":"markdown","a39fb953":"markdown"},"source":{"1c1d114d":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer # Applies different functions to different columns\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler","c560efdd":"train = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/train.csv\", index_col=\"id\")\ntest = pd.read_csv(\"\/kaggle\/input\/30-days-of-ml\/test.csv\", index_col=\"id\")","aa246805":"y = train.target\nX = train.drop(\"target\", axis=1)\n\n# Selecting numerical and categorical features\ncat_X = X.select_dtypes(exclude=[\"float\"])\nnum_X = X.select_dtypes(exclude=[\"object\"])\ncat_columns = list(cat_X.columns)\nnum_columns = list(num_X.columns)\n\n# Selecting features to apply one hot encodding and ordinal encoding\nlow_car_features = [col for col in cat_columns if X[col].nunique()<5]\nhigh_car_features = list(set(cat_columns)-set(low_car_features))","a53cbb82":"my_pipeline = ColumnTransformer([\n    (\"num\", StandardScaler(), num_columns),\n    (\"Ord_encoder\", OrdinalEncoder(), high_car_features),\n    (\"OH_encoder\", OneHotEncoder(handle_unknown=\"error\", drop='first'), low_car_features),\n])\nX_preprocessed = my_pipeline.fit_transform(X)","e5538bfd":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint, uniform\n\nparam_distributions = {\n    \"n_estimators\": randint(low=100, high=500),\n    \"learning_rate\": uniform(loc=0.0, scale=1.0),\n    \"reg_alpha\":uniform(loc=0.5, scale=3.0),\n    \"reg_lambda\":uniform(loc=0.5, scale=4.0)\n} \n\nxgb_regressor = XGBRegressor(random_state=83, tree_method='gpu_hist')\nrandom_search = RandomizedSearchCV(xgb_regressor, param_distributions=param_distributions,\n                                  n_iter=10, cv=5, scoring=\"neg_mean_squared_error\", \n                                   random_state=83)\nrandom_search.fit(X_preprocessed, y)","275657b0":"cv_results = random_search.cv_results_\nfor score, params in zip(cv_results[\"mean_test_score\"], cv_results[\"params\"]):\n    print(params)\n    print(np.sqrt(-1*score))","bc8ec917":"np.sqrt(-1*(random_search.best_score_))","d8663e3b":"random_search.best_estimator_","f9deffce":"random_search.best_params_","81ed6b0c":"# Loading and preprocessing data","848fa1ee":"# Introduction","872ff537":"In a previous [notebook](https:\/\/www.kaggle.com\/alberth95\/eda-mutual-information-pca) I've perform an EDA over this dataset. This time, I focuse on training a machine learning algorithm, more precisely, a XGBoost + GPU and I tune the hyperparameters using randomized search of the best parameters.","cf532531":"# XGBoost regressor + gpu_hist and randomized hyperparameter tunning","a39fb953":"Insted of searching for the best hyperparameters by hand, I tried a randmized search, method which could be more effective. This method was my personal best score. "}}