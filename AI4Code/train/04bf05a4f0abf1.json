{"cell_type":{"7027a8f3":"code","26002f1d":"code","a743b099":"code","8a33a2ea":"code","d97a0909":"code","ccd833f2":"code","432784ee":"code","e7ccf455":"code","d41faaaf":"code","4e190e77":"code","4a01fec3":"code","c19acad3":"code","a9796f56":"code","53552bb6":"code","40b38911":"code","9210eac2":"code","09e2b7eb":"code","3a2ed778":"code","c1337bbe":"code","ba05b9e9":"code","96b5acd8":"code","f8d0e4c1":"code","45f2b197":"markdown","e7670ee5":"markdown","12189f2d":"markdown","af230c24":"markdown","3dfce560":"markdown"},"source":{"7027a8f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","26002f1d":"sentences = []\nlabels = []\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\nprint(len(stopwords))","a743b099":"import csv\nimport tensorflow as tf\nimport numpy as np\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n","8a33a2ea":"with open('..\/input\/amazon-fine-food-reviews\/Reviews.csv','r') as csvfile:\n    reader = csv.reader(csvfile, delimiter = ',')\n    next(reader)\n    for row in reader:\n        labels.append(row[6])\n        sentence = row[9]\n        for word in stopwords:\n            token = ' '+word+' '\n            sentence.replace(token,' ')\n            sentence.replace('  ',' ')\n        sentences.append(sentence)\n\nprint(len(labels))\nprint(len(sentences))\nprint(sentences[0])","d97a0909":"vocab_size = 10000\nembedding_dim = 16\nmax_length = 120\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"","ccd833f2":"tokenizer = Tokenizer(num_words = vocab_size,oov_token = oov_tok)\ntokenizer.fit_on_texts(sentences)\nword_index = tokenizer.word_index","432784ee":"print(len(word_index))","e7ccf455":"sequences = tokenizer.texts_to_sequences(sentences)","d41faaaf":"print(sequences[0])","4e190e77":"padded = pad_sequences(sequences, padding = padding_type, maxlen = max_length,truncating = trunc_type)","4a01fec3":"print(padded[0])\nprint(type(padded))\nprint(type(labels))","c19acad3":"labels = np.array(labels)\nprint(type(labels))","a9796f56":"label_tokenizer = Tokenizer()\nlabel_tokenizer.fit_on_texts(labels)","53552bb6":"word_index_labels = label_tokenizer.word_index\nprint(word_index_labels)","40b38911":"label_seq = label_tokenizer.texts_to_sequences(labels)\nlabel_padd = pad_sequences(label_seq)","9210eac2":"print(label_padd[0])","09e2b7eb":"from keras.models import Model \nfrom keras.layers import *\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential","3a2ed778":"max_features = vocab_size\nmaxlen = max_length\nfilters = 250\nkernel_size = 3\nhidden_dims = 250\n\nmodel = Sequential()\nmodel.add(Embedding(max_features,embedding_dim,input_length=maxlen))\nmodel.add(Dropout(0.2))\nmodel.add(Conv1D(filters,kernel_size,padding='valid',activation='relu',strides=1))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(hidden_dims))\nmodel.add(Dropout(0.2))\nmodel.add(Activation('relu'))\n\n# We project onto a single unit output layer, and squash it with a sigmoid:\nmodel.add(Dense(6))\nmodel.add(Activation('softmax'))\n# compile \nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) ","c1337bbe":"model.summary() ","ba05b9e9":"plot_model(model, show_shapes=True, to_file='model.png') ","96b5acd8":"num_epochs = 5\nhistory = model.fit(padded, label_padd, epochs=num_epochs, verbose=1, validation_split=0.3)","f8d0e4c1":"import matplotlib.pyplot as plt\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(15,5))\nfig.suptitle(\"Performance of Model without pretrained embeddings\")\nax1.plot(history.history['accuracy'])\nax1.plot(history.history['val_accuracy'])\nvline_cut = np.where(history.history['val_accuracy'] == np.max(history.history['val_accuracy']))[0][0]\nax1.axvline(x=vline_cut, color='k', linestyle='--')\nax1.set_title(\"Model Accuracy\")\nax1.legend(['train', 'test'])\n\nax2.plot(history.history['loss'])\nax2.plot(history.history['val_loss'])\nvline_cut = np.where(history.history['val_loss'] == np.min(history.history['val_loss']))[0][0]\nax2.axvline(x=vline_cut, color='k', linestyle='--')\nax2.set_title(\"Model Loss\")\nax2.legend(['train', 'test'])\nplt.show()","45f2b197":"Model","e7670ee5":"Plotting the model outcomes","12189f2d":"Pre Processing of Data","af230c24":"List of Stopwords","3dfce560":"Reading from CSV file and removing the stopwords"}}