{"cell_type":{"5a801543":"code","f02bde58":"code","74e7aedc":"code","4d428a72":"code","0bf348ab":"code","a1da00bd":"code","921d10bc":"code","0d60fe44":"code","0a00fa53":"code","1962c089":"code","66b76ba9":"code","d61806c7":"code","f020c480":"code","c9cf5864":"code","263ffdb9":"code","59f2552a":"code","92420d5e":"code","7a1f6736":"code","29919482":"code","864001c3":"code","e3ba2b6c":"code","f21d1322":"code","35f0665c":"code","f3f61461":"code","bca79dc3":"code","0439a992":"code","2869a8d1":"code","1bd4102f":"code","ecc41495":"code","86ad0ea5":"code","4d662cc4":"code","94f11823":"code","7688cb19":"code","3597690f":"code","e65728e8":"code","e67a7701":"code","1c687816":"markdown"},"source":{"5a801543":"import tensorflow as tf\nfrom gensim.models import word2vec\nfrom gensim.models import Word2Vec\nimport pandas as pd\nimport glob\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\n%matplotlib inline","f02bde58":"os.listdir('..\/input\/gan-text-to-image-102flowers-rieyuguanghua')","74e7aedc":"n_input = 100\nn_hidden = 128\nimage_height = 64\nimage_width = 64\nimage_depth = 3\nnoise_dim = 100\nmaxlength = 250\nNUM_EPOCHS = 100\nbatch_size = 64","4d428a72":"os.listdir('..\/input\/102flowersdataset\/')","0bf348ab":"if not os.path.exists('102flowers'):\n    !mkdir 102flowers\n    !tar zxvf ..\/input\/102flowersdataset\/102flowers.tgz -C .\/102flowers\/","a1da00bd":"all_text_filename = glob.glob('..\/input\/cvpr2016\/cvpr2016_flowers\/text_c10\/class_*\/image_*.txt')\n\nall_text_filename.sort(key=lambda x:x.split('\/')[-1])","921d10bc":"all_image_filename = glob.glob('.\/102flowers\/jpg\/*.jpg')\n\nall_image_filename.sort()","0d60fe44":"all_image_filename[3001]","0a00fa53":"all_text_filename[3001]","1962c089":"all_text_filename = np.array(all_text_filename)\nall_image_filename = np.array(all_image_filename)\nwrong_image_filename = all_image_filename[np.random.permutation(len(all_image_filename))]","66b76ba9":"dataset_image = tf.data.Dataset.from_tensor_slices((all_image_filename, wrong_image_filename))","d61806c7":"dataset_image","f020c480":"if not os.path.exists('..\/input\/gan-text-to-image-102flowers-rieyuguanghua\/all_text.txt'):\n    with open('all_text.txt', 'at') as f:\n        for a_text in all_text_filename:\n            f.write(open(a_text).read().replace('\\n', '') + '\\n')\nif not os.path.exists('..\/input\/gan-text-to-image-102flowers-rieyuguanghua\/word_model'):\n    sentences = word2vec.Text8Corpus('all_text.txt')\n    model = word2vec.Word2Vec(sentences, size=100)\n    model.save('word_model')\nelse:\n    model = Word2Vec.load('..\/input\/gan-text-to-image-102flowers-rieyuguanghua\/word_model')\n    !cp ..\/input\/gan-text-to-image-102flowers-rieyuguanghua\/all_text.txt .\/\n    !cp ..\/input\/gan-text-to-image-102flowers-rieyuguanghua\/word_model .\/\nword_vectors = model.wv","c9cf5864":"maxlength = max([len(open(a_text).read().split()) for a_text in all_text_filename])","263ffdb9":"n_steps = maxlength","59f2552a":"def pad(x, maxlength=200):\n    x1 = np.zeros((maxlength,100))\n    x1[:len(x)] = x\n    return x1","92420d5e":"def text_vec(text_filenames):\n    vec = []\n    for a_text in text_filenames:\n        all_word = open(a_text).read().split()\n        all_vec = [word_vectors[w] for w in all_word if w in word_vectors]\n        vec.append(all_vec)\n    data = pd.Series(vec)\n    data = data.apply(pad, maxlength=maxlength)\n    data_ = np.concatenate(data).reshape(len(data),maxlength,100)\n    return data_","7a1f6736":"data_text_emb = text_vec(all_text_filename)","29919482":"data_text_emb.shape","864001c3":"def read_image(image_filename):\n    image = tf.read_file(image_filename)\n    image = tf.image.decode_jpeg(image, channels=3)\n    image = tf.image.resize_image_with_crop_or_pad(image, 512, 512)\n    image = tf.image.resize_images(image, (256, 256))\n    #image = tf.image.convert_image_dtype(image, tf.float32)\n    image = (image - tf.reduce_min(image))\/(tf.reduce_max(image) - tf.reduce_min(image))\n    return image","e3ba2b6c":"def _pre_func(real_image_name, wrong_image_name):\n    wrong_image = read_image(wrong_image_name)\n    real_image = read_image(real_image_name)\n    return real_image, wrong_image","f21d1322":"dataset_image = dataset_image.map(_pre_func)","35f0665c":"dataset_image = dataset_image.batch(batch_size)","f3f61461":"iterator = tf.data.Iterator.from_structure(dataset_image.output_types, dataset_image.output_shapes)\nreal_image_batch, wrong_image_batch = iterator.get_next()","bca79dc3":"input_text = tf.placeholder(tf.float32, [None, n_steps, n_input])\ninputs_noise = tf.placeholder(tf.float32, [None, noise_dim], name='inputs_noise')","0439a992":"def length(shuru):\n    return tf.reduce_sum(tf.sign(tf.reduce_max(tf.abs(shuru),reduction_indices=2)),reduction_indices=1)","2869a8d1":"def text_rnn(input_text, batch_size=64, reuse=None):\n    cell = tf.contrib.rnn.GRUCell(n_hidden,\n                                  kernel_initializer = tf.truncated_normal_initializer(stddev=0.0001),\n                                  bias_initializer = tf.truncated_normal_initializer(stddev=0.0001),\n                                  reuse=reuse)\n    output, _ = tf.nn.dynamic_rnn(\n                                  cell,\n                                  input_text,\n                                  dtype=tf.float32,\n                                  sequence_length = length(input_text)\n                                  )\n\n    index = tf.range(0,batch_size)*n_steps + (tf.cast(length(input_text),tf.int32) - 1)\n    flat = tf.reshape(output,[-1,int(output.get_shape()[2])])\n    last = tf.gather(flat,index)\n    return last","1bd4102f":"def get_generator(noise_img, image_depth, condition_label, is_train=True, alpha=0.2):\n    with tf.variable_scope(\"generator\", reuse=(not is_train)):\n        # 100 x 1 to 4 x 4 x 512\n        # \u5168\u8fde\u63a5\u5c42\n        noise_img = tf.to_float(noise_img)\n        noise_img = tf.layers.dense(noise_img, n_hidden)\n        noise_img = tf.maximum(alpha * noise_img, noise_img)\n        noise_img_ = tf.concat([noise_img, condition_label], 1)\n        layer1 = tf.layers.dense(noise_img_, 4*4*512)\n        layer1 = tf.reshape(layer1, [-1, 4, 4, 512])\n        layer1 = tf.layers.batch_normalization(layer1, training=is_train)\n        layer1 = tf.nn.relu(layer1)\n        # batch normalization\n        #layer1 = tf.layers.batch_normalization(layer1, training=is_train)\n        # ReLU\n        #layer1 = tf.nn.relu(layer1)\n        # dropout\n        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n        \n        # 4 x 4 x 512 to 8 x 8 x 256\n        layer2 = tf.layers.conv2d_transpose(layer1, 256, 3, strides=2, padding='same')\n        layer2 = tf.layers.batch_normalization(layer2, training=is_train)\n        layer2 = tf.nn.relu(layer2)\n        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n        \n        # 8 x 8 256 to 16x 16 x 128\n        layer3 = tf.layers.conv2d_transpose(layer2, 128, 3, strides=2, padding='same')\n        layer3 = tf.layers.batch_normalization(layer3, training=is_train)\n        layer3 = tf.nn.relu(layer3)\n        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n        \n        # 16 x 16 x 128 to 32 x 32 x 64\n        layer4 = tf.layers.conv2d_transpose(layer3, 64, 3, strides=2, padding='same')\n        layer4 = tf.layers.batch_normalization(layer4, training=is_train)\n        layer4 = tf.nn.relu(layer4)\n        \n        # 64 x 64 x 32\n        layer5 = tf.layers.conv2d_transpose(layer4, 32, 3, strides=2, padding='same')\n        layer5 = tf.layers.batch_normalization(layer5, training=is_train)\n        layer5 = tf.nn.relu(layer5)\n        \n        # 128 x 128 x 16\n        layer6 = tf.layers.conv2d_transpose(layer5, 16, 3, strides=2, padding='same')\n        layer6 = tf.layers.batch_normalization(layer6, training=is_train)\n        layer6 = tf.nn.relu(layer6)  \n        \n        #  256 x 256 x 3\n        logits = tf.layers.conv2d_transpose(layer6, image_depth, 3, strides=2, padding='same')\n        outputs = tf.tanh(logits)\n        outputs = (outputs\/2) + 0.5\n        outputs = tf.clip_by_value(outputs, 0.0, 1.0)\n        return outputs","ecc41495":"def get_discriminator(inputs_img, condition_label, reuse=False, alpha=0.2):\n    with tf.variable_scope(\"discriminator\", reuse=reuse):\n        # 256 x 256 x 3 to 128 x 128 x 16\n        # \u7b2c\u4e00\u5c42\u4e0d\u52a0\u5165BN\n        layer1 = tf.layers.conv2d(inputs_img, 16, 3, strides=2, padding='same')\n        layer1 = tf.maximum(alpha * layer1, layer1)\n        layer1 = tf.nn.dropout(layer1, keep_prob=0.8)\n        \n        # 128 x 128 x 16 to 64 x 64 x 32\n        layer2 = tf.layers.conv2d(layer1, 32, 3, strides=2, padding='same')\n        layer2 = tf.layers.batch_normalization(layer2, training=True)\n        layer2 = tf.maximum(alpha * layer2, layer2)\n        layer2 = tf.nn.dropout(layer2, keep_prob=0.8)\n        \n        # 32 x 32 x 64\n        layer3 = tf.layers.conv2d(layer2, 64, 3, strides=2, padding='same')\n        layer3 = tf.layers.batch_normalization(layer3, training=True)\n        layer3 = tf.maximum(alpha * layer3, layer3)\n        layer3 = tf.nn.dropout(layer3, keep_prob=0.8)\n        \n        # 16*16*128\n        layer4 = tf.layers.conv2d(layer3, 128, 3, strides=2, padding='same')\n        layer4 = tf.layers.batch_normalization(layer4, training=True)\n        layer4 = tf.maximum(alpha * layer4, layer4)\n        \n        \n         # 8*8*256\n        layer5 = tf.layers.conv2d(layer4, 256, 3, strides=2, padding='same')\n        layer5 = tf.layers.batch_normalization(layer5, training=True)\n        layer5 = tf.maximum(alpha * layer5, layer5)\n        \n         # 4*4*512\n        layer6 = tf.layers.conv2d(layer5, 512, 3, strides=2, padding='same')\n        layer6 = tf.layers.batch_normalization(layer6, training=True)\n        layer6 = tf.maximum(alpha * layer6, layer6)\n        \n    \n        \n        text_emb = tf.layers.dense(condition_label, 512)\n        text_emb = tf.maximum(alpha * text_emb, text_emb)\n        text_emb = tf.expand_dims(text_emb, 1)\n        text_emb = tf.expand_dims(text_emb, 2)\n        text_emb = tf.tile(text_emb, [1,4,4,1])\n        \n        layer_concat = tf.concat([layer6, text_emb], 3)\n        \n        layer7 = tf.layers.conv2d(layer_concat, 512, 1, strides=1, padding='same')\n        layer7 = tf.layers.batch_normalization(layer7, training=True)\n        layer7 = tf.maximum(alpha * layer7, layer7)\n        \n        flatten = tf.reshape(layer7, (-1, 4*4*512))\n        logits = tf.layers.dense(flatten, 1)\n        outputs = tf.sigmoid(logits)\n        \n        return logits, outputs","86ad0ea5":"def get_loss(inputs_image, wrong_image, inputs_noise, condition_label, image_depth, smooth=0.1):\n    g_outputs = get_generator(inputs_noise, image_depth, condition_label, is_train=True)\n    d_logits_real, d_outputs_real = get_discriminator(inputs_image, condition_label)\n    d_logits_fake, d_outputs_fake = get_discriminator(g_outputs, condition_label, reuse=True)\n    d_logits_wrong, d_outputs_wrong = get_discriminator(wrong_image, condition_label, reuse=True)\n    \n    print(inputs_image.get_shape(), condition_label.get_shape())\n    \n    # \u8ba1\u7b97Loss\n    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake, \n                                                                    labels=tf.ones_like(d_outputs_fake)*(1-smooth)))\n    \n    #g_loss_l1 = tf.reduce_mean(tf.abs(g_outputs - inputs_image))\n    \n    #g_loss = g_loss_ + g_loss_l1\n    \n    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n                                                                         labels=tf.ones_like(d_outputs_real)*(1-smooth)))\n    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n                                                                         labels=tf.ones_like(d_outputs_fake)*smooth))\n    d_loss_wrong = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_wrong,\n                                                                         labels=tf.ones_like(d_outputs_wrong)*smooth))\n    \n    d_loss = d_loss_real + d_loss_fake + d_loss_wrong\n    \n    return g_loss, d_loss","4d662cc4":"def get_optimizer(g_loss, d_loss, beta1=0.4, learning_rate=0.001):\n    train_vars = tf.trainable_variables()\n    \n    g_vars = [var for var in train_vars if var.name.startswith(\"generator\")]\n    d_vars = [var for var in train_vars if var.name.startswith(\"discriminator\")]\n    \n    # Optimizer\n    with tf.control_dependencies(tf.get_collection(tf.GraphKeys.UPDATE_OPS)):\n        g_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(g_loss, var_list=g_vars)\n        d_opt = tf.train.AdamOptimizer(learning_rate, beta1=beta1).minimize(d_loss, var_list=d_vars)\n    \n    return g_opt, d_opt","94f11823":"def plot_images(samples):\n    #samples = (samples+1)\/2\n    fig, axes = plt.subplots(nrows=1, ncols=10, sharex=True, sharey=True, figsize=(20,2))\n    for img, ax in zip(samples, axes):\n        ax.imshow(img.reshape((256, 256, 3)))\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n    fig.tight_layout(pad=0)","7688cb19":"def show_generator_output(sess, n_images, inputs_noise, output_dim, test_text_vec):\n#    condition_text = tf.to_float(condition_text)\n#    last, b_size = sess.run(text_vec(condition_text, batch_size=n_images, reuse=True))\n    samples = sess.run(get_generator(inputs_noise, output_dim, test_text_vec, is_train=False))\n    return samples","3597690f":"# \u5b9a\u4e49\u53c2\u6570\nn_samples = 10\nlearning_rate = 0.0002\nbeta1 = 0.5","e65728e8":"# \u5b58\u50a8loss\nlosses = []\nstep = 0\nlast = text_rnn(input_text)\ng_loss, d_loss = get_loss(real_image_batch, wrong_image_batch, inputs_noise, last, image_depth, smooth=0.1)\ng_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    #sess.run(tf.global_variables_initializer())\n    model_file=tf.train.latest_checkpoint('..\/input\/gan-text-to-image-102flowers-rieyuguanghua')\n    saver.restore(sess, model_file)\n    \n    for epoch in range(631, 701):\n        index = np.random.permutation(len(all_image_filename))\n        data_text_emb = data_text_emb[index]\n        all_image_filename = all_image_filename[index]\n        wrong_image_filename = all_image_filename[np.random.permutation(len(all_image_filename))] \n        dataset_image = tf.data.Dataset.from_tensor_slices((all_image_filename, wrong_image_filename))\n        dataset_image = dataset_image.map(_pre_func)\n        dataset_image = dataset_image.repeat(1)\n        dataset_image = dataset_image.batch(batch_size)\n        \n        dataset_image_op = iterator.make_initializer(dataset_image)\n\n        sess.run(dataset_image_op)\n        i = 0\n        while True: \n        \n            try:\n                batch_noise = np.random.uniform(-1, 1, size=(batch_size, noise_dim))\n                text_emb_batch = data_text_emb[i: i + batch_size]\n                i = i + batch_size\n                _ = sess.run([g_train_opt, d_train_opt], feed_dict={input_text: text_emb_batch,\n                                                            inputs_noise: batch_noise})\n\n#               if step % 50 == 0:\n#                   saver.save(sess, \".\/model10.ckpt\")\n#                   train_loss_d = d_loss.eval({input_text: text_emb_batch,\n#                                               inputs_noise: batch_noise})\n#                   train_loss_g = g_loss.eval({input_text: text_emb_batch,\n#                                               inputs_noise: batch_noise})\n#                   \n#                   losses.append((train_loss_d, train_loss_g))\n#                   print(\"Step {}....\".format(step+1), \n#                         \"Discriminator Loss: {:.4f}....\".format(train_loss_d),\n#                         \"Generator Loss: {:.4f}....\". format(train_loss_g))\n                \n\n                    \n                    # \u663e\u793a\u56fe\u7247\n                step += 1    \n            #except tf.errors.OutOfRangeError as e:\n            except:\n#                saver.save(sess, \".\/model10.ckpt\")\n                print('epoch', epoch, 'step', step)\n                #print(e)\n                #try:\n                #    sess.run(real_image_batch)\n                #except Exception as e:\n                #    print(e)\n                break\n    \n        if epoch%10 == 0:\n            #saver.save(sess, \".\/model10.ckpt\")\n            n_samples = 10\n            condition_text = data_text_emb[:n_samples]\n            test_noise = np.random.uniform(-1, 1, size=[n_samples, noise_dim])\n            last_test = text_rnn(input_text, batch_size=n_samples, reuse=True)\n            test_text_vec = sess.run(last_test, feed_dict={input_text: condition_text})\n            samples = show_generator_output(sess, n_samples, test_noise, 3, test_text_vec)\n            plot_images(samples)\n    saver.save(sess, \".\/model11.ckpt\")","e67a7701":"!rm -rf 102flowers","1c687816":"vec = []\ntest_word = \"\"\"\nthe petals on this flower are yellow with a red center,the petals on this flower are yellow with a red center\n\"\"\"\nall_vec = [word_vectors[w] for w in test_word if w in word_vectors]\nvec.append(all_vec)\ndata = pd.Series(vec)\ndata = data.apply(pad, maxlength=maxlength)\ndata_ = np.concatenate(data).reshape(len(data),maxlength,100)\ntest_text_vec = data_\n\n\nlosses = []\nstep = 0\nlast = text_rnn(input_text)\ng_loss, d_loss = get_loss(real_image_batch, wrong_image_batch, inputs_noise, last, image_depth, smooth=0.1)\ng_train_opt, d_train_opt = get_optimizer(g_loss, d_loss, beta1, learning_rate)\n\n\n\nsaver = tf.train.Saver()\nwith tf.Session() as sess:\n    model_file=tf.train.latest_checkpoint('..\/input\/gan-text-to-image-102flowers-rieyuguanghua')\n    saver.restore(sess, model_file)\n    n_samples = 10\n    test_noise = np.random.uniform(-1, 1, size=[n_samples, noise_dim])\n    last_test = text_rnn(input_text, batch_size=n_samples, reuse=True)\n    test_text_vec = sess.run(last_test, feed_dict={input_text: test_text_vec})\n    samples = show_generator_output(sess, n_samples, test_noise, 3, test_text_vec)\n    plot_images(samples)"}}