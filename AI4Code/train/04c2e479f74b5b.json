{"cell_type":{"19e60091":"code","21257ac3":"code","def62be0":"code","c1b47e58":"code","67a9bade":"code","a787bd06":"code","8c5b85c7":"code","283d9b31":"code","9da6abf4":"code","e2fb71ac":"code","3db767a1":"code","2be1fdb0":"code","46a0dafa":"code","b2c22961":"code","d803e978":"code","6b741dbd":"code","2432f0a8":"code","295522b3":"code","7d10fd09":"code","5806f055":"code","0f4710b0":"code","9b1f6e33":"code","12ac7f6f":"code","1d274a4e":"code","ce167ea5":"code","44d04c91":"code","f9779d1b":"code","d8aff510":"code","4e9cf10d":"code","a64abc89":"code","b5fef224":"markdown","85b972ab":"markdown"},"source":{"19e60091":"import pandas as pd\nimport matplotlib\nimport numpy as np\nimport re\nimport nltk\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom gensim.models import KeyedVectors\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.callbacks import EarlyStopping\nfrom nltk.stem import SnowballStemmer\nfrom nltk.corpus import stopwords\nfrom keras import backend as K\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.impute import KNNImputer\nimport category_encoders as ce\nfrom sklearn.model_selection import cross_val_score\nimport langid\npd.set_option('display.max_rows', 1000)\nfrom keras.preprocessing.text import Tokenizer","21257ac3":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ndfs = [df_train, df_test]\ndf = df_train.copy() # just to fill free to mess up the data a bit:)\n","def62be0":"df","c1b47e58":"import re\nfrom nltk.stem import SnowballStemmer\nfrom nltk import WordNetLemmatizer\nimport nltk\nwnl = WordNetLemmatizer()\nfrom nltk.corpus import stopwords, wordnet\nnltk.download('wordnet')","67a9bade":"def cleanSentences(text, remove_stopwords=True, stem_words=True):\n    # Clean the text, with the option to remove stopwords and to stem words.\n    \n    # Convert words to lower case and split them\n    text = text.lower().replace(\"<br \/>\", \" \")\n    text = text.split()\n\n    # Optionally, remove stop words\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        text = [w for w in text if not w in stops]\n    \n    text = \" \".join(text)\n\n    # Clean the text\n    text = re.sub(r\"[^A-Za-z0-9^,!.\\\/'+-=]\", \" \", text)\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"cannot \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\",\", \" \", text)\n    text = re.sub(r\"\\.\", \" \", text)\n    text = re.sub(r\"!\", \" ! \", text)\n    text = re.sub(r\"\\\/\", \" \", text)\n    text = re.sub(r\"\\^\", \" ^ \", text)\n    text = re.sub(r\"\\+\", \" + \", text)\n    text = re.sub(r\"\\-\", \" - \", text)\n    text = re.sub(r\"\\=\", \" = \", text)\n    text = re.sub(r\"'\", \" \", text)\n    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n    text = re.sub(r\":\", \" : \", text)\n    text = re.sub(r\" e g \", \" eg \", text)\n    text = re.sub(r\" b g \", \" bg \", text)\n    text = re.sub(r\" u s \", \" american \", text)\n    text = re.sub(r\"\\0s\", \"0\", text)\n    text = re.sub(r\" 9 11 \", \"911\", text)\n    text = re.sub(r\"e - mail\", \"email\", text)\n    text = re.sub(r\"j k\", \"jk\", text)\n    text = re.sub(r\"\\s{2,}\", \" \", text)\n    \n    def get_wordnet_pos(tag):\n        if tag.startswith('J'):\n            return wordnet.ADJ\n        elif tag.startswith('V'):\n            return wordnet.VERB\n        elif tag.startswith('N'):\n            return wordnet.NOUN\n        elif tag.startswith('R'):\n            return wordnet.ADV\n        else:\n            return wordnet.NOUN\n    text = text.split()\n    text = nltk.tag.pos_tag(text)\n    text = [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in text]\n    text = [wnl.lemmatize(word, tag) for word, tag in text]\n    text = ' '.join(text)\n    # Optionally, shorten words to their stems\n#     if stem_words:\n#         text = text.split()\n#         stemmer = SnowballStemmer('english')\n#         stemmed_words = [stemmer.stem(word) for word in text]\n#         text = \" \".join(stemmed_words)\n    \n    # Return a list of words\n    return(text)","a787bd06":"df[\"clean\"] = df.text.apply(lambda x: cleanSentences(x))","8c5b85c7":"from sklearn.model_selection import train_test_split\nX = np.array(df.clean)\n\nY = np.array(df.target)\n\nY = Y.reshape(-1,1)\nprint(Y.shape)\n# Y_reg = Y_reg.reshape(-1,1)\n\n\nX_train, X_test, Y_train, Y_test,  = train_test_split(X, Y, test_size=0.1, random_state = 42)","283d9b31":"MAX_WORDS = 250000\nEMBEDDING_DIM = 300\ntok = Tokenizer(num_words=MAX_WORDS)\ntok.fit_on_texts(X)\nsequences = tok.texts_to_sequences(X_train)\nword_index = tok.word_index\nprint('Found %s unique tokens' % len(word_index))\n","9da6abf4":"MAX_SEQUENCE_LENGTH = 300\nsequences_matrix = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint(sequences_matrix)\nprint('Shape of data tensor:', sequences_matrix.shape)","e2fb71ac":"nb_words = min(MAX_WORDS, len(word_index))+1\nprint(nb_words)","3db767a1":"import spacy\n\nnlp = spacy.load('en_core_web_lg')","2be1fdb0":"\nembedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\nprint(embedding_matrix.shape)\nfor word, i in word_index.items():\n    embedding_matrix[i] = np.array(nlp(word)[0].vector)\n#     if word in word2vec.vocab:\n#         embedding_matrix[i] = Word2Vec(word, min_count=2)\n#         embedding_matrix[i] = word2vec.word_vec(word)\nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\nprint(embedding_matrix.shape)","46a0dafa":"def RNN():\n    inputs = Input(name='inputs',shape=[MAX_SEQUENCE_LENGTH])\n    layer = Embedding(nb_words, EMBEDDING_DIM, weights=[embedding_matrix],\n                      input_length=MAX_SEQUENCE_LENGTH, trainable=False)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FullyConnectedCls1')(layer)\n    layer = Activation('relu')(layer)\n    layer_branching = Dropout(0.5)(layer)\n    \n    \n    \n    layer = Dense(64,name='FullyConnectedCls2')(layer_branching)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    \n    layer = Dense(1,name='out_layer_cls')(layer)\n    layer_out_cls = Activation('sigmoid',name='ActivationCls2')(layer)\n    model = Model(inputs=inputs, outputs=layer_out_cls)\n    \n    \n    return model","b2c22961":"model = RNN()\n# model.summary()\n\nmodel.compile(loss='binary_crossentropy', optimizer=Adam(lr=0.00005), metrics=['acc'])","d803e978":"(trainX, testX, trainY, testY) = train_test_split(sequences_matrix, Y_train, test_size=0.2, random_state=42)\n\nhist = model.fit(trainX,\n                  trainY, batch_size=100,\n                 epochs=22,\n                 validation_data=(testX, testY)\n                )\n","6b741dbd":"matplotlib.pyplot.plot(hist.history['acc'])\nmatplotlib.pyplot.plot(hist.history['val_acc'])\nmatplotlib.pyplot.title('Classification model accuracy')\nmatplotlib.pyplot.ylabel('accuracy')\nmatplotlib.pyplot.xlabel('epoch')\nmatplotlib.pyplot.legend(['train', 'test'], loc='upper left')\nmatplotlib.pyplot.show()","2432f0a8":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)\naccr = model.evaluate(test_sequences_matrix,Y_test)\nprint(\"test loss, test acc:\", accr)\nprediction = model.predict(test_sequences_matrix)\nprediction_recovered = np.round(prediction * 10)\n\nbinary_prediction = [0 if item[0]<0.5 else 1 for item in prediction]\ndf_tesset = pd.DataFrame()\ndf_tesset['text'] = X_test\ndf_tesset['binary'] = binary_prediction\ndf_tesset['label'] = Y_test","295522b3":"df_test[\"clean\"] = df_test.text.apply(lambda x: cleanSentences(x))\nX_test = df_test[\"clean\"]","7d10fd09":"test_sequences = tok.texts_to_sequences(X_test)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=MAX_SEQUENCE_LENGTH)\n# accr = model.evaluate(test_sequences_matrix,Y_test)\n# print(\"test loss, test acc:\", accr)\nprediction = model.predict(test_sequences_matrix)\nprediction_recovered = np.round(prediction * 10)\n\nbinary_prediction = [0 if item[0]<0.5 else 1 for item in prediction]\ndf_tesset = pd.DataFrame()\ndf_tesset['id'] = df_test.id\ndf_tesset['target'] = binary_prediction\n","5806f055":"df_tesset.to_csv ('submission.csv', index = False, header=True, sep = \",\")","0f4710b0":"df_tesset = pd.DataFrame()\ndf_tesset['text'] = X_test\ndf_tesset['binary'] = binary_prediction\ndf_tesset['label'] = Y_test","9b1f6e33":"df_tesset","12ac7f6f":"len(df_tesset.loc[df_tesset.binary == df_tesset.label])\/len(df_tesset)","1d274a4e":"df[\"lang\"]  = df[\"clean\"].apply(lambda x: langid.classify(x)[0])","ce167ea5":"df.lang.value_counts()","44d04c91":"df.loc[df.lang == \"de\"]","f9779d1b":"df.location.value_counts()","d8aff510":"groups = df.groupby([df.keyword,df.target]).count()\ngroups.id","4e9cf10d":"fig = px.bar(groups, x = \"id\")\nfig.show()","a64abc89":"sns.countplot(df.target)","b5fef224":"**Importing necessary modules**","85b972ab":"**Load files from Kaggle**"}}