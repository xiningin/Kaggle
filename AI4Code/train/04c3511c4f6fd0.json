{"cell_type":{"e63a91ae":"code","0c4195f8":"code","d65250f0":"code","b85c4719":"code","20e638c1":"code","641fa3c8":"code","9f253e42":"code","0bc8f7b9":"code","7db45712":"code","127121e6":"code","e287ecc3":"code","33701759":"code","2319d6c0":"code","d7359b4d":"code","ac811cf5":"code","47d94a79":"code","4f51e120":"code","5b8cb26e":"code","1847aa7e":"code","822f7bee":"code","70205b1c":"code","0d8d4884":"code","815df252":"code","db46cd4f":"code","afc7d05b":"code","68af1275":"code","e56074e5":"code","7df773a1":"code","1c3eba9e":"code","93851a48":"code","f92d3713":"code","8ea189ec":"code","6a35af7b":"code","5a2aab16":"code","4833aff9":"code","10364e28":"code","16ebd121":"code","0e15a958":"code","9e68897f":"code","134a793c":"code","4882806e":"markdown","4f3f0934":"markdown","7190690a":"markdown","80d58ebf":"markdown","7a9b3e52":"markdown","da219553":"markdown","d3e27e29":"markdown","963ae285":"markdown","8bcb06f1":"markdown","7e993fa4":"markdown","305a91fd":"markdown","b59690bc":"markdown"},"source":{"e63a91ae":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import StandardScaler, RobustScaler\nfrom sklearn.metrics import mean_squared_error, r2_score, accuracy_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.impute import KNNImputer\nfrom sklearn.tree import DecisionTreeClassifier\n\n\nimport warnings\n\nfrom sklearn.exceptions import ConvergenceWarning\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(\"ignore\", category=ConvergenceWarning)\n\n\npd.pandas.set_option(\"display.max_columns\", None)\npd.set_option(\"display.float_format\", lambda x: '%.4f' % x)","0c4195f8":"# Lets conbine train and test datasets.\ntrain = pd.read_csv(\"..\/input\/house-price-dataset\/test.csv\")\ntest = pd.read_csv(\"..\/input\/house-price-dataset\/train.csv\")\ndf = train.append(test).reset_index()\ndf.head()","d65250f0":"def check_dataframe(dataframe):\n    \"\"\"\n    -> provides an overview of the data\n\n    :param dataframe:  dataframe to overview\n\n    \"\"\"\n\n    print(\"Data Frame Raws Lenght : \", dataframe.shape[0],\n          \"\\nData Frame Columns Lenght : \", dataframe.shape[1])\n\n    print(\"\\nData Frame Columns Names : \", list(dataframe.columns))\n\n    print(\"\\nIs data frame has null value? : \", dataframe.isnull().any())\n\n    print(\"\\nHow many missing values are in which columns? :\\n\", dataframe.isnull().sum())\n\n    cat_names = [col for col in dataframe.columns if dataframe[col].dtype == \"O\"]\n    num_names = [col for col in dataframe.columns if dataframe[col].dtype != \"O\"]\n\n    print(\"\\nHow many columns are in the object type? : \", len(cat_names), \"\\n\", cat_names)\n\n    print(\"\\nHow many columns are in the numerical type? : \", len(num_names), \"\\n\", num_names)","b85c4719":"def get_categorical_and_numeric_columns(dataframe, exit_columns, number_of_unique_classes=25):\n    \"\"\"\n    -> Determines categorical and numerical variables\n\n    :param dataframe: dataframe to process\n    :param exit_columns: Ignoring variable name\n    :param number_of_unique_classes: Frequency limit of classes of variables\n    :return: Returns the name of categorical classes as the first value and the name of numeric variables as the second value.\n\n    \"\"\"\n\n    categorical_columns = [col for col in dataframe.columns\n                           if len(dataframe[col].unique()) <= number_of_unique_classes]\n\n    numeric_columns = [col for col in dataframe.columns if len(dataframe[col].unique()) > number_of_unique_classes\n                       and dataframe[col].dtype != \"O\"\n                       and col not in exit_columns]\n\n    return categorical_columns, numeric_columns","20e638c1":"def cat_summary(dataframe, categorical_columns, target, plot=False):\n    \"\"\"\n    -> Shows the proportion of classes of categorical variables and the median at target.\n\n    :param dataframe: dataframe to process\n    :param categorical_columns: name of categorical variables\n    :param target: Dataframe'de ilgilendi\u011fimiz de\u011fi\u015fken.\n    :param plot: Argument for plotting graphics : True\/False\n\n    \"\"\"\n    for col in categorical_columns:\n        print(col, \" : \", dataframe[col].nunique(), \" unique classes.\\n\")\n\n        print(col, \" : \", dataframe[col].value_counts().sum(), \"\\n\")\n\n        print(pd.DataFrame({\"COUNT\": dataframe[col].value_counts(),\n                            \"RATIO ( % )\": 100 * dataframe[col].value_counts() \/ len(dataframe),\n                            \"TARGET_MEDIAN\": dataframe.groupby(col)[target].median()}), end=\"\\n\\n\\n\")\n\n        if plot:\n            sns.countplot(x=col, data=dataframe)\n\n            plt.show()","641fa3c8":"def hist_for_numeric_columns(dataframe, numeric_columns):\n    \"\"\"\n    -> Plots histograms of numerical variables.\n\n    :param dataframe: dataframe to process.\n    :param numeric_columns: name of numerical variables\n\n    \"\"\"\n    col_counter = 0\n\n    data = dataframe.copy()\n\n    for col in numeric_columns:\n        data[col].hist(bins=20)\n\n        plt.xlabel(col)\n\n        plt.title(col)\n\n        plt.show()\n\n        col_counter += 1\n\n    print(col_counter, \"variables have been plotted!\")","9f253e42":"def find_correlation(dataframe, numeric_columns, target, corr_limit=0.60):\n    \"\"\"\n    -> Examines the correlation of numeric variables with the target.\n\n    :param dataframe: dataframe to process\n    :param numeric_columns: name of numerical variables\n    :param target: Target variable to look at in the correlation relation\n    :param corr_limit: correlation limit. if it is below the limit is low correlation and above the boundary is high correlation.\n    :return: First value is variables with low correlation, second value is variables with high correlation\n    \"\"\"\n    high_correlations = []\n\n    low_correlations = []\n\n    for col in numeric_columns:\n        if col == target:\n            pass\n\n        else:\n            correlation = dataframe[[col, target]].corr().loc[col, target]\n\n            if abs(correlation) > corr_limit:\n                high_correlations.append(col + \" : \" + str(correlation))\n\n            else:\n                low_correlations.append(col + \" : \" + str(correlation))\n\n    return low_correlations, high_correlations","0bc8f7b9":"def target_summary_with_categorical_columns(dataframe, categorical_columns, target):\n    \"\"\"\n    -> Performs target analysis according to categorical variables.\n\n    :param dataframe: dataframe to process\n    :param categorical_columns: name of categorical variables \n    :param target: Name of the target variable to be analyzed\n    :return:\n    \"\"\"\n    for col in categorical_columns:\n        if col != target:\n            print(pd.DataFrame({\"Target_Median\": dataframe.groupby(col)[target].median()}), end=\"\\n\\n\\n\")","7db45712":"\ndef target_summary_with_numeric_columns(dataframe, numeric_columns, exit_columns, target):\n    \"\"\"\n    -> Target analysis according to numerical variables\n\n    :param dataframe: dataframe to process\n    :param numeric_columns: name of numerical variables\n    :param exit_columns: Name of variable not wanted to look at\n    :param target: Name of the target variable to be analyzed\n    :return:\n    \"\"\"\n\n    for col in numeric_columns:\n        if col != target or col != exit_columns:\n            print(dataframe.groupby(target).agg({col: np.median}), end=\"\\n\\n\\n\")","127121e6":"def outlier_thresholds(dataframe, variable, low_quantile=0.05, up_quantile=0.95):\n    \"\"\"\n    -> Calculates and returns the top and bottom outliers of the given value.\n\n    :param dataframe:Dataframe to be processed\n    :param variable: The name of the variable whose outlier is to be caught\n    :param low_quantile: The quantile value for the calculation of the lower threshold value\n    :param up_quantile: The quantile value for the calculation of the upper threshold value\n    :return: Returns the lower bound value of the variable given as the first value, the upper bound value as the second value\n    \"\"\"\n    quantile_one = dataframe[variable].quantile(low_quantile)\n\n    quantile_three = dataframe[variable].quantile(up_quantile)\n\n    interquantile_range = quantile_three - quantile_one\n\n    up_limit = quantile_three + 1.5 * interquantile_range\n\n    low_limit = quantile_one - 1.5 * interquantile_range\n\n    return low_limit, up_limit","e287ecc3":"def has_outliers(dataframe, numeric_columns, plot=False):\n    \"\"\"\n    -> Are there any outliers in numerical variables?\n    -> If there is, it performs the task of drawing box plot optionally.\n    -> It also returns the names of variables that have outliers..\n\n    :param dataframe:  Dataframe to be processed\n    :param numeric_columns: Numeric variable names to look for outliers\n    :param plot: Takes a bool value to plot the boxplot chart. True \/ False\n    :return: Returns the names of variables with outliers\n    \"\"\"\n    variable_names = []\n\n    for col in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, col)\n\n        if dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].any(axis=None):\n            number_of_outliers = dataframe[(dataframe[col] > up_limit) | (dataframe[col] < low_limit)].shape[0]\n\n            print(col, \" : \", number_of_outliers, \" ayk\u0131r\u0131 g\u00f6zlem.\")\n\n            variable_names.append(col)\n\n            if plot:\n                sns.boxplot(x=dataframe[col])\n                plt.show()\n\n    return variable_names","33701759":"def remove_outliers(dataframe, numeric_columns):\n    \"\"\"\n     In the dataframes, it deletes outlier observations of the given numeric variables and returns the dataframe.\n\n    :param dataframe: Dataframe to be processed\n    :param numeric_columns: Numeric variable names to delete outlier observations\n    :return: Outlier observations return deleted dataframe\n    \"\"\"\n\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n\n        dataframe_without_outliers = dataframe[~((dataframe[variable] < low_limit) | (dataframe[variable] > up_limit))]\n\n    return dataframe_without_outliers","2319d6c0":"def replace_with_thresholds(dataframe, numeric_columns):\n    \"\"\"\n    Suppersing method\n\n    It is the best alternative to not erasing.\n\n    Since Loc is used, it implements the operation in the dataframe.\n\n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param numeric_columns: Names of numerical variables whose outliers will be suppressed\n    \"\"\"\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","d7359b4d":"def missing_values_table(dataframe):\n    \"\"\"\n    Show that variables with missing values and returns those values.\n     \n    :param dataframe: Dataframe to be processed\n    :return: Returns the names of variables with missing values.\n    \"\"\"\n    variables_with_na = [col for col in dataframe.columns\n                         if dataframe[col].isnull().sum() > 0]\n\n    n_miss = dataframe[variables_with_na].isnull().sum().sort_values(ascending=False)\n\n    ratio = (dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n\n    # ratio = (100 * dataframe[variables_with_na].isnull().sum() \/ dataframe.shape[0]).sort_values(ascending=False)\n\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=[\"n_miss\", \"ratio\"])\n\n    print(missing_df)\n\n    return variables_with_na","ac811cf5":"# In terms of the dependent variable, we look at whether there is a deficiency or not.\n# If the ratio is 0.5 - 0.5, it is not important for the dependent variable.\n# It should be taken into account as the ratio increases.\n\ndef missing_vs_target(dataframe, target, variable_with_na):\n    \"\"\"\n    This function allows us to look at the effect of variables with missing values on the target variable.\n    Creates a new variable: examined value + _NA_FLAG\n    This new variable is assigned 1 when it is missing from the variable under examination otherwise 0.\n    Then, grouping according to these variables, the target is examined.\n    \n    :param dataframe: \u0130\u015flem yap\u0131lacak dataframe\n    :param target: Analizi yap\u0131lacak hedef de\u011fi\u015fkenin ad\u0131\n    :param variable_with_na: Eksik de\u011ferlere sahip de\u011fi\u015fkenlerin ad\u0131.\n\n    \"\"\"\n    temp_df = dataframe.copy()\n\n    for variable in variable_with_na:\n        temp_df[variable + \"_NA_FLAG\"] = np.where(temp_df[variable].isnull(), 1, 0)\n\n    flags_na = temp_df.loc[:, temp_df.columns.str.contains(\"_NA_\")].columns\n\n    for variable in flags_na:\n        print(pd.DataFrame({\"TARGET_MEDIAN\": temp_df.groupby(variable)[target].median()}),\n              end=\"\\n\\n\\n\")","47d94a79":"\ndef label_encoder(dataframe, categorical_columns):\n    \"\"\"\n    Encoding categorical variable with 2 classes\n\n    :param dataframe: Dataframe to be processed\n    :param categorical_columns: Categorical variable names to label encoding\n    :return:\n    \"\"\"\n    labelencoder = preprocessing.LabelEncoder()\n\n    for col in categorical_columns:\n\n        if dataframe[col].nunique() == 2:\n            dataframe[col] = labelencoder.fit_transform(dataframe[col])\n\n    return dataframe","4f51e120":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    \"\"\"\n\n    :param dataframe: Dataframe to be processed\n    :param categorical_columns: Categorical variable names to be applied to One-Hot Encode\n    :param nan_as_category: Should \"NaN\" create variables? True \/ False\n    :return: The new variable names created after One-hot encode operation executed.\n    \"\"\"\n    original_columns = list(dataframe.columns)\n\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n\n    return dataframe, new_columns","5b8cb26e":"\ndef rare_analyser(dataframe, categorical_columns, target, rare_perc):\n    \"\"\"\n    If any class of data frame variables has a frequency less than the given threshold, it shows those variables.\n\n    :param dataframe:Dataframe to be processed\n    :param categorical_columns: Categorical variable names to be Rare analyzed\n    :param target: Target variable name to analyze\n    :param rare_perc: Limit value for Rare. Those below fall into the rare category.\n    :return:\n    \"\"\"\n    rare_columns = [col for col in categorical_columns\n                    if (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        print(var, \" : \", len(dataframe[var].value_counts()))\n\n        print(pd.DataFrame({\"COUNT\": dataframe[var].value_counts(),\n                            \"RATIO\": dataframe[var].value_counts() \/ len(dataframe),\n                            \"TARGET_MEAN\": dataframe.groupby(var)[target].mean(),\n                            \"TARGET_MEDIAN\": dataframe.groupby(var)[target].median()}),\n              end=\"\\n\\n\\n\")\n\n    print(\"There are\", len(rare_columns), \"variables with rare classes.\")","1847aa7e":"def rare_encoder(dataframe, categorical_columns, rare_perc):\n    \"\"\"\n    -> It converts rare classes to rare.\n\n    -> It captures variables that have any class under the rare limit from the given categorical variables.\n\n    -> Later in these classes, the index captures the range that is below the limit rare.\n\n    -> Using the captured indexes, it writes \"Rare\" to classes that have these indexes in the temporary dataframe.\n\n    -> Returns the temporary dataframe that has occurred.\n\n    :param dataframe:Dataframe to be processed\n    :param rare_perc: Limit value for Rare. Those below fall into the rare category.\n    :param categorical_columns: Categorical variable names to be Rare analyzed\n    :return: Returns a temporary dataframe with classes written in Rare.\n    \"\"\"\n    temp_df = dataframe.copy()\n\n    rare_columns = [col for col in categorical_columns\n                    if (dataframe[col].value_counts() \/ len(dataframe) < rare_perc).any(axis=None)]\n\n    for var in rare_columns:\n        tmp = temp_df[var].value_counts() \/ len(temp_df)\n\n        rare_labels = tmp[tmp < rare_perc].index\n\n        temp_df[var] = np.where(temp_df[var].isin(rare_labels), \"Rare\", temp_df[var])\n\n    return temp_df","822f7bee":"def robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.10)\n            quartile3 = variable.quantile(0.99)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) \/ interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)","70205b1c":"check_dataframe(df)","0d8d4884":"categorical_columns, numerical_columns = get_categorical_and_numeric_columns(df, \"Id\")\nprint(\"\\nCategorical columns : \", categorical_columns,\n      \"\\n\\nNumeric Columns : \", numerical_columns)","815df252":"cat_summary(df, categorical_columns, \"SalePrice\")","db46cd4f":"hist_for_numeric_columns(df, numerical_columns)","afc7d05b":"low_corr_list, up_corr_list = find_correlation(df, numerical_columns, \"SalePrice\")\nprint(\"\\nHighly correlated list : \", len(up_corr_list))\nfor up in up_corr_list:\n    print(up)\nprint(\"\\nLow correlated list : \", len(low_corr_list))\nfor low in low_corr_list:\n    print(low)","68af1275":"miss_values = missing_values_table(df)","e56074e5":"df['Exterior1st'] = df['Exterior1st'].fillna(df['Exterior1st'].mode()[0])\n\ndf['MSZoning'] = df.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n\ndf[\"LotFrontage\"] = df.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(lambda x: x.fillna(x.median()))\n\ndf[\"Functional\"] = df[\"Functional\"].fillna(\"Typ\")\n\nfill_none_col = [\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\"GarageType\",\"GarageFinish\",\"GarageQual\",\"GarageCond\",\n                 \"BsmtQual\",\"GarageCond\",\"BsmtQual\",\"BsmtCond\",\"BsmtExposure\",\"BsmtFinType1\",\"BsmtFinType2\",\"MasVnrType\",\"MSSubClass\"]\nfor col in fill_none_col:\n    df[col] = df[col].fillna(\"None\")\n\nfill_0_col = [\"GarageYrBlt\",\"GarageArea\",\"GarageCars\",\"BsmtFinSF1\",\"BsmtFinSF2\",\"BsmtUnfSF\",\"TotalBsmtSF\",\"BsmtFullBath\",\"BsmtHalfBath\",\"MasVnrArea\"]\nfor col in fill_0_col:\n    df[col] = df[col].fillna(0)\n\nfill_mod_col = [\"Electrical\",\"KitchenQual\",\"Exterior2nd\",\"SaleType\"]\nfor col in fill_mod_col:\n    df[col] = df[col].fillna(df[col].mode()[0])","7df773a1":"#Checking missing values\nnew_miss_values = missing_values_table(df)","1c3eba9e":"rare_analyser(df, categorical_columns, \"SalePrice\", 0.02);","93851a48":"first_drop_list = [\"Street\",'LotFrontage','GarageYrBlt', \"Utilities\", \"Condition2\", \"RoofMatl\", \"PoolQC\", \"MiscFeature\",\"KitchenAbvGr\"]\nfor col in first_drop_list:\n    df.drop(col, axis=1, inplace=True)\n\n\nnew_categorical_columns, new_numerical_columns = get_categorical_and_numeric_columns(df, \"Id\")\nprint(\"\\nCategorical columns : \", new_categorical_columns,\n      \"\\n\\nNumeric Columns : \", new_numerical_columns)","f92d3713":"rare_analyser(df, new_categorical_columns, \"SalePrice\", 0.02)","8ea189ec":"rare_labels = [\"TA\", \"Fa\",\"Gd\"]\ndf[\"BsmtQual\"] = np.where(df[\"BsmtQual\"].isin(rare_labels),\"TF\",df[\"BsmtQual\"])\nrare_labels = [\"Gd\", \"TA\"]\ndf[\"BsmtCond\"] = np.where(df[\"BsmtCond\"].isin(rare_labels),\"GT\",df[\"BsmtCond\"])\ndf.BsmtExposure.replace(['Av','Gd','Mn','No'], [1,1,1, 0], inplace=True)\nrare_labels = [\"ALQ\", \"BLQ\",\"LwQ\",\"Rec\",\"Unf\"]\ndf[\"BsmtFinType1\"] = np.where(df[\"BsmtFinType1\"].isin(rare_labels),\"Rare\",df[\"BsmtFinType1\"])\nrare_labels = [\"LwQ\",\"Rec\"]\ndf[\"BsmtFinType2\"] = np.where(df[\"BsmtFinType2\"].isin(rare_labels),\"Rare\",df[\"BsmtFinType2\"])\nrare_labels = [\"ALQ\", \"GLQ\",\"Unf\"]\ndf[\"BsmtFinType2\"] = np.where(df[\"BsmtFinType2\"].isin(rare_labels),\"R\",df[\"BsmtFinType2\"])\nrare_labels = [\"Unf\", \"RFn\"]\ndf[\"GarageFinish\"] = np.where(df[\"GarageFinish\"].isin(rare_labels),\"Urf\",df[\"GarageFinish\"])\nrare_labels = [\"Fa\", \"Po\"]\ndf[\"GarageQual\"] = np.where(df[\"GarageQual\"].isin(rare_labels),\"R\",df[\"GarageQual\"])\nrare_labels = [\"Gd\", \"Ex\"]\ndf[\"GarageQual\"] = np.where(df[\"GarageQual\"].isin(rare_labels),\"ge\",df[\"GarageQual\"])\nrare_labels = [\"Attchd\", \"BuiltIn\"]\ndf[\"GarageType\"] = np.where(df[\"GarageType\"].isin(rare_labels),\"AB\",df[\"GarageType\"])\nrare_labels = [\"CarPort\", \"Detchd\"]\ndf[\"GarageType\"] = np.where(df[\"GarageType\"].isin(rare_labels),\"cd\",df[\"GarageType\"])\nrare_labels = [3,4]\ndf[\"GarageCars\"] = np.where(df[\"GarageCars\"].isin(rare_labels),3,df[\"GarageCars\"])\nrare_labels = [1,2]\ndf[\"GarageCars\"] = np.where(df[\"GarageCars\"].isin(rare_labels),1,df[\"GarageCars\"])\nrare_labels = [\"Ex\", \"TA\",\"Gd\"]\ndf[\"ExterCond\"] = np.where(df[\"ExterCond\"].isin(rare_labels),\"EGT\",df[\"ExterCond\"])\nrare_labels = [\"Fa\",\"Po\"]\ndf[\"ExterCond\"] = np.where(df[\"ExterCond\"].isin(rare_labels),\"FP\",df[\"ExterCond\"])\nrare_labels = [\"Floor\",\"Wall\",\"Grav\"]\ndf[\"Heating\"] = np.where(df[\"Heating\"].isin(rare_labels),\"FWG\",df[\"Heating\"])\nrare_labels = [\"Lvl\",\"Bnk\"]\ndf[\"LandContour\"] = np.where(df[\"LandContour\"].isin(rare_labels),\"LB\",df[\"LandContour\"])\nrare_labels = [\"CulDSac\",\"FR3\"]\ndf[\"LotConfig\"] = np.where(df[\"LotConfig\"].isin(rare_labels),\"CF\",df[\"LotConfig\"])\nrare_labels = [\"Corner\",\"FR2\",\"Inside\"]\ndf[\"LotConfig\"] = np.where(df[\"LotConfig\"].isin(rare_labels),\"CF2I\",df[\"LotConfig\"])\n\n\n# MSZoning \ndf.loc[df[\"MSZoning\"] == \"RH\", [\"MSZoning\"]] = \"RM\"\n\n# LotShape \ndf.loc[df[\"LotShape\"] == \"IR3\", [\"LotShape\"]] = \"IR1\"\ndf.loc[df[\"LotShape\"] == \"IR2\", [\"LotShape\"]] = \"IR1\"\n\n\n\n# LandSlope \ndf.loc[df[\"LandSlope\"] == \"Sev\", [\"LandSlope\"]] = \"Mod\"\n\n# Condition1 \ndf.loc[df[\"Condition1\"] == \"PosA\", [\"Condition1\"]] = \"NEW_PosAN\"\ndf.loc[df[\"Condition1\"] == \"PosN\", [\"Condition1\"]] = \"NEW_PosAN\"\n\ndf.loc[df[\"Condition1\"] == \"RRNe\", [\"Condition1\"]] = \"NEW_RRANe\"\ndf.loc[df[\"Condition1\"] == \"RRAe\", [\"Condition1\"]] = \"NEW_RRANe\"\n\ndf.loc[df[\"Condition1\"] == \"RRAn\", [\"Condition1\"]] = \"NEW_RRANn\"\ndf.loc[df[\"Condition1\"] == \"RRNn\", [\"Condition1\"]] = \"NEW_RRANn\"\n\n# HouseStyle \ndf.loc[df[\"HouseStyle\"] == \"2.5Fin\", [\"HouseStyle\"]] = \"2Story\"\n\n# OverallQual \ndf.loc[df[\"OverallQual\"] == 10, [\"OverallQual\"]] = 8\ndf.loc[df[\"OverallQual\"] == 9, [\"OverallQual\"]] = 8\n\ndf.loc[df[\"OverallQual\"] == 1, [\"OverallQual\"]] = 4\ndf.loc[df[\"OverallQual\"] == 2, [\"OverallQual\"]] = 4\ndf.loc[df[\"OverallQual\"] == 3, [\"OverallQual\"]] = 4\n\n\n# RoofStyle \ndf.loc[df[\"RoofStyle\"] == \"Mansard\", [\"RoofStyle\"]] = \"Hip\"\ndf.loc[df[\"RoofStyle\"] == \"Flat\", [\"RoofStyle\"]] = \"Hip\"\ndf.loc[df[\"RoofStyle\"] == \"Gambrel\", [\"RoofStyle\"]] = \"Gable\"\n\n# Exterior1st \ndf.loc[df[\"Exterior1st\"] == \"WdShing\", [\"Exterior1st\"]] = \"Wd Sdng\"\n\ndf = df[~(df[\"Exterior1st\"] == \"AsphShn\")]\ndf = df[~(df[\"Exterior1st\"] == \"CBlock\")]\ndf = df[~(df[\"Exterior1st\"] == \"ImStucc\")]\ndf = df[~(df[\"Exterior1st\"] == \"Stone\")]\ndf = df[~(df[\"Exterior1st\"] == \"BrkComm\")]\n\n# Exterior2nd \n\ndf.loc[df[\"Exterior2nd\"] == \"Wd Shng\", [\"Exterior2nd\"]] = \"Wd Sdng\"\n\ndf = df[~(df[\"Exterior2nd\"] == \"AsphShn\")]\ndf = df[~(df[\"Exterior2nd\"] == \"CBlock\")]\ndf = df[~(df[\"Exterior2nd\"] == \"ImStucc\")]\ndf = df[~(df[\"Exterior2nd\"] == \"Stone\")]\ndf = df[~(df[\"Exterior2nd\"] == \"BrkComm\")]\n\n# MasVnrType \ndf.loc[df[\"MasVnrType\"] == \"BrkCmn\", [\"MasVnrType\"]] = \"None\"\n\n# ExterQual \ndf.loc[df[\"ExterQual\"] == \"Fa\", [\"ExterQual\"]] = \"TA\"\n\n\n# Foundation \ndf.loc[df[\"Foundation\"] == \"Stone\", [\"Foundation\"]] = \"CBlock\"\ndf.loc[df[\"Foundation\"] == \"Slab\", [\"Foundation\"]] = \"CBlock\"\n\n\n# HeatingQC \n# df.loc[df[\"HeatingQC\"] == \"Po\", [\"HeatingQC\"]] = \"TA\"\ndf.loc[df[\"HeatingQC\"] == \"Fa\", [\"HeatingQC\"]] = \"TA\"\ndf = df[~(df[\"HeatingQC\"] == \"Po\")]\n\n\n# BedroomAbvGr \n\ndf.loc[df[\"BedroomAbvGr\"] == 8, [\"BedroomAbvGr\"]] = 4\ndf.loc[df[\"BedroomAbvGr\"] == 5, [\"BedroomAbvGr\"]] = 4\ndf.loc[df[\"BedroomAbvGr\"] == 6, [\"BedroomAbvGr\"]] = 4\ndf.loc[df[\"BedroomAbvGr\"] == 2, [\"BedroomAbvGr\"]] = 1\n\n# TotRmsAbvGrd \ndf.loc[df[\"TotRmsAbvGrd\"] == 11, [\"TotRmsAbvGrd\"]] = 10\ndf.loc[df[\"TotRmsAbvGrd\"] == 14, [\"TotRmsAbvGrd\"]] = 8\ndf.loc[df[\"TotRmsAbvGrd\"] == 3, [\"TotRmsAbvGrd\"]] = 4\n\n# TotRmsAbvGrd \ndf = df[~(df[\"TotRmsAbvGrd\"] == 0)]\ndf.loc[df[\"TotRmsAbvGrd\"] == 11, [\"TotRmsAbvGrd\"]] = 10\ndf.loc[df[\"TotRmsAbvGrd\"] == 14, [\"TotRmsAbvGrd\"]] = 8\ndf.loc[df[\"TotRmsAbvGrd\"] == 12, [\"TotRmsAbvGrd\"]] = 8\n\n# Functional \ndf.loc[df[\"Functional\"] == \"Maj1\", [\"Functional\"]] = \"NEW_Others\"\ndf.loc[df[\"Functional\"] == \"Maj2\", [\"Functional\"]] = \"NEW_Others\"\ndf.loc[df[\"Functional\"] == \"Min1\", [\"Functional\"]] = \"NEW_Others\"\ndf.loc[df[\"Functional\"] == \"Min2\", [\"Functional\"]] = \"NEW_Others\"\ndf.loc[df[\"Functional\"] == \"Mod\", [\"Functional\"]] = \"NEW_Others\"\ndf = df[~(df[\"Functional\"] == \"Sev\")]\n\n\n\n# FireplaceQu k\ndf.loc[df[\"FireplaceQu\"] == \"Po\", [\"FireplaceQu\"]] = \"No\"\ndf.loc[df[\"FireplaceQu\"] == \"Fa\", [\"FireplaceQu\"]] = \"TA\"\ndf.loc[df[\"FireplaceQu\"] == \"Ex\", [\"FireplaceQu\"]] = \"Gd\"\n\n\n# GarageCond \ndf.loc[df[\"GarageCond\"] == \"Po\", [\"GarageCond\"]] = \"No\"\ndf.loc[df[\"GarageCond\"] == \"Fa\", [\"GarageCond\"]] = \"No\"\ndf.loc[df[\"GarageCond\"] == \"Ex\", [\"GarageCond\"]] = \"TA\"\ndf.loc[df[\"GarageCond\"] == \"Gd\", [\"GarageCond\"]] = \"TA\"\n\n# Fence - \ndf.loc[df[\"Fence\"] == \"GdPrv\", [\"Fence\"]] = \"NEW_Fence\"\ndf.loc[df[\"Fence\"] == \"GdWo\", [\"Fence\"]] = \"NEW_Fence\"\ndf.loc[df[\"Fence\"] == \"MnPrv\", [\"Fence\"]] = \"NEW_Fence\"\ndf.loc[df[\"Fence\"] == \"MnWw\", [\"Fence\"]] = \"NEW_Fence\"\n\n# SaleType \ndf.loc[df[\"SaleType\"] == \"ConLI\", [\"SaleType\"]] = \"Con\"\ndf.loc[df[\"SaleType\"] == \"ConLw\", [\"SaleType\"]] = \"Con\"\ndf.loc[df[\"SaleType\"] == \"ConLD\", [\"SaleType\"]] = \"Con\"\ndf.loc[df[\"SaleType\"] == \"Oth\", [\"SaleType\"]] = \"Con\"\n\ndf.loc[df[\"SaleType\"] == \"CWD\", [\"SaleType\"]] = \"WD\"\n\n# SaleCondition \ndf.loc[df[\"SaleCondition\"] == \"AdjLand\", [\"SaleCondition\"]] = \"Normal\"\n\ndf.loc[df[\"SaleCondition\"] == \"Alloca\", [\"SaleCondition\"]] = \"Abnorml\"\ndf.loc[df[\"SaleCondition\"] == \"Family\", [\"SaleCondition\"]] = \"Normal\"","6a35af7b":"rare_analyser(df, new_categorical_columns, \"SalePrice\", 0.5)\nlast_miss = missing_values_table(df)\ndf.head()","5a2aab16":"df['New_TotalSF'] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']\n\ndf[\"NEW_TotalBathRoom\"] = df[\"FullBath\"] + df[\"HalfBath\"]*0.5 + df[\"BsmtFullBath\"] + df[\"BsmtHalfBath\"]*0.5\n\ndf[\"NEW_TotalFeet\"]= df[\"GrLivArea\"] + df[\"TotalBsmtSF\"]\n\ndf[\"New_Relation\"] = (df[\"OpenPorchSF\"]+df[\"EnclosedPorch\"] + df[\"ScreenPorch\"] + df[\"WoodDeckSF\"]) \/ (df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF'])","4833aff9":"df_temp = df\n\ndf.head()\n\ndf, new_cols_ohe = one_hot_encoder(df, new_categorical_columns)\n\noutlier_columns = has_outliers(df, new_numerical_columns)\n\nreplace_with_thresholds(df, new_numerical_columns)\n\ndf.head()\n","10364e28":"like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 20]\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Id\"\n                   and col not in \"SalePrice\"\n                   and col not in like_num]\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])","16ebd121":"train_df = df[df['SalePrice'].notnull()]\ntest_df = df[df['SalePrice'].isnull()]\n\nmissing_values_table(train_df)\nmissing_values_table(test_df)\n\ntest_df.isnull().sum()","0e15a958":"train_df.to_pickle(\".\/\/train.pkl\")\ntest_df.to_pickle(\".\/test.pkl\")","9e68897f":"drop_list = [\"index\", \"Id\"]\ntrain_df.drop(drop_list, axis=1, inplace=True)\n\nX = train_df.drop('SalePrice', axis=1) # bag\u0131ms\u0131z deg\u0131skenler\ny = np.ravel(train_df[[\"SalePrice\"]]) # bag\u0131ml\u0131 deg\u0131sken","134a793c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=46)\n\ny_train = np.ravel(y_train)\n\nmodels = [('LinearRegression', LinearRegression()),\n          ('Ridge', Ridge()),\n          ('Lasso', Lasso()),\n          ('ElasticNet', ElasticNet())]\n\nresults = []\nnames = []\n\nfor name, model in models:\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    result = np.sqrt(mean_squared_error(y_test, y_pred))\n    results.append(result)\n    names.append(name)\n\n    msg = \"%s: %f\" % (name, result)\n    print(msg)","4882806e":"# Step 5- Correlation Analysis","4f3f0934":"# Step 2- Obtaining Categorical and Numerical Variables","7190690a":"**Lets get start to use our functions.**\n","80d58ebf":"**Save the final version of the data frames as a pickle**","7a9b3e52":"# Step 1 - Overview","da219553":"# Step 9- Feature Engineering","d3e27e29":"\n# Step 6 -Missing Value Analysis","963ae285":"# Step 8- Rare Analysis","8bcb06f1":"# Step 3 - Categorical Variable vs Target Analysis","7e993fa4":"There is only one class in these variables 96% and above so these don't have information for us.","305a91fd":"# Step 7 - Filling in missing values","b59690bc":"# Step 4- Histogram Plot for Numerical Variables"}}