{"cell_type":{"c84d6b21":"code","dd332810":"code","950c8847":"code","4cae1f78":"code","373d529e":"code","c27dccbf":"code","1a90e5d5":"code","ec83f868":"code","551a96a2":"code","a8233541":"code","796e6317":"code","4fd7dfc8":"code","becf7476":"code","83bfd34d":"code","ed05d1d1":"code","055a7ce8":"code","ad95bb43":"code","f0070122":"code","80cf296f":"code","216055e5":"code","82379921":"code","24fd45da":"code","95adc4bb":"code","2a89b27c":"code","aeea1d88":"code","5cc98879":"code","b18d0b93":"code","8c2a556f":"code","69dc4aa4":"code","e5f41541":"code","e6b53bdd":"code","2a3f9d44":"code","6a511607":"code","8d68f5b7":"code","e01055f3":"code","6d2835dc":"code","fdf4f346":"code","4bf93aa9":"code","163e34af":"code","d716944c":"code","adbd344d":"code","43736efc":"code","21491558":"code","a61edb67":"code","ba8ee297":"code","d745a16e":"code","da070908":"code","d9625d37":"code","8af91aac":"code","3e4fb6cd":"code","19ccb032":"code","38892ec3":"code","2f005c97":"code","e8f54a41":"code","890cad31":"code","48120200":"code","90373599":"code","2dd7c85b":"code","a5785fe6":"code","a09ca4bd":"code","dbd0e00e":"code","5dc149dd":"code","0331647a":"code","267a0406":"code","0a216104":"code","d33fc696":"code","9c34ddf8":"code","e45a9ea2":"code","70442271":"code","4e60dbf5":"code","811f1617":"code","6cc06e60":"code","26d36acc":"code","bdae2a4c":"code","b0001873":"code","3e1b1c6a":"code","d86d2425":"code","4c706315":"code","40eb65a7":"code","420e3666":"code","019d1a65":"code","e8fe97f3":"code","5a3fd412":"code","2ec2fd06":"code","436d8a9d":"code","dc0029fc":"code","626b35bf":"code","41cb8ff9":"code","08af85ec":"code","ccdf2bf1":"code","af16f8ec":"markdown","93351703":"markdown","e4f7363b":"markdown","a4d94aab":"markdown","e3456f57":"markdown","33f9f61b":"markdown","c790c1f5":"markdown","b0fd2d75":"markdown","43ba0010":"markdown","2b79c7bc":"markdown","f21d4603":"markdown","3558f20d":"markdown","97a33020":"markdown","3c69c27e":"markdown","ac37c29a":"markdown","e9c29c1e":"markdown","38cd6c86":"markdown","729491df":"markdown","e78eb7d4":"markdown","ef74c58e":"markdown","53c01b15":"markdown","d40d6715":"markdown","11c5e4a9":"markdown","c4944a12":"markdown","e9c0acac":"markdown","ab0ba840":"markdown","9170b798":"markdown","3701b6ac":"markdown","ec821236":"markdown","afbd7886":"markdown","972a7b91":"markdown","f49f7e67":"markdown","03516974":"markdown","2181c7e6":"markdown","d544ddf5":"markdown","de3076db":"markdown","0802de45":"markdown","6c6522c4":"markdown","5d71de4c":"markdown","7ddd9e6c":"markdown","5600eb4f":"markdown","14a0e010":"markdown","626e5a4d":"markdown","eeff3ed7":"markdown"},"source":{"c84d6b21":"import os\nimport pandas as pd\nfrom pandas import DataFrame\nimport pylab as pl\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport matplotlib.pyplot as plt\n%matplotlib inline","dd332810":"boston=pd.read_csv(\"..\/input\/boston.csv\") #Importing Data","950c8847":"pd.options.display.float_format = '{:.2f}'.format\nprint(boston.shape)     #Checking rows and Column\nprint(boston.head())    #Checking the data firts 5 Columns ","4cae1f78":"boston.info()  #Checking for Nulls and Datatypes of each Predictors","373d529e":"pd.options.display.float_format = '{:.4f}'.format\nboston.describe()","c27dccbf":"plt.figure(figsize=(15,10))\nboston.boxplot(patch_artist=True,vert=False)","1a90e5d5":"for k, v in boston.items():\n    q1 = v.quantile(0.25)\n    q3 = v.quantile(0.75)\n    irq = q3 - q1\n    v_col = v[(v <= q1 - 1.5 * irq) | (v >= q3 + 1.5 * irq)]\n    perc = np.shape(v_col)[0] * 100.0 \/ np.shape(boston)[0]\n    print(\"Column %s outliers = %.2f%%\" % (k, perc))","ec83f868":"pd.options.display.float_format = '{:.4f}'.format\nmy_corr=boston.corr()\nmy_corr","551a96a2":"plt.figure(figsize=(15,10))\nsns.heatmap(my_corr,linewidth=0.5)\nplt.show()","a8233541":"pearson_coef, p_value = stats.pearsonr(boston['CRIM'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of CRIM is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"CRIM\", y=\"MV\", data=boston)\nplt.ylim(0,)","796e6317":"sns.distplot(boston['CRIM'])       # We can see that CRIM data is Positevely Skewed and also has an outliers","4fd7dfc8":"pearson_coef, p_value = stats.pearsonr(boston['INDUS'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of INDUS is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"INDUS\", y=\"MV\", data=boston)\nplt.ylim(0,)","becf7476":"sns.distplot(boston['INDUS'])                  # this predictor shows Multimodal Distribution  ","83bfd34d":"pearson_coef, p_value = stats.pearsonr(boston['NOX'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of NOX is\", pearson_coef, \" with a P-value of P =\", p_value) \nsns.regplot(x=\"NOX\", y=\"MV\", data=boston)\nplt.ylim(0,)","ed05d1d1":"sns.distplot(boston['NOX'])     ","055a7ce8":"pearson_coef, p_value = stats.pearsonr(boston['RM'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of RM is\", pearson_coef, \" with a P-value of P =\", p_value) \nsns.regplot(x=\"RM\", y=\"MV\", data=boston)\nplt.ylim(0,)","ad95bb43":"sns.distplot(boston['RM'])          # This distribution seems Normally distributed ","f0070122":"pearson_coef, p_value = stats.pearsonr(boston['AGE'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of AGE is\", pearson_coef, \" with a P-value of P =\", p_value) \nsns.regplot(x=\"AGE\", y=\"MV\", data=boston)\nplt.ylim(0,)","80cf296f":"sns.distplot(boston['AGE'])","216055e5":"pearson_coef, p_value = stats.pearsonr(boston['DIS'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of DIS is\", pearson_coef, \" with a P-value of P =\", p_value) \nsns.regplot(x=\"DIS\", y=\"MV\", data=boston)\nplt.ylim(0,)","82379921":"sns.distplot(boston['DIS'])","24fd45da":"pearson_coef, p_value = stats.pearsonr(boston['TAX'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of TAX is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"TAX\", y=\"MV\", data=boston)\nplt.ylim(0,)","95adc4bb":"sns.distplot(boston['TAX'])    # Multimodal Distribution","2a89b27c":"pearson_coef, p_value = stats.pearsonr(boston['PT'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of PT is\", pearson_coef, \" with a P-value of P =\", p_value) \nsns.regplot(x=\"PT\", y=\"MV\", data=boston)\nplt.ylim(0,)","aeea1d88":"sns.distplot(boston['PT'])","5cc98879":"pearson_coef, p_value = stats.pearsonr(boston['B'], boston['MV'])\nprint(\"The Pearson Correlation Coefficient of B is\", pearson_coef, \" with a P-value of P =\", p_value)  \nsns.regplot(x=\"B\", y=\"MV\", data=boston)\nplt.ylim(0,)","b18d0b93":"sns.distplot(boston['B'])","8c2a556f":"sns.distplot(boston['MV'])     # Distribution of Target Variable ","69dc4aa4":"Corr_result=[['CRIM',-0.3883046116575091,1.1739862423663313e-19], ['INDUS',-0.4837251712814335,4.900242319351878e-31], ['NOX',-0.4273207763683765,7.06503408465202e-24], ['RM',0.695359937127267,2.4872456897496148e-74], ['AGE',-0.37695456714288655,1.5699814570835983e-18], ['DIS',0.24992873873512159,1.206610952424503e-08], ['TAX',-0.46853593528654547,5.637730675534297e-29], ['PT',-0.5077867038116088,1.609499278902899e-34], ['B',0.3334608226834166,1.3181119682130765e-14]]\nPearson_Pvalue = pd.DataFrame(Corr_result, columns = ['Predictors', 'pearson_Correlation','P-value'])\npd.options.display.float_format = '{:.5f}'.format\nPearson_Pvalue ","e5f41541":"#Variance Inflation Factor\n\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(boston)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","e6b53bdd":"boston.columns","2a3f9d44":"from sklearn.model_selection import train_test_split","6a511607":"X_train, X_test, Y_train, Y_test = train_test_split(boston.drop(\"MV\", axis=1), boston['MV'], test_size = 0.2,\\\n                                                    random_state=112)","8d68f5b7":"from sklearn.linear_model import LinearRegression","e01055f3":"my_model = LinearRegression(normalize=True)  #Create an object of LinearRegression class.","6d2835dc":"my_model.fit(X_train, Y_train)               #Fitting the linear regression model to our training set.","fdf4f346":"predictions = my_model.predict(X_test)       #Make predictions on the test set","4bf93aa9":"pd.DataFrame({'actual value': Y_test, 'predictions':predictions}).sample(5)   #Compare a sample of 5 actual Y values from test set and corresponding predicted values ","163e34af":"my_model.score(X_test, Y_test)           #Check the  R2  value","d716944c":"my_model.coef_","adbd344d":"my_model.intercept_","43736efc":"from sklearn import metrics","21491558":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","a61edb67":"plt.scatter(Y_test,predictions)\nplt.xlabel('Y_test')\nplt.ylabel('Predicted_Y')","ba8ee297":"import statsmodels.api as sm         #Import statsmodels API","d745a16e":"from sklearn.model_selection import train_test_split  #Divide the data into train and test sets","da070908":"X_train, X_test, Y_train, Y_test = train_test_split(boston.drop(\"MV\", axis=1), boston['MV'], test_size = 0.2,\\\n                                                    random_state=112)","d9625d37":"X_train = sm.add_constant(X_train)   #Add the constant term to the training data","8af91aac":"my_model = sm.OLS(Y_train, X_train)  #Fit the OLS model","3e4fb6cd":"result = my_model.fit()","19ccb032":"print(result.summary()) ","38892ec3":"from sklearn.metrics import r2_score","2f005c97":"predictions = result.predict(sm.add_constant(X_test))","e8f54a41":"r2_score(Y_test, predictions)","890cad31":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","48120200":"plt.scatter(Y_test,predictions)\nplt.xlabel('Y_test')\nplt.ylabel('Predicted_Y')","90373599":"X_train = X_train.drop(\"TAX\", axis=1)","2dd7c85b":"updated_model_result = sm.OLS(Y_train, X_train).fit()","a5785fe6":"print(updated_model_result.summary())","a09ca4bd":"from sklearn.metrics import r2_score","dbd0e00e":"X_test = X_test.drop(\"TAX\", axis=1)","5dc149dd":"predictions = updated_model_result.predict(sm.add_constant(X_test))","0331647a":"r2_score(Y_test, predictions)","267a0406":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","0a216104":"plt.scatter(Y_test,predictions)\nplt.xlabel('Y_test')\nplt.ylabel('Predicted_Y')","d33fc696":"X_train = X_train.drop(\"INDUS\", axis=1)","9c34ddf8":"updated_model_result1 = sm.OLS(Y_train, X_train).fit()","e45a9ea2":"print(updated_model_result1.summary())","70442271":"X_test = X_test.drop(\"INDUS\", axis=1)","4e60dbf5":"predictions = updated_model_result1.predict(sm.add_constant(X_test))","811f1617":"r2_score(Y_test, predictions)","6cc06e60":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","26d36acc":"# Visual inspection of Measured and Predicted\nfig, ax = plt.subplots()\nax.scatter(Y_test, predictions)\nax.plot([Y_test.min(), Y_test.max()], [Y_test.min(), Y_test.max()], 'k--', lw=4)\nax.set_xlabel('measured')\nax.set_ylabel('predicted')\nplt.show()","bdae2a4c":"sns.residplot(predictions,Y_test)    # Residual Plot","b0001873":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(boston)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","3e1b1c6a":"Model_Iterations=[['Trial 1(a)',0.7256164320858531,3.527748598537018,25.057161469613312, 5.00571288325782], ['Trial 1(b)',0.7256164320858531,3.5277485985370145,25.05716146961326, 5.005712883257814], ['Trial 2',0.7259104836332606,3.508617903749012,25.030308195709896,5.003029901540655], ['Trial 3',0.734199332192343,3.462822477847541,24.27335682897541,4.926799856801107]]\nScores = pd.DataFrame(Model_Iterations, columns = ['Iterations','R-square_Score','MAE', 'MSE','RMSE '])\npd.options.display.float_format = '{:.5f}'.format\nScores","d86d2425":"sns.distplot(boston['CRIM'])","4c706315":"boston['log_CRIM'] = np.log(boston['CRIM'])\n\n#boston.hist('log_DIS',figsize=(8,5))\n#plt.title('MV vs log(CRIM)')\n#plt.ylabel('MV')\n#plt.xlabel(\"log(CRIM)\")\n#sns.distplot(boston['log_CRIM'])\nsns.jointplot(x=\"log_CRIM\", y=\"MV\", data=boston, kind=\"reg\");","40eb65a7":"boston.columns","420e3666":"import statsmodels.formula.api as smf","019d1a65":"r_style_model = smf.ols('MV~INDUS+NOX+RM+AGE+DIS+TAX+PT+B+log_CRIM', data=boston)","e8fe97f3":"result = r_style_model.fit()","5a3fd412":"print(result.summary())","2ec2fd06":"print('MAE',metrics.mean_absolute_error(Y_test,predictions))\nprint('MSE',metrics.mean_squared_error(Y_test,predictions))\nprint('RMSE',np.sqrt(metrics.mean_squared_error(Y_test,predictions)))","436d8a9d":"plt.scatter(Y_test,predictions)\nplt.xlabel('Y_test')\nplt.ylabel('Predicted_Y')","dc0029fc":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(boston)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","626b35bf":"boston.drop(['TAX'],axis=1,inplace=True)","41cb8ff9":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(boston)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","08af85ec":"boston.drop(['INDUS'],axis=1,inplace=True)","ccdf2bf1":"from statsmodels.stats.outliers_influence import variance_inflation_factor as vif\nfrom statsmodels.tools.tools import add_constant\nX = add_constant(boston)\nvifs = [vif(X.values, i) for i in range(len(X.columns))]\npd.Series(data=vifs, index=X.columns).sort_values(ascending=False)","af16f8ec":"#### TRIAL 2: DROPPING TAX COLUMN & THEN GETTING THE SCORE","93351703":"# Interpreting Coefficients \n$$ Yhat = a + b_1 X_1 + b_3 X_3 + b_4 X_4 + b_5 X_5+ b_6 X_6+ b_8 X_8 + b_9 X_9 $$","e4f7363b":"## Interpretation of Result on Basis of Final Model : Trial 3","a4d94aab":"### Summary of Above Hypothesis Testing:\nFrom the above **Hypothesis Testing** result conducted separately with each predictor with target variable, with  **95% Confidence** and at 5% Significance level,  I can conclude that the **P-values** for all predictors are less than significance level and therefore **I reject the Null Hypothesis** which states that there is predictors cannot do predictions of Median value of Houses in Boston and therefore **I accept the Alternative Hypothesis** ststing that predictors can predict the Median value of Houses in Boston","e3456f57":"### Final Model\n\n$$ Yhat = 14.4501 + (-0.1313)*CRIM + (-17.6109)*NOX + (6.6400)*RM + (-0.0593)*AGE + (-1.2635)*DIS + (-0.9857)*PT + (0.0105)*B $$","33f9f61b":"#### Checking the Outliers Percentage in Each Columns","c790c1f5":"#### Importing Packages for Pre Analysis ","b0fd2d75":"#### CHECKING FOR MULTICOLLINEARITY USING VIF(variance_inflation_factor) ","43ba0010":"**AIC\/BIC:** (Akaike\u2019s Information Criteria\/Bayesian information criteria) **2535\/2567** which is least among above Trails of OLS Models. Thus we can say it is better model among the above iterations done.","2b79c7bc":"## Problem Statement:\n### Use Multiple Linear Regression to  Build a model that will predict housing values in Boston suburbs using various predictor variables that you have available using boston.csv dataset with the median value(MV) of owner-occupied homes as the target variable and the rest as predictors.","f21d4603":"### Multiple Linear Regression using Scikit-Learn \n\n#### TRIAL 1(A) : running Model with dropping any Predictor using Scikit","3558f20d":"Calculating Pearson Correlation and P-value to check which predictors are significant and which are not.\n\nDistribution Plot with Histogram has been plotted to view the distribution of Predicted Columns","97a33020":"### Hypothesis Testing","3c69c27e":"### Dataset Description\n<li> CRIM - per capita crime rate by town <\/li>\n<li>INDUS - proportion of non-retail business acres per town.<\/li>\n<li>NOX - nitric oxides concentration (parts per 10 million)<\/li>\n<li>RM - average number of rooms per dwelling<\/li>\n<li>AGE - proportion of owner-occupied units built prior to 1940<\/li>\n<li>DIS - weighted distances to five Boston employment centres<\/li>\n<li>TAX - full-value property-tax rate per $10,000<\/li>\n<li>PT - pupil-teacher ratio by town<\/li>\n\n<li>B - 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town<\/li>\n<li>MV - Median value of owner-occupied homes in $1000's<\/li>","ac37c29a":"We can see that the fitted values are reasonably close to the actual values, since the two distributions overlap a bit. However, there is definitely some room for improvement.","e9c29c1e":"While finding a Regression Score we have dropped TAX and INDUS Columns so lets drop manually and see what willbe the VIF Score ","38cd6c86":"**Adj. R-squared :** The **Adj. R-squared** value of the Final Model is **0.632**.Closer to **R-squared** So I cans ay we have included Relevant features.","729491df":"As we can see All predictors haave VIF Score > 5 which says that there is High Multicollinearity in Data if we use log_CRIM. ","e78eb7d4":"References:\n   1. https:\/\/canvas.uchicago.edu\/courses\/25628\/modules\/items\/905064\n   2. https:\/\/seaborn.pydata.org\/tutorial\/regression.html\n   3. https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/what-is-multicollinearity\/","ef74c58e":"### Result of  Summary Statistics for Boston Dataset and Boxplot to show the Outliers\n\nUsing .describe() in the dataframe we get the summary ststistics of whole dataset where we can make following infrenece by looking at mean, max and 75%  of data that  75 % data is around mean wheras maximum is 88.97 in CRIM column with which we can  say there are outliers present in this predictors.\nIn cloumn named B we can seee min value as 0.32 and Max value as 396.89 and Mean and 75% of data are close to 396.89 with which we can infer that there will be outliers in lower side of Quartile.\nThis will be more clear when we Plot a Boxplot using matplotlib library.","53c01b15":"#### Getting Summary Statistics of Dataset\n","d40d6715":"### Multiple Linear Regression using statsmodels\n\n#### TRIAL 1(B) : Running Model without dropping any Predictors","11c5e4a9":"We can see that VIF score of log_CRIM has value greater than 5 so converting CRIM to log_CRIm wasn't a great idea. ","c4944a12":"##### From the above distribution plot we can see that data in CRIM column is Right Skewed. So, I tried to do some feature engineering in that column by applying log. So that data somewhat becomes Normally Distributed.\nThe Codes for log conversion is given below","e9c0acac":"#### Checking for Multicollinearity in Multiple Regression\n1. VIF starts at 1 and has no upper limit\n2. VIF = 1, no correlation between the independent variable and the other variables\n3. VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n\nHere we can see that all the predictors are having VIF values less than 5. But still there are some multicollinearity in data as we can see NOX, INDUS, DIS are close to 5","ab0ba840":"**F-statistic:99.77**  & **Prob (F-statistic):2.16e-83** : F Statistics is used for accessing the Significance of overall regression model. \nHere: \n\n**Ho(Null Hypothesis): All coeffieients are 0 (i.e Model has no predictive capability)   \nHa(Alternative Hypothesis): Intercept only model is worse than Our current Model**\n\n**Significance Value: 0.05**\n\nSo, In this case our P-Value is less than Significance Value. therefore we will reject Null Hypothesis and Accept the alternative hypothesis which says Our current Model is better than Intercept only Model. \nAlso, F-Statistics is large. So we can say that there is linear relationship between Predictors and Target Varibales. ","9170b798":"**Prob(F-Statistic):** **2.16e-83** This tells the overall significance of the regression. This is to assess the significance level of all the variables together unlike the t-statistic that measures it for individual variables.Prob(F-statistics) depicts the probability of null hypothesis being true. As per the above results, probability is close to zero. This implies that overall the regressions is meaningful.","3701b6ac":"This Feature transformation is done just for Checking Model Accuracy and Making data normally distributed from Skewed","ec821236":"Importing linear Regression Package","afbd7886":"**Durbin-Watson: 1.966** Here value lies between 0-2 so we can say that predictors shows weak positive correlation between predictors and Traget variables.","972a7b91":"Since P value of TAX & INDUS is high which is 54.9% and 5.6%. Which seems higher than the defined Significance Value therefore we will drop the TAX & INDUS and Re-Run the models.","f49f7e67":"#### Loading Data for Analysis and Building Model\n","03516974":"**Consideration for Conducting Hypothesis Testing**\n\n**Null Hypothesis: $H_0$** :No Significant predictions of Median value of Houses in Boston by  Predictors('CRIM', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PT', 'B'). \n\n**Alternative Hypothesis: $H_a$** : Yes, there is Significant predictions of Median value of Houses in Boston by  Predictors('CRIM', 'INDUS', 'NOX', 'RM', 'AGE', 'DIS', 'TAX', 'PT', 'B'). \n\n**Significance Level : 5%**\n\n**Confidence Level : 95%**","2181c7e6":"The prediction made by the model is on the x-axis and the accuracy of the prediction is on the y-axis. The distance from the line at 0 is how bad the prediction was for that value.\nSince\u2026\nResidual = Observed \u2013 Predicted\nThe positive values for the residual (on the y-axis) mean the prediction was too low, and negative values mean the prediction was too high; 0 means the guess was exactly correct.","d544ddf5":"From the above table Trial(3) seems to have better R-square value.\n\nHigher the R-square and lower the errors better the accuracy of the model.","de3076db":"#### TRIAL 3: DROPPING INDUS & TAX COLUMN AND THEN GETTING THE SCORE","0802de45":"#### Correlation Matrix and Heatmap Showing correlation between Predictors and Target Variables\n","6c6522c4":"# Interpreting Coefficients \n$$ Yhat = a + b_1 CRIM + b_1 INDUS + b_2 NOX + b_3 RM + b_4 AGE + b_5 DIS + b_6 TAX + b_7 PT+ b_8 B $$","5d71de4c":"$$\nb_1 \\ CRIM :-0.1313  \\ For \\ a \\ Unit \\ Decrease \\ in \\ CRIM.\\ The \\ Median \\ value \\ of \\ house \\ Increases \\ by \\ b_1*MV(1000$)) \\\\\nb_3 \\ NOX  :-17.6109 \\ For \\ a \\ Unit \\ Decrease \\ in \\ NOX. \\ The \\ Median \\ value \\ of \\ house \\ Increases \\ by \\ b_3*MV(1000$)) \\\\\nb_4 \\ RM   :+6.6400  \\ For \\ a \\ Unit \\ Increase \\ in \\ RM.  \\ The \\ Median \\ value \\ of \\ house \\ Increases \\ by \\ b_4*MV(1000$)) \\\\\nb_5 \\ AGE  :-0.0593  \\ For \\ a \\ Unit \\ Increase \\ in \\ AGE. \\ The \\ Median \\ value \\ of \\ house \\ Decreases \\ by \\ b_5*MV(1000$)) \\\\\nb_6 \\ DIS  :-1.2635  \\ For \\ a \\ Unit \\ Increase \\ in \\ DIS. \\ The \\ Median \\ value \\ of \\ house \\ Decreases \\ by \\ b_6*MV(1000$)) \\\\\nb_8 \\ PT   :-0.9857  \\ For \\ a \\ Unit \\ Decrease \\ in \\ PT . \\ The \\ Median \\ value \\ of \\ house \\ Decreases \\ by \\ b_8*MV(1000$)) \\\\\nb_9 \\ B    :+0.0105  \\ For \\ a \\ Unit \\ Increase \\ in \\ B.   \\ The \\ Median \\ value \\ of \\ house \\ Increases \\ by \\ b_9*MV(1000$)) \\\\\n$$","7ddd9e6c":"**t-test**: \n**Ho: Coefficient of features is 0.**\n\n**Ha: Coefficient of features not equal to 0.**\n\n**Significance Value: 0.05**\n\nOn performing t-test we can see Higher the T-value higher the chance of rejecting the Null Hypothesis and accepting Alternative Hypothesis. In Trial 1(b) and Trial 2 we can see that  that P-value of some predictors is greater than Significance Value and therefore we drop those predictor.","5600eb4f":"**R-squared :** The R-squared value of the Final Model is **0.734199332192343**, which says that Mean prediction error is **73.4 %** smaller when we use Regression line than we employee the Mean.  \n**OR**\n**73.4%** of variance in MV(target variable) is explained by Predictors. \n","14a0e010":"Identifying Multicollinearity in Multiple Regression\n1. VIF starts at 1 and has no upper limit\n2. VIF = 1, no correlation between the independent variable and the other variables\n3. VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others\n\nHere we can see that all the predictors are having VIF values over 5 or 10 and thus we can say there is High Multicollinearity in data.","626e5a4d":"###  Additional Research for knowledge        #### Doing log transformation on CRIM column. ","eeff3ed7":"#### From the above r_style_model summary we can see converting CRIM to log_CRIM was not a good idea. As we can see 'INDUS', 'TAX', 'log_CRIM' are not significant. So Therefore we will not do any conversion for this model.\n\n##### We should check for VIF(Variation Inflation Factor) for Multicollinearity."}}