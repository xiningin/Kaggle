{"cell_type":{"718249bc":"code","a129c8fc":"code","769f21a8":"code","6cecbc1c":"code","66d574fb":"code","169aa2e8":"code","3d346369":"code","5c6cced2":"code","d8075095":"code","350208ef":"code","c66fdcd2":"code","1eb95e20":"code","e1e4a198":"code","01db11d8":"code","42c4dc71":"code","65342da3":"code","7b1dd7b1":"code","ba585bb7":"code","9fa0d86d":"code","f4965043":"code","d8725f6d":"code","e0446a65":"code","39920312":"code","14b5d210":"code","8774af28":"code","0248b5f1":"code","c6402b2f":"code","75abe64a":"code","02e9e93a":"code","b7209d27":"code","e4a02a91":"code","3e0f6eab":"code","46ec2832":"code","7c47ccd3":"code","ec5e64ab":"code","efc7e310":"code","c97e6f66":"code","9e53d434":"code","2ec8150c":"code","b5b3bec5":"code","48569c76":"code","5277a663":"code","275a2e55":"code","7b6564b1":"code","7689b953":"code","60dd5afc":"code","b0c2efe7":"code","3170c473":"code","237faae3":"code","456040b2":"code","03a42b42":"code","50569012":"code","ce41010c":"code","b5043f06":"code","356f3675":"code","7e996019":"code","b0c4deef":"code","96da2c68":"code","92379753":"code","239ff291":"code","6851393a":"code","17160dff":"code","3310a9de":"code","144a7d2f":"code","afbd534d":"code","e76b50d5":"code","21acb70f":"code","127b7245":"code","2de8cda6":"code","5476f39a":"code","6088ab11":"code","e2a14553":"code","bbd3152c":"code","cc10c263":"code","47314017":"code","8c4819e8":"code","0950a6b8":"markdown","68b9aaea":"markdown","f231d243":"markdown","58ac7d6f":"markdown","6a55ad93":"markdown","59e721bb":"markdown","60ce8c45":"markdown","9be7072e":"markdown","b7f36ce2":"markdown","758504cf":"markdown","0e1eb1a4":"markdown","7ef159af":"markdown","c0b00048":"markdown","1101bc67":"markdown","d6f44203":"markdown","6d54959d":"markdown"},"source":{"718249bc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a129c8fc":"#These installations are necessary in order to open all scans\n!conda install -c conda-forge gdcm -y\n!conda install -c conda-forge pillow -y\n!conda install -c conda-forge pydicom -y\n!conda install -c conda-forge tslearn -y","769f21a8":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\nimport time\nimport math\n\nimport cv2\nimport random\nimport pydicom\nimport warnings\n\nfrom glob import glob\nfrom tqdm import tqdm","6cecbc1c":"ROOT = '..\/input\/osic-pulmonary-fibrosis-progression'\ntrain_df = pd.read_csv(f'{ROOT}\/train.csv')\ntrain_df.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)\ngroupedbypatient = train_df.groupby('Patient')\npatientimages = groupedbypatient.Patient.first().to_numpy()","66d574fb":"INPUT_FOLDER = '\/kaggle\/input\/osic-pulmonary-fibrosis-progression\/train\/'","169aa2e8":"def load_scan(path):\n    \"\"\"\n    Loads scans from a folder and into a list.\n    \n    Parameters: path (Folder path)\n    \n    Returns: slices (List of slices)\n    \"\"\"\n    \n    slices = [pydicom.read_file(path + '\/' + s) for s in os.listdir(path)]\n    slices.sort(key = lambda x: int(x.InstanceNumber))   \n   \n        \n    return slices","3d346369":"def get_segmentation_model():\n    \n    class FixedDropout(tf.keras.layers.Dropout):\n        def _get_noise_shape(self, inputs):\n            if self.noise_shape is None:\n                return self.noise_shape\n\n            symbolic_shape = tf.keras.backend.shape(inputs)\n            noise_shape = [symbolic_shape[axis] if shape is None else shape\n                           for axis, shape in enumerate(self.noise_shape)]\n            return tuple(noise_shape)\n\n    def DiceCoef(y_trues, y_preds, smooth=1e-5, axis=None):\n        intersection = tf.reduce_sum(y_trues * y_preds, axis=axis)\n        union = tf.reduce_sum(y_trues, axis=axis) + tf.reduce_sum(y_preds, axis=axis)\n        return tf.reduce_mean((2*intersection+smooth) \/ (union + smooth))\n\n    def DiceLoss(y_trues, y_preds):\n        return 1.0 - DiceCoef(y_trues, y_preds)\n\n    get_custom_objects().update({'swish': tf.keras.layers.Activation(tf.nn.swish)})\n    get_custom_objects().update({'FixedDropout':FixedDropout})\n    get_custom_objects().update({'DiceCoef' : DiceCoef})\n    get_custom_objects().update({'DiceLoss' : DiceLoss})\n    \n    print('Load segmentation model...')\n    model = tf.keras.models.load_model('..\/input\/lung-ct-segmentation-pretrain\/osic_segmentation_model.h5')\n    return model","5c6cced2":"def scale_and_resize(dcm):\n    DIM = 256\n    image = dcm.pixel_array\n    image = ((image - np.min(image)) \/ (np.max(image) - np.min(image)) * 255).astype(np.uint8)\n\n    if image.shape[0] != 512 or image.shape[1] != 512:\n        old_x, old_y = image.shape[0], image.shape[1]\n        x = (image.shape[0] - 512) \/\/ 2\n        y = (image.shape[1] - 512) \/\/ 2\n        image = image[x : old_x-x, y : old_y-y]\n        image = image[:512, :512]\n\n    image = cv2.resize(image, (DIM,DIM), cv2.INTER_AREA)\n    image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n    return image\/ 255.0","d8075095":"def get_volume(patient, trace = False):\n    #loading\n    scans = load_scan(INPUT_FOLDER + patient)   \n    stack = np.array([scale_and_resize(s) for s in scans])    \n    \n    #masks\n    pred_masks = model.predict(stack, verbose=0)    \n    pred_masks = (pred_masks>0.5).astype(np.float32)    \n        \n    n = len(scans)\n    volume = []\n    pixelcount = 0    \n\n    #volume of each slice\n    for i in range(n):   \n        pixelfactor = scans[i].pixel_array.shape[0] * scans[i].pixel_array.shape[1] \/ (pred_masks.shape[1] * pred_masks.shape[2])\n        thickness = float(scans[i].SliceThickness)\n        if (i < n - 1 and hasattr(scans[i], 'SliceLocation') and hasattr(scans[i+1], 'SliceLocation')):\n            thickness = min(abs(scans[i].SliceLocation - scans[i+1].SliceLocation), thickness)        \n        voxelsize = float(scans[i].PixelSpacing[0]) * float(scans[i].PixelSpacing[1]) * thickness\n        pixelcount = np.sum(pred_masks[i].astype('int')) * pixelfactor\n        volume.append(pixelcount * voxelsize)    \n      \n    #linear interpolation between slices with spacing larger than slice thickness\n    totalvolume = 0\n    for i in range(n-1):\n        meanv = (volume[i] + volume[i + 1]) \/ 2\n        meanth = ((scans[i].SliceThickness + scans[i+1].SliceThickness) \/ 2)\n        if (hasattr(scans[i], 'SliceLocation') and hasattr(scans[i+1], 'SliceLocation')):\n            meanth = min(abs(scans[i].SliceLocation - scans[i+1].SliceLocation), meanth)\n        distance = meanth if (not hasattr(scans[i], 'SliceLocation') or not hasattr(scans[i+1], 'SliceLocation')) else np.abs(scans[i].SliceLocation - scans[i+1].SliceLocation)\n        totalvolume += volume[i] +  (distance - meanth) * meanv\n\n    totalvolume += volume[-1]\n    \n    del scans\n    del stack\n    del pred_masks\n\n    return totalvolume \/ (100*100*100) * 1000","350208ef":"def get_volumes(patients):\n    volumes = np.zeros(len(patients))\n    count = 0\n    for p in patients:\n        volumes[count] = get_volume(p,False)\n        print(volumes[count])\n        print(count)\n        count += 1      \n        \n    return volumes","c66fdcd2":"volumes = None\nvolumesPath = '..\/input\/osicpulmonaryfibrosislungvolumes\/LungVolumesInterpolated.npy'\n\ntry:\n    volumes = np.load(volumesPath) \nexcept:\n    import tensorflow as tf\n    from sklearn.model_selection import train_test_split\n    from tensorflow.keras.utils import get_custom_objects\n\n    warnings.filterwarnings('ignore')\n    print('Tensorflow version : {}'.format(tf.__version__))\n    \n    model = get_segmentation_model()\n    volumes = get_volumes(patientimages)\n    ","1eb95e20":"import tensorflow as tf\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.utils import get_custom_objects\n\nwarnings.filterwarnings('ignore')\nprint('Tensorflow version : {}'.format(tf.__version__))\n    \nmodel = get_segmentation_model()","e1e4a198":"ROOT = '..\/input\/osic-pulmonary-fibrosis-progression'\ntrain_df = pd.read_csv(f'{ROOT}\/train.csv')","01db11d8":"train_df.drop_duplicates(subset=['Patient','Weeks'], keep = False, inplace = True)","42c4dc71":"groupedfvc = groupedbypatient.FVC.apply(list).reset_index()['FVC']\ngroupedprecents = groupedbypatient.Percent.apply(list).reset_index()['Percent']\ngroupedweeks = groupedbypatient.Weeks.apply(list).reset_index()['Weeks']","65342da3":"maxfvc = np.array([max(groupedfvc[i]) for i in range(groupedfvc.shape[0])])\nmaxpercent = np.array([max(groupedprecents[i]) for i in range(groupedprecents.shape[0])])\nminweeks = np.array([min(groupedweeks[i]) for i in range(groupedweeks.shape[0])])","7b1dd7b1":"plt.scatter(maxfvc,volumes)","ba585bb7":"maskfraction = (maxfvc \/ volumes) < 0.8\nmaskfraction = maskfraction & ((maxfvc \/ volumes) > 0.4)\nmaskvolume = volumes < 6000\nmaskfv = maskfraction & maskvolume","9fa0d86d":"plt.scatter(maxfvc[maskfv],volumes[maskfv])","f4965043":"plt.scatter(maxfvc[maskfv]\/volumes[maskfv],maxpercent[maskfv])","d8725f6d":"patientimagesmasked = patientimages[maskfv]","e0446a65":"#Defining health using calculated volumes\nhealth = maxfvc[maskfv] \/ volumes[maskfv]","39920312":"#Defining health using Percent\nhealth = maxpercent[maskfv]","14b5d210":"#Testing the health metric\npercentmasked = maxpercent[maskfv]\nweeksmasked = minweeks[maskfv]\nmhpercent = percentmasked[np.max(health) == health][0]\nmcpercent = percentmasked[np.min(health) == health][0]\nmhminweek = weeksmasked[np.max(health) == health][0]\nmcminweek = weeksmasked[np.min(health) == health][0]\nprint('Most healthy patient by estimation has a percent of ' + str(mhpercent))\nprint('Most healthy patient by estimation has a min week of ' + str(mhminweek))\nprint('Most critical patient by estimation has a percent of ' + str(mcpercent))\nprint('Most critical patient by estimation has a min week of ' + str(mcminweek))","8774af28":"mosthealthypatient = patientimagesmasked[np.max(health) == health][0]\nmostcriticalpatient = patientimagesmasked[np.min(health) == health][0]","0248b5f1":"mhscan = load_scan(INPUT_FOLDER + mosthealthypatient)\nplt.imshow(mhscan[len(mhscan) \/\/ 2].pixel_array)","c6402b2f":"mcscan = load_scan(INPUT_FOLDER + mostcriticalpatient)\nplt.imshow(mcscan[len(mcscan) \/\/ 2].pixel_array)","75abe64a":"lastw = 0\nfor i in range(weeksmasked.shape[0]):\n    if np.max(weeksmasked[i]) > lastw:\n        lastw = np.max(weeksmasked[i])","02e9e93a":"minw = -100 #week marking an interval\nmaxw = 0 #week marking an interval\ninterval = np.zeros((2,weeksmasked.shape[0])) #intervals\nindex = 0\nminamount = 10#minmum amount of patients per interval\n\nj = maxw\nwhile j <= lastw + 1:\n    count = 0\n    while(count < minamount and maxw <= lastw + 1):\n        count = 0\n        for i in range(weeksmasked.shape[0]):\n            mask1 = minw <= weeksmasked[i]\n            mask2 = weeksmasked[i] < maxw\n            r = weeksmasked[i][mask1 & mask2]   \n            if r.shape[0] > 0:\n                count += 1\n        \n        maxw += 1\n        \n        \n    interval[0,index] = minw\n    interval[1,index] = maxw\n    index += 1\n    minw = maxw\n    maxw = minw + 1\n    j = maxw","b7209d27":"mask = np.zeros((interval.shape[0],interval.shape[1])).astype('bool')\nmask[0,:] = interval[1,:] > 0\nmask[1,:] = mask[0,:]","e4a02a91":"interval = interval[mask].reshape(2,int(interval[mask].shape[0]\/2)).astype('int')","3e0f6eab":"print('Intervals:')\ninterval","46ec2832":"columnlist = ['Patient']\nfor i in range(interval.shape[1]):\n    columnlist.append(str(interval[0,i]) + \"_\" + str(interval[1,i]))","7c47ccd3":"fvcmasked = np.array(groupedfvc[maskfv])\nvolumesmasked = volumes[maskfv]\npercentsmasked = np.array(groupedprecents[maskfv])","ec5e64ab":"fvcpervolintervalmeans_df = None\nfor i in range(len(patientimagesmasked)):\n    row = list()\n    patient = patientimagesmasked[i]\n    row.append(patient)\n    for j in range(interval.shape[1]):       \n        maskinterval = (weeksmasked[i] >= interval[0,j]) \n        maskinterval2 = weeksmasked[i] < (interval[1,j] - 1)         \n        mean = (np.mean(fvcmasked[i][maskinterval2 & maskinterval])) \/ volumesmasked[i]\n        row.append(mean)\n   \n    df = pd.DataFrame([row], columns=columnlist)\n    \n    if(i == 0):\n        fvcpervolintervalmeans_df = df\n    else:\n        fvcpervolintervalmeans_df = pd.concat([fvcpervolintervalmeans_df,df])","efc7e310":"def get_intervalmeans(fvcperv):\n    intervalmeans_df = None\n    for i in range(len(patientimagesmasked)):\n        row = list()\n        patient = patientimagesmasked[i]\n        row.append(patient)\n        for j in range(interval.shape[1]):       \n            maskinterval = (weeksmasked[i] >= interval[0,j]) \n            maskinterval2 = weeksmasked[i] < (interval[1,j] - 1)         \n            mean = (np.mean(fvcmasked[i][maskinterval2 & maskinterval])) \/ volumesmasked[i] if fvcperv else (np.mean(percentsmasked[i][maskinterval2 & maskinterval]))\n            row.append(mean)\n\n        df = pd.DataFrame([row], columns=columnlist)\n\n        if(i == 0):\n            intervalmeans_df = df\n        else:\n            intervalmeans_df = pd.concat([intervalmeans_df,df])\n            \n    return intervalmeans_df        ","c97e6f66":"fvcpervolintervalmeans_df = get_intervalmeans(True)\npercentintervalmeans_df = get_intervalmeans(False)","9e53d434":"fvcpervolintervalmeans_df.head()","2ec8150c":"def get_largest_slices(nslices, patient):\n    scans = load_scan(INPUT_FOLDER + patient)   \n    stack = np.array([scale_and_resize(s) for s in scans])    \n    pred_masks = model.predict(stack, verbose=0)    \n    pred_masks = (pred_masks>0.5).astype(np.float32)\n    ps = np.sum(pred_masks, axis = 1)\n    ps = np.sum(ps, axis = 1)\n    ps = ps.reshape(ps.shape[0])\n    \n    del scans\n    del stack\n    del pred_masks\n    \n    ps2 = ps.copy() \n    indices = []\n    indiceshelper = np.arange(0, ps.shape[0])\n    for i in range(nslices):\n        maxtemp = np.max(ps2)\n        indices.append(indiceshelper[ps == maxtemp][0])\n        ps2[ps2 == maxtemp] = -1\n        \n    return indices\n   ","b5b3bec5":"intervalmeanspath = '..\/input\/osicintervalmeansandlargestslices\/intervalmeans_df.csv'\n\ntry:\n    largestslices_df = pd.read_csv(intervalmeanspath)\nexcept:\n    nslices = 6\n    largestslices = []\n    count = 0\n    for p in intervalmeans_df['Patient']:\n        print(count)\n        count += 1\n        largestslices.append(get_largest_slices(nslices, p))\n\n    intervalmeans_df['LargestSlices'] = largestslices    \n    intervalmeans_df.to_csv('intervalmeans_df',index=False)\n    ","48569c76":"def get_files_names(interval, intervalmeans_df, getlabels = True):\n   \n    fnames = []\n    labels = []\n    for p in intervalmeans_df['Patient']:\n        if getlabels:\n            if math.isnan(intervalmeans_df[intervalmeans_df['Patient'] == p][interval].to_list()[0]):\n                continue\n        sa = [int(s.split(\".\")[0]) for s in os.listdir(INPUT_FOLDER + p)]\n        sa.sort()\n        largestslices = largestslices_df[largestslices_df['Patient'] == p]['LargestSlices']     \n        print(largestslices)\n        largestslices = largestslices.to_list()[0].strip('][').split(', ') if type(largestslices.to_list()[0]) == str else largestslices[0]    \n        print(largestslices)\n        for s in largestslices:  \n            #print(type(s))\n            #print(s)\n            fnames.append(p + '\/' + str(sa[int(s)]) + '.dcm')\n            if getlabels: labels.append(intervalmeans_df[intervalmeans_df['Patient'] == p][interval].to_list()[0])   \n    \n    \n    return fnames, labels","5277a663":"fnames, labelsfvcperv = get_files_names(fvcpervolintervalmeans_df.columns[2], fvcpervolintervalmeans_df)","275a2e55":"fnames, labelspercent = get_files_names(fvcpervolintervalmeans_df.columns[2], percentintervalmeans_df)","7b6564b1":"!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n!pip install fastai==2.0.9","7689b953":"from fastai.vision.all import *\nfrom fastai.data.all import *","60dd5afc":"def get_image_file_paths(inputfolder):\n    return [inputfolder+f for f in fnames]","b0c2efe7":"def get_labels_fvcperv(fname):    \n    return np.array(labelsfvcperv)[np.array(fnames) == (fname).split('\/')[-2] + '\/' + (fname).split('\/')[-1]]","3170c473":"def get_labels_percent(fname):    \n    return np.array(labelspercent)[np.array(fnames) == (fname).split('\/')[-2] + '\/' + (fname).split('\/')[-1]] \/ 100","237faae3":"def get_pixels_hu(scans):\n    \"\"\"\n    Converts raw images to Hounsfield Units (HU).\n    \n    Parameters: scans (Raw images)\n    \n    Returns: image (NumPy array)\n    \"\"\"\n    \n    image = np.stack([s.pixel_array for s in scans])\n    image = image.astype(np.int16)\n\n    # Since the scanning equipment is cylindrical in nature and image output is square,\n    # we set the out-of-scan pixels to 0\n    image[image == -2000] = 0\n    \n    \n    # HU = m*P + b\n    intercept = scans[0].RescaleIntercept\n    slope = scans[0].RescaleSlope\n    \n    if slope != 1:\n        image = slope * image.astype(np.float64)\n        image = image.astype(np.int16)\n        \n    image += np.int16(intercept)\n    \n    return np.array(image, dtype=np.int16)","456040b2":"def create(fn):            \n    img1 = pydicom.read_file(fn)\n    img1 = get_pixels_hu([img1])\n    img1 = PILImage(Image.fromarray(img1[0], mode=None))\n    img1 = Resize(224)(img1)       \n        \n    return img1","03a42b42":"def CustomImageBlock(): return TransformBlock(type_tfms=create, batch_tfms=IntToFloatTensor)","50569012":"dblockfvcperv = DataBlock(blocks    = (CustomImageBlock, RegressionBlock),\n                   get_items = get_image_file_paths,\n                   get_y     = get_labels_fvcperv,\n                   splitter  = RandomSplitter())\ndsets = dblockfvcperv.datasets(INPUT_FOLDER)\ndsets.train[0]","ce41010c":"dblockpercent = DataBlock(blocks    = (CustomImageBlock, RegressionBlock),\n                   get_items = get_image_file_paths,\n                   get_y     = get_labels_percent,\n                   splitter  = RandomSplitter())\ndsets = dblockpercent.datasets(INPUT_FOLDER)\ndsets.train[0]","b5043f06":"dlsfvcperv = dblockfvcperv.dataloaders(INPUT_FOLDER)\ndlspercent = dblockpercent.dataloaders(INPUT_FOLDER)\ndlspercent.cuda().one_batch()","356f3675":"torch.cuda.is_available()","7e996019":"learnpercent = cnn_learner(dlspercent.cuda(), resnet34, metrics=error_rate)","b0c4deef":"learnpercent.lr_find()","96da2c68":"learnpercent.fine_tune(10,1e-2)","92379753":"learnfvcperv = cnn_learner(dlsfvcperv.cuda(), resnet34, metrics=error_rate)\nlearnfvcperv.lr_find()","239ff291":"learnfvcperv.fine_tune(10,1e-2)","6851393a":"ntest = 10","17160dff":"testpatients = patientimages[np.bitwise_not(maskfv)][10:10+ntest]","3310a9de":"nslices = 6\nlargestslices = []\ncount = 0\nfor p in testpatients:\n    print(count)\n    count += 1\n    #print(INPUT_FOLDER + patient)\n    largestslices.append(get_largest_slices(nslices, p))\n","144a7d2f":"fnamestest = []    \nfor p in testpatients:    \n    sa = [int(s.split(\".\")[0]) for s in os.listdir(INPUT_FOLDER + p)]\n    sa.sort()\n    lp = np.array(largestslices)[testpatients == p]    \n    lp = lp[0]\n    for s in lp:  \n        #print(type(s))\n        #print(s)\n        fnamestest.append(p + '\/' + str(sa[int(s)]) + '.dcm')    \n","afbd534d":"predsfvcpervol = []\npredspercent = []\nfor fn in fnamestest:\n    predsfvcpervol.append(learnfvcperv.predict(create(INPUT_FOLDER+fn)))\n    predspercent.append(learnpercent.predict(create(INPUT_FOLDER+fn)))","e76b50d5":"predsfvcpervol = [p[0][0] for p in predsfvcpervol]","21acb70f":"predsfvcpervolmeaned = []\nfor i in range(len(predsfvcpervol) \/\/ 6):\n    predsfvcpervolmeaned.append(np.mean(predsfvcpervol[i*6:(i+1)*6]))    ","127b7245":"plt.hist(predsfvcpervolmeaned)","2de8cda6":"predspercent = [p[0][0] for p in predspercent]","5476f39a":"predspercentmeaned = []\nfor i in range(len(predspercent) \/\/ 6):\n    predspercentmeaned.append(np.mean(predspercent[i*6:(i+1)*6]))    ","6088ab11":"plt.hist(predspercentmeaned)","e2a14553":"predsindexhelper = np.arange(0,ntest)","bbd3152c":"minpercentim = pydicom.read_file(INPUT_FOLDER + fnamestest[predsindexhelper[np.array(predspercentmeaned) == min(predspercentmeaned)][0]*6]).pixel_array\nplt.imshow(minpercentim)","cc10c263":"maxpercentim = pydicom.read_file(INPUT_FOLDER + fnamestest[predsindexhelper[np.array(predspercentmeaned) == max(predspercentmeaned)][0]*6]).pixel_array\nplt.imshow(maxpercentim)","47314017":"minfvcpervolim = pydicom.read_file(INPUT_FOLDER + fnamestest[predsindexhelper[np.array(predsfvcpervolmeaned) == min(predsfvcpervolmeaned)][0]*6]).pixel_array\nplt.imshow(minfvcpervolim)","8c4819e8":"maxfvcpervolim = pydicom.read_file(INPUT_FOLDER + fnamestest[predsindexhelper[np.array(predsfvcpervolmeaned) == max(predsfvcpervolmeaned)][0]*6]).pixel_array\nplt.imshow(maxfvcpervolim)","0950a6b8":"* Choosing the six slices with the largest area for training the classifier","68b9aaea":"# Segmentation","f231d243":"* Determining the intervals","58ac7d6f":"* Clearly there is a linear correlation\n* This supports the thesis that FVC\/Volume should be a better indicator for the state of the disease, than FVC alone\n* Let's investigate how the FVC\/Volume correlates with Percent","6a55ad93":"* Apparently there is no clear correlation\n* This suggests, that the two quantities do not describe the lung's state of fibrosis equivalently","59e721bb":"The volumes can be retrieved from the data set","60ce8c45":"# Introduction","9be7072e":"* If the assumption made in the introduction is correct the calculated volumes should correlate linearly with the maximum FVC values","b7f36ce2":"# Calculating volumes","758504cf":"* The goal of this notebook is to investigate a metric for describing the state of the lung's fibrosis allowing for comparison among patients of varying characterisitcs. \n* FVC alone should not be a great indicator for the state of the lung's fibrosis as it most probably should depend linearly on the lungs's volume regarded as an empty vessel.\n* The tabular data offers the Percent column which is described as the fraction of an FVC value divided by a FVC value typical for a person with similar characterisitcs. \n* However, I thought the most straightforward way for describing the state of the lung's fibrosis should be the FVC divided by the lung's volume regarded as an empty vessel.","0e1eb1a4":"Taken from @Xie29's great notebook(https:\/\/www.kaggle.com\/xiejialun\/lung-ct-scan-segmentation-model)","7ef159af":"* The low valid_loss shows, that both quantities can be extracted from the images\n* Let's see what they predict on a test set","c0b00048":"* The idea is to divide the weeks into intervals with enough training data for a neural network","1101bc67":"# How do the metrics influence image classification?","d6f44203":"# Analysing volumes","6d54959d":"* The multiple clusters in the scatter plot suggest that some aspect was not properly included to get consistent volumes for all patients\n* The large cluster at the bottom however suggests that there is some value to the assumption, let's look at it closer"}}