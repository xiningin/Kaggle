{"cell_type":{"6b92c043":"code","6896d92a":"code","f852157c":"code","c3a64397":"code","fba53f61":"code","dafa038b":"code","2ee69a01":"code","84dbc8df":"code","aec0a8e6":"code","f03a3880":"code","5ddbf1c0":"code","c179befa":"code","e438c887":"code","c918cdee":"code","c8526b1d":"code","66d0cc99":"markdown","10c6a65b":"markdown","e2da8065":"markdown","38ced261":"markdown","8b88e2ce":"markdown"},"source":{"6b92c043":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly import tools\nimport plotly.plotly as py\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndf = pd.read_csv('..\/input\/diabetes.csv')\ndf.head()\n\n# Any results you write to the current directory are saved as output.","6896d92a":"df.describe()","f852157c":"df.info()","c3a64397":"df.shape","fba53f61":"df.groupby('Outcome').size()","dafa038b":"f, ax = plt.subplots(1, 2, figsize = (15, 7))\nf.suptitle(\"Diabetes?\", fontsize = 18.)\n_ = df.Outcome.value_counts().plot.bar(ax = ax[0], rot = 0, color = (sns.color_palette()[2], sns.color_palette()[3])).set(xticklabels = [\"No\", \"Yes\"])\n_ = df.Outcome.value_counts().plot.pie(labels = (\"No\", \"Yes\"), autopct = \"%.2f%%\", label = \"\", fontsize = 13., ax = ax[1],\\\ncolors = (sns.color_palette()[2], sns.color_palette()[3]), wedgeprops = {\"linewidth\": 1.5, \"edgecolor\": \"white\"}), ax[1].texts[1].set_color(\"white\"), ax[1].texts[3].set_color(\"white\")","2ee69a01":"df.hist(figsize=(10,8))","84dbc8df":"\n\nfig, ax = plt.subplots(4,2, figsize=(16,16))\nsns.distplot(df.Age, bins = 20, ax=ax[0,0]) \nsns.distplot(df.Pregnancies, bins = 20, ax=ax[0,1]) \nsns.distplot(df.Glucose, bins = 20, ax=ax[1,0]) \nsns.distplot(df.BloodPressure, bins = 20, ax=ax[1,1]) \nsns.distplot(df.SkinThickness, bins = 20, ax=ax[2,0])\nsns.distplot(df.Insulin, bins = 20, ax=ax[2,1])\nsns.distplot(df.DiabetesPedigreeFunction, bins = 20, ax=ax[3,0]) \nsns.distplot(df.BMI, bins = 20, ax=ax[3,1]) \n\n","aec0a8e6":"corr=df.corr()\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\n\nsns.heatmap(corr, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap=\"Greens\",linecolor=\"black\")\nplt.title('Correlation between features');","f03a3880":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)","5ddbf1c0":"#Model\nDT = DecisionTreeClassifier()\n\n#fiting the model\nDT.fit(X_train, y_train)\n\n#prediction\ny_pred = DT.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", DT.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g', cmap=\"Blues\")\nplt.show()","c179befa":"#Model\nmodel = GradientBoostingClassifier()\n\n#fiting the model\nmodel.fit(X_train, y_train)\n\n#prediction\ny_pred = model.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", model.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g', cmap=\"Blues\")\nplt.show()\n\n","e438c887":"#Model\nLR = LogisticRegression()\n\n#fiting the model\nLR.fit(X_train, y_train)\n\n#prediction\ny_pred = LR.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", LR.score(X_test, y_test)*100)\n\n#Plot the confusion matrix\nsns.set(font_scale=1.5)\ncm = confusion_matrix(y_pred, y_test)\nsns.heatmap(cm, annot=True, fmt='g', cmap=\"Blues\")\nplt.show()","c918cdee":"from sklearn.naive_bayes import GaussianNB\n\ngnb = GaussianNB()\n\n#fiting the model\ngnb.fit(X_train, y_train)\n\n#prediction\ny_pred = gnb.predict(X_test)\n\n#Accuracy\nprint(\"Accuracy \", gnb.score(X_test, y_test)*100)","c8526b1d":"from xgboost import XGBClassifier\n\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\n\n# Use Fitted training model to make predictions on test data\ny_pred = model.predict(X_test)\npredictions = [round(value) for value in y_pred]\n\n#Evaluate the performance of the developed model after predictions\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))","66d0cc99":"### Data\n\nThe datasets consists of several medical predictor variables and one target variable, Outcome. Predictor variables includes the number of pregnancies the patient has had, their BMI, insulin level, age, and so on.\n\n    Pregnancies: Number of times pregnant\n    Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n    BloodPressure: Diastolic blood pressure (mm Hg)\n    SkinThickness: Triceps skin fold thickness (mm)\n    Insulin: 2-Hour serum insulin (mu U\/ml)\n    BMI: Body mass index (weight in kg\/(height in m)^2)\n    DiabetesPedigreeFunction: Diabetes pedigree function\n    Age: Age (years)\n    Outcome: Class variable (0 or 1)\n","10c6a65b":"\n### Gradient Boosting\n\nGradient boosting is a machine learning technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do, and it generalizes them by allowing optimization of an arbitrary differentiable loss function.\n","e2da8065":"\n### Decision Tree\n\nDecision tree builds regression or classification models in the form of a tree structure. It breaks down a dataset into smaller and smaller subsets while at the same time an associated decision tree is incrementally developed. The final result is a tree with decision nodes and leaf nodes. A decision node (e.g., Outlook) has two or more branches (e.g., Sunny, Overcast and Rainy), each representing values for the attribute tested. Leaf node (e.g., Hours Played) represents a decision on the numerical target. The topmost decision node in a tree which corresponds to the best predictor called root node. Decision trees can handle both categorical and numerical data.\n","38ced261":"\n### Logistic Regression\n\nLogistic regression is the appropriate regression analysis to conduct when the dependent variable is binary. Like all regression analyses, the logistic regression is a predictive analysis. Logistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n","8b88e2ce":"\n### Predictive Modeling\n"}}