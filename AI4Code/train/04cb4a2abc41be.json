{"cell_type":{"e94229dc":"code","25eb0a86":"code","10a4be70":"code","b6575972":"code","78416a8d":"code","18a9fdb5":"code","4467de18":"code","3298b73e":"code","30f67cf0":"code","93574f63":"code","66eb494c":"code","aa0614fe":"code","a5850b49":"code","5357a5a2":"code","1fb61403":"code","d8d6a3a5":"code","78ebae91":"code","2c95f9d7":"code","eefedd97":"code","8ede7035":"code","6cc60ac6":"code","39d80539":"code","79a27330":"code","f50b2a78":"code","5cef883c":"code","6683d6f8":"code","2ed2980e":"code","ea27167d":"code","e2f21bd8":"code","667e86c7":"markdown","0c40b39a":"markdown","0c0b2b28":"markdown","36c9c513":"markdown","a1228992":"markdown","3f586ef3":"markdown","4478a68c":"markdown","d44c3824":"markdown","55ba5a0a":"markdown","58de0f44":"markdown","e2788ce3":"markdown","36eafa39":"markdown","f55a5010":"markdown","470ef765":"markdown","547ca550":"markdown","231a286d":"markdown","b63524b7":"markdown","629aa6a1":"markdown","fae78d46":"markdown"},"source":{"e94229dc":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","25eb0a86":"# \u0447\u0438\u0442\u0430\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435\ndf = pd.read_csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\", \n                 usecols=[0,5],\n                 sep=',',\n                 header=None,\n                 encoding='latin',\n                 dtype={0:int, 5:object})\n\n# \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u044f \u043a\u043e\u043b\u043e\u043d\u043e\u043a\ndf.columns = ['target', 'text']\n\ndf.head()","10a4be70":"df.info()","b6575972":"# \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u0440\u0430\u0434\u0430\u0446\u0438\u0439 target\ndf.loc[:,'target'].value_counts()","78416a8d":"# \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c target \u0432 0\/1 (0 - \u043d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435 \u044d\u043c\u043e\u0446\u0438\u0438 (0), 1 - \u043f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435 (4))                                        \ndf['target'] = df['target'].apply(lambda x: 0 if x == 0 else 1)\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c: \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0433\u0440\u0430\u0434\u0430\u0446\u0438\u0439 target \u043f\u043e\u0441\u043b\u0435 \u043f\u0435\u0440\u0435\u0438\u043c\u0435\u043d\u043e\u0432\u0430\u043d\u0438\u044f\ndf.loc[:,'target'].value_counts()","18a9fdb5":"# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u0438 \u0432 c\u0442\u043e\u043b\u0431\u0446\u0430\u0445\ndf.isna().sum()","4467de18":"# \u043e\u043f\u0438\u0441\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445\nprint(df.groupby('target')['target'].count())\n\n# \u043f\u0440\u043e\u0446\u0435\u043d\u0442 target=1\ntarget_count = df[df['target'] == 1]['target'].count()\ntotal = df['target'].count()\ntarget_share = target_count\/total\nprint(\"\u0414\u043e\u043b\u044f \u0434\u0430\u043d\u043d\u044b\u0445, \u043f\u043e\u043a\u0430\u0437\u044b\u0432\u0430\u044e\u0449\u0438\u0445 \u0446\u0435\u043b\u0435\u0432\u0443\u044e \u0433\u0440\u0443\u043f\u043f\u0443 \\\"target=1\\\" {0:.2f}\".format(target_share))\n\n# \u0433\u0438\u0441\u0442\u043e\u0433\u0440\u0430\u043c\u043c\u0430\ndf[df['target'] == 0]['target'].astype(int).hist(label='\u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435', grid = False, bins=1, rwidth=0.8)\ndf[df['target'] == 1]['target'].astype(int).hist(label='\u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435', grid = False, bins=1, rwidth=0.8)\nplt.xticks((0,1),('\u041d\u0435\u0433\u0430\u0442\u0438\u0432\u043d\u044b\u0435', '\u041f\u043e\u0437\u0438\u0442\u0438\u0432\u043d\u044b\u0435'))\nplt.show()","3298b73e":"df = df.sample(frac=0.001, random_state = 7).reset_index(drop=True)\ndf.shape","30f67cf0":"from sklearn.model_selection import train_test_split\n\n# \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 X \u0438 y\nX = df['text']\ny = df['target']\n\n# \u0440\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 train \u0438 test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3,random_state = 42)","93574f63":"# regular expressions library\nimport re\n\ndef text_clean(text):\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 \u043d\u0438\u0436\u043d\u0438\u0439 \u0440\u0435\u0433\u0438\u0441\u0442\u0440\n    text.lower()\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c https:\/\/ \u0438 \u0442.\u043f. \u0430\u0434\u0440\u0435\u0441\u0430 \u0432 \u0442\u0435\u043a\u0441\u0442 \"URL\"\n    text = re.sub('((www\\.[^\\s]+)|(https?:\/\/[^\\s]+))','URL',text)\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0438\u043c\u044f \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u044f @username \u0432 \"AT_USER\"\n    text = re.sub('@[^\\s]+','AT_USER', text)\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u043c\u043d\u043e\u0436\u0435\u0441\u0442\u0432\u0435\u043d\u044b\u0435 \u043f\u0440\u043e\u0431\u0435\u043b\u044b \u0432 \u043e\u0434\u0438\u043d \u043f\u0440\u043e\u0431\u0435\u043b\n    text = re.sub('[\\s]+', ' ', text)\n    # \u043f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0445\u044d\u0448\u0442\u0435\u0433 #\u0442\u0435\u043c\u0430 \u0432 \"\u0442\u0435\u043c\u0430\"\n    text = re.sub(r'#([^\\s]+)', r'\\1', text)\n    return text\n\nX_train = X_train.apply(text_clean)\nX_test = X_test.apply(text_clean)\nX_train","66eb494c":"import nltk\nfrom nltk.corpus import stopwords\nimport string\n\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","aa0614fe":"# \u0444\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u044f \u043f\u0440\u0438\u043b\u0430\u0433\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0445 (ADJ), \u0433\u043b\u0430\u0433\u043e\u043b\u043e\u0432 (VERB), \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 (NOUN) \u0438 \u043d\u0430\u0440\u0435\u0447\u0438\u0439 (ADV)\nfrom nltk.corpus import wordnet as wn\n\ndef get_simple_pos(tag):\n    if tag.startswith('J'):\n        return wn.ADJ\n    elif tag.startswith('V'):\n        return wn.VERB\n    elif tag.startswith('N'):\n        return wn.NOUN\n    elif tag.startswith('R'):\n        return wn.ADV\n    else:\n        return wn.NOUN\n\n# \u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f \nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import pos_tag\n\n# (\u043e\u0442\u0431\u0440\u0430\u0441\u044b\u0432\u0430\u0435\u043c \u0432\u0441\u0451 \u043b\u0438\u0448\u043d\u0435\u0435 \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0438, \u043f\u0440\u0438\u0432\u043e\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430 \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435\u0438 \u043f\u043e\u043b\u0443\u0447\u0430\u0435\u043c \u0438\u0445 \u0441\u043f\u0438\u0441\u043e\u043a)\n# 'Once upone a time a man walked into a door' -> ['upone', 'time', 'man', 'walk', 'door']\nlemmatizer = WordNetLemmatizer()\ndef lemmatize_words(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            pos = pos_tag([i.strip()])\n            word = lemmatizer.lemmatize(i.strip(),get_simple_pos(pos[0][1]))\n            final_text.append(word.lower())\n    return final_text   \n\n# \u041e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u0439 \u0441\u043f\u0438\u0441\u043e\u043a \u0432 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u0435\n# ['upone', 'time', 'man', 'walk', 'door'] -> 'upone time man walk door '\ndef join_text(text):\n    string = ''\n    for i in text:\n        string += i.strip() +' '\n    return string\n\n# \u0417\u0430\u043f\u0443\u0441\u043a \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u0438 \u0438 \u0441\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043d\u043e\u0432\u043e\u0433\u043e \u043f\u043e\u043b\u044f \u0441 \u043b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u0440\u0435\u0434\u043b\u043e\u0436\u0435\u043d\u0438\u044f\u043c\u0438 'text_lemma'\nX_train = X_train.apply(lemmatize_words).apply(join_text)\nX_test = X_test.apply(lemmatize_words).apply(join_text)\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0430\nX_train","a5850b49":"from sklearn.feature_extraction.text import TfidfVectorizer\n\n# \u041f\u0440\u0438\u0441\u0432\u043e\u0435\u043d\u0438\u0435 \u0432\u0435\u0441\u043e\u0432 \u0441\u043b\u043e\u0432\u0430\u043c \u0441 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043d\u0438\u0435\u043c TfidfVectorizer\ntv = TfidfVectorizer(min_df=0,max_df=1,use_idf=True,ngram_range=(1,2))\ntv_X_train = tv.fit_transform(X_train)\ntv_X_test = tv.transform(X_test)\n\nprint('TfidfVectorizer_train:', tv_X_train.shape)\nprint('TfidfVectorizer_test:', tv_X_test.shape)","5357a5a2":"# \u043f\u0435\u0440\u0435\u0432\u0435\u0440\u043d\u0451\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u044b \u0441\u043e \u0441\u043b\u043e\u0432\u0430\u043c\u0438 \u0438\u0437 sparse \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0441 \u043e\u0434\u043d\u0438\u043c \u0441\u0442\u043e\u043b\u0431\u0446\u043e\u043c \u0432 numpy \u043c\u0430\u0441\u0441\u0438\u0432 \u0441\u043e \u043c\u043d\u043e\u0433\u0438\u043c\u0438 \u0441\u0442\u043e\u043b\u0431\u0446\u0430\u043c\u0438\ntv_X_train = tv_X_train.toarray()\ntv_X_test = tv_X_test.toarray()","1fb61403":"from sklearn.preprocessing import StandardScaler\n\n# \u041d\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u0443\u0435\u043c \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 tv_X_train\/test\nscaler_tv = StandardScaler(copy=False)\n\ntv_X_train_scaled = scaler_tv.fit_transform(tv_X_train)\ntv_X_test_scaled = scaler_tv.transform(tv_X_test)\n\n# \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c tv_X_train\/test sparse numpy \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0432 pandas Data Frame\ntv_X_train_pd_scaled = pd.DataFrame(data=tv_X_train_scaled, \n                             index=X_train.index, \n                             columns=np.arange(0, np.size(tv_X_train_scaled,1)))\ntv_X_test_pd_scaled = pd.DataFrame(data=tv_X_test_scaled, \n                             index=X_test.index, \n                             columns=np.arange(0, np.size(tv_X_test_scaled,1)))\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c\ntv_X_train_pd_scaled.shape","d8d6a3a5":"tv_X_train = tv_X_train_pd_scaled\ntv_X_test = tv_X_test_pd_scaled","78ebae91":"tv_X_train.loc[:,:3]","2c95f9d7":"from sklearn.preprocessing import PolynomialFeatures\n\ndef create_polinomial(X, degree = 2):\n    return PolynomialFeatures(degree).fit_transform(X)\n\nX_train_poly = create_polinomial(tv_X_train.loc[:,:3], 2)\nX_test_poly = create_polinomial(tv_X_test.loc[:,:3], 2)\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c\nprint(X_train_poly.shape, X_test_poly.shape)\nX_train_poly","eefedd97":"# \u0442\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0438\u0440\u0443\u0435\u043c X_train_poly\/test numpy \u043c\u0430\u0442\u0440\u0438\u0446\u044b \u0432 pandas Data Frame\nX_train_poly_pd = pd.DataFrame(data=X_train_poly, \n                             index=tv_X_train.index, \n                             columns=np.arange(0, np.size(X_train_poly,1)))\nX_test_poly_pd = pd.DataFrame(data=X_test_poly, \n                             index=tv_X_test.index, \n                             columns=np.arange(0, np.size(X_test_poly,1)))\n\n# \u043e\u0431\u044a\u0435\u0434\u0438\u043d\u044f\u0435\u043c \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0435 \u043f\u0435\u0440\u0435\u043c\u0435\u043d\u043d\u044b\u0435 \u0441\u043e \u0441\u0433\u0435\u043d\u0435\u0440\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u043c\u0438\ntv_X_train = tv_X_train.join(X_train_poly_pd, rsuffix='_poly')\ntv_X_test = tv_X_test.join(X_test_poly_pd, rsuffix='_poly')\n\n# \u043f\u0440\u043e\u0432\u0435\u0440\u044f\u0435\u043c\ntv_X_train.shape, tv_X_test.shape","8ede7035":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\n\nlr = LogisticRegression(penalty='l2', max_iter=500, C=1, random_state=42)\n\n# \u0442\u0440\u0435\u043d\u0438\u0440\u0443\u0435\u043c\nlr_tfidf=lr.fit(tv_X_train, y_train)\nprint(lr_tfidf)\n\n# \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u044b\u0432\u0430\u0435\u043c\nlr_tfidf_predict=lr.predict(tv_X_test)\n\n# Accuracy\nlr_tfidf_score=accuracy_score(y_test,lr_tfidf_predict)\nprint(\"lr_tfidf_score :\",lr_tfidf_score)\n\n# Classification report\nlr_tfidf_report=classification_report(y_test,lr_tfidf_predict,target_names=['0','1'])\nprint(lr_tfidf_report)\n\n# Confusion matrix\nplot_confusion_matrix(lr_tfidf, tv_X_test, y_test,display_labels=['\u041e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435','\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435'],cmap=\"Blues\",values_format = '')\n\n# R^2\nr2 = r2_score(y_test, lr_tfidf_predict)\nprint (f\"R2 score \/ LR = {r2}\")\n\n# MAE\nmeanae = mean_absolute_error(y_test, lr_tfidf_predict)\nprint (\"MAE (Mean Absolute Error) {0}\".format(meanae))","6cc60ac6":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nknn = KNeighborsClassifier()\n\n# \u0417\u0430\u0434\u0430\u0434\u0438\u043c \u0441\u0435\u0442\u043a\u0443 - \u0441\u0440\u0435\u0434\u0438 \u043a\u0430\u043a\u0438\u0445 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0439 \u0432\u044b\u0431\u0438\u0440\u0430\u0442\u044c \u043d\u0430\u0438\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440.\nknn_grid = {'n_neighbors': np.array(np.linspace(1, 31, 3), dtype='int')} # \u043f\u0435\u0440\u0435\u0431\u0438\u0440\u0430\u0435\u043c \u043f\u043e \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u0443 <<n_neighbors>>, \u043f\u043e \u0441\u0435\u0442\u043a\u0435 \u0437\u0430\u0434\u0430\u043d\u043d\u043e\u0439 np.linspace\n\n# \u0421\u043e\u0437\u0434\u0430\u0435\u043c \u043e\u0431\u044a\u0435\u043a\u0442 \u043a\u0440\u043e\u0441\u0441-\u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\ngs = GridSearchCV(knn, knn_grid, cv=3)\n\n# \u041e\u0431\u0443\u0447\u0430\u0435\u043c \u0435\u0433\u043e\ngs.fit(tv_X_train, y_train)\n\n# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\ndef grid_plot(x, y, x_label, title, y_label):\n    plt.figure(figsize=(12, 6))\n    plt.grid(True)\n    plt.plot(x, y, 'go-')\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)\n\n# \u0421\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0442 \u0447\u0438\u0441\u043b\u0430 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\ngrid_plot(knn_grid['n_neighbors'], gs.cv_results_['mean_test_score'], 'n_neighbors', 'KNeighborsClassifier', 'accuracy')\n\n# \u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\nprint('\u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440: ', gs.best_params_, gs.best_score_)","39d80539":"# KNN \u0441 \u043b\u0443\u0447\u0448\u0438\u043c \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u043c\n# \u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0432\u0430\u0440\u044c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043e\u0442 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043a \u0432\u044b\u0431\u043e\u043a\u0435, \u0442.\u043a. \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u0435\u043d\u044c\u0448\u0435, \u0447\u0435\u043c \u0445\u043e\u0442\u0435\u043b\u043e\u0441\u044c \u0431\u044b, \u0432\u0432\u0438\u0434\u0443 \u043d\u0438\u0437\u043a\u043e\u0439 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 Kaggel kernel\nclf_knn = KNeighborsClassifier(n_neighbors=31)\n\n# \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\nclf_knn.fit(tv_X_train, y_train)\ny_knn = clf_knn.predict(tv_X_test)\n\n# classification report\nprint(classification_report(y_test, y_knn))\n\n# confusion matrix\nplot_confusion_matrix(clf_knn, tv_X_test, y_test, display_labels=['\u041e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435','\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435'], cmap=\"Blues\", values_format = '')","79a27330":"from sklearn.svm import SVC\n\nalg = SVC()\n\ngrid = {'C': np.array(np.linspace(-5, 5, 3), dtype='float'),\n        'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n        }\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1, scoring = 'f1')\ngs.fit(tv_X_train, y_train)\ngs.best_params_, gs.best_score_","f50b2a78":"# \u043b\u0443\u0447\u0448\u0438\u0439 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440 \u0432\u0430\u0440\u044c\u0438\u0440\u0443\u0435\u0442\u0441\u044f \u043e\u0442 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043a \u0432\u044b\u0431\u043e\u043a\u0435, \u0442.\u043a. \u0434\u0430\u043d\u043d\u044b\u0445 \u043c\u0435\u043d\u044c\u0448\u0435, \u0447\u0435\u043c \u0445\u043e\u0442\u0435\u043b\u043e\u0441\u044c \u0431\u044b, \u0432\u0432\u0438\u0434\u0443 \u043d\u0438\u0437\u043a\u043e\u0439 \u043f\u0440\u043e\u0438\u0437\u0432\u043e\u0434\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0441\u0442\u0438 Kaggel kernel\nalg = SVC(C = 5.0, kernel = 'linear')\nalg.fit(tv_X_train, y_train)\npreds = alg.predict(tv_X_test)\n\n# classification report\nprint(classification_report(y_test, preds))\n\n# confusion matrix\nplot_confusion_matrix(alg, tv_X_test, y_test, display_labels=['\u041e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435','\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435'], cmap=\"Blues\", values_format = '')","5cef883c":"from sklearn.naive_bayes import BernoulliNB\n\nalg = BernoulliNB()\n\ngrid = {'alpha': np.array(np.linspace(0, 6, 30), dtype='float'),}\n\ngs = GridSearchCV(alg, grid, verbose=2, n_jobs = -1, scoring = 'f1')\ngs.fit(tv_X_train, y_train)\ngs.best_params_, gs.best_score_","6683d6f8":"# \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u043e\u0442\u0440\u0438\u0441\u043e\u0432\u043a\u0438 \u0433\u0440\u0430\u0444\u0438\u043a\u043e\u0432\ndef grid_plot(x, y, x_label, title, y_label='f1'):\n    # \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0438\u043b\u0438 \u0440\u0430\u0437\u043c\u0435\u0440 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n    plt.figure(figsize=(12, 6))\n    # \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u0441\u0435\u0442\u043a\u0443 \u043d\u0430 \u0444\u043e\u043d\n    plt.grid(True)\n    # \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u043b\u0438 \u043f\u043e \u0445 - \u0447\u0438\u0441\u043b\u043e \u0441\u043e\u0441\u0435\u0434\u0435\u0439, \u043f\u043e y - \u0442\u043e\u0447\u043d\u043e\u0441\u0442\u044c\n    plt.plot(x, y, 'go-')\n    # \u0434\u043e\u0431\u0430\u0432\u0438\u043b\u0438 \u043f\u043e\u0434\u043f\u0438\u0441\u0438 \u043e\u0441\u0435\u0439 \u0438 \u043d\u0430\u0437\u0432\u0430\u043d\u0438\u0435 \u0433\u0440\u0430\u0444\u0438\u043a\u0430\n    plt.xlabel(x_label)\n    plt.ylabel(y_label)\n    plt.title(title)","2ed2980e":"# \u0421\u0442\u0440\u043e\u0438\u043c \u0433\u0440\u0430\u0444\u0438\u043a \u0437\u0430\u0432\u0438\u0441\u0438\u043c\u043e\u0441\u0442\u0438 \u043a\u0430\u0447\u0435\u0441\u0442\u0432\u0430 \u043e\u0442 \u0447\u0438\u0441\u043b\u0430 \u0441\u043e\u0441\u0435\u0434\u0435\u0439\n# \u0437\u0430\u043c\u0435\u0447\u0430\u043d\u0438\u0435: \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0445\u0440\u0430\u043d\u044f\u0442\u0441\u044f \u0432 \u0430\u0442\u0440\u0438\u0431\u0443\u0442\u0435 cv_results_ \u043e\u0431\u044a\u0435\u043a\u0442\u0430 gs\ngrid_plot(grid['alpha'], gs.cv_results_['mean_test_score'], 'n_neighbors', 'BernoulliNB')","ea27167d":"# \u043f\u0440\u043e\u0433\u043d\u043e\u0437\npreds = gs.predict(tv_X_test)","e2f21bd8":"# classification report\nprint(classification_report(y_test, preds))\n\n# confusion matrix\nplot_confusion_matrix(gs, tv_X_test, y_test, display_labels=['\u041e\u0442\u0440\u0438\u0446\u0430\u0442\u0435\u043b\u044c\u043d\u044b\u0435','\u041f\u043e\u043b\u043e\u0436\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0435'], cmap=\"Blues\", values_format = '')","667e86c7":"### Stemming \u0438 Lemmatization","0c40b39a":"# \u0420\u0430\u0437\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432\u044b\u0431\u043e\u0440\u043a\u0438 \u043d\u0430 train \u0438 test","0c0b2b28":"# \u0417\u0430\u0440\u0433\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","36c9c513":"\u0422.\u043a. \u0434\u0430\u043d\u043d\u044b\u0445 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u0434\u043b\u044f \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u043e\u0439 \u0430\u043d\u0430\u043b\u0438\u0442\u0438\u043a\u0438 (Lemmatization, Stemming \u0438 TF-IDF \u0437\u0430\u043d\u0438\u043c\u0430\u044e\u0442 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u043c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438, \u0430 KNN \u043a\u0440\u0430\u0439\u043d\u0435 \u043c\u0435\u0434\u043b\u0435\u043d\u043d\u044b\u0439 \u0430\u043b\u0433\u043e\u0440\u0438\u0442\u043c), \u0442\u043e \u043c\u044b \u0441\u0434\u0435\u043b\u0430\u0435\u043c \u0440\u0435\u043f\u0440\u0435\u0437\u0435\u043d\u0442\u0430\u0442\u0438\u0432\u043d\u0443\u044e \u0432\u044b\u0431\u043e\u0440\u043a\u0443.","a1228992":"# KNN","3f586ef3":"# \u041c\u043e\u0434\u0435\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435","4478a68c":"# Naive Baies","d44c3824":"\u0418\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u0435\u043c Stemming (\u043f\u043e\u0438\u0441\u043a \u043e\u0441\u043d\u043e\u0432\u044b \u0441\u043b\u043e\u0432\u0430) Lemmatization (\u043f\u0440\u0438\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u043a \u043d\u043e\u0440\u043c\u0430\u043b\u044c\u043d\u043e\u0439 \u0444\u043e\u0440\u043c\u0435: \u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0438\u0442\u0435\u043b\u044c\u043d\u044b\u0445 \u043a \u0435\u0434\u0438\u043d\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u043c\u0443 \u0447\u0438\u0441\u043b\u0443, \u043c\u0443\u0436\u0441\u043a\u043e\u043c\u0443 \u0440\u043e\u0434\u0443, \u0433\u043b\u0430\u0433\u043e\u043b\u043e\u0432 \u043a \u0438\u043d\u0444\u0438\u043d\u0438\u0442\u0438\u0432\u0443 \u0438 \u0442.\u043f) \u0434\u043b\u044f \u0441\u043e\u043a\u0440\u0430\u0449\u0435\u043d\u0438\u044f \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u0430 \u0441\u043b\u043e\u0432\u043e\u0444\u043e\u0440\u043c: am, are, is -> be; car, cars, car's, cars' -> car etc. https:\/\/ru.wikipedia.org\/wiki\/\u0421\u0442\u0435\u043c\u043c\u0438\u043d\u0433 https:\/\/ru.wikipedia.org\/wiki\/\u041b\u0435\u043c\u043c\u0430\u0442\u0438\u0437\u0430\u0446\u0438\u044f","55ba5a0a":"# \u0412\u044b\u0431\u043e\u0440\u043a\u0430 \u0447\u0430\u0441\u0442\u0438 \u0434\u0430\u043d\u043d\u044b\u0445","58de0f44":"# \u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u043e\u0432\u0430\u043d\u0438\u0435 \u0442\u0435\u043a\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445","e2788ce3":"## \u041b\u043e\u0433\u0438\u0441\u0442\u0438\u0447\u0435\u0441\u043a\u0430\u044f \u0440\u0435\u0433\u0440\u0435\u0441\u0441\u0438\u044f","36eafa39":"TFIDFVectorizer \u043a\u0430\u0436\u0434\u043e\u043c\u0443 \u0441\u043b\u043e\u0432\u0443 \u0441\u0442\u0430\u0432\u0438\u0442 \u0447\u0430\u0441\u0442\u043e\u0442\u043d\u043e\u0441\u0442\u044c \u043f\u043e\u044f\u0432\u043b\u0435\u043d\u0438\u044f, \u043f\u0440\u0438 \u044d\u0442\u043e\u043c \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0443\u0432\u0435\u043b\u0438\u0447\u0438\u0432\u0430\u0435\u0442\u0441\u044f \u043f\u0440\u043e\u043f\u043e\u0440\u0446\u0438\u043e\u043d\u0430\u043b\u044c\u043d\u043e \u0441\u0447\u0435\u0442\u0443, \u043d\u043e \u0441\u043c\u0435\u0449\u0430\u0435\u0442\u0441\u044f \u043d\u0430 \u0447\u0430\u0441\u0442\u043e\u0442\u0443 \u0441\u043b\u043e\u0432\u0430 \u0432 \u043a\u043e\u0440\u043f\u0443\u0441\u0435.","f55a5010":"\u041f\u0440\u0435\u043e\u0431\u0440\u0430\u0437\u0443\u0435\u043c \u0442\u0435\u043a\u0441\u0442 \u0432 \u0431\u043e\u043b\u0435\u0435 \u0443\u0434\u043e\u0431\u043d\u044b\u0439 \u0432\u0438\u0434 (\u043e\u0431\u0440\u0430\u0431\u043e\u0442\u0430\u0435\u043c URL, \u0445\u044d\u0448\u0442\u0435\u0433\u0438 \u0438 \u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u0435\u043b\u0435\u0439 \u0432 \u0442\u0435\u043a\u0441\u0442\u0435)","470ef765":"# \u0410\u043d\u0430\u043b\u0438\u0437 \u0434\u0430\u043d\u043d\u044b\u0445","547ca550":"# SVM","231a286d":"\u0441\u043e\u0437\u0434\u0430\u0434\u0438\u043c \u0441\u043b\u043e\u0432\u0430\u0440\u044c \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432 (\u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u043d\u0435 \u0432\u043b\u0438\u044f\u044e\u0442 \u043d\u0430 \u0430\u043d\u0430\u043b\u0438\u0437, \u0442\u0430\u043a\u0438\u0435 \u043a\u0430\u043a he, have, it, the \u0438 \u0442.\u043f.), \u0434\u043e\u0431\u0430\u0432\u0438\u043c \u043a \u0441\u0442\u043e\u043f-\u0441\u043b\u043e\u0432\u0430\u043c \u043f\u0443\u043d\u043a\u0442\u0443\u0430\u0446\u0438\u044e","b63524b7":"# \u0422\u0440\u0430\u043d\u0441\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u044f \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0435\u043d\u0438\u0435 \u043f\u0440\u043e\u043f\u0443\u0441\u043a\u043e\u0432","629aa6a1":"\u0414\u0430\u043d\u043d\u044b\u0435 \u0441\u0431\u0430\u043b\u0430\u043d\u0441\u0438\u0440\u043e\u0432\u0430\u043d\u043d\u044b","fae78d46":"# \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u043f\u043e\u043b\u0438\u043d\u043e\u043c\u0438\u0430\u043b\u044c\u043d\u044b\u0445"}}