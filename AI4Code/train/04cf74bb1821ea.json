{"cell_type":{"1b3866ea":"code","f75c7512":"code","56eaed67":"code","c65c20c0":"code","8177374d":"code","87f222b9":"code","05631148":"code","4ef998b6":"code","7091059d":"code","dfb78c90":"code","acca496b":"code","78265bcd":"code","21c72209":"code","bc4a1a6d":"code","8e01848a":"code","e583d21d":"code","0702eb2f":"code","77f0b272":"code","a809de14":"code","02b4cfc0":"code","128aa39a":"code","ba463978":"code","753e89a0":"code","ebdcd0c5":"code","09d6dafd":"code","11000b0c":"code","dc1288a2":"code","c517a482":"code","f7579166":"code","b228b037":"code","96018b48":"code","62be111b":"code","d86bcc00":"code","a41ae846":"code","8ff0bbdb":"code","6e66bcb0":"code","c1f2879f":"code","97ea5cdc":"code","9b1c2a06":"code","e9778d8d":"code","cd2760ce":"code","8bd269f2":"code","8847a0fa":"code","3da556d5":"markdown","104d264d":"markdown","34378b36":"markdown","43cf89c1":"markdown","df0a439b":"markdown","6070e254":"markdown","9985d989":"markdown","214be337":"markdown","72edaec2":"markdown","f1e6a1e0":"markdown","a6b02991":"markdown","cc3dc9ff":"markdown","d9587756":"markdown","610f6faa":"markdown","702fe866":"markdown","74f87920":"markdown","ce3cf8e6":"markdown","72da0447":"markdown","130f6623":"markdown","5cdafb2b":"markdown","eecb8525":"markdown","7ef96446":"markdown","fa347884":"markdown"},"source":{"1b3866ea":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f75c7512":"#Creating our test and train data frames.\ndf_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nprint(df_train.shape)\nprint(df_test.shape)","56eaed67":"#Importing libraries\nimport matplotlib.pyplot as plt #Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python\nimport seaborn as sns #Seaborn is a Python data visualization library based on matplotlib\nfrom sklearn import linear_model #Ordinary least squares Linear Regression","c65c20c0":"df_train.columns","8177374d":"#Creating a histogram of SalePrice to understand its distribution.\nsns.distplot(df_train[\"SalePrice\"])","87f222b9":"#Calculate the correlations across the variables\ncorr_train = df_train.corr()\nprint(corr_train)","05631148":"#Change the plot size \nfig, ax = plt.subplots(figsize=(30,1))\n\n#Visualizing a heatmap of the correlation with seaborn\nsns.heatmap(corr_train.sort_values(by=[\"SalePrice\"], ascending=False).head(1), annot = True, fmt='.1g')","4ef998b6":"#Scatter plot for TotalBsmtSF against SalePrice\nplt.scatter(df_train[\"TotalBsmtSF\"], df_train[\"SalePrice\"])\nplt.xlabel(\"TotalBsmtSF\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"SalePrice for TotalBsmtSF\")","7091059d":"#Scatter plot for 1stFlrSF against SalePrice\nplt.scatter(df_train[\"1stFlrSF\"], df_train[\"SalePrice\"])\nplt.xlabel(\"1stFlrSF\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"SalePrice for 1stFlrSF\")","dfb78c90":"#Scatter plot for GrLivArea against SalePrice\nplt.scatter(df_train[\"GrLivArea\"], df_train[\"SalePrice\"])\nplt.xlabel(\"GrLivArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"SalePrice for GrLivArea\")","acca496b":"#Scatter plot for GarageArea against SalePrice\nplt.scatter(df_train[\"GarageArea\"], df_train[\"SalePrice\"])\nplt.xlabel(\"GarageArea\")\nplt.ylabel(\"SalePrice\")\nplt.title(\"SalePrice for GarageArea\")","78265bcd":"#Set the graph size\nfig, ax = plt.subplots(figsize=(15,15))\n\n#Create a boxplot of GarageArea for GarageCars\nsns.boxplot(x=df_train[\"GarageCars\"], y=df_train[\"GarageArea\"]).set_title(\"GarageArea for GarageCars\")","21c72209":"#Set the graph size\nfig, ax = plt.subplots(figsize=(15,15))\n\n#Create a boxplot of SalePrice for OverallQual\nsns.boxplot(x=df_train[\"OverallQual\"], y=df_train[\"SalePrice\"]).set_title(\"SalePrice for OverallQual\")","bc4a1a6d":"#Set the graph size\nfig, ax = plt.subplots(figsize=(15,15))\n\n#Create a boxplot of SalePrice for FullBath\nsns.boxplot(x=df_train[\"FullBath\"], y=df_train[\"SalePrice\"]).set_title(\"SalePrice for FullBath\")","8e01848a":"#Set the graph size\nfig, ax = plt.subplots(figsize=(15,15))\n\n#Create a boxplot of SalePrice for GarageCars\nsns.boxplot(x=df_train[\"GarageCars\"], y=df_train[\"SalePrice\"]).set_title(\"SalePrice for GarageCars\")","e583d21d":"#Combining the train and test dataframes into one\ndf_both = pd.concat((df_train, df_test)).reset_index(drop=True)\ndf_both.drop([\"SalePrice\"], axis=1, inplace=True)\ndf_both.shape","0702eb2f":"#Cheking for missing values\ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\n#Removing all variables where there are no missing values\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)","77f0b272":"#Replacing \"NA\" with \"None\" for PoolQC\ndf_both[\"PoolQC\"] = df_both[\"PoolQC\"].fillna(\"None\")","a809de14":"#Replacing the null variables with None\nfor i in (\"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageType\", \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\", \"BsmtFinType2\", \"MasVnrType\", \"MSSubClass\"):\n    df_both[i] = df_both[i].fillna(\"None\")\n\n\ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)\n","02b4cfc0":"sns.distplot(df_train[\"LotFrontage\"])","128aa39a":"#Replacing null values with the median\ndf_both[\"LotFrontage\"] = df_both[\"LotFrontage\"].fillna(df_train[\"LotFrontage\"].median())\n\ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)","ba463978":"#Replacing null values with 0\nfor i in (\"GarageYrBlt\", \"GarageArea\", \"GarageCars\", \"BsmtFinSF1\", \"BsmtFinSF2\", \"BsmtUnfSF\", \"TotalBsmtSF\", \"BsmtFullBath\", \"BsmtHalfBath\", \"MasVnrArea\"):\n    df_both[i] = df_both[i].fillna(0)\n    \ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)","753e89a0":"#Print the value frequency for our remaining missing data columns\nfor i in (\"MSZoning\", \"Functional\", \"Utilities\", \"SaleType\", \"KitchenQual\", \"Electrical\", \"Exterior2nd\", \"Exterior1st\"):\n    print(df_both[i].value_counts())\n","ebdcd0c5":"#Replacing null values with the most common value\ndf_both[\"MSZoning\"] = df_both[\"MSZoning\"].fillna(\"RL\")\ndf_both[\"Functional\"] = df_both[\"Functional\"].fillna(\"Typ\")\ndf_both[\"Utilities\"] = df_both[\"Utilities\"].fillna(\"AllPub\")\ndf_both[\"SaleType\"] = df_both[\"SaleType\"].fillna(\"WD\")\ndf_both[\"Electrical\"] = df_both[\"Electrical\"].fillna(\"SBrkr\")\ndf_both[\"Exterior2nd\"] = df_both[\"Exterior2nd\"].fillna(\"VinylSd\")\ndf_both[\"Exterior1st\"] = df_both[\"Exterior1st\"].fillna(\"VinylSd\")\n\ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)","09d6dafd":"#Removing the row with the missing value\nKitchenQual_df = df_both.dropna(axis=0)\nKitchenQual_df.shape\n\n","11000b0c":"#Creating a dummy column where the categorical KitchenQual values are placed with dummy numerical ones so we can train our model\nprint(KitchenQual_df.dtypes)\nKitchenQual_df[\"dummy_KitchenQual\"] = KitchenQual_df[\"KitchenQual\"].map({\"Ex\": 4, \"Gd\": 3, \"TA\": 2, \"Fa\": 1, \"Po\": 0})\nprint(KitchenQual_df.dtypes)","dc1288a2":"corr_KitchenQual = KitchenQual_df.corr()\nfig, ax = plt.subplots(figsize=(30,1))\nsns.heatmap(corr_KitchenQual.sort_values(by=[\"dummy_KitchenQual\"], ascending=False).head(1), annot = True, fmt='.1g')","c517a482":"print(df_both[[\"OverallQual\", \"YearBuilt\", \"YearRemodAdd\", \"GarageCars\", \"GarageArea\", \"KitchenQual\"]].loc[[1555]])","f7579166":"from sklearn import linear_model\n\nKitchenQual_X = KitchenQual_df[[\"OverallQual\", \"YearBuilt\", \"YearRemodAdd\", \"GarageCars\", \"GarageArea\"]]\nKitchenQual_Y = KitchenQual_df[\"dummy_KitchenQual\"]\n\nregr_KitchenQual = linear_model.LinearRegression()\nregr_KitchenQual.fit(KitchenQual_X, KitchenQual_Y)\n\n\nprint(\"Predicted missing KitchenQual value: \" + str(regr_KitchenQual.predict(df_both[[\"OverallQual\", \"YearBuilt\", \"YearRemodAdd\", \"GarageCars\", \"GarageArea\"]].loc[[1555]])))","b228b037":"df_both[\"KitchenQual\"] = df_both[\"KitchenQual\"].fillna(\"TA\")\n\ndf_both_null = df_both.isnull().sum() \/ len(df_both) * 100\n\ndf_both_null = df_both_null[df_both_null != 0].sort_values(ascending=False)\nprint(df_both_null)\n","96018b48":"df_both['TotalSF'] = df_both['TotalBsmtSF'] + df_both['1stFlrSF'] + df_both['2ndFlrSF']","62be111b":"df_both.columns","d86bcc00":"#Variable which have data types object\nvar_cat = df_both.dtypes[df_both.dtypes==object].index.values.tolist()\n#Dataframe with object datatype variables\ndf_cat = df_both[var_cat]\ndf_cat.head(5)","a41ae846":"\nprint(list(df_cat.columns))\nprint(\"\\n\\n\")\n\ndf_cat_dummies = pd.get_dummies(df_cat)\n\n#from sklearn.preprocessing import OneHotEncoder\n#encode = OneHotEncoder(drop='first',sparse=False)\n#encode.fit(df_cat)\n\n#df_cat_dummies = encode.transform(df_cat)\n#df_cat_dummies = pd.DataFrame(df_cat_dummies, columns=encode.get_feature_names(), index=df_cat.index)\nprint(list(df_cat_dummies.columns))","8ff0bbdb":"df_cat_dummies.head(10)","6e66bcb0":"print(f\"The shape of combined: {df_both.shape}\")\nprint(f\"The shape of categorical variable: {df_cat.shape}\")\nprint(f\"The shape of categoricall dummy variables: {df_cat_dummies.shape}\")","c1f2879f":"print(df_both.columns)","97ea5cdc":"df_all_variables = df_both.join(df_cat_dummies)\ndf_all_variables = df_all_variables.drop(df_cat, axis=1)\ndf_all_variables.shape\n","9b1c2a06":"print(list(df_all_variables.columns))","e9778d8d":"ntrain = df_train.shape[0]\nntest = df_test.shape[0]\nprint(ntrain, ntest)","cd2760ce":"print(df_train.shape, df_test.shape)\ndf_train_final = df_all_variables[:ntrain]\ndf_test_final = df_all_variables[ntrain:]\ndf_train_final[\"SalePrice\"] = df_train[\"SalePrice\"]\n\nprint(df_train_final.shape, df_test_final.shape)","8bd269f2":"X_train = df_train_final.drop('SalePrice', axis=1)\ny_train = df_train_final[\"SalePrice\"]\nX_test = df_test_final\n\nprint(f\"X_train shape: {X_train.shape}\")\nprint(f\"X_test shape: {X_test.shape}\")\nprint(f\"y_train shape: {y_train.shape}\")\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train, y_train)","8847a0fa":"y_pred = regr.predict(X_test)\n\nfinal_submission = pd.DataFrame({\n        \"Id\": X_test[\"Id\"],\n        \"SalePrice\": y_pred\n    })\n\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","3da556d5":"# Iowa House Pricing Analysis and Modeling \n\nIn this notebook we will experiment with different tools and techniques in order to predict the **SalePrice** (Price for which houses were sold) in Iowa. Apologies in advance if some of the techniques used seem silly or \"not best practice\", we will just be trying out different tools. Nevertheless please do comment below for any tips for improvement, advice or mistakes your find, that way we can all learn. \n\n[Link](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data) to the data we will be using, with an explanation for each of the 79 explanatory variables.\n\n\n\n","104d264d":"***\nGreat now that our data is numerical and ready for use, let's see how our variables correlate with KitchenQual.\n***","34378b36":"***\n\nFor **MSZoning, Functional, Utilities, SaleType, Electrical, Exterior2nd** and **Exterior1st** there is one value that is dominantly more common than the others and so we will replace the null values of these variables with the most common value.  \n\nHowever, for KitchenQual we will do what we mentioned before. We will use machine learning algorithms to predict the most lickly value for our missing data point.\n\n***","43cf89c1":"***\n\n**Interpreting the missing data**\n\nAbove we can see all the missing data points we have and what percentage of them is missing.\n\n* PoolQC = according the the [description](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques\/data?select=data_description.txt) NA mean that their is no pool, so we can replace our NA values with \"None\". \n\n***","df0a439b":"***\n\nSo we can see our predicted **KitchenQual** value is **1.80673069**. Because we used dummy values for our model we need to see which dummy value our prediction is closest to and that would be **\"TA\"** (2).\n\nSo we just need to replace our last missing value with **\"TA\"** in our dataset and we will complete have all our datapoints. \n\n***","6070e254":"***\n\nWe can see that the following values have the highest correlation with kitchenQual:\n\n* **OverallQual**\n* **YearBuilt**\n* **YearRemodAdd**\n* **GarageCars**\n* **GarageArea**\n\nLooking at these variables it seems logical that there is a strong causality between them, except for **GarageCars** and **GarageArea** where the causality is slightly less clear. However, it can be argued that good quality requires size and so the house is generally big, has a big garage with many cars and a big kitchen which leads to a higher **KitchenQual**.\n\n*It's important to remember that correlation doesn't always prove causation and applying some logic beyond the numbers can sometimes provide added values. Although in general when predicting the causation isn't too significant, you are likely to get more accurate prediction by including correlation that have no causation.*\n\n[spurious correlations](https:\/\/www.tylervigen.com\/spurious-correlations)\n\nWe will nose use these values to train a linear regression model and use it to predict out missing value for **KitchenQual**.\n\n***","9985d989":"***\n**Great now that we trained our model, we can predict the SalePrice from our test sample and submit out results.**\n***","214be337":"***\nTo strengthen our prediction we will combine 3 of our variables to create a new variable **TotalSF** which is the total surface area of the house. House prices tend be strongly affected by their surface area and so adding this variable could be a good idea for strengthening our model. \n\n***","72edaec2":"***\n\n**Observing the correlations**\n\nLooking at the correlation heatmap it appears that the following variables correlate with **SalePrice**:\n\n* OverallQual = 0.8\n* TotalBsmtSF = 0.6\n* 1stFlrSF = 0.6\n* GrLivArea = 0.7\n* FullBath = 0.6\n* GarageCars = 0.6\n* GarageArea = 0.6 \n\n***GarageArea** and **GarageCars** both have a pearson correlation coefficient of 0.6 with **SalePrice** and 0.882475 with each other. This raises a red flag as they both have a similar correlation coefficient and they both relate to the house's garage (**GarageArea** is the area of the garage and **GarageCars** is the amount of cars that can fit in the garage), hence, they could be very closely related and thus including both of them in our modeling overrepresent this feature and make our models less accurate.*\n\nLet's graph these variables against **SalePrice** to better understand the relationship (we will also graph **GarageArea** and **GarageCars**).\n\n***","f1e6a1e0":"***\n\n# 4)Modeling\n\nBased on our data exploration we will use multiple linear regression to model our data and try and predict out test **SalePrice** values.\n\nLet's begin by splitting our combined dataset into our original train and test samples (with the added columns and completed values ofcourse). \n\n*make sure to include the **SalePrice** in our train sample, as we removed it in our combined dataset.*\n\n***","a6b02991":"***\n\nWe narrowed down our missing data to just 19 columns.\n\nHowever, **lotFrontage's** missing data can't be replaced with \"None\" like we did for the previous variables as all houses have \"Linear feet of street connected to property\", so we will have to use a different method to complete our data.\n\nLet's look at the distribution of the train sample **lotFrontage**.\n\n*We will use the train sample to avoid leakage which can make our modells overly optimistic.*\n\n***","cc3dc9ff":"***\n\n# 2)Data cleaning and feature engineering \n\nLet's remember what we hope to achieve from this part of our journey.\n\n* Deal with our missing values. \n* Cope with our different data types. \n* Prepare our data for modeling. \n* Fix any errors that may arise from our data wrangling.  \n\n**It's very important we proceed our data manipulations with caution. Often if we are not careful we can make mistakes that will lead to biased input for our models.**\n\nWe will begin by combining our training and test samples so we can see all the missing data we have, and have a larger sample at our disposal for completing these missing data points.\n\n***","d9587756":"***\n# 1) Data Exploration\n\nIn data exploration we try to better understand our dataset. We will use data visualization and profiling. What we discover in this stage of our analysis will affect how we model our data later on, so it's important to be open minded, methodical, and creative to ensure we cover as many significant insights we can.\n\n*As we saw we have about 1460 houses in each data frame (test and train). *\n\n****Aim in data exploration:****\n* What data types do we have? \n* Which of the factors are categorical and which are continuous \n* Descriptive statistics \n* Distributions\n* Relationships and associations \n\nData exploration is meant to help us understand the dataset better so that eventually we can model it effectively, so keep in mind when exploring how we can use our discoveries when modeling.  \n***","610f6faa":"***\n\nSo we now have our additional variable **TotalSF**, we are almost ready to begind modeling.\n\nAt the moment a large portion of our variables are categorical and don't use numbers, and as we will be using models that require numerical input for training we will have to change that.\n\nWe can create dummy variables to represent our categorical variables in our model (similar to what we did with KitchenQual).\n\n***","702fe866":"***\n\nWe can see that the distribution  is fairly skewed and there seem to be several outliers, hence making the median a more sensible choice than the mean as a replacement for our missing values. \n\n*Ideally we would use a machine learning technique to predict each of the missing values based on the values for which we have data. This would allow us to fill in the missing values much more accurately than simply using the median.*\n\n***","74f87920":"***\n\nFor our remaining variables we will have have to find the most sensible way to fill them in. Let's look at the frequency of the data for each value. \n\n***","ce3cf8e6":"***\n\nNow that we've created our dummy columns and added them to our data we can can finally begin modeling. \n\n***","72da0447":"***\n\n**SalePrice has a positive skew**\n\nAs we can tell by our graph, the distribution of **SalePrice** is positively skewed.\n\n\n\n*I've seen that several other notebooks transformed the data so it is normally distributed, because regression works better with normally distributed dependent variables. I understood that the only assumption about normal distribution is that the errors are normally distributed. Thank you to anyone that can confirm in the comments.*\n\n\nBy looking at our columns we can already suspect certain variables will influence **SalePrice** more than offers. However, to ensure we don't miss out on anything let's look at all the correlations among the numerical variables in our dataset. \n***","130f6623":"***\n\nWe can conclude the following from our boxplots:\n\n* **OverallQual** : The higher the **OverallQual** the higher the **SalePrice** on average (possibly even exponential).\n* **FullBath** : There appears to be a fairly strong association between **FullBath** and **SalePrice** with the averages ranging at approximetly 170,000. \n* **GarageCars** : Although **GarageCars** does appear to have a weaker association than the other variables specifically the **SalePrice** of 3 **GarageCars** appears to be significantly higher.  \n\nIt appears that all of these variables have a certain relationship with **SalePrice**, hence we will use them in our model.\n\nGreat! So far we explored the variables for which we saw a high correlation in our heatmap. However, our heatmap didn't take into account non numerical variables as well as any variables which had a missing data point.\n\nSo now we will begin tacking our missing data problem.\n\n***","5cdafb2b":"***\n\nAfter filling in **lotFrontage** we only have 18 variables with missing data let's refer back to the data description to see what else we can fill in. \n\nFor the null values in **GarageYrBlt** we can see that it's the case when there is no garage and so we can replace it with 0. The same is the case for the following variables: GarageArea, **GarageCars, BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath, BsmtHalfBath, MasVnrArea**.\n\n***","eecb8525":"***\n\n### 3)Feature engineering\n\nFirst we will need to remove the rows with missing values, that way we have clean data to train our model.\n\n*Perhaps it is also bad practice to use the combined dataset for training the model, as it might cause leakage please let me know in the discussion if this is the case.*\n\nAnother problem with our data is that **KitchenQual** is categorical and its data is not in numbers. For our models we will want our input data to be numerical. So we will have to use dummy variables to represent the categorical variables in our model.\n***","7ef96446":"***\n\nThe same applies for the following variables: **MiscFeature, Alley, Fence , FireplaceQu, GarageType, GarageFinish, GarageQual, GarageCond, BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, MasVnrType, MSSubClass**.  So we will also replace those null values with \"None\".\n\n***","fa347884":"***\n**Linear relationship**\n\nLooking at the scatter plots it appears that the continuous variables for which we found a high correlation(**GarageArea**, **GrLivArea**, **1stFlrSF** and **TotalBsmtSF**) have a positive linear relationship with **SalePrice**. This is a strong indication the a linear model might be a good choice. \n\nLooking at the relationship between **GarageArea** and **GarageCars**, although there is a very obvious relationship it's insufficient to justify removing one of the variables when modeling.  *-VIF analysis should be used to make a more informed decision-* \n\nNow that we explored the relationship between our continuous variables let's use boxplots to explore the relationship for the categorical ones.\n***"}}