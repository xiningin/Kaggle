{"cell_type":{"3acbb733":"code","05ca73fc":"code","58b0a2b6":"code","31f9cc8f":"code","1e4ee709":"code","b2ffeff4":"code","33152666":"code","9e653c51":"code","053ccecd":"code","44a29768":"code","cddb5c55":"code","6f3fe06b":"code","0d52de15":"code","f0cf0877":"code","c42cd1e0":"code","23538567":"code","aace1398":"code","a0a07b59":"code","645f45ee":"code","8dc3b3d8":"code","27970473":"code","dd17d56e":"code","267aa44c":"code","b0e4d20e":"code","245652e2":"code","6e14af49":"code","5c0887c8":"code","c8be9731":"code","3ef151bb":"code","17f85656":"code","87eb67fd":"code","b5091e61":"code","f44e5529":"code","b9276398":"code","74e2907b":"code","9b5d46c1":"code","89394c81":"code","e3ac224b":"code","01f6fa14":"code","1968dce8":"code","b2c7525d":"code","5cfafaff":"code","9076e4e2":"code","ac42fea4":"code","ec75e43c":"code","ae062966":"code","32bccc2d":"code","d7b0a4ff":"code","d5bd881b":"code","2a9e8ae6":"markdown","72f160d2":"markdown","b28b2873":"markdown","9b6b7af1":"markdown","23d1c25f":"markdown"},"source":{"3acbb733":"!pip uninstall fastai -y","05ca73fc":"import os,sys\nsys.path.append('..\/input\/fastaiv1')\n!pip install -q object-detection-fastai","58b0a2b6":"import warnings\nwarnings.filterwarnings(\"ignore\")","31f9cc8f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom collections import defaultdict\nimport os\nfrom tqdm.notebook import tqdm\nfrom fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks import *\nimport seaborn as sns \nimport matplotlib.pyplot as plt\nimport matplotlib.image as immg\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom sklearn.model_selection import StratifiedKFold,KFold\n\nfrom object_detection_fastai.helper.object_detection_helper import *\nfrom object_detection_fastai.loss.RetinaNetFocalLoss import RetinaNetFocalLoss\nfrom object_detection_fastai.models.RetinaNet import RetinaNet\nfrom object_detection_fastai.callbacks.callbacks import BBLossMetrics, BBMetrics, PascalVOCMetric","1e4ee709":"import pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut","b2ffeff4":"sns.set_style('darkgrid')","33152666":"df = pd.read_csv('..\/input\/vinbigdata-weighted-bbox-fusion\/weighted_box_fused_train_vinBigData.csv')\nimg_dim = pd.read_csv('..\/input\/vinbigdata-resized-image-512\/train_meta.csv')\ntr_img_dir = Path('..\/input\/vinbigdata-resized-image-512\/train')\nts_img_dir = Path('..\/input\/vinbigdata-resized-image-512\/test')","9e653c51":"tr_df = df.merge(img_dim,on='image_id',how='left')","053ccecd":"tr_df.head(10)","44a29768":"tr_df1 = tr_df[(tr_df['class_id']!=14)&(tr_df['class_id']!=4)&(tr_df['class_id']!=2)&(tr_df['class_id']!=9)].copy()","cddb5c55":"tr_df1['x_min'] = tr_df1['x_min']*512\/tr_df['dim1']\ntr_df1['x_max'] = tr_df1['x_max']*512\/tr_df['dim1']\ntr_df1['y_min'] = tr_df1['y_min']*512\/tr_df['dim0']\ntr_df1['y_max'] = tr_df1['y_max']*512\/tr_df['dim0']","6f3fe06b":"tr_df1.head()","0d52de15":"tr_df1.class_id.value_counts()","f0cf0877":"unq_id = tr_df1.image_id.unique()","c42cd1e0":"df_grp = tr_df1.groupby(['image_id'])","23538567":"one_hot = {}\nfor i in tqdm(unq_id):\n    l = np.zeros(14)\n    temp = df_grp.get_group(i)\n    for j in temp.class_id.unique():\n        l[j] = 1\n    one_hot[i] = l","aace1398":"df_grp.get_group('9a5094b2563a1ef3ff50dc5c7ff71345')","a0a07b59":"fold_df = pd.DataFrame.from_dict(one_hot,orient='index');\nfold_df.reset_index(inplace=True)\nfold_df.columns = ['image_id']+fold_df.columns.tolist()[1:]","645f45ee":"sys.path.append('..\/input\/multistartifiedkfold')","8dc3b3d8":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold","27970473":"fold_df['kfold'] = -1\nfold_df = fold_df.sample(frac=1.0).reset_index(drop=True)\ny = fold_df[fold_df.columns.tolist()[1:]].values\nkf = MultilabelStratifiedKFold(n_splits=5)\nfor fld, (trn_,val_) in enumerate(kf.split(X=fold_df,y=y)):\n    fold_df.loc[val_,'kfold'] = fld","dd17d56e":"fold_df.head()","267aa44c":"b_fea = ['x_min', 'y_min', 'x_max', 'y_max']","b0e4d20e":"name = '0c7a38f293d5f5e4846aa4ca6db4daf1'\nloc = '..\/input\/vinbigdata-resized-image-512\/train\/'+name+'.png'\naaa = df_grp.get_group(name)\nbbx = aaa.loc[:,b_fea]\nimg = immg.imread(loc)\nfig,ax = plt.subplots(figsize=(18,10))\nax.imshow(img,cmap='binary')\nfor i in range(len(bbx)):\n    box = bbx.iloc[i].values\n    x,y,w,h = box[0], box[1], box[2]-box[0], box[3]-box[1]\n    rect = patches.Rectangle((x,y),w,h,linewidth=1,edgecolor='r',facecolor='none',)\n    ax.text(*box[:2], aaa['class_id'].iloc[i], verticalalignment='top', color='white', fontsize=12, weight='bold')\n    ax.add_patch(rect)\nplt.show()","245652e2":"def get_lbl_img(train):\n    chest2bbox = {}\n    grp = train.image_id.unique()\n    tr_gr = train.groupby(['image_id'])\n    from tqdm.notebook import tqdm\n    for i in tqdm(range(len(grp))):\n        name = str(grp[i])+'.png'\n        bbox = []\n        lbls = []\n        temp_b = []\n        temp = tr_gr.get_group(grp[i])\n        tt = temp.loc[:, (['class_id','x_min', 'y_min', 'x_max', 'y_max'])].values\n        for j in range(len(temp)):\n            lbls.append(tt[j][0].astype(int))\n            b = list(np.round(tt[j][1:]))   # x,y, width, height\n            # Currently our coordinates are x,w,l,h and we want x1,y1,x2,y2\n            # To convert it, we need to add our width and height to the respective x and y.\n            t1 = [b[1],b[0],b[3],b[2]]\n\n            temp_b.append(t1)\n        bbox.append(temp_b)\n        bbox.append(lbls)\n        chest2bbox[name] = bbox\n    return chest2bbox","6e14af49":"chest2bbox = get_lbl_img(tr_df1)","5c0887c8":"chest2bbox['0c7a38f293d5f5e4846aa4ca6db4daf1.png']","c8be9731":"get_y_func = lambda o: chest2bbox[Path(o).name] ","3ef151bb":"tr = tr_df1.image_id.value_counts()\ntr = pd.DataFrame({'image_id':tr.index,'image_count':tr.values})\ntr = tr.sample(frac=1.,random_state=2020).reset_index(drop=True)","17f85656":"fold_df.to_csv('kfold_train.csv')","87eb67fd":"FOLD = 2","b5091e61":"trn_idx,val_idx = fold_df[fold_df['kfold']!=FOLD].index,fold_df[fold_df['kfold']==FOLD].index","f44e5529":"len(trn_idx),len(val_idx)","b9276398":"data = (ObjectItemList.from_df(fold_df,path='..\/input\/vinbigdata-resized-image-512', folder = 'train' ,cols='image_id',suffix='.png')\n        #Where are the images? ->\n        .split_by_idxs(train_idx=trn_idx, valid_idx=val_idx)                         \n        #How to split in train\/valid? -> randomly with the default 20% in valid\n        .label_from_func(get_y_func)\n        #How to find the labels? -> use get_y_func on the file name of the data\n        .transform(size=512,resize_method=ResizeMethod.SQUISH)\n        #.add_test(ts)\n        #Data augmentation? -> Standard transforms; also transform the label images\n        .databunch(bs=16, collate_fn=bb_pad_collate))","74e2907b":"data.show_batch( 1, figsize = (15,10))","9b5d46c1":"len(data.train_ds),len(data.valid_ds)","89394c81":"data.classes,data.c","e3ac224b":"size = 512","01f6fa14":"anchors = create_anchors(sizes=[(32,32),(16,16),(8,8),(4,4)], ratios=[0.5, 1.0, 2.0], scales=[0.25, 0.65, 1.25, 2.5, 3.5, 4.5])","1968dce8":"fig,ax = plt.subplots(figsize=(10,10))\nax.imshow(image2np(data.valid_ds[0][0].data))\n\nfor i, bbox in enumerate(anchors[:18]):\n    bb = bbox.numpy()\n    x = (bb[0] + 1) * size \/ 2 \n    y = (bb[1] + 1) * size \/ 2 \n    w = bb[2] * size \/ 2\n    h = bb[3] * size \/ 2\n    \n    rect = [x,y,w,h]\n    draw_rect(ax,rect)","b2c7525d":"len(anchors)","5cfafaff":"n_classes = data.train_ds.c\n\ncrit = RetinaNetFocalLoss(anchors)\n\nencoder = create_body(models.resnet34, True, -2)\n\nmodel = RetinaNet(encoder, n_classes=data.train_ds.c, n_anchors=18, sizes=[32,16,8,4], chs=32, final_bias = -4., n_conv = 2)","9076e4e2":"voc = PascalVOCMetric(anchors, size, [i for i in data.train_ds.y.classes[1:]])\nlearn = Learner(data,\n                model, \n                loss_func=crit,\n                callback_fns=[BBMetrics],\n                metrics=[voc],\n                model_dir = '\/kaggle\/working\/')","ac42fea4":"learn.split([model.encoder[6], model.c5top5]);\nlearn.freeze_to(-2)\n#learn = learn.to_fp16()","ec75e43c":"learn.fit_one_cycle(3, 1e-3 ,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='valid_loss', name ='best_model1',mode='min')])","ae062966":"learn.unfreeze()\nlearn.fit_one_cycle(10,1e-3\/2,callbacks = [SaveModelCallback(learn, every ='improvement', monitor ='valid_loss', name ='best_model2',mode='min')])","32bccc2d":"learn.recorder.plot_losses()","d7b0a4ff":"learn.load('best_model2');","d5bd881b":"show_results_side_by_side(learn, anchors, detect_thresh=0.4, nms_thresh=0.1, image_count=10)","2a9e8ae6":"# Fastai Tutorial for VinBigDate Chest Xray Abnormalities Detection\n\n## With Weighted Bbox fusion","72f160d2":"## Get lblbox","b28b2873":"## Rescaling bounding boxes according to resized images","9b6b7af1":"## Sample Check","23d1c25f":"## MultiStratified KFOLD"}}