{"cell_type":{"a9393978":"code","e435301e":"code","7b3e2849":"code","68ef7439":"code","89d66c68":"code","840080a9":"code","11ea1bb1":"code","5505c238":"code","73027e60":"code","3e02f46e":"code","f3db4c7f":"code","e006ec7c":"code","76fbeb10":"code","ea9e7513":"code","a7c010e6":"code","9aa4cb00":"code","305446cf":"code","a7c78372":"code","ae07eb59":"code","4ac8087c":"code","6d94c723":"code","85375803":"code","57119eff":"code","4f848647":"code","c2488596":"code","a23b864e":"code","159b29fa":"code","02ac8ef4":"code","06315633":"code","dec6d0f8":"code","976711e5":"code","98c106ba":"code","5a2cfb5c":"code","305cf54c":"code","5c245181":"code","3ee152fa":"code","35253fcd":"code","db6b8b0c":"code","01358eaa":"code","234b5995":"code","7eb612da":"code","d9ec81a0":"code","63b3625e":"code","22d46ae8":"code","959cd646":"code","9047c0b3":"code","280b731b":"code","96a586c5":"code","6f294b09":"code","1a003f24":"code","dbc6f46c":"code","8d2261ec":"code","c9f742ea":"code","d2d7fa51":"code","def14f44":"markdown","d3591d71":"markdown","93b0c34f":"markdown","d30125f0":"markdown","ccf14e01":"markdown","766ac5b1":"markdown","0d100574":"markdown","0822e723":"markdown","13ea4e6d":"markdown","1bef954c":"markdown","d48e7b01":"markdown","6aac3f6f":"markdown","02982cee":"markdown","ff875897":"markdown","d2923f96":"markdown","fc1e3e3d":"markdown","dafde32f":"markdown"},"source":{"a9393978":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import WordNetLemmatizer\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras import Sequential\nfrom keras.layers.recurrent import LSTM,GRU\nfrom keras.layers import Dense,Dropout,BatchNormalization,Bidirectional,Embedding,Flatten\nfrom keras.layers import Conv1D,MaxPool1D,GlobalAveragePooling1D\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport gensim\nfrom gensim.models.word2vec import Word2Vec\n\nnltk.download('stopwords')\nstopwords=stopwords.words(\"english\")\nnltk.download('wordnet')","e435301e":"train=pd.read_csv(\"\/content\/drive\/MyDrive\/Colab Notebooks\/ComparisonB wEmbedding techniques\/Twitter_Data.csv\")","7b3e2849":"train.shape","68ef7439":"train.head(3)","89d66c68":"# Changing the category labels\ntarget=[]\nfor i in train['category']:\n  if i==-1.0:\n    target.append(0)\n  elif i==0.0:\n    target.append(1)\n  else:\n    target.append(2)\n\ntrain['target']=target","840080a9":"# The dataset looks imbalanced \ntrain['target'].value_counts()","11ea1bb1":"def cleaning(text):\n  cleaned_text=[]\n  lemm_obj=WordNetLemmatizer()\n  tokens_list=re.split(\" \",str(text))\n  for token in tokens_list:\n    token_small=token.lower() #converting to lower case\n    punc_removed=re.sub(\"[^a-z A-Z 0-9]\",'',token_small)\n\n    if punc_removed not in stopwords:\n      cleaned_text.append(lemm_obj.lemmatize(punc_removed))\n  clean_text=\" \".join(cleaned_text)\n  return clean_text\n","5505c238":"%%time\ntrain[\"Cleaned tweet\"]=train[\"clean_text\"].apply(cleaning)","73027e60":"train.head(3)","3e02f46e":"tokenizer_obj=Tokenizer()\ntokenizer_obj.fit_on_texts(train['Cleaned tweet'])","f3db4c7f":"# Looking at some of the key value pairs generated\n\n# tokenizer_obj.word_index.keys()","e006ec7c":"#Defining the vocabulary size for training embedding layer\nvocab_size=len(tokenizer_obj.word_index)+1\nprint(vocab_size)","76fbeb10":"def one_hot_encoding_text(df_list):\n  encoded_tweets=[]\n  for tweets in df_list:\n    encoded_tweets.append(tokenizer_obj.texts_to_sequences([tweets])[0])\n  return encoded_tweets","ea9e7513":"%%time\nencoded_tweet=np.array(one_hot_encoding_text(train['Cleaned tweet']))\n# test_tweet=np.array(one_hot_encoding_text(test['Cleaned tweet'])) #test data","a7c010e6":"# Adding padding to make dimensions of all the rows same\nmax_length=100\npadded_encTweet=pad_sequences(encoded_tweet,maxlen=max_length,padding=\"post\") #train data\n# padded_testt=pad_sequences(test_tweet,maxlen=max_length,padding=\"post\") #test data\npadded_encTweet[0]","9aa4cb00":"X=padded_encTweet\ny=train['target']","305446cf":"x_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=32)","a7c78372":"model=Sequential()\n\n#Embedding Layer\nmodel.add(Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length))\nmodel.add(Dense(units=500,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(units=300,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\nmodel.add(Dense(units=200,activation='relu'))\n\nmodel.add(Flatten()) # To flatten the 3d matrix to 2d ,can use globalpooling1d also\nmodel.add(Dense(units=3,activation='softmax'))","ae07eb59":"model.summary()","4ac8087c":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","6d94c723":"#The loss is still dreacreasing and the accuracy is increasing, so train it for at least 30 epochs to get a decent accuracy\nmodel.fit(x=x_train,y=y_train,epochs=5,batch_size=512,validation_data=(x_test,y_test))","85375803":"model=Sequential()\n\n#Embedding Layer\nmodel.add(Embedding(input_dim=vocab_size,output_dim=300,input_length=max_length))\nmodel.add(Dense(units=500,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(units=300,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv1D(filters=300,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv1D(filters=100,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=3))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Flatten()) # To flatten the 3d matrix to 2d ,can use globalpooling1d also\nmodel.add(Dense(units=3,activation='softmax'))","57119eff":"model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])","4f848647":"#The loss is still dreacreasing and the accuracy is increasing, so train it for at least 30 epochs to get a decent accuracy\nmodel.fit(x=x_train,y=y_train,epochs=5,batch_size=512,validation_data=(x_test,y_test))\n\n# Epoch 1\/5\n# 223\/223 [==============================] - 165s 606ms\/step - loss: 1.1508 - accuracy: 0.4791 - val_loss: 1.6432 - val_accuracy: 0.3389\n# Epoch 2\/5\n# 223\/223 [==============================] - 134s 600ms\/step - loss: 0.8148 - accuracy: 0.6325 - val_loss: 1.4757 - val_accuracy: 0.5078\n# Epoch 3\/5\n# 223\/223 [==============================] - 133s 598ms\/step - loss: 0.5691 - accuracy: 0.7765 - val_loss: 0.7508 - val_accuracy: 0.6751\n# Epoch 4\/5\n# 223\/223 [==============================] - 133s 598ms\/step - loss: 0.5024 - accuracy: 0.8135 - val_loss: 0.6313 - val_accuracy: 0.7545\n# Epoch 5\/5\n# 223\/223 [==============================] - 133s 598ms\/step - loss: 0.3840 - accuracy: 0.8760 - val_loss: 0.4831 - val_accuracy: 0.8462\n\n# <keras.callbacks.History at 0x7fe8f3ec6650>\n\n","c2488596":"%%time\n#Lets first load the file and look at first few words\n\ndata_file = open(\"\/content\/drive\/MyDrive\/Colab Notebooks\/ComparisonB wEmbedding techniques\/glove.6B.100d.txt\",encoding=\"UTF-8\")\n\ncount=0\nfor i in data_file:\n  if count<=3:\n    print(i)\n  else:\n    break\n  count+=1\n","a23b864e":"%%time\n#Now lets create a dictionary having words as keys and vectors as value\n\nglove_dict={}\nfor i in data_file:\n  splitted_data=i.split()\n  word=splitted_data[0]\n  vect=np.array(splitted_data[1:],dtype=\"float32\")\n  glove_dict[word]=vect\n\nprint(f\"The length of the dictionary created is \",{len(glove_dict)})","159b29fa":"train['target'][1]","02ac8ef4":"%%time\ndef OneHotUsingGlove(df_data):\n  OneHotVect=[]\n  noVec=[]\n  y_new=[]\n  counter=0\n  for j in df_data:\n    temp=[]\n    tokens_list=j.split()\n    for i in tokens_list:\n      if i in glove_dict.keys():\n        temp.append(glove_dict[i])\n      else:\n        noVec.append(i)\n    if len(temp) != 0:\n      OneHotVect.append(np.array(temp))\n      y_new.append(train['target'][counter])\n      counter+=1\n  return np.array(OneHotVect),noVec,y_new\n\nEncoded_vect,exemptedWords,y_new=OneHotUsingGlove(train['Cleaned tweet'])","06315633":"# print(exemptedWords)","dec6d0f8":"added_vec=[]\ncounter=0\nfor i in range(0,len(Encoded_vect)):\n  added_vec.append(Encoded_vect[i].sum(axis=0).reshape(1,100))\n\nadded_vec=np.array(added_vec)","976711e5":"added_vec[0].shape","98c106ba":"type(y_new[0])","5a2cfb5c":"X=added_vec.reshape(-1,100)\ny=np.array(y_new)\n\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=32)","305cf54c":"scalar=StandardScaler()\nx_scaled=scalar.fit_transform(x_train)\nx_valid_scaled=scalar.transform(x_test)","5c245181":"#training simple ANN\n\nmodel = Sequential()\n\nmodel.add(Dense(500, input_dim=100, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(400, activation='relu'))\n\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.3))\n\nmodel.add(Dense(3,activation=\"softmax\"))\n\n# compile the model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\nmodel.summary()","3ee152fa":"model.fit(x_scaled,y_train,batch_size=512,epochs=30,validation_data=(x_valid_scaled,y_test))","35253fcd":"#check the upper code to understand these variables\n#here since i is starting from i=0 therefore while scripting we use i+1 since in the tokenized word\n#indexing starts from 1\n\nweight_matrix = np.zeros((vocab_size,100))\nnovec=[]\nfor i,word in enumerate(tokenizer_obj.word_index.keys()):\n  temp=glove_dict.get(word)\n  if temp is not None:\n    weight_matrix[i+1]=temp\n  else:\n    novec.append(word)\n","db6b8b0c":"X=padded_encTweet\ny=train['target']\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=32)","01358eaa":"model=Sequential()\n\n#Embedding Layer\nmodel.add(Embedding(input_dim=vocab_size,output_dim=100,input_length=max_length,weights=[weight_matrix],trainable=\"False\"))\nmodel.add(Dense(units=500,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(units=300,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv1D(filters=300,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=2))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(Dropout(0.6))\n\nmodel.add(Conv1D(filters=100,kernel_size=3,activation='relu'))\nmodel.add(MaxPool1D(pool_size=3))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.6))\n\nmodel.add(Flatten()) # To flatten the 3d matrix to 2d ,can use globalpooling1d also\nmodel.add(Dense(units=3,activation='softmax'))\nmodel.summary()","234b5995":"model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])","7eb612da":"model.fit(x_train,y_train,epochs=10,batch_size=512 ,validation_data=(x_test,y_test))","d9ec81a0":"# pip install gensim","63b3625e":"#lets first tokenize the words and convert it into sepratae lists\n\n#It does do any sort of stemming or lemmatizing, we can always create a custom function for this, but lets\n#leave it for simplicity\n\ngensim.utils.simple_preprocess('Hello my name is! N@Ilay, I am a runner')","22d46ae8":"gensim_tweet=[]\nfor i in train['clean_text']:\n  gensim_tweet.append(gensim.utils.simple_preprocess(str(i)))\n\ntrain['gensim_tweet']=gensim_tweet","959cd646":"train.head(3)","9047c0b3":"#lets create the word2vec model\n%%time\nw2v_model= Word2Vec(sentences=train['gensim_tweet'],window=5,min_count=3,workers=4)\nw2v_model.train(train['gensim_tweet'],epochs=5,total_examples=len(train))","280b731b":"#lets check similar word\nw2v_model.wv.most_similar('hello',topn=5)","96a586c5":"#lets see the vector representation of the word\nw2v_model.wv[\"good\"]","6f294b09":"w2v_model.wv[\"king\"]-w2v_model.wv[\"man\"]+w2v_model.wv[\"woman\"]","1a003f24":"w2v_weight_matrix = np.zeros((vocab_size,100))\nnovec=[]\nfor word,i in (tokenizer_obj.word_index.items()):\n  try:\n    temp=w2v_model.wv[word]\n    w2v_weight_matrix[i]=temp\n  except:\n    novec.append(word)\n","dbc6f46c":"# novec","8d2261ec":"X=padded_encTweet\ny=train['target']\nx_train,x_test,y_train,y_test=train_test_split(X,y,test_size=0.3,random_state=32)","c9f742ea":"#Just copy paste the previous Model that we used for the glove vectors\nmodel=Sequential()\n\n#Embedding Layer\nmodel.add(Embedding(input_dim=vocab_size,output_dim=100,input_length=max_length,weights=[w2v_weight_matrix],trainable=\"False\"))\nmodel.add(Dense(units=500,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.8))\n\nmodel.add(Dense(units=300,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.9))\n\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.8))\nmodel.add(Dense(units=200,activation='relu'))\nmodel.add(Dropout(0.8))\nmodel.add(Flatten()) # To flatten the 3d matrix to 2d ,can use globalpooling1d also\nmodel.add(Dense(units=3,activation='softmax'))\nmodel.summary()","d2d7fa51":"model.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\",metrics=[\"accuracy\"])\nmodel.fit(x_train,y_train,epochs=30,batch_size=512 ,validation_data=(x_test,y_test))","def14f44":"One thing we can see that most of the words that didn't had a glove vector are either hindi word or some misspelled words, however it might be that the performance of glove might be affected because we don't have word vectors for hindi words in this case whereas in the embedding layer since we trained our own vocabulary so it might be that in that case the performance could be little good because we also had word vectors for the hindi words as well in that case","d3591d71":"Train test split","93b0c34f":"Though in this case the presence of pooling layers isn't helping the model that much but sometimes pooling also helps in increasing the accuracy","d30125f0":"<h1>Using Glove<\/h1>","ccf14e01":"Importing Libraries","766ac5b1":"As we can see using Simple ANN with glove vectors we get a very bad training and test accuracy. So, lets try to create a non trainable embedding layer having the weights from the Glovevectors","0d100574":"So we can see the accuracy of Glove vectors with embedding layer is pretty good and its the best till now though glove vectors didn't had some of the hindi words in their vocabulary still it performs very good on the twitter data.\nNow, we will try to train Word2Vec model using Gensim Library","0822e723":"Lets now create one hot encoding of the cleaned tweet using keras tokenizer class","13ea4e6d":"<center><h1> Thank You<\/h1><\/center>\n\nIn case of any queries or suggestions, you can reach me over LinkedIn :- https:\/\/www.linkedin.com\/in\/nilaykush\/","1bef954c":"<h1>Embedding layer using Glove<\/h1>","d48e7b01":"Now we will create the word2vec weight matrix in the similar fashion as we created for the Glove vectors and then feed it to the embedding layer of the neural network","6aac3f6f":"Now lets one hot encode encode the cleaned tweets on the basis of this glove vectors ","02982cee":"<h1>Word2Vec Using Gensim <\/h1>","ff875897":"<b>In this notebook we will make a comparison between the various word embedding techniques mainly embedding Layer of Keras, GloVe and Word2Vec on twitter dataset<b>","d2923f96":"The data in the glove text file that we have downloaded from the Standford website is in key value pair, where the word is represented as the key and the 100 dimensional vector is as the value","fc1e3e3d":"<h1> Using Conv and Pooling layer to check if it helps <\/h1>","dafde32f":"<h1>Embedding Layer<\/h1>"}}