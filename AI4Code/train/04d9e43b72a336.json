{"cell_type":{"3f5f4cbc":"code","2fced68d":"code","8036d761":"code","53bf2bed":"code","f9aa87af":"code","15a35337":"code","24d3b697":"code","c31010cb":"code","a8b39ca0":"code","f309b2c2":"code","31c8efbf":"code","da182984":"code","03a4864b":"code","11ce404b":"code","42134085":"markdown","52707ead":"markdown","303afac8":"markdown","290b3d9f":"markdown","af5606ba":"markdown","1c0fa290":"markdown","fd820649":"markdown","3cd94960":"markdown"},"source":{"3f5f4cbc":"%matplotlib inline\nimport d2l_utils_eg as d2l\nfrom mxnet import autograd, contrib, gluon, image, init, nd\nfrom mxnet.gluon import nn\nfrom mxnet.gluon.model_zoo import vision\n","2fced68d":"class residual_block(nn.Block):\n    def __init__(self, c, use_conv1x1=False, strides=1,**kwargs):\n        super().__init__(**kwargs)\n        self.main_path = nn.Sequential()\n        \n        self.main_path.add(\n             nn.Conv2D(c,kernel_size=3,padding=1, strides=strides),\n             nn.BatchNorm(),\n             nn.Activation('relu'),\n             nn.Conv2D(c,kernel_size=3, padding=1),\n             nn.BatchNorm()\n        )\n        self.conv1x1 = None\n        if use_conv1x1:\n            self.conv1x1 = nn.Sequential()\n            self.conv1x1.add(\n                 nn.Conv2D(c,kernel_size=1, strides=strides)\n            )\n        self.final_relu = nn.Activation('relu')\n    def forward(self,x):\n        o = self.main_path(x)\n        if self.conv1x1:\n            return self.final_relu(o+self.conv1x1(x))\n        else: \n            return self.final_relu(o+x)\nclass resnet_block(nn.Block):\n    def __init__(self, c, num_residuals, first_block = False, **kwargs):\n        super().__init__(**kwargs)\n        self.block = nn.Sequential()\n        for i in range(num_residuals):\n            if i == 0 and not first_block:\n                self.block.add(residual_block(c=c, use_conv1x1=True, strides=2))\n            else:\n                self.block.add(residual_block(c=c))\n    def forward(self, x):\n        return self.block(x)\n        \nclass Resnet18(nn.Block):\n    \n    def __init__(self, **kwargs):\n        super().__init__(**kwargs)\n        self.model = nn.Sequential()\n        \n        self.model.add(\n            nn.Conv2D(channels=64, kernel_size=7, strides=2, padding=3),\n            nn.BatchNorm(),\n            nn.Activation('relu'),\n            nn.MaxPool2D(pool_size=3, strides=2,padding=1),\n            resnet_block(c = 64, num_residuals=2, first_block=True),\n            resnet_block(c = 128, num_residuals=2),\n            resnet_block(c = 256, num_residuals=2),\n            resnet_block(c = 512, num_residuals=2)\n        )\n    def forward(self, x):\n        return self.model(x)","8036d761":"def base_net():\n    net = Resnet18()\n    net.initialize(ctx=d2l.try_gpu())\n    return net\n# m = base_net()","53bf2bed":"m = base_net()","f9aa87af":"def cls_predictor(num_anchors, num_classes):\n    return nn.Conv2D(num_anchors * (num_classes + 1), kernel_size=3,\n                     padding=1)\ndef bbox_predictor(num_anchors):\n    return nn.Conv2D(num_anchors * 4, kernel_size=3, padding=1)\n\ndef forward(x, block):\n    block.initialize()\n    return block(x)\n\ndef flatten_pred(pred):\n    return pred.transpose((0, 2, 3, 1)).flatten()\n\ndef concat_preds(preds):\n    return nd.concat(*[flatten_pred(p) for p in preds], dim=1)\ndef down_sample_blk(num_channels):\n    blk = nn.Sequential()\n    for _ in range(2):\n        blk.add(nn.Conv2D(num_channels, kernel_size=3, padding=1),\n                nn.BatchNorm(in_channels=num_channels),\n                nn.Activation('relu'))\n    blk.add(nn.MaxPool2D(2))\n    return blk\n# def base_net():\n#     blk = nn.Sequential()\n#     for num_filters in [16, 32, 64]:\n#         blk.add(down_sample_blk(num_filters))\n#     return blk\n\n\ndef base_net():\n    net = Resnet18()\n    net.initialize(ctx=d2l.try_gpu())\n    return net\n\n# forward(nd.zeros((2, 3, 256, 256)), base_net()).shape\ndef get_blk(i):\n    if i == 0:\n        blk = base_net()\n    elif i == 4:\n        blk = nn.GlobalMaxPool2D()\n    else:\n        blk = down_sample_blk(128)\n    return blk\ndef blk_forward(X, blk, size, ratio, cls_predictor, bbox_predictor):\n    Y = blk(X)\n    anchors = contrib.ndarray.MultiBoxPrior(Y, sizes=size, ratios=ratio)\n    cls_preds = cls_predictor(Y)\n    bbox_preds = bbox_predictor(Y)\n    return (Y, anchors, cls_preds, bbox_preds)\nsizes = [[0.2, 0.272], [0.37, 0.447], [0.54, 0.619], [0.71, 0.79],\n         [0.88, 0.961]]\nratios = [[1, 2, 0.5]] * 5\nnum_anchors = len(sizes[0]) + len(ratios[0]) - 1\n\nclass TinySSD(nn.Block):\n    def __init__(self, num_classes, **kwargs):\n        super(TinySSD, self).__init__(**kwargs)\n        self.num_classes = num_classes\n        for i in range(5):\n            # The assignment statement is self.blk_i = get_blk(i)\n            setattr(self, 'blk_%d' % i, get_blk(i))\n            setattr(self, 'cls_%d' % i, cls_predictor(num_anchors,\n                                                      num_classes))\n            setattr(self, 'bbox_%d' % i, bbox_predictor(num_anchors))\n\n    def forward(self, X):\n        X = X \/ 255.0\n        anchors, cls_preds, bbox_preds = [None] * 5, [None] * 5, [None] * 5\n        for i in range(5):\n            # getattr(self, 'blk_%d' % i) accesses self.blk_i\n            X, anchors[i], cls_preds[i], bbox_preds[i] = blk_forward(\n                X, getattr(self, 'blk_%d' % i), sizes[i], ratios[i],\n                getattr(self, 'cls_%d' % i), getattr(self, 'bbox_%d' % i))\n        # In the reshape function, 0 indicates that the batch size remains\n        # unchanged\n        return (nd.concat(*anchors, dim=1),\n                concat_preds(cls_preds).reshape(\n                    (0, -1, self.num_classes + 1)), concat_preds(bbox_preds))","15a35337":"from load_pascal import *\nbatch_size = 32\ntrain_iter, val_iter = load_pascal_dataset(batch_size=32, edge_size=256)","24d3b697":"\nJSON_PATH = Path(\"..\/input\/pascal\/PASCAL_VOC\/PASCAL_VOC\")\n#     'pascal_train2007.json'\ntrn_j = json.load((JSON_PATH\/\"pascal_train2012.json\").open())\nIMAGES,ANNOTATIONS,CATEGORIES = ['images', 'annotations', 'categories']\nFILE_NAME,ID,IMG_ID,CAT_ID,BBOX,WIDTH, HEIGHT = 'file_name','id','image_id','category_id','bbox', 'width', 'height'\ncats = {o[ID]:o['name'] for o in trn_j[CATEGORIES]} #cat_id -> cat_string\nbatch = train_iter.next()\nbatch.data[0].shape, batch.label[0].shape\nedge_size = 256\nimgs = (batch.data[0][0:10].transpose((0, 2, 3, 1))) \/ 255\naxes = d2l.show_images(imgs, 2, 5, scale=2)\nfor ax, label in zip(axes, batch.label[0][0:10]):\n#     print()\n    d2l.show_bboxes(ax, [label[0][1:5] * edge_size], labels=cats[label[0][0].asscalar()],colors=['w'])","c31010cb":"cls_loss = gluon.loss.SoftmaxCrossEntropyLoss()\nbbox_loss = gluon.loss.L1Loss()\n\ndef calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels, bbox_masks):\n    cls = cls_loss(cls_preds, cls_labels)\n    bbox = bbox_loss(bbox_preds * bbox_masks, bbox_labels * bbox_masks)\n    return cls + bbox\ndef cls_eval(cls_preds, cls_labels):\n    # Because the category prediction results are placed in the final\n    # dimension, argmax must specify this dimension\n    return (cls_preds.argmax(axis=-1) == cls_labels).sum().asscalar()\n\ndef bbox_eval(bbox_preds, bbox_labels, bbox_masks):\n    return ((bbox_labels - bbox_preds) * bbox_masks).abs().sum().asscalar()","a8b39ca0":"ctx, net = d2l.try_gpu(), TinySSD(num_classes=20)\nnet.initialize(init=init.Xavier(), ctx=ctx)\ntrainer = gluon.Trainer(net.collect_params(), 'sgd',\n                        {'learning_rate': 0.2, 'wd': 5e-4})\n\nnum_epochs, timer = 20, d2l.Timer()\nanimator = d2l.Animator(xlabel='epoch', xlim=[1, num_epochs],\n                        legend=['class error', 'bbox mae'])\nfor epoch in range(num_epochs):\n    # accuracy_sum, mae_sum, num_examples, num_labels\n    metric = d2l.Accumulator(4)\n    train_iter.reset()  # Read data from the start.\n    for batch in train_iter:\n        timer.start()\n        X = batch.data[0].as_in_context(ctx)\n        Y = batch.label[0].as_in_context(ctx)\n        with autograd.record():\n            # Generate multiscale anchor boxes and predict the category and\n            # offset of each\n            anchors, cls_preds, bbox_preds = net(X)\n            # Label the category and offset of each anchor box\n            bbox_labels, bbox_masks, cls_labels = contrib.nd.MultiBoxTarget(\n                anchors, Y, cls_preds.transpose((0, 2, 1)))\n            # Calculate the loss function using the predicted and labeled\n            # category and offset values\n            l = calc_loss(cls_preds, cls_labels, bbox_preds, bbox_labels,\n                          bbox_masks)\n        l.backward()\n        trainer.step(batch_size)\n        metric.add(cls_eval(cls_preds, cls_labels), cls_labels.size,\n                    bbox_eval(bbox_preds, bbox_labels, bbox_masks),\n                    bbox_labels.size)\n    cls_err, bbox_mae = 1-metric[0]\/metric[1], metric[2]\/metric[3]\n    animator.add(epoch+1, (cls_err, bbox_mae))\nprint('class err %.2e, bbox mae %.2e' % (cls_err, bbox_mae))\nprint('%.1f exampes\/sec on %s'%(train_iter.num_image\/timer.stop(), ctx))","f309b2c2":"# ! ls \"..\/input\/pascal\/VOCtrainval_06-Nov-2007\/VOCdevkit\/VOC2007\/JPEGImages\"","31c8efbf":"def predict(X):\n    anchors, cls_preds, bbox_preds = net(X.as_in_context(ctx))\n    print(f'anchor shape: {anchors.shape}')\n    print(f'cls shape: {cls_preds.shape}')\n    cls_probs = cls_preds.softmax().transpose((0, 2, 1))\n    output = contrib.nd.MultiBoxDetection(cls_probs, bbox_preds, anchors)\n    idx = [i for i, row in enumerate(output[0]) if row[0].asscalar() != -1]\n    return output[0, idx]\ndef display(img, output, threshold):\n    d2l.set_figsize((5, 5))\n    fig = d2l.plt.imshow(img.asnumpy())\n    for row in output:\n        score = row[1].asscalar()\n        if score < threshold:\n            continue\n        h, w = img.shape[0:2]\n        print(row)\n        print(cats[int(row[0].asscalar())])\n        bbox = [row[2:6] * nd.array((w, h, w, h), ctx=row.context)]\n        d2l.show_bboxes(fig.axes, bbox, '%.2f' % score, 'w')","da182984":"img = image.imread(\"..\/input\/pascal\/VOCtrainval_06-Nov-2007\/VOCdevkit\/VOC2007\/JPEGImages\/006223.jpg\")\nfeature = image.imresize(img, 256, 256).astype('float32')\nX = feature.transpose((2, 0, 1)).expand_dims(axis=0)\noutput = predict(X)\ndisplay(img, output, threshold=0.05)\n","03a4864b":"img = image.imread(\"..\/input\/pascal\/VOCtrainval_06-Nov-2007\/VOCdevkit\/VOC2007\/JPEGImages\/007914.jpg\")\nfeature = image.imresize(img, 256, 256).astype('float32')\nX = feature.transpose((2, 0, 1)).expand_dims(axis=0)\noutput = predict(X)\ndisplay(img, output, threshold=0.1)\n","11ce404b":"img = image.imread(\"..\/input\/pascal\/VOCtrainval_06-Nov-2007\/VOCdevkit\/VOC2007\/JPEGImages\/002866.jpg\")\nfeature = image.imresize(img, 256, 256).astype('float32')\nX = feature.transpose((2, 0, 1)).expand_dims(axis=0)\noutput = predict(X)\ndisplay(img, output, threshold=0.1)\n","42134085":"### Define loss (copied from SSD notebook without change)","52707ead":"### Prediction","303afac8":"### Visualizing dataset","290b3d9f":"## Prediction helper function","af5606ba":"### Loading Pascal dataset","1c0fa290":"* Experiment log:\n\ndefault - pikachu\nclass err 2.33e-03, bbox mae 2.53e-03\n2758.5 exampes\/sec on gpu(0)\n\ndefault - pascal\nclass err 7.65e-03, bbox mae 7.99e-03\n7777.6 exampes\/sec on gpu(0)\n\n","fd820649":"### Training","3cd94960":"## Define Model (copied from SSD notebok with no change)"}}