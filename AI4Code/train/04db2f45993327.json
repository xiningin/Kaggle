{"cell_type":{"aeb365dc":"code","8960ff9d":"code","88ec973b":"code","384d0f9a":"code","c7433f4a":"code","da190ddf":"code","24afb3fb":"code","71d1c0f3":"code","f6066de2":"code","4187a525":"code","7bcd8c1a":"code","af93024d":"code","f2aeca1c":"code","38478da5":"code","a852cd9a":"code","88b066c8":"code","a652d8dc":"code","024192b2":"code","e5816df4":"code","94cd2a62":"code","bc2ab227":"code","67dc9f65":"code","3daca56d":"code","d702e10f":"code","4d897175":"code","1c5da722":"code","5155b1bc":"code","ae4ae805":"code","8cdd807e":"code","dd3b1658":"code","5bf0e901":"code","5126fc3a":"code","c263e307":"code","4ad565b5":"code","77ae044b":"code","d2abe05d":"code","720a42c6":"markdown","2f5f44cd":"markdown","262b0fb6":"markdown","09dcc15f":"markdown","91c718ff":"markdown","337ba8c8":"markdown","19b53e5a":"markdown","4fac1deb":"markdown","708e53db":"markdown","2208d696":"markdown","8bffaf3b":"markdown","8f90e5f8":"markdown","21f1b64b":"markdown","19d479e4":"markdown","bbcbcbd6":"markdown","058fa159":"markdown","1444e5c8":"markdown"},"source":{"aeb365dc":"import requests               # a simple HTTP library for Python\nfrom bs4 import BeautifulSoup # a great library for webscrapping\nimport pickle                 # a library for data stream format","8960ff9d":"# Let take a look at the website scrapsfromtheloft.com , it's a nice source for movie detail information\n# We will gather the data from the website: scrapsfromtheloft.com\n\n\ndef url_to_transcript(url):\n    '''Returns transcript data specifically from scrapsfromtheloft.com.'''\n    page = requests.get(url).text\n    soup = BeautifulSoup(page, \"lxml\")\n    text = [p.text for p in soup.find(class_=\"post-content\").find_all('p')]\n    print(url)\n    return text\n\n# URLs of transcripts in scope\nurls = ['http:\/\/scrapsfromtheloft.com\/2017\/05\/06\/louis-ck-oh-my-god-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/04\/11\/dave-chappelle-age-spin-2017-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2018\/03\/15\/ricky-gervais-humanity-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/08\/07\/bo-burnham-2013-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/05\/24\/bill-burr-im-sorry-feel-way-2014-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/04\/21\/jim-jefferies-bare-2014-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/08\/02\/john-mulaney-comeback-kid-2015-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/10\/21\/hasan-minhaj-homecoming-king-2017-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/09\/19\/ali-wong-baby-cobra-2016-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/08\/03\/anthony-jeselnik-thoughts-prayers-2015-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2018\/03\/03\/mike-birbiglia-my-girlfriends-boyfriend-2013-full-transcript\/',\n        'http:\/\/scrapsfromtheloft.com\/2017\/08\/19\/joe-rogan-triggered-2016-full-transcript\/']\n\n# Comedian names\ncomedians = ['louis', 'dave', 'ricky', 'bo', 'bill', 'jim', 'john', 'hasan', 'ali', 'anthony', 'mike', 'joe']","88ec973b":"\n# Requesting transcripts\ntranscripts = [url_to_transcript(u) for u in urls]","384d0f9a":"# Make a new directory to hold the text files\n!mkdir transcripts\n\nfor i, c in enumerate(comedians):\n    with open(\"transcripts\/\" + c + \".txt\", \"wb\") as file:\n        pickle.dump(transcripts[i], file)","c7433f4a":"# Load pickle file\ndata = {}\nfor i, c in enumerate(comedians):\n    with open(\"transcripts\/\" + c + \".txt\", \"rb\") as file:\n        data[c] = pickle.load(file)","da190ddf":"# Take a look at the data\ndata.keys() ,data['louis'][:1]","24afb3fb":"def combine_text(list_of_text):\n    '''\n    Take a list of texts and combine them into one large chunk of text\n    \n    Return a text (larger one)\n    '''\n    return ' '.join(list_of_text)\n    ","71d1c0f3":"# Combining\ndata_combined = {key: [combine_text(value)] for key, value in data.items()}","f6066de2":"import pandas as pd\npd.set_option('max_colwidth', 150)\n\ndata_df = pd.DataFrame.from_dict(data_combined).transpose()\ndata_df.columns = ['transcript']\ndata_df = data_df.sort_index()\ndata_df.tail()","4187a525":"import re       # stand for regular expression library\nimport string\n\ndef clean_text_re(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation,\n       remove words containing numbers, additional punctuation and non-sensical text.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('[\u2018\u2019\u201c\u201d\u2026]', '', text)\n    text = re.sub('\\n', '', text)\n    return text\n\nstep1 = lambda x: clean_text_re(x)\n\n# Let clean the data:\ndata_clean = pd.DataFrame(data_df.transcript.apply(step1))\ndata_df.to_pickle(\"corpus.pkl\")\nprint('DATA after cleaning with regular expression')\ndata_clean.tail()","7bcd8c1a":"from sklearn.feature_extraction.text import CountVectorizer\n\ncv = CountVectorizer(stop_words='english')\ndata_cv = cv.fit_transform(data_clean.transcript)\ndata_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_dtm.index = data_clean.index\ndata_dtm.tail()","af93024d":"## Save the data_dtm for later use \ndata_dtm.to_pickle('dtm.pkl') \ndata_clean.to_pickle('data_clean.pkl')\npickle.dump(cv, open(\"cv.pkl\", \"wb\"))","f2aeca1c":"# Reload the data\ndf = pd.read_pickle('dtm.pkl')\ndf = df.transpose()\ndf.head()","38478da5":"# Let fine top 30 common word said by each commedian\n\ntop_dict = {}   # Defined a dict to save the top word corresponding to respectively commedian\n\nfor person in df.columns:\n    top_word = df[person].sort_values(ascending = False).head(30)\n    top_dict[person] = list(zip(top_word.index, top_word.values))","a852cd9a":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 30 words for each comedian\nwords = []\nfor comedian in df.columns:\n    top = [word for (word, count) in top_dict[comedian]]\n    for t in top:\n        words.append(t)\n\nprint(words[:5])\n        ","88b066c8":"# Let's aggregate this list and identify the most common words along with how many routines they occur in\nprint(Counter(words).most_common())","a652d8dc":"# If more than half of the comedians have it as a top word, exclude it from the list\nadd_stop_words = [word for word, count in Counter(words).most_common() if count > 6]\nprint(add_stop_words)","024192b2":"# Let's update our document-term matrix with the new list of stop words\nfrom sklearn.feature_extraction import text \nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Read in cleaned data\ndata_clean = pd.read_pickle('data_clean.pkl')\n\n# Add new stop words\nstop_words = text.ENGLISH_STOP_WORDS.union(add_stop_words)\n\n# Recreate document-term matrix\ncv = CountVectorizer(stop_words=stop_words)\ndata_cv = cv.fit_transform(data_clean.transcript)\ndata_stop = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\ndata_stop.index = data_clean.index\n\n# Pickle it for later use\n\npickle.dump(cv, open(\"cv_stop.pkl\", \"wb\"))\ndata_stop.to_pickle(\"dtm_stop.pkl\")","e5816df4":"from wordcloud import WordCloud\n\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)","94cd2a62":"# Reset the output dimensions\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = [16, 6]\n\nfull_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n\n\n\n# Create subplots for each comedian\nfor index, comedian in enumerate(df.columns):\n    wc.generate(data_clean.transcript[comedian])\n    plt.subplot(3, 4, index+1)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.title(full_names[index])\n    \nplt.show()","bc2ab227":"# Find the number of unique words that each comedian uses\n\n# Identify the non-zero items in the document-term matrix, meaning that the word occurs at least once\nunique_list = []\nfor comedian in df.columns:\n    uniques = df[comedian].to_numpy().nonzero()[0].size\n    unique_list.append(uniques)\n\n# Create a new dataframe that contains this unique word count\ndata_words = pd.DataFrame(list(zip(full_names, unique_list)), columns=['comedian', 'unique_words'])\ndata_unique_sort = data_words.sort_values(by='unique_words')\ndata_unique_sort","67dc9f65":"# Calculate the words per minute of each comedian\n\n# Find the total number of words that a comedian uses\ntotal_list = []\nfor comedian in df.columns:\n    totals = sum(df[comedian])\n    total_list.append(totals)\n    \n# Comedy special run times from IMDB, in minutes\nrun_times = [60, 59, 80, 60, 67, 73, 77, 63, 62, 58, 76, 79]\n\n# Let's add some columns to our dataframe\ndata_words['total_words'] = total_list\ndata_words['run_times'] = run_times\ndata_words['words_per_minute'] = data_words['total_words'] \/ data_words['run_times']\n\n# Sort the dataframe by words per minute to see who talks the slowest and fastest\ndata_wpm_sort = data_words.sort_values(by='words_per_minute')\ndata_wpm_sort","3daca56d":"# Let's plot our findings\nimport numpy as np\n\ny_pos = np.arange(len(data_words))\n\nplt.subplot(1, 2, 1)\nplt.barh(y_pos, data_unique_sort.unique_words, align='center')\nplt.yticks(y_pos, data_unique_sort.comedian)\nplt.title('Number of Unique Words', fontsize=20)\n\nplt.subplot(1, 2, 2)\nplt.barh(y_pos, data_wpm_sort.words_per_minute, align='center')\nplt.yticks(y_pos, data_wpm_sort.comedian)\nplt.title('Number of Words Per Minute', fontsize=20)\n\nplt.tight_layout()\nplt.show()","d702e10f":"# Let's isolate just these bad words\ndata_bad_words = df.transpose()[['fucking', 'fuck', 'shit']]\ndata_profanity = pd.concat([data_bad_words.fucking + data_bad_words.fuck, data_bad_words.shit], axis=1)\ndata_profanity.columns = ['f_word', 's_word']\ndata_profanity","4d897175":"# Let's create a scatter plot of our findings\nplt.rcParams['figure.figsize'] = [10, 8]\n\nfor i, comedian in enumerate(data_profanity.index):\n    x = data_profanity.f_word.loc[comedian]\n    y = data_profanity.s_word.loc[comedian]\n    plt.scatter(x, y, color='blue')\n    plt.text(x+1.5, y+0.5, full_names[i], fontsize=10)\n    plt.xlim(-5, 155) \n    \nplt.title('Number of Bad Words Used in Routine', fontsize=20)\nplt.xlabel('Number of F Bombs', fontsize=15)\nplt.ylabel('Number of S Words', fontsize=15)\n\nplt.show()","1c5da722":"data = pd.read_pickle('corpus.pkl')\ndata.tail()","5155b1bc":"from textblob import TextBlob\n\npol = lambda x: TextBlob(x).sentiment.polarity\nsub = lambda x: TextBlob(x).sentiment.subjectivity\n\ndata['polarity'] = data['transcript'].apply(pol)\ndata['subjectivity'] = data['transcript'].apply(sub)\ndata.tail()","ae4ae805":"full_names = ['Ali Wong', 'Anthony Jeselnik', 'Bill Burr', 'Bo Burnham', 'Dave Chappelle', 'Hasan Minhaj',\n              'Jim Jefferies', 'Joe Rogan', 'John Mulaney', 'Louis C.K.', 'Mike Birbiglia', 'Ricky Gervais']\n\ndata['full_name'] = full_names","8cdd807e":"# Let's plot the results\nimport matplotlib.pyplot as plt\n\nplt.rcParams['figure.figsize'] = [10, 8]\n\nfor index, comedian in enumerate(data.index):\n    x = data.polarity.loc[comedian]\n    y = data.subjectivity.loc[comedian]\n    plt.scatter(x, y, color='blue')\n    plt.text(x+.001, y+.001, data['full_name'][index], fontsize=10)       \n    plt.xlim(-.01, .12) \n    \nplt.title('Sentiment Analysis', fontsize=20)\nplt.xlabel('<-- Negative -------- Positive -->', fontsize=15)\nplt.ylabel('<-- Facts -------- Opinions -->', fontsize=15)\n\nplt.show()","dd3b1658":"# Split each routine into 10 parts\nimport numpy as np\nimport math\n\ndef split_text(text, n=10):\n    '''Takes in a string of text and splits into n equal parts, with a default of 10 equal parts.'''\n\n    # Calculate length of text, the size of each chunk of text and the starting points of each chunk of text\n    length = len(text)\n    size = math.floor(length \/ n)\n    start = np.arange(0, length, size)\n    \n    # Pull out equally sized pieces of text and put it into a list\n    split_list = []\n    for piece in range(n):\n        split_list.append(text[start[piece]:start[piece]+size])\n    return split_list","5bf0e901":"# Let's create a list to hold all of the pieces of text\nlist_pieces = []\nfor t in data.transcript:\n    split = split_text(t)\n    list_pieces.append(split)","5126fc3a":"# The list has 10 elements, one for each transcript\nlen(list_pieces)","c263e307":"# Each transcript has been split into 10 pieces of text\nlen(list_pieces[0])","4ad565b5":"# Calculate the polarity for each piece of text\n\npolarity_transcript = []\nfor lp in list_pieces:\n    polarity_piece = []\n    for p in lp:\n        polarity_piece.append(TextBlob(p).sentiment.polarity)\n    polarity_transcript.append(polarity_piece)\n    ","77ae044b":"# Show the plot for one comedian\nplt.plot(polarity_transcript[0])\nplt.title(data['full_name'].index[0])\nplt.show()","d2abe05d":"# Show the plot for all comedians\nplt.rcParams['figure.figsize'] = [16, 12]\n\nfor index, comedian in enumerate(data.index):    \n    plt.subplot(3, 4, index+1)\n    plt.plot(polarity_transcript[index])\n    plt.plot(np.arange(0,10), np.zeros(10))\n    plt.title(data['full_name'][index])\n    plt.ylim(ymin=-.2, ymax=.3)\n    \nplt.show()","720a42c6":"# START\n\nNormally we import all the libraries needed in the first cell of notebook, But it may sometimes make you feel confused that why we suddendly import them without any reasons. \n\nSo I will do the oppisite way, gradually import needed library for respectively concept.\nThis way will bring the concept to your mind naturally and comfortably\n\n## Gathering the data with BeautifulSoup\n\n","2f5f44cd":"## Sentiment Analysis\n\nSo far, all of the analysis we've done has been pretty generic - looking at counts, creating scatter plots, etc. These techniques could be applied to numeric data as well.\n\nWhen it comes to text data, there are a few popular techniques that we'll be going through in the next few notebooks, starting with sentiment analysis. A few key points to remember with sentiment analysis.\n\n1. **TextBlob Module:** Linguistic researchers have labeled the sentiment of words based on their domain expertise. Sentiment of words can vary based on where it is in a sentence. The TextBlob module allows us to take advantage of these labels.\n2. **Sentiment Labels:** Each word in a corpus is labeled in terms of polarity and subjectivity (there are more labels as well, but we're going to ignore them for now). A corpus' sentiment is the average of these.\n   * **Polarity**: How positive or negative a word is. -1 is very negative. +1 is very positive.\n   * **Subjectivity**: How subjective, or opinionated a word is. 0 is fact. +1 is very much an opinion.\n\nFor more info on how TextBlob coded up its [sentiment function](https:\/\/planspace.org\/20150607-textblob_sentiment\/).\n\nLet's take a look at the sentiment of the various transcripts, both overall and throughout the comedy routine.","262b0fb6":"Ali Wong stays generally positive throughout her routine. Similar comedians are Louis C.K. and Mike Birbiglia.\n\nOn the other hand, you have some pretty different patterns here like Bo Burnham who gets happier as time passes and Dave Chappelle who has some pretty down moments in his routine.","09dcc15f":"## Amount of Profanity","91c718ff":"#### A lot of people say F-word, Let's dig into that later.","337ba8c8":"## Sentiment of Routine Over Time\nInstead of looking at the overall sentiment, let's see if there's anything interesting about the sentiment over time throughout each routine.","19b53e5a":"### 2. Stop Word\n\nStop word is a kind of word that have very little meaning and could be added to a stop words list","4fac1deb":"## II.Exploratory Text-Data Analysis\nBefore applying any fancy algorithms, it's always important to explore the data first.\n\nWhen working with numerical data, some of the exploratory data analysis (EDA) techniques we can use include finding the average of the data set, the distribution of the data, the most common values, etc. The idea is the same when working with text data. We are going to find some more obvious patterns with EDA before identifying the hidden patterns with machines learning (ML) techniques. We are going to look at the following for each comedian:\n\n1. **Most common words** - find these and create word clouds\n1. **Size of vocabulary** - look number of unique words and also how quickly someone speaks\n1. **Amount of profanity** - most common terms","708e53db":"* **Vocabulary**\n   * Ricky Gervais (British comedy) and Bill Burr (podcast host) use a lot of words in their comedy\n   * Louis C.K. (self-depricating comedy) and Anthony Jeselnik (dark humor) have a smaller vocabulary\n\n\n* **Talking Speed**\n   * Joe Rogan (blue comedy) and Bill Burr (podcast host) talk fast\n   * Bo Burnham (musical comedy) and Anthony Jeselnik (dark humor) talk slow\n   \n","2208d696":"### 1. Most Common Words","8bffaf3b":"### I.a : Cleaning data with Python regular expressions:\n1. Transform to lower case\n1. remove text in square brackets\n1. remove punctuation\n1. remove words containing numbers\n1. remove additional punctuation\n1. remove non-sensical text\n\nFor more Python regular-expression technique, you can visit at https:\/\/docs.python.org\/3\/howto\/regex.html\n","8f90e5f8":"### I.b : ORGANIZING THE DATA\nWe organizing the data in two standard text-format\n1. **Corpus**  (corpus is a collection of texts, and they are all put together neatly in a pandas dataframe )\nThe data_clean above is already stored as a Corpus\n2. **Tokenization** with Document term matrix.\nTokenization is just breaking down the text into words. There are several way to do tokenization.\n   If you familiar with NLP & Deep leanring,  we can use word_embedding instead of document term matrix but in this cope of kernel, will introduce the simpler one: Document term matrix with scikit learn.\n   \n   Using **scikit-learn's CountVectorizer**, where every row will represent a different document and every column will represent a different word.\nIn addition, with CountVectorizer, we can remove stop words. Stop words are common words that add no additional meaning to text such as 'a', 'the', etc.","21f1b64b":"![image.png](attachment:image.png)","19d479e4":"#### Now the data is stored as dictionary : {commedian : [text]} \nLet convert data to Pandas Dataframe to utilize the power of pandas preprocessing","bbcbcbd6":"### I.c: SAVING DATA\n\nUsing to_pickle() method to save the data for later use. If you run this kernel and check the output folder :\/kaggle\/working, you can see a file dtm.pkl, data_clean.pkl, cv.pkl appreared already","058fa159":"## I. CLEANING DATA\n\n1. Make text all lower case\n1. Remove punctuation\n1. Remove numerical values\n1. Remove common non-sensical text (\/n)\n1. Tokenize text\n1. Remove stop words\n\nAs we can see above, the data format is a list consists of 'text', like: ['text1', 'text2', 'text3',...] for just one commedian, so we need to combine them to one large chunk of text ['text1 + text2 + text3 +...]. The reason to do that because all of this texts are belong to each commedian, so combine them to the only 'text' will help us preprocessing on them effectively","1444e5c8":"In Natural Language processing and data science, text -based data is the most common type of data that every companies have to dealing with. Knowing how to apply NLP techniques in text-based data will be an invaluable skill for your data science career.\n\nThis kernel introduce an end to end - text based data NLP project with the **complete workflow**, mainly focusing on handling the noisy of text-data, how to do processing the text data into standard format ( the most painful part in real world text based NLP problems) .Then we can apply modern NLP algorithmns on these data further\n\nIf you like this content, Please give me an upvote and I am very glad :3"}}