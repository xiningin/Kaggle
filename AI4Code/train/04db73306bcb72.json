{"cell_type":{"1640f600":"code","2de53383":"code","67bef5df":"code","b6e76a93":"code","89338a0c":"code","e3c6b4d4":"code","94bea11d":"code","7c4c6dee":"code","b095014f":"code","a75c4aa1":"code","02df525c":"code","f1c873a5":"code","af0720aa":"code","1056f003":"code","f1ac7e91":"code","d979ad48":"code","ca35c040":"code","843aa2df":"code","739b4ebc":"code","7ed93dfc":"code","59e311fd":"code","bf616c9d":"code","6ba61935":"code","a82bc738":"code","3c1e78cd":"code","5c7c330d":"code","c1c5a75a":"code","1a5d13a2":"code","9f086757":"code","4992f961":"code","e9d37a46":"code","838ee6ba":"markdown","917e316d":"markdown","62a12d64":"markdown","0cf9bcf5":"markdown","5c5ed137":"markdown","46efc379":"markdown","af1728c7":"markdown","6829f8fb":"markdown","fb0a830a":"markdown","86573d85":"markdown","57f35bf9":"markdown","2fdba04f":"markdown","4e30c0b2":"markdown","e16d29f6":"markdown","a8a2ab88":"markdown","0b7a1d93":"markdown","a73157e5":"markdown","d269934d":"markdown","a7b6e0b4":"markdown","2e067244":"markdown","a4ebb711":"markdown","3aacd2c9":"markdown","816689ae":"markdown","1b474911":"markdown","eefc3bad":"markdown","8df8e694":"markdown"},"source":{"1640f600":"import pandas as pd\ntitanic_train=pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_test=pd.read_csv('..\/input\/titanic\/test.csv')\ntitanic_train.describe()","2de53383":"titanic_train.isnull().sum()","67bef5df":"titanic_test.isnull().sum()","b6e76a93":"titanic_train['Age'].fillna(titanic_train['Age'].median(), inplace = True)\ntitanic_test['Age'].fillna(titanic_test['Age'].median(), inplace = True)\ntitanic_train['Embarked'].fillna(titanic_train['Embarked'].mode(),inplace=True)\ntitanic_test['Fare'].fillna(titanic_test['Fare'].median(), inplace = True)","89338a0c":"titanic_train['familysize']=titanic_train['SibSp']+titanic_train['Parch']+1\ntitanic_train['Solo']=(titanic_train['familysize'] >1 ).astype(int)\ntitanic_train['Solo'],titanic_train['familysize']","e3c6b4d4":"titanic_test['familysize']=titanic_test['SibSp']+titanic_test['Parch']+1\ntitanic_test['Solo']=(titanic_test['familysize'] >1 ).astype(int)\ntitanic_train['Solo'].value_counts()","94bea11d":"Age_wise_survival=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Age']).sum())\nAge_wise_dist=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Age']).count())\nAge_wise_dist","7c4c6dee":"import matplotlib.pyplot as plt\nplt.plot(Age_wise_survival,label=\"Survived\")\nplt.plot(Age_wise_dist,label=\"Total\")\nplt.legend()","b095014f":"titanic_train['AgeBin'] = pd.cut(titanic_train['Age'].astype(int), 5)\ntitanic_train['AgeBin'].values","a75c4aa1":"titanic_test['AgeBin'] = pd.cut(titanic_test['Age'].astype(int), 5)","02df525c":"Gender_wise_survival=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Sex']).sum())\nGender_wise_count=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Sex']).count())\nGender_wise_survival\/Gender_wise_count","f1c873a5":"titanic_train['vip'] = [1 if x =='Master' else 0 for x in titanic_train['Name']] \ntitanic_test['vip'] = [1 if x =='Master' else 0 for x in titanic_test['Name']] ","af0720aa":"Fare_wise_dist=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Fare']).count())\nFare_wise_survival=pd.DataFrame(titanic_train['Survived'].groupby(titanic_train['Fare']).sum())\nFare_wise_s_per=(Fare_wise_survival\/Fare_wise_dist)*100\nimport matplotlib.pyplot as plt\nplt.plot(Fare_wise_s_per,'bo',label=\"% of survived by fare\")\nplt.legend()","1056f003":"titanic_train['farebin'] = pd.cut(titanic_train['Fare'].astype(int), 3)\ntitanic_train['farebin'].values","f1ac7e91":"titanic_test['farebin'] = pd.cut(titanic_test['Fare'].astype(int), 3)","d979ad48":"y=\"Survived\"\ntrain_x=titanic_train[['Pclass','Sex','familysize','Solo','AgeBin','vip','farebin','Survived']]\ntest_x=titanic_test[['Pclass','Sex','familysize','Solo','AgeBin','vip','farebin']]","ca35c040":"from sklearn.preprocessing import LabelEncoder\nlabel = LabelEncoder()\ntrain_x=train_x.apply(lambda col: label.fit_transform(col), axis=0, result_type='expand')\ntest_x=test_x.apply(lambda col: label.fit_transform(col), axis=0, result_type='expand')\ntrain_x","843aa2df":"import h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","739b4ebc":"trframe=h2o.H2OFrame(train_x)","7ed93dfc":"teframe=h2o.H2OFrame(test_x)","59e311fd":"titanic_model = H2OAutoML(max_runtime_secs = 120, seed = 1, project_name = \"titanic_kaggle_nishant\")\ntitanic_model.train(y = y, training_frame = trframe)","bf616c9d":"titanic_model.leaderboard","6ba61935":"titanic_model.leader.params.keys()","a82bc738":"titanic_model.leader.params['colsample_bytree'],titanic_model.leader.params['stopping_rounds']","3c1e78cd":"lb=titanic_model.leader","5c7c330d":"m = h2o.get_model(lb)","c1c5a75a":"pred_h2o = titanic_model.leader.predict(teframe)\npred_h2o","1a5d13a2":"pred_pandas=pred_h2o.as_data_frame(use_pandas=True)\npred_pandas","9f086757":"pred_pandas['Survived'] = [1 if x > 0.5 else 0 for x in pred_pandas['predict']] \npred_pandas","4992f961":"output= titanic_test.merge(pred_pandas['Survived'], left_index=True, right_index=True)\noutput","e9d37a46":"output_final=output[['PassengerId','Survived']]\noutput.to_csv('GBM_NISHANT.csv',index=\"FALSE\")\n","838ee6ba":"H2OAutoml has two primary requirements one y variable and training dataframe , and two options, one max_runtime in seconds , this closes the algorithm within that time limit and max models which limits the number of models excluding stacked models, that automl generates if we dont specify this option it will generate as long as it encounters max runtime , i think both are some type of hyperparameters to reduce overfitting , if we give large run time probably this may lead to vanishing gradient problem , I am trying with 120 seconds.\n\nMore documentation is here:\nhttp:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html","917e316d":"The XGBoost_grid__1_AutoML_20200513_144044_model_2 gives us the lowest root mean square error and this the best fitted model at this run time. If you go to the local server page http:\/\/127.0.0.1:54321\/flow\/index.html you can find the option in getmodel in assistance.Search this model and you get plethora of details about model. Let me higlight few of them here","62a12d64":"As we suspected the survived blue line has a gap from total passengers if you are between 15-35 , so we have to create bins for child ,adult and olds.Let us create 5 bins in which middle age is slightly more divided , since few above age 40 had families together and they might have been rescued too.","0cf9bcf5":"The training deviance graph by number of trees This feature unfortunately is only availble through H2o flow server page  \n\nKindly check the graph there","5c5ed137":"# predicting using h2o.predict","46efc379":"let us use 50% above as cutoff for saying survived ","af1728c7":"# Using the title = Master to create a column VIP\n\nI am assuming that master title has some signifance as it is used for children and that increases there chance of survival, let me treat them as VIP's :)","6829f8fb":"# The H2O package from H2O.AI and AutoML function\nH2O.AI is the company which provide this package h2o, it runs on both R and python.The package has an function Automl which run GBM, Distributed Random Forest , DNN and combination of these stacked models. It compares all the models based on lowest error rate\/accuracy this can be used to generate decent models with above average performance in short time.\n\nThere is detailed explanation of the Automl function from fellow Kaggler : Parul \n\nhttps:\/\/towardsdatascience.com\/a-deep-dive-into-h2os-automl-4b1fe51d3f3e","fb0a830a":"# Variable importance, cross validation details and model stats","86573d85":"# Let us explore gender wise survival too though we will use column as it is","57f35bf9":"# Data loading and EDA","2fdba04f":"Imputation with median for numeric variables and mode for character variables","4e30c0b2":"# Using the Leader board option to arrive at best model.","e16d29f6":"# Lets prepare the data for H2O automl and select only important columns\n\nH2o do not take pandas dataframe directly , it has its own dataframe so we will convert pandas datframe to H2o dataframe, one key diffrence is that in train data you must have the dependent column and this need not be passed like seperate like in GBM and NN","a8a2ab88":"Loading the H2O package","0b7a1d93":"Exploring Fare and creating categories","a73157e5":"# Feature Engineering ","d269934d":"If you are single and male the chances of survival would be low, as historical account of the incident women and children were the first one to be rescued, hence we will create a column solo traveller. We create a family size using siblings\/spouse and parents\/child column , if family size=1 then passenger is the solo traveller","a7b6e0b4":"Here are few basic inferences we can get from numeric data:\n1. the median age is around 28 which suggest that majority passengers were young.\n2. The median fare is 14 pound while 75 percentile is 31 suggesting only very few passengers have paid for super luxury like above 500 pounds.\n3. Majority of passengers are from class 3","2e067244":"Well the person who paid 500 pound did survive while the relationship is not linear but we do have slightly high chance of survival if we paid more than 100 pounds. I am making three catgeories with equal width","a4ebb711":"We have 5 categories cut off from 16, then at 32, 48, 64.","3aacd2c9":"# PARAMETERS DETAILS","816689ae":"Only 19% male passengers survived as compared to 74% female","1b474911":"# Age - creating bins as we know young were last to be rescued, so lets explore this relation","eefc3bad":"Encoding the variables","8df8e694":"# Checking for missing values"}}