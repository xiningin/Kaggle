{"cell_type":{"11d1cd97":"code","61ea3d7c":"code","841cf9e9":"code","7e3f34f9":"code","9ff8fe87":"code","1bccbc40":"code","8ff1e597":"code","9a058dea":"code","d41d549e":"code","2a8342a1":"code","d5e42f4a":"code","55c59b3d":"code","93d92eef":"code","cbfeb1ea":"code","25f7b82e":"code","7f5090be":"code","623e382a":"code","5160c73c":"code","3ed87e02":"code","27bb1dcc":"code","16d9e7b4":"code","763b531c":"code","897746d5":"code","67e7cc25":"code","314f5dab":"code","c73b3e4f":"code","2395cb9c":"code","7cb12d18":"code","72d48ac4":"markdown","c832bcf5":"markdown","b0a46403":"markdown","15105825":"markdown"},"source":{"11d1cd97":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","61ea3d7c":"# load in training and test data\ntrain = pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\nprint(train.shape, test.shape)","841cf9e9":"# check data\n# 1 = unreliable\n# 0 = reliable\ntrain.head(10)","7e3f34f9":"# check data types\ntrain.info()","9ff8fe87":"# printing some unreliable information, seems to be an opinion piece\nprint(train[\"text\"][2])","1bccbc40":"# printing some reliable information\nprint(train[\"text\"][1])","8ff1e597":"# visualise data\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# seeing the balance of the two labels \n# 1 = unreliable \/ fake\n# 0 = reliable \/ real\nplt.figure(figsize = (8, 2))\nsns.countplot(y = train.label)","9a058dea":"# check nulls\ntrain.isnull().sum()","d41d549e":"# preprocessing the data \n\n# drop the null rows\ntrain = train.dropna()\ntrain.shape","2a8342a1":"# seeing the balance of the two labels \n# 1 = unreliable \/ fake\n# 0 = reliable \/ real\nplt.figure(figsize = (8, 2))\nsns.countplot(y = train.label)","d5e42f4a":"train.head(10)","55c59b3d":"# combine the data into one vector\ntrain[\"all\"] = train.title + \" \" + train.text","93d92eef":"# check nulls\ntrain.isnull().sum()","cbfeb1ea":"from nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nimport re\nfrom string import punctuation\n\n# create a list of stop words - common words that will have little to no predictive value\nstop_words = stopwords.words(\"english\")\n\n# stems words such as plurals (not that accurate)\nps = PorterStemmer()\n\ndef preprocess(text):\n\n    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n    text = text.lower().split()\n    text = [ps.stem(word) for word in text if word not in stop_words]\n    text = \" \".join(text)\n    text = \"\".join(p for p in text if p not in punctuation)\n    \n    return text\n\n\n\ntrain[\"cleaned\"] = train[\"all\"].apply(preprocess)","25f7b82e":"train[\"cleaned\"][0]","7f5090be":"from wordcloud import WordCloud\n\nplt.figure(figsize=(20,20))\nwordCloud = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(train[train[\"label\"] == 0][\"cleaned\"]))\nplt.title(\"Wordmap of words for real news\")\nplt.imshow(wordCloud, interpolation = \"bilinear\")","623e382a":"plt.figure(figsize=(20,20))\nwordCloud = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = stop_words).generate(\" \".join(train[train[\"label\"] == 1][\"cleaned\"]))\nplt.title(\"Wordmap of words for fake news\")\nplt.imshow(wordCloud, interpolation = \"bilinear\")","5160c73c":"# prepare data for training\ny_train = np.asarray(train.label)\nX_train = train[\"cleaned\"]","3ed87e02":"# modelling LSTM\nimport tensorflow as tf\n\nimport keras\nfrom keras import Sequential\nfrom keras.layers import LSTM, Bidirectional, Dense, Embedding, Dropout\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\n# word tokenizer\nvocab_size = 20000\nembedding_dim = 120\n\ntokenizer = Tokenizer(num_words = vocab_size)\ntokenizer.fit_on_texts(X_train)\ntrain_sequences = tokenizer.texts_to_sequences(X_train)\n\n# padding the vectors\npadded_train = pad_sequences(train_sequences, maxlen = 40, padding = 'post', truncating = 'post')\n\n# defining model LSTM bidirectional model\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embedding_dim))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","27bb1dcc":"model.compile(optimizer=\"adam\", loss = \"binary_crossentropy\", metrics = [\"accuracy\"])","16d9e7b4":"# perform training on the train data\n\nfrom sklearn.model_selection import train_test_split\n\n# splitting data for cross validation\nx_train, x_test, y_train, y_test = train_test_split(padded_train, y_train, test_size=0.20, random_state=42)\nmodel_trained = model.fit(x_train, y_train, batch_size = 64, validation_data=(x_test,y_test), epochs = 20)","763b531c":"# perform on test data\ntest_id = test.id\n\ntest = test.drop([\"id\"], axis = 1)\ntest.head()","897746d5":"# applying preprocessing to the test data\n# merging the texts title and text\ntest[\"whole_text\"] = test.title  + \" \" + test.text\n# filling nas since we cannot drop\ntest.fillna(method = \"ffill\", inplace = True)\n\ntest.isnull().sum()","67e7cc25":"test.shape","314f5dab":"# apply preprocessing to test data\ntest[\"cleaned\"] = test[\"whole_text\"].apply(preprocess)\n\nX_test = test[\"cleaned\"]\n\ntest_sequences = tokenizer.texts_to_sequences(X_test)\npadded_test = pad_sequences(test_sequences,maxlen = 40, truncating = 'post') \n\npred = model.predict_classes(padded_test)\n\npred","c73b3e4f":"# forming submission file\n# submission file must have 2 columns\n# id\n# label 1, 0\n\ntest_labels = []\nfor i in pred:\n    test_labels.append(i[0])\n\nsubmission = pd.DataFrame({\"id\": test_id, \"label\": test_labels})\n\nsubmission.shape","2395cb9c":"submission.head()","7cb12d18":"submission.to_csv(\"Submission.csv\",index=False)","72d48ac4":"# Testing model","c832bcf5":"# Exploring the data","b0a46403":"# Preprocesing the data","15105825":"# Training model"}}