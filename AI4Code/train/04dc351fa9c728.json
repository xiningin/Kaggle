{"cell_type":{"32fa342f":"code","d5419e69":"code","d4748de9":"code","dceede9c":"code","c597f684":"code","7c4afe24":"code","1a4aece5":"code","9a264e21":"code","d49f41ea":"code","7bec0245":"code","96feee0f":"code","c7399459":"code","81a58897":"code","a34dbc78":"code","bdec3c8a":"code","e2103f7d":"code","d31ec430":"code","2200fa55":"code","8542db53":"code","9f9a9a9f":"code","faa3667d":"code","e9978cdf":"code","8f88b151":"code","c2deb285":"code","e672acc9":"code","59822414":"code","e6406c54":"code","3a0bf9c2":"code","466048bc":"code","f4844c67":"code","22d40f74":"code","5f5a0cd8":"code","70fc7a0c":"code","a2d41b20":"code","b1cc359e":"code","f1b61d3d":"code","a35da6db":"code","8d8ec3de":"code","3e25dd03":"code","13beffec":"code","6dfb0e81":"code","46c1b199":"code","8e3d74fd":"code","b94ad9e7":"code","c7066994":"code","5b70c991":"code","e95468ff":"code","b1ca16c1":"code","9af3e96e":"code","91150f79":"code","3d920807":"code","445ca58b":"code","7a4af789":"code","8d5cbd1c":"code","d82f172d":"code","76adc765":"code","a0d38dc6":"code","cf191a70":"markdown","c962003c":"markdown","bd782bea":"markdown","7a8be26b":"markdown","a5351aff":"markdown","f2a735f9":"markdown","84be4cc9":"markdown","1b722c13":"markdown","20d018f6":"markdown","c4e0bcfe":"markdown","9c3abd73":"markdown","e45b0840":"markdown","2f1472e8":"markdown","9aa879dc":"markdown","e8831df4":"markdown","889bb6de":"markdown","05ebaf7b":"markdown","89ddc895":"markdown","adc8a87f":"markdown","91ebc984":"markdown","6216be80":"markdown","a8020368":"markdown","674f56b4":"markdown","b80f0e6c":"markdown","9affca7d":"markdown","c82e6cc3":"markdown","a649eab1":"markdown","c8d32103":"markdown","9090d716":"markdown","3d3f3da3":"markdown","e1a3b70f":"markdown","e0bcb4cb":"markdown","1304246e":"markdown","5908e618":"markdown","5976853c":"markdown"},"source":{"32fa342f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d5419e69":"!pip install missingno\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport statsmodels.api as sm\nimport warnings\nwarnings.simplefilter(\"ignore\")\nimport missingno as msno\nfrom datetime import date\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler, RobustScaler\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score,roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import KNNImputer","d4748de9":"pd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('display.float_format', lambda x: '%.5f' % x)","dceede9c":"df_ = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")","c597f684":"df = df_.copy()\ndf.head()","7c4afe24":"def check_df(dataframe, head=5, tail = 5):\n    print(\"##################### Shape #####################\")\n    print(dataframe.shape)\n    print(\"##################### Types #####################\")\n    print(dataframe.dtypes)\n    print(\"##################### Head ######################\")\n    print(dataframe.head(head))\n    print(\"##################### Tail ######################\")\n    print(dataframe.tail(tail))\n    print(\"##################### NA ########################\")\n    print(dataframe.isnull().sum())\n    print(\"##################### Quantiles #####################\")\n    print(dataframe.quantile([0, 0.05, 0.50, 0.95, 0.99, 1]).T)\n\ncheck_df(df)","1a4aece5":"def grab_col_names(dataframe, cat_th=10, car_th=20):\n    \"\"\"\n    It gives the names of categorical, numerical and categorical but cardinal variables in the data set.\n    Note: Categorical variables with numerical appearance are also included in categorical variables.\n    Parameters\n    ------\n        dataframe: dataframe\n                The dataframe from which variable names are to be retrieved\n        cat_th: int, optional\n                Class threshold for numeric but categorical variables\n        car_th: int, optinal\n                Class threshold for categorical but cardinal variables\n    Returns\n    ------\n        cat_cols: list\n                Categorical variable list\n        num_cols: list\n                Numeric variable list\n        cat_but_car: list\n                Categorical view cardinal variable list\n    Examples\n    ------\n        import seaborn as sns\n        df = sns.load_dataset(\"iris\")\n        print(grab_col_names(df))\n    Notes\n    ------\n        cat_cols + num_cols + cat_but_car = total number of variables\n        num_but_cat is inside cat_cols.\n        The sum of 3 lists with return is equal to the total number of variables: cat_cols + num_cols + cat_but_car = number of variables\n    \"\"\"\n\n    # cat_cols, cat_but_car\n    cat_cols = [col for col in dataframe.columns if dataframe[col].dtypes == \"O\"]\n    num_but_cat = [col for col in dataframe.columns if dataframe[col].nunique() < cat_th and\n                   dataframe[col].dtypes != \"O\"]\n    cat_but_car = [col for col in dataframe.columns if dataframe[col].nunique() > car_th and\n                   dataframe[col].dtypes == \"O\"]\n    cat_cols = cat_cols + num_but_cat\n    cat_cols = [col for col in cat_cols if col not in cat_but_car]\n\n    # num_cols\n    num_cols = [col for col in dataframe.columns if dataframe[col].dtypes != \"O\"]\n    num_cols = [col for col in num_cols if col not in num_but_cat]\n\n    print(f\"Observations: {dataframe.shape[0]}\")\n    print(f\"Variables: {dataframe.shape[1]}\")\n    print(f'cat_cols: {len(cat_cols)}')\n    print(f'num_cols: {len(num_cols)}')\n    print(f'cat_but_car: {len(cat_but_car)}')\n    print(f'num_but_cat: {len(num_but_cat)}')\n    return cat_cols, num_cols, cat_but_car","9a264e21":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","d49f41ea":"cat_cols","7bec0245":"num_cols","96feee0f":"df[num_cols].describe().T","c7399459":"df[cat_cols].describe()","81a58897":"def cat_summary(dataframe, col_name, plot=False):\n    print(pd.DataFrame({col_name: dataframe[col_name].value_counts(),\n                        \"Ratio\": 100 * dataframe[col_name].value_counts() \/ len(dataframe)}))\n    print(\"##########################################\")\n    if plot:\n        sns.countplot(x=dataframe[col_name], data=dataframe)\n        plt.show()","a34dbc78":"cat_summary(df, \"Outcome\", True)","bdec3c8a":"def num_summary(dataframe, numerical_col, plot=False):\n    quantiles = [0.05, 0.10, 0.20, 0.30, 0.40, 0.50, 0.60, 0.70, 0.80, 0.90, 0.95, 0.99]\n    print(dataframe[numerical_col].describe(quantiles).T)\n\n    if plot:\n        dataframe[numerical_col].hist(bins=20)\n        plt.xlabel(numerical_col)\n        plt.title(numerical_col)\n        plt.show()","e2103f7d":"num_summary(df, num_cols, True)","d31ec430":"def target_summary_with_num(dataframe, target, numerical_col):\n    print(dataframe.groupby(target).agg({numerical_col: \"mean\"}), end=\"\\n\\n\\n\")\n\nfor col in num_cols:\n    target_summary_with_num(df, \"Outcome\", col)","2200fa55":"df.corr()\n\nf, ax = plt.subplots(figsize=[18, 13])\nsns.heatmap(df.corr(), annot=True, fmt=\".2f\", ax=ax, cmap=\"magma\")\nax.set_title(\"Correlation Matrix\", fontsize=20)\nplt.show()","8542db53":"def pairplot(dataset, target_column):\n    sns.set(style=\"ticks\")\n    sns.pairplot(dataset, hue=target_column)\n    plt.show()\n\npairplot(df, 'Outcome')","9f9a9a9f":"y = df[\"Outcome\"]\nX = df.drop(\"Outcome\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n\nrf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\n\nprint(f\"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}\")\nprint(f\"Recall: {round(recall_score(y_pred,y_test),3)}\")\nprint(f\"Precision: {round(precision_score(y_pred,y_test), 2)}\")\nprint(f\"F1: {round(f1_score(y_pred,y_test), 2)}\")\nprint(f\"Auc: {round(roc_auc_score(y_pred,y_test), 2)}\")","faa3667d":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(rf_model, X)","e9978cdf":"zero_columns = [col for col in df.columns if (df[col].min() == 0 and col not in [\"Pregnancies\", \"Outcome\"])]\n\nzero_columns","8f88b151":"for col in zero_columns:\n    df[col] = np.where(df[col] == 0, np.nan, df[col])","c2deb285":"def missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 2)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df, end=\"\\n\")\n    if na_name:\n        return na_columns","e672acc9":"na_columns = missing_values_table(df, na_name=True)","59822414":"for col in zero_columns:\n    df.loc[df[col].isnull(), col] = df[col].median()","e6406c54":"def outlier_thresholds(dataframe, col_name, q1=0.25, q3=0.75):\n    quartile1 = dataframe[col_name].quantile(q1)\n    quartile3 = dataframe[col_name].quantile(q3)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","3a0bf9c2":"def check_outlier(dataframe, col_name):\n    low_limit, up_limit = outlier_thresholds(dataframe, col_name)\n    if dataframe[(dataframe[col_name] > up_limit) | (dataframe[col_name] < low_limit)].any(axis=None):\n        return True\n    else:\n        return False","466048bc":"def replace_with_thresholds(dataframe, variable, q1=0.05, q3=0.95):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable, q1=0.05, q3=0.95)\n    dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n    dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","f4844c67":"for col in df.columns:\n    print(col, check_outlier(df, col))\n    if check_outlier(df, col):\n        replace_with_thresholds(df, col)","22d40f74":"for col in df.columns:\n    print(col, check_outlier(df, col))","5f5a0cd8":"df.loc[(df[\"Age\"] >= 21) & (df[\"Age\"] < 50), \"New_Age_Cat\"] = \"Mature\"\ndf.loc[(df[\"Age\"] >= 50), \"New_Age_Cat\"] = \"Senior\"","70fc7a0c":"df['New_BMI'] = pd.cut(x=df['BMI'], bins=[0, 18.5, 24.9, 29.9, 100],labels=[\"Underweight\", \"Healthy\", \"Overweight\", \"Obese\"])","a2d41b20":"df[\"New_Glucose\"] = pd.cut(x=df[\"Glucose\"], bins=[0, 140, 200, 300], labels=[\"Normal\", \"Prediabetes\", \"Diabetes\"])","b1cc359e":"df.loc[(df[\"BMI\"] < 18.5) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_BMI_Nom\"] = \"Under_Weight_Mature\"\ndf.loc[(df[\"BMI\"] < 18.5) & (df[\"Age\"] >= 50), \"New_Age_BMI_Nom\"] = \"Under_Weight_Senior\"\ndf.loc[((df[\"BMI\"] >= 18.5) & (df[\"BMI\"] < 25)) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_BMI_Nom\"] = \"Healthy_Mature\"\ndf.loc[((df[\"BMI\"] >= 18.5) & (df[\"BMI\"] < 25)) & (df[\"Age\"] >= 50), \"New_Age_BMI_Nom\"] = \"Healthy_Senior\"\ndf.loc[((df[\"BMI\"] >= 25) & (df[\"BMI\"] < 30)) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_BMI_Nom\"] = \"Overweight_Mature\"\ndf.loc[((df[\"BMI\"] >= 25) & (df[\"BMI\"] < 30)) & (df[\"Age\"] >= 50), \"New_Age_BMI_Nom\"] = \"Overweight_Senior\"\ndf.loc[(df[\"BMI\"] > 18.5) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_BMI_Nom\"] = \"Obese_Mature\"\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"Age\"] >= 50), \"New_Age_BMI_Nom\"] = \"Obese_Senior\"","f1b61d3d":"df.loc[(df[\"Glucose\"] < 70) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_Glucose_Nom\"] = \"Low_Mature\"\ndf.loc[(df[\"Glucose\"] < 70) & (df[\"Age\"] >= 50), \"New_Age_Glucose_Nom\"] = \"Low_Senior\"\ndf.loc[((df[\"Glucose\"] >= 70) & (df[\"Glucose\"] < 100)) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_Glucose_Nom\"] = \"Normal_Mature\"\ndf.loc[((df[\"Glucose\"] >= 70) & (df[\"Glucose\"] < 100)) & (df[\"Age\"] >= 50), \"New_Age_Glucose_Nom\"] = \"Normal_Senior\"\ndf.loc[((df[\"Glucose\"] >= 100) & (df[\"Glucose\"] <= 125)) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_Glucose_Nom\"] = \"Hidden_Mature\"\ndf.loc[((df[\"Glucose\"] >= 100) & (df[\"Glucose\"] <= 125)) & (df[\"Age\"] >= 50), \"New_Age_Glucose_Nom\"] = \"Hidden_Senior\"\ndf.loc[(df[\"Glucose\"] > 125) & ((df[\"Age\"] >= 21) & (df[\"Age\"] < 50)), \"New_Age_Glucose_Nom\"] = \"High_Mature\"\ndf.loc[(df[\"Glucose\"] > 125) & (df[\"Age\"] >= 50), \"New_Age_Glucose_Nom\"] = \"High_Senior\"\n","a35da6db":"def set_insulin(dataframe, col_name=\"Insulin\"):\n    if 16 <= dataframe[col_name] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"","8d8ec3de":"df[\"New_Insulin_Score\"] = df.apply(set_insulin, axis=1)","3e25dd03":"df[\"New_Glucose*Insulin\"] = df[\"Glucose\"] * df[\"Insulin\"]","13beffec":"df[\"New_Glucose*Pregnancies\"] = df[\"Glucose\"] * df[\"Pregnancies\"]\n#df[\"New_Glucose*Pregnancies\"] = df[\"Glucose\"] * (1+ df[\"Pregnancies\"])","6dfb0e81":"cat_cols, num_cols, cat_but_car = grab_col_names(df)","46c1b199":"def label_encoder(dataframe, binary_col):\n    labelencoder = LabelEncoder()\n    dataframe[binary_col] = labelencoder.fit_transform(dataframe[binary_col])\n    return dataframe","8e3d74fd":"binary_cols = [col for col in df.columns if df[col].dtypes == \"O\" and df[col].nunique() == 2]\nbinary_cols","b94ad9e7":"for col in binary_cols:\n    df = label_encoder(df, col)","c7066994":"cat_cols = [col for col in cat_cols if col not in binary_cols and col not in [\"Outcome\"]]\ncat_cols","5b70c991":"def one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe","e95468ff":"df = one_hot_encoder(df, cat_cols, drop_first=True)\n\ndf.head()","b1ca16c1":"df.info()","9af3e96e":"num_cols","91150f79":"scaler = RobustScaler()","3d920807":"df[num_cols] = scaler.fit_transform(df[num_cols])","445ca58b":"df.head()","7a4af789":"df.shape","8d5cbd1c":"df.isnull().sum()","d82f172d":"df.describe().T","76adc765":"y = df[\"Outcome\"]\nX = df.drop(\"Outcome\", axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=17)\n\nrf_model = RandomForestClassifier(random_state=46).fit(X_train, y_train)\ny_pred = rf_model.predict(X_test)\n\nprint(f\"Accuracy: {round(accuracy_score(y_pred, y_test), 2)}\")\nprint(f\"Recall: {round(recall_score(y_pred,y_test),3)}\")\nprint(f\"Precision: {round(precision_score(y_pred,y_test), 2)}\")\nprint(f\"F1: {round(f1_score(y_pred,y_test), 2)}\")\nprint(f\"Auc: {round(roc_auc_score(y_pred,y_test), 2)}\")","a0d38dc6":"def plot_importance(model, features, num=len(X), save=False):\n    feature_imp = pd.DataFrame({'Value': model.feature_importances_, 'Feature': features.columns})\n    print(feature_imp.sort_values(\"Value\",ascending=False))\n    plt.figure(figsize=(10, 10))\n    sns.set(font_scale=1)\n    sns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\",\n                                                                     ascending=False)[0:num])\n    plt.title('Features')\n    plt.tight_layout()\n    plt.show()\n    if save:\n        plt.savefig('importances.png')\n\nplot_importance(rf_model, X)","cf191a70":"***There is only the \"Outcome\" variable in the cat_cols variable.***","c962003c":"***As can be seen, the Glucose variable in the data set is in the 1st row and the BMI variable is in the 2nd row. This ranking is ordered according to the importance of how they affect the Outcome variable.***","bd782bea":"**Bussiness Problem**\n\n***It is desired to develop a machine learning model that can predict whether people have diabetes when their characteristics are specified. You are expected to perform the necessary data analysis and feature engineering steps before developing the model.***\n\n**Dataset Story**\n\n***The dataset is part of the large dataset held at the National Institutes of Diabetes-Digestive-Kidney Diseases in the USA. Data used for diabetes research on Pima Indian women aged 21 and over living in Phoenix, the 5th largest city of the State of Arizona in the USA. The target variable is specified as \"outcome\"; 1 indicates positive diabetes test result, 0 indicates negative.***\n\n\n**Pregnancies** : Number of pregnancies\n\n**Glucose** : 2-hour plasma glucose concentration in the oral glucose tolerance test\n\n**Blood Pressure** : Blood Pressure (Low blood pressure) (mmHg)\n\n**SkinThickness** : Skin Thickness\n\n**Insulin** : 2-hour serum insulin (mu U\/ml)\n\n**DiabetesPedigreeFunction** : Function (2 hour plasma glucose concentration in oral glucose tolerance test)\n\n**BMI** : Body Mass Index\n\n**Age** : Age(year)\n\n**Outcome** : Have the disease (1) or not (0)\n\n\n**Project Tasks**\n\n**Step 1** : Examine the overall picture.\n\n**Step 2** : Capture the numeric and categorical variables.\n\n**Step 3** : Analyze the numerical and categorical variables.\n\n**Step 4** : Perform target variable analysis. (The mean of the target variable according to the categorical variables, the mean of the numeric variables according to the target variable)\n\n**Step 5** : Analyze the outlier observation.\n\n**Step 6** : Perform missing observation analysis.\n\n**Step 7** : Perform correlation analysis.\n\n\n**Task 2 : Feature Engineering**\n\n\n**Step 1** : Take necessary actions for missing and outlier values. There are no missing observations in the data set, but Glucose, Insulin etc. Observation units containing 0 in the variables may represent the missing value. E.g; a person's glucose or insulin value will not be 0. Considering this situation, you can assign the zero values to the relevant values as NaN and then apply the operations to the missing values.\n\n**Step 2** : Create new variables.\n\n**Step 3** : Perform the encoding operations.\n\n**Step 4** : Standardize for numeric variables.\n\n**Step 5** : Create the model.","7a8be26b":"***Here we first looked at the variables and the number of observation units of our dataset. The data set consists of 768 observation units and 9 variables.***\n\n***Then we looked at which data type our variables have, the first 5 observation units, the last 5 observation units, their missing values and descriptive statistics values.***","a5351aff":"![G3V7C.png](attachment:c1fabe06-5e8e-4c86-8982-db8c2011c016.png)\n\n***Here we used the robust scaler method to standardize our numerical variables. Robust scaler is more resistant to outliers than standard scaler. Subtract the median from all values and divide by IQR.***","f2a735f9":"![1_9VU1fPokjaMjohu0qDMVtQ.png](attachment:d996d7cc-2f75-49b1-bd4e-bfbbb1587af4.png)\n***Although there is no ordinal difference in a variable, we apply one hot encoding in such cases because when we apply label encoding, it categorizes it as if there is an ordinal difference. We need to convert the observations of a categorical variables into variables. There may be a case of a dummy variable here. When we apply \"drop_first=True\" we get rid of dummy variables. We updated the newly created variables and applied One-Hot Encoding here to encode them. ***","84be4cc9":"![0_Amq1f59OybLuuyJ2.png](attachment:5c7d29ef-9851-4e25-86bc-adfd8c5c3209.png)***Catching outliers is, in other words, finding threshold values. We need to calculate the quarter values of a variable so that we can capture the IQR values. We create a function called \"outlier_thresholds\" to see if there are outliers, by creating lower and upper values, and to see if there are any outliers.***","1b722c13":"***We see that there are improvements in accuracy, precision, recall, f1score, auc values between the base model and the developed model.***","20d018f6":"***We divided the BMI variable into categories and created a new BMI variable. A BMI below 18.5 is underweight, between 18.5 and 24.9 is normal, between 24.9 and 29.9 is Overweight, and above 30 is obese.***","c4e0bcfe":"***We quickly displayed the frequency of the deficiencies in the data set, the names of the variables with missing data, and the rate of these deficiencies.***","9c3abd73":"***In this function, we check whether there are any missing values in the variables.***","e45b0840":"***The num_cols variable contains the variables \"Pregnancies, Glucose, BloodPressure, SkinThickness, Insulin, BMI, DiabetesPedigreeFunction, 'Age\".***","2f1472e8":"***Here we show and examine the relationship of our variables with respect to the target variable.***","9aa879dc":"***In the above section, we examine separately the descriptive statistics data of the variables num_cols and cat_cols.***","e8831df4":"***We derived a new variable by dividing the insulin variable into categories. We also need to pay attention to the zero values in the Pregnacies variable, so we need to remove the 0 values.***","889bb6de":"***The units of measurement in some variables are 0. Variables that have a value of 0 in them are: Glucose, BloodPressure, SkinThickness, Insulin, BMI. The values of the observation units in these variables should not be 0. When we examine the data, it is very important to do a literature research on the data. For this reason, we can replace the 0 value with NaN.***","05ebaf7b":"***We divided the age variable into categories and created a new age variable.***","89ddc895":"***We insert our data set into the model without deriving any new variables. Because after the changes we will make, we need to compare whether the model output is successful or not.***","adc8a87f":"*** Pairplot() plots bidirectional relationships for numeric columns across the entire data frame.***","91ebc984":"![1.jpg](attachment:ca6ccda6-86e7-4021-987d-32fd8ef6a45c.jpg)","6216be80":"***We redefine the numeric and categorical variables.***","a8020368":"***Returns the variables with the names of missing values named \"dataframe\" in the defined argument of this function and \"na_columns\" in the second argument. In the first line, there is an argument in which the names of the variables with missing values in this data set are selected when the relevant data set is defined. Then we create an argument named \"n_miss\" to find the number of missing values. Under the \"n_miss\" argument, we define an argument named \"ratio\" that gives a ratio for missing values. Finally, we convert them into a dataset called \"missing_df\" and combine them with the \"concat\" operation.***","674f56b4":"***We created a categorical variable by considering the age variable and BMI variable together.***","b80f0e6c":"***To create a new Feature, knowledge of how variables affect each other is required. For this reason, we examine the correlations of the variables with each other.***","9affca7d":"***We divided the glucose variable into categories and created the \"New_Glucose\" variable.***","c82e6cc3":"***If a categorical variable has two classes and these classes are encoded as 1-0, it is called binary encoding. If it has more than two classes, it is called label encoding. We call the LabelEncoder algorithm and assign it to a variable named labelencoder.***","a649eab1":"***The num_but_cat variable is actually inside the cat_cols variable. Printed for reporting purposes.***","c8d32103":"***Here, we suppress the outliers that find according to the IQR values which found before.***","9090d716":"***We fill in the missing values using the median. Here, the KNN method can also be used, but before using it, standardization should be made on other variables except the target variable. Afterwards, it should be converted back to its old numeric values with the inverse_transform method.***","3d3f3da3":"***To summarize this function, we first select the categorical variables, then we select the numeric but categorical ones. We set cat_th at the beginning of the function. We can call this at categoric threshold. Even if a variable is numeric if it has less than 10 classes, that variable is a categorical variable for us. In other words, if the number of classes in a variable is less than 10, it can be a categorical variable even if it is numeric. Then we set the cardinal threshold. If a categorical variable has more than 20 classes and its type is categorical, it appears categorical and is cardinal.***\n\n***We build the cat_cols list from scratch. There were categorical variables that looked numeric here, and we're adding them as well. There were also high cardinal variables in cat_cols, we select categorical but non-cardinal variables and remove them.***\n\n***num_cols refers to numeric columns. Here, those whose type is different from object, namely int, float ones, will be included in num_cols. We will also subtract numeric and categorical ones.***","e1a3b70f":"***The analyzes of categorical variables are generally done over frequencies. Frequency is the number of observations of categories (elements) in a variable.***","e0bcb4cb":"***Here, on the other hand, we analyze the numerical variables by looking at their frequencies and show the graph.***","1304246e":"***When we look at the priority order for the target variable, the Glucose variable was in the 1st place during the priority in our base model, and the Glucose variable was in the 1st place after the feature engineering applications. The \"Glucose * Insulin\" variable we created later takes the second place here.***\n\n\n***Here, we see that there are serious changes in the variables with the priority order for the target variable.***","5908e618":"**First, we observe the dataset in a general framework.**","5976853c":"***We created a new categorical variable by considering the age variable and the glucose variable together.***"}}