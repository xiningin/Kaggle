{"cell_type":{"e10beaf8":"code","65ac65ff":"code","f37578d0":"code","168b2f0f":"code","079d0426":"code","06be6b1e":"code","0122a511":"code","788697a4":"code","11371383":"code","32da821e":"code","7b9dae4f":"code","30f6ea14":"code","fe4620e5":"code","933be5b1":"code","e2afba34":"code","6eb4547f":"code","8512975a":"code","2df53b1b":"code","13380719":"code","3d6f3c46":"markdown","93d663b2":"markdown","bf433c23":"markdown","c144f1a4":"markdown","1d648853":"markdown","97ae7311":"markdown","aa29895f":"markdown","669b0f28":"markdown","a68ba783":"markdown","697f3f57":"markdown","db789426":"markdown","ea76c235":"markdown","31b9af28":"markdown","4695d366":"markdown","027cc800":"markdown","9c2a1e95":"markdown","0f15ca99":"markdown","d1cdfb59":"markdown","fad9e204":"markdown","dbf7f0bd":"markdown"},"source":{"e10beaf8":"# Imports needed for the script\nimport numpy as np\nimport pandas as pd\nimport re\nimport xgboost as xgb\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\nfrom sklearn import tree\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom IPython.display import Image as PImage\nfrom subprocess import check_call\nfrom PIL import Image, ImageDraw, ImageFont\n\n# Loading the data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n# Store our test passenger IDs for easy access\nPassengerId = test['PassengerId']\n\n# Showing overview of the train dataset\ntrain.head(3)","65ac65ff":"# \"original_train = train\" will create a reference to the train variable (changes in 'train' will apply to 'original_train')\noriginal_train = train.copy() # Using 'copy()' allows to clone the dataset, creating a different object with the same values\n\n# Feature engineering steps taken from Sina and Anisotropic, with minor changes to avoid warnings\nfull_data = [train, test]\n\n# Feature that tells whether a passenger had a cabin on the Titanic\ntrain['Has_Cabin'] = train[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\ntest['Has_Cabin'] = test[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n\n# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n# Create new feature IsAlone from FamilySize\nfor dataset in full_data:\n    dataset['IsAlone'] = 0\n    dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# Remove all NULLS in the Embarked column\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna('S')\n# Remove all NULLS in the Fare column\nfor dataset in full_data:\n    dataset['Fare'] = dataset['Fare'].fillna(train['Fare'].median())\n\n# Remove all NULLS in the Age column\nfor dataset in full_data:\n    age_avg = dataset['Age'].mean()\n    age_std = dataset['Age'].std()\n    age_null_count = dataset['Age'].isnull().sum()\n    age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n    # Next line has been improved to avoid warning\n    dataset.loc[np.isnan(dataset['Age']), 'Age'] = age_null_random_list\n    dataset['Age'] = dataset['Age'].astype(int)\n\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\nfor dataset in full_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\nfor dataset in full_data:\n    # Mapping Sex\n    dataset['Sex'] = dataset['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n    \n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Master\": 2, \"Mrs\": 3, \"Miss\": 4, \"Rare\": 5}\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\n    # Mapping Embarked\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n    \n    # Mapping Fare\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] \t\t\t\t\t\t        = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] \t\t\t\t\t\t\t        = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n    \n    # Mapping Age\n    dataset.loc[ dataset['Age'] <= 16, 'Age'] \t\t\t\t\t       = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] ;","f37578d0":"# Feature selection: remove variables no longer containing relevant information\ndrop_elements = ['PassengerId', 'Name', 'Ticket', 'Cabin', 'SibSp']\ntrain = train.drop(drop_elements, axis = 1)\ntest  = test.drop(drop_elements, axis = 1)","168b2f0f":"train.head(3)","079d0426":"colormap = plt.cm.viridis\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor='white', annot=True)","06be6b1e":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since \"Survived\" is a binary class (0 or 1), these metrics grouped by the Title feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n\n# title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5} ","0122a511":"train[['Sex', 'Survived']].groupby(['Sex'], as_index=False).agg(['mean', 'count', 'sum'])\n# Since Survived is a binary feature, this metrics grouped by the Sex feature represent:\n    # MEAN: survival rate\n    # COUNT: total observations\n    # SUM: people survived\n    \n# sex_mapping = {{'female': 0, 'male': 1}} ","788697a4":"# Let's use our 'original_train' dataframe to check the sex distribution for each title.\n# We use copy() again to prevent modifications in out original_train dataset\ntitle_and_sex = original_train.copy()[['Name', 'Sex']]\n\n# Create 'Title' feature\ntitle_and_sex['Title'] = title_and_sex['Name'].apply(get_title)\n\n# Map 'Sex' as binary feature\ntitle_and_sex['Sex'] = title_and_sex['Sex'].map( {'female': 0, 'male': 1} ).astype(int)\n\n# Table with 'Sex' distribution grouped by 'Title'\ntitle_and_sex[['Title', 'Sex']].groupby(['Title'], as_index=False).agg(['mean', 'count', 'sum'])\n\n# Since Sex is a binary feature, this metrics grouped by the Title feature represent:\n    # MEAN: percentage of men\n    # COUNT: total observations\n    # SUM: number of men","11371383":"# Define function to calculate Gini Impurity\ndef get_gini_impurity(survived_count, total_count):\n    survival_prob = survived_count\/total_count\n    not_survival_prob = (1 - survival_prob)\n    random_observation_survived_prob = survival_prob\n    random_observation_not_survived_prob = (1 - random_observation_survived_prob)\n    mislabelling_survided_prob = not_survival_prob * random_observation_survived_prob\n    mislabelling_not_survided_prob = survival_prob * random_observation_not_survived_prob\n    gini_impurity = mislabelling_survided_prob + mislabelling_not_survided_prob\n    return gini_impurity","32da821e":"# Gini Impurity of starting node\ngini_impurity_starting_node = get_gini_impurity(342, 891)\ngini_impurity_starting_node","7b9dae4f":"# Gini Impurity decrease of node for 'male' observations\ngini_impurity_men = get_gini_impurity(109, 577)\ngini_impurity_men","30f6ea14":"# Gini Impurity decrease if node splited for 'female' observations\ngini_impurity_women = get_gini_impurity(233, 314)\ngini_impurity_women","fe4620e5":"# Gini Impurity decrease if node splited by Sex\nmen_weight = 577\/891\nwomen_weight = 314\/891\nweighted_gini_impurity_sex_split = (gini_impurity_men * men_weight) + (gini_impurity_women * women_weight)\n\nsex_gini_decrease = weighted_gini_impurity_sex_split - gini_impurity_starting_node\nsex_gini_decrease","933be5b1":"# Gini Impurity decrease of node for observations with Title == 1 == Mr\ngini_impurity_title_1 = get_gini_impurity(81, 517)\ngini_impurity_title_1","e2afba34":"# Gini Impurity decrease if node splited for observations with Title != 1 != Mr\ngini_impurity_title_others = get_gini_impurity(261, 374)\ngini_impurity_title_others","6eb4547f":"# Gini Impurity decrease if node splited for observations with Title == 1 == Mr\ntitle_1_weight = 517\/891\ntitle_others_weight = 374\/891\nweighted_gini_impurity_title_split = (gini_impurity_title_1 * title_1_weight) + (gini_impurity_title_others * title_others_weight)\n\ntitle_gini_decrease = weighted_gini_impurity_title_split - gini_impurity_starting_node\ntitle_gini_decrease","8512975a":"cv = KFold(n_splits=10)            # Desired number of Cross Validation folds\naccuracies = list()\nmax_attributes = len(list(test))\ndepth_range = range(1, max_attributes + 1)\n\n# Testing max_depths from 1 to max attributes\n# Uncomment prints for details about each Cross Validation pass\nfor depth in depth_range:\n    fold_accuracy = []\n    tree_model = tree.DecisionTreeClassifier(max_depth = depth)\n    # print(\"Current max depth: \", depth, \"\\n\")\n    for train_fold, valid_fold in cv.split(train):\n        f_train = train.loc[train_fold] # Extract train data with cv indices\n        f_valid = train.loc[valid_fold] # Extract valid data with cv indices\n\n        model = tree_model.fit(X = f_train.drop(['Survived'], axis=1), \n                               y = f_train[\"Survived\"]) # We fit the model with the fold train data\n        valid_acc = model.score(X = f_valid.drop(['Survived'], axis=1), \n                                y = f_valid[\"Survived\"])# We calculate accuracy with the fold validation data\n        fold_accuracy.append(valid_acc)\n\n    avg = sum(fold_accuracy)\/len(fold_accuracy)\n    accuracies.append(avg)\n    # print(\"Accuracy per fold: \", fold_accuracy, \"\\n\")\n    # print(\"Average accuracy: \", avg)\n    # print(\"\\n\")\n    \n# Just to show results conveniently\ndf = pd.DataFrame({\"Max Depth\": depth_range, \"Average Accuracy\": accuracies})\ndf = df[[\"Max Depth\", \"Average Accuracy\"]]\nprint(df.to_string(index=False))","2df53b1b":"# Create Numpy arrays of train, test and target (Survived) dataframes to feed into our models\ny_train = train['Survived']\nx_train = train.drop(['Survived'], axis=1).values \nx_test = test.values\n\n# Create Decision Tree with max_depth = 3\ndecision_tree = tree.DecisionTreeClassifier(max_depth = 3)\ndecision_tree.fit(x_train, y_train)\n\n# Predicting results for test dataset\ny_pred = decision_tree.predict(x_test)\nsubmission = pd.DataFrame({\n        \"PassengerId\": PassengerId,\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)\n\n# Export our trained model as a .dot file\nwith open(\"tree1.dot\", 'w') as f:\n     f = tree.export_graphviz(decision_tree,\n                              out_file=f,\n                              max_depth = 3,\n                              impurity = True,\n                              feature_names = list(train.drop(['Survived'], axis=1)),\n                              class_names = ['Died', 'Survived'],\n                              rounded = True,\n                              filled= True )\n        \n#Convert .dot to .png to allow display in web notebook\ncheck_call(['dot','-Tpng','tree1.dot','-o','tree1.png'])\n\n# Annotating chart with PIL\nimg = Image.open(\"tree1.png\")\ndraw = ImageDraw.Draw(img)\nfont = ImageFont.truetype('\/usr\/share\/fonts\/truetype\/liberation\/LiberationSerif-Bold.ttf', 26)\ndraw.text((10, 0), # Drawing offset (position)\n          '\"Title <= 1.5\" corresponds to \"Mr.\" title', # Text to draw\n          (0,0,255), # RGB desired color\n          font=font) # ImageFont object with desired font\nimg.save('sample-out.png')\nPImage(\"sample-out.png\")\n\n# Code to check available fonts and respective paths\n# import matplotlib.font_manager\n# matplotlib.font_manager.findSystemFonts(fontpaths=None, fontext='ttf')","13380719":"acc_decision_tree = round(decision_tree.score(x_train, y_train) * 100, 2)\nacc_decision_tree","3d6f3c46":"Our dataset is now much cleaner than before, with only numerical values and potentially meaningful features. Let's now explore the relationship between our variables by plotting the Pearson Correlation between all the attributes in our dataset (credit to [Anisotropic][1] for this beautiful plot):\n\n\n  [1]: https:\/\/www.kaggle.com\/arthurtok\/titanic\/introduction-to-ensembling-stacking-in-python","93d663b2":"The data shows that less 'Mr' survived (15,67%) than men in general (18.89%): *Title* seems therefore to be more useful than *Sex* for our purpose. This may be because *Title* implicitly includes information about *Sex* in most cases. To verify this, we can use the copy we made of the original training data without mappings and check the distribution of *Sex* grouped by *Title*.","bf433c23":"Thanks to this overview we can see that our dataset needs some treatment. The class *Survived* is already in binary format so no additional formatting is necessary, but features like *Name*, *Ticket* or *Cabin* need to be adapted for the problem we're trying to solve, and we can also engineer some new features by merging or regrouping existing ones. There's already extended work on this so we're just using one the best approches out there (credit to [Sina][1], [Anisotropic][2] and also [Megan Risdal][3] for the suggestion of the \"Title\" feature).\n\n\n  [1]: https:\/\/www.kaggle.com\/sinakhorami\/titanic\/titanic-best-working-classifier\n  [2]: https:\/\/www.kaggle.com\/arthurtok\/titanic\/introduction-to-ensembling-stacking-in-python\n  [3]: https:\/\/www.kaggle.com\/mrisdal\/titanic\/exploring-survival-on-the-titanic","c144f1a4":"## Gini Impurity ##\n\nBefore start working with *Decision Trees*, let's briefly explain how they work. The goal of their learning algorithms is always to find the best split for each node of the tree. But measuring the \"goodness\" of a given split is a subjective question so, in practice, different metrics are used for evaluating splits. One commonly used metric is [Information Gain][1]. The *sklearn* library we're gonna use implements [Gini Impurity][2], another common measure, so let\u2019s explain it.\n\nGini Impurity measures the disorder of a set of elements. It is calculated as the probability of mislabelling an element assuming that the element is randomly labelled according the the distribution of all the classes in the set. *Decision Trees* will try to find the split which decreases Gini Impurity the most across the two resulting nodes. For the titanic example it can be calculated as follows (code should be explicit enough):\n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Information_gain_in_decision_trees\n  [2]: https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning#Gini_impurity","1d648853":"Finally, here we have our *Decision Tree*! It achieves an accuracy of 82.38% across the training dataset. Let's begin explaining how to read the graph.\n\nThe first line of each node (except those of the final row) shows the splitting condition in the form \"*feature* <= *value*\".\n\nNext, we find the Gini Impurity of the node, already explained in this kernel. \"Samples\" is simply the number of observations contained in the node.\n\n\"Value\" shows the class distribution of the samples ([count non_survived, count survived]).\n\n\nLastly, \"class\" correspond to the predominant class of each node, and this is how our model will classify an observation. The colour also represents the class, the opacity increasing with the actual distribution of samples.\n\nOur model can therefore be summarised with 4 simple rules:\n\n - If our observation includes de \"Mr\" *Title*, then we classify it as not survived (all the branches in the left side of the tree lead to an orange node)\n - If it doesn't include \"Mr\" *Title*, and *FamilySize* is 4 or less, then we classify it as survived.\n - If it doesn't include \"Mr\" *Title*,  *FamilySize* is  more than 4 and *Pclass* is 2 or less, then we classify it as survived.\n - If it doesn't include \"Mr\" *Title*,  *FamilySize* is  more than 4 and *Pclass* is more than 2, then we classify it as not survived.\n","97ae7311":"## Preparing the Titanic dataset ##\n\nFor the Titanic challenge we need to guess wheter the individuals from the *test* dataset had survived or not. But for our current purpose let's also find out what can the data tell us about the shipwreck with the help of a *Classification Tree*. Let's load the data and get an overview.","aa29895f":"*Title* VS *Sex*\n-------","669b0f28":"## Visualising processed data ##","a68ba783":"## Final Tree ##","697f3f57":"## Finding best tree depth with the help of Cross Validation ##\n\nAfter exploring the data, we're going to find of much of it can be relevant for our decision tree. This is a critical point for every Data Science project, since too much train data can easily result in bad model generalisation (accuracy on test\/real\/unseen observations). Over-fitting (a model excessively adapted to the train data) is a common reason. In other cases, too much data can also hide meaningful relationships either because they evolve with time or because highly correlated features prevent the model from capturing properly the value of each single one.\n\nIn the case of decision trees, the 'max_depth' parameter determines the maximum number of attributes the model is going to use for each prediction (up to the number of available features in the dataset).  A good way to find the best value for this parameter is just iterating through all the possible depths and measure the accuracy with a robust method such as [Cross Validation][1].\n\n*Cross Validation* is a model validation technique that splits the training dataset in a given number of \"folds\". Each split uses different data for training and testing purposes, allowing the model to be trained and tested with different data each time. This allows the algorithm to be trained and tested with all available data across all folds, avoiding any splitting bias and giving a good idea of the generalisation of the chosen model. The main downside is that *Cross Validation* requires the model to be trained for each fold, so the computational cost can be very high for complex models or huge datasets.\n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics)","db789426":"Let's use our *Sex* and *Title* features as an example and calculate how much each split will decrease the overall weighted Gini Impurity. First, we need to calculate the Gini Impurity of the starting node including all 891 observations in our train dataset. Since only 342 observations survived, the survival probability is around 38,38% (342\/891).","ea76c235":"The best *max_depth* parameter seems therefore to be 3 (82.8% average accuracy across the 10 folds), and feeding the model with more data results in worst results probably due to over-fitting. We'll therefore use 3 as the *max_depth* parameter for our final model.","31b9af28":"If we split by *Title* == 1 (== Mr), we'll have the two following nodes:\n\n - Node with only Mr: 517 observations with only 81 survived\n - Node with other titles: 374 observations with 261 survived","4695d366":"We're now going to simulate both splits, calculate the impurity of resulting nodes and then obtain the weighted Gini Impurity after the split to measure how much each split has actually reduced impurity.\n\nIf we split by *Sex*, we'll have the two following nodes:\n\n - Node with men: 577 observations with only 109 survived\n - Node with women: 314 observations with 233 survived","027cc800":"Thanks to these rules we can infer some insights about the shipwreck. \"Misters\" seem to have honoured their title and sacrificed themselves in favour on women and men with more exotic titles like \"Master\" or \"Dr\".  We can also note that smaller families had better chances to survive, maybe because bigger families tried to stick together or look for missing members and therefore didn't had places left in the lifeboats. Finally, we can observe that 3rd class passengers had also less chances to survive so probably passengers belonging to upper social social classes were privileged, or simply 3rd class cabins may have been further away of the lifeboats.\n\nOur submission to the Titanic competition results in scoring 2234 out of 5672 competition entries. This result only accounts for part of the submission dataset and is indicative while the competition is running. Not bad for a simple *Decision Tree*!\n\nAnd remember, any suggestions, comments or critics are welcome!\n\nThanks for reading,\n\nDiego","9c2a1e95":"We find that, excepting for a single observation (a female with 'Dr' title), all the observations for a given *Title* share the same *Sex*. Therefore the feature *Title* is capturing all the information present in *Sex*. In addition, *Title* may be more valuable to our task by capturing other characteristics of the individuals like age, social class, personality, ...\n\nIt's true that by regrouping rare titles into a single category, we are losing some information regarding *Sex*. We could create two categories \"Rare Male\" and \"Rare Female\", but the separation will be almost meaningless due to the low occurrence of \"Rare\" *Titles* (2.6%, 23 out of 891 samples).\n\nThanks to this in-depth analysis of the *Sex* and *Title* features we've seen that, even if the correlation of the feature *Sex* with the class *Survived* was higher,  *Title* is a richer feature because it carries the *Sex* information but also adds other characteristics. Therefore is very likely that *Title* is going to be the first feature in our final decision tree, making *Sex* useless after this initial split.","0f15ca99":"We find that the *Title* feature is slightly better at reducing the Gini Impurity than *Sex*. This confirms our previous analysis, and we're now sure that *Title* will be used for the first split. *Sex* will therefore be neglected since the information is already included in the *Title* feature.  If you want to learn more about how *Decision Trees* work, I recommend you to follow the links in this [Kaggle discussion][3].\n\n  [3]: https:\/\/www.kaggle.com\/c\/titanic\/discussion\/10169","d1cdfb59":"This heatmap is very useful as an initial observation because you can easily get an idea of the predictive value of each feature. In this case, *Sex* and *Title* show the highest correlations (in absolute terms) with the class (*Survived*): 0.54 and 0.49 respectively. But the absolute correlation between both is also very high (0.86, the highest in our dataset), so they are probably carrying the same information and using the two as inputs for the same model wouldn't be a good idea.  High chances are one of them will be used for the first node in our final decision tree, so let's first explore further these features and compare them.","fad9e204":"You can easily compare features and their relationship with the class by grouping them and calculating some basic statistics for each group.  The code below does exactly this in one line, and explains the meaning of each metric when working with a binary class.","dbf7f0bd":"Introduction\n--------------------\n\nWhen applying Machine Learning algorithms, it's critical to always keep in mind the problem we're trying to solve. In most cases, the most accurate and robust model might be what you're looking for. But sometimes we need to actually get insights from the available data and in these cases transparent, easy to understand models like *Decision Trees* will greatly simplify our task.\n\nIf we need to build a model that will be directly used for some task and **only show it's end results**, then we don't really care about building some kind of \"blackbox\" if it's accurate enough (image or speech recognition for example). That's why advanced techniques such as [*Deep Learning*][1] or [*Ensemble Learning*][2]  (cf. [Anisotropic Kernel][3]) are commonly used for complex tasks. But remember the KISS principle (Keep It Simple, Stupid)! Always consider the complexity\/accuracy trade-off: complex techniques should only be used if they offer significant improvements. Simpler models are also less prone to over-fitting and tend to generalise better.\n\nBut if we're using Machine Learning to actually **get insights from the data**, \"blackbox\" models are almost useless and it's best to stick with simpler, transparent techniques. Let's take the case of a supermarket looking to better understand customer behaviour: the straightforward [*Apriori*][4] algorithm can quickly offer relevant insights like \"80% of customers who bought a suit also bought a tie\" so they may try to increase tie sales by offering a discount to clients buying a suit . Of course, a complex classification algorithm will do better at identifying the customers who bought a tie by taking into account more features, but is that really useful for the supermarket?\n\n*Decision Trees* can also help a lot when we need to understanding the data. A good example is the traditional problem of classifying Iris flowers included in the [sklearn documentation][5], were we can learn about the characteristics of each flower type in the resulting tree. Given their transparency and relatively low computational cost, *Decision Trees* are also very useful for exploring your data before applying other algorithms. They're helpful for checking the quality of engineered features and identifying the most relevant ones by visualising the resulting tree.\n\nThe main downsides of *Decision Trees* are their tendency to over-fit, their inability to grasp relationships between features, and the use of greedy learning algorithms (not guaranteed to find the global optimal model). Using them in a [*Random Forest*][6] helps mitigate some of this issues.\n\nAfter this short introduction to *Decision Trees* and their place in Machine Learning, let's see how to apply them for the Titanic challenge. First, we're going to prepare the dataset and discuss the most relevant features. We'll then find the best tree depth to avoid over-fitting, generate the final model, and explain how to visualise the resulting tree.\n\n\n  [1]: https:\/\/en.wikipedia.org\/wiki\/Deep_learning\n  [2]: https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning\n  [3]: https:\/\/www.kaggle.com\/arthurtok\/titanic\/introduction-to-ensembling-stacking-in-python\n  [4]: https:\/\/en.wikipedia.org\/wiki\/Apriori_algorithm\n  [5]: http:\/\/scikit-learn.org\/stable\/modules\/tree.html\n  [6]: https:\/\/en.wikipedia.org\/wiki\/Random_forest"}}