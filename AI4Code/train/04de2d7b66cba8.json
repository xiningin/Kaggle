{"cell_type":{"253190a5":"code","7dc21313":"code","b8d3fa96":"code","ca69a682":"code","0c0af2c1":"code","853e1ebf":"code","b44048ed":"code","0bee71d7":"code","b5807cd5":"code","13d115a6":"code","dbf03956":"code","2f259409":"code","1d567fa2":"code","1d4bb9f4":"code","b571fd34":"code","f89cecb9":"code","e8bddb32":"code","96711653":"code","ab10a3e1":"markdown","f83ac1a7":"markdown","fb59d484":"markdown","a5b075c9":"markdown","e45bd3e0":"markdown","761e2cde":"markdown","91e7e0fc":"markdown","da48927b":"markdown","84d81826":"markdown","578f3284":"markdown","9888313c":"markdown"},"source":{"253190a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7dc21313":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression \nfrom statistics import mean\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBClassifier","b8d3fa96":"data = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","ca69a682":"data.head()","0c0af2c1":"data.info()","853e1ebf":"data.status.unique()","b44048ed":"data[data.status == 'Not Placed']['salary']","0bee71d7":"data.salary.fillna(0, inplace = True)  # filling missing values with 0 because 'nan' is for 'not placed candicates'","b5807cd5":"data.describe()","13d115a6":"ax = sns.countplot('status',data=data)\n\ntotal = float(len(data))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height+.5,'{:1.0%}'.format((height\/total)),ha=\"center\")","dbf03956":"from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndata['gender'] = number.fit_transform(data['gender'])\ndata['ssc_b'] = number.fit_transform(data['ssc_b'])\ndata['hsc_b'] = number.fit_transform(data['hsc_b'])\ndata['hsc_s'] = number.fit_transform(data['hsc_s'])\ndata['degree_t'] = number.fit_transform(data['degree_t'])\ndata['specialisation'] = number.fit_transform(data['specialisation'])\ndata['workex'] = number.fit_transform(data['workex'])\ndata['status'] = number.fit_transform(data['status'])","2f259409":"data1=data.drop(columns=['salary','sl_no'])","1d567fa2":"plt.figure(figsize=(14,12))\nsns.heatmap(data1.corr(), linewidth=0.2, annot=True)","1d4bb9f4":"fig,axes = plt.subplots(3,2, figsize=(20,12))\nsns.lineplot(y = \"ssc_p\", x= 'sl_no', data = data, hue = \"status\",ax=axes[0][0])\nsns.lineplot(y = \"hsc_p\", x= 'sl_no', data = data, hue = \"status\",ax=axes[0][1])\nsns.lineplot(y = \"degree_p\", x= 'sl_no', data = data, hue = \"status\",ax=axes[1][0])\nsns.lineplot(y = \"mba_p\", x= 'sl_no', data = data, hue = \"status\",ax=axes[1][1])\nsns.lineplot(y = \"etest_p\", x= 'sl_no', data = data, hue = \"status\",ax=axes[2][0])\nfig.delaxes(ax = axes[2][1]) ","b571fd34":"ax = sns.countplot('specialisation',data=data)\n\ntotal = float(len(data))\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x()+p.get_width()\/2.,height+.5,'{:1.0%}'.format((height\/total)),ha=\"center\")","f89cecb9":"X= data.drop(['status'],axis=1)\nY= data['status']","e8bddb32":"# Spliting the data using kFold split(n_splits= 3)\ntrain_accuracy_log = []\ntest_accuracy_log = []\ntrain_accuracy_rf = []\ntest_accuracy_rf = []\ntrain_accuracy_xgb = []\ntest_accuracy_xgb = []\n\nparameters = { 'max_features': [2, 4, 6, 8]}\nparam_grid = {'max_depth':np.arange(1,6),'learning_rate':[0.1,0.01,0.001]}\n\nkf = KFold(n_splits=3)\nfor train_index, test_index in kf.split(X,Y):\n    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = Y.iloc[train_index], Y.iloc[test_index]\n    \n    model = LogisticRegression()\n    model.fit(X_train,y_train)\n    \n    y_train_predict_log = model.predict(X_train)\n    train_accuracy_log.append(accuracy_score(y_train,y_train_predict_log))\n    \n    y_test_predict_log = model.predict(X_test)\n    test_accuracy_log.append(accuracy_score(y_test,y_test_predict_log))\n    \n    \n    # RandomForestClassifier\n    tune_model_rf = GridSearchCV(RandomForestClassifier(),parameters,cv=5,scoring='accuracy')\n    tune_model_rf.fit(X_train,y_train)\n        \n    model_rf = RandomForestClassifier(n_estimators=150,max_depth = tune_model_rf.best_params_['max_features'],verbose=1,random_state=82)\n    model_rf.fit(X_train,y_train)\n\n    y_train_predict_rf = model_rf.predict(X_train)\n    train_accuracy_rf.append(accuracy_score(y_train,y_train_predict_rf))\n    \n    y_test_predict_rf = model_rf.predict(X_test)\n    test_accuracy_rf.append(accuracy_score(y_test,y_test_predict_rf))\n    \n    # XGBCClassifier\n    tune_model = GridSearchCV(XGBClassifier(objective = 'binary:logistic'),param_grid,cv=5)\n    tune_model.fit(X_train,y_train)\n    \n    model_xgb = XGBClassifier(objective = 'binary:logistic',random_state=82,learning_rate= tune_model.best_params_['learning_rate'], max_depth=tune_model.best_params_['max_depth'] )\n    model_xgb.fit(X_train,y_train)\n    \n    y_train_predict_xgb = model_xgb.predict(X_train)\n    train_accuracy_xgb.append(accuracy_score(y_train,y_train_predict_xgb))\n    \n    y_test_predict_xgb = model_xgb.predict(X_test)\n    test_accuracy_xgb.append(accuracy_score(y_test,y_test_predict_xgb))\n","96711653":"print('Logistic regression Train accuracy : ' , mean(train_accuracy_log)) \nprint('Logistic regression Test accuracy : ' , mean(test_accuracy_log)) \n\nprint('RandomForest classifier Train accuracy: ', mean(train_accuracy_rf)) \nprint('RandomForest classifier Test accuracy: ', mean(test_accuracy_rf) )\n\nprint('XGboost classifier Train accuracy : ', mean(train_accuracy_xgb))   \nprint('XGboost classifier Test accuracy : ', mean(test_accuracy_xgb))","ab10a3e1":"Updating the missing values","f83ac1a7":"**Model Building using Machine Learning Algorithms**","fb59d484":"**Question 3:-Which degree specialization is much demanded by corporate?**","a5b075c9":"**1. EDA **","e45bd3e0":"From the above Heatmap we can consider below attributes influence a candidate in getting placed\n\n* Candidate with higher SSC Percentage ,HSC Percentage and Higher Degree Percentage have high changes of getting placed.","761e2cde":"* 69% of the students where placed\n* 31% of students where not placed","91e7e0fc":"* From above analysis(HeatMap) we can say that percentage matter for placement.\n* From lineplot we can say that the ratio of student with higher percentage in SSC,HSC and Degree have got placed.","da48927b":"**Question 2:- Does percentage matters for one to get placed?**","84d81826":"Question 1:- Which factor influenced a candidate in getting placed?","578f3284":"0 - Marketing and Finance Specialization , \n1 - Marketing and HR Specialization\n\n* Marketing and Finance Specialization as much demanded by corporate.","9888313c":"**Questions**\n\n*     Which factor influenced a candidate in getting placed?\n*     Does percentage matters for one to get placed?\n*     Which degree specialization is much demanded by corporate?\n*     Play with the data conducting all statistical tests.\n"}}