{"cell_type":{"c00904a5":"code","a12a76dd":"code","90054b25":"code","f955f083":"code","301d1c55":"code","cdfb5399":"code","9cc28db7":"code","2759b8f0":"code","27508eaa":"code","713e22d7":"markdown","e3d2a557":"markdown"},"source":{"c00904a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/where_am_i\/data\/examples\/may_the_4_be_with_u\/where_am_i\/train\"))\n\n# Any results you write to the current directory are saved as output.","a12a76dd":"from sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport os\nimport glob\nimport cv2\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt","90054b25":"labelmap=pd.read_csv('..\/input\/mid_term_mapping.txt',names=['place','index'])","f955f083":"labelmap","301d1c55":"# \u4ee5 \"index\" \u70ba\u57fa\u6e96\uff0c\u91cd\u65b0\u6392\u5217\nsortlabel=labelmap.sort_values('index')\nsortlabel","cdfb5399":"targetlist=sortlabel.place.values.tolist()\ntargetlist","9cc28db7":"sz=10\nx=[]\ny=[]\nfor i,c in enumerate(targetlist):\n    ff=glob.glob(('..\/input\/where_am_i\/data\/examples\/may_the_4_be_with_u\/where_am_i\/train\/'+c+'\/*jpg'))\n    for f in ff:\n        img=cv2.imread(f,0)\n        img200=cv2.resize(img,(sz,sz))\n        iii=(np.reshape(img200,(sz,sz,1))).astype(float)\n        x.append(iii)\n        nn=np.zeros(15,dtype=float)\n        nn[i]=1\n        y.append(nn)\ntrain_image=np.array(x)\ntrain_target=np.array(y)","2759b8f0":"train_image","27508eaa":"train_target","713e22d7":"## General writing flow\n1. import required libraries\n2. load data and do some data pre-processing\n3. split your data into training and validation set\n4. build the network\n5. train the model and record\/monitoring the performance","e3d2a557":"## 1. Import required libries and set some parameters "}}