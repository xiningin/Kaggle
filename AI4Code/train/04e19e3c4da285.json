{"cell_type":{"ddf11b13":"code","3ea90a20":"code","a7021c5c":"code","905d6893":"code","dbaf2455":"code","0546fbb1":"markdown","e4384b00":"markdown","bf5172aa":"markdown","a04d379f":"markdown","a28d94c3":"markdown","65007b34":"markdown"},"source":{"ddf11b13":"#Remove HTML tags\ndef remove_html_tags(text):\n    \"\"\"Remove html tags from a string\"\"\"\n    import re\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)\n\n#Remove extra whitespaces\ndef remove_extra_space(txt):\n    import re\n    return re.sub(' +',' ',txt)\n\n#Language identification \ndef language(txt):\n    #pip install langdetect\n    import langdetect\n    lan = langdetect.detect(txt)\n    if lan == \"en\":\n        print(\"Uploaded Language is English\")\n        return (txt)\n    else:\n        return(\"Uploaded language is not in english\")\n\n#Remove non-ASCII characters from list of tokenized words   \ndef remove_non_ascii(words):\n    import unicodedata\n    new_words = []\n    for word in words:\n        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n        new_words.append(new_word)\n        rst = ''.join(new_words)\n    return rst    \n    \n#Expand contractions\ndef expand_contraction(txt):\n    #pip install contractions\n    import contractions\n    return contractions.fix(txt)\n\n#Remove special characters\ndef remove_punctuation(txt):\n    import string\n    result = txt.translate(str.maketrans('','',string.punctuation))\n    return result\n\n#Lowercase all texts\ndef lower_text(txt):\n    return txt.lower()\n\n#Remove numbers\ndef remove_no(txt):\n    import re\n    return re.sub(r\"\\d+\",\"\",txt)\n\n# convert number into words \ndef convert_number(text): \n    #pip install inflect \n    # split string into list of words \n    temp_str = text.split() \n    # initialise empty list \n    new_string = [] \n    for word in temp_str: \n        # if word is a digit, convert the digit \n        # to numbers and append into the new_string list \n        if word.isdigit(): \n            temp = p.number_to_words(word) \n            new_string.append(temp) \n  \n        # append the word as it is \n        else: \n            new_string.append(word) \n  \n    # join the words of new_string to form a string \n    temp_str = ' '.join(new_string) \n    return temp_str \n\n#Text Normalization\ndef normalize(txt):\n    words = remove_non_ascii(txt)\n    words = lower_text(words)\n    words = remove_punctuation(words)\n    words = remove_no(words)\n    return words\n#Remove URL\ndef removeurl(txt):\n    import re\n    return re.sub(r'http?\\S+|www\\.\\S+', '',txt)\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                               u\"\\U00002702-\\U000027B0\"\n                               u\"\\U000024C2-\\U0001F251\"\n                               \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n#named entity recognition\ndef name_entity(text):\n    import spacy\n    sp = spacy.load('en_core_web_sm')\n    sen = sp(text)\n    txt = []\n    entity = []\n    for i in sen.ents:\n            txt.append(i.text)\n            entity.append(i.label_)\n    import pandas as pd\n    df = pd.DataFrame({\"Text\":txt,\n                    \"Entity\":entity})\n    return df\n\ndef name_entity_list(text_list):\n    import spacy\n    sp = spacy.load('en_core_web_sm')\n    txt = []\n    entity = []\n    for i in text_list:\n        sen = sp(i)\n        for i in sen.ents:\n            txt.append(i.text)\n            entity.append(i.label_)\n    import pandas as pd\n    df = pd.DataFrame({\"Text\":txt,\n                      \"Entity\":entity})\n    return df\n\n\"\/--------------------------------------Tokenization------------------------------------\/\"\n#Tokenization\ndef token_words(txt):\n    import nltk\n    return nltk.word_tokenize(txt)\n\n#Remove stop words\ndef Token_and_removestopword(txt):\n    import nltk\n    from nltk.corpus import stopwords\n    stop_words = set(stopwords.words(\"english\"))\n    words = nltk.word_tokenize(txt)\n    without_stop_words = []\n    for word in words:\n        if word not in stop_words:\n            without_stop_words.append(word)\n    return without_stop_words\n\n#Lemmatization \ndef lemmatize_word(tokens,pos=\"v\"): \n    import nltk\n    from nltk.stem import WordNetLemmatizer\n    lemmatizer = WordNetLemmatizer()\n    # provide context i.e. part-of-speech \n    lemmas = [lemmatizer.lemmatize(word, pos =pos) for word in tokens] \n    return lemmas\n\n#Stemming\ndef stem_words(tokens): \n    import nltk\n    from nltk.stem.porter import PorterStemmer \n    stemmer = PorterStemmer()  \n    stems = [stemmer.stem(word) for word in tokens] \n    return stems \n\n#Part of speech tagging\u00a0\ndef pos_tagging(word_tokens):\n    import nltk\n    from nltk import pos_tag \n    pos_tag(word_tokens)\n    txt_gra = pos_tag(word_tokens)\n    #chunking function \n    # Input from Pos_tagging\n    text = []\n    grammer = []\n    for i in range(len(txt_gra)):\n        txt = txt_gra[i][0]\n        gram = txt_gra[i][1]\n        text.append(txt)\n        grammer.append(gram)\n    return  text, grammer \n\n     \ndef correct_spellings(tokens):\n    from spellchecker import SpellChecker\n    spell = SpellChecker()\n    misspelled_words = spell.unknown(tokens)\n    corrected_text = []\n    for word in tokens:\n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","3ea90a20":"#1-hot encoding model\ndef Onehotencoding(doc_list):\n    from numpy import array\n    from numpy import argmax\n    import pandas as pd\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    from sklearn.preprocessing import LabelEncoder\n    from sklearn.preprocessing import OneHotEncoder\n    values = array(toktotxt(doc_list))\n    label_encoder = LabelEncoder()\n    integer_encoded = label_encoder.fit_transform(values)\n    onehot_encoder = OneHotEncoder(sparse=False)\n    integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n    onehot_encoded = onehot_encoder.fit_transform(integer_encoded)    \n    def unique(lst): \n        x = np.array(lst) \n        return np.unique(x) \n    col = unique(values)\n    df = pd.DataFrame(onehot_encoded,index=values,columns=col)\n    return onehot_encoded\n\n#Bag-of-words model (Bow) [Countvectorizer] & N-grams language model\ndef countvectorizer(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import CountVectorizer\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = CountVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n#TF-IDF\ndef TFIDF(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = TfidfVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n#Word2vec language model\ndef word2vector(txt_int,word,min_count=1):\n    def texttotok(txt_int):\n        import nltk\n        if isinstance(txt_int[0], list):\n            tokens = txt_int\n        else:\n            tokens = nltk.word_tokenize(txt_int)\n        return tokens\n    tokens = []\n    for i in txt_int:\n        tok = texttotok(i)\n        tokens.append(tok)\n    #print(tokens)\n    from gensim.models import Word2Vec\n    model = Word2Vec(tokens,min_count=min_count)\n    index_lst = list(model.wv.vocab)\n    ary = model[word]\n    return ary\n\n#Hash Vectorizer\ndef Hashvect(train_int,test_int=None,Ngram_min=1,Ngram_max=1):\n    import pandas as pd\n    from sklearn.feature_extraction.text import HashingVectorizer\n    def toktotxt(txt_int):\n        if isinstance(txt_int[0], list):\n            text = txt_int.apply(lambda x :\" \".join(str(i) for i in x))\n        else:\n            text = txt_int\n        return text\n    train_txt = toktotxt(train_int)  \n    vectorizer = HashingVectorizer(ngram_range = (Ngram_min,Ngram_max))\n    vectorizer.fit(train_txt)\n    X = vectorizer.transform(train_txt)\n    train = X.toarray()\n    if test_int is None:\n        out = train\n    else:\n        test_txt = toktotxt(test_int)\n        Y = vectorizer.transform(test_txt)\n        test = Y.toarray()\n        out = train, test\n    return out\n\n#GloVe language model","a7021c5c":"#Component Analysis\n#Principal Component Analysis (PCA)\ndef PCA(X_train,Y_train=None,X_test=None,n=10):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=n)\n    X = pca.fit(X_train,y_train)\n    train = pca.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n#Independent Component Analysis (ICA)\ndef ICA(X_train,Y_train=None,X_test=None,n=100):\n    from sklearn.decomposition import FastICA\n    mod = FastICA(n_components=n)\n    X = mod.fit(X_train,y_train)\n    train = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n#Linear Discriminant Analysis (LDA)\ndef LDA(X_train,Y_train=None,X_test=None,n=100):\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    LDA = LinearDiscriminantAnalysis(n_components=n)\n    X = LDA.fit(X_train,y_train)\n    test = LDA.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n\n#Non-Negative Matrix Factorization (NMF)\ndef NMF(X_train,Y_train=None,X_test=None,n=100,init='random',random_state=0):\n    from sklearn.decomposition import NMF\n    mod = NMF(n_components=n, init=init, random_state=random_state)\n    X = mod.fit(X_train,y_train)\n    test = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n\n#Gaussian Random Projection\ndef gaurandpro(X_train,y_train=None,X_test=None):\n    from sklearn import random_projection\n    mod = random_projection.GaussianRandomProjection()\n    X = mod.fit(X_train,y_train)\n    test = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n\n#Sparse random projection\ndef sparandpro(X_train,y_train=None,X_test=None):\n    from sklearn import random_projection\n    mod = random_projection.SparseRandomProjection()\n    X = mod.fit(X_train,y_train)\n    test = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n\n#Johnson Lindenstrauss Lemma\ndef JLL(X_train,y_train=None,X_test=None,n=100,init='random'):\n    from sklearn.random_projection import johnson_lindenstrauss_min_dim\n    mod = NMF(n_components=n, init=init, random_state=0)\n    X = mod.fit(X_train,y_train)\n    test = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out\n\n#T- distributed Stochastic Neighbor Embedding (t-SNE)\ndef TSNE(X_train,y_train=None,X_test=None,n=100):\n    from sklearn.manifold import TSNE\n    mod = TSNE(n_components=n)\n    X = mod.fit(X_train,y_train)\n    test = mod.transform(X_train)\n    if X_test is None:\n        out = train\n    else:\n        test = pca.transform(X_test)\n        out = train, test\n    return out","905d6893":"#Rocchio Classification\ndef rocchio_clas(X_train, y_train,X_test):\n    from sklearn.neighbors.nearest_centroid import NearestCentroid\n    model = NearestCentroid()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Boosting\ndef boosting(X_train, y_train,X_test,n=20):\n    from sklearn.ensemble import GradientBoostingClassifier\n    model = GradientBoostingClassifier(n_estimators=n)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Bagging\ndef Bagging(X_train, y_train,X_test,n=20):\n    from sklearn.ensemble import BaggingClassifier\n    model = BaggingClassifier(n_estimators=n)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Logistic Regression\ndef logreg(X_train, y_train,X_test,n=20):\n    from sklearn.linear_model import LogisticRegression\n    model = LogisticRegression()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Naive Bayes Text Classification\n#MultinomialNB\ndef naivebiase_multinomial(X_train, y_train,X_test,n=20):\n    from sklearn.naive_bayes import MultinomialNB\n    model = MultinomialNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n\n#Gaussian\ndef naivebiase_gaussian(X_train, y_train,X_test,n=20):\n    from sklearn.naive_bayes import GaussianNB\n    model = GaussianNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n\n#Bernoullies\ndef naivebiase_gaussian(X_train, y_train,X_test,n=20):\n    from sklearn.naive_bayes import BernoulliNB\n    model = BernoulliNB()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n\n#K-Nearest Neighbors Algorithm (KNN)\ndef KNeighbors(X_train, y_train,X_test,n=2):\n    from sklearn.neighbors import KNeighborsClassifier\n    model = KNeighborsClassifier(n)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Support Vector Machine (SVM)\ndef SVM(X_train, y_train,X_test):\n    from sklearn.svm import LinearSVC\n    model = LinearSVC()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Decision Tree\ndef Decisiontree(X_train, y_train,X_test):\n    from sklearn import tree\n    model = tree.DecisionTreeClassifier()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n#Random Forests\ndef Randomforest(X_train, y_train,X_test):\n    from sklearn.ensemble import RandomForestClassifier\n    model = RandomForestClassifier()\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n\n#Different input formate for crf\n#Conditional Random Field (CRF)\ndef crf(X_train, y_train,X_test,alg='lbfgs'):\n    import sklearn_crfsuite\n    model = sklearn_crfsuite.CRF(algorithm=alg)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return preds\n\n#Deep Neural Networks (DNN)\n#Recurrent Neural Network (RNN)\n#Convolutional Neural Networks (CNN)\n#Deep Belief Network (DBN)\n#Hierarchical Attention Networks (HAN)","dbaf2455":"#Matthew correlation coefficient (MCC)\ndef MCC(y_true, y_pred):\n    from sklearn.metrics import matthews_corrcoef\n    return matthews_corrcoef(y_true, y_pred)\n\n#Recall\ndef recall(y_true, y_pred):\n    from sklearn.metrics import recall_score\n    return recall_score(y_true, y_pred)\n\n#Precision\ndef precision(y_true, y_pred):\n    from sklearn.metrics import precision_score\n    return precision_score(y_true, y_pred)\n\n#Accuracy\ndef accuracy(y_true, y_pred):\n    from sklearn.metrics import accuracy_score\n    return accuracy_score(y_true, y_pred)\n\n#F Score\ndef f_score(y_true, y_pred):\n    from sklearn.metrics import f1_score\n    return f1_score(y_true, y_pred)\n\n#Receiver Operating Characteristics (ROC)\ndef ROC(y_true, y_pred):\n    from sklearn.metrics import roc_curve\n    return roc_curve(y_true, y_pred)\n    \n#Area Under ROC Curve (AUC)\ndef AUC(y_true, y_pred):\n    from sklearn.metrics import roc_auc_score\n    return roc_auc_score(y_true, y_pred)","0546fbb1":"# Vectorization","e4384b00":"# Dimensional Reduction","bf5172aa":"# Data Cleaning","a04d379f":"# NLP Code Snippets","a28d94c3":"# Evaluation","65007b34":"# ML Algorithm"}}