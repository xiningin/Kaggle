{"cell_type":{"11392a13":"code","9accbd70":"code","0b27553c":"code","7510c445":"code","a81b01e9":"code","8e3f74c9":"code","1d31e276":"code","6ddda70b":"code","419889c2":"code","9420d811":"code","0809a324":"code","5806af28":"code","f668db9b":"code","fe8b61b0":"code","2f45d710":"code","62cf4ceb":"code","f970f502":"code","bc61f6fc":"code","74a2444c":"code","9a7ee851":"code","2c816a94":"code","7c8f4a63":"code","585f2dfe":"code","31d23e90":"code","e81d46b6":"code","0843949f":"code","24c1918e":"code","4d8b2dcf":"code","bcfd8fc9":"code","7adb4269":"code","af153b5a":"code","549f7015":"markdown","9fd6e5c1":"markdown","c5fb3e1e":"markdown","c698c2f3":"markdown","fe1e9f60":"markdown","0df7fc75":"markdown","c1e37095":"markdown","b54158c0":"markdown","449ea739":"markdown","ddbaaff1":"markdown","198d32aa":"markdown","b06529e3":"markdown"},"source":{"11392a13":"#Import Packages\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity='all' # shows outputs of all commands executed in 1 cell","9accbd70":"# Input data files are available in the \"..\/input\/\" directory.\n\n# List all files under the input directory\n\ninput_path = '\/kaggle\/input\/widsdatathon2020'\n\nfor dirpath, dirname, filenames in os.walk(input_path):\n    for name in filenames:\n        print (os.path.join(dirpath , name))\n        \n# Any results you write to the current directory are saved as output.","0b27553c":"# read file\nfname = 'training_v2.csv'\ntrain_df = pd.read_csv(os.path.join(input_path , fname))\n\nfname = 'unlabeled.csv'\ntest_df = pd.read_csv(os.path.join(input_path , fname))\n\nfname = 'solution_template.csv'\nsolution_df = pd.read_csv(os.path.join(input_path , fname))\n\n","7510c445":"print('solution_df')\nsolution_df.head() \nsolution_df.info()\nsolution_df.shape","a81b01e9":"solution_df['encounter_id'].describe()","8e3f74c9":"print('test_df')\ntest_df.head()\ntest_df.info()\ntest_df.shape\ntest_df['encounter_id'].describe()","1d31e276":"print('train_df')\ntrain_df.head() \ntrain_df.info()\ntrain_df.shape","6ddda70b":"train_df['hospital_death'].dtype\ntest_df['hospital_death'].dtype\n","419889c2":"def display_columns_properties(df):\n    for i, col in enumerate(df.columns.tolist()):\n         print('\\n ({} {})  Missing: {}  UniqValsSz: {}'.format(i,col, df[col].isnull().sum() ,df[col].unique().size))\n    print('\\n')","9420d811":"display_columns_properties(train_df)","0809a324":"display_columns_properties(test_df)","5806af28":"cat_train_df = train_df.select_dtypes(include='object')\ncat_train_df.head()\ncat_train_df.info()","f668db9b":"cat_test_df = test_df.select_dtypes(include='object')\ncat_test_df.head()\ncat_test_df.info()","fe8b61b0":"def display_columns_uniqvals(df):\n    for i, col in enumerate(df.columns.tolist()):\n         print('\\n ({} {}) Uniq: {} UniqSz: {}'.format(i,col, df[col].unique(),df[col].unique().size ))\n    print('\\n')","2f45d710":"display_columns_uniqvals(cat_test_df)","62cf4ceb":"# Get list of categorical variables\ns = (train_df.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)","f970f502":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n# Function for comparing different approaches\ndef score_dataset(X_train, X_valid, y_train, y_valid):\n    model = RandomForestRegressor(n_estimators=100, random_state=0)\n    model.fit(X_train, y_train)\n    preds = model.predict(X_valid)\n    return mean_absolute_error(y_valid, preds)","bc61f6fc":"from sklearn.model_selection import train_test_split\n\n# copy the data\ntrain = train_df.copy()\n\n\n# Select target\ny = train['hospital_death']\n\n\n# To keep things simple, we'll use only numerical predictors\npredictors = train.drop(['hospital_death','encounter_id','hospital_id','patient_id','icu_id','readmission_status','ethnicity'], axis=1)\n\n\n\nX = predictors.select_dtypes(exclude=['object'])\n\n\n\n# Divide data into training and validation subsets\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                      random_state=0)\n\nX_train.shape\nX_valid.shape\n","74a2444c":"\nfrom sklearn.impute import SimpleImputer\n\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_train = pd.DataFrame(my_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(my_imputer.transform(X_valid))\n\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n","9a7ee851":"'''\nfrom fancyimpute import KNN\nknn_imputer = KNN()\nimputed_X_train = pd.DataFrame(knn_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(knn_imputer.transform(X_valid))\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n'''","2c816a94":"'''\nfrom fancyimpute import IterativeImputer\nMICE_imputer = IterativeImputer()\n\nimputed_X_train = pd.DataFrame(MICE_imputer.fit_transform(X_train))\nimputed_X_valid = pd.DataFrame(MICE_imputer.transform(X_valid))\n# Imputation removed column names; put them back\nimputed_X_train.columns = X_train.columns\nimputed_X_valid.columns = X_valid.columns\n'''","7c8f4a63":"\n#display_columns_properties(imputed_X_train)","585f2dfe":"\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n\n# Define model. Specify a number for random_state to ensure same results each run.\ndt_model = DecisionTreeRegressor(random_state=1)\n\n# Fit model using Traing data\ndt_model.fit(imputed_X_train, y_train)\n\n# get predicted prices on validation data\npredicted_values = dt_model.predict(imputed_X_valid)\n\n# Find difference\nscore = mean_absolute_error(y_valid, predicted_values)\nprint('MAE:', score)","31d23e90":"\ntest = test_df.copy()\n\n#Separate target\ny_test = test['hospital_death']\n\n# To keep things simple, we'll use only numerical predictors\n\npredictors_test = test.drop(['hospital_death','encounter_id','hospital_id','patient_id','icu_id','readmission_status','ethnicity'], axis=1)\n\nX_test = predictors_test.select_dtypes(exclude=['object'])\n\n\n\nX_test.shape\nX_test.head()","e81d46b6":"\n# Imputation\nmy_imputer = SimpleImputer()\nimputed_X_test = pd.DataFrame(my_imputer.fit_transform(X_test))\n\n\n# Imputation removed column names; put them back\nimputed_X_test.columns = X_test.columns\n","0843949f":"'''\nMICE_imputer = IterativeImputer()\n\nimputed_X_test = pd.DataFrame(MICE_imputer.fit_transform(X_test))\n\n# Imputation removed column names; put them back\nimputed_X_test.columns = X_test.columns\n'''","24c1918e":"imputed_X_test.head()","4d8b2dcf":"\n'''\n# get predictions on test data\npreds = dt_model.predict(imputed_X_test)\n\n# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'encounter_id': test.encounter_id,\n                       'hospital_death': preds},dtype=np.int32)\n \noutput.to_csv('submission.csv', index=False)\noutput.head()\n'''","bcfd8fc9":"### Conclusion\n#Used Decision tree model, simple imputation and only numerical columns.","7adb4269":"from xgboost import XGBRegressor\nX_train = imputed_X_train\nX_valid = imputed_X_valid\n\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.05, n_jobs=4)\n\nxgb_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n\npredictions = xgb_model.predict(X_valid)\n\nprint(\"Mean Absolute Error: \" + str(mean_absolute_error(predictions, y_valid)))","af153b5a":"# get predictions on test data\npreds = xgb_model.predict(imputed_X_test)\n\n# Save predictions in format used for competition scoring\noutput_xgb = pd.DataFrame({'encounter_id': test.encounter_id,\n                       'hospital_death': preds},dtype=np.int32)\n \noutput_xgb.to_csv('submission_xgb1.csv', index=False)\noutput_xgb.head()","549f7015":"## Tables ","9fd6e5c1":"## Data Split\nWe need to split data into Training set(80%) and Validation set(20%).\nWe will use Validation set for prediction and deciding from it score which of numerous models\/approaches work better.","c5fb3e1e":"## Introduction\n\nThe challenge is to create a model that uses data from the first 24 hours of intensive care to predict patient survival. \n\n\nSo let us take first glimpse of data. \n\n\nLabeled training data are provided for model development. \n\n\nWe have to upload our predictions for unlabeled data to Kaggle.\n\nThis notebook uses XGBoost model to find solution. ","c698c2f3":"We want to make prediction using an ML Algorithm.\nFit o traing data, Predict on validation data and validation.\nWe also want to measure how algorithm performs using Mean Absolute Error.","fe1e9f60":"## Imputation\nIF we use Machine Learning on data having missing values, we will get Errors.\nTo handle that we have 2 approaches: \n\n1. Drop all columns with missing values. But it leads to data loss especially if there are a lot of missing values.\n\n2. Imputation - Fill some values. \n- For categorical, we can fill with lost frequent value for that column.\n- For numerical, we can fill with mean or median value for that column.\nNew values filled may be far away from what actual values should be.\n\nIt is of two types. \nSimple Imputation - fills some value and does not remember which all positions had been missing.\n\nImputation with extension - fills some value and remembers which all values aere missing. New columns are created to store which positions had missing values.\n\n","0df7fc75":"### Machine Learning ","c1e37095":"## Columns","b54158c0":"For supervised learning **classification** problem target variable is hospital_death. \n\n1. Solution_df has encounter_id and target variable( NaN). \nIt has same no of observations as test_df.\n\n2. test_df shows NaN for hospital_death for top 5 rows. \nThere are 39308 ** observations and **186** features. Out of 186 features, 170 are float and 8 are int. Rest 8 are object type.\n\n3. train_df shows 0 for hospital_death for first 5 rows.\nIt has 91713 observations and 186 features. Out of 186 features, 171 are float and 7 are int. Rest 8 are object type.\n\n4. From 2 and 3 above, it looks like 1 int of train has been changed to 1 float of test because of values of hospital_death target variable. Let us confirm the same.","449ea739":"### Encode Categoricals","ddbaaff1":"## Getting to know the data","198d32aa":"## Categorical columns","b06529e3":"Hope it is useful to you.     Please do UPVOTE if you like it.\n\n"}}