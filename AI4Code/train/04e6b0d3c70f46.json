{"cell_type":{"6a02c4b4":"code","d4e1a302":"code","272e2e33":"code","a2a87ed3":"code","7642dc31":"code","5e46ca68":"code","8f5a4c01":"code","d81b4d2f":"code","f4cef695":"code","8012d830":"code","0b7d761f":"code","f0a09aed":"code","474f4a9b":"code","cc27e584":"code","5e760d81":"code","140c787d":"code","79f78ae7":"code","1b0bb4ce":"code","09b5c1fc":"code","b3b0f2f3":"code","0daab7d2":"code","517a09bd":"code","6a91d7ac":"code","fd390fab":"code","dbd89f7c":"code","90cf6201":"code","a01dcb66":"code","1f57c94f":"code","4a66804d":"code","f17eb164":"code","f23f59ae":"code","3af8b067":"markdown","8845992f":"markdown","41ca3d58":"markdown","c0851743":"markdown","8c503530":"markdown","38b660da":"markdown","17665971":"markdown","5adb3620":"markdown","ace1d673":"markdown","6c0ffaf7":"markdown","aed6daec":"markdown","40fcbe70":"markdown","ba00b80d":"markdown","c91078cd":"markdown","9ce3baf2":"markdown","7e72dd5b":"markdown"},"source":{"6a02c4b4":"#References\n#https:\/\/www.groundai.com\/project\/environment-sound-classification-using-multiple-feature-channels-and-deep-convolutional-neural-networks\/1\n","d4e1a302":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os     \nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport librosa\nimport librosa.display\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import specgram\nfrom sklearn.model_selection import KFold\n\nimport tensorflow as tf\nfrom tensorflow import keras\n#!pip install python_speech_features\n%matplotlib inline\nplt.style.use('ggplot')\nimport glob\nimport glob\nimport librosa\nfrom librosa import feature\nimport numpy as np\nfrom pathlib import Path","272e2e33":"# Detect hardware, return appropriate distribution strategy\ndef get_strategy():\n    gpu = \"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        print('Running on TPU ', tpu.master())     \n    except ValueError:\n        tpu = None\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n        gpu = tf.config.list_physical_devices(\"GPU\")\n        if len(gpu) == 1:\n            print('Running on GPU ', gpu)\n    if tpu:\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        GCS_PATH = KaggleDatasets().get_gcs_path('birdsong-recognition')\n    elif len(gpu) == 1:\n        strategy = tf.distribute.OneDeviceStrategy(device=\"\/gpu:0\")\n        tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\":True})\n        GCS_PATH = \"\/kaggle\/input\/birdsong-recognition\/\"\n    else:\n        strategy = tf.distribute.get_strategy()\n        GCS_PATH = \"\/kaggle\/input\/birdsong-recognition\/\"\n\n    print(\"REPLICAS: \", strategy.num_replicas_in_sync)\n    base_dir = \"..\/input\/birdsong-recognition\/\"\n    print(base_dir)\n    return strategy, GCS_PATH, base_dir\n\nstrategy,GCS_PATH, base_dir = get_strategy()\nsns.set_palette(\"pastel\")\npalette = sns.color_palette()","a2a87ed3":"#Load sound file\ndef load_sound_files(file_paths):\n    raw_sounds = []\n    for fp in file_paths:\n        X,sr = librosa.load(fp)\n        raw_sounds.append(X)\n    return raw_sounds","7642dc31":"#EDA\n#from python_speech_features import mfcc\nfrom scipy.signal.windows import hann\ndef load_sample_files():\n    \n    list_class_names = np.array(class_list)[np.random.randint(0, len(class_list), 2)]\n    sample1 = train_data[train_data[\"ebird_code\"] == list_class_names[0]].sample(2)\n    sample2 = train_data[train_data[\"ebird_code\"] == list_class_names[1]].sample(2)\n\n    sound_file_paths1 = [base_dir + \"train_audio\/\" + list_class_names[0] + \"\/\" + file for file in sample1[\"filename\"].values]\n    sound_file_paths2 = [base_dir + \"train_audio\/\" + list_class_names[1] + \"\/\" + file for file in sample2[\"filename\"].values]\n    sound_file_paths = sound_file_paths1 + sound_file_paths2\n    sound_names = list(sample1[\"ebird_code\"].values) + list(sample2[\"ebird_code\"].values)\n    raw_sounds = load_sound_files(sound_file_paths)\n    return sound_names, raw_sounds\n\ndef plot_waves(sound_names,raw_sounds, plot_type):\n    i = 1\n    max_row = 2\n    max_col = 2\n    fig, ax = plt.subplots(max_row, max_col, figsize=(20,8))\n    row,col = 0,  0\n    c = palette[3]\n    \n    \n    n_mfcc = 13\n    n_mels = 40\n    n_fft = 512 \n    hop_length = 160\n    fmin = 0\n    fmax = None\n    sr = 22050\n\n\n    for n,f in zip(sound_names,raw_sounds):\n        if plot_type == \"mfcc\":\n            mfcc_librosa = librosa.feature.mfcc(y=f, sr=sr, n_fft=n_fft,\n                                        n_mfcc=n_mfcc, n_mels=n_mels,\n                                        hop_length=hop_length,\n                                        fmin=fmin, fmax=fmax, htk=False)\n\n        #mfcc_speech = mfcc(signal=f, samplerate=sr, winlen=n_fft \/ sr, winstep=hop_length \/ sr,\n        #                                   numcep=n_mfcc, nfilt=n_mels, nfft=n_fft, lowfreq=fmin, highfreq=fmax,\n        #                                  preemph=0.0, ceplifter=0, appendEnergy=False, winfunc=hann)\n        \n        #sns.heatmap(mfcc_librosa, ax=ax[row,col])\n            ax[row,col].plot(mfcc_librosa.T)\n        else:\n            librosa.display.waveplot(f,sr=22050, ax=ax[row,col], color=c)\n        ax[row,col].set_title(n)\n        col = col + 1\n        if col == max_col:\n            col = 0\n            row = row + 1\n            c = palette[0]\n    if plot_type == \"mfcc\":\n        plt.suptitle('Figure 1: Waveplot',x=0.5, y=0.915,fontsize=18)\n    else:\n        plt.suptitle('Figure 1: MFCC',x=0.5, y=0.915,fontsize=18)\n    plt.show()\n\n\n    \ndef group_n_plot():\n    group_data = train_data.groupby(\"ebird_code\").agg(num_audio=(\"filename\",\"count\"), tot_audio_length=(\"duration\",\"sum\"), median_audio_length=(\"duration\",\"median\"))\n    group_data = group_data.reset_index().reset_index()\n    \n    fig, ax= plt.subplots(1,3,figsize=(20,4))\n    sns.lineplot(x=\"index\", y=\"num_audio\", data=group_data, ax=ax[0], color=palette[0])\n    sns.lineplot(x=\"index\", y=\"median_audio_length\", data=group_data, ax=ax[1], color=palette[1])\n    sns.lineplot(x=\"index\", y=\"tot_audio_length\", data=group_data, ax=ax[2], color=palette[2])\n    ax[0].set_title(\"No Of Audios\");\n    ax[1].set_title(\"Median Audio Length\");\n    ax[2].set_title(\"Total Audio Length\");\n    return group_data\n\n","5e46ca68":"#Unused EDA MEthods\ndef plot_specgram(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,60))\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(len(sound_names),1,i)\n        specgram(np.array(f), Fs=22050)\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Figure 2: Spectrogram',x=0.5, y=0.915,fontsize=18)\n    plt.show()\n\ndef plot_log_power_specgram(sound_names,raw_sounds):\n    i = 1\n    fig = plt.figure(figsize=(25,60))\n    for n,f in zip(sound_names,raw_sounds):\n        plt.subplot(10,1,i)\n        D = librosa.logamplitude(np.abs(librosa.stft(f))**2, ref_power=np.max)\n        librosa.display.specshow(D,x_axis='time' ,y_axis='log')\n        plt.title(n.title())\n        i += 1\n    plt.suptitle('Figure 3: Log power spectrogram',x=0.5, y=0.915,fontsize=18)\n    plt.show()","8f5a4c01":"#Data Cleansing\ndef drop_excess_files(train_data, group_data):\n    median_audio_length = train_data[\"duration\"].median()\n    print(\"Median Audio Length\",median_audio_length)\n    train_data[\"deviation_from_median\"] = np.abs(train_data[\"duration\"] - median_audio_length)\n    train_data.sort_values([\"ebird_code\",\"deviation_from_median\"], inplace=True)\n    train_data[\"cum_duration\"] = train_data.groupby(\"ebird_code\")[\"duration\"].cumsum()\n    train_data[\"cum_duration\"] = train_data[\"cum_duration\"] - train_data[\"duration\"]\n    median_duration = group_data[\"tot_audio_length\"].median()\n    train_data[[\"ebird_code\",\"duration\",\"deviation_from_median\",\"cum_duration\"]].iloc[-100:].head(5)\n    train_data = train_data[train_data[\"cum_duration\"] <= median_duration]\n    return train_data","d81b4d2f":"#Feature Engineering - Not using this code as of now\nfn_list_i = [\n     feature.spectral_centroid,\n     feature.spectral_bandwidth,\n     feature.spectral_rolloff,\n     feature.melspectrogram,\n     feature.spectral_contrast\n]\n    \nfn_list_ii = [\n     feature.rms,\n     feature.zero_crossing_rate\n]\n\ndef parse_audio_files(file_name):\n    y, sr = librosa.load(file_name)\n    feat_vect_i = [ np.mean(funct(y,sr).T, axis=0) for funct in fn_list_i]\n    feat_vect_ii = [ np.mean(funct(y).T, axis=0) for funct in fn_list_ii] \n    stft = np.abs(librosa.stft(y))\n    chroma = [ np.mean(librosa.feature.chroma_stft(S=stft, sr=sr).T, axis=0) ]\n    mfccs = [ np.mean(librosa.feature.mfcc(y=y, sr=sr, n_mfcc=40).T, axis=0) ]\n    tonnetz = [ np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(y),sr=sr).T, axis=0) ]\n    features = feat_vect_i + feat_vect_ii + chroma + mfccs + tonnetz\n    return features\n","f4cef695":"#Feature Engineering\n\ndef extract_feature1(file_name, X=None, sample_rate=0):\n    global global_X\n    global global_sr\n    if X is None:\n        X, sample_rate = librosa.load(file_name)\n    stft = np.abs(librosa.stft(X))\n    \n    mfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T,axis=0)\n    chroma = np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n    mel = np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n    contrast = np.mean(librosa.feature.spectral_contrast(S=stft,sr=sample_rate).T,axis=0)\n    tonnetz = np.mean(librosa.feature.tonnetz(y=librosa.effects.harmonic(X),sr=sample_rate).T,axis=0)\n    return mfccs,chroma,mel,contrast,tonnetz\n\ndef parse_audio_files1(file_name):\n    try:\n        #print(file_name)\n        features = np.empty((0,193))\n        mfccs, chroma, mel, contrast,tonnetz = extract_feature1(file_name)\n        ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n        features = np.vstack([features,ext_features])\n        return features[0]\n    except:\n        return None\n    \ndef parse_audio_files2(X, sample_rate):\n    try:\n        #print(file_name)\n        features = np.empty((0,193))\n        mfccs, chroma, mel, contrast,tonnetz = extract_feature1(\"\",X, sample_rate)\n        ext_features = np.hstack([mfccs,chroma,mel,contrast,tonnetz])\n        features = np.vstack([features,ext_features])\n        return features[0]\n    except:\n        return None\n\nnum_features = 0\ndef process_class_name(class_name):\n    global num_features\n    #print(class_name)\n    feature_file_name = class_name + \"_features.csv\"\n    full_feature_file_name = \"\/kaggle\/input\/\" + \"birdcall\/\" + feature_file_name\n    if os.path.exists(full_feature_file_name):\n        #print(\"File found!\", full_feature_file_name)\n        features_df = pd.read_csv(full_feature_file_name)\n        num_features = len(features_df.columns.values)\n    else:\n        print(\"Not found:\", full_feature_file_name)\n        if 1==2:\n            df = pd.DataFrame({\"file_name\":os.listdir(base_dir + \"train_audio\/\" + class_name)})\n            df[\"class_name\"] = class_name\n            df[\"file_name\"] = df.apply(lambda row: base_dir + \"train_audio\/\" + row[\"class_name\"] + \"\/\" + row[\"file_name\"], axis=1)\n            df[\"features\"] = df[\"file_name\"].map(lambda x: parse_audio_files1(x))\n            num_features= len(df.head(1)[\"features\"].values[0])\n            features_df = df[[\"features\"]]\n            for i in range(num_features):\n                features_df[\"features_\" + str(i)] = features_df[\"features\"].map(lambda x: x[i] if x else None)\n            features = features_df.pop(\"features\")\n        return None\n    #features_df.to_csv(feature_file_name, index=False)\n    features_df[\"class_name\"] = class_name\n    features_df[\"file_name\"] = os.listdir(base_dir + \"train_audio\/\" + class_name)\n    features_df[\"file_name\"] = features_df.apply(lambda row: base_dir + \"train_audio\/\" + row[\"class_name\"] + \"\/\" + row[\"file_name\"], axis=1)\n\n    return features_df","8012d830":"import cv2","0b7d761f":"num_train_data_per_class = 1\nn_fft1 = int(0.0025 * 22050)\nhop_length1 = int(0.001 * 22050)\n\nn_fft2 = int(0.005 * 22050)\nhop_length2 = int(0.0025 * 22050)\n\nn_fft3 = int(0.01 * 22050)\nhop_length3 = int(0.005 * 22050)\nn_mels = 128\nfmin = 20\nfmax = 8000\n\ndef load_test_clip(path, start_time, duration=5):\n    #if os.path.exists(TEST_FOLDER):\n    return librosa.load(path, offset=start_time, duration=duration)\n\ndef get_audio_length(path):\n    data, sr = librosa.load(path)\n    return len(data),sr\n    \n    \n\ndef process_class_name_for_spectrogram(ebird_code, train_data):\n    int_ebird_code = dic_ebird_code[ebird_code]\n    df = train_data[train_data[\"int_ebird_code\"] == int_ebird_code][[\"ebird_code\", \"filename\", \"duration\", \"channels\"]]\n    num_files = df.shape[0]\n    for i in range(num_train_data_per_class):\n        int_file = np.random.randint(0,num_files, (1))\n        row = df.iloc[int_file]\n        print(row)\n        filename =row[\"filename\"].values[0]\n        duration = row[\"duration\"].values[0]\n        print(duration)\n        filepath = base_dir + \"train_audio\/\" + ebird_code + \"\/\" + filename\n        if duration == 5:\n            start_time=0\n        else:\n            start_time = np.random.randint(0,int(duration)-5, (1))\n        clip, sr = load_test_clip(filepath, start_time)\n        print(sr)\n        mel_spec1 = librosa.feature.melspectrogram(clip, n_fft=n_fft1, hop_length=hop_length1, n_mels=n_mels, sr=sr, power=1.0, fmin=fmin, fmax=fmax)\n        mel_spec_db1 = librosa.amplitude_to_db(mel_spec1, ref=np.max)\n        print(mel_spec_db1.shape)\n        \n        mel_spec2 = librosa.feature.melspectrogram(clip, n_fft=n_fft2, hop_length=hop_length2, n_mels=n_mels, sr=sr, power=1.0, fmin=fmin, fmax=fmax)\n        mel_spec_db2 = librosa.amplitude_to_db(mel_spec2, ref=np.max)\n        print(mel_spec_db2.shape)\n        \n        mel_spec3 = librosa.feature.melspectrogram(clip, n_fft=n_fft3, hop_length=hop_length3, n_mels=n_mels, sr=sr, power=1.0, fmin=fmin, fmax=fmax)\n        mel_spec_db3 = librosa.amplitude_to_db(mel_spec3, ref=np.max)\n        print(mel_spec_db3.shape)\n        \n        mel_spec1 = cv2.resize(mel_spec1, (224, 224))\n        mel_spec2 = cv2.resize(mel_spec2, (224, 224))\n        mel_spec3 = cv2.resize(mel_spec3, (224, 224))\n        mel_spec1 = mel_spec1*255\/mel_spec1.max()\n        mel_spec2 = mel_spec2*255\/mel_spec2.max()\n        mel_spec3 = mel_spec3*255\/mel_spec3.max()\n       \n        mel_spec = np.stack([mel_spec1, mel_spec2, mel_spec3], axis=-1)\n        print(mel_spec.shape)\n    \n    return df, mel_spec, mel_spec1, mel_spec2, mel_spec3","f0a09aed":"\ntrain_data = pd.read_csv(base_dir + \"train.csv\")\n\nebird_code_list = train_data[\"ebird_code\"].unique()\ndic_ebird_code = {k:v for v,k in enumerate(ebird_code_list)}\ntrain_data[\"int_ebird_code\"] = train_data[\"ebird_code\"].map(dic_ebird_code)\n\ndf, mel_spec, mel_spec1, mel_spec2, mel_spec3 = process_class_name_for_spectrogram(\"amecro\", train_data)\n","474f4a9b":"plt.imshow(mel_spec)","cc27e584":"def get_model(num_features, n_classes):\n    keras.backend.clear_session()\n    l1 = keras.layers.Input(shape=(num_features,), name=\"feature\")\n    l2 = keras.layers.Dense(2048, activation=\"tanh\")(l1)\n    l3 = keras.layers.Dense(1024, activation=\"tanh\")(l2)\n    l4 = keras.layers.Dense(512, activation=\"tanh\")(l3)\n    l5 = keras.layers.Dense(n_classes, activation = \"sigmoid\")(l4)\n    model = keras.models.Model(inputs={\"feature\":l1},outputs=l5)\n    return model","5e760d81":"class_list = os.listdir(base_dir+\"train_audio\/\")","140c787d":"train_data = pd.read_csv(base_dir + \"train.csv\")\ntrain_data.head(1)","79f78ae7":"group_data = group_n_plot()","1b0bb4ce":"train_data = drop_excess_files(train_data, group_data)\ngroup_n_plot()","09b5c1fc":"sound_names, raw_sounds =   load_sample_files()  \nplot_waves(sound_names,raw_sounds, \"\")","b3b0f2f3":"plot_waves(sound_names,raw_sounds, \"mfcc\")","0daab7d2":"with strategy.scope():\n    arr_df = []\n    from multiprocessing import Pool\n    p = Pool(2)\n    arr_df = p.map(process_class_name, class_list)\n    p.close()\n    p.join() \narr_df_new = []\nfor df in arr_df:\n    if df is not None:\n        arr_df_new.append(df)\nall_train_data = pd.concat(arr_df_new)\nall_train_data.head(1)","517a09bd":"class_list = list(all_train_data[\"class_name\"].unique())\nn_classes = all_train_data[\"class_name\"].unique().shape[0]\ndic_class_name = {k:v for v,k in enumerate(class_list)}\ndic_class_name_rev = {v:k for v,k in enumerate(class_list)}\nall_train_data[\"label\"] = all_train_data[\"class_name\"].map(dic_class_name)\nlabel_data = all_train_data.pop(\"label\")\nlabel_data = tf.keras.utils.to_categorical(label_data, n_classes)\n\nclass_name = all_train_data.pop(\"class_name\")\nfile_name = all_train_data.pop(\"file_name\")\n\nall_train_data = all_train_data.astype(np.float32)\nall_train_data.head(1)","6a91d7ac":"data = all_train_data.values\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(data)\ndata = scaler.transform(data)\nall_train_data[list(all_train_data.columns.values)] = data","fd390fab":"from sklearn.model_selection import train_test_split\ntrain_data, test_data, train_label, test_label = train_test_split(all_train_data, label_data, test_size=0.1, stratify=label_data)","dbd89f7c":"with strategy.scope():\n    model = get_model(193, n_classes)\n    model.compile(optimizer=keras.optimizers.Adam(1e-4),\n        loss=keras.losses.CategoricalCrossentropy(),\n        metrics=[\"accuracy\"])\n    model.summary()","90cf6201":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, shuffle=True)\n\nfor train_idx, val_idx in kf.split(range(train_data.shape[0])):\n    train_ds = tf.data.Dataset.from_tensor_slices(({\"feature\":train_data.values[train_idx].reshape(-1,193,)}, train_label[train_idx]))\n    train_ds = train_ds.shuffle(40000).batch(1024) \n    for data,label in train_ds.take(1):\n        print(data[\"feature\"].shape, label.shape)\n    \n    val_ds = tf.data.Dataset.from_tensor_slices(({\"feature\":train_data.values[val_idx].reshape(-1,193,)}, train_label[val_idx]))\n    val_ds = val_ds.batch(1024) \n    for data,label in val_ds.take(1):\n        print(data[\"feature\"].shape, label.shape)\n    model_history = model.fit(train_ds, epochs = 1, verbose = 1, validation_data=val_ds)","a01dcb66":"model.evaluate(test_data.values, test_label)","1f57c94f":"### Create predictions\ndef load_test_clip(path, start_time, duration=5):\n    #if os.path.exists(TEST_FOLDER):\n    return librosa.load(path, offset=start_time, duration=duration)\n    #else:\n    #    path = base_dir + \"train_audio\/aldfly\/XC134874.mp3\"\n    #    return librosa.load(path, offset=start_time, duration=duration)\n    \ndef make_prediction(block, sr):\n    split_file_data = pd.DataFrame({\"X\":[block]})\n    split_file_data[\"features\"] = split_file_data[\"X\"].map(lambda x: parse_audio_files2(x, sr))\n    test_feature_df = split_file_data[[\"features\"]]\n    for i in range(193):\n        test_feature_df[\"features_\" + str(i)] = test_feature_df[\"features\"].map(lambda x: x[i])\n    test_features = test_feature_df.pop(\"features\")\n    test_feature_data = scaler.transform(test_feature_df.values)\n    return list((model.predict(test_feature_data)>0.5).astype(int))[0]\n","4a66804d":"TEST_FOLDER = '..\/input\/birdsong-recognition\/test_audio\/'\ntest_info = pd.read_csv('..\/input\/birdsong-recognition\/test.csv')\ntest_info.head()\n","f17eb164":"try:\n    preds = []\n    for index, row in test_info.iterrows():\n        # Get test row information\n        site = row['site']\n        start_time = row['seconds'] - 5\n        row_id = row['row_id']\n        audio_id = row['audio_id']\n\n        # Get the test sound clip\n        if site == 'site_1' or site == 'site_2':\n            sound_clip, sr = load_test_clip(TEST_FOLDER + audio_id + '.mp3', start_time)\n        else:\n            sound_clip, sr = load_test_clip(TEST_FOLDER + audio_id + '.mp3', 0, duration=None)\n\n        # Make the prediction\n        pred = make_prediction(sound_clip, sr)\n\n        # Store prediction\n        preds.append([row_id, pred])\n\n    preds = pd.DataFrame(preds, columns=['row_id', 'pred'])\n    preds[\"pred2\"] = preds[\"pred\"].map(lambda x: [i for i in range(x.shape[0]) if x[i]>0])\n    preds[\"birds\"] = preds[\"pred2\"].map(lambda x: \" \".join(list(np.sort([dic_class_name_rev[i] for i in x]))))\n    preds[\"birds\"] = preds[\"birds\"].map(lambda x: \"nocall\" if x==\"\" else x)\n\n\n    preds[[\"row_id\",\"birds\"]].to_csv('submission.csv', index=False)\nexcept:\n    preds = pd.read_csv('..\/input\/birdsong-recognition\/sample_submission.csv')\n    preds[[\"row_id\",\"birds\"]].to_csv('submission.csv', index=False)\n","f23f59ae":"if 1==2:\n    fig = plt.figure()\n    ax = fig.gca(projection='3d')\n\n    ax.plot(data_broken[:,0], data_broken[:,1], data_broken[:,2],lw=0.5)\n    ax.set_xlabel(\"X Axis\")\n    ax.set_ylabel(\"Y Axis\")\n    ax.set_zlabel(\"Z Axis\")\n    ax.set_title(\"Lorenz Attractor\")\n\n\n    np.fft.fft(data_broken).real","3af8b067":"### Normalize features","8845992f":"### Detect Hardware and accordingly set strategy","41ca3d58":"### Get feature and label from data frame","c0851743":"### Split data into train and test","8c503530":"### Define Model","38b660da":"### Feature Engineering\n* Let us see what a mp3 file looks like. A wave will have values with time. Will be good to plot this.\n* Let us see what a fourier transform adjusted to mel scale looks like. \n* Most literature uses melspectrogram for the sound file as the image feature set and then regular cnn layers based models are used for classification.\n* One literature suggested using muliple features returned by librosa for the model and this we will use as our baseline model.\n* So let us get a sense of these features by doing some EDA","17665971":"### Execute the pipeline","5adb3620":"### EDA Helper Routines\n* How many classes are there and for each class what is the average length of recording, total recording time and number of recording","ace1d673":"### Model 1: This model gave result of 0.005. ","6c0ffaf7":"### Import Required Libraries","aed6daec":"### Build Model","40fcbe70":"### For each class get features","ba00b80d":"### Load Audio File","c91078cd":"### Spectrogram Features","9ce3baf2":"### Training The Model\nDepending on features, we will train the model on TPU\/GPU or CPU","7e72dd5b":"### Data Cleansing\n#### Balance audio length\n* One thing we can do is to drop excess audio files for birds."}}