{"cell_type":{"19b4910a":"code","4890dcdb":"code","5f30f717":"code","e88d7fed":"code","5bedce17":"code","b50553fa":"code","fc242388":"code","9668fcc4":"code","e10ff980":"code","c5ce73b3":"code","3d91c796":"code","5ad61d33":"code","477fa426":"code","8dce5467":"code","37717d55":"code","89c50e52":"code","824e8c1d":"code","61e9514b":"code","e7255e43":"code","0081e885":"code","733e8bec":"code","d4066e85":"code","1641e970":"code","2a73f1b2":"code","2069fb98":"code","c4786e0e":"code","fb68c801":"code","d09d3e45":"code","facb6b28":"code","457231e5":"code","bc7b0f80":"code","914f4329":"code","1864a318":"code","e2d2dec8":"code","c473fed6":"code","36ec94f4":"code","0cd3aa13":"code","b1ef97e1":"code","aecfa2b2":"code","1b0a5a5d":"code","0c253c1e":"code","138d6897":"code","81b70d9b":"code","0857b1a8":"code","792fa776":"code","c8b3d396":"code","dd03551d":"code","cd4caa39":"code","4118ee11":"code","eeb69100":"code","e9ab9759":"code","d2b2e5a9":"code","883dd41a":"code","851b407c":"code","02786379":"code","c5e4e30f":"markdown","30e53934":"markdown","38f0af20":"markdown","00e05df6":"markdown","434694a0":"markdown","6499d582":"markdown","368a9517":"markdown","ce62d0f0":"markdown","1641a266":"markdown","13a084ed":"markdown","1dadf7ab":"markdown","b8854585":"markdown","9a3a91e4":"markdown","c3626784":"markdown","fcad3d7d":"markdown","d8cf91e9":"markdown","3526ceec":"markdown","52f994a4":"markdown","e2c1ef53":"markdown","a4231f85":"markdown"},"source":{"19b4910a":"#IMPORT THE LIBRARIES\nimport pandas as pd\nimport numpy as np\n\nimport os\nimport sys\n\n# librosa is a Python library for analyzing audio and music. It can be used to extract the data from the audio files we will see it later.\nimport librosa\nimport librosa.display\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.model_selection import train_test_split\n\n# to play the audio files\nimport IPython.display as ipd\nfrom IPython.display import Audio\n\nimport keras\nfrom keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Embedding\nfrom keras.layers import LSTM,BatchNormalization\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.layers import Input, Flatten, Dropout, Activation\nfrom keras.layers import Conv1D, MaxPooling1D, AveragePooling1D\nfrom keras.models import Model\nfrom keras.callbacks import ModelCheckpoint\n\nimport warnings\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) ","4890dcdb":"# path to the directory\nRAV = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/\"","5f30f717":"dir_list = os.listdir(RAV)\ndir_list.sort()\n\nemotion = []\ngender = []\npath = []\nfor i in dir_list:\n    fname = os.listdir(RAV + i)\n    for f in fname:\n        part = f.split('.')[0].split('-')\n        emotion.append(int(part[2]))\n        temp = int(part[6])\n        if temp%2 == 0:\n            temp = \"female\"\n        else:\n            temp = \"male\"\n        gender.append(temp)\n        path.append(RAV + i + '\/' + f)\n\n        \nRAV_df = pd.DataFrame(emotion)\nRAV_df = RAV_df.replace({1:'neutral', 2:'calm', 3:'happy', 4:'sad', 5:'angry', 6:'fear', 7:'disgust', 8:'surprise'})\nRAV_df = pd.concat([pd.DataFrame(gender),RAV_df],axis=1)\nRAV_df.columns = ['gender','emotion']\nRAV_df['labels'] =RAV_df.gender + '_' + RAV_df.emotion\nRAV_df['source'] = 'RAVDESS'  \nRAV_df = pd.concat([RAV_df,pd.DataFrame(path, columns = ['path'])],axis=1)\nRAV_df = RAV_df.drop(['gender', 'emotion'], axis=1)\nRAV_df.labels.value_counts()","e88d7fed":"plt.figure(figsize=(12, 5))\nplt.title('Count of Emotions', size=16)\nsns.countplot(RAV_df.labels)\nplt.ylabel('Count', size=12)\nplt.xlabel('Emotions', size=12)\nplt.xticks(rotation=45)\nsns.despine(top=True, right=True, left=False, bottom=False)\nplt.show()","5bedce17":"# MALE NEUTRAL\nfname1=RAV+'Actor_01\/03-01-01-01-01-01-01.wav'\ndata, sr = librosa.load(fname1)\nipd.Audio(fname1) \n","b50553fa":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Male Neutral')\nplt.colorbar(format='%+2.0f dB');","fc242388":"# FEMALE NEUTRAL\nfname2=RAV+'Actor_14\/03-01-01-01-01-01-14.wav'\ndata, sr = librosa.load(fname2)\nipd.Audio(fname2) ","9668fcc4":"# CREATE LOG MEL SPECTROGRAM\nplt.figure(figsize=(10, 5))\nspectrogram = librosa.feature.melspectrogram(y=data, sr=sr, n_mels=128,fmax=8000) \nspectrogram = librosa.power_to_db(spectrogram)\nlibrosa.display.specshow(spectrogram, y_axis='mel', fmax=8000, x_axis='time');\nplt.title('Mel Spectrogram - Female Neutral')\nplt.colorbar(format='%+2.0f dB');","e10ff980":"# Pick a fearful track\nfname3 = RAV + 'Actor_14\/03-01-06-02-02-02-14.wav'  \ndata, sr = librosa.load(fname3)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Fearful')\n# Lets play the audio \nipd.Audio(fname3)","c5ce73b3":"# Pick a happy track\nfname4 = RAV + 'Actor_14\/03-01-03-02-02-02-14.wav'  \ndata, sr = librosa.load(fname4)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(data, sr=sr)\nplt.title('Waveplot - Female Happy')\n\n# Lets play the audio \nipd.Audio(fname4)","3d91c796":"# Gender - Female; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()","5ad61d33":"# Gender - Male; Emotion - Angry \npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmfcc = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\n\n\n# MFCC\nplt.figure(figsize=(16, 10))\nplt.subplot(3,1,1)\nlibrosa.display.specshow(mfcc, x_axis='time')\nplt.ylabel('MFCC')\nplt.colorbar()\n\nipd.Audio(path)","477fa426":"# Gender - Female; Emotion - angry\npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_08\/03-01-05-02-01-01-08.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nfemale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nfemale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(female))\n\n# Gender - Male; Emotion - angry\npath = \"\/kaggle\/input\/ravdess-emotional-speech-audio\/audio_speech_actors_01-24\/Actor_09\/03-01-05-01-01-01-09.wav\"\nX, sample_rate = librosa.load(path, res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)  \nmale = librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13)\nmale = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13), axis=0)\nprint(len(male))\n\n# Plot the two audio waves together\nplt.figure(figsize=(16,10))\nplt.subplot(3,1,1)\nplt.plot(female, label='female')\nplt.plot(male, label='male')\nplt.legend()","8dce5467":"# NOISE\ndef noise(data):\n    noise_amp = 0.035*np.random.uniform()*np.amax(data)\n    data = data + noise_amp*np.random.normal(size=data.shape[0])\n    return data\n# STRETCH\ndef stretch(data, rate=0.8):\n    return librosa.effects.time_stretch(data, rate)\n# SHIFT\ndef shift(data):\n    shift_range = int(np.random.uniform(low=-5, high = 5)*1000)\n    return np.roll(data, shift_range)\n# PITCH\ndef pitch(data, sampling_rate, pitch_factor=0.7):\n    return librosa.effects.pitch_shift(data, sampling_rate, pitch_factor)","37717d55":"# Trying different functions above\npath = np.array(RAV_df['path'])[303]\ndata, sample_rate = librosa.load(path)","89c50e52":"# NORMAL AUDIO\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=data, sr=sample_rate)\nAudio(path)","824e8c1d":"# AUDIO WITH NOISE\nx = noise(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","61e9514b":"# STRETCHED AUDIO\nx = stretch(data)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","e7255e43":"# SHIFTED AUDIO\nx = shift(data)\nplt.figure(figsize=(12,5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","0081e885":"# AUDIO WITH PITCH\nx = pitch(data, sample_rate)\nplt.figure(figsize=(12, 5))\nlibrosa.display.waveplot(y=x, sr=sample_rate)\nAudio(x, rate=sample_rate)","733e8bec":"\ndef feat_ext(data):\n    mfcc = np.mean(librosa.feature.mfcc(y=data, sr=sample_rate).T, axis=0)\n    return mfcc\n\ndef get_feat(path):\n    data, sample_rate = librosa.load(path, duration=2.5, offset=0.6)\n    # normal data\n    res1 = feat_ext(data)\n    result = np.array(res1)\n    #data with noise\n    noise_data = noise(data)\n    res2 = feat_ext(noise_data)\n    result = np.vstack((result, res2))\n    #data with stretch and pitch\n    new_data = stretch(data)\n    data_stretch_pitch = pitch(new_data, sample_rate)\n    res3 = feat_ext(data_stretch_pitch)\n    result = np.vstack((result, res3))\n    return result","d4066e85":"RAV_df.head()","1641e970":"X, Y = [], []\nfor path, emotion in zip(RAV_df['path'], RAV_df['labels']):\n    feature = get_feat(path)\n    for ele in feature:\n        X.append(ele)\n        Y.append(emotion)","2a73f1b2":"Features = pd.DataFrame(X)\nFeatures['labels'] = Y\nFeatures.to_csv('features.csv', index=False)\nFeatures.head()","2069fb98":"# can use this directly from saved feature .csv file\nFeatures = pd.read_csv('..\/input\/features\/features.csv')\nFeatures.head()","c4786e0e":"X = Features.iloc[: ,:-1].values\nY = Features['labels'].values","fb68c801":"# As this is a multiclass classification problem onehotencoding our Y.\nencoder = OneHotEncoder()\nY = encoder.fit_transform(np.array(Y).reshape(-1,1)).toarray()","d09d3e45":"# splitting data\nx_train, x_test, y_train, y_test = train_test_split(X, Y, random_state=0, shuffle=True)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","facb6b28":"# scaling our data with sklearn's Standard scaler\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.transform(x_test)\nx_train.shape, y_train.shape, x_test.shape, y_test.shape","457231e5":"from sklearn.neighbors import KNeighborsClassifier\nclf1=KNeighborsClassifier(n_neighbors=3)\nclf1.fit(x_train,y_train)","bc7b0f80":"y_pred=clf1.predict(x_test)","914f4329":"print(\"Training set score: {:.3f}\".format(clf1.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf1.score(x_test, y_test)))","1864a318":"from sklearn.neural_network import MLPClassifier\nclf2=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=400)\nclf2.fit(x_train,y_train)","e2d2dec8":"y_pred=clf2.predict(x_test)","c473fed6":"print(\"Training set score: {:.3f}\".format(clf2.score(x_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(clf2.score(x_test, y_test)))","36ec94f4":"x_traincnn =np.expand_dims(x_train, axis=2)\nx_testcnn= np.expand_dims(x_test, axis=2)\nx_traincnn.shape, y_train.shape, x_testcnn.shape, y_test.shape","0cd3aa13":"model = Sequential()\nmodel.add(Conv1D(2048, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(1024, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv1D(512, kernel_size=5, strides=1, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2, strides = 2, padding = 'same'))\nmodel.add(BatchNormalization())\n\nmodel.add(LSTM(256, return_sequences=True))\n\nmodel.add(LSTM(128))\n\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\n\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(16, activation='softmax'))\n\noptimiser = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer=optimiser,\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\nmodel.summary()","b1ef97e1":"history = model.fit(x_traincnn, y_train, batch_size=64, epochs=200, validation_data=(x_testcnn, y_test))","aecfa2b2":"print(\"Accuracy of our model on test data : \" , model.evaluate(x_testcnn,y_test)[1]*100 , \"%\")","1b0a5a5d":"epochs = [i for i in range(200)]\nfig , ax = plt.subplots(1,2)\ntrain_acc = history.history['accuracy']\ntrain_loss = history.history['loss']\ntest_acc = history.history['val_accuracy']\ntest_loss = history.history['val_loss']\n\nfig.set_size_inches(20,6)\nax[0].plot(epochs , train_loss , label = 'Training Loss')\nax[0].plot(epochs , test_loss , label = 'Testing Loss')\nax[0].set_title('Training & Testing Loss')\nax[0].legend()\nax[0].set_xlabel(\"Epochs\")\n\nax[1].plot(epochs , train_acc , label = 'Training Accuracy')\nax[1].plot(epochs , test_acc , label = 'Testing Accuracy')\nax[1].set_title('Training & Testing Accuracy')\nax[1].legend()\nax[1].set_xlabel(\"Epochs\")\nplt.show()","0c253c1e":"pred_test = model.predict(x_testcnn)\ny_pred = encoder.inverse_transform(pred_test)\n\ny_test = encoder.inverse_transform(y_test)\n\ndf = pd.DataFrame(columns=['Predicted Labels', 'Actual Labels'])\ndf['Predicted Labels'] = y_pred.flatten()\ndf['Actual Labels'] = y_test.flatten()\n\ndf.head(10)","138d6897":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nplt.figure(figsize = (12, 10))\ncm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\nsns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","81b70d9b":"model_name = 'Emotion_Voice_Detection_Model.h5'\nsave_dir = os.path.join(os.getcwd(), 'saved_models')\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)","0857b1a8":"import json\nmodel_json = model.to_json()\nwith open(\"model.json\", \"w\") as json_file:\n    json_file.write(model_json)","792fa776":"from keras.models import model_from_json\njson_file = open('model.json', 'r')\nloaded_model_json = json_file.read()\njson_file.close()\nloaded_model = model_from_json(loaded_model_json)\n# load weights into new model\nloaded_model.load_weights(\"\/kaggle\/working\/saved_models\/Emotion_Voice_Detection_Model.h5\")\nprint(\"Loaded model from disk\")","c8b3d396":"data, sampling_rate = librosa.load('..\/input\/output\/output10.wav')","dd03551d":"% pylab inline\nimport os\nimport pandas as pd\nimport librosa\nimport glob ","cd4caa39":"plt.figure(figsize=(15, 5))\nlibrosa.display.waveplot(data, sr=sampling_rate)","4118ee11":"#livedf= pd.DataFrame(columns=['feature'])\nX, sample_rate = librosa.load('..\/input\/output\/output10.wav', res_type='kaiser_fast',duration=2.5,sr=22050*2,offset=0.5)\nsample_rate = np.array(sample_rate)\nmfccs = np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=13),axis=0)\nfeaturelive = mfccs\nlivedf2 = featurelive","eeb69100":"livedf2= pd.DataFrame(data=livedf2)\nlivedf2 = livedf2.stack().to_frame().T\nlivedf2","e9ab9759":"twodim= np.expand_dims(livedf2, axis=2)","d2b2e5a9":"livepreds = loaded_model.predict(twodim, \n                         batch_size=32, \n                         verbose=1)","883dd41a":"livepreds","851b407c":"livepreds.shape","02786379":"livepredictions = (encoder.inverse_transform((livepreds)))\nlivepredictions","c5e4e30f":"## **CNN**","30e53934":"After listening to all augmented audio, it's decided to use noise, stretch and pitch for augmenting data.","38f0af20":"For the happy track it actually felt like it was a fearful tone at the start, up until the end. We had to play it 3 or 4 times to finally be convienced that it is indeed a happy sound. Looking at the wave plot between the 2 files, we noticed the only difference is the amplitute wherein this happy track has a higher amplituted at various points.\n\n\nNext, we compare the MFCC feature for male and female angry audio clips","00e05df6":"**PROBLEM STATEMENT**\n\nThe purpose is to recognize the emotion and affective state of the speaker from his\/her speech signal. ","434694a0":"# DATA AUGMENTATION\n\n* Data augmentation is the process by which we create new synthetic data samples by adding small perturbations on our initial training set.\n* To generate syntactic data for audio, we can apply noise injection, shifting time, changing pitch and speed.\n* The objective is to make our model invariant to those perturbations and enhace its ability to generalize.\nIn order to this to work adding the perturbations must conserve the same label as the original training sample.","6499d582":"## **SAVING THE MODEL**","368a9517":"# FEATURE EXTRACTION","ce62d0f0":"Now, we will compare mel spectograms of male and female neutral audio clips.","1641a266":"# INTRODUCTION","13a084ed":"# EXPLORATORY DATA ANALYSIS\n\nThe key features of the audio data are namely, MFCC (Mel Frequency Cepstral Coefficients), Mel Spectrogram and Chroma.\n\n* MFCC (Mel Frequency Cepstral Coefficients)- \nMFCC is taken on a Mel scale which is a scale that relates the perceived frequency of a tone to the actual measured frequency. It scales the frequency in order to match more closely what the human ear can hear. The envelope of the temporal power spectrum of the speech signal is representative of the vocal tract and MFCC accurately represents this envelope.\n\n\n* Mel Spectrogram- \nA Fast Fourier Transform is computed on overlapping windowed segments of the signal, and we get what is called the spectrogram. This is just a spectrogram that depicts amplitude which is mapped on a Mel scale.\n\n* Chroma- \nA Chroma vector is typically a 12-element feature vector indicating how much energy of each pitch class is present in the signal in a standard chromatic scale.\n\nFor the EDA we have used MFCC and Mel Spectogram","1dadf7ab":"# **SPEECH EMOTION RECOGNITION**\n\n\n**TABLE OF CONTENTS**\n* INTRODUCTION\n* EXPLORATORY DATA ANALYSIS (EDA)\n* DATA AUGMENTATION\n* FEATURE EXTRACTION\n* MODEL","b8854585":"For the same sentence being uttered, there is a clear distint difference between male and female in that females tends to have a higher pitch.","9a3a91e4":"# DATA PREPROCESSING","c3626784":"# LIVE DEMO","fcad3d7d":"Since we got the best accuracy with the cnn model we will further be using it.","d8cf91e9":"Next, we compare the waveplots of happy and fearful tracks","3526ceec":"# IMPLOYING MODELS","52f994a4":"## **KNN**","e2c1ef53":"## **MLP CLASSIFIER**","a4231f85":"**DATA SOURCE USED**\n\nWe have used the RAVDESS dataset in this project.It is one of the more common dataset used for this excercise by others. It's well liked because of its quality of speakers, recording and it has 24 actors of different genders.\nHere's the filename identifiers as per the official RAVDESS website:\n\n* Modality (01 = full-AV, 02 = video-only, 03 = audio-only).\n* Vocal channel (01 = speech, 02 = song).\n* Emotion (01 = neutral, 02 = calm, 03 = happy, 04 = sad, 05 = angry, 06 = fearful, 07 = disgust, 08 = surprised).\n* Emotional intensity (01 = normal, 02 = strong). NOTE: There is no strong intensity for the 'neutral' emotion.\n* Statement (01 = \"Kids are talking by the door\", 02 = \"Dogs are sitting by the door\").\n* Repetition (01 = 1st repetition, 02 = 2nd repetition).\n* Actor (01 to 24. Odd numbered actors are male, even numbered actors are female).\n\nHere's an example of an audio filename. 02-01-06-01-02-01-12.mp4"}}