{"cell_type":{"b247b6a4":"code","c3ed7728":"code","276030bd":"code","4167ca5e":"code","7e103157":"code","74863829":"code","f92c7533":"code","dc70e52c":"code","94c2e1af":"code","e5b5ec3e":"code","f53af6e2":"code","74352ba6":"code","3cf2cf79":"code","52cf2df3":"code","a7f15270":"code","870de287":"markdown","18770d88":"markdown","4c09d21d":"markdown","e810ac9e":"markdown","ae250c6a":"markdown","a85ee7fc":"markdown","74066651":"markdown","5c1d238a":"markdown","80ab5566":"markdown","795b792d":"markdown","4c0fd3fa":"markdown","c3c78f1b":"markdown","36238785":"markdown","23a92ef1":"markdown","1a641d88":"markdown","286dfa9e":"markdown","cad253e3":"markdown","a3a80a40":"markdown","936e5bd4":"markdown","9ca9f42a":"markdown","8f070c78":"markdown"},"source":{"b247b6a4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.optimizers import *\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import GridSearchCV\nimport pprint\npp = pprint.PrettyPrinter(indent = 4)","c3ed7728":"import os\nprint(os.listdir(\"..\/input\"))","276030bd":"train = \"..\/input\/fashion-mnist_train.csv\"\ntest  = \"..\/input\/fashion-mnist_test.csv\"","4167ca5e":"def read_dataset(data_file):\n    df = pd.read_csv(data_file)\n    label_column = 'label'\n    y = df[label_column].values\n    X = df.drop(label_column, axis=1).values\n    return (X, y)","7e103157":"(x_train, y_train) = read_dataset(train)\n(x_test, y_test) = read_dataset(test)","74863829":"print(\"There are {} fashion images with {} pixels in x_train dataset. \\n\".format(x_train.shape[0],x_train.shape[1]))\n\nprint(\"There are {} fashion Labels in y_train dataset. \\n\".format(y_train.shape[0]))\n\nprint(\"There are {} fashion images with {} pixels in x_train dataset. \\n\".format(x_test.shape[0],x_test.shape[1]))\n\nprint(\"There are {} fashion Labels in y_test dataset. \\n\".format(y_test.shape[0]))","f92c7533":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(x_train[i].reshape(28, 28))\nplt.show()","dc70e52c":"def draw_articles(articles, labels):\n    fig, axs = plt.subplots(1, len(articles), figsize=(30,30))\n    for i in range(len(articles)):\n        axs[i].set_title(labels[i])\n        axs[i].imshow(articles[i].reshape((28,28)), cmap=plt.cm.binary)\n    plt.show()","94c2e1af":"def ImageDisplay(list_data, label, one_hot=False):\n    fig = pyplot.figure()\n    axis = fig.add_subplot(1,1,1)\n    list_data=np.reshape(list_data, (28,28))\n    plot_img = axis.imshow(list_data, cmap=mpl.cm.Greys)\n    plot_img.set_interpolation('none')\n    if one_hot :\n        ShowLabelName (label)\n    else:\n        print (\"Label : \"+str(CLASSES[str(label)]))","e5b5ec3e":"label_map = {0: 'T-Shirt\/Top', 1: 'Trouser', 2: 'Pullover', 3: 'Dress', 4: 'Coat', 5: 'Sandal', 6: 'Shirt', 7: 'Sneaker', 8: 'Bag', 9: 'Ankle boot'}\nexamples = []\nlabels = []\n\nfor i in label_map:\n    k = np.where(y_train==i)[0][0]\n    examples.append(x_train[k])\n    labels.append(label_map[i])\ndraw_articles(examples, labels)","f53af6e2":"x_train = x_train.astype('float32') \/ 255\ny_train = to_categorical(y_train)\n\nx_test = x_test.astype('float32') \/ 255\ny_test = to_categorical(y_test)","74352ba6":"def build_model(optimizer, learning_rate, activation, dropout_rate,\n                initializer,num_unit):\n    keras.backend.clear_session()\n    model = Sequential()\n    model.add(Dense(num_unit, kernel_initializer=initializer,\n                    activation=activation, input_shape=(784,)))\n    model.add(Dropout(dropout_rate))\n    model.add(Dense(num_unit, kernel_initializer=initializer,\n                    activation=activation))\n    model.add(Dropout(dropout_rate)) \n    model.add(Dense(10, activation='softmax'))\n    model.compile(loss='categorical_crossentropy',\n                  optimizer=optimizer(lr=learning_rate),\n                  metrics=['accuracy'])\n    return model","3cf2cf79":"# [:1] is for testing\n\nbatch_size = [20, 50, 100][:1]\n\nepochs = [1, 20, 50][:1]\n\ninitializer = ['lecun_uniform', 'normal', 'he_normal', 'he_uniform'][:1]\n\nlearning_rate = [0.1, 0.001, 0.02][:1]\n\ndropout_rate = [0.3, 0.2, 0.8][:1]\n\nnum_unit = [10, 5][:1]\n\nactivation = ['relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'linear'][:1]\n\noptimizer = [SGD, RMSprop, Adagrad, Adadelta, Adam, Adamax, Nadam][:1]","52cf2df3":"# Creat the wrapper and pass params to GridSearchCV \n# parameters is a dict with all values\n\nparameters = dict(batch_size = batch_size,\n                  epochs = epochs,\n                  dropout_rate = dropout_rate,\n                  num_unit = num_unit,\n                  initializer = initializer,\n                  learning_rate = learning_rate,\n                  activation = activation,\n                  optimizer = optimizer)\n\nmodel = KerasClassifier(build_fn=build_model, verbose=0)\n\nmodels = GridSearchCV(estimator = model, param_grid=parameters, n_jobs=1)","a7f15270":"best_model = models.fit(x_train, y_train)\nprint('Best model :')\npp.pprint(best_model.best_params_)","870de287":"# Hyper-parameter Optimization Using Keras on Fashion MNIST","18770d88":"# Overview of the Training and Test Datasets","4c09d21d":"# How they look like?","e810ac9e":"* Here we are going to standardize all those pixel-values from 0-255 to 0-1. This will allow us to increase the performance of model calculations.\n\n* By the other hand, we will change the labels (our y variable) from integers to categorical variables","ae250c6a":"Now the intention is to test all the posible combinations between the hyper-parameters that we have defined in order to determine the optimal values that will enhance the performance of our model, letting us know the best set up for them.","a85ee7fc":"# Standardization and Categorization","74066651":"**Fashion-MNIST** is a dataset of Zalando's article images consisting of a **training** set of **60,000** examples and a **test** set of **10,000** examples. Each example is a **28x28** **grayscale** **image** for a total of **784 pixels**. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This **pixel-value** is an integer **between 0 and 255**.\n\nEach image is associated with a label from 10 classes of clothes:\n\n* **0** T-shirt\/top\n* **1** Trouser\n* **2** Pullover\n* **3** Dress\n* **4** Coat\n* **5** Sandal\n* **6** Shirt\n* **7** Sneaker\n* **8** Bag\n* **9** Ankle boot","5c1d238a":"We are going to assign the values that each hyper-parameter can take.","80ab5566":"# Reading the Training and Test Datasets","795b792d":"# Model Wrapper and GridSearch","4c0fd3fa":"# Objective","c3c78f1b":"The previous result let us see that this is the optimal configuration considering the given hypper-parameter values:\n\n* Activation Function = relu\n* Batch Size = 20\n* Dropout Rate = 0.3\n* epochs = 1\n* Initializers = lecun_uniform\n* Learning Rate = 0.1\n* Number of Units = 10\n* Optimizers = SGD","36238785":"# Model Definition","23a92ef1":"# Define Hyper-Parameters Values","1a641d88":"# Results","286dfa9e":"The following lines of code will allow us define Our Neural Network Architecture with:\n\n* 1 input layer of 784 neurons\n* 1 Hidden layer\n* 2 Dropout Layers\n* 1 output layer of 10 classes\n\nAs well, we will define the hyperparameters objects that will take the different choices that we have mentioned.","cad253e3":"This notebook contains the code samples from the following sources:\n\nhttps:\/\/www.youtube.com\/watch?v=OLlWnW3Qpe4&feature=youtu.be\n\nhttps:\/\/github.com\/moustaphacheikh\/youtube\/blob\/master\/Hyperparameters_optimisation___Keras.ipynb\n\nby Moustapha Cheikh.","a3a80a40":"# Let's see each type of image with their corresponding Labels","936e5bd4":"One of the biggest challenges when we are talking about Deep Learning is how to tune the hyper-parameters, this is a difficult part of the process that takes a lot of time and effort to be done. Here we will see how to test different combinations between different choices of hyperparameters.\n\nIn this Kernel we will try the following:\n\n* Varying choices of **Activation Functions**: **relu** , **tanh** , **sigmoid** , **hard_sigmoid** and **linear**\n* Different **Batch Size**: **20** , **50** and **100**\n* Varying **Dropout rates**: **0.3** , **0.2** and **0.8**\n* Different numbers of iterations or **epochs**: **1** , **20** and **50**\n* Different **Initializers** or statistical distributions to use in order to generate the intitial weights of the Neural Network: **lecun_uniform** , **normal** , **he_normal** and      **he_uniform**\n* Different **Learning Rates**: **0.1** , **0.001** and **0.02**\n* Different number of neurons or **Number of Units**: **10** and **5**\n* Different optimization algorithms or **optimizers** = **SGD** , **RMSprop** , **Adagrad** , **Adadelta** , **Adam** , **Adamax** and **Nadam**","9ca9f42a":"# Train the Models","8f070c78":"# Import Libraries"}}