{"cell_type":{"b7767161":"code","2eb438ba":"code","40764622":"code","b43c2737":"code","165071b0":"code","278ef810":"code","7373d33f":"code","d01b560f":"code","5092674e":"code","019830f0":"code","ef0858c0":"code","353e3e84":"code","7f4edd28":"code","d8e2d321":"code","eb28926e":"code","ca96ef00":"code","fa99de36":"code","20ee14c8":"code","e894a514":"code","8b16fb0a":"code","0b9a949c":"code","1d84bf2f":"code","ed2cffb1":"code","1aff57ad":"code","157813c6":"code","7c1e74a6":"code","35b95c9f":"code","2fc6dde3":"code","8573d6ff":"code","8b3b4a65":"code","7837dc71":"code","30501888":"code","a00fbf34":"code","2ef301cb":"code","8b6bf598":"code","d4519399":"code","4567b3f2":"code","c647f0e9":"code","d11050a3":"code","acf32734":"code","03c293e1":"code","7e6a7484":"code","328bbd80":"code","3fddb3a2":"code","eadde7a0":"code","d0064ef7":"code","3b6366e3":"code","d1275db6":"code","d56076fa":"markdown","1d5e9e8e":"markdown"},"source":{"b7767161":"# Load libraries\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport pickle\nimport numpy as np\nimport os\nfrom keras.applications.resnet50 import ResNet50\nfrom keras.optimizers import Adam\nfrom keras.layers import Dense, Flatten,Input, Convolution2D, Dropout, LSTM, TimeDistributed, Embedding, Bidirectional, Activation, RepeatVector,Concatenate\nfrom keras.models import Sequential, Model\nfrom keras.utils import np_utils\nimport random\nfrom keras.preprocessing import image, sequence\nimport matplotlib.pyplot as plt","2eb438ba":"# Load data\nimages_dir = os.listdir(\"..\/input\/flickr_data\/Flickr_Data\/\")\n\nimages_path = '..\/input\/flickr_data\/Flickr_Data\/Images\/'\ncaptions_path = '..\/input\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr8k.token.txt'\ntrain_path = '..\/input\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.trainImages.txt'\nval_path = '..\/input\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.devImages.txt'\ntest_path = '..\/input\/flickr_data\/Flickr_Data\/Flickr_TextData\/Flickr_8k.testImages.txt'\n\ncaptions = open(captions_path, 'r').read().split(\"\\n\")\nx_train = open(train_path, 'r').read().split(\"\\n\")\nx_val = open(val_path, 'r').read().split(\"\\n\")\nx_test = open(test_path, 'r').read().split(\"\\n\")","40764622":"# Loading captions as values and images as key in dictionary\ntokens = {}\n\nfor ix in range(len(captions)-1):\n    temp = captions[ix].split(\"#\")\n    if temp[0] in tokens:\n        tokens[temp[0]].append(temp[1][2:])\n    else:\n        tokens[temp[0]] = [temp[1][2:]]","b43c2737":"# displaying an image and captions given to it\ntemp = captions[10].split(\"#\")\nfrom IPython.display import Image, display\nz = Image(filename=images_path+temp[0])\ndisplay(z)\n\nfor ix in range(len(tokens[temp[0]])):\n    print(tokens[temp[0]][ix])","165071b0":"# Creating train, test and validation dataset files with header as 'image_id' and 'captions'\ntrain_dataset = open('flickr_8k_train_dataset.txt','wb')\ntrain_dataset.write(b\"image_id\\tcaptions\\n\")\n\nval_dataset = open('flickr_8k_val_dataset.txt','wb')\nval_dataset.write(b\"image_id\\tcaptions\\n\")\n\ntest_dataset = open('flickr_8k_test_dataset.txt','wb')\ntest_dataset.write(b\"image_id\\tcaptions\\n\")","278ef810":"# Populating the above created files for train, test and validation dataset with image ids and captions for each of these images\nfor img in x_train:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        train_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        train_dataset.flush()\ntrain_dataset.close()\n\nfor img in x_test:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        test_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        test_dataset.flush()\ntest_dataset.close()\n\nfor img in x_val:\n    if img == '':\n        continue\n    for capt in tokens[img]:\n        caption = \"<start> \"+ capt + \" <end>\"\n        val_dataset.write((img+\"\\t\"+caption+\"\\n\").encode())\n        val_dataset.flush()\nval_dataset.close()","7373d33f":"# Loading 50 layer Residual Network Model and getting the summary of the model\nfrom IPython.core.display import display, HTML\ndisplay(HTML(\"\"\"<a href=\"http:\/\/ethereon.github.io\/netscope\/#\/gist\/db945b393d40bfa26006\">ResNet50 Architecture<\/a>\"\"\"))\nmodel = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')\nmodel.summary()\n# Note: For more details on ResNet50 architecture you can click on hyperlink given below","d01b560f":"# Helper function to process images\ndef preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","5092674e":"train_data = {}\nctr=0\nfor ix in x_train:\n    if ix == \"\":\n        continue\n    if ctr >= 3000:\n        break\n    ctr+=1\n    if ctr%1000==0:\n        print(ctr)\n    path = images_path + ix\n    img = preprocessing(path)\n    pred = model.predict(img).reshape(2048)\n    train_data[ix] = pred","019830f0":"train_data['2513260012_03d33305cf.jpg'].shape","ef0858c0":"# opening train_encoded_images.p file and dumping it's content\nwith open( \"train_encoded_images.p\", \"wb\" ) as pickle_f:\n    pickle.dump(train_data, pickle_f )  ","353e3e84":"# Loading image and its corresponding caption into a dataframe and then storing values from dataframe into 'ds'\npd_dataset = pd.read_csv(\"flickr_8k_train_dataset.txt\", delimiter='\\t')\nds = pd_dataset.values\nprint(ds.shape)","7f4edd28":"pd_dataset.head()","d8e2d321":"# Storing all the captions from ds into a list\nsentences = []\nfor ix in range(ds.shape[0]):\n    sentences.append(ds[ix, 1])\n    \nprint(len(sentences))","eb28926e":"# First 5 captions stored in sentences\nsentences[:5]","ca96ef00":"# Splitting each captions stored in 'sentences' and storing them in 'words' as list of list\nwords = [i.split() for i in sentences]","fa99de36":"# Creating a list of all unique words\nunique = []\nfor i in words:\n    unique.extend(i)\nunique = list(set(unique))\n\nprint(len(unique))\n\nvocab_size = len(unique)","20ee14c8":"# Vectorization\nword_2_indices = {val:index for index, val in enumerate(unique)}\nindices_2_word = {index:val for index, val in enumerate(unique)}","e894a514":"word_2_indices['UNK'] = 0\nword_2_indices['raining'] = 8253","8b16fb0a":"indices_2_word[0] = 'UNK'\nindices_2_word[8253] = 'raining'","0b9a949c":"print(word_2_indices['<start>'])\nprint(indices_2_word[4011])\nprint(word_2_indices['<end>'])\nprint(indices_2_word[8051])","1d84bf2f":"vocab_size = len(word_2_indices.keys())\nprint(vocab_size)","ed2cffb1":"max_len = 0\n\nfor i in sentences:\n    i = i.split()\n    if len(i) > max_len:\n        max_len = len(i)\n\nprint(max_len)","1aff57ad":"padded_sequences, subsequent_words = [], []\n\nfor ix in range(ds.shape[0]):\n    partial_seqs = []\n    next_words = []\n    text = ds[ix, 1].split()\n    text = [word_2_indices[i] for i in text]\n    for i in range(1, len(text)):\n        partial_seqs.append(text[:i])\n        next_words.append(text[i])\n    padded_partial_seqs = sequence.pad_sequences(partial_seqs, max_len, padding='post')\n\n    next_words_1hot = np.zeros([len(next_words), vocab_size], dtype=np.bool)\n    \n    #Vectorization\n    for i,next_word in enumerate(next_words):\n        next_words_1hot[i, next_word] = 1\n        \n    padded_sequences.append(padded_partial_seqs)\n    subsequent_words.append(next_words_1hot)\n    \npadded_sequences = np.asarray(padded_sequences)\nsubsequent_words = np.asarray(subsequent_words)\n\nprint(padded_sequences.shape)\nprint(subsequent_words.shape)","157813c6":"print(padded_sequences[0])","7c1e74a6":"for ix in range(len(padded_sequences[0])):\n    for iy in range(max_len):\n        print(indices_2_word[padded_sequences[0][ix][iy]],)\n    print(\"\\n\")\n\nprint(len(padded_sequences[0]))","35b95c9f":"num_of_images = 2000","2fc6dde3":"captions = np.zeros([0, max_len])\nnext_words = np.zeros([0, vocab_size])","8573d6ff":"for ix in range(num_of_images):#img_to_padded_seqs.shape[0]):\n    captions = np.concatenate([captions, padded_sequences[ix]])\n    next_words = np.concatenate([next_words, subsequent_words[ix]])\n\nnp.save(\"captions.npy\", captions)\nnp.save(\"next_words.npy\", next_words)\n\nprint(captions.shape)\nprint(next_words.shape)","8b3b4a65":"with open('..\/input\/train_encoded_images.p', 'rb') as f:\n    encoded_images = pickle.load(f, encoding=\"bytes\")","7837dc71":"imgs = []\n\nfor ix in range(ds.shape[0]):\n    if ds[ix, 0].encode() in encoded_images.keys():\n#         print(ix, encoded_images[ds[ix, 0].encode()])\n        imgs.append(list(encoded_images[ds[ix, 0].encode()]))\n\nimgs = np.asarray(imgs)\nprint(imgs.shape)","30501888":"images = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        images.append(imgs[ix])\n        \nimages = np.asarray(images)\n\nnp.save(\"images.npy\", images)\n\nprint(images.shape)","a00fbf34":"image_names = []\n\nfor ix in range(num_of_images):\n    for iy in range(padded_sequences[ix].shape[0]):\n        image_names.append(ds[ix, 0])\n        \nimage_names = np.asarray(image_names)\n\nnp.save(\"image_names.npy\", image_names)\n\nprint(len(image_names))","2ef301cb":"captions = np.load(\"captions.npy\")\nnext_words = np.load(\"next_words.npy\")\n\nprint(captions.shape)\nprint(next_words.shape)","8b6bf598":"images = np.load(\"images.npy\")\n\nprint(images.shape)","d4519399":"imag = np.load(\"image_names.npy\")\n        \nprint(imag.shape)","4567b3f2":"embedding_size = 128\nmax_len = 40","c647f0e9":"image_model = Sequential()\n\nimage_model.add(Dense(embedding_size, input_shape=(2048,), activation='relu'))\nimage_model.add(RepeatVector(max_len))\n\nimage_model.summary()","d11050a3":"language_model = Sequential()\n\nlanguage_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=max_len))\nlanguage_model.add(LSTM(256, return_sequences=True))\nlanguage_model.add(TimeDistributed(Dense(embedding_size)))\n\nlanguage_model.summary()","acf32734":"conca = Concatenate()([image_model.output, language_model.output])\nx = LSTM(128, return_sequences=True)(conca)\nx = LSTM(512, return_sequences=False)(x)\nx = Dense(vocab_size)(x)\nout = Activation('softmax')(x)\nmodel = Model(inputs=[image_model.input, language_model.input], outputs = out)\n\n# model.load_weights(\"..\/input\/model_weights.h5\")\nmodel.compile(loss='categorical_crossentropy', optimizer='RMSprop', metrics=['accuracy'])\nmodel.summary()","03c293e1":"hist = model.fit([images, captions], next_words, batch_size=512, epochs=200)","7e6a7484":"model.save_weights(\"model_weights.h5\")","328bbd80":"def preprocessing(img_path):\n    im = image.load_img(img_path, target_size=(224,224,3))\n    im = image.img_to_array(im)\n    im = np.expand_dims(im, axis=0)\n    return im","3fddb3a2":"def get_encoding(model, img):\n    image = preprocessing(img)\n    pred = model.predict(image).reshape(2048)\n    return pred","eadde7a0":"resnet = ResNet50(include_top=False,weights='imagenet',input_shape=(224,224,3),pooling='avg')","d0064ef7":"img = \"..\/input\/flickr_data\/Flickr_Data\/Images\/1453366750_6e8cf601bf.jpg\"\n\ntest_img = get_encoding(resnet, img)","3b6366e3":"def predict_captions(image):\n    start_word = [\"<start>\"]\n    while True:\n        par_caps = [word_2_indices[i] for i in start_word]\n        par_caps = sequence.pad_sequences([par_caps], maxlen=max_len, padding='post')\n        preds = model.predict([np.array([image]), np.array(par_caps)])\n        word_pred = indices_2_word[np.argmax(preds[0])]\n        start_word.append(word_pred)\n        \n        if word_pred == \"<end>\" or len(start_word) > max_len:\n            break\n            \n    return ' '.join(start_word[1:-1])\n\nArgmax_Search = predict_captions(test_img)","d1275db6":"z = Image(filename=img)\ndisplay(z)\n\nprint(Argmax_Search)","d56076fa":"### Predictions","1d5e9e8e":"###  **Model**"}}