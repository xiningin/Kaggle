{"cell_type":{"0ed249a0":"code","889d1135":"code","0c49f119":"code","51a49546":"code","5c6c7a5d":"code","44f94193":"code","2823bfa0":"code","0f8b1f13":"code","93c9c109":"code","0feecab2":"code","0a492dfd":"markdown","73d7677c":"markdown","1da04e8f":"markdown","94a97f45":"markdown","30d602d0":"markdown","d521b344":"markdown","c4f12239":"markdown","645d769a":"markdown","766fcb70":"markdown","a1130f13":"markdown"},"source":{"0ed249a0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","889d1135":"%%time\ndata = pd.read_csv(\"\/kaggle\/input\/swarm-behavior\/Swarm Behavior Data\/Flocking.csv\", low_memory = False) \ndata","0c49f119":"data.info()","51a49546":"# let's immediately fix the bug in the name of the class column and x1[24015]\ndata['Class'] = data['Class ']\ndata = data.drop(['Class '], axis=1)\ndata.x1[24015] = 0","5c6c7a5d":"X_train = data.copy()\ny = X_train['Class']\nX_train = X_train.drop(['Class'], axis=1)","44f94193":"X_train = X_train.apply(pd.to_numeric)","2823bfa0":"%%time\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\n\ns_X_train, s_X_test, s_y_train, s_y_test = train_test_split(X_train, y, train_size = 0.2, test_size = 0.8, random_state = 0)\n\nmodel_baseline = RandomForestClassifier()\nmodel_baseline.fit(s_X_train, s_y_train)\n\nbaseline_predictions = model_baseline.predict(s_X_test)","0f8b1f13":"# Let's evaluate the model. As the metric of the classification problem, we choose ROC_AUC\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import roc_auc_score\nfrom matplotlib import pyplot as plt\n\nroc_auc_score(s_y_test, baseline_predictions)","93c9c109":"from sklearn.metrics import accuracy_score\naccuracy_score(baseline_predictions, s_y_test)","0feecab2":"s_X_train_1, s_X_test_1, s_y_train_1, s_y_test_1 = train_test_split(X_train, y, train_size = 0.0003, test_size = 0.9997, random_state = 0)\nmodel_baseline.fit(s_X_train_1, s_y_train_1)\nbaseline_predictions_1 = model_baseline.predict(s_X_test_1)\na_1 = roc_auc_score(s_y_test_1, baseline_predictions_1)\n\ns_X_train_2, s_X_test_2, s_y_train_2, s_y_test_2 = train_test_split(X_train, y, train_size = 0.0005, test_size = 0.9995, random_state = 0)\nmodel_baseline.fit(s_X_train_2, s_y_train_2)\nbaseline_predictions_2 = model_baseline.predict(s_X_test_2)\na_2 = roc_auc_score(s_y_test_2, baseline_predictions_2)\n\ns_X_train_3, s_X_test_3, s_y_train_3, s_y_test_3 = train_test_split(X_train, y, train_size = 0.0007, test_size = 0.9993, random_state = 0)\nmodel_baseline.fit(s_X_train_3, s_y_train_3)\nbaseline_predictions_3 = model_baseline.predict(s_X_test_3)\na_3 = roc_auc_score(s_y_test_3, baseline_predictions_3)\n\ns_X_train_4, s_X_test_4, s_y_train_4, s_y_test_4 = train_test_split(X_train, y, train_size = 0.001, test_size = 0.999, random_state = 0)\nmodel_baseline.fit(s_X_train_4, s_y_train_4)\nbaseline_predictions_4 = model_baseline.predict(s_X_test_4)\na_4 = roc_auc_score(s_y_test_4, baseline_predictions_4)\n\ns_X_train_5, s_X_test_5, s_y_train_5, s_y_test_5 = train_test_split(X_train, y, train_size = 0.0012, test_size = 0.9988, random_state = 0)\nmodel_baseline.fit(s_X_train_5, s_y_train_5)\nbaseline_predictions_5 = model_baseline.predict(s_X_test_5)\na_5 = roc_auc_score(s_y_test_5, baseline_predictions_5)\n\ns_X_train_6, s_X_test_6, s_y_train_6, s_y_test_6 = train_test_split(X_train, y, train_size = 0.004, test_size = 0.996, random_state = 0)\nmodel_baseline.fit(s_X_train_6, s_y_train_6)\nbaseline_predictions_6 = model_baseline.predict(s_X_test_6)\na_6 = roc_auc_score(s_y_test_6, baseline_predictions_6)\n\ns_X_train_7, s_X_test_7, s_y_train_7, s_y_test_7 = train_test_split(X_train, y, train_size = 0.05, test_size = 0.95, random_state = 0)\nmodel_baseline.fit(s_X_train_7, s_y_train_7)\nbaseline_predictions_7 = model_baseline.predict(s_X_test_7)\na_7 = roc_auc_score(s_y_test_7, baseline_predictions_7)\n\nprint(\"Using to train 0.03% of data:\", a_1, \"\\nUsing to train 0.05% of data:\", a_2, \n      \"\\nUsing to train 0.07% of data:\", a_3, \"\\nUsing to train 0.1% of data:\", a_4, \n      \"\\nUsing to train 0.12% of data:\", a_5,\"\\nUsing to train 0.4% of data:\",  a_6, \n      \"\\nUsing to train 5% of data:\", a_7)","0a492dfd":"As far as can be judged from this analysis, predicting whether birds are in flock or not based on so many features is fairly straightforward for the base model.","73d7677c":"# <a id = \"3\"> 2. A quick look at the data<\/a>","1da04e8f":"A baseline is a model that is both simple to set up and has a reasonable chance of providing decent results.\nAs such a model, we will choose the *RandomForestClassifier* in the basic configuration (without selection of parameters) on all the presented data. Yes, it is resource-intensive, but as long as we can afford it, we will do it. We need this result for a comparative analysis to show that on a modified dataset we will not lose accuracy, while improving accuracy (we count on this).","94a97f45":"## <a id = \"2\"> Description of features<\/a>\n\nThe features are:\n 1. *xm* and *ym* as the (X,Y) position of each boid;\n 2. *xVeln* and *yVeln* as the velocity vector;\n 3. *xAm* and *yAm* as the alignment vector;\n 4. *xSm* and *ySm* as the separation vector;\n 5. *xCm* and *yCm* as the cohesion vector;\n 6. *nACm* as the number of boids in the radius of Alignment\/Cohesion;\n 7. *nSm* as the number of boids in the radius of Separation.\n\nThese attributes are repeated for all m boids, where m = 1,...,200. (12*200 = 2 400 features)\n\nSo, as we can see, the dataset is very massive! Therefore, the main goal will be to create a high-quality predictive model on as few lines and features as possible.","30d602d0":"5% of data it is just 0.05 * 24016 = 1 201 rows to predict other 22 815 rows!","d521b344":"## <a id = \"stepend\"> Table of contents <\/a>\n1. [Dataset Description](#1)\n    * [Description of features](#2)\n2. [A quick look at the data](#3)\n3. [Baseline Model](#4)   \n4. [Conclusion](#5)","c4f12239":"# <a id = \"1\"> 1. Dataset Description<\/a>\n\nThis dataset is a set of measurements (24 016 units) of [boids](https:\/\/en.wikipedia.org\/wiki\/Boids), by which we can judge whether the birds are **grouped** at the moment, whether they are in a **flock** or **aligned**.\n\nIn this work, we will solve the *classification problem* for only one target variable - in a **flock** boids or not. (Class labels are binary, which 1 refers to flocking, grouped, and aligned, and 0 refers to not flocking, not grouped, and not aligned).\n> Flocking behaviour refers to the way that groups of birds, insects, fish or other animals, move close to each other. They are able to move as a group with the same velocity, yet without running into each other.","645d769a":"# <a id = \"5\"> 4. Conclusions <\/a>","766fcb70":"# <a id = \"4\"> 3. Baseline Model <\/a>","a1130f13":"The model has extremely high prediction accuracy, let's determine how many rows we need to achieve almost 100% prediction accuracy."}}