{"cell_type":{"abc88f6d":"code","2a8f6c20":"code","ec856523":"code","8f1d90b9":"code","40b8ac20":"code","e16b2de1":"code","1ef1eb59":"code","9e26dad6":"markdown","e23030ad":"markdown","f9005826":"markdown","6d345f7d":"markdown","4c219330":"markdown"},"source":{"abc88f6d":"import os\nimport numpy as np\nimport tensorflow as tf\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tensorflow.keras import layers, losses, Sequential\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping","2a8f6c20":"def load_nbaiot(filename):\n    return np.loadtxt(\n        os.path.join(\"\/kaggle\/input\/nbaiot-dataset\", filename),\n        delimiter=\",\",\n        skiprows=1\n    )\n\nbenign = load_nbaiot(\"1.benign.csv\")\nX_train = benign[:40000]\nX_test0 = benign[40000:]\nX_test1 = load_nbaiot(\"1.mirai.scan.csv\")\nX_test2 = load_nbaiot(\"1.mirai.ack.csv\")\nX_test3 = load_nbaiot(\"1.mirai.syn.csv\")\nX_test4 = load_nbaiot(\"1.mirai.udp.csv\")\nX_test5 = load_nbaiot(\"1.mirai.udpplain.csv\")","ec856523":"print(X_train.shape, X_test0.shape, X_test1.shape, X_test2.shape,\n      X_test3.shape, X_test4.shape, X_test5.shape)","8f1d90b9":"class Autoencoder(Model):\n    def __init__(self):\n        super(Autoencoder, self).__init__()\n        self.encoder = Sequential([\n            layers.Dense(115, activation=\"relu\"),\n            layers.Dense(86, activation=\"relu\"),\n            layers.Dense(57, activation=\"relu\"),\n            layers.Dense(37, activation=\"relu\"),\n            layers.Dense(28, activation=\"relu\")\n        ])\n        self.decoder = Sequential([\n            layers.Dense(37, activation=\"relu\"),\n            layers.Dense(57, activation=\"relu\"),\n            layers.Dense(86, activation=\"relu\"),\n            layers.Dense(115, activation=\"sigmoid\")\n        ])\n    \n    def call(self, x):\n        encoded = self.encoder(x)\n        decoded = self.decoder(encoded)\n        return decoded","40b8ac20":"scaler = MinMaxScaler()\nx = scaler.fit_transform(X_train)\n\nae = Autoencoder()\nae.compile(optimizer=Adam(learning_rate=0.01), loss='mse')\nmonitor = EarlyStopping(\n    monitor='val_loss',\n    min_delta=1e-9,\n    patience=5,\n    verbose=1,\n    mode='auto'\n)\nae.fit(\n    x=x,\n    y=x,\n    epochs=800,\n    validation_split=0.3,\n    shuffle=True,\n    callbacks=[monitor]\n)\n\ntraining_loss = losses.mse(x, ae(x))\nthreshold = np.mean(training_loss)+np.std(training_loss)","e16b2de1":"def predict(x, threshold=threshold, window_size=82):\n    x = scaler.transform(x)\n    predictions = losses.mse(x, ae(x)) > threshold\n    # Majority voting over `window_size` predictions\n    return np.array([np.mean(predictions[i-window_size:i]) > 0.5\n                     for i in range(window_size, len(predictions)+1)])\n\ndef print_stats(data, outcome):\n    print(f\"Shape of data: {data.shape}\")\n    print(f\"Detected anomalies: {np.mean(outcome)*100}%\")\n    print()","1ef1eb59":"test_data = [X_test0, X_test1, X_test2, X_test3, X_test4, X_test5]\n\nfor i, x in enumerate(test_data):\n    print(i)\n    outcome = predict(x)\n    print_stats(x, outcome)","9e26dad6":"## Botnet Detection with an Autoencoder\n20 May 2021  \nThis notebook was created for a course at Istanbul Technical University.\n- We implement (a simplified version of) the autoencoder-based anomaly detection described in the N-BaIoT paper [1].\n\n[1] Meidan, Yair, et al. \"N-BaIoT\u2014Network-based Detection of IoT Botnet Attacks Using Deep Autoencoders.\" IEEE Pervasive Computing 17.3 (2018): 12-22. https:\/\/arxiv.org\/pdf\/1805.03409  ","e23030ad":"---\n# 2. Autoencoder Architecture\nRelevant parts of [1] describing the autoencoder architecture:\n\n- The general idea is autoencoder-based anomaly detection, p. 4:\n> [W]e use deep autoencoders and maintain a  \n> model for each IoT device separately. An autoencoder is a neural  \n> network which is trained to reconstruct its inputs after some  \n> compression. The compression ensures that the network learns the  \n> meaningful concepts and the relation among its input features. If an  \n> autoencoder is trained on benign instances only, then it will succeed  \n> at reconstructing normal observations, but fail at reconstructing  \n> abnormal observations (unknown concepts). When a significant re-  \n> construction error is detected, then we classify the given  \n> observations as being an anomaly.\n\n- Details, p. 5:\n> Each autoencoder had an input layer whose dimension is equal to the  \n> number of features in the dataset (i.e., 115). As noted by [16] and  \n> [15], autoencoders effectively perform dimen- sionality reduction  \n> internally, such that the code layer be- tween the encoder(s) and  \n> decoder(s) efficiently compresses the input layer and reflects its  \n> essential characteristics. In our experiments, four hidden layers of  \n> encoders were set at decreasing sizes of 75%, 50%, 33%, and 25% of the  \n> input layer\u2019s dimension. The next layers were decoders, with the same  \n> sizes as the encoders, however with an increasing order (starting from  \n> 33%).\n\n- Anomaly Detection threshold, p.4:\n> This anomaly threshold, above which an instance is considered  \n> anomalous, is calculated as the sum of the sample mean and standard  \n> deviation of [the mean squared error over the validation set].\n\n- Sequences of packets, p.4:\n> Preliminary experiments revealed that deciding whether a device\u2019s  \n> packet stream is anomalous or not based on a single instance enables  \n> very accurate detection of IoT-based botnet attacks (high TPR).  \n> However, benign instances were too often (in approximately 5-7% of  \n> cases) falsely marked as anomalous. Thus we base the abnormality  \n> decision on a sequence of instances by implementing a majority vote on  \n> a moving window. We determine the minimal window size ws\u2217 as the  \n> shortest sequence of instances, a majority vote which produces 0% FPR  \n> on [the validation set].\n\n- Final hyperparameters for the Danmini smart doorbell, p. 5:  \n    - Learning rate: 0.012  \n    - Number of epochs: 800  \n    - Anomaly Threshold: 0.042  \n    - Window Size: 82\n\n---\n# 3. Python Reimplementation\nAdapting this Keras tutorial: https:\/\/github.com\/tensorflow\/docs\/blob\/master\/site\/en\/tutorials\/generative\/autoencoder.ipynb","f9005826":"---\n1. N-BaIoT Dataset\n2. Autoencoder Architecture\n3. Python Reimlementation\n4. Conclusion\n\n---\n# 1. N-BaIoT Dataset\nhttps:\/\/archive.ics.uci.edu\/ml\/datasets\/detection_of_IoT_botnet_attacks_N_BaIoT\n- Normal traffic was captured for 9 IoT devices connected to the network.\n- Then, they were infected with Mirai and BASHLITE (aka gafgyt) malware.\n- Traffic was captured for each device for different phases of the malware execution.\n- From the network traffic, 115 features were extracted as described in [1].\n\nFor now, we start with data from a smart doorbell: normal execution and the different phases of Mirai.","6d345f7d":"How can we determine the hyperparameters?\n- In Keras, the fault learning rate for Adam optimizer is `0.001`. With that, training is relatively slow, so we quickly tried `0.01`.\n- We use Early Stopping to find the number of epochs.\n- The anomaly threshold is calculated as one standard deviation above the mean of training data losses.","4c219330":"---\n# 4. Conclusion\nAccording to the above output, it seems to work well.\n\nThe following are some possibilities where to go from here:\n- Run on the full N-BaIoT dataset: all 9 devices, all attacks.\n- Implement (some of) the improvements from [2].\n- In [3], it is suggested to use a subset of 23 features instead of all 115. However, different algorithms were used.\n    - Question: Are these 23 features enough also for an autoencoder system like in [1] or [2]?\n- Run on the MedBIoT dataset [4].\n- Run on the IoT-23 dataset [5] after performing feature extraction.\n\n# References\n[1] Meidan, Yair, et al. \"N-BaIoT\u2014Network-based Detection of IoT Botnet Attacks Using Deep Autoencoders.\" IEEE Pervasive Computing 17.3 (2018): 12-22. https:\/\/arxiv.org\/pdf\/1805.03409  \n[2] Mirsky, Yisroel et al. \"Kitsune: An Ensemble of Autoencoders for Online Network Intrusion Detection\", NDSS (2018). https:\/\/arxiv.org\/abs\/1802.09089v2  \n[3] Alhowaide, Alaa, et al. \"Towards the design of real-time autonomous IoT NIDS.\" Cluster Computing (2021): 1-14. https:\/\/doi.org\/10.1007\/s10586-021-03231-5  \n[4] Guerra-Manzanares, Alejandro, et al. \"MedBIoT: Generation of an IoT Botnet Dataset in a Medium-sized IoT Network.\" ICISSP 1 (2020): 207-218. https:\/\/doi.org\/10.5220\/0009187802070218  \n[5] Garcia, Sebastian et al. \"IoT-23: A labeled dataset with malicious and benign IoT network traffic\" (2020). (Version 1.0.0) [Data set]. Zenodo. http:\/\/doi.org\/10.5281\/zenodo.4743746"}}