{"cell_type":{"e84c6940":"code","17862ffb":"code","31a720c9":"code","5fa44ac3":"code","0624afb2":"code","61584c8d":"code","a5e86962":"code","765dfb85":"code","24ba75fd":"code","2ceb8977":"code","9ccf8151":"code","2db64a23":"code","b3c3bd97":"markdown","d9ad062f":"markdown","6d0a486b":"markdown","3b16fee3":"markdown","b32229fc":"markdown","5b0e3af6":"markdown","d03e841a":"markdown","c0bdc162":"markdown"},"source":{"e84c6940":"# whether to convert the pawpularity scores in the range of 0-1\nsigmoid = True\n\n# train validation split size\nsplit_size = 0.1\n\n# input image size\nIMAGE_SIZE = [224, 224]","17862ffb":"import tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D, Concatenate, Dense\nfrom tensorflow.keras.layers import Flatten, GlobalAveragePooling2D, BatchNormalization\nfrom tensorflow.keras.callbacks import LearningRateScheduler, ModelCheckpoint\n\nimport numpy as np\nimport pandas as pd\nimport random\nimport os\nfrom tqdm.auto import tqdm\nfrom sklearn.model_selection import train_test_split\n\nfrom matplotlib import pyplot as plt\nfrom kaggle_datasets import KaggleDatasets\nAUTO = tf.data.experimental.AUTOTUNE\nprint(\"Tensorflow version \" + tf.__version__)\ntf.executing_eagerly()","31a720c9":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\nexcept ValueError:\n    tpu = None\n    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n    \n# Select appropriate distribution strategy\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.TPUStrategy(tpu) # Going back and forth between TPU and host is expensive. Better to run 128 batches on the TPU before reporting back.\n    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])  \nelif len(gpus) > 1:\n    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n    print('Running on multiple GPUs ', [gpu.name for gpu in gpus])\nelif len(gpus) == 1:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on single GPU ', gpus[0].name)\nelse:\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    print('Running on CPU')\nprint(\"Number of accelerators: \", strategy.num_replicas_in_sync)","5fa44ac3":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nuser_credential = user_secrets.get_gcloud_credential()\nuser_secrets.set_tensorflow_credential(user_credential)","0624afb2":"data = pd.read_csv('..\/input\/petfinder-pawpularity-score\/train.csv')\nif sigmoid:\n    data['Pawpularity'] = data['Pawpularity'] \/ 100.0","61584c8d":"filenames = tf.io.gfile.glob(str('..\/input\/preprocessing-pawfinder\/train\/*'))\ntrain_filenames, val_filenames = train_test_split(filenames, test_size=split_size, random_state=123)\n\ncolumns = list(data.columns)\n\ndef create_metadata(filenames):\n    meta_data = pd.DataFrame(columns=columns)\n\n    for f in tqdm(filenames):\n        Id = os.path.basename(f).replace('.jpg','')\n        df = data.loc[data['Id']==Id]\n        meta_data = pd.concat([meta_data, df], ignore_index = True)    \n        \n    return meta_data\n\ntrain_meta_data = create_metadata(train_filenames)\nval_meta_data = create_metadata(val_filenames)","a5e86962":"def decode_img(img):\n    # convert the compressed string to a 3D uint8 tensor\n    img = tf.image.decode_jpeg(img, channels=3)\n    # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n    img = tf.cast(img, tf.float32)\n    # resize the image to the desired size.\n    return tf.image.resize(img, IMAGE_SIZE)\n\ndef process_path(file_path):\n    # load the raw data from the file as a string\n    img = tf.io.read_file(file_path)\n    img = decode_img(img)\n    return img\n\ndef process_dataset(meta_data, filenames):\n    if sigmoid:\n        labels = meta_data['Pawpularity'].to_numpy().astype('float32')\n    else:\n        labels = meta_data['Pawpularity'].to_numpy().astype('int32')\n            \n    meta_data = meta_data.drop(['Id', 'Pawpularity'], axis=1).to_numpy().astype('int32')\n\n    label_dataset = tf.data.Dataset.from_tensor_slices(labels)\n    meta_dataset = tf.data.Dataset.from_tensor_slices(meta_data)\n\n    list_ds = tf.data.Dataset.from_tensor_slices(filenames)\n    ds = list_ds.map(process_path, num_parallel_calls=AUTO)\n\n    dataset = tf.data.Dataset.zip((ds, meta_dataset))\n\n    ds = tf.data.Dataset.zip((dataset, label_dataset))\n    return ds\n\ntrain_ds = process_dataset(train_meta_data, train_filenames)\nval_ds = process_dataset(val_meta_data, val_filenames)","765dfb85":"def prepare_for_training(ds, cache=True, shuffle_buffer_size=1024):\n    # This is a small dataset, only load it once, and keep it in memory.\n    # use `.cache(filename)` to cache preprocessing work for datasets that don't\n    # fit in memory.\n    ds = ds.batch(128)\n    if cache:\n        if isinstance(cache, str):\n            ds = ds.cache(cache)\n        else:\n            ds = ds.cache()\n\n    ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n\n    # Repeat forever\n    ds = ds.repeat()\n\n    # `prefetch` lets the dataset fetch batches in the background while the model\n    # is training.\n    ds = ds.prefetch(buffer_size=AUTO)\n\n    return ds\n\ntrain_ds = prepare_for_training(train_ds)\nval_ds = prepare_for_training(val_ds)","24ba75fd":"train_ds","2ceb8977":"num_val_images = len(val_filenames)\nnum_train_images = len(train_filenames)\n\ntraining_steps_per_epoch = num_train_images \/\/ 128\nvalidation_steps_per_epoch = -(-num_val_images \/\/ 128)\nprint(\"No. of Training images: \", num_train_images, \", Steps per epoch: \", training_steps_per_epoch)\nprint(\"No. of Validation Images: \", num_val_images, \", Steps per epoch: \", validation_steps_per_epoch)","9ccf8151":"'''\nFeature Extraction is performed by EfficientNetB0 pretrained on imagenet weights. \nInput size is 224 x 224.\n'''\n\nfrom tensorflow.keras.applications import EfficientNetB0\n\ndef feature_extractor(inputs):\n\n    feature_extractor = EfficientNetB0(input_shape=(224, 224, 3),\n                                       include_top=False,\n                                       weights='imagenet'\n                                      )(inputs)\n    \n    return feature_extractor\n\n\n'''\nDefines final dense layers for classification.\n'''\ndef classifier(inputs):\n    x = GlobalAveragePooling2D()(inputs)\n    x = Flatten()(x)\n    x = Dense(1024, activation=\"relu\")(x)\n    x = BatchNormalization()(x)\n    x = Dense(512, activation=\"relu\")(x)\n    x = Dense(128, activation=\"relu\")(x)\n    return x\n\n'''\nConnect the feature extraction and \"classifier\" layers to build the model.\n'''\ndef final_model(inputs):\n    eff_feature_extractor = feature_extractor(inputs)\n    classification_output = classifier(eff_feature_extractor)\n    return classification_output\n\n'''\nDefine the model and compile it. \n'''\ndef define_compile_model(initial_lr):\n    input_1 = tf.keras.layers.Input(shape=(224, 224, 3))\n    input_2 = tf.keras.layers.Input(shape=(12))\n\n  \n    classification_output = final_model(input_1)\n    \n    concat_layer= Concatenate()([input_2, classification_output])\n    x = Dense(128, activation='relu')(concat_layer)\n    x = Dense(64, activation='relu')(x)\n    \n    if sigmoid:\n        output = Dense(1, activation='sigmoid')(x)\n    else:\n        output = Dense(1)(x)\n    \n    model = Model(inputs=(input_1, input_2), outputs = output)\n    \n    optimizer = tf.keras.optimizers.SGD(learning_rate=initial_lr) \n    model.compile(optimizer=optimizer, \n                loss = tf.keras.losses.Huber(), \n                metrics=[tf.keras.metrics.RootMeanSquaredError()]\n                )\n    \n    return model","2db64a23":"checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\"pawfinder_concat.h5\",\n                                                   save_best_only=True,\n                                                   monitor='val_root_mean_squared_error',\n                                                   mode='min'\n                                                  )\n\n\nwith strategy.scope():\n    model = define_compile_model(initial_lr=0.1)\n\n    model.summary()\n    \n# Train the custom model\nhistory = model.fit(\n        train_ds,\n        steps_per_epoch=training_steps_per_epoch,\n        epochs=2,\n        validation_data=val_ds,\n        validation_steps=validation_steps_per_epoch,\n        callbacks=[checkpoint_cb]\n)","b3c3bd97":"## Credentials","d9ad062f":"This notebook is for all those folks who wants to know how to train a multi-input keras model using TF dataset. This notebook covers the following:\n* TF Dataset creation for a multi-input model\n* Defining layers for a multi-input model (Concatenate)\n\nAdvantages of TF dataset is that the model will be trained quickly. As you all know TPU queue is long nowadays in Kaggle and all are not able to get their hands on the TPU. So, for them relying on TF dataset might be a good idea. Also, if you belong to the those lucky ones i.e., you get to train your model on TPU, then you're in for a treat. Cos, with this TF dataset clubbed with TPU you can even train your model for 500 epochs and won't even have to wait for hours, unlike the case in GPUs.\n\n**NOTE:** \n* I've used images which were centre cropped.\n* During inference create a pseudo column for the 'Pawpularity' score while loading the test.csv and create the TF dataset in the same manner as train_ds or val_ds in this notebook. O\/w you'd get an error while predicting saying 'ValueError: Layer model expects 2 input(s), but it received 1 input tensors.'","6d0a486b":"## Model","3b16fee3":"## Training","b32229fc":"## Import","5b0e3af6":"## TF dataset Creation","d03e841a":"## Distribution Strategy","c0bdc162":"## Load CSV"}}