{"cell_type":{"f637ae75":"code","411dbe6b":"code","8bedb820":"code","fcab5d05":"code","3fa0cb0e":"code","ae791420":"code","fab10436":"code","7f013496":"code","bbe25209":"code","5a154e97":"code","85710a6b":"code","47418ef1":"code","f187873a":"code","c91ad947":"code","c9e70fad":"code","0673c8fa":"code","c958a07a":"markdown","10dbeb20":"markdown","a8d1429d":"markdown","08e459ca":"markdown","4e7f8d16":"markdown","8e142ae6":"markdown"},"source":{"f637ae75":"# libraries\nimport numpy as np                \nimport pandas as pd\nimport matplotlib.pyplot as plt","411dbe6b":"# read csv\ndata = pd.read_csv(\"..\/input\/weatherAUS.csv\")","8bedb820":"# drop the unnecessary columns\ndata.drop([\"Date\", \"Location\", \"WindGustDir\", \"WindDir9am\", \"WindDir3pm\"], axis=1, inplace=True)","fcab5d05":"# convert \"NO\" to 0 and \"YES\" to 1\ndata.RainTomorrow = [ 1 if each == \"Yes\" else 0 for each in data.RainTomorrow ]\ndata.RainToday = [ 1 if each == \"Yes\" else 0 for each in data.RainToday ]\n\n# convert Nan to 0\ndata = data.fillna(0)\n\ndata.head()","3fa0cb0e":"data.info()","ae791420":"# y is results. x is features without \"RainTomorrow\" and 'nan'\ny = data.RainTomorrow.values\nx = data.drop([\"RainTomorrow\"], axis=1)\n","fab10436":"# normalization\nx = ((x - np.min(x))\/(np.max(x) - np.min(x))).values","7f013496":"# train\/test split\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nx_train = x_train.T\nx_test = x_test.T\ny_train = y_train.T\ny_test = y_test.T\n\nprint(\"x_train shape: \", x_train.shape)\nprint(\"x_test shape: \", x_test.shape)\nprint(\"y_train shape: \", y_train.shape)\nprint(\"y_test shape: \", y_test.shape)","bbe25209":"# initialize weight and bias\ndef initialize_w_and_b(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w,b","5a154e97":"# sigmoid func\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","85710a6b":"# forward & backward propagation func\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward\n    z = np.dot(w.T,x_train)+b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]\n    \n    # backward\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1]\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]\n    gradients = {\"derivative_weight\": derivative_weight, \"derivative_bias\": derivative_bias}\n    \n    return cost, gradients","47418ef1":"# updating\ndef update(w,b,x_train,y_train,learning_rate,num_iterations):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    \n    for i in range(num_iterations):\n        cost,gradients = forward_backward_propagation(w,b,x_train, y_train)\n        cost_list.append(cost)\n        \n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        param = { \"weight\": w, \"bias\": b }\n        \"\"\"if i % 50 == 0:\n            index.append(i)\n            cost_list2.append(cost)\n            \n    \n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"iteation\")\n    plt.ylabel(\"cost\")\n    plt.show()\"\"\"  # wanna see graph, delete \"\"\"\n    \n    return param, gradients, cost_list","f187873a":"# prediction\ndef predict(w,b,x_test):\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    \n    for i in range(z.shape[1]):\n        if z[0,i] <= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n    return Y_prediction","c91ad947":"# logistic regression\ndef logistic_regression(x_train, y_train, x_test, y_test, learning_rate, num_iterations):\n    # initialize\n    dimension = x_train.shape[0]   # that is 30\n    w,b = initialize_w_and_b(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate, num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"], x_test)\n    \n    # print test error\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","c9e70fad":"logistic_regression(x_train, y_train, x_test, y_test, learning_rate = 1, num_iterations = 300)","0673c8fa":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(x_train.T, y_train.T)\nprint(\"test accuracy: {}\".format(lr.score(x_test.T, y_test.T)))","c958a07a":"****EDA (Explotary Data Analysis)****","10dbeb20":"We have to get rid of **NaN**, **YES** and **NO**.","a8d1429d":"now, lets call logistic regression func","08e459ca":"****Logistic Regression with sklearn****","4e7f8d16":"1. EDA (Explotary Data Analysis)\n2. Hand-made Logistic Regression\n3. Logistic Regression with sklearn","8e142ae6":"****Hand-made Logistic Regression****"}}