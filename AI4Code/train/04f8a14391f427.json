{"cell_type":{"435c257f":"code","d181fc73":"code","9ca57c1e":"code","2247f8ea":"code","bf4832eb":"code","ff1a9757":"code","4464ba3b":"code","4a869951":"code","eac06617":"code","0fe35685":"code","349350b1":"code","fbcf7419":"code","c6044fa7":"code","3490cf63":"code","ccdf9d57":"code","76c2b56a":"code","b271cd56":"code","3804cf55":"code","9b588d0c":"code","504d5958":"code","699ca52c":"code","9ecf509e":"code","221ca0ad":"code","4331e5a4":"code","e4240cb3":"code","5f3ad1cc":"code","c55e95f5":"code","22222ef1":"code","b1badb4d":"code","ab0ae2ed":"code","4be4cf14":"code","95895242":"markdown","7c3ff59c":"markdown","a0b5e6ef":"markdown","6727bb9c":"markdown","a88283c1":"markdown","382bd72f":"markdown","113308d1":"markdown","37387ea1":"markdown","616b44b3":"markdown","329724de":"markdown","601f39a3":"markdown","d464cd7c":"markdown","6d2865e3":"markdown","39b52d5d":"markdown"},"source":{"435c257f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d181fc73":"from sklearn.model_selection import train_test_split","9ca57c1e":"df = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf[\"sentiment\"] = df[\"sentiment\"].map(lambda x: 1 if x == \"positive\" else 0)","2247f8ea":"df.head()","bf4832eb":"df.sentiment.value_counts()","ff1a9757":"X_train, X_test, y_train, y_test = train_test_split(df[\"review\"], \n                                                    df[\"sentiment\"],\n                                                    test_size=0.20, \n                                                    random_state=42)","4464ba3b":"X_train.shape, y_train.shape, X_test.shape, y_test.shape","4a869951":"X_train[13653]","eac06617":"y_train[13653]","0fe35685":"import nltk\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","349350b1":"analyser = SentimentIntensityAnalyzer()\ndef get_sentiment(sentence):\n    scores = analyser.polarity_scores(sentence)\n    if scores[\"compound\"] > 0:\n        return 1\n    else:\n        return 0\ny_preds = X_test.map(get_sentiment)","fbcf7419":"y_preds","c6044fa7":"(y_preds == y_test).mean()","3490cf63":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import naive_bayes","ccdf9d57":"count_vec = CountVectorizer()\ncount_vec.fit(X_train)","76c2b56a":"X_train_tfm = count_vec.transform(X_train)\nX_test_tfm = count_vec.transform(X_test)","b271cd56":"model = naive_bayes.MultinomialNB()\nmodel.fit(X_train_tfm, y_train)","3804cf55":"y_preds = model.predict(X_test_tfm)","9b588d0c":"(y_preds == y_test).mean()","504d5958":"count_vec = CountVectorizer(analyzer=\"word\", stop_words=\"english\", ngram_range=(1, 3), max_df=.2, min_df=2)\ncount_vec.fit(X_train)\n\nX_train_tfm = count_vec.transform(X_train)\nX_test_tfm = count_vec.transform(X_test)\n\nmodel = naive_bayes.MultinomialNB()\nmodel.fit(X_train_tfm, y_train)\n\ny_preds = model.predict(X_test_tfm)\n\n(y_preds == y_test).mean()","699ca52c":"from sklearn.feature_extraction.text import TfidfVectorizer","9ecf509e":"tfidf_vec = TfidfVectorizer(analyzer=\"word\", stop_words=\"english\", ngram_range=(1, 2))\ntfidf_vec.fit(X_train)","221ca0ad":"X_train_tfidf_tfm = tfidf_vec.transform(X_train)\nX_test_tfidf_tfm = tfidf_vec.transform(X_test)\n\nmodel = naive_bayes.MultinomialNB()\nmodel.fit(X_train_tfidf_tfm, y_train)\n\ny_preds = model.predict(X_test_tfidf_tfm)\n\n(y_preds == y_test).mean()","4331e5a4":"import gensim\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm","e4240cb3":"def sentence_to_vec(s, embedding_dict, stop_words, tokenizer):\n    words = str(s).lower()\n    words = tokenizer(words)\n    words = [w for w in words if not w in stop_words]\n    words = [w for w in words if w.isalpha()]\n    \n    M = []\n    for w in words:\n        if w in embedding_dict:\n            M.append(embedding_dict[w])\n    \n    if len(M) == 0:\n        return np.zeros(300)\n    \n    M = np.array(M)\n    v = M.sum(axis=0)\n    v \/ np.sqrt((v ** 2).sum())\n    return v","5f3ad1cc":"embeddings = gensim.models.KeyedVectors.load_word2vec_format(\n    \"..\/input\/googles-trained-word2vec-model-in-python\/GoogleNews-vectors-negative300.bin\", \n    binary=True\n)","c55e95f5":"X_train_ft = []\nfor review in tqdm(X_train.values):\n    X_train_ft.append(\n        sentence_to_vec(\n            s = review,\n            embedding_dict = embeddings,\n            stop_words = [],\n            tokenizer = word_tokenize\n        )\n    )\n\nX_test_ft = []\nfor review in tqdm(X_test.values):\n    X_test_ft.append(\n        sentence_to_vec(\n            s = review,\n            embedding_dict = embeddings,\n            stop_words = [],\n            tokenizer = word_tokenize\n        )\n    )\n\nX_train_ft = np.array(X_train_ft)\nX_test_ft = np.array(X_test_ft)","22222ef1":"X_train_ft.shape, X_test_ft.shape","b1badb4d":"X_train[1]","ab0ae2ed":"X_train_ft[1]","4be4cf14":"from sklearn import linear_model \nmodel = linear_model.LogisticRegression(max_iter=1000)\nmodel.fit(X_train_ft, y_train)\n\ny_preds = model.predict(X_test_ft)\n\n(y_preds == y_test).mean()","95895242":"## Create sparse matrices","7c3ff59c":"## Pros\/Cons\n- **Pros:** Can use data to improve results, fast to train\n- **Cons:** More complex that previous method, worst results compared to more complex methods","a0b5e6ef":"# Fourth approach (word2vec, Logistic Regression)","6727bb9c":"## Pros\/Cons\nSame as CountVectorizer","a88283c1":"# Third approach (TF-IDF Vectorizer, Naive Bayes)","382bd72f":"## Learn vocabulary dictionary","113308d1":"## Here are the results after playing around with some of the parameters","37387ea1":"# Attributions\nMost of the examples are base on: https:\/\/www.amazon.com\/Approaching-Almost-Machine-Learning-Problem-ebook\/dp\/B089P13QHT","616b44b3":"# Other approaches\n- Use Neural Network LSTM, GRU, CNN\n- Train your own embedding using a semi-supervised approach\n- Train a language model and fine-tune it for this task (ULMFit)\n- Fine-tune a pre-trained DL model (e.g., BERT)","329724de":"## Pros\/Cons\n- **Pros:** Incorporates additional data, will probably handle unseen data better, usually better results after tuning\n- **Cons:** More complex to use, fine-tuning the embeddings will be slower than previous methods","601f39a3":"## Train model and generate predictions[](http:\/\/)","d464cd7c":"# First approach (Vader)","6d2865e3":"# Second approach (CountVectorizer + NaiveBayes)","39b52d5d":"## Pros \/ Cons\n- **Pros:** Quick to execute, does not require training\n- **Cons:** Not specific to context, does not make use of data, usually poor results"}}