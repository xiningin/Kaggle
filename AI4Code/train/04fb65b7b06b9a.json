{"cell_type":{"dd3f0604":"code","237d706c":"code","434457ba":"code","441b4675":"code","43930edd":"code","1c624672":"code","e70ad95c":"code","3b300578":"code","7f9c9093":"code","6277e606":"code","dac29846":"code","6159d662":"code","907dd1a6":"code","81ab27f0":"code","2e19cd4d":"code","2d806b76":"code","585c3e8d":"code","c2e8ae56":"code","b21c0444":"code","93d38f77":"code","0175cafd":"code","9cf0e4f4":"code","304e3525":"code","9368fa40":"code","06e63ab2":"code","5f187b9f":"code","b8b001e0":"code","07f89bd0":"code","52e066c4":"code","63b33a0a":"code","1a957b6d":"code","730dcef7":"code","0f9c4c87":"code","3600bcd3":"code","d1231d9f":"code","364f914d":"code","076c440e":"code","87cf6393":"code","32dec47a":"code","c8ecf9d6":"code","c96b238a":"code","75ccae75":"code","ad315a37":"code","34222ad7":"code","48f4e1e4":"code","c995aa70":"code","eee3cd9e":"code","afea0103":"code","8b1a1fa3":"code","1221663d":"code","dd108ec8":"code","e04c605b":"code","1ce348c3":"code","997e1993":"markdown","69090db6":"markdown","0c6318af":"markdown","e53c5594":"markdown","4ef5551e":"markdown","0d922e04":"markdown","52c8d987":"markdown","fc199e23":"markdown","033a63e2":"markdown","d0686683":"markdown","23429020":"markdown","243ced04":"markdown","2d1b6411":"markdown","38b314b5":"markdown"},"source":{"dd3f0604":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","237d706c":"\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","434457ba":"df=pd.read_csv(os.path.join(dirname, filename))","441b4675":"df.head()","43930edd":"df.dtypes","1c624672":"df.drop(['id','Unnamed: 32'],axis=1,inplace=True)","e70ad95c":"df.head()","3b300578":"df.isnull().sum()","7f9c9093":"df.describe()","6277e606":"plt.figure(figsize=(10,10))\nsns.countplot(df['diagnosis'])","dac29846":"independent_cols=list(df.columns)[1:]\nmean_col=['diagnosis']\nse_col=['diagnosis']\nworst_col=['diagnosis']\nfor i in independent_cols:\n    if '_mean' in i:\n        mean_col.append(i)\n    if '_se' in i:\n        se_col.append(i)\n    if '_worst' in i:\n        worst_col.append(i)      \n","6159d662":"mean_features=df[mean_col]\nX=mean_features.iloc[:,1:]\ny=mean_features.iloc[:,0]\nnew_data = (X - X.mean()) \/(X.std())             \ndata = pd.concat([y,new_data],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')","907dd1a6":"plt.figure(figsize=(20,20))\nsns.boxplot(x='features',y='value',hue='diagnosis',data=data)","81ab27f0":"se_features=df[se_col]\nX=se_features.iloc[:,1:]\ny=se_features.iloc[:,0]\nnew_data = (X - X.mean()) \/(X.std())             \ndata = pd.concat([y,new_data],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,20))\nsns.boxplot(x='features',y='value',hue='diagnosis',data=data)","2e19cd4d":"worst_features=df[worst_col]\nX=worst_features.iloc[:,1:]\ny=worst_features.iloc[:,0]\nnew_data = (X - X.mean()) \/(X.std())             \ndata = pd.concat([y,new_data],axis=1)\ndata = pd.melt(data,id_vars=\"diagnosis\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(20,20))\nsns.boxplot(x='features',y='value',hue='diagnosis',data=data)","2d806b76":"plot_value=1\nplt.figure(figsize=(20,15))\nfor i in independent_cols:\n    if plot_value<=30:\n        plt.subplot(5,6,plot_value)\n        sns.histplot(df[i])\n        plot_value=plot_value+1\nplt.tight_layout()","585c3e8d":"plot_value=1\nplt.figure(figsize=(15,15))\nfor i in independent_cols:\n    if plot_value<=30:\n        plt.subplot(5,6,plot_value)\n        sns.boxplot(x=df[i])\n        plot_value=plot_value+1\nplt.tight_layout()","c2e8ae56":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(),annot=True)","b21c0444":"df['diagnosis']=df['diagnosis'].map({'B':0,'M':1})\ndata=df.copy()","93d38f77":"#Handling Outlier Values\n#From the boxplot or the histplot we can interpret the features which are right_skewed,left_skewed or normal distribution\nnormal_distributed=['texture_mean','smoothness_mean','symmetry_mean','concave points_se','symmetry_se','fractal_dimension_se','texture_worst','smoothness_worst','symmetry_worst']\nright_skew_distributed=[]\nfor i in independent_cols:\n    if i not in normal_distributed:\n        right_skew_distributed.append(i)\n","0175cafd":"#For normal distributed features\nfor i in  normal_distributed:\n    uppper_boundary=data[i].mean() + (3* data[i].std())\n    lower_boundary=data[i].mean() - (3* data[i].std())\n    data.loc[data[i]>uppper_boundary,i]=uppper_boundary\n    data.loc[data[i]<lower_boundary,i]=lower_boundary\n    ","9cf0e4f4":"#For right skewed distribution\nfor i in right_skew_distributed:\n    IQR=df[i].quantile(0.75)-df[i].quantile(0.25)\n    lower_boundary=df[i].quantile(0.25)-(IQR*1.5)\n    upper_boundary=df[i].quantile(0.75)+(IQR*1.5)\n    data.loc[data[i]>upper_boundary,i]=upper_boundary\n    data.loc[data[i]<lower_boundary,i]=lower_boundary\n","304e3525":"plot_value=1\nplt.figure(figsize=(20,15))\nfor i in independent_cols:\n    if plot_value<=30:\n        plt.subplot(5,6,plot_value)\n        sns.histplot(data[i])\n        plot_value=plot_value+1\nplt.tight_layout()","9368fa40":"#Scaling the data before feature selection\nfrom sklearn.preprocessing import StandardScaler\nscaler=StandardScaler()\ndata_scaled=scaler.fit_transform(data.iloc[:,1:])","06e63ab2":"data.columns[1:]","5f187b9f":"data_scaled=pd.DataFrame(data_scaled,columns=data.columns[1:])","b8b001e0":"data_scaled.head()","07f89bd0":"diagnosis_feature=data.iloc[:,0]\nnew_data=pd.concat([diagnosis_feature,data_scaled],axis=1)","52e066c4":"new_data.head()","63b33a0a":"#In order carry out feature selection we will be using the train dataset as we dont want feature selection to be influenced by test data and cause an overfitting model\nfrom sklearn.model_selection import train_test_split\nX=new_data.iloc[:,1:]\ny=new_data.iloc[:,0]\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","1a957b6d":"def correlation(dataset, threshold):\n    col_corr = set()  \n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold: \n                colname = corr_matrix.columns[i]  \n                col_corr.add(colname)\n    return col_corr\nhighly_correlated_features=correlation(X_train,0.9)","730dcef7":"highly_correlated_features","0f9c4c87":"X_train.drop(highly_correlated_features,axis=1,inplace=True)\nX_test.drop(highly_correlated_features,axis=1,inplace=True)","3600bcd3":"#LOGISTIC REGRESSION\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nlog_reg_model=LogisticRegression()\nlog_reg_model.fit(X_train,y_train)","d1231d9f":"pred_results=log_reg_model.predict(X_test)","364f914d":"print(accuracy_score(y_test,pred_results))","076c440e":"log_reg=accuracy_score(y_test,pred_results)\n","87cf6393":"# KNN CLASSIFICATION\nfrom sklearn.neighbors import KNeighborsClassifier","32dec47a":"# Finding optimum K value using elbow method\nerror_rate=[]\nfor i in range(1,51):\n    knn_model=KNeighborsClassifier(n_neighbors=i)\n    knn_model.fit(X_train,y_train)\n    y_pred=knn_model.predict(X_test)\n    error_rate.append(np.mean(y_pred!=y_test))\n    \n    ","c8ecf9d6":"figure=plt.figure(figsize=(10,10))\nplt.plot(range(1,51),error_rate,linestyle='dashed',marker='o')","c96b238a":"knn_model=KNeighborsClassifier(n_neighbors=37)\nknn_model.fit(X_train,y_train)\npred_results=knn_model.predict(X_test)\naccuracy_score(y_test,pred_results)","75ccae75":"knn_mod=accuracy_score(y_test,pred_results)","ad315a37":"#Random Forest Classification\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier_model=RandomForestClassifier()\nclassifier_model.fit(X_train,y_train)\npred_results=classifier_model.predict(X_test)\naccuracy_score(y_test,pred_results)\n","34222ad7":"rfc=accuracy_score(y_test,pred_results)","48f4e1e4":"from sklearn.ensemble import GradientBoostingClassifier","c995aa70":"gb_model=GradientBoostingClassifier()","eee3cd9e":"gb_model.fit(X_train,y_train)","afea0103":"pred_results=gb_model.predict(X_test)","8b1a1fa3":"accuracy_score(y_test,pred_results)","1221663d":"gb_tech=accuracy_score(y_test,pred_results)","dd108ec8":"dic={'Techniques':['Gradient Boosting','K nearest Neighbors','Random Forest Classification','Logistic Regression'],'Accuracy':[gb_tech,knn_mod,rfc,log_reg]}","e04c605b":"accuracy_table=pd.DataFrame(dic)","1ce348c3":"plt.figure(figsize=(10,10))\nsns.barplot(x='Accuracy',y='Techniques',data=accuracy_table)","997e1993":"The counts above clearly shows that it is not an imbalanced dataset","69090db6":"In feature engineering we are handling outliers in the data and scaling the data","0c6318af":"In this dataset,I carry out basic exploratory data analysis,feature engineering,feature selection and model implementation.\nAttribute Information:\n\n1) ID number<br>\n2) Diagnosis (M = malignant, B = benign)<br>\nTen real-valued features are computed for each cell nucleus:<br>\n\na) radius (mean of distances from center to points on the perimeter)<br>\nb) texture (standard deviation of gray-scale values)<br>\nc) perimeter<br>\nd) area<br>\ne) smoothness (local variation in radius lengths)<br>\nf) compactness (perimeter^2 \/ area - 1.0)<br>\ng) concavity (severity of concave portions of the contour)<br>\nh) concave points (number of concave portions of the contour)<br>\ni) symmetry<br>\nj) fractal dimension (\"coastline approximation\" - 1)<br>\n\nThe mean, standard error and \"worst\" or largest (mean of the three<br>\nlargest values) of these features were computed for each image,<br>\nresulting in 30 features. For instance, field 3 is Mean Radius, field<br>\n13 is Radius SE, field 23 is Worst Radius.<br>\n\nAll feature values are recoded with four significant digits.<br>\n\nMissing attribute values: none<br>\n\nClass distribution: 357 benign, 212 malignant<br>","e53c5594":"Used a few classification models like Logistic Regression,Random Forest Classification,KNN Classification and Gradient Boosting.Scope for doing hyperparameter tuning and using cross validation","4ef5551e":"The boxplots above give us a hollistic view of the data. Swarmplots or violin plots can also be used.","0d922e04":"Dividing the columns into 3 categories for visualization purpose ","52c8d987":"FEATURE ENGINEERING","fc199e23":"Getting an idea about the distribution of the data and the number of outliers","033a63e2":"As we can see in the distribution above outliers have been eliminated","d0686683":"FEATURE SELECTION","23429020":"Just eliminated the highly correlated features and used remaining features for model building.SelectKbest and other methods can also be sued for feature selection","243ced04":"EXPLORATORY DATA ANALYSIS","2d1b6411":"Correlation heatmap is mandatory to understand relationship among different independent features","38b314b5":"MODEL EVALUATION"}}