{"cell_type":{"9eefd511":"code","4422c6c5":"code","1b55d6fd":"code","322f5003":"code","d80eda51":"code","5e2574d1":"code","708ac7e8":"code","80a0137f":"code","89362156":"code","95d96813":"code","25c6159e":"code","d3678827":"code","12cd3ad3":"code","86a9f638":"code","bb1a80a6":"markdown","709e24bc":"markdown","09b2fa30":"markdown","52e7fe8a":"markdown","3e7f6e42":"markdown","e5ff5dfe":"markdown","8b8249d2":"markdown","60441bcb":"markdown","fb71ec9c":"markdown","f79d8e7c":"markdown","eca5a7d7":"markdown","93199c50":"markdown","901613bf":"markdown"},"source":{"9eefd511":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","4422c6c5":"from nltk.stem import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n#create an object of class PorterStemmer\nporter = PorterStemmer()\nlancaster=LancasterStemmer()\n#provide a word to be stemmed\nprint(\"Porter Stemmer\")\nprint(\"cats => \",porter.stem(\"cats\"))\nprint(\"trouble => \",porter.stem(\"trouble\"))\nprint(\"troubling =>\", porter.stem(\"troubling\"))\nprint(\"troubled => \",porter.stem(\"troubled\"))\nprint(\"Lancaster Stemmer\")\nprint(\"cats => \",lancaster.stem(\"cats\"))\nprint(\"trouble => \",lancaster.stem(\"trouble\"))\nprint(\"troubling =>\",lancaster.stem(\"troubling\"))\nprint(\"troubled => \",lancaster.stem(\"troubled\"))","1b55d6fd":"from nltk.tokenize import sent_tokenize, word_tokenize\ndef stemSentence(sentence):\n    token_words=word_tokenize(sentence)\n    token_words\n    stem_sentence=[]\n    for word in token_words:\n        stem_sentence.append(porter.stem(word))\n        stem_sentence.append(\" \")\n    return \"\".join(stem_sentence)\n\nsentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\nx=stemSentence(sentence)\nprint(x)\n","322f5003":"import nltk\nfrom nltk.stem import WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\nsentence = \"He was running and eating at same time. He has bad habit of swimming after playing long hours in the Sun.\"\npunctuations=\"?:!.,;\"\nsentence_words = nltk.word_tokenize(sentence)\nfor word in sentence_words:\n    if word in punctuations:\n        sentence_words.remove(word)\n\nsentence_words\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word)))\n","d80eda51":"print(\"Lemmatization with POS Tagging \")\nprint(\"{0:20}{1:20}\".format(\"Word\",\"Lemma\"))\nfor word in sentence_words:\n    print (\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word, pos=\"v\")))\n","5e2574d1":"from nltk.tokenize import word_tokenize\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"..\/input\/textdb\/articles.txt\", \"r\")\narticle = f.read()\n#print()\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Create a Counter with the lowercase tokens: bow_simple\nbow_simple = Counter(lower_tokens)\n\n# Print the 10 most common tokens\nprint(bow_simple.most_common(10))\n","708ac7e8":"# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"..\/input\/textdb\/articles.txt\", \"r\")\narticle = f.read()\n#print()\n# Tokenize the article: tokens\ntokens = word_tokenize(article)\n\n# English Stop words\nenglish_stops = set(stopwords.words('english'))\n\n# Convert the tokens into lowercase: lower_tokens\nlower_tokens = [t.lower() for t in tokens]\n\n# Retain alphabetic words: alpha_only\nalpha_only = [t for t in lower_tokens if t.isalpha()]\n\n# Remove all stop words: no_stops\nno_stops = [t for t in alpha_only if t not in english_stops]\n\n# Instantiate the WordNetLemmatizer\nwordnet_lemmatizer = WordNetLemmatizer()\n\n# Lemmatize all tokens into a new list: lemmatized\nlemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n\n# Create the bag-of-words: bow\nbow = Counter(lemmatized)\n\n# Print the 10 most common tokens\nprint(bow.most_common(10))\n","80a0137f":"import nltk\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\n\n# Read text files\nf = open(\"..\/input\/textdb\/articles.txt\", \"r\")\narticle = f.read()\n#print()\n\n# Tokenize the article into sentences: sentences\nsentences = nltk.sent_tokenize(article)\n\n# Tokenize each sentence into words: token_sentences\ntoken_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n\n# Tag each tokenized sentence into parts of speech: pos_sentences\npos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n\n# Create the named entity chunks: chunked_sentences\nchunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n\n# Test for stems of the tree with 'NE' tags\nfor sent in chunked_sentences:\n    for chunk in sent:\n        if hasattr(chunk, \"label\") and chunk.label() == \"NE\":\n            print(chunk)\n","89362156":"import nltk\n# Import WordNetLemmatizer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\n# Import Counter\nfrom collections import Counter\nfrom collections import defaultdict\nimport matplotlib.pyplot as plt\n\n# Read text files\nf = open(\"..\/input\/textdb\/articles.txt\", \"r\")\narticle = f.read()\n\n# Tokenize the article into sentences: sentences\nsentences = nltk.sent_tokenize(article)\n\n# Tokenize each sentence into words: token_sentences\ntoken_sentences = [nltk.word_tokenize(sent) for sent in sentences]\n\n# Tag each tokenized sentence into parts of speech: pos_sentences\npos_sentences = [nltk.pos_tag(sent) for sent in token_sentences] \n\n# Create the named entity chunks: chunked_sentences\nchunked_sentences = nltk.ne_chunk_sents(pos_sentences,binary=True)\n\n# Create the defaultdict: ner_categories\nner_categories = defaultdict(int)\n\n# Create the nested for loop\nfor sent in chunked_sentences:\n    for chunk in sent:\n        if hasattr(chunk, 'label'):\n            ner_categories[chunk.label()] += 1\n            \n# Create a list from the dictionary keys for the chart labels: labels\nlabels = list(ner_categories.keys())\n\n# Create a list of the values: values\nvalues = [ner_categories.get(l) for l in labels]\n\n# Create the pie chart\nplt.pie(values, labels=labels, autopct='%1.1f%%', startangle=140)\n\n# Display the chart\nplt.show()\n","95d96813":"# Import spacy\nimport spacy\n\n# Read text files\nf = open(\"..\/input\/textdb\/articles.txt\", \"r\")\narticle = f.read()\n\n# Instantiate the English model: nlp\nnlp = spacy.load('en',tagger=False, parser=False, matcher=False)\n\n# Create a new document: doc\ndoc = nlp(article)\n\n# Print all of the found entities and their labels\nfor ent in doc.ents:\n    print(ent.text, ent.label_)\n","25c6159e":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#importing csv\ndf = pd.read_csv('..\/input\/textdb3\/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n# Initialize a CountVectorizer object: count_vectorizer\ncount_vectorizer = CountVectorizer(stop_words='english')\n\n# Transform the training data using only the 'text' column values: count_train \ncount_train = count_vectorizer.fit_transform(X_train.values)\n\n# Transform the test data using only the 'text' column values: count_test \ncount_test = count_vectorizer.transform(X_test.values)\n\n# Print the first 10 features of the count_vectorizer\nprint(count_vectorizer.get_feature_names()[:10])\n","d3678827":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n#importing csv\ndf = pd.read_csv('..\/input\/textdb3\/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n","12cd3ad3":"# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n#importing csv\ndf = pd.read_csv('..\/input\/textdb3\/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Print the first 10 features\nprint(tfidf_vectorizer.get_feature_names()[:10])\n\n# Print the first 5 vectors of the tfidf training data\nprint(tfidf_train.A[:5])\n","86a9f638":"\n# Import the necessary modules\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\n# Import TfidfVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n\n#importing csv\ndf = pd.read_csv('..\/input\/textdb3\/fake_or_real_news.csv')\n# Print the head of df\nprint(df.head())\n\n# Create a series to store the labels: y\ny = df.label\n\n# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(df['text'],y,test_size=0.33, random_state=53)\n\n\n# Initialize a TfidfVectorizer object: tfidf_vectorizer\ntfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n\n# Transform the training data: tfidf_train \ntfidf_train = tfidf_vectorizer.fit_transform(X_train)\n\n# Transform the test data: tfidf_test \ntfidf_test = tfidf_vectorizer.transform(X_test)\n\n# Create a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(tfidf_train,y_train)\n\n# Create the predicted tags: pred\npred = nb_classifier.predict(tfidf_test)\n\n# Calculate the accuracy score: score\nscore = metrics.accuracy_score(y_test,pred)\nprint(\"Accuracy Score : \",score)\n\n# Calculate the confusion matrix: cm\ncm = metrics.confusion_matrix(y_test,pred, labels=['FAKE', 'REAL'])\nprint(\"Confusion Matrix : \\n\",cm)\n\n# Inspecting your model\n\nprint('Inspecting your model')\n\n# Get the class labels: class_labels\nclass_labels = nb_classifier.classes_\n\n# Extract the features: feature_names\nfeature_names = tfidf_vectorizer.get_feature_names()\n\n# Zip the feature names together with the coefficient array and sort by weights: feat_with_weights\nfeat_with_weights = sorted(zip(nb_classifier.coef_[0], feature_names))\n\n# Print the first class label and the top 20 feat_with_weights entries\nprint('First class label and the top 20 feat_with_weights entries')\nprint(class_labels[0], feat_with_weights[:20])\n\n# Print the second class label and the bottom 20 feat_with_weights entries\nprint('Second class label and the bottom 20 feat_with_weights entries')\nprint(class_labels[1], feat_with_weights[-20:])\n\n","bb1a80a6":"In the above code output, you must be wondering that no actual root form has been given for any word, this is because they are given without context. \n\nYou need to provide the context in which you want to lemmatize that is the parts-of-speech (POS). This is done by giving the value for pos parameter in wordnet_lemmatizer.lemmatize.\n","709e24bc":"**Training and Testing the \"fake news\" Model with TfidfVectorizer**\n\nNow that you have evaluated the model using the\u00a0CountVectorizer, you'll do the same using the\u00a0TfidfVectorizer\u00a0with a Naive Bayes model.\n\nThe training and test sets have been created, and\u00a0tfidf_vectorizer,\u00a0tfidf_train, and\u00a0tfidf_test\u00a0have been computed. Additionally,\u00a0MultinomialNB\u00a0and\u00a0metricshave been imported from, respectively,\u00a0sklearn.naive_bayesand\u00a0sklearn.\n\n","09b2fa30":"**Stemming Words Code Examples**\n\nStemming\u00a0usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.","52e7fe8a":"## Comparing NLTK with spaCy NER\n\nUsing the same text you used in the earlier exercise, you'll now see the results using spaCy's NER annotator. How will they compare?\n\nThe article has been pre-loaded as\u00a0article. To minimize execution times, you'll be asked to specify the keyword arguments\u00a0tagger=False, parser=False, matcher=False\u00a0when loading the spaCy model, because you only care about the\u00a0entity\u00a0in this exercise.\n","3e7f6e42":"**TfidfVectorizer for text classification**\n\nSimilar to the sparse\u00a0CountVectorizer\u00a0created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a\u00a0TfidfVectorizer\u00a0and investigate some of its features.\nIn this exercise, you'll use\u00a0pandas\u00a0and\u00a0sklearn\u00a0along with the same\u00a0X_train,\u00a0y_train\u00a0and\u00a0X_test,\u00a0y_test\u00a0DataFrames and Series you created in the last exercise.\n","e5ff5dfe":"**Charting Practice**\n\nIn this exercise, you'll use some extracted named entities and their groupings from a series of newspaper articles to chart the diversity of named entity types in the articles.\n\nYou'll use a\u00a0defaultdict\u00a0called\u00a0ner_categories, with keys representing every named entity group type, and values to count the number of each different named entity type. You have a chunked sentence list called\u00a0chunked_sentences\u00a0similar to the last exercise, but this time with non-binary category names.\n\nYou can use\u00a0hasattr()\u00a0to determine if each chunk has a\u00a0'label'\u00a0and then simply use the chunk's\u00a0.label()\u00a0method as the dictionary key.\n","8b8249d2":"**TfidfVectorizer for Text Classification**\n\nSimilar to the sparse\u00a0CountVectorizer\u00a0created in the previous exercise, you'll work on creating tf-idf vectors for your documents. You'll set up a\u00a0TfidfVectorizer\u00a0and investigate some of its features.\n\nIn this exercise, you'll use\u00a0pandas\u00a0and\u00a0sklearn\u00a0along with the same\u00a0X_train,\u00a0y_train\u00a0and\u00a0X_test,\u00a0y_test\u00a0DataFrames and Series you created in the last exercise.\n","60441bcb":"**Text Pre-Processing in Practice**\n\nLets clean up text for better NLP results by removing stopwords, and lemmatizing. You'll need to remove stop words and non-alphabetic characters, lemmatize, and perform a new bag-of-words on your cleaned text.\n\n\n","fb71ec9c":"**Stemming a  Complete Sentence**","f79d8e7c":"## Token Count","eca5a7d7":"## NER with NLTK\n\nYou're now going to have some fun with named-entity recognition! \n\nYour task is to use\u00a0nltk\u00a0to find the named entities in this article.\n\nWhat might the article be about, given the names you found?\n\nAlong with\u00a0nltk,\u00a0sent_tokenize\u00a0and\u00a0word_tokenize\u00a0from\u00a0nltk.tokenize\u00a0have been pre-imported.\n","93199c50":"**Lemmatization**\n\nStemming\u00a0usually refers to a crude heuristic process that chops off the ends of words in the hope of achieving this goal correctly most of the time, and often includes the removal of derivational affixes.\u00a0\n\nLemmatization\u00a0usually refers to doing things properly with the use of a vocabulary and morphological analysis of words, normally aiming to remove inflectional endings only and to return the base or dictionary form of a word, which is known as the\u00a0lemma\u00a0. If confronted with the token\u00a0saw, stemming might return just\u00a0s, whereas lemmatization would attempt to return either\u00a0see\u00a0or\u00a0saw\u00a0depending on whether the use of the token was as a verb or a noun.\u00a0\n\nFor example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.\n\nPython NLTK provides WordNet Lemmatizer that uses the WordNet Database to lookup lemmas of words.\n","901613bf":"**CountVectorizer for text classification**\n\nIt's time to begin building your text classifier! \n\nThe\u00a0data\u00a0has been loaded into a DataFrame called\u00a0df. Explore it in the IPython Shell to investigate what columns you can use. The\u00a0.head()\u00a0method is particularly informative.\n\nIn this exercise, you'll use\u00a0pandas\u00a0alongside scikit-learn to create a sparse text vectorizer you can use to train and test a simple supervised model. \n\nTo begin, you'll set up a\u00a0CountVectorizer\u00a0and investigate some of its features.\n"}}