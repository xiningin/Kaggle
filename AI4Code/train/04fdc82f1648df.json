{"cell_type":{"9d9c4c72":"code","e2c0e932":"code","8feb1e5f":"code","c5a91261":"code","ab73b641":"code","aaa9bcf4":"code","18fe29d9":"code","233df363":"code","28fe9ee2":"code","cbf49beb":"code","ea7d9ff1":"code","d80c8dc3":"code","e408c514":"code","3e19db9a":"code","078fcf32":"code","b75430c7":"code","c86e6d0c":"code","e0020b91":"code","466d776d":"code","adcc06af":"code","1daf47d4":"code","1ec24e1c":"code","37a4b169":"code","1230f449":"code","f00b322d":"code","fcd14e49":"code","eba72110":"code","765d32e4":"code","7632c778":"code","c4f5592f":"code","d7a08642":"code","2a55204f":"code","379b6177":"code","3c120c26":"code","2d0bbc95":"code","826bca2a":"code","0a067a9b":"code","55a1d854":"code","a027b562":"code","517f5dfd":"code","198e502e":"code","4b5f3075":"code","1c8c33ad":"code","4c8785d6":"code","986351bf":"code","b365345b":"markdown","efdbabb0":"markdown","6ce1dfd9":"markdown","df6df3ed":"markdown","4a5384b0":"markdown","9e4c03bf":"markdown","9e904214":"markdown","80cdeffa":"markdown","da9c1fab":"markdown","5631b7f7":"markdown","99c1ad71":"markdown","82ee49ef":"markdown","f364df22":"markdown","ba5f5aab":"markdown","e1b6f26b":"markdown","c54caad2":"markdown"},"source":{"9d9c4c72":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n#import numpy as np # linear algebra\n#import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    #for filename in filenames:\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2c0e932":"import pandas as  pd\nimport numpy as np","8feb1e5f":"import matplotlib.pyplot as plt\nimport seaborn as sns ","c5a91261":"from cv2 import imread","ab73b641":"import textwrap","aaa9bcf4":"pip install imagesize","18fe29d9":"\nimport imagesize","233df363":"from tensorflow import keras\nimport tensorflow as tf","28fe9ee2":"# NLTK library for text processing\nimport nltk\nfrom string import punctuation","cbf49beb":"# Sklearn library for textprocessing & clustering\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA,TruncatedSVD\nfrom sklearn.metrics.pairwise import cosine_similarity, cosine_distances","ea7d9ff1":"# all path info \ntrain_csv = \"..\/input\/shopee-product-matching\/train.csv\"\ntest_csv = \"..\/input\/shopee-product-matching\/test.csv\"\ntrain_dir = \"..\/input\/shopee-product-matching\/train_images\/\"\ntest_dir =\"..\/input\/shopee-product-matching\/test_images\/\"","d80c8dc3":"# reading train_csv & test_cs to data frame\n\ntrain_df = pd.read_csv( train_csv, sep =\",\")\ntest_df  = pd.read_csv( test_csv, sep =\",\")\nprint (\"Current dimension of train data frame ={}\".format( train_df.shape))\nprint (\"Current dimension of test data frame ={}\".format( test_df.shape))","e408c514":"train_df.head(10)","3e19db9a":"print ( \"Number of categories present in the train df ={}\".format( train_df[\"label_group\"].nunique()) )","078fcf32":"print ( \"Number of Unique images present in the train df ={}\".format( train_df[\"image\"].nunique()) )","b75430c7":"print (\"Max number of images present for each group\/category ={}\".format( train_df.groupby( 'label_group').count().max().iloc[0] ) )\nprint (\"Min number of images present for each group\/category ={}\".format( train_df.groupby( 'label_group').count().min().iloc[0] ) )","c86e6d0c":"train_df[[\"image\",\"label_group\",\"title\"]].iloc[23].values","e0020b91":"def create_image():\n    fig = plt.figure( figsize = (7,7), dpi = 80 )\n    #for i in range( 0, 4 ):\n    plt.subplot ( 1, 2, 1 )    \n    random_num = np.random.randint(train_df.shape[0])\n    image,group,caption = train_df[[\"image\",\"label_group\",\"title\"]].iloc[random_num].values\n\n    plt.imshow( imread(train_dir + image ))\n    plt.title( \"group = \" +str(group))\n    plt.subplot ( 1, 2,2 )\n    plt.text( 0.5,0.5 , \"\\n\".join( textwrap.wrap( caption, 20 ) ) )\n\n","466d776d":"create_image()\ncreate_image()","adcc06af":"dimension = np.zeros( shape = ( train_df[\"image\"].nunique(),2)  )","1daf47d4":"for i, x in enumerate( train_df[\"image\"].unique() ) :\n    dimension[i] = imagesize.get ( train_dir + x )","1ec24e1c":"fig = plt.figure( figsize = (5,5), dpi =90 )\nsns.scatterplot( x = [ x[0] for x  in dimension ], y = [ x[1] for x  in dimension ] )\n","37a4b169":"def remove_stop_words_and_tokenize( data_list ):\n\n    processed_text_data = []\n    stop_words = nltk.corpus.stopwords.words (\"english\")  +list(punctuation)\n\n    for x in data_list:\n\n        nltk_tokenized_data = nltk.word_tokenize( x.lower()  )\n        nltk_tokenized_data = [ x for x in nltk_tokenized_data if  ( (x not in stop_words) and x.isalpha() and len(x) >1 ) ]\n        #nltk_tokenized_data = [ x for x in nltk_tokenized_data if len(x) >1 ]\n        processed_text_data.append( \" \".join ( nltk_tokenized_data) )\n\n    print (\"length of tokenized words from train data = {}\".format(len(processed_text_data) ) )\n    print ( \"first 10 words = {}\".format( processed_text_data[3:10]))\n    \n    return ( processed_text_data )","1230f449":"train_df[\"new_title\"] = remove_stop_words_and_tokenize( train_df[\"title\"])\ntrain_df.head()","f00b322d":"#Tokanizing text to words & removingspecial characters \n\ntokenized_data = keras.preprocessing.text.Tokenizer ( num_words = None,\n                                                    filters = '!\"#$%&()*+,-.\/:;=?@[\\\\]^_`{|}~\\t\\n',\n                                                    lower = True,\n                                                    oov_token =\"<unk>\",\n                                                    split=\" \")\ntokenized_data.fit_on_texts ( train_df[\"new_title\"] )\ntokenized_data.word_index[\"<pad>\"] = 0\ntokenized_data.index_word[0] =\"<pad>\"\ntrain_seq_text = tokenized_data.texts_to_sequences ( train_df[\"new_title\"])","fcd14e49":"print (\"total number of words are ={}\".format ( len ( tokenized_data.word_counts)  ) )","eba72110":"\nwords_df = pd.DataFrame( data ={ \"words\":[ x for x,y in tokenized_data.word_counts.items()],\n                                \"count\":[ y for x,y in tokenized_data.word_counts.items()]}\n                       )\nwords_df.sort_values( \"count\", inplace = True,ascending = False )\nwords_df.head()","765d32e4":"fig ,axis = plt.subplots( nrows = 2, ncols = 1,  figsize =(20,20 ), dpi = 90 )\nsns.barplot( data =words_df.head(100), x = \"words\",y =\"count\", ax = axis [0])\naxis[0].tick_params(axis=\"x\", rotation = 90 )\naxis[0].title.set_text( \" First 100 most common words \")\nsns.barplot( data =words_df.tail(100), x = \"words\",y =\"count\", ax = axis [1])\naxis[1].tick_params(axis=\"x\", rotation = 90 )\naxis[1].title.set_text( \" last 100 most common words \")","7632c778":"#Tokanizing text to words & removingspecial characters \n\ntokenized_data = keras.preprocessing.text.Tokenizer ( num_words = None,\n                                                    filters = '!\"#$%&()*+,-.\/:;=?@[\\\\]^_`{|}~\\t\\n',\n                                                    lower = True,\n                                                    oov_token =\"<unk>\",\n                                                    split=\" \")\ntokenized_data.fit_on_texts ( train_df[\"new_title\"] )\ntokenized_data.word_index[\"<pad>\"] = 0\ntokenized_data.index_word[0] =\"<pad>\"\ntrain_seq_text = tokenized_data.texts_to_sequences ( train_df[\"new_title\"])\nprint (\"total number of words are ={}\".format ( len ( tokenized_data.word_counts)  ) )","c4f5592f":"train_df[\"caption_vector\"] = train_seq_text\ntrain_df.head()","d7a08642":"NGRAMS_TXT = 3\ndef remove_stop_words_and_tokenize_tf_idf( data_list, NGRAMS_TXT = NGRAMS_TXT ):\n\n    #processed_text_data = []\n    stop_words = nltk.corpus.stopwords.words (\"english\")  +list(punctuation)\n    #print (data_list)\n    #for x in data_list:\n\n    nltk_tokenized_data = nltk.word_tokenize( data_list.lower()  )\n    nltk_tokenized_data = [ x for x in nltk_tokenized_data if  ( (x not in stop_words) and x.isalpha() and len(x) >1 ) ]\n    nltk_tokenized_data = [ x for x in nltk_tokenized_data if len(x) >1 ]\n    #n_grams_data= list ( nltk.ngrams( nltk_tokenized_data, NGRAMS_TXT ) )\n    #processed_text_data.append( \" \".join ( nltk_tokenized_data) )\n\n    #print (\"length of tokenized words from train data = {}\".format(len(processed_text_data) ) )\n    #print ( \"first 10 words = {}\".format( processed_text_data[3:10]))\n    \n    return ( nltk_tokenized_data )#+ n_grams_data )","2a55204f":"processed_data = remove_stop_words_and_tokenize_tf_idf (train_df[\"title\"].iloc[4] )\nprint( \"processed data = {}\".format(processed_data))\nprint (  train_df[\"title\"].iloc[10] )","379b6177":"#NGRAMS_TXT = \ntf_idf = TfidfVectorizer( analyzer= remove_stop_words_and_tokenize_tf_idf,ngram_range= (1,4)).fit( train_df[\"title\"] )\n#tf_idf = TfidfVectorizer( ngram_range= (1,2),lowercase=True,stop_words= \"english\")\n#tf_idf.fit( train_df[\"title\"] )","3c120c26":"print (\"Lenght of vocabulary ={} words\".format( len ( tf_idf.vocabulary_) ) )","2d0bbc95":"transformed_tf_idf = tf_idf.fit_transform( train_df[\"title\"])","826bca2a":" np.where ( cosine_similarity( transformed_tf_idf[6], transformed_tf_idf[1:] ) > 0.8)[1] ","0a067a9b":"label= train_df[\"label_group\"].iloc[4]\ngroup = train_df.groupby( by =\"label_group\" )\ngroup.get_group( label)","55a1d854":"transformed_tf_idf[4] ","a027b562":"group.get_group( label)","517f5dfd":"plt.imshow( imread(r\"..\/input\/shopee-product-matching\/train_images\/\" + \"00117e4fc239b1b641ff08340b429633.jpg\" ))","198e502e":"plt.imshow( imread(r\"..\/input\/shopee-product-matching\/train_images\/\" + \"52f5b2e6f6647325817eb99db17709f0.jpg\" ))","4b5f3075":"## Tensorflow Extracting features from iamge ","1c8c33ad":"def load_image(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img, channels=3)\n    img = tf.image.resize(img, (299, 299))\n    img = tf.keras.applications.inception_v3.preprocess_input(img)\n    return img, image_path","4c8785d6":"img, path = load_image( \"..\/input\/shopee-product-matching\/train_images\/\" + train_df[\"image\"].iloc[1] )","986351bf":"cosine_similarity ( img,img)","b365345b":"# As we have close 18k words, Picking 10K frequent words for model building ","efdbabb0":"# Lets chek for number of categories present in the data frame train_df","6ce1dfd9":"# 11014 unique groups are present in the data frame","df6df3ed":"# Text Processing usig NLTK","4a5384b0":"    # Not all words in the caption are english, have take care of it as well ","9e4c03bf":"# We have 32412 unique images, were as total number of rows we have is 34250\n# Roughly we mush have good number sample images for each group","9e904214":"# Lets visualize few images with, caption with 3X3 grid\n    * We have to perform 2 operation \n        1. to read image to numpy array \n        2. then plot the read data to image along with Text.","80cdeffa":"# >Lets check captions and its features","da9c1fab":"    1. Tokenizing Text data\n    2. Remove stop words \n    ","5631b7f7":"# Required Data path\n#     1. train_csv --> Info about Train images + caption info \n#     2. test_csv  --> Has Test images + caption info \n#     3. train_dir --> Directory containing train images\n#     4. test_dir  --> Directory containing test images\n#     5. train_df  --> train_csv file read to pandas data frame for further processing \n#     6. test_df   --> test_csv file read to pandas data frame for further processing","99c1ad71":"1. Importing pandas & numpy library\n2. Importing Seaborn & Matplotlib to image rendering\n3. Importing IMread from Open CV\n4. Text library to make longer to multiple lines\n5. Library to read images without completly loading file\n6. Tensor flow library required for Text + image processing + model\n7. NLTK library for text processing + other text library","82ee49ef":"# Dimension of image is not uniform, wehave scale image toresonable size","f364df22":"    * Adding new text to train_df pandas data frame","ba5f5aab":"# Lets check number of unique image present in the train_df ","e1b6f26b":"# Lets check dimension of all images, how see the dimension spread","c54caad2":"# Checking train_df ( train data frame ) to check how data table is."}}