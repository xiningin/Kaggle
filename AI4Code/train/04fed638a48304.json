{"cell_type":{"4148a40f":"code","500d2b5d":"code","217d4a3c":"code","ad157924":"code","d4d37090":"code","0dbdd0ae":"code","c10dc8bb":"code","391153e1":"code","f38827d3":"code","e671d3bd":"code","b2a247a0":"code","698171ac":"code","ba6504b9":"code","dd5298ac":"code","727bb494":"code","ad049c84":"code","40c003a5":"code","da0e54ab":"code","28348ada":"code","12ac66c8":"code","6ff9cc33":"code","08eab7b9":"code","6e5c3e3c":"code","6d716bd9":"code","75f11276":"code","1957188e":"code","6804a2e3":"code","23e2cda0":"code","956fabf2":"code","478d06ac":"code","95185c23":"code","e577742e":"code","baa3a9c8":"code","c6bb26ff":"code","5ee75078":"code","7f581e57":"code","38e28f37":"code","ad57b3bd":"code","cc77d9bb":"code","282af8b7":"code","48be0481":"code","d2952a8f":"code","93b7967e":"code","caf953d7":"code","a4af1257":"code","0fc9a31a":"code","08328265":"code","ff2cb926":"code","165fa0ac":"code","0f80a12c":"code","90a05bf0":"code","dd91116c":"code","4b40fc8d":"code","924bb133":"code","98d052ee":"code","b9ea7c86":"code","d14f2c64":"code","b5aa4b8c":"code","409b691d":"code","6b21518a":"code","5bb403ca":"code","2d255def":"code","1eb19f0a":"code","dc1a9156":"code","a5414509":"code","833ddd62":"code","4dd9d4eb":"code","6d79efc2":"code","9aebdd82":"code","d2b6f28d":"markdown","b1aa4387":"markdown","1ed1c086":"markdown","9ad97a35":"markdown","6ed80da9":"markdown","d8cb5a69":"markdown","87014ecc":"markdown","86aaa2f5":"markdown","6f0b3857":"markdown","16b28261":"markdown","adf6b825":"markdown","5528cff4":"markdown","6364c2e3":"markdown","e95d33a1":"markdown","08b7c081":"markdown","aaab6622":"markdown","7bca201f":"markdown","dd45e070":"markdown","d5523db3":"markdown","d54648d1":"markdown","f4116edb":"markdown","adae711a":"markdown","e22239d6":"markdown","62d46fe2":"markdown","b14c7afb":"markdown","e996c058":"markdown","e7df2a50":"markdown","06478e9a":"markdown","bff09b58":"markdown","b65f9b0c":"markdown","696dbe78":"markdown","807d8fab":"markdown","89d64ee4":"markdown","cb9cc3d2":"markdown","0b19cf8d":"markdown","fcb214e9":"markdown","8bba0535":"markdown","5376d1c9":"markdown","c745a712":"markdown","dadbacc9":"markdown","4214bc6f":"markdown","ef902337":"markdown","3bb3715e":"markdown","b85789a8":"markdown","7dc43fb8":"markdown","da7b10a7":"markdown","53aba9bc":"markdown","21b6ac91":"markdown","cfaa6e89":"markdown","0c9b8f80":"markdown","d004ee5a":"markdown","45264a19":"markdown","1e38ad4b":"markdown","2be3b336":"markdown","f1e95e0b":"markdown","af6ad002":"markdown","5121f789":"markdown","00511448":"markdown","769d5f0a":"markdown","b37802fd":"markdown","4e1ee68a":"markdown","a4f4b35b":"markdown","eb4ee28a":"markdown","0bca2a5c":"markdown","c4a84eba":"markdown","817b5c8d":"markdown","87a2e5f9":"markdown"},"source":{"4148a40f":"import numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score \nfrom sklearn import tree \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier  \nfrom sklearn.metrics import confusion_matrix, plot_confusion_matrix, balanced_accuracy_score, accuracy_score, classification_report  \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.svm import SVC, NuSVC, SVR, NuSVR\nfrom sklearn.model_selection import KFold\nimport warnings\nwarnings.filterwarnings('ignore')\n","500d2b5d":"income_df = pd.read_csv(\"\/kaggle\/input\/income-classification\/income_evaluation.csv\")\nincome_df.head()","217d4a3c":"#dimensions\nprint(\"Dimensions\",income_df.shape)\nprint(\"\\n\")\n#data types\nprint(income_df.info())","ad157924":"names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n\nincome_df.columns = names","d4d37090":"income_df.describe()","0dbdd0ae":"income_df['sex'].value_counts().plot(kind='bar', rot=\"50\", colormap=\"coolwarm\")","c10dc8bb":"income_df['race'].value_counts().plot(kind='bar', rot=\"50\", colormap=\"coolwarm\")","391153e1":"income_df['occupation'].value_counts().plot(kind='bar', rot=\"50\", colormap=\"coolwarm\")","f38827d3":"income_df['education'].value_counts().plot(kind='bar', rot=\"50\", colormap=\"coolwarm\")","e671d3bd":"income_df['workclass'].value_counts().plot(kind='bar', rot=\"50\", colormap=\"coolwarm\")","b2a247a0":"income_df['native_country'].value_counts().plot(kind='bar', rot=\"90\",colormap=\"coolwarm\")","698171ac":"f, ax = plt.subplots(figsize=(8,6))\nx = income_df['age']\nsns.color_palette(\"hls\", 8)\nx = pd.Series(x, name=\"age\")\nax = sns.distplot(x, bins=10)\nax.set_title(\"Distribution of age variable\")\nplt.show()","ba6504b9":"more_40 = income_df[(income_df['hours_per_week']>= 40) & (income_df['hours_per_week']< 50)]['hours_per_week']\nmore_50 = income_df[(income_df['hours_per_week']>= 50) & (income_df['hours_per_week'] < 60)]['hours_per_week']\nmore_60 = income_df[(income_df['hours_per_week']>= 60) & (income_df['hours_per_week'] < 70)]['hours_per_week']","dd5298ac":"pd.crosstab(more_40,income_df['income']).sum()\npd.crosstab(more_50,income_df['income']).sum()\npd.crosstab(more_60,income_df['income']).sum()","727bb494":"pd.crosstab(income_df['sex'],income_df['income']).plot(kind=\"bar\",figsize=(15,6),colormap=\"coolwarm\")\nplt.title('Income-sex correlation')\nplt.xticks(rotation=0)\nplt.ylabel('Frequency')\nplt.show()","ad049c84":"pd.crosstab(income_df['education'],income_df['income']).plot(kind=\"bar\",figsize=(20,6), colormap=\"coolwarm\")\nplt.title('Income-education correlation')\nplt.xticks(rotation=0)\nplt.ylabel('Frequency')\nplt.show()","40c003a5":"pd.crosstab(income_df['workclass'],income_df['income']).plot(kind=\"bar\",figsize=(14,5), colormap=\"coolwarm\")\nplt.title('Income-workclass correlation')\nplt.xticks(rotation=0)\nplt.ylabel('Frequency')\nplt.show()","da0e54ab":"f, ax = plt.subplots(figsize=(10, 6))\nsns.set_palette(\"Set2\")\nax = sns.boxplot(x=\"income\", y=\"age\", data=income_df)\nax.set_title(\"Income-age correlation\")\nplt.show()","28348ada":"# strip the values of spaces\ndf_obj = income_df.select_dtypes(['object'])\nincome_df[df_obj.columns] = df_obj.apply(lambda x: x.str.strip())","12ac66c8":"def remove_question_mark(dataset):\n    for col in dataset.columns:\n        dataset[col].replace('?', np.NaN, inplace=True)\n    return dataset.info()    \nremove_question_mark(income_df)","6ff9cc33":"observations_0 = income_df.shape[0] \nincome_df.dropna(inplace=True)\nobservations_1 = income_df.shape[0] \nprint(\"Number of rows dropped: {}\".format(observations_0-observations_1))","08eab7b9":"income_df['marital_status'] = income_df['marital_status'].replace([\"Married-civ-spouse\", \"Married-AF-spouse\",  \"Married-spouse-absent\"], 'Married')","6e5c3e3c":"pd.crosstab(income_df['marital_status'],income_df['income']).plot(kind=\"bar\",figsize=(10,4),colormap=\"coolwarm\")\nplt.title('Income-marital_status correlation')\nplt.xticks(rotation=0)\nplt.ylabel('Frequency')\nplt.show()","6d716bd9":"pd.crosstab(income_df['marital_status'],income_df['income'])","75f11276":"income_df[\"capital\"] = income_df[\"capital_gain\"] - income_df[\"capital_loss\"]\nincome_df.drop([\"capital_gain\", \"capital_loss\"], inplace=True, axis=1)","1957188e":"income_df.loc[(income_df[\"native_country\"] != 'United-States'),'native_country']='Other'","6804a2e3":"income_df.drop([\"fnlwgt\", \"education\"], inplace=True, axis=1)","23e2cda0":"income_df['relationship'] = income_df['relationship'].replace([\"Husband\", \"Wife\"], 'Spouse')","956fabf2":"income_df.loc[(income_df[\"income\"] == '<=50K'),'income']= 0\nincome_df.loc[(income_df[\"income\"] == '>50K'),'income']= 1\nincome_df[\"income\"] = income_df[\"income\"].astype(int)\nlabel = income_df[\"income\"]\nincome_df.drop(\"income\", inplace=True, axis=1)","478d06ac":"income_df.loc[(income_df[\"sex\"] == 'Female'),'sex']= 0\nincome_df.loc[(income_df[\"sex\"] == 'Male'),'sex']= 1\nincome_df[\"sex\"] = income_df[\"sex\"].astype(int)","95185c23":"dataplot = sns.heatmap(income_df.corr(), cmap=\"YlGnBu\", annot=True)\nplt.show()","e577742e":"df_encoded = pd.get_dummies(income_df)\ndf_encoded","baa3a9c8":"scaler = MinMaxScaler()\ndf_encoded[['age', 'hours_per_week', \"capital\", \"education_num\" ]] = scaler.fit_transform(df_encoded[['age', 'hours_per_week', \"capital\", \"education_num\" ]])\ndf_encoded","c6bb26ff":"X_train, X_test, y_train, y_test = train_test_split(df_encoded, label, test_size=0.2)\n","5ee75078":"tree_model_overfitted = tree.DecisionTreeClassifier(max_depth = 2000 ) \ntree_model_overfitted.fit(df_encoded, label) \nprint(\"Accuracy of the overfitted model is: \",accuracy_score(tree_model_overfitted.predict(df_encoded), label))   \nplot_confusion_matrix(tree_model_overfitted, df_encoded, label, display_labels = ['<=50K', '>50k'])","7f581e57":"tree_max_depth = range(1,20) \ncross_validation_scores = np.zeros((len(tree_max_depth)+1))\nfor depth in tree_max_depth: \n    tree_model = tree.DecisionTreeClassifier(max_depth = depth) \n    cross_validation_scores[depth] = np.mean(cross_val_score(tree_model, df_encoded, label, cv=5))","38e28f37":"max_accuracy_depth_idx = np.argmax(cross_validation_scores)\nprint(\"The maximum accuracy of \",cross_validation_scores[max_accuracy_depth_idx] ,\"related to max_depth is met in the \",max_accuracy_depth_idx, \" point.\")   \nplt.xticks(list(tree_max_depth)) \nplt.ylabel(\"Mean accuracy across the folds\") \nplt.xlabel(\"Max_Depth\")\nplt.plot(list(tree_max_depth), cross_validation_scores[1:])   ","ad57b3bd":"tree_model_max_depth = tree.DecisionTreeClassifier(max_depth = max_accuracy_depth_idx)  \ntree_model_max_depth.fit(X_train, y_train)  \nplot_confusion_matrix(tree_model_max_depth, X_test, y_test, display_labels = ['<=50K', '>50k'])  \ntree_max_depth_report = classification_report(y_test, tree_model_max_depth.predict(X_test), target_names = ['<=50K', '>50k'], digits=4, output_dict=True)\nprint(pd.DataFrame(tree_max_depth_report))","cc77d9bb":"tree_max_leaf_count = range(2,1000) \ncross_validation_scores = np.zeros((len(tree_max_leaf_count)+2))\nfor leaf_count in tree_max_leaf_count: \n    tree_model = tree.DecisionTreeClassifier(max_leaf_nodes = leaf_count) \n    cross_validation_scores[leaf_count] = np.mean(cross_val_score(tree_model, df_encoded, label, cv=5))","282af8b7":"max_accuracy_leafs_idx = np.argmax(cross_validation_scores)\nprint(\"The maximum accuracy of \",cross_validation_scores[max_accuracy_leafs_idx] ,\"related to max_leaf is met in the \",max_accuracy_leafs_idx, \" point.\")   \nplt.ylabel(\"Mean accuracy across the folds\") \nplt.xlabel(\"Max_Leaf_Count\")\nplt.plot(list(tree_max_leaf_count), cross_validation_scores[2:])  ","48be0481":"tree_model_max_leaf = tree.DecisionTreeClassifier(max_leaf_nodes = max_accuracy_leafs_idx)  \ntree_model_max_leaf.fit(X_train, y_train)  \nplot_confusion_matrix(tree_model_max_leaf, X_test, y_test, display_labels = ['<=50K', '>50k'])  \ntree_max_leaf_report = classification_report(y_test, tree_model_max_leaf.predict(X_test), target_names = ['<=50K', '>50k'], digits=4,output_dict=True)\nprint(pd.DataFrame(tree_max_leaf_report))","d2952a8f":"tree_max_depth = range(1,20) \nbagging_cross_validation_scores = np.zeros((len(tree_max_depth)+1))\nfor depth in tree_max_depth: \n    bagging_model = RandomForestClassifier(max_depth = depth, n_estimators = 100, max_features = X_train.iloc[0].size,random_state=3125)\n    bagging_cross_validation_scores[depth] = np.mean(cross_val_score(bagging_model, df_encoded, label, cv=5))","93b7967e":"max_accuracy_depth_idx = np.argmax(bagging_cross_validation_scores)\nprint(\"The maximum accuracy of \",bagging_cross_validation_scores[max_accuracy_depth_idx] ,\"related to max_depth is met in the \",max_accuracy_depth_idx, \" point.\")   \nplt.ylabel(\"Mean accuracy across the folds\") \nplt.xlabel(\"Max_Depth\")\nplt.plot(list(tree_max_depth), bagging_cross_validation_scores[1:])   ","caf953d7":"bagging_model = RandomForestClassifier(max_depth = max_accuracy_depth_idx, n_estimators = 100, max_features = X_train.iloc[0].size,random_state=3125)\nbagging_model.fit(X_train, y_train) \nplot_confusion_matrix(bagging_model, X_test, y_test, display_labels = ['<=50K', '>50k'])  \nbagging_report = classification_report(y_test, bagging_model.predict(X_test), target_names = ['<=50K', '>50k'], digits=4, output_dict=True)\nprint(pd.DataFrame(bagging_report))","a4af1257":"forest_max_depth = range(10,20)  \nforest_max_features = range(1, X_train.iloc[0].size,3)  \nforest_cross_validation_scores = np.zeros((np.max(forest_max_depth)+1,X_train.iloc[0].size+1))\nfor depth in forest_max_depth:  \n    for features in forest_max_features:\n        forest_model = RandomForestClassifier(max_depth = depth, n_estimators = 100, max_features = features ,random_state=3125)\n        forest_cross_validation_scores[depth][features] = np.mean(cross_val_score(forest_model, df_encoded, label, cv=5))","0fc9a31a":"print(\"The maximum score across the max_depth parameter and the max_features parameter is: \",np.max(forest_cross_validation_scores))","08328265":"for idx,row in enumerate(forest_cross_validation_scores): \n    if np.max(row) == np.max(forest_cross_validation_scores): \n        idx_max = idx \nprint(idx_max,np.argmax(forest_cross_validation_scores[idx_max]))","ff2cb926":"forest_model = RandomForestClassifier(max_depth = idx_max, n_estimators = 100, max_features = np.argmax(forest_cross_validation_scores[idx]) ,random_state=3125)\nforest_model.fit(X_train, y_train) \nplot_confusion_matrix(forest_model, X_test, y_test, display_labels = ['<=50K', '>50k'])  \nforest_report = classification_report(y_test, forest_model.predict(X_test), target_names = ['<=50K', '>50k'], digits=4, output_dict=True)\nprint(pd.DataFrame(forest_report))","165fa0ac":"boosting_max_depth = range(5,20)  \nboosting_learning_rate = [0.1,0.01,0.001]  \nboost_scores = np.zeros((3,15))\nfor lrn_idx,learning_rate in enumerate(boosting_learning_rate): \n    for depth_idx,depth in enumerate(boosting_max_depth):   \n        boosting_model  = GradientBoostingClassifier(learning_rate = learning_rate, n_estimators = 100, max_depth = depth)  \n        boost_scores[lrn_idx][depth_idx] = np.mean(cross_val_score(boosting_model, df_encoded, label, cv=5))","0f80a12c":"boost_lr = 0\nfor idx,row in enumerate(boost_scores): \n    if np.max(row) == np.max(boost_scores): \n        boost_lr = idx \nprint(boosting_learning_rate[boost_lr],boosting_max_depth[np.argmax(boost_scores[boost_lr])])","90a05bf0":"boosting_model = GradientBoostingClassifier(learning_rate = boosting_learning_rate[boost_lr], n_estimators = 100, max_depth = boosting_max_depth[np.argmax(boost_scores[boost_lr])]) \nboosting_model.fit(X_train, y_train) \nplot_confusion_matrix(boosting_model, X_test, y_test, display_labels = ['<=50K', '>50k'])  \nboosting_report = classification_report(y_test, boosting_model.predict(X_test), target_names = ['<=50K', '>50k'], digits=4, output_dict=True)\nprint(pd.DataFrame(boosting_report))","dd91116c":"accuracy_boos = boosting_report[\"accuracy\"]\naccuracy_forest = forest_report[\"accuracy\"]\naccuracy_tree_max_leaf = tree_max_leaf_report[\"accuracy\"]","4b40fc8d":"print(\"Classification metrics for the different algorithms\")\nprint(\"Tree pruned with max_depth:\",tree_max_depth_report, sep = '\\n') \nprint(\"Tree pruned with max_leaf:\",tree_max_leaf_report, sep = '\\n')  \nprint(\"Bagging:\",bagging_report, sep = '\\n')  \nprint(\"Forest:\",forest_report, sep = '\\n') \nprint(\"Boosting:\",boosting_report, sep = '\\n') ","924bb133":"print(pd.Series(boosting_model.feature_importances_, index = df_encoded.columns).sort_values(ascending=False)[0:10])","98d052ee":"model_LR= LogisticRegression()\n\nparameters_log = {\n    'solver': ['liblinear', \"saga\"],\n    'C': [0.1, 1, 0.01],\n    'penalty': ['l1', 'l2', 'elasticnet'],\n    'l1_ratio': [0.5, 0.2, 0.7]\n    }\n\ngrid_log = GridSearchCV(model_LR, param_grid = parameters_log, scoring='accuracy', cv=5)\ngrid_log.fit(X_train,y_train)","b9ea7c86":"print(\"Tuned Hyperparameters :\", grid_log.best_params_)\nprint(\"Accuracy :\",grid_log.best_score_)\n\ny_pred_train_log = grid_log.predict(X_train)\ny_pred_test_log = grid_log.predict(X_test)\naccuracy_log = accuracy_score(y_test.to_list(), y_pred_test_log)\n\nprint(\"Accuracy on train data: {}\".format(accuracy_score(y_train.to_list(), y_pred_train_log)))\nprint(\"Accuracy on test data: {}\".format(accuracy_log))","d14f2c64":"print(\"Test accuracy for Logistic Regression: \",accuracy_log)","b5aa4b8c":"cm_matrix_log = pd.DataFrame(data=confusion_matrix(y_test,y_pred_test_log), columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix_log, annot=True, fmt='d')","409b691d":" check_parameters = {\n     'hidden_layer_sizes': [ (30, 50, 80, 100, 80, 50, 30), (10, 15, 30, 50, 30, 15, 10), (10, 20, 50, 100, 50, 20, 10), (50, 100, 50), (100, 150, 100), (50, 90, 150, 90, 50), (50, 100, 50), (100, 150, 200, 150, 100)],\n     'activation': ['relu'],\n     'solver': ['adam'],\n     'alpha': [1e-1, 1e-2, 1e-3, 1e-4],\n     'learning_rate': ['constant','adaptive'],\n     'learning_rate_init': [1e-2, 1e-3, 1e-4]    \n }\n\n#mlpclassifier = MLPClassifier(verbose=True)\n\n#grid = GridSearchCV(mlpclassifier, check_parameters, cv=5, scoring='accuracy', return_train_score=True, verbose=10)\n#grid.fit(df_encoded, label)\n#print(f'\\n Best params:\\n{grid.best_estimator_}')\n#print(f'\\n Results:\\n{grid.cv_results_}') ","6b21518a":"# mlpclassifier = MLPClassifier(activation='relu', max_iter=400, alpha=0.01, hidden_layer_sizes=(10, 20, 50, 100, 50, 20, 10), learning_rate='constant', learning_rate_init=0.01, solver='adam', verbose=True)\n# metric_values_cv = cross_validate(mlpclassifier, df_encoded, label, cv=5, verbose=3, return_train_score=True, scoring=['accuracy', 'roc_auc', 'precision', 'recall', 'jaccard', 'f1', 'neg_mean_absolute_error', 'neg_mean_squared_error', ])\n# print(metric_values_cv)\nmetric_values_cv = {'fit_time': np.array([30.86233687, 54.97358775, 22.17505503, 49.24040318, 18.96408892]), \n                     'score_time': np.array([0.09203935, 0.08286905, 0.10035205, 0.0903666 , 0.08880067]), \n                     'test_accuracy': np.array([0.85048898, 0.85479861, 0.84731432, 0.85377984, 0.84731432]), \n                     'train_accuracy': np.array([0.85780596, 0.85776452, 0.8480315 , 0.85656859, 0.84977207]), \n                     'test_roc_auc': np.array([0.90782341, 0.90169276, 0.90337428, 0.91180117, 0.90410233]), \n                     'train_roc_auc': np.array([0.91623707, 0.92019307, 0.91119514, 0.91755585, 0.90871577]), \n                     'test_precision': np.array([0.74752475, 0.76843911, 0.77204503, 0.77030568, 0.72784314]), \n                     'train_precision': np.array([0.76895759, 0.77197802, 0.77751423, 0.77016129, 0.73672698]), \n                     'test_recall': np.array([0.60319574, 0.59653795, 0.54830113, 0.58760826, 0.61784288]), \n                     'train_recall': np.array([0.61288711, 0.60822511, 0.54569669, 0.60412852, 0.61688312]), \n                     'test_jaccard': np.array([0.50110619, 0.50564334, 0.47190367, 0.5, 0.50189292]), \n                     'train_jaccard': np.array([0.51757593, 0.51559633, 0.47199424, 0.51184767, 0.50545703]), \n                     'test_f1': np.array([0.66764923, 0.67166417, 0.64121543, 0.66666667, 0.66834714]), \n                     'train_f1': np.array([0.68210877, 0.68038741, 0.64129903, 0.6771154 , 0.67149977]), \n                     'test_neg_mean_absolute_error': np.array([-0.14951102, -0.14520139, -0.15268568, -0.14622016, -0.15268568]), \n                     'train_neg_mean_absolute_error': np.array([-0.14219404, -0.14223548, -0.1519685 , -0.14343141, -0.15022793]), \n                     'test_neg_mean_squared_error': np.array([-0.14951102, -0.14520139, -0.15268568, -0.14622016, -0.15268568]), \n                     'train_neg_mean_squared_error': np.array([-0.14219404, -0.14223548, -0.1519685 , -0.14343141, -0.15022793])}\n\n\nprint('FFCV Results:')\n\nprint('Train accuracy: ', metric_values_cv['train_accuracy'].mean().round(4))\nprint('Test accuracy: ', metric_values_cv['test_accuracy'].mean().round(4))\nprint('Train AUC: ', metric_values_cv['train_roc_auc'].mean().round(4))\nprint('Test AUC: ',metric_values_cv['test_roc_auc'].mean().round(4))\n\nprint('Train precision: ', metric_values_cv['train_precision'].mean().round(4))\nprint('Test precision: ', metric_values_cv['test_precision'].mean().round(4))\nprint('Train recall: ', metric_values_cv['train_recall'].mean().round(4))\nprint('Test recall: ', metric_values_cv['test_recall'].mean().round(4))","5bb403ca":"mlpclassifier = MLPClassifier(activation='relu', max_iter=400, alpha=0.01, hidden_layer_sizes=(10, 20, 50, 100, 50, 20, 10), learning_rate='constant', learning_rate_init=0.01, solver='adam', verbose=True)\nmlpclassifier.fit(X_train, y_train)","2d255def":"y_pred_mlp = mlpclassifier.predict(X_test)\naccuracy_mlp = accuracy_score(y_test,y_pred_mlp)","1eb19f0a":"print(\"Test accuracy for MLP: \",accuracy_mlp)","dc1a9156":"cm_matrix_mlp = pd.DataFrame(data=confusion_matrix(y_test,y_pred_mlp), columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix_mlp, annot=True, fmt='d')","a5414509":"pipelines = []\n\npipelines.append(('SVC', Pipeline([('SVC', SVC())])))\npipelines.append(('NuSVC', Pipeline([('NuSVC', NuSVC())])))\npipelines.append(('SVR', Pipeline([('SVR', SVR())])))\npipelines.append(('NuSVR', Pipeline([('NuSVR', NuSVR())])))\n\nresults = []\nnames = []\nfor name, model in pipelines:\n    kfold = KFold(n_splits=5)\n    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='neg_mean_squared_error')\n    results.append(cv_results)\n    names.append(name)\n    msg = f\"{name}: {cv_results.mean()} (std: {cv_results.std()})\"\n    print(msg)\n    \nprint(results)\nprint(names)","833ddd62":"svc_model = SVC()\n\nparameters = {\n    'kernel': ['rbf', 'linear', 'sigmoid', 'poly'],\n    'C': [0.0001, 0.001, 0.1, 1, 10]}\n\ngrid = GridSearchCV(model, param_grid = parameters, scoring='accuracy', cv=5)\ngrid.fit(X_train,y_train)","4dd9d4eb":"print(\"Tuned Hyperparameters :\", grid.best_params_)\nprint(\"Accuracy :\",grid.best_score_)\n\ny_pred_train = grid.predict(X_train)\ny_pred_test = grid.predict(X_test)\naccuracy_svc = accuracy_score(y_test.to_list(), y_pred_test)\n\nprint('Accuracy on train data: {}'.format(accuracy_score(y_train.to_list(), y_pred_train)))\nprint('Accuracy on test data: {}'.format(accuracy_svc))","6d79efc2":"cm_matrix_SVC = pd.DataFrame(data=confusion_matrix(y_test,y_pred_test), columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix_SVC, annot=True, fmt='d')","9aebdd82":"final_results = {\"Accuracy\": {\"logisticRegression\": round(accuracy_log,3),\n                \"SVC\":round(accuracy_mlp,3),\n                \"MLP\": round(accuracy_svc,3),\n                \"Boosting\": round(accuracy_boos,3),\n                \"RandomForest\":round(accuracy_forest,3),\n                \"DecisionTree\":round(accuracy_tree_max_leaf,3)}}\nprint(pd.DataFrame(final_results))","d2b6f28d":"Group people who worked more than 40h in 3 categories","b1aa4387":"race bar chart\n","1ed1c086":"### 8. MLPClassifier","9ad97a35":"#### Train-Test split","6ed80da9":"##### \u2018Capital gain\u2019 and \u2018capital loss\u2019 can be converted into one \u2018capital\u2019 variable which is calculated by subtracting capital loss from capital gain.","d8cb5a69":"### 3. Exploratory Data Analysis","87014ecc":"### Analysis between the target variable and other explanatory variables","86aaa2f5":"#### 8.2 Predictions MLPClassifier","6f0b3857":"##### There are not many individuals in each distinct native country category other than USA, therefore we bin them. Changing the \u2018Native country\u2019 levels to \u2018USA\u2019 or \u2018Other\u2019 will increase this attributes\u2019 predictive power when modelling.","16b28261":"#### 7.1 Declare the model and fit into train set ","adf6b825":"### 2.Read the dataset","5528cff4":"workclass bar chart","6364c2e3":"Correlation plot","e95d33a1":"Next i will be searching for optimal pruning hyperparameters using a five-fold cross-validation approach and utilising the mean accuracy of the folds as a comparison metric.\n","08b7c081":"sex bar chart\n","aaab6622":"#### 6.3.1. Predictors \n\nNow that the best algorithm has been established, we can see what the most important predictors were:","7bca201f":"#### 3.1 Visualization","dd45e070":"### 1.Import libraries","d5523db3":"#### 8.1 Declare the model and fit into train set ","d54648d1":"Drop label variable","f4116edb":"#### 9.4 Confusion Matrix SVC","adae711a":"##### The \u2018fnlwgt\u2019 feature stands for final weight. It has no predictive power since it is a feature aimed to allocate similar weights to people with similar demographic characteristics. \u2018Education\u2019 is a label on \u2018education_num\u2019 (number of years of education). Both columns are dropped.","e22239d6":"#### 2.1 View dimensions of the dataset and data types of columns","62d46fe2":"##### First categorical feature to take into consideration is Marital status. It has has 3 similar \u2018married\u2019 levels, these can be combined into one \u2018married\u2019 level.","b14c7afb":"I can see that the accuracy is in the vecinity of one, so the algorithm in itself is capable of capturing the relationship between the predicted label and the predictors. \nWhile there are misclassified samples, they are in the threshold that can be attributed to noise in the data.","e996c058":"### 6.1 Decision Trees","e7df2a50":"##### 6.2.1. Bagging","06478e9a":"### 4. Feature engineering","bff09b58":"##### 6.2.2. Random Forests","b65f9b0c":"#### 9.2. Support Vector Classifier\u00b6\n","696dbe78":"Using the best hyperparameters obtained after the GridSearchCV procedure, five fold cross validation (FFCV) was performed and the results obtained following this procedure can be found in the metric_values_cv dictionary.","807d8fab":"The second pruning approach by limiting the number of leafs that the tree can build is more fine grained (and more computationally expensive to check) but it only produced a 0.005 improvement in accuracy. \nWhile the accuracy improvement is not notable, there can be seen a slight improvement in the less representated classes recall (<50k) ","89d64ee4":"#### 8.4 Confusion Matrix MLPClassifier","cb9cc3d2":"#### 9.1 Cross validation with SV algorithms\n","0b19cf8d":"#### 7.3 Performance for Logistic Regression","fcb214e9":"Finally, the optimal architecture contains a number of 260 neurons divided into 7 layers. The training was done using the Adam optimizer and a constant learning rate of 1e-2.","8bba0535":"Judging by the weighted accuracy average, the classification methods are fairly similar in performance, but given that this classification problem has imbalanced label counts, we should take a look at the unweighted statistics: \nThere's a clear progression in unweighted f1 scores making me chose the boosting algorithm as the better fitted algorithm for this problem.","5376d1c9":"##### 6.2.3. Gradient Boosting","c745a712":"Encode binary variable \"Sex\"","dadbacc9":"##### The relationship variable has two similar values, wife and husband, which signals the same idea, of being a spouse.","4214bc6f":"The income class is most strongly corelated with having a spouse, the capital gain, education, age and hours worked per week while having a managerial ocupation, sex, self employment, speciality and working in the private sector also play a role.","ef902337":"From the above table, we can extract the following information:\n* there are nine categorical features and six numeric features\n* there are no missing values\n* income column is the target variable ","3bb3715e":"#### 6.1.1 Tuning a simple Decision Tree model \nFirst I will be running the algorithm on the entire data set to see if it is capable of learning the model:","b85789a8":"occupation bar chart\n","7dc43fb8":"For this task we've settled on trying the following machine learning algorithms and choosing the best one based on various performance metrics: \n* Multylayer Perceptron (MLP) \n* Logistic Regression \n* Support-vector Machine (SVM)\n* Decision Trees ","da7b10a7":"#### 4.2 Encode categorical variables","53aba9bc":"native_country bar chart","21b6ac91":"### The problem statement\nThe main task of this notebook is to build machine learning models and compare them, considering their ability to predict whether a person makes over 50k a year or not.","cfaa6e89":"### 7.Logistic Regression","0c9b8f80":"### 5. Building the classification model","d004ee5a":"#### 6.2. Ensemble learning \nNext I will try to further improve the model by employing ensemble learning techniques which use the Tree Classifier model as a weak learner.  \nI will use the same five-fold cross-validation methodology that's been used in the past chapter in order to assess the model's performance.\n","45264a19":"#### 2.3 Statistical measures","1e38ad4b":"The number of rows dropped is not a neglectable amount, but having in mind that we still have 30 000 observations and that the missing values couldn't have been imputed in a decent manner, we will proceed accordingly. ","2be3b336":"The first pruning approach of limiting the maximum depth for the tree produced the best results around the 10 depth point.","f1e95e0b":"#### 4.1 Missing values\nA note to be made is that question mark appears many times throughout the observations. This is an encoder for missing values, but python didn't considered that because it looks for Nan or None. Thus, we will replace ? by Nan.","af6ad002":"Distribution of age variable","5121f789":"#### 7.2 Predictions Logistic Regression","00511448":"#### 8.3 Performance for MLPClassifier","769d5f0a":"#### 9.3. Performance for SVC\n","b37802fd":"Relationship between age and income using a boxplot\nYounger people make less money, which was expected. People earning less than 50k have a median of ~35 years and those earning more, a median of ~45.","4e1ee68a":"Using MLP Classifier, a search for the optimal hyperparameters was performed using GridSearchCV and dividing the dataset into 5 folds. Both small and deeper architectures, various learning rate values as well as various values for the regularization term parameter were tried. ","a4f4b35b":"### 9. Support Vector Algorithms","eb4ee28a":"#### 4.3 Feature Scaling\n##### Since the data doesn't follow a Gaussian distribution, we'll use normalize instead of standardization.","0bca2a5c":"#### 6.3 Performance for trees methods","c4a84eba":"#### 2.2 Given that some columns contain dashes, we will transform them in underscore in order to access them","817b5c8d":"Now, we can observe that there are 3 columns that have missing values, all categorical. In this situation, imputation could have been a solution, using the Mode. But, from the charts we plotted, we can see that all 3 variables have a highly skewed ditribution. Replacing the missing values with the Mode would make the data even more imbalanced. Let's consider dropping the rows.","87a2e5f9":"education bar chart\n"}}