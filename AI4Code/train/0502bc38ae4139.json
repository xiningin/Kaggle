{"cell_type":{"567d97cf":"code","6d44ac57":"code","ee893f9c":"code","512d0c58":"code","2763cf58":"code","614df020":"code","ae43ad5b":"code","863c1f84":"code","36c3b8d2":"code","5b2634e7":"code","ecc2c082":"code","d1da78f8":"code","eba58e47":"code","1795b7eb":"code","8c74569d":"code","75215f57":"code","5ff5ddbc":"code","9345724e":"code","a6087f3a":"code","57a07350":"code","e016fc56":"code","94e247a6":"code","14518806":"code","ed050bd0":"code","aba2470d":"code","2afc5b26":"code","a7b28268":"code","5d207367":"code","b5b24a13":"code","7a012b3d":"code","949e06e2":"code","030364d0":"code","ea92c463":"code","720f2ef9":"code","4323c0ec":"code","5665e51f":"code","05fb59f8":"code","82f59292":"code","b605bbf6":"code","9cb1eb70":"code","547d35df":"code","2f9935d6":"code","66cd18fc":"code","bf66a517":"code","0a59e090":"code","006aacc4":"code","13acf52c":"code","149cd20e":"code","596df75a":"code","7f7593a2":"code","05f3a70b":"code","1228ec31":"code","4c397a1b":"code","697b6403":"markdown","a191ac8b":"markdown","eed078bb":"markdown","5fe5a655":"markdown","81b54ca4":"markdown","8ba3df9e":"markdown","a0ca9afc":"markdown","20071fa6":"markdown","48a0abc9":"markdown","00845068":"markdown","aba9c83e":"markdown","d0cff6b6":"markdown","6fb35020":"markdown","587c3e78":"markdown","8dfa727e":"markdown","241847ca":"markdown","4cd351a6":"markdown","6b9e0b3c":"markdown","8d6a80cc":"markdown","e4a911be":"markdown","40da1cc9":"markdown","cef74dd8":"markdown","2409eec8":"markdown","f4165570":"markdown","d571c3fb":"markdown","94caabba":"markdown","df748a65":"markdown","d572c3ed":"markdown","58fa3b94":"markdown","1548676e":"markdown","4da7e12f":"markdown"},"source":{"567d97cf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\n# print(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","6d44ac57":"training_data = pd.read_csv(\"..\/input\/train.csv\")","ee893f9c":"# Let's import the tools\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set() # sets as default for plots","512d0c58":"training_data['female'] = training_data.Sex == 'female'\ntraining_data.head()","2763cf58":"# This time, before filling NaN (such as age), I am going to look to see if there are other data which provide signal\ntraining_data.isnull().sum()","614df020":"# Let's see how many unique values there are\nn = 0\nfor col in training_data:\n    print (training_data.dtypes.index[n], \"         \", len(training_data[col].unique()))\n    n += 1","ae43ad5b":"# one quick way to view the data is to show a bar chart\nsurvived = training_data[training_data['Survived']==1]['Sex'].value_counts()\nperished = training_data[training_data['Survived']==0]['Sex'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\ndf.plot(kind='bar', stacked = False, figsize = (6,4.5));","863c1f84":"# however it gets repetitive to continually type so much, so let's put it in a function\ndef show_bar_chart(feature):\n    survived = training_data[training_data['Survived']==1][feature].value_counts()\n    perished = training_data[training_data['Survived']==0][feature].value_counts()\n    df = pd.DataFrame([survived, perished])\n    df.index = ['Survived', 'Perished']\n    df.plot(kind='bar', stacked = False, figsize = (6,4.5));","36c3b8d2":"show_bar_chart('Pclass')","5b2634e7":"show_bar_chart('SibSp')","ecc2c082":"survived = training_data[training_data['Survived']==1]['SibSp'].value_counts()\nperished = training_data[training_data['Survived']==0]['SibSp'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\ng = df.plot(kind='bar', stacked = False, figsize = (6,4.5))\ng.set(ylim=(0,20));","d1da78f8":"show_bar_chart('Parch')","eba58e47":"survived = training_data[training_data['Survived']==1]['Parch'].value_counts()\nperished = training_data[training_data['Survived']==0]['Parch'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\ng = df.plot(kind='bar', stacked = False, figsize = (6,4.5))\ng.set(ylim=(0,70));","1795b7eb":"show_bar_chart('Embarked')","8c74569d":"# Let's use a for loop to splice the title out of the strings. I've seen people use regex and str.extract, but I am not yet experienced with those so I'll just build my own way to do it\ntitle = []\nfor i in training_data['Name']:\n    period = i.find(\".\")\n    comma = i.find(\",\")\n    title_value = i[comma+2:period]\n    title.append(title_value)\ntraining_data['title'] = title\n# Sweet, that works. Let's use a dictionary to make sure that synonyms typed differently (like Ms vs Miss) are understood the same.","75215f57":"# I want to make sure I know how much of each title there is\nfrom collections import Counter\ntitle_count = pd.DataFrame([Counter(title).keys(), Counter(title).values()])\ntitle_count.head()","5ff5ddbc":"rev_list = []\nfor row in training_data.title:\n    if row == \"Rev\":\n        rev_list.append(True)\n    else:\n        rev_list.append(False)\n\nis_rev = pd.Series(rev_list)\nis_rev.head()","9345724e":"training_data[is_rev]","a6087f3a":"masters_list = []\nfor row in training_data.title:\n    if row == \"Master\":\n        masters_list.append(True)\n    else:\n        masters_list.append(False)\n\nis_master = pd.Series(masters_list)\ntraining_data[is_master]","57a07350":"survived = training_data[training_data['Survived']==1]['title'].value_counts()\nperished = training_data[training_data['Survived']==0]['title'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\n\n# fill NaN with 0\ndf = df.fillna(0)\n\n#calc survival rates\nsurvival_rates = []\nfor i in df.dtypes.index:\n    survival_rate = round((df[i][0]\/(df[i][0] + df[i][1])),4)\n    survival_rates.append([i,survival_rate])","e016fc56":"# making a df for survival rates\ndfsr = pd.DataFrame(survival_rates)\ndfsr.columns = ['title', 'survival_rate']\ndfsr","94e247a6":"# When ready, we'll clean the titles into a new cleaned_title list\ntitle_arr = pd.Series(title)\ntitle_dict = {\n    'Mr' : 'Mr',\n    'Mrs' : 'Mrs',\n    'Miss' : 'Miss',\n    'Master' : 'Master',\n    'Don' : 'Formal',\n    'Rev' : 'Religious',\n    'Dr' : 'Academic',\n    'Mme' : 'Mrs',\n    'Ms' : 'Miss',\n    'Major' : 'Formal',\n    'Lady' : 'Formal',\n    'Sir' : 'Formal',\n    'Mlle' : 'Miss',\n    'Col' : 'Formal',\n    'Capt' : 'Formal',\n    'the Countess' : 'Formal',\n    'Jonkheer' : 'Formal',\n}\n\ncleaned_title = title_arr.map(title_dict)\ntraining_data['cleaned_title'] = cleaned_title\n\ncleaned_title_count = pd.DataFrame([Counter(cleaned_title).keys(), Counter(cleaned_title).values()])\ncleaned_title_count.head()\n\n# I'm not sure if this replacement\/dict is the best.","14518806":"survived = training_data[training_data['Survived']==1]['cleaned_title'].value_counts()\nperished = training_data[training_data['Survived']==0]['cleaned_title'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\n\n# fill NaN with 0\ndf = df.fillna(0)\n\n#calc survival rates\nsurvival_rates_cleaned = []\nfor i in df.dtypes.index:\n    survival_rate = round((df[i][0]\/(df[i][0] + df[i][1])),4)\n    survival_rates_cleaned.append([i,survival_rate])\n\n# making a df for survival rates\ndfsrf = pd.DataFrame(survival_rates_cleaned)\ndfsrf.columns = ['title', 'survival_rate']\ndfsrf","ed050bd0":"training_data.isnull().sum()","aba2470d":"embarked_nulls_list = []\nfor row in training_data.Embarked:\n    if pd.isnull(row):\n        embarked_nulls_list.append(True)\n    else:\n        embarked_nulls_list.append(False)\n\nembarked_null = pd.Series(embarked_nulls_list)\ntraining_data[embarked_null]","2afc5b26":"survived = training_data[training_data['Survived']==1]['Embarked'].value_counts()\nperished = training_data[training_data['Survived']==0]['Embarked'].value_counts()\ndf = pd.DataFrame([survived, perished])\ndf.index = ['Survived', 'Perished']\n\n# fill NaN with 0\ndf = df.fillna(0)\n\n#calc survival rates\nsurvival_rates_cleaned = []\nfor i in df.dtypes.index:\n    survival_rate = round((df[i][0]\/(df[i][0] + df[i][1])),4)\n    survival_rates_cleaned.append([i,survival_rate])\n\n# making a df for survival rates\ndfsre = pd.DataFrame(survival_rates_cleaned)\ndfsre.columns = ['embarked', 'survival_rate']\ndfsre","a7b28268":"test_data = pd.read_csv(\"..\/input\/test.csv\")\ntest_data.isnull().sum()","5d207367":"training_data['Embarked'] = training_data['Embarked'].fillna(\"S\") ","b5b24a13":"# OHE \ntraining_data = pd.concat([training_data, pd.get_dummies(training_data['Embarked'])], axis = 1)\n\n# Port mapping in order of passenger pickups\nport = {\n    'S' : 1,\n    'C' : 2,\n    'Q' : 3\n}\ntraining_data['pickup_order'] = training_data['Embarked'].map(port)","7a012b3d":"training_data.head()","949e06e2":"training_data.isnull().sum()","030364d0":"age_nulls = []\nfor row in training_data.Age:\n    if pd.isnull(row):\n        age_nulls.append(True)\n    else:\n        age_nulls.append(False)\n\nage_null = pd.Series(age_nulls)\ntraining_data[age_null]","ea92c463":"# First I group by some relevant features. I don't want to overfit, but using gender, title, and ticket class, we can get a fair number of groups.\ngrouped = training_data.groupby(['female','Pclass', 'cleaned_title'])  \n# I can also view the median age by group\ngrouped.Age.median()","720f2ef9":"# We'll check the counts to make sure it's not toooo overfit\ngrouped.Age.count()","4323c0ec":"# It looks like there may be some overfit with some of the more rare titles, but there's not much we can do about that. ","5665e51f":"# Now I want to fill the NaN will the medians from those groups\ntraining_data['Age'] = grouped.Age.apply(lambda x: x.fillna(x.median()))","05fb59f8":"training_data.isnull().sum()","82f59292":"# Voila!","b605bbf6":"# Encode childhood\ntraining_data['child'] = training_data.Age < 16","9cb1eb70":"# Using OHE to assign each title its own column as they are categorical but not necessarily ordinal\ntraining_data = pd.concat([training_data, pd.get_dummies(training_data['cleaned_title'])], axis = 1)","547d35df":"training_data.head()","2f9935d6":"training_data['Cabin'] = training_data['Cabin'].fillna(\"U\") ","66cd18fc":"# I am sure there are ways to do this with less code but I don't know those yet.\ncabin_group = []\nfor i in training_data['Cabin']:\n    cabin_group.append(i[0])\n\ntraining_data['cabin_group'] = cabin_group","bf66a517":"# Dummies for cabin group\ntraining_data = pd.concat([training_data, pd.get_dummies(training_data['cabin_group'])], axis = 1)","0a59e090":"training_data['family_size'] = training_data.Parch + training_data.SibSp + 1","006aacc4":"training_data.head()","13acf52c":"# Dummies for Pclass\ntraining_data = pd.concat([training_data, pd.get_dummies(training_data['Pclass'])], axis = 1)","149cd20e":"training_data.head()","596df75a":"training_data.dtypes.index","7f7593a2":"# wait, what's this? Are $0 fare people crew members? Does that provide signal?\nmin(training_data['Fare'].unique())","05f3a70b":"# How many were there? ... 15\nfree_board = []\nfor row in training_data.Fare:\n    if row == 0:\n        free_board.append(True)\n    else:\n        free_board.append(False)\n\nwas_free = pd.Series(free_board)\ntraining_data[was_free]","1228ec31":"# Well, Johnkheer. John Reuchlin was most assuredly not a crew member, which is what I was hoping free tickets would indicate. But hm, what's that LINE ticket?\nLINE_ticket = []\nfor row in training_data.Ticket:\n    if row == \"LINE\":\n        LINE_ticket.append(True)\n    else:\n        LINE_ticket.append(False)\n\nwas_LINE = pd.Series(LINE_ticket)\ntraining_data[was_LINE]","4c397a1b":"# here are the models I'll use for a first-try\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n# here are the metrics I'll check them with\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score\n# and the code to split the test\/train data\nfrom sklearn.model_selection import train_test_split\n\n# I also want to split the training_data dataframe into a training and testing portion\ntrain_baseline, test_baseline = train_test_split(training_data, random_state = 0)\n\nfeatures = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare', 'female', 'child', 'S', 'C', 'Q', 'pickup_order', 'Academic', 'Formal', 'Master', 'Miss', 'Mr', 'Mrs', 'Religious', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'T', 'U', 'family_size', 1, 2, 3]\ntarget = 'Survived'\n# Define the models\n################################################################################################### NOTE - I cannot justify setting random_state=0 for any reason other than reproducability of results\nmodel01 = RandomForestClassifier(random_state=0);\nmodel02 = DecisionTreeClassifier(random_state=0);\nmodel03 = LogisticRegression();\n\n# Fit the models\nmodel01.fit(train_baseline[features], train_baseline[target]);\nmodel02.fit(train_baseline[features], train_baseline[target]);\nmodel03.fit(train_baseline[features], train_baseline[target]);\n\n# Define a function to make reading recall score easier\ndef printTestRecall(model_number):\n    print(\"Test Recall: \", round(recall_score(test_baseline[target], model_number.predict(test_baseline[features]))*100,2), \"%\")\n\n# Print results\nprint(\"Random Forest Classifier\")\nprintTestRecall(model01);\nprint(\"\\n\\nDecision Tree Classifier\")\nprintTestRecall(model02);\nprint(\"\\n\\nLogistic Regression\")\nprintTestRecall(model03);","697b6403":"This chart shows that people in first class were more likely to survive, people in third class were more likely to perish, and people in second class had an even chance.","a191ac8b":"With this information, it appears that people with 1 or two siblings\/spouse were more likely to survive than average. People with 1 sibling\/spouse significantly more so.","eed078bb":"## Fare","5fe5a655":"Southhampton has the lower survival rate. ","81b54ca4":"## Now let's add a child column","8ba3df9e":"## Title","a0ca9afc":"## Let's get rid of the null cabin values \nby filling with U for unknown\nAnd then add dummies","20071fa6":"## While we're dummying everything, it may make sense to get dummies for Pclass as well","48a0abc9":"Now there are a few options to encode it. I can simply map it to numerical values, or I can get_dummies (one-hot encoding\/OHE). <br>\nThere is an argument for both. The OHE argument is that the embarked locations are not scaled in a normal way. The map argument is that they are scaled by order of onboarding. <br>\nSo I'll do both.","00845068":"This chart shows that people with 1 sibling\/spouse aboard were more likely to survive than not. Otherwise, it seems people were more likely to die.  Let's look closer","aba9c83e":"Interestingly, this appears to show that while people who boarded at port S and Q were more likely to die than survive (as is to be expected), people who boarded at port C were more likely to survive.\nNote that the NaN values are not charted here. ","d0cff6b6":"Frankly, this looks to me to have too many small-count titles to be really useful. However, I need to verify my cleaned_titles thing is actually reasonable. Time to google to determine what types of titles were each. <br>\nAfter googlin', I've decided to denote formal honorifics with \"Formal\", \"Religious\", or \"Academic\".\n","6fb35020":"Wonder what happened to Thomas Storey. Must be in the Test set. Either way, it's interesting, but not going to provide signal.","587c3e78":"Well I don't see anything here to improve my results.","8dfa727e":"## Exploring Title Information","241847ca":"So, I still have to fix the NaN","4cd351a6":"## Ticket","6b9e0b3c":"## Another possible step is to combine Parch and SibSp into a single number for family size\nThe total may have a relationship inverse to either part","8d6a80cc":"So, both of these people embarked at Southhampton. I know this because Google. However, I don't think that's a good habit - in some sense, yes, it makes sense to fix data where possible. But I have no idea that every person with a missing value departed from Southhampton, so it's dishonest to just fillna with S. <br>\nTherefore it really only seems right that I make a solution which can generalize. And that may be filling S. ","e4a911be":"Ah! My suspicions appear valid. It looks like Master is a title for a young boy. There are a few median values, but I will look into if those were inserted by fillna. <br>\n<br>\nSo I want to find a better way of filling in the NaN for ages. Let's see if women have a similar title by age by graphing the average age by title.","40da1cc9":"# Calculating new results","cef74dd8":"As one would assume, gender is important as females were much more likely to live.","2409eec8":"A little internet research on that Ticket and those names led me to this:\n> The British coal strike caused scheduling problems and Philadelphia's westbound voyage was cancelled, with William and several other shipmates; Andrew Shannon [Lionel Leonard], Alfred Johnson, William Henry T\u00f6rnquist, Alfred Carver and Thomas Storey) forced to travel aboard Titanic as passengers.","f4165570":"## Let's encode the cleaned titles so they are usable","d571c3fb":"Now let's look at the categorical features we found before:\n* Pclass\n* Sex (proxied by 'female' as well)\n* SibSp (# of siblings + spouse)\n* Parch (# of parents + children)\n* Embarked","94caabba":"This makes it seem like people with one or two parents\/ children were more likely to survive, but let's look closer:","df748a65":"# Feature Engineering\nNow let's see if we can make other features usable. Like information from cabin, title from name, etc. <br>\nThe first feature I'll address is Title, which is a substring within the Name field. <br>\nIn Excel, the formula I'd use is =RIGHT(LEFT(D2,SEARCH(\".\",D2)),LEN(LEFT(D2,SEARCH(\".\",D2)))-SEARCH(\",\",D2)-1) where D is the name column and D2 is the first non-header row in the name column. Then I'd drag-fill downward to iterate that formula for each row. <br>\nThere are a few different ways in Python, but splicing is what I'm familiar with at the moment, so I'll try that first.","d572c3ed":"People with one or two parent\/children aboard were more likely to survive, but the sample sizes are negligible for numbers above that.","58fa3b94":"## Dealing with Embarked nulls","1548676e":"# Results for further exploration\nSo, what I did today has helped the Logistic Regression algorithm a lot. However, it's made a hot mess of the Decision Tree Classifier (probably because of all the One-Hot Encoding) and hasn't really improved the random forest.\n|","4da7e12f":"Well, there are no missing Embarked values in the test set. So I'll just fill with S and call it good. We'll need to encode it after."}}