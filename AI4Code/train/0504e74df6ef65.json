{"cell_type":{"93486c7f":"code","f4c401d5":"code","9d36bdce":"code","95eae697":"code","3d0a4c98":"code","403c9d9f":"code","43a8a3d7":"code","fc2cb7b6":"code","e233b14d":"code","cbcb647f":"code","25860faa":"code","5883f33b":"code","14c8687f":"code","958747f7":"code","c4e7e6e1":"code","3439a960":"code","5b0fa100":"code","d99a2a15":"code","d9020f82":"code","d32c1bff":"code","ee78d7cc":"code","117dd6aa":"code","27500a13":"code","f23d6844":"code","79b76eb4":"code","e994a666":"code","e81d3995":"markdown","ea6dfc33":"markdown","af431c28":"markdown","788d0aed":"markdown","f5805144":"markdown","d5227fd9":"markdown","7d2f120d":"markdown","a51e17e9":"markdown","b9519f64":"markdown","7bc8e344":"markdown","a6d744aa":"markdown","6623cf37":"markdown","e8370173":"markdown","58e05346":"markdown","8734e5ae":"markdown","eb841caf":"markdown","23b357c0":"markdown","8aa29479":"markdown","166bcba9":"markdown","94fd6e20":"markdown"},"source":{"93486c7f":"%matplotlib inline\nimport os\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn import tree\nfrom sklearn import metrics","f4c401d5":"data = pd.read_csv(os.path.join(\"..\/input\", \"loan_sub.csv\"), sep=',')","9d36bdce":"data.columns","95eae697":"# safe_loans =  1 => safe\n# safe_loans = -1 => risky\ndata['safe_loans'] = data['bad_loans'].apply(lambda x: 1 if x == 0 else -1)\ndata = data.drop(['bad_loans'], axis = 1)","3d0a4c98":"data['safe_loans'].value_counts(normalize=True)","403c9d9f":"cols = ['grade', 'term','home_ownership', 'emp_length']\ntarget = 'safe_loans'\n\ndata = data[cols + [target]]\ndata.head()","43a8a3d7":"data['safe_loans'].value_counts()","fc2cb7b6":"# use the percentage of bad and good loans to under-sample the safe loans.\nbad_ones = data[data[target] == -1]\nsafe_ones = data[data[target] == 1]\npercentage = len(bad_ones) \/ len(safe_ones)\n\nrisky_loans = bad_ones\nsafe_loans = safe_ones.sample(frac = percentage, random_state = 33)\n\n# combine two kinds of loans\ndata_set = pd.concat([risky_loans, safe_loans], axis = 0)","e233b14d":"data_set[target].value_counts(normalize=True)","cbcb647f":"data_set[target].value_counts()","25860faa":"def dummies(data, columns=['pclass','name_title','embarked', 'sex']):\n    for col in columns:\n        data[col] = data[col].apply(lambda x: str(x))\n        new_cols = [col + '_' + i for i in data[col].unique()]\n        data = pd.concat([data, pd.get_dummies(data[col], prefix=col)[new_cols]], axis=1)\n        del data[col]\n    return data","5883f33b":"#grade, home_ownership, target\ncols = ['grade', 'term','home_ownership', 'emp_length']\ndata_set = dummies(data_set, cols)\ndata_set.head()","14c8687f":"train_data, test_data = train_test_split(data_set, test_size = 0.2, random_state = 33)\ntrainX, trainY = train_data[train_data.columns[1:]], pd.DataFrame(train_data[target])\ntestX, testY = test_data[test_data.columns[1:]], pd.DataFrame(test_data[target])","958747f7":"def count_errors(labels_in_node):\n    if len(labels_in_node) == 0:\n        return 0\n    \n    positive_ones = labels_in_node.apply(lambda x: x == 1).sum()\n    negative_ones = labels_in_node.apply(lambda x: x == -1).sum()\n    \n    return min(positive_ones, negative_ones)\n\n\ndef best_split(data, features, target):\n    # return the best feature\n    best_feature = None\n    best_error = 2.0 \n    num_data_points = float(len(data))  \n\n    for feature in features:\n        \n        # \u5de6\u5206\u652f\u5bf9\u5e94\u5f53\u524d\u7279\u5f81\u4e3a0\u7684\u6570\u636e\u70b9\n        left_split = data[data[feature] == 0]\n        \n        # \u53f3\u5206\u652f\u5bf9\u5e94\u5f53\u524d\u7279\u5f81\u4e3a1\u7684\u6570\u636e\u70b9\n        right_split = data[data[feature] == 1]\n        \n        # \u8ba1\u7b97\u5de6\u8fb9\u5206\u652f\u91cc\u72af\u4e86\u591a\u5c11\u9519\n        left_misses = count_errors(left_split[target])            \n\n        # \u8ba1\u7b97\u53f3\u8fb9\u5206\u652f\u91cc\u72af\u4e86\u591a\u5c11\u9519\n        right_misses = count_errors(right_split[target])\n            \n        # \u8ba1\u7b97\u5f53\u524d\u5212\u5206\u4e4b\u540e\u7684\u5206\u7c7b\u72af\u9519\u7387\n        error = (left_misses + right_misses) * 1.0 \/ num_data_points\n\n        # \u66f4\u65b0\u5e94\u9009\u7279\u5f81\u548c\u9519\u8bef\u7387\uff0c\u6ce8\u610f\u9519\u8bef\u8d8a\u4f4e\u8bf4\u660e\u8be5\u7279\u5f81\u8d8a\u597d\n        if error < best_error:\n            best_error = error\n            best_feature = feature\n    return best_feature","c4e7e6e1":"def entropy(labels_in_node):\n    # \u4e8c\u5206\u7c7b\u95ee\u9898: 0 or 1\n    \n    n = len(labels_in_node)\n    s1 = (labels_in_node == 1).sum()\n    \n    if s1 == 0 or s1 == n:\n        return 0\n    \n    p1 = float(s1) \/ n\n    p0 = 1 - p1\n    return -p0 * np.log2(p0) - p1 * np.log2(p1)\n\n\ndef best_split_entropy(data, features, target):\n    \n    best_feature = None\n    best_info_gain = float('-inf') \n    num_data_points = float(len(data))\n    # \u8ba1\u7b97\u5212\u5206\u4e4b\u524d\u6570\u636e\u96c6\u7684\u6574\u4f53\u71b5\u503c\n    entropy_original = entropy(data[target])\n\n    for feature in features:\n        \n        # \u5de6\u5206\u652f\u5bf9\u5e94\u5f53\u524d\u7279\u5f81\u4e3a0\u7684\u6570\u636e\u70b9\n        left_split = data[data[feature] == 0]\n        \n        # \u53f3\u5206\u652f\u5bf9\u5e94\u5f53\u524d\u7279\u5f81\u4e3a1\u7684\u6570\u636e\u70b9\n        right_split = data[data[feature] == 1]\n        \n        # \u8ba1\u7b97\u5de6\u8fb9\u5206\u652f\u7684\u71b5\u503c\n        left_entropy = entropy(left_split[target])          \n\n        # \u8ba1\u7b97\u53f3\u8fb9\u5206\u652f\u7684\u71b5\u503c\n        right_entropy = entropy(right_split[target]) \n            \n        # \u8ba1\u7b97\u5de6\u8fb9\u5206\u652f\u4e0e\u53f3\u5206\u652f\u71b5\u503c\u7684\u52a0\u6743\u548c\uff08\u6570\u636e\u96c6\u5212\u5206\u540e\u7684\u71b5\u503c\uff09\n        entropy_split = (len(left_split) * left_entropy + len(right_split) * right_entropy) \/ num_data_points\n        \n        # \u8ba1\u7b97\u5212\u5206\u524d\u4e0e\u5212\u5206\u540e\u7684\u71b5\u503c\u5dee\u5f97\u5230\u4fe1\u606f\u589e\u76ca\n        info_gain = entropy_original - entropy_split\n\n        # \u66f4\u65b0\u6700\u4f73\u7279\u5f81\u548c\u5bf9\u5e94\u7684\u4fe1\u606f\u589e\u76ca\u7684\u503c\n        if info_gain > best_info_gain:\n            best_info_gain = info_gain\n            best_feature = feature\n    return best_feature\n    ","3439a960":"class TreeNode:\n    def __init__(self, is_leaf, prediction, split_feature):\n        self.is_leaf = is_leaf\n        self.prediction = prediction\n        self.split_feature = split_feature\n        self.left = None\n        self.right = None      ","5b0fa100":"from sklearn.base import BaseEstimator\nfrom sklearn.metrics import accuracy_score\n\nclass MyDecisionTree(BaseEstimator):\n    \n    def __init__(self, max_depth, min_error):\n        self.max_depth = max_depth\n        self.min_error = min_error\n    \n    def fit(self, X, Y, data_weights = None):\n        \n        data_set = pd.concat([X, Y], axis = 1)\n        features = X.columns\n        target = Y.columns[0]\n        self.root_node = self.create_tree(data_set, features, target, current_depth = 0, max_depth = self.max_depth, min_error = self.min_error)\n        \n        \n    def predict(self, X):\n        prediction = X.apply(lambda row: self.predict_single_data(self.root_node, row), axis = 1)\n        return prediction\n        \n        \n    def score(self, testX, testY):\n        target = testY.columns[0]\n        result = self.predict(testX)\n        return accuracy_score(testY[target], result)\n    \n    \n    def create_tree(self, data, features, target, current_depth = 0, max_depth = 10, min_error=0):\n        \"\"\"\n        \u63a2\u7d22\u4e09\u79cd\u4e0d\u540c\u7684\u7ec8\u6b62\u5212\u5206\u6570\u636e\u96c6\u7684\u6761\u4ef6  \n  \n        termination 1, \u5f53\u9519\u8bef\u7387\u964d\u5230min_error\u4ee5\u4e0b, \u7ec8\u6b62\u5212\u5206\u5e76\u8fd4\u56de\u53f6\u5b50\u8282\u70b9  \n        termination 2, \u5f53\u7279\u5f81\u90fd\u7528\u5b8c\u4e86, \u7ec8\u6b62\u5212\u5206\u5e76\u8fd4\u56de\u53f6\u5b50\u8282\u70b9  \n        termination 3, \u5f53\u6811\u7684\u6df1\u5ea6\u7b49\u4e8e\u6700\u5927max_depth\u65f6, \u7ec8\u6b62\u5212\u5206\u5e76\u8fd4\u56de\u53f6\u5b50\u8282\u70b9\n        \"\"\"\n          \n        # \u62f7\u8d1d\u4ee5\u4e0b\u53ef\u7528\u7279\u5f81\n        remaining_features = features[:]\n\n        target_values = data[target]\n\n        # termination 1\n        if count_errors(target_values) <= min_error:\n            print(\"Termination 1 reached.\")     \n            return self.create_leaf(target_values)\n\n        # termination 2\n        if len(remaining_features) == 0:\n            print(\"Termination 2 reached.\")    \n            return self.create_leaf(target_values)  \n\n        # termination 3\n        if current_depth >= max_depth: \n            print(\"Termination 3 reached.\")\n            return self.create_leaf(target_values)\n\n        # \u9009\u51fa\u6700\u4f73\u5f53\u524d\u5212\u5206\u7279\u5f81\n        #split_feature = best_split(data, features, target)   #\u6839\u636e\u6b63\u786e\u7387\u5212\u5206\n        split_feature = best_split_entropy(data, features, target)  # \u6839\u636e\u4fe1\u606f\u589e\u76ca\u6765\u5212\u5206\n\n        # \u9009\u51fa\u6700\u4f73\u7279\u5f81\u540e\uff0c\u8be5\u7279\u5f81\u4e3a0\u7684\u6570\u636e\u5206\u5230\u5de6\u8fb9\uff0c\u8be5\u7279\u5f81\u4e3a1\u7684\u6570\u636e\u5206\u5230\u53f3\u8fb9\n        left_split = data[data[split_feature] == 0]\n        right_split = data[data[split_feature] == 1]\n\n        # \u5254\u9664\u5df2\u7ecf\u7528\u8fc7\u7684\u7279\u5f81\n        remaining_features = remaining_features.drop(split_feature)\n        print(\"Split on feature %s. (%s, %s)\" % (split_feature, str(len(left_split)), str(len(right_split))))\n\n        # \u5982\u679c\u5f53\u524d\u6570\u636e\u5168\u90e8\u5212\u5206\u5230\u4e86\u4e00\u8fb9\uff0c\u76f4\u63a5\u521b\u5efa\u53f6\u5b50\u8282\u70b9\u8fd4\u56de\u5373\u53ef\n        if len(left_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(left_split[target])\n        if len(right_split) == len(data):\n            print(\"Perfect split!\")\n            return self.create_leaf(right_split[target])\n\n        # \u9012\u5f52\u4e0a\u9762\u7684\u6b65\u9aa4\n        left_tree = self.create_tree(left_split, remaining_features, target, current_depth + 1, max_depth, min_error)     \n        right_tree = self.create_tree(right_split, remaining_features, target, current_depth + 1, max_depth, min_error) \n\n        #\u751f\u6210\u5f53\u524d\u7684\u6811\u8282\u70b9\n        result_node = TreeNode(False, None, split_feature)\n        result_node.left = left_tree\n        result_node.right = right_tree\n        return result_node    \n    \n    \n    \n    def create_leaf(self, target_values):\n        # \u7528\u4e8e\u521b\u5efa\u53f6\u5b50\u7684\u51fd\u6570\n\n        # \u521d\u59cb\u5316\u4e00\u4e2a\u6811\u8282\u70b9\n        leaf = TreeNode(True, None, None)\n\n        # \u7edf\u8ba1\u5f53\u524d\u6570\u636e\u96c6\u91cc\u6807\u7b7e\u4e3a+1\u548c-1\u7684\u4e2a\u6570\uff0c\u8f83\u5927\u7684\u90a3\u4e2a\u5373\u4e3a\u5f53\u524d\u8282\u70b9\u7684\u9884\u6d4b\u7ed3\u679c\n        num_positive_ones = len(target_values[target_values == 1])\n        num_negative_ones = len(target_values[target_values == 1])\n\n        if num_positive_ones > num_negative_ones:\n            leaf.prediction = 1\n        else:\n            leaf.prediction = -1\n\n        # \u8fd4\u56de\u53f6\u5b50        \n        return leaf \n    \n    \n    \n    def predict_single_data(self, tree, x, annotate = False):   \n        # \u5982\u679c\u5df2\u7ecf\u662f\u53f6\u5b50\u8282\u70b9\u76f4\u63a5\u8fd4\u56de\u53f6\u5b50\u8282\u70b9\u7684\u9884\u6d4b\u7ed3\u679c\n        \n        if tree.is_leaf:\n            if annotate: \n                print(\"leaf node, predicting %s\" % tree.prediction)\n            return tree.prediction \n        else:\n            # \u67e5\u8be2\u5f53\u524d\u8282\u70b9\u7528\u6765\u5212\u5206\u6570\u636e\u96c6\u7684\u7279\u5f81\n            split_feature_value = x[tree.split_feature]\n\n            if annotate: \n                print(\"Split on %s = %s\" % (tree.split_feature, split_feature_value))\n            if split_feature_value == 0:\n                #\u5982\u679c\u6570\u636e\u5728\u8be5\u7279\u5f81\u4e0a\u7684\u503c\u4e3a0\uff0c\u4ea4\u7ed9\u5de6\u5b50\u6811\u6765\u9884\u6d4b\n                return self.predict_single_data(tree.left, x, annotate)\n            else:\n                #\u5982\u679c\u6570\u636e\u5728\u8be5\u7279\u5f81\u4e0a\u7684\u503c\u4e3a0\uff0c\u4ea4\u7ed9\u53f3\u5b50\u6811\u6765\u9884\u6d4b\n                return self.predict_single_data(tree.right, x, annotate)    \n    \n    def count_leaves(self):\n        \n        return self.count_leaves_helper(self.root_node)\n    \n    def count_leaves_helper(self, tree):\n        \n        if tree.is_leaf:\n            return 1\n        return self.count_leaves_helper(tree.left) + self.count_leaves_helper(tree.right)\n    \n","d99a2a15":"m = MyDecisionTree(max_depth = 10, min_error = 1e-15)","d9020f82":"m.fit(trainX, trainY)","d32c1bff":"m.score(testX, testY)","ee78d7cc":"m.count_leaves()","117dd6aa":"model_1 = MyDecisionTree(max_depth = 3, min_error = 1e-15)\nmodel_2 = MyDecisionTree(max_depth = 7, min_error = 1e-15)\nmodel_3 = MyDecisionTree(max_depth = 15, min_error = 1e-15)\n","27500a13":"model_1.fit(trainX, trainY)\nmodel_2.fit(trainX, trainY)\nmodel_3.fit(trainX, trainY)","f23d6844":"print(\"model_1 training accuracy :\", model_1.score(trainX, trainY))\nprint(\"model_2 training accuracy :\", model_2.score(trainX, trainY))\nprint(\"model_3 training accuracy :\", model_3.score(trainX, trainY))","79b76eb4":"print(\"model_1 testing accuracy :\", model_1.score(testX, testY))\nprint(\"model_2 testing accuracy :\", model_2.score(testX, testY))\nprint(\"model_3 testing accuracy :\", model_3.score(testX, testY))","e994a666":"print(\"model_1 complexity is: \", model_1.count_leaves())\nprint(\"model_2 complexity is: \", model_2.count_leaves())\nprint(\"model_3 complexity is: \", model_3.count_leaves())","e81d3995":"## Preprocessing your features","ea6dfc33":"## \u5c06\u6570\u636e\u5206\u6210\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6","af431c28":"## \u5efa\u81ea\u5df1\u7684\u51b3\u7b56\u6811!  \n\n\u4efb\u52a1\uff1a  \n1 \u5b9e\u73b0\u6839\u636eerror\u6765\u9009\u62e9\u6700\u4f73\u5212\u5206\u7279\u5f81\u7684\u51fd\u6570best_split()  \n2 \u5b9e\u73b0\u6839\u636eentropy\u6765\u9009\u62e9\u6700\u4f73\u7279\u5f81\u7684\u51fd\u6570best_split_entropy()  \n3 \u5b9e\u73b0\u6811\u8282\u70b9\u7684\u7c7bTreeNode  \n4 \u5b9e\u73b0\u6a21\u578b\u7684\u7c7bMyDecisionTree  ","788d0aed":"## \u8bfb\u53d6\u6570\u636e","f5805144":"#### \u4efb\u52a11\uff0c \u5b9e\u73b0\u6839\u636eerror\u6765\u9009\u62e9\u6700\u4f73\u5212\u5206\u7279\u5f81\u7684\u51fd\u6570best_split()  \n\u7ea6\u5b9a\u6811\u7684\u5de6\u8fb9\u5bf9\u5e94target == 0\uff0c \u6811\u7684\u53f3\u8fb9\u5bf9\u5e94target == 1","d5227fd9":"### \u51b3\u7b56\u6811\u590d\u6742\u5ea6\u7684\u63a2\u8ba8","7d2f120d":"## \u9884\u5904\u7406\u9884\u6d4b\u76ee\u6807\n\n\u9884\u6d4b\u76ee\u6807\u662f\u4e00\u5217'bad_loans'\u7684\u6570\u636e\u3002\u5176\u4e2d**1**\u8868\u793a\u7684\u662f\u4e0d\u826f\u8d37\u6b3e\uff0c**0**\u8868\u793a\u7684\u662f\u4f18\u8d28\u8d37\u6b3e\u3002\n\n\u5c06\u9884\u6d4b\u76ee\u6807\u5904\u7406\u6210\u66f4\u7b26\u5408\u76f4\u89c9\u7684\u6807\u7b7e\uff0c\u521b\u5efa\u4e00\u5217 `safe_loans`. \u5e76\u4e14: \n\n* **+1** \u8868\u793a\u4f18\u8d28\u8d37\u6b3e, \n* **-1** \u8868\u793a\u4e0d\u826f\u8d37\u6b3e. ","a51e17e9":"#### \u8fd9\u662f\u4e00\u4e2a\u4e0d\u5e73\u8861\u6570\u636e\uff0c \u597d\u7684\u8d37\u6b3e\u8fdc\u6bd4\u574f\u7684\u8d37\u6b3e\u8981\u591a. ","b9519f64":"#### \u4efb\u52a12\uff0c \u5b9e\u73b0\u6839\u636eentropy\u6765\u9009\u62e9\u6700\u4f73\u7279\u5f81\u7684\u51fd\u6570best_split_entropy()  \n","7bc8e344":"## \u6253\u5370\u4f18\u8d28\u8d37\u6b3e\u4e0e\u4e0d\u826f\u8d37\u6b3e\u7684\u6bd4\u4f8b","a6d744aa":"\u91cd\u8981\u7684\u4e8b\u60c5\u8bf4\u4e09\u904d!!  \n\n**\u628a\u4f60\u7684\u722a\u5b50\u4eceTEST DATA\u4e0a\u62ff\u5f00!!**   \n**\u628a\u4f60\u7684\u722a\u5b50\u4eceTEST DATA\u4e0a\u62ff\u5f00!!**  \n**\u628a\u4f60\u7684\u722a\u5b50\u4eceTEST DATA\u4e0a\u62ff\u5f00!!**  \n","6623cf37":"Now, let's verify that the resulting percentage of safe and risky loans are each nearly 50%.","e8370173":"## \u521b\u5efa\u66f4\u4e3a\u5e73\u8861\u7684\u6570\u636e\u96c6  \n\n* \u5bf9\u5360\u591a\u6570\u7684\u6807\u7b7e\u8fdb\u884c\u4e0b\u91c7\u6837  \n* \u6ce8\u610f\u6709\u5f88\u591a\u65b9\u6cd5\u5904\u7406\u4e0d\u5e73\u8861\u6570\u636e\uff0c\u4e0b\u91c7\u6837\u53ea\u662f\u5176\u4e2d\u4e4b\u4e00","58e05346":"#### \u63a2\u7d22\u4e0d\u540c\u6811\u6df1\u5ea6\u5bf9\u51b3\u7b56\u6811\u7684\u5f71\u54cd  \n\n1 max_depth = 3  \n2 max_depth = 7  \n3 max_depth = 15\n","8734e5ae":"## \u7528\u51b3\u7b56\u6811\u6765\u5206\u7c7b\u8d37\u6b3e\u662f\u5426\u4f18\u826f","eb841caf":"[LendingClub](https:\/\/www.lendingclub.com\/) \u662f\u4e00\u5bb6\u8d37\u6b3e\u516c\u53f8. \u5728\u672c\u6b21\u4f5c\u4e1a\u4e2d,\u6211\u4eec\u9700\u8981\u624b\u52a8\u5b9e\u73b0\u51b3\u7b56\u6811\u6765\u9884\u6d4b\u4e00\u4efd\u8d37\u6b3e\u662f\u5426\u5b89\u5168\uff0c\u5e76\u5bf9\u6bd4\u4e0d\u540c\u590d\u6742\u5ea6\u4e0b\u51b3\u7b56\u6811\u7684\u8868\u73b0","23b357c0":"## \u9009\u53d6\u7528\u4e8e\u9884\u6d4b\u7684\u7279\u5f81","8aa29479":"## \u6253\u5370\u53ef\u7528\u7279\u5f81","166bcba9":"#### \u4efb\u52a13\uff0c\u5b9e\u73b0\u6811\u8282\u70b9\u7684\u7c7bTreeNode\uff0c\u6bcf\u4e2a\u6811\u8282\u70b9\u5e94\u8be5\u5305\u542b\u5982\u4e0b\u4fe1\u606f:  \n\n   3.1 is_leaf: True\/False  \u8868\u793a\u5f53\u524d\u8282\u70b9\u662f\u5426\u4e3a\u53f6\u5b50\u8282\u70b9  \n   \n   3.2 prediction: \u5f53\u524d\u8282\u70b9\u505a\u5168\u6c11\u516c\u6295\u7684\u9884\u6d4b\u7ed3\u679c\n   \n   3.3 left: \u5de6\u5b50\u6811  \n   \n   3.4 right: \u53f3\u5b50\u6811 \n   \n   3.5 split_feature: \u5f53\u524d\u8282\u70b9\u7528\u6765\u5212\u5206\u6570\u636e\u65f6\u6240\u91c7\u7528\u7684\u7279\u5f81","94fd6e20":"#### \u4efb\u52a14\uff0c\u5b9e\u73b0\u6a21\u578b\u7684\u7c7bMyDecisionTree\uff0c \u5b9e\u73b0\u5982\u4e0b\u4e3b\u8981\u51fd\u6570  \n  \n  \n   4.1 fit(): \u6a21\u578b\u5728\u8bad\u7ec3\u96c6\u4e0a\u7684\u5b66\u4e60  \n   \n   4.2 predict(): \u6a21\u578b\u5728\u6570\u636e\u96c6\u4e0a\u7684\u9884\u6d4b\n   \n   4.3 score(): \u6a21\u578b\u5728\u6d4b\u8bd5\u96c6\u4e0a\u7684\u5f97\u5206   \n   \n   \n   \n   \n   \u4e3a\u4e86\u5b9e\u73b04.1 - 4.3\u7684\u65b9\u6cd5\uff0c \u9700\u8981\u5b9e\u73b0\u5982\u4e0b\u8f85\u52a9\u51fd\u6570  \n   4.4 create_tree(): \u521b\u5efa\u4e00\u68f5\u6811  \n   \n   4.5 create_leaf(): \u521b\u5efa\u53f6\u5b50\u8282\u70b9  \n   \n   4.6 predict_single_data(): \u6a21\u578b\u9884\u6d4b\u5355\u4e2a\u6570\u636e  \n   \n   4.7 count_leaves(): \u7edf\u8ba1\u6a21\u578b\u7684\u53f6\u5b50\u6570"}}