{"cell_type":{"9f24865a":"code","b27dbcf4":"code","4946e467":"code","dfe3a6e4":"code","dec5f997":"code","df666d23":"code","2856d472":"code","df796830":"code","e362ced7":"code","1626e225":"code","1f1ab7e9":"code","18ddf0ba":"code","ea4ebab9":"code","d00ced56":"code","9bcb3b30":"code","20c9b068":"code","ffa40848":"code","01c40fe6":"code","d9187008":"code","bdfaa8bd":"code","8eded92f":"code","8bfa284a":"code","7aff9b71":"code","93400e39":"code","0abcfb49":"code","cc720585":"code","cefe744e":"code","be0a10ef":"code","20e38143":"code","e850c5b9":"code","5b157fbf":"code","55fdd6c4":"code","4d67b961":"code","63d5acc3":"code","9be760c2":"code","ca48e7ca":"code","c1047bd1":"code","a9f9b4b5":"code","ba282e05":"code","3853072f":"code","85f7d04b":"code","b355fa67":"code","81781a93":"code","9789e5b0":"code","18c5c2a8":"code","39d12ede":"code","50b25d6c":"code","67bd7c40":"code","8a87b5a7":"code","7f98489e":"code","ad72cbeb":"code","f76aa85c":"code","abb893ee":"code","d040e259":"code","e0480873":"code","37958e0d":"code","f24efcbe":"code","f34b203c":"code","368d3846":"code","76df3f8c":"code","4a40e69e":"code","3cea78c8":"code","bf98b76f":"code","17ca3240":"code","1a2fec89":"code","49cd4dc0":"code","6f005f09":"code","c55c9f0e":"code","2008c779":"code","73973291":"code","ed55fc34":"code","10203724":"code","97107efd":"code","0f20ea97":"code","c96f0e67":"code","41ce5895":"code","4248c87c":"code","c1952385":"code","4b7a4acc":"code","44341b70":"code","9e39ac86":"code","2ca3d0ef":"code","8dd265f9":"code","fec45a69":"code","e73e0a8f":"code","5b28c84e":"code","f97dd171":"code","bb9d52c5":"code","709c6de5":"code","3661e4bd":"code","f4a7173b":"code","70fe2621":"code","fdd2a6a6":"code","a2a6f382":"code","7267aa9b":"code","7c354f86":"code","d337cffc":"code","c59f6b26":"code","f9126ef7":"code","17a5a009":"code","37bab4ab":"code","04eed58d":"code","d68a4822":"code","ff2ee87a":"code","0e6c9c69":"code","6b0e6f9c":"code","8d9580fc":"code","845de882":"code","3b225f54":"code","60fa5cc9":"code","873de0fa":"code","c9df0980":"code","84f777bc":"code","d4503cb3":"code","e8aa1902":"code","c7f6adbf":"code","6acebd66":"code","6dbed383":"code","08634923":"code","8d33f323":"code","ab08f7f9":"code","1bc0e824":"code","3a57f470":"code","5d2727a8":"code","6fa15bb7":"code","9a344a32":"code","ca840ab2":"code","5b1fe9ab":"code","85a6bf03":"code","0b671040":"code","1d13b6ea":"code","b54e8929":"code","bd0ba3d6":"code","de569f5d":"code","b769805c":"markdown","49f39a25":"markdown","58624efc":"markdown","ecb6e560":"markdown","26524e41":"markdown","bcfa7da3":"markdown","b88d3f87":"markdown","41822547":"markdown","c7e1b11e":"markdown","3a46f0b7":"markdown","35e308df":"markdown","84a39957":"markdown","ec47343e":"markdown","fb51801e":"markdown","bd6c8785":"markdown","39130a5a":"markdown","0ecec16b":"markdown","714baa8d":"markdown","e81b0320":"markdown","1ba96a3e":"markdown","b4a45e5e":"markdown","d7dcaec2":"markdown","afb000c0":"markdown","a5523393":"markdown","ceda714d":"markdown","578fb5b0":"markdown","f315333b":"markdown","2c0a35d4":"markdown","8aecf23e":"markdown","8d014828":"markdown","0ca40f39":"markdown","570abc51":"markdown","bd97b2de":"markdown","0a96f3dc":"markdown","e103001f":"markdown","ec12d2e9":"markdown","85792c06":"markdown","cc668c7c":"markdown","a2f2908d":"markdown","e7edc84b":"markdown"},"source":{"9f24865a":"%pylab inline\n%config InlineBackend.figure_formats = ['retina']\n\n# Loading libraries\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport seaborn as sns # data visualization\nimport matplotlib.pyplot as plt # data visualization\n\nimport plotly.express as px # interactive plots\nfrom plotly.subplots import make_subplots\n\nimport statsmodels.api as sm # statistic model - OLS Regression\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# sklearn\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PolynomialFeatures\nfrom sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression, RFE\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nfrom functools import reduce\nfrom statistics import stdev","b27dbcf4":"# define a function to calculate feature importance deviations due to varying input\ndef feature_importance_deviation(model, n_splits, X, y, attr = 'coef_'):\n    \"\"\"\n    This function calculates feature importance deviations due to varying input.\n\n    Function requires arguments:\n    - model: model to fit\n    - n_splits: how many splits to perform on the data to obtain input subsets\n    - X: data frame with predicting features\n    - y: target feature\n    - attr: {'coef_', 'feature_importances_'}, default = 'coef_' \n        Specify attribute to get feature importance values from the model\n    \"\"\"\n    \n    # define splits\n    kf = KFold(n_splits = n_splits)\n    \n    # set up dictionary to save coefficients of each model as data frames\n    i = 0\n    folds = dict()\n    \n    # itterate over folds and get the coefficients\n    for train, test in kf.split(X):\n        \n        # split data\n        X_train, X_test = X.iloc[train,], X.iloc[test,]\n        y_train, y_test = y.iloc[train,], y.iloc[test,]\n        \n        # fit model\n        model.fit(X_train, y_train)\n        \n        # get feature names and feature importance values\n        feature_names = X_train.columns\n        if attr == 'coef_':\n            feature_importances = abs(model.coef_).tolist()\n        elif attr == 'feature_importances_':\n            feature_importances = model.feature_importances_.tolist()\n        else:\n            print(\"Argument 'attr' should be: 'coef_' or 'feature_importances_'\")\n            return\n        \n        # create data frame for this fold\n        df = pd.DataFrame(feature_importances, feature_names, columns = ['fold_{}'.format(str(i))])\n        df['features'] = df.index\n        \n        # add data frame to the dict\n        folds[str(i)] = df\n        \n        # increment i for the next fold\n        i += 1\n    \n    # merge dfs into one\n    dfs = list(folds.values())\n    df_merged = reduce(lambda left, right: pd.merge(left, right, \n                                                    on = 'features', how = 'outer'), dfs)\n    \n    # reorder columns\n    # get a list of columns\n    cols = list(df_merged)\n    fold_cols = [col for col in df_merged.columns if col != 'features']\n    \n    # move the column 'features' to head \n    df_merged = df_merged[['features'] + fold_cols]\n    \n    # calculate standard deviation of the feature importance values for each feature\n    df_merged['stdev'] = df_merged.apply(lambda row: \n                                         stdev([row[col] for col in fold_cols]), \n                                         axis = 1)\n    \n    # calculate average value of the feature importance for each feature\n    df_merged['mean_fi'] = df_merged.apply(lambda row: \n                                         mean([row[col] for col in fold_cols]), \n                                         axis = 1)   \n    \n    return df_merged.sort_values(by = ['stdev'], ascending=False)","4946e467":"# loading data and checking data types\ndata = pd.read_csv('..\/input\/world-metrics-2016\/2016_world_metrics.csv')\nprint(data.info())\ndata.head()","dfe3a6e4":"# visualize happiness scores on the map\nfig = px.choropleth(data, locations = 'country',\n                    color = 'happiness_score', \n                    locationmode = 'country names',\n                    hover_name = 'country',\n                    hover_data = ['happiness_score'],\n                    title = 'Happiness scores',\n                    color_continuous_scale = px.colors.diverging.RdYlBu[::-1],\n                    projection = 'orthographic')\n\n# adjusting size of map, legend place, and background colour\nfig.update_layout(\n    autosize=False,\n    width=600,\n    height=450,\n    coloraxis_colorbar=dict(title=\"Happiness\"),\n    margin=dict(\n        l=10,\n        r=10,\n        b=20,\n        t=40,\n        pad=4\n    ),\n    template='seaborn',\n    paper_bgcolor='rgb(234, 234, 242)',\n    legend=dict(\n        orientation='v',\n        yanchor='auto',\n        y=1.02,\n        xanchor='right',\n        x=1\n))\n\nfig.show()\n# references: \n# https:\/\/plotly.github.io\/plotly.py-docs\/generated\/plotly.express.choropleth.html\n# https:\/\/www.kaggle.com\/sevgisarac\/map-with-plotly","dec5f997":"# look at distribution of values for each variable\nstatistics = data.describe().T\nstatistics['range'] = statistics.apply(lambda row: row['max'] - row['min'], axis = 1)\nstatistics","df666d23":"# create separate columns for bio deficit and bio reserve\ndata[\"bio_deficit\"] = data.apply(lambda row: abs(row[\"deficit_or_reserve\"]) if row[\"deficit_or_reserve\"] < 0 \n                                 else 0, axis = 1)\ndata[\"bio_reserve\"] = data.apply(lambda row: row[\"deficit_or_reserve\"] if row[\"deficit_or_reserve\"] > 0 \n                                 else 0, axis = 1)\n\n# drop previous column\ndata.drop(['deficit_or_reserve'], axis=1)\n\n# reorder columns\ndata = data[['country', 'life_expect', 'life_exp60', 'adult_mortality', \n            'infant_mort', 'age1-4mort', 'alcohol', 'bmi', 'basic_water', \n            'pop_mils', 'development_index', 'gdp_per_capita', 'eco_footprint',\n            'biocapacity', 'bio_deficit', 'bio_reserve', 'pf_rol', 'pf_ss', 'pf_movement',\n            'pf_religion', 'pf_expression', 'pf_identity', 'pf_score',\n            'ef_government', 'ef_legal', 'ef_money', 'ef_trade', 'ef_regulation',\n            'ef_score', 'hf_score', 'happiness_score']]","2856d472":"# build histograms for all numerical variables\nfig, ax = plt.subplots(10, 3, sharex=False, sharey=False, figsize=(10, 20))\n\nm=1\nfor i in range(10):\n    for j in range(3):\n        data.hist(column = data.columns[m], bins = 10, ax=ax[i,j])\n        m += 1\n            \nfig.tight_layout()\nfig.show()","df796830":"# calculate skewness scores\nskewness = data.skew(axis=0, numeric_only = True).to_dict()\n\n# define skewness threshold\nskewness_threshold = 0.5\n\n# create lists of columns that require normalizing \n# positively and negatively skewed variables to be processed by different transformations\npos_skewed_cols = []\nneg_skewed_cols = []\nfor i in skewness:\n    if abs(skewness[i]) > skewness_threshold:\n        if skewness[i] > 0:\n            pos_skewed_cols.append(i)\n        else:\n            neg_skewed_cols.append(i)\n\n# print results\nfor i in pos_skewed_cols:\n    print(f\"Column {i} is positively skewed: score {round(skewness[i], 2)}\")\n\nprint('-'*40)\n\nfor i in neg_skewed_cols:\n    print(f\"Column {i} is negatively skewed: score {round(skewness[i], 2)}\")","e362ced7":"# create a data frame with skewness coefficients before and after different transformations to choose the ones to use\n# for positively skewed variables\n\n# skewness before transformations\npos_no_transf_dict = data[pos_skewed_cols].skew(axis=0, numeric_only = True).to_dict()\npos_skew = pd.DataFrame(list(pos_no_transf_dict.items()), columns = ['vars','skew_no_transform'])\n\n# skewness after Logarithmic Transformation\npos_log_transf_dict = data[pos_skewed_cols].apply(np.log1p).skew(axis=0, numeric_only = True).to_dict()\npos_skew = pd.merge(pos_skew, \n                    pd.DataFrame(list(pos_log_transf_dict.items()), columns = ['vars','skew_log_transform']), \n                    on = ['vars'])\n\n# skewness after Cubic Root Transformation\npos_cbrt_transf_dict = data[pos_skewed_cols].apply(np.cbrt).skew(axis=0, numeric_only = True).to_dict()\npos_skew = pd.merge(pos_skew, \n                    pd.DataFrame(list(pos_cbrt_transf_dict.items()), columns = ['vars','skew_cbrt_transform']), \n                    on = ['vars'])\n\n# skewness after 5th Root Transformation\npos_5rt_transf_dict = data[pos_skewed_cols].apply(lambda x: pow(x, 1\/5)).skew(axis=0, numeric_only = True).to_dict()\npos_skew = pd.merge(pos_skew, \n                    pd.DataFrame(list(pos_5rt_transf_dict.items()), columns = ['vars','skew_5rt_transform']), \n                    on = ['vars'])\n\n# skewness after 7th Root Transformation\npos_7rt_transf_dict = data[pos_skewed_cols].apply(lambda x: pow(x, 1\/7)).skew(axis=0, numeric_only = True).to_dict()\npos_skew = pd.merge(pos_skew, \n                    pd.DataFrame(list(pos_7rt_transf_dict.items()), columns = ['vars','skew_7rt_transform']), \n                    on = ['vars'])\n\npos_skew","1626e225":"# create lists of columns for each transformation\nto_log_transform = ['adult_mortality', 'pop_mils', 'gdp_per_capita', 'eco_footprint', 'biocapacity']\nto_cbrt_transform = ['bio_deficit']\nto_7rt_transform = ['infant_mort', 'age1-4mort', 'bio_reserve']","1f1ab7e9":"# for negatively skewed variables\n# skewness before transformations\nneg_no_transf_dict = data[neg_skewed_cols].skew(axis=0, numeric_only = True).to_dict()\nneg_skew = pd.DataFrame(list(neg_no_transf_dict.items()), columns = ['vars','skew_no_transform'])\n\n# skewness after Square Transformation\nneg_square_transf_dict = data[neg_skewed_cols].apply(lambda x: pow(x, 2)).skew(axis=0, numeric_only = True).to_dict()\nneg_skew = pd.merge(neg_skew, \n                    pd.DataFrame(list(neg_square_transf_dict.items()), columns = ['vars','skew_square_transform']), \n                    on = ['vars'])\n\n# skewness after Cubic Transformation\nneg_cubic_transf_dict = data[neg_skewed_cols].apply(lambda x: pow(x, 3)).skew(axis=0, numeric_only = True).to_dict()\nneg_skew = pd.merge(neg_skew, \n                    pd.DataFrame(list(neg_cubic_transf_dict.items()), columns = ['vars','skew_cubic_transform']), \n                    on = ['vars'])\n\n# skewness after 5th Power Transformation\nneg_5p_transf_dict = data[neg_skewed_cols].apply(lambda x: pow(x, 5)).skew(axis=0, numeric_only = True).to_dict()\nneg_skew = pd.merge(neg_skew, \n                    pd.DataFrame(list(neg_5p_transf_dict.items()), columns = ['vars','skew_5p_transform']), \n                    on = ['vars'])\n\n# skewness after 7th Power Transformation\nneg_7p_transf_dict = data[neg_skewed_cols].apply(lambda x: pow(x, 7)).skew(axis=0, numeric_only = True).to_dict()\nneg_skew = pd.merge(neg_skew, \n                    pd.DataFrame(list(neg_7p_transf_dict.items()), columns = ['vars','skew_7p_transform']), \n                    on = ['vars'])\n\nneg_skew","18ddf0ba":"# create lists of columns for each transformation\nto_square_transform = ['ef_regulation']\nto_cubic_transform = ['pf_ss', 'pf_religion', 'pf_expression', 'pf_identity', 'ef_trade', 'ef_score']\nto_5p_transform = ['life_expect', 'pf_movement']\nto_7p_transform = ['basic_water', 'ef_money']","ea4ebab9":"# copy data frame into new one for transformations\ndata_norm = data.copy()\n\n# do Logarithmic Transformation\nfor i in to_log_transform:\n    data_norm[i] = data_norm[i].apply(np.log1p)\n    \n# do Cubic Root Transformation\nfor i in to_cbrt_transform:\n    data_norm[i] = data_norm[i].apply(np.cbrt)\n    \n# do 7th Root Transformation\nfor i in to_7rt_transform:\n    data_norm[i] = data_norm[i].apply(lambda x: pow(x, 1\/7))\n\n# do Square Transformation\nfor i in to_square_transform:\n    data_norm[i] = data_norm[i].apply(lambda x: pow(x, 2))\n\n# do Cubic Transformation\nfor i in to_cubic_transform:\n    data_norm[i] = data_norm[i].apply(lambda x: pow(x, 3))\n    \n# do 5th Power Transformation\nfor i in to_5p_transform:\n    data_norm[i] = data_norm[i].apply(lambda x: pow(x, 5))\n    \n# do 7th Power Transformation\nfor i in to_7p_transform:\n    data_norm[i] = data_norm[i].apply(lambda x: pow(x, 7))\n\n# add info on how the variable has been transformed to the column name\ndata_norm.columns = [f'{c}_log' if c in data_norm[to_log_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_cbrt' if c in data_norm[to_cbrt_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_7rt' if c in data_norm[to_7rt_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_^2' if c in data_norm[to_square_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_^3' if c in data_norm[to_cubic_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_^5' if c in data_norm[to_5p_transform] else f'{c}' for c in data_norm]\ndata_norm.columns = [f'{c}_^7' if c in data_norm[to_7p_transform] else f'{c}' for c in data_norm]\n\n# look at the skewness after tranformations\ndata_norm.skew(axis=0, numeric_only = True).sort_values()","d00ced56":"# build histograms for all numerical variables after transformations\nfig, ax = plt.subplots(10, 3, sharex=False, sharey=False, figsize=(10, 20))\n\nm=1\nfor i in range(10):\n    for j in range(3):\n        data_norm.hist(column = data_norm.columns[m], bins = 10, ax=ax[i,j])\n        m += 1\n            \nfig.tight_layout()\nfig.show()","9bcb3b30":"num_cols = data_norm.select_dtypes(include=np.number).columns.tolist()\ntarget = 'happiness_score'\nfeatures = [x for x in num_cols if x != target]\n\n# VIF dataframe\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = features\n  \n# calculating VIF for each feature\nvif_data[\"VIF\"] = [variance_inflation_factor(data_norm[features].values, i)\n                          for i in range(len(features))]\n\nvif_data.sort_values('VIF', ascending = False).head()","20c9b068":"# calculate correlation between numeric variables and create a heat map\nsns.set_context('notebook', font_scale=0.7)\nfig, ax = pyplot.subplots(figsize=(15,15))\nsns.heatmap(ax=ax, data=data_norm[num_cols].corr(), annot=True, fmt= '.2f')","ffa40848":"# rank variables by their correlation with target\ncorr = data_norm[num_cols].corr()\ncorr.reindex(corr.happiness_score.abs().sort_values(ascending = False).index).happiness_score","01c40fe6":"# combine data on VIF and correlation with the target to decide if we should remove some of the highly correlated predicting features\ncorr['feature'] = corr.index\nVIF_and_target_corr = pd.merge(vif_data, corr[['happiness_score', 'feature']], on = 'feature')\nVIF_and_target_corr.rename(columns={\"happiness_score\": \"corr_with_target\"}, inplace=True)\nVIF_and_target_corr.head()","d9187008":"# create a scatter plot for Variance Inflation Factor vs Correlation with target\nfig = px.scatter(VIF_and_target_corr, x=\"VIF\", y=\"corr_with_target\",\n                 width=800, height=500,\n                 hover_data = VIF_and_target_corr[['feature']],\n                 labels={\n                     \"VIF\": \"Variance Inflation Factor\",\n                     \"corr_with_target\": \"Correlation with target\"\n                 },\n                 title='Feature usefulness: Variance Inflation Factor vs Correlation with target',\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=12,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.show()","bdfaa8bd":"# drop some of the features with high VIF\ndata_norm_reduced = data_norm.drop(columns = ['pf_score', 'ef_score_^3', 'hf_score'])","8eded92f":"# define features\nnum_cols = data_norm_reduced.select_dtypes(include=np.number).columns.tolist()\nfeatures = [x for x in num_cols if x != target]\n\n# scale features for OLS Regression\ndata_norm_scaled = data_norm_reduced.copy()\ns = MinMaxScaler()\ndata_norm_scaled[features] = s.fit_transform(data_norm_scaled[features])\n\n# look at scaled variables\ndata_norm_scaled.describe().T","8bfa284a":"# get features and target\nX = data_norm_scaled[features]\nY = data_norm_scaled[target]\n\n# add constant for OLS Regression\nX = sm.add_constant(X)\n\n# build OLS Regression model\nmodel = sm.OLS(Y, X).fit()\npredictions = model.predict(X) \n\n# look at summary\nprint_model = model.summary()\nprint(print_model)","7aff9b71":"# get features and target for sklearn regression models\nX_data = data_norm_reduced[features]\ny_data = data_norm_reduced[target]\n\n# split data into train and test sets\n# I will do cross validation and model tuning on the train set and then calculate scores for tuned models on the test set\nX_train, X_test, y_train, y_test = train_test_split(X_data, y_data, \n                                                    test_size=0.3, random_state=42)\n\n# define function that will split train dataset for cross-validation\nkf = KFold(shuffle = True, random_state=105, n_splits = 4)","93400e39":"# add feature interactions by PolynomialFeatures() and scale the data\nprep = Pipeline([(\"polynomial_features\", PolynomialFeatures(degree = 2, interaction_only = True)),\n                 (\"scaler\", MinMaxScaler())])\nX_train_pf_array = prep.fit_transform(X_train)\nX_test_pf_array = prep.transform(X_test)\n\nnew_feature_names = prep.named_steps['polynomial_features'].get_feature_names(X_train.columns)\nX_train_pf = pd.DataFrame(X_train_pf_array, columns = new_feature_names)\nX_test_pf = pd.DataFrame(X_test_pf_array, columns = new_feature_names)\n\nprint(\"X_train features (shape): \", X_train_pf.shape)\nprint(\"y_train target (shape): \", y_train.shape)\nprint(\"-\"*40)\nprint(\"X_test features (shape): \", X_test_pf.shape)\nprint(\"y_test target (shape): \", y_test.shape)\nprint(\"-\"*40)\nprint(\"Feature names: \", X_train_pf.columns)","0abcfb49":"# Linear Regression\nlr = LinearRegression()\n\n# calculate train MAE\nlr_mae_train = cross_val_score(lr, X_train_pf, y_train, scoring = 'neg_mean_absolute_error', cv=kf).mean()\n\n# train model\nlr.fit(X_train_pf, y_train)\n\n# predict target values for test set using Linear Regression model\ny_predict_lr = lr.predict(X_test_pf)\n\n# calculate R2 score\nlr_r2 = r2_score(y_test, y_predict_lr)\n\n# calculate mean absolute error\nlr_mae = mean_absolute_error(y_test, y_predict_lr)\n\n# check the stability of the model\nlr_fi_dev = feature_importance_deviation(LinearRegression(), 10, \n                               pd.concat([X_train_pf, X_test_pf]),\n                               pd.concat([y_train, y_test]))\n\n# create data frame for scores and add scores for Linear Regression model\nscores = pd.DataFrame(data = {'model': ['linear_regression'],  \n                              'mae_train': [-lr_mae_train],\n                              'mae_test': [lr_mae],\n                              'r2_test': [lr_r2],\n                              'max_fi_std': [max(lr_fi_dev['stdev']\/abs(lr_fi_dev['mean_fi']))],\n                              'av_fi_std': [mean(lr_fi_dev['stdev']\/abs(lr_fi_dev['mean_fi']))],\n                              'non_zero_fi': [len([x for x in lr_fi_dev['mean_fi'].tolist() if x > 0])]})\nscores\n","cc720585":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_lr)\nz = numpy.polyfit(y_test, y_predict_lr, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Linear Regression\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","cefe744e":"# study coefficient deviation\nlr_fi_dev.head()","be0a10ef":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Linear Regression')\n\nax1.hist(x = lr_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = lr_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","20e38143":"# melt data frame into long format to visualise feature importance of each split\nlr_fi_dev_long = pd.melt(lr_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in lr_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","e850c5b9":"# visualise feature importance values with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Linear Regression')\n\nax1.scatter(x = lr_fi_dev_long.head(300)['coef'], \n            y = lr_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(lr_fi_dev_long['coef']) - 0.2, max(lr_fi_dev_long['coef']) + 0.2)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = lr_fi_dev_long.tail(300)['coef'], \n            y = lr_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(lr_fi_dev_long['coef']) - 0.2, max(lr_fi_dev_long['coef']) + 0.2)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","5b157fbf":"# visualize average feature importance vs standard deviation\nfig = px.scatter(lr_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = lr_fi_dev[['features']],\n                 labels={\n                     \"mean_fi\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Linear Regression')\nfig.show()","55fdd6c4":"# Ridge Regression\n# define pipeline\nestimator_ridge = Ridge()\n\n# define parameters for tuning\nparams_ridge = {'alpha': np.geomspace(4, 6, 10)}\n\n# do grid search\ngrid_ridge = GridSearchCV(estimator_ridge, params_ridge, scoring = 'neg_mean_absolute_error', \n                          cv=kf, n_jobs = -1)\ngrid_ridge.fit(X_train_pf, y_train)","4d67b961":"# look at tuned model\ngrid_ridge.best_score_, grid_ridge.best_params_","63d5acc3":"# save best parameters to use in model stability evaluation\nridge_best_alpha = grid_ridge.best_params_['alpha']","9be760c2":"# predict target values for test set using tuned Ridge Regression model\ny_predict_ridge = grid_ridge.predict(X_test_pf)\n\n# calculate R2 score\nridge_r2 = r2_score(y_test, y_predict_ridge)\n\n# calculate mean absolute error\nridge_mae = mean_absolute_error(y_test, y_predict_ridge)\n\n# check the stability of the model\nridge_fi_dev = feature_importance_deviation(Ridge(alpha = ridge_best_alpha), 10, \n                               pd.concat([X_train_pf, X_test_pf]),\n                               pd.concat([y_train, y_test]))\n\n# add scores for tuned Ridge Regression model to scores data frame\nscores = scores.append({'model': 'ridge_regression', \n                        'mae_train': -grid_ridge.best_score_,\n                        'mae_test': ridge_mae,\n                        'r2_test': ridge_r2,\n                        'max_fi_std': max(ridge_fi_dev['stdev']\/abs(ridge_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(ridge_fi_dev['stdev']\/abs(ridge_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in ridge_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","ca48e7ca":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_ridge)\nz = numpy.polyfit(y_test, y_predict_ridge, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Ridge Regression\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","c1047bd1":"# study feature importance deviation\nridge_fi_dev.head()","a9f9b4b5":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Ridge Regression')\n\nax1.hist(x = ridge_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = ridge_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","ba282e05":"# melt data frame into long format to visualise feature importance of each split\nridge_fi_dev_long = pd.melt(ridge_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in ridge_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","3853072f":"# visualise feature importance values with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Ridge Regression')\n\nax1.scatter(x = ridge_fi_dev_long.head(300)['coef'], \n            y = ridge_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(ridge_fi_dev_long['coef']) - 0.01, max(ridge_fi_dev_long['coef']) + 0.01)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = ridge_fi_dev_long.tail(300)['coef'], \n            y = ridge_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(ridge_fi_dev_long['coef']) - 0.01, max(ridge_fi_dev_long['coef']) + 0.01)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","85f7d04b":"# visualize average feature importance value vs standard deviation\nfig = px.scatter(ridge_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_fi\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Ridge Regression')\nfig.show()","b355fa67":"# Lasso Regression\n# define pipeline\nestimator_lasso = Lasso()\n\n# define parameters for tuning\nparams_lasso = {\n    'alpha': np.geomspace(0.001, 1, 10),\n    'max_iter': [100000]\n}\n\n# do grid search\ngrid_lasso = GridSearchCV(estimator_lasso, params_lasso, scoring = 'neg_mean_absolute_error', \n                          cv=kf, n_jobs = -1)\ngrid_lasso.fit(X_train_pf, y_train)","81781a93":"# look at tuned model\ngrid_lasso.best_score_, grid_lasso.best_params_","9789e5b0":"# save best parameters to use in model stability evaluation\nlasso_best_alpha = grid_lasso.best_params_['alpha']\nlasso_best_max_iter = grid_lasso.best_params_['max_iter']","18c5c2a8":"# predict target values for test set using tuned Lasso Regression model\ny_predict_lasso = grid_lasso.predict(X_test_pf)\n\n# calculate R2 score\nlasso_r2 = r2_score(y_test, y_predict_lasso)\n\n# calculate mean absolute error\nlasso_mae = mean_absolute_error(y_test, y_predict_lasso)\n\n# check the stability of the model\nlasso_fi_dev = feature_importance_deviation(Lasso(alpha = lasso_best_alpha, max_iter = lasso_best_max_iter), 10, \n                               pd.concat([X_train_pf, X_test_pf]),\n                               pd.concat([y_train, y_test]))\n\n# add scores for tuned Lasso Regression model to scores data frame\nscores = scores.append({'model': 'lasso_regression', \n                        'mae_train': -grid_lasso.best_score_,\n                        'mae_test': lasso_mae,\n                        'r2_test': lasso_r2,\n                        'max_fi_std': max(lasso_fi_dev['stdev']\/abs(lasso_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(lasso_fi_dev['stdev']\/abs(lasso_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in lasso_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","39d12ede":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_lasso)\nz = numpy.polyfit(y_test, y_predict_lasso, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Lasso Regression\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","50b25d6c":"# study feature importance deviation\nlasso_fi_dev.head()","67bd7c40":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Lasso Regression')\n\nax1.hist(x = lasso_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = lasso_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","8a87b5a7":"# melt data frame into long format to visualise feature importance of each split\nlasso_fi_dev_long = pd.melt(lasso_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in lasso_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","7f98489e":"# visualise feature importance values with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Lasso Regression')\n\nax1.scatter(x = lasso_fi_dev_long.head(300)['coef'], \n            y = lasso_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(lasso_fi_dev_long['coef']) - 0.05, max(lasso_fi_dev_long['coef']) + 0.05)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = lasso_fi_dev_long.tail(300)['coef'], \n            y = lasso_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(lasso_fi_dev_long['coef']) - 0.05, max(lasso_fi_dev_long['coef']) + 0.05)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","ad72cbeb":"# visualize average feature importance vs standard deviation\nfig = px.scatter(lasso_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_coef\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Lasso Regression')\nfig.show()","f76aa85c":"# Elastic Net\n# define pipeline\nestimator_net = ElasticNet()\n\n# define parameters for tuning\nparams_net = {\n    'alpha': np.geomspace(0.01, 1, 5),\n    'max_iter': [100000],\n    'l1_ratio': np.geomspace(0.01, 0.9, 5)\n}\n\n# do grid search\ngrid_net = GridSearchCV(estimator_net, params_net, scoring = 'neg_mean_absolute_error', \n                        cv=kf, n_jobs = -1)\ngrid_net.fit(X_train_pf, y_train)","abb893ee":"# look at tuned model\ngrid_net.best_score_, grid_net.best_params_","d040e259":"# save best parameters to use in model stability evaluation\nnet_best_alpha = grid_net.best_params_['alpha']\nnet_best_l1_ratio = grid_net.best_params_['l1_ratio']\nnet_best_max_iter = grid_net.best_params_['max_iter']","e0480873":"# predict target values for test set using tuned Net Regression model\ny_predict_net = grid_net.predict(X_test_pf)\n\n# calculate R2 score\nnet_r2 = r2_score(y_test, y_predict_net)\n\n# calculate mean absolute error\nnet_mae = mean_absolute_error(y_test, y_predict_net)\n\n# check the stability of the model\nnet_fi_dev = feature_importance_deviation(ElasticNet(alpha = net_best_alpha, \n                                                l1_ratio = net_best_l1_ratio, \n                                                max_iter = net_best_max_iter), \n                                     10, \n                                     pd.concat([X_train_pf, X_test_pf]),\n                                     pd.concat([y_train, y_test]))\n\n# add scores for tuned Elastic Net Regression model to scores data frame\nscores = scores.append({'model': 'elastic_net_regression', \n                        'mae_train': -grid_net.best_score_,\n                        'mae_test': net_mae,\n                        'r2_test': net_r2,\n                        'max_fi_std': max(net_fi_dev['stdev']\/abs(net_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(net_fi_dev['stdev']\/abs(net_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in net_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","37958e0d":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_net)\nz = numpy.polyfit(y_test, y_predict_net, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Elastic Net Regression\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","f24efcbe":"# study feature importance deviation\nnet_fi_dev.head()","f34b203c":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Elastic Net Regression')\n\nax1.hist(x = net_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = net_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","368d3846":"# melt data frame into long format to visualise feature importance of each split\nnet_fi_dev_long = pd.melt(net_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in net_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","76df3f8c":"# visualise feature importance values with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Elastic Net Regression')\n\nax1.scatter(x = net_fi_dev_long.head(300)['coef'], \n            y = net_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(net_fi_dev_long['coef']) - 0.02, max(net_fi_dev_long['coef']) + 0.02)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = net_fi_dev_long.tail(300)['coef'], \n            y = net_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(net_fi_dev_long['coef']) - 0.02, max(net_fi_dev_long['coef']) + 0.02)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","4a40e69e":"# visualize average feature importance vs standard deviation\nfig = px.scatter(net_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_fi\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Elastic Net Regression')\nfig.show()","3cea78c8":"# Ridge Regression with SelectKBest feature selector\n# add 'feature_selector' step \nestimator_ridge_kbest = Pipeline([(\"feature_selector\", SelectKBest(score_func=mutual_info_regression)),\n                                 (\"ridge_regression\", Ridge())])\n\n# define parameters for tuning - the number of features to select\nparams_ridge_kbest = {\n    'feature_selector__k': [i for i in range(X_train_pf.shape[1] - 180, X_train_pf.shape[1] - 120, 5)],\n    'ridge_regression__alpha': np.geomspace(1, 5, 3)\n}\n\n# tune model\ngrid_ridge_kbest = GridSearchCV(estimator_ridge_kbest, params_ridge_kbest, scoring = 'neg_mean_absolute_error', \n                                cv=kf, n_jobs = -1)\ngrid_ridge_kbest.fit(X_train_pf, y_train)","bf98b76f":"# look at tuned model\ngrid_ridge_kbest.best_score_, grid_ridge_kbest.best_params_","17ca3240":"# save best parameters to use in model stability evaluation\nridge_kbest_best_k = grid_ridge_kbest.best_params_['feature_selector__k']\nridge_kbest_best_alpha = grid_ridge_kbest.best_params_['ridge_regression__alpha']","1a2fec89":"# do feature selection with best parameters\nfs = SelectKBest(score_func=mutual_info_regression, k=ridge_kbest_best_k)\nfs.fit(X_train_pf, y_train)\nX_train_pf_fs_array = fs.transform(X_train_pf)\nX_test_pf_fs_array = fs.transform(X_test_pf)\n\n# get feature names\nmask = fs.get_support()\nridge_kbest_new_feature_names = X_train_pf.columns[mask]\n\n# create data frames with new feature names\nX_train_pf_fs = pd.DataFrame(X_train_pf_fs_array, columns = ridge_kbest_new_feature_names)\nX_test_pf_fs = pd.DataFrame(X_test_pf_fs_array, columns = ridge_kbest_new_feature_names)\n\nprint(\"Train set: \", X_train_pf_fs.shape)\nprint(\"Test set: \", X_test_pf_fs.shape)\nprint(\"Features selected: \", X_train_pf_fs.columns)","49cd4dc0":"# predict target values for test set using tuned Ridge Regression model with KBest feature selection\ny_predict_ridge_kbest = grid_ridge_kbest.predict(X_test_pf)\n\n# calculate R2 score\nridge_kbest_r2 = r2_score(y_test, y_predict_ridge_kbest)\n\n# calculate mean absolute error\nridge_kbest_mae = mean_absolute_error(y_test, y_predict_ridge_kbest)\n\n# check the stability of the model\nridge_kbest_fi_dev = feature_importance_deviation(Ridge(alpha = ridge_kbest_best_alpha), \n                                     10, \n                                     pd.concat([X_train_pf_fs, X_test_pf_fs]),\n                                     pd.concat([y_train, y_test]))\n\n# add scores for tuned Elastic Net Regression model to scores data frame\nscores = scores.append({'model': 'ridge_regression_kbest', \n                        'mae_train': -grid_ridge_kbest.best_score_,\n                        'mae_test': ridge_kbest_mae,\n                        'r2_test': ridge_kbest_r2,\n                        'max_fi_std': max(ridge_kbest_fi_dev['stdev']\/abs(ridge_kbest_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(ridge_kbest_fi_dev['stdev']\/abs(ridge_kbest_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in ridge_kbest_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","6f005f09":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_ridge_kbest)\nz = numpy.polyfit(y_test, y_predict_ridge_kbest, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Ridge Regression with KBest feature selection\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","c55c9f0e":"# study feature importance deviation\nridge_kbest_fi_dev.head()","2008c779":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Ridge Regression with KBest feature selection')\n\nax1.hist(x = ridge_kbest_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = ridge_kbest_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","73973291":"# melt data frame into long format to visualise feature importance of each split\nridge_kbest_fi_dev_long = pd.melt(ridge_kbest_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in ridge_kbest_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","ed55fc34":"# visualise feature importance values for the features with biggest and smallest deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Ridge Regression with KBest feature selection')\n\nax1.scatter(x = ridge_kbest_fi_dev_long.head(300)['coef'], \n            y = ridge_kbest_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(ridge_kbest_fi_dev_long['coef']) - 0.02, max(ridge_kbest_fi_dev_long['coef']) + 0.02)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = ridge_kbest_fi_dev_long.tail(300)['coef'], \n            y = ridge_kbest_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(ridge_kbest_fi_dev_long['coef']) - 0.02, max(ridge_kbest_fi_dev_long['coef']) + 0.02)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","10203724":"# visualize average feature importance value vs standard deviation\nfig = px.scatter(ridge_kbest_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_fi\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Ridge Regression with KBest feature selection')\nfig.show()","97107efd":"# Ridge Regression with RFE\n# add 'feature_selector' step \nestimator_ridge_rfe = Pipeline([(\"feature_selector\", RFE(estimator=Ridge(alpha = ridge_best_alpha))),\n                                (\"ridge_regression\", Ridge())])\n\n# define parameters for tuning - the number of features to select\nparams_ridge_rfe = {\n    'feature_selector__n_features_to_select': [i for i in range(X_train_pf.shape[1] - 300, X_train_pf.shape[1] - 200, 20)],\n    'ridge_regression__alpha': np.geomspace(1, 10, 5)\n}\n\n# tune model\ngrid_ridge_rfe = GridSearchCV(estimator_ridge_rfe, params_ridge_rfe, scoring = 'neg_mean_absolute_error', \n                              cv=kf, n_jobs = -1)\ngrid_ridge_rfe.fit(X_train_pf, y_train)","0f20ea97":"# look at tuned model\ngrid_ridge_rfe.best_score_, grid_ridge_rfe.best_params_","c96f0e67":"# save best parameters to use in model stability evaluation\nridge_rfe_best_n_features_to_select = grid_ridge_rfe.best_params_['feature_selector__n_features_to_select']\nridge_rfe_best_alpha = grid_ridge_rfe.best_params_['ridge_regression__alpha']","41ce5895":"# do RFE feature selection with best parameters\nrfe = RFE(estimator = Ridge(alpha = ridge_best_alpha), \n          n_features_to_select = ridge_rfe_best_n_features_to_select)\nrfe.fit(X_train_pf, y_train)\nX_train_pf_rfe_array = rfe.transform(X_train_pf)\nX_test_pf_rfe_array = rfe.transform(X_test_pf)\n\n# get feature names\nmask = rfe.get_support()\nridge_rfe_new_feature_names = X_train_pf.columns[mask]\n\n# create data frames with new feature names\nX_train_pf_rfe = pd.DataFrame(X_train_pf_rfe_array, columns = ridge_rfe_new_feature_names)\nX_test_pf_rfe = pd.DataFrame(X_test_pf_rfe_array, columns = ridge_rfe_new_feature_names)\n\nprint(\"Train set: \", X_train_pf_rfe.shape)\nprint(\"Test set: \", X_test_pf_rfe.shape)\nprint(\"Features selected (first 10): \", X_train_pf_rfe.columns[0:10])","4248c87c":"# predict target values for test set using tuned Ridge Regression model with RFE feature selection\ny_predict_ridge_rfe = grid_ridge_rfe.predict(X_test_pf)\n\n# calculate R2 score\nridge_rfe_r2 = r2_score(y_test, y_predict_ridge_rfe)\n\n# calculate mean absolute error\nridge_rfe_mae = mean_absolute_error(y_test, y_predict_ridge_rfe)\n\n# check the stability of the model\nridge_rfe_fi_dev = feature_importance_deviation(Ridge(alpha = ridge_rfe_best_alpha), \n                                     10, \n                                     pd.concat([X_train_pf_rfe, X_test_pf_rfe]),\n                                     pd.concat([y_train, y_test]))\n\n# add scores for tuned Elastic Net Regression model to scores data frame\nscores = scores.append({'model': 'ridge_regression_rfe', \n                        'mae_train': -grid_ridge_rfe.best_score_,\n                        'mae_test': ridge_rfe_mae,\n                        'r2_test': ridge_rfe_r2,\n                        'max_fi_std': max(ridge_rfe_fi_dev['stdev']\/abs(ridge_rfe_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(ridge_rfe_fi_dev['stdev']\/abs(ridge_rfe_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in ridge_rfe_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","c1952385":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_ridge_rfe)\nz = numpy.polyfit(y_test, y_predict_ridge_rfe, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Ridge Regression with RFE feature selection\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","4b7a4acc":"# study feature importance deviation\nridge_rfe_fi_dev.head()","44341b70":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Ridge Regression with RFE feature selection')\n\nax1.hist(x = ridge_rfe_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = ridge_rfe_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","9e39ac86":"# melt data frame into long format to visualise feature importance of each split\nridge_rfe_fi_dev_long = pd.melt(ridge_rfe_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in ridge_rfe_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","2ca3d0ef":"# visualise feature importance values for the features with biggest and smallest deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Ridge Regression with RFE feature selection')\n\nax1.scatter(x = ridge_rfe_fi_dev_long.head(300)['coef'], \n            y = ridge_rfe_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(ridge_rfe_fi_dev_long['coef']) - 0.02, max(ridge_rfe_fi_dev_long['coef']) + 0.02)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = ridge_rfe_fi_dev_long.tail(300)['coef'], \n            y = ridge_rfe_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(ridge_rfe_fi_dev_long['coef']) - 0.02, max(ridge_rfe_fi_dev_long['coef']) + 0.02)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","8dd265f9":"# visualize average feature importance vs standard deviation\nfig = px.scatter(ridge_rfe_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_coef\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Ridge Regression with RFE feature selection')\nfig.show()","fec45a69":"# Ridge Regression with PCA\n# add 'feature_selector' step \nestimator_ridge_pca = Pipeline([(\"feature_selector\", PCA()),\n                                (\"ridge_regression\", Ridge())])\n\n# define parameters for tuning - the number of features to select\nparams_ridge_pca = {\n    'feature_selector__n_components': [i for i in range(50, 71, 2)],\n    'ridge_regression__alpha': np.geomspace(4, 10, 5)\n}\n\n# tune model\ngrid_ridge_pca = GridSearchCV(estimator_ridge_pca, params_ridge_pca, scoring = 'neg_mean_absolute_error', \n                              cv=kf, n_jobs = -1)\ngrid_ridge_pca.fit(X_train_pf, y_train)","e73e0a8f":"# look at tuned model\ngrid_ridge_pca.best_score_, grid_ridge_pca.best_params_","5b28c84e":"# save best parameters to use in model stability evaluation\nridge_pca_best_n_components = grid_ridge_pca.best_params_['feature_selector__n_components']\nridge_pca_best_alpha = grid_ridge_pca.best_params_['ridge_regression__alpha']","f97dd171":"# do PCA feature selection with best parameters\npca = PCA(n_components = ridge_pca_best_n_components)\npca.fit(X_train_pf, y_train)\nX_train_pf_pca_array = pca.transform(X_train_pf)\nX_test_pf_pca_array = pca.transform(X_test_pf)\n\n# get feature names\nridge_pca_new_feature_names = ['feature_{}'.format(x) for x in range(1, X_train_pf_pca_array.shape[1] + 1)]\n\n# create data frames with new feature names\nX_train_pf_pca = pd.DataFrame(X_train_pf_pca_array, columns = ridge_pca_new_feature_names)\nX_test_pf_pca = pd.DataFrame(X_test_pf_pca_array, columns = ridge_pca_new_feature_names)\n\nprint(\"Train set: \", X_train_pf_pca.shape)\nprint(\"Test set: \", X_test_pf_pca.shape)\nprint(\"Features selected (first 10): \", X_train_pf_pca.columns[0:10])","bb9d52c5":"# predict target values for test set using tuned Ridge Regression model with PCA feature selection\ny_predict_ridge_pca = grid_ridge_pca.predict(X_test_pf)\n\n# calculate R2 score\nridge_pca_r2 = r2_score(y_test, y_predict_ridge_pca)\n\n# calculate mean absolute error\nridge_pca_mae = mean_absolute_error(y_test, y_predict_ridge_pca)\n\n# check the stability of the model\nridge_pca_fi_dev = feature_importance_deviation(Ridge(alpha = ridge_pca_best_alpha), \n                                     10, \n                                     pd.concat([X_train_pf_pca, X_test_pf_pca]),\n                                     pd.concat([y_train, y_test]))\n\n# add scores for tuned Elastic Net Regression model to scores data frame\nscores = scores.append({'model': 'ridge_regression_pca', \n                        'mae_train': -grid_ridge_pca.best_score_,\n                        'mae_test': ridge_pca_mae,\n                        'r2_test': ridge_pca_r2,\n                        'max_fi_std': max(ridge_pca_fi_dev['stdev']\/abs(ridge_pca_fi_dev['mean_fi'])),\n                        'av_fi_std': mean(ridge_pca_fi_dev['stdev']\/abs(ridge_pca_fi_dev['mean_fi'])),\n                        'non_zero_fi': len([x for x in ridge_pca_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","709c6de5":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_ridge_pca)\nz = numpy.polyfit(y_test, y_predict_ridge_pca, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Ridge Regression with PCA demensionality reduction\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","3661e4bd":"# study feature importance deviation\nridge_pca_fi_dev.head()","f4a7173b":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Ridge Regression with PCA demensionality reduction')\n\nax1.hist(x = ridge_pca_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = ridge_pca_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","70fe2621":"# melt data frame into long format to visualise feature importance of each split\nridge_pca_fi_dev_long = pd.melt(ridge_pca_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in ridge_pca_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='coef').sort_values(by = ['stdev', 'features'], ascending=False)","fdd2a6a6":"# visualise feature importance values for the features with biggest and smallest deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Ridge Regression with PCA demensionality reduction')\n\nax1.scatter(x = ridge_pca_fi_dev_long.head(300)['coef'], \n            y = ridge_pca_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(ridge_pca_fi_dev_long['coef']) - 0.02, max(ridge_pca_fi_dev_long['coef']) + 0.02)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = ridge_pca_fi_dev_long.tail(300)['coef'], \n            y = ridge_pca_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(ridge_pca_fi_dev_long['coef']) - 0.02, max(ridge_pca_fi_dev_long['coef']) + 0.02)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","a2a6f382":"# visualize average feature importance vs standard deviation\nfig = px.scatter(ridge_pca_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = ridge_fi_dev[['features']],\n                 labels={\n                     \"mean_coef\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Ridge Regression with PCA demensionality reduction')\nfig.show()","7267aa9b":"# get average feature importance value for new features\nridge_pca_new_fi_array = np.array(ridge_pca_fi_dev['mean_fi']).reshape((58, 1))\n\n# multiply feature importance by components (abs(pca feature components))\nridge_pca_feature_importance_2d_array = np.multiply(abs(ridge_pca_new_fi_array), abs(pca.components_))\n\n# put into data frame\nridge_pca_feature_importance_2d = pd.DataFrame(ridge_pca_feature_importance_2d_array, columns = X_train_pf.columns).set_index(pd.Index(ridge_pca_new_feature_names))\n\n# sum all importance copmonents for each old feature\nridge_pca_feature_importance = ridge_pca_feature_importance_2d.sum(axis=0).to_frame(name='mean_fi').rename_axis('features').reset_index()\nridge_pca_feature_importance.head(3)","7c354f86":"# plot distribution of feature importance deviation\nplt.figure(figsize=(5, 5))\nridge_pca_feature_importance.hist(column = 'mean_fi', bins = 20)\nplt.title(\"Distridution of the importance values for the original features\\n measured by Ridge Regression with PCA demensionality reduction\")\nplt.xlabel(\"Importance values for the original features\")\nplt.ylabel(\"Count\")\nplt.show()","d337cffc":"# Random Forest Regressor\n# define pipeline\nestimator_rf = RandomForestRegressor()\n\n# define parameters for tuning\nparams_rf = {\n    'n_estimators': [300, 500, 1000], \n    'max_features': ['sqrt'],\n    'max_depth': [10, 30, 50, 80], \n    'min_samples_split': [2, 5, 10],\n    'min_samples_leaf': [1, 2, 4],\n    'bootstrap': [False]\n}\n\n# do grid search\nrandom_rf = RandomizedSearchCV(estimator_rf, params_rf, cv=3, n_iter = 70,\n                               scoring = 'neg_mean_absolute_error', random_state=42, \n                               n_jobs = -1, verbose=1)\nrandom_rf.fit(X_train_pf, y_train)","c59f6b26":"# look at tuned model\nrandom_rf.best_score_, random_rf.best_params_","f9126ef7":"# random forest model with best parameters\nbest_rf = RandomForestRegressor(n_estimators = random_rf.best_params_['n_estimators'],\n                                min_samples_split = random_rf.best_params_['min_samples_split'],\n                                min_samples_leaf = random_rf.best_params_['min_samples_leaf'],\n                                max_features = random_rf.best_params_['max_features'],\n                                max_depth = random_rf.best_params_['max_depth'],\n                                bootstrap = random_rf.best_params_['bootstrap'])\n\n# predict target values for test set using tuned Random Forest model \ny_predict_rf = random_rf.best_estimator_.predict(X_test_pf)\n\n# calculate R2 score\nrf_r2 = r2_score(y_test, y_predict_rf)\n\n# calculate mean absolute error\nrf_mae = mean_absolute_error(y_test, y_predict_rf)\n\n# check the stability of the model\nrf_fi_dev = feature_importance_deviation(best_rf, \n                                     10, \n                                     pd.concat([X_train_pf, X_test_pf]),\n                                     pd.concat([y_train, y_test]),\n                                     attr = 'feature_importances_')\n\n# add scores for tuned Random Forest model to scores data frame\nscores = scores.append({'model': 'random_forest', \n                        'mae_train': -random_rf.best_score_,\n                        'mae_test': rf_mae,\n                        'r2_test': rf_r2,\n                        'max_fi_std': max(rf_fi_dev['stdev']\/rf_fi_dev['mean_fi']),\n                        'av_fi_std': mean(rf_fi_dev['stdev']\/rf_fi_dev['mean_fi']),\n                        'non_zero_fi': len([x for x in rf_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","17a5a009":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_rf)\nz = numpy.polyfit(y_test, y_predict_rf, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Random Forest\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","37bab4ab":"# study feature importances deviation\nrf_fi_dev.head()","04eed58d":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Random Forest')\n\nax1.hist(x = rf_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = rf_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","d68a4822":"# melt data frame into long format to visualise feature importance of each split\nrf_fi_dev_long = pd.melt(rf_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in rf_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='f_importance').sort_values(by = ['stdev', 'features'], ascending=False)","ff2ee87a":"# visualise feature importance values for the features with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Random Forest')\n\nax1.scatter(x = rf_fi_dev_long.head(300)['f_importance'], \n            y = rf_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(rf_fi_dev_long['f_importance']) - 0.004, max(rf_fi_dev_long['f_importance']) + 0.004)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = rf_fi_dev_long.tail(300)['f_importance'], \n            y = rf_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(rf_fi_dev_long['f_importance']) - 0.004, max(rf_fi_dev_long['f_importance']) + 0.004)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","0e6c9c69":"# visualize average feature importance vs standard deviation\nfig = px.scatter(rf_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = rf_fi_dev[['features']],\n                 labels={\n                     \"mean_fi\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Random Forest')\nfig.show()","6b0e6f9c":"# Gradient Boosting Regressor\n# define pipeline\nestimator_gbr = GradientBoostingRegressor()\n\n# define parameters for tuning\nparams_gbr = {\n    'n_estimators': [500, 1000, 1500], \n    'max_depth': [2], \n    'learning_rate': [0.01],\n    'loss': ['huber'],\n    'criterion': ['mse'],\n    'max_features': ['sqrt']\n}\n\n# do random search\ngrid_gbr = GridSearchCV(estimator_gbr, params_gbr, cv=kf,\n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = -1, verbose=1)\ngrid_gbr.fit(X_train_pf, y_train)","8d9580fc":"# look at tuned model\ngrid_gbr.best_score_, grid_gbr.best_params_","845de882":"# random forest model with best parameters\nbest_gbr = GradientBoostingRegressor(n_estimators = grid_gbr.best_params_['n_estimators'],\n                                     max_depth = grid_gbr.best_params_['max_depth'],\n                                     learning_rate = grid_gbr.best_params_['learning_rate'],\n                                     max_features = grid_gbr.best_params_['max_features'],\n                                     loss = 'huber',\n                                     criterion = 'mse')\n\n# predict target values for test set using tuned Gradient Boosting Regressor\ny_predict_gbr = grid_gbr.best_estimator_.predict(X_test_pf)\n\n# calculate R2 score\ngbr_r2 = r2_score(y_test, y_predict_gbr)\n\n# calculate mean absolute error\ngbr_mae = mean_absolute_error(y_test, y_predict_gbr)\n\n# check the stability of the model\ngbr_fi_dev = feature_importance_deviation(best_gbr, \n                                     10, \n                                     pd.concat([X_train_pf, X_test_pf]),\n                                     pd.concat([y_train, y_test]),\n                                     attr = 'feature_importances_')\n\n# add scores for tuned Gradient Boosting model to scores data frame\nscores = scores.append({'model': 'gradient_boosting', \n                        'mae_train': -grid_gbr.best_score_,\n                        'mae_test': gbr_mae,\n                        'r2_test': gbr_r2,\n                        'max_fi_std': max(gbr_fi_dev['stdev']\/gbr_fi_dev['mean_fi']),\n                        'av_fi_std': mean(gbr_fi_dev['stdev']\/gbr_fi_dev['mean_fi']),\n                        'non_zero_fi': len([x for x in gbr_fi_dev['mean_fi'].tolist() if x > 0])},\n                        ignore_index=True)\n\nscores","3b225f54":"# visualize predicted vs actual target values\nplt.figure(figsize=(6, 6))\nplt.scatter(x = y_test, \n            y = y_predict_gbr)\nz = numpy.polyfit(y_test, y_predict_gbr, 1)\np = numpy.poly1d(z)\nplt.plot(y_test,p(y_test),\"r--\")\nplt.title(\"Happiness Scores: Actual vs Predicted by Gradient Boosting\")\nplt.xlabel(\"Actual Happiness Scores\")\nplt.ylabel(\"Predicted Happiness Scores\")","60fa5cc9":"# study feature importances deviation\ngbr_fi_dev.head()","873de0fa":"# plot distribution of feature importance values and their standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,4))\nfig.suptitle('Feature importance values and threir variance for Gradient Boosting')\n\nax1.hist(x = gbr_fi_dev['mean_fi'], bins = 20)\nax1.set_xlabel(\"Average feature importance\")\nax1.set_ylabel(\"Count\")\n\nax2.hist(x = gbr_fi_dev['stdev'], bins = 20)\nax2.set_xlabel(\"Standard deviation of the feature importance\")\nax2.set_ylabel(\"Count\")\n\nfig.tight_layout()\nfig.show()","c9df0980":"# melt data frame into long format to visualise feature importance of each split\ngbr_fi_dev_long = pd.melt(gbr_fi_dev, id_vars=['features', 'stdev', 'mean_fi'], \n                      value_vars=[col for col in gbr_fi_dev.columns \n                                  if col not in ['features', 'stdev', 'mean_fi']],\n                      var_name='fold', \n                      value_name='f_importance').sort_values(by = ['stdev', 'features'], ascending=False)","84f777bc":"# visualise feature importances values for the features with biggest and smallest standard deviation\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10,7))\nfig.suptitle('Feature importance variance for Gradient Boosting')\n\nax1.scatter(x = gbr_fi_dev_long.head(300)['f_importance'], \n            y = gbr_fi_dev_long.head(300)['features'])\nax1.set_title(\"30 features with biggest standard deviation of the feature importance\")\nax1.set_xlabel(\"Feature importance\")\nax1.set_xlim(min(gbr_fi_dev_long['f_importance']) - 0.004, max(gbr_fi_dev_long['f_importance']) + 0.004)\nax1.set_ylabel(\"Features\")\nax1.set_ylim(-1, 30)\nax1.invert_yaxis()\n\nax2.scatter(x = gbr_fi_dev_long.tail(300)['f_importance'], \n            y = gbr_fi_dev_long.tail(300)['features'])\nax2.set_title(\"30 features with smallest standard deviation of the feature importance\")\nax2.set_xlabel(\"Feature importance\")\nax2.set_xlim(min(gbr_fi_dev_long['f_importance']) - 0.004, max(gbr_fi_dev_long['f_importance']) + 0.004)\nax2.set_ylim(-1, 30)\nax2.invert_yaxis()\n\nfig.tight_layout()\nfig.show()","d4503cb3":"# visualize average feature importance vs standard deviation\nfig = px.scatter(gbr_fi_dev, x=\"mean_fi\", y=\"stdev\",\n                 width=500, height=500,\n                 hover_data = gbr_fi_dev[['features']],\n                 labels={\n                     \"mean_importance\": \"Average feature importance\",\n                     \"stdev\": \"Standard deviation\"\n                 },\n                 title='Average feature importance vs Standard deviation <br>for Gradient Boosting')\nfig.show()","e8aa1902":"# put best models in a list\nbest_models = ['ridge_regression', 'elastic_net_regression', \n               'ridge_regression_rfe', 'ridge_regression_pca', \n               'random_forest', 'gradient_boosting']\n\nbest_models_predicted = [y_predict_ridge, y_predict_net, \n                         y_predict_ridge_rfe, y_predict_ridge_pca, \n                         y_predict_rf, y_predict_gbr]\n\n# build scatter plot of actual vs predicted happiness scores for best models\nfig, axs = plt.subplots(2, 3, sharex=True, sharey=True, figsize=(9, 6))\n\nn = 0\nfor i in range(2):\n    for j in range(3):\n        axs[i, j].scatter(x = y_test, \n                          y = best_models_predicted[n])\n        z = numpy.polyfit(y_test, best_models_predicted[n], 1)\n        p = numpy.poly1d(z)\n        axs[i, j].plot(y_test,p(y_test),\"r--\")\n        axs[i, j].set_title(best_models[n])\n        axs[i, j].text(3.5, 7.5, 'R2 test score: {:.3f}'.format(scores[scores.model ==  best_models[n]]['r2_test'].tolist()[0]))\n        n += 1\n        \nfig.tight_layout()\nfig.show()","c7f6adbf":"# combine feature importance values for best models into one dataframe for comparison\nbest_models_fi_ranking = []\nbest_models_fi_values = []\n\ndevs = [ridge_fi_dev, net_fi_dev, ridge_rfe_fi_dev, ridge_pca_feature_importance, rf_fi_dev, gbr_fi_dev]\nbest_models_suffix = ['ridge', 'net', 'ridge_rfe', 'ridge_pca', 'rf', 'gbr']\n\nfor i,df in enumerate(devs):\n    # extract feature importance values\n    fi_values =  df[['features', 'mean_fi']].copy()\n    fi_ranks = df[['features', 'mean_fi']].copy()\n    \n    #add ranking for features\n    fi_ranks[best_models_suffix[i]] = fi_ranks['mean_fi'].rank(axis=0, method='max', na_option='bottom', ascending=False)\n    fi_ranks.drop(['mean_fi'], axis=1, inplace=True)\n    best_models_fi_ranking.append(fi_ranks)\n    \n    # scale feature importance for comparison\n    mms = MinMaxScaler()\n    fi_values[['mean_fi']] = mms.fit_transform(fi_values[['mean_fi']])\n    \n    fi_values.rename(columns={'mean_fi': best_models_suffix[i]}, inplace=True)\n    best_models_fi_values.append(fi_values)\n\n# merge data for best models into one dataframe\nfi_ranking = reduce(lambda left, right: pd.merge(left, right,  on = 'features', how = 'outer'), best_models_fi_ranking)\nfi_ranking.fillna(len(fi_ranking['features']), inplace=True)\nfi_ranking['mean'] = fi_ranking.apply(lambda row: mean([row[col] for col in fi_ranking.columns if col != 'features']), axis = 1)\n\nfi_values = reduce(lambda left, right: pd.merge(left, right, on = 'features', how = 'outer'), best_models_fi_values)\nfi_values.fillna(0, inplace=True)\nfi_values['mean'] = fi_values.apply(lambda row: mean([row[col] for col in fi_values.columns if col != 'features']), axis = 1)\n\n# create long data frame for visualization\nfi_ranking_long = pd.melt(fi_ranking, id_vars=['features'], \n                          value_vars=[col for col in fi_ranking.columns if col != 'features'],\n                          var_name='model', value_name='rank')\n\nfi_values_long = pd.melt(fi_values, id_vars=['features'], \n                          value_vars=[col for col in fi_values.columns if col != 'features'],\n                          var_name='model', value_name='feature_importance')\n\nfi_long = fi_values_long.merge(fi_ranking_long, how = 'outer', on = ['features', 'model'])","6acebd66":"# look at how feature importance is distributed among features for each of the best models\nfig = px.scatter(fi_long, x = 'feature_importance', y = 'rank', color = 'model',\n                 width=800, height=500,\n                 hover_data = fi_long[['features']],\n                 labels={\n                     \"feature_importance\": \"Average feature importance\",\n                     \"rank\": \"Feature rank\",\n                     \"model\": \"Model\"\n                 },\n                 title='Average feature importance vs Feature ranks <br>for different models')\nfig.show()","6dbed383":"# visualize and compare feature importance values for different models\nfig = px.imshow(fi_values.sort_values(by = ['mean'], ascending=False).set_index('features').T, aspect=\"auto\",\n                width=1000, height=500,\n               labels=dict(x=\"Features\", y=\"Models\", color=\"Feature Importance\"))\nfig.update_xaxes(side=\"top\", tickangle=270, \n                 rangeslider=dict(autorange=True, thickness=0.05))\nfig.show()","08634923":"# compare feature importance values\n# Elastic Net vs Ridge \nfig = px.scatter(fi_values, x=\"ridge\", y=\"net\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"ridge\": \"Measured by Ridge Regression\",\n                     \"net\": \"Measured by Elastic Net Regression\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\nfig.show()","8d33f323":"# compare feature importance values\n# Ridge with RFE vs Ridge \nfig = px.scatter(fi_values, x=\"ridge\", y=\"ridge_rfe\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"ridge\": \"Measured by Ridge Regression\",\n                     \"ridge_rfe\": \"Measured by Ridge Regression with RFE\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\nfig.show()","ab08f7f9":"# compare feature importance values\n# Ridge with PCA vs Ridge \nfig = px.scatter(fi_values, x=\"ridge\", y=\"ridge_pca\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"ridge\": \"Measured by Ridge Regression\",\n                     \"ridge_pca\": \"Measured by Ridge Regression with PCA\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\nfig.show()","1bc0e824":"# compare feature importance values\n# Random Forest vs Ridge \nfig = px.scatter(fi_values, x=\"ridge\", y=\"rf\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"ridge\": \"Measured by Ridge Regression\",\n                     \"rf\": \"Measured by Random Forest\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\nfig.show()","3a57f470":"# compare feature importance values\n# Gradient Boosting vs Ridge \nfig = px.scatter(fi_values, x=\"ridge\", y=\"gbr\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"ridge\": \"Measured by Ridge Regression\",\n                     \"gbr\": \"Measured by Gradient Boosting\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\nfig.show()","5d2727a8":"# compare feature importance values\n# Random Forest vs Gradient Boosting\nfig = px.scatter(fi_values, x=\"gbr\", y=\"rf\", color=\"mean\",\n                 width=800, height=500,\n                 trendline=\"ols\", \n                 hover_data = fi_values[['features']],\n                 labels={\n                     \"gbr\": \"Measured by Gradient Boosting\",\n                     \"rf\": \"Measured by Random Forest\",\n                     \"mean\": \"Average feature importance\"\n                 },\n                 title='Feature Importance')\n\nfig.show()","6fa15bb7":"# look at smaller set of best models - remove Ridge with RFE and Ridge with PCA\nfi_values_small = fi_values.drop(columns = ['mean', 'ridge_pca', 'ridge_rfe']).copy()\nfi_values_small['mean'] = fi_values_small.apply(lambda row: mean([row[col] for col in fi_values_small.columns if col != 'features']), axis = 1)","9a344a32":"# visualize and compare feature importance values for 4 models:\n# 2 based on Linear Regression and 2 based on Decision Trees\nfig = px.imshow(fi_values_small.sort_values(by = ['mean'], ascending=False).set_index('features').T, aspect=\"auto\",\n                width=1000, height=500,\n               labels=dict(x=\"Features\", y=\"Models\", color=\"Feature Importance\"))\nfig.update_xaxes(side=\"top\", tickangle=270, \n                 rangeslider=dict(autorange=True, thickness=0.05))\nfig.show()","ca840ab2":"# sort feature by their average importance values\nfi_values_small.sort_values(by = ['mean'], ascending=False).reset_index(drop=True).head(30)","5b1fe9ab":"# Most important economic-related feature combinations with 'Life expectancy at age 60'\ndf = data[['country', 'life_exp60', 'development_index', 'eco_footprint',\n           'gdp_per_capita', 'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'life_exp60', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'life_exp60', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"life_exp60\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400,\n                 trendline=\"ols\", trendline_options=dict(log_x=True),\n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"life_exp60\": \"Life expectancy at age 60 (years)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"gdp_per_capita\", \"development_index\", \"eco_footprint\"]},\n                 title=\"Most important economic-related feature combinations with 'Life expectancy at age 60'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","85a6bf03":"# Most important health-related feature combinations with 'Life expectancy at age 60'\ndf = data[['country', 'life_exp60', 'infant_mort', 'bmi', 'adult_mortality', \n           'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'life_exp60', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'life_exp60', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"life_exp60\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400,\n                 trendline=\"ols\", trendline_options=dict(log_x=True),\n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"life_exp60\": \"Life expectancy at age 60 (years)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"bmi\", \"infant_mort\", \"adult_mortality\"]},\n                 title=\"Most important health-related feature combinations with 'Life expectancy at age 60'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","0b671040":"# Most important health-related feature combinations with 'GDP per capita'\ndf = data[['country', 'life_expect', 'infant_mort', 'bmi', \n           'gdp_per_capita', 'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'gdp_per_capita', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'gdp_per_capita', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"gdp_per_capita\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400, \n                 trendline=\"lowess\", \n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"gdp_per_capita\": \"GDP per capita (dollars)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"life_expect\", \"bmi\", \"infant_mort\"]},\n                 title=\"Most important health-related feature combinations with 'GDP per capita'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","1d13b6ea":"# Other important feature combinations with 'GDP per capita'\ndf = data[['country', 'gdp_per_capita', 'happiness_score',\n          'basic_water', 'pf_rol', 'eco_footprint']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'gdp_per_capita', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'gdp_per_capita', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"gdp_per_capita\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400, \n                 trendline=\"lowess\", \n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"gdp_per_capita\": \"GDP per capita (dollars)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"basic_water\", \"eco_footprint\", \"pf_rol\"]},\n                 title=\"Other important feature combinations with 'GDP per capita'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","b54e8929":"# Most important feature combinations with 'Human Development Index'\ndf = data[['country', 'infant_mort', 'bmi', 'development_index', \n           'gdp_per_capita', 'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'development_index', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'development_index', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"development_index\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400, \n                 trendline=\"ols\", trendline_options=dict(log_x=True),\n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"development_index\": \"Human Development Index\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"gdp_per_capita\", \"bmi\", \"infant_mort\"]},\n                 title=\"Most important feature combinations with 'Human Development Index'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","bd0ba3d6":"# Important feature combinations with 'Life expectancy at birth'\ndf = data[['country', 'life_expect', 'development_index', 'eco_footprint',\n           'pf_rol', 'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'life_expect', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'life_expect', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"life_expect\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1200, height=400, \n                 trendline=\"ols\", trendline_options=dict(log_x=True),\n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"life_expect\": \"Life expectancy at birth (years)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"eco_footprint\", \"development_index\", \"pf_rol\"]},\n                 title=\"Important feature combinations with 'Life expectancy at birth'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","de569f5d":"# Important health-related feature combinations with 'Life expectancy at birth'\ndf = data[['country', 'life_expect', 'life_exp60', 'adult_mortality',\n           'infant_mort', 'bmi', 'happiness_score']].copy()\ndf_long = pd.melt(df, id_vars=['country', 'life_expect', 'happiness_score'],\n                  value_vars=[col for col in df.columns if col not in ['country', 'life_expect', 'happiness_score']],\n                  var_name='variable', value_name='value')\ndf_long.variable = df_long.variable.astype(\"category\")\n\nfig = px.scatter(df_long, y=\"life_expect\", x=\"value\", \n                 color=\"happiness_score\", facet_col=\"variable\",\n                 width=1500, height=400, \n                 trendline=\"ols\", trendline_options=dict(log_x=True),\n                 hover_data = df_long[['country', 'happiness_score']],\n                 labels={\n                     \"life_expect\": \"Life expectancy at birth (years)\",\n                     \"variable\": \"Variable\",\n                     \"happiness_score\": \"Happiness Score\"\n                 },\n                 category_orders={\"variable\": [\"life_exp60\", \"bmi\", \"infant_mort\", \"adult_mortality\"]},\n                 title=\"Important health-related feature combinations with 'Life expectancy at birth'\",\n                 trendline_color_override=\"black\")\nfig.update_traces(marker=dict(size=10,\n                              line=dict(width=2,\n                                        color='DarkSlateGrey')),\n                  selector=dict(mode='markers'))\nfig.update_xaxes(matches=None)\nfig.show()","b769805c":"Heatmap above shows that feature importance values agree with each other and with mean values for most of the models. The exception seems to be Ridge Regression with `PCA()`.  \n   \nTo get a better idea of how feature importance values measured by different models compare to each other, I will build scatter plots using baseline Ridge Regression as a reference.","49f39a25":"The average feature importance distribution is strongly positively skewed. Even though Random Forest uses the same amount of features as baseline Ridge Regression, it has a lot of features with importance values close to zero.","58624efc":"### Ridge Regression with `RFE()`","ecb6e560":"One can see from the graph above that there are two distinctive types of curves:\n* s-type curves for baseline Ridge Regression and Ridge Regression with `PCA()`;\n* reciprocal-type curves for Random Forest and Gradient Boosting.  \n   \nLinear regression models seem to distribute importance among features more evenly, but Decision Tree based models put more weight on some of the features and almost zero out the rest of them.\n   \nElastic Net Regression and Ridge Regression with `RFE()` both have a reduced amount of features.  \nRidge Regression with `RFE()` model displays s-type curve as well, but Elastic Net Regression's curve resembles both types of curves.  \n   \nThe curve for Ridge Regression with `PCA()` is shifted to the right which represents higher feature importance values overall.","26524e41":"### Elastic Net Regression","bcfa7da3":"## Linear Regression Models\n### Linear Regression","b88d3f87":"To interpret **OLS Regression summary** I used [Interpreting Linear Regression Through statsmodels .summary()](https:\/\/medium.com\/swlh\/interpreting-linear-regression-through-statsmodels-summary-4796d359035a) by Tim McAleer as guidelines.\n\n#### R-squared \n> R-squared is the measurement of how much of the independent variable is explained by changes in our dependent variables.\n\nR-squared for this model is 0.816, which means that the model explains 81.6% of the change in our target variable.\n\n#### Adjusted R-squared\n> The adjusted R-squared penalizes the R-squared formula based on the number of variables, therefore a lower adjusted score may be telling you some variables are not contributing to your model\u2019s R-squared properly.\n\nThe adjusted R-squared is 0.773. It is lower than R-squared, so some of the variables are probably contributing poorly to the model. Feature selection will be an important step in this analysis.  \nLasso Regression model is likely to perform feature selection on its own, but the Ridge Regression model might benefit from the Feature Selection technique added to the Pipeline.\n\n#### Prob (F-Statistic)\n> The F-statistic in linear regression is comparing your produced linear model for your variables against a model that replaces your variables\u2019 effect to 0, to find out if your group of variables are statistically significant. To interpret this number correctly, using a chosen alpha value and an F-table is necessary. Prob (F-Statistic) uses this number to tell you the accuracy of the null hypothesis, or whether it is accurate that your variables\u2019 effect is 0.\n\nProb (F-Statistic) is 1.85e-29. It means that there is 1.85\\*10^-27 % chance that the null hypothesis (variables\u2019 effect on the target is 0) is accurate. Prob (F-Statistic) is extremely small so we reject null hypothesis and state that this set of variables has an effect on the target.\n\n#### Coefficients, p-values  and confidence intervals\n> P>|t| is one of the most important statistics in the summary. It uses the t statistic to produce the p value, a measurement of how likely your coefficient is measured through our model by chance.\n> Proper model analysis will compare the p value to a previously established alpha value, or a threshold with which we can apply significance to our coefficient. A common alpha is 0.05.\n\nIf alpha = 0.05, then only one variable ('infant_mort_cbrt') passes the threshold. Summary also shows that for almost all other variables confidence intervals ([0.025   0.975]) include 0. Almost all of our coefficients were likely measured through our model by chance. It is a bad result for model interpretation, simple OLS Regression used with this set of features does not allow us to determine the significance of variables for predicting the target.\n  \nFor example, let's look at the 'development_index'. It has one of the strongest correlations with the target but the OLS Regression coefficient for it is -0.0904 as if changes in Development Index had almost no effect on the Happiness Score. Now, P>|t| for 'development_index' is 0.938, so there is a 93.8% probability that the model measured this coefficient purely by chance.  \n   \nBased on this summary, I expect Linear Regression to perform poorly on this set of features. These features definitely have an effect on the target, but they don't work together very well (probably due to multicollinearity). Some feature engineering and feature selection steps are needed to improve a regression model performance.","41822547":"Elastic Net Regression:\n* performs slightly worse than Ridge Regression but better than LASSO Regression;\n* uses less amount of features than Ridge Regression but more than LASSO Regression;\n* shows roughly the same stability as LASSO Regression.","c7e1b11e":"The MAE for the train set is significantly smaller than MAE for the test set which indicates overfitting.  \nR2 score is negative, we can conclude that the Linear Regression without any regularizations performs poorly.","3a46f0b7":"**Observations:**\n1. The most prominent features are:\n    * *health-related*:\n         * Life expectancy at age 60\n         * Mean Body Mass Index (BMI)\n         * Life expectancy at birth\n         * Infant Mortality\n         * Adult Mortality\n         * Access to basic drinking-water services\n    * *economic-related*:\n         * Gross Domestic Product (GDP) per capita\n         * Human Development Index\n         * Ecological footprint (use of ecological assets)\n2. *Freedom-related* features do not appear in the top 25, and the most prominent freedom-related features are:\n    * Rule of Law Personal Freedom score\n    * Security and Safety Personal Freedom score\n    * Religion Personal Freedom score   \n   \n**Feature relationships from top 30 to visualize:**   \n   \n|Y-axis                   |X-axis - subplots                                                                                                                      |\n|:------------------------|:--------------------------------------------------------------------------------------------------------------------------------------|\n|Life expectancy at age 60|- GDP per capita (rank 1); <br>- Human Development Index (rank 7); <br>- Ecological footprint (rank 13)                                |\n|Life expectancy at age 60|- BMI (rank 2); <br>- Infant Mortality (rank 9); <br>- Adult Mortality (rank 20)                                                       |\n|GDP per capita           |- Life expectancy at birth (rank 4); <br>- BMI (rank 6); <br>- Infant Mortality (rank 8)                                               |\n|GDP per capita           |- Access to drinking-water services (rank 18); <br>- Ecological footprint (rank 25); <br>- Rule of Law Personal Freedom score (rank 26)|\n|Human Development Index  |- GDP per capita (rank 3); <br>- BMI (rank 21); <br>- Infant Mortality (rank 22)                                                       |\n|Life expectancy at birth |- Ecological footprint (rank 10); <br>- Human Development Index (rank 14); <br>- Rule of Law Personal Freedom score (rank 26)          |\n|Life expectancy at birth |- Life expectancy at age 60 (rank 12); <br>- BMI (rank 15); <br>- Infant Mortality (rank 16); <br>- Adult Mortality (rank 17)          |","35e308df":"It is interesting to note that almost all feature importance values recalculated for the original set of features are larger than 0.1. This model has very few features with an importance value close to zero.","84a39957":"## Linear Regression Models with Feature Selection or Dimensionality Reduction\n### Ridge Regression with `SelectKBest()`","ec47343e":"Predicting features that have the **strongest correlation with the target** are:\n* GDP per capita\n* Human development index\n* Life expectancy at birth   \n   \nPredicting features that have the **weakest correlation with the target** are:\n* Population\n* Personal Freedom score (Religion)\n* Biocapacity Reserve","fb51801e":"Gradient Boosting Regressor:\n* performs better than baseline Ridge Regression but worse than Random Forest Regressor;\n* uses the same amount of features as baseline Ridge Regression;\n* shows worse stability than baseline Ridge Regression and Random Forest Regressor.","bd6c8785":"`SelectKBest()` feature selection doesn't improve the performance of Ridge Regression.","39130a5a":"## Multicollinearity analysis","0ecec16b":"# Models\n","714baa8d":"## Compare best models\nI chose 6 models with good performance for initial consideration:\n* Ridge Regression\n* Elastic Net Regression\n* Ridge Regression with `RFE()`\n* Ridge Regression with `PCA()`\n* Random Forest Regressor\n* Gradient Boosting Regressor","e81b0320":"<p style=\"text-align: center; font-weight: 700;\"> \nThank you for reading!<br \/>\nAny feedback is greatly appreciated.\n\n<\/p>","1ba96a3e":"### Gradient Boosting Regressor","b4a45e5e":"## Data normalization","d7dcaec2":"LASSO Regression:\n* performs slightly worse than Ridge Regression;\n* has much less amount of features with non-zero importance\/coefficient;\n* is less stable than Ridge Regression.","afb000c0":"### LASSO Regression","a5523393":"From the 'statistics' table above, one can see that variables have very different ranges. That is why scaling the data before fitting a model is extremely important when the focus of the analysis is on the interpretation. With scaled data, I will interpret the magnitude of the regression coefficient in the terms of how much the change of the given feature variable for 1%  within its range would impact the target variable.  \nOnly one variable - 'deficit_or_reserve' - has negative values. They could cause problems during the feature transformation step. Let's break this variable into 2: 'bio_deficit' and 'bio_reserve'.","ceda714d":"Ridge Regression with `RFE()` feature selection:\n* performs slightly better than Ridge Regression with `SelectKBest()` but still worse than baseline Ridge Regression;\n* uses significantly fewer features than baseline Ridge Regression;\n* shows slightly better stability than baseline Ridge Regression.","578fb5b0":"Ridge Regression:\n* performs much better than Linear Regression;\n* uses almost the same amount of features;\n* shows roughly the same stability as Linear Regression.","f315333b":"Random Forest Regressor:\n* performs significantly better than baseline Ridge Regression;\n* uses the same amount of features as baseline Ridge Regression;\n* shows similar stability as baseline Ridge Regression.","2c0a35d4":"# Introduction\nIn this notebook, I will dive deeper into the tuning, evaluation, and interpretation of various regression models to find out which health-related, economic-related, and freedom-related metrics have more prominent effects on world countries' happiness levels.  \n   \n> The World Happiness Report is a landmark survey of the state of global happiness. The first report was published in 2012, the second in 2013, the third in 2015, and the fourth in the 2016 Update. The World Happiness 2017, which ranks 155 countries by their happiness levels, was released at the United Nations at an event celebrating International Day of Happiness on March 20th.  \n> The reports review the state of happiness in the world today and show how the new science of happiness explains personal and national variations in happiness.  \n> The happiness scores and rankings use data from the Gallup World Poll. The scores are based on answers to the main life evaluation question asked in the poll. This question, known as the Cantril ladder, asks respondents to think of a ladder with the best possible life for them being a 10 and the worst possible life being a 0 and to rate their own current lives on that scale.  \n>  \n> [World Happiness Report](https:\/\/www.kaggle.com\/unsdsn\/world-happiness)  \n  \n## Main objective\nIn this project, I will look at world metrics such as [health and life expectancy data](https:\/\/www.kaggle.com\/mmattson\/who-national-life-expectancy), [ecological footprint](https:\/\/www.kaggle.com\/footprintnetwork\/ecological-footprint), and [human freedom scores](https:\/\/www.kaggle.com\/gsutters\/the-human-freedom-index) for year 2016 and try to figure out which of them have more prominent effects on [happiness levels](https:\/\/www.kaggle.com\/unsdsn\/world-happiness). I will build multiple ML models, tune them and focus on the interpretation and feature imporatance.\n\n## Data\nThe data set I prepared for this project is composed of 4 merged data sets and covers 137 countries for the year 2016. The table below shows a brief description of data set attributes.  \n\n|No|Column              |Description                                                       |Data type|Source|\n|--|--------------------|------------------------------------------------------------------|:-------:|:----:|\n|01|country             |Country name                                                      |object   |      |\n|02|life_expect         |Life expectancy at birth (years)                                  |float    |1     |\n|03|life_exp60          |Life expectancy at age 60 (years)                                 |float    |1     |\n|04|adult_mortality     |Adult (15 to 60 years) Mortality Rates (per 1000 population)      |float    |1     |\n|05|infant_mort         |Death rate up to age 1                                            |float    |1     |\n|06|age1-4mort          |Death rate between ages 1 and 4                                   |float    |1     |\n|07|alcohol             |Alcohol consumption per capita (15+, in litres of pure alcohol)   |float    |1     |\n|08|bmi                 |Mean Body Mass Index (kg\/m^2) (18+, age-standardized estimate)    |float    |1     |\n|09|basic_water         |Population using at least basic drinking-water services           |float    |1     |\n|10|pop_mils            |Population (millions)                                             |float    |2     |\n|11|development_index   |Human Development Index                                           |float    |2     |\n|12|gdp_per_capita      |Gross Domestic Product (GDP) per capita (dollars)                 |float    |2     |\n|13|eco_footprint       |Use of ecological assets (measured in gha per person)             |float    |2     |\n|14|biocapacity         |Productivity of ecological assets (measured in gha per person)    |float    |2     |\n|15|defecit_or_reserve  |Biocapacity Deficit or Reserve (measured in gha per person)       |float    |2     |\n|16|pf_rol              |Personal Freedom score (Rule of Law)                              |float    |3     |\n|17|pf_ss               |Personal Freedom score (Security and Safety)                      |float    |3     |\n|18|pf_movement         |Personal Freedom score (Movement)                                 |float    |3     |\n|19|pf_religion         |Personal Freedom score (Religion)                                 |float    |3     |\n|20|pf_expression       |Personal Freedom score (Expression and Information)               |float    |3     |\n|21|pf_identity         |Personal Freedom score (Identity and Relationships)               |float    |3     |\n|22|pf_score            |General Personal Freedom score                                    |float    |3     |\n|23|ef_government       |Economic Freedom score (Size of Government)                       |float    |3     |\n|24|ef_legal            |Economic Freedom score (Legal System and Property Rights)         |float    |3     |\n|25|ef_money            |Economic Freedom score (Access to Sound Money)                    |float    |3     |\n|26|ef_trade            |Economic Freedom score (Freedom to Trade Internationally)         |float    |3     |\n|27|ef_regulation       |Economic Freedom score (Regulation of Credit, Labor, and Business)|float    |3     |\n|28|ef_score            |General Economic Freedom score                                    |float    |3     |\n|29|hf_score            |General Human Freedom score                                       |float    |3     |\n|30|happiness_score     |Ladder score (between 10 (best) and 0 (worst) possible life)      |float    |4     |\n  \n  \nYou can find the notebook on cleaning and merging source data sets [here](https:\/\/www.kaggle.com\/dariasvasileva\/merging-world-metrics-sets).\n  \n#### Sources\n1. [WHO national life expectancy](https:\/\/www.kaggle.com\/mmattson\/who-national-life-expectancy)\n2. [2016 Global Ecological Footprint](https:\/\/www.kaggle.com\/footprintnetwork\/ecological-footprint)\n3. [The Human Freedom Index](https:\/\/www.kaggle.com\/gsutters\/the-human-freedom-index)\n4. [World Happiness Report](https:\/\/www.kaggle.com\/unsdsn\/world-happiness)  \n  \n## Project methodology\n\n#### Data cleaning   \nI prepared this data set myself so I know it doesn't have NAs and duplicates.   \n   \n#### Data normalization   \nThis analysis focuses on the interpretation of the models, therefore I need to keep track of all variable transformations I perform. For this purpose, I will only use relatively simple transformations like logarithmic, root, or power transformations to normalize data distributions. I will also keep track of transformations by renaming variables accordingly to the performed transformations. I found the article [Data transformation: a focus on the interpretation](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC7714623\/) by Dong Kyu Lee very useful to get more understanding on the topic.\n   \n#### Feature engineering  \nI will use `PolynomialFeatures()` transform to explore various interactions between features.   \nMost of the linear regression models will benefit from additional feature interactions. Though some of the models I am planning to use are based on Decision Trees (`RandomForestRegressor()` and `GradientBoostingRegressor()`) and adding polynomial features will probably have no effect on their performance. But since my main goal is to rank the features I have by their importance for predicting the target, I will use the same set of features for all of my models.\n   \n#### Data scaling  \nI will use `MinMaxScaler()` to scale my variables before fitting a model.   \n   \n#### Feature selection   \nWith this data set, I expect that a lot of features are strongly correlated with each other. Ideally, predicting features in the regression model are supposed to be independent of each other. If they are strongly correlated, **multicollinearity** occurs.\n>Multicollinearity causes the following two basic types of problems:\n>   * The coefficient estimates can swing wildly based on which other independent variables are in the model. The coefficients become very sensitive to small changes in the model.   \n>   * Multicollinearity reduces the precision of the estimated coefficients, which weakens the statistical power of your regression model. You might not be able to trust the p-values to identify independent variables that are statistically significant.   \n>   \n>The potential solutions include the following:\n>   * Remove some of the highly correlated independent variables.\n>   * Linearly combine the independent variables, such as adding them together.\n>   * Perform an analysis designed for highly correlated variables, such as principal components analysis or partial least squares regression.\n>   * LASSO and Ridge regression are advanced forms of regression analysis that can handle multicollinearity.    \n>[Multicollinearity in Regression Analysis: Problems, Detection, and Solutions](https:\/\/statisticsbyjim.com\/regression\/multicollinearity-in-regression-analysis\/)   \n    \nTo analyze multicollinearity in my data set I will:\n* Look at the **correlation coefficients** between various features and between features and target to estimate the strength of the linear relationship between them;\n* Calculate the [**Variance Inflation Factor (VIF)**](https:\/\/www.geeksforgeeks.org\/detecting-multicollinearity-with-vif-python\/) and use it to decide if I want to remove some of the highly correlated predicting features;\n* Build **Ordinary Least Squares (OLS) Regression** model using statsmodels library and look at the summary to get an idea of how well the original features work together to predict the target.  \n   \nI will also try various [**Feature Selection** and **Dimensionality Reduction** methods](https:\/\/machinelearningmastery.com\/feature-selection-with-real-and-categorical-data\/) to reduce the number of input variables to those that are most useful to a model in order to predict the target variable.  \n   \n**Methods to try**:\n* Filter: Select subsets of features based on their relationship with the target (`SelectKBest()` with `mutual_info_regression`)\n* Wrapper: Search for well-performing subsets of features (`RFE()` - [**Recursive Feature Elimination**](https:\/\/machinelearningmastery.com\/rfe-feature-selection-in-python\/))\n* Intrinsic: Algorithms that perform automatic feature selection during training (LASSO and Elastic Net Regressions)   \n* Dimensionality Reduction: `PCA()` - **Principal Component Analysis**  \n   \n#### Models   \n* Linear Regression Models\n    * Linear Regression\n    * Ridge Regression\n    * LASSO Regression\n    * Elastic Net Regression\n* Linear Regression Models with Feature Selection or Dimensionality Reduction methods\n    * Ridge Regression with `SelectKBest()`\n    * Ridge Regression with `RFE()`\n    * Ridge Regression with `PCA()`\n* Other Regression Models with feature importance scores\n    * Random Forest Regressor\n    * Gradient Boosting Regressor   \n       \nFor each model, I will calculate Mean Absolute Error (MAE) for train and test subsets to check for overfitting. I will also calculate R2 score to evaluate the performance of the model.   \n     \n#### Evaluate model stability\nMulticollinearity causes coefficients to become very sensitive to small changes in the model, therefore I needed to come up with a method that would allow me to evaluate the stability of my models and generalize feature importance ranking. I wrote a function that splits data into *n* pairs of train-test subsets and then subsequently fits the model on each of the train subsets to get feature importance values. The function returns a data frame with feature importance values measured for each split, average feature importance value, and standard deviation. To evaluate the stability of the model I will use maximum and average values for normalized standard deviation ( *feature importance standard deviation \/ mean feature importance* ).","8aecf23e":"### Ridge Regression with `PCA()`","8d014828":"Ridge Regression with `PCA()` dimensionality reduction:\n* performs slightly better than baseline Ridge Regression;\n* uses significantly fewer features than baseline Ridge Regression;\n* shows better stability than baseline Ridge Regression.","0ca40f39":"I will calculate skewness scores to decide which variables require normalizing. Positively and negatively skewed distributions usually require different transformations to normalize.     \n* For *positively skewed* variables I am going to try Logarithmic, Cubic Root, and 5th and 7th Root Transformations.  \n* For *negatively skewed* variables I am going to try Power (square, cubic, the 5th and the 7th power) Transformations.    \n   \nI will choose the most suitable transformation for each variable based on its skewness score after the transformation.","570abc51":"# Loading data","bd97b2de":"Gradient Boosting Regressor shows a strongly positively skewed distribution of the average feature importance and has a lot of features with importance values close to zero.","0a96f3dc":"### Ridge Regression","e103001f":"To get my final feature importance ranking I will choose 4 models that more or less agree with each other: \n* 2 linear regression based models:\n    * Ridge Regression\n    * Elastic Net Regression\n* 2 decision tree based models \n    * Random Forest Regression\n    * Gradient Boosting Regression","ec12d2e9":"## Visualize the most important features\n\n","85792c06":"There are 4 features with **extremely high VIF**:\n* 'hf_score'\n* 'ef_score_^3'\n* 'pf_score'\n* 'life_expect_^5'   \n   \nLife expectancy at birth ('life_expect_^5') is one of the top 3 features with the strongest correlation with the target, therefore I will not remove it.   \n   \nGeneral Human Freedom score ('hf_score') is a composite of General Personal ('pf_score') and General Economic ('ef_score_^3') Freedom scores, which are in term composites of multiple components. I can either remove general scores or their components. I would rather keep components of freedom scores since they might give me some insights into which freedoms contribute to happiness score the most.\n","cc668c7c":"## Other Regression Models with feature importance scores\n### Random Forest Regressor","a2f2908d":"# Data preprocessing and EDA","e7edc84b":"The heat map shows a lot of correlation coefficients larger than 0.7 between predicting features and 4 predicting features have Variance Inflation Factor larger than 90.   \nWe should expect problems caused by multicollinearity."}}