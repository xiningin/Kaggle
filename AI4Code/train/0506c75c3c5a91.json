{"cell_type":{"b2cc363f":"code","99c517cc":"code","524e2169":"code","592f6548":"code","a6d01289":"code","5b0bbe89":"code","6b66b8bd":"code","1d6f164f":"code","bad034c2":"code","572d48da":"code","4c8cb7f7":"code","d47199c6":"code","08cae034":"code","7cd93a10":"code","3c7b0f84":"code","16b14270":"code","7baa8f7d":"code","7825b1ba":"code","8ef9fb58":"code","6d52985e":"code","44ec7947":"code","dc38fae8":"code","e561c014":"code","9c9329f3":"code","99cfd39c":"code","cd89cbac":"code","66d4179b":"code","349843c0":"code","d60e59ec":"code","7daaee6c":"code","35631274":"code","3c738cbf":"code","fb693e11":"code","c1721bf1":"code","2c41c59b":"code","2a5f17e0":"code","b6ad63f2":"code","92b29d93":"code","7cab8122":"code","d7f86959":"code","843a9451":"code","f915fa06":"code","742580f5":"code","736f308c":"code","729be376":"code","4fc5d6de":"code","b32e4b6e":"code","5e46e3af":"code","8dff1c22":"code","f22a362a":"code","4111e07d":"code","962b5802":"code","5416e792":"code","2788fafc":"code","769b50c5":"code","ed8aa410":"code","f8eac6b0":"code","f2162072":"code","97babb9a":"code","ffb7511e":"code","ab9216df":"code","4fd14a9c":"code","bfc17314":"code","7364b5a5":"code","afad0cd9":"code","f5066cac":"code","46208735":"code","d288599f":"code","dd5eb36b":"code","381bbf6e":"code","ef67e193":"code","03dbe5a3":"code","9e0254cc":"code","18cbd976":"markdown","12bb0df6":"markdown","3218fc0e":"markdown","29c088d9":"markdown","69696d5f":"markdown","c167a75c":"markdown","7f5eecff":"markdown","48d9e88d":"markdown","5d9e6879":"markdown","bd9fa6f5":"markdown","c3ff01b0":"markdown","684d8055":"markdown","8342dd8b":"markdown","caa5f64a":"markdown","41d31e48":"markdown","999639f3":"markdown","835f902f":"markdown","fc8f4868":"markdown","d649ed8e":"markdown","2633cdb7":"markdown","54847da8":"markdown","528074d9":"markdown","dc2d3424":"markdown","324b1118":"markdown"},"source":{"b2cc363f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","99c517cc":"#Import Library\n\n#Graphic Components\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n#Statistical Inference Analyis\nimport statsmodels.api as sm\nimport scipy.stats as stats\n\n#Data Precessing \nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.impute import SimpleImputer\n\n#Model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\n\n#Feature Selection\/ Model Optimization\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import SequentialFeatureSelector\n\n#Validation \nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score","524e2169":"#Import Data\nTitanic_train = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\nTitanic_train.head()","592f6548":"Titanic_predict = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')\nTitanic_predict.head()","a6d01289":"Titanic_train.describe()","5b0bbe89":"Titanic_train.describe(include = ['O'])","6b66b8bd":"print('Train Set:',Titanic_train.shape)\nprint('Test Set:',Titanic_predict.shape)","1d6f164f":"#Check Data Type\nTitanic_train.dtypes","bad034c2":"#Check # of Unique\nTitanic_train.nunique()","572d48da":"#Check NA of the Columns:\nfor i in Titanic_train.columns:\n    print(f\"{i}:\", Titanic_train[i].isna().sum())","4c8cb7f7":"#Precleansing\n\n#Remove Passenger ID, Ticket Number, Name as they are meaningless index \nTitanic_train = Titanic_train.drop(columns = ['PassengerId','Ticket','Name'])\n\n#Also Drop Cabin as 687\/891 are NAN \nTitanic_train = Titanic_train.drop(columns = ['Cabin'])","d47199c6":"Titanic_train.select_dtypes(include = ['float64', 'int64'])","08cae034":"Correlation_df = Titanic_train.copy()\n\n#Split to Numeric and Categorical Data for Normalization\nnum_col = Titanic_train.select_dtypes(include = ['float64', 'int64']).columns\ncategory_col = Titanic_train.select_dtypes(include = ['object']).columns\n    \nCorrelation_df_category = pd.DataFrame(OneHotEncoder(drop = 'first').fit(Correlation_df[category_col]).transform(Correlation_df[category_col]).toarray(), columns = OneHotEncoder(drop='first').fit(Correlation_df[category_col]).get_feature_names(Correlation_df[category_col].columns))\nCorrelation_df_category = Correlation_df_category[Correlation_df_category['Embarked_nan'] == 0.0].drop(columns = ['Embarked_nan'])\n    \n#Normalization\nCorrelation_df_num = pd.DataFrame(StandardScaler().fit(Correlation_df[num_col]).transform(Correlation_df[num_col]), columns = Correlation_df[num_col].columns)\n\n#Fill NA\nCorrelation_df = pd.concat([Correlation_df_num, Correlation_df_category], axis = 1)\nCorrelation_df = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(Correlation_df), columns = Correlation_df.columns)\n\nCorrelation_df","7cd93a10":"Correlation_df.corr(method='kendall', min_periods=1)","3c7b0f84":"# Kendall's \u03c4 coefficient\nsns.heatmap(Correlation_df.corr(method='kendall', min_periods=1))","16b14270":"Biserial_df = pd.DataFrame()\nfor i in Correlation_df.drop(columns = ['Survived']).columns:\n    Biserial_df = Biserial_df.append(pd.DataFrame({'Variable': i,\n                                                   'Correlation': stats.pointbiserialr(Correlation_df[i], Correlation_df['Survived']).correlation,\n                                                   'P-Value': round(stats.pointbiserialr(Correlation_df[i], Correlation_df['Survived']).pvalue, 3)\n                                                  }, index = [0]))\n    \n    \nBiserial_df.index = Biserial_df['Variable']\nBiserial_df","7baa8f7d":"sns.heatmap(Biserial_df[['Correlation']])","7825b1ba":"#Define X and Y\nTitanic_train_x = Titanic_train.drop(columns = ['Survived'])\nTitanic_train_y = Titanic_train['Survived']\n\n#Split to Numeric and Categorical Data for Normalization\nnum_col = Titanic_train_x.select_dtypes(include = ['float64', 'int64']).columns\ncategory_col = Titanic_train_x.select_dtypes(include = ['object']).columns\n\n#Normalization\nTitanic_train_num = pd.DataFrame(StandardScaler().fit(Titanic_train_x[num_col]).transform(Titanic_train_x[num_col]), columns = Titanic_train_x[num_col].columns)\n\nTitanic_train_category = pd.DataFrame(OneHotEncoder(drop = 'first').fit(Titanic_train_x[category_col]).transform(Titanic_train_x[category_col]).toarray(), columns = OneHotEncoder(drop='first').fit(Titanic_train_x[category_col]).get_feature_names(Titanic_train_x[category_col].columns))\nTitanic_train_category = Titanic_train_category[Titanic_train_category['Embarked_nan'] == 0.0].drop(columns = ['Embarked_nan'])\n\n#Concat the File\nTitanic_train_x = pd.concat([Titanic_train_num, Titanic_train_category], axis = 1)\n\n#Fill NA Data\nTitanic_train_x = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(Titanic_train_x), columns = Titanic_train_x.columns)\n\n#Train Test Split\nTitanic_train_x, Titanic_test_x, Titanic_train_y, Titanic_test_y = train_test_split(Titanic_train_x, Titanic_train_y, test_size=0.3, random_state=42)","8ef9fb58":"log_reg = sm.Logit(Titanic_train_y, Titanic_train_x).fit()","6d52985e":"print(log_reg.summary())","44ec7947":"#Select the Useful Feature\nSFS = SequentialFeatureSelector(LogisticRegression(random_state=0),\n                                direction = 'backward',\n                                scoring = 'roc_auc',\n                                cv = 5,\n                                n_features_to_select=3).fit(Titanic_train_x,Titanic_train_y)","dc38fae8":"# See Who Is Chosen\nSFS_Results = pd.DataFrame({'Variable': Titanic_train_x.columns,\n                            'Chosen': SFS.get_support()})\nSFS_Results","e561c014":"SFS_Variable = SFS_Results[SFS_Results['Chosen'] == True]['Variable']\nlog_reg = sm.Logit(Titanic_train_y, Titanic_train_x[SFS_Variable]).fit()","9c9329f3":"print(log_reg.summary())","99cfd39c":"# Build a DataFrame To Score the Performance\n\nPerformance_df = pd.DataFrame(columns = ['Model', 'Feature Selection', 'Accuracy', 'Log Loss', 'ROC'])\nPerformance_df","cd89cbac":"#Find the Alpha through Pruning\nalphas = DecisionTreeClassifier(random_state=0).cost_complexity_pruning_path(Titanic_train_x, Titanic_train_y)['ccp_alphas']\n                    \n#Pools of Parameters\nrandom_parameters = {'n_estimators': [10,100,1000],\n                     'criterion':['gini','entropy'],\n                     'max_depth': [10,100,1000],\n                     'max_features':[\"auto\",\"sqrt\", \"log2\"],\n                     'bootstrap' :[True,False],\n                     'class_weight': [\"balanced\", \"balanced_subsample\"], \n                     'ccp_alpha': alphas\n                    }\n\n#Randomized Cross Validation for Hyperparameters Tuning\nRFC = RandomizedSearchCV(RandomForestClassifier(), \n                         param_distributions = random_parameters,\n                         n_iter = 100,\n                         scoring = 'accuracy',\n                         n_jobs = 10,\n                         cv = 3,\n                         verbose = 2,\n                         random_state=0,\n                         return_train_score = True)\nRFC.fit(Titanic_train_x, Titanic_train_y)","66d4179b":"Best_Parameter = RFC.best_params_\n\n#Test the Preforamnce of Best Parameters \n\nRFC = RandomForestClassifier(n_estimators = Best_Parameter['n_estimators'],\n                             criterion = Best_Parameter['criterion'],\n                             max_depth = Best_Parameter['max_depth'],\n                             max_features = Best_Parameter['max_features'],\n                             bootstrap = Best_Parameter['bootstrap'],\n                             class_weight = Best_Parameter['class_weight'],\n                             ccp_alpha = Best_Parameter['ccp_alpha']\n                            )\nRFC.fit(Titanic_train_x, Titanic_train_y)","349843c0":"#Validation\npred = RFC.predict(Titanic_test_x)\nPerformance_df = Performance_df.append(pd.DataFrame([['RFC', 'Full', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","d60e59ec":"#Plot ROC Curve\nplot_roc_curve(RFC, Titanic_test_x, Titanic_test_y)","7daaee6c":"#Find the Alpha\nalphas = DecisionTreeClassifier(random_state=0).cost_complexity_pruning_path(Titanic_train_x[SFS_Variable], Titanic_train_y)['ccp_alphas']\n\n#Pools of Parameters        \nrandom_parameters = {'n_estimators': [10,100,1000],\n                     'criterion':['gini','entropy'],\n                     'max_depth': [10,100,1000],\n                     'max_features':[\"auto\",\"sqrt\", \"log2\"],\n                     'bootstrap' :[True,False],\n                     'class_weight': [\"balanced\", \"balanced_subsample\"], \n                     'ccp_alpha': alphas\n                    }\n  \n#Randomized Cross Validation for Hyperparameters Tuning\nRFC = RandomizedSearchCV(RandomForestClassifier(), \n                         param_distributions = random_parameters,\n                         n_iter = 100,\n                         scoring = 'accuracy',\n                         n_jobs = 10,\n                         cv = 3,\n                         verbose = 2,\n                         random_state=0,\n                         return_train_score = True)\nRFC.fit(Titanic_train_x[SFS_Variable], Titanic_train_y)","35631274":"Best_Parameter = RFC.best_params_\n\n#Test the Preforamnce of Best Parameters \nRFC = RandomForestClassifier(n_estimators = Best_Parameter['n_estimators'],\n                             criterion = Best_Parameter['criterion'],\n                             max_depth = Best_Parameter['max_depth'],\n                             max_features = Best_Parameter['max_features'],\n                             bootstrap = Best_Parameter['bootstrap'],\n                             class_weight = Best_Parameter['class_weight'],\n                             ccp_alpha = Best_Parameter['ccp_alpha']\n                            )\nRFC.fit(Titanic_train_x[SFS_Variable], Titanic_train_y)","3c738cbf":"#Validation\npred = RFC.predict(Titanic_test_x[SFS_Variable])\nPerformance_df = Performance_df.append(pd.DataFrame([['RFC', 'Selected', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","fb693e11":"#Plot ROC Curve\nplot_roc_curve(RFC, Titanic_test_x[SFS_Variable], Titanic_test_y)","c1721bf1":"#Pools of Parameters\nrandom_parameters = {'C': stats.expon(scale=100), \n                     'gamma': stats.expon(scale=.1),\n                     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n                     'class_weight':['balanced', None]\n                     }\n   \n#Randomized Cross Validation for Hyperparameters Tuning\nSupport_Vector = RandomizedSearchCV(SVC(), \n                                    param_distributions = random_parameters,\n                                    n_iter = 100,\n                                    scoring = 'accuracy',\n                                    n_jobs = 10,\n                                    cv = 3,\n                                    verbose = 2,\n                                    random_state=0,\n                                    return_train_score = True)\nSupport_Vector.fit(Titanic_train_x, Titanic_train_y)","2c41c59b":"Best_Parameter = Support_Vector.best_params_\n\n#Test the Preforamnce of Best Parameters \nSupport_Vector = SVC(C = Best_Parameter['C'],\n                     gamma = Best_Parameter['gamma'],\n                     kernel = Best_Parameter['kernel'],\n                     class_weight = Best_Parameter['class_weight']\n                    )\nSupport_Vector.fit(Titanic_train_x, Titanic_train_y)","2a5f17e0":"#Validation\npred = Support_Vector.predict(Titanic_test_x)\nPerformance_df = Performance_df.append(pd.DataFrame([['SVC', 'Full', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","b6ad63f2":"#Plot ROC Curve\nplot_roc_curve(Support_Vector, Titanic_test_x, Titanic_test_y)","92b29d93":"#Pools of Parameters   \n\nrandom_parameters = {'C': stats.expon(scale=100), \n                     'gamma': stats.expon(scale=.1),\n                     'kernel': ['linear', 'poly', 'rbf', 'sigmoid'], \n                     'class_weight':['balanced', None]\n                     }\n\n#Randomized Cross Validation for Hyperparameters Tuning\n\nSupport_Vector = RandomizedSearchCV(SVC(), \n                                    param_distributions = random_parameters,\n                                    n_iter = 100,\n                                    scoring = 'accuracy',\n                                    n_jobs = 10,\n                                    cv = 3,\n                                    verbose = 2,\n                                    random_state=0,\n                                    return_train_score = True)\nSupport_Vector.fit(Titanic_train_x[SFS_Variable], Titanic_train_y)","7cab8122":"Best_Parameter = Support_Vector.best_params_\n\n#Test the Preforamnce of Best Parameters\nSupport_Vector = SVC(C = Best_Parameter['C'],\n                     gamma = Best_Parameter['gamma'],\n                     kernel = Best_Parameter['kernel'],\n                     class_weight = Best_Parameter['class_weight']\n                    )\nSupport_Vector.fit(Titanic_train_x[SFS_Variable], Titanic_train_y)","d7f86959":"#Validation\npred = Support_Vector.predict(Titanic_test_x[SFS_Variable])\nPerformance_df = Performance_df.append(pd.DataFrame([['SVC', 'Selected', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","843a9451":"#Plot ROC Curve\nplot_roc_curve(Support_Vector, Titanic_test_x[SFS_Variable], Titanic_test_y)","f915fa06":"import xgboost as xgb \n\nXGB_Train_df = xgb.DMatrix(Titanic_train_x, label = Titanic_train_y)\nXGB_Test_df = xgb.DMatrix(Titanic_test_x, label = Titanic_test_y)\n\nparameters = {'max_depth':6,\n          'min_child_weight': 1,\n          'eta':0.3,\n          'subsample': 0.7,\n          'colsample_bytree': 1,\n          'objective':'binary:hinge',\n}\n\nBest_parameters = {'max_depth':6,\n                  'min_child_weight': 1,\n                  'eta':0.3,\n                  'subsample': 0.7,\n                  'colsample_bytree': 1,\n                  'objective':'binary:hinge',\n}","742580f5":"#Cross Validation for Depth and \n\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]\n\n# Define initial best params and AOC\nmax_auc = 0\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    \n    # Update our parameters\n    parameters['max_depth'] = max_depth\n    parameters['min_child_weight'] = min_child_weight\n\n    cv_results = xgb.cv(parameters,\n                        XGB_Train_df,\n                        num_boost_round=999,\n                        seed=42,\n                        nfold=5,\n                        metrics={'auc'},\n                        early_stopping_rounds=10\n                        )\n    \n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tROC AUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = (max_depth, min_child_weight)\n\nBest_parameters['max_depth'],Best_parameters['min_child_weight'] = best_params\nprint(\"Best params: {}, {}, ROC AUC: {}\".format(best_params[0], best_params[1], max_auc))","736f308c":"#Cross Validation for Learning Rate\n\nmax_auc = 0\nbest_params = None\n\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    parameters['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(parameters,\n                        XGB_Train_df,\n                        num_boost_round=999,\n                        seed=42,\n                        nfold=5,\n                        metrics=['auc'],\n                        early_stopping_rounds=10\n                       )\n    # Update best score\n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tROC AUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = eta\n\nBest_parameters['eta'] = best_params        \nprint(\"Best params: {}, ROC AUC: {}\".format(best_params, max_auc))\n","729be376":"Best_parameters","4fc5d6de":"#Test Test Model\nbest_model = xgb.train(Best_parameters,\n                       XGB_Train_df,\n                       num_boost_round=999,\n                       evals=[(XGB_Test_df, \"Test\")],\n                       early_stopping_rounds = 10)","b32e4b6e":"#Validation\npred = best_model.predict(XGB_Test_df).astype(int)\nPerformance_df = Performance_df.append(pd.DataFrame([['XGBC', 'Full', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","5e46e3af":"import xgboost as xgb \n\nXGB_Train_df = xgb.DMatrix(Titanic_train_x[SFS_Variable], label = Titanic_train_y)\nXGB_Test_df = xgb.DMatrix(Titanic_test_x[SFS_Variable], label = Titanic_test_y)\n\nparameters = {'max_depth':6,\n          'min_child_weight': 1,\n          'eta':0.3,\n          'subsample': 0.7,\n          'colsample_bytree': 1,\n          'objective':'binary:hinge',\n}\n\nBest_parameters = {'max_depth':6,\n                  'min_child_weight': 1,\n                  'eta':0.3,\n                  'subsample': 0.7,\n                  'colsample_bytree': 1,\n                  'objective':'binary:hinge',\n}","8dff1c22":"#Cross Validation for Depth and \n\ngridsearch_params = [\n    (max_depth, min_child_weight)\n    for max_depth in range(9,12)\n    for min_child_weight in range(5,8)\n]\n\n# Define initial best params and AOC\nmax_auc = 0\nbest_params = None\nfor max_depth, min_child_weight in gridsearch_params:\n    print(\"CV with max_depth={}, min_child_weight={}\".format(\n                             max_depth,\n                             min_child_weight))\n    \n    # Update our parameters\n    parameters['max_depth'] = max_depth\n    parameters['min_child_weight'] = min_child_weight\n\n    cv_results = xgb.cv(parameters,\n                        XGB_Train_df,\n                        num_boost_round=999,\n                        seed=42,\n                        nfold=5,\n                        metrics={'auc'},\n                        early_stopping_rounds=10\n                        )\n    \n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tROC AUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = (max_depth, min_child_weight)\n\nBest_parameters['max_depth'],Best_parameters['min_child_weight'] = best_params\nprint(\"Best params: {}, {}, ROC AUC: {}\".format(best_params[0], best_params[1], max_auc))","f22a362a":"#Cross Validation for Learning Rate\n\nmax_auc = 0\nbest_params = None\n\nfor eta in [.3, .2, .1, .05, .01, .005]:\n    print(\"CV with eta={}\".format(eta))\n    # We update our parameters\n    parameters['eta'] = eta\n    # Run and time CV\n    cv_results = xgb.cv(parameters,\n                        XGB_Train_df,\n                        num_boost_round=999,\n                        seed=42,\n                        nfold=5,\n                        metrics=['auc'],\n                        early_stopping_rounds=10\n                       )\n    # Update best score\n    mean_auc = cv_results['test-auc-mean'].max()\n    boost_rounds = cv_results['test-auc-mean'].argmax()\n    print(\"\\tROC AUC {} for {} rounds\".format(mean_auc, boost_rounds))\n    if mean_auc > max_auc:\n        max_auc = mean_auc\n        best_params = eta\n\nBest_parameters['eta'] = best_params        \nprint(\"Best params: {}, ROC AUC: {}\".format(best_params, max_auc))\n","4111e07d":"Best_parameters","962b5802":"#Test Test Model\nbest_model = xgb.train(Best_parameters,\n                       XGB_Train_df,\n                       num_boost_round=999,\n                       evals=[(XGB_Test_df, \"Test\")],\n                       early_stopping_rounds = 10)","5416e792":"#Validation\npred = best_model.predict(XGB_Test_df).astype(int)\nPerformance_df = Performance_df.append(pd.DataFrame([['XGBC', 'Selected', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","2788fafc":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import EarlyStopping","769b50c5":"#Defining Early Stopping \nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n#Defining Neural Model\nDL_Model =  keras.Sequential([\n    layers.Dense(256, input_shape = [8]),\n    layers.Dense(128, activation = 'relu'),\n    layers.Dense(1, activation='sigmoid'),\n])\n\n#Compile Model Fit\nDL_Model.compile(\n    optimizer='adam',\n    loss='BinaryCrossentropy',\n    metrics = 'binary_accuracy'\n)","ed8aa410":"#Record The Epoch \nHistory = DL_Model.fit(\n    Titanic_train_x, Titanic_train_y,\n    validation_data=(Titanic_test_x, Titanic_test_y,),\n    callbacks=[early_stopping],\n    batch_size=100,\n    epochs=10000,\n    verbose=0,\n)","f8eac6b0":"# Convert the Training History to a Dataframe\nhistory_df = pd.DataFrame(History.history)\n#Pandas native plot method\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","f2162072":"#Validation\npred = DL_Model.predict(Titanic_test_x).round(0).astype(int)\nPerformance_df = Performance_df.append(pd.DataFrame([['Tensorflow', 'Full', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","97babb9a":"#Defining Early Stopping \nearly_stopping = EarlyStopping(\n    min_delta=0.001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n)\n\n#Defining Neural Model\nDL_Model =  keras.Sequential([\n    layers.Dense(256, input_shape = [3]),\n    layers.Dense(1, activation='sigmoid'),\n])\n\n#Compile Model Fit\nDL_Model.compile(\n    optimizer='adam',\n    loss='BinaryCrossentropy',\n    metrics = 'binary_accuracy'\n)","ffb7511e":"#Record The Epoch \nHistory = DL_Model.fit(\n    Titanic_train_x[SFS_Variable], Titanic_train_y,\n    validation_data=(Titanic_test_x[SFS_Variable], Titanic_test_y),\n    callbacks=[early_stopping],\n    batch_size=100,\n    epochs=10000,\n    verbose=0,\n)","ab9216df":"# Convert the Training History to a Dataframe\nhistory_df = pd.DataFrame(History.history)\n#Pandas native plot method\nhistory_df.loc[:, ['loss', 'val_loss']].plot();\nhistory_df.loc[:, ['binary_accuracy', 'val_binary_accuracy']].plot()","4fd14a9c":"#Validation\npred = DL_Model.predict(Titanic_test_x[SFS_Variable]).round(0).astype(int)\nPerformance_df = Performance_df.append(pd.DataFrame([['Tensorflow', 'Selected', accuracy_score(Titanic_test_y, pred), log_loss(Titanic_test_y, pred), roc_auc_score(Titanic_test_y, pred)]],\n                                                    columns = ['Model', 'Feature Selection','Accuracy', 'Log Loss', 'ROC']), sort = False)\n\nprint('Accuracy:', accuracy_score(Titanic_test_y, pred))\nprint('Log Loss:', log_loss(Titanic_test_y, pred))\nprint('ROC Accuracy:', roc_auc_score(Titanic_test_y, pred))\nprint('Confusion Matrix:\\n', \n       confusion_matrix(Titanic_test_y, pred))","bfc17314":"Performance_df['Hue'] = Performance_df['Model'] + ' ' + Performance_df['Feature Selection'] \nPerformance_df = Performance_df.reset_index(drop = True)\nPerformance_df","7364b5a5":"sns.set_theme()\nsns.scatterplot(data = Performance_df, x = 'Log Loss',y = 'Accuracy', hue = 'Hue', legend = 'brief')","afad0cd9":"Grid = sns.FacetGrid(data = Performance_df.reset_index(drop = True), col = 'Model', hue = 'Feature Selection')\nGrid.map(sns.scatterplot, 'Log Loss','Accuracy', legend = True)\nGrid.add_legend()","f5066cac":"Titanic_predict","46208735":"Titanic_predict.describe()","d288599f":"Titanic_predict.describe(include = ['O'])","dd5eb36b":"#Check # of Unique\nTitanic_predict.nunique()","381bbf6e":"#Check NA of the Columns:\nfor i in Titanic_predict.columns:\n    print(f\"{i}:\", Titanic_predict[i].isna().sum())","ef67e193":"#Cleansing\n\n#Remove Passenger ID, Ticket Number, Name as they are meaningless index \nTitanic_predict_x = Titanic_predict.drop(columns = ['PassengerId','Ticket','Name'])\n\n#Also Drop Cabin as 327\/418 are NAN \nTitanic_predict_x = Titanic_predict_x.drop(columns = ['Cabin'])\n\n#Split to Numeric and Categorical Data for Normalization\nnum_col = Titanic_predict_x.select_dtypes(include = ['float64', 'int64']).columns\ncategory_col = Titanic_predict_x.select_dtypes(include = ['object']).columns\n\n#Normalization\nTitanic_predict_num = pd.DataFrame(StandardScaler().fit(Titanic_predict_x[num_col]).transform(Titanic_predict_x[num_col]), columns = Titanic_predict_x[num_col].columns)\n\nTitanic_predict_category = pd.DataFrame(OneHotEncoder(drop = 'first').fit(Titanic_predict_x[category_col]).transform(Titanic_predict_x[category_col]).toarray(), columns = OneHotEncoder(drop='first').fit(Titanic_predict_x[category_col]).get_feature_names(Titanic_predict_x[category_col].columns))\n\n#Concat the File\nTitanic_predict_x = pd.concat([Titanic_predict_num, Titanic_predict_category], axis = 1)\n\n#Fill NA Data\nTitanic_predict_x = pd.DataFrame(SimpleImputer(missing_values=np.nan, strategy='most_frequent').fit_transform(Titanic_predict_x), columns = Titanic_predict_x.columns)\n\nTitanic_predict_x","03dbe5a3":"#Use Tensorflow (Full) with Highest Accuracy to Predict the Outcome \n\n#Defining Neural Model\nDL_Model =  keras.Sequential([\n    layers.Dense(256, input_shape = [8]),\n    layers.Dense(1, activation='sigmoid'),\n])\n\n#Compile Model Fit\nDL_Model.compile(\n    optimizer='adam',\n    loss='BinaryCrossentropy',\n    metrics = 'binary_accuracy'\n)\n\nDL_Model.fit(\n    Titanic_train_x, Titanic_train_y,\n    validation_data=(Titanic_test_x, Titanic_test_y),\n    callbacks=[early_stopping],\n    batch_size=100,\n    epochs=10000,\n    verbose=0,\n)\n\nSubmision_df = pd.DataFrame()\nSubmision_df['PassengerId'] = Titanic_predict['PassengerId']\nSubmision_df['Survived'] = DL_Model.predict(Titanic_predict_x).round(0).astype(int)\nSubmision_df","9e0254cc":"Submision_df.to_csv(r'prediction_submission.csv')","18cbd976":"## Random Forest for All Variables","12bb0df6":"## Point-biserial correlation coefficient","3218fc0e":"## XGBosst (Selected)","29c088d9":"## SVC Significant Variables","69696d5f":"# Titanic Surivials - Disasters Analysis\n\n### Research Questions:\n\n- What variables are significantly attributed to the survivals ratio?\n\n- What variables are insignificant and noises for our prediction? \n\n- Ockham's Razor - How to use the least variables to make powerful predictions for applications?\n\n### Planned Workflows\n\n1. Data Exploration and Cleansing - Processing Categorial Data and Normalization\n\n2. Feature Selection through Statistical Inference - Correlation Analysis and Logistic Regression\n\n3. Model Selection - Random Forest Classification, Support Vector Machine, Extreme Gradient Boost (XGBoost), Tensorflow Neural Network\n\n\nPotential Areas of Applications - Risk Management, Actuarial Studies, Insurance, Safety, Design, Crisis Management\n","c167a75c":"# Make Predictions","7f5eecff":"# Conclusion\n\nFrom the plot between log loss and accuracy, we found that deep learning shows robust predictive accuracy and low errors with the testing sample, outperforming the other models. Therefore, we purpose to use the deep-learning model as the classification methodology for our submission.\n\n## Notes:\nThe prediction submission is with 75% accuracy, which is not bad at all, but still, need further reflection and improvements.\n\n## Reflection:\nOther than a cross model validation, we also conducted a cross features analysis to check whether few variables can have a similar performance. \n\nIt is found that in some machine learning models like SVC and XGBoost, selected features are not only comparable to the Full version but even overweighted. Even dropping half of the variables, the model can use fewer efforts and data to make a better prediction.\n\nHowever, TensorFlow and random forest classifier did not show a similar pattern. I guess that it is because the original data is too small (600) and larger-N input will increase the performance significantly under the repeating stochastic gradience boosting and neural networks, but still, my hypothesis requires further examination through a larger learning dataset.\n","48d9e88d":"## Deep Learning (Full Data)","5d9e6879":"### Notes:\nParch and Fare both show high p-values, under which we cannot reject the null hypothesis that it is insignificant to the dependent variables, which should be removed at first.\n\nTo simplify the feature selection process, we employed the Sequential Feature Selection function to examine and cross-validate the performance of different combinations of variables with the metrics of ROC accuracy, which measures the roc distribution of True\/False matrix between actual and prediction. ","bd9fa6f5":"## Cleansing Data For Prediction","c3ff01b0":"## Kendall's \u03c4 coefficient","684d8055":"## Random Forest for Significant Variables","8342dd8b":"# Statistical Inference & Feature Selection","caa5f64a":"# Data Cleansing for Model","41d31e48":"# Review","999639f3":"### Notes:\n\nThrough Logistic Regression and Sequential Feature Selection, we could further consolidate our findings and hypotheses from the previous Correlation Analysis. \n\nThe outcome reveals that Pclass, Age, and Sex_male are the significant variables for prediction and the remaining will be conceived as insignificant noise and removed accordingly.","835f902f":"# Exploratory Analysis","fc8f4868":"## Support Vector Machine","d649ed8e":"## XGBoost (Full)","2633cdb7":"### Insights so far\n\n1. From the exploration through Correlation Analysis, we have found some collinear variables, namely Sibsp, Parch, and Fare are highly correlated with 0.5 coefficient to each other, perhaps implying that these variables are telling us the same things, therefore low entropy.\n\n2. Another noteworthy set of variables are pclass and fare with -0.5 coefficient and it is logical to anticipate that class and fare the self-implying. \n\nWe will keep the findings in mind and further examine whether to remove them or not.","54847da8":"# Importing Data","528074d9":"# Model Selection","dc2d3424":"## Deep Leaning (Selected)","324b1118":"### Findings from the Exploration:\n\n1. Male is very unlikely to live \n\n2. The higher the class, the lower the chance of survival \n\n3. Children will be more like to be saved \n"}}