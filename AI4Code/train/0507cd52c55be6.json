{"cell_type":{"13b28957":"code","2f1f998d":"code","7b3efc90":"code","71ba662e":"code","b388a3f2":"code","23c2ac6e":"code","a882584c":"code","819ee8f1":"code","b82e0ecc":"markdown","f4da413b":"markdown","ab9b8776":"markdown","1c595a07":"markdown","b0a0d637":"markdown","d6bb4d67":"markdown","d9438b9c":"markdown","4f2e4bf4":"markdown"},"source":{"13b28957":"import pandas as pd, os\nimport cudf, cuml, cupy\nfrom tqdm import tqdm\nimport numpy as np\nprint('RAPIDS',cudf.__version__)","2f1f998d":"# https:\/\/www.kaggle.com\/raghavendrakotala\/fine-tunned-on-roberta-base-as-ner-problem-0-533\ntrain_names, train_texts = [], []\nfor f in tqdm(list(os.listdir('..\/input\/feedback-prize-2021\/train'))):\n    train_names.append(f.replace('.txt', ''))\n    train_texts.append(open('..\/input\/feedback-prize-2021\/train\/' + f, 'r').read())\ntrain_text_df = cudf.DataFrame({'id': train_names, 'text': train_texts})\ntrain_text_df.head()","7b3efc90":"train_text_df.tail()","71ba662e":"from cuml.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(stop_words='english', binary=True, max_features=25_000)\ntext_embeddings = tfidf.fit_transform( train_text_df.text ).toarray()","b388a3f2":"from cuml import UMAP\numap = UMAP()\nembed_2d = umap.fit_transform(text_embeddings)\nembed_2d = cupy.asnumpy( embed_2d )","23c2ac6e":"from cuml import KMeans\nkmeans = cuml.KMeans(n_clusters=15)\nkmeans.fit(embed_2d)\ntrain_text_df['cluster'] = kmeans.labels_","a882584c":"import matplotlib.pyplot as plt\n\ncenters = kmeans.cluster_centers_\nprint(kmeans.labels_)\nplt.figure(figsize=(10,10))\nplt.scatter(embed_2d[:,0], embed_2d[:,1], s=1, c=kmeans.labels_)\nplt.title('UMAP Plot of Train Text using Tfidf features\\nRAPIDS Discovers the 15 essay topics!',size=16)\n\nfor k in range(len(centers)):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.argmax(mm)\n    top_word = tfidf.vocabulary_.iloc[ii]\n    plt.text(centers[k,0]-1,centers[k,1]+0.75,f'{k+1}-{top_word}',size=16)\n\nplt.show()","819ee8f1":"for k in range(5):\n    mm = cupy.mean( text_embeddings[train_text_df.cluster.values==k],axis=0 )\n    ii = cupy.asnumpy( cupy.argsort(mm)[-5:][::-1] )\n    top_words = tfidf.vocabulary_.to_array()[ii]\n    print('#'*25)\n    print(f'### Essay Topic {k+1}')\n    print('### Top 5 Words',top_words)\n    print('#'*25)\n    tmp = train_text_df.loc[train_text_df.cluster==k].sample(3, random_state=123)\n    for j in range(3):\n        txt = tmp.iloc[j,1]\n        print('-'*10,f'Example {j+1}','-'*10)\n        print(txt,'\\n')","b82e0ecc":"# RAPIDS UMAP, Tfidf, and KMeans Discovers 15 Essay Topics\nIn this notebook we will find the essay topics using RAPIDS cudf, UMAP, Tfidf, and KMeans. First we will convert each text into a Tfidf embedding. Then we will use UMAP to reduce these embeddings to two dimensions. Lastly we will use KMeans to find the essay topics!","f4da413b":"# Display Essay Topics\nWe will display the result of UMAP which reduced text to two dimension. We observe that the essays cluster into 15 groups. These are the 15 essay topics! Additionally we will plot the most important word from each group.","ab9b8776":"# Display Example Text\nWe will display three example text from each essay topic. And we will display the five most important words from each topic.","1c595a07":"# RAPIDS KMeans\nWe will use KMeans to find clusters of essays. These are the essay topics!","b0a0d637":"# Load RAPIDS","d6bb4d67":"# RAPIDS Tfidf\nWe will use Tfidf to convert each text into a embedding vector of length 25,000.","d9438b9c":"# RAPIDS UMAP\nWe will use UMAP to reduce embedding vectors to two dimensions","4f2e4bf4":"# RAPIDS cudf\nWe will read train text into a RAPIDS cudf."}}