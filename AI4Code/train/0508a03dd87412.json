{"cell_type":{"cc346952":"code","f6cbae28":"code","1ce97800":"code","60c27f83":"code","ca1e20d1":"code","c75d7a27":"code","969f3ab4":"code","92699505":"code","d2f6c6d8":"code","38acb613":"code","fbcbff3f":"code","affcdf90":"code","7fdcc94a":"code","36f9931a":"code","df0b1e7d":"code","63c21741":"code","71d5077b":"code","1a43cd28":"code","aba518ff":"code","03979703":"code","e33425cf":"code","78e8f5be":"code","6a4485f7":"code","5d656e7d":"code","2ef7724f":"code","38178ef2":"code","fd3fa710":"code","a5301527":"code","2f8f0e4c":"code","d3c44df5":"code","48ea0933":"code","6aea140a":"code","75dcd82a":"code","c28b9439":"code","36b56bff":"code","cbed570b":"code","149b39ab":"code","55b09566":"code","c0ef7aa4":"code","896ef161":"code","b7e1fdc0":"code","37472e0a":"code","0a26826d":"code","3397860d":"code","ff50da46":"code","90b0225e":"code","77d5ea64":"code","05098e04":"code","f363fe74":"code","aedff840":"code","f47572c9":"code","75a53e82":"code","ceb4b05a":"code","0f3e29ce":"code","6804c787":"code","468f2894":"code","fc97f828":"code","f560239b":"code","e031fc72":"code","8bb29ceb":"code","076761f5":"code","d1ffaeb5":"code","78cc26ba":"code","ae21fe0e":"code","60383448":"code","467ce9a3":"code","bdd2e8d6":"code","1a9da8c4":"code","63fc5d65":"code","4f54d57b":"code","6e39e5e4":"code","caea371a":"code","477ec113":"markdown","8f4b2167":"markdown","c4432981":"markdown","a07e8bbd":"markdown","8269bb48":"markdown","139d3000":"markdown","b02a08a3":"markdown","11329995":"markdown","f382fae4":"markdown","1cde6ef1":"markdown","34dab628":"markdown","b06b8e56":"markdown","18649b84":"markdown","ca3d2bbb":"markdown","9979d118":"markdown","0e9ed991":"markdown","8b8266e3":"markdown","650428cb":"markdown","27c69d23":"markdown","18eaaef5":"markdown","42b25e26":"markdown","b0ea0b6d":"markdown","331d0578":"markdown","9444c3c7":"markdown","eb232ca5":"markdown","46aeda18":"markdown","c7e37d11":"markdown","0dd397c9":"markdown","4ee86b69":"markdown","8a794970":"markdown","f95f9a17":"markdown","dff340af":"markdown","1b9308ea":"markdown","a297a5ad":"markdown","bf9c5a77":"markdown","c694cbea":"markdown","518f4278":"markdown","3f0221f1":"markdown","0c3d20fe":"markdown","9e2ec92b":"markdown","94170012":"markdown","43c7dce3":"markdown","4905411b":"markdown","606956d1":"markdown","8759e82c":"markdown","089f7612":"markdown","c2101b47":"markdown","79ea2a74":"markdown","23521e0a":"markdown","8bd84dce":"markdown","62306a20":"markdown","198c90db":"markdown","1aad66d0":"markdown","31cce808":"markdown","fdfc855e":"markdown","041bfa13":"markdown","cbace57e":"markdown","4187478e":"markdown","b9e72e00":"markdown","9f9bbed1":"markdown","7952bee5":"markdown","2a2a6917":"markdown","ad116f5b":"markdown","378d84ce":"markdown","ba562f30":"markdown","181ae1d8":"markdown","ce970c23":"markdown","01896cb9":"markdown","1d5d38f9":"markdown","f6e5340e":"markdown","e55d8a44":"markdown","641ba2b9":"markdown","46fd3633":"markdown","658889ad":"markdown","829547f8":"markdown","1242c77b":"markdown","91aa8f9a":"markdown","fb092ec8":"markdown","e91e2a71":"markdown","397b86ff":"markdown","dd785532":"markdown","8ec5b305":"markdown","de2c6681":"markdown","00a59ed2":"markdown","03bc7a23":"markdown","c2ab582f":"markdown","8a93c6c7":"markdown"},"source":{"cc346952":"from IPython.display import HTML\nfrom IPython.display import Image\nImage(url= \"https:\/\/images.mapsofindia.com\/liveblog\/2018\/10\/air-quality-index-of-the-biggest-cities-in-india-f1.jpg\")","f6cbae28":"from IPython.core.display import HTML\nHTML('''<script>\ncode_show=true; \nfunction code_toggle() {\n if (code_show){\n $('div.input').hide();\n } else {\n $('div.input').show();\n }\n code_show = !code_show\n} \n$( document ).ready(code_toggle);\n<\/script>\nThe raw code for this IPython notebook is by default hidden for easier reading.\nTo toggle on\/off the raw code, click <a href=\"javascript:code_toggle()\">here<\/a>.''')","1ce97800":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import Imputer\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (10, 7)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_log_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.tools import add_constant\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport warnings; warnings.simplefilter('ignore')","60c27f83":"dataset=pd.read_csv('..\/input\/data.csv',encoding=\"ISO-8859-1\")\ndataset.describe()","ca1e20d1":"print(dataset.info())\n\n#Now, we can immediatly see that there are quite a few nulls in various columns, \n#which need work and first need a closer inspection","c75d7a27":"dataset.head()","969f3ab4":"dataset.drop(['stn_code','agency','sampling_date','location_monitoring_station'],axis=1,inplace=True)\ndataset.info()\ndataset.head()","92699505":"#Finding missing values in the data set \ntotal = dataset.isnull().sum()[dataset.isnull().sum() != 0].sort_values(ascending = False)\npercent = pd.Series(round(total\/len(dataset)*100,2))\npd.concat([total, percent], axis=1, keys=['total_missing', 'percent'])","d2f6c6d8":"def remove_outlier(df_in, col_name):\n    q1 = df_in[col_name].quantile(0.25)\n    q3 = df_in[col_name].quantile(0.75)\n    iqr = q3-q1 #Interquartile range\n    fence_low  = q1-1.5*iqr\n    fence_high = q3+1.5*iqr\n    df_out = df_in.loc[(df_in[col_name] > fence_low) & (df_in[col_name] < fence_high)]\n    #return df_out\n\nremove_outlier(dataset,'so2')\nremove_outlier(dataset,'no2')\nremove_outlier(dataset,'rspm')\nremove_outlier(dataset,'spm')\n\n","38acb613":"dataset.groupby('state')[['spm','pm2_5','rspm','so2','no2']].mean()","fbcbff3f":"by_State=dataset.groupby('state')\n\ndef impute_mean(series):\n    return series.fillna(series.mean())\n\ndataset['rspm']=by_State['rspm'].transform(impute_mean)\ndataset['so2']=by_State['so2'].transform(impute_mean)\ndataset['no2']=by_State['no2'].transform(impute_mean)\ndataset['spm']=by_State['spm'].transform(impute_mean)\ndataset['pm2_5']=by_State['pm2_5'].transform(impute_mean)\n\n\n\n\n\n#imputer = Imputer(missing_values = 'NaN', strategy = 'median', axis = 0)\n#imputer = imputer.fit(dataset.groupby('state').iloc[:, 3:8].values)\n#dataset.iloc[:,3:8] = imputer.transform(dataset.iloc[:, 3:8].values)\n#dataset.info()\n","affcdf90":"#Missing values being filled in columns\nfor col in dataset.columns.values:\n    if dataset[col].isnull().sum() == 0:\n        continue\n    if col == 'date':\n        guess_values = dataset.groupby('state')['date'].apply(lambda x: x.mode().max())\n    elif col=='type':\n        guess_values = dataset.groupby('state')['type'].apply(lambda x: x.mode().max())\n    else:\n        guess_values = dataset.groupby('state')['location'].apply(lambda x: x.mode().max())\ndataset.head()","7fdcc94a":"def calculate_si(so2):\n    si=0\n    if (so2<=40):\n     si= so2*(50\/40)\n    elif (so2>40 and so2<=80):\n     si= 50+(so2-40)*(50\/40)\n    elif (so2>80 and so2<=380):\n     si= 100+(so2-80)*(100\/300)\n    elif (so2>380 and so2<=800):\n     si= 200+(so2-380)*(100\/420)\n    elif (so2>800 and so2<=1600):\n     si= 300+(so2-800)*(100\/800)\n    elif (so2>1600):\n     si= 400+(so2-1600)*(100\/800)\n    return si\ndataset['si']=dataset['so2'].apply(calculate_si)\ndf= dataset[['so2','si']]\ndf.head()","36f9931a":"#Function to calculate no2 individual pollutant index(ni)\ndef calculate_ni(no2):\n    ni=0\n    if(no2<=40):\n     ni= no2*50\/40\n    elif(no2>40 and no2<=80):\n     ni= 50+(no2-40)*(50\/40)\n    elif(no2>80 and no2<=180):\n     ni= 100+(no2-80)*(100\/100)\n    elif(no2>180 and no2<=280):\n     ni= 200+(no2-180)*(100\/100)\n    elif(no2>280 and no2<=400):\n     ni= 300+(no2-280)*(100\/120)\n    else:\n     ni= 400+(no2-400)*(100\/120)\n    return ni\ndataset['ni']=dataset['no2'].apply(calculate_ni)\ndf= dataset[['no2','ni']]\ndf.head()","df0b1e7d":"#Function to calculate rspm individual pollutant index(rpi)\ndef calculate_(rspm):\n    rpi=0\n    if(rpi<=30):\n     rpi=rpi*50\/30\n    elif(rpi>30 and rpi<=60):\n     rpi=50+(rpi-30)*50\/30\n    elif(rpi>60 and rpi<=90):\n     rpi=100+(rpi-60)*100\/30\n    elif(rpi>90 and rpi<=120):\n     rpi=200+(rpi-90)*100\/30\n    elif(rpi>120 and rpi<=250):\n     rpi=300+(rpi-120)*(100\/130)\n    else:\n     rpi=400+(rpi-250)*(100\/130)\n    return rpi\ndataset['rpi']=dataset['rspm'].apply(calculate_si)\ndf= dataset[['rspm','rpi']]\ndf.head()\n#df.tail()","63c21741":"#Function to calculate spm individual pollutant index(spi)\ndef calculate_spi(spm):\n    spi=0\n    if(spm<=50):\n     spi=spm*50\/50\n    elif(spm>50 and spm<=100):\n     spi=50+(spm-50)*(50\/50)\n    elif(spm>100 and spm<=250):\n     spi= 100+(spm-100)*(100\/150)\n    elif(spm>250 and spm<=350):\n     spi=200+(spm-250)*(100\/100)\n    elif(spm>350 and spm<=430):\n     spi=300+(spm-350)*(100\/80)\n    else:\n     spi=400+(spm-430)*(100\/430)\n    return spi\ndataset['spi']=dataset['spm'].apply(calculate_spi)\ndf= dataset[['spm','spi']]\ndf.head()\n","71d5077b":"#Function to calculate pm2_5 individual pollutant index(pmi)\ndef calculate_pmi(pm2_5):\n    pmi=0\n    if(pm2_5<=50):\n     pmi=pm2_5*(50\/50)\n    elif(pm2_5>50 and pm2_5<=100):\n     pmi=50+(pm2_5-50)*(50\/50)\n    elif(pm2_5>100 and pm2_5<=250):\n     pmi= 100+(pm2_5-100)*(100\/150)\n    elif(pm2_5>250 and pm2_5<=350):\n     pmi=200+(pm2_5-250)*(100\/100)\n    elif(pm2_5>350 and pm2_5<=450):\n     pmi=300+(pm2_5-350)*(100\/100)\n    else:\n     pmi=400+(pm2_5-430)*(100\/80)\n    return pmi\ndataset['pmi']=dataset['pm2_5'].apply(calculate_pmi)\ndf= dataset[['pm2_5','pmi']]\n#df.tail()","1a43cd28":"from IPython.display import Image\nImage(url= \"http:\/\/airquality.deq.idaho.gov\/Information_AQI_files\/image002.jpg\")","aba518ff":"#function to calculate the air quality index (AQI) of every data value\n#its is calculated as per indian govt standards\ndef calculate_aqi(si,ni,spi,rpi):\n    aqi=0\n    if(si>ni and si>spi and si>rpi):\n     aqi=si\n    if(spi>si and spi>ni and spi>rpi):\n     aqi=spi\n    if(ni>si and ni>spi and ni>rpi):\n     aqi=ni\n    if(rpi>si and rpi>ni and rpi>spi):\n     aqi=rpi\n    return aqi\ndataset['AQI']=dataset.apply(lambda x:calculate_aqi(x['si'],x['ni'],x['spi'],x['rpi']),axis=1)\ndf= dataset[['state','si','ni','rpi','spi','AQI']]\ndf.head()\n\n\ndf.head()","03979703":"#Visualization of AQI across india \n\ndataset['date'] = pd.to_datetime(dataset['date'],format='%Y-%m-%d') # date parse\ndataset['year'] = dataset['date'].dt.year # year\ndataset['year'] = dataset['year'].fillna(0.0).astype(int)\ndataset = dataset[(dataset['year']>0)]\n\ndf = dataset[['AQI','year','state']].groupby([\"year\"]).median().reset_index().sort_values(by='year',ascending=False)\nf,ax=plt.subplots(figsize=(15,10))\nsns.pointplot(x='year', y='AQI', data=df)","e33425cf":"#Exploring air pollution state-wise\ndataset.fillna(0.0,inplace=True)\nstates=dataset.groupby(['state','location'],as_index=False).mean()\nstate=states.groupby(['state'],as_index=False).mean()\nstate","78e8f5be":"fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(40,40))\nstate_pollution = dataset[['state','so2']].groupby('state').mean()\nstate_pollution.reset_index(inplace = True)\nstate_pollution.sort_values('so2', ascending= False, inplace = True)\nstate_pollution.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[0][0])\n\nstate_pollution_no2 = dataset[['state','no2']].groupby('state').mean()\nstate_pollution_no2.reset_index(inplace = True)\nstate_pollution_no2.sort_values('no2', ascending= False, inplace = True)\nstate_pollution_no2.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[0][1])\n\nstate_pollution_rspm = dataset[['state','rspm']].groupby('state').mean()\nstate_pollution_rspm.reset_index(inplace = True)\nstate_pollution_rspm.sort_values('rspm', ascending= False, inplace = True)\nstate_pollution_rspm.plot(kind = 'bar', figsize= (20,40), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[1][0])\n\n\nstate_pollution_spm = dataset[['state','spm']].groupby('state').mean()\nstate_pollution_spm.reset_index(inplace = True)\nstate_pollution_spm.sort_values('spm', ascending= False, inplace = True)\nstate_pollution_spm.plot(kind = 'bar', figsize= (20,40), x = 'state', fontsize= 15, title = 'States & Pollutant Levels',ax=axes[1][1])\n\n\nstate_pollution = dataset[['state','pm2_5']].groupby('state').mean()\nstate_pollution.reset_index(inplace = True)\nstate_pollution.sort_values('pm2_5', ascending= False, inplace = True)\nstate_pollution.plot(kind = 'bar', figsize= (20,10), x = 'state', fontsize= 15, title = 'States & Pollutant Levels')\n\n","6a4485f7":"#correlation\ndataset.corr()\nplt.figure(figsize=(16,12))\nax=plt.axes()\n#sns.heatmap(data=fifa_dataset.iloc[:,2:].corr(),annot=True,fmt='.2f',cmap='coolwarm',ax=ax)\nmask = np.zeros_like(dataset.iloc[:,:].corr())\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    ax = sns.heatmap(data=dataset.iloc[:,:].corr(), mask=mask, vmax=.3, annot=True,fmt='.2f', square=True, cmap='coolwarm')\n    \nax.set_title('Heatmap showing correlated values for the Dataset')\nplt.show()\n","5d656e7d":"#Heatmap Pivot with State as Row, Year as Col, AQI as Value\ndataset['date'] = pd.to_datetime(dataset['date'],format='%Y-%m-%d') # date parse\ndataset['year'] = dataset['date'].dt.year # year\ndataset['year'] = dataset['year'].fillna(0.0).astype(int)\ndataset = dataset[(dataset['year']>0)]\nf, ax = plt.subplots(figsize=(40,40))\nax.set_title('{} by state and year'.format('AQI'))\nsns.heatmap(dataset.pivot_table('AQI', index='state',\n                columns=['year'],aggfunc='mean',margins=True),\n                annot=True,cmap=\"BuPu\", linewidths=.75, ax=ax,cbar_kws={'label': 'Annual Average'})","2ef7724f":"fig, axes = plt.subplots(nrows=2, ncols=5, figsize=(34,20))\nplt.subplots_adjust(hspace=0.4)\n\nz = pd.Series()\nfor col in dataset.columns.values[3:15]:\n    if ((col!='AQI')&(col!='state')&(col!='location')&(col!='type')&(col!='date')&(col!='year')&(col!='state_label')&(col!='type_label')):\n      \n        colums=np.array(dataset[col])\n        z[col]=colums\n#p=z.loc[z.index]\n\nfor i in range(2):\n    for j in range(5):\n        \n        #x=z.index.values[i*3+j]\n        #sns.barplot(z.index[i*3+j],z.values[i*3+j])\n        #x=z.index.values[i*3+j]\n        \n        y_label=z.index[i*5+j]\n        x_label=z[i*5+j]\n        \n        sns.regplot(data=dataset, x=z.index[i*5+j], y='AQI',ax=axes[i,j])\n\n\nfig.suptitle('Distribution of Correlated Factors', fontsize='25')\nplt.show()","38178ef2":"dataset['state_label'] = dataset['state'].astype('category')\ncat_columns = dataset.select_dtypes(['category']).columns\ndataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)\n","fd3fa710":"dataset[\"type_label\"]=dataset[\"type\"].astype(str)\ndataset[\"type_label\"] = np.where(dataset[\"type\"].str.contains('Residential, Rural and other Areas'), 1, 0)\n","a5301527":"\ndataset.head()","2f8f0e4c":"data_p = dataset.drop(['state', 'location', 'type','date','AQI','year'],axis=1)\ncorr = data_p.corr()\ncolumns = np.full((corr.shape[0],), True, dtype=bool)\nselected_column=data_p.columns[columns]                \ndata_p=data_p[selected_column]\n\n","d3c44df5":"x_=data_p\ny=dataset['AQI']\n#factors with p-value\nselected_columns_1 = selected_column[0:10].values\nselected_columns_2=selected_column[11:].values\nselected_columns=np.concatenate((selected_columns_1,selected_columns_2),axis=0)\nimport statsmodels.formula.api as smf\ndef backwardelimination(x, Y, sl, columns):\n    numVars = len(x[0])\n    \n    for i in range(0, numVars):\n        regressor_OLS = smf.OLS(Y, x).fit()\n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        \n        if maxVar > sl:\n          \n            for j in range(0, numVars - i):\n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    x = np.delete(x,j,1)\n                    columns = np.delete(columns, j)\n                    \n    regressor_OLS.summary()\n    return x, columns\nSL = 0.05\ndata_modeled, selected_columns = backwardelimination(x_.iloc[:,:].values, y.values, SL,selected_columns )\n\n\n","48ea0933":"result = pd.DataFrame()\nresult['AQI'] = dataset.iloc[:,10]\n#result.head()\n\ndata_p_selected = pd.DataFrame(data= data_modeled[:,0:9], columns = selected_columns)\ndata_p_selected.head()","6aea140a":"def linear_regression(X,y):\n#SPLIT TEST AND TRAIN\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n#One Hot Encoding\n    X_train = pd.get_dummies(X_train)\n    X_test = pd.get_dummies(X_test)\n\n#Linear Regression\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    predictions = LR.predict(X_test)\n\n    print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\n    print('r2_Square:%.2f '% r2_score(y_test, predictions))\n    print('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, predictions)))\n    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())\n    \n    regressor_OLS = smf.OLS(y_train, X_train).fit()\n    \n    plt.figure(figsize=(18,10))\n    plt.scatter(predictions,y_test,alpha = 0.3)\n    plt.xlabel('Predictions')\n    plt.ylabel('AQI')\n    plt.title(\"Linear Prediction \")\n    plt.show()\n#cross validation    \n    Kfold = KFold(len(X), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\n    z=print(regressor_OLS.summary())\n    return z\n\n   \n        \n    \n    \n    ","75dcd82a":"####function to calculate cross validation score only\ndef cross_val(X,y):\n    #SPLIT TEST AND TRAIN\n    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)\n\n#One Hot Encoding\n    X_train = pd.get_dummies(X_train)\n    X_test = pd.get_dummies(X_test)\n\n#Linear Regression\n    LR = LinearRegression()\n    LR.fit(X_train, y_train)\n    predictions = LR.predict(X_test)\n    Kfold = KFold(len(X), shuffle=True)\n    print(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(LR,X_train,y_train,cv=10).mean())\n    ","c28b9439":"X_1=dataset[['si','ni','rpi','spi']]\ny_1=dataset['AQI']\nlinear_regression(X_1,y_1)","36b56bff":"#Linear Regression\nX_2= dataset.drop([ 'AQI','state','location','type','rspm','si','ni','rpi','spi','pmi','date','year','state_label','type_label'], axis = 1)\ny_2 = dataset['AQI']\nlinear_regression(X_2,y_2)\n#regressor_OLS = smf.OLS(y_train, X_train).fit()\n#print(\"Model Summary\")\n#regressor_OLS.summary()\n","cbed570b":"#Linear Regression\nX_3= dataset.drop(['AQI','state','location','type','date','pmi','pm2_5','year','state_label','type_label','so2','no2','si','ni'], axis = 1)\ny_3 = dataset['AQI']\nlinear_regression(X_3,y_3)\n#SPLIT TEST AND TRAIN\n#X_train, X_test, y_train, y_test = train_test_split(dataset_LR_3, target, test_size=0.2)\n\n#One Hot Encoding\n#X_train = pd.get_dummies(X_train)\n#X_test = pd.get_dummies(X_test)\n#print(X_test.shape,X_train.shape,y_test.shape,y_train.shape)","149b39ab":"def AQI_Range(x):\n    if x<=50:\n        return \"Good\"\n    elif x>50 and x<=100:\n        return \"Moderate\"\n    elif x>100 and x<=200:\n        return \"Unhealthy for sensitive groups\"\n    elif x>200 and x<=300:\n        return \"Unhealthy\"\n    elif x>300 and x<=400:\n        return \"Very unhealthy\"\n    elif x>400:\n        return \"Hazardous\"\n\ndataset['AQI_Range'] = dataset['AQI'] .apply(AQI_Range)\ndataset.head()\n","55b09566":"dataset['AQI_label'] = dataset['AQI_Range'].astype('category')\ncat_columns = dataset.select_dtypes(['category']).columns\ndataset[cat_columns] = dataset[cat_columns].apply(lambda x: x.cat.codes)\n\ndataset.head()\n","c0ef7aa4":"def AQI_Range_Binary_Label(x):\n    if x<=200:\n        return 0\n    elif x>200:\n        return 1\n    \ndataset['AQI_Range_Binary'] = dataset['AQI'] .apply(AQI_Range_Binary_Label)\ndataset.head()","896ef161":"dataset_box=dataset.drop(['state', 'location', 'date','state_label',\n       'type_label','type','AQI_label','AQI_Range_Binary','year','AQI_Range'],axis=1)\nf, ax = plt.subplots(figsize=(20, 15))\n\nax.set_facecolor('#FFFFFF')\nplt.title(\"Box Plot AQI Dataset Scaled\")\nax.set(xlim=(-10, 250))\nax = sns.boxplot(data = dataset_box, \n  orient = 'h', \n  palette = 'Set3')","b7e1fdc0":"predictor_names=dataset_box.columns.get_values()\npredictor_names=predictor_names.tolist()\npredictor_names.pop()\npredictor_names\n","37472e0a":"def rank_predictors(dat,l,f='AQI_Range_Binary'):\n    rank={}\n    max_vals=dat.max()\n    median_vals=dat.groupby(f).median()  # We are using the median \n    for p in l:\n        score=np.abs((median_vals[p][1]-median_vals[p][0])\/max_vals[p])\n        rank[p]=score\n    return rank\ncat_rank=rank_predictors(dataset,predictor_names) \ncat_rank","0a26826d":"# Take the top predictors based on mean difference\ncat_rank=sorted(cat_rank.items(), key=lambda x: x[1],reverse= True)\n\nranked_predictors=[]\nfor f in cat_rank:\n    ranked_predictors.append(f[0])\nranked_predictors\n\n","3397860d":"data_log=dataset.drop(['state','type','date','location'],axis=1)\n","ff50da46":"def logistic_regression(x,y):\n    x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.25,random_state=0)\n    sc = StandardScaler()\n    \n    # Feature scaling\n    x_train = sc.fit_transform(x_train)\n    x_test = sc.fit_transform(x_test)\n    \n    #Fitting logistic regression to the training set\n    classifier = LogisticRegression(random_state = 0)\n    classifier.fit(x_train,y_train)\n    \n    \n    # Logistic regression cross validation\n    #Kfold = KFold(len(ranked_predictors), shuffle=False)\n    #print(\"KfoldCrossVal mean score using Logistic regression is %s \\n\" %cross_val_score(classifier,x,y,cv=10).mean())\n    k_fold = KFold(n_splits=10, shuffle=True, random_state=0)\n    cvs=cross_val_score(classifier, x_train, y_train, cv=k_fold).mean()\n    print(\"KfoldCrossVal mean score using Logistic regression is %s \\n\"%cvs)\n    \n    print(\"Logistic Analysis Report\")\n    y_pred = classifier.predict(x_test)\n    print(classification_report(y_test,y_pred))\n    print(y_pred)\n    #Accuracy score\n    print (\"Accuracy Score:%.2f\" % metrics.accuracy_score(y_test,classifier.predict(x_test)))\n    #probabilty of dependent variable\n    y_pred_proba = classifier.predict_proba(x_test)[::,1]\n    print('Probabilty of dependent variable')\n    print(y_pred_proba.mean())\n    fpr, tpr, _ = metrics.roc_curve(y_test,  y_pred_proba)\n    auc = metrics.roc_auc_score(y_test, y_pred_proba)\n    plt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.legend(loc=4)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n   \n    \n\n","90b0225e":"def logit_summary(y,X):\n    logit_model=sm.Logit(y,X)\n    result=logit_model.fit()\n    print(\"Model Summary\")\n    print(result.summary2())","77d5ea64":"data_log_1=dataset.drop(['state','type','date','location','AQI_label','AQI_Range','AQI','AQI_Range_Binary'],axis=1)\nx=data_log_1.iloc[:,:]\ny=data_log.iloc[:,16]\n\nlogistic_regression(x,y)\nlogit_summary(y,x)","05098e04":"#Logistic Regression Model 2\ndata_log_2=dataset.drop(['state','type','date','location','AQI_Range','type_label','AQI_label','AQI'],axis=1)\n","f363fe74":"x=data_log_2.iloc[:,:]\ny=dataset['type_label']\nlogistic_regression(x,y)\nlogit_summary(y,x)","aedff840":"x=dataset[predictor_names]\ny=data_log.iloc[:,13]\nlogistic_regression(x,y)\nlogit_summary(y,x)","f47572c9":"def variance_IF(X):\n    vif=vif = pd.DataFrame()\n    vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif[\"features\"] = X.columns\n    return vif\n","75a53e82":"##For first Linear Model\nvariance_IF(X_1)","ceb4b05a":"##For second Linear Model\nvariance_IF(X_2)","0f3e29ce":"##For third Linear Model\nvariance_IF(X_3)","6804c787":"#Linear Model 1:-\nX_M1=X_1[['si','ni']]\nprint('Linear Model 1')\ncross_val(X_M1,y_1)","468f2894":"#Linear Model 2:-\nprint('Linear Model 2')\ncross_val(X_2,y_2)","fc97f828":"#Linear Model 2:-\nprint('Linear Model 3')\ncross_val(X_3,y_3)","f560239b":"#https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\nimport statsmodels.api as sm\n\nX=X_2.astype(float)\ny=dataset['AQI']\n\n\ndef stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        \n        for new_column in excluded:\n            model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = model.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.argmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        model = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = model.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = pvalues.argmax()\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\nresult = stepwise_selection(X, y)\n\nprint('resulting features:')\nprint(result)\n","e031fc72":"dataset_LR_2=dataset[['so2', 'spm', 'no2', 'pm2_5']]\ntarget=dataset['AQI']\nlinear_regression(dataset_LR_2,target)\n#X_train, X_test, y_train, y_test = train_test_split(dataset_LR_2, target, test_size=0.2)\n","8bb29ceb":"def evaluateModel (model):\n    print(\"RSS = \", ((dataset.AQI - model.predict())**2).sum())\n    print(\"R2 = \", model.rsquared)","076761f5":"modelAll = smf.ols('AQI ~ no2 + rspm + spm', dataset).fit()\nprint(modelAll.summary().tables[1])\nevaluateModel(modelAll)","d1ffaeb5":"modelNo2_rspm = smf.ols('AQI ~ no2 + rspm ', dataset).fit()\nprint(modelNo2_rspm.summary().tables[1])\nevaluateModel(modelNo2_rspm)","78cc26ba":"modelNo2_spm = smf.ols('AQI ~ no2 + spm ', dataset).fit()\nprint(modelNo2_spm.summary().tables[1])\nevaluateModel(modelNo2_spm)","ae21fe0e":"modelspm_rspm = smf.ols('AQI ~ spm + rspm ', dataset).fit()\nprint(modelspm_rspm.summary().tables[1])\nevaluateModel(modelspm_rspm)\n","60383448":"model_spm = smf.ols('AQI ~ spm', dataset).fit()\nprint(model_spm.summary().tables[1])\nevaluateModel(model_spm)","467ce9a3":"model_no2 = smf.ols('AQI ~ no2', dataset).fit()\nprint(model_no2.summary().tables[1])\nevaluateModel(model_no2)","bdd2e8d6":"model_rspm = smf.ols('AQI ~ rspm', dataset).fit()\nprint(model_rspm.summary().tables[1])\nevaluateModel(model_rspm)","1a9da8c4":"modelSynergy = smf.ols('AQI ~ spm + rspm + spm*rspm', dataset).fit()\nprint(modelSynergy.summary().tables[1])\nevaluateModel(modelSynergy)","63fc5d65":"modelSynergy = smf.ols('AQI ~ no2 + spm + no2*spm', dataset).fit()\nprint(modelSynergy.summary().tables[1])\nevaluateModel(modelSynergy)","4f54d57b":"\n#Regularization:- l2\n\nfrom sklearn.linear_model import Ridge\n#for i in range(0, 1): Matrix\nX_train, X_test, y_train, y_test = train_test_split(X_1,y_1, test_size=0.2)\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\n\nridgeReg = Ridge(alpha=0.05, normalize=True)\nridgeReg.fit(X_train,y_train)\npred = ridgeReg.predict(X_test)\n\nprint(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\nprint('r2_Square:%.2f '% r2_score(y_test, pred))\nprint('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n\nregressor_OLS = smf.OLS(y_train, X_train).fit()\n\nplt.figure(figsize=(18,10))\nplt.scatter(pred,y_test,alpha = 0.3)\nplt.xlabel('Predictions')\nplt.ylabel('AQI')\nplt.title(\"Linear Prediction \")\nplt.show()\n#cross validation    \nKfold = KFold(len(X_1), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\nprint(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(ridgeReg,X_train,y_train,cv=10).mean())\n\n\nprint(regressor_OLS.summary())\n\n\n\n\n\n","6e39e5e4":"from sklearn.linear_model import Ridge\n\n# Create an array of alphas and lists to store scores\nalpha_space = np.logspace(-4, 0, 50)\nridge_scores = []\nridge_scores_std = []\n\n# Create a ridge regressor: ridge\nridge = Ridge(normalize=True)\n\n# Compute scores over range of alphas\nfor alpha in alpha_space:\n\n    # Specify the alpha value to use: ridge.alpha\n    ridge.alpha = alpha\n    \n    # Perform 10-fold CV: ridge_cv_scores\n    ridge_cv_scores = cross_val_score(ridge, X_train, y_train, cv=10)\n    \n    # Append the mean of ridge_cv_scores to ridge_scores\n    ridge_scores.append(np.mean(ridge_cv_scores))\n    \n    # Append the std of ridge_cv_scores to ridge_scores_std\n    ridge_scores_std.append(np.std(ridge_cv_scores))\n\n# Use this function to create a plot    \ndef display_plot(cv_scores, cv_scores_std):\n    fig = plt.figure()\n    ax = fig.add_subplot(1,1,1)\n    ax.plot(alpha_space, cv_scores)\n    \n    std_error = cv_scores_std \/ np.sqrt(10)\n\n    ax.fill_between(alpha_space, cv_scores + std_error, cv_scores - std_error, alpha=0.2)\n    ax.set_ylabel('CV Score +\/- Std Error')\n    ax.set_xlabel('Alpha')\n    ax.axhline(np.max(cv_scores), linestyle='--', color='.5')\n    ax.set_xlim([alpha_space[0], alpha_space[-1]])\n    ax.set_xscale('log')\n    plt.show()\n\n# Display the plot\ndisplay_plot(ridge_scores, ridge_scores_std)","caea371a":"\"\"\"X_train, X_test, y_train, y_test = train_test_split(X_3,dataset['AQI'], test_size=0.2)\nX_train = pd.get_dummies(X_train)\nX_test = pd.get_dummies(X_test)\n\nlassoReg = Lasso(alpha=0.3, normalize=True)\nlassoReg.fit(X_train,y_train)\n\nprint(X_test.shape,X_train.shape,y_test.shape,y_train.shape)\nprint('r2_Square:%.2f '% r2_score(y_test, pred))\nprint('MSE:%.2f '% np.sqrt(mean_squared_error(y_test, pred)))\n\nregressor_OLS = smf.OLS(y_train, X_train).fit()\n\nplt.figure(figsize=(18,10))\nplt.scatter(pred,y_test,alpha = 0.3)\nplt.xlabel('Predictions')\nplt.ylabel('AQI')\nplt.title(\"Linear Prediction \")\nplt.show()\n#cross validation    \nKfold = KFold(len(X), shuffle=True)\n    #X_train = sc.fit_transform(X_train)\n    #X_test = sc.transform(X_test)\nprint(\"KfoldCrossVal mean score using Linear regression is %s\" %cross_val_score(lassoReg,X_train,y_train,cv=10).mean())\n\n\nregressor_OLS.summary()\"\"\"","477ec113":"### 6.4 Interaction Effect-SPM & RSPM<a id='RSPMSPM'><\/a>","8f4b2167":"### Following tables gives information about new dataset after dropping of unneccessary columns","c4432981":"The Following plot gives us our first Linear Model along with the summary for the same.\n\n>Training Features :- si, ni, rpi, spi, pmi\n\n>Target Feature:- AQI\n","a07e8bbd":"Copyright (c) 2019 Manali Sharma\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n","8269bb48":"### 6.1 Interaction Effect-All Columns<a id='AllColumns'><\/a>","139d3000":"Following gives us the dictionary of predictors that have been ranked in order to see which variables we should use for our logistic model. We are using median as to rank the predictors.","b02a08a3":"## 2.3 Linear Model 3 <a id='LinearModel3'><\/a>\n\nThe Following plot gives us our second Linear Model along with the summary for the same.\n\n>Training Features :- rspm, spm, spi, rpi\n\n>Target Feature:- AQI","11329995":"#### Table Overview:- Gives the first 5 rows of the dataset with converted categorical variables i.e state_label and type_label","f382fae4":"##   1.5.1  Pointplot  <a id='pointplot'><\/a>","1cde6ef1":"Function to calculate the RSS and R2","34dab628":"### 6.3 Interaction Effect-NO2 & SPM<a id='NO2SPM'><\/a>","b06b8e56":"## 1.5.5 Categorical Conversion  <a id='catconversion'><\/a>\n#### Our analysis requires at least one independent variable which needs to be a multi-class categorical variable and a binary categorical variable and its conversion to numeric data.\n\nWe will be using cat coding and one hot encoding for the same, cat coding converts categorical data into numeric for use , basically it provides numbers for ordinal data. Cat coding creates a mapping of our sortable categories, e. g. old < renovated < new \u2192 0, 1, 2\n\nOne hot encoding (binary values from categorical data)\nA one hot encoding is a representation of categorical variables as binary vectors.  This first requires that the categorical values be mapped to integer values. Then, each integer value is represented as a binary vector that is all zero values except the index of the integer, which is marked with a 1.\n\nAnd in our dataset state is a multiclass variable and type is a binary categorical variable. So we are changing them eventually. Further in our analysis we will be using cat coding only to convert our ordinal data.\n\nWe have mostly used cat coding as it changes the target column itself while in one hot encoding based on number of types the column holds, new columns be created according to the type with binary values i.e 1 and 0 . Thereby making data set much more complex and increasing redundancy. Although both methods be used for categorical to numeric conversion , we have preferred cat coding over one hot encoding","18649b84":"## 2.1 Linear Model 1 <a id='LinearModel1'><\/a>","ca3d2bbb":"#### Significant Questions on the basis of the all 3 Logistic Models\n##### * Is the relationship significant?   \n\nRelationship between AQI_Range_Binary and all other pollutants and their indexes(independent variable) are significant as we can see the first Logistic Model 1 .\nRelationship between type_label(Dependent Variable) and other independent variable is somewhat relatable though we cannot say for sure they are highly related or not.\n\n##### * Are any model assumptions violated?      \n\nThe logistic regression method assumes that:\n\n1. The outcome is a binary or dichotomous variable like yes vs no, positive vs negative, 1 vs 0.\n2. There is a linear relationship between the logit of the outcome and each predictor variables. Recall that the logit function is logit(p) = log(p\/(1-p)), where p is the probabilities of the outcome (see Chapter @ref(logistic-regression)).\n3. There is no influential values (extreme values or outliers) in the continuous predictors\n4. There is no high intercorrelations (i.e. multicollinearity) among the predictors.\n\nby the results we can say that outcome is binary. Since we have scaled the dataset , there are no outliers and as for multicollinearity please refer to section [Multicollinearity](#Multicollinearity)\n\n\n##### * Does the model make sense?  Interpret the meaning of each independent variable.  \n\nExcept for model 1 in which dependent variable is AQI_Range_Binary accuracy is 99% , so we can say that model makes sense . Since with the value of each independent variable the dependent is increasing.\nBut for model 2 and model 3 , the dependent variable is Type_label and accuracy is less , so though it does make sense but a better model can be used in its place as well.\n\n##### * Cross-validate the model. How well did it do? \nCross Validation of each model is done with each models generation. And according to cross validation score only first model gives an accurate score .\n\n##### * Calculate the probability of getting the dependent variable\nDone within modeling. Please see each model","9979d118":"## 1.5.2 Heatmap  <a id='heatmap'><\/a>","0e9ed991":"Following gives us the list of predictors that we need to rank in order to see which variables we should use for our logistic model","8b8266e3":"The Following plot gives us our first Logistic Model along with the summary for the same.\n\n>Training Features :- 'so2', 'no2', 'rspm', 'spm', 'pm2_5', 'si', 'ni', 'rpi', 'spi', 'pmi',\n       'year', 'state_label', 'type_label'\n\n>Target Feature:- AQI_Range_Binary\n","650428cb":"## <p style=\"text-align: center;\">5. Stepwise Regression<\/p> <a id='StepwiseRegression'><\/a>\n\nStepwise regression is a method of fitting regression models in which the choice of predictive variables is carried out by an automatic procedure. (p-value in our case). \n\n>BACKWARD STEPWISE REGRESSION is a stepwise regression approach, that begins with a full model and at each step gradually eliminates variables from the regression model to find a reduced model that best explains the data. Also known as Backward Elimination regression.\n\n>FORWARD STEPWISE REGRESSION is a type of stepwise regression which begins with an empty model and adds in variables one by one. ... It is one of two commonly used methods of stepwise regression; the other is backward elimination, and is almost opposite.Se","27c69d23":"### Information about each column and about null values for each column","18eaaef5":"#### Following table gives statewise distribution of all the major pollutants i.e so2, no2, rspm, spm, pm2_5","42b25e26":"### Statistcal analysis of given dataset","b0ea0b6d":"Table Overview:- Following Table gives us dataset with new column i.e AQI_Range that specifies the type of AQI effect","331d0578":"#### Function to calculate the air quality index (AQI) of every data value its is calculated as per indian govt standards\nThe purpose of the AQI is to help you understand what\nlocal air quality means to your health. Also it is scaled from 0 to 500.","9444c3c7":"## 3.3 Logistic Model 3 <a id='LogisticModel3'><\/a>","eb232ca5":"### Table Overview:- Exploring air pollution state-wise ","46aeda18":"#   1.4 Dataset Cleaning  <a id='dataset_cleaning'><\/a>","c7e37d11":"### 6.9 Interaction Effect-NO2* SPM<a id='NO2&SPM'><\/a>","0dd397c9":"The Following plot gives us our first Logistic Model along with the summary for the same.\n\n>Training Features :- predictor_names\n\n>Target Feature:- Type_label","4ee86b69":"## 1.5.4 Regplot  <a id='Regplot'><\/a>\n\nDistribution of important independent variables and their relation with dependent variable i.e AQI is being depicted by the graph. Basically it plots data and a linear regression model fit.","8a794970":"## <p style=\"text-align: center;\">10. Citation<\/p> <a id='Citation'><\/a>\n\n1. http:\/\/joshlawman.com\/metrics-classification-report-breakdown-precision-recall-f1\/\n2. https:\/\/github.com\/nikbearbrown\/INFO_6105\/tree\/master\/Week_2\n3. https:\/\/www.kaggle.com\/anbarivan\/indian-air-quality-analysis-prediction-using-ml\n4. https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm\n5. https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/a-comprehensive-guide-for-linear-ridge-and-lasso-regression\/\n6. https:\/\/www.kaggle.com\/marcogdepinto\/feature-engineering-eda-data-cleaning-tutorial\n7. https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/\n","f95f9a17":"### Missing values being filled in columns","dff340af":"\n## <p style=\"text-align: center;\">3. Logistic Regression<\/p> <a id='LogisticRegression'><\/a>\n\nLogistic regression is used to describe data and to explain the relationship between one dependent binary variable and one or more nominal, ordinal, interval or ratio-level independent variables.\n\nDifference between logistic and linear regression:-\nIn linear regression, the outcome (dependent variable) is continuous. It can have any one of an infinite number of possible values. In logistic regression, the outcome (dependent variable) has only a limited number of possible values(discrete). Also , we use logistic for classification purpose i.e for categorical variables.\n\n","1b9308ea":"## 3.1 Logistic Model 1 <a id='LogisticModel1'><\/a>","a297a5ad":"# <p style=\"text-align: center;\">1. Introduction<\/p>","bf9c5a77":"### Derivation for Individual Pollutant Index and AQI\n\nThe AQI is an index for reporting daily air quality. It tells you how clean or polluted your air is, and what associated health effects might be a concern for you. The AQI focuses on health effects you may experience within a few hours or days after breathing polluted air. EPA\ncalculates the AQI for five major air pollutants regulated by the Clean Air Act: groundlevel ozone, particle pollution Air quality directly affects (also known as particulate our quality of life. matter), carbon monoxide, sulfur dioxide, and nitrogen dioxide. For each of these\npollutants, EPA has established national air quality standards to protect public health.\n\nAQI is calculated on the range of 0-500, we are scaling the values according to the AQI calculation formula\n","c694cbea":"Now that we know that our dataset has missing values, we need to find the columns which has those values alongwith, the percentage effect it has with respect to whole dataset.\n\n#### Table Overview:- Following table gives us the column names with the number of missing values and percentage effect it has with respect to dataset","518f4278":"Since we are to form 3 different linear models for our given project, it would be better if form a function for linear regression that could be utilised everytime we want to perform linear regression . Also, it makes the code less complex and easier to read.\n\nWe will also be validating our model. \nCross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n\nWe will be performing K-Fold Cross Validation.Below are the steps for it:\n\n1. Randomly split your entire dataset into k\u201dfolds\u201d\n2. For each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n3. Record the error you see on each of the predictions\n4. Repeat this until each of the k-folds has served as the test set\n5. The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model","3f0221f1":"#### Function to calculate spm individual pollutant index(spi)\nThe index category for rspm is scaled between 0-430. So on applying formula which is used to calculate AQI","0c3d20fe":"## <p style=\"text-align: center;\">7. Regularization<\/p> <a id='Regularization'><\/a>\n\nRegularization is a very important technique in machine learning to prevent overfitting. Mathematically speaking, it adds a regularization term in order to prevent the coefficients to fit so perfectly to overfit. The difference between the L1 and L2 is just that L2 is the sum of the square of the weights, while L1 is just the sum of the weights\n\nIs there collinearity among some features? L2 regularization can improve prediction quality in this case, as implied by its alternative name, \"ridge regression.\" However, it is true in general that either form of regularization will improve out-of-sample prediction, whether or not there is multicollinearity and whether or not there are irrelevant features, simply because of the shrinkage properties of the regularized estimators. L1 regularization can't help with multicollinearity it will just pick the feature with the largest correlation to the outcome  (which isn't useful if you have an interest in estimating coefficients for all features which are strongly correlated with your target). Ridge regression can obtain coefficient estimates even when you have more features than examples... but the probability that any will be estimated precisely at 0 is 0.","9e2ec92b":"## 1.5.3 Heatmap Pivot  <a id='heatmp'><\/a>\n\n### AQI By State and Year","94170012":"# <p style=\"text-align: center;\"> Table of Contents <\/p>\n- ## 1. Introduction\n   - ### 1.1 [Abstract](#abstract)\n   - ### 1.2 [Importing Libraries](#importing_libraries)\n   - ### 1.3 [Dataset Summary](#dataset_summary)\n   - ### 1.4 [Dataset Cleaning](#dataset_cleaning)\n   - ### 1.5 [Exploratory Data Analysis (EDA)](#eda)\n        - ### 1.5.1[Pointplot](#pointplot) \n        - ### 1.5.2[Heatmap](#heatmap)\n        - ### 1.5.3[HeatMap Pivot](#heatmp)\n        - ### 1.5.4[Regplot](#Regplot)\n        - ### 1.5.5[Categorical Conversion](#catconversion)\n- ## 2. [Linear Regression](#LinearRegression)\n    - ### 2.1 [Linear Model 1](#LinearModel1)\n    - ### 2.2 [Linear Model 2](#LinearModel2)\n    - ### 2.3 [Linear Model 3](#LinearModel3)\n- ## 3. [Logistic Regression](#LogisticRegression)\n    - ### 3.1 [Logistic Model 1](#LogisticModel1)\n    - ### 3.2 [Logistic Model 2](#LogisticModel2)\n    - ### 3.3 [Logistic Model 3](#LogisticModel3) \n- ## 4. [Multicollinearity](#Multicollinearity)\n- ## 5. [Stepwise Regression](#StepwiseRegression)\n- ## 6. [Interaction Effect](#InteractionEffect)\n    - ### 6.1 [All Columns-NO2,RSPM, SPM](#AllColumns)\n    - ### 6.2 [RSPM & NO2](#RSPMNO2)\n    - ### 6.3 [SPM & NO2](#SPMNO2)\n    - ### 6.4 [RSPM & SPM](#RSPMSPM)\n    - ### 6.5 [SPM](#SPM)\n    - ### 6.6 [NO2](#NO2)\n    - ### 6.7 [RSPM](#RSPM)\n    - ### 6.8 [SPM & RSPM Interaction](#SPM&RSPM)\n    - ### 6.9 [NO2 & SPM Interaction](#NO2&SPM)\n- ## 7. [Regularization](#Regularization)\n- ## 8. [Conclusion](#Conclusion)\n- ## 9. [Contribution](#Contribution)\n- ## 10. [Citation](#Citation)\n- ## 11. [License](#License)","43c7dce3":"# 1.3 Dataset Summary <a id='dataset_summary'><\/a>","4905411b":"#### Function to calculate rspm individual pollutant index(rpi)\nThe index category for rspm is scaled between 0-400. So on applying formula which is used to calculate AQI","606956d1":"### The following table shows the first five rows of the given dataset, thereby giving us insight about what sort of dataset it is. And what are the attributes included in the dataset.","8759e82c":"### 6.6 Interaction Effect-NO2<a id='NO2'><\/a>","089f7612":"### 6.8 Interaction Effect-SPM*RSPM<a id='SPM&RSPM'><\/a>","c2101b47":"#### Table Overview:- For Linear Model 3 we can see that VIF for rspm, rpi ,spm and spi is much greater than 5. So model needs to be changed on a whole basis.\u00b6","79ea2a74":"#### Function to calculate pm2_5 individual pollutant index(pmi)\nThe index category for rspm is scaled between 0-430. So on applying formula which is used to calculate AQI","23521e0a":"## <p style=\"text-align: center;\">9. Contribution<\/p> <a id='Contribution'><\/a>\n\nI contributed around 65% in terms of coding for the given assignment.","8bd84dce":"Since we are to form 3 different logistic models for our given project, it would be better if form a function for logistic regression that could be utilised everytime we want to perform logistic regression . Also, it makes the code less complex and easier to read.\n\nWe will also be validating our model. Cross Validation:- Cross Validation is a technique which involves reserving a particular sample of a dataset on which we do not train the model. Later, we will test our model on this sample before finalizing it.\n\nWe will be performing K-Fold Cross Validation.Below are the steps for it:\n\nRandomly split your entire dataset into k\u201dfolds\u201d\nFor each k-fold in your dataset, build your model on k \u2013 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\nRecord the error you see on each of the predictions\nRepeat this until each of the k-folds has served as the test set\nThe average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model.\n\n    \n    \n##### Predicting the Test set results(Classification Report - Interpreting meaning of values we get in it)\nPrecision \u2013 Accuracy of positive predictions.\n\n\\begin{equation*}\nPrecision =(\\frac{TP}{TP+FP})   \n\\end{equation*}\n\nFN \u2013 False Negatives\nRecall (aka sensitivity or true positive rate): Fraction of positives That were correctly identified.\n\n\\begin{equation*}\nRecall =(\\frac{TP}{TP+FN})   \n\\end{equation*}\n\nF1 Score (aka F-Score or F-Measure) \u2013 A helpful metric for comparing two classifiers. F1 Score takes into account precision and the recall. It is created by finding the the harmonic mean of precision and recall.\n\n\\begin{equation*}\nF1 = {2}\\times(\\frac{precision\\times recall}{precision + recall})   \n\\end{equation*}\n\n\n\n\nAUC\/ROC curve:- The ROC curve is a fundamental tool for diagnostic test evaluation.\nIn a ROC curve the true positive rate (Sensitivity) is plotted in function of the false positive rate (100-Specificity) for different cut-off points of a parameter. Each point on the ROC curve represents a sensitivity\/specificity pair corresponding to a particular decision threshold. The area under the ROC curve (AUC) is a measure of how well a parameter can distinguish between two diagnostic groups (diseased\/normal).","62306a20":"## <p style=\"text-align: center;\">8. Conclusion<\/p> <a id='Conclusion'><\/a>\n1. We can say that on the basis of AIC, BIC and R^2 value for linear regression Model 1 is the best , since it has low values overall. \n2. Lower the value of AIC and BIC means better model since both measure the loss of data while modeling data, and low value denotes less data is lost overall.\n3. AQI is highly correlated with all the independent variables(so2, no2, rspm, spm and pm2_5) .\n4. AQI has been increasing over the years.\n5. As for logistic regression , only model 1 provides us with accurate results more so because AQI_Range_Binary is the dependent variable we used . But for model 2 and model 3 dependent variable is type_label(sort of area) and the accuracy results are comparitively lower so we can say that though the factors are related to type label in a way but there relations are not enough to be used as for prediction and estimation purposes.\n6. As for significant variables concerned we cannot use regular regression coefficients and p-value to calculate the same(explained above).\n7. Our dataset also contains multicollinearity, all the independent variables are somewhat related to each other as we can see in our results.\n8. After concluding that Multicollinearity do exist in our dataset, and when we try to remove highly multicollinear variables the value of R^2 drops , thus making us conclude that our dataset is not fit for both linear regression and logistic regression (because assumptions for same are violated).\n9. After stepwise regression, we conclude that the most significant variables that should be used for regression with our dependent variable are 'so2', 'spm', 'no2', 'pm2_5' (with p-value < 0.05).\n10. Regularization as such as no effect on the model, though there is a slight increase in the accuracy and cross_val_score but it is not that big that we should do it.\n11. Interaction effect for (spm*rspm) is maximum and that we have utilised and and has gotten good R^2 value","198c90db":"\\begin{equation*}\nAQI = AQI_{min} +  \\frac{PM_{Obs}-PM_{Min}}{AQI_{Max}-AQI_{Min}}{(PM_{Max}-PM_{Min})}\n\\end{equation*}","1aad66d0":"Since we already know that our dataset contains missing values , and we need to fill them for our further analysis . We will be using Imputation to fill inour missing values. Imputation is the process of replacing missing data with substituted values . Because missing data can create problems for analyzing data, imputation is seen as a way to avoid pitfalls involved with listwise deletion of cases that have missing values. ","31cce808":"##### * Is the relationship significant?   \n\nBy the results of all three models we can say , that the relationship between the dependent variable (AQI) and significant independent variables(so2,no2,si,ni,pm2_5,rpi,rspm,spm) are significant. \n\n##### * Are any model assumptions violated?    \n\nAssumptions for Linear Regression are as follow:-\n1. Linear relationship.\n2. Multivariate normality.(correlated variables do clustering around mean value)\n3. No or little multicollinearity.\n5. No auto-correlation.\n6. Homoscedasticity.(all random variables in the sequence have the same finite variance)\n\nby the results we can say that they have linear relationship and no auto correlation for multicollinearity and multivariate normality please refer to section [Multicollinearity](#Multicollinearity)\n\n##### * Does the model make sense? Interpret the meaning of each independent variable.   \n\nAll three model makes sense.And we can say as the value of each independent variable we have used increase AOI will increase.\n\n##### * Cross-validate the model. How well did it do? \n\nFor all three models cross validation gives good result, such that it is accurate.\n\n##### * Compare the AIC, BIC and adjusted R^2.  Do they agree in their ranking of the models?  \nAIC,BIC and adjusted R^2  for Linear Model 1 is the lowest, lowest and highest , hence it is the the one we should choose.","fdfc855e":"## Cross Validation of Linear Model 2","041bfa13":"The dataset consists primarily 5 different types pollutants measured over the years in different states and cities of India. Where *SO2* and *NO2* are harmful gaseous emmissions; *rspm, spm* and *pm2_5* come under susended air pollutants.","cbace57e":"The Following plot gives us our first Logistic Model along with the summary for the same.\n\n>Training Features :- 'so2', 'no2', 'rspm', 'spm', 'pm2_5', 'si', 'ni', 'rpi', 'spi', 'pmi',\n       'year', 'state_label', 'AQI_Range_Binary'\n\n>Target Feature:- 'type_label'\n","4187478e":"#### Significant Questions on the basis of the all 3 Linear Models","b9e72e00":"# <p style=\"text-align: center;\"> AQI in India <\/p>","9f9bbed1":"#### Table Overview:- For Linear Model 1 we can see that VIF for si, ni is fine but for rpi and spi it is greater than 5. So we can remove one of them and can see the results.","7952bee5":"#### Table Overview:- AQI_Range is converted to AQI_Label according to its 5 types using cat coding","2a2a6917":"### 6.5 Interaction Effect-SPM<a id='SPM'><\/a>","ad116f5b":"#    <p style=\"text-align: center;\">1.5 Exploratory Data Analysis (EDA)  <a id='eda'><\/a>\n\nExploratory data analysis (EDA) is an approach to analyzing data sets to summarize their main characteristics, often with visual methods. A statistical model can be used or not, but primarily EDA is for seeing what the data can tell us beyond the formal modeling or hypothesis testing task.","378d84ce":"## <p style=\"text-align: center;\">4. Multicollinearity<\/p> <a id='Multicollinearity'><\/a>\n\nIn statistics, multicollinearity (also collinearity) is a phenomenon in which one predictor variable in a multiple regression model can be linearly predicted from the others with a substantial degree of accuracy.\n\nWhy Multicollinearity is a problem?\n>Multicollinearity occurs when independent variables in a regression model are correlated. This correlation is a problem because independent variables should be independent. If the degree of correlation between variables is high enough, it can cause problems when you fit the model and interpret the results.\n\nVIF assesses whether factors are correlated to each other (multicollinearity)\nHigh variance inflation factor means that they \"have\" the same variance within this dataset. We would need to discard one of these variables before moving on to model building or risk building a model with high multicolinearity.\nIf the VIF is equal to 1 there is no multicollinearity among factors, but if the VIF is greater than 1, the predictors may be moderately correlated. If the VIF for a factor is near or above 5 we may have to remove highly correlated factors.","ba562f30":"## 2.2 Linear Model 2 <a id='LinearModel2'><\/a>\n\nThe Following plot gives us our second Linear Model along with the summary for the same.\n\n>Training Features :- so2, no2, pm2_5, spm\n\n>Target Feature:- AQI\n","181ae1d8":"Clearly there are lots of null values, noticeably in stn_code,agency, both of which should therefore be not ncluded further in the analysis.\n\nIntuitively, these two columns will hardly add much value to analysis.\n\nNow, focusing on the categorical variables, we are left with location_monitoring_station which consists of considerable nulls (approximately 27000).\n\n**It would have been useful to have those values for an in depth analysis, but for now we will keep it out because of the null values and come back later if needed.\n\nOut of the two dates columns, immediate attention goes to sampling date which has different formats within, highlighting some data input issues.\nWhile, it is importnat to have this metric, more useful is to go back to the origin of the dataset and ask relevant questions,as to why are there different formats? Is it a human error or error due to incorporating different formats.For now, we will keep it out and only have the date column.","ce970c23":"### 6.7 Interaction Effect-RSPM<a id='RSPM'><\/a>","01896cb9":"#### Understanding the pollutants briefly .\n\nNO2: Nitrogen Dioxide and is emmitted mostly from combustion from power sources or transport.\n\nSO2: Sulphur Dioxide and is emmitted mostly from coal burning, oil burning, manufacturing of Sulphuric acid.\n\nspm: Suspended particulate matter and are known to be the deadliest form of air pollution. They are microscopic in nature and are found to be suspended in earth's atmosphere.\n\nrspm: Respirable suspended particulate matter. A sub form of spm and are respnsible for respiratory diseases.\n\npm2_5: Suspended particulate matter with diameters less than 2.5 micrometres. They tend to remain suspended for longer durations and potentially very harmful.\n\n","1d5d38f9":"## <p style=\"text-align: center;\">6. Interaction Effect<\/p> <a id='InteractionEffect'><\/a>\nInteraction effects occur when the effect of one variable depends on the value of another variable. Interaction effects are common in regression analysis. In any study, many variables can affect the outcome. Changing these variables can affect the outcome directly. In more complex study areas, the independent variables might interact with each other. Interaction effects indicate that a third variable influences the relationship between an independent and dependent variable. This type of effect makes the model more complex, but if the real world behaves this way, it is critical to incorporate it in your model. \n\nHere we are taking 3 independant variables and seeing their individual standard error, t score and P values, and these values in presence of each other. And finally we see interaction of age and potential. The 3 independent variables are Age, International Reputation and Potential","f6e5340e":"## 3.2 Logistic Model 2 <a id='LogisticModel2'><\/a>","e55d8a44":"Correlation is any statistical association, though in common usage it most often refers to how close two variables are to having a linear relationship with each other. The correlation coefficient r measures the strength and direction of a linear relationship between two variables on a scatterplot. if r>0 higher the correlation and if r<0 correlation is inversely related.\n\n### Visual representation in form of heatmap for correlated data","641ba2b9":"#### Table Overview:- Top 5 rows of the dataset consisting of independent variables that have p-value<0.05\nAnd we will be using the following Independent Variables present in the given dataset for modeling our data using linear regression.","46fd3633":"#### Significant Questions on the basis of the all 3 Linear Models\n##### * Is there any multi-colinearity in the model?   \n\nYes there is multicollinearity in all models except for linear model 2 .(based on VIF values calculated)\n\n##### * In the multiple regression models are predictor variables independent of all the other predictor variables?  \nFor Model 1:- spi and rpi are nor exactly independent of all other predictor variables , since the calculated VIF value is greater than 5.\n\nFor Model 2:- Model seems fine, because all the independent variables are not that related since VIF<5.\n\nFor Model 3:- The worst model out of all , all the independent variables are dependent on each other highly because VIF value for all are much higher than expected.\n\n##### * In multiple regression models rank the most significant predictor variables and exclude insignificant ones from the model. \n\nThe regular\u00a0regression coefficients\u00a0that we see in statistical output describe the relationship between the independent variables and the dependent variable. The\u00a0coefficient\u00a0value\u00a0represents the\u00a0mean\u00a0change of the dependent variable given a one-unit shift in an independent variable Consequently, we might feel that we can use the absolute sizes of the\u00a0coefficients\u00a0to identify the most important variable. After all, a larger coefficient signifies a greater change in the mean of the independent variable. However, the independent variables can have dramatically different types of units, which make comparing the coefficients meaningless.\n\nCalculations for p-values include various properties of the variable, but importance is not one of them. A very small p-value does not indicate that the variable is important in a practical sense. \n\nWe have already excluded the variables in the previous answer and about ranking the significant variables. Output for stepwise regression gives us the important variables. [Stepwise Regression](#StepwiseRegression)\n\n##### * Cross-validate the models. How well did they do?   ","658889ad":"## <p style=\"text-align: center;\">11. License<\/p> <a id='License'><\/a>\n","829547f8":"### 6.2 Interaction Effect-NO2 & RSPM<a id='NO2RSPM'><\/a>","1242c77b":"#### Since we don't have any idea about how the data is distributed, and what to take as a measure of central tendency ,it is always advisable to remove outliers. Since outliers has a huge effect on mean , though it does not effect mode and median very much. And usually we use mean as the measure, so we will be removing outliers for the dataset for important columns","91aa8f9a":"#### *Now that we have a definitive dataset, that is one without null values we can employ various machine learning algorithms to see* *how are dependent and independent variable are related. And also to do Exploratory Data Analysis on the given dataset.*","fb092ec8":"## <p style=\"text-align: center;\">2. Linear Regression<\/p> <a id='LinearRegression'><\/a>\n\nLinear regression is basically a linear approach to model the relationship shared between a scalar response (or dependent variable) i.e AQI and one or more explanatory variables (or independent variables) i.e SO2,NO2 etc\n\n### Multiple Linear Regression\n\n#### The case of multiple explanatory variable (independent variable) is called multiple linear regression.\nTo build a well-performing machine learning (ML) model, it is important to seperate data into training and testing dataset . Basically we are training the model on and testing it against the data that comes from the same set of target distribution. \n\n### Simple Linear Regression\n\nThe case of single explanatory variable (independent variable) is called single linear regression.\u00b6  \n\n#### We applied linear regression model on our dataset and calculated the value for Root Mean Squared Error ,Mean Squared Error(log), AIC and BIC\n\nRoot Mean Squared Error:-\nRoot Mean Square Error (RMSE) mathematically is the standard deviation of the residuals. Residuals is the measure od how far the data points are spreaded across the line of regression which we get by our training data set. RMSE is the measure of how spread out these residuals are. In other words, it tells you how concentrated the data is around the line of best fit.\n\n##### Mean Squared Error:-\nThe Mean Squared Error (MSE) is a measure of how close a fitted line is to data points. It is the sum, over all the data points, of the square of the difference between the predicted and actual target variables, divided by the number of data points. RMSE is the square root of MSE.\n\n##### Akaike information criterion (AIC):-\nThe Akaike information criterion (AIC) is an estimator of the relative quality of statistical models for a given set of data. Given a collection of models for the data, AIC estimates the quality of each model, relative to each of the other models. Thus, AIC provides a means for model selection. When a statistical model is used to represent the process that generated the data, the representation will almost never be exact; so some information will be lost by using the model to represent the process. AIC estimates the relative amount of information lost by a given model: the less information a model loses, the higher the quality of that model.\n\nIn estimating the amount of information lost by a model, AIC deals with the trade-off between the goodness of fit of the model and the simplicity of the model. In other words, AIC deals with both the risk of overfitting and the risk of underfitting.\n>\n>\\begin{equation*}\nAIC = {2}\\cdot\\ {k} + {n}log(RSS\/N)       \n\\end{equation*}\n>\n<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows<\/p>\n\n##### Bayesian information criterion (BIC) :-\nIt is a criterion for model selection among a finite set of models; the model with the lowest BIC is preferred. It is based, in part, on the likelihood function and it is closely related to the Akaike information criterion (AIC).\n\nWhen fitting models, it is possible to increase the likelihood by adding parameters, but doing so may result in overfitting. Both BIC and AIC attempt to resolve this problem by introducing a penalty term for the number of parameters in the model; the penalty term is larger in BIC than in AIC.\n>\n\\begin{equation*}\nBIC = {n}\\cdot \\ln(RSS\/n) + {k}\\cdot\\ln(n)      \n\\end{equation*}\n>\n<p style=\"text-align: center;\">where k=number of parameters, RSS=Residual Sum of Squares and n= number of rows<\/p>\n\n \n##### P-value:-\nIn statistics, the p-value is a function of the observed sample results (a statistic) that is used for testing a statistical hypothesis. Before the test is performed, a threshold value is chosen, called the significance level of the test, traditionally 5% or 1% and denoted as $\\alpha$.\n\nIf the p-value is equal to or smaller than the significance level ($\\alpha$), it suggests that the observed data are inconsistent with the assumption that the null hypothesis is true and thus that hypothesis must be rejected (but this does not automatically mean the alternative hypothesis can be accepted as true). When the p-value is calculated correctly, such a test is guaranteed to control the Type I error rate to be no greater than $\\alpha$.\n","e91e2a71":"### Box Plot AQI Dataset Scaled","397b86ff":"### Graph Overview:- Visualization of AQI across india (Year-wise)","dd785532":"#### Function to calculate no2 individual pollutant index(ni)\nThe index category for NO2 is scaled between 0-400. So on applying formula which is used to calculate AQI","8ec5b305":"Sorted List of Ranked Predictors depending on previous function","de2c6681":"#   1.1 Abstract  <a id='abstract'><\/a>\n\nIn this Kernel , The following dataset \"India Air Quality Data\" by Shruti Bhargava has been analyzed. The main focus of this project is learning about modeling of data by supervised algorithms i.e (Linear Regression (regression) and Logistic Regression (classification)). The main focus of this particular kernel is AQI(Air Quality Index), and factors that affects AQI i.e (so2, no2, spm, rspm, pm2_5). Since in the following dataset we have the concentration of pollutants and we need each pollutants index for calculating the air quality index , so that is been calculated further in the process and has been utilised in analysis . Also in the following project there is a brief explanation of how combination of the independent variables (Interaction effect) has what impact on dependent variable and how is the accuracy of the model has been changed because of the same and how interdependence\/ correlation (Multicollinearity) between various independent variable has adverse effect on the dependent varaiable and given data model. The solution to the problems of multicollinearity is also been discussed in the following kernel i.e Regularization and Stepwise Regression. Both of which gives us an enhanced model , with better predictors and estimators in alignment with dependent variable. Furthermore, following kernel contains some EDA(Explorartory Data Analysis)E which is usually the first step in your data analysis process. We take a broader look at patterns, trends, outliers, unexpected results and so on in the dataset, using visual and quantitative methods to get a sense of the story it tells. ","00a59ed2":"#   1.2 Importing Libraries  <a id='importing_libraries'><\/a>","03bc7a23":"#### Function to calculate so2 individual pollutant index(si)\nThe index category for SO2 is scaled between 0-1600. So on applying formula which is used to calculate AQI","c2ab582f":"#### Table Overview:- AQI_Range is converted to AQI_Range_Binary according to its 5 types using custom coding. Since for logistic we require binary categorical values only so we need to use either one hot encoding or custom coding .\n","8a93c6c7":"#### Table Overview:- For Linear Model 2 we can see that VIF for so2, no2 ,spm and pm2_5 is fine. So model needs no change."}}