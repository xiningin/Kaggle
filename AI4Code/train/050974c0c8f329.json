{"cell_type":{"dfab0a9b":"code","b1ddc9af":"code","2dab549b":"code","3aefee26":"code","34a00de3":"code","16a1bd2b":"code","bea9ebd8":"code","c4205d73":"code","f4703362":"code","58e71596":"code","fa0f5187":"code","fc0ac3d0":"code","3f5a0f52":"code","a72a1d4f":"code","e099fd19":"code","5bffa415":"code","f0413a9c":"code","e18985d3":"code","2dcacf45":"code","88fc652f":"code","bafb2691":"code","91cbbee2":"code","3cb49a2b":"code","5f35a1cb":"code","0445d5db":"code","06e3d685":"code","7b3d552a":"code","6986e0e5":"code","7d5b7e5d":"code","5e9ef0ce":"code","c29d681d":"code","01493c75":"code","675347b6":"code","3727765d":"code","ef461498":"code","bddd0fae":"code","3403b131":"code","4ef89e05":"code","d7cef742":"code","f5b25e45":"code","e9906677":"code","ca8d4d8d":"code","92ee48f3":"code","76ab740b":"code","42aeb051":"code","d7bf7157":"code","c715296b":"code","b526fcd5":"code","806c8552":"code","b6042f09":"code","82f6c3e7":"code","293b6eb1":"code","014ec8c9":"code","a695296b":"code","b639f8bb":"code","302e4381":"code","f862b4fa":"code","8c718efc":"code","7763f947":"code","f4d8199f":"code","2238faa7":"code","fdc5b553":"code","42e71573":"code","8c3061df":"code","50ad50a5":"code","30798c53":"code","7a00340f":"code","b9e4ca75":"code","318b56d9":"code","060a99c5":"code","294ddcb5":"code","b4c45de9":"code","31026878":"code","a2675010":"code","4f0d949d":"code","3d0ec82f":"code","d1e0b2ee":"code","ca2e8f8f":"code","42054b6b":"code","c2551562":"code","6ac0cf11":"code","4cf2c575":"code","b2cdcbbf":"code","49740413":"code","5e1152ba":"code","6513bd17":"code","21413e4d":"code","370e80e1":"code","817c3d0a":"code","c4b60566":"markdown","175632c1":"markdown","be7061be":"markdown","5b8a837d":"markdown","a7727514":"markdown","5cbdcc16":"markdown","5ecbb7d4":"markdown","15999771":"markdown","b5ea3048":"markdown","b3100b54":"markdown","ebb0e28b":"markdown","beffce2d":"markdown","1be6a767":"markdown","e668843d":"markdown","2ff96b8c":"markdown","4a19eb40":"markdown","f278bc56":"markdown","0246f917":"markdown","4f970529":"markdown","66d088d5":"markdown","9b32f4ac":"markdown","263762b1":"markdown","9f81e830":"markdown","793ae6b9":"markdown","ce091e64":"markdown","d0584e45":"markdown","35b398ea":"markdown","c63035c3":"markdown","b94f3b95":"markdown","c7bc2468":"markdown","b591f815":"markdown","f80fa6d1":"markdown","1f8bbcb3":"markdown","f37dad1c":"markdown","1b261984":"markdown","4844a69a":"markdown","98d0604d":"markdown","e54c4834":"markdown","74a9dad2":"markdown","350cfaee":"markdown","8b930db8":"markdown","f6af6e97":"markdown","165c6b3e":"markdown","af02d37e":"markdown","0a5e474f":"markdown","61a43667":"markdown","7c97b889":"markdown","0e1f29d9":"markdown","607b2509":"markdown","31d86e2f":"markdown","94bab3da":"markdown","6febc95d":"markdown","f304e9a0":"markdown","1801c30b":"markdown","67f0d3ed":"markdown","539d2408":"markdown","bec0e6c7":"markdown","00eaa013":"markdown","c9234af5":"markdown","2a6e3454":"markdown","c4c0cb89":"markdown","450c32ab":"markdown","8133741c":"markdown","f0a181a9":"markdown","4f23171f":"markdown","4ca299f2":"markdown","68ec0e5d":"markdown","8d19a50f":"markdown","441b71ec":"markdown","f3bb96df":"markdown","da50222c":"markdown","4f3ee77d":"markdown","9364dead":"markdown","03b9a4c0":"markdown","f480fb64":"markdown","17461e0a":"markdown","f7bb26cc":"markdown","d9af1f8f":"markdown","b1f7b2e1":"markdown","1c8df6ff":"markdown"},"source":{"dfab0a9b":"import pandas as pd\nimport sklearn as sk\nimport regex as re\nimport nltk\nimport numpy as np\nimport math\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\nfrom tqdm import tqdm\nimport json\ntqdm.pandas()","b1ddc9af":"train_df = pd.read_csv('..\/input\/home-depot-product-search-relevance\/train.csv.zip', encoding='ISO-8859-1')\ntest_df = pd.read_csv('..\/input\/home-depot-product-search-relevance\/test.csv.zip', encoding='ISO-8859-1')\ndes_df = pd.read_csv('..\/input\/home-depot-product-search-relevance\/product_descriptions.csv.zip', encoding='ISO-8859-1')\nattributes_df = pd.read_csv('..\/input\/home-depot-product-search-relevance\/attributes.csv.zip', encoding=\"ISO-8859-1\")","2dab549b":"# \u0110\u1ecdc file m\u00e0u (t\u1eeb repo top 3 private leaderboard)\ncolors = None\nwith open(\"..\/input\/home-depot-spell-check\/color.json\", encoding = 'utf-8') as f:\n    colors = json.loads(f.read())\n\n# \u0110\u1ecdc file s\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 theo t\u1eeb trong dataset (l\u1ea5y t\u1eeb m\u1ed9t notebook tr\u00ean forum c\u1ee7a dataset n\u00e0y)\ndict_replace = None\nwith open(\"..\/input\/home-depot-spell-check\/dict_replace.json\", encoding = 'utf-8') as f:\n    dict_replace = json.loads(f.read())\n\n# \u0110\u1ecdc file s\u1eeda l\u1ed7i ch\u00ednh t\u1ea3 search_term theo c\u1ea3 c\u1ee5m (l\u1ea5y t\u1eeb m\u1ed9t notebook tr\u00ean forum c\u1ee7a dataset n\u00e0y)\nspell_check = None\nwith open(\"..\/input\/home-depot-spell-check\/spell_check.json\", encoding = 'utf-8') as f:\n    spell_check = json.loads(f.read())","3aefee26":"train_df.head(5)","34a00de3":"test_df.head(5)","16a1bd2b":"des_df.head(5)","bea9ebd8":"attributes_df.head(5)","c4205d73":"print('Train set:  ')\nprint(train_df.isna().sum())\nprint(train_df.describe())\nprint('----------------------')\nprint('Test set')\nprint(test_df.isna().sum())","f4703362":"print(\"S\u1ed1 l\u01b0\u1ee3ng search term unique trong t\u1eadp train: \", len(train_df.search_term.unique()))\nprint(\"S\u1ed1 l\u01b0\u1ee3ng s\u1ea3n ph\u1ea9m unique trong t\u1eadp train: \", len(train_df.product_uid.unique()))\n\nprint(\"S\u1ed1 l\u01b0\u1ee3ng search term unique trong t\u1eadp test: \", len(test_df.search_term.unique()))\nprint(\"S\u1ed1 l\u01b0\u1ee3ng s\u1ea3n ph\u1ea9m unique trong t\u1eadp test: \", len(test_df.product_uid.unique()))","58e71596":"print('S\u1ed1 l\u01b0\u1ee3ng c\u00e1c h\u00e0ng c\u00f3 search_term v\u00e0 product_uid tr\u00f9ng l\u1eb7p (Train set): ',train_df[['search_term', 'product_uid']].duplicated().sum())\nprint('S\u1ed1 l\u01b0\u1ee3ng c\u00e1c h\u00e0ng c\u00f3 search_term v\u00e0 product_uid tr\u00f9ng l\u1eb7p (Test set): ',test_df[['search_term', 'product_uid']].duplicated().sum())","fa0f5187":"print(des_df.isna().sum())\nprint(\"C\u00f3 t\u1ed5ng c\u1ed9ng\", len(des_df),\"s\u1ea3n ph\u1ea9m\")\nprint(\"C\u00f3\", des_df['product_uid'].duplicated().sum(),\"s\u1ea3n ph\u1ea9m tr\u00f9ng nhau\")","fc0ac3d0":"import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nset1 = set(train_df.product_uid.to_list())\nset2 = set(test_df.product_uid.to_list())\nvenn2([set1, set2], ('Train set', 'Test set'))\nplt.title(\"Venn Diagram c\u1ee5m s\u1ea3n ph\u1ea9m tr\u00f9ng l\u1eb7p\")\nplt.show()","3f5a0f52":"import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nset1 = set(train_df.search_term.to_list())\nset2 = set(test_df.search_term.to_list())\nvenn2([set1, set2], ('Train set', 'Test set'))\nplt.title(\"Venn Diagram c\u1ee5m search_term tr\u00f9ng l\u1eb7p\")\nplt.show()","a72a1d4f":"import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nset1 = set(train_df['search_term'].astype(str) + ' ' + train_df['product_uid'].astype(str))\nset2 = set(test_df['search_term'].astype(str) + ' ' + test_df['product_uid'].astype(str))\nvenn2([set1, set2], ('Train set', 'Test set'))\nplt.title(\"Venn Diagram c\u1ee5m search_term v\u00e0 s\u1ea3n ph\u1ea9m tr\u00f9ng l\u1eb7p\")\nplt.show() ","e099fd19":"train_df['relevance'].value_counts().sort_index(ascending=False).plot(kind='barh')","5bffa415":"fig, axs = plt.subplots(2, figsize=(10,10))\nfig.suptitle('Length of search_term')\nlens_train = train_df.progress_apply(lambda x: len(x['search_term']), axis=1)\nlens_train.hist(ax=axs[0])\nlens_test = test_df.progress_apply(lambda x: len(x['search_term']), axis=1)\nlens_test.hist(ax=axs[1])\naxs[0].set_title('Train set')\naxs[1].set_title('Test set')","f0413a9c":"fig, axs = plt.subplots(2, figsize=(10,10))\nfig.suptitle('Length of search_term')\n# lens_train = train_df.progress_apply(lambda x: len(x['search_term']), axis=1)\ntrain_df['product_uid'].hist(ax=axs[0])\n# lens_test = test_df.progress_apply(lambda x: len(x['search_term']), axis=1)\ntest_df['product_uid'].hist(ax=axs[1])\naxs[0].sharex(axs[1])\naxs[0].set_title('Train set')\naxs[1].set_title('Test set')","e18985d3":"attributes_df.isna().sum()","2dcacf45":"# C\u00f3 uid nh\u01b0ng kh\u00f4ng c\u00f3 value\nattributes_df[np.invert(attributes_df.isnull().product_uid) & attributes_df.isnull().value]\n# C\u00f3 value nh\u01b0ng ko c\u00f3 uid th\u00ec kh\u00f4ng c\u00f3 tr\u01b0\u1eddng h\u1ee3p n\u00e0o c\u1ea3","88fc652f":"# Kh\u00f4ng c\u00f3 uid\nattributes_df[attributes_df.isnull().product_uid].isnull().sum()\n# C\u00e1c tr\u01b0\u1eddng h\u1ee3p ko c\u00f3 uid th\u00ec s\u1ebd ko c\u00f3 c\u1ea3 c\u00e1c tr\u01b0\u1eddng c\u00f2n l\u1ea1i.","bafb2691":"attributes_df.product_uid.dropna().astype('int64')","91cbbee2":"import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nset1 = set(attributes_df.product_uid.dropna().astype('int64'))\nset2 = set(des_df.product_uid)\nvenn2([set1, set2], ('attribute', 'description'))\nplt.title(\"Venn Diagram c\u00e1c s\u1ea3n ph\u1ea9m trong attribute table v\u00e0 description table\")\nplt.show() ","3cb49a2b":"attributes_df.name.value_counts().plot()","5f35a1cb":"import matplotlib.pyplot as plt\nfrom matplotlib_venn import venn2\nset1 = set(attributes_df[attributes_df['name'] == 'MFG Brand Name'].product_uid.astype('int64'))\nset2 = set(des_df.product_uid)\nvenn2([set1, set2], ('attribute', 'description'))\nplt.title(\"S\u1ed1 l\u01b0\u1ee3ng s\u1ea3n ph\u1ea9m c\u00f3 brand name\")\nplt.show() ","0445d5db":"# Drop c\u1ed9t id kh\u00f4ng c\u1ea7n thi\u1ebft c\u1ee7a b\u1ea3ng train\ntrain_df = train_df.drop(['id'], axis=1)\n\n\n# Drop null value b\u1ea3ng attribute\nattributes_df = attributes_df.dropna()\n\n# B\u1ea3ng attribute l\u1ed7i c\u1ed9t uid l\u00e0 d\u1ea1ng float64 n\u00ean convert sang int64 \u0111\u1ec3 \u0111\u1ed3ng b\u1ed9 v\u1edbi b\u1ea3ng kh\u00e1c\nattributes_df = attributes_df.astype({'product_uid': 'int64'})\n\n# L\u1ea5y d\u1eef li\u1ec7u v\u1ec1 branding \u0111\u1ec3 ph\u1ee5c v\u1ee5 sau n\u00e0y (khi n\u1ed1i c\u00e1c c\u1ed9t name v\u00e0 value c\u1ee7a b\u1ea3ng attribute s\u1ebd kh\u00f4ng l\u1ea5y l\u1ea1i \u0111c c\u00e1c gi\u00e1 tr\u1ecb brand)\nbrand_temp = attributes_df[attributes_df['name'] == 'MFG Brand Name']\nbrand_temp['brand'] = brand_temp['value']\nbrand_temp = brand_temp.drop(['name', 'value'], axis=1)","06e3d685":"# N\u1ed1i 2 c\u1ed9t Name v\u00e0 value l\u1ea1i th\u00e0nh 1 c\u1ed9t l\u00e0 att_summary\nattributes_df['att_summary'] = attributes_df.progress_apply(lambda x: (x['name'] + ' ' + x['value']) ,axis=1)\n\n# B\u1ecf 2 c\u1ed9t c\u0169 (sau khi \u0111\u00e3 n\u1ed1i xong)\nattributes_df = attributes_df.drop(labels=['name', 'value'], axis=1)\n\n# T\u1ea1o b\u1ea3ng m\u1edbi (pivot table) c\u00f3 index l\u00e0 product_uid v\u00e0 v\u1edbi c\u00e1c h\u00e0ng c\u00f3 c\u00f9ng product_uid s\u1ebd ti\u1ebfn h\u00e0nh join ch\u00fang l\u1ea1i th\u00e0nh 1.\nattributes_df = pd.pivot_table(attributes_df, index='product_uid', aggfunc= lambda x : \" \".join(x))","7b3d552a":"attributes_df","6986e0e5":"train_df = train_df.merge(des_df, on='product_uid', how='left')\n\ntest_df = test_df.merge(des_df, on='product_uid', how='left')\n\ntrain_df\n","7d5b7e5d":"train_df = train_df.merge(attributes_df, on='product_uid', how='left')\ntest_df = test_df.merge(attributes_df, on='product_uid', how='left')\ntrain_df","5e9ef0ce":"train_df = train_df.merge(brand_temp, on='product_uid', how='left')\ntest_df = test_df.merge(brand_temp, on='product_uid', how='left')\ntrain_df","c29d681d":"train_df['att_summary'] = train_df['att_summary'].fillna('NoAttribute')\ntrain_df['brand'] = train_df['brand'].fillna('NoBrand')\ntrain_df.isna().sum()\n\ntest_df['att_summary'] = test_df['att_summary'].fillna('NoAttribute')\ntest_df['brand'] = test_df['brand'].fillna('NoBrand')","01493c75":"def search_term_correction(s):\n    # Ki\u1ec3m tra c\u1ee5m search_term c\u00f3 trong danh s\u00e1ch spell check hay kh\u00f4ng, n\u1ebfu c\u00f3 th\u00ec s\u1eeda l\u1ea1i\n    if s in spell_check.keys():\n        s = spell_check[s]\n    s = s.lower().strip().split(' ')\n    \n    # Ki\u1ec3m tra c\u00e1c word c\u1ee7a search_term c\u00f3 trong file dict_replace hay kh\u00f4ng, \n    # n\u1ebfu c\u00f3 th\u00ec s\u1eeda l\u1ea1i sau \u0111\u00f3 join list l\u1ea1i th\u00e0nh string ban \u0111\u1ea7u\n    for index, i in enumerate(s):\n        if i in dict_replace.keys():\n            s[index] = dict_replace[i]\n    s = ' '.join(s)\n    return s\n\ntrain_df['search_term'] = train_df.progress_apply(lambda x: search_term_correction(x['search_term']), axis=1)\n\ntest_df['search_term'] = test_df.progress_apply(lambda x: search_term_correction(x['search_term']), axis=1)","675347b6":"print(\"Test Regex [^A-Za-z0-9]+ for this string 90\u00c2\u00b0 : \" + re.sub('[^A-Za-z0-9]+', ' ', ' 90\u00c2\u00b0'))\ndef remove_special_char(row):\n    row['product_title'] = re.sub('[^A-Za-z0-9]+', ' ', row['product_title'])\n    row['search_term'] = re.sub('[^A-Za-z0-9]+', ' ', row['search_term'])\n    row['product_description'] = re.sub('[^A-Za-z0-9]+', ' ', row['product_description'])\n    row['att_summary'] = re.sub('[^A-Za-z0-9]+', ' ', row['att_summary'])\n\n    row['product_title'] = row['product_title'].lower()\n    row['search_term'] = row['search_term'].lower()\n    row['product_description'] = row['product_description'].lower()\n    row['att_summary'] = row['att_summary'].lower()\n    return row\ntrain_df = train_df.progress_apply(remove_special_char, axis=1)\n\ntest_df = test_df.progress_apply(remove_special_char, axis=1)","3727765d":"def measurement_standardize(s):\n    s = re.sub(r\"([0-9])( *)\\.( *)([0-9])\", r\"\\1.\\4\", s)\n    s = re.sub(r\"([0-9]+)( *)(inches|inch|in|')\\.?\", r\"\\1 in. \", s)\n    s = re.sub(r\"([0-9]+)( *)(foot|feet|ft|'')\\.?\", r\"\\1 ft. \", s)\n    s = re.sub(r\"([0-9]+)( *)(pounds|pound|lbs|lb)\\.?\", r\"\\1 lb. \", s)\n    s = re.sub(r\"([0-9]+)( *)(square|sq) ?\\.?(feet|foot|ft)\\.?\", r\"\\1 sq.ft. \", s)\n    s = re.sub(r\"([0-9]+)( *)(cubic|cu) ?\\.?(feet|foot|ft)\\.?\", r\"\\1 cu.ft. \", s)\n    s = re.sub(r\"([0-9]+)( *)(gallons|gallon|gal)\\.?\", r\"\\1 gal. \", s)\n    s = re.sub(r\"([0-9]+)( *)(ounces|ounce|oz)\\.?\", r\"\\1 oz. \", s)\n    s = re.sub(r\"([0-9]+)( *)(centimeters|cm)\\.?\", r\"\\1 cm. \", s)\n    s = re.sub(r\"([0-9]+)( *)(milimeters|mm)\\.?\", r\"\\1 mm. \", s)\n    s = s.replace(\"\u00b0\",\" degrees \")\n    s = re.sub(r\"([0-9]+)( *)(degrees|degree)\\.?\", r\"\\1 deg. \", s)\n    s = s.replace(\" v \",\" volts \")\n    s = re.sub(r\"([0-9]+)( *)(volts|volt)\\.?\", r\"\\1 volt. \", s)\n    s = re.sub(r\"([0-9]+)( *)(watts|watt)\\.?\", r\"\\1 watt. \", s)\n    s = re.sub(r\"([0-9]+)( *)(amperes|ampere|amps|amp)\\.?\", r\"\\1 amp. \", s)\n    s = re.sub(' +', \" \", s)\n    return s\ntrain_df['product_title'] = train_df.progress_apply(lambda x: measurement_standardize(x['product_title']), axis=1)\ntrain_df['search_term'] = train_df.progress_apply(lambda x: measurement_standardize(x['search_term']), axis=1)\ntrain_df['att_summary'] = train_df.progress_apply(lambda x: measurement_standardize(x['att_summary']), axis=1)\ntrain_df['product_description'] = train_df.progress_apply(lambda x: measurement_standardize(x['product_description']), axis=1)\n\ntest_df['product_title'] = test_df.progress_apply(lambda x: measurement_standardize(x['product_title']), axis=1)\ntest_df['search_term'] = test_df.progress_apply(lambda x: measurement_standardize(x['search_term']), axis=1)\ntest_df['att_summary'] = test_df.progress_apply(lambda x: measurement_standardize(x['att_summary']), axis=1)\ntest_df['product_description'] = test_df.progress_apply(lambda x: measurement_standardize(x['product_description']), axis=1)\n","ef461498":"# train_df.to_csv('w2veccorpus.csv', encoding='utf-8')","bddd0fae":"train_df.to_csv('processed_train_df.csv', encoding='utf-8')\ntest_df.to_csv('processed_test_df.csv', encoding='utf-8')","3403b131":"train_df = pd.read_csv('.\/processed_train_df.csv', encoding='utf-8')\ntrain_df = train_df.drop(['Unnamed: 0'], axis=1)\ntest_df = pd.read_csv('.\/processed_test_df.csv', encoding='utf-8')\ntest_df = test_df.drop(['Unnamed: 0', 'id'], axis=1)","4ef89e05":"train_df['query_length'] = train_df.progress_apply(lambda x: len(x['search_term']), axis=1)\ntest_df['query_length'] = test_df.progress_apply(lambda x: len(x['search_term']), axis=1)","d7cef742":"train_df['last_word_in_title'] = train_df.progress_apply(lambda x: x['search_term'].split()[-1] in x['product_title'].split(), axis=1)\n\ntest_df['last_word_in_title'] = test_df.progress_apply(lambda x: x['search_term'].split()[-1] in x['product_title'].split(), axis=1)","f5b25e45":"train_df['last_word_in_des'] = train_df.progress_apply(lambda x: x['search_term'].split()[-1] in x['product_description'].split(), axis=1)\n\ntest_df['last_word_in_des'] = test_df.progress_apply(lambda x: x['search_term'].split()[-1] in x['product_description'].split(), axis=1)","e9906677":"# NoBrand\n\ntrain_df['have_brand'] = train_df.progress_apply(lambda x: x['brand'] != 'NoBrand', axis=1)\n\ntest_df['have_brand'] = test_df.progress_apply(lambda x: x['brand'] != 'NoBrand', axis=1)","ca8d4d8d":"# noattribute\ntrain_df['have_att'] = train_df.progress_apply(lambda x: x['att_summary'] != 'noattribute', axis=1)\n\ntest_df['have_att'] = test_df.progress_apply(lambda x: x['att_summary'] != 'noattribute', axis=1)","92ee48f3":"def count_number_common(sent1, sent2):\n    search_term_tokens = sent1.split(' ')\n    # Check t\u1eebng word trong search_term, v\u00e0 attribute, \n    # n\u1ebfu l\u00e0 s\u1ed1 th\u00ec \u0111\u01b0a v\u00e0o set v\u00e0 l\u1ea5y s\u1ed1 ph\u1ea7n t\u1eed chung ( & ) c\u1ee7a 2 set \u0111\u00f3\n    numbers_search_term = set([i for i in search_term_tokens if i.isdigit()])\n    att_tokens = sent2.split(' ')\n    numbers_att = set([i for i in att_tokens if i.isdigit()])\n    return len(numbers_search_term & numbers_att)\n\ntrain_df['number_in_att'] = train_df.progress_apply(lambda x: count_number_common(x['search_term'], x['att_summary']), axis=1)\n\ntest_df['number_in_att'] = test_df.progress_apply(lambda x: count_number_common(x['search_term'], x['att_summary']), axis=1)","76ab740b":"train_df['number_in_title'] = train_df.progress_apply(lambda x: count_number_common(x['search_term'], x['product_title']), axis=1)\n\ntest_df['number_in_title'] = test_df.progress_apply(lambda x: count_number_common(x['search_term'], x['product_title']), axis=1)","42aeb051":"train_df['number_in_des'] = train_df.progress_apply(lambda x: count_number_common(x['search_term'], x['product_description']), axis=1)\n\ntest_df['number_in_des'] = test_df.progress_apply(lambda x: count_number_common(x['search_term'], x['product_description']), axis=1)","d7bf7157":"from nltk import ngrams\nfrom nltk.metrics import edit_distance\n\ndef common_words_count_ngram(sent1, sent2, ngram=1, edit_distance_thresshold=0):\n    sent1 = sent1.split(' ')\n    sent2 = sent2.split(' ')\n    ngrams1 = set(ngrams(sent1, ngram))\n    ngrams2 = set(ngrams(sent2, ngram))\n    # Sau khi c\u00f3 list 1 gram or bigram c\u1ee7a 2 c\u00e2u th\u00ec for loop qua t\u1eebng pair c\u1ee7a 2 c\u00e2u\n    # N\u1ebfu edit distance c\u1ee7a 2 ph\u1ea7n t\u1eeb nh\u1ecf h\u01a1n ng\u01b0\u1ee1ng n\u00e0o \u0111\u00f3 (default = 0) th\u00ec common words t\u0103ng l\u00ean 1\n    if edit_distance_thresshold == 0:\n        return len(ngrams1 & ngrams2)\n    else:\n        count = 0\n        for i in ngrams1:\n            for j in ngrams2:\n                if edit_distance(i, j) <= edit_distance_thresshold:\n                    count+=1\n        return count\na = 'This is a test'\nb = 'abc is a abc a test'\ncommon_words_count_ngram(a,b, ngram=2)","c715296b":"train_df['title_common_words_count'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_title'], edit_distance_thresshold=1), axis=1)\ntrain_df['des_common_words_count'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_description'], edit_distance_thresshold=1), axis=1)\ntrain_df['att_common_words_count'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['att_summary'], edit_distance_thresshold=1), axis=1)\n\ntest_df['title_common_words_count'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_title'], edit_distance_thresshold=1), axis=1)\ntest_df['des_common_words_count'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_description'], edit_distance_thresshold=1), axis=1)\ntest_df['att_common_words_count'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['att_summary'], edit_distance_thresshold=1), axis=1)","b526fcd5":"train_df['title_common_words_count_2gram'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_title'], ngram=2, edit_distance_thresshold=4), axis=1)\ntrain_df['des_common_words_count_2gram'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_description'], ngram=2,edit_distance_thresshold=4), axis=1)\ntrain_df['att_common_words_count_2gram'] = train_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['att_summary'], ngram=2,edit_distance_thresshold=4), axis=1)\n\ntest_df['title_common_words_count_2gram'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_title'], ngram=2, edit_distance_thresshold=4), axis=1)\ntest_df['des_common_words_count_2gram'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['product_description'], ngram=2,edit_distance_thresshold=4), axis=1)\ntest_df['att_common_words_count_2gram'] = test_df.progress_apply(lambda x: common_words_count_ngram(x['search_term'], x['att_summary'], ngram=2,edit_distance_thresshold=4), axis=1)","806c8552":"def cosine_similarity_sent(sent1, sent2):\n    set1 = set(sent1.split())\n    set2 = set(sent2.split())\n    numerator = len(set1 & set2)\n    denominator = math.sqrt(len(set1)) * math.sqrt(len(set2))\n    \n    if not denominator:\n        return 0.0\n    else:\n        return numerator \/ denominator\n\ntrain_df['cosine_ST'] = train_df.progress_apply(lambda row: cosine_similarity_sent(row['search_term'], row['product_title']), axis=1) \ntrain_df['cosine_SD'] = train_df.progress_apply(lambda row: cosine_similarity_sent(row['search_term'], row['product_description']), axis=1)\n# train_df['cosine_SB'] = train_df.apply(lambda row: cosine_similarity_sent(row['corrected_search'], row['brand']), axis=1)\n\ntest_df['cosine_ST'] = test_df.progress_apply(lambda row: cosine_similarity_sent(row['search_term'], row['product_title']), axis=1) \ntest_df['cosine_SD'] = test_df.progress_apply(lambda row: cosine_similarity_sent(row['search_term'], row['product_description']), axis=1)","b6042f09":"def have_color(sent1, sent2):\n    common_word = set(sent1.strip().split(' ')) & set(sent2.strip().split(' '))\n    for i in common_word:\n        if i in colors:\n            return True\n    return False\n\ntrain_df['color_st_title'] = train_df.progress_apply(lambda x: have_color(x['search_term'], x['product_title']), axis=1)\ntrain_df['color_st_des'] = train_df.progress_apply(lambda x: have_color(x['search_term'], x['product_description']), axis=1)\ntrain_df['color_st_att'] = train_df.progress_apply(lambda x: have_color(x['search_term'], x['att_summary']), axis=1)\n\ntest_df['color_st_title'] = test_df.progress_apply(lambda x: have_color(x['search_term'], x['product_title']), axis=1)\ntest_df['color_st_des'] = test_df.progress_apply(lambda x: have_color(x['search_term'], x['product_description']), axis=1)\ntest_df['color_st_att'] = test_df.progress_apply(lambda x: have_color(x['search_term'], x['att_summary']), axis=1)\n","82f6c3e7":"from nltk.metrics import edit_distance\ndef includeBrand(search_term, brand, edit_dis_thresshold = 1):\n    search_term = search_term.split(' ')\n    brand = brand.split(' ')\n    count = 0\n    for i in search_term:\n        for j in brand:\n            if edit_distance(i, j) < edit_dis_thresshold:\n                count+=1\n    return count\n    \ntrain_df['search_term_include_brand'] = train_df.progress_apply(lambda x: includeBrand(x['search_term'], x['brand']), axis=1)\n\ntest_df['search_term_include_brand'] = test_df.progress_apply(lambda x: includeBrand(x['search_term'], x['brand']), axis=1)","293b6eb1":"from nltk.metrics.distance import edit_distance\nfrom nltk.metrics.distance import jaccard_distance\nfrom nltk.metrics.distance import edit_distance\nfrom nltk.metrics.distance import jaccard_distance\n\ndef min_jaccard_coef(sent1, sent2):\n    sent1 = sent1.strip().split(' ')\n    sent2 = sent2.strip().split(' ')\n    min_dis = 99999\n    for i in sent1:\n        for j in sent2:\n            min_dis = min(jaccard_distance(set(i), set(j)), min_dis)\n    return min_dis\n\ndef min_edit_distance(sent1, sent2):\n    sent1 = sent1.split(' ')\n    sent2 = sent2.split(' ')\n    min_dis = 99999\n    for i in sent1:\n        for j in sent2:\n            min_dis = min(edit_distance(i, j), min_dis)\n    return min_dis\n\ntrain_df['min_jaccard_brand_with_search_term'] = train_df.progress_apply(lambda x: min_jaccard_coef(x['search_term'], x['brand']), axis=1)\ntrain_df['min_edit_brand_with_search_term'] = train_df.progress_apply(lambda x: min_edit_distance(x['search_term'], x['brand']), axis=1)\n\ntest_df['min_jaccard_brand_with_search_term'] = test_df.progress_apply(lambda x: min_jaccard_coef(x['search_term'], x['brand']), axis=1)\ntest_df['min_edit_brand_with_search_term'] = test_df.progress_apply(lambda x: min_edit_distance(x['search_term'], x['brand']), axis=1)","014ec8c9":"train_df['min_jaccard_title_with_search_term'] = train_df.progress_apply(lambda x: min_jaccard_coef(x['search_term'], x['product_title']), axis=1)\ntrain_df['min_edit_title_with_search_term'] = train_df.progress_apply(lambda x: min_edit_distance(x['search_term'], x['product_title']), axis=1)\n\ntest_df['min_jaccard_title_with_search_term'] = test_df.progress_apply(lambda x: min_jaccard_coef(x['search_term'], x['product_title']), axis=1)\ntest_df['min_edit_title_with_search_term'] = test_df.progress_apply(lambda x: min_edit_distance(x['search_term'], x['product_title']), axis=1)","a695296b":"def getMeanJaccardDistanceSearchTermAndTitle(row):\n    search_term_word = row['search_term'].split(' ')\n    des_word = row['product_title'].split(' ')\n    mean = 0.0\n    # Loop qua t\u1eebng ph\u1ea7n t\u1eed trong search_term \u0111\u1ec3 t\u00ecm ph\u1ea7n t\u1eeb c\u00f3 Jaccard Coef nh\u1ecf nh\u1ea5t trong description\n    for i in search_term_word:\n        if i == '':\n            continue\n        min_ = 99999\n        for j in des_word:\n            if j == '':\n                continue\n            min_ = min(min_, jaccard_distance(set(i), set(j)))\n        mean+=min_\n    mean = mean\/len(search_term_word)\n    return mean\n\ntrain_df['mean_jaccard_product_title_with_search_term'] = train_df.progress_apply(getMeanJaccardDistanceSearchTermAndTitle, axis=1)\n\ntest_df['mean_jaccard_product_title_with_search_term'] = test_df.progress_apply(getMeanJaccardDistanceSearchTermAndTitle, axis=1)","b639f8bb":"def sum_jaccard_coef(sent1, sent2):\n    return jaccard_distance(set(sent1.strip().split(' ')), set(sent2.strip().split(' ')))\ndef sum_edit_distance(sent1, sent2):\n    return edit_distance(sent1, sent2)\n    \ntrain_df['sum_jaccard_product_title_with_search_term'] = train_df.progress_apply(lambda x: sum_jaccard_coef(x['search_term'], x['product_title']), axis=1)\ntrain_df['sum_edit_product_title_with_search_term'] = train_df.progress_apply(lambda x: sum_edit_distance(x['search_term'], x['product_title']), axis=1)\n\ntest_df['sum_jaccard_product_title_with_search_term'] = test_df.progress_apply(lambda x: sum_jaccard_coef(x['search_term'], x['product_title']), axis=1)\ntest_df['sum_edit_product_title_with_search_term'] = test_df.progress_apply(lambda x: sum_edit_distance(x['search_term'], x['product_title']), axis=1)","302e4381":"train_df['sum_jaccard_product_description_with_search_term'] = train_df.progress_apply(lambda x: sum_jaccard_coef(x['search_term'], x['product_description']), axis=1)\n# train_df['sum_edit_product_description_with_search_term'] = train_df.progress_apply(getSumEditDistanceSearchTermAndDes, axis=1)\n\ntest_df['sum_jaccard_product_description_with_search_term'] = test_df.progress_apply(lambda x: sum_jaccard_coef(x['search_term'], x['product_description']), axis=1)","f862b4fa":"def getMeanJaccardDistanceSearchTermAndDes(row):\n    search_term_word = row['search_term'].split(' ')\n    des_word = row['product_description'].split(' ')\n    mean = 0.0\n    for i in search_term_word:\n        if i == '':\n            continue\n        min_ = 99999\n        for j in des_word:\n            if j == '':\n                continue\n            min_ = min(min_, jaccard_distance(set(i), set(j)))\n        mean+=min_\n    mean = mean\/len(search_term_word)\n    return mean\n\ntrain_df['mean_jaccard_product_des_with_search_term'] = train_df.progress_apply(getMeanJaccardDistanceSearchTermAndDes, axis=1)\n\ntest_df['mean_jaccard_product_des_with_search_term'] = test_df.progress_apply(getMeanJaccardDistanceSearchTermAndDes, axis=1)","8c718efc":"def getMeanJaccardDistanceSearchTermAndAtt(row):\n    search_term_word = row['search_term'].split(' ')\n    des_word = row['att_summary'].split(' ')\n    mean = 0.0\n    for i in search_term_word:\n        if i == '':\n            continue\n        min_ = 99999\n        for j in des_word:\n            if j == '':\n                continue\n            min_ = min(min_, jaccard_distance(set(i), set(j)))\n        mean+=min_\n    mean = mean\/len(search_term_word)\n    return mean\n\ntrain_df['mean_jaccard_att_with_search_term'] = train_df.progress_apply(getMeanJaccardDistanceSearchTermAndAtt, axis=1)\n\ntest_df['mean_jaccard_att_with_search_term'] = test_df.progress_apply(getMeanJaccardDistanceSearchTermAndAtt, axis=1)","7763f947":"# T\u1ea1o corpus t\u1eeb ch\u00ednh dataset \u0111ang s\u1eed d\u1ee5ng trong competition c\u1ee5 th\u1ec3 l\u00e0 c\u00e1c c\u1ed9t description, title, search_term\ntemp = train_df['product_description']\ntemp = np.concatenate((temp, test_df['product_description']), axis=0)\ntemp = np.unique(temp)\nsentences = [i.split(' ') for i in temp]\ntemp = train_df['search_term']\ntemp = np.concatenate((temp, test_df['search_term']), axis=0)\ntemp = np.unique(temp)\ntemp = [i.split(' ') for i in temp]\nsentences.extend(temp)\ntemp = train_df['product_title']\ntemp = np.concatenate((temp, test_df['product_title']), axis=0)\ntemp = np.unique(temp)\ntemp = [i.split(' ') for i in temp]\nsentences.extend(temp)\nlen(sentences)","f4d8199f":"!pip install -U gensim","2238faa7":"from gensim.test.utils import common_texts, get_tmpfile\nfrom gensim.models import Word2Vec\nfrom gensim.models.callbacks import CallbackAny2Vec\n\n# B\u1edfi v\u00ec loss do gensim t\u00ednh l\u00e0 cummulative loss (t\u1ed5ng loss t\u1eeb epoch \u0111\u1ea7u \u0111\u1ebfn epoch \u0111ang x\u00e9t) n\u00ean c\u1ea7n t\u00ednh = c\u00e1ch\n# l\u1ea5y loss epoch sau tr\u1eeb loss epoch tr\u01b0\u1edbc \u0111\u00f3.\nclass callback(CallbackAny2Vec):\n    '''Callback to print loss after each epoch.'''\n    def __init__(self):\n        self.epoch = 0\n        self.loss_to_be_subed = 0\n    \n    def on_epoch_end(self, model):\n        loss = model.get_latest_training_loss()\n        loss_now = loss - self.loss_to_be_subed\n        self.loss_to_be_subed = loss\n        print('Loss after epoch {}: {}'.format(self.epoch, loss_now))\n        self.epoch += 1\n        \n#Train t\u1ed5ng c\u1ed9ng 20 epochs v\u1edbi vector dimension l\u00e0 100 v\u00e0 window l\u00e0 5\nmodel = Word2Vec(sentences=sentences, vector_size=100, window=5, workers=4, min_count=1, \n                 compute_loss=True, callbacks=[callback()], epochs=20)\nmodel.save('w2v_homedepot_100_2.model')","fdc5b553":"# Load Pretrained Word2Vec Model\nmodel = Word2Vec.load(\"w2v_homedepot_100_2.model\")\nvector1 = model.wv['computer']  # get numpy vector of a word\nvector2 = model.wv['fax']\nsims = model.wv.most_similar('computer', topn=10)  # get other similar words\nsims","42e71573":"from sklearn.metrics.pairwise import cosine_similarity\ncosine_similarity([vector1], [vector2]).max(axis=1).shape[0]","8c3061df":"def getW2VSearchTermMaxSimScoreTitle(row):\n    title = row['product_title'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    title_vec =  [model.wv[title_].tolist() for title_ in title if title_ in model.wv]\n    search_term_vec = [model.wv[search_term_].tolist() for search_term_ in search_term if search_term_ in model.wv]\n    mean = 0.0\n    if len(search_term_vec) == 0 or len(title_vec) == 0: return 0\n    score_matrix = cosine_similarity(search_term_vec, title_vec)\n    score_matrix = score_matrix.max(axis = 1)\n    assert(score_matrix.shape[0] == len(search_term_vec)) \n    mean = score_matrix.sum()\/len(search_term_vec)\n    return mean\ntrain_df['max_cosine_sim_per_word_st_pt'] = train_df.progress_apply(getW2VSearchTermMaxSimScoreTitle, axis=1)\n\ntest_df['max_cosine_sim_per_word_st_pt'] = test_df.progress_apply(getW2VSearchTermMaxSimScoreTitle, axis=1)\n","50ad50a5":"def getW2VSearchTermMaxSimScoreDes(row):\n    title = row['product_description'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    \n    # L\u1ea5y list c\u00e1c vector c\u1ee7a c\u00e1c t\u1eeb trong title n\u1ebfu ch\u00fang c\u00f3 trong model\n    title_vec =  [model.wv[title_].tolist() for title_ in title if title_ in model.wv]\n    \n    # L\u1ea5y list c\u00e1c vector c\u1ee7a c\u00e1c t\u1eeb trong search_term n\u1ebfu ch\u00fang c\u00f3 trong model\n    search_term_vec = [model.wv[search_term_].tolist() for search_term_ in search_term if search_term_ in model.wv]\n    \n    mean = 0.0\n    # N\u1ebfu nh\u01b0 kh\u00f4ng c\u00f3 word n\u00e0o t\u1ed3n t\u1ea1i trong word2vec list c\u1ee7a model trong search_term ho\u1eb7c title th\u00ec return 0\n    if len(search_term_vec) == 0 or len(title_vec) == 0: return 0\n    \n    # T\u00ednh cosine sim theo d\u1ea1ng ma tr\u1eadn\n    score_matrix = cosine_similarity(search_term_vec, title_vec)\n    # L\u1ea5y gi\u00e1 tr\u1ecb l\u1edbn nh\u1ea5t c\u1ee7a m\u1ed7i h\u00e0ng \u1ee9ng v\u1edbi cosine sim l\u1edbn nh\u1ea5t c\u1ee7a 1 word trong search_term v\u1edbi 1 t\u1eeb n\u00e0o \u0111\u00f3 trong title\n    score_matrix = score_matrix.max(axis = 1)\n    assert(score_matrix.shape[0] == len(search_term_vec)) \n    mean = score_matrix.sum()\/len(search_term_vec)\n    return mean\ntrain_df['max_cosine_sim_per_word_st_pd'] = train_df.progress_apply(getW2VSearchTermMaxSimScoreDes, axis=1)\n\ntest_df['max_cosine_sim_per_word_st_pd'] = test_df.progress_apply(getW2VSearchTermMaxSimScoreDes, axis=1)\n","30798c53":"def w2v_n_sim_st_pt(row):\n    title = row['product_title'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    title = [i for i in title if i in model.wv]\n    search_term = [i for i in search_term if i in model.wv]\n    if not len(title) or not len(search_term): return 0\n    return model.wv.n_similarity(title, search_term)\n\ntrain_df['w2v_n_similarity_st_pt'] = train_df.progress_apply(w2v_n_sim_st_pt, axis=1)\n\ntest_df['w2v_n_similarity_st_pt'] = test_df.progress_apply(w2v_n_sim_st_pt, axis=1)\n","7a00340f":"def w2v_n_sim_st_pd(row):\n    description = row['product_description'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    description = [i for i in description if i in model.wv]\n    search_term = [i for i in search_term if i in model.wv]\n    if not len(description) or not len(search_term): return 0\n    return model.wv.n_similarity(description, search_term)\n\ntrain_df['w2v_n_similarity_st_pd'] = train_df.progress_apply(w2v_n_sim_st_pd, axis=1)\n\ntest_df['w2v_n_similarity_st_pd'] = test_df.progress_apply(w2v_n_sim_st_pd, axis=1)\n","b9e4ca75":"from sklearn.metrics import mean_squared_error\ndef w2v_centroid_rmse_st_pd(row):\n    description = row['product_description'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    description = [model.wv[i] for i in description if i in model.wv]\n    search_term = [model.wv[i] for i in search_term if i in model.wv]\n    if not len(description) or not len(search_term): return 0\n    mean_des = np.mean(description, axis=0)\n    mean_ser = np.mean(search_term, axis=0)\n    return mean_squared_error(mean_ser, mean_des)\n\ntrain_df['w2v_centroid_rmse_st_pd'] = train_df.progress_apply(w2v_centroid_rmse_st_pd, axis=1)\n\ntest_df['w2v_centroid_rmse_st_pd'] = test_df.progress_apply(w2v_centroid_rmse_st_pd, axis=1)\n","318b56d9":"from sklearn.metrics import mean_squared_error\ndef w2v_centroid_rmse_st_pt(row):\n    description = row['product_title'].strip().split(' ')\n    search_term = row['search_term'].strip().split(' ')\n    description = [model.wv[i] for i in description if i in model.wv]\n    search_term = [model.wv[i] for i in search_term if i in model.wv]\n    if not len(description) or not len(search_term): return 0\n    mean_des = np.mean(description, axis=0)\n    mean_ser = np.mean(search_term, axis=0)\n    return mean_squared_error(mean_ser, mean_des)\n\ntrain_df['w2v_centroid_rmse_st_pt'] = train_df.progress_apply(w2v_centroid_rmse_st_pt, axis=1)\n\ntest_df['w2v_centroid_rmse_st_pt'] = test_df.progress_apply(w2v_centroid_rmse_st_pt, axis=1)\n","060a99c5":"train_df.corr()","294ddcb5":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\ntitle_desc_train = train_df[\"product_title\"].astype(str) + '. ' + train_df[\"product_description\"].astype(str)\ntitle_desc_test = test_df[\"product_title\"].astype(str) + '. ' + test_df[\"product_description\"].astype(str)\ntitle_desc_all =  title_desc_train.append(title_desc_test)\ntitle_desc_unique = title_desc_all.unique()\nvectorizer = TfidfVectorizer(smooth_idf=True, min_df=2, max_features=10000, stop_words='english')\n\nvectorizer.fit(title_desc_unique)\n\nX_title_desc_all = vectorizer.transform(title_desc_all)\n\nX_title_desc_train = vectorizer.transform(title_desc_train)\nprint(X_title_desc_train.shape) # check shape of the document-term matrix\n\nX_title_desc_test = vectorizer.transform(title_desc_test)\n\n# SVD represent documents and terms in vectors \nsvd_model = TruncatedSVD(n_components=500, algorithm='randomized', n_iter=10, random_state=122)\nsvd_model.fit(X_title_desc_all)\n\ntruncated_title_desc_train = svd_model.transform(X_title_desc_train)\n\ntruncated_title_desc_test = svd_model.transform(X_title_desc_test)\n\nprint(truncated_title_desc_train.shape)\ntsvd_var_ratios = svd_model.explained_variance_ratio_\nprint('variance explained', tsvd_var_ratios.sum())\n\nX_search = vectorizer.transform(train_df['search_term'])\nprint(X_search.shape) # check shape of the document-term matrix\n\ntransformed_search_train = svd_model.transform(X_search)\n\nX_search = vectorizer.transform(test_df['search_term'])\nprint(X_search.shape) # check shape of the document-term matrix\n\ntransformed_search_test = svd_model.transform(X_search)","b4c45de9":"from sklearn.metrics.pairwise import cosine_similarity\n# T\u00ednh cosine similarity c\u1ee7a t\u1eebng c\u1eb7p search_term v\u00e0 (title+des) (sentence level) sau khi \u0111\u00e3 truncated\ncosine_similarity_tfidf_search_title_des = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_train, truncated_title_desc_train)]\ntrain_df['cosine_similarity_tfidf_search_title_des'] = [i[0][0] for i in cosine_similarity_tfidf_search_title_des]\n\ncosine_similarity_tfidf_search_title_des = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_test, truncated_title_desc_test)]\ntest_df['cosine_similarity_tfidf_search_title_des'] = [i[0][0] for i in cosine_similarity_tfidf_search_title_des]","31026878":"truncated_pt_train = svd_model.transform(vectorizer.transform(train_df[\"product_title\"].astype(str)))\ntruncated_pt_test = svd_model.transform(vectorizer.transform(test_df[\"product_title\"].astype(str)))\n# T\u00ednh cosine similarity c\u1ee7a t\u1eebng c\u1eb7p search_term v\u00e0 title (sentence level) sau khi \u0111\u00e3 truncated\ncosine_similarity_tfidf_search_title = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_train, truncated_pt_train)]\ntrain_df['cosine_similarity_tfidf_search_title'] = [i[0][0] for i in cosine_similarity_tfidf_search_title]\ncosine_similarity_tfidf_search_title = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_test, truncated_pt_test)]\ntest_df['cosine_similarity_tfidf_search_title'] = [i[0][0] for i in cosine_similarity_tfidf_search_title]\n\ntruncated_pd_train = svd_model.transform(vectorizer.transform(train_df[\"product_description\"].astype(str)))\ntruncated_pd_test = svd_model.transform(vectorizer.transform(test_df[\"product_description\"].astype(str)))\n\n# T\u00ednh cosine similarity c\u1ee7a t\u1eebng c\u1eb7p search_term v\u00e0 des (sentence level) sau khi \u0111\u00e3 truncated\ncosine_similarity_tfidf_search_des = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_train, truncated_pd_train)]\ntrain_df['cosine_similarity_tfidf_search_des'] = [i[0][0] for i in cosine_similarity_tfidf_search_des]\ncosine_similarity_tfidf_search_des = [cosine_similarity([x], [y]) for x,y in zip(transformed_search_test, truncated_pd_test)]\ntest_df['cosine_similarity_tfidf_search_des'] = [i[0][0] for i in cosine_similarity_tfidf_search_des]","a2675010":"def bm25_fit(corpus):\n    tfidf_model = TfidfVectorizer(smooth_idf=False, token_pattern=r\"(?u)\\b\\w+\\b\")\n    tfidf_model.fit(corpus)\n    idf_dict = dict(zip(tfidf_model.get_feature_names(), list(tfidf_model.idf_)))\n    avgdl = np.mean([len(doc.split()) for doc in corpus])\n    params = {'idf_dict': idf_dict, \n            'avgdl' : avgdl,\n            'N' : len(corpus)}\n    return params\n# C\u00e1c tham s\u1ed1 \u0111\u00e3 gi\u1ea3i th\u00edch tr\u00ean ph\u1ea7n c\u00f4ng th\u1ee9c\ndef bm25_score(query, doc, params, k=1.2, b=0.75):\n\n    idf_dict = params['idf_dict']\n    avgdl = params['avgdl']\n    N = params['N']\n    score_query = 0\n\n    for word in query.split():\n        dl = len(doc.split())\n        tf = doc.count(word)\n        if word in idf_dict.keys():\n            idf = idf_dict[word]\n        else: \n            idf = np.log(N+1) \n\n        score_word = idf*(tf*(k+1))\/(tf + k*(1-b) + b*dl\/avgdl)\n        score_query += score_word\n\n    return score_query\n\nparams_bm25_title = bm25_fit(train_df['product_title'])\ntrain_df['bm25_ST'] = train_df.progress_apply(lambda row: bm25_score(row['search_term'], row['product_title'], params_bm25_title), axis=1)\ntrain_df['bm25_ST'] = train_df.progress_apply(lambda row: bm25_score(row['search_term'], row['product_title'], params_bm25_title), axis=1)\n\n# params_bm25_title = bm25_fit(test_df['product_title'])\ntest_df['bm25_ST'] = test_df.progress_apply(lambda row: bm25_score(row['search_term'], row['product_title'], params_bm25_title), axis=1)\ntest_df['bm25_ST'] = test_df.progress_apply(lambda row: bm25_score(row['search_term'], row['product_title'], params_bm25_title), axis=1)","4f0d949d":"# Train set\n\nidf_dict = dict(zip(vectorizer.get_feature_names(), list(vectorizer.idf_)))\nN = len(title_desc_unique)\nparams = {'idf_dict':idf_dict, 'N':N}\n\nmax_tf = []\nmax_idf = []\nmax_tfidf = []\n\nmin_tf = []\nmin_idf = []\nmin_tfidf = []\n\nsum_tf = []\nsum_idf = []\nsum_tfidf = []\n\n\nfor ind, row in train_df.iterrows():\n    search = row['search_term']\n    text = row['product_title']\n    tf_vals = []\n    idf_vals = []\n    tfidf_vals = []\n    for word in search.split():\n        if word in idf_dict.keys():\n            tf = text.count(word)\n            idf = idf_dict[word]\n        else:\n            tf = text.count(word)\n            idf = np.log(N+1)\n\n        tf_vals.append(tf)\n        idf_vals.append(idf)\n        tfidf_vals.append(tf*idf)\n  \n    max_tf.append(max(tf_vals))\n    min_tf.append(min(tf_vals))\n    sum_tf.append(sum(tf_vals))\n\n    max_idf.append(max(idf_vals))\n    min_idf.append(min(idf_vals))\n    sum_idf.append(sum(idf_vals))\n\n    max_tfidf.append(max(tfidf_vals))\n    min_tfidf.append(min(tfidf_vals))\n    sum_tfidf.append(sum(tfidf_vals))\n\ntrain_df['max_tf_ST'] = max_tf\ntrain_df['max_idf_ST'] = max_idf\ntrain_df['max_tfidf_ST'] = max_tfidf\n\ntrain_df['min_tf_ST'] = min_tf\ntrain_df['min_idf_ST'] = min_idf\ntrain_df['min_tfidf_ST'] = min_tfidf\n\ntrain_df['sum_tf_ST'] = sum_tf\ntrain_df['sum_idf_ST'] = sum_idf\ntrain_df['sum_tfidf_ST'] = sum_tfidf","3d0ec82f":"# Test set\n\nidf_dict = dict(zip(vectorizer.get_feature_names(), list(vectorizer.idf_)))\nN = len(title_desc_unique)\nparams = {'idf_dict':idf_dict, 'N':N}\n\nmax_tf = []\nmax_idf = []\nmax_tfidf = []\n\nmin_tf = []\nmin_idf = []\nmin_tfidf = []\n\nsum_tf = []\nsum_idf = []\nsum_tfidf = []\n\n\nfor ind, row in test_df.iterrows():\n    search = row['search_term']\n    text = row['product_title']\n    tf_vals = []\n    idf_vals = []\n    tfidf_vals = []\n    for word in search.split():\n        if word in idf_dict.keys():\n            tf = text.count(word)\n            idf = idf_dict[word]\n        else:\n            tf = text.count(word)\n            idf = np.log(N+1)\n\n        tf_vals.append(tf)\n        idf_vals.append(idf)\n        tfidf_vals.append(tf*idf)\n  \n    max_tf.append(max(tf_vals))\n    min_tf.append(min(tf_vals))\n    sum_tf.append(sum(tf_vals))\n\n    max_idf.append(max(idf_vals))\n    min_idf.append(min(idf_vals))\n    sum_idf.append(sum(idf_vals))\n\n    max_tfidf.append(max(tfidf_vals))\n    min_tfidf.append(min(tfidf_vals))\n    sum_tfidf.append(sum(tfidf_vals))\n\ntest_df['max_tf_ST'] = max_tf\ntest_df['max_idf_ST'] = max_idf\ntest_df['max_tfidf_ST'] = max_tfidf\n\ntest_df['min_tf_ST'] = min_tf\ntest_df['min_idf_ST'] = min_idf\ntest_df['min_tfidf_ST'] = min_tfidf\n\ntest_df['sum_tf_ST'] = sum_tf\ntest_df['sum_idf_ST'] = sum_idf\ntest_df['sum_tfidf_ST'] = sum_tfidf","d1e0b2ee":"train_df.corr()","ca2e8f8f":"train_df.corr().abs().sort_values(by='relevance', axis=0, ascending=False)['relevance'].to_dict().keys()","42054b6b":"train_df.to_csv('Feature_train.csv')\ntest_df.to_csv('Feature_test.csv')","c2551562":"FEATURES_LIST = [\n                'have_brand',\n                'have_att',\n                'query_length', \n                'search_term_include_brand',\n                'cosine_ST',\n                'cosine_SD',\n                'min_jaccard_brand_with_search_term', \n                'min_edit_brand_with_search_term', \n                'sum_jaccard_product_title_with_search_term', \n                'mean_jaccard_product_des_with_search_term',\n                'mean_jaccard_att_with_search_term',\n                'last_word_in_title',\n                # 'last_word_in_des',\n                # 'last_word_in_brand',\n                'number_in_att',\n                'number_in_title',\n                'number_in_des',\n                'title_common_words_count',\n                'des_common_words_count',\n                'att_common_words_count',\n                'title_common_words_count_2gram',\n                'des_common_words_count_2gram',\n                'att_common_words_count_2gram',\n                'min_jaccard_title_with_search_term',\n                'min_edit_title_with_search_term',\n                'max_cosine_sim_per_word_st_pt',\n                # 'cosine_similarity_tfidf_search_title_des',\n                'cosine_similarity_tfidf_search_title',\n                'cosine_similarity_tfidf_search_des',\n                'bm25_ST',\n                'max_idf_ST',\n                'min_tf_ST',\n                'min_tfidf_ST',\n                'sum_tf_ST',\n                'sum_idf_ST',\n                'color_st_des',\n                'color_st_title',\n                'color_st_att',\n                'max_cosine_sim_per_word_st_pd',\n                'w2v_n_similarity_st_pt',\n                'w2v_n_similarity_st_pd',\n                # 'w2v_centroid_rmse_st_pd',\n                # 'w2v_centroid_rmse_st_pt',\n                'sum_jaccard_product_description_with_search_term',\n                ]\n# FEATURES_LIST = ['max_cosine_sim_per_word_st_pt', 'min_tf_ST', 'mean_jaccard_product_des_with_search_term', 'cosine_similarity_tfidf_search_title_des', 'min_tfidf_ST', 'sum_jaccard_product_title_with_search_term', 'last_word_in_title', 'bm25_ST', 'title_common_words_count', 'min_jaccard_title_with_search_term', 'last_word_in_des', 'min_edit_title_with_search_term', 'des_common_words_count', 'max_idf_ST', 'product_uid', 'id', 'sum_tf_ST', 'sum_idf_ST', 'sum_jaccard_product_description_with_search_term']","6ac0cf11":"from sklearn.decomposition import TruncatedSVD\nsvd_model2 = TruncatedSVD(n_components=20, algorithm='randomized', n_iter=10, random_state=122)\ntransformed_pt_pd_unique = vectorizer.transform(title_desc_unique)\nsvd_model2.fit(transformed_pt_pd_unique)","4cf2c575":"def getTFIDFVec(df, row_name, svd_model):\n    np_arr = vectorizer.transform(df[row_name])\n    return svd_model.transform(np_arr)","b2cdcbbf":"#TFIDF Vector c\u1ee7a search_term, title, des\nst_vec = getTFIDFVec(train_df, 'search_term',svd_model2)\npt_vec = getTFIDFVec(train_df, 'product_title',svd_model2)\npd_vec = getTFIDFVec(train_df, 'product_description',svd_model2)\n\n# L\u1ea5y vector feature \u0111\u00e3 ch\u1ecdn v\u00e0 convert sang numpy array \u0111\u1ec3 ti\u1ec7n concat v\u1edbi c\u00e1c vector TFIDF\nnew_train_df = train_df[FEATURES_LIST].to_numpy()\nnew_train_df= np.concatenate((new_train_df, st_vec), axis=1)\nnew_train_df = np.concatenate((new_train_df, pt_vec), axis=1)\nnew_train_df = np.concatenate((new_train_df, pd_vec), axis=1)\n\n# T\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\u00ean v\u1edbi test dataframe\nst_vec_submit = getTFIDFVec(test_df, 'search_term',svd_model2)\npt_vec_submit = getTFIDFVec(test_df, 'product_title',svd_model2)\npd_vec_submit = getTFIDFVec(test_df, 'product_description',svd_model2)\n\nX_submit = test_df[FEATURES_LIST].to_numpy()\nX_submit = np.concatenate((X_submit, st_vec_submit), axis=1)\nX_submit = np.concatenate((X_submit, pt_vec_submit), axis=1)\nX_submit = np.concatenate((X_submit, pd_vec_submit), axis=1)","49740413":"X_train, X_test, y_train, y_test = train_test_split(new_train_df, train_df['relevance'], test_size=0.2, random_state=42)","5e1152ba":"from sklearn import ensemble\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import GridSearchCV \n\n# Test th\u1eed v\u1edbi HistGradientBoosting\nparams_search = {\n    \"max_depth\": [3, 4, 5, 6, 8],\n    \"learning_rate\": [0.1, 0.01, 0.05],\n    \"loss\": [\"squared_error\"],\n    \"max_iter\": [100,200,400]\n}\nparams = {  \n            'learning_rate': 0.05,\n            'max_depth': 8,\n            'max_iter': 400\n        }\n\ngbr = HistGradientBoostingRegressor(**params)\ngbr.fit(X_train, y_train)\n# gbr = GridSearchCV(HistGradientBoostingRegressor(), params_search, refit = True, verbose = 3) \n# gbr.fit(X_train, y_train)\n","6513bd17":"rmse_test = math.sqrt(mean_squared_error(y_test, gbr.predict(X_test)))\nrmse_train = math.sqrt(mean_squared_error(y_train, gbr.predict(X_train)))\n\nprint(\"Root mean square error (Test set): \" + str(rmse_test))\nprint(\"Root mean square error (Train set): \" + str(rmse_train))\n","21413e4d":"from sklearn.linear_model import RidgeCV\nfrom sklearn.ensemble import StackingRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nestimators = [\n    ('rfr', RandomForestRegressor(max_depth=8, random_state=1, min_samples_leaf=3)),\n    ('hgbr', HistGradientBoostingRegressor(learning_rate=0.05, max_depth= 8, max_iter=600)),\n    ('etg', ExtraTreesRegressor(n_estimators=100, random_state=0)),\n    ('rcv', RidgeCV())\n]\nmodel = StackingRegressor(estimators=estimators, final_estimator=RidgeCV())\nmodel.fit(X_train, y_train)\n\n","370e80e1":"rmse_test = math.sqrt(mean_squared_error(y_test, model.predict(X_test)))\nrmse_train = math.sqrt(mean_squared_error(y_train, model.predict(X_train)))\n\nprint(\"Root mean square error (Test set): \" + str(rmse_test))\nprint(\"Root mean square error (Train set): \" + str(rmse_train))","817c3d0a":"rs = model.predict(X_submit)\ntest_df_temp = pd.read_csv('..\/input\/home-depot-product-search-relevance\/test.csv.zip', encoding='ISO-8859-1')\nfor index, i in enumerate(rs):\n    if i < 1:\n        rs[index] = 1\n    if i > 3:\n        rs[index] = 3\ndata = {\n    'id': test_df_temp.id,\n    'relevance': rs\n}\nsubmission = pd.DataFrame(data=data)\nsubmission.to_csv('submission.csv', index=False)","c4b60566":"## Join c\u00e1c h\u00e0ng attribute","175632c1":"Tr\u00edch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng l\u00e0 qu\u00e1 tr\u00ecnh chuy\u1ec3n \u0111\u1ed5i d\u1eef li\u1ec7u th\u00f4 th\u00e0nh c\u00e1c \u0111\u1eb7c tr\u01b0ng th\u1ec3 hi\u1ec7n t\u1ed1t h\u01a1n c\u00e1c pattern c\u01a1 b\u1ea3n \u0111\u1ed1i v\u1edbi c\u00e1c m\u00f4 h\u00ecnh d\u1ef1 \u0111o\u00e1n, d\u1eabn \u0111\u1ebfn c\u1ea3i thi\u1ec7n \u0111\u1ed9 ch\u00ednh x\u00e1c c\u1ee7a m\u00f4 h\u00ecnh tr\u00ean d\u1eef li\u1ec7u kh\u00f4ng nh\u00ecn th\u1ea5y.","be7061be":"## TF-IDF Feature","5b8a837d":"# Make Submission","a7727514":"## Cosine similarity word level","5cbdcc16":"T\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a c\u00e1c s\u1ea3n ph\u1ea9m c\u0169ng kh\u00f4ng t\u01b0\u01a1ng \u0111\u1ed3ng nhau, m\u1ed9t s\u1ed1 s\u1ea3n ph\u1ea9m c\u00f3 uid th\u1ea5p xu\u1ea5t hi\u1ec7n kh\u00e1 nhi\u1ec1u trong c\u1ea3 2 t\u1eadp d\u1eef li\u1ec7u (t\u01b0\u01a1ng \u0111\u1ed3ng tr\u00ean c\u1ea3 2 t\u1eadp).","5ecbb7d4":"C\u00f3 kh\u00e1 nhi\u1ec1u l\u1ed7i ch\u00ednh t\u1ea3 trong search term (m\u1eb7c d\u00f9 l\u1ed7i ch\u00ednh t\u1ea3 g\u00f3p ph\u1ea7n l\u00e0m gi\u1ea3m \u0111i\u1ec3m relevance \u0111i so v\u1edbi \u0111\u00fang ch\u00ednh t\u1ea3 trong train set, v\u00ed d\u1ee5: t\u1eeb rustoleum so v\u1edbi rust oleum cho c\u00f9ng 1 s\u1ea3n ph\u1ea9m th\u00ec vi\u1ebft \u0111\u00fang ch\u00ednh t\u1ea3 s\u1ebd cho \u0111i\u1ec3m relevance cao h\u01a1n \u1edf training set tuy nhi\u00ean ch\u00fang s\u1ebd g\u00e2y nhi\u1ec5u kh\u00e1 l\u1edbn v\u00e0 khi\u1ebfn model kh\u00f3 c\u00f3 th\u1ec3 t\u1ed1t \u0111\u01b0\u1ee3c b\u1edfi s\u1ed1 l\u01b0\u1ee3ng qu\u00e1 l\u1edbn).<br>\nFile spell_check.json \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb m\u1ed9t discussion c\u1ee7a cu\u1ed9c thi n\u00e0y (s\u1eed d\u1ee5ng google search \u0111\u1ec3 check l\u1ed7i).<br>\nC\u00f3 th\u1ec3 s\u1eed d\u1ee5ng c\u00e1c th\u01b0 vi\u1ec7n s\u1eeda l\u1ed7i tuy nhi\u00ean c\u00e1c search term \u1edf \u0111\u00e2y bias v\u1ec1 ph\u00eda c\u00e1c t\u1eeb li\u00ean quan \u0111\u1ebfn home depot d\u1eabn \u0111\u1ebfn c\u00e1c corrector lib \u0111\u00f3 th\u01b0\u1eddng cho k\u1ebft qu\u1ea3 kh\u00e1 sai l\u1ec7ch (\u0111\u00e3 th\u1eed v\u1edbi autocorrector, SymSpell v\u00e0 m\u1ed9t v\u00e0i th\u01b0 vi\u1ec7n d\u1ea1ng statistical based nh\u01b0ng t\u00e1c d\u1ee5ng kh\u00f4ng t\u1ed1t l\u1eafm). ","15999771":"## Explore each dataframe","b5ea3048":"### Word2Vec Cosine Similarity\nT\u00ecm max Cosine Similarity c\u1ee7a 1 word trong search_term v\u00e0 1 word trong title","b3100b54":"Ch\u1ec9 \u0111\u01a1n thu\u1ea9n l\u00e0 loop qua c\u00e1c t\u1eeb trong search term sau \u0111\u00f3 t\u00ednh TF, IDF, TFIDF min max v\u00e0 sum c\u1ee7a c\u00e1c gi\u00e1 tr\u1ecb v\u1eeba n\u00f3i \u1edf trong title c\u1ee7a s\u1ea3n ph\u1ea9m. ","ebb0e28b":"### Jaccard Coef v\u00e0 Edit Distance Description v\u00e0 Search_term","beffce2d":"T\u00ednh **n_similarity** (m\u1ed9t d\u1ea1ng t\u00ednh cosine sim nh\u01b0ng cho 2 c\u00e2u ch\u1ee9 kh\u00f4ng ph\u1ea3i 2 word b\u1eb1ng c\u00e1ch l\u1ea5y mean c\u1ee7a t\u1ea5t c\u1ea3 c\u00e1c vector trong 2 c\u00e2u v\u00e0 t\u00ednh cosine sim tr\u00ean 2 vector mean \u0111\u00f3)","1be6a767":"M\u00f4 h\u00ecnh s\u1eed d\u1ee5ng:\n* Random Forest\n* Ridge (Cross Validation)\n* HistGradientBoosting\n\n**V\u1ec1 Random Forest:**\n\u00dd t\u01b0\u1edfng c\u1ee7a RF l\u00e0 d\u00f9ng nhi\u1ec1u m\u00f4 h\u00ecnh c\u00f3 bias th\u1ea5p nh\u01b0ng variance cao nh\u01b0 Decision Tree \u0111\u1ec3 l\u00e0m gi\u1ea3m variance c\u1ee7a m\u00f4 h\u00ecnh xu\u1ed1ng. <br>\nGi\u1ea3 s\u1eed m\u00f4 h\u00ecnh DecisionTree c\u1ee7a ch\u00fang ta l\u00e0 m\u1ed9t bi\u1ebfn ng\u1eabu nhi\u00ean X c\u00f3 mean l\u00e0 0 v\u00e0 variance l\u00e0 1, v\u1edbi m\u1ed7i pack d\u1eef li\u1ec7u m\u00e0 ch\u00fang ta l\u1ea5y ra ng\u1eabu nhi\u00ean v\u00e0 d\u00f9ng ch\u00fang \u0111\u1ec3 training ta thu \u0111\u01b0\u1ee3c 1 gi\u00e1 tr\u1ecb c\u1ee7a X (m\u1ed9t m\u00f4 h\u00ecnh) g\u1ecdi l\u00e0 $X_1$.<br>\nNh\u01b0 v\u1eady n\u1ebfu ch\u00fang ta l\u00e0m nh\u01b0 tr\u00ean b l\u1ea7n ta s\u1ebd thu \u0111\u01b0\u1ee3c b m\u00f4 h\u00ecnh c\u00f3 $X_1$ \u0111\u1ebfn $X_b$ (hay c\u00f2n g\u1ecdi l\u00e0 l\u1ea5y m\u1eabu, nh\u01b0ng kh\u00f4ng ph\u1ea3i l\u1ea5y m\u1eabu d\u1eef li\u1ec7u m\u00e0 l\u00e0 l\u1ea5y m\u1eabu m\u00f4 h\u00ecnh). Khi t\u00ednh sample variance ta thu \u0111\u01b0\u1ee3c l\u00e0 1\/b + p (con s\u1ed1 p ph\u1ea3n \u00e1nh vi\u1ec7c l\u1ea5y m\u1eabu c\u00f3 \u0111\u1ed9c l\u1eadp v\u1edbi nhau hay kh\u00f4ng, n\u1ebfu c\u00f3 p s\u1ebd x\u1ea5p x\u1ec9 0) v\u00e0 variance s\u1ebd gi\u1ea3m c\u00f2n 1\/b.<br>\n[M\u1ed9t b\u00e0i vi\u1ebft kh\u00e1 clear v\u1ec1 m\u00f4 h\u00ecnh n\u00e0y](https:\/\/towardsdatascience.com\/understanding-the-effect-of-bagging-on-variance-and-bias-visually-6131e6ff1385).<br>\n![image.png](attachment:2ee7c3b9-0bcd-42af-9b3f-e3fb8a39876a.png)<br>\nMinh h\u1ecda v\u1ec1 vi\u1ec7c s\u1eed d\u1ee5ng nhi\u1ec1u tree v\u00e0 1 tree.<br>\n![image.png](attachment:27354430-a8c1-4993-9501-a6fd085232ef.png)<br>\n![image.png](attachment:cbd4919e-c32f-4231-9d87-e9aeab11ea1d.png)","e668843d":"### Jaccard Coef v\u00e0 Edit Distance Brand v\u00e0 Search_term","2ff96b8c":"### Prepare TFIDF using TFIDF Vectorizer (sklearn)","4a19eb40":"### TFIDF Cosine similarity","f278bc56":"## Jaccard Coefficient v\u00e0 Edit Distance \nT\u00ednh Jaccard Coef v\u00e0 Edit Distance nh\u1ecf nh\u1ea5t c\u00f3 th\u1ec3 (gi\u1eefa 2 word) c\u1ee7a search_term v\u00e0 brand, title l\u00e0 bao nhi\u00eau\n<img src=\"https:\/\/i.ytimg.com\/vi\/Ah_4xqvS1WU\/maxresdefault.jpg\" alt=\"drawing\" style=\"width:500px;\"\/><br>\nC\u00e1ch t\u00ednh c\u1ee7a Jaccard Coefficient l\u00e0 s\u1ed1 l\u01b0\u1ee3ng c\u1ee7a c\u00e1c t\u1eeb (unique) c\u00f3 chung trong c\u1ea3 2 c\u00e2u chia cho s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb unique khi union 2 c\u00e2u.<br>\nC\u00e1ch t\u00ednh c\u1ee7a Edit Distance ch\u1ec9 \u0111\u01a1n gi\u1ea3n l\u00e0 cho 2 t\u1eeb ```\"ABC\"``` v\u00e0 ```\"ADC\"``` n\u1ebfu nh\u01b0 cho c\u00e1c h\u00e0nh \u0111\u1ed9ng th\u00eam, s\u1eeda, x\u00f3a 1 k\u00fd t\u1ef1 n\u00e0o \u0111\u00f3 trong t\u1eeb \u0111\u1ec1u l\u00e0 1 \u0111i\u1ec3m th\u00ec s\u1ed1 \u0111i\u1ec3m nh\u1ecf nh\u1ea5t c\u1ea7n b\u1ecf ra \u0111\u1ec3 \u0111i\u1ec1u ch\u00ednh t\u1eeb ```\"ABC\" -> \"ADC\"``` s\u1ebd ti\u00eau t\u1ed1n bao nhi\u00eau \u0111i\u1ec3m (\u1ede trong v\u00ed d\u1ee5 n\u00e0y l\u00e0 1 \u0111i\u1ec3m).<br>\n\u1ede \u0111\u00e2y ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y kho\u1ea3ng c\u00e1ch nh\u1ecf nh\u1ea5t c\u00f3 th\u1ec3 gi\u1eefa 2 t\u1eeb trong search_term v\u00e0 c\u00e1c field kh\u00e1c c\u00f3 \u1ea3nh h\u01b0\u1edfng t\u1edbi relevance (t\u00ecm , ngo\u00e0i ra t\u1ed5ng kho\u1ea3ng c\u00e1ch c\u0169ng c\u00f3 \u1ea3nh h\u01b0\u1edfng.<br>\n\nNgo\u00e0i ra mean distance gi\u1eefa search_term v\u1edbi c\u00e1c field \u1edf \u0111\u00e2y \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng c\u00e1ch t\u00ecm c\u00e1ch sau: (v\u00ed d\u1ee5 \u0111\u1ed1i v\u1edbi description)\n* Split search_term th\u00e0nh c\u00e1c word\n* Loop qua c\u00e1c word tr\u00ean v\u00e0 t\u00ecm ra word (trong description) c\u00f3 kho\u1ea3ng c\u00e1ch nh\u1ecf nh\u1ea5t v\u1edbi word \u0111ang x\u00e9t (trong search_term)\n* T\u00ednh t\u1ed5ng c\u00e1c kho\u1ea3ng c\u00e1ch nh\u1ecf nh\u1ea5t \u0111\u00f3 chia cho length c\u1ee7a search_term (b\u1edfi v\u00ec kh\u00f4ng c\u00f3 tr\u01b0\u1eddng h\u1ee3p search_term r\u1ed7ng n\u00ean s\u1ebd kh\u00f4ng bao gi\u1edd chia cho kh\u00f4ng)","0246f917":"## Last word in description","4f970529":"### Merge Brand v\u00e0o b\u1ea3ng train","66d088d5":"Ki\u1ec3m tra xem s\u1ea3n ph\u1ea9m c\u00f3 brand ho\u1eb7c attribute hay kh\u00f4ng b\u1edfi nh\u1eefng s\u1ea3n ph\u1ea9m nh\u01b0 n\u00e0y th\u01b0\u1eddng x\u01a1 x\u00e0i v\u00e0 \u00edt \u0111\u01b0\u1ee3c quan t\u00e2m v\u00e0 c\u00f3 relevance th\u1ea5p","9b32f4ac":"### Jaccard Coef v\u00e0 Edit Distance Title v\u00e0 Search_term","263762b1":"\u00dd t\u01b0\u1edfng c\u1ee7a Word2Vec \u0111\u00f3 l\u00e0 d\u1ef1a tr\u00ean c\u00e1c t\u1eeb xung quanh (context word) \u0111\u1ec3 d\u1ef1 \u0111o\u00e1n t\u1eeb n\u1eb1m gi\u1eefa (target word) \u0111\u1ed1i v\u1edbi m\u00f4 h\u00ecnh CBOW ho\u1eb7c d\u1ef1a v\u00e0o t\u1eeb \u1edf gi\u1eefa \u0111o\u00e1n c\u00e1c t\u1eeb xung quanh \u0111\u1ed1i v\u1edbi m\u00f4 h\u00ecnh Skip-gram.\nC\u1ea3 2 m\u00f4 h\u00ecnh n\u00e0y \u0111\u1ec1u c\u00f3 th\u1ec3 \u0111\u01b0\u1ee3c hu\u1ea5n luy\u1ec7n Unsupervised b\u1eb1ng Neural Network. <br>\n\n![image.png](attachment:df1e9b9b-97cd-40e5-85ee-3876166d5356.png)!<br>\nL\u1ea5y v\u00ed d\u1ee5 v\u1ec1 m\u00f4 h\u00ecnh Skip-gram:<br>\nM\u1ee5c ti\u00eau c\u1ee7a m\u00f4 h\u00ecnh l\u00e0 maximize x\u00e1c su\u1ea5t c\u00f3 \u0111i\u1ec1u ki\u1ec7n c\u1ee7a c\u00e1c t\u1eeb context quick, brown, jumps khi bi\u1ebft t\u1eeb target l\u00e0 fox\n$P(\"quick\",\"brown\",\"jumps\",\"over\"|\"fox\")$<br>\n\u0110\u1ec3 \u0111\u01a1n gi\u1ea3n h\u00f3a m\u00f4 h\u00ecnh ch\u00fang ta gi\u1ea3 s\u1eed x\u00e1c su\u1ea5t c\u00f3 \u0111i\u1ec1u ki\u1ec7n c\u1ee7a c\u00e1c t\u1eeb context l\u00e0 \u0111\u1ed9c l\u1eadp ngh\u0129a l\u00e0\n$P(\"quick\",\"brown\",\"jumps\",\"over\"|\"fox\") = P(\"quick\"|\"fox\")P(\"brown\"|\"fox\")P(\"jumps\"|\"fox\")P(\"over\"|\"fox\")$<br>\nHay ch\u00ednh l\u00e0 t\u1ed1i thi\u1ec3u h\u00f3a h\u00e0m Negative Log Likelihood sau:<br>\n$\\large \\mathcal{L}(\\mathbf{U}, \\mathbf{V}; w_t) = -\\sum_{c \\in \\mathcal{C}_t} \\log \\frac{\\exp(\\mathbf{u}_t^T\\mathbf{v}_c)}{\\sum_{i=1}^{N}\\exp(\\mathbf{u}_t^T\\mathbf{v}_i)}$<br>\nU \u1edf \u0111\u00e2y ch\u00ednh l\u00e0 ma tr\u1eadn c\u00f3 k\u00edch th\u01b0\u1edbc ```s\u1ed1 chi\u1ec1u embedding * s\u1ed1 t\u1eeb c\u1ee7a t\u1eeb \u0111i\u1ec3n```. Th\u00f4ng th\u01b0\u1eddng c\u00e1c vector \u0111\u1ea7u v\u00e0o c\u1ee7a m\u00f4 h\u00ecnh ch\u00ednh l\u00e0 one-hot vector n\u00ean khi nh\u00e2n v\u1edbi ma tr\u1eadn U th\u00ec s\u1ebd cho k\u1ebft qu\u1ea3 l\u00e0 1 vector c\u1ed9t c\u00f3 s\u1ed1 th\u1ee9 t\u1ef1 l\u00e0 s\u1ed1 th\u1ee9 t\u1ef1 c\u1ee7a t\u1eeb \u0111\u1ea7u v\u00e0o trong t\u1eeb \u0111i\u1ec3n $\\bf{u}_t$ cho t\u1eeb th\u1ee9 t trong t\u1eeb \u0111i\u1ec3n. Nh\u01b0 v\u1eady b\u1ea3n ch\u1ea5t vector $\\bf{u}_t$ ch\u00ednh l\u00e0 vector embedding c\u1ee7a t\u1eeb \u0111\u00f3.<br>\nV t\u01b0\u01a1ng t\u1ef1 nh\u01b0 U nh\u01b0ng c\u00f3 k\u00edch th\u01b0\u1edbc ng\u01b0\u1ee3c l\u1ea1i so v\u1edbi U. m\u1ed7i c\u1ed9t trong V s\u1ebd l\u00e0 1 vector c\u00f3 s\u1ed1 chi\u1ec1u l\u00e0 ```s\u1ed1 chi\u1ec1u embedding * 1```. Nh\u01b0 v\u1eady m\u1ed7i vector c\u1ed9t $\\bf{v_c}$ l\u1ea1i \u1ee9ng v\u1edbi d\u1ea1ng \u0111\u01b0\u1ee3c g\u1ecdi l\u00e0 context vector c\u1ee7a t\u1eeb th\u1ee9 c trong t\u1eeb \u0111i\u1ec3n.<br>\n=> B\u1ea3n ch\u1ea5t c\u1ee7a ph\u00e9p nh\u00e2n ma tr\u1eadn tr\u00ean l\u00e0 nh\u00e2n vector embedding v\u1edbi t\u1eebng vector context sao cho v\u1edbi context l\u00e0 vector c\u1ee7a t\u1eeb \u0111\u1ee9ng c\u1ea1nh t\u1eeb fox nh\u01b0 quick brown,... th\u00ec k\u1ebft qu\u1ea3 c\u1ee7a ph\u00e9p nh\u00e2n tr\u00ean l\u00e0 l\u1edbn h\u01a1n so v\u1edbi c\u00e1c vector context kh\u00e1c.\n[Read more here (in Vietnamese)](https:\/\/machinelearningcoban.com\/tabml_book\/ch_embedding\/word2vec.html)\n","9f81e830":"T\u00ecm max Cosine Similarity c\u1ee7a 1 word trong search_term v\u00e0 1 word trong description","793ae6b9":"Nh\u01b0 v\u1eady b\u1ea3ng description l\u00e0 \u0111\u1ea7y \u0111\u1ee7 nh\u1ea5t, ch\u1ee9a to\u00e0n b\u1ed9 c\u00e1c s\u1ea3n ph\u1ea9m c\u00f3 trong c\u1ea3 train set, test set v\u00e0 c\u00e1c s\u1ea3n ph\u1ea9m c\u00f3 trong b\u1ea3ng attributes.","ce091e64":"## Number in description\nT\u01b0\u01a1ng t\u1ef1 nh\u01b0 [Number in attributes](#Number-in-attributes)","d0584e45":"### Attributes dataframe","35b398ea":"## Data fields\n* ```id``` : index c\u1ee7a c\u1eb7p search_term v\u00e0 product_id\n* ```product_uid```: ID c\u1ee7a s\u1ea3n ph\u1ea9m\n* ```product_title```: Ti\u00eau \u0111\u1ec1 c\u1ee7a s\u1ea3n ph\u1ea9m\n* ```product_description```: M\u00f4 t\u1ea3 th\u00f4ng tin c\u1ee7a s\u1ea3n ph\u1ea9m\n* ```search_term```: Query t\u00ecm ki\u1ebfm\n* ```relevance```: \u0110\u00e1nh gi\u00e1 s\u1ef1 li\u00ean quan c\u1ee7a t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm v\u1edbi s\u1ea3n ph\u1ea9m\n* ```name```: T\u00ean thu\u1ed9c t\u00ednh c\u1ee7a s\u1ea3n ph\u1ea9m (Bullet01, Bullet02, ...)\n* ```value```: Gi\u00e1 tr\u1ecb \u1ee9ng v\u1edbi t\u00ean thu\u1ed9c t\u00ednh t\u01b0\u01a1ng \u1ee9ng(V\u00ed d\u1ee5: thu\u1ed9c t\u00ednh Bullet04 c\u00f3 gi\u00e1 tr\u1ecb l\u00e0 Dimensions: 3 in. x 3 in. x 1-1\/2 in.)","c63035c3":"Training set c\u00f3 ph\u00e2n b\u1ed1 nghi\u00eang v\u1ec1 m\u1ee9c high relevance (t\u1eeb 2.67 \u0111\u1ebfn 3) v\u00e0 c\u00f3 kh\u00e1 \u00edt c\u00e1c gi\u00e1 tr\u1ecb \u0111i\u1ec3m th\u1ea5p","b94f3b95":"## BM25 Ranking\nFeature n\u00e0y \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb m\u1ed9t tutorial v\u1ec1 dataset n\u00e0y ([Link](https:\/\/towardsdatascience.com\/modeling-product-search-relevance-in-e-commerce-home-depot-case-study-8ccb56fbc5ab)) <br>\nBM25 \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng trong c\u00e1c engine t\u00ecm ki\u1ebfm c\u1ed5 \u0111i\u1ec3n ng\u00e0y tr\u01b0\u1edbc c\u00f3 c\u00f4ng th\u1ee9c nh\u01b0 sau: <br>\n$\\large{\\displaystyle {\\text{score}}(D,Q)=\\sum _{i=1}^{n}{\\text{IDF}}(q_{i})\\cdot {\\frac {f(q_{i},D)\\cdot (k_{1}+1)}{f(q_{i},D)+k_{1}\\cdot \\left(1-b+b\\cdot {\\frac {|D|}{\\text{avgdl}}}\\right)}}}$ <br>\nTrong \u0111\u00f3: \n* $Q$ l\u00e0 query (search_term) c\u00f2n $D$ ch\u00ednh l\u00e0 document c\u1ea7n \u0111\u00e1nh gi\u00e1 relevance (\u1edf \u0111\u00e2y l\u00e0 product_title or description).\n* $IDF(q_i)$ ch\u00ednh l\u00e0 Inverse Document Frequency trong TFIDF.\n* $f(q_i, D)$ l\u00e0 TF c\u1ee7a word $q_i$ trong c\u1ee5m query $Q$ v\u1edbi document $D$.\n* $avgdl$ l\u00e0 \u0111\u1ed9 d\u00e0i trung b\u00ecnh c\u1ee7a c\u00e1c document.\n* $|D|$ l\u00e0 \u0111\u1ed9 d\u00e0i c\u1ee7a document \u0111ang x\u00e9t.\n* $k_1$ v\u00e0 $b$ l\u00e0 tham s\u1ed1 t\u1ef1 do th\u01b0\u1eddng ch\u1ecdn l\u00e0  ${\\displaystyle k_{1}\\in [1.2,2.0]}$ $b=0.75$","c7bc2468":"K\u1ebft qu\u1ea3 thu \u0111\u01b0\u1ee3c b\u1ea3ng c\u00f3 format d\u1ea1ng ```product_uid``` k\u00e8m theo c\u1ed9t ```att_summary```  ``` t\u00ean_thu\u1ed9c_t\u00ednh_1 gi\u00e1_tr\u1ecb_thu\u1ed9c_t\u00ednh_1 t\u00ean_thu\u1ed9c_t\u00ednh_2 gi\u00e1_tr\u1ecb_thu\u1ed9c_t\u00ednh_2 ...``` nh\u01b0 sau:","b591f815":"T\u01b0\u01a1ng t\u1ef1 nh\u01b0 tr\u00ean","f80fa6d1":"C\u00e1c search_term th\u01b0\u1eddng vi\u1ebft c\u00e1c \u0111\u01a1n v\u1ecb \u0111o l\u01b0\u1eddng theo nhi\u1ec1u c\u00e1ch kh\u00e1c nhau n\u00ean c\u1ea7n \u0111\u01b0a v\u1ec1 1 d\u1ea1ng v\u00ed d\u1ee5 2 inches, 2in. -> 2 in ","1f8bbcb3":"## Search term length\n\u0110\u1ed9 d\u00e0i c\u1ee7a search_term ","f37dad1c":"# Data Exploration","1b261984":"C\u00e1c h\u00e0ng c\u00f3 product_uid r\u1ed7ng th\u00ec name c\u0169ng s\u1ebd r\u1ed7ng, c\u00e1c h\u00e0ng c\u00f3 value r\u1ed7ng th\u00ec c\u00f3 th\u1ec3 ch\u1ec9 value r\u1ed7ng ho\u1eb7c c\u1ea3 3 c\u1ed9t \u0111\u1ec1u r\u1ed7ng.","4844a69a":"### Chu\u1ea9n h\u00f3a \u0111\u01a1n v\u1ecb \u0111o l\u01b0\u1eddng","98d0604d":"## Word2Vec Feature Self Corpus","e54c4834":"## Min,Max TF, IDF","74a9dad2":"Lo\u1ea1i b\u1ecf c\u00e1c k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t trong c\u00e2u nh\u01b0 @, -, $, ...","350cfaee":"### Prepare Word2Vec Model using Gensim\nS\u1eed d\u1ee5ng ngay corpus ch\u00ednh l\u00e0 dataset n\u00e0y \u0111\u1ec3 training word2vec do c\u00e1c corpus kh\u00e1c cho k\u1ebft qu\u1ea3 kh\u00f4ng qu\u00e1 t\u1ed1t.","8b930db8":"S\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm \u0111\u1ed9c l\u1eadp l\u00e0 11795, chi\u1ebfm 16% t\u1ed5ng s\u1ed1 ph\u1ea7n t\u1eed c\u1ee7a t\u1eadp train <br>\nS\u1ed1 l\u01b0\u1ee3ng c\u00e1c s\u1ea3n ph\u1ea9m \u0111\u1ed9c l\u1eadp l\u00e0 54667, chi\u1ebfm 73.8% t\u1ed5ng s\u1ed1 ph\u1ea7n t\u1eed c\u1ee7a t\u1eadp train","f6af6e97":"## Have brand, atributes or not","165c6b3e":"Ch\u1ec9 c\u00f3 m\u1ed9t s\u1ed1 l\u01b0\u1ee3ng r\u1ea5t nh\u1ecf c\u00e1c thu\u1ed9c t\u00ednh l\u00e0 c\u00f3 s\u1ed1 l\u01b0\u1ee3ng l\u1edbn s\u1ea3n ph\u1ea9m \u0111\u1ec1u c\u00f3, c\u00f2n \u0111a ph\u1ea7n c\u00e1c thu\u1ed9c t\u00ednh c\u00f2n l\u1ea1i \u0111\u1ec1u c\u00f3 r\u1ea5t \u00edt c\u00e1c s\u1ea3n ph\u1ea9m c\u00f3 ch\u00fang.<br>\nC\u00e1c s\u1ea3n ph\u1ea9m c\u00f3 thu\u1ed9c t\u00ednh th\u00ec \u0111\u1ec1u s\u1ebd c\u00f3 brand name","af02d37e":"TFIDF, vi\u1ebft t\u1eaft t\u1eeb c\u1ee5m t\u1eeb ti\u1ebfng Anh: term frequency\u2013inverse document frequency, l\u00e0 m\u1ed9t th\u1ed1ng k\u00ea s\u1ed1 h\u1ecdc nh\u1eb1m ph\u1ea3n \u00e1nh t\u1ea7m quan tr\u1ecdng c\u1ee7a m\u1ed9t t\u1eeb \u0111\u1ed1i v\u1edbi m\u1ed9t v\u0103n b\u1ea3n trong m\u1ed9t t\u1eadp h\u1ee3p hay m\u1ed9t ng\u1eef li\u1ec7u v\u0103n b\u1ea3n (\u0111\u1ecbnh ngh\u0129a b\u1edfi Wikipedia).<br>\n\u00dd t\u01b0\u1edfng ch\u00ednh c\u1ee7a TFIDF \u0111\u00f3 l\u00e0 \u0111\u00e1nh gi\u00e1 c\u00e1c t\u1eeb d\u1ef1a tr\u00ean t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb \u0111\u00f3 trong v\u0103n b\u1ea3n \u0111ang x\u00e9t v\u00e0 t\u1ea7n su\u1ea5t xu\u1ea5t hi\u1ec7n c\u1ee7a t\u1eeb \u0111\u00f3 trong to\u00e0n b\u1ed9 c\u00e1c v\u0103n b\u1ea3n (s\u1ed1 l\u01b0\u1ee3ng v\u0103n b\u1ea3n m\u00e0 t\u1eeb \u0111\u00f3 c\u00f3 xu\u1ea5t hi\u1ec7n). <br>\nTFIDF gi\u00fap kh\u1eafc ph\u1ee5c nh\u01b0\u1ee3c \u0111i\u1ec3m c\u1ee7a c\u00e1c t\u1eeb ph\u1ed5 bi\u1ebfn (v\u00ed d\u1ee5 stopword) c\u00f3 t\u1ea7n su\u1ea5t qu\u00e1 l\u1edbn trong khi \u0111\u00f3 c\u00e1c t\u1eeb quan tr\u1ecdng xu\u1ea5t hi\u1ec7n v\u1edbi t\u1ea7n su\u1ea5t v\u1eeba ph\u1ea3i h\u01a1n nh\u01b0ng c\u00f3 gi\u00e1 tr\u1ecb t\u1ea7n su\u1ea5t nh\u1ecf h\u01a1n c\u00e1c t\u1eeb \u0111\u00f3. Tuy nhi\u00ean TFIDF kh\u00f4ng gi\u1ea3i quy\u1ebft \u0111\u01b0\u1ee3c v\u1ea5n \u0111\u1ec1 v\u1ec1 v\u1ecb tr\u00ed c\u1ee7a t\u1eeb trong c\u00e2u c\u0169ng nh\u01b0 v\u1ea5n \u0111\u1ec1 v\u1ec1 ng\u1eef ngh\u0129a (semantics).<br>\n\n**TF- term frequency** \u2013 t\u1ea7n s\u1ed1 xu\u1ea5t hi\u1ec7n c\u1ee7a 1 t\u1eeb trong 1 v\u0103n b\u1ea3n. C\u00e1ch t\u00ednh:\n\n${\\displaystyle \\mathrm {tf} (t,d)={\\frac {\\mathrm {f} (t,d)}{\\max\\{\\mathrm {f} (w,d):w\\in d\\}}}}$\nTh\u01b0\u01a1ng c\u1ee7a s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n 1 t\u1eeb trong v\u0103n b\u1ea3n v\u00e0 s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t c\u1ee7a m\u1ed9t t\u1eeb b\u1ea5t k\u1ef3 trong v\u0103n b\u1ea3n \u0111\u00f3. (gi\u00e1 tr\u1ecb s\u1ebd thu\u1ed9c kho\u1ea3ng $[0, 1]$).<br>\n* $f(t,d)$ - s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n t\u1eeb t trong v\u0103n b\u1ea3n d.\n* $max{f(w,d):w\u2208d}$ - s\u1ed1 l\u1ea7n xu\u1ea5t hi\u1ec7n nhi\u1ec1u nh\u1ea5t c\u1ee7a m\u1ed9t t\u1eeb b\u1ea5t k\u1ef3 trong v\u0103n b\u1ea3n.\n\n**IDF \u2013 inverse document frequency**. T\u1ea7n s\u1ed1 ngh\u1ecbch c\u1ee7a 1 t\u1eeb trong t\u1eadp v\u0103n b\u1ea3n (corpus).\n\nT\u00ednh IDF \u0111\u1ec3 gi\u1ea3m gi\u00e1 tr\u1ecb c\u1ee7a nh\u1eefng t\u1eeb ph\u1ed5 bi\u1ebfn. M\u1ed7i t\u1eeb ch\u1ec9 c\u00f3 1 gi\u00e1 tr\u1ecb IDF duy nh\u1ea5t trong t\u1eadp v\u0103n b\u1ea3n.\n\n${\\displaystyle \\mathrm {idf} (t,D)=\\log {\\frac {|D|}{|\\{d\\in D:t\\in d\\}|}}}$\n* ${\\displaystyle |D|}{\\displaystyle |D|}$: - t\u1ed5ng s\u1ed1 v\u0103n b\u1ea3n trong t\u1eadp D\n* ${\\displaystyle |\\{d\\in D:t\\in d\\}|}{\\displaystyle |\\{d\\in D:t\\in d\\}|}$: - s\u1ed1 v\u0103n b\u1ea3n ch\u1ee9a t\u1eeb nh\u1ea5t \u0111\u1ecbnh, v\u1edbi \u0111i\u1ec1u ki\u1ec7n ${\\displaystyle t}$ xu\u1ea5t hi\u1ec7n trong v\u0103n b\u1ea3n d (i.e., ${\\displaystyle \\mathrm {tf} (t,d)\\neq 0}$).\n* N\u1ebfu t\u1eeb \u0111\u00f3 kh\u00f4ng xu\u1ea5t hi\u1ec7n \u1edf b\u1ea5t c\u1ee9 1 v\u0103n b\u1ea3n n\u00e0o trong t\u1eadp th\u00ec m\u1eabu s\u1ed1 s\u1ebd b\u1eb1ng 0 => ph\u00e9p chia cho kh\u00f4ng kh\u00f4ng h\u1ee3p l\u1ec7, v\u00ec th\u1ebf ng\u01b0\u1eddi ta th\u01b0\u1eddng thay b\u1eb1ng m\u1eabu th\u1ee9c ${\\displaystyle 1+|\\{d\\in D:t\\in d\\}|}$. (l\u00e0 tham s\u1ed1 **smooth_idf** trong sklearn)","0a5e474f":"C\u00f3 13 s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 brand name v\u00e0 c\u00f3 38k s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 Attribute n\u00ean c\u1ea7n fill gi\u00e1 tr\u1ecb r\u1ed7ng c\u1ee7a ch\u00fang \u0111\u1ec3 ti\u1ec7n x\u1eed l\u00fd sau n\u00e0y.","61a43667":"L\u01b0u l\u1ea1i file train v\u00e0 test \u0111\u00e3 qua x\u1eed l\u00fd \u0111\u1ec3 kh\u00f4ng c\u1ea7n ch\u1ea1y l\u1ea1i l\u1ea7n sau","7c97b889":"# Load data and dependency","0e1f29d9":"**V\u1ec1 GradientBoosting:** <br>\nDo ch\u01b0a t\u00ecm hi\u1ec3u k\u1ef9 v\u1ec1 HistGradientBoosting (LightGBM implementation) nh\u01b0ng c\u00f3 l\u1ebd v\u1ec1 \u00fd t\u01b0\u1edfng s\u1ebd gi\u1ed1ng v\u1edbi GradientBoosting n\u00ean em s\u1ebd vi\u1ebft v\u1ec1 GB.<br>\nBoosting l\u00e0 m\u1ed9t ph\u01b0\u01a1ng ph\u00e1p gi\u00fap t\u0103ng \u0111\u1ed9 ch\u00ednh x\u00e1c cho c\u00e1c m\u00f4 h\u00ecnh c\u00f3 \u0111\u1ed9 ch\u00ednh x\u00e1c th\u1ea5p (nh\u01b0 Decision tree v\u1edbi \u0111\u1ed9 s\u00e2u th\u1ea5p).<br>\n\u00dd t\u01b0\u1edfng ch\u00ednh c\u1ee7a Boosting l\u00e0 c\u1eadp nh\u1eadt m\u00f4 h\u00ecnh b\u1eb1ng t\u1ed5 h\u1ee3p tuy\u1ebfn t\u00ednh c\u1ee7a c\u00e1c m\u00f4 h\u00ecnh y\u1ebfu sao cho h\u00e0m l\u1ed7i gi\u1ea3m d\u1ea7n.\\\nSau m\u1ed7i b\u01b0\u1edbc c\u1ed9ng \u0111\u00f3, c\u00e1c gi\u00e1 tr\u1ecb d\u1ef1 \u0111o\u00e1n sai s\u1ebd \u0111\u01b0\u1ee3c c\u1eadp nh\u1eadt 1 tr\u1ecdng s\u1ed1 \u0111\u1ec3 t\u0103ng gi\u00e1 tr\u1ecb h\u00e0m l\u1ed7i m\u00e0 b\u1ed9 gi\u00e1 tr\u1ecb \u0111o\u00e1n sai \u0111\u00f3 g\u00e2y \u1ea3nh h\u01b0\u1edfng l\u00ean h\u00e0m l\u1ed7i trong b\u01b0\u1edbc ti\u1ebfp theo.<br>\nC\u00e1c b\u01b0\u1edbc c\u1ee7a GradientBoosting nh\u01b0 h\u00ecnh sau:<br>\n![image.png](attachment:d9bac418-8a07-4930-a0d3-9d4b356fa685.png) <br>\n\u0110i\u1ec3m kh\u00e1c bi\u1ec7t c\u1ee7a AdaBoost v\u1edbi GB n\u1eb1m \u1edf b\u01b0\u1edbc 2.1 v\u00e0 2.2, ch\u00fang ta t\u00ednh pseudo-residuals tr\u00ean t\u1eebng ph\u1ea7n t\u1eed $x_i$ \u0111\u1ec3 thu \u0111\u01b0\u1ee3c 1 dataset m\u1edbi l\u00e0 {${(x_i, r_{im})}^{n}_{i=1}$} v\u00e0 weak leaner $h_m$ s\u1ebd \u0111\u01b0\u1ee3c train \u0111\u1ec3 fit v\u1edbi dataset m\u1edbi n\u00e0y. <br>\nNh\u01b0 v\u1eady \u00fd t\u01b0\u1edfng \u1edf \u0111\u00e2y kh\u00e1 gi\u1ed1ng v\u1edbi gradient descent n\u1ebfu ch\u00fang ta coi h\u00e0m l\u1ed7i l\u00e0 RMSE($y$,$\\hat{y}$) v\u00e0 model l\u00e0 $F_{m-1}$ (trong tr\u01b0\u1eddng h\u1ee3p c\u1ee7a GB th\u00ec ch\u00ednh l\u00e0 $\\hat{y}$=$F_{m-1}$), n\u1ebfu mu\u1ed1n t\u1ed1i thi\u1ec3u h\u00e0m l\u1ed7i (coi $\\hat{y}$ l\u00e0 bi\u1ebfn c\u1ee7a h\u00e0m l\u1ed7i) th\u00ec ta c\u1ea7n d\u1ecbch chuy\u1ec3n bi\u1ebfn $\\hat{y}$ n\u00e0y ng\u01b0\u1ee3c chi\u1ec1u c\u1ee7a \u0111\u1ea1o h\u00e0m h\u00e0m l\u1ed7i (\u0111\u1ea1o h\u00e0m cho $\\hat{y}$).<br>\nDo \u0111\u00f3 kho\u1ea3ng d\u1ecbch chuy\u1ec3n s\u1ebd l\u00e0 ```\u0111\u1ea1o h\u00e0m * learning_rate```, vi\u1ec7c \u0111i t\u00ecm $h_m$ fit v\u1edbi \u0111\u1ea1o h\u00e0m l\u00e0 do $F_m = F_{m-1} + \\gamma h_m(x)$ hay $\\hat{y} = \\hat{y} + \\gamma h_m(x)$.<br>\n\u1ede tr\u00ean learning rate ch\u00ednh l\u00e0 $\\gamma$ c\u00f2n \u0111\u1ea1o h\u00e0m ch\u00ednh l\u00e0 $h_m(x)$ ch\u1ec9 kh\u00e1c \u1edf gradient descent \u1edf ch\u1ed7 $\\gamma$ \u1edf \u0111\u00e2y kh\u00f4ng ph\u1ea3i hyperparameter m\u00e0 ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 t\u00ecm \u0111\u01b0\u1ee3c nh\u1edd gi\u1ea3i b\u00e0i to\u00e1n t\u1ed1i \u01b0u \u1edf b\u01b0\u1edbc 2.3\n","607b2509":"C\u00f3 155 h\u00e0ng c\u00f3 c\u1ed9t product_uid kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb, 155 h\u00e0ng c\u00f3 c\u1ed9t name kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb v\u00e0 2284 h\u00e0ng c\u00f3 c\u1ed9t value kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb trong b\u1ea3ng thu\u1ed9c t\u00ednh s\u1ea3n ph\u1ea9m. \u0110i\u1ec1u n\u00e0y kh\u00e1 hi\u1ec3n nhi\u00ean b\u1edfi v\u00ec c\u00f3 1 s\u1ed1 s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 m\u1ed9t v\u00e0i thu\u1ed9c t\u00ednh, tuy nhi\u00ean l\u1ebd ra thu\u1ed9c t\u00ednh kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb th\u00ec ch\u00fang kh\u00f4ng n\u00ean \u0111\u01b0\u1ee3c \u0111\u01b0a v\u00e0o b\u1ea3ng n\u00e0y v\u00e0 \u0111\u1ec3 r\u1ed7ng nh\u01b0 v\u1eady.","31d86e2f":"## Number in title \nT\u01b0\u01a1ng t\u1ef1 nh\u01b0 [Number in attributes](#Number-in-attributes)","94bab3da":"Nh\u00ecn v\u00e0o Venn Diagram \u1edf tr\u00ean v\u00e0 tr\u00ean n\u1eefa, c\u00f3 th\u1ec3 th\u1ea5y ch\u1ec9 c\u00f3 13 s\u1ea3n ph\u1ea9m l\u00e0 kh\u00f4ng c\u00f3 Brand Name, ho\u00e0n to\u00e0n c\u00f3 th\u1ec3 fill null b\u1eb1ng 1 gi\u00e1 tr\u1ecb n\u00e0o \u0111\u00f3 v\u00e0 \u00edt \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn dataset, vi\u1ec7c s\u1eed d\u1ee5ng Brand kh\u00e1 quan tr\u1ecdng v\u00ec thang \u0111i\u1ec3m relevance \u0111\u01b0\u1ee3c t\u00ednh d\u1ef1a tr\u00ean Brand Name c\u00f3 kh\u1edbp hay kh\u00f4ng.","6febc95d":"V\u1edbi m\u1ed7i h\u00e0ng trong b\u1ea3ng attribute l\u00e0 \u1ee9ng v\u1edbi 1 thu\u1ed9c t\u00ednh c\u1ee7a 1 s\u1ea3n ph\u1ea9m, ch\u00fang ta s\u1ebd join c\u00e1c h\u00e0ng c\u1ee7a ch\u00fang l\u1ea1i v\u1edbi nhau th\u00e0nh 1 h\u00e0ng \u1ee9ng v\u1edbi to\u00e0n b\u1ed9 thu\u1ed9c t\u00ednh c\u1ee7a 1 s\u1ea3n ph\u1ea9m","f304e9a0":"# Data processing","1801c30b":"### Merge attribute v\u00e0o b\u1ea3ng train","67f0d3ed":"# Student Infomation\n**Student ID:** 19021222 <br>\n**Fullname:** Tr\u1ea7n Xu\u00e2n B\u00e1ch <br>\n**Class:** INT3405E_20\n\n# Problem Statement\nKh\u00e1ch h\u00e0ng c\u1ee7a HomeDepot h\u00e0ng ng\u00e0y \u0111\u1ec1u l\u00ean trang t\u00ecm ki\u1ebfm s\u1ea3n ph\u1ea9m c\u1ee7a h\u1ecd \u0111\u1ec3 c\u00f3 th\u1ec3 mua c\u00e1c s\u1ea3n ph\u1ea9m m\u1edbi nh\u1ea5t m\u1ed9t c\u00e1ch nhanh nh\u1ea5t c\u00f3 th\u1ec3 cho nhu c\u1ea7u s\u1eeda ch\u1eefa, c\u1ea3i thi\u1ec7n nh\u00e0 c\u1eeda c\u1ee7a h\u1ecd. Ch\u00ednh v\u00ec v\u1eady h\u1ecd mong mu\u1ed1n ch\u1ec9 v\u1edbi v\u00e0i c\u00e1i click chu\u1ed9t v\u00e0 t\u00ecm ki\u1ebfm th\u00ec c\u00f3 th\u1ec3 t\u00ecm th\u1ea5y ngay s\u1ea3n ph\u1ea9m m\u00e0 h\u1ecd \u0111ang c\u1ea7n nhanh ch\u00f3ng v\u00e0 ch\u00ednh x\u00e1c.<br>\nV\u1edbi cu\u1ed9c thi n\u00e0y Home Depot mu\u1ed1n c\u00e1c Kagglers gi\u00fap h\u1ecd c\u1ea3i thi\u1ec7n tr\u1ea3i nghi\u1ec7m c\u1ee7a kh\u00e1ch h\u00e0ng b\u1eb1ng c\u00e1ch t\u1ea1o ra m\u1ed9t model c\u00f3 th\u1ec3 d\u1ef1 \u0111o\u00e1n ch\u00ednh x\u00e1c s\u1ef1 li\u00ean quan c\u1ee7a t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm c\u1ee7a kh\u00e1ch h\u00e0ng v\u1edbi c\u00e1c s\u1ea3n ph\u1ea9m c\u1ee7a h\u1ecd.<br>\nSearch relevancy l\u00e0 m\u1ed9t th\u01b0\u1edbc \u0111o ng\u1ea7m gi\u00fap Home Depot c\u00f3 th\u1ec3 \u0111\u01b0a \u0111\u1ebfn kh\u00e1ch h\u00e0ng \u0111\u00fang s\u1ea3n ph\u1ea9m ph\u00f9 h\u1ee3p v\u1edbi nhu c\u1ea7u. Hi\u1ec7n t\u1ea1i vi\u1ec7c \u0111\u00e1nh gi\u00e1 relevance c\u00f2n kh\u00e1 th\u1ee7 c\u00f4ng v\u00e0 h\u1ecd mu\u1ed1n gi\u1ea3m thi\u1ebfu ho\u1eb7c lo\u1ea1i b\u1ecf c\u00f4ng s\u1ee9c c\u1ee7a con ng\u01b0\u1eddi c\u1ea7n b\u1ecf ra cho c\u00f4ng vi\u1ec7c n\u00e0y.<br>\n\n## Input:\nD\u1eef li\u1ec7u \u0111\u1ea7u v\u00e0o l\u00e0 th\u00f4ng tin s\u1ea3n ph\u1ea9m (t\u00ean, m\u00f4 t\u1ea3, t\u00ean thu\u1ed9c t\u00ednh v\u00e0 c\u00e1c gi\u00e1 tr\u1ecb c\u1ee7a thu\u1ed9c t\u00ednh), d\u1eef li\u1ec7u v\u1ec1 t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm.\n\n## Output:\nS\u1ed1 \u0111i\u1ec3m relevance (t\u1eeb 1 \u0111\u1ebfn 3) th\u1ec3 hi\u1ec7n m\u1ee9c \u0111\u1ed9 li\u00ean quan c\u1ee7a t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm \u0111\u1ebfn c\u00e1c s\u1ea3n ph\u1ea9m.","539d2408":"**Word2Vec centroid RMSE**, feature n\u00e0y kh\u00f4ng \u0111em l\u1ea1i qu\u00e1 nhi\u1ec1u gi\u00e1 tr\u1ecb, \u0111\u01b0\u1ee3c t\u00ednh b\u1eb1ng c\u00e1ch l\u1ea5y mean c\u1ee7a c\u00e1c vector word trong c\u00e2u r\u1ed3i t\u00ednh RMSE d\u1ef1a tr\u00ean 2 mean vector c\u1ee7a 2 c\u00e2u \u0111\u00f3)","bec0e6c7":"**V\u1ec1 Ridge:** <br>\nB\u1ea3n ch\u1ea5t ch\u00ednh l\u00e0 Linear Regression nh\u01b0ng Loss function \u0111\u01b0\u1ee3c th\u00eam v\u00e0o 1 gi\u00e1 tr\u1ecb penalty kh\u00e1c g\u1ecdi l\u00e0 L2 regularization \u0111\u1ec3 tr\u00e1nh m\u00f4 h\u00ecnh b\u1ecb overfit. L2 c\u00f3 gi\u00e1 tr\u1ecb = t\u1ed5ng b\u00ecnh ph\u01b0\u01a1ng c\u1ee7a c\u00e1c weight m\u00f4 h\u00ecnh \u0111ang h\u1ecdc. L2 gi\u00fap weight c\u00f3 th\u1ec3 duy tr\u00ec \u1edf con s\u1ed1 v\u1eeba ph\u1ea3i (kh\u00f4ng qu\u00e1 l\u1edbn) nh\u1eb1m tr\u00e1nh vi\u1ec7c 1 future c\u00f3 \u1ea3nh h\u01b0\u1edfng qu\u00e1 l\u1edbn \u0111\u1ebfn k\u1ebft qu\u1ea3 m\u00e0 b\u1ecf qua c\u00e1c feature kh\u00e1c d\u1eabn \u0111\u1ebfn overfit. <br>\n![image.png](attachment:9e313b79-884d-4144-bd66-4d852cf1c203.png)","00eaa013":"D\u1eef li\u1ec7u train bao g\u1ed3m 74066 records, ngo\u00e0i ra kh\u00f4ng c\u00f3 ph\u1ea7n t\u1eed n\u00e0o b\u1ecb r\u1ed7ng trong b\u1ea3ng train. Relevance c\u00f3 gi\u00e1 tr\u1ecb trung b\u00ecnh l\u00e0 2.381634 v\u1edbi \u0111\u1ed9 l\u1ec7ch chu\u1ea9n m\u1eabu (sample standard deviation) l\u00e0 0.533984 v\u1edbi s\u1ed1 l\u01b0\u1ee3ng m\u1eabu kh\u00e1 l\u1edbn l\u00e0 74067 th\u00ec \u0111\u1ed9 l\u1ec7ch chu\u1ea9n n\u00e0y kh\u00e1 g\u1ea7n v\u1edbi \u0111\u1ed9 l\u1ec7ch chu\u1ea9n to\u00e0n c\u1ee5c (population standard deviation). V\u00ec v\u1eady model mong \u0111\u1ee3i c\u1ea7n d\u1ef1 \u0111o\u00e1n v\u1edbi sai s\u1ed1 RMSE tr\u00ean t\u1eadp test (split t\u1eeb training set) \u00edt nh\u1ea5t l\u00e0 nh\u1ecf h\u01a1n con s\u1ed1 tr\u00ean (t\u1ed1t h\u01a1n vi\u1ec7c \u0111o\u00e1n to\u00e0n b\u1ed9 l\u00e0 gi\u00e1 tr\u1ecb trung b\u00ecnh 2.38)","c9234af5":"## Number in attributes\nKi\u1ec3m tra trong search term c\u00f3 bao nhi\u00eau s\u1ed1 tr\u00f9ng v\u1edbi c\u00e1c s\u1ed1 c\u00f3 trong attributes c\u1ee7a s\u1ea3n ph\u1ea9m (v\u00ed d\u1ee5 12in. bracket...)","2a6e3454":"\u1ede \u0111\u00e2y em \u0111\u00e3 s\u1eed d\u1ee5ng Gridsearch \u0111\u1ec3 t\u00ecm c\u00e1c hyperparameter cho c\u00e1c m\u00f4 h\u00ecnh \u1edf m\u00e1y local (v\u00ec kaggle ch\u1ea1y r\u1ea5t ch\u1eadm) sau \u0111\u00f3 m\u1edbi stack c\u00e1c model l\u1ea1i \u0111\u1ec3 \u0111\u01b0\u1ee3c m\u1ed9t m\u00f4 h\u00ecnh m\u1ea1nh h\u01a1n v\u1edbi lv2 model l\u00e0 RidgeCV","c4c0cb89":"### Train Set and Test Set","450c32ab":"## Load checkpoint","8133741c":"T\u00ednh cosine similarity \u1edf m\u1ee9c word.<br>\nC\u00f4ng th\u1ee9c nh\u01b0 sau: <br>\n![image.png](attachment:41a83ee7-86e9-40af-9b73-07c711df0d95.png) <br>\nC\u00f4ng th\u1ee9c n\u00e0y s\u1ebd \u0111\u01b0\u1ee3c t\u00ednh theo d\u1ea1ng vector Bag of words. T\u00edch $\\sum_{i=1}^{n} \\text{A}_i \\text{B}_i$ s\u1ebd l\u00e0 vector c\u00e1c t\u1eeb xu\u1ea5t hi\u1ec7n trong c\u1ea3 2 c\u00e2u, m\u1eabu s\u1ed1 ch\u00ednh l\u00e0 t\u00edch c\u0103n b\u1eadc 2 \u0111\u1ed9 d\u00e0i c\u1ee7a 2 c\u00e2u","f0a181a9":"### B\u1ecf k\u00fd t\u1ef1 \u0111\u1eb7c bi\u1ec7t, s\u1eeda ch\u00ednh t\u1ea3","4f23171f":"## Merge c\u00e1c b\u1ea3ng l\u1ea1i v\u1edbi nhau\n\u0110\u1ec3 cho thu\u1eadn ti\u1ec7n trong vi\u1ec7c training v\u00e0 tr\u00edch ch\u1ecdn \u0111\u1eb7c tr\u01b0ng th\u00ec ch\u00fang ta s\u1ebd merge c\u00e1c b\u1ea3ng l\u1ea1i th\u00e0nh m\u1ed9t b\u1ea3ng (d\u1ef1a theo index l\u00e0 uid c\u1ee7a s\u1ea3n ph\u1ea9m)","4ca299f2":"H\u1ea7u h\u1ebft c\u00e1c s\u1ea3n ph\u1ea9m \u0111\u1ec1u c\u00f3 brand name v\u00e0 m\u1ed9t v\u00e0i thu\u1ed9c t\u00ednh kh\u00e1c","68ec0e5d":"## Drop null value","8d19a50f":"# Feature Engineering","441b71ec":"C\u00f3 t\u1ed5ng c\u1ed9ng 27699 s\u1ea3n ph\u1ea9m tr\u00f9ng l\u1eb7p gi\u1eefa train set v\u00e0 test set.<br>\nH\u1ea7u h\u1ebft c\u00e1c search_term trong train set \u0111\u1ec1u t\u1ed3n t\u1ea1i trong test set <br>\nC\u00e1c c\u1ee5m search_term v\u00e0 s\u1ea3n ph\u1ea9m kh\u00f4ng c\u00f3 s\u1ef1 tr\u00f9ng l\u1eb7p gi\u1eefa 2 t\u1eadp train v\u00e0 test","f3bb96df":"## Search term include brand ?\nKi\u1ec3u tra s\u1ed1 l\u1ea7n Brand Name xu\u1ea5t hi\u1ec7n trong search_term (nh\u1eefng search_term c\u00f3 brand c\u1ee7a s\u1ea3n ph\u1ea9m \u0111ang x\u00e9t th\u01b0\u1eddng s\u1ebd c\u00f3 relevance cao h\u01a1n)","da50222c":"### Merge description v\u00e0o b\u1ea3ng train","4f3ee77d":"## Last word in title\nTh\u00f4ng th\u01b0\u1eddng, khi t\u00ecm ki\u1ebfm m\u1ed9t s\u1ea3n ph\u1ea9m n\u00e0o \u0111\u00f3 (English) th\u00ec ng\u01b0\u1eddi ta s\u1ebd th\u01b0\u1eddng t\u00ecm ki\u1ebfm theo format thu\u1ed9c t\u00ednh + s\u1ea3n ph\u1ea9m. Ch\u00ednh v\u00ec v\u1eady t\u1eeb kh\u00f3a cu\u1ed1i th\u01b0\u1eddng l\u00e0 t\u00ean c\u1ee7a lo\u1ea1i s\u1ea3n ph\u1ea9m \u0111\u00f3. V\u00ec v\u1eady t\u1eeb kh\u00f3a n\u00e0y s\u1ebd c\u00f3 \u1ea3nh h\u01b0\u1edfng \u0111\u1ebfn relevance c\u1ee7a s\u1ea3n ph\u1ea9m.","9364dead":"## Common word count","03b9a4c0":"\u0110\u1ebfm s\u1ed1 l\u01b0\u1ee3ng c\u00e1c t\u1eeb c\u00f3 chung gi\u1eefa search_term v\u00e0 title, description, attributes (theo d\u1ea1ng ngrams v\u00ed d\u1ee5 d\u01b0\u1edbi code 2 c\u00e2u ```this is a test``` v\u00e0 ```abc is a abc a test``` c\u00f3 2 bigram chung l\u00e0 ```is a``` v\u00e0 ```a test```). \u1ede \u0111\u00e2y ch\u1ec9 s\u1eed d\u1ee5ng ngrams = 1 v\u00e0 2","f480fb64":"## Color in title, description, attributes\nKi\u1ec3m tra c\u00f3 m\u00e0u s\u1eafc chung xu\u1ea5t hi\u1ec7n trong pair search_term v\u1edbi title, search_term v\u1edbi description, search_term v\u1edbi attributes. Danh s\u00e1ch c\u00e1c word m\u00e0u s\u1eafc \u0111\u01b0\u1ee3c l\u1ea5y t\u1eeb repo c\u1ee7a top 3 private leaderboard, \u0111\u00e3 \u0111\u01b0\u1ee3c chia s\u1ebb tr\u00ean forum (team Turing Test), [Link Repo](https:\/\/github.com\/ChenglongChen\/kaggle-HomeDepot\/blob\/master\/Data\/dict\/color_data.py)","17461e0a":"# Model","f7bb26cc":"## File Description\n* ```train.csv``` - Ch\u1ee9a c\u00e1c th\u00f4ng tin v\u1ec1 s\u1ea3n ph\u1ea9m (id, ti\u00eau \u0111\u1ec1), t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm v\u00e0 \u0111\u00e1nh gi\u00e1 s\u1ef1 li\u00ean quan c\u1ee7a t\u1eeb kh\u00f3a \u0111\u00f3 v\u1edbi s\u1ea3n ph\u1ea9m theo thang \u0111i\u1ec3m t\u1eeb 1 \u0111\u1ebfn 3.\n* ```test.csv``` - T\u01b0\u01a1ng t\u1ef1 v\u1edbi training set ngo\u1ea1i tr\u1eeb kh\u00f4ng c\u00f3 c\u1ed9t ch\u1ee9a thang \u0111i\u1ec3m \u0111\u00e1nh gi\u00e1 s\u1ef1 li\u00ean quan c\u1ee7a t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm v\u1edbi s\u1ea3n ph\u1ea9m.\n* ```product_descriptions.csv``` - Ch\u1ee9a th\u00f4ng tin m\u00f4 t\u1ea3 s\u1ea3n ph\u1ea9m (id, m\u00f4 t\u1ea3) d\u01b0\u1edbi d\u1ea1ng text\n* ```attributes.csv``` - Ch\u1ee9a c\u00e1c thu\u1ed9c t\u00ednh c\u1ee7a s\u1ea3n ph\u1ea9m (m\u1ed7i h\u00e0ng s\u1ebd l\u00e0 id s\u1ea3n ph\u1ea9m, t\u00ean thu\u1ed9c t\u00ednh, gi\u00e1 tr\u1ecb thu\u1ed9c t\u00ednh)\n* ```sample_submission.csv``` - File submission m\u1eabu c\u1ee7a cu\u1ed9c thi\n* ```relevance_instructions.docx``` - Ch\u1ee9a ch\u1ec9 d\u1eabn v\u1ec1 c\u00e1ch \u0111\u00e1nh gi\u00e1 \u0111i\u1ec3m c\u1ee7a t\u1eeb kh\u00f3a t\u00ecm ki\u1ebfm v\u1edbi s\u1ea3n ph\u1ea9m (theo thang t\u1eeb 1 \u0111\u1ebfn 3 do 3 ng\u01b0\u1eddi \u0111\u00e1nh gi\u00e1 v\u00e0 l\u1ea5y gi\u00e1 tr\u1ecb trung b\u00ecnh)","d9af1f8f":"D\u1eef li\u1ec7u \u0111\u01b0\u1ee3c s\u1eed d\u1ee5ng \u0111\u1ec3 fit TFIDF Vectorizer \u1edf \u0111\u00e2y l\u00e0 d\u1eef li\u1ec7u v\u1ec1 title + description c\u1ee7a c\u00e1c s\u1ea3n ph\u1ea9m trong dataset (\u0111\u00e3 lo\u1ea1i b\u1ecf tr\u00f9ng l\u1eb7p).<br>\nVectorizer \u0111\u01b0\u1ee3c config l\u1ea5y t\u1ed1i \u0111a 10000 word (max_features) \u0111\u01b0\u1ee3c l\u1ea5y theo th\u1ee9 t\u1ef1 t\u1eeb cao \u0111\u1ebfn b\u00e9 gi\u00e1 tr\u1ecb TF c\u1ee7a t\u1eeb.<br>\nB\u1edfi v\u00ec s\u1ed1 l\u01b0\u1ee3ng chi\u1ec1u l\u00e0 10000 qu\u00e1 l\u1edbn n\u00ean c\u1ea7n thi\u1ebft ph\u1ea3i s\u1eed d\u1ee5ng 1 ph\u01b0\u01a1ng ph\u00e1p gi\u1ea3m b\u1edbt s\u1ed1 chi\u1ec1u c\u1ee7a vector m\u00e0 v\u1eabn gi\u1eef l\u1ea1i \u0111\u01b0\u1ee3c nhi\u1ec1u th\u00f4ng tin ch\u00ednh. C\u00f3 th\u1ec3 s\u1eed d\u1ee5ng SVD ho\u1eb7c PCA, \u1edf \u0111\u00e2y em d\u00f9ng TruncatedSVD c\u1ee7a sklearn \u0111\u1ec3 gi\u1ea3m s\u1ed1 chi\u1ec1u t\u1eeb 10000 xu\u1ed1ng c\u00f2n 500.","b1f7b2e1":"\u1ede \u0111\u00e2y th\u1ef1c hi\u1ec7n vi\u1ec7c concat c\u00e1c vector tfidf c\u1ee7a search_term v\u00e0 title, description v\u00e0o vector feature. B\u01b0\u1edbc n\u00e0y l\u00e0m theo 1 notebook trong forrum c\u1ee7a kaggle. (em kh\u00f4ng hi\u1ec3u l\u00fd do t\u1ea1i sao nh\u01b0ng vi\u1ec7c n\u00e0y th\u1ef1c s\u1ef1 improve k\u1ebft qu\u1ea3 submission)","1c8df6ff":"C\u00e1c search_term trong train set v\u00e0 test set c\u00f3 \u0111\u1ed9 d\u00e0i kh\u00e1 t\u01b0\u01a1ng \u0111\u1ed3ng v\u00e0 ph\u00e2n ph\u1ed1i g\u1ea7n ph\u00e2n ph\u1ed1i chu\u1ea9n v\u1edbi mean l\u00e0 kho\u1ea3ng 16"}}