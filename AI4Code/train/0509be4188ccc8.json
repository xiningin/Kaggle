{"cell_type":{"cd6b4aa6":"code","410cf73f":"code","451854b9":"code","82f3cb37":"code","176ff4c8":"code","0d7d6a38":"code","7bca93b2":"code","7fca8be1":"code","7b33c332":"code","3f8fd240":"code","db633680":"code","2ecf8cfb":"code","cf06ad9c":"code","aaf067ed":"code","1cfe0d31":"code","75951b44":"code","6f81b3a3":"code","a886a499":"code","58a0211f":"code","61f89ffc":"code","ae7df7da":"code","7b8a15a1":"code","a5c01439":"code","3e9730e5":"code","ece5ce3d":"code","685390d7":"code","8a6934f7":"code","8a3293e2":"code","3dd070b2":"code","70c4cbbc":"code","2bcb9fc9":"code","0d0a734a":"code","eb05f9af":"code","b54fa4f4":"code","4a2cb785":"code","f5616746":"code","b2f5d14d":"code","c3e6bbf4":"code","63ce562f":"code","63ea2964":"code","a1b259e4":"code","d8b49928":"code","770c51a4":"code","b4d720ea":"code","dc63e749":"code","3d060092":"code","10303978":"code","63532a45":"code","74699dce":"code","eb907518":"code","25fc045b":"code","aa23b169":"code","99cb860d":"code","0ba8770f":"code","b8d5857a":"code","035fe261":"code","7e0128b2":"code","6c502774":"markdown","d1653f53":"markdown","600958cb":"markdown","0179cc1a":"markdown","83673180":"markdown","e4aa8e4a":"markdown","c006688c":"markdown","67db7ab3":"markdown","f47b3891":"markdown","3ced78e2":"markdown","cc75de93":"markdown","f31cf2b1":"markdown","6837b27b":"markdown","9109cd98":"markdown","2bce5c92":"markdown","210a121c":"markdown","f821e854":"markdown","c560bf29":"markdown","3a632320":"markdown","c0992556":"markdown","d26aff55":"markdown","4add44fe":"markdown","a0169841":"markdown","2f4f0656":"markdown","15df6533":"markdown","bdb05f7b":"markdown","b69177a2":"markdown","140b3a53":"markdown","16db2124":"markdown","c6fa54b3":"markdown","dfb1129f":"markdown","bd074c8d":"markdown","1dd9b448":"markdown","e2968ae5":"markdown","91ef19f4":"markdown","fb98ccee":"markdown","9215c8c7":"markdown","758a90c2":"markdown","4264b823":"markdown","4bcb93ef":"markdown","cfe03178":"markdown","0f4e45c7":"markdown","6411f3d3":"markdown","b9b9bd01":"markdown","37cabc4f":"markdown","b10c1524":"markdown","4bcc4ad3":"markdown","fcb5f2fb":"markdown","8b89e093":"markdown","2bec62f3":"markdown","1843409d":"markdown","cd27e719":"markdown","70cc571f":"markdown","212c98d5":"markdown","4e2e4140":"markdown","3c8ac27d":"markdown","56ae0faf":"markdown","14f06d5a":"markdown","af62b279":"markdown","5a83efbb":"markdown","c1930f34":"markdown","171eb85e":"markdown","8c2b8dc4":"markdown","fb619649":"markdown","8a7bb34d":"markdown","454030bf":"markdown","51cfb4b6":"markdown","33de476e":"markdown","4fd7efb7":"markdown","87ca1c8f":"markdown","c0007380":"markdown","73d0e3b1":"markdown","13ff74cb":"markdown","8cacc590":"markdown","14c6a465":"markdown","f971d8f6":"markdown","12ff1dbb":"markdown","e3557d23":"markdown","471655ab":"markdown","f14d72f6":"markdown","be7b46b2":"markdown","8c146183":"markdown"},"source":{"cd6b4aa6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# To split into training-test data set\nfrom sklearn.model_selection import train_test_split\n\n# To scale the dataset\nfrom sklearn.preprocessing import MinMaxScaler\n\n# To get statistical information of the model\nimport statsmodels.api as sm\n\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# To calculate the R-squared score, RMSE, MAE on the test set.\nfrom sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\nfrom math import sqrt\n\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","410cf73f":"import warnings\nwarnings.filterwarnings('ignore')","451854b9":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        file_name = os.path.join(dirname, filename)\n        print(file_name)\n        \nbike_df = pd.read_csv(file_name)\nbike_df.head() # Checking the top 5 rows of the dataframe","82f3cb37":"# checking botton 5 rows of the dataframe\nbike_df.tail()","176ff4c8":"# Checking the shape of the dataframe\nbike_df.shape","0d7d6a38":"# Checking the size of the dataframe\nbike_df.size","7bca93b2":"# Inspecting type\nprint(bike_df.dtypes)","7fca8be1":"# How many types of each data type column exists and total memory usage\nbike_df.info()","7b33c332":"# Looking for any null value in any column \nprint(bike_df.isnull().any())","3f8fd240":"# Checking the numerical columns data distribution\nbike_df.describe()","db633680":"# Checking the number of unique values each column possess to identify categorical columns\nbike_df.nunique().sort_values()","2ecf8cfb":"categorical_columns = ['season','mnth','weekday','weathersit']\nfor col in categorical_columns:\n    bike_df[col] =pd.Categorical(bike_df[col])\nbike_df.info() # Observe the data frame after conversion","cf06ad9c":"sns.pairplot(bike_df[['temp','atemp','hum','windspeed','cnt']])\nplt.show()","aaf067ed":"# Let's check the correlation coefficients to see which variables are highly correlated\nplt.figure(figsize = (16, 10))\nsns.heatmap(bike_df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","1cfe0d31":"drop_cols = ['instant', 'dteday', 'casual', 'registered','atemp']\nbike_df.drop(labels=drop_cols,axis=1,inplace=True)\nbike_df.info()","75951b44":"plt.figure(figsize=(20, 12))\n\nfeatures = ['yr','mnth','season','weathersit','holiday','workingday','weekday']\n\nfor i in enumerate(features):\n    plt.subplot(2,4,i[0]+1)\n    sns.boxplot(x = i[1], y = 'cnt', data = bike_df)\n    plt.title(i[1])","6f81b3a3":"plt.figure(figsize=(20, 6))\n\nfeatures = ['temp','hum','windspeed','cnt']\n\nfor i in enumerate(features):\n    plt.subplot(1,4,i[0]+1)\n    sns.distplot(bike_df[i[1]])\n    plt.title(i[1])\nplt.show()","a886a499":"# Replacing the numbers with strings from data dictionary so that the column names will be meaningful\n\nbike_df[['season']]=bike_df[['season']].apply(lambda x: x.map({1: \"spring\", 2: \"summer\", 3:\"fall\", 4:\"winter\"}))\nbike_df[['mnth']]=bike_df[['mnth']].apply(lambda x: x.map({1:\"jan\",2:\"feb\",3:\"mar\",4:\"apr\",5:\"may\",6:\"jun\",7:\"jul\",8:\"aug\",9:\"sep\",10:\"oct\",11:\"nov\",12:\"dec\"}))\nbike_df[['weekday']]=bike_df[['weekday']].apply(lambda x: x.map({1:\"mon\",2:\"tue\",3:\"wed\",4:\"thur\",5:\"fri\",6:\"sat\",0:\"sun\"}))\nbike_df[['weathersit']]=bike_df[['weathersit']].apply(lambda x: x.map({1:\"clear\",2:\"mist\",3:\"snow\",4:\"rain\"}))\n\nbike_df.head()","58a0211f":"# Let's drop the first column from corresponding dummy variables in df using 'drop_first = True'\nseason = pd.get_dummies(bike_df['season'], drop_first = True)\nmnth = pd.get_dummies(bike_df['mnth'], drop_first = True)\nweekday = pd.get_dummies(bike_df['weekday'], drop_first = True)\nweathersit = pd.get_dummies(bike_df['weathersit'], drop_first = True)\n\n# Add the results to the original bike sharing dataframe\nbike_df = pd.concat([bike_df, season,mnth,weekday,weathersit], axis = 1)\n\n# Drop the categorical variables as we have created the dummies for it\nbike_df.drop(['season','mnth','weekday','weathersit'], axis = 1, inplace = True)\nbike_df.head()","61f89ffc":"bike_df.info() # Observe the dataframe after creation of dummy variables and dropping categorical variables","ae7df7da":"df_train, df_test = train_test_split(bike_df, train_size = 0.70, test_size = 0.30, random_state = 10) ","7b8a15a1":"df_train.shape # Observe the train dataset shape","a5c01439":"df_test.shape# Observe the train dataset shape","3e9730e5":"df_train.isnull().any()","ece5ce3d":"df_test.isnull().any()","685390d7":"scaler = MinMaxScaler()\n# Apply scaler() to all the columns(only for continuous variables) except the 'yes-no' and 'dummy' variables\nnum_vars = ['temp','hum','windspeed','cnt']\n\ndf_train[num_vars] = scaler.fit_transform(df_train[num_vars])\ndf_train.head()","8a6934f7":"df_train.describe()","8a3293e2":"y_train = df_train.pop('cnt') # target variable \nX_train_initial = df_train # target variable should not be in predictor list\nprint(X_train_initial.head(),\"\\nY train:\\n\",y_train.head())","3dd070b2":"# Use linear regression as the model and RFE to select 15 features\nlr = LinearRegression()\n#rank all features, i.e continue the elimination until the last one\nrfe = RFE(lr, n_features_to_select=15)\nrfe = rfe.fit(X_train_initial, y_train)\n\nprint(\"All features by their rank:\")\nprint(sorted(list(zip(X_train_initial.columns,rfe.support_,rfe.ranking_))))","70c4cbbc":"col = X_train_initial.columns[rfe.support_]\nprint(\"RFE selected columns:\\n\", col)","2bcb9fc9":"def build_lr_model(feature_list):\n    X_train = X_train_initial[feature_list] # get feature list \n    X_train_lm = sm.add_constant(X_train) # required by statsmodels \n    lr = sm.OLS(y_train, X_train_lm).fit() # build model and learn coefficients\n    print(\"Co-efficients:\\n\",lr.params) # OLS coefficients\n    print(lr.summary()) # OLS summary with R-squared, adjusted R-squared, p-value etc.\n    calculate_vif(X_train) # Calculate VIF for features\n    return(lr, X_train_lm) # return the model and the X_train fitted with constant\n\ndef calculate_vif(X_train):\n    vif = pd.DataFrame()\n    vif['Features'] = X_train.columns # Read the feature names\n    vif['ViF'] = [variance_inflation_factor(X_train.values,i) for i in range(X_train.shape[1])] # calculate VIF\n    vif['ViF'] = round(vif['ViF'],2)\n    vif.sort_values(by='ViF', ascending = False, inplace=True)  \n    print(vif) # prints the calculated VIFs for all the features","0d0a734a":"features = list(col) #  Use RFE selected variables\nlr_model1, X_train_lm1 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","eb05f9af":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from RFE features list\nlr_model2, X_train_lm2 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","b54fa4f4":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from RFE features list as per model 2\nfeatures.remove('temp') # Remove 'temp' from RFE features list\nlr_model3, X_train_lm3 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","4a2cb785":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nlr_model2, X_train_lm2 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","f5616746":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum')  # Remove 'hum' from features list\nlr_model4, X_train_lm4 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","b2f5d14d":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum')  # Remove 'hum' from features list as per model 4\nfeatures.remove('aug') # Remove 'aug' from features list\nlr_model5, X_train_lm5 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","c3e6bbf4":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum') # Remove 'hum' from features list as per model 4 \nfeatures.remove('aug') # Remove 'aug' from features list as per model 5\nfeatures.remove('fall') # Remove 'aug' from features list\nlr_model6, X_train_lm6 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","63ce562f":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum') # Remove 'hum' from features list as per model 4 \nfeatures.remove('aug') # Remove 'aug' from features list as per model 5\nfeatures.remove('fall') # Remove 'aug' from features list as per model 6\nfeatures.remove('mar') # Remove 'fall' from features list\nlr_model7, X_train_lm7 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","63ea2964":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum') # Remove 'hum' from features list as per model 4 \nfeatures.remove('aug') # Remove 'aug' from features list as per model 5\nfeatures.remove('fall') # Remove 'aug' from features list as per model 6\nfeatures.remove('mar') # Remove 'fall' from features list as per model 7\nfeatures.remove('oct') # Remove 'fall' from features list\nlr_model8, X_train_lm8 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","a1b259e4":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum') # Remove 'hum' from features list as per model 4 \nfeatures.remove('aug') # Remove 'aug' from features list as per model 5\nfeatures.remove('fall') # Remove 'aug' from features list as per model 6\nfeatures.remove('mar') # Remove 'fall' from features list as per model 7\nfeatures.remove('oct') # Remove 'fall' from features list as per model 8\nfeatures.remove('mist') # Remove 'mist' from features list\nlr_model9, X_train_lm9 = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","d8b49928":"features = list(col) #  Use RFE selected variables\nfeatures.remove('may') # Remove 'may' from features list as per model 2\nfeatures.remove('hum') # Remove 'hum' from features list as per model 4 \nfeatures.remove('aug') # Remove 'aug' from features list as per model 5\nfeatures.remove('fall') # Remove 'fall' from features list as per model 6\nfeatures.remove('mar') # Remove 'mar' from features list as per model 7\nfeatures.remove('oct') # Remove 'oct' from features list as per model 8\nlr_model, X_train_lm = build_lr_model(features) # Call the function and get the model and the X_train_lm for prediction","770c51a4":"lr_model.params","b4d720ea":"lr_model.summary()","dc63e749":"y_train_predicted = lr_model.predict(X_train_lm) # get predicted value on training dataset using statsmodels predict()\nresidual_values = y_train - y_train_predicted # difference in actual Y and predicted value\nplt.figure(figsize=[10,5])\nplt.subplot(121)\nsns.distplot(residual_values, bins = 15) # Plot the histogram of the error terms\nplt.title('Residuals follow normal distribution', fontsize = 18)\nplt.subplot(122) \nplt.scatter(y_train_predicted, residual_values) # Residual vs Fitted Values\nplt.plot([0,0],'r') # draw line at 0,0 to show that residuals have constant variance\nplt.title('Residual vs Fitted Values: No Pattern Seen')\nplt.show()","3d060092":"num_vars = ['temp','hum','windspeed','cnt']\n\ndf_test[num_vars] = scaler.transform(df_test[num_vars]) # Use the scaler of training data set and transform test dataset\ny_test = df_test.pop('cnt') # actual target values \nX_test = df_test # remove target variable from the features","10303978":"# Creating X_test_model dataframe by selecting features of the model\nprint(\"Model features are \", features)\nX_test_model = X_test[features] # features have the list of variables in the model\nprint(\"Checking test data set features: \" , X_test_model.columns)\n\nX_test_model = sm.add_constant(X_test_model) # Adding constant variable as required by statsmodels\n\n# Making predictions using the final model\ny_predicted_model = lr_model.predict(X_test_model)","63532a45":"rmse = sqrt(mean_squared_error(y_test,y_predicted_model))\nprint('Root mean square error :',rmse)","74699dce":"mae=mean_absolute_error(y_test,y_predicted_model)\nprint('Mean absolute error :',mae)","eb907518":"train_r2 = round(r2_score(y_train, y_train_predicted),3)\ntrain_r2","25fc045b":"n = df_train.shape[0]\np = len(features)\nprint(n,p)","aa23b169":"round(1-(1-train_r2)*(n-1)\/(n-p-1),3) ","99cb860d":"test_r2 = round(r2_score(y_test, y_predicted_model),4)\ntest_r2","0ba8770f":"n = df_test.shape[0]\np = len(features)\nprint(n,p)","b8d5857a":"round(1-(1-test_r2)*(n-1)\/(n-p-1),4)","035fe261":"features","7e0128b2":"lr_model.params","6c502774":"Low RMSE and MAE values indicate that the model is good.","d1653f53":"### 4.5 Evaluate the prediction of the model using R2score and RMSE","600958cb":"#### Visualising Categorical Variables\n\nLet's visualize the categorical variables against count of rentals","0179cc1a":"### 4.3 Scaling the numerical features on the test data set\n\nNow that we have fitted the model and checked the normality of error terms and proved the assumptions of linear regression hold true for this model, it's time to go ahead and make predictions using the final model.\n\nUse __transform()__ on test data set using the scaler created with scaled parameters of training data set for prediction.","83673180":"## 0. Import necessary libraries\n\nIgnore warnings and import necessary libraries:\n\n### 0.1 Importing Libraries\n","e4aa8e4a":"#### FINAL COMMENTS and SUGGESTIONS","c006688c":"#### INFERENCES:\n\n- `yr` boxplot indicate that the demand for the bike has increased in 2019 compared to 2018.\n- `working day` and `holiday` box plots indicate that more bicycles are rent during normal working days than on weekends or holidays. \n- From the `weathersit` plot, we can observe that during `clear,partly cloudy` weather the bike rental count is highest and the second highest is during `mist cloudy` weather and followed by during `light snow` and there is no rental data during `heavy rain` weather.\n- From the `season` boxplot, we can observe that increasing the bike rental count in `fall` and `summer` season and then decreasing bike rental count in `spring` season\n- There are very few outliers which we can ignore.\n","67db7ab3":"### 3.1 : Dividing into X and Y sets for the model building","f47b3891":"The bike rent count can be predicted using the following linear algebric equation identified by the model:\n\ncnt = 0.0973 + 0.55  \\times  **temp** - 0.2659 \\times **weathersit being 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds** + 0.25 \\times **yr being 2019** - 0.1386 \\times **windspeed** + 0.1457 \\times **season being 4:winter** + 0.1011 \\times **mnth being 9:sep** + 0.0932 \\times **season being 2:summer** - 0.0834 \\times **weathersit being 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist** - 0.0974 \\times **holiday according to http:\/\/dchr.dc.gov\/page\/holiday-schedule**","3ced78e2":"### 2.3 Scaling the numerical features on the training data set","cc75de93":"## Step 3: Building the model","f31cf2b1":"#### Answering the original questions of this task:\n\n1. Which variables are significant in predicting the demand for shared bikes?","6837b27b":"#### Null check on training and test dataset","9109cd98":"The difference between R2 score of train and test data set seem less than 5 percent hence this model is good for prediction","2bce5c92":"We specify `random_state` so that the train and test data set always have the same rows; changing random state will change the analysis and the model. ","210a121c":"### 3.6 Best Fit Line:\n\nWe can formulate the equation of our best fitted line as\n\ncnt = 0.0973 + 0.55  \\times  **temp** - 0.2659 \\times **weathersit being 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds** + 0.25 \\times **yr being 2019** - 0.1386 \\times **windspeed** + 0.1457 \\times **season being 4:winter** + 0.1011 \\times **mnth being 9:sep** + 0.0932 \\times **season being 2:summer** - 0.0834 \\times **weathersit being 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist** - 0.0974 \\times **holiday according to http:\/\/dchr.dc.gov\/page\/holiday-schedule**\n\nwhere `weathersit being 3: Light Snow` is boolean value and if its true, the number of bike rentals go down by 0.2659 and so on for other categorical variables. ","f821e854":"#### Univariate distribution","c560bf29":"#### Model 8: Remove `oct` whose OLS coefficient is 0.05","3a632320":"## Problem Statement\nA bike-sharing system is a service in which bikes are made available for shared use to individuals on a short term basis for a price or free. Many bike share systems allow people to borrow a bike from a \"dock\" which is usually computer-controlled wherein the user enters the payment information, and the system unlocks it. This bike can then be returned to another dock belonging to the same system.\n\n\nA US bike-sharing provider BoomBikes has recently suffered considerable dips in their revenues due to the ongoing Corona pandemic. The company is finding it very difficult to sustain in the current market scenario. So, it has decided to come up with a mindful business plan to be able to accelerate its revenue as soon as the ongoing lockdown comes to an end, and the economy restores to a healthy state. \n\n\nIn such an attempt, BoomBikes aspires to understand the demand for shared bikes among the people after this ongoing quarantine situation ends across the nation due to Covid-19. They have planned this to prepare themselves to cater to the people's needs once the situation gets better all around and stand out from other service providers and make huge profits.\n\n\nThey have contracted a consulting company to understand the factors on which the demand for these shared bikes depends. Specifically, they want to understand the factors affecting the demand for these shared bikes in the American market. The company wants to know:\n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n\nBased on various meteorological surveys and people's styles, the service provider firm has gathered a large dataset on daily bike demands across the American market based on some factors. ","c0992556":"#### Test R2 score","d26aff55":"Both `nunique()` and `describe()` shows that there are some categorical variables in the dataframe which are in int format\n\n- Columns such as `yr`, `holiday`, `workingday` have only 2 distinct values and hence its ok to keep these as int and use the same columns.\n- Columns such as `weathersit`, `season`, `weekday`, `mnth` have around 3 to 12 distinct values and hence we will convert these columns to categorical data type.","4add44fe":"#### Model 3: Remove `temp` as its VIF is higher","a0169841":"#### Steps:\n1. Reading, understanding and visualing the data\n2. Preprocessing the data\n   - Encode the categorical variables using dummy variables.\n   - Divide into Train and Test data set.\n   - Scaling on training data set\n3. Building the model\n   - Create X and Y variables\n   - Use RFE and build initial model with RFE recommended features.\n   - Feature selection and Iteratively work on better model by dropping features based on p-value and VIF\n4. Model Evaluation\n   - Prove that the assumptions of linear regression are true\n   - Residual analysis\n   - Scaling on test data set\n   - Predict target variable using the fitted model\n   - Evaluate the prediction on the test data set\n5. Conclusion","2f4f0656":"Now all the p-values are less than 0.05 indicating that all these features are signicant. \n\nVIF is also in acceptable range. \n\nTotal number of features in this `model6` is 11, which is slightly above our targeted features.\n\nSo lets check deleting `mar`(OLS coefficient: 0.04) or `oct`(0.05) which have very less OLS coefficient one by one and see how the model is affected.\n\nWe are going for less OLS coefficient as these will have less impact by the best fit line equation. i.e. one unit of change in these features will only bring about 0.04 or 0.05 units of change in the target variable.","15df6533":"### 3.4 FINAL MODEL","bdb05f7b":"#### Root mean square error ( RMSE )\n","b69177a2":"#### Null Value analysis","140b3a53":"The impact is not much on the model. Rsquared has reduced from 0.83 to 0.827 and adjusted Rsquared has reduced from 0.826 to 0.824.\n\nSo it seems removing `oct` does not have much impact and hence lets drop this.\n\nWe now have 9 features and let try deleting `mist` whose OLS coefficient is -0.08 and see how the model stats are.","16db2124":"## Build a multiple linear regression model for the prediction of demand for shared bikes","c6fa54b3":"Inspect the various aspects of the `bike_df` dataframe such as \n- `shape` for number of rows and columns\n- `size` for memory usage\n- `info()` for the presence of null values\n- `describe()` for statistical information","dfb1129f":"#### Model 4: Remove `hum` as its VIF is higher","bd074c8d":"#### Model evaluation","1dd9b448":"Rsquared is not affected much. So removing `hum` is a good decision for the model. Lets proceed and check the stats.\n\nNow p-value of `aug` is greater than 0.05 and hence we can drop this.","e2968ae5":"So `BoomBikes` can anticipate the demand for bikes \n- on workingdays \n- in the month of September\n- when the temperature is between 10 and 30\n- when the windspeed is less than 20\n- when the season is summer or winter\n- weather is clear\n\nand with year, the demand seem to be increasing and hence once the corona pandemic effect is gone, the demand for bikes will increase according to the above features.","91ef19f4":"## Step 1.2 Visualizing the dataset\n\n- We can identify whether any of the predictive features have association with the outcome variable.\n- Find if there is any multicollinearity - any independent feature is correlated with another independent feature \n\nLets visualize only the numerical variables and see if there is scope for linear regression:","fb98ccee":"- The categorical variables are converted to dummy variables.\n- Dummy variables are concatenated with the original `bike_df` data frame\n- Hence the actual categorical variables such as `season`, `mnth`, `weekday`, `weathersit` are deleted from the `bike_df`","9215c8c7":"#### Mean absolute error (MAE)","758a90c2":"#### INFERENCES:\n\n- Looking at the pairplot among the numerical variables, the variables **temp** and **atemp** seem to have highest correlation with the target variable **cnt**\n- There seem to be linear relationship between `temp`, `atemp`, other parameters and `cnt` which is target variable\n- There seem to be strong correlation between `temp` and `atemp`\n- Higher temperatures lead to an increasing number of bike rents and lower temperatures decrease the number of rents.\n- There are few outliers in all the variables - `temp`, `atemp`, `hum`, `windspeed` which we can ignore for now.\n- There are some of the zero number of rentals for few of the temperatures, atemp, humidity etc.  \n\nLets check correlation matrix to detect `multicolinearity`","4264b823":"## Step 1: Exploratory Data Analysis\n## Step 1.1 : Reading and understanding data:\nRead the given dataset into a dataframe called `bike_df`","4bcb93ef":"n=510 for training data set and p=9","cfe03178":"## 5. CONCLUSION","0f4e45c7":"## 4 Model Evaluation\n\n### 4.1 Assumptions of Linear Regression:\n\nLinear regression model is based on the following assumption and we will prove these assumptions with the model we built:\n\n1. **Linear relationship** between X and Y: This was proved in the pair plot\n2. Error terms are **normally distributed**\n3. Error terms are **independent of each other**\n4. Error terms have constant variance (**homoscedasticity**)","6411f3d3":"#### Model 1 : Use all 15 RFE selected features","b9b9bd01":"### 4.2 Residual analysis on training dataset","37cabc4f":"#### Train Adjusted R2 score:\n\nadj r2=1-(1-R2)*(n-1)\/(n-p-1)\n\nwhere n is the sample size\nand p is the number of features","b10c1524":"This score matches the statsmodels summary R2 and adjusted R2.","4bcc4ad3":"#### Model 7: Remove `mar` whose OLS coefficient is 0.04 ","fcb5f2fb":"## 3.5 Interpreting the regression results\n\n- R-squared of the model(82.7%) and adjusted R-sqaured(82.4%) indicate that the selected `9` features are significant for prediction.\n- P(F-statistic) of the model is close to 0 which indicates that this model is a good fit.\n- Coefficients of the features indicate how they are related with `cnt`.\n\n\nTop 3 significant features that contribute towards explaining towards the demand of the shared bikes:\n1. **temp** with coefficient 0.55 indicating as temperature increases, the demand for bikes increase.\n2. **snow** with coefficient -0.2659 indicating when there is snow, there is less demand for bikes.\n3. **yr** with coefficient 0.25 indicating that the bike demand has increased from last year and is expected to increase in future.","8b89e093":"One row seem to have missed from train_test_split. Lets ignore for now as its just one row. There are no null values in both the dataframes.","2bec62f3":"Features such as \n- Temperature in Celsius\n- Weathersit being `Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist` or `Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds` have negative impact\n- Year\n- Windspeed having negative impact\n- Season being `summer` or `winter` have positive impact\n- Month being `September` having positive impact\n- Day being `holiday` have negative impact\n\non bike demand in the order detailed here.\n","1843409d":"#### Model 2: Remove `may` as its p-value > 0.05","cd27e719":"#### Train R2 score","70cc571f":"## Step 2: Data Preparation\n\n### 2.1 Dummy Variables\n- Linear regression assumes that all the numerical values are independent data points.\n- When we have a categorical variable in a regression which takes more than two values, we can represent them using `dummy variables`\n- The `dummy variables` will have only two quantitative values i.e. 0 and 1 and is easier to interpret.\n- When the variable is present, it will take the value of 1 and absence of the variable is indicated by 0.\n- For e.g. the variable `season` has four values like 1,2,3,4. We need to convert these levels into integer as well. \nBut since these dummy variables would have column names as 1,2,3,4 and it will not give any meaning, its better to use the `data dictionary` provided and map these values to string\n- For any categorical variable with `k` distinct values, we would need to create `k-1` dummy variables.\n- For e.g. `season` have 4 distinct values and we can map those 4 values with 3 columns:\n\n| summer | fall | winter |\n|------|------|------|\n|  1   |  0   |   0  |\n|  0   |  0   |   1  |\n|  0   |  1   |   0  |\n|  0   |  0   |   0  |\n\nAll zeros would indicate `spring` and hence it does not require a separate column.\n\nOnce we have encoded the categorical variable as dummy variable, it can be used in the below linear regression equation:\n$cnt = const + m_1x_1 + m_2x_2 + ... + m_nx_n$\n\nwhere \n-  $x_1$ is the coefficient for the first level of categorical feature e.g. `summer`\n-  $x_2$ is the coefficient for the second level of categorical feature e.g. `fall`\n-  $m_n$ is the coefficient for the nth feature<br>\n\nand `summer` which has `000` is the `base value` and all other groups are evaluated against this base group.\n\n","212c98d5":"n=219 for test data set and p=9","4e2e4140":"We can see that adjusted R-squared has significantly reduced from `0.843`(model 2) to `0.781`. Hence `temp` is an important predictor as we have seen in the pair plots and correlation heat map also. \n\n#### Revert to `model2`","3c8ac27d":"#### Test Adjusted R2 score:\n\nadj r2=1-(1-R2)*(n-1)\/(n-p-1)\n\nwhere n is the sample size\nand p is the number of features","56ae0faf":"We can see from the `describe()` output that all the variables have values between 0(min) and 1(max). ","14f06d5a":"### 4.4 Predict target variable using the fitted model","af62b279":"#### Model 9: Remove `mist` whose OLS coefficient is -0.08","5a83efbb":"#### INFERENCES:\n\n- `hum` seem to have few of the values as zero; need to check if its valid as relative humidity is usually above than 0.\n- `temp` seem to have most of the values between 10 and 30.\n- `hum` seem to have values across and most of the values range between 40 and 80.\n- `windspeed` seem to have values between 10 and 20 mostly.\n\nFrom the pair plot and other plots, its visible that **linear regression** can be applied to this business problem to predict the `cnt` variable\n\n\nEquation of linear regression<br>\n$cnt = const + m_1x_1 + m_2x_2 + ... + m_nx_n$\n\n-  $cnt$ is the target\n-  $const$ is the intercept\n-  $m_1$ is the coefficient for the first feature\n-  $m_n$ is the coefficient for the nth feature<br>\n\nWe have to find the coefficients and the appropriate features to fit into linear regression model.","c1930f34":"### 3.3 Feature selection\n\nTwo main statistics to select the features:\n- p-value of the feature\n- VIFs.\n\n**Hypothesis of the test** is that a feature is `insignificant` in the model and by having p-value less than 0.05, we `reject this hypothesis` and hence the feature is selected. So wherever the p-value of a feature is higher than 0.05, it indicates that we `failed to reject the hypothesis` that the feature is insignificant.\n\nWhen there is a feature in the model that can be explained by other features, then the variable's VIF will be higher than 10 and we can eliminate those variables.\n\n#### Strategies:\n\n- When a feature has high p-value and high VIF, then it can be dropped first.\n- When a feature has high p-value and low VIF, then we will drop it as it is not significant.\n- When a feature has low p-value and high VIF, we try to drop other features which have higher p-value and if there is none, we will remove this feature and build the model and check the R-squared.\n- When a feature has low p-value and low VIF, then it is significant and not explained by other features and hence we keep these features in the model.\n","171eb85e":"#### Functions for building Linear regression model using statsmodels and calculate VIF using sklearn:\n\nIteratively we can drop the features one by one from the model using **p-value** and **VIF** and build the model using **statsmodels**\n\nWe will try to achieve a model not more than 10 variables and Rsquared is around 80 percent.\n\nFollowing functions are created to call iteratively and build `linear regression` model using `statsmodels` and calculate `VIF` using `sklearn` and print statistics summary of each model.","8c2b8dc4":"### 3.2 Identify the top significant features using RFE ( Recursive Feature Elimination - automated approach)\n","fb619649":"### 2.2 Split the dataset into train and test dataset","8a7bb34d":"#### INFERENCES:\n\n1. From first plot, it can be proved that `Residuals follow normal distribution and centered around zero`\n2. From second plot, it can be proved that `Residuals do not follow any pattern indicating error terms are independent`\n3. Also from second plot, we can see that `Error terms have constant variance and do not exhibit Heteroskedasticity`\n4. Also from second plot, we can see that the `linear relationship is evident`","454030bf":"Now all the variables have p-value less than 0.05. So lets check the `VIF` to find if there is `multi-collinearity` and remove features accordingly. As we have seen already `temp` is an important predictor. So we will try to remove the next high `VIF` variable `fall` and see if `VIF` of `temp` is reduced by removing `fall` from the model.","51cfb4b6":"#### Dropping the unnecessary columns which are not required for building a model\n\nDrop the following columns:\n- `instant` (it has just record index)\n- `dteday` (other fields represent this)\n- `casual`,`registered` (represented by `cnt`) \n- `atemp` due to high correlation","33de476e":"Both `info()` and `isnull().any()` output indicate that there are no null values in the given dataset.","4fd7efb7":"### Convert to Categorical Columns so that these can be used in Pandas get_dummies()","87ca1c8f":"- As see above, we can use linear algebra to solve this linear regression model. \n- Most of the times, your dataset will contain features highly varying in magnitudes, units and range. \n- Linear regression use distance between two data points in its computations. \n- If we do not scale, it only take in the magnitude of features neglecting the units. \n- The results would vary greatly between different units, 5kg and 5000gms. \n- The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes.\n- To supress this effect, we need to bring all features to the same level of magnitudes. This can be acheived by **scaling**.\n- If you rescale your independent variables, you will get identical regression results, except that the constant term and coefficients will be changed to offset the rescaling.\n\nThere are two common techniques in scaling:\n1. **Min-Max Scaling** or **Normalization**:\n   - Scales all the values between 0 and 1\n   - Computes value using (x-xmin) \/ (xmax-xmin)\n   \n   \n2. **Standardisation** :\n   - Standardisation replaces the values by their Z scores\n   - Computes value using (x-mu)\/sigma\n   - This redistributes the features with their mean \u03bc = 0 and standard deviation \u03c3 =1\n   \n`Normalization` is chosen for this model building ","c0007380":"There are `730` rows and `16` columns in the given dataset.","73d0e3b1":"2. How well those variables describe the bike demands","13ff74cb":"#### Model 6: Remove `fall` as its VIF is higher","8cacc590":"The impact is not much on the model. Rsquared has reduced from 0.832 to 0.83 and adjusted Rsquared has reduced from 0.828 to 0.826. \n\nSo it seems removing `mar` does not have much impact and hence lets drop this.\n\nWe now have 10 features still lets proceed checking whether deleting `oct` has significant impact on the model. If not, deleting it would help to bring the number of features to 9.","14c6a465":"#### INFERENCES:\n\n- From correlation plot, we can observed that some features are positively correlated or some are negatively correlated to each other. \n\n- `casual` and `registered` contain direct information about the bike sharing count which we have to predict. Therefore we can remove these from the feature set.\n\n- The variables `temp` and `atemp` are highly positively correlated which means both are having same information. \n\nWe can drop either of `temp` or `atemp` as one of those can explain the other but `temp` is retained considering \n- it will be simpler measure for the business\n- it will be used for calculating `atemp`\n- `atemp` is a measure of multiple entries and any of the columns outliers, missing values will affect `atemp` and there is a more chance of error. \n\nSo we can drop `atemp` before building the model itself as part of `dimensionality reduction` ","f971d8f6":"Now all the variables have p-value less than 0.05. We can check if there is `multi-collinearity` by looking at the `VIF` and remove features accordingly.","12ff1dbb":"### Impact of categorical variables on the prediction:\n\nThis equation has the following categorical variables:\n\n- `Weathersit` being 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds or 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist\n- `Yr` being 2019\n- `season` being 4:winter or 2:summer\n- `mnth` being 9:sep\n- `holiday` being 1\n\nWeathersit has 3 values in dataset and they are denoted by columns mist and snow as\n- `Clear` is represented as 00\n- `Mist` is represented as 10\n- `Snow` is represented as 01\n\nSo when the weathersit is **Clear**, then the above equation becomes \n\ncnt = 0.0973 + 0.55 \\times temp + 0.25 \\times yr being 2019 - 0.1386 \\times windspeed + 0.1457 \\times season being 4:winter + 0.1011 \\times mnth being 9:sep + 0.0932 \\times season being 2:summer - 0.0974 \\times holiday \n\nWhen the weathersit is **Mist**, then the equation becomes\n\ncnt = 0.0973 + 0.55 \\times temp + 0.25 \\times yr being 2019 - 0.1386 \\times windspeed + 0.1457 \\times season being 4:winter + 0.1011 \\times mnth being 9:sep + 0.0932 \\times season being 2:summer - 0.0834 - 0.0974 \\times holiday\n\ni.e. The cnt reduces by 0.08 when compared to weathersit being Clear.\n\n\nWhen the weathersit is **Snow**, then the equation becomes\n\ncnt = 0.0973 + 0.55 \\times temp - 0.2659 + 0.25 \\times yr being 2019 - 0.1386 \\times windspeed + 0.1457 \\times season being 4:winter + 0.1011 \\times mnth being 9:sep + 0.0932 \\times season being 2:summer - 0.0974 \\times holiday\n\ni.e. cnt reduces by 0.2659 when compared to weathersit being Clear.","e3557d23":"## Business Goal :\n   \n- Build a model which will be used by management to understand the demand for shared bikes with the available independent variables. \n- Explain how exactly the demands vary with different features so that the business strategy can be modified to meet the demand levels and meet the customer's expectations. \n- Further, the model will be a good way for management to understand the demand dynamics of a new market. \n    \n## ML Tasks:    \n\n- Which variables are significant in predicting the demand for shared bikes.\n- How well those variables describe the bike demands\n\nYou can observe in the dataset that some of the variables like 'weathersit' and 'season' have values as 1, 2, 3, 4 which have specific labels associated with them (as can be seen in the data dictionary). These numeric values associated with the labels may indicate that there is some order to them - which is actually not the case (Check the data dictionary and think why). So, it is advisable to convert such feature values into categorical string values before proceeding with model building. Please refer the data dictionary to get a better understanding of all the independent variables.","471655ab":"#### Model 5: Remove `aug` as its p-value > 0.05","f14d72f6":"### 0.2 Suppressing Warnings","be7b46b2":"Here the Rsquared has reduced significantly from 0.827 to 0.8 and this indicates `mist` and other features are significant to compute `cnt`. \n\nSo lets revert to model 8 and proceed our residual analysis, linear regression assumptions and the predict using X_test and compare our prediction with the available Y_test","8c146183":"#### Selected features of the model and its coefficients"}}