{"cell_type":{"c8eaf51c":"code","0ec27a16":"code","5ad585cc":"code","b760c23e":"code","95991c02":"code","774b63bd":"code","7b0cd4b8":"code","88415962":"code","952109a8":"code","e71048a3":"code","7dc1e1da":"code","a7fcd2cb":"code","9943f4a1":"code","b033cd89":"code","13dd4cb6":"code","137119e4":"code","6c378ea1":"code","d5416485":"code","4d09f230":"code","4382506c":"code","2b56981b":"code","3e9405c4":"code","83936978":"code","e4e90c57":"code","d3bdc8b6":"code","9990f1de":"code","e1e8f2df":"code","49b87f82":"code","a475f53a":"code","d317582c":"code","fa5121a7":"code","89adaac4":"code","8d36597e":"code","f22a5592":"code","5930fff4":"code","88780fa4":"code","f07098d1":"code","b79902e4":"code","c86f3cf7":"code","59791001":"code","f3eddc9a":"code","1ec4c166":"code","2519f6e6":"code","da26cf14":"code","abb56f36":"code","a4b829d4":"code","3335a354":"code","cdf4e87e":"code","05f26b46":"code","23096ee7":"code","3ed4ec10":"code","f0ef14f4":"code","9315a6ca":"code","5f2805e7":"code","de68490d":"code","97362e61":"code","33379f49":"code","3a3c49c7":"code","ac51ae76":"code","ee97f48d":"code","f7979e29":"code","dfd58b5a":"code","c9635794":"code","dc6bb93b":"markdown","167a22b3":"markdown","0f43b86b":"markdown","adb76ded":"markdown","9ba3e535":"markdown","83e57068":"markdown","bceb7497":"markdown","861bef0b":"markdown","31bcd507":"markdown","ade5e574":"markdown","abf428ac":"markdown","7e7e5436":"markdown","a790ccf6":"markdown","c58a9d9d":"markdown","c5e62ab8":"markdown","256592ba":"markdown","70f06862":"markdown","172625c9":"markdown","a8ac37c5":"markdown","c81e0117":"markdown","d1075b5d":"markdown","2aba97da":"markdown","a90cd4ef":"markdown","8ebffdd2":"markdown","9851273d":"markdown","044ac06d":"markdown","65ed6b4b":"markdown","39f62059":"markdown","cc4bc173":"markdown","77e07f03":"markdown","997d0691":"markdown","e88762bd":"markdown","39c27954":"markdown","31131b58":"markdown","b4dc1f10":"markdown","c53bf640":"markdown","3cc3de7e":"markdown","4350f610":"markdown","9f2c9d6b":"markdown","07871592":"markdown","21f8258f":"markdown","2dd627ca":"markdown","25d67381":"markdown","1e02e167":"markdown","c653b6d3":"markdown","583168a6":"markdown","568541e5":"markdown","784dfe5c":"markdown","8ad38f62":"markdown","00daf172":"markdown","97f8ae7c":"markdown","f4ab3ca4":"markdown","67268129":"markdown","febea89d":"markdown","2be0b63b":"markdown","4f47dda2":"markdown","fcc63fb6":"markdown","2f6a21a6":"markdown","ad3eafea":"markdown","e8cbcea4":"markdown","05635d94":"markdown"},"source":{"c8eaf51c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0ec27a16":"import numpy as np\nimport  pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5ad585cc":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv')","b760c23e":"train_df.columns","95991c02":"test_df.columns","774b63bd":"train_df.info()\n# Age, Cabin, Embarked has null values","7b0cd4b8":"train_df.describe()","88415962":"Gend_male = pd.DataFrame(train_df[train_df[\"Sex\"] == \"male\"])\nmean_age_male = Gend_male['Age'].mean()\n\nGend_female = pd.DataFrame(train_df[train_df[\"Sex\"] == \"female\"])\nmean_age_female = Gend_female['Age'].mean()\ntrain_df.loc[(train_df['Age'].isna()) & (train_df['Sex']=='male'), 'Age']=mean_age_male\ntrain_df.loc[(train_df['Age'].isna()) & (train_df['Sex']=='female'), 'Age']=mean_age_female\n\nGend_male_test = pd.DataFrame(test_df[test_df[\"Sex\"] == \"male\"])\nmean_age_male_test = Gend_male_test['Age'].mean()\n\nGend_female_test = pd.DataFrame(test_df[test_df[\"Sex\"] == \"female\"])\nmean_age_female_test = Gend_female_test['Age'].mean()\ntest_df.loc[(test_df['Age'].isna()) & (test_df['Sex']=='male'), 'Age']=mean_age_male\ntest_df.loc[(test_df['Age'].isna()) & (test_df['Sex']=='female'), 'Age']=mean_age_female\n\n\n\n\n","952109a8":"train_df['Cabin'].fillna('NaN',inplace=True)\ntest_df['Cabin'].fillna('NaN',inplace=True)","e71048a3":"train_df['Embarked'].fillna('Unknown',inplace=True)","7dc1e1da":"\nSurvival_rate = {'Survived_count': [0],\n                'Not_Survived_count' : [0],\n                'Total' : [0]}\n\nSurvival_rate['Survived_count'] =  train_df.Survived.value_counts()[0]\nSurvival_rate['Not_Survived_count'] =  train_df.Survived.value_counts()[1]\nSurvival_rate['Total'] =  Survival_rate['Survived_count'] + Survival_rate['Not_Survived_count']\n\n# Create the index \nindex_ = ['Survival_Rate'] \n  \n# Set the index \n\nSurvival=pd.DataFrame([Survival_rate])\nSurvival.index = index_ \nSurvival.transpose()\n","a7fcd2cb":"Survival.hist(figsize=(10,10),grid=False)\n#plt.plot()","9943f4a1":"\ns_count = {'Not_Survived':[0],\n          'Survived':[0]}\nsf_count = {'Not_Survived':[0],\n          'Survived':[0]}\nc_count = {'Not_Survived':[0],\n          'Survived':[0]}\ncf_count = {'Not_Survived':[0],\n          'Survived':[0]}\nq_count = {'Not_Survived':[0],\n          'Survived':[0]}\nqf_count = {'Not_Survived':[0],\n          'Survived':[0]}\ns_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='S'),'Survived'].value_counts()[0]\ns_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='S'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked S'] \n# Set the index \nmale_S=pd.DataFrame([s_count])\nmale_S.index = index_ \n#male_S.transpose()\n#male_S\nsf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='S'),'Survived'].value_counts()[0]\nsf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='S'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Female passengers : Embarked S'] \n# Set the index \nfemale_S=pd.DataFrame([sf_count])\nfemale_S.index = index_\nc_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='C'),'Survived'].value_counts()[0]\nc_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='C'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked C'] \n# Set the index \nmale_C=pd.DataFrame([c_count])\nmale_C.index = index_ \n#male_C\ncf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='C'),'Survived'].value_counts()[0]\ncf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='C'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Female passengers : Embarked C'] \n# Set the index \nfemale_C=pd.DataFrame([cf_count])\nfemale_C.index = index_ \nq_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[0]\nq_count['Survived'] = train_df.loc[(train_df['Sex']=='male') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['Male passengers : Embarked Q'] \n# Set the index \nmale_Q=pd.DataFrame([q_count])\nmale_Q.index = index_ \n#male_Q\nqf_count['Not_Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[0]\nqf_count['Survived'] = train_df.loc[(train_df['Sex']=='female') & (train_df['Embarked']=='Q'),'Survived'].value_counts()[1]\n#Create the index \nindex_ = ['female passengers : Embarked Q'] \n# Set the index \nfemale_Q=pd.DataFrame([qf_count])\nfemale_Q.index = index_ \n#male_Q\nframes=[male_S,female_S,male_C,female_C,male_Q,female_Q]\nresult=pd.concat(frames)\nGender_Embarked = pd.DataFrame(result)\nGender_Embarked\n","b033cd89":"Gender_Embarked[[\"Not_Survived\",\"Survived\"]].plot(kind=\"bar\",stacked=True)","13dd4cb6":"\na = {'Not_Survived':[0],\n          'Survived':[0]}\nb = {'Not_Survived':[0],\n          'Survived':[0]}\nc = {'Not_Survived':[0],\n          'Survived':[0]}\nd = {'Not_Survived':[0],\n          'Survived':[0]}\ne = {'Not_Survived':[0],\n          'Survived':[0]}\nf = {'Not_Survived':[0],\n          'Survived':[0]}\na=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 1),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 1'] \n# Set the index \nfemale_1=pd.DataFrame([a])\nfemale_1.index = index_ \nb=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 2),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 2'] \n# Set the index \nfemale_2=pd.DataFrame([b])\nfemale_2.index = index_ \nc=train_df.loc[(train_df['Sex']=='female') & (train_df['Pclass']== 3),'Survived'].value_counts()\n#Create the index \nindex_ = ['Female passengers : PClass 3'] \n# Set the index \nfemale_3=pd.DataFrame([c])\nfemale_3.index = index_ \nd=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 1),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 1'] \n# Set the index \nmale_1=pd.DataFrame([d])\nmale_1.index = index_ \ne=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 2),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 2'] \n# Set the index \nmale_2=pd.DataFrame([e])\nmale_2.index = index_ \nf=train_df.loc[(train_df['Sex']=='male') & (train_df['Pclass']== 3),'Survived'].value_counts()\n#Create the index \nindex_ = ['Male passengers : PClass 3'] \n# Set the index \nmale_3=pd.DataFrame([f])\nmale_3.index = index_ \n#male_Q\nframes=[male_1,female_1,male_2,female_2,male_1,female_2,male_3,female_3]\nresult=pd.concat(frames)\nGender_PClass = pd.DataFrame(result)\nGender_PClass","137119e4":"ax2 = Gender_PClass.plot.pie(subplots=True,figsize=(20,20), autopct='%1.1f%%',shadow=True)\nplt.legend(loc='center left')\nplt.show()","6c378ea1":"ax=sns.countplot(x='Pclass',hue='Survived',data=train_df)","d5416485":"ax=sns.FacetGrid(train_df,col=\"Survived\")\nax=ax.map(plt.hist,'Age',color=\"g\",bins=10)","4d09f230":"sns.catplot(x=\"Pclass\",y=\"Age\",hue=\"Sex\",data=train_df)","4382506c":"train_df['age_bins'] = pd.cut(x=train_df['Age'], bins=8, labels=False, retbins=False, include_lowest=True)\ntest_df['age_bins'] = pd.cut(x=test_df['Age'], bins=8, labels=False, retbins=False, include_lowest=True)","2b56981b":"train_df['Fare_cat']=0\ntrain_df.loc[train_df['Fare']<=7.91,'Fare_cat']=0\ntrain_df.loc[(train_df['Fare']>7.91)&(train_df['Fare']<=14.454),'Fare_cat']=1\ntrain_df.loc[(train_df['Fare']>14.454)&(train_df['Fare']<=31),'Fare_cat']=2\ntrain_df.loc[(train_df['Fare']>31)&(train_df['Fare']<=93.5),'Fare_cat']=3\ntrain_df.loc[(train_df['Fare']>93.5)&(train_df['Fare']<=164.8667),'Fare_cat']=4\ntrain_df.loc[(train_df['Fare']>164.8667)&(train_df['Fare']<=512.3292),'Fare_cat']=5\n\ntest_df['Fare_cat']=0\ntest_df.loc[test_df['Fare']<=7.91,'Fare_cat']=0\ntest_df.loc[(test_df['Fare']>7.91)&(test_df['Fare']<=14.454),'Fare_cat']=1\ntest_df.loc[(test_df['Fare']>14.454)&(test_df['Fare']<=31),'Fare_cat']=2\ntest_df.loc[(test_df['Fare']>31)&(test_df['Fare']<=93.5),'Fare_cat']=3\ntest_df.loc[(test_df['Fare']>93.5)&(test_df['Fare']<=164.8667),'Fare_cat']=4\ntest_df.loc[(test_df['Fare']>164.8667)&(test_df['Fare']<=512.3292),'Fare_cat']=5","3e9405c4":"name = train_df['Name']\n#Extract the initials\ntrain_df['Title'] = name.str.extract(pat = \"(Mr|Master|Mrs|Miss|Major|Rev|Lady|Dr|Mme|Mlle|Col|Capt)\\\\.\")\ntest_df['Title'] = name.str.extract(pat = \"(Mr|Master|Mrs|Miss|Major|Rev|Lady|Dr|Mme|Mlle|Col|Capt)\\\\.\")\ntrain_df['Title'].astype(str)\ntest_df['Title'].astype(str)\n#Assign Rare for the rare initials\ntrain_df.Title[train_df.Title == 'Rev'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Major'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Lady'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Dr'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Mme'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Mlle'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Col'] = 'Rare'\ntrain_df.Title[train_df.Title == 'Capt'] = 'Rare'\n\ntest_df.Title[test_df.Title == 'Rev'] = 'Rare'\ntest_df.Title[test_df.Title == 'Major'] = 'Rare'\ntest_df.Title[test_df.Title == 'Lady'] = 'Rare'\ntest_df.Title[test_df.Title == 'Dr'] = 'Rare'\ntest_df.Title[test_df.Title == 'Mme'] = 'Rare'\ntest_df.Title[test_df.Title == 'Mlle'] = 'Rare'\ntest_df.Title[test_df.Title == 'Col'] = 'Rare'\ntest_df.Title[test_df.Title == 'Capt'] = 'Rare'\n# Categorize the Initial\ntrain_df['Title'].replace(['Mr','Mrs','Miss','Master','Rare'],[1,2,3,4,5],inplace=True)\ntest_df['Title'].replace(['Mr','Mrs','Miss','Master','Rare'],[1,2,3,4,5],inplace=True)\n#train_df\n\n# Missing values Imputation\ntrain_df['Title'].fillna(0,inplace=True)\ntest_df['Title'].fillna(0,inplace=True)\n","83936978":"train_df['Sex'].replace(['male','female'],[0,1],inplace=True)\ntest_df['Sex'].replace(['male','female'],[0,1],inplace=True)","e4e90c57":"#train_df['Embarked'].unique()\ntrain_df['Embarked'] = train_df['Embarked'].map({'S': 1, 'C': 2, 'Q': 3,'Unknown':0} ).astype(int)\ntest_df['Embarked'] = test_df['Embarked'].map({'S': 1, 'C': 2, 'Q': 3,'Unknown':0} ).astype(int)\n#dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","d3bdc8b6":"\ntrain_df['Family_Size']=0\ntrain_df['Family_Size'] = train_df['SibSp'] + train_df['Parch']\ntrain_df['IsAlone']=0\ntrain_df.loc[(train_df['Family_Size']==1),'IsAlone']=1\ntrain_df.loc[(train_df['Family_Size']==0) | (train_df['Family_Size']>1),'IsAlone']=0\n\ntest_df['Family_Size']=0\ntest_df['Family_Size'] = test_df['SibSp'] + test_df['Parch']\ntest_df['IsAlone']=0\ntest_df.loc[(test_df['Family_Size']==1),'IsAlone']=1\ntest_df.loc[(test_df['Family_Size']==0) | (test_df['Family_Size']>1),'IsAlone']=0","9990f1de":"train_df.info()","e1e8f2df":"test_df.info()","49b87f82":"sns.heatmap(train_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","a475f53a":"sns.heatmap(test_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","d317582c":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import ExtraTreesClassifier\ny = train_df['Survived'] \n#X = pd.DataFrame(train_df)\n#df.drop(['A'], axis = 1)\nX = train_df.drop(['Survived','Name','Ticket','Cabin'],axis=1) ","fa5121a7":"# Building the model \nextra_tree_forest = ExtraTreesClassifier(n_estimators = 5,criterion ='entropy', max_features = 5) \n  \n# Training the model \nextra_tree_forest.fit(X, y) \n  \n# Computing the importance of each feature \nfeature_importance = extra_tree_forest.feature_importances_ \n  \n# Normalizing the individual importances \nfeature_importance_normalized = np.std([tree.feature_importances_ for tree in extra_tree_forest.estimators_], axis = 0) \n\n# Plotting a Bar Graph to compare the models \nplt.figure(figsize=(20,6))\nplt.bar(X.columns,feature_importance_normalized,align='edge', width=0.3) \nplt.xlabel('Feature Labels') \nplt.ylabel('Feature Importances') \nplt.title('Comparison of different Feature Importances') \n\nplt.show() \n","89adaac4":"from sklearn.feature_selection import SelectKBest \nfrom sklearn.feature_selection import chi2 \nX.shape\n# Two features with highest chi-squared statistics are selected \nchi2_features = SelectKBest(chi2, k = 9) \nX_kbest_features = chi2_features.fit_transform(X, y) \n  \n# Reduced features \nprint('Original feature number:', X.shape[1]) \nprint('Reduced feature number:', X_kbest_features.shape[1])\n#X\nX_kbest_features\n#PClass, Age, Sex, Fare, Fare_cat, Title, Family Size - Top 7 features\n\n","8d36597e":"X.head()","f22a5592":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","5930fff4":"train_df = train_df.drop(\"Name\", axis=1)\ntrain_df = train_df.drop(\"Ticket\", axis=1)\ntrain_df = train_df.drop(\"Cabin\", axis=1)\ntrain_df = train_df.drop(\"Fare\", axis=1)\n#train_df = train_df.drop(\"Embarked\", axis=1)\ntrain_df = train_df.drop(\"IsAlone\", axis=1)\n#train_df = train_df.drop(\"SibSp\", axis=1)\n#train_df = train_df.drop(\"Parch\", axis=1)\n#train_df = train_df.drop(\"Pclass\", axis=1)\n#train_df = train_df.drop(\"Age\", axis=1)\ntrain_df = train_df.drop(\"age_bins\", axis=1)\ntrain_df = train_df.drop(\"PassengerId\", axis=1)\n\n\ntest_df = test_df.drop(\"Name\", axis=1)\ntest_df = test_df.drop(\"Ticket\", axis=1)\ntest_df = test_df.drop(\"Cabin\", axis=1)\ntest_df = test_df.drop(\"Fare\", axis=1)\n#test_df = test_df.drop(\"Embarked\", axis=1)\ntest_df = test_df.drop(\"IsAlone\", axis=1)\n#test_df = test_df.drop(\"SibSp\", axis=1)\n#test_df = test_df.drop(\"Parch\", axis=1)\n#test_df = test_df.drop(\"Pclass\", axis=1)\n#test_df = test_df.drop(\"Age\", axis=1)\ntest_df = test_df.drop(\"age_bins\", axis=1)\n","88780fa4":"sns.heatmap(train_df.corr(), annot=True).set_title(\"Corelation of attributes\")\nfig=plt.gcf()\nfig.set_size_inches(15,15)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","f07098d1":"X_train = train_df.drop(\"Survived\", axis=1)\ny_train = train_df[\"Survived\"]\nX_test  = test_df.drop(\"PassengerId\", axis=1).copy()\nX_train.shape, y_train.shape, X_test.shape","b79902e4":"X_test.head()","c86f3cf7":"model = LogisticRegression()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\n#acc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nlogistic_score = round(model.score(X_train,y_train) * 100,2)\nprint(logistic_score)","59791001":"from sklearn import svm\nmodel = svm.SVC(kernel='rbf',C=1,gamma=0.1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_svc = round(model.score(X_train, y_train) * 100, 2)\nacc_svc","f3eddc9a":"model = svm.SVC(kernel='linear',C=0.1,gamma=0.1)\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_l_svc = round(model.score(X_train, y_train) * 100, 2)\nacc_l_svc","1ec4c166":"#KNN Classification\na_index=list(range(1,11))\na=pd.Series()\nx=[0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model=KNeighborsClassifier(n_neighbors=i)\n    model.fit(X_train,y_train)\n    y_pred=model.predict(X_test)\n    a=a.append(pd.Series(model.score(X_train,y_train)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())\nacc_knn = round(model.score(X_train,y_train)*100,2)\nacc_knn","2519f6e6":"model=GaussianNB()\nmodel.fit(X_train,y_train)\ny_pred=model.predict(X_test)\nacc_gaus = round(model.score(X_train,y_train)*100,2)\nacc_gaus","da26cf14":"model = DecisionTreeClassifier()\nmodel.fit(X_train, y_train)\ny_pred = model.predict(X_test)\nacc_dec = round(model.score(X_train, y_train) * 100, 2)\nacc_dec","abb56f36":"from sklearn.feature_selection import RFE\nrfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\nrfe.fit(X_train, y_train)\ny_pred = rfe.predict(X_test)\nrfe.score(X_train, y_train)\nrfe_scc = round(rfe.score(X_train, y_train) * 100, 2)\n\nfor i in range(X_train.shape[1]):\n    print('Column: %d, Name: %s, Selected %s, Rank: %.3f' % (i, X_train.columns[i],rfe.support_[i], rfe.ranking_[i]))","a4b829d4":"model = RandomForestClassifier(n_estimators=100)\nmodel.fit(X_train, y_train)\ny_pred_ran = model.predict(X_test)\nmodel.score(X_train, y_train)\nacc_forest = round(model.score(X_train, y_train) * 100, 2)\nacc_forest","3335a354":"from sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nxyz=[]\naccuracy=[]\nstd=[]\nclassifiers=['Linear Svm','Radial Svm','Logistic Regression','KNN','Decision Tree','Naive Bayes','Random Forest']\nmodels=[svm.SVC(kernel='linear'),svm.SVC(kernel='rbf'),LogisticRegression(),KNeighborsClassifier(n_neighbors=9),DecisionTreeClassifier(),GaussianNB(),RandomForestClassifier(n_estimators=100)]\nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X_train,y_train, cv = kfold,scoring = \"accuracy\")\n    cv_result=cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\nnew_models_dataframe2=pd.DataFrame({'CV Mean':xyz,'Std':std},index=classifiers)       \nnew_models_dataframe2\n","cdf4e87e":"plt.subplots(figsize=(12,6))\nbox=pd.DataFrame(accuracy,index=[classifiers])\nbox.T.boxplot()\n","05f26b46":"new_models_dataframe2['CV Mean'].plot.barh(width=0.8)\nplt.title('Average CV Mean Accuracy')\nfig=plt.gcf()\nfig.set_size_inches(12,5)\nplt.show()","23096ee7":"from sklearn.metrics import confusion_matrix\nf,ax=plt.subplots(3,3,figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,0],annot=True,fmt='2.0f')\nax[0,0].set_title('Matrix for rbf-SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,1],annot=True,fmt='2.0f')\nax[0,1].set_title('Matrix for Linear-SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[0,2],annot=True,fmt='2.0f')\nax[0,2].set_title('Matrix for KNN')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,0],annot=True,fmt='2.0f')\nax[1,0].set_title('Matrix for Random-Forests')\n\ny_pred = cross_val_predict(LogisticRegression(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,1],annot=True,fmt='2.0f')\nax[1,1].set_title('Matrix for Logistic Regression')\n\ny_pred = cross_val_predict(DecisionTreeClassifier(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[1,2],annot=True,fmt='2.0f')\nax[1,2].set_title('Matrix for Decision Tree')\n\ny_pred = cross_val_predict(GaussianNB(),X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,y_pred),ax=ax[2,0],annot=True,fmt='2.0f')\nax[2,0].set_title('Matrix for Naive Bayes')\n\nplt.subplots_adjust(hspace=0.2,wspace=0.2)\nplt.show()","3ed4ec10":"from sklearn.model_selection import GridSearchCV\nC=[2,2.1,2.5]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper={'kernel':kernel,'C':C,'gamma':gamma}\ngd=GridSearchCV(estimator=svm.SVC(),param_grid=hyper,verbose=True)\ngd.fit(X_train,y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","f0ef14f4":"n_estimators=range(100,1000,1100)\nhyper={'n_estimators':n_estimators}\ngd=GridSearchCV(estimator=RandomForestClassifier(random_state=0),param_grid=hyper,verbose=True)\ngd.fit(X_train,y_train)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","9315a6ca":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb\nimport lightgbm as lgb","5f2805e7":"from sklearn.ensemble import VotingClassifier\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(X_train,y_train)\nprint('The accuracy for ensembled model is:',round(ensemble_lin_rbf.score(X_train,y_train)*100,2))\nvot = round(ensemble_lin_rbf.score(X_train,y_train)*100,2)\ncross=cross_val_score(ensemble_lin_rbf,X_train,y_train, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())\n\n\n\n","de68490d":"from sklearn.ensemble import BaggingClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('The accuracy for bagged KNN is:',round(model.score(X_train,y_train)*100,2))\nbag_knn = round(model.score(X_train,y_train)*100,2)\nresult=cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',round(result.mean()*100,2))\n\n\n","97362e61":"model=BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0,n_estimators=100)\nmodel.fit(X_train,y_train)\nprediction=model.predict(X_test)\nprint('The accuracy for bagged Decision Tree is:',round(model.score(X_train,y_train)*100,2))\nbag_ran = round(model.score(X_train,y_train)*100,2)\nresult=cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:',round(result.mean()*100,2))","33379f49":"ada = AdaBoostClassifier(random_state=1,n_estimators=1000)\nada.fit(X_train, y_train)\ny_pred = ada.predict(X_test)\nada.score(X_train, y_train)\nada_boost = round(ada.score(X_train, y_train) * 100, 2)\nada_boost\n","3a3c49c7":"grad = GradientBoostingClassifier(n_estimators=1000,learning_rate=0.01,random_state=0)\ngrad.fit(X_train, y_train)\ny_pred = grad.predict(X_test)\ngrad.score(X_train, y_train)\ngrad_boost = round(grad.score(X_train, y_train) * 100, 2)\ngrad_boost","ac51ae76":"extreme = xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\nextreme.fit(X_train,y_train)\ny_pred = extreme.predict(X_test)\nextreme.score(X_train, y_train)\nextreme_boost = round(extreme.score(X_train, y_train) * 100, 2)\nextreme_boost","ee97f48d":"xgb=XGBClassifier(n_estimators=1000,random_state=0,learning_rate=0.01)\nresult=cross_val_predict(xgb,X_train,y_train,cv=10)\nsns.heatmap(confusion_matrix(y_train,result),cmap='winter',annot=True,fmt='2.0f')\nplt.show()","f7979e29":"import xgboost as xgb\nf,ax=plt.subplots(2,2,figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=1000,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0])\nax[0,0].set_title('Feature Importance in Random Forests')\n\nmodel=AdaBoostClassifier(n_estimators=1000,learning_rate=0.01,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='#ddff11')\nax[0,1].set_title('Feature Importance in AdaBoost')\n\nmodel=GradientBoostingClassifier(n_estimators=1000,learning_rate=0.1,random_state=0)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],cmap='RdYlGn_r')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\n\nmodel=xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\n#extreme = xgb.XGBClassifier(n_estimators=1000,learning_rate=0.1)\nmodel.fit(X_train,y_train)\npd.Series(model.feature_importances_,X_train.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='#FD0F00')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.show()","dfd58b5a":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": y_pred_ran})\nsubmission\nsubmission.to_csv('submission.csv',index=False)\n#test_df=pd.read_csv('..\/input\/titanic\/test.csv')","c9635794":"models = pd.DataFrame({'Model': ['Radial SVC', 'KNN', 'Logistic Regression','Random Forest', 'Naive Bayes', 'Linear SVC', 'Decision Tree','VotingClassifier','Bagged KNN','Bagged DecisionTree','AdaBoost','GradientBoost','XGBoost'],\n    'Score': [acc_svc, acc_knn, logistic_score,acc_forest, acc_gaus, acc_l_svc, acc_dec,vot,bag_knn,bag_ran,ada_boost,grad_boost,extreme_boost]})\nmodels.sort_values(by='Score', ascending=False)","dc6bb93b":"**Gradient Boosting**","167a22b3":"**Logistic Regression**","0f43b86b":"# Feature Importance by ExtraTreeClassifier","adb76ded":"**KNN Classification**","9ba3e535":"**Categorize by Age bins, PClass and Sex**","83e57068":"Boosting","bceb7497":"**Random Forest Classification**","861bef0b":"Boosting is an ensemble method for improving the model predictions of any given learning algorithm. The idea of boosting is to train weak learners sequentially, each trying to correct its predecessor.\nBoosting is an iterative technique which adjusts the weight of an observation based on the last classification.","31bcd507":"**Family Size Computation**","ade5e574":"# **Cross Validation**","abf428ac":"**Drop unnecessary columns in train and test set before predictions**","7e7e5436":"# **Total number of survived and not survived**","a790ccf6":"Random Forest Classification","c58a9d9d":"#  Table of Contents\n\n  1. Missing Data Analysis\n  2. Exploratory Data Analysis\n  3. Feature Engineering\n  4. Feature Selection\n  5. Feature Importance\n  6. Classification algorithmsand metrics explained\n  7. Hyperparameter Tuning\n  8. Ensemble Techniques for Prediction\n  9. Model Evaluation\n  10. Submission","c5e62ab8":"The positive corelated attributes are age : age_bins (0.97), SibSp : Family_size (0.89), Parch : Family_size (0.78), Sex:Title (0.58), Survived : Sex(0.54). Negative corelated : Name, Ticket, Cabin, Passenger_Id, Fare, Fare_Cat, Age","256592ba":"Confusion matrix for the best model","70f06862":"Bagged DecisionTree","172625c9":"RBF SVM","a8ac37c5":"**Split the data into train and test set for classifcation predictions**","c81e0117":"The best score for RBF-SVM is 82.2697 with C=1 and gamma = 0.5. The best score for Random Forest is 82.0456 with n_estimators=200","d1075b5d":"The feature engineering process includes :\n    1. Testing features.\n    2. Deciding what features to create.\n    3. Creating features.\n    4. Checking how the features work with your model.\n    5. Improving your features if needed.\n    6. Create more features until the work is done.\nFeature Engineering techniques include imputation, handling outliers, binning, log transform one-hot encoding, grouping operations, feature split, scaling and extracting date.","2aba97da":"**Missing values Imputation**","a90cd4ef":"Based on the feature selection techniques, retaining the most important features in the dataset for prediction and dropping the unnecessary features.","8ebffdd2":"# **Survived and Not Survived by Age and Embarked**","9851273d":"**Extreme Gradient Boosting**","044ac06d":"**Computation of Fare Range****","65ed6b4b":"Bagged KNN","39f62059":"**Corelation map (Heatmap) after Feature Selection and dropping columns for prediction**","cc4bc173":"Lists of the 6 input features and whether or not they were selected as well as their relative ranking of importance.","77e07f03":"# Prediction - Classification Algorithms","997d0691":"**Feature Selection using RFE (Recursive Feature Elimination)**","e88762bd":"**Voting Classifier**","39c27954":"**Linear SVC**","31131b58":"**Categorize Embarked to numeric variable**","b4dc1f10":"**Categorize sex to numeric variable**","c53bf640":"# **Ensemble Algorithms**","3cc3de7e":"Kernel SVM","4350f610":"*The RMS Titanic was a British passenger liner that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after it collided with an iceberg during its maiden voyage from Southampton to New York City. There were an estimated 2,224 passengers and crew aboard the ship, and more than 1,500 died, making it one of the deadliest commercial peacetime maritime disasters in modern history. The RMS Titanic was the largest ship afloat at the time it entered service and was the second of three Olympic-class ocean liners operated by the White Star Line.*\n\n**The goal of this project is to complete the analysis of what sorts of people were likely to survive.**","9f2c9d6b":"A hyperparameter is a parameter whose value is set before the learning process begins. Hyperparameter tuning is choosing a set of optimal parameters for a learning algorithm. Two different methods for optimizing hyperparameters : **GridSearch** and **RandomSearch**","07871592":"# **Survived and Non Survived male and female by PClass**","21f8258f":"# **Titanic Survival Prediction**","2dd627ca":"**Bagging**","25d67381":"Feature selection is one of the core concepts in machine learning which hugely impacts the performance of the model. Irrelavant or partially relavant features can negatively impact the performance of the model. Feature selection and data cleaning should be the first and most important step of your model designing.\n**Benefits of Feature Selection :**\n    1. Reduces Overfitting\n    2. Improves accuracy\n    3. Reduces training time\n**Feature Selection Methods :**\n    1. Intrinsic\n    2. Wrapper methods\n    3. Filter methods\nThe **intrinsic method** uses an algorithm ExtraTrees classifier.\nThe **Wrapper method** uses techniques such as Forward feature selection, Backward elimination, Recursive feature elimination.\nThe **filter method** is divided into two types. Statistical approach and Feature Importance. The statistical approaches are Pearson Coefficient, Spearman Coefficient, ANOVA, Chisquared Test and mutual information test. Corelation Heatmap is drawn to identify the feature importances.\nThe **embedded method** is a combination of wrapper and filter methods. The techniques in embedded methods are Ridge regression and Lasso regression.","1e02e167":"**Decision Tree Classification**","c653b6d3":"# **Hyperparameter Tuning**","583168a6":"Hyperparameters tuning for Kernel SVM, Decision Tree and Random Forest.","568541e5":"**SVM Classification**","784dfe5c":"Bagging also known as Bootstrap aggregation is a way to decrease the variance in the prediction and helps to avoid overfitting. Although it is usually applied to decision tree methods, it can be used with any type of method.\nBagging works best with models with high variance. An example for this can be Decision Tree or Random Forests. We can use KNN with small value of n_neighbours, as small value of n_neighbours.","8ad38f62":"# Feature Selection","00daf172":"**Survived and not survived by Pclass**","97f8ae7c":"**Corelation of all the attributes by Heatmap**","f4ab3ca4":"***Computation of Age Bins*******","67268129":"**Gaussian Naive Bayes Classification**","febea89d":"Survived and Not Survived by Embarked","2be0b63b":"**Extract Initials from the Name feature. Categorize the Initials by different values**","4f47dda2":"**Ada Boost Classifier**","fcc63fb6":"Ensemble methods are tecghniques that create multiple models and then combine them to produce improved results. Ensemble methods usually produce more accurate solutions than a single model would.\nIn ensemble algorithms, **bagging methods** form a class of algorithms which build several instances of a black-box estimator on random subsets of the original training set and then aggregate their individual predictions to form a final prediction.\n**Boosting** is an ensemble meta-algorithm for primarily reducing bias, and also variance.","2f6a21a6":"**Confusion Matrix**","ad3eafea":"Feature Importance","e8cbcea4":"# Feature Engineering","05635d94":"**Chisquare Test for Feature Selection**"}}