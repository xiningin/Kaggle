{"cell_type":{"cf42769f":"code","9269661c":"code","ed4133f0":"code","bc6f48a5":"code","6b6acd3b":"code","3543cb81":"code","10a98b85":"code","32491ebf":"code","a0127374":"code","9d07bd74":"code","986c31cc":"code","3ce1454f":"code","586649a7":"code","8da39911":"code","4d90f29d":"code","a6ff20b5":"code","e3ae412c":"code","6926a99a":"code","9b76a9c1":"code","223b8a2e":"code","db5532c4":"code","3a71debe":"code","bb3d1080":"code","489172be":"code","54e06bfc":"code","8c6bb6fb":"code","6a11c991":"code","a971bef3":"code","40dfe0e7":"code","756f589a":"code","99b1eb84":"markdown","43b5a5e3":"markdown","71d511d7":"markdown","63db2fa6":"markdown","b82d7d00":"markdown","151b9550":"markdown","a4ddd471":"markdown","82c3cf92":"markdown","574841c3":"markdown","12d253ba":"markdown","bd5f2de7":"markdown","e8a70cc3":"markdown"},"source":{"cf42769f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, cross_val_score, GridSearchCV\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn import preprocessing\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9269661c":"train_data = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","ed4133f0":"#Summary statistics\ntrain_data.describe()","bc6f48a5":"train_data['FamilySize'] = 1+train_data['Parch']+train_data['SibSp']\ntest_data['FamilySize'] = 1+test_data['Parch']+test_data['SibSp']","6b6acd3b":"train_data['Age'].fillna(train_data['Age'].mean(), inplace=True)\ntrain_data['Embarked'].value_counts()\ntrain_data['Embarked'].fillna('S',inplace=True)\ntrain_data.apply(lambda x: sum(x.isnull()),axis=0)","3543cb81":"test_data['Age'].fillna(test_data['Age'].mean(), inplace=True)\ntest_data['Fare'].fillna(test_data['Fare'].mean(), inplace=True)\ntest_data['Embarked'].value_counts()\ntest_data['Embarked'].fillna('S',inplace=True)\ntest_data.apply(lambda x: sum(x.isnull()),axis=0)","10a98b85":"sns.barplot(x='Pclass', y='Survived', data=train_data)","32491ebf":"features = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\"]\ntrain_data = pd.get_dummies(train_data, columns=features, prefix = features)\ntest_data = pd.get_dummies(test_data, columns=features, prefix = features)","a0127374":"train_data.sample(5)","9d07bd74":"sns.boxplot(x='Survived', y='Age', data=train_data)","986c31cc":"sns.boxplot(x='Survived', y='Fare', data=train_data)","3ce1454f":"sns.boxplot(x='Survived', y='FamilySize', data=train_data)","586649a7":"train_data[['normalized_Age', 'normalized_Fare']] = preprocessing.StandardScaler().fit_transform(train_data[['Age', 'Fare']])\ntest_data[['normalized_Age', 'normalized_Fare']] = preprocessing.StandardScaler().fit_transform(test_data[['Age', 'Fare']])","8da39911":"corr = train_data.corr()\nsns.heatmap(corr, cmap='coolwarm',annot=False,\n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values)\n\ntop_corr_features = corr.index\ntop_corr_features","4d90f29d":"bestfeatures = SelectKBest(score_func=chi2, k=10)\nX = train_data[['Age', 'Fare', 'FamilySize','Pclass_1', 'Pclass_2',\n       'Pclass_3', 'Sex_female', 'Sex_male', 'SibSp_0', 'SibSp_1', 'SibSp_2',\n       'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8', 'Parch_0', 'Parch_1',\n       'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5', 'Parch_6', 'Embarked_C',\n       'Embarked_Q', 'Embarked_S']]\ny = train_data['Survived']\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","a6ff20b5":"agebins = [0, 2, 12, 17, 60, 100]\ngroup_names = [1,2,3,4,5]\ntrain_data['AgeBin'] = pd.cut(train_data['Age'], agebins, labels=group_names)\ntest_data['AgeBin'] = pd.cut(test_data['Age'], agebins, labels=group_names)\n\nfarebins = [0, 8, 10, 20, 40, 600]\ngroup_names = [1,2,3,4,5]\ntrain_data['FareBin'] = pd.cut(train_data['Fare'], farebins, labels=group_names)\ntest_data['FareBin'] = pd.cut(test_data['Fare'], farebins, labels=group_names)\n\nfamilybins = [0, 1, 4, 6, 11]\ngroup_names = [1,2,3,4]\ntrain_data['FamilyBin'] = pd.cut(train_data['FamilySize'], familybins, labels=group_names)\ntest_data['FamilyBin'] = pd.cut(test_data['FamilySize'], familybins, labels=group_names)\n\n\n#sns.barplot(x='AgeBin', y='Survived', data=test_data)","e3ae412c":"features = [\"AgeBin\", \"FareBin\", \"FamilyBin\"]\ntrain_data = pd.get_dummies(train_data, columns=features, prefix = features)\ntest_data = pd.get_dummies(test_data, columns=features, prefix = features)","6926a99a":"train_data.columns","9b76a9c1":"X = train_data[['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'AgeBin_1',\n       'AgeBin_2', 'AgeBin_3', 'AgeBin_4', 'AgeBin_5', 'FareBin_1',\n       'FareBin_2', 'FareBin_3', 'FareBin_4', 'FareBin_5', 'FamilyBin_1',\n       'FamilyBin_2', 'FamilyBin_3', 'FamilyBin_4']]\ny = train_data['Survived']\n\nk_fold = KFold(n_splits=10)\nclassifier = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100","223b8a2e":"X = train_data[['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'SibSp_0',\n       'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8',\n       'Parch_0', 'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5',\n       'Parch_6', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'AgeBin_1',\n       'AgeBin_2', 'AgeBin_3', 'AgeBin_4', 'AgeBin_5', 'FareBin_1',\n       'FareBin_2', 'FareBin_3', 'FareBin_4', 'FareBin_5']]\ny = train_data['Survived']\n\nk_fold = KFold(n_splits=10)\nclassifier = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100\n","db5532c4":"classifier = XGBClassifier()\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100","3a71debe":"Model 3","bb3d1080":"classifier = DecisionTreeClassifier(max_depth=5)\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100","489172be":"clf = RandomForestClassifier(n_estimators=200, bootstrap=False, criterion='gini',min_samples_leaf = 1,\n min_samples_split=2, max_depth=5, random_state=42).fit(X,y)\nclf.score(X,y)","54e06bfc":"k_fold = KFold(n_splits=10)\nclassifier = RandomForestClassifier(n_estimators=200, bootstrap=False, criterion='gini',min_samples_leaf = 1,\n min_samples_split=2, max_depth=5, random_state=42)\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100","8c6bb6fb":"features = X.columns\nimportances = clf.feature_importances_\nindices = np.argsort(importances)\n\nplt.title('Feature Importances')\nplt.barh(range(len(indices)), importances[indices], color='b', align='center')\nplt.yticks(range(len(indices)), [features[i] for i in indices])\nplt.xlabel('Relative Importance')\nplt.show()","6a11c991":"clf = XGBClassifier(n_estimators=70, eta=0.06, gamma=0.1, max_depth = 5, objective = 'binary:logistic', reg_lambda = 2).fit(X,y)\nclf.score(X,y)","a971bef3":"k_fold = KFold(n_splits=10)\nclassifier = XGBClassifier(n_estimators=70, eta=0.06, gamma=0.1, max_depth = 5, objective = 'binary:logistic', reg_lambda = 2)\nresults_kfold_CV = cross_val_score(classifier, X, y, cv=k_fold, n_jobs=-1)\n\nresults_kfold_CV.mean()*100","40dfe0e7":"X_test = test_data[['Pclass_1', 'Pclass_2', 'Pclass_3', 'Sex_female', 'Sex_male', 'SibSp_0',\n       'SibSp_1', 'SibSp_2', 'SibSp_3', 'SibSp_4', 'SibSp_5', 'SibSp_8',\n       'Parch_0', 'Parch_1', 'Parch_2', 'Parch_3', 'Parch_4', 'Parch_5',\n       'Parch_6', 'Embarked_C', 'Embarked_Q', 'Embarked_S', 'AgeBin_1',\n       'AgeBin_2', 'AgeBin_3', 'AgeBin_4', 'AgeBin_5', 'FareBin_1',\n       'FareBin_2', 'FareBin_3', 'FareBin_4', 'FareBin_5']]\npred = clf.predict(X_test)","756f589a":"testDF = pd.DataFrame()\n#pred = clf.predict(X_test)\ntestDF['PassengerId'] = test_data['PassengerId']\ntestDF['Survived'] = pred\ntestDF.to_csv('submission.csv', index=False)","99b1eb84":"Missing values for training data","43b5a5e3":"Final model","71d511d7":"Model 2","63db2fa6":"XGB classifier","b82d7d00":"clf.best_params_","151b9550":"Model 1","a4ddd471":"params={\"n_estimators\":[67,70,100,120],'reg_lambda':[2,1],'gamma':[0,0.3,0.2,0.1]\n       ,'eta':[0.06,0.05,0.04]\n        ,\"max_depth\":[3,5],'objective':['binary:logistic']}\nmodel = XGBClassifier() \nclf=GridSearchCV(model,params,cv=10,n_jobs=-1,verbose=1) \nclf.fit(X, y)","82c3cf92":"clf.best_params_","574841c3":"grid_param = {\n 'n_estimators': [100, 200, 300],\n 'criterion':['gini', 'entropy'],\n 'min_samples_split': [2, 10, 20],\n 'min_samples_leaf': [1, 5],\n 'bootstrap': [True, False],\n    'max_depth': [1,2,3,4,5],\n}\n\nmodel = RandomForestClassifier(random_state=42)\nclf=GridSearchCV(model,grid_param,cv=10,n_jobs=-1,verbose=1)\nclf.fit(X, y)","12d253ba":"Categorize fare and age to improve robsutness","bd5f2de7":"Here, we first load training data and test data","e8a70cc3":"Missing values for test data"}}