{"cell_type":{"0832aecf":"code","dfcc5d0e":"code","3fac10f6":"code","ccee2137":"code","6a65935b":"code","6ebdc782":"code","7abf0db9":"code","f3213eb7":"code","7e06e415":"code","fc3327fd":"code","1ce2ad66":"code","102c6e9c":"code","0a5a359c":"code","af2ad544":"code","af4bd54b":"code","f103ecdf":"code","06ff3657":"code","c209330b":"code","8eac0b03":"code","f824ede0":"code","0cffc608":"code","d3fe4491":"code","245960fb":"code","028335d8":"code","6ce3ca45":"code","e85d51e9":"markdown","1d48c80c":"markdown","eb397320":"markdown","d90b5dd1":"markdown","4d3dbb17":"markdown","901cfff0":"markdown","17edfed1":"markdown","5d475483":"markdown","b89083ea":"markdown","e4f17df0":"markdown","b9d4a859":"markdown","d1a6ca5e":"markdown"},"source":{"0832aecf":"# IMPORTING LIBRARIES & MAIN PATH\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\n","dfcc5d0e":"# IMPORTING and INVESTIGATING DATA\n\nstars_data = pd.read_csv('..\/input\/star-type-classification\/Stars.csv')\nstars_data.info()","3fac10f6":"# Let's take a look at our data\n\nstars_data.head()","ccee2137":"# Looking for duplicates\n\nstars_data.duplicated().sum()","6a65935b":"# Looking at NaN % within the data\n\nnan = pd.DataFrame(stars_data.isna().sum(), columns = ['NaN_sum'])\nnan['feat'] = nan.index\nnan['Perc(%)'] = (nan['NaN_sum']\/1460)*100\nnan = nan[nan['NaN_sum'] > 0]\nnan = nan.sort_values(by = ['NaN_sum'])\nnan['Usability'] = np.where(nan['Perc(%)'] > 20, 'Discard', 'Keep')\nnan","6ebdc782":"# Among how many classes do we have to discriminate ?\n\nstars_type = ['Red Dwarf','Brown Dwarf','White Dwarf','Main Sequence','Super Giants','Hyper Giants']\nstars_data['Class'] =  stars_data['Type'].replace(stars_data['Type'].unique(),stars_type)\nprint(stars_data['Class'].unique())","7abf0db9":"# Are the classes balanced ?\n\nsns.set_theme('paper')\nplt.figure(figsize = (10,5))\nsns.countplot(x='Class', data = stars_data)\nplt.title(\"Classes' distribution across data\", fontsize = 14)\nplt.xlabel(' ', fontsize = 12)\nplt.ylabel('Class Frequency (n)', fontsize = 12)\nplt.show()","f3213eb7":"# Color feature correction\n\nstars_data['Color'].loc[stars_data['Color'] =='Blue-white'] = 'Blue-White'\nstars_data['Color'].loc[stars_data['Color'] =='Blue White'] = 'Blue-White'\nstars_data['Color'].loc[stars_data['Color'] =='Blue white'] = 'Blue-White'\nstars_data['Color'].loc[stars_data['Color'] =='yellow-white'] = 'White-Yellow'\nstars_data['Color'].loc[stars_data['Color'] =='Yellowish White'] = 'White-Yellow'\nstars_data['Color'].loc[stars_data['Color'] =='white'] = 'White'\nstars_data['Color'].loc[stars_data['Color'] =='yellowish'] = 'Yellowish'","7e06e415":"# Let's look at the descriptive statistics\n\nstars_data.describe()","fc3327fd":"# Looking at our numerical features' ditributions\n\nf, ax = plt.subplots(1,4, figsize = (20,6))\n\nsns.distplot(stars_data['Temperature'],fit=norm, color='#FB8861', ax = ax[0])\nax[0].set_title('Temperature \\n Normal dist.', fontsize=14)\n\nsns.distplot(stars_data['L'],fit=norm, color='#C5B3F9', ax = ax[1])\nax[1].set_title('Main-sequence luminosity \\n Normal dist.', fontsize=14)\n\nsns.distplot(stars_data['R'],fit=norm,color='g', ax = ax[2])\nax[2].set_title('Main-sequence radius \\n Normal dist.', fontsize=14)\n\nsns.distplot(stars_data['A_M'],fit=norm, color='y', ax = ax[3])\nax[3].set_title('Absolute Magnitute \\n Normal dist.', fontsize=14)\n\nplt.show()","1ce2ad66":"# Looking at our numerical features' descriptive aspects \nf, ax = plt.subplots(2,2, figsize = (20,15))\n\nsns.boxplot(x = stars_data['Class'], y = stars_data['Temperature'], color='#FB8861', ax = ax[0][0])\nax[0][0].set_title('Temperature by Class', fontsize=14)\nax[0][0].set_xlabel(' ')\n\nsns.boxplot(x = stars_data['Class'], y = stars_data['L'], color='#C5B3F9', ax = ax[0][1])\nax[0][1].set_title('Main-sequence luminosity by Class', fontsize=14)\nax[0][1].set_xlabel(' ')\n\nsns.boxplot(x = stars_data['Class'], y = stars_data['R'], color='g', ax = ax[1][0])\nax[1][0].set_title('Main-sequence radius by Class', fontsize=14)\nax[1][0].set_xlabel(' ')\n\nsns.boxplot(x = stars_data['Class'], y = stars_data['A_M'], color='y', ax = ax[1][1])\nax[1][1].set_title('Absolute Magnitute by Class', fontsize=14)\nax[1][1].set_xlabel(' ')\n\nplt.show()","102c6e9c":"# Correlation Heatmaps (Pearson - Spearman)\n\nf, ax = plt.subplots(1,2, figsize=(15, 15))\n\nmat_p = stars_data.drop('Type', axis = 1).corr('pearson')\nmat_s = stars_data.drop('Type', axis = 1).corr('spearman')\n\nmask_p = np.triu(np.ones_like(mat_p, dtype=bool))\nmask_s = np.triu(np.ones_like(mat_s, dtype=bool))\n\ncmap = sns.diverging_palette(230, 20, as_cmap=True)\n\nsns.heatmap(mat_p, mask=mask_p, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},ax = ax[0])\nax[0].set_title('Correlation Heatmap (Pearson)', fontsize = 14)\n\nsns.heatmap(mat_s, mask=mask_s, cmap=cmap, vmax=1, center=0, annot = True,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5},ax = ax[1])\nax[1].set_title('Correlation Heatmap (Spearman)', fontsize = 14)\n\nplt.show()","0a5a359c":"# Let's look at the scatterplots\n\nsns.pairplot(data=stars_data,hue=\"Type\")\nplt.show()","af2ad544":"# Outliers removal function\n\ndef outliers_removal(feature,feature_name,dataset):\n    \n    # Identify 25th & 75th quartiles\n\n    q25, q75 = np.percentile(feature, 25), np.percentile(feature, 75)\n    print('Quartile 25: {} | Quartile 75: {}'.format(q25, q75))\n    feat_iqr = q75 - q25\n    print('iqr: {}'.format(feat_iqr))\n    \n    feat_cut_off = feat_iqr * 1.5\n    feat_lower, feat_upper = q25 - feat_cut_off, q75 + feat_cut_off\n    print('Cut Off: {}'.format(feat_cut_off))\n    print(feature_name +' Lower: {}'.format(feat_lower))\n    print(feature_name +' Upper: {}'.format(feat_upper))\n    \n    outliers = [x for x in feature if x < feat_lower or x > feat_upper]\n    print(feature_name + ' number of outliers deleted: {}'.format(len(outliers)))\n    #print(feature_name + ' outliers:{}'.format(outliers))\n\n    dataset = dataset.drop(dataset[(dataset[feature_name] > feat_upper) | (dataset[feature_name] < feat_lower)].index)\n    print('-' * 65)\n    \n    return dataset\n\n\ndata_cleaned = outliers_removal(stars_data['L'],'L', stars_data)","af4bd54b":"# Separating numerical from categorical variables\n\nnum_feat = stars_data.drop(['Color','Spectral_Class','Type','Class'], axis = 1)\ncat_feat = stars_data.drop(['Temperature','L','R','A_M','Type','Class'], axis = 1)","f103ecdf":"# Getting dummy variables\n\ndata_dummy = pd.get_dummies(cat_feat)\ndata_dummy.head()","06ff3657":"# Scaling\n\nfrom sklearn.preprocessing import StandardScaler, Normalizer, MinMaxScaler\n\nscaler = MinMaxScaler()\ndata_scaled = scaler.fit_transform(num_feat)\ndata_scaled = pd.DataFrame(data_scaled, columns = num_feat.columns)\ndata_scaled.head()","c209330b":"# Crating our final train dataframe\n\ndata_complete = data_scaled.join(data_dummy)\ndata_complete.head()","8eac0b03":"# Splitting the data into Train & Test sets\n\nfrom sklearn.model_selection import train_test_split\n\n# Defining our labels\n\nlabels = stars_data['Class']\n\n# Splitting the data\n\nXtrain,X_test,ytrain,y_test = train_test_split(data_complete,labels,\n                                               test_size = 0.1,\n                                               stratify = labels,\n                                               shuffle = True)\n\n\nX_train,X_val,y_train,y_val = train_test_split(Xtrain,ytrain,\n                                               test_size = 0.1,\n                                               stratify = ytrain,\n                                               shuffle = True)","f824ede0":"# Machine Learning Libraries\n\nimport sklearn\nfrom sklearn import tree\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score\nfrom sklearn.metrics import recall_score, f1_score, roc_auc_score\nfrom sklearn.metrics import classification_report","0cffc608":"# LOGISTIC REGRESSION\n\nl_reg = LogisticRegression(random_state = 42)\nlog_model = l_reg.fit(X_train,y_train)\npred = log_model.predict(X_val)\n\nprint(classification_report(y_val, pred , target_names = stars_data['Class'].unique()))","d3fe4491":"# RANDOM FOREST CLASSIFIER\n\nrf = RandomForestClassifier(random_state = 42)\nrf_model = rf.fit(X_train,y_train)\npred_rf = rf_model.predict(X_val)\n\nprint(classification_report(y_val, pred_rf , target_names = stars_data['Class'].unique()))","245960fb":"# Looking at feature importance\n\nimportance_rf = rf_model.feature_importances_\nfeat_names = list(zip(importance_rf, X_train.columns))\n\n# summarize feature importance\nfor i,v in enumerate(importance_rf):\n    #print('Feature: %0d, Score: %.5f' % (i,v))\n    print(str(feat_names[i][1]) +': '+ '%.5f' % (v))","028335d8":"# Plotting feature importance\n\nplt.figure(figsize = (15,7))\nsns.barplot(y = X_train.columns, x = importance_rf, orient = 'h')\nplt.title('Random Forest Features importance', fontsize = 14)\nplt.xlabel('Importance', fontsize = 12)\nplt.show()","6ce3ca45":"from IPython.display import Image\nfrom subprocess import call\nfrom sklearn.tree import export_graphviz\n\nestimator = rf_model.estimators_[5]\n\nexport_graphviz(estimator, \n                out_file='tree.dot', \n                feature_names = X_train.columns,\n                class_names = stars_data['Class'],\n                rounded = True, proportion = False, \n                precision = 2, filled = True)\n\ncall(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600'])\nImage(filename = 'tree.png')","e85d51e9":"Definitely YES! Our classes are equally distributed among the data \ud83e\uddbe. Now we can make some considerations about our dataset and the single features that compose it.\n\n## General data considerations:\n\nAs we can see, the dataset is quite small (240 observations); this should reduce our probabilities to obtain robust results (at least at the scientific level). Nevertheless, the data are already enough to get an idea of which are the main features used by different algorithms to achieve the task.\n\n\n## Features Description:\n\nnumerical features:\n\n- **Temperature**  (Kelvin)\n- **Main-sequence luminosity**  (L - Solar luminosity)\n- **Main-sequence radius**  (R - Solar radius)\n- **Absolute Magnitute**  (AM - is a measure of the luminosity of a celestial object, on an inverse logarithmic astronomical magnitude scale)\n\nand categorical features:\n- **Color**\n- **Spectral Class**  (*Morgan\u2013Keenan* (MK) system using the letters: O (hottest),B,A,F,G,K,M (coldest))\n- **Type**  (Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants)\n\n","1d48c80c":"Looking at the plot we can clearly see that **Absolute Magnitude, Main_sequence Radius, Main_sequence Luminosity and Temperature** are the ones with the highest level of importance. These are the features with the highest weight in the final classification. Now, let's investigate more in detail how an estimator reaches the final decision:","eb397320":"## INTRODUCTION TO STARS CLASSIFICATION\n\nWELCOME to this notebook!\n\nThe aim of this work is to find a way to classify the different types of stars defined in the **Hertzsprung-Russel diagram** (shown here below). This graph is a scatter plot of stars showing the relationship between the stars' absolute magnitudes or luminosities versus their stellar classifications or effective temperatures. You can find a more detailed description of this graph [here](https:\/\/en.wikipedia.org\/wiki\/Hertzsprung%E2%80%93Russell_diagram). \n","d90b5dd1":"![image.png](attachment:b93394a8-6d27-4a71-9c90-067c20d41bd6.png)","4d3dbb17":"Let's take a look at the properties of our numerical variables:","901cfff0":"**Still under processing...**","17edfed1":"Looking carefully into the dataset we can see that the '*Color*' feature contains different spelling errors that define more colors than we have. I used the snippet of the code above to correct them.","5d475483":"## EXPLORATORY DATA ANALYSIS (EDA) \ud83d\udcca","b89083ea":"WoW, no missing data are present either! Ok, now that we have controlled our dataset for duplicates and missing values we need to know something more about the classes that we need to discriminate:","e4f17df0":"No duplicated observations are present in our dataset. What about missing values (**Nan**)?","b9d4a859":"Let's start importing all the libraries necessary to complete the project! ","d1a6ca5e":"## EVALUATION"}}