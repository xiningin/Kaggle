{"cell_type":{"33d53614":"code","3791e377":"code","2cc1c936":"code","3fc66e98":"code","ebb3d54a":"code","ec1ef257":"code","5b8ed346":"code","850b8459":"code","d0fce351":"markdown","7d14d336":"markdown","df1f5fc6":"markdown","2681f71b":"markdown"},"source":{"33d53614":"import matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.stats import norm","3791e377":"class LogVarianceNorm:\n    def calculate_gradient(self, t, m, log_var):\n        var =  np.exp(log_var)\n        gradients = [\n            (m - t) \/ var,\n            1 - ((t - m) ** 2) \/ var\n        ]\n\n        fisher_matrix = [\n            [1 \/ var, 0],\n            [0, 1]\n        ]\n\n        return np.array(gradients), np.array(fisher_matrix)\n    \n    def score(self, t, m, log_var):\n        var = np.exp(log_var)\n        return - norm.logpdf(t, loc=m, scale=var ** .5)\n","2cc1c936":"def true_function(X):\n    return np.sin(3 * X)\n\ndef true_noice_scale(X):\n    return abs(np.sin(X))\n\nn_samples = 200\nnp.random.seed(71)\nX = np.random.uniform(-3, 1, n_samples)\ny = true_function(X) + np.random.normal(scale=true_noice_scale(X), size=n_samples)","3fc66e98":"xx = np.linspace(-3.3, 1.2, 301)\nplt.scatter(X, y)\nplt.plot(xx, true_function(xx))","ebb3d54a":"from sklearn.tree import DecisionTreeRegressor\n\ndef get_weak_learner():\n    return DecisionTreeRegressor(max_depth=3, min_samples_leaf=3)","ec1ef257":"\nn_samples = 200\nnp.random.seed(71)\nX = np.random.uniform(-3, 1, n_samples)\ny = true_function(X) + np.random.normal(scale=abs(X), size=n_samples)\n\nlog_var_norm = LogVarianceNorm()\n\n# hyper parameters\nlearning_rate = 2e-2\nn_iterations = 1000\n\nestimators = []\nstep_sizes = []\n\nX = X.reshape(-1, 1)\nN = len(X)\n\n# initialize parameters by whole dataset mean and variance\nm, log_var = np.zeros(shape=(N,)), np.zeros(shape=(N,))\nm += np.mean(y)\nlog_var += np.log(np.var(y) ** .5)\n\nfor iteration in range(n_iterations):\n\n    # 1. compute natural gradient\n    grads = []\n    for i in range(N):\n        score, fisher = log_var_norm.calculate_gradient(y[i], m[i], log_var[i])\n        grad = np.linalg.solve(fisher, score)\n        grads.append(grad)\n    grads = np.array(grads)\n\n    # 2. fit weak learner\n    # mean estimator\n    clf_mean = get_weak_learner()\n    clf_mean.fit(X, y=grads[:, 0])\n\n    # log_var estimator\n    clf_var = get_weak_learner()\n    clf_var.fit(X, y=grads[:, 1])\n\n    directions = np.zeros(shape=(N, 2))\n    directions[:, 0] = clf_mean.predict(X)\n    directions[:, 1] = clf_var.predict(X) \n    estimators.append(\n        [clf_mean, clf_var]\n    )\n\n    # 3. linear search\n    scores = []\n    stepsize_choices = np.linspace(.5, 2, 21)\n    for stepsize in stepsize_choices:\n        d = directions * stepsize\n        score_i = log_var_norm.score(y, m - d[:, 0], log_var - d[:, 1]).mean()\n        scores.append(score_i)\n\n    best_idx = np.argmin(scores)\n    rho_i = stepsize_choices[best_idx]\n    \n    stepsize_i = learning_rate * rho_i\n    step_sizes.append(stepsize_i)\n\n    # 4. update parameters\n    grad_parameters = directions * stepsize_i\n    m -= grad_parameters[:, 0]\n    log_var -= grad_parameters[:, 1]\n    \n    if iteration % 50 == 0:\n        print(f'[iter: {iteration}]\\tscore: {scores[best_idx]:.4f}\\tstepsize: {rho_i:.3f}')","5b8ed346":"xx = np.linspace(-4, 2, 501) # predict input X\n\nxx_mean = np.zeros_like(xx) + np.mean(y)\nxx_log_var = np.zeros_like(xx) + np.log(np.var(y) ** .5)\n\n# Prediction results of all weak learners are weighted and added together\nfor i in range(len(estimators)):\n    xx_mean -= step_sizes[i] * estimators[i][0].predict(xx.reshape(-1, 1))\n    xx_log_var -= step_sizes[i] * estimators[i][1].predict(xx.reshape(-1, 1))\n\nxx_var = np.exp(xx_log_var)","850b8459":"fig, axes = plt.subplots(figsize=(10, 8), nrows=2, sharex=True)\naxes[0].scatter(X, y, label='Training Points')\naxes[0].plot(xx, xx_mean, c='C1', label='Predict (mean)')\naxes[0].fill_between(xx, xx_mean - xx_var ** .5, xx_mean + xx_var ** .5, alpha=.3, color='C1', label='One Sigma')\naxes[0].legend()\n\naxes[1].plot(xx, xx_var ** .5, label='Predict (std)')\naxes[1].plot(xx, true_noice_scale(xx), label='Groud Truth')\naxes[1].set_ylabel('model uncertainty')\naxes[1].legend()\nfig.tight_layout()","d0fce351":"Visualize Model Predict Mean and variance","7d14d336":"# Simple NGBoost Implementation","df1f5fc6":"## Distribution\n\nIn NGBoost, fit target distribution's parameter $\\theta$ instead of target directly. In this notebook, consider normal distribution and estimate paramsters, mean and logarithm variance.","2681f71b":"Generate Toy Samples"}}