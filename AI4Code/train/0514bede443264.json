{"cell_type":{"9448b8ba":"code","2a97b915":"code","4e8df344":"code","f71e81be":"code","4bc94400":"code","5d38b7ba":"code","a6799630":"code","983ddacc":"code","5aebaeed":"code","ce664a61":"code","9e91c546":"code","5840fd18":"code","90c8ce42":"code","519ecf00":"code","31598c98":"code","e4a9d48d":"code","5c4b8dfa":"code","345c5233":"code","d12910d2":"code","e8f04579":"code","bc383bab":"code","83583af2":"code","a0fac787":"code","d558be63":"code","499fe1b9":"code","951b5bc8":"code","3fbdaf8e":"code","f48cdbe0":"code","e036a64c":"code","ad604c25":"code","e15a77ba":"code","eee93a89":"markdown","33dbcacf":"markdown","ee719f4c":"markdown","1b3bd132":"markdown","c1275502":"markdown","1ca910df":"markdown","bb48b8a0":"markdown","667ac1f4":"markdown","99eb7407":"markdown","f54ea9c4":"markdown","fdbc16fd":"markdown","370680c7":"markdown","a7c75488":"markdown","34cb8bc3":"markdown","5a368b7f":"markdown","76695d61":"markdown","aa8adbc1":"markdown","e19441f4":"markdown","da829730":"markdown","d88c58ba":"markdown","4cda1828":"markdown","67de4c28":"markdown","29c40965":"markdown","b2a7f63e":"markdown","e13be1f4":"markdown","ae868fab":"markdown"},"source":{"9448b8ba":"#importing libraries\nimport numpy as np\nimport pandas as pd\nfrom pandas import DataFrame\nimport matplotlib as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor\nimport xgboost\nimport seaborn as sns\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Importing data\ntrainF = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntestF = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","2a97b915":"trainF.head()","4e8df344":"print('\\nTrain Data Columns :',trainF.info())","f71e81be":"trainF.describe().transpose()","4bc94400":"trainF_corr = trainF.corr()\ntrainF_corr.style.background_gradient(cmap='coolwarm',axis=None)","5d38b7ba":"\ntrainF_corr[['SalePrice','OverallQual']].style.background_gradient(cmap='coolwarm',axis=None)","a6799630":"pd.set_option(\"max_columns\",None)\npd.set_option(\"max_rows\", None)\n\ntrainF.head(20)","983ddacc":"pd.set_option(\"max_columns\",None)\npd.set_option(\"max_rows\", None)\nprint(trainF.isnull().sum() ,'\\n\\n\\n',testF.isnull().sum())","5aebaeed":"trainF['Train'] = 1\nr = trainF['SalePrice']\nprint(trainF.shape)\n\ntestF['Train'] = 0\nprint(testF.shape)\n\ncombined = pd.concat([trainF,testF])\ncombined.shape","ce664a61":"total2 = combined.isnull().sum().sort_values(ascending=False)\n#print(total2.shape)\n\npercent2 = (combined.isnull().sum()\/combined.isnull().count()).sort_values(ascending=False)\n#print(percent2.shape)\n\nmissing_data2 = pd.concat([total2,percent2], axis=1, keys=['Total','Percent'])\n\nprint(missing_data2.head(40))","9e91c546":"combined = combined.drop((missing_data2[missing_data2['Total']>1]).index,1)\n\n#combined.fillna(combined.mean(), inplace=True)\n#combined = combined.drop(combined.loc[combined['Electrical'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['Exterior1st'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['Exterior2nd'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['BsmtFinSF1'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['BsmtFinSF2'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['BsmtUnfSF'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['TotalBsmtSF'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['KitchenQual'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['GarageCars'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['GarageArea'].isnull()].index)\n#combined = combined.drop(combined.loc[combined['SaleType'].isnull()].index)\n\nprint(combined.isnull().sum() ,'\\n\\n\\n')","5840fd18":"#Categorical Data\ncategorical = ['SaleType','KitchenQual','Electrical','Exterior1st','Exterior2nd']\nfor feature in categorical:\n    combined[feature].fillna(combined[feature].mode()[0],inplace=True)\n    \nprint(combined.isnull().sum() ,'\\n\\n\\n')","90c8ce42":"combined.info()","519ecf00":"q = combined.select_dtypes(exclude=['int64','float64'])\nprint(q.columns)\nq = pd.get_dummies(q)\n\nq.info()","31598c98":"    \ncombined = combined.drop(['Street', 'LotShape', 'LandContour', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'ExterQual',\n       'ExterCond', 'Foundation', 'Heating', 'HeatingQC', 'CentralAir',\n       'Electrical', 'KitchenQual', 'PavedDrive', 'SaleType', 'SaleCondition'],axis=1)\n","e4a9d48d":"print(combined.head())","5c4b8dfa":" combined = pd.concat([combined, q], axis=1)","345c5233":"combined.head()","d12910d2":"trainf = combined[combined['Train']== 1]\ntrainf = pd.concat([trainf,r],axis=1)\n\ntestf = combined[combined['Train']== 0]\n\ntrainf.drop(['Train'],axis=1,inplace=True)\ntestf.drop(['Train'],axis=1,inplace=True)","e8f04579":"trainf.head()","bc383bab":"testf.info()","83583af2":"#model = RandomForestRegressor(random_state=42,n_estimators=1000,criterion='mae')\nmodel = GradientBoostingRegressor(random_state=58,n_estimators=500,loss='huber',max_depth=3,max_features=25)","a0fac787":"#from sklearn.metrics import median_absolute_error\n#GS = GridSearchCV(model,random_grid,cv = 3,scoring='neg_median_absolute_error',verbose=1,n_jobs=-1)\n#Fit the random search model\n#GS.fit(X,y)","d558be63":"y = trainf['SalePrice']\nX = trainf.drop(columns = ['SalePrice'])\nX.fillna(X.mean(), inplace=True)\n#np.where(X.values >= np.finfo(np.float32).max)\n\nmodel.fit(X,y)\nmodel.score(X,y)\n\nmodel.fit(X, y)\n","499fe1b9":"#testf = testf.drop(columns = ['Id'])\ntestf.fillna(testf.mean(), inplace=True)","951b5bc8":"y_pred = model.predict(testf)","3fbdaf8e":"y_pred","f48cdbe0":"model.score(testf,y_pred)","e036a64c":"submission = pd.DataFrame({\n    'Id':testf.Id,\n    'SalePrice':y_pred\n})\nsubmission.to_csv('submission.csv',index=False)\nprint('submission.csv')","ad604c25":"#y = combined['SalePrice']\n#X = combined.drop(combined['SalePrice'])\n\n# Train Test split\n#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n","e15a77ba":"# Linear Regression Model\n#lr = LinearRegression()\n#lr.fit(X_train, y_train)\n#y_pred = lr.predict(X_test)","eee93a89":"Seprating train and test dataset","33dbcacf":"> Predicting SalePrice for test data","ee719f4c":"# 1. Import libraries and Data","1b3bd132":"# 3.Training Model","c1275502":"Filling Missing Categorical Data (SaleType,KitchenQual,Electrical,Exterior1st,Exterior2nd)","1ca910df":"# 4.Testing Model ","bb48b8a0":"> Now no missing value","667ac1f4":"Concating Qualatitive and Quantative dataset","99eb7407":"model = RandomForestRegressor(random_state=42,bootstrap = True,max_depth = 10,\n                              max_features ='auto',\n                              min_samples_leaf=2,\n                              min_samples_split=5,\n                              n_estimators=200,\n                            criterion='mae')","f54ea9c4":"#my_imputer = SimpleImputer()\n#data_with_imputed_values = my_imputer.fit_transform(trainF)\n\nfrom sklearn.impute import SimpleImputer\n\nfor i in range(trainF.shape[1]):\n\t# count number of rows with missing values\n\tn_miss = trainF[[i]].isnull().sum()\n\tperc = n_miss \/ trainF.shape[0] * 100\n\tprint('> %d, Missing: %d (%.1f%%)' % (i, n_miss, perc))\n\n# Missing values is represented using NaN and hence specified. If it \n# is empty field, missing values will be specified as ''\n\n\n#imputer = SimpleImputer(missing_values=None, strategy='most_frequent')\n\n#trainF.gender = imputer.fit_transform(trainF['gender'].values.reshape(-1,1))[:,0]\n\n#trainF","fdbc16fd":"1. **Import necesssary libraries and import data.**\n2. **Exploratory Data Analysis.**\n3. **Training Model.**\n4. **Testing Model.**","370680c7":"**\nMissing values**","a7c75488":"Correlation ","34cb8bc3":"> Using RandomForestRegressor ","5a368b7f":"Upvote , if you find this notebook helpful.","76695d61":"Percentage of total missing data","aa8adbc1":"> Handling dummy data\n","e19441f4":"> Check columns of data","da829730":"*Checking missing values in both train and test data*","d88c58ba":"# 2.Exploratory Data Analysis","4cda1828":"Thank You for giving your valuble Time.\nConsider Upvote","67de4c28":"Merging train and test data for further encoding","29c40965":"Dropping unnecessary features and rows.","b2a7f63e":"#SalePrice has highest correlation with Overall_Qual","e13be1f4":"Dropping old dummy variables","ae868fab":"*Describe data*"}}