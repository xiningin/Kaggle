{"cell_type":{"bc00391a":"code","b9280cc8":"code","5fd55912":"code","199d661e":"code","72be5e54":"code","4ac91d61":"code","30082f0f":"code","97ad9909":"code","20f93fa6":"code","e5d379bf":"code","28644980":"code","43fbb686":"code","7121c2ff":"code","02975efa":"code","2ce3bb43":"code","3f587fe5":"code","3188b2e0":"code","6b8af311":"code","d4ad070b":"code","f1a015e7":"code","617c7fe9":"markdown","d9f0f9c1":"markdown","dc1d6c86":"markdown","6f79cf16":"markdown","f7fe3372":"markdown","18a9b5f0":"markdown","fb2f395f":"markdown","7e0dd9c3":"markdown","b3c9fdb5":"markdown","dc54abea":"markdown","c3fe958a":"markdown","0d2e2c4e":"markdown","ee8a82a5":"markdown","b996577d":"markdown","45f60783":"markdown","2edf82f6":"markdown","58788ada":"markdown","f2c937f8":"markdown","0a4bdf24":"markdown","55cd4069":"markdown","551a8de8":"markdown","4fafb42d":"markdown","ffb459b1":"markdown","20651711":"markdown","0ba2fa67":"markdown","fa5d534a":"markdown","3ed7305b":"markdown","d480e2b4":"markdown","d90ac012":"markdown","c7f092e0":"markdown"},"source":{"bc00391a":"!pip install kaggle","b9280cc8":"!mkdir ~\/.kaggle\/","5fd55912":"!cp kaggle.json ~\/.kaggle\/","199d661e":"!chmod 600 ~\/.kaggle\/kaggle.json","72be5e54":"!kaggle datasets download -d splcher\/animefacedataset","4ac91d61":"!unzip animefacedataset -d '\/content\/AnimeFaces\/' ","30082f0f":"import os\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset \nimport cv2\nfrom PIL import Image\nimport numpy as np\nfrom PIL import ImageFile\nimport csv\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport torch.nn.functional as F\nfrom IPython.display import display\nto_pil = torchvision.transforms.ToPILImage()\nimport random\n# Set random seed for reproducibility\nmanualSeed = 999\n#manualSeed = random.randint(1, 10000) # use if you want new results\nprint(\"Random Seed: \", manualSeed)\nrandom.seed(manualSeed)\ntorch.manual_seed(manualSeed)","97ad9909":"# Root directory for dataset\ndataroot = \"\/content\/AnimeFaces\"\n\n# Number of workers for dataloader\nworkers = 2\n\n# Batch size during training\nbatch_size = 128\n\n# Spatial size of training images. All images will be resized to this\n#   size using a transformer.\nimage_size = 64\n\n# Number of channels in the training images. For color images this is 3\nnc = 3\n\n# Size of z latent vector (i.e. size of generator input)\nnz = 100\n\n# Size of feature maps in generator\nngf = 64\n\n# Size of feature maps in discriminator\nndf = 64\n\n# Number of training epochs\nnum_epochs = 5\n\n# Learning rate for optimizers\nlr = 0.0002\n\n# Beta1 hyperparam for Adam optimizers\nbeta1 = 0.5\n\n# Number of GPUs available. Use 0 for CPU mode.\nngpu = 1","20f93fa6":"#Defining the transformations to be applied on the images\ntransform = transforms.Compose([\n    transforms.Resize((64 , 64)) , \n    transforms.ToTensor() , \n    transforms.Normalize((0.5 , 0.5 , 0.5), (0.5 , 0.5 , 0.5)),\n])\n\n# Creating the dataset by using the ImageFolder class\ndataset = torchvision.datasets.ImageFolder(root = dataroot, transform = transform)\n\n#loading the data using pytorch's DataLoader\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, drop_last = True, shuffle=True)\n\n# Defining the device. Here we will be using an Nvidia GPU in order to train the Neural Network.\ndevice = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")","e5d379bf":"# Plot some training images\nreal_batch = next(iter(dataloader))\nplt.figure(figsize=(8,8))\nplt.axis(\"off\")\nplt.title(\"Training Images\")\nplt.imshow(np.transpose(torchvision.utils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))","28644980":"def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)","43fbb686":"class Generator(nn.Module):\n    def __init__(self, ngpu):\n        super(Generator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is Z, going into a convolution\n\n            nn.ConvTranspose2d( nz, ngf * 8, 4, 1, 0, bias=False),\n            nn.BatchNorm2d(ngf * 8),\n            nn.ReLU(True),\n            # state size. (ngf*8) x 4 x 4\n\n            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 4),\n            nn.ReLU(True),\n            # state size. (ngf*4) x 8 x 8\n\n            nn.ConvTranspose2d( ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf * 2),\n            nn.ReLU(True),\n            # state size. (ngf*2) x 16 x 16\n            \n            nn.ConvTranspose2d( ngf * 2, ngf, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ngf),\n            nn.ReLU(True),\n            # state size. (ngf) x 32 x 32\n            \n            nn.ConvTranspose2d( ngf, nc, 4, 2, 1, bias=False),\n            nn.Tanh()\n            # state size. (nc) x 64 x 64\n        )\n\n    def forward(self, input):\n        return self.main(input)","7121c2ff":"# Create the generator\nnetG = Generator(ngpu).to(device)\n\n# Apply the weights_init function to randomly initialize all weights\n#  to mean=0, stdev=0.2.\nnetG.apply(weights_init)\n\n# Print the model\nprint(netG)","02975efa":"class Discriminator(nn.Module):\n    def __init__(self, ngpu):\n        super(Discriminator, self).__init__()\n        self.ngpu = ngpu\n        self.main = nn.Sequential(\n            # input is (nc) x 64 x 64\n            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf) x 32 x 32\n            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 2),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*2) x 16 x 16\n            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 4),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*4) x 8 x 8\n            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(ndf * 8),\n            nn.LeakyReLU(0.2, inplace=True),\n            # state size. (ndf*8) x 4 x 4\n            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n            nn.Sigmoid()\n        )\n\n    def forward(self, input):\n        return self.main(input)","2ce3bb43":"# Create the Discriminator\nnetD = Discriminator(ngpu).to(device)\n\n# Apply the weights_init function to randomly initialize all weights to mean=0, stdev=0.2.\nnetD.apply(weights_init)\n\n# Print the model\nprint(netD)","3f587fe5":"# Initialize BCELoss function\ncriterion = nn.BCELoss()\n\n# Create batch of latent vectors that we will use to visualize the progression of the generator\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\n# Establish convention for real and fake labels during training\nreal_label = 1.\nfake_label = 0.\n\n# Setup Adam optimizers for both G and D\noptimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))","3188b2e0":"\n# Lists to keep track of progress\nimg_list = []\nG_losses = []\nD_losses = []\niters = 0\n\nprint(\"Starting Training Loop...\")\n# For each epoch\nfor epoch in range(num_epochs):\n    # For each batch in the dataloader\n    for i, data in enumerate(tqdm(dataloader, 0)):\n\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        ## Train with all-real batch\n        netD.zero_grad()\n        # Format batch\n        real_cpu = data[0].to(device)\n        b_size = real_cpu.size(0)\n        label = torch.full((b_size,), real_label, dtype=torch.float, device=device)\n        # Forward pass real batch through D\n        output = netD(real_cpu).view(-1)\n        # Calculate loss on all-real batch\n        errD_real = criterion(output, label)\n        # Calculate gradients for D in backward pass\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        ## Train with all-fake batch\n        # Generate batch of latent vectors\n        noise = torch.randn(b_size, nz, 1, 1, device=device)\n        # Generate fake image batch with G\n        fake = netG(noise)\n        label.fill_(fake_label)\n        # Classify all fake batch with D\n        output = netD(fake.detach()).view(-1)\n        # Calculate D's loss on the all-fake batch\n        errD_fake = criterion(output, label)\n        # Calculate the gradients for this batch\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        # Add the gradients from the all-real and all-fake batches\n        errD = errD_real + errD_fake\n        # Update D\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        label.fill_(real_label)  # fake labels are real for generator cost\n        # Since we just updated D, perform another forward pass of all-fake batch through D\n        output = netD(fake).view(-1)\n        # Calculate G's loss based on this output\n        errG = criterion(output, label)\n        # Calculate gradients for G\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        # Update G\n        optimizerG.step()\n\n        # Output training stats\n        if i % 50 == 0:\n            print('[%d\/%d][%d\/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f \/ %.4f'\n                  % (epoch, num_epochs, i, len(dataloader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n\n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n\n        # Check how the generator is doing by saving G's output on fixed_noise\n        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n            with torch.no_grad():\n                fake = netG(fixed_noise).detach().cpu()\n            img_list.append(torchvision.utils.make_grid(fake, padding=2, normalize=True))\n\n        iters += 1\n","6b8af311":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses,label=\"G\")\nplt.plot(D_losses,label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","d4ad070b":"#%%capture\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","f1a015e7":"# Grab a batch of real images from the dataloader\nreal_batch = next(iter(dataloader))\n\n# Plot the real images\nplt.figure(figsize=(64,64))\nplt.subplot(1,2,1)\nplt.axis(\"off\")\nplt.title(\"Real Images\")\nplt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n\n# Plot the fake images from the last epoch\nplt.subplot(1,2,2)\nplt.axis(\"off\")\nplt.title(\"Fake Images\")\nplt.imshow(np.transpose(img_list[-1],(1,2,0)))\nplt.show()","617c7fe9":"## Data\n\nIn this session, we will be using the AnimeFaces dataset. The dataset is downloaded from kaggle and is unzipped in the google colab runtime. The directory structure is:\n<b>\n<div class=\"highlight-default notranslate\"><div class=\"highlight\"><pre><span><\/span><span class=\"n\">\/<\/span><span class=\"o\">content<\/span><span class=\"n\">\/<\/span><span class=\"o\">AnimeFaces<\/span>\n    <span class=\"o\">-&gt;<\/span> <span class=\"n\">images<\/span>\n        <span class=\"o\">-&gt;<\/span> <span class=\"mf\">188242.<\/span><span class=\"n\">jpg<\/span>\n        <span class=\"o\">-&gt;<\/span> <span class=\"mf\">173822.<\/span><span class=\"n\">jpg<\/span>\n        <span class=\"o\">-&gt;<\/span> <span class=\"mf\">284702.<\/span><span class=\"n\">jpg<\/span>\n        <span class=\"o\">-&gt;<\/span> <span class=\"mf\">537394.<\/span><span class=\"n\">jpg<\/span>\n           <span class=\"o\">...<\/span>\n<\/pre><\/div>\n<\/div><\/b>","d9f0f9c1":"## Implementation\n\nWith our input parameters set and the dataset prepared, we can now get into the implementation. We will start with the weigth initialization strategy, then talk about the generator, discriminator, loss functions, and training loop in detail.","dc1d6c86":"<h3> To summarize<\/h3>\n\nDuring the discriminator training:\n   * The discriminator classifies both real data and fake data from the generator\n   * We find the BCE loss between:\n       * real data and real label\n       * fake data and fake label\n   * The discriminator loss penalzes the discriminator for misclassifying a real instance as fake or a fake instance as real.\n   * The discriminator updates its weights through back propagation from the discriminator loss through the discriminator network","6f79cf16":"As the training progresses, the generator gets closer to producing output that can fool the discriminator:\n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/ok_gan.svg\">","f7fe3372":"## Import Necessary Libraries\n\nIn our notebook we will be using Pytorch. Pytorch is an open source machine learning frameowrk developed by Facebook. We will also be using matplotlib to draw the loss graphs, tqdm to monitor the epochs and PIL to preprocess the images. We will also be using pandas in case we want to store the data in a csv file.","18a9b5f0":"<div class=\"section\" id=\"discriminator\">\n<h3>Discriminator<a class=\"headerlink\" href=\"#discriminator\" title=\"Permalink to this headline\"><\/a><\/h3>\n<p>As mentioned, the discriminator, <span class=\"math notranslate nohighlight\">$(D)$<\/span>, is a binary classification\nnetwork that takes an image as input and outputs a scalar probability\nthat the input image is real (as opposed to fake). Here, <span class=\"math notranslate nohighlight\">\\(D\\)<\/span> takes\na 3x64x64 input image, processes it through a series of Conv2d,\nBatchNorm2d, and LeakyReLU layers, and outputs the final probability\nthrough a Sigmoid activation function. This architecture can be extended\nwith more layers if necessary for the problem, but there is significance\nto the use of the strided convolution, BatchNorm, and LeakyReLUs. The\nDCGAN paper mentions it is a good practice to use strided convolution\nrather than pooling to downsample because it lets the network learn its\nown pooling function. Also batch norm and leaky relu functions promote\nhealthy gradient flow which is critical for the learning process of both\n<span class=\"math notranslate nohighlight\">\\(G\\)<\/span> and <span class=\"math notranslate nohighlight\">\\(D\\)<\/span>.<\/p>\n \n<div class=\"figure\">\n<img alt=\"dcgan_discriminator\" src=\"https:\/\/miro.medium.com\/max\/924\/1*Sa16hNLBZEP_7_B8M9M8bA.png\"\/>\n<\/div>\n \n<p><b>Discriminator Code :<\/b><\/p>","fb2f395f":"Finally, if the generator training goes well, the discriminator gets worse at telling the difference between real and fake. It starts to classify fake data as real, and it's accuracy decreases. This means that the generator is getting good at producing fake data. \n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/good_gan.svg\">","7e0dd9c3":"### This notebook is an introduction to Deep Convolutional GANs. I have attempted to provide an intuitive explanation of what GANs are, how to build a GAN. In order to keep it simple I have used the Anime Face dataset which contains the pictures of faces of cartoon characters. So, without any further delay, let's get right into it.\n\n## Objectives:\n   * To demonstrate and explain the working of a GAN.\n   * To demonstrate how to build and train a GAN with pytorch.","b3c9fdb5":"<p><strong>Part 2 - Train the Generator<\/strong><\/p>\n<p>As stated in the original paper, we want to train the Generator by\nminimizing <span class=\"math notranslate nohighlight\">$(log(1-D(G(z))))$<\/span> in an effort to generate better fakes.\nAs mentioned, this was shown by Goodfellow to not provide sufficient\ngradients, especially early in the learning process. As a fix, we\ninstead wish to maximize <span class=\"math notranslate nohighlight\">$(log(D(G(z))))$<\/span>. In the code we accomplish\nthis by: classifying the Generator output from Part 1 with the\nDiscriminator, computing G\u2019s loss <em>using real labels as GT<\/em>, computing\nG\u2019s gradients in a backward pass, and finally updating G\u2019s parameters\nwith an optimizer step. It may seem counter-intuitive to use the real\nlabels as GT labels for the loss function, but this allows us to use the\n<span class=\"math notranslate nohighlight\">$(log(x))$<\/span> part of the BCELoss (rather than the <span class=\"math notranslate nohighlight\">$(log(1-x))$<\/span>\npart) which is exactly what we want.<\/p>\n    \n<img src = \"https:\/\/developers.google.com\/machine-learning\/gan\/images\/gan_diagram_generator.svg\">","dc54abea":"<p><strong>Loss versus training iteration<\/strong><\/p>\n<p>Below is a plot of D &amp; G\u2019s losses versus training iterations.<\/p>\n","c3fe958a":"<div class=\"section\" id=\"results\">\n<h2>Results<a class=\"headerlink\" href=\"#results\" title=\"Permalink to this headline\"><\/a><\/h2>\n<p>Finally, lets check out how we did. Here, we will look at three\n    different results.<\/p>\n    \n    * First, we will see how D and G\u2019s losses changed during training. \n    \n    * Second, we will visualize G\u2019s output on the fixed_noise batch for every epoch. \n    \n    * Third, we will look at a batch of real data next to a batch of fake data from G.\n","0d2e2c4e":"<p><strong>Visualization of G\u2019s progression<\/strong><\/p>\n\nWe saved the generator's output after every epoch by generating a batch of images from a fixed noise vector. Now we will be visualizing the Generator's progression ","ee8a82a5":"### Now that all the preprocessing parts are completed, let's get started with building the model","b996577d":"<div class=\"section\" id=\"generator\">\n<h3>Generator<a class=\"headerlink\" href=\"#generator\" title=\"Permalink to this headline\"><\/a><\/h3>\n<p>The generator part of a GAN learns to create fake data by incorporating feedback from the discriminator. It learns to make the discriminator classify its output as real.   \nThe generator, <span class=\"math notranslate nohighlight\">$(G)$<\/span>, is designed to map the latent space vector\n(<span class=\"math notranslate nohighlight\">\\(z\\)<\/span>) to data-space. Since our data are images, converting\n<span class=\"math notranslate nohighlight\">\\(z\\)<\/span> to data-space means ultimately creating a RGB image with the\nsame size as the training images (i.e. 3x64x64). In practice, this is\naccomplished through a series of strided two dimensional convolutional\ntranspose layers, each paired with a 2d batch norm layer and a relu\nactivation. The output of the generator is fed through a tanh function\nto return it to the input data range of <span class=\"math notranslate nohighlight\">\\([-1,1]\\)<\/span>. It is worth\nnoting the existence of the batch norm functions after the\nconv-transpose layers, as this is a critical contribution of the DCGAN\npaper. These layers help with the flow of gradients during training. An\nimage of the generator from the DCGAN paper is shown below.<\/p>\n\n<div class=\"figure\">\n<img alt=\"dcgan_generator\" src=\"https:\/\/pytorch.org\/tutorials\/_images\/dcgan_generator.png\"\/>\n<\/div>\n    \n<p>Notice, the how the inputs we set in the input section (<em>nz<\/em>, <em>ngf<\/em>, and\n<em>nc<\/em>) influence the generator architecture in code. <em>nz<\/em> is the length\nof the z input vector, <em>ngf<\/em> relates to the size of the feature maps\nthat are propagated through the generator, and <em>nc<\/em> is the number of\nchannels in the output image (set to 3 for RGB images).<\/p>\n\n<p><b>Generator Code:<\/b><\/p>","45f60783":"<div class=\"section\" id=\"loss-functions-and-optimizers\">\n<h3>Loss Functions and Optimizers<a class=\"headerlink\" href=\"#loss-functions-and-optimizers\" title=\"Permalink to this headline\"><\/a><\/h3>\n<p>With <span class=\"math notranslate nohighlight\">$(D)$<\/span> and <span class=\"math notranslate nohighlight\">$(G)$<\/span> setup, we can specify how they learn\nthrough the loss functions and optimizers. We will use the Binary Cross\nEntropy loss function which is defined in PyTorch as:<\/p>\n<div class=\"math notranslate nohighlight\">\n\n$$\\ell(x, y) = L = \\{l_1,\\dots,l_N\\}^\\top, \\quad l_n = - \\left[ y_n \\cdot \\log x_n + (1 - y_n) \\cdot \\log (1 - x_n) \\right]$$<\/div>\n<p>Notice how this function provides the calculation of both log components\nin the objective function (i.e. <span class=\"math notranslate nohighlight\">\\(log(D(x))\\)<\/span> and\n<span class=\"math notranslate nohighlight\">\\(log(1-D(G(z)))\\)<\/span>). We can specify what part of the BCE equation to\nuse with the <span class=\"math notranslate nohighlight\">\\(y\\)<\/span> input. This is accomplished in the training loop\nwhich is coming up soon, but it is important to understand how we can\nchoose which component we wish to calculate just by changing <span class=\"math notranslate nohighlight\">\\(y\\)<\/span>\n(i.e. GT labels).<\/p>\n<p>Next, we define our real label as 1 and the fake label as 0. These\nlabels will be used when calculating the losses of <span class=\"math notranslate nohighlight\">\\(D\\)<\/span> and\n<span class=\"math notranslate nohighlight\">\\(G\\)<\/span>, and this is also the convention used in the original GAN\npaper. Finally, we set up two separate optimizers, one for <span class=\"math notranslate nohighlight\">\\(D\\)<\/span> and\none for <span class=\"math notranslate nohighlight\">\\(G\\)<\/span>. As specified in the DCGAN paper, both are Adam\noptimizers with learning rate 0.0002 and Beta1 = 0.5. For keeping track\nof the generator\u2019s learning progression, we will generate a fixed batch\nof latent vectors that are drawn from a Gaussian distribution\n(i.e. fixed_noise) . In the training loop, we will periodically input\nthis fixed_noise into <span class=\"math notranslate nohighlight\">\\(G\\)<\/span>, and over the iterations we will see\nimages form out of the noise.<\/p>","2edf82f6":"## Q) What is a GAN ?\nImagine a situation in which person A always lies to person B and person B tries to catch the lie. If they continue this game for a long time, eventually person A will get pretty good at lying to person B and person B will get pretty good at catching the lies of person A. \n\nIf we can come up with any such situation in which two entities are continusly trying to get the upper hand by competing against one another, then we will be able to get an intuition about how GANS work.\n\n### GANS are very simillar to this condition !!!!\n\n","58788ada":"<h3>To summarize:<\/h3>\n\n   * Sample the random noise vector\n   * Produce generator output from the sampled classification for generator \n   * Pass the generated output theough the Discriminator and the Discriminator output as \"Real\" or \"Fake\"\n   * Calculate the BCE loss between <b>generated data<\/b> and <b>real labels<\/b>\n   * Backpropagate through both the discriminator and generator to obtain gradients.\n   * Use gradients to change only the generator weights.","f2c937f8":"<p><b>Now, as with the generator, we can create the discriminator, apply the\n<code class=\"docutils literal notranslate\"><span class=\"pre\">weights_init<\/span><\/code> function, and print the model\u2019s structure.<\/b><\/p>","0a4bdf24":"<p><strong>Real Images vs. Fake Images<\/strong><\/p>\n<p>Finally, lets take a look at some real images and fake images side by\nside.<\/p>","55cd4069":"### LET'S SEE THE TRAINING IMAGES !!","551a8de8":"## How does a GAN look ?\n\nWell, here is a picture of the whole system:\n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/gan_diagram.svg\">\n\nBoth the generator and the discriminator are neural networks. The generator output is connected directly to the discriminator input. Through the process of backpropagation, the discriminator's classification provides a signal that the generator uses to update its weights.\n\nBefore diving any deeper into the workings of a GAN, let's get started with building it and we can understand as we proceed.","4fafb42d":"### Let's define some variables:\n\n   * <b>dataroot<\/b> - the path to the root of the dataset folder. \n   * <b>workers<\/b> -the number of worker threads for loading the data with the DataLoader\n   * <b>batch_size<\/b> - the batch size used in training. The DCGAN paper uses a batch size of 128\n   * <b>image_size<\/b> - the spatial size of the images used for training. \n   * <b>nc<\/b> - number of color channels in the input image. here we are using RGB images, hence nc = 3\n   * <b>nz<\/b> - length of the latent vector\n   * <b>ngf<\/b> - relates to the depth of the feature maps carried through the generator \n   * <b>ndf<\/b> - sets the depth of feature maps propagated through the discriminator\n   * <b>num_epochs<\/b> - number of training epochs to run. Training for longer will lead to better results but it will take a lot of time.\n   * <b>lr<\/b> - learning rate for training. As described in the DCGAN paper this should be 0.0002\n   * <b>beta1<\/b> - beta1 hyperparametter for Adam optimizer. As described in the DCGAN paper this should be 0.5\n   * <b>ngpu<\/b> - number of GPUs available. If this is 0, code will run in CPU mode.","ffb459b1":"<p><b>Now, we can instantiate the generator and apply the <code class=\"docutils literal notranslate\"><span class=\"pre\">weights_init<\/span><\/code>\nfunction. Check out the printed model to see how the generator object is\n    structured.<\/b><\/p>","20651711":"<h1>Overview of the GAN structure<\/h1>\n<p>\nA generative adversarial network (GAN) has two parts:\n\n   * The generator learns to generate plausible data. The generated instances become negative training examples        for the discriminator.\n    \n   * The discriminator learns to distinguish the generator's fake data from real data. The discriminator penalizes      the generator for producing implausible results.\n\nWhen the training begins, the generator produces obviously fake data, and the discriminator quickly learns to tell that it's fake:<\/p>\n<p>\n<img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/bad_gan.svg\">\n<\/p>","0ba2fa67":"## Weight Initialization\n\nFrom the DCGAN paper, the authors specify that all model weights shall be randomly initialized from a Normal distribution with mean=0, stdev=0.02. The <b>weights_init<\/b> function takes an initialized model as input and reinitializes all convolutional, convolutional-transpose, and batch normalization layers to meet this criteria. This function is applied to the models immediately after initialization.","fa5d534a":"<p><strong>Part 1 - Train the Discriminator<\/strong><\/p>\n<p>Recall, the goal of training the discriminator is to maximize the\nprobability of correctly classifying a given input as real or fake. In\nterms of Goodfellow, we wish to \u201cupdate the discriminator by ascending\nits stochastic gradient\u201d. Practically, we want to maximize\n<span class=\"math notranslate nohighlight\">$log(D(x)) + log(1-D(G(z)))$<\/span>. We will calculate this in two steps. First, we\nwill construct a batch of real samples from the training set, forward\npass through <span class=\"math notranslate nohighlight\">\\(D\\)<\/span>, calculate the loss (<span class=\"math notranslate nohighlight\">\\(log(D(x))\\)<\/span>), then\ncalculate the gradients in a backward pass. Secondly, we will construct\na batch of fake samples with the current generator, forward pass this\nbatch through <span class=\"math notranslate nohighlight\">\\(D\\)<\/span>, calculate the loss (<span class=\"math notranslate nohighlight\">\\(log(1-D(G(z)))\\)<\/span>),\nand <em>accumulate<\/em> the gradients with a backward pass. Now, with the\ngradients accumulated from both the all-real and all-fake batches, we\ncall a step of the Discriminator\u2019s optimizer i.e, update it's weights.<\/p>\n\n<img src=\"https:\/\/developers.google.com\/machine-learning\/gan\/images\/gan_diagram_discriminator.svg\">\n","3ed7305b":"This is an important step because we will be using the <b>ImageFolder<\/b> dataset class, which requires there to be subdirectories in the dataset\u2019s root folder. Now, we can create the dataset, create the dataloader, set the device to run on, and finally visualize some of the training data.\n\n<b>We will be using the following transformations on the images:<\/b>\n   * We will be resizing the images to be 64x46 images\n   * We will be converting these images to torch tensors\n   * We will normalizing the pixel values to bring them between -1 and 1 because that is the range of the Tanh() function","d480e2b4":"<p>Finally, we will do some statistic reporting and at the end of each\nepoch we will push our fixed_noise batch through the generator to\nvisually track the progress of G\u2019s training. The training statistics\nreported are:<\/p>\n<ul class=\"simple\">\n<li><strong>Loss_D<\/strong> - discriminator loss calculated as the sum of losses for\nthe all real and all fake batches (<span class=\"math notranslate nohighlight\">$(log(D(x)) + log(D(G(z))))$<\/span>).<\/li>\n<li><strong>Loss_G<\/strong> - generator loss calculated as <span class=\"math notranslate nohighlight\">\\(log(D(G(z)))\\)<\/span><\/li>\n<li><strong>D(x)<\/strong> - the average output (across the batch) of the discriminator\nfor the all real batch. This should start close to 1 then\ntheoretically converge to 0.5 when G gets better.<\/li>\n<li><strong>D(G(z))<\/strong> - average discriminator outputs for the all fake batch.\nThe first number is before D is updated and the second number is\nafter D is updated. These numbers should start near 0 and converge to\n0.5 as G gets better.<\/li>\n<\/ul>","d90ac012":"<h1> DCGAN <\/h1>\n <h2>Deep Convolutional Generative Adversarial Network<\/h2>\n\n<p>\n<img src=\"https:\/\/raw.githubusercontent.com\/znxlwm\/pytorch-MNIST-CelebA-GAN-DCGAN\/master\/CelebA_DCGAN_crop_results\/generation_animation.gif\" >\n<\/p>","c7f092e0":"<h3>Training<a class=\"headerlink\" href=\"#training\" title=\"Permalink to this headline\"><\/a><\/h3>\n<p>Finally, now that we have all of the parts of the GAN framework defined,\nwe can train it. Be mindful that training GANs is somewhat of an art\nform, as incorrect hyperparameter settings lead to mode collapse with\nlittle explanation of what went wrong. Here, we will closely follow\nAlgorithm 1 from Goodfellow\u2019s paper, while abiding by some of the best\npractices shown in the DCGAN paper.\nNamely, we will \u201cconstruct different mini-batches for real and fake\u201d\nimages, and also adjust G\u2019s objective function to maximize\n<span class=\"math notranslate nohighlight\">$logD(G(z))$<\/span>. Training is split up into two main parts. <b>Part 1<\/b>\n    updates the Discriminator and <b>Part 2<\/b> updates the Generator.<\/p>\n    \n<img src=\"https:\/\/miro.medium.com\/max\/1600\/1*6svU8runkDRjyiIvZ6VKMw.gif\">\n"}}