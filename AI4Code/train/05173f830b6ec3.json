{"cell_type":{"68eb7be8":"code","dc012467":"code","f5a21b8a":"code","d15b0510":"code","7d66e056":"code","891fc73b":"code","db57b346":"code","94a86614":"code","8020c406":"code","882a3e27":"code","fcf7105e":"code","89f57820":"code","0a5b5f5b":"code","34bf5a52":"code","002bd3b9":"code","9e431823":"code","51f4aed6":"code","9152b773":"code","eb2b6eb8":"code","8b530b66":"code","6c58f3c8":"code","4fbaea6e":"code","f65a5505":"code","0eccdaf5":"code","62ce6075":"code","07284178":"code","036d46f3":"code","0dd2e849":"markdown","4b6053f9":"markdown","0506809f":"markdown","2f506a45":"markdown","ae506a31":"markdown","025bb4e3":"markdown","c6c75f1e":"markdown","0f0b9aae":"markdown","144b9976":"markdown","6021179e":"markdown","96aac6ff":"markdown","d367c2a6":"markdown","fb6f59c2":"markdown","fe3e957c":"markdown","51a01281":"markdown","38a527bd":"markdown","d1947265":"markdown","ede29329":"markdown"},"source":{"68eb7be8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","dc012467":"# Read in the raw text\nrawdata = open(\"..\/input\/smsspam\/SMSSpamCollection.tsv\").read()\n#printing raw data\nrawdata[0:500]","f5a21b8a":"data  = pd.read_csv(\"..\/input\/smsspam\/SMSSpamCollection.tsv\", sep=\"\\t\", names=[\"label\", \"body_text\"], header=None)\ndata.head()","d15b0510":"import string\nstring.punctuation","7d66e056":"# Function to remove punctuations.\ndef remove_punc(text):\n    nonP_text = \"\".join([char for char in text if char not in string.punctuation])\n    return nonP_text\n\ndata[\"body_text_clean\"] = data[\"body_text\"].apply(lambda x: remove_punc(x))\n\ndata.head()","891fc73b":"import re\n\n#function to apply tokenization\ndef tokenize(text):\n    tokens = re.split(\"\\W+\", text)# W+ means all capital, small alphabets and integers 0-9\n    return tokens\n\ndata[\"body_text_tokenized\"] = data[\"body_text_clean\"].apply(lambda x: tokenize(x))\n\ndata.head()","db57b346":"import nltk\nstopwords = nltk.corpus.stopwords.words(\"english\")\n\ndef remove_stopwords(token):\n    text = [word for word in token if word not in stopwords]# to remove all stopwords\n    return text\n\ndata[\"body_text_nonstop\"] = data[\"body_text_tokenized\"].apply(lambda x: remove_stopwords(x))\ndata.head()","94a86614":"ps = nltk.PorterStemmer()\n\ndef stemming(t_text):\n    text = [ps.stem(word) for word in t_text]\n    return text\n\ndata[\"body_text_stemmed\"] = data[\"body_text_nonstop\"].apply(lambda x: stemming(x))\ndata.head()","8020c406":"wn = nltk.WordNetLemmatizer()\n\ndef lemmatizer(t_text):\n    text = [wn.lemmatize(word) for word in t_text]\n    return text\n\ndata[\"body_text_lemmatized\"] = data[\"body_text_stemmed\"].apply(lambda x: lemmatizer(x))\ndata.head()","882a3e27":"import pandas as pd\nimport re\nimport string\nimport nltk\npd.set_option('display.max_colwidth', 100) # To extend column width\n\nstopwords = nltk.corpus.stopwords.words('english')\nps = nltk.PorterStemmer()\n\ndata = pd.read_csv(\"..\/input\/smsspam\/SMSSpamCollection.tsv\", sep='\\t')\ndata.columns = ['label', 'body_text']\n","fcf7105e":"def clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopwords]\n    return text","89f57820":"from sklearn.feature_extraction.text import CountVectorizer\n\ncount = CountVectorizer(analyzer=clean_text)\nx_count = count.fit_transform(data[\"body_text\"])\n\nx_count.shape\ncount.get_feature_names()","0a5b5f5b":"x_count_df = pd.DataFrame(x_count.toarray(), columns=count.get_feature_names())\nx_count_df.head(10)","34bf5a52":"from sklearn.feature_extraction.text import CountVectorizer\n\nngram_vect = CountVectorizer(ngram_range=(2,2),analyzer=clean_text) # It applies only bigram vectorizer\nX_counts = ngram_vect.fit_transform(data['body_text'])\nprint(X_counts.shape)\nprint(ngram_vect.get_feature_names())","002bd3b9":"X_counts_df = pd.DataFrame(X_counts.toarray(), columns=ngram_vect.get_feature_names())\nX_counts_df.head(10)","9e431823":"from sklearn.feature_extraction.text import TfidfVectorizer\n\ntfidf_vect = TfidfVectorizer(analyzer=clean_text)\nX_tfidf = tfidf_vect.fit_transform(data['body_text'])\nprint(X_tfidf.shape)\nprint(tfidf_vect.get_feature_names())","51f4aed6":"X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vect.get_feature_names())\nX_tfidf_df.head(10)","9152b773":"print(X_tfidf_df.loc[(X_tfidf_df!=0).any(axis=1)])","eb2b6eb8":"import pandas as pd\n\ndata = pd.read_csv(\"..\/input\/smsspam\/SMSSpamCollection.tsv\", sep='\\t')\ndata.columns = ['label', 'body_text']","8b530b66":"import string\n\n# Function to calculate length of message excluding space\ndata['body_len'] = data['body_text'].apply(lambda x: len(x) - x.count(\" \"))\n\ndata.head()\n\ndef count_punct(text):\n    count = sum([1 for char in text if char in string.punctuation])\n    return round(count\/(len(text) - text.count(\" \")), 3)*100\n\ndata['punct%'] = data['body_text'].apply(lambda x: count_punct(x))\n\ndata.head()","6c58f3c8":"import seaborn as sns\n#Setting the size and grid for plotting\nsns.set(rc= {\"figure.figsize\": (8, 6)})\n\nbins = np.linspace(0, 200, 40)\n\nplt.hist(data[data['label']=='spam']['body_len'], bins, alpha=0.5, label='spam')\nplt.hist(data[data['label']=='ham']['body_len'], bins, alpha=0.5, label='ham')\nplt.legend(loc='upper left')\nplt.show()","4fbaea6e":"bins = np.linspace(0, 50, 40)\n\nplt.hist(data[data['label']=='spam']['punct%'], bins, alpha=0.5, label='spam')\nplt.hist(data[data['label']=='ham']['punct%'], bins, alpha=0.5, label='ham')\nplt.legend(loc='upper right')\nplt.show()","f65a5505":"from sklearn.model_selection import train_test_split\n\nX=data[['body_text', 'body_len', 'punct%']]\ny=data['label']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)","0eccdaf5":"tfidf_vect = TfidfVectorizer(analyzer=clean_text)\ntfidf_vect_fit = tfidf_vect.fit(X_train['body_text'])\n\ntfidf_train = tfidf_vect_fit.transform(X_train['body_text'])\ntfidf_test = tfidf_vect_fit.transform(X_test['body_text'])\n\nX_train_vect = pd.concat([X_train[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_train.toarray())], axis=1)\nX_test_vect = pd.concat([X_test[['body_len', 'punct%']].reset_index(drop=True), \n           pd.DataFrame(tfidf_test.toarray())], axis=1)\n\nX_train_vect.head()","62ce6075":"from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nfrom sklearn.metrics import accuracy_score as acs\nfrom sklearn.metrics import classification_report, confusion_matrix","07284178":"rf = RandomForestClassifier(n_estimators=150, max_depth=None, n_jobs=-1)\n\nrf_model = rf.fit(X_train_vect, y_train)\n\ny_pred = rf_model.predict(X_test_vect)\n\nprecision, recall, fscore, train_support = score(y_test, y_pred, pos_label='spam', average='binary')\nprint('Precision: {} \/ Recall: {} \/ F1-Score: {} \/ Accuracy: {}%'.format(\n    round(precision, 3), round(recall, 3), round(fscore,3), round(acs(y_test,y_pred)*100, 3)))\n\nsns.set(rc= {\"figure.figsize\": (8, 6)})\n\ncm = confusion_matrix(y_test, y_pred)\nclass_label = [\"ham\", \"spam\"]\ndf_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\nax = sns.heatmap(df_cm, annot=True, fmt='d')\nbottom, top = ax.get_ylim()\nax.set_ylim(bottom + 0, top - 0)\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","036d46f3":"report = classification_report(y_test, y_pred)\nprint(report)","0dd2e849":"## Feature Engineering: Feature Creation","4b6053f9":"## 4. Lemmatizer","0506809f":"### Vectorize text","2f506a45":"# Inroduction to NLP","ae506a31":"## Apply CountVectorizer(N-Grams)","025bb4e3":"## 2. Tokenization","c6c75f1e":"## 3. Remove Stopwords","0f0b9aae":"## 2. TF-IDF\n## Apply TFIDFvectorizer","144b9976":"# Vectorize Raw Data: \n## 1. N Grams","6021179e":"## 3. Stemming","96aac6ff":"## UPVOTE IF YOU LIKE THE NOTEBOOK","d367c2a6":"## 1. Removing Punctuations","fb6f59c2":"### Create feature for text message length and % of punctuation in text ","fe3e957c":"### Split into train\/test","51a01281":"## Apply Count Vectorizer\nEncoding Text as integer i.e. numeric form to create feature vectors so that we can apply ML algos on it.","38a527bd":"### Final evaluation of models","d1947265":"## Vectorizing Data: Bag-Of-Words","ede29329":"# Pre-Processing"}}