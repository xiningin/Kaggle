{"cell_type":{"561dc852":"code","aeadb0e2":"code","11062397":"code","c86ba483":"code","4a8bf8a8":"code","08b630da":"code","cf949d11":"code","6bdee422":"code","86068877":"markdown","a966cbeb":"markdown","2a13a6d4":"markdown"},"source":{"561dc852":"# import libraries\nimport numpy as np, pandas as pd, matplotlib.pyplot as plt\nimport json\nimport gc\nimport os","aeadb0e2":"# common function to reduce the memory usage thus allowing us to work with larger datasets\ndef reduce_mem_usage(df, verbose=True):\n    \"\"\"\n    Common function to reduce the size of the entries in a pandas DataFrame.\n    \"\"\"\n    import numpy as np\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df\n\n# simple function to read the data in the competition files\ndef readData(submission_only=False,PATH='\/kaggle\/input\/'):\n    import pandas as pd\n    print('Reading files...')\n    submission = pd.read_csv(PATH+'m5-forecasting-accuracy\/sample_submission.csv')\n    if submission_only:\n        return submission\n    else:\n        calendar = pd.read_csv(PATH+'m5-forecasting-accuracy\/calendar.csv')\n        calendar = reduce_mem_usage(calendar)\n        print('Calendar has {} rows and {} columns'.format(calendar.shape[0], calendar.shape[1]))\n\n        sell_prices = pd.read_csv(PATH+'m5-forecasting-accuracy\/sell_prices.csv')\n        sell_prices = reduce_mem_usage(sell_prices)\n        print('Sell prices has {} rows and {} columns'.format(sell_prices.shape[0], sell_prices.shape[1]))\n\n        sales_train_validation = pd.read_csv(PATH+'m5-forecasting-accuracy\/sales_train_validation.csv')\n        print('Sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n\n    return calendar, sell_prices, sales_train_validation, submission\n\n","11062397":"# process the data to get it into a tabular format; pd.melt is especially useful to 'unpack' the target variable (demand)\ndef melt_and_merge(nrows=5.5e7):\n    calendar, sell_prices, sales_train_validation, submission = readData()\n    # drop some calendar features\n    calendar.drop(['weekday', 'wday', 'month', 'year'], inplace = True, axis = 1)\n    \n    # melt sales data, get it ready for training\n    sales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], \n                                     var_name = 'day', value_name = 'demand')\n    \n    #print('Melted sales train validation has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    sales_train_validation = reduce_mem_usage(sales_train_validation)\n    \n    # seperate test dataframes\n    test1_rows = [row for row in submission['id'] if 'validation' in row]\n    #test2_rows = [row for row in submission['id'] if 'evaluation' in row]\n    test1 = submission[submission['id'].isin(test1_rows)]\n    #test2 = submission[submission['id'].isin(test2_rows)]\n    \n    # change column names\n    test1.columns = ['id'] + ['d_{}'.format(i) for i in range(1914,1942)]\n    #test2.columns = ['id'] + ['d_{}'.format(i) for i in range(1942,1970)]\n\n\n    # get product table\n    product = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\n    \n    # merge with product table\n    #test2['id'] = test2['id'].str.replace('_evaluation','_validation')\n    test1 = test1.merge(product, how = 'left', on = 'id')\n    #test2 = test2.merge(product, how = 'left', on = 'id')\n    #test2['id'] = test2['id'].str.replace('_validation','_evaluation')\n    \n    # \n    test1 = pd.melt(test1, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n                    value_name = 'demand')\n    #test2 = pd.melt(test2, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day',\n    #                value_name = 'demand')\n    \n    sales_train_validation = pd.concat([sales_train_validation, test1], axis = 0) # include test2 later\n    \n    del test1#, test2\n    gc.collect()\n    \n    # delete first entries otherwise memory errors\n    sales_train_validation = sales_train_validation.loc[nrows:]\n    \n    # delete test2 for now\n    #data = data[data['part'] != 'test2']\n    \n    sales_train_validation = pd.merge(sales_train_validation, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\n    sales_train_validation.drop(['d', 'day'], inplace = True, axis = 1)\n    \n    # get the sell price data (this feature should be very important)\n    sales_train_validation = sales_train_validation.merge(sell_prices, on = ['store_id', 'item_id', 'wm_yr_wk'], how = 'left')\n    print('Our final dataset to train has {} rows and {} columns'.format(sales_train_validation.shape[0], sales_train_validation.shape[1]))\n    \n    del calendar, sell_prices; gc.collect();\n    \n    return sales_train_validation","c86ba483":"# this function fills up the Nan values and encodes the categorical variables\ndef transform(data):\n    from sklearn.preprocessing import LabelEncoder\n    # convert to datetime object\n    data['date'] = pd.to_datetime(data.date)\n    \n    # fill NaN features with unknown\n    nan_features = ['event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in nan_features:\n        data[feature].fillna('unknown', inplace = True)\n    \n    # Encode categorical features\n    cat = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\n    for feature in cat:\n        encoder = LabelEncoder()\n        data[feature] = encoder.fit_transform(data[feature])\n    \n    return data","4a8bf8a8":"# this function computes useful laging features from the target variable and the price\n# to convert what is initially a sequence to sequence mapping into a regression task (sequence to one),\n# the author used lagged values starting from the minimum lag of 28 days, which is the forecasting horizon\ndef simple_fe(data):\n    \n    # rolling demand features\n    data['lag_t28'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28))\n    data['lag_t29'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(29))\n    data['lag_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(30))\n    data['rolling_mean_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).mean())\n    data['rolling_std_t7'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(7).std())\n    data['rolling_mean_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).mean())\n    data['rolling_mean_t90'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(90).mean())\n    data['rolling_mean_t180'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(180).mean())\n    data['rolling_std_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).std())\n    data['rolling_skew_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).skew())\n    data['rolling_kurt_t30'] = data.groupby(['id'])['demand'].transform(lambda x: x.shift(28).rolling(30).kurt())\n    \n    \n    # price features\n    data['lag_price_t1'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1))\n    data['price_change_t1'] = (data['lag_price_t1'] - data['sell_price']) \/ (data['lag_price_t1'])\n    data['rolling_price_max_t365'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.shift(1).rolling(365).max())\n    data['price_change_t365'] = (data['rolling_price_max_t365'] - data['sell_price']) \/ (data['rolling_price_max_t365'])\n    data['rolling_price_std_t7'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(7).std())\n    data['rolling_price_std_t30'] = data.groupby(['id'])['sell_price'].transform(lambda x: x.rolling(30).std())\n    data.drop(['rolling_price_max_t365', 'lag_price_t1'], inplace = True, axis = 1)\n    \n    # time features\n    data['date'] = pd.to_datetime(data['date'])\n    data['year'] = data['date'].dt.year\n    data['month'] = data['date'].dt.month\n    data['week'] = data['date'].dt.week\n    data['day'] = data['date'].dt.day\n    data['dayofweek'] = data['date'].dt.dayofweek\n    \n    \n    return data","08b630da":"new_fe = True\nif new_fe:\n    data = melt_and_merge(nrows=2.75e7)\n    submission = readData(submission_only=True)\n    data = transform(data)\n    print('There are {:e} \/ {:e} NaN entries in the sell_price column'.format(data.sell_price.isna().sum(),data.shape[0]))\n    data = simple_fe(data)\n    data = reduce_mem_usage(data)\n    #data.to_pickle('engineered_data.pkl')\nelse:\n    data = pd.read_pickle('engineered_data.pkl')\n    submission = readData(submission_only=True)","cf949d11":"from hyperopt import hp, tpe\nfrom hyperopt.fmin import fmin\nfrom sklearn.metrics import make_scorer\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.metrics import make_scorer\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\n# define list of features\nfeatures = ['item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'year', 'month', 'week', 'day', 'dayofweek', 'event_name_1',\n            'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX', 'snap_WI', 'sell_price', 'lag_t28', 'lag_t29',\n            'lag_t30', 'rolling_mean_t7', 'rolling_std_t7', 'rolling_mean_t30', 'rolling_mean_t90', 'rolling_mean_t180', \n            'rolling_std_t30', 'price_change_t1', 'price_change_t365', 'rolling_price_std_t7', 'rolling_price_std_t30', \n            'rolling_skew_t30', 'rolling_kurt_t30']\n\ndef optimize_parameters(x_train,max_evals=20):\n    # define fixed hyperparameters\n    params = {\n        'tree_learner':'voting',\n        'boosting_type': 'gbdt',\n        'objective': 'tweedie',\n        'tweedie_variance_power': 1.1,\n        'metric': 'rmse',\n        'subsample': 0.5,\n        'subsample_freq': 1,\n        'sub_feature' : 0.8,\n        'sub_row' : 0.75,\n        'bagging_freq' : 1,\n        'lambda_l2' : 0.1,\n        'verbosity': 1,\n        'boost_from_average': True,\n        'n_jobs': -1,\n        'learning_rate':0.1,\n        'seed': 3008,\n        'verbose': -1}\n    \n    # define floating hyperparameters\n    space = {\n        'n_estimators': hp.quniform('n_estimators', 25, 600, 25),\n        'max_depth': hp.quniform('max_depth', 1, 6, 1),\n        'num_leaves': hp.quniform('num_leaves', 10, 120, 1)\n    }\n    \n    # define the objective function to optimize the hyperparameters\n    def objective(floating_params):\n        params['n_estimators'] = int(floating_params['n_estimators'])\n        params['max_depth'] = int(floating_params['max_depth'])\n        params['num_leaves'] = int(floating_params['num_leaves'])\n        print(params)\n        regressor = lgb.LGBMRegressor(**params)\n        \n        x_sample = x_train.sample(int(x_train.shape[0]\/50))\n        x_train_sample, y_train_sample = x_sample[features], x_sample['demand']\n        \n        score = cross_val_score(regressor, x_train_sample, y_train_sample, cv=StratifiedKFold(),\n                                scoring=make_scorer(mean_squared_error, greater_is_better=False)\n                                ).mean()\n        print(\"rmse {:.3f} params {}\".format(score, params))\n        return score\n\n    best = fmin(fn=objective,\n                space=space,\n                algo=tpe.suggest,\n                max_evals=max_evals)\n    \n    best['num_iterations'] =  1500\n    best['n_estimators'] = int(best['n_estimators']); best['max_depth'] = int(best['max_depth']); \n    best['num_leaves'] = int(best['num_leaves'])\n    \n    with open('params.txt', 'w') as file:\n        file.write(json.dumps(params))\n    \n    return params\n    \ndef optimized_lgb(data,update_hyperparameters=False):\n    \n    # going to evaluate with the last 28 days\n    x_train = data[data['date'] <= '2016-03-27']\n    y_train = x_train['demand']\n    x_val = data[(data['date'] > '2016-03-27') & (data['date'] <= '2016-04-24')]\n    y_val = x_val['demand']\n    test = data[(data['date'] > '2016-04-24')]\n    del data\n    gc.collect()\n    \n\n    \n    if (update_hyperparameters and os.path.exist('\/kaggle\/working\/params.txt')):\n        with open('params.txt') as params_file:    \n            params = json.load(params_file)\n    else:\n        params = optimize_parameters(x_train)\n        \n    train_set = lgb.Dataset(x_train[features], y_train)\n    val_set = lgb.Dataset(x_val[features], y_val)\n    del x_train, y_train\n    gc.collect()\n    \n    model = lgb.train(params, train_set, num_boost_round = 2500, early_stopping_rounds = 50, \n                      valid_sets = [train_set, val_set], verbose_eval = 100)\n    \n    val_pred = model.predict(x_val[features])\n    val_score = np.sqrt(mean_squared_error(val_pred, y_val))\n    print(f'Our val rmse score is {val_score}')\n    y_pred = model.predict(test[features])\n    test['demand'] = y_pred\n    \n    return test\n\ndef append_predictions(test, submission):\n    predictions = test[['id', 'date', 'demand']]\n    predictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\n    predictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]\n\n    evaluation_rows = [row for row in submission['id'] if 'evaluation' in row] \n    evaluation = submission[submission['id'].isin(evaluation_rows)]\n\n    validation = submission[['id']].merge(predictions, on = 'id')\n    final = pd.concat([validation, evaluation])\n    final.to_csv('submission.csv', index = False)","6bdee422":"# eliminate the first 90 datapoints\ndata = data[~data.rolling_mean_t180.isna()]\ndata = reduce_mem_usage(data)\ntest = optimized_lgb(data,update_hyperparameters=False)\nappend_predictions(test, submission)","86068877":"### Initial notebook:\n1. Read in data from calendar, sell_prices, sales_train_validation, submission and apply common method to reduce the memory usage\n2. Merge data into a multivariate time series (MTS) format with each entry in the series describing the target variable (demand) and its exogeneous variables in different dates for different item_id, dept_id, cat_id, store_id and state_id\n3. Transform features into the MTS (fill NaN events with category 'unknown' and encode all categorical variables)\n4. Engineer additional features based on the series of prices and demand for each good. Examples include aggregations over lagged sub-series of the two variables  \n\n### Personal contributions (thus far)\n5. Hand-crafted parameters for the lightGBM regressor based upon [documentation](https:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters-Tuning.html)\n6. Implemented hyper-parameter tuning pipeline for optimizing the n_estimators, max_depth, num_leaves, learning_rate  \n\n### Future plans\n7. Find a way to read in the full dataset and avoid memory problems\n8. Implement a feature selection method (sklearn.feature_selection)\n9. Engineer additional features","a966cbeb":"# Credit to [ragnar](https:\/\/www.kaggle.com\/ragnar123). This notebook builds upon their data pipeline found in  [ragnar's notebook](https:\/\/www.kaggle.com\/ragnar123\/very-fst-model)","2a13a6d4":"### Apply hyperopt to select the parameters of the lightGBM\n1. Define a loss function\n2. Define the parameter space\n3. Run Bayesian Optimization"}}