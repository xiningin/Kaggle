{"cell_type":{"73c1cac9":"code","22090fc2":"code","df9476a2":"code","47f86ee6":"code","7a354ebc":"code","1d485b4e":"code","b3e06494":"code","bced1a0a":"code","64acbe78":"code","e996bb52":"code","d097774d":"code","5561e492":"code","0c9f6c16":"code","a6341f95":"code","31d6fb5b":"code","e664c830":"code","521baedc":"code","33f044a8":"code","c8e50201":"code","99884c50":"code","c3e1fbd9":"code","7a33b62a":"code","42be369f":"code","6ad8212e":"code","825124dc":"code","f11737f8":"code","fb40256e":"code","7cefdba1":"code","091bd5d4":"code","eb62a936":"code","06832214":"code","ae93e138":"code","caf28d32":"code","35318bc0":"code","2b7a13ac":"code","a6d7332e":"code","b9ab566f":"code","c0bb8ede":"code","6ede9479":"code","945760cf":"code","3c26573c":"code","2c01c53b":"code","94325d28":"code","7fd47ff8":"code","249b4827":"code","6cfc7251":"code","c6207e58":"code","c2799609":"code","1df5f806":"code","d091e3ed":"code","e2e084cf":"code","a4887c1b":"code","124e5171":"code","5082c8d6":"code","58ed388e":"code","835666b7":"code","91e2d58e":"code","d44d0a91":"code","cc34911b":"code","c7fa579c":"code","cf96328d":"code","055f60fc":"code","324108e1":"code","df671cfd":"code","8bf05d44":"code","bbe8c90f":"code","390e702b":"code","b705acc0":"code","e14209c0":"code","02e4ad3f":"code","20998cfc":"code","3aaf3f7f":"code","4a93ec8f":"code","7985234e":"code","9ffd7ce0":"code","caf7bd33":"code","d089917d":"code","a472758b":"code","50d231f1":"code","c8c4ff15":"code","8d60ac79":"code","833f2c77":"code","b1dc31ec":"code","843064a3":"code","f101d508":"code","884bc561":"code","431a15a0":"code","dad2fe59":"code","c278a198":"code","b40c5a8e":"code","6a77aaf2":"code","d585cd4d":"code","3f7c7f37":"code","de994e8a":"code","92f387a0":"code","1cf790f9":"code","02615256":"code","337f4a0f":"code","cb3319c2":"code","43a5aa0d":"code","793a4776":"code","6935e0ee":"code","e234b701":"code","f8d973de":"code","9960c5c2":"code","c71f437c":"code","0fbbbe9d":"code","d6be43f6":"code","2e30f29d":"code","dad362fe":"code","ee1d0042":"code","d581dad3":"code","9bed006c":"code","d0ec800c":"code","0f6275d7":"code","b14ac224":"code","0cb84a93":"code","b76ae16a":"code","90e78614":"code","04739241":"code","7589769d":"code","2a5c3b0b":"code","16bc0e41":"markdown","88bdd27d":"markdown","6ba3f2ed":"markdown","dec67b3d":"markdown","a9a0a3fa":"markdown","5e6fbe9f":"markdown","b8684cd0":"markdown","583f47ef":"markdown","248cb531":"markdown","54527235":"markdown","9c278ad0":"markdown","6f3b5026":"markdown","2b37627d":"markdown","b7959bb5":"markdown","c5bdbc72":"markdown","0d91b5bf":"markdown","a7779827":"markdown","2efd1c6f":"markdown","e2d91dfd":"markdown","d7915ccb":"markdown","e54c6d4b":"markdown","972557d7":"markdown","f41ca153":"markdown","059e21ff":"markdown","87f31455":"markdown","7fdf2ccf":"markdown","66ad3bf4":"markdown","ff1f3dd4":"markdown","5178364a":"markdown","551ed656":"markdown","db60985a":"markdown","6cf38caf":"markdown","09921f37":"markdown","3d826bbb":"markdown","f830a5fa":"markdown","acd850a4":"markdown","76f49093":"markdown","c2742957":"markdown","cdac2ec6":"markdown","0cb49a16":"markdown","982e3275":"markdown","4232aace":"markdown","eacb2974":"markdown","36e8599b":"markdown","892f11bf":"markdown","f6c663ff":"markdown","f3e632a3":"markdown","0eff6e08":"markdown","4e7bdc4f":"markdown","e880d29d":"markdown","7f6f5035":"markdown","2d4044a2":"markdown","802ecd07":"markdown","233e2cf8":"markdown","3206e2c1":"markdown","9b859150":"markdown","02380df2":"markdown","5299a2e2":"markdown","357965be":"markdown","4d931691":"markdown","6167c92f":"markdown","a42193df":"markdown","d5999eaf":"markdown","5660c5da":"markdown","25bae719":"markdown","62ca575e":"markdown","688381fc":"markdown","e4316da4":"markdown","ab74e592":"markdown","b33ce344":"markdown","3b98babc":"markdown","ac8daf17":"markdown","7204c787":"markdown","dbf05806":"markdown"},"source":{"73c1cac9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","22090fc2":"file = open('\/kaggle\/input\/ense3-ict\/files\/ch03\/adult.data', 'r')","df9476a2":"def chr_int(a):\n    if a.isdigit():\n        return int(a)\n    else:\n        return 0\n                \ndata=[]\nfor line in file:\n     data1=line.split(', ')\n     if len(data1)==15:\n        data.append([chr_int(data1[0]),data1[1],chr_int(data1[2]),data1[3],chr_int(data1[4]),data1[5],data1[6],\\\n            data1[7],data1[8],data1[9],chr_int(data1[10]),chr_int(data1[11]),chr_int(data1[12]),data1[13],\\\n            data1[14]])","47f86ee6":"print (data[1:2])","7a354ebc":"%matplotlib inline\nimport pandas as pd\n\ndf = pd.DataFrame(data) #  Two-dimensional size-mutable, potentially heterogeneous tabular data structure with labeled axes \n\ndf.columns = ['age', 'type_employer', 'fnlwgt', 'education', \n                \"education_num\",\"marital\", \"occupation\", \"relationship\", \"race\",\"sex\",\n                \"capital_gain\", \"capital_loss\", \"hr_per_week\",\"country\",\"income\"]\ndf.head()","1d485b4e":"df.tail()","b3e06494":"df.shape","bced1a0a":"counts = df.groupby('country').size()\n\nprint (counts) ","64acbe78":"counts = df.groupby('age').size() # grouping by age\nprint (counts)\n","e996bb52":"counts.head(60)","d097774d":"counts.tail(20)","5561e492":"ml = df[(df.sex == 'Male')] # grouping by sex\nml.shape\nml1 = df[(df.sex == 'Male')&(df.income=='>50K\\n')]\nml1.shape","0c9f6c16":"fm =df[(df.sex == 'Female')]\nfm.shape\nfm1 =df[(df.sex == 'Female')&(df.income=='>50K\\n')]\nfm1.shape","a6341f95":"df1=df[(df.income=='>50K\\n')]\n\nprint ('The rate of people with high income is: ', int(len(df1)\/float(len(df))*100), '%.' )\nprint ('The rate of men with high income is: ', int(len(ml1)\/float(len(ml))*100), '%.' )\nprint ('The rate of women with high income is: ', int(len(fm1)\/float(len(fm))*100), '%.' )","31d6fb5b":"print ('The average age of men is: ', ml['age'].mean(), '.' )\nprint ('The average age of women is: ', fm['age'].mean(), '.')","e664c830":"print ('The average age of high-income men is: ', ml1['age'].mean(), '.' )\nprint ('The average age of high-income women is: ', fm1['age'].mean(), '.')","521baedc":"ml_mu = ml['age'].mean()\nfm_mu = fm['age'].mean()\nml_var = ml['age'].var()\nfm_var = fm['age'].var()\nml_std = ml['age'].std()\nfm_std = fm['age'].std()\n\nprint ('Statistics of age for men: mu:', ml_mu, 'var:', ml_var, 'std:', ml_std)\nprint ('Statistics of age for women: mu:', fm_mu, 'var:', fm_var, 'std:', fm_std)","33f044a8":"ml_mu_hr = ml['hr_per_week'].mean()\nfm_mu_hr = fm['hr_per_week'].mean()\nml_var_hr = ml['hr_per_week'].var()\nfm_var_hr = fm['hr_per_week'].var()\nml_std_hr = ml['hr_per_week'].std()\nfm_std_hr = fm['hr_per_week'].std()\n\nprint ('Statistics of hours per week for men: mu:', ml_mu_hr, 'var:', ml_var_hr, 'std:', ml_std_hr)\nprint ('Statistics of hours per week for women: mu:', fm_mu_hr, 'var:', fm_var_hr, 'std:', fm_std_hr)","c8e50201":"ml_median= ml['age'].median()\nfm_median= fm['age'].median()\n\nprint (\"Median age per men and women: \", ml_median, fm_median)","99884c50":"ml_median_age= ml1['age'].median()\nfm_median_age= fm1['age'].median()\n\nprint (\"Median age per men and women with high-income: \", ml_median_age, fm_median_age)","c3e1fbd9":"ml_median_hr= ml['hr_per_week'].median()\nfm_median_hr= fm['hr_per_week'].median()\nprint (\"Median hours per week per men and women: \", ml_median_hr, fm_median_hr)","7a33b62a":"import matplotlib.pyplot as plt\nml_age=ml['age']\nml_age.hist(density=0, histtype='stepfilled', bins=20)","42be369f":"fm_age=fm['age']\nfm_age.hist(density=0, histtype='stepfilled', bins=10)\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Female samples',fontsize=15)\nplt.show()","6ad8212e":"import seaborn as sns\nfm_age.hist(density=0, histtype='stepfilled', alpha=.5, bins=20)   # default number of bins = 10\nml_age.hist(density=0, histtype='stepfilled', alpha=.5, color=sns.desaturate(\"indianred\", .75), bins=10)\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Samples',fontsize=15)\nplt.show()","825124dc":"fm_age.hist(density=1, histtype='stepfilled', alpha=.5, bins=20)   # default number of bins = 10\nml_age.hist(density=1, histtype='stepfilled', alpha=.5, color=sns.desaturate(\"indianred\", .75), bins=10)\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('PMF',fontsize=15)\nplt.show()","f11737f8":"ml_age.hist(density=1, histtype='stepfilled', bins=20)\n\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Probability',fontsize=15)\nplt.show()","fb40256e":"fm_age.hist(density=1, histtype='stepfilled', bins=20)\n\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Probability',fontsize=15)\nplt.show()","7cefdba1":"ml_age.hist(density=1, histtype='step', cumulative=True, linewidth=3.5, bins=20)\n\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('CDF',fontsize=15)\nplt.show()","091bd5d4":"fm_age.hist(density=1, histtype='step', cumulative=True, linewidth=3.5, bins=20)\n\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('CDF',fontsize=15)\nplt.show()","eb62a936":"ml_age.hist(bins=10, density=1, histtype='stepfilled', alpha=.5)   # default number of bins = 10\nfm_age.hist(bins=10, density=1, histtype='stepfilled', alpha=.5, color=sns.desaturate(\"indianred\", .75))\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Probability',fontsize=15)\nplt.show()","06832214":"ml_age.hist(density=1, histtype='step', cumulative=True,  linewidth=3.5, bins=20)\nfm_age.hist(density=1, histtype='step', cumulative=True,  linewidth=3.5, bins=20, color=sns.desaturate(\"indianred\", .75))\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('CDF',fontsize=15)\nplt.show()","ae93e138":"print (\"The mean sample difference is \", ml_age.mean() - fm_age.mean())","caf28d32":"df['age'].median()","35318bc0":"len(df[(df.income == '>50K\\n') & (df['age'] < df['age'].median() - 15)])\n","2b7a13ac":"len(df[(df.income == '>50K\\n') & (df['age'] > df['age'].median() + 35)])","a6d7332e":"df2 = df.drop(df.index[(df.income=='>50K\\n') & (df['age']>df['age'].median() +35) & (df['age'] > df['age'].median()-15)])\n\ndf2.shape","b9ab566f":"ml1_age=ml1['age']\nfm1_age=fm1['age']","c0bb8ede":"ml2_age = ml1_age.drop(ml1_age.index[(ml1_age >df['age'].median()+35) & (ml1_age>df['age'].median() - 15)])\n\nfm2_age = fm1_age.drop(fm1_age.index[(fm1_age > df['age'].median()+35) & (fm1_age > df['age'].median()- 15)])","6ede9479":"mu2ml = ml2_age.mean()\nstd2ml = ml2_age.std()\nmd2ml = ml2_age.median()\n\n# Computing the mean, std, median, min and max for the high-income male population\n\nprint (\"Men statistics: Mean:\", mu2ml, \"Std:\", std2ml, \"Median:\", md2ml, \"Min:\", ml2_age.min(), \"Max:\",ml2_age.max())","945760cf":"mu3ml = fm2_age.mean()\nstd3ml = fm2_age.std()\nmd3ml = fm2_age.median()\n\n# Computing the mean, std, median, min and max for the high-income female population\nprint (\"Women statistics: Mean:\", mu2ml, \"Std:\", std2ml, \"Median:\", md2ml, \"Min:\", fm2_age.min(), \"Max:\",fm2_age.max())","3c26573c":"print ('The mean difference with outliers is: %4.2f.'% (ml_age.mean() - fm_age.mean()))\nprint (\"The mean difference without outliers is: %4.2f.\"% (ml2_age.mean() - fm2_age.mean()))","2c01c53b":"plt.figure(figsize=(13.4,5))\n\ndf.age[(df.income == '>50K\\n')].plot(alpha=.25, color='blue')\ndf2.age[(df2.income == '>50K\\n')].plot(alpha=.45,color='red')\n\nplt.ylabel('Age')\nplt.xlabel('Samples')","94325d28":"import numpy as np\n\ncountx,divisionx = np.histogram(ml2_age, normed=True)\ncounty,divisiony = np.histogram(fm2_age, normed=True)","7fd47ff8":"import matplotlib.pyplot as plt\n\nval = [(divisionx[i]+divisionx[i+1])\/2 for i in range(len(divisionx)-1)]\n\nplt.plot(val, countx-county,'o-')\nplt.title('Differences in promoting men vs. women')\nplt.xlabel('Age',fontsize=15)\nplt.ylabel('Differences',fontsize=15)\nplt.show()","249b4827":"print (\"Remember:\\n We have the following mean values for men, women and the difference:\\nOriginally: \", ml_age.mean(), fm_age.mean(),  ml_age.mean()- fm_age.mean()) # The difference between the mean values of male and female populations.)\nprint (\"For high-income: \", ml1_age.mean(), fm1_age.mean(), ml1_age.mean()- fm1_age.mean()) # The difference between the mean values of male and female populations.)\nprint (\"After cleaning: \", ml2_age.mean(), fm2_age.mean(), ml2_age.mean()- fm2_age.mean()) # The difference between the mean values of male and female populations.)\n\nprint (\"\\nThe same for the median:\")\nprint (ml_age.median(), fm_age.median(), ml_age.median()- fm_age.median()) # The difference between the mean values of male and female populations.)\nprint (ml1_age.median(), fm1_age.median(), ml1_age.median()- fm1_age.median()) # The difference between the mean values of male and female populations.)\nprint (ml2_age.median(), fm2_age.median(), ml2_age.median()- fm2_age.median()), # The difference between the mean values of male and female populations.)","6cfc7251":"def skewness(x):\n    res=0\n    m=x.mean()\n    s=x.std()\n    for i in x:\n        res+=(i-m)*(i-m)*(i-m)\n    res\/=(len(x)*s*s*s)\n    return res\n\nprint (\"The skewness of the male population is:\", skewness(ml2_age))\nprint (\"The skewness of the female population is:\", skewness(fm2_age))","c6207e58":"def pearson(x):\n    return 3*(x.mean()-x.median())\/x.std()\n\nprint (\"The Pearson's coefficient of the male population is:\", pearson(ml2_age))\nprint (\"The Pearson's coefficient of the female population is:\", pearson(fm2_age))","c2799609":"ml1 = df[(df.sex == 'Male')&(df.income=='>50K\\n')]\n\nml2 = ml1.drop(ml1.index[(ml1['age']>df['age'].median() +35)&(ml1['age']> df['age'].median()- 15)])\n\nfm2 = fm1.drop(fm1.index[(fm1['age']> df['age'].median() + 35)& (fm1['age']> df['age'].median() - 15)])\n\nprint (ml2.shape, fm2.shape)","1df5f806":"print (\"Men grouped in 3 categories:\")\nprint (\"Young:\",int(round(100*len(ml2_age[ml2_age<41])\/float(len(ml2_age.index)))),\"%.\")\nprint (\"Elder:\", int(round(100*len(ml2_age[ml2_age >44])\/float(len(ml2_age.index)))),\"%.\")\nprint (\"Average age:\", int(round(100*len(ml2_age[(ml2_age>40) & (ml2_age< 45)])\/float(len(ml2_age.index)))),\"%.\")","d091e3ed":"print (\"Women grouped in 3 categories:\")\nprint (\"Young:\",int(round(100*len(fm2_age[fm2_age <41])\/float(len(fm2_age.index)))),\"%.\")\nprint (\"Elder:\", int(round(100*len(fm2_age[fm2_age >44])\/float(len(fm2_age.index)))),\"%.\")\nprint (\"Average age:\", int(round(100*len(fm2_age[(fm2_age>40) & (fm2_age< 45)])\/float(len(fm2_age.index)))),\"%.\")","e2e084cf":"print (\"The male mean:\", ml2_age.mean())\nprint (\"The female mean:\", fm2_age.mean())","a4887c1b":"ml2_young = len(ml2_age[(ml2_age<41)])\/float(len(ml2_age.index))\nfm2_young  = len(fm2_age[(fm2_age<41)])\/float(len(fm2_age.index))\nprint (\"The relative risk of female early promotion is: \", 100*(1-ml2_young\/fm2_young))","124e5171":"ml2_elder = len(ml2_age[(ml2_age>44)])\/float(len(ml2_age.index))\nfm2_elder  = len(fm2_age[(fm2_age>44)])\/float(len(fm2_age.index))\nprint (\"The relative risk of male late promotion is: \", 100*ml2_elder\/fm2_elder)","5082c8d6":"l = 3\nx=np.arange(0,2.5,0.1)\ny= 1- np.exp(-l*x)\n\nplt.plot(x,y,'-')\nplt.title('Exponential CDF: $\\lambda$ =%.2f'% l ,fontsize=15)\nplt.xlabel('x',fontsize=15)\nplt.ylabel('CDF',fontsize=15)\nplt.show()","58ed388e":"from __future__ import division\nimport scipy.stats as stats\n\nl = 3\nx=np.arange(0,2.5,0.1)\ny= l * np.exp(-l*x)\n\nplt.plot(x,y,'-')\nplt.title('Exponential PDF: $\\lambda$ =%.2f'% l, fontsize=15)\nplt.xlabel('x', fontsize=15)\nplt.ylabel('PDF', fontsize=15)\nplt.show()","835666b7":"l = 0.25\n\nx=np.arange(0,25,0.1)\ny= l * np.exp(-l*x)\n\nplt.plot(x,y,'-')\nplt.title('Exponential: $\\lambda$ =%.2f' %l ,fontsize=15)\nplt.xlabel('x',fontsize=15)\nplt.ylabel('PDF',fontsize=15)\nplt.show()","91e2d58e":"u=6 # mean\ns=2 # standard deviation\n\nx=np.arange(0,15,0.1)\n\ny=(1\/(np.sqrt(2*np.pi*s*s)))*np.exp(-(((x-u)**2)\/(2*s*s)))\n\nplt.plot(x,y,'-')\nplt.title('Gaussian PDF: $\\mu$=%.1f, $\\sigma$=%.1f'%(u,s),fontsize=15)\nplt.xlabel('x',fontsize=15)\nplt.ylabel('Probability density',fontsize=15)\nplt.show()","d44d0a91":"fig, ax = plt.subplots(1, 4, sharey=True, squeeze=True, figsize=(14, 5))\nx = np.linspace(0, 1, 100)\nfor i in range(4):\n    f = np.mean(np.random.random((10000, i+1)), 1)\n    m, s = np.mean(f), np.std(f, ddof=1)\n    fn = (1\/(s*np.sqrt(2*np.pi)))*np.exp(-(x-m)**2\/(2*s**2))  # normal pdf            \n    ax[i].hist(f, 40, density=True, color=[0, 0.2, .8, .6]) \n    ax[i].set_title('n=%d' %(i+1))\n    ax[i].plot(x, fn, color=[1, 0, 0, .6], linewidth=5)\nplt.suptitle('Demonstration of the central limit theorem for a uniform distribution', y=1.05)\nplt.show()","cc34911b":"from scipy.stats.distributions import norm\n\n# Some random data\ny = np.random.random(15) * 10\nx = np.linspace(0, 10, 100)\n\nx1 = np.random.normal(-1, 2, 15) # parameters: (loc=0.0, scale=1.0, size=None)\nx2 = np.random.normal(6, 3, 10)\ny = np.r_[x1, x2] # r_ Translates slice objects to concatenation along the first axis.\nx = np.linspace(min(y), max(y), 100)\n\n# Smoothing parameter\ns = 0.4\n\n# Calculate the kernels\nkernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n\nplt.plot(x, kernels, 'k:')\nplt.plot(x, kernels.sum(1), 'r')\nplt.plot(y, np.zeros(len(y)), 'go', ms=10)","c7fa579c":"from scipy.stats import kde\n\nx1 = np.random.normal(-1, 0.5, 15)\n\n# parameters: (loc=0.0, scale=1.0, size=None)\n\nx2 = np.random.normal(6, 1, 10)\ny = np.r_[x1, x2]\n\n# r_ Translates slice objects to concatenation along the first axis.\n\nx = np.linspace(min(y), max(y), 100)\ns = 0.4   # Smoothing parameter\n\nkernels = np.transpose([norm.pdf(x, yi, s) for yi in y])\n\n# Calculate the kernels\ndensity = kde.gaussian_kde(y)\n\nplt.plot(x, kernels, 'k:')\nplt.plot(x, kernels.sum(1), 'r')\nplt.plot(y, np.zeros(len(y)), 'bo', ms=10)","cf96328d":"xgrid = np.linspace(x.min(), x.max(), 200)\nplt.hist(y, bins=28, density=True)\nplt.plot(xgrid, density(xgrid), 'r-')","055f60fc":"# Create a bi-modal distribution with a mixture of Normals.\n\nx1 = np.random.normal(-1, 2, 15) # parameters: (loc=0.0, scale=1.0, size=None)\nx2 = np.random.normal(6, 3, 10)\n\n# Append by row\nx = np.r_[x1, x2]\n\n# r_ Translates slice objects to concatenation along the first axis.\nplt.hist(x, bins=18, density=True)","324108e1":"density = kde.gaussian_kde(x)\nxgrid = np.linspace(x.min(), x.max(), 200)\nplt.hist(x, bins=18, density=True)\nplt.plot(xgrid, density(xgrid), 'r-')","df671cfd":"x = np.random.normal(0.0, 1.0, 10000)\na = plt.hist(x,50,density='True')","8bf05d44":"print ('The empirical mean of the sample is ', x.mean())","bbe8c90f":"NTs=200\nmu=0.0\nvar=1.0\nerr = 0.0\nNPs=1000\nfor i in range(NTs):\n    x = np.random.normal(mu, var, NPs)\n    err += (x.mean()-mu)**2\n\nprint ('MSE: ', err\/NTs)","390e702b":"def Cov(X, Y):\n    def _get_dvis(V):\n        return [v - np.mean(V) for v in V]\n    dxis = _get_dvis(X)\n    dyis = _get_dvis(Y)\n    return np.sum([x * y for x, y in zip(dxis, dyis)])\/len(X)\n\n\nX = [5, -1, 3.3, 2.7, 12.2]\nX= np.array(X)\nY = [10, 12, 8, 9, 11]\n\nprint (\"Cov(X, X) = %.2f\" % Cov(X, X))\nprint (\"Var(X) = %.2f\" % np.var(X))\n\nprint (\"Cov(X, Y) = %.2f\" % Cov(X, Y))","b705acc0":"MAXN=100\nMAXN=40\n\nX=np.array([[1,9],[3, 2], [5,3],[5.5,4],[6,4],[6.5,4],[7,3.5],[7.5,3.8],[8,4],\n[8.5,4],[9,4.5],[9.5,7],[10,9],[10.5,11],[11,11.5],[11.5,12],[12,12],[12.5,12],[13,10]])","e14209c0":"plt.subplot(1,2,1)\nplt.scatter(X[:,0],X[:,1],color='b',s=120, linewidths=2,zorder=10)\nplt.xlabel('Economic growth(T)',fontsize=15)\nplt.ylabel('Stock market returns(T)',fontsize=15)\nplt.gcf().set_size_inches((20,6))","02e4ad3f":"X=np.array([[1,8],[2, 7], [3,6],[4,8],[5,8],[6,7],[7,7],[8,5],[9,5],[10,6],[11,4],[12,5],[13,3],[14,2],[15,2],[16,1]])\n\nplt.subplot(1,2,1)\nplt.scatter(X[:,0],X[:,1],color='b',s=120, linewidths=2,zorder=10)\nplt.xlabel('World Oil Production(T)',fontsize=15)\nplt.ylabel('Gasoline prices(T)',fontsize=15)\nplt.gcf().set_size_inches((20,6))","20998cfc":"def Corr(X, Y):\n    assert len(X) == len(Y)\n    return Cov(X, Y) \/ np.prod([np.std(V) for V in [X, Y]])\n\nprint (\"Corr(X, X) = %.5f\" % Corr(X, X))\n\nY=np.random.random(len(X))\n\nprint (\"Corr(X, Y) = %.5f\" % Corr(X, Y))","3aaf3f7f":"def list2rank(l):\n    #l is a list of numbers\n    # returns a list of 1-based index; mean when multiple instances\n    return [np.mean([i+1 for i, sorted_el in enumerate(sorted(l)) if sorted_el == el]) for el in l]\n\nl = [7, 1, 2, 5]\nprint (\"ranks: \", list2rank(l))\n\ndef spearmanRank(X, Y):\n    # X and Y are same-length lists\n    print (list2rank(X) )\n    print (list2rank(Y))\n    return Corr(list2rank(X), list2rank(Y))\n\nX = [10, 20, 30, 40, 1000]\nY = [-70, -1000, -50, -10, -20]\nplt.plot(X,'ro')\nplt.plot(Y,'go')\n\nprint (\"Pearson rank coefficient: %.2f\" % Corr(X, Y))\nprint (\"Spearman rank coefficient: %.2f\" % spearmanRank(X, Y))","4a93ec8f":"X=np.array([[10.0, 8.04,10.0, 9.14, 10.0, 7.46, 8.0, 6.58],\n[8.0,6.95, 8.0, 8.14, 8.0, 6.77, 8.0, 5.76],\n[13.0,7.58,13.0,8.74,13.0,12.74,8.0,7.71],\n[9.0,8.81,9.0,8.77,9.0,7.11,8.0,8.84],\n[11.0,8.33,11.0,9.26,11.0,7.81,8.0,8.47],\n[14.0,9.96,14.0,8.10,14.0,8.84,8.0,7.04],\n[6.0,7.24,6.0,6.13,6.0,6.08,8.0,5.25],\n[4.0,4.26,4.0,3.10,4.0,5.39,19.0,12.50],\n[12.0,10.84,12.0,9.13,12.0,8.15,8.0,5.56],\n[7.0,4.82,7.0,7.26,7.0,6.42,8.0,7.91],\n[5.0,5.68,5.0,4.74,5.0,5.73,8.0,6.89]])","7985234e":"plt.subplot(2,2,1)\nplt.scatter(X[:,0],X[:,1],color='r',s=120, linewidths=2,zorder=10)\nplt.xlabel('x1',fontsize=15)\nplt.ylabel('y1',fontsize=15)","9ffd7ce0":"plt.subplot(2,2,2)\nplt.scatter(X[:,2],X[:,3],color='r',s=120, linewidths=2,zorder=10)\nplt.xlabel('x1',fontsize=15)\nplt.ylabel('y1',fontsize=15)\nplt.subplot(2,2,3)\nplt.scatter(X[:,4],X[:,5],color='r',s=120, linewidths=2,zorder=10)\nplt.xlabel('x1',fontsize=15)\nplt.ylabel('y1',fontsize=15)","caf7bd33":"plt.subplot(2,2,4)\nplt.scatter(X[:,6],X[:,7],color='r',s=120, linewidths=2,zorder=10)\nplt.xlabel('x1',fontsize=15)\nplt.ylabel('y1',fontsize=15)\nplt.gcf().set_size_inches((10,10))","d089917d":"import matplotlib.pylab as plt","a472758b":"from sklearn import metrics\nmetrics.homogeneity_score([0, 0, 1, 1], [1, 1, 0, 0])","50d231f1":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 1, 2]))","c8c4ff15":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 2, 3]))","8d60ac79":"print(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint (metrics.completeness_score([0, 0, 1, 1], [1, 1, 0, 0]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 0, 0, 0]))\nprint(metrics.completeness_score([0, 1, 2, 3], [0, 0, 1, 1]))\nprint(metrics.completeness_score([0, 0, 1, 1], [0, 1, 0, 1]))\nprint(metrics.completeness_score([0, 0, 0, 0], [0, 1, 2, 3]))","833f2c77":"print (metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 1]))\nprint (metrics.v_measure_score([0, 0, 1, 1], [1, 1, 0, 0]))","b1dc31ec":"print(\"%.3f\" % metrics.completeness_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.homogeneity_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 0, 0]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 2], [0, 0, 1, 1]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 1, 2, 3], [0, 0, 1, 1]))","843064a3":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 1, 2]))\nprint(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 1, 2, 3]))","f101d508":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 0, 0], [0, 1, 2, 3]))","884bc561":"print(\"%.3f\" % metrics.v_measure_score([0, 0, 1, 1], [0, 0, 0, 0]))","431a15a0":"import numpy as np\n\n#Create some data\nMAXN=40\nX = np.concatenate([1.25*np.random.randn(MAXN,2), 5+1.5*np.random.randn(MAXN,2)])\nX = np.concatenate([X,[8,3]+1.2*np.random.randn(MAXN,2)])\nX.shape","dad2fe59":"#Just for visualization purposes, create the labels of the 3 distributions\ny = np.concatenate([np.ones((MAXN,1)),2*np.ones((MAXN,1))])\ny = np.concatenate([y,3*np.ones((MAXN,1))])\n\n\nplt.subplot(1,2,1)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\nplt.title('Data as were generated')\n\nplt.subplot(1,2,2)\nplt.scatter(X[:,0],X[:,1],color='r')\nplt.title('Data as the algorithm sees them')\n\nplt.savefig(\"\/kaggle\/working\/sample.png\",dpi=300, bbox_inches='tight')\n\nfrom sklearn import cluster\n\nK=3 # Assuming to be 3 clusters!\n\nclf = cluster.KMeans(init='random', n_clusters=K)\nclf.fit(X)\n","c278a198":"print (clf.labels_) # or\nprint (clf.predict(X)) # equivalent","b40c5a8e":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])","6a77aaf2":"print (X[(y==1).ravel(),0]) #numpy.ravel() returns a flattened array\nprint (X[(y==1).ravel(),1])\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))","d585cd4d":"x = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nsz=XX.shape\ndata=np.c_[XX.ravel(),YY.ravel()]\n# c_ translates slice objects to concatenation along the second axis.","3f7c7f37":"Z=clf.predict(data) # returns the labels of the data\nprint (Z)","de994e8a":"# Visualize space partition\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\nplt.title('Space partitions', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working\/samples3.png\",dpi=300, bbox_inches='tight')","92f387a0":"clf = cluster.KMeans(n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\ndata=np.c_[XX.ravel(),YY.ravel()]\nZ=clf.predict(data) # returns the clustering labels of the data","1cf790f9":"plt.title('Final result of K-means', size=14)\n\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b')\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g')\n\nplt.imshow(Z.reshape(sz), interpolation='bilinear', origin='lower',\nextent=(-5,15,-5,15),alpha=0.3, vmin=0, vmax=K-1)\n\nx = np.linspace(-5,15,200)\nXX,YY = np.meshgrid(x,x)\nfig = plt.gcf()\nfig.set_size_inches((6,5))\n\nplt.savefig(\"\/kaggle\/working\/randscore.png\",dpi=300, bbox_inches='tight')","02615256":"clf = cluster.KMeans(init='random', n_clusters=K, random_state=0)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\nZx=clf.predict(X)\n\nplt.subplot(1,3,1)\nplt.title('Original labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='b') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='g') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,2)\nplt.title('Data without labels', size=14)\nplt.scatter(X[(y==1).ravel(),0],X[(y==1).ravel(),1],color='r')\nplt.scatter(X[(y==2).ravel(),0],X[(y==2).ravel(),1],color='r') # b\nplt.scatter(X[(y==3).ravel(),0],X[(y==3).ravel(),1],color='r') # g\nfig = plt.gcf()\nfig.set_size_inches((12,3))\n\nplt.subplot(1,3,3)\nplt.title('Clustering labels', size=14)\nplt.scatter(X[(Zx==1).ravel(),0],X[(Zx==1).ravel(),1],color='r')\nplt.scatter(X[(Zx==2).ravel(),0],X[(Zx==2).ravel(),1],color='b')\nplt.scatter(X[(Zx==0).ravel(),0],X[(Zx==0).ravel(),1],color='g')\nfig = plt.gcf()\nfig.set_size_inches((12,3))","337f4a0f":"from sklearn import metrics\n\nclf = cluster.KMeans(n_clusters=K, init='k-means++', random_state=0,\nmax_iter=300, n_init=10)\n#initialize the k-means clustering\nclf.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint('Inertia: %.2f' % clf.inertia_)\n\nprint('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf.labels_))\n\nprint('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf.labels_))\n\nprint('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf.labels_))\n\nprint('V_measure %.2f' % metrics.v_measure_score(y.ravel(), clf.labels_))\n\nprint('Silhouette %.2f' % metrics.silhouette_score(X, clf.labels_,\nmetric='euclidean'))\n\nclf1 = cluster.KMeans(n_clusters=K, init='random', random_state=0,\nmax_iter=2, n_init=2)\n#initialize the k-means clustering\nclf1.fit(X) #run the k-means clustering\n\nprint ('Final evaluation of the clustering:')\n\nprint ('Inertia: %.2f' % clf1.inertia_)\n\nprint ('Adjusted_rand_score %.2f' % metrics.adjusted_rand_score(y.ravel(),\nclf1.labels_))\n\nprint ('Homogeneity %.2f' % metrics.homogeneity_score(y.ravel(),\nclf1.labels_))\n\nprint ('Completeness %.2f' % metrics.completeness_score(y.ravel(),\nclf1.labels_))\n\nprint ('V_measure %.2f' % metrics.v_measure_score(y.ravel(),\nclf1.labels_))\n\nprint ('Silhouette %.2f' % metrics.silhouette_score(X, clf1.labels_,\nmetric='euclidean'))","cb3319c2":"#Read and check the dataset downloaded from the EuroStat\n\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import cluster\n\nedu=pd.read_csv('\/kaggle\/input\/ense3-ict\/files\/ch07\/educ_figdp_1_Data.csv',na_values=':')\nedu.head()","43a5aa0d":"edu.tail()","793a4776":"#Pivot table in order to get a nice feature vector representation with dual indexing by TIME and GEO\npivedu=pd.pivot_table(edu, values='Value', index=['TIME', 'GEO'], columns=['INDIC_ED'])\npivedu.head()","6935e0ee":"print ('Let us check the two indices:\\n')\nprint ('\\nPrimary index (TIME): \\n' + str(pivedu.index.levels[0].tolist()))\nprint ('\\nSecondary index (GEO): \\n' + str(pivedu.index.levels[1].tolist()))","e234b701":"#Extract 2010 set of values\nedu2010=pivedu.loc[2010]\nedu2010.head()","f8d973de":"#Store column names and clear them for better handling. Do the same with countries\nedu2010 = edu2010.rename(index={'Euro area (13 countries)': 'EU13',\n'Euro area (15 countries)': 'EU15',\n'European Union (25 countries)': 'EU25',\n'European Union (27 countries)': 'EU27',\n'Former Yugoslav Republic of Macedonia, the': 'Macedonia',\n'Germany (until 1990 former territory of the FRG)': 'Germany'\n})\nfeatures = edu2010.columns.tolist()\n\ncountries = edu2010.index.tolist()\n\nedu2010.columns=range(12)\nedu2010.head()","9960c5c2":"#Check what is going on in the NaN data\nnan_countries=np.sum(np.where(edu2010.isnull(),1,0),axis=1)\nplt.bar(np.arange(nan_countries.shape[0]),nan_countries)\nplt.xticks(np.arange(nan_countries.shape[0]),countries,rotation=90,horizontalalignment='left',\nfontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","c71f437c":"#Remove non info countries\nwrk_countries = nan_countries<4\n\neduclean=edu2010.loc[wrk_countries] #.ix - Construct an open mesh from multiple sequences.\n\n#Let us check the features we have\nna_features = np.sum(np.where(educlean.isnull(),1,0),axis=0)\nprint (na_features)\n\nplt.bar(np.arange(na_features.shape[0]),na_features)\nplt.xticks(fontsize=12)\nfig = plt.gcf()\nfig.set_size_inches((8,4))","0fbbbe9d":"#Option A fills those features with some value, at risk of extracting wrong information\n#Constant filling : edufill0=educlean.fillna(0)\nedufill=educlean.fillna(educlean.mean())\nprint ('Filled in data shape: ' + str(edufill.shape))\n\n#Option B drops those features\nedudrop=educlean.dropna(axis=1)\n#dropna: Return object with labels on given axis omitted where alternately any or\n# all of the data are missing\nprint ('Drop data shape: ' + str(edudrop.shape))","d6be43f6":"scaler = StandardScaler() #Standardize features by removing the mean and scaling to unit variance\n\nX_train_fill = edufill.values\nX_train_fill = scaler.fit_transform(X_train_fill)\n\nclf = cluster.KMeans(init='k-means++', n_clusters=3, random_state=42)\n\nclf.fit(X_train_fill) #Compute k-means clustering.\n\ny_pred_fill = clf.predict(X_train_fill)\n#Predict the closest cluster each sample in X belongs to.\n\nidx=y_pred_fill.argsort()","2e30f29d":"plt.plot(np.arange(35),y_pred_fill[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using filled in data', size=15)\nplt.yticks([0,1,2])\nfig = plt.gcf()\n\nfig.set_size_inches((12,5))","dad362fe":"X_train_drop = edudrop.values\nX_train_drop = scaler.fit_transform(X_train_drop)\n\nclf.fit(X_train_drop) #Compute k-means clustering.\ny_pred_drop = clf.predict(X_train_drop) #Predict the closest cluster of each sample in X.","ee1d0042":"idx=y_pred_drop.argsort()\nplt.plot(np.arange(35),y_pred_drop[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],\nrotation=90,horizontalalignment='left',fontsize=12)\nplt.title('Using dropped missing values data',size=15)\nfig = plt.gcf()\nplt.yticks([0,1,2])\nfig.set_size_inches((12,5))","d581dad3":"plt.plot(y_pred_drop+0.2*np.random.rand(35),y_pred_fill+0.2*np.random.rand(35),'bo')\nplt.xlabel('Predicted clusters for the filled in dataset.')\nplt.ylabel('Predicted clusters for the dropped missing values dataset.')\nplt.title('Correlations')\nplt.xticks([0,1,2])\nplt.yticks([0,1,2])\nplt.savefig(\"\/kaggle\/working\/correlationkmeans.png\",dpi=300, bbox_inches='tight')","9bed006c":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==0]))\nprint ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]))\nprint ('\\n')\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==1]))\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==1]))\nprint ('\\n')\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_fill)\nif item==2]))\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==2]))\nprint ('\\n')","d0ec800c":"width=0.3\np1 = plt.bar(np.arange(8),scaler.inverse_transform(clf.cluster_centers_[1]),width,color='b')\n# Scale back the data to the original representation\np2 = plt.bar(np.arange(8)+width,scaler.inverse_transform(clf.cluster_centers_[2]),\nwidth,color='yellow')\np0 = plt.bar(np.arange(8)+2*width,scaler.inverse_transform(clf.cluster_centers_[0]),\nwidth,color='r')\n\nplt.legend( (p0[0], p1[0], p2[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicators')\nplt.ylabel('Average expanditure')\nfig = plt.gcf()\n\nplt.savefig(\"\/kaggle\/working\/clusterexpenditure.png\",dpi=300, bbox_inches='tight')","0f6275d7":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\n#the distance of the elements of cluster 0 to the center of cluster 1\n\nfx = np.vectorize(np.int)\n\nplt.plot(np.arange(p.shape[0]),\nfx(p)\n)\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)","b14ac224":"from scipy.spatial import distance\np = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[1]],'euclidean')\npown = distance.cdist(X_train_drop[y_pred_drop==0,:],[clf.cluster_centers_[0]],'euclidean')\n\nwidth=0.45\np0=plt.plot(np.arange(p.shape[0]),fx(p),width)\np1=plt.plot(np.arange(p.shape[0])+width,fx(pown),width,color = 'red')\n\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\nzero_countries_names = [wrk_countries_names[i] for i,item in enumerate(y_pred_drop)\nif item==0]\nplt.xticks(np.arange(len(zero_countries_names)),zero_countries_names,rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.legend( (p0[0], p1[0]), ('d -> 1', 'd -> 0') ,loc=1)\nplt.savefig(\"\/kaggle\/working\/dist2cluster01.png\",dpi=300, bbox_inches='tight')","0cb84a93":"X_train = edudrop.values\nclf = cluster.KMeans(init='k-means++', n_clusters=4, random_state=0)\nclf.fit(X_train)\ny_pred = clf.predict(X_train)\n\nidx=y_pred.argsort()\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i] for i in idx],rotation=90,\nhorizontalalignment='left',fontsize=12)\nplt.title('Using drop features',size=15)\nplt.yticks([0,1,2,3])\nfig = plt.gcf()\nfig.set_size_inches((12,5))","b76ae16a":"width=0.2\np0 = plt.bar(np.arange(8)+1*width,clf.cluster_centers_[0],width,color='r')\np1 = plt.bar(np.arange(8),clf.cluster_centers_[1],width,color='b')\np2 = plt.bar(np.arange(8)+3*width,clf.cluster_centers_[2],width,color='yellow')\np3 = plt.bar(np.arange(8)+2*width,clf.cluster_centers_[3],width,color='pink')\n\nplt.legend( (p0[0], p1[0], p2[0], p3[0]), ('Cluster 0', 'Cluster 1', 'Cluster 2',\n'Cluster 3') ,loc=9)\nplt.xticks(np.arange(8) + 0.5, np.arange(8),size=12)\nplt.yticks(size=12)\nplt.xlabel('Economical indicator')\nplt.ylabel('Average expenditure')\nfig = plt.gcf()\nfig.set_size_inches((12,5))\nplt.savefig(\"\/kaggle\/working\/distances4clusters.png\",dpi=300, bbox_inches='tight')","90e78614":"print ('Cluster 0: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==0]))\n\nprint ('Cluster 1: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==1]))\n\nprint ('Cluster 2: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==2]))\n\nprint ('Cluster 3: \\n' + str([wrk_countries_names[i] for i,item in enumerate(y_pred) if item==3]))\n\n#Save data for future use.\nimport pickle\nofname = open('edu2010.pkl', 'wb')\ns = pickle.dump([edu2010, wrk_countries_names,y_pred ],ofname)\nofname.close()","04739241":"from scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.spatial.distance import pdist\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neighbors import kneighbors_graph\nfrom sklearn.metrics import euclidean_distances\n\nX = StandardScaler().fit_transform(edudrop.values)\n\ndistances = euclidean_distances(edudrop.values)\n\nspectral = cluster.SpectralClustering(n_clusters=4, affinity=\"nearest_neighbors\")\nspectral.fit(edudrop.values)\n\ny_pred = spectral.labels_.astype(np.int)","7589769d":"idx=y_pred.argsort()\n\nplt.plot(np.arange(35),y_pred[idx],'ro')\nwrk_countries_names = [countries[i] for i,item in enumerate(wrk_countries) if item ]\n\nplt.xticks(np.arange(len(wrk_countries_names)),[wrk_countries_names[i]\nfor i in idx],rotation=90,horizontalalignment='left',fontsize=12)\n\nplt.yticks([0,1,2,3])\n\nplt.title('Applying Spectral Clustering on the drop features',size=15)\nfig = plt.gcf()\nfig.set_size_inches((12,5))","2a5c3b0b":"X_train = edudrop.values\ndist = pdist(X_train,'euclidean')\nlinkage_matrix = linkage(dist,method = 'complete');\nplt.figure() # we need a tall figure\nfig = plt.gcf()\nfig.set_size_inches((12,12))\ndendrogram(linkage_matrix, orientation=\"right\", color_threshold = 4,labels = wrk_countries_names, leaf_font_size=20);\n\nplt.savefig(\"\/kaggle\/working\/ACCountires.png\",dpi=300, bbox_inches='tight')\nplt.show()\n\nplt.tight_layout() # fixes margins","16bc0e41":"SciPy implements a Gaussian KDE that automatically chooses an appropriate bandwidth. Let\u2019s create a bi-modal distribution of data that is not easily summarized by a parametric distribution:","88bdd27d":"There are four features with missing data. At this point we can proceed in two ways:\n\n1.Fill in the features with some non-informative, non-biasing data.\n2.Drop the features with missing values.\n\nIf we have many features and only a few have missing values then it is not much harmful to drop them. However, if missing values are spread across the features, we have to eventually deal with them. In our case, both options seem reasonable, so we will proceed with both at the same time.","6ba3f2ed":"10. we have determined median age per men and women","dec67b3d":" The graph shows the sum of kernals which gives a continuous function when we normalize the density of distritbution.","a9a0a3fa":"10. we have determined median age per men and women with high income","5e6fbe9f":"7. first it filters the people(in percentage) who are earning more than 50k and then we are separating men and women (in percentage) with high income ","b8684cd0":"men","583f47ef":"We can normalize the frequencies of the histogram by dividing\/normalizing by n, the number of samples. The normalized histogram is called the Probability Mass Function (PMF).\n\nThe graph shows the frequency of each value.X-plot is age of both men & women and y-plot is samples.\n\nNote that we are visualizing the absolute values of the numbers of people in our data set according to their age. As a side effect, we can see that there are many more men in these conditions than women.","248cb531":"After exploring the data, we obtained some apparent effects that support our initial assumptions. For example, the mean age for men in our dataset is 39.4 years; while for women, is 36.8 years. When analyzing the high-income salaries, the mean age for men increased to 44.6 years; while for women, increased to 42.1 years. When the data were cleaned from outliers, we obtained mean age for high-income men: 44.3, and for women: 41.8. Moreover, histograms and other statistics show the skewness of the data and the fact that women used to be promoted a little bit earlier than men, in general.","54527235":"4.2.5.3 Central limit theorem\nThe normal distribution is also important, because it is involved in the Central Limit Theorem:\n\nTake the mean of n random samples from ANY arbitrary distribution with a well-defined standard deviation \u03c3 and mean \u03bc. As n gets bigger the distribution of the sample mean will always converge to a Gaussian (normal) distribution with mean \u03bc and standard deviation \u03c3n\u221a.\n\nColloquially speaking, the theorem states the distribution of an average tends to be normal, even when the distribution from which the average is computed is decidedly non-normal. This explains the ubiquity of the Gaussian distribution in science and statistics.","9c278ad0":"4.The command shape gives exactly the number of data samples (in rows) and features (in columns): \n\n\n32561rows and 15 columns ","6f3b5026":"The graph shows the frequency of each value.X-plot is age of both men & women and y-plot is probability.If the cumlative is true with linewidth 3.5, a step will create for each age and a stepfilled histogram is given.For female indianred color is given to sort them out.","2b37627d":"Definition: Estimation is the process of inferring the parameters (e.g. mean) of a distribution from a statistic of samples drown from a population.\n\nFor example: What is the estimated mean \u03bc\u0302 of the following normal data?\n\nWe can use our definition of empirical mean:","b7959bb5":"Let us check the profile of the clusters by looking at the centroids:","c5bdbc72":"Well, it seems that Spain belongs to cluster \u201c0\u201c, it is the closest to change to a policy in the lines of the other clusters.\n\nAdditionally, we can also check the distance to the centroid of cluster \u201c0\u201c.","0d91b5bf":"4.3.2.6 Spearsman\u2019s rank correlation","a7779827":"4.2 Estimation\n\nAn important aspect when working with statistical data is being able to use estimates to approximate the values of unknown parameters of the dataset. In this section, we will review different kinds of estimators (estimated mean, variance, standard score, etc.).","2efd1c6f":"4.How many \u201cmisclusterings\u201d do we have?\n\n13","e2d91dfd":"1.Labelings that assign all classes members to the same clusters are: complete, but not homogeneous.","d7915ccb":"If we visualise the results:","e54c6d4b":"The obtained result is the mean of the distribution:0.0010014940903805701\n\nOne approximation is given by the sample mean, x \u0304. This process is called estimation and the statistic (e.g., the sample mean) is called an estimator.","972557d7":"To the process of reshaping stacked data into a table is sometimes called pivoting.","f41ca153":"One can easily see that the skewness for a normal distribution is zero, and any symmetric data must have a skewness of zero. Note that skewness can be affected by outliers! A simpler alternative is to look at the relationship between the mean \u03bcand the median \u03bc1\/2.\nThe skewness of the male population is: 0.2664443838432819\nThe skewness of the female population is: 0.38633352491285977","059e21ff":"The Cumulative Distribution Function (CDF), or just distribution function, describes the probability that a real-valued random variable X with a given proba- bility distribution will be found to have a value less than or equal to x. Let us show the CDF of age distribution for both men and women.\nThe graph shows the frequency of each value.X-plot is age of both men & women and y-plot is PMF.","87f31455":"The graph shows the frequency of each value.X-plot is age of both men & women and y-plot is probability.The default number of bins and stepfilled histogram is given.For female indianred color is given to sort them out.","7fdf2ccf":"4.2.5.1 Exponential distribution","66ad3bf4":"8.we are obtaining overall average age of mean and women.","ff1f3dd4":"10. first we have calculated the total hours per week and then we have determined median hours per week per men and women","5178364a":"Summarizing can be dangerous: very different data can be described by the same statistics. It must be validated by inspecting the data.\n\nWe can look at the data distribution, which describes how often (frequency) each value appears.\n\nWe can normalize the frequencies of the histogram by dividing\/normalizing by n, the number of samples. The normalized histogram is called Probability Mass Function (PMF).\n\nLet\u2019s visualize and compare the MPF of male and female age in our example:","551ed656":"4.3.1.5 Pearson\u2019s correlation","db60985a":"Spain is still in cluster \u201c0\u201c. But as we observed in our previous clustering it was very close to changing cluster. This time cluster \u201c0\u201c includes the averages values for the EU members. Just for the sake of completeness, let us write down the name of the countries in the clusters.","6cf38caf":"> 1. a slice of rows (1 row)\n","09921f37":"The graph shows the frequency of each value.X-plot is age of men and y-plot is CDF.If the cumlative is true with linewidth 3.5, a step will create for each age.","3d826bbb":"In the fill-in option, we have decided to fill the data with the mean value of the feature. This will not bias the distribution of the feature, though it has consequences in the interpretation of the results.\n\nLet us now apply a K-means clustering technique on this data in order to partition the countries according to their investment in education and check their profiles","f830a5fa":"6. What is the age of the most represented people? \n\nage-36( 898 people represented)","acd850a4":"Let us apply the clustering on the dataset with dropped missing values:","76f49093":" The graph shows the frequency of each value.X-plot is age of men and y-plot is Probability.","c2742957":"Well, looking at both methods, both may yield the same results, but not necessarily always. This is mainly due to two aspects: the random initialisation of the k-means clustering and the fact that each method works in a different space (dropped data vs. filled-in data).\n\nLet us check the list of countries in both methods. Note that we should not consider the cluster value, since it is irrelevant.","cdac2ec6":"Observe that we have ten years information on these indicators, and as expected we have all members of the European Union with some aggregates and control\/reference countries. For the sake of simplicity, let us focus on values on year 2010.","0cb49a16":"The graph shows the frequency of each value.X-plot is age of women and y-plot is CDF.If the cumlative is true with linewidth 3.5, a step will create for each age.","982e3275":"The cumulative distribution function (CDF), or just distribution function, describes the probability that a real-valued random variable X with a given probability distribution will be found to have a value less than or equal to x. For our example, the CDFs will be:","4232aace":" There are a lot of real world events that can be described with this distribution.\n\nThe time until a radioactive particle decays,\nThe time it takes before your next telephone call,\nThe time until default (on payment to company debt holders) in reduced form credit risk modeling.","eacb2974":"2.To see how the data looks, we can use the method head(), which shows just the first five rows.","36e8599b":"Exercise: Obtain for the Anscombe's quartet [2] given in the figures bellow, the different estimators (mean, variance, covariance for each pair, Pearson's correlation and Spearman's rank correlation.","892f11bf":"9. Here,we have calculated mean, variance and standard deviation for men and women in terms of hours per week.","f6c663ff":"2.If classes members are completely split across different clusters, the assignment is totally incomplete, hence the V-Measure is null.","f3e632a3":"It looks like cluster \u201c1\u201c spends more on education while cluster \u201c0\u201c is the one with less resources on education. What about Spain?\n\nLet us refine a little bit more cluster \u201c0\u201c and check how close are members from this cluster to cluster \u201c1\u201c. This may give us a hint on a possible ordering.","0eff6e08":"We can repeat the process using the alternative clustering techniques and compare their results. Let us first apply the spectral clustering. The corresponding code will be:","4e7bdc4f":"4.2.6 Kernel Density\n\nIn many real problems, we may not be interested in the parameters of a particular distribution of data, but just a continuous representation of the data. In this case, we should estimate the distribution non-parametrically (i.e., making no assumptions about the form of the underlying distribution) using kernel density estimation.","e880d29d":"Let us visualise the result of the K-means clustering:","7f6f5035":"The overall mean sample difference between male & female age is 2.5753","2d4044a2":"3.To see how the data looks, we can use the method tail(), which shows just the last five rows.","802ecd07":"As we can observe, this is not a clean data set, there are missing values. Some countries may not collect or have access to some indicators and there are countries without any indicators. Let us display this effect.","233e2cf8":"4.2.5.2 Normal distribution","3206e2c1":"3. CASE STUDY: EUROSTAT data analysis\n\nApplying clustering to analyze countries according to their education resourses","9b859150":"A step forward for discovering knowledge using unsupervised learning","02380df2":"We have sorted the data for better visualization. At a simple glance we can see that both partitions can be different. We can better check this effect plotting the clusters values of one technique against the other.","5299a2e2":"5. How many items are there for USA? and for Mexico? \n\n29170(USA) and 643(mexico)","357965be":"We do not have info on Albania, Macedonia and Greece. And very limited info from Liechtenstein, Luxembourg and Turkey. So let us work without them. Now let us check the features.","4d931691":"Let us clean and store the names of the features and the countries.","6167c92f":"9. Here, we are determining mean, variance and standard deviation for men and women in terms of age.","a42193df":"4.3.1.1 Mean","d5999eaf":" The graph shows the frequency of each value.X-plot is age of women and y-plot is Probability.","5660c5da":"HO3-Assignment","25bae719":"3.Clusters that include samples from totally different classes totally destroy the homogeneity of the labelling, hence","62ca575e":"In fact, the library SciPy3 implements a Gaussian kernel density estimation that automatically chooses the appropriate bandwidth parameter for the kernel. Thus, the final construction of the density estimate will be obtained by:","688381fc":"female","e4316da4":"Let us imagine that we have a set of data measurements without knowing their distribution and we need to estimate the continuous representation of their distribution. In this case, we can consider a Gaussian kernel to generate the density around the data. Let us consider a set of random data generated by a bimodal normal distribution. If we consider a Gaussian kernel around the data, the sum of those kernels can give us a continuous function that when normalized would approximate the density of the distribution.","ab74e592":"5.Shall the centroids belong to the original set of points?\n\nNo","b33ce344":"8. here we have obtained average age of both men and women (separately) with high income","3b98babc":"Note that in general, the spectral clustering intends to obtain more balanced clusters. In this way, the predicted cluster 1 merges the cluster 2 and 3 of the K-means clustering, cluster 2 corresponds to the cluster 1 of the K-means clustering, cluster 0 mainly goes to cluster 2, and clusters 3 corresponds to cluster 0 of the K-means.\n\nApplying the agglomerative clustering, we obtain not only the different clusters, but also we can see how different clusters are obtained. This, in some way it is giving us information on which are the pairs of countries and clusters that are most similar. The corresponding code that applies the agglomerative clustering is:","ac8daf17":"The graph shows the frequency of each value.X-plot is age of men and y-plot is total working men","7204c787":"The graph shows the frequency of each value.X-plot is age of women and y-plot is female samples","dbf05806":"Let us redo the clustering with K=4 and see what we can conclude."}}