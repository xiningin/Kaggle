{"cell_type":{"ec7055c6":"code","37db2149":"code","011af389":"code","b0a6c776":"code","eb268e06":"code","a358bdf2":"code","4ed1a179":"code","4ba7f952":"code","491b2b59":"code","55bb9afd":"code","d70deec3":"markdown","ad544777":"markdown","e510d275":"markdown","c995a8eb":"markdown","6d7859ee":"markdown"},"source":{"ec7055c6":"! pip install -q dabl","37db2149":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom colorama import Fore, Style\n\nimport dabl\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\n\nplt.style.use(\"classic\")\nwarnings.filterwarnings('ignore')","011af389":"def cout(string: str, color=Fore.RED):\n    \"\"\"\n    Saves some work \ud83d\ude05\n    \"\"\"\n    print(color+string+Style.RESET_ALL)","b0a6c776":"data = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\ndata.head()","eb268e06":"dabl.plot(data, target_col='DEATH_EVENT')","a358bdf2":"# First, we split the data\ndata = data[['age', 'ejection_fraction', 'serum_creatinine', 'serum_sodium', 'time', 'DEATH_EVENT']]\n\ntrainX, testX, trainY, testY = train_test_split(data.drop(['DEATH_EVENT'], axis=1), data['DEATH_EVENT'], test_size=0.2, random_state=1)\n\ncout(f\"Training Data Shape is: {trainX.shape}\", Fore.RED)\ncout(f\"Testing Data Shape is: {testX.shape}\", Fore.BLUE)","4ed1a179":"# Define all models\nnames = [\"Logistic Regression\", \"Nearest Neighbors\", \"RBF SVM\", \"Gaussian Process\",\n         \"Decision Tree\", \"Random Forest\", \"Neural Net\", \"AdaBoost\",\n         \"Naive Bayes\", \"QDA\", \"XGBoost\", \"CatBoost\"]\n\nclassifiers = [\n    LogisticRegression(),\n    KNeighborsClassifier(4),\n    SVC(kernel=\"rbf\", random_state=0),\n    GaussianProcessClassifier(2.0 * RBF(1.0)),\n    DecisionTreeClassifier(max_depth=20),\n    RandomForestClassifier(n_estimators = 17, criterion='gini', random_state=0),\n    MLPClassifier(alpha=3, max_iter=2000),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    QuadraticDiscriminantAnalysis(),\n    XGBClassifier(),\n    CatBoostClassifier()\n]","4ba7f952":"# Train all models\n\nclf_results_roc = {}\nclf_results_acc = {}\n\nfor name, clf in zip(names, classifiers):    \n    # Fit on the traning data\n    clf.fit(trainX, trainY)\n    \n    # Get the test time prediction\n    preds = clf.predict(testX)\n    \n    # Calculate Test ROC_AUC\n    score = roc_auc_score(testY, preds)\n    \n    # Calculate the val accuracy\n    val_acc = clf.score(testX, testY)\n    \n    # Store the results in a dictionary\n    clf_results_roc[name] = score\n    clf_results_acc[name] = val_acc\n    \n    cout(f\"Classifier: {name}\", Fore.YELLOW)\n    cout(f\"\\nval_acc: {val_acc:.3f} | roc_auc: {score:.3f}\\n\", Fore.BLUE)","491b2b59":"# Sort the Model Accuracies based on the test score\nsort_clf = dict(sorted(clf_results_acc.items(), key=lambda x: x[1], reverse=True))\n\n# Get the names and the corresponding scores\nclf_names = list(sort_clf.keys())[::-1]\nclf_scores = list(sort_clf.values())[::-1]\n\n# Plot the results\nplt.figure(figsize=(14, 8))\nsns.barplot(x=clf_names, y=clf_scores)\nplt.xlabel(\"Models\")\nplt.ylabel(\"Validation Accuracy\")\nplt.xticks(rotation=45)\nplt.title(\"Model Comparison - Validation Accuracy\")\nplt.show()","55bb9afd":"# Sort the Model Accuracies based on the roc-auc score\nsort_clf = dict(sorted(clf_results_roc.items(), key=lambda x: x[1], reverse=True))\n\n# Get the names and the corresponding scores\nclf_names = list(sort_clf.keys())[::-1]\nclf_scores = list(sort_clf.values())[::-1]\n\n# Plot the results\nplt.figure(figsize=(14, 8))\nsns.barplot(x=clf_names, y=clf_scores)\nplt.xlabel(\"Models\")\nplt.ylabel(\"ROC-AUC Score\")\nplt.xticks(rotation=45)\nplt.title(\"Model Comparison - ROC-AUC Scores\")\nplt.show()","d70deec3":"### Performance comparison based on Validation Accuracy","ad544777":"# Introduction\nThis notebook consists of small EDA (with DABL) and comparison of performance of 11 different models.\n\n<span style=\"color:red\">If you like my work, please don't forget to upvote this notebook!<\/span>\n\n<span style=\"color:blue\">If you don't, atleast leave a comment on what should I do to improve it!<\/span>","e510d275":"### Performance comparison based on ROC-AUC Value","c995a8eb":"# Multi-Model Training and Comparison\nLet's initialize, train and test multiple models","6d7859ee":"# Quick EDA\nSince, this notebook isn't about EDA, let's do some quick plotting only using dabl"}}