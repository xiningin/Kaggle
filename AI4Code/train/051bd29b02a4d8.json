{"cell_type":{"be6f1fd2":"code","b376fc81":"code","80acde52":"code","fc5439f3":"code","8ec8ecfc":"code","fb87a6f1":"code","56227a40":"code","82e5677c":"code","1a425f85":"code","ded427ca":"markdown","5925f40c":"markdown","053edd1f":"markdown","39fd175f":"markdown","a92349e7":"markdown","3f8f912b":"markdown","d6224688":"markdown","17db62cf":"markdown","6d47fc40":"markdown","84062c7d":"markdown","99cc138e":"markdown","534b5136":"markdown","a4b9de5c":"markdown"},"source":{"be6f1fd2":"import numpy as np \nimport pandas as pd \n\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv2D, MaxPooling2D\nfrom keras.layers.core import Activation, Flatten, Dense\nfrom keras import backend as K\nfrom keras.optimizers import SGD\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelBinarizer","b376fc81":"data = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\")\n\nlabels = data.iloc[:, 0]\npixels = data.iloc[:, 1:]\n\nnp_labels = labels.to_numpy()\nnp_pixels = pixels.to_numpy()\n\nprint(np_labels.shape)\nprint(np_pixels.shape)","80acde52":"train_data, test_data, train_labels, test_labels = train_test_split(np_pixels \/ 255.0, np_labels, test_size = 0.2, random_state = 777)\n\ntrain_data = train_data.reshape(train_data.shape[0], 28, 28, 1).astype(\"float32\")\ntest_data = test_data.reshape(test_data.shape[0], 28, 28, 1).astype(\"float32\")\n\nlb = LabelBinarizer()\ntrain_labels = lb.fit_transform(train_labels)\ntest_labels = lb.transform(test_labels)\n\nprint(train_data.shape)\nprint(train_labels.shape)\nprint(test_data.shape)\nprint(test_labels.shape)","fc5439f3":"class Model:\n    def build(width, height, depth, classes):\n        model = Sequential()\n        inputShape = (height, width, depth)\n        \n        model.add(Conv2D(20, (5, 5), padding = \"same\", input_shape = inputShape))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n        \n        model.add(Conv2D(50, (5, 5), padding = \"same\", input_shape = inputShape))\n        model.add(Activation(\"relu\"))\n        model.add(MaxPooling2D(pool_size = (2, 2), strides = (2, 2)))\n        \n        model.add(Flatten())\n        model.add(Dense(500))\n        model.add(Activation(\"relu\"))\n        \n        model.add(Dense(classes))\n        model.add(Activation(\"softmax\"))\n        \n        return model","8ec8ecfc":"model = Model.build(28, 28, 1, 10)\nopt = SGD(lr = 0.01)\n\nmodel.compile(loss = \"categorical_crossentropy\", optimizer = opt, metrics = [\"accuracy\"])\n\nX = model.fit(train_data, train_labels, validation_data = (test_data, test_labels), batch_size = 128, epochs = 20, verbose = 1)","fb87a6f1":"score = model.evaluate(test_data, test_labels, batch_size = 128)\nprint(score)\n\nmodel.summary()","56227a40":"kaggle_test_data = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\nkaggle_test_data = kaggle_test_data.to_numpy()\nkaggle_test_data = kaggle_test_data.reshape(kaggle_test_data.shape[0], 28, 28, 1).astype(\"float32\")\n\nkaggle_test_predictions = model.predict(kaggle_test_data, batch_size = 128)\n\nkaggle_test_submission = pd.DataFrame(kaggle_test_predictions)\n\nkaggle_test_submission[\"Label\"] = pd.DataFrame(kaggle_test_submission.idxmax(axis = 1))\nkaggle_test_submission[\"ImageId\"] = kaggle_test_submission.index + 1\n\nkaggle_test_submission = kaggle_test_submission[[\"ImageId\", \"Label\"]]\n\nkaggle_test_submission.head()","82e5677c":"kaggle_test_submission.to_csv(\".\/submission.csv\", encoding = \"utf-8\", index = False)","1a425f85":"kaggle_test = pd.read_csv(\"submission.csv\")\nkaggle_test.head()","ded427ca":"# Calculate Score and Summary","5925f40c":"# Define Model","053edd1f":"# Build Model and Fit Data","39fd175f":"# Generate Predictions on Test Data","a92349e7":"# Generate a CSV File","3f8f912b":"# Introduction\nThis notebook is meant to demonstrate an implementation of the LeNet neural network architecture as written by Adrian Rosebrock in his book **Deep Learning for Computer Vision with Python**.\n\nThe LeNet architecture is a small and simple feed-forward convolutional neural network excellent for a first introduction to deep learning.\n\nHere, we will implement the LeNet architecture and train it on the Kaggle-Specific MNIST dataset to make predictions on the test data and explain each step on the way.","d6224688":"# Check if CSV File is Valid","17db62cf":"**The parameters we use for the train_test_split function are as follows:**\n\n*(np_pixels \/ 255.0)* This is our numpy array of pixel data divided by 255.0, the reason for dividing by 255.0 is to normalize our data between [0-1] to make training easier.\n\n*(np_labels)* This is our numpy array of label data.\n\n*(test_size = 0.2)* This represents how we intend to split our data. With a value of 0.2, 20% of our data will be allocated into our test set, and the remaining 80% will be used for training.\n\n*(random_state = 777)* This is set to 777 to ensure that every time we run our code, the dataset is split in exactly the same way. Any value can be used here as long as it isn't changed during evaluation.","6d47fc40":"# Import Packages\nFirst, we must import the packages necessary for building, training and evaluating our model.\n\n***numpy*** and ***pandas*** are imported to access *numpy arrays*, *pandas DataFrames* and other functions we'll be using to manage our data.\n\nFrom ***keras.models*** we import ***Sequential***, a model type which is a stack of layers with one input and one output.\n\nFrom ***keras.layers.convolutional*** we import ***Conv2D*** and ***MaxPooling2D***, which are the *Convolutional* layers and *Pooling* layers we will be using, respectively.\n\nFrom ***keras.layers.core*** we import ***Activation***, ***Flatten*** and ***Dense***, which are our *Activation* functions, *Flatten* method and *Dense* layers that we need to build our model.\n\nFrom ***keras.optimizers*** we import ***SGD*** which stands for *Stochastic Gradient Descent*, the optimization method we'll be using for our model.\n\nFrom ***sklearn.model_selection*** we import ***train_test_split***, the function we'll be using to split our dataset into a training set and test set.\n\nFrom ***sklearn.preprocessing*** we import ***LabelBinarizer***, the function we'll be using to encode our data labels to a format suitable for fitting the model.","84062c7d":"# Import Data","99cc138e":"Using the ***train_test_split*** function import from the ***sklearn*** library, we split the numpy arrays containing our pixel data and labels into a set for training and a set for testing our model.","534b5136":"# Preprocess Data","a4b9de5c":"First, we'll use pandas to import our dataset, which is given in the form of a CSV (Comma Seperated Values) file and convert it into a pandas DataFrame.\n\nThen, we split our data into the labels (which consist of the entire first column) and the numerical intensities of each pixel (which consist of all other columns).\n\nFinally, we convert the data we split into numpy arrays and print their shape to explore their dimensions."}}