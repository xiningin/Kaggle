{"cell_type":{"01a6875c":"code","9c4b2fb2":"code","e208a839":"code","e118c11d":"code","2c2827db":"code","a615a2e2":"code","5705e792":"code","a4d1e453":"code","cb219b80":"code","e30265d4":"code","594d9ce7":"code","ac11cba4":"code","43588d2c":"code","2a205723":"code","76663646":"code","f0259322":"code","84b4edf5":"code","26148b34":"code","e5a92a92":"code","befdd51a":"code","43bea669":"code","ded40c12":"code","b5078a9a":"code","39b23867":"code","af0a7c26":"code","31aec507":"code","d14249ef":"markdown","d0eaed9b":"markdown","4c352a97":"markdown","19899d2c":"markdown","ff13799b":"markdown","09120907":"markdown","4e4a4963":"markdown","25359916":"markdown"},"source":{"01a6875c":"import riiideducation\nimport numpy as np \nimport pandas as pd \n\nfrom sklearn.metrics import roc_auc_score\nfrom  sklearn.tree import DecisionTreeClassifier\nfrom  sklearn.model_selection import train_test_split\n\nenv = riiideducation.make_env()","9c4b2fb2":"train_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', \n                      usecols=[1,2,3,4,5,7,8,9],nrows=50000000)\nlectures = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')\nquestions = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\nexample_test = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')\nexample_sample_submission = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_sample_submission.csv')","e208a839":"train_df = train_df[train_df['content_type_id'] == 0]\n#keeping just the questions \n\ntrain_df= train_df.drop(columns=['content_type_id'])\n#dropping the column content_type_id and the answer of the users column","e118c11d":"results_c = train_df[['content_id','answered_correctly']].groupby(['content_id']).agg(['mean'])\nresults_c.columns = [\"answered_correctly_content\"]\n\nresults_u = train_df[['user_id','answered_correctly']].groupby(['user_id']).agg(['mean', 'sum', 'count'])\nresults_u.columns = ['answered_correctly_user', 'sum', 'count']","2c2827db":"train_df =train_df.sort_values(['timestamp'], ascending=True)\ntrain_df = train_df.iloc[10000000:,:]\n# sort the dataset by timestamp and than take only the last N observation. In this way all the values with timestamp = 0 are removed, and the db is \n# more easy to treat    ","a615a2e2":"train_df = pd.merge(train_df, results_u, on=['user_id'], how=\"left\")\ntrain_df = pd.merge(train_df, results_c, on=['content_id'], how=\"left\")","5705e792":"questions = questions.rename(columns={'question_id':'content_id'})\ntrain_df = train_df.merge(questions)\ntrain_df= train_df.drop(columns=['correct_answer'])\ntrain_df= train_df.drop(columns=['bundle_id'])\n#merging together the 2 db and dropping the column correct_answer and bundle_id","a4d1e453":"train_df['task_container_id'] = (\n    train_df\n    .groupby('user_id')['task_container_id']\n    .transform(lambda x : pd.factorize(x)[0])\n    .astype('int16')\n)\n#this is a function that assure the monotonicity of task container id","cb219b80":"from sklearn.preprocessing import LabelEncoder\nlb_make = LabelEncoder()\ntrain_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(train_df[\"prior_question_had_explanation\"])\ntrain_df = train_df.drop(columns=['prior_question_had_explanation'])\n#this is just for encoding 0-1 the variable prior question had explanation","e30265d4":"train_df = train_df.sort_values(by=['user_id'])\n# sorting by user_id","594d9ce7":"train_df=train_df.drop(columns=['part'])\ntrain_df=train_df.drop(columns=['tags'])\n\n#I remove part and tag, I put these command here because in a second moment I would like to integrete these 2 variables, it could be useful.","ac11cba4":"from datetime import datetime\ntrain_df['timestamp'] = pd.to_datetime(train_df['timestamp'], unit='ms',origin='2017-1-1')\ntrain_df['month']=(train_df.timestamp.dt.month)\ntrain_df['day']=(train_df.timestamp.dt.day)\n#i trasform timestamp in date format, then I extrapolte month and day to generate 2 columns\n\naveg = train_df[['user_id','month','day','prior_question_elapsed_time']].groupby(['user_id','month','day']).mean()\/1000\naveg.columns=['mean']\n#with the 2 columns generated before it is now possible \n#to calculate the average elapsed time for each user for each month for each day. \n\ntrain_df = pd.merge(train_df, aveg, on=['user_id','month','day'], how='left')\n# merge the 2 db","43588d2c":"y = train_df[[\"answered_correctly\"]]\n# extrapolate the dependent variable ","2a205723":"train_df.isnull().sum(axis = 0)\n#checking for any missing value","76663646":"keep = ['prior_question_had_explanation_enc',\n        'mean', \n        'answered_correctly_user',\n        'sum', \n        'count',\n        'answered_correctly_content']\nx=train_df[keep]","f0259322":"x.head(20)","84b4edf5":"Xt, Xv, Yt, Yv = train_test_split(x, y, test_size =0.2, shuffle=False)\n# split train in train and validation \n\nimport lightgbm as lgb\n\n'''\nhttps:\/\/lightgbm.readthedocs.io\/en\/latest\/Parameters.html\n'''\n\nparams = {\n    'objective': 'binary', #specify how is the dependet variable, binary can be used for logistic regression or log loss classification\n    'max_bin': 600, #max number of bins that features values will be bucketed in. Small number may reduce training accuracy but may increase general power\n    'learning_rate': 0.02, #learning_rate refers to the step size at each interation while moving toward an optimal point\n    'num_leaves': 80 # maximum number of leaves in a tree, where a leave is a final termination of a tree\n}\n\n\nlgb_train = lgb.Dataset(Xt, Yt)\nlgb_eval = lgb.Dataset(Xv, Yv, reference=lgb_train)\n#lightgbm need to take as argument lightgbm dataset, it is required to make this trasformation\n\nmodel = lgb.train(\n    params, lgb_train, #it is required to insert the parameters, then the train set\n    valid_sets=[lgb_train, lgb_eval],\n    verbose_eval=10,\n    num_boost_round=1000, # number of boosting iterations \n    early_stopping_rounds=10 # will stop training if one metric of one validation data doesn\u2019t improve in last early_stopping_round rounds, so if \n    #  for ten 'epochs' the model will stop, in this way the num_boost_round is a maximum value.  \n)  ","26148b34":"y_pred = model.predict(Xv)\ny_true = np.array(Yv)\nroc_auc_score(y_true, y_pred)","e5a92a92":"example_test = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')\nexample_test[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(example_test[\"prior_question_had_explanation\"])\nexample_test.head(10)","befdd51a":"example_test['timestamp'] = pd.to_datetime(example_test['timestamp'], unit='ms',origin='2017-1-1')\nexample_test['month']=(example_test.timestamp.dt.month)\nexample_test['day']=(example_test.timestamp.dt.day)\naveg = example_test[['user_id','month','day','prior_question_elapsed_time']].groupby(['user_id','month','day']).mean()\/1000\naveg.columns=['mean']\n\nexample_test = pd.merge(example_test, results_u, on=['user_id'], how=\"left\")\nexample_test = pd.merge(example_test, results_c, on=['content_id'], how=\"left\")\nexample_test = pd.merge(example_test, aveg, on=['user_id','month','day'], how='left')\nexample_test = example_test[keep]\nexample_test.head(10)\n","43bea669":"'''\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nimp = IterativeImputer(max_iter=10, random_state=0)\nimp.fit(example_test)\nexample_test = pd.DataFrame(imp.transform(example_test), columns = example_test.columns)\n'''","ded40c12":"example_test['answered_correctly_user'].fillna(example_test['answered_correctly_user'].mean(), inplace=True)\nexample_test['sum'].fillna(example_test['sum'].mean(), inplace=True)\nexample_test['count'].fillna(example_test['count'].mean(), inplace=True)","b5078a9a":"example_test.head(10)","39b23867":"y_pred = model.predict(example_test[keep])\nexample_test['answered_correctly'] = y_pred","af0a7c26":"'''\n    imp = IterativeImputer(max_iter=10, random_state=0)\n    imp.fit(test_df)\n    test_df = pd.DataFrame(imp.transform(test_df), columns = test_df.columns)\n'''\n\niter_test = env.iter_test()\nfor (test_df, sample_prediction_df) in iter_test:\n    test_df['prior_question_had_explanation'].fillna(False, inplace=True)\n    test_df[\"prior_question_had_explanation_enc\"] = lb_make.fit_transform(test_df[\"prior_question_had_explanation\"])\n    \n    test_df['timestamp'] = pd.to_datetime(test_df['timestamp'], unit='ms',origin='2017-1-1')\n    test_df['month']=(test_df.timestamp.dt.month)\n    test_df['day']=(test_df.timestamp.dt.day)\n    avegm = test_df[['user_id','month','day','prior_question_elapsed_time']].groupby(['user_id','month','day']).mean()\/1000\n    avegm.columns=['mean']\n    \n    test_df = pd.merge(test_df, results_u, on=['user_id'],  how=\"left\")\n    test_df = pd.merge(test_df, results_c, on=['content_id'],  how=\"left\")\n    test_df = pd.merge(test_df, avegm, on=['user_id','month','day'], how='left')\n    \n    test_df['answered_correctly_user'].fillna(test_df['answered_correctly_user'].mean(), inplace=True)\n    test_df['sum'].fillna(test_df['sum'].mean(), inplace=True)\n    test_df['count'].fillna(test_df['count'].mean(), inplace=True)\n    \n    test_df['answered_correctly'] =  model.predict(test_df[keep])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","31aec507":"print('finish')","d14249ef":"It's now possible to submit the model prediction using the method explained by the host of the competition.","d0eaed9b":"### SUBMIT MODEL PREDICTIONS ","4c352a97":"#### ENCODING VARIABLE \"PRIOR_QUESTION_HAD_EXPLANATION\"","19899d2c":"#### MERGING TRAIN DATASET WITH QUESTION DATASET","ff13799b":"Before submitting of the real test sets, let's try out if evrything it's working on the example test. Let's repeat all the operation computed before.","09120907":"#### SELECTS ONLY QUESTIONS ","4e4a4963":"### CHECK ON EXAMPLE TEST","25359916":"## PREPROCESSING"}}