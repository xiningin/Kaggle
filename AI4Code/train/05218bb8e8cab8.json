{"cell_type":{"8e9fdb3a":"code","eab706c9":"code","253cfb0d":"code","feb7d7ee":"code","84e92fb0":"code","4e39fe86":"code","5f5442c9":"code","5cb9e750":"code","003a9ff8":"code","70c6d0d7":"code","a64d6a64":"code","8ff48d4f":"code","45b1fb57":"code","7b26ea73":"code","f307e7f2":"code","f9ba14d0":"code","14f2824d":"code","eb0a4064":"code","fc503bfb":"code","645cacb2":"code","9c148f72":"code","0da6ff23":"code","78cb0ef3":"code","775523c8":"code","666c2258":"code","e8da46f0":"code","3e88c91e":"code","e2907135":"code","b34dbeeb":"code","cd6c1252":"code","97b468ea":"code","970106af":"code","466e4ff8":"code","7a225da6":"code","c2cc050d":"code","593b5f43":"code","ba2f991b":"code","258ed686":"code","1cc3f34d":"code","d8231468":"code","7d7a3623":"code","84878665":"code","43b1f427":"code","4bd059be":"code","92044b5e":"code","c8319ea4":"code","6f6f6efd":"markdown","78aa8944":"markdown","9bf2d4d3":"markdown","8cf1aae5":"markdown","f8d05441":"markdown","64982cb5":"markdown","e37cdd12":"markdown","dc5c2d9d":"markdown","55e675bc":"markdown","dc8ffc33":"markdown","a48852fe":"markdown","4aea2e2b":"markdown","5332f9e8":"markdown","632d95fa":"markdown"},"source":{"8e9fdb3a":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf, tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Dense,BatchNormalization,Dropout\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import optimizers\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.optimizers import Adam\nimport seaborn as sns\nfrom sklearn.utils.class_weight import compute_class_weight\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA\nimport time\nfrom sklearn.cluster import DBSCAN\nfrom sklearn import metrics\nfrom sklearn.datasets import make_blobs\nfrom sklearn.preprocessing import StandardScaler","eab706c9":"#TPU preparation\n\nAUTO = tf.data.experimental.AUTOTUNE\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy()\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","253cfb0d":"#paths:\nADDITIONAL_DATA_NO_BG_IMAGES_PATH = 'plant-pathology-more-data-no-background' \nADDITIONAL_DATA_IMAGES_PATH = 'plant-pathology-2020-preprocessed-images'\nORIGINAL_DATA_IMAGES_PATH = 'plant-pathology-2020-fgvc7'\n\nCSV_TRAIN_PATH = '..\/input\/plant-pathology-more-data-no-background\/train.csv'\nCSV_TEST_PATH =  '..\/input\/plant-pathology-more-data-no-background\/test.csv'\n\nUSE_ADDITIONAL_DATA = False","feb7d7ee":"#important constants:\nIMG_SIZE = 784\nBATCH_SIZE = 8*strategy.num_replicas_in_sync\nnb_classes = 4\nCATEGORY_NAMES = ['healthy','multiple_diseases','rust','scab']","84e92fb0":"def get_train_data(path = ORIGINAL_DATA_IMAGES_PATH):\n    train = pd.read_csv(CSV_TRAIN_PATH)\n    train_id = train.pop('image_id')\n        \n    y_train = train.to_numpy().astype('float32')\n    \n    if not USE_ADDITIONAL_DATA:\n        train_id = [idee for idee in train_id if idee[0]=='T']\n        y_train = y_train[:len(train_id)]\n    \n    GCS_DS_PATH = KaggleDatasets().get_gcs_path(path)\n    \n    root_img = 'images'\n    if path == ADDITIONAL_DATA_NO_BG_IMAGES_PATH:\n        root_img = 'images\/images'\n    if path == ADDITIONAL_DATA_IMAGES_PATH:\n        root_img = 'plant_processed\/images'\n    \n    images_paths = [os.path.join(GCS_DS_PATH,root_img,idee+'.jpg') for idee in train_id]\n    \n    return images_paths,y_train\n\n\n\ndef train_val_split(x_train,y_train, test_size = 0.2):\n    x_train = np.array(x_train)\n    y_train = np.array(y_train)\n    sss = StratifiedShuffleSplit(n_splits=1, test_size = test_size, random_state=0)\n    for train_index, test_index in sss.split(x_train, y_train):\n        x_train, x_test = x_train[train_index], x_train[test_index]\n        y_train, y_test = y_train[train_index], y_train[test_index]\n    return x_train, y_train, x_test, y_test\n\n\ndef read_train_image(path, label = None):\n    bits = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, (IMG_SIZE,IMG_SIZE))\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n    \n    \n    \ndef augment_train_image(image, label = None, seed=1024):\n    image = tf.image.random_flip_left_right(image, seed=seed)\n    image = tf.image.random_flip_up_down(image, seed=seed)\n    \n    if label is None:\n        return image\n    else:\n        return image, label\n\n\n\ndef get_train_val_datasets(x_train,y_train,x_val = None ,y_val = None):\n    train_dataset = (tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .map(read_train_image, num_parallel_calls=AUTO)\n    .map(augment_train_image, num_parallel_calls=AUTO)\n    .repeat()\n    .shuffle(512)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n    )\n       \n    if x_val is None:\n        return train_dataset\n    \n    val_dataset = (tf.data.Dataset\n               .from_tensor_slices((x_val,y_val))                \n               .map(read_train_image,num_parallel_calls=AUTO)\n               .batch(BATCH_SIZE)\n               .cache()\n               .prefetch(AUTO)\n              )\n   \n    return train_dataset, val_dataset","4e39fe86":"def get_class_weights(y):\n    class_weights = compute_class_weight('balanced',np.unique(y.argmax(axis=1)),y.argmax(axis=1))\n    print('class weights: ',class_weights)\n    return class_weights\n\ndef plot_class_weights(class_weights,title = 'Categories distribution'):\n    plt.bar(range(4),1 \/ class_weights,color=['springgreen', 'lightcoral', 'mediumpurple', 'gold'],width=0.9)\n    plt.xticks(range(4), CATEGORY_NAMES) \n    plt.title(title);\n    plt.ylabel('Probability')\n    plt.xlabel('Data')\n    plt.show()","5f5442c9":"#helper functions to show image or images\n\ndef show_image(filename, image_height = 10, image_width = 10):\n    fig=plt.figure(figsize=(8, 8))\n    img = read_train_image(filename)\n    fig.add_subplot(1, 1, 1)\n    plt.imshow(img)\n    plt.show()\n\n\ndef show_images(images, height = 20, width = 20, images_in_one_row = 5):\n    \n    images = np.array(images)\n    size = images.size\n    \n    width = (size+images_in_one_row-1)\/\/images_in_one_row*images_in_one_row\n    rows = (size + images_in_one_row - 1) \/\/ images_in_one_row\n    fig=plt.figure(figsize=(height, width))\n    \n    for i in range(size):\n        cur_image = read_train_image(images[i])   \n        fig.add_subplot(rows, images_in_one_row, i + 1)\n        plt.imshow(cur_image)\n    plt.show()","5cb9e750":"#installing effnet\n\n!pip install efficientnet\nimport efficientnet.tfkeras as efn","003a9ff8":"#custom metrics\n\ndef f1(y_true, y_pred):\n    y_pred = K.round(y_pred)\n    tp = K.sum(K.cast(y_true*y_pred, 'float32'), axis=0)\n    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n    fp = K.sum(K.cast((1-y_true)*y_pred, 'float32'), axis=0)\n    fn = K.sum(K.cast(y_true*(1-y_pred), 'float32'), axis=0)\n \n    p = tp \/ (tp + fp + K.epsilon())\n    r = tp \/ (tp + fn + K.epsilon())\n \n    f1 = 2*p*r \/ (p+r+K.epsilon())\n    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n    return K.mean(f1)","70c6d0d7":"MODEL_CFG = {\n    'optimizer': Adam(lr=0.0001),\n    'loss': 'categorical_crossentropy',\n    'metrics': ['accuracy',tf.keras.metrics.AUC(),f1],\n    'name':'effNetB7.h5'\n}\n\nCL_BEST_MODEL= ModelCheckpoint(MODEL_CFG['name'], \n                                 monitor='val_loss', \n                                 verbose=1, \n                                 save_best_only=True,\n                                 save_weights_only=True,\n                                 mode='min')\n    \nCL_REDUCE_LR = ReduceLROnPlateau(monitor='val_loss',\n                                  factor=0.5,\n                                  verbose=0,\n                                  epsilon = 1e-4,\n                                  min_lr = 1e-5,\n                                  patience=10)\n\nCL_EARLY_STOPPING = EarlyStopping(monitor = \"val_loss\" , verbose = 1 , mode = 'min' , patience = 50)\nCALLBACKS = [CL_BEST_MODEL,CL_REDUCE_LR,CL_EARLY_STOPPING]","a64d6a64":"def get_model(): \n    with strategy.scope():\n        base_model = efn.EfficientNetB7(weights='imagenet',\n                              include_top=False,\n                              input_shape=(IMG_SIZE,IMG_SIZE, 3),\n                              pooling='avg')\n        \n        x = base_model.output\n        '''\n        x = Dropout(0.2)(x)\n        x = BatchNormalization()(x)\n        x = Dense(512,activation = 'relu')(x)\n        x = Dense(256,activation='relu')(x)\n        '''\n        \n        predictions = Dense(4,activation='softmax')(x)\n\n        model = Model(inputs=base_model.input, outputs=predictions)\n\n        model.compile(\n            optimizer = MODEL_CFG['optimizer'],\n            loss = MODEL_CFG['loss'],\n            metrics = MODEL_CFG['metrics'],\n        )\n        \n        return model\n    ","8ff48d4f":"def vizualize_history(history):\n    \n    #plot accuracy\n    plt.rcParams[\"figure.figsize\"] = (8,8)\n    plt.plot(history.history[\"accuracy\"])\n    plt.plot(history.history['val_accuracy'])\n    plt.title(\"model accuracy\")\n    plt.ylabel(\"Accuracy\")\n    plt.xlabel(\"Epoch\")\n    plt.legend([\"Accuracy\",\"Val. Accuracy\"])\n    plt.show()\n    \n    #plot loss\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title(\"model loss\")\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Epoch\")\n    plt.legend([\"Loss\",\"Validation Loss\"])\n    plt.show()\n    \n\ndef train_model(model,\n                train_dataset,\n                val_dataset = None,\n                epochs = 10,\n                steps = 22,\n                class_weights = None\n               ):\n\n    history = model.fit(train_dataset,\n                    steps_per_epoch=steps,\n                    epochs=epochs,\n                    verbose=1,\n                    validation_data=val_dataset,\n                    callbacks=CALLBACKS,\n                    class_weight = class_weights\n                       )\n    \n    return history","45b1fb57":"def read_test_image(path):\n    bits = tf.io.read_file(path)\n    image = tf.image.decode_jpeg(bits, channels=3)\n    image = tf.cast(image, tf.float32) \/ 255.0\n    image = tf.image.resize(image, (IMG_SIZE,IMG_SIZE))\n    \n    return image\n\ndef augment_test_image(image):\n    image = tf.image.random_flip_left_right(image)\n    image = tf.image.random_flip_up_down(image)\n           \n    return image","7b26ea73":"def get_test_data(path):\n\n    test = pd.read_csv(CSV_TEST_PATH)\n    test_id = test['image_id']\n\n    GCS_DS_PATH = KaggleDatasets().get_gcs_path(path)\n    \n    root_img = 'images'\n    if path == ADDITIONAL_DATA_NO_BG_IMAGES_PATH:\n        root_img = 'images\/images'\n    if path == ADDITIONAL_DATA_IMAGES_PATH:\n        root_img = 'plant_processed\/images'\n    \n    x_test = [(os.path.join(GCS_DS_PATH,root_img,idee+'.jpg')) for idee in test_id]\n    \n    return x_test\n\ndef get_test_dataset(x_test):\n    test_dataset = (\n        tf.data.Dataset\n        .from_tensor_slices(x_test)\n        .map(read_test_image, num_parallel_calls=AUTO)\n        .batch(BATCH_SIZE)\n        )\n    return test_dataset","f307e7f2":"def get_augment_predict(model,test_dataset,iterations=5):\n    y_pred = model.predict(test_dataset.map(augment_test_image,num_parallel_calls=AUTO),verbose=1)\n    return y_pred\n\ndef get_predict(model,test_dataset,iterations = 5,USE_TTA = True):\n    \n    y_predictions = []\n    \n    if USE_TTA:\n        for i in range(iterations):\n            y_predictions.append(get_augment_predict(model,test_dataset,iterations))\n            print(y_predictions[i])\n    else:\n        y_predictions.append(model.predict(test_dataset,verbose=1))\n        print(y_predictions)\n    \n    return np.mean(y_predictions,axis=0)","f9ba14d0":"#function to save results to submission.csv\ndef save_results(y_pred):\n    \n    test = pd.read_csv(CSV_TEST_PATH)\n    test_id = test['image_id']\n\n    result = pd.read_csv(CSV_TRAIN_PATH)[:y_pred.shape[0]]\n    result['image_id'] = test_id\n    \n    labels = result.keys()\n    print(labels)\n    for i in range(1,5):\n        result[labels[i]] = y_pred[:,i-1]\n\n    result.to_csv('submission.csv',index=False)\n    print(result.head)","14f2824d":" def compute_confusion_matrix(true, pred):\n    result = np.zeros((nb_classes,nb_classes))\n    \n    true = true.argmax(axis=1)\n    pred = pred.argmax(axis=1)\n    \n    for i in range(len(true)):\n        result[true[i]][pred[i]] += 1\n        \n    return result.astype('uint16')\n\n\ndef plot_confusion_matrix(matrix, labels_x=CATEGORY_NAMES, labels_y=CATEGORY_NAMES):\n    \n    # vertical axis = true, horizontal axis = pred\n    \n    plt.title(\"Confusion Matrix\")\n    ax = sns.heatmap(matrix, annot=True, fmt='d', xticklabels = labels_x, yticklabels = labels_y)\n    ax.set(ylabel=\"True Label\", xlabel=\"Predicted Label\")\n    ","eb0a4064":"def get_train_emb(x_train,y_train):\n    train_emb = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, y_train))\n    .map(read_train_image, num_parallel_calls=AUTO)\n    .map(augment_train_image, num_parallel_calls=AUTO)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n    )\n    \n    return train_emb\n\ndef get_val_emb(x_val,y_val):\n    val_emb = (tf.data.Dataset\n               .from_tensor_slices((x_val,y_val))                \n               .map(read_train_image,num_parallel_calls=AUTO)\n               .batch(BATCH_SIZE)\n               .cache()\n               .prefetch(AUTO)\n              )\n        \n    return val_emb\n\n#def get_intermediate_layer_output(model,dataset,layer_name= -2):\n    #intermediate_layer_model = Model(inputs=model.input,\n                                     #outputs=model.layers[layer_name].output)\n    \n    #intermediate_output = intermediate_layer_model.predict(dataset,verbose=1)\n    \n    #return intermediate_output\n\n\n\ndef get_intermediate_layer_output(model,dataset,layer_name= 'stem_bn'):\n    intermediate_layer_model = Model(inputs=model.input,\n                                     outputs=model.get_layer(layer_name).output)\n    \n    intermediate_output = intermediate_layer_model.predict(dataset,verbose=1)\n    \n    return intermediate_output\n","fc503bfb":"def fit_PCA(x):\n    pca_50 = PCA(n_components=50)\n    pca = pca_50.fit(x)\n    return pca\n\ndef transform_PCA(pca,x):\n    return pca.transform(x)\n\ndef tsne_fit_transform(x):\n    tsne = TSNE(n_components=2, verbose=1,n_iter=100000)\n    tsne_pca_results = tsne.fit_transform(x)\n    return tsne_pca_results\n    \ndef vizualize_latent_space(x,y):\n    sns.set(rc={'figure.figsize':(11.7,8.27)})\n    palette = sns.color_palette(\"bright\", 4)\n    plt.title('latent space vizualization')\n    y_hue = [CATEGORY_NAMES[i] for i in y.argmax(axis=1)]\n    sns.scatterplot(x[:,0], x[:,1],hue=y_hue, legend='full', palette=palette)","645cacb2":"def dbscan(X,eps=10):\n    db = DBSCAN(eps = eps).fit(X)\n    return db\n\ndef vizualize_clusters(db,X):\n    \n    core_samples_mask = np.zeros_like(db.labels_, dtype=bool)\n    core_samples_mask[db.core_sample_indices_] = True\n    labels = db.labels_\n\n    n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n    n_noise_ = list(labels).count(-1)\n\n    print('Estimated number of clusters: %d' % n_clusters_)\n    print('Estimated number of noise points: %d' % n_noise_)\n\n    unique_labels = set(labels)\n    colors = [plt.cm.Spectral(each)\n              for each in np.linspace(0, 1, len(unique_labels))]\n    for k, col in zip(unique_labels, colors):\n        if k == -1:\n            # Black used for noise.\n            col = [0, 0, 0, 1]\n\n        class_member_mask = (labels == k)\n\n        xy = X[class_member_mask & core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                 markeredgecolor='k', markersize=5)\n\n        xy = X[class_member_mask & ~core_samples_mask]\n        plt.plot(xy[:, 0], xy[:, 1], 'o', markerfacecolor=tuple(col),\n                 markeredgecolor='k', markersize=5)\n\n    plt.title('Estimated number of clusters: %d' % n_clusters_)\n    plt.show()\n    \n    \ndef get_clusters(db,y):\n    num_clusters = db.labels_.max()+1\n    clusters = [[] for i in range(num_clusters)]\n    labels = db.labels_\n\n    noise = []\n    for i in range(len(labels)):\n        if labels[i]==-1:\n            noise.append(i)\n        else:\n            clusters[labels[i]].append(i)\n\n    cluster_labels = num_clusters*[0]\n\n    for cluster,ptr in zip(clusters,range(num_clusters)):\n        cnt = np.array(num_clusters*[0])\n        for idx in cluster:\n            cnt[y[idx].argmax()]+=1\n\n        cluster_labels[ptr] = cnt.argmax()\n        \n    return clusters,cluster_labels,noise\n\n\ndef get_incorrectly_predicted_data(clusters,cluster_labels,y,noise = []):\n    incorrect_data = []\n    for idx in noise:\n        incorrect_data.append(idx)\n    \n    for cluster,ptr in zip(clusters,range(len(clusters))):\n        for idx in cluster:\n            if cluster_labels[ptr]!=y[idx].argmax():\n                incorrect_data.append(idx)\n    return np.array(incorrect_data)","9c148f72":"x_train,y_train = get_train_data()\nx_train,y_train,x_val,y_val = train_val_split(x_train,y_train)\ntrain_dataset,val_dataset = get_train_val_datasets(x_train,y_train,x_val,y_val)","0da6ff23":"c_w = get_class_weights(y_train)\nplot_class_weights(c_w,title = 'Train distribution')\nc_w_train = dict(zip(range(4),c_w))","78cb0ef3":"plot_class_weights(get_class_weights(y_val),title = 'Validation distribution')","775523c8":"model = get_model()","666c2258":"model.summary()","e8da46f0":"hist = train_model(model,train_dataset,val_dataset,epochs=10)","3e88c91e":"vizualize_history(hist)","e2907135":"x_test = get_test_data(ORIGINAL_DATA_IMAGES_PATH)\ntest_dataset = get_test_dataset(x_test)","b34dbeeb":"#model.load_weights(MODEL_CFG['name'])\n#model.load_weights('..\/input\/model-effnet\/effNetPlants_val_loss_0.099.h5')","cd6c1252":"y_pred = get_predict(model,test_dataset)","97b468ea":"save_results(y_pred)","970106af":"val_pred = get_predict(model,val_dataset,USE_TTA = False)\nval_conf_matrix = compute_confusion_matrix(y_val,val_pred)\nplot_confusion_matrix(val_conf_matrix)","466e4ff8":"x_emb,y_emb = get_train_data()\ntrain_emb = get_train_emb(x_emb,y_emb)\nx = get_intermediate_layer_output(model,train_emb,layer_name = 'stem_bn')","7a225da6":"pca = fit_PCA(x)\npca_result = transform_PCA(pca,x)\npca_tsne_result = tsne_fit_transform(pca_result)\n\nvizualize_latent_space(pca_tsne_result,y_emb)","c2cc050d":"val_emb = get_val_emb(x_val,y_val)\nx = get_intermediate_layer_output(model,train_emb,layer_name = -2)","593b5f43":"pca_result = transform_PCA(pca,x)\npca_tsne_result = tsne_fit_transform(pca_result)\n\nvizualize_latent_space(pca_tsne_result,y_train)","ba2f991b":"db = dbscan(pca_tsne_result,eps = 10)","258ed686":"vizualize_clusters(db,pca_tsne_result)","1cc3f34d":"clusters,cluster_labels,noise = get_clusters(db,y_emb)\nincorrect_predicts = get_incorrectly_predicted_data(clusters,cluster_labels,y_emb,noise)","d8231468":"x_emb = np.array(x_emb)\nx_incorrect,y_incorrect = x_emb[incorrect_predicts],y_emb[incorrect_predicts]\nincorrect_train_dataset = get_train_val_datasets(x_incorrect,y_incorrect)","7d7a3623":"hist = train_model(model,incorrect_train_dataset,epochs=7)","84878665":"x_emb,y_emb = get_train_data()\ntrain_emb = get_train_emb(x_emb,y_emb)\nx = get_intermediate_layer_output(model,train_emb,layer_name = -2)","43b1f427":"pca_result = transform_PCA(pca,x)\npca_tsne_result = tsne_fit_transform(pca_result)\n\nvizualize_latent_space(pca_tsne_result,y_emb)","4bd059be":"x_test = get_test_data(ORIGINAL_DATA_IMAGES_PATH)\ntest_dataset = get_test_dataset(x_test)","92044b5e":"y_pred = get_predict(model,test_dataset)","c8319ea4":"save_results(y_pred)","6f6f6efd":"### Train model","78aa8944":"### Confusion matrix for validation dataset","9bf2d4d3":"# Your code starts HERE!!!","8cf1aae5":"### Train model using incorrectly predicted data","f8d05441":"### Clustering","64982cb5":"### Latent space vizualization for full trainset","e37cdd12":"### Getting incorrectly predicted data for learning","dc5c2d9d":"### Class weights","55e675bc":"### Latent space vizualization for valset","dc8ffc33":"### Get predictions","a48852fe":"### Preparing model","4aea2e2b":"### Vizualize history","5332f9e8":"### Prepare test dataset","632d95fa":"### Saving results"}}