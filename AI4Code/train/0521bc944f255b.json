{"cell_type":{"ab8af399":"code","77d8e49a":"code","bc3131ed":"code","3d747199":"code","0d426eb3":"code","6c91af7e":"code","b64c8b9b":"code","a1f04043":"code","f3ed9f82":"code","d15664ae":"code","8219ee95":"code","ddb5b4af":"code","888c342b":"code","167ab543":"code","a4407bd7":"code","a0578bf9":"code","a5b6adf7":"code","b4d1fb6b":"code","2d6fe858":"code","03a8ff06":"code","348a4d80":"code","a9f22943":"code","e31f02b9":"code","12a78391":"code","db25c1d4":"markdown","0a4b23b3":"markdown","f8273320":"markdown","70fc5eae":"markdown","744ef476":"markdown","1ecc19a4":"markdown","923aa499":"markdown","fa2b1764":"markdown","6fb972c7":"markdown","a1298ba3":"markdown","c8cb6d53":"markdown"},"source":{"ab8af399":"#import essential libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud","77d8e49a":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bc3131ed":"train = pd.read_csv(\"..\/input\/twitter-sentiment-anaylsis\/train_2kmZucJ.csv\")\ntest = pd.read_csv(\"..\/input\/twitter-sentiment-anaylsis\/test_oJQbWVk.csv\")","3d747199":"train.head()","0d426eb3":"train.tweet[:20]","6c91af7e":"badword='$&@*#'   #we initialize a variable which contain this profane term\ntt=train['tweet'].apply(lambda x:x.split()) #tokenize the tweet text","b64c8b9b":"tt[:5]","a1f04043":"z=[]   #create a list \nfor i in tt: #run through the loop \n    l=[]\n    for j in i:\n        if j.__contains__('$&@*#'):     #and compare each and every word to badword\n            j=j.replace(j,'bad')        #if found then replace that word with 'bad'\n        l.append(j)                     #append it into a list\n    z.append(l)                         #append the inner list to main list inorder to create list of list","f3ed9f82":"train['tweets']=z  #create a new column for the processed tweets","d15664ae":"print(train.tweet[26])\nprint(train.tweets[26])\n#we can see that the first term from tweet feature is replaced with bad in tweets feature","8219ee95":"l=[]\nfor i in train['tweets']:\n    l.append(\" \".join(i))\ntrain['tweet_r']=l  #create new feature in which we stitch back the tokenized tweets","ddb5b4af":"def process_tweets(train):\n    train['mail_tweets']=train['tweet_r'].str.replace('https?:\/\/[A-Za-z0-9.\/]+',' ')#replace all the url links\n    train['clean_tweets']=train['tweet_r'].str.replace('[^a-zA-Z]',' ')#replacing special characters with space\nprocess_tweets(train)","888c342b":"train.head() #our cleaned train data.","167ab543":"train.drop(columns=['tweet','tweets','id','tweet_r','mail_tweets'],inplace=True)\n#we will drop columns except 'clean_tweets' and 'label'","a4407bd7":"#Visualizing all the positive tweets using wordcloud plot.\npositive=' '.join([text for text in train['clean_tweets'][train['label']==0]])\nwordcloud=WordCloud(width=800,height=400,random_state=21,max_font_size=110).generate(positive)\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.title('positive tweets')\nplt.show()","a0578bf9":"#Visualizing all the negative tweets using wordcloud plot.\npositive=' '.join([text for text in train['clean_tweets'][train['label']==1]])\nwordcloud=WordCloud(width=800,height=400,random_state=21,max_font_size=110).generate(positive)\nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\") \nplt.axis('off') \nplt.title('negative tweets')\nplt.show()","a5b6adf7":"train['label'].value_counts().plot(kind='barh')","b4d1fb6b":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n\nbow_vectorizer = CountVectorizer(max_df=0.90, min_df=2,max_features=1000, stop_words='english') \nbow = bow_vectorizer.fit_transform(train['clean_tweets']) \nbow.shape","2d6fe858":"tdf_vectorize=TfidfVectorizer(max_df=0.90, min_df=2,max_features=1000)\ntfidf = tdf_vectorize.fit_transform(train['clean_tweets'])\ntfidf.shape","03a8ff06":"from sklearn.linear_model import LogisticRegression #import LinerRegression from sklearn\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.metrics import f1_score","348a4d80":"xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(bow,train['label'],random_state=42,test_size=0.3)","a9f22943":"xtrain_tdf, xvalid_tdf, ytrain, yvalid = train_test_split(tfidf, train['label'],random_state=42,test_size=0.3)","e31f02b9":"#using Bag of word features\nlr=LogisticRegression()\nlr.fit(xtrain_bow,ytrain)\nlr.score(xvalid_bow,yvalid)","12a78391":"#using TF-IDF features\nlr=LogisticRegression()\nlr.fit(xtrain_tdf,ytrain)\nlr.score(xvalid_tdf,yvalid)","db25c1d4":"# DATA PREPROCESSING","0a4b23b3":"Given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.\n\nlet us now view the twitter data","f8273320":"For this classification task the metric used for evaluating the performance of classification model would be weighted F1-Score.\n\nwe have to perform similar preprocessing and cleaning techinique for test data. You can combine the train and test data. \n\ntry out different models like random forest classifier,support vector machine and XGboost.\n\nAfter performing similar process for test data and predicting the values for test data. F1 score was *0.7661612130885873*.\n\nThank you!","70fc5eae":"# EXTRACT FEATURES\n\nTo analyse data we need to convert the data into features.We can construct features using\n* Bag of words\n* TF-IDF\n* Word2vec","744ef476":"we can observe that negative tweets have iphone in common and positive tweets have samsung in common.why? \ud83d\ude02","1ecc19a4":"This train data has 7920 rows and 3 columns and test data has 1953 rows and 2 columns(id and tweet)\n\ntrain data columns are :\n*     id\n*     label (target which we have to predict )\n*     tweet","923aa499":"here we observe that negative tweets are approx 25% of the total tweets.for better classification we can use SMOTE(Synthetic Minority Over-sampling Technique) for imbalanced data. here i will proceed without using the SMOTE techique.","fa2b1764":"# MODEL BUILDING\nwe will use logistic regression . we will first split our into train and validation set.","6fb972c7":"we can see that our data is unstructured and for better analysis we first need to structure these tweets, remove the unwanted words, replace the misspelled words with the correct ones, replace the abriviation with full words.\n\nMost profane and vulgar terms in the tweets have been replaced with \u201c$&@*#\u201d. Our task is to replace these terms with 'bad' .The reason why we cannot remove these terms is because these words are frequantly used and can help us determine negative sentiment correctly.\n\nlet us see how to replace these terms","a1298ba3":"# DATA ANALYSIS\n","c8cb6d53":"# DATA CLEANING\nwe observed that tweet data has links,punctuations and numbers. cleaning the raw data is important \n* clean URL links\n* remove twitter handle (@user)\n* remove special characters and number\n\nnote: we won't remove hashtags because it helps better prediction\n \nif we have text as 'MY LOVELY CARING FRIEND #NOTANYMORE #FOE #ENEMY'. As a human we know that this tweet is negative but how to make machine know? \n \n write your opinion in the comment :)"}}