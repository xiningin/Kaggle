{"cell_type":{"46fda856":"code","1e91e95d":"code","ee58cf28":"code","1e8c4fac":"code","6dcc334d":"code","25ba19a4":"code","e134803d":"code","3738b60d":"code","43ea01df":"code","da1d33ba":"code","73fe6b9f":"code","afcd84a5":"code","e4a8c6ea":"code","dae5fe7a":"code","11b31be9":"code","6b4d7f97":"code","97a8eed1":"code","a01aab0d":"code","ee9e622f":"code","24715ceb":"code","1f322444":"code","a249b231":"code","2929f9d1":"code","0f79466a":"code","b5ead9a7":"code","d3b88c93":"code","b5a5c8cf":"code","56d041c3":"code","28d6643d":"markdown","0af5d2e6":"markdown","3e5ed5a0":"markdown","576e06dc":"markdown","b8011294":"markdown","e7b6702a":"markdown","efff2003":"markdown","35d41e2e":"markdown","e843a481":"markdown","7ce1ed31":"markdown","83776a11":"markdown","3940917b":"markdown","d7b06843":"markdown"},"source":{"46fda856":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport unicodedata\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nnltk.download('vader_lexicon')\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom wordcloud import WordCloud","1e91e95d":"df_1 = pd.read_csv(\"\/kaggle\/input\/trump-tweets\/trumptweets.csv\")\ndf_2 = pd.read_csv(\"\/kaggle\/input\/trump-tweets\/realdonaldtrump.csv\")","ee58cf28":"print(df_1.shape)\nprint(df_2.shape)","1e8c4fac":"df_1.head(3)","6dcc334d":"df_2.head(3)","25ba19a4":"df_1 = df_1[[\"content\", \"date\", \"retweets\", \"favorites\", \"mentions\", \"hashtags\"]]\ndf_2 = df_2[[\"content\", \"date\", \"retweets\", \"favorites\", \"mentions\", \"hashtags\"]]\n#\ndf_dt = pd.concat([df_1, df_2], axis=0)\ndf_dt.shape","e134803d":"size_before = len(df_dt)\ndf_dt = df_dt.drop_duplicates(subset=[\"content\"])\nsize_after = len(df_dt)\nprint(str(size_before - size_after) + \" duplicates were removed.\")","3738b60d":"max_tweet_length = 0\ntweet_length = []\n#\nfor tweet in df_dt[\"content\"]:\n    tweet_length.append(len(tweet))\n    if len(tweet) > max_tweet_length:\n        max_tweet_length = len(tweet)\nprint(\"Longest tweet: \" + str(max_tweet_length) + \" characters\")","43ea01df":"100 * df_dt.isnull().sum().sort_values(ascending=False)\/len(df_dt)","da1d33ba":"df_dt.drop(columns=[\"hashtags\", \"mentions\"], inplace=True)","73fe6b9f":"100 * df_dt.isnull().sum().sort_values(ascending=False)\/len(df_dt)","afcd84a5":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 6)\nsns.histplot(df_dt[\"retweets\"], palette='Blues', stat='count', bins=50, ax=ax1);\nax1.set_xlabel('Retweets count');\nsns.histplot(df_dt[\"favorites\"], palette='Blues', stat='count', bins=50, ax=ax2);\nax2.set_xlabel('Favorites count');\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\nax1.set_xlim(-10, 50000)\nax2.tick_params(axis='x', labelsize=16)\nax2.tick_params(axis='y', labelsize=16)\nax2.set_ylabel(\"\")\nax2.set_xlim(-10, 200000)\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","e4a8c6ea":"df = df_dt.copy()\ndf[\"cleanTweet\"] = df[\"content\"]\nlabel = 'cleanTweet'","dae5fe7a":"# lowercase\ndf[label] = df[label].str.lower()\n\n# remove \\n \\r \\t\ndf[label] = df[label].apply(lambda x: x.replace(\"\\n\", \" \"))\ndf[label] = df[label].apply(lambda x: x.replace(\"\\r\", \" \"))\ndf[label] = df[label].apply(lambda x: x.replace(\"\\t\", \" \"))\n\n# remove emails\ndf[label] = df[label].apply(lambda x: re.sub(r\"\"\"(?:[a-z0-9!#$%&'*+\/=?^_`{|}~-]+(?:\\.[a-z0-9!#$%&'*+\/=?^_`{|}~-]+)*|\"(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21\\x23-\\x5b\\x5d-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])*\")@(?:(?:[a-z0-9](?:[a-z0-9-]*[a-z0-9])?\\.)+[a-z0-9](?:[a-z0-9-]*[a-z0-9])?|\\[(?:(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9]))\\.){3}(?:(2(5[0-5]|[0-4][0-9])|1[0-9][0-9]|[1-9]?[0-9])|[a-z0-9-]*[a-z0-9]:(?:[\\x01-\\x08\\x0b\\x0c\\x0e-\\x1f\\x21-\\x5a\\x53-\\x7f]|\\\\[\\x01-\\x09\\x0b\\x0c\\x0e-\\x7f])+)\\])\"\"\", \" \", x))\n\n# remove mentions\ndf[label] = df[label].apply(lambda x: x.replace(\"@ \", \"@\"))\ndf[label] = df[label].apply(lambda x: re.sub(r\"@([a-zA-Z0-9_.-]{1,100})\", \" \", x))\n\n# remove hyperlinks\ndf[label] = df[label].apply(lambda x: re.sub(r\"http\\S+\", \" \", x))\n\n# remove hashtags\ndf[label] = df[label].apply(lambda x: re.sub(r\"#\\w+\", \" \", x))\n\n# remove html tags\ndf[label] = df[label].apply(lambda x: re.sub(r\"<.*?>\", \" \", x))\n\n# remove numbers\ndf[label] = df[label].apply(lambda x: re.sub(r\"\\d+\", \" \", x))\n\n# encode unknown characters\ndf[label] = df[label].apply(lambda x: unicodedata.normalize(\"NFD\", x).encode('ascii', 'ignore').decode(\"utf-8\"))\n\n# remove punctuation and accented characters\ndf[label] = df[label].apply(lambda x: re.sub(r'[^\\w\\s]', ' ', x))","11b31be9":"def remove_stop_words(text, stopwords=set(stopwords.words('english'))):\n    \"\"\" This function removes stop words from a text\n        inputs:\n         - stopword list\n         - text \"\"\"\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n    \n    # stop words updated\n    # stopwords = stopwords.union({\"amp\", \"grocery store\", \"covid\", \"supermarket\", \"people\", \"grocery\", \"store\", \"price\", \"time\"})\n    \n    # loop\n    for word in text_splitted:\n        if word not in stopwords:\n            text_new.append(word)\n    return \" \".join(text_new)\n\ndef clean_stopwords(df, label):\n    \"\"\" This function removes stopwords \"\"\"\n    df[label] = df[label].apply(lambda x: remove_stop_words(x))\n    return df\n#\ndf = clean_stopwords(df, label)","6b4d7f97":"# removes remaining one-letter words and two letters words \ndf[label] = df[label].apply(lambda x: re.sub(r'\\b\\w{1,2}\\b', \" \", x))\n\n# replaces multiple spaces by one single space\ndf[label] = df[label].apply(lambda x: re.sub(r\"[ \\t]{2,}\", \" \", x))\n\n# drop empty lines\ndf[label] = df[label].apply(lambda x: x if len(x) != 1 else '')\ndf[label] = df[label].apply(lambda x: np.nan if x == '' else x)\ndf = df.dropna(subset=[label], axis=0).reset_index(drop=True).copy()","97a8eed1":"def lemmatize_one_text(text):\n    \"\"\" This function lemmatizes words in text (it changes word to most close root word)\n        inputs:\n         - lemmatizer\n         - text \"\"\"\n\n    # initialize lemmatizer\n    lemmatizer = WordNetLemmatizer()\n    \n    # tags\n    lem_tags = ['a', 'r', 'n', 'v']\n\n    # prepare new text\n    text_splitted = text.split(\" \")\n    text_new = list()\n\n    # change bool\n    changed = ''\n    \n    # loop\n    for word in text_splitted:\n        #text_new.append(lemmatizer.lemmatize(word))\n        changed = ''\n        for tag in lem_tags:\n            if lemmatizer.lemmatize(word, tag) != word:\n                changed = tag\n        if changed == '':\n            text_new.append(word)\n        else:\n            text_new.append(lemmatizer.lemmatize(word, changed))\n\n    return \" \".join(text_new)\n\ndef lemmatize(df, label):\n    \"\"\" This function lemmatizes texts \"\"\"\n    df[label] = df[label].apply(lambda x: lemmatize_one_text(x))\n    return df\n#\ndf = lemmatize(df, label)","a01aab0d":"df.head(5)","ee9e622f":"dtf = df.copy()","24715ceb":"dtf['word_count'] = dtf[\"cleanTweet\"].apply(lambda x: len(str(x).split(\" \")))\ndtf['char_count'] = dtf[\"cleanTweet\"].apply(lambda x: sum(len(word) for word in str(x).split(\" \")))\ndtf.head()","1f322444":"parameters = {'axes.labelsize': 20,\n              'axes.titlesize': 30}\n#\nplt.rcParams.update(parameters)\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 6)\nsns.histplot(dtf[\"word_count\"], palette='Blues', stat='count', bins=30, ax=ax1);\nax1.set_xlabel('Word count');\nsns.histplot(dtf[\"char_count\"], palette='Blues', stat='count', bins=30, ax=ax2);\nax2.set_xlabel('Character count');\nax1.tick_params(axis='x', labelsize=16)\nax1.tick_params(axis='y', labelsize=16)\nax1.set_ylabel(\"\")\n#ax1.set_xlim(-10, 50000)\nax2.tick_params(axis='x', labelsize=16)\nax2.tick_params(axis='y', labelsize=16)\nax2.set_ylabel(\"\")\n#ax2.set_xlim(-10, 200000)\nfig.tight_layout(pad=2.0)\nplt.rcParams.update(parameters)","a249b231":"def compute_vader_scores(df, label):\n    sid = SentimentIntensityAnalyzer()\n    df[\"vader_neg\"] = df[label].apply(lambda x: sid.polarity_scores(x)[\"neg\"])\n    df[\"vader_neu\"] = df[label].apply(lambda x: sid.polarity_scores(x)[\"neu\"])\n    df[\"vader_pos\"] = df[label].apply(lambda x: sid.polarity_scores(x)[\"pos\"])\n    df[\"vader_comp\"] = df[label].apply(lambda x: sid.polarity_scores(x)[\"compound\"])\n    return df","2929f9d1":"%%time\ndf = compute_vader_scores(df, \"cleanTweet\")","0f79466a":"df['comp_score'] = df['vader_comp'].apply(lambda c: 'pos' if c >=0 else 'neg')","b5ead9a7":"percent_pos = round(100*df[df['comp_score']==\"pos\"].shape[0]\/df['comp_score'].shape[0], 2)\npercent_neg = round(100 - percent_pos, 2)\nprint(str(percent_pos) + \"% of Donald Trump tweets have positive sentiment according to VADER\")\nprint(str(percent_neg) + \"% of Donald Trump tweets have negative sentiment according to VADER\")","d3b88c93":"fig, ax = plt.subplots()\nfig.set_size_inches(15, 6)\nfig.suptitle(\"Count\", fontsize=12)\ndf.groupby(\"comp_score\").count()[[\"content\"]].rename(columns={\"content\": \"count\"}).plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.text(df[\"comp_score\"].value_counts()[0]*(1-0.1), 0.96, str(percent_pos)+\"%\", fontdict={\"fontsize\": 20, \"color\": \"white\"})\nplt.text(df[\"comp_score\"].value_counts()[1]*(1-0.3), -0.04, str(percent_neg)+\"%\", fontdict={\"fontsize\": 20, \"color\": \"white\"})\nplt.show()","b5a5c8cf":"cloud_negative_tweets = \" \".join([text for text in df[df[\"comp_score\"]==\"neg\"][\"cleanTweet\"]])\nwordcloud_negative = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Reds\").generate(cloud_negative_tweets)\n#\ncloud_positive_tweets = \" \".join([text for text in df[df[\"comp_score\"]==\"pos\"][\"cleanTweet\"]])\nwordcloud_positive = WordCloud(width=800, height=600, max_font_size=120, background_color=\"white\", colormap=\"Greens\").generate(cloud_positive_tweets)","56d041c3":"parameters = {'axes.labelsize': 12,\n              'axes.titlesize': 10}\n#\nfig, (ax1, ax2) = plt.subplots(1, 2)\nfig.set_size_inches(18.5, 7)\nax1.imshow(wordcloud_positive, interpolation='bilinear')\nax1.axis(\"off\")\nax1.set_title(\"WordCloud of positive tweets\", fontsize=12)\nax2.imshow(wordcloud_negative, interpolation='bilinear')\nax2.axis(\"off\")\nax2.set_title(\"WordCloud of negative tweets\", fontsize=12)\nplt.rcParams.update(parameters)\nplt.show()","28d6643d":"# 2. Loading data","0af5d2e6":"<div style=\"display: flex; height:400px; text-align:center\">\n     <img src=\"https:\/\/imgur.com\/WmCHcjw.jpg\" style=\"display:block; margin:auto\" width=300>\n<\/div>","3e5ed5a0":"# 5. Overview","576e06dc":"## 5.2. Missing values","b8011294":"# 7. VADER sentiment analysis","e7b6702a":"# 3. Merge dataframes","efff2003":"# 1. Imports","35d41e2e":"## 6. Cleaning","e843a481":"# 8. WordCloud","7ce1ed31":"# 4. Duplicates","83776a11":"## 5.1. Tweets length","3940917b":"I didn't understood that the dataframes were almost identical","d7b06843":"86% values are missing in hashtag column and 46% values are missing in mentions column, we can drop them."}}