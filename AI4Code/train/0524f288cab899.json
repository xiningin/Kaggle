{"cell_type":{"e928e025":"code","8ba9c20b":"code","9c98c51d":"code","fc8e59bc":"code","07418a6f":"code","983267a1":"code","4ca5160e":"code","65087f56":"code","612738d3":"code","d8839f08":"code","43a2f8ca":"code","c120ed48":"code","f6310869":"code","1f4bba13":"code","05bb2a75":"code","d8127a2d":"code","072c78f5":"code","81199600":"code","12090875":"code","d01065cc":"code","cbc83666":"code","df79bf36":"code","2ed2c6cd":"code","ed32cb85":"code","cecc5f41":"code","5fa48e0c":"code","7c60783a":"code","0555ac26":"code","54a070ee":"code","e9910cf4":"code","49364fb8":"code","267eb7e4":"code","3d3c7cdf":"code","fb95c1a3":"code","5f07e352":"code","41f14256":"code","19d78348":"code","b28db0de":"code","0a9e65b8":"code","52f180fa":"code","bb723da0":"code","0a68b385":"code","550a592f":"code","37f1a9b0":"code","15a68fe1":"code","ebc6ad4b":"code","2d867a6e":"code","f623e2d6":"code","df42b489":"code","556f9e25":"code","78012058":"code","4873438e":"code","416d920a":"code","d36ef8e8":"markdown","96e7eb06":"markdown","c717d851":"markdown","857b319b":"markdown","8b5a0245":"markdown","a0440169":"markdown","b435045c":"markdown","0f16b98d":"markdown","3d336ea0":"markdown","1e7f87cd":"markdown","0f8a9754":"markdown","83ac0bce":"markdown","804591f6":"markdown","b4543f8f":"markdown","b86fb3f2":"markdown","29f97619":"markdown","096469a2":"markdown","c6314838":"markdown"},"source":{"e928e025":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8ba9c20b":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain_df.head(10)","9c98c51d":"print(\"Train Set shape: {}\".format(train_df.shape))\nprint(\"Test Set shape: {}\".format(test_df.shape))","fc8e59bc":"train_df.info()","07418a6f":"train_df.describe()","983267a1":"train_df['Survived'].value_counts()","4ca5160e":"missing_data_train = train_df.isnull().sum()\n\nprint(missing_data_train[missing_data_train > 0])","65087f56":"missing_data_test = test_df.isnull().sum()\n\nprint(missing_data_test[missing_data_test > 0])","612738d3":"import missingno as msno\nmsno.matrix(train_df)","d8839f08":"msno.matrix(test_df)","43a2f8ca":"#creating copies of training and test datasets\ntrain_data = train_df.copy()\ntest_data = test_df.copy()","c120ed48":"def drop_columns(df, columns):\n    df = df.drop(columns, axis=1, inplace=True)\n    return df\n\ncolumns = ['Cabin']\ndrop_columns(train_df, columns)\ndrop_columns(test_df, columns)\n\ntrain_df.shape","f6310869":"#Age distribution\nimport seaborn as sns\n\nsns.displot(train_df, x=\"Age\", hue= \"Survived\", multiple=\"stack\")","1f4bba13":"sns.boxplot(x=train_df[\"Age\"])","05bb2a75":"#we have some outliers so we will use median to fill in missing values in Age.\n\ndef fill_missing(df, column):\n    df[column].fillna(df[column].median(),inplace=True)\n    return df\n\ncolumn = \"Age\"\n\nfill_missing(train_df, column)\nfill_missing(test_df, column)\n\nmissing_data_test = test_df.isnull().sum()\n\nprint(missing_data_test[missing_data_test > 0])","d8127a2d":"missing_data_train = train_df.isnull().sum()\n\nprint(missing_data_train[missing_data_train > 0])","072c78f5":"train_df['Fare'].value_counts()","81199600":"sns.kdeplot(data=train_df, x=\"Fare\", hue=\"Survived\")","12090875":"sns.boxplot(x=test_df[\"Fare\"])","d01065cc":"#we will use median again to fill in missing values for Fare\nfill_missing(test_df, \"Fare\")\n\ntest_df.isnull().sum()","cbc83666":"#relationship between Fare and Embarked\nsns.stripplot(x=\"Embarked\", y=\"Fare\", data=train_df)","df79bf36":"#relationship between Fare and Age\nsns.scatterplot(data=train_df, x=\"Age\", y=\"Fare\")","2ed2c6cd":"sns.countplot(x=\"Embarked\", hue=\"Survived\", data=train_df)","ed32cb85":"sns.countplot(x=\"Pclass\", hue=\"Survived\", data=train_df)","cecc5f41":"#Relationship between Age, Sex and Survived\nsns.violinplot(x=\"Sex\", y=\"Age\", hue=\"Survived\",\n                    data=train_df, palette=\"muted\")","5fa48e0c":"sns.countplot(x='Sex', hue='Survived', data=train_df)","7c60783a":"train_df.info()","0555ac26":"train_df['Ticket'].value_counts()","54a070ee":"columns = ['PassengerId', 'Ticket']\n\ndrop_columns(train_df, columns)\ndrop_columns(test_df, columns)\n\nprint(test_df.shape)\nprint(train_df.shape)","e9910cf4":"columns = ['Name']\n\ndrop_columns(train_df, columns)\ndrop_columns(test_df, columns)","49364fb8":"train_df['SibSp'].value_counts()","267eb7e4":"sns.countplot(x='SibSp', hue='Survived', data=train_df)","3d3c7cdf":"train_df['Parch'].value_counts()","fb95c1a3":"#Is there a relationship between SibSp and Parch\nsns.jointplot(data=train_df, x=\"Parch\", y=\"SibSp\")","5f07e352":"#for the Sex feature, we replace Male with 1 and Female with 0\n\ntrain_df['Sex']=train_df['Sex'].replace('male', 0)\ntrain_df['Sex']=train_df['Sex'].replace('female', 1)\n\ntest_df['Sex']=test_df['Sex'].replace('male', 0)\ntest_df['Sex']=test_df['Sex'].replace('female', 1)","41f14256":"train_df.info()","19d78348":"train_df = pd.get_dummies(train_df, columns=['Embarked'], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=['Embarked'], drop_first=True)","b28db0de":"train_df.isnull().sum()","0a9e65b8":"train_df.head()","52f180fa":"test_df.head()","bb723da0":"X = train_df.iloc[:, 1:]\ny = train_df.iloc[:, 0]","0a68b385":"#Normalization of numerical features\nfrom sklearn.preprocessing import MinMaxScaler\nminmax = MinMaxScaler()\n\ntrain_minmax = minmax.fit_transform(X)\n\nX = pd.DataFrame(train_minmax, columns=X.columns)\nX.head()","550a592f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","37f1a9b0":"from sklearn.linear_model import LogisticRegression\n\nlog_reg_model = LogisticRegression()\n\nlog_reg_model.fit(X_train, y_train)","15a68fe1":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier()\n\nknn_model.fit(X_train, y_train)","ebc6ad4b":"from sklearn.tree import DecisionTreeClassifier\n\ntree_model = DecisionTreeClassifier()\n\ntree_model.fit(X_train, y_train)","2d867a6e":"from sklearn.svm import SVC\n\nsvm_model = SVC()\n\nsvm_model.fit(X_train, y_train)","f623e2d6":"from sklearn.ensemble import RandomForestClassifier\n\nforest_model = RandomForestClassifier()\n\nforest_model.fit(X_train, y_train)","df42b489":"import xgboost as xgb\n\nxgb_model = xgb.XGBClassifier(use_label_encoder=False)\n\nxgb_model.fit(X_train, y_train)","556f9e25":"from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n\ndef evaluate_model(model):\n    y_pred = model.predict(X_test)\n    print(\"Metrics for {} model are: \".format(model) + \"\\n\")\n    print(\"f1 score: {}\".format(f1_score(y_test, y_pred, average=\"macro\")))\n    print(\"precision score : {}\".format(precision_score(y_test, y_pred, average=\"macro\")))\n    print(\"recall score : {}\".format(recall_score(y_test, y_pred, average=\"macro\")))\n    print(\"\\n\")\n\nmodel_list = [xgb_model, tree_model, log_reg_model, forest_model, svm_model, knn_model]\n\nfor model in model_list:\n    evaluate_model(model)   ","78012058":"#Using the KNN model\nknn_model.fit(X, y)","4873438e":"y_preds = knn_model.predict(test_df)","416d920a":"# Save predictions in format used for competition scoring\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId,\n                       'Survived': y_preds})\noutput.to_csv('submission.csv', index=False)","d36ef8e8":"# Problem Definition\nThis is a binary classification problem. So we are going to prepare our data to train it for the following classification models and pick the best:\n\n* Logistic Regression.\n* k-Nearest Neighbors.\n* Decision Trees.\n* Support Vector Machine.\n* Naive Bayes","96e7eb06":"We seem to have more passengers embarking at Southampton port, than others. More of the passengers who embarked at that port died, than those who survived.","c717d851":"# Train , test split","857b319b":"We observe that those that paid lower fares had a higher probability of dying","8b5a0245":"In the train set, cabin has a lot of missing values, so we are going to drop it","a0440169":"# Model Training\nLogistic Regression.\nk-Nearest Neighbors.\nDecision Trees.\nSupport Vector Machine.\nNaive Bayes","b435045c":"There is almost a 50% death rate per age group, save for age group 65 - 75 with no fatalities. Total fatalities for age group 75 - 80.","0f16b98d":"in the train set, Cabin has a lot of missing values as well","3d336ea0":"# Model Evaluation\n\n","1e7f87cd":"We are predicting the Survival label","0f8a9754":"# Categorical Feature Encoding","83ac0bce":"More passengers were in Pclass 3, where there was a significantly lower survival rate than the other two classes.","804591f6":"# Exploratory Data Analysis\nLet's do some EDA to see if we can gain insights that might help is in filling missing data, and in feature selection","b4543f8f":"# Missing Data\n\nWe will inspect the data for missing values and try to rectify where possible","b86fb3f2":"Ticket has too many categorical attributes so we will drop it, together with PassengerId","29f97619":"There is no visible relationship between SibSp and Parch","096469a2":"We split features and labels on our train data set","c6314838":"More males died as compared to females"}}