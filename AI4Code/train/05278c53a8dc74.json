{"cell_type":{"acf6008c":"code","35acd4b9":"code","9f36246d":"code","03812bb0":"code","be46c5db":"code","268a2a9d":"code","05c935d3":"code","2562660e":"code","f1d8e92f":"code","3a18601a":"code","bf0be171":"code","e4ec36ea":"code","77e4825e":"code","34e0ebed":"code","9a3f24ed":"code","34bfda90":"code","33d63cf1":"code","7822bf9b":"code","be68d1fe":"code","fe8f5bcd":"code","d607368a":"code","dd90965f":"code","65c643b3":"code","709049c3":"code","1cc03371":"code","57333ffc":"code","3dfb0af7":"code","410ac3aa":"code","333a800d":"code","31f3b77d":"code","d8825977":"code","452a92c0":"code","cae43e39":"code","e6d14f37":"code","c2fdd5e3":"code","b6e68ef5":"code","bd520729":"code","e86da722":"code","2af9479b":"code","782e6090":"code","db58d2f0":"code","82510d4a":"code","128c83c4":"code","789ace9f":"code","c8647818":"code","54d86744":"code","a540d92c":"code","ce981e51":"code","5922b251":"code","52549ea0":"code","7cbb3d51":"code","662e82d1":"code","23501e3c":"code","dcc5f5cd":"markdown","8b8fb986":"markdown","b3f493db":"markdown","c98917a9":"markdown","95f871a4":"markdown","2c5a15e2":"markdown","9db6cec4":"markdown","db279073":"markdown","98321593":"markdown","5bc9b360":"markdown","855393e4":"markdown","ca77cc0d":"markdown","f9768f41":"markdown","e59729d0":"markdown","006c41a3":"markdown","f997ddce":"markdown","f63a6090":"markdown","a5a96764":"markdown","da4a5d0e":"markdown","f73692e3":"markdown","83611ec0":"markdown","0b0321eb":"markdown","da125071":"markdown","57e1e5a3":"markdown","c750f6ad":"markdown","862e0e3d":"markdown","3f37ce40":"markdown"},"source":{"acf6008c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","35acd4b9":"# Importa\u00e7\u00e3o do conjunto de dados \ndf= pd.read_csv('..\/input\/hmeq-data\/hmeq.csv')","9f36246d":"# Verificando o Dataset\ndf","03812bb0":"# Verificando a imens\u00e3o do Dataset\nprint(df.shape)","be46c5db":"# Verificando os tipos de vari\u00e1veis\ndf.dtypes\n","268a2a9d":"#Analises de estat\u00edsticas descritivas \nprint(df.describe().T)","05c935d3":"# imprimindo os primeiros registros do Dataset\ndf.head(10)","2562660e":"# verificando os valores missing nas vari\u00e1veis n\u00famericas e categ\u00f3ricas\nfeat_missing = []\n\nfor f in df.columns:\n    missings = df[f].isnull().sum()\n    if missings > 0:\n        feat_missing.append(f)\n        missings_perc = missings\/df.shape[0]\n        \n        # printing summary of missing values\n        print('Variable {} has {} records ({:.2%}) with missing values'.format(f, missings, missings_perc))\n\n# how many variables do present missing values?\nprint()\nprint('O total, de {} vari\u00e1veis com valores missings'.format(len(feat_missing)))","f1d8e92f":"#dropping rows that have missing data\ndf.dropna(axis=0, how='any', inplace=True)\ndf.info()\n","3a18601a":"df['VALUE_MORTDUE'] = df['VALUE'] - df['MORTDUE']","bf0be171":"df.info()","e4ec36ea":"# Visualiza\u00e7\u00f5es \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nax = sns.countplot(y='BAD', data=df).set_title(\"Situa\u00e7\u00e3o dos Clientes\")","77e4825e":"#Analise do Dataset\nimport pandas\nfrom pandas.plotting import scatter_matrix\nfrom matplotlib import pyplot","34e0ebed":"# Visualiza\u00e7\u00e3o em Gr\u00e1ficos das vari\u00e1veis categ\u00f3rias\nsns.set( rc = {'figure.figsize': (4, 4)})\nfcat = ['REASON','JOB']\n\nfor col in fcat:\n    plt.figure()\n    sns.countplot(y=df[col], data=df, palette=\"RdPu\")\n    plt.show()","9a3f24ed":"# visualizing numeric variables using seaborn\nf, axes = plt.subplots(3,3, figsize=(20,20))\nsns.distplot( df[\"LOAN\"] , color=\"skyblue\", bins=15, kde=False, ax=axes[0, 0])\nsns.distplot( df[\"DEBTINC\"] , color=\"olive\", bins=15, kde=False, ax=axes[0, 1])\nsns.distplot( df[\"MORTDUE\"] , color=\"orange\", bins=15, kde=False, ax=axes[0, 2])\nsns.distplot( df[\"YOJ\"] , color=\"yellow\", bins=15, kde=False, ax=axes[1, 0])\nsns.distplot( df[\"VALUE\"] , color=\"pink\", bins=15, kde=False, ax=axes[1, 1])\nsns.distplot( df[\"CLAGE\"] , color=\"gold\", bins=15, kde=False, ax=axes[1, 2])\nsns.distplot( df[\"CLNO\"] , color=\"teal\", bins=15, kde=False, ax=axes[2, 1])\nsns.distplot( df['DEROG'], color=\"blue\", bins=15, kde=False, ax=axes[2, 2])\nsns.distplot( df['DELINQ'], color=\"green\", bins=15, kde=False, ax=axes[2, 0])","34bfda90":"# Criar vari\u00e1veis Dummies\nDum_df = pd.get_dummies(df)\nprint(Dum_df.info())","33d63cf1":"ax = sns.boxplot(x=\"VALUE_MORTDUE\", data=Dum_df)","7822bf9b":"import seaborn as sns\n\nsns.scatterplot(data=Dum_df, x='VALUE', y='MORTDUE')","be68d1fe":"sns.scatterplot(data=Dum_df, x='VALUE', y='LOAN')","fe8f5bcd":"#Base sem Dummies\ncorr = df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","d607368a":"#Base com Dummies\ncorr = Dum_df.corr()\n#Plot figsize\nfig, ax = plt.subplots(figsize=(10,8))\n#Generate Color Map\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\n#Generate Heat Map, allow annotations and place floats in map\nsns.heatmap(corr, cmap=colormap, annot=True, fmt=\".2f\")\n#Apply xticks\nplt.xticks(range(len(corr.columns)), corr.columns);\n#Apply yticks\nplt.yticks(range(len(corr.columns)), corr.columns)\n#show plot\nplt.show()","dd90965f":"# Dividindo o DataFrame\nfrom sklearn.model_selection import train_test_split\n\n# Treino e teste\ntrain, test = train_test_split(Dum_df, test_size=0.15, random_state=42)\n\n# Veificando o tanho dos DataFrames\ntrain.shape, test.shape\n\n","65c643b3":"Dum_df.info()","709049c3":"# Selecionado as features\nfeats = [c for c in Dum_df.columns if c not in ['BAD']]","1cc03371":"Dum_df.head()","57333ffc":"# Trabalhando com RandomForest\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier(n_estimators=200, min_samples_split=5, max_depth=4, random_state=42)\nrf.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], rf.predict(test[feats]))","3dfb0af7":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","410ac3aa":"# Usar o cross validation\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(rf, train[feats], train['BAD'], n_jobs=-1, cv=5)\n\nscores, scores.mean()","333a800d":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","31f3b77d":"# Trabalhando com XGBoost\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier(n_estimators=200, learning_rate=0.09, random_state=42)\nxgb.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], xgb.predict(test[feats]))","d8825977":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","452a92c0":"# Trabalhando com GBM\nfrom sklearn.ensemble import GradientBoostingClassifier\ngbm = GradientBoostingClassifier(n_estimators=200, learning_rate=1.0, max_depth=1, random_state=42)\ngbm.fit(train[feats], train['BAD'])\naccuracy_score(test['BAD'], gbm.predict(test[feats]))","cae43e39":"# Feature Importance com RF\npd.Series(rf.feature_importances_, index=feats).sort_values().plot.barh()","e6d14f37":"print(\"cross validation - \", scores, scores.mean())\nprint(\"RandomForest - \",accuracy_score(test['BAD'], rf.predict(test[feats])))\nprint(\"GBM - \",accuracy_score(test['BAD'], gbm.predict(test[feats])))\nprint(\"XGBoost - \",accuracy_score(test['BAD'], xgb.predict(test[feats])))","c2fdd5e3":"# Determinando a quantidade de clusters\n\n# Importando o k-means\nfrom sklearn.cluster import KMeans\n\n# Selecionando as variaveis para utilizar no modelo.\nX= Dum_df[['MORTDUE','LOAN', 'YOJ']]\n\n# C\u00e1lculo do SSE - Sum of Squared Erros\nsse = []\n\nfor k in range (1, 12):\n    kmeans = KMeans(n_clusters=k, random_state=42).fit(X)\n    sse.append(kmeans.inertia_)\nprint(sse)","b6e68ef5":"# Definindo a quantidade clusters utilizando o m\u00e9todo Elbow \nimport matplotlib.pyplot as plt\n\nplt.plot(range(1, 12), sse, 'bx-')\nplt.title('Elbow Method')\nplt.xlabel('N\u00famero de clusters')\nplt.ylabel('SSE')\nplt.show()","bd520729":"# Excecutando a clusteriza\u00e7\u00e3o com 4 clusters\nkmeans = KMeans(n_clusters=4, init='k-means++', random_state=42)\ncluster_id = kmeans.fit_predict(X)\ncluster_id","e86da722":"# Guardar os resultados no dataframe\nX['cluster_id'] = cluster_id\n\n#veficando o tamanho do DF\nX.sample(10)\n\n","2af9479b":"X.head()","782e6090":"# Plotando os agrupamentos e os centro\u00eddes\nfig = plt.figure(figsize=(12,8))\n\nplt.scatter(X.values[:,0], X.values[:,1], c=kmeans.labels_)\nplt.scatter(kmeans.cluster_centers_[:,0] ,kmeans.cluster_centers_[:,1], color='red', marker=\"x\", s=200)\nplt.show()\n","db58d2f0":"Dum_df.info()","82510d4a":"import statsmodels.api as sm\nimport statsmodels.formula.api as smf\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report,\\\n                            accuracy_score, confusion_matrix, auc\n\n#Utilizando o statsmodels\n#Verificar as chances de um cliente ser inadimplente em fun\u00e7\u00e3o das vari\u00e1veis LOAN + YOJ + MORTDUE (Valor do empr\u00e9stimo, ocupa\u00e7\u00e3o e o valor da hipoteca)\n#BAD = 1 (Cliente Inadimplente)\n#BAD = 2 (Cliente adimplente)\n\nmodelo = smf.glm(formula='BAD ~ LOAN + YOJ + MORTDUE', data=Dum_df,\n                family = sm.families.Binomial()).fit()\nprint(modelo.summary())","128c83c4":"#gerar osdados em percentuais relativos de chances de inadimplencia\nprint(np.exp(modelo.params[1:]))","789ace9f":"(np.exp(modelo.params[1:]) -1) * 100\n","c8647818":"# Agora vamos fazer com sklearn para aproveitar as m\u00e9tricas\nmodel1 = LogisticRegression(penalty='none', solver='newton-cg')\nbaseline_df = Dum_df[['LOAN', 'YOJ', 'MORTDUE']].dropna()\ny = Dum_df.BAD\nX = pd.get_dummies(Dum_df[['LOAN', 'YOJ', 'MORTDUE']], drop_first=True)\nprint(X)","54d86744":"model1.fit(X, y)","a540d92c":"#Aplicando a Regress\u00e3o log\u00edstica com sklearn\nLogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n                   multi_class='warn', n_jobs=None, penalty='none',\n                   random_state=None, solver='newton-cg', tol=0.0001, verbose=0,\n                   warm_start=False)","ce981e51":"print(model1.coef_) ","5922b251":"# Predizendo as probabilidades\nyhat = model1.predict_proba(X)\n","52549ea0":"# manter somente para a classe positiva\nyhat = yhat[:, 1] ","7cbb3d51":"# usando a fun\u00e7\u00e3o do sklearn\nconfusion_matrix(y, model1.predict(X)) ","662e82d1":"#imprimindo a matriz de confus\u00e3o\npd.crosstab(y, model1.predict(X))  ","23501e3c":"acuracia = accuracy_score(y, model1.predict(X))\nprint('O modelo obteve %0.4f de acur\u00e1cia.' % acuracia)","dcc5f5cd":"Os grupos s\u00e3o agrupados por suas similaridades baseado no conceito de dist\u00e2ncia, a t\u00e9cnica consiste em atribuir um centr\u00f3ide a cada elemento, em seguida, utilizando a dist\u00e2ncia euclidiana, calcula-se as dist\u00e2ncias de cada elemento, e divide os centr\u00f3ides de acordo com o n\u00famero de clustes, nesse caso 4.\nA partir de ent\u00e3o,uUma vez que os pontos foram atribu\u00eddos aos clusters conforme sua dist\u00e2ncia, recalcula-se o valor dos centr\u00f3ide, ent\u00e3o \u00e9 calculada a m\u00e9dia dos valores dos pontos de dados de cada cluster e o valor m\u00e9dio ser\u00e1 o novo centr\u00f3ide. E a partir do centr\u00f3ide s\u00e3o calculadas as dist\u00e2ncias de cada elemento e agrupando aqueles com maior pr\u00f3ximidade. \nAtav\u00e9s dessa t\u00e9cnica verificamos que os grupos laranja e preto tiveram uma quantidade de elementos com menor dist\u00e2ncia, tornan-se grupos maiores e dois grupos com quantidade menor de elementos por esses elementos estarem mais distantes dos centr\u00f3ides. \n\n","8b8fb986":"# **Tratando Missing**","b3f493db":"# **An\u00e1lise da Vari\u00e1vel - Target (Preditora)**","c98917a9":"1. A an\u00e1lise explorat\u00f3ria serr\u00e1 realizada para conhecer o conjunto de dados de modo a resumir as caracter\u00edsticas do departamento de cr\u00e9dito de um banco. O objetivo \u00e9 a automatizar o processo de tomada de decis\u00e3o para aprova\u00e7\u00e3o das linhas de cr\u00e9dito do patrimonio l\u00edquido. Assim, seguindo as recomenda\u00e7\u00f5es da Lei de Igualdade de Oportunidades de Cr\u00e9dito, ser\u00e1 criado um modelo de pontua\u00e7\u00e3o derivado e estatisticamente s\u00f3lido baseado nos dados coletados pelas conce\u00e7\u00f5es recentes de cr\u00e9dito atrav\u00e9s do processo atual. O modelo ser\u00e1 constru\u00eddo a partir de ferramentas de modelagem preditiva. ","95f871a4":"A base de dados padronizada possui 3364 registros e 13 colunas, um conjunto de dados relativamente pequeno para aplicar modelos de predi\u00e7\u00e3o. Assim vamos criar uma nova coluna que ir\u00e1 verificar se o valor atual da propriedade \u00e9 maior ou menor que o valor devido da hipoteca. \nVALUE_MORTDUE = Valor da propriedade - valor da hipoteca","2c5a15e2":"* **Data Mining e Machine Learning II** \n* Professor: MARCOS GUIMAR\u00c3ES\n* Aluna: Viviane Silvestre","9db6cec4":"O conjunto de dados possuir em seu maior n\u00famero 5960 registros, no entanto, algumas vari\u00e1veis possuem valores nulos, a vari\u00e1vel DEBTINC possui 4693, sendo necess\u00e1rio utilizar t\u00e9cnicas para padronizar o conjunto de dados. Que podem ser imputa\u00e7\u00e3o ou exclus\u00e3o dos valores missings. ","db279073":"A vari\u00e1vel **VALUE_MORTDUE** foi criada para verificar se existe saldo suficiente sobre o valor do im\u00f3vel (VALUE) caso seja necess\u00e1rio executar o valor da hip\u00f3teca (MORTDUE).\n\nNo gr\u00e1fico de boxplot observou-se que a m\u00e9dia do saldo do im\u00f3vel est\u00e1 em torno de 20.000. \nExistem valores extremos negativos, que significa que o valor do im\u00f3vel n\u00e3o \u00e9 suficiente para pagar a hipoteca e valores extremos positivos mostram que os valores dos im\u00f3veis \u00e9 superior ao valor da hipoteca.","98321593":"# **An\u00e1lises Explorat\u00f3rias**","5bc9b360":"# **Modelo de Regress\u00e3o Log\u00edstica**\n","855393e4":"Verifica-se que o cluster 2 possui um maior numero de iforma\u00e7\u00f5es.","ca77cc0d":"A analise demonstrou que a maior parte dos valores missings est\u00e1 na vari\u00e1vel MOTDUE, que \u00e9 a vari\u00e1vel que possui o valor devido pelos clientes nas hip\u00f3tecas. Como esses valores s\u00e3o muito diferentes, a decis\u00e3o foi por excluir os valores missings, e assim analisar apenas os valores reais da base de dados. ","f9768f41":"O gr\u00e1fico do modelo \"Gr\u00e1fico de Cotovelo\" mostrou que o n\u00famero bom de clusters \u00e9 4, uma vez que de 1 a 3 ter\u00edamos poucos agrupamentos, ou seja, a informa\u00e7\u00e3o estaria muito concentrada e de 5 a 6 os agrupamentos estariam muitos dispersos. ","e59729d0":"# **Modelos estat\u00edsticos**","006c41a3":"Para a regress\u00e3o logistica foi utilizada a base com a cria\u00e7\u00e3o das vari\u00e1veis dummies. \nTodos os coeficientes foram estat\u00edsticamente significativos para o modelo, pois o p-valor foi abaixo do n\u00edvel de signific\u00e2ncia de 5%.\nA chance de um cliente ser inadimplente \u00e9 de 99% se for considerado o montande do empr\u00e9stimo LOAN, 96% est\u00e1 associado a ocupa\u00e7\u00e3o YOJ e 99% ao valor devido da hipoteca MORTDUE. \n","f997ddce":"Nas an\u00e1lises de acima o algoritmo que melhor explica o modelo \u00e9 XGBoost com 95% de acur\u00e1cia.","f63a6090":"* Para as pr\u00f3ximas an\u00e1lises ser\u00e1 criado vari\u00e1veis Dummies com objetivo de melhorar a base de dados nas predi\u00e7\u00f5es que ser\u00e3o realizadas. ","a5a96764":"1. **An\u00e1lises das vari\u00e1veis n\u00famericas - Gr\u00e1ficos de Histograma **\n* As vari\u00e1veis quantitativas demosntram em todos os casos distru\u00ed\u00e7\u00e3o assim\u00e9trica a esquerda.\n* Vari\u00e1vel LOAN - Os valores dos empr\u00e9stimos est\u00e3o concentrados at\u00e9 20.000,00, com alguns valores extremos acima de 80.000,00\n* Vari\u00e1vel DEBTINC - que \u00e9 a raz\u00e3o entre a d\u00edvida e o rendimento ou seja a capacidade de pagamento, possui mais concentra\u00e7\u00e3o entre 30 e 40.\n* Vari\u00e1vel MORTDUE - que verifica o valor devido da hip\u00f3teca, mostra que a maioria dos clientes posseum d\u00edvidas que variam entre 40.000 e 80.000\n* Vari\u00e1vel YOJ - Verifica os anos que trabalha no emprego atual. A maioria trabalha at\u00e9 5 anos na empresa, alguns clientes trabalham a mais de 40 anos (n\u00famero fora do padr\u00e3o)\n* Vari\u00e1vel VALUE - Que verifica os valores dos im\u00f3veis demonstra que a maioria dos im\u00f3veis est\u00e3o avaliados entre 50.000 e 100.000, existindo casos de im\u00f3veis acima de 400.000\n* Vari\u00e1vel CLAGE - verifica a idade em meses da mais da linha de cr\u00e9dito, a concentra\u00e7\u00e3o est\u00e1 entre 100 e 300 meses (entre 8 e 25 anos) em que adquiriu a primeira linha de cr\u00e9dito. \n* Vari\u00e1vel CLNO - verifica quantas linhas de cr\u00e9dito o cliente possui. A maioria possui entre 10 e 25 linhas de cr\u00e9dito. \n* Vari\u00e1vel DEROG - quantidade relat\u00f3rios depreciativos a concentra\u00e7\u00e3o est\u00e1 entre 1 e 3.\n* Vari\u00e1vel DELINQ - Quantidade de linhas de cr\u00e9dito inadimplentes concentram-se entre 0 e 3.","da4a5d0e":"O primeiro passo \u00e9 verificar a quantidade de missings em cada tipo de vari\u00e1vel e assim tomar a decis\u00e3o de que tipo de t\u00e9cnica ser\u00e1 utilizada para padronizar a base de dados. ","f73692e3":"No gr\u00e1fico de correla\u00e7\u00e3o antes da cria\u00e7\u00e3o das vari\u00e1veis dummies mostra uma correla\u00e7\u00e3o forte entre as vari\u00e1veis \"MORTDUE\" e \"VALUE\" em rela\u00e7\u00e3o a vari\u00e1vel target BAD.","83611ec0":"**An\u00e1lises dos gr\u00e1ficos \nReason**\n1. DebtCon = debt consolidation = Consolida\u00e7\u00e3o da D\u00edvida\n1. HomeImp = home improvement - Melhoria na casa\nA an\u00e1lise demonstrou que a maior parte dos clientes tiveram a d\u00edvida consolidade e que apenas 1000 clientes fizeram melhorias no im\u00f3vel.\n\n**Job\n1. Refere-se a Seix Categorias Ocupacionais** \nOther, Office, Mgr, ProfExe, Sales, Self\nA maioria dos clientes n\u00e3o se enquandraram nas categorias e classificaram a ocupa\u00e7\u00e3o como outras, a maior parte s\u00e3o ProfExe, Office e Mgr. \n\n","0b0321eb":"Para a aplica\u00e7\u00e3o dos modelos estat\u00edsticos a base de dados ser\u00e1 dividida entre teste e treino. \n1. Ser\u00e3o aplicados os modelos de RandomForestClassifier, cross_val_score, XGBoost e GradientBoostingClassifier. Com o objetivo de verificar se o modelo \u00e9 capaz de auxiliar na tomada de decis\u00e3o de conceder ou n\u00e3o o cr\u00e9dito.","da125071":"Ap\u00f3s a cria\u00e7\u00e3o das vari\u00e1veis dummies percebe-se uma melhora na correla\u00e7\u00e3o das vari\u00e1veis LOAN YOJ, continuando mostrar correla\u00e7\u00e3o forte entre VALUE e LOAN.","57e1e5a3":"# **Gr\u00e1ficos de Correla\u00e7\u00e3o**","c750f6ad":"A vari\u00e1vel BAD \"Target\" verifica se o cliente est\u00e1 inadimplente\na resposta 1 = Inadimplete e 0 = Pagamento em Dia\nO gr\u00e1fico demonstrou que a maioria dos clientes est\u00e3o em dia com o pagamento.","862e0e3d":"# **Clusteriza\u00e7\u00e3o**","3f37ce40":"Outra metodologia aplicada ser\u00e1 a clusteriza\u00e7\u00e3o, o primeiro passo ser\u00e1 selecionar as vari\u00e1veis, para isso considerou o gr\u00e1fico de correla\u00e7\u00e3o. As vari\u00e1veis escolhidas foram 'MORTDUE', 'LOAN' e 'YOJ'. O objetivo \u00e9 determinar a quantidade de clusters (grupos) necess\u00e1rios para explicar o modelo. "}}