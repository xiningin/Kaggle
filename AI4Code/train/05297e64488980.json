{"cell_type":{"6fc3fd6d":"code","62378c87":"code","16238b2c":"code","48a8d8e7":"code","5029d054":"code","ef53f1d6":"code","8a9dccab":"code","b95f5eb7":"code","bcd60e43":"code","5ab4e5d2":"code","74e909d6":"code","f617e9e5":"code","d15d6914":"code","aabdc42f":"code","141258a4":"code","333e410f":"code","ab139101":"code","d68312a6":"code","749831f4":"code","47335732":"code","698a1114":"code","436108b0":"code","b2052da6":"code","82522a0c":"code","2bbec2d6":"code","2066e78a":"code","e2a43302":"code","407537dc":"code","1bddfb86":"code","1209ca22":"code","5a8fa438":"code","3fd49c3e":"code","320bc595":"code","729f5ac8":"code","6be1bc88":"code","d69543fe":"code","2eb638b5":"code","a4e35074":"code","40a50101":"code","ee043b71":"code","7bcac204":"code","521dc67f":"code","44895095":"code","d019b16c":"code","34d8dc76":"code","cf2690a1":"code","45c8b03f":"code","7e208340":"markdown","2a28e5b2":"markdown","c97b0743":"markdown","3c1b462f":"markdown","5c8485c3":"markdown","5aae841b":"markdown","1f82f2f3":"markdown","338ca5fe":"markdown","2fbba1f2":"markdown","b21d808f":"markdown","b308d56e":"markdown","5febb7ec":"markdown","08782504":"markdown","bffc217f":"markdown","7babee54":"markdown","d6ab1f59":"markdown","5b595235":"markdown","808081f9":"markdown","74ba0bdd":"markdown","c3334b41":"markdown","e236905c":"markdown","88e68dbe":"markdown","ccb7b465":"markdown","0b2b1038":"markdown","8707135f":"markdown","1505ab7d":"markdown","812d00e2":"markdown","381dad0c":"markdown"},"source":{"6fc3fd6d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeClassifier","62378c87":"data = pd.read_csv('..\/input\/real-estate-dataset\/Real estate.csv')\ndata.head(5)","16238b2c":"data.columns","48a8d8e7":"features=['house age', 'distance to the nearest MRT station','number of convenience stores', 'latitude', 'longitude']\nX=data[features]\ny=data['house price of unit area']\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","5029d054":"model1=LinearRegression()","ef53f1d6":"model1.fit(train_X,train_y)","8a9dccab":"val_predictions1 = model1.predict(val_X)\nval_predictions1","b95f5eb7":"from sklearn.metrics import mean_absolute_error","bcd60e43":"print(\"Mean absolute error: \")\nprint(mean_absolute_error(val_y, val_predictions1))","5ab4e5d2":"print(\"Coefficients\") \nprint(model1.coef_)\nprint(\"Intercept\") \nprint(\"%2f\"%model1.intercept_)","74e909d6":"data = pd.read_csv('..\/input\/bank-note-authentication-uci-data\/BankNote_Authentication.csv')\ndata.head(5)","f617e9e5":"features=['variance','skewness','curtosis','entropy']\nX=data[features]\ny=data['class']\n# Split into validation and training data\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, random_state=1)","d15d6914":"# Specify Model\nmodel2 = DecisionTreeClassifier(random_state=1)\n# Fit Model\nfit=model2.fit(train_X, train_y)","aabdc42f":"val_predictions2 = model2.predict(val_X)","141258a4":"print(\"Mean absolute error: \")\nprint(mean_absolute_error(val_y, val_predictions2))","333e410f":"from sklearn import tree\nfig, ax = plt.subplots(figsize=(25, 10))\ntree.plot_tree(fit,fontsize=10,filled=True,feature_names=features)\nplt.title(\"Decision Tree\",size=25)","ab139101":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nclassifier = Sequential() \n\nclassifier.add(Dense(units = 16, activation ='relu', input_dim = 4))\nclassifier.add(Dense(units = 8, activation = 'relu'))\nclassifier.add(Dense(units = 6, activation = 'relu'))\nclassifier.add(Dense(units = 1, activation = 'sigmoid'))","d68312a6":"classifier.compile(optimizer = 'rmsprop', loss = 'binary_crossentropy')","749831f4":"classifier.fit(train_X, train_y, batch_size = 1, epochs = 50)","47335732":"val_predictions3 = classifier.predict(val_X)","698a1114":"pip install ann_visualizer","436108b0":"classifier.summary()","b2052da6":"from keras.utils.vis_utils import plot_model\n\nplot_model(classifier, show_shapes=True,show_layer_names=True)","82522a0c":"mae=mean_absolute_error(val_y,val_predictions3)\nprint(mae)","2bbec2d6":"from sklearn.manifold import TSNE\nimport seaborn as sns","2066e78a":"tsne = TSNE(n_components=2, random_state=0)\n#fitting tSNE model to our data\n#this is the same data we used above (bank note authentication)\ndata_tsne = tsne.fit_transform(data)\n#converting into dataframe\ndf_tsne=pd.DataFrame(data=data_tsne,columns=['X','Y'])\ndf_tsne['label']=data['class']","e2a43302":"plt.figure(figsize=(6, 5))\ng = sns.lmplot(x='X',y='Y',data=df_tsne, fit_reg=False,hue='label', size=6)\nplt.title(\"tSNE plot\",size=25)","407537dc":"from sklearn.datasets import load_digits","1bddfb86":"df =load_digits()\nred_df=df.data[:1000]","1209ca22":"model1 = TSNE(n_components=2, random_state=0, perplexity=30, learning_rate=200, n_iter=1000)\nmodel2 = TSNE(n_components=2, random_state=0, perplexity=50, learning_rate=200, n_iter=2000)\ntsne1 = model1.fit_transform(red_df)\ntsne2 = model2.fit_transform(red_df)\nprint(tsne1)\nprint(tsne2)\n# fit_transform(self, X[, y])\n# Fit X into an embedded space and return that transformed output.","5a8fa438":"df_tsne1 = pd.DataFrame(data=tsne1, columns=[\"X\", \"Y\"])\ndf_tsne1['label']=df.target[:1000]\ndf_tsne1","3fd49c3e":"df_tsne2 = pd.DataFrame(data=tsne2, columns=[\"X\", \"Y\"])\ndf_tsne2['label']=df.target[:1000]\ndf_tsne2","320bc595":"g = sns.lmplot(x='X',y='Y',data=df_tsne1, fit_reg=False, hue='label',size=6)\nplt.title(\"tSNE plot 1\",size=25)","729f5ac8":"g = sns.lmplot(x='X',y='Y',data=df_tsne2, fit_reg=False,hue='label', size=6)\nplt.title(\"tSNE plot 2\",size=25)","6be1bc88":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score, homogeneity_score, completeness_score, v_measure_score","d69543fe":"df=data\n#dropping class column to perform k means\ndf.drop('class',axis=1)","2eb638b5":"kmeans = KMeans(n_clusters=2, random_state=0).fit(df)","a4e35074":"#kmeans.labels_","40a50101":"h=homogeneity_score(data['class'], kmeans.labels_)\nh","ee043b71":"c=completeness_score(data['class'], kmeans.labels_)\nc","7bcac204":"print(v_measure_score(data['class'], kmeans.labels_) )\n#harmonic mean of completeness and homogeneity\n2\/((1\/h)+(1\/c))","521dc67f":"silhouette_score(df, kmeans.labels_)","44895095":"data['class'].value_counts()","d019b16c":"df2 = pd.read_csv('..\/input\/wholesale-customers-data\/Wholesale customers data.csv')\ndf2.head()","34d8dc76":"kmeans = KMeans(n_clusters=2, random_state=0).fit(df2)","cf2690a1":"silhouette_score(df2, kmeans.labels_)","45c8b03f":"from yellowbrick.cluster import KElbowVisualizer\nmodel = KMeans(random_state=0)\n\n# Call the KElbowVisualizer with the silhouette metric \nviz = KElbowVisualizer(model, k=(2,6), metric='silhouette', timings=False)\n\n# Fit the data and visualize\nviz.fit(df2)\nviz.poof() ","7e208340":"The error is negligible. This implies that we have successfully trained and correctly predicted.","2a28e5b2":"## Neural Networks","c97b0743":"CASE 1","3c1b462f":"The above scores indicate that there is very poor clustering probably because k means assumes that all classes have the same variance i.e. each cluster has roughly equal number of observations.","5c8485c3":"Steps followed:\n1. Loading data\n2. Taking the first 1000 values for convenience(time consuming process)\n3. Applying tSNE and fitting the model\n4. Converting the fit into dataframe for plotting\n5. Plotting the model","5aae841b":"The various colours denote the various digits like 0,1,2,3,4...etc.\nPoints corresponding to same digit are clustered together.\n\nWe have applied model 2 below with increased perplexity and number of iterations to see if the accuracy is increased.","1f82f2f3":"We have checked error to find the accuracy and it is found to be almost 0.","338ca5fe":"INTERPRETATION -\nIn linear regression,\n\n\nThe hypothesis function h(theta)=\ntheta_0 + theta_1 * x1 + theta_2 * x2 +...\nwhere theta 1 , theta 2 are coefficients and x1,x2 are features.\n\nHere, h = -433.789415 -2.39157343e-01(house age) -4.73776424e-03 (distance to the nearest MRT station)+ 1.09332205e+00(number of convenience stores) + 2.24000967e+02(latitude) -4.20969538e+01(longitude) \n\nThe above function will fit the data given (not perfectly since mean absolute error of 6 is involved).","2fbba1f2":"CASE 2","b21d808f":"## DECISION TREE","b308d56e":"We are using Keras in the sequential API to define a Neural network that will be train this data with an input dimension of 4 since there are 4 features.We will then have a layer of 16, then 8, then 6, and finally 1(last layer is a vector). The final layer will be activated by a sigmoid function which will push it towards a 1 or a 0. This Neural Network can then be used to predict future values. ","5febb7ec":"## ERROR METRICS","08782504":"## tSNE on text","bffc217f":"We fit the data using fit, passing it the training data -- i.e. for this set of X, this is what the Y should look like. The training itself takes a Fit function. Here in the training we pass x's and y's, and specify how many times it will loop, where a loop is it making a guess at the relationship between the x and the y.It measures how well or how bad it does using the loss function, and then it improves on its guess using the optimizer. \n\nThe NN will then spot the patterns in the data, and build a neural network that could replicate that. ","7babee54":"INTERPRETATION -\nWe have 2 labels 0 and 1 denoting non authentic and authentic banknotes.\nThe data was 4 dimensional which was converted to 2D by using tSNE. \n\nNow, the above plot uses these 2 dimensions of tSNE dataframe. The 2 categories are colour coded using class column of the data. Thus, It is observed thar records of same categories are clustered together(except a few outliers).\n","d6ab1f59":"From silhouette score, it can be said that the clustering performance is moderate.","5b595235":"To predict new values, the Neural Network uses predict. We are passing the test values for X (which the Neural Network hasn't previously seen) and it will give back a set of predictions.","808081f9":"Here, we have to specify the optimizer and loss function. On each iteration, it measures how well it did in training using the loss function. It then tries to improve on that using the optimizer.","74ba0bdd":"### SILHOUETTE SCORE\nReturns the mean of all Silhouette coefficients.\n\n\nCoefficient of a sample s= (b - a)\/max(a,b) where,\n\n\na is the average distance between s and all the other data points in the cluster to which s belongs\n\nb is the minimum average distance from s to all clusters to which it does not belong","c3334b41":"When evaluating a clustering algorithm we have 2 cases -\n1. when we know the actual class variables, we can use homogeneity,completeness and v_measure score\n2. If we don't know ground truth labels, we need to use silhouette score.","e236905c":"## tSNE on image MNIST","88e68dbe":"### Parameters of tsne\nn_components - int, optional (default: 2)\nDimension of the embedded space.\n\nrandom_state - int, RandomState instance, default=None\nDetermines the random number generator. Pass an int for reproducible results across multiple function calls. \n\nperplexity - float, optional (default: 30)\nThe perplexity is related to the number of nearest neighbors(expected density). \nLarger datasets usually require a larger perplexity. Consider selecting a value between 5 and 50. \nDifferent values can result in significanlty different results.\n\nlearning_rate - float, optional (default: 200.0)\nThe learning rate for t-SNE is usually in the range [10.0, 1000.0]. \nIf the learning rate is too high, the data may look like a \u2018ball\u2019 with any point approximately equidistant from its nearest neighbours. \nIf the learning rate is too low, most points may look compressed in a dense cloud with few outliers. \n\nn_iter - int, optional (default: 1000)\nMaximum number of iterations for the optimization. Should be at least 250.","ccb7b465":"We observe that the images corresponding to the different digits are separated into different clusters of points.There is very little overlapping because of some similarity. For example, all the blue(dark) points and pink points that represent 0 and 6 respectilvely are very separate whereas there are some outliers of 9 and 1.","0b2b1038":"Clearly k should be 2 as it has the highest silhouette score.","8707135f":"## LINEAR REGRESSION","1505ab7d":"We calculate the error to get an idea of accuracy","812d00e2":"The number of clusters is taken as 2 previously. Using the Silhouette score for different no. of clusters we can check the appropriate k to be used.","381dad0c":"INTERPRETATION -\n\nThe colour codes (blue and brown) are for 2 classes - authentic and non authentic notes.\n\nThe tree is divided by minimising the gini impurity everytime. Gini impurity is a measure of variables that are classified incorrectly. For example, at the root node, variance<=0.765 given the minimum gini(0.494) among all the splits. Once it becomes 0,the group is perfectly homogeneous and there is no further classification(the leaf nodes).\n\nWhen we need to predict the class, we will check the constraint on every node down the tree to the left and right child till we reach the gini impurity 0 ie leaf node."}}