{"cell_type":{"2aa21f4e":"code","eb08a84a":"code","36e3dda3":"code","7627fecf":"code","46d05871":"code","919dbeba":"code","faf63970":"code","6556a561":"code","86e03254":"code","92a730b6":"code","cf399a67":"code","54b95f3f":"code","919bae48":"code","4c2912c7":"code","9103aa15":"code","4a40ade3":"code","451ff4c5":"code","457b112a":"code","910c3bad":"code","5f5de830":"code","32e25ebb":"code","fd7b25c2":"code","9835a062":"code","1b645fda":"code","04346404":"code","1d08dc6f":"code","24837637":"code","33016cc3":"code","91e730ff":"code","bbc531fe":"markdown","92249b11":"markdown","4af6e02e":"markdown","11913f19":"markdown","18ccebe1":"markdown","d5268c3a":"markdown","dca0c668":"markdown","0920ab30":"markdown","cd927ee1":"markdown","ed3bcb6a":"markdown","93cbf7f9":"markdown","6225fb85":"markdown","beba7526":"markdown","a2e2288a":"markdown","9665541e":"markdown","3ec12791":"markdown","18554109":"markdown"},"source":{"2aa21f4e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","eb08a84a":"#load the required libraries to read the data","36e3dda3":"import pandas as pd\nimport numpy as np","7627fecf":"\nCereals = pd.read_csv(\"..\/input\/Cereals.csv\")\nCereals.head(5)","46d05871":"Cereals['label']= Cereals['name']+ '('+ Cereals['shelf'].astype(str)+ \"-\"+ round(Cereals['rating'],2).astype(str)+\")\"\nCereals.drop([\"name\", 'shelf', 'rating'], axis =1, inplace = True)","919dbeba":"#check the head\nCereals.head(5)","faf63970":"Cereals.describe()","6556a561":"## Select all columns except \"label\"\nCereals_label = Cereals['label']\nCereals = Cereals[Cereals.columns.difference(['label'])]","86e03254":"### Check the data\nCereals.columns","92a730b6":"Cereals.isnull().sum(axis = 0)","cf399a67":"from sklearn.preprocessing import Imputer\nmean_Imputer = Imputer()\nimputed_Cereals = pd.DataFrame(mean_Imputer.fit_transform(Cereals),columns=Cereals.columns)","54b95f3f":"imputed_Cereals.isnull().sum(axis=0)","919bae48":"from sklearn.preprocessing import StandardScaler\nstandardizer = StandardScaler()\nstandardizer.fit(imputed_Cereals)\nstd_x = standardizer.transform(imputed_Cereals)\nstd_Cereals = pd.DataFrame(std_x, columns = imputed_Cereals.columns)\nstd_Cereals.head()","4c2912c7":"std_Cereals.describe()","9103aa15":"## Loading the models\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nimport matplotlib.pyplot as plt\n%matplotlib notebook\n","4a40ade3":"## preparing linkage matrix\nlinkage_matrix = linkage(std_Cereals, method = 'ward', metric = 'euclidean')\n## plotting\ndendrogram(linkage_matrix, labels = Cereals_label.as_matrix())\nplt.tight_layout()\nplt.show()","451ff4c5":"from sklearn.cluster import AgglomerativeClustering\n## Instantiating object\nagg_clust = AgglomerativeClustering(n_clusters=6,affinity=\"euclidean\",linkage = \"ward\")","457b112a":"## Training model and return class labels\nagg_Clusters = agg_clust.fit_predict(std_Cereals)","910c3bad":"## label - Cluster\nagg_result = pd.DataFrame({\"labels\":Cereals_label,\"agg_Cluster\":agg_Clusters}).sort_values(\"agg_Cluster\")\nagg_result.head()","5f5de830":"agg_result.tail(5)","32e25ebb":"from sklearn.cluster import KMeans\nkmeans_object = KMeans(n_clusters=5,random_state=123)\nkmeans_object.fit(std_Cereals)\nkmeans_clusters = kmeans_object.predict(std_Cereals)\nkmeans_result = pd.DataFrame({\"labels\":Cereals_label,\"kmeans_cluster\":kmeans_clusters})\nkmeans_result.head()","fd7b25c2":"kmeans_result.tail(5)","9835a062":"kmeans_object.cluster_centers_","1b645fda":"cluster_centroids = pd.DataFrame(standardizer.inverse_transform(kmeans_object.cluster_centers_),\n                                columns = Cereals.columns)\ncluster_centroids","04346404":"sse = {}\nfor k in range(1,10):\n    kmeans = KMeans(n_clusters = k, max_iter = 1000).fit(std_Cereals)\n    std_Cereals[\"clusters\"] = kmeans.labels_\n    print(std_Cereals[\"clusters\"])\n    sse[k] = kmeans.inertia_ #Inertia : Sum of distances of samples to their closet cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of Cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","1d08dc6f":"## Standard plotting code copied from sklearn documentation.\n## Just change \"X_matrix\" to the data of your choice\n\nfrom __future__ import print_function\n\nfrom sklearn.datasets import make_blobs\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_samples, silhouette_score\n\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nimport numpy as np\n\nprint(__doc__)\n\n### Just change this to your dataframe\nX_matrix = std_Cereals.as_matrix()\n\nrange_n_clusters = [3, 4, 5, 6, 7, 8, 9, 10, 11]\n\nfor n_clusters in range_n_clusters:\n    # Create a subplot with 1 row and 1 column\n    fig, (ax1) = plt.subplots(1, 1)\n    fig.set_size_inches(9, 5)\n\n    # The 1st subplot is the silhouette plot\n    # The silhouette coefficient can range from -1, 1 but in this example all\n    # lie within [-0.1, 1]\n    ax1.set_xlim([-0.25, .5])\n    # The (n_clusters+1)*10 is for inserting blank space between silhouette\n    # plots of individual clusters, to demarcate them clearly.\n    ax1.set_ylim([0, len(X_matrix) + (n_clusters + 1) * 10])\n\n    # Initialize the clusterer with n_clusters value and a random generator\n    # seed of 10 for reproducibility.\n    clusterer = KMeans(n_clusters=n_clusters, random_state=10)\n    cluster_labels = clusterer.fit_predict(X_matrix)\n\n    # The silhouette_score gives the average value for all the samples.\n    # This gives a perspective into the density and separation of the formed\n    # clusters\n    silhouette_avg = silhouette_score(X_matrix, cluster_labels)\n\n    # Compute the silhouette scores for each sample\n    sample_silhouette_values = silhouette_samples(X_matrix, cluster_labels)\n\n    y_lower = 10\n    for i in range(n_clusters):\n        # Aggregate the silhouette scores for samples belonging to\n        # cluster i, and sort them\n        ith_cluster_silhouette_values = \\\n            sample_silhouette_values[cluster_labels == i]\n\n        ith_cluster_silhouette_values.sort()\n\n        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n        y_upper = y_lower + size_cluster_i\n\n        color = cm.nipy_spectral(float(i) \/ n_clusters)\n        ax1.fill_betweenx(np.arange(y_lower, y_upper),\n                          0, ith_cluster_silhouette_values,\n                          facecolor=color, edgecolor=color, alpha=0.7)\n\n        # Label the silhouette plots with their cluster numbers at the middle\n        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n\n        # Compute the new y_lower for next plot\n        y_lower = y_upper + 10  # 10 for the 0 samples\n\n    ax1.set_title(\"The silhouette plot for the various clusters.\")\n    ax1.set_xlabel(\"The silhouette coefficient values\")\n    ax1.set_ylabel(\"Cluster label\")\n\n    # The vertical line for average silhouette score of all the values\n    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n\n    ax1.set_yticks([])  # Clear the yaxis labels \/ ticks\n    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n\n    plt.suptitle((\"For %d clusters, silhouette avg coeff = %f \" % (n_clusters,silhouette_avg)),\n                 fontsize=14, fontweight='bold')\n    plt.show()","24837637":"best_kmeans = KMeans(n_clusters=7, random_state=1240)\nbest_kmeans.fit(std_Cereals)\nbest_kmeans_labels = best_kmeans.predict(std_Cereals)","33016cc3":"from sklearn.decomposition import PCA\npca_obj = PCA(n_components=3).fit(std_Cereals)\npca_train = pca_obj.transform(std_Cereals)\nprint(pca_obj.explained_variance_ratio_)\nprint(\"Cumulative variance explained\", np.sum(pca_obj.explained_variance_ratio_))","91e730ff":"from mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = plt.axes(projection='3d')\n\nax.scatter(xs=pca_train[:,0], ys=pca_train[:,1], zs=pca_train[:,2],  c=best_kmeans.labels_, depthshade=False)\nplt.show()","bbc531fe":"## Interactive 3D plot","92249b11":"## Clustering - Cereals DataSet","4af6e02e":"### Selecting the best k value - using Elbow plot","11913f19":"#### Aggregating \"name\", \"Shelf\", \"rating\", to make labels","18ccebe1":"### Inspecting cluster centroids to understand average statistics for each cluster","d5268c3a":"## Data Exploration","dca0c668":"**Here we're doing Inverse transform to get the original values bec. on the top we did standardization for comparision**","0920ab30":"### Checking the missing values","cd927ee1":"**Decouple label from the features**","ed3bcb6a":"## Standardization","93cbf7f9":"### Ckecking for NA again","6225fb85":"### K- Means Clustering\n**Parameter description**\n\n**n_cluster:** The number of clusters to find\n\n**tol:** Relative tolerance with regards to inertia to declare convergence\n\n**n_int:** Number of times the k-means algorithm will be run different centroid seeds. The final results will be the best output of n_int consecutive runs in terms of intertia.\n\n**max_int:** max interations of recomputing new cluster centroids\n\n**n_jobs:** the number of jobs to use for the computation. This works by computing each of the n_init runs in parallel.","beba7526":"## Agglomerative Clustering\n\n*parameter description*\n\nn-cluster : The number of clusters to find.\n\nlinkage : {'ward', 'complete', 'average' }\n\n\n1. ward minimizes the variance of the clusters being merged.\n\n2. complete usess the maximum distances between all observations of the two sets.\n\n3. average uses the averages of the distances of each observation of the two sets.\n\n**affinity : {'euclidean', \"l1\", \"l2\", 'manhatten\", \"cosine\"}**\n \n -> Metric used to compute the linkage","a2e2288a":"**Note - Clustering is an unsupervised method and hence we are not concerned about train-test split or prediction accuracies.****","9665541e":"## Imputation","3ec12791":"### Using PCA to VISUALIZE DATA","18554109":"**Check summary statistics**"}}