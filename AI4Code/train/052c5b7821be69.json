{"cell_type":{"c97f25a2":"code","90cb2425":"code","474054cc":"code","1291b774":"code","63f315fe":"code","0e2f89b2":"code","eced40fd":"code","e5f0fb40":"code","51f15ce6":"code","68677708":"code","d623b74e":"code","e3738b76":"code","627bf165":"code","c95479cd":"code","fde5ad62":"code","507baf32":"code","5c357860":"code","85dfdfbd":"code","c3aa9da5":"code","c0c88d2f":"code","ca76425f":"code","e380e40a":"code","e54a60b8":"code","fbba6815":"code","a074313b":"code","6d7108e8":"code","09f9f5cb":"code","077462ee":"code","10573b02":"code","c69cee54":"code","afa76001":"code","14f17069":"code","169f33bb":"code","d58c094a":"code","9f06e95f":"code","b507230c":"code","3fda65a1":"code","4c97aced":"code","4abafc03":"code","2fc43f1f":"code","23d6f171":"code","39c22727":"code","1065dd83":"code","b63582ca":"code","ad6029bd":"code","befac47f":"code","008f7e2f":"code","5c16a320":"code","eff3b808":"code","da7281c5":"code","889faf1d":"code","6cfc19d7":"code","ff740e37":"code","2c9c6523":"code","e9c50635":"code","02467c47":"code","8b6c172a":"code","98f51216":"code","93e02119":"code","317eca7a":"code","81981a16":"code","e70f93aa":"code","0f46588a":"code","05d5c9ba":"code","9ff226bf":"code","b46db10c":"markdown","199f36db":"markdown","ab49f5c3":"markdown","b37646d5":"markdown","9e19c419":"markdown","756c6d03":"markdown","be45b343":"markdown","dbd01a82":"markdown","5e10dfd6":"markdown","371826ad":"markdown","6519aed8":"markdown","07614ebc":"markdown","18ca9060":"markdown","7b24dabb":"markdown","896a0c27":"markdown","30a9b891":"markdown","f4d5ce50":"markdown","60c6efd4":"markdown","153ff9ba":"markdown","ff3db5f2":"markdown","483c305b":"markdown","e8249dcc":"markdown","56846833":"markdown","e979612b":"markdown","23cfe2ec":"markdown","327ce553":"markdown","7a47ed38":"markdown","ffddf2fe":"markdown","561a5055":"markdown","9052b556":"markdown","426351d9":"markdown","f7a9baf4":"markdown","f15c693b":"markdown","202e7352":"markdown","90e623aa":"markdown","3d1ed667":"markdown","538b16d9":"markdown","d53ad615":"markdown","9e08c610":"markdown","d3208c00":"markdown","a425656f":"markdown","0f139c77":"markdown","a42a9123":"markdown","54953734":"markdown","21c11c05":"markdown","0757252a":"markdown","10aee758":"markdown","2afbb1d9":"markdown","0fe38cbd":"markdown","b768b7fe":"markdown","5cdd1ae0":"markdown","3e69bcf8":"markdown","d85cef65":"markdown","3a86eb66":"markdown","fbca8be0":"markdown","32a94b8d":"markdown","cc99f47d":"markdown","63cb22c6":"markdown","93346a16":"markdown"},"source":{"c97f25a2":"# Import Packages. \n\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport os\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport statsmodels.api as sm\nimport category_encoders as ce\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import Ridge\nfrom sklearn import metrics\nfrom sklearn.feature_selection import RFE\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.datasets import make_regression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.datasets import make_regression\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error,r2_score\nfrom matplotlib import pyplot\nfrom xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","90cb2425":"# Import test and training datasets.\ntrain = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\", encoding='latin-1') #the given training dataset from Kaggle\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\", encoding='latin-1') #the true testing dataset using for submission score","474054cc":"# Use head to return first five rows and ensure that importing was done correctly. \ntrain.head()","1291b774":"# Use head to return first five rows and ensure that importing was done correctly. \ntest.head() #Note that the test dataset is used for submission score. The column SalePrice is excluded. ","63f315fe":"# Get info on our training dataset.\ntrain.info()","0e2f89b2":"train.shape #1460 rows, 81 columns","eced40fd":"# Before making transformations, let's make copies of our dataframe to which we can make changes.\ntrain2 = train.copy()\ntest2 = test.copy()","e5f0fb40":"# Check for NaN's in training and testing dataset.\nmissing_values = pd.concat([train.isnull().sum(), test.isnull().sum()], axis = 1, keys = ['TRAIN', 'TEST'])\nmissing_values[missing_values.sum(axis=1)>0]","51f15ce6":"merged_data = train2.append(test2)","68677708":"merged_data = merged_data.drop(['PoolQC','MiscFeature','Alley', 'Id'], axis=1)","d623b74e":"for col in ['LotFrontage', 'MasVnrArea', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n           'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath','FireplaceQu', \n            'KitchenQual', 'Functional', 'FireplaceQu', 'GarageYrBlt','GarageFinish', 'GarageCars', 'GarageArea',\n            'GarageQual','GarageCond', 'Fence']:\n    merged_data[col] = merged_data[col].fillna(0)\nelse:\n    merged_data[col] = merged_data[col].fillna('None')","e3738b76":"ordinal_cols = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n           'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'Functional', 'FireplaceQu', 'GarageFinish',\n           'GarageQual', 'GarageCond', 'Fence']\n\nencoder = ce.OrdinalEncoder(cols=ordinal_cols)\nmerged_data = encoder.fit_transform(merged_data)","627bf165":"merged_data.head()","c95479cd":"# Next, we can create a correlation chart for our quantitative variables and determine if any variables are highly correlated.\ntrain2 = merged_data.iloc[:len(train2)]\ntrain2_quant = train2.select_dtypes(exclude=['object'])\n\nplt.subplots(figsize=(55, 40))\nsns.heatmap(train2_quant.corr('pearson').abs(), annot = True, square = True, linewidths = 0.5)\nplt.show()","fde5ad62":"# Determine strong pairs:\ncor_pairs = train2_quant.corr().unstack()\nstrong_pairs = cor_pairs[((cor_pairs) > 0.8) & ((cor_pairs) < 1.0)]\nstrong_pairs\n\n# Here are variables that are contenders to be cut due to being close to perfectly correlated:","507baf32":"# For these variables, the best alternative would be to drop one of the highly correlated variables.\n# Dropping 'GarageYrBlt', '1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageCars', 'GarageCond'\n\nmerged_data = merged_data.drop(['GarageYrBlt', '1stFlrSF', 'GarageArea', 'TotRmsAbvGrd', 'GarageCars', 'GarageCond'], axis=1)","5c357860":"# Let's also take a look at what variables are most correlated with our dependent variable 'SalesPrice'. \n\nsales_cor = train2.corr('pearson').abs()['SalePrice']\n\n# Sort by strongest correlations. \nsorted_cor_target = sales_cor.sort_values(kind = 'Quicksort', ascending=False)\nsorted_cor_target","85dfdfbd":"train2_quant = train2.select_dtypes(exclude=['object'])\n\nquant_eda = train2.hist(column = train2_quant.columns, figsize = (30,40))","c3aa9da5":"# Applying log transformations to all of the variables and compare the distribution\n\ntrain_quant_log = train2_quant.copy()\n\nfor col in train_quant_log.columns:\n    train_quant_log[col]=np.log1p(train_quant_log[col])\n    \ntrain_quant_log.hist(figsize=(30, 40), bins=50, xlabelsize=8, ylabelsize=8);","c0c88d2f":"# From the above plots, we conclude that logging is effective at smoothing skewness for SalePrice, LotArea, BsmtUnfSF\n# & GrLivArea.\n\nmerged_data['SalePrice'] = np.log1p(merged_data['SalePrice'])\nmerged_data['LotArea'] = np.log1p(merged_data['LotArea'])\nmerged_data['BsmtUnfSF'] = np.log1p(merged_data['BsmtUnfSF'])\nmerged_data['GrLivArea'] = np.log1p(merged_data['GrLivArea'])","ca76425f":"# In previous steps, we have transformed ordinal data by OrdinalEncoding\n# For the rest categorical features, we create one-hot encode them\nobject_cols = merged_data.select_dtypes(include=['object']).columns\ndummy_cols = []\nfor col in object_cols:\n    if col not in ordinal_cols:\n        dummy_cols.append(col)\nmerged_data = pd.get_dummies(merged_data, columns = dummy_cols) \n\nTrain = merged_data.iloc[:len(train2)]\nTest = merged_data.iloc[len(train2):].drop(\"SalePrice\", axis = 1)","e380e40a":"Train.head()","e54a60b8":"Test.head()","fbba6815":"quant_features = list((Train.dtypes != 'object')[Train.dtypes != 'object'].index)\nlow_cor_features = set()\nfor i in quant_features:\n    if abs(Train[i].corr(Train[\"SalePrice\"])) < 0.02:\n        low_cor_features.add(i)\nlow_cor_features","a074313b":"# Create data copies for model 1, and drop the low correlation features\ntrain1 = Train.copy()\ntest1 = Test.copy()\ntrain1 = train1.drop(columns = low_cor_features)\ntest1 = test1.drop(columns = low_cor_features)","6d7108e8":"X_train = train1.drop(\"SalePrice\", axis = 1)\nY_train = train1[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","09f9f5cb":"len(x_train.columns)","077462ee":"linreg = LinearRegression()\nlinreg.fit(x_train, y_train)\n\nlinear_pred = linreg.predict(x_valid)\n\nr2 = linreg.score(x_valid, y_valid)\nprint(\"coefficient of determination: %.2f\" % r2)\nprint(\"adjusted R-square: %.2f\" % (1 - (1-r2)*(len(y_valid)-1)\/(len(y_valid)-x_valid.shape[1]-1)))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid,linear_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid,linear_pred)))","10573b02":"plt.figure(figsize=(10, 5))\nplt.scatter(y_valid, linear_pred, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.tight_layout()","c69cee54":"# Let's compare this feature selection method to recursive feature elimination in linear regression to determine the most \n# effective method of feature selection.\n\nlinreg = LinearRegression()\nrfecv = RFECV(estimator=linreg)\nrfecv.fit(x_train, y_train)\n\nrce_pred = rfecv.predict(x_valid)\n\nprint('Best number of features selected: %d' % rfecv.n_features_)\n\nr2 = rfecv.score(x_valid, y_valid)\nprint(\"coefficient of determination: %.2f\" % r2)\nprint(\"adjusted R-square: %.2f\" % (1 - (1-r2)*(len(y_valid)-1)\/(len(y_valid)-x_valid.shape[1]-1)))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid,rce_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid,rce_pred)))","afa76001":"plt.figure(figsize=(10, 5))\nplt.scatter(y_valid, rce_pred, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.tight_layout()","14f17069":"y_pred = rfecv.predict(test1)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model1.csv', index=False)","169f33bb":"train2 = Train.copy()\ntest2 = Test.copy()","d58c094a":"train2['BsmtFullBath^2'] = np.power(train2['BsmtFullBath'],2)\ntrain2['BsmtFullBath^3'] = np.power(train2['BsmtFullBath'],3)\ntrain2['FullBath^2'] = np.power(train2['FullBath'],2)\ntrain2['FullBath^3'] = np.power(train2['FullBath'],3)\ntrain2['HalfBath^2'] = np.power(train2['HalfBath'],2)\ntrain2['HalfBath^3'] = np.power(train2['HalfBath'],3)\n\ntest2['BsmtFullBath^2'] = np.power(test2['BsmtFullBath'],2)\ntest2['BsmtFullBath^3'] = np.power(test2['BsmtFullBath'],3)\ntest2['FullBath^2'] = np.power(test2['FullBath'],2)\ntest2['FullBath^3'] = np.power(test2['FullBath'],3)\ntest2['HalfBath^2'] = np.power(test2['HalfBath'],2)\ntest2['HalfBath^3'] = np.power(test2['HalfBath'],3)","9f06e95f":"X_train = train2.drop(\"SalePrice\", axis = 1)\nY_train = train2[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","b507230c":"parameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]}\n\nelnet = GridSearchCV(ElasticNet(), parameters, cv = 5, n_jobs=-1, verbose=1)\n\nelnet.fit(x_train, y_train)\n\nprint('Best Model Parameters:', elnet.best_estimator_)\nprint('R2 score:', elnet.best_score_)\n","3fda65a1":"en_best = elnet.best_estimator_\nen_pred_y = en_best.predict(x_valid)\nprint('R2 score:', en_best.score(x_valid, y_valid))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid, en_pred_y))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, en_pred_y)))","4c97aced":"plt.figure(figsize=(10, 5))\nplt.scatter(y_valid, en_pred_y, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.tight_layout()","4abafc03":"en_coef = pd.Series(en_best.coef_, index = x_train.columns)","2fc43f1f":"print(\"Elastic Net selected \" + str(sum(en_coef != 0)) + ' features')\n\nprint(\"Elastic Net eliminated \" + str(sum(en_coef == 0)) + ' features')","23d6f171":"sig_coef = pd.concat([en_coef.sort_values().head(8), en_coef.sort_values().tail(8)])\nplt.figure(figsize=(10, 5))\nplt.title('Significant Variables via Elastic Net Selection')\nplt.xlabel('coefficient')\nsig_coef.plot(kind = 'barh')","39c22727":"y_pred = en_best.predict(test2)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model2.csv', index=False)","1065dd83":"train3 = Train.copy()\ntest3 = Test.copy()","b63582ca":"train3['BsmtFullBath^2'] = np.power(train3['BsmtFullBath'],2)\ntrain3['BsmtFullBath^3'] = np.power(train3['BsmtFullBath'],3)\ntrain3['FullBath^2'] = np.power(train3['FullBath'],2)\ntrain3['FullBath^3'] = np.power(train3['FullBath'],3)\ntrain3['HalfBath^2'] = np.power(train3['HalfBath'],2)\ntrain3['HalfBath^3'] = np.power(train3['HalfBath'],3)\n\ntest3['BsmtFullBath^2'] = np.power(test3['BsmtFullBath'],2)\ntest3['BsmtFullBath^3'] = np.power(test3['BsmtFullBath'],3)\ntest3['FullBath^2'] = np.power(test3['FullBath'],2)\ntest3['FullBath^3'] = np.power(test3['FullBath'],3)\ntest3['HalfBath^2'] = np.power(test3['HalfBath'],2)\ntest3['HalfBath^3'] = np.power(test3['HalfBath'],3)","ad6029bd":"X_train = train3.drop(\"SalePrice\", axis = 1)\nY_train = train3[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","befac47f":"parameters = {'alpha': [1e-5, 1e-4, 1e-3, 1e-2, 1e-1, 0.0, 1.0, 10.0, 100.0]}\n\nridge = GridSearchCV(Ridge(), parameters, cv = 10, n_jobs=-1, verbose=1)\n\nridge.fit(x_train, y_train)\n\nprint('Best Model Parameters:', ridge.best_params_)\nprint('R2 score:', ridge.best_score_)\n","008f7e2f":"ridge_best = ridge.best_estimator_\nridge_pred_y = ridge_best.predict(x_valid)\nprint('R2 score:', ridge_best.score(x_valid, y_valid))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid, ridge_pred_y))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, ridge_pred_y)))","5c16a320":"plt.figure(figsize=(10, 5))\nplt.scatter(y_valid, ridge_pred_y, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.tight_layout()","eff3b808":"y_pred = ridge_best.predict(test3)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model3.csv', index=False)","da7281c5":"# Train\/Validation split\ntrain4 = Train.copy()\ntest4 = Test.copy()\nX_train = train4.drop(\"SalePrice\", axis = 1)\nY_train = train4[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","889faf1d":"#Decision Tree Regressor; we'll run through Cross-Validation with 5 Folds, and will vary \n#the parameters using GridSearchCV to evaluate the best model. \n\nparameters = {\n    'min_samples_split' : range(1,20),\n    'min_samples_leaf' : range(1,20),\n    'max_depth' : range(1,32)\n}\n\nDecision_Tree = GridSearchCV(DecisionTreeRegressor(), parameters, cv = 5, n_jobs=-1, verbose=1)\n\nDecision_Tree.fit(x_train, y_train)\n\nprint('Best Model Parameters:', Decision_Tree.best_estimator_)\nprint('R2 score:', Decision_Tree.best_score_)\n","6cfc19d7":"dt_best = Decision_Tree.best_estimator_\ndt_pred_y = dt_best.predict(x_valid)\nprint('R2 score:', dt_best.score(x_valid, y_valid))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid, dt_pred_y))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid, dt_pred_y)))","ff740e37":"# Prediction Graph for our best Decision Tree. \nplt.figure(figsize=(10, 5))\nplt.scatter(y_valid, dt_pred_y, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.tight_layout()","2c9c6523":"y_pred = dt_best.predict(test4)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model4.csv', index=False)","e9c50635":"# Train\/Validation split\ntrain5 = Train.copy()\ntest5 = Test.copy()\nX_train = train5.drop(\"SalePrice\", axis = 1)\nY_train = train5[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","02467c47":"# Get best fitting model by evaluating different parameters.\nparameters = {\n    'n_estimators'      : [25, 50, 100, 200, 300, 400, 500],\n    'max_depth'         : [15, 20, 25, 30, 35, 40, 45]   \n}\n\nrandom_forest = GridSearchCV(RandomForestRegressor(), parameters, cv = 10, n_jobs=10, verbose=1)\n\nrandom_forest.fit(x_train, y_train)\n\nprint('Best Model Parameters:', random_forest.best_estimator_)\nprint('R2 score:', random_forest.best_score_)","8b6c172a":"rf_best = random_forest.best_estimator_\nrf_pred_y = rf_best.predict(x_valid)\nprint('R2 score:', rf_best.score(x_valid, y_valid))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid,rf_pred_y))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid,rf_pred_y)))","98f51216":"# Plot predicted vs. Actuals \n\nplt.figure(figsize=(10, 5))\nplt.scatter(y_valid,rf_pred_y, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.show()","93e02119":"y_pred = rf_best.predict(test5)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model5.csv', index=False)","317eca7a":"# Train\/Validation split\ntrain6 = Train.copy()\ntest6 = Test.copy()\nX_train = train6.drop(\"SalePrice\", axis = 1)\nY_train = train6[\"SalePrice\"]\nx_train, x_valid, y_train, y_valid = train_test_split(X_train, Y_train, test_size = 0.2,random_state=42)","81981a16":"# Get best fitting model by evaluating different parameters.\nparameters = {\n    'n_estimators'      : [7200],\n    'learning_rate'     : [0.01],\n    'max_depth'         : [4],\n    'min_child_weight'  : [1.5],\n    'gamma'             : [0.0],\n    'reg_alpha'         : [0.9],\n    'subsample'         : [0.2],\n    'seed'              : [42]\n}\n\nxgb = GridSearchCV(XGBRegressor(), parameters, n_jobs=-1, verbose=1)\n\nxgb.fit(x_train, y_train)\n\nprint('Best Model Parameters:', xgb.best_params_)\nprint('R2 score:', xgb.best_score_)","e70f93aa":"xgb_best = xgb.best_estimator_\nxgb_pred_y = xgb_best.predict(x_valid)\nprint('R2 score:', xgb_best.score(x_valid, y_valid))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_valid,xgb_pred_y))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_valid,xgb_pred_y)))","0f46588a":"# Plot predicted vs. Actuals \n\nplt.figure(figsize=(10, 5))\nplt.scatter(y_valid,xgb_pred_y, s=30)\nplt.title('Predicted vs. Actual Sale Price')\nplt.xlabel('Actual Sale Price')\nplt.ylabel('Predicted Sale Price')\n\nplt.plot([min(y_valid), max(y_valid)], [min(y_valid), max(y_valid)])\nplt.show()","05d5c9ba":"y_pred = xgb_best.predict(test6)\ny_pred = np.exp(1)**y_pred\n\ny_test = pd.DataFrame(y_pred, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_test['SalePrice']], axis = 1)\n\ntest_results.to_csv('rf_output_model6.csv', index=False)","9ff226bf":"y_pred2 = en_best.predict(test2)\ny_pred2 = np.exp(1)**y_pred2\n\n\ny_pred6 = xgb_best.predict(test6)\ny_pred6 = np.exp(1)**y_pred6\n\ny_pred_final = (y_pred2+y_pred6)\/2\n\ny_final = pd.DataFrame(y_pred_final, columns=['SalePrice'])\n\ntest_results = pd.concat([test['Id'],y_final['SalePrice']], axis = 1)\n\ntest_results.to_csv('output_final.csv', index=False)","b46db10c":"Train\/Validation split for model2.","199f36db":"We have done some preprocessing of the data. Now we can check if any transformation is necessary.","ab49f5c3":"For this feature selection method, we retained 193 features.","b37646d5":"Candidates for transformation are 2ndFlrSF, BsmtFinSF1, BsmtFinSF2, ScreenPorch, PoolArea, MiscVal, LowQualFinSF, OpenPorchSF, LotArea, SalePrice. \n\nOur dependent variable ('SalePrice') may also be worth examining further.\n\nWe could use a logarithmic transformation to even out the distributions to more normal & smooth outliers.","9e19c419":"Feature selection using Recursive Feature Elimination","756c6d03":"OOS Performance","be45b343":"### NA Values","dbd01a82":"First add degrees 2 or 3 to some suitable features","5e10dfd6":"Note that each feature has 1460 entries. We observe that 'Alley', 'PoolQC', and 'MiscFeature' are missing >90% of cases in both datasets. We can proceed by dropping these columns.\nWe can also drop the ID column, which doesn't provide any value other than identifying a unique row.","371826ad":"Create Submission file for Model 2 - Elastic Net","6519aed8":"Definitely can tune more parameters here. ","07614ebc":"Fit decision tree regresser","18ca9060":"OOS Performance","7b24dabb":"### Multicollinearity","896a0c27":"Create Submission file for Model 3 - Ridge","30a9b891":"## Model 2 - Elastic Net","f4d5ce50":"# Summary","60c6efd4":"Next, for ordinal columns and numeric columns, we repalce NA with 0.","153ff9ba":"# 4. Final Submission","ff3db5f2":"First, drop features with very low correlation with SalePrice (<0.020)","483c305b":"## Model 4 - Decision Tree","e8249dcc":"Create Submission File for Model 5 - Random Forest","56846833":"Fit Elastic Net","e979612b":"### EDA & Transformations","23cfe2ec":"Do a train\/validation split for the training data","327ce553":"### Authors: Qihan Guan, Michael Harris, Luyao Wang","7a47ed38":"## Model 3 - Ridge","ffddf2fe":"### Examine the preprocessed dataset","561a5055":"# 1. Import Packages, data, and view summary statistics.","9052b556":"Since elastic net performs automatic feature selection, we can see how many features are retained and what the most important features are.","426351d9":"# 3. Model building and feature selection","f7a9baf4":"Plot predicted value vs. actual value","f15c693b":"We will combine the predictions from the Elastic Net model and the XGBoost model by averaging them to get our final submission.","202e7352":"Display the significant features:","90e623aa":"Do a train\/validation split for the training data","3d1ed667":"Fit Random Forest","538b16d9":"OOS performance","d53ad615":"## Model 6 - XGBoost","9e08c610":"## Model 1 - Linear Regression","d3208c00":"For this method, we retained 174 features. The performance was roughly the same as the previous method. \nBut a simpler model is always better. We will use this model as the final linear regression model. ","a425656f":"Plot predicted value vs. actual value","0f139c77":"Plot predicted value vs. actual value","a42a9123":"Next, for ordinal columns that are not stored in a numeric format, such as ExterCond and BsmtQual, we use OrdinalEncoder to encode them instead of just dummies. ","54953734":"Fit Ridge","21c11c05":"OOS performance:","0757252a":"Performed EDA, data preprocessing, feature transformation.\\\nApplied 6 models:\n* OLS\n* Elastic Net\n* Ridge \n* Decision Tree\n* Random Forest[](http:\/\/)\n* XGBoost\n","10aee758":"OOS Performance","2afbb1d9":"Train\/Validation split for model 3.","0fe38cbd":"First add degrees 2 or 3 to some suitable features","b768b7fe":"Create Submission File for Model 4 - Decision Tree","5cdd1ae0":"Fit linear regression ","3e69bcf8":"Fit XGBoost","d85cef65":"Plot predicted value vs. actual value","3a86eb66":"Create Submission File for Model 1 - Linear Regression","fbca8be0":"Create submission file for model 6","32a94b8d":"### Categorical Variables: Transforming to Dummy Variables ","cc99f47d":"## Model 5 - Random Forest ","63cb22c6":"Plot predicted value vs. actual value","93346a16":"# 2. Data Preprocessing and EDA"}}