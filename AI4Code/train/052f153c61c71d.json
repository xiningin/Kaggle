{"cell_type":{"b50ac9ff":"code","4a522aea":"code","b7ff0ae2":"code","30cbeef7":"code","28681041":"code","4ec23524":"code","e7fcd4ed":"code","1d66c09f":"code","aed06dc3":"code","bfa07fbd":"code","0a163b87":"code","e753f517":"code","9da857ee":"code","cf40fafd":"code","6fc2a713":"code","df4f8431":"code","fd5aaaa0":"markdown"},"source":{"b50ac9ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4a522aea":"%matplotlib inline\nimport xgboost as xgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\nimport gc\nimport matplotlib.pyplot as plt\nimport shap\n\n# load JS visualization code to notebook\nshap.initjs()\nxgb.__version__","b7ff0ae2":"train = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/test.csv')\nsub = pd.read_csv('..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv')","30cbeef7":"train.head()","28681041":"train.shape","4ec23524":"columns = test.columns[1:]\ncolumns","e7fcd4ed":"claim = train['claim'].values","1d66c09f":"cat_features = columns[:19]\ncat_features","aed06dc3":"def label_encode(train_df, test_df, column):\n    le = LabelEncoder()\n    new_feature = \"{}_le\".format(column)\n    le.fit(train_df[column].unique().tolist() + test_df[column].unique().tolist())\n    train_df[new_feature] = le.transform(train_df[column])\n    test_df[new_feature] = le.transform(test_df[column])\n    return new_feature","bfa07fbd":"cat_cols = [col for col in columns if 'cat' in col]\ncont_cols = [col for col in columns if 'cont' in col]","0a163b87":"le_cols = []\nfor feature in cat_cols:\n    le_cols.append(label_encode(train, test, feature))","e753f517":"columns = le_cols + cont_cols","9da857ee":"xgb_params=  {'learning_rate': 0.005,\n              'objective': 'binary:logistic',\n              'eval_metric': 'auc',\n                'tree_method': 'gpu_hist',\n                'predictor': 'gpu_predictor',\n                'gpu_id': 0,\n                'max_bin': 623,\n                'max_depth': 10,\n                'alpha': 0.5108154566815425,\n                'gamma': 1.9276236172849432,\n                'reg_lambda': 11.40999855634382,\n                'colsample_bytree': 0.705851334291963,\n                'subsample': 0.8386116751473301,\n                'min_child_weight': 2.5517043283716605}","cf40fafd":"test = xgb.DMatrix(test[columns])","6fc2a713":"train_oof = np.zeros((train[columns].shape[0],))\ntest_preds = 0\ntrain_oof_shap = np.zeros((train[columns].shape[0],train[columns].shape[1]+1))\ntest_preds_shap = 0\ntrain_oof_shap.shape","df4f8431":"NUM_FOLDS = 10\nkf = KFold(n_splits=NUM_FOLDS, shuffle=True, random_state=0)\n\nfor f, (train_ind, val_ind) in tqdm(enumerate(kf.split(train, claim))):\n        #print(f'Fold {f}')\n        train_df, val_df = train.iloc[train_ind][columns], train.iloc[val_ind][columns]\n        train_claim, val_claim = claim[train_ind], claim[val_ind]\n        \n        train_df = xgb.DMatrix(train_df, label=train_claim)\n        val_df = xgb.DMatrix(val_df, label=val_claim)\n        \n        model =  xgb.train(xgb_params, train_df,7000)\n        temp_oof = model.predict(val_df)\n        temp_oof_shap = model.predict(val_df, pred_contribs=True)\n        temp_test = model.predict(test)\n        temp_test_shap = model.predict(test, pred_contribs=True)\n\n        train_oof[val_ind] = temp_oof\n        test_preds += temp_test\/NUM_FOLDS\n        \n        train_oof_shap[val_ind, :] = temp_oof_shap\n        test_preds_shap += temp_test_shap\/NUM_FOLDS\n        \n        \n        print(roc_auc_score(val_claim, temp_oof))\n        \nprint(roc_auc_score(claim, train_oof))","fd5aaaa0":"In this notebook we'll explore feature importance using SHAP values. SHAP values are the most mathematically consistent way for getting feature importances, and they work particulalry nicely with the tree-based models. Unfortunately, calculating SHAP values is an extremely resource intensive process. However, starting with XGBoost 1.3 it is possible to calcualte these values on GPUs, which speeds up the process by a factor of 20X - 50X compared to calculating the same on a CPU. Furthermore, it is also possible to calculate SHAP values for feature interactions. The GPU speedup for those is even more dramatic - it takes a few minutes, as opposed to days or even longer on a CPU."}}