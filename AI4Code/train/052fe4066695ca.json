{"cell_type":{"16dcc2ed":"code","3f317d8d":"code","05dd82de":"code","b1b807ad":"code","424a292b":"code","856fa216":"code","2661905c":"code","3be8022d":"code","85060247":"code","f8f9e2d6":"code","5845396a":"code","dca85ba2":"code","4594e9b9":"code","38f67c6e":"code","8b9092c8":"code","0b0c0109":"code","3faad3ae":"code","9327a55e":"code","fa44bc3d":"code","7b776866":"code","3e1fb184":"code","45e64837":"code","4266c9da":"code","b2619781":"code","3bb51aaa":"code","7402db62":"code","99e641db":"code","93b55c6a":"code","f944053e":"code","b2b29d7f":"code","0ae9e627":"code","d60e9fe2":"code","20018a53":"code","32a80cda":"code","4da47dc9":"code","0e4f4c48":"code","d724b26c":"code","9c614414":"code","3223c1de":"code","5eb2c152":"code","451ac3d9":"code","6aae0025":"code","586694e5":"code","d5aaa354":"code","1e4f134f":"code","e28c1f22":"code","fe9ef933":"code","b38348aa":"code","7976910d":"code","e1a9cf1f":"code","f0430d5b":"code","46cfab42":"markdown","8642c72d":"markdown","3af531ff":"markdown","cb120142":"markdown","e5074947":"markdown","db91a193":"markdown","47bfab97":"markdown","a70a5e8a":"markdown","ca2d7e16":"markdown","d4cdd9e0":"markdown","f12bdd74":"markdown","224f1707":"markdown","3dd28d1f":"markdown","2f2237e4":"markdown","0307f7b9":"markdown","90282ab4":"markdown","8559ea5c":"markdown","02c846aa":"markdown","5b01e198":"markdown","3cb0d70d":"markdown","178f67ff":"markdown","e274f5b2":"markdown","38bafd4d":"markdown","054ef8f6":"markdown","013b1e91":"markdown","06228e62":"markdown","e0e768c8":"markdown","d90fab4e":"markdown","31531a17":"markdown","c4b4e37e":"markdown","7741ec89":"markdown","d14fd566":"markdown","32a9fdb6":"markdown","e3548fde":"markdown","aa912a95":"markdown","8ccd0aa8":"markdown","da80fc00":"markdown","c90aa53d":"markdown","fb5d9658":"markdown","61e57f7a":"markdown","d27c435f":"markdown","bc62c878":"markdown","8342753b":"markdown","724f87f3":"markdown","1d5c0cce":"markdown","12738f3f":"markdown","737d4469":"markdown","afeaf88b":"markdown","b5d34ce8":"markdown","d106cbc3":"markdown","ebb92c43":"markdown","afce81a1":"markdown","f4b7e6ab":"markdown","d6ba2ac0":"markdown","014940bc":"markdown","b8f9c555":"markdown","8935dac9":"markdown","40d4b4e8":"markdown","1dff0173":"markdown","15650652":"markdown","9ed134c0":"markdown","a510fd61":"markdown","750ad915":"markdown","d1f14fbc":"markdown","96603b6f":"markdown","e6486783":"markdown"},"source":{"16dcc2ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport glob\nimport plotly.express as px\nimport warnings\nimport plotly as py\nimport plotly.graph_objs as go\nimport collections\nimport seaborn as sns\nfrom datetime import datetime\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.metrics import mean_squared_error,classification_report\nimport statsmodels.api as sm\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\n\n\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom yellowbrick.cluster import KElbowVisualizer\n\n# Local server SQL database\nimport sqlite3 as sq\n\nimport sys\n\nimport pandas_datareader.data as web\nimport statsmodels.formula.api as smf\nimport statsmodels.tsa.api as smt\nimport statsmodels.api as sm\nimport scipy.stats as scs\nimport matplotlib as mpl\np = print\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f317d8d":"! pip install pmdarima\nfrom pmdarima.arima import auto_arima\nfrom pmdarima.model_selection import train_test_split\nfrom pmdarima import arima\nfrom pmdarima import datasets\nfrom pmdarima import utils","05dd82de":"# Set-up a connection to a newly named project1.db\ncon = sq.connect('project1.db')\ncur = con.cursor()\n# Get the count of customers_dataset table\ncur.execute(''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv' OR name='customers_dataset' ''')\n\n# If the count is 1, then customers_dataset table already exists\nif cur.fetchone()[0]==1 : {\n    print('Table already created.')\n}\nelse: # Read all files from csv to db format\n    path = '..\/input\/brazilian-ecommerce'\n    all_files = glob.glob(path + \"\/*.csv\")\n    for file in all_files: # For all files in our directory\n        df = pd.read_csv(file, index_col=0) # Read each CSV file\n        df.to_sql(file, con) # Create the read file as a table in the database.","b1b807ad":"# Get the count of customers_dataset table\ncur.execute(''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='..\/input\/marketing-funnel-olist\/olist_closed_deals_dataset.csv' OR name='deals_dataset' ''')\n# If the count is 1, then customers_dataset table already exists\nif cur.fetchone()[0]==1 : {\n    print('Table already created.')\n}\nelse: # Read all files from csv to db format\n    path = '..\/input\/marketing-funnel-olist'\n    all_files = glob.glob(path + \"\/*.csv\")\n    for file in all_files: # For all files in our directory\n        df = pd.read_csv(file, index_col=0) # Read each CSV file\n        df.to_sql(file, con) # Create the read file as a table in the database.","424a292b":"# Get the count of customers_dataset table\ncur.execute(''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='customers_dataset' ''')\n\n# If the count is 1, then customers_dataset table already renamed\nif cur.fetchone()[0]==1:\n\tprint('Table already renamed.')\n\nelse:\n    # Rename all tables\n    rename_tables_query1 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_customers_dataset.csv\" RENAME TO \"customers_dataset\"'\n    rename_tables_query2 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_geolocation_dataset.csv\" RENAME TO \"geolocation_dataset\"'\n    rename_tables_query3 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_orders_dataset.csv\" RENAME TO \"orders_dataset\"'\n    rename_tables_query4 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_order_items_dataset.csv\" RENAME TO \"order_items_dataset\"'\n    rename_tables_query5 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_order_payments_dataset.csv\" RENAME TO \"order_payments_dataset\"'\n    rename_tables_query6 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_order_reviews_dataset.csv\" RENAME TO \"order_reviews_dataset\"'\n    rename_tables_query7 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_products_dataset.csv\" RENAME TO \"products_dataset\"'\n    rename_tables_query8 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/olist_sellers_dataset.csv\" RENAME TO \"sellers_dataset\"'\n    rename_tables_query9 = 'ALTER TABLE \"..\/input\/brazilian-ecommerce\/product_category_name_translation.csv\" RENAME TO \"product_category_name_translation\"'\n\n    # The function read_sql takes a query string and a database connection, and performs the query.\n    rename_tables1 = pd.read_sql(rename_tables_query1, con)\n    rename_tables2 = pd.read_sql(rename_tables_query2, con)\n    rename_tables3 = pd.read_sql(rename_tables_query3, con)\n    rename_tables4 = pd.read_sql(rename_tables_query4, con)\n    rename_tables5 = pd.read_sql(rename_tables_query5, con)\n    rename_tables6 = pd.read_sql(rename_tables_query6, con)\n    rename_tables7 = pd.read_sql(rename_tables_query7, con)\n    rename_tables8 = pd.read_sql(rename_tables_query8, con)\n    rename_tables9 = pd.read_sql(rename_tables_query9, con)","856fa216":"# Get the count of customers_dataset table\ncur.execute(''' SELECT count(name) FROM sqlite_master WHERE type='table' AND name='deals_dataset' ''')\n\n# If the count is 1, then customers_dataset table already renamed\nif cur.fetchone()[0]==1:\n\tprint('Table already renamed.')\n\nelse:\n    # Rename all tables\n    rename_tables_query10 = 'ALTER TABLE \"..\/input\/marketing-funnel-olist\/olist_closed_deals_dataset.csv\" RENAME TO \"deals_dataset\"'\n    rename_tables_query11 = 'ALTER TABLE \"..\/input\/marketing-funnel-olist\/olist_marketing_qualified_leads_dataset.csv\" RENAME TO \"mql_dataset\"'\n    \n\n    # The function read_sql takes a query string and a database connection, and performs the query.\n    rename_tables10 = pd.read_sql(rename_tables_query10, con)\n    rename_tables11 = pd.read_sql(rename_tables_query11, con)\n    ","2661905c":"# Read all table names\ntable_list = [a for a in cur.execute(\"SELECT name FROM sqlite_master WHERE type = 'table'\")]\n\n# Table list\nprint(table_list)","3be8022d":"# Number of customers per state\n\nq1 = '''SELECT count(customer_unique_id) AS N_Customers, customer_state as State\n        FROM customers_dataset\n        GROUP BY customer_state\n        ORDER BY count()'''\nr1 = pd.read_sql(q1, con)\n\nplt.style.use('seaborn')\nplt.figure(figsize=(15,10))\nplt.bar(r1['State'], r1['N_Customers'])\nplt.title('Number of customers per state')\nplt.show()","85060247":"# Total Orders and Total Sales\nq2 = (\n      'SELECT count(a.order_id) AS Total_Orders, '\n      '       sum(b.price + b.freight_value) AS Total_Sales '\n      'FROM orders_dataset AS a '\n      'INNER JOIN order_items_dataset AS b '\n      'ON a.order_id = b.order_id '\n      )\n\nr2 = pd.read_sql(q2, con)\nr2","f8f9e2d6":"# Orders by Time\nq3 = (\n      'SELECT strftime(\"%Y-%m\", order_approved_at) AS date, '\n      '       COUNT(order_id) AS order_qty '\n      'FROM orders_dataset '\n      'GROUP BY date '\n      'ORDER BY date '\n      )\n\nr3 = pd.read_sql(q3, con)\n\n\n# Plotting orders by time data \nfig1 = px.bar(r3, x=\"date\", y=\"order_qty\", orientation='v', title='Orders by Date in Brazilian E-Commerce (2016-2018)')\niplot(fig1)\n","5845396a":"# Write a SQL query of Sales by Time\nq4 = (\n      'SELECT strftime(\"%Y-%m\", a.order_approved_at) AS date, '\n      '       SUM(b.price) AS sales '\n      'FROM orders_dataset AS a '\n      'INNER JOIN order_items_dataset AS b '\n      'ON a.order_id = b.order_id '\n      'GROUP BY date '\n      'ORDER BY date '\n      )\n\n# Convert the SQL query to Pandas data Frame\nr4 = pd.read_sql(q4, con)\n\n# Plotting sales by time data \nfig2 = px.line(r4, x=\"date\", y=\"sales\", title='Sales by Time in Brazilian E-Commerce (2016-2018)')\niplot(fig2)","dca85ba2":"#Selling Product Categories Quantity\nq5 = (\n      'SELECT a.product_category_name_english AS product, '\n      '       COUNT(b.product_category_name) AS qty '\n      'FROM product_category_name_translation AS a '\n      'INNER JOIN products_dataset AS b '\n      'ON a.product_category_name = b.product_category_name '\n      'GROUP BY product '\n      'ORDER BY qty DESC '\n      )\n\nr5 = pd.read_sql(q5, con)\n\n# Plotting selling product categories quantity data\nfig3 = px.bar(r5.head(20), x=\"qty\", y=\"product\", orientation='h', barmode=\"group\",title='Top 20 Selling Product Categories in Brazilian E-Commerce (2016-2018)')\nfig3.update_layout(yaxis={'categoryorder':'total ascending'})\niplot(fig3)","4594e9b9":"# Sales by month\n\nq6 = (\n      'SELECT strftime(\"%m\", a.order_approved_at) AS date, '\n      '       SUM(b.price) AS sales '\n      'FROM orders_dataset AS a '\n      'INNER JOIN order_items_dataset AS b '\n      'ON a.order_id = b.order_id '\n      'GROUP BY date '\n      'ORDER BY date '\n      )\n\n# Convert the SQL query to Pandas data Frame\nr6 = pd.read_sql(q6, con)\n\n# Plotting orders by time data \nfig4 = px.bar(r6, x=\"date\", y=\"sales\", orientation='v', title='Orders by month in Brazilian E-Commerce (2016-2018)')\niplot(fig4)","38f67c6e":"# sales by category\n\nq7 = (\n      'SELECT SUM(a.price) AS sales , c.product_category_name_english AS product '\n      'FROM order_items_dataset AS a '\n      'INNER JOIN products_dataset AS b '\n      'ON a.product_id = b.product_id '\n      'INNER JOIN product_category_name_translation AS c '\n      'ON b.product_category_name = c.product_category_name '\n      'GROUP BY product '\n      )\n\n\nr7 = pd.read_sql(q7, con)\n\n# Plotting selling product categories quantity data\nfig5 = px.bar(r7.head(20), x=\"sales\", y=\"product\", orientation='h', barmode=\"group\", title='Sales by Categories in Brazilian E-Commerce (2016-2018)')\nfig5.update_layout(yaxis={'categoryorder':'total ascending'})\niplot(fig5)","8b9092c8":"# Avg price by category\nq8 = (\n      'SELECT AVG(a.price) AS Avg_Price, c.product_category_name_english AS product '\n      'FROM order_items_dataset AS a '\n      'INNER JOIN products_dataset AS b '\n      'ON a.product_id = b.product_id '\n      'INNER JOIN product_category_name_translation AS c '\n      'ON b.product_category_name = c.product_category_name '\n      'GROUP BY product '\n      )\n\nr8 = pd.read_sql(q8, con)\n\n\n# Plotting selling product categories quantity data\nfig6 = px.bar(r8.head(20), x=\"Avg_Price\", y=\"product\", orientation='h', barmode=\"group\", title='Avg Price by Categories in Brazilian E-Commerce (2016-2018)')\nfig6.update_layout(yaxis={'categoryorder':'total ascending'})\niplot(fig6)","0b0c0109":"# most common payment method\n\nq9 = ('''\n      SELECT payment_type, COUNT(*) as N_payments\n      FROM order_payments_dataset\n      GROUP BY payment_type\n       \n     ''' )\n\nr9 = pd.read_sql(q9, con)\n\nfig7 = px.pie(r9, values='N_payments', names='payment_type', title='Number of payments by Payment type')\niplot(fig7)","3faad3ae":"# average score by category\n\nq10 = ('''\n      SELECT AVG(a.review_score) as \"Avg_Review_Score\", d.product_category_name_english AS product\n      FROM order_reviews_dataset AS a\n      INNER JOIN order_items_dataset AS b\n      ON a.order_id = b.order_id\n      INNER JOIN products_dataset AS c\n      ON b.product_id = c.product_id\n      INNER JOIN product_category_name_translation AS d\n      ON c.product_category_name = d.product_category_name\n      GROUP BY product    \n      \n     ''' )\n\nr10 = pd.read_sql(q10, con)\n\n\n# Plotting selling product categories quantity data\nfig8 = px.bar(r10, x=\"Avg_Review_Score\", y=\"product\", orientation='h', barmode=\"group\", title='Avg Review Score by Categories')\nfig8.update_layout(yaxis={'categoryorder':'total ascending'})\niplot(fig8)\n","9327a55e":"# relationship between avg price, N of categories, avg review score and sales\nsale_price_cat_relation = pd.merge(r7,r8, on=\"product\")\nsale_price_cat_relation = sale_price_cat_relation.merge(r5,on = \"product\")\nsale_price_cat_relation = sale_price_cat_relation.merge(r10,on = 'product')\nsns.pairplot(sale_price_cat_relation, height=1.7, aspect = 1.7)","fa44bc3d":"# About the delivery speed\n\nq11 = ('''\n      SELECT strftime(\"%Y-%m-%d\", order_delivered_customer_date) AS delivered_date, \n      strftime(\"%Y-%m-%d\", order_purchase_timestamp) as purchased_date\n      FROM orders_dataset   \n      WHERE order_status = 'delivered'\n\n     ''' )\n\nr11 = pd.read_sql(q11, con)\n\nr11['delivered_date']= pd.to_datetime(r11['delivered_date'])\nr11['purchased_date']= pd.to_datetime(r11['purchased_date'])\n\nr11['Difference'] = (r11['delivered_date'] - r11['purchased_date']).dt.days\n\n\nfig = sns.displot(r11['Difference'],bins = 50,height= 5, aspect= 2)\nplt.title('Delivery speed in number of days')\nplt.xlabel(\"Diference\")\nplt.ylabel(\"Count\")","7b776866":"r11['Difference'].describe([0.8,0.85,0.90,0.99])","3e1fb184":"# Write a SQL query of Sales by Time\nq12 = (\n      'SELECT strftime(\"%Y-%m-%d\", a.order_approved_at) AS date, '\n      '       SUM(b.price) AS sales '\n      'FROM orders_dataset AS a '\n      'INNER JOIN order_items_dataset AS b '\n      'ON a.order_id = b.order_id '\n      'GROUP BY date '\n      'ORDER BY date '\n      )\n\n# Convert the SQL query to Pandas data Frame\nr12 = pd.read_sql(q12, con)\n\n# Plotting sales by time data \nfig = px.line(r12, x=\"date\", y=\"sales\", title='Sales by day in Brazilian E-Commerce (2016-2018)')\niplot(fig)","45e64837":"def tsplot(y, lags=None, figsize=(10, 8), style='bmh'):\n    if not isinstance(y, pd.Series):\n        y = pd.Series(y)\n    with plt.style.context(style):    \n        fig = plt.figure(figsize=figsize)\n        #mpl.rcParams['font.family'] = 'Ubuntu Mono'\n        layout = (3, 2)\n        ts_ax = plt.subplot2grid(layout, (0, 0), colspan=2)\n        acf_ax = plt.subplot2grid(layout, (1, 0))\n        pacf_ax = plt.subplot2grid(layout, (1, 1))\n        qq_ax = plt.subplot2grid(layout, (2, 0))\n        pp_ax = plt.subplot2grid(layout, (2, 1))\n        \n        y.plot(ax=ts_ax)\n        ts_ax.set_title('Time Series Analysis Plots')\n        smt.graphics.plot_acf(y, lags=lags, ax=acf_ax, alpha=0.5)\n        smt.graphics.plot_pacf(y, lags=lags, ax=pacf_ax, alpha=0.5)\n        sm.qqplot(y, line='s', ax=qq_ax)\n        qq_ax.set_title('QQ Plot')        \n        scs.probplot(y, sparams=(y.mean(), y.std()), plot=pp_ax)\n\n        plt.tight_layout()\n    return ","4266c9da":"# Eliminate the unknown date,the first and the last which where incomplete\nr12.drop([0,1,610], inplace = True)\nr12.reset_index(drop = True, inplace = True)","b2619781":"tsplot(r12.sales,30)","3bb51aaa":"# evaluating a differentiated serie\n_ = tsplot(np.diff(r12.sales), lags=30)","7402db62":"# evaluating a log serie\n_ = tsplot(np.log(r12.sales), lags=30)","99e641db":"#Stationarity test for the differentiated serie\n\nX = np.diff(r12.sales)\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","93b55c6a":"# Splitting data and adjusting best ARIMA model\n\ntrain, test = train_test_split(r12, train_size=0.90)\n\narima_model = auto_arima(np.log(train.sales), start_p=0, start_q=0,start_d=0, start_P=0, start_Q=0,\n                    max_p=5, max_q=5,max_d=3, seasonal=True,m = 30,\n                     stepwise=True, suppress_warnings=True,\n                    error_action='ignore')\narima_model.summary()","f944053e":"# constructing the predictions\ntrain, test = train_test_split(r12, train_size=0.90)\nhistory = [x for x in train.sales.values]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\tmodel = sm.tsa.statespace.SARIMAX(history, trend='c', order=(1,1,1), seasonal_order = (1,0,1,30))\n\tmodel_fit = model.fit()\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test.sales.values[t]\n\thistory.append(obs)\n# evaluate forecasts\nrmse = np.sqrt(mean_squared_error(test.sales.values, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(test.sales.values,label='Expected')\nplt.plot(predictions, color='red',label='Predicted')\nplt.legend()\nplt.show()","b2b29d7f":"# TSA by week\nr12['date'] = pd.to_datetime(r12['date'])\nweekly = r12.groupby(r12['date'].dt.week).sum()\n\nfig = px.line(weekly, x=weekly.index, y=\"sales\", title='Sales by Time in Brazilian E-Commerce (2016-2018)')\niplot(fig)","0ae9e627":"tsplot(weekly.sales,4)","d60e9fe2":"# differenciated weekly serie\n\ntsplot(np.diff(weekly.sales),4)","20018a53":"#Stationarity test for the differentiated serie\n\nX = np.diff(weekly.sales)\nresult = adfuller(X)\nprint('ADF Statistic: %f' % result[0])\nprint('p-value: %f' % result[1])\nprint('Critical Values:')\nfor key, value in result[4].items():\n\tprint('\\t%s: %.3f' % (key, value))","32a80cda":"# Splitting data and adjusting best ARIMA model\n\ntrain, test = train_test_split(weekly, train_size=0.80)\n\narima_model2 = auto_arima(train.sales, start_p=0, start_q=0,start_d=0, start_P=0, start_Q=0,\n                    max_p=5, max_q=5,max_d=3, seasonal=True,m = 4,\n                     stepwise=True, suppress_warnings=True,\n                    error_action='ignore')\narima_model.summary()","4da47dc9":"train, test = train_test_split(weekly, train_size=0.80)\nhistory = [x for x in train.sales.values]\npredictions = list()\n# walk-forward validation\nfor t in range(len(test)):\n\tmodel = sm.tsa.statespace.SARIMAX(history, trend='c', order=(1,1,1), seasonal_order = (1,0,1,4))\n\tmodel_fit = model.fit()\n\toutput = model_fit.forecast()\n\tyhat = output[0]\n\tpredictions.append(yhat)\n\tobs = test.sales.values[t]\n\thistory.append(obs)\n# evaluate forecasts\nrmse = np.sqrt(mean_squared_error(test.sales.values, predictions))\nprint('Test RMSE: %.3f' % rmse)\n# plot forecasts against actual outcomes\nplt.plot(test.sales.values,label='Expected')\nplt.plot(predictions, color='red',label='Predicted')\nplt.legend()\nplt.show()","0e4f4c48":"# time series to supevised learning from machine learning mastery\n\n# transform a time series dataset into a supervised learning dataset\ndef series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n\tn_vars = 1 if type(data) is list else data.shape[1]\n\tdf = pd.DataFrame(data)\n\tcols = list()\n\t# input sequence (t-n, ... t-1)\n\tfor i in range(n_in, 0, -1):\n\t\tcols.append(df.shift(i))\n\t# forecast sequence (t, t+1, ... t+n)\n\tfor i in range(0, n_out):\n\t\tcols.append(df.shift(-i))\n\t# put it all together\n\tagg = pd.concat(cols, axis=1)\n\t# drop rows with NaN values\n\tif dropnan:\n\t\tagg.dropna(inplace=True)\n\treturn agg.values","d724b26c":"# model with standar scaler\nsl_data = series_to_supervised(list(r12.sales))\ntrain, test = train_test_split(sl_data, train_size=0.90)\ns = StandardScaler()\nX_train_s = s.fit_transform(train[:,0].reshape(-1,1))\ny_train_s = s.transform(train[:,1].reshape(-1,1))\nforest = RandomForestRegressor(n_estimators= 400)\nforest.fit(X_train_s,y_train_s)","9c614414":"X_test_s = s.transform(test[:,0].reshape(-1,1))\ny_test_s = s.transform(test[:,1].reshape(-1,1))\n\nyhat = forest.predict(X_test_s)\n\nplt.figure(figsize=(12,8))\nplt.plot(test[:,1], label='Expected')\nplt.plot(s.inverse_transform(yhat), label='Predicted')\nplt.legend()\nplt.show()","3223c1de":"rmse = np.sqrt(mean_squared_error(test[:,0].reshape(-1,1), s.inverse_transform(yhat)))\nprint('Test RMSE: %.3f' % rmse)","5eb2c152":"\n# average score by category\n\nq14 = ('''\n      SELECT a.customer_id,b.order_approved_at,c.order_item_id, c.price, f.product_category_name_english, b.order_id, d.review_score, g.payment_value\n      FROM customers_dataset AS a\n      INNER JOIN orders_dataset AS b\n      ON a.customer_id = b.customer_id\n      INNER JOIN order_items_dataset AS c\n      ON b.order_id = c.order_id\n      INNER JOIN order_reviews_dataset AS d\n      ON c.order_id = d.order_id\n      INNER JOIN products_dataset AS e\n      ON c.product_id = e.product_id\n      INNER JOIN product_category_name_translation AS f\n      ON e.product_category_name = f.product_category_name\n      INNER JOIN order_payments_dataset AS g\n      ON b.order_id = g.order_id  \n      WHERE  b.order_status = 'delivered'\n\n     ''' )\n\n\nr14 = pd.read_sql(q14, con)","451ac3d9":"Data_Corte = datetime(2018,9,13)\nr14['order_approved_at2'] = pd.to_datetime(r14['order_approved_at'])\n\nr14['order_approved_at3'] = r14['order_approved_at2']\n","6aae0025":"# Creating Recency, Frequency, and Total Amount Spent - Customer View (each df line is a customer)\ndf1 = r14.groupby('customer_id').agg({'order_approved_at2': lambda x: (Data_Corte - x.max()).days, 'order_id': lambda x: len(x), 'payment_value': lambda x: x.sum(),'order_approved_at3': lambda x: (Data_Corte - x.min()).days,'review_score':np.average, 'product_category_name_english':scs.mode})\n\n\n#df1['order_approved_at2'] = df1['order_approved_at2'].astype(int)\n#df1['order_approved_at3'] = df1['order_approved_at3'].astype(int)\n\ndf1.rename(columns={'order_approved_at2': 'Recency', \n                    'order_id': 'Frequency', \n                    'payment_value': 'Monetary_Value',\n                    'order_approved_at3': 'Client_Since',\n                   'product_category_name_english': 'common_category',\n                    'review_score': 'average_review_perClient'}, inplace=True)\n\ndf1.dropna(inplace = True)\n\nl=list()\n\nfor i in df1.common_category:\n    l.append(i[0][0])\ndf1[\"common_category\"] = l\n\ndf1.head()\n","586694e5":"# getting data ready for the model\n\n\nnum_attributes = ['Recency', 'Frequency', 'Monetary_Value', 'Client_Since']\ncat_attributes = ['average_review_perClient', 'common_category']\n\npipeline = ColumnTransformer([\n        ('num', StandardScaler(), num_attributes),\n        ('cat', OneHotEncoder(), cat_attributes),\n])\ndf_prepared = pipeline.fit_transform(df1)\ndf_prepared","d5aaa354":"df_prepared = df_prepared.toarray()\npca = PCA(n_components=0.95)\ndf_reduced = pca.fit_transform(df_prepared)\ndf_reduced.shape","1e4f134f":"# iterations\nk_range = range(2, 15)\nkmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(df_reduced)\n                for k in k_range]\ninertias = [model.inertia_ for model in kmeans_per_k]","e28c1f22":"# plotting\nplt.figure(figsize=(15, 8))\nplt.plot(k_range, inertias, 'bo-')\nplt.xlabel('K', fontsize=16)\nplt.ylabel('Inertia', fontsize=16)\nplt.show()","fe9ef933":"model = KMeans()\nvisualizer = KElbowVisualizer(model, k=(2,12))\n\nvisualizer.fit(df_reduced)        # Fit the data to the visualizer\nvisualizer.show() ","b38348aa":"best_model = kmeans_per_k[2]\nbest_model","7976910d":"y_pred = best_model.fit_predict(df_reduced)\nfor i in range(4):\n    print(f\"cluster {i + 1} contains: {np.sum(y_pred == i)} customers\")","e1a9cf1f":"#Assingningcluster to the customers\ndf1[\"cluster\"] = y_pred\ndf1.groupby('cluster').agg({'Recency': np.average, \n                            'Frequency': np.average,\n                            'Client_Since': np.average,\n                            'average_review_perClient':np.average,\n                            'Monetary_Value': np.average, \n                            'common_category': scs.mode })","f0430d5b":"#Some visualizations about the clusters\n\nplt.figure(figsize = (20,15))\nplt.subplot(4,1,1)\nsns.scatterplot(x = 'Frequency', y = 'Monetary_Value',hue='cluster',data = df1,legend='full',palette=\"Set1\")\nplt.subplot(4,1,2)\nsns.scatterplot(x = 'Client_Since', y = 'Monetary_Value',hue='cluster', data = df1,legend='full',palette=\"Set1\")\nplt.subplot(4,1,3)\nsns.scatterplot(x = 'average_review_perClient', y = 'Frequency',hue='cluster',data= df1,legend='full',palette=\"Set1\")\nplt.subplot(4,1,4)\nsns.scatterplot(x = 'average_review_perClient', y = 'Monetary_Value',hue='cluster',data= df1,legend='full',palette=\"Set1\")\nplt.show()","46cfab42":"### Precio Promedio por categor\u00eda","8642c72d":"### Utilizando Random Forest para predecir ventas diarias","3af531ff":"Nuevamente ajustamos los mejores par\u00e1metros y con una funci\u00f3n iterativa hacemos validaci\u00f3n cruzada y graficamos las predicciones del modelo. Aunque pareciera tener mejor desempe\u00f1o, realmente el modelo comet un error mucho m\u00e1s grande, como se puede notar en el RMSE (ra\u00edz de la suma de cuadrados del error)","cb120142":"Como es l\u00f3gico, la capital es la que m\u00e1s clientes tiene con gran diferencia. Le siguen R\u00edo de Janeiro y Minas Gerais. \nTambi\u00e9n vemos el total de ventas en millones de d\u00f3lares","e5074947":"References\n\n\n* https:\/\/www.kaggle.com\/fldcosta\/olist-sales-prediction\n* https:\/\/www.kaggle.com\/muhammadrifki\/data-visualization-aggregation-with-python-sql\n* http:\/\/www.blackarbs.com\/blog\/time-series-analysis-in-python-linear-models-to-garch\/11\/1\/2016\n* https:\/\/machinelearningmastery.com\/\n* https:\/\/www.kaggle.com\/yacinerouizi\/e-commerce-customer-segmentation","db91a193":"<b style=\"font-family: Arials; line-height: 2; font-size: 18px; font-weight: bold; letter-spacing: 2px; text-align: center\">Algunas conclusiones del an\u00e1lisis de series de tiempo<\/b>\n\n<hr style=\"height: 0.5px; border: 0; background-color: #808080\">\n\n* Las ventas tiene un comportamiento creciente pero su varianza no es estable\n* La serie cumple los requisitos de estacionariedad para usar modelos cl\u00e1sicos de series de tiempo\n* Aun as\u00ed, los modelos diarios y semanales SARIMA presentaron error considerable en sus estimaciones\n* Un modelo de Random Forest super\u00f3 al cl\u00e1sico modelo SARIMA.\n* Los modelos m\u00e1s complejos podr\u00edan implementarse para mejorar las predicciones.\n","47bfab97":"Vemos que el punto de inflexion es alrdedor del 6. Pero usaremos K-means elbow visualizer para saber con mayor certeza.","a70a5e8a":"Vemos que nuevamente los datos diferenciados son estacionarios y se pueden  ajustar modelos como ARIMA a la serie","ca2d7e16":"### \u00bfPor qu\u00e9 usar PCA?","d4cdd9e0":"<b style=\"font-family: Arials; line-height: 2; font-size: 18px; font-weight: bold; letter-spacing: 2px; text-align: center\">Algunas conclusiones del an\u00e1lisis exploratorio<\/b>\n\n<hr style=\"height: 0.5px; border: 0; background-color: #808080\">\n\n* La empresa tiene una tendencia creciente de ventas\n* Los tiempos de entrega son relativamente constantes\n* Los usuarios est\u00e1n en general, satisfechos con el servicio\n* La ubicaci\u00f3n geogr\u00e1fica tiene gran influencia en las ventas\n* Los \u00faltimos 4 meses del a\u00f1o son los de menor ganancia\n* El pago con tarjeta de cr\u00e9dito sigue liderando en ventas on-line\n\n","f12bdd74":"Una vez agrupados,podemos obtener informaci\u00f3n de esos clientes para conocerlos mejor y hacer todo tipo de campa\u00f1as publicitarias, ofertas, promociones....\nObservamos la cantidad de personas que pertenecen a cada grupo. Asignamos estos grupos como variables en el Data Frame y podemos obtener informaci\u00f3n relevante de ellos con respecto a las variables definidas.\n","224f1707":"Teniendo nuestra matriz transformada de 19 variables construidas con componentes principales, procedemos a ajustar un modelo K-means. Buscamos el mejor n\u00famero de grupos iterando sobre las posibilidades y optimizando sobre la variabilidad dentro de los grupos. Recordemos que a mayor n\u00famero de grupos, menor variabilidad interna pero m\u00e1s probable es el sobre ajuste.","3dd28d1f":"Obtener componentes principales de un conjunto de datos puede proveer muchas ventajas cuando se trabaja con m\u00e9todos no supervisados. \n\n* Primeramente reduce las variables de insumo lo que le facilita al modelo obtener informaci\u00f3n relevante\n* Al reducir la dimensi\u00f3n, las variables que no tengan mucha influencia pierden fuerza y se evade el posible ruido que puedan suministrar al modelo\n* El modelo tiende a requerir menos tiempo de entrenamiento.\n","2f2237e4":"### \u00bfPor qu\u00e9 usar SQL?\n \nCuando se gestionan m\u00faltiples tablas interconectadas para realizar cualquier tipo de an\u00e1lisis, utilizar SQL es la mejor pr\u00e1ctica. Tiene importantes ventajas:\n \n* Recuperar s\u00f3lo los datos espec\u00edficos que se necesitan\n* Preprocesamiento de la salida en la consulta\n* Escalable. Ya que la misma consulta recuperar\u00e1 siempre el resultado mientras no se cambie el nombre de la columna\n* Se necesita menos memoria.\n* Usando **INNER JOIN** podemos recuperar s\u00f3lo los resultados coincidentes sin alterar el conjunto de datos\n \nPor supuesto, hay una desventaja: hay que hacer una consulta diferente cada vez que se necesitan datos diferentes. Esto no suele ser un gran problema","0307f7b9":"#### Eliminando valores del primer y ultimo dia que no est\u00e1n completos","90282ab4":"#### Serie diferenciada","8559ea5c":"### Ventas por mes del a\u00f1o","02c846aa":"La diferenciaci\u00f3n parece corregir la mayor\u00eda de los inconvenientes excepto la varianza no constante.\n","5b01e198":"Observamos que, aunque sigue teniendo problemas para predecir las ventas, el modelo de Random Forest tiene mejor desempe\u00f1o que un modelo SARIMA. Modelos mas complejos y con mas variables podr\u00edan ser ajustados para mejorar las predicciones","3cb0d70d":"Observamos la serie y vemos que posiblemente tambi\u00e9n requiere una diferenciaci\u00f3n para llevarla a la estacionariedad ","178f67ff":"##### Serie semanal diferenciada","e274f5b2":"Estos datos deben prepararse para el modelo: se estandarizan las variables continuas y se categorizan las categor\u00edas. La salida es una sparse matrix. Esto es \u00fatil cuando hay miles de categor\u00edas porque la codificaci\u00f3n onehot convierte estas categor\u00edas en una matriz llena de ceros, excepto un \u00fanico 1 por fila.\n\nEn nuestro caso, necesitamos convertir esta matriz dispersa en un array de numpy porque necesitamos realizar una reducci\u00f3n de dimensional con PCA ","38bafd4d":"Se observa una tendencia creciente de las ventas que se estabilizan en 2018.","054ef8f6":"Este gr\u00e1fico interactivo muestra el n\u00famero de ventas por categor\u00eda de las 20 m\u00e1s vendidas. Podemos observar las categor\u00edas favoritas: Cosas del hogar, deportes, decoraci\u00f3n...","013b1e91":"Luego de hacer la query en nuestra base de datos, creamos 2 variables de tiempo que usaremos para crear las variables recency y frequency. Nos deshacemos de los valores faltantes, agrupamos por cliente agregando sobre las variables de inter\u00e9s de la forma m\u00e1s pertinente, cambiamos los nombres y obtenemos la siguiente tabla.","06228e62":"#### Funci\u00f3n de ayuda para los gr\u00e1ficos de TS m\u00e1s importantes","e0e768c8":"<b style=\"font-family: Arials; line-height: 2; font-size: 18px; font-weight: bold; letter-spacing: 2px; text-align: center\">Algunas conclusiones de la segmentaci\u00f3n de mercado<\/b>\n\n<hr style=\"height: 0.5px; border: 0; background-color: #808080\">\n\n* Los componentes principales ayudaron a mejorar el proceso de segmentaci\u00f3n, evitando por ejemplo malas agrupaciones debido al puntaje promedio del cliente.\n* Los m\u00e9todos para calcular el n\u00famero de grupos no siempre son 100% eficaces y debe revisarse si se encuentran grupo poco poblados\n* Las visualizaciones solo con los datos definidos en primera instancia para los clientes revelaron que los grupos si tienen una separaci\u00f3n l\u00f3gica, lo que podr\u00eda ayudarnos a construir estrategias de mercado m\u00e1s personalizadas.\n* Esta segmentaci\u00f3n podr\u00eda apoyar un algoritmo de recomendaciones. ","d90fab4e":"# Segmentaci\u00f3n de clientes basada en RFM, puntajes y categor\u00eda con K-means y PCA\n","31531a17":"Se realiza an\u00e1lisis exploratorio de datos y visualizaciones","c4b4e37e":"Los gr\u00e1ficos de autocorrelaci\u00f3n y normalidad indican que la serie requiere transformaciones y\/o diferenciaciones. Generalmente no se requiere m\u00e1s de una diferenciaci\u00f3n.","7741ec89":"#### Ventas por dia","d14fd566":"Es interesante observar que precios altos no significan menos ventas. Los ordenadores tienen el mayor precio medio y est\u00e1n en el top 10 de ventas por categor\u00eda. ","32a9fdb6":"### Pedidos por tiempo agrupados por mes","e3548fde":"La transformaci\u00f3n logar\u00edtmica no tiene ning\u00fan efecto positivo en la serie por lo que se descarta su uso","aa912a95":"### Most common payment method","8ccd0aa8":"Veamos que tan bien se desempe\u00f1a un modelo haciendo estimaciones de ventas agrupadas por semana.","da80fc00":"Los primeros y \u00faltimos **valores bajos** se deben a que los datos de esos meses est\u00e1n incompletos. Es f\u00e1cil observar la correlaci\u00f3n l\u00f3gica entre el n\u00famero de pedidos y las ventas a lo largo del tiempo.","c90aa53d":"Vemos que el modelo tiene problemas para hacer predicciones precisas de las ventas diarias. ","fb5d9658":"## Cargar paquetes, leer datos y realizar la conexi\u00f3n local SQL","61e57f7a":"Podemos observar una baja variaci\u00f3n en las puntuaciones. En general, las puntuaciones parecen estar cerca de 4 y 5, excluyendo la seguridad y los servicios que, como vemos, es tambi\u00e9n el \u00faltimo lugar en ventas.","d27c435f":"![Image](https:\/\/pbs.twimg.com\/profile_images\/1357301109455478785\/ETvAXWxE.jpg)","bc62c878":"# TSA para las ventas en el tiempo","8342753b":"Este pair plot muestra informaci\u00f3n interesantes:\n \n* La puntuaci\u00f3n se distribuye normalmente. As\u00ed que podr\u00edamos decir que los productos est\u00e1n siendo juzgados de forma justa, independientemente del precio y el n\u00famero de ventas\n* Parece haber una relaci\u00f3n lineal entre la cantidad de productos vendidos y el importe de las ventas, lo cual es l\u00f3gico\n* Hay muchas m\u00e1s ventas peque\u00f1as (bajo costo) que grandes","724f87f3":"Podemos observar un patr\u00f3n. Los \u00faltimos 4 meses del a\u00f1o son los de menor \u00edndice de ventas.","1d5c0cce":"Primero debemos transformar la serie de tiempo en un problema de aprendizaje supervisado. Esto lo hacemos usando una funci\u00f3n prove\u00edda por [Machine Learning Mastery](https:\/\/machinelearningmastery.com\/convert-time-series-supervised-learning-problem-python\/), para los modelos de ML, es una pr\u00e1ctica com\u00fan y ventajosa escalar los datos. Por lo cual es nuestro segundo paso. ","12738f3f":"Podemos observar las categor\u00edas que producen m\u00e1s ganancias. Observando las primeras 20 categor\u00edas podemos notar que la categor\u00eda m\u00e1s vendida es la que produce m\u00e1s dinero, pero la segunda m\u00e1s vendida no est\u00e1 en el top 20 siquiera.","737d4469":"#### TSA por semana","afeaf88b":"Observamos algunas caracter\u00edsticas de los grupos:\n\n* El grupo 3 tiene los valores m\u00e1s altos de frecuencia y valor monetario\n* El n\u00famero de d\u00edas desde que se es cliente parece ser un factor relevante para la separaci\u00f3n de los grupos 0,1 y 2\n* EL puntaje promedio no parece ser una variable importante en la separaci\u00f3n de los grupos lo cual es l\u00f3gico, ya que el puntaje tiene una distribuci\u00f3n ligeramente uniforme alrededor de 4\n","b5d34ce8":"<p style=\"font-family: Arials; line-height: 2; font-size: 24px; font-weight: bold; letter-spacing: 2px; text-align: center; color: #5F8DC2\">An\u00e1lisis completo de E-Commerce de Brasil usando SQL <\/p>","d106cbc3":"Es com\u00fan observar tarjetas de cr\u00e9dito en las compras en l\u00ednea. El segundo lugar es para el *boleto* que es un m\u00e9todo de pago nacional en Brasil","ebb92c43":"### Ventas por categor\u00eda ","afce81a1":"### Product Sales by category","f4b7e6ab":"Gracias al uso de la librer\u00eda *pdmarima* podemos iterar sobre los mejores par\u00e1metros posibles para un modelo SARIMA para ajustar a los datos. Luego, con la ayuda de una funci\u00f3n que construimos, hacemos validaci\u00f3n cruzada con el set de entrenamiento que se defini\u00f3, y se puede evaluar la capacidad predictiva del modelo en base a una medida como la **suma de cuadrados del error** ","d6ba2ac0":"Observamos que la serie tiene una ligera tendencia creciente. Pero el mayor problema es la varianza no constante en la cual trabajaremos. ","014940bc":"La velocidad de entrega medida en d\u00edas, parece distribuirse relativamente normal alrededor de los 10 d\u00edas de espera. Siendo una tienda en on-line es un tiempo prudente para env\u00edos nacionales. Vemos un valor at\u00edpicos de 210 d\u00edas que podr\u00eda ser estudiado m\u00e1s a fondo.","b8f9c555":"Valor p <= 0,05: Rechaza la hip\u00f3tesis nula (H0), los datos no tienen ra\u00edz unitaria y son estacionarios. Esto indica que podemos usar los **datos diferenciados** para ajustar modelos como ARIMA a la serie y usarlos de l\u00ednea base de comparaci\u00f3n ","8935dac9":"### Customers by state","40d4b4e8":"Aunque el numero optimo de grupos alrededor de 5, esto deja grupos con muy pocos miembros. Asi que se reducira a 4 grupos. \nTambien se nota que 4 grupos, es un punto de inflexion en las graficas anteriores.","1dff0173":"#### Serie con transformaci\u00f3n logar\u00edtmica","15650652":"# Nota importante, graficos interactivos\n\nSi los graficos interactivos de **plotly** no se muestran, basta con recargar la pagina. ","9ed134c0":"### Visualizando los grupos de acuerdo a las variables estudiadas","a510fd61":"### Puntaje de satisfacci\u00f3n promedio por categor\u00eda","750ad915":"En las siguientes celdas se leen los datos y se construye la conexi\u00f3n local SQL\n","d1f14fbc":"## Lectura de datos y uso de SQL ","96603b6f":"# EDA con Visualizaciones","e6486783":"### Ventas en el tiempo agrupadas por mes"}}