{"cell_type":{"865c883e":"code","c2a6a94a":"code","7a49f01b":"code","2265fa28":"code","559acbf5":"code","376c428f":"code","bade1f66":"code","9090932d":"code","339fb189":"code","6aa4cbf0":"code","7debaa27":"code","2c7c34b9":"code","ccd791bc":"code","e23a0626":"code","19fe0cef":"code","1c36c511":"markdown","57148b85":"markdown","4e549dbb":"markdown","ec6c66c8":"markdown","2b3b6131":"markdown","5b0fdc3b":"markdown","70b74965":"markdown","3d4d3a91":"markdown"},"source":{"865c883e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\nimport seaborn as sns\nimport sys\nimport csv\nimport datetime\nimport operator\nimport joblib\nimport warnings","c2a6a94a":"train = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/test.csv\")","7a49f01b":"train.isnull().sum()[train.isnull().sum() != 0]","2265fa28":"train_df = train.drop(['id'], axis = 1)\ntest_df = test.drop(['id'], axis = 1)","559acbf5":"# data segmentation\nX = train_df.drop('target', axis=1)\ny = train_df['target']\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, shuffle=True, random_state=0) # train, valid 8:2 \ubd84\ud560","376c428f":"xgboost_model= xgb.XGBClassifier(max_depth = 8,\n                                 learning_rate = 0.005,\n                                 n_estimators = 10000,\n                                 objective = 'binary:logistic',\n                                 tree_method = 'gpu_hist',\n                                 booster = 'gbtree',\n                                 gamma = 0.64,\n                                 max_delta_step = 3,\n                                 min_child_weight = 7,\n                                 subsample = 0.7,\n                                 colsample_bytree = 0.8,\n                                 n_jobs = -1\n                                 )","bade1f66":"start = datetime.datetime.now()\nxgb = xgboost_model.fit(X_train,\n                       y_train,\n                       eval_set = [(X_train, y_train), (X_valid, y_valid)], \n                       eval_metric = 'auc',\n                       early_stopping_rounds = 15,\n                       verbose = True)\nend = datetime.datetime.now()\nend-start","9090932d":"y_pred = xgboost_model.predict(X_valid)","339fb189":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, roc_auc_score, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import scale\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import cohen_kappa_score\nfrom collections import OrderedDict\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom scipy.stats import norm, skew, probplot\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.preprocessing import QuantileTransformer\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.model_selection import StratifiedKFold\n\ndef classifier_eval(y_valid , y_pred) :\n  print('\uc815\ud655\ub3c4(accuracy_score) : ', accuracy_score(y_valid, y_pred))\n  print('\uc815\ubc00\ub3c4(precision_score) : ', precision_score(y_valid, y_pred))\n  print('\uc7ac\ud604\uc728(recall_score) : ', recall_score(y_valid, y_pred))\n  print('F1 : ', f1_score(y_valid, y_pred))\n  print('AUC : ', roc_auc_score(y_valid, y_pred))\n\nclassifier_eval(y_valid, y_pred)","6aa4cbf0":"x = np.array([accuracy_score(y_valid, y_pred),\n              precision_score(y_valid, y_pred),\n              recall_score(y_valid, y_pred),\n              f1_score(y_valid, y_pred),\n              roc_auc_score(y_valid, y_pred)])\n\nx","7debaa27":"label = ['accuracy', 'precision', 'recall_score', 'f1_score', 'roc_auc']\n\nindex = np.arange(len(label))\n\n\nplt.bar(index, x, width=0.5)\nplt.title('evaluation index', fontsize=20)\nplt.ylabel('%', fontsize=18)\nplt.xticks(index, label, fontsize=15,rotation=90)    # X\ucd95\uc758 \ubc94\uc704: [xmin, xmax]\nplt.ylim([0, 1])     # Y\ucd95\uc758 \ubc94\uc704: [ymin, ymax]\nplt.show()","2c7c34b9":"fi_vals = xgb.get_booster().get_score(importance_type = 'weight')\nfi_dict = {X_train.columns[i]:float(fi_vals.get('f'+str(i),0.)) for i in range(len(X_train.columns))}\nfeature_importance_ = sorted(fi_dict.items(), key=operator.itemgetter(1), reverse=True)\nfeature_importance_result = OrderedDict(feature_importance_)\n\nimportance = pd.DataFrame(feature_importance_)\nimportance.columns = ['feature','weight']\nimportance.head(10)","ccd791bc":"xgboost_prediction = xgboost_model.predict(test_df)","e23a0626":"submission_XGBoost = pd.DataFrame({'id':test['id'], 'target':xgboost_prediction})\nsubmission_XGBoost.head()","19fe0cef":"submission_XGBoost.to_csv('.\/submission.csv', index=False)","1c36c511":"reference model : https:\/\/www.kaggle.com\/hadeux\/tps-nov-xgboost-baseline","57148b85":"**Features importance**","4e549dbb":"# XGBoost Modeling","ec6c66c8":"\n\n    booster='gbtree'  1. \ud2b8\ub9ac,\ud68c\uadc0(gblinear) \ud2b8\ub9ac\uac00 \ud56d\uc0c1  \n                         Tree, a gblinear tree is always\n                      2. \ub354 \uc88b\uc740 \uc131\ub2a5\uc744 \ub0b4\uae30 \ub54c\ubb38\uc5d0 \uc218\uc815\ud560 \ud544\uc694\uc5c6\ub2e4\uace0\ud55c\ub2e4. \n                         It is said that there is no need to modify it because it gives better performance.\n    \n    silent=True       1. running message output X.\n                      2. \ubaa8\ub378\uc774 \uc801\ud569\ub418\ub294 \uacfc\uc815\uc744 \uc774\ud574\ud558\uae30\uc704\ud574\uc120 False\uc73c\ub85c\ud55c\ub2e4.\n                      To understand how the model is fitted, set it to False.\n    \n    min_child_weight=10    1. \uac12\uc774 \ub192\uc544\uc9c0\uba74 under-fitting \ub418\ub294 \uacbd\uc6b0\uac00 \uc788\ub2e4. CV\ub97c \ud1b5\ud574 \ud29c\ub2dd\ub418\uc5b4\uc57c \ud55c\ub2e4.\n                              Higher values \u200b\u200bmay lead to under-fitting. It should be tuned via CV.\n    \n    max_depth=8      1. \ud2b8\ub9ac\uc758 \ucd5c\ub300 \uae4a\uc774\ub97c \uc815\uc758\ud568.\n                        Defines the maximum depth of the tree.\n    \n                     2. \ub8e8\ud2b8\uc5d0\uc11c \uac00\uc7a5 \uae34 \ub178\ub4dc\uc758 \uac70\ub9ac.\n                        Distance of longest node from root.\n                     3. 8\uc774\uba74 \uc911\uc694\ubcc0\uc218\uc5d0\uc11c \uacb0\ub860\uae4c\uc9c0 \ubcc0\uc218\uac00 9\uac1c\uac70\uce5c\ub2e4.\n                        If the value is 8, 9 variables pass from the important variable to the conclusion.\n                     4. Typical Value is 3-10. \n    \n    gamma =0         1. \ub178\ub4dc\uac00 split \ub418\uae30 \uc704\ud55c loss function\uc758 \uac12\uc774 \uac10\uc18c\ud558\ub294 \ucd5c\uc18c\uac12\uc744 \uc815\uc758\ud55c\ub2e4. \n                        gamma \uac12\uc774 \ub192\uc544\uc9c8 \uc218\ub85d \uc54c\uace0\ub9ac\uc998\uc740 \ubcf4\uc218\uc801\uc73c\ub85c \ubcc0\ud558\uace0, loss function\uc758 \uc815\uc758\uc5d0 \n                        \ub530\ub77c \uc801\uc815\uac12\uc774 \ub2ec\ub77c\uc9c0\uae30\ub54c\ubb38\uc5d0 \ubc18\ub4dc\uc2dc \ud29c\ub2dd.\n                        Defines the minimum value at which the value of the loss function for the node to split is decreased. As the gamma value increases, the algorithm changes conservatively, and the appropriate value changes according to the definition of the loss function, \n                        so it must be tunning.\n    \n    nthread =4       1. XGBoost\ub97c \uc2e4\ud589\ud558\uae30 \uc704\ud55c \ubcd1\ub82c\ucc98\ub9ac(\uc4f0\ub808\ub4dc) \uac2f\uc218. 'n_jobs' \ub97c \uc0ac\uc6a9\ud574\ub77c.\n                        The number of parallel processing (threads) to execute XGBoost. Use 'n_jobs' .\n    \n    colsample_bytree=0.8    1. \ud2b8\ub9ac\ub97c \uc0dd\uc131\ud560\ub54c \ud6c8\ub828 \ub370\uc774\ud130\uc5d0\uc11c \ubcc0\uc218\ub97c \uc0d8\ud50c\ub9c1\ud574\uc8fc\ub294 \ube44\uc728. \ubcf4\ud1b5 0.6~0.9\n                               The rate at which variables are sampled from the training data when creating the tree. Usually 0.6-0.9\n    \n    colsample_bylevel=0.9   1. \ud2b8\ub9ac\uc758 \ub808\ubca8\ubcc4\ub85c \ud6c8\ub828 \ub370\uc774\ud130\uc758 \ubcc0\uc218\ub97c \uc0d8\ud50c\ub9c1\ud574\uc8fc\ub294 \ube44\uc728. \ubcf4\ud1b50.6~0.9\n                               The rate at which the variables in the training data are sampled for each level of the tree. Usually 0.6~0.9\n    \n    n_estimators =(int)     1. \ubd80\uc2a4\ud2b8\ud2b8\ub9ac\uc758 \uc591 amount of boost tree\n                            2. \ud2b8\ub9ac\uc758 \uac2f\uc218. number of trees.\n     \n    objective = 'reg:linear','binary:logistic','multi:softmax','multi:softprob'\n                 1.  regression case 'reg',\n                 2.  In case of binary classification 'binary',\n                 3.  Multiple classification case 'multi',\n                 4.  When returning a classified class 'softmax',\n                 5.  When returning the probability of belonging to each class 'softprob'*\n    \n    random_state =   1. random number seed.\n                     2. like seed.\n\n","2b3b6131":"# Submission","5b0fdc3b":"# Import Data","70b74965":"**XGBoost hyperparameter**","3d4d3a91":"**result visualization**"}}