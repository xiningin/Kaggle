{"cell_type":{"e6010aed":"code","90b22468":"code","e6bac499":"code","f472fd74":"code","6f46ddad":"code","481e8412":"code","579aed32":"code","a2fb322c":"code","a836a210":"code","376ab5aa":"code","3f512f2f":"code","1e4334d8":"code","f6541bb1":"code","6c0675f1":"code","d51b1e4a":"code","51311a4e":"code","0e36ebea":"code","26c59c5f":"code","b39f2c6c":"code","6ded2ac8":"code","305018e6":"code","6bd4a535":"code","03658ce1":"code","228d4571":"code","8bc0489d":"code","496bcd25":"code","3fdf79b7":"code","8ca25c38":"code","bc47711f":"code","81055393":"code","acf116f3":"code","88715f7b":"code","d8dbf78c":"code","fdd0aef9":"code","645d5218":"code","e236b3ce":"code","4f9ee456":"code","a150b55b":"code","16c39c38":"code","41e6539a":"code","4887b65c":"code","718b6df8":"markdown","c6600e9d":"markdown","bee9eb26":"markdown","9139c2df":"markdown","deae9c4e":"markdown","dd8114d8":"markdown","21f2903a":"markdown","6750c649":"markdown","60cd4836":"markdown","c9e62783":"markdown","327b70f5":"markdown","f4861d10":"markdown","d8de0aab":"markdown","73c80dbf":"markdown","73717a1d":"markdown","7f8af717":"markdown"},"source":{"e6010aed":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input\/income'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","90b22468":"df = pd.read_csv('\/kaggle\/input\/income\/train.csv')","e6bac499":"df.columns","f472fd74":"df_orig = df.copy()","6f46ddad":"df.head()","481e8412":"df.rename(columns={\"income_>50K\": \"income >50K\"}, inplace=True, errors='raise')","579aed32":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom xgboost import XGBRegressor, XGBClassifier\nfrom sklearn.metrics import mean_absolute_error, accuracy_score, classification_report,confusion_matrix\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix,accuracy_score,roc_curve,classification_report\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.model_selection import GridSearchCV,RandomizedSearchCV,KFold,StratifiedKFold\nimport pandas_profiling as pp\nimport warnings\nwarnings.filterwarnings('ignore')\nimport missingno as msno #Visualize null\n\nsns.set_style('ticks') #No grid with ticks\nprint(sns.__version__)","a2fb322c":"df.info()","a836a210":"df.columns","376ab5aa":"df.isna().any()","3f512f2f":"df.info()","1e4334d8":"df.isnull().sum()","f6541bb1":"cols = ['age', 'workclass', 'education', 'educational-num', 'marital-status',\n       'occupation', 'relationship', 'race', 'capital-gain',\n       'capital-loss', 'hours-per-week', 'native-country', 'income >50K']\n\nfor i in cols:\n  print(df[i].value_counts())","6c0675f1":"new_data = df.dropna()\nnew_data.info()","d51b1e4a":"new_data.isna().sum()","51311a4e":"new_data.dtypes","0e36ebea":"labelencoder = LabelEncoder()\n\ndf_max_scaled = new_data.copy()\n\n## FEATURE ENGINEERING\n\ndf_max_scaled[\"workclass\"] = df_max_scaled[\"workclass\"].replace(['Self-emp-not-inc','Self-emp-inc'], 'Self-emp')\ndf_max_scaled[\"workclass\"] = df_max_scaled[\"workclass\"].replace(['Never-worked','Without-pay'], 'Un-emp')\ndf_max_scaled[\"workclass\"] = df_max_scaled[\"workclass\"].replace(['State-gov','Federal-gov','Local-gov'], 'Government')\n\ndf_max_scaled['education'] = df_max_scaled[\"education\"].replace(['12th','7th-8th','9th','10th', '11th','5th-6th','1st-4th','Preschool'], '<HS')\ndf_max_scaled['education'] = df_max_scaled[\"education\"].replace(['Assoc-voc','Assoc-acdm','Some-college'], 'Associate')\ndf_max_scaled['education'] = df_max_scaled[\"education\"].replace(['Masters','Prof-school'], 'Mas-Prof')\n\ndf_max_scaled['marital-status'] = df_max_scaled[\"marital-status\"].replace(['Divorced','Separated'], 'Sep-Div')\ndf_max_scaled['marital-status'] = df_max_scaled[\"marital-status\"].replace(['Married-civ-spouse','Married-spouse-absent','Married-AF-spouse'], 'Married')\n\ndf_max_scaled['relationship'] = df_max_scaled[\"relationship\"].replace(['Husband','Wife'], 'Married')\n\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['England', 'Italty', 'Germany', 'France','Yugoslavia', 'Poland', 'Greece', 'Ireland', 'Scotland',\n       'Hungary','Holand-Netherlands','Portugal'], 'Europe')\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['China', 'Philippines','Vietnam','Thailand','Taiwan','Laos','Cambodia','Japan', 'Hong','India','Iran'], 'Asia')\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['Jamaica','Dominican-Republic','Cuba','Haiti','Trinadad&Tobago', 'Puerto-Rico'], 'Carribean')\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['United-States','Canada'], 'N.America')\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['Mexico','Honduras','El-Salvador','Guatemala','Nicaragua'], 'C.America')\ndf_max_scaled['native-country'] = df_max_scaled['native-country'].replace(['Columbia','Ecuador','Peru'], 'S.America')\n\ndf_max_scaled = df_max_scaled.astype({\n    'workclass' : 'category',\n    'education' : 'category',\n    'marital-status' : 'category',\n    'relationship' : 'category',\n    'native-country' : 'category',\n    'race' : 'category',\n    'occupation' : 'category'\n})\n\ncat_cols = [i for i  in df_max_scaled.columns if df_max_scaled[i].dtype not in ['int64', 'float64']]\n\nfor col in cat_cols:\n  df_max_scaled[col + \"-cat\"] = labelencoder.fit_transform(df_max_scaled[col])\n\nnum_cols = [col for col in df_max_scaled.columns if df_max_scaled[col].dtype in ['int', 'float']]\n\nfor i in cat_cols:\n  df_max_scaled.drop([i], axis= 1, inplace= True)\n\nfor col in num_cols:\n  df_max_scaled[col] = df_max_scaled[col] \/ df_max_scaled[col].abs().max() \n\ndf_max_scaled.head()","26c59c5f":"df_max_scaled.info()","b39f2c6c":"num_cols = [col for col in df_max_scaled.columns if df_max_scaled[col].dtype in ['int64','float64']]\n\ncat_cols = [col for col in df_max_scaled.columns if df_max_scaled[col].dtype not in ['int64', 'float64']]","6ded2ac8":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nplt.figure(figsize = (50, 30))\nplotnumber = 1\n\nfor i in num_cols:\n    if plotnumber <= 14:\n        ax = plt.subplot(7, 2, plotnumber)\n        sns.distplot(df_max_scaled[i])\n        plt.xlabel(i, fontsize = 15)\n        \n    plotnumber += 1\n    \nplt.tight_layout()\nplt.show()","305018e6":"def dist_box(data):\n # function plots a combined graph for univariate analysis of continous variable \n #to check spread, central tendency , dispersion and outliers  \n    Name=data.name.upper()\n    fig,(ax_box,ax_dis)  =plt.subplots(2,1,gridspec_kw = {\"height_ratios\": (.25, .75)},figsize=(8, 5))\n    mean=data.mean()\n    median=data.median()\n    mode=data.mode().tolist()[0]\n    fig.suptitle(\"SPREAD OF DATA FOR \"+ Name  , fontsize=18, fontweight='bold')\n    sns.boxplot(x=data,showmeans=True, orient='h',color=\"violet\",ax=ax_box)\n    ax_box.set(xlabel='')\n    sns.distplot(data,kde=False,color='blue',ax=ax_dis)\n    ax_dis.axvline(mean, color='r', linestyle='--',linewidth=2)\n    ax_dis.axvline(median, color='g', linestyle='-',linewidth=2)\n    ax_dis.axvline(mode, color='y', linestyle='-',linewidth=2)\n    plt.legend({'Mean':mean,'Median':median,'Mode':mode})","6bd4a535":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfor i in range(len(num_cols)):\n    dist_box(df_max_scaled[num_cols[i]])","03658ce1":"# heatmap\nplt.figure(figsize = (16, 7))\n\ncorr = df_max_scaled.corr()\nmask = np.triu(np.ones_like(corr, dtype = bool))\n\nsns.heatmap(corr, mask = mask, annot = True, fmt = '0.2g', linewidths = 1)\nplt.show()","228d4571":"plt.figure(figsize=(15,5))\nsns.heatmap(df_max_scaled.corr(),annot=True ,cmap=\"YlGn\")\nplt.show()","8bc0489d":"## AGE VS INCOME\n\nsns.boxplot(x=df_max_scaled['income >50K'],\n              y=df_max_scaled['age'])","496bcd25":"## CAPITAL GAIN VS INCOME\n\nsns.boxplot(x=df_max_scaled['income >50K'],\n              y=df_max_scaled['capital-gain'])","3fdf79b7":"## HOURS PER WEEK VS INCOME\n\nsns.boxplot(x=df_max_scaled['income >50K'],\n              y=df_max_scaled['capital-gain'])","8ca25c38":"df_max_scaled.info()","bc47711f":"data = df_max_scaled.drop_duplicates()","81055393":"data.head()","acf116f3":"y = data[\"income >50K\"]\nX = data.drop('income >50K',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","88715f7b":"df_max_scaled.shape","d8dbf78c":"## RANDOM FOREST\n\nm1 = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(classification_report(y_test,rf_predicted))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(rf, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m1, cv_results.mean(), cv_results.std())\nprint(msg)","fdd0aef9":"## LOGISTIC REGRESSION\n\nm2 = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(classification_report(y_test,lr_predict))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(lr, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m2, cv_results.mean(), cv_results.std())\nprint(msg)","645d5218":"m3 = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(classification_report(y_test,nbpred))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(nb, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m3, cv_results.mean(), cv_results.std())\nprint(msg)","e236b3ce":"## XGBOOSTING\n\nm4 = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(classification_report(y_test,xgb_predicted))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(xgb, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m4, cv_results.mean(), cv_results.std())\nprint(msg)","4f9ee456":"## KNN\n\nm5 = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=5)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(classification_report(y_test,knn_predicted))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(knn, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m5, cv_results.mean(), cv_results.std())\nprint(msg)","a150b55b":"## DECISION TREE\n\nm6 = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=None,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(classification_report(y_test,dt_predicted))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(dt, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m6, cv_results.mean(), cv_results.std())\nprint(msg)","16c39c38":"m7 = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(classification_report(y_test,svc_predicted))\n\nkfold = KFold(n_splits=10, random_state=None)\ncv_results = cross_val_score(svc, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % (m7, cv_results.mean(), cv_results.std())\nprint(msg)","41e6539a":"from mlxtend.classifier import StackingCVClassifier","4887b65c":"scv=StackingCVClassifier(classifiers=[lr,knn,rf],\n                         meta_classifier= svc)\n\nscv.fit(np.asarray(X_train),np.asarray(y_train))\nscv_predicted = scv.predict(X_test)\nscv_conf_matrix = confusion_matrix(y_test, scv_predicted)\nscv_acc_score = accuracy_score(y_test, scv_predicted)\nprint(\"confussion matrix\")\nprint(scv_conf_matrix)\nprint(\"\\n\")\nprint(\"Accuracy of StackingCVClassifier:\",scv_acc_score*100,'\\n')\nprint(classification_report(y_test,scv_predicted))\n\nkfold = KFold(n_splits=10)\ncv_results = cross_val_score(svc, X_train, y_train, cv=kfold, scoring='accuracy')\nmsg = \"%s: %f (%f)\" % ('Stacking CV Cassifier', cv_results.mean(), cv_results.std())\nprint(msg)","718b6df8":"##### numeric","c6600e9d":"## *6. DECISION TREE*","bee9eb26":"## *5. KNN*","9139c2df":"# MODELLING","deae9c4e":"#### *~ NUMERIC DATA VS CATEGORICAL DATA*\n","dd8114d8":"### LIBRARY IMPORTS","21f2903a":"##*2. LOGISTIC REGRESSION*","6750c649":"## *ENSEMBLING*","60cd4836":"We will use the following algos:\n\n\n1.   Random Forest Classifier\n2.   Logistic Regression\n3.   Naive Bayes\n4.   Extreme Gradient Booster\n5.   KNN (K - Nearest Neighbours)\n6.   Decision Tree\n7.   SVM\n\n\n\n","c9e62783":"## *7. SVM*","327b70f5":"## UNIVARIATE","f4861d10":"## *4. EXTREME GB*","d8de0aab":"# DATA PREPARATIONS","73c80dbf":"## *3. NAIVE BAYES*","73717a1d":"## *1. RANDOM FOREST CLASSIFIER*","7f8af717":"# EXPLORATORY DATA ANALYSIS"}}