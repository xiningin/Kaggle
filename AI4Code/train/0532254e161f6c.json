{"cell_type":{"1ff5f685":"code","d5d55e40":"code","20ff22b1":"code","9c5bea86":"code","15da041c":"code","772b9265":"code","7ec0395f":"code","acdba8ed":"code","977a38f0":"code","de6f6ef7":"code","cc0a5275":"code","237426f0":"code","4f406426":"code","80e31a9d":"code","1b4e17ad":"code","4233102e":"code","d467ac1c":"code","368ec47d":"code","b0bcd0b4":"code","660f2e12":"code","9c1020eb":"code","d5e9936d":"code","7fa33299":"code","0c9ac330":"code","a6f0a54e":"code","2beb5406":"code","83e0f62a":"markdown","b0c6297b":"markdown","a8ddd545":"markdown","d97780ae":"markdown","7a037d97":"markdown","6fb6b3e1":"markdown","877e5246":"markdown","4005d278":"markdown","41fbb9bd":"markdown","fe4630cc":"markdown","bda0e88a":"markdown","ccdc4758":"markdown","b3b5f946":"markdown"},"source":{"1ff5f685":"# import libraries\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# To display all the columns of the dataframe\npd.set_option('max_columns', None)","d5d55e40":"# import data\ntrain= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest= pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","20ff22b1":"# shape of train dataset (rows, columns)\nprint(train.shape)","9c5bea86":"# Top 8 rows of train dataset\ntrain.head(8)","15da041c":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(train.drop(['Id','SalePrice'],axis=1),train['SalePrice'],test_size=0.2,random_state=0)\nprint(X_train.shape)\nprint(X_test.shape)","772b9265":"# First we will handle Categorical Features having nan values            # here O = object(string)(categrical value)\nfeatures_nan=[feature for feature in train.columns if train[feature].isnull().sum()>1 and train[feature].dtypes=='O']\n\nfor feature in features_nan:\n    print(\"{}: {}% missing values\".format(feature,np.round(train[feature].isnull().mean(),3)))","7ec0395f":"# Replace missing value of categorical feature\ndef replace_cat_feature(dataset,features_nan):\n    data=train.copy()\n    data[features_nan]=data[features_nan].fillna('Missing')\n    return data\n\ntrain=replace_cat_feature(train,features_nan)","acdba8ed":"# check number of missing vaues after replacing nan with missing\ntrain[features_nan].isnull().sum()","977a38f0":"train.head()","de6f6ef7":"# Now lets check the numerical variables having missing values\nnumerical_with_nan=[feature for feature in train.columns if train[feature].isnull().sum()>1 and train[feature].dtypes!='O']\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(train[feature].isnull().mean(),4)))","cc0a5275":"# Replace the numerical Missing Values\nfor feature in numerical_with_nan:\n    # Replace nan values by median since there are outliers(check EDA note book)\n    median_value=train[feature].median()\n    \n    # Creating a new feature to capture nan values\n    train[feature+'nan']=np.where(train[feature].isnull(),1,0)\n    train[feature].fillna(median_value,inplace=True)\n    \ntrain[numerical_with_nan].isnull().sum()","237426f0":"train.head(10)","4f406426":"# Temporal Variables (Date Time Variables)\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    train[feature]=train['YrSold']-train[feature]","80e31a9d":"train.head()","1b4e17ad":"train[['YearBuilt','YearRemodAdd','GarageYrBlt']].head()","4233102e":"# our data before appling log normal distribution\ntrain.head()","d467ac1c":"import numpy as np\nnum_features=['LotFrontage', 'LotArea', '1stFlrSF', 'GrLivArea', 'SalePrice']\n\nfor feature in num_features:\n    train[feature]=np.log(train[feature])","368ec47d":"# our data after appling log normal distribution\ntrain.head()","b0bcd0b4":"categorical_features=[feature for feature in train.columns if train[feature].dtype=='O']\ncategorical_features","660f2e12":"for feature in categorical_features:\n    temp=train.groupby(feature)['SalePrice'].count()\/len(train)\n    temp_df=temp[temp>0.01].index\n    train[feature]=np.where(train[feature].isin(temp_df),train[feature],'Rare_var')","9c1020eb":"train.head(20)","d5e9936d":"from sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\nfor feature in categorical_features:\n    train[feature]= label_encoder.fit_transform(train[feature])","7fa33299":"train.head()","0c9ac330":"# neglict the columns which will not be used as independent\nfeature_scale=[feature for feature in train.columns if feature not in ['Id','SalePrice']]\n\nfrom sklearn.preprocessing import MinMaxScaler\nscaler=MinMaxScaler()\ntrain[feature_scale]= pd.DataFrame(scaler.fit_transform(train[feature_scale]))","a6f0a54e":"train.head()","2beb5406":"#train.to_csv('train.csv',index=False)","83e0f62a":"## 3. Categorical variables: remove rare labels\n#### In this step, we will remove categorical variables that are present less than 1% of the observations","b0c6297b":"#### **Now finally our data is ready for creating the model.**","a8ddd545":"#### **Remember:** To prevent data leakage we must split the data and then apply Feature Engineering","d97780ae":"#### This note book is the second Part of the Lifecycle of a Data Science Project.\n#### [Click here](https:\/\/www.kaggle.com\/wajahatparvez99\/exploratory-data-analysis-advance-house-data) for part one of Lifecycle of a Data Science Project which is EDA.","7a037d97":"#### Download this dataset to use in the next Feature Selection notebook.","6fb6b3e1":"## Feature Engineering\n\n#### We will be performing all the below steps in Feature Engineering\n\n#### 1. Missing values\n#### 2. Temporal variables\n#### 3. Categorical variables: remove rare labels\n#### 4. Feature Encoding: Encode categorical features using any encoding scheme.\n#### 5. Feature Scaling","877e5246":"## 5. Feature Scaling","4005d278":"## 1. Missing Values","41fbb9bd":"## 2. Temporal Variables","fe4630cc":"#### Since the numerical variables are skewed(read [EDA](https:\/\/www.kaggle.com\/wajahatparvez99\/exploratory-data-analysis-advance-house-data) note book) we will perform log normal distribution","bda0e88a":"## 4. Feature Encoding\n#### Encode categorical features using Label encoding scheme.","ccdc4758":"## Complete Lifecycle of a Data Science Project\n#### 1. [Data Analysis](https:\/\/www.kaggle.com\/wajahatparvez99\/exploratory-data-analysis-advance-house-data)\n#### **2. Feature Engineering**\n#### 3. [Feature Selection](https:\/\/www.kaggle.com\/wajahatparvez99\/feature-selection-advance-house-data\/)\n#### 4. Model Building\n#### 5. Model Deployment","b3b5f946":"#### **Note: In this note book feature engineering is done only on train dataset. But you have to do the same process on your test dataset also while working on real life project.**"}}