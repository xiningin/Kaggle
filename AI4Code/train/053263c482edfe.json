{"cell_type":{"01183db9":"code","c76f3a79":"code","8493afd3":"code","60066791":"code","8f97113f":"code","b4fa5989":"code","51e103dd":"code","a19b76ba":"code","707e44b7":"code","3b18c40a":"code","f70cdcf5":"code","ec8b18c0":"code","eac28b69":"code","8054ca32":"code","b8b58686":"code","18f9f0ec":"markdown","3745959e":"markdown","6ffce50a":"markdown","bd5ef9dd":"markdown","bc9e8a26":"markdown","0fb24e77":"markdown","5c5cb52a":"markdown","aab51dce":"markdown"},"source":{"01183db9":"!ls","c76f3a79":"!ls ..\/input","8493afd3":"!ls ..\/input\/train\/ | head -5","60066791":"!head -n 2 ..\/input\/train.csv","8f97113f":"from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","b4fa5989":"train_data = pd.read_csv(\"..\/input\/train.csv\")","51e103dd":"train_data.head()","a19b76ba":"image_name = train_data.iloc[0,0]","707e44b7":"plt.title(str(train_data.iloc[0,1]))\nplt.imshow(io.imread(os.path.join(\"..\/input\/train\/\",image_name)))\n","3b18c40a":"#type of the variable that stores id\ntype(train_data.iloc[0,1])","f70cdcf5":"class WhaleIdDataset(Dataset):\n    \"\"\"the whale identification dataset\"\"\"\n    def __init__(self, csv, rootDir, transform=None ):\n        \"\"\"\n        Args:\n            csv (string): path to the file with Id\n            rootDir (string): path to the root directory\n            transform (callable,optional): optional \n                transforms to be applied on the samples.\n        \"\"\"\n        self.data_frame = pd.read_csv(csv)\n        self.root_dir = rootDir\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data_frame)\n    \n    def __getitem__(self,idx):\n        image_name = self.data_frame.iloc[idx,0]\n        image_path = os.path.join(self.root_dir,\n                                  image_name)\n        image = io.imread(image_path)\n        Id = self.data_frame.iloc[idx,1]\n        sample = {\"image\": image,\"id\": Id}\n        \n        if self.transform:\n            sample = self.transform(sample)\n        \n        return sample \n        ","ec8b18c0":"whaleDataset = WhaleIdDataset(\"..\/input\/train.csv\",\n                              \"..\/input\/train\/\")","eac28b69":"whaleDataset[1][\"image\"].shape","8054ca32":"def show_image(image, id):\n    plt.title(id)\n    plt.imshow(image)\n","b8b58686":"fig = plt.figure()\nfor i in range(3):\n    a = fig.add_subplot(1, 3, i+1)\n    a.axis(\"off\")\n    a.set_title(whaleDataset[i][\"id\"])\n    plt.imshow(whaleDataset[i][\"image\"])","18f9f0ec":"accessing an element","3745959e":"peeking into the input directory","6ffce50a":"looking into the first 2 rows of `train.csv`","bd5ef9dd":"peeking into the train directory","bc9e8a26":"iterating over some values in the dataset","0fb24e77":"Visualizing the image","5c5cb52a":"loaded the dataset (initiated the class)","aab51dce":"## here we are following the PyTorch tutorial to import datasets with built in functions\nDataset class\n-------------\n\n``torch.utils.data.Dataset`` is an abstract class representing a\ndataset.\nYour custom dataset should inherit ``Dataset`` and override the following\nmethods:\n\n-  ``__len__`` so that ``len(dataset)`` returns the size of the dataset.\n-  ``__getitem__`` to support the indexing such that ``dataset[i]`` can\n   be used to get $i$\\ th sample\n\nLet's create a dataset class for our whale identificatin dataset. We will\nread the csv in ``__init__`` but leave the reading of images to\n``__getitem__``. This is memory efficient because all the images are not\nstored in the memory at once but read as required.\n\nSample of our dataset will be a dict\n``{'image': image, 'id': ids}``. Our datset will take an\noptional argument ``transform`` so that any required processing can be\napplied on the sample. We will see the usefulness of ``transform`` in the\nnext section.\n\n\n\n"}}