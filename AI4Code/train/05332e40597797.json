{"cell_type":{"eebf79c7":"code","702957f4":"code","8034316b":"code","20e25542":"code","54e47785":"code","fbef4ade":"code","4049aa41":"code","5e34b58e":"code","4a8e2e76":"code","dc8d44d8":"code","4faf83c1":"code","3058ade2":"code","6620e955":"code","52bbd9f4":"code","f1b3d468":"code","a1a9c2f3":"code","d0ff528b":"code","8d2b6a95":"code","f8a1d28f":"code","edad0675":"code","11466f7f":"code","314117be":"code","fbd73d12":"code","1c8ed367":"code","39223623":"markdown","b097922f":"markdown","09968572":"markdown","77c444e9":"markdown","2e3782db":"markdown","d1cf80f8":"markdown","c528830a":"markdown","4a4e5aa5":"markdown","2fcd2b01":"markdown"},"source":{"eebf79c7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n","702957f4":"try:\n    train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv')\n    print(\"The dataset has {} observations with {} features\".format(*train.shape))\nexcept:\n    print(\"Error when reading the file\")","8034316b":"train.shape","20e25542":"train['Cover_Type'].value_counts()","54e47785":"train.drop('Id', axis=1, inplace=True)","fbef4ade":"import pandas_profiling\ntrain.profile_report()","4049aa41":"train.drop(['Soil_Type15', 'Soil_Type17'], axis=1, inplace=True)","5e34b58e":"#separate target and features\ny = train.Cover_Type\ntrain.drop(['Cover_Type'], axis=1, inplace=True)","4a8e2e76":"# split into train and test datasets\nX_train, X_valid, y_train, y_valid = train_test_split(train, y, train_size=0.8, test_size=0.2, random_state=1)\n\n# Shape of training data (num_rows, num_columns)\nprint(X_train.shape)\n","dc8d44d8":"plt.figure(figsize=(16,10))\nsns.heatmap(pd.concat([y, train], axis=1).corr(), cmap=\"coolwarm\", linecolor=\"white\", linewidth=1)","4faf83c1":"from sklearn import ensemble\nclf = ensemble.RandomForestClassifier()\nprint(clf.get_params())","3058ade2":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt', 'log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nprint(random_grid)","6620e955":"# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = clf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train)","52bbd9f4":"rf_random.best_params_","f1b3d468":"from sklearn.metrics import accuracy_score\n\ndef evaluate(model, X, y):\n    predictions = model.predict(X)\n    errors = abs(y - predictions)\n    acc_score = accuracy_score(y, predictions)\n    print('Model Performance')\n    print('Average Error: {:0.3f}.'.format(np.mean(errors)))\n    print('Accuracy = {:0.3f}%.'.format(acc_score))    \n    return acc_score\n\ndefault_model = ensemble.RandomForestClassifier(n_estimators = 10, random_state = 1)\ndefault_model.fit(X_train, y_train)\ndefault_accuracy = evaluate(default_model, X_valid, y_valid)\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, X_valid, y_valid)\n\nprint('Improvement of {:0.2f}%.'.format( 100 * (random_accuracy - default_accuracy) \/ default_accuracy))\n","a1a9c2f3":"from sklearn.model_selection import GridSearchCV\n\n# Create the parameter grid based on the results of random search \nparams = {\n    'bootstrap': [False],\n    'max_depth': [15, 30],\n    'max_features': ['auto'],\n    'min_samples_leaf': [1, 2, 3],\n    'min_samples_split': [2, 4],\n    'n_estimators': [1000, 1400]\n}\n\ngsv = GridSearchCV(clf, params, cv=4, n_jobs=-1, scoring='accuracy')\ngsv.fit(X_train, y_train)\ngsv.best_estimator_.feature_importances_\n\n","d0ff528b":"print(gsv.best_params_)\ngsv_accuracy = evaluate(gsv.best_estimator_, X_valid, y_valid)\nprint('Improvement of {:0.2f}%.'.format( 100 * (gsv_accuracy - default_accuracy) \/ default_accuracy))\n","8d2b6a95":"# def feature_importances(clf, X, y, figsize=(18, 6)):\n#     clf = clf.fit(X, y)\n    \n#     importances = pd.DataFrame({'Features': X.columns, \n#                                 'Importances': clf.feature_importances_})\n    \n#     importances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\n\n#     fig = plt.figure(figsize=figsize)\n#     sns.barplot(x='Features', y='Importance', data=importances)\n#     plt.xticks(rotation='vertical')\n#     plt.show()\n#     return importances\n    \n# importances = feature_importances(gsv, X_train, y_train)    ","f8a1d28f":"print(classification_report(y_train, gsv.best_estimator_.predict(X_train)))\n\nprint(classification_report(y_valid, gsv.best_estimator_.predict(X_valid)))","edad0675":"from sklearn.metrics import accuracy_score\nacc_score_train = accuracy_score(y_train, gsv.best_estimator_.predict(X_train))\nacc_score_valid = accuracy_score(y_valid, gsv.best_estimator_.predict(X_valid))\nprint('Accuracy score (training data): {}'.format(acc_score_train))\nprint('Accuracy score (validation data): {}'.format(acc_score_valid))","11466f7f":"try:\n    test = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv')\n    print(\"The test dataset has {} observations with {} features\".format(*test.shape))\nexcept:\n    print(\"Error when reading the file\")","314117be":"test.head()","fbd73d12":"# exclude the removed features (Id, Soil_Type15 and Soil_Type7)\nused_features = train.columns\npredictions =  gsv.best_estimator_.predict(test[used_features])#[:, 1]  \n","1c8ed367":"#Create a  DataFrame with predictions\nsubmission = pd.DataFrame({'Id': test['Id'].tolist(), 'Cover_Type':predictions.tolist()})\nfilename = 'My submission Forest Types.csv'\nsubmission.to_csv(filename,index=False)\nprint('Saved file: ' + filename)","39223623":" The best parameters found by the random search:","b097922f":"n_iter = the number of different combinations to try. The more iterations, the wider search space is covered <br>\ncv = the number of folds to use for cross validation. The more cv folds, the less overfitting<br>\n\n\n","09968572":"Use the random grid to search for best hyperparameters\n\n","77c444e9":"To check if random search gives us a better model, we can compare the default model with the best random search model:","2e3782db":"Classes are balanced: ","d1cf80f8":"Soil_Type15 and Soil_Type7 have constant value \"0\" so they can be removed:","c528830a":"We will use categorization accuracy for evaluation.<br>\nIn multilabel classification, the <i>sklearn.metrics.accuracy_score<\/i> function computes subset accuracy: the set of labels predicted for a sample must exactly match the corresponding set of labels in y_true.\nIf the entire set of predicted labels for a sample strictly match with the true set of labels, then the subset accuracy is 1.0; otherwise it is 0.0.","4a4e5aa5":"Adjusting hyperparameters:\n* n_estimators = number of trees in the foreset\n* max_features = max number of features considered for splitting a node\n* max_depth = max number of levels in each decision tree\n* min_samples_split = min number of data points placed in a node before the node is split\n* min_samples_leaf = min number of data points allowed in a leaf node\n* bootstrap = method for sampling data points (with or without replacement)","2fcd2b01":"Random search allows us to narrow the range of values for each hyperparameter.\nInstead of sampling randomly from a distribution, GridSearch tries all combinations we specify"}}