{"cell_type":{"f0b51cfc":"code","c104fa89":"code","d4821fd5":"code","5f6ce143":"code","13e41fcf":"code","3a0416c5":"code","b1e69b2f":"code","8fe42c6c":"code","ac684667":"code","5ed0c0e7":"code","e62b20ff":"code","5f6acf0f":"code","9024f16e":"code","d56d3f33":"code","9bacf1d6":"code","5992ac63":"code","d7d01993":"code","25f92dc9":"code","ff1c72a5":"code","7ecc58db":"code","867ac436":"code","aaeb3316":"code","36db4195":"code","922a98a9":"code","ac721821":"code","f2635712":"code","9f04fb8c":"code","8aef5643":"code","8da30d2d":"code","c741e70d":"code","54e6af76":"code","4eb6bb3d":"code","43c24bca":"code","72bc8d6e":"code","075435b9":"code","5dd132c2":"markdown","bcd02dd4":"markdown","6d3bcb29":"markdown","ebdb5ea7":"markdown","bf988289":"markdown","bbd426ec":"markdown","76055774":"markdown","8d42e973":"markdown","3a8c0286":"markdown","3730af9e":"markdown","4f48a34c":"markdown","47ea25be":"markdown","c8b5b372":"markdown","e37e3e1b":"markdown","55fddf8e":"markdown","723ee8cf":"markdown","87360b29":"markdown","3216a97b":"markdown","d4d3e5cf":"markdown","002d09f7":"markdown","89235704":"markdown"},"source":{"f0b51cfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c104fa89":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","d4821fd5":"df = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin1')\ndf.head()\n","5f6ce143":"print(\"Null Values entry in Unnamed:2 column={}\\nNull Values entry in Unnamed:3 column={}\\nNull Values entry in Unnamed:4 column={}\".format(df['Unnamed: 2'].isnull().sum(),df['Unnamed: 3'].isnull().sum(),df['Unnamed: 2'].isnull().sum()))","13e41fcf":"print(\"shape of dataset: {}\".format(df.shape))","3a0416c5":"df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)","b1e69b2f":"df.head(10)","8fe42c6c":"df.rename(columns={'v1':'Label','v2':'Msg'},inplace=True)","ac684667":"print(\"total no. of ham msgs: {}\\ntotal no. of spam msgs: {}\".format(df['Label'].value_counts()[0],df['Label'].value_counts()[1]))\ndf['Label'].value_counts().plot.bar()","5ed0c0e7":"df.describe()","e62b20ff":"df['len']=df['Msg'].apply(len)\ndf.head()","5f6acf0f":"print(\"Message={}\\n\\nLabel={}\".format(df['Msg'][df['len'].idxmax()],df['Label'][df['len'].idxmax()]))","9024f16e":"print(\"Message={}\\n\\nLabel={}\".format(df['Msg'][df['len'].idxmin()],df['Label'][df['len'].idxmin()]))","d56d3f33":"plt.style.use('seaborn-darkgrid')\nplt.figure(figsize=(10,5))\nsns.distplot(df['len'],kde=False,color='red',hist=True)\nplt.xlabel(\"Message Length\",size=15)\nplt.ylabel(\"Frequency\",size=15)\nplt.title(\"Length Histogram\",size=15)","9bacf1d6":"plt.figure(figsize=(12, 8))\n\ndf[df['Label']=='ham'].len.plot(bins=35, kind='hist', color='red', \n                                       label='Ham messages', alpha=0.6)\ndf[df['Label']=='spam'].len.plot(kind='hist', color='blue', \n                                       label='Spam messages', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Message Length\")","5992ac63":"import re\nimport nltk\nnltk.download('stopwords')\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","d7d01993":"ps = PorterStemmer() # Using porterstemmer for text preprocessing\nmessage = []\nfor i in range(0, df.shape[0]):\n    review = re.sub('[^a-zA-Z]', ' ', df['Msg'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    message.append(review)","25f92dc9":"df['clean_msg']=np.empty((len(message),1))\nfor i in range(len(message)):\n    df['clean_msg'][i]=message[i]\ndf['clean_msg_len']=df['clean_msg'].apply(len)\ndf.head()\n","ff1c72a5":"df['Msg'].describe()","7ecc58db":"df['clean_msg'].describe()","867ac436":"df=pd.concat([df, pd.get_dummies(df['Label'])], axis=1)\ndf.drop(['Label'],axis=1,inplace=True)\ndf.drop(['spam'],axis=1,inplace=True)\ndf.rename(columns={'ham':'label'},inplace=True)\ndf.head()","aaeb3316":"X=df['clean_msg']\nX","36db4195":"Y=df['label']\n","922a98a9":"from sklearn.feature_extraction.text import CountVectorizer\ncv = CountVectorizer(max_features=2500)\nX = cv.fit_transform(message).toarray()\nX\n\n\n\n","ac721821":"# print(X)\nY=np.array(Y)\ntype(Y)\n# t(Y)","f2635712":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.10, random_state = 0)\n","9f04fb8c":"print(\"X_train shape: {}\\n X_test shape: {}\\nY_train shape: {}\\nY_test shape: {}\".format(X_train.shape,X_test.shape,y_train.shape,y_test.shape))","8aef5643":"# list for storing accuracy score of different algorithms\nacc=[]","8da30d2d":"from sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nspam_detect_model = MultinomialNB().fit(X_train, y_train)\npred=spam_detect_model.predict(X_test)\n\nprint(\"Accuracy of Naive Bayes Classifier is: {}\".format(metrics.accuracy_score(y_test,pred)))\nacc.append(metrics.accuracy_score(y_test,pred))","c741e70d":"from sklearn.linear_model import LogisticRegression\nLR=LogisticRegression(solver='liblinear')\nLR.fit(X_train,y_train)\nyhat = LR.predict(X_test)\nprint(\"LogisticRegression's Accuracy:{0}\".format(metrics.accuracy_score(y_test, yhat)))\nacc.append(metrics.accuracy_score(y_test,yhat))","54e6af76":"from sklearn import svm\nclf = svm.SVC(kernel='rbf')\nclf.fit(X_train, y_train) \nyhat = clf.predict(X_test)\nprint(\"SVM's Accuracy:{0}\".format(metrics.accuracy_score(y_test, yhat)))\nacc.append(metrics.accuracy_score(y_test, yhat))","4eb6bb3d":"from sklearn.ensemble import RandomForestClassifier\nRandom_forest = RandomForestClassifier(n_estimators=50)\nRandom_forest.fit(X_train,y_train)\nrandomForest_predict = Random_forest.predict(X_test)\nrandomForest_score = metrics.accuracy_score(y_test, randomForest_predict)\nprint(\"Random Forest Score :\",randomForest_score)\nacc.append(metrics.accuracy_score(y_test,randomForest_predict ))","43c24bca":"from sklearn.ensemble import GradientBoostingClassifier\ngbk = GradientBoostingClassifier(random_state=100, n_estimators=150,min_samples_split=100, max_depth=6)\ngbk.fit(X_train, y_train)\ngbk_predict = gbk.predict(X_test)\nprint(\"Gradient Boosting Score :\",metrics.accuracy_score(y_test,gbk_predict ))\nacc.append(metrics.accuracy_score(y_test,gbk_predict ))","72bc8d6e":"from sklearn.neighbors import KNeighborsClassifier\nmx=-1\nfor i in range(1,25):\n    \n    neigh=KNeighborsClassifier(n_neighbors = i).fit(X_train,y_train)\n    yhat = neigh.predict(X_test)\n    KNN_score=metrics.accuracy_score(y_test, yhat)\n    print(\"KNN Accuracy at {} is {}\".format(i,KNN_score))\n    mx=max(mx,KNN_score)\n    print(\"\\n\")\nacc.append(mx)\nmx","075435b9":"algo_name=['Naive Bayes Classifier','Logistic Regression','SVM','Random Forest Classifier','Gradient Boosting','KNN']\nacc=np.array(acc)\nfrom numpy import median\nplt.figure(figsize=(10,8))\nsns.barplot(y=acc*100,x=algo_name,estimator=median,palette=\"Blues_d\")\nplt.xlabel('Algorithm Name',size=30)\nplt.xticks(rotation=45)\nplt.ylabel('Accuracy',size=30)\n\n","5dd132c2":"# Conclusion","bcd02dd4":"# Spam Message Classifier","6d3bcb29":"# Data Visualization","ebdb5ea7":"# Model Training","bf988289":"# Data Preprocessing","bbd426ec":"**Gradient Boosting**","76055774":"**Dataset reading--->**","8d42e973":"**Table of Contents--**<br>\n<ol>\n    <li>Importing Libraries<\/li>\n    <li>Data Visualization<\/li>\n    <li>Data Preprocessing<\/li>\n    <li>Model Training<\/li>\n    <li>Conclusion<\/li>\n<\/ol>","3a8c0286":"Let's see the shortest msg and its label","3730af9e":"# Vectorization","4f48a34c":"# Please Upvote, if you found this notebook helpful.","47ea25be":"**Naive Bayes Classifier**","c8b5b372":"**Random Forest Classifier**","e37e3e1b":"**Logistic Regression**","55fddf8e":"**Importing libraries-->**","723ee8cf":"From this we can see that, spam label messages are of shorter length than ham messages","87360b29":"**SVM**","3216a97b":"Let's see the longest msg  and its label","d4d3e5cf":"\n**Note**: 1-Ham, 0-Spam","002d09f7":"**Note: I'm using Countvectorizer for vectorization**","89235704":"**KNN**"}}