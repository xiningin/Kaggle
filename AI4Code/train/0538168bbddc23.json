{"cell_type":{"d123fc0a":"code","f9f0f8a6":"code","46d45529":"code","b359af24":"code","c75ab13c":"code","44fab74f":"code","8c27331f":"code","a374e1d4":"code","49ff1d81":"code","7181e5bd":"code","c2be0bb9":"code","adae2ebe":"code","42f48ef2":"code","3a690d2b":"code","13f5b2d7":"code","d19b74be":"code","77cd1ea5":"code","b2de6ffb":"code","497473eb":"code","e6d05c35":"code","7ce741b2":"code","23c1ee9a":"code","7bc7566c":"code","6cbd1074":"code","3e8a241d":"code","cca56924":"code","bccb6a4b":"code","0414ea72":"code","3cea25a1":"code","d0725382":"code","a920fe18":"code","23e82875":"code","78da938d":"code","c8566c9b":"code","1e264e7f":"code","e26119a8":"code","ca382835":"code","da569a9c":"code","198d7e44":"code","7b2e47b1":"code","36d64021":"code","9ff8994a":"code","c1310523":"code","031681e8":"code","449d7e69":"code","c3e483c8":"code","4146bdfd":"code","7dbca1bf":"code","9d9fe327":"code","d97853a3":"markdown","035f7d97":"markdown","e29e102e":"markdown","9bef379f":"markdown","361aeac5":"markdown","90b17231":"markdown","afab03a3":"markdown","bea19d23":"markdown","4d28096c":"markdown","e11f5ecd":"markdown","3a989555":"markdown"},"source":{"d123fc0a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9f0f8a6":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom sklearn.feature_selection import VarianceThreshold","46d45529":"train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","b359af24":"train.head()","c75ab13c":"#concat the both datasets so I get the same featrues for the test set\ndataset = pd.concat([train,test])\ndataset","44fab74f":"#check which numerical features that have nan values\nnumerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerical_values = dataset.select_dtypes(include=numerics)\nnumerical_values.isnull().sum()","8c27331f":"dataset.fillna(dataset.mean(), inplace=True)","a374e1d4":"#select all category features and check which of them has nan values\ncategorical_values = dataset.select_dtypes(include=object)\ncategorical_values.isnull().sum() \/ len(dataset)","49ff1d81":"#drop all columns aht have more than 80% missing values\ndataset.drop(['MiscFeature', 'Fence', 'PoolQC', 'Alley', 'FireplaceQu'], axis=1, inplace=True)\n#update teh categorical_values variable after removing some features\ncategorical_values = dataset.select_dtypes(include=object)","7181e5bd":"categorical_values = dataset.select_dtypes(include=object)\ncategorical_values","c2be0bb9":"#create a function to find all categorical features that have more than 5 unique values\ndef find_features(dataset):\n        \n    features = []\n\n    for x in range(len(dataset.columns)):\n        if dataset[dataset.columns[x]].nunique() > 5:\n            features.append(dataset.columns[x])\n            \n    return features","adae2ebe":"features_to_remove = find_features(categorical_values)","42f48ef2":"features_to_remove","3a690d2b":"#I remove all categorical features that have more than 5 unique values\ndataset.drop(features_to_remove, axis=1, inplace=True)","13f5b2d7":"categorical_values = dataset.select_dtypes(include=object)\ncategorical_values.isnull().sum()","d19b74be":"#Fill all nan values for the categorical features\ndataset = dataset.fillna(dataset.mode().iloc[0])","77cd1ea5":"#create function to loop throug categorical features and add dummy values\ndef dummy_df(df, todummylist):\n    for x in todummylist:\n       dummies = pd.get_dummies(df[x], prefix=x, dummy_na = False)\n       df = df.drop(x, 1)\n       df = pd.concat([df, dummies], axis = 1)\n    return df","b2de6ffb":"#create a new dataset where the categorical variables are transformed to numerical features\ndummies = list(categorical_values)\ndataset_eda = dummy_df(dataset, dummies)","497473eb":"sns.countplot(data = train, x='MSZoning', palette='cool')","e6d05c35":"sns.barplot(data = train , x='MSZoning', y='SalePrice', palette='cool')","7ce741b2":"sns.countplot(data = train , x='LotShape', palette='GnBu')","23c1ee9a":"sns.barplot(data = train , x='LotShape', y='SalePrice', palette='GnBu')","7bc7566c":"sns.countplot(data = train , x='LandContour', palette='cividis')","6cbd1074":"sns.barplot(data = train , x='LandContour', y='SalePrice', palette='cividis')","3e8a241d":"sns.barplot(data = train, x = 'OverallQual', y='SalePrice')","cca56924":"sns.scatterplot(data = train , x='GrLivArea', y='SalePrice')","bccb6a4b":"sns.barplot(data = train , x='GarageCars', y='SalePrice', palette='mako')","0414ea72":"sns.scatterplot(data = train , x='GarageArea', y='SalePrice', palette='mako')","3cea25a1":"#check the correlation efter I've transformed the categorical variables\ncorr=dataset_eda.corr()\nprint (corr['SalePrice'].sort_values(ascending=False)[:20], '\\n')\nprint (corr['SalePrice'].sort_values(ascending=False)[-20:])","d0725382":"#create function to decide which features that are correlated which eachother.\ndef correlation(dataset, threshold):\n    \n    col_corr = set()\n    corr_matrix = dataset.corr()\n    for i in range(len(corr_matrix.columns)):\n        for j in range(i):\n            if abs(corr_matrix.iloc[i, j]) > threshold:\n                colname = corr_matrix.columns[i]\n                col_corr.add(colname)\n    return col_corr           ","a920fe18":"corr_features = correlation(dataset_eda, 0.8)\nlen(set(corr_features))\ncorr_features","23e82875":"dataset_eda.drop(labels=corr_features, axis=1, inplace=True)","78da938d":"dataset_eda","c8566c9b":"train_data = dataset_eda[:len(train)]\ntest_data = dataset_eda[len(train):]","1e264e7f":"train_data.shape, test_data.shape","e26119a8":"from sklearn.preprocessing import MinMaxScaler, RobustScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n","ca382835":"X = train_data.drop(['SalePrice', 'Id'], axis=1)\ny= train_data['SalePrice']","da569a9c":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","198d7e44":"#check how many features that have constant values\nconstants = VarianceThreshold(threshold=0)\nconstants.fit(X_train)\nsum(constants.get_support())","7b2e47b1":"#check which features that are constant and will be dropped\n[x for x in X_train.columns if x not in X_train.columns[constants.get_support()]]","36d64021":"#remove the features that have constant values\nX_train = constants.transform(X_train)\nX_test = constants.transform(X_test)","9ff8994a":"#check how many features that have 99% the same values\nquasi_constants = VarianceThreshold(threshold=0.01)\nquasi_constants.fit(X_train)\n\nsum(quasi_constants.get_support())","c1310523":"X_train = quasi_constants.transform(X_train)\nX_test = quasi_constants.transform(X_test)","031681e8":"X_train.shape, X_test.shape","449d7e69":"regressor = RandomForestRegressor(criterion = 'mse', max_depth = 5)\nregressor.fit(X_train, y_train.ravel())","c3e483c8":"## Applying grid search  to find the best model and the best parameters\nparameters = [{'n_estimators': [10,20,50,80,100,200,300,400, 500,600,800],\n               'criterion': ['mae', 'mse'],\n               'max_depth': [3,5,10,15,20]\n                }]\ngrid_search = GridSearchCV(estimator = regressor, param_grid = parameters, cv=10)\n\ngrid_search = grid_search.fit(X_train, y_train)\n\nbest_para = grid_search.best_params_\nprint(best_para)","4146bdfd":"y_pred = regressor.predict(X_test)","7dbca1bf":"#check accuracy with Cross val score\naccuracies = cross_val_score(estimator = regressor, X = X_train, y = y_train, cv=10)\nprint(accuracies.mean())\nprint(accuracies.std())","9d9fe327":"r2_score(y_test, y_pred)","d97853a3":"**Missing values**","035f7d97":"**I choose to drop all features with ~50% missing values**","e29e102e":"**Setup the data for the model**","9bef379f":"# Load Data","361aeac5":"# Model","90b17231":"# Exploratory Data Analysis","afab03a3":"**Try to find correlations between the features with different plots**","bea19d23":"# Catergorical variables","4d28096c":"**I choose to remove the features that are correlated to another feature**","e11f5ecd":"# Correlation","3a989555":"**After I've fixed my dataset I split up the dataset_eda so I get back the train and test sets**"}}