{"cell_type":{"722f37b2":"code","69884852":"code","7673428e":"code","7a3a4d7e":"code","d602a72a":"code","5198d297":"code","c38e1ee7":"code","446bd5d8":"code","0a9911a4":"code","27aa9f50":"code","78360eb1":"code","5fd6000e":"code","fc110356":"code","f4283728":"code","c0fc088f":"code","81a3a1cd":"code","2df93428":"code","30bde168":"code","f05b67d3":"code","bf57b8a7":"code","fe58fa7c":"code","131647b9":"code","18f93250":"code","9469f8f5":"code","00bcda44":"code","66d3a22b":"code","702d86eb":"code","2b6998a2":"code","8f067fac":"code","f78004c1":"code","39e73396":"code","2d985ba1":"code","1549d641":"code","996ab000":"code","d6e377af":"code","868594a9":"code","5f5c63bc":"markdown","274ea67e":"markdown","ad84a48a":"markdown","8e43a031":"markdown","6aedbba0":"markdown","1475f0d7":"markdown","64ce6a12":"markdown","82daf0d5":"markdown","6fd470ef":"markdown","f4ba919c":"markdown","ee101e92":"markdown","aa25d04e":"markdown","56bda718":"markdown","3b882638":"markdown","c9e5aef7":"markdown","44697967":"markdown","2edda579":"markdown","df307122":"markdown","764abecc":"markdown","aa98fc21":"markdown"},"source":{"722f37b2":"from PIL import Image\nImage.open('..\/input\/imagenes-propias\/ejemplos.png')","69884852":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom scipy import sparse\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport os\nprint(os.listdir(\"..\/input\/the-movies-dataset\"))","7673428e":"movies = pd.read_csv('..\/input\/the-movies-dataset\/movies_metadata.csv')\nmovies = movies.drop([19730, 29503, 35587])\n#Check EDA Notebook for how and why I got these indices.\nmovies['id'] = movies['id'].astype('int')\nmovies.head(2)","7a3a4d7e":"# Load the movielens-100k dataset (download it if needed),\ndata = pd.read_csv('..\/input\/the-movies-dataset\/ratings_small.csv')\ndata = data[data['movieId'].isin(movies['id'])]\ndata = data[['userId','movieId','rating']]\ndata.head()","d602a72a":"Image.open('..\/input\/imagenes-propias\/filtros_colaborativos_intro.png')","5198d297":"# Import surprise modules\nfrom surprise import Dataset\nfrom surprise import Reader\nfrom surprise import Dataset\nfrom surprise import accuracy\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import train_test_split\nfrom surprise.model_selection import GridSearchCV\nfrom surprise import SVD, SVDpp, NMF, SlopeOne, CoClustering, KNNBaseline, KNNWithZScore, KNNWithMeans, KNNBasic, BaselineOnly, NormalPredictor\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity","c38e1ee7":"# Dictionaty of user and items\nusers = list(data['userId'].unique())\nitems = list(data['movieId'].unique())\n\ndef movie_id_to_name(id):\n    return movies.loc[movies['id']==id].original_title\nmovie_id_to_name(1371)","446bd5d8":"# Specific reader for surpirse to work\nreader = Reader(rating_scale= (1,5))\ndata_sp = Dataset.load_from_df(data, reader=reader)","0a9911a4":"Image.open('..\/input\/imagenes-propias\/objetivo_filtros.png')","27aa9f50":"# We'll use the famous SVD algorithm.\nalgo =  [SVD(), NMF(), BaselineOnly(), SlopeOne(), KNNBasic()]\n\nnames = [algo[i].__class__.__name__ for i in range(len(algo))]\n\nresult = np.zeros((len(algo),4))            \nfor i in range(len(algo)):\n    print('Procesando ', names[i], '...')\n    tab = cross_validate(algo[i], data_sp,cv=5, verbose = False)\n    rmse = np.mean(tab['test_rmse']).round(3)\n    mae = np.mean(tab['test_mae']).round(3)\n    ft = np.mean(tab['fit_time']).round(3)\n    tt = np.mean(tab['test_time']).round(3)\n    result[i]=[rmse, mae, ft, tt]\n    \nresult = result.round(3)\nresult = pd.DataFrame(result, index = names, columns =['RMSE','MAE','fit_time','test_time'])","78360eb1":"plt.figure(figsize=(12,8))\nsns.heatmap(result,\n            annot=True,linecolor=\"w\",\n            linewidth=2,cmap=sns.color_palette(\"Blues\"))\nplt.title(\"Data summary\")\nplt.show()","5fd6000e":"param_grid = {'n_epochs': [5, 10, 20], 'lr_all': [0.002, 0.005],\n              'reg_all': [0.4, 0.6], 'n_factors' : [15, 30, 100]}\n\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data_sp)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","fc110356":"Image.open('..\/input\/imagenes-propias\/NMF_large.png')","f4283728":"Image.open('..\/input\/imagenes-propias\/NMF_explicacion_l.png')","c0fc088f":"Image.open('..\/input\/imagenes-propias\/NFM_final.png')","81a3a1cd":"#Crea la matriz de predicciones\n#n = len(users)\n#m = len(items)\n#recomendation = np.zeros((n,m))\n\n#for k in users:\n #   u = users.index(k)\n  #  for l in items:\n   #     i = items.index(l)\n    #    recomendation[u,i] = algo.predict(k,l)[3]","2df93428":"#algo.pu # Users\n#algo.qi # Items\n#sim_users = cosine_similarity(algo.pu, algo.pu)\n#sim_items = cosine_similarity(algo.qi, algo.qi)","30bde168":"from lightfm import LightFM\nfrom lightfm.evaluation import precision_at_k","f05b67d3":"# Function to create an interaction matrix dataframe from transactional type interactions\ninteractions = data.groupby(['userId', 'movieId'])['rating'].sum().unstack().reset_index().fillna(0).set_index('userId')\n    \ninteractions.head()\ninteractions.shape","bf57b8a7":"# Function to create a user dictionary based on their index and number in interaction dataset\nuser_id = list(interactions.index)\nuser_dict = {}\ncounter = 0 \nfor i in user_id:\n    user_dict[i] = counter\n    counter += 1\n\n# Function to create an item dictionary based on their item_id and item name\nmovies = movies.reset_index()\nitem_dict ={}\nfor i in range(movies.shape[0]):\n    item_dict[(movies.loc[i,'id'])] = movies.loc[i,'original_title']","fe58fa7c":"item_dict[5]","131647b9":"# Function to run matrix-factorization algorithm\nx = sparse.csr_matrix(interactions.values)\nmodel = LightFM(no_components= 150, loss='warp')\nmodel.fit(x,epochs=3000,num_threads = 4)","18f93250":"# Evaluate the trained model\nk = 10\nprint('Train precision at k={}:\\t{:.4f}'.format(k, precision_at_k(model, x, k=k).mean()))\n#print('Test precision at k={}:\\t\\t{:.4f}'.format(k, precision_at_k(model, test_matrix, k=k).mean()))","9469f8f5":"def sample_recommendation_user(model, interactions, user_id, user_dict, \n                               item_dict,threshold = 0,nrec_items = 10, show = True):\n    '''\n    Function to produce user recommendations\n    Required Input - \n        - model = Trained matrix factorization model\n        - interactions = dataset used for training the model\n        - user_id = user ID for which we need to generate recommendation\n        - user_dict = Dictionary type input containing interaction_index as key and user_id as value\n        - item_dict = Dictionary type input containing item_id as key and item_name as value\n        - threshold = value above which the rating is favorable in new interaction matrix\n        - nrec_items = Number of output recommendation needed\n    Expected Output - \n        - Prints list of items the given user has already bought\n        - Prints list of N recommended items  which user hopefully will be interested in\n    '''\n    n_users, n_items = interactions.shape\n    user_x = user_dict[user_id]\n    scores = pd.Series(model.predict(user_x,np.arange(n_items)))\n    scores.index = interactions.columns\n    scores = list(pd.Series(scores.sort_values(ascending=False).index))\n    \n    known_items = list(pd.Series(interactions.loc[user_id,:] \\\n                                 [interactions.loc[user_id,:] > threshold].index) \\\n                                .sort_values(ascending=False))\n    #print(known_items)\n    \n    scores = [x for x in scores if x not in known_items]\n    return_score_list = scores[0:nrec_items]\n    known_items = list(pd.Series(known_items).apply(lambda x: item_dict[x]))\n    scores = list(pd.Series(return_score_list).apply(lambda x: item_dict[x]))\n    if show == True:\n        print(\"Known Likes:\")\n        counter = 1\n        for i in known_items:\n            #print(i)\n            print(str(counter) + '- ' + i)\n            counter+=1\n\n        print(\"\\n Recommended Items:\")\n        counter = 1\n        for i in scores:\n            #print(i)\n            print(str(counter) + '- ' + i)\n            counter+=1\n    return return_score_list","00bcda44":"## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = item_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)","66d3a22b":"def sample_recommendation_item(model,interactions,item_id,user_dict,item_dict,number_of_user):\n    '''\n    Funnction to produce a list of top N interested users for a given item\n    Required Input -\n        - model = Trained matrix factorization model\n        - interactions = dataset used for training the model\n        - item_id = item ID for which we need to generate recommended users\n        - user_dict =  Dictionary type input containing interaction_index as key and user_id as value\n        - item_dict = Dictionary type input containing item_id as key and item_name as value\n        - number_of_user = Number of users needed as an output\n    Expected Output -\n        - user_list = List of recommended users \n    '''\n    n_users, n_items = interactions.shape\n    x = np.array(interactions.columns)\n    scores = pd.Series(model.predict(np.arange(n_users), np.repeat(x.searchsorted(item_id),n_users)))\n    user_list = list(interactions.index[scores.sort_values(ascending=False).head(number_of_user).index])\n    return user_list \n\n\n## Calling 15 user recommendation for item id 1\nsample_recommendation_item(model = model,\n                           interactions = interactions,\n                           item_id = 1,\n                           user_dict = user_dict,\n                           item_dict = item_dict,\n                           number_of_user = 15)","702d86eb":"def create_item_emdedding_distance_matrix(model,interactions):\n    '''\n    Function to create item-item distance embedding matrix\n    Required Input -\n        - model = Trained matrix factorization model\n        - interactions = dataset used for training the model\n    Expected Output -\n        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b\/w items\n    '''\n    df_item_norm_sparse = sparse.csr_matrix(model.item_embeddings)\n    similarities = cosine_similarity(df_item_norm_sparse)\n    item_emdedding_distance_matrix = pd.DataFrame(similarities)\n    item_emdedding_distance_matrix.columns = interactions.columns\n    item_emdedding_distance_matrix.index = interactions.columns\n    return item_emdedding_distance_matrix\n\n## Creating item-item distance matrix\nitem_item_dist = create_item_emdedding_distance_matrix(model = model,\n                                                       interactions = interactions)\n## Checking item embedding distance matrix\n#item_item_dist.head()","2b6998a2":"def item_item_recommendation(item_emdedding_distance_matrix, item_id, \n                             item_dict, n_items = 10, show = True):\n    '''\n    Function to create item-item recommendation\n    Required Input - \n        - item_emdedding_distance_matrix = Pandas dataframe containing cosine distance matrix b\/w items\n        - item_id  = item ID for which we need to generate recommended items\n        - item_dict = Dictionary type input containing item_id as key and item_name as value\n        - n_items = Number of items needed as an output\n    Expected Output -\n        - recommended_items = List of recommended items\n    '''\n    recommended_items = list(pd.Series(item_emdedding_distance_matrix.loc[item_id,:]. \\\n                                  sort_values(ascending = False).head(n_items+1). \\\n                                  index[1:n_items+1]))\n    if show == True:\n        print(\"Item of interest :{0}\".format(item_dict[item_id]))\n        print(\"Item similar to the above item:\")\n        counter = 1\n        for i in recommended_items:\n            print(str(counter) + '- ' +  item_dict[i])\n            counter+=1\n    return recommended_items\n\n## Calling 10 recommended items for item id \nrec_list = item_item_recommendation(item_emdedding_distance_matrix = item_item_dist,\n                                    item_id = 6,\n                                    item_dict = item_dict,\n                                    n_items = 10)","8f067fac":"Image.open('..\/input\/imagenes-propias\/filtros_colaborativos_intro.png')","f78004c1":"from sklearn.metrics.pairwise import cosine_similarity\nfrom ast import literal_eval\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.metrics.pairwise import linear_kernel, cosine_similarity\nfrom nltk.stem.snowball import SnowballStemmer\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n\nimport warnings; warnings.simplefilter('ignore')","39e73396":"md = movies\n\nmd['genres'] = md['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nlinks_small = pd.read_csv('..\/input\/the-movies-dataset\/links_small.csv')\nlinks_small = links_small[links_small['tmdbId'].notnull()]['tmdbId'].astype('int')\n\n#Check EDA Notebook for how and why I got these indices.\nsmd = md[md['id'].isin(links_small)]\n\ncredits = pd.read_csv('..\/input\/the-movies-dataset\/credits.csv')\nkeywords = pd.read_csv('..\/input\/the-movies-dataset\/keywords.csv')\nkeywords['id'] = keywords['id'].astype('int')\ncredits['id'] = credits['id'].astype('int')\nmd['id'] = md['id'].astype('int')\n\nmd = md.merge(credits, on='id')\nmd = md.merge(keywords, on='id')\n\nsmd = md[md['id'].isin(links_small)]\n\nsmd['cast'] = smd['cast'].apply(literal_eval)\nsmd['crew'] = smd['crew'].apply(literal_eval)\nsmd['keywords'] = smd['keywords'].apply(literal_eval)\nsmd['cast_size'] = smd['cast'].apply(lambda x: len(x))\nsmd['crew_size'] = smd['crew'].apply(lambda x: len(x))\ndef get_director(x):\n    for i in x:\n        if i['job'] == 'Director':\n            return i['name']\n    return np.nan\nsmd['director'] = smd['crew'].apply(get_director)\nsmd['cast'] = smd['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\nsmd['cast'] = smd['cast'].apply(lambda x: x[:3] if len(x) >=3 else x)\nsmd['keywords'] = smd['keywords'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])\n\nsmd['cast'] = smd['cast'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\nsmd['director'] = smd['director'].astype('str').apply(lambda x: str.lower(x.replace(\" \", \"\")))\nsmd['director'] = smd['director'].apply(lambda x: [x,x, x])\n\n#Keywords\ns = smd.apply(lambda x: pd.Series(x['keywords']),axis=1).stack().reset_index(level=1, drop=True)\ns.name = 'keyword'\ns = s.value_counts()\ns = s[s > 1]\n\nstemmer = SnowballStemmer('english')\n\ndef filter_keywords(x):\n    words = []\n    for i in x:\n        if i in s:\n            words.append(i)\n    return words\n\nsmd['keywords'] = smd['keywords'].apply(filter_keywords)\nsmd['keywords'] = smd['keywords'].apply(lambda x: [stemmer.stem(i) for i in x])\nsmd['keywords'] = smd['keywords'].apply(lambda x: [str.lower(i.replace(\" \", \"\")) for i in x])\nsmd['soup'] = smd['keywords'] + smd['cast'] + smd['director'] + smd['genres']\nsmd['soup'] = smd['soup'].apply(lambda x: ' '.join(x))\n\ncount = CountVectorizer(analyzer='word',ngram_range=(1, 2),min_df=0, stop_words='english')\ncount_matrix = count.fit_transform(smd['soup'])\n\nitem_features = count_matrix","2d985ba1":"Image.open('..\/input\/imagenes-propias\/espacio_vectorial_rec.png')","1549d641":"cosine_sim = linear_kernel(item_features, item_features)\n\nsmd = smd.reset_index()\ntitles = smd['title']\nindices = pd.Series(smd.index, index=smd['title'])\n\ndef get_recommendations(title):\n    idx = indices[title]\n    sim_scores = list(enumerate(cosine_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:31]\n    movie_indices = [i[0] for i in sim_scores]\n    return titles.iloc[movie_indices]\n\nget_recommendations('The Godfather').head(10)","996ab000":"Image.open('..\/input\/imagenes-propias\/contenido_3.png')","d6e377af":"x = sparse.csr_matrix(interactions.values)\nmodel = LightFM(no_components= 15, loss='warp',k=5)\nmodel.fit(x,item_features=item_features,epochs=4,num_threads = 2)","868594a9":"## Calling 10 movie recommendation for user id 11\nrec_list = sample_recommendation_user(model = model, \n                                      interactions = interactions, \n                                      user_id = 11, \n                                      user_dict = user_dict,\n                                      item_dict = item_dict, \n                                      threshold = 4,\n                                      nrec_items = 10,\n                                      show = True)","5f5c63bc":"Existen varios enfoques para hacer recomendaciones de pel\u00edculas y en general de cualquier \u00edtem. Este notebook se va a concentrar en tres de ellos:\n* Filtros colaborativos\n* Basados en contenido\n* Hibrido \n\nEs indispensable tener una matriz de rankings por usuarios para completar el ejercicio de filtros colaborativos y una base de caracter\u00edsticas de productos o usuarios para hacer el ejercicio basado en contenido. Muchas veces el camino a tomar se basa en que informaci\u00f3n se tiene disponible, en este caso en particular se pueden desarrollar ambas metodolog\u00edas e incluso mezclarlas para generar un resultado m\u00e1s robusto.","274ea67e":"La matriz recomendation contiene los resultados para todos los \u00edtems para todos los usuarios. Solo en caso de querer comparaciones entre usuarios se podr\u00eda aplicar los siguientes m\u00e9todos que se explicaran con m\u00e1s detalle m\u00e1s adelante. (no se ejecuta dado el tiempo de procesamiento)","ad84a48a":"\nLos resultado muestran que por* Root Mean Squared Error (RMSE) *y *Mean Absolute Error (MAE)* las **matrices de factorizaci\u00f3n no negativas** tienen el mejor desempe\u00f1o sin embargo singular value decomposition es una metodolog\u00eda muy popular as\u00ed que se har\u00e1 un poco de optimizaci\u00f3n de sus hyperparametros con el fin de ver si mejora sus resultados.\n","8e43a031":"### Surprise\n\n[Surprise](http:\/\/surpriselib.com\/) es una librer\u00eda basada en *sklearn* que facilita el desarrollo de diferentes tipos de sistemas de recomendaci\u00f3n. Adem\u00e1s provee de las herramientas para evaluar, analizar y comparar diferentes modelos con sus respectivas medidas de desempe\u00f1o. Su nombre SurPRISE habla de su objetivo mismo *Simple Python RecommendatIon System Engine*","6aedbba0":"Este notebook est\u00e1 basado en el **dataset de peliculas**. Sobre las diferentes tablas disponibles se trabajara en dos principalmente: \n* Descripci\u00f3n cualitativa de la pel\u00edcula\n* Calificaciones de varios usuarios sobre las pel\u00edculas disponibles.","1475f0d7":"Con estas caracter\u00edsticas se puede a llegar a identificar \u00edtems en este caso pel\u00edculas que compartan varias caracter\u00edsticas. Esto se hace mediante la representaci\u00f3n vectorial de dichas caracter\u00edsticas y una medida de que tan similares son.","64ce6a12":"En general el pre procesamiento busca estandarizar texto como caracter\u00edsticas propias de cada \u00edtem, para esto usa el texto keywords, extrae los 3 actores principales, el director y g\u00e9neros y con ello crea una matriz de caracter\u00edsticas para cada \u00edtem. Usa la metodologia Tf-idf.","82daf0d5":"## Lightfm\n\n[LightFM]( https:\/\/github.com\/lyst\/lightfmes) una librer\u00eda que utiliza los algoritmos de perdida WARP (Weighted Approximate-Rank Pairwise y BPR (Bayesian Personalised Ranking) con el fin de crear truplas (usuario, \u00edtem positivo, \u00edtem negativo) con el fin de estimar las preferencias desconocidas. Adem\u00e1s hace posible combinar meta datos dentro de sistemas colaborativos.","6fd470ef":"# Sistemas de Recomendaci\u00f3n\nBusca identificar las preferencias de los usuarios a partir de informaci\u00f3n de sus selecciones, de los \u00edtems, y de la poblaci\u00f3n. Un sistema de recomendaci\u00f3n o motor compara el perfil de usuarios con caracter\u00edsticas de los \u00edtems y predictivamente intenta identificar la calificaci\u00f3n que este le dar\u00eda. Estas caracter\u00edsticas pueden ser dadas por el mismo \u00edtem o inferidas impl\u00edcitamente a trav\u00e9s del contexto que dan otros usuarios al mismo.\n\n*  *\u201cMore than 80 per cent of the TV shows people watch on Netflix are discovered through the platform\u2019s recommendation system\u201d*\n*  *\u201c35% of Amazon.com\u2019s revenue is generated by its recommendation engine\u201d*\n\nUsualmente se usan en estrategias de venta cruzada, lealtad, awerness, targeting e innovaci\u00f3n. Estos son algunos ejemplos:\n","f4ba919c":"# Aplicaciones","ee101e92":"Cada factor entonces se convierte en una caracter\u00edstica de un perfil general del usuario y los \u00edtems. Es importante tambi\u00e9n agregar el sesgo que tanto usuarios como \u00edtems pueden tener; en este contexto, un usuario que lo califique todo bien o una pel\u00edcula que muy popular que todos opinen que es muy buena.","aa25d04e":"Con el fin de simplificar el c\u00f3digo y hacer uso de librer\u00edas m\u00e1s populares para este fin se va a considerar el paquete *Surprise* y *Lightfm*.","56bda718":"Al modelo NMF calcula factores que son una descripci\u00f3n simplificada de lo que es un usuario y un \u00edtem. Luego de tener estos optimizando su c\u00e1lculo con un ejercicio de minimizaci\u00f3n de los errores es posible extrapolar dichos factores para calcular ranking no conocidos. Trabajar con factorizaci\u00f3n de matrices tiene varios beneficios, el primero es que simplifica el problema y hace que el ruido de rannkigs no desdibuje el perfil del usuario o el \u00edtem.","3b882638":"El proceso de selecci\u00f3n del mejor algoritmo sigue los mismos principios de un proceso de **Machine Learning** convencional. [Notebook](https:\/\/www.kaggle.com\/camiloemartinez\/a-new-pulsar-star-supervised-machine-learning) para m\u00e1s informaci\u00f3n.","c9e5aef7":"Dado que no mejora los resultados nos quedamos con NFM.","44697967":"All the [preprocesing](https:\/\/www.kaggle.com\/rounakbanik\/movie-recommender-systems) is part of this notebook. This notebook start after all the cleaning and preprocessing.","2edda579":"Aplicamos un pre procesamiento con el fin de que la librer\u00eda funcione de la mejor manera.","df307122":"Diferentes maneras de medir el desempe\u00f1o.","764abecc":"# Hibrido","aa98fc21":"Explicacion: SVD(), NMF(), BaselineOnly(), SlopeOne(), KNNBasic()]"}}