{"cell_type":{"7e701cd8":"code","05280297":"code","e647c240":"code","4c13197c":"code","6a1ac483":"code","7161af75":"code","42275a09":"code","30ab44de":"code","ca72a4cb":"code","03657336":"code","3f62f9f8":"code","3582149e":"code","dbf71e3d":"code","9cb457ea":"code","4cf6fe95":"code","2254b55a":"code","a28c1082":"code","f4aea40f":"code","be7e38be":"code","f3050e13":"code","ecc6ba4b":"code","ae57bc33":"code","49383105":"markdown","3fda3f2c":"markdown"},"source":{"7e701cd8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport random\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","05280297":"data = pd.read_csv(\"\/kaggle\/input\/salary-data-with-age-and-experience\/Salary_Data.csv\", delimiter=\",\")\ndata = np.array(data)\nm, n = data.shape\nseed = 4\nfor i in range(n):\n    random.seed(seed)\n    random.shuffle(data[:, i])\nprint(data)","e647c240":"m, n = data.shape\nm_test = round(m\/5)\nm_train = m-m_test\nX_train = data[:m_train,0:2]\nX_test = data[(m_train):m,0:2]\ny_train = data[:m_train, 2]\ny_test = data[(m_train):m, 2]\nprint(X_test)\nprint(y_test)\nprint(m, n)","4c13197c":"print(m_test, m_train)","6a1ac483":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D","7161af75":"fig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(121, projection='3d')\nfor i in range(m_train):\n  ax.scatter3D(X_train[i][0], X_train[i][1], y_train[i], cmap='viridis')\nplt.xlabel('x0')\nplt.ylabel('x1')\nplt.title('y')","42275a09":"def featureNormalize(X):\n  mu = np.mean(X, axis = 0)\n  sigma = np.std(X, axis = 0)\n  X_norm = (X-mu)\/sigma\n  return X_norm, mu, sigma","30ab44de":"X_norm, mu, sigma = featureNormalize(X_train)\nprint(X_norm)\nX_train = np.concatenate([np.ones((m_train, 1)), X_norm], axis = 1)\nprint(X_train)","ca72a4cb":"def costFunction(theta, X, y):\n    m, n = X.shape\n    h = X.dot(theta)\n    J = 1\/(2*m)*np.linalg.norm(X.dot(theta)-y)**2\n    grad = 1\/m*X.T.dot(h-y)\n    return J, grad","03657336":"def gradientDescentMulti(X, y, theta, alpha, num_Iterations):\n    J_history = []\n    m, n = X.shape\n    for i in range(num_Iterations):\n        gr = costFunction(theta, X, y)[1]\n        theta -= alpha*gr\n        tmp=costFunction(theta, X, y)[0]\n        J_history.append(tmp)\n    return theta, J_history","3f62f9f8":"alpha = 1.0\nnum_Iterations = 700\ntheta = np.zeros(3)\ntheta, J_history = gradientDescentMulti(X_train, y_train, theta, alpha, num_Iterations=1000)\nplt.plot(np.arange(len(J_history)), J_history, lw = 2)\nprint(costFunction(theta, X_train, y_train)[0])\nprint(theta)\nplt.xlabel('Number of iterations')\nplt.ylabel('Cost')\nplt.show()","3582149e":"X_norm_test = (X_test-mu)\/sigma\nX_test = np.concatenate([np.ones((m_test, 1)), X_norm_test], axis = 1)\nprint(X_test)","dbf71e3d":"cost, grad = costFunction(theta, X_test, y_test)\nprint('Cost of data test: {:.3f}'.format(cost))","9cb457ea":"X_train = data[:m_train,0:2]\nX_train = np.concatenate([np.ones((m_train, 1)), X_train], axis = 1)\nX_test = data[(m_train):m,0:2]\nX_test = np.concatenate([np.ones((m_test, 1)), X_test], axis = 1)\ny_train = data[:m_train, 2]\ny_test = data[(m_train):m, 2]\ntheta = (np.linalg.pinv(X_train.T.dot(X_train))).dot(X_train.T).dot(y_train)\nprint('Cost of data train use calculus: {:.3f}'.format(costFunction(theta, X_train, y_train)[0]))\nprint('Cost of data test use calculus: {:.3f}'.format(costFunction(theta, X_test, y_test)[0]))","4cf6fe95":"def createFeature(X1, X2, power):\n    out = []\n    for i in range(1, power+1):\n        for j in range(i+1):\n            out.append((X1**(i-j))*(X2**(j)))\n    return np.stack(out, axis = 1)\n            \n            ","2254b55a":"X_train = data[:m_train,0:2]\nX_test = data[(m_train):m,0:2]","a28c1082":"X_train = createFeature(X_train[:, 0], X_train[:, 1], 3)\nX_test = createFeature(X_test[:, 0], X_test[:, 1], 3)\nX_train, mu ,sigma = featureNormalize(X_train)\nX_train = np.concatenate([np.ones((m_train, 1)), X_train], axis = 1)\nX_test = (X_test-mu)\/sigma\nX_test = np.concatenate([np.ones((m_test, 1)), X_test], axis = 1)\nprint(X_test)","f4aea40f":"def costFunction2(theta, X, y, lambda_):\n    m, n = X.shape\n    h = X.dot(theta)\n    J=0\n    gr=[]\n    J = 1\/(2*m)*(np.linalg.norm(h-y))**2\n    for i in range(1, n):\n        J+=lambda_\/(2*m)*(theta[i]*theta[i])\n    grad = 1\/m*X.T.dot(h-y)\n    for i in range (1, n):\n        grad[i] += lambda_\/m*theta[i]\n    return J, grad","be7e38be":"def gradientDescent2(theta, X, y, alpha, lambda_, numIterations):\n    m, n = X.shape\n    for i in range(num_Iterations):\n        gr = costFunction2(theta, X, y, lambda_)[1]\n        theta -= alpha*gr\n        tmp=costFunction2(theta, X, y, lambda_)[0]\n    return theta","f3050e13":"def J_lambda(theta, X, y, alpha, lambda_, num_Iteration):\n    alpha = 0.22\n    theta = np.zeros(X_train.shape[1])\n    theta = gradientDescent2(theta, X, y, alpha, lambda_, num_Iterations)\n    cost = costFunction2(theta, X_train, y_train, lambda_)[0]\n    return theta, costFunction2(theta, X_train, y_train, lambda_)[0]\n\n","ecc6ba4b":"J_vals = np.zeros((50, 50))\nlambda_ = np.linspace(0.02, 1, 50)\nalpha = np.linspace(0.02, 1, 50)\nminn = float(pow(10, 100))\nalpha_min = 0\nlambda_min = 0\nfor i in range(10):\n    for j in range(10):\n        tmp_theta, J_vals[i, j] = (J_lambda(theta, X_train, y_train, alpha[i], lambda_[j], 1000))\n#         print(J_vals[i, j])\n        if minn >= J_vals[i, j]:\n            minn = J_vals[i, j]\n            alpha_min = alpha[i]\n            lambda_min = lambda_[j]\n            theta = tmp_theta\nJ_vals = J_vals.T\n\n# surface plot\nfig = plt.figure(figsize=(12, 5))\nax = fig.add_subplot(121, projection='3d')\nax.plot_surface(lambda_, alpha, J_vals, cmap='viridis')\nplt.xlabel('lambda')\nplt.ylabel('alpha')\nplt.title('J')\nprint(minn, alpha_min, lambda_min)\nprint(theta)\n\n","ae57bc33":"print('Cost of data test use polynomail features: {:.3f}'.format(costFunction2(theta, X_test, y_test, lambda_min)[0]))","49383105":"Use Polynomial Feature","3fda3f2c":"Use Calculus"}}