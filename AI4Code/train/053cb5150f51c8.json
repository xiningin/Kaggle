{"cell_type":{"89ec1138":"code","85f981bc":"code","f5ebb43a":"code","d5f9c6fe":"code","be2f14b8":"code","cb589498":"code","75f060d3":"code","2087fe59":"code","6a48f619":"code","ed0e8fdc":"code","377be66a":"code","431c0ea8":"code","6e1c6e9c":"code","96314b89":"code","b2a5ddec":"code","3ba71d75":"code","6f791228":"code","4502168e":"code","064f4301":"code","3e6605cd":"code","5db954e3":"code","0face05c":"code","9a16e1e8":"code","ffcfaaf7":"code","d5314e8b":"code","36a27375":"code","b96427bd":"code","adc7c6b7":"code","0a80c68b":"code","4df805c1":"code","44d486d5":"code","1a8630d3":"code","24968c88":"code","c7eaaab5":"code","df4dc3ef":"code","0642aa7b":"code","2d650990":"code","930bae39":"code","fdaaee5b":"code","6c58b1d9":"code","307a955d":"code","f1aa096d":"code","6086d9f6":"code","aeb35ad9":"code","c2b0dc63":"code","43b406d7":"code","5e63c078":"code","6d1ef716":"code","6fab8f94":"code","711c64ad":"code","2df505c0":"code","c8ea96c6":"code","13ed3c45":"code","0eaa864e":"markdown","934a28c0":"markdown","6cb3a65e":"markdown","4d68f4d2":"markdown","aa98c80c":"markdown","6035b387":"markdown","35fa5554":"markdown","02ac8bf5":"markdown","ff606a68":"markdown","418bce49":"markdown","1efeb0a7":"markdown","8cf4c82b":"markdown","655e6358":"markdown","282d8f05":"markdown","19630335":"markdown","71cfd7ac":"markdown"},"source":{"89ec1138":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import cross_val_score","85f981bc":"# Import dataset\ndf = pd.read_csv('..\/input\/loan-default-prediction\/train_v2.csv.zip')\ndf.head()","f5ebb43a":"df.shape","d5f9c6fe":"# Check duplication in dataframe\ndf[df.duplicated()].shape","be2f14b8":"# The number of each data type in the dataframe\ndf.dtypes.value_counts()","cb589498":"# Loss Distribution\nfig , ax = plt.subplots()\nplt.hist(df['loss'], bins = 20, range=(0,100))\nax.set_ylim([0,3000])\nplt.show()","75f060d3":"# Calculate percent of missing in each row\ndf['num_missing'] = df.isnull().sum(axis = 1)\/df.shape[1]\n\n# Drop row that percent of missing more than 20%\nmissing_row = df[df['num_missing'] > 0.20].index\ndf.drop(df.index[missing_row], inplace = True)\ndf.shape","2087fe59":"# Drop id and num_missing collumn\ndf.drop(columns = ['id','num_missing'], inplace = True)","6a48f619":"# Calculate percent of missing in each column\ncol_pct_miss = []\nfor col in df.columns:\n    percent_miss = np.mean(df[col].isnull())*100\n    if percent_miss > 0:\n        col_pct_miss.append([col, percent_miss])\n    \ncol_pct_miss_df = pd.DataFrame(col_pct_miss, columns = ['column_name','% of Missing']).sort_values(by = '% of Missing', ascending = False)\ncol_pct_miss_df","ed0e8fdc":"# Impute missing value in numeric columns with median \nnumeric_cols = df.select_dtypes(include=['number']).columns.values\n\nfor col in numeric_cols:\n    if col in list(col_pct_miss_df.column_name) :\n        med = df[col].median()\n        df[col] = df[col].fillna(med)","377be66a":"# Impute missing value in categorical columns with mode\nnot_numeric_cols = df.select_dtypes(exclude=['number']).columns.values\n\nfor col in not_numeric_cols:\n    if col in list(col_pct_miss_df.column_name):\n        mode = df[col].mode()[0]\n        df[col] = df[col].fillna(mode)","431c0ea8":"# Check missing value\ndf.isnull().sum().value_counts()","6e1c6e9c":"# Drop Highly Corelated Columns\n\n# Create correlation matrix\ncorr_matrix = df.corr().abs()\n\n# Select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# Find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n\ndf.drop(columns = to_drop, inplace = True)","96314b89":"#Drop Repetitive Columns\nnum_rows = df.shape[0]\nrep_cols = []\n\nfor col in df.loc[:, df.columns != 'loss'].columns :\n    cnts = df[col].value_counts()\n    top_pct = (cnts\/num_rows).iloc[0]\n    \n    if top_pct > 0.80:\n        rep_cols.append([col,top_pct])\n        \nrep_col_df = pd.DataFrame(rep_cols, columns = ['column_name','% top repetitve value']).sort_values(by = '% top repetitve value', ascending = False).reset_index(drop=True)\nrep_col_df\n\ndf.shape","b2a5ddec":"cat_cols = df.select_dtypes(exclude=['number']).columns.values\n\ndrop_cols = []\nkeep_cols = []\nfor col in cat_cols:\n    if df[col].value_counts().count() > 20000 : \n        print('column {} has {} categories > drop'.format(col,df[col].value_counts().count()))\n        drop_cols.append(col)\n    else : \n        print('column {} has {} categories > keep'.format(col,df[col].value_counts().count()))\n        keep_cols.append(col)","3ba71d75":"# Binary Encoding\nimport category_encoders as ce\nencoder = ce.BinaryEncoder(cols = keep_cols)\nbi_enc_df = encoder.fit_transform(df[keep_cols])\nbi_col_name = bi_enc_df.columns\nbi_enc_df.head()\n\n#Add Binary Encding to dataframe and drop all categorical columns\ndf = pd.concat([df,bi_enc_df],axis = 1)\ndf.head()","6f791228":"# Add a 'loan_status' collumn which 1 represents default loan and 0 represents not default loan.\ndf['loan_status'] = np.where(df['loss'] > 0, 1, 0)\ndf.head()","4502168e":"# After generate a visualization from loan_status in dataframe. \n# We found that the data is imbalance.\n\nax = sns.countplot(x = 'loan_status', data=df)\nplt.show()\n\ndf['loan_status'].value_counts()","064f4301":"#Resampling Data\nfrom sklearn.utils import resample\n\n#Seperate each target class into 2 dataframes\nnot_default = df[df['loan_status'] == 0]\ndefault = df[df['loan_status'] == 1]\n\n#Resample dataframe\nresample_df_d = resample(default,\n                       replace = False,\n                       n_samples = 9778,\n                       random_state = 1234)\n\nresample_df_n = resample(not_default,\n                       replace = False,\n                       n_samples = 9778,\n                       random_state = 1234)\n\nresample_df = pd.concat([resample_df_n, resample_df_d])","3e6605cd":"from sklearn.model_selection import train_test_split\n\nX = resample_df.drop(columns = ['loss','loan_status'])\nY = resample_df['loss']\n\nX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 0.2, random_state = 1234, stratify = resample_df['loan_status'])\nprint('training set = {} records, test set= {} records'.format(X_train.shape[0],X_test.shape[0]))","5db954e3":"from sklearn.feature_selection import SelectPercentile , SelectKBest, f_regression , f_classif","0face05c":"#Select top 170 important numerical columns with filter method\nX_train_num = X_train.drop(columns = bi_col_name)\n\nselector = SelectKBest(score_func = f_regression, k = 170)\nselector.fit(X_train_num,Y_train)\n\nselect_cols = selector.get_support(indices = True)\nselect_num_cols = X_train_num.iloc[:,select_cols]\n\nselect_num_col_name = select_num_cols.columns\nselect_num_cols.head()","9a16e1e8":"#Select top 150 important numerical columns with RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nselector = RFE(LogisticRegression(), n_features_to_select=150, step=1, verbose = 2)\nselector = selector.fit(select_num_cols, Y_train)\nselect_cols = selector.get_support(indices = True)\nselect_cols_df = select_num_cols.iloc[:,select_cols]\n\nbest_X_col_name = select_cols_df.columns\nselect_cols_df.head()","ffcfaaf7":"# Select top 5 important categorical columns with filter method\n\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nX_train_cat = X_train.select_dtypes(exclude = 'number').copy()\n\n# Create encoder\nle = LabelEncoder()\nX_train_cat = X_train_cat.apply(lambda col: le.fit_transform(col.astype(str)), axis=0, result_type='expand')\n\n# Prepare input data \noe = OrdinalEncoder()\noe.fit(X_train_cat)\nX_train_cat_enc = oe.transform(X_train_cat)\n\nselector = SelectKBest(score_func = f_classif , k=5)\nselector.fit(X_train_cat_enc,Y_train)\n\nselect_cols = selector.get_support(indices = True)\nselect_cat_cols = X_train_cat.iloc[:,select_cols]\n\nselect_cat_col_name = select_cat_cols.columns\nselect_cat_cols.head()","d5314e8b":"#Combine categorical and non-categorical dataframe together\ndef filter_x_df(x):\n    df = x.copy()\n    all_filter_col = []\n    \n    for keep in select_cat_col_name[select_cat_col_name.isin(keep_cols)]:\n        filter_col = [col for col in df.columns if col.startswith(str(keep))]\n        for col in filter_col : \n            if col not in keep_cols:\n                all_filter_col.append(col)\n        \n    drop_cat_df = df.drop(columns = cat_cols)\n    \n    new_df = pd.concat([drop_cat_df[best_X_col_name],drop_cat_df[all_filter_col]],axis = 1)\n    return new_df","36a27375":"# Create filter_X_train and filter_X_test dataframe \n# Apply filter_x_df function to X_train and X_test\n\nfilter_X_train = filter_x_df(X_train)\nfilter_X_test = filter_x_df(X_test)","b96427bd":"filter_X_train.head()","adc7c6b7":"from sklearn.preprocessing import StandardScaler \n\nscaler = StandardScaler()  \nscaler.fit(filter_X_train)\n\nX_train_scal = scaler.fit_transform(filter_X_train)\nX_test_scal = scaler.fit_transform(filter_X_test)","0a80c68b":"from sklearn.neighbors import KNeighborsClassifier\n\nneigh = KNeighborsClassifier()\nneigh.fit(X_train_scal, Y_train)\n\nKnn_y_pred_train =  neigh.predict(X_train_scal)\nKnn_y_pred_test =  neigh.predict(X_test_scal)\n\nscores_kn = cross_val_score(estimator = neigh, y = Y_train, X = X_train_scal, cv=5)\nprint('Cross Validation Score:', np.mean(scores_kn))","4df805c1":"from sklearn.linear_model import LogisticRegression\n\nlogisticRegr = LogisticRegression()\nlogisticRegr = logisticRegr.fit(X_train_scal, Y_train)\n\nLr_y_pred_train = logisticRegr.predict(X_train_scal)\nLr_y_pred_test = logisticRegr.predict(X_test_scal)\n\nscores_lr = cross_val_score(estimator = logisticRegr, y = Y_train, X = X_train_scal, cv=5)\nprint('Cross Validation Score:', np.mean(scores_lr))","44d486d5":"from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier(max_depth = 70)\nrf.fit(filter_X_train, Y_train)\n\nrf_y_pred_train =  rf.predict(filter_X_train)\nrf_y_pred_test =  rf.predict(filter_X_test)\n\nscores_rf = cross_val_score(estimator = rf, y = Y_train, X = filter_X_train, cv=5)\nprint('Cross Validation Score:', np.mean(scores_rf))","1a8630d3":"from  xgboost import XGBClassifier\n\nxgb = XGBClassifier(gamma=0, learning_rate=0.1, max_depth=100, n_estimators=100)\nxgb.fit(filter_X_train,Y_train)\n\nxgb_y_pred_train = xgb.predict(filter_X_train)\nxgb_y_pred_test = xgb.predict(filter_X_test)\n\nscores_xg = cross_val_score(estimator = rf, y = Y_train, X = filter_X_train, cv=5)\nprint('Cross Validation Score:', np.mean(scores_xg))","24968c88":"# Importing SMOTE\nfrom imblearn.over_sampling import SMOTE\n\n# Oversampling the data\ndf_sm = df.drop(columns = cat_cols)\nsmote = SMOTE(random_state = 1234)\nX_sm, Y_sm = smote.fit_resample(df_sm.drop(columns = 'loan_status'), df_sm['loan_status'])\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(Y_sm == 1))) \nprint(\"After OverSampling, counts of label '0': {}\".format(sum(Y_sm == 0)))","c7eaaab5":"# Split Train Set & Test Set\nfrom sklearn.model_selection import train_test_split\n\nY = X_sm['loss']\nX = X_sm.drop(columns = 'loss')\n\nX_train, X_test, Y_train, Y_test = train_test_split( X, Y, test_size = 0.2, random_state = 1234, stratify = Y_sm)\nprint('training set = {} records, test set= {} records'.format(X_train.shape[0],X_test.shape[0]))","df4dc3ef":"# Select top 170 important numerical columns with filter method\nX_train_num = X_train.drop(columns = bi_col_name)\n\nselector = SelectKBest(score_func = f_regression, k = 170)\nselector.fit(X_train_num,Y_train)\n\nselect_cols_sm = selector.get_support(indices = True)\nselect_num_cols_sm = X_train_num.iloc[:,select_cols_sm]\n\nselect_num_col_name = select_num_cols_sm.columns\nselect_num_cols_sm.head()","0642aa7b":"# Select top 150 important numerical columns with RFE\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\nselector = RFE(LogisticRegression(), n_features_to_select=150, step=1, verbose = 2)\nselector = selector.fit(select_num_cols_sm, Y_train)\nselect_cols_sm = selector.get_support(indices = True)\nselect_cols_df_sm = select_num_cols_sm.iloc[:,select_cols_sm]\n\nbest_X_col_name_sm = select_cols_df_sm.columns\nselect_cols_df_sm.head()","2d650990":"# Combine categorical and non-categorical dataframe together\ndef filter_x_df_sm(x):\n    df = x.copy()\n    all_filter_col = []\n    \n    for keep in select_cat_col_name[select_cat_col_name.isin(keep_cols)]:\n        filter_col = [col for col in df.columns if col.startswith(str(keep))]\n        for col in filter_col : \n            if col not in keep_cols:\n                all_filter_col.append(col)\n                \n    new_df = pd.concat([df[best_X_col_name_sm],df[all_filter_col]],axis = 1)\n    return new_df","930bae39":"# Create filter_X_train_sm and filter_X_test_sm dataframe \n# Apply filter_x_df_sm function to X_train and X_test\nfilter_X_train_sm = filter_x_df_sm(X_train)\nfilter_X_test_sm = filter_x_df_sm(X_test)","fdaaee5b":"#Standardize \nscaler = StandardScaler()  \nscaler.fit(filter_X_train_sm)\n\nX_train_scal_sm = scaler.fit_transform(filter_X_train_sm)\nX_test_scal_sm = scaler.fit_transform(filter_X_test_sm)","6c58b1d9":"# Train Logistic Regression model \nlogisticRegr_sm = LogisticRegression()\nlogisticRegr_sm = logisticRegr_sm.fit(X_train_scal_sm, Y_train)\n\nLr_y_pred_train_sm = logisticRegr_sm.predict(X_train_scal_sm)\nLr_y_pred_test_sm = logisticRegr_sm.predict(X_test_scal_sm)\n\nscores_lr_sm = cross_val_score(estimator = logisticRegr_sm, y = Y_train, X = X_train_scal_sm, cv=5)\nprint('Cross Validation Score:', np.mean(scores_lr_sm))","307a955d":"cat_cols = df.select_dtypes(exclude=['number']).columns.values\n\nfor col in cat_cols:\n    if df[col].value_counts().count() > 20000 : \n        print('Column {} has {} categories'.format(col,df[col].value_counts().count()))","f1aa096d":"featurename = filter_X_train.columns\nimportances = list(rf.feature_importances_)\n\nfeature_importances = [(feature, round(importance, 3)) for feature, importance in zip(featurename, importances)]\nfeature_importances = sorted(feature_importances, key = lambda x: x[1], reverse = True)\n\nprint('Top 50 Importance Features\\n')\n[print('Variable: {} Importance Score: {}'.format(*pair)) for pair in feature_importances[:50]];","6086d9f6":"test_df = pd.read_csv('..\/input\/loan-default-prediction\/test_v2.csv.zip')\ntest_df.head()","aeb35ad9":"# Binary Encoding\nencoder = ce.BinaryEncoder(cols = keep_cols)\nbi_enc_df = encoder.fit_transform(test_df[keep_cols])\nbi_col_name = bi_enc_df.columns\n\ntest_df = pd.concat([test_df,bi_enc_df],axis = 1)\ntest_df.head()","c2b0dc63":"# Create select_test_df by drop some columns in test_df\nselect_test_df = pd.concat([test_df['id'],test_df[filter_X_train.columns]],axis = 1)\nselect_test_df.head()","43b406d7":"# Check missing value\nselect_test_df.isnull().sum().value_counts()","5e63c078":"# Impute missing value in numeric columns with median \nnumeric_cols = select_test_df.select_dtypes(include=['number']).columns.values\n\nfor col in numeric_cols:\n    if col in list(col_pct_miss_df.column_name):\n        med = df[col].median()\n        select_test_df[col] = select_test_df[col].fillna(med)\n        \nnot_numeric_cols = select_test_df.select_dtypes(exclude=['number']).columns.values\n\nfor col in not_numeric_cols:\n        mode = df[col].mode()\n        select_test_df[col] = select_test_df[col].fillna(mode[0])","6d1ef716":"# Check missing value\nselect_test_df.isnull().sum().value_counts()","6fab8f94":"# Find columns that contain missing value\nnan_columns = select_test_df.isna().any()\ncolumns_with_nan = select_test_df.columns[nan_columns].tolist()\ncolumns_with_nan","711c64ad":"# Replace missing value with zero\nselect_test_df[columns_with_nan] = select_test_df[columns_with_nan].fillna(0)","2df505c0":"select_test_df.head()","c8ea96c6":"# Random Forest\ntest_df_rf = test_df.copy()\ntest_df_rf['loss'] = rf.predict(select_test_df.loc[:,select_test_df.columns != 'id'])\ntest_df_rf.head()","13ed3c45":"# Export sample_submission of random forest\nsample_submission = test_df_rf[['id','loss']]\nsample_submission.to_csv('sample_submission_rf.csv', index = False)","0eaa864e":"**XGboost**","934a28c0":"**Logistic Regression**","6cb3a65e":"**Feature Selection - Filter Method & RFE Method**\n\nTo increase model accuracy, I did feature selection using 2 techniques.\nFirst, I have filtered the less important both numerical and non-numerical features by using filter method. Then I selected top 150 important numerical columns using recursive feature elimination.","4d68f4d2":"**Undersampling**\n\nTo balance the loan_status in dataset, I apply undersampling technique to solve imbalanced dataset problem.","aa98c80c":"**Data Standardization**","6035b387":"**Drop Unnecessary Columns**","35fa5554":"**K-nearest neighbors**","02ac8bf5":"**Business Suggestion**\n\nAfter the model performance reached a level as desired, the bank could release a personalized lending rate which calculated from the model predicting score. \n\nDue to the research, lack of financial literacy causes the loan default. After I have explored the dataset. I found that more than 90% of a customer's loan is not default. Therefore, I would like to suggest the bank to persuade the loan lending customers to use financial advisor service more. \nBesides reducing the loan default rate, the bank will reach more customer insight and can use them in   model training.\n\n\nReference : https:\/\/www.bot.or.th\/Thai\/MonetaryPolicy\/MonetPolicyComittee\/MPR\/BOX_MPR\/BOX_2_3_MPR_TH_Mar19.pdf","ff606a68":"# Summary\n\nAfter I used k-fold cross validation to evaluate my models, I found that Random Forest gives the highest scores (0.5000) in predicting loan default, but it is not significantly different from XGboost and Logistic Regression scores.\n\nTo improve the model performance, I suggest training the Random Forest model with oversampling dataset using Synthetic Minority Oversampling Technique (SMOTE) which is an oversampling technique where the synthetic samples are generated for the minority class. I have done that with a Logistic Regression model before and it gave a better model accuracy score, but consumed high memory for processing.\n\n","418bce49":"**Split Train Set & Test Set**","1efeb0a7":"Random Foreset","8cf4c82b":"**Sample Submission**","655e6358":"**Handling with too many categories**\n\nAs we can see, there are many attributes containing more than 20,000 categories. \nAlthough we did filter selection,some of categories in the filtered attribute might not significantly different and cause a prediction error. To reduce the number of categories, the bank should redesign the data collecting format.\n","282d8f05":"**Handling with irrelevant columns**\n\nThe last way to improve model performance is dropping outlier and less relevant columns from training dataset based on their importance scores. But, dropping all outliers from a large dataset might not be a good idea, you can drop only outliers in the important features to improve model performance.\n","19630335":"**Handling with Missing Value**","71cfd7ac":"**Encoding Category Columns**\n\nAs you can see, there are many attributes containing more that 20,000 categories.\nSo, I decided to drop them and convert remaining categories in each attribute to binary encoding form."}}