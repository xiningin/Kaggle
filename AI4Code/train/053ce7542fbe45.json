{"cell_type":{"b3598963":"code","da1708de":"code","94a435f6":"code","8d4960af":"code","2326b549":"code","551833b1":"code","62a78f04":"markdown","3d15b909":"markdown","33cec226":"markdown","c28efbcc":"markdown"},"source":{"b3598963":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n\n# Loading data - select which cases to include in the training\/validation set (commented out cases are held out)\ncases = ['DUCT_1100',\n         'DUCT_1150',\n         'DUCT_1250',\n         'DUCT_1300',\n         'DUCT_1350',\n         'DUCT_1400',\n         'DUCT_1500',\n         'DUCT_1600',\n         'DUCT_1800',\n         #'DUCT_2000',\n         'DUCT_2205',\n         'DUCT_2400',\n         'DUCT_2600',\n         'DUCT_2900',\n         'DUCT_3200',\n         #'DUCT_3500',\n         'PHLL_case_0p5',\n         'PHLL_case_0p8',\n         'PHLL_case_1p0',\n         #'PHLL_case_1p2',\n         'PHLL_case_1p5',\n         'BUMP_h20',\n         'BUMP_h26',\n         'BUMP_h31',\n         #'BUMP_h38',\n         'BUMP_h42',\n         'CNDV_12600',\n         'CNDV_20580',\n         'CBFS_13700'\n         ]\n\n#Convenient functions for loading dataset\ndef loadCombinedArray(cases,field):\n    data = np.concatenate([np.load('\/kaggle\/input\/ml-turbulence-dataset\/'+dataset+'\/'+dataset+'_'+case+'_'+field + '.npy') for case in cases])\n    return data\n\ndef loadLabels(cases,field):\n    data = np.concatenate([np.load('\/kaggle\/input\/ml-turbulence-dataset\/'+'labels\/'+case+'_'+field + '.npy') for case in cases])\n    return data\n\n# Select RANS model\ndataset = 'kepsilon' \n\nprint('Loading features and labels from the dataset: '+ dataset)\n\n#Load the set of ten basis tensors (N,10,3,3), from Pope \"A more general effective-viscosity hypothesis\" (1975).\nTensors = loadCombinedArray(cases,'Tensors')\nprint('Shape of basis tensor array: '+str(Tensors.shape))\n\n#Load the 47 invariants (N,47) used by Wu et al. \"Physics-informed machine learning approach for augmenting turbulence models: A comprehensive framework\" (2018)\nInvariants = loadCombinedArray(cases,'I')\nprint('Shape of invariant features array: '+str(Invariants.shape))\n\n#Load the additional scalars (N,5): \n#    q[:,0] = Ratio of excess rotation to strain rate,\n#    q[:,1] = Wall-distance based Reynolds number, \n#    q[:,2] = Ratio of turbulent time scale to mean strain time scale\n#    q[:,3] = Ratio of total Reynolds stress to 1\/2 * normal Reynolds stress (TKE)\nScalars = loadCombinedArray(cases,'q')\nprint('Shape of scalar features array: '+str(Scalars.shape))\n\n#Combine the invariants and scalars to form a feature set\nFeatures = np.column_stack((Invariants,Scalars))\nprint('Shape of combined features array: '+str(Features.shape))\n\n#Optional: remove outliers based on the number of standard deviations away from the mean. \n#Note: must be careful, as there are naturally large variations in flow features. Even a 5*stdev critera removes many valid near-wall points.\ndef remove_outliers(Features):\n    stdev = np.std(Features,axis=0)\n    means = np.mean(Features,axis=0)\n    ind_drop = np.empty(0)\n    for i in range(len(Features[0,:])):\n        ind_drop = np.concatenate((ind_drop,np.where((Features[:,i]>means[i]+5*stdev[i]) | (Features[:,i]<means[i]-5*stdev[i]) )[0]))\n    return ind_drop.astype(int)\n\noutlier_removal_switch = 0\nif outlier_removal_switch == 1:\n    outlier_index = remove_outliers(Features)\n    print('Found '+str(len(outlier_index))+' outliers in the input feature set')\n    Features = np.delete(Features,outlier_index,axis=0)\n    Tensors = np.delete(Tensors,outlier_index,axis=0)\n    Labels = np.delete(Labels,outlier_index,axis=0)\n\n#Load the label set from DNS\/LES:\nLabels = loadLabels(cases,'b')\n#If desired, reshape the 3x3 symmetric anisotropy tensor into a 1x6 vector\nLabels = np.delete(Labels.reshape((len(Labels),9)),[3,6,7],axis=1)\nprint('Shape of DNS\/LES labels array: '+str(Labels.shape))\n\n# Split the datasets into training and validation\nindices = np.arange(Features.shape[0])\ninput_shape = Features.shape[1]\n\nx_train, x_val, y_train, y_val, ind_train, ind_val = train_test_split(Features, Labels, indices, test_size=0.2, random_state=10, shuffle=True)\n\nbasis_train = Tensors[ind_train]\nbasis_val = Tensors[ind_val]\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_val = scaler.transform(x_val)\n\nprint(' ')\nprint('Training features:')\nprint(x_train.shape)\nprint('Training tensor basis:')\nprint(basis_train.shape)\nprint('Training labels:')\nprint(y_train.shape)\nprint(' ')\nprint('Validation features:')\nprint(x_val.shape)\nprint('Validation tensor basis:')\nprint(basis_val.shape)\nprint('Validation labels:')\nprint(y_val.shape)\nprint(' ')","da1708de":"import tensorflow as tf\nfrom tensorflow import keras\nkeras.backend.clear_session()\n\n#The model has two inputs, a set of input features with a learned mapping, and the tensor basis layer\ninput_layer = keras.layers.Input(shape=(input_shape),name ='input_layer')\ninput_tensor_basis = keras.layers.Input(shape=(10,3,3),name='Tensor_input_layer')\n\n#Hidden layer definition\nhidden1 = keras.layers.Dense(20,name='Hidden1', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(input_layer)\nhidden2 = keras.layers.Dense(20,name='Hidden2', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden1)\nhidden3 = keras.layers.Dense(20,name='Hidden3', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-3), activation = \"selu\")(hidden2)\n\n#The layer of gn, which are coefficients for each of the ten Tn\ngn = keras.layers.Dense(10,name='gn', kernel_initializer=\"lecun_normal\",kernel_regularizer=tf.keras.regularizers.l2(1E-4), activation = \"selu\")(hidden3)\n\n#Multiply the gn by Tn, with the output being the anisotropy tensor\nshaped = keras.layers.Reshape((10,1,1),name='Shape_for_dot_product')(gn)\nmerge = keras.layers.Dot(axes=1, name='Dot_product')([shaped,input_tensor_basis])\n\n#Reshape the output anisotropy tensor, and trim out duplicate values (it is a symmetric matrix). The end result is a 6 component vector.\nshaped_output = keras.layers.Reshape((9,1),name='Shaped_output')(merge)\ntrimmed_output1 = keras.layers.Lambda(lambda x : x[:,0])(shaped_output)\ntrimmed_output2 = keras.layers.Lambda(lambda x : x[:,1])(shaped_output)\ntrimmed_output3 = keras.layers.Lambda(lambda x : x[:,2])(shaped_output)\ntrimmed_output4 = keras.layers.Lambda(lambda x : x[:,4])(shaped_output)\ntrimmed_output5 = keras.layers.Lambda(lambda x : x[:,5])(shaped_output)\ntrimmed_output6 = keras.layers.Lambda(lambda x : x[:,8])(shaped_output)\nmerged_output = tf.keras.layers.Concatenate()([trimmed_output1,trimmed_output2,trimmed_output3,trimmed_output4,trimmed_output5,trimmed_output6])\n\n\nmodel=keras.Model(inputs=[input_layer, input_tensor_basis], outputs=[merged_output])\n\n\noptimizer = tf.keras.optimizers.Nadam(learning_rate = 5E-4, clipnorm=1000)\nmodel.compile(optimizer,loss='mse',metrics=['mae', 'mse'])\nprint(model.summary())\n\n","94a435f6":"reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(\n    monitor=\"loss\",\n    factor=0.3,\n    patience=10,\n    verbose=0,\n    mode=\"auto\",\n    min_delta=0.0001,\n    cooldown=0,\n    min_lr=0,\n)\n\nearly_stop = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=0,\n    patience=40,\n    verbose=0,\n    mode=\"auto\",\n    baseline=None,\n    restore_best_weights=True,\n)\n\nhistory = model.fit([x_train,basis_train], y_train, \n                    batch_size=1000,\n                    epochs=1000, \n                    validation_data = ([x_val,basis_val],y_val), \n                    verbose=1, \n                    callbacks=[\n                            early_stop,\n                            reduce_lr\n                            ]\n                   )","8d4960af":"model.evaluate([x_train,basis_train],y_train)","2326b549":"model.evaluate([x_val,basis_val],y_val)","551833b1":"rand_ind = np.random.randint(0,len(x_train),5)\nprint('Picked 5 random indices: ' +str(rand_ind))\n\nfor i in range(len(rand_ind)):\n    print('Index: '+str(rand_ind[i]))\n    print('Label anisotropy values: ')\n    print(y_train[rand_ind[i],:])\n    print('Model prediction: ')\n    print(model.predict([x_train[rand_ind[i],:].reshape(1,x_train.shape[1]),basis_train[rand_ind[i],:,:,:].reshape(1,10,3,3)]))","62a78f04":"# Train the tensor basis neural network","3d15b909":"# Creating a tensor basis neural network (TBNN)\nThe following model has 3 hidden layers, with 40 neurons each.","33cec226":"# Description\nThis notebook demonstrates how to use the dataset. A large set of features and labels is formed. A tensor basis neural network (TBNN) is created using Keras. The network is trained and evaluated on some sample points.","c28efbcc":"# Loading the dataset features and labels, creating a training\/validation dataset"}}