{"cell_type":{"e2361502":"code","77a9c9c3":"code","1ec091ac":"code","e49e6993":"code","e11963f9":"code","c9ddfd99":"code","dd457d0b":"code","ec91f287":"code","2b7d2687":"code","4acdb6ed":"code","e47c33cd":"code","1769c381":"code","da3231f8":"code","f7e4b6fa":"code","5ce51f49":"code","b728793f":"code","4c43628c":"code","5dbebe05":"code","d0712f50":"code","0cced598":"code","556ff250":"code","0da3ac25":"code","3e279321":"code","35cd69c9":"code","2792595b":"code","679a64f3":"code","47883fff":"code","400ecb99":"code","fe0395e7":"code","a7ec9636":"code","b03e2aec":"code","d399b4dc":"code","5f45bb41":"code","a39343a5":"code","e3c0d9fc":"code","436fee60":"code","9343d799":"code","0311ab40":"code","f24d50ad":"code","0b8f3fb8":"code","df227a2b":"markdown","199745e4":"markdown","9377f688":"markdown","da309761":"markdown","299a4105":"markdown","80739462":"markdown","cdf24b39":"markdown","2907abf4":"markdown","76c5842e":"markdown","8ea27590":"markdown","180cef79":"markdown","bf077774":"markdown","4ccfb69f":"markdown","2996d9b8":"markdown","a640c0d2":"markdown"},"source":{"e2361502":"%%html\n\n<style>\ntable {float: left;}\ntable, th, tr, td {border: 1px solid #ccc !important}\n<\/style>","77a9c9c3":"# Baic Libraries\nimport numpy as np\nimport pandas as pd\n\n# Visualisation\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport plotly.express as px\n\n# Missing\nimport missingno as msno\n\n# Scientific Libraries\nfrom scipy import stats\nfrom scipy.stats import norm\nfrom scipy.stats import kurtosis, skew\n\n# ML Modeling\nimport xgboost as xgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import Lasso\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import GradientBoostingRegressor","1ec091ac":"dt = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")","e49e6993":"dt.shape","e11963f9":"dt.columns","c9ddfd99":"dt.info()","dd457d0b":"# pd.set_option('display.max_rows', 500)\n# pd.set_option('display.max_columns', 500)\n# pd.set_option('display.width', 1000)","ec91f287":"dt.describe()","2b7d2687":"print(\"Data Types\\n----------------------\")\nprint(dt.dtypes.value_counts())\nprint(\"\\n\")\nwith pd.option_context('display.max_rows', None, 'display.max_columns', None):\n    na = dt.isnull().sum().sort_values(ascending = False)\n    nap = (dt.isnull().sum().sort_values(ascending = False) * 100 \/ len(dt)).round(decimals=2)\n    print(\"Null Values \\n------------------\")\n    print(na[na > 0], \"\\n\")\n    print(\"%age of Null Values \\n------------------\")\n    print(nap[nap > 0])","4acdb6ed":"# Dropping Variables > 90% missing and ID\ndt.drop(['Id', 'PoolQC', 'MiscFeature', 'Alley'], axis = 1, inplace = True)","e47c33cd":"dt.GarageYrBlt.fillna(dt.GarageYrBlt.mode()[0], inplace=True)\ndt.GarageYrBlt = dt.GarageYrBlt.astype(int)\ndt.GarageYrBlt.value_counts()\n\ndt.LotFrontage.fillna(dt.LotFrontage.mean(), inplace=True)\ndt.Electrical.fillna(dt.Electrical.mode()[0], inplace=True)","1769c381":"# Applying Categoric Transformation\n\n# Binary: True\/False\n\ndt['CentralAir'] = dt['CentralAir'].replace({'Y':1, 'N':0})\n\n# Ordinal:\n\nMSSubClass_d = {20:1, 30:2, 40:3, 45:4, 50:5, 60:6, 70:7, 75:8, 80:9, 85:10, 90:11, 120:12, 160:13, 180:14, 190:15}\ndt.MSSubClass = dt.MSSubClass.map(MSSubClass_d)\n\nLotShape_d = {'IR3':1, 'IR2':2, 'IR1':3, 'Reg':4}\ndt.LotShape = dt.LotShape.map(LotShape_d)\n\nUtilities_d = {'NoSeWa':1, 'AllPub':2}\ndt.Utilities = dt.Utilities.map(Utilities_d)\n\nLotConfig_d = {'Inside':1, 'Corner':2, 'CulDSac':3, 'FR2':4, 'FR3':5}\ndt.LotConfig = dt.LotConfig.map(LotConfig_d)\n\nLandSlope_d = {'Gtl':1, 'Mod':2, 'Sev':3}\ndt.LandSlope = dt.LandSlope.map(LandSlope_d)\n\nBldgType_d = {'1Fam':1, 'TwnhsE':2, 'Duplex':3, 'Twnhs':4, '2fmCon':5}\ndt.BldgType = dt.BldgType.map(BldgType_d)\n\nHouseStyle_d = {'1Story':1, '2Story':2, '1.5Fin':3, 'SLvl':4, 'SFoyer':5, '1.5Unf':6, '2.5Unf':7, '2.5Fin':8}\ndt.HouseStyle = dt.HouseStyle.map(HouseStyle_d)\n\n# OverallQual: Already Numerical\n# OverallCond: Already Numerical\n\nExterQual_d = {'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}\ndt.ExterQual = dt.ExterQual.map(ExterQual_d)\n\nExterCond_d = {'Po':1, 'Fa':2, 'TA':3, 'Gd':4, 'Ex':5}\ndt.ExterCond = dt.ExterCond.map(ExterCond_d)\n\nBsmtQual_d = {np.nan:1, 'Ex':2, 'Gd':3, 'TA':4, 'Fa':5}\ndt.BsmtQual = dt.BsmtQual.map(BsmtQual_d)\n\nBsmtCond_d = {np.nan:1, 'Gd':2, 'TA':3, 'Fa':4, 'Po':5}\ndt.BsmtCond = dt.BsmtCond.map(BsmtCond_d)\n\nBsmtExposure_d = {np.nan:1, 'No':2, 'Mn':3, 'Av':4, 'Gd':5}\ndt.BsmtExposure = dt.BsmtExposure.map(BsmtExposure_d)\n\nBsmtFinType1_d = {np.nan:1, 'Unf':2, 'LwQ':3, 'Rec':4, 'BLQ':5, 'ALQ':6, 'GLQ':7}\ndt.BsmtFinType1 = dt.BsmtFinType1.map(BsmtFinType1_d)\n\nBsmtFinType2_d = {np.nan:1, 'Unf':2, 'LwQ':3, 'Rec':4, 'BLQ':5, 'ALQ':6, 'GLQ':7}\ndt.BsmtFinType2 = dt.BsmtFinType2.map(BsmtFinType2_d)\n\nHeatingQC_d = {'Ex':1, 'TA':2, 'Gd':3, 'Fa':4, 'Po':5}\ndt.HeatingQC = dt.HeatingQC.map(HeatingQC_d)\n\nElectrical_d = {'Mix':1, 'FuseP':2, 'FuseF':3, 'FuseA':4, 'SBrkr':5}\ndt.Electrical = dt.Electrical.map(Electrical_d)\n\nKitchenQual_d = {'Fa':1, 'TA':2, 'Gd':3, 'Ex':4}\ndt.KitchenQual = dt.KitchenQual.map(KitchenQual_d)\n\nFunctional_d = {'Typ':1, 'Min1':2, 'Min2':3, 'Mod':4, 'Maj1':5, 'Maj2':6, 'Sev':7}\ndt.Functional = dt.Functional.map(Functional_d)\n\nFireplaceQu_d = {np.nan:1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6}\ndt.FireplaceQu = dt.FireplaceQu.map(FireplaceQu_d)\n\nGarageType_d = {np.nan:1, 'Detchd':2, 'CarPort':3, 'BuiltIn':4, 'Basment':5, 'Attchd':6, '2Types':7}\ndt.GarageType = dt.GarageType.map(GarageType_d)\n\nGarageFinish_d = {np.nan:1, 'Unf':2, 'RFn':3, 'Fin':4}\ndt.GarageFinish = dt.GarageFinish.map(GarageFinish_d)\n\nGarageQual_d = {np.nan:1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6}\ndt.GarageQual = dt.GarageQual.map(GarageQual_d)\n\nGarageCond_d = {np.nan:1, 'Po':2, 'Fa':3, 'TA':4, 'Gd':5, 'Ex':6}\ndt.GarageCond = dt.GarageCond.map(GarageCond_d)\n\nPavedDrive_d = {'N':1, 'P':2, 'Y':3}\ndt.PavedDrive = dt.PavedDrive.map(PavedDrive_d)\n\nFence_d = {np.nan:1, 'MnWw':2, 'GdWo':3, 'MnPrv':4, 'GdPrv':5}\ndt.Fence = dt.Fence.map(Fence_d)","da3231f8":"nominal_list = [\"Neighborhood\", \"Condition1\", \"Condition2\", \"RoofStyle\", \"RoofMatl\", \"Exterior1st\", \n                \"Exterior2nd\", \"MasVnrType\", \"Heating\", \"SaleType\", \"SaleCondition\", \"Street\", \"LandContour\", \"Foundation\"]\n\ndt2 = dt[[i for i in nominal_list]]\n    \ndt2 = pd.get_dummies(dt2, drop_first=True)\n\ndt.drop(nominal_list[1:], inplace=True, axis=1)\n\ndt_final = pd.merge(dt, dt2, left_index=True, right_index=True)","f7e4b6fa":"corr = dt_final.corr()","5ce51f49":"# dt_final.groupby(\"GarageYrBlt\").mean().plot(kind=\"bar\")\nfig, ax = plt.subplots(figsize=(70,20))\n\n\n# ax.tick_params(axis='both', which='major', labelsize=20)\n# ax.tick_params(axis='both', which='minor', labelsize=20)\nsns.lineplot(x=\"Electrical\", y=\"SalePrice\", data=dt_final)\n\nsns.set(style='white')\nplt.xticks(rotation = 45, fontsize=30)\nplt.yticks(rotation = 45, fontsize=30)\n\nplt.xlabel('Electrical',fontsize=40)\nplt.ylabel('SalePrice',fontsize=40)\n\nplt.show()","b728793f":"# corr.style.background_gradient(cmap='coolwarm')","4c43628c":"fig, ax = plt.subplots(figsize=(10,10))\n\nk = 10 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(dt_final[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","5dbebe05":"dt_final","d0712f50":"# px.scatter(dt_final, x=\"OverallQual\", y=col, size_max=60)","0cced598":"# px.scatter(dt_final, x=\"OverallQual\", y=col, size_max=60)","556ff250":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']\nsns.pairplot(dt_final[cols], height = 5)\nplt.show();","0da3ac25":"dt_final2 = dt_final.copy()\n\nfor col in dt_final2.columns: \n     dt_final2[col] = stats.mstats.winsorize(dt_final2[col], limits=0.01)","3e279321":"res = stats.probplot(dt_final['SalePrice'], plot=plt)","35cd69c9":"dep_vars = ['OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'SalePrice']\n\n# Log Transformations\n\nd1 = np.log(dt_final[dep_vars[0]])\nd2 = np.log(dt_final[dep_vars[1]])\n# d3 = np.log(dt_final[dep_vars[2]])\n# d4 = np.log(dt_final[dep_vars[3]])\nd5 = np.log(dt_final[dep_vars[4]])\n\n# BoxCox Transformations\n\nfd1, fl1 = stats.boxcox(dt_final[dep_vars[0]]) \nfd2, fl2 = stats.boxcox(dt_final[dep_vars[1]]) \n# fd3, fl3 = stats.boxcox(dt_final[dep_vars[2]]) # data not positive\n# fd4, fl4 = stats.boxcox(dt_final[dep_vars[3]])\nfd5, fl5 = stats.boxcox(dt_final[dep_vars[4]]) ","2792595b":"print(\"Skew (after transformation)\")\nprint(\"---------------------------\")\nprint(f'{\"Variable\":11} {\"Before\":9} {\"Log\":7} {\"BoxCox\":5}')\n\nprint(f'{dep_vars[0]:11} {round(skew(dt_final[dep_vars[0]]),2):4.2f}  {round(skew(d1), 2):7.2f} {round(skew(fd1),2):7.2f}')\nprint(f'{dep_vars[1]:8} {round(skew(dt_final[dep_vars[1]]),2):7.2f}  {round(skew(d2), 2):7.2f} {round(skew(fd2),2):7.2f}')\n# print(f'{dep_vars[2]:11} {round(skew(dt_final[dep_vars[2]]),2):5.2f} {round(skew(d3), 2):7.2f} {\"__\":7}')\n# print(f'{dep_vars[3]:5} {round(skew(dt_final[dep_vars[3]]),2):6.2f}  {round(skew(d4), 2):7.2f} {\"__\":7}')\nprint(f'{dep_vars[4]:5} {round(skew(dt_final[dep_vars[4]]),2):6.2f}  {round(skew(d5), 2):7.2f} {round(skew(fd5),2):7.2f}')","679a64f3":"sqrt = np.sqrt(dt_final[dep_vars[2]])\ncbrt = np.cbrt(dt_final[dep_vars[2]])\nsqrt2 = np.sqrt(dt_final[dep_vars[3]])\ncbrt2 = np.cbrt(dt_final[dep_vars[3]])\nprint(skew(sqrt), skew(cbrt), skew(sqrt2), skew(cbrt2))","47883fff":"# Applying the selected transformation\n\ndt_final[dep_vars[0]], fl1 = stats.boxcox(dt_final[dep_vars[0]]) \ndt_final[dep_vars[1]], fl2 = stats.boxcox(dt_final[dep_vars[1]]) \n# dt_final[dep_vars[2]], fl3 = stats.boxcox(dt_final[dep_vars[2]]) \n# fd4, fl4 = stats.boxcox(dt_final[dep_vars[3]])\ndt_final[dep_vars[3]], fl5 = stats.boxcox(dt_final[dep_vars[4]]) ","400ecb99":"f_vars = ['OverallQual', 'GrLivArea', 'GarageCars', 'SalePrice']\n\nsns.pairplot(dt_final[f_vars], height = 5)\nplt.show()","fe0395e7":"dt_final.SalePrice.describe()","a7ec9636":"sns.displot(dt_final.SalePrice, kde = False, color ='red', bins = 30, height=10, aspect=5) \nplt.show()","b03e2aec":"index = dt_final[(dt_final[\"SalePrice\"] >= 500000)].index\ndt_final.drop(index, inplace=True)","d399b4dc":"# Splitiing the dependent and independent vars\nind_vars = ['OverallQual', 'GrLivArea', 'GarageCars']\n\nX = dt_final.loc[:, [i for i in ind_vars]]\ny = dt_final.SalePrice\n\n# Scaling the data\nscaler = MinMaxScaler()\nX[[i for i in ind_vars]] = scaler.fit_transform(X[[i for i in ind_vars]])\n\n# Splitiing the train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","5f45bb41":"# plt.scatter(y_test, predictions, color=\"blue\")\n# plt.xlabel(\"True Values\")\n# plt.ylabel(\"Predictions\")\n# plt.show()","a39343a5":"# Linear Regression ===============================================================\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg_pred = lin_reg.predict(X_test)\n\n# # Polynomial Regression\n# poly = PolynomialFeatures(degree = 4)\n# X_poly = poly.fit_transform(X_train)\n# poly.fit(X_poly, y_train)\n# lin2 = LinearRegression()\n# lin2.fit(X_poly, y_train)\n# poly_reg_pred = lin2.predict(X_test)\n\n# Random Forest ===============================================================\nrf_reg = RandomForestRegressor(n_estimators = 1000, random_state = 0) \nrf_reg.fit(X_train, y_train) \nrf_reg_pred = rf_reg.predict(X_test)\n\n# XGBoost ===============================================================\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 21, seed = 0) \nxgb_reg.fit(X_train, y_train) \nxgb_reg_pred = xgb_reg.predict(X_test)\n\n# Lasso ===============================================================\nlasso_model = Lasso(alpha=0.0005, random_state=5)\nlasso_model.fit(X_train, y_train)\nlasso_val_pred = lasso_model.predict(X_test)\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002, random_state=5)\nridge_model.fit(X_train, y_train)\nridge_val_pred = ridge_model.predict(X_test)\n\n# Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\ngbr_model.fit(X_train, y_train)\ngbr_val_pred = gbr_model.predict(X_test)","e3c0d9fc":"# RMSE\n\nprint(\"RMSE from Linear Regression:\", np.sqrt(mean_squared_error(y_test,lin_reg_pred)))\n# print(\"RMSE from Polynomial Regression:\", np.sqrt(mean_squared_error(y_test,poly_reg_pred)))\nprint(\"RMSE from Random Forest Regression:\", np.sqrt(mean_squared_error(y_test,rf_reg_pred)))\nprint(\"RMSE from XGBoost Regression:\", np.sqrt(mean_squared_error(y_test,xgb_reg_pred)))\nprint(\"RMSE from Lasso Regression:\", np.sqrt(mean_squared_error(y_test,lasso_val_pred)))\nprint(\"RMSE from Ridge Regression:\", np.sqrt(mean_squared_error(y_test,ridge_val_pred)))\nprint(\"RMSE from Gradient Boosting Regression:\", np.sqrt(mean_squared_error(y_test,gbr_val_pred)))","436fee60":"from sklearn.model_selection import cross_val_score\nfrom sklearn.impute import SimpleImputer\n\n\nimputer = SimpleImputer()\nimputed_X = imputer.fit_transform(X)\nn_folds = 10\n\n# =========================================================================\nscores = cross_val_score(xgb_reg, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nxgb_mae_scores = np.sqrt(-scores)\n\nprint('For LASSO model:')\nprint('Mean RMSE = ' + str(xgb_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(xgb_mae_scores.std().round(decimals=3)))\n\n# =========================================================================\nscores = cross_val_score(gbr_model, imputed_X, y, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nxbr_mae_scores = np.sqrt(-scores)\n\nprint('For gbr_model model:')\nprint('Mean RMSE = ' + str(xbr_mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(xbr_mae_scores.std().round(decimals=3)))","9343d799":"# Splitiing the dependent and independent vars\nind_vars = ['OverallQual', 'GrLivArea', 'GarageCars']\n\nX = dt_final2.loc[:, [i for i in ind_vars]]\ny = dt_final2.SalePrice\n\n# Scaling the data\nscaler = MinMaxScaler()\nX[[i for i in ind_vars]] = scaler.fit_transform(X[[i for i in ind_vars]])\n\n# Splitiing the train and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\n# Linear Regression\nlin_reg = LinearRegression()\nlin_reg.fit(X, y)\nlin_reg_pred = lin_reg.predict(X_test)\n\n# # Polynomial Regression\n# poly = PolynomialFeatures(degree = 4)\n# X_poly = poly.fit_transform(X_train)\n# poly.fit(X_poly, y_train)\n# lin2 = LinearRegression()\n# lin2.fit(X_poly, y_train)\n# poly_reg_pred = lin2.predict(X_test)\n\n# Random Forest\nrf_reg = RandomForestRegressor(n_estimators = 1000, random_state = 0) \nrf_reg.fit(X_train, y_train) \nrf_reg_pred = rf_reg.predict(X_test)\n\n# XGBoost\nxgb_reg = xgb.XGBRegressor(objective ='reg:squarederror', n_estimators = 21, seed = 0) \nxgb_reg.fit(X_train, y_train) \nxgb_reg_pred = xgb_reg.predict(X_test)\n\n# Lasso ===============================================================\nlasso_model = Lasso(alpha=0.0005, random_state=5)\nlasso_model.fit(X_train, y_train)\nlasso_val_pred = lasso_model.predict(X_test)\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002, random_state=5)\nridge_model.fit(X_train, y_train)\nridge_val_pred = ridge_model.predict(X_test)\n\n# Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\ngbr_model.fit(X_train, y_train)\ngbr_val_pred = gbr_model.predict(X_test)\n\n# RMSE\n\nprint(\"RMSE from Linear Regression:\", np.sqrt(mean_squared_error(y_test,lin_reg_pred)))\n# print(\"RMSE from Polynomial Regression:\", np.sqrt(mean_squared_error(y_test,poly_reg_pred)))\nprint(\"RMSE from Random Forest Regression:\", np.sqrt(mean_squared_error(y_test,rf_reg_pred)))\nprint(\"RMSE from XGBoost Regression:\", np.sqrt(mean_squared_error(y_test,xgb_reg_pred)))\nprint(\"RMSE from Lasso Regression:\", np.sqrt(mean_squared_error(y_test,lasso_val_pred)))\nprint(\"RMSE from Ridge Regression:\", np.sqrt(mean_squared_error(y_test,ridge_val_pred)))\nprint(\"RMSE from Gradient Boosting Regression:\", np.sqrt(mean_squared_error(y_test,gbr_val_pred)))","0311ab40":"plt.scatter(y_test, xgb_reg_pred, color=\"green\")\nplt.xlabel(\"True Values\")\nplt.ylabel(\"Predictions\")\nplt.show()","f24d50ad":"lambdas = (0.001, 0.01, 0.1, 0.5, 1, 2, 10)\nl_num = 7\npred_num = X.shape[1]\n\n# prepare data for enumerate\ncoeff_a = np.zeros((l_num, pred_num))\ntrain_r_squared = np.zeros(l_num)\ntest_r_squared = np.zeros(l_num)\n# enumerate through lambdas with index and i\nfor ind, i in enumerate(lambdas):    \n    reg = Lasso(alpha = i)\n    reg.fit(X_train, y_train)\n\n    coeff_a[ind,:] = reg.coef_\n    train_r_squared[ind] = reg.score(X_train, y_train)\n    test_r_squared[ind] = reg.score(X_test, y_test)","0b8f3fb8":"# Plotting\nplt.figure(figsize=(18, 8))\nplt.plot(train_r_squared, 'bo-', label=r'$R^2$ Training set', color=\"darkblue\", alpha=0.6, linewidth=3)\nplt.plot(test_r_squared, 'bo-', label=r'$R^2$ Test set', color=\"darkred\", alpha=0.6, linewidth=3)\nplt.xlabel('Lamda index'); plt.ylabel(r'$R^2$')\nplt.xlim(0, 6)\nplt.title(r'Evaluate lasso regression with lamdas: 0 = 0.001, 1= 0.01, 2 = 0.1, 3 = 0.5, 4= 1, 5= 2, 6 = 10')\nplt.legend(loc='best')\nplt.grid()","df227a2b":"#### Null Values\n\n- There are ***4*** columns with ***> 75%*** Null Values: \n\n***PoolQC***: Pool quality\n\n***MiscFeature***: Miscellaneous feature not covered in other categories\n\n***Alley***: Type of alley access to property\n\n***Fence***: Fence quality\n\n\n\n| Variable    | #NA  | % NA |\n|:------------|:-----|:--------|\n| PoolQC      | 1453 | 99.52   |\n| MiscFeature | 1406 | 96.30   |\n| Alley       | 1369 | 93.77   |\n| Fence       | 1179 | 80.75   |","199745e4":"- **Vars with High Correlation with Target**: OverallQual, GrLivArea, ExterQual, KitchenQual, GarageCars, GarageArea, TotalBsmtSF, 1stFlrSF\n- **Vars with High Correlation among themselves**: (GarageCars, GarageArea): GarageCars, (TotalBsmtSF, 1stFlrSF): TotalBsmtSF, (OverallQual, ExterQual, KitchenQual): OverallQual\n- **Final List**: OverallQual, GrLivArea, GarageCars, TotalBsmtSF","9377f688":"### ToDo\n\n- Outlier Removal by CLipping specific to the variable based on research\n- Can we create new columns?\n- KFOLD validation\n- Analyse the test data for imputation, cardinality, etc.","da309761":"## Feature Selection","299a4105":"## Importing Libraries","80739462":"## Numeric Tranformation","cdf24b39":"#### Selected Numeric Transformation Method\n| Variable    | Transformation |\n|:------------|:---------------|\n| OverallQual | BoxCox         |\n| 1stFlrSF    | BoxCox         |\n| GrLivArea   | BoxCox         |\n| GarageCars  | None           |\n| SalePrice   | BoxCox         |","2907abf4":"## EDA","76c5842e":"### EDA Result","8ea27590":"## Loading Dataset From CSV","180cef79":"- Data contains ***3*** float, ***35*** integer and ***43*** object values.\n- **ID** is useless, can be dropped","bf077774":"## Changing Default Table Style","4ccfb69f":"### Binary\n\n---\nCentralAir\n\n\n### Nominal\n\n---\nAlley\nNeighborhood\nCondition1\nCondition2\nRoofStyle\nRoofMatl\nExterior1st\nExterior2nd\nMasVnrType\nHeating\nMiscFeature\nSaleType\nSaleCondition\nStreet\nLandContour\nFoundation\n\n### Ordinal\n\n---\nMSSubClass\nLotShape\nUtilities\nLotConfig\nLandSlope\nBldgType\nHouseStyle\nOverallQual\nOverallCond\nExterQual\nExterCond\nBsmtQual\nBsmtCond\nBsmtExposure\nBsmtFinType1\nBsmtFinType2\nHeatingQC\nElectrical\nKitchenQual\nFunctional\nFireplaceQu\nGarageType\nGarageFinish\nGarageQual\nGarageCond\nPavedDrive\nPoolQC\nFence","2996d9b8":"## Encoding Categorical Variables","a640c0d2":"#### Data Types\n\n| Type        | #  |\n|:------------|:---|\n| object      | 43 |\n| int64       | 35 |\n| float64     | 3  |"}}