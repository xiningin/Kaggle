{"cell_type":{"de9e4f30":"code","69680656":"code","154033f1":"code","4156da73":"code","23e02d46":"code","6b6d8898":"code","7deed6a5":"code","7b9e2cbf":"code","e33be78b":"code","0e333ac3":"code","2ce7c55d":"code","f06e5b05":"code","b636c290":"code","97c261b6":"code","56b6f588":"markdown","4a7945a5":"markdown","bdf5db40":"markdown","f81e717e":"markdown","c70424f2":"markdown","151b9e37":"markdown","a26e812e":"markdown","9696d600":"markdown","d2f49015":"markdown","2248723c":"markdown","46de497a":"markdown","18a955b7":"markdown"},"source":{"de9e4f30":"import spacy\nimport en_core_web_lg\n\nimport keras\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,LSTM,Embedding\nfrom tensorflow.keras.models import load_model\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nimport numpy as np\n\nfrom pickle import dump,load\nfrom pickle import load\n\nfrom random import randint\nimport pandas as pd","69680656":"# nlp = spacy.load('en_core_web_lg')\n# nlp = en_core_web_lg.load(disable=['parser', 'tagger', 'ner'])\nnlp = spacy.load('en_core_web_lg',disable=['parser', 'tagger', 'ner'])","154033f1":"def read_file(filepath):\n    with open(filepath) as f:\n        str_text = f.read()\n        \n    return str_text\n\ndef separate_punc(doc_text):\n    return [token.text.lower() for token in nlp(doc_text) if token.text not in '\\n\\n \\n\\n\\n!\"-#$%&()--.*+,-\/:;<=>?@[\\\\]^_`{|}~\\t\\n ']\n\nd = read_file('..\/input\/the-iliad\/illiad.txt')\n\ntokens = separate_punc(d)","4156da73":"len(tokens)","23e02d46":"tokens = tokens[0:50000]","6b6d8898":"nlp.max_length = len(tokens)\ntrain_len = 25 + 1\n\ntext_sequences = []\n\nfor i in range(train_len, len(tokens)):\n    seq = tokens[i-train_len:i]\n    text_sequences.append(seq)\n\n# Print an exemple\ntext_sequences[0:2]","7deed6a5":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(text_sequences)\n\nsequences = tokenizer.texts_to_sequences(text_sequences)\nsequences = np.array(sequences)\n\n\nvocabulary_size = len(tokenizer.word_counts)\n\nX = sequences[:,:-1]\ny = sequences[:,-1]\ny = to_categorical(y, num_classes=vocabulary_size+1)\n\nseq_len = X.shape[1]","7b9e2cbf":"print(\"X shape\")\nprint(str(X.shape) + \"\\n\")\nprint(\"y shape\")\nprint(y.shape)","e33be78b":"def create_model(vocabulary_size, seq_len):\n    model = Sequential()\n    model.add(Embedding(vocabulary_size, 25, input_length=seq_len))\n    model.add(LSTM(150, return_sequences=True))\n    model.add(LSTM(150))\n    model.add(Dense(150, activation='relu'))\n\n    model.add(Dense(vocabulary_size, activation='softmax'))\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n   \n    model.summary()\n    \n    return model\n\nmodel = create_model(vocabulary_size+1, seq_len)","0e333ac3":"model.fit(X, y, batch_size=128, epochs=150,verbose=1)","2ce7c55d":"loss = pd.DataFrame(model.history.history)\nloss.plot(figsize = (15, 8))","f06e5b05":"def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n    \n    # Final Output\n    output_text = []\n    \n    # Intial Seed Sequence\n    input_text = seed_text\n    \n    # Create num_gen_words\n    for i in range(num_gen_words):\n        \n        # Take the input text string and encode it to a sequence\n        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n        \n        # In case there is more or less words than the number of words on what we train the model\n        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating='pre')\n        \n        # Predict Class Probabilities for each word\n        predict_x=model.predict(pad_encoded, verbose=0)\n        pred_word_ind=np.argmax(predict_x,axis=1)[0]\n        \n        # Grab word\n        pred_word = tokenizer.index_word[pred_word_ind] \n        \n        # Update the sequence of input text (shifting one over with the new word)\n        input_text += ' ' + pred_word\n        \n        output_text.append(pred_word)\n        \n    # Make it look like a sentence.\n    return ' '.join(output_text)","b636c290":"import random\n\n# If you want to take always the same text :\nrandom.seed(42)\n\nrandom_pick = random.randint(0,len(text_sequences))\nrandom_seed_text = text_sequences[random_pick]\nseed_text = ' '.join(random_seed_text)\nseed_text","97c261b6":"generate_text(model,tokenizer,seq_len,seed_text=seed_text,num_gen_words=50)","56b6f588":"**Train the model**","4a7945a5":"**To avoid memory error in Kaggle, we will take only the first 50k elements of the array**","bdf5db40":"**If you want to take a random part of the text do the following thing. You can also write whatever you want in input of the model**","f81e717e":"**Transform into multiple array of 25 words each. Each array is the same as the previous one shifted by 1**","c70424f2":"# DATA PREP","151b9e37":"**Open the text file and load it with spacy**","a26e812e":"**Load spacy**","9696d600":"**Create the model structure**","d2f49015":"**Transform words into numbers**","2248723c":"# IMPORT","46de497a":"# MODEL","18a955b7":"# Generate text"}}