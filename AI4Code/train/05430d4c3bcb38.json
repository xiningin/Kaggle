{"cell_type":{"594316ca":"code","a470206a":"code","36c1426c":"code","f41f506d":"code","4312e39f":"code","bf296dbe":"code","45184846":"code","8682af84":"code","b22f874d":"code","561ae948":"code","0e1d86aa":"code","aed6e4c6":"code","b198be7f":"code","19e3caf8":"code","f6c7adeb":"code","3c462880":"code","a2291211":"code","ab9cbc5b":"code","5b88f54d":"code","091a358e":"code","4f8892b6":"code","cf09b23e":"code","192eeab8":"code","0e68d8f9":"code","4afdfda8":"code","fdaf5bc1":"code","20d999f2":"code","2e8cc17c":"code","030906cc":"code","20df625e":"code","a68bf999":"code","0d302db7":"code","8d09e56b":"code","5687d602":"code","c3931f62":"code","9cd47b9c":"code","773980cc":"code","6f588f6b":"code","e81bef97":"code","23a2ef29":"code","2d00b382":"code","1921a26b":"markdown","52ecaf6a":"markdown","a48c9414":"markdown","646453f8":"markdown","a459216c":"markdown","e20b1ff5":"markdown","88dee76c":"markdown","01466825":"markdown","c4ea1b62":"markdown","9c028600":"markdown","8ae752b7":"markdown","99ddfc51":"markdown","b17d22cc":"markdown","63ba50cd":"markdown","726ceec4":"markdown","719ca565":"markdown","c2c5c34b":"markdown","8886a383":"markdown","f1f4f941":"markdown","7bd0b55e":"markdown","824134fc":"markdown","e524d9c0":"markdown","197bd0ed":"markdown","64294d3f":"markdown","cb849952":"markdown","335e0c22":"markdown","f9f9d0d3":"markdown","457d0a77":"markdown","74105a3f":"markdown","b95ca661":"markdown","840a7de4":"markdown","134d5bae":"markdown","46a76c01":"markdown"},"source":{"594316ca":"print('Hello My name is Bashir Abubakar and welcome to this exploration!')","a470206a":"# Import Necessary Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib as matplot\nimport seaborn as sns\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n!pip install chart_studio\n!pip install cufflinks\nfrom chart_studio.plotly import plot, iplot\nfrom plotly.offline import init_notebook_mode, iplot\nimport plotly.figure_factory as ff\nimport cufflinks\ncufflinks.go_offline()\ncufflinks.set_config_file(world_readable=True, theme='pearl')\nimport plotly.graph_objs as go\nimport chart_studio.plotly as py\nimport plotly\nimport chart_studio\nchart_studio.tools.set_credentials_file(username='bashman18', api_key='\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022\u2022')\ninit_notebook_mode(connected=True)\nprint('All modules imported')","36c1426c":"df = pd.read_csv('..\/input\/HR_COM1.CSV', index_col=None)","f41f506d":"# Check to see if there are missing values in data set\ndf.isnull().any()","4312e39f":"df.head()","bf296dbe":"df = df.rename(columns={'satisfaction_level': 'satisfaction', \n                        'last_evaluation': 'evaluation',\n                        'number_project': 'projectCount',\n                        'average_montly_hours': 'averageMonthlyHours',\n                        'time_spend_company': 'yearsAtCompany',\n                        'Work_accident': 'workAccident',\n                        'promotion_last_5years': 'promotion',\n                        'sales' : 'department',\n                        'left' : 'turnover'\n                        })","45184846":"front = df['turnover']\ndf.drop(labels=['turnover'], axis=1,inplace = True)\ndf.insert(0, 'turnover', front)\ndf.head()","8682af84":"df.shape","b22f874d":"df.dtypes","561ae948":"# From observation about 76.1% of employees stayed and 23.8% of employees left. \nturnover_rate = df.turnover.value_counts() \/ 14999\nturnover_rate","0e1d86aa":"df.describe()","aed6e4c6":"# Overview of summary (Turnover V.S. Non-turnover)\nturnover_Summary = df.groupby('turnover')\nturnover_Summary.mean()","b198be7f":"corr = df.corr()\ncorr = (corr)\nax = plt.axes()\nsns.heatmap(corr, \n            xticklabels=corr.columns.values,\n            yticklabels=corr.columns.values, ax=ax)\nax.set_title('Correlation Matrix Chart')\nplt.show()\ncorr","19e3caf8":"# Compare the means of our employee turnover satisfaction against the employee population satisfaction\nemp_population_satisfaction = df['satisfaction'].mean()\nemp_turnover_satisfaction = df[df['turnover']==1]['satisfaction'].mean()\n\nprint( 'The mean for the employee population is: ' + str(emp_population_satisfaction) )\nprint( 'The mean for the employees that had a turnover is: ' + str(emp_turnover_satisfaction) )","f6c7adeb":"import scipy.stats as stats\nstats.ttest_1samp(a=  df[df['turnover']==1]['satisfaction'], # Sample of Employee satisfaction who had a Turnover\n                  popmean = emp_population_satisfaction)  # Employee Population satisfaction mean","3c462880":"degree_freedom = len(df[df['turnover']==1])\n\nLQ = stats.t.ppf(0.025,degree_freedom)  # Left Quartile\n\nRQ = stats.t.ppf(0.975,degree_freedom)  # Right Quartile\n\nprint ('The t-distribution left quartile range is: ' + str(LQ))\nprint ('The t-distribution right quartile range is: ' + str(RQ))\n","a2291211":"import plotly.express as px\n\nfig = px.histogram(df, x=\"satisfaction\", y=\"Emp ID\", color=\"turnover\",\n                   marginal=\"box\", # or violin, rug\n                   hover_data=df.columns)\nfig.update_layout(title_text='Employee Distribution Plot - Satisfaction Level',xaxis_title=\"Satisfaction\",\n    yaxis_title=\"No of Employees\")\nfig.show()","ab9cbc5b":"import plotly.express as px\n\nfig = px.histogram(df, x=\"evaluation\", y=\"Emp ID\", color=\"turnover\",\n                   marginal=\"box\", # or violin, rug\n                   hover_data=df.columns)\nfig.update_layout(title_text='Employee Distribution Plot - Evaluation Level',xaxis_title=\"Evaluation\",\n    yaxis_title=\"No of Employees\")\nfig.show()","5b88f54d":"import plotly.express as px\n\nfig = px.histogram(df, x=\"averageMonthlyHours\", y=\"Emp ID\", color=\"turnover\",\n                   marginal=\"box\", # or violin, rug\n                   hover_data=df.columns)\nfig.update_layout(title_text='Employee Distribution Plot - Average Monthly Hours',xaxis_title=\"Average Monthly Hours\",\n    yaxis_title=\"No of Employees\")\nfig.show()","091a358e":"from plotly import graph_objs as go\n\nfig = go.Figure()\nfor name, group in df.groupby('turnover'):\n    trace = go.Histogram()\n    trace.name = name\n    trace.x = group['salary']\n    fig.add_trace(trace)\nfig.update_layout(title_text='Employee Evaluation Distribution - Salary V.S. Turnover',xaxis_title=\"Salary\",\n    yaxis_title=\"No of Employees\")\nfig.show()","4f8892b6":"import plotly.express as px\nfig = go.Figure()\nfor name, group in df.groupby('turnover'):\n    trace = go.Histogram()\n    trace.name = name\n    trace.x = group['dept']\n    fig.add_trace(trace)\nfig.update_layout(title_text='Employee Evaluation Distribution - Department V.S. Turnover',xaxis_title=\"Department\",\n    yaxis_title=\"No of Employees\")\nfig.show()","cf09b23e":"fig = go.Figure()\nfor name, group in df.groupby('turnover'):\n    trace = go.Histogram()\n    trace.name = name\n    trace.x = group['projectCount']\n    fig.add_trace(trace)\nfig.update_layout(title_text='Employee Evaluation Distribution - Turnover V.S. Project Count',xaxis_title=\"Project Count\",\n    yaxis_title=\"No of Employees\")    \nfig.show()","192eeab8":"import plotly.figure_factory as ff\n\nx1 = df.loc[(df['turnover'] == 0),'evaluation']\nx2 = df.loc[(df['turnover'] == 1),'evaluation']\n\ngroup_labels = ['no turnover', 'turnover']\n\ncolors = ['blue', 'red']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1, x2], group_labels, bin_size=1.0,\n                         curve_type='kde', # override default 'kde'\n                         colors=colors, show_hist=False)\n\n# Add title\nfig.update_layout(title_text='Employee Evaluation Distribution KDE - Turnover V.S. No Turnover')\nfig.show()","0e68d8f9":"import plotly.figure_factory as ff\n\nx1 = df.loc[(df['turnover'] == 0),'averageMonthlyHours']\nx2 = df.loc[(df['turnover'] == 1),'averageMonthlyHours']\n\ngroup_labels = ['no turnover', 'turnover']\n\ncolors = ['blue', 'red']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1, x2], group_labels, bin_size=1.0,\n                         curve_type='kde', # override default 'kde'\n                         colors=colors, show_hist=False)\n\n# Add title\nfig.update_layout(title_text='Employee AverageMonthly Hours Distribution KDE - Turnover V.S. No Turnover')\nfig.show()","4afdfda8":"x1 = df.loc[(df['turnover'] == 0),'satisfaction']\nx2 = df.loc[(df['turnover'] == 1),'satisfaction']\n\ngroup_labels = ['no turnover', 'turnover']\n\ncolors = ['blue', 'red']\n\n# Create distplot with curve_type set to 'normal'\nfig = ff.create_distplot([x1, x2], group_labels, bin_size=1.0,\n                         curve_type='kde', # override default 'kde'\n                         colors=colors, show_hist=False)\n\n# Add title\nfig.update_layout(title_text='Employee Satisfaction Distribution KDE - Turnover V.S. No Turnover')\nfig.show()","fdaf5bc1":"fig = px.box(df,x=\"projectCount\", y=\"averageMonthlyHours\", color=\"turnover\")\nfig.update_layout(title_text='Project Count V.S. Average Monthly Hours',xaxis_title=\"Project Count\",\n    yaxis_title=\"Average Monthly Hours\")\nfig.show()","20d999f2":"#ProjectCount VS Evaluation\n#Looks like employees who did not leave the company had an average evaluation of around 70% even with different projectCounts\n#There is a huge skew in employees who had a turnover though. It drastically changes after 3 projectCounts. \n#Employees that had two projects and a horrible evaluation left. Employees with more than 3 projects and super high evaluations left\nfig = px.box(df,x=\"projectCount\", y=\"evaluation\", color=\"turnover\")\nfig.update_layout(title_text='Project Count V.S. Evaluation',xaxis_title=\"Project Count\",\n    yaxis_title=\"Evaluation\")\nfig.show()","2e8cc17c":"import plotly.express as px\nfig = px.scatter(df, x=\"satisfaction\", y=\"evaluation\",color=\"turnover\")\nfig.update_layout(title_text='Satisfaction V.S. Evaluation Cluster Chart', xaxis_title=\"Satisfaction\",\n    yaxis_title=\"Evaluation\")\nfig.show()","030906cc":"import plotly.express as px\n\nfig = px.histogram(df, x=\"yearsAtCompany\", y=\"Emp ID\", color=\"turnover\",\n                   marginal=\"\", # or violin, rug\n                   hover_data=df.columns)\nfig.update_layout(title_text='Turnover Rate V.S. Years Spent', xaxis_title=\"Years Spent\",\n    yaxis_title=\"No of Employees\")\nfig.show()","20df625e":"# Import KMeans Model\nfrom sklearn.cluster import KMeans\n\n# Graph and create 3 clusters of Employee Turnover\nkmeans = KMeans(n_clusters=3,random_state=2)\nkmeans.fit(df[df.turnover==1][[\"satisfaction\",\"evaluation\"]])\n\nkmeans_colors = ['green' if c == 0 else 'blue' if c == 2 else 'red' for c in kmeans.labels_]","a68bf999":"# Determine number of clusters with K-means elbow method\n# The arc of the elbow shows that number 3 is our best fit and as thus we would create  3 clusters of employee\nsse={}\nbr = df[df.turnover==1][[\"satisfaction\",\"evaluation\"]]\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(df[df.turnover==1][[\"satisfaction\",\"evaluation\"]])\n\n    sse[k] = kmeans.inertia_ \nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of clusters\")\nplt.show()","0d302db7":"import plotly.graph_objects as go\n\nfig = go.Figure(data=[go.Scatter(\n    x=df[\"satisfaction\"], y=df[\"evaluation\"],\n    mode='markers',\n    marker=dict(\n        color= kmeans_colors,\n        opacity=[1, 0.8, 0.6, 0.4],\n        size=[40, 60, 80, 100],\n    )\n)])\nfig.update_layout(title_text='Employee Cluster Chart')\nfig.show()","8d09e56b":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score, confusion_matrix, precision_recall_curve\nfrom sklearn.preprocessing import RobustScaler\n\n# Create dummy variables for the 'department' and 'salary' features, since they are categorical \ndepartment = pd.get_dummies(data=df['dept'],drop_first=True,prefix='dep') #drop first column to avoid dummy trap\nsalary = pd.get_dummies(data=df['salary'],drop_first=True,prefix='sal')\ndf.drop(['dept','salary'],axis=1,inplace=True)\ndf = pd.concat([df,department,salary],axis=1)","5687d602":"# Create base rate model\ndef base_rate_model(X) :\n    y = np.zeros(X.shape[0])\n    return y","c3931f62":"# Create train and test splits\ntarget_name = 'turnover'\nX = df.drop('turnover', axis=1)\nrobust_scaler = RobustScaler()\nX = robust_scaler.fit_transform(X)\ny=df[target_name]\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.15, random_state=123, stratify=y)","9cd47b9c":"# Check accuracy of base rate model\ny_base_rate = base_rate_model(X_test)\nfrom sklearn.metrics import accuracy_score\nprint (\"Base rate accuracy is %2.2f\" % accuracy_score(y_test, y_base_rate))","773980cc":"# Check accuracy of Logistic Model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(penalty='l2', C=1)\n\nmodel.fit(X_train, y_train)\nprint (\"Logistic accuracy is %2.2f\" % accuracy_score(y_test, model.predict(X_test)))","6f588f6b":"# Using 10 fold Cross-Validation to train our Logistic Regression Model\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nkfold = model_selection.KFold(n_splits=10, random_state=7)\nmodelCV = LogisticRegression(class_weight = \"balanced\")\nscoring = 'roc_auc'\nresults = model_selection.cross_val_score(modelCV, X_train, y_train, cv=kfold, scoring=scoring)\nprint(\"AUC: %.3f (%.3f)\" % (results.mean(), results.std()))","e81bef97":"# Compare the Logistic Regression Model V.S. Base Rate Model V.S. Random Forest Model\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import tree\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import VotingClassifier\n\n\nprint (\"---Base Model---\")\nbase_roc_auc = roc_auc_score(y_test, base_rate_model(X_test))\nprint (\"Base Rate AUC = %2.2f\" % base_roc_auc)\nprint(classification_report(y_test, base_rate_model(X_test)))\n\n# NOTE: By adding in \"class_weight = balanced\", the Logistic Auc increased by about 10%! This adjusts the threshold value\nlogis = LogisticRegression(class_weight = \"balanced\")\nlogis.fit(X_train, y_train)\nprint (\"\\n\\n ---Logistic Model---\")\nlogit_roc_auc = roc_auc_score(y_test, logis.predict(X_test))\nprint (\"Logistic AUC = %2.2f\" % logit_roc_auc)\nprint(classification_report(y_test, logis.predict(X_test)))\n\n# Decision Tree Model\ndtree = tree.DecisionTreeClassifier(\n    #max_depth=3,\n    class_weight=\"balanced\",\n    min_weight_fraction_leaf=0.01\n    )\ndtree = dtree.fit(X_train,y_train)\nprint (\"\\n\\n ---Decision Tree Model---\")\ndt_roc_auc = roc_auc_score(y_test, dtree.predict(X_test))\nprint (\"Decision Tree AUC = %2.2f\" % dt_roc_auc)\nprint(classification_report(y_test, dtree.predict(X_test)))\n\n# Random Forest Model\nrf = RandomForestClassifier(\n    n_estimators=1000, \n    max_depth=None, \n    min_samples_split=10, \n    class_weight=\"balanced\"\n    #min_weight_fraction_leaf=0.02 \n    )\nrf.fit(X_train, y_train)\nprint (\"\\n\\n ---Random Forest Model---\")\nrf_roc_auc = roc_auc_score(y_test, rf.predict(X_test))\nprint (\"Random Forest AUC = %2.2f\" % rf_roc_auc)\nprint(classification_report(y_test, rf.predict(X_test)))\n\n\n# Ada Boost\nada = AdaBoostClassifier(n_estimators=400, learning_rate=0.1)\nada.fit(X_train,y_train)\nprint (\"\\n\\n ---AdaBoost Model---\")\nada_roc_auc = roc_auc_score(y_test, ada.predict(X_test))\nprint (\"AdaBoost AUC = %2.2f\" % ada_roc_auc)\nprint(classification_report(y_test, ada.predict(X_test)))","23a2ef29":"# Create ROC Graph\nfrom sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, logis.predict_proba(X_test)[:,1])\nrf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test, rf.predict_proba(X_test)[:,1])\ndt_fpr, dt_tpr, dt_thresholds = roc_curve(y_test, dtree.predict_proba(X_test)[:,1])\nada_fpr, ada_tpr, ada_thresholds = roc_curve(y_test, ada.predict_proba(X_test)[:,1])\n\nplt.figure()\n\n# Plot Logistic Regression ROC\nplt.plot(fpr, tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)\n\n# Plot Random Forest ROC\nplt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)\n\n# Plot Decision Tree ROC\nplt.plot(dt_fpr, dt_tpr, label='Decision Tree (area = %0.2f)' % dt_roc_auc)\n\n# Plot AdaBoost ROC\nplt.plot(ada_fpr, ada_tpr, label='AdaBoost (area = %0.2f)' % ada_roc_auc)\n\n# Plot Base Rate ROC\nplt.plot([0,1], [0,1],label='Base Rate' 'k--')\n\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('ROC Graph')\nplt.legend(loc=\"lower right\")\nplt.show()","2d00b382":"plt.style.use('fivethirtyeight')\nplt.rcParams['figure.figsize'] = (12,6)\n\nimportances = dtree.feature_importances_\nfeat_names = df.drop(['turnover'],axis=1).columns\n\nindices = np.argsort(importances)[::-1]\nplt.figure(figsize=(12,6))\nplt.title(\"Feature importances by DecisionTreeClassifier\")\nplt.bar(range(len(indices)), importances[indices], color='lightblue',  align=\"center\")\nplt.step(range(len(indices)), np.cumsum(importances[indices]), where='mid', label='Cumulative')\nplt.xticks(range(len(indices)), feat_names[indices], rotation='vertical',fontsize=14)\nplt.xlim([-1, len(indices)])\nplt.show()","1921a26b":"##  Salary V.S. Turnover\n***\n**Summary:** This is not unusual. Here's what I found:\n - Majority of employees who left either had **low** or **medium** salary.\n - Barely any employees left with **high** salary\n - Employees with low to average salaries tend to leave the company.\n \n**Observation** \n - What is the work environment like for low, medium, and high salaries?\n - What made employees with high salaries to leave?","52ecaf6a":"## Description\n***\nWhat type of employees are leaving? Determine which employees are prone to leave next. Reasons why employees are leaving?","a48c9414":"# Exploratory Data Analysis\n*** ","646453f8":"### T-Test Quantile\n***\nIf the t-statistic value we calculated above **(-39.109)** is outside the quantiles, then we can reject the null hypothesis","a459216c":"## Statistical Test for Correlation\n***\n\n### One-Sample T-Test (Measuring Satisfaction Level)\nA one-sample t-test checks whether a sample mean differs from the population mean. Let's test to see whether the average satisfaction level of employees that had a turnover differs from the entire employee population.\n\n**Hypothesis Testing:** Is there significant difference in the **means of satisfaction level** between employees who had a turnover and the entire employee population?\n\n - **Null Hypothesis:** *(H0: pTS = pES)* The null hypothesis would be that there is **no** difference in satisfaction level between employees who did turnover and the entire employee population.\n\n - **Alternate Hypothesis:** *(HA: pTS != pES)* The alternative hypothesis would be that there **is** a difference in satisfaction level between employees who did turnover and the entire employee population.","e20b1ff5":"##  Turnover V.S. Satisfaction \n***\n**Summary:** \n - There is a **tri-modal** distribution for employees that turnovered\n - Employees who had really low satisfaction levels **(0.2 or less)** left the company more\n - Employees who had low satisfaction levels **(0.3~0.5)** left the company more\n - Employees who had really high satisfaction levels **(0.7 or more)** left the company more","88dee76c":"### One-Sample T-Test Summary\n***\n#### **T-Test = -39.109**       |        **P-Value = 9.01e-279**       |       **Reject Null Hypothesis**\n\n**Reject the null hypothesis because:**\n - T-Test score is outside the quantiles\n - P-value is lower than confidence level of 5%\n\nBased on the statistical analysis of a one sample t-test, there seems to be some significant difference between the mean satisfaction of employees who had a turnover and the entire employee population. The super low P-value of **9.012e-279** at a 5% confidence level is a good indicator to **reject the null hypothesis**. \n\nBut this does not neccessarily mean that there is practical significance. We would have to conduct more analysis.","01466825":"##  Statistical Overview \n***\nThe dataset contains:\n - About 14,999 employee observations and 11 features \n - The company had a turnover rate of about 23.8%\n - Mean satisfaction of employees is 61.2%","c4ea1b62":"##  Distribution Plots (Satisfaction - Evaluation - AverageMonthlyHours)\n***\n**Summary:** Let's examine the distribution on some of the employee's features. Here's what I found:\n - **Satisfaction** - There is a huge spike for employees with low satisfaction and high satisfaction.\n - **Evaluation** - There is a bimodal distrubtion of employees for low evaluations (less than 0.6) and high evaluations (more than 0.8)\n - **AverageMonthlyHours** - There is another bimodal distribution of employees with lower and higher average monthly hours (less than 150 hours & more than 250 hours)\n - The evaluation and average monthly hour graphs both share a similar distribution. \n - Employees with lower average monthly hours were evaluated less and vice versa.\n - If you look back at the correlation matrix, the high correlation between evaluation and averageMonthlyHours does support this finding.\n \n**Observations** \n - Is there a reason for the high spike in low satisfaction of employees?\n - Could employees be grouped in a way with these features?\n - Is there a correlation between evaluation and averageMonthlyHours?","9c028600":"##  Turnover V.S. YearsAtCompany \n***\n**Summary:** Let's see if theres a point where employees start leaving the company. Here's what I found:\n - More than half of the employees with **4 and 5** years left the company\n - Employees with **5** years should **highly** be looked into \n \n**Observation** \n - Why are employees leaving mostly at the **3-5** year range?\n - Who are these employees that left?\n - Are these employees part-time or contractors? ","8ae752b7":"### Conducting the T-Test\n***\nConduct a t-test at **95% confidence level** and see if it correctly rejects the null hypothesis that the sample comes from the same distribution as the employee population. To conduct a one sample t-test, we can use the **stats.ttest_1samp()** function:","99ddfc51":"# Data Cleaning \n***","b17d22cc":"# Feature Importance\n***\n\n**Top 3 Features:**\n1. Satisfaction\n2. YearsAtCompany\n3. Evaluation\n","63ba50cd":"### T-Test Result\n***\nThe test result shows the **test statistic \"t\" is equal to -39.109**. This test statistic tells us how much the sample mean deviates from the null hypothesis. If the t-statistic lies **outside** the quantiles of the t-distribution corresponding to our confidence level and degrees of freedom, we reject the null hypothesis. We can check the quantiles with **stats.t.ppf()**:","726ceec4":"##  Correlation Matrix & Heatmap\n***\n**Moderate Positively Correlated Features:** \n- projectCount vs evaluation: 0.349333\n- projectCount vs averageMonthlyHours: 0.417211\n- averageMonthlyHours vs evaluation: 0.339742\n\n**Moderate Negatively Correlated Feature:**\n - satisfaction vs turnover:  -0.388375\n\n**Observations**\n- What features affect our target variable the most?\n- What features have strong correlations with each other?\n- Can we do an in depth analysis of these features?\n\n**Summary:**\n\nFrom the heatmap, there is a **positive(+)** correlation between projectCount, averageMonthlyHours, and evaluation. Which could mean that the employees who spent more hours and did more projects were evaluated highly. \n\nFor the **negative(-)** relationships, turnover and satisfaction are highly correlated. I'm assuming that people tend to leave a company more when they are less satisfied. ","719ca565":"# Let's Connect on LinkedIn!\nIf anybody would like to discuss any other projects or just have a chat about data science topics, I'll be more than happy to connect with you on **LinkedIn:**\nhttps:\/\/www.linkedin.com\/in\/bashir-abubakar-61935417b\/","c2c5c34b":"##  Turnover V.S. Evaluation \n***\n**Summary:** \n - There is a biomodal distribution for those that had a turnover. \n - Employees with **low** performance tend to leave the company more\n - Employees with **high** performance tend to leave the company more\n - The **sweet spot** for employees that stayed is within **0.6-0.8** evaluation","8886a383":"##  ProjectCount VS AverageMonthlyHours \n***\n\n**Summary:**\n - As project count increased, so did average monthly hours\n - Something weird about the boxplot graph is the difference in averageMonthlyHours between people who had a turnver and did not. \n - Looks like employees who **did not** have a turnover had **consistent** averageMonthlyHours, despite the increase in projects\n - In contrast, employees who **did** have a turnover had an increase in averageMonthlyHours with the increase in projects\n\n**Observation** \n - What could be the meaning for this? \n - **Why is it that employees who left worked more hours than employees who didn't, even with the same project count?**","f1f4f941":"# Modeling the Data\n***\n The best model performance out of the four (Decision Tree Model, AdaBoost Model, Logistic Regression Model, Random Forest Model) was **AdaBoost**! \n \n **Note: Base Rate** \n ***\n - A **Base Rate Model** is a model that always selects the target variable's **majority class**. It's just used for reference to compare how better another model is against it. In this dataset, the majority class that will be predicted will be **0's**, which are employees who did not leave the company. \n - If you recall back to **Part 3: Exploring the Data**, 24% of the dataset contained 1's (employee who left the company) and the remaining 76% contained 0's (employee who did not leave the company). The Base Rate Model would simply predict every 0's and ignore all the 1's. \n - **Example**: The base rate accuracy for this data set, when classifying everything as 0's, would be 76% because 76% of the dataset are labeled as 0's (employees not leaving the company).","7bd0b55e":"## Logistic Regression V.S. Random Forest V.S. Decision Tree V.S. AdaBoost Model\n***","824134fc":"# Human Resources Analytics - Employee Attrition Problem\n***","e524d9c0":"##  Turnover V.S. AverageMonthlyHours \n***\n**Summary:** \n - Another bi-modal distribution for employees that turnovered \n - Employees who had less hours of work **(~150hours or less)** left the company more\n - Employees who had too many hours of work **(~250 or more)** left the company \n - Employees who left generally were **underworked** or **overworked**.","197bd0ed":"## Objective\n***\nThe data is for company X which is trying to control attrition. There are two sets of data: \u201cExisting employees\u201d and \u201cEmployees who have left\u201d. Following attributes are available for every employee.<br>\n1. Satisfaction Level<br>\n2. Last evaluation<br>\n3. Number of projects<br>\n4. Average monthly hours<br>\n5. Time spent at the company<br>\n6. Whether they have had a work accident<br>\n7. Whether they have had a promotion in the last 5 years<br>\n8. Departments (column sales)<br>\n9. Salary<br>\n10. Whether the employee has left<br>","64294d3f":"# Data Preprocessing\n***","cb849952":"##  Satisfaction VS Evaluation\n***\n**Summary:** This is by far the most compelling graph. This is what I found:\n - There are **3** distinct clusters for employees who left the company\n \n**Cluster 1 (Hard-working and Sad Employee):** Satisfaction was below 0.2 and evaluations were greater than 0.75. Which could be a good indication that employees who left the company were good workers but felt horrible at their job. \n - **Question:** What could be the reason for feeling so horrible when you are highly evaluated? Could it be working too hard? Could this cluster mean employees who are \"overworked\"?\n\n**Cluster 2 (Bad and Sad Employee):** Satisfaction between about 0.35~0.45 and evaluations below ~0.58. This could be seen as employees who were badly evaluated and felt bad at work.\n - **Question:** Could this cluster mean employees who \"under-performed\"?\n\n**Cluster 3 (Hard-working and Happy Employee):** Satisfaction between 0.7~1.0 and evaluations were greater than 0.8. Which could mean that employees in this cluster were \"ideal\". They loved their work and were evaluated highly for their performance. \n - **Question:** Could this cluser mean that employees left because they found another job opportunity?","335e0c22":"##  Turnover V.S. ProjectCount \n***\n**Summary:** This graph is quite interesting as well. Here's what I found:\n - More than half of the employees with **2,6, and 7** projects left the company\n - Majority of the employees who did not leave the company had **3,4, and 5** projects\n - All of the employees with **7** projects left the company\n - There is an increase in employee turnover rate as project count increases\n \n**Observation** \n - Why are employees leaving at the lower\/higher spectrum of project counts?\n - Does this means that employees with project counts 2 or less are not worked hard enough or are not highly valued, thus leaving the company?\n - Do employees with 6+ projects are getting overworked, thus leaving the company?\n\n","f9f9d0d3":"## K-Means Clustering of Employee Turnover\n***\n**Cluster 1 (Blue):** Hard-working and Sad Employees\n\n**Cluster 2 (Red):** Bad and Sad Employee \n\n**Cluster 3 (Green):** Hard-working and Happy Employee \n\n**Clustering PROBLEM:**\n    - How do we know that there are \"3\" clusters?\n    - Determine number of clusters with K-means elbow method algorithm","457d0a77":"## ROC Graph\n***","74105a3f":"https:\/\/www.linkedin.com\/in\/bashir-abubakar-61935417b\/","b95ca661":"##  ProjectCount VS Evaluation\n***\n**Summary:** This graph looks very similar to the graph above. What I find strange with this graph is with the turnover group. There is an increase in evaluation for employees who did more projects within the turnover group. But, again for the non-turnover group, employees here had a consistent evaluation score despite the increase in project counts. \n\n**Observations*\n - **Why is it that employees who left, had on average, a higher evaluation than employees who did not leave, even with an increase in project count? **\n - Shouldn't employees with lower evaluations tend to leave the company more? ","840a7de4":"##  Department V.S. Turnover \n***\n**Summary:** Let's see more information about the departments. Here's what I found:\n - The **sales, technical, and support department** were the top 3 departments to have employee turnover\n - The management department had the smallest amount of turnover\n \n**Observation** \n - If we had more information on each department, can we pinpoint a more direct cause for employee turnover?","134d5bae":"# To Recap\n***\n\n**Summary:** \n 1. Employees generally left when they are **underworked** (less than 150hr\/month or 6hr\/day)\n 2. Employees generally left when they are **overworked** (more than 250hr\/month or 10hr\/day)\n 3. Employees with either **really high or low evaluations** should be taken into consideration for high turnover rate\n 4. Employees with **low to medium salaries** are the bulk of employee turnover\n 5. Employees that had **2,6, or 7 project count** was at risk of leaving the company\n 6. Employee **satisfaction** is the highest indicator for employee turnover.\n 7. Employee that had **4 and 5 yearsAtCompany** should be taken into consideration for high turnover rate\n 8. Employee **satisfaction**, **yearsAtCompany**, and **evaluation** were the three biggest factors in determining turnover.","46a76c01":"<img src=\"https:\/\/content.linkedin.com\/content\/dam\/brand\/site\/img\/logo\/logo-tm.png\"\/>"}}