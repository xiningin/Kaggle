{"cell_type":{"7a1908aa":"code","38da20a4":"code","82192f5d":"code","9fbc7e09":"code","c5a4b6d6":"code","c631eb10":"code","7d568740":"code","4bd0bb1d":"code","03f58ff1":"code","fb55c529":"code","a690826c":"code","5bb48a25":"code","bd06cb41":"code","dd774632":"code","b6cecb97":"code","4a0dc268":"code","7dda5976":"code","d4211518":"code","3a16d2c1":"code","8b7e0a69":"code","3b3fc650":"code","21497e8e":"code","525af17d":"code","61dca1a7":"code","d0249c59":"code","37abcc39":"code","b431c6d1":"code","523960af":"code","cde44870":"code","e4a77cff":"code","85dfbdab":"code","7a79be6f":"code","70257ae5":"code","89ccb976":"code","38504999":"code","4665f525":"code","cb7a4715":"code","21611a16":"code","0354e892":"markdown","bfdcdb4a":"markdown","5eb5fec6":"markdown","ff4d6ebe":"markdown","fde5cc8b":"markdown","b8f338e6":"markdown","06e64252":"markdown","47c17392":"markdown","1c3cca46":"markdown","bb7e7a0f":"markdown","b5117b92":"markdown"},"source":{"7a1908aa":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob","38da20a4":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","82192f5d":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    return wap\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","9fbc7e09":"book_train = pd.read_parquet(data_dir + \"book_train.parquet\/stock_id=15\")\nbook_train.head()","c5a4b6d6":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","c631eb10":"%%time\nfile_path = data_dir + \"book_train.parquet\/stock_id=0\"\npreprocessor_book(file_path)","7d568740":"trade_train = pd.read_parquet(data_dir + \"trade_train.parquet\/stock_id=0\")\ntrade_train.head(15)","4bd0bb1d":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","03f58ff1":"%%time\nfile_path = data_dir + \"trade_train.parquet\/stock_id=0\"\npreprocessor_trade(file_path)","fb55c529":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","a690826c":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","5bb48a25":"train = pd.read_csv(data_dir + 'train.csv')","bd06cb41":"train_ids = train.stock_id.unique()","dd774632":"%%time\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)","b6cecb97":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","4a0dc268":"df_train.head()","7dda5976":"test = pd.read_csv(data_dir + 'test.csv')","d4211518":"test_ids = test.stock_id.unique()","3a16d2c1":"%%time\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)","8b7e0a69":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","3b3fc650":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 10, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","21497e8e":"df_train.head()","525af17d":"df_test.head()","61dca1a7":"import lightgbm as lgbm","d0249c59":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","37abcc39":"X = df_train.drop(['row_id','target'],axis=1)\ny = df_train['target']","b431c6d1":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))\n\ndef feval_RMSPE(preds, lgbm_train):\n    labels = lgbm_train.get_label()\n    return 'RMSPE', round(rmspe(y_true = labels, y_pred = preds),5), False\n\nparams = {\n      \"objective\": \"rmse\", \n      \"metric\": \"rmse\", \n      \"boosting_type\": \"gbdt\",\n      'early_stopping_rounds': 30,\n      'learning_rate': 0.01,\n      'lambda_l1': 1,\n      'lambda_l2': 1,\n      'feature_fraction': 0.8,\n      'bagging_fraction': 0.8,\n  }","523960af":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=5, random_state=19901028, shuffle=True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0                         # validation score","cde44870":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n\n    print(\"Fold :\", fold+1)\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n    #RMSPE weight\n    weights = 1\/np.square(y_train)\n    lgbm_train = lgbm.Dataset(X_train,y_train,weight = weights)\n\n    weights = 1\/np.square(y_valid)\n    lgbm_valid = lgbm.Dataset(X_valid,y_valid,reference = lgbm_train,weight = weights)\n    \n    # model \n    model = lgbm.train(params=params,\n                      train_set=lgbm_train,\n                      valid_sets=[lgbm_train, lgbm_valid],\n                      num_boost_round=5000,         \n                      feval=feval_RMSPE,\n                      verbose_eval=100,\n                      categorical_feature = ['stock_id']                \n                     )\n    \n    # validation \n    y_pred = model.predict(X_valid, num_iteration=model.best_iteration)\n\n    RMSPE = round(rmspe(y_true = y_valid, y_pred = y_pred),3)\n    print(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n\n    #keep scores and models\n    scores += RMSPE \/ 5\n    models.append(model)\n    print(\"*\" * 100)","e4a77cff":"scores","85dfbdab":"df_test.columns","7a79be6f":"df_train.columns","70257ae5":"y_pred = df_test[['row_id']]\nX_test = df_test.drop(['time_id', 'row_id'], axis = 1)","89ccb976":"X_test","38504999":"target = np.zeros(len(X_test))\n\n#light gbm models\nfor model in models:\n    pred = model.predict(X_test[X_valid.columns], num_iteration=model.best_iteration)\n    target += pred \/ len(models)","4665f525":"y_pred = y_pred.assign(target = target)","cb7a4715":"y_pred","21611a16":"y_pred.to_csv('submission.csv',index = False)","0354e892":"## LightGBM","bfdcdb4a":"## Main function for preprocessing book data","5eb5fec6":"## Functions for preprocess","ff4d6ebe":"## Training set","fde5cc8b":"### Cross Validation","b8f338e6":"# Test set","06e64252":"## Test set","47c17392":"## Model Building","1c3cca46":"## Main function for preprocessing trade data","bb7e7a0f":"## Combined preprocessor function","b5117b92":"## Target encoding by stock_id"}}