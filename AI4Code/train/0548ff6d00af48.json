{"cell_type":{"86212986":"code","cc6aa56e":"code","f3038a35":"code","ff9b167d":"code","6cfe464d":"code","f423bdbd":"code","9c3dbe0d":"code","96e8d99f":"code","d6f9fee2":"code","b0df8520":"code","6bf62b55":"code","51d86314":"code","9c90e4c5":"code","97ebb0d0":"code","5175c21f":"code","31ef693a":"code","676ff87a":"code","66067b4f":"code","9acca469":"code","38f17118":"code","f0175e75":"code","f3b99c07":"code","18ec3b61":"code","603342cf":"code","1d6d6fd6":"code","2afc61aa":"code","fb087260":"markdown","7ac85d06":"markdown","79ed207b":"markdown","4e452fe9":"markdown","0a53b689":"markdown","59ff59cf":"markdown","fdb63695":"markdown","1a28620f":"markdown"},"source":{"86212986":"#import basic module\nimport numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport seaborn as sns","cc6aa56e":"RANDOM_SEED = 42","f3038a35":"fake_news = pd. read_csv('..\/input\/fake-and-real-news-dataset\/Fake.csv')\ntrue_news = pd.read_csv('..\/input\/fake-and-real-news-dataset\/True.csv')","ff9b167d":"print(fake_news.shape)\nprint(true_news.shape)","6cfe464d":"fake_news.head()","f423bdbd":"true_news.head()","9c3dbe0d":"words = []\nwords.append(list(fake_news['text'].apply(len)))\nwords.append(list(true_news['text'].apply(len)))\nax = sns.boxplot(data=words)\nax.set(xticklabels=['fake', 'true'])","96e8d99f":"words = []\nwords.append(list(fake_news['title'].apply(len)))\nwords.append(list(true_news['title'].apply(len)))\nax = sns.boxplot(data=words)\nax.set(xticklabels=['fake', 'true'])","d6f9fee2":"import collections\ndef calc_unique_words(col: pd.Series):\n    col = list(col)\n    unique = set()\n    for x in col:\n        unique |= set(x.split())\n    return len(unique)\nunique_fake = calc_unique_words(fake_news['text'])\nunique_true = calc_unique_words(true_news['text'])","b0df8520":"print(unique_fake, unique_true)\n# fake news have unique words","6bf62b55":"fake_news['subject'].value_counts()","51d86314":"true_news['subject'].value_counts()","9c90e4c5":"fake_news['fake_flg'] = 1\ntrue_news['fake_flg'] = 0","97ebb0d0":"df = pd.concat([fake_news, true_news])","5175c21f":"df.head()","31ef693a":"df.tail()","676ff87a":"import re\nimport nltk\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer ","66067b4f":"import string\n# eliminate puctuation\nprint(f'puncuations: {string.punctuation}')\nnopunc = [c for c in df['title'] if c not in string.punctuation]","9acca469":"from tqdm.notebook import tnrange\ncorpus = []\nfor i in tnrange(len(df)):\n    #elminate number, other signs\n    title = re.sub('[^a-zA-Z]', ' ', nopunc[i]) \n    title = title.lower()\n    title = title.split()\n    \n    #word stemming(\"likes\"->\"like\")\n    ps = PorterStemmer()\n    title = [ps.stem(words) for words in title if not words in set(stopwords.words('english'))]\n\n    title = ' '.join(title)\n    corpus.append(title)","38f17118":"corpus[3]","f0175e75":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(corpus, df['fake_flg'], test_size = 0.20, random_state = RANDOM_SEED)","f3b99c07":"from sklearn.pipeline import Pipeline \nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#vectorize text with tfidf(https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.feature_extraction.text.TfidfVectorizer.html)\n\ntfidf = TfidfVectorizer()\ntfidf.fit(X_train) #train should be done only with train data\nX_train = tfidf.transform(X_train)\nX_test = tfidf.transform(X_test)","18ec3b61":"#function for easy training and valuation\nfrom sklearn.metrics import classification_report,roc_auc_score\ndef train_and_predict(clf):\n    clf.fit(X_train, y_train)\n    y_pred = clf.predict(X_test)\n    auc_score = roc_auc_score(y_test, y_pred)\n    print('auc: {:.5}'.format(auc_score))\n    print(classification_report(y_test, y_pred))\n    return clf, y_pred","603342cf":"from sklearn.naive_bayes import MultinomialNB\nclf = MultinomialNB()\ntrain_and_predict(clf)","1d6d6fd6":"import lightgbm as lgb\nclf = lgb.LGBMClassifier()\ntrain_and_predict(clf)","2afc61aa":"#hyper parmerter seach \nfor i in [50, 100, 200, 400, 1000]:\n    print(f'num_leaves: {i}')\n    clf = lgb.LGBMClassifier(num_leaves=i)\n    train_and_predict(clf)","fb087260":"## compare length of text\/subject","7ac85d06":"## Data processing","79ed207b":"## read data and quick review","4e452fe9":"## prepare data","0a53b689":"length of fake news is wider range","59ff59cf":"## Look into subject","fdb63695":"## train and valuate models","1a28620f":"## compare number of unique words"}}