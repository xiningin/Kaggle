{"cell_type":{"897d29b7":"code","c97ac75c":"code","cc7ab039":"code","10169d5e":"code","c930c22f":"code","f6f0d230":"code","4d2c46d6":"code","76001cb3":"code","b6cfbbf3":"code","1cf5fd15":"code","4d110e1e":"code","737fda85":"code","ca58f5bb":"code","ccfce9f7":"code","cfd30e36":"code","c109a1bd":"code","d2d8fb4f":"code","8ad4af3e":"code","8e958900":"code","09c79b6d":"code","d8e332a1":"code","8e3a5c29":"code","1c05fbe1":"code","8304488a":"code","7a3eb826":"code","82e03214":"code","980a9013":"code","feb24587":"code","861d87be":"code","1487a8c3":"code","07ab1f18":"code","11124279":"code","6521bef9":"code","06f3bc00":"code","1a064f75":"code","6905db26":"code","9c6f9a75":"code","49692b6f":"code","5e1fb567":"code","d0e011fa":"code","2b2476f1":"code","1f4788e6":"code","391de786":"code","b17bbdc6":"markdown","46a19b8a":"markdown","f6fe9706":"markdown","e9ea28dd":"markdown","963c4ada":"markdown","7546cd22":"markdown","de2c8bdc":"markdown","7ce58ec9":"markdown","e34f801f":"markdown","e243b761":"markdown","88991581":"markdown","70fa68f1":"markdown","8b443da5":"markdown","4c780a2d":"markdown","61e053a8":"markdown","b049e0a4":"markdown","84302cba":"markdown","1f72ecb3":"markdown"},"source":{"897d29b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport re\nimport matplotlib.pyplot as plt\nfrom tensorflow.keras.layers import Embedding,LSTM,Dense,Dropout\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c97ac75c":"train=pd.read_csv(\"\/kaggle\/input\/twitter-sentiment-analysis-hatred-speech\/train.csv\")\ntrain.head()","cc7ab039":"#Checking length of each tweet\ntrain[\"length\"]=train[\"tweet\"].apply(len)","10169d5e":"train.head()","c930c22f":"train.info()","f6f0d230":"train.isnull().sum()","4d2c46d6":"#Dropping the id column as it is of no use\ntrain=train.drop([\"id\"],axis=1)","76001cb3":"#Representing labels column as a countplot\nimport seaborn as sns\nsns.countplot(train[\"label\"])","b6cfbbf3":"#Distributio for length of tweets\nsns.distplot(train[\"length\"])","1cf5fd15":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","4d110e1e":"stop=stopwords.words(\"english\")","737fda85":"#Calculating the number of stopwords in a tweet\ndef stop_words(df):\n    df['stopwords'] = df['tweet'].apply(lambda x: len([x for x in x.split() if x in stop]))\n    print(df[['tweet','stopwords']].head())","ca58f5bb":"stop_words(train)","ccfce9f7":"#Removing the punctuations from the tweets as they do not help in prediction\ndef punctuation_removal(df):\n    df['tweet'] = df['tweet'].str.replace('[^\\w\\s]','')\n    print(df['tweet'].head())","cfd30e36":"punctuation_removal(train)","c109a1bd":"#Checking the frequency of words in all the tweets\nfreq = pd.Series(' '.join(train['tweet']).split()).value_counts()[:10]\nfreq","d2d8fb4f":"freq = list(freq.index)","8ad4af3e":"#Removing the most frequent words from the dataset\ndef frequent_words_removal(df):  \n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","8e958900":"frequent_words_removal(train)","09c79b6d":"#Checking for rare words in the tweets\nfreq = pd.Series(' '.join(train['tweet']).split()).value_counts()[-10:]\nfreq","d8e332a1":"#Removing all the rare words from the tweets\nfreq = list(freq.index)\ndef rare_words_removal(df):\n    df['tweet'] = df['tweet'].apply(lambda x: \" \".join(x for x in x.split() if x not in freq))\n    print(df['tweet'].head())","8e3a5c29":"rare_words_removal(train)","1c05fbe1":"stemmer=PorterStemmer()","8304488a":"corpus=[]\nfor i in range(len(train)):\n    #replacing everything other than alphabets with a space\n    review=re.sub(\"[^a-zA-Z]\",\" \",str(train[\"tweet\"][i]))\n    #Lowering the tweets\n    review=review.lower()\n    #Converting in a list\n    review=review.split()\n    #Finding and removing stopwords\n    review=[stemmer.stem(word) for word in review if not word in set(stopwords.words(\"english\"))]\n    #Joining after removal of stopwords\n    review=\" \".join(review)\n    corpus.append(review)","7a3eb826":"corpus","82e03214":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfv=TfidfVectorizer()\nX=tfv.fit_transform(corpus).toarray()\ny=train[\"label\"]","980a9013":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=0)","feb24587":"X_train.shape,y_train.shape","861d87be":"X_test.shape,y_test.shape","1487a8c3":"from sklearn.naive_bayes import MultinomialNB\nclassifier=MultinomialNB()\nclassifier.fit(X_train,y_train)","07ab1f18":"y_pred=classifier.predict(X_test)","11124279":"from sklearn.metrics import confusion_matrix,accuracy_score\nconfusion_matrix(y_pred,y_test)","6521bef9":"acc_MNB=accuracy_score(y_pred,y_test)\nacc_MNB","06f3bc00":"#taking dictionary size 5000\nvocab_size = 5000\n\n#one hot encoding\none_hot_dir = [one_hot(words,vocab_size) for words in corpus]\n\n#length of all rows should be equal therefore applying padding\n#this will adjust size by adding 0 at staring of the shorter rows\nembedded_layer = pad_sequences(one_hot_dir,padding = 'pre')\nembedded_layer","1a064f75":"#converting into numpy arrays.\nX_lstm = np.array(embedded_layer)\ny_lstm = np.array(y)","6905db26":"from sklearn.model_selection import train_test_split\nX_train_lstm,X_test_lstm,y_train_lstm,y_test_lstm=train_test_split(X_lstm,y_lstm,test_size=0.2,random_state=0)","9c6f9a75":"model = Sequential()\nmodel.add(Embedding(vocab_size,64,input_length = len(X_lstm[0])))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(128))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation=\"sigmoid\"))\nmodel.compile(loss=\"binary_crossentropy\",optimizer=\"adam\",metrics=[\"accuracy\"])\nmodel.summary()","49692b6f":"model.fit(X_train_lstm, y_train_lstm, validation_data = (X_test_lstm,y_test_lstm), epochs = 5, batch_size = 32)","5e1fb567":"y_pred_lstm = model.predict(X_test_lstm)\ny_pred_lstm = (y_pred_lstm > 0.5)","d0e011fa":"#confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconfusion_matrix(y_test_lstm,y_pred_lstm)","2b2476f1":"#checking accuracy\nfrom sklearn.metrics import accuracy_score\nacc_lstm=accuracy_score(y_test_lstm,y_pred_lstm)\nacc_lstm","1f4788e6":"z=[acc_MNB,acc_lstm]","391de786":"sns.countplot(z)","b17bbdc6":"# Import Dataset","46a19b8a":"# Removing the rare words","f6fe9706":"# Checking Accuracies and Creating Confusion Matrix","e9ea28dd":"# Stemming and Removing Stopwords","963c4ada":"![image.png](attachment:80243c10-d2d3-4af5-be43-f2aea2d768eb.png)","7546cd22":"# Predicting using LSTM ","de2c8bdc":"# Training the LSTM model","7ce58ec9":"# Creating TFIDF","e34f801f":"# Data Preprocessing","e243b761":"# Predicting results","88991581":"# Training the LSTM model","70fa68f1":"# Creating Confusion Matrix and Checking Accuracy","8b443da5":"# Splitting the Dataset into Train and Test set","4c780a2d":"# Removing the most frequent words","61e053a8":"## Number of stopwords in each tweet","b049e0a4":"# Removal of Punctuation Marks","84302cba":"# Splitting the Dataset into training and test set","1f72ecb3":"# Training the Naive Bayes model"}}