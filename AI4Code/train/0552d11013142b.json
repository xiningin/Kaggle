{"cell_type":{"881e01d4":"code","de6c9d3e":"code","d47ed769":"code","3d2749af":"code","3385af47":"code","2b584d1d":"code","f748eadb":"code","37a603d8":"code","99e4fca7":"code","9080e3af":"code","ce343eaa":"code","e9a81397":"code","7b184d8f":"code","acbbe285":"code","40727bba":"code","6c9d856a":"code","925b15d8":"code","a519b5d3":"code","c7121b52":"code","1d6e8738":"code","5a8bfe1a":"code","c48c59a8":"code","a327d6e7":"code","300f6d0b":"code","3fbbe40e":"code","1b2e0813":"code","676ec1ac":"code","86c572d7":"code","f7fb2290":"code","0c27a0a6":"code","be786589":"code","2adb1ea9":"code","bc896fa1":"code","42ba19cf":"code","05ea8f0d":"code","40f4078c":"code","770f28e1":"code","1e26cac3":"code","451fcbe2":"code","ff7a7c11":"code","333bbb77":"code","ae3c2247":"code","e9c9fcd6":"code","2dfeff6a":"code","57dca5c1":"code","73aa663d":"code","7fe82329":"markdown","c837127b":"markdown","1ba70fd0":"markdown","8a313e18":"markdown","18eccf33":"markdown","a5b567c3":"markdown","df666f08":"markdown","e71265e3":"markdown","95fa9ecb":"markdown","1e7e7894":"markdown","677a5822":"markdown","246fa366":"markdown","d2359263":"markdown","2ba61dfb":"markdown","7757e03c":"markdown","fa035f0a":"markdown","241ba1fe":"markdown","916af92b":"markdown","80c7834f":"markdown","34fcc4f3":"markdown","924995b4":"markdown","4047ba9f":"markdown","5aec0245":"markdown","0534cc63":"markdown"},"source":{"881e01d4":"import sys, os\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.simplefilter(action='ignore')\nimport seaborn as sns\nimport pandas as pd\nfrom datetime import datetime\nimport tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, SimpleRNN, LSTM, Activation, Dropout,RNN\nimport math\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nplotsize = (12,5)","de6c9d3e":"base  = pd.read_csv(\"..\/input\/covid19-in-india\/covid_19_india.csv\")","d47ed769":"base","3d2749af":"base.describe()","3385af47":"import pandas_profiling as pp\nprofile = pp.ProfileReport(base)\nprofile.to_file(\"output.html\")","2b584d1d":"profile","f748eadb":"base['Date'] = pd.to_datetime(base['Date'])","37a603d8":"data = base.groupby(by=['Date']).sum().diff()","99e4fca7":"data","9080e3af":"data.fillna(0,inplace=True)\ndata.rename(columns={\"Confirmed\":\"Cases\"},inplace=True)","ce343eaa":"data","e9a81397":"figure, axes = plt.subplots(3,sharex=True)\ndata['Cases'].plot(ax=axes[0],title='Cases',figsize=plotsize)\ndata['Deaths'].plot(ax=axes[1],title='Deaths',figsize=plotsize)\ndata['Cured'].plot(ax=axes[2],title='Cured',figsize=plotsize)","7b184d8f":"cases_weekly = data['Cases'].resample('W').sum()\ncases_weekly.plot(title='Weekly cases')","acbbe285":"cases_monthly = data['Cases'].resample('M').sum()\ncases_monthly.plot(title='Monthly cases')","40727bba":"def get_n_last_days(df, series_name, n_days):\n\n    return df[series_name][-(n_days):] \n\ndef plot_n_last_days(df, series_name, n_days):\n\n    plt.figure(figsize = (10,5))   \n    plt.plot(get_n_last_days(df, series_name, n_days), 'k-')\n    plt.title('{0} - {1} days'\n              .format(series_name, n_days))\n    plt.xlabel('Recorded day')\n    plt.ylabel('Reading')\n    plt.grid(alpha=0.3)","6c9d856a":"plot_n_last_days(data,'Cases',200)","925b15d8":"def get_keras_format_series(series):\n\n    series = np.array(series)\n    return series.reshape(series.shape[0],series.shape[1],1)\n\n\n\ndef get_train_test_data(df, series_name, series_days, input_hours, \n                        test_hours, sample_gap=3):\n\n    forecast_series = get_n_last_days(df, series_name, series_days).values # reducing our forecast series to last n days\n\n    train = forecast_series[:-test_hours] # training data is remaining days until amount of test_hours\n    test = forecast_series[-test_hours:] # test data is the remaining test_hours\n\n    train_X, train_y = [], []\n\n    # range 0 through # of train samples - input_hours by sample_gap. \n    # This is to create many samples with corresponding\n    for i in range(0, train.shape[0]-input_hours, sample_gap): \n        train_X.append(train[i:i+input_hours]) # each training sample is of length input hours\n        train_y.append(train[i+input_hours]) # each y is just the next step after training sample\n\n    train_X = get_keras_format_series(train_X) # format our new training set to keras format\n    train_y = np.array(train_y) # make sure y is an array to work properly with keras\n    \n    # The set that we had held out for testing (must be same length as original train input)\n    test_X_init = test[:input_hours] \n    test_y = test[input_hours:] # test_y is remaining values from test set\n    \n    return train_X, test_X_init, train_y, test_y","a519b5d3":"series_days = 600\ninput_days = 5\ntest_days = 10\n\ntrain_X, test_X_init, train_y, test_y = \\\n    (get_train_test_data(data, 'Cases', series_days, \n                         input_days, test_days))","c7121b52":"print('Training input shape: {}'.format(train_X.shape))\nprint('Training output shape: {}'.format(train_y.shape))\nprint('Test input shape: {}'.format(test_X_init.shape))\nprint('Test output shape: {}'.format(test_y.shape))","1d6e8738":"def fit_LSTM(X_train, y_train, epochs):\n    \n    # initialize model\n    regressor = Sequential()\n\n    # Adding the first LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True, input_shape = (X_train.shape[1], 1)))\n    regressor.add(Dropout(0.2))\n\n    # Adding a second LSTM layer nd some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True))\n    regressor.add(Dropout(0.2))\n\n    # Adding a third LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45, return_sequences = True))\n    regressor.add(Dropout(0.2))\n\n    # Adding a fourth LSTM layer and some Dropout regularisation\n    regressor.add(LSTM(units = 45))\n    regressor.add(Dropout(0.2))\n\n    # Adding the output layer\n    regressor.add(Dense(units = 1))\n    # define the loss function \/ optimization strategy, and fit\n    # the model with the desired number of passes over the data (epochs) \n    regressor.compile(loss='mean_squared_error', optimizer='adam')\n    regressor.fit(train_X, train_y, epochs=epochs, batch_size=64, verbose=1)\n    \n    return regressor","5a8bfe1a":"model1 = fit_LSTM(train_X, train_y, epochs=1000)","c48c59a8":"def mse(observations, estimates):\n\n    # check arg types\n    assert type(observations) == type(np.array([])), \"'observations' must be a numpy array\"\n    assert type(estimates) == type(np.array([])), \"'estimates' must be a numpy array\"\n    # check length of arrays equal\n    assert len(observations) == len(estimates), \"Arrays must be of equal length\"\n    \n    # calculations\n    difference = observations - estimates\n    sq_diff = difference ** 2\n    mse = sum(sq_diff)\n    \n    return mse","a327d6e7":"def predict(X_init, n_steps, model):\n\n    \n    X_init = X_init.copy().reshape(1,-1,1)\n    preds = []\n    \n    # iteratively take current input sequence, generate next step pred,\n    # and shift input sequence forward by a step (to end with latest pred).\n    # collect preds as we go.\n    for _ in range(n_steps):\n        pred = model.predict(X_init)\n        preds.append(pred)\n        X_init[:,:-1,:] = X_init[:,1:,:]\n        X_init[:,-1,:] = pred \n    \n    preds = np.array(preds).reshape(-1,1)\n    \n    return preds\n\ndef predict_and_plot(X_init, y, model, title):\n\n    y_preds = predict(test_X_init, n_steps=len(y), model=model) # predict through length of y\n    # Below ranges are to set x-axes\n    start_range = range(1, test_X_init.shape[0]+1) #starting at one through to length of test_X_init to plot X_init\n    predict_range = range(test_X_init.shape[0], test_days)  #predict range is going to be from end of X_init to length of test_hours\n    \n    #using our ranges we plot X_init\n    plt.plot(start_range, test_X_init)\n    #and test and actual preds\n    plt.plot(predict_range, test_y, color='orange')\n    plt.plot(predict_range, y_preds, color='teal', linestyle='--')\n    \n    plt.title(title)\n    plt.legend(['Initial Series','Target Series','Predictions'])\n    print(y_preds)\n    print(\"MSE:{}\".format(np.mean(mse(y,y_preds))))","300f6d0b":"predict_and_plot(test_X_init, test_y, model1,\n                 'Test Data and LSTM Predictions')","3fbbe40e":"from statsmodels.tsa.seasonal import seasonal_decompose\ndata.drop(columns=['Cured','Deaths'],inplace=True)\ndata.columns = ['ds', 'y']\nss_decomposition = seasonal_decompose(x=data['y'], model='additive',freq=7)\nestimated_trend = ss_decomposition.trend\nestimated_seasonal = ss_decomposition.seasonal\nestimated_residual = ss_decomposition.resid","1b2e0813":"fig, axes = plt.subplots(4, 1)\nfig.set_figheight(10)\nfig.set_figwidth(15)\n\naxes[0].plot(data['y'], label='Original')\naxes[0].legend(loc='upper left');\n\naxes[1].plot(estimated_trend, label='Trend')\naxes[1].legend(loc='upper left');\n\naxes[2].plot(estimated_seasonal, label='Seasonality')\naxes[2].legend(loc='upper left');\n\naxes[3].plot(estimated_residual, label='Residuals')\naxes[3].legend(loc='upper left');","676ec1ac":"from statsmodels.graphics.tsaplots import plot_acf\nplot_acf(data['y'])","86c572d7":"def run_sequence_plot(x, y, title, xlabel=\"time\", ylabel=\"series\"):\n    plt.plot(x, y, 'k-')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.grid(alpha=0.3);","f7fb2290":"chunks = np.split(data['y'], indices_or_sections=7)\nprint(\"{} | {:7} | {}\".format(\"Chunk\", \"Mean\", \"Variance\"))\nprint(\"-\" * 26)\nfor i, chunk in enumerate(chunks, 1):\n    print(\"{:5} | {:.6} | {:.6}\".format(i, np.mean(chunk), np.var(chunk)))","0c27a0a6":"pd.Series(data['y']).hist();","be786589":"from statsmodels.tsa.stattools import adfuller\nadf, pvalue, usedlag, nobs, critical_values, icbest = adfuller(data['y'])","2adb1ea9":"print(\"ADF:{}\".format(adf))\nprint(\"Pvalue:{}\".format(pvalue))","bc896fa1":"print(estimated_residual)","42ba19cf":"adf_after, pvalue_after, usedlag_, nobs_, critical_values_, icbest_ = adfuller(estimated_residual[3:-3])\nprint(\"ADF: \", adf_after)\nprint(\"p-value: \", pvalue_after)","05ea8f0d":"new_hetero = data['y'] + 38\nrun_sequence_plot(data.index, new_hetero,\n                  title=\"Nonstationary Data w\/Heteroscedasticity\")","40f4078c":"log_new_hetero = np.log(new_hetero)\nrun_sequence_plot(data.index, log_new_hetero,\n                  title=\"Nonstationary Data w\/Heteroscedasticity\")","770f28e1":"df_diff = data['y'].diff()\ndf_diff","1e26cac3":"run_sequence_plot(data.index,df_diff,\n                  title=\"dataset(differenced)\")","451fcbe2":"train = np.array(data['y'][1:-30])\ntest = np.array(data['y'][-30:])","ff7a7c11":"from statsmodels.tsa.api import SimpleExpSmoothing\n\nsingle = SimpleExpSmoothing(train).fit(optimized=True)\nsingle_preds = single.forecast(len(test))\nsingle_mse = mse(test, single_preds)\nprint(\"Predictions: \", single_preds)\nprint(\"MSE: \", single_mse)","333bbb77":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], single_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Simple Exponential Smoothing\")\nplt.grid(alpha=0.3);","ae3c2247":"from statsmodels.tsa.api import Holt\n\ndouble = Holt(train).fit(optimized=True)\ndouble_preds = double.forecast(len(test))\ndouble_mse = mse(test, double_preds)\nprint(\"Predictions: \", double_preds)\nprint(\"MSE: \", double_mse)","e9c9fcd6":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], double_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Double Exponential Smoothing\")\nplt.grid(alpha=0.3);","2dfeff6a":"from statsmodels.tsa.api import ExponentialSmoothing\n\ntriple = ExponentialSmoothing(train,\n                              trend=\"additive\",\n                              seasonal=\"additive\",\n                              seasonal_periods=13).fit(optimized=True)\ntriple_preds = triple.forecast(len(test))\ntriple_mse = mse(test, triple_preds)\nprint(\"Predictions: \", triple_preds)\nprint(\"MSE: \", triple_mse)","57dca5c1":"plt.plot(data.index[1:-30], train, 'b--', label=\"train\")\nplt.plot(data.index[-30:], test, color='orange', linestyle=\"--\", label=\"test\")\nplt.plot(data.index[-30:], triple_preds, 'r--', label=\"predictions\")\nplt.legend(loc='upper left')\nplt.title(\"Triple Exponential Smoothing\")\nplt.grid(alpha=0.3);","73aa663d":"print(\"Single MSE :{}\".format(single_mse))\nprint(\"Double MSE :{}\".format(double_mse))\nprint(\"Triple MSE :{}\".format(triple_mse))","7fe82329":"Any time series has 3 components associated with it:\n1. Trend\n2. Seasonality\n3. Residual","c837127b":"### Dividing the dataset into chunks to analyze data in specific time periods","1ba70fd0":"### Augmented Dickey Fuller test (ADF Test) is a common statistical test used to test whether a given Time series is stationary or not. ","8a313e18":"Analysing Number of cases","18eccf33":"# Resampling number of cases by:\n1. Weekly data\n2. Monthly data","a5b567c3":"Grouping data by 'Date' to find cumulative sum of cases in India.","df666f08":"Converting date column to a 'Datetime' object.","e71265e3":"# Decomposing the Time series.","95fa9ecb":"Removing 'NAN' values and replacing with 0.","1e7e7894":"We will be making predictions for 30 days","677a5822":"p-value obtained is greater than significance level. Hence we cannot reject the null hypothesis. Therefore, We conclude the Time series is non-stationary.","246fa366":"# Plotting time series of 3 Variables.\n1. Cases\n2. Deaths\n3. Cured","d2359263":"# Making predictions.\n\nFunctions used\n1. predict :  Given an input series matching the model's expected format generates model's predictions for next n_steps in the series.\n\n2. predict_and_plot: Given an input series matching the model's expected format generates model's predictions for next n_steps in the series, and plots these predictions against the ground truth for those steps \n    \n    arguments\n\n    X_init (array): initial sequence, must match model's input shape.\n\n    y (array): true sequence values to predict, follow X_init.\n\n    model (keras.models.Sequential): trained neural network.\n\n    title (string): plot title.  ","2ba61dfb":"The deep learning model fails to learn due to small number of training instances.","7757e03c":"## Please Upvote if you appreciate the work. It would be really Helpful :)","fa035f0a":"### Comparing the results of the 3 statistical models.","241ba1fe":"# Some more helper functions\n1. get_keras_format_series :  Convert a series to a numpy array of shape \n    [n_samples, time_steps, features]\n\n\n\n\n2. get_train_test_data : Utility processing function that splits an hourly time series into train and test with keras-friendly format, according to user-specified choice of shape.  \n    \n    arguments\n    ---------\n    df (dataframe): dataframe with time series columns.\n\n    series_name (string): column name in df.\n\n    series_days (int): total days to extract.\n\n    input_days (int): length of sequence input to network.\n\n    test_days (int): length of held-out terminal sequence.\n    \n    sample_gap (int): step size between start of train sequences; default 5\n    \n    returns\n    ---------\n    tuple: train_X, test_X_init, train_y, test_y     ","916af92b":"# Making sense of Data using Pandas Profiling","80c7834f":"# Setting up helper functions for forecasting\n\n1. get_n_last_days : Extract last n_days of a time series.\n2. plot_n_last_days : Plot last n_days of a time series","34fcc4f3":"# Defining model architecture\n\n1. LSTM\n\n    Fit LSTM to data train_X, train_y .\n    \n    arguments\n\n    train_X (array): input sequence samples for training.\n\n    train_y (list): next step in sequence targets.\n\n    cell_units (int): number of hidden units for LSTM cells.\n\n    epochs (int): number of training epochs   \n   \n","924995b4":"### Plotting Auto-Corellation function","4047ba9f":"# Importing necessary libraries","5aec0245":"# Reading CSV file having Covid-19 records in India","0534cc63":"### Dividing Time series into Train and test for predictions."}}