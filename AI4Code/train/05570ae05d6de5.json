{"cell_type":{"26269e65":"code","0c2a7032":"code","bfb506e6":"code","8b5d2cbc":"code","49762520":"code","e1ec472f":"code","8f6c16ca":"code","695828bd":"code","066dc46a":"code","ac95341d":"code","048fd953":"code","73bf4a5a":"code","fbd8bebe":"code","2b9762f6":"markdown","219bde34":"markdown","d53317d7":"markdown","5f2b457a":"markdown","888dfa26":"markdown","275a1e44":"markdown"},"source":{"26269e65":"! pip install -q torch==1.10.0+cu111 torchvision==0.11.1+cu111 torchaudio==0.10.0+cu111 -f https:\/\/download.pytorch.org\/whl\/cu111\/torch_stable.html\n! pip install -qU torchtext wandb\n! pip install -q pytorch-lightning==1.4.9\n! pip install -q \"git+https:\/\/github.com\/PyTorchLightning\/lightning-flash.git#egg=lightning-flash[text]\"","0c2a7032":"! nvidia-smi\n! mkdir \/kaggle\/temp\n\n%matplotlib inline\n%reload_ext autoreload\n%autoreload 2","bfb506e6":"import os\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport pytorch_lightning as pl\nimport flash\nimport wandb\n\nfrom dataclasses import asdict, dataclass\n\nfrom flash import Trainer\nfrom flash.text import QuestionAnsweringData, QuestionAnsweringTask","8b5d2cbc":"# Setup and login into wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nWANDB_API_KEY = user_secrets.get_secret(\"WANDB_API_KEY\")\n\n! wandb login $WANDB_API_KEY","49762520":"@dataclass\nclass HyperParams:\n    seed: int = 42\n    \n    train_val_split: float = 0.1\n    batch_size: int = 2\n    \n    backbone: str = \"xlm-roberta-base\"\n    \n    ## Optimizer and Scheduler specific\n    optimizer = 'adamw'\n    learning_rate = 1e-5\n\n    # Training\/Finetuning args\n    debug: bool = False\n    num_gpus: int = torch.cuda.device_count()\n    accumulate_grad_batches: int = 2\n    max_epochs: int = 5\n    finetuning_strategy: str= (\"freeze_unfreeze\", 2)\n\nHYPER_PARAMS = HyperParams()\npl.seed_everything(HYPER_PARAMS.seed)","e1ec472f":"INPUT_DIR = \"\/kaggle\/input\/chaii-hindi-and-tamil-question-answering\"\nTEMP_PATH = \".\/\"\nINPUT_DATA_PATH = os.path.join(INPUT_DIR, \"train.csv\")\nTRAIN_DATA_PATH = os.path.join(TEMP_PATH, \"_train.csv\")\nVAL_DATA_PATH = os.path.join(TEMP_PATH, \"_val.csv\")\nPREDICT_DATA_PATH = os.path.join(INPUT_DIR, \"test.csv\")\n\n# Display a small portion of the dataset\ndf = pd.read_csv(INPUT_DATA_PATH)\ndisplay(df.head())","8f6c16ca":"fraction = 1 - HYPER_PARAMS.train_val_split\n\n# Splitting data into train and val beforehand since preprocessing will be different for datasets.\ntamil_examples = df[df[\"language\"] == \"tamil\"]\ntrain_split_tamil = tamil_examples.sample(frac=fraction,random_state=200)\nval_split_tamil = tamil_examples.drop(train_split_tamil.index)\n\nhindi_examples = df[df[\"language\"] == \"hindi\"]\ntrain_split_hindi = hindi_examples.sample(frac=fraction,random_state=200)\nval_split_hindi = hindi_examples.drop(train_split_hindi.index)\n\ntrain_split = pd.concat([train_split_tamil, train_split_hindi]).reset_index(drop=True)\nval_split = pd.concat([val_split_tamil, val_split_hindi]).reset_index(drop=True)\n\ntrain_split.to_csv(TRAIN_DATA_PATH, index=False)\nval_split.to_csv(VAL_DATA_PATH, index=False)","695828bd":"datamodule = QuestionAnsweringData.from_csv(\n    train_file=TRAIN_DATA_PATH,\n    val_file=VAL_DATA_PATH,\n    batch_size=HYPER_PARAMS.batch_size,\n    backbone=HYPER_PARAMS.backbone\n)","066dc46a":"model = QuestionAnsweringTask(\n    backbone=HYPER_PARAMS.backbone,\n    learning_rate=HYPER_PARAMS.learning_rate,\n    optimizer=HYPER_PARAMS.optimizer,\n)","ac95341d":"callbacks = [\n    pl.callbacks.ModelCheckpoint(\n        monitor='rouge2_fmeasure',\n        save_top_k=1,\n        filename='checkpoint\/{epoch:02d}-{rouge2_fmeasure:.4f}',\n        mode='max',\n    ),\n]\nwandb_logger = pl.loggers.WandbLogger(\n    project='chaii-competition',\n    config=asdict(HYPER_PARAMS),\n    group='XLM Roberta', \n    job_type='finetune',\n    log_model=False,\n)","048fd953":"trainer = Trainer(\n    logger=wandb_logger,\n    callbacks=callbacks,\n    gpus=HYPER_PARAMS.num_gpus,\n    max_epochs=HYPER_PARAMS.max_epochs,\n    accumulate_grad_batches=HYPER_PARAMS.accumulate_grad_batches,\n)\n\nwandb_logger.watch(model)\ntrainer.finetune(model, datamodule, strategy=HYPER_PARAMS.finetuning_strategy)\nwandb.finish()","73bf4a5a":"# Convert the prediction queries to dictionary format.\npredict_data = pd.read_csv(PREDICT_DATA_PATH)\npredict_data = predict_data[predict_data.columns[:3]].to_dict(orient=\"list\")\n\n# Answer some Questions!\npredictions = model.predict(predict_data)\nprint(predictions)","fbd8bebe":"# Create submission.\nsubmission = {\"id\": [], \"PredictionString\": []}\nfor prediction in predictions:\n    submission[\"id\"].extend(prediction.keys())\n    submission[\"PredictionString\"].extend(prediction.values())\nsubmission = pd.DataFrame(submission)\nsubmission.to_csv(\".\/submission.csv\", index=False)","2b9762f6":"# Install the required dependencies","219bde34":"## 4. Predictions","d53317d7":"## 3. Create the trainer and finetune the model","5f2b457a":"## 1. Create the DataModule","888dfa26":"## 2. Build the task","275a1e44":"# Hindi and Tamil Question Answering with Lightning Flash"}}