{"cell_type":{"83c447a4":"code","c4ab4494":"code","c30eb95c":"code","793706a7":"code","ab78cf56":"code","f7e27340":"code","589dbbdd":"code","cdad0b5c":"code","73d6e462":"code","1ebcfc95":"code","61c8e440":"code","143f491f":"code","167aa75b":"code","7b5c1178":"code","40f20d2d":"code","d823906e":"markdown","461b2e1a":"markdown"},"source":{"83c447a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c4ab4494":"import tensorflow as tf\nimport tensorflow.keras.datasets.boston_housing as dataset\n","c30eb95c":"from  sklearn.model_selection import train_test_split","793706a7":"(X_train,y_train),(X_test,y_test) = dataset.load_data(test_split=0.3, seed=42)\nX_train,X_valid,y_train,y_valid = train_test_split(X_train,y_train, test_size=0.1, random_state=42)","ab78cf56":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","f7e27340":"filenames=['Training_data.tfrecord','Validation_data.tfrecord','Testing_data.tfrecord']","589dbbdd":"train_data = np.c_[X_train, y_train]\nvalidation_data = np.c_[X_valid, y_valid]\ntest_data = np.c_[X_test, y_test]","cdad0b5c":"train_data.shape","73d6e462":"def _float_feature(value):\n  \"\"\"Returns a float_list from a float \/ double.\"\"\"\n  return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))","1ebcfc95":"with tf.io.TFRecordWriter(filenames[0]) as train_record:\n    for sample in train_data:\n        features = dict()\n        for i in range(train_data.shape[1]-1):\n            features[\"feature\"+str(i)] = _float_feature(sample[i])\n        features[\"target\"] = _float_feature(sample[-1])\n        example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n        train_record.write(example_proto.SerializeToString())\n","61c8e440":"with tf.io.TFRecordWriter(filenames[1]) as validation_record:\n    for sample in validation_data:\n        features = dict()\n        for i in range(validation_data.shape[1]-1):\n            features[\"feature\"+str(i)] = _float_feature(sample[i])\n        features[\"target\"] = _float_feature(sample[-1])\n        example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n        validation_record.write(example_proto.SerializeToString())","143f491f":"with tf.io.TFRecordWriter(filenames[1]) as test_record:\n    for sample in test_data:\n        features = dict()\n        for i in range(test_data.shape[1]-1):\n            features[\"feature\"+str(i)] = _float_feature(sample[i])\n        features[\"target\"] = _float_feature(sample[-1])\n        example_proto = tf.train.Example(features=tf.train.Features(feature=features))\n        test_record.write(example_proto.SerializeToString())","167aa75b":"def decode_fn(record_bytes):\n    schema = dict()\n    for i in range(13):\n        schema[\"feature\"+str(i)] = tf.io.FixedLenFeature([], dtype=tf.float32)\n    schema[\"target\"] = tf.io.FixedLenFeature([], dtype=tf.float32)\n    return tf.io.parse_single_example(record_bytes,schema)\n","7b5c1178":"train_dataset = tf.data.TFRecordDataset([filenames[0]]).map(decode_fn)\nvalidation_dataset = tf.data.TFRecordDataset([filenames[1]]).map(decode_fn)\ntest_dataset = tf.data.TFRecordDataset([filenames[2]]).map(decode_fn)","40f20d2d":"for element in train_dataset:\n    print(element['target'])","d823906e":"### s\u00e9parer les donn\u00e9es:\n60% d'entrainement\n10% de validation\n30% de test","461b2e1a":"## Importation des donn\u00e9es"}}