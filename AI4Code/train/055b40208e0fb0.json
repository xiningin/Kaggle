{"cell_type":{"55bef6dd":"code","a702f2de":"code","a3972bd4":"code","2e9f296d":"code","e22fb0eb":"code","aff3435a":"code","549c2afa":"code","401995f6":"code","13220d4a":"code","bbbced63":"code","ef308d20":"code","1cbdc70b":"code","a4627e6a":"code","5ad5bfe9":"code","4259343b":"code","af2fa8e1":"code","7bc0fda5":"code","9e77fca8":"code","77bb56c2":"code","67ce72da":"code","c13ba0d0":"code","fb14f0b1":"code","eb3d0c06":"code","3f56f201":"code","2635a042":"code","9bc7bfc7":"code","50eee9ec":"code","c7f2d964":"code","cd7c4266":"code","7e199424":"markdown","ea29821c":"markdown","a7d98fac":"markdown","d0a8e235":"markdown","0985ae80":"markdown","aee47b6d":"markdown","1692f624":"markdown","03a35786":"markdown","bc3e2a39":"markdown","2f27a910":"markdown","fa04f2e7":"markdown","4d4d2b3e":"markdown","6d3242e6":"markdown","3b63bbea":"markdown","f04735b5":"markdown","5099236b":"markdown","89391dbd":"markdown","66708a37":"markdown","ac8e1634":"markdown","b1af0344":"markdown","442c5644":"markdown","479a04e8":"markdown","f0bca3e1":"markdown"},"source":{"55bef6dd":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport os\nimport tensorflow as tf\nimport re as re\n\nfrom sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import MultinomialNB \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom mlxtend.classifier import StackingCVClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.impute import SimpleImputer\nprint(\"# Imported the libaries\")","a702f2de":"dir_contents = os.listdir('..\/input\/')\nprint(\"Dataset contents : {}\".format(dir_contents))\nprint(\"Size of Training dataset : \" + str(round(os.path.getsize('..\/input\/train.csv')\/(1024*2), 2)) +\" KB\")\nprint(\"Size of Testing dataset : \" + str(round(os.path.getsize('..\/input\/test.csv')\/(1024*2), 2)) +\" KB\")","a3972bd4":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\npassengerIds = test_data['PassengerId']\nprint(\"Shape of Train dataset : {}\".format(train_data.shape))\nprint(\"Shape of Test dataset : {}\".format(test_data.shape))","2e9f296d":"train_data.head(4)","e22fb0eb":"train_data['Survived'].value_counts(normalize=True) * 100","aff3435a":"print(train_data['Age'].describe())\nprint(\"***************************************\")\n\nplt.close()\nplt.figure(figsize=(18, 5))\nplt.subplot(1, 3, 1)\nplt.title(\"Distribution Plot for Age\")\nsns.distplot(train_data[train_data['Age'].notnull()]['Age'], bins=50)\nplt.grid()  \n\nplt.subplot(1, 3, 2)\nplt.title(\"PDF-CDF Plot for Age\")\ncounts, bin_edges = np.histogram(train_data[train_data['Age'].notnull()]['Age'], bins = 50, density = True)\npdf = counts \/ sum(counts)\ncdf = np.cumsum(pdf)\nplt.plot(bin_edges[1:], pdf, label=\"PDF\")\nplt.plot(bin_edges[1:], cdf, label=\"CDF\")\nplt.legend()\nplt.grid()  \n\nplt.subplot(1, 3, 3)\nplt.title(\"Violin Plot for Age\")\nsns.violinplot(x=\"Survived\", y=\"Age\", data=train_data)\n\nplt.grid()    \nplt.show()","549c2afa":"train_data[(train_data['Age'] < 10) & (train_data['SibSp'] == 0) & (train_data['Parch'] == 0)]","401995f6":"print(\"# Gender counts\")\nprint(train_data['Sex'].value_counts())\nprint(\"*************************\")\nprint(\"Percentage of Males who survived   : {}%\".format(round(train_data[(train_data['Sex']=='male') & \n                                                               (train_data['Survived']==1)].shape[0] * 100 \n                                                           \/ train_data.shape[0], 2)))\nprint(\"Percentage of Females who survived : {}%\".format(round(train_data[(train_data['Sex']=='female') & \n                                                               (train_data['Survived']==1)].shape[0] * 100 \n                                                           \/ train_data.shape[0], 2)))","13220d4a":"full_data = [train_data , test_data]\nlen(full_data)","bbbced63":"for data in full_data:\n    print(\"**************************\")\n    print(data.info())\n    print(\"**************************\")","ef308d20":"train_data['Name']","1cbdc70b":"def getTitleFromName(nameText):\n    title = str(nameText.split(', ')[1])\n    title_search = re.search('([A-Za-z]+)\\.', title).group(1)\n    return title_search\n\nfor data in full_data:\n    imp = SimpleImputer(missing_values=np.nan, strategy='median')\n    \n    # Introducting a new feature - Family Size\n    data['FamilySize'] = data['SibSp'] + data['Parch'] + 1\n    \n    # Introducing a new feature - IsAlone\n    data['IsAlone'] = 0\n    data.loc[data['FamilySize']==1, 'IsAlone'] = 1\n        \n    # Introducing a new Feature - CabinAlloted\n    data['CabinAllotment'] = 0\n    data.loc[data['Cabin'].notnull(), 'CabinAllotment'] = 1\n    \n    # If data has null values in Embarked, just replace it with 'S' as 'S' is quite frequent\n    data['Embarked'] = data['Embarked'].fillna('S')\n    \n    # Mapping Sex\n    data['Sex'] = data['Sex'].map({'male': 0, 'female': 1}).astype(int)\n    \n    # Mapping Embarked\n    data['Embarked'] = data['Embarked'].map({'S': 0, 'C': 1, 'Q': 2}).astype(int)\n    \n    # Since Age has many null\/NA values, we will process it with Median values\n    data['Age'] = imp.fit_transform(data[['Age', 'Sex', 'FamilySize']])[:,0].astype(int)\n    \n    # Since Fare has also some null\/Na Values, we will process it with Median values\n    data['Fare'] = imp.fit_transform(data[['Fare', 'FamilySize', 'CabinAllotment']])\n    \n    # Introducing a new feature - PerTicket\n    data['PerTicket'] = data['Fare'] \/ data['FamilySize']\n    data.loc[ data['PerTicket'] <= 7.25, 'PerTicket'] = 0\n    data.loc[(data['PerTicket'] > 7.25) & (data['PerTicket'] <= 8.3), 'PerTicket'] = 1\n    data.loc[(data['PerTicket'] > 8.3) & (data['PerTicket'] <= 23.667), 'PerTicket'] = 2\n    data.loc[ data['PerTicket'] > 23.667, 'PerTicket'] = 3\n    data['PerTicket'] = data['PerTicket'].astype(int)\n    \n    # Mapping the Age Values to Categories (Children, Youth, Adults, Senior)\n    data.loc[(data['Age'] >=0) & (data['Age'] <= 14), 'Age'] = 0       #Children\n    data.loc[(data['Age'] >=15) & (data['Age'] <= 24), 'Age'] = 1      #Youth\n    data.loc[(data['Age'] >=25) & (data['Age'] <= 64), 'Age'] = 2      #Adults\n    data.loc[data['Age'] >=65, 'Age'] = 3    #Senior\n    data['Age'] = data['Age'].astype(int)\n    \n    #Name Feature Engineering\n    data['Title'] = data['Name'].apply(getTitleFromName)\n    data['Title'] = data['Title'].replace('Ms', 'Miss')\n    data['Title'] = data['Title'].replace('Mlle', 'Miss')\n    data['Title'] = data['Title'].replace('Mme', 'Mrs')\n    \n    data['Status'] = \"General\"\n    data.loc[data['Title'] == 'Capt','Status'] = 'Military'\n    data.loc[data['Title'] == 'Col','Status'] = 'Military'\n    data.loc[data['Title'] == 'Countess','Status'] = 'Political'\n    data.loc[data['Title'] == 'Don','Status'] = 'Military'\n    data.loc[data['Title'] == 'Dr','Status'] = 'General'\n    data.loc[data['Title'] == 'Jonkheer','Status'] = 'Political'\n    data.loc[data['Title'] == 'Lady','Status'] = 'Political'\n    data.loc[data['Title'] == 'Major','Status'] = 'Military'\n    data.loc[data['Title'] == 'Master','Status'] = 'General'\n    data.loc[data['Title'] == 'Rev','Status'] = 'Political'\n    data.loc[data['Title'] == 'Sir','Status'] = 'Military'\n    \n    data['Rank'] = 0\n    data.loc[data['Title'] == 'Capt', 'Rank'] = 1\n    data.loc[data['Title'] == 'Col', 'Rank'] = 1\n    data.loc[data['Title'] == 'Major', 'Rank'] = 2\n    data.loc[data['Title'] == 'Don', 'Rank'] = 2\n    data.loc[data['Title'] == 'Sir', 'Rank'] = 0\n    data.loc[data['Title'] == 'Dr', 'Rank'] = 1\n    data.loc[data['Title'] == 'Master', 'Rank'] = 0\n    data.loc[data['Title'] == 'Miss', 'Rank'] = 0\n    data.loc[data['Title'] == 'Mr', 'Rank'] = 0\n    data.loc[data['Title'] == 'Mrs', 'Rank'] = 0\n    data.loc[data['Title'] == 'Countess', 'Rank'] = 2\n    data.loc[data['Title'] == 'Jonkheer', 'Rank'] = 0\n    data.loc[data['Title'] == 'Lady', 'Rank'] = 1\n    data.loc[data['Title'] == 'Rev', 'Rank'] = 1\n    \n    data['Status'] = data['Status'].map({ 'General': 0, 'Military': 1, 'Political': 2})\n\n# Feature Selection\nremove_features = ['PassengerId', 'Ticket', 'Cabin', 'SibSp', 'Parch', 'Name', 'Title', 'Fare']\ntrain_data = train_data.drop(remove_features, axis=1)\ntest_data = test_data.drop(remove_features, axis=1)\n\ntrain_data.head(5)","a4627e6a":"y_train = train_data['Survived'].values\nX_train = train_data.drop('Survived', axis=1)\nX_test = test_data.values","5ad5bfe9":"encoding_clf = OneHotEncoder()\ntrain_data_new = encoding_clf.fit_transform(X_train).astype('int')\ntest_data_new = encoding_clf.transform(X_test).astype('int')","4259343b":"print(train_data_new.shape)\nprint(train_data_new.shape)","af2fa8e1":"y_train_new = tf.keras.utils.to_categorical(y_train, num_classes=2)","7bc0fda5":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train_data.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","9e77fca8":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Dense(35, input_shape=(35,), activation=tf.nn.relu, kernel_initializer='he_uniform'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(100, activation=tf.nn.relu, kernel_initializer='he_uniform'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(40, activation=tf.nn.relu,kernel_initializer='he_uniform'))\nmodel.add(tf.keras.layers.BatchNormalization())\nmodel.add(tf.keras.layers.Dropout(0.5))\nmodel.add(tf.keras.layers.Dense(10, activation=tf.nn.relu,kernel_initializer='he_uniform'))\nmodel.add(tf.keras.layers.Dense(2, activation=tf.nn.softmax))\n\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nrelu_model = model.fit(train_data_new, y_train_new, epochs=500, batch_size=7, verbose=1, validation_split=0.20)","77bb56c2":"pred = model.predict_classes(test_data_new)\npred","67ce72da":"DL_Submission = pd.DataFrame({ 'PassengerId': passengerIds,\n                            'Survived': pred })\nDL_Submission.to_csv(\"DL_Submission.csv\", index=False)","c13ba0d0":"# mask = ['Pclass', 'Sex', 'Age', 'Embarked', 'FamilySize', 'IsAlone', 'CabinAllotment', 'PerTicket', 'Status', 'Rank']\n# X = train_data[mask]\n# y = train_data['Survived']","fb14f0b1":"# from sklearn.model_selection import GridSearchCV, train_test_split\n# from sklearn.calibration import CalibratedClassifierCV\n# from sklearn.metrics import roc_curve, f1_score, confusion_matrix, auc\n# g_xtrain, g_xtest, g_ytrain, g_ytest = train_test_split(X, y, test_size=0.30)","eb3d0c06":"from sklearn.linear_model import SGDClassifier\n\nalpha_range = list([10** i for i in range(-10, 6, 1)] + [2**i for i in range(-5, -1, 1)])\nloss_range = list(['hinge', 'log', 'modified_huber', 'squared_hinge'])\npenalty_range = list(['l1', 'l2', 'elasticnet'])\nparameters = {'loss': loss_range, 'penalty': penalty_range, 'alpha': alpha_range}\nmodel = SGDClassifier()\ng_clf = GridSearchCV(model, parameters, cv=10, n_jobs=-1, scoring=\"accuracy\")\ng_clf.fit(g_xtrain, g_ytrain)\n\nprint(\"Model fitted perfectly.\")\nprint(\"Best Score (TRAIN): {}\".format(g_clf.best_score_))\nprint(\"Best Params       : {}\".format(g_clf.best_params_))","3f56f201":"# optimal_alpha = g_clf.best_params_['alpha']\n# optimal_loss = g_clf.best_params_['loss']\n# optimal_penalty = g_clf.best_params_['penalty']\n\n# clf_model = SGDClassifier(alpha=optimal_alpha, loss=optimal_loss, penalty=optimal_penalty)\n# ccv_clf = CalibratedClassifierCV(clf_model, cv=10)\n# ccv_clf.fit(g_xtrain, g_ytrain)\n\n# # Get predicted values for test data\n# pred_train = ccv_clf.predict(g_xtrain)\n# pred_test = ccv_clf.predict(g_xtest)\n# pred_proba_train = ccv_clf.predict_proba(g_xtrain)[:,1]\n# pred_proba_test = ccv_clf.predict_proba(g_xtest)[:,1]\n\n# fpr_train, tpr_train, thresholds_train = roc_curve(g_ytrain, pred_proba_train, pos_label=1)\n# fpr_test, tpr_test, thresholds_test = roc_curve(g_ytest, pred_proba_test, pos_label=1)\n# conf_mat_train = confusion_matrix(g_ytrain, pred_train, labels=[0, 1])\n# conf_mat_test = confusion_matrix(g_ytest, pred_test, labels=[0, 1])\n# f1_sc = f1_score(g_ytest, pred_test, average='binary', pos_label=1)\n# auc_sc_train = auc(fpr_train, tpr_train)\n# auc_sc = auc(fpr_test, tpr_test)\n\n# print(\"Optimal Alpha: {} with Penalty: {} with AUC: {:.2f}%\".format(optimal_alpha, optimal_penalty, float(auc_sc*100)))\n\n\n\n# plt.figure(figsize=(13,7))\n# # Plot ROC curve for training set\n# plt.subplot(2, 2, 1)\n# plt.title('Receiver Operating Characteristic - TRAIN SET')\n# plt.plot(fpr_train, tpr_train, color='red', label='AUC - Train - {:.2f}'.format(float(auc_sc_train * 100)))\n# plt.plot([0, 1], ls=\"--\")\n# plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n# plt.ylabel('True Positive Rate')\n# plt.xlabel('False Positive Rate')\n# plt.grid()\n# plt.legend(loc='best')\n\n# # Plot ROC curve for test set\n# plt.subplot(2, 2, 2)\n# plt.title('Receiver Operating Characteristic - TEST SET')\n# plt.plot(fpr_test, tpr_test, color='blue', label='AUC - Test - {:.2f}'.format(float(auc_sc * 100)))\n# plt.plot([0, 1], ls=\"--\")\n# plt.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n# plt.ylabel('True Positive Rate')\n# plt.xlabel('False Positive Rate')\n# plt.grid()\n# plt.legend(loc='best')\n\n# #Plotting the confusion matrix for train\n# plt.subplot(2, 2, 3)\n# plt.title('Confusion Matrix for Training set')\n# df_cm = pd.DataFrame(conf_mat_train, index = [\"Negative\", \"Positive\"],\n#                   columns = [\"Negative\", \"Positive\"])\n# sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n\n# #Plotting the confusion matrix for test\n# plt.subplot(2, 2, 4)\n# plt.title('Confusion Matrix for Testing set')\n# df_cm = pd.DataFrame(conf_mat_test, index = [\"Negative\", \"Positive\"],\n#                   columns = [\"Negative\", \"Positive\"])\n# sns.heatmap(df_cm, annot=True,cmap='Blues', fmt='g')\n\n# plt.tight_layout()\n# plt.show()\n","2635a042":"# final_clf = SGDClassifier(alpha=optimal_alpha, loss=optimal_loss, penalty=optimal_penalty)\n# final_clf.fit(X,y)\n\n# pred = final_clf.predict(test_data)\n\n# SGDSubmission = pd.DataFrame({ 'PassengerId': passengerIds,\n#                             'Survived': pred })\n# SGDSubmission.to_csv(\"SGDSubmission.csv\", index=False)\n# os.listdir()","9bc7bfc7":"# # Initializing models\n# clf1 = SGDClassifier(n_jobs=-1)\n# clf2 = GradientBoostingClassifier(n_estimators=50)\n# clf3 = RandomForestClassifier(n_estimators=50, n_jobs=-1)\n# clf4 = AdaBoostClassifier(n_estimators=50)\n# meta_clf = XGBClassifier(n_estimators=100, n_jobs=-1)\n\n# sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4], \n#                             meta_classifier=meta_clf)\n\n# params = {\n#             'sgdclassifier__alpha': alpha_range,\n#             'sgdclassifier__loss': loss_range,\n#             'sgdclassifier__penalty': penalty_range,\n#             'randomforestclassifier__max_depth': list([5, 10, 15]),\n#             'meta-xgbclassifier__max_depth': list([3, 7, 11])\n#         }\n\n# grid = GridSearchCV(estimator=sclf, \n#                     param_grid=params, \n#                     cv=10, n_jobs=-1, scoring='accuracy',\n#                     refit=True)\n# grid.fit(X, y)\n\n# print('Best parameters: %s' % grid.best_params_)\n# print('Accuracy: %.2f' % grid.best_score_)","50eee9ec":"# # Initializing models\n# clf1 = KNeighborsClassifier()\n# clf2 = RandomForestClassifier()\n# clf3 = MultinomialNB()\n# clf4 = SVC(kernel='rbf')\n# clf5 = LogisticRegression()\n# clf6 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=1),algorithm=\"SAMME\")\n# meta_clf = XGBClassifier(n_jobs=-1)\n\n# sclf = StackingCVClassifier(classifiers=[clf1, clf2, clf3, clf4, clf5, clf6], \n#                             meta_classifier=meta_clf)\n\n# params = {\n#             'kneighborsclassifier__n_neighbors': range(1, 50, 5),\n#             'multinomialnb__alpha': [10**i for i in range(-5, 4, 1)],\n#             'svc__C': [10**i for i in range(-5, 4, 1)],\n#             'logisticregression__C': [10**i for i in range(-5, 4, 1)],\n#         }\n\n# grid = GridSearchCV(estimator=sclf, \n#                     param_grid=params, \n#                     cv=10, n_jobs=-1, scoring='accuracy',\n#                     refit=True)\n# grid.fit(X, y)\n\n# print('Best parameters: %s' % grid.best_params_)\n# print('Accuracy: %.2f' % grid.best_score_)","c7f2d964":"# #Predict from the Test Data\n# pred = grid.predict(test_data)","cd7c4266":"# StackingSubmission = pd.DataFrame({ 'PassengerId': passengerIds,\n#                             'Survived': pred })\n# StackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)\n# os.listdir()","7e199424":"**Observation**\nAround 38% of the passengers only survived. Thus it is an imbalanced dataset but not too much. If necessary, we can also try upsampling the dataset. But that's not necessary right now.","ea29821c":"One thing that that the Pearson Correlation plot can tell us is that there are not too many features strongly correlated with one another. This is good from a point of view of feeding these features into your learning model because this means that there isn't much redundant or superfluous data in our training set and we are happy that each feature carries with it some unique information. ","a7d98fac":"Lets apply SGDClassfier(With Log loss) i.e Logistic Regression.","d0a8e235":"**Observation:** Females survival rate from the catastrophe was more than that of males.","0985ae80":"I seriously do not know how this is possible. This passenger is 5 year old female and do not have a sibling or even a parent on board. This is quite strange as she survived the catastrophe.\n\nMy first guess would be this observation would be a typo\/error in reporting in Age or SibSp or Parch.","aee47b6d":"> Loading the important libraries","1692f624":"**Observation:**\n1. As we can see, The mean 'Age' of the passengers is about 30. \n2. About 90% of passengers have age less than 50 years.\n3. About 10% of passengers have age less than or equal to 10.\n4. From the violin plots we cannot infer anything as both the plots are highly overlapping. However, if we look closely, childrens\/infants which have age less than 10 have a high survival rate.","03a35786":"**This dataset contains minimal features.**\nFeatures\/Attributes - \n1. PassengerId - Id \n2. Survived - Survival Status (0 -> Died 1 -> Survived)\n3. PClass - Ticket Class\n4. Name - Name of the Passenger\n5. Sex - Gender of the Passenger\n6. Age - Age of the Passenger in years\n7. SibSp - # of siblings \/ spouses aboard\n8. Parch - # of parents \/ children aboard the Titanic\n9. Ticket - Ticket Number\n10. fare - Ticket Fare\n11. Cabin - Cabin Number\n12. Embarked - Port of Embarkation\n\nHere, the Survived Column would be the Classification Column\/Prediction Column\/Output Column","bc3e2a39":"Checking for NaN, null values in whole dataset","2f27a910":"# StackingCVClassifier\n[](http:\/\/)\n*An ensemble-learning meta-classifier for stacking using cross-validation to prepare the inputs for the level-2 classifier to prevent overfitting.*\n\n**Stacking** is an *ensemble* learning technique to combine multiple classification models via a meta-classifier. The StackingCVClassifier extends the standard stacking algorithm (implemented as StackingClassifier) using cross-validation to prepare the input data for the level-2 classifier.\n\nIn the standard stacking procedure, the first-level classifiers are fit to the same training set that is used prepare the inputs for the second-level classifier, which may lead to overfitting. The StackingCVClassifier, however, uses the concept of cross-validation: the dataset is split into k folds, and in k successive rounds, k-1 folds are used to fit the first level classifier; in each round, the first-level classifiers are then applied to the remaining 1 subset that was not used for model fitting in each iteration. The resulting predictions are then stacked and provided -- as input data -- to the second-level classifier. After the training of the StackingCVClassifier, the first-level classifiers are fit to the entire dataset as illustrated in the figure below.\n\n<img src='http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier_files\/stacking_cv_classification_overview.png'>\n\n*More formally, the Stacking Cross-Validation algorithm can be summarized as follows :*\n\n<img src='http:\/\/rasbt.github.io\/mlxtend\/user_guide\/classifier\/StackingCVClassifier_files\/stacking_cv_algorithm.png'>[](http:\/\/)","fa04f2e7":"Imported the libaries.. Now check the input files and their sizes","4d4d2b3e":"This is really a small dataset with only ~900 observations for training set and ~400 observations for testing set.","6d3242e6":"Pearson Correlation Heatmap","3b63bbea":"> Now let's do some **EXPLORATORY DATA ANALYSIS** on the dataset(Train)","f04735b5":"Titanic: Machine Learning Problem to predict the survival on the titanic","5099236b":"Pretty small dataset with size less than 30Kilobytes for train set and less than 15KB for testing set. Considering the size of dataset, it is going to be really tough to fetch appropriate and great results.\n\nNow that we have keys to open the locks of the door, let's get our hands dirty to post mortem the dataset and solve this god damn problem.","89391dbd":"Let's process **Name** attribute\/feature to fetch the Title","66708a37":"**Competition Description** : *The sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.*\n\n*One of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.*\n\n*In this challenge, we tend to complete the analysis of what sorts of people were likely to survive. In particular, we try to apply the tools of machine learning to predict which passengers survived the tragedy.*","ac8e1634":"PLEASE UPVOTE IF YOU FIND THIS KERNEL HELPFUL FOR YOUR ANALYSIS IN ANY WAY.","b1af0344":"**Objective**:\n*Predict the Survival of the onboard passengers. How much likely they can survive.*","442c5644":"**> Data Preprocessing and Feature Engineering**","479a04e8":"As we can see,\n1. Age & Cabin attributes in both dataset contains NaN\/null values to a much greater extent.\n2. Embarked attribute in Train data has 2 missing values. \n3. Fare attribute in Test data has one missing value.\n\nWe need to pre process such attributes.","f0bca3e1":"## Neural Network on Titanic Dataset"}}