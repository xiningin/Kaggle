{"cell_type":{"f4ec0437":"code","4a3aeb21":"code","013ac9b0":"code","f9bd8fbd":"code","f67de4c0":"code","97bc3d9b":"code","b5828b62":"code","f9e697f1":"code","1faccae2":"code","e1e3162b":"code","478fd700":"code","e7f0e0a8":"code","e6c2b8aa":"code","9346740e":"code","34e833a8":"markdown","d720f5fa":"markdown","605782da":"markdown","44a98159":"markdown","1e434c8c":"markdown","f572c1b8":"markdown","18308030":"markdown"},"source":{"f4ec0437":"#LOAD DATA\nimport pandas as pd\nX = pd.read_csv('..\/input\/train.csv')\n# X = X.sample(frac = 0.5, replace=False, random_state=2019)\nX.head()","4a3aeb21":"#FEATURE TRANSFORMATION\nimport pandas as pd\n\ny = X.Survived\n\nX = X.drop(['PassengerId','Survived','Name','Ticket','Cabin'], axis=1)\nX = pd.get_dummies(X)\n\nX.head()\n","013ac9b0":"# TRAIN TEST SPLIT\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_eval, y_train, y_eval = train_test_split(X, y, test_size=0.5, random_state=2019)","f9bd8fbd":"# #TRAIN MODEL - BAYESIAN SEARCH\nimport xgboost as xgb\nfrom skopt import BayesSearchCV\nfrom sklearn.model_selection import StratifiedKFold\n\nbayes_cv_tuner = BayesSearchCV(estimator = xgb.XGBClassifier(\n                                    n_jobs = -1,\n                                    objective = 'binary:logistic',\n                                    eval_metric = 'auc',\n                                    silent=1,\n                                    early_stopping = 200,\n                                    tree_method='approx'),\n                search_spaces = {\n                    'min_child_weight': (1, 50),\n                    'max_depth': (3, 10),\n                    'max_delta_step': (0, 20),\n                    'subsample': (0.01, 1.0, 'uniform'),\n                    'colsample_bytree': (0.01, 1.0, 'uniform'),\n                    'colsample_bylevel': (0.01, 1.0, 'uniform'),\n                    'reg_lambda': (1e-2, 1000, 'log-uniform'),\n                    'reg_alpha': (1e-2, 1.0, 'log-uniform'),\n                    'gamma': (1e-2, 0.5, 'log-uniform'),\n                    'min_child_weight': (0, 20),\n                    'scale_pos_weight': (1e-6, 500, 'log-uniform'),\n                    'n_estimators': (150,1000),\n                    'learning_rate':(0.01,0.08,'uniform'),\n                    'subsample':(0.01,1,'uniform'),\n                    'eta':(0.01,0.2,'uniform')\n                },    \n                scoring = 'roc_auc',\n                cv = StratifiedKFold(\n                    n_splits=5,\n                    shuffle=True,\n                    random_state=42),\n                n_jobs = -1,\n                n_iter = 10,   \n                verbose = 1,\n                refit = True,\n                random_state = 786)\n\ntunning_model = bayes_cv_tuner.fit(X_train, y_train)","f67de4c0":"# TRAINING BEST MODEL RESULTS\nprint('BEST ESTIMATOR: '+ str(tunning_model.best_estimator_))\nprint('BEST SCORE: '+ str(tunning_model.best_score_))\nprint('BEST PARAMS: '+ str(tunning_model.best_params_))","97bc3d9b":"#TEST MODEL\ny_pred = tunning_model.predict_proba(X_eval)\n\n# roc_auc score\nfrom sklearn.metrics import roc_auc_score\nprint('ROC AUC SCORE ON TESTING DATA: '+str(roc_auc_score(y_eval,y_pred[:,1])))","b5828b62":"# FIND BEST ACCURACY PROB\nimport numpy as np\nfrom sklearn.metrics import accuracy_score\n\nbest_acc = 0\nbest_prob = 0\n\nfor i in np.arange(0,1,0.01):\n    y_pred_tmp = np.where((y_pred[:,1] >= i),1,0)\n    acc_tmp = accuracy_score(y_eval, y_pred_tmp) \n    if acc_tmp > best_acc:\n        best_prob = i\n        best_acc = acc_tmp\n        \nprint('Model Best Accracy: {0} - with Prob: {1}'.format(best_acc,best_prob))\n","f9e697f1":"# TRAIN FINAL MODEL\nimport xgboost as xgb\n\nfinal_model = xgb.XGBClassifier(**tunning_model.best_params_,\n                                n_jobs = -1,\n                                objective = 'binary:logistic',\n                                eval_metric = 'auc',\n                                silent=1,\n                                early_stopping = 200,\n                                tree_method='approx')\nfinal_model.fit(X, y)","1faccae2":"#SAVE MODEL\nfrom sklearn.externals import joblib\njoblib.dump(final_model, 'model_xgboost.pkl')","e1e3162b":"# LOAD SUBMIT DATA\nX_submit = pd.read_csv('..\/input\/test.csv')\nX_submit.head()","478fd700":"# FEATURE TRANSFORMATION\nimport pandas as pd\n\nPassengerId = X_submit.PassengerId\nX_submit = X_submit.drop(['PassengerId','Name','Ticket','Cabin'], axis=1)\nX_submit = pd.get_dummies(X_submit)\n\nX_submit.head()","e7f0e0a8":"# PREDICT\ny_submit = final_model.predict_proba(X_submit)","e6c2b8aa":"# CONVERT PROBABILITIES PREDICTED TO Survived = 0 \/ 1\nimport numpy as np\ny_submit_final = np.where((y_submit[:,1] >= best_prob),1,0)","9346740e":"#SAVE SUBMISSION\nsubmission = pd.DataFrame({'PassengerId':PassengerId,\n                           'Survived':y_submit_final})\nsubmission.to_csv('submission_xgboost.csv',index=False)","34e833a8":"## Create submission","d720f5fa":"### Train final model\n\nWe will use the hyperparameters found + 100% of the data.","605782da":"### Bayesian Search for finding the best Hyperparameters\n\nFirst we will use bayesian search on 50% of the data to find the best Hyperparameters and then with those parameters we will train the final model with 100% of the train data.","44a98159":"# XGBOOST + BAYESIAN SEARCH CV","1e434c8c":"## Train the model","f572c1b8":"## Load data\n\nFirst we will be loading the titanic training dataset.\n\n_Note the sample function, usefull in case the dataset is too big and you want to get only a sample from it. With the frac parameter yo define % of data to keep._","18308030":"## Feature Engineering\n\nFor now we will just drop features that can't be used as is, and just binnarize the categorical features to use them in the model.\n\nIn future updates I will be adding some feature engineering."}}