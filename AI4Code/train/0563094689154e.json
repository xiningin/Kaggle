{"cell_type":{"90553d7a":"code","da12c929":"code","4542dc31":"code","59825260":"code","00ec5ab9":"code","bf981c73":"code","411b0ff8":"code","5518a649":"code","a7a4eda9":"code","5491d91e":"code","7699b005":"code","080c2cd5":"code","457bf7ee":"code","1071a41d":"code","4ff6a570":"code","633f282a":"code","ebcba101":"code","b593da36":"code","f567b46a":"code","77de4f3e":"code","04a54f6a":"code","82ece162":"code","e0996f92":"code","da755873":"code","7f4afd38":"code","113c7841":"markdown","ec1ac933":"markdown","49341a9d":"markdown","92ea04f8":"markdown","6e2c04bf":"markdown","d6494b65":"markdown","214c90ee":"markdown","402096f8":"markdown","98a363e6":"markdown","d65d522d":"markdown","60c46d43":"markdown","362dd1f2":"markdown","80beab42":"markdown","4acded1b":"markdown","beb61b2e":"markdown","8306fe8d":"markdown","e380db61":"markdown","7b66f474":"markdown","42b7f8eb":"markdown","86c69be7":"markdown","b8de424f":"markdown","d961ee93":"markdown","ff0f51ed":"markdown"},"source":{"90553d7a":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:90% !important; }<\/style>\"))\n\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport copy\nfrom tqdm import tqdm\nfrom numba import jit \nimport gc\nfrom catboost import CatBoostRegressor, Pool\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom datetime import datetime\nimport os\nimport warnings\nimport lightgbm as lgb","da12c929":"LOCAL_EXECUTION=False   # we will use this flag to indicate whether \n                        # the notebook is running on a local computer or\n                        # on Kaggle cloud\n        \nPRINT_LOGS=True         # this flag will be passed as a default value \n                        # for \"verbose\" paramiters of all functions \n                        # to indicate whether they should print detailed logs","4542dc31":"# Variables for path of the data\n\nif LOCAL_EXECUTION:\n    ORIGINAL_DATA_DIR_PATH=\".\/orig_data\/\"\n    PREPROCESSED_DATA_DIR_PATH=\".\/generated_data\/\"\nelse:\n    ORIGINAL_DATA_DIR_PATH=\"\/kaggle\/input\/data-science-bowl-2019\/\"\n    PREPROCESSED_DATA_DIR_PATH=\"\/kaggle\/input\/dsb-preprocessed-data\/\"\n\nif LOCAL_EXECUTION:\n    import qgrid as qg  \n    qg.enable()\n    qg.set_grid_option(\"forceFitColumns\",False)","59825260":"# paramiters for efficient data loading via pd.read_csv()\n# copied from https:\/\/www.kaggle.com\/tdobson\/30x-speedup-on-io\n\ntrain_test_args = {\n    'dtype': {\n        'event_id': 'category',\n        'game_session': 'category',\n        'event_data': 'object',\n        'installation_id': 'category',\n        'event_count': 'int64',\n        'event_code': 'category',\n        'game_time': 'int64',\n        'title': 'category',\n        'type': 'category',\n        'world': 'category'\n    },\n    \"parse_dates\": ['timestamp']\n}","00ec5ab9":"# Global variables - will be filled with data a few pages later.\n\nDSB_train_d=pd.DataFrame()  # Main datasets for DSB competition\nDSB_test_d=pd.DataFrame()   \n\nDSB_features=pd.DataFrame() # All features generated for a combined \n                            # train+test datased for all the assessment sessions\n                            # (one session per row).\n\nDSB_train_assessments={}    # Here we will store lists of assessment game_session id-s\nDSB_test_assessments={}     ","bf981c73":"# Function to load the test data\n\ndef load_test_data(verbose=PRINT_LOGS):\n    \n    global DSB_test_d\n    \n    test_filename=ORIGINAL_DATA_DIR_PATH + \"test.csv\"\n    \n    if verbose: \n        print(f\"Loading process for file {test_filename} has started.\")\n        \n    DSB_test_d=pd.read_csv(test_filename,**train_test_args )        \n    DSB_test_d=process_dataset(DSB_test_d,remove_empty_assessments=False)\n    \n    gc.collect()\n    \n    if verbose: \n        print(\"File loading and pre-processing have finished. \\n\")\n        print(f\"Shape of DSB_test_d is {DSB_test_d.shape}\")\n        print(f\"\\nColumn names: {list(DSB_test_d.columns)} \\n\" )","411b0ff8":"# Function to load the train data\n# The function will check if preprocessed file exist to avoid duplicate work. If so load the preprocessed data.\n# Otherwise, the function will load the new data.\n\ndef load_train_data(verbose=PRINT_LOGS,save_pickle=LOCAL_EXECUTION):\n    \n    global DSB_train_d\n    \n    train_filename=ORIGINAL_DATA_DIR_PATH + \"train.csv\"\n    \n    if verbose: \n        print(f\"Loading process for file {train_filename} has started.\")\n        \n    t_file_version_id=(\"TRAINPP_v\" +\"Jan18\"+\"_Added_\" \n                       +str(len(BEST_EXT_NUM_VAR_NAME_SET)+2) \n                       + \"_Vars_\")\n    \n    original_filename_size=os.path.getsize(train_filename)\n    \n    preprocessed_filename=(PREPROCESSED_DATA_DIR_PATH \n                            + t_file_version_id \n                            + str(original_filename_size) +\".pkl\")\n    \n    # Check if preprocessed file exists. If so, load the preprocessed file.\n    # Otherwise, load the new file.\n    if os.path.exists(preprocessed_filename):\n        if verbose: \n            print(f\"Loading pre-processed file {preprocessed_filename}\")\n        DSB_train_d=pd.read_pickle(preprocessed_filename)\n    else:\n        if verbose: \n            print(f\"Pre-processed file {preprocessed_filename} was NOT FOUND\")\n            print(f\"Loading and pre-processing original file {train_filename} \")\n        \n        DSB_train_d=pd.read_csv(train_filename,**train_test_args )        \n        DSB_train_d=process_dataset(DSB_train_d,remove_empty_assessments=True)\n        \n        if save_pickle:\n            DSB_train_d.to_pickle(preprocessed_filename)\n        \n    gc.collect()\n    \n    if verbose: \n        print(\"File loading and pre-processing for train data have finished. \\n\")\n        print(f\"Shape of DSB_train_d is {DSB_train_d.shape}\")\n        print(f\"\\nColumn names: {list(DSB_train_d.columns)} \\n\" )","5518a649":"# the full list of extra variables from 'event_data' with which we experimented\nEXT_NUM_VAR_NAME_FULL_LIST=[\"round\",\"level\",\"misses\" ,\"tutorial_step\"\n                       ,\"round_number\",\"size\",\"stage_number\"\n                       ,\"dwell_time\",\"time_played\",\"duration\"\n                       ,\"total_duration\", \"session_duration\"\n                       ,\"shell_size\",\"buglength\",\"target_weight\"\n                       ,\"table_weights\", \"dinosaur_count\",\"distance\"\n                       ,\"total_bowls\",\"starting_weights\",\"cloud_size\"\n                       ,\"target_size\",\"dinosaur_weight\",\"target_water_level\"\n                      ]\n\n# the set of best performing extra variables which was identified empirically\nBEST_EXT_NUM_VAR_NAME_CB400_SET=set(['round', 'misses', 'size'\n                                    , 'stage_number', 'duration'\n                                    , 'total_duration'\n                                    , 'session_duration'\n                                    , 'buglength', 'target_weight'\n                                    , 'distance'])\n\n\nBEST_EXT_NUM_VAR_NAME_SET=BEST_EXT_NUM_VAR_NAME_CB400_SET\n\nBEST_EXT_NUM_VAR_NAME_LIST=list(BEST_EXT_NUM_VAR_NAME_SET)\n\nassert len(BEST_EXT_NUM_VAR_NAME_SET)==10","a7a4eda9":"# Functions for filtering the assessment events\ndef filter_4100(dataset):\n    return (  (dataset.type==\"Assessment\")\n            & (dataset.title != \"Bird Measurer (Assessment)\")\n            & (dataset.event_code == \"4100\")  )\n    \ndef filter_4110(dataset):\n    return (  (dataset.type==\"Assessment\")\n            & (dataset.title == \"Bird Measurer (Assessment)\")\n            & (dataset.event_code == \"4110\")  )\n\ndef filter_scored_assessments(dataset):\n    return filter_4100(dataset) | filter_4110(dataset)","5491d91e":"def process_dataset(data_input, remove_empty_assessments=True):\n\n    data_s=copy.deepcopy(data_input)\n    \n    ####### Creatung a column named attempt_result #########\n    \n    if remove_empty_assessments:\n        sessions_w_scored_assessments=data_s[filter_scored_assessments(data_s)].game_session.unique()\n        installs_to_keep=data_s[data_s.game_session.isin(sessions_w_scored_assessments)].installation_id.unique()\n    else:\n        sessions_w_assessments=data_s[data_s.type==\"Assessment\"].game_session.unique()\n        installs_to_keep=data_s[data_s.game_session.isin(sessions_w_assessments)].installation_id.unique()\n    \n    \n    data_s=data_s[data_s.installation_id.isin(installs_to_keep)]\n\n    data_s[\"attempt_result\"]=np.nan\n\n    data_s.loc[filter_scored_assessments(data_s) \n                & data_s.event_data.str.contains('\"correct\":true',regex=False),\"attempt_result\"\n               ]=\"ASSESSMENT_GOOD_ATTEMPT\"\n    \n    data_s.loc[filter_scored_assessments(data_s)\n                & data_s.event_data.str.contains('\"correct\":false',regex=False),\"attempt_result\"\n               ]=\"ASSESSMENT_BAD_ATTEMPT\"\n    \n    \n    data_s[\"attempt_result\"]=data_s[\"attempt_result\"].astype(\"category\")\n    \n    ####### Creating a column named accuracy_group #########\n    ## The column will mostly have NaNs, except for the very last attempt events in an assessment session\n    ## for those events it will contain actual value of the accuracy\n    \n    assessments_with_attempts=data_s[ (data_s.attempt_result==\"ASSESSMENT_BAD_ATTEMPT\") \n                                     | (data_s.attempt_result==\"ASSESSMENT_GOOD_ATTEMPT\") ]\n    \n    assessment_results=pd.pivot_table(assessments_with_attempts, index=\"game_session\"\n                                  ,columns=\"attempt_result\", values=\"event_id\"\n                                  ,aggfunc=len, fill_value=0)\n\n    assessment_results.columns=list(assessment_results.columns)\n    \n    assessment_results[\"accuracy_group\"]=0.0\n\n    assessment_results[\"accuracy_group\"]=assessment_results[\"ASSESSMENT_GOOD_ATTEMPT\"]*3.0\n\n    assessment_results.loc[(assessment_results.accuracy_group==3) \n                           & (assessment_results[\"ASSESSMENT_BAD_ATTEMPT\"]==1)\n                           ,\"accuracy_group\"]=2\n\n    assessment_results.loc[(assessment_results.accuracy_group==3) \n                           & (assessment_results[\"ASSESSMENT_BAD_ATTEMPT\"]>=2)\n                           ,\"accuracy_group\"]=1\n\n    final_attempts=copy.deepcopy(assessments_with_attempts[[\"game_session\",\"attempt_result\"]]\n                                 .drop_duplicates(subset=\"game_session\",keep=\"last\"))\n    final_attempts[\"old_index\"]=final_attempts.index\n    final_attempts=final_attempts.merge(assessment_results,on=\"game_session\")\n    final_attempts.index=final_attempts[\"old_index\"]\n    final_attempts.index.name=\"new_index\"\n    data_s[\"accuracy_group\"]=np.nan\n    data_s[\"accuracy_group\"]=final_attempts[\"accuracy_group\"]\n\n\n    ####### Extracting numeric features from event_data #########\n    for ext_var_name in BEST_EXT_NUM_VAR_NAME_SET:\n        ext_var_full_name='\"'+ ext_var_name + r'\":\\s*'\n        extraction_condition=\"(\"+ext_var_full_name+\")\"\n        extraction_outcome=r\"([0-9]+)\"\n        rez=data_s.event_data.str.extract(extraction_condition+extraction_outcome)\n        data_s[ext_var_name]=rez[1].astype(\"float\")\n\n    data_s.drop(columns=\"event_data\", inplace=True)\n    \n    data_s.sort_values(by=\"timestamp\",inplace=True)\n    \n    return data_s","7699b005":"# ASSESSMENT_TITLES=list(set(DSB_train_d[DSB_train_d.type==\"Assessment\"].title.unique()) \n#                        | set(DSB_test_d[DSB_test_d.type==\"Assessment\"].title.unique()))\n\nASSESSMENT_TITLES_LIST=['Mushroom Sorter (Assessment)', 'Cauldron Filler (Assessment)', 'Chest Sorter (Assessment)'\n                   , 'Bird Measurer (Assessment)', 'Cart Balancer (Assessment)']\n\n\nASSESSMENT_TITLES_SET=set(ASSESSMENT_TITLES_LIST)\n\nassert len(ASSESSMENT_TITLES_LIST)==5\n\n# NON_ASSESSMENT_TITLES=list(set(DSB_train_d[DSB_train_d.type!=\"Assessment\"].title.unique())\n#                            | set(DSB_test_d[DSB_test_d.type!=\"Assessment\"].title.unique()))\n\nNON_ASSESSMENT_TITLES_LIST =['Crystal Caves - Level 3', 'Chow Time', 'Dino Dive',\n 'Slop Problem', 'Crystal Caves - Level 2', 'Scrub-A-Dub', 'Crystals Rule',\n 'Happy Camel', 'Magma Peak - Level 2', 'Egg Dropper (Activity)', 'Fireworks (Activity)',\n 'Flower Waterer (Activity)', 'Magma Peak - Level 1',\n 'Welcome to Lost Lagoon!', 'Treasure Map', '12 Monkeys', 'Watering Hole (Activity)',\n 'Sandcastle Builder (Activity)', 'Rulers', 'Ordering Spheres',\n 'Tree Top City - Level 1', 'Bug Measurer (Activity)',\n 'Leaf Leader', 'Tree Top City - Level 3', 'Crystal Caves - Level 1',\n 'All Star Sorting', 'Air Show', 'Dino Drink', 'Pan Balance', 'Lifting Heavy Things',\n 'Bottle Filler (Activity)', 'Tree Top City - Level 2',\n 'Heavy, Heavier, Heaviest', 'Costume Box', 'Balancing Act',\n \"Pirate's Tale\", 'Chicken Balancer (Activity)', 'Bubble Bath', 'Honey Cake']\n\nALL_TITLES_LIST=ASSESSMENT_TITLES_LIST+NON_ASSESSMENT_TITLES_LIST\n\nBEST_TITLES_CB400_LIST=['Mushroom Sorter (Assessment)', 'Cauldron Filler (Assessment)'\n                           , 'Chest Sorter (Assessment)', 'Bird Measurer (Assessment)'\n                           , 'Cart Balancer (Assessment)', 'Crystal Caves - Level 3'\n                           , 'Chow Time', 'Crystal Caves - Level 2', 'Scrub-A-Dub'\n                           , 'Crystals Rule', 'Happy Camel', 'Magma Peak - Level 2'\n                           , 'Egg Dropper (Activity)', 'Fireworks (Activity)'\n                           , 'Flower Waterer (Activity)', 'Watering Hole (Activity)'\n                           , 'Sandcastle Builder (Activity)', 'Tree Top City - Level 1'\n                           , 'Bug Measurer (Activity)', 'Tree Top City - Level 3'\n                           , 'Crystal Caves - Level 1', 'All Star Sorting', 'Air Show'\n                           , 'Dino Drink', 'Pan Balance', 'Tree Top City - Level 2'\n                           , 'Chicken Balancer (Activity)']\n\nBEST_TITLES_SET=set(BEST_TITLES_CB400_LIST)\n\nassert len(ALL_TITLES_LIST)==44\nassert len(BEST_TITLES_SET)==27\n\n\n# ALL_TYPES_LIST=list(set(DSB_data[\"train\"].type.unique()) | set(DSB_test_d.type.unique()))\n\nALL_TYPES_LIST=['Assessment', 'Activity', 'Clip', 'Game']\nBEST_TYPES_CB400_LIST=['Assessment', 'Activity', 'Game']\n\nBEST_TYPES_SET=set(BEST_TYPES_CB400_LIST)\n\nassert len(ALL_TYPES_LIST)==4\nassert len(BEST_TYPES_SET)==3\n\n# ALL_WORLDS_LIST=list(set(DSB_train_d.world.unique()) | set(DSB_test_d.world.unique()))\n\nALL_WORLDS_LIST=['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES', 'NONE']\nBEST_WORLDS_CB400_LIST=['MAGMAPEAK', 'TREETOPCITY', 'CRYSTALCAVES']\nBEST_WORLDS_SET=set(BEST_WORLDS_CB400_LIST)\n\nassert len(ALL_WORLDS_LIST)==4\nassert len(BEST_WORLDS_SET)==3\n\nASSESSMENT_RESULTS_LIST=['ASSESSMENT_GOOD_ATTEMPT', 'ASSESSMENT_BAD_ATTEMPT']\nASSESSMENT_RESULTS_SET=set(ASSESSMENT_RESULTS_LIST)\n\nassert len(ASSESSMENT_RESULTS_SET)==2\n\n# ALL_EVENT_CODES_LIST=list(set(DSB_train_d.event_code.unique()) | set(DSB_test_d.event_code.unique()))\n\nALL_EVENT_CODES_LIST = ['3110', '4050', '2000', '4080', '5010', '2080', '3021',\n                   '2010', '4021', '2060', '4035', '2083', '4022', '4100', \n                   '2081', '4070', '4025', '4010', '4230', '5000', '4031', \n                   '4095', '2025', '4020', '2075', '3120', '2030', '3020', \n                   '4030', '4090', '2050', '3010', '2020', '4045', '4110', \n                   '4235', '2040', '4040', '2035', '3121', '4220', '2070']\n\nBEST_EVENT_CODES_CB400_LIST = ['3110', '2000', '3021', '4021', '4035', '4022'\n                               , '4100', '4070', '4025', '4020', '3120', '2030'\n                               , '3020', '4030', '4090', '3010', '2020', '4045'\n                               , '4040', '2035', '3121', '2070']\n\nBEST_EVENT_CODES_SET=set(BEST_EVENT_CODES_CB400_LIST)\n\nassert len(ALL_EVENT_CODES_LIST)==42\nassert len(BEST_EVENT_CODES_SET)==22\n\n# ALL_EVENT_IDS_LIST=list(set(DSB_train_d.event_id.unique()) \n#                         | set(DSB_test_d.event_id.unique()))\n\nBEST_EVENT_IDS_CB400_LIST=['c58186bf', 'e694a35b', '5f0eb72c', '15f99afc'\n                           , 'c74f40cd', 'd3f1e122', '7da34a02', 'd185d3ea'\n                           , '2230fab4', '3afb49e6', '160654fd', '5a848010'\n                           , '84538528', 'ca11f653', '363c86c9', '0330ab6a'\n                           , '923afab1', 'a44b10dc', '28520915', 'e9c52111'\n                           , '2dcad279', '5290eab1', '88d4a5be', 'acf5c23f'\n                           , '562cec5f', '6cf7d25c', 'e4f1efe6', '47026d5f'\n                           , '9d29771f', '3ee399c3', 'ecaab346', '828e68f9'\n                           , 'c51d8688', '709b1251', '71fe8f75', '884228c8'\n                           , '30614231', '7372e1a5', 'f71c4741', '56817e2b'\n                           , '4ef8cdd3', '0d1da71f', '5e812b27', '5c2f29ca'\n                           , '1bb5fbdb', 'ac92046e', 'df4940d3', '04df9b66'\n                           , '4a09ace1', '0db6d71d', 'a52b92d5', '27253bdc'\n                           , '26fd2d99', 'b88f38da', '1325467d', '3323d7e9'\n                           , '38074c54', 'f7e47413', '598f4598', '565a3990'\n                           , 'a1bbe385', 'b5053438', '3bf1cf26']\n\n\nBEST_EVENT_IDS_SET=set(BEST_EVENT_IDS_CB400_LIST)\n\nassert len(BEST_EVENT_IDS_SET)==63","080c2cd5":"#this function builds features for just one assessment session\n\ndef build_a_row(s,      # game_session for which we are building features\n                s_data  # input dataframe, which contains all past\n                        # events for the installation_id, to which s belongs,\n                        # \"past\" means they happened before s started\n               ):\n    \n    new_row={}\n    new_row[\"game_session\"]=s\n    \n    ###-ASSESSMENTS-SPECIFIC-################################################################################\n\n    g=s_data.groupby([\"title\",\"attempt_result\"])[\"type\"].count()\n    g_indx=list(g.index)\n    for t in ASSESSMENT_TITLES_LIST:\n        for r in ASSESSMENT_RESULTS_LIST:\n            feature_name=r+\" total attempt event count in title \"+t \n            if (t,r) in g_indx:\n                new_row[feature_name]=g[(t,r)]\n                \n    g=s_data[s_data.attempt_result.isin(ASSESSMENT_RESULTS_LIST)].groupby([\"title\"])[\"type\"].count()\n    for t in ASSESSMENT_TITLES_SET & set(g.index):\n        feature_name=\"Total attempt event count in title \"+t \n        new_row[feature_name]=g[t]\n\n    g=s_data.groupby([\"attempt_result\"])[\"type\"].count()\n    for r in ASSESSMENT_RESULTS_SET & set(g.index):\n        feature_name=r+\" total attempt count\"  \n        new_row[feature_name]=g[r]\n\n    new_row[\"ASSESSMENT total attempt count\"]=(new_row[\"ASSESSMENT_GOOD_ATTEMPT total attempt count\"]\n                                               +new_row[\"ASSESSMENT_BAD_ATTEMPT total attempt count\"])\n\n    ###-SESSION-SPECIFIC-FEATURES#######################################################\n\n    g=s_data[[\"event_code\",\"timestamp\"]].groupby(\"event_code\").count()\n    for e in BEST_EVENT_CODES_SET & set(g.index):\n        new_row[\"Event_code \"+ e +\" preceding sessions event count\"]=g.at[e,\"timestamp\"]\n\n    g=s_data[[\"event_id\",\"timestamp\"]].groupby(\"event_id\").count()\n    for ei in BEST_EVENT_IDS_SET & set(g.index):\n        new_row[\"Event_id \"+ ei +\" preceding sessions event_id count\"]=g.at[ei,\"timestamp\"]\n    \n    \n    ## Features based on variables, extracted from event_data <<<<<<<<<<<<<<<<<<<<<<<\n    for e_var in [\"accuracy_group\"] + BEST_EXT_NUM_VAR_NAME_LIST:\n        new_row[\"EXTR: \" + e_var + \" for preceding sessions count\"]=s_data[e_var].count()\n        new_row[\"EXTR: \" + e_var + \" for preceding sessions total\"]=s_data[e_var].sum()\n        new_row[\"EXTR: \" + e_var + \" for preceding sessions mean\"]=s_data[e_var].mean()\n        new_row[\"EXTR: \" + e_var + \" for preceding sessions median\"]=s_data[e_var].median()\n        new_row[\"EXTR: \" + e_var + \" for preceding sessions max\"]=s_data[e_var].max()\n    \n        g=s_data[[\"title\",e_var]].groupby(\"title\").agg([\"count\",\"sum\",\"max\",\"mean\",\"median\"])\n        for t in BEST_TITLES_SET & set(g.index):\n            new_row[\"Title \"+ t +\" preceding sessions var count for \"+e_var]=g.at[t,(e_var,\"count\")]\n            new_row[\"Title \"+ t +\" preceding sessions var total for \"+e_var]=g.at[t,(e_var,\"sum\")]\n            new_row[\"Title \"+ t +\" preceding sessions var max for \"+e_var]=g.at[t,(e_var,\"max\")]\n            new_row[\"Title \"+ t +\" preceding sessions var mean for \"+e_var]=g.at[t,(e_var,\"mean\")]\n            new_row[\"Title \"+ t +\" preceding sessions var median for \"+e_var]=g.at[t,(e_var,\"median\")]\n        \n        g=s_data[[\"event_id\",e_var]].groupby(\"event_id\").agg([\"count\",\"sum\",\"max\",\"mean\",\"median\"])\n        for ei in BEST_EVENT_IDS_SET & set(g.index):\n            new_row[\"event_id \"+ ei +\" preceding sessions var count for \"+e_var]=g.at[ei,(e_var,\"count\")]\n            new_row[\"event_id \"+ ei +\" preceding sessions var total for \"+e_var]=g.at[ei,(e_var,\"sum\")]\n            new_row[\"event_id \"+ ei +\" preceding sessions var max for \"+e_var]=g.at[ei,(e_var,\"max\")]\n            new_row[\"event_id \"+ ei +\" preceding sessions var mean for \"+e_var]=g.at[ei,(e_var,\"mean\")]\n            new_row[\"event_id \"+ ei +\" preceding sessions var median for \"+e_var]=g.at[ei,(e_var,\"median\")]\n        \n        g=s_data[[\"event_code\",e_var]].groupby(\"event_code\").agg([\"count\",\"sum\",\"max\",\"mean\",\"median\"])\n        for ec in BEST_EVENT_CODES_SET & set(g.index):\n            new_row[\"event_code \"+ ec +\" preceding sessions var count for \"+e_var]=g.at[ec,(e_var,\"count\")]\n            new_row[\"event_code \"+ ec +\" preceding sessions var total for \"+e_var]=g.at[ec,(e_var,\"sum\")]\n            new_row[\"event_code \"+ ec +\" preceding sessions var max for \"+e_var]=g.at[ec,(e_var,\"max\")]\n            new_row[\"event_code \"+ ec +\" preceding sessions var mean for \"+e_var]=g.at[ec,(e_var,\"mean\")]\n            new_row[\"event_code \"+ ec +\" preceding sessions var median for \"+e_var]=g.at[ec,(e_var,\"median\")]\n        \n        g=s_data[[\"type\",e_var]].groupby(\"type\").agg([\"count\",\"sum\",\"max\",\"mean\",\"median\"])\n        for t in BEST_TYPES_SET & set(g.index):\n            new_row[\"Type \"+ t +\" preceding sessions var count for \"+e_var]=g.at[t,(e_var,\"count\")]\n            new_row[\"Type \"+ t +\" preceding sessions var total for \"+e_var]=g.at[t,(e_var,\"sum\")]\n            new_row[\"Type \"+ t +\" preceding sessions var max for \"+e_var]=g.at[t,(e_var,\"max\")]\n            new_row[\"Type \"+ t +\" preceding sessions var mean for \"+e_var]=g.at[t,(e_var,\"mean\")]\n            new_row[\"Type \"+ t +\" preceding sessions var median for \"+e_var]=g.at[t,(e_var,\"median\")]\n        \n        g=s_data[[\"world\",e_var]].groupby(\"world\").agg([\"count\",\"sum\",\"max\",\"mean\",\"median\"])\n        for w in BEST_WORLDS_SET & set(g.index):\n            new_row[\"World \"+ w +\" preceding sessions var count for \"+e_var]=g.at[w,(e_var,\"count\")]\n            new_row[\"World \"+ w +\" preceding sessions var total for \"+e_var]=g.at[w,(e_var,\"sum\")]\n            new_row[\"World \"+ w +\" preceding sessions var max for \"+e_var]=g.at[w,(e_var,\"max\")]\n            new_row[\"World \"+ w +\" preceding sessions var mean for \"+e_var]=g.at[w,(e_var,\"mean\")]\n            new_row[\"World \"+ w +\" preceding sessions var median for \"+e_var]=g.at[w,(e_var,\"median\")]\n        \n        \n    ## <<<<<<<<<<<<<<<<<<<<<<<\n    \n    s_data.drop_duplicates(subset=[\"game_session\"],keep=\"last\",inplace=True)\n    s_data[\"session_count\"]=1\n\n    new_row[\"preceding sessions count\"]=s_data.game_session.nunique()\n    new_row[\"preceding sessions total duration\"]=s_data.game_time.sum()\n    new_row[\"preceding sessions mean duration\"]=s_data.game_time.mean()\n    new_row[\"preceding sessions median duration\"]=s_data.game_time.median()\n    new_row[\"preceding sessions max duration\"]=s_data.game_time.max()\n    \n    new_row[\"preceding sessions total event count\"]=s_data.event_count.sum()\n    new_row[\"preceding sessions mean event count per session\"]=s_data.event_count.mean()\n    new_row[\"preceding sessions median event count per session\"]=s_data.event_count.median()\n    new_row[\"preceding sessions max event count per session\"]=s_data.event_count.max()\n    \n\n    ## ---------------\n    \n    g=s_data[[\"type\",\"game_session\",\"game_time\",\"event_count\",\"session_count\"]\n            ].groupby(\"type\").agg([\"mean\",\"median\",\"max\",\"sum\"])\n    for t in BEST_TYPES_SET & set(g.index):\n        new_row[\"Type \"+ t +\" preceding sessions median duration\"]=g.at[t,(\"game_time\",\"median\")]\n        new_row[\"Type \"+ t +\" preceding sessions median event count\"]=g.at[t,(\"event_count\",\"median\")]\n        new_row[\"Type \"+ t +\" preceding sessions mean duration\"]=g.at[t,(\"game_time\",\"mean\")]\n        new_row[\"Type \"+ t +\" preceding sessions mean event count\"]=g.at[t,(\"event_count\",\"mean\")]\n        new_row[\"Type \"+ t +\" preceding sessions max duration\"]=g.at[t,(\"game_time\",\"max\")]\n        new_row[\"Type \"+ t +\" preceding sessions max event count\"]=g.at[t,(\"event_count\",\"max\")]\n        new_row[\"Type \"+ t +\" preceding sessions total count\"]=g.at[t,(\"session_count\",\"sum\")]\n        new_row[\"Type \"+ t +\" preceding sessions total duration\"]=g.at[t,(\"game_time\",\"sum\")]\n        new_row[\"Type \"+ t +\" preceding sessions total event count\"]=g.at[t,(\"event_count\",\"sum\")]\n    \n    g=s_data[[\"world\",\"game_session\",\"game_time\",\"event_count\",\"session_count\"]\n            ].groupby(\"world\").agg([\"mean\",\"median\",\"max\",\"sum\"])\n    for w in BEST_WORLDS_SET & set(g.index):\n        new_row[\"World \"+ w +\" preceding sessions median duration\"]=g.at[w,(\"game_time\",\"median\")]\n        new_row[\"World \"+ w +\" preceding sessions median event count\"]=g.at[w,(\"event_count\",\"median\")]\n        new_row[\"World \"+ w +\" preceding sessions mean duration\"]=g.at[w,(\"game_time\",\"mean\")]\n        new_row[\"World \"+ w +\" preceding sessions mean event count\"]=g.at[w,(\"event_count\",\"mean\")]\n        new_row[\"World \"+ w +\" preceding sessions max duration\"]=g.at[w,(\"game_time\",\"max\")]\n        new_row[\"World \"+ w +\" preceding sessions max event count\"]=g.at[w,(\"event_count\",\"max\")]\n        new_row[\"World \"+ w +\" preceding sessions total count\"]=g.at[w,(\"session_count\",\"sum\")]\n        new_row[\"World \"+ w +\" preceding sessions total duration\"]=g.at[w,(\"game_time\",\"sum\")]\n        new_row[\"World \"+ w +\" preceding sessions total event count\"]=g.at[w,(\"event_count\",\"sum\")]\n    \n    g=s_data[[\"title\",\"game_session\",\"game_time\",\"event_count\",\"session_count\"]\n            ].groupby(\"title\").agg([\"mean\",\"median\",\"max\",\"sum\"])\n    for t in BEST_TITLES_SET & set(g.index):\n        new_row[\"Title \"+ t +\" preceding sessions median duration\"]=g.at[t,(\"game_time\",\"median\")]\n        new_row[\"Title \"+ t +\" preceding sessions median event count\"]=g.at[t,(\"event_count\",\"median\")]\n        new_row[\"Title \"+ t +\" preceding sessions mean duration\"]=g.at[t,(\"game_time\",\"mean\")]\n        new_row[\"Title \"+ t +\" preceding sessions mean event count\"]=g.at[t,(\"event_count\",\"mean\")]\n        new_row[\"Title \"+ t +\" preceding sessions max duration\"]=g.at[t,(\"game_time\",\"max\")]\n        new_row[\"Title \"+ t +\" preceding sessions max event count\"]=g.at[t,(\"event_count\",\"max\")]\n        new_row[\"Title \"+ t +\" preceding sessions total count\"]=g.at[t,(\"session_count\",\"sum\")]\n        new_row[\"Title \"+ t +\" preceding sessions total duration\"]=g.at[t,(\"game_time\",\"sum\")]\n        new_row[\"Title \"+ t +\" preceding sessions total event count\"]=g.at[t,(\"event_count\",\"sum\")]\n    \n    s_data.sort_values(by=\"timestamp\",ascending=False)\n    \n    if len(s_data)>0:\n        p_title=s_data.loc[s_data.index[0],\"title\"]\n        p_duration=s_data.loc[s_data.index[0],\"game_time\"]\n        p_events=s_data.loc[s_data.index[0],\"event_count\"]\n        new_row[f\"Previous session had a duration of\"]=p_duration\n        new_row[f\"Previous session had this number of events\"]=p_events\n        new_row[f\"Previous session had a title of\"]=p_title\n    else:\n        new_row[f\"Previous session had a title of\"]=\"_NOT_A_TITLE_\"\n    \n    return new_row   ","457bf7ee":"# This function takes a dataset and a list of assesment sessions, \n# and iterates through the list to generate all the features for each of the sessions.\n# There are a few tricks implemented here to optimize for memory usage and speed\n\ndef build_X_main(all_target_sessions,       # A list of assesment sessions\n                 raw_data,                  # A full dataset\n                 columns_to_keep_list=[]    # An empty list if we are running feature generation \n                                            # for the first time, which means we are free to de-dup \n                                            # in any way we want to; otherwise a list of column names to keep\n                ):\n    \n     \n    sessions_info=raw_data[raw_data.game_session.isin(all_target_sessions)\n                          ][[\"game_session\",\"title\",\"world\"]].drop_duplicates()\n\n    all_rows=[]\n    partial_dataframes=[]\n    \n    raw_data.sort_values(by=\"timestamp\",inplace=True)\n    \n    session_start_times=raw_data.drop_duplicates(subset=[\"game_session\"]\n                                                 ,keep=\"first\")[[\"game_session\"\n                                                                 ,\"timestamp\"\n                                                                 ,\"installation_id\"]]\n    session_start_times.set_index(\"game_session\",inplace=True)\n    session_start_times.columns=[\"start_time\", \"installation_id\"]\n    \n    raw_data_groupped_by_inst=raw_data.groupby(\"installation_id\")\n    \n    iteration_counter=0\n    \n    for s_installation_id, installation_data in tqdm(raw_data_groupped_by_inst):\n        \n        sessions_in_installation_list=list(installation_data.game_session.unique())\n        \n        sessions_to_process_in_inst_list=list(set(sessions_in_installation_list) \n                                              & set(all_target_sessions)\n                                             )\n        \n        for s in sessions_to_process_in_inst_list:\n            s_start_time=session_start_times.loc[s,\"start_time\"]\n            s_data=copy.deepcopy(installation_data[ installation_data.timestamp < s_start_time ])\n            s_data.sort_values(by=\"timestamp\",inplace=True)\n            new_row=build_a_row(s,s_data)        \n            all_rows.append(new_row)    \n    \n        # every 500 iterations we are optimizing memory usage  \n        iteration_counter+=1           \n        if (iteration_counter%500)==0:  \n            partial_dataframes.append(pd.DataFrame(all_rows)\n                                      .set_index(\"game_session\"))\n            all_rows=[]\n            gc.collect() \n    \n    if len(all_rows)>0:\n        partial_dataframes.append(pd.DataFrame(all_rows)\n                                  .set_index(\"game_session\"))\n        all_rows=[]\n    \n    result=pd.concat(partial_dataframes, ignore_index=False, sort=True)\n    \n    result.sort_index(inplace=True)\n\n    result=result.merge(sessions_info, on=\"game_session\")\n\n    result.set_index(\"game_session\", inplace=True)\n    \n    result.sort_index(inplace=True)\n    \n    categorical_columns_list=[\"title\",\"world\" ,\"Previous session had a title of\"]\n    for c in categorical_columns_list:\n            result[c]=result[c].astype(\"category\")\n    \n    categorical_columns_df=copy.deepcopy(result[categorical_columns_list])\n    result.drop(columns=categorical_columns_list,inplace=True)\n    result=result.astype(\"float\")\n    result.fillna(0,inplace=True)\n    \n    if len(columns_to_keep_list)==0:\n        # removing duplicating features    \n        result=result.T.drop_duplicates().T\n        result=result.join(categorical_columns_df)\n    else:\n        # keeping only pre-defined features\n        result=result.join(categorical_columns_df)\n        final_list_of_columns=list(set(columns_to_keep_list)\n                                   & set(result.columns))\n        result=result[final_list_of_columns]\n    \n    del partial_dataframes\n    \n    gc.collect()\n    \n    return result","1071a41d":"## The function takes a dataset (train or test or combinded) \n## and returns important groups of Assessment sessions \n## (as lists of session ID-s)\ndef count_assessments(raw_data, verbose=PRINT_LOGS):    \n    \n    if verbose:\n        print(f\"Starting counting assessments for a dataset with \\n\"\n              ,f\"{len(raw_data)} lines and the following columns:\\n\"\n              ,list(raw_data.columns))\n    \n    result_ids={}\n    \n    raw_data.sort_values(by=\"timestamp\",inplace=True)\n\n    result_ids[\"all\"]=list(raw_data[raw_data.type==\"Assessment\"].game_session.unique())\n\n    result_ids[\"all_w_outcomes\"]=list(raw_data[(raw_data.type==\"Assessment\") & \n                                               (raw_data.attempt_result.notnull())\n                                              ].game_session.unique())\n\n    last_assessments=raw_data[raw_data.game_session.isin(result_ids[\"all\"])\n                             ].drop_duplicates(subset=[\"installation_id\"], keep=\"last\")\n\n    result_ids[\"last\"]=list(last_assessments.game_session.unique())\n    \n    last_assessments_w_outcomes=raw_data[raw_data.game_session.isin(result_ids[\"all_w_outcomes\"])\n                                        ].drop_duplicates(subset=[\"installation_id\"], keep=\"last\")\n\n    result_ids[\"last_w_outcomes\"]=list(last_assessments_w_outcomes.game_session.unique())\n    \n    if verbose:\n        print(\"Assessments: \",{k:len(result_ids[k]) for k in result_ids})\n        print(\" \\n\")\n    \n    return result_ids","4ff6a570":"# This function first loads test\/train datasets, and uses them to generate features,\n# which are then saved into a file.\n# The next time the function is executed, it will check if the file exists, and (if yes)\n# it will load the features from the file instead of calculating them again.\n# However, when the features are loaded from the file, an additional check is executed \n# to see if the file covers all instalaltion_id-s from the test dataset.\n# If some instalaltion_id-s are missing, for these id-s the features will be calucated.\n# The final resul is storied into a global dataframe named DSB_features\n# This multi-tier logic is implemented in order to be able to pre-calculate features locally for \n# train.csv and public version of test.csv, and later calculate features for additional \n# installation_id-s from a private version of test.csv\n\ndef build_features(save_pickle=LOCAL_EXECUTION\n                   , verbose=PRINT_LOGS\n                   , features_to_keep=[]   # Non-empty list if the function should keep only \n                                           # specific features (defined in this list),\n                                           # otherwise all possible features will be created \n                                           # and then the list will be reduced  \n                                           # by auto de-dup procedure \n                  ):\n    \n    global DSB_features, DSB_train_d, DSB_test_d, DSB_train_assessments, DSB_test_assessments\n    \n    if(verbose): \n        print(\"------------------------------------\")\n        print(\"Starting working on features\")\n    \n    if len(DSB_train_d)==0:\n        load_train_data(verbose=verbose, save_pickle=save_pickle)\n        \n    if len(DSB_test_d)==0:\n        load_test_data(verbose=verbose)\n        \n    if len(DSB_train_assessments)==0:\n        DSB_train_assessments=count_assessments(DSB_train_d,verbose=verbose)\n    \n    if len(DSB_test_assessments)==0:\n        DSB_test_assessments=count_assessments(DSB_test_d,verbose=verbose)\n        \n    all_assessments_list=(DSB_train_assessments[\"all\"]\n                        +DSB_test_assessments[\"all\"])\n    \n    total_num_of_assessments=len(all_assessments_list)\n        \n    f_file_version_id=(\"FEATURESPP_\" \n                       + str(len(BEST_EXT_NUM_VAR_NAME_SET)) \n                       + \"Jan18_V1\")\n    \n    f_file_name=(PREPROCESSED_DATA_DIR_PATH \n                 + f_file_version_id + \".pkl\")\n    \n\n    if(verbose):\n        print(f\"A total set of {total_num_of_assessments} assessment sessions needs features\")\n\n    if not os.path.exists(f_file_name):\n        \n        if(verbose):\n            print(f\"File {f_file_name} does not exist\")\n            print(\"Starting a process of building features for all sessions\")\n        \n        train_features_df=build_X_main(DSB_train_assessments[\"all\"]\n                                       ,DSB_train_d\n                                       ,features_to_keep)\n        \n        test_features_df=build_X_main(DSB_test_assessments[\"all\"]\n                                      ,DSB_test_d\n                                      ,list(train_features_df.columns))\n        \n        DSB_features=pd.concat([train_features_df ,test_features_df]\n                               ,ignore_index=False\n                               ,sort=True)\n        \n        if save_pickle:\n            if(verbose):\n                print(f\"Saving file {f_file_name}\")\n                \n            features_to_save=pd.concat([train_features_df\n                                        ,test_features_df.drop(index=[test_features_df.index[10]\n                                                                      ,test_features_df.index[-10]])]\n                                       ,ignore_index=False)\n                        # we are dropping two game_sessions in order \n                        # to aways test feature re-building functionality \n                        # (which should be triggered automatically when\n                        # loading pre-processed file with missing sessions)\n            features_to_save.to_pickle(f_file_name)\n    else:\n        if(verbose):\n            print(f\"Loading file {f_file_name} \")\n                  \n        prebuilt_features_df=pd.read_pickle(f_file_name)\n        \n        assert prebuilt_features_df.index.name==\"game_session\"\n        \n        if len(features_to_keep)>0:\n            good_features=list(set(features_to_keep) \n                               & set(list(prebuilt_features_df.columns)))\n            bad_features=list(set(list(prebuilt_features_df.columns))\n                              -set(features_to_keep))\n            if len(bad_features)>0:\n                prebuilt_features_df.drop(columns=bad_features, inplace=True)\n        \n        prebuilt_assessments_list=list(prebuilt_features_df.index.unique())\n        \n        if(verbose):\n            print(f\"The file contains features for \"\n                  +f\"{len(prebuilt_assessments_list)} assessment sessions \")\n        \n        missing_assessments_list=list(set(all_assessments_list)\n                                      -set(prebuilt_assessments_list))\n        \n        if len(missing_assessments_list)>0:\n            \n            test_missing_inst_ids_list=list(DSB_test_d[DSB_test_d\n                                                       .game_session\n                                                       .isin(missing_assessments_list)\n                                                      ].installation_id.unique())\n            \n            train_missing_inst_ids_list=list(DSB_train_d[DSB_train_d\n                                                         .game_session\n                                                         .isin(missing_assessments_list)\n                                                        ].installation_id.unique())\n            \n            assert len(train_missing_inst_ids_list)==0\n            assert len(test_missing_inst_ids_list)>0\n            \n            if(verbose):\n                print(f\"{len(missing_assessments_list)} sessions \"\n                      + f\"from {len(test_missing_inst_ids_list)} installations were not found in the file\")\n                print(\"Starting a process of building features for missing sessions\")\n            \n            missing_instids_df=copy.deepcopy(DSB_test_d[DSB_test_d\n                                                        .installation_id\n                                                        .isin(test_missing_inst_ids_list)\n                                                       ])\n\n            missing_instids_df[\"installation_id\"]= missing_instids_df[\"installation_id\"].astype(\"object\")\n            \n            missing_features_df=build_X_main(missing_assessments_list\n                                             ,missing_instids_df\n                                             ,columns_to_keep_list=list(prebuilt_features_df.columns))\n\n            DSB_features=pd.concat([prebuilt_features_df\n                                    , missing_features_df]\n                                   ,ignore_index=False\n                                   ,sort=True)\n        else:\n            print(f\"No missing assessments identified while processing file {f_file_name}\")\n            print(f\"THIS IS AN ERROR: there is always at least one missing assessment !!!!\")\n            print(\"However, we are moving forward with just loaded assessments\")\n            DSB_features=prebuilt_features_df\n              \n    if \"Previous session had a title of\" in list(DSB_features.columns):\n            DSB_features[\"Previous session had a title of\"\n                        ]=DSB_features[\"Previous session had a title of\"\n                                      ].astype(\"category\")\n    if(verbose): \n        print(\"Feature-building process is complete\")\n        print(f\"DSB_features.shape is {DSB_features.shape}\")","633f282a":"# List of top-performig CatBoost features\n# identified via experiments\nTOP_CB400_FEATURES_LIST=['title',\n 'EXTR: accuracy_group for preceding sessions mean',\n 'EXTR: accuracy_group for preceding sessions median',\n 'world',\n 'event_code 4100 preceding sessions var mean for accuracy_group',\n 'World TREETOPCITY preceding sessions var mean for accuracy_group',\n 'EXTR: misses for preceding sessions mean',\n 'event_code 2030 preceding sessions var mean for misses',\n 'World TREETOPCITY preceding sessions var median for accuracy_group',\n 'Event_code 4070 preceding sessions event count',\n 'event_code 4100 preceding sessions var median for accuracy_group',\n 'Type Game preceding sessions var mean for misses',\n 'event_code 4020 preceding sessions var median for duration',\n 'Event_id 27253bdc preceding sessions event_id count',\n 'event_code 3021 preceding sessions var mean for total_duration',\n 'preceding sessions mean event count per session',\n 'Type Assessment preceding sessions var mean for total_duration',\n 'Title Sandcastle Builder (Activity) preceding sessions var mean for total_duration',\n 'event_code 2030 preceding sessions var median for duration',\n 'Event_code 2000 preceding sessions event count',\n 'event_id 1bb5fbdb preceding sessions var mean for duration',\n 'preceding sessions max duration',\n 'Type Assessment preceding sessions var median for duration',\n 'event_id 1bb5fbdb preceding sessions var median for duration',\n 'Type Game preceding sessions var median for round',\n 'event_id 84538528 preceding sessions var median for duration',\n 'Event_id 7372e1a5 preceding sessions event_id count',\n 'Title Sandcastle Builder (Activity) preceding sessions var median for total_duration',\n 'Type Activity preceding sessions median event count',\n 'Type Assessment preceding sessions var mean for duration',\n 'event_code 2030 preceding sessions var mean for duration',\n 'Title All Star Sorting preceding sessions var mean for total_duration',\n 'event_code 4035 preceding sessions var median for duration',\n 'event_code 3121 preceding sessions var mean for duration',\n 'event_code 4020 preceding sessions var mean for duration',\n 'event_code 3120 preceding sessions var median for duration',\n 'event_code 3121 preceding sessions var median for duration',\n 'event_id 5a848010 preceding sessions var mean for duration',\n 'Title Chest Sorter (Assessment) preceding sessions var median for accuracy_group',\n 'Title Chest Sorter (Assessment) preceding sessions var mean for accuracy_group',\n 'Type Assessment preceding sessions var median for total_duration',\n 'Type Game preceding sessions median duration',\n 'event_code 3121 preceding sessions var max for duration',\n 'event_code 3020 preceding sessions var mean for total_duration',\n 'World MAGMAPEAK preceding sessions var mean for misses',\n 'event_code 3120 preceding sessions var total for duration',\n 'Title Scrub-A-Dub preceding sessions var mean for size',\n 'Title Happy Camel preceding sessions var mean for misses',\n 'World MAGMAPEAK preceding sessions var mean for total_duration',\n 'EXTR: duration for preceding sessions median',\n 'event_code 4070 preceding sessions var count for round',\n 'event_code 3121 preceding sessions var total for duration',\n 'Type Assessment preceding sessions mean event count',\n 'EXTR: total_duration for preceding sessions median',\n 'Type Assessment preceding sessions median duration',\n 'Title Dino Drink preceding sessions var median for total_duration',\n 'Type Game preceding sessions median event count',\n 'Title Fireworks (Activity) preceding sessions var mean for total_duration',\n 'Type Activity preceding sessions median duration',\n 'World CRYSTALCAVES preceding sessions var mean for total_duration',\n 'Type Activity preceding sessions var median for total_duration',\n 'Title Chest Sorter (Assessment) preceding sessions var max for accuracy_group',\n 'event_code 4025 preceding sessions var median for duration',\n 'Type Activity preceding sessions var median for duration',\n 'EXTR: total_duration for preceding sessions mean',\n 'event_code 3020 preceding sessions var median for total_duration',\n 'Title Mushroom Sorter (Assessment) preceding sessions var median for accuracy_group',\n 'event_code 3110 preceding sessions var mean for duration',\n 'event_code 4035 preceding sessions var max for duration',\n 'World TREETOPCITY preceding sessions var median for duration',\n 'event_code 3110 preceding sessions var max for duration',\n 'World CRYSTALCAVES preceding sessions mean event count',\n 'event_code 3010 preceding sessions var mean for total_duration',\n 'event_id 84538528 preceding sessions var mean for duration',\n 'Type Game preceding sessions var mean for round',\n 'Title Flower Waterer (Activity) preceding sessions var median for duration',\n 'Type Activity preceding sessions mean event count',\n 'Title Bird Measurer (Assessment) preceding sessions var mean for accuracy_group',\n 'event_code 3120 preceding sessions var mean for duration',\n 'World CRYSTALCAVES preceding sessions var median for duration',\n 'Type Assessment preceding sessions median event count',\n 'Title Crystal Caves - Level 3 preceding sessions total count',\n 'Title Happy Camel preceding sessions var median for misses',\n 'event_code 3110 preceding sessions var median for duration',\n 'Type Activity preceding sessions var mean for total_duration',\n 'event_code 4035 preceding sessions var mean for duration',\n 'event_id 4ef8cdd3 preceding sessions var median for duration',\n 'preceding sessions max event count per session',\n 'World CRYSTALCAVES preceding sessions var median for total_duration',\n 'Title Chest Sorter (Assessment) preceding sessions var mean for total_duration',\n 'preceding sessions mean duration',\n 'event_code 4035 preceding sessions var count for duration',\n 'Title Scrub-A-Dub preceding sessions var mean for duration',\n 'World CRYSTALCAVES preceding sessions median event count',\n 'event_code 3010 preceding sessions var median for total_duration',\n 'Title Mushroom Sorter (Assessment) preceding sessions var mean for accuracy_group',\n 'event_id 0d1da71f preceding sessions var median for duration',\n 'event_code 4020 preceding sessions var mean for size',\n 'World CRYSTALCAVES preceding sessions var mean for misses',\n 'EXTR: duration for preceding sessions mean',\n 'preceding sessions total duration',\n 'event_id f7e47413 preceding sessions var median for duration',\n 'EXTR: round for preceding sessions median',\n 'event_id 26fd2d99 preceding sessions var mean for size',\n 'event_id 04df9b66 preceding sessions var total for total_duration',\n 'World TREETOPCITY preceding sessions total count',\n 'Title Air Show preceding sessions var mean for duration',\n 'World TREETOPCITY preceding sessions max duration',\n 'Type Activity preceding sessions var mean for duration',\n 'ASSESSMENT_BAD_ATTEMPT total attempt event count in title Cauldron Filler (Assessment)',\n 'event_code 4035 preceding sessions var total for duration',\n 'Type Assessment preceding sessions max duration',\n 'Type Assessment preceding sessions mean duration',\n 'Event_code 4025 preceding sessions event count',\n 'Event_code 4035 preceding sessions event count',\n 'Title Crystal Caves - Level 3 preceding sessions max event count',\n 'preceding sessions median duration',\n 'event_code 4020 preceding sessions var total for duration',\n 'Type Assessment preceding sessions var median for session_duration',\n 'World TREETOPCITY preceding sessions max event count',\n 'World CRYSTALCAVES preceding sessions total count',\n 'event_id e694a35b preceding sessions var median for duration',\n 'Event_code 4030 preceding sessions event count',\n 'event_id 47026d5f preceding sessions var mean for total_duration',\n 'Title Bug Measurer (Activity) preceding sessions var mean for total_duration',\n 'event_code 3021 preceding sessions var total for total_duration',\n 'event_code 2030 preceding sessions var max for duration',\n 'preceding sessions median event count per session',\n 'World CRYSTALCAVES preceding sessions mean duration',\n 'event_id b88f38da preceding sessions var median for duration',\n 'event_code 4020 preceding sessions var max for duration',\n 'World CRYSTALCAVES preceding sessions var mean for duration',\n 'Title Cauldron Filler (Assessment) preceding sessions var median for duration',\n 'EXTR: session_duration for preceding sessions median',\n 'Type Assessment preceding sessions total duration',\n 'Event_code 3120 preceding sessions event count',\n 'Title Cart Balancer (Assessment) preceding sessions median duration',\n 'event_id d185d3ea preceding sessions var max for duration',\n 'event_id 562cec5f preceding sessions var median for duration',\n 'Title Chest Sorter (Assessment) preceding sessions var max for total_duration',\n 'event_code 3020 preceding sessions var total for total_duration',\n 'Title Fireworks (Activity) preceding sessions var median for total_duration',\n 'event_code 4020 preceding sessions var count for duration',\n 'Type Assessment preceding sessions var total for duration',\n 'EXTR: stage_number for preceding sessions mean',\n 'event_id 4a09ace1 preceding sessions var mean for duration',\n 'Type Game preceding sessions var median for duration',\n 'event_id 5290eab1 preceding sessions var total for duration',\n 'event_id e4f1efe6 preceding sessions var median for duration',\n 'EXTR: size for preceding sessions mean',\n 'Event_code 4020 preceding sessions event count',\n 'event_id 7372e1a5 preceding sessions var total for round',\n 'World MAGMAPEAK preceding sessions var median for duration',\n 'Type Activity preceding sessions mean duration',\n 'Event_id 1325467d preceding sessions event_id count',\n 'Title Cauldron Filler (Assessment) preceding sessions var mean for total_duration',\n 'event_id c58186bf preceding sessions var total for duration',\n 'Type Game preceding sessions var mean for duration',\n 'event_code 4070 preceding sessions var total for round',\n 'Title Crystal Caves - Level 2 preceding sessions total count',\n 'event_code 4022 preceding sessions var median for duration',\n 'Event_code 3121 preceding sessions event count',\n 'event_code 2020 preceding sessions var median for round',\n 'event_id e4f1efe6 preceding sessions var max for duration',\n 'World MAGMAPEAK preceding sessions var max for duration',\n 'preceding sessions total event count',\n 'Type Game preceding sessions var median for total_duration',\n 'World MAGMAPEAK preceding sessions median event count',\n 'event_code 4040 preceding sessions var median for duration',\n 'event_code 3110 preceding sessions var total for duration',\n 'Type Game preceding sessions var mean for total_duration',\n 'EXTR: session_duration for preceding sessions max',\n 'Type Game preceding sessions mean duration',\n 'event_id 30614231 preceding sessions var median for duration',\n 'Event_code 3020 preceding sessions event count',\n 'event_code 3010 preceding sessions var total for total_duration',\n 'EXTR: duration for preceding sessions count',\n 'Title Chow Time preceding sessions var median for misses',\n 'Title Chest Sorter (Assessment) preceding sessions var median for duration',\n 'event_code 2030 preceding sessions var total for duration',\n 'Type Game preceding sessions mean event count',\n 'Type Game preceding sessions var mean for size',\n 'event_code 2020 preceding sessions var mean for stage_number',\n 'World MAGMAPEAK preceding sessions total count',\n 'event_id 3afb49e6 preceding sessions var mean for total_duration',\n 'World TREETOPCITY preceding sessions var mean for total_duration',\n 'event_id 5f0eb72c preceding sessions var median for duration',\n 'event_code 2030 preceding sessions var count for misses',\n 'Title Chest Sorter (Assessment) preceding sessions var total for accuracy_group',\n 'EXTR: total_duration for preceding sessions count',\n 'World TREETOPCITY preceding sessions var median for total_duration',\n 'Event_code 4090 preceding sessions event count',\n 'event_id 4ef8cdd3 preceding sessions var mean for duration',\n 'Event_id 04df9b66 preceding sessions event_id count',\n 'World TREETOPCITY preceding sessions var mean for duration',\n 'Title Chow Time preceding sessions var mean for duration',\n 'Type Assessment preceding sessions var total for session_duration',\n 'Title Chow Time preceding sessions var mean for misses',\n 'World TREETOPCITY preceding sessions median event count',\n 'EXTR: duration for preceding sessions max',\n 'event_id f71c4741 preceding sessions var mean for total_duration',\n 'event_id 15f99afc preceding sessions var mean for duration',\n 'Title Chow Time preceding sessions var mean for target_weight',\n 'event_id e4f1efe6 preceding sessions var mean for duration',\n 'Type Assessment preceding sessions var max for duration',\n 'event_code 4040 preceding sessions var mean for duration',\n 'Title Sandcastle Builder (Activity) preceding sessions var mean for size',\n 'Type Assessment preceding sessions var max for total_duration',\n 'Event_code 3110 preceding sessions event count',\n 'Type Game preceding sessions max duration',\n 'World MAGMAPEAK preceding sessions var mean for duration',\n 'World MAGMAPEAK preceding sessions median duration',\n 'event_id c74f40cd preceding sessions var mean for duration',\n 'Title All Star Sorting preceding sessions var mean for size',\n 'event_code 2035 preceding sessions var median for duration',\n 'Title Crystal Caves - Level 1 preceding sessions total count',\n 'event_id 38074c54 preceding sessions var total for duration',\n 'event_code 3021 preceding sessions var median for total_duration',\n 'event_code 3120 preceding sessions var max for duration',\n 'event_id ca11f653 preceding sessions var median for duration',\n 'event_code 4025 preceding sessions var mean for duration',\n 'event_id 56817e2b preceding sessions var median for duration',\n 'event_id 923afab1 preceding sessions var median for total_duration',\n 'World TREETOPCITY preceding sessions var mean for misses',\n 'EXTR: misses for preceding sessions count',\n 'event_id c51d8688 preceding sessions var total for duration',\n 'World TREETOPCITY preceding sessions total event count',\n 'event_id df4940d3 preceding sessions var max for duration',\n 'Type Assessment preceding sessions var mean for misses',\n 'EXTR: session_duration for preceding sessions mean',\n 'Title Cauldron Filler (Assessment) preceding sessions var mean for duration',\n 'event_id 5e812b27 preceding sessions var mean for size',\n 'Title Cart Balancer (Assessment) preceding sessions var median for duration',\n 'event_id 4ef8cdd3 preceding sessions var max for duration',\n 'EXTR: session_duration for preceding sessions total',\n 'event_id d185d3ea preceding sessions var mean for duration',\n 'event_id 3afb49e6 preceding sessions var total for total_duration',\n 'Title All Star Sorting preceding sessions var median for duration',\n 'World MAGMAPEAK preceding sessions mean event count',\n 'Title Mushroom Sorter (Assessment) preceding sessions median event count',\n 'Type Assessment preceding sessions max event count',\n 'event_id c58186bf preceding sessions var max for duration',\n 'Title Tree Top City - Level 3 preceding sessions total count',\n 'World TREETOPCITY preceding sessions mean duration',\n 'Type Activity preceding sessions total duration',\n 'Title Cart Balancer (Assessment) preceding sessions var mean for duration',\n 'Title Tree Top City - Level 1 preceding sessions total count',\n 'Title Bird Measurer (Assessment) preceding sessions var median for accuracy_group',\n 'Title Pan Balance preceding sessions var median for total_duration',\n 'Previous session had a title of',\n 'Event_code 3021 preceding sessions event count',\n 'World CRYSTALCAVES preceding sessions total event count',\n 'EXTR: target_weight for preceding sessions mean',\n 'Title Chest Sorter (Assessment) preceding sessions var total for session_duration',\n 'Title Tree Top City - Level 2 preceding sessions total count',\n 'Event_id 5290eab1 preceding sessions event_id count',\n 'event_code 4030 preceding sessions var mean for round',\n 'Type Activity preceding sessions var max for duration',\n 'event_id 38074c54 preceding sessions var median for duration',\n 'EXTR: total_duration for preceding sessions total',\n 'event_id 2dcad279 preceding sessions var median for duration',\n 'Title All Star Sorting preceding sessions var mean for misses',\n 'Title Chest Sorter (Assessment) preceding sessions var median for session_duration',\n 'World TREETOPCITY preceding sessions mean event count',\n 'Title Fireworks (Activity) preceding sessions var median for duration',\n 'event_id e9c52111 preceding sessions var median for duration',\n 'event_id 28520915 preceding sessions var median for duration',\n 'event_id 3afb49e6 preceding sessions var max for total_duration',\n 'Title Cauldron Filler (Assessment) preceding sessions var mean for misses',\n 'World CRYSTALCAVES preceding sessions median duration',\n 'Type Assessment preceding sessions var mean for session_duration',\n 'event_code 4035 preceding sessions var count for size',\n 'Type Assessment preceding sessions total event count',\n 'event_id 6cf7d25c preceding sessions var mean for total_duration',\n 'event_id 9d29771f preceding sessions var mean for total_duration',\n 'Title Crystals Rule preceding sessions var mean for misses',\n 'event_id d3f1e122 preceding sessions var median for duration',\n 'World MAGMAPEAK preceding sessions var median for total_duration',\n 'Type Activity preceding sessions max duration',\n 'event_code 2035 preceding sessions var mean for duration',\n 'Event_code 4100 preceding sessions event count',\n 'Title Cart Balancer (Assessment) preceding sessions median event count',\n 'EXTR: buglength for preceding sessions median',\n 'Title Chest Sorter (Assessment) preceding sessions var mean for session_duration',\n 'event_code 4022 preceding sessions var mean for duration',\n 'Title Chow Time preceding sessions var median for duration',\n 'event_id 38074c54 preceding sessions var max for duration',\n 'Event_id 7da34a02 preceding sessions event_id count',\n 'event_code 3021 preceding sessions var max for total_duration',\n 'Type Assessment preceding sessions var total for total_duration',\n 'event_id df4940d3 preceding sessions var median for duration',\n 'event_id 0d1da71f preceding sessions var mean for duration',\n 'event_code 4070 preceding sessions var mean for round',\n 'Type Activity preceding sessions max event count',\n 'Title Chest Sorter (Assessment) preceding sessions var mean for duration',\n 'Title Chest Sorter (Assessment) preceding sessions var max for session_duration',\n 'Event_id 3ee399c3 preceding sessions event_id count',\n 'EXTR: duration for preceding sessions total',\n 'event_code 4021 preceding sessions var median for duration',\n 'Event_code 4040 preceding sessions event count',\n 'Type Activity preceding sessions var count for duration',\n 'ASSESSMENT_GOOD_ATTEMPT total attempt event count in title Chest Sorter (Assessment)',\n 'event_code 4030 preceding sessions var mean for size',\n 'Event_code 3010 preceding sessions event count',\n 'Title Watering Hole (Activity) preceding sessions var mean for total_duration',\n 'event_id 5c2f29ca preceding sessions var mean for duration',\n 'event_id 5290eab1 preceding sessions var median for duration',\n 'event_code 2020 preceding sessions var mean for target_weight',\n 'event_id 88d4a5be preceding sessions var median for duration',\n 'event_id 9d29771f preceding sessions var median for total_duration',\n 'World CRYSTALCAVES preceding sessions max event count',\n 'Title Chicken Balancer (Activity) preceding sessions var median for total_duration',\n 'Event_id 3afb49e6 preceding sessions event_id count',\n 'Title Sandcastle Builder (Activity) preceding sessions var mean for duration',\n 'event_id 709b1251 preceding sessions var mean for duration',\n 'event_id 3323d7e9 preceding sessions var median for duration',\n 'event_id ac92046e preceding sessions var mean for duration',\n 'event_id 5290eab1 preceding sessions var mean for duration',\n 'Title Egg Dropper (Activity) preceding sessions var median for total_duration',\n 'event_id 15f99afc preceding sessions var median for duration',\n 'event_id e4f1efe6 preceding sessions var total for duration',\n 'event_code 2030 preceding sessions var mean for stage_number',\n 'event_id b5053438 preceding sessions var median for duration',\n 'Title Crystal Caves - Level 1 preceding sessions max event count',\n 'Title Scrub-A-Dub preceding sessions var median for duration',\n 'Event_id 884228c8 preceding sessions event_id count',\n 'event_id 828e68f9 preceding sessions var median for duration',\n 'Type Assessment preceding sessions var count for session_duration',\n 'Title Mushroom Sorter (Assessment) preceding sessions mean event count',\n 'Event_id acf5c23f preceding sessions event_id count',\n 'ASSESSMENT_BAD_ATTEMPT total attempt count',\n 'event_code 3010 preceding sessions var mean for round',\n 'Title Magma Peak - Level 2 preceding sessions total count',\n 'event_id 598f4598 preceding sessions var median for duration',\n 'World CRYSTALCAVES preceding sessions var mean for accuracy_group',\n 'EXTR: session_duration for preceding sessions count',\n 'event_code 2020 preceding sessions var mean for size',\n 'event_id 160654fd preceding sessions var mean for total_duration',\n 'event_id a1bbe385 preceding sessions var mean for duration',\n 'Event_id 565a3990 preceding sessions event_id count',\n 'event_id 5290eab1 preceding sessions var max for duration',\n 'Title Chicken Balancer (Activity) preceding sessions var median for duration',\n 'Type Game preceding sessions total duration',\n 'Event_id a44b10dc preceding sessions event_id count',\n 'World MAGMAPEAK preceding sessions mean duration',\n 'event_id 828e68f9 preceding sessions var mean for duration',\n 'event_id 84538528 preceding sessions var mean for size',\n 'event_code 4021 preceding sessions var mean for size',\n 'event_id a52b92d5 preceding sessions var mean for duration',\n 'event_id 30614231 preceding sessions var mean for duration',\n 'event_code 3121 preceding sessions var mean for round',\n 'EXTR: round for preceding sessions mean',\n 'event_id 5c2f29ca preceding sessions var median for duration',\n 'World TREETOPCITY preceding sessions total duration',\n 'Type Activity preceding sessions var total for total_duration',\n 'Type Activity preceding sessions var total for duration',\n 'Title Cart Balancer (Assessment) preceding sessions var mean for accuracy_group',\n 'Title Mushroom Sorter (Assessment) preceding sessions var median for misses',\n 'event_code 3021 preceding sessions var mean for round',\n 'event_id 923afab1 preceding sessions var mean for total_duration',\n 'event_id b88f38da preceding sessions var mean for duration',\n 'Type Assessment preceding sessions var count for misses',\n 'Title Chicken Balancer (Activity) preceding sessions var mean for total_duration',\n 'event_id 2dcad279 preceding sessions var mean for duration',\n 'event_id e694a35b preceding sessions var max for duration',\n 'Type Activity preceding sessions var count for total_duration',\n 'EXTR: distance for preceding sessions mean',\n 'Title Chest Sorter (Assessment) preceding sessions var max for duration',\n 'event_id e694a35b preceding sessions var mean for duration',\n 'Title Egg Dropper (Activity) preceding sessions var median for duration',\n 'event_code 4040 preceding sessions var total for duration',\n 'event_code 4035 preceding sessions var mean for round',\n 'event_id 0330ab6a preceding sessions var mean for total_duration',\n 'event_code 2070 preceding sessions var median for duration',\n 'event_code 2030 preceding sessions var median for stage_number',\n 'event_id 0db6d71d preceding sessions var total for duration',\n 'World CRYSTALCAVES preceding sessions max duration',\n 'Title Flower Waterer (Activity) preceding sessions median duration',\n 'event_code 3020 preceding sessions var median for round',\n 'Title Cauldron Filler (Assessment) preceding sessions median event count',\n 'Title Mushroom Sorter (Assessment) preceding sessions var mean for duration',\n 'event_id 3bf1cf26 preceding sessions var max for duration',\n 'event_code 4045 preceding sessions var mean for round',\n 'event_code 4030 preceding sessions var count for round',\n 'event_code 4090 preceding sessions var count for round',\n 'event_id ecaab346 preceding sessions var total for duration',\n 'EXTR: stage_number for preceding sessions max',\n 'World MAGMAPEAK preceding sessions var mean for size',\n 'event_id 71fe8f75 preceding sessions var mean for duration',\n 'Event_id 363c86c9 preceding sessions event_id count',\n 'World MAGMAPEAK preceding sessions max duration',\n 'event_code 2020 preceding sessions var max for stage_number',\n 'event_id 2230fab4 preceding sessions var mean for duration',\n 'Title Cauldron Filler (Assessment) preceding sessions var median for total_duration',\n 'Title Chest Sorter (Assessment) preceding sessions median duration',\n 'Title Cart Balancer (Assessment) preceding sessions var median for session_duration',\n 'event_id 6cf7d25c preceding sessions var median for total_duration',\n 'Title Egg Dropper (Activity) preceding sessions var mean for total_duration',\n 'Title Cart Balancer (Assessment) preceding sessions mean event count',\n 'Title Mushroom Sorter (Assessment) preceding sessions var median for duration']\n\nassert len(TOP_CB400_FEATURES_LIST)==400","ebcba101":"def build_X(sessions):\n    return copy.deepcopy(DSB_features[DSB_features.index.isin(sessions)])","b593da36":"def build_Y(sessions):\n    all_assessments_df=pd.concat([DSB_train_d[DSB_train_d.accuracy_group.notnull()],\n                                  DSB_test_d[DSB_test_d.accuracy_group.notnull()]],\n                                  ignore_index=True,sort=True)\n    result=all_assessments_df[all_assessments_df\n                              .game_session\n                              .isin(sessions)\n                             ][[\"game_session\",\"accuracy_group\"]]\n    result.set_index(\"game_session\" ,inplace=True)\n    result.sort_index(inplace=True)\n    \n    return result[\"accuracy_group\"] ","f567b46a":"# An LGBM+CatBoost model created for using with\n# DSB_cross_val_score function\nclass DSB_CBpLGBM_Regressor:\n    \n    params = {'n_estimators':1250\n            ,'boosting_type': 'gbdt'\n            ,'objective': 'regression'\n            ,'metric': 'rmse'\n            ,'subsample': 0.75\n            ,'subsample_freq': 1\n            ,'learning_rate': 0.01\n            ,'feature_fraction': 0.9\n            ,'max_depth': 15\n            ,'lambda_l1': 1  \n            ,'lambda_l2': 1\n            }\n        \n    def __init__(self, **other_args):\n        if(\"loss_function\" not in other_args): other_args[\"loss_function\"]=\"RMSE\"\n        if(\"verbose\" not in other_args): other_args[\"verbose\"]=False\n        if(\"iterations\" not in other_args): other_args[\"iterations\"]=1250\n        self.CBmodel = CatBoostRegressor(**other_args)\n        \n        self.mapping_thresholds={}\n\n    def fit(self,inp_X,y, **other_args):\n        \n        X=copy.deepcopy(inp_X)\n        X.columns=[\"\".join([c if c.isalnum() else \"_\" for c in str(x)]) for x in inp_X.columns]\n        \n        if(\"categorical_feature\" not in other_args):\n            other_args[\"categorical_feature\"]=[]\n            dt=pd.DataFrame(X.dtypes)\n            for i in dt.index:\n                if dt.loc[i][0].name==\"category\":\n                    other_args[\"categorical_feature\"\n                              ].append(dt.loc[i].name)\n        \n        if len(other_args[\"categorical_feature\"])==0:\n            del other_args[\"categorical_feature\"]\n        \n        lgbm_train_XY=lgb.Dataset(copy.deepcopy(X)\n                                  ,label=copy.deepcopy(y)\n                                  ,**other_args)\n        \n        self.LGBModel=lgb.train(self.params\n                                ,lgbm_train_XY\n                                , **other_args)\n    \n        LGBM_regr_predicted_y=self.LGBModel.predict(X)\n         \n        if \"categorical_feature\" in other_args:\n            other_args[\"cat_features\"]=other_args[\"categorical_feature\"]\n            del other_args[\"categorical_feature\"]   \n            \n        self.CBmodel.fit(X,y, **other_args)\n        CB_regr_predicted_y=self.CBmodel.predict(X)\n                        \n        regr_predicted_y=(CB_regr_predicted_y+LGBM_regr_predicted_y)\/2.0\n                           \n        actuals_counts=y.value_counts()\n        \n        sorted_predictions=sorted(list(regr_predicted_y))\n        self.mapping_thresholds[0]=(sorted_predictions[actuals_counts[0]-1]\n                                    +sorted_predictions[actuals_counts[0]])\/2.0\n        self.mapping_thresholds[1]=(sorted_predictions[actuals_counts[0]+actuals_counts[1]-1]\n                                    +sorted_predictions[actuals_counts[0]+actuals_counts[1]])\/2.0\n        \n        self.mapping_thresholds[2]=(sorted_predictions[actuals_counts[0]+actuals_counts[1]+actuals_counts[2]-1]\n                                    +sorted_predictions[actuals_counts[0]+actuals_counts[1]+actuals_counts[2]])\/2.0\n        \n        self.feature_importances_=[] #self.LGBModel.feature_importances_\n        self.feature_names_=[] #self.LGBModel.feature_names_\n                                    \n        \n    def regr_mapper(self,x):\n        if x <= self.mapping_thresholds[0]:\n            return 0\n        elif x <= self.mapping_thresholds[1]:\n            return 1\n        elif x <= self.mapping_thresholds[2]:\n            return 2\n        else:\n            return 3\n    \n    def predict(self,inp_X,**other_args):\n        assert len(self.mapping_thresholds)==3\n        X=copy.deepcopy(inp_X)\n        X.columns=[\"\".join([c if c.isalnum() else \"_\" for c in str(x)]) for x in inp_X.columns]\n        cb_predicted_y=self.CBmodel.predict(X,**other_args)\n        lgbm_predicted_y=self.LGBModel.predict(X,**other_args)\n        predicted_y=(cb_predicted_y+lgbm_predicted_y)\/2.0\n        results=[self.regr_mapper(p) for p in predicted_y]\n        return results","77de4f3e":"# Calculation of the quadratic weighted kappa\n# Source code from\n# https:\/\/www.kaggle.com\/cpmpml\/ultra-fast-qwk-calc-method\ndef qwk3(a1, a2, max_rat=3):\n    assert(len(a1) == len(a2))\n    a1 = np.asarray(a1, dtype=int)\n    a2 = np.asarray(a2, dtype=int)\n\n    hist1 = np.zeros((max_rat + 1, ))\n    hist2 = np.zeros((max_rat + 1, ))\n\n    o = 0\n    for k in range(a1.shape[0]):\n        i, j = a1[k], a2[k]\n        hist1[i] += 1\n        hist2[j] += 1\n        o +=  (i - j) * (i - j)\n\n    e = 0\n    for i in range(max_rat + 1):\n        for j in range(max_rat + 1):\n            e += hist1[i] * hist2[j] * (i - j) * (i - j)\n\n    e = e \/ a1.shape[0]\n\n    return (1 - o \/ e)","04a54f6a":"# Ground truth for evaluating\/comparing our models before submitting \n# the models to Kaggle.\n# We expect that in a DSB-2019 competition\n# a local CV score should be more trustworthy than a Kaggle public LB score\n# Details: https:\/\/www.kaggle.com\/c\/data-science-bowl-2019\/discussion\/121828#696708\ndef DSB_cross_val_score(X_full_df                   # all the features for all the sessions\n                        ,Y_full                     # target values for all the sessions\n                        ,all_inst_ids_df            # DataFrame that maps \n                                                    # game_session-s to installation_id-s \n                        ,training_sessions_list     # game_session-s that can be \n                                                    # used for training\n                        ,validation_sessions_list   # game_session-s that can be \n                                                    # used for validation\n                                                    # Recommended strategy is \n                                                    # to always provide here\n                                                    # \"last_w_outcomes\" sessions \n                        ,the_model                  # a model to be used to fit\/predict \n                                                    # while calculating CV score\n                        ,verbose=PRINT_LOGS         # should the function print \n                                                    # logging info during its execution?\n                        ,consolidate_feature_rangking=True\n                        ,NUM_CIRCLES=10  \n                        ,NUM_FOLDS=3  \n                       ):\n    \n    \n    if verbose:\n        print(f\"\\nCalculating CV score via {NUM_CIRCLES*NUM_FOLDS} iterations. \\n\"\n              ,f\"X.shape is {X_full_df.shape} \\n\"\n              ,f\"training_sessions_list has {len(training_sessions_list)} items,\"\n              ,f\" validation_sessions_list has {len(validation_sessions_list)} items.\"\n              ,f\"Starting time: {datetime.now().strftime('%X')} \\n\")\n    \n    assert len(X_full_df)==len(Y_full)\n    assert len(X_full_df)<=all_inst_ids_df.game_session.nunique()\n    assert X_full_df.index.name==\"game_session\"\n    \n    training_inst_ids_list=all_inst_ids_df[all_inst_ids_df\n                                           .game_session\n                                           .isin(training_sessions_list)\n                                          ].installation_id.unique()\n    \n    validation_inst_ids_list=all_inst_ids_df[all_inst_ids_df\n                                             .game_session\n                                             .isin(validation_sessions_list)\n                                            ].installation_id.unique()\n\n    assert set(training_inst_ids_list)==set(validation_inst_ids_list)\n    \n    cv_inst_ids_list=training_inst_ids_list\n    cv_inst_ids_df=all_inst_ids_df[all_inst_ids_df\n                                   .installation_id\n                                   .isin(cv_inst_ids_list)]\n    \n    model_to_evaluate=copy.deepcopy(the_model)\n    all_scores_list=[]              \n    \n    all_feature_ranks=[]\n    \n    i=0\n    for n in range(NUM_CIRCLES):\n        \n        skf = KFold(n_splits=NUM_FOLDS,shuffle=True)\n        \n        for train_index, test_index in skf.split(cv_inst_ids_df):\n            \n            train_intst_ids_df=cv_inst_ids_df.iloc[train_index]\n            train_intst_ids_df=train_intst_ids_df[train_intst_ids_df\n                                                  .game_session\n                                                  .isin(training_sessions_list)]\n              \n            test_intst_ids_df=cv_inst_ids_df.iloc[test_index]\n            test_intst_ids_df=test_intst_ids_df[test_intst_ids_df\n                                                .game_session\n                                                .isin(validation_sessions_list)]  \n            \n            train_game_session_list=train_intst_ids_df.game_session.unique()\n            test_game_session_list=test_intst_ids_df.game_session.unique()\n            \n            train_index=X_full_df.index.isin(train_game_session_list)\n            test_index=X_full_df.index.isin(test_game_session_list)  \n              \n            model_to_evaluate.fit(X_full_df[train_index],Y_full[train_index])\n            \n            if consolidate_feature_rangking:\n                new_feature_ranking=pd.DataFrame(model_to_evaluate.feature_importances_\n                                               , index=model_to_evaluate.feature_names_\n                                               , columns=[\"importance\"]\n                                              ).sort_values(by=\"importance\"\n                                                            , ascending=False)\n                all_feature_ranks.append(new_feature_ranking)\n              \n              \n              \n            predictions=model_to_evaluate.predict(X_full_df[test_index])\n            a_score=qwk3(predictions,Y_full[test_index])\n            all_scores_list.append(a_score)\n            \n            if verbose:\n                i+=1\n                print(f\"Iteration {i} out of {NUM_CIRCLES*NUM_FOLDS},\"\n                      ,f\" score: {a_score:.4} .\"\n                      ,f\" Cumulative mean score: {np.mean(all_scores_list):.4} .\"\n                      ,f\" Time: {datetime.now().strftime('%X')}\"\n                     )\n            \n    result={}\n    \n    result[\"score_mean\"]=np.mean(all_scores_list)\n    result[\"score_std\"]=np.std(all_scores_list)\n    result[\"all_scores\"]=all_scores_list\n    \n    if consolidate_feature_rangking:\n        result[\"all_feature_ranks\"]=all_feature_ranks\n    \n    if verbose:\n        print(f\"\\nSCORE MEAN : {result['score_mean']:.4} .\")\n        print(f\"MIN SCORE: {min(result['all_scores']):.4}\" , f\"MAX SCORE: {max(result['all_scores']):.4}.\")\n        print(f\"SCORE STANDARD DEVIATION: {result['score_std']:.5} . \\n\")    \n              \n    return result","82ece162":"%%time\nbuild_features(features_to_keep=TOP_CB400_FEATURES_LIST)\n\ntrain_X=build_X(DSB_train_assessments[\"all_w_outcomes\"]\n                +DSB_test_assessments[\"all_w_outcomes\"])\n\ntrain_Y=build_Y(DSB_train_assessments[\"all_w_outcomes\"]\n                +DSB_test_assessments[\"all_w_outcomes\"])\n\ntrain_X[\"Y\"]=train_Y\ndel train_Y\ntrain_Y=train_X[\"Y\"]\ndel train_X[\"Y\"]\n\ntest_X=build_X(DSB_test_assessments[\"last\"])\n\nprint(f'train_X shape: {train_X.shape}')\nprint(f'train_Y shape: {train_Y.shape}')\nprint(f'test_X shape: {test_X.shape}')","e0996f92":"%%time\nif LOCAL_EXECUTION:\n    train_installation_ids=DSB_train_d[[\"game_session\",\"installation_id\"]].drop_duplicates()\n    test_installation_ids=DSB_test_d[[\"game_session\",\"installation_id\"]].drop_duplicates()\n    installation_ids=pd.concat([train_installation_ids,test_installation_ids], ignore_index=True)\n\n    new_model=DSB_CBpLGBM_Regressor()\n\n    s=DSB_cross_val_score(train_X,train_Y,installation_ids\n                          ,(DSB_train_assessments[\"all_w_outcomes\"]\n                            +DSB_test_assessments[\"all_w_outcomes\"])\n                          ,(DSB_train_assessments[\"last_w_outcomes\"]\n                            +DSB_test_assessments[\"last_w_outcomes\"])\n                          ,new_model, verbose=True\n                          ,NUM_CIRCLES=10 ,NUM_FOLDS=4)\n    s[\"score_mean\"],s[\"score_std\"]\n    \n    the_f=sum(s[\"all_feature_ranks\"]).sort_values(by=\"importance\", ascending=False)","da755873":"the_model=DSB_CBpLGBM_Regressor()\n\nsessions_to_train_on=DSB_train_assessments[\"all_w_outcomes\"]+DSB_test_assessments[\"all_w_outcomes\"]\n\nthe_model.fit(train_X[train_X.index.isin(sessions_to_train_on)]\n              ,train_Y[train_Y.index.isin(sessions_to_train_on)])\n\na_prediction=the_model.predict(train_X[train_X.index.isin(sessions_to_train_on)])\n\na_score=qwk3(a_prediction,train_Y[train_Y.index.isin(sessions_to_train_on)])\na_score","7f4afd38":"final_prediction=the_model.predict(test_X)\n\ntest_X[\"accuracy_group\"]=final_prediction\n\nresult=copy.deepcopy(test_X[[\"accuracy_group\"]])\nresult[\"accuracy_group\"]=result[\"accuracy_group\"].astype(\"int\")\nresult.index.name=\"g_s\"\nresult[\"game_session\"]=result.index\nresult[\"game_session\"]=result[\"game_session\"].astype(\"category\")\n\ninstallations=DSB_test_d.drop_duplicates(subset=[\"installation_id\"],keep=\"last\")\ninstallations=installations[[\"installation_id\",\"game_session\"]]\n\nr=result.merge(installations, on=\"game_session\")\n\nr[[\"installation_id\",\"accuracy_group\"]].to_csv(\"submission.csv\",index=False)","113c7841":"#### Variables","ec1ac933":"## 2 - Model Execution","49341a9d":"#### Functions","92ea04f8":"#### Functions","6e2c04bf":"### 1.4 Feature Selection\n\n400 most important features were selected for the final model. We ranked features through experiment with the Catboost model via local cross-validation. Note, permutation could have been a more general feature selection mechanism. However, since our final submission is based on Catboost and LightGBM, using important features ranked by the same model seems more relevant.","d6494b65":"## Import Libraries","214c90ee":"### 1.6 Model Evaluation","402096f8":"#### Variables","98a363e6":"#### Functions","d65d522d":"### 1.5 Model","60c46d43":"### 1.2 Data Processing","362dd1f2":"### 2.1 Process Train\/Test Data","80beab42":"#### Variables","4acded1b":"#### Functions","beb61b2e":"# Kaggle Compitition - 2019 Data Science Bowl\n\n## Executive Summary:\n### Introduction\nThe competition, sponsored by PBS KIDS, is to gain insights into how media can help children learn important skills for success in school and life. The participants use anonymous gameplay data from PBS KIDS Measure Up! App to develop models that predict scores on in-game assessments and create an algorithm that will lead to better-designed games and improved learning outcomes. Detailed description of the competition and data can be found on Kaggle throught the link below:\nhttps:\/\/www.kaggle.com\/c\/data-science-bowl-2019\n\n### Results\nThe best performing score was produced by the average of the predictions by CatBoost and LightGBM models. \n* Private Score: 0.543\n* Public Score: 0.544\n* Kaggle Rank: 167\/3497 (Silver Medal, Top 5%)\n\n### Approach\n**Data Preparation**: The data preparation includes data processing, feature engineering, and feature selection. \n- **Data Processing**: Removing assessment sessions without outcomes and calculating the accuracy group for the train data.\n- **Feature Engineering**: We generate over 20,000 features using information from the given numerical (e.g., time) and categorical features (e.g., event code, title, etc.) and statistics of the data extracted from the `event_data` column. Detail see [1.2 Data Processing](#1.2-Data-Processing). There were ~2000 unique features after removing the duplicate columns.\n- **Feature Selection**: 400 most important features were selected for the final model. We ranked features through experiments with the Catboost model via local cross-validation. Note, permutation could have been a more general feature selection mechanism. However, since our final submission is based on Catboost and LightGBM, using important features ranked by the same model seems more relevant. Details of the op features can be found in [1.4 Feature Selection](#1.4-Feature-Selection). \n\n**Modeling**: We evaluated linear model, logistic regression, random forest, neural network, Catboost, and LightGBM algorithms. Based on the local cross-validation performance, we decided to choose the hybrid model with Catboost and LightGBM (best local cross-validation score) and single LightGBM model (best public score) as the two final submissions. \n\n**Model Evaluation**: We are aware that the public LB could be misleading especially when the model overfits the testing data used to calculate the public score. Our model selection relied on the local cross-validation. We performed 3-Fold cross-validation so that the size of the validation set was close to the actual testing set. We set the unit of analysis to be `installation_id` that was used to partition the data during the cross-validation. Ten iterations (total 30 validatinos) were performed and the average scores were used as the metric for model selection.\n\n### Team Members\n* [Vlad Pavlov](https:\/\/www.kaggle.com\/vlpavlov)  \n* [Kai Zhao](https:\/\/www.kaggle.com\/miecky)\n\n## Notebook Structure\n* [1 - Functions & Variables](#1---Functions-&-Variables)\n    * [1.1 Load Data](#1.1-Load-Data)\n    * [1.2 Data Processing](#1.2-Data-Processing)\n    * [1.3 Feature Engineering](#1.3-Feature-Engineering)\n    * [1.4 Feature Selection](#1.4-Feature-Selection)\n    * [1.5 Model](#1.5-Model)\n    * [1.6 Model Evaluation](#1.6-Model-Evaluation)\n* [2 - Model Execution](#2---Model-Execution)\n    * [2.1 Process Train\/Test Data](#2.1-Process-Train\/Test-Data)\n    * [2.2 Run Model: Local](#2.2-Run-Model:-Local)\n    * [2.3 Run Model: Kaggle](#2.3-Run-Model:-Kaggle)","8306fe8d":"#### Functions","e380db61":"### 2.3 Run Model: Kaggle","7b66f474":"## 1 - Functions & Variables\nFunction were grouped into `load data`, `process data`, `feature engineering`, `model` and `model evaluation` modules. Their reltionship are entailed in the figure below.","42b7f8eb":"### 1.1 Load Data\n\nData will be loaded directly from Kaggle if running this notebook on Kaggle. Otherwise, data will be loaded from the local device.","86c69be7":"#### Variables","b8de424f":"### 1.3 Feature Engineering","d961ee93":"![DSB_chart.png](attachment:DSB_chart.png)","ff0f51ed":"### 2.2 Run Model: Local"}}