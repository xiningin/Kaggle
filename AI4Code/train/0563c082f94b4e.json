{"cell_type":{"9572a864":"code","cccd7361":"code","607311f6":"code","2201646b":"code","a3c24fd2":"code","9606c031":"code","452dc74d":"code","04c7a9a2":"code","4d468a57":"code","3451e617":"code","83c1a48a":"code","0b4d213e":"code","7fb80cfd":"code","50de272e":"code","682e3cc9":"code","edca42a3":"code","bbaa0900":"code","1761f9dc":"code","1ee7df8d":"code","cd505a7f":"code","2820d056":"code","d135c329":"code","7d465dac":"code","f651f2c6":"code","72e41d20":"code","bbdc1219":"code","f40faae3":"markdown","d9bccfed":"markdown","89ca6992":"markdown","030b1224":"markdown","99909bd4":"markdown","c00cd72e":"markdown","8a3b1b93":"markdown","d2274945":"markdown","b2ad7d62":"markdown","20abaa8d":"markdown","7dbf47dd":"markdown"},"source":{"9572a864":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cccd7361":"df = pd.read_csv('\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip')\nrslt_df = df[(df['toxic'] == 0) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nrslt_df2 = df[(df['toxic'] == 1) & (df['severe_toxic'] == 0) & (df['obscene'] == 0) & (df['threat'] == 0) & (df['insult'] == 0) & (df['identity_hate'] == 0)]\nnew1 = rslt_df[['id', 'comment_text', 'toxic']].iloc[:23000].copy() \nnew2 = rslt_df2[['id', 'comment_text', 'toxic']].iloc[:900].copy()\nnew = pd.concat([new1, new2], ignore_index=True)","607311f6":"new.head()","2201646b":"output=['toxic','severe_toxic','obscene','threat','insult','identity_hate']\n","a3c24fd2":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot(new.toxic)\nplt.xlabel('class')\nplt.title('Count of labels')","9606c031":"import nltk\ntk=nltk.tokenize.TreebankWordTokenizer()\ncomment_tokens = [tk.tokenize(sent) for sent in new['comment_text']]","452dc74d":"len(comment_tokens)","04c7a9a2":"from nltk.corpus import stopwords\nfor i in range(len(comment_tokens)):\n    comment_tokens[i] = [w for w in comment_tokens[i] if w not in stopwords.words('english')]","4d468a57":"# training own word2vec model\n\n# from gensim.models import Word2Vec\n\n# word2vec = Word2Vec(comment_tokens, min_count=1,size=300)\n\n# vocabulary = word2vec.wv.vocab\n\n# word2vec.save(\"my_word2vec.model\")\n\n\n# def document_vector(doc):\n#     \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n#     doc = [word for word in doc if word in word2vec.wv.vocab]\n#     print(doc)\n#     return np.mean(word2vec[doc], axis=0)\n\n# import pickle\n# with open(\"w2v_list.txt\", \"wb\") as fp:   #Pickling\n#     pickle.dump(list_w2v, fp)","3451e617":"# using pretrained model\nimport gensim\nmodel = gensim.models.KeyedVectors.load_word2vec_format('\/kaggle\/input\/nlpword2vecembeddingspretrained\/GoogleNews-vectors-negative300.bin', binary=True)","83c1a48a":"\nvocabulary = model.wv.vocab\nlen(vocabulary)","0b4d213e":"model.most_similar('fuck')","7fb80cfd":"# importing bokeh library for interactive dataviz\nimport bokeh.plotting as bp\nfrom bokeh.models import HoverTool, BoxSelectTool\nfrom bokeh.plotting import figure, show, output_notebook\n\n# defining the chart\noutput_notebook()\nplot_tfidf = bp.figure(plot_width=500, plot_height=400, title=\"A map of 10000 word vectors\",\n    tools=\"pan,wheel_zoom,box_zoom,reset,hover\",\n    x_axis_type=None, y_axis_type=None, min_border=1)\n\n# getting a list of word vectors. limit to 10000. each is of 300 dimensions\nword_vectors = [model[w] for w in list(model.wv.vocab.keys())[:5000]]\n\n# dimensionality reduction. converting the vectors to 2d vectors\nfrom sklearn.manifold import TSNE\ntsne_model = TSNE(n_components=2, verbose=1, random_state=0)\ntsne_w2v = tsne_model.fit_transform(word_vectors)\n\n# putting everything in a dataframe\ntsne_df = pd.DataFrame(tsne_w2v, columns=['x', 'y'])\ntsne_df['words'] = list(model.wv.vocab.keys())[:5000]\n\n# plotting. the corresponding word appears when you hover on the data point.\nplot_tfidf.scatter(x='x', y='y', source=tsne_df,color = 'darkcyan')\nhover = plot_tfidf.select(dict(type=HoverTool))\nhover.tooltips={\"word\": \"@words\"}\nshow(plot_tfidf)","50de272e":"# def document_vector(doc):\n#     \"\"\"Create document vectors by averaging word vectors. Remove out-of-vocabulary words.\"\"\"\n#     doc = [word for word in doc if word in model.wv.vocab]\n#     print(doc)\n#     return np.mean(model[doc], axis=0)","682e3cc9":"# comment_tokens","edca42a3":"documents = []\nfor x in comment_tokens:\n    document = [word for word in x if word in model.wv.vocab]\n    documents.append(document)\n#now this document have only those words which are present in our model's vocab\n# documents   ","bbaa0900":"len(documents)","1761f9dc":"type(documents)","1ee7df8d":"#checking if there is any empty list inside documents\ncounter = 0\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        counter += 1\nprint(counter)","cd505a7f":"list_v=[]\nfor i in range (0,len(documents)):\n    if documents[i] == []:\n        list_v.append(np.zeros(300,))\n    else:\n        list_v.append(np.mean(model[documents[i]], axis=0))\n        ","2820d056":"len(list_v)","d135c329":"from collections import Counter\nprint('Original dataset shape before smote %s' % Counter(new['toxic']))\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX, y = oversample.fit_resample(list_v, new['toxic'])\nprint('Original dataset shape after smote %s' % Counter(y))","7d465dac":"#test-train split\nfrom sklearn.model_selection import train_test_split\nXw_train, Xw_test, yw_train, yw_test = train_test_split(X,y, test_size=0.3, random_state=42)","f651f2c6":"from sklearn.linear_model import LogisticRegression\nclf=LogisticRegression(max_iter=1000)\nclf.fit(Xw_train,yw_train)","72e41d20":"predicted_res=clf.predict(Xw_test)\nfrom sklearn.metrics import accuracy_score\naccuracy=accuracy_score(yw_test,predicted_res)\naccuracy","bbdc1219":"import numpy as np\n\nz=1.96\ninterval = z * np.sqrt( (0.8244 * (1 - 0.8244)) \/ yw_test.shape[0])\ninterval","f40faae3":"For example, consider the sentence: \u201cI have topped at times but I never stopped trying\u201d.  Let\u2019s say we want to learn the embedding of the word \u201ctopped\u201d. So, here the focus word is \u201ctopped\u201d.\n\nThe first step is to define a context window. A context window refers to the number of words appearing on the left and right of a focus word. The words appearing in the context window are known as neighboring words (or context). Let\u2019s fix the context window to 1 and then see input and output for both approaches:\n\nContinuous Bag-of-Words: Input = [ have, at ],  Output = topped\n\nSkip-gram: Input = topped, Output = [ have, at ]\n\nAs you can see here, CBOW accepts multiple words as input and produces a single word as output whereas Skip-gram accepts a single word as input and produces multiple words as output.\n\nSo, let us define the architecture according to the input and output. But keep in mind that each word is fed into a model as a one-hot vector:","d9bccfed":"> A we can clearly see that data is skewed therefore we will oversample it.","89ca6992":"> So there were in total this much empty vectors(output of above cell) which were form due to removal of words whch are not present in our pretrained model's vocab, now we will fill those vectors with zeros","030b1224":"![image](https:\/\/www.goalcast.com\/wp-content\/uploads\/2017\/06\/toxic-people.jpg)","99909bd4":"## SMOTE \n\nSMOTE (synthetic minority oversampling technique) is one of the most commonly used oversampling methods to solve the imbalance problem.\nIt aims to balance class distribution by randomly increasing minority class examples by replicating them.\nSMOTE synthesises new minority instances between existing minority instances. It generates the virtual training records by linear interpolation for the minority class. These synthetic training records are generated by randomly selecting one or more of the k-nearest neighbors for each example in the minority class. After the oversampling process, the data is reconstructed and several classification models can be applied for the processed data.","c00cd72e":"## LOGISTIC REGRESSION","8a3b1b93":"![smote](https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/smote.png)","d2274945":"### Pretrained word2vec Implementation","b2ad7d62":"![](https:\/\/cdn.analyticsvidhya.com\/wp-content\/uploads\/2020\/03\/Screenshot-from-2020-03-12-13-05-42.png)","20abaa8d":"> Confidence interval(95%) - [87.15  88.41]","7dbf47dd":"## Google\u2019s Word2vec Pretrained Word Embedding\nWord2Vec is one of the most popular pretrained word embeddings developed by Google. Word2Vec is trained on the Google News dataset (about 100 billion words). It has several use cases such as Recommendation Engines, Knowledge Discovery, and also applied in the different Text Classification problems.\n\nThe architecture of Word2Vec is really simple. It\u2019s a feed-forward neural network with just one hidden layer. Hence, it is sometimes referred to as a Shallow Neural Network architecture.\n\nDepending on the way the embeddings are learned, Word2Vec is classified into two approaches:\n\n1. Continuous Bag-of-Words (CBOW)\n\n2. Skip-gram model"}}