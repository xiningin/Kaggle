{"cell_type":{"8e695047":"code","f8d1a39b":"code","a9f23db1":"code","0e4d4af9":"code","8ca0269d":"code","356d7a7b":"code","573b3efd":"code","0e489d6b":"code","4226f4a3":"code","a2524fab":"code","d1ea292a":"code","455ac7a0":"code","94bff6f1":"code","fd5ec8bd":"code","1d8de392":"code","885a60d9":"code","57c73f98":"code","41fef869":"code","a9c3c686":"code","27425dd7":"code","05957671":"code","6e34a417":"code","d6ac1a9c":"code","0dd80b93":"code","d2bd6f6e":"code","018699f2":"code","e473aaef":"code","fd08dc29":"code","7bdc1e3e":"code","ae69309d":"code","e7c72c84":"code","4df20826":"code","0ed58d0b":"code","5134a8a5":"code","d2eb5538":"code","f7584fdc":"code","ff604c85":"code","217bfe46":"code","03b45eda":"code","8c7e0d1a":"code","7813eb4f":"code","122bb1ec":"code","96aef4b6":"code","d00d44ba":"code","a5a5e8a9":"code","fad7b76d":"code","3b8cfc70":"code","4ef562f8":"code","72e76381":"code","f8faca2e":"code","ef321714":"code","f9b5b316":"code","c31095ef":"code","ae4a9828":"code","de34330d":"code","e889f98d":"code","88fe8156":"code","376b891b":"code","7a5e3765":"code","76850a32":"code","ea125485":"code","57d8e94b":"code","1f2c642d":"code","242a2158":"code","77be2da5":"code","ff5cb082":"code","75445fef":"code","1eb06095":"code","f2aaaf0c":"markdown","7585ef00":"markdown","e7e1997d":"markdown","2fd7ea75":"markdown","17d21619":"markdown","fd232421":"markdown","daa80396":"markdown","258c3e28":"markdown","cafe9bb4":"markdown","d1ece2db":"markdown","12a3cc64":"markdown","9dfcb0ab":"markdown","cdbca0ec":"markdown","1b8be19a":"markdown","48e82b8e":"markdown","2bbcf5e3":"markdown","730f8385":"markdown","d86167e8":"markdown","f0c1b412":"markdown","d9e519cb":"markdown","c025931e":"markdown","ec7d5ea9":"markdown","38cd071f":"markdown","0f8d1a08":"markdown","805afaeb":"markdown","a0a7284b":"markdown","1d239188":"markdown","e5ae2329":"markdown","fbafde10":"markdown","fabd4a2b":"markdown","07d9b459":"markdown","9e608704":"markdown","83b25aa2":"markdown","4d980c84":"markdown","d6ac87a0":"markdown","c8407de3":"markdown","e5e9a140":"markdown","5d83cc0a":"markdown","9ab2550c":"markdown","98258845":"markdown","5107a6fd":"markdown","8e79454c":"markdown","f6796d86":"markdown","8fb73dfd":"markdown","12628ea9":"markdown","37bce9d3":"markdown","9aadf0d7":"markdown","bfb9c9ce":"markdown","ccab732f":"markdown","ab1f375c":"markdown"},"source":{"8e695047":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sn\n\nipl_auction_df = pd.read_csv(\"..\/input\/ipl-2013\/IPL 2013.csv\")\nipl_auction_df.head(5)","f8d1a39b":"ipl_auction_df.info()","a9f23db1":"X_features = ['AGE','COUNTRY', 'PLAYING ROLE', 'T-WKTS', 'ODI-RUNS-S', 'ODI-SR-B','ODI-WKTS', 'ODI-SR-BL',\n             'CAPTAINCY EXP', 'RUNS-S', 'HS', 'AVE', 'SR-B', 'SIXERS', 'RUNS-C','WKTS', 'AVE-BL', 'ECON', 'SR-BL']","0e4d4af9":"#Initialize the list with the catagorical feature names\ncategorical_features = ['AGE', 'COUNTRY', 'PLAYING ROLE', 'CAPTAINCY EXP']\n\n#get_dummies() is invoked to return the dummy features\nipl_auction_encoded_df =pd.get_dummies(ipl_auction_df[X_features],\n                                      columns = categorical_features,\n                                      drop_first = True)","8ca0269d":"#To display all features along with new dummy feautures we use the following\nipl_auction_encoded_df.columns","356d7a7b":"X = ipl_auction_encoded_df\nY = ipl_auction_df['SOLD PRICE']","573b3efd":"from sklearn.preprocessing import StandardScaler","0e489d6b":"#Initialize the standardscaler\nX_scaler = StandardScaler()\n#Standardize all the feature columns\nX_scaled = X_scaler.fit_transform(X)\n\n# Standardizing Y explicitly by subtracting mean and dividing by standard deviation\nY = (Y - Y.mean()) \/ Y.std()","4226f4a3":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(\nX_scaled,\nY,\ntest_size = 0.2,\nrandom_state = 42)","a2524fab":"from sklearn.linear_model import LinearRegression\n\nlinreg = LinearRegression()\nlinreg.fit(X_train, y_train)","d1ea292a":"linreg.coef_","455ac7a0":"#the dataframe has two columns to store feature name and the corresponding coefficient values\ncolumns_coef_df = pd.DataFrame({ 'columns': ipl_auction_encoded_df.columns,\n                               'coef': linreg.coef_ })\n\n#sorting the feature by cofficient values in descending order\nsorted_coef_vals = columns_coef_df.sort_values('coef',\n                                              ascending=False)","94bff6f1":"#Creating a bar plot\nplt.figure(figsize = (14, 12))\nsn.barplot(x=\"coef\", y=\"columns\", data=sorted_coef_vals);\nplt.xlabel(\"Coefficients from Linear Regression\")\nplt.ylabel(\"Features\")","fd5ec8bd":"from sklearn import metrics\n\n#Take a model as parameter\n#Print the RMSE on train and test set\n\ndef get_train_test_rmse(model):\n    #predicting on training dataset\n    y_train_pred = model.predict(X_train)\n    #comapre the actual y with predicted y in the training datasets\n    rmse_train = round(np.sqrt(metrics.mean_squared_error(y_train,\n                                                         y_train_pred)),3)\n    \n    #predicting on test dataset\n    y_test_pred = model.predict(X_test)\n    #comapare the actual y with predicted y in the test dataset\n    rmse_test = round(np.sqrt(metrics.mean_squared_error(y_test,\n                                                        y_test_pred)), 3)\n    print(\"train: \", rmse_train, \"test: \", rmse_test)","1d8de392":"get_train_test_rmse(linreg)","885a60d9":"#Importing Ridge Regerssion\nfrom sklearn.linear_model import Ridge\nridge = Ridge(alpha = 1, max_iter = 500)\nridge.fit(X_train, y_train)","57c73f98":"get_train_test_rmse(ridge)","41fef869":"# Importing Lasso Regression\nfrom sklearn.linear_model import Lasso\nlasso = Lasso(alpha = 0.01, max_iter = 500)\nlasso.fit(X_train, y_train)","a9c3c686":"get_train_test_rmse( lasso)","27425dd7":"#storing the feature names and coefficient values in the Dataframe\nlasso_coef_df = pd.DataFrame ({ 'columns': ipl_auction_encoded_df.columns,\n                              'coef': lasso.coef_})","05957671":"#filtering out coefficients with zeros\nlasso_coef_df[lasso_coef_df.coef == 0]","6e34a417":"bank_df =pd.read_csv(\"..\/input\/bank-dataset\/bank.csv\")\nbank_df.head(5)","d6ac1a9c":"bank_df.info()","0dd80b93":"bank_df.subscribed.value_counts()","d2bd6f6e":"#sklearn.utils has resample method to help with upsampling\nfrom sklearn.utils import resample\n\n#seperate the case of yes-subscriber and no-subscriber\nbank_subscribed_no = bank_df[bank_df.subscribed == 'no']\nbank_subscribed_yes = bank_df[bank_df.subscribed == 'yes']\n\n#upsampling the yes-subscribed cases\ndf_minority_upsampled = resample(bank_subscribed_yes,\n                                 replace=True,\n                                 n_samples=2000)","018699f2":"#combine majority class with unsampled minority class\nnew_bank_df = pd.concat([ bank_subscribed_no, df_minority_upsampled])","e473aaef":"from sklearn.utils import shuffle\nnew_bank_df = shuffle(new_bank_df)","fd08dc29":"#assigning list of columns names in the DataFrame\nX_features = list (new_bank_df.columns)\n#remove the response variable from the list\nX_features.remove('subscribed')\nX_features","7bdc1e3e":"#get_dummies() will convert all the columns with datatypes as object\nencoded_bank_df = pd.get_dummies( new_bank_df [X_features ],\n                                drop_first = True)\nX = encoded_bank_df","ae69309d":"#encoding the subscribed columns and assigning to Y\nY = new_bank_df.subscribed.map( lambda x: int(x == 'yes'))","e7c72c84":"from sklearn.model_selection import train_test_split\ntrain_X, test_X, train_y, test_y = train_test_split(X,\n                                                   Y,\n                                                   test_size = 0.3,\n                                                   random_state = 42)","4df20826":"from sklearn.linear_model import LogisticRegression\n\n#Initializing the model\nlogit = LogisticRegression()\n## Fitting the model with X and Y values of the dataset\nlogit.fit( train_X, train_y)","0ed58d0b":"pred_y = logit.predict(test_X)","5134a8a5":"from sklearn import metrics\n#defining the matrix to draw the confusion  matrix from actual and predicted class lebels\ndef draw_cm( actual, predicted ):\n    cm = metrics.confusion_matrix( actual, predicted, [1,0])\n    #the labels are configured to better interpretation from the plot \n    sn.heatmap(cm, annot=True, fmt='.2f',\n              xticklabels = [\"Subscribed\", \"Not Subscribed\"],\n              yticklabels = [\"Subscribed\", \"Not Subscribed\"])\n    \n    plt.ylabel('True lebels')\n    plt.xlabel('Preicted label')\n    plt.show()\n","d2eb5538":"cm = draw_cm(test_y, pred_y)","f7584fdc":"print(metrics.classification_report(test_y, pred_y))","ff604c85":"#predicting the probability values for test cases\npredict_proba_df = pd.DataFrame(logit.predict_proba(test_X))\npredict_proba_df.head()","217bfe46":"test_results_df = pd.DataFrame({'actual': test_y})\ntest_results_df = test_results_df.reset_index()\n#assigning the probability values for class label 1\ntest_results_df['chd_1'] = predict_proba_df.iloc[:,1:2]","03b45eda":"test_results_df.head(5)","8c7e0d1a":"#passing the actual class labels and predicted probability values to compute ROC AUC score\nauc_score = metrics.roc_auc_score(test_results_df.actual,\n                                 test_results_df.chd_1)\nround(float(auc_score ), 2)","7813eb4f":"def draw_roc_curve( model, test_X, test_y ):\n## Creating and initializing a results DataFrame with actual labels\n    test_results_df = pd.DataFrame( { 'actual': test_y } )\n    test_results_df = test_results_df.reset_index()\n    # predict the probabilities on the test set\n    predict_proba_df = pd.DataFrame( model.predict_proba( test_X ) )\n    ## selecting the probabilities that the test example belongs to class 1\n    test_results_df['chd_1'] = predict_proba_df.iloc[:,1:2]\n    ## Invoke roc_curve() to return the fpr, tpr and threshold values.\n    ## threshold values contain values from 0.0 to 1.0\n    fpr, tpr, thresholds = metrics.roc_curve( test_results_df.actual,\n    test_results_df.chd_1,\n    drop_intermediate = False )\n    ## Getting the roc auc score by invoking metrics.roc_auc_score method\n    auc_score = metrics.roc_auc_score( test_results_df.actual, test_results_df.chd_1 )\n    \n    ## Setting the size of the plot\n    plt.figure(figsize=(8, 6))\n    ## plotting the actual fpr and tpr values\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    ## plotting th diagnoal line from (0,1)\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    ## Setting labels and titles\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    return auc_score, fpr, tpr, thresholds","122bb1ec":"## Invoking draw_roc_curve with the logistic regresson model\n_, _, _, _ = draw_roc_curve( logit, test_X, test_y )","96aef4b6":"# importing the knn classifier algorithms\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Initialize the classifier\nknn_clf = KNeighborsClassifier()\n#Fitting the model with the training set\nknn_clf.fit(train_X, train_y)","d00d44ba":"#invoking draw_roc_curve with the knn model\n_, _, _, _ = draw_roc_curve(knn_clf, test_X, test_y)","a5a5e8a9":"## Predicting on test set\npred_y = knn_clf.predict(test_X)\n## Drawing the confusion matrix for KNN model\ndraw_cm( test_y, pred_y )","fad7b76d":"print( metrics.classification_report( test_y, pred_y ) )","3b8cfc70":"## Importing GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n## Creating a dictionary with hyperparameters and possible values for searching\ntuned_parameters = [{'n_neighbors': range(5,10),\n'metric': ['canberra', 'euclidean', 'minkowski']}]\n## Configuring grid search\nclf = GridSearchCV(KNeighborsClassifier(),\ntuned_parameters,\ncv=10,\nscoring='roc_auc')\n## fit the search with training set\nclf.fit(train_X, train_y )","4ef562f8":"clf.best_score_","72e76381":"clf.best_params_","f8faca2e":"# Importing Random Forest Classifier from the sklearn.ensemble\nfrom sklearn.ensemble import RandomForestClassifier\n# Initializing the Random Forest Classifier with max_dept and n_estimators\nradm_clf = RandomForestClassifier( max_depth=10, n_estimators=10)\nradm_clf.fit( train_X, train_y )","ef321714":"_, _, _, _ = draw_roc_curve( radm_clf, test_X, test_y );","f9b5b316":"# Configuring parameters and values for searched\ntuned_parameters = [{'max_depth': [10, 15],\n'n_estimators': [10,20],\n'max_features': ['sqrt', 'auto']}]\n# Initializing the RF classifier\nradm_clf = RandomForestClassifier()\n# Configuring search with the tunable parameters\nclf = GridSearchCV(radm_clf,\ntuned_parameters,\ncv=5,\nscoring='roc_auc')\n# Fitting the training set\nclf.fit(train_X, train_y )","c31095ef":"clf.best_score_","ae4a9828":"clf.best_params_","de34330d":"# Initializing the Random Forest Mode with the optimal values\nradm_clf = RandomForestClassifier( max_depth=15, n_estimators=20, max_features =\n'auto')\n# Fitting the model with the training set\nradm_clf.fit( train_X, train_y )","e889f98d":"_, _, _, _ = draw_roc_curve( clf, test_X, test_y )","88fe8156":"pred_y = radm_clf.predict( test_X )\ndraw_cm( test_y, pred_y )","376b891b":"print( metrics.classification_report( test_y, pred_y ) )","7a5e3765":"import numpy as np\n# Create a dataframe to store the featues and their corresponding importances\nfeature_rank = pd.DataFrame( { 'feature': train_X.columns,\n'importance': radm_clf.feature_importances_ } )\n# Sorting the features based on their importances with most important feature at top.\nfeature_rank = feature_rank.sort_values('importance', ascending = False)\nplt.figure(figsize=(8, 6))\n# plot the values\nsn.barplot( y = 'feature', x = 'importance', data = feature_rank );","76850a32":"# Importing Adaboost classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n## Initializing logistic regression to use as base classifier\nlogreg_clf = LogisticRegression()\n## Initilizing adaboost classifier with 50 classifers\nada_clf = AdaBoostClassifier(logreg_clf, n_estimators=50)\n## Fitting adaboost model to training set\nada_clf.fit(train_X, train_y )","ea125485":"_, _, _, _ = draw_roc_curve( ada_clf, test_X, test_y )","57d8e94b":"## Importing Gradient Boosting classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n## Initializing Gradient Boosting with 500 estimators and max depth as 10.\ngboost_clf = GradientBoostingClassifier( n_estimators=500, max_depth=10)\n## Fitting gradient boosting model to training set\ngboost_clf.fit(train_X, train_y )","1f2c642d":"_, _, _, _ = draw_roc_curve( gboost_clf, test_X, test_y )","242a2158":"from sklearn.model_selection import cross_val_score\ngboost_clf = GradientBoostingClassifier( n_estimators=500, max_depth=10)\ncv_scores = cross_val_score( gboost_clf, train_X, train_y, cv = 10, scoring = 'roc_auc' )","77be2da5":"print( cv_scores )\nprint( \"Mean Accuracy: \", np.mean(cv_scores), \" with standard deviation of: \",\nnp.std(cv_scores))","ff5cb082":"gboost_clf.fit(train_X, train_y )\npred_y = gboost_clf.predict( test_X )\ndraw_cm( test_y, pred_y )","75445fef":"print( metrics.classification_report( test_y, pred_y ) )","1eb06095":"import numpy as np\n# Create a dataframe to store the featues and their corresponding importances\nfeature_rank = pd.DataFrame( { 'feature': train_X.columns,\n'importance': gboost_clf.feature_importances_ } )\n## Sorting the features based on their importances with most important feature at top.\nfeature_rank = feature_rank.sort_values('importance', ascending = False)\nplt.figure(figsize=(8, 6))\n# plot the values\nsn.barplot( y = 'feature', x = 'importance', data = feature_rank );","f2aaaf0c":"# 5) AdaBoost","7585ef00":"- Plotting the coefficient Values","e7e1997d":"![](https:\/\/thumbs.gfycat.com\/WhichThoughtfulEasternnewt-size_restricted.gif)","2fd7ea75":"- Loading the dataset","17d21619":"![](https:\/\/i.ytimg.com\/vi\/goPiwckWE9M\/maxresdefault.jpg)","fd232421":" # 6) Gradient Boosting","daa80396":"![](https:\/\/images.akira.ai\/glossary\/gradient-boosting-ml-technique-akira-ai.png)","258c3e28":"- Plotting ROC Curve","cafe9bb4":"![](https:\/\/www.jmp.com\/en_ch\/statistics-knowledge-portal\/what-is-multiple-regression\/fitting-multiple-regression-model\/_jcr_content\/par\/styledcontainer_2069\/par\/lightbox_4130\/lightboxImage.img.png\/1548703926664.png)","d1ece2db":"![](https:\/\/hackernoon.com\/hn-images\/0*jW2hAGmYEFH0RP9W.)","12a3cc64":"- Calculate RMSE","9dfcb0ab":"### Applying Regularization","cdbca0ec":"- **Building the final model with optimal parameter values**","1b8be19a":"![](https:\/\/miro.medium.com\/max\/1180\/1*DUaQoSKHX09hLG0QcGApTg.png)","48e82b8e":"![](https:\/\/slideplayer.com\/slide\/9547466\/30\/images\/16\/Comparing+Ridge+and+LASSO.jpg)","2bbcf5e3":"![](https:\/\/www.cellstrat.com\/wp-content\/uploads\/2018\/09\/AdaBoost.png)","730f8385":"**It can be noticed that model is not overfitting and the difference between train and test is very small. LASSO reduces some of the coefficient values to 0, which indicates that these features are not necessasry for explaining the variance in the outcome variable**","d86167e8":"# 3) K-Nearest Neighbours(KNN)","f0c1b412":"![](https:\/\/cambridgecoding.files.wordpress.com\/2016\/01\/knn2.jpg)","d9e519cb":"### Dealing with Imbalance Datasets","c025931e":"**2) Unsupervised Machine Learning**","ec7d5ea9":"![](https:\/\/miro.medium.com\/max\/1280\/0*xvehxiXoUVx-z8AA.gif)","38cd071f":"![](https:\/\/lh3.googleusercontent.com\/proxy\/7517sYlIMrd28WEEVXwlhas0_67X3oD8LBw-ch1ywSiMDSyZ7TesK6qseFASl5jEFGq5B-ptIbfmlszwaBwdh7zNnmbRL5-0_LKNRNlErq4py1TIOTSSNEA6jW8wbqTJU8FrlX6us09wOfwu7w)","0f8d1a08":"**The dataset is quite imbalanced. Both the classes are not equally represented. In such a case model may not be able to learn and may be over biased towards the class that is over-represented\nOne approach to deal with imbalanced dataset is Bootstrapping. It involves resampling techniques such as Upsampling and Downsampling**.\n\n**1)Upsampling - Increase the instances of under-represented minority class by replicating the observation in dataset.Sampling with replacement is used for this purpose called as oversampling.**\n\n**2)Downsampling - Reduce the instances of over-represented majority class by removing the existing observations from the dataset and is also called Undersampling**","805afaeb":"- Finding important features","a0a7284b":"# 4) Random Forest","1d239188":"- Drawing the confusion matrix","e5ae2329":"**Machine learning (ML) is the study of computer algorithms that improve automatically through experience. It is seen as a subset of artificial intelligence. Machine learning algorithms build a mathematical model based on sample data, known as \"training data\", in order to make predictions or decisions without being explicitly programmed to do so.Machine learning algorithms are used in a wide variety of applications, such as email filtering and computer vision, where it is difficult or infeasible to develop conventional algorithms to perform the needed tasks.**","fbafde10":"**Types of Machine Learning :**\n\n**1) Supervised Machine Learning**","fabd4a2b":"![](https:\/\/thumbs.gfycat.com\/OrangeMiserlyKiskadee-size_restricted.gif)","07d9b459":"- Grid Search for Optimal Parameters","9e608704":"## -------------------If You Found This Helpful Do Upvote Please ----------------------","83b25aa2":"### Ensemble Methods","4d980c84":"- GridSerach for most optimal parameters","d6ac87a0":"**One of these, there are four catagorical feature that need to be encoded into dummy features using one Hot Encoding**","c8407de3":"![](https:\/\/i.ytimg.com\/vi\/QqkV7ZtRv7w\/maxresdefault.jpg)","e5e9a140":"# 2) Logistics Regression ","5d83cc0a":"- Confusion Matrix","9ab2550c":"- Split the dataset into Train and Test","98258845":"![](https:\/\/cdn-images-1.medium.com\/max\/744\/1*zfH9946AssCx4vzjaizWeg.png)","5107a6fd":"- Split the Dataset into Train and Test","8e79454c":"- ROC AUC Score","f6796d86":"![](https:\/\/miro.medium.com\/max\/1280\/0*9IwCpy_JSKZMGlW0.gif)","8fb73dfd":"- Classification Report","12628ea9":"# 1) Linear Regression Model ","37bce9d3":"![](https:\/\/lh3.googleusercontent.com\/proxy\/7517sYlIMrd28WEEVXwlhas0_67X3oD8LBw-ch1ywSiMDSyZ7TesK6qseFASl5jEFGq5B-ptIbfmlszwaBwdh7zNnmbRL5-0_LKNRNlErq4py1TIOTSSNEA6jW8wbqTJU8FrlX6us09wOfwu7w)","9aadf0d7":"- Confusion Matrix","bfb9c9ce":"### --------------------------------------------Let's Get Started--------------------------------------------------","ccab732f":"- Standardization of X and Y","ab1f375c":"### Boosting"}}