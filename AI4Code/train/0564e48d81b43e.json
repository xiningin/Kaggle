{"cell_type":{"397a424c":"code","7af97687":"code","9ca3c0a8":"code","75a0e971":"code","b6e72db2":"code","4e6d62cc":"code","d770e241":"code","149cbb5c":"code","54d9a0e6":"code","bd9cd248":"code","115b938e":"code","cb9404e2":"code","650d40a1":"code","8c9a5f8d":"code","74867d5b":"code","c8b23fa0":"code","f604c8fa":"code","1d031e6d":"code","885c7e9f":"code","da98380f":"code","8b810f84":"code","85dfc35d":"code","f75928c7":"code","863a7323":"code","35b33fc9":"code","2d2e7a74":"code","1fe0915a":"code","85723714":"code","fd7dde16":"code","687c02cf":"code","6d29972c":"code","01583535":"code","3781acba":"code","f784cb47":"code","c5b9dcdd":"code","44717d64":"code","4ea4e012":"code","ad0107fd":"markdown","9291082c":"markdown","b7896742":"markdown","d543f786":"markdown","8e69c2cb":"markdown","195c75a2":"markdown","a12a7adc":"markdown","2bda69f1":"markdown","5ffa76ab":"markdown","4dc5dd8e":"markdown","97018c49":"markdown","2a2f4942":"markdown","aee4cee3":"markdown","80b115dd":"markdown","827ed1d2":"markdown","bc25ca0f":"markdown","1dcc5b5f":"markdown","f5e6699c":"markdown"},"source":{"397a424c":"#importing libraries\n#main ones\nimport pandas as pd\nimport numpy as np\n\n#classifiers and metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, learning_curve, train_test_split\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix, roc_curve, precision_recall_curve, accuracy_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import metrics\n\n#for plotting\nimport matplotlib.pyplot as plt\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n%matplotlib inline\nimport itertools\nfrom itertools import chain\n\n\n#extra\nimport warnings\nimport time\n\n#for neural links\nfrom keras.models import Sequential\nfrom keras.layers import Dense\n\nwarnings.filterwarnings('ignore')","7af97687":"#readin data\ndata = pd.read_csv('..\/input\/data.csv')\n","9ca3c0a8":"#taking a look at data\ndata.info()","75a0e971":"#dropping extra columns\n\ndata = data.drop(['Unnamed: 32','id'],axis = 1)\n\ndata.diagnosis.replace(to_replace = dict(M = 1, B = 0), inplace = True)","b6e72db2":"#looking at data and labels\n\ndata.describe()","4e6d62cc":"# making it 2 datasets\n\nM = data[(data['diagnosis'] != 0)]\nB = data[(data['diagnosis'] == 0)]","d770e241":"#how the initial distribution is\n\ntrace = go.Pie(labels = ['benign','malignant'], values = data['diagnosis'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['gray', 'red'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of diagnosis variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","149cbb5c":"#dividing in three parts\n\nfeatures_mean= list(data.columns[0:11])\nfeatures_se= list(data.columns[11:20])\nfeatures_worst=list(data.columns[21:31])","54d9a0e6":"#correlation\ncorrelation = data.corr()\nmatrix_cols = correlation.columns.tolist()\ncorr_array  = np.array(correlation)\n\n#Plotting\ntrace = go.Heatmap(z = corr_array,\n                   x = matrix_cols,\n                   y = matrix_cols,\n                   xgap = 2,\n                   ygap = 2,\n                   colorscale='Reds',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 720,\n                        width   = 800,\n                        margin  = dict(r = 0 ,l = 210,\n                                       t = 25,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 12)),\n                        xaxis   = dict(tickfont = dict(size = 12)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","bd9cd248":"y = np.array(data.diagnosis.tolist())\ndata = data.drop('diagnosis', 1)\nX = np.array(data.as_matrix())","115b938e":"scaler = StandardScaler()\nX = scaler.fit_transform(X)","cb9404e2":"random_state = 42\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.12, random_state = random_state)\nprint(X_train.shape)\nprint(X_test.shape)","650d40a1":"def plot_confusion_matrix(cm, classes,\n                          normalize = False,\n                          title = 'Confusion matrix\"',\n                          cmap = plt.cm.RdGy) :\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])) :\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \n# Show metrics \ndef show_metrics():\n    tp = cm[1,1]\n    fn = cm[1,0]\n    fp = cm[0,1]\n    tn = cm[0,0]\n    print('Accuracy  =     {:.3f}'.format((tp+tn)*100\/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp*100\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp*100\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn))*100)\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","8c9a5f8d":"def cross_val_metrics(model) :\n    scores = ['accuracy', 'precision', 'recall']\n    for sc in scores:\n        scores = cross_val_score(model, X, y, cv = 5, scoring = sc)\n        print('[%s] : %0.5f (+\/- %0.5f)'%(sc, scores.mean(), scores.std()))","74867d5b":"lgr_clf_start_time = time.time()\n\nlgr_clf=LogisticRegression(random_state = random_state)\nlgr_clf.fit(X_train, y_train)\n\nprint(\"--- %s seconds ---\" % (time.time() - lgr_clf_start_time))\n","c8b23fa0":"y_pred =lgr_clf.predict(X_test)\ny_score = lgr_clf.decision_function(X_test)\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Logistic Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\ncross_log = cross_val_metrics(lgr_clf)","f604c8fa":"log_clf = LogisticRegression(random_state = random_state)\nparam_grid = {\n            'penalty' : ['l2','l1'],  \n            'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n            }\n\nCV_log_clf = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'accuracy', verbose = 1, n_jobs = -1)\nCV_log_clf.fit(X_train, y_train)\n\nbest_parameters = CV_log_clf.best_params_\nprint('The best parameters for using this model is', best_parameters)","1d031e6d":"CV_log_clf = LogisticRegression(C = best_parameters['C'], \n                                penalty = best_parameters['penalty'], \n                                random_state = random_state)\n\nCV_log_clf.fit(X_train, y_train)\ny_pred = CV_log_clf.predict(X_test)\ny_score = CV_log_clf.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned Logistic Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(CV_log_clf)","885c7e9f":"dtree_start_time = time.time()\n\ndtree = DecisionTreeClassifier()\ndtree.fit(X_train, y_train)\n\nprint(\"--- %s seconds ---\" % (time.time() - dtree_start_time))\n\ny_pred = dtree.predict(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title=' Decision Tree Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(dtree)","da98380f":"def Classification_model_gridsearchCV(model,param_grid,X_data,y_data):\n    clf = GridSearchCV(model,param_grid,cv=10,scoring=\"accuracy\")\n  \n    clf.fit(X_train,y_train)\n    print(\"The best parameter found on development set is :\")\n    print(clf.best_params_)\n    print(\"the bset estimator is \")\n    print(clf.best_estimator_)\n    print(\"The best score is \")\n    print(clf.best_score_)","8b810f84":"param_grid = {'max_features': ['auto', 'sqrt', 'log2'],\n              'min_samples_split': [2,3,4,5,6,7,8,9,10], \n              'min_samples_leaf':[2,3,4,5,6,7,8,9,10] }\n\ndtree= DecisionTreeClassifier()\nClassification_model_gridsearchCV(dtree,param_grid,X_train,y_train)","85dfc35d":"dtree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n            max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=5, min_samples_split=3,\n            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n            splitter='best')\ndtree.fit(X_train, y_train)\n\ny_pred = dtree.predict(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned Logistic Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(dtree)","f75928c7":"svm_start_time = time.time()\n\nclf_svm=svm.SVC()\nclf_svm.fit(X_train, y_train)\n\nprint(\"--- %s seconds ---\" % (time.time() - svm_start_time))\n\ny_pred = clf_svm.predict(X_test)\ny_score = clf_svm.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='SVM Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(clf_svm)","863a7323":"clf_svm=svm.SVC(kernel='linear', C=1000000) \nclf_svm.fit(X_train, y_train)\ny_pred = clf_svm.predict(X_test)\ny_score = clf_svm.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned SVM Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(clf_svm)","35b33fc9":"clf_svm=svm.SVC(kernel='linear', C=0.1) \nclf_svm.fit(X_train, y_train)\ny_pred = clf_svm.predict(X_test)\ny_score = clf_svm.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned SVM Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(clf_svm)","2d2e7a74":"clf_svm=svm.SVC(kernel='rbf', C=1000000) \nclf_svm.fit(X_train, y_train)\ny_pred = clf_svm.predict(X_test)\ny_score = clf_svm.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned SVM Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(clf_svm)","1fe0915a":"clf_svm=svm.SVC(kernel='rbf', C=0.1) \nclf_svm.fit(X_train, y_train)\ny_pred = clf_svm.predict(X_test)\ny_score = clf_svm.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned SVM Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(clf_svm)","85723714":"knn_start_time = time.time()\n\nclf_knn = KNeighborsClassifier(n_neighbors=1)\nclf_knn.fit(X_train, y_train)\n\nprint(\"--- %s seconds ---\" % (time.time() - knn_start_time))\n\n\ny_pred =clf_knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='KNN Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\ncross_log = cross_val_metrics(clf_knn)","fd7dde16":"clf_knn = KNeighborsClassifier(n_neighbors=10)\nclf_knn.fit(X_train, y_train)\ny_pred =clf_knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned KNN Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\ncross_log = cross_val_metrics(clf_knn)","687c02cf":"clf_knn = KNeighborsClassifier(n_neighbors=50)\nclf_knn.fit(X_train, y_train)\ny_pred =clf_knn.predict(X_test)\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='Tuned KNN Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\n\ncross_log = cross_val_metrics(clf_knn)","6d29972c":"GNB_clf_start_time = time.time()\n\nGNB_clf = GaussianNB()\nGNB_clf.fit(X_train, y_train)\n\nprint(\"--- %s seconds ---\" % (time.time() - GNB_clf_start_time))\n\ny_pred = GNB_clf.predict(X_test)\n#y_score = GNB_clf.decision_function(X_test)\n\n# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      classes=class_names, \n                      title='GaussianNB Confusion matrix')\nplt.savefig('6')\nplt.show()\n\nshow_metrics()\ncross_log = cross_val_metrics(GNB_clf)","01583535":"\nfrom keras import metrics\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.utils import to_categorical\n\nnetwork_start_time = time.time()\n\nnetwork = Sequential()\nnetwork.add(Dense (60, activation='relu' , input_shape=(30,)))\nnetwork.add(Dense (60, activation='relu'))\nnetwork.add(Dense (1, activation='sigmoid'))\n            \nnetwork.compile(optimizer='sgd',\n             loss='binary_crossentropy',\n             metrics=['accuracy',])\n            \nhistory = network.fit(X_train, y_train, epochs = 10, batch_size=4, validation_data=(X_test, y_test))\n\n\nprint(\"--- %s seconds ---\" % (time.time() - network_start_time))\n\nnetwork.evaluate(X_test, y_test)\n\n\nloss = history.history['loss']\nval_loss = history.history ['val_loss']\n\nepochs = range(1, 11)\n\nplt.plot(epochs, loss, 'bo', label= 'training loss')\nplt.plot(epochs, val_loss, 'b', label= 'Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\n\nplt.show()\n\n","3781acba":"network2_start_time = time.time()\n\nnetwork2 = Sequential()\nnetwork2.add(Dense (60, activation='relu' , input_shape=(30,)))\nnetwork2.add(Dense (60, activation='relu'))\nnetwork2.add(Dense (1, activation='sigmoid'))\n            \nnetwork2.compile(optimizer='rmsprop',\n             loss='binary_crossentropy',\n             metrics=['accuracy',])\n            \nhistory2 = network2.fit(X_train, y_train, epochs = 10, batch_size=4, validation_data=(X_test, y_test))\n\n\nprint(\"--- %s seconds ---\" % (time.time() - network2_start_time))\n\nnetwork2.evaluate(X_test, y_test)\n\nloss = history2.history['loss']\nval_loss = history2.history ['val_loss']\n\nepochs = range(1, 11)\n\nplt.plot(epochs, loss, 'bo', label= 'training loss')\nplt.plot(epochs, val_loss, 'b', label= 'Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\n\nplt.show()","f784cb47":"network3_start_time = time.time()\n\nnetwork3 = Sequential()\nnetwork3.add(Dense (60, activation='relu' , input_shape=(30,)))\nnetwork3.add(Dense (60, activation='relu'))\nnetwork3.add(Dense (1, activation='sigmoid'))\n            \nnetwork3.compile(optimizer='adam',\n             loss='binary_crossentropy',\n             metrics=['accuracy',])\n            \nhistory3 = network3.fit(X_train, y_train, epochs = 10, batch_size=4, validation_data=(X_test, y_test))\n\n\nprint(\"--- %s seconds ---\" % (time.time() - network3_start_time))\n\nnetwork3.evaluate(X_test, y_test)\n\nloss = history3.history['loss']\nval_loss = history3.history ['val_loss']\n\nepochs = range(1, 11)\n\nplt.plot(epochs, loss, 'bo', label= 'training loss')\nplt.plot(epochs, val_loss, 'b', label= 'Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\n\nplt.show()","c5b9dcdd":"network4_start_time = time.time()\n\nnetwork4 = Sequential()\nnetwork4.add(Dense (60, activation='relu' , input_shape=(30,)))\nnetwork4.add(Dense (60, activation='relu'))\nnetwork4.add(Dense (1, activation='sigmoid'))\n            \nnetwork4.compile(optimizer='adamax',\n             loss='binary_crossentropy',\n             metrics=['accuracy',])\n            \nhistory4 = network4.fit(X_train, y_train, epochs = 10, batch_size=4, validation_data=(X_test, y_test))\n\n\nprint(\"--- %s seconds ---\" % (time.time() - network4_start_time))\n\nnetwork4.evaluate(X_test, y_test)\n\nloss = history4.history['loss']\nval_loss = history4.history ['val_loss']\n\nepochs = range(1, 11)\n\nplt.plot(epochs, loss, 'bo', label= 'training loss')\nplt.plot(epochs, val_loss, 'b', label= 'Validation loss')\nplt.xlabel('epochs')\nplt.ylabel('loss')\nplt.legend()\n\nplt.show()","44717d64":"models_metrics = {'logistic regression': [0.97891, 0.98106, 0.96224], \n                 'tuned logistic regression': [0.98242, 0.99036, 0.96235],\n                 'decision tree' : [0.92445,0.87080,0.91528],\n                 'tuned decision tree' : [0.91566,0.93318,0.91561],\n                 'svm' : [0.97544,0.97173,0.96235],\n                 'tuned svm' : [0.97541,0.98548,0.94817],\n                  'knn' : [0.95075,0.93891,0.92901],\n                 'tuned knn' : [0.96494,0.98986,0.91528],\n                  'gaussiannb' : [0.92808,0.91233,0.89657]\n                }\ndf = pd.DataFrame(data = models_metrics)\ndf.rename(index={0:'Accuracy',1:'Precision', 2: 'Recall'}, \n                 inplace=True)\nax = df.plot(kind='bar', figsize = (15,10), ylim = (0.86, 1), \n        color = ['#c6e2ff', '#7c96b3', '#fa85af', '#ccb1bd', '#a2b970', '#9bae88', '#d2ccda', '#9690a8' , '#f08080'],\n        rot = 0, title ='Models performance (cross val mean)',\n        edgecolor = 'grey', alpha = 0.5)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005))\nplt.show()","4ea4e012":"models_metrics = {'logistic regression': [0.0044], \n                 'decision tree' : [0.0270],\n                 'svm' : [0.0078],\n                  'knn' : [0.0017],\n                  'gaussiannb' : [0.0017]\n                }\ndf = pd.DataFrame(data = models_metrics)\ndf.rename(index={0:'time'}, \n                 inplace=True)\nax = df.plot(kind='bar', figsize = (15,10), ylim = (0.0015, 0.028), \n        color = ['#c6e2ff', '#fa85af', '#a2b970','#d2ccda', '#f08080'],\n        rot = 0, title ='Models performance (cross val mean)',\n        edgecolor = 'grey', alpha = 0.5)\nfor p in ax.patches:\n    ax.annotate(str(p.get_height()), (p.get_x() * 1.01, p.get_height() * 1.0005))\nplt.show()","ad0107fd":"###  <a id='3.1'>3.1 Tuning Parameters<\/a>","9291082c":"##  <a id='1.2'>1.2 Metrics and Plots<\/a>","b7896742":"##  <a id='7'>7. Neural Network<\/a>","d543f786":"##  <a id='2'>2. Logistic Regression<\/a>","8e69c2cb":"##  <a id='0'>0. Loading Libraries<\/a>","195c75a2":"##  <a id='5'>5. KNN<\/a>","a12a7adc":"###  <a id='4.1'>3.1 Tuning Parameters<\/a>","2bda69f1":"###  <a id='5.1'>5.1 Tuning Parameters<\/a>","5ffa76ab":"## <a id='8'>8. Results and Comparing<\/a>","4dc5dd8e":"### <a id='2.1'>2.1 Tuning Parameters<\/a>","97018c49":"Different optimizers:","2a2f4942":"# <a id='7.5'>Breast Cancer Detection<\/a>","aee4cee3":"##  <a id='6'>6. GaussianNB<\/a>","80b115dd":"##  <a id='4'>4. SVM<\/a>","827ed1d2":"**Overfitting!!!!**","bc25ca0f":"##  <a id='1.1'>1.1 Refining Data<\/a>","1dcc5b5f":"##  <a id='3'>3. Decision Tree<\/a>","f5e6699c":"\n> ## <a id='1'>1. Loading Data<\/a>"}}