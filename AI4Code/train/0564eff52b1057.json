{"cell_type":{"9899fd12":"code","44cbd344":"code","b1d8be76":"code","75fce25f":"code","b19c8884":"code","a4941433":"code","33691ba1":"code","e1c8d95d":"code","d2642d24":"code","7a92d779":"code","242d685d":"code","6b90e5e8":"code","251dac92":"code","36fe90cd":"code","016a4c2d":"code","126509a9":"code","01df4829":"code","345b78ab":"code","0a87f86e":"code","105c362a":"code","912addab":"code","777cb204":"code","51091373":"code","af39d328":"code","9049915d":"code","12652d98":"code","0bc05a5b":"code","baa996e3":"code","2ba48fe4":"code","32e2cf3a":"code","0844417f":"code","96014c43":"code","7440c557":"code","0b7213b8":"code","a476496c":"code","7ac9876e":"code","300af591":"code","235316da":"code","7d7a6d35":"code","1a2e6e18":"code","21443d33":"code","e042530e":"code","91faad1b":"code","f187b6e0":"code","2ef6aa5f":"code","563ddfa3":"code","70c5996e":"code","ee4d1030":"code","6e505bf9":"code","d0cfe447":"code","3ee8afe0":"code","67e9bb4c":"code","5f66a61c":"code","be57c9b6":"code","0075c2cd":"code","ed5d4e5a":"code","18cd4743":"code","4efaa697":"code","1703d2c4":"code","d2a1734b":"code","bb276cfa":"code","d85ee6a7":"code","29c3d00e":"code","b418d1a3":"code","28409bd4":"code","2c6a6742":"code","fdcd314a":"code","b90813d0":"code","b1132ab3":"code","8b3f8260":"code","bd108a10":"code","470099a7":"markdown","3e75baab":"markdown","b8781acf":"markdown","97c1cc9f":"markdown","9bcdb2a0":"markdown","fb9968e6":"markdown","6fceaa6a":"markdown","4b0e244b":"markdown","40fe15da":"markdown","0b42428a":"markdown","a07df390":"markdown","b4ee4499":"markdown","e1540726":"markdown","4679f36f":"markdown","08bac3f2":"markdown","02f37afd":"markdown","6f220158":"markdown","835b7438":"markdown","647bda5a":"markdown"},"source":{"9899fd12":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings \nwarnings.filterwarnings('ignore')\nplt.style.use('ggplot')","44cbd344":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","b1d8be76":"train.head()","75fce25f":"train.shape","b19c8884":"plt.figure(figsize=(15,5))\nplt.scatter(train.index,train.SalePrice.sort_values().reset_index(drop=True))\nplt.title('Distribution Plot for Sales Prices')\nplt.ylabel('Sales Price');","a4941433":"sns.heatmap(train.isnull(),yticklabels=False, cmap='plasma')","33691ba1":"train.isnull().sum().sort_values(ascending=False)[0:19]","e1c8d95d":"test.isnull().sum().sort_values(ascending=False)[0:33]","d2642d24":"train.LotFrontage.head()","7a92d779":"train.LotFrontage.isnull().sum()","242d685d":"train['LotFrontage'] = train['LotFrontage'].fillna(train.LotFrontage.mean())","6b90e5e8":"test.LotFrontage.isnull().sum()","251dac92":"test['LotFrontage'] = test['LotFrontage'].fillna(test.LotFrontage.mean())","36fe90cd":"train.Alley.value_counts(dropna=False)","016a4c2d":"train.drop(columns=['Alley'], inplace=True)","126509a9":"test.Alley.value_counts(dropna=False)","01df4829":"test.drop(columns=['Alley'], inplace=True)","345b78ab":"train.BsmtCond.value_counts(dropna=False)","0a87f86e":"train['BsmtCond'] = train['BsmtCond'].fillna(train.BsmtCond.mode()[0])","105c362a":"test['BsmtCond'] = test['BsmtCond'].fillna(test.BsmtCond.mode()[0])","912addab":"list1 = ['BsmtQual', 'FireplaceQu', 'GarageType', 'GarageCond', 'GarageFinish', 'GarageQual', 'MasVnrType', 'MasVnrArea',\n         'BsmtExposure','BsmtFinType2']\n\nfor item in list1:\n    train[item] = train[item].fillna(train[item].mode()[0])\n    test[item] = test[item].fillna(test[item].mode()[0])","777cb204":"list1 = ['GarageYrBlt', 'PoolQC', 'Fence', 'MiscFeature']\n\nfor item in list1:\n    train.drop(columns=item, inplace=True)\n    test.drop(columns=item, inplace=True)","51091373":"train.isnull().sum().sort_values(ascending=False)","af39d328":"train.dropna(inplace=True)","9049915d":"train.drop(columns=['Id'], inplace=True)","12652d98":"train.shape","0bc05a5b":"test.isnull().sum().sort_values(ascending=False)[0:17]","baa996e3":"test['MSZoning']=test['MSZoning'].fillna(test['MSZoning'].mode()[0])","2ba48fe4":"columns = ['BsmtFinType1', 'Utilities','BsmtFullBath', 'BsmtHalfBath', 'Functional', 'SaleType', 'Exterior2nd', \n           'Exterior1st', 'KitchenQual']\ncolumns1 = ['GarageCars', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',  'TotalBsmtSF', 'GarageArea']\n\nfor item in columns:\n    test[item] = test[item].fillna(test[item].mode()[0])\nfor item in columns1:\n    test[item] = test[item].fillna(test[item].mean())","32e2cf3a":"test.drop(columns=['Id'], inplace=True)","0844417f":"test.shape","96014c43":"train.isnull().any().any()","7440c557":"test.isnull().any().any()","0b7213b8":"columns = ['MSZoning', 'Street',\n       'LotShape', 'LandContour', 'Utilities', 'LotConfig', 'LandSlope',\n       'Neighborhood', 'Condition1', 'Condition2', 'BldgType', 'HouseStyle', 'RoofStyle', \n       'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual', 'BsmtCond',\n       'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n       'Heating', 'HeatingQC', 'CentralAir', 'Electrical',\n       'KitchenQual', 'Functional', 'FireplaceQu', 'GarageType', 'GarageFinish',\n       'GarageQual', 'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']","a476496c":"len(columns)","7ac9876e":"final_df = pd.concat([train, test], axis=0)","300af591":"final_df.shape","235316da":"def One_hot_encoding(columns):\n    df_final=final_df\n    i=0\n    for fields in columns:\n        df1=pd.get_dummies(final_df[fields],drop_first=True)\n        \n        final_df.drop([fields],axis=1,inplace=True)\n        if i==0:\n            df_final=df1.copy()\n        else:           \n            df_final=pd.concat([df_final,df1],axis=1)\n        i=i+1\n       \n        \n    df_final=pd.concat([final_df,df_final],axis=1)\n        \n    return df_final","7d7a6d35":"final_df = One_hot_encoding(columns)","1a2e6e18":"final_df.shape","21443d33":"final_df =final_df.loc[:,~final_df.columns.duplicated()]","e042530e":"final_df.shape","91faad1b":"df_Train=final_df.iloc[:1422,:]\ndf_Test=final_df.iloc[1422:,:]","f187b6e0":"df_Test.drop(['SalePrice'],axis=1,inplace=True)","2ef6aa5f":"X_train=df_Train.drop(['SalePrice'],axis=1)\ny_train=df_Train['SalePrice']","563ddfa3":"from sklearn.preprocessing import StandardScaler\nX_std = StandardScaler().fit_transform(X_train)\n\nmy_columns = X_train.columns\nnew_df = pd.DataFrame(X_std, columns=my_columns)","70c5996e":"from sklearn.decomposition import PCA\npca = PCA(n_components = 2)\ndf_pca = pca.fit_transform(new_df)","ee4d1030":"plt.figure(figsize =(8, 6))\nplt.scatter(df_pca[:, 0], df_pca[:, 1], c = y_train, cmap ='plasma')\n# labeling x and y axes\nplt.xlabel('First Principal Component')\nplt.ylabel('Second Principal Component');","6e505bf9":"from sklearn.ensemble import RandomForestRegressor\n\nregressor = RandomForestRegressor()","d0cfe447":"from sklearn.model_selection import RandomizedSearchCV\n\nn_estimators = [100, 500, 900]\ncriterion = ['mse', 'mae']\ndepth = [3,5,10,15]\nmin_split=[2,3,4]\nmin_leaf=[2,3,4]\nbootstrap = ['True', 'False']\nverbose = [5]\n\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':depth,\n    'criterion':criterion,\n    'bootstrap':bootstrap,\n    'verbose':verbose,\n    'min_samples_split':min_split,\n    'min_samples_leaf':min_leaf\n    }\n\nrandom_cv = RandomizedSearchCV(estimator=regressor,\n                               param_distributions=hyperparameter_grid,\n                               cv=5, \n                               scoring = 'neg_mean_absolute_error',\n                               n_jobs = 4, \n                               return_train_score = True,\n                               random_state=42)","3ee8afe0":"random_cv.fit(X_train,y_train)","67e9bb4c":"random_cv.best_estimator_","5f66a61c":"regressor = RandomForestRegressor(bootstrap='True', ccp_alpha=0.0, criterion='mae',\n                      max_depth=15, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=2,\n                      min_samples_split=4, min_weight_fraction_leaf=0.0,\n                      n_estimators=100, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=5, warm_start=False)","be57c9b6":"regressor.fit(X_train,y_train)","0075c2cd":"y_pred = regressor.predict(df_Test)","ed5d4e5a":"y_pred","18cd4743":"pred=pd.DataFrame(y_pred)\nsamp = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsub = pd.concat([samp['Id'],pred], axis=1)\nsub.columns=['Id','SalePrice']","4efaa697":"sub","1703d2c4":"#sub.to_csv('My_sub.csv',index=False)","d2a1734b":"import xgboost","bb276cfa":"regressor=xgboost.XGBRegressor()","d85ee6a7":"n_estimators = [100, 500, 900, 1100, 1500]\nmax_depth = [2, 3, 5, 10, 15]\nbooster=['gbtree','gblinear']\nlearning_rate=[0.05,0.1,0.15,0.20]\nmin_child_weight=[1,2,3,4]\nbase_score=[0.25,0.5,0.75,1]\n\n# Define the grid of hyperparameters to search\nhyperparameter_grid = {\n    'n_estimators': n_estimators,\n    'max_depth':max_depth,\n    'learning_rate':learning_rate,\n    'min_child_weight':min_child_weight,\n    'booster':booster,\n    'base_score':base_score\n    }\nrandom_cv = RandomizedSearchCV(estimator=regressor,\n            param_distributions=hyperparameter_grid,\n            cv=5, n_iter=50,\n            scoring = 'neg_mean_absolute_error',n_jobs = 4,\n            verbose = 5, \n            return_train_score = True,\n            random_state=42)","29c3d00e":"random_cv.fit(X_train,y_train)","b418d1a3":"random_cv.best_estimator_","28409bd4":"regressor = xgboost.XGBRegressor(base_score=0.25, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.1, max_delta_step=0, max_depth=2,\n             min_child_weight=1, missing=None, monotone_constraints='()',\n             n_estimators=900, n_jobs=0, num_parallel_tree=1,\n             objective='reg:squarederror', random_state=0, reg_alpha=0,\n             reg_lambda=1, scale_pos_weight=1, subsample=1, tree_method='exact',\n             validate_parameters=1, verbosity=None)","2c6a6742":"regressor.fit(X_train,y_train)","fdcd314a":"y_pred = regressor.predict(df_Test)","b90813d0":"y_pred","b1132ab3":"pred=pd.DataFrame(y_pred)\nsamp = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsub = pd.concat([samp['Id'],pred], axis=1)\nsub.columns=['Id','SalePrice']","8b3f8260":"sub","bd108a10":"#sub.to_csv('My_sub1.csv',index=False)","470099a7":" Now let's solve the issue of missing value by looking into every feature\n \n **- LonFrontage**","3e75baab":"# Import dataset","b8781acf":"**- BsmtCond, BsmtQual, FirePlaceQu, GarageType, GarageCond, GarageFinish, GarageQual**","97c1cc9f":"<center><h1 style='color:green'>PLz upvote if you find it valuable!","9bcdb2a0":"# Hyperparameter Tunning\n\n**Do not trust the defaults!, let's change the default parameters by different values**","fb9968e6":"**Let's train our models.**","6fceaa6a":"## Feature Engineering by OneHotEncoding\n\nCreating the list of categorical features that needs to be converted in binary values","4b0e244b":"# Handle Missing Values\n\nlet's look at train and test set and determine the missing values!","40fe15da":"**- Alley**","0b42428a":"**- GarageYrBlt, PoolQC, Fence, MiscFeature**","a07df390":"# PCA(Principle component analysis)\nlet\u2019s visualize our final dataset by implementing PCA and plot the graph","b4ee4499":"# - Contents:\n\n### 1. Include Libraries\n### 2. Import DataSet\n### 3. Handle Missing Value\n### 4. Feature Engineering by OneHotEncoding\n### 5. PCA(Principle component analysis)\n### 6. Hyperparameter Tunning\n### 7. Train Random Forest Regressor\n### 8. Train Xgboost Regressor\n","e1540726":"Taking mode for all similar features like BsmtCond","4679f36f":"# Train Random Forest Regressor\n\n### Caution: Remember to use random_forest_regressor(avoid random_forest_classifier),as this is a regressor type problem. I have tried using random forest classifier the scores after training is as follows,\n\n- random forest classifier - 0.21\n- random forest regressor - 0.15\n\nRandom Forest regressor has a massive increase in score.","08bac3f2":"**Handle Remaining missing values**","02f37afd":"# Train Xgboost Regressor","6f220158":"**Un comment the below code to generate csv file.**","835b7438":"# Include Libraries","647bda5a":"### Checking for missing values if any!"}}