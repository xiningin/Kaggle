{"cell_type":{"894e7783":"code","59152016":"code","70c28d1e":"code","2fece886":"code","ee21bb60":"code","6956aebd":"code","515e9c3b":"code","81cfcede":"code","a006cb45":"code","7ea7eab7":"code","0f89ecf8":"code","192124eb":"code","3c8f0bd5":"code","4e24e6ce":"code","be01fe1f":"code","8302933e":"code","0535d244":"code","8aa3e5f1":"code","dd1e16c3":"code","98d7686c":"code","ab0fc8de":"code","71cd4277":"code","4cbbc09d":"code","04a9a5b0":"code","11e02796":"code","fdcc5e51":"code","6e7a8b40":"code","6cb9d9e5":"code","823fdda0":"code","8b51618b":"code","ec641426":"code","27b04a71":"code","4c1fe261":"code","9599cba3":"code","c6371044":"code","0f3681ca":"code","af18ab09":"markdown","c4a210f9":"markdown","37287da8":"markdown","e334bba2":"markdown"},"source":{"894e7783":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport spacy\nimport nltk\n# from textblob import TextBlob\nfrom bs4 import BeautifulSoup\nimport unicodedata\nimport gensim\nimport re\nspacy_nlp = spacy.load('en')\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nsns.set(style = 'darkgrid') # setting style of facetgrid in seaborn\n\n","59152016":"# setting display options for dataframe\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_colwidth', 500)","70c28d1e":"# loading the dataset from my github\ndata = pd.read_json('https:\/\/raw.githubusercontent.com\/matsujju\/All_Data_files\/master\/stackoverflow_scrapped\/stackoverflow_scrapped_data.json', encoding = 'utf8')","2fece886":"data.shape","ee21bb60":"# There won't be any missing values as I scrapped the data myself but still good to check\nprint('Number of missing Questions:')\ndata['Text_of_quest'].isnull().sum()","6956aebd":"data['Text_of_quest'] = [item.replace('...','') for item in data['Text_of_quest']]","515e9c3b":"data.drop(columns =['_type'] , inplace = True )","81cfcede":"# All the functions needed to clean the text data for proper usage\nfrom nltk.tokenize import word_tokenize\nfrom nltk.tokenize.toktok import ToktokTokenizer\ntokenizer = ToktokTokenizer()\nstopword_list = nltk.corpus.stopwords.words('english')\n\ndef data_clean(text):\n    text = text.lower()  # lowering of characters\n    text = text.strip()  # removeing extra spaces before and after the sentence\n    # text = TextBlob(text).correct() # spelling mistakes correction  (very slow) (maybe try directly on dataframe column)\n    return text\n\n\n# remove accented characters (S\u00f3m\u011b \u00c1cc\u011bnt\u011bd t\u011bxt)\ndef remove_accented_chars(text):\n    text = (\n        unicodedata.normalize(\"NFKD\", text)\n        .encode(\"ascii\", \"ignore\")\n        .decode(\"utf-8\", \"ignore\")\n    )\n    return text\n\n\ndef strip_html_tags(text):  # function for removing html tags\n    soup = BeautifulSoup(text, \"html.parser\")\n    stripped_text = soup.get_text()\n    return stripped_text\n\n\ndef remove_special_characters(text, remove_digits=False):  # function for removing punctuations , before using this remove any contractions(like \"I'll\" --> \"I will\") in text data\n    pattern = r\"[^a-zA-z0-9\\s]\" if not remove_digits else r\"[^a-zA-z\\s]\"\n    text = re.sub(pattern, \" \", text)\n    return text\n\n\n# # custom way to remove punctuations of your choice from all of them (than regex one)\n# puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3',\n#  '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026',\n#  '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500',\n#  '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e',\n#  '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\n\n# def punctuation(x):  # before using this remove any contractions(like \"I'll\" --> \"I will\") in text data\n#     x = str(x)\n#     for punct in puncts:\n#         if punct in x:\n#             x = x.replace(punct, ' ')\n#     return x\n\n\ndef remove_links_emojis(text):\n    pattern = re.compile( \"http[s]?:\/\/(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+\")  # removing https:www.examples.com\n    text = pattern.sub(\"\", text)\n\n    emoji = re.compile(\"[\"u\"\\U0001F600-\\U0001FFFF\"  # emoticons\n                        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                        u\"\\U00002702-\\U000027B0\"\n                        u\"\\U000024C2-\\U0001F251\"\"]+\",\n        flags=re.UNICODE,\n    )\n    text = emoji.sub(r\"\", text)\n    return text\n\ndef contractions(text):  # contraction of text i.e remove_apostrophes\n    text = re.sub(\"ain't\", \"is not\", str(text))\n    text = re.sub(\"aren't\", \"are not\", str(text))\n    text = re.sub(\"can't\", \"cannot\", str(text))\n    text = re.sub(\"can't've\", \"cannot have\", str(text))\n    text = re.sub(\"'cause\", \"because\", str(text))\n    text = re.sub(\"could've\", \"could have\", str(text))\n    text = re.sub(\"couldn't\", \"could not\", str(text))\n    text = re.sub(\"couldn't've\", \"could not have\", str(text))\n    text = re.sub(\"didn't\", \"did not\", str(text))\n    text = re.sub(\"doesn't\", \"does not\", str(text))\n    text = re.sub(\"don't\", \"do not\", str(text))\n    text = re.sub(\"hadn't\", \"had not\", str(text))\n    text = re.sub(\"hadn't've\", \"had not have\", str(text))\n    text = re.sub(\"hasn't\", \"has not\", str(text))\n    text = re.sub(\"haven't\", \"have not\", str(text))\n    text = re.sub(\"he'd\", \"he would\", str(text))\n    text = re.sub(\"he'd've\", \"he would have\", str(text))\n    text = re.sub(\"he'll\", \"he will\", str(text))\n    text = re.sub(\"he'll've\", \"he he will have\", str(text))\n    text = re.sub(\"he's\", \"he is\", str(text))\n    text = re.sub(\"how'd\", \"how did\", str(text))\n    text = re.sub(\"how'd'y\", \"how do you\", str(text))\n    text = re.sub(\"how'll\", \"how will\", str(text))\n    text = re.sub(\"how's\", \"how is\", str(text))\n    text = re.sub(\"I'd\", \"I would\", str(text))\n    text = re.sub(\"I'd've\", \"I would have\", str(text))\n    text = re.sub(\"I'll\", \"I will\", str(text))\n    text = re.sub(\"I'll've\", \"I will have\", str(text))\n    text = re.sub(\"I'm\", \"I am\", str(text))\n    text = re.sub(\"I've\", \"I have\", str(text))\n    text = re.sub(\"i'd\", \"i would\", str(text))\n    text = re.sub(\"i'd've\", \"i would have\", str(text))\n    text = re.sub(\"i'll\", \"i will\", str(text))\n    text = re.sub(\"i'll've\", \"i will have\", str(text))\n    text = re.sub(\"i'm\", \"i am\", str(text))\n    text = re.sub(\"i've\", \"i have\", str(text))\n    text = re.sub(\"isn't\", \"is not\", str(text))\n    text = re.sub(\"it'd\", \"it would\", str(text))\n    text = re.sub(\"it'd've\", \"it would have\", str(text))\n    text = re.sub(\"it'll\", \"it will\", str(text))\n    text = re.sub(\"it'll've\", \"it will have\", str(text))\n    text = re.sub(\"it's\", \"it is\", str(text))\n    text = re.sub(\"let's\", \"let us\", str(text))\n    text = re.sub(\"ma'am\", \"madam\", str(text))\n    text = re.sub(\"mayn't\", \"may not\", str(text))\n    text = re.sub(\"might've\", \"might have\", str(text))\n    text = re.sub(\"mightn't\", \"might not\", str(text))\n    text = re.sub(\"mightn't've\", \"might not have\", str(text))\n    text = re.sub(\"must've\", \"must have\", str(text))\n    text = re.sub(\"mustn't\", \"must not\", str(text))\n    text = re.sub(\"mustn't've\", \"must not have\", str(text))\n    text = re.sub(\"needn't\", \"need not\", str(text))\n    text = re.sub(\"needn't've\", \"need not have\", str(text))\n    text = re.sub(\"o'clock\", \"of the clock\", str(text))\n    text = re.sub(\"oughtn't\", \"ought not\", str(text))\n    text = re.sub(\"oughtn't've\", \"ought not have\", str(text))\n    text = re.sub(\"shan't\", \"shall not\", str(text))\n    text = re.sub(\"sha'n't\", \"shall not\", str(text))\n    text = re.sub(\"shan't've\", \"shall not have\", str(text))\n    text = re.sub(\"she'd\", \"she would\", str(text))\n    text = re.sub(\"she'd've\", \"she would have\", str(text))\n    text = re.sub(\"she'll\", \"she will\", str(text))\n    text = re.sub(\"she'll've\", \"she will have\", str(text))\n    text = re.sub(\"she's\", \"she is\", str(text))\n    text = re.sub(\"should've\", \"should have\", str(text))\n    text = re.sub(\"shouldn't\", \"should not\", str(text))\n    text = re.sub(\"shouldn't've\", \"should not have\", str(text))\n    text = re.sub(\"so've\", \"so have\", str(text))\n    text = re.sub(\"so's\", \"so as\", str(text))\n    text = re.sub(\"that'd\", \"that would\", str(text))\n    text = re.sub(\"that'd've\", \"that would have\", str(text))\n    text = re.sub(\"that's\", \"that is\", str(text))\n    text = re.sub(\"there'd\", \"there would\", str(text))\n    text = re.sub(\"there'd've\", \"there would have\", str(text))\n    text = re.sub(\"there's\", \"there is\", str(text))\n    text = re.sub(\"they'd\", \"they would\", str(text))\n    text = re.sub(\"they'd've\", \"they would have\", str(text))\n    text = re.sub(\"they'll\", \"they will\", str(text))\n    text = re.sub(\"they'll've\", \"they will have\", str(text))\n    text = re.sub(\"they're\", \"they are\", str(text))\n    text = re.sub(\"they've\", \"they have\", str(text))\n    text = re.sub(\"to've\", \"to have\", str(text))\n    text = re.sub(\"wasn't\", \"was not\", str(text))\n    text = re.sub(\"we'd\", \"we would\", str(text))\n    text = re.sub(\"we'd've\", \"we would have\", str(text))\n    text = re.sub(\"we'll\", \"we will\", str(text))\n    text = re.sub(\"we'll've\", \"we will have\", str(text))\n    text = re.sub(\"we're\", \"we are\", str(text))\n    text = re.sub(\"we've\", \"we have\", str(text))\n    text = re.sub(\"weren't\", \"were not\", str(text))\n    text = re.sub(\"what'll\", \"what will\", str(text))\n    text = re.sub(\"what'll've\", \"what will have\", str(text))\n    text = re.sub(\"what're\", \"what are\", str(text))\n    text = re.sub(\"what's\", \"what is\", str(text))\n    text = re.sub(\"what've\", \"what have\", str(text))\n    text = re.sub(\"when's\", \"when is\", str(text))\n    text = re.sub(\"when've\", \"when have\", str(text))\n    text = re.sub(\"where'd\", \"where did\", str(text))\n    text = re.sub(\"where's\", \"where is\", str(text))\n    text = re.sub(\"where've\", \"where have\", str(text))\n    text = re.sub(\"who'll\", \"who will\", str(text))\n    text = re.sub(\"who'll've\", \"who will have\", str(text))\n    text = re.sub(\"who's\", \"who is\", str(text))\n    text = re.sub(\"who've\", \"who have\", str(text))\n    text = re.sub(\"why's\", \"why is\", str(text))\n    text = re.sub(\"why've\", \"why have\", str(text))\n    text = re.sub(\"will've\", \"will have\", str(text))\n    text = re.sub(\"won't\", \"will not\", str(text))\n    text = re.sub(\"won't've\", \"will not have\", str(text))\n    text = re.sub(\"would've\", \"would have\", str(text))\n    text = re.sub(\"wouldn't\", \"would not\", str(text))\n    text = re.sub(\"wouldn't've\", \"would not have\", str(text))\n    text = re.sub(\"y'all\", \"you all\", str(text))\n    text = re.sub(\"y'all'd\", \"you all would\", str(text))\n    text = re.sub(\"y'all'd've\", \"you all would have\", str(text))\n    text = re.sub(\"y'all're\", \"you all are\", str(text))\n    text = re.sub(\"y'all've\", \"you all have\", str(text))\n    text = re.sub(\"you'd\", \"you would\", str(text))\n    text = re.sub(\"you'd've\", \"you would have\", str(text))\n    text = re.sub(\"you'll\", \"you will\", str(text))\n    text = re.sub(\"you'll've\", \"you will have\", str(text))\n    text = re.sub(\"you're\", \"you are\", str(text))\n    text = re.sub(\"you've\", \"you have\", str(text))\n    return text\n\n\n# document = document.replace(\u2018\u2026\u2019, \u2018\u2019) # small nuances\n# document = document.replace(\u2018Mr.\u2019, \u2018Mr\u2019).replace(\u2018Mrs.\u2019, \u2018Mrs\u2019) # small nuances to look for\n\n\ndef remove_hashtags(text):\n    for item in text.split():\n        if item.startswith(\"#\"):\n            text = text.append(item)\n    return text\n\n\ndef lemmatize_text(text):  # function for lemmetization of text\n    text = spacy_nlp(text)\n    text = str( \" \".join([word.lemma_ if word.lemma_ != \"-PRON-\" else word.text for word in text]))\n    return text\n\ndef remove_stopwords(text):\n    tokens = tokenizer.tokenize(text)\n    tokens = [token.strip() for token in tokens]\n    \n    filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n    filtered_text = ' '.join(filtered_tokens)    \n    return filtered_text","a006cb45":"# function to call above function in one go\ndef preprocess(text, raw_data = True , \n               contracted_text = True, \n               accented_chars = True,\n               want_lemmatization = True, link_emoji = True,\n               html_tags = True, \n               special_characters = True,\n               hashtags = False,remove_digits = True,\n               stop_words = True):\n    \n# cleaned data (lower & strip whitespaces & spelling mistake)\n    if raw_data:\n        text = data_clean(text)\n    #strip html\n    if html_tags:\n        text = strip_html_tags(text)\n    # accented char removal\n    if accented_chars:\n        text = remove_accented_chars(text)\n    # hashtags\n    if hashtags:\n        text = remove_hashtags(text)\n    # exapand contraction\n    if contracted_text:\n        text = contractions(text)\n    if stop_words:\n        text = remove_stopwords(text)\n    # lemmetization\n    if want_lemmatization:\n        text = lemmatize_text(text)\n    # punctuations and digits\n    if special_characters:\n        text = remove_special_characters(text,remove_digits = remove_digits)\n\n    # remove extra whitespace\n    text = re.sub(' +', ' ', text)\n\n    \n    \n    return text","7ea7eab7":"# Testing my custom preprocessing functions\ntext = 'think this #90 may be an issue in scikit. C++ cross_val_score ultimately makes a call to the score function for whatever estimator is passed to it. Typically, score (e.g. in KMeans) returns a float. And when a KMeans estimator is passed to cross_val_score, all is well:'\npreprocess(text, remove_digits = False)","0f89ecf8":"data.columns","192124eb":"data['full_text'] = data['Ques']+' '+data['Text_of_quest']","3c8f0bd5":"data[\"cleaned_text\"] = [preprocess(item ,html_tags =False, remove_digits = False) for item in data[\"full_text\"]]","4e24e6ce":"data.sample(2)","be01fe1f":"data.drop(columns = ['Text_of_quest','Ques','full_text'], inplace= True)","8302933e":"print(f'There are {len(pd.DataFrame(data.Tags.values.tolist()).stack().value_counts())} number of Tags present in the data')","0535d244":"count = pd.DataFrame(data.Tags.values.tolist()).stack().value_counts()","8aa3e5f1":"# this number(250) came with lots of hit and trial, without losing much Questions\ncount[count>250][:5] ","dd1e16c3":"filtered = count[count>250]","98d7686c":"filtered = filtered.index.tolist()","ab0fc8de":"# remove the tags which are not frequent (i.e not found more than 250 in all questions)\ndata['filtered_tags'] = [[item for item in l if item in set(filtered)] for l in data['Tags']]","71cd4277":"data.shape","4cbbc09d":"data['length'] = [len(item) for item in data['Tags']]","04a9a5b0":"# data['tags'] = [','.join(map(str,item))for item in data['Tags']]","11e02796":"data.head()","fdcc5e51":"df2 = pd.DataFrame(data.Tags.values.tolist()).stack().value_counts().to_frame().reset_index()\ndf2.columns = ['tags','counts']","6e7a8b40":"df2.head()","6cb9d9e5":"# This is saved and uploaded to my github for modelling\n# data.to_json('cleaned_stackoverflow_questions.json',orient = 'records',lines=True)","823fdda0":"# Number of Questions in each category of Tags\nimport plotly.express as px\nplt.figure(figsize = (35,10))\nfig = px.line(df2,y = df2['counts'].values[:100],x = df2.tags[:100],labels={\n                     \"y\": \"# of Occurrences\",\n                     \"x\": \"Tags\",\n                     \n                 },)\nfig.update_layout( \n        title = 'Number of comments per labels',\n        hovermode=\"y\",  # it shows the value along x-axis ticks on hovering in plot\n        hoverdistance=30000,  # distance above\/right of the plot till where the user can hover\n        spikedistance=30000,  # length of spike according to data max value\n        xaxis=dict(\n            tickmode=\"linear\",  # how tick value will change\n            tick0=1,  # starting tick in plot along respective axis\n            dtick=1,  # gap between each ticks\n            visible=True,\n            showgrid=False,\n            showspikes=True,  # Show spike line for X-axis\n            # Format spike\n            spikethickness=2,\n            spikedash=\"dot\",\n            spikecolor=\"#999999\",\n            spikemode=\"across\",\n            linecolor=\"#737373\",\n            tickangle = 90,),\n        yaxis = dict(showspikes=True,\n                     spikethickness=2,\n                     spikedash=\"dot\",\n                     spikecolor=\"#999999\",\n                     spikemode=\"across\",\n                     linecolor=\"#737373\",))\nfig.show()","8b51618b":"max(data['length'])","ec641426":"# How many Questions have multi labels?\nfig = px.bar(data['length'].value_counts(),title = 'Multiple Tags per Question', labels=\n            {'index':'# of Tags','value' :'# of Occurrences'})\nfig.layout.update(showlegend = False)\nfig.show()","27b04a71":"#distribution of the number of words in question texts\nlens = data.cleaned_text.str.len()\nlens.hist(bins = np.arange(0,500,10))","4c1fe261":"data['tags'] = [','.join(map(str,item))for item in data['Tags']]","9599cba3":"text = \" \".join(item for item in data.cleaned_text)\ntags = \" \".join(item for item in data.tags)\nprint (\"There are {} words in the combination of all Questions.\".format(len(text)))\nprint (\"There are {} words in the combination of all tags.\".format(len(tags)))","c6371044":"from PIL import Image\nfrom wordcloud import WordCloud, ImageColorGenerator\n\ntext_wordcloud = WordCloud(max_words=10000, height = 400, width = 800).generate(text)  # since it is cleaned text so we need not to worry about stopwords\nplt.imshow(text_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","0f3681ca":"tags_wordcloud = WordCloud(max_words=10000).generate(tags)  # since it is cleaned text so we need not to worry about stopwords\nplt.imshow(tags_wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","af18ab09":"### This problem is Multi label Classification as for every text there will be more than one label. \n1. [For more details visit sklearn-multi-label](http:\/\/scikit.ml\/api\/skmultilearn.html#module-skmultilearn.ensemble)\n2. [Model Evaluation](https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html)\n3. [sklearn multi label model](https:\/\/scikit-learn.org\/stable\/modules\/multiclass.html#multilabel-classification-format)\n4. [Detailed Article (Analytics Vidya)](https:\/\/www.analyticsvidhya.com\/blog\/2017\/08\/introduction-to-multi-label-classification\/)\n   - Approaches:\n       - Problem Transformation :\n           * Three different ways:\n               - Binary Relevance : treats each label as a separate single class classification problem. \n               - Classifier Chains : first classifier is trained just on the input data and then each next classifier is trained on the input space and all the previous classifiers in the chain.  \n               - Label Powerset : transform the problem into a multi-class problem with one multi-class classifier is trained on all unique label combinations found in the training data. \n       - Adapted Algorithm : adapting the algorithm to directly perform multi-label classification, rather than transforming the problem into different subsets of problems. \n       - Ensemble approaches : ensembling classification functions, which we can use for obtaining better results. ","c4a210f9":"Most of the Question text length are within 150 characters, with some outliers up to 250 characters long.","37287da8":"### **We are going to build model on top of our cleaned and processed data in next notebook. So look for that.**","e334bba2":"#### These are too many labels and if build models for these it will be huge in terms of computation as well as time to train it. So we will reduce it down to as less as possible where we don't lose much Questions."}}