{"cell_type":{"d6d42a36":"code","a5122806":"code","596422ee":"code","719a782c":"code","ff1b043e":"code","76af4050":"code","5f92c638":"code","f0816a1b":"code","b6dfdfea":"code","d858a57a":"code","8caf33fd":"code","6de22049":"code","4e72277e":"code","c05ad015":"code","1a62238b":"code","02ee2b4c":"code","7b5dc71a":"code","185aa889":"code","caab4644":"code","7fbf1419":"code","9aa00521":"code","eb164457":"code","389a6f06":"code","cb9995e0":"code","28f85fcd":"code","454d0c96":"code","70e00fac":"code","c8bddf85":"code","15a143c0":"code","b08fcad3":"code","798df2a0":"code","607b9000":"code","7097f4de":"markdown","c08ae953":"markdown","ebcbd52d":"markdown","b1efc63a":"markdown","c04128e5":"markdown","4d244ffb":"markdown","aa6e877a":"markdown","4cdec056":"markdown","5b232e2f":"markdown","9246e476":"markdown","8509619d":"markdown","772605eb":"markdown"},"source":{"d6d42a36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# import the used lib\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport sklearn\nfrom matplotlib import pyplot as plt\nimport json\nimport glob\nfrom wordcloud import WordCloud\nimport re","a5122806":"\nroot_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nmetadata_path = f'{root_path}\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'title': str,\n    'pubmed_id': str,\n    'Microsoft Academic Paper ID': str, \n    'doi': str\n})\nmeta_df.head(2)","596422ee":"meta_df.info()","719a782c":"all_json = glob.glob(f'{root_path}\/**\/*.json', recursive=True)\nlen(all_json)","ff1b043e":"with open(all_json[10]) as file:\n    print(all_json[10])\n    first_entry = json.load(file)\n#     print(json.dumps(first_entry, indent=4)) \n    \nkeys = first_entry.keys()\nprint(keys)\nfor ky in keys:\n    print(len(ky))\n    \nprint(type(first_entry))\n\nfor key in first_entry.keys():\n    print(key)\n    value = first_entry[key]\n    if type(value).__name__ == 'list':\n        print(value[0].keys())\n    elif type(value).__name__ == 'dict':\n        print(value.keys())\n    else:\n        print(value)","76af4050":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.title = content['metadata']['title']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n            \n    def __repr__(self):\n        return f'paper_id:{self.paper_id}:\\ntitle:{self.title}\\nabstract:{self.abstract[:200]}...\\nbody:{self.body_text[:200]}...'\nfirst_entry = FileReader(all_json[0])\nprint(first_entry)","5f92c638":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","f0816a1b":"dict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'publish_time': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:# abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add the journal information and publish time\n    dict_['journal'].append(meta_data['journal'].values[0])\n    dict_['publish_time'].append(meta_data['publish_time'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title', 'journal', 'publish_time', 'abstract_summary'])\ndf_covid.head(2)","b6dfdfea":"df_covid.info()","d858a57a":"dict_ = None","8caf33fd":"df_covid['abstract_word_count'] = df_covid['abstract'].apply(lambda x: len(x.strip().split()))\ndf_covid['body_word_count'] = df_covid['body_text'].apply(lambda x: len(x.strip().split()))\ndf_covid.head(2)","6de22049":"df_covid.describe(include='all')","4e72277e":"output_path = '.\/clean_covid_data.csv'\ndf_covid.drop_duplicates(['abstract','body_text'], inplace=True)\ndf_covid.describe(include='all')\ndf_covid.to_csv(output_path)","c05ad015":"# clean_data_path = '.\/clean_covid_data.csv'\n# df_covid = pd.read_csv(clean_data_path)","1a62238b":"# df_title = df_covid.loc[:, [\"title\"]].dropna()\n# df_title.info()\n# df_abstract = df_covid.loc[:, [\"abstract\"]].dropna()\n# df_abstract.info()","02ee2b4c":"# def lower_case(x):\n#     return x.lower()\n\n# df_title[\"title\"] = df_title['title'].apply(lambda x: lower_case(x))\n# df_title[\"title\"] = df_title['title'].apply(lambda x: x.strip())\n# df_title[\"title\"] = df_title['title'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n# df_title[\"title\"] = df_title['title'].apply(lambda x: re.sub(' +',' ',x))\n# titles = ' '.join(df_title[\"title\"])\n\n\n# df_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: lower_case(x))\n# df_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: x.strip())\n# df_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: re.sub('[^a-zA-z0-9\\s]','',x))\n# df_abstract[\"abstract\"] = df_abstract['abstract'].apply(lambda x: re.sub(' +',' ',x))\n# abstracts = ' '.join(df_abstract[\"abstract\"])\n\n# print(titles[:100])\n# print(abstracts[:200])","7b5dc71a":"# from nltk.corpus import stopwords\n# import scipy.misc\n# from matplotlib.pyplot import imread\n# stopword = stopwords.words('english')  # remove the stop words\n\n# wordcloud_title = WordCloud(max_font_size=None, background_color='white', \n#                       collocations=False, stopwords=stopword,\n#                       width=1000, height=1000).generate(titles)\n\n# wordcloud_abstract = WordCloud(max_font_size=None, background_color='white', \n#                       collocations=False, stopwords=stopword,\n#                       width=1000, height=1000).generate(abstracts)\n","185aa889":"# plt.figure(figsize=(15,15))\n# plt.subplot(1,2,1)\n# plt.axis(\"off\")\n# plt.imshow(wordcloud_title)\n# plt.title('Common Words in Title')\n# plt.subplot(1,2,2)\n# plt.axis(\"off\")\n# plt.imshow(wordcloud_abstract)\n# plt.title('Common Words in Abstract')\n# plt.show()","caab4644":"# import nltk\n# import time\n\n# wnl = nltk.stem.WordNetLemmatizer()\n\n# word_count = {}\n\n# def Pos_tag(text, publish_time):\n#     token = nltk.word_tokenize(text)\n#     pos = nltk.pos_tag(token)\n#     try:\n#         timeStruct = time.strptime(publish_time, \"%Y-%m-%d\")\n#     except:\n#         timeStruct = time.strptime(publish_time, \"%Y\")\n#     return token, pos, timeStruct.tm_year\n\n# # 'NN*', 'VB*'\n# def add2vocab(pos_tag):\n#     for w, p in pos_tag:\n#         if (re.match('NN',p) or re.match('VB',p)) and w not in stopword and w.isalnum() and len(w) > 1:\n#             w = w.lower()\n#             if re.match('NN',p):\n#                 w = wnl.lemmatize(w, pos='n')  \n#             if re.match('VB',p):\n#                 w = wnl.lemmatize(w, pos='v')\n#             if w in word_count:\n#                 word_count[w] += 1\n#             else:\n#                 word_count[w] = 1  ","7fbf1419":"# df_subset = df_covid.loc[:, ['abstract','publish_time']].dropna()\n# all_pos = []\n# all_year = []\n# for idx, (abstract, publish_time) in df_subset.iterrows():\n#     token, pos, year = Pos_tag(abstract, publish_time)\n#     add2vocab(pos)\n#     all_pos.append(pos)\n#     all_year.append(year)","9aa00521":"# # vocab = [k for k,v in word_count if v > 5]\n# word_count_sort = sorted(word_count.items(), key=lambda d: d[1], reverse=True)\n# vocab = [k for k,v in word_count_sort[:100]]\n# count = [v for k,v in word_count_sort[:100]]\n# print(vocab)","eb164457":"# plt.figure(figsize=(10,10))\n# plt.barh(range(len(vocab[:50])), count[:50], height=0.3, color='steelblue', alpha=0.8)      # \u4ece\u4e0b\u5f80\u4e0a\u753b\n# plt.yticks(range(len(vocab[:50])), vocab)\n# # plt.xlim(30,47)\n# plt.xlabel(\"frequency\")\n# plt.title(\"Most Frequent words\")\n# # for x, y in enumerate(count):\n# #     plt.text(y + 0.2, x - 0.1, '%s' % y)\n# plt.show()","389a6f06":"# max_year = max(all_year)\n# min_year = min(all_year)\n# print('articles are from %d year to %d year.' % (min_year, max_year))\n# publish_count = np.zeros(max_year-min_year+1)\n# for y in all_year:\n#     publish_count[y-min_year] += 1\n# year_list = list(range(min_year, max_year+1))","cb9995e0":"# # choose_vocab = ['virus', 'infection', 'cell', 'protein', 'disease', 'patient', 'gene', 'respiratory',\n# #                 'rna', 'vaccine', 'sample', 'strain', 'expression', 'level', 'antibody', 'pathogen', 'assay', \n# #                 'detect', 'factor', 'mouse', 'associate', 'treatment', 'coronavirus', 'influenza', 'target', \n# #                 'replication', 'development', 'demonstrate', 'risk', 'outbreak', 'mechanism', 'detection', \n# #                 'review', 'indicate', 'child', 'function', 'population', 'structure', 'transmission', 'region', 'research', 'sars', 'conclusion', 'change', 'induce', 'syndrome', 'genome', 'infect', 'process', 'determine', 'interaction', 'age', 'approach', 'receptor', 'animal', 'specie', 'evaluate', 'acid', 'drug', 'observe', 'dna', 'reveal', 'investigate']\n# word2ix = {word:ix for ix, word in enumerate(vocab)}\n# matrix = np.zeros((max_year-min_year+1) * len(vocab)).reshape(max_year-min_year+1, len(vocab))\n# for pos, year in zip(all_pos, all_year):\n#     for w,p in pos:\n#         if re.match('NN',p):\n#             w = wnl.lemmatize(w, pos='n')\n#             if w in vocab:\n#                 matrix[year-min_year][word2ix[w]] += 1\n#         elif re.match('VB',p):\n#             w = wnl.lemmatize(w, pos='v')\n#             if w in vocab:\n#                 matrix[year-min_year][word2ix[w]] += 1","28f85fcd":"# # sub_axix = filter(lambda x:x%200 == 0, x_axix)\n# small_matrix = matrix[:-1,:20].copy()  # 1957-2019, top20 words\n# plt.figure(figsize=(15,10))\n# plt.title('Words Trend')\n# size1, size2 = small_matrix.shape\n# year_num = year_list[:-1]\n# colors = ['g', 'r', 'b', 'k', 'y', 'c', 'm']\n# for idx in range(size2):\n#     plt.plot(year_num, list(small_matrix[:, idx]), color=colors[idx%7], label=vocab[idx])\n# plt.plot(year_num, publish_count[:-1], 'r*', label='publications')\n# plt.legend() # \u663e\u793a\u56fe\u4f8b\n\n# plt.xlabel('year')\n# plt.ylabel('word frequency')\n# plt.show()","454d0c96":"# small_matrix = None","70e00fac":"# small_matrix = matrix[-20:-1,:20].copy()  # 2000-2019, top20 words\n# small_count = publish_count[-20:-1]\n# for idx in range(small_matrix.shape[0]):\n#     small_matrix[idx,:] = small_matrix[idx,:]\/small_count[idx]\n# plt.figure(figsize=(15,10))\n# plt.title('Words Trend')\n# size1, size2 = small_matrix.shape\n# year_num = year_list[-20:-1]\n# print(year_num)\n# colors = ['g', 'r', 'b', 'k', 'y', 'c', 'm']\n# for idx in range(size2):\n#     plt.plot(year_num, list(small_matrix[:, idx]), color=colors[idx%7], label=vocab[idx])\n# plt.legend() \n\n# plt.xlabel('year')\n# plt.ylabel('word frequency')\n# plt.show()","c8bddf85":"# matrix = None\n# small_matrix = None","15a143c0":"# cluster_covid = df_covid.loc[:, [\"paper_id\", \"title\", \"abstract\"]].dropna()\n# cluster_covid = cluster_covid.drop_duplicates(['abstract','paper_id', 'title'])","b08fcad3":"# from sklearn.feature_extraction.text import CountVectorizer \n# from sklearn.feature_extraction.text import TfidfTransformer \n\n# vectorizer = CountVectorizer()\n# transformer = TfidfTransformer()  # \u8fd9\u91cc\u9700\u8981\u9650\u5236\u7ef4\u5ea6\uff0c\u5360\u7528\u5185\u5b58\u592a\u5927\u4e86\n\n# X = vectorizer.fit_transform(list(cluster_covid['abstract']))\n# tfidf = transformer.fit_transform(X).toarray()\n# word = vectorizer.get_feature_names()\n# print(tfidf.shape)","798df2a0":"# from sklearn.decomposition import PCA\n# from sklearn.cluster import KMeans\n# pca_sk = PCA(n_components=3)\n\n# pca_result = pca_sk.fit_transform(tfidf)\n\n# y_preds = KMeans(n_clusters=6,random_state=0).fit(pca_result)\n\n# labels = y_preds.labels_\n# centers = y_preds.cluster_centers_","607b9000":"# import seaborn as sns\n# plt.figure(figsize=(10,10))\n\n# # sns settings\n# sns.set(rc={'figure.figsize':(15,15)})\n\n# # colors\n# palette = sns.color_palette(\"bright\", len(set(y)))\n\n# # plot\n# sns.scatterplot(pca_result[:,0], pca_result[:,1], hue=y, legend='full', palette=palette)\n# plt.title(\"PCA Covid-19 Articles - Clustered (K-Means) - Tf-idf with Plain Text\")\n# # plt.savefig(\"plots\/pca_covid19_label_TFID.png\")\n# plt.show()","7097f4de":"## 3. remove duplicate abstract and body_text","c08ae953":"## 3. Use Tf-idf to cluster","ebcbd52d":"Data preprocessing following the notebook by Ivan Ega Pratama and MaksimEkin, from Kaggle.\n\nCite: [Dataset Parsing Code | Kaggle, COVID EDA: Initial Exploration Tool](https:\/\/www.kaggle.com\/ivanegapratama\/covid-eda-initial-exploration-tool)\n\nCite: [COVID-19 Literature Clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering#Take-a-Look-at-the-Data:)","b1efc63a":"As can be seen from the figure, there are some unrelated words. In the following work, I will try to filter out more relevant words. ","c04128e5":"## 2. analysis the trend","4d244ffb":"# Data Preprocessing","aa6e877a":"## 2. Add the Word Count Columns","4cdec056":"## 1. Load Data \nload metadata and all *.json","5b232e2f":"First, we look the word frequency trend.","9246e476":"# Analyze the Data","8509619d":"Now let's look the rate trend.","772605eb":"## 1. see the title and abstract's words cloud"}}