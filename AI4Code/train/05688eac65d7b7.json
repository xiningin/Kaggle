{"cell_type":{"5ee0a4ed":"code","78525bb7":"code","20279e4d":"code","5fabd3be":"code","1b11d140":"code","15b3ce85":"code","29135ff0":"code","1dabee09":"code","fc75230e":"code","b967cd50":"code","85033fe9":"code","9f86aca3":"code","322e6b83":"code","4eca5112":"code","4eb327fc":"code","42127619":"code","d56a5ee6":"code","8cf24abc":"code","fecadbde":"code","845948a3":"code","9f5ac8c1":"code","9e4854c6":"code","cce98d61":"code","e7551d0d":"code","d2e33665":"code","76cc1fde":"code","1335a5a8":"code","2836856c":"code","bfd079ba":"code","38615edb":"code","4f828188":"code","04af5128":"markdown","648d9e92":"markdown","2db054e7":"markdown","14fd4639":"markdown","0ba54cd6":"markdown","60cadc0e":"markdown","8d949bd2":"markdown","76afb255":"markdown","4f8e0444":"markdown","29453400":"markdown"},"source":{"5ee0a4ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","78525bb7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","20279e4d":"# Loading the dataset\ndataset = pd.read_csv('\/kaggle\/input\/heart-attack-analysis-prediction-dataset\/heart.csv')","5fabd3be":"data = dataset.copy()","1b11d140":"data.head()","15b3ce85":"data.shape","29135ff0":"data.info()","1dabee09":"data.describe()","fc75230e":"# check weather null values are there\ndata.isnull().sum()","b967cd50":"# Seperating Numerical, Discreate ,continous and categorical variables\n\ndata_numerical = [feature for feature in data.columns if data[feature].dtype != 'O']\ndata[data_numerical].head()\n","85033fe9":"# Discrete Variables\ndata_discreate = [feature for feature in data_numerical if len(data[feature].unique())<25]\ndata[data_discreate].head()","9f86aca3":"feature_discrete = [data[feature].value_counts() for feature in data_discreate]\nfeature_discrete","322e6b83":"# Now Visualize the relationship between each feature_discrete and output\nfor feature in data_discreate[:-1]:\n\n    sns.countplot(data=data,x=data[feature],hue='output')\n    plt.xlabel(feature)\n    plt.title(feature)\n    plt.show()","4eca5112":"# Categorical variables\ndata_categorical = [feature for feature in data.columns if data[feature].dtype == 'O']\ndata_categorical\n# we dont have any categorical variables in dataset","4eb327fc":"# Continous Variables\ndata_continous = [feature for feature in data_numerical if feature not in data_discreate]\ndata[data_continous].head()","42127619":"sns.displot(data[data['output']==1].age, kind='kde')\nplt.ylabel([data['output'].value_counts()])\nplt.show()","d56a5ee6":"sns.displot(data[data['output']==0].cp, kind='kde')\nplt.ylabel([data['output'].value_counts()])\nplt.show()","8cf24abc":"sns.displot(data[data['output']==0].fbs, kind='kde')\nplt.ylabel([data['output'].value_counts()])\nplt.show()","fecadbde":"sns.displot(data[data['output']==0].restecg, kind='kde')\nplt.ylabel([data['output'].value_counts()])\nplt.show()","845948a3":"for feature  in data:\n    sns.histplot(data=data,x= data[feature],kde=True,label=feature,color='green')\n    sns.color_palette(\"pastel\")\n    plt.xlabel(feature)\n    plt.legend()\n    plt.show()","9f5ac8c1":"for feature in data:\n    sns.boxplot(data[feature])\n    plt.show()","9e4854c6":"# Spliting the data into independent and dependent features\n\nx=data.drop('output',axis=1)\ny=data['output']","cce98d61":"from sklearn.preprocessing import MinMaxScaler\nscalerX = MinMaxScaler(feature_range=(0, 1))\nx[x.columns] = scalerX.fit_transform(x[x.columns])","e7551d0d":"# All the values are converted between 0 to 1\nx.head()\n","d2e33665":"# seperate the data into train and test\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.3,random_state=0)\nx_train.shape,x_test.shape","76cc1fde":"# lets see the correlation of features\nplt.figure(figsize=(10,8))\nsns.heatmap(x_train.corr(),data=x,annot=True)\nplt.show()","1335a5a8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#ensemble Techniques\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom xgboost import XGBClassifier\n\n# finding the performance of the models\nfrom sklearn.metrics import accuracy_score,confusion_matrix\n\n","2836856c":"key = ['LogisticRegression','SVC','DecisionTreeClassifier','KNeighborsClassifier','RandomForestClassifier','GradientBoostingClassifier','AdaBoostClassifier','XGBClassifier']\nvalue =[LogisticRegression(random_state=9),SVC(),DecisionTreeClassifier(),KNeighborsClassifier(),RandomForestClassifier(),GradientBoostingClassifier(),AdaBoostClassifier(),XGBClassifier()]\nmodels = dict(zip(key,value))","bfd079ba":"predicted = []\nfor name , algorithm in models.items():\n    model=algorithm\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    accu = accuracy_score(y_test,y_pred)\n    predicted.append(accu)\n    print(name,\"-\"*10,accu)\n    ","38615edb":"#confusion matrix\nfor name , algorithm in models.items():\n    model=algorithm\n    model.fit(x_train,y_train)\n    y_pred = model.predict(x_test)\n    cf = confusion_matrix(y_test,y_pred)\n    plt.figure(figsize=(7,6))\n    print(name)\n    sns.heatmap(cf,annot=True,fmt='d')\n    plt.show()\n","4f828188":"plt.figure(figsize = (10,5))\nsns.barplot(x = predicted, y = key, palette='viridis')","04af5128":"# About this dataset\u00b6\nAge : Age of the patient\n\nSex : Sex of the patient\n\nexang: exercise induced angina (1 = yes; 0 = no)\n\nca: number of major vessels (0-3)\n\ncp : Chest Pain type chest pain type\n\nValue 1: typical angina Value 2: atypical angina Value 3: non-anginal pain Value 4: asymptomatic trtbps : resting blood pressure (in mm Hg)\n\nchol : cholestoral in mg\/dl fetched via BMI sensor\n\nfbs : (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false)\n\nrest_ecg : resting electrocardiographic results\n\nValue 0: normal Value 1: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV) Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria thalach : maximum heart rate achieved\n\ntarget : 0= less chance of heart attack 1= more chance of heart attack","648d9e92":"## Conclusion \nFrom the above figure we can see that KNeighborsClassifier model give an accuracy greater than 83%.","2db054e7":"####  Let's find the Outliers","14fd4639":"Note : From the above graph we can observe that the age between 50 to 65 has high chances of getting heart attack","0ba54cd6":"### Scaling the data using MinMax Scaler\n  All the data values present in dataset will convert to between 0 to 1","60cadc0e":"# # Observation\n1. As per the sex we can observe that compare to females the males are mostly affecting by Heart attack.\n2. Those who are having non-anginal Chest pain are mostly affecting by heart attack compare to atypical angina & typical angina and those who are suffering with asymptomatic are having less chances of heart attack\n3. fbs People belong to category 0 has more chance of heart attack where 1 has less chance of heart     attack\n4. People having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV)\n   are highy affecting by heart attack followed by normal people\n\n","8d949bd2":"###  Let's create the models","76afb255":"### Let's see Data is normally distributed or not","4f8e0444":"NOTE : We find that some colums present outliers, but we are not removing the outliers since it is medical data it may consist important data","29453400":"# HEART ATTACK ANALYSIS AND PREDICTION\n![](https:\/\/source.wustl.edu\/wp-content\/uploads\/2019\/02\/HeartImage-760x594.jpg)"}}