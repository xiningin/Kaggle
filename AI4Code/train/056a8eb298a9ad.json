{"cell_type":{"cab78ac9":"code","6dc7cc94":"code","f83a2cfe":"code","a3be5b78":"code","44bbdcae":"code","4e178b4e":"code","9de22972":"code","d05bc54a":"code","5e71ca1d":"code","4e5375f6":"code","6999724f":"code","767b1cfa":"code","b276aa93":"code","e15b5d8d":"code","2cfe0627":"code","4d718e5e":"code","9b3fab0a":"code","c2fa7d70":"code","690485b4":"code","5ed71076":"code","78c0cf62":"code","bd1b2a5e":"code","ba6ea9c1":"code","a540ff15":"code","928c6d0f":"code","864eaae3":"markdown","a22fbc44":"markdown","c8f9f1ce":"markdown","02cfcbad":"markdown","2ddc8838":"markdown","278bad21":"markdown","97c66cc4":"markdown","56a7443b":"markdown","53d87cbd":"markdown","a0b566e0":"markdown","cb70becc":"markdown","d00e8a0c":"markdown","4a2bc4f6":"markdown","adde41ea":"markdown","c03d1c32":"markdown","d309cf2d":"markdown","8ba16af1":"markdown","ea93c1fc":"markdown","a9ead6c6":"markdown","b3bd4596":"markdown","80ee98e9":"markdown","c7b5b51a":"markdown","16b983f8":"markdown","f688764a":"markdown","c04fc030":"markdown","ae8b71d8":"markdown","e95c3829":"markdown","36325862":"markdown","6da8c7f6":"markdown"},"source":{"cab78ac9":"import os               \nimport numpy                   as np\nimport pandas                  as pd \nimport matplotlib.pyplot       as plt\nimport seaborn                 as sns\nimport plotly.express          as ex\nimport plotly.graph_objs       as go\nimport plotly.offline          as pyo\nimport scipy.stats             as stats\nimport pymc3                   as pm\nimport theano.tensor           as tt\nfrom plotly.subplots           import make_subplots\nfrom sklearn.preprocessing     import StandardScaler\nfrom sklearn.decomposition     import TruncatedSVD,PCA\nfrom sklearn.ensemble          import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.tree              import DecisionTreeClassifier\nfrom sklearn.linear_model      import LinearRegression,LogisticRegressionCV\nfrom sklearn.svm               import SVC\nfrom sklearn.metrics           import mean_squared_error,r2_score\nfrom sklearn.pipeline          import Pipeline\nfrom sklearn.model_selection   import cross_val_score,train_test_split,GridSearchCV,RandomizedSearchCV\nfrom sklearn.manifold          import Isomap,TSNE\nfrom sklearn.feature_selection import mutual_info_classif\nfrom tqdm.notebook             import tqdm\nfrom scipy.stats               import ttest_ind\n\n#%pip install tune_sklearn\n#from tune_sklearn              import TuneGridSearchCV\n\n\nsns.set_style('darkgrid')\npyo.init_notebook_mode()\n%matplotlib inline\n\n\nplt.rc('figure',figsize=(18,11))\nsns.set_context('paper',font_scale=2)","6dc7cc94":"water_df = pd.read_csv('\/kaggle\/input\/water-potability\/water_potability.csv')\nwater_df.head(4)","f83a2cfe":"plt.title('Missing Values Per Feature')\nnans = water_df.isna().sum().sort_values(ascending=False).to_frame()\nsns.heatmap(nans,annot=True,fmt='d',cmap='vlag')","a3be5b78":"# Impute Missing Values with Label Matching Mean\nfor col in ['Sulfate','ph','Trihalomethanes']:\n    missing_label_0 = water_df.query('Potability == 0')[col][water_df[col].isna()].index\n    water_df.loc[missing_label_0,col] = water_df.query('Potability == 0')[col][water_df[col].notna()].mean()\n\n    missing_label_1 = water_df.query('Potability == 1')[col][water_df[col].isna()].index\n    water_df.loc[missing_label_1,col] = water_df.query('Potability == 1')[col][water_df[col].notna()].mean()\n","44bbdcae":"T = water_df.copy()\nT.Potability =  T.Potability.map({1:'Potable',0:'Not Potable'})\nex.pie(T,names='Potability',title='Distribution of Target Labels (Drinkability)')","4e178b4e":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True,subplot_titles=('Perason Correaltion',  'Spearman Correaltion','Kendall Correlation'))\ncolorscale=     [[1.0              , \"rgb(165,0,38)\"],\n                [0.8888888888888888, \"rgb(215,48,39)\"],\n                [0.7777777777777778, \"rgb(244,109,67)\"],\n                [0.6666666666666666, \"rgb(253,174,97)\"],\n                [0.5555555555555556, \"rgb(254,224,144)\"],\n                [0.4444444444444444, \"rgb(224,243,248)\"],\n                [0.3333333333333333, \"rgb(171,217,233)\"],\n                [0.2222222222222222, \"rgb(116,173,209)\"],\n                [0.1111111111111111, \"rgb(69,117,180)\"],\n                [0.0               , \"rgb(49,54,149)\"]]\n\ns_val =water_df.corr('pearson')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,name='pearson',showscale=False,xgap=1,ygap=1,colorscale=colorscale),\n    row=1, col=1\n)\n\n\ns_val =water_df.corr('spearman')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale),\n    row=2, col=1\n)\n\ns_val =water_df.corr('kendall')\ns_idx = s_val.index\ns_col = s_val.columns\ns_val = s_val.values\nfig.add_trace(\n    go.Heatmap(x=s_col,y=s_idx,z=s_val,xgap=1,ygap=1,colorscale=colorscale,showscale=False),\n    row=3, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Inner Correlations Coefficients\")\nfig.show()","9de22972":"non_potabale = water_df.query('Potability == 0')\npotabale     = water_df.query('Potability == 1')\n\nfor ax,col in enumerate(water_df.columns[:9]):\n    plt.subplot(3,3,ax+1)\n    plt.title(f'Distribution of {col}')\n    sns.kdeplot(x=non_potabale[col],label='Non Potabale')\n    sns.kdeplot(x=potabale[col],label='Potabale')\n    plt.legend(prop=dict(size=10))\n    \n\nplt.tight_layout()","d05bc54a":"ttest_results_pvalues,ttest_results_statistic = [],[]\nfor ax,col in enumerate(water_df.columns[:9]):\n    statistic,pvalue = ttest_ind(non_potabale[col],potabale[col])\n    ttest_results_pvalues.append(pvalue)\n    ttest_results_statistic.append(statistic)\n    \nttest_res_df = pd.DataFrame({'S':ttest_results_statistic,'P':ttest_results_pvalues,'F':water_df.columns[:9]})\nttest_res_df = ttest_res_df.sort_values(by='P')","5e71ca1d":"tr  = go.Bar(x=ttest_res_df['F'] ,y=ttest_res_df['P'] ,name='T-test P Value')\ntr2 = go.Bar(x=ttest_res_df['F'] ,y=ttest_res_df['S'] ,name='T-test F Statistic')\n\ndata = [tr2,tr]\nfig = go.Figure(data=data,layout={'title':'T-test Results For Each Feature in Our Dataset','barmode':'overlay'})\nfig.show()\n","4e5375f6":"mutual_info = []\nfor i in range(0,9):\n    mi = mutual_info_classif(X=water_df.iloc[:,i].to_numpy().reshape(-1, 1),y=water_df.iloc[:,-1],random_state=42)\n    mutual_info.append(mi[0])\nmutual_info = pd.DataFrame({'Feature':water_df.columns[:9],'MI':mutual_info})\nmutual_info = mutual_info.sort_values(by='MI')\ntr  = go.Bar(x=mutual_info['Feature'] ,y=mutual_info['MI'] ,name='Mutual Information')\n\ndata = [tr]\nfig = go.Figure(data=data,layout={'title':'Mutual Information Between Our Features and Potability','barmode':'overlay','yaxis_title':'Mutal Information'})\nfig.show()\n","6999724f":"plt.title('Potability as A Function of Turbidity')\nsns.scatterplot(x=water_df.iloc[:,8],y=water_df.iloc[:,-1])\nplt.show()","767b1cfa":"with pm.Model() as model:\n    beta = pm.Normal(\"beta\", mu=0, tau=0.001, testval=0)\n    alpha = pm.Normal(\"alpha\", mu=0, tau=1\/water_df.Turbidity.std(), testval=0)\n    p = pm.Deterministic(\"p_parm\", 1.0\/(1. + tt.exp(beta*water_df.Turbidity + alpha)))","b276aa93":"with model:\n    observed = pm.Bernoulli(\"obs\", p, observed=water_df.Potability)\n    start = pm.find_MAP()\n    step = pm.Metropolis()\n    trace = pm.sample(32000, step=step, start=start)\n    burned_trace = trace[20000::2]","e15b5d8d":"alpha_samples = burned_trace[\"alpha\"][:, None]\nbeta_samples = burned_trace[\"beta\"][:, None]\nplt.subplot(211)\nplt.title(r\"Posterior distributions of the variables $\\alpha, \\beta$\")\nsns.histplot(beta_samples, bins=35, alpha=0.85,label=r\"posterior of $\\beta$\", palette=[\"#7A68A6\"],stat='probability')\nplt.legend()\n\nplt.subplot(212)\nsns.histplot(alpha_samples, bins=35, alpha=0.85,label=r\"posterior of $\\alpha$\", palette=[\"#A60628\"],stat='probability')\nplt.legend();","2cfe0627":"t = np.linspace(water_df.Turbidity.min() - 2, water_df.Turbidity.max()+2, 150)[:, None]\ndef logistic(x, beta, alpha=0):\n    return 1.0 \/ (1.0 + np.exp(np.dot(beta, x) + alpha))\n\np_t = logistic(t.T, beta_samples, alpha_samples)\n\nmean_prob_t = p_t.mean(axis=0)\n\nplt.plot(t, mean_prob_t, lw=3, label=\"average posterior \\nprobability \\ of potability\")\nplt.plot(t, p_t[0, :], ls=\"--\", label=\"realization from posterior\")\nplt.plot(t, p_t[-2, :], ls=\"--\", label=\"realization from posterior\")\nplt.scatter(water_df.Turbidity, water_df.Potability, color=\"tab:red\", s=50, alpha=0.5)\nplt.title(\"Posterior expected value of probability of a water sample being Potable; \\\nplus realizations\")\nplt.legend()\nplt.ylim(-0.1, 1.1)\nplt.xlim(t.min(), t.max())\nplt.ylabel(\"probability\")\nplt.xlabel(\"turbidity\");","4d718e5e":"N = 5 \npca_pipeline = Pipeline(steps = [\n    ('scale',StandardScaler()),\n    ('PCA',PCA(N))\n])\n\ntf_data = pca_pipeline.fit_transform(water_df.iloc[:,:9])\ntf_data = pd.DataFrame({'PC1':tf_data[:,0],'PC2':tf_data[:,1],'PC3':tf_data[:,2],'PC4':tf_data[:,3],'PC5':tf_data[:,4],\n                        'label':water_df.iloc[:,-1].map({0:'Not Potabale',1:'Potable'})})\n\n","9b3fab0a":"ex.scatter_3d(tf_data,x='PC1',y='PC2',z='PC3',color='label',color_discrete_sequence=['salmon','green'],title=r'$\\textit{Data in Reduced Dimension } R^9 \\rightarrow R^3$')","c2fa7d70":"components = tf_data[['PC1','PC2','PC3','PC4','PC5']].to_numpy()\n\nlabels = {\n    str(i): f\"PC {i+1} ({var:.1f}%)\"\n    for i, var in enumerate(pca_pipeline['PCA'].explained_variance_ratio_ * 100)\n}\n\nfig = ex.scatter_matrix(\n    components,\n    labels=labels,\n    dimensions=range(N),\n    color=tf_data['label'],\n    color_discrete_sequence=['salmon','green']\n)\nfig.update_traces(diagonal_visible=False)\nfig.update_layout(title='Data Spread Based on Different 2D Combinations of Principal Components')\n\nfig.show()","690485b4":"\nevr = pca_pipeline['PCA'].explained_variance_ratio_\ntotal_var = evr.sum() * 100\ncumsum_evr = np.cumsum(evr)\n\ntrace1 = {\n    \"name\": \"individual explained variance\", \n    \"type\": \"bar\",\n    'y':evr}\ntrace2 = {\n    \"name\": \"cumulative explained variance\", \n    \"type\": \"scatter\", \n     'y':cumsum_evr}\ndata = [trace1, trace2]\nlayout = {\n    \"xaxis\": {\"title\": \"Principal components\"}, \n    \"yaxis\": {\"title\": \"Explained variance ratio\"},\n  }\nfig = go.Figure(data=data, layout=layout)\nfig.update_layout(title='{:.2f}% of the Original Feature Variance Can Be Explained Using {} Dimensions'.format(np.sum(evr)*100,N))\nfig.show()","5ed71076":"features = ttest_res_df.iloc[:3,:].F.to_list()\nfeatures.append('Turbidity')\ntrain_x,test_x,train_y,test_y = train_test_split(water_df[features],water_df.iloc[:,-1],random_state=42,shuffle=True)\n","78c0cf62":"RandomForest_Pipeline     = Pipeline(steps = [('scale',StandardScaler()),('RF',RandomForestClassifier(random_state=42))])\nAdaBoost_Pipeline         = Pipeline(steps = [('scale',StandardScaler()),('AB',AdaBoostClassifier(random_state=42))])\nSVC_Pipeline              = Pipeline(steps = [('scale',StandardScaler()),('SVM',SVC(random_state=42))])\n\nRandomForest_CV_f1     = cross_val_score(RandomForest_Pipeline,water_df[features],water_df.iloc[:,-1],cv=10,scoring='f1')\nAdaBoost_CV_f1         = cross_val_score(AdaBoost_Pipeline,water_df[features],water_df.iloc[:,-1],cv=10,scoring='f1')\nSVC_CV_f1              = cross_val_score(SVC_Pipeline,water_df[features],water_df.iloc[:,-1],cv=10,scoring='f1')\n","bd1b2a5e":"fig = make_subplots(rows=3, cols=1,shared_xaxes=True,subplot_titles=('Random Forest Cross Val Scores',\n                                                                     'AdaBoost Cross Val Scores',\n                                                                     'SVM Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(SVC_CV_f1)),y=RandomForest_CV_f1,name='Random Forest'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(SVC_CV_f1)),y=AdaBoost_CV_f1,name='AdaBoost'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=np.arange(0,len(SVC_CV_f1)),y=SVC_CV_f1,name='SVM'),\n    row=3, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Baseline Models 10 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"RMSE\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","ba6ea9c1":"\n\n# RFBE = RandomForestClassifier(random_state=42)\n\n# AdaBoost_Pipeline         = Pipeline(steps = [('scale',StandardScaler()),('AB',AdaBoostClassifier(random_state = 42,\n#                                                                                                  base_estimator = RFBE))])\n\n# AdaBoost_Pipeline.fit(train_x,train_y)\n\n# parameters = {'AB__base_estimator__max_depth':[2,3,5],\n#               'AB__base_estimator__min_samples_leaf':[2,5,10],\n#               'AB__base_estimator__criterion':['entropy','gini'],\n#               'AB__base_estimator__bootstrap':[True,False],\n#               'AB__n_estimators':[5,10,25],\n#               'AB__learning_rate':[0.01,0.1]}\n\n# #ADA_RF_GS  = TuneGridSearchCV(AdaBoost_Pipeline,parameters,cv=3,verbose=1)\n# ADA_RF_GS  = GridSearchCV(AdaBoost_Pipeline,parameters,cv=3,verbose=10)\n# ADA_RF_GS.fit(water_df[features],water_df.iloc[:,-1])\n\n# print(\"Best parameter (CV score=%0.3f):\" % ADA_RF_GS.best_score_)\n# print(ADA_RF_GS.best_params_)","a540ff15":"{'AB__base_estimator__bootstrap': True, 'AB__base_estimator__criterion': 'gini', 'AB__base_estimator__max_depth': 5, 'AB__base_estimator__min_samples_leaf': 10, 'AB__learning_rate': 0.01, 'AB__n_estimators': 5}","928c6d0f":"RFBE = RandomForestClassifier(random_state=42,bootstrap=True,criterion='gini',max_depth=5,min_samples_leaf=10)\nAdaBoost_Pipeline         = Pipeline(steps = [('scale',StandardScaler()),('AB',AdaBoostClassifier(random_state = 42,\n                                                                                                 base_estimator = RFBE,\n                                                                                                 learning_rate=0.01,\n                                                                                                 n_estimators=5))])\n\nAdaBoost_Pipeline.fit(train_x,train_y)\nf1 = AdaBoost_Pipeline.score(test_x,test_y)\nprint('F1 - Score of AdaBoost Model with Random Forest Base Estimators and Cross Validation Grid Search -[',np.round(f1,2),']')","864eaae3":"**Explanation**: In order to test for any significant difference between \"potable\" and \"non-potable\" water samples, we will treat both labels as two separate populations from which we sampled 'n'  and 'k' samples (n = the number of \"potable\" samples, 'k' = the number of \"non-potable\" samples).\nWe will perform a two-tailed t-test to check if there is any significant difference between the two sample means, considering the sample size differences and unequal variance.\nWe expect to see low p-values for the features that indeed are significantly different between the labels.\nWe will set our significance level alpha to be equal to or less than 0.1.","a22fbc44":"**Action**: We will impute the outliers in our data using the corresponding mean to label, i.e., all missing values that are labeled \"potable\" will be imputed using the mean of all non-missing \"potable\" samples, and the same action will be applied to \"non-potable\" samples with missing values.","c8f9f1ce":"**Explanation**: Our baseline model will be a Random Forest model as it can provide us with a separation of our domain that is nor linear and not polynomial; we will also see if AdaBoost using a decision tree that is conceptually similar to the Random Forest model will be able to provide us with some interesting results.\nThe third model test in this baseline section is a classifier based on SVM, and that is to confirm the hypothesis stated earlier that our data is not separable in higher dimensions.","02cfcbad":"**Observation**: After using Principal Components Analysis to reduce the dimensionality of our data from R9 to R3, we see no visible linear\/polynomial separation between the labels, a key point that decreases our belief in models that exist rely heavily on spatial separation like SVM.\n\n","2ddc8838":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Introduction<\/h3>\n\n![](http:\/\/cdn.shopify.com\/s\/files\/1\/0079\/0082\/3665\/articles\/water_1200x1200.jpg?v=1579072340)\n\nFresh water is the primary source of human health, prosperity, and security. By around 2050 the world's population is expected to reach about nine billion. Assuming that standards of living continue to rise, the requirement of potable water for human consumption will amount to the resources of about three planet Earths. A key United Nations report indicates that water shortages will affect 2.3 billion people or 30% of the world's population in four dozen nations by 2025. Already, the crisis of potable water in most developing countries is creating public health emergencies of staggering proportions. In Bangladesh, for example, it is officially recognized by the government of Bangladesh that 50% of the country's approximately 150 million people, are at risk of arsenic poisoning from groundwater used for drinking. Recently, the government of Bangladesh, in its Action Plan for Poverty Reduction, stated its desire to ensure 100% access to pure drinking water across the region within the shortest possible time frame [3]. This is also consistent with key goals of the Millennium Development Goal \u201cEradication of extreme poverty and hunger\u201d and \u201cHalving by 2015, the proportion of people without sustainable access to safe drinking water\u201d. Whether this is achievable within the stated time is debatable, but it clearly delineates the state of the world we live in. - Abul Hussam, in Monitoring Water Quality, 2013\n\n\nThis notebook will explore the different features related to water potability, Modeling, and predicting water potability.\nWe will dive into an in-depth analysis of what separates potable water from non-potable using traditional statistics, bayesian inference, and other machine learning approaches that will help us uncover the underlying process.\n","278bad21":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">General  Data Analysis<\/h3>\n\n","97c66cc4":"It hard to tell whether *the probability* of a water sample being potabale increases as the turbidity increases. We are interested in modeling the probability here. The best we can do is ask, \"At turbidity Value $X$, what is the probability of a water sample being potable?\". The goal of the following experiment is that question.\n\nWe need a function of turbidity, call it $p(X)$, that is bounded between 0 and 1 and changes from 1 to 0 as we increase turbidity. Such a function is well defined and known to us all, the *logistic function.*\n\n$$p(X) = \\frac{1}{ 1 + e^{ \\;\\beta X } } $$\n\nIn this model, $\\beta$ is the variable we are uncertain about. Below are some examples for different value of beta plotted for $\\beta = -2, 52, 7$.","56a7443b":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Model Selection \/ Baseline Model Evaluation<\/h3>\n\n","53d87cbd":"**Observation**: As expected, we see some fair results both in our baseline Random Forest model and our baseline AdaBoost model.\nConsidering these results, we will try to create a more accurate and optimized model by using AdaBoost to envoke Random Forest models as its base estimators.\n\nNotice that the SVM classifier indeed does an awful job confirming our hypothesis.\n","a0b566e0":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Exploratory Data Analysis<\/h3>\n\n","cb70becc":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Loading the Data and Imputing Missing Values<\/h3>\n","d00e8a0c":"**Observation**: It appears that there is no linear\/ranked correlation between our output label and our features, mostly due to the fact that we have a binary label and continuous features, traditional linear correlation coefficients won't tell us the true underlying story about the relationships between our features and the target variable.\nLater in this notebook, we will perform more in depth analysis to try and uncover some of the relationships hidden in our data.","4a2bc4f6":"We see that after exploring and modeling the potability as a process of turbidity, the underlying posterior distribution of a logistic model that should have found a threshold of classification if it was possible to gain confidence about potability of water based on turbidity, unfortunately, this is not the case.","adde41ea":"**Observation**: After performing the two-tailed t-test, we see that only \"Solids\" and \"Organic carbon\" have p-values below our pre-defined alpha value, even though there are two more features closer to our alpha level than the other 4.\nWhen we get to the modeling stage, the 4 features we will use will be all the features we see in the above plot with p-values below 0.18 (first 4 features in the plot)\n","c03d1c32":"\n<img src=\"https:\/\/i.ibb.co\/vh4bJK4\/Screenshot-2021-06-27-143926.jpg\" width=\"1200\" height=\"600\">","d309cf2d":"$$ \\text{Sample is Potabale, $M_i$} \\sim \\text{Ber}( \\;P(turbidity_i)\\; ), \\;\\; i=1..N$$\n\nwhere $p(turbidity)$ is our logistic function and $turbidity_i$ are the turbidity values in our dataset.","8ba16af1":"**Notice**:Due to the reasonably long search time consumed by Grid Search, this block is commented and provided for any Kaggler who wishes to play and test different models\/parameter values by himself.\nThe result of the grid search is given below.","ea93c1fc":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Probabilistic Inference<\/h3>\n\n","a9ead6c6":"**Observation**: As an additional metric for consideration, we use \"Mutal Information\" to test and see if there is any similarity between the probability distribution of or continuous features with the Bernoulli distribution that represent our target.\nWe see that some of the worst scoring features in our t-test have the highest mutual information with our target label, conceptually meaning that knowing something about \"Ph\" decreases my uncertainty in assuming about \"Potability,\" unfortunately, mutual information doesn't tell me exactly to what assumption does \"Ph\" contribute. Still, none the less it is an indicator of relationship and a strong what in the matter, so we will indeed include it as well in our modeling section.\n","b3bd4596":"**Observation**: We see that we have some degree of unbalancedness in our data; we will not apply any upsampling\/downsampling methodology as the proportions are more close to equal than to be extremely balanced (cases like 90% \/ 10% where upsampling is crucial).\nAlso, the more significant label (\"Not potable\") is the one with more samples; logically, we would prefer a model that will have more false negatives rather than a model that has more false positives.","80ee98e9":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Hyperparameter Tuning<\/h3>\n\n","c7b5b51a":"**Observation**: Looking at the distribution of all our features divided by our target label, we see that some of them have some difference, a key point that can help us select the features with which we will train our models.\nTo better understand the differences between the features with respect to the target label, a more robust analysis is required to confirm any hypothesis we may have at this point just from looking at the distribution plots.\n\n","16b983f8":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Domain Analysis via Dimensionality Recudtion<\/h3>\n\n","f688764a":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Final Model<\/h3>\n\n","c04fc030":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:150%;text-align:center;border-radius: 15px 50px;\">Statistical Difference Analysis<\/h3>\n\n","ae8b71d8":"**Explanation**: The feature we select for our modeling stage are all the features we tested during our EDA section and found evidence of some difference or relationship to the \/ with respect to the target label.","e95c3829":"Notice in the above code we had to set the values of `beta` and `alpha` to 0. The reason for this is that if `beta` and `alpha` are very large, they make `p` equal to 1 or 0. Unfortunately, `pm.Bernoulli` does not like probabilities of exactly 0 or 1, though they are mathematically well-defined probabilities. So by setting the coefficient values to `0`, we set the variable `p` to be a reasonable starting value.","36325862":"**Observation**: Using five components (out of initially 9), we can see that we can only preserve 60 percent of the original variance; we can learn from this fact that our features are indeed uncorrelated between them and there is no linear combination that can tell us a better story regarding the target label after looking at the different permutations of principal components.\n\n","6da8c7f6":"<h3 style=\"background-color:orange;font-family:newtimeroman;font-size:250%;text-align:center;border-radius: 15px 50px;\">Libraries and Utilities<\/h3>\n"}}