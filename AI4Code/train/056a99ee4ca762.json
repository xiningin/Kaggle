{"cell_type":{"7066eb58":"code","486f07e6":"code","b34a21c5":"code","3dc971a2":"code","2d878c53":"code","eb945954":"code","4d1ff1f8":"code","f5bc7e71":"code","d174e0b4":"code","63b0ae0f":"code","f4b064f5":"code","0ad230f1":"code","82b21ade":"code","e9c086db":"code","f4ffa7b0":"code","0a75ee49":"code","30dbc41f":"code","a464dd57":"code","f36da3b8":"code","fd89e0b7":"code","e8fdc70a":"code","7c27a190":"code","11e05bec":"code","5d5904c9":"code","40a23c15":"code","218e7249":"code","6348bf18":"code","8d8e71b8":"code","ac460fb1":"code","98981462":"code","32c74fa7":"code","6520536d":"code","d87c9c0f":"code","c50c353e":"code","ad767fbc":"code","fdcb46a3":"code","ee1a57d6":"code","dbd28905":"code","fae48005":"code","4362559d":"code","6acbb9a0":"code","0342f742":"code","aa2cbdda":"code","e0c364a1":"code","d8e407b6":"code","119fd6f2":"code","245230e4":"code","28d5cc39":"code","ee42ee26":"code","92ad28b1":"code","ea1a641f":"code","3e4ce282":"code","2903b39c":"code","f8e72260":"code","9b808d74":"code","782895c2":"code","7a5dd6ed":"code","2d559685":"code","3573fa81":"code","e433e228":"code","c422ee67":"code","7efa607a":"code","4d67e061":"code","68e943ed":"code","7e11af53":"code","bf5fef81":"code","da74553e":"code","97a17939":"code","5e3ca404":"code","2b45cf29":"code","2b20c553":"code","8ea3dd13":"code","f5692a45":"code","45a254ea":"code","cff908c7":"code","3469d87d":"code","1e1ef27b":"code","3507dd90":"code","f3448045":"code","e5ab2641":"code","c2065664":"code","2ee607a3":"code","d7818010":"code","14636f64":"code","446dacab":"code","1b9e1dec":"code","142754e6":"code","319bd9f3":"code","e04d5e21":"code","540d956c":"code","77ec2495":"code","99cc5195":"code","1af47c18":"code","24a44751":"code","5f825c79":"code","58db3b22":"code","208fae5b":"code","0ef84b88":"code","61e2a27e":"code","3e93e186":"code","8e8bacaf":"code","fb0e857e":"code","f00155e2":"code","5f460ad7":"code","d5ec61f6":"code","2f49db1e":"code","7a78c491":"code","36046b13":"code","1e08f51f":"code","79628552":"code","5381116c":"code","0cd967c3":"code","1217737f":"code","10e0f016":"code","5e1c1c64":"code","90ce6ed5":"code","29a621a8":"code","77a8b207":"code","b85b5e12":"code","2d893692":"code","df4962d7":"code","42be2b8c":"code","7f081430":"code","8fefb988":"code","8eb989f6":"code","3af0e2ae":"code","6641f526":"code","19d3cecc":"code","ec5ccf0a":"code","7ba17752":"code","978ebd49":"code","e9d5a197":"code","f8e16997":"code","d4f3a954":"code","01d2dda3":"code","c61dcc7c":"code","be69eaba":"code","3d48b907":"code","d8d9c94c":"code","f51e8715":"code","db232f3d":"code","33f16691":"code","342b6865":"code","2751cad2":"markdown","688ad976":"markdown","98474de8":"markdown","0aded10c":"markdown","ea0ef106":"markdown","9e9b941d":"markdown","587bbda9":"markdown","1a8ce3bb":"markdown","a0b01860":"markdown","b28008cd":"markdown","664129f8":"markdown","c69425ef":"markdown","f150f943":"markdown","abf0ba99":"markdown","96eac250":"markdown","6f3a49ce":"markdown","4c754655":"markdown","4804084e":"markdown","22ea127f":"markdown","c3191dec":"markdown","a31b3654":"markdown","1a8b1030":"markdown","d172840f":"markdown","7e4a06c2":"markdown","a14b395d":"markdown","f485e661":"markdown","60589140":"markdown","444bb755":"markdown","31c61410":"markdown","c7708587":"markdown","9a28d0d3":"markdown","0913b2a0":"markdown","81651e53":"markdown","350b9953":"markdown","3e9ca588":"markdown","23fc1100":"markdown","594fda4b":"markdown","d6a4e1de":"markdown","20ed9212":"markdown","84533bab":"markdown","169b608d":"markdown","dd361b94":"markdown","7d480038":"markdown","de1708b1":"markdown","76137fc1":"markdown","8b76603b":"markdown","6a6ea3d8":"markdown","89656544":"markdown","2334e7f5":"markdown","1fa1d136":"markdown","50bb8798":"markdown","54267da9":"markdown","df8b7221":"markdown","b6fcd29f":"markdown","f16e2c8e":"markdown","e55d5010":"markdown","58f173fa":"markdown","b8cf900a":"markdown","20a4624a":"markdown","b92405d2":"markdown","4f12f50b":"markdown","f8d39da3":"markdown","2bcff083":"markdown","b0f90c08":"markdown","7940731f":"markdown","df8d09b7":"markdown","192404de":"markdown","bc3e55d1":"markdown","894564ec":"markdown","e57446e8":"markdown","c4600a53":"markdown","207ea4c7":"markdown","397c8063":"markdown","3d59276c":"markdown","433736c7":"markdown","f6c034a6":"markdown","c0029661":"markdown","abeadbd4":"markdown","42918cfd":"markdown","489fecfb":"markdown","b98b7d6c":"markdown","9dba083c":"markdown","c692ff47":"markdown","fc3bb303":"markdown","a233bf99":"markdown","66a50a04":"markdown","392dbd82":"markdown","b3c7bbfb":"markdown","0b10edc6":"markdown","f98571e6":"markdown","d475e3f2":"markdown","69ed9be7":"markdown","c723c73e":"markdown","3004e552":"markdown","d8ace34d":"markdown","2a1690fa":"markdown","9df8f0d3":"markdown","d4824f7e":"markdown","fe537983":"markdown","a87a6263":"markdown","03ca996e":"markdown","d37280c7":"markdown","85187a96":"markdown","bb629e30":"markdown","8fdec9b2":"markdown","da824084":"markdown","64fdab2c":"markdown","d938cd0b":"markdown","1823d784":"markdown","d711674a":"markdown","ab649636":"markdown","e184caa2":"markdown","d94f185c":"markdown","9eee7820":"markdown","81ff0944":"markdown","4ce73a52":"markdown","2f8d0ba1":"markdown","b160be87":"markdown","692013ae":"markdown"},"source":{"7066eb58":"import pandas as pd\nimport numpy as np\nimport sys\nimport os\nimport random\nfrom pathlib import Path\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom plotly import tools\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=False)\nimport scipy\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.svm import SVC\nimport xgboost as xgb","486f07e6":"IS_LOCAL = True\nif(IS_LOCAL):\n    PATH=\"..\/input\/titanic-machine-learning-from-disaster\"\nelse:\n    PATH=\"..\/input\/\"\nos.listdir(PATH)","b34a21c5":"train_df=pd.read_csv(PATH+'\/train.csv')\ntest_df=pd.read_csv(PATH+'\/test.csv')","3dc971a2":"train_df.sample(5).head()","2d878c53":"test_df.sample(5).head()","eb945954":"print(\"Train: rows:{} cols:{}\".format(train_df.shape[0], train_df.shape[1]))\nprint(\"Test:  rows:{} cols:{}\".format(test_df.shape[0], test_df.shape[1]))","4d1ff1f8":"def missing_data(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    return pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data(train_df)","f5bc7e71":"missing_data(test_df)","d174e0b4":"def get_categories(data, val):\n    tmp = data[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","63b0ae0f":"def get_survived_categories(data, val):\n    tmp = data.groupby('Survived')[val].value_counts()\n    return pd.DataFrame(data={'Number': tmp.values}, index=tmp.index).reset_index()","f4b064f5":"def draw_trace_bar(data_df,color='Blue'):\n    trace = go.Bar(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\n\ndef plot_bar(data_df, title, xlab, ylab,color='Blue'):\n    trace = draw_trace_bar(data_df, color)\n    data = [trace]\n    layout = dict(title = title,\n              xaxis = dict(title = xlab, showticklabels=True, tickangle=0,\n                          tickfont=dict(\n                            size=10,\n                            color='black'),), \n              yaxis = dict(title = ylab),\n              hovermode = 'closest'\n             )\n    fig = dict(data = data, layout = layout)\n    iplot(fig, filename='draw_trace')\n","0ad230f1":"def plot_two_bar(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_bar(data_df1, color='Blue')\n    trace2 = draw_trace_bar(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1, 1, 1)\n    fig.append_trace(trace2, 1, 2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","82b21ade":"def plot_survived_bar(data_df, var):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Bar(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Bar(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = 'Number of survived and not survived passengers by {}'.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = 'Number of passengers'),\n          hovermode = 'closest',\n                  width = 800\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')\n","e9c086db":"plot_two_bar(get_categories(train_df,'Sex'), get_categories(test_df,'Sex'), \n             'Train data', 'Test data',\n             'Sex', 'Number of passengers')","f4ffa7b0":"plot_survived_bar(get_survived_categories(train_df,'Sex'), 'Sex')","0a75ee49":"plot_two_bar(get_categories(train_df,'Age'), get_categories(test_df,'Age'), \n             'Train data', 'Test data',\n             'Age', 'Number of passengers')","30dbc41f":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","a464dd57":"plot_two_bar(get_categories(train_df,'SibSp'), get_categories(test_df,'SibSp'), \n             'Train data', 'Test data',\n             'SibSp', 'Number of passengers')","f36da3b8":"plot_survived_bar(get_survived_categories(train_df,'SibSp'), 'SibSp')","fd89e0b7":"plot_two_bar(get_categories(train_df,'Parch'), get_categories(test_df,'Parch'), \n             'Train data', 'Test data',\n             'Parch', 'Number of passengers')","e8fdc70a":"plot_survived_bar(get_survived_categories(train_df,'Parch'), 'Parch')","7c27a190":"def draw_trace_histogram(data_df,color='Blue'):\n    trace = go.Histogram(\n            x = data_df['index'],\n            y = data_df['Number'],\n            marker=dict(color=color),\n            text=data_df['index']\n        )\n    return trace\n\ndef plot_two_histogram(data_df1, data_df2, title1, title2, xlab, ylab):\n    trace1 = draw_trace_histogram(data_df1, color='Blue')\n    trace2 = draw_trace_histogram(data_df2, color='Lightblue')\n    \n    fig = tools.make_subplots(rows=1,cols=2, subplot_titles=(title1,title2))\n    fig.append_trace(trace1,1,1)\n    fig.append_trace(trace2,1,2)\n    \n    fig['layout']['xaxis'].update(title = xlab)\n    fig['layout']['xaxis2'].update(title = xlab)\n    fig['layout']['yaxis'].update(title = ylab)\n    fig['layout']['yaxis2'].update(title = ylab)\n    fig['layout'].update(showlegend=False)\n    \n\n    iplot(fig, filename='draw_trace')","11e05bec":"def plot_survived_histogram(data_df, var):\n    dfS = data_df[data_df['Survived']==1]\n    dfN = data_df[data_df['Survived']==0]\n\n    traceS = go.Histogram(\n        x = dfS[var],y = dfS['Number'],\n        name='Survived',\n        marker=dict(color=\"Blue\"),\n        text=dfS['Number']\n    )\n    traceN = go.Histogram(\n        x = dfN[var],y = dfN['Number'],\n        name='Not survived',\n        marker=dict(color=\"Red\"),\n        text=dfS['Number']\n    )\n    \n    data = [traceS, traceN]\n    layout = dict(title = 'Number of survived and not survived passengers by {}'.format(var),\n          xaxis = dict(title = var, showticklabels=True), \n          yaxis = dict(title = 'Number of passengers'),\n          hovermode = 'closest'\n    )\n    fig = dict(data=data, layout=layout)\n   \n    iplot(fig, filename='draw_trace')","5d5904c9":"plot_two_histogram(get_categories(train_df,'Fare'), get_categories(test_df,'Fare'), \n             'Train data', 'Test data',\n             'Fare', 'Passengers')","40a23c15":"plot_survived_histogram(get_survived_categories(train_df,'Fare'), 'Fare')","218e7249":"plot_two_bar(get_categories(train_df,'Embarked'), get_categories(test_df,'Embarked'), \n             'Train data', 'Test data',\n             'Embarked', 'Number of passengers')","6348bf18":"plot_survived_bar(get_survived_categories(train_df,'Embarked'), 'Embarked')","8d8e71b8":"plot_two_bar(get_categories(train_df,'Pclass'), get_categories(test_df,'Pclass'), \n             'Train data', 'Test data',\n             'Pclass', 'Number of passengers')","ac460fb1":"plot_survived_bar(get_survived_categories(train_df,'Pclass'), 'Pclass')","98981462":"train_df['Ticket'].value_counts().head(10)","32c74fa7":"train_df['Cabin'].value_counts().head(10)","6520536d":"train_df['Name'].head(10)","d87c9c0f":"all_df = [train_df, test_df]","c50c353e":"for dataset in all_df:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)","ad767fbc":"pd.crosstab(train_df['Title'], train_df['Sex'])","fdcb46a3":"for dataset in all_df:\n    #unify `Miss`\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    #unify `Mrs`\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","ee1a57d6":"train_df[(train_df['Title'] == 'Dr') & (train_df['Sex'] == 'female')]","dbd28905":"train_df[train_df['Cabin']=='D17']","fae48005":"train_df.loc[train_df.PassengerId == 797, 'Title'] = 'Mrs'","4362559d":"train_df[train_df['Cabin']=='D17']","6acbb9a0":"for dataset in all_df:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n     'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')","0342f742":"train_df[['Title', 'Sex', 'Survived']].groupby(['Title', 'Sex'], as_index=False).mean()","aa2cbdda":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","e0c364a1":"for dataset in all_df:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","d8e407b6":"train_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean()","119fd6f2":"plot_survived_bar(get_survived_categories(train_df,'FamilySize'), 'FamilySize')","245230e4":"for dataset in all_df:\n    dataset['Deck'] = dataset.Cabin.str.extract('^([A-Za-z]+)', expand=False)","28d5cc39":"train_df[['Deck', 'Survived']].groupby(['Deck'], as_index=False).mean()","ee42ee26":"plot_survived_bar(get_survived_categories(train_df,'Deck'), 'Deck')","92ad28b1":"for dataset in all_df:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)","ea1a641f":"age_aprox = np.zeros((2,3))\nfor dataset in all_df:\n    for i in range(0, 2):\n        for j in range(0, 3):\n            aprox_age = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_aprox[i,j] = aprox_age.median()\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),'Age'] = \\\n                    age_aprox[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)","3e4ce282":"tmp = train_df.groupby(['Title', 'Pclass'])['Survived'].value_counts()\ndf = pd.DataFrame(data={'Passengers': tmp.values}, index=tmp.index).reset_index()\ndf","2903b39c":"title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in all_df:\n    dataset['Title'] = dataset['Title'].map(title_mapping)   ","f8e72260":"plot_survived_bar(get_survived_categories(train_df,'Title'), 'Title')","9b808d74":"for dataset in all_df:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3","782895c2":"plot_survived_bar(get_survived_categories(train_df,'Fare'), 'Fare')","7a5dd6ed":"for dataset in all_df:\n    dataset.loc[ dataset['Age'] <= 16, 'Age']  = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age'] = 4","2d559685":"plot_survived_bar(get_survived_categories(train_df,'Age'), 'Age')","3573fa81":"for dataset in all_df:\n    dataset.loc[ dataset['FamilySize'] <= 1, 'FamilySize'] = 0\n    dataset.loc[(dataset['FamilySize'] > 1) & (dataset['FamilySize'] <= 4), 'FamilySize'] = 1\n    dataset.loc[ dataset['FamilySize'] > 4, 'FamilySize'] = 2","e433e228":"for dataset in all_df:\n    dataset['Class*Age'] = dataset['Pclass'] * dataset['Age']","c422ee67":"plot_survived_bar(get_survived_categories(train_df,'Class*Age'), 'Class*Age')","7efa607a":"train_df.head(3)","4d67e061":"test_df.head(3)","68e943ed":"#We are using 80-20 split for train-test\nVALID_SIZE = 0.2\n#We also use random state for reproducibility\nRANDOM_STATE = 2018\n\ntrain, valid = train_test_split(train_df, test_size=VALID_SIZE, random_state=RANDOM_STATE, shuffle=True )","7e11af53":"predictors = ['Sex', 'Age']\ntarget = 'Survived'","bf5fef81":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","da74553e":"RFC_METRIC = 'gini'  #metric used for RandomForrestClassifier\nNUM_ESTIMATORS = 100 #number of estimators used for RandomForrestClassifier\nNO_JOBS = 4 #number of parallel jobs used for RandomForrestClassifier","97a17939":"clf = RandomForestClassifier(n_jobs=NO_JOBS, \n                             random_state=RANDOM_STATE,\n                             criterion=RFC_METRIC,\n                             n_estimators=NUM_ESTIMATORS,\n                             verbose=False)","5e3ca404":"clf.fit(train_X, train_Y)","2b45cf29":"preds = clf.predict(valid_X)","2b20c553":"def plot_feature_importance():\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': clf.feature_importances_})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance',fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","8ea3dd13":"plot_feature_importance()","f5692a45":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","45a254ea":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","cff908c7":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","3469d87d":"def plot_confusion_matrix():\n    cm = pd.crosstab(valid_Y, preds, rownames=['Actual'], colnames=['Predicted'])\n    fig, (ax1) = plt.subplots(ncols=1, figsize=(5,5))\n    sns.heatmap(cm, \n                xticklabels=['Not Survived', 'Survived'],\n                yticklabels=['Not Survived', 'Survived'],\n                annot=True,ax=ax1,\n                linewidths=.2,linecolor=\"Darkblue\", cmap=\"Blues\")\n    plt.title('Confusion Matrix', fontsize=14)\n    plt.show()","1e1ef27b":"plot_confusion_matrix()","3507dd90":"predictors = ['Sex', 'Age', 'Pclass', 'Fare']\ntarget = 'Survived'","f3448045":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","e5ab2641":"clf.fit(train_X, train_Y)","c2065664":"preds = clf.predict(valid_X)","2ee607a3":"plot_feature_importance()","d7818010":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","14636f64":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","446dacab":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","1b9e1dec":"plot_confusion_matrix()","142754e6":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp']\ntarget = 'Survived'","319bd9f3":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","e04d5e21":"clf.fit(train_X, train_Y)","540d956c":"preds = clf.predict(valid_X)","77ec2495":"plot_feature_importance()","99cc5195":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","1af47c18":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","24a44751":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","5f825c79":"plot_confusion_matrix()","58db3b22":"predictors = ['Sex', 'Age', 'Pclass', 'Fare', 'Parch', 'SibSp', 'FamilySize', 'Title']\ntarget = 'Survived'","208fae5b":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","0ef84b88":"clf.fit(train_X, train_Y)","61e2a27e":"preds = clf.predict(valid_X)","3e93e186":"plot_feature_importance()","8e8bacaf":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","fb0e857e":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","f00155e2":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","5f460ad7":"plot_confusion_matrix()","d5ec61f6":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","2f49db1e":"train_X = train[predictors]\ntrain_Y = train[target].values\nvalid_X = valid[predictors]\nvalid_Y = valid[target].values","7a78c491":"rf_clf = clf.fit(train_X, train_Y)","36046b13":"rf_clf","1e08f51f":"preds = clf.predict(valid_X)","79628552":"plot_feature_importance()","5381116c":"clf.score(train_X, train_Y)\nacc = round(clf.score(train_X, train_Y) * 100, 2)\nprint(\"RandomForest accuracy (train set):\", acc)","0cd967c3":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","1217737f":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","10e0f016":"plot_confusion_matrix()","5e1c1c64":"test_X = test_df[predictors]\npred_Y = clf.predict(test_X)","90ce6ed5":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission.csv', index=False)","29a621a8":"rf_clf = clf.fit(train_X, train_Y)","77a8b207":"parameters = {\n    'n_estimators': (50, 75,100),\n    'max_features': ('auto', 'sqrt'),\n    'max_depth': (3,4,5),\n    'min_samples_split': (2,5,10),\n    'min_samples_leaf': (1,2,3)\n}","b85b5e12":"%%time\ngs_clf = GridSearchCV(rf_clf, parameters, n_jobs=-1, cv = 5, verbose = 5)\ngs_clf = gs_clf.fit(train_X, train_Y)","2d893692":"print('Best scores:',gs_clf.best_score_)\nprint('Best params:',gs_clf.best_params_)","df4962d7":"preds = gs_clf.predict(valid_X)","42be2b8c":"clf.score(valid_X, valid_Y)\nacc = round(clf.score(valid_X, valid_Y) * 100, 2)\nprint(\"RandomForest accuracy (validation set):\", acc)","7f081430":"print(metrics.classification_report(valid_Y, preds, target_names=['Not Survived', 'Survived']))","8fefb988":"test_X = test_df[predictors]\npred_Y = gs_clf.predict(test_X)","8eb989f6":"submission = pd.DataFrame({\"PassengerId\": test_df[\"PassengerId\"],\"Survived\": pred_Y})\nsubmission.to_csv('submission_hyperparam_optimization.csv', index=False)","3af0e2ae":"NUMBER_KFOLDS = 5\nkf = KFold(n_splits = NUMBER_KFOLDS, random_state = RANDOM_STATE, shuffle = True)","6641f526":"# Class to extend the Sklearn classifier\nclass SklearnBasicClassifier(object):\n    def __init__(self, clf, seed=2018, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n\n    def get_feature_importances(self,x,y):\n        return (self.clf.fit(x,y).feature_importances_)\n","19d3cecc":"ntrain = train_df.shape[0]\nntest = test_df.shape[0]\ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NUMBER_KFOLDS, ntest))\n    \n    for i, (train_idx, valid_idx) in enumerate(kf.split(train_df)):\n        x_tr = x_train[train_idx]\n        y_tr = y_train[train_idx]\n        x_val = x_train[valid_idx]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[valid_idx] = clf.predict(x_val)\n        oof_test_skf[i, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","ec5ccf0a":"# AdaBoost parameters\nada_params = {\n    'n_estimators': 200,\n    'learning_rate' : 0.75\n}\n\n# CatBoost parameters\ncat_params = {\n    'iterations': 150,\n    'learning_rate': 0.02,\n    'depth': 12,\n    'bagging_temperature':0.2,\n    'od_type':'Iter',\n    'metric_period':400,\n}  \n\n# Extra Trees Parameters\next_params = {\n    'n_jobs': -1,\n    'n_estimators':100,\n    'max_depth': 8,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n\n# Gradient Boosting parameters\ngbm_params = {\n    'n_estimators': 200,\n    'max_depth': 5,\n    'min_samples_leaf': 3,\n    'verbose': 0\n}\n\n# Random Forest parameters\nrfo_params = {\n    'n_jobs': -1,\n    'n_estimators': 50,\n    'max_depth': 5,\n    'min_samples_leaf': 5,\n    'max_features' : 'auto',\n    'verbose': 0\n}\n\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.02\n    }","7ba17752":"# Create 6 objects that represent our 6 models\nada = SklearnBasicClassifier(clf=AdaBoostClassifier, seed=RANDOM_STATE, params=ada_params)\ncat = SklearnBasicClassifier(clf=CatBoostClassifier, seed=RANDOM_STATE, params=cat_params)\next = SklearnBasicClassifier(clf=ExtraTreesClassifier, seed=RANDOM_STATE, params=ext_params)\ngbm = SklearnBasicClassifier(clf=GradientBoostingClassifier, seed=RANDOM_STATE, params=gbm_params)\nrfo = SklearnBasicClassifier(clf=RandomForestClassifier, seed=RANDOM_STATE, params=rfo_params)\nsvc = SklearnBasicClassifier(clf=SVC, seed=RANDOM_STATE, params=svc_params)","978ebd49":"predictors = ['FamilySize', 'Title', 'Class*Age']\ntarget = 'Survived'","e9d5a197":"y_train = train_df['Survived'].values\ntrain = train_df[predictors]\ntest = test_df[predictors]\nx_train = train.values\nx_test = test.values","f8e16997":"print(\"Start training\")\nada_oof_train, ada_oof_test = get_oof(ada, x_train, y_train, x_test) # AdaBoost Classifier\nprint(\"End AdaBoost\")\ncat_oof_train, cat_oof_test = get_oof(cat, x_train, y_train, x_test) # CatBoost Classifier\nprint(\"End CatBoost\")\next_oof_train, ext_oof_test = get_oof(ext, x_train, y_train, x_test) # Extra Trees \nprint(\"End ExtraTrees\")\nrfo_oof_train, rfo_oof_test = get_oof(rfo,x_train, y_train, x_test) # Random Forest Classifier\nprint(\"End RandomForest\")\ngbm_oof_train, gbm_oof_test = get_oof(gbm,x_train, y_train, x_test) # Gradient Boost Classifier\nprint(\"End GradientBoost\")\nsvc_oof_train, svc_oof_test = get_oof(svc,x_train, y_train, x_test) # Support Vector Classifier\nprint(\"End training\")","d4f3a954":"ada_feature_importance = ada.get_feature_importances(x_train,y_train)\ncat_feature_importance = cat.get_feature_importances(x_train,y_train)\next_feature_importance = ext.get_feature_importances(x_train,y_train)\ngbm_feature_importance = gbm.get_feature_importances(x_train,y_train)\nrfo_feature_importance = rfo.get_feature_importances(x_train,y_train)","01d2dda3":"def plot_feature_importance(feature_importance, classifier):\n    tmp = pd.DataFrame({'Feature': predictors, 'Feature importance': feature_importance[0:len(predictors)]})\n    tmp = tmp.sort_values(by='Feature importance',ascending=False)\n    plt.figure(figsize = (7,4))\n    plt.title('Features importance {}'.format(classifier),fontsize=14)\n    s = sns.barplot(x='Feature',y='Feature importance',data=tmp)\n    s.set_xticklabels(s.get_xticklabels(),rotation=90)\n    plt.show()   ","c61dcc7c":"plot_feature_importance(ada_feature_importance, '- AdaBoost')\nplot_feature_importance(cat_feature_importance, '- CatBoost')\nplot_feature_importance(ext_feature_importance, '- ExtraTrees')\nplot_feature_importance(gbm_feature_importance, '- GradientBoosting')\nplot_feature_importance(rfo_feature_importance, '- RandomForest')","be69eaba":"base_predictions_train = pd.DataFrame( {\n     'AdaBoost': ada_oof_train.ravel(),\n     'CatBoost': cat_oof_train.ravel(),\n     'ExtraTrees': ext_oof_train.ravel(),\n     'GradientBoost': gbm_oof_train.ravel(),\n     'RandomForest': rfo_oof_train.ravel(),\n     'SVM': svc_oof_train.ravel()\n    })\nbase_predictions_train.head(10)","3d48b907":"trace = go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Rainbow',\n            showscale=True,\n            reversescale = False\n    )\ndata = [trace]\nlayout = dict(width = 600, height=600)\nfig = dict(data=data, layout=layout)\niplot(fig, filename='heatmap')","d8d9c94c":"x_train = np.concatenate(( ada_oof_train, cat_oof_train, ext_oof_train, gbm_oof_train, rfo_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( ada_oof_test, cat_oof_test, ext_oof_test, gbm_oof_test, rfo_oof_test, svc_oof_test), axis=1)","f51e8715":"clf = xgb.XGBClassifier(\n learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)","db232f3d":"xgbm = clf.fit(x_train, y_train)","33f16691":"predictions = xgbm.predict(x_test)","342b6865":"submissionStacking = pd.DataFrame({ 'PassengerId': test_df[\"PassengerId\"],'Survived': predictions })\nsubmissionStacking.to_csv(\"submission_ensamble.csv\", index=False)","2751cad2":"There are **train** and **test** data as well as an example submission file.  \n\nLet's load the **train** and **test** data.","688ad976":"We will pick this last model for submission.\nIt doesn't have the best accuracy for train set classification but have one of the best precision and accuracy for the validation set and also the smallest recall. \n\nLet's prepare the submission.","98474de8":"There are few values given in this report \/ each category in the target (`Survived` or `Not survived`):\n\n* **Precision**  \n* **Recall**  \n* **F1-score**  \n\nLet's explain each of them:\n\n* **Precision** identifies the frequency with which a model was correct when predicting the positive class. That is:\n\n$$Precision = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Positives}}$$\n\n* **Recall** answers the following question: Out of all the possible positive labels, how many did the model correctly identify? That is:\n\n$$Recall = \\frac{\\textrm{True Positives}}{\\textrm{True Positives} + \\textrm{False Negatives}}$$\n\n* **F1-score** is the harmonic mean of **precision** and **recall**.\n\n$$\\textrm{F1-score} = 2  \\frac{Precision * Recall}{Precision + Recall}$$\n\n\nPrecison is better for the `Not survived` as well as Recall and F1-score.\n\nLet's also show the confusion matrix.","0aded10c":"We fit the model.","ea0ef106":"Family Size is mapped then to only 3 sizes (0 to 2), the first corresponding to the case when someone is alone.","9e9b941d":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='54'>Submission<\/a>\n\n\nFirst, we predict for test data using the trained model.","587bbda9":"The accuracy for the train data classification improved further but the accuracy and precision for validation did not improved.    \n\nLet's add few more engineering features.\n\n\n### Model with {Sex, Age, Pclass, Fare, Parch, SibSp, FamilySize, Title} features","1a8ce3bb":"Let's also plot the classification report for the validation set.","a0b01860":"We see that we obtained an substantial improvement of the classification precision for the validation set.   Also the recall is better than in the case of the model with best accuracy and precision until now, besides this one.\n\nLet's also plot the confusion matrix.","b28008cd":"Let's see the train set and validation set classification accuracy.","664129f8":"We predict the validation set.","c69425ef":"Let's check the features importance for the 5 out of 6 models. We will not include Support Vector Classifier, this one does not have feature importance available.","f150f943":"Let's initialize the GradientSearchCV parameters. We will set only few parameters, as following:\n\n* **n_estimators**: number of trees in the foreset;  \n* **max_features**: max number of features considered for splitting a node;  \n* **max_depth**: max number of levels in each decision tree;  \n* **min_samples_split**: min number of data points placed in a node before the node is split;  \n* **min_samples_leaf**: min number of data points allowed in a leaf node.\n","abf0ba99":"Let's check the validation classification report.","96eac250":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='52'>Evaluate the model<\/a>  \n\nLet's evaluate the model performance. \n\nWe are evaluating first the accuracy for the train set.\n\n","6f3a49ce":"Let's verify the relationship between `Title` and `Sex`.","4c754655":"Let's plot feature importance.","4804084e":"## <a id='44'>More features engineering<\/a>  \n\n\nLet's map all titles to numeric values.","22ea127f":"From the total female passengers, 74% survived.  \nIn the same time, from the total male passengers, only 18% survived.","c3191dec":"  All rare female titles were saved.   \n  Married females had the highest survival rate besides these very rare cases, of 79%. Young unmarried women followed with 70% survival rate. \n  Lowest survival rate had the men with `Mr` title. \n  Between men, the ones with `Master` title had a much higher survival rate than  these, with 57%.","a31b3654":"## <a id='56'>Submission (model with hyperparameters optimization)<\/a>\n\n\nFirst, we predict for test data using the trained model.\n","1a8b1030":"The model performance with train data improved, i.e. the model is now representing better the training set.  The accuracy and precision with the validation set was as well improved. The recall and f1-score are smaller for `Survived`. \n\nLet's repeat the experiment adding more features.","d172840f":"There are male only titles: `Capt`, `Col`, `Don`, `Jonkheer`, `Major`, `Master`, `Mr`, `Rev` and `Sir`. \nAs well, there are female only titles: `Countess`, `Lady`, `Miss`,  `Mlle`,  `Mme`, `Mrs`, `Ms`.\nThere is a female `Dr` (and other 6 males).\n\nMost of these titles are quite rare. We will either group them as `Rare` or correct them (for example, to reunite all young womens with the title `Miss`.\n\nLet's start by grouping all `Miss` and `Mrs` variations under a single name.","7e4a06c2":"The confusion matrix.","a14b395d":"We predict the validation set.","f485e661":"The training set classification accuracy is still good, although lower than that of the previous, more complex model.\nThe validation set accuracy is the best obtained until now.\n\n**Title** is the most important feature.  \n\nLet's check now the classification report for the validation set.","60589140":"We predict the validation set.","444bb755":"<h1><center><font size=\"6\">Tutorial for Classification<\/font><\/center><\/h1>\n\n<h2><center><font size=\"4\">Dataset used: Titanic - Machine Learning from Disaster<\/font><\/center><\/h2>\n\n\n\n<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fd\/RMS_Titanic_3.jpg\/640px-RMS_Titanic_3.jpg\"><\/img>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n - <a href='#21'>Load packages<\/a>  \n - <a href='#22'>Load the data<\/a>   \n- <a href='#3'>Data exploration<\/a>   \n - <a href='#31'>Check for missing data<\/a>  \n - <a href='#32'>Sex, Age, SibSp, Parch<\/a>   \n - <a href='#33'>Fare, Embarked, Pclass<\/a>  \n - <a href='#34'>Ticket, Cabin, Name<\/a> \n- <a href='#4'>Feature engineering<\/a>\n - <a href='#41'>Extract Title from Name<\/a>\n - <a href='#42'>Build families<\/a>\n - <a href='#43'>Extract Deck from Ticket<\/a>  \n - <a href='#44'>Estimate age<\/a>  \n - <a href='#45'>More features engineering<\/a>  \n- <a href='#5'>Predictive model for survival<\/a>\n - <a href='#50'>Split the data<\/a>  \n - <a href='#51'>Build a baseline model<\/a>  \n - <a href='#52'>Model evaluation<\/a>    \n - <a href='#53'>Model refinement<\/a> \n - <a href='#54'>Submission<\/a>  \n - <a href='#55'>Hyperparameters optimization<\/a>\n - <a href='#56'>Submission (model with hyperparameters optimization)<\/a>  \n- <a href='#6'>Use model ensambling<\/a>\n - <a href='#61'>Create the ensamble framework<\/a>\n - <a href='#62'>Create the Out-Of-Fold Predictions<\/a>\n - <a href='#63'>Train the first level models<\/a>\n - <a href='#64'>Correlation of the results<\/a>\n - <a href='#65'>Build the second level (ensamble) model<\/a>\n - <a href='#66'>Submission (ensamble)<\/a>\n- <a href='#7'>References<\/a>    ","31c61410":"### Model with {Sex, Age, Pclass, Fare, Parch, SibSp} features","c7708587":"Let's plot also the feature importance.","9a28d0d3":"## <a id='65'>Build the second level (ensamble) model<\/a>\n\n\nWe prepare now, using the Out-Of-Folds values, the training and the test set for the second level model. We concatenate the OOFs from the 6 first level models.","0913b2a0":"Most of the passengers traveled alone. From the passengers travelling alone, only 34% survived.\nThe passengers with only one or two sibbilings survived in around 50% of the cases. \nSurvival rates decrease considerably for number of sibilings or spouses of 3, 4, 5 and is practically 0 for 8.","81651e53":"And let's plot the `Class*Age` grouped by the survived and not survived.","350b9953":"Majority of the passengers were between 20 and 35 years old.\n\nThe survival rate for the age interval 15-35 years was quite small.\n","3e9ca588":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='22'>Load the data<\/a>  \n\nLet's see first what data files do we have in the root directory. ","23fc1100":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='34'>Ticket, Cabin, Name<\/a>  \n\nLet's check now the data (in **train** and **test**) for  `Ticket`,  `Cabin` and`Name`.\n\nAll these are alphanumeric (contains both letters and numbers), like `Ticket` and `Cabin` or are text fields (`Name`). \n\nWe will have to process them in order to use as features.  \n\nLet's look to `Ticket` first.\n\n","594fda4b":"Similarly, we map `Age` to 5 main segments, labeled from 0 to 4. ","d6a4e1de":"Let's check the accuracy for the validation set.","20ed9212":"The plot shows just the data for the `Deck` information that could be extracted i.e. where a `Cabin` was defined.","84533bab":"And let's plot the `Fare` clusters grouped by the survived and not survived.","169b608d":"Let's verify the average survival ratio for the passengers with the aggregated titles and sex.","dd361b94":"Survival rate increases considerably with the fare value. This confirm the image that richer people survived better. We will validate this observation as well with the class information.","7d480038":"Let's fit the model with the new data.","de1708b1":"Let's also add one more feature, `Class*Age`, calculated as `Class` x `Age`.","76137fc1":"We succesfully set passenger #797 as a `Mrs`.\n\nLet's also group all the rare titles under a `Rare` title:","8b76603b":"## <a id='62'>Create the Out-of-Fold Predictions<\/a>\n\nLet's now define out-of-folds predictions. If we would train the base models on the full training data and generate predictions on the full test set and then output these for the second-level training we might go into trouble. The risk here is that the base model predictions would have seen the test set and thus overfitting when feeding those predictions.","6a6ea3d8":"We predict the validation set.","89656544":"Let's predict with the validation data.","2334e7f5":"Both **train** and **test** files contains the following values:  \n\n* **PassengerID** - the index of the passenger (in the dataset);  \n* **PClass** - the class of the passenger (from 1 to 3);\n* **Name** - the name of the passenger;\n* **Sex** - the sex of the passenger (female or male);  \n* **Age** - the age (where available) of the passenger;  \n* **SibSp** - the number of sibilings \/ spouses aboard of Titanic;  \n* **Parch** - the number of parents \/ children aboard of Titanic;  \n* **Ticket** - the ticket number;  \n* **Fare** - the passenger fare (ticket cost);  \n* **Cabin** - the cabin number;  \n* **Embarked** - the place of embarcation of the passenger (C = Cherbourg, Q = Queenstown, S = Southampton).  \n\nThe **train** data has as well the target value, **Survived**.\n\nIt is important, before going to create a model, to have a good understanding of the data. We will therefore explore the various features.","1fa1d136":"The classification report for the validation set.","50bb8798":"We see that in the `Name` field we have the Family name, the title (which might indicate as well the social status or marital status), and the first name. So, `Name` is actually a quite rich feature column, which can be further exploited (and we will exploit in the next sections). ","54267da9":"Let's see also the classification report for the validation set.","df8b7221":"We plot the feature importance.","b6fcd29f":"The training set accuracy improved even more. In the same time, validation score is not very much improved. This indicates that most probably the model where we added more features is overfitting on the training set. Actually, the best model obtained until now was the one with {Sex, Age, Pclass, Fare} features, where accuracies for training set and validation set were 82% and 86% (with quite good precision for all categories and small recall for `Survived`).\n\nLet's try with a simpler model.\n\n\n### Model with {Title, FamilySize, Pclass} features\n\nThe simple model we will try now, with only three features, is actually using only engineered features.","f16e2c8e":"Unmarried passengers and passengers with very large families had the lowest survival rate (30% singles and less than 20% the family members with families larger than 5). This can be explained by the fact that singles were most probably mens in lower classes while families with small number of children most probably saved at least a part of them (for example the mother with the childrens) cooperating between them to ensure salvation. Large families might had lower survival rates due to various reasons, including maybe difficulties to coordinate or more difficult decision on whom to embark on the boats (if not possible for all to embark). Another reason for larger families to survive might be related with the class.","e55d5010":"Let's prepare a simple model, using Random Forest. We set few algorithm parameters and initialize a clasiffier.","58f173fa":"Let's see the accuracy for the training set and for the validation set.","b8cf900a":"We fit the model.","20a4624a":"Then, we prepare the submission dataset and export it in the submission file.","b92405d2":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='55'>Hyperparameters optimization<\/a>\n\n\nLet's continue with tunning the model hyperparameters.   \nWe define a set of parameters with several values and will run an algorithm called Gradient Search to detect the best combination of parameters for our model.  \nFirst, let's fit the model and assign the output of it to **rf_clf**.","4f12f50b":"## <a id='32'>Sex, Age, SibSp, Parch<\/a>  \n\nLet's check now the data (in **train** and **test**) for `Sex`, `Age`, `SibSp` and `Parch`.","f8d39da3":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n# <a id='4'>Features engineering<\/a>\n\nFrom the original features, we will create new features.","2bcff083":"Let's plot the features importance. This shows the relative importance of the predictors features for the current model. With this information, we are able to select the features we will use for our gradually refined models.","b0f90c08":"We also map `Fare` to 4 main fare segments and label them from 0 to 3.","7940731f":"Let's see now how looks like train and test set.","df8d09b7":"\nLet's start by checking if there are missing data, unlabeled data or data that is inconsistently labeled. ","192404de":"## <a id='61'>Create the ensamble framework<\/a>\n\n\nWe start by creating a generic classifier, that extends the functionality of a simple classifier. This generic classifier will be instanciated with few different first level classifiers and then used in the ensamble. We are also using cross-validation (with KFolds).\n\nFirst step will be to create the folds used in cross validation.","bc3e55d1":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n## <a id='53'>Model refinement<\/a> \n\nLet's rebuild the model and add more features to it.","894564ec":"Because she is traveling with a friend about the same age and with a `Mrs` title, we might want to set her as well as a `Mrs`. Let's do it.","e57446e8":"\n## <a id='50'>Split the data<\/a>  \n\nLet's split the training and validation set. We will use a 80-20 split.\nWe also set the matrices for train and validation and the vectors with the target values.","c4600a53":"# <a id='2'>Prepare the data analysis<\/a>   \n\n\nBefore starting the analysis, we need to make few preparation: load the packages, load and inspect the data.\n","207ea4c7":"We will start with a simple model, with just few predictors.\n\nLet's set now the predictors list and the target value. We start with two predictors, the `Sex` and `Pclass`.","397c8063":"And let's plot the `Age` clusters grouped by the survived and not survived.","3d59276c":"Let's fit the model with the new predictors.","433736c7":"Let's check the correlation between family size and survival rate.","f6c034a6":"She is traveling in Cabin `D17` in 1st class. Let's see if she is alone.","c0029661":"First class passengers survived in a percent of 63% while less than 50% survived in 2nd class. For passengers in 3rd class, only 24% survived.","abeadbd4":"With the fitted second level model we do the prediction for the test data.","42918cfd":"\n## <a id='51'>Build a baseline model<\/a>  ","489fecfb":"The best survival rate is for passengers embarked in Cherbourg (more than 50%), the worst for passengers embarked in Southampton.","b98b7d6c":"The accuracy for the validation is much better than the accuracy for the training set. \nThis means we have higher bias than variation. The model does not learn too well the training set, we are not overfitting (yet). The validation is better, i.e. we are generalizing well. Before improving the variation, we will try to improve the bias, while looking how this affects the variation as well.\n\nLet's plot now the classification report for validation data.","9dba083c":"We check the features importance.","c692ff47":"## <a id='64'>Correlation of the results<\/a>\n\nLet's see the first few results of the predictions using first level models.","fc3bb303":"We fit the model.","a233bf99":"Here we plot the confusion matrix.","66a50a04":"## <a id='44'>Estimate age<\/a>\n\nA relativelly large number of passengers have missing Age information. We will try to estimate this missing information from other available data, `Sex` and `Pclass`. Before doing this, we will also map sex values to numeric values.","392dbd82":"We prepare as well the second level classifier. \n\nWe will use in this case a eXtreme Boost Classifier.","b3c7bbfb":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n# <a id='6'>Use model ensambling<\/a>\n\n\nLet's continue with creation of second level models. We will train several models and will then use these first level models to train a second level model. This method is powerfull and can enhance the performance of first level models, especially when there is little correlation between the results of first level models.\n","0b10edc6":"Both in **train** and **test** datasets, `Cabin` has more than 77% missing, `Age` more than 19%. `Embarked` is missing in 2 cases for **train** and `Fare` misses in  1 case for **test**.   \n\nWe will discuss, through this tutorial, the various possible methods to deal with missing data.","f98571e6":"## <a id='21'>Load packages<\/a>\n\nWe load the packages used for the analysis. There are packages for data manipulation, visualization, models, hyperparameter optimization and model metrics..","d475e3f2":"Now we replaced the missing Age values with the one obtained from the approximations.","69ed9be7":"## <a id='63'>Train the first level models<\/a>  \n\n\nLet's define and train few first level models.\n\nWe will use the following first level models:\n\n\n* AdaBoost classifer\n* CatBoost Classifier\n* Extra Trees classifier  \n* Gradient Boosting classifer\n* Random Forest classifier  \n* Support Vector Machine\n\n\nLet's define the parameters used for training each classifier:","c723c73e":"## <a id='41'>Extract Title from Name<\/a>\n\nLet's start with processing the names. We will extract the title from the names.\n\nFirst, let's create a list with **train** and **test** datasets, to process both in the same time.","3004e552":"We apply the rule for extracting the title.","d8ace34d":"And let's plot the `Title` grouped by the survived and not survived.","2a1690fa":"# <a id='1'>Introduction<\/a>  \n\nThis Kernel will take you through the process of analyzing the data to understand the predictive values of various features and the possible correlation between different features, selection of features with predictive value, features engineering to create features with higher predictive value, creation of a baseline model, succesive refinement of the model (we are using RandomForest) through selection of features and, at the end, submission of the best solution found. \n\nNext, we take the model and define a multi-dimmensional matrix of hyperparameters we would like to test. We use Gradient Search and cross-validation to select the best set of hyperparameters. The best model is then used for the second submission.\n\nNext, we will use ensambling (second level model trained with the output of first level models). We create several models (with the same set of parameters used with the previous model). We use AdaBoost, CatBoost, ExtraTrees, GradientBoosting, RandomForest and SupportVectorMachines. We are using Out-Of-Folds to avoid the risk that the base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions. The Out-Of-Folds are concatenated and feed to the second level model (XGBoost Classifier) and the prediction using the second level model is submitted as the solution.\n\nThe dataset used for this tutorial is the famous now **Titanic** dataset.\n\n\n<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  ","9df8f0d3":"We create 6 objects of type `SklearnBasicClassifier` that represent our 6 models(AdaBoost, CatBoost, ExtraTrees, GradientBoosting, RandomForest and Support Vector Machines).","d4824f7e":"Let's see the best parameters.","fe537983":"## <a id='31'>Check for missing data<\/a>  \n\nLet's create a function that check for missing data in the two datasets (train and test).","a87a6263":"The result means that the total number of correct predictions divided by the total number of examples in the training set is around 0.77. Because we have a binary classification, this means:\n\n$$Accuracy =\\frac{ \\textrm{True Positives} + \\textrm{True Negatives}}{\\textrm{Number of Examples}}$$ \n\n\nThen we evaluate the accuracy for the validation set. ","03ca996e":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n# <a id='5'>Predictive model for survival<\/a>  \n\nLet's start with creation of the predictive models. We will create a very simple model for starting.","d37280c7":"## <a id='42'>Build families<\/a>\n\nFrom `SibSp` and `Parch` we create a new feature, `FamilySize`.","85187a96":"The precision obtained for the test set is approx. **0.79**.","bb629e30":"## <a id='43'>Extract Deck from Cabin<\/a>\n\nWe will extract the deck name from the Cabin name by separating the first character from each cabin name. \nUnfortunatelly, a very small number of passengers have `Cabin` information therefore also the `Deck` information will be only available for a reduced number of passengers.","8fdec9b2":"Then we prepare the submission dataset and save it to the submission file.","da824084":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n# <a id='3'>Data exploration<\/a>  \n\n","64fdab2c":"We initialize GridSearchCV with the classifier, the set of parameters, number of folds and also the level of verbose for printing out progress.","d938cd0b":"Let's check if this worked well.","1823d784":"## <a id='66'>Submission (ensamble)<\/a>\n\nWe form the submission dataset and save it to the submission file.","d711674a":"`Cabin` has the first letter that is, most probably, giving the information on the deck. This might have a predictive value and we will process further in the next sections.\n\n`Name` might contain multiple information. Let's check few of the `Name` fields.","ab649636":"**Sex** is the dominant feature, followed by **Pclass** and **Fare**.  \nLet's see the accuracy for the training set and also for the validation set classification.","e184caa2":"<a href=\"#0\"><font size=\"1\">Go to top<\/font><\/a>  \n\n\n## <a id='33'>Fare, Embarked, Pclass<\/a>  \n\nLet's check now the data (in **train** and **test**) for  `Fare`,  `Embarked` and`Pclass`.  \n","d94f185c":"Then, let's set the female Dr as one of the female tipical roles.","9eee7820":"Let's plot the parameters for the classifier.","81ff0944":"For the number of children or parents equal to 0, the survival rate is only 34%. \nFor values of 1,2 and 3, the survival rate is 50%, decreasing for larger numbers.","4ce73a52":"### Model with {Sex, Age, Pclass, Fare} features","2f8d0ba1":"Then, we fit the classifier with the train data prepared before.","b160be87":"`Ticket` has, most probably, little predictive value.  \n\nLet's see also the `Cabin`.\n","692013ae":"Let's show now the correlation of the predictions using the first level models. The ensamble prediction is best when we have models with good accuracy and less correlated."}}