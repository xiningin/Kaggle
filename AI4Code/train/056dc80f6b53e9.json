{"cell_type":{"a530d623":"code","2e67d190":"code","6b904989":"code","8b3a7200":"code","8028e83e":"code","bf279ff7":"code","ee1e56f8":"code","c74df012":"code","8ba2806a":"code","315d337f":"markdown","4724bf71":"markdown","c9a4c278":"markdown"},"source":{"a530d623":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2e67d190":"from keras.preprocessing import sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation\nfrom keras.layers import Embedding\nfrom keras.layers import Conv1D, GlobalMaxPooling1D\nfrom keras.callbacks import EarlyStopping\nfrom keras.datasets import imdb","6b904989":"import numpy as np\nimport json\nimport warnings\n\n\ndef load_data(path='..\/input\/imdb.npz', num_words=None, skip_top=0,\n              maxlen=None, seed=113,\n              start_char=1, oov_char=2, index_from=3, **kwargs):\n    \"\"\"Loads the IMDB dataset.\n    # Arguments\n        path: where to cache the data (relative to `~\/.keras\/dataset`).\n        num_words: max number of words to include. Words are ranked\n            by how often they occur (in the training set) and only\n            the most frequent words are kept\n        skip_top: skip the top N most frequently occurring words\n            (which may not be informative).\n        maxlen: sequences longer than this will be filtered out.\n        seed: random seed for sample shuffling.\n        start_char: The start of a sequence will be marked with this character.\n            Set to 1 because 0 is usually the padding character.\n        oov_char: words that were cut out because of the `num_words`\n            or `skip_top` limit will be replaced with this character.\n        index_from: index actual words with this index and higher.\n    # Returns\n        Tuple of Numpy arrays: `(x_train, y_train), (x_test, y_test)`.\n    # Raises\n        ValueError: in case `maxlen` is so low\n            that no input sequence could be kept.\n    Note that the 'out of vocabulary' character is only used for\n    words that were present in the training set but are not included\n    because they're not making the `num_words` cut here.\n    Words that were not seen in the training set but are in the test set\n    have simply been skipped.\n    \"\"\"\n    # Legacy support\n    if 'nb_words' in kwargs:\n        warnings.warn('The `nb_words` argument in `load_data` '\n                      'has been renamed `num_words`.')\n        num_words = kwargs.pop('nb_words')\n    if kwargs:\n        raise TypeError('Unrecognized keyword arguments: ' + str(kwargs))\n\n    with np.load(path) as f:\n        x_train, labels_train = f['x_train'], f['y_train']\n        x_test, labels_test = f['x_test'], f['y_test']\n\n    np.random.seed(seed)\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n\n    indices = np.arange(len(x_test))\n    np.random.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n\n    xs = np.concatenate([x_train, x_test])\n    labels = np.concatenate([labels_train, labels_test])\n\n    if start_char is not None:\n        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n    elif index_from:\n        xs = [[w + index_from for w in x] for x in xs]\n\n    if maxlen:\n        xs, labels = _remove_long_seq(maxlen, xs, labels)\n        if not xs:\n            raise ValueError('After filtering for sequences shorter than maxlen=' +\n                             str(maxlen) + ', no sequence was kept. '\n                             'Increase maxlen.')\n    if not num_words:\n        num_words = max([max(x) for x in xs])\n\n    # by convention, use 2 as OOV word\n    # reserve 'index_from' (=3 by default) characters:\n    # 0 (padding), 1 (start), 2 (OOV)\n    if oov_char is not None:\n        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x]\n              for x in xs]\n    else:\n        xs = [[w for w in x if skip_top <= w < num_words]\n              for x in xs]\n\n    idx = len(x_train)\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_test, y_test)\n\n\n","8b3a7200":"n_words = 1000\n(X_train, y_train), (X_test, y_test) = load_data(num_words=n_words)\nprint('Train seq: {}'.format(len(X_train)))\nprint('Test seq: {}'.format(len(X_train)))","8028e83e":"print('Train example: \\n{}'.format(X_train[0]))\nprint('\\nTest example: \\n{}'.format(X_test[0]))","bf279ff7":"# Pad sequences with max_len\nmax_len = 200\nX_train = sequence.pad_sequences(X_train, maxlen=max_len)\nX_test = sequence.pad_sequences(X_test, maxlen=max_len)","ee1e56f8":"# Define network architecture and compile\nmodel = Sequential()\nmodel.add(Embedding(n_words, 50, input_length=max_len))\nmodel.add(Dropout(0.5))\nmodel.add(Conv1D(128, 3, padding='valid', activation='relu', strides=1,))\nmodel.add(GlobalMaxPooling1D())\nmodel.add(Dense(250, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(loss='binary_crossentropy',  optimizer='adam', metrics=['accuracy'])\nmodel.summary()","c74df012":"callbacks = [EarlyStopping(monitor='val_acc', patience=3)]\nbatch_size = 128\nn_epochs = 3\n\nmodel.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, validation_split=0.2, callbacks=callbacks)","8ba2806a":"print('\\nAccuracy on test set: {}'.format(model.evaluate(X_test, y_test)[1]))\n\n# Accuracy on test set: 0.873","315d337f":"**IMDB Movie reviews sentiment classification**\n\nDataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive\/negative). Reviews have been preprocessed, and each review is encoded as a sequence of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n\nAs a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word.","4724bf71":"**Introduction**\n\nCNNs are typically used with image data. As stated in the introduction, CNNs can also be applied to other types of input data. In this example, we will apply a CNN to textual data. More specifically, we will use the structure of CNNs to classify text. Unlike images, which are 2D, text has 1D input data.\nTherefore, we will be using 1D convolutional layers in our next recipe. The Keras framework makes it really easy to pre-process the input data.","c9a4c278":"The simple model used here is already able to classify the sentiment of text quite accurately, with an accuracy of 87% on the test set. "}}