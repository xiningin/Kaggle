{"cell_type":{"6152485c":"code","809d6cfd":"code","9a3cd19c":"code","38013c32":"code","d51fb7ce":"code","5aaca3f5":"code","7f276951":"code","c4adf293":"code","07a67cec":"code","4d086379":"code","9ef2711f":"code","db71ae99":"code","ece1ad8b":"code","ff643faa":"code","eb134043":"markdown","6f230df9":"markdown","e4a43a33":"markdown","6e8753d0":"markdown","f4383b31":"markdown"},"source":{"6152485c":"import numpy as np \nimport pandas as pd\n\nimport os\nprint(os.listdir(\"..\/input\"))","809d6cfd":"print(os.listdir(\"..\/input\/frame-sample\/frame\"))","9a3cd19c":"# Loading libraries & datasets\nimport tensorflow as tf\nimport numpy as np\nfrom IPython.display import YouTubeVideo\n\nvideo_no1_record = \"..\/input\/frame-sample\/frame\/train00.tfrecord\"\nvalidate_no1_record = \"..\/input\/validate-sample\/validate\/validate00.tfrecord\"\nvideo_no2_record = \"..\/input\/frame-sample\/frame\/train01.tfrecord\"\nvalidate_no2_record = \"..\/input\/validate-sample\/validate\/validate01.tfrecord\"\n\nvid_file_list = [video_no1_record, video_no2_record]\nval_file_list = [validate_no1_record, validate_no2_record]","38013c32":"vid_numb = 0\nfor vid_file in vid_file_list:\n    for video in tf.python_io.tf_record_iterator(vid_file):\n        tf_seq_example = tf.train.SequenceExample.FromString(video)\n        tf_example = tf.train.Example.FromString(video)\n        segment_start_times = tf_example.features.feature['segment_start_times'].int64_list.value\n        segment_end_times = tf_example.features.feature['segment_end_times'].int64_list.value\n        seg_rgb = tf_seq_example.feature_lists.feature_list['rgb'].feature[0].bytes_list.value[0]\n        vid_numb += 1\n        if vid_numb == 20:\n            #print(tf_seq_example)\n            break","d51fb7ce":"print(vid_numb)\nprint(segment_start_times)\nprint(segment_end_times)\nprint(seg_rgb)","5aaca3f5":"for val_file in val_file_list: \n    for example in tf.python_io.tf_record_iterator(val_file):\n        tf_example = tf.train.Example.FromString(example)\n        #print(tf_example)\n        break","7f276951":"val_vid_ids = []\nval_vid_labels = []\nvid_segs_dict = {}\n\nnum_of_videos = 0\nfor val_file in val_file_list: \n    for example in tf.python_io.tf_record_iterator(val_file):\n        tf_example = tf.train.Example.FromString(example)\n        vid_id = tf_example.features.feature['id'].bytes_list.value[0].decode(encoding='UTF-8')\n        val_vid_ids.append(vid_id)\n        val_vid_labels.append(tf_example.features.feature['labels'].int64_list.value)\n\n        seg_info_dict = {}\n\n        segment_start_times = tf_example.features.feature['segment_start_times'].int64_list.value\n        segment_end_times = tf_example.features.feature['segment_end_times'].int64_list.value\n        segment_labels = tf_example.features.feature['segment_labels'].int64_list.value\n        segment_scores = tf_example.features.feature['segment_scores'].float_list.value\n\n        segment_start_times, segment_end_times, segment_labels, segment_scores\\\n            = (list(t) for t in zip(*sorted(zip(segment_start_times, segment_end_times, segment_labels, segment_scores),reverse=True)))\n\n        seg_info_dict['seg_start_times'] = segment_start_times\n        seg_info_dict['seg_end_times'] = segment_end_times\n        seg_info_dict['seg_labels'] = segment_labels\n        seg_info_dict['seg_scores'] = segment_scores\n\n        vid_segs_dict[vid_id] = seg_info_dict\n\n        num_of_videos += 1\n        if num_of_videos == 20:\n            #print(tf_example)\n            break","c4adf293":"print(vid_segs_dict['ww00']['seg_start_times'])","07a67cec":"for test_file in test_files:\n    new_test_filename = test_file[:8] + \"NEW\" + test_file[8:]\n    writer = tf.python_io.TFRecordWriter(new_test_filename)\n    for vid in tf.python_io.tf_record_iterator(test_file):\n        tf_seq_example = tf.train.SequenceExample.FromString(vid)\n        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n        # iterate through frames\n        n_segments = n_frames \/\/ 5\n        \n        tf_example = tf.train.Example.FromString(vid)\n        vid_id = tf_example.features.feature['id']\\\n                       .bytes_list.value[0].decode(encoding='UTF-8')\n        \n        for seg_no in range(n_segments):\n            seg_id = vid_id + \":{}\".format(seg_no*5)\n            seg_id_tf = tf.train.Feature(bytes_list=tf.train.BytesList(value=[seg_id.encode('utf-8')]))\n            \n            sess = tf.InteractiveSession()\n            \n            rgb1 = tf_seq_example.feature_lists.feature_list['rgb'].\\\n                            feature[seg_no*5]\n            rgb2 = tf_seq_example.feature_lists.feature_list['rgb'].\\\n                            feature[seg_no*5 + 1]\n            rgb3 = tf_seq_example.feature_lists.feature_list['rgb'].\\\n                            feature[seg_no*5 + 2]\n            rgb4 = tf_seq_example.feature_lists.feature_list['rgb'].\\\n                            feature[seg_no*5 + 3]\n            rgb5 = tf_seq_example.feature_lists.feature_list['rgb'].\\\n                            feature[seg_no*5 + 4]\n                \n            aud1 = tf_seq_example.feature_lists.feature_list['audio'].\\\n                            feature[seg_no*5]\n            aud2 = tf_seq_example.feature_lists.feature_list['audio'].\\\n                            feature[seg_no*5 + 1]\n            aud3 = tf_seq_example.feature_lists.feature_list['audio'].\\\n                            feature[seg_no*5 +2]\n            aud4 = tf_seq_example.feature_lists.feature_list['audio'].\\\n                            feature[seg_no*5 + 3]\n            aud5 = tf_seq_example.feature_lists.feature_list['audio'].\\\n                            feature[seg_no*5 + 4]\n            \n            sess.close()\n            \n            rgb_list_tf = [rgb1, rgb2, rgb3, rgb4, rgb5]\n            aud_list_tf =[aud1, aud2, aud3, aud4, aud5]\n            \n            rgb = tf.train.FeatureList(feature=rgb_list_tf)\n            audio = tf.train.FeatureList(feature=aud_list_tf)\n            \n            seg_tf_dict = {'rgb': rgb, 'audio': audio}\n            \n            seg_tf_features = tf.train.FeatureLists(feature_list=seg_tf_dict)\n\n            seg_context = tf.train.Features(feature={'id': seg_id_tf})\n            \n            example = tf.train.SequenceExample(context=seg_context, feature_lists=seg_tf_features)\n            \n            writer.write(example.SerializeToString())\n            \n    writer.close()","4d086379":"import time\nstart_t = time.time()","9ef2711f":"print(vid_segs_dict.keys())","db71ae99":"train_vid_ids = []\ntrain_seg_rgb = []\ntrain_seg_aud = []\ntrain_seg_labels = [] #label is not the target. we will train 1000 different nets for each label\ntrain_seg_targets = []\n\nvideo_num = 0\nfor vid_file in vid_file_list:\n    for video in tf.python_io.tf_record_iterator(vid_file):\n\n        video_num += 1\n\n        tf_example = tf.train.Example.FromString(video)\n\n        vid_id = tf_example.features.feature['id']\\\n                       .bytes_list.value[0].decode(encoding='UTF-8')\n\n        if vid_id in vid_segs_dict.keys():\n            vid_segment_start_times = vid_segs_dict[vid_id]['seg_start_times']\n            vid_segment_end_times = vid_segs_dict[vid_id]['seg_end_times']\n            vid_segment_labels = vid_segs_dict[vid_id]['seg_labels']\n            vid_segment_scores = vid_segs_dict[vid_id]['seg_scores']\n            print(video_num)\n        else:\n            continue\n\n        tf_seq_example = tf.train.SequenceExample.FromString(video)\n        n_frames = len(tf_seq_example.feature_lists.feature_list['audio'].feature)\n\n        seg_rgb_record = []\n        seg_audio_record = []\n        recording = \"no\"\n\n        sess = tf.InteractiveSession()\n\n        # iterate through frames\n\n        for i in range(n_frames):\n\n            if i in vid_segment_start_times:\n                vid_segment_start_times.pop()\n                recording = \"yes\"\n                train_seg_labels.append(vid_segment_labels.pop())\n                train_seg_targets.append(vid_segment_scores.pop())\n                train_vid_ids.append(vid_id)\n\n            elif i in vid_segment_end_times:\n                vid_segment_end_times.pop()\n                recording = \"no\"\n                train_seg_rgb.append(seg_rgb_record)\n                train_seg_aud.append(seg_audio_record)\n                seg_rgb_record = []\n                seg_audio_record = []\n\n            if recording == \"yes\":\n                seg_rgb_record.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['rgb']\n                      .feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n                seg_audio_record.append(tf.cast(tf.decode_raw(\n                    tf_seq_example.feature_lists.feature_list['audio']\n                      .feature[i].bytes_list.value[0],tf.uint8)\n                           ,tf.float32).eval())\n\n        sess.close()\n\n        if video_num == 16:\n            break","ece1ad8b":"print(\"total time: \", (time.time() - start_t)) ","ff643faa":"print(video_num)\nprint(len(train_vid_ids))\nprint(len(train_seg_labels))\nprint(len(train_seg_targets))\nprint(len(train_seg_rgb))\nprint(len(train_seg_aud))\nprint(len(train_seg_rgb[0]))\nprint(len(train_seg_aud[0]))\nprint(len(train_seg_rgb[4]))\nprint(len(train_seg_aud[4]))","eb134043":"Why is the validation file 3.5MB with so little info, I cannot understand. Must have some other information in there.","6f230df9":"****This notebook is based on @inversion's Starter Kernel. It extracts the data from TFRecord format using a subsample of the YouTube-8M `frame-level` and `validate` data. \n\nCorrections made over prev version.","e4a43a33":"for vid in tf.python_io.tf_record_iterator(video_no1_record):\n    tf_seq_example = tf.train.SequenceExample.FromString(vid)\n    print(tf_seq_example)\n    break","6e8753d0":"Here is the code to break the TEST set into tfrecords of 5 sec segments. You can just use this to break the test set and then use Github starter code as explained. The dynamic RNN should yield results no matter what video length. I have not tried this myself yet, but the below code works. ","f4383b31":"The starter data they gave us is comprised of only 30 something validation examples and over 2000 videos. Below is a script to find validation data for the video files they gave. Turns out none are there. This is only 0.4% of the entire dataset. The validation file is even smaller, like 0.01% (I don't get why)."}}