{"cell_type":{"fc4b127b":"code","0b19fbfe":"code","b246738e":"code","fe81bee2":"code","a60ae27f":"code","7487c8c1":"code","9d6e9d1b":"code","648d3f06":"code","31d88bb5":"code","02b6004d":"code","f0dea4f9":"code","6cf0b123":"code","2bac162c":"code","16a5b600":"code","08d20f1d":"code","5006f591":"code","c7687032":"code","95393a75":"code","4fa99dc9":"markdown","de89a50c":"markdown","0e4c675a":"markdown","9052a917":"markdown","d3cb97d6":"markdown"},"source":{"fc4b127b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0b19fbfe":"import pandas as pd","b246738e":"data = pd.read_csv('\/kaggle\/input\/trip-advisor-hotel-reviews\/tripadvisor_hotel_reviews.csv')\ndata.head()","fe81bee2":"# change range of rating from 1-5 to 0-4, its easier for the algo\ndata['Rating'] = data['Rating'] - 1","a60ae27f":"import tensorflow as tf","7487c8c1":"dataset = tf.data.Dataset.from_tensor_slices((data.Review, data.Rating))","9d6e9d1b":"for X_batch, y_batch in dataset.batch(2).take(1):\n    for review, label in zip(X_batch.numpy(), y_batch.numpy()):\n        print(\"Review:\", review.decode(\"utf-8\")[:200], \"...\")\n        print(\"Rating\", label)\n        print()","648d3f06":"def preprocess(X_batch, y_batch):\n    X_batch = tf.strings.substr(X_batch, 0, 300)\n    X_batch = tf.strings.regex_replace(X_batch, rb\"<br\\s*\/?>\", b\" \")\n    X_batch = tf.strings.regex_replace(X_batch, b\"[^a-zA-Z']\", b\" \")\n    X_batch = tf.strings.split(X_batch)\n    return X_batch.to_tensor(default_value=b\"<pad>\"), y_batch","31d88bb5":"preprocess(X_batch, y_batch)\n","02b6004d":"from collections import Counter\n\n# get a vocabulary based on the data\nvocabulary = Counter()\nfor X_batch, y_batch in dataset.batch(32).map(preprocess):\n    for review in X_batch:\n        vocabulary.update(list(review.numpy()))","f0dea4f9":"vocabulary.most_common()[:3]\n","6cf0b123":"vocab_size = 10000\ntruncated_vocabulary = [\n    word for word, count in vocabulary.most_common()[:vocab_size]]","2bac162c":"words = tf.constant(truncated_vocabulary)\nword_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)\nvocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)\nnum_oov_buckets = 1000\ntable = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)","16a5b600":"def encode_words(X_batch, y_batch):\n    return table.lookup(X_batch), y_batch\n\ntrain_set = dataset.repeat().batch(32).map(preprocess)\ntrain_set = train_set.map(encode_words).prefetch(1)","08d20f1d":"for X_batch, y_batch in train_set.take(1):\n    print(X_batch)\n    print(y_batch)","5006f591":"from tensorflow import keras\n\nembed_size = 128\nmodel = keras.models.Sequential([\n    keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size,\n                           mask_zero=True, # not shown in the book\n                           input_shape=[None]),\n    keras.layers.GRU(128, return_sequences=True),\n    keras.layers.GRU(128),\n    keras.layers.Dense(5, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\nhistory = model.fit(train_set, steps_per_epoch=len(data) \/\/ 32, epochs=5)","c7687032":"import tensorflow_hub as hub\n\nmodel = keras.Sequential([\n    hub.KerasLayer(\"https:\/\/tfhub.dev\/google\/tf2-preview\/nnlm-en-dim50\/1\",\n                   dtype=tf.string, input_shape=[], output_shape=[50]),\n    keras.layers.Dense(128, activation=\"relu\"),\n    keras.layers.Dense(5, activation=\"softmax\")\n])\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\",\n              metrics=[\"accuracy\"])","95393a75":"import tensorflow_datasets as tfds\n\n\nX_train = data['Review']\ny_train = data['Rating']\nhistory = model.fit(X_train,y_train, epochs=5)","4fa99dc9":"# Rating prediction\nSummary: predict how many stars a rating would get based on the review ","de89a50c":"## Train Model","0e4c675a":"## get and preprocess data\n","9052a917":"### Model 1 (Custom GRU)\n\nmodel one is a non pre trained GRU model that needs a solid amount of preprocessing to be used, but you get good results.","d3cb97d6":"### Model 2 (Pre-trained Model)\n\nThis model doesnt need as much preprocessing but the performance is not as good as the custom model"}}