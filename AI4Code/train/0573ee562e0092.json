{"cell_type":{"1e122b3b":"code","580dd1bf":"code","540e6610":"code","57562f8a":"code","53779785":"code","c85c6935":"code","47159062":"code","edd7752e":"code","f895f114":"code","e199fc65":"code","dcb93494":"code","a593a523":"code","3473a12e":"code","40d72662":"code","1f1770a1":"code","7daa10de":"code","27c993a6":"code","7080904d":"code","d53857b4":"code","14f0bcdd":"code","d844157f":"code","cce118ba":"code","41b75b42":"code","4b40fa14":"code","9bec762b":"code","bf5dcdf6":"code","c75fa846":"code","0db99056":"code","499f339b":"code","09d081cd":"code","a29e7b8a":"code","a6607ceb":"code","ada5f3b0":"code","4e6f7731":"code","33683015":"code","74a5e282":"code","e8f6a914":"code","22a34203":"code","812ed272":"code","05ef66ad":"code","08c5bb89":"code","36788d45":"code","b29fe4dd":"code","ca34cad1":"markdown","086d9175":"markdown","d4765843":"markdown","a0b0cdf5":"markdown","b2b294bd":"markdown","92f5c732":"markdown","0c85aaa5":"markdown","166c31a3":"markdown","c9f87835":"markdown","463ce13c":"markdown","e5ef743c":"markdown","2764197b":"markdown","e2510f88":"markdown","d5c336ef":"markdown"},"source":{"1e122b3b":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","580dd1bf":"df = pd.read_csv(\"..\/input\/walmart\/Walmart_Store_sales.csv\")","540e6610":"df","57562f8a":"df.info()","53779785":"# transfer to Datetime type \ndf.Date = pd.to_datetime(df.Date)","c85c6935":"#  Split **Date** to YY-MM-DD fields seperately for the future time series annlysis \ndf[\"Day\"]= pd.DatetimeIndex(df['Date']).day\ndf['Month'] = pd.DatetimeIndex(df['Date']).month\ndf['Year'] = pd.DatetimeIndex(df['Date']).year","47159062":"df.head()","edd7752e":"# Generate a attribute **Quarter** for the future analysis\ndf[\"Quarter\"] = 0\nQuarter_one = df[df[\"Month\"]<4]\nQuarter_two = df[(df[\"Month\"]>=4)& (df[\"Month\"]<7)]\nQuarter_three = df[(df[\"Month\"]>=7)& (df[\"Month\"]<10)]\nQuarter_four = df[(df[\"Month\"]>=10)& (df[\"Month\"]<=12)]\nprint (df.shape)\nprint (Quarter_one.shape[0]+Quarter_two.shape[0]+Quarter_three.shape[0]+Quarter_four.shape[0])\nQuarter_one.loc[:,'Quarter'] = 1 \nQuarter_two.loc[:,'Quarter'] = 2 \nQuarter_three.loc[:,'Quarter'] = 3\nQuarter_four.loc[:,'Quarter'] = 4\ndf1 = pd.concat([Quarter_one,Quarter_two,Quarter_three,Quarter_four],axis=0).reset_index().drop(columns='index')","f895f114":"df1","e199fc65":"# To determine which store has maximum sales, the dataset will be grouped by store and sum the weekly sales together for each store\nfig, ax = plt.subplots(figsize=(20,10))\nax = df1.groupby(\"Store\")[\"Weekly_Sales\"].sum().sort_values().plot(kind='bar')\nbar = ax.patches[0] # the store has the lowest sale \nax.annotate(\"The store has minimum sales is 33 with ${0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.2, 0.32), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nbar = ax.patches[-1] # the store has the lowest sale \nax.annotate(\"The store has maximum sales is 20 with ${0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.8, 0.95), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nplt.xticks(rotation=0)\n# no sentific reading format for y-axis\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.title('Total Sales for Each Store')\nplt.xlabel('Store')\nplt.ylabel('Total Sales');","dcb93494":"# To find out which store has maximum standard deviation, date is needed to be grouped by store and std aggregation function will be used.\n# Also, find out the coefficient of mean to standard deviation\n","a593a523":"fig, ax = plt.subplots(figsize=(20,10))\nax = df1.groupby(\"Store\")[\"Weekly_Sales\"].std().sort_values().plot(kind=\"bar\")\nbar = ax.patches[0]\nax.annotate(\"Overall sales of store 37 varies least with the minimum std {0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.2, 0.42), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nbar = ax.patches[-1]\nax.annotate(\"Overall sales of store 14 varies most with the maximum std {0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.7, 0.92), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nplt.xticks(rotation=0)\n# no sentific reading format for y-axis\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.title('Total Std in Sales for Each Store')\nplt.xlabel('Store')\nplt.ylabel('Total Std');","3473a12e":"fig, ax = plt.subplots(figsize=(20,10))\n\nax = ((df1.groupby(\"Store\")[\"Weekly_Sales\"].mean())\/(df1.groupby(\"Store\")[\"Weekly_Sales\"].std())).sort_values().plot(kind='bar')\nbar = ax.patches[0]\nax.annotate(\"Store 35 has the minimum coefficient which is {0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.2, 0.42), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nbar = ax.patches[-1]\nax.annotate(\"Store 37 has the maximum coefficient which is {0:.2f} \".format((bar.get_height())),\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.7, 0.92), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')\n\nplt.xticks(rotation=0)\n# no sentific reading format for y-axis\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.title('Coefficient of Mean to STD for Each Store')\nplt.xlabel('Store')\nplt.ylabel('Coefficient');","40d72662":"# To find out which store\/s has good quarterly growth rate in Q3\u20192012, we need to compare each store sales in Q2'2012 and Q3'2012","1f1770a1":"fig, ax = plt.subplots(figsize=(20,10))\nax = df1[(df1['Year']==2012)&(df1['Quarter']==3)].groupby('Store')['Weekly_Sales'].sum().plot(\n    ax = df1[(df1['Year']==2012)&(df1['Quarter']==2)].groupby('Store')['Weekly_Sales'].sum().plot(kind=\"bar\",color='r',alpha=0.5,label = \"Q2'2012\"),\n    kind=\"bar\",\n    color = \"g\",\n    alpha=0.5,\n    legend=True,\n    label=\"Q3'2012\")\nax.legend();\n\nbar = ax.patches[15]\nax.annotate(\"Store 16 has the best quarterly growth rate in Q3\u20192012 \",\n            xy=(bar.get_x(), bar.get_height()), xycoords='data',\n            xytext=(0.4, 0.98), textcoords='axes fraction',\n            arrowprops=dict(arrowstyle=\"->\", connectionstyle=\"arc3\"),\n            horizontalalignment='center', verticalalignment='center')","7daa10de":"Q3 = df1[(df1['Year']==2012)&(df1['Quarter']==3)].groupby('Store')['Weekly_Sales'].sum()   # 6441311.11\nQ2 = df1[(df1['Year']==2012)&(df1['Quarter']==2)].groupby('Store')['Weekly_Sales'].sum()   # 6626133.44\ngrow_rate = (Q3-Q2)\/Q3\n\nfig, ax = plt.subplots(figsize=(20,5))\nax = grow_rate.sort_values(ascending=False).plot(kind='bar')\n\nplt.xticks(rotation=0)\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.title('Growth rate in Q3\u20192012')\nplt.xlabel('Store')\nplt.ylabel('Growth rate');","27c993a6":"# Some holidays have a negative impact on sales.\n# To find out holidays which have higher sales than the mean sales in non-holiday season for all stores together,\n# we need to plot aggregate the weekly sales for all the stores and diaplay it in time series line chart.\n# Then the week of holiday is needed to be highlighted to visually check if the holiday has negative impact on sales or not.","7080904d":"from datetime import datetime\n\ndef show_holidays(ax, events, col,title):\n    for i in events:\n        day = datetime.strptime(i, '%d-%m-%Y')\n        ax.axvline(x=day, linestyle='--', c=col)\n    ax.set_title(title, fontsize=\"xx-large\")\n\n\nThanksgiving = ['26-11-2010', '25-11-2011', '23-11-2012']\nLabour_Day =  ['10-9-2010', '9-9-2011', '7-9-2012']\nSuper_Bowl =['12-2-2010', '11-2-2011', '10-2-2012']\nChristmas = ['31-12-2010', '30-12-2011', '28-12-2012']\n\nfig, ax = plt.subplots(4,figsize=(30,15))\nax[0].plot(df1.groupby('Date')['Weekly_Sales'].sum())\nax[1].plot(df1.groupby('Date')['Weekly_Sales'].sum())\nax[2].plot(df1.groupby('Date')['Weekly_Sales'].sum())\nax[3].plot(df1.groupby('Date')['Weekly_Sales'].sum())\n\nshow_holidays(ax[0],Thanksgiving,'r','Thanksgivng')\nshow_holidays(ax[1],Labour_Day,'r','Labour_Day')\nshow_holidays(ax[2],Super_Bowl,'r','Super_Bowl')\nshow_holidays(ax[3],Christmas,'r','Christmas')\n\n","d53857b4":"# Provide a monthly and semester view of sales in units and give insights","14f0bcdd":"kk = df1.groupby([\"Store\"])","d844157f":"fig, ax = plt.subplots(figsize=(20, 20))\nimport matplotlib.dates as mdates\n\nfor i in range(1,46):\n    temp = kk.get_group(i).groupby(['Year','Month'])['Weekly_Sales'].sum().reset_index()\n    ax.plot(temp['Year'].map(str)+ '-M' +temp['Month'].map(str),temp['Weekly_Sales'],label=\"Store \"+str(i))\nax.legend()\nax.set_title(\"Monthly Sales for each Store\")\nplt.xticks(rotation=90)\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')","cce118ba":"fig, ax = plt.subplots(figsize=(20, 20))\nimport matplotlib.dates as mdates\nfor i in range(1,46):\n    temp = kk.get_group(i).groupby(['Year','Quarter'])['Weekly_Sales'].sum().reset_index()\n    ax.plot(temp['Year'].map(str)+ '-Q' +temp['Quarter'].map(str),temp['Weekly_Sales'],label=\"Store \"+str(i))\nax.legend()\nax.set_title(\"Quarterly Sales for each Store\")\nplt.xticks(rotation=0)\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')","41b75b42":"fig, ax = plt.subplots(2,figsize=(20,10))\nmonth = df1.groupby('Month')['Weekly_Sales'].mean().reset_index()\nax[0].bar(month['Month'],month['Weekly_Sales'])\nquarter = df1.groupby('Quarter')['Weekly_Sales'].mean().reset_index()\nax[1].bar(quarter['Quarter'],quarter['Weekly_Sales'])\nax[0].ticklabel_format(useOffset=False, style='plain', axis='y')\nax[1].ticklabel_format(useOffset=False, style='plain', axis='y')\nax[0].set_title(\"Average Monthly Sales for all Stores\")\nax[1].set_title(\"Average Quarterly Sales for all Stores\")","4b40fa14":"df1.info()","9bec762b":"# To build a linear regression model, first determine\/remove outliers and then do the select the features.\nfig, axs = plt.subplots()\nsns.boxplot(df1['Fuel_Price'])\nfig, axs = plt.subplots()\nsns.boxplot(df1['Unemployment'])\nfig, axs = plt.subplots()\nsns.boxplot(df1['CPI'])\nfig, axs = plt.subplots()\nsns.boxplot(df1['Temperature'])","bf5dcdf6":"# Remove the outliers detected from Temperature and Unemployment\ndf2 = df1[(df1['Unemployment']<10.5) & (df1['Unemployment']>4.5) & (df1['Temperature']>8)]\nfig, axs = plt.subplots()\nsns.boxplot(df2['Fuel_Price'])\nfig, axs = plt.subplots()\nsns.boxplot(df2['Unemployment'])\nfig, axs = plt.subplots()\nsns.boxplot(df2['CPI'])\nfig, axs = plt.subplots()\nsns.boxplot(df2['Temperature'])","c75fa846":"sns.set(style=\"white\")\ncorr = df.corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","0db99056":"df3 = pd.DataFrame()\ndf3['Store'] = df2['Store']\ndf3['Fuel_Price'] = df2['Fuel_Price']\ndf3['Unemployment'] = df2['Unemployment']\ndf3['CPI'] = df2['CPI']\ndf3['Temperature'] = df2['Temperature']\ndf3['Holiday_Flag'] = df2['Holiday_Flag'].astype(\"category\")\ndf3['Month'] = df2['Month'].astype(\"category\")\ndf3['Year'] = df2['Year'].astype(\"category\").cat.codes\ndf3['Weekly_Sales'] = df2['Weekly_Sales']\n","499f339b":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.linear_model import LinearRegression\n\n\nX = df3[['Store','Fuel_Price','CPI','Unemployment','Holiday_Flag','Temperature','Month']]\ny = df3['Weekly_Sales']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n\nprint (\"Linear Regressor:\")\nprint()\n\nreg = LinearRegression()\nreg.fit(X_train, y_train)\ny_pred = reg.predict(X_test)\nprint('Accuracy:',reg.score(X_train, y_train)*100)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\nsns.scatterplot(y_pred, y_test);\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.ticklabel_format(useOffset=False, style='plain', axis='x')","09d081cd":"# Random Forest Regressor\nprint('Random Forest Regressor:')\nprint()\nrfr = RandomForestRegressor()        \nrfr.fit(X_train,y_train)\ny_pred=rfr.predict(X_test)\n\n\nprint('Accuracy:',rfr.score(X_train, y_train)*100)\nprint('Accuracy:',rfr.score(X_test, y_test)*100)\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))\n\nsns.scatterplot(y_pred, y_test);\nplt.ticklabel_format(useOffset=False, style='plain', axis='y')\nplt.ticklabel_format(useOffset=False, style='plain', axis='x')","a29e7b8a":"# Arima time series model building \n# let choose one store first \n# check the stationarity\nimport statsmodels.api as sm\nfrom statsmodels.tsa.seasonal import seasonal_decompose\n# import statsmodels.formula.api as smf\n# import statsmodels.tsa.api as smt","a6607ceb":"fig, ax = plt.subplots(figsize=(20,10))\nstore_one = df1[df1['Store']==1][['Date','Weekly_Sales']].sort_values(by=\"Date\",).set_index('Date')\nax.plot(store_one['Weekly_Sales'])\nax.set_title(\"Weekly Sales of Store One\", fontsize=\"large\")\nplt.ticklabel_format(style='plain',axis='y')\n","ada5f3b0":"result = seasonal_decompose(store_one, model='additive', period=12)\nfig, ax = plt.subplots(3,figsize=(20,15))\nresult.trend.plot(ax=ax[0])\nresult.resid.plot(ax=ax[1])\nresult.seasonal.plot(ax=ax[2])\nfig.subplots_adjust(hspace = 0.8)\nax[0].set_title(\"Trend\", fontsize=\"large\")\nax[1].set_title(\"Resid\",fontsize=\"large\")\nax[2].set_title(\"Seasonal\",fontsize=\"large\")\nax[0].ticklabel_format(style=\"plain\",axis=\"y\")","4e6f7731":"# differecing \nfig, ax = plt.subplots(2)\n\nstore_one['Weekly_Sales'].diff(1).plot(ax= ax[0],figsize=(15,10))\nstore_one['Weekly_Sales'].diff(1).diff(1).plot(ax= ax[1],figsize=(15,10))\nax[0].set_title(\"D = 1\",fontsize=\"large\")\nax[1].set_title(\"D = 2\",fontsize=\"large\")\nax[0].ticklabel_format(style='plain',axis='y')\nax[1].ticklabel_format(style='plain',axis='y')\n\nfig.subplots_adjust(hspace = 0.8)","33683015":"ax1 = sm.graphics.tsa.plot_acf(store_one['Weekly_Sales'],lags=50)\nax2 = sm.graphics.tsa.plot_pacf(store_one['Weekly_Sales'],lags=50)","74a5e282":"!pip install pmdarima","e8f6a914":"from pmdarima.arima import auto_arima","22a34203":"stepwise_model = auto_arima(store_one, start_p=1, d=2, start_q=1,\n                           max_p=5, max_q=5, m=12,\n                           start_P=0, seasonal=True,\n                           D=1, trace=True,\n                           error_action='ignore',  \n                           suppress_warnings=True)","812ed272":"train = store_one.loc[:'2012-05-01']\ntest = store_one.loc['2012-05-01':]\nstepwise_model.fit(train)","05ef66ad":"future_forecast = stepwise_model.predict(n_periods=len(test))","08c5bb89":"future_forecast = pd.DataFrame(future_forecast,index = test.index,columns=['Prediction'])\npd.concat([test,future_forecast],axis=1).plot()","36788d45":"pd.concat([store_one,future_forecast],axis=1).plot(figsize=(15, 6),\n                                             title=\"Walmart Sales in One Department of One Store Feb 2010 to Oct 2012\")","b29fe4dd":"result = stepwise_model.plot_diagnostics(figsize=(15,8))","ca34cad1":"# Dataset Description\n\nHistorical sales date of Walmart 45 stores located in different regions are available for analysis and model building.\nThe historical data covers sales from 2010-02-05 to 2012-11-01 and includes the attributes as follows:\n\n* Store - the store number\n* Date - the week of sales\n* Weekly_Sales -  sales for the given store\n* Holiday_Flag - whether the week is a special holiday week 1 \u2013 Holiday week 0 \u2013 Non-holiday week\n* Temperature - Temperature on the day of sale\n* Fuel_Price - Cost of fuel in the region\n* CPI \u2013 Prevailing consumer price index\n* Unemployment - Prevailing unemployment rate\n\n\n\n\n**Holiday Events:**\n* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labour Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\n\n*Note: Walmart runs several promotional events throughout the year. These events precede prominent holidays, the four largest of all, which are the Super Bowl, Labour Day, Thanksgiving, and Christmas.*","086d9175":"After checking the data field types, there are several data wrangling tasks needed to be conducted \n1. **Date** - transfer to **Datetime** type \n2. Split **Date** to YY-MM-DD fields seperately for the future time series annlysis \n3. Generate a attribute **Quarter** for the future analysis","d4765843":"**Insight Three: Thanksgiving has positive impact on sales as we can seen from the graph that the weekly sales during the Thanksgiving weeks are higher the other weeks. \nUnlike Thanksgiving, Christmas seems cause a negative impact on sales. Super Bowl and Labour Day have relatively neutral impacts on sales since there are no apprent ups and downs.**","a0b0cdf5":"* **Insight Four: as can be seen from the plots of Monthly and Quarterly sales of each store, each store has different sales but they all have similar fluctuation trends.**\n* **Insight Five: as can be seen from the plots of Monthly and Quarterly sales of all sotres, Quater 4 tends to have the higher sales especially in Decemeber. Quarter 1 hs the lowest sales especially in January.**","b2b294bd":"# Background Introduction\n\nOne of the leading retail stores in the world, Walmart, would like to predict the ***sales*** and ***demands*** accurately. \nThe business is facing challenges due to unforeseen demands and runs out of stock some times, as the fact that there are certain events and holidays whihc impact sales to some extents. The current system fails to forecast the demand and sales appropriately. There is a need to develop an appropriate machine learning algorithm to predict demand accurately and ingest factors such as economic conditions including CPI, Unemployment Index, etc.\n\n","92f5c732":"**Insight One:** **As can be seen from the above plot, store 20 has the maximum sales among all stores which is almost 10 times higher then the store 33 which has the minimum sales.**","0c85aaa5":"# Data Modeling\n* Build prediction models to forecast demand\n* Linear Regression \u2013 Utilize variables like date and restructure dates as 1 for 5 Feb 2010 (starting from the earliest date in order). Hypothesize if CPI, unemployment, and fuel price have any impact on sales.\n* Select the model which gives best accuracy.","166c31a3":"# Retail Analysis with Walmart Data\n\n**Procedures:**\n* Background Introduction\n* Dataset Description\n* Objectives of Analysis\n* Analysis Tasks to be performed","c9f87835":"# Data Wrangling","463ce13c":"# Data Exploration ","e5ef743c":"**Insight Six:**\n* Fuel_Price and Year are positively correlated.\n* Weekly_Sales and Store are negatively correalted.\n* Holiday_Flag and Month are positvely correlated.\n* Unemployment and CPI are negatively correlated.\n\nSelected Feature: Store, Fuel_Price, CPI, Unemployment, Holiday_Flag, Temperature, Month, Year\nResponse Variable: Weekly_Sales","2764197b":"# Objectives of Analysis\n\n* Leveraging the available data to perform extensive analysis mine the insights and patterns.\n* Develop a machine learning algorithm to predict the future demand and sales.\n\n","e2510f88":"**Insight Two: from the plot above, we can see all the store had experieced the decline in sale at Q3'2012 to some extenteds. Store 16 had the highest negative growth rate and store 14 had the lowest negative growth rate comparing with the privious quarter Q2'2012.**","d5c336ef":"#  Analysis Tasks to be Performed\n\n* data wrangling \n* data exploration \n* data modeling \n\n**Basic Statistics tasks**\n1. Which store has ***maximum*** sales\n2. Which store has ***maximum standard deviation*** i.e., the sales vary a lot. Also, find out the ***coefficient of mean to standard deviation***\n3. Which store\/s has good quarterly growth rate in Q3\u20192012\n4. Some holidays have a negative impact on sales. Find out holidays which have higher sales than the mean sales in non-holiday season for all stores together\n5. Provide a monthly and semester view of sales in units and give insights\n\n**Statistical Model**\n1. Build  prediction models to forecast demand\n\n2. Linear Regression \u2013 Utilize variables like date and restructure dates as 1 for 5 Feb 2010 (starting from the earliest date in order). Hypothesize if CPI, unemployment, and fuel price have any impact on sales.\n\n3. Select the model which gives best accuracy."}}