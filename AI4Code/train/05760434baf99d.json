{"cell_type":{"cc6f539b":"code","c91ce271":"code","665c018d":"code","f8dcd0a2":"code","ddce3f55":"code","bcb287c3":"code","e9688b44":"code","c1153cc9":"code","57c3235e":"code","8475a880":"code","969d49fd":"code","7c680f42":"code","6fb7358c":"code","beb19210":"code","d97328c1":"code","c5f618e7":"code","511a8930":"code","083a91e8":"code","b88f535e":"code","bcdb5243":"markdown","479e86b6":"markdown","5f2a2a39":"markdown","3021aa3c":"markdown","da3e689c":"markdown","8517211a":"markdown","b80d8875":"markdown","da019573":"markdown","c4ffdd12":"markdown","5fc93fe3":"markdown"},"source":{"cc6f539b":"!pip -q install ..\/input\/pytorchtabnet\/pytorch_tabnet-3.1.1-py3-none-any.whl\n!pip -q install ..\/input\/talib-binary\/talib_binary-0.4.19-cp37-cp37m-manylinux1_x86_64.whl","c91ce271":"import os\nimport gc\nimport joblib\nimport random\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nfrom argparse import Namespace\nfrom collections import defaultdict\n\n\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow import keras\nfrom scipy import stats\n\nimport matplotlib.pyplot as plt\nfrom scipy.stats import pearsonr\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.model_selection import TimeSeriesSplit, StratifiedKFold, train_test_split\n\nfrom pytorch_tabnet.metrics import Metric\nfrom pytorch_tabnet.tab_model import TabNetRegressor\nfrom pytorch_tabnet.pretraining import TabNetPretrainer\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('max_columns', 64)\n\ndef seed_everything(seed: int = 42) -> None:\n    random.seed(seed)\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    ","665c018d":"args = Namespace(\n    INFER=True,\n    debug=False,\n    seed=21,\n    folds=5,\n    workers=4,\n    min_time_id=None, \n    holdout=True,\n    num_bins=16,\n    data_path=Path(\"..\/input\/ubiquant-market-prediction-half-precision-pickle\"),\n    dnn_path = '..\/input\/dnnmodel',\n    tabnet_path = '..\/input\/ubiquanttabnetbaseline'\n)\nseed_everything(args.seed)\n\nif args.debug:\n    setattr(args, 'min_time_id', 1100)\n","f8dcd0a2":"%%time\nn_features = 300\nfeatures = [f'f_{i}' for i in range(n_features)]\ntrain = pd.read_pickle(f'{str(args.data_path)}\/train.pkl')\ntrain.head()","ddce3f55":"investment_id = train['investment_id']\ninvestment_id.head()","bcb287c3":"y = train['target']\ny.head()","e9688b44":"%%time\ninvestment_ids = list(investment_id.unique())\ninvestment_id_size = len(investment_ids) + 1\ninvestment_id_lookup_layer = layers.IntegerLookup(max_tokens=investment_id_size)\ninvestment_id_lookup_layer.adapt(pd.DataFrame({\"investment_ids\":investment_ids}))","c1153cc9":"import tensorflow as tf\ndef preprocess(X, y):\n    return X, y\ndef make_dataset(feature, investment_id, y, batch_size=1024, mode=\"train\"):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature), y))\n    ds = ds.map(preprocess)\n    if mode == \"train\":\n        ds = ds.shuffle(4096)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds","57c3235e":"def get_model():\n    investment_id_inputs = tf.keras.Input((1, ), dtype=tf.uint16)\n    features_inputs = tf.keras.Input((300, ), dtype=tf.float16)\n    \n    investment_id_x = investment_id_lookup_layer(investment_id_inputs)\n    investment_id_x = layers.Embedding(investment_id_size, 32, input_length=1)(investment_id_x)\n    investment_id_x = layers.Reshape((-1, ))(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    investment_id_x = layers.Dense(64, activation='swish')(investment_id_x)\n    \n    feature_x = layers.Dense(256, activation='swish')(features_inputs)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    feature_x = layers.Dense(256, activation='swish')(feature_x)\n    \n    x = layers.Concatenate(axis=1)([investment_id_x, feature_x])\n    x = layers.Dense(512, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(128, activation='swish', kernel_regularizer=\"l2\")(x)\n    x = layers.Dense(32, activation='swish', kernel_regularizer=\"l2\")(x)\n    output = layers.Dense(1)(x)\n    rmse = keras.metrics.RootMeanSquaredError(name=\"rmse\")\n    model = tf.keras.Model(inputs=[investment_id_inputs, features_inputs], outputs=[output])\n    model.compile(optimizer=tf.optimizers.Adam(0.001), loss='mse', metrics=['mse', \"mae\", \"mape\", rmse])\n    return model","8475a880":"%%time\nfrom sklearn.model_selection import StratifiedKFold\nkfold = StratifiedKFold(5, shuffle=True, random_state=42)\nmodels_dnn = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, investment_id)):\n    #if args.INFER == False:\n    X_train, X_val = train[features].iloc[train_indices], train[features].iloc[valid_indices]\n    investment_id_train = investment_id[train_indices]\n    y_train, y_val = y.iloc[train_indices], y.iloc[valid_indices]\n    investment_id_val = investment_id[valid_indices]\n    if args.INFER == False:\n        train_ds = make_dataset(X_train, investment_id_train, y_train)\n    valid_ds = make_dataset(X_val, investment_id_val, y_val, mode=\"valid\")\n    model = get_model()\n    checkpoint = keras.callbacks.ModelCheckpoint(f\"{args.dnn_path}\/model_{index}.tf\", save_best_only=True)\n    early_stop = keras.callbacks.EarlyStopping(patience=10)\n    if args.INFER == False:\n        history = model.fit(train_ds, epochs=30, validation_data=valid_ds, callbacks=[checkpoint, early_stop])\n    model.load_weights(f\"{args.dnn_path}\/model_{index}.tf\")\n    models_dnn.append(model)\n    \n    pearson_score = stats.pearsonr(model.predict(valid_ds).ravel(), y_val.values)[0]\n    print('Pearson:', pearson_score)\n    if args.INFER == False:\n        pd.DataFrame(history.history, columns=[\"mse\", \"val_mse\"]).plot()\n        plt.title(\"MSE\")\n        plt.show()\n        pd.DataFrame(history.history, columns=[\"mae\", \"val_mae\"]).plot()\n        plt.title(\"MAE\")\n        plt.show()\n        pd.DataFrame(history.history, columns=[\"rmse\", \"val_rmse\"]).plot()\n        plt.title(\"RMSE\")\n        plt.show()\n        \n    del investment_id_train\n    del investment_id_val\n    del X_train\n    del X_val\n    del y_train\n    del y_val\n    if args.INFER == False:\n        del train_ds\n    del valid_ds\n    gc.collect()\n    break","969d49fd":"time_id_df = (\n    train.filter(regex=r\"^(?!f_).*\")\n    .groupby(\"investment_id\")\n    .agg({\"time_id\": [\"min\", \"max\"]})\n    .reset_index()\n)\ntime_id_df[\"time_span\"] = time_id_df[\"time_id\"].diff(axis=1)[\"max\"]\ntime_id_df.head(6)","7c680f42":"train = train.merge(time_id_df.drop(columns=\"time_id\").droplevel(level=1, axis=1), on=\"investment_id\")\ntrain.time_span.hist(bins=args.num_bins, figsize=(16,8))\ndel time_id_df\ngc.collect()","6fb7358c":"train[\"fold\"] = -1\n_target = pd.cut(train.time_span, args.num_bins, labels=False)\nskf = StratifiedKFold(n_splits=args.folds)\nfor fold, (train_index, valid_index) in enumerate(skf.split(_target, _target)):\n    train.loc[valid_index, 'fold'] = fold\n    \nfig, axs = plt.subplots(nrows=args.folds, ncols=1, sharex=True, figsize=(16,8), tight_layout=True)\nfor ax, (fold, df) in zip(axs, train[[\"fold\", \"time_span\"]].groupby(\"fold\")):\n    ax.hist(df.time_span, bins=args.num_bins)\n    ax.text(0, 40000, f\"fold: {fold}, count: {len(df)}\", fontsize=16)\nplt.show()\ndel _target, train_index, valid_index\n_=gc.collect()","beb19210":"X = train.drop(['target', 'time_id'], axis = 1)\ny = train['target']","d97328c1":"num_features = [f\"f_{i}\" for i in range(300)]\ncat_features = [\"investment_id\"]\nfeatures = num_features + cat_features\nfeatures += [\"time_id\"]","c5f618e7":"from sklearn.preprocessing import MinMaxScaler, LabelEncoder, StandardScaler\nimport torch\nfrom torch.optim import Adam, SGD\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau, CosineAnnealingWarmRestarts\nfrom sklearn.metrics import mean_squared_error\n\ndef rmse(y_true, y_pred):\n    return mean_squared_error(y_true,y_pred, squared=False)\ndef rmspe(y_true, y_pred):\n    # Function to calculate the root mean squared percentage error\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\nclass RMSPE(Metric):\n    def __init__(self):\n        self._name = \"rmspe\"\n        self._maximize = False\n\n    def __call__(self, y_true, y_score):\n        \n        return np.sqrt(np.mean(np.square((y_true - y_score) \/ y_true)))\n    \n\ndef RMSPELoss(y_pred, y_true):\n    return torch.sqrt(torch.mean( ((y_true - y_pred) \/ y_true) ** 2 )).clone()\n\n\n\ncat_idxs = [ i for i, f in enumerate(X.columns.tolist()) if f in cat_features]\n\n\ndef run():    \n    tabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n    y = train['target']\n    train['preds'] = -1000\n    scores = defaultdict(list)\n    features_importance= pd.DataFrame()\n    \n    for fold in range(args.folds):\n        print(f\"=====================fold: {fold}=====================\")\n        trn_ind, val_ind = train.fold!=fold, train.fold==fold\n        print(f\"train length: {trn_ind.sum()}, valid length: {val_ind.sum()}\")\n        X_train=train.loc[trn_ind, features].values\n        y_train=y.loc[trn_ind].values.reshape(-1,1)\n        X_val=train.loc[val_ind, features].values\n        y_val=y.loc[val_ind].values.reshape(-1,1)\n\n        clf =  TabNetRegressor(**tabnet_params)\n        clf.fit(\n          X_train, y_train,\n          eval_set=[(X_val, y_val)],\n          max_epochs = 355,\n          patience = 50,\n          batch_size = 1024*20, \n          virtual_batch_size = 128*20,\n          num_workers = 4,\n          drop_last = False,\n\n          )\n        \n        clf.save_model(f'TabNet_seed{args.seed}_{fold}')\n\n\n        preds = clf.predict(train.loc[val_ind, features].values)\n        train.loc[val_ind, \"preds\"] = preds\n        \n        scores[\"rmse\"].append(rmse(y.loc[val_ind], preds))\n     \n        del X_train,X_val,y_train,y_val\n        gc.collect()\n        \n        \n    print(f\"TabNet {args.folds} folds mean rmse: {np.mean(scores['rmse'])}\")\n    train.filter(regex=r\"^(?!f_).*\").to_csv(\"preds.csv\", index=False)\n #   return features_importance\nif args.INFER:\n    pass\nelse:\n    run()  \n#del df, train\ngc.collect()","511a8930":"import os\nimport zipfile\n \ndef zipDir(dirpath, outFullName):\n\n    zip = zipfile.ZipFile(outFullName, \"w\", zipfile.ZIP_DEFLATED)\n    for path, dirnames, filenames in os.walk(dirpath):\n\n        fpath = path.replace(dirpath, '')\n\n        for filename in filenames:\n            zip.write(os.path.join(path, filename), os.path.join(fpath, filename))\n    zip.close()\n    \n\nif args.INFER:\n    for fold in range(5):\n        input_path =f'{args.tabnet_path}\/fold{fold}'\n        output_path = f\".\/fold{fold}.zip\"\n        zipDir(input_path, output_path)\nelse:\n    input_path =f'.\/TabNet_seed{args.seed}_{fold}'\n    output_path = f\".\/fold{fold}.zip\"\n\n    zipDir(input_path, output_path)\ntabnet_params = dict(\n        cat_idxs=cat_idxs,\n        cat_emb_dim=1,\n        n_d = 16,\n        n_a = 16,\n        n_steps = 2,\n        gamma =1.4690246460970766,\n        n_independent = 9,\n        n_shared = 4,\n        lambda_sparse = 0,\n        optimizer_fn = Adam,\n        optimizer_params = dict(lr = (0.024907164557092944)),\n        mask_type = \"entmax\",\n        scheduler_params = dict(T_0=200, T_mult=1, eta_min=1e-4, last_epoch=-1, verbose=False),\n        scheduler_fn = CosineAnnealingWarmRestarts,\n        seed = 42,\n        verbose = 10, \n    )    \n\n\n\nimport copy\nclf =  TabNetRegressor(**tabnet_params)\nmodels_tabnet = []\nfor fold in range(args.folds):\n    clf.load_model(f\"fold{fold}.zip\")\n    model=copy.deepcopy(clf)\n    models_tabnet.append(model)","083a91e8":"def preprocess_test(investment_id, feature):\n    return (investment_id, feature), 0\n\ndef make_test_dataset(feature, investment_id, batch_size=1024):\n    ds = tf.data.Dataset.from_tensor_slices(((investment_id, feature)))\n    ds = ds.map(preprocess_test)\n    ds = ds.batch(batch_size).cache().prefetch(tf.data.experimental.AUTOTUNE)\n    return ds\n\ndef make_test_dataset_lgbm(test_df,folds=5):\n    features = [f\"f_{i}\" for i in range(300)]\n    test_df[features] = scaler.fit_transform(test_df[features]) \n    clu = [kmodels[fold].predict(test_df[features]) for fold in range(folds)]\n    test_df_l = [test_df for fold in range(folds)]\n    for f in range(folds):\n        test_df_l[f]['cluster'] = clu[f]\n    return test_df_l\n\ndef inference(models, ds):\n    y_preds = []\n    for model in models:\n        y_pred = model.predict(ds)\n        y_preds.append(y_pred)\n    return np.mean(y_preds, axis=0)\n\ndef inference_lgbm(models,ds,folds=5):\n    features = [f\"f_{i}\" for i in range(300)]\n    features_1 = features + ['cluster']\n    final_pred = [models[fold].predict(ds[fold][features_1]) for fold in range(folds)]\n    return np.mean(np.stack(final_pred), axis=0)\n\ndef inference_tabnet(models,test_df,args):\n    num_features = [f\"f_{i}\" for i in range(300)]\n    cat_features = [\"investment_id\"]\n    features = num_features + cat_features\n    features += [\"time_id\"]\n    test_df[\"time_id\"] = test_df.row_id.str.extract(r\"(\\d+)_.*\").astype(np.uint16) # extract time_id form row_id\n    final_pred = [models[fold].predict(test_df[features].values) for fold in range(args.folds)]\n    return np.mean(np.stack(final_pred), axis=0)\n","b88f535e":"import ubiquant\nenv = ubiquant.make_env()\niter_test = env.iter_test() \nfor (test_df, sample_prediction_df) in iter_test:\n\n    features_dnn = [f'f_{i}' for i in range(300)]\n    ds = make_test_dataset(test_df[features_dnn], test_df[\"investment_id\"])\n\n    tabnet_output = inference_tabnet(models_tabnet,test_df,args)\n    dnn_output = inference(models_dnn, ds)\n    final_output = dnn_output * 0.4 + tabnet_output *0.6\n    #final_output = tabnet_output\n    sample_prediction_df['target'] = final_output\n    env.predict(sample_prediction_df) ","bcdb5243":"### Train Model","479e86b6":"## Import Dataset","5f2a2a39":"## Prepare Test Inference","3021aa3c":"## Make Tensorflow Dataset","da3e689c":"## DNN Model","8517211a":"## Merge Time ID DF","b80d8875":"## Create IntegerLookup Later for investment_id input","da019573":"## Create Time ID DF","c4ffdd12":"# Model Ensemble DNN + TabNet\n- TabNet: https:\/\/www.kaggle.com\/wangqihanginthesky\/baseline-tabnet\/notebook\n- DNN: https:\/\/www.kaggle.com\/andrej0marinchenko\/ubiquant-market-prediction-dnn","5fc93fe3":"## TabNet"}}