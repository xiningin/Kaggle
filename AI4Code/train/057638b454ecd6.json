{"cell_type":{"8d7e3366":"code","4114d304":"code","a20acc40":"code","7a49d019":"code","db202933":"code","02078903":"code","04ed82a8":"code","c4018c42":"code","17f5190e":"code","391b35b6":"code","1cd48245":"code","42b66043":"code","3d49c3a9":"code","345e564a":"code","6fbc6b84":"code","a6d8aa1e":"code","3d8b06f1":"code","c58f07a6":"code","6d1e255d":"code","a46402df":"code","6967b21e":"code","ec0a3675":"code","eb82996d":"code","e5f53d0b":"code","ee0acbdc":"markdown","72821031":"markdown","041d2f38":"markdown","65792a9c":"markdown","5fe44ed1":"markdown","1d6e06ef":"markdown","fbc1f62b":"markdown","2eea09db":"markdown","2bbec864":"markdown","cd6b44d3":"markdown","93bf474c":"markdown","6e2e1d81":"markdown","4392cf29":"markdown","763f0b46":"markdown","1a2d06cc":"markdown","ec4db511":"markdown"},"source":{"8d7e3366":"import numpy as np \nimport pandas as pd \n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4114d304":"from scipy.sparse import csr_matrix\nfrom sklearn.neighbors import NearestNeighbors\nimport matplotlib.pyplot as plt\nimport seaborn as sns","a20acc40":"movies = pd.read_csv(\"\/kaggle\/input\/movie-lens-small-latest-dataset\/movies.csv\")\nratings = pd.read_csv(\"\/kaggle\/input\/movie-lens-small-latest-dataset\/ratings.csv\")","7a49d019":"movies.head()","db202933":"ratings.head()","02078903":"final_dataset = ratings.pivot(index='movieId', columns='userId', values='rating')\nfinal_dataset.head()","04ed82a8":"final_dataset.fillna(0, inplace=True)\nfinal_dataset.head()","c4018c42":"no_user_voted = ratings.groupby('movieId')['rating'].agg('count')\nno_movies_voted = ratings.groupby('userId')['rating'].agg('count')","17f5190e":"f, ax = plt.subplots(1,1, figsize=(16,4))\n\n# ratings['rating'].plot(kind='hist')\nplt.scatter(no_user_voted.index, no_user_voted, color='mediumseagreen')\nplt.axhline(y=10, color='r')\nplt.xlabel('MovieId')\nplt.ylabel('No. of users voted')\nplt.show()","391b35b6":"final_dataset = final_dataset.loc[no_user_voted[no_user_voted > 10].index,:]","1cd48245":"f, ax = plt.subplots(1,1, figsize=(16,4))\nplt.scatter(no_movies_voted.index, no_movies_voted, color='mediumseagreen')\nplt.axhline(y=50, color='r')\nplt.xlabel('UserId')\nplt.ylabel('No. of votes by user')\nplt.show()","42b66043":"final_dataset = final_dataset.loc[:, no_movies_voted[no_movies_voted >50].index]\nfinal_dataset.head()","3d49c3a9":"sample = np.array([[0,0,3,0,0], [4,0,0,0,2], [0,0,0,0,1]])\nsparsity = 1.0 - (np.count_nonzero(sample) \/ float(sample.size))\nprint(sparsity)","345e564a":"csr_sample = csr_matrix(sample)\nprint(csr_sample)","6fbc6b84":"csr_data = csr_matrix(final_dataset.values)\nfinal_dataset.reset_index(inplace=True)","a6d8aa1e":"from sklearn.utils.extmath import randomized_svd\n\nU, S, V = randomized_svd(csr_data, \n                              n_components=15,\n                              n_iter=5,\n                              random_state=42)","3d8b06f1":"csr_data","c58f07a6":"movie_data = pd.read_csv(\"\/kaggle\/input\/movie-lens-small-latest-dataset\/movies.csv\")\ndata = pd.read_csv(\"\/kaggle\/input\/movie-lens-small-latest-dataset\/ratings.csv\")","6d1e255d":"#Computing the Singular Value Decomposition (SVD)\n\n#Function to calculate the cosine similarity (sorting by most similar and returning the top N)\ndef top_cosine_similarity(data, movie_id, top_n=10):\n    index = movie_id - 1 # Movie id starts from 1 in the dataset\n    movie_row = data[index, :]\n    magnitude = np.sqrt(np.einsum('ij, ij -> i', data, data))\n    similarity = np.dot(movie_row, data.T) \/ (magnitude[index] * magnitude)\n    sort_indexes = np.argsort(-similarity)\n    return sort_indexes[:top_n]\n\n# Function to print top N similar movies\ndef print_similar_movies(movie_data, movie_id, top_indexes):\n    print('Recommendations for {0}: \\n'.format(\n    movie_data[movie_data.movieId == movie_id].title.values))\n    for id in top_indexes + 1:\n        print((movie_data[movie_data.movieId == id].title.values)[0])","a46402df":"#k-principal components to represent movies, movie_id to find recommendations, top_n print n results        \nk = 50\nmovie_id = 10 # (getting an id from movies.dat)\ntop_n = 10\nsliced = V.T[:, :k] # representative data\nindexes = top_cosine_similarity(sliced, movie_id, top_n)\n\n#Printing the top N similar movies\nprint_similar_movies(movie_data, movie_id, indexes)","6967b21e":"from surprise import SVD\n\nfrom surprise.prediction_algorithms.matrix_factorization import SVD\nfrom surprise import Dataset, Reader\nfrom surprise.model_selection import cross_validate","ec0a3675":"reader = Reader(line_format = 'user item rating', rating_scale=(0,5))","eb82996d":"data = Dataset.load_from_df(ratings[['userId', 'movieId', 'rating']], reader)","e5f53d0b":"svd = SVD()\n\n# Run 5-fold cross-validation and then print results\ncross_validate(svd, data, measures=['RMSE', 'MAE'])","ee0acbdc":"*Now, it\u2019s much easier to interpret that userId 1 has rated movieId 1& 3 4.0 but has not rated movieId 3,4,5 at all (therefore they are represented as NaN ) and therefore their rating data is missing.\n\nLet\u2019s fix this and impute NaN with 0 to make things understandable for the algorithm and also making the data more eye-soothing.*","72821031":"# Reading Ratings","041d2f38":"*As you can see there is no sparse value in the csr_sample and values are assigned as rows and column index. for the 0th row and 2nd column, the value is 3.*\n\nApplying the csr_matrix method to the dataset :","65792a9c":"**Making the movie recommendation system model**\n\n*We will be using the KNN algorithm to compute similarity with  metric which is very fast and more preferable than pearson coefficient*","5fe44ed1":"*Making the necessary modifications as per the threshold set.*","1d6e06ef":"# Let\u2019s visualize how these filters look like\n\n*Aggregating the number of users who voted and the number of movies that were voted.*","fbc1f62b":"# Reading Movies","2eea09db":"# Evaluate Model","2bbec864":"# Removing Noise from the data\n\n> In the real-world, ratings are very sparse and data points are mostly collected from very popular movies and highly engaged users. We wouldn\u2019t want movies that were rated by a small number of users because it\u2019s not credible enough. Similarly, users who have rated only a handful of movies should also not be taken into account.\n> \n> So with all that taken into account and some trial and error experimentations,  we will reduce the noise by adding some filters for the final dataset.\n> \n* >     To qualify a movie, a minimum of 10 users should have voted a movie.\n* >     To qualify a user, a minimum of 50 movies should have voted by the user.\n","cd6b44d3":"*Let\u2019s visualize the number of users who voted with our threshold of 10.*","93bf474c":"# Recommend some movies","6e2e1d81":"*Let\u2019s visualize the number of votes by each user with our threshold of 50.*","4392cf29":"*Making the necessary modifications as per the threshold set.*","763f0b46":"*Here, we can see that userId 1 has watched movieId 1 & 3 and rated both of them 4.0 but has not rated movieId 2 at all. This interpretation is harder to extract from this dataframe. Therefore, to make things easier to understand and work with, we are going to make a new dataframe where each column would represent each unique userId and each row represents each unique movieId.*","1a2d06cc":"# Making the recommendation function\n\n*The working principle is very simple. We first check if the movie name input is in the database and if it is we use our recommendation system to find similar movies and sort them based on their similarity distance and output only the top 10 movies with their distances from the input movie.*","ec4db511":"# Why is sparsity bad for machine learning?\n\n*Sparse matrices are computationally expensive because of the large amount of redundant zero's that are present in the matrix structure. The problem of having a large size increases the space complexity enormously, and it becomes challenging to tackle these problems.*\n\n# Removing sparsity\n\n*Our final_dataset has dimensions of 2121 * 378 where most of the values are sparse. We are using only a small dataset but for the original large dataset of movie lens which has more than 100000 features, our system may run out of computational resources when that is feed to the model. To reduce the sparsity we use the csr_matrix function from the scipy library.*"}}