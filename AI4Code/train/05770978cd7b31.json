{"cell_type":{"6391c533":"code","7e071dab":"code","df86e1fd":"code","2ca63f5c":"code","b66e81c6":"code","b6e5b84d":"code","9301c464":"code","dd21ca81":"code","6392dc3c":"code","57f88026":"code","952232ce":"code","00b48209":"code","9269f31c":"code","727f6390":"code","a45b2529":"code","8744a7e7":"code","09a03ab8":"markdown","6d495284":"markdown","291c60aa":"markdown","9bdaa4e9":"markdown","c822c528":"markdown"},"source":{"6391c533":"import pandas as pd\nimport numpy as np\nimport time\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import r2_score, mean_squared_error\n\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.linear_model import Ridge, LinearRegression\nfrom sklearn.neural_network import MLPRegressor\n","7e071dab":"data = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')","df86e1fd":"data","2ca63f5c":"def preprocess_inputs(df):\n    df = df.copy()\n    \n    # drop id column\n    df = df.drop('id', axis=1)\n    \n    # transform yr_renovated into binary classification\n    df['renovated'] = df['yr_renovated'].apply(lambda x: 1 if x else 0)\n    df = df.drop('yr_renovated', axis=1)\n    \n    # encode date column\n    df['year'] = df['date'].apply(lambda x: x[0:4]).astype(np.int64)\n    df['month'] = df['date'].apply(lambda x: x[4:6]).astype(np.int64)\n    df['day'] = df['date'].apply(lambda x: x[6:8]).astype(np.int64)\n    df = df.drop('date', axis=1)\n    \n    # onehot encode zipcode column\n    dummies = pd.get_dummies(df['zipcode'], prefix='zip')\n    df = pd.concat([df, dummies], axis=1)\n    df = df.drop('zipcode', axis=1)\n\n    # split into X and y\n    y = df['price']\n    X = df.drop('price', axis=1)\n    \n    return X, y","b66e81c6":"X, y = preprocess_inputs(data)","b6e5b84d":"X","9301c464":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)","dd21ca81":"scaler = StandardScaler()","6392dc3c":"X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)","57f88026":"X","952232ce":"models = {\n    'Random Forest': RandomForestRegressor(),\n    'Gradient Boosting': GradientBoostingRegressor(),\n    'Ridge': Ridge(),\n    'Linear Regression': LinearRegression(),\n    'Neural Net': MLPRegressor()\n}","00b48209":"for name, model in models.items():\n    start = time.time()\n    model.fit(X_train, y_train)\n    stop = time.time()\n    print(f'{name} model trained in {stop - start: .4f}s.')","9269f31c":"for name, model in models.items():\n    preds = model.predict(X_test)\n    rmse = mean_squared_error(preds, y_test, squared=False)\n    r2 = r2_score(preds, y_test)\n    print(f'{name}\\n  RMSE: {rmse: .4f}\\n  R^2:  {r2: .4f}')","727f6390":"gb_feature_scores = pd.Series(models['Gradient Boosting'].feature_importances_, index=X.columns)\nrf_feature_scores = pd.Series(models['Random Forest'].feature_importances_, index=X.columns)","a45b2529":"fig, ax = plt.subplots(figsize=(8,8))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance Score\")\nsns.set_style('white')\n\nplt.title('Gradient Boosting Model Feature Importance')\nsns.barplot(x=gb_feature_scores.sort_values(ascending=False)[:5].index, y=gb_feature_scores.sort_values(ascending=False)[:5].values);","8744a7e7":"fig, ax = plt.subplots(figsize=(8,8))\nplt.xlabel(\"Feature\")\nplt.ylabel(\"Importance Score\")\nsns.set_style('white')\n\nplt.title('Random Forest Model Feature Importance')\nsns.barplot(x=rf_feature_scores.sort_values(ascending=False)[:5].index, y=rf_feature_scores.sort_values(ascending=False)[:5].values);","09a03ab8":"# Exploring Feature importance and Model Similarity","6d495284":"**Random Forset and Gradient Boosting are the winners !**","291c60aa":"# Splitting and Scaling","9bdaa4e9":"# Preprocesing","c822c528":"# Model Selection"}}