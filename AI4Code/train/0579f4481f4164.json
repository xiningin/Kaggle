{"cell_type":{"50cc6925":"code","74346512":"code","4fe2292d":"code","53c66ded":"code","9ee4fd3a":"code","4f0c5fab":"code","290af47f":"code","30c4fc68":"code","d8b6160a":"code","6b15a943":"code","e51b014e":"code","677bd698":"code","3dbc9102":"code","781f11af":"code","1a08f4a6":"code","2f2e2188":"code","759e5a52":"code","0c327eec":"code","b4f0e401":"code","30da902a":"code","625067c4":"code","c7e8782c":"code","653648d9":"code","53f21cff":"code","1f550b7a":"code","4d8b2309":"code","b7fcdffa":"code","3fa53eac":"code","d0e37c0b":"code","8df938c5":"code","ef60a40d":"code","152f6565":"code","f97a3bbc":"code","468313c3":"code","d95516ce":"code","fb5fea64":"code","0f286eb1":"code","ca59c7bf":"code","c82c6fae":"code","9a3fae37":"code","15b37d8e":"code","f08b5865":"code","56783688":"code","d5a94e00":"code","9bdac5d6":"code","2c522df2":"code","d1a1f8b6":"code","1ac8ce5c":"code","08bebdca":"code","133f9e16":"code","64aae49a":"code","059ae3d9":"code","ac1a7d07":"code","2184594a":"code","45d11909":"code","300b9d50":"code","63e073df":"code","f02c2812":"code","3415543e":"code","68501073":"code","b6573450":"code","afd2df98":"code","1bb23e2f":"code","b0aacf9f":"code","1ea9ebe5":"code","378cb376":"code","a805a2a9":"code","61f58663":"code","aff9c74d":"code","83a1c304":"code","fc7acb37":"code","a4692e98":"code","d29a1073":"code","5ae18927":"code","92ab0b89":"code","cdfec8de":"code","eca3e7b8":"code","f4a44c8f":"code","2419c789":"code","d2aa22c2":"code","697bb410":"code","ac8b5ec1":"code","dae39e48":"code","a30b3929":"code","ede670c2":"code","d14b1705":"code","dcdce6c2":"code","531301d2":"code","410e273c":"code","904371bd":"code","9fc8e3ae":"code","9a9cfb04":"code","f47bec38":"code","6f2dc0dc":"code","f47cd0c3":"code","95c9943d":"code","ade199f1":"code","e4cb13bc":"code","c87ac23e":"code","5e0e2dba":"markdown","5cb1ebad":"markdown","84064b6c":"markdown","a5ed4c2a":"markdown","a48d8a66":"markdown","e029dc54":"markdown","fa3ff21d":"markdown","e68ab788":"markdown","174a8f21":"markdown","15840b53":"markdown","3a98a830":"markdown","43e4dfcc":"markdown","0b76d8ef":"markdown","4511fd3d":"markdown","23e84934":"markdown","55020881":"markdown","52fc285c":"markdown","a1bd1938":"markdown","114b505e":"markdown","08ad717c":"markdown","087d7528":"markdown","538d3deb":"markdown","09acb450":"markdown","bcbcdc83":"markdown","27faf918":"markdown","86727fd3":"markdown","98963ffc":"markdown","17f7bcea":"markdown","6dd4ac28":"markdown","0af3d991":"markdown"},"source":{"50cc6925":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")","74346512":"df = pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")","4fe2292d":"df.head()","53c66ded":"df.info()","9ee4fd3a":"df.tail()","4f0c5fab":"df.columns","290af47f":"def plot_hist(df,feature):\n    plt.hist(df[feature], bins = 50)\n    plt.xlabel(feature)\n    plt.ylabel(\"Frequency\")\n    plt.title(\"{} distribution with hist\".format(feature))\n    plt.show()","30c4fc68":"numericVar = df.columns[:-1]\nfor n in numericVar:\n    plot_hist(df,n)","d8b6160a":"#Define the number of ZERO s and their percentage\ndef ZEROs(df):\n    print(\"               # of ZEROs  \\t Length \\t  Percent \"  )\n    print(\"------------------------------------------------------\"  )\n\n    for i in range(6) :\n        feature=df[df.columns[i]]\n        ZeroSum=(feature==0).sum()\n        Percent=int(round((ZeroSum\/(len(feature)))*100))\n        print(df.columns[i], \"\\t:\", ZeroSum , \"\\t\\t:\", len(feature), \"\\t \\t:\", Percent,\"% \"  )","6b15a943":"ZEROs(df)","e51b014e":"# Defining X and y\nX = df.iloc[:,:-1]   # Dependants\ny = df.iloc[:, -1]   # Outcome","677bd698":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=0, strategy='median')\n\nX =imputer.fit_transform(X)    # it transform it as an array\nX = pd.DataFrame(X,columns=df.columns[:-1])     # Needs to be made as DataFrame again\ndf=pd.concat([X,y],axis=1)\ndf2=df  # \u0131t xill be used later in KNN ","3dbc9102":"ZEROs(df)","781f11af":"pd.options.display.float_format = \"{:,.0f}\".format\ndf","1a08f4a6":"numericVar = df\nfor n in numericVar:\n    plot_hist(df,n)","2f2e2188":"def detect_outliers(df, features):\n    outlier_indices = []\n    \n    for c in features:\n        # 1st quartile Q1\n        Q1 = np.percentile(df[c], 25)\n        # 3st quartile Q3\n        Q3 = np.percentile(df[c], 75)\n        # IQR\n        IQR = Q3 - Q1\n        # Outlier step\n        outlier_step = IQR * 1.5\n        # detect outlier and their indices\n        outlier_list_col = df[(df[c] < Q1 - outlier_step) | (df[c] > Q3 + outlier_step)].index\n        # store indices\n        outlier_indices.extend(outlier_list_col)\n        \n    outlier_indices = Counter(outlier_indices)\n    multiple_outliers = list(i for i, v in outlier_indices.items() if v > 2)\n    \n    return multiple_outliers","759e5a52":"df.loc[detect_outliers(df, ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age'])]","0c327eec":"# drop outliers\ndf = df.drop(detect_outliers(df, ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']), axis = 0).reset_index(drop= True)","b4f0e401":"list1 = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']","30da902a":"sns.heatmap(df[list1].corr(), annot = True, cmap='Blues', fmt = \".2f\")\nplt.show()","625067c4":"#No meaningful inter correlation ","c7e8782c":"sns.heatmap(df.corr(), annot = True, cmap='Blues', fmt = \".2f\")\nplt.show()","653648d9":"x=X\nfor i in x:\n    g = sns.distplot(x[i], color = \"b\", label = \"Skewness : %.2f\"%(x[i].skew()))\n    g = g.legend(loc = \"best\")\n    plt.show()","53f21cff":"for i in X:\n    g = sns.FacetGrid(df, col = \"Outcome\")\n    g.map(sns.distplot, i, bins= 25)        \n    plt.show()","1f550b7a":"def plotHistogram(values,label,feature,title):\n    sns.set_style(\"whitegrid\")\n    plotOne = sns.FacetGrid(values, hue=label,aspect=2)\n    plotOne.map(sns.distplot,feature,kde=False)\n    plotOne.set(xlim=(0, values[feature].max()))\n    plotOne.add_legend()\n    plotOne.set_axis_labels(feature, 'Proportion')\n    plotOne.fig.suptitle(title)\n    plt.show()\nfor i in X:\n    plotHistogram(df,\"Outcome\",i,' Diagnosis (Blue = Healthy; Orange = Diabetes)')","4d8b2309":"for i in X.columns[:-1]:\n    sns.factorplot(x = \"Age\", y= i, hue= \"Outcome\", data = df)\n    plt.show()","b7fcdffa":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)","3fa53eac":"print(\"X_train\", X_train.shape)\nprint(\"X_test\", X_test.shape)\nprint(\"y_train\", y_train.shape)\nprint(\"y_test\", y_test.shape)","d0e37c0b":"from sklearn.preprocessing import StandardScaler\nscale = StandardScaler()","8df938c5":"X_train = scale.fit_transform(X_train)\nX_test = scale.transform(X_test)","ef60a40d":"from sklearn.linear_model import LogisticRegression\nlog_model = LogisticRegression()\n","152f6565":"log_model.fit(X_train, y_train)\ny_pred = log_model.predict(X_test)\ny_train_pred = log_model.predict(X_train)","f97a3bbc":"print(\"log_model.coef_:\",log_model.coef_,\"\\nlog_model.intercept_ :\",log_model.intercept_)","468313c3":"from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score","d95516ce":"scr=[precision_score, recall_score, accuracy_score, f1_score]\nfor i in scr:\n    print(i(y_train, y_train_pred))","fb5fea64":"for i in scr:\n    print(i(y_test, y_pred))","0f286eb1":"print(\"f1_score(y_train, y_train_pred, average = weighted) \\t :\",f1_score(y_train, y_train_pred, average = \"weighted\"))\nprint(\"f1_score(y_test, y_pred, average = weighted)\\t\\t :\",f1_score(y_test, y_pred, average = \"weighted\"))","ca59c7bf":"from sklearn.metrics import confusion_matrix, plot_confusion_matrix,plot_roc_curve\nconfusion_matrix(y_test, y_pred)","c82c6fae":"plot_confusion_matrix(log_model, X_test, y_test)","9a3fae37":"plot_confusion_matrix(log_model, X_test, y_test, normalize= \"all\")","15b37d8e":"from sklearn.metrics import confusion_matrix,classification_report","f08b5865":"print(confusion_matrix(y_pred,y_test))","56783688":"print(classification_report(y_pred,y_test))","d5a94e00":"from sklearn.model_selection import cross_val_score, cross_validate","9bdac5d6":"model = LogisticRegression()","2c522df2":"scores = cross_val_score(model, X_train, y_train, cv= 10)\nprint(\"Cross- validation mean of accuracy scores\", scores.mean())\nprint(\"\")\nprint(scores)","d1a1f8b6":"scores = cross_val_score(model, X_test, y_test, cv= 10)\nprint(\"Cross- validation mean of accuracy scores\", scores.mean())\nprint(\"\")\nprint(scores)","1ac8ce5c":"pd.options.display.float_format = \"{:,.4f}\".format\nmodel = LogisticRegression()\n\nscores = cross_validate(model, X_train, y_train, scoring = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \n                                                               \"f1_weighted\"], cv = 10)\ndf_scores = pd.DataFrame(scores, index= range(1, 11))\ndf_scores","08bebdca":"df_scores.mean()[2:]","133f9e16":"model = LogisticRegression()\n\nscores = cross_validate(model, X_test, y_test, scoring = [\"accuracy\", \"precision_weighted\", \"recall_weighted\", \n                                                               \"f1_weighted\"], cv = 10)\ndf_scores = pd.DataFrame(scores, index= range(1, 11))\ndf_scores","64aae49a":"df_scores.mean()[2:]","059ae3d9":"from sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","ac1a7d07":"print(classification_report(y_train, y_train_pred))","2184594a":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","45d11909":"df2.head()  # ZEROs converted to median ","300b9d50":"df2.Outcome.value_counts()","63e073df":"colors = [ \"green\",\"purple\"]\nsns.countplot(df2.Outcome, palette= colors)\nplt.show()","f02c2812":"non_diabet= df[df.Outcome == 0]\ndiabet= df[df.Outcome == 1]","3415543e":"plt.scatter(diabet.Age, diabet.Glucose, color= \"red\", label= \"k\u00f6t\u00fc\")\nplt.scatter(non_diabet.Age, non_diabet.Glucose, color= \"green\", label = \"iyi\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Glucose\")\nplt.legend()\nplt.show()","68501073":"df.Outcome","b6573450":"labels = [\"Healthy\",\"Diabetics\"]\nexplode = [0, 0]\nsizes = df2.Outcome.value_counts().values\n\nplt.figure(figsize= (7, 7))\nplt.pie(sizes, explode= explode, labels= labels, autopct = \"%1.1f%%\",)\nplt.title(\"Percentage of diagnosis diabetics\", color = \"orange\", fontsize = 20)\nplt.show()","afd2df98":"corr = df2.corr().Outcome\ncorr[np.argsort(corr, axis= 0)[:-1]]","1bb23e2f":"y = df2.Outcome\nx = df2.drop(columns = \"Outcome\")","b0aacf9f":"x.head()","1ea9ebe5":"x.columns","378cb376":"y.head()","a805a2a9":"from sklearn.preprocessing import Normalizer\nx = Normalizer().fit_transform(x)\nx=pd.DataFrame(x,columns=df2.columns[:-1])\nx","61f58663":"for i in x:\n    g = sns.distplot(x[i], color = \"b\", label = \"Skewness : %.2f\"%(x[i].skew()))\n    g = g.legend(loc = \"best\")\n    plt.show()","aff9c74d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state= 42)","83a1c304":"print(\"X_train: {}\\nx_test: {}\\ny_train: {}\\ny_test: {}\".format(X_train.shape, X_test.shape, y_train.shape, y_test.shape))","fc7acb37":"from sklearn.neighbors import KNeighborsClassifier\nknn_model = KNeighborsClassifier(n_neighbors= 3)\nknn_model.fit(x_train, y_train)\ny_pred = knn_model.predict(x_test)","a4692e98":"#   p <= %50 = 0,  p > %50 = 1 ","d29a1073":"y_pred","5ae18927":"y_pred_proba = knn_model.predict_proba(x_test)\ny_pred_proba","92ab0b89":"pd.DataFrame(y_pred_proba).sample(10)","cdfec8de":"my_dict = {\"Actual\": y_test, \"Pred\": y_pred, \"Pred_proba\": y_pred_proba[:,1]}\npd.DataFrame.from_dict(my_dict).sample(20)","eca3e7b8":"from sklearn.metrics import confusion_matrix, classification_report, plot_confusion_matrix","f4a44c8f":"confusion_matrix(y_test, y_pred)","2419c789":"plot_confusion_matrix(knn_model, x_test, y_test);","d2aa22c2":"plot_confusion_matrix(knn_model, x_test, y_test, normalize= \"all\")","697bb410":"print(classification_report(y_test, y_pred))","ac8b5ec1":"y_train_pred = knn_model.predict(x_train)\nprint(classification_report(y_train, y_train_pred))","dae39e48":"from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score","a30b3929":"test_error_rates = []\nfor k in range(20, 50):\n    knn_model = KNeighborsClassifier(n_neighbors= k)\n    knn_model.fit(x_train, y_train)\n    \n    y_pred_test = knn_model.predict(x_test)\n    \n    test_error = 1 - accuracy_score(y_test, y_pred_test)\n    test_error_rates.append(test_error)","ede670c2":"plt.figure(figsize = (15, 8))\nplt.plot(range(20,50), test_error_rates, color = \"blue\", linestyle = \"--\", marker = \"o\",\n         markerfacecolor= \"red\", markersize= 10)\nplt.title(\"Error Rate vs K-Values\")\nplt.xlabel(\"K_values\")\nplt.ylabel(\"Error Rate\")\nplt.hlines(y= 0.34, xmin= 20, xmax= 50,colors = \"r\", linestyles= \"--\")\nplt.hlines(y= 0.32, xmin= 20, xmax= 50,colors = \"r\", linestyles= \"--\")\nplt.show()","d14b1705":"# k = 1\nknn = KNeighborsClassifier(n_neighbors= 28)\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","dcdce6c2":"knn = KNeighborsClassifier(n_neighbors= 45)\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","531301d2":"from sklearn.model_selection import cross_val_score, cross_validate","410e273c":"model = KNeighborsClassifier(n_neighbors= 45)\n\nscores = cross_validate(model, x_train, y_train, scoring = [\"accuracy\", \"precision\", \"recall\", \"f1\"], cv = 10)","904371bd":"df_scores = pd.DataFrame(scores, index = range(1,11))\ndf_scores","9fc8e3ae":"from sklearn.model_selection import GridSearchCV","9a9cfb04":"knn_grid = KNeighborsClassifier()","f47bec38":"k_values = range(1, 50)","6f2dc0dc":"param_grid = {\"n_neighbors\": k_values, \"weights\": [\"uniform\", \"distance\"]}","f47cd0c3":"knn_grid_model = GridSearchCV(knn_grid, param_grid, cv= 10, scoring = \"accuracy\")","95c9943d":"knn_grid_model.fit(x_train, y_train)","ade199f1":"knn_grid_model.best_estimator_","e4cb13bc":"knn = KNeighborsClassifier(n_neighbors= 45)\n\nknn.fit(x_train, y_train)\npred = knn.predict(x_test)\n\nprint(\"TEST\")\nprint(\"\\n\")\nprint(confusion_matrix(y_test, pred))\nprint(\"\\n\")\nprint(classification_report(y_test, pred))","c87ac23e":"knn = KNeighborsClassifier(n_neighbors= 45)\n\nknn.fit(x_train, y_train)\npred = knn.predict(x_train)\n\nprint(\"TRAIN\")\nprint(\"\\n\")\nprint(confusion_matrix(y_train, pred))\nprint(\"\\n\")\nprint(classification_report(y_train, pred))","5e0e2dba":"## Evaluate Model","5cb1ebad":"## Model Performance on Classification Tasks","84064b6c":"## Scores by Various K Values","a5ed4c2a":"## Train - Test split","a48d8a66":"##  Read Dataset","e029dc54":"# Diabetes Dataset - Homework\n\n    * Use Logistic Regression and KNN Classification","fa3ff21d":"# Data Preparation","e68ab788":"## normalization","174a8f21":"## Cross Validate","15840b53":"## Determine Dependent and Independent Variables","3a98a830":"* In the x variable, there are variables affecting the diabetic patient.\n* We have labels in the y variable.","43e4dfcc":"## Elbow Method for Choosing Reasonable K Values","0b76d8ef":"## Train - Test Split","4511fd3d":"# Statistical Analysis","23e84934":"Precision quantifies the number of positive class predictions that actually belong to the positive class.\nRecall quantifies the number of positive class predictions made out of all positive examples in the dataset.\nF-Measure provides a single score that balances both the concerns of precision and recall in one number.\n\naccuracy_score = R2_score","55020881":"* The most correlation with depandant comes from Glucose:0.49 ","52fc285c":"# knn_classifier","a1bd1938":"* Mustafa Oztemiz ","114b505e":"## Outliers and skew check","08ad717c":"## Import Library","087d7528":"* 1 - k de\u011ferini sec\n* 2 - k en yak\u0131n data noktalar\u0131n\u0131 bul\n* 3 - k en yak\u0131n kom\u015fu aras\u0131ndaki hangi classtan ka\u00e7 tane var hesapla\n* 4 - test etti\u011finiz point yada data hangi classa ait tespit et","538d3deb":"* We are free from zeros anymore. Lets continue to investigate the features","09acb450":"## Model Fitting and Compare Actual and Predicted Labels","bcbcdc83":"# Improve Model \n    * Use Cross Validate\n    * Use Grid-Search","27faf918":"##  Gridseach Method for Choosing Reasonable K Values","86727fd3":"## Modeling","98963ffc":"# Import Library","17f7bcea":"# EDA","6dd4ac28":"# Read Dataset","0af3d991":"# Outlier Detection"}}