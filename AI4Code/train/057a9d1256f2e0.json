{"cell_type":{"625066d0":"code","9d079c22":"code","a0ccb03b":"code","58d7ba5a":"code","1e4250ec":"code","46174c81":"code","6f73060c":"code","23b43fcd":"code","0348f54f":"code","64e00e59":"code","f0d74156":"code","86857080":"code","5c0dab56":"code","d2e0f384":"code","1cfd64d9":"code","ea0b5244":"code","e0302a77":"code","0e75eaa2":"code","59353b6a":"code","602f4d42":"code","43fd1dd7":"code","9ea93c58":"code","bec18963":"code","445626f6":"code","fe838798":"code","155f17d5":"code","1a80987b":"code","f3dcce5b":"code","c0f37427":"code","094b8fb4":"code","0dcff93c":"code","18fe7f17":"code","2a634846":"code","24b8713a":"code","7e5b0954":"code","1301bb92":"code","3d4dfaf7":"markdown","b9dfb86c":"markdown","63a145ca":"markdown","23996225":"markdown","3717e5da":"markdown","3fa717bc":"markdown","cbb0ee5d":"markdown","71134c4d":"markdown","43b3f16a":"markdown"},"source":{"625066d0":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom itertools import product\n\nimport xgboost as xgb","9d079c22":"#Reading project data\n\nitems=pd.read_csv('..\/input\/items.csv')\nitem_categories=pd.read_csv('..\/input\/item_categories.csv')\nshops=pd.read_csv('..\/input\/shops.csv')\n\ntest=pd.read_csv('..\/input\/test.csv.gz',compression='gzip')\nsample_submission=pd.read_csv('..\/input\/sample_submission.csv.gz',compression='gzip')\ntrain=pd.read_csv('..\/input\/sales_train.csv.gz',compression='gzip')","a0ccb03b":"# Understand How many Items are there\nprint(items.info())\nprint(items.describe())\nitems.head(5)","58d7ba5a":"# To understand a little more about the item categories\nprint(item_categories.info())\nprint(item_categories.describe())\nitem_categories.head(5)","1e4250ec":"train.head(5)","46174c81":"test.head(5)","6f73060c":"# Adding item categories to the data set\nitems_to_add = ['item_category_id', 'item_name']\n\nfor item in items_to_add:\n    train[item] = train['item_id'].map(items[item])\n    test[item] = test['item_id'].map(items[item])\n\ntrain['item_category_name'] = (train['item_category_id'].map\n                               (item_categories['item_category_name']))\ntest['item_category_name'] = (test['item_category_id'].map\n                               (item_categories['item_category_name']))\n\n# Adding dates \ntrain['date'] = pd.to_datetime(train['date'], format=\"%d.%m.%Y\")\ntrain['year'], train['day'] = train['date'].dt.year, train['date'].dt.day\ntrain['month'] = train['date'].dt.month","23b43fcd":"# See How many items are in each price bins\nmax_item_prices = train.groupby('item_id')['item_price'].max()\nmin_item_prices = train.groupby('item_id')['item_price'].min()\n\nprint('---- MAX PRICES ----\\n', max_item_prices.describe())\nprint('\\n---- MIN PRICES ----\\n', min_item_prices.describe())\n\nexpensive_items = sum(max_item_prices > 10000)\ncheap_items = sum(min_item_prices < 0)\n\nprint('\\nThere are {0} number of items cost more than $10,000.\\\n        \\nThere are {1} number of item cost less than $0'.format(\n    expensive_items, cheap_items))\n'''\nThere seems to be an item that costs $307,980.\nThere is an item that cost -$1. need to investigate more.\n\nI am going to exclude the expensive items in this study.\n'''\nplt.figure()\nplt.hist(max_item_prices[max_item_prices < 10000])\nplt.hist(min_item_prices[(0 < min_item_prices) & (min_item_prices < 10000)])","0348f54f":"'''\nFrom the above history, we can tell that most items are in the range of\n$0 - $500. Therefore, we are going to study more items prices in that\nrange.\n'''\n\nplt.figure()\nplt.hist(max_item_prices[max_item_prices < 500])\nplt.hist(min_item_prices[(0 < min_item_prices) & (min_item_prices < 500)])","64e00e59":"# count how many items sold in each stores\ntrain_store_item_diversity = train.groupby('shop_id')['item_id'].nunique()\n\nprint(train_store_item_diversity.describe())\nplt.figure()\nplt.plot(train_store_item_diversity)","f0d74156":"# count how many items sold per item\ntrain_item_sold = train.groupby('item_id')['item_cnt_day'].sum()\n\nprint(train_item_sold.describe())\n\nplt.figure()\nplt.plot(train_item_sold)","86857080":"most_bought_item = train[train.item_id == train_item_sold.argmax()]\n\nmost_bought_item.item_name.iloc[0]","5c0dab56":"# which item gets returned the most\nreturned_counts = -train[train['item_cnt_day'] < 0].groupby('item_id')['item_cnt_day'].sum()\n\nprint(returned_counts.describe())\n\nplt.figure()\nplt.plot(returned_counts)","d2e0f384":"most_returned_item = train[train.item_id == returned_counts.idxmax()].item_name.iloc[0]\n\nprint('The most returned item is: {0}'.format(most_returned_item))","1cfd64d9":"# Calculate Revenues of the items\ntrain['revenue'] = train['item_price'] * train['item_cnt_day']\nitems_num = train.groupby(['month', 'day'])['item_cnt_day'].sum()\n\nprint(items_num.describe())\n# How shopping trends vary in a year\nitems_num.plot()","ea0b5244":"# see which item makes the most revenue\nitem = train.groupby('item_id')['revenue'].sum().idxmax()\n\nprint('Item ID {} is the most profitable item '.format(item))\n\n# see which item makes the least revenue\nitem = train.groupby('item_id')['revenue'].sum().idxmin()\n\nprint('Item ID {} is the least profitable item '.format(item))","e0302a77":"#There is no null values\/missing values\ntrain.isnull().values.sum()","0e75eaa2":"index_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = [] \nfor block_num in train['date_block_num'].unique():\n    cur_shops = train[train['date_block_num']==block_num]['shop_id'].unique()\n    cur_items = train[train['date_block_num']==block_num]['item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\n\n#turn the grid into pandas dataframe\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\n\n#get aggregated values for (shop_id, item_id, month)\ngb = train.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})\n\n#fix column names\ngb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]\n#join aggregated data to the grid\nall_data = pd.merge(grid,gb,how='left',on=index_cols).fillna(0)\n#sort the data\nall_data.sort_values(['date_block_num','shop_id','item_id'],inplace=True)","59353b6a":"# Expanding Mean Encoding\ncumsum = all_data.groupby('item_id').target.cumsum() - all_data.target\ncumcnt = all_data.groupby('item_id').target.cumcount()\n\nall_data['item_target_enc'] = cumsum \/ cumcnt\n\n# Fill NaNs\nall_data['item_target_enc'].fillna(0.3343, inplace=True) ","602f4d42":"# Clean up item category name - using the first part\nitem_categories['meta_category'] = item_categories.item_category_name.apply(lambda x: x.split()[0])\nprint('There are {} unique meta categories.'.format(item_categories['meta_category'].nunique()))","43fd1dd7":"sales = pd.read_csv('..\/input\/sales_train.csv.gz',compression='gzip')\nsales['item_category_id'] = sales['item_id'].map(items['item_category_id'])","9ea93c58":"# Create \"grid\" with columns\nindex_cols = ['shop_id', 'item_id', 'date_block_num']\n\n# For every month we create a grid from all shops\/items combinations from that month\ngrid = []\nfor block_num in sales['date_block_num'].unique():\n    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()\n    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()\n    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))\ngrid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)","bec18963":"# cleaning the outliers\nsales = sales[sales.item_price<100000]\nsales = sales[sales.item_cnt_day<=1000]","445626f6":"mean_sales = sales.groupby(['date_block_num','shop_id','item_id']).agg({'item_cnt_day': 'sum','item_price': np.mean}).reset_index()\nmean_sales = pd.merge(grid,mean_sales,on=['date_block_num','shop_id','item_id'],how='left').fillna(0)\n# adding the category id too\nmean_sales = pd.merge(mean_sales,items,on=['item_id'],how='left')","fe838798":"for type_id in ['item_id','shop_id','item_category_id']:\n    for column_id,aggregator,aggtype in [('item_price',np.mean,'avg'),('item_cnt_day',np.sum,'sum'),('item_cnt_day',np.mean,'avg')]:\n\n        mean_df = sales.groupby([type_id,'date_block_num']).aggregate(aggregator).reset_index()[[column_id,type_id,'date_block_num']]\n        mean_df.columns = [type_id+'_'+aggtype+'_'+column_id,type_id,'date_block_num']\n\n        mean_sales = pd.merge(mean_sales,mean_df,on=['date_block_num',type_id],how='left')","155f17d5":"lag_variables  = list(mean_sales.columns[7:])+['item_cnt_day']\nlags = [1 ,2 ,3 ,6]\nfor lag in lags:\n    sales_new_df = mean_sales.copy()\n    sales_new_df.date_block_num+=lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    mean_sales = pd.merge(mean_sales, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","1a80987b":"# fill nan with zeros\nfor feat in mean_sales.columns:\n    if 'item_cnt' in feat:\n        mean_sales[feat] = mean_sales[feat].fillna(0)\n    elif 'item_price' in feat:\n        mean_sales[feat] = mean_sales[feat].fillna(mean_sales[feat].median())\n\n# drop non-lag features\ncols_to_drop = lag_variables[:-1] + ['item_name','item_price']\n\n# recent\nrecent_mean_sales = mean_sales[mean_sales['date_block_num'] > 12]\n\n# Split X_train and X_valid\nX_train = mean_sales[mean_sales['date_block_num']<33].drop(cols_to_drop, axis=1)\nX_valid =  mean_sales[mean_sales['date_block_num']==33].drop(cols_to_drop, axis=1)","f3dcce5b":"# limit the range of max items\ndef limit_sales(x):\n    if x>40:\n        return 40\n    elif x<0:\n        return 0\n    else:\n        return x","c0f37427":"X_train['item_cnt_day'] = X_train.apply(lambda x: limit_sales(x['item_cnt_day']),axis=1)\nX_valid['item_cnt_day'] = X_valid.apply(lambda x: limit_sales(x['item_cnt_day']),axis=1)","094b8fb4":"# convert mean sales to xgboost\nX_train_xgb = xgb.DMatrix(X_train.iloc[:, X_train.columns != 'item_cnt_day'].values,\n                             X_train.iloc[:, X_train.columns == 'item_cnt_day'].values)\n\n# Parameters\nparam = {'max_depth':10, \n         'subsample':1,\n         'min_child_weight':0.5,\n         'eta':0.3, \n         'num_round':1000, \n         'seed':1,\n         'silent':0,\n         'eval_metric':'rmse'}","0dcff93c":"bst = xgb.train(param, X_train_xgb)","18fe7f17":"importance = xgb.plot_importance(bst)\nimportance.figure.set_size_inches(10, 30)","2a634846":"columns = list(X_train.columns)\ndel columns[columns.index('item_cnt_day')]\ncolumns =  [columns[x] for x in [2, 0, 1, 5, 8, 3, 4, 23, 9, 11]]","24b8713a":"test = pd.read_csv('..\/input\/test.csv.gz',compression='gzip')","7e5b0954":"test['date_block_num'] = 34\ntest = pd.merge(test,items,on=['item_id'],how='left')","1301bb92":"for lag in lags:\n    sales_new_df = mean_sales.copy()\n    sales_new_df.date_block_num+=lag\n    sales_new_df = sales_new_df[['date_block_num','shop_id','item_id']+lag_variables]\n    sales_new_df.columns = ['date_block_num','shop_id','item_id']+ [lag_feat+'_lag_'+str(lag) for lag_feat in lag_variables]\n    test = pd.merge(test, sales_new_df,on=['date_block_num','shop_id','item_id'] ,how='left')","3d4dfaf7":"# Simple XGBoost\nRun a simple XGBoost to figure out which features are more important","b9dfb86c":"One item is sold over 187,642 units. Let's see which item it is.","63a145ca":"Based on above graph, most stores sell ~7000 unique items. \n\nThe minimum number items a store would sell is 258. \n\n","23996225":"One item is returned 60 times. Let's figure out what item is it.","3717e5da":"# Create Lag Features\nSince we are predicting future values, we add lags, previous sales data, to the data.\nThe following data are used to create lag features:\n* 'item_id_avg_item_price',\n* 'item_id_sum_item_cnt_day'\n* 'item_id_avg_item_cnt_day'\n* 'shop_id_avg_item_price'\n* 'shop_id_sum_item_cnt_day'\n* 'shop_id_avg_item_cnt_day'\n* 'item_category_id_avg_item_price'\n* 'item_category_id_sum_item_cnt_day'\n* 'item_category_id_avg_item_cnt_day'\n* 'item_cnt_day'","3fa717bc":"Based on the above information, there are\n- 22170 different items in the data\n- 84 different item_category\n\nNow, we are going to understand how our data looks like","cbb0ee5d":"Based on the two above histograms, I can conclude that little items cost more than $10,000 \n\nMajority of the items are in the price range of $0 - 300.\n","71134c4d":"# Create a Empty Grid\ncreating empty grids for all shop and item blocks. Even if there is no sales information from the sales data, it should still be presented for Machine Learning algorithm.","43b3f16a":"I did some google search. The item is '\u0424\u0438\u0440\u043c\u0435\u043d\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442 \u043c\u0430\u0439\u043a\u0430 1\u0421 \u0418\u043d\u0442\u0435\u0440\u0435\u0441 \u0431\u0435\u043b\u044b\u0439 (34*42) 45 \u043c\u043a\u043c', which is just plastic bags in Russian. No surprise."}}