{"cell_type":{"b30613f9":"code","11875d19":"code","a08b06c9":"code","04a83f71":"code","eca5e06d":"code","c10056e9":"code","611c782e":"code","200cc8cd":"code","634a75a2":"code","6fcacc41":"code","881db311":"code","d3599eb9":"code","03254ae2":"code","b0b00918":"code","45e1762d":"code","be2f1699":"code","dc2bdc82":"code","556d0e4d":"code","a6190850":"code","8c3e6fc1":"code","d781bffe":"code","33a45b3d":"code","89abd400":"code","04d699b3":"code","f19a57b1":"code","a89f796b":"code","df569e26":"code","e2f0fc01":"code","9d3e286f":"code","3864b854":"code","a3df6352":"code","83de56ae":"code","5f5d0da4":"code","77c992af":"code","e165e408":"code","6f9d76fd":"code","bd893734":"code","057fe97c":"code","ee83cbda":"code","9c8de340":"code","8fc00ccd":"code","4f67756d":"markdown","6afdfc41":"markdown","4943048d":"markdown","3937e401":"markdown","88fcc62b":"markdown","b6e9bc0c":"markdown","8ca69fa6":"markdown","c73204fc":"markdown","e7ff0942":"markdown"},"source":{"b30613f9":"import numpy as np\nimport pandas as pd\nimport cv2\nfrom glob import glob\nimport os\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils.np_utils import to_categorical\nimport random\nimport ast\nimport csv","11875d19":"IMG_SIZE = 64\nIMG_BASE_SIZE = 256\nBATCH_SIZE = 512\nTRAIN_CSV_PATH_LIST = glob('..\/input\/train_simplified\/*.csv')\nSKIP_RECORD = 0\nRECORD_RANGE = 10000","a08b06c9":"TRAIN_CSV_PATH_LIST[:5]","04a83f71":"len(TRAIN_CSV_PATH_LIST)","eca5e06d":"class_list = []\nfor item in TRAIN_CSV_PATH_LIST:\n    classname = os.path.basename(item).split('.')[0]\n    class_list.append(classname)","c10056e9":"class_list = sorted(class_list)\nclass_list[:5]","611c782e":"len(class_list)","200cc8cd":"word_encoder = LabelEncoder()\nword_encoder.fit(class_list)","634a75a2":"word_encoder.transform(class_list[:5])","6fcacc41":"def my_one_hot_encoder(word):\n    return to_categorical(word_encoder.transform([word]),num_classes=340).reshape(340)","881db311":"test_y = my_one_hot_encoder('The Eiffel Tower')","d3599eb9":"test_y","03254ae2":"test_y.shape","b0b00918":"def train_generator(path_list, img_size, batch_size, lw=6):\n    while True:\n        csv_path_list = random.choices(path_list, k=batch_size)\n        x = np.zeros((batch_size, img_size, img_size, 3))\n        y = np.zeros((batch_size, 340))\n        for j in range(batch_size):\n            csv_path = csv_path_list[j]\n            f = open(csv_path, 'r')\n            reader = csv.reader(f)\n            for _ in range(SKIP_RECORD+1):\n                __ = next(reader)\n            i = 0\n            s = np.random.randint(RECORD_RANGE)\n            for row in reader:\n                if i == s:\n                    drawing = row[1]\n                    break\n                else:\n                    i += 1\n            f.close()\n            lst = ast.literal_eval(drawing)\n            img = np.zeros((IMG_BASE_SIZE, IMG_BASE_SIZE), np.uint8)\n            for t, stroke in enumerate(lst):\n                color = 255 - min(t, 10) * 13\n                for i in range(len(stroke[0]) - 1):\n                    _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n            if img_size != IMG_BASE_SIZE:\n                x[j, :, :, 0] = cv2.resize(img, (img_size, img_size))\/255\n                x[j, :, :, 1] = cv2.resize(img, (img_size, img_size))\/255\n                x[j, :, :, 2] = cv2.resize(img, (img_size, img_size))\/255\n            else:\n                x[j, :, :, 0] = img\/255\n                x[j, :, :, 1] = img\/255\n                x[j, :, :, 2] = img\/255\n            classname = os.path.basename(csv_path).split('.')[0]\n            y_tmp = my_one_hot_encoder(classname)\n            y[j] = y_tmp\n        yield x, y","45e1762d":"datagen = train_generator(path_list=TRAIN_CSV_PATH_LIST, img_size=IMG_SIZE, batch_size=BATCH_SIZE, lw=6)","be2f1699":"x, y = next(datagen)","dc2bdc82":"x.shape, y.shape, x.min(), x.max(), y.min(), y.max(), y.sum(), y[0].sum()","556d0e4d":"VAL_IMAGES_PER_CLASS = 20\nVAL_CLASS = 170\nVAL_SKIP_RECORD = SKIP_RECORD + RECORD_RANGE","a6190850":"def create_val_set(path_list, val_class, val_images_per_class, img_size, lw=6):\n    csv_path_list = random.sample(path_list, k=val_class)\n    x = np.zeros((val_class*val_images_per_class, img_size, img_size, 3))\n    y = np.zeros((val_class*val_images_per_class, 340))\n    for k in range(val_class):\n        csv_path = csv_path_list[k]\n        f = open(csv_path, 'r')\n        reader = csv.reader(f)\n        for _ in range(VAL_SKIP_RECORD+1):\n            __ = next(reader)\n        s = 0\n        for row in reader:\n            if s == val_images_per_class:\n                break\n            else:\n                drawing = row[1]\n                lst = ast.literal_eval(drawing)\n                img = np.zeros((IMG_BASE_SIZE, IMG_BASE_SIZE), np.uint8)\n                for t, stroke in enumerate(lst):\n                    color = 255 - min(t, 10) * 13\n                    for i in range(len(stroke[0]) - 1):\n                        _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n                if img_size != IMG_BASE_SIZE:\n                    x[k*val_images_per_class+s, :, :, 0] = cv2.resize(img, (img_size, img_size))\/255\n                    x[k*val_images_per_class+s, :, :, 1] = cv2.resize(img, (img_size, img_size))\/255\n                    x[k*val_images_per_class+s, :, :, 2] = cv2.resize(img, (img_size, img_size))\/255\n                else:\n                    x[k*val_images_per_class+s, :, :, 0] = img\/255\n                    x[k*val_images_per_class+s, :, :, 1] = img\/255\n                    x[k*val_images_per_class+s, :, :, 2] = img\/255\n                classname = os.path.basename(csv_path).split('.')[0]\n                y_tmp = my_one_hot_encoder(classname)\n                y[k*val_images_per_class+s,:] = y_tmp\n                s += 1\n        f.close()\n    return x, y","8c3e6fc1":"valid_x, valid_y = create_val_set(path_list=TRAIN_CSV_PATH_LIST, val_class=VAL_CLASS,\n                                  val_images_per_class=VAL_IMAGES_PER_CLASS, img_size=IMG_SIZE, lw=6)","d781bffe":"valid_x.shape, valid_y.shape, valid_x.min(), valid_x.max(), valid_y.min(), valid_y.max(), valid_y.sum(), valid_y[0].sum()","33a45b3d":"def calc_map3_per_image(true_label, pred_3label):\n    if true_label == pred_3label[0]:\n        score = 1\n    elif true_label == pred_3label[1]:\n        score = 1\/2\n    elif true_label == pred_3label[2]:\n        score = 1\/3\n    else:\n        score = 0\n    return score","89abd400":"def calc_map3_allimage(y_trues, y_preds):\n    num = y_trues.shape[0]\n    scores = list()\n    for i in range(num):\n        true_label = y_trues[i].argsort()[::-1][0]\n        pred_3label = y_preds[i].argsort()[::-1][:3]\n        score = calc_map3_per_image(true_label, pred_3label)\n        scores.append(score)\n    return np.mean(scores)","04d699b3":"import tensorflow as tf\ndef my_metric(y_trues, y_preds):\n    return tf.py_func(calc_map3_allimage, [y_trues, y_preds], tf.float64)","f19a57b1":"from keras import Model\nfrom keras.layers import GlobalAveragePooling2D\nfrom keras.layers import Dense\nfrom keras.layers import Dropout\nfrom keras.applications import ResNet50\nfrom keras import optimizers\n\ndef get_model(input_shape):\n    base_model = ResNet50(input_shape=input_shape, include_top=False, weights=None)\n    for l in base_model.layers:\n        l.trainable = True\n    x = base_model.output\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(1024, activation='relu')(x)\n    x = Dropout(0.5)(x)\n    x = Dense(340, activation='softmax')(x)\n    model = Model(base_model.input, x)\n    \n    return model","a89f796b":"model = get_model(input_shape=(IMG_SIZE,IMG_SIZE,3))","df569e26":"model.summary()","e2f0fc01":"c = optimizers.adam(lr = 0.001)","9d3e286f":"model.compile(loss='categorical_crossentropy', optimizer=c, metrics=[my_metric])","3864b854":"history = model.fit_generator(datagen, epochs=40, steps_per_epoch=30, verbose=1, validation_data=(valid_x, valid_y))","a3df6352":"test_df = pd.read_csv('..\/input\/test_simplified.csv')","83de56ae":"test_df.shape","5f5d0da4":"test_df.head()","77c992af":"def create_test_data(img_size, lw=6):\n    x = np.zeros((test_df.shape[0], img_size, img_size, 3))\n    for j in range(test_df.shape[0]):\n        drawing = test_df.loc[j,'drawing']\n        lst = ast.literal_eval(drawing)\n        img = np.zeros((IMG_BASE_SIZE, IMG_BASE_SIZE), np.uint8)\n        for t, stroke in enumerate(lst):\n            color = 255 - min(t, 10) * 13\n            for i in range(len(stroke[0]) - 1):\n                _ = cv2.line(img, (stroke[0][i], stroke[1][i]), (stroke[0][i + 1], stroke[1][i + 1]), color, lw)\n        if img_size != IMG_BASE_SIZE:\n            x[j, :, :, 0] = cv2.resize(img, (img_size, img_size))\/255\n            x[j, :, :, 1] = cv2.resize(img, (img_size, img_size))\/255\n            x[j, :, :, 2] = cv2.resize(img, (img_size, img_size))\/255\n        else:\n            x[j, :, :, 0] = img\/255\n            x[j, :, :, 1] = img\/255\n            x[j, :, :, 2] = img\/255\n    return x","e165e408":"test_x = create_test_data(img_size=IMG_SIZE, lw=6)","6f9d76fd":"test_x.shape","bd893734":"test_pred = model.predict(test_x, batch_size=128, verbose=1)","057fe97c":"import warnings\nwarnings.filterwarnings('ignore')","ee83cbda":"pred_rows = []\nfor i in range(test_df.shape[0]):\n    test_top3 = test_pred[i].argsort()[::-1][:3]\n    test_top3_words = word_encoder.inverse_transform(test_top3).tolist()\n    test_top3_words = [k.replace(' ', '_') for k in test_top3_words]\n    pred_words = test_top3_words[0] + ' ' + test_top3_words[1] + ' ' + test_top3_words[2]\n    pred_rows += [{'key_id': test_df.loc[i, 'key_id'], 'word': pred_words}] ","9c8de340":"sub = pd.DataFrame(pred_rows)[['key_id', 'word']]\nsub.to_csv('submission.csv', index=False)","8fc00ccd":"sub.head()","4f67756d":"# Create metric function","6afdfc41":"# Set Parameters","4943048d":"# Predict and Create Submission file","3937e401":"# Create Model","88fcc62b":"# Training","b6e9bc0c":"# Create Validation Set\ncontained classes of VAL_CLASS (choiced random)","8ca69fa6":"# Create One Hot Encoder","c73204fc":"# Create Train Data  Generator","e7ff0942":"# Load Packages"}}