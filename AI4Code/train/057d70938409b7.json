{"cell_type":{"47fa953b":"code","0d075e8d":"code","ad9f2daa":"code","07195b8b":"code","23093142":"code","fc16e31f":"code","5be9d702":"code","a7d6896c":"code","f0027ff4":"code","b56688c0":"code","6095d82c":"code","aa7ff0b6":"code","4ec156b9":"code","1b875ab2":"code","d4cc1487":"code","67097e0b":"code","8d482ef6":"code","ffa2c89d":"code","7f0cda5d":"code","0caf0bbb":"code","89517712":"code","6b8e9a83":"code","737e32e2":"code","c426c032":"code","842ec2e1":"code","d4b61e7f":"code","3fb4f93d":"code","b87fcce7":"code","fad3d748":"code","14202533":"code","da1b6ea2":"code","58610618":"code","a1df3998":"code","8221ab17":"code","ab9d4024":"code","6e32fa8b":"code","b7037686":"code","49cd32b7":"code","609a41f8":"code","f7bb92b8":"code","a5ff52b3":"code","393a573e":"code","93a80167":"code","ad135466":"code","9287ad93":"code","6bbf500a":"code","6c6048ba":"code","8129d65e":"code","9629c54e":"code","c57f815e":"code","5b750a11":"code","48925bb9":"code","a2d7d395":"code","33851876":"code","8f49270a":"code","d45d32f0":"code","a7eb98a7":"code","48c7ac03":"code","a313a031":"code","6f61e756":"code","207ddb9d":"code","6edcae0a":"code","6cc152c0":"code","070e9a54":"code","2f6c86b5":"code","df2fb626":"code","57803b3d":"code","66094517":"code","68c367aa":"code","dde1ca6e":"code","b31eb5c7":"code","16d38b93":"code","d3ec5e9d":"code","5900649a":"code","84d6e261":"code","dce133dd":"code","22f1ef9e":"code","71b686f1":"code","889fab8c":"code","bc6ac634":"code","16123b19":"markdown","1c3b51c0":"markdown","bb87b8c5":"markdown","bbb11f25":"markdown","2723f2b7":"markdown","8394a76f":"markdown","74dbee0f":"markdown","4f788fd4":"markdown","1af14ce6":"markdown","3df7b1fc":"markdown","16362a04":"markdown","10f5f946":"markdown","b5961793":"markdown","706016ee":"markdown","1514d0cc":"markdown","c90778dd":"markdown","631bed0a":"markdown","24cfbae9":"markdown","dd7f7916":"markdown","9b5e399c":"markdown"},"source":{"47fa953b":"#import library\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV, cross_val_predict\nimport string\nimport warnings\nwarnings.filterwarnings('ignore')","0d075e8d":"#current data directory and data file name\nimport os\nfor root, dir, filenames in os.walk('input'):\n    for filename in filenames:\n        print(os.path.join(root, filename))","ad9f2daa":"#import library\nimport pandas as pd\n# Read training data\ntrain_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv')\n# Read testing data\ntest_data = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')","07195b8b":"#look at first 5 rows of data\nprint(train_data.head())\n#shape of data\nprint('train shape', train_data.shape)\nprint('test shape', test_data.shape)","23093142":"#get variables\nprint(train_data.columns.tolist())\n#missing values\nprint(train_data.isnull().sum().sort_values(ascending = False))\n#percentage\nprint(train_data.isnull().mean().sort_values(ascending = False))                         ","fc16e31f":"target = train_data['SalePrice']","5be9d702":"full_data = pd.concat([train_data, test_data], sort = True).reset_index(drop = True)#together\ncombine_data = [train_data,test_data]\nfull_data.info()","a7d6896c":"column_name_noncategorical = full_data.select_dtypes(exclude=['object']).columns.tolist()\ncolumn_name_noncategorical.remove('SalePrice')\ncolumn_name_noncategorical","f0027ff4":"column_name_categorical = full_data.select_dtypes(include=['object']).columns.tolist()\ncolumn_name_categorical","b56688c0":"print(len(column_name_noncategorical), len(column_name_categorical))","6095d82c":"full_data_describe = full_data.select_dtypes(exclude=['object']).describe(percentiles = [.25, .5, .75,.90])\nfull_data_describe","aa7ff0b6":"full_data_describe[7:8]","4ec156b9":"full_data_describe[8:9]","1b875ab2":"fig = plt.figure(figsize=(18,24))\nfor index,col in enumerate(column_name_noncategorical):\n    plt.subplot(10,4,index+1)\n    sns.distplot(full_data[column_name_noncategorical].loc[:,col].dropna())\nfig.tight_layout(pad=1.0)","d4cc1487":"column_name_distinct = []\ncolumn_name_continuous = []\nfor col_name in column_name_noncategorical:\n    if len(full_data[column_name_noncategorical][col_name].unique().tolist())< 15:\n        column_name_distinct.append(col_name)\n    else:\n        column_name_continuous.append(col_name)\nprint(column_name_distinct, column_name_continuous)","67097e0b":"fig = plt.figure(figsize=(18,24))\nfor index,col in enumerate(column_name_noncategorical):\n    plt.subplot(10,4,index+1)\n    sns.boxplot(full_data[column_name_noncategorical].loc[:,col].dropna())\nfig.tight_layout(pad=1.0)","8d482ef6":"fig = plt.figure(figsize=(18,24))\nfor index,col in enumerate(column_name_noncategorical):\n    plt.subplot(10,4,index+1)\n    sns.scatterplot(x = train_data[column_name_noncategorical].iloc[:,index].dropna(), y = target, data = train_data[column_name_noncategorical].dropna())\nfig.tight_layout(pad=1.0)","ffa2c89d":"#list for outlayers:\ncolumn_name_noncategorical[33]","7f0cda5d":"#for categorical values\nfig = plt.figure(figsize=(18,24))\nfor index,col in enumerate(column_name_categorical):\n    plt.subplot(11,4,index+1)\n    sns.countplot(x = full_data[column_name_categorical].iloc[:,index], data = full_data[column_name_categorical].dropna())\nfig.tight_layout(pad=1.0)","0caf0bbb":"column_name_cat_noncategorical = []\ncolumn_name_cat_categorical = []\nfor col_name in column_name_categorical:\n    if len(full_data[column_name_categorical][col_name].unique().tolist())< 30:\n        column_name_cat_categorical.append(col_name)\n    else:\n        column_name_continuous.append(col_name)\nprint(column_name_cat_noncategorical, column_name_continuous)","89517712":"plt.figure(figsize=(20,20))\nsns.heatmap(full_data[column_name_noncategorical].corr(), mask = full_data[column_name_noncategorical].corr() <0.8, linewidth=0.5, cmap='Reds')","6b8e9a83":"full_data_corr = full_data[column_name_noncategorical].corr().abs().unstack().sort_values(kind = \"quicksort\", ascending = False).to_frame()\nfull_data_corr.iloc[38:46]# 37 columns with corr of 1","737e32e2":"#important correlation with target\ntarget_corr = full_data.corr()['SalePrice'].abs().sort_values(ascending = False)[1:]\ntarget_corr.iloc[0:5]","c426c032":"np.unique(train_data['YrSold']).tolist()","842ec2e1":"train_data.shape","d4b61e7f":"train_data = train_data.drop('SalePrice', axis = 1)\ntrain_data.shape","3fb4f93d":"drop_col = []\nfor col in column_name_noncategorical:\n    if train_data[col].value_counts().iloc[0] \/ len(train_data[col].dropna()) > 0.9:\n        drop_col.append(col)","b87fcce7":"for col in column_name_categorical:\n    if train_data[col].value_counts().iloc[0] \/ len(train_data[col].dropna()) > 0.9:\n        drop_col.append(col)","fad3d748":"train_data = train_data.drop(drop_col, axis = 1) ","14202533":"test_data = test_data.drop(drop_col, axis = 1)","da1b6ea2":"train_data.shape","58610618":"test_data.shape","a1df3998":"full_data_null_precentage = full_data.isnull().sum().sort_values(ascending = False).divide(full_data.shape[0])\nfull_data_null_precentage","8221ab17":"del_list = []# list for delete, when there is too many missing values\nfill_Auto = []# list for fill directly, when missing value precentage between 0.2 and 0.75\nfill_other = []#other columns\nfull_data = pd.concat([train_data, test_data], sort = True).reset_index(drop = True)#together\nfull_data_columns = full_data.columns.tolist()\nfor col in full_data_columns:\n    if full_data_null_precentage[col] >0.75:\n        del_list.append(col)\n    if 0.2<full_data_null_precentage[col] <=0.75:\n        fill_Auto.append(col)\n    if 0<full_data_null_precentage[col]<=0.2:\n        fill_other.append(col)\ndel_list,fill_Auto,fill_other","ab9d4024":"full_data.drop(del_list, axis = 1, inplace = True)\nfull_data.shape","6e32fa8b":"missing_list = []\nfull_data_missing = full_data.isnull().sum().sort_values(ascending = False)\nfull_data_columns = full_data.columns.tolist()\nfor col in full_data_columns:\n    if full_data_missing[col] !=0:\n        missing_list.append(col)\nmissing_list","b7037686":"non_object_list = full_data[missing_list].select_dtypes(exclude=['object']).columns.tolist()#not object list\nnon_object_list","49cd32b7":"full_data_copy = full_data.copy()\nfull_data_copy['SalePrice'] = target\ntarget_corr = full_data_copy.corr()['SalePrice'].abs().sort_values(ascending = False)[1:]\ntarget_corr","609a41f8":"full_data_copy.fillna('None')\nfull_data_copy = full_data_copy.apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index))\ntarget_corr = full_data_copy.corr()['SalePrice'].abs().sort_values(ascending = False)[1:]\nsort_name = target_corr.index.tolist()\nsort_name[0:6]","f7bb92b8":"full_data_copy.GarageFinish","a5ff52b3":"full_data.GarageFinish","393a573e":"distinct_name = []\ncontinuous_name = []\nfor name in non_object_list:\n    if name in column_name_distinct:\n        distinct_name.append(name)\n        if name in fill_Auto:\n            full_data[name] = full_data[name].fillna(0)\n        else:\n            full_data[name] = full_data.groupby(sort_name[0])[name].apply(lambda x: x.fillna(x.mode()))\n            full_data[name] = full_data.fillna(full_data[name].mode())\n    if name in column_name_continuous:\n        continuous_name.append(name)\n        if name in fill_Auto:\n            full_data[name] = full_data.groupby(sort_name[0])[name].apply(lambda x: x.fillna(x.mean()))\n        else:\n            full_data[name] = full_data.groupby(sort_name[0])[name].apply(lambda x: x.fillna(x.mean()))","93a80167":"distinct_name","ad135466":"full_data[non_object_list].isnull().sum().sort_values(ascending = False)","9287ad93":"object_list = full_data[missing_list].select_dtypes(include=['object']).columns.tolist()# object list\nobject_list","6bbf500a":"# since no strange items(like names), directly fill the data.\nsort_name = target_corr.index.tolist()\nfor name in object_list:\n    if name in fill_Auto:\n        full_data[name] = full_data[name].fillna('None')  \n    else:\n        full_data[name] = full_data.groupby(sort_name[0])[name].apply(lambda x: x.fillna(x.mode()[0]))","6c6048ba":"full_data.isnull().sum().sort_values(ascending = False)","8129d65e":"#https:\/\/scikit-learn.org\/stable\/auto_examples\/impute\/plot_iterative_imputer_variants_comparison.html\n#we can try methods from this website. Now just fill the data","9629c54e":"full_data.drop('Id', axis = 1, inplace = True)\nfull_data.isnull().sum().sort_values(ascending = False)","c57f815e":"non_object_list = full_data.select_dtypes(exclude=['object']).columns.tolist()\ndistinct_name = []\ncontinuous_name = []\nfor name in non_object_list:\n    if name in column_name_distinct:\n        distinct_name.append(name)\n    if name in column_name_continuous:\n        continuous_name.append(name)\nobject_list = full_data.select_dtypes(include=['object']).columns.tolist()\nfull_data['SalePrice'] = target\nfull_data.shape","5b750a11":"print(object_list)\nprint(distinct_name)\nprint(continuous_name)\nprint(len(object_list+distinct_name+continuous_name))","48925bb9":"full_data.head()","a2d7d395":"#Drop outliers rows in train_data\ntrain_data  = full_data.loc[:1459]\ntest_data = full_data.loc[1460:]\ntest_data.drop('SalePrice', axis = 1, inplace = True)\ndata_combine = [train_data, test_data]\ntrain_data.shape","33851876":"#ctrl C + ctrl V\ntrain_data = train_data.drop(index = train_data['1stFlrSF'][train_data['1stFlrSF'] > 4000].index)\ntrain_data = train_data.drop(index = train_data[train_data['BsmtFinSF1'] > 5000].index)\ntrain_data = train_data.drop(index = train_data[train_data['BsmtFinSF2'] > 1400].index)\ntrain_data = train_data.drop(index = train_data[train_data['EnclosedPorch'] > 500].index)\ntrain_data = train_data.drop(index = train_data[train_data['GrLivArea'] > 4500].index)\ntrain_data = train_data.drop(index = train_data[train_data['LotFrontage'] > 300].index)\ntrain_data = train_data.drop(index = train_data[train_data['TotalBsmtSF'] > 5000].index)\ntrain_data = train_data.drop(index = train_data[train_data['WoodDeckSF'] > 750].index)","8f49270a":"train_data.reset_index(inplace = True)","d45d32f0":"train_data","a7eb98a7":"#distinct numerical value does not need labelEncoder\nfor dataset in data_combine:\n    dataset[distinct_name] = dataset[distinct_name].astype(int)\ntrain_data.head(2)","48c7ac03":"train_data[object_list] = train_data[object_list].apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index))\ntest_data[object_list] = test_data[object_list].apply(lambda series: pd.Series(\n    LabelEncoder().fit_transform(series[series.notnull()]),\n    index=series[series.notnull()].index))\ntrain_data","a313a031":"test_data.head()","6f61e756":"train_data.head(2)","207ddb9d":"OHEncoder_train = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOHEncoder_test = OneHotEncoder(handle_unknown='ignore', sparse=False)\noh_list = object_list + distinct_name\nlow_cardinality_cols = [col for col in oh_list if train_data[col].nunique() < 5]\nhigh_cardinality_cols = [col for col in oh_list if train_data[col].nunique() >= 5]\nprint(low_cardinality_cols,high_cardinality_cols)","6edcae0a":"train_data.isnull().sum().sort_values(ascending = False)","6cc152c0":"Onehog_Dataframe = []\nfor col in low_cardinality_cols:\n    Onehog_list = OneHotEncoder().fit_transform(train_data[col].values.reshape(-1, 1)).toarray()\n    num_col = train_data[col].nunique()\n    cols = ['{}_{}'.format(col, num_col) for num_col in range(1, num_col + 1)]\n    Onehog_df = pd.DataFrame(Onehog_list, columns=cols)\n    Onehog_df.index = train_data.index\n    Onehog_Dataframe.append(Onehog_df)\n\ntrain_data = pd.concat([*Onehog_Dataframe,train_data[continuous_name],train_data[['SalePrice']],train_data[high_cardinality_cols]], axis = 1)\ntrain_data.tail()","070e9a54":"train_data.shape","2f6c86b5":"train_data.tail()","df2fb626":"train_data.dropna(axis = 0, inplace = True)\ntrain_data.tail()","57803b3d":"train_data.isnull().sum().sort_values(ascending = False)","66094517":"Onehog_Dataframe_test = []\nfor col in low_cardinality_cols:\n    Onehog_list = OneHotEncoder().fit_transform(test_data[col].values.reshape(-1, 1)).toarray()\n    num_col = test_data[col].nunique()\n    cols = ['{}_{}'.format(col, num_col) for num_col in range(1, num_col + 1)]\n    Onehog_df = pd.DataFrame(Onehog_list, columns=cols)\n    Onehog_df.index = test_data.index\n    Onehog_Dataframe_test.append(Onehog_df)\n    \n#test_data_onehog = pd.DataFrame(OHEncoder_test.fit_transform(test_data[low_cardinality_cols]), index = range(1460,2919))\ntest_data = pd.concat([*Onehog_Dataframe_test,test_data[continuous_name],test_data[high_cardinality_cols]], axis = 1)\ntest_data.tail()","68c367aa":"test_data_columns = test_data.columns.tolist()\ntrain_data_columns = train_data.columns.tolist()\ntrain_data_columns.remove('SalePrice')\nfor name in test_data_columns:\n    if name not in train_data_columns:\n        test_data.drop(name, axis = 1, inplace = True)\nfor name in train_data_columns:\n    if name not in test_data_columns:\n        train_data.drop(name, axis = 1, inplace = True)\ntrain_data.shape","dde1ca6e":"test_data.shape","b31eb5c7":"from sklearn.metrics import mean_squared_error, mean_absolute_error\nfrom xgboost import XGBRegressor\nfrom sklearn import ensemble\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n","16d38b93":"from sklearn.preprocessing import RobustScaler\ncolumns = continuous_name\ntrain_data[columns] = RobustScaler(quantile_range = (15,85)).fit_transform(train_data[columns])\ntest_data[columns] = RobustScaler(quantile_range = (15,85)).fit_transform(test_data[columns])","d3ec5e9d":"gbr = ensemble.GradientBoostingRegressor(learning_rate=0.02, n_estimators=2000,\n                                           max_depth=5, min_samples_split=2,\n                                           loss='ls', max_features=35)\n#gbr.fit(train_data.drop('SalePrice',axis = 1),train_data['SalePrice'])","5900649a":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score,mean_absolute_error","84d6e261":"#StratifiedKFold\ntrain_X = train_data.drop('SalePrice',axis = 1)\ntrain_y = train_data['SalePrice']\nnum = 0\nN = 5\nsaleprice_lis = []\naccuracy_score_lis = []\nskf = StratifiedKFold(n_splits=N, random_state=5, shuffle=False)\nsaleprice = pd.DataFrame(np.zeros((len(test_data), N)), columns=['Fold_{}'.format(i) for i in range(1, N + 1)])\nfor train_index, test_index in skf.split(train_X, train_y):\n    num +=1\n    X_train1, X_test1 = train_X.iloc[train_index,:], train_X.iloc[test_index,:]\n    y_train1, y_test1 = train_y[train_index], train_y[test_index]\n    gbr.fit(X_train1, y_train1)\n    \n    saleprice.loc[:, 'Fold_{}'.format(num)] = gbr.predict(test_data)\n    prediction = gbr.predict(X_test1)\n    #auc score\n    auc_score = mean_absolute_error(y_test1, prediction)\n    accuracy_score_lis = accuracy_score_lis + [auc_score]\n    print(\"MAE score: \", auc_score)","dce133dd":"train_data.shape","22f1ef9e":"test_data.shape","71b686f1":"submit = saleprice.sum(axis=1) \/ N\nsubmission = pd.DataFrame({'Id': np.array(list(range(1461,2920))),\n                           'SalePrice': submit})\nsubmission.to_csv(\"..\/..\/kaggle\/working\/submission.csv\", index=False)","889fab8c":"submission","bc6ac634":"#13878.07430\n#imporvement methods:\n#gridsearchCV\n#test and add more prediction methods\n#fillNA with best correlation for each columns","16123b19":"<a id=\"2\"><\/a>\n2. explore data","1c3b51c0":"<a id=\"3\"><\/a>\n3. null value","bb87b8c5":"RobustScaler Definition:\n    \nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.RobustScaler.html\n\n(need to add test model preformance)","bbb11f25":"numeric data","2723f2b7":"Then, fill the missing values","8394a76f":"<a id=\"1\"><\/a>\n1.prepare data","74dbee0f":"So there is no such columns like people's name, house's name, good!\n\nAlso there is many columns with many 0. Then let us look at correlation matrix","4f788fd4":"Read training data","1af14ce6":"basic data infomation","3df7b1fc":"labelEncoder, convert categorical value to numeric","16362a04":"<h1>Housing Prices Competition for Kaggle Learn Users<\/h1>\n<h3><a> <href=\"https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data?select=data_description.txt\"> Link to Kaggle<\/a><\/h3>\n    <h3>Jupyter book author: Tao Shan <\/h3>\n\n1. [prepare data](#1) \n2. [explore data](#2)\n3. [null value](#3)\n4. [data engineering](#4)\n5. [predicting model and submit solution](#5)\n\n","10f5f946":"Low correlation. We need to find a high one","b5961793":"Why do complicated onehog? because there is differences between onehog columns in train & test data","706016ee":"From outlier analysis above,\n\ndelete\n\n'1stFlrSF' > 4000\n\n'BsmtFinSF1'>5000\n\n'BsmtFinSF2' > 1400\n\n'EnclosedPorch'>500\n\n'GrLivArea' > 4500\n\n'LotFrontage' > 300\n\n'TotalBsmtSF' > 5000\n\n'WoodDeckSF' > 750)","1514d0cc":"Drop columns with too many 0's","c90778dd":"<a id=\"4\"><\/a>\n4. data engineering \n\n> from https:\/\/www.kaggle.com\/c\/home-data-for-ml-course\/data, ","631bed0a":"https:\/\/www.sciencedirect.com\/science\/article\/pii\/S0895435618308710\n\nFrom the website, 25 to 30% is the maximum missing values are allowed, we use 25%","24cfbae9":"Outliers:\n \n* '1stFlrSF' > 4000\n* 'BsmtFinSF1'>5000\n* 'BsmtFinSF2' > 1400\n* 'EnclosedPorch'>500\n* 'GrLivArea' > 4500\n* 'LotFrontage' > 300\n* 'TotalBsmtSF' > 5000\n* 'WoodDeckSF' > 750\n\nAlso there is many columns with many 0","dd7f7916":"We removes columns that mostly 0 for noncategorical variables, it is not helpful to our prediction.\n","9b5e399c":"<a id=\"5\"><\/a>\n5.predicting model and submit solution"}}