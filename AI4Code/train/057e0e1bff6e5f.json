{"cell_type":{"0acee302":"code","0d8b9385":"code","33d248a1":"code","df85e70c":"code","5c415c6c":"code","b2a665ee":"code","d31b5677":"code","ff48f33d":"code","8af9b519":"code","d0b7675e":"code","251a5e0d":"code","b743ed55":"code","fc0d2368":"code","5c94d01f":"code","931521b6":"code","353553c6":"code","418e2071":"code","f7989a56":"code","637046a5":"code","2b58dc26":"code","92e70383":"code","e0f02dc5":"code","2a216e40":"code","be08f938":"code","4fa0e62e":"markdown","1476af51":"markdown","e8915e62":"markdown","a0e98e75":"markdown","e7f0a40c":"markdown","a6c4c576":"markdown","74bd8800":"markdown","ef3fcbe7":"markdown"},"source":{"0acee302":"import os \nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\n\nimport torch\nimport torch.utils as utils\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nimport matplotlib.pyplot as plt\n%matplotlib inline","0d8b9385":"if torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\nprint(f\"# Using device: {device}\")","33d248a1":"torch.cuda.empty_cache()","df85e70c":"class dehazer(): \n    \n    def __init__(self, IMG_SIZE,LABEL_DIR,LABEL_NAME):\n\n        self.IMG_SIZE = IMG_SIZE\n        self.LABEL_DIR = LABEL_DIR\n        self.LABEL_NAME = LABEL_NAME\n    \n        self.training_data=[]\n    \n    def make_training_data(self):\n        \n        NUM_IMAGES = len(os.listdir(self.LABEL_DIR))\n        \n        for f in tqdm(range(1, NUM_IMAGES+1)):\n            f = \"{:02d}\".format(f) + '_' + self.LABEL_NAME + '.png'\n            path=os.path.join(self.LABEL_DIR,f)\n            img=cv2.imread(path)\n            img=cv2.resize(img,(self.IMG_SIZE,self.IMG_SIZE))\n            self.training_data.append(np.array(img))\n\n            \n        np.save(f'{self.LABEL_NAME}.npy',self.training_data)","5c415c6c":"REBUILD_DATA=True\n        \nIMG_SIZE=256\ngt_dir='..\/input\/hazing-images-dataset-cvpr-2019\/GT' \nhazy_dir='..\/input\/hazing-images-dataset-cvpr-2019\/hazy' \n    \nif (REBUILD_DATA):\n    dehazing=dehazer(IMG_SIZE, gt_dir, 'GT')\n    dehazing.make_training_data()\n    \n    dehazing=dehazer(IMG_SIZE, hazy_dir, 'hazy')\n    dehazing.make_training_data()","b2a665ee":"patch = np.load('GT.npy',allow_pickle=True)\nmask = np.load('hazy.npy',allow_pickle=True)\n\nlen(patch), len(mask)","d31b5677":"patch.shape, mask.shape","ff48f33d":"for i in range(0,len(patch),5):\n    \n    fig=plt.figure(figsize=(15, 5))\n    \n    ax = plt.subplot(131)\n    plt.imshow(patch[i])\n    \n    ax = plt.subplot(132)\n    plt.imshow(mask[i])\n    plt.show()","8af9b519":"IMG_SIZE = 256\n\nEPOCHS = 1000\nbatch_size = 1\nlearning_rate = 0.0005","d0b7675e":"patch_loader = torch.utils.data.DataLoader(dataset=patch,batch_size=batch_size,shuffle=False)\n\nfor data in patch_loader:\n    print(data.size())\n    print(type(data))\n    break","251a5e0d":"X_orig=torch.Tensor([patch[i] for i in range(len(patch))])\nX_hazy=torch.Tensor([mask[i] for i in range(len(mask))])\n\nX_orig=X_orig\/255\nX_hazy=X_hazy\/255","b743ed55":"print('X_orig: ',X_orig.size())\n\nX_orig_T=np.transpose(X_orig,(0,3,1,2))\nX_hazy_T=np.transpose(X_hazy,(0,3,1,2))\nprint('X_orig_T: ',X_orig_T.shape)\n\nX_orig_flat=X_orig_T.reshape(-1,1,IMG_SIZE,IMG_SIZE)\nX_hazy_flat=X_hazy_T.reshape(-1,1,IMG_SIZE,IMG_SIZE)\nprint('X_orig_flat: ',X_orig_flat.shape)","fc0d2368":"class Encoder(nn.Module):\n    def __init__(self):\n        super(Encoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.Conv2d(1,32,3,padding=1),   # batch x 32 x 256 x 256\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),             \n                        nn.Conv2d(32,32,3,padding=1),   # batch x 32 x 256 x 256\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.Conv2d(32,64,3,padding=1),  # batch x 64 x 256 x 256\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.Conv2d(64,64,3,padding=1),  # batch x 64 x 256 x 256\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.MaxPool2d(2,2)   # batch x 64 x 128 x 128\n        )\n        self.layer2 = nn.Sequential(\n                        nn.Conv2d(64,128,3,padding=1),  # batch x 128 x 128 x 128\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.Conv2d(128,128,3,padding=1),  # batch x 128 x 128 x 128\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.MaxPool2d(2,2),\n                        nn.Conv2d(128,256,3,padding=1),  # batch x 256 x 64 x 64\n                        nn.ReLU()\n        )\n                \n    def forward(self,x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.view(batch_size, -1)\n        return out\n    \nencoder = Encoder().cuda()","5c94d01f":"class Decoder(nn.Module):\n    def __init__(self):\n        super(Decoder,self).__init__()\n        self.layer1 = nn.Sequential(\n                        nn.ConvTranspose2d(256,128,3,2,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.ConvTranspose2d(128,128,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(128),\n                        nn.ConvTranspose2d(128,64,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64),\n                        nn.ConvTranspose2d(64,64,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(64)\n        )\n        self.layer2 = nn.Sequential(\n                        nn.ConvTranspose2d(64,32,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.ConvTranspose2d(32,32,3,1,1),\n                        nn.ReLU(),\n                        nn.BatchNorm2d(32),\n                        nn.ConvTranspose2d(32,1,3,2,1,1),\n                        nn.ReLU()\n        )\n        \n    def forward(self,x):\n        out = x.view(batch_size,256,64,64)\n        out = self.layer1(out)\n        out = self.layer2(out)\n        return out\n\ndecoder = Decoder().cuda()","931521b6":"# Check output of encoder-decoder\n\ntrain_orig_loader = torch.utils.data.DataLoader(dataset=X_orig_flat,batch_size=batch_size,shuffle=False)\ntrain_hazy_loader = torch.utils.data.DataLoader(dataset=X_hazy_flat,batch_size=batch_size,shuffle=False)\n\nfor train_orig, train_hazy in zip(train_orig_loader, train_hazy_loader):\n    orig_image = Variable(train_orig).cuda()\n    hazy_image = Variable(train_hazy).cuda()\n    \n    encoder_op = encoder(hazy_image)\n    output = decoder(encoder_op)\n    \n    print('Image Dim: ',orig_image.size())\n    print('Hazy Image Dim: ',hazy_image.size())\n    print('Encoder Output Dim: ',encoder_op.size())\n    print('Output Dim: ',output.size())\n    break","353553c6":"# In order to use multi parameters with one optimizer, concat parameters after changing into list\n\nparameters = list(encoder.parameters())+ list(decoder.parameters())\nloss_func = nn.MSELoss()\noptimizer = torch.optim.Adam(parameters, lr=learning_rate)\nlosses=[]\nX_orig1=X_orig\n\nfor epoch in tqdm(range(EPOCHS)):\n    \n    rand_idx=torch.randperm(X_orig1.size()[0])\n    X_orig_iter=X_orig[rand_idx]\n    X_hazy_iter=X_hazy[rand_idx]\n\n    X_orig_iter1=np.transpose(X_orig_iter,(0,3,1,2))\n    X_hazy_iter1=np.transpose(X_hazy_iter,(0,3,1,2))\n\n    X_orig_iter2=X_orig_iter1.reshape(-1,1,IMG_SIZE,IMG_SIZE)\n    X_hazy_iter2=X_hazy_iter1.reshape(-1,1,IMG_SIZE,IMG_SIZE)\n\n    train_orig_loader = torch.utils.data.DataLoader(dataset=X_orig_iter2,batch_size=batch_size,shuffle=False)\n    train_hazy_loader = torch.utils.data.DataLoader(dataset=X_hazy_iter2,batch_size=batch_size,shuffle=False)\n\n    for train_orig, train_hazy in zip(train_orig_loader, train_hazy_loader):\n        orig_image = Variable(train_orig).cuda()\n        hazy_image = Variable(train_hazy).cuda()\n        \n        optimizer.zero_grad()\n\n        encoder_op = encoder(hazy_image)\n        output = decoder(encoder_op)\n        \n        loss=loss_func(output,orig_image)\n        loss.backward()\n        optimizer.step()\n\n    losses.append(loss)\n    \ntorch.save([encoder,decoder],'dehaze_autoencoder.pkl')","418e2071":"plt.title('MSE Loss Plot')\nplt.xlabel('Epochs')\nplt.ylabel('Value')\nplt.plot(losses)\nplt.show()","f7989a56":"encoder, decoder = torch.load('dehaze_autoencoder.pkl')","637046a5":"train_hazy_loader = torch.utils.data.DataLoader(dataset=X_hazy_flat,batch_size=batch_size,shuffle=False)\n\ndehazed_output=[]\nfor train_hazy in tqdm(train_hazy_loader):\n    hazy_image = Variable(train_hazy).cuda()\n    \n    encoder_op = encoder(hazy_image)\n    output = decoder(encoder_op)\n    \n    output=output.cpu()\n    output=output.detach()\n    dehazed_output.append(output)","2b58dc26":"X_dehazed=dehazed_output\n\nX_dehazed=torch.stack(X_dehazed)\nprint(X_dehazed.size())\n\nX_dehazed=X_dehazed.view(-1,1,256,256)\nprint(X_dehazed.size())\n\nX_dehazed=X_dehazed.view(-1,3,256,256)\nprint(X_dehazed.size())\n\nX_dehazed=X_dehazed.permute(0,2,3,1)\nprint(X_dehazed.shape)","92e70383":"for i in range(0,len(X_orig),10):\n    \n    fig=plt.figure(figsize=(15, 5))\n    ax = plt.subplot(131)\n    plt.title('Original Image')\n    plt.imshow(X_orig[i])\n    \n    ax = plt.subplot(132)\n    plt.title('Hazy Image')\n    plt.imshow(X_hazy[i])\n    \n    ax = plt.subplot(133)\n    plt.title('Dehazed Image')\n    plt.imshow(X_dehazed[i])\n    plt.show()","e0f02dc5":"X_dehazed=X_dehazed.numpy()\nX_orig=X_orig.numpy()\n\nX_orig=X_orig*255\nX_dehazed=X_dehazed*255\n\nref_mat=X_orig\nres_mat=X_dehazed\n\nprint(ref_mat.shape)\nprint(res_mat.shape)","2a216e40":"ref_mat = ref_mat.astype('float') \/ 255.0\nres_mat = res_mat.astype('float') \/ 255.0\n\ndef output_psnr_mse(img_orig, img_out):\n    squared_error = np.square(img_orig - img_out)\n    mse = np.mean(squared_error)\n    psnr = 10 * np.log10(1.0 \/ mse)\n    return psnr\n\ndef mean_psnr_srgb(ref_mat, res_mat):\n    n_blk, h, w, c = ref_mat.shape\n    mean_psnr = 0\n    for b in range(n_blk):\n        ref_block = ref_mat[b, :, :, :]\n        res_block = res_mat[b, :, :, :]\n        ref_block = np.reshape(ref_block, (h, w, c))\n        res_block = np.reshape(res_block, (h, w, c))\n        psnr = output_psnr_mse(ref_block, res_block)\n        mean_psnr += psnr\n    return mean_psnr \/ n_blk\n\n#PSNR\nmean_psnr = mean_psnr_srgb(ref_mat, res_mat)\nprint('mean_psnr:')\nprint(mean_psnr)","be08f938":"from skimage.metrics import structural_similarity as ssim\n\ndef mean_ssim_srgb(ref_mat, res_mat):\n    n_blk, h, w, c = ref_mat.shape\n    mean_ssim = 0\n    for b in range(n_blk):\n        ref_block = ref_mat[b, :, :, :]\n        res_block = res_mat[b, :, :, :]\n        ref_block = np.reshape(ref_block, (h, w, c))\n        res_block = np.reshape(res_block, (h, w, c))\n        ssim1 = ssim(ref_block, res_block, gaussian_weights=True, use_sample_covariance=False,\n                     multichannel=True)\n        mean_ssim += ssim1\n    return mean_ssim \/ n_blk\n\n# SSIM\nmean_ssim = mean_ssim_srgb(ref_mat, res_mat)\nprint('mean_ssim:')\nprint(mean_ssim)","4fa0e62e":"### Data Processing\n\nWe create numpy arrays for hazy and non-hazy images while reducing image size to perform faster computations and training.\n\nWe also make sure that the images are concatenated in the same order to create (hazy, non-hazy) image pairs.","1476af51":"### Visualizing Data \nLet's look at a few pairs of hazy and non-hazy images.","e8915e62":"### Model Architecture: Encoder-Decoder \n\nThink about the basic difference between machine learning and deep learning. In the former, it's the data scientist\/model maker who builds the features which are then passed on to the model. However, in the latter, we delegate the task of creating features as well to the machine apart from learning the patterns in data.\n\nSo basically, any deep learning problem can be broadly thought of as:\n\n<code>Input (Tabular, Image, Text etc.) -> Encoder for converting input to numerical representations -> Learning -> Prediction -> Decoder for converting predictions to original form -> Output (Tabular, Image, Text etc.)<\/code>\n\nHowever, it's possible that there might be a few steps that are not involved in the above pipeline. For example - if your data is purely numeric, you might not have to encode and decode it, but for unstructured input like images, text or others, you will have to. This is the basic idea behind Encoder-Decoder Architecture.\n\nConsider a machine translation problem of translating sentences from English to French. Here, the encoder function maps the input space (English text) to a latent space, followed by a decoder function that maps the latent space to a different target space (French text). \n\n**In our case, the input space is hazy images and the output space is non-hazy\/ground-truth images.**","a0e98e75":"### Loss function and Optimizer\n\n**Loss Function:** Mean Squared Error (MSE) Loss\n\nAs our objective is to minimize the difference between decoder output (dehazed image) and ground-truth, we aim to minimize the MSE between two images.\n    \n**Optimizer**: Adam (Adaptive Moment Estimation), commonly seen as combination of Adagrad and Momentum\n\n1. Adaptive learning rate for different parameters\n2. Faster converging through Momentum, which results in accelerated gradients \n    ","e7f0a40c":"### EVALUATION METRICS\n\n### PSNR (Peak signal-to-noise ratio)\n\nIt is an expression for the ratio between maximum possible power of a signal and the power of corrupting noise that affects the fidelity of its representation. It is usually expressed in terms of logarithmic decibel scale, due to its wide dynamic range. It does not have a range. **The higher it is, the better.**\n\n### Structural Similarity (SSIM) Index. \n\nSSIM is used for measuring the similarity between two images. The SSIM Index is a method for predicting the perceived quality of digital television and cinematic pictures, as well as other kinds of digital images and videos. It is a full reference metric, which means that the measurement or prediction of image quality is based\non an initial uncompressed or distortion-free image as reference. It ranges between -1 to 1 and SSIM equals 1 only when the images are identical. **The closer to 1, the better.**","a6c4c576":"## Dataset: Dense-Haze\n\nDense-Haze dataset contains 55 pairs of hazy and non-hazy images. It is used as a benchmark for image dehazing tasks.\n\nReference: https:\/\/data.vision.ee.ethz.ch\/cvl\/ntire19\/\/dense-haze\/","74bd8800":"### MODEL HYPERPARAMETERS","ef3fcbe7":"Since the number of images are fairly less (55 in total), we will train it on complete data.\n\nAs the number of images are less, we will train keeping a batch size of 1 i.e. **Stochastic Gradient Descent**. The learning might be noisy and it might take longer to train. And so, we will keep the learning rate fairly low and train it on sufficiently large number of epochs."}}