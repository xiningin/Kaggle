{"cell_type":{"20792095":"code","72c2a3ee":"code","b780b5f2":"code","f2c24c73":"code","def0f189":"code","29b8272c":"code","bd1c763c":"code","d86bcefa":"code","b6a25c78":"code","21808e75":"code","5c1c0cb7":"code","7162068c":"code","224909e3":"code","4c09dde4":"code","577755b9":"code","50f6e578":"code","866e3768":"code","0a5fa142":"code","87202f20":"code","57b880a4":"code","f4f74363":"code","ad964065":"code","5cfcfdea":"code","69f3bdb3":"code","e8c71495":"code","1684253f":"code","d5406c9f":"code","c5d62a9b":"code","fa8bb7c4":"code","67324ccb":"code","86faba7a":"code","58cb3550":"code","dc81fd34":"code","59ce3604":"code","20267a4b":"code","85cf84ca":"code","a5265b49":"code","c9b4f1e8":"markdown","9000435c":"markdown"},"source":{"20792095":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","72c2a3ee":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nimport warnings\nwarnings.filterwarnings('ignore')","b780b5f2":"import pandas as pd\nimport numpy as np\nfrom mlxtend.feature_selection import SequentialFeatureSelector as sfs\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nfrom sklearn.metrics import confusion_matrix,accuracy_score,recall_score,roc_auc_score,roc_curve, precision_score,f1_score,auc,precision_recall_curve,classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.preprocessing import StandardScaler\n%matplotlib inline","f2c24c73":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score,classification_report\nle=LabelEncoder()\ndt=DecisionTreeClassifier()\nrf=RandomForestClassifier()\nfrom sklearn.preprocessing import StandardScaler\nss=StandardScaler()","def0f189":"train=pd.read_csv('..\/input\/train-jantahack-cabmobilitycsv\/train_JantaHack_Cabmobility.csv')\n# train.dropna()[label_col].value_counts()\/train.dropna()[label_col].shape[0]","29b8272c":"test=pd.read_csv(\"..\/input\/test-jantahack-cabmobilitycsv\/test_JantaHack_Cabmobility.csv\")","bd1c763c":"# append two training and test dataset\ndf=train.append(test)\n","d86bcefa":"df.shape","b6a25c78":"#Understanding Proportion of Null Values\ndf.isnull().mean()*100","21808e75":"label_col='Surge_Pricing_Type'","5c1c0cb7":"df.shape","7162068c":"def desc1(_DF,corr=0,label_col='pass_label_col',orderbyColumn='default',__order=True):\n    from sklearn.preprocessing import normalize\n    import math\n    \"\"\"\n       1. Pearson linear correlation (value lies between -1 to +1.)\n       Perfect value:\n        \u03c1 = +1 means perfect positive relationship (x increases, y also increases).\n        \u03c1 = -1 means perfect negative relationship (x decreases, y also decreases).\n        \u03c1 = 0 means no relation between variables.\n        High degree: If \u03c1 value lies between \u00b1 0.50 and \u00b1 1, then it is said to be a strong correlation.\n        Moderate degree: If \u03c1 value lies between \u00b1 0.30 and \u00b1 0.49, then it is said to be a medium correlation.\n        Low degree: If \u03c1 value lies below \u00b1 0.29, then it is said to be a small correlation.\n        Drawback\n            Very good when we have linear relationship. Which may not be true always in real world. So,\n             it does not work well when you do not have linear relationship between variables.\n    \"\"\"\n\n    unique_val=pd.DataFrame(index=_DF.columns)\n    def obtain_variance(_DF):\n        \n        if _DF.dtypes in ('float64','int64'):\n             xy1= pd.DataFrame(np.array(_DF*1.0))\n             # variance values less than 0.006 ( threash hold), drop the column\n            # If it is categorical binary column and if values 95:5 ratio, you drop the variable.\n            #If it is continuous column and variance is less than 0.0066, you drop the variable (remember 0.0066 is value obtained after normalizing the variable).\n\n             return \"%3g\"%xy1.var()[0]\n        elif _DF.dtypes =='object':\n            xy1= pd.DataFrame(_DF)\n            xy1.reset_index()        \n            return 0.0\n        else:\n            return 0.0     \n    def obtain_std(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%3g\"%pd.DataFrame(_DF*1.0).std()[0]\n        else:\n            return 0.0\n    def obtain_mean(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%3g\"%pd.DataFrame(_DF*1.0).mean()[0]\n        else:\n            return 0.0\n    def obtain_min(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%3g\"%pd.DataFrame(_DF*1.0).min()[0]\n        else:\n            return 0.0\n    def obtain_max(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%3g\"%pd.DataFrame(_DF*1.0).max()[0]\n        else:\n            return 0.0\n    def obtain_skew(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%.3f\"%pd.DataFrame(_DF).skew()\n        else:\n            return 0.0\n    def obtain_kurtosis(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return \"%.3f\"%pd.DataFrame(_DF).kurt()\n        else:\n            return 0.0\n    def obtain_Numeric_pearSonCorr(_DF):\n        if _DF.dtypes in ('float64','int64'):\n            return np.abs(_DF.corr(_ytrain))\n        else:\n            return 0.0\n        \n    for i in _DF.columns:\n        unique_val.loc[i,'dtypes']=_DF[i].dtypes\n        unique_val.loc[i,'null_count']=_DF[i].isnull().sum() \n        unique_val.loc[i,'total count']=_DF[i].notnull().sum()\n        unique_val.loc[i,'unique_count']=_DF[i].nunique()\n        unique_val.loc[i,'missing value ratio']= round((_DF[i].isnull().sum()\/len(_DF))*100,2)\n        unique_val.loc[i,'variance of numerics']= round(float(obtain_variance(_DF[i])),2)\n        unique_val.loc[i,'std']= round(float(obtain_std(_DF[i])),2)\n        unique_val.loc[i,'mean']= round(float(obtain_mean(_DF[i])),2)\n        unique_val.loc[i,'min']= float(obtain_min(_DF[i]))\n        unique_val.loc[i,'max']= round(float(obtain_max(_DF[i])),2)\n        unique_val.loc[i,'skew']= float(obtain_skew(_DF[i]))\n        unique_val.loc[i,'kurt']= float(obtain_kurtosis(_DF[i]))\n        if corr==2: \n            _ytrain=_DF[label_col]\n            unique_val.loc[i,'Oridinals SpearmanCorr withTarget']=np.abs(_DF[i].corr(_ytrain,'spearman').round(5))\n        if corr==1: \n            _ytrain=_DF[label_col]\n            unique_val.loc[i,'Numeric pearSonCorr withTarget']=obtain_Numeric_pearSonCorr(_DF[i]) # _DF[i].corr(ytrain,'pearson').round(5)\n\n    if (orderbyColumn=='default') and (corr==0):\n        return unique_val.sort_values(by=['unique_count','missing value ratio'])\n    \n    elif (corr==0) and (orderbyColumn== 'Corr withTarget'):\n        return unique_val.sort_values(by=['unique_count','missing value ratio'])\n    elif corr==1:\n        return unique_val.sort_values(by=['Numeric pearSonCorr withTarget','unique_count'],ascending=[False,True])\n    elif corr==2:\n        return unique_val.sort_values(by=['Oridinals SpearmanCorr withTarget','unique_count'],ascending=[False,True])\n    \n    else: \n        return unique_val.sort_values(by=[orderbyColumn],ascending=__order)","224909e3":"sammy=desc1(df,2,label_col)\nsammy","4c09dde4":"# cat types cols\ncat_colsdf=sammy[sammy.unique_count<15]\ncat_colsdf","577755b9":"#  1. Half rounding (auto ceiling and flooring) of float\nfrom decimal import localcontext, Decimal, ROUND_HALF_UP\ndf['CR']=df['Customer_Rating'].apply(lambda x: Decimal(x).to_integral_exact(rounding=ROUND_HALF_UP))\ndf['CR']=df['CR'].astype('int32')\n","50f6e578":"# 2. Imputing \n\ndf['Customer_Since_Months'].describe()\ndf['Customer_Since_Months'].fillna(6.006048,inplace=True)\n\ndf['Life_Style_Index'].describe()\ndf['Life_Style_Index'].fillna(2.79,inplace=True)\n\nCustSinceMonth_2CabType_MAP=df.groupby(['Customer_Since_Months'])['Type_of_Cab'].agg(pd.Series.mode)\ndf.loc[df['Type_of_Cab'].isnull(),'Type_of_Cab']=df.loc[df['Type_of_Cab'].isnull(),'Customer_Since_Months'].map(CustSinceMonth_2CabType_MAP)\n\nCustSinceMonth_2Confidence_Life_Style_Index_MAP=df.groupby(['Customer_Since_Months'])['Confidence_Life_Style_Index'].agg(pd.Series.mode)\ndf.loc[df['Confidence_Life_Style_Index'].isnull(),'Confidence_Life_Style_Index']=df.loc[df['Confidence_Life_Style_Index'].isnull(),'Customer_Since_Months'].map(CustSinceMonth_2Confidence_Life_Style_Index_MAP)\n\ndf['Var1'].describe()\ndf['Var1'].fillna(61.0,inplace=True)","866e3768":"# 3. Billing of Continous Variable\ndef BiningContiCols_over_yBinary(contiColList,dataframe,bins=4):\n    \"\"\"\n        Pass a list of continues columns , It will convert it into four bins and \n\n    \"\"\"\n\n    group=[i for i in range(1,bins+1)]\n    dict1={'Low':1,'Average':2,'High':3, 'Very high':4}\n    binsdf= pd.DataFrame()\n    for conti_colname in contiColList:\n        _colData=pd.DataFrame({conti_colname: dataframe[conti_colname]\n                              },index=dataframe.index)\n        _n_cols=pd.qcut(dataframe[conti_colname],q=bins,duplicates='drop').nunique()\n        _colData['bins']=pd.qcut(dataframe[conti_colname], q=bins,duplicates='drop',labels=group[:_n_cols])\n        \n        binsdf[conti_colname+'_bins'] = _colData['bins']#.map(dict1)\n        binsdf=binsdf.astype('int')\n    return binsdf\n\nContiBins=BiningContiCols_over_yBinary(['Var1','Var2','Var3','Customer_Since_Months','Customer_Rating','Trip_Distance','Life_Style_Index'],df,5)\nContiBins=ContiBins.astype('object')\ndf[ContiBins.columns]=ContiBins","0a5fa142":"ntrain=train.shape[0]","87202f20":"desc1(df,2,label_col)\n# 4. Describe method\n# focus on columns\n# [dtypes ,null_count,total count,unique_count,missing value ratio,variance of numerics,std,mean,min,max,skew,kurt,Oridinals SpearmanCorr withTarget]\n# If you have continous valibale Use 1 in the 2nd parameter of the desc1 method , it will return pearson correlatinn with target label column","57b880a4":"#['Var1','Var2','Var3','Customer_Rating','Trip_Distance','Life_Style_Index']\n#def FE_2Add_CatFrequencyCount3(_ntrain,_df,apply=1):\n#    \"\"\" Its a percentage encoding of each category\"\"\"\n    #import category_encoders as ce\n    #from imblearn import categorical_encoders as ce\n#     if apply==1:\n#         cat_temp=['Type_of_Cab','Cancellation_Last_1Month','CR','Life_Style_Index_bins','Destination_Type','Customer_Since_Months','Gender']\n#         _cols=[i+\"_freq_encode\" for i in cat_temp]\n#         freq_encoder=ce.CountFrequencyCategoricalEncoder(encoding_method=\"frequency\")\n#         _fe_train=imp.fit_transform(_df[:_ntrain][[\"Type_of_Cab\",\"Confidence_Life_Style_Index\",\"Destination_Type\",\"Gender\"]])\n#         _fe_train.columns=_cols\n#         _fe_test=imp.fit_transform(_df[_ntrain:][[\"Type_of_Cab\",\"Confidence_Life_Style_Index\",\"Destination_Type\",\"Gender\"]])\n#         _fe_test.columns=_cols\n#         _fe_all=_fe_train.append(_fe_test)\n#         _df=pd.concat([_df,_fe_all],1)\n#         return _df","f4f74363":"# 5. Freqeuency Encoding of Cat columns\ndef FE_2Add_CatFrequencyCount3(_ntrain,_df,apply_or_test=0):\n    print(\"\"\" Note: Its a count frequence encoding of each category\n                # if categories have same frequency it can be an issue\n                # will need to change it to ranked frequency encoding\n                from scipy.stats import rankdata\n          \"\"\")\n    \n    cat_temp=['Type_of_Cab','Cancellation_Last_1Month','CR','Life_Style_Index','Destination_Type','Customer_Since_Months','Gender']   \n    \n    _fe_train=pd.DataFrame()\n    _fe_test=pd.DataFrame()\n    for c in cat_temp:\n        dic_train=dict(_df[:ntrain][c].value_counts()\/_ntrain)\n        _fe_train['FreqEncode_'+c]=_df[:ntrain][c].map(dic_train).astype('float32')\n        dic_test=dict(_df[ntrain:][c].value_counts()\/_ntrain)\n        _fe_test['FreqEncode_'+c]=_df[ntrain:][c].map(dic_test).astype('float32')\n\n    if apply_or_test==1:   \n        _fe=_fe_train.append(_fe_test)\n        _df=pd.concat([_df,_fe],1)\n        return _df\n    else:\n        print(_fe_train.var().sort_values(ascending=False))\nFE_2Add_CatFrequencyCount3(ntrain,df,apply_or_test=0)","ad964065":"# 6. Mean Encoding of Conti columns\ndef FE_2Add_CatMeanEncoding2(_label_Col,_ntrain,_df,apply_or_test=0):\n    print(\"\"\" Note: Its a mean\/target encoding of each category\n               \n          \"\"\")\n    \n    cat_temp=['Type_of_Cab','Cancellation_Last_1Month','CR','Life_Style_Index','Destination_Type','Customer_Since_Months','Gender']   \n    \n    _fe=pd.DataFrame()\n    for c in cat_temp:\n        dic_train1=_df[:ntrain].groupby([c]).agg({label_col:['mean']})\n        dic_train1.columns=['target']\n        dic_train=dict(dic_train1.target.round(3))\n        #data_df = data_df.merge(mean_encoding,on=c,how='left')\n        _fe['MeanEncode_'+c]=_df[c].map(dic_train)#.astype('float32')\n\n    if apply_or_test==1:   \n        _df=pd.concat([_df,_fe],1)\n        return _df\n    else:\n        print(_fe.var().sort_values(ascending=False))\n","5cfcfdea":"# For now below method is use less .. need to investiage on this concept\ndef FE_2Add_CatMeanOverContiAggregation2(_label_col,_df,apply=1):   \n    if apply==1:\n        cat_temp=['Type_of_Cab','CR','Destination_Type','Customer_Since_Months','Gender']\n        conti_temp=['Var1','Var2','Var3','Customer_Rating','Trip_Distance','Life_Style_Index']\n        \n        tempdata=_df[[_label_col]+cat_temp+conti_temp].copy()\n        _returnDF= pd.DataFrame()\n        for conti_colname in conti_temp:\n            print(conti_colname)\n            for cat_colname in cat_temp :\n                for aggr in ['mean']:\n                    #print(conti_colname,cat_colname,aggr)\n                    tempAgg1=tempdata[tempdata[_label_col]==1].groupby(cat_colname)[conti_colname].agg(aggr)\n                    tempAgg2=tempdata[tempdata[_label_col]==2].groupby(cat_colname)[conti_colname].agg(aggr)\n                    tempAgg3=tempdata[tempdata[_label_col]==3].groupby(cat_colname)[conti_colname].agg(aggr)\n                    tempAgg12=tempAgg1\/tempAgg2\n                    tempAgg23=tempAgg2\/tempAgg3\n                    tempAgg123=1\/tempAgg12* (1\/tempAgg23)\n                    #tempdata[cat_colname+'0-'+aggr+'-over-'+conti_colname]=df_all[cat_colname].map(dict(tempAgg1))\n                    #tempdata[cat_colname+'1-'+aggr+'-over-'+conti_colname]=df_all[cat_colname].map(dict(tempAgg0))\n                    _returnDF[cat_colname+'1\/0-'+aggr+'-over-'+conti_colname]=tempdata[cat_colname].map(dict(tempAgg123))\n    return _returnDF\n    #df_all=tempdata.copy()\n    #del tempdata\n#resultDF=FE_2Add_CatMeanOverContiAggregation2(label_col,df)\n#df[resultDF.columns]=resultDF\n#resultDF.isna().sum()\n#'Customer_Since_Months'\n","69f3bdb3":"# 7.  dummyfying \ndef dummify(_df):\n    cat_cols=list(_df.columns[_df.dtypes=='object'])\n    print(cat_cols)\n    if label_col in cat_cols:\n        cat_cols.pop(label_col)\n    \n    print(_df.shape)\n    _df[cat_cols]\n    dummiedDF=pd.get_dummies(_df[cat_cols],cat_cols,drop_first=True)\n    _df[dummiedDF.columns]=dummiedDF\n    _df.drop(cat_cols,1,inplace=True)\n\n\n    # Lets drop all the columns in entire data where testing data columns holding single value\n    #### 4.1 Zero test for train and test data drop lowest variablity data\n    zerotest_testing_col=[]\n    for i in dummiedDF.columns:\n        if i in _df.columns:\n            zerotest_cond=_df[i][ntrain:].sum()==0\n            if zerotest_cond:\n                zerotest_testing_col.append(i)\n    print(len(zerotest_testing_col),\"-\",zerotest_testing_col)\n    if len(zerotest_testing_col) > 0:\n       _df.drop(zerotest_testing_col,1,inplace=True)\n    _df.shape\n\n    # # Lets drop all the columns in entire data where training data columns holding single value\n    zerotest_training_col=[]\n    for i in dummiedDF.columns:\n        if i in _df.columns:\n            zerotest_cond=_df[i][:ntrain].sum()==0\n            if zerotest_cond:\n                zerotest_training_col.append(i)\n    print(len(zerotest_training_col),\"-\",zerotest_training_col)\n    if len(zerotest_training_col) > 0:\n       _df.drop(zerotest_training_col,1,inplace=True)\n    return _df\n","e8c71495":"def statandardize(_label_col,_ntrain, _full_data,applyScale=''):\n    if _label_col in _full_data.columns:\n        _ytrain=_full_data[_label_col][:_ntrain]\n        _full_data=_full_data.drop([_label_col],1)\n    # Feature Scaling\n    _ytrain = pd.Series([int(i) for i in _ytrain])\n    _xtrain=_full_data[:ntrain]\n    _xtest=_full_data[ntrain:]\n    \n    if applyScale =='std':\n        from sklearn.preprocessing import StandardScaler\n        sc = StandardScaler()\n        _xtrain = pd.DataFrame(sc.fit_transform(_xtrain),columns=_xtrain.columns)\n        _xtest =  pd.DataFrame(sc.fit_transform(_xtest),columns=_xtest.columns)\n    if applyScale =='minmax':\n        from sklearn.preprocessing import MinMaxScaler\n        sc = MinMaxScaler()\n        _xtrain = pd.DataFrame(sc.fit_transform(_xtrain),columns=_xtrain.columns)\n        _xtest =  pd.DataFrame(sc.fit_transform(_xtest),columns=_xtest.columns)\n\n    return _xtrain,_ytrain,_xtest\n#xtrain,ytrain,xtest=statandardize(label_col,ntrain,final_data,'') \n\ndef bestRFE(ybinary,_xdata,k_value):\n    from sklearn.feature_selection import RFE\n    from sklearn.svm import SVR\n    #estimator = SVR(kernel=\"linear\")\n    model_RFE = LogisticRegression()\n    from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n    model_lda1 = LinearDiscriminantAnalysis()\n    rfe = RFE(estimator=model_lda1, n_features_to_select=k_value, step=2)\n    fit= rfe.fit(_xdata, ybinary)\n    ranking_df = pd.DataFrame()\n    ranking_df['Feature_name'] = _xdata.columns[fit.n_features_]\n    return _xdata.columns[fit.support_]","1684253f":"# 8. Cross Validation method\ndef classificationModelfit_CV2(_algo,_xdata,_ydata,_cv):\n    from sklearn.model_selection import cross_validate\n    _algo.fit(_xdata, _ydata)\n    #Perform cross-validation:\n    from sklearn.metrics import confusion_matrix\n    def tn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 0]\n    def fp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[0, 1]\n    def fn(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 0]\n    def tp(y_true, y_pred): return confusion_matrix(y_true, y_pred)[1, 1]\n#     scoring1 = {'tp': make_scorer(tp), 'tn': make_scorer(tn),\n#                 'fp': make_scorer(fp), 'fn': make_scorer(fn)}\n    cv_matrix_score= cross_validate(_algo, _xdata, _ydata, cv=_cv,\n                                    #scoring=('accuracy', 'precision','f1', 'recall','roc_auc','neg_log_loss',),\n                         return_train_score=True)\n    avg_model_Scores=pd.DataFrame(cv_matrix_score).mean()\n    for i in avg_model_Scores.index:\n        if 'neg' in i :\n            score=\"%s =%0.3f\" %(i, avg_model_Scores[i]*-1) \n            #_df.loc[i]=np.sqrt(avg_model_Scores[i]*-1)\n            print(score)\n        else:\n            score=\"%s =%0.3f\" %(i, avg_model_Scores[i] )\n            #_df.loc[i]=\n            #avg_model_Scores[i]\n            print(score)\n    return avg_model_Scores\n","d5406c9f":"def generate_submission_file(submission_csv_name,model,org_test,cleaned_test):\n    y_pred = model.predict(cleaned_test)\n\n    #Export submission file:\n    org_test[label_col]=[int(i) for i in y_pred]\n    submission=org_test[['Trip_ID',label_col]]\n    submission[label_col]= submission[label_col]#.map({1:\"Y\",0:\"N\"})\n#     negRec=len(submission[submission.Item_Outlet_Sales<0])\n#     if negRec >0:\n#         print(\"Output contain Negative records:\", negRec)\n#         submission.to_csv(submission_csv_name, index=False)\n#         return submission[submission.Item_Outlet_Sales<0]\n#     else:\n    print(\"saved File :\",submission_csv_name)\n    submission.to_csv(submission_csv_name, index=False)\n        \n    return submission[label_col]\n\n","c5d62a9b":"# 9. Normal Quitile Tranformation methid\ndef get_conti_cols_transformed(_conti_cols,_df,e=0,type='NQ'):\n    from sklearn.preprocessing import PowerTransformer\n    from sklearn.preprocessing import QuantileTransformer\n    boxCox = PowerTransformer(method='box-cox')\n    YeoJohnson = PowerTransformer(method='yeo-johnson')\n    # n_quantiles is set to the training set size rather than the default value\n    # to avoid a warning being raised by this example\n    QuantileTransF = QuantileTransformer(n_quantiles=500, output_distribution='normal',\n                             random_state=1123)\n\n    def getTF(columns,_df,e):\n        _df11=pd.DataFrame()\n        _df11[[columns]]=_df[[columns]]\n        org_skew=round(_df[[columns]].skew()[0],3)\n        _df11['box-cox']=boxCox.fit_transform(_df[[columns]]+e)\n        _df11['Yeo-Johnson']=YeoJohnson.fit_transform(_df[[columns]]+e)\n        _df11['QuantileTrans']=QuantileTransF.fit_transform(_df[[columns]]+e)\n        #_df11['exponential']=np.power(_df[[columns]],e)\n        #_df11['Nroot']=np.power(_df[[columns]],1\/e)\n        #_df11['nplog']=np.log(_df[[columns]]+e )\n        \n        # Create dataFrame for all the skew values of all the transformation\n        _df12=pd.DataFrame(_df11.skew(),columns=[columns])\n        _df12_min=_df12[_df12[columns]>0 ].min()[0]\n        _df12_max=_df12[_df12[columns]<0 ].max()[0]\n        import math\n      \n        if math.isnan(_df12_max) == False:\n             best_transformation=list(_df12[_df12[columns]==_df12_max ].index)[0]\n             ret1= (columns,best_transformation,round(_df12_max,3),org_skew)\n        elif math.isnan(_df12_min) == False:\n            best_transformation=list(_df12[_df12[columns]==_df12_min ].index)[0]\n            ret1=  (columns,best_transformation,round(_df12_min,3),org_skew)\n        return _df11[best_transformation]\n    def getBC(columns,_df,e):\n        return boxCox.fit_transform(_df[[columns]]+e)\n    def getYJ(columns,_df,e):\n        return YeoJohnson.fit_transform(_df[[columns]]+e)\n    def getNorm_Quantile(columns,_df,e):\n        return QuantileTransformer(output_distribution='normal').fit_transform(_df[[columns]]+e)\n                                           \n\n    _df13=pd.DataFrame()\n    for i  in _conti_cols[:20]:\n        print(i)\n        if type=='NQ':\n            best_transformation=getNorm_Quantile(i,_df,e)\n        elif type=='BC':\n            best_transformation=getBC(i,_df,e)\n        elif type=='YJ':\n            best_transformation=getYJ(i,_df,e)\n        elif type=='CUST':\n            best_transformation=getTF(i,_df,e)   \n        else:\n            best_transformation=getNorm_Quantile(i,_df,e)\n        best_transformation=pd.DataFrame(best_transformation,columns=[i])\n        #print(best_transformation)\n        _df13[i]=best_transformation[i]\n    return _df13","fa8bb7c4":"# 9. Normal Quitile Tranformation\ndef ContiTransformation(_df):\n    conti_cols_FE=['Var2','Var1', 'Var3','Life_Style_Index','Trip_Distance','Customer_Rating','Customer_Since_Months_X_Customer_Rating']\n    NORMQUINT_TRANS=get_conti_cols_transformed(conti_cols_FE[1:],_df,e=1,type='NQ')\n    _df[NORMQUINT_TRANS.columns]=np.array(NORMQUINT_TRANS) \n    return _df\n","67324ccb":"# 10. Feature Engineering\ndef customFE(_df):\n    cond=[(_df['Gender']=='Male')& (_df['Cancellation_Last_1Month']==0), # 4\n          (_df['Gender']=='Female')& (_df['Cancellation_Last_1Month']==0), # 3\n          (_df['Gender']=='Male')& (_df['Cancellation_Last_1Month'].isin([1])), # 3\n          (_df['Gender']=='Female')& (_df['Cancellation_Last_1Month'].isin([1,2])), # 2\n          (_df['Gender']=='Male')& (_df['Cancellation_Last_1Month'].isin([2,3])), # 2\n         ]\n    choices=[4,3,3,2,2]\n    np.select(cond,choices,default=1)\n    _df['GenderWise_Cancellation_Last_1Month']=np.select(cond,choices,default=1)\n    _df['GenderWise_Cancellation_Last_1Month_OT']=_df['GenderWise_Cancellation_Last_1Month'].astype('object')\n    print(_df['GenderWise_Cancellation_Last_1Month'].isna().sum())\n    \n    \n    cond=[ (_df['Confidence_Life_Style_Index']=='A')&(_df['Gender'] =='Male'),\n       (_df['Confidence_Life_Style_Index']=='B')&(_df['Gender'] =='Male'),\n      (_df['Confidence_Life_Style_Index']=='C')&(_df['Gender'] =='Male'),\n      (_df['Confidence_Life_Style_Index']=='C')&(_df['Gender'] =='Female'),\n     ]\n\n    _df['GenderWise_Confidence_Life_Style_Index']=np.select(cond,[1,2,1,4],default=3)\n    _df['GenderWise_Confidence_Life_Style_Index_OT']=_df['GenderWise_Confidence_Life_Style_Index'].astype('object')\n    _df['GenderWise_Confidence_Life_Style_Index_OT']\n\n    return _df\n    ","86faba7a":"################### Main Execution #########################\n\nfinal_data=df.copy()\n# Feature Engeering 1\nfinal_data=customFE(final_data)\n# Feature Engeering 2\nfinal_data['Customer_Since_Months_X_Customer_Rating']=final_data['Customer_Since_Months']*final_data['Customer_Rating']\nfinal_data['CRWiseMeanTripDistance']=final_data['CR'].map(df.groupby('CR')['Trip_Distance'].mean().astype('int32'))\n\n# Continous variable Transformation 2\nfinal_data=ContiTransformation(final_data)\nif 'Trip_ID' in final_data.columns:\n    final_data.drop('Trip_ID',1,inplace=True)\n\n# apply frequency Endcoding    \nfinal_data=FE_2Add_CatFrequencyCount3(ntrain,final_data,apply_or_test=1)\nprint(final_data.isna().sum())\n# apply mean Endcoding    \n\nfinal_data=FE_2Add_CatMeanEncoding2(label_col,ntrain,final_data,apply_or_test=1)\nfdata=dummify(final_data)\n\nfinal_data.head()\nsummy=desc1(final_data,2,label_col)\nc=summy[summy['dtypes']=='object'].index","58cb3550":"# Feature Scaling\nytrain = fdata[:ntrain][label_col]\n_xtrain=fdata[:ntrain].drop(label_col,1)\n_xtest=fdata[ntrain:].drop(label_col,1)\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\nxtrain = pd.DataFrame(sc.fit_transform(_xtrain),columns=_xtrain.columns)\nxtest =  pd.DataFrame(sc.fit_transform(_xtest),columns=_xtest.columns)\n","dc81fd34":"# 11. Lets me reduce the dataset\n#(pd.Series(ytrain).value_counts()* .35)\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.datasets import make_imbalance\nX, y = make_imbalance(xtrain, ytrain,\n                      sampling_strategy={1: 9524, 2: 19854, 3: 16702},\n                      random_state=45)","59ce3604":"xtrain.shape,xtest.shape,X.shape,y.shape","20267a4b":"# Xgboost estimator\n# [.1,.08,0.05,0.03,.01]\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBClassifier\nxgb1 = XGBClassifier(learning_rate =0.05, n_estimators=10, max_depth=8, min_child_weight=4, subsample=0.8,\n                     colsample_bytree=0.8, objective= 'multi:softmax', \n                                 num_class = 3, nthread=4, seed=27)#, gamma=2,scale_pos_weight=.90,reg_alpha=2,reg_lambda=2)\n#modelfit(xgb1, train, predictors)\nd=classificationModelfit_CV2(xgb1,X,y,5)\n#xGBC_pred=generate_submission_file(\"n1.csv\",xgb1,test,xtest)","85cf84ca":"from catboost import CatBoostClassifier\ncb=CatBoostClassifier()\nd=classificationModelfit_CV2(cb,X,y,5)","a5265b49":"from catboost import Pool, cv\n# from catboost import pool\ncv_dataset = Pool(data=X,\n                  label=y)\n\nparams = {\"iterations\": 100,\n          \"depth\": 6,\n          \"loss_function\": \"MultiClass\",\n          \"verbose\": False}\n\nscores = cv(cv_dataset,\n            params,\n            fold_count=2)\n\neval_dataset = Pool(data=X)\nmodel = CatBoostClassifier(**params)\nmodel.fit(cv_dataset)\npred=model.predict(eval_dataset)\naccuracy_score(pred,y)","c9b4f1e8":"# CatBoost\nIt does auto-generation numerical features that are based on categorical features that are calculated differently for the training and validation datasets, For this reason the loss value on the validation dataset might be better than the loss value for the training and Also one hot encoding or label encoding is not required in the catboost.\n\nIt required less parameter tuning, thats the main reason of its popularity\n\nIn Catboost, each tree contains leaves if the depth is set to , because CatBoost builds full symmetric trees by default. The recommended depth is 6, which works well in most cases. In rare cases it's useful to increase the depth value up to 10.\n\nIts  not possible to select best model with best parameter using CV. The CatBoost cv function is intended for cross-validation only,  it can not be used for tuning parameter.The only parameter that can be selected based on cross-validation is the number of iterations.\n\nCatBoost can handle missing values internally. None values should be used for missing value representation.\nIf the dataset is read from a file, missing values can be represented as strings like N\/A, NAN, None, empty string and the like.\n\nIt uses gradient boosting which in turn is useful for predictive models that analyze ordered (continuous) data and categorical data.\n\nIt\u2019s not a good idea to build very big trees in boosting since they overfit the data but it boost the performance with each iteration by focusing and correcting the wrong prediction made by previous step.\n\nNote: CatBoost uses symmetric or oblivious trees. The trees from the music example above are symmetric.\n","9000435c":"# Dataset Taken from Analytical site:\n    https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-mobility-analytics\/?utm_source=auto-email#About\n\n# Target is MultiClass with three classes\n    \n\n### What you will Learn Here:\n    1. Null imputation\n    2. Half rounding (auto ceiling and flooring) of float\n    3. Binning\n    4. Customise Describe method with very advance information\n    5. Frequency Encoding for categorical columns\n    6. Mean Encoding  for categorical columns\n    7. dummyfying with drop first to just ignore the first columns which hold duplicate info and causing Multi-collineerity\n    6. Data set is huge , How to reduce dataset\n    7. Applying QuantileTransformer transformation over Contineos Variable\n        This method has option to apply alternate BoxCox or Johnson tranformation\n    8. XgBoost Classifier \n    9. Generic customized Cross Validation method, which can be used anywhere.\n\n    \n    "}}