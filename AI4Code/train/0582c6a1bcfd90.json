{"cell_type":{"a77b4cef":"code","533006d9":"code","d20cf13d":"code","129ee2a5":"code","e0ae7605":"code","51f9c3a6":"code","7b7abff9":"code","f04d9f86":"code","f8c733d9":"code","41cdeef8":"code","5925529e":"code","d017bacf":"code","b0cc3c41":"code","305ac7f7":"code","c40453a9":"code","db06cce1":"code","84d1b911":"code","d71a1bb9":"code","86e9055a":"code","2b77789f":"code","c139f24d":"code","217be6c5":"code","b0558437":"code","87ad355c":"code","707a2217":"code","f1945cdd":"code","a02bb55d":"code","e4591c06":"code","557c89e3":"code","83c85f41":"code","2569a43b":"code","5b59005c":"code","12423e32":"code","3f406783":"markdown","e2c17e7a":"markdown","2a70f1e6":"markdown","2359b17b":"markdown","3ed4221d":"markdown","79283576":"markdown","5112beb7":"markdown","1e747316":"markdown","df77e518":"markdown","4211389a":"markdown","315744b7":"markdown","0da2a76b":"markdown","7ba90e66":"markdown","1fc9c600":"markdown","f10e8a1b":"markdown","f5abc7e3":"markdown","b4044cb7":"markdown","fe163032":"markdown","47d891bc":"markdown","44ab352d":"markdown","09bf29b0":"markdown","37a8d67e":"markdown","485a08e7":"markdown","cf295d54":"markdown","145e83ca":"markdown","1f26539b":"markdown","18a9415f":"markdown","8f4e7a08":"markdown","140e5ed6":"markdown","d15d084d":"markdown","970bf2be":"markdown","7d3631c7":"markdown","891c5a76":"markdown","4e60330e":"markdown","9b535971":"markdown","b4af4706":"markdown","3be8374c":"markdown","d8b53c4d":"markdown","f6d1f82c":"markdown"},"source":{"a77b4cef":"%matplotlib notebook\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.io import loadmat\nfrom scipy import stats\nimport copy\nimport pylab\nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score","533006d9":"data=pd.read_csv('..\/input\/superconduct_train.csv')\nprint(data.shape)\n","d20cf13d":"data.head()","129ee2a5":"target_clm = 'critical_temp' # the critical temperature is our target variable\nn_trainset = 200 # size of the training set\nn_testset = 500 #size of the test set","e0ae7605":"# set random seed to make sure every test set is the same\nnp.random.seed(seed=1)\n\nidx = np.arange(data.shape[0])\nidx_shuffled = np.random.permutation(idx) # shuffle indices to split into training and test set\n\ntest_idx = idx_shuffled[:n_testset]\ntrain_idx = idx_shuffled[n_testset:n_testset+n_trainset]\ntrain_full_idx = idx_shuffled[n_testset:]\n\nX_test = data.loc[test_idx, data.columns != target_clm].values\ny_test = data.loc[test_idx, data.columns == target_clm].values\nprint('Test set shapes (X and y)', X_test.shape, y_test.shape)\n\nX_train = data.loc[train_idx, data.columns != target_clm].values\ny_train = data.loc[train_idx, data.columns == target_clm].values\nprint('Small training set shapes (X and y):',X_train.shape, y_train.shape)\n\nX_train_full = data.loc[train_full_idx, data.columns != target_clm].values\ny_train_full = data.loc[train_full_idx, data.columns == target_clm].values\nprint('Full training set shapes (X and y):',X_train_full.shape, y_train_full.shape)","51f9c3a6":"# Histogram of the target variable\nplt.hist(y_train_full)\n# ADD YOUR CODE HERE","7b7abff9":"#Trials to normalize tha array X_train_full:\n\nnormalized_X_train_full=(X_train_full-X_train_full.min())\/(X_train_full.max()-X_train_full.min())\nnormalized_X_train_full.shape\nnormalized_df=(X_train_full-X_train_full.mean())\/X_train_full.std()\nimport pandas as pd\nfrom sklearn import preprocessing\n\nx = X_train_full #returns a numpy array\nmin_max_scaler = preprocessing.MinMaxScaler()\nx_scaled = min_max_scaler.fit_transform(x)\ndf = pd.DataFrame(x_scaled)\nnormalized_X_train_full.shape","f04d9f86":"\n    plt.scatter(x_scaled[:,1], y_train_full, c=np.random.rand(3,), alpha=0.5)\n","f8c733d9":"#Trials to write the loop\n# Scatter plots of the target variable vs. features\n\n\n#fig=plt.figure(figsize=(8, 8))\n#columns = 9\n#rows = 9\n#feat_idxs = range(1,81)\n\n#for idx in feat_idxs:\n #   plt.scatter(X_train_full[:,idx], y_train_full, c=np.random.rand(3,), alpha=0.5)\n  #  fig.add_subplot(rows, columns, idx)\n#fig, ax = plt.subplots(9, 9, sharex='col', sharey='row', figsize=(10, 10))\n#idx = 0\n#for lidx in range(9):\n #   for cidx in range(9):\n  #      ax[lidx, cidx].scatter( X_train_full[:,idx],y_train_full, \n   #             c=np.random.rand(3,),\n    #            alpha=0.5)\n#         ax[lidx, cidx].text(0.5, 0.5, str((lidx, cidx, idx)),\n#                       fontsize=8, ha='center')\n     #   idx += 1\n\n# ADD YOUR CODE HERE\nidx = 0\nfig, axs = plt.subplots(9, 9, sharex=True, sharey=True, figsize=(10, 10))\n# add a big axes, hide frame\nfig.add_subplot(111, frameon=False)\n# hide tick and tick label of the big axes\nplt.tick_params(labelcolor='none', top='off', bottom='off', left='off', right='off')\nplt.grid(False)\nplt.xlabel(\"Different material properties\")\nplt.ylabel(\"Critical temperature\")\n\nfor n in range(81):\n    axs[n \/\/ 9, n % 9].scatter(x_scaled[:, idx], y_train_full, c = np.random.rand(3,), alpha=0.5)\n    idx += 1\n                                            ","41cdeef8":"#from here we can return the name of properties that might be useful and their coordinates of our 9x9 plot\nind = 1\nfor col in data.columns[1:81]: \n    print(col, ind\/\/9 , ind%9)\n    ind += 1\n    \n    ","5925529e":"def plot_regression_results(y_test,y_pred,weights):\n    '''Produces three plots to analyze the results of linear regression:\n        -True vs predicted\n        -Raw residual histogram\n        -Weight histogram\n        \n    Inputs:\n        y_test: (n_observations,) numpy array with true values\n        y_pred: (n_observations,) numpy array with predicted values\n        weights: (n_weights) numpy array with regression weights'''\n    \n    print('MSE: ', mean_squared_error(y_test,y_pred))\n    print('r^2: ', r2_score(y_test,y_pred))\n    \n    fig,ax = plt.subplots(1,3,figsize=(9,3))\n    #predicted vs true\n    ax[0].scatter(y_test,y_pred)\n    ax[0].set_title('True vs. Predicted')\n    ax[0].set_xlabel('True %s' % (target_clm))\n    ax[0].set_ylabel('Predicted %s' % (target_clm))\n\n    #residuals\n    error = np.squeeze(np.array(y_test)) - np.squeeze(np.array(y_pred))\n    ax[1].hist(np.array(error),bins=30)\n    ax[1].set_title('Raw residuals')\n    ax[1].set_xlabel('(true-predicted)')\n\n    #weight histogram\n    ax[2].hist(weights,bins=30)\n    ax[2].set_title('weight histogram')\n\n    plt.tight_layout()","d017bacf":"# weights is a vector of length 82: the first value is the intercept (beta0), then 81 coefficients\nweights = np.random.randn(82)\n\n# Model predictions on the test set\ny_pred_test = np.random.randn(y_test.size)\n\nplot_regression_results(y_test, y_pred_test, weights)","b0cc3c41":"def OLS_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization on the training set and \n       returns weights and testset predictions.\n    \n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set \n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         \n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n         \n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.        \n    \n    '''\n    \n    # ADD YOUR CODE HERE\n    weights = np.dot(np.dot(np.dot(X_train.T, X_train)**(-1), X_train.T), y_train)\n    #print('weights: ',weights)\n    \n    return weights, y_pred\n","305ac7f7":"np.dot(X_train.T * X_train)","c40453a9":"np.dot(np.dot(np.dot(X_train.T, X_train)**(-1), X_train.T), y_train)","db06cce1":"np.dot(X_train.T, X_train)**(-1)) * X_train.transpose() * y_train","84d1b911":"weights, y_pred = OLS_regression(X_test, X_train, y_train)\nplot_regression_results(y_test, y_pred, weights)","d71a1bb9":"def sklearn_regression(X_test, X_train, y_train):\n    '''Computes OLS weights for linear regression without regularization using the sklearn library on the training set and \n       returns weights and testset predictions.\n    \n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set \n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         \n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n          \n         \n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.     \n    \n    '''\n    \n    # ADD YOUR CODE HERE\n    \n    return weights, y_pred","86e9055a":"weights, y_pred = sklearn_regression(X_test, X_train, y_train)\nplot_regression_results(y_test, y_pred, weights)","2b77789f":"weights, y_pred = sklearn_regression(X_test, X_train_full, y_train_full)\nplot_regression_results(y_test, y_pred, weights)","c139f24d":"def ridge_regression(X_test, X_train, y_train, alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha \n       on the training set and returns weights and testset predictions.\n    \n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set \n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n         \n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n          \n       Note:\n         Both the training and the test set need to be appended manually by a columns of 1s to add\n         an offset term to the linear regression model.       \n    \n    '''\n\n    # ADD YOUR CODE HERE\n        \n    return weights, y_pred","217be6c5":"alphas = np.logspace(-7,7,100)\n\n# ADD YOUR CODE HERE\n","b0558437":"# ADD YOUR CODE HERE\n","87ad355c":"# ADD YOUR CODE HERE\n","707a2217":"def ridge_regression_sklearn(X_test, X_train, y_train,alpha):\n    '''Computes OLS weights for regularized linear regression with regularization strength alpha using the sklearn\n       library on the training set and returns weights and testset predictions.\n    \n       Inputs:\n         X_test: (n_observations, 81), numpy array with predictor values of the test set \n         X_train: (n_observations, 81), numpy array with predictor values of the training set\n         y_train: (n_observations,) numpy array with true target values for the training set\n         alpha: scalar, regularization strength\n         \n       Outputs:\n         weights: The weight vector for the regerssion model including the offset\n         y_pred: The predictions on the TEST set\n          \n       Note:\n         The sklearn library automatically takes care of adding a column for the offset.     \n   \n    \n    '''\n    \n    # ADD YOUR CODE HERE\n            \n    return weights, y_pred","f1945cdd":"# ADD YOUR CODE HERE","a02bb55d":"def ridgeCV(X, y, n_folds, alphas):\n    '''Runs a n_fold-crossvalidation over the ridge regression parameter alpha. \n       The function should train the linear regression model for each fold on all values of alpha.\n    \n      Inputs: \n        X: (n_obs, n_features) numpy array - predictor\n        y: (n_obs,) numpy array - target\n        n_folds: integer - number of CV folds\n        alphas: (n_parameters,) - regularization strength parameters to CV over\n        \n      Outputs:\n        cv_results_mse: (n_folds, len(alphas)) numpy array, MSE for each cross-validation fold \n        \n      Note: \n        Fix the seed for reproducibility.\n        \n        '''    \n    \n    cv_results_mse = np.zeros((n_folds, len(alphas)))\n    np.random.seed(seed=2)\n\n    \n    # ADD YOUR CODE HERE\n            \n    return cv_results_mse    ","e4591c06":"alphas = np.logspace(-7,7,100)\nmse_cv = ridgeCV(X_train, y_train, n_folds=10, alphas=alphas)","557c89e3":"plt.figure(figsize=(6,4))\nplt.plot(alphas, mse_cv.T, '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","83c85f41":"plt.figure(figsize=(6,4))\nplt.plot(alphas, np.mean(mse_cv,axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","2569a43b":"alphas = np.logspace(-7,7,100)\nmse_cv_full = ridgeCV(X_train_full, y_train_full, n_folds=10, alphas=alphas)","5b59005c":"plt.figure(figsize=(6,4))\nplt.plot(alphas, np.mean(mse_cv_full,axis=0), '.-')\nplt.plot(alphas, np.mean(mse_cv,axis=0), '.-')\nplt.xscale('log')\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","12423e32":"plt.figure(figsize=(6,4))\nplt.plot(alphas, np.mean(mse_cv_full,axis=0), '.-')\nplt.xscale('log')\nminValue = np.min(np.mean(mse_cv_full,axis=0))\nplt.ylim([minValue-.01, minValue+.02])\nplt.xlabel('alpha')\nplt.ylabel('Mean squared error')\nplt.tight_layout()","3f406783":"Now we run 10-fold cross-validation using the training data of a range of $\\alpha$s.","e2c17e7a":"We zoom in on the blue curve to the very right:","2a70f1e6":"We will now run cross-validation on the full training data. This will take a moment, depending on the speed of your computer. Afterwards, we will again plot the mean CV curves for the full data set (blue) and the small data set (orange).","2359b17b":"## Task 5: Cross-validation [15 pts]\n\nUntil now, we always estimated the error on the test set directly. However, we typically do not want to tune hyperparameters of our inference algorithms like $\\alpha$ on the test set, as this may lead to overfitting. Therefore, we tune them on the training set using cross-validation. As discussed in the lecture, the training data is here split in `n_folds`-ways, where each of the folds serves as a held-out dataset in turn and the model is always trained on the remaining data. Implement a function that performs cross-validation for the ridge regression parameter $\\alpha$. You can reuse functions written above.","3ed4221d":"Which material properties may be useful for predicting superconductivity? What other observations can you make?","79283576":"YOUR ANSWER HERE","5112beb7":"# Machine Learning I - Practical I\n\nName: {YOUR NAME}\n\nCourse: {NAME OF YOUR PROGRAM}","1e747316":"Fit the model using the larger training set, `X_train_full` and `y_train_full`, and again evaluate on `X_test`.","df77e518":"The dataset consists of over 20.000 materials and lists their physical features. From these features, we want to learn how to predict the critical temperature, i.e. the temperature we need to cool the material to so it becomes superconductive. First load and familiarize yourself with the data set a bit.","4211389a":"Why are the values of the weights largest on the left? Do they all change monotonically? ","315744b7":"## Task 2:  Implement your own OLS estimator [10 pts]\n\nWe want to use linear regression to predict the critical temperature. Implement the ordinary least squares estimator without regularization 'by hand':\n\n$w = (X^TX)^{-1}X^Ty$\n\nTo make life a bit easier, we provide a function that can be used to plot regression results. In addition it computes the mean squared error and the squared correlation between the true and predicted values. ","0da2a76b":"## The  dataset","7ba90e66":"YOUR ANSWER HERE","1fc9c600":"Make a single plot that shows for each coefficient how it changes with $\\alpha$, i.e. one line per coefficient. Also think about which scale (linear or log) is appropriate for your $\\alpha$-axis. You can set this using `plt.xscale(...)`.","f10e8a1b":"Now test a range of log-spaced $\\alpha$s (~10-20), which cover several orders of magnitude, e.g. from 10^-7 to 10^7. \n\n* For each $\\alpha$, you will get one model with one set of weights. \n* For each model, compute the error on the test set. \n\nStore both the errors and weights of all models for later use. You can use the function `mean_squared_error` from sklearn (imported above) to compute the MSE.\n","f5abc7e3":" YOUR ANSWER HERE:\n as we can see from coordinates above, these properties, for instance can be useful:\nstd_Density 4 3\nwtd_std_Density 4 4\nmean_ElectronAffinity 4 5\nwtd_mean_ElectronAffinity 4 6\ngmean_ElectronAffinity 4 7\nwtd_mean_Valence 8 0\ngmean_Valence 8 1\n\n\nIn every subplot we can see an element with max. cr. temperature;\n\n\n ","b4044cb7":"## Task 1: Plot the dataset [5 pts]\n\nTo explore the dataset, use `X_train_full` and `y_train_full` for two descriptive plots:\n\n* **Histogram** of the target variable. Use `plt.hist`.\n\n* **Scatterplots** relating the target variable to one of the feature values. For this you will need 81 scatterplots. Arrange them in one big figure with 9x9 subplots. Use `plt.scatter`. You may need to adjust the marker size and the alpha blending value. ","fe163032":"This notebook provides you with the assignments and the overall code structure you need to complete the assignment. Each exercise indicates the total number of points allocated. There are also questions that you need to answer in text form. Please use full sentences and reasonably correct spelling\/grammar.\n\nRegarding submission & grading:\n\n- Solutions can be uploaded to ILIAS until the start of the next lecture. Please upload a copy of this notebook and a PDF version of it after you ran it.\n\n- Please hand in your own solution. You are encouraged to discuss your code with classmates and help each other, but after that, please sit down for yourself and write your own code. \n\n- We will grade not only based on the outputs, but also look at the code. So please use comments make us understand what you intended to do :)\n\n- For plots you create yourself, all axes must be labeled. \n\n- DO NOT IN ANY CASE change the function interfaces.\n\n- If you are not familiar with python, but used MATLAB before, check out this reference pages listing what you want to use as python equivalent of a certain MATLAB command:\n\n    https:\/\/docs.scipy.org\/doc\/numpy\/user\/numpy-for-matlab-users.html\n    \n    http:\/\/www.eas.uccs.edu\/~mwickert\/ece5650\/notes\/NumPy2MATLAB.pdf\n    \n    http:\/\/mathesaurus.sourceforge.net\/matlab-python-xref.pdf\n    \n    or, if you prefer to read a longer article, try: \n    \n    https:\/\/realpython.com\/matlab-vs-python\/#learning-about-pythons-mathematical-libraries\n    \n    ","47d891bc":"If you implemented everything correctly, the MSE is again 599.74.","44ab352d":"We plot the MSE trace for each fold separately:","09bf29b0":"Now implement the same model using sklearn. Use the `linear_model.Ridge` object to do so.\n","37a8d67e":"What is the optimal $\\alpha$? Is it similar to the one found on the test set? Do the cross-validation MSE and the test-set MSE match well or differ strongly?","485a08e7":"## Task 3: Compare your implementation to sklearn [5 pts]\n\nNow, familarize yourself with the sklearn library. In the section on linear models:\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model\n\nyou will find `sklearn.linear_model.LinearRegression`, the `sklearn` implementation of the OLS estimator. Use this sklearn class to implement OLS linear regression. Again obtain estimates of the weights on `X_train` and `y_train` and compute the MSE and $r^2$ on `X_test`.\n","cf295d54":"Which value of $\\alpha$ gives the minimum MSE? Is it better than the unregularized model? Why should the curve reach ~600 on the left?","145e83ca":"We also plot the average across folds:","1f26539b":"YOU ANSWER HERE","18a9415f":"Because the dataset is rather large, we prepare a small subset of the data as training set, and another subset as test set. To make the computations reproducible, we set the random seed.","8f4e7a08":"Note: Don't worry if the curve is not exactly identical to the one you got above. The loss function we wrote down in the lecture  has $\\alpha$ defined a bit differently compared to sklearn. However, qualitatively it should look the same.","140e5ed6":"This time, only plot how the performance changes as a function of $\\alpha$. ","d15d084d":"## Task 4: Regularization with ridge regression [15 pts]\n\nWe will now explore how a penalty term on the weights can improve the prediction quality for finite data sets. Implement the analytical solution of ridge regression \n\n$w = (XX^T + \\alpha I_D)^{-1}X^Ty$\n\n\nas a function that can take different values of $\\alpha$, the regularization strength, as an input. In the lecture, this parameter was called $\\lambda$, but this is a reserved keyword in Python.","970bf2be":" How does test set performance change? What else changes?","7d3631c7":"What do you observe? Is the linear regression model good?","891c5a76":"YOUR ANSWER HERE","4e60330e":"YOUR ANSWER HERE","9b535971":"As an example, we here show you how to use this function with random data. ","b4af4706":"Implement OLS linear regression yourself. Use `X_train` and `y_train` for estimating the weights and compute the MSE and $r^2$ from `X_test`. When you call our plotting function with the regession result, you should get mean squared error of 599.7.","3be8374c":"Why does the CV curve on the full data set look so different? What is the optimal value of $\\alpha$ and why is it so much smaller than on the small training set?","d8b53c4d":"YOUR ANSWER HERE","f6d1f82c":"Plot how the performance (i.e. the error) changes as a function of $\\alpha$. Again, use appropriate scaling of the x-axis. As a sanity check, the MSE value for very small $\\alpha$s should be close to the test-set MSE of the unregularized solution, i.e. 599."}}