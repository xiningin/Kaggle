{"cell_type":{"1fe523f1":"code","1a3c43ff":"code","d869f2a9":"code","b8c0ffe5":"code","2fb55727":"code","fc14f00b":"code","5b481d68":"code","81797bd8":"code","79b73eba":"code","9ea945c7":"code","c34d969a":"code","565e0148":"code","091f3bb1":"code","bceafb63":"code","ae648ee6":"code","bc84a58a":"code","4754b615":"code","d5c2a4b1":"code","a2d0d380":"code","ed3127f2":"code","4fb01a1e":"code","64c1df45":"code","e6fb1153":"code","7608c0ad":"code","5ed02f69":"code","d839fae5":"code","7c8922b8":"code","fdb2a784":"markdown","04893e1d":"markdown","368f17df":"markdown","bb0c65b5":"markdown","c48683c7":"markdown","b8e2ecad":"markdown","417b216b":"markdown","feca9e1c":"markdown","7ba8f1af":"markdown","eb7813cd":"markdown","8fe67570":"markdown","5d0d8bd8":"markdown","6d2f62ae":"markdown","f99044be":"markdown","e56f88c5":"markdown","13047c63":"markdown"},"source":{"1fe523f1":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \n\nfrom sklearn.utils import shuffle\n\nimport re\nimport nltk\nnltk.download('stopwords')\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.stem import LancasterStemmer\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\n\nimport keras \nfrom keras.models import Sequential, Model \nfrom keras import layers\nfrom keras.layers import Dense, Dropout, Input, Embedding","1a3c43ff":"dataset_cols = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\ndataset = pd.read_csv('\/kaggle\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', header=None, encoding='ISO-8859-1', names=dataset_cols)","d869f2a9":"dataset.shape","b8c0ffe5":"dataset.head()","2fb55727":"word_bank = []\n\ndef preprocess(text):\n    review = re.sub('[^a-zA-Z]',' ',text) \n    review = review.lower()\n    review = review.split()\n    ps = LancasterStemmer()\n    all_stopwords = stopwords.words('english')\n    all_stopwords.remove('not')\n    review = [ps.stem(word) for word in review if not word in set(all_stopwords)]\n    return ' '.join(review)","fc14f00b":"df = shuffle(dataset,random_state=42)\ndf = df[1:800000]","5b481d68":"df['target'].value_counts()","81797bd8":"df['text'] = df['text'].apply(lambda x: preprocess(x))","79b73eba":"y = df['target']\nle = LabelEncoder()\ny = le.fit_transform(y)","9ea945c7":"X_train, X_test, y_train, y_test = train_test_split(df['text'], y, test_size = 0.20, random_state = 0)","c34d969a":"tfidf = TfidfVectorizer(max_features = 600)\nX_train_tf = tfidf.fit_transform(X_train).toarray() \nX_test_tf = tfidf.transform(X_test).toarray()","565e0148":"X_train_tf.shape, X_test_tf.shape, y_train.shape, y_test.shape","091f3bb1":"lr = LogisticRegression(random_state = 0)\nstart_time = time.time()\nlr.fit(X_train_tf, y_train) \nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","bceafb63":"y_pred_lr = lr.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_lr))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_lr))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_lr))","ae648ee6":"nb = MultinomialNB()\nstart_time = time.time()\nnb.fit(X_train_tf,y_train)\nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","bc84a58a":"y_pred_nb = nb.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nb))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nb))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nb))","4754b615":"xg = xgb.XGBClassifier()\nstart_time = time.time()\nxg.fit(X_train_tf, y_train)  \nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","d5c2a4b1":"y_pred_xg = xg.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_xg))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xg))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_xg))","a2d0d380":"dc = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nstart_time = time.time()\ndc.fit(X_train_tf, y_train)\nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","ed3127f2":"y_pred_dc = dc.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_dc))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_dc))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_dc))","4fb01a1e":"rf = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nstart_time = time.time()\nrf.fit(X_train_tf, y_train)\nprint(\"Execution Time:\", time.time()-start_time,\"secs\")","64c1df45":"y_pred_rf = rf.predict(X_test_tf)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_rf))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_rf))","e6fb1153":"model = Sequential()\n\nmodel.add(Dense(256, activation='relu', input_dim=600))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(32, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(8, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nmodel.summary()","7608c0ad":"history = model.fit(X_train_tf, y_train, epochs=20, batch_size=32,validation_data=(X_test_tf,y_test))\nloss, accuracy = model.evaluate(X_train_tf, y_train, verbose=False)\nprint(\"Training Accuracy: {:.4f}\".format(accuracy))\nloss, accuracy = model.evaluate(X_test_tf, y_test, verbose=False)\nprint(\"Testing Accuracy:  {:.4f}\".format(accuracy))","5ed02f69":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","d839fae5":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","7c8922b8":"y_pred_new = model.predict(X_test_tf)\ny_pred_nn = np.where(y_pred_new>0.5,1,0)\nprint(\"Accuracy:\\n\", accuracy_score(y_test, y_pred_nn))\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred_nn))\nprint(\"Classification Report:\\n\", classification_report(y_test, y_pred_nn))","fdb2a784":"# Importing Libraries","04893e1d":"# Logisitic Regression","368f17df":"# Data Preprocessing","bb0c65b5":"# Neural Network","c48683c7":"### Applying the Preprocess Function on the subset data","b8e2ecad":"### Label Encoding of the Target Variable","417b216b":"# XGBoost Classifier","feca9e1c":"# Naive Bayes Classifier","7ba8f1af":"# Conclusion\n\n## The accuracy score of various classification algorithms on Twitter Sentiment Data is as follows:\n#### **1. Logistic Regression: 73.651875%**\n\n#### **2. Naive Bayes: 72.59%**\n\n#### **3. XGBoost: 73.273125%**\n\n#### **4. Decision Tree: 68.585625%**\n\n#### **5. Random Forest: 72.481875%**\n\n#### **6. Neural Network: 73.76875%**","eb7813cd":"### TF-IDF Vectorization and Creating Feature Matrix using 600 most frequent words","8fe67570":"# Importing Sentiment Analysis Dataset","5d0d8bd8":"# Splitting the data into Training and Testing","6d2f62ae":"# Random Forest Classifier","f99044be":"### Target class is balanced after subsetting the data","e56f88c5":"# Decision Tree Classifier","13047c63":"### Data shuffling and subsetting the data to work on initial 800000 tweets"}}