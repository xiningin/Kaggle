{"cell_type":{"fb2b9c3b":"code","2e7b34c8":"code","7b8ab776":"code","80e19688":"code","6c42e66b":"code","6a15b83e":"code","e54a23dc":"code","1a3240cf":"code","9e65a052":"code","5025c8b2":"code","5356552f":"code","a6da9609":"code","933bda80":"code","3508ce34":"code","2739b0f8":"code","621ee7f7":"code","4f2b5a42":"code","ab1e462d":"code","49889d82":"code","839c5457":"code","dc274493":"code","f7c3e6fb":"code","f4f111b7":"code","84661fa7":"code","fddec9d0":"code","e10e20e5":"code","a06cf8e6":"code","936d2511":"code","561a8feb":"code","0590e61b":"code","c4c8f235":"code","ae8775f9":"code","c49904b4":"code","25baaefa":"code","08558044":"code","84eac6f5":"code","77638314":"code","993911db":"code","892310a4":"code","7bd810d2":"code","81bae43c":"code","78d4b99e":"code","d7b5570c":"code","86bfcf15":"code","bb19bbaf":"code","ec9e8c84":"code","01ab03c2":"code","16de6d97":"code","268aaa68":"code","b56f14f6":"code","95a46c87":"code","19bf4cbd":"code","1acb651c":"code","7576332a":"code","26de40e3":"code","0f12e93e":"code","5c154722":"code","e79240e3":"code","29ddad68":"code","b43f8618":"code","5e838293":"code","3dc1a8ac":"code","bc43a35c":"code","ca3cb2ae":"code","f0b1fcf5":"code","dd8b71ac":"code","7791c8c0":"code","efc131c2":"code","f8821aaf":"code","0ab6a7bf":"code","e9f1aad9":"code","068a3dfc":"code","2fe6e08a":"code","1e8abeac":"code","17414df4":"code","4ccaafa8":"code","41b72054":"code","f1a2e30c":"markdown","f0777810":"markdown","37cebb22":"markdown","2e6cc064":"markdown","2db1de2c":"markdown","fc5d72fa":"markdown","83572bb5":"markdown","4288e124":"markdown","cabc936a":"markdown","7b7093ec":"markdown","9f9fa7d2":"markdown","3a354415":"markdown","980458ae":"markdown","caf6554d":"markdown","5eed3007":"markdown","a9b97b34":"markdown","f0a31ff6":"markdown","2b6c2482":"markdown","5119c30f":"markdown","0b9ab3cf":"markdown","61672079":"markdown","6aa27547":"markdown","c310dae4":"markdown","b2baf8db":"markdown","5eca9346":"markdown","54f164be":"markdown","f4e919d2":"markdown","678f0eaf":"markdown","64b8246b":"markdown","6cef6b66":"markdown","bbba467f":"markdown","53c897e2":"markdown","d57ee9cc":"markdown","27650291":"markdown","a96ccfdf":"markdown","6d7fa1d3":"markdown","ca0d63ba":"markdown","d94239ab":"markdown","3ce6e9b6":"markdown","980cb4f4":"markdown","0244f458":"markdown","cb3dbf33":"markdown","33202bb2":"markdown","28935498":"markdown","f46c51f6":"markdown","a6e9d69c":"markdown","efbe1cd6":"markdown","5f705825":"markdown","adb84371":"markdown","55aa8af7":"markdown","f4a97c7e":"markdown","17467162":"markdown","373b49d5":"markdown","51ee0093":"markdown","8ffe4606":"markdown","f6c4e03a":"markdown","1a3b9bee":"markdown","4cc5a9d8":"markdown","e53ed547":"markdown","3e9c9d2b":"markdown","498ef119":"markdown","5afc9597":"markdown","e941486a":"markdown","2b62229b":"markdown","4ceb17e2":"markdown","6817ccf8":"markdown","d6bc49cb":"markdown","602f677f":"markdown","7e0545f9":"markdown","8c22f245":"markdown","739a42cd":"markdown"},"source":{"fb2b9c3b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom scipy.stats import norm, skew\nfrom scipy import stats\n\n# Display all the columns of the dataframe\npd.pandas.set_option('display.max_columns',None)","2e7b34c8":"train= pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","7b8ab776":"train.head()","80e19688":"print(\"Before Dropping Id feature is : \", train.shape)\n\n#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\ntrain.drop(columns=['Id'], inplace=True)\ntest.drop(columns=['Id'], inplace=True)\n\nprint(\"After Dropping Id feature is : \", train.shape)","6c42e66b":"train.describe()","6a15b83e":"train[\"SalePrice\"].describe()","e54a23dc":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","1a3240cf":"# Deleting Outliers\n\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)\n\nfig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=15)\nplt.xlabel('GrLivArea', fontsize=15)\nplt.show()","9e65a052":"sns.distplot(train['SalePrice'] , fit=norm);\n\n(mu, sigma) = norm.fit(train['SalePrice'])\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\nfig = plt.figure()\nplt.show()","5025c8b2":"cat_data = [w for w in train.columns if train[w].dtype == 'O']","5356552f":"def make_frequency(index):\n    vc = train[cat_data[index]].value_counts().reset_index()\n    vc.columns = ['category', 'frequency']\n    \n    return vc\n\nfig = plt.figure(figsize=(20, 38))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\n\nfor i in range(1, len(cat_data)+1):\n    ax = fig.add_subplot(9, 5, i)\n    vc = make_frequency(i-1)\n    sns.barplot(x = 'category', y = 'frequency', data = vc)\n    plt.xlabel(cat_data[i-1])","a6da9609":"num_data = [c for c in train.columns if train[c].dtype !='O']","933bda80":"year_feature = [feature for feature in num_data if 'Yr' in feature or 'Year' in feature]\nyear_feature","3508ce34":"for feature in year_feature:\n     print(feature, train[feature].unique())","2739b0f8":"fig = plt.figure(figsize=(20, 18))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, len(year_feature)):\n    ax = fig.add_subplot(5, 7, i)\n#     ax.hist(train[year_feature[i]])\n    sns.histplot(train[year_feature[i]], kde=True, stat=\"density\", linewidth=0)\n    plt.xlabel(year_feature[i])","621ee7f7":"discrete_feature=[feature for feature in num_data if len(train[feature].unique())<25 and feature not in year_feature+['Id']]\nprint(\"Discrete Variables Count: {}\".format(len(discrete_feature)))","4f2b5a42":"discrete_feature","ab1e462d":"fig = plt.figure(figsize=(20, 18))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, len(discrete_feature)):\n    ax = fig.add_subplot(5, 7, i)\n#     ax.hist(train[year_feature[i]])\n    sns.histplot(train[discrete_feature[i]], kde=True, stat=\"density\", linewidth=0)\n    plt.xlabel(discrete_feature[i])","49889d82":"continuous_feature=[feature for feature in num_data if feature not in discrete_feature+year_feature+['Id']]\nprint(\"Continuous feature Count {}\".format(len(continuous_feature)))","839c5457":"fig = plt.figure(figsize=(20, 18))\nfig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, len(continuous_feature)):\n    ax = fig.add_subplot(5, 7, i)\n#     ax.hist(train[year_feature[i]])\n    sns.histplot(train[continuous_feature[i]], kde=True, stat=\"density\", linewidth=0)\n    plt.xlabel(continuous_feature[i])","dc274493":"def boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\nf = pd.melt(train, id_vars=['SalePrice'], value_vars=cat_data)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=3, sharex=False, sharey=False, height=5)\ng = g.map(boxplot, \"value\", \"SalePrice\")\nplt.show()","f7c3e6fb":"num_data = train.select_dtypes(include=[np.number])\nnum_data.head()","f4f111b7":"num_data.shape","84661fa7":"for feature in year_feature:\n    data=train.copy()\n    ## We will capture the difference between year variable and year the house was sold for\n    train[feature]=train['YrSold']-train[feature]\n\n    plt.scatter(train[feature],train['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.show()","fddec9d0":"len(year_feature)","e10e20e5":"fig = plt.figure(figsize=(20, 40))\nfig.subplots_adjust()\n# fig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, len(discrete_feature)):\n    axs = fig.add_subplot(8, 4, i)\n#     plt.scatter(x = col_numeric[i], y = col_numeric[-1], data = dtrain)\n    sns.scatterplot(x = discrete_feature[i], y = num_data[\"SalePrice\"], data = train)\n    plt.xlabel(discrete_feature[i])","a06cf8e6":"len(discrete_feature)","936d2511":"fig = plt.figure(figsize=(20, 40))\nfig.subplots_adjust()\n# fig.subplots_adjust(hspace=0.4, wspace=0.4)\nfor i in range(1, len(continuous_feature)):\n    axs = fig.add_subplot(8, 4, i)\n#     plt.scatter(x = col_numeric[i], y = col_numeric[-1], data = dtrain)\n    sns.scatterplot(x = continuous_feature[i], y = num_data[\"SalePrice\"], data = train)\n    plt.xlabel(continuous_feature[i])","561a8feb":"len(continuous_feature)","0590e61b":"# Combine Train and Test Dataset\n\nntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","c4c8f235":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head(30)","ae8775f9":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","c49904b4":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","25baaefa":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","08558044":"all_data['Alley'] = all_data['Alley'].fillna(\"None\")","84eac6f5":"all_data['Fence'] = all_data['Fence'].fillna(\"None\")","77638314":"all_data['FireplaceQu'] = all_data['FireplaceQu'].fillna(\"None\")","993911db":"all_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","892310a4":"for i in ('GarageType', 'GarageFinish', 'GarageQual', 'GarageCond'):\n    all_data[i] = all_data[i].fillna('None')","7bd810d2":"for i in ('BsmtQual', 'BsmtCond', 'BsmtFinType1', 'BsmtFinType2', 'BsmtExposure'):\n    all_data[i] = all_data[i].fillna('None')","81bae43c":"for i in ('BsmtFinSF1', 'BsmtUnfSF', 'BsmtFullBath', 'BsmtHalfBath', 'BsmtFinSF2'):\n    all_data[i] = all_data[i].fillna(0)","78d4b99e":"all_data['MasVnrType'] = all_data['MasVnrType'].fillna('None')\nall_data['MasVnrArea'] = all_data['MasVnrArea'].fillna(0)","d7b5570c":"all_data['MSZoning'] = all_data['MSZoning'].fillna(train['MSZoning'].mode()[0])","86bfcf15":"all_data['BsmtFullBath'] = all_data['BsmtFullBath'].fillna(0)","bb19bbaf":"all_data['Functional'] = all_data['Functional'].fillna('Typ')","ec9e8c84":"all_data['Utilities'] = all_data['Utilities'].fillna('AllPub')","01ab03c2":"for col in ('GarageArea', 'GarageCars', 'GarageYrBlt'):\n    all_data[col] = all_data[col].fillna(0)","16de6d97":"all_data['TotalBsmtSF'] = all_data['TotalBsmtSF'].fillna(0)","268aaa68":"all_data['KitchenQual'] = all_data['KitchenQual'].fillna('TA')","b56f14f6":"all_data['Electrical'] = all_data['Electrical'].fillna('SBrkr')","95a46c87":"for i in ('Exterior1st', 'Exterior2nd'):\n    all_data[i] = all_data[i].fillna('VinylSd')","19bf4cbd":"all_data['SaleType'] = all_data['SaleType'].fillna('WD')","1acb651c":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","7576332a":"corr_mat = train.corr()\nf, ax = plt.subplots(figsize=(24, 18))\nsns.heatmap(corr_mat, vmax=0.8, square=True, annot=True, fmt='.1f', ax=ax)","26de40e3":"top_correlations = train.corr()\ntop_feature_columns = top_correlations['SalePrice'][top_correlations['SalePrice'].values > 0.2].index.values\ntop_feature_columns","0f12e93e":"train['MasVnrArea'].fillna((train['MasVnrArea'].mean()), inplace=True)\n\n\nheat_map_with_top_correlated_features = np.append(top_feature_columns, np.array(['SalePrice']))\npearson_correlation_coefficients = np.corrcoef(train[heat_map_with_top_correlated_features[::-1]].T)\nplt.figure(figsize=(16,16))\nsns.set(font_scale=1)\nwith sns.axes_style('white'):\n    sns.heatmap(pearson_correlation_coefficients, yticklabels=heat_map_with_top_correlated_features[::-1], xticklabels=heat_map_with_top_correlated_features[::-1], fmt='.2f', annot_kws={'size': 10}, annot=True, square=True, cmap=None)","5c154722":"# Add New Feature\nall_data[\"TotalSF\"] = all_data[\"TotalBsmtSF\"] + all_data[\"1stFlrSF\"] + all_data[\"2ndFlrSF\"] + all_data['GrLivArea']\n\n# Drop Multicolinearity\nall_data.drop(columns=['GarageCars', '1stFlrSF', 'TotRmsAbvGrd'], inplace=True)","e79240e3":"not_top_correlations = train.corr()\nnot_top_feature_columns = not_top_correlations['SalePrice'][not_top_correlations['SalePrice'].values < 0.2].index.values\nnot_top_feature_columns","29ddad68":"all_data.drop(columns=['YearBuilt', 'YearRemodAdd', 'BsmtFinSF2', 'LowQualFinSF', 'BedroomAbvGr', 'BsmtHalfBath', 'KitchenAbvGr', 'GarageYrBlt', 'EnclosedPorch', '3SsnPorch','ScreenPorch', 'PoolArea', 'MiscVal', 'MoSold'], inplace=True)","b43f8618":"y_train = np.log1p(y_train)\n \nsns.distplot(y_train , fit=norm);\n(mu, sigma) = norm.fit(y_train)\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\nfig = plt.figure()\nres = stats.probplot(y_train, plot=plt)\nplt.show()","5e838293":"# MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n# Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)","3dc1a8ac":"all_data = all_data.replace({\"Alley\" : {\"None\" : 0, \"Grvl\" : 1, \"Pave\" : 2},\n                       \"BsmtCond\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"None\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"None\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"None\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"None\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"GarageFinish\": {\"None\": 0, \"Unf\": 1, \"RFn\": 2, \"Fin\": 3},\n                       \"CentralAir\": {\"N\" :0, \"Y\" :1},      \n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","bc43a35c":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head()","ca3cb2ae":"# Using Box-Cox Transformation\n\nskewness = skewness[abs(skewness.Skew)>0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","f0b1fcf5":"all_data = pd.get_dummies(all_data)\nprint(all_data.shape)","dd8b71ac":"all_data.head()","7791c8c0":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC, LassoCV\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom catboost import CatBoostRegressor\nimport xgboost as xgb\nimport lightgbm as lgb","efc131c2":"train = all_data[:ntrain]\ntest = all_data[ntrain:]","f8821aaf":"n_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","0ab6a7bf":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nscore = rmsle_cv(model_lgb)\nprint(\"LightGBM score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","e9f1aad9":"model_lgb.fit(train, y_train)\nlgb_train_pred = model_lgb.predict(train)\nlgb_pred = np.expm1(model_lgb.predict(test))\n\nprint(\"RMSE Train: %.4f\" % np.sqrt(mean_squared_error(y_train, lgb_train_pred)))","068a3dfc":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213,\n                             random_state =7, nthread = -1)\n\nscore = rmsle_cv(model_xgb)\nprint(\"XGBoost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2fe6e08a":"model_xgb.fit(train, y_train)\nxgb_train_pred = model_xgb.predict(train)\nxgb_pred = np.expm1(model_xgb.predict(test))\n\nprint(\"RMSE Train: %.4f\" % np.sqrt(mean_squared_error(y_train, xgb_train_pred)))","1e8abeac":"from sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\n\nkf = KFold(n_splits=12, random_state=42, shuffle=True)\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nmodel_ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\nscore = rmsle_cv(model_ridge)\nprint(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","17414df4":"model_ridge.fit(train, y_train)\nridge_train_pred = model_ridge.predict(train)\nridge_pred = np.expm1(model_ridge.predict(test))\n\nprint(\"RMSE Train: %.4f\" % np.sqrt(mean_squared_error(y_train, ridge_train_pred)))","4ccaafa8":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = lgb_pred\nsub.to_csv('submission_lgb.csv',index=False)","41b72054":"sub = pd.read_csv('submission_lgb.csv')\nsub.head(30)","f1a2e30c":"* **Transform**","f0777810":"* **BsmtFullBath**, just like other **Bsmt** if there is no value it means it doesn't have **Bsmt**, cause this **Feature** has a data type **Numeric** we will fill it with the value **0**.","37cebb22":"in **Discrete Feature** tends to have a **Symmetrical** shape","2e6cc064":"* Year Build vs Sale Price","2db1de2c":"## **3.3 Correlations**","fc5d72fa":"* **Fence**, in this case **NA** means **No Fence**, so we will fill it with **None**","83572bb5":"* **Handle Outliers**","4288e124":"* **BsmtFinSF1, BsmtUnfSF, BsmtFullBath, BsmtHalfBath, BsmtFinSF2**, if the value is missing it means there is no basement so we will fill it with **0** because the data type is **Float64**.","cabc936a":"## **Credit:**\n* https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard - **Serigne**\n* https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset - **Juliencs**","7b7093ec":"**Categorical Features**","9f9fa7d2":"## **4.2 Model**","3a354415":"* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2**, if the value is **NA**, it means that it does not have a basement so we will fill it with **None**.","980458ae":"* **Top Correlations**","caf6554d":"* **MSZoning**, if we see the value of **RL** is the one that appears the most, so we will fill the missing value with **RL** using **Mode** because it happens that **Feature** is of type **Categorical**","5eed3007":"* **Alley**, in this case **NA** means **No Allet Access**, so we will fil it with **None**","a9b97b34":"****","f0a31ff6":"**Numerical Features vs Sale Price**","2b6c2482":"* Countinous Features vs Sale Price","5119c30f":"**Numerical Features**","0b9ab3cf":"# **3. Feature Engineering**","61672079":"It can be seen that **YearRemodAdd** and **GarageYrBlt** are **Negative Skewed** data","6aa27547":"## **2.1 Target Variabel Analysis**","c310dae4":"As we can see, in **Continuous Variable** a lot of data is **Positive Skewed**, later we will try **Model** which is strong enough to deal with **Outlier**, this may be better than trying to delete **Outlier** as we did in **SalePrice**.","b2baf8db":"* **XGBoost**","5eca9346":"* **Ridge Regressor**","54f164be":"* **Label Encoding**","f4e919d2":"* **KitchenQual**, for missing values we will fill it with **TA** because this is the value that occurs most often.","678f0eaf":"## **2.3 Bi-Variate Analysis**","64b8246b":"## **3.4 Transform, Label Encoding, Outliers, One-Hot**","6cef6b66":"## **4.3 Submissions**","bbba467f":"* **LightGBM**","53c897e2":"* Date Time Variable (Temporal Variable)","d57ee9cc":"**Categorical Features vs Sale Price**","27650291":"* **MiscFeature**, in this case **NA** means **No Misc Feature**, so we will fil it with **None**","a96ccfdf":"* **GarageArea, GarageCars, GarageYrBlt**, same as other **Garage** if there is a **NA** value, it means that the place does not have a garage, then we will fill it with the value **0**.","6d7fa1d3":"## **4.1 Prepare Data and Library**","ca0d63ba":"* Imputing Missing Values\n* Correlations\n* Target Variable \n* Transform, Label Encoding, Adding Feature and Handle Outliers, Dummy","d94239ab":"# **2. Data Analysis**\n* Sale Price\n* Univariate Analysis\n* Bi-Variate Analysis","3ce6e9b6":"* **GarageFinish, GarageQual, GarageCond, GarageType**, missing value means **No Garage** and has something to do with **GarageYrBlt**, If **GarageYrBlt** doesn't exist how will we know **GarageFinish, GarageQual, GarageCond, GarageType ?**.   So we will fill it with **None**","980cb4f4":"* **Functional**, in the **Data Description** it says that **NA** means **Typ** then if we look at the values \u200b\u200bthat appear the most is **Typ**, so you can be sure to fill in the missing values with **Typ** is the right decision.","0244f458":"# **1. Load and Describing Data**","cb3dbf33":"* **Split Data**","33202bb2":"**What do we get from analyzing this correlation?**\n* **GarageCars** and **GarageArea** which are **Independent Variables** have a strong **Correlation**, this is clearly an indication of **Multicollinearity** and we will only take one of them, in this case we will just take **GarageArea**.\n\n* **1stFlrSF** and **TotalBsmtSF** also have **Multicollinearity**, here we will keep **TotalBsmtSF** because it has a higher score, but before removing **1stFlrSF** we will create **Feature* * newly named **TotalSF**. Because **1stFlrSF** is the area of the 1st floor and **2ndFlrSF** is the area of the 2nd floor, then **TotalBsmtSF** is the area of the Basement, **GrLivArea** also has a unit of measurement **SF** so we will add it as well. I think combining these four variables is a good thing because when we buy a house, the area is one of the things we think about.\n> From the experiment I did, making the **TotalSF** variable can reduce **RMSE** so that the error gets smaller, and removing **Multicolinarity** also has an effect.\n\n* **GrLivArea** and **TotRmsAbvGrd** also have **Multicolinearity**, and we will only take **GrLivArea** because they have a stronger correlation to **SalePrice**.","28935498":"* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF**, still the same as the other **Bsmt** an missing value here means it doesn't have a basement. So we will fill it with the value **0** because it happens to have a **Numeric** data type.","f46c51f6":"as we can see **Outliers** has been successfully deleted, but why are we deleting it? is it safe?, the answer is yes **Safe** because we only delete 2 **Outliers** while we still have many rows of data.","a6e9d69c":"* **TotalBsmtSF**, if you have Misssing VAlues it means you don't have a basement.","efbe1cd6":"* **FireplaceQu**, in this case **NA** means **No Fireplace**, so we will fill it with **None**","5f705825":"# **4. Model Building**","adb84371":"**A common analysis when the target variable is numeric is to look at the histogram, this is what we got:**\n* Have Posotive Skewness\n* Deviative from Normal Distributions\n* Minimum Values is Above 0","55aa8af7":"If we look at Numerical Variable which is changed to Categorical Variable usually has a value of Discrete, for example like MoSold which is worth Discrete because the number of Months is only 12 and will continue like that, so we can change it to Categorical Varibale. In contrast to YrSold although the value is limited to 2006-2010, it is possible that YrSold could be worth, say 2045 in the future when we enter new data, because as we know the value of Years is increasing every year.\n\nHowever, not all Numerical Variables of type Discrete are changed to Categorical especially for Feature whose values indicate level, such as OverallCond we don't change it to Categorical .\n\nThen maybe you will ask why we use Label Encoding instead of using One Hot Encoding, so the reason for using Lable Encoding is because most of the Categorical Features are Ordinal a.k.a have levels such as GarageQual, GarageCond and OverallCond, these variables clearly have levels.\n\n**We use One-Hot Encoding when:**\n\n* Non-ordinal Categorical Features (Like Country Name, City Name).\n* The number of Categorical Features is small so One-Hot Encoding can be implemented effectively.\n\n**We use Label Encoding when:**\n\n* Categorical Feature is ordinal (Like an assessment of the condition of a room).\n* The number of Categorical Features is so large that if we use One-Hot Encoding it will take up a lot of memory.\n\n\n**Source** [https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/one-hot-encoding-vs-label-encoding-using-scikit-learn\/](http:\/\/)","f4a97c7e":"* **PoolQC**, if we look in **train.csv** most of the data contains **NA** which means **No Pool** so we will fill it with **None**, and it makes sense that most houses don't have **Pool** ","17467162":"* **SaleType**, as before we will fill mising value with the **Mode** i.e. **WD**.","373b49d5":"## **2.2 Univariate Analysis**","51ee0093":"**Check if there are still have missing values**","8ffe4606":"* **LotFrontage** : Since the area of each street connected to the house property, we can **fill in missing values by the median LotFrontage of the neighborhood**.","f6c4e03a":"* **Not Top Correlations**","1a3b9bee":"* **Validation**","4cc5a9d8":"* Continuous Variable","e53ed547":"#### **Final Submissions Score:** 0.12556(RMSE)","3e9c9d2b":"* **MasVnrType dan MasVnrArea**, missing value means that there is no masonry veneer for the house, because **MasVnrType** is of type **Object** we will fill it with **None**, then for **MasVnrArea** because the data type is **Int** we will fill it with **0**.","498ef119":"The first thing we do when buying a house is the **size**.\n\nSo we're going to find the **relationship** between **SalePrice** and **GrLivArea**, and it should be linear.\nAs we can see two dots on the right side where the **GrLivingArea** is more and the **SalePrice** is very less. It could be the agricultural land nearby the city. But if you analyse the data carefully, these two points doesnt fit in here. These are called **Outliers**.","5afc9597":"## **3.1 Imputing Missing Values**","e941486a":"* **Utilities**, if we look again into **Dataset** most have value **AllPub**, so we will fill in the empty value with **AllPub**","2b62229b":"* **Import Library**","4ceb17e2":"****","6817ccf8":"* Discrete Feature vs Sale Price","d6bc49cb":"* **Dummy**","602f677f":"* Discrete Variable","7e0545f9":"* **Electrical**, we will fill with **SBrkr** cause that value appears the most.","8c22f245":"# **Workflow**\n### 1. Load and Describing Data\n### 2. Data Analysis\n### 3. Feature Engineering\n### 4. Model Building\n### 5. Submissions","739a42cd":"## **3.3 Target Variable**"}}