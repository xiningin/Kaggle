{"cell_type":{"c0d335b6":"code","5fb6205c":"code","6ab33c09":"code","9cd5c891":"code","78918de1":"code","a433aada":"code","3d77290b":"code","25a4f6e5":"code","3d91237e":"code","804cb2b8":"code","84cbe4c5":"code","9739c2ef":"code","3f495459":"code","8940048b":"markdown","1aa1c699":"markdown","5f9995f9":"markdown","f91e0a58":"markdown","64c9c31c":"markdown","d99dd4b8":"markdown","563a575e":"markdown","b0695944":"markdown","5ff5f52b":"markdown","66e30c14":"markdown"},"source":{"c0d335b6":"import numpy as np\nimport pandas as pd\nimport os\nimport sys\nimport seaborn as sns\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom sklearn.model_selection import StratifiedKFold, KFold\nimport logging\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom wordcloud import WordCloud,STOPWORDS\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize,sent_tokenize\nimport re,string,unicodedata\nfrom kaggle_datasets import KaggleDatasets\nimport matplotlib.pyplot as plt\n\nsns.set_style(\"darkgrid\")\nlogging.basicConfig(level=logging.INFO)","5fb6205c":"train = pd.read_csv('..\/input\/commonlitreadabilityprize\/train.csv')\ntest = pd.read_csv('..\/input\/commonlitreadabilityprize\/test.csv')\n\ntrain.head()","6ab33c09":"debug = False\nif debug:\n    train = train.sample(1000)","9cd5c891":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(train[train.target > 0].excerpt))\nplt.imshow(wc , interpolation = 'bilinear')","78918de1":"plt.figure(figsize = (20,20))\nwc = WordCloud(max_words = 1000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(train[train.target < 0].excerpt))\nplt.imshow(wc , interpolation = 'bilinear')","a433aada":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=train[train.target < 0]['excerpt'].str.len()\nax1.hist(text_len,color='red')\nax1.set_title('Low Readability')\ntext_len=train[train.target > 0]['excerpt'].str.len()\nax2.hist(text_len,color='green')\nax2.set_title('High Readability')\nfig.suptitle('Characters in texts')\nplt.show()","3d77290b":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(12,8))\ntext_len=train[train.target < 0]['excerpt'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='red')\nax1.set_title('Low Readability')\ntext_len=train[train.target > 0]['excerpt'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='green')\nax2.set_title('High Readability')\nfig.suptitle('Words in texts')\nplt.show()","25a4f6e5":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=train[train.target < 0]['excerpt'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Low Readability')\nword=train[train.target > 0]['excerpt'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('High Readability')\nfig.suptitle('Average word length in each text')","3d91237e":"%%time\nsys.path.append('..\/input\/tokenization')\nimport tensorflow_hub as hub \nimport tokenization\nmodule_url = '..\/input\/bert-en-uncased-l12-h768-a122'\nbert_layer = hub.KerasLayer(module_url, trainable=True)","804cb2b8":"tf.gfile = tf.io.gfile\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ndef bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence) + [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef build_model(bert_layer, max_len=512):\n    input_word_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = tf.keras.Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    net = tf.keras.layers.Dense(64, activation='relu')(clf_output)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    net = tf.keras.layers.Dense(16, activation='relu')(net)\n    net = tf.keras.layers.Dropout(0.2)(net)\n    out = tf.keras.layers.Dense(1, activation='linear')(net)\n    \n    model = tf.keras.models.Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(tf.keras.optimizers.Adam(lr=1e-5), loss='mean_squared_error')\n    \n    return model","84cbe4c5":"preds = None\nkf = KFold(n_splits = 5 , shuffle = True , random_state = 42)\nfor fold , (train_index , val_index) in enumerate(kf.split(train[\"excerpt\"] , train['target'])):\n    print(\"Training Fold {}\".format(fold))\n    \n    x_train, x_val = train.excerpt.values[train_index], train.excerpt.values[val_index]\n    y_train, y_val = train.target.values[train_index], train.target.values[val_index]\n    \n    max_len = 300\n    train_input = bert_encode(x_train, tokenizer, max_len=max_len)\n    val_input = bert_encode(x_val, tokenizer, max_len=max_len)\n    test_input = bert_encode(test.excerpt.values, tokenizer, max_len=max_len)\n    \n    BATCH_SIZE = 16\n    \n    name = \"model_fold_{}\".format(fold) +\".h5\"\n    \n    checkpoint = tf.keras.callbacks.ModelCheckpoint(name, monitor='val_loss', save_best_only=True, mode='min', verbose=1)\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor = 0.5 , patience=2, mode='min', verbose=1)\n\n    load_locally = tf.saved_model.LoadOptions(experimental_io_device='\/job:localhost')\n    bert_layer = hub.KerasLayer(module_url, trainable=True , load_options=load_locally)\n    model = build_model(bert_layer, max_len=max_len)\n    \n    train_history = model.fit(\n        train_input, y_train, \n        epochs=15,\n        callbacks=[checkpoint, reduce_lr],\n        batch_size=BATCH_SIZE,\n        validation_data = (val_input, y_val),\n        verbose=1)\n    \n    model.load_weights(name)\n    \n    if preds is None:\n        preds = model.predict(test_input)\n    else:\n        preds += model.predict(test_input)\n\npreds = preds\/5","9739c2ef":"preds[:5]","3f495459":"%%time\nsub = pd.read_csv('..\/input\/commonlitreadabilityprize\/sample_submission.csv')\n\nsub['target'] = preds\nsub.to_csv('submission.csv', index=False)","8940048b":"# Step_1 load packages and data","1aa1c699":"# Step_3: Run model and export predictions","5f9995f9":"# Step_2 data cleaning","f91e0a58":"# Step_2: build bert_layer and model","64c9c31c":"**Wordcloud for HIGH readability text**","d99dd4b8":"**Number of characters in texts**","563a575e":"**Average word length in a text**","b0695944":"**Wordcloud for LOW readability text**","5ff5f52b":"1. We will need bert `Tokenization` class","66e30c14":"**Number of words in each text**"}}