{"cell_type":{"569060c9":"code","bee204f7":"code","69f7291a":"code","df1a7a93":"code","9e865b44":"code","f59838d0":"code","4e2f49fe":"code","8ccce164":"code","7158b503":"code","4f8e04ab":"code","8e21f2f0":"code","8e174d34":"code","4df43516":"code","12393384":"code","c0fc6859":"code","1bba724f":"code","df90a26d":"code","ee0a67d0":"code","cab33b0a":"code","56b24a6c":"code","892f8a1c":"code","7611d5d6":"code","b22bed4a":"code","a120d082":"code","efbc4a50":"code","08ceb326":"code","e53b1705":"code","7ca5a008":"code","e08a66ee":"code","ce27caba":"code","14fa6fca":"code","b4e67c4b":"code","0e110515":"code","3158b5bf":"code","740d3bf1":"code","4ab6bf34":"code","b0fb556d":"code","b7b8a038":"code","171083ff":"code","5914b48b":"code","cdcd6129":"code","322da9bc":"code","4b4d3870":"code","14e106d8":"code","ad8af829":"code","904dae7d":"code","03064114":"code","58d5abc5":"code","fdde1177":"code","a679aa0b":"code","fe953303":"code","06c06794":"code","195421f0":"code","d9cae3f7":"code","6671d8f3":"code","a94e93a2":"code","52d1f8b4":"code","db7b1f14":"code","aaf74c77":"code","823fd32b":"code","2094699d":"code","2967732d":"code","5c6e2060":"code","19292eb7":"code","d853b9c0":"code","a8fe36b7":"code","c9e8e76e":"code","d0fc6230":"code","aaab3629":"code","76ffcd78":"code","3d910fd0":"code","206e4e3e":"code","5e4d8176":"code","da82963e":"code","51af16ca":"code","1a509f53":"markdown","cac5aafe":"markdown","bfff8e7b":"markdown","104fc2e6":"markdown","2244e96c":"markdown","f89cf367":"markdown","f4890bc7":"markdown","1f405a88":"markdown","e988d44c":"markdown","e2b55454":"markdown","31290825":"markdown","f858467b":"markdown","e8d2b43d":"markdown","1e2f2d39":"markdown","b2f9acc1":"markdown","b03e2d6f":"markdown","9012e0a1":"markdown","baf44b52":"markdown","dc169aca":"markdown","23d2e5c6":"markdown","eedbdd1b":"markdown","d196ffc4":"markdown","2d2ce493":"markdown","bcbdb280":"markdown","4c276630":"markdown","fcd2f1a4":"markdown","7c537dc8":"markdown"},"source":{"569060c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bee204f7":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.model_selection import (\n    GridSearchCV,\n    RandomizedSearchCV,\n    ShuffleSplit,\n    cross_val_score,\n    cross_validate,\n    train_test_split,\n)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","69f7291a":"import warnings\nwarnings.filterwarnings(\"ignore\")","df1a7a93":"conda install -c conda-forge eli5","9e865b44":"!pip install shap","f59838d0":"import shap\nimport eli5","4e2f49fe":"heart_df = pd.read_csv('\/kaggle\/input\/heart-disease-uci\/heart.csv')","8ccce164":"heart_df.head(3)","7158b503":"heart_df.shape","4f8e04ab":"heart_df.info()","8e21f2f0":"# Finding out if the target is numerical or categorical\nheart_df.target.value_counts()","8e174d34":"heart_df.sex.value_counts()","4df43516":"# Chest pain type\nheart_df.cp.value_counts()","12393384":"# fasting blood sugar\nheart_df.fbs.value_counts()","c0fc6859":"heart_df.restecg.value_counts()","1bba724f":"heart_df.exang.value_counts()","df90a26d":"heart_df.slope.value_counts()","ee0a67d0":"heart_df.ca.value_counts()","cab33b0a":"heart_df.target.value_counts()","56b24a6c":"scoring_metric = 'accuracy'","892f8a1c":"train_df, test_df = train_test_split(heart_df, test_size=0.2, random_state=2)\ntrain_df.head()","7611d5d6":"X_train, y_train = train_df.drop(columns=[\"target\"]), train_df['target']\nX_test, y_test = test_df.drop(columns=[\"target\"]), test_df['target']","b22bed4a":"def mean_std_cross_val_scores(model, X_train, y_train, **kwargs):\n    \"\"\"\n    Returns mean and std of cross validation\n    \"\"\"\n    scores = cross_validate(model, \n                            X_train, y_train, \n                            **kwargs)    \n    \n    mean_scores = pd.DataFrame(scores).mean()\n    std_scores = pd.DataFrame(scores).std()\n    out_col = []\n\n    for i in range(len(mean_scores)):  \n        out_col.append((f\"%0.3f (+\/- %0.3f)\" % (mean_scores[i], std_scores[i])))\n\n    return pd.Series(data = out_col, index = mean_scores.index)","a120d082":"numeric_features = ['age','trestbps', 'chol',\n                     'thalach', 'exang', 'oldpeak', 'ca', 'thal']\n\ncategorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope']","efbc4a50":"numeric_transformer = make_pipeline(SimpleImputer(strategy='median'), \n                                    StandardScaler())\n\ncategorical_transformer = make_pipeline(SimpleImputer(strategy='constant'),\n                                        OneHotEncoder(handle_unknown='ignore', sparse=False))","08ceb326":"preprocessor = make_column_transformer(\n    (numeric_transformer, numeric_features),\n    (categorical_transformer, categorical_features) \n)","e53b1705":"results = {}","7ca5a008":"from sklearn.dummy import DummyClassifier\ndummy = DummyClassifier(strategy='stratified')\nresults['Dummy'] = mean_std_cross_val_scores(dummy, X_train, y_train, return_train_score=True, scoring=scoring_metric)","e08a66ee":"from sklearn.tree import DecisionTreeClassifier\npipe_decision_tree = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123))\nresults['Decision tree'] = mean_std_cross_val_scores(pipe_decision_tree, X_train, y_train, return_train_score=True, scoring=scoring_metric)\npd.DataFrame(results)","ce27caba":"from sklearn.ensemble import RandomForestClassifier\npipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\nresults['Random forests'] = mean_std_cross_val_scores(pipe_rf, X_train, y_train, return_train_score=True, scoring=scoring_metric)\npd.DataFrame(results)","14fa6fca":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom lightgbm.sklearn import LGBMClassifier\nfrom catboost import CatBoostClassifier\n\npipe_lr = make_pipeline(preprocessor, LogisticRegression(max_iter=2000, random_state=123))\npipe_dt = make_pipeline(preprocessor, DecisionTreeClassifier(random_state=123))\npipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=123))\npipe_xgb = make_pipeline(preprocessor, XGBClassifier(random_state=123))\npipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\npipe_catboost = make_pipeline(preprocessor, CatBoostClassifier(verbose=0, random_state=123))\nclassifiers = {\n    'logistic regression' : pipe_lr,\n    'decision tree' : pipe_dt,\n    'random forest' : pipe_rf,\n    'XGBoost' : pipe_xgb, \n    'LightGBM' : pipe_lgbm,\n    'CatBoost' : pipe_catboost\n}","b4e67c4b":"for (name, model) in classifiers.items():    \n    results[name] = mean_std_cross_val_scores(model, X_train, y_train, return_train_score=True, scoring=scoring_metric)","0e110515":"pd.DataFrame(results)","3158b5bf":"\npipe_lr.fit(X_train, y_train)\ndisp = plot_confusion_matrix(pipe_lr, X_train, y_train, \n                      display_labels=[0, 1], \n                      values_format=\"d\", \n                      cmap=plt.cm.Blues);\n","740d3bf1":"pipe_dt.fit(X_train, y_train)\ndisp = plot_confusion_matrix(pipe_dt, X_train, y_train, \n                      display_labels=[0, 1], \n                      values_format=\"d\", \n                      cmap=plt.cm.Blues);","4ab6bf34":"pipe_rf.fit(X_train, y_train)\ndisp = plot_confusion_matrix(pipe_rf, X_train, y_train, \n                      display_labels=[0, 1], \n                      values_format=\"d\", \n                      cmap=plt.cm.Blues);","b0fb556d":"pipe_catboost.fit(X_train, y_train)\ndisp = plot_confusion_matrix(pipe_catboost, X_train, y_train, \n                      display_labels=[0, 1], \n                      values_format=\"d\", \n                      cmap=plt.cm.Blues);","b7b8a038":"classifiers.keys()","171083ff":"from sklearn.ensemble import VotingClassifier\n\naveraging_model = VotingClassifier(list(classifiers.items()), voting='soft')","5914b48b":"averaging_model.fit(X_train, y_train);","cdcd6129":"results['Voting'] = mean_std_cross_val_scores(averaging_model, X_train, y_train)","322da9bc":"pd.DataFrame(results)","4b4d3870":"classifiers_ndt = classifiers.copy()\ndel classifiers_ndt[\"decision tree\"]\naveraging_model_ndt = VotingClassifier(list(classifiers_ndt.items()), voting='soft') # need the list() here for cross_val to work!\n\nresults['Voting_ndt'] = mean_std_cross_val_scores(averaging_model_ndt, X_train, y_train, return_train_score=True, scoring=scoring_metric)","14e106d8":"pd.DataFrame(results)","ad8af829":"from sklearn.ensemble import StackingClassifier","904dae7d":"stacking_model = StackingClassifier(list(classifiers.items()))","03064114":"stacking_model.fit(X_train, y_train);","58d5abc5":"pd.DataFrame(data=stacking_model.final_estimator_.coef_[0], index=classifiers.keys(), columns=[\"Coefficient\"])","fdde1177":"results['Stacking'] = mean_std_cross_val_scores(stacking_model, X_train, y_train, return_train_score=True, scoring=scoring_metric)","a679aa0b":"pd.DataFrame(results)","fe953303":"stacking_model_tree = StackingClassifier(list(classifiers.items()), \n                 final_estimator=DecisionTreeClassifier(max_depth=3))","06c06794":"stacking_model_tree.fit(X_train, y_train);","195421f0":"import re\nimport graphviz\nfrom sklearn.tree import export_graphviz\n\ndef display_tree(feature_names, tree):\n    \"\"\" For binary classification only \"\"\"\n    dot = export_graphviz(tree, out_file=None, feature_names=feature_names, class_names=tree.classes_.astype(str), impurity=False, precision=4)\n    # adapted from https:\/\/stackoverflow.com\/questions\/44821349\/python-graphviz-remove-legend-on-nodes-of-decisiontreeclassifier\n    dot = re.sub('(\\\\\\\\nsamples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])(\\\\\\\\nclass = [A-Za-z0-9]+)', '', dot)\n    dot = re.sub(     '(samples = [0-9]+)(\\\\\\\\nvalue = \\[[0-9]+, [0-9]+\\])\\\\\\\\n', '', dot)\n    return graphviz.Source(dot)","d9cae3f7":"from sklearn import tree\nimport graphviz\ndisplay_tree(list(classifiers.keys()), stacking_model_tree.final_estimator_)","6671d8f3":"preprocessor.fit(X_train)\npreprocessor.named_transformers_","a94e93a2":"ohe_columns = list(preprocessor.named_transformers_['pipeline-2'].named_steps['onehotencoder'].get_feature_names(categorical_features))","52d1f8b4":"new_columns = numeric_features + ohe_columns","db7b1f14":"X_train_enc = pd.DataFrame(preprocessor.transform(X_train), index=X_train.index, columns=new_columns)\nX_train_enc","aaf74c77":"feature_names = (\n    numeric_features + ohe_columns\n)\nfeature_names[:10]","823fd32b":"data = {\n    \"coefficient\": pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist(),\n    \"magnitude\": np.absolute(\n        pipe_lr.named_steps[\"logisticregression\"].coef_[0].tolist()\n    ),\n}\ncoef_df = pd.DataFrame(data, index=feature_names).sort_values(\n    \"magnitude\", ascending=False\n)\ncoef_df[:10]","2094699d":"pipe_rf = make_pipeline(preprocessor, RandomForestClassifier(random_state=2))\npipe_rf.fit(X_train, y_train);","2967732d":"ohe_columns","5c6e2060":"data = {\n    \"Importance\": pipe_rf.named_steps[\"randomforestclassifier\"].feature_importances_,\n}\nimps = pd.DataFrame(data=data, index=feature_names,).sort_values(\n    by=\"Importance\", ascending=False\n)[:10]\nimps","19292eb7":"data = {\n    \"random forest importance\": pipe_rf.named_steps[\n        \"randomforestclassifier\"\n    ].feature_importances_,\n    \"logistic regression importances\": pipe_lr.named_steps[\"logisticregression\"]\n    .coef_[0]\n    .tolist(),\n}\nimps = pd.DataFrame(\n    data=data,\n    index=feature_names,\n)\nimps.sort_values(by=\"random forest importance\", ascending=False)[:10]","d853b9c0":"pipe_lgbm = make_pipeline(preprocessor, LGBMClassifier(random_state=123))\npipe_lgbm.fit(X_train, y_train)\neli5.explain_weights(\n    pipe_lgbm.named_steps[\"lgbmclassifier\"], feature_names=feature_names\n)","a8fe36b7":"X_train_enc.head()","c9e8e76e":"rf = RandomForestClassifier()\nrf.fit(X_train_enc, y_train)\nX_train_sample = X_train_enc.sample(100, random_state=2)\nexplainer = shap.TreeExplainer(rf)\nshap_values = explainer.shap_values(X_train_sample)","d0fc6230":"shap_values","aaab3629":"shap_values[0].shape","76ffcd78":"values = np.abs(shap_values[0]).mean(0)\npd.DataFrame(data=values, index=feature_names, columns=[\"SHAP\"]).sort_values(\n    by=\"SHAP\", ascending=False\n)","3d910fd0":"# load JS visualization code to notebook\nshap.initjs()","206e4e3e":"X_train_sample.iloc[11, :]","5e4d8176":"rf.predict(X_train_sample)[11]","da82963e":"explainer.expected_value","51af16ca":"shap.force_plot(\n    explainer.expected_value[0],\n    shap_values[0][11, :],\n    X_train_sample.iloc[11, :],\n    matplotlib=True,\n)","1a509f53":"Finding out about the nature of the features. Although the ifo shows that they are all numerical but some numerical ones might be categorical (having specified levels).","cac5aafe":"- In the feature importance process for nonlinear models the results do not give us the direction of impact. It might be that an important feature might first increase the prediction and then decreas it. This does not happen in linear models, as we are dealing with a linear model.","bfff8e7b":"## Comparing the feature importance of the two models:","104fc2e6":"### 1. Importing necessary packages.","2244e96c":"## Tree Based Models\n- Decision Tree Classifier\n- Logistic Regression\n- XGBoost \n- CatBoost\n- LightGBM","f89cf367":"### 2. Reading data. ","f4890bc7":"## Voting Classifier","1f405a88":"- These are the results with default hyperparameters.\n- The next step is to carry out hyperparameter optimization.\n- At this stage we choose the classifier with highest CV score, which is CatBoost as it is the least  overfitting classifier also.","e988d44c":"Models agree on some features importances. 'thalach' is improtant for both models.","e2b55454":"### 8. Separating numeric and categorical features and preparing preprocessing pipeline.","31290825":"## Baseline Dummy Classifier","f858467b":"# Feature importnace for non sklearn models","e8d2b43d":"## Random Forest Feature Importance","1e2f2d39":"### 5. Spliting the data. ","b2f9acc1":"## Stacking Classifier","b03e2d6f":"### 4. Identifying the best score to use.","9012e0a1":"## Logistic Regression Feature Importance","baf44b52":"## Baseline Decision Tree","dc169aca":"## Fitting Stacking Classifier","23d2e5c6":"### At this stage we choose Stacking as it has the highest validation score and it is not overfitting also.","eedbdd1b":"### 6. Separating target from features.","d196ffc4":"### 3. Exploring data.","2d2ce493":"## Improting Packages and EDA","bcbdb280":"## Displaying the tree","4c276630":"### 7. Creating a function to store the scores.","fcd2f1a4":"## Random Forest","7c537dc8":"#### 3.1 Identifying numeric and categorical features."}}