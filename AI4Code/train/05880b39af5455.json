{"cell_type":{"41d5e623":"code","efe33240":"code","3717af77":"code","af6ae539":"code","d0a0963d":"code","d304f6e5":"code","c35b3988":"code","a9456994":"code","45c46678":"code","af0cb884":"code","bb81509e":"code","5daf4243":"code","699a6246":"code","425c0a27":"code","f512afa7":"code","ae2309fe":"code","906f1762":"code","63c65776":"code","ba9633c7":"code","eca5acef":"code","6ec4772f":"code","ffef5285":"code","a71a07c6":"code","f6c78cc9":"code","5b936729":"code","1079e99b":"code","3a869e7b":"code","1561e5cb":"code","0fd00323":"code","0aa68850":"code","5b06d0e2":"code","da935fcf":"code","3ce429a7":"code","9710db5d":"code","2fd4229b":"code","466f58e3":"code","6c13d408":"code","af62a115":"code","5cd6a188":"code","8619e72e":"code","cf63fad2":"code","6f293a22":"code","943c01ee":"code","90329ed2":"code","ec44b0d9":"code","7e7e9e88":"code","c63b4387":"code","5830cf69":"code","3f932c31":"code","e1c43f66":"code","0d912920":"code","3789ebbd":"code","cdab4542":"code","e26e43b5":"code","febb3d90":"code","d0abb1bf":"code","bf1843e5":"code","8d6f00d0":"code","0c7373e7":"code","2c342c7f":"code","a5cb7305":"code","ecbbd040":"code","4cbf1488":"code","072e5058":"code","0d1d6521":"code","6b129598":"code","303dfeb6":"code","b469e745":"code","77fe6647":"code","86e529a9":"code","34510e2a":"code","47c75bad":"code","9795b7ab":"code","5779a21b":"code","89c4733f":"code","d78aa509":"code","411e007c":"code","f507ed9e":"code","6f44a1f9":"code","3e0aa733":"code","55d2fba8":"code","30c18812":"code","61c2e2da":"code","dc1f0614":"code","f676d629":"code","1de21e08":"code","ce3232b0":"code","b0e9aa11":"code","2ea285b7":"code","b5547cb4":"code","1e88321c":"code","54d6dfc1":"code","86d0667c":"code","31a42965":"code","13cd2ed4":"code","d13e0f86":"code","d9542753":"code","7f58807c":"code","f4972cd7":"code","f2321af6":"code","44efc690":"markdown","e10f65ff":"markdown","66a91ef3":"markdown","8178b9e8":"markdown","fb7d8315":"markdown","0cea44b4":"markdown","3202ff18":"markdown","3fab88d9":"markdown","1c9c6768":"markdown","1c54d2a8":"markdown","08e0aa8c":"markdown","521b980c":"markdown","dfcbe343":"markdown","c7a1e32f":"markdown","b003fde9":"markdown","fabd9191":"markdown","33acfd50":"markdown","52fd172f":"markdown","0b033282":"markdown","d73fd075":"markdown","ba61d1d1":"markdown","6f2fa10d":"markdown","92f7f0e1":"markdown","bf8cc77d":"markdown","2a002583":"markdown","d1f7086c":"markdown","f1643650":"markdown","0be4a21c":"markdown","98412f6e":"markdown","740766c3":"markdown","ca69aa6e":"markdown","8b21be50":"markdown","9ceb3c79":"markdown","52fffc07":"markdown","05760067":"markdown","f5b1cea3":"markdown","8d5a004a":"markdown","1470c21a":"markdown","dbebe1d0":"markdown","b4377df8":"markdown","0d4d4d7e":"markdown","660328a4":"markdown","419646ea":"markdown","34289071":"markdown","215daeb6":"markdown","6343af2f":"markdown","19fdc863":"markdown","99a2978b":"markdown","6ddfb983":"markdown","e6f0508d":"markdown","30e0df02":"markdown"},"source":{"41d5e623":"import contextlib\nimport time\n\n@contextlib.contextmanager\ndef timer():\n    start = time.time()\n    \n    yield\n\n    end = time.time()\n    runtime = 'Runtime: {:.2f}s \\n'.format(end - start)\n    print(runtime)","efe33240":"pip install datatable #==0.11.0 > \/dev\/null","3717af77":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datatable as dt\n\n%matplotlib inline\n\n#train = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/train.csv\")\ntrain_datatable = dt.fread('..\/input\/jane-street-market-prediction\/train.csv')\ntrain = train_datatable.to_pandas()\n\nfeatures = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/features.csv\")\nexample_test = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/example_test.csv\")\nexample_sample_submission = pd.read_csv(\"\/kaggle\/input\/jane-street-market-prediction\/example_sample_submission.csv\")","af6ae539":"train.shape","d0a0963d":"train.head(10)","d304f6e5":"features.shape","c35b3988":"features.head(10)","a9456994":"example_test.shape","45c46678":"example_test.head(10)","af0cb884":"#Feature categorization\nid_feature = \"ts_id\"\ntarget_feature = \"resp\"\nevaluation_features = [\"weight\", \"resp_1\", \"resp_2\", \"resp_3\", \"resp_4\", \"resp\"]\nanonymized_features = [x for x in train.columns if \"feature\" in x]\ndatetime_features = [\"date\"]","bb81509e":"train_sample = train.sample(n=100000, random_state=1).sort_index()","5daf4243":"from scipy.stats import kendalltau, pearsonr, spearmanr\nimport scipy.stats as stats\nfrom sklearn.feature_selection import chi2\nfrom sklearn.metrics import mutual_info_score\nfrom sklearn.metrics import adjusted_mutual_info_score\n\n#Features overview function\n\ndef kendall_pval(x,y):\n    return kendalltau(x,y)[1]\n\ndef pearsonr_pval(x,y):\n    return pearsonr(x,y)[1]\n\ndef spearmanr_pval(x,y):\n    return spearmanr(x,y)[1]\n\ndef generate_features_overview(df):\n    df_info = pd.DataFrame()\n    df_info[\"type\"] = df.dtypes\n    df_info[\"missing_count\"] = df.isna().sum()\n    df_info[\"missing_perc\"] = df_info[\"missing_count\"] \/ len(df)\n    df_info[\"unique\"] = df.nunique()\n    df_info[\"skew\"] = df.skew()\n    df_info[\"kurt\"] = df.kurt()\n    df_info[\"corr\"] = df.corrwith(df[target_feature], method=\"spearman\")\n    df_info[\"corr_p_value\"] = df.corrwith(df[target_feature], method=spearmanr_pval)\n    df_info = pd.concat([df_info, df.describe().T], axis=1)\n    \n    return df_info","699a6246":"#Generate overview dataframe\ndf_info = generate_features_overview(train_sample)","425c0a27":"df_info[\"type\"].value_counts()","f512afa7":"df_info[df_info[\"type\"] == \"int64\"].index","ae2309fe":"train_sample[\"feature_0\"].value_counts()","906f1762":"eval_info = df_info.loc[evaluation_features]\neval_info","63c65776":"sns.distplot(train_sample[\"weight\"])","ba9633c7":"train_sample[\"weight\"].value_counts()","eca5acef":"cumsum_df = pd.DataFrame()\ncumsum_df[\"resp_1\"] = train_sample['resp_1'].cumsum()\ncumsum_df[\"resp_2\"] = train_sample['resp_2'].cumsum()\ncumsum_df[\"resp_3\"] = train_sample['resp_3'].cumsum()\ncumsum_df[\"resp_4\"] = train_sample['resp_4'].cumsum()\ncumsum_df[\"resp\"] = train_sample['resp'].cumsum()","6ec4772f":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ncumsum_df[\"resp\"].plot()","ffef5285":"sns.distplot(train_sample[\"resp\"])","a71a07c6":"fig, ax = plt.subplots(2,2,figsize=(15,10))\n\neval_info.drop(\"weight\", inplace=True)\n\nsns.barplot(x=eval_info.index, y=\"mean\", data=eval_info, ax=ax[0, 0])\nsns.barplot(x=eval_info.index, y=\"std\", data=eval_info, ax=ax[1, 0])\nsns.barplot(x=eval_info.index, y=\"skew\", data=eval_info, ax=ax[0, 1])\nsns.barplot(x=eval_info.index, y=\"kurt\", data=eval_info, ax=ax[1, 1])","f6c78cc9":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ncumsum_df[\"resp_1\"].plot()\ncumsum_df[\"resp_2\"].plot()\ncumsum_df[\"resp_3\"].plot()\ncumsum_df[\"resp_4\"].plot()\ncumsum_df[\"resp\"].plot()\nplt.legend(loc=\"upper left\")","5b936729":"train_sample[\"action\"] = train_sample[\"resp\"].apply(lambda x: 1 if x > 0 else 0)","1079e99b":"anony_info = df_info.loc[anonymized_features]\nanony_info","3a869e7b":"fig, ax = plt.subplots(1,1,figsize=(20,7))\nsns.barplot(x=anony_info.index, y=\"min\", data=anony_info)\nsns.barplot(x=anony_info.index, y=\"max\", data=anony_info)\n\nax.set_xticklabels(range(0,130), rotation=90)\nax.set_ylabel('min \/ max')","1561e5cb":"#Number of unique values\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nsns.barplot(x=anony_info.index, y=\"unique\", data=anony_info)\n\nax = ax.set_xticklabels(range(0,130), rotation=90)","0fd00323":"#Percentage of missing values\nfig, ax = plt.subplots(1,1,figsize=(20,5))\nsns.barplot(x=anony_info.index, y=\"missing_perc\", data=anony_info)\n\nax = ax.set_xticklabels(range(0,130), rotation=90)","0aa68850":"high_missing = anony_info[anony_info[\"missing_perc\"] > 0.10].index\nhigh_missing","5b06d0e2":"#Mean and standard deviation\nfig, ax = plt.subplots(2,1,figsize=(20,10))\n\ng1 = sns.barplot(x=anony_info.index, y=\"mean\", data=anony_info, ax=ax[0])\ng2 = sns.barplot(x=anony_info.index, y=\"std\", data=anony_info, ax=ax[1])\n\ng2.set_xticklabels(range(0,130), rotation=90)\ng1.set(xlabel=None)\ng1.set(xticklabels=[])","da935fcf":"#Skewness and kurtosis\nfig, ax = plt.subplots(2,1,figsize=(20,10))\ng1 = sns.barplot(x=anony_info.index, y=\"skew\", data=anony_info, ax=ax[0])\ng2 = sns.barplot(x=anony_info.index, y=\"kurt\", data=anony_info, ax=ax[1])\ng2.set_xticklabels(range(0,130), rotation=90)\ng1.set(xlabel=None)\ng1.set(xticklabels=[])","3ce429a7":"fig, ax = plt.subplots(2,1,figsize=(20,10))\ng1 = sns.barplot(x=anony_info.index, y=\"corr\", data=anony_info, ax=ax[0])\ng2 = sns.barplot(x=anony_info.index, y=\"corr_p_value\", data=anony_info, ax=ax[1])\ng2.set_xticklabels(range(0,130), rotation=90)\n#g1.set_xticklabels(range(0,130), rotation=90)\ng1.set(xlabel=None)\ng1.set(xticklabels=[])","9710db5d":"#Fill missing values\nfor i in range(0, 130):\n    feat = \"feature_{}\".format(i)\n    train_sample[feat].fillna(train_sample[feat].mean(), inplace=True)","2fd4229b":"#Correlation function\n\ndef show_corr_heatmap(df, method=\"pearson\", width=10, calc_corr=False, annot=True):\n    \n    if calc_corr == True:\n        if method == \"MI\":\n            corr = MI_correlations(df)\n        else:\n            corr = df.corr(method)\n    else:\n        corr = df\n        \n    # Generate a mask for the upper triangle\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n\n    # Set up the matplotlib figure\n    f, ax = plt.subplots(figsize=(width, width))\n\n    # Generate a custom diverging colormap\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n\n    # Draw the heatmap with the mask and correct aspect ratio\n    sns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, center=0,\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=annot, fmt=\".2f\")\n    \n    if calc_corr == True:\n        return corr\n\n\ndef MI_correlations(df):\n    corrs = {}\n    for col_init in df.columns:\n        corrs[col_init] = {}\n        for col_corr in df.columns:\n            if col_init != col_corr:\n                corrs[col_init][col_corr] = calc_MI(df[col_init], df[col_corr])\n\n    return pd.DataFrame(corrs)\n\ndef calc_MI(col_init, col_corr):\n    \n    if col_init.dtype == np.object:\n        col_init = col_init.astype('category').cat.codes\n    elif col_init.dtype.name == \"category\":\n        col_init = col_init.cat.codes\n        \n    if col_corr.dtype == np.object:\n        col_corr = col_corr.astype('category').cat.codes\n    elif col_corr.dtype.name == \"category\":\n        col_corr = col_corr.cat.codes\n\n    mi = mutual_info_score(col_init, col_corr)\n\n    return mi","466f58e3":"#Calculate correlation matrix using custom function\ncorr_matrix = show_corr_heatmap(train_sample[anonymized_features], method=\"spearman\", width=30, calc_corr=True, annot=False)","6c13d408":"#Features dataframe heatmap\nfig, ax = plt.subplots(figsize=(30, 30))\nsns.heatmap(features.iloc[:,1:])","af62a115":"show_corr_heatmap(corr_matrix.iloc[0:41, 0:41], width=30)","5cd6a188":"fig, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(features.iloc[:41,1:])","8619e72e":"#Possible feature groups, add 1 to get even values\n\ng1_feat = [1, [3, 5]]\ng1_feat_1 = [7, [17, 27]] #add 2 to get other resp values","cf63fad2":"show_corr_heatmap(corr_matrix.iloc[41:72, 41:72], width=30)","6f293a22":"fig, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(features.iloc[41:72,1:])","943c01ee":"#Possible feature groups\n\ng2_feat = [[41, 42, 43, 53], [44, 45], [46, 47, 48, 49, 50, 51, 52], 54, [60, 61], [62, 63], 64, [65, 66], [67, 68], [69, 70, 71]]\ng2_feat_1 = [55] #add 1 to get other resp values","90329ed2":"show_corr_heatmap(corr_matrix.iloc[72:120, 72:120], width=30)","ec44b0d9":"fig, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(features.iloc[72:120,1:])","7e7e9e88":"#Possible feature groups\n\ng3_feat = [[77, 83], [89, 95, 101], [107, 113, 119]] #last 2 elements can be grouped\ng3_feat_1 = [[72, 78], [84, 96, 108], [90, 102, 114]] #add 1 at first element to get other resp values, last 2 elements can be grouped","c63b4387":"show_corr_heatmap(corr_matrix.iloc[120:130, 120:130])","5830cf69":"fig, ax = plt.subplots(figsize=(20, 12))\nsns.heatmap(features.iloc[120:130,1:])","3f932c31":"#Possible feature groups\ng4_feat_1 = [120, 121] #add 2 to get other resp values","e1c43f66":"# All features groups\n# g1_feat = [1, [3, 5]] #add 1 to get even values\n# g2_feat = [[41, 42, 43, 53], [44, 45], [46, 47, 48, 49, 50, 51, 52], 54, [60, 61], [62, 63], 64, [65, 66], [67, 68], [69, 70, 71]]\n# g3_feat = [[77, 83], [89, 95, 101], [107, 113, 119]] #last 2 elements can be grouped\n\n# g1_feat_1 = [7, [17, 27]] #add 2 to get other resp values, add 1 to get even values\n# g2_feat_1 = [55] #add 1 to get other resp values\n# g3_feat_1 = [[72, 78], [84, 96, 108], [90, 102, 114]] #add 1 to get other resp values, last 2 elements can be grouped\n# g4_feat_1 = [120, 121] #add 2 to get other resp values","0d912920":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_7\"].cumsum().plot()\ntrain_sample[\"feature_9\"].cumsum().plot()\ntrain_sample[\"feature_11\"].cumsum().plot()\ntrain_sample[\"feature_13\"].cumsum().plot()\ntrain_sample[\"feature_15\"].cumsum().plot()\nplt.legend(loc=\"upper left\")","3789ebbd":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_17\"].cumsum().plot()\ntrain_sample[\"feature_19\"].cumsum().plot()\ntrain_sample[\"feature_21\"].cumsum().plot()\ntrain_sample[\"feature_23\"].cumsum().plot()\ntrain_sample[\"feature_25\"].cumsum().plot()\nplt.legend(loc=\"upper left\")","cdab4542":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_55\"].cumsum().plot()\ntrain_sample[\"feature_56\"].cumsum().plot()\ntrain_sample[\"feature_57\"].cumsum().plot()\ntrain_sample[\"feature_58\"].cumsum().plot()\ntrain_sample[\"feature_59\"].cumsum().plot()\nplt.legend(loc=\"upper left\")","e26e43b5":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_84\"].cumsum().plot()\ntrain_sample[\"feature_85\"].cumsum().plot()\ntrain_sample[\"feature_86\"].cumsum().plot()\ntrain_sample[\"feature_87\"].cumsum().plot()\ntrain_sample[\"feature_88\"].cumsum().plot()\nplt.legend(loc=\"upper left\")","febb3d90":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_1\"].cumsum().plot()\ntrain_sample[\"feature_2\"].cumsum().plot()","d0abb1bf":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_3\"].cumsum().plot()\ntrain_sample[\"feature_4\"].cumsum().plot()","bf1843e5":"fig, ax = plt.subplots(1,1,figsize=(15,5))\ntrain_sample[\"feature_7\"].cumsum().plot()\ntrain_sample[\"feature_8\"].cumsum().plot()","8d6f00d0":"#Select between features 3 - 6\ndf_info.iloc[10:14,:]","0c7373e7":"fig, ax = plt.subplots(1,4,figsize=(20,5))\n\nsns.scatterplot(x=\"feature_3\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_4\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])\nsns.scatterplot(x=\"feature_5\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[2])\nsns.scatterplot(x=\"feature_6\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[3])","2c342c7f":"#Select between features 17 - 36\ndf_info.loc[[\"feature_25\", \"feature_26\",\"feature_35\", \"feature_36\"]]","a5cb7305":"fig, ax = plt.subplots(1,4,figsize=(20,5))\n\nsns.scatterplot(x=\"feature_25\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_26\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])\nsns.scatterplot(x=\"feature_35\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[2])\nsns.scatterplot(x=\"feature_36\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[3])","ecbbd040":"selected_group_1 = [1, 3, 15, 35]","4cbf1488":"#Select between features 41 - 43, 53\ndf_info.loc[[\"feature_41\", \"feature_42\",\"feature_43\", \"feature_53\"]]","072e5058":"fig, ax = plt.subplots(1,4,figsize=(20,5))\n\nsns.scatterplot(x=\"feature_41\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_42\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])\nsns.scatterplot(x=\"feature_43\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[2])\nsns.scatterplot(x=\"feature_53\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[3])","0d1d6521":"#Select between features 44 - 45\ndf_info.loc[[\"feature_44\", \"feature_45\"]]","6b129598":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_44\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_45\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","303dfeb6":"#Select between features 46 - 52\ndf_info.loc[[\"feature_46\", \"feature_47\", \"feature_48\", \"feature_49\", \"feature_50\", \"feature_51\", \"feature_52\"]]","b469e745":"fig, ax = plt.subplots(2,4,figsize=(20,10))\nsns.scatterplot(x=\"feature_46\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 0])\nsns.scatterplot(x=\"feature_47\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 1])\nsns.scatterplot(x=\"feature_48\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 2])\nsns.scatterplot(x=\"feature_49\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 3])\nsns.scatterplot(x=\"feature_50\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 0])\nsns.scatterplot(x=\"feature_51\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 1])\nsns.scatterplot(x=\"feature_52\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 2])","77fe6647":"#Select between features 60 - 61\ndf_info.loc[[\"feature_60\", \"feature_61\"]]","86e529a9":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_60\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_61\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","34510e2a":"#Select between features 62 - 63\ndf_info.loc[[\"feature_62\", \"feature_63\"]]","47c75bad":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_62\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_63\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","9795b7ab":"#Select between features 65 - 66\ndf_info.loc[[\"feature_65\", \"feature_66\"]]","5779a21b":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_65\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_66\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","89c4733f":"#Select between features 67 - 68\ndf_info.loc[[\"feature_67\", \"feature_68\"]]","d78aa509":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_67\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_68\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","411e007c":"#Select between features 69 - 71\ndf_info.loc[[\"feature_69\", \"feature_70\", \"feature_71\"]]","f507ed9e":"fig, ax = plt.subplots(1,3, figsize=(15,5))\n\nsns.scatterplot(x=\"feature_69\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_70\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])\nsns.scatterplot(x=\"feature_71\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[2])","6f44a1f9":"selected_group_2 = [43, 44, 52, 54, 59, 60, 62, 64, 65, 67, 70]","3e0aa733":"#Select between features 77 - 83\ndf_info.loc[[\"feature_77\", \"feature_83\"]]","55d2fba8":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_77\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_83\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","30c18812":"#Select between features 89, 95, 101, 107, 113, 119\ndf_info.loc[[\"feature_89\", \"feature_95\", \"feature_101\", \"feature_107\", \"feature_113\", \"feature_119\"]]","61c2e2da":"fig, ax = plt.subplots(2,3, figsize=(15,10))\n\nsns.scatterplot(x=\"feature_89\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 0])\nsns.scatterplot(x=\"feature_101\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 1])\nsns.scatterplot(x=\"feature_113\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 2])\n\nsns.scatterplot(x=\"feature_95\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 0])\nsns.scatterplot(x=\"feature_107\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 1])\nsns.scatterplot(x=\"feature_119\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 2])","dc1f0614":"#Select between features 76, 82\ndf_info.loc[[\"feature_76\", \"feature_82\"]]","f676d629":"fig, ax = plt.subplots(1,2, figsize=(10,5))\n\nsns.scatterplot(x=\"feature_76\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0])\nsns.scatterplot(x=\"feature_82\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1])","1de21e08":"#Select between features 88, 100, 112, 94, 106, 118\ndf_info.loc[[\"feature_88\", \"feature_100\", \"feature_112\", \"feature_94\", \"feature_106\", \"feature_118\"]]","ce3232b0":"fig, ax = plt.subplots(2,3, figsize=(15,10))\n\nsns.scatterplot(x=\"feature_88\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 0])\nsns.scatterplot(x=\"feature_100\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 1])\nsns.scatterplot(x=\"feature_112\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[0, 2])\n\nsns.scatterplot(x=\"feature_94\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 0])\nsns.scatterplot(x=\"feature_106\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 1])\nsns.scatterplot(x=\"feature_118\", y=\"resp\", hue=\"action\", data=train_sample, ax=ax[1, 2])","b0e9aa11":"selected_group_3 = [76, 83, 88, 107]\nselected_group_4 = [128, 129]","2ea285b7":"selected_features = selected_group_1 + selected_group_2 + selected_group_3 + selected_group_4\nprint(selected_features)","b5547cb4":"selected_features_names = [\"feature_\" + str(index) for index in selected_features]\ndf_info.loc[selected_features_names]","1e88321c":"print(selected_features_names)","54d6dfc1":"# All features groups\n# g1_feat = [1, [3, 5]] #add 1 to get even values\n# g2_feat = [[41, 42, 43, 53], [44, 45], [46, 47, 48, 49, 50, 51, 52], 54, [60, 61], [62, 63], 64, [65, 66], [67, 68], [69, 70, 71]]\n# g3_feat = [[77, 83], [89, 95, 101], [107, 113, 119]] #last 2 elements can be grouped\n\n# g1_feat_1 = [7, [17, 27]] #add 2 to get other resp values, add 1 to get even values\n# g2_feat_1 = [55] #add 1 to get other resp values\n# g3_feat_1 = [[72, 78], [84, 96, 108], [90, 102, 114]] #add 1 to get other resp values, last 2 elements can be grouped\n# g4_feat_1 = [120, 121] #add 2 to get other resp values","86d0667c":"datetime_info = df_info.loc[datetime_features]\ndatetime_info","31a42965":"fig, ax = plt.subplots(1,1, figsize=(10,5))\nsns.distplot(train_sample[train_sample[\"action\"] == 0][\"date\"], hist=False, label=\"action_0\")\nsns.distplot(train_sample[train_sample[\"action\"] == 1][\"date\"], hist=False, label=\"action_1\")\nplt.legend(loc=\"lower center\")","13cd2ed4":"group = train_sample.groupby(\"date\").agg({\"weight\": [np.median]}).reset_index()\ngroup.columns = group.columns.get_level_values(0)","d13e0f86":"fig, ax = plt.subplots(1,1, figsize=(10,5))\nsns.lineplot(x=\"date\", y=\"weight\", data=group)","d9542753":"selected_features_names = ['feature_0', 'feature_1', 'feature_5', 'feature_15', 'feature_35', 'feature_41', 'feature_43', 'feature_44', 'feature_45', 'feature_52', 'feature_59', 'feature_60', 'feature_62', 'feature_64', 'feature_65', 'feature_67', 'feature_70', 'feature_76', 'feature_83', 'feature_107', 'feature_128']","7f58807c":"#Select above threshold anonymus features\nfeatures_to_remove_outliers = []\nfeatures_to_scale = []\n\n#anonymized_features\nfor feat in selected_features_names:\n    if df_info.loc[feat, \"kurt\"] > 2:\n        features_to_remove_outliers.append(feat)\n\n    if abs(df_info.loc[feat, \"skew\"]) > 0.5:\n        features_to_scale.append(feat)","f4972cd7":"from scipy import stats\n\ndef transform(df_temp):\n    \n    df = df_temp.copy()\n        \n    # Remove weight = 0\n    with timer():\n        df = df[df[\"weight\"] != 0]\n        \n    # Calculate z-scores values and remove outliers\n    with timer():\n        for feat in features_to_remove_outliers:\n            feat_zscore = feat + '_zscore'\n            df[feat_zscore] = (df[feat] - df[feat].mean())\/df[feat].std()\n        \n        df[\"max_feat_zscore\"] = df[df.columns[-len(features_to_remove_outliers):]].abs().max(axis=1)\n        df = df[df[\"max_feat_zscore\"] < 6]\n\n    # Use arcsinh on features with high skewness\n    with timer():\n        for feat in features_to_scale:\n            df[feat] = np.arcsinh(df[feat])\n        \n\n    #Fill missing values\n    with timer():\n        for feat in selected_features_names:\n            df[feat].fillna(method='ffill', inplace=True)\n\n    #Normalization\n    with timer():\n        df[selected_features_names]=(df[selected_features_names] - df[selected_features_names].mean()) \/ df[selected_features_names].std()\n    \n    return df","f2321af6":"%%time\ntrain_transformed = transform(train)[selected_features_names]","44efc690":"Majority of features remain between 100 and -100, however there are a few far above those margins. Features 55 to 59 seem similar.\n\nNow let's take a look at the number of unique values and missing values.","e10f65ff":"Correlation matrix is really large and confusing but there are clearly some patterns. I will cut it in parts for easier understanding and compare it to features dataframe.","66a91ef3":"There are 130 numerical values and as we already mentioned, the first one is binary.\n\nFirst I will plot the values ranges.","8178b9e8":"First I am categorizing features into logical groups.","fb7d8315":"Highlights:\n- Tag 23 is the category for this group of features\n- Tags 24, 25, 26 and 27 are subcategories, the correlation also indicates this split\n- Tags 15 and 17 also split those subcategories\n- Tags 0 - 4 could be indicating resp\n- Features groups 72-77, 78-83, 84-89, 90-95, 96-101, 102-107, 108-113, 114-119 have the same pattern in missing values - they could show the same data\n- High correlation between features 95, 107, 119 and 89, 101, 113 - only one combo of tag 5 \/ tag 15 and tag 17 \/ tag 24, 25 and 26 should be used\n- Tag 15 and 17 features are also in a strong correlation, meaning only features in one of those tags should be used","0cea44b4":"Highlights:\n- Even features 120, 122, 124, 126, 128 and odd ones 121, 123, 125, 127, 129 are strong positive correlated.\n- The pattern again give us one more proof that tag 0 to 4 features are very similar and we should only use one of them","3202ff18":"Checking the features data types.","3fab88d9":"All of the features are continuous:\n- Weight ranges between 0 and 162 with the median of 0.55 (I am using median because of high skewness)\n- Resp features are distributed around 0","1c9c6768":"Mean and standard deviation charts look very similar. Raising mean tells us the resp is growing over time, but the std is also growing resulting in higher risk.\nSkewness remains in the margins of normal like distribution and kurtosis drop results in reduction of outliers over time, however the number is still high - we should expect outliers.\n\nComparing all the resp values indicates that the resp_4 and resp value should be swapped to make resp values chronological.\n\nI will create new feature named \"action\" for easier data visualisation. If resp > 0 then 1, otherwise 0.","1c54d2a8":"### Feature 120 - 129 ###","08e0aa8c":"### Feature 72 - 119 ###","521b980c":"I am using datatable trick to speed up the train data cvs to pandas conversion. Source: https:\/\/www.kaggle.com\/rohanrao\/tutorial-on-reading-large-datasets","dfcbe343":"There are lots of unique values in most features which indicates most features are continuos, however some of them stand out. Those are feature 0, 43, 51, 52, 53 and 69 and they should be further inspected.","c7a1e32f":"<a id=\"section-2.2\"><\/a>\n## 2.2 Anonymized features ##","b003fde9":"<a id=\"section-2\"><\/a>\n# 2. DATA EXPLORATION AND CLEANING #","fabd9191":"Feature 3 \/ 4 is much better choice since it is more compact and has less outliers.","33acfd50":"Most of the features have less them 4% of missing values, but there are also some with more than 16%.\n\nWe can also see a clear pattern here indicating that some features could be grouped.\n\nPossible feature groups:\n- Feature 7 - 36 (30 features)\n- Feature 72 - 119 (48 features)\n\nNext I will take a look at the distribution moments.","52fd172f":"## Correlations of anonymous features ##","0b033282":"<a id=\"section-1\"><\/a>\n# 1. ENVIRONMENT CREATION AND DATA IMPORT #","d73fd075":"<a id=\"section-2.3\"><\/a>\n## 2.3 Date features ##","ba61d1d1":"I will now plot cumulative sums for resp related features.","6f2fa10d":"Feature_0 type is even more than integer, it is binary!","92f7f0e1":"Train dataset is very large with 2.390.491 rows and 138 columns. 130 of those columns are anonymized features (we have metadata in features dataframe).\nRemaining 8 columns represent:\n- date - the day of trading opportunity\n- weight - the trading opportunity magnitude (the higher the value, the more it contributes to the final result)\n- resp_x - trading opportunity gain\/loss for different time horizons\n- resp - trading opportunity final gain\/loss - target_feature\n- ts_id - time ordering, it is also the id_feature","bf8cc77d":"All odd features in group 1 have their twins in even features, this is also indicated by tag_9. I will now plot some pairs, to get better insight into data.","2a002583":"I will take feature 35 since it has the lowest kurtosis, however it has relatively high skewness, therefore it will have to be transformed later.\nFrom the first group we have selected the features [1, 3, 15, 35].\n\nNow to the second group, which is much more complex.","d1f7086c":"Table of contents:\n- [1. ENVIRONMENT CREATION AND DATA IMPORT](#section-1)\n\n\n- [2. DATA EXPLORATION, CLEANING AND FEATURE SELECTION](#section-2)\n    - [Evaluation features](#section-2.1)\n    - [Anonymized features](#section-2.2)\n    - [Date features](#section-2.3)\n    - [Features transformation](#section-2.4)","f1643650":"Mean values are very close to zero but there are some groups forming is the chart (features 72 to 83).\nSome standard deviations are above 6 (features 55 - 59). ","0be4a21c":"The test set example is the same as train, except for resp columns. It confirms that we are predicting resp values. If the value is more than 0, the trading opportunity should be accepted, otherwise declined.\n\nThe resp value is continous, meaning the regression models will have to be used.","98412f6e":"We can see 2 peaks in date distribution data. The first peak has more trading opportunities where we should take action, the second peak is the other way around.\nWeight values seems to be raising over time, therefore we can gain more in the second peak but it is riskier.","740766c3":"Comparing some of the chart pairs shows almost the same data, which is also indicated by features 100% correlation. I will use the first item from those pairs.\nGroup 2 selected features: [43, 44, 52, 54, 59, 60, 62, 64, 65, 67, 69]\n\nNext we have group 3","ca69aa6e":"Comparing the charts above to the resp cumulative sum values, we can see that purple feature best corresponds to our resp value. Therefore we can make the following assumptions:\n- Tag_0 = resp_4\n- Tag_1 = resp\n- Tag_2 = resp_3\n- Tag_3 = resp_2\n- Tag_4 = resp_1\n\nNow I will go through all the features and select the most relavant.","8b21be50":"<a id=\"section-2.4\"><\/a>\n## 2.4 Features transformation ##","9ceb3c79":"Resp value trend is extremely positive. It is also distributed with low skewness and high kurtosis. Other resp features should be very similar. Let's compare them.","52fffc07":"Train dataframe is very large, that is why data exploration should be performed on its sample. Then I am running my custom function for general data overview.","05760067":"Highlights:\n- High correlation between features 46 - 52, grouped in tag 19\n- High correlation between features 55 - 59, those are also categorized in tag 0 - 4 (resp?). Also grouped by tag 21. They have much higher std\/skew\/kurtosis then the other columns.\n- Feature 44 has one one good correlation in this group - feature 45, could be related to tag 15\n- There is a clear pattern between features 60 - 68, they are all tagged with 12 and 13, which were also positive correlated in the previous group. The exception is feature 64.\n- Good correlation between features 69 - 71 grouped in tag 20. Feature 70 and 71 could have alternating tag 9 like in previous group","f5b1cea3":"The evaluation metric is based on the product between weight and resp. Therefore, we take only the opportunities where resp is higher than 0, otherwise we will make a loss.\nOn the other hand, weight magnifies the gain\/loss, if the weight is 0, we can't gain nor lose.","8d5a004a":"## Imported datasets overview ##","1470c21a":"I will group those features in one list and create another list with actual column names.","dbebe1d0":"After the feature analysis we should take care of skewness and kurtosis for each of them. Skewness should be between -0,5 and 0,5 for each feature to become normal like distribution. To take care of kurtosis I will remove outliers with z-score above 5 and below -5. I am trying to get the value between +2 and -2.\n\nFinally I will downgrade the features for faster calculations.","b4377df8":"Most of the features are floats, but there are also 3 integers. According to the competition documentation, \"date\" represents a day and \"ts_id\" represents time ordering.\nHowever the third one is interesting and should be inspected further.","0d4d4d7e":"Looks like odd and even values act as some kind of boundaries, where odd values are upper boundary and even values are bottom boundary.","660328a4":"<a id=\"section-2.1\"><\/a>\n## 2.1 Evaluation features ##","419646ea":"There seems to be no correlation to target feature, since the highest coefficient value is around 0.04. Again we can see some features groups forming.\n\nNext I will take a look at the correlation between anonymous features themselves, I will also use features dataframe.\nBut first I am filling the missing values with mean for faster calculation.","34289071":"### Feature 41 - 71 ###\n\nThis is my second group according to the correlation matrix and the lack of correlation between features 41-71 to 17-40.","215daeb6":"### Feature 0 - 40 ###\n\nAccording to the correlation matrix and features df it seem like a good first step. My goal is to decode the features tags.","6343af2f":"For every anonymized feature, there is 29 boolean columns of metadata.","19fdc863":"Highlights:\n- Feature 0 is binary and has strong correlation with features 17-40.\n- Odd features from 1 to 39 are strong positive correlated to even features 2 to 40 - tag 9 could be indicating this\n- Features group 17-26 & 37-38 and group 27-36 & 39-40 are all strong positive correlated, however both groups are strong negative correlated between each other\n- There should be used only one feature in the range 17-40 due to mutual correlations\n\nTags Decoding:\n- Tags 0 to 4 could represent different resp values\n- The features in tag 4 also have high percentage of missing values\n- Alternating tag 9 could be representing the strong odd\/even features correlation, therefore we should use one or another\n- Tag 6 could be representing first main category of features\n- Tags 7, 8, 10, 11, 12 and 13 could be the subcategory of 6 due to their features mutual correlation","99a2978b":"Again we can see some very similar values, here is my selection: [76, 83, 88, 107]. I group 4 we need to select only the correct resp features: [128, 129].","6ddfb983":"Weight feature is extremely right skewed since we have 17.072 (app. 17% of the data) values 0. It's kurtosis of 72 is also extremely high meaning there are lots of outliers.","e6f0508d":"Skewness of most features is far above +\/- 0.5 (threshold for normal distribution). There are also many high values of kurtosis, meaning we will have to deal with outliers.\nAgain we can see the group of features 55-59, which should be further inspected as a group.\n\nBut first I will take a look at the correlation to the target feature.","30e0df02":"I am creating cumulative sums on resp values to visualise the trend."}}