{"cell_type":{"4cfc3fcf":"code","b75b4231":"code","35b67e2b":"code","e9476d76":"code","f5b3746b":"code","b14ef996":"code","59f610b1":"code","6ad077f4":"code","4e43e171":"code","797dac4b":"code","e506c642":"code","70bce035":"code","190638e7":"code","0d909463":"code","8d395546":"code","e2454826":"code","6e37b52f":"code","b7f38442":"code","30db4b3b":"code","e54f510c":"code","30533b7e":"code","458377a5":"code","a92230d0":"code","5e613929":"code","3a94fcd7":"code","5304c3f9":"code","e431aebb":"code","92317f01":"code","e1fb3595":"code","987e5979":"code","fe068e15":"code","73565914":"code","9b1e32d8":"code","6422de46":"code","a0b134c1":"code","5cb02ea9":"code","aef34a00":"code","56fd1923":"code","98045e17":"code","b00e5527":"code","2cc900ec":"code","7ed05859":"code","b31d0718":"code","49ea0cc5":"code","c9c873ae":"markdown","1e91ca77":"markdown","64be13d4":"markdown","6bb37b53":"markdown","2d752c24":"markdown","26180cf7":"markdown","e5886b58":"markdown","16199bf4":"markdown","5559a8ba":"markdown","60b7cec5":"markdown","69d71d18":"markdown","3fd794c8":"markdown","69b6e604":"markdown","b7252484":"markdown","441c7df6":"markdown","a49721da":"markdown","576eb957":"markdown","a8e7dddc":"markdown","00f7609c":"markdown","a625bc49":"markdown","b89faa82":"markdown","07e231e9":"markdown","cd1acb58":"markdown","c77a2d83":"markdown","ed7c8932":"markdown"},"source":{"4cfc3fcf":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport datetime\nimport calendar","b75b4231":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\n","35b67e2b":"from keras.models import Sequential\nfrom keras.layers.recurrent import LSTM, SimpleRNN\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom keras.optimizers import RMSprop,Adam","e9476d76":"import warnings\nwarnings.filterwarnings(\"ignore\")","f5b3746b":"path_in = '..\/input\/'\nprint(os.listdir(path_in))","b14ef996":"train_data = pd.read_csv(path_in + 'train.csv', parse_dates = ['datetime'],\n                         index_col='datetime', infer_datetime_format=True)\ntest_data = pd.read_csv(path_in + 'test.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)\nsamp_subm = pd.read_csv(path_in+'sampleSubmission.csv', parse_dates = ['datetime'],\n                        index_col='datetime', infer_datetime_format=True)","59f610b1":"def plot_bar(data, feature):\n    \"\"\" Plot distribution \"\"\"\n    \n    fig = plt.figure(figsize=(5,3))\n    sns.barplot(x=feature, y='count', data=data, palette='Set3',orient='v')","6ad077f4":"def plot_timeseries(data, feature):\n    \"\"\" Plot timeseries \"\"\"\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(data.index, data[feature])\n    plt.title(feature)\n    plt.grid()","4e43e171":"def plot_timeseries_train_and_predict(train, predict, year, month):\n    \"\"\" Compare train and predict data for a month \"\"\"\n    \n    start_date = datetime.datetime(year, month, 1, 0, 0, 0)\n    last_day_of_month = calendar.monthrange(year, month)[1]\n    end_date = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n    \n    fig = plt.figure(figsize=(16,9))\n    plt.plot(train[start_date: end_date].index, train.loc[start_date:end_date, 'count'], 'b', label = 'train')\n    plt.plot(predict[start_date: end_date].index, predict.loc[start_date:end_date, 'count'], 'r', label = 'predict')\n    plt.title('Train and Predict')\n    plt.legend()\n    plt.grid()","797dac4b":"def rmse(y_true, y_pred):\n    \"\"\" root_mean_squared_error \"\"\"\n    \n    return K.sqrt(K.mean(K.square(y_pred - y_true)))","e506c642":"# Parameters\nnum_months_per_year = 12\nyear_list = [2011, 2012]","70bce035":"train_data.head()","190638e7":"test_data.head()","0d909463":"samp_subm.head()","8d395546":"train_data_temp = pd.DataFrame(columns=train_data.columns)\n\nfor year in year_list:\n    for month in range(num_months_per_year):\n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        # Fill missing timestamps\n        temp = train_data[start_date:end_date].resample('H').asfreq()\n        # Handle missing values\n        features_fill_zero = ['casual', 'registered', 'count']\n        temp[features_fill_zero] = temp[features_fill_zero].fillna(0)\n        features_fill_bbfil = ['season', 'holiday', 'workingday', 'weather']\n        temp[features_fill_bbfil] = temp[features_fill_bbfil].fillna(method='bfill')\n        features_fill_linear = ['temp', 'atemp', 'humidity', 'windspeed']\n        temp[features_fill_linear] = temp[features_fill_linear].interpolate(method='linear')\n        \n        train_data_temp = train_data_temp.append(temp)\n        \ntrain_data = train_data_temp","e2454826":"test_data_temp = pd.DataFrame(columns=test_data.columns)\nfor year in year_list:\n    for month in range(num_months_per_year):\n        start_date = datetime.datetime(year, month+1, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month+1)[1]\n        end_date = datetime.datetime(year, month+1, last_day_of_month, 23, 0, 0)\n        # Fill missing timestamps\n        temp = test_data[start_date:end_date].resample('H').asfreq()\n        # Handle missing values\n        features_fill_bbfil = ['season', 'holiday', 'workingday', 'weather']\n        temp[features_fill_bbfil] = temp[features_fill_bbfil].fillna(method='bfill')\n        features_fill_linear = ['temp', 'atemp', 'humidity', 'windspeed']\n        temp[features_fill_linear] = temp[features_fill_linear].interpolate(method='linear')\n        \n        test_data_temp = test_data_temp.append(temp)\n        \ntest_data = test_data_temp","6e37b52f":"train_data['weekday'] = train_data.index.weekday\ntrain_data['hour'] = train_data.index.hour\n# train_data['month'] = train_data.index.month\n# train_data['year'] = train_data.index.year\ntest_data['weekday'] = test_data.index.weekday\ntest_data['hour'] = test_data.index.hour\n# test_data['month'] = test_data.index.month\n# test_data['year'] = test_data.index.year","b7f38442":"for year in year_list:\n    for month in range(num_months_per_year): \n        start_date = datetime.datetime(year, month+1, 1, 0, 0, 0)\n        end_date = datetime.datetime(year, month+1, 19, 23, 0, 0)\n        count_mean = train_data[start_date:end_date]['count'].mean()\n        train_data.loc[start_date:end_date, 'count_mean'] = count_mean\n        \n        start_date = datetime.datetime(year, month+1, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month+1)[1]\n        end_date = datetime.datetime(year, month+1, last_day_of_month, 23, 0, 0)\n        test_data.loc[start_date:end_date, 'count_mean'] = count_mean","30db4b3b":"test_data.head()","e54f510c":"plot_bar(train_data, 'season')\nplt.grid()","30533b7e":"plot_bar(train_data, 'weekday')\nplt.grid()","458377a5":"plot_bar(train_data, 'hour')\nplt.grid()","a92230d0":"def hour_group(s):\n    if((0<=s) & (s<=6)):\n        return 1\n    elif((s==7) | (s==9)):\n        return 2\n    elif((s==8) | (s==16) | (s==19)):\n        return 3\n    elif((10<=s) & (s<=15)):\n        return 4\n    elif((s==17) | (s==18)):\n        return 5\n    elif(20<=s):\n        return 6","5e613929":"train_data['hour_group'] = train_data['hour'].apply(hour_group)\ntest_data['hour_group'] = test_data['hour'].apply(hour_group)","3a94fcd7":"# features_cyc = ['hour', 'weekday']\n# for feature in features_cyc:\n#     train_data[feature+'_sin'] = np.sin((2*np.pi*train_data[feature])\/max(train_data[feature]))\n#     train_data[feature+'_cos'] = np.cos((2*np.pi*train_data[feature])\/max(train_data[feature]))\n#     test_data[feature+'_sin'] = np.sin((2*np.pi*test_data[feature])\/max(test_data[feature]))\n#     test_data[feature+'_cos'] = np.cos((2*np.pi*test_data[feature])\/max(test_data[feature]))\n# train_data = train_data.drop(features_cyc, axis=1)\n# test_data = test_data.drop(features_cyc, axis=1)","5304c3f9":"# features_one_hot = ['weekday', 'weather']\n# train_data[features_one_hot] = train_data[features_one_hot].astype(int).astype(str)\n# test_data[features_one_hot] = test_data[features_one_hot].astype(int).astype(str)","e431aebb":"# train_data = pd.get_dummies(train_data)\n# test_data = pd.get_dummies(test_data)","92317f01":"scale_features = ['temp', 'atemp', 'humidity', 'hour', 'windspeed']","e1fb3595":"# scaler = MinMaxScaler()\n# train_data[scale_features] = scaler.fit_transform(train_data[scale_features])\n# test_data[scale_features] = scaler.transform(test_data[scale_features])","987e5979":"# Features\nfeature_list = ['holiday', 'workingday', 'weather', 'temp', 'atemp',\n                'humidity', 'windspeed', 'hour_group',\n                'hour', 'weekday', 'count_mean']\nno_features = ['casual', 'registered']#, 'count']","fe068e15":"train_data['workingday'].value_counts()","73565914":"# Parameters\nlookback = 3\nhorizon = 2\nmonth = 1\nyear = 2011","9b1e32d8":"def create_data(data, lookback, horizon, start, end, kind):\n    X, y = [], []\n    \n    if kind == 'train':\n        start_shifted = start\n    else:\n        start_shifted = start - datetime.timedelta(hours=lookback)\n        \n    temp = data[start_shifted:end].copy()\n    temp.index = range(len(temp.index))\n    \n    start_ix = lookback\n    \n    n_samples = int((len(temp.index)-lookback)\/horizon)\n    \n    for i in range(n_samples):\n        end_ix = start_ix+horizon\n        seq_X = temp[(start_ix-lookback):start_ix][data.columns.difference(no_features)].values\n        seq_y = temp[start_ix:end_ix]['count'].values\n        \n        X.append(seq_X)\n        y.append(seq_y)\n        \n        start_ix = end_ix\n    \n    return np.array(X), np.array(y)","6422de46":"# Parameters\nlookback = 24\nhorizon = 1\nmonth = 12\nyear = 2012\n\n# define time range\nstart_train = datetime.datetime(year, month, 1, 0, 0, 0)\nend_train = datetime.datetime(year, month, 19, 23, 0, 0)\nstart_test = datetime.datetime(year, month, 20, 0, 0, 0)\nlast_day_of_month = calendar.monthrange(year,month)[1]\nend_test = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n\n\n# concate train and test data\ndata = pd.concat([train_data[start_train:end_train],\n                  test_data[start_test:end_test]])\n\n# scale data\nfeatures_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\ndata_scaled = data.copy()\ndata_mean = data_scaled.mean(axis=0)\ndata_scaled[data.columns.difference(features_no_scale)] -= data_mean\ndata_std = data_scaled.std(axis=0)\ndata_scaled[data.columns.difference(features_no_scale)] \/= data_std\n\n# create train and test data\nX_train, y_train = create_data(data_scaled, lookback, horizon, start_train, end_train, 'train')\nprint(X_train[0])\n# define model\nn_steps = X_train.shape[1]\nn_features = X_train.shape[2]\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(SimpleRNN(64, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(32))\nmodel.add(Dense(horizon))\n\n# define optimizer and compile model\noptimizer = Adam(lr=1e-4)\nmodel.compile(optimizer=optimizer, loss='mse', metrics = ['mse'])\n\n# fit model\nhistory = model.fit(X_train, y_train,\n                    epochs=1, verbose=1)\n\n# predict y_test\nstart_test_temp = start_test\n\nn_days = last_day_of_month-19\nfor i in range(n_days*24+1):\n    end_test_temp = start_test+datetime.timedelta(hours=i)\n    #print(start_test_temp, end_test_temp)\n    X_test, y_test = create_data(data_scaled, lookback, horizon, start_test_temp, end_test_temp, 'test')\n    if X_test.size != 0: \n        y_test = model.predict(X_test, verbose=0)\n\n        data_scaled.loc[start_test_temp:(end_test_temp-datetime.timedelta(hours=1)), 'count'] = y_test[0]\n        start_test_temp = end_test_temp\n\n# write in submission data        \n#samp_subm[start_test:end_test]['count'] = data_scaled[start_test:end_test]['count']*data_std['count']+data_mean['count']\nsamp_subm.loc[start_test:end_test, 'count'] = data_scaled.loc[samp_subm[start_test:end_test].index]['count']*data_std['count']+data_mean['count']\nsamp_subm['count'] = np.where(samp_subm['count']<0, 0, samp_subm['count'])\nsamp_subm['count'] = samp_subm['count'].interpolate()","a0b134c1":"X_train.shape, y_train.shape","5cb02ea9":"loss = history.history['loss']\nacc = history.history['mse']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'ro', label='loss_train')\nplt.plot(epochs, acc, 'b', label='accuracy_train')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","aef34a00":"plot_timeseries_train_and_predict(train_data, samp_subm, year, month)","56fd1923":"# parameters\nlookback = 24\nhorizon = 1\nn_months = 12\nyear_list = [2011, 2012]\n","98045e17":"n_samples = 0#int(19*24)-lookback\nn_features = len(train_data[train_data.columns.difference(no_features)].columns)\nprint('features used: ', train_data[train_data.columns.difference(no_features)].columns)\nX_train, y_train = np.empty(shape=(n_samples, lookback, n_features)), np.empty(shape=(n_samples, horizon))\n\nfor year in year_list:\n    for month in range(1,n_months+1):\n        #print('year: ', year, ' month: ', month)\n        start_train = datetime.datetime(year, month, 1, 0, 0, 0)\n        end_train = datetime.datetime(year, month, 19, 23, 0, 0)\n        \n        data = train_data[start_train:end_train]\n        \n        # scale data\n        features_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\n        data_scaled = data.copy()\n        data_mean = data_scaled.mean(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] -= data_mean\n        data_std = data_scaled.std(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] \/= data_std\n                \n        # create train and test data\n        X_temp, y_temp = create_data(data_scaled, lookback, horizon, start_train, end_train, 'train')\n    \n        X_train = np.vstack((X_train, X_temp))\n        y_train = np.vstack((y_train, y_temp))\n","b00e5527":"n_steps = X_train.shape[1]\nn_features = X_train.shape[2]\nmodel = Sequential()\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features), dropout=0.0, recurrent_dropout=0.2,))\nmodel.add(LSTM(64, return_sequences=True, input_shape=(n_steps, n_features), dropout=0.0, recurrent_dropout=0.2,))\n#model.add(LSTM(164, return_sequences=True, input_shape=(n_steps, n_features)))\n#model.add(LSTM(164, return_sequences=True, input_shape=(n_steps, n_features)))\nmodel.add(LSTM(32))\nmodel.add(Dense(horizon))\n\n# define optimizer and compile model\noptimizer = Adam(lr=1e-4)\nmodel.compile(optimizer=optimizer, loss='mse', metrics = ['mse'])\n\n# fit model\nhistory = model.fit(X_train, y_train,\n                    validation_split=0.1,\n                    epochs=150, verbose=1)","2cc900ec":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nepochs = range(1, len(loss)+1)\nplt.plot(epochs, loss, 'ro', label='loss_train')\nplt.plot(epochs, val_loss, 'b', label='loss_val')\nplt.title('value of the loss function')\nplt.xlabel('epochs')\nplt.ylabel('value of the loss function')\nplt.legend()\nplt.grid()\nplt.show()","7ed05859":"for year in year_list:\n    for month in range(1,n_months+1):\n        #print('year: ', year, ' month: ', month)\n        start_train = datetime.datetime(year, month, 1, 0, 0, 0)\n        end_train = datetime.datetime(year, month, 19, 23, 0, 0)\n        start_test = datetime.datetime(year, month, 20, 0, 0, 0)\n        last_day_of_month = calendar.monthrange(year,month)[1]\n        end_test = datetime.datetime(year, month, last_day_of_month, 23, 0, 0)\n\n        # concate train and test data\n        data = pd.concat([train_data[start_train:end_train],\n                          test_data[start_test:end_test]])\n        \n        # scale data\n        features_no_scale = ['season', 'holiday', 'workingday', 'count_mean']\n        data_scaled = data.copy()\n        data_mean = data_scaled.mean(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] -= data_mean\n        data_std = data_scaled.std(axis=0)\n        data_scaled[data.columns.difference(features_no_scale)] \/= data_std\n        \n        # predict y_test\n        start_test_temp = start_test\n\n        n_days = last_day_of_month-19\n        for i in range(n_days*24+1):\n            end_test_temp = start_test+datetime.timedelta(hours=i)\n        \n            X_test, y_test = create_data(data_scaled, lookback, horizon, start_test_temp, end_test_temp, 'test')\n            if X_test.size != 0: \n                y_test = model.predict(X_test, verbose=0)\n\n                data_scaled.loc[start_test_temp:(end_test_temp-datetime.timedelta(hours=1)), 'count'] = y_test[0]\n                start_test_temp = end_test_temp\n        \n        # write submission file\n        #samp_subm.loc[start_test:end_test, 'count'] = data_scaled[start_test:end_test]['count']*data_std['count']+data_mean['count']\n        samp_subm.loc[start_test:end_test, 'count'] = data_scaled.loc[samp_subm[start_test:end_test].index]['count']*data_std['count']+data_mean['count']\n        samp_subm['count'] = np.where(samp_subm['count']<0, 0, samp_subm['count'])\n        samp_subm['count'] = samp_subm['count'].interpolate()        ","b31d0718":"output = pd.DataFrame({'datetime': samp_subm.index,\n                       'count': samp_subm['count']})\noutput.to_csv('submission.csv', index=False)","49ea0cc5":"plot_timeseries_train_and_predict(train_data, samp_subm, 2012, 12)","c9c873ae":"## One Hot Encoding for categorical variables","1e91ca77":"## Count Monthly Mean","64be13d4":"# Predict All Months","6bb37b53":"# Load Libraries","2d752c24":"# Generate Output","26180cf7":"# Predict Monthly","e5886b58":"## Fit Model","16199bf4":"## Feature Season","5559a8ba":"# Scale Data","60b7cec5":"# Input path","69d71d18":"The datetime and the seasons are cyclic features. So we can use a cyclic encoding for it.","3fd794c8":"# Encoding \n## Cyclic features","69b6e604":"# Functions","b7252484":"# Load Data","441c7df6":"## Feature Weekday","a49721da":"Fill missing timestamps:","576eb957":"# Intro Bike Sharing Demand Competition\nSource: https:\/\/www.kaggle.com\/c\/bike-sharing-demand\n\nThis notebook is a starter code for all beginners and easy to understand. We will give an introduction to analysis and feature engineering.<br> \nTherefore we focus on\n* a simple analysis of the data,\n* create new features,\n* encoding and\n* scale data.\n\nWe use categorical feature encoding techniques, compare <br>\nhttps:\/\/www.kaggle.com\/drcapa\/categorical-feature-encoding-challenge-xgb <br>\n\nFor this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.","a8e7dddc":"# EDA\nYou are provided hourly rental data spanning two years. For this competition, the training set is comprised of the first 19 days of each month, while the test set is the 20th to the end of the month. You must predict the total count of bikes rented during each hour covered by the test set, using only information available prior to the rental period.","00f7609c":"# Create new features\nBased on the datetime we create new features for the month, the weekday the hour and the year. These are also cyclic features.","a625bc49":"## Feature Hour\nAdd new feature hour_group by group of hours.","b89faa82":"# Predict One Month","07e231e9":"Test Data:","cd1acb58":"## Predict Test Data","c77a2d83":"# Analyse Results","ed7c8932":"# Missing Timestamps\nThere are losts of missing hours in the train dataset. We expect $ 2 years*12 months*19 days *24 hours = 10944 timesteps$. We count 10886 timesteps so there are 58 missing. Every month in the train data set hast 456 timestamps.  "}}