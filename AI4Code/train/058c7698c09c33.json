{"cell_type":{"8f082ad6":"code","77822a4a":"code","59ba481d":"code","6ab5832d":"code","b4cbb064":"code","b6816e80":"code","e62f7886":"code","cf777298":"code","8c232767":"code","ce0d50fc":"code","d929c4ae":"code","725d46aa":"code","0b134bd9":"code","a3031cc0":"code","bbfb0269":"code","ea9ae845":"code","c2936ba2":"code","e9adca70":"code","fc719ab4":"code","9f814b69":"code","94de1338":"code","7253e0d3":"code","669cc47d":"code","043339c1":"code","d80f139a":"code","8cb2bd89":"code","8b8066bc":"code","2c82d9c1":"code","fd3da7da":"code","6121054d":"code","f6aa9d25":"code","c830798e":"code","2a236240":"code","43ac0b1c":"code","27f8db1d":"code","ca4a8686":"code","8c97cb11":"code","251fcd82":"code","8b4ce10f":"code","d9890ddf":"code","e946e606":"code","03458dc9":"code","0bd22303":"code","06ba556f":"code","a89272f2":"code","f571d4f1":"code","e6ad439f":"code","0f024c7b":"code","68007056":"code","0b2ab13c":"code","e5c1c377":"code","74dd5ed9":"code","e61d947a":"code","4e40b10d":"code","e5ada8ac":"code","44388885":"code","bee7f694":"code","e2ad7de1":"code","f8006893":"code","331786b3":"code","3064a5eb":"code","78ae548d":"code","4e6f4042":"code","8b3b6b0a":"code","b4dbcd47":"code","e6ba8ab7":"code","0124f961":"code","fb8e32c2":"code","447f69e3":"code","07a124ba":"code","c74883cd":"code","b86c1b1e":"code","70e3f367":"code","9ca0933c":"code","0bcab20e":"code","a18e154f":"code","06c8190b":"code","938f19ba":"code","514b0c2b":"code","1ed44803":"code","e98536e5":"code","6dd22224":"code","3f05a7b3":"code","37fddb6e":"code","34cdc111":"code","d9515359":"code","6d9f9392":"code","0927d322":"code","5c2208b2":"code","1bde2b9b":"code","76a7f3ca":"code","d637a6a9":"code","f8b73692":"code","6f0d7cb1":"code","e92252d6":"code","5226ffb7":"code","9be95d68":"code","4c8a3a91":"code","cd42e9ff":"code","5254e634":"code","7948c25e":"code","05ff1877":"code","e7ad30a6":"code","ee700ce4":"code","1743944d":"code","1c5750bf":"code","f840ec9d":"code","82ab5c2a":"code","411b9015":"code","109adf19":"code","6060b8b7":"code","f9437338":"code","c94acd34":"code","f6c79214":"code","6f15fa26":"markdown","c906b0eb":"markdown","c80fca10":"markdown","a26d8ed3":"markdown","850b991e":"markdown","53f54acf":"markdown","f988773e":"markdown","19970eb6":"markdown","7aaa2596":"markdown","049498df":"markdown","72a4341a":"markdown","8f0352e7":"markdown","604f14d8":"markdown","7f2082a3":"markdown","7ee3b783":"markdown"},"source":{"8f082ad6":"import numpy as np \nimport pandas as pd ","77822a4a":"import yaml\n\nPATH = \"..\/input\/mymusicalprefrences\/\" \ntrain = pd.read_csv(f\"{PATH}train.csv\")\ntest = pd.read_csv(f\"{PATH}test.csv\")\ndescription = yaml.load(open(f\"{PATH}Description.yaml\",'r'),Loader=yaml.FullLoader)\ndf = pd.concat([train,test]).reset_index(drop=True)\ntr_mask = ~df.Category.isna()","59ba481d":"train.isnull().sum()","6ab5832d":"df.describe()","b4cbb064":"df","b6816e80":"df.isnull().sum()","e62f7886":"#cleaning up and sorting data\ndf.columns = [i.strip() for i in df.columns]\n\ncat_features = {\"Artists\",\"Track\",\"Version\",\"Artists_Genres\",\"Album\",\"Album_type\",\"Labels\",\"Vocal\",\"Country\",\"Key\"}\n\ncon_features = {\"Duration\",\"Release_year\",\"BPM\",\"Energy\",\"Dancebility\",\"Happiness\"}\n\ndisplay(df[cat_features].head())\ndisplay(df[con_features].head())","cf777298":"df['Vocal'].unique()","8c232767":"import seaborn as sns\npalette = ['#c06c84',\"#6c5b7b\",\"#355c7d\"]\nsns.palplot(palette)","ce0d50fc":"df[\"Category\"] = df[\"Category\"].fillna(\"none\").replace({0:\"dislike\",1:\"like\"})\ndf[\"Category\"].head()\n\nimport plotly.graph_objects as go\n\ndef plot_commulative_onehot(onehot):\n    \"\"\"\n    Method of plotting commulative values of the one hot feature representation\n    \"\"\"\n    _df = onehot.groupby(\"Category\").sum()\n    fig = go.Figure()\n    for i in range(len(_df.index)):\n        k = _df.index[i]\n        x,y=[],[]\n        for g in _df.columns:\n            if _df.loc[k,g]!=0:\n                x.append(g)\n                y.append(_df.loc[k,g])\n        fig.add_trace(go.Bar(x=x, y=y,name=k,marker=dict(color=palette[i])))\n    fig.show()","d929c4ae":"\ndf['Vocal'] = df['Vocal'].fillna('N')\n\nonehot = np.zeros((len(df),2))\nfor i in range(len(df)):\n    x = df.iloc[i]['Vocal']\n    if x == 'F':\n        onehot[i] = [1,0]\n    elif x == 'M':\n        onehot[i] = [0,1]\n    elif x == 'N':\n        onehot[i] = [1,1]\n\ndf[[\"Fem_voc\",\"Mal_voc\"]] = onehot\n\ndf = df.drop('Vocal',axis=1)\ndf.head()","725d46aa":"description['Release year']","0b134bd9":"import plotly.express as xp\nxp.scatter(df, x=\"Release_year\", y=\"Track\",color=\"Category\", height=500, color_discrete_sequence=palette)","a3031cc0":"\ndf.loc[:,'Release_decade'] = df.loc[:,'Release_year']\/\/10 * 10\n\ndef summarize(a_number):\n    if a_number < 1990:\n        return int(1990)\n    else:\n        return int(a_number)\ndf['Release_decade'] = df['Release_decade'].map(summarize)\n\n_df = df.groupby([\"Release_decade\",\"Category\"], as_index=False).count()\nxp.bar(_df,x=\"Release_decade\", y=\"Track\",color=\"Category\",height=500, color_discrete_sequence=palette)","bbfb0269":"pd.crosstab(df[\"Category\"] ,df[\"Release_decade\"], normalize=True)","ea9ae845":"df.drop(columns =[\"Release_year\"], inplace = True)","c2936ba2":"df[['Happiness','Dancebility','Energy']].head()","e9adca70":"df['BPM'].head()","fc719ab4":"df['Key'].isnull().sum()","9f814b69":"df.head()","94de1338":"# new df with columns\nnew = df[\"Key\"].str.split(\" \", n = 1, expand = True)\n  \ndf[\"new_key\"]= new[0]\ndf[\"is_Minor\"]= new[1]\n\ndf.drop(columns =[\"Key\"], inplace = True)\n\ndf","7253e0d3":"df.new_key.unique()","669cc47d":"def key_change(key):\n    if key == 'D\u266d':\n        return 'C#'\n    elif key == 'E\u266d':\n        return 'D#'\n    elif key == 'G\u266d':\n        return 'F#'\n    elif key == 'A\u266d':\n        return 'G#'\n    elif key == 'B\u266d':\n        return 'A#'\n    else:\n        return key\n\n_df.new_key = df.new_key.map(key_change)\n_df.new_key.unique()","043339c1":"df.new_key = df.new_key.map(key_change)\ndf.new_key","d80f139a":"df = pd.get_dummies(df, columns = ['new_key'])\ndf","8cb2bd89":"df.is_Minor","8b8066bc":"def is_Minor(a_str):\n    if a_str == 'Minor':\n        return 1\n    if a_str == 'Major':\n        return 0\ndf.is_Minor = df.is_Minor.map(is_Minor)\ndf.is_Minor","2c82d9c1":"df.head()","fd3da7da":"onehot = np.zeros((len(df),7))\nfor i in range(len(df)):\n    x = df.iloc[i]['Artists_Genres'].split('|')\n    if 'pop' in x or 'ruspop'in x or 'kpop' in x:\n        onehot[i] = [1,0,0,0,0,0,0]\n    elif 'films' in x or 'soundtrack' in x or 'classicalmasterpieces' in x or 'classical' in x:\n        onehot[i] = [0,1,0,0,0,0,0]\n    elif 'rock' in x or 'rusrock' in x or 'hardrock' in x:\n        onehot[i] = [0,0,1,0,0,0,0]\n    elif 'rap' in x or 'foreignrap' in x or 'rusrap' in x:\n        onehot[i] = [0,0,0,1,0,0,0]\n    elif 'house' in x or 'prog' in x or 'trance' in x or 'dnb' in x or 'industrial' in x or 'dance' in x:\n        onehot[i] = [0,0,0,0,1,0,0]\n    elif 'soul' in x or 'jazz' in x or 'blues' in x:\n        onehot[i] = [0,0,0,0,0,1,0]\n    else:\n        onehot[i] = [0,0,0,0,0,0,1]\n\ndf[[\"sum_pop\",\"sum_films\",'sum_rock','sum_rap','sum_house','sum_soul','sum_others']] = onehot\n__df = df[['Category',\"sum_pop\",\"sum_films\",'sum_rock','sum_rap','sum_house','sum_soul','sum_others']]\ndf = df.drop('Artists_Genres',axis=1)\ndf.head()","6121054d":"plot_commulative_onehot(__df)","f6aa9d25":"df.head()","c830798e":"df = df.drop(\"Labels\", axis=1)\ndf","2a236240":"df.Version.unique()\ndf.Version = df.Version.fillna('Other')\ndf.Version.isnull().sum()","43ac0b1c":"df = pd.get_dummies(df, columns = ['Version'])\ndf","27f8db1d":"df.Album_type = df.Album_type.fillna('Other')","ca4a8686":"df = pd.get_dummies(df, columns = ['Album_type'])\ndf","8c97cb11":"__df = df[['Category','Album_type_Other','Album_type_compilation','Album_type_single']]\nplot_commulative_onehot(__df)","251fcd82":"df.Country.unique()\ndf.Country = df.Country.fillna('Other')\ndf.Country.isnull().sum()","8b4ce10f":"onehot = np.zeros((len(df),4))\nfor i in range(len(df)):\n    x = df.iloc[i]['Country'].split('|')\n    if 'GB' in x:\n        onehot[i] = [1,0,0,0]\n    elif 'USA' in x:\n        onehot[i] = [0,1,0,0]\n    elif 'RUS' in x:\n        onehot[i] = [0,0,1,0]\n    else:\n        onehot[i] = [0,0,0,1]\n","d9890ddf":"df[[\"GB\",\"USA\",'RUS','other_country']] = onehot\n__df = df[['Category',\"GB\",\"USA\",'RUS','other_country']]\ndf.head()","e946e606":"plot_commulative_onehot(__df)","03458dc9":"df = df.drop('Country',axis=1)","0bd22303":"df","06ba556f":"df.head()","a89272f2":"df.Duration.describe()","f571d4f1":"df","e6ad439f":"#df.Track.head()\ndf = df.drop('Track',axis=1)","0f024c7b":"df.Artists = df.Artists.fillna(\"NA\")\n\nall_artists = []\nfor i in df.index:\n    all_artists.extend(df.loc[i, \"Artists\"].split(\"|\"))\n\nlen(set(all_artists))","68007056":"from collections import Counter\n\nother = Counter(all_artists)","0b2ab13c":"threshold = 3\nothers = Counter(all_artists)\nothers = [k for k in others if others[k]<=threshold]\n\na_others = Counter(others)","e5c1c377":"len(others)","74dd5ed9":"in_train, in_test = [], []\nfor i in df.loc[tr_mask].index:\n    in_train.extend(df.loc[i, \"Artists\"].split(\"|\"))\nfor i in df.loc[~tr_mask].index:\n    in_test.extend(df.loc[i, \"Artists\"].split(\"|\"))\n    \nonly_test = set(in_test) - set(in_train)\nonly_train = set(in_train) - set(in_test)\ndisplay(len(only_test))\ndisplay(len(only_train))","e61d947a":"all_artists = list(set(all_artists) - set(others) - only_test - only_train)\nprint(len(all_artists))\nothers = set(others) | only_test | only_train\nprint(len(others))","4e40b10d":"res = []\ndef prune(x):\n    vector = np.zeros(len(all_artists)+1) #for others\n    x = [i for i in x.split(\"|\")]\n    for i in range(len(all_artists)):\n        vector[i]=1 if all_artists[i] in x else 0\n    if len(x)>sum(vector):\n        vector[-1]=1\n    res.append(vector)\n\ndf[\"Artists\"].apply(prune)\nonehot_artists= pd.DataFrame(res, columns = all_artists+[\"Others\"], index=df.index)","e5ada8ac":"df[\"Other_Artists\"] = onehot_artists[\"Others\"]\nonehot_artists = onehot_artists.drop(\"Others\", axis=1)\nonehot_artists[\"Category\"] = df[\"Category\"]","44388885":"from sklearn.cluster import KMeans\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import StandardScaler, normalize\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import silhouette_score\n\n__df = onehot_artists.drop('Category',axis=1)\n\npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(__df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head()","bee7f694":"import matplotlib.pyplot as plt\n\nsse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","e2ad7de1":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)","f8006893":"plt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","331786b3":"_a = pd.DataFrame(X_principal)","3064a5eb":"df['kmean_artist_1'] = X_principal['P1']\ndf['kmean_artist_2'] = X_principal['P2']","78ae548d":"df = df.drop('Artists',axis=1)","4e6f4042":"df","8b3b6b0a":"df[\"Album\"] = df[\"Album\"].fillna(\"NA\")\nall_albums = []\nfor i in df.index:\n    all_albums.extend(df.loc[i, \"Album\"].split(\"|\"))\nlen(set(all_albums))","b4dbcd47":"other = Counter(all_albums)","e6ba8ab7":"threshold = 3\nothers = Counter(all_albums)\nothers = [k for k in others if others[k]<=threshold]\nlen(others)\na_others = Counter(others)","0124f961":"in_train, in_test = [], []\nfor i in df.loc[tr_mask].index:\n    in_train.extend(df.loc[i, \"Album\"].split(\"|\"))\nfor i in df.loc[~tr_mask].index:\n    in_test.extend(df.loc[i, \"Album\"].split(\"|\"))\n    \nonly_test = set(in_test) - set(in_train)\nonly_train = set(in_train) - set(in_test)\ndisplay(len(only_test))\ndisplay(len(only_train))","fb8e32c2":"all_albums = list(set(all_albums) - set(others) - only_test - only_train)\nprint(len(all_albums))\nothers = set(others) | only_test | only_train\nprint(len(others))","447f69e3":"res = []\ndef prune(x):\n    vector = np.zeros(len(all_albums)+1) #for others\n    x = [i for i in x.split(\"|\")]\n    for i in range(len(all_albums)):\n        vector[i]=1 if all_albums[i] in x else 0\n    if len(x)>sum(vector):\n        vector[-1]=1\n    res.append(vector)\n\ndf[\"Album\"].apply(prune)\nonehot_albums= pd.DataFrame(res, columns = all_albums+[\"Others\"], index=df.index)","07a124ba":"df[\"Other_Albums\"] = onehot_albums[\"Others\"]\nonehot_albums = onehot_albums.drop(\"Others\", axis=1)\nonehot_albums[\"Category\"] = df[\"Category\"]","c74883cd":"onehot_albums","b86c1b1e":"__df = onehot_albums.drop('Category',axis=1)\n\npca = PCA(n_components = 2) \nX_principal = pca.fit_transform(__df) \nX_principal = pd.DataFrame(X_principal) \nX_principal.columns = ['P1', 'P2'] \n  \nX_principal.head()","70e3f367":"import matplotlib.pyplot as plt\n\nsse = {}\nfor k in range(1, 10):\n    kmeans = KMeans(n_clusters=k, max_iter=1000).fit(X_principal)\n    sse[k] = kmeans.inertia_ # Inertia: Sum of distances of samples to their closest cluster center\nplt.figure()\nplt.plot(list(sse.keys()), list(sse.values()))\nplt.xlabel(\"Number of cluster\")\nplt.ylabel(\"SSE\")\nplt.show()","9ca0933c":"kmeans = KMeans(n_clusters=3)\nkmeans.fit(X_principal)","0bcab20e":"plt.scatter(X_principal['P1'], X_principal['P2'],  \n           c = KMeans(n_clusters = 3).fit_predict(X_principal), cmap =plt.cm.winter) \nplt.show() ","a18e154f":"_a = pd.DataFrame(X_principal)","06c8190b":"df['kmean_album_1'] = X_principal['P1']\ndf['kmean_album_2'] = X_principal['P2']","938f19ba":"df.head()","514b0c2b":"df = df.drop('Album',axis=1)","1ed44803":"df","e98536e5":"x, y = df.loc[tr_mask].iloc[:,2:], df.loc[tr_mask,\"Category\"]\ndeploy = df.loc[~tr_mask].iloc[:,2:]","6dd22224":"def zero_one(a_string):\n    if a_string == 'like':\n        return 1\n    else:\n        return 0\ny = y.map(zero_one)","3f05a7b3":"from sklearn import preprocessing","37fddb6e":"min_max_scaler = preprocessing.MinMaxScaler()\nX_scale = min_max_scaler.fit_transform(x)","34cdc111":"from sklearn.model_selection import train_test_split","d9515359":"X_train, X_val_and_test, Y_train, Y_val_and_test = train_test_split(X_scale, y, test_size=0.3)","6d9f9392":"X_val, X_test, Y_val, Y_test = train_test_split(X_val_and_test, Y_val_and_test, test_size=0.5) ","0927d322":"from keras.models import Sequential\nfrom keras.layers import Dense","5c2208b2":"model = Sequential([\n    Dense(32, activation='relu', input_shape=(47,)),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(1, activation='sigmoid'),\n])","1bde2b9b":"from keras.utils.vis_utils import plot_model\nplot_model(model, to_file='model_plot.png', show_shapes=True, show_layer_names=True)","76a7f3ca":"model.compile(optimizer='sgd',  #sgd\u2019 refers to stochastic gradient descent (over here, it refers to mini-batch gradient descent),\n              loss='binary_crossentropy', #The loss function for outputs that take the values 1 or 0 is called binary cross entropy.\n              metrics=['accuracy'])  #we want to track accuracy on top of the loss function.","d637a6a9":"result = model.fit(X_train, Y_train,\n          batch_size=47, epochs=200, # these parameters can significantly change your accuracy.\n          validation_data=(X_val, Y_val))","f8b73692":"plt.plot(result.history['loss'])\nplt.plot(result.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","6f0d7cb1":"from keras.layers import Dropout\nfrom keras import regularizers","e92252d6":"model_3 = Sequential([\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(47,)), # lambda sign\n    Dropout(0.2),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    Dropout(0.2),\n    Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n    Dropout(0.2),\n    Dense(1, activation='sigmoid', kernel_regularizer=regularizers.l2(0.001)),\n])","5226ffb7":"model_3.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nhist_3 = model_3.fit(X_train, Y_train,\n          batch_size=5, epochs=100,\n          validation_data=(X_val, Y_val))","9be95d68":"plt.plot(hist_3.history['loss'])\nplt.plot(hist_3.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.ylim(top=1.2, bottom=0)\nplt.show()","4c8a3a91":"plt.plot(hist_3.history['accuracy'])\nplt.plot(hist_3.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","cd42e9ff":"errors = abs(np.array(hist_3.history['loss']) - np.array(hist_3.history['val_loss']))\nprint('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')","5254e634":"# mean absolute percentage error (MAPE)\nmape = 100 * (errors \/ np.array(hist_3.history['loss']))\naccuracy = 100 - np.mean(mape)\nprint('Accuracy:', round(accuracy, 2), '%.')","7948c25e":"import xgboost as xgb\n\ndeploy_scale = np.array(min_max_scaler.fit_transform(deploy))\n\ntrain = xgb.DMatrix(X_train, label=Y_train)\ntest = xgb.DMatrix(X_test, label=Y_test)\n","05ff1877":"param = {\n    'max_depth': 4,\n    'eta': 0.03,\n    'objective': 'multi:softmax',\n    'num_class': 3} \nepochs = 10","e7ad30a6":"model_5 = xgb.train(param, train, epochs)","ee700ce4":"predictions = model_5.predict(test)","1743944d":"print(predictions)","1c5750bf":"from sklearn.metrics import accuracy_score\n\naccuracy_score(Y_test, predictions)","f840ec9d":"from xgboost import XGBRegressor\n\nmy_model = XGBRegressor()\nmy_model.fit(X_train, Y_train, verbose=False)","82ab5c2a":"predictions = my_model.predict(X_test)\n\nfrom sklearn.metrics import mean_absolute_error\nprint(\"Mean Absolute Error : \" + str(mean_absolute_error(predictions, Y_test)))","411b9015":"my_model = XGBRegressor(n_estimators=1000)\nmy_model.fit(X_train, Y_train, early_stopping_rounds=5, \n             eval_set=[(X_test, Y_test)], verbose=False)","109adf19":"print(predictions)","6060b8b7":"predictions_y = [round(value) for value in predictions]\nprint(predictions_y)","f9437338":"deploy","c94acd34":"sample = pd.read_csv(f\"{PATH}sample_submition.csv\")\n\nsample[\"Category\"] = my_model.predict(deploy_scale).round().astype(int)","f6c79214":"sample.to_csv(\"deploy.csv\", index=False)","6f15fa26":"**Vocals**","c906b0eb":"**Country**","c80fca10":"**Track**","a26d8ed3":" **Artists**","850b991e":"**Labels**","53f54acf":"**Album**","f988773e":"**Key**","19970eb6":"**Release Year**","7aaa2596":"**Happiness, Energy, Dancebility, BPM**","049498df":"**Test**","72a4341a":"**Genres**","8f0352e7":"**Duration**","604f14d8":"**Version**","7f2082a3":"**Album Type**","7ee3b783":"**Model**"}}