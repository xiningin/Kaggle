{"cell_type":{"a7461bf1":"code","5822acc4":"code","6472a746":"code","7fcdc58d":"code","205981ae":"code","d59ac818":"code","e257d270":"code","f251f9d8":"code","fd520541":"code","39e31d69":"code","7de606ee":"code","20a1df0e":"code","b7592e02":"code","cefcbeb9":"code","6bfd7257":"code","8160f09f":"code","330b5f5f":"code","229e28d4":"code","8ec70c25":"code","82a98b03":"code","e40f9ef1":"code","813048be":"code","82aafb13":"code","6de6f248":"code","778ad447":"markdown","0e38f9e8":"markdown","14e4326b":"markdown","94229760":"markdown","995c7193":"markdown","85069a92":"markdown","258e8744":"markdown","859f4cb0":"markdown","08ba567a":"markdown","0adc22cb":"markdown","a210423a":"markdown","6252ace1":"markdown","47fcb293":"markdown"},"source":{"a7461bf1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport io\n\n#Collection\nfrom collections import Counter\n\n# Visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno\n\n# Machine learning \nfrom sklearn.model_selection import train_test_split\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5822acc4":"path=\"\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv\"\ndf=pd.read_csv(path)\ndf.dataframeName = 'Admission Predection'\nnRow, nCol = df.shape\nprint(f'There are {nRow} rows and {nCol} coloumns')\ndf.head()\ndf.describe()","6472a746":"#to check missing data graphically\nmissingno.matrix(df,figsize=(12,10),color=(.3,.5,.5))","7fcdc58d":"#column details\ndf.columns\n#df.drop(\"Serial No.\", axis=1, inplace=True)","205981ae":"from scipy.stats import iqr\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = iqr(df[col])\n        # outlier step\n        outlier_step = 1.5 * IQR\n\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   ","d59ac818":"# QQ Plot\nfrom statsmodels.graphics.gofplots import qqplot\nfrom matplotlib import pyplot\n\ncols = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Research']\nfor col in cols:\n  print(f\"QQ plot for {col}\")\n  qqplot(df[col], line='s')\n  pyplot.show()","e257d270":"# Shapiro-Wilk Test\nfrom scipy.stats import shapiro\n\ndef Shapiro_test(df, col):\n  # normality test\n  result = False\n  print(f\"\\nShapiro-Wilk Test for {col}\")\n  stat, p = shapiro(df[col])\n  print('Statistics=%.3f, p=%.3f' % (stat, p))\n  # interpret\n  alpha = 0.05\n  if p > alpha:\n    result = True\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n  \tprint('Sample does not look Gaussian (reject H0)')\n  return result","f251f9d8":"# D'Agostino and Pearson's Test\nfrom scipy.stats import normaltest\n\ndef DAgostino_test(df, col):\n  result = False\n  # normality test\n  print(f\"\\nD'Agostino and Pearson's Test for {col}\")\n  stat, p = normaltest(df[col])\n  # interpret\n  alpha = 0.05\n  if p > alpha:\n    result = True\n    print('Sample looks Gaussian (fail to reject H0)')\n  else:\n    print('Sample does not look Gaussian (reject H0)')\n  return result","fd520541":"# Anderson-Darling Test\nfrom scipy.stats import anderson\n\ndef AndersonDarling_test(df, cols):\n  # normality testf\n  result = False\n  for col in cols:\n    print(f\"\\nAnderson-Darling Test for {col}\")\n    result = anderson(df[col])\n    print('Statistic: %.3f' % result.statistic)\n    p = 0\n    for i in range(len(result.critical_values)):\n      sl, cv = result.significance_level[i], result.critical_values[i]\n      if result.statistic < result.critical_values[i]:\n        result = True\n        print('%.3f: %.3f, data looks normal (fail to reject H0)' % (sl, cv))\n      else:\n        print('%.3f: %.3f, data does not look normal (reject H0)' % (sl, cv)) \n  return result    ","39e31d69":"from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\nrobustscaler = RobustScaler()\nstandardscaler = StandardScaler()\nminmax = MinMaxScaler()\n\ncols = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA', 'Research']\nfor col in cols:\n  print(f\"\\n*****{col}****\")\n  temp = df[col].values.reshape(-1,1)\n  if(Shapiro_test(df, col) == True and DAgostino_test(df, col) == True):\n    print(\"Apply Standard scaler\")\n    df[col] = standardscaler.fit_transform(temp)\n  else:\n    outliers_to_drop=detect_outliers(df,2,cols)\n    #outliers rows count\n    if(len(df.loc[outliers_to_drop])>0):\n      print(\"Apply Robust scaler\")\n      df[col] = robustscaler.fit_transform(temp)\n    else:\n      print(\"Apply min max scaler\")\n      df[col] = minmax.fit_transform(temp)\n\ndf.head()","7de606ee":"#Correlation among variables\ncorr = df.corr()\nfig, ax = plt.subplots(figsize=(9, 9))\ndropSelf = np.zeros_like(corr)\ndropSelf[np.triu_indices_from(dropSelf)] = True\ncolormap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=colormap, linewidths=.5, annot=True, fmt=\".3f\", mask=dropSelf)\nplt.show()","20a1df0e":"sns.pairplot(df)","b7592e02":"df.head()","cefcbeb9":"df_data = df.iloc[:,:-1]\ndf_target = df.iloc[:,-1:]\nXtrain,Xtest,ytrain,ytest = train_test_split(df_data,df_target,test_size=0.2, random_state=10)\nxtrain = Xtrain.drop(\"Serial No.\", axis=1)\nxtest = Xtest.drop(\"Serial No.\", axis=1)","6bfd7257":"df_data.drop(\"Serial No.\", axis=1, inplace = True)\nfrom sklearn.ensemble import RandomForestRegressor\nclassifier = RandomForestRegressor()\nclassifier.fit(df_data,df_target)\nfeature_names = df_data.columns\nimportance_frame = pd.DataFrame()\nimportance_frame['Features'] = df_data.columns\nimportance_frame['Importance'] = classifier.feature_importances_\nimportance_frame = importance_frame.sort_values(by=['Importance'], ascending=True)","8160f09f":"plt.barh([1,2,3,4,5,6,7], importance_frame['Importance'], align='center', alpha=0.5)\nplt.yticks([1,2,3,4,5,6,7], importance_frame['Features'])\nplt.xlabel('Importance')\nplt.title('Feature Importances')\nplt.show()","330b5f5f":"df_data.head()","229e28d4":"df_data.drop(\"SOP\", axis = 1, inplace=True)\ndf_data.drop(\"LOR \", axis = 1, inplace=True)\ndf_data.drop(\"Research\", axis = 1, inplace=True)\ndf_data.drop(\"University Rating\", axis = 1, inplace=True)\n#df_data.drop(\"TOEFL Score\", axis=1, inplace=True)\n#df_data.drop(\"GRE Score\", axis=1, inplace=True)\n","8ec70c25":"#df_data.drop(\"TOEFL Score\", axis=1, inplace=True)\n#df_data.drop(\"GRE Score\", axis=1, inplace=True)\ndf_data.head()","82a98b03":"df_data = df.iloc[:,:-1]\ndf_target = df.iloc[:,-1:]\nXtrain,Xtest,ytrain,ytest = train_test_split(df_data,df_target,test_size=0.2, random_state=10)\nxtrain = Xtrain.drop(\"Serial No.\", axis=1)\nxtest = Xtest.drop(\"Serial No.\", axis=1)","e40f9ef1":"from sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingRegressor,GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostRegressor,AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesRegressor,ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsRegressor,KNeighborsClassifier\nfrom sklearn.svm import SVR,SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom sklearn.metrics import accuracy_score,mean_squared_error, r2_score","813048be":"regressors=[['Linear Regression :',LinearRegression()],\n       ['Decision Tree Regression :',DecisionTreeRegressor()],\n       ['Random Forest Regression :',RandomForestRegressor(n_estimators=50)],\n       ['Gradient Boosting Regression :', GradientBoostingRegressor()],\n       ['Ada Boosting Regression :',AdaBoostRegressor()],\n       ['Extra Tree Regression :', ExtraTreesRegressor()],\n       ['K-Neighbors Regression :',KNeighborsRegressor()],\n       ['Support Vector Regression :',SVR()]]\nreg_pred=[]\nR2_SCORE=[]\nMSE = []\nfor name,model in regressors:\n    model=model\n    model.fit(xtrain,ytrain)\n    predictions = model.predict(xtest)\n    rms=np.sqrt(mean_squared_error(ytest, predictions))\n    reg_pred.append(rms)\n    print(name,rms)\n    # The mean squared error\n    MSE.append(mean_squared_error(ytest, predictions))\n    print(\"Mean squared error: %.8f\"% mean_squared_error(ytest, predictions))\n    R2_SCORE.append(r2_score(ytest, predictions))\n    print('R2 score: %.8f' % r2_score(ytest, predictions))  \n    print(\"*\"*50)  ","82aafb13":"df_model=pd.DataFrame({'Modelling Algorithm':regressors,'R2_score':R2_SCORE,\"MSE\":MSE})\ndf_model=df_model.sort_values(by=\"R2_score\",ascending=False).reset_index()\nprint(df_model)","6de6f248":"df_model.head(4)","778ad447":"From the above pairplot its clearly visible that GRE score TOEFL score and CGPA all are linearly related to each other Also it can be inferred that Research Students tend to Score higher by all means\n\nLets split the dataset with training and testing set and prepare the inputs and outputs","0e38f9e8":"Finding Correlation among variables:","14e4326b":"From the above test, we conclude applying Non-Paramaetric statistics, except for CGPA column which follows gaussian distribution. And also since we don't have any outliners, we can use MinMaxScaler for all the columns except CGPA, for CGPA we will consider StandardScaler\n\nLet's standardize our data","94229760":"3. Anderson-Darling Test\n\nAnderson-Darling Test is a statistical test that can be used to evaluate whether a data sample comes from one of among many known data samples, named for Theodore Anderson and Donald Darling.\n\nIt can be used to check whether a data sample is normal. The test is a modified version of a more sophisticated nonparametric goodness-of-fit statistical test called the Kolmogorov-Smirnov test.\n\nA feature of the Anderson-Darling test is that it returns a list of critical values rather than a single p-value. This can provide the basis for a more thorough interpretation of the result.\n\nThe anderson() SciPy function implements the Anderson-Darling test. It takes as parameters the data sample and the name of the distribution to test it against. By default, the test will check against the Gaussian distribution (dist=\u2019norm\u2019).","995c7193":"Lets take a look at the outlier values and remove them\n\nSince outliers can have a dramatic effect on the prediction (especially for regression problems), Its better to remove them to get better results.\n\nI used the Tukey method (Tukey JW., 1977) to detect ouliers which defines an interquartile range comprised between the 1st and 3rd quartile of the distribution values (IQR). An outlier is a row that have a feature value outside the (IQR +- an outlier step).\n\nI decided to detect outliers from the numerical values features (GRE Score, TOEFL Score, University Rating, SOP, LOR , CGPA, Research). Then, i considered outliers as rows that have at least two outlied numerical values","85069a92":"Visualize Feature Importances","258e8744":"**The dataset contains several parameters which are considered important during the application for Masters Programs. The parameters included are :**\n\nGRE Scores ( out of 340 )\nTOEFL Scores ( out of 120 )\nUniversity Rating ( out of 5 )\nStatement of Purpose and Letter of Recommendation Strength ( out of 5 )\nUndergraduate GPA ( out of 10 )\nResearch Experience ( either 0 or 1 )\nChance of Admit ( ranging from 0 to 1 )","859f4cb0":"2. D\u2019Agostino\u2019s K^2 Test\n\nThe D\u2019Agostino\u2019s K^2 test calculates summary statistics from the data, namely kurtosis and skewness, to determine if the data distribution departs from the normal distribution, named for Ralph D\u2019Agostino.\n\nSkew is a quantification of how much a distribution is pushed left or right, a measure of asymmetry in the distribution. Kurtosis quantifies how much of the distribution is in the tail. It is a simple and commonly used statistical test for normality. The D\u2019Agostino\u2019s K^2 test is available via the normaltest() SciPy function and returns the test statistic and the p-value.","08ba567a":"Normality check\n1. Shapiro-Wilk Test\n\nThe Shapiro-Wilk test evaluates a data sample and quantifies how likely it is that the data was drawn from a Gaussian distribution, named for Samuel Shapiro and Martin Wilk.\n\nIn practice, the Shapiro-Wilk test is believed to be a reliable test of normality, although there is some suggestion that the test may be suitable for smaller samples of data, e.g. thousands of observations or fewer.\n\nThe shapiro() SciPy function will calculate the Shapiro-Wilk on a given dataset. The function returns both the W-statistic calculated by the test and the p-value.","0adc22cb":"From above output we infer ,there are no outliers because all the values are inside a fixed range and none of them go lower\/beyond that range which therefore produces no outliers\n\nNow we will perform scaling of the data column by column using below approches\n\n1. Quantile-Quantile Plot\n\nAnother popular plot for checking the distribution of a data sample is the quantile-quantile plot, Q-Q plot, or QQ plot for short.\n\nThis plot generates its own sample of the idealized distribution that we are comparing with, in this case the Gaussian distribution. The idealized samples are divided into groups (e.g. 5), called quantiles. Each data point in the sample is paired with a similar member from the idealized distribution at the same cumulative distribution.\n\nThe resulting points are plotted as a scatter plot with the idealized value on the x-axis and the data sample on the y-axis.\n\nA perfect match for the distribution will be shown by a line of dots on a 45-degree angle from the bottom left of the plot to the top right. Often a line is drawn on the plot to help make this expectation clear. Deviations by the dots from the line shows a deviation from the expected distribution.\n\nWe can develop a QQ plot in Python using the qqplot() statsmodels function. The function takes the data sample and by default assumes we are comparing it to a Gaussian distribution. We can draw the standardized line by setting the \u2018line\u2018 argument to \u2018s\u2018.","a210423a":"Considering only important fetaures (CGPA, GRE Score, TOFEL Score) for the model input","6252ace1":"Here we can see that the chance of admit is highly correlated with CGPA, GRE and TOEFEL scores are highly correlated.","47fcb293":"Lets find out feature imporatance"}}