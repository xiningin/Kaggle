{"cell_type":{"29887b47":"code","551e8491":"code","f949d44f":"code","9cc496d8":"code","50acf02e":"code","33223478":"code","9d6d6380":"code","14706483":"code","0818e46a":"code","1dc67327":"code","66a44040":"code","4c71dcd9":"code","1167a896":"code","e0d95ef5":"code","2cd84be6":"code","6c4379bf":"code","307dc87b":"code","53334fa2":"code","acbdc2c9":"code","657d7cba":"code","ad109b43":"code","b1e81176":"code","b718b49b":"code","4f4f4aa2":"code","080bbd8f":"markdown","7e9b2568":"markdown","9f5abdd3":"markdown","fdc86950":"markdown","aa408f39":"markdown","5dcb81b1":"markdown","82de3625":"markdown","791ca31f":"markdown"},"source":{"29887b47":"import pandas as pd\nimport numpy as np","551e8491":"PATH = '..\/input\/google-quest-challenge\/'","f949d44f":"train = pd.read_csv(PATH+'train.csv')\ntest = pd.read_csv(PATH+'test.csv')","9cc496d8":"train.head()","50acf02e":"train_x = train.loc[:, 'qa_id':'host']","33223478":"train_x.head()","9d6d6380":"train_y = train.loc[:, 'question_asker_intent_understanding':'answer_well_written']","14706483":"train_y.head()","0818e46a":"train_x = train_x[['question_title','question_body','answer']]\ntest_x = test[['question_title','question_body','answer']]","1dc67327":"train_x.head()","66a44040":"test_x.head()","4c71dcd9":"import tensorflow_hub as hub\nimport tensorflow as tf\nimport bert_tokenization as tokenization\nfrom tensorflow.keras.models import Model      \nimport tensorflow.keras.backend as K\nimport math","1167a896":"MAX_SEQ_LENGTH = 512\nBERT_PATH = '..\/input\/bert-base-from-tfhub\/bert_en_uncased_L-12_H-768_A-12'","e0d95ef5":"def create_model(max_seq_length=MAX_SEQ_LENGTH,bert_path=BERT_PATH):\n    # BERT needs 3 inputs: ids, masks, segments     \n    input_word_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                           name=\"input_word_ids\")\n    input_mask = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                       name=\"input_mask\")\n    segment_ids = tf.keras.layers.Input(shape=(max_seq_length,), dtype=tf.int32,\n                                        name=\"segment_ids\")\n    # pretrained BERT_base     \n    bert_layer = hub.KerasLayer(bert_path,\n                                trainable=True)\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    # Output layer for 30 classes to predict     \n    pooling = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    dropout = tf.keras.layers.Dropout(0.2)(pooling)\n    out = tf.keras.layers.Dense(30, activation=\"sigmoid\", name=\"dense_output\")(dropout)\n\n    return Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)","2cd84be6":"def get_masks(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef get_segments(tokens, max_seq_length):\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    current_segment_id = 0\n    first_sep = True\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n            \n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef get_ids(tokens, tokenizer, max_seq_length):\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef trim_input(title, question, answer, max_sequence_length, \n                t_max_len=30, q_max_len=239, a_max_len=239):\n    \n    t_len,q_len,a_len = len(title),len(question),len(answer)\n\n    if (t_len+q_len+a_len+4) > max_sequence_length:\n        \n        if t_max_len <= t_len:\n            t_new_len = t_max_len\n        else:\n            t_new_len = t_len\n            a_max_len = a_max_len + math.floor((t_max_len - t_len)\/2)\n            q_max_len = q_max_len + math.ceil((t_max_len - t_len)\/2)            \n      \n        if a_max_len > a_len:\n            a_new_len = a_len \n            q_new_len = q_max_len + (a_max_len - a_len)\n        elif q_max_len > q_len:\n            a_new_len = a_max_len + (q_max_len - q_len)\n            q_new_len = q_len\n        else:\n            a_new_len = a_max_len\n            q_new_len = q_max_len            \n            \n        if t_new_len+a_new_len+q_new_len+4 != max_sequence_length:\n            raise ValueError(\"New sequence length should be %d, but is %d\" \n                             % (max_sequence_length, (t_new_len+a_new_len+q_new_len+4)))\n            \n        title,question,answer = title[:t_new_len], question[:q_new_len], answer[:a_new_len]\n    \n    return title,question,answer\n\ndef get_inputs(title, question, answer, tokenizer,max_seq_length):\n    t = tokenizer.tokenize(title)\n    q = tokenizer.tokenize(question)\n    a = tokenizer.tokenize(answer)\n    t,q,a = trim_input(t, q, a,max_seq_length)\n    stokens = [\"[CLS]\"] + t + [\"[SEP]\"] + q + [\"[SEP]\"] + a + [\"[SEP]\"]\n\n    input_ids = get_ids(stokens, tokenizer, max_seq_length)\n    input_masks = get_masks(stokens, max_seq_length)\n    input_segments = get_segments(stokens, max_seq_length)\n    return input_ids,input_masks,input_segments\n\ndef compute_input_arays(df, tokenizer, max_sequence_length):\n    input_ids, input_masks, input_segments = [], [], []\n    for _, instance in df.iterrows():\n        t, q, a = instance.question_title, instance.question_body, instance.answer\n\n        ids, masks, segments = get_inputs(t, q, a, tokenizer, max_sequence_length)\n        input_ids.append(ids)\n        input_masks.append(masks)\n        input_segments.append(segments)\n        \n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]","6c4379bf":"from scipy.stats import spearmanr","307dc87b":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )","53334fa2":"def train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback","acbdc2c9":"from sklearn.model_selection import GroupKFold","657d7cba":"gkf = GroupKFold(n_splits=5).split(X=train_x.question_body, groups=train_x.question_body)","ad109b43":"tokenizer = tokenization.FullTokenizer(BERT_PATH + '\/assets\/vocab.txt')","b1e81176":"inputs = compute_input_arays(train_x, tokenizer,MAX_SEQ_LENGTH)\ntest_inputs = compute_input_arays(test_x, tokenizer,MAX_SEQ_LENGTH)\noutputs = np.asarray(train_y)","b718b49b":"histories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < 3:\n        K.clear_session()\n        model = create_model()\n\n        train_inputs = [inputs[i][train_idx] for i in range(3)]\n        train_outputs = outputs[train_idx]\n\n        valid_inputs = [inputs[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs[valid_idx]\n\n        # history contains two lists of valid and test preds respectively:\n        #  [valid_predictions_{fold}, test_predictions_{fold}]\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs, \n                          learning_rate=3e-5, epochs=4, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n\n        histories.append(history)","4f4f4aa2":"test_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub = pd.read_csv(PATH + 'sample_submission.csv')\n\ndf_sub.iloc[:, 1:] = test_predictions\n\ndf_sub.to_csv('submission.csv', index=False)","080bbd8f":"### Bert model","7e9b2568":"### KFold","9f5abdd3":"### Training","fdc86950":"## Time for BERT","aa408f39":"### Data preprocessing functions","5dcb81b1":"### Exclude useless columns","82de3625":"### Callback for spearman calculation","791ca31f":"## EDA"}}