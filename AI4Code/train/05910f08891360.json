{"cell_type":{"743513d3":"code","7b6dbd44":"code","23691c63":"code","e8c5ece7":"code","41424bc6":"code","0ef9d363":"code","da3febc1":"code","2fe7a7af":"code","e48fe95d":"code","c27be7ba":"code","4737b02f":"code","e21c5048":"code","41ddfd0c":"code","70c035a9":"code","c84faad8":"code","fef6f144":"code","eacf781b":"code","b5dd63fe":"code","29bce6fa":"code","f2f50c3e":"code","928a8766":"code","a6a027e5":"code","c097d408":"markdown","6c0216da":"markdown","d808fa28":"markdown","63ec0f55":"markdown","3c5a5e3b":"markdown","c1e3e9f1":"markdown","bfcf1220":"markdown","55156685":"markdown","3362b8a5":"markdown"},"source":{"743513d3":"import numpy as np\nimport pandas as pd\nimport optuna\nimport os\nfrom sklearn import preprocessing\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ndata = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","7b6dbd44":"data.head(10)","23691c63":"data.info()","e8c5ece7":"data.describe()","41424bc6":"data.diagnosis.value_counts()","0ef9d363":"label_enc = preprocessing.LabelEncoder()\ndata.diagnosis = label_enc.fit_transform(data.diagnosis)\nlabels = data.diagnosis\ntrain = data.drop(['diagnosis', 'Unnamed: 32', \"id\"], axis = 1)","da3febc1":"corr = data.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(2000, 13, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, center=0,square=True, linewidths=.5)","2fe7a7af":"data.columns","e48fe95d":"sns.pairplot(data[['diagnosis', 'radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean']], hue = 'diagnosis')","c27be7ba":"fig = plt.figure(figsize = (20,10))\nax = fig.gca()\nsns.boxplot(data= train[['area_mean', 'area_worst']], orient=\"h\", palette=\"Set1\", ax = ax)","4737b02f":"train.boxplot()","e21c5048":"from scipy import stats\n\nrows = np.any(stats.zscore(train.values) > 2.5, axis=1)\noutliers = train.loc[rows]\noutliers.shape","41ddfd0c":"# load the iris datasets\n# fit an Extra Trees model to the data\n\nx_train, x_test, y_train, y_test = train_test_split(train.values, labels.values, test_size=0.2, random_state=42 )\nclf = ExtraTreesClassifier()\nclf.fit(x_train,y_train)\n# display the relative importance of each attribute\nz = clf.feature_importances_\n#make a dataframe to display every value and its column name\ndf = pd.DataFrame()\n\ndf[\"values\"] = z\ndf['column'] = list(train.columns.values)\n# Sort then descendingly to get the worst features at the end\ndf.sort_values(by='values', ascending=False, inplace = True)\ndf.head(100)","70c035a9":"x_train, x_test, y_train, y_test = train_test_split(train.values, labels.values, test_size=0.2, random_state=42 )","c84faad8":"clf = XGBClassifier(random_state=0)\n\nclf.fit(x_train, y_train)\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))","fef6f144":"clf_et = ExtraTreesClassifier(n_estimators=950, random_state=0)\n\nclf_et.fit(x_train, y_train)\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf_et.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf_et.score(x_test, y_test) * 100))","eacf781b":"def objective(trial,data=train.values,target=labels.values):\n    \n    train_x, test_x, train_y, test_y = train_test_split(data, target, test_size=0.15,random_state=42)\n    \n    param = {\n        'tree_method':'gpu_hist',  # this parameter means using the GPU when training our model to speedup the training process\n        'lambda': trial.suggest_loguniform('lambda', 1e-3, 10.0),\n        'alpha': trial.suggest_loguniform('alpha', 1e-3, 10.0),\n        'colsample_bytree': trial.suggest_categorical('colsample_bytree', [0.3,0.4,0.5,0.6,0.7,0.8,0.9, 1.0]),\n        'subsample': trial.suggest_categorical('subsample', [0.4,0.5,0.6,0.7,0.8,1.0]),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.008,0.009,0.01,0.012,0.014,0.016,0.018, 0.02]),\n        'n_estimators': trial.suggest_int('n_estimators', 100, 4000, 100),\n        'max_depth': trial.suggest_categorical('max_depth', [5,7,9,11,13,15,17,20]),\n        'random_state': trial.suggest_categorical('random_state', [24, 48,2020]),\n    }\n    \n    model = XGBClassifier(**param)  \n    model.fit(train_x,train_y,eval_set=[(test_x,test_y)],early_stopping_rounds=100,verbose=False)\n    preds = model.predict(test_x)\n    acc = accuracy_score(test_y, preds)\n    return acc\n\n\nstudy = optuna.create_study(direction='maximize')\nstudy.optimize(objective, n_trials=50)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)","b5dd63fe":"param = {'lambda': 7.726177577712451, \n         'alpha': 0.020954967406242572, \n         'colsample_bytree': 0.8, \n         'subsample': 0.4, \n         'learning_rate': 0.018, \n         'n_estimators': 1000, \n         'max_depth': 9, \n         'random_state': 48, \n         'min_child_weight': 2}","29bce6fa":"clf = XGBClassifier(**param)\n\nclf.fit(x_train, y_train)\n\nprint('Accuracy of classifier on training set: {:.2f}'.format(clf.score(x_train, y_train) * 100))\nprint('Accuracy of classifier on test set: {:.2f}'.format(clf.score(x_test, y_test) * 100))","f2f50c3e":"from sklearn.metrics import confusion_matrix","928a8766":"from sklearn.utils.multiclass import unique_labels\nfrom warnings import simplefilter\nfrom collections import defaultdict","a6a027e5":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=False,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    classes = [\"Sp\" + str(x) for x in unique_labels(y_true, y_pred)]\n\n    fig, ax = plt.subplots()\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Loop over data dimensions and create text annotations.\n    fmt = 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax\n\n\nnp.set_printoptions(precision=2)\nplot_confusion_matrix(y_test, clf.predict(x_test), classes=np.unique(y_train), normalize=False,\n                      title='Normalized confusion matrix')\n\nplt.show()","c097d408":"<a id=\"section-two\"><\/a>\n\n### Feature Selection\nFeature selection is one of the essential steps in the Machine Learning pipeline. It can be very helpful to solve the 'Curse of dimentionality' if the number of features is huge.\nThere are many feature selection techniques including:\n\n* model based feature selection which requires training a secondary Machine Learning model,\n* Statistical based Feature selection which relies on statistical methods (eg: Hypothesis testing, Correlations)","6c0216da":"#### categorical Variables:\n Variables that consist of a set of dicrete finite categories or labels, it is divided into two main types:\n * Nominal: there is no natural priority or order for the different categories (You can use dummies function) <br>https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.get_dummies.html\n * Ordinal: Can be ordered or ranked. We can use Ordinal Encoder or LabelEncoder from Sklearn <br>\n https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html\n https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html\n \n\nLet's now check the number of unique labels\/categories in the 'diagnosis' column using the value_counts() method from pandas.DataFrame()","d808fa28":"#### Hyperparameters Tuning\n\nThere are many Machine Learning Models that can be used with this type of data. It is usually the case that when some Machine Learning model will give a good result on simillar dataset, for this kind of tabular data, tree-based ensembling methods usually gives the best results.","63ec0f55":"# GUC Brain ML workshop\n\nby AbdElRhman ElMoghazy\n\n\nIn this Tutorial we will learn how to classify the breast cancer dataset using Ensemble learning techniques. This tutorial is part of  GUC Brain ML workshop and will cover the following:\n\n* [Data Exploration and Analysis](#section-one)\n* [Feature Selection](#section-two)\n* [Model choice and hyperparameters optmization](#section-three)\n* [Model Evaluation](#section-seven)","3c5a5e3b":"<a id=\"section-seven\"><\/a>","c1e3e9f1":"The column consists of two categories only, B and M. Let's explore further if we can order those or just one-hot encode them.\n\n#### Note:\nOne hot encoding a feature adds new features for each unique category, so if you have only two catogries \"B\" and \"M\" in diagnosis feature, you will have two new columns B and M where B feature will have 1s in the places diagnosis = \"B\" and M feature will have 1's in the places diagnosis = \"M\"\n\n#### Example\n\nOne-hot encoding:\n\ndiagnosis &nbsp;&nbsp;&nbsp; B | M <br>\nB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nM &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nM &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\nB &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;         1 | 0 <br>\n\nLabel encoding: if B is ranked lower than M: <br>\ndiagnosis  &nbsp;    diagnosis_new <br>\nB  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nM  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nM  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            2 <br>\nB  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\nB  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;            1 <br>\n\n\nBut how can we really know if the data is ranked or not? This can be done using dommain knowledge of the data, for example most of the features are described in the main dataset page, You can also determine this in real life problems using your own knowledge of the problem and the data collected.\nHere in this data it is obvious that B means Begnign and M means Melignant, also note that diagnosis is the target variable so I will go for label encoding the variable to get one output for each row.","bfcf1220":"<a id=\"section-three\"><\/a>\n\n### Model Selection\n\nLet's now try our first model before diving into further Feature Engineering.\n\n#### Ensembling\nAn ensamble of weak classifiers usually gives better results than strong individual classifiers. Ensemble Learning consists of three main types:\n* Bagging\n* Boosting\n* Stacking\n\nIn this workshop we will try bagging and boosting ensembles and leave the third type for another workshop\n\n\nFirst we need to split the data into training and testing to be able to evaluate the model later. We will use from train_test_split from sklearn.model_selection","55156685":"<a id=\"section-one\"><\/a>\n\n### Exploratoratory Data Analysis\nwe will start by exploring the data statistically and visually. We will explore the distribution of data using describe() function from pd.DataFrame and will check for anomalies or possible outliers.\nUsing info() function from pd.DataFrame, we can get an idea about the data type of each feature and whether it contains Null or NaN values as well.\nWe will then learn how to deal with categorical data (although we don't have any feature except the target in the categorical form). Then we will explore correlations between features and also other methods for outliers detection including the boxplot -IQR method- and also the z-score way.","3362b8a5":"#### Outliers Detection\n\nThere are many methods that can be used to detect outliers in a dataset. In this workshop we will discuss the following:\n* Box Plot method\n* Standarization (Z-sore) method\n\n##### Box Plot :: Consists of five main components:\n* Q1, first quartile (Midean of the first half of the data)\n* Q2, Midean of the data\n* Q3, midean of the second half of the data\n* Max value\n* Min value\n\n##### Main equations in box plots:\n$$ IQR = Q3 - Q1 $$\n$$ Outliers = Q3 + 1.5 * IQR$$\n$$ Q1 - 1.5 * IQR $$\n\n##### Z-score method\nZ-score represents the number of standard deviations removed from the mean for each data point. In a simpler way, it is the distance for a point from the mean in standard deviations.\n$$ z-score = {x - mean \\over std} $$"}}