{"cell_type":{"06e8519c":"code","dc014402":"code","feda4821":"code","c2a6f6af":"code","483bb209":"code","2e224bc4":"code","3d7f243b":"code","3860c858":"code","32f5c0a7":"code","c6b88d5f":"code","36a90413":"code","be133e58":"code","a37a25b6":"code","28be3e93":"code","0882c72f":"code","e7c6d465":"code","3a050f71":"code","9aa89c46":"code","8f0b679c":"code","dfc24dc4":"code","992cc577":"code","953a4828":"code","4cfa0024":"code","e80917c0":"code","0be99564":"code","15d46577":"code","fa6e93b9":"code","a4760816":"code","480cccf3":"code","83567127":"code","7ca0d74a":"code","b3d81911":"code","ab9e47df":"code","bac6c9f1":"code","78e82e68":"code","f8b484f7":"code","b34c6213":"code","7e85e22b":"code","a78c7d2a":"code","6f679401":"code","2a197100":"code","75f3bc95":"code","6a373577":"code","0e6901de":"code","8c2e8a36":"code","171fecac":"code","a8f3d289":"code","6468b800":"code","cb3a889e":"code","eafecd2a":"code","0c0831cb":"code","14ddcb27":"markdown","820bf3f9":"markdown","e1453df0":"markdown","9f2b2009":"markdown","eb96a9f5":"markdown","f0a1bb5e":"markdown","0cd3fa54":"markdown","53eeea0d":"markdown","af53e1cc":"markdown","c423bff6":"markdown","ab45ceed":"markdown","d45cd541":"markdown","487bf19b":"markdown","71add041":"markdown","468f66f1":"markdown","c7d2d772":"markdown","856f0df1":"markdown","d31d2f15":"markdown","68293f36":"markdown"},"source":{"06e8519c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","dc014402":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport random","feda4821":"def unpickle(file):\n    import pickle\n    with open(file, 'rb') as fo:\n        dict = pickle.load(fo, encoding='bytes')\n    return dict","c2a6f6af":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","483bb209":"qmnist = unpickle('\/kaggle\/input\/qmnist-the-extended-mnist-dataset-120k-images\/MNIST-120k')\n\ndata = qmnist['data']\nlabels = qmnist['labels']","2e224bc4":"print(data.shape)\nprint(labels.shape)","3d7f243b":"data.dtype","3860c858":"X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.33, random_state=29)","32f5c0a7":"X_train.shape","c6b88d5f":"plt.imshow(X_train[627,:,:], cmap='Greys')","36a90413":"plt.imshow(X_train[7480,:,:], cmap='Greys')","be133e58":"X_train = X_train.reshape(-1, 28*28)\nX_test = X_test.reshape(-1, 28*28)","a37a25b6":"from sklearn.linear_model import SGDClassifier","28be3e93":"sgd_clf = SGDClassifier(random_state=29)","0882c72f":"sgd_clf.fit(X_train, y_train.ravel())","e7c6d465":"y_pred_sgd = sgd_clf.predict(X_test)","3a050f71":"accuracy_score(y_test, y_pred_sgd)","9aa89c46":"sgd_acc = accuracy_score(y_test, y_pred_sgd)","8f0b679c":"r = list(range(1000))\nrandom.shuffle(r)\n\nplt.figure(figsize=(10, 10))\n\nfor i, j in enumerate(r[:25]):\n    plt.subplot(5,5,i+1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.grid(False)\n    plt.imshow(X_test[j].reshape(28, 28), cmap='binary')\n    plt.xlabel(y_pred_sgd[j])\n\nplt.suptitle('SGD Classifier ' + str(round(sgd_acc, 3)), size=35)\nplt.show()","dfc24dc4":"from sklearn.preprocessing import StandardScaler","992cc577":"scaler = StandardScaler()","953a4828":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.fit_transform(X_test)","4cfa0024":"sgd_1 = SGDClassifier(loss='modified_huber', average=True, penalty='elasticnet', n_jobs=-1,\n                     learning_rate='optimal', alpha=0.0000001, power_t=0.8, max_iter=5000)","e80917c0":"sgd_1.fit(X_train_scaled, y_train.ravel())","0be99564":"y_pred_sgd_1 = sgd_1.predict(X_test_scaled)","15d46577":"accuracy_score(y_test, y_pred_sgd_1)","fa6e93b9":"from sklearn.decomposition import PCA","a4760816":"pca = PCA(n_components=3)","480cccf3":"pca.fit(X_train)","83567127":"X_train_pca = pca.fit_transform(X_train)\n","7ca0d74a":"X_test_pca = pca.fit_transform(X_test)","b3d81911":"sgd_pca = SGDClassifier(random_state=29)","ab9e47df":"sgd_pca.fit(X_train_pca, y_train.ravel())","bac6c9f1":"y_pred_pca = sgd_pca.predict(X_test_pca)","78e82e68":"accuracy_score(y_test, y_pred_pca)","f8b484f7":"pca.explained_variance_ratio_","b34c6213":"pca1 = PCA(n_components=0.95, svd_solver='full')","7e85e22b":"pca1.fit(X_train)","a78c7d2a":"X_train_pca1 = pca1.transform(X_train)\nX_test_pca1 = pca1.transform(X_test)","6f679401":"len(pca1.explained_variance_ratio_)","2a197100":"sgd_pca1 = SGDClassifier(loss='modified_huber', average=True, penalty='elasticnet', n_jobs=-1,\n                     learning_rate='optimal', alpha=0.0000001, power_t=0.8)","75f3bc95":"sgd_pca1.fit(X_train_pca1, y_train.ravel())","6a373577":"y_pred_pca1 = sgd_pca1.predict(X_test_pca1)","0e6901de":"accuracy_score(y_test, y_pred_pca1)","8c2e8a36":"pca2 = PCA(n_components=0.95, svd_solver='full')","171fecac":"pca2.fit(X_train_scaled)","a8f3d289":"X_train_pca2 = pca2.transform(X_train_scaled)\nX_test_pca2 = pca2.transform(X_test_scaled)","6468b800":"sgd_pca2 = SGDClassifier(loss='modified_huber', average=True, penalty='elasticnet', n_jobs=-1,\n                     learning_rate='optimal', alpha=0.0000001, power_t=0.8)","cb3a889e":"sgd_pca2.fit(X_train_pca2, y_train.ravel())","eafecd2a":"y_pred_pca2 = sgd_pca2.predict(X_test_pca2)","0c0831cb":"accuracy_score(y_test, y_pred_pca2)","14ddcb27":"#### The result is pretty good but it feels like it can still be improved","820bf3f9":"#### We can see that the number of dimensions that will contain 95% of variance in the principal component is 154","e1453df0":"#### Lets try the SGDClassifier. From the documentation, SGDClassifier is a linear classifier that implements Stochastic Gradient Descent training. By default, it fits a linear SVM (Support Vector Machine)","9f2b2009":"#### Check the data path","eb96a9f5":"#### Import the important libraries","f0a1bb5e":"#### The score is increasing! Now lets try to scale the data before finding the principal component","0cd3fa54":"#### Lets try reducing the dimensionality using Principal Component Analysis (PCA)","53eeea0d":"#### For starters, try reducing the dimension from originally 784 to only 3 and see how well it performs using SGD Classifier","af53e1cc":"#### Check the shape of the X data","c423bff6":"#### It turns out the accuracy score is much lower","ab45ceed":"#### We can see with only 3 dimensions, there are no significant variance conserved in the dataset anymore. We need to reduce dimensionality to find the principal component that has at least 95% of variance","d45cd541":"#### Check on the data","487bf19b":"#### The predictions on SGDClassifier model also have misclassifications here and there.","71add041":"#### Create the unpickle function, as provided by the source","468f66f1":"#### Unpickle the data","c7d2d772":"#### Separate the data into training and testing X and y data. Along this notebook the random state will be set to 29","856f0df1":"#### Since the X data is in 2-D, they need to be reshaped into 1-D array","d31d2f15":"#### The score is now really good! With using PCA, the runtime is much faster since the dimension of the data is reduced, in this case, from 784 to 154. We can also see that the accuracy score is not very different than the one without the PCA","68293f36":"### This is an image classifier notebook using the QMNIST data as a showcase for my basic machine learning understanding. Several linear models are being used but will dwelve more in trying to enhance its accuracy score.\n\n### This is my first kaggle notebook so please lower the expectations. However, feedbacks and any constructive criticisms are very much appreciated"}}