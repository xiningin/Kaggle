{"cell_type":{"07d9a928":"code","8d464e70":"code","60f1762b":"code","75865a96":"code","7b1003fc":"code","f0821902":"code","9bb17197":"code","37a55bf3":"code","d0a30189":"code","e7d3e506":"code","202a95a9":"code","3a810d5c":"code","a1946e64":"code","df243953":"code","1e19d16a":"code","42c47cf5":"code","14d925f2":"code","2540ba90":"code","6b68ba71":"code","bfc4fee1":"code","5d3e5bee":"code","3cc23b9c":"code","531f7f93":"code","86a21c16":"code","219fa48f":"code","60bba9f8":"code","b4772e78":"code","3b4a8719":"code","f8172d43":"code","349d4164":"code","e28b01df":"code","270e9e01":"code","8b517ff0":"code","a942284d":"code","064a1636":"code","a43352d4":"markdown","7896cc71":"markdown","4b7c92de":"markdown","f4e48c56":"markdown","da7a489a":"markdown","3d6210be":"markdown","6a1190f7":"markdown","7ca6748d":"markdown","5122966d":"markdown"},"source":{"07d9a928":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8d464e70":"import warnings\nwarnings.filterwarnings('ignore')\n%config Completer.use_jedi = False # if autocompletion doesnot work in kaggle notebook | hit tab","60f1762b":"train_df = pd.read_csv('..\/input\/nlp-getting-started\/train.csv',usecols=['id','text','target'])\ntest_df = pd.read_csv('..\/input\/nlp-getting-started\/test.csv',usecols=['id','text'])\nsample_df = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","75865a96":"test_df.head()","7b1003fc":"train_df.head()","f0821902":"train_df.shape","9bb17197":"!pip install text_hammer ","37a55bf3":"import text_hammer as th","d0a30189":"%%time\n\nfrom tqdm._tqdm_notebook import tqdm_notebook\ntqdm_notebook.pandas()\n\ndef text_preprocessing(df,col_name):\n    column = col_name\n    df[column] = df[column].progress_apply(lambda x:str(x).lower())\n#     df[column] = df[column].progress_apply(lambda x: th.cont_exp(x)) #you're -> you are; i'm -> i am\n    df[column] = df[column].progress_apply(lambda x: th.remove_emails(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_html_tags(x))\n#     df[column] = df[column].progress_apply(lambda x: ps.remove_stopwords(x))\n\n    df[column] = df[column].progress_apply(lambda x: th.remove_special_chars(x))\n    df[column] = df[column].progress_apply(lambda x: th.remove_accented_chars(x))\n#     df[column] = df[column].progress_apply(lambda x: th.make_base(x)) #ran -> run,\n    return(df)","e7d3e506":"train_cleaned_df = text_preprocessing(train_df,'text')\n","202a95a9":"train_cleaned_df[train_cleaned_df.target == 0]","3a810d5c":"train_df = train_cleaned_df.copy()","a1946e64":"from transformers import AutoTokenizer,TFBertModel\ntokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\nbert = TFBertModel.from_pretrained('bert-large-uncased')","df243953":"tokenizer('this is me abhishek and i am a very bad boy &*&*&&')","1e19d16a":"print(\"max len of tweets\",max([len(x.split()) for x in train_df.text]))\nmax_length = 36","42c47cf5":"x_train = tokenizer(\n    text=train_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=36,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n","14d925f2":"x_train['input_ids'].shape","2540ba90":"x_train['attention_mask'].shape","6b68ba71":"y_train = train_df.target.values\ny_train","bfc4fee1":"train_df.target.value_counts()","5d3e5bee":"from tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom tensorflow.keras.losses import CategoricalCrossentropy,BinaryCrossentropy\nfrom tensorflow.keras.metrics import CategoricalAccuracy,BinaryAccuracy\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.utils import plot_model","3cc23b9c":"max_len = 36\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Input, Dense\n\ninput_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\ninput_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n\n\nembeddings = bert(input_ids,attention_mask = input_mask)[1] #(0 is the last hidden states,1 means pooler_output)\n# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\nout = tf.keras.layers.Dropout(0.1)(embeddings)\n\nout = Dense(128, activation='relu')(out)\nout = tf.keras.layers.Dropout(0.1)(out)\nout = Dense(32,activation = 'relu')(out)\n\ny = Dense(1,activation = 'sigmoid')(out)\n    \nmodel = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\nmodel.layers[2].trainable = True\n# for training bert our lr must be so small","531f7f93":"model.summary()","86a21c16":"optimizer = Adam(\n    learning_rate=6e-06, # this learning rate is for bert model , taken from huggingface website \n    epsilon=1e-08,\n    decay=0.01,\n    clipnorm=1.0)\n\n# Set loss and metrics\nloss = BinaryCrossentropy(from_logits = True)\nmetric = BinaryAccuracy('accuracy'),\n# Compile the model\nmodel.compile(\n    optimizer = optimizer,\n    loss = loss, \n    metrics = metric)","219fa48f":"plot_model(model, show_shapes = True)","60bba9f8":"import tensorflow as tf\ntf.config.experimental.list_physical_devices('GPU')","b4772e78":"train_history = model.fit(\n    x ={'input_ids':x_train['input_ids'],'attention_mask':x_train['attention_mask']} ,\n    y = y_train,\n#     validation_split = 0.1,\n  epochs=10,\n    batch_size=10\n)","3b4a8719":"test_df","f8172d43":"# test_df = text_preprocessing(test_df,'text')\n# without cleaning the model gives better accuracy on uplaoding ","349d4164":"x_test = tokenizer(\n    text=test_df.text.tolist(),\n    add_special_tokens=True,\n    max_length=36,\n    truncation=True,\n    padding=True, \n    return_tensors='tf',\n    return_token_type_ids = False,\n    return_attention_mask = True,\n    verbose = True)\n","e28b01df":"predicted = model.predict({'input_ids':x_test['input_ids'],'attention_mask':x_test['attention_mask']})","270e9e01":"y_predicted = np.where(predicted>0.5,1,0)","8b517ff0":"y_predicted = y_predicted.reshape((1,3263))[0]","a942284d":"sample_df['id'] = test_df.id\nsample_df['target'] = y_predicted","064a1636":"sample_df.to_csv('submission4.csv',index = False)","a43352d4":"#### Building the model architecture ","7896cc71":"#### TESTING PHASE\non this phase we will make predictions out of our model and then submit to kaggle comptetions","4b7c92de":"#### Convert Our text data into BERT input format ","f4e48c56":"#### Loading the data ","da7a489a":"here target 1 means we are talking about any accident or disaster and 0 means just a formal tweets with not much attention\n\nso far we have cleaned our text data and now lets load our model","3d6210be":"here sample_df shows up the format of data to be uploaded on kaggle comptetions","6a1190f7":"#### Loading Pretrained BERT Model","7ca6748d":"#### We need to do some text cleaning\nhere some signs and characters need to be removed , again cleaning the text data before training is a good practice well bert is more advanced architecture , it doesn't much affect if we dont do data cleaning \n, bert dont need extensive text_cleaning because bert comes with 40\/60000 words hence its really not necessary to do text_cleaning but removing the special characters are good practice","5122966d":"### Filtering all the warnings sign"}}