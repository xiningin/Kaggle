{"cell_type":{"eaf4a984":"code","76e2cf59":"code","dabf6623":"code","95f02be2":"code","986415ed":"code","bb7239a7":"code","11017b6f":"code","bae7db71":"code","a182bb5e":"code","8178cbcd":"code","14e2cdca":"code","f8c1deb5":"code","779e139c":"code","859e126a":"code","048a4efc":"code","737cf4c5":"code","66687b4a":"code","60d4b955":"code","768cb31a":"code","25eef992":"code","8e99aa30":"code","9e69973b":"code","e7b0589c":"code","0f99eaed":"code","a1ae6d42":"code","7c93949b":"code","efd934b1":"code","73b26ac2":"code","9b87dece":"code","5bfc3a5b":"code","766fe5cd":"code","fbbbd1af":"code","45ba0bf6":"code","967d7f2a":"code","a81a7209":"code","ed86cd3e":"code","d58bbfe6":"code","71829242":"code","71ba7d1b":"code","ac093c6b":"code","6e46d6d3":"code","fcdff03b":"code","e045e00e":"code","146c1eb4":"code","821c6e06":"code","e05d1763":"code","46e3ac71":"code","77206184":"code","9a72c3bb":"code","938a29bf":"code","dde26d35":"code","bfcf160c":"markdown","392c353f":"markdown","460f0453":"markdown","829bbb6f":"markdown","1ecf00c8":"markdown","b676c7ed":"markdown","07707c29":"markdown","f6593cfd":"markdown","d7f0dbbf":"markdown","35373f3a":"markdown","149ec044":"markdown","59fb52c4":"markdown","ae9dd750":"markdown","6d248e6b":"markdown","f80af423":"markdown","712964a1":"markdown","cb829206":"markdown","0181c48c":"markdown","10992b0f":"markdown","b7cd2d25":"markdown","3cbc3e2a":"markdown","f330c871":"markdown","b061a7ff":"markdown","fd927bd2":"markdown","c70363b5":"markdown","085ee801":"markdown","4f54b6a5":"markdown","184288d3":"markdown"},"source":{"eaf4a984":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\npd.set_option('display.max_columns',50)","76e2cf59":"import pycaret\nfrom pycaret.datasets import get_data\nfrom pycaret.classification import *\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings('ignore')","dabf6623":"data_directory = '..\/input\/jobathon-analytics-vidhya'\n\ntrain_path = os.path.join(data_directory, 'train.csv')\ntest_path = os.path.join(data_directory, 'test.csv')","95f02be2":"traindf = pd.read_csv(train_path)\ntestdf = pd.read_csv(train_path)\n\nprint(traindf.shape)\ntraindf.head(3)","986415ed":"traindf.isnull().sum()","bb7239a7":"nonclients = ['Holding_Policy_Duration','Holding_Policy_Type']\nfor col in nonclients:\n    traindf[col] = traindf[col].fillna(0)\n    testdf[col] = testdf[col].fillna(0)","11017b6f":"traindf['Health Indicator'] = traindf['Health Indicator'].fillna(traindf['Health Indicator'].mode()[0])\ntestdf['Health Indicator'] = testdf['Health Indicator'].fillna(testdf['Health Indicator'].mode()[0])","bae7db71":"traindf['Long_Term_Cust'] = traindf['Holding_Policy_Duration'].apply(lambda x: 'Yes' if x == '14+' else 'No')\ntestdf['Long_Term_Cust'] = testdf['Holding_Policy_Duration'].apply(lambda x: 'Yes' if x == '14+' else 'No')\n\ntraindf['Holding_Policy_Duration'] = traindf['Holding_Policy_Duration'].replace('14+',15).astype(float).astype(int)\ntestdf['Holding_Policy_Duration'] = testdf['Holding_Policy_Duration'].replace('14+',15).astype(float).astype(int)","a182bb5e":"traindf.rename(columns={'Is_Spouse':'Married','Health Indicator':'Health_Indicator'},inplace=True)\ntestdf.rename(columns={'Is_Spouse':'Married','Health Indicator':'Health_Indicator'},inplace=True)\n\ntraindf['Avg_Age'] = (traindf['Upper_Age'] + traindf['Lower_Age']) \/ 2\ntestdf['Avg_Age'] = (testdf['Upper_Age'] + testdf['Lower_Age']) \/ 2","8178cbcd":"# feature engineering\ntraindf['Prim_Prem_Ratio'] = traindf['Reco_Policy_Premium'] \/ traindf['Upper_Age']\ntestdf['Prim_Prem_Ratio'] = testdf['Reco_Policy_Premium'] \/ testdf['Upper_Age']","14e2cdca":"traindf.drop(['ID','Region_Code','Upper_Age','Lower_Age'],axis=1,inplace=True)\ntestdf2 = testdf.copy()\ntestdf.drop(['ID','Region_Code','Upper_Age','Lower_Age'],axis=1,inplace=True)","f8c1deb5":"numcols = testdf.select_dtypes('number').columns\nfor col in numcols:\n    traindf[col] = traindf[col].astype(int)\n    testdf[col] = testdf[col].astype(int)","779e139c":"# copy for final analysis\ndf = traindf.copy()","859e126a":"vals = {'Rented':1,'Owned':2,'Individual':1,'Joint':2,'No':0,'Yes':1}\ncols = ['Accomodation_Type','Reco_Insurance_Type','Married','Long_Term_Cust']\n\nfor col in cols:\n    traindf[col] = traindf[col].replace(vals)\n    testdf[col] = testdf[col].replace(vals)","048a4efc":"ordinal = ['Holding_Policy_Type','Reco_Policy_Cat']\nfor col in ordinal:\n    traindf[col] = traindf[col].astype('O')\n    testdf[col] = testdf[col].astype('O')","737cf4c5":"corr = traindf.corr() # analyzing correlation\nfig, ax = plt.subplots(figsize=(12,10))\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\nsns.heatmap(corr, mask=mask, square=True, annot=True, cmap='YlGnBu')\nax.patch.set_edgecolor('black')  \nax.patch.set_linewidth('1')\nax.set_title('Correlation & Heat Map', fontsize=15, fontfamily='serif')\nplt.show()","66687b4a":"traindf.drop(['Married'],axis=1,inplace=True)\ntestdf.drop(['Married'],axis=1,inplace=True)","60d4b955":"targetdf = df.groupby('Response').mean().head()\ntargetdf.style.background_gradient(cmap='Reds')","768cb31a":"fig, ax = plt.subplots(figsize=(10,6))\nax = sns.countplot(data=df[df['Holding_Policy_Type']!=0],x='Holding_Policy_Type',hue='Response',palette='mako');\nfor p in ax.patches:\n        ax.annotate(p.get_height(),(p.get_x()+0.09, p.get_height()+75))\nfig.savefig('policytypecount.jpg',dpi=200,bbox_inches='tight')","25eef992":"fig, ax = plt.subplots(figsize=(10,6))\nsns.violinplot(data=df[df['Holding_Policy_Type']!=0],x='Holding_Policy_Type',y='Avg_Age',hue='Response',palette='mako');\nfig.savefig('policytypexage.jpg',dpi=200,bbox_inches='tight')","8e99aa30":"traincat_vars = [var for var in traindf.columns if traindf[var].dtype == 'O']\ntestcat_vars = [var for var in testdf.columns if testdf[var].dtype == 'O']","9e69973b":"def replace_categories(df, var, target):\n    # Order variable categories | lowest to highest against target (price)\n    ordered_labels = df.groupby([var])[target].mean().sort_values().index\n    # Dictionary of ordered categories to integer values\n    ordinal_label = {k: i for i, k in enumerate(ordered_labels, 0)}\n    # Replace the categorical strings by integers using dictionary\n    df[var] = df[var].map(ordinal_label)","e7b0589c":"for var in traincat_vars:\n    replace_categories(traindf, var, 'Avg_Age')","0f99eaed":"for var in testcat_vars:\n    replace_categories(testdf, var, 'Avg_Age')","a1ae6d42":"# labelencoder = preprocessing.LabelEncoder()\n# scaler = preprocessing.StandardScaler()","7c93949b":"# traindf['City_Code'] = labelencoder.fit_transform(traindf['City_Code'])\n# testdf['City_Code'] = labelencoder.fit_transform(testdf['City_Code'])\n# traindfscaled = scaler.fit_transform(traindf)\n# testdfscaled = scaler.fit_transform(testdf)","efd934b1":"dataset = traindf.copy()\ndata = dataset.sample(frac=0.80, random_state=786)\ndata_unseen = dataset.drop(data.index).reset_index(drop=True)\nprint('Data for Modeling: ' + str(data.shape))\nprint('Unseen Data For Predictions: ' + str(data_unseen.shape))","73b26ac2":"clf = setup(data=data,target='Response',session_id=123,numeric_features=['Long_Term_Cust','Health_Indicator','Accomodation_Type','Reco_Insurance_Type','Holding_Policy_Duration','Holding_Policy_Type'])","9b87dece":"compare_models()","5bfc3a5b":"def model_visuals (model, X_test, y_test):\n    '''Plots the confusion matrix and ROC-AUC plot'''\n    fig, axes = plt.subplots(figsize = (12, 6), ncols = 2)  # confusion matrix\n    metrics.plot_confusion_matrix(model, X_test, y_test, normalize = 'true', \n                          cmap = 'Blues', ax = axes[0])\n    axes[0].set_title('Confusion Matrix');\n    # ROC-AUC Curve\n    roc_auc = metrics.plot_roc_curve(model, X_test, y_test,ax=axes[1])\n    axes[1].plot([0,1],[0,1],ls=':')\n    axes[1].set_title('ROC-AUC Plot')\n    axes[1].grid()\n    axes[1].legend()\n    fig.tight_layout()\n    plt.show()","766fe5cd":"X_train, X_test, y_train, y_test = train_test_split(traindf.drop(columns=['Response'],axis=1),traindf['Response'],test_size=0.2, random_state=42)","fbbbd1af":"gbclf = GradientBoostingClassifier(random_state=42)\ngbclf.fit(X_train,y_train)","45ba0bf6":"param_grid = {\n    'learning_rate': [0.1,0.2],\n    'max_depth': [6],\n    'subsample': [0.5,0.7,1],\n    'n_estimators': [100]\n}","967d7f2a":"grid_clf = GridSearchCV(gbclf,param_grid,scoring='roc_auc',cv=None,n_jobs=1)\ngrid_clf.fit(X_train,y_train)\n\nbest_parameters = grid_clf.best_params_\n\nprint('Grid search found the following optimal parameters: ')\nfor param_name in sorted(best_parameters.keys()):\n    print('%s: %r' % (param_name,best_parameters[param_name]))\n    \ntraining_preds = grid_clf.predict(X_train)\ntest_preds = grid_clf.predict(X_test)\ntraining_accuracy = accuracy_score(y_train,training_preds)\ntest_accuracy = accuracy_score(y_test,test_preds)\n\nprint('')\nprint('Training Accuracy: {:.4}%'.format(training_accuracy*100))\nprint('Validation Accuracy: {:.4}%'.format(test_accuracy*100))","a81a7209":"gbclf = GradientBoostingClassifier(max_depth=6,learning_rate=0.1,n_estimators=100,subsample=1,random_state=42)\ngbclf.fit(X_train,y_train)\n# predict\ntraining_preds = gbclf.predict(X_train)\ntest_preds = gbclf.predict(X_test)\n# accuracy\ntraining_accuracy = accuracy_score(y_train,training_preds)\ntest_accuracy = accuracy_score(y_test,test_preds)","ed86cd3e":"print(classification_report(y_test, test_preds), '\\n\\n')\nmodel_visuals (gbclf, X_test, y_test) # class report \/ plots","d58bbfe6":"# Feature Importance\nX = traindf.drop(columns=['Response'],axis=1)\nclf_feature = pd.DataFrame({'Importance':gbclf.feature_importances_,'Column':X.columns})\nclf_feature = clf_feature.sort_values(by='Importance',ascending=False) \nclf_feature = clf_feature[:8] # top 8 features\nclf_feature.plot(kind='barh',x='Column',y='Importance',figsize=(20, 10))\nplt.title('Gradient Boosting Feature Importance \\n',fontsize=16)\nplt.savefig('featureimportance.jpg',dpi=200,bbox_inches='tight')\nplt.show()","71829242":"df['Reco_Policy_Cat'] = df['Reco_Policy_Cat'].astype('O')\n\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(data=df,x='Reco_Policy_Cat',hue='Response',palette='mako');\nfig.savefig('policycategoryxresponse.jpg',dpi=200,bbox_inches='tight')","71ba7d1b":"RPC = df.groupby(['Reco_Policy_Cat','Response'])['Response'].count().unstack()\nRPC['PositiveRatio'] = RPC[1] \/ (RPC[1] + RPC[0])\nRPC = RPC.sort_values(by='PositiveRatio', ascending=False)[:5].reset_index()\n# RPC\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 5 Reco Policy Categories', fontdict={'fontsize': 14})\nsns.barplot(data=RPC,x='Reco_Policy_Cat',y='PositiveRatio',palette='mako');\nfig.savefig('top5categoryxresponse.jpg',dpi=200,bbox_inches='tight')","ac093c6b":"# Binning Ages for Visualizations\ndf['Premium(bin)'] = df['Reco_Policy_Premium'].apply(lambda x: '0-9999' if x < 10000\n                                                     else '10000-14999' if x < 15000 \n                                                     else '15000-19999' if x < 20000 \n                                                     else '20000-24999' if x < 25000 \n                                                     else '25000-29999' if x < 30000 \n                                                     else '30000+')","6e46d6d3":"df = df.sort_values(['Premium(bin)'], ascending=True)\nfig, ax = plt.subplots(figsize=(10,6))\nsns.countplot(data=df,x='Premium(bin)',hue='Response',palette='mako');\nfig.savefig('premiumbin.jpg',dpi=200,bbox_inches='tight')","fcdff03b":"PRE = df.groupby(['Premium(bin)','Response'])['Response'].count().unstack()\nPRE['PositiveRatio'] = PRE[1] \/ (PRE[1] + PRE[0])\nPRE = PRE.sort_values(by='PositiveRatio', ascending=False)\nPRE","e045e00e":"PRE = PRE.reset_index()\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 5 Premium Bins', fontdict={'fontsize': 16})\nsns.barplot(data=PRE,x='Premium(bin)',y='PositiveRatio',palette='mako');\nfig.savefig('top5premiumbin.jpg',dpi=200,bbox_inches='tight')","146c1eb4":"fig, ax = plt.subplots(figsize=(15,6))\nsns.countplot(data=df,x='City_Code',hue='Response',palette='mako');\nfig.savefig('citycode.jpg',dpi=200,bbox_inches='tight')","821c6e06":"CITY = df.groupby(['City_Code','Response'])['Response'].count().unstack()\nCITY['PositiveRatio'] = CITY[1] \/ (CITY[1] + CITY[0])\nCITY = CITY.sort_values(by='PositiveRatio', ascending=False)[:11]\nCITY","e05d1763":"CITY = CITY.reset_index()\nfig, ax = plt.subplots(figsize=(10,6))\nplt.title('Top 11 City Categories', fontdict={'fontsize': 16})\nsns.barplot(data=CITY,x='City_Code',y='PositiveRatio',palette='mako');\nfig.savefig('top11citycode.jpg',dpi=200,bbox_inches='tight')","46e3ac71":"features = testdf.columns\ntarget = ['Response']","77206184":"# preparing submission\ngbclf.fit(traindf[features], traindf[target].values.ravel())\npredictions = gbclf.predict_proba(testdf[features])[:,1]\nsubmission = pd.DataFrame({'ID': testdf2['ID'],'Response': predictions})","9a72c3bb":"submission['Response'].describe()","938a29bf":"submission['Response'] = submission['Response'].apply(lambda x: 0 if x < 0.3 else 1)","dde26d35":"submission.to_csv('submission.csv', index=False)","bfcf160c":"## Recommendations\nThe model's top 3 features were Reco Policy Category, Reco Policy Premium and City Code. Within those three categories, subcategories yielded the highest positive to total response ratios. It is recommended to focus on clients in\/with:\n\n* City Codes: C1, C2, C13, C23\n* Reco Policy Categories: 15, 22\n* Reco Policy Premiums between: 15,000 & 19,999.\n\n### Limitations\nThe project was limited by the anonymity of the data. Specifically the geographic data that could have been used for additional feature engineering leading to higher scores.\n\n### Future Work\nFuture models can be created using more complicated feature engineering and analysis such as clustering of the geographic features. For the purposes of this project, doing so would have complicated the output and made it difficult to implement within a real workplace.\nFor any additional questions, please reach out via email at santana2.miguel@gmail.com, on [LinkedIn](https:\/\/www.linkedin.com\/in\/miguel-angel-santana-ii-mba-51467276\/) or on [Twitter](https:\/\/twitter.com\/msantana_ds).","392c353f":"#### Submission","460f0453":"Customers who elect to receive additional information typically hold existing policies longer and are classified under a larger policy category with a slightly larger premium.","829bbb6f":"In this particular case I would recommend focusing on the top 11 scoring positive ratios. Ranks 10 and 11 are exponentially larger in volume than the first 9 and have the potential to yield high ROI with positive to total ratios close to 25%.","1ecf00c8":"# Model\n## Pycaret","b676c7ed":"# Data Cleaning | Scrub\n### Null Values","07707c29":"## Final Transformations","f6593cfd":"#### GridSearchCV","d7f0dbbf":"# Interpret Results\n### Feature Importance","35373f3a":"Reco Policy Category 15 is the clear front runner\n\n### Reco Policy Premium","149ec044":"Features 'Accommodation Type', 'Reco Insurance Type', 'Is Spouse' will be converted to binary (0 and 1).","59fb52c4":"\nThe two feature that stand out are 'Holding Policy Type' and 'Reco Policy Cat' which are listed under numerical but most likely correspond to type and category of policy in existing customers.\n\n# Exploratory Data Analysis","ae9dd750":"The violin plot gives an interesting take on Average Age versus Holding Policy Type. HPT 3 shows a pretty even distribution across age groups while HPT 1 is heavily made up of younger individuals.","6d248e6b":"Similar accuracy in the training and test sets suggests minimal under\/over fitting.\n\n### Final Model","f80af423":"# Health Insurance Lead Prediction - Kaggle Competition\n## Job-A-Thon - Analytics Vidhya, Health Insurance\nAuthor: Miguel Santana\n\n### Project Methodology\nFinMan Company is looking to leverage their client base by cross selling insurance products to existing customers. Insurance policies are offered to prospective and existing clients based on website landing and consumer election to fill out additional information forms. FinMan company would like to leverage their acquired information to classify positive leads for outreach programs using machine learning classifiers.\n\n### Data and Analytical Structure\nThe project dataset is provided by Analytics Vidhya via Kaggle. Data includes demographic features, policy features (for current customers) and example positive classifications for ML model validation and interpretation. The source can be found [here](https:\/\/www.kaggle.com\/imsparsh\/jobathon-analytics-vidhya?select=sample_submission.csv). The project analysis will follow the OSEMN framework: Obtain, Scrub, Explore, Model and Interpret.\n\n# Data & Packages | Obtain","712964a1":"The unique 'ID' and 'Region Code' columns will be dropped in order to simplify the data. 'Region Code' consists of far too many categorical values which would need to be one hot encoded. The feature is dropped as the data still retains the 'City Code' feature to capture some level of geographical distinction. In addition, the upper and lower age features will be dropped being represented by average age.","cb829206":"#### Renaming Features","0181c48c":"Lets confirm the positive to total response ratios before we make a recommendation.","10992b0f":"This graph may be misleading as each policy category caries a significantly different client count. Lets break down the top five categories based on positive over total responses.","b7cd2d25":"Holding Policy Type three has the highest number of positive responses but all four of the categories have approximately 30% positive to negative client responses.","3cbc3e2a":"## Scikit-Learn","f330c871":"### Reco Policy Category","b061a7ff":"### Feature Selection","fd927bd2":"In following the theme of the test and train datasets as well as presenting a client list of a respectable size, the cut off for positive response predictions will be 30%.","c70363b5":"In this case, most of the ratios are extremely close so the recommendation would be to focus on individuals who pay an annual premium between 15,000 and 19,999 as they convert at approximately the same rate as the front runner but represent a much larger group of clients. The high conversion along with the larger client volume will lead to higher profit.\n\n### City Code","085ee801":"With each of the categorical values mapped to values with respect to average age, the resulting values will end up on a similar scale as the rest of the dataset. In order to minimize data manipulation for modeling, no label encoding or standard scaling will occur.","4f54b6a5":"With many of these prospects not clearly identified as current clients, its safe to assume that null values in the 'Policy Duration' and 'Policy Type' columns are tied to non existing accounts and may be filled with zeros.\n\n## Feature Engineering\nConvert to numerical: Holding_Policy_Duration\n* Feature engineer long term customers\n* Convert '14+' to '15' \/ convert to numerical\n\nNote: (after EDA) Convert to binary | Accomodation_Type, Reco_Insurance_Type, Is_Spouse\n\n#### Categorical Features","184288d3":"Typically, insurance products are priced and underwritten based on the age of the applicant or applicants. This is especially the case in most health insurance pricing. To reflect this and retain data, an average age feature will be created and the original two features will be dropped."}}