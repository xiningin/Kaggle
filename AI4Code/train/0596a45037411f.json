{"cell_type":{"ab7e1d5b":"code","dc0d526d":"code","c9725717":"code","c463892c":"code","16f6a4f8":"code","e8adcb45":"code","94d403fb":"code","eb7f28b0":"code","9708ef21":"code","c0d268c6":"code","d7cb4d83":"code","5362e709":"code","6b60ee1b":"code","87f91f42":"code","0bb800ca":"code","de5f047a":"code","81833db3":"code","dec1f78a":"code","5b7f0bdd":"code","0d01daad":"code","11f32e68":"code","b0cb81a9":"code","010f0ed3":"code","26c639ac":"code","06147c1c":"code","ea2ba0a3":"code","e6ebc7b0":"code","064ee518":"code","4950e535":"code","51487878":"code","27256d20":"code","e233f9c0":"code","c05cfa74":"code","b51e9656":"markdown","1c083bd3":"markdown","5409d7ea":"markdown","14882727":"markdown","55a58e62":"markdown","906029f1":"markdown","6fd2a3d4":"markdown"},"source":{"ab7e1d5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc0d526d":"import json\n\ndef load_rows(filepath, nrows = None):\n    with open(filepath) as json_file:\n        count = 0\n        objs = []\n        line = json_file.readline()\n        while (nrows is None or count < nrows) and line:\n            count += 1\n            obj = json.loads(line)\n            objs.append(obj)\n            line = json_file.readline()\n        return pd.DataFrame(objs)\n    ","c9725717":"business = load_rows('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_business.json',10000)\nprint('Business objects loaded. Count = {}'.format(business.shape[0]))\n\ntip = load_rows('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_tip.json',10000)\nprint('tip objects loaded. Count = {}'.format(tip.shape[0]))\n\nuser = load_rows('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_user.json',10000)\nprint('user objects loaded. Count = {}'.format(user.shape[0]))\n\nreview = load_rows('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_review.json',10000)\nprint('checkin objects loaded. Count = {}'.format(review.shape[0]))\n\ncheckin = load_rows('\/kaggle\/input\/yelp-dataset\/yelp_academic_dataset_checkin.json',10000)\nprint('checkin objects loaded. Count = {}'.format(checkin.shape[0]))","c463892c":"business.head()\n","16f6a4f8":"tip.head()","e8adcb45":"user.head()","94d403fb":"review.head()","eb7f28b0":"checkin.head()","9708ef21":"# change data type from float to category\nbusiness['stars'] = business['stars'].astype('category')\nprint(business['stars'].values[0:10])\n# count plot for category\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nsns.countplot('stars', data=business)\nplt.title('Rating star distribution \\n (1,2,3,4,5)')\nplt.show()","c0d268c6":"# calculate the count and percentage of each rating class\ncount = business.groupby('stars')[\"stars\"].agg([\"count\"])\ncount['percentage'] = count.apply(lambda x: x\/sum(x))\ncount","d7cb4d83":"business[business.categories.isnull()]","5362e709":"business = business.dropna(axis=0, subset=['categories'])\ntext = ''.join(business['categories'])\ncats=pd.DataFrame(text.split(','),columns=['category'])\ncats.head()\nprint(\"There are {} number of categories in this business\".format(len(cats.value_counts())))","6b60ee1b":"cats.groupby('category').size().sort_values(ascending=False).reset_index().rename(columns ={0:'qty'}).loc[:10]\n\ncats = cats.groupby('category').size().sort_values(ascending=False).reset_index().rename(columns ={0:'qty'}).loc[:10]","87f91f42":"cats.head()","0bb800ca":"plt.figure(figsize = (12,7))\nax = sns.barplot(x=\"qty\", y=\"category\", data=cats,color =\"#F1C40F\")\nplt.show()","de5f047a":"city = business.groupby('city')[\"business_id\"].agg({'size'}).sort_values(ascending=False, by = 'size').reset_index().loc[0:10]\n\nplt.figure(figsize = (12,7))\nax = sns.barplot(y=\"city\", x=\"size\", data=city,color =\"#F1C40F\")\nplt.show()","81833db3":"from wordcloud import WordCloud\ntext = ''.join(business['categories'])\n# Generate a word cloud image\nwordcloud = WordCloud().generate(text)\n\n\n# lower max_font_size\nwordcloud = WordCloud(max_font_size=40).generate(text)\nplt.figure(figsize = (10,13))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","dec1f78a":"# create a dataframe that \ndf = business.merge(review,on = 'business_id')\ntop_10_df = df.groupby('city')['funny'].count().reset_index().rename(columns = {'funny':'qty'}).sort_values(by = 'qty',ascending=False).head(10)\ntop_10_df","5b7f0bdd":"df.head()","0d01daad":"df.dtypes","11f32e68":"from datetime import datetime\n# 2015-04-15 05:21:16\npattern = '%Y-%m-%d %H:%M:%S'\ndf['DoW'] = df[\"date\"].apply(lambda x: datetime.strptime(x,pattern).weekday())\ndf.groupby('DoW').size().reset_index().rename(columns = {0:'count'})\n","b0cb81a9":"reviews_qty = review.groupby(['user_id']).size()\n","010f0ed3":"checkin.head()","26c639ac":"checkin.loc[0,'date']","06147c1c":"business[['name', 'review_count', 'city', 'stars']].sort_values(ascending=False, by=\"review_count\")[0:10]\n","ea2ba0a3":"# clean the text for tokenizing\nimport string\n\ndef cleaner(text):\n    text = text.replace(\".\", \" fullstop \")\n    text = text.replace(\",\", \" comma \")\n    text = \"\".join(v for v in text if v not in string.punctuation).lower()\n    text = text.encode(\"utf8\").decode(\"ascii\",'ignore')\n    text = text.replace(\"\\n\", \" \")\n    return text","e6ebc7b0":"cleaned_text = [cleaner(i) for i in text.split(',')]","064ee518":"# Converting the text to sequences of tokens\nfrom keras.preprocessing.text import Tokenizer\n\ntokenizer = Tokenizer()\n\ndef get_tokens(reviews):\n    \n    # fit the tokenizer on all of the strings in 'corpus' (or the cleaned review text)\n    tokenizer.fit_on_texts(reviews)\n    \n    # the total number of words is:\n    total_words = len(tokenizer.word_index) + 1\n    \n    input_sequences = []\n    \n    # for every review in the list of reviews, use the fitted tokenizer to convert the text to a sequence\n    for line in reviews:\n        \n        # get the list of words within that review\n        token_list = tokenizer.texts_to_sequences([line])[0]\n        \n        # for each word, append the corresponding token to the input_sequences list (the sentence made up of tokenized words)\n        for i in range(1, len(token_list)):\n            n_gram_sequence = token_list[:i+1]\n            input_sequences.append(n_gram_sequence)\n    return input_sequences, total_words\n\ninp_sequences, total_words = get_tokens(cleaned_text)\n","4950e535":"# Padding the sequences of tokens so that they have the same length as inputs to the RNN\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nimport keras.utils as ku\n\ndef generate_padded_sequences(input_sequences):\n    \n    # find the longest sequence of words\n    max_sequence_len = max([len(x) for x in input_sequences])\n    \n    # create a numpy array of the sequences, padded to the maximum sequence length\n    input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n    \n    # assign the predictors as the first column, and the labels as the last column?\n    predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n    \n    # use kaggle utilities to convert the class vector (integers) to binary class matrix\n    label = ku.to_categorical(label, num_classes=total_words)\n    \n    return predictors, label, max_sequence_len\n\npredictors, label, max_sequence_len = generate_padded_sequences(inp_sequences)","51487878":"# Creating the RNN using LSTM (long short term memory)\n\nfrom keras.models import Sequential\nfrom keras import layers\n\ndef create_model(max_sequence_len, total_words):\n    \n    # number of inputs to the RNN is the longest combination of words\n    input_len = max_sequence_len-1\n    \n    model = Sequential([\n        # Input embedding layer\n        layers.Embedding(total_words, 10, input_length=input_len),\n        # LSTM hidden later\n        layers.LSTM(256, return_sequences=True),\n        layers.LSTM(256),\n        # Dropout 10% of the layer to reduce over fitting\n        layers.Dropout(0.1),\n        # Output later\n        layers.Dense(total_words, activation='softmax')  \n    ])\n    \n    model.compile(\n            loss='categorical_crossentropy',\n            optimizer='adam'\n    )\n    \n    return model\n    \nmodel = create_model(max_sequence_len, total_words)\nmodel.summary()","27256d20":"# train the model\n\nmodel.fit(predictors, label, epochs=5, verbose=5)\n","e233f9c0":"# generate fake news\ndef generate_review(seed_text, model, max_sequence_len, next_words=20):\n   \n    # for each new word\n    for _ in range(next_words):\n        \n        # turn the seed text into a value using the tokenizer\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        \n        # pad the seed text so that it's the same length as the longest sequence of words\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        \n        # predice the next word, based on the tokenised words used so far\n        predicted = model.predict_classes(token_list, verbose=0)\n        \n        output_word = \"\"\n        \n        # for each word in the list of words in the tokenizer, check if the predicted word matches\n        # if it does then the append the new word to the seed_text and make another prediction\n        for word,index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \"+output_word\n    \n    # replace the punctuation words with actual punctuation\n    seed_text = seed_text.replace(\" fullstop\", \".\")\n    seed_text = seed_text.replace(\" comma\", \",\")\n    \n    # Regex to start all sentances with a capital\n    seed_text = '. '.join(sentence.capitalize() for sentence in seed_text.split(\". \")[:-1]) + \".\"\n    \n    return seed_text\n","c05cfa74":"print(generate_review(\"I ate\", model, max_sequence_len, next_words=20))\nprint(generate_review(\"I really\", model, max_sequence_len, next_words=30))\nprint(generate_review(\"That was\", model, max_sequence_len, next_words=40))\nprint(generate_review(\"Wow\", model, max_sequence_len, next_words=50))\nprint(generate_review(\"I am\", model, max_sequence_len, next_words=60))\nprint(generate_review(\"Oh my\", model, max_sequence_len, next_words=70))\nprint(generate_review(\"This is\", model, max_sequence_len, next_words=80))\nprint(generate_review(\"What a\", model, max_sequence_len, next_words=90))\nprint(generate_review(\"My\", model, max_sequence_len, next_words=100))","b51e9656":"# list the top 10 funny review city","1c083bd3":"# list the day of week","5409d7ea":"it shows that the average ratiing score is around 3 - 4.","14882727":"# generate fake news","55a58e62":"# Rating star distribution","906029f1":"# review count\n","6fd2a3d4":"# top category"}}