{"cell_type":{"22f80c48":"code","563402d8":"code","f2fecff5":"code","ae91e608":"code","8664d621":"code","0f755474":"code","2df95b84":"code","d22bb84b":"code","96154e10":"code","a6f0926b":"code","af38c0a9":"code","f69082df":"code","f08518bb":"code","c996f4d1":"code","bf830c5b":"code","d19cc729":"code","7983a42c":"code","6dc25df9":"code","2c72b54a":"markdown","6c29c527":"markdown","fa052141":"markdown","ef49d84d":"markdown","b2a5e612":"markdown","f2a27601":"markdown"},"source":{"22f80c48":"# import library \nimport numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)  \npd.set_option('display.float_format', '{:20,.2f}'.format)\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(16,4))\nimport seaborn as sns\nfrom keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\nfrom keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import LearningRateScheduler","563402d8":"# read data \ntrain = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/train.csv')\ntest = pd.read_csv('..\/input\/jigsaw-unintended-bias-in-toxicity-classification\/test.csv')","f2fecff5":"train.head(3)","ae91e608":"# IDENTITY_COLUMNS (\uc2e0\uc6d0\uc744 \ud30c\uc545\ud560 \uc218 \uc788\ub294 \ubcc0\uc218)\nIDENTITY_COLUMNS = ['male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish','muslim', 'black', 'white', 'psychiatric_or_mental_illness']\n\n# AUX_COLUMNS (additional toxicity subtype attributes) \nAUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']","8664d621":"tmp = train.isnull().sum(axis=0) \/ len(train)\ntmp[tmp > 0]","0f755474":"identities = [\n    'male','female','transgender','other_gender','heterosexual','homosexual_gay_or_lesbian',\n    'bisexual','other_sexual_orientation','christian','jewish','muslim','hindu','buddhist',\n    'atheist','other_religion','black','white','asian','latino','other_race_or_ethnicity',\n    'physical_disability','intellectual_or_learning_disability','psychiatric_or_mental_illness','other_disability'\n]\n\ntmp = train.loc[:, ['target'] + identities ].dropna()\ntoxic_df = tmp[tmp['target'] >= .5][identities]\nnon_toxic_df = tmp[tmp['target'] < .5][identities]\nprint('toxic_df',len(toxic_df)\/len(tmp))\nprint('non_toxic_df',len(non_toxic_df)\/len(tmp))","2df95b84":"print(train[train.target==1].iloc[1,2])","d22bb84b":"print(train[train.severe_toxicity==1].iloc[0,2])","96154e10":"print(train[train.obscene==1].iloc[1,2])","a6f0926b":"print(train[train.identity_attack==1].iloc[1,2])","af38c0a9":"print(train[train.insult==1].iloc[1,2])","f69082df":"print(train[train.threat==1].iloc[1,2])","f08518bb":"# tox label counts\ntox_labels = train[AUX_COLUMNS].copy()\nrowsums = tox_labels.sum(axis=1)\ntrain['clean']=(rowsums==0)\ntox_labels['clean']=train['clean'].copy()\nx = tox_labels.sum().copy()\nsns.barplot(x.index, x.values)","c996f4d1":"# corr matrix\ncorr = train[AUX_COLUMNS].corr()\nprint(corr)\nsns.heatmap(corr) ","bf830c5b":"train['total_length'] = train['comment_text'].apply(len)\ntrain['capitals'] = train['comment_text'].apply(lambda comment: sum(1 for c in comment if c.isupper()))\ntrain['caps_vs_length'] = train.apply(lambda row: float(row['capitals'])\/float(row['total_length']),axis=1)\ntrain['num_exclamation_marks'] = train['comment_text'].apply(lambda comment: comment.count('!'))\ntrain['num_question_marks'] = train['comment_text'].apply(lambda comment: comment.count('?'))\ntrain['num_punctuation'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '.,;:'))\ntrain['num_symbols'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in '*&$%'))\ntrain['num_words'] = train['comment_text'].apply(lambda comment: len(comment.split()))\ntrain['num_unique_words'] = train['comment_text'].apply(lambda comment: len(set(w for w in comment.split())))\ntrain['words_vs_unique'] = train['num_unique_words'] \/ train['num_words']\ntrain['num_smilies'] = train['comment_text'].apply(lambda comment: sum(comment.count(w) for w in (':-)', ':)', ';-)', ';)')))","d19cc729":"train['all_capitals_YN'] = train.apply(lambda row: float(row['capitals'])==float(row['total_length']),axis=1)","7983a42c":"features = [\n'total_length', 'capitals', 'caps_vs_length', 'num_exclamation_marks','num_question_marks', \n'num_punctuation', 'num_words', 'num_unique_words','words_vs_unique', 'num_smilies', 'num_symbols',\n'all_capitals_YN'\n]    \ntmp = train[['target']+features].copy()\ntmp = tmp.apply(lambda x:np.where(x>=0.5,1,0))\ntmp.groupby(features)['target'].agg([np.mean,np.sum,np.size]).T","6dc25df9":"pd.crosstab(tmp.target,tmp.all_capitals_YN)","2c72b54a":"# correlation matrix","6c29c527":"# extra features ","fa052141":"# missinv values","ef49d84d":"# check the comments","b2a5e612":"# check data","f2a27601":"# count sub-target"}}