{"cell_type":{"4a04ba26":"code","887dacff":"code","249086dd":"code","e2723c4a":"code","a061e2b7":"code","52bbf9e7":"code","3b67949b":"code","922bc537":"code","a0a12d2d":"code","9fd0e334":"code","8908e8fb":"code","88ae7616":"code","e89f1786":"code","ddb06124":"code","64161e4a":"code","e145351f":"code","8fb26983":"code","4a06e713":"code","bded93c1":"markdown","c03109fa":"markdown","589fe0e4":"markdown"},"source":{"4a04ba26":"#DataFrame of sample submission file\n#Percentile Start    End  Aggligation\n#0.005         0       0  Total\n#              1       3  state\n#              4      13  store\n#             14      16  cat\n#             17      23  dept\n#             14      32  state, cat\n#             33      53  state, dept\n#             54      83  store, cat\n#             84     153  store, dept\n#            154    3203  item\n#           3203   12349  item, state\n#          12350   42839  item, store\n#0.025     42840   42840  Total\n#          42841   42843  state\n#          42844   42853  store\n#          42854   42856  cat\n#          42857   42863  dept\n#          42864   42872  state, cat\n#          42873   42893  state, dept\n#          42894   42923  store, cat\n#          42924   42993  store, dept\n#          42994   46042  item\n#          46043   55189  item, state\n#          55190   85679  item, store\n#\n#  Continue 0.165 , 0.25 , ,,, 0.995 , same procedure of validation and evalunation.\n#\n#0.995    728280  728280  Total\n#         728281  728283  state\n#         728284  728293  store\n#         728294  728296  cat\n#         728297  728303  dept\n#         728304  728312  state, cat\n#         728313  728333  state, dept\n#         728334  728363  store, cat\n#         728364  728433  store, dept\n#         728434  731482  item\n#         731483  740629  item, state\n#         740630  771119  item, store         ","887dacff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","249086dd":"validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/calendar.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sample_submission.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-uncertainty\/sell_prices.csv')","e2723c4a":"import numpy as np\nimport pandas as pd\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm","a061e2b7":"ss=sample_submission\nss_array=np.zeros((771120,29))","52bbf9e7":"#Order of sample_submission is changed to same order of \"sort_values\". \n#(such that descending order)\n\nfor k in range(2):\n    for i in range(9):\n        ss_i_1=ss.iloc[385560*k+42840*i+1:385560*k+42840*i+4,:]\n        ss_i_1_s=ss_i_1.sort_values([\"id\"])\n        ss_i_1_s_r=ss_i_1_s.reset_index(drop=True)\n        ss_i_1_s_r.index=ss_i_1_s_r.index+1+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+1:385560*k+42840*i+4,:]=ss_i_1_s\n        \n        ss_i_2=ss.iloc[385560*k+42840*i+4:385560*k+42840*i+14,:]\n        ss_i_2_s=ss_i_2.sort_values([\"id\"])\n        ss_i_2_s_r=ss_i_2_s.reset_index(drop=True)\n        ss_i_2_s_r.index=ss_i_2_s_r.index+4+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+4:385560*k+42840*i+14,:]=ss_i_2_s\n        \n        ss_i_3=ss.iloc[385560*k+42840*i+14:385560*k+42840*i+17,:]\n        ss_i_3_s=ss_i_3.sort_values([\"id\"])\n        ss_i_3_s_r=ss_i_3_s.reset_index(drop=True)\n        ss_i_3_s_r.index=ss_i_3_s_r.index+14+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+14:385560*k+42840*i+17,:]=ss_i_3_s\n        \n        ss_i_4=ss.iloc[385560*k+42840*i+17:385560*k+42840*i+24,:]\n        ss_i_4_s=ss_i_4.sort_values([\"id\"])\n        ss_i_4_s_r=ss_i_4_s.reset_index(drop=True)\n        ss_i_4_s_r.index=ss_i_4_s_r.index+17+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+17:385560*k+42840*i+24,:]=ss_i_4_s\n        \n        ss_i_5=ss.iloc[385560*k+42840*i+24:385560*k+42840*i+33,:]\n        ss_i_5_s=ss_i_5.sort_values([\"id\"])\n        ss_i_5_s_r=ss_i_5_s.reset_index(drop=True)\n        ss_i_5_s_r.index=ss_i_5_s_r.index+24+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+24:385560*k+42840*i+33,:]=ss_i_5_s\n        \n        ss_i_6=ss.iloc[385560*k+42840*i+33:385560*k+42840*i+54,:]\n        ss_i_6_s=ss_i_6.sort_values([\"id\"])\n        ss_i_6_s_r=ss_i_6_s.reset_index(drop=True)\n        ss_i_6_s_r.index=ss_i_6_s_r.index+33+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+33:385560*k+42840*i+54,:]=ss_i_6_s\n        \n        ss_i_7=ss.iloc[385560*k+42840*i+54:385560*k+42840*i+84,:]\n        ss_i_7_s=ss_i_7.sort_values([\"id\"])\n        ss_i_7_s_r=ss_i_7_s.reset_index(drop=True)\n        ss_i_7_s_r.index=ss_i_7_s_r.index+54+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+54:385560*k+42840*i+84,:]=ss_i_7_s\n        \n        ss_i_8=ss.iloc[385560*k+42840*i+84:385560*k+42840*i+154,:]\n        ss_i_8_s=ss_i_8.sort_values([\"id\"])\n        ss_i_8_s_r=ss_i_8_s.reset_index(drop=True)\n        ss_i_8_s_r.index=ss_i_8_s_r.index+84+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+84:385560*k+42840*i+154,:]=ss_i_8_s\n        \n        ss_i_9=ss.iloc[385560*k+42840*i+154:385560*k+42840*i+3203,:]\n        ss_i_9_s=ss_i_9.sort_values([\"id\"])\n        ss_i_9_s_r=ss_i_9_s.reset_index(drop=True)\n        ss_i_9_s_r.index=ss_i_9_s_r.index+154+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+154:385560*k+42840*i+3203,:]=ss_i_9_s\n        \n        ss_i_10=ss.iloc[385560*k+42840*i+3203:385560*k+42840*i+12350,:]\n        ss_i_10_s=ss_i_10.sort_values([\"id\"])\n        ss_i_10_s_r=ss_i_10_s.reset_index(drop=True)\n        ss_i_10_s_r.index=ss_i_10_s_r.index+3203+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+3203:385560*k+42840*i+12350,:]=ss_i_10_s\n        \n        ss_i_11=ss.iloc[385560*k+42840*i+12350:385560*k+42840*i+42840,:]\n        ss_i_11_s=ss_i_11.sort_values([\"id\"])\n        ss_i_11_s_r=ss_i_11_s.reset_index(drop=True)\n        ss_i_11_s_r.index=ss_i_11_s_r.index+12350+42840*i+385560*k\n        ss.iloc[385560*k+42840*i+12350:385560*k+42840*i+42840,:]=ss_i_11_s_r","3b67949b":"per=[0.5,2.5,16.5,25,50,75,83.5,97.5,99.5]","922bc537":"#Transform to nparray (To calculate fast)\n\nvalidation=validation.T\nvalidation=validation.reset_index()\nvalidation=validation.T\nvalidation_array=validation.values","a0a12d2d":"#Use only last 2 month data\n\nvalidtion_array_del=np.delete(validation_array,np.s_[6:1854],1)","9fd0e334":"#Return to DataFrame\n\nvalidation_del=pd.DataFrame(validtion_array_del)\narray_list=validation_del[0:1].values\nvalidation_del.columns=array_list[0,:]\nvalidation=validation_del.drop(0)\nvalidation=validation.reset_index()\nvalidation=validation.drop(['index'],axis=1)","8908e8fb":"validation","88ae7616":"#Calculate TOTAL\n\nindex=[0]\nunc_all_w=(np.zeros((7,9)))\nallsales = pd.DataFrame(index=index,columns=[])\n\nfor i in range(1848,1913):\n    allsales[f\"d{i+1}\"]=validation[f\"d_{i+1}\"].sum()\n    \nallsales = allsales.T\nallsales=allsales.reset_index()\nallsales=allsales.rename(columns={0:'allitem'})\n\ncalendar_wday=calendar['wday'].iloc[1848:1913].reset_index()\nallsales['wday']=calendar_wday['wday']\n\n\nfor k in range(0,9):\n    for i in range(0,7):\n        unc_all_w[i,k] = np.percentile(allsales.query('wday == {}'.format(i+1)).allitem,q=per[k])\n\n","e89f1786":"\nfor j in range(0,9):\n    for i in range(0,4):\n        ss_array[j*42840,i*7+1]=unc_all_w[2,j]\n        ss_array[j*42840,i*7+2]=unc_all_w[3,j]\n        ss_array[j*42840,i*7+3]=unc_all_w[4,j]\n        ss_array[j*42840,i*7+4]=unc_all_w[5,j]\n        ss_array[j*42840,i*7+5]=unc_all_w[6,j]\n        ss_array[j*42840,i*7+6]=unc_all_w[0,j]\n        ss_array[j*42840,i*7+7]=unc_all_w[1,j]\n    \n","ddb06124":"#Calculate aggregation sales\n\ndef calculate_sales(series, select_id, drop_list, previous_end):\n   \n    unc_w=(np.zeros((7,9)))\n    \n    sales = pd.DataFrame(index=index,columns=[])\n    \n    sales = validation.groupby(select_id).sum() #select_id\n        \n    sales = sales.T\n    sales=sales.reset_index()\n    sales=sales.drop(drop_list,axis=0) #drop_list\n    sales=sales.reset_index(drop=True)\n    \n    sales.columns=[i for i in range(series+1)]\n    \n    \n    calendar_wday=calendar['wday'].iloc[1848:1913].reset_index()\n    sales['wday']=calendar_wday['wday']\n    \n    \n    \n    #Calculate wday=1 (Monday)\n    sales_1=sales.query('wday == [1]')\n    sales_1=sales_1.drop([0],axis=1)\n    sales_1_val=sales_1.values\n    \n    unc_w1=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w1[i,k] = np.percentile(sales_1_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+6]=unc_w1[l,r]       #previous_end  \n            \n    \n    #Calculate wday=2 (Tuesday)\n    sales_2=sales.query('wday == [2]')\n    sales_2=sales_2.drop([0],axis=1)\n    sales_2_val=sales_2.values\n    \n    unc_w2=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w2[i,k] = np.percentile(sales_2_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+7]=unc_w2[l,r]   #previous_end\n    \n    #Calculate wday=3 (Wednesday)\n    sales_3=sales.query('wday == [3]')\n    sales_3=sales_3.drop([0],axis=1)\n    sales_3_val=sales_3.values\n    \n    unc_w3=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w3[i,k] = np.percentile(sales_3_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+1]=unc_w3[l,r]       #previous_end   \n    \n                \n    #Calculate wday=4 (Thursday)\n    sales_4=sales.query('wday == [4]')\n    sales_4=sales_4.drop([0],axis=1)\n    sales_4_val=sales_4.values\n    \n    unc_w4=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w4[i,k] = np.percentile(sales_4_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+2]=unc_w4[l,r]  #previous_end\n                \n                \n    #Calculate wday=5 (Friday)\n    sales_5=sales.query('wday == [5]')\n    sales_5=sales_5.drop([0],axis=1)\n    sales_5_val=sales_5.values\n    \n    unc_w5=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w5[i,k] = np.percentile(sales_5_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+3]=unc_w5[l,r]  #previous_end\n                \n                \n    #Calculate wday=6 (Saturday)\n    sales_6=sales.query('wday == [6]')\n    sales_6=sales_6.drop([0],axis=1)\n    sales_6_val=sales_6.values\n    \n    unc_w6=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w6[i,k] = np.percentile(sales_6_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+4]=unc_w6[l,r]     #previous_end        \n    \n                \n    #Calculate wday=7 (Sunday)\n    sales_7=sales.query('wday == [7]')\n    sales_7=sales_7.drop([0],axis=1)\n    sales_7_val=sales_7.values\n    \n    unc_w7=np.zeros((series,9))\n    for k in range(0,9):\n        for i in range(0,series):\n            unc_w7[i,k] = np.percentile(sales_7_val[:,i],q=per[k])\n    \n    \n    for l in range(0,series):\n        for r in range(0,9):\n            for n in range(0,4):\n                ss_array[r*42840+l+previous_end,n*7+5]=unc_w7[l,r]  #previous_end\n            \n\n","64161e4a":"calculate_sales(3,['state_id'],[0,1,2,3,4],1)\ncalculate_sales(10,['store_id'],[0,1,2,3,4],4)\ncalculate_sales(3,['cat_id'],[0,1,2,3,4],14)\ncalculate_sales(7,['dept_id'],[0,1,2,3,4],17)\ncalculate_sales(9,['state_id','cat_id'],[0,1,2,3],24)\ncalculate_sales(21,['state_id','dept_id'],[0,1,2,3],33)\ncalculate_sales(30,['store_id','cat_id'],[0,1,2,3],54)\ncalculate_sales(70,['store_id','dept_id'],[0,1,2,3],84)\ncalculate_sales(3049,['item_id'],[0,1,2,3,4],154)\ncalculate_sales(9147,['item_id','state_id'],[0,1,2,3],3203)\ncalculate_sales(30490,['item_id','store_id'],[0,1,2,3],12350)","e145351f":"ss_array.shape","8fb26983":"ss_array[385560:771120,:]=ss_array[0:385560,:]\nss_array_df=pd.DataFrame(ss_array)\nss_array_df[0]=ss['id']\nss_array_df.columns=['id','F1','F2','F3','F4','F5','F6','F7','F8',\n                   'F9','F10','F11','F12','F13','F14','F15','F16',\n                   'F17','F18','F19','F20','F21','F22','F23','F24',\n                   'F25','F26','F27','F28']","4a06e713":"ss_array_df.to_csv(\"submission.csv\", index=False)","bded93c1":"And one more attention is that permutation of 'item, store' is not descending order.\n\nSo, when I use 'groupby', it become different order between result of 'groupby' and submission file's order. \n","c03109fa":"# Very simple idea.\n\n1. Calculate distribution by weekday from last 2 months.\n2. And calculate percentile 0.5%,2.5%,16.5%,25%,50%,75%,83.5%,97.5%,99.5%.\n\n# But complication is sample submission file's data structure.","589fe0e4":"# I will try not only percentile but Poisson, Gamma, Norm, etc."}}