{"cell_type":{"fd99c58a":"code","c30c4bd3":"code","e514aeb6":"code","02e7e49e":"code","44915735":"code","532391f8":"code","2b63ae99":"code","22257a51":"code","3cd2d253":"code","2c25ec05":"code","aa302c8f":"code","7fddd9dd":"code","ef2763f3":"markdown","a8b05240":"markdown","5d7a1c50":"markdown","9f03769c":"markdown","0785e981":"markdown","7dd1a6fe":"markdown","cdfb5979":"markdown"},"source":{"fd99c58a":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport gc\npd.set_option('display.max_columns', 500)\npd.set_option('display.max_rows', 500)","c30c4bd3":"def reduce_memory(df):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    return df","e514aeb6":"calendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ncalendar = reduce_memory(calendar)\nsales_train_validation = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\nsales_train_validation = reduce_memory(sales_train_validation)\nsubmission = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sample_submission.csv')","02e7e49e":"valid_rows = [row for row in submission['id'] if 'validation' in row]\nvalidation = submission.query(\"id in @valid_rows\")\nvalidation.columns = ['id', 'd_1914', 'd_1915', 'd_1916', 'd_1917', 'd_1918', 'd_1919', 'd_1920', 'd_1921', 'd_1922', 'd_1923', 'd_1924', 'd_1925', 'd_1926', 'd_1927', 'd_1928', 'd_1929', 'd_1930', 'd_1931', \n                      'd_1932', 'd_1933', 'd_1934', 'd_1935', 'd_1936', 'd_1937', 'd_1938', 'd_1939', 'd_1940', 'd_1941']\nproduct = sales_train_validation[['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id']].drop_duplicates()\nvalidation = validation.merge(product, how = 'left', on = 'id')\nvalidation = pd.melt(validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\nsales_train_validation = pd.melt(sales_train_validation, id_vars = ['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name = 'day', value_name = 'demand')\ndata = pd.concat([sales_train_validation, validation], axis = 0)\ncalendar.drop(['wm_yr_wk','weekday', 'wday', 'month', 'year','event_name_1','event_type_1','event_name_2','event_type_2','snap_CA','snap_TX','snap_WI'], inplace = True, axis = 1)\ndata = pd.merge(data, calendar, how = 'left', left_on = ['day'], right_on = ['d'])\ndel sales_train_validation, calendar\ngc.collect()\ndata.drop(['d', 'day'], inplace = True, axis = 1)\ndata['date'] = pd.to_datetime(data['date'])","44915735":"data_new = data[(data['date'] > '2016-02-28') & (data['date'] <= '2016-04-24')][['id','date','demand']]\ndata_old = data[(data['date'] <= '2016-02-28')][['id','date','demand']]\nvalidation = data[(data['date'] > '2016-04-24')][['id','date','demand']]","532391f8":"del data\ngc.collect()","2b63ae99":"number_of_product_new = data_new.groupby('id')['demand'].sum().reset_index()\nnumber_of_product_old = data_old.groupby('id')['demand'].sum().reset_index()\nnumber_of_product_new.columns = ['id','demand_new']\nnumber_of_product_old.columns = ['id','demand_old']\nnumber_of_product = pd.merge(number_of_product_old, number_of_product_new, how='inner', left_on=['id'], right_on=['id'])\nnumber_of_product['diff'] = number_of_product['demand_new'] - number_of_product['demand_old']\nnumber_of_product = number_of_product[(number_of_product['diff'] >= 0)]['id']","22257a51":"len(number_of_product)","3cd2d253":"data_new['weekday'] = data_new['date'].dt.weekday\ndata_predict_median = data_new.groupby(['id','weekday'])['demand'].median().to_dict()","2c25ec05":"def predict_median(x):\n    return data_predict_median[(x[0], x[1])]","aa302c8f":"validation['weekday'] = validation['date'].dt.weekday\nvalidation.query(\"id in @number_of_product\", inplace=True)\nvalidation['demand'] = validation[['id','weekday']].apply(lambda x: predict_median(x), axis=1)\nvalidation['demand'].fillna(0,inplace=True)\npredictions = validation[['id', 'date', 'demand']]\npredictions = pd.pivot(predictions, index = 'id', columns = 'date', values = 'demand').reset_index()\npredictions.columns = ['id'] + ['F' + str(i + 1) for i in range(28)]","7fddd9dd":"submission_lightGBM = pd.read_csv('\/kaggle\/input\/lightgbm\/submission_lightGBM.csv')\nsubmission = pd.concat([predictions, submission_lightGBM]).drop_duplicates(subset =\"id\", keep = 'first') \nsubmission.to_csv('submission.csv', index=False, header=True)","ef2763f3":"The score increases from 0.51029 to 0.50990","a8b05240":"Get the forecast for these products","5d7a1c50":"Select all products sold after March 2016. These products are new products.","9f03769c":"Get 47 products","0785e981":"\"submission_lightGBM\" - result of lightGBM.\nReplace the values with the median values","7dd1a6fe":"Select new products that had increasing demand","cdfb5979":"Data processing"}}