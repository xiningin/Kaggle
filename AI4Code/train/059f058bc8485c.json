{"cell_type":{"b7af335a":"code","160846cf":"code","144bd848":"code","dbd66cd4":"code","14454955":"code","6d3d6849":"code","1e54d4b2":"code","b5904cb8":"code","dd1d4ad6":"code","762c15c8":"code","1b15c31f":"code","a347aacd":"code","e083d439":"code","a24a47ba":"code","93176e79":"code","f8682d58":"code","11c5d6e4":"code","0e4d9149":"code","5418635e":"code","c4bcaf2c":"code","7faad0fb":"code","83c64b6d":"code","006982ff":"code","eaced5c5":"code","55d933ef":"code","631e42c8":"code","71bf57b2":"code","d1f3baea":"code","74d2fed1":"code","278939af":"code","bfb3dab1":"code","09f3258c":"code","b7c8dbe7":"code","9245c234":"code","f9e55863":"code","ef05552b":"code","3880d399":"code","663494da":"code","42dd5517":"code","ec0ecbd5":"code","af60a35d":"code","5f64dad9":"code","f57bca2b":"code","0546dc4a":"code","ad0eacda":"code","992efa0f":"code","278e93d7":"code","0dc06d37":"code","9afcc608":"code","1a1290a4":"code","5e1afb0c":"code","a24a55ed":"code","ffa0ad51":"code","747afc1b":"code","efadb8ab":"code","0787a2cf":"code","0c16a44f":"code","327e132c":"code","00c8daaf":"code","09df8a7e":"code","a4e7edf8":"code","89988b22":"code","6907e352":"code","9f427531":"code","9aedc45e":"code","334a154e":"code","9d236e18":"code","fa6337bb":"code","3c43b201":"code","4b4d1564":"code","098a4471":"code","1a6fd8eb":"code","eabd1a25":"code","52e15dcc":"code","1d6fc72a":"code","fabd6b70":"code","86dcf9a3":"code","8bae6af6":"code","e0ea057b":"code","8b534c04":"code","3e949f07":"code","ce5ad161":"code","829b0d54":"code","a4a37932":"code","a15aeb33":"code","448702c8":"code","f46bc308":"code","76cb161e":"code","5e97b1c6":"code","f5555958":"code","9b11b90b":"markdown","39a2e69f":"markdown","c1e7c40b":"markdown","c5dfc4d4":"markdown","bbb6dd8e":"markdown","d3de5f12":"markdown","c714e4c0":"markdown","087158de":"markdown","3bb940b2":"markdown","ff430ef3":"markdown","635bccf3":"markdown","dc97439a":"markdown","acdabe23":"markdown","7d198493":"markdown","d1e9a2b7":"markdown","2c64bd9c":"markdown","56457bc4":"markdown","f5dc75ff":"markdown","ceeb4647":"markdown","33c51e0f":"markdown","ccaf7f57":"markdown","34b81bea":"markdown","c226e888":"markdown","14f96d92":"markdown","188d5550":"markdown","3a3f562e":"markdown","a0ff6cbb":"markdown","82695525":"markdown","f701e3ec":"markdown","6950b5a3":"markdown","5cb5a559":"markdown","3272314b":"markdown","c6cec799":"markdown","1f429473":"markdown","445756db":"markdown","5cf05bd5":"markdown","29af6c0b":"markdown","a60ccfbc":"markdown","e585cdcf":"markdown","87fe4651":"markdown","23853b6a":"markdown"},"source":{"b7af335a":"!nvidia-smi","160846cf":"!nvcc --version","144bd848":"!pip install geopandas==0.8.1","dbd66cd4":"!pip install osmnx","14454955":"import multiprocessing as mp\nimport numpy as np\nimport networkx as nx\nimport osmnx as ox\nimport requests\nimport matplotlib\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n%matplotlib inline\nox.config(use_cache=True, log_console=True,timeout=1000)\nox.__version__","6d3d6849":"from datetime import datetime","1e54d4b2":"import sys\n!cp ..\/input\/rapids\/rapids.0.19.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","b5904cb8":"#import nvstrings\nimport numpy as np\nimport pandas as pd\nimport cudf, cuml\nimport dask_cudf\nimport io, requests\nimport math\nimport gc\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n#Learning\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom cuml.linear_model import LinearRegression\nfrom scipy.stats import uniform\n\nimport cuspatial\nimport cugraph\n\nfrom cuml.solvers import SGD as cumlSGD\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.ensemble import RandomForestRegressor as cuRF\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import ForestInference\nfrom cuml import Ridge\nfrom cuml import Lasso\nfrom cuml import ElasticNet\nfrom cuml.solvers import CD\nfrom cuml.svm import SVR\n\nimport xgboost as xgb\nfrom cuml.svm import SVC\n\nimport pandas as pd\n\n#import dask_ml.model_selection as dcv\n#from dask.distributed import Client, wait\n#from dask_cuda import LocalCUDACluster\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom cuml.metrics.regression import r2_score\nfrom cuml.metrics.regression import mean_squared_error\npd.__version__ #1.1.4","dd1d4ad6":"import rmm\nrmm.mr.set_current_device_resource(rmm.mr.ManagedMemoryResource())","762c15c8":"cudf.set_allocator(allocator=\"managed\")\n#cudf.set_allocator(pool=True)\n#cudf.set_allocator(\"managed\")","1b15c31f":"G = ox.graph_from_place('New York City, New York, USA',simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nfig, ax = ox.plot_graph(G,figsize=(50,50))","a347aacd":"print(nx.info(G))","e083d439":"#https:\/\/osmnx.readthedocs.io\/en\/stable\/osmnx.html#osmnx.distance.get_nearest_nodes\nX = [-73.961206,-73.972854]\nY = [40.721153,40.723742]\nids = ox.distance.get_nearest_nodes(G,X,Y)","a24a47ba":"def get_distance(A_lat,A_long,B_lat,B_long): #simplified distance function\n        long_dist = abs(A_long-B_long)\n        lat_dist = abs(A_lat-B_lat)\n        distance = math.sqrt(math.pow((111000*lat_dist),2)+math.pow((long_dist*111000*math.cos(A_lat)),2))\n        return distance\ndistance_between_pickup_and_dropoff = get_distance(40.721153, -73.961206,40.723742, -73.972854)\nprint(\"Distance between pickup and Dropoff = \",distance_between_pickup_and_dropoff)","93176e79":"#https:\/\/networkx.org\/documentation\/stable\/reference\/algorithms\/generated\/networkx.algorithms.shortest_paths.generic.shortest_path.html#networkx.algorithms.shortest_paths.generic.shortest_path\npath = nx.shortest_path(G,ids[0],ids[1],weight = 'length')\npath_length = nx.shortest_path_length(G,ids[0],ids[1],weight = 'length')\nprint(\"Shortest path length = \",path_length)\nox.plot_graph_route(G,path,figsize=(20,20), bbox=(40.76,40.70,-73.95,-73.99))\n","f8682d58":"print(\"Shortest Route is {} times longer than the haversine distance between pickup and dropoff!\".format(path_length\/distance_between_pickup_and_dropoff))","11c5d6e4":"G = ox.add_edge_speeds(G, precision = 3)\nG = ox.add_edge_travel_times(G,precision = 3)\n\nfastest_path = nx.shortest_path(G,ids[0],ids[1],weight = 'travel_time')\nfastest_path_time = nx.shortest_path_length(G,ids[0],ids[1],weight = 'travel_time')\nprint(\"Fastest path travel time = \",fastest_path_time)\nox.plot_graph_routes(G,[path,fastest_path],route_colors = ['r','g'],figsize=(20,20), bbox=(40.76,40.70,-73.95,-73.99))","0e4d9149":"types =      {'fare_amount': 'float32',\n              'pickup_datetime':'str',\n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'int8'}\nSpalten = list(types.keys())\ndf_train = cudf.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/train.csv', usecols=Spalten, dtype=types)#,nrows=10000000\ndf_test = cudf.read_csv('..\/input\/new-york-city-taxi-fare-prediction\/test.csv', usecols=Spalten, dtype=types)\n","5418635e":"df_train['pickup_datetime'] = df_train['pickup_datetime'].astype('datetime64[ns]')\ndf_test['pickup_datetime'] = df_test['pickup_datetime'].astype('datetime64[ns]')\n#Getting interger numbers from the pickup_datetime\ndf_train[\"hour\"] = df_train.pickup_datetime.dt.hour.astype('int8')\ndf_train[\"minute\"] = df_train.pickup_datetime.dt.minute.astype('int8')\ndf_train[\"weekday\"] = df_train.pickup_datetime.dt.weekday.astype('int8')\ndf_train[\"month\"] = df_train.pickup_datetime.dt.month.astype('int8')\ndf_train[\"year\"] = df_train.pickup_datetime.dt.year.astype('int16')\ndf_train[\"year\"] = df_train[\"year\"]-2000\ndf_train[\"year\"] = df_train[\"year\"].astype('int8')\n\ndf_train[\"day\"]=df_train.pickup_datetime.dt.day.astype('int8')\n\ndf_test[\"hour\"] = df_test.pickup_datetime.dt.hour.astype('int8')\ndf_test[\"minute\"]= df_test.pickup_datetime.dt.minute.astype('int8')\ndf_test[\"weekday\"] = df_test.pickup_datetime.dt.weekday.astype('int8')\ndf_test[\"month\"] = df_test.pickup_datetime.dt.month.astype('int8')\ndf_test[\"year\"] = df_test.pickup_datetime.dt.year.astype('int8')\ndf_test[\"year\"] = df_test[\"year\"]-2000\ndf_test[\"year\"] = df_test[\"year\"].astype('int8')\ndf_test[\"day\"]=df_test.pickup_datetime.dt.day.astype('int8')\n\ndf_train.drop(columns = ['pickup_datetime'])\ndf_test.drop(columns = ['pickup_datetime'])\ndf_train.head()","c4bcaf2c":"df_train.describe()","7faad0fb":"df_test.describe()","83c64b6d":"df_train.nans_to_nulls()\ndf_train= df_train.dropna()\nZeilen_vor_Bearbeitung=df_train.shape[0]\ndf_train.describe()","006982ff":"#Negative fareamount will be dropped\ndf_train = df_train[df_train['fare_amount'] > 0]\n#Some of the GPS Coordinates are fare away from New York, so we use a Bounding Box.\ndf_train = df_train[(df_train['pickup_longitude'] < -72) & (df_train['pickup_longitude'] > -74.3)]\ndf_train = df_train[(df_train['pickup_latitude'] > 40.5) & (df_train['pickup_latitude'] < 41.71)]\ndf_train = df_train[(df_train['dropoff_longitude'] < -72) & (df_train['dropoff_longitude'] > -74.3)]\ndf_train = df_train[(df_train['dropoff_latitude'] > 40.5) & (df_train['dropoff_latitude'] < 41.71)]\ndf_train = df_train[(df_train['passenger_count'] > 0) & (df_train['passenger_count'] < 7)] #Passengercount 0 or higher than 6 is not of interest.\nZeilen_nach_Bearbeitung=df_train.shape[0]\nZeilenverlust = Zeilen_vor_Bearbeitung-Zeilen_nach_Bearbeitung\nprint(\"Number of datasamples eliminated= {}\".format(Zeilenverlust))\ndf_train.describe()","eaced5c5":"def select_within_boundingbox(df, BB):\n    return (df.pickup_longitude >= BB[0]) & (df.pickup_longitude <= BB[1]) & \\\n           (df.pickup_latitude >= BB[2]) & (df.pickup_latitude <= BB[3]) & \\\n           (df.dropoff_longitude >= BB[0]) & (df.dropoff_longitude <= BB[1]) & \\\n           (df.dropoff_latitude >= BB[2]) & (df.dropoff_latitude <= BB[3])\ndef plot_hires(df, BB, figsize=(40, 40), ax=None, c=('r', 'b')):\n    if ax == None:\n        fig, ax = plt.subplots(1, 1, figsize=figsize)\n\n    idx = select_within_boundingbox(df, BB)\n    ax.scatter(df[idx].pickup_longitude, df[idx].pickup_latitude, c=c[0], s=0.01, alpha=0.5)\n    ax.scatter(df[idx].dropoff_longitude, df[idx].dropoff_latitude, c=c[1], s=0.01, alpha=0.5)","55d933ef":"df_train_pd = df_train.to_pandas()\nplot_hires(df_train_pd,(-74.3,-73,40.5,41.2))","631e42c8":"import os\n\n#file_path = '\/data\/Staten_Island.shp'\n#directory = os.path.dirname(file_path)\ndirectory = 'data'\ntry:\n    os.stat(directory)\nexcept:\n    os.mkdir(directory) ","71bf57b2":"Staten_Island = ox.geocode_to_gdf(['Staten Island, USA'])\nLong_Island   = ox.geocode_to_gdf(['Long Island, USA'])\nBrooklyn      = ox.geocode_to_gdf(['Brooklyn,New York, USA'])\nQueens        = ox.geocode_to_gdf(['Queens,New York, USA'])\nBronx         = ox.geocode_to_gdf(['Bronx,New York, USA',])\nManhattan     = ox.geocode_to_gdf(['Manhattan,New York, USA',])\n\nStaten_Island= Staten_Island.rename(columns={'display_name':'NAME'})\n#Staten_Island= Staten_Island[['NAME','geometry']]\nStaten_Island.to_file('data\/Staten_Island.shp')\n\nLong_Island= Long_Island.rename(columns={'display_name':'NAME'})\n#Long_Island= Long_Island[['NAME','geometry']]\nLong_Island.to_file('data\/Long_Island.shp')\n\nBrooklyn= Brooklyn.rename(columns={'display_name':'NAME'})\n#Brooklyn= Brooklyn[['NAME','geometry']]\nBrooklyn.to_file('data\/Brooklyn.shp')\n\nQueens= Queens.rename(columns={'display_name':'NAME'})\n#Queens= Queens[['NAME','geometry']]\nQueens.to_file('data\/Queens.shp')\n\nBronx= Bronx.rename(columns={'display_name':'NAME'})\n#Bronx= Bronx[['NAME','geometry']]\nBronx.to_file('data\/Bronx.shp')\n\nManhattan= Manhattan.rename(columns={'display_name':'NAME'})\n#Manhattan= Manhattan[['NAME','geometry']]\nManhattan.to_file('data\/Manhattan.shp')\n\ndel Staten_Island\ndel Long_Island\ndel Brooklyn\ndel Queens\ndel Bronx\ndel Manhattan\n\n#ox.plot.plot_footprints(Queens,figsize=(50,50))","d1f3baea":"import geopandas as gp \nNYC = gp.read_file('https:\/\/services5.arcgis.com\/GfwWNkhOj9bNBqoJ\/arcgis\/rest\/services\/NYC_Community_Districts\/FeatureServer\/0\/query?where=1%3D1&outFields=*&outSR=4326&f=geojson')\n#NYC = gp.read_file('nycd.shp')\nNYC = NYC.to_crs(\"EPSG:4326\")\nManhattan = NYC[NYC['BoroCD']<200]\ndel NYC\nox.plot.plot_footprints(Manhattan,figsize=(50,50))","74d2fed1":"Lower_Manhattan = Manhattan[Manhattan['BoroCD']<104]\nLower_Manhattan['NAME']='Lower Manhattan'\nLower_Manhattan = Lower_Manhattan.dissolve(by = 'NAME')\nLower_Manhattan.to_file('data\/Lower_Manhattan.shp')\n\nMidtown = Manhattan[Manhattan['BoroCD']<107]\nMidtown = Midtown[Midtown['BoroCD']>103]\nMidtown['NAME']='Midtown'\nMidtown = Midtown.dissolve(by = 'NAME')\nMidtown.to_file('data\/Midtown.shp')\n\nUpper_Westside = Manhattan[Manhattan['BoroCD']==107]\nUpper_Eastside = Manhattan[Manhattan['BoroCD']==108]\nUpper_Westside['NAME']='Upper Westside'\nUpper_Eastside['NAME']='Upper Eastside'\nUpper_Westside.to_file('data\/Upper_Westside.shp')\nUpper_Eastside.to_file('data\/Upper_Eastside.shp')\n\nUpper_Manhattan = Manhattan[Manhattan['BoroCD']<150]\nUpper_Manhattan = Upper_Manhattan[Upper_Manhattan['BoroCD']>108]\nUpper_Manhattan['NAME']='Upper Manhattan'\nUpper_Manhattan = Upper_Manhattan.dissolve(by = 'NAME')\nUpper_Manhattan.to_file('data\/Upper_Manhattan.shp')\n\nCentral_Park = Manhattan[Manhattan['BoroCD']>150]\nCentral_Park['NAME']='Central Park'\nCentral_Park.to_file('data\/Central_Park.shp')\n\nox.plot.plot_footprints(Lower_Manhattan,figsize=(50,50))\n\ndel Manhattan\ndel Lower_Manhattan\ndel Midtown\ndel Upper_Westside\ndel Upper_Eastside\ndel Upper_Manhattan\ndel Central_Park\n\n","278939af":"import geopandas as gp \nNYS = gp.read_file(\"https:\/\/gisservices.its.ny.gov\/arcgis\/rest\/services\/NYS_Civil_Boundaries\/FeatureServer\/3\/query?where=1%3D1&outFields=*&outSR=4326&f=geojson\")\n#NYS = gp.read_file('Counties_Shoreline.shp')\nNYS = NYS.to_crs(\"EPSG:4326\")\nNYS = NYS[NYS['NYSP_ZONE']!='Long Island']\nNYS = NYS[NYS['NYSP_ZONE']!='West']\nNYS = NYS[NYS['NYSP_ZONE']!='Central']\nNYS = NYS[NYS['ABBREV']!='STLA']\nNYS = NYS[NYS['ABBREV']!='CLIN']\nNYS = NYS[NYS['ABBREV']!='FRAN']\nNYS = NYS[NYS['ABBREV']!='ESSE']\nNYS = NYS[NYS['ABBREV']!='HAMI']\nNYS = NYS[NYS['ABBREV']!='WARR']\nNYS = NYS[NYS['ABBREV']!='HERK']\nNYS = NYS[NYS['ABBREV']!='WASH']\nNYS = NYS[NYS['ABBREV']!='SARA']\nNYS = NYS[NYS['ABBREV']!='SCHE']\nNYS = NYS[NYS['ABBREV']!='MONT']\nNYS = NYS[NYS['ABBREV']!='FULT']\nNYS = NYS[NYS['ABBREV']!='RENS']\nNYS = NYS[NYS['ABBREV']!='ALBA']\nNYS = NYS[NYS['ABBREV']!='SCHO']\nNYS = NYS[NYS['ABBREV']!='OTSE']\nNYS = NYS[NYS['ABBREV']!='DELA']\nNYS = NYS[NYS['ABBREV']!='GREE']\nNYS = NYS[NYS['ABBREV']!='COLU']\nNYS['dummy'] = 0\nNew_York_State = NYS.dissolve(by = 'dummy')\n#New_York_State = New_York_State[['NAME','geometry']]\nNew_York_State.to_file('data\/New_York_State.shp')\nox.plot.plot_footprints(New_York_State,figsize=(50,50))\ndel NYS\ndel New_York_State","bfb3dab1":"CT = gp.read_file(\"https:\/\/services1.arcgis.com\/FjPcSmEFuDYlIdKC\/arcgis\/rest\/services\/Connecticut_Towns_NoLabels\/FeatureServer\/1\/query?where=1%3D1&outFields=*&outSR=4326&f=geojson\")\n#CT = gp.read_file('Town_Polygon.shp')\nCT.head()","09f3258c":"CT = CT[CT['COAST_POLY']=='Inland Polygons']\nCT['dummy'] = 0\nConnecticut = CT.dissolve(by = 'dummy')\n#Connecticut = Connecticut[['TOWN','geometry']]\nConnecticut = Connecticut.rename(columns={'TOWN':'NAME'})\nConnecticut.to_file('data\/Connecticut.shp')\nox.plot.plot_footprints(Connecticut,figsize=(50,50))\ndel Connecticut\ndel CT","b7c8dbe7":"NJ = gp.read_file(\"https:\/\/maps.nj.gov\/arcgis\/rest\/services\/Framework\/Government_Boundaries\/MapServer\/2\/query?where=1%3D1&outFields=*&outSR=4326&f=geojson\")\n#NJ = gp.read_file('Municipal_Boundaries_of_NJ.shp')\nNJ = NJ.to_crs(\"EPSG:4326\")","9245c234":"NJ = NJ[NJ['COUNTY']!='SUSSEX']\nNJ = NJ[NJ['COUNTY']!='WARREN']\nNJ = NJ[NJ['COUNTY']!='HUNTERDON']\nNJ = NJ[NJ['COUNTY']!='MERCER']\nNJ = NJ[NJ['COUNTY']!='BURLINGTON']\nNJ = NJ[NJ['COUNTY']!='OCEAN']\nNJ = NJ[NJ['COUNTY']!='CAMDEN']\nNJ = NJ[NJ['COUNTY']!='GLOUCESTER']\nNJ = NJ[NJ['COUNTY']!='SALEM']\nNJ = NJ[NJ['COUNTY']!='CUMBERLAND']\nNJ = NJ[NJ['COUNTY']!='ATLANTIC']\nNJ = NJ[NJ['COUNTY']!='MORRIS']\nNJ = NJ[NJ['COUNTY']!='SOMERSET']\nNJ = NJ[NJ['COUNTY']!='CAPE MAY']\nNJ.head()","f9e55863":"NJ['dummy'] = 0\nNew_Jersey = NJ.dissolve(by = 'dummy')\n#New_Jersey = New_Jersey[['NAME','geometry']]\nNew_Jersey.to_file('data\/New_Jersey.shp')\nox.plot.plot_footprints(New_Jersey,figsize=(50,50))\ndel NJ\ndel New_Jersey","ef05552b":"from shapely.geometry import Point, Polygon","3880d399":"JFK_coords = [[40.649279, -73.795067],\n              [40.633968, -73.795423],\n              [40.632889, -73.777102],\n              [40.647104, -73.766159],\n              [40.654283, -73.782681]]\nLGA_coords = [[40.766340, -73.863005],\n              [40.768537, -73.865400],\n              [40.770438, -73.868098],\n              [40.771190, -73.869536],\n              [40.771580, -73.870689],\n              [40.771900, -73.872492],\n              [40.771961, -73.874069],\n              [40.771799, -73.875984],\n              [40.771155, -73.878300],\n              [40.769887, -73.880747],\n              [40.768099, -73.884373],\n              [40.768031, -73.885808],\n              [40.779945, -73.889108],\n              [40.787910, -73.868231],\n              [40.765435, -73.847899],\n              [40.764702, -73.859288]]\nEWR_coords = [[40.697, -73.185],\n              [40.687, -73.185],\n              [40.687, -73.175],\n              [40.697, -73.175]]\nRoosevelt_Island_coords = [[40.773330,-73.939915],\n                           [40.772429,-73.942333],\n                           [40.769805,-73.945004],\n                           [40.752010,-73.960918],\n                           [40.749331,-73.961929],\n                           [40.749386,-73.960846],\n                           [40.752229,-73.957634],\n                           [40.756630,-73.952871],\n                           [40.763668,-73.946713],\n                           [40.768803,-73.942099],\n                           [40.769461,-73.940919],\n                           [40.771671,-73.939417]\n                          ]\n\nJFK = Polygon(JFK_coords)\nLGA = Polygon(LGA_coords)\nEWR = Polygon(EWR_coords)\nRoosevelt = Polygon(Roosevelt_Island_coords)","663494da":"temp_pd = pd.DataFrame({'NAME':['JFK'],'geometry':[JFK]})\nJFK_geopd =  gp.GeoDataFrame(temp_pd)\nJFK_geopd.to_file('data\/JFK.shp')\n\ntemp_pd = pd.DataFrame({'NAME':['LGA'],'geometry':[LGA]})\nLGA_geopd =  gp.GeoDataFrame(temp_pd)\nLGA_geopd.to_file('data\/LGA.shp')\n\ntemp_pd = pd.DataFrame({'NAME':['EWR'],'geometry':[EWR]})\nEWR_geopd =  gp.GeoDataFrame(temp_pd)\nEWR_geopd.to_file('data\/EWR.shp')\n\ntemp_pd = pd.DataFrame({'NAME':['Roosevelt'],'geometry':[Roosevelt]})\nRoosevelt_geopd =  gp.GeoDataFrame(temp_pd)\nRoosevelt_geopd.to_file('data\/Roosevelt.shp')","42dd5517":"df_train['Pickup_Island']= 20\ndf_train['Dropoff_Island']= 20\ndf_test['Pickup_Island']= 20\ndf_test['Dropoff_Island']= 20\n\ndf_train['Pickup_Borough']= 20\ndf_train['Dropoff_Borough']= 20\ndf_test['Pickup_Borough']= 20\ndf_test['Dropoff_Borough']= 20\n\ndf_train['Pickup_Borough']=df_train['Pickup_Borough'].astype('int8')\ndf_train['Dropoff_Borough']=df_train['Dropoff_Borough'].astype('int8')\ndf_train['Pickup_Island'] = df_train['Pickup_Island'].astype('int8')\ndf_train['Dropoff_Island'] = df_train['Dropoff_Island'].astype('int8')\n\ndf_test['Pickup_Borough']=df_test['Pickup_Borough'].astype('int8')\ndf_test['Dropoff_Borough']=df_test['Dropoff_Borough'].astype('int8')\ndf_test['Pickup_Island'] = df_test['Pickup_Island'].astype('int8')\ndf_test['Dropoff_Island'] = df_test['Dropoff_Island'].astype('int8')","ec0ecbd5":"def getInclusion_shape(latitude,longitude,shape):\n  result = cuspatial.point_in_polygon(latitude,longitude,cudf.Series([0],index='resultcolumn'),shape[1],shape[2]['y'],shape[2]['x'])\n  return result","af60a35d":"\ndef check_Zones(shape,name):\n    result_eins = getInclusion_shape(df_train.pickup_latitude,df_train.pickup_longitude,shape)\n    result_zwei = getInclusion_shape(df_train.dropoff_latitude,df_train.dropoff_longitude,shape)\n    df_train.loc[result_eins['resultcolumn'],'Pickup_Borough']=name\n    df_train.loc[result_zwei['resultcolumn'],'Dropoff_Borough']=name\n    result_eins = getInclusion_shape(df_test.pickup_latitude,df_test.pickup_longitude,shape)\n    result_zwei = getInclusion_shape(df_test.dropoff_latitude,df_test.dropoff_longitude,shape)\n    df_test.loc[result_eins['resultcolumn'],'Pickup_Borough']=name\n    df_test.loc[result_zwei['resultcolumn'],'Dropoff_Borough']=name\n    return ","5f64dad9":"\ndef check_Islands(shape,name):\n    result_eins = getInclusion_shape(df_train.pickup_latitude,df_train.pickup_longitude,shape)\n    result_zwei = getInclusion_shape(df_train.dropoff_latitude,df_train.dropoff_longitude,shape)\n    df_train.loc[result_eins['resultcolumn'],'Pickup_Island']=name\n    df_train.loc[result_zwei['resultcolumn'],'Dropoff_Island']=name\n    result_eins = getInclusion_shape(df_test.pickup_latitude,df_test.pickup_longitude,shape)\n    result_zwei = getInclusion_shape(df_test.dropoff_latitude,df_test.dropoff_longitude,shape)\n    df_test.loc[result_eins['resultcolumn'],'Pickup_Island']=name\n    df_test.loc[result_zwei['resultcolumn'],'Dropoff_Island']=name\n    return ","f57bca2b":"NYS_cd = cuspatial.read_polygon_shapefile('data\/New_York_State.shp')\nCT_cd = cuspatial.read_polygon_shapefile('data\/Connecticut.shp')\nNJ_cd = cuspatial.read_polygon_shapefile('data\/New_Jersey.shp')\nLI_cd = cuspatial.read_polygon_shapefile('data\/Long_Island.shp')\nSI_cd = cuspatial.read_polygon_shapefile('data\/Staten_Island.shp')\nBK_cd = cuspatial.read_polygon_shapefile('data\/Brooklyn.shp')\nQ_cd = cuspatial.read_polygon_shapefile('data\/Queens.shp')\nBX_cd = cuspatial.read_polygon_shapefile('data\/Bronx.shp')\nM_cd = cuspatial.read_polygon_shapefile('data\/Manhattan.shp')\nLM_cd = cuspatial.read_polygon_shapefile('data\/Lower_Manhattan.shp')\nMM_cd = cuspatial.read_polygon_shapefile('data\/Midtown.shp')\nUE_cd = cuspatial.read_polygon_shapefile('data\/Upper_Eastside.shp')\nUW_cd = cuspatial.read_polygon_shapefile('data\/Upper_Westside.shp')\nUM_cd = cuspatial.read_polygon_shapefile('data\/Upper_Manhattan.shp')\nCP_cd = cuspatial.read_polygon_shapefile('data\/Central_Park.shp')\nJFK_cd = cuspatial.read_polygon_shapefile('data\/JFK.shp')\nLGA_cd = cuspatial.read_polygon_shapefile('data\/LGA.shp')\nEWR_cd = cuspatial.read_polygon_shapefile('data\/EWR.shp')\nRI_cd = cuspatial.read_polygon_shapefile('data\/Roosevelt.shp')","0546dc4a":"%%time\ncheck_Zones(CT_cd, 1)\ncheck_Zones(NYS_cd,2)\ncheck_Zones(NJ_cd, 3)\ncheck_Zones(LI_cd, 4)\ncheck_Zones(SI_cd, 5)\ncheck_Zones(EWR_cd,6)\ncheck_Zones(RI_cd, 7)\ncheck_Zones(BX_cd, 8)\ncheck_Zones(M_cd,  9)\ncheck_Zones(UM_cd, 10)\ncheck_Zones(CP_cd, 11)\ncheck_Zones(JFK_cd,12)\ncheck_Zones(LGA_cd,13)\ncheck_Zones(UE_cd, 14)\ncheck_Zones(UW_cd, 15)\ncheck_Zones(Q_cd,  16)\ncheck_Zones(BK_cd, 17)\ncheck_Zones(MM_cd, 18)\ncheck_Zones(LM_cd, 19)","ad0eacda":"%%time\ncheck_Islands(CT_cd, 1)\ncheck_Islands(NYS_cd,1)\ncheck_Islands(BX_cd, 1)\ncheck_Islands(NJ_cd, 2)\ncheck_Islands(LI_cd, 4)\ncheck_Islands(SI_cd, 3)\ncheck_Islands(M_cd,  5)","992efa0f":"del NYS_cd\ndel CT_cd\ndel NJ_cd\ndel LI_cd\ndel SI_cd\ndel BK_cd\ndel Q_cd \ndel BX_cd\ndel M_cd \ndel LM_cd\ndel MM_cd\ndel UE_cd\ndel UW_cd\ndel UM_cd\ndel CP_cd\ndel JFK_cd\ndel LGA_cd\ndel EWR_cd\ndel RI_cd","278e93d7":"df_train['Pickup_Borough']=df_train['Pickup_Borough'].astype('int8')\ndf_train['Dropoff_Borough']=df_train['Dropoff_Borough'].astype('int8')\ndf_train['Pickup_Island'] = df_train['Pickup_Island'].astype('int8')\ndf_train['Dropoff_Island'] = df_train['Dropoff_Island'].astype('int8')\n\ndf_test['Pickup_Borough']=df_test['Pickup_Borough'].astype('int8')\ndf_test['Dropoff_Borough']=df_test['Dropoff_Borough'].astype('int8')\ndf_test['Pickup_Island'] = df_test['Pickup_Island'].astype('int8')\ndf_test['Dropoff_Island'] = df_test['Dropoff_Island'].astype('int8')","0dc06d37":"shape_with_water = df_train.shape[0]\ndf_train = df_train[df_train['Pickup_Island']<20]\ndf_train = df_train[df_train['Dropoff_Island']<20]\nshape_without_water = df_train.shape[0]\ndf_train_pd = df_train.to_pandas()\nprint(\"Number of rides eleminated = {}\".format((shape_with_water-shape_without_water)))\n#plot_hires(df_train_pd,(-74.3,-73,40.5,41.2))","9afcc608":"#plot_hires(df_train_pd,(-74.256,-73.69,40.49,40.93))\n#plot_hires(df_train_pd,(-74.05,-73.9,40.69,40.9))\nplot_hires(df_train_pd,(-74.3,-73,40.5,41.2))","1a1290a4":"print(\"{} Rows get deleted, because the Ride started or ended in Water\".format((shape_with_water-shape_without_water)))","5e1afb0c":"def Manhattan_distance(lat1,lon1,lat2,lon2):\n  lat_dist = lat1-lat2\n  lon_dist = lon1-lon2\n  distance = 111000*abs(lat_dist)+abs(lon_dist*111000*np.cos(lat1))\n  return distance","a24a55ed":"%%time\ndf_train['Manhattan_distance']=Manhattan_distance(df_train['pickup_latitude'],df_train['pickup_longitude'],df_train['dropoff_latitude'],df_train['dropoff_longitude'])\ndf_test['Manhattan_distance']=Manhattan_distance(df_test['pickup_latitude'],df_test['pickup_longitude'],df_test['dropoff_latitude'],df_test['dropoff_longitude'])\ndf_train['Manhattan_distance']=df_train['Manhattan_distance'].astype('float32')\ndf_test['Manhattan_distance']=df_test['Manhattan_distance'].astype('float32')\ndf_train.head()","ffa0ad51":"count_array = np.array([[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0],[0,0,0,0,0]])\ncount_sum = 0\n#df_train_pd = df_train.to_pandas()\nfor i in range(0,5):\n    for j in range(0,5):\n        df_special = df_train[(df_train['Pickup_Island'] == (i+1)) & (df_train['Dropoff_Island'] == (1+j))]\n        count_array[i][j]=df_special.shape[0]\nprint(count_array)","747afc1b":"import os\n\n#file_path = '\/data\/Staten_Island.shp'\n#directory = os.path.dirname(file_path)\ndirectory = 'split_parts'\ntry:\n    os.stat(directory)\nexcept:\n    os.mkdir(directory) ","efadb8ab":"def split_Dataframe(df):\n    df_Manhattan_gesamt = df[True==((df['Pickup_Island'] == 5) & (df['Dropoff_Island'] == 5))]\n    df_nicht_Manhattan = df[False==((df['Pickup_Island'] == 5) & (df['Dropoff_Island'] == 5))]\n    \n    df_Roosevelt_eins   = df_Manhattan_gesamt[True==((df_Manhattan_gesamt['Pickup_Borough'] == 7))]\n    #df_Roosevelt_eins.to_csv('Roosevelt_eins.csv',index=False,chunksize=1000000)\n    df_Manhattan_gesamt = df_Manhattan_gesamt[False==((df_Manhattan_gesamt['Pickup_Borough'] == 7))]\n    df_Roosevelt_zwei   = df_Manhattan_gesamt[True==((df_Manhattan_gesamt['Dropoff_Borough'] == 7))]\n    #df_Roosevelt_zwei.to_csv('Roosevelt_zwei.csv',index=False,chunksize=1000000)\n    df_Manhattan_gesamt = df_Manhattan_gesamt[False==((df_Manhattan_gesamt['Dropoff_Borough'] == 7))]\n    print(\"Number of Rides that stay within Manhattan: {}\".format(df_Manhattan_gesamt.shape[0]))\n    \n    df_Manhattan_eins = df_Manhattan_gesamt[0:16000000]\n    df_Manhattan_zwei = df_Manhattan_gesamt[16000000:32000000]\n    df_Manhattan_drei = df_Manhattan_gesamt[32000000:]\n    del df_Manhattan_gesamt\n    \n    df_Manhattan_eins.to_csv('split_parts\/Manhattan_eins.csv',index=False,chunksize=1000000)\n    df_Manhattan_zwei.to_csv('split_parts\/Manhattan_zwei.csv',index=False,chunksize=1000000)\n    df_Manhattan_drei.to_csv('split_parts\/Manhattan_drei.csv',index=False,chunksize=1000000)\n    #df_Manhattan_vier.to_csv('split_parts\/Manhattan_vier.csv',index=False,chunksize=1000000)\n    #df_Manhattan_f\u00fcnf.to_csv('split_parts\/Manhattan_f\u00fcnf.csv',index=False,chunksize=1000000)\n    \n    \n    del df_Manhattan_eins\n    del df_Manhattan_zwei\n    del df_Manhattan_drei\n    #del df_Manhattan_vier\n    #del df_Manhattan_f\u00fcnf\n    \n    \n    df_Queens = df[(df['Pickup_Borough']==16)&(df['Dropoff_Borough']==16)]\n    print(\"Number of Rides that stay within Queens: {}\".format(df_Queens.shape[0]))\n    df_Brooklyn = df[(df['Pickup_Borough']==17)&(df['Dropoff_Borough']==17)]\n    print(\"Number of Rides that stay within Brooklyn: {}\".format(df_Brooklyn.shape[0]))\n    df_Brooklyn_Queens = df[(df['Pickup_Borough']==17)&(df['Dropoff_Borough']==16)]\n    df_Queens_Brooklyn = df[(df['Pickup_Borough']==16)&(df['Dropoff_Borough']==17)]\n    df_Queens_Brooklyn_gesamt = df_Queens\n    df_Queens_Brooklyn_gesamt = df_Queens_Brooklyn_gesamt.append(df_Brooklyn_Queens)\n    df_Queens_Brooklyn_gesamt = df_Queens_Brooklyn_gesamt.append(df_Queens_Brooklyn)\n    print(\"Number of Rides between Brooklyn and Queens: {}\".format(df_Queens_Brooklyn_gesamt.shape[0]))\n    \n    df_Queens.to_csv('split_parts\/Queens.csv',index=False,chunksize=1000000)\n    df_Brooklyn.to_csv('split_parts\/Brooklyn.csv',index=False,chunksize=1000000)\n    df_Brooklyn_Queens.to_csv('split_parts\/Brooklyn_Queens.csv',index=False,chunksize=1000000)\n    df_Queens_Brooklyn.to_csv('split_parts\/Queens_Brooklyn.csv',index=False,chunksize=1000000)\n    df_Queens_Brooklyn_gesamt.to_csv('split_parts\/Queens_Brooklyn_total.csv',index=False,chunksize=1000000)\n    \n    del df_Queens\n    del df_Brooklyn\n    del df_Brooklyn_Queens\n    del df_Queens_Brooklyn\n    del df_Queens_Brooklyn_gesamt\n    \n    df_Long_Island_rest = df[(df['Pickup_Island'] == 4) & (df['Dropoff_Island'] == 4)]\n    df_Long_Island_rest = df_Long_Island_rest[False==((df_Long_Island_rest['Pickup_Borough'] == 16) & (df_Long_Island_rest['Dropoff_Borough'] == 16))]\n    df_Long_Island_rest = df_Long_Island_rest[False==((df_Long_Island_rest['Pickup_Borough'] == 16) & (df_Long_Island_rest['Dropoff_Borough'] == 17))]\n    df_Long_Island_rest = df_Long_Island_rest[False==((df_Long_Island_rest['Pickup_Borough'] == 17) & (df_Long_Island_rest['Dropoff_Borough'] == 16))]\n    df_Long_Island_rest = df_Long_Island_rest[False==((df_Long_Island_rest['Pickup_Borough'] == 17) & (df_Long_Island_rest['Dropoff_Borough'] == 17))]\n    \n    df_Long_Island_rest.to_csv('split_parts\/Long_Island.csv',index=False,chunksize=1000000)\n    print(\"Number of Rides that stay within the rest of Long Island: {}\".format(df_Long_Island_rest.shape[0]))\n    \n    #df_Long_Island_gesamt_eins = df_Long_Island_gesamt[((df_Long_Island_gesamt['Pickup_Borough']==2)|(df_Long_Island_gesamt['Dropoff_Borough']==2)==False)]\n    df_Staten_Island_gesamt = df[(df['Pickup_Island'] == 3) & (df['Dropoff_Island'] == 3)]\n    df_Staten_Island_gesamt.to_csv('split_parts\/Staten_Island.csv',index=False,chunksize=1000000)\n    print(\"Number of Rides that stay within Staten Island: {}\".format(df_Staten_Island_gesamt.shape[0]))\n    del df_Long_Island_rest\n    del df_Staten_Island_gesamt\n    \n    df_rest = df_nicht_Manhattan[False==((df_nicht_Manhattan['Pickup_Island'] == 4) & (df_nicht_Manhattan['Dropoff_Island'] == 4))]\n    del df_nicht_Manhattan\n    df_rest = df_rest[False==((df_rest['Pickup_Island'] == 3) & (df_rest['Dropoff_Island'] == 3))]\n    \n    df_rest = df_rest.append(df_Roosevelt_eins)\n    df_rest = df_rest.append(df_Roosevelt_zwei)\n    rest_shape_before_split = df_rest.shape[0]\n    print(\"Number of Rides not included so far: {}\".format(df_rest.shape[0]))\n    df_spezial = df_rest[True==((df_rest['pickup_longitude'] <-73.69 ) & (df_rest['pickup_longitude'] > -74.256 )&(df_rest['pickup_latitude'] > 40.49) & (df_rest['pickup_latitude'] < 40.93 )&(df_rest['dropoff_longitude'] <-73.69 ) & (df_rest['dropoff_longitude'] > -74.256 )&(df_rest['dropoff_latitude'] > 40.49 ) & (df_rest['dropoff_latitude'] < 40.93 ))]\n    df_most_spezial = df_spezial[True==((df_spezial['Pickup_Borough'] >8) & (df_spezial['Dropoff_Borough'] >8))]\n    df_spezial = df_spezial[False==((df_spezial['Pickup_Borough'] >8) & (df_spezial['Dropoff_Borough'] >8))]\n    df_rest =   df_rest[False==((df_rest['pickup_longitude'] <-73.69 ) & (df_rest['pickup_longitude'] > -74.256 )&(df_rest['pickup_latitude'] > 40.49) & (df_rest['pickup_latitude'] < 40.93 )&(df_rest['dropoff_longitude'] <-73.69 ) & (df_rest['dropoff_longitude'] > -74.256 )&(df_rest['dropoff_latitude'] > 40.49 ) & (df_rest['dropoff_latitude'] < 40.93 ))]\n\n    \n    df_most_spezial.to_csv('split_parts\/most_spezial.csv',index=False,chunksize=1000000)\n    df_spezial.to_csv('split_parts\/spezial.csv',index=False,chunksize=1000000)\n    df_rest.to_csv('split_parts\/rest.csv',index=False,chunksize=1000000)\n\n    \n    \n    \n    del df_most_spezial\n    del df_spezial\n    del df_rest\n    \n    df_List_names = [\"Manhattan_eins\",\"Manhattan_zwei\",\"Manhattan_drei\",#\"Manhattan_vier\",\"Manhattan_f\u00fcnf\",\n                    \"Brooklyn\",\n                    \"Queens_Brooklyn_total\",\n                    \"Long_Island\",\n                    \"Staten_Island\",\n                    \"most_spezial\",\n                    \"spezial\",\n                    \"rest\",]\n    \n    return df_List_names\n","0787a2cf":"%%time\nsplitted_list = split_Dataframe(df_train)","0c16a44f":"G = ox.graph_from_place('Manhattan, New York, USA',simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') #4503 Nodes, 9668 Edges\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","327e132c":"G = ox.graph_from_place(['Brooklyn, New York, USA'],simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","00c8daaf":"G = ox.graph_from_bbox(40.885,40.49,-74.05,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","09df8a7e":"G = ox.graph_from_place(['Long Island, USA'],simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","a4e7edf8":"G = ox.graph_from_place('Staten Island, New York, USA',simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","89988b22":"G = ox.graph_from_bbox(40.885,40.49,-74.05,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","6907e352":"G = ox.graph_from_bbox(40.93,40.49,-74.256,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","9f427531":"G = ox.graph_from_bbox(41.71,40.48,-74.352,-72,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') \nprint(nx.info(G))\nfig, ax = ox.plot_graph(G,figsize=(50,50))","9aedc45e":"df_train.to_csv('df_train_before_route_calculation.csv',index=False,chunksize=1000000)\n#print(df_train.memory_usage())\ndf_train.shape[0]","334a154e":"df_train = None","9d236e18":"del df_train","fa6337bb":"!nvidia-smi","3c43b201":"!nvidia-smi","4b4d1564":"!pip install osmnx","098a4471":"#import nvstrings\nimport numpy as np\nimport pandas as pd\nimport cudf, cuml\nimport dask_cudf\nimport io, requests\nimport math\nimport gc\n\n#Plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns \n\n#Learning\nfrom cuml.preprocessing.model_selection import train_test_split\nfrom cuml.linear_model import LinearRegression\nfrom scipy.stats import uniform\n\nimport cuspatial\nimport cugraph\n\nfrom cuml.solvers import SGD as cumlSGD\nfrom cuml.linear_model import LogisticRegression\nfrom cuml.ensemble import RandomForestRegressor as cuRF\nfrom cuml.neighbors import KNeighborsRegressor\nfrom cuml import ForestInference\nfrom cuml import Ridge\nfrom cuml import Lasso\nfrom cuml import ElasticNet\nfrom cuml.solvers import CD\nfrom cuml.svm import SVR\n\nimport xgboost as xgb\nfrom cuml.svm import SVC\n\nimport pandas as pd\n\n#import dask_ml.model_selection as dcv\n#from dask.distributed import Client, wait\n#from dask_cuda import LocalCUDACluster\n\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import accuracy_score\nfrom cuml.metrics.regression import r2_score\nfrom cuml.metrics.regression import mean_squared_error\npd.__version__ #1.1.4","1a6fd8eb":"import rmm\nrmm.mr.set_current_device_resource(rmm.mr.ManagedMemoryResource())","eabd1a25":"cudf.set_allocator(allocator=\"managed\")","52e15dcc":"import multiprocessing as mp\nimport numpy as np\nimport networkx as nx\nimport osmnx as ox\nimport requests\nimport matplotlib\nimport matplotlib.cm as cm\nimport matplotlib.colors as colors\n%matplotlib inline\nox.config(use_cache=True, log_console=True,timeout=1000)\nox.__version__","1d6fc72a":"types =      {'fare_amount': 'float32',\n              'pickup_datetime':'str',\n              'pickup_longitude': 'float32',\n              'pickup_latitude': 'float32',\n              'dropoff_longitude': 'float32',\n              'dropoff_latitude': 'float32',\n              'passenger_count': 'int8'}\ntypes_nach_bearbeitung =     {'fare_amount': 'float32',\n                              'pickup_longitude': 'float32',\n                              'pickup_latitude': 'float32',\n                              'dropoff_longitude': 'float32',\n                              'dropoff_latitude': 'float32',\n                              'passenger_count': 'int8',\n                              'hour': 'int8',\n                              'minute': 'int8',\n                              'weekday': 'int8',\n                              'month': 'int8',\n                              'year': 'int8',\n                              'day': 'int8',\n                              'Pickup_Island': 'int8',\n                              'Dropoff_Island': 'int8',\n                              'Pickup_Borough': 'int8',\n                              'Dropoff_Borough': 'int8',\n                              'Manhattan_distance': 'float32',\n                             }\nSpalten = list(types.keys())\nSpalten_nach_bearbeitung = list(types_nach_bearbeitung.keys())\n#df_train = cudf.read_csv('df_train_before_route_calculation.csv', usecols=Spalten, dtype=types_nach_bearbeitung)#,nrows=10000000\n#teadf_test = cudf.read_csv('test.csv', usecols=Spalten, dtype=types)","fabd6b70":"splitted_list = [\"Manhattan_eins\",\"Manhattan_zwei\",\"Manhattan_drei\",#\"Manhattan_vier\",\"Manhattan_f\u00fcnf\",\n                    \"Brooklyn\",\n                    \"Queens_Brooklyn_total\",\n                    \"Long_Island\",\n                    \"Staten_Island\",\n                    \"most_spezial\",\n                    \"spezial\",\n                    \"rest\",]","86dcf9a3":"def create_cu_Graph(Graph):\n    pd_edge = nx.to_pandas_edgelist(Graph) # generating a pandas dataframe containing all edgeinformations based on the given Graph\n    pd_edge_zwei = pd_edge[[\"source\",\"target\",\"length\"]] # simplification of the dataframe\n    cd_edge = cudf.from_pandas(pd_edge_zwei)\n    coords = np.array([[node, data['x'], data['y']] for node, data in Graph.nodes(data=True)]) #extracting nodenumber and coordinates of each node out of the Graph\n    node_frame = pd.DataFrame(data=coords,columns=['Node','longitude','latitude'])\n    cd_node = cudf.from_pandas(node_frame)\n    \n    G_drei = cugraph.Graph() # building a cugraph Graph from the edgelist dataframe\n    G_drei.from_cudf_edgelist(cd_edge,source='source', destination='target',\n                             edge_attr='length', renumber=True)\n    \n    # This part currently not in use is intended to reduce the overhead generated due to the renumbering process which itself causes a mapping of external to internal vertex ids when using sssp()\n    # While this can reduce the runtime by 20% to 30%, I strongly advice not to use that code unless you are sure you understand how it works.\n    \"\"\"\n    cd_node_renumbered = G_drei.add_internal_vertex_id(cd_node,internal_column_name = 'NewNode',external_column_name = 'Node',drop = False)\n    cd_node_renumbered = cd_node_renumbered.drop(columns = ['longitude'])\n    cd_node_renumbered = cd_node_renumbered.drop(columns = ['latitude'])\n    \n    cd_node = cudf.from_pandas(node_frame)\n    cd_node_new = cd_node.merge(cd_node_renumbered,how = 'left', on = 'Node')\n    cd_node_new = cd_node_new.drop(columns = ['Node'])\n    cd_node_new = cd_node_new.rename(columns={'NewNode':'Node'})\n    \n    \n    cd_node_renumbered = cd_node_renumbered.rename(columns={'Node':'source'})\n    cd_edge_new = cd_edge.merge(cd_node_renumbered,how = 'left', on = 'source')\n    cd_edge_new = cd_edge_new.rename(columns={'NewNode':'new source'})\n    \n    cd_node_renumbered = cd_node_renumbered.rename(columns={'source':'target'})\n    cd_edge = cd_edge_new.merge(cd_node_renumbered,how = 'left', on = 'target')\n    cd_edge = cd_edge.rename(columns={'NewNode':'new target'})\n    \n    G_zwei = cugraph.Graph() # building a cugraph Graph from the edgelist dataframe\n    G_zwei.from_cudf_edgelist(cd_edge,source='new source', destination='new target',\n                             edge_attr='length', renumber=False)\n    \n    del cd_edge\n    del cd_edge_new\n    del pd_edge\n    del pd_edge_zwei\n    del coords\n    del node_frame\n    del cd_node_renumbered\n    del cd_node\n    del G_drei\n    return G_zwei, cd_node_new\"\"\"\n    del pd_edge\n    del pd_edge_zwei\n    del coords\n    del node_frame\n    del cd_edge\n    return G_drei, cd_node","8bae6af6":"import numba\nimport math\nfrom numba import cuda\n@cuda.jit\ndef calc_nearest_point(long,lat,node,distance,lenght,node_number,node_long,node_lat,node_length):\n    #node = Number of next node, \n    #distance = distance to next node, \n    #length = length of dataframes containing all points, \n    #node_length = length of dataframe containing all nodes\n    def get_distance(current_long,current_lat,current_node_long,current_node_lat): #simplified distance function\n        long_dist = abs(current_long-current_node_long)\n        lat_dist = abs(current_lat-current_node_lat)\n        distance = math.sqrt(math.pow((111000*lat_dist),2)+math.pow((long_dist*111000*math.cos(current_lat)),2))\n        return distance\n    def haversine_distance(current_long,current_lat,current_node_long,current_node_lat): #funktion to calculate the haversine distance between a node and a point.\n        R = 6372800 # this is in miles.  For Earth radius in kilometers use 6372.8 km\n        long_dist = abs(current_long-current_node_long)\n        lat_dist = abs(current_lat-current_node_lat)\n        dLat = (3.14159265359\/180) * lat_dist\n        dLon = (3.14159265359\/180) * long_dist\n        lat1 = (3.14159265359\/180) * current_lat\n        lat2 = (3.14159265359\/180) * current_node_lat\n        a = math.sin(dLat\/2)*math.sin(dLat\/2) + math.cos(lat1)*math.cos(lat2)*math.sin(dLon\/2)*math.sin(dLon\/2)\n        c = 2*math.atan2(math.sqrt(a), math.sqrt(1-a))\n        return R * c\n    i = cuda.grid(1)\n    if i <lenght:\n        point_long=long[i]\n        point_lat =lat[i]\n        min_dist = 10000000\n        node_numb=-1\n        for j in range(0,node_length):\n            dist_to_node = get_distance(point_long,point_lat,node_long[j],node_lat[j])\n            if(dist_to_node<min_dist):\n                min_dist = dist_to_node\n                node_numb=node_number[j]\n        node[i]=node_numb\n        distance[i]=min_dist","e0ea057b":"def get_nodes_for_frame(df,Graph,cd_node): \n    # df = Dataframe containing pickup and dropoff coordinates\n    # graph = Graph from osmnx with the streetnetwork\n    node_numb_arr=cd_node['Node'].to_gpu_array()\n    node_long_arr=cd_node['longitude'].to_gpu_array()\n    node_lat_arr=cd_node['latitude'].to_gpu_array()\n    node_length=cd_node.shape[0]\n    pickup_long_arr = df['pickup_longitude'].to_gpu_array()\n    pickup_lat_arr  = df['pickup_latitude'].to_gpu_array()\n    drop_long_arr   = df['dropoff_longitude'].to_gpu_array()\n    drop_lat_arr    = df['dropoff_latitude'].to_gpu_array()\n    df['pickup_node']=-1\n    df['dropoff_node']=-1\n    df['distance_to_pickup']=-1.0\n    df['distance_to_dropoff']=-1.0\n    pickup_node_arr = df['pickup_node'].to_gpu_array()\n    dropoff_node_arr = df['dropoff_node'].to_gpu_array()\n    pickup_dist_arr = df['distance_to_pickup'].to_gpu_array()\n    dropoff_dist_arr = df['distance_to_dropoff'].to_gpu_array()\n    df_length = df.shape[0]\n    blockspergrid = (df_length+(128-1)) #defining blocks per grid and threads per block, further information under https:\/\/numba.readthedocs.io\/en\/stable\/cuda\/kernels.html\n    calc_nearest_point[blockspergrid,128](pickup_long_arr,pickup_lat_arr,pickup_node_arr,pickup_dist_arr,df_length,node_numb_arr,node_long_arr,node_lat_arr,node_length)\n    cuda.synchronize()\n    calc_nearest_point[blockspergrid,128](drop_long_arr,drop_lat_arr,dropoff_node_arr,dropoff_dist_arr,df_length,node_numb_arr,node_long_arr,node_lat_arr,node_length)\n    cuda.synchronize()\n    df['pickup_node']=pickup_node_arr\n    df['dropoff_node']=dropoff_node_arr\n    df['distance_to_pickup']=pickup_dist_arr\n    df['distance_to_dropoff']=dropoff_dist_arr\n    del pickup_node_arr\n    del dropoff_node_arr\n    del pickup_dist_arr\n    del dropoff_dist_arr\n    del pickup_long_arr\n    del pickup_lat_arr\n    del drop_long_arr\n    del drop_lat_arr\n    del df_length\n    \n    return df","8b534c04":"@cuda.jit\ndef kernelfunktionzehn(cal_src,source, destination,cost, cudf_length, vertex,weight,nodes):\n    # calc_src =  current Source for which the Single Source Shortest Path calculation was made\n    # source = pickup node column\n    # destination = dropoff node column \n    # cost = edgecost column that we want to compute\n    # cudf_length = length of the dataframe containing each taxi ride, for which we want to compute the routes.\n    # vertex = destination for which we computed the shortest route length\n    # weight = computed cost for the shortest route to the corresponding vertex\n    # nodes = number of nodes, for which the shortest route was computed.\n  i = cuda.grid(1)\n  if i <cudf_length:\n    if (source[i] == cal_src):\n      for j in range(0,nodes):\n        if (vertex[j]==destination[i]):\n          cost[i]=weight[j]","3e949f07":"\ngpu = cuda.get_current_device()\ndef group_and_find_cost_basic(df,Graph,G_zwei):\n  # df = Dataframe containing pickup and dropoff coordinates\n  # graph = Graph from osmnx with the streetnetwork  \n    \n    #print(\"Build graph\")\n    \n    df['shortest_path_length'] = -1\n    df['shortest_path_length'] = df['shortest_path_length'].astype('float32')\n    #value_series_pd = df['pickup_node'].to_pandas()\n    #unique_values = value_series_pd.unique()\n    unique_values = df['pickup_node'].unique().to_pandas()\n    #print(\"Transferring series as array to gpu\")\n    cost_arr    = df['shortest_path_length'].to_gpu_array()\n    src_arr     = df['pickup_node'].to_gpu_array()\n    dst_arr     = df['dropoff_node'].to_gpu_array()\n    group_length = df.shape[0]\n    #counter = len(unique_values)\n    #print(\"Before for loop\")\n    for name in unique_values: #iterating over all unique pickup nodes\n        distances = cugraph.sssp(G_zwei,name) # Single Source Shortest Path Calculation for one source node\n        #distances_zwei= cugraph.filter_unreachable(distances) #Filter that can be helpful, but isn't used in this case\n        blockspergrid = (group_length+(128-1)) #defining blocks per grid and threads per block, further information under https:\/\/numba.readthedocs.io\/en\/stable\/cuda\/kernels.html\n        vertex_arr    = distances['vertex'].to_gpu_array()\n        distance_arr  = distances['distance'].to_gpu_array() \n        nodes_length  = distances.shape[0]\n        #cost_arr    = df['fastest_path_time'].to_gpu_array()\n        #sec_cost_arr    = df['fastest_path_length'].to_gpu_array()\n        #src_arr     = df['pickup_node'].to_gpu_array()\n        #dst_arr     = df['fastest_path_length'].to_gpu_array()\n        kernelfunktionzehn[blockspergrid,128](name,src_arr,dst_arr,cost_arr,group_length,\n                                         vertex_arr,distance_arr, nodes_length) # Kernelfunction to combine the dataframe containing the routes and costs with the dataframe containing the pickup and dropoff nodes.\n        #cost_arr.to_host()\n        #src_arr.to_host()\n        #dst_arr.to_host()\n        #df['fastest_path_time'] = cost_arr\n        #df['fastest_path_length'] = sec_cost_arr\n        cuda.synchronize()  # important to counter any possible race condition or memory inconsistency due to parallelization\n        #counter = counter - 1\n        #print(\"{} Durchl\u00e4ufe verbleibend\".format(counter))\n        \n        del distances\n    df['shortest_path_length'] = cost_arr\n    #del G_zwei\n    #del cd_edge\n    #del pd_edge\n    #del cd_edge\n    return df","ce5ad161":"def calc_routes_and_times_zwei(df,Graph,cd_Graph,cd_node):\n    start_time = time.time()\n    #print(\"Nodebestimmung gestartet\")\n    df_result_eins = get_nodes_for_frame(df,Graph,cd_node)\n    #print(\"df_result_eins typ = {}\".format(type(df_result_eins)))\n    #print(\"Nodes bestimmt\")\n    end_time = time.time()\n    duration = end_time-start_time\n    print(\"Runtime Nearest Node Search: {} seconds\".format(duration))\n    \n    #print(\"Spalten wurden eingef\u00fchrt\")\n    df_result_eins = df_result_eins.sort_values('pickup_node')\n    start_time = time.time()\n    df_result_eins = group_and_find_cost_basic(df_result_eins,Graph,cd_Graph)\n    print(\"shortest path found\")\n    \n    end_time = time.time()\n    duration = end_time-start_time\n    print(\"runtime  of shortest path calculation: {} seconds\".format(duration))\n    \n    df_result_eins['shortest_path_length']=df_result_eins['shortest_path_length']+df_result_eins['distance_to_pickup']+df_result_eins['distance_to_dropoff']\n    df_result_eins['shortest_path_time']=0\n    df_result_eins = df_result_eins.drop(columns = ['pickup_node'])\n    df_result_eins = df_result_eins.drop(columns = ['dropoff_node'])\n    df_result_eins = df_result_eins.drop(columns = ['distance_to_pickup'])\n    df_result_eins = df_result_eins.drop(columns = ['distance_to_dropoff'])\n    df_result_eins = df_result_eins.drop(columns = ['shortest_path_time'])\n    return df_result_eins","829b0d54":"def load_part(Filename):\n    start_time = time.time()\n    df = cudf.read_csv(('split_parts\/'+Filename+'.csv'), usecols=Spalten_nach_bearbeitung, dtype=types_nach_bearbeitung)#,nrows=10000000\n    end_time = time.time()\n    duration = end_time-start_time\n    print(\"Loadtime DataFrame: {}\".format(duration))\n    \n    return df","a4a37932":"import time\ndef calculate_splitpart(df_List):\n    result_list = []\n    df_result_concat=None\n    for i in range(0,len(df_List)):\n        print(\"======================================================================\")\n        G=None\n        start_time = time.time()\n        if(i<3):\n            print(\"Manhattan Part {} started\".format(i+1))\n            G = ox.graph_from_place('Manhattan, New York, USA',simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') \n        if(i==3):\n            print(\"Brooklyn Part started\")\n            G = ox.graph_from_place(['Brooklyn, New York, USA'],simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') \n        if(i==4):\n            print(\"Brooklyn-Queens Part started\")\n            G = ox.graph_from_bbox(40.885,40.49,-74.05,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\n        if(i==5):\n            print(\"Long Island Part started\")\n            G = ox.graph_from_place(['Long Island, USA'],simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') \n        if(i==6):\n            print(\"Staten Island Part started\")\n            G = ox.graph_from_place('Staten Island, New York, USA',simplify=True,truncate_by_edge=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\n        if(i==7):\n            print(\"Between Islands and Most Inner Part started\")\n            G = ox.graph_from_bbox(40.885,40.49,-74.05,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\n        if(i==8):\n            print(\"Between Islands and Inner Part started\")\n            G = ox.graph_from_bbox(40.93,40.49,-74.256,-73.69,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|residential|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]')\n        if(i==9):\n            print(\"Between Islands and Outer Part started\")\n            G = ox.graph_from_bbox(41.71,40.48,-74.352,-72,simplify=True,custom_filter='[\"area\"!~\"yes\"][\"highway\"~\"motorway|trunk|primary|secondary|tertiary|motorway_link|trunk_link|primary_link|secondary_link|tertiary_link\"][\"highway\"!~\"service\"]') \n        end_time = time.time()\n        duration = end_time-start_time\n        print(\"loadtime graph: {} seconds\".format(duration))\n        \n        print(nx.info(G))\n        #ox.add_edge_speeds(G,precision=1)\n        #ox.add_edge_travel_times(G, precision=1)\n        start_time = time.time()\n        df_part = load_part(df_List[i])\n        G_zwei, cd_node = create_cu_Graph(G)\n        \n        df_result = None\n        if(df_part.shape[0]>0):\n            #print(type(df_result))\n            df_result = calc_routes_and_times_zwei(df_part,G,G_zwei,cd_node)\n        end_time = time.time()\n        duration = end_time-start_time\n        print(\"runtime of calculation for part: {} seconds\".format(duration))\n        \n        #print(df_result)\n        if(df_result is None):\n            df_result = None\n        else:\n            df_result_pd = df_result.to_pandas()\n            result_list.append(df_result_pd)\n        df_result = None\n        del df_result\n        del df_part\n    print(\"======================================================================\")\n    df_result_concat_pd = pd.concat(result_list)\n    df_result_concat = cudf.from_pandas(df_result_concat_pd)\n    del df_result_concat_pd\n    df_result_concat.shape[0]\n    del result_list\n    return df_result_concat","a15aeb33":"!nvidia-smi","448702c8":"%%time\n#df_train = calculate_splitpart(splitted_list_pd)\ndf_train = calculate_splitpart(splitted_list)","f46bc308":"df_train.head()","76cb161e":"df_train.describe()","5e97b1c6":"df_train.shape[0]","f5555958":"%%time\ndf_train.to_csv('df_train_after_route_calculation.csv',index=False,chunksize=1000000)","9b11b90b":"## Final words\n\nI hope when you now try to check the correlation between shortest_path_length and fare_amount as well as the correlation between Manhattan_distance and fare_amount, you will see the same improvement I saw when I used the data for my project. I hope I was able to show you why this approach might be better than a simpler distance calculation and perhaps even the Borough labels are helpful for your machine learning algorithm. Maybe you are interested in making your own improvements to that notebook. There are multiple possibilities like tracing back each path and calculating the travel time for the shortest path on your own, or maybe changing the edgeweight so it represents the dollar fare value by combining both travel_time and length of each edge. Maybe you can even use that notebook to calculate when and how often a node or edge is included in a path of a ride, so you can predict traffic jams or adjust the travel_time in accordance to the time of a day.  The possibilities are almost endless. I hope you can have some fun with that notebook and enjoyed reading it. If so feel free to leave a comment below.\n","39a2e69f":"### New York City Data\n\nSince I would like to get a more precise information in which area of Manhattan a ride starts or end, I use publicly available data to differentiate  between Lower Manhattan, Midtown, Upper Westside, Upper Eastside, Upper Manhattan, and Central Park.","c1e7c40b":"As you can clearly see some of the GPS-Coordinates don't make sense, so we use a bounding box to only use useful GPS-Coordinates. We can also elimante all data with a negative fare amount, because that would mean the driver is paying you for a ride in his\/her taxi. Less than 1 and more than 6 passengers are also not of interest. ","c5dfc4d4":"## Requirements:\n\nTo get this notebook to work on your own pc you will need a the following:\n\n- Nvidia GPU with more than 8GB VRAM\n- RAPIDS installed (from rapids.ai, version 1.14 or higher)\n- OSMNX installed\n- NetworkX installed","bbb6dd8e":"### point-in-polygon test\n\nTo calculate which Ride starts and ends in which area, I created four new columns. The first and second column **\"Pickup_Island\"** and **\"Dropoff_Island\"** will define, if the ride started or ended on an Island, in New Jersey or in the New York State Mainland as well as in Connecticut. The third and fourth column **\"Pickup_Borough\"** and **\"Dropoff_Borough\"** will give a more precise definition about the area. It not only includes labels for areas like Manhattan, but also special labels for Midtown, Upper Westside, Brooklyn or JFK, to just name a few. This more precise form of labeling can be helpful later on in a machine learning part.\n\nTo do our calculations, I just load each polygon from a shapefile into the gpu memory and use cuspatials **point_in_polygon** function to determine if a point is within that area.","d3de5f12":"## First Step\nTo start the route calculation on the gpu I had to find a way to parallelize all steps I made to calculate one route with osmnx and networkx. \nThe first step was to get a graph, I already found a way to do that, but I still need to transfer the graph into the gpu. Therefor I extract an Pandas edgelist Dataframe out of the graph, transfer it into the gpu memory as a cudf Dataframe, generate a new cugraph Graph and use the cudf edgelist dataframe to add all edges to the new Graph. \nI also extract a Dataframe containing information about each node and the corresponding GPS coordinates out of the graph in preparation of the nearest node search.","c714e4c0":"As you can see now, the street map gets visualized as a graph with nodes and edges. That visualization also leads us to the answer on how to calculate the routes. Since the street map can also be seen as a graph or network, we can use known algorithms to calculate the shortest distance by using the real distance between the nodes as edgecosts. There are several possible implementations for such a calculation like the single source shortest path implementation in NetworkX.\n\nThe problem in using osmnx and NetworkX is the lack of parallelization. For the number of routes the route calculation would take several days. But for a start lets look at how we can use the approach implemented in NetworkX to solve our problem and to see what steps we need to take to get there.\n\nThe first step is already done, we have a graph of New York City. \n\nAs a second step we need to get the IDs of the node or vertex closest to dropoff and pickup. NetworkX doesn't provide a function for that, but OSMNX does. \n\n- pickup point at Marsha P. Johnson State Park = 40.721153, -73.961206\n- dropoff point at East River Park Field 8 = 40.723742, -73.972854\n\n","087158de":"At the End, I want to give a special thank you to my friend Sanja Hoermann, how studies geoinformatics and helped me with her ideas to realize this notebook. If you like my work or hers, feel free to follow us on LinkedIn.\n\nMe: https:\/\/www.linkedin.com\/in\/jonas-ziegler-108120200\/ \n\nSanja: https:\/\/www.linkedin.com\/in\/sanja-maria-hoermann-52810a214\/ ","3bb940b2":"As you can see in this plot, we can see the shape of Manhattan Brooklyn and Queens as well as the JFK Airport quite clearly. What seems strange though, is, that a considerable number of rides seem to start or end in water. To eliminate those rides, I made some adjustments to the idea I saw in https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration, which also tries to eliminate those rides. ","ff430ef3":"Now that we cleaned the data, lets take a look at a heatmap of our rides. Herefore I'm using a function from https:\/\/www.kaggle.com\/breemen\/nyc-taxi-fare-data-exploration","635bccf3":"### Brooklyn\nAll rides that stay within Brooklyn","dc97439a":"### Long Island\nAll rides that stay within Long Island","acdabe23":"### OSMNX Data\nFor each Island as well as for Queens and Brooklyn a polygon can be downloaded via the OSMNX API","7d198493":"# Route Calculation in less then two hours using GPU ","d1e9a2b7":"### Manhattan distance\n\nTo have a comparison to the shortest route, I want to use a common distance calculation called taxicab geometry, also known as Manhattan distance. \"*The latter name\\[s\\] allude to the grid layout of most streets on the island of Manhattan, which causes the shortest path a car could take between two intersections in the borough to have length equal to the intersections' distance in taxicab geometry.\"* (https:\/\/en.wikipedia.org\/wiki\/Taxicab_geometry)","2c64bd9c":"As a third step, we can calculate the shortest path between those points. For this, we can use NetworkX shortest_path and plot that route with osmnx","56457bc4":"As you can see in the heatmap created after cleaning, the data no longer includes rides that starts or end in water. This helps, since all rides that start or ended in water can be considered outlier, since the shortest path or route calculation relies on the street network, and therefore could produce incorrect distance calculations, which itself would negatively affect the modell training with machine learning algorithms.","f5dc75ff":"# Intro\n\nIn this notebook I will show you how I used GPU acceleration to calculate the distance between pickup and dropoff by calculating the shortest route between those points using OpenStreetMap Data and a GPU.\n\nBut before I start, let me show you why this approach can be a valuable improvement to your notebook.\n\nLet's say you wanted to meet a friend in a park close to the east river. For some unknown reason you forgot the exact location and go to your usual spot, the Marsha P. Johnson State Park in Brooklyn. After you waited a few minutes, you get a call from your friend, how is waiting for you at the East River Park Field 8 in Manhattan. You apologize for your mistake and take a taxi to get to your friend. If you look at the position of both the pickup point (Marsha P. Johnson State Park) and the dropoff point (East River Park Field 8), you will see that there is no direct connection between those point. To get to the dropoff point you have to cross the east river. The nearest possible crossing would be the Williamsburg Bridge or the Queens Midtown Tunnel. Either way, the shortest route the taxi can take will definitely exceed twice the harversine distance between those points.\n\nThis is just one possible example in which the haversine distance between dropoff and pickup differs from the shortest driving distance by more than just a small margin. Since the driving distance and time between pickup and dropoff will determine the taxi fare, this example would most likely be an outlier in a scatter plot for correlation between haversine distance and fare amount. Even with a good machine learning algorithm you will have difficulties to predict the real fare amount for such a ride. \n\nSo what can you do? The simplest answer: Ask google maps. If you take a minute and do some research you will see that there actually is a API for google maps, that can be used to calculate such distances. The main problem in this case is, that google will charge you some money for using the API. In our example with somewhere around 55 Million documented taxi rides, that could amount to a little bit of money that you would have to pay. Since I'm still a student at a university with no regular income, and this is just a project that is supposed to be a replacement for an exam, I'm not willing to afford that. \n\nThe second alternative you may find, is an API from a website called openrouteservice. You can use that API to get route calculations, but in the free version you are currently limited to 2000 request per day. Even if the same pickup-dropoff-pairs occur multiple times in the given dataset, you would still have so many different pairs, that you would have to come up with some ideas to reduce the number of needed requests in order to do the task in a reasonable time. Even if you can come up with enough ideas to make that work, the calculated shortest distance will still be of by a considerable margin from the real shortest driving distance. For me, that ain't worth my time. \n\nAfter all the research I did to find a possible solution, I wondered, if there may be a way to get access to the street map and calculate the shortest route on my own. Luckily I found someone who came up with the same idea and a possible solution. He published that solution as a notebook on kaggle.com. \n\nNotebook:\nhttps:\/\/www.kaggle.com\/usui113yst\/basic-network-analysis-tutorial\n\nThis notebook uses free accessible data from OpenStreetMap and, after some other analysis, gives an example on how to calculate the routes using NetworkX. Before I proceed, let us load the street map from New York City.\n","ceeb4647":"As you can see from this matrix, 46 million rides stay within Manhattan. 1.8 million rides stay within Long Island. Around 5.4 Million rides are between Manhattan and Long Island. Since efficiency is particularly important in parallel algorithms, I want to use my resources efficiently adjusting the size of the graph for the **single source shortest path (sssp)** calculation, and thereby reduce the runtime of the code.\nTo achieve that, I split the date in several parts by using the Island labels, Borough labels as well as bounding boxes. For each part I will use a special Graph. \n\nBecause generating multiple new dataframes containing the same data as df_train would cause an out_of_memory exception on my gpu, I transfer the cudf Dataframe to pandas and split the data using the CPU and RAM. This will consume more time than doing the same steps with a gpu, but I have to stay within the limits of my hardware. This is also the reason why I write each new Dataframe into a csv file, so that I do not need to keep everything in the system memory.","33c51e0f":"### New Jersey Data\nAs you could see in the heatmap generated earlier in this notebook, many rides started or ended in New Jersey. To reduce the size and complexity of merged polygon I filtered the publicly available Data to exclude several Counties of New Jersey.","ccaf7f57":"Now that we loaded the data, we can make some adjustments so that we won't use to much memory. This can be quite helpful, since out_of_memory exceptions can accure quite easily.","34b81bea":"### Staten Island\nAll rides that stay within Staten Island","c226e888":"## Second Step\nThe second step is the nearest node search. For this step I could not find a implementation, which is why I wrote my own using numba. Since the underlying principle should be easy to understand by just inspecting the code, I won't explain it. There are only two things important enough to explain. The first thing is, that to use that numba kernel you need to generate a gpu array out of a cudf.Series by using *to_gpu_array()* and after the execution of the kernelfunction you need to reassign the cudf.Series with the gpu array, otherwise the changes made to the gpu array won't be saved into the cudf.Series.\nThe second thing is, that in order to execute the kernelfunction I needed to define a blocksize and a number of threads_per block. Those numbers may not work for you, so if the code does not work for you, start with those numbers.","14f96d92":"## Final step\nThe final step is to load each dataframe part and execute the code with the correct graph","188d5550":"### New York State data\n\nAlthough this competition is names New York City Taxi Fare Prediction some of rides do not stay within New York City, which is why I had to include polygons for several Counties in New York State.\nI was able to filter those counties so that I could reduce the size and complexity of the resulting polygon.","3a3f562e":"### Manhattan \nAll rides that stay within Manhattan","a0ff6cbb":"## Execution\nWith a 2080ti the runtime for all parts was around 1 hour and 40 minutes. This time may vary depending on your hardware. ","82695525":"## Second Cleaning\n\nInstead of using a picture of a map, I wanted group all rides by area. If a ride doesn't start or end in one of the predefined areas, it would start or end in water and I could eliminate that special ride.\nTo group the data by area I wanted to use polygons to determine which GPS Point belongs to which area. Unfortunately all implementations I could find used a slow sequential approach. Luckily, a friend of mine studied geoinformatics and recommended to use a GIS system. There are several APIs to include a GIS system into Python. The most interesting one is cuspatial, which is included in RAPIDS. It uses a GPU-accelerated solution that according to its developers has shown a significant speed-up in comparison to CPU-based implementations. For further information, check out https:\/\/medium.com\/rapids-ai\/acclerating-gis-data-science-with-rapids-cuspatial-and-gpus-fd012b27af0a\n\nSince we want to check 55 Million taxi rides, with 2 GPS Points each, any speed-up we can achieve is more than welcome.\n\nAfter we now know, how to check those points, we still have to find a way how to get those polygons.\n\n### Loading polygons from public available data\n\nLuckily I didn't need to create those polygons on my own. Some of those polygons can be easily downloaded using the OSMNX API. For some areas, like the State of New Jersey or Connecticut those polygons do not represent the coastline as precisely as you can see it in Google Maps. Thanks to the census, many states collect data about those areas, and it is often available publicly. Thankfully I was able to find out that most of the data, including precise polygons, is stored as ARCGIS data, that can be easily accessed via a REST API. Within some filters applied and some polygons merged I can create multiple precise polygons and save those polygons as a shapefile.","f701e3ec":"Now that I have shown you how to make that calculation with OSMNX and NetworkX, let's get back to the parallelization. As mentioned before, NetworkX and OSMNX do not use a parallel approach to calculate the shortest path. \n\nLuckily RAPIDS has a parallel implementation on a gpu for the single source shortest path algorithm. But before I can show you how the same calculations can be made with RAPIDS, let me show you how to load the data and prepare it, so we can use it for our calculation\n\n## Preparation\n\n### Loading the Data \n\nTo load the data you can use a simple function that will read the data from a csv file directly into your gpu memory. Unless you use Google Colab to run this notebook, this will only take a few seconds.","6950b5a3":"As you can see from this plot, both paths will include the Williamsburg Bridge. The only difference is a small part close to the pickup point. Such a small difference is not always the case for all routes. I plotted several routes within Manhattan for different pickup and dropoffpoints, and not all fastest routes seemed to be reasonable in comparison to the shortest route. Because of this I will only use shortest routes for all future calculations.","5cb5a559":"### Queens and Brooklyn\nAll rides that stay within Queens as well as rides between Queens and Brookyln","3272314b":"# End of Part One\n\nSince gpu memory is limited, and I wasn't able to free memory manually, I will restart the kernel, and unless you have a gpu with more than 11GB VRAM, I would advise you to do the same, and then run all cells below manually. I tried to optimize the second part both in regard to memory utilization and in regard to total runtime. A kernel restart still can be helpful even with highend hardware.\n\n# Start of Part Two\nIn this part I will explain who to do the actual shortest path calculation. ","c6cec799":"As you can see in this Plot, the shortest route is more than four times longer as the haversine distance between the pickup and dropoff point. Since OSMNX allows to add the travel time to each edge, we can calculate the fastest route as well and see how it differs from the shortest route.\n","1f429473":"## Third step\nThe third step is the shortest path calculation. I found an existing implementation of the single source shortest path algorithm included in RAPIDS cuspatial. Calling sssp with the graph and a node\/vertex id as input will result in a dataframe containing all shortest path lengths from that source to all nodes in the graph. To assign the correct results to each source-destination-pair I wrote my own numba function. I later found out, that this can also be achieved with cudf.merge(), but therefor I would need to make some adjustments, and the numba kernelfunction seems to work just fine for me.","445756db":"After loading the data and changing its format, we can inspect it and clean it just as easy as we would in pandas.\n\n(If you are familiar with the usual cleaning process only the Second Cleaning will be of interest to you.)\n### First Cleaning","5cf05bd5":"## Graphs for each dataframe part\nThese generated dataframe parts will use the following graphs for the shortest path calculation:","29af6c0b":"### Connecticut Data\nSince I was not completely sure if some rides would not end in Connecticut I had to included Connecticut as well. By applying a filter, I was able to keep all 'Inland Polygons' a thereby generate merged polygon with a better representation of the coastline then I could get via OSMNX.","a60ccfbc":"### Eliminating the rides that start or end in water\n\nNow that we have labels for each data, all rows can be deleted, where one or both GPS-Points (pickup and dropoff) are not included in the range of possible areas. To get the most precise elimination, I use the **\"Pickup_Island\"** and **\"Dropoff_Island\"** label.","e585cdcf":"### Special Areas\nFor some special areas of interest there are no polygons available, like for the 3 airports JFK, La Guardia and Newark Liberty International Airport. Since a high number of rides start or end at the airport, as we can see in the heatmap plotted earlier, and in case of JFK a ride between the airport and Manhattan will have a special tariff, I would like to have a special label. \n\nThe same goes for Roosevelt Island, which is often included in the polygons for Manhattan, although there is no direct road connection between Roosevelt Island an Manhattan.\n\nTo create Polygons for these areas, I manually extracted GPS-point surrounding these areas out of Google Maps. I than use those points to create Polygons, build a GeoPandas Dataframe and write that into a shapefile.","87fe4651":"## Shortest path calculation\n\nBefore I start with the shortest path calculation, I want to take advantage of the **\"Pickup_Borough\"** label. One graph for all pickup- and dropoff-points in our data would not be an efficient way to handle the calculation, since a lot of routes stay within a small area. To calculate these rides a graph of a smaller area would be better suited, since the runtime of a single source shortest path calculation will scale with the number of nodes and edges in the graph used for this calculation. To see who many rides stay within an island or area, I use the **\"Pickup_Island\"** and **\"Dropoff_Island\"** label to generate a matrix, which will show how many rides stay within an island and how many rides start on one island and end on another island. This should help to determine which graphs to use for the calculation.\n","23853b6a":"### Other\nAll rides that are not included in previous parts make use of three different graphs, starting with a smaller graph, that should be well suited for rides between Manhattan and Brooklyn\/Queens and two bigger graphs for other rides. The key difference between the second and third graph for all other rides is not only the size, but that no residential roads are included, which reduce the accuracy of the calculation with that graph. I was willing to accept that loss in accuracy in order to lower the runtime of the shortest path calculation."}}