{"cell_type":{"a938bc1d":"code","c6869b11":"code","9980f631":"code","9c33188d":"code","c8a1b319":"code","7f472697":"code","413f4481":"code","643518b4":"code","05f6a913":"code","4769a963":"markdown","ac93f3d7":"markdown","aa8c9da3":"markdown","507b4308":"markdown","3d9e26aa":"markdown","0997df00":"markdown"},"source":{"a938bc1d":"# Familiar imports\nimport numpy as np\nimport pandas as pd","c6869b11":"df = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\")\nsample_submission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","9980f631":"# extract useful features\nuseful_features = df.columns.drop([\"id\", \"target\"])\n\n# extract categorical and numerical columns\ncat_cols = list()\nnum_cols = list()\nfor col in useful_features:\n    if 'cat' in col:\n        cat_cols.append(col)\n    else:\n        num_cols.append(col)","9c33188d":"from sklearn.model_selection import train_test_split\n\nX = df[useful_features]\ny = df.target\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2)","c8a1b319":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n\n# preprocess for numerical data\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\n# preprocess for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('oe', OrdinalEncoder())\n])\n\n# bundle preprocessing for all data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_cols),\n        ('cat', categorical_transformer, cat_cols)\n    ]\n)","7f472697":"def create_pipeline(**model_params):\n    # define model\n    my_model = XGBRegressor(\n        random_state=42,\n        tree_method=\"gpu_hist\",\n        gpu_id=0,\n        n_estimators=7000,\n        **model_params\n    )\n    \n    # create pipline \n    my_pipeline = Pipeline(\n        steps=[\n            ('preprocess', preprocessor),\n            ('model', my_model)\n    ])\n    \n    # fit pipeline into training set\n    my_pipeline.fit(X_train, y_train)\n    \n    return my_pipeline","413f4481":"from xgboost import XGBRegressor\nfrom sklearn.metrics import mean_squared_error\nimport optuna\n\ndef run(trial):\n    # params for XGBRegressor\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-2, 0.25, log=True)\n    reg_lambda = trial.suggest_loguniform(\"reg_lambda\", 1e-8, 100.0)\n    reg_alpha = trial.suggest_loguniform(\"reg_alpha\", 1e-8, 100.0)\n    subsample = trial.suggest_float(\"subsample\", 0.1, 1.0)\n    colsample_bytree = trial.suggest_float(\"colsample_bytree\", 0.1, 1.0)\n    max_depth = trial.suggest_int(\"max_depth\", 1, 7)\n\n    # create pipline \n    my_pipeline = create_pipeline(\n        learning_rate=learning_rate,\n        reg_lambda=reg_lambda,\n        reg_alpha=reg_alpha,\n        subsample=subsample,\n        colsample_bytree=colsample_bytree,\n        max_depth=max_depth \n    )\n        \n    # predict for testing set\n    valid_preds = my_pipeline.predict(X_valid)\n    rmse = mean_squared_error(y_valid, valid_preds, squared=False)\n    \n    return rmse","643518b4":"study = optuna.create_study(direction=\"minimize\")\nstudy.optimize(run, n_trials=10)","05f6a913":"# x test\nX_test = df_test[useful_features]\n\n# create model\nmy_pipeline = create_pipeline(**study.best_params)\nmy_predits = my_pipeline.predict(X_test)\n\n# submit\nsample_submission.target = my_predits\nsample_submission.to_csv(\"submission.csv\", index=False)","4769a963":"## Train model","ac93f3d7":"## Splitting data","aa8c9da3":"## Import data","507b4308":"## Define pre-processing steps for data","3d9e26aa":"## Extract features","0997df00":"## Submit"}}