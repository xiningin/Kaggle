{"cell_type":{"c1e88d6e":"code","ea273787":"code","c308fd65":"code","77325023":"code","c80669b7":"code","395517bb":"code","07f6a6fe":"code","20b893e0":"code","fae41648":"code","8d6e5762":"code","77fe1fef":"code","a67e9e90":"code","0868c754":"code","cad96cb3":"code","ab1170bc":"code","280acbab":"code","5d80dfac":"code","14723f6b":"code","4f1f977c":"code","0c5439b4":"code","500bb7e9":"code","88f78c1f":"code","bafc856c":"code","0f8ead38":"code","a7415215":"code","61b8d663":"code","179f8375":"code","bd1dc182":"code","326aea07":"markdown","9a2482d3":"markdown","86786a4f":"markdown","2dbe7ff1":"markdown","4a2334e3":"markdown","c99b68f1":"markdown","4cc81583":"markdown","c0ec6628":"markdown","a8391591":"markdown","632ab2b2":"markdown","5dbf55a8":"markdown","787caab0":"markdown","65ab4a8b":"markdown","c2add9d7":"markdown","f5c3b3a7":"markdown","4128e304":"markdown","041da1f3":"markdown","4144ff03":"markdown","ef4cab45":"markdown","480b141d":"markdown","2505e6fb":"markdown","2fa3c79c":"markdown","63a4a72a":"markdown","32ddffa5":"markdown"},"source":{"c1e88d6e":"#pip install pandas mlxtend\n#!pip install pandas-profiling\n#!pip install --upgrade pandas_profiling","ea273787":"# Importation of libraries\nimport pandas as pd\nimport numpy as np\nimport io\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom mlxtend.frequent_patterns import apriori\nfrom mlxtend.frequent_patterns import association_rules\nimport mlxtend as ml\n\nprint(f'Libraries loaded!')","c308fd65":"# Open and Creating the dataset\ndf_initial = pd.read_csv('..\/input\/transactions-from-a-bakery\/BreadBasket_DMS.csv', header = 0)\ndf_initial.head()","77325023":"# Dataset info\ndisplay(df_initial.info())\n\n# Descriptive statistics of the data\ndisplay(df_initial.describe())","c80669b7":"# Identify null values on dataset\nprint('  ' * 10 + \" Display information about column types and number of null values \" + '  ' * 10 )\nprint('--' * 50)\n\ntab_info = pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\ntab_info = tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values'}))\ntab_info = tab_info.append(pd.DataFrame(df_initial.isnull().sum()\/df_initial.shape[0]*100).T.rename(index={0:'null values (%)'}))\n\nif(any(df_initial.isnull().any())):\n    print()\n    display(tab_info)\nelse:\n    print('NO missing data')","395517bb":"#Let's check the 'hidden' missing values in the dataset\nmissing_value = [\"NaN\", \"NONE\", \"None\", \"Nil\", \"nan\", \"none\", \"nil\", 0]\n\nprint('---------------------------------------------------------')\nprint(\"There are {0} 'hidden' missing values in the 'Item'column.\".format(len(df_initial[df_initial.Item.isin(missing_value)])))\nprint(\"There are {0} 'hidden' missing values in the 'Transaction'.\".format(len(df_initial[df_initial.Transaction.isin(missing_value)])))\nprint('---------------------------------------------------------')\ndf_initial[df_initial.Item.isin(missing_value)].head()","07f6a6fe":"# Selecting the row values to drop in the selected column\nbread = df_initial.drop(df_initial[df_initial.Item == \"NONE\"].index)\nprint(\"Number of rows: {0:,} (original 21,293) \".format(len(bread)))\nprint('----------------------------------------')\nbread.head()\n\n# After removing the missing values, the number of rows left is 20,507 (original 21,293 minus 786 missing)","20b893e0":"# Creating a column of DateTimeIndex\nbread['Datetime'] = pd.to_datetime(bread['Date'] + ' ' + bread['Time'])\nbread = bread[[\"Datetime\", \"Transaction\", \"Item\"]].set_index(\"Datetime\")\nbread.head()","fae41648":"# Extract hour of the day and weekday of the week\n# For Datetimeindex: the day of the week are Monday=0, Sunday=6, thereby +1 to become Monday=1, Sunday=7\nbread[\"Hour\"] = bread.index.hour\nbread[\"Weekday\"] = bread.index.weekday + 1\n\nbread.head()","8d6e5762":"total_items = len(bread)\ntotal_days = len(np.unique(bread.index.date))\ntotal_months = len(np.unique(bread.index.month))\naverage_items = int(total_items \/ total_days)\nunique_items = bread.Item.unique().size\n\nprint(\"Total unique_items: {} sold by the Bakery\".format(unique_items))\nprint('-----------------------------')\nprint(\"Total sales: {} items sold in {} days throughout {} months\".format(total_items, total_days, total_months))\nprint('-----------------------------')\nprint(\"Average_items daily sales: {}\".format(average_items))","77fe1fef":"# Rank the top 10 best-selling items\ncounts = bread.Item.value_counts()\npercent = bread.Item.value_counts(normalize=True).mul(100).round(1).astype(str) + '%'\ntop_10 = pd.DataFrame({'counts': counts, '%': percent})[:10]\n\nprint('-----------------------------')\nprint('Top 10 items')\nprint('-----------------------------')\ndisplay(top_10)","a67e9e90":"# Rank by percentage\nplt.figure(figsize=(8,5))\nbread.Item.value_counts(normalize=True)[:10].plot(kind=\"bar\", title=\"Percentage of Sales by Item\").set(xlabel=\"Item\", ylabel=\"Percentage\")\nplt.show()\n\n# Rank by value\nplt.figure(figsize=(8,5))\nbread.Item.value_counts()[:10].plot(kind=\"bar\", title=\"Total Number of Sales by Item\").set(xlabel=\"Item\", ylabel=\"Total Number\")\nplt.show()","0868c754":"# Number of items sold by day\nbread[\"Item\"].resample(\"D\").count().plot(figsize=(15,5), grid=True, title=\"Total Number of Items Sold by Date\").set(xlabel=\"Date\", ylabel=\"Total Number of Items Sold\")\nplt.show()","cad96cb3":"# Number of items sold by month\nbread[\"Item\"].resample(\"M\").count().plot(figsize=(15,5), grid=True, title=\"Total Number by Items Sold by Month\").set(xlabel=\"Date\", ylabel=\"Total Number of Items Sold\")\nplt.show()","ab1170bc":"# Aggregate item sold by hour\nbread_groupby_hour = bread.groupby(\"Hour\").agg({\"Item\": lambda item: item.count()\/total_days})\nprint(bread_groupby_hour)\n\n# Plot items sold by hour\nplt.figure(figsize=(8,5))\nsns.countplot(x='Hour',data=bread)\nplt.title('Items Sales by hour')\nplt.show()","280acbab":"# sales groupby weekday\nbread_groupby_weekday = bread.groupby(\"Weekday\").agg({\"Item\": lambda item: item.count()})\nbread_groupby_weekday.head()","5d80dfac":"# but we need to find out how many each weekday in that period of transaction\n# in order to calculate the average items per weekday\n\nimport datetime \ndaterange = pd.date_range(datetime.date(2016, 10, 30), datetime.date(2017, 4, 9))\n\nmonday = 0\ntuesday = 0\nwednesday = 0\nthursday = 0\nfriday = 0\nsaturday = 0\nsunday = 0\n\nfor day in np.unique(bread.index.date):\n    if day.isoweekday() == 1:\n        monday += 1\n    elif day.isoweekday() == 2:\n        tuesday += 1\n    elif day.isoweekday() == 3:\n        wednesday += 1\n    elif day.isoweekday() == 4:\n        thursday += 1        \n    elif day.isoweekday() == 5:\n        friday += 1        \n    elif day.isoweekday() == 6:\n        saturday += 1        \n    elif day.isoweekday() == 7:\n        sunday += 1        \n        \nall_weekdays = monday + tuesday + wednesday + thursday + friday + saturday + sunday\n\nprint(\"monday = {0}, tuesday = {1}, wednesday = {2}, thursday = {3}, friday = {4}, saturday = {5}, sunday = {6}, total = {7}\".format(monday, tuesday, wednesday, thursday, friday, saturday, sunday, all_weekdays))","14723f6b":"# apply the conditions to calculate the average items for each weekday\nconditions = [\n    (bread_groupby_weekday.index == 1),\n    (bread_groupby_weekday.index == 2),\n    (bread_groupby_weekday.index == 3),\n    (bread_groupby_weekday.index == 4),\n    (bread_groupby_weekday.index == 5),\n    (bread_groupby_weekday.index == 6),\n    (bread_groupby_weekday.index == 7)]\n\nchoices = [bread_groupby_weekday.Item\/21, bread_groupby_weekday.Item\/23, bread_groupby_weekday.Item\/23, bread_groupby_weekday.Item\/23, bread_groupby_weekday.Item\/23, bread_groupby_weekday.Item\/23, bread_groupby_weekday.Item\/23]\n\nbread_groupby_weekday[\"Average\"] = np.select(conditions, choices, default=0)\nbread_groupby_weekday","4f1f977c":"bread_groupby_weekday.plot(y=\"Average\", figsize=(12,5), title=\"Average Number by Items Sold by Day of the Week\").set(xlabel=\"Day of the Week (1=Monday, 7=Sunday)\", ylabel=\"Average Number of Items Sold\")\nplt.show()","0c5439b4":"# Define dataset to machine learning\ndf_basket = bread.groupby([\"Transaction\",\"Item\"]).size().reset_index(name=\"Count\")\n\nmarket_basket = (df_basket.groupby(['Transaction', 'Item'])['Count'].sum().unstack().reset_index().fillna(0).set_index('Transaction'))\nmarket_basket.head()","500bb7e9":"# Convert all of our numbers to either a 1 or a 0 (negative numbers are converted to zero, positive numbers are converted to 1)\ndef encode_data(datapoint):\n  if datapoint <= 0:\n    return 0\n  else:\n    return 1","88f78c1f":"# Process the transformation into the market_basket dataset\nmarket_basket = market_basket.applymap(encode_data)\n\n# Check the result\nmarket_basket.head()","bafc856c":"# Apriori method request a min_support: Support is defined as the percentage of time that an itemset appears in the dataset.\n# Defined to start seeing data\/results with min_support of 2%\nitemsets = apriori(market_basket, min_support= 0.02, use_colnames=True)","0f8ead38":"# Build your association rules using the mxltend association_rules function.\n# min_threshold can be thought of as the level of confidence percentage that you want to return\n# Defined to use 50% of min_threshold\nrules = association_rules(itemsets, metric='lift', min_threshold=0.5)","a7415215":"# Below the list of products sales combinations\n# It can use this information to build a cross-sell recommendation system that promotes these products with each other \nrules.sort_values(\"lift\", ascending = False, inplace = True)\nrules.head(10)","61b8d663":"support = rules.support.to_numpy()\nconfidence = rules.confidence.to_numpy()\n\nfor i in range (len(support)):\n    support[i] = support[i]\n    confidence[i] = confidence[i]\n\nplt.figure(figsize=(8,6))    \nplt.title('Assonciation Rules')\nplt.xlabel('support')\nplt.ylabel('confidance')\nsns.regplot(x=support, y=confidence, fit_reg=False)\nplt.show()","179f8375":"# Recommendation of Market Basket\nrec_rules = rules[ (rules['lift'] > 1) & (rules['confidence'] >= 0.5) ]","bd1dc182":"# Recommendation of Market Basket Dataset\ncols_keep = {'antecedents':'item_1', 'consequents':'item_2', 'support':'support', 'confidence':'confidence', 'lift':'lift'}\ncols_drop = ['antecedent support', 'consequent support', 'leverage', 'conviction']\n\nrecommendation_basket = pd.DataFrame(rec_rules).rename(columns= cols_keep).drop(columns=cols_drop).sort_values(by=['lift'], ascending = False)\nrecommendation_basket['item_1'] = recommendation_basket['item_1'].str.join('()')\nrecommendation_basket['item_2'] = recommendation_basket['item_2'].str.join('()')\ndisplay(recommendation_basket)","326aea07":"\n### Identifying Features & Labels","9a2482d3":"### Recommendations filter\n","86786a4f":"\n* Interpretation of recoomendation_basket\n  * The recommendation_basket selected 8 itemsets sorted by lift > 1, confidence >= 0.5 and support > 1%\n  * The support indicates how many transaction of the itemset (item_1 and item_2) occurs together.\n  * The confidence >= 0.5 indicates that the itemset has the likelihood of the item_2 is bought when someone buys the item_1\n  * The lift shows how item_1 helps to increase the sales of the item_2 when buying those items together\n  * Ex: \n    * Toast and Coffee occurs nearly 2.4% of all transactions (support)\n    * The confidence is about 70% showing that the probability to happen this transaction is high\n    * Buying a toast shows that it increase in 47% the probability to buy Coffee showing the influence of Toast over Coffee.\n    \n  * So with the recommendation_basket the team (leader and employees) can be aligned to develop strategies and training to incentive customers who buys item_1 to include the item_2, doing a cross-selling and increasing the store's revenue.","2dbe7ff1":"Time Series","4a2334e3":"Top 10 items","c99b68f1":"Extract hour and days of week\n","4cc81583":"### Exploring data attributes","c0ec6628":"Bar Charts","a8391591":"As weekend shows to be the best days of sales, the bakery can produce more in those days and reduce in the others days.\nAnother strategy is giving to the customer the perception that there are promotions in other days to increase the visit of the customer to the bakery to check what kind of promotion exists making the average sales during the weekdays increase","632ab2b2":"### Import Libraries\n\n\n\n\n\n","5dbf55a8":"Convert to DatetimeIndex\n","787caab0":"**Bakery Market Basket Analysis**\n---\n","65ab4a8b":"* In food market finding a new way to increase sales and customer engagement \nare the most common and difficult task to deal. Increase the number of competitors, economy, seazonal issues and a lot of others features that \ninfluence in this market.\n\n* To have advatange it is very important to understand the customer behavior, store layout and do customized offers to this customer.\n\n* Thinking about that Machine Learning can help to understand the customer behavior and analyze the sales history to make predictions to boost the sales. And one option to do that is using Market Basket Analysis, that is a method to understand the combination of items that estimulate more sales.\n\n* In this project I am going to use Apriori algorithm to develop a Market Basket Analysis to create a recommendation basket to develop new strategies for customers.","c2add9d7":"The period of lunch time is when there are more customers in the bakery, giving an opportunity to develop strategy of experimentation and promotions to customers experience new products","f5c3b3a7":"Check for hidden missing values\n","4128e304":"### Missing Value Identification","041da1f3":"## **Exploratory Data Analysis (EDA)**\n---\n\nDoing EDA will help to understand the dataset and which features to select to find the label of prediction\n","4144ff03":"### Preparing the dataset to Machine Learning","ef4cab45":"*   Support:\n  * refers to the default popularity of an item and can be calculated by finding number of transactions containing a particular item divided by total number of transactions\n\n\n*   Confidence:\n  * refers to the likelihood that an item B is also bought if item A is bought. It can be calculated by finding the number of transactions where A and B are bought together, divided by total number of transactions where A is bought\n\n*   Lift:\n  * refers to the increase in the ratio of sale of B when A is sold. Lift(A \u2013> B) can be calculated by dividing Confidence(A -> B) divided by Support(B)\n\n*   Leverage:\n  * computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent\n\n*   Conviction:\n   * A high conviction value means that the consequent is highly depending on the antecedent\n","480b141d":"### Load file & Data preparation\n\n\n\n\n\n","2505e6fb":"## **Machine Learning Algorithm**\n---","2fa3c79c":"Drop the missing values\n","63a4a72a":"### Building the Apriori model","32ddffa5":"### Dataset Transformation\n"}}