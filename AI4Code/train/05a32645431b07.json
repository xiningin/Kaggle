{"cell_type":{"6333b803":"code","cbcc73cb":"code","4033fe31":"code","fc37788a":"code","3e64e900":"code","f7a55a11":"code","78a2d61b":"code","6aab8b7b":"code","445a9359":"code","83d76f8f":"code","aad1131d":"code","dc847cce":"code","58e151f4":"code","eb7a8de4":"code","d902f3c0":"code","2fb6b78f":"code","113132cd":"code","5a283509":"code","4dddde37":"code","8e729478":"code","7b7515b3":"code","f1e09041":"code","afb6b767":"code","413321d9":"code","1ac664d2":"code","55d874a2":"code","d5dc9f90":"code","88299099":"code","366e6418":"code","9d45a421":"code","17bc964c":"code","21292ffd":"code","4d3c1d95":"code","ac0260ff":"code","5525862d":"code","c57e8fcb":"code","9cba3523":"code","d7165cf5":"code","44daccf5":"code","7d694ea4":"code","8d6db9f9":"code","a66d6a4c":"code","c168ac56":"code","ba1f6987":"code","473f05f5":"code","87867a38":"code","247854d5":"code","ed8f4ce0":"code","640be159":"code","b16b14d3":"code","ac598a16":"code","5168d434":"code","8260db7b":"code","bc580c41":"code","bbe14890":"code","739c7884":"code","7587db0a":"code","399a6999":"code","496e2d77":"code","114cc8d6":"code","cb720424":"code","c40745fb":"code","ff3d2e83":"code","a0433974":"code","65d70ca3":"code","f2dd087a":"code","01240c29":"code","f38ad52c":"code","a68d5438":"code","e8c524f1":"code","ec2a9714":"code","677da61c":"code","a337678c":"code","506e5b5b":"code","94a2cbf8":"code","047a3b18":"code","7846fde3":"code","a6e7680a":"code","93f6c3a8":"code","0c99aa01":"code","3e99a3e2":"code","d29c4bec":"code","448be535":"code","8800811d":"markdown","ad658d11":"markdown","c0bac0aa":"markdown","e84453fc":"markdown","bc67f83b":"markdown","52a33c60":"markdown","b97b957f":"markdown","b816b901":"markdown","83ba28e4":"markdown","ed8efdd2":"markdown","688d40de":"markdown","8bb39fda":"markdown","8768c375":"markdown","104dcf95":"markdown"},"source":{"6333b803":"#import library\nimport pickle \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn import metrics","cbcc73cb":"df = pd.read_csv('..\/input\/spamemailsdataset\/Spam.csv')","4033fe31":"df.shape","fc37788a":"df.head()","3e64e900":"df.describe()","f7a55a11":"df.info()","78a2d61b":"#check correlation of each variable\ndf.corr()","6aab8b7b":"#visualize correlation of variable using pearson correlation\nplt.figure(figsize = (8,6))\nsns.heatmap(df.corr(), vmax = 0.9, cmap = 'YlGnBu')\nplt.title('Pearson Correlation', fontsize = 15, pad = 12, color = 'r')\nplt.show()","445a9359":"#transform spam column to categorical data\ndf.spam[df['spam'] == 0] = 'ham'\ndf.spam[df['spam'] == 1] = 'spam'\ndf.head()","83d76f8f":"#analyze of spam status based on capital run length average\npd.pivot_table(df, index = 'spam', values = 'capital_run_length_average', \n               aggfunc = {'capital_run_length_average' : np.mean}).sort_values('capital_run_length_average', ascending = False)","aad1131d":"#analyze of spam status based on count of capital run length longest\npd.pivot_table(df, index = 'spam', values = 'capital_run_length_longest',\n              aggfunc = {'capital_run_length_longest' : np.sum}).sort_values('capital_run_length_longest', ascending = False)","dc847cce":"#anayze of spam status based on count of capital run length total\npd.pivot_table(df, index = 'spam', values = 'capital_run_length_total',\n              aggfunc = {'capital_run_length_total' : np.sum}).sort_values('capital_run_length_total', ascending = False)","58e151f4":"#anayze of spam status based on capital run length average, capital run length longest and capital run length total\npd.pivot_table(df, index = 'spam', values = ['capital_run_length_average', 'capital_run_length_longest', \n                                             'capital_run_length_total'], \n               aggfunc = {'capital_run_length_average' : np.mean, 'capital_run_length_longest' : np.sum, \n                          'capital_run_length_total' : np.sum}).sort_values(['capital_run_length_average', \n                                                                             'capital_run_length_longest', \n                                                                             'capital_run_length_total'], ascending = False)","eb7a8de4":"#visualize the factor of spam message based on capital run length average, capital run length longest and capital run length total\nplt.figure(figsize = (14,6))\nchart = df.boxplot()\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90)\nplt.title('The Factor of Spam Message', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('Factor')\nplt.ylabel('Count')\nplt.show()","d902f3c0":"#visualize of spam status based on capital run length average\nchart = df.groupby('spam')['capital_run_length_average'].mean().sort_values(ascending = False).plot(kind = 'bar', \n                                                                                                    color = 'darkorange')\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 0)\nplt.title('Average Capital Length of Spam Status', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('Spam Status')\nplt.ylabel('Average of Capital Length')\nplt.show()\n\n#visualize of spam status based on capital run length longest\nchart = df.groupby('spam')['capital_run_length_longest'].sum().sort_values(ascending = False).plot(kind = 'bar', \n                                                                                                    color = 'lawngreen')\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 0)\nplt.title('Capital Length Longest of Spam Status', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('Spam Status')\nplt.ylabel('Count of Capital Length Longest')\nplt.show()\n\n#visualize of spam status based on capital run length total\nchart = df.groupby('spam')['capital_run_length_total'].sum().sort_values(ascending = False).plot(kind = 'bar', \n                                                                                                    color = 'orangered')\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 0)\nplt.title('Capital Length Total of Spam Status', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('Spam Status')\nplt.ylabel('Count of Capital Length Total')\nplt.show()","2fb6b78f":"#visualize correlation of capital run length total and capital run length average\nplt.scatter(df['capital_run_length_total'], df['capital_run_length_average'], marker = 'o', color = 'r')\nplt.title('Capital Run Length Total ~ Capital Run Length Average', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('Count of Capital Run Length Total')\nplt.ylabel('Capital Run Length Average')\nplt.show()\n\n#visualize correlation of capital run length total and capital run length longest\nplt.scatter(df['capital_run_length_total'], df['capital_run_length_longest'], marker = '+', color = 'b')\nplt.title('Capital Run Length Total ~ Capital Run Length Longest', fontsize = 15, pad = 12, color = 'r')\nplt.xlabel('Count of Capital Run Length Total')\nplt.ylabel('Count of Capital Run Length Longest')\nplt.show()\n\n#visualize correlation of capital run length longest and capital run length average\nplt.scatter(df['capital_run_length_longest'], df['capital_run_length_average'], marker = '^', color = 'lightseagreen')\nplt.title('Capital Run Length Longest ~ Capital Run Length Average', fontsize = 15, pad = 12, color = 'purple')\nplt.xlabel('Count of Capital Run Length Longest')\nplt.ylabel('Capital Run Length Average')\nplt.show()","113132cd":"#visualize spam status\nspam = [np.count_nonzero(df['spam'] == 'ham'),\n       np.count_nonzero(df['spam'] == 'spam')]\nactivities = ['Ham', 'Spam']\nplt.pie(spam, labels = activities, autopct = '%1.2f%%')\nplt.title('Spam Message Status', fontsize = 15, pad = 12, color = 'r')\nplt.show()","5a283509":"#handling categorical data\ndf['spam'] = df['spam'].astype('category').cat.codes\n\n#splitting data\nX = df.drop('spam', axis = 1)\ny = df['spam']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","4dddde37":"#build model\ngnb = GaussianNB()\n%time gnb.fit(X_train, y_train)\nprint('Accuracy Score :', gnb.score(X_test, y_test))","8e729478":"#calculate prediction probability\nprob_train = np.squeeze(gnb.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(gnb.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","7b7515b3":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","f1e09041":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Gaussian Naive Bayes', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","afb6b767":"#prediction\ny_pred = gnb.predict(X_test)\nprint(y_pred)","413321d9":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'autumn')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Gaussian Naive Bayes)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","1ac664d2":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Gaussian Naive Bayes : \\n')\nprint(report)","55d874a2":"#build model\nrf = RandomForestClassifier(random_state = 1, max_features = 'sqrt', n_jobs = 1, verbose = 1)\n%time rf.fit(X_train, y_train)\nprint('Accuracy Score : ', rf.score(X_test, y_test))","d5dc9f90":"#calculate prediction probability\nprob_train = np.squeeze(rf.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(rf.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","88299099":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","366e6418":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Random Forest', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","9d45a421":"#prediction\ny_pred = rf.predict(X_test)\nprint(y_pred)","17bc964c":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'crest')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Random Forest)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","21292ffd":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Random Forest : \\n')\nprint(report)","4d3c1d95":"#check feature importance\nfeature = pd.Series(rf.feature_importances_, index = X_train.columns).sort_values(ascending = False)\nprint(feature)","ac0260ff":"#visualize feature\nplt.figure(figsize = (15,6))\nchart = sns.barplot(x = feature.index, y = feature)\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90)\nplt.title(\"Feature Importance\")\nplt.xlabel('Features')\nplt.ylabel('Score')\nplt.show()","5525862d":"#build model\nabc = AdaBoostClassifier()\n%time abc.fit(X_train, y_train)\nprint('Accuracy Score :', abc.score(X_test, y_test))","c57e8fcb":"#calculate prediction probability\nprob_train = np.squeeze(abc.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(abc.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","9cba3523":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","d7165cf5":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Adaptive Boosting', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","44daccf5":"#prediction\ny_pred = abc.predict(X_test)\nprint(y_pred)","7d694ea4":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'icefire')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Adaptive Boosting)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","8d6db9f9":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Adaptive Boosting : \\n')\nprint(report)","a66d6a4c":"#setup arrays to store training and test accuracy\nneighbors = np.arange(1,9)\ntrain_accuracy = np.empty(len(neighbors))\ntest_accuracy = np.empty(len(neighbors))\n\nfor i, k in enumerate(neighbors):\n    knn = KNeighborsClassifier(n_neighbors = k)\n    knn.fit(X_train, y_train)\n    train_accuracy[i] = knn.score(X_train, y_train)\n    test_accuracy[i] = knn.score(X_test, y_test) ","c168ac56":"#find good K-value from error rate\nerror_rate = []\n\nfor i in range(1,9):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))\n    \nplt.plot(range(1,9), error_rate, color = 'blue',\n                linestyle = 'dashed', marker = 'o',\n                markerfacecolor = 'red', markersize = 10)\n\nplt.title('Find Error Rate from K-Value', fontsize = 15, pad = 12, color = 'g')\nplt.xlabel('K-Value', fontsize = 14)\nplt.ylabel('Error Rate', fontsize = 14)\nplt.show()","ba1f6987":"#visualize knn model for training and testing\nplt.plot(neighbors, test_accuracy, label = 'Testing Accuracy', color = 'r')\nplt.plot(neighbors, train_accuracy, label = 'Training accuracy', color = 'b')\nplt.title('K-NN Validation Accuracy of Neighbors', fontsize = 15, pad = 12, color = 'maroon')\nplt.legend()\nplt.xlabel('Number of Neighbors', fontsize = 14)\nplt.ylabel('Accuracy', fontsize = 14)\nplt.show()","473f05f5":"#build model\nknn = KNeighborsClassifier(n_neighbors = 5)\n%time knn.fit(X_train, y_train)\nprint('Accuracy Score :', knn.score(X_test, y_test))","87867a38":"#calculate prediction probability\nprob_train = np.squeeze(knn.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(knn.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","247854d5":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","ed8f4ce0":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of K-Nearest Neighbors', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","640be159":"#prediction\ny_pred = knn.predict(X_test)\nprint(y_pred)","b16b14d3":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'magma')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(K-Nearest Neighbors)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","ac598a16":"#classification report \nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of K-Nearest Neighbors : \\n')\nprint(report)","5168d434":"#build model\nlsvm = LinearSVC()\n%time lsvm.fit(X_train, y_train)\nprint('Accuracy Score :', lsvm.score(X_test, y_test))","8260db7b":"#calculate prediction probability\nprob_train = np.squeeze(lsvm._predict_proba_lr(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(lsvm._predict_proba_lr(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","bc580c41":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","bbe14890":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Linear Support Vector Machine', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","739c7884":"#prediction\ny_pred = lsvm.predict(X_test)\nprint(y_pred)","7587db0a":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'RdBu')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Linear Support Vector Machine)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","399a6999":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Linear Support Vector Machine : \\n')\nprint(report)","496e2d77":"#build model\ndtc = DecisionTreeClassifier(criterion = 'gini', max_depth = 3)\n%time dtc.fit(X_train, y_train)\nprint('Accuracy Score :', dtc.score(X_test, y_test))","114cc8d6":"#calculate prediction probability\nprob_train = np.squeeze(dtc.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(dtc.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","cb720424":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","c40745fb":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Decision Tree', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","ff3d2e83":"#prediction \ny_pred = dtc.predict(X_test)\nprint(y_pred)","a0433974":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'Spectral')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Decision Tree)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","65d70ca3":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Decision Tree : \\n')\nprint(report)","f2dd087a":"#build model\nlda = LinearDiscriminantAnalysis()\n%time lda.fit(X_train, y_train)\nprint('Accuracy Score :', lda.score(X_test, y_test))","01240c29":"#calculate prediction probability\nprob_train = np.squeeze(lda.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(lda.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","f38ad52c":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","a68d5438":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Linear Discriminant Analysis', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","e8c524f1":"#prediction\ny_pred = lda.predict(X_test)\nprint(y_pred)","ec2a9714":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'rocket')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Linear Discriminant Analysis)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","677da61c":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Linear Discriminant Analysis : \\n')\nprint(report)","a337678c":"#build model\nqda = QuadraticDiscriminantAnalysis()\n%time qda.fit(X_train, y_train)\nprint('Accuracy Score :', qda.score(X_test, y_test))","506e5b5b":"#calculate prediction probability\nprob_train = np.squeeze(qda.predict_proba(X_train)[:,1].reshape(1,-1))\nprob_test = np.squeeze(qda.predict_proba(X_test)[:,1].reshape(1,-1))\nprint('Probability of Training Set : \\n', prob_train)\nprint('\\nProbability of Testing Set : \\n', prob_test)","94a2cbf8":"#false positive rate, true positive rate, thresholds\nfpr1, tpr1, thresholds1 = metrics.roc_curve(y_test, prob_test)\nfpr2, tpr2, thresholds2 = metrics.roc_curve(y_train, prob_train)\n\n#auc score\nauc1 = metrics.auc(fpr1, tpr1)\nauc2 = metrics.auc(fpr2, tpr2)\nprint('Testing of Area Under the Curve (AUC) :', auc1)\nprint('Training of Area Under the Curve (AUC) :', auc2)","047a3b18":"#plot auc \nplt.plot(fpr1, tpr1, color = 'blue', label = 'Testing ROC curve area = %0.2f' % auc1)\nplt.plot(fpr2, tpr2, color = 'green', label = 'Training ROC curve area = %0.2f' % auc2)\nplt.plot([0,1],[0,1], 'r--')\nplt.xlim([-0.1, 1.1])\nplt.ylim([-0.1, 1.1])\nplt.title('Receiver Operating Curve (ROC Curves) of Quadratic Discriminant Analysis', fontsize = 15, pad = 12, color = 'b')\nplt.xlabel('False Positive Rate', size = 14)\nplt.ylabel('True Positive Rate', size = 14)\nplt.legend(loc = 'lower right')\nplt.show() ","7846fde3":"#prediction\ny_pred = qda.predict(X_test)\nprint(y_pred)","a6e7680a":"#confusion matrix\nmatrix = pd.DataFrame((metrics.confusion_matrix(y_test, y_pred)), ('Spam', 'Ham'), ('Spam', 'Ham'))\nprint(matrix)\n\n#visualize confusion matrix\nheatmap = sns.heatmap(matrix, annot = True, annot_kws = {'size': 14}, fmt = 'd', cmap = 'inferno')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), fontsize = 12)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), fontsize = 12)\n\nplt.title('Confusion Matrix\\n(Quadratic Discriminant Analysis)', fontsize = 18, pad = 12, color = 'darkblue')\nplt.ylabel('True label', fontsize = 14)\nplt.xlabel('Predicted label', fontsize = 14)\nplt.show()","93f6c3a8":"#classification report\nreport = metrics.classification_report(y_test, y_pred)\nprint('Classification Report of Quadratic Discriminant Analysis : \\n')\nprint(report)","0c99aa01":"#prepare models\nclassifiers = [\n    KNeighborsClassifier(n_neighbors = 5),\n    LinearSVC(),\n    DecisionTreeClassifier(criterion = 'gini', max_depth = 3),\n    RandomForestClassifier(random_state = 1, max_features = 'sqrt', n_jobs = 1, verbose = 1),\n    AdaBoostClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis()]","3e99a3e2":"#report models\nlog_cols = [\"Classifier\", \"Accuracy\", \"Log Loss\"]\nlog = pd.DataFrame(columns = log_cols)\n\nfor clf in classifiers:\n    clf.fit(X_train, y_train)\n    name = clf.__class__.__name__\n    print(\"=\" * 30)\n    print(name)\n    print('****Results****')\n    \n    train_predictions = clf.predict(X_test)\n    acc = metrics.accuracy_score(y_test, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    \n    train_predictions = clf.predict(X_test)\n    ll = metrics.log_loss(y_test, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    print(\"\\n\")\n    \n    log_entry = pd.DataFrame([[name, acc * 100, ll]], columns = log_cols)\n    log = log.append(log_entry)\n    \nprint(\"=\" * 30)","d29c4bec":"#visualize accuracy models\nsns.set_color_codes(\"muted\")\nsns.barplot(x = 'Accuracy', y = 'Classifier', data = log, color = 'dodgerblue')\nplt.xlabel('Accuracy %')\nplt.title('Accuracy Score of Classification Model')\nplt.show()\n\n#visualize log loss models\nsns.barplot(x = 'Log Loss', y = 'Classifier', data = log, color = 'seagreen')\nplt.xlabel('Log Loss')\nplt.title('Log Loss of Classification Model')\nplt.show()","448be535":"#deploying the model\nrf.fit(X, y)\npickle.dump(rf, open('Random_Forest.pkl', 'wb'))","8800811d":"# Comparison of Machine Learning Algorithm","ad658d11":"# Quadratic Discriminant Analysis","c0bac0aa":"# Random Forest","e84453fc":"# Data Extraction","bc67f83b":"# Exploratory Data Analysis (EDA)","52a33c60":"# Gaussian Naive Bayes","b97b957f":"# Spam Classification from Email","b816b901":"# Adaptive Boosting","83ba28e4":"# Deployment","ed8efdd2":"# Decision Tree","688d40de":"# Linear Discriminant Analysis","8bb39fda":"# K-Nearest Neighbors","8768c375":"# Linear Support Vector Machine ","104dcf95":"**Spam Label Classification :**\n- 0 -> Ham\n- 1 -> Spam\n\n**Conclusion :**\n- Random Forest Classifier is very suitable model to predict spam classification message on E-mail\n- Accuracy Score on 96%\n- Testing Set of ROC Curves Area on 99%\n- Training Set of ROC Curves Area on 100%\n- Ham labels with precision on 95%, recall on 97% and f1-score on 96%\n- Spam labels with precision on 96%, recall on 93% and f1-score on 95%\n- Ham labels have 804 samples more than Spam labels have 577 samples"}}