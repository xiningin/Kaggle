{"cell_type":{"97d84389":"code","b7b273a6":"code","6ae2fa1a":"code","f0731c18":"code","fbee8adc":"code","7491ade3":"code","69d903bc":"code","df25d975":"code","bec915f5":"code","b9d3af54":"code","ed70e822":"code","99bcb33f":"code","78ba491c":"code","c08e0afe":"code","699c9c41":"code","8650e8a8":"code","4da79593":"code","bf9f8273":"code","bd4ac820":"code","ef464d1c":"code","76e359de":"code","f2f20a39":"code","4cd6a99f":"code","a67e5815":"code","ef8d4f7a":"code","440997ab":"markdown","b7ec8c28":"markdown","90615c93":"markdown","bda23e10":"markdown","fcb0a2cd":"markdown","d930a8d2":"markdown","18cca1a2":"markdown","4ff0d9d4":"markdown"},"source":{"97d84389":"#importing libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport random\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b7b273a6":"#loading the dataset\ndf=pd.read_csv(\"..\/input\/new_churn_data.csv\")","6ae2fa1a":"#list of columns\nlist(df)","f0731c18":"df.head()","fbee8adc":"#detecting missing values\ndf.isna().any()","7491ade3":"plt.figure(figsize=(12,10))\nsns.heatmap(df.corr())","69d903bc":"df2=df[['age','cards_clicked','cards_viewed','cash_back_engagement']]\n#histogram to get the distributions of different variables\ndf2.hist(bins=70, figsize=(20,20))\nplt.show()","df25d975":"user_id = df['userid']\ndf=df.drop(columns=['userid'])","bec915f5":"#transforming into dummy variables\ndf.rent_or_own.value_counts()\ndf.groupby('rent_or_own')['churn'].nunique().reset_index()\ndf = pd.get_dummies(df)\ndf.columns\ndf = df.drop(columns = ['rent_or_own_na', 'zodiac_sign_na', 'payfreq_na'])","b9d3af54":"#splitting our data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop(columns = 'churn'), df['churn'],\n                                                    test_size = 0.2,\n                                                    random_state = 1992)\n","ed70e822":"# Balancing the Training Set\ny_train.value_counts()\n\npos_index = y_train[y_train.values == 1].index\nneg_index = y_train[y_train.values == 0].index\n\nif len(pos_index) > len(neg_index):\n    higher = pos_index\n    lower = neg_index\nelse:\n    higher = neg_index\n    lower = pos_index\n\nrandom.seed(1992)\nhigher = np.random.choice(higher, size=len(lower))\nlower = np.asarray(lower)\nnew_indexes = np.concatenate((lower, higher))\n\nX_train = X_train.loc[new_indexes,]\ny_train = y_train[new_indexes]","99bcb33f":"#scaling the variables\nfrom sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\nX_train2 = pd.DataFrame(sc_X.fit_transform(X_train))\nX_test2 = pd.DataFrame(sc_X.transform(X_test))\nX_train2.columns = X_train.columns.values\nX_test2.columns = X_test.columns.values\nX_train2.index = X_train.index.values\nX_test2.index = X_test.index.values\nX_train = X_train2\nX_test = X_test2","78ba491c":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 1492)\nclassifier.fit(X_train, y_train)","c08e0afe":"y_pred=classifier.predict(X_test)","699c9c41":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprecision_score(y_test, y_pred) # tp \/ (tp + fp)\nrecall_score(y_test, y_pred) # tp \/ (tp + fn)\nf1_score(y_test, y_pred)\n\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))\nprint(\"Precission score: %0.4f\" % precision_score(y_test, y_pred))\nprint(\"Recall score: %0.4f\" % recall_score(y_test, y_pred))\nprint(\"F1 score: %0.4f\" %f1_score(y_test, y_pred))\n","8650e8a8":"# Applying k-Fold Cross Validation\nfrom sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier, X = X_train, y = y_train, cv = 10)\nprint(\"SVM Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","4da79593":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train.columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","bf9f8273":"# Recursive Feature Elimination (RFE)\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LogisticRegression\n\n# Model to Test\nclassifier = LogisticRegression()","bd4ac820":"# Select Best X Features\nrfe = RFE(classifier, 20)\nrfe = rfe.fit(X_train, y_train)\n# summarize the selection of the attributes\nprint(rfe.support_)\nprint(rfe.ranking_)\nX_train.columns[rfe.support_]","ef464d1c":"#new heatmap\n# New Correlation Matrix\nsns.set(style=\"white\")\n\n# Compute the correlation matrix\ncorr = X_train[X_train.columns[rfe.support_]].corr()\n\n# Generate a mask for the upper triangle\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(18, 15))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})  ","76e359de":"classifier = LogisticRegression()\nclassifier.fit(X_train[X_train.columns[rfe.support_]], y_train)\n\n# Predicting Test Set\ny_pred = classifier.predict(X_test[X_train.columns[rfe.support_]])","f2f20a39":"from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, precision_score, recall_score\ncm = confusion_matrix(y_test, y_pred)\naccuracy_score(y_test, y_pred)\nprecision_score(y_test, y_pred) # tp \/ (tp + fp)\nrecall_score(y_test, y_pred) # tp \/ (tp + fn)\nf1_score(y_test, y_pred)\n\ndf_cm = pd.DataFrame(cm, index = (0, 1), columns = (0, 1))\nplt.figure(figsize = (10,7))\nsns.set(font_scale=1.4)\nsns.heatmap(df_cm, annot=True, fmt='g')\nprint(\"Test Data Accuracy: %0.4f\" % accuracy_score(y_test, y_pred))\nprint(\"Precission score: %0.4f\" % precision_score(y_test, y_pred))\nprint(\"Recall score: %0.4f\" % recall_score(y_test, y_pred))\nprint(\"F1 score: %0.4f\" %f1_score(y_test, y_pred))","4cd6a99f":"from sklearn.model_selection import cross_val_score\naccuracies = cross_val_score(estimator = classifier,\n                             X = X_train[X_train.columns[rfe.support_]],\n                             y = y_train, cv = 10)\nprint(\"SVM Accuracy: %0.3f (+\/- %0.3f)\" % (accuracies.mean(), accuracies.std() * 2))","a67e5815":"# Analyzing Coefficients\npd.concat([pd.DataFrame(X_train[X_train.columns[rfe.support_]].columns, columns = [\"features\"]),\n           pd.DataFrame(np.transpose(classifier.coef_), columns = [\"coef\"])\n           ],axis = 1)","ef8d4f7a":"# Formatting Final Results\nfinal_results = pd.concat([y_test, user_id], axis = 1).dropna()\nfinal_results['predicted_churn'] = y_pred\nfinal_results = final_results[['userid', 'churn', 'predicted_churn']].reset_index(drop=True)\nprint(final_results)","440997ab":"# Minimizing the Churn Rate Through Analysis of Financial Habits \n\n# Table of Contents: \n* [1-Exploratory Data Analysis](#eda)\n* [2-Preprocessing the data](#preprocessing)\n* [3-Training the model](#training)\n* [4-Evaluating the model](#evaluation)\n* [5-Feature selection](#features)\n    * [5.1-Evaluating the new model](#evaluation2)\n* [6-Conclusions](#conclusions)\n","b7ec8c28":"# Training the model <a class='anchor' id='training'><\/a>","90615c93":"# Preprocessing the data <a class='anchor' id='preprocessing'><\/a>","bda23e10":"# Feature selection <a class='anchor' id='features'><\/a>","fcb0a2cd":"# Evaluating the model <a class='anchor' id='evaluation2'><\/a>","d930a8d2":"# Exploratory Data Analysis <a class='anchor' id='eda'><\/a>","18cca1a2":"# Conclusions <a class='anchor' id='conclusions'><\/a>\n\n### The model has around the 65% of accuracy. The next steps to improve the score could be: \n### 1- Try with another method such as XGBoost.\n### 2- Select or discard other features.\n### 3- Do oversampling or undersampling to the dataset.","4ff0d9d4":"# Evaluating the model <a class='anchor' id='evaluation'><\/a>"}}