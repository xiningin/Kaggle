{"cell_type":{"bbefdd49":"code","51ee55d9":"code","1e7feb99":"code","8b2882c4":"code","8e894e65":"code","19296b6a":"code","671713f4":"code","da3e4b3c":"code","28f0b2fb":"code","30fa579a":"code","716d43f9":"code","2f646846":"code","7fc44097":"code","d662ec73":"code","e0fc7e18":"code","d7c0bd87":"markdown","1a7c7a89":"markdown"},"source":{"bbefdd49":"# check what's in the current directory\n!ls","51ee55d9":"import os\nfrom pathlib import Path\nimport shutil\n\nreadonly_checkpoint_dir = Path('\/kaggle\/input\/cmdwheatdet\/torch_cache\/hub\/checkpoints')\nreadwrite_checkpoint_dir = Path('\/root\/.cache\/torch\/hub\/checkpoints')\nif not (readwrite_checkpoint_dir).is_dir():\n    os.makedirs(readwrite_checkpoint_dir)\ncheckpoint_path = sorted(readonly_checkpoint_dir.glob('*.pth'))[0]\nshutil.copy(checkpoint_path, readwrite_checkpoint_dir)","1e7feb99":"# I appreciate that when you add a Kaggle dataset, it unzips and untars everything, but it's actually inconvenient\n# for me in this situation where `pip download` give you a tar.gz file and not a wheel.\n# seems like `pip install --find-links <directory-path>` doesn't work when the install file, normally .tar.gz, is unzipped.\n# anyway, this feels silly, but we can fix it by tarring up the untarred files for configobj.\n!mkdir \/kaggle\/working\/deps\/\n!cd \/kaggle\/input\/cmdwheatdet\/deps\/deps\/configobj-5.0.6\/ && tar -czf \/kaggle\/working\/deps\/configobj-5.0.6.tar.gz configobj-5.0.6\/","8b2882c4":"!pip install --no-index --find-links \/kaggle\/input\/cmdwheatdet\/deps\/deps --find-links \/kaggle\/working\/deps\/ cmd-wheat-det","8e894e65":"import pytorch_lightning as pl\npl.__version__","19296b6a":"import os\nfrom pathlib import Path\nimport pprint\n\nimport numpy as np\nimport torch\n\nfrom wheat.config import load_config\nfrom wheat.scripts import train, evaluate, predict","671713f4":"# load the default configuration file\nconfig = load_config()","da3e4b3c":"config['numpy_seed'] = 1234\nconfig['data_dir'] = '\/kaggle\/input\/global-wheat-detection'","28f0b2fb":"pp = pprint.PrettyPrinter(indent=2)\npp.pprint(config)","30fa579a":"!nvidia-smi","716d43f9":"# set pytorch lightning flags here\npl_args_dict = dict(\n    max_epochs=30,\n    gpus=1,\n)","2f646846":"# run training!\ntrain.train(config, pl_args_dict)","7fc44097":"# find the last model checkpoint\ncheckpoint_dir = Path('lightning_logs\/version_0\/checkpoints')\ncheckpoint_files = checkpoint_dir.glob('*.ckpt')\n# there should only be one checkpoint file saved, so just get the first match\ncheckpoint_path = next(checkpoint_files)\ncheckpoint_path","d662ec73":"# run evaluation\n# if this environment variable is set, detections and ground truth annotations\n# will be saved to .csv files for easy loading and analysis later on\nos.environ['CMD_WHEAT_OUTPUT_DIR'] = str(checkpoint_dir.parent)\nevaluate.evaluate(config, {'gpus': 1}, checkpoint_path)","e0fc7e18":"# run inference\npredict.predict(config, pl_args_dict, checkpoint_path)","d7c0bd87":"## Set up pre-trained weights\nWith internet enabled, PyTorch will check the cache for pre-trained weights as needed and, if the required file is not found, automatically download it. To run without internet, the pre-trained checkpoint file must exist locally, so I've uploaded a pre-trained ResNet-50 backbone as part of my private dataset. We can copy the file to the default torch home location `~\/.cache\/torch`, and PyTorch will find it there instead of needing to download it.","1a7c7a89":"## Install dependencies\nI'm installing my little Python package that I put together for this Kaggle wheat detection challenge. The source code is here: https:\/\/github.com\/sheromon\/wheat-detection."}}