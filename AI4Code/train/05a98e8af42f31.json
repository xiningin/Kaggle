{"cell_type":{"7d98a043":"code","ba1ddfdf":"code","b0e5c1fb":"code","daf9409b":"code","ba7d4234":"code","7682a732":"code","80fb22b8":"code","9a45d1cd":"code","c5b9b801":"markdown"},"source":{"7d98a043":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import StandardScaler,OneHotEncoder\nimport seaborn as sns\nfrom sklearn.linear_model import LinearRegression\n\n#import libaries to transform our features \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\n\n","ba1ddfdf":"# Read the data into csvs and see if there's null values within the data\nTRAIN_DATA=\"\/kaggle\/input\/cais-exec-team-in-house\/train.csv\"\nSUBMISSIONS_DATA=\"\/kaggle\/input\/cais-exec-team-in-house\/sampleSubmission.csv\"\nTEST_DATA=\"\/kaggle\/input\/cais-exec-team-in-house\/test.csv\"\ndf=pd.read_csv(TRAIN_DATA,index_col='id')\ntest_df=pd.read_csv(TEST_DATA,index_col='id')\nsub_df=pd.read_csv(SUBMISSIONS_DATA,index_col='id')\ndf.info()","b0e5c1fb":"# See descriptive statistics of the data\ndf.describe()\n","daf9409b":"# look at the distribution of features and their correlations with respect to the target value\n%matplotlib inline\ndf.hist(bins=20 , figsize=(20,15))\nplt.show()\n# heat map of correlation of features\ncorrelation_matrix = df.corr()\nfig = plt.figure(figsize=(12,9))\nsns.heatmap(correlation_matrix,vmax=0.8,square = True)\nplt.show()","ba7d4234":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnum_attribs=list(df.select_dtypes(include=numerics))\nprint(num_attribs)\nnum_attribs.remove(\"grade\")\ncat_attribs=list(df.select_dtypes(exclude=numerics))\nnum_pipline=make_pipeline(StandardScaler())\nfull_pipeline=make_column_transformer(\n(num_pipline,num_attribs),\n(OneHotEncoder(),cat_attribs))\n\n\nX=df.drop(columns=\"grade\")\nfull_pipeline=full_pipeline.fit(X)\nX=full_pipeline.transform(X)\ny=df.grade\n","7682a732":"from sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nmodels=[\n    LinearRegression(),\n    DecisionTreeRegressor(),\n    RandomForestRegressor() \n]\nfor model in models:\n    scores= cross_val_score(model,X,y,scoring=\"neg_mean_squared_error\",cv=5)\n    real_scores=np.sqrt(-scores)\n    print(f\"The scores for {model.__class__.__name__} were {real_scores} and the average was {np.average(real_scores)}\")\n    print(\"-------------------------------------------------\")\n","80fb22b8":"\nbestModel=RandomForestRegressor()\nbestModel.fit(X,y)\n\ntest_X=full_pipeline.transform(test_df)\npredictions=bestModel.predict(test_X)","9a45d1cd":"sub_df.grade=predictions\nsub_df.to_csv(\"predictions.csv\",index=True)","c5b9b801":"<h1>Thoughts<\/h1>\n* As you can see I didn't do anything special with my model. \n* I'm sure that you're aware that I have a decent test score . But I believe I got lucky especially when you notice that your testing scores are significantly higher than what i got in the submission board\n\n# Things you may want to try out\n* Try out K-NN \n* Using a Neural network \n* Create a bunch of different models and ensemble them together for the final score \n* Hyperparmeter Tuning \n* Feature selection \n* Feature extraction \n\n\n# Please Beat my Score\n\n"}}