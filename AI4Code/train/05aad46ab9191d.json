{"cell_type":{"537a5899":"code","3469b5dc":"code","d203e9e0":"code","f10085da":"code","06f2d787":"code","72b20c68":"code","3a0e6ef6":"code","426c17c6":"code","a4865f98":"markdown","5dc92ff2":"markdown","62d13309":"markdown","50bb164c":"markdown","609fe9cf":"markdown","21020848":"markdown","67a1dd47":"markdown"},"source":{"537a5899":"%matplotlib inline","3469b5dc":"import numpy as np \nimport pandas as pd\nimport matplotlib as mpl \nimport matplotlib.pyplot as plt \n\nfrom sklearn.linear_model import Lasso, LassoCV, Ridge, RidgeCV\nfrom sklearn.model_selection import cross_val_predict, train_test_split\n\nfrom yellowbrick.regressor import AlphaSelection, PredictionError, ResidualsPlot\n\nmpl.rcParams['figure.figsize'] = (9,6)","d203e9e0":"# Load the data into a pandas dataframe\ndf = pd.read_csv('..\/input\/Concrete_Data_Yeh.csv')\n\n# Rename the columns\ndf.columns = ['cement', 'slag', 'ash', 'water', 'splast', 'coarse', 'fine', 'age', 'strength']","f10085da":"# Extract the X and y data from the DataFrame \nX = df.drop('strength', axis=1)\ny = df['strength']\n\n# Create the train and test data \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","06f2d787":"# Instantiate the linear model and visualizer # Insta \nmodel = Ridge()\nvisualizer = ResidualsPlot(model)\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data \ng = visualizer.poof()             # Draw\/show\/poof the data","72b20c68":"# Instantiate the linear model and visualizer # Insta \nmodel = Ridge()\nvisualizer = ResidualsPlot(model, hist=False)\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data \ng = visualizer.poof()             # Draw\/show\/poof the data","3a0e6ef6":"# Instantiate the linear model and visualizer\nmodel = Lasso()\nvisualizer = PredictionError(model)\n\nvisualizer.fit(X_train, y_train)  # Fit the training data to the visualizer\nvisualizer.score(X_test, y_test)  # Evaluate the model on the test data \ng = visualizer.poof()             # Draw\/show\/poof the data","426c17c6":"# Create a list of alphas to cross-validate against \nalphas = np.logspace(-10, 1, 400)\n\n# Instantiate the linear model and visualizer \nmodel = LassoCV(alphas=alphas, cv=3)\nvisualizer = AlphaSelection(model)\n\nvisualizer.fit(X, y)              # Fit the data to the visualizer\ng = visualizer.poof()             # Draw\/show\/poof the data","a4865f98":"## Residuals Plot\nA residual is the difference between the observed value of the target variable (`y`) and the predicted value (`\u0177`), i.e. the error of the prediction. The `ResidualsPlot` Visualizer shows the difference between residuals on the vertical axis and the dependent variable on the horizontal axis, allowing you to detect regions within the target that may be susceptible to more or less error.\n\nIf the points are randomly dispersed around the horizontal axis, a linear regression model is usually well-suited for the data; otherwise, a non-linear model is more appropriate. The following example shows a fairly random, uniform distribution of the residuals against the target in two dimensions. This seems to indicate that our linear model is performing well.","5dc92ff2":"#### Load the Data\n\nThe concrete dataset contains 1030 instances and 9 attributes. Eight of the attributes are explanatory variables, including the age of the concrete and the materials used to create it, while the target variable strength is a measure of the concrete's compressive strength (MPa).","62d13309":"\nYellowbrick's `ResidualsPlot` Visualizer also displays a histogram of the error values along the right-hand side. In the example above, the error is normally distributed around zero, which also generally indicates a well-fitted model. If the histogram is not desired, it can be turned off with the `hist=False` flag.","50bb164c":"\n## Alpha Selection Visualizer\nThe `AlphaSelection` Visualizer demonstrates how different values of alpha influence model selection during the regularization of linear models. Since regularization is designed to penalize model complexity, the higher the alpha, the less complex the model, decreasing the error due to variance (overfit). However, alphas that are too high increase the error due to bias (underfit). Therefore, it is important to choose an optimal alpha such that the error is minimized in both directions.\n\nTo do this, typically you would you use one of the \"RegressionCV\u201d models in scikit-learn. E.g. instead of using the `Ridge` (L2) regularizer, use `RidgeCV` and pass a list of alphas, which will be selected based on the cross-validation score of each alpha. This visualizer wraps a \u201cRegressionCV\u201d model and visualizes the alpha\/error curve. If the visualization shows a jagged or random plot, then potentially the model is not sensitive to that type of regularization and another is required (e.g. L1 or `Lasso` regularization).","609fe9cf":"Yellowbrick also offers Classification, Clustering, Feature Analysis, Model Selection, and Text Modeling Visualizers. You can learn more about them by reading their [documentation](http:\/\/www.scikit-yb.org\/en\/latest\/index.html) and checking out my other Yellowbrick notebooks for more examples and customization tips.","21020848":"## Prediction Error Plot\nYellowbrick's `PredictionError` Visualizer plots the actual targets from the dataset against the predicted values generated by the model. This allows us to see how much variance is in the model. Data scientists can diagnose regression models using this plot by comparing against the 45-degree line, where the prediction exactly matches the model.","67a1dd47":"\n# Yellowbrick &mdash; Regression Examples\n\nThe Yellowbrick library is a diagnostic visualization platform for machine learning that allows data scientists to steer the model selection process. It extends the scikit-learn API with a new core object: the `Visualizer`. Visualizers allow visual models to be fit and transformed as part of the scikit-learn pipeline process, providing visual diagnostics throughout the transformation of high-dimensional data.\n\nEstimator score visualizers *wrap* scikit-learn estimators and expose the Estimator API such that they have `fit()`, `predict()`, and `score()` methods that call the appropriate estimator methods under the hood. Score visualizers can wrap an estimator and be passed in as the final step in a `Pipeline` or `VisualPipeline`.\n\nIn machine learning, regression models attempt to predict a target in a continuous space. Yellowbrick has implemented the following regressor score visualizers that display the instances in model space to better understand how the model is making predictions:\n- `AlphaSelection` visual tuning of regularization hyperparameters\n- `PredictionError` plot the expected vs. the actual values in the model space\n- `Residuals Plot` plot the difference between the expected and actual values"}}