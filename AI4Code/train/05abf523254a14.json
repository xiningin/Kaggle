{"cell_type":{"b745631f":"code","d84b9087":"code","df854432":"code","1f525599":"code","afe5d018":"code","05cdad93":"code","70a63491":"code","4eb4e843":"code","ce84e6e2":"code","7847992b":"code","2fe6057f":"code","cfb4f71c":"code","059f03e6":"code","0873fafc":"code","8e10b288":"code","d46d967c":"code","b9589944":"code","db9c4475":"code","eb086ef8":"code","5fb1dcd3":"code","5013833f":"code","26ef71cb":"code","6003a85d":"code","f7070c6d":"code","8074e966":"code","e8150ec3":"code","4d3388d0":"code","62b985ba":"code","08368095":"code","2c79020b":"code","25f6de9d":"code","dc060f8a":"code","29ab5e53":"code","e8fd908d":"code","b6adc979":"code","6df7d0e6":"code","020666cd":"code","69ef1301":"code","62ee4cb7":"code","5e85acfc":"markdown","ab8524e9":"markdown","aa9cc1fe":"markdown","9db9e750":"markdown","769be8a1":"markdown","1487e78a":"markdown","b9f123e4":"markdown","c50ac4d0":"markdown","b7338b47":"markdown"},"source":{"b745631f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# visualiser les donn\u00e9es\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n# Any results you write to the current directory are saved as output.\n\nimport warnings\nwarnings.filterwarnings('ignore')","d84b9087":"%matplotlib inline\nsns.set({'figure.figsize':(16,8)})","df854432":"train = pd.read_csv(\"..\/input\/nyc-taxi-duration-eda-by-nguyen-khac-bao-anh\/training_data.csv\")\ntest = pd.read_csv(\"..\/input\/nyc-taxi-duration-eda-by-nguyen-khac-bao-anh\/testing_data.csv\")","1f525599":"print(f\"shape of training set{train.shape}\")\nprint(f\"shape of testing set{test.shape}\")","afe5d018":"train.head()","05cdad93":"test.head()","70a63491":"col_diff = list(set(train.columns).difference(set(test.columns)))\nprint(f\"La diff\u00e9rence de la variable entre data training et data testing:\\\n{set(train.columns).difference(set(test.columns))}\")","4eb4e843":"xtrain = train.drop(['id', 'pickup_datetime', 'dropoff_datetime', 'trip_duration', 'log_trip_duration'], axis = 1).as_matrix()\nxtest = test.drop(['id', 'pickup_datetime', ], axis = 1).as_matrix()\ny = train['log_trip_duration'].values\ndel(train, test)","ce84e6e2":"from sklearn.model_selection import train_test_split, cross_val_score","7847992b":"X_train, X_valid, y_train, y_valid = train_test_split(xtrain,y, test_size=0.2, random_state=42)\nX_train.shape, X_valid.shape, y_train.shape, y_valid.shape","2fe6057f":"#from sklearn.ensemble import RandomForestRegressor","cfb4f71c":"#rf_defaut = RandomForestRegressor()\n# crosse validation pour tester si le model est stable \n#rf_cv = cross_val_score(rf_defaut, X_train, y_train, cv=5)\n#rf_cv","059f03e6":"#plt.plot(range(1,len(rf_cv)+1), rf_cv)\n#plt.ylim(np.min(rf_cv)-0.1,np.max(rf_cv)+0.1)\n#plt.xlabel(\"nombre de fold\")\n#plt.ylabel(\"score du model Random Forest Regressor\");","0873fafc":"# la fonction permet de nous donner un score qui est le r\u00e8gle de cette comp\u00e9tition\n# (root mean squared log error)\n#from sklearn.metrics import mean_squared_log_error, mean_squared_error\ndef rmse(y,pred):\n    return np.sqrt(np.mean(np.square(np.log(np.exp(y))-np.log(np.exp(pred)))))","8e10b288":"#rf_defaut = RandomForestRegressor()\n#rf_defaut.fit(X_train, y_train)","d46d967c":"#y_pred = rf_defaut.predict(X_valid)","b9589944":"#print(rmse(y_valid,y_pred))","db9c4475":"#from sklearn.model_selection import GridSearchCV","eb086ef8":"# n_estimators et max_depth pour fitter bien le model mais causer overfitting\n# par contre, min_samples_leaf et min_samples_split nous permets de \u00e9viter overfitting en donnant la valeur\n# plus grand\n#params = {\n#    'n_estimators' : [10, 15, 20],\n#    'max_depth': [30, 50, 100],\n#    'min_samples_leaf': [100],\n#    'min_samples_split': [150]\n#}\n#rf2 = RandomForestRegressor()\n#gs_rf2 = GridSearchCV(rf2, param_grid=params, scoring='neg_mean_squared_error',cv=3, verbose=10, n_jobs=-1)\n#gs_rf2.fit(X_train, y_train)\n#gs_rf2.best_score_\n#best_rf2 = gs_rf2.best_estimator_","5fb1dcd3":"#rf2 = RandomForestRegressor(n_estimators=10,min_samples_leaf=100, min_samples_split=150)","5013833f":"#rf2_cv = cross_val_score(rf2, X_train, y_train, cv=5)\n#rf2_cv","26ef71cb":"#plt.plot(range(1,len(rf2_cv)+1), rf2_cv)\n#plt.ylim(np.min(rf2_cv)-0.1,np.max(rf2_cv)+0.1)\n#plt.xlabel(\"nombre de fold\")\n#plt.ylabel(\"score du model Random Forest Regressor avec hyperparameters\");","6003a85d":"#rf2.fit(X_train, y_train)","f7070c6d":"#pred = rf2.predict(X_valid)","8074e966":"#print(rmse(y_valid,pred))","e8150ec3":"import lightgbm as lgb","4d3388d0":"#lgb_train = lgb.Dataset(X_train, y_train)\n#lgb_valid = lgb.Dataset(X_valid, y_valid)\n# training all dataset\ndtrain = lgb.Dataset(xtrain,y)\ndel(X_train, y_train, X_valid, y_valid, xtrain,y)","62b985ba":"lgb_params = {\n    'learning_rate': 0.1,\n    'max_depth': 8,\n    'num_leaves': 55, \n    'objective': 'regression',\n    #'metric': {'rmse'},\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.5,\n    #'bagging_freq': 5,\n    'max_bin': 300}       # 1000","08368095":"#cv_result_lgb = lgb.cv(lgb_params,\n#                       lgb_train, \n#                       num_boost_round=1000, \n#                       nfold=3,\n#                       early_stopping_rounds=50, \n#                       verbose_eval=100, \n#                       show_stdv=True,stratified=False)","2c79020b":"#n_rounds = len(cv_result_lgb['rmsle-mean'])\n#print('num_boost_rounds_lgb=' + str(n_rounds))","25f6de9d":"# visualisation des r\u00e9sultat dans cv\n# CV scores\n#train_scores = np.array(cv_result_lgb['rmsle-mean'])\n#train_stds = np.array(cv_result_lgb['rmsle-stdv'])\n#plt.plot(train_scores, color='violet')\n#plt.fill_between(range(len(cv_result_lgb['rmsle-mean'])), \n#                 train_scores - train_stds, train_scores + train_stds, \n#                 alpha=0.1, color='violet')\n#plt.title('LightGMB CV-results')\n#plt.xlabel(\"number of rounds\")\n#plt.ylabel(\"score\");","dc060f8a":"# Train a model\n#model_lgb = lgb.train(lgb_params, \n#                      dtrain, \n#                      feval=lgb_rmsle_score, \n#                      num_boost_round=n_rounds)","29ab5e53":"## Predict on train\n#y_train_pred = model_lgb.predict(X_train)\n#print('RMSLE on train = {}'.format(rmse(y_train_pred, y_train)))\n## Predict on validation\n#y_valid_pred = model_lgb.predict(X_valid)\n#print('RMSLE on valid = {}'.format(rmse(y_valid_pred, y_valid)))","e8fd908d":"# Train a model\nmodel_lgb = lgb.train(lgb_params, \n                      dtrain,\n                      num_boost_round=1500)","b6adc979":"submit = pd.read_csv('..\/input\/nyc-taxi-trip-duration\/sample_submission.csv')\nsubmit.head()","6df7d0e6":"pred_test = np.exp(model_lgb.predict(xtest))","020666cd":"submit['trip_duration'] = pred_test","69ef1301":"submit.head()","62ee4cb7":"submit.to_csv(\"submit_file.csv\", index=False)","5e85acfc":"# submission on kaggle","ab8524e9":"# LightGBM pour pr\u00e9dire trip duration","aa9cc1fe":"# Data Loading","9db9e750":"# Select dataset","769be8a1":"## Grid search pour optimiser l'algorithme Random Forest","1487e78a":"# Select model","b9f123e4":"**This kernel is for applicated Machine Learning to predicting trip duration in NYC taxi**\n**Please to check my kernel [NYC EDA](https:\/\/www.kaggle.com\/baoanh\/nyc-taxi-duration-eda-by-nguyen-khac-bao-anh) to understanding this dataset and knowing kind of dataset that i used for this kernel**","c50ac4d0":"> le line chart au-dessus, il nous dit que ce model est stable, c'est \u00e0 dire, ce model est capable de g\u00e9n\u00e9raliser","b7338b47":"### Data Training vs Data testing:"}}