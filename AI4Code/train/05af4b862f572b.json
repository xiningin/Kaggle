{"cell_type":{"cdc27855":"code","7af285e3":"code","5cf66ef0":"code","f0adfffd":"code","3d33daba":"code","6d449cba":"code","93189fc4":"code","18ae4f3b":"code","d5d9d317":"code","fba1b365":"code","8a1904c4":"code","3b2a63d7":"code","117726e8":"code","3bb078dc":"code","494c08e5":"code","5196e2b2":"code","5b3928a6":"code","7f228380":"code","334d9817":"code","d9fb56ce":"code","e9999303":"code","b688d16a":"code","8edca10d":"code","a61aca54":"code","9fb766fc":"code","0518ccde":"code","cec7e1fa":"code","9bbe1429":"code","ca5f4b7a":"code","a7ec308b":"code","f47a2b28":"code","347f688b":"code","9535abb5":"code","cb166333":"code","8cf73cf8":"code","8a4e06a3":"code","2f0777ec":"code","c454413f":"code","3f130b5f":"code","20a1fc5f":"code","baca3df1":"code","51bbb7d3":"code","dfecffc5":"code","48d28e1f":"code","6a9c3dd8":"code","9a30dd7c":"code","531aae36":"markdown","2b2a3a60":"markdown","000f323b":"markdown","2fc24903":"markdown","e58d5ebd":"markdown","56953567":"markdown","c7c0163e":"markdown","33208139":"markdown","d7efbf61":"markdown","cad56dc5":"markdown","cfd3b609":"markdown","d9f3d918":"markdown","0525ad89":"markdown","789f5da4":"markdown","53ea4463":"markdown","6c39f113":"markdown","aac45c04":"markdown","56fdc803":"markdown","f3bd059e":"markdown","a9ec5c29":"markdown","a3b811c8":"markdown","314a5963":"markdown","64c66b25":"markdown","ce56ebbf":"markdown"},"source":{"cdc27855":"#this cell is valid when you're working on google colab and you want to upload the data to colab environment to use in your notebook\n#uploading the data file from your Desktop\n#from google.colab import files\n#files.upload()","7af285e3":"import pandas as pd\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n%matplotlib inline\npd.set_option('display.max_columns',40)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV\nimport plotly.offline as py\npy.init_notebook_mode(connected=False)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n","5cf66ef0":"def configure_plotly_browser_state():\n  import IPython\n  display(IPython.core.display.HTML('''\n        <script src=\"\/static\/components\/requirejs\/require.js\"><\/script>\n        <script>\n          requirejs.config({\n            paths: {\n              base: '\/static\/base',\n              plotly: 'https:\/\/cdn.plot.ly\/plotly-latest.min.js?noext',\n            },\n          });\n        <\/script>\n        '''))","f0adfffd":"#Loading the dataset in Pandas dataframe\ndf_cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf_cancer.head()","3d33daba":"print(df_cancer.columns)\nprint()\nprint(\"Cancer dataset dimensions : {}\".format(df_cancer.shape))\nprint()\nprint(\"Rows:\",df_cancer.shape[0])\nprint()\nprint(\"Columns:\",df_cancer.shape[1])","6d449cba":"df_cancer = df_cancer.drop('Unnamed: 32',axis=1)","93189fc4":"print(df_cancer.columns)\ndf_cancer.head()","18ae4f3b":"df_cancer.describe().T","d5d9d317":"print(df_cancer.isnull().any().any())","fba1b365":"configure_plotly_browser_state()\ntrace = go.Pie(labels = ['benign','malignant'], values = df_cancer['diagnosis'].value_counts(), \n               textfont=dict(size=10), opacity = 0.7,\n               marker=dict(colors=['green', 'red'], \n               line=dict(color='#000000', width=1.0)))\n           \n\nlayout= go.Layout(\n        title={\n        'text': \"Distribution of dependent(diagnosis) variable\",\n        'y':0.8,\n        'x':0.45,\n        'xanchor': 'center',\n        'yanchor': 'top'})\n\nfig = go.Figure(data = [trace], layout=layout)\nfig.show()","8a1904c4":"df_cancer['diagnosis']= df_cancer['diagnosis'].map({'M':1,'B':0})\ndf_cancer.head()","3b2a63d7":"df_cancer['diagnosis'].value_counts()","117726e8":"mal = df_cancer[(df_cancer['diagnosis'] != 0)]\nprint(mal.shape)\nben = df_cancer[(df_cancer['diagnosis'] == 0)]\nprint(ben.shape)\ndef show_plots(column, bin_size) :  \n    t1 = mal[column]\n    t2 = ben[column]\n    \n    hist_data = [t1, t2]\n    \n    group_labels = ['Malignant', 'Benign']\n    colors = ['red', 'green']\n\n    fig = ff.create_distplot(hist_data, group_labels, colors = colors, show_hist = True, bin_size = bin_size, curve_type='kde')\n    \n    fig['layout'].update(title = column)\n    fig.show()","3bb078dc":"configure_plotly_browser_state()\nshow_plots('radius_mean', .3)\nshow_plots('texture_mean', .3)\nshow_plots('perimeter_mean',3)\nshow_plots('area_mean',20)\n","494c08e5":"configure_plotly_browser_state()\nshow_plots('radius_se', 0.1)\nshow_plots('texture_se', .1)\nshow_plots('perimeter_se', .5)\nshow_plots('area_se', 5)\n","5196e2b2":"configure_plotly_browser_state()\nshow_plots('radius_worst', .5)\nshow_plots('texture_worst', .5)\nshow_plots('perimeter_worst', 5)\nshow_plots('area_worst', 15)\n","5b3928a6":"plt.figure(figsize=(25,12))\nsns.heatmap(df_cancer.corr(),annot=True)","7f228380":"sns.scatterplot(x='area_mean',y='smoothness_mean',hue='diagnosis',data=df_cancer)","334d9817":"features = ['radius_mean', 'texture_mean', 'perimeter_mean',\n       'area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean',\n       'radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se',\n       'compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst',\n       'perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst',\n       'symmetry_worst', 'fractal_dimension_worst']\nlen(features)","d9fb56ce":"X =df_cancer[features].values\ny =df_cancer['diagnosis']\nprint(X.shape)\nprint(y.shape)","e9999303":"X_train, X_val, y_train, y_val = train_test_split(X,y,test_size=0.3,random_state=22,stratify=y)\nprint(\"Shape of train dataset:\")\nprint(X_train.shape)\nprint(y_train.shape)\nprint(\"\\n\")\nprint(\"Shape of val dataset:\")\nprint(X_val.shape)\nprint(y_val.shape)\nprint(\"\\n\")","b688d16a":"from sklearn.ensemble import RandomForestClassifier\n\nmodel1 = RandomForestClassifier(max_depth=1, random_state=0, verbose=0,n_estimators=50)\nmodel1.fit(X_train,y_train)","8edca10d":"y_pred1 = model1.predict(X_val)","a61aca54":"cnf1 = confusion_matrix(y_val,y_pred1)\nsns.heatmap(cnf1,annot=True,cmap='summer',fmt='g')","9fb766fc":"acc1 = accuracy_score(y_val,y_pred1)\nprint(\"Accuracy: for baseline model is: %0.3f\"%acc1)\n\nprint(\"RF train accuracy: %0.3f\" % model1.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model1.score(X_val, y_val))","0518ccde":"print(classification_report(y_val,y_pred1))","cec7e1fa":"coef1= model1.feature_importances_\nprint(coef1.shape)\nprint(len(features))\ncoefs1 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef1})\nfeature_imp1 = coefs1.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp1)","9bbe1429":"param_grid={'n_estimators':[50,100,150,200,250],\n            'max_depth':[1,2,3,4],\n            'min_samples_split':[2,3,5],\n            'max_features':['auto','sqrt','log2']}","ca5f4b7a":"model2= GridSearchCV(RandomForestClassifier(),param_grid,refit=True,verbose=0,n_jobs=-1)\nmodel2.fit(X_train,y_train)","a7ec308b":"print(model2.best_params_)\ny_pred2 = model2.predict(X_val)","f47a2b28":"cnf2 = confusion_matrix(y_val,y_pred2)\nsns.heatmap(cnf2,annot=True,fmt='g',cmap='Blues')","347f688b":"acc2 = accuracy_score(y_val,y_pred2)\nprint(\"Accuracy with GridSearch: %0.3f\"%acc2)\n\nprint(\"RF train accuracy: %0.3f\" % model2.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model2.score(X_val, y_val))","9535abb5":"print(classification_report(y_val,y_pred2))","cb166333":"coef2= model2.best_estimator_.feature_importances_\nprint(coef2.shape)\nprint(len(features))\ncoefs2 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef2})\nfeature_imp2 = coefs2.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp2)","8cf73cf8":"from sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import ExtraTreesClassifier\nmodel3 = Pipeline([\n  ('feature_selection', SelectFromModel(ExtraTreesClassifier(n_estimators=50))),\n  ('classification', RandomForestClassifier())\n])\nmodel3.fit(X_train, y_train)","8a4e06a3":"y_pred3 = model3.predict(X_val)\ncnf3 = confusion_matrix(y_val,y_pred3)\nsns.heatmap(cnf3,annot=True,cmap='summer',fmt='g')\nacc3 = accuracy_score(y_val,y_pred3)\nprint(\"Accuracy on Model3 is: %0.3f\"%acc3)\nprint(\"RF train accuracy: %0.3f\" % model3.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model3.score(X_val, y_val))","2f0777ec":"print(classification_report(y_val,y_pred3))","c454413f":"#Feature Importance\nf1 = model3.steps[0][1].get_support()\nnew_f = [features[i] for i,val in enumerate(f1) if val==True]\nprint(new_f)\ncoef3 = model3.steps[1][1].feature_importances_\nprint(coef3.shape)\nprint(len(new_f))\ncoefs3 = pd.DataFrame({\"Features\":new_f,\"Coefficients\":coef3})\nfeature_imp3 = coefs3.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp3)","3f130b5f":"from xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nmodel4 = XGBClassifier()\nmodel4.fit(X_train, y_train)","20a1fc5f":"y_pred4 = model4.predict(X_val)\ncnf4 = confusion_matrix(y_val,y_pred4)\nsns.heatmap(cnf4,annot=True,cmap='summer',fmt='g')\nacc4 = accuracy_score(y_val,y_pred4)\nprint(\"Accuracy on Model3 is: %0.3f\"%acc4)\nprint(\"RF train accuracy: %0.3f\" % model4.score(X_train, y_train))\nprint(\"RF test accuracy: %0.3f\" % model4.score(X_val, y_val))","baca3df1":"print(classification_report(y_val,y_pred4))","51bbb7d3":"coef4= model4.feature_importances_\nprint(coef4.shape)\nprint(len(features))\ncoefs4 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef4})\nfeature_imp4 = coefs4.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp4)","dfecffc5":"from lightgbm import LGBMClassifier\nmodel5 = LGBMClassifier()\nmodel5.fit(X_train, y_train)","48d28e1f":"y_pred5 = model5.predict(X_val)\ncnf5 = confusion_matrix(y_val,y_pred5)\nsns.heatmap(cnf5,annot=True,cmap='summer',fmt='g')\nacc5 = accuracy_score(y_val,y_pred5)\nprint(\"Accuracy on Model5 is: %0.3f\"%acc5)\nprint(\"Ligtgbm train accuracy: %0.3f\" % model5.score(X_train, y_train))\nprint(\"LightGBM test accuracy: %0.3f\" % model5.score(X_val, y_val))","6a9c3dd8":"print(classification_report(y_val,y_pred5))","9a30dd7c":"coef5= model5.feature_importances_\nprint(coef5.shape)\nprint(len(features))\ncoefs5 = pd.DataFrame({\"Features\":features,\"Coefficients\":coef5})\nfeature_imp5 = coefs5.sort_values(by='Coefficients',ascending=False)\nplt.figure(figsize=(15,10))\nsns.barplot(y='Features',x='Coefficients',data=feature_imp5)","531aae36":"### Model5. Light GBM","2b2a3a60":"## Model Building","000f323b":"#### Splitting the data into train and val to build the model on train and validate it on val data.","2fc24903":"***`We were able to increase our performance of 94% on val data(baseline model) to 98.8% on val data just by using LightGBM without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.\nWith only around 10 lines of code you can achieve the 98.8% accuracy on val data using LightGBM\nModel Performance of XGboost and LightGBM are similar.`***","e58d5ebd":"### checking for any missing value in data, if there are any missing value we will be doing missing value imputation","56953567":"#### Model Building","c7c0163e":"***`We were able to increase our performance of 94% on val data(baseline model) to 98.8% on val data just by using Xgboost without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.\nWith only around 10 lines of code you can achieve the 98.8% accuracy on val data using Xgboost`***","33208139":"## Loading all the relevant libraries","d7efbf61":"It is always a good practice to see some **stats(mean,median,percentiles)** of all the variables involve, and pandas has a describe() functions especially for this purpose.\nWe are doing Transpose of the describe() output since we have almost 30 columns to see.\nThis can also be used to see outliers without using any plot.","cad56dc5":"### Model2. Random Forest using Parameter Tuning\nWe will use grid search to tune the parametrs of Random Forest","cfd3b609":"### Model1. Baseline Model\nWe will build a model by using all the variables present in our model using random forest classifier. This will be our baseline model which we will try to beat by using feature selection and also by changing the classifier to xgboost.","d9f3d918":"number of benign classes are much more than malignant","0525ad89":"#### Model Evaluation","789f5da4":"***`We were able to increase our performance of 94% on val data to 98.8% on val data just by introducing paramter tuning using Grid Search CV.`***","53ea4463":"There is one column in the end which is random so we will drop this column","6c39f113":"Let's see what we have in our dependent variable(**diagnosis**). Here we are using graph objects of plotly library.","aac45c04":"And there are no missing records in the given data.\nWell done.\nLet's move ahead.","56fdc803":"## Exploratory Data Analysis(EDA)","f3bd059e":"### Model4. Xgboost","a9ec5c29":"***`We were able to increase our performance of 94% on val data to 98.2% on val data just by using Feature Selection of Extratree classifier. Here we are using 11 features to train our model that too without any paramter tuning and we are able to match the Model2 performance where we used 31 variables and parameter tuning.`***","a3b811c8":"Mapping the categories of dependent variable to 1 and 0. We will be predicting whether the record is malignant cancer or not so it makes sense to tag malignant as 1","314a5963":"### Model3.  Using Feature Selection\nAdding the feature selection before feeding all the variables to any model.","64c66b25":"heatmap is a good visualtization plot to see the corrleation among vaiables and there is not point of feeding highly correlated variables into any ML model because we are not providing any extra information through that variables and we are adding a complexity to any ML model by adding 1 variable. We want out model to as generic and simpleas possible.","ce56ebbf":"## Data Overview"}}