{"cell_type":{"aeed5c2c":"code","eaa2c55f":"code","b0937c8f":"code","6fe56d05":"code","e6a5e557":"code","edc0facd":"code","125bb29a":"code","8e38b565":"code","062a38e4":"code","caf04320":"code","1b19eb68":"code","f8c6eb9d":"code","b07b5f9c":"code","0ad96463":"code","652e50e6":"code","f13acf3c":"code","7dbd8826":"code","5abe092a":"code","f8e3a4af":"code","5bbf2c87":"code","c6a14169":"code","896b5159":"code","975d8f66":"markdown","efa1009f":"markdown","00a5d47d":"markdown","1200eaa7":"markdown","0825f43e":"markdown","dd050b4b":"markdown","e7616ff5":"markdown","ebaa0ee3":"markdown","3850bec5":"markdown","a639d47d":"markdown","51dcbb7c":"markdown","7fba1fc2":"markdown","3d6ac1ae":"markdown","397a01e5":"markdown","7f5bf71e":"markdown","9d0835e3":"markdown","71f1cd5b":"markdown","96d1de71":"markdown"},"source":{"aeed5c2c":"import pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.stats import shapiro\nfrom scipy.stats import anderson\nfrom scipy.stats import normaltest\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport re\nimport warnings\nfrom pandas.api.types import is_string_dtype\nfrom pandas.api.types import is_numeric_dtype\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\ndata = pd.read_csv('..\/input\/data-scientist-jobs\/DataScientist.csv')\ndata.head()","eaa2c55f":"data = data.drop('Unnamed: 0', 1)\ndata = data.drop('index', 1)\n\nprint(data.shape)\nprint(data.columns)","b0937c8f":"def count_missing_values():\n    for column in data:\n        nullAmount = None\n        if (is_numeric_dtype(data[column])):\n            nullAmount = data[data[column] == -1].shape[0]\n        else:\n            nullAmount = data[data[column] == \"-1\"].shape[0]\n        print('{}{},  \\t{:2.1f}%'.format(column.ljust(20),nullAmount, nullAmount*100\/data[column].shape[0]))\n    \ncount_missing_values()","6fe56d05":"data = data.drop('Competitors', 1)\ndata = data.drop('Easy Apply', 1)","e6a5e557":"data = data.replace(-1, np.nan)\ndata[\"Rating\"].interpolate(method='linear', direction = 'forward', inplace=True) \n\ndata.drop(data[data['Headquarters'] == \"-1\"].index, inplace=True)\ndata.drop(data[data['Size'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Type of ownership'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Revenue'].str.contains(\"-1\")].index, inplace=True)\nprint(data.shape)\ncount_missing_values()","edc0facd":"data.drop(data[data['Sector'].str.contains(\"-1\")].index, inplace=True)\ndata.drop(data[data['Industry'].str.contains(\"-1\")].index, inplace=True)\nprint(data.shape)\ncount_missing_values()","125bb29a":"data['Job Title'].value_counts()","8e38b565":"data =  data[data['Job Title'].str.contains(\"Data Scientist\") | data['Job Title'].str.contains(\"Data Analyst\")]\nprint(data.shape)","062a38e4":"HOURS_PER_WEEK = 40\nWEEKS_PER_YEAR = 52\nTHOUSAND = 1000\n\ndef return_digits(x):\n    result = re.findall(r'\\d+', str(x))\n    result = int(result[0]) if result else 0\n    return result\n\ndef return_salary(string, isFrom):\n    patternMain = None\n    patternPerHour = None\n    if(isFrom):\n        patternMain = r'^\\$\\d+K';\n        patternPerHour = r'^\\$\\d+';\n    else:\n        patternMain = r'-\\$\\d+K';\n        patternPerHour = r'-\\$\\d+';\n    \n    result = None\n    if('Per Hour' in string):\n        result = re.findall(patternPerHour, str(string))\n        result = return_digits(result[0]) if result else 0\n        result = result * HOURS_PER_WEEK * WEEKS_PER_YEAR\n    else:\n        result = re.findall(patternMain, str(string))\n        result = return_digits(result[0]) if result else 0\n        result = result * THOUSAND\n    return result\n\ndef return_average_salary(x):\n    from_salary = return_salary(x, True)\n    to_salary = return_salary(x, False)\n    result = (from_salary+to_salary)\/2\n    return result\n\ndata['SalaryAverage'] =  data['Salary Estimate'].apply(return_average_salary)\nprint(data['SalaryAverage'].describe())\nprint(sns.distplot(data['SalaryAverage']))","caf04320":"#SalaryAverage\/Rating plot\nprint(sns.pairplot(x_vars=[\"Rating\"], y_vars=[\"SalaryAverage\"],data=data,  size=5))","1b19eb68":"#SalaryAverage\/Sector plot\nprint(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"Sector\"],data=data,  size=5))","f8c6eb9d":"#SalaryAverage\/Location plot\nprint(sns.pairplot(x_vars=[\"Location\"], y_vars=[\"SalaryAverage\"],data=data,  size=5))","b07b5f9c":"def return_state(string):\n    patternMain = r',\\s[A-Z]{2}';    \n    result = re.findall(patternMain, str(string))\n    if result:\n        result = re.findall(r'[A-Z]{2}', str(result[0]))[0]\n    else:\n        result = string.split(r', ')[1]\n    return result\n\ndata['State'] =  data['Location'].apply(return_state)\nprint(data['State'].head())\nprint(data['State'].value_counts())\nprint(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"State\"],data=data,  size=5))","0ad96463":"dataBiggerSalary = data[data['State'].isin(['NY', 'NJ', 'CA'])] \nprint(sns.distplot(dataBiggerSalary['SalaryAverage'], fit=norm))\nprint(dataBiggerSalary.shape)","652e50e6":"from scipy.stats import norm, expon, cauchy\ndataSmallerSalary = data[~data['State'].isin(['TX', 'NY', 'NJ', 'CA'])] \nprint(dataSmallerSalary.shape)\nprint(sns.distplot(dataSmallerSalary['SalaryAverage']))","f13acf3c":"print(sns.pairplot(x_vars=[\"SalaryAverage\"], y_vars=[\"State\"],data=dataBiggerSalary,  size=5))\nprint(dataBiggerSalary.boxplot(by ='State', column =['SalaryAverage']))\nprint(dataBiggerSalary[\"SalaryAverage\"].describe())","7dbd8826":"dataBiggerSalary.drop(dataBiggerSalary[dataBiggerSalary['SalaryAverage'] < 75000].index, inplace=True)\nprint(dataBiggerSalary.shape)\nprint(sns.distplot(dataBiggerSalary['SalaryAverage'], fit=norm))","5abe092a":"def testNormality(data):\n    stat, p = shapiro(data)\n    print('Statistics=%.3f, p=%.3f' % (stat, p))\n    alpha = 0.05\n    if p > alpha:\n        print('Sample looks Gaussian (fail to reject H0)')\n    else:\n        print('Sample does not look Gaussian (reject H0)')\n        \ntestNormality(dataBiggerSalary['SalaryAverage'])","f8e3a4af":"print(data.columns)\nprint(sns.countplot(y='Sector',data=data, order = data['Sector'].value_counts().index))","5bbf2c87":"print(sns.countplot(y='State',data=data, order = data['State'].value_counts().index))","c6a14169":"print(sns.countplot(y='Size',data=data, order = data['Size'].value_counts().index))\nprint(data[\"Company Name\"].value_counts())","896b5159":"plt.figure(figsize=(15,16))\nprint(sns.countplot(x='Rating',data=data, order = data['Rating'].value_counts().index))","975d8f66":"It looks like we don't need columns: \"Unnamed: 0\" and \"index\"","efa1009f":"First import all the libraries we need and read the dataset:","00a5d47d":"We can fill missing values for the \"Rating\" column using interpolation of the values we already had. I think we can just remove rows with missing values from columns like \"Headquarters\", \"Size\", \"Type of ownership\" and \"Revenue\". Let's do it and take one more looks at how many missing values we still have:","1200eaa7":"I see that in some columns we have \"-1\" value, which can be interpreted as null value, so let's check how many \"null\" values we have in every column","0825f43e":"## Exploratory Data Analysis","dd050b4b":"So now we have filled dataframe with 13 columns and 3356 rows. Let's take a look at what we have there, let's do the EDA.\n\nFirst let's see which job positions we have:","e7616ff5":"We can see that columns like \"Competitors\" and \"Easy Apply\" has 70.6% and 95.8% of null-values, so we can just delete this columns.","ebaa0ee3":"The average salary distribution in other states then NY, NJ and CA, looks different then normal, more like It looks like the average salary in the NY, NJ and CA states has an approximately bell-shape and can be  normal distributed, but it also could have an outliers. Let's check the outliers in the average salary in the NY, NJ and CA: ","3850bec5":"The plot above shows that most of the jobs position in the states like NY, NJ, CA are probably spread around bigger salary then in other places. Let's split data by state and see how our distribution will change. For example, let's take a look at average salary distribution in the states mentioned above:","a639d47d":"The plots above shows us that this average salary distribution can be explained by location. Let's check it. First, let's get states or country where jobs are located:","51dcbb7c":"Next let's parse 'Salary Estimate' column to 'SalaryAverage' one:","7fba1fc2":"Now we have only 8.4% of missing values in columns \"Industry\" and \"Sector\", it's relatively small value so I think we can also just remove this rows. ","3d6ac1ae":"Let's check normality with Shapiro Wilk test (as it's the most powerful test when testing for a normal distribution):","397a01e5":"So the results show that the average salary in this states (NY, NJ and CA) is not normal distributed.\n\nLet's see maybe we can find some interesting and usefull information from the data other then salary in our dataset: ","7f5bf71e":"We can see that the salary less then 75000 can be outliers. Let's remove it:","9d0835e3":"From the plots above we can see that most of the job positions is in IT and Business Services sectors, from companies like IBM, Amazon, Apple and Facebook and located in the states like CA and TX.","71f1cd5b":"Let's make analysis for the \"Data Scientist\" jobs:","96d1de71":"The average salary distribution has 2 peaks, lets' try to see what can explain this peaks:"}}