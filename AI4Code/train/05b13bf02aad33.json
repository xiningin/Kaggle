{"cell_type":{"a5fd995b":"code","35bbb11d":"code","0e7cf625":"code","dae69cb6":"code","de238f7c":"code","f57bfd07":"code","107fef2e":"code","7e11628d":"code","2092d86e":"code","4b12a862":"code","b7535ce0":"code","fc398064":"code","da46d1b1":"code","68df5834":"code","cfcdfdb0":"code","a36194b2":"code","047203d2":"code","23d24222":"code","1cd1453a":"code","eca12fb9":"code","a716ad7a":"code","c5476895":"code","536e9080":"code","c1383f1c":"code","bcec2deb":"code","79613ee1":"code","a4981791":"code","b4771ca6":"code","f4b3cc4c":"code","ceae5aec":"code","be27917c":"code","c1b55ab2":"code","94eeaedc":"code","cc6e2e60":"code","4528cddf":"code","acbf7b75":"code","25f802aa":"code","cc64e066":"code","31f57f87":"code","61f8f0e7":"code","8907b570":"code","b5d30091":"code","b493897c":"code","3fca7875":"code","805eae61":"code","670c6c8b":"code","e61cd3a4":"code","22ec693c":"code","8209ed1e":"markdown","098739a5":"markdown","e3e0bbe2":"markdown","a77c3e13":"markdown","988256f8":"markdown","aa4b80fb":"markdown","9c888c86":"markdown","a817a4b7":"markdown","dfddf16e":"markdown","79550869":"markdown","84dc83b1":"markdown","7a8ec8f0":"markdown","50d4cbd1":"markdown","b7fa4036":"markdown","dc8b6d8e":"markdown","3747e1e1":"markdown","1f429aa0":"markdown","8311685f":"markdown","2834c785":"markdown","4906d332":"markdown","f33c547b":"markdown","befa5412":"markdown","5da54ae7":"markdown","e27b8030":"markdown","17db7a1f":"markdown","0cfce3ed":"markdown","0dbabe03":"markdown","53c04951":"markdown","e1dcab00":"markdown","6de1880d":"markdown","b5c07fbf":"markdown","1ce625db":"markdown","1d983d6f":"markdown"},"source":{"a5fd995b":"#Import Libraries\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import  stats \nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n","35bbb11d":"# import train data\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","0e7cf625":"df_train.head()","dae69cb6":"df_train.columns","de238f7c":"df_train['SalePrice'].describe()","f57bfd07":"#histogram\nsns.distplot(df_train['SalePrice']);","107fef2e":"# skewness and kurtosis\nprint('Skewness : %f '% df_train['SalePrice'].skew())\nprint('Kurtosis : %f '% df_train['SalePrice'].kurt())","7e11628d":"var = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'],df_train[var]],axis=1)\ndata.plot.scatter(x=var, y='SalePrice',ylim=(0.000000))\nprint(data[:5])","2092d86e":"# scatter plot totalbmsf and saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'],df_train[var]], axis=1)\ndata.plot.scatter(x=var,y='SalePrice', ylim=(0.000000))\nprint(data[:5])","4b12a862":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","b7535ce0":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf,ax = plt.subplots(figsize=(16,8))\nfig = sns.boxplot(x=var, y='SalePrice',data=data)\nfig.axis(ymin=0,ymax=800000);\nplt.xticks(rotation=90)\nprint(data[:5])","fc398064":"# correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=.8, square=True)","da46d1b1":"# saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index   # nlargest : pick the most powerfull correlation\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","68df5834":"#scatterplot\nsns.set()\ncols = ['SalePrice','OverallQual','GrLivArea','GarageCars','TotalBsmtSF','FullBath','YearBuilt']\nsns.pairplot(df_train[cols],size=2.5)\nplt.show();","cfcdfdb0":"# missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent], axis=1, keys=['Total','Percent'])\nmissing_data.head(20)","a36194b2":"#dealing with missing data\ndf_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max() #just checking that there's no missing data missing...","047203d2":"# standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]  # rearrange ascending\nhigh_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]  # rearrange descending\nprint('outer range (low) of the distribution : ')\nprint(low_range)\nprint('\\nouter range (high) of the distribution : ')\nprint(high_range)","23d24222":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","1cd1453a":"# dealing point \ndf_train.sort_values(by = 'GrLivArea', ascending=False)[:2]\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)\n","eca12fb9":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","a716ad7a":"# histogram and normal probability plot \nsns.distplot(df_train['SalePrice'],fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","c5476895":"# applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])\n","536e9080":"# histogram and normal probability plot \nsns.distplot(df_train['SalePrice'],fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","c1383f1c":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","bcec2deb":"# applying log transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])\n","79613ee1":"# histogram and normal probability plot \nsns.distplot(df_train['GrLivArea'],fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","a4981791":"# histogram and normal probability plot \nsns.distplot(df_train['TotalBsmtSF'],fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","b4771ca6":"# create column for new variale (one is enough because it's a binary categorical feature )\n# if area > 0 it gets 1, for area ==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index = df_train.index)\ndf_train['HasBsmt'] = 0\ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1\n\nprint(df_train['TotalBsmtSF'][:10])\nprint(df_train['HasBsmt'][:10])","f4b3cc4c":"# transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])","ceae5aec":"# histogram and normal probabiloty plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","be27917c":"#scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice']);","c1b55ab2":"# scatter plot \nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","94eeaedc":"df_train.head()","cc6e2e60":"# convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","4528cddf":"df_train.head()","acbf7b75":"df_train['SalePrice']","25f802aa":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","cc64e066":"X = df_train.drop('SalePrice', axis=1)\ny = df_train['SalePrice']","31f57f87":"X.head()","61f8f0e7":"y.head()","8907b570":"#Import Libraries\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_absolute_error \nfrom sklearn.metrics import mean_squared_error \nfrom sklearn.metrics import median_absolute_error\n","b5d30091":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection import train_test_split\n#Splitting data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=20, shuffle =True)","b493897c":"# Fitting Simple Linear Regression to the Training set\nfrom sklearn.linear_model import LinearRegression\nBostonRegressor = LinearRegression()\nBostonRegressor.fit(X_train, y_train)\n","3fca7875":"# Evaluating the model\nprint('BostonRegressor Train Score is : ' , BostonRegressor.score(X_train, y_train)*100)\nprint('BostonRegressor Test Score is : ' , BostonRegressor.score(X_test, y_test)*100)\n","805eae61":"# Predicting the Test set results\ny_pred = BostonRegressor.predict(X_test)\nmean_absolute_error(y_test, y_pred)\nprint('Mena Squard Error IS :     ',mean_squared_error(y_test, y_pred))\nprint('Mean Absolute Error Is :   ',mean_absolute_error(y_test, y_pred))\nprint('Median Absolute Error Is : ',median_absolute_error(y_test, y_pred))","670c6c8b":"from sklearn.metrics import mean_squared_error\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor\nimport xgboost\n\n# Function to invert target variable array from log scale\ndef inv_y(transformed_y):\n    return np.exp(transformed_y)\n\n# Series to collect RMSE for the different algorithms: \"algortihm name + RMSE\"\nrmse_compare = pd.Series()\nrmse_compare.index.name = \"Model\"\n\n# Series to collect the accuracy for the different algorithms: \"algorithms name + score\"\nscores_compare = pd.Series()\nscores_compare.index.name = \"Model\"\n\n# Model 1: Linear Regression =======================\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\nlinear_val_predictions = linear_model.predict(X_test)\nlinear_val_rmse = mean_squared_error(inv_y(linear_val_predictions), inv_y(y_test))\nlinear_val_rmse = np.sqrt(linear_val_rmse)\nrmse_compare['LinearRegression'] = linear_val_rmse\n\nlr_score = linear_model.score(X_test, y_test)*100\nscores_compare['LinearRegression'] = lr_score\n\n#Model 2: Decision Tress ===========================\ndtree_model = DecisionTreeRegressor(random_state=5)\ndtree_model.fit(X_train, y_train)\n\ndtree_val_predictions = dtree_model.predict(X_test)\ndtree_val_rmse = mean_squared_error(inv_y(dtree_val_predictions), inv_y(y_test))\ndtree_val_rmse = np.sqrt(dtree_val_rmse)\nrmse_compare['DecisionTreeRegressor'] = dtree_val_rmse\n\ndtree_score = dtree_model.score(X_test, y_test)*100\nscores_compare['DecisionTreeRegressor'] = dtree_score\n\n# Model 3: Random Forest ==========================\nrf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(X_train, y_train)\n\nrf_val_predictions = rf_model.predict(X_test)\nrf_val_rmse = mean_squared_error(inv_y(rf_val_predictions), inv_y(y_test))\nrf_val_rmse = np.sqrt(rf_val_rmse)\nrmse_compare['RandomForest'] = rf_val_rmse\n\nrf_score = rf_model.score(X_test, y_test)*100\nscores_compare['RandomForest'] = rf_score\n\n\n# Model 4: Gradient Boostinf Regression ===========\ngbr_model = GradientBoostingRegressor(n_estimators=300, learning_rate=0.05, max_depth=4, random_state=5)\ngbr_model.fit(X_train, y_train)\n\ngbr_val_predictions = gbr_model.predict(X_test)\ngbr_val_rmse = mean_squared_error(inv_y(gbr_val_predictions), inv_y(y_test))\ngbr_val_rmse = np.sqrt(gbr_val_rmse)\nrmse_compare['GradientBoostingRegression'] = gbr_val_rmse\n\ngbr_score = gbr_model.score(X_test, y_test)*100\nscores_compare['GradientBoostingRegression'] = gbr_score","e61cd3a4":"print(\"RMSE values for different algorithms:\")\nrmse_compare.sort_values(ascending=True).round()","22ec693c":" print(\"Accuracy scores for different algorithms\")\nscores_compare.sort_values(ascending=False).round(3)","8209ed1e":"* Deviate from the normal distribution.\n* Have appreciable positive skewness.\n* Show peakedness","098739a5":"**Different Algorithms For Regression :**","e3e0bbe2":"Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.\n\n**Note:** we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years.","a77c3e13":"Done! Let's check what's going on with 'GrLivArea'.\n\n","988256f8":"# 1. First things  : analysing **SalePrice**\n","aa4b80fb":"**Scatter plots between 'SalePrice' and correlated variales**","9c888c86":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem.","a817a4b7":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.","dfddf16e":"**Bivariate analysis**\n\n","79550869":"**Linear Regression**","84dc83b1":"Ok, now we are dealing with the big boss. What do we have here?\n\n* Something that, in general, presents skewness.\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.","7a8ec8f0":"Let's analyse this to understand how to handle the missing data.\n\nWe'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.\n\nIn what concerns the remaining cases, we can see that 'GarageX' variables have the same number of missing data. I bet missing data refers to the same set of observations (although I will not check it; it's just 5% and we should not spend 20 in5  problems). Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'GarageX' variables. The same logic applies to 'BsmtX' variables.\n\nRegarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.\n\nIn summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.","50d4cbd1":"# Data Exploration\n","b7fa4036":"**Univariate analysis**\n\n\nThe primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","dc8b6d8e":"We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. We can live with that, so we'll not do anything.","3747e1e1":"**'SalePrice' correlation matrix (zoomed heatmap style)**","1f429aa0":"Hmmmm ... It seems that **SalePrice** and **GrLivArea** are really old friends, with a **Linear Relationship.**","8311685f":"**In summary**\n\nStories aside, we can conclude that:\n\n* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n\n\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).","2834c785":"Very well .... It seems that your minimum price is larger than zero.\n**Excellent!** You don't have one those personal traits that would destroy my model!","4906d332":"**Out liars!**\n\nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","f33c547b":"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!","befa5412":"**Last but not the least, dummy variables**\n","5da54ae7":"# What we are doing in this section is something like :\n\n\n1.   **Understand the problem :** We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n2.   **Univriable Study :** We'll just focus on the dependent variable **SalePrice** and try to know a little bit more about it.\n3.   **Multivariate Study :** We'll try to understand how the dependent variable and independent variable retate.\n4.   **Basic Cleaning :** We'll check the dataset and handle the missing data, outliers and categorical variables.\n5.   **Test assumptions :** We'll check if our data meets the assumptions required by most multivarate techniques.\n\n\nNow, It's Time Ho Have Fun!\n\n\n\n\n","e27b8030":"**TotalBsmtSF** is also a great friend of **SalePrice** but this seems a much more emotional relationship! Everything is ok and suddenly, in a **Strong Linear (exponential?)** reaction, Everything changes. Moreover , it's clear that somrtimes **TotalBsmtSF** closes in itself and gives zero credit to **SalePrice** ","17db7a1f":"How 'SalePrice' looks with her new clothes:\n\n* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.","0cfce3ed":"# Creating Machine Learning Algorithms :","0dbabe03":"Important questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?\n\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth","53c04951":"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n* 'FullBath'?? Really?\n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'.\n\nLet's proceed to the scatter plots.\n\n\n","e1dcab00":"At first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'GarageX' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.","6de1880d":"# Missing Data","b5c07fbf":"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!","1ce625db":"**Relationship with categorical features**","1d983d6f":"**Relationship with numerical variables**"}}