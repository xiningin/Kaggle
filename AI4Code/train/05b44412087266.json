{"cell_type":{"a1124990":"code","cf0965d7":"code","58fda104":"code","6170d729":"code","9a3940f9":"code","2f20c7b9":"code","b4695aab":"code","180a12ce":"code","1e3ae40f":"code","0b811698":"code","e48b7bcb":"code","e7844173":"code","5e59fdb3":"code","76969752":"code","8b52bd3d":"code","0bded479":"code","7bb2dd4d":"code","e45b491f":"markdown","addd12fd":"markdown","23062b7e":"markdown","163846e5":"markdown","6658e51c":"markdown","6e0e38db":"markdown","96332e4f":"markdown","bdcda7e3":"markdown","4579e179":"markdown","fe68ae7a":"markdown","dd7a81c9":"markdown","7dbccf84":"markdown","d3867fcc":"markdown","6ba0d07c":"markdown","f9c11769":"markdown","af00fcfb":"markdown","d3d057fb":"markdown","22e70a23":"markdown"},"source":{"a1124990":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA, TruncatedSVD","cf0965d7":"from sklearn.datasets import load_boston\nboston = load_boston()\nboston.keys()","58fda104":"unscaled_X = pd.DataFrame(boston.data, columns = boston.feature_names)\ny = pd.DataFrame(boston.target)","6170d729":"unscaled_X.head()","9a3940f9":"y.head()","2f20c7b9":"unscaled_X.shape","b4695aab":"unscaled_X.isnull().sum()","180a12ce":"y.isnull().sum()","1e3ae40f":"scaler = StandardScaler()\nscaler.fit(unscaled_X)\nX = scaler.transform(unscaled_X)","0b811698":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 4)","e48b7bcb":"lr = LinearRegression()\n\n# Train the model\nmodel = lr.fit(X_train, y_train)\n\n# Prediction\ny_pred =  lr.predict(X_test)\n\n# Accuracy Score\nlr.score(X_test, y_test).round(4)","e7844173":"pca = PCA(n_components = 10, whiten = True)\npca.fit(X)\npca_X = pca.transform(X)","5e59fdb3":"pca_X_train, pca_X_test, pca_y_train, pca_y_test = train_test_split(pca_X, y, test_size = 0.2, random_state = 4)","76969752":"pca_lr = LinearRegression()\n\n# Train the model\npca_model = pca_lr.fit(pca_X_train, pca_y_train)\n\n# Prediction\npca_y_pred =  pca_lr.predict(pca_X_test)\n\n# Accuracy Score\npca_lr.score(pca_X_test, pca_y_test).round(4)","8b52bd3d":"svd = TruncatedSVD(n_components = 10)\nsvd.fit(X)\nsvd_X = pca.transform(X)","0bded479":"svd_X_train, svd_X_test, svd_y_train, svd_y_test = train_test_split(svd_X, y, test_size = 0.2, random_state = 4)","7bb2dd4d":"svd_lr = LinearRegression()\n\n# Train the model\nsvd_model = svd_lr.fit(svd_X_train, svd_y_train)\n\n# Prediction\npca_y_pred =  svd_lr.predict(svd_X_test)\n\n# Accuracy Score\nsvd_lr.score(svd_X_test, svd_y_test).round(4)","e45b491f":"Well, I got a very close accuracy after applying the Dimensionality Reduction. But after that, our feature number gets decreased. This is called dimensionality loss. But PCA and SVD choose the best feature for our model.  \nWe set the value 10 to the parameter **n_components** to let PCA and SVD choose 10 features among the total number of features. We can change the value as our own choice. But be carefull about the dimensionality loss. Removing too many features sometimes can face an abrupt drop of accuracy of the model.","addd12fd":"<br\/>\n<br\/>\n\n# 4. Linear Regression Model with SVD\nSVD stands for **Singular Value Decomposition**","23062b7e":"<br\/>\n<br\/>\n\n## 1. What are PCA and SVD?  \nPCA stands for **Principle Component Analysis** and **SVD for Singular Value Decomposition**. They are both used to reduce the dimensionality of a dataset without changing the model accuracy in a large scale.  \nWorking with too many features are always a tedious job. And when it comes with a lot of features which even don't have any importance to the model, it becomes meaningless to move forward with them. And the features which are correlated with one another more than the terget variable, it brings multicollinearity and dimension curse. And here comes up **Dimensionality Reduction** which actually chooses the best features for the model.  \n\nThere are a lot of methods to execute Dimensionality Reduction such as **PCA**, **SVD** and **REF**. But in this kernel, I will discuss about only PCA and SVD.","163846e5":"<br\/>\n<br\/>\n\n### Let me first import some necessary modules and load the dataset\nWe will  use the famous **Boston Housing Price** dataset from Scikit-Learn.","6658e51c":"Before applying Dimensionality Reduction, must scale down your features. Otherwise the accuracy difference will get bigger.","6e0e38db":"Sometimes we will notice that Dimensionality Reduction doesn't work very well. Even in this model, it's performance wasn't that satisfactory. What just happened here?  \nMainly Dimensionality Reduction removes the features which are strongly correlated with each other more than the terget variable. But if we plot a correlation matrics of our dataset, we will see there are no such strong correlation between the features themselves. That's why PCA and SVD didn't perfom that good in this case.","96332e4f":"<br\/>\n<br\/>\n\n## 5. The impact of PCA and SVD on the model accuracy","bdcda7e3":"<br\/>\n<br\/>\n\n# 3. Linear Regression Model with PCA","4579e179":"<br\/>\n<br\/>\n\n### 6. Reasons of why sometimes PCA and SVD doesn't work well","fe68ae7a":"<br\/>\n\n## **Main components of this kernel:**\n1. What are PCA and SVD  \n2. Linear Regression without applying PCA and SVD \n3. Linear Regression with PCA  \n4. Linear Regression with SVD  \n5. The impact of PCA and SVD on the model accuracy  \n6. Reasons of why sometimes PCA and SVD doesn't work well  \n7. Cautions while applying Dimensionality Reduction","dd7a81c9":"<br\/>\n<br\/>\n\n### 7. Cautions while applying Dimensionality Reduction","7dbccf84":"**Here also I got nearly 70%**","d3867fcc":"![geometric-PCA-3-and-4-centered-with-first-component.png](attachment:geometric-PCA-3-and-4-centered-with-first-component.png)","6ba0d07c":"**After applying PCA, I got nearly 70% accuaracy.  \nBut what just happend? I will talk it later on.**","f9c11769":"**I got about 72% accuracy with all the features.  \nNow let's see what Dimensionality Reduction can do for us.**","af00fcfb":"<br\/>\n\n**Image source:** learnche.org","d3d057fb":"<br\/>\n<br\/>\n\n# 2. Linear Regression Model without Dimensionality Reduction","22e70a23":"### Scaling the features\nI will talk about it later. But for now, just remember that it carries a lot of importance in Dimensionality Reduction."}}