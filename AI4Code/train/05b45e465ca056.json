{"cell_type":{"729b52ef":"code","a5067256":"code","65333e68":"code","a9942f6f":"code","13580ddd":"code","6e279151":"code","b2f83f3c":"code","4204a3af":"code","0d09673c":"code","f2c5ef63":"code","6f1e7033":"code","a1008d09":"markdown"},"source":{"729b52ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport xgboost as xgb\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a5067256":"# Import all the requisite sklearn packages\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import confusion_matrix,ConfusionMatrixDisplay\nfrom sklearn.model_selection import RandomizedSearchCV","65333e68":"# Data Loading and Formatting\ntrain = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nfeatures = ['feature_'+str(i) for i in range(71)]\n\n# Label Encoder to transform the targets to numerical values for use in XGB\nle = LabelEncoder()\n\n# Standard Scaler for use with PCA\nss = StandardScaler()\ntrain['target'] = le.fit_transform(train['target'])\ntrain_pca = pd.DataFrame(columns=features,index=range(200000))\ntrain_pca.loc[:,features] = ss.fit_transform(train[features])\ntrain.head()","a9942f6f":"# PCA - Shows little covariance between the different features\npca = PCA()\ndata = pca.fit_transform(train_pca[features])\ntrain_pca[features]=data\npca.explained_variance_ratio_","13580ddd":"# Creating data sets for all possible uses\n# Train test splits for impromptu testing\nX_train,X_test,y_train,y_test = train_test_split(train[features],train['target'])\n\n# DataMatrices for using XGB without the SKLearn wrapper\ndtrain = xgb.DMatrix(data=train[features],label=train['target'])\ndtest = xgb.DMatrix(data=X_test)\n\n# Sanity Check\nprint(X_train.shape,X_test.shape,y_train.shape,y_test.shape)","6e279151":"# Cross Validation - Implemented in Colab using GPU\n\n# estimator = xgb.XGBClassifier(num_class=9,objective='multi:softmax',use_label_encoder=False,verbosity=1)\n# grid = {'max_depth': [1, 5, 10, 15, 20],\n#        'learning_rate': np.logspace(-3,1,5),\n#        'n_estimators': [100, 200, 300, 500, 700],\n#        'reg_lambda': np.append(np.logspace(-4,0,5),[0]),\n#        'reg_alpha': np.append(np.logspace(-4,0,5),[0])}\n# clf = RandomizedSearchCV(estimator,grid,verbose=4,cv=3)\n# clf.fit(train[features],train['target'])","b2f83f3c":"# Fitting a model using the best parameters found in the random grid search\n# This may take a while without a GPU :(\n\"\"\"\nBest Parameters:\n{'learning_rate': 0.01,\n 'max_depth': 5,\n 'n_estimators': 300,\n 'reg_alpha': 0.01,\n 'reg_lambda': 1.0}\n {reg_lambda=0.0001,\n reg_alpha=0.0001,\n n_estimators=300,\n max_depth=2,\n learning_rate=0.1}\n learning_rate=0.1,max_depth=5,n_estimators=100,reg_alpha=0.01,reg_lambda=0.1\n\"\"\"\nmodel = xgb.XGBClassifier(num_class=9,\n                          objective='multi:softmax',\n                          use_label_encoder=False,\n                          learning_rate=0.1,\n                          max_depth=5,\n                          n_estimators=100,\n                          reg_alpha=0.01,\n                          reg_lambda=0.1,\n                          verbosity=0)\n\n# Fit the model, you can change the \nmodel.fit(train[features],train['target'])","4204a3af":"# Plotting a confusion matrix to visualize the training results\n\ny_pred = model.predict(train[features])\n\ncm = confusion_matrix(train['target'], y_pred, labels=[i for i in range(9)])\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                               display_labels=[i for i in range(9)])\n\ndisp.plot()","0d09673c":"# Visualize the training label distribution\nplt.hist(train['target'],bins=9);","f2c5ef63":"# Create Submission Predictions and save them\ntest_pred = model.predict_proba(test[features])\nsubmission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission[['Class_'+str(i) for i in range(1,10)]] = test_pred\nsubmission.to_csv('submission.csv',index=False)\n\n# Display Submitted Label distributions\npred_vals = model.predict(test[features])\nplt.hist(pred_vals,bins=9);","6f1e7033":"# Compare to a ~very~ simple NN\n\n\"\"\"\nThe MLP classifier should perform a little worse\nthan the tuned XGB Classifier\nbased on the constriants of the presented parameter grid.\n\"\"\"\n\nfrom sklearn.neural_network import MLPClassifier\nmodel = MLPClassifier(hidden_layer_sizes=(9,9,9))\nmodel.fit(X_train,y_train)\nmodel.score(X_test,y_test)","a1008d09":"# XGBoost Implementation: 1.75670"}}