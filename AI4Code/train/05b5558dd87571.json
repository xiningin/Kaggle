{"cell_type":{"f103513d":"code","ac5d7286":"code","02e9b497":"code","069862ef":"code","f325d922":"code","83e41e75":"code","ed953f69":"code","7d8f54de":"code","5b3b6947":"code","7a0a5d81":"code","c58b1f0d":"code","3b4c0891":"code","1ca38cb9":"code","b0485587":"code","64afa3a4":"code","e427b625":"code","4db40529":"markdown","6ffed5fc":"markdown","eae042f5":"markdown","f0e8e014":"markdown","15f56fae":"markdown","9d5bfa49":"markdown","07df60ee":"markdown","867eb220":"markdown","54f41dcc":"markdown","aa83865b":"markdown"},"source":{"f103513d":"!pip install natsort","ac5d7286":"import os\nimport natsort\nimport torch\nfrom tqdm import tqdm\nimport numpy as np \nimport glob\nfrom PIL import Image\nimport albumentations\nimport torch.nn.functional as F\nimport torchvision\nimport torch.nn as nn\nfrom sklearn import model_selection\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom torchvision import datasets, models , transforms\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport cv2\nimport helper\nfrom PIL import ImageFile\n\nImageFile.LOAD_TRUNCATED_IMAGES = True","02e9b497":"Data_dir_images = \"..\/input\/CVC-ClinicDB\/Original\"\nData_dir_masks = \"..\/input\/CVC-ClinicDB\/Ground_Truth\"\n\n#this size coz U-net paper has similar input size\nimage_size = (572,572)\n\nepochs = 20\n\n# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\n    device = \"cpu\"\nelse:\n    print('CUDA is available!  Training on GPU ...')\n    device =\"cuda\"\n\nprint(device)\n\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)\nbatch_size = 2\n\n# class_names = [\"polyp\"]\n\nimages_list = glob.glob(f\"{Data_dir_images}\/*\")\nimages_list = natsort.natsorted(images_list, reverse = False)\nprint(len(images_list))\nmasks_list = glob.glob(f\"{Data_dir_masks}\/*\")\nmasks_list = natsort.natsorted(masks_list, reverse = False)\nprint(len(masks_list))\n\nsplit=0.1\ntotal_size = len(images_list)\nvalid_size = int(split * total_size)\ntest_size = int(split * total_size)\n\ntrain_img, valid_img , train_masks, valid_masks = model_selection.train_test_split(images_list, masks_list, test_size=valid_size, random_state = 42)\n\ntrain_img, test_img , train_masks, test_masks = model_selection.train_test_split(train_img, train_masks, test_size=test_size, random_state = 42)","069862ef":"#traindataset \n\nclass PolypDataset:\n  def __init__(self, images, masks, img_transforms, mask_transforms):\n    self.images = images\n    self.masks = masks\n    self.img_transforms = img_transforms\n    self.mask_transforms = mask_transforms\n\n  def __len__(self):\n    return len(self.images)\n\n  def __getitem__(self, item):\n    image = cv2.imread(self.images[item])#we use cv2 because PIL didnt load the image and we convert later to PIL image\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    image = Image.fromarray(image) #Converting CV2 to PIL\n    masks = cv2.imread(self.masks[item])#we use cv2 because PIL didnt load the image and we convert later to PIL image\n    masks = cv2.cvtColor(masks, cv2.COLOR_BGR2RGB)\n    masks = Image.fromarray(masks)#we use cv2 because PIL didnt load the image and we convert later to PIL image\n    transforms_image = self.img_transforms(image)\n    transforms_masks = self.mask_transforms(masks)\n\n    return transforms_image , transforms_masks","f325d922":"#training data\ntrain_transform_images = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean, std)])\ntrain_transform_masks = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor()])\n\ntrain_dataset = PolypDataset(train_img, train_masks, train_transform_images, train_transform_masks)\n\n\ntrain_dataloader = torch.utils.data.DataLoader(train_dataset,num_workers=4, batch_size=batch_size, shuffle=True)","83e41e75":"#validation data\nvalid_transform_images = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean, std)])\nvalid_transform_masks = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor()])\n\nvalid_dataset = PolypDataset(valid_img, valid_masks, valid_transform_images, valid_transform_masks)\n\nvalid_dataloader = torch.utils.data.DataLoader(valid_dataset,num_workers=4, batch_size=batch_size)","ed953f69":"#testing data\ntest_transform_images = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize(mean, std)])\ntest_transform_masks = transforms.Compose([\n                                      transforms.Resize(image_size),\n                                      transforms.ToTensor()])\n\ntest_dataset = PolypDataset(test_img, test_masks, test_transform_images, test_transform_masks)\n\ntest_dataloader = torch.utils.data.DataLoader(test_dataset,num_workers=4, batch_size=batch_size)","7d8f54de":"\ndataloaders = {\n  'train': train_dataloader,\n  'val': valid_dataloader\n}","5b3b6947":"#visualising the images \ndef imshow_images(dataloader, bs=2):\n  fig = plt.figure(figsize=(24, 16))\n  fig.tight_layout()\n  images , masks = next(iter(dataloader))\n\n  for num, (image, mask) in enumerate(zip(images[:bs], masks[:bs])):\n      plt.subplot(4,6,num+1)\n      plt.axis('off')\n      image = image.cpu().numpy()\n      out = np.transpose(image, (1,2,0))\n      mean = np.array([0.485, 0.456, 0.406])\n      std = np.array([0.229, 0.224, 0.225])\n      inp = std * out + mean\n      inp = np.clip(inp, 0, 1)\n      plt.imshow(inp)\n\n\n#visualising the images \ndef imshow_masks(dataloader, bs=2):\n  fig = plt.figure(figsize=(24, 16))\n  fig.tight_layout()\n  images , masks = next(iter(dataloader))\n\n  for num, (image, mask) in enumerate(zip(images[:bs], masks[:bs])):\n      plt.subplot(4,6,num+1)\n      plt.axis('off')\n      mask = mask.cpu().numpy()\n      mask = np.transpose(mask, (1,2,0))\n      plt.imshow(mask)\n","7a0a5d81":"#show the images and masks\nimshow_images(valid_dataloader)\nimshow_masks(valid_dataloader)","c58b1f0d":"'''\nhttps:\/\/github.com\/milesial\/Pytorch-UNet\/blob\/master\/unet\/unet_parts.py\n\nSome of the snippets are borrowed from this github repo\nplease check them out too ( they have explained in more depth)\n'''\n\n\ndef db_conv(in_c, out_c):\n    conv = nn.Sequential(\n        nn.Conv2d(in_c, out_c, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True),\n        nn.Conv2d(out_c, out_c, kernel_size=3, padding=1),\n        nn.BatchNorm2d(out_c),\n        nn.ReLU(inplace=True)\n    )\n    return conv\n\n\ndef down_sample(in_channels, out_channels):\n    maxpool_conv = nn.Sequential(\n            nn.MaxPool2d(2),\n            db_conv(in_channels, out_channels)\n        )\n    return maxpool_conv\n\nclass Up(nn.Module):\n    \"\"\"Upscaling then double conv\"\"\"\n\n    def __init__(self, in_channels, out_channels,):\n        super().__init__()\n\n        self.up = nn.ConvTranspose2d(in_channels , out_channels, kernel_size=2, stride=2)\n        self.conv = db_conv(in_channels, out_channels)\n\n\n    def forward(self, x1, x2):\n        x1 = self.up(x1)\n        # input is CHW\n        diffY = x2.size()[2] - x1.size()[2]\n        diffX = x2.size()[3] - x1.size()[3]\n\n        x1 = F.pad(x1, [diffX \/\/ 2, diffX - diffX \/\/ 2,\n                        diffY \/\/ 2, diffY - diffY \/\/ 2])\n        # if you have padding issues, see\n        # https:\/\/github.com\/HaiyongJiang\/U-Net-Pytorch-Unstructured-Buggy\/commit\/0e854509c2cea854e247a9c615f175f76fbb2e3a\n        # https:\/\/github.com\/xiaopeng-liao\/Pytorch-UNet\/commit\/8ebac70e633bac59fc22bb5195e513d5832fb3bd\n        x = torch.cat([x2, x1], dim=1)\n        return self.conv(x)\n","3b4c0891":"class UNet(nn.Module):\n    def __init__(self, in_channels, n_classes):\n        super(UNet, self).__init__()\n\n        self.in_channels = in_channels\n        self.classes = n_classes\n        self.first_layer = db_conv(in_channels, 64)\n        self.down_sample1 = down_sample(64, 128)\n        self.down_sample2 = down_sample(128, 256)\n        self.down_sample3 = down_sample(256, 512)\n        self.down_sample4 = down_sample(512, 1024)\n\n        self.up1 = Up(1024, 512)\n        self.up2 = Up(512, 256)\n        self.up3 = Up(256, 128)\n        self.up4 = Up(128, 64)\n        \n        self.output = nn.Conv2d(64, n_classes, kernel_size=1)\n\n    def forward(self, image):\n        #encoder\n        x1 =self.first_layer(image)\n        x2 = self.down_sample1(x1)\n        x3 = self.down_sample2(x2)\n        x4 = self.down_sample3(x3)\n        x5 = self.down_sample4(x4)\n        x = self.up1(x5, x4)\n        x = self.up2(x, x3)\n        x = self.up3(x, x2)\n        x = self.up4(x, x1)\n        x= self.output(x)\n        return x\n\nUnet = UNet(in_channels=3, n_classes=3)","1ca38cb9":"from collections import defaultdict\nimport torch.nn.functional as F\n\n\ndef dice_loss(pred, target, smooth = 1.):\n    pred = pred.contiguous()\n    target = target.contiguous()    \n\n    intersection = (pred * target).sum(dim=2).sum(dim=2)\n    \n    loss = (1 - ((2. * intersection + smooth) \/ (pred.sum(dim=2).sum(dim=2) + target.sum(dim=2).sum(dim=2) + smooth)))\n    \n    return loss.mean()\n\ncheckpoint_path = \"checkpoint.pth\"\n\ndef calc_loss(pred, target, metrics, bce_weight=0.5):\n    # print(pred.shape,target.shape)\n  \n    bce = F.binary_cross_entropy_with_logits(pred, target)\n\n    pred = torch.sigmoid(pred)\n    dice = dice_loss(pred, target)\n\n    loss = bce * bce_weight + dice * (1 - bce_weight)\n\n    metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n    metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n    metrics['loss'] += loss.data.cpu().numpy() * target.size(0)\n\n    return loss\n\ndef print_metrics(metrics, epoch_samples, phase):\n    outputs = []\n    for k in metrics.keys():\n        outputs.append(\"{}: {:4f}\".format(k, metrics[k] \/ epoch_samples))\n\n    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n\ndef train_model(model, optimizer, scheduler, num_epochs=25):\n    best_loss = 1e10\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 30)\n\n        since = time.time()\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            metrics = defaultdict(float)\n            epoch_samples = 0\n\n            for inputs, labels in tqdm(dataloaders[phase], total = len(dataloaders[phase])):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                \n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = calc_loss(outputs, labels, metrics)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                epoch_samples += inputs.size(0)\n\n            print_metrics(metrics, epoch_samples, phase)\n            epoch_loss = metrics['loss'] \/ epoch_samples\n\n            if phase == 'train':\n              scheduler.step()\n              for param_group in optimizer.param_groups:\n                  print(\"LR\", param_group['lr'])\n\n            # save the model weights\n            if phase == 'val' and epoch_loss < best_loss:\n                print(f\"saving best model to {checkpoint_path}\")\n                best_loss = epoch_loss\n                torch.save(model.state_dict(), checkpoint_path)\n\n        time_elapsed = time.time() - since\n        print('{:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n\n    print('Best val loss: {:4f}'.format(best_loss))\n\n    # load best model weights\n    model.load_state_dict(torch.load(checkpoint_path))\n    return model","b0485587":"import torch\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport time\n\n\nif train_on_gpu:\n  Unet.cuda()\n\n\noptimizer_ft = optim.Adam( Unet.parameters(), lr=1e-4)\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=8, gamma=0.1)\n\nmodel = train_model(Unet, optimizer_ft, exp_lr_scheduler, num_epochs=epochs)","64afa3a4":"def reverse_transform_mask(inp):\n  inp = np.transpose(inp,(1, 2, 0))\n  inp = np.clip(inp, 0, 1)\n  inp = (inp * 255).astype(np.uint8)\n  return inp\n\ndef reverse_transform_images(inp):\n  inp =  np.transpose(inp,(1, 2, 0))\n  mean = np.array([0.485, 0.456, 0.406])\n  std = np.array([0.229, 0.224, 0.225])\n  inp = std * inp + mean\n  inp = np.clip(inp, 0, 1)\n\n  return inp","e427b625":"\n\nmodel.eval()   # Set model to the evaluation mode\n# Create a new simulation dataset for testing\n# Get the first batch\nimages, masks = next(iter(test_dataloader))\nimages = images.to(device)\nmasks = masks.to(device)\n# Predict\npred = model(images)\n\n# The loss functions include the sigmoid function.\nprediction = torch.sigmoid(pred)\n\nprediction = pred.data.cpu().numpy()\nfig = plt.figure(figsize=(24, 16))\nfig.tight_layout()\n\n\nfor num, (img, mask, pred) in enumerate(zip(images[:batch_size], masks[:batch_size], prediction[:batch_size])):\n      plt.subplot(4,6,num+1)\n      plt.axis('off')\n      img = img.cpu().numpy()\n      mask = mask.cpu().numpy()\n      image_original = reverse_transform_images(img)\n      original_label = reverse_transform_mask(mask)\n      predictions = reverse_transform_mask(pred)\n    \n\n      white_line = np.ones((572, 10, 3))\n\n      all_images = [\n          image_original, white_line,\n          original_label, white_line,predictions]\n      image = np.concatenate(all_images, axis=1)\n      imgplot = plt.imshow(image)\n","4db40529":"# Training and Validation","6ffed5fc":"# The data we used here is from \"https:\/\/polyp.grand-challenge.org\/CVCClinicDB\/\"","eae042f5":"# Visualising the images for our batch size","f0e8e014":"# Visualising Test images","15f56fae":"**As you can see there is not much accurate predicitons, you can tune the model with (using different optimizers , here we used Adam , you can use SGD, RMSPROP etc) and play with it Data augmentation ( use random augmentations )etc**\n\n**Or you can include a Pre-Trained model like resnet18 to the backbone and use UNet as Transfer learning method**\n\n# Thank You for reading my kernel :D","9d5bfa49":"# **Importing all the necessary libraries**","07df60ee":"# Data Preparations","867eb220":"# Preparing Train , val and Test dataloaders\n","54f41dcc":"# Unet Architecure","aa83865b":"# Making our own custom dataset for pytorch"}}