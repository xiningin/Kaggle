{"cell_type":{"0b17e1e1":"code","dac7bf96":"code","1ac11b51":"code","c6adc802":"code","f2e86d40":"code","34e0ca40":"code","e5671f38":"code","a6b39c48":"code","2e15b3c9":"code","3e5966b5":"code","30c74f25":"code","a4c876ee":"code","7d0c4e4a":"code","2d8d8a82":"code","353cfa6b":"code","1a216b2d":"code","74791063":"code","a4985dbf":"code","2c28279f":"code","ce11c529":"code","c09fa455":"code","72015e8a":"code","5a01cc93":"code","73959a87":"code","d7efbaa7":"code","aa82708e":"code","ee306ac7":"code","4d3f18eb":"code","ff0dbc85":"code","4bdf4bf9":"code","43dfa9f6":"code","c1a0e126":"code","6c2f0bf2":"code","baf7fed1":"code","2e66dd29":"code","7bb7804c":"code","b26c9319":"code","73b17f21":"code","acc8265d":"code","7b7135a0":"code","da267668":"code","f08675b9":"code","acdd2c43":"code","4da33bea":"code","660085eb":"code","9eedbb21":"markdown","33c7c3d0":"markdown","756dc06d":"markdown","c0cf059f":"markdown","3f956c87":"markdown","9089bb04":"markdown","476983b6":"markdown","4fb9c972":"markdown","09f0f7ab":"markdown","c1476d95":"markdown","9e117042":"markdown","4dbfa303":"markdown","ec92ad96":"markdown","7f512f90":"markdown","2dbd2f31":"markdown"},"source":{"0b17e1e1":"# import relevant libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","dac7bf96":"bike = pd.read_csv('..\/input\/train.csv')","1ac11b51":"# Check for missing values - AWESOME!! no missing values in the \"Training\" dataset\n# The heatmap tool of seaborn can also be used for a visual representation for checking missing values\nbike.info()","c6adc802":"# check out the head to see categorical & quant variables and initial observations\nbike.head()","f2e86d40":"bike['datetime'].head()","34e0ca40":"import datetime\nbike['datetime'].iloc[0]","e5671f38":"bike['datetime'] = pd.to_datetime(bike['datetime'])\nbike['Hour'] = bike['datetime'].apply(lambda time: time.hour)\nbike['Month'] = bike['datetime'].apply(lambda time: time.month)\nbike['year'] = bike['datetime'].apply(lambda time:time.year)\nbike['Day of Week'] = bike['datetime'].apply(lambda time: time.dayofweek)","a6b39c48":"bike.head()","2e15b3c9":"sns.countplot(x='Day of Week',data=bike,palette='viridis')","3e5966b5":"plt.scatter('Hour', 'count', data = bike)","30c74f25":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# How do we select the 4 best predictive features out of the feature set available?\ndf = bike.drop('datetime', axis = 1)\nX = df.drop('count', axis = 1)\n#Example = yourdf.drop(['columnheading1', 'columnheading2'], axis=1, inplace=True)\ny = df['count']","a4c876ee":"test = SelectKBest(score_func = chi2, k = 4) # Instantiating selectkbest \nfit = test.fit(X,y)  # Now we fit selectkbest to the data\nprint(fit.scores_)\nfeatures = fit.transform(X)\nprint(features[0:5,:]) # higher the score, better the rating","7d0c4e4a":"plt.figure(figsize = (18,18))\nsns.heatmap(bike.corr(), cmap='coolwarm', annot = True)","2d8d8a82":"select_features = bike[['temp', 'casual', 'registered', 'Hour', 'humidity']]","353cfa6b":"select_features.head()","1a216b2d":"sns.distplot(bike['temp'], bins = 10) # we'll assume a normal distribution and move ahead  ","74791063":"sns.countplot(x='Month',data=bike,palette='viridis')","a4985dbf":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(select_features,bike['count'],\n                                                    test_size=0.30, random_state=101)","2c28279f":"from sklearn.ensemble import RandomForestRegressor\nrfc = RandomForestRegressor(n_estimators=100)\nrfc.fit(X_train, y_train) # This has been run on scaled features","ce11c529":"rfc_pred = rfc.predict(X_test)","c09fa455":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y_test, rfc_pred))","72015e8a":"rms","5a01cc93":"from sklearn.preprocessing import StandardScaler # need to scale feature set to fit KNN","73959a87":"scaler = StandardScaler() # initialise a scaler object to run on a dataframe","d7efbaa7":"select_features = bike[['temp', 'casual', 'registered', 'Hour', 'humidity']]","aa82708e":"scaler.fit(select_features) # run the above scaler method on the selected dataframe","ee306ac7":"scaled_features = scaler.transform(select_features)","4d3f18eb":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(select_features,bike['count'],\n                                                    test_size=0.30, random_state=101)","ff0dbc85":"from sklearn.neighbors import KNeighborsClassifier","4bdf4bf9":"knn = KNeighborsClassifier(n_neighbors=20) # initialise the KNN classifier with neighbours=20 in this case","43dfa9f6":"knn.fit(X_train,y_train)","c1a0e126":"pred = knn.predict(X_test) #run the KNN model on the test data","6c2f0bf2":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y_test, pred))","baf7fed1":"rms","2e66dd29":"select_features = bike[['temp', 'casual', 'registered', 'Hour', 'humidity']] # Adding year though it has a low correlation with the target variable - 'count'","7bb7804c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(select_features,bike['count'],\n                                                    test_size=0.30, random_state=101)","b26c9319":"from sklearn.ensemble import AdaBoostRegressor","73b17f21":"from sklearn.tree import DecisionTreeRegressor","acc8265d":"dt = DecisionTreeRegressor() \nclf = AdaBoostRegressor(n_estimators=100, base_estimator=dt,learning_rate=1)\n#Above I have used decision tree as a base estimator, you can use any ML learner as base estimator if it accepts sample weight \nclf.fit(X_train,y_train)","7b7135a0":"clf_pred = clf.predict(X_test)","da267668":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\nrms = sqrt(mean_squared_error(y_test, clf_pred))","f08675b9":"rms","acdd2c43":"from sklearn import metrics\nprint (metrics.accuracy_score(y_test, clf_pred))","4da33bea":"bike['count'].describe()","660085eb":"sns.distplot(bike['count'], bins = 10)","9eedbb21":"- temp\/atemp are correlated and hence one of them can be dropped without losing predicting ability\n- count is highly correlated with registered (so that's an important feature to predict demand)\n- We need to further select fewer features which have a meaningful impact on the demand. With an initial glance, we can look at features\/variables which have a significant correlation with the output ('count' variable)\n    - temp, casual, registered, hour and maybe humidity\n\nFor these variable, we can check for their distribution","33c7c3d0":"# Checklist to run once we have read the data and imported the libraries\n- What problem are we solving here - regression or classification? This will decide the algorithm to use\n- Does the data have missing values (.info() method)\n- Do we need to do feature selection? Or can we just use all the features? Which features to drop\n    - What EDA can be done to aid this process?\n    - Drop correlated features, outliers which might skew predictions (seaborn heatmap with correlations)\n    - What's the split in categorical\/quant variables? (.info() method)\n        - Are the Quant variables normally distributed? (distplot of each variable or the summary pair plot)\n- Feature engineering\n    - which features have an impact on the outcome (bike rentals\/hour)- (PAIR PLOT can help here)","756dc06d":"# Reviewing correlation for feature selection ","c0cf059f":"# Check the result with Adaboost","3f956c87":"# Running a RF on the above 5 selected features only","9089bb04":"As per our initial checklist\n- no missing values\n- all numerical variables\/features, though (season, holiday, workingday) are categorical variables\n- We can check for distribution & outliers of the quant variables\n- number of features is less, so we can look at correlated variables which can be dropped\n- datetime feature needs to be split up further to understand\n    - trends by yr, month, day, time of day","476983b6":"Top 4 influencing features as per SelectKBest are (Selected as per the scores in descending order)\n- 10th feature - registered\n- 9th feature - casual\n- 7th feature - humidity\n- 11th feature - hour","4fb9c972":"# How does the \"count\" variable differ with other features\/variables\n- BY DAY","09f0f7ab":"# Lets check the result in a KNN with the same above features","c1476d95":"**QUESTIONS TO PONDER ON**\n**\n- Which other date pre-processing steps could've been used? PCA?\n- What do the Fit and transform methods do (KNN method)\n- What's an ideal RMS value here?\n- What happens if we change the number of trees in Adaboost from the current one set at 100\n    - how about changing the learning rate**\n","9e117042":"** Objective - predict the bike-share demand (regression problem)\n\nimport the necessary libraries for EDA\n\n** Idea of this notebook is to explore basic feature selection\/engg methods use multiple ML models on a well known dataset and see how they perform and learn how to improve results\n\n** Models used - KNN, Random forests, Decision trees with boosting\n## I strongly suggest to ask \"Why\" for every step in this notebook and go through the questions at the end\n\n","4dbfa303":"** How do we evaluate the above RMS scores for the various models\nOne way is to compare it to the actual distribution of the \"count\" variable ","ec92ad96":"# How good or bad is the above model, seeing the RMS score?\n- how can we improve this RMSE without any major changes to the model","7f512f90":"# Running any ML algo (Supervised) follows a standard pattern once we have completed - EDA, Feature engg (includes scaling of features), Feature selection\n1. Import the necessary algo (Classifier or regressor)\n2. Instantiate the above algorithm\n3. Fit the training data in the above instantiated algorith\n4. Make predictions\n4. Check for accuracy of prediction of the model","2dbd2f31":"** \n1. One method to identify the top influencing variables (feature selection) is to check for correlation between the variables and the outcome\/target variable\n2. Other is to use SelecKBest optimized by Chi2 to select top features\n3. There's also the \"Common sense\"way for selecting features (As Andrew Ng mentions in his basic ML course)\n    - How would someone work-out the answer by considering features which are most likely to impact the target variable.\n    - Lets think about this - When would it be most likely that you'll use a Bike & not an alternate mode of transport\n        - Weather is favorable (Temperature, Wind, Humidity etc)\n        - Do you have access to a bike?\n        - Is it affordable?\n        etc etc \n\nWe'll limit our feature selection methods to the above for this example    "}}