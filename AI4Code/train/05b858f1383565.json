{"cell_type":{"4d2423dd":"code","e4b5eb87":"code","1ad9afdf":"code","01c5ae63":"code","69d076c8":"code","787349d2":"code","a5e431af":"code","30348683":"code","fa4e4df0":"code","b85cb33f":"code","2f9be9f7":"code","3be519e4":"code","2e208f4f":"code","42e50bd8":"code","12616ce9":"code","009668d0":"code","158ede61":"code","2eec91c8":"markdown","019b1e86":"markdown","65a14f02":"markdown","dce47d9f":"markdown","de9e2c02":"markdown","73ca21a0":"markdown","d11d37e3":"markdown","7328d716":"markdown","a4ef52a2":"markdown","e8f4bc50":"markdown","3359c2b9":"markdown","b6a241b8":"markdown","ed5152dc":"markdown","928a3928":"markdown","399020f4":"markdown","bcbf7fc1":"markdown","9cafbcc0":"markdown"},"source":{"4d2423dd":"# Data Manipulation and Linear Algebra\nimport pandas as pd\nimport numpy as np\n\n# Plots\nimport seaborn as sns\nsns.set_style(\"darkgrid\")\nimport matplotlib.pyplot as plt\n\n# Machine Learning\nfrom sklearn.model_selection import StratifiedShuffleSplit, cross_val_score, cross_val_predict\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report, precision_recall_curve, roc_curve, mean_absolute_error\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn import ensemble\n\n#ignore warning messages \nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom pprint import pprint","e4b5eb87":"df = pd.read_csv(\"..\/input\/churn-for-bank-customers\/churn.csv\")\ndf","1ad9afdf":"drop_cols = [\"RowNumber\", \"CustomerId\", \"Surname\"]\ndf.drop(columns=drop_cols, inplace=True)","01c5ae63":"df.info() # Luckiley we dont have any null values","69d076c8":"df.describe()","787349d2":"plt.figure(dpi=80, figsize=(10, 8))\nsns.heatmap(df.corr(), annot=True, cmap=\"YlGnBu\") # Not many correlations between features\nplt.show()","a5e431af":"plt.figure(figsize=(15, 10), dpi=80)\n\nplt.subplot(221)\nsns.histplot(df[\"Age\"], kde=True) # Age is Right Skewed and most of the people are around 30 - 40 years old\n\nplt.subplot(222)\nsns.histplot(df[\"EstimatedSalary\"], kde=True) # Salary is Well Distributed\n\nplt.subplot(223)\nsns.histplot(df[\"CreditScore\"], kde=True) # Credit Score is Left Skewed and most people have 600 - 700 Credit Score\n\nplt.subplot(224)\nsns.histplot(df[\"Balance\"], kde=True) # Balance Data is also well distributed if 0 is ignored","30348683":"plt.figure(figsize=(10, 6))\n\nplt.subplot(121)\nsns.boxplot(x=\"IsActiveMember\", y=\"Age\", data=df)\nplt.title(\"IsActiveMember vs Age\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n\nplt.subplot(122)\nsns.boxplot(x=\"Exited\", y=\"Age\", data=df)\nplt.title(\"Exited vs Age\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n\n# People of Higher age i.e., 40 years and above tend to leave the bank more","fa4e4df0":"plt.figure(figsize=(15, 6))\n\nplt.subplot(121)\nsns.boxplot(x=\"Exited\", y=\"Balance\", data=df)\nplt.title(\"Exited vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n# If we observe Median value people with Higher balance tend to leave bank\n\nplt.subplot(122)\nsns.violinplot(x=\"Exited\", y=\"Balance\", hue=\"Gender\", data=df)\nplt.title(\"Exited vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})","b85cb33f":"plt.figure(figsize=(10, 6))\nsns.barplot(x=\"NumOfProducts\", y=\"Balance\", data=df)\nplt.title(\"NumOfProducts vs Balance\", fontdict={\"fontsize\": 14, \"fontweight\":500})\n# Most of the people have 1 or 4 products","2f9be9f7":"sns.pairplot(df, hue=\"Gender\")\nplt.show()","3be519e4":"df = pd.get_dummies(df)\ndf.reset_index(drop=True, inplace=True)\ndf","2e208f4f":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, test_index in split.split(df, df['Exited']):\n    train = df.loc[train_index]\n    test = df.loc[test_index]","42e50bd8":"X_train = train.drop(\"Exited\", axis=1)\ny_train = train[\"Exited\"]\n\nX_test = test.drop(\"Exited\", axis=1)\ny_test = test[\"Exited\"]","12616ce9":"cols_to_scale = [\"CreditScore\", \"Age\", \"Balance\", \"EstimatedSalary\"]\n\nsc = StandardScaler()\n\nX_train[cols_to_scale] = sc.fit_transform(X_train[cols_to_scale])\nX_test[cols_to_scale] = sc.transform(X_test[cols_to_scale])","009668d0":"def MLA_testing(MLA, X_train, X_test, y_train, y_test):      \n    # Training The Model\n    MLA.fit(X_train, y_train)\n\n    # KFold Accuracies on Training Data\n    kfold_accuracy = cross_val_score(estimator = MLA, X = X_train, y = y_train, cv = 10, n_jobs=-1)\n    print(\"K-Fold Accuracies:\\n\", kfold_accuracy, \"\\n\")\n    \n    # Prediction on Testing Data\n    y_pred = cross_val_predict(estimator = MLA, X = X_test, y = y_test, cv = 10, n_jobs=-1)\n    \n    # Accuracy for y_test and y_pred\n    classifier_accuracy_score = accuracy_score(y_test, y_pred)\n    print(\"Accuracy Score:\\n\", classifier_accuracy_score, \"\\n\")\n    \n    # Confusion Matrix\n    conf_mtx = confusion_matrix(y_test, y_pred)\n    print(\"Confusion Matrix:\\n\", conf_mtx, \"\\n\")\n    \n    # Classification Report\n    class_rep = classification_report(y_test, y_pred)\n    print(\"Classification Report:\\n\", class_rep, \"\\n\")\n    \n    try:\n        # Precision - Recall Curve\n        yhat = MLA.predict_proba(X_test)\n        precision, recall, _ = precision_recall_curve(y_test, yhat[:, 1])\n        \n        plt.figure(dpi=100, figsize=(15, 6))\n        plt.subplot(121)\n        sns.lineplot([0, 1], [1, 0], linestyle='--', label='No Skill')\n        sns.lineplot(recall, precision, marker='.', label=MLA.__class__.__name__)\n        plt.title(\"Recall vs Precision Curve\")\n        plt.xlabel('Recall')\n        plt.ylabel('Precision')\n        plt.legend()\n        \n        # ROC Curve\n        plt.subplot(122)\n        sns.lineplot([0, 1], [0, 1], linestyle='--', label='No Skill')\n        fpr, tpr, _ = roc_curve(y_test, yhat[:, 1])\n        sns.lineplot(fpr, tpr, marker='.', label=MLA.__class__.__name__)\n        plt.title(\"ROC Curve\")\n        plt.xlabel('False Positive Rate')\n        plt.ylabel('True Positive Rate')\n        plt.legend()\n        \n        plt.show()\n    except:\n        pass\n    \n    # Important Features for The Algorithms\n    imp_cols = pd.DataFrame()\n    imp_cols[\"Features\"] = X_train.columns\n    imp_cols[\"Importance\"] = MLA.feature_importances_\n    imp_cols = imp_cols.sort_values(by=\"Importance\", ascending=False)\n    \n    plt.figure(dpi=80, figsize=(10, 8))\n    sns.barplot(y=\"Features\", x=\"Importance\", data=imp_cols)\n    plt.title(\"Importance of Features\")\n    plt.show()","158ede61":"rf_clf = ensemble.RandomForestClassifier()\n\nMLA_testing(rf_clf, X_train, X_test, y_train, y_test)","2eec91c8":"## Age vs IsActiveMember, Exited","019b1e86":"# Data Preprocessing","65a14f02":"# EDA","dce47d9f":"## Plotting Correlation Heatmap Before Encoding Categorical Variables","de9e2c02":"## OneHotEncoding Data","73ca21a0":"# RandomForestClassifier","d11d37e3":"## Pair Plot","7328d716":"## Feature Scaling","a4ef52a2":"## Balance vs Exited","e8f4bc50":"## Distribution Plots","3359c2b9":"## Stratified Train Test Split\n### Evenly Spreading the Dependent Variable \"status\" in Train and Test set","b6a241b8":"# Import Necessary Libraries","ed5152dc":"## The Code which i wrote after this for HyperParameter Tuning was a little too compute intensive for Kaggle to run so i ran it on my local machine and uploaded it on Github and you can find it on the link below\nhttps:\/\/github.com\/DataRohit\/Churn-Prediction-For-Bank\/blob\/master\/bank-rf-clf.ipynb","928a3928":"## Getting the Data ","399020f4":"## NumOfProducts vs Balance","bcbf7fc1":"## Dropping some Columns of Less Importance","9cafbcc0":"# Preparing Data"}}