{"cell_type":{"539de539":"code","c28765e6":"code","e51b0369":"code","87dae272":"code","ca986726":"code","0631567f":"code","5234b857":"code","c617a10e":"code","25704aa6":"code","33ad4367":"code","e40aaeae":"code","e07ac478":"code","97968a9e":"code","293bf99b":"markdown","bada9c02":"markdown","c475cc5b":"markdown","106e7410":"markdown","d8019d96":"markdown","751448bd":"markdown","80ba3d93":"markdown","436557e4":"markdown","3fa3bef2":"markdown","c78ec554":"markdown"},"source":{"539de539":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\n%matplotlib inline\n\ndataset = pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndataset.head()","c28765e6":"#Ploting distplot for all columns(for checking the distribution of data)\nfig, axs = plt.subplots(nrows=2, ncols=4, figsize=(20, 10))\nindex = 0\naxs = axs.flatten()\nfor k,v in dataset.iloc[:,0:8].items():\n    sns.distplot(v, ax=axs[index])\n    index += 1\nplt.plot()","e51b0369":"# Removing the missing values\ndataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n         'BMI']] = dataset[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.NaN)\ndataset.dropna(inplace=True)\n\n# Spliting dataset into features(X) and target(y)\nX = dataset.iloc[:,0:8].values\ny = dataset[['Outcome']].values\n\n# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nscalerX = StandardScaler().fit(X)\nX = scalerX.transform(X)","87dae272":"X_train1, X_test1, y_train1, y_test1 = train_test_split(X, y, test_size = 0.2, random_state = 89)\nlogreg = LogisticRegression()\nlogreg.fit(X_train1, y_train1.ravel())\ny_pred1 = logreg.predict(X_test1)\ncm1 = confusion_matrix(y_test1, y_pred1)\nacc1 = (cm1[0,0]+cm1[1,1])\/79\nprint(\"confusion matrix:\\n\", cm1)\nprint(classification_report(y_test1, y_pred1))\nprint(\"Accuracy:\", acc1)","ca986726":"X_train2, X_test2, y_train2, y_test2 = train_test_split(X, y, test_size=0.2, random_state=89)\nsvm = SVC(C=5)\nsvm.fit(X_train2, y_train2.ravel())\ny_pred2 = svm.predict(X_test2)\ncm2 = confusion_matrix(y_test2, y_pred2)\nacc2 = (cm2[0,0]+cm2[1,1])\/79\nprint(\"confusion matrix:\\n\", cm2)\nprint(classification_report(y_test2, y_pred2))\nprint(\"Accuracy:\", acc2)","0631567f":"# Finding kvalue\nX_train3, X_test3, y_train3, y_test3 = train_test_split(X, y, test_size=0.2, random_state=89)\ntraining_accuracy = []\ntest_accuracy = []\nneighbors_settings = range(1, 11)\nfor n_neighbors in neighbors_settings:\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn.fit(X_train3, y_train3.ravel())\n    training_accuracy.append(knn.score(X_train3, y_train3))\n    test_accuracy.append(knn.score(X_test3, y_test3))\nplt.plot(neighbors_settings, training_accuracy, label=\"training accuracy\")\nplt.plot(neighbors_settings, test_accuracy, label=\"test accuracy\")\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()","5234b857":"knn = KNeighborsClassifier(n_neighbors=9) #Kvalue 9 from above graph\nknn.fit(X_train3, y_train3.ravel())\ny_pred3 = knn.predict(X_test3)\ncm3 = confusion_matrix(y_test3, y_pred3)\nacc3 = (cm3[0,0]+cm3[1,1])\/79\nprint(\"confusion matrix:\\n\", cm3)\nprint(classification_report(y_test3, y_pred3))\nprint(\"Accuracy:\", acc3)","c617a10e":"X_train4, X_test4, y_train4, y_test4 = train_test_split(X, y, test_size=0.2, random_state=89)\ntree = DecisionTreeClassifier(max_depth=1)\ntree.fit(X_train4, y_train4.ravel())\ny_pred4 = tree.predict(X_test4)\ncm4 = confusion_matrix(y_test4, y_pred4)\nacc4 = (cm4[0,0]+cm4[1,1])\/79\nprint(\"confusion matrix:\\n\", cm4)\nprint(classification_report(y_test4, y_pred4))\nprint(\"Accuracy:\", acc4)","25704aa6":"X_train5, X_test5, y_train5, y_test5 = train_test_split(X, y, test_size=0.2, random_state=89)\nrf = RandomForestClassifier(max_depth=3, random_state=15)\nrf.fit(X_train5, y_train5.ravel())\ny_pred5 = rf.predict(X_test5)\ncm5 = confusion_matrix(y_test5, y_pred5)\nacc5 = (cm5[0,0]+cm5[1,1])\/79\nprint(\"confusion matrix:\\n\", cm5)\nprint(classification_report(y_test5, y_pred5))\nprint(\"Accuracy:\", acc5)","33ad4367":"X_train6, X_test6, y_train6, y_test6 = train_test_split(X, y, test_size=0.2, random_state=89)\ngb = GradientBoostingClassifier(learning_rate=0.01, max_depth=4)\ngb.fit(X_train6, y_train6.ravel())\ny_pred6 = gb.predict(X_test6)\ncm6 = confusion_matrix(y_test6, y_pred6)\nacc6 = (cm6[0,0]+cm6[1,1])\/79\nprint(\"confusion matrix:\\n\", cm6)\nprint(classification_report(y_test6, y_pred6))\nprint(\"Accuracy:\", acc6)","e40aaeae":"X_train7, X_test7, y_train7, y_test7 = train_test_split(X, y, test_size=0.2, random_state=89)\ngnb = GaussianNB()\ngnb.fit(X_train7, y_train7.ravel())\ny_pred7 = gnb.predict(X_test7)\ncm7 = confusion_matrix(y_test7, y_pred7)\nacc7 = (cm7[0,0]+cm7[1,1])\/79\nprint(\"confusion matrix:\\n\", cm7)\nprint(classification_report(y_test7, y_pred7))\nprint(\"Accuracy:\", acc7)","e07ac478":"X_train8, X_test8, y_train8, y_test8 = train_test_split(X, y, test_size=0.2, random_state=89)\nada = AdaBoostClassifier(learning_rate=0.01, n_estimators=30)\nada.fit(X_train8, y_train8.ravel())\ny_pred8 = ada.predict(X_test8)\ncm8 = confusion_matrix(y_test8, y_pred8)\nacc8 = (cm8[0,0]+cm8[1,1])\/79\nprint(\"confusion matrix:\\n\", cm8)\nprint(classification_report(y_test8, y_pred8))\nprint(\"Accuracy:\", acc8)","97968a9e":"#Plot of accuracy of each type of classifier\nacc=np.array([acc1, acc2, acc3, acc4, acc5, acc6, acc7, acc8])\nx=np.array(['Logistic Regression', 'Support Vector Machine', 'K-Neighbor Classifier', 'Decision Tree Classifier',\n            'Random Forest classifier', 'Gradient Boost classifier', 'Gaussian Naive Bayes', 'Ada Boost Classifier'])\nplt.scatter(x,acc,s=200,color=['cyan','green','red','yellow','black','magenta','blue','orange'])\nplt.xticks(rotation=90)\nplt.ylabel('Accuracy')","293bf99b":"<font size=5>**Classification**<\/font>\n\nIn this notebook I split the given data into train and test set and then used the following classification models and compared their accuracy (from confusion metrix) in predicting the test set:\n<ul>\n<li>Logistic Regression \n<li>Support Vector Machine\n<li>K-Neighbor Classifier\n<li>Decision Tree Classifier\n<li>Random Forest classifier\n<li>Gradient Boost classifier\n<li>Gaussian Naive Bayes\n<li>Ada Boost Classifier\n    <\/ul>","bada9c02":"<h2>Random Forest","c475cc5b":"<h2>SVM","106e7410":"<h2>Naive Bayes","d8019d96":"<h2>KNN","751448bd":"**We can draw some major conclusion from the above plots and domain knowledge:\n<br>Glucose, BloodPressure, SkinThickness, Insulin, BMI cannot be zero for any person.\n<br>These are missing values replaced with zeros as mentioned in section 3.7 of this paper:**<br>https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352914816300016?via%3Dihub#s0050","80ba3d93":"<h2>Decision Tree","436557e4":"<h2>AdaBoost","3fa3bef2":"<h2>Logistic Regression","c78ec554":"<h2>Gradient Boosting"}}