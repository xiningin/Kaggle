{"cell_type":{"903d19e1":"code","8df29c18":"code","833da0ae":"code","7b240fc6":"code","9048b811":"code","931731b5":"code","bbe2db03":"code","b1d84ace":"code","f9ba53f3":"code","6cded552":"code","469fdf46":"code","08a61684":"code","7e982780":"code","2804fb67":"code","dd0f466c":"code","71c51b9a":"code","58cd9c90":"markdown","07d09dd7":"markdown","147a698c":"markdown","5d9b40ce":"markdown","b834d92e":"markdown","f1567b0f":"markdown","db2b72fe":"markdown","c7102bc3":"markdown","353b8fc8":"markdown","6a82a1fd":"markdown"},"source":{"903d19e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8df29c18":"import pandas as pd\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix\nfrom matplotlib import pyplot\nfrom sklearn.model_selection import RandomizedSearchCV\n","833da0ae":"!pip install openpyxl==3.0.9","7b240fc6":"df = pd.read_excel('..\/input\/dry-beans\/Dry_Bean_Dataset.xlsx')\nif df.isna().sum().sum() == 0:\n    print('There is no missing values')","9048b811":"sns.pairplot(df,hue=\"Class\")","931731b5":"indexNames = df[df['ShapeFactor4'] < 0.96].index\ndf.drop(indexNames, inplace=True)\nindexNames = df[df['ShapeFactor1'] > 0.01].index\ndf.drop(indexNames, inplace=True)\nindexNames = df[df['Solidity'] < 0.95].index\ndf.drop(indexNames, inplace=True)\nindexNames = df[df['roundness'] < 0.6].index\ndf.drop(indexNames, inplace=True)","bbe2db03":"X = df.drop('Class', axis=1)\ny = df['Class']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=0)","b1d84ace":"clf = RandomForestClassifier(random_state=0)\nclf.fit(X_train, y_train)\nprint(\"Train Accuracy RF method:\", clf.score(X_train, y_train))\nprint(\"Test Accuracy RF method:\", clf.score(X_test, y_test))\nprint(\"Validation Accuracy RF method:\", clf.score(X_val, y_val))","f9ba53f3":" model_params = {'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n                \n               }\nrf_model = RandomForestClassifier()\n\nclf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5,\n                             random_state=1)\nmodel = clf.fit(X_train, y_train)\n# print winning set of hyperparameters\nfrom pprint import pprint\npprint(model.best_estimator_.get_params())","6cded552":"clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n           class_weight=None, criterion='gini',  max_depth=10,\n           max_leaf_nodes=None,  max_features=10,  min_samples_leaf=1,\n           min_weight_fraction_leaf=0.0,  n_estimators=200,  n_jobs=None,\n           oob_score=False,  random_state=0,  verbose=0,  warm_start=False)\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(\"Train Accuracy RF method:\", clf.score(X_train, y_train))\nprint(\"Test Accuracy RF method:\", clf.score(X_test, y_test))\nprint(\"Validation Accuracy RF method:\", clf.score(X_val, y_val))\nplot_confusion_matrix(clf, X_test, y_test)\nplt.xticks(rotation=90)","469fdf46":"train_scores, test_scores = list(), list()\n# define the tree depths to evaluate\nvalues = [i for i in range(1, 20)]\n# evaluate a decision tree for each depth\nfor i in values:\n    model = RandomForestClassifier(max_depth=i, n_estimators=116)\n    model.fit(X_train, y_train)\n    train_acc = model.score(X_train, y_train)\n    train_scores.append(train_acc)\n    test_acc = model.score(X_test, y_test)\n    test_scores.append(test_acc)\n    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc),\n              train_acc-test_acc)\n# plot of train and test scores vs tree depth\npyplot.plot(values, train_scores, '-o', label='Train')\npyplot.plot(values, test_scores, '-o', label='Test')\nplt.xlabel('Max depth')\nplt.ylabel('Score')\npyplot.legend()\npyplot.show()","08a61684":"sm = SMOTE(random_state=2)\nX_m, y_m = sm.fit_resample(X, y)\nXm_train, Xm_test, ym_train, ym_test = train_test_split(X_m, y_m,test_size=0.25, random_state=0)\nXm_train, Xm_val, ym_train, ym_val = train_test_split(Xm_train, ym_train, test_size=0.25, random_state=0)","7e982780":"train_scores, test_scores = list(), list()\n# define the tree depths to evaluate\nvalues = [i for i in range(1, 20)]\n# evaluate a decision tree for each depth\nfor i in values:\n    model = RandomForestClassifier(max_depth=i, n_estimators=116)\n    model.fit(Xm_train, ym_train)\n    train_acc = model.score(Xm_train, ym_train)\n    train_scores.append(train_acc)\n    test_acc = model.score(Xm_test, ym_test)\n    test_scores.append(test_acc)\n    print('>%d, train: %.3f, test: %.3f' % (i, train_acc, test_acc),\n              train_acc-test_acc)\n# plot of train and test scores vs tree depth\npyplot.plot(values, train_scores, '-o', label='Train')\npyplot.plot(values, test_scores, '-o', label='Test')\nplt.xlabel('Max depth')\nplt.ylabel('Score')\npyplot.legend()\npyplot.show()","2804fb67":"model_params = {'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]}\n\nrf_model = RandomForestClassifier()\n\nclf = RandomizedSearchCV(rf_model, model_params, n_iter=100, cv=5,\n                             random_state=1)\nmodel = clf.fit(Xm_train, ym_train)\n# print winning set of hyperparameters\nfrom pprint import pprint\npprint(model.best_estimator_.get_params())","dd0f466c":"clf = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n           class_weight=None, criterion='gini',  max_depth=10,\n           max_leaf_nodes=None,  max_features=10,  min_samples_leaf=1,\n           min_weight_fraction_leaf=0.0,  n_estimators=100,  n_jobs=None,\n           oob_score=False,  random_state=0,  verbose=0,  warm_start=False)\nclf.fit(Xm_train, ym_train)\ny_pred = clf.predict(Xm_test)\nprint(\"Train Accuracy RF method:\", clf.score(Xm_train, ym_train))\nprint(\"Test Accuracy RF method:\", clf.score(Xm_test, ym_test))\nprint(\"Validation Accuracy RF method:\", clf.score(Xm_val, ym_val))\nplot_confusion_matrix(clf, Xm_test, ym_test)\nplt.xticks(rotation=90)","71c51b9a":"clf = RandomForestClassifier(random_state=0)\nclf.fit(Xm_train, ym_train)\nfeature_imp = pd.Series(clf.feature_importances_,\n                            index=X.columns).sort_values(ascending=False)\nsns.barplot(x=feature_imp, y=feature_imp.index)\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.legend()\nplt.show()","58cd9c90":"# Hyperparameter tuning ","07d09dd7":"# Load data and search for missing values","147a698c":"# Train and test Plot","5d9b40ce":"# Train test validation split ","b834d92e":"# Removing the outliers","f1567b0f":"# Random Forest Classifier ","db2b72fe":"# Feature importance","c7102bc3":"clf = RandomForestClassifier(random_state=0)\nclf.fit(X_train, y_train)\nplot_confusion_matrix(clf, X_test, y_test)\nplt.xticks(rotation=90)\nplt.show()","353b8fc8":"# Looking for outliers","6a82a1fd":"# Using SMOTE() for unbalanced DATA"}}