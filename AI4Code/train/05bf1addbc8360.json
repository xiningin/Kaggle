{"cell_type":{"a636c1f4":"code","deaf6303":"code","5db05fdd":"code","216fd5c5":"code","8889abd5":"code","b2d9c4fd":"code","f674a56e":"code","9b2cabae":"code","d0953728":"code","567d12cd":"code","91ce4b24":"code","faa84951":"code","e65903e3":"code","7e0a8ab0":"code","be99f8cc":"code","2a8c04d3":"code","ad9454e8":"code","1a7f37c2":"code","ccaaf40f":"code","b0cdba0e":"code","b3008949":"code","f398175a":"code","20e5c708":"code","0df6d750":"code","e17799d8":"code","8d534bed":"code","dad125d3":"code","5d37d6ac":"code","7dc8f59e":"code","523eb01e":"code","c5f1d653":"code","657e01c2":"code","ead89dac":"code","477ef5ac":"code","7a7fd589":"code","8f88b4d6":"code","b464a3ac":"code","e39d0636":"code","e3f6280e":"code","55a1a1b5":"code","0c5c2583":"code","8f6649b4":"code","045d78f8":"code","f641c4aa":"code","d4aa7524":"code","865ce63b":"code","cb2cfbc1":"code","b1467f6f":"code","13b08fae":"code","dd60d622":"code","6753ba1b":"code","c929743e":"markdown","5e3fe802":"markdown","7f6500ae":"markdown","ad595e6c":"markdown","c11fc138":"markdown","cbc0cdbc":"markdown","94164db1":"markdown","0ba87576":"markdown","dd0dffc4":"markdown","e5562782":"markdown","f9d83064":"markdown","a9b26cf0":"markdown","3fd53249":"markdown","69e7ddfc":"markdown","34da4811":"markdown","d346df16":"markdown","f0c66ffa":"markdown","d7a422c7":"markdown","bd201623":"markdown","c2c81ac2":"markdown","1cff9228":"markdown","f182ac35":"markdown","ab90e47c":"markdown","309b5d93":"markdown","399026d8":"markdown","a83496ea":"markdown","f5e69148":"markdown","7429550c":"markdown","3e358e52":"markdown","b899e4f7":"markdown","71d3deb1":"markdown","5c4a9c30":"markdown","3c94df48":"markdown"},"source":{"a636c1f4":"!pip install chart-studio","deaf6303":"# load libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nimport warnings \nwarnings.filterwarnings('ignore')\n# %matplotlib inline","5db05fdd":"# load dataset\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')\n\ntrain_data = train_data.sample(n=1000)\ntest_data = test_data.sample(n=1000)","216fd5c5":"# print the shape of datasets\nprint('Shape of train dataset : ', train_data.shape)\nprint('Shape of test dataset : ', test_data.shape)\n\n# sample entries from the train dataset\ntrain_data.head()","8889abd5":"# Now time to handle the missing values\nprint('Missing values in the train dataset : ', train_data[train_data.isnull()].count().sum())\nprint('Missing values in the test dataset : ', test_data[test_data.isnull()].count().sum())","b2d9c4fd":"# Is the 'target' variable biased?\ntrain_data['target'].hist()","f674a56e":"# the counts\nmajority_class_count, minority_class_count = train_data['target'].value_counts()\nprint('The majority class count :  ', majority_class_count)\nprint('The minority class count :  ', minority_class_count)\n","9b2cabae":"# majority and minority class dataframes\ntrain_data_majority_class = train_data[train_data['target'] == 0]\ntrain_data_minority_class = train_data[train_data['target'] == 1]\n\nmaj_class_percent = round((majority_class_count\/minority_class_count)\/len(train_data)*100)\nmin_class_percent = round((minority_class_count\/minority_class_count)\/len(train_data)*100)\n\nprint('Majority class (%): ', maj_class_percent)\nprint('Minority class (%): ', minority_class_count)","d0953728":"# let's introduce a new plot function for visualizing the impacts\ndef plot2DClusters(X,y,label='Classes'):\n    colors = ['#1F77B4', '#FF7F0E']\n    markers = ['o', 's']\n    for l, c, m in zip(np.unique(y), colors, markers):\n        plt.scatter(\n            X[y==l, 0],\n            X[y==l, 1],\n            c=c, label=l, marker=m\n        )\n    plt.title(label)\n    plt.legend(loc='Upper right')\n    plt.show()\n\n\n    \nimport chart_studio.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly_express as px\n\ndef plot3DClusters(dataset):\n    fig = px.scatter_3d(dataset, x='dim-1', y='dim-2', z='dim-3', color='target', opacity=0.8)\n    iplot(fig, filename='jupyter-parametric_plot')","567d12cd":"# split the dataset into labels and IVs\nX = train_data.drop(['ID_code', 'target'], axis=1)\ny = train_data['target']\n\ntemp_X_holder = X\ntemp_y_holder = y\n\n\n# It is not practical to visualize the classes or clusters in the dataset using 2DPlot (as dimensions > 3)\n# So we will perform the PCA to reduce the dimension\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 3)\nX = pca.fit_transform(temp_X_holder)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = temp_y_holder.values\nplot3DClusters(test)","91ce4b24":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\nX = pca.fit_transform(temp_X_holder)\n\nplot2DClusters(X,temp_y_holder.values,label='Imbalanced Dataset (PCA)')","faa84951":"new_train_data_majority_class = train_data_majority_class.sample(minority_class_count, replace=True)\n\n# create new dataset\ndownsampled_data = pd.concat([train_data_minority_class, new_train_data_majority_class], axis=0)","e65903e3":"# check results\nprint(downsampled_data['target'].value_counts())\ndownsampled_data['target'].hist()","7e0a8ab0":"# split the dataset into labels and IVs\ny = downsampled_data['target']\nX = downsampled_data.drop(['ID_code', 'target'], axis=1)\n\ntemp_X_holder = X\ntemp_y_holder = y\n\n\n# It is not practical to visualize the classes or clusters in the dataset using 2DPlot (as dimensions > 3)\n# So we will perform the PCA to reduce the dimension\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 3)\nX = pca.fit_transform(temp_X_holder)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = temp_y_holder.values\nplot3DClusters(test)","be99f8cc":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\nX = pca.fit_transform(temp_X_holder)\n\nplot2DClusters(X,temp_y_holder.values,label='Balanced Dataset (PCA)')","2a8c04d3":"new_train_data_minority_class = train_data_minority_class.sample(majority_class_count, replace=True) \\\n\n# concatenate the dataframes to create the new one\nupsampled_data = pd.concat([new_train_data_minority_class, train_data_majority_class], axis=0)\n\n# check the results\nprint(upsampled_data['target'].value_counts())\nupsampled_data['target'].hist()","ad9454e8":"# split the dataset into labels and IVs\ny = upsampled_data['target']\nX = upsampled_data.drop(['ID_code', 'target'], axis=1)\n\ntemp_X_holder = X\ntemp_y_holder = y\n\n\n# It is not practical to visualize the classes or clusters in the dataset using 2DPlot (as dimensions > 3)\n# So we will perform the PCA to reduce the dimension\nfrom sklearn.decomposition import PCA\n\npca = PCA(n_components = 3)\nX = pca.fit_transform(temp_X_holder)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = temp_y_holder.values\nplot3DClusters(test)","1a7f37c2":"from sklearn.decomposition import PCA\n\npca = PCA(n_components = 2)\nX = pca.fit_transform(temp_X_holder)\n\nplot2DClusters(X,temp_y_holder.values,label='Balanced Dataset (PCA)')","ccaaf40f":"from imblearn.under_sampling import TomekLinks\n\nimb_tomek = TomekLinks(return_indices = True, ratio = 'majority')\n\nX_imb_tomek, y_imb_tomek, Id_imb_tomek = imb_tomek.fit_sample(temp_X_holder, temp_y_holder)\n\nprint('Number of data points deleted : ', len(Id_imb_tomek))","b0cdba0e":"# let's check the results\nX_imb_tomek = pd.DataFrame(X_imb_tomek)\ny_imb_tomek = pd.DataFrame(y_imb_tomek)\n\ny_imb_tomek.hist()","b3008949":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_tomek)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_tomek.values\nplot3DClusters(test)","f398175a":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_tomek)\n\nplot2DClusters(X,y_imb_tomek[0],label='Balanced Dataset (PCA)')","20e5c708":"from imblearn.under_sampling import ClusterCentroids\n\n# imb_cc = ClusterCentroids(ratio={0:100}) # we want to save 100 points from each class\nimb_cc = ClusterCentroids()\nX_imb_cc, y_imb_cc = imb_cc.fit_sample(temp_X_holder, temp_y_holder)","0df6d750":"# let's check the results\nX_imb_cc = pd.DataFrame(X_imb_cc)\ny_imb_cc = pd.DataFrame(y_imb_cc)\n\ny_imb_cc.hist()","e17799d8":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_cc)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_cc.values\nplot3DClusters(test)","8d534bed":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_cc)\n\nplot2DClusters(X, y_imb_cc[0],label='Down sampling with Cluster Centroids')","dad125d3":"from imblearn.under_sampling import NearMiss\n\nimb_nn = NearMiss() # we want to save 100 points from each class\n\nX_imb_nn, y_imb_nn = imb_nn.fit_sample(temp_X_holder, temp_y_holder)\n\n# let's check the results\nX_imb_nn = pd.DataFrame(X_imb_nn)\ny_imb_nn = pd.DataFrame(y_imb_nn)\n\ny_imb_nn.hist()","5d37d6ac":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_nn)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_nn.values\nplot3DClusters(test)","7dc8f59e":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_nn)\n\nplot2DClusters(X, y_imb_nn[0],label='Down sampling with Cluster Centroids')","523eb01e":"# SMOTE\nfrom imblearn.over_sampling import SMOTE\n\nimb_smote = SMOTE(ratio='minority')\n\nX_imb_smote, y_imb_smote = imb_smote.fit_sample(temp_X_holder, temp_y_holder)","c5f1d653":"# let's check the results\nX_imb_smote = pd.DataFrame(X_imb_smote)\ny_imb_smote = pd.DataFrame(y_imb_smote)\n\ny_imb_smote.hist()","657e01c2":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_smote)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_smote.values\nplot3DClusters(test)","ead89dac":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_smote)\n\nplot2DClusters(X, y_imb_smote[0],label='Oversampling with SMOTE')","477ef5ac":"# ADASYN\nfrom imblearn.over_sampling import ADASYN\n\nimb_adasyn = ADASYN(ratio='minority')\n\nX_imb_adasyn, y_imb_adasyn = imb_adasyn.fit_sample(temp_X_holder, temp_y_holder)","7a7fd589":"# let's check the results\nX_imb_adasyn = pd.DataFrame(X_imb_adasyn)\ny_imb_adasyn = pd.DataFrame(y_imb_adasyn)\n\ny_imb_adasyn.hist()\n","8f88b4d6":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_adasyn)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_adasyn.values\nplot3DClusters(test)","b464a3ac":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_adasyn)\n\nplot2DClusters(X, y_imb_adasyn[0],label='Oversampling with ADASYN')","e39d0636":"from imblearn.combine import SMOTETomek\n\nimb_smotetomek = SMOTETomek(ratio='auto')\n\nX_imb_smotetomek, y_imb_smotetomek = imb_smotetomek.fit_sample(temp_X_holder, temp_y_holder)\n\n# let's check the results\nX_imb_smotetomek = pd.DataFrame(X_imb_smotetomek)\ny_imb_smotetomek = pd.DataFrame(y_imb_smotetomek)\n\ny_imb_smotetomek.hist()","e3f6280e":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_smotetomek)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_smotetomek.values\nplot3DClusters(test)","55a1a1b5":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_smotetomek)\n\nplot2DClusters(X, y_imb_smotetomek[0],label='Balanced Dataset (PCA)')","0c5c2583":"from imblearn.combine import SMOTEENN\n\nimb_smoteenn = SMOTEENN(random_state=0)\n\nX_imb_smoteenn, y_imb_smoteenn = imb_smoteenn.fit_sample(temp_X_holder, temp_y_holder)\n\n# let's check the results\nX_imb_smoteenn = pd.DataFrame(X_imb_smoteenn)\ny_imb_smoteenn = pd.DataFrame(y_imb_smoteenn)\n\ny_imb_smoteenn.hist()","8f6649b4":"pca = PCA(n_components = 3)\nX = pca.fit_transform(X_imb_smoteenn)\n\ntest = pd.DataFrame(columns=['dim-1', 'dim-2', 'dim-3'], data=X)\ntest['target'] = y_imb_smoteenn.values\nplot3DClusters(test)","045d78f8":"pca = PCA(n_components = 2)\nX = pca.fit_transform(X_imb_smoteenn)\n\nplot2DClusters(X, y_imb_smoteenn[0],label='Balanced Dataset (PCA)')","f641c4aa":"from sklearn.model_selection import train_test_split\n\n# load dataset\ndataset = pd.read_csv('..\/input\/train.csv')\ndataset = dataset.sample(n=500)\n\ny = dataset['target']\nX = dataset.drop(['ID_code', 'target'], axis=1)\n\n# time to split into train-test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\nprint('Shape of Train data : ', X_train.shape)\nprint('Shape of Test data : ', X_test.shape)","d4aa7524":"# helper methods for the dataset preperation and benchmarking\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\ndef resample(resampler, X, y):\n    print(\"Resamping with {}\".format(resampler.__class__.__name__))\n    X_resampled, y_resampled = resampler.fit_sample(X, y)\n    return resampler.__class__.__name__, pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)\n\ndef simulation(resampling_type, X, y):\n    lr = LogisticRegression(penalty='l1')\n    parameter_grid = {'C':[0.01, 0.1, 1, 10]}\n    gs = GridSearchCV(estimator=lr, param_grid=parameter_grid, scoring='accuracy', cv=3, verbose=2) # cv=5\n    gs = gs.fit(X.values, y.values.ravel())\n    return resampling_type, gs.best_score_, gs.best_params_['C']\n","865ce63b":"%%time\n# we will use the random under and over sampling methods of imblearn instead that of pandas\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nresampled_datasets = []\nresampled_datasets.append((\"base dataset\", X_train, y_train))\nresampled_datasets.append(resample(SMOTE(n_jobs=-1),X_train,y_train))\nresampled_datasets.append(resample(RandomOverSampler(),X_train,y_train))\nresampled_datasets.append(resample(ClusterCentroids(n_jobs=-1),X_train,y_train))\nresampled_datasets.append(resample(NearMiss(n_jobs=-1),X_train,y_train))\nresampled_datasets.append(resample(RandomUnderSampler(),X_train,y_train))\nresampled_datasets.append(resample(SMOTEENN(),X_train,y_train))\nresampled_datasets.append(resample(SMOTETomek(),X_train,y_train))","cb2cfbc1":"%%time\nbenchmark_scores = []\nfor resampling_type, X, y in resampled_datasets:\n    print('______________________________________________________________')\n    print('{}'.format(resampling_type))\n    benchmark_scores.append(simulation(resampling_type, X, y))\n    print('______________________________________________________________')","b1467f6f":"benchmark_scores_df = pd.DataFrame(columns = ['Methods', 'Accuracy', 'Parameter'], data = benchmark_scores)\nbenchmark_scores_df","13b08fae":"%%time\n\nfrom sklearn.metrics import recall_score,accuracy_score,confusion_matrix, f1_score, precision_score, auc,roc_auc_score,roc_curve, precision_recall_curve\n\nscores = []\n# train models based on benchmark params\nfor sampling_type,score,param in benchmark_scores:\n    print(\"Training on {}\".format(sampling_type))\n    lr = LogisticRegression(penalty = 'l1',C=param)\n    for s_type,X,y in resampled_datasets:\n        if s_type == sampling_type:\n            lr.fit(X.values,y.values.ravel())\n            pred_test = lr.predict(X_test.values)\n            pred_test_probs = lr.predict_proba(X_test.values)\n            probs = lr.decision_function(X_test.values)\n            fpr, tpr, thresholds = roc_curve(y_test.values.ravel(),pred_test)\n            p,r,t = precision_recall_curve(y_test.values.ravel(),probs)\n            scores.append((sampling_type,\n                           f1_score(y_test.values.ravel(),pred_test),\n                           precision_score(y_test.values.ravel(),pred_test),\n                           recall_score(y_test.values.ravel(),pred_test),\n                           accuracy_score(y_test.values.ravel(),pred_test),\n                           auc(fpr, tpr),\n                           auc(p,r,reorder=True),\n                           confusion_matrix(y_test.values.ravel(),pred_test)))","dd60d622":"sampling_results = pd.DataFrame(scores,columns=['Sampling Type','f1','precision','recall','accuracy','auc_roc','auc_pr','confusion_matrix'])\nsampling_results","6753ba1b":"# let's visulize the confusion metrices\n\nf, axes = plt.subplots(2, 4, figsize=(15, 5), sharex=True)\nsns.despine(left=True)\n\nsns.heatmap(sampling_results['confusion_matrix'][0], annot=True, fmt='g', ax=axes[0, 0])\nsns.heatmap(sampling_results['confusion_matrix'][1], annot=True, fmt='g', ax=axes[0, 1])\nsns.heatmap(sampling_results['confusion_matrix'][2], annot=True, fmt='g', ax=axes[0, 2])\nsns.heatmap(sampling_results['confusion_matrix'][3], annot=True, fmt='g', ax=axes[0, 3])\nsns.heatmap(sampling_results['confusion_matrix'][4], annot=True, fmt='g', ax=axes[1, 0])\nsns.heatmap(sampling_results['confusion_matrix'][5], annot=True, fmt='g', ax=axes[1, 1])\nsns.heatmap(sampling_results['confusion_matrix'][6], annot=True, fmt='g', ax=axes[1, 2])\nsns.heatmap(sampling_results['confusion_matrix'][7], annot=True, fmt='g', ax=axes[1, 3])","c929743e":"#### 3.1 Over-sampling followed by Under-sampling (SMOTE-Tomek Links)\nBy combining SMOTE and Tomek Links methods. <br>\nUsing Tomek links in over-sampled dataset as a cleaning methode. <br>\nSo instead of removing only the major class from Tomek links, values of both classes are removed.","5e3fe802":"### K-fold validation is the right way\nAs the dataset is not balanced, it is important to consider that we covered it in testing part also.<br>\nThe model training on the Over sampled dataset tend to overfit. Hence the K-fold cross validation will help to introduces some <br>\nlevel of randomness so that the model can be genaralized. ","7f6500ae":"### Resampling with Imbalanced learn Package\nimblearn is the popular package used to perform resampling.<br>\nIt contains the Under, Over and Combined sampling methods. Let's take a look at the popular methods provided by it.\n\n <img src=\"https:\/\/drive.google.com\/uc?export=view&id=1tyBEdstPU6zHzF4mSEsIydD5Z5vuhyRi\" alt=\"\" width=\"800\" \/>\n","ad595e6c":"#### Findings\nThe random resampling methods are not enough to handle the class imbalance problem as,\n* the model trained in the <b>Random Over Sampled dataset<\/b> will be over-fitting due to the presence of dupicate data points of minor class.\n* the model trained in the <b>Random Under Sampled dataset<\/b> will lose many useful information. So model will be misleading one.\n<br>\nHence it is clear that, we need more advanced methods to handle the class imbalance problem. Let's take a look at them.","c11fc138":"#### 2. Over Sampling with Pandas\nRandom over sampling of major class.","cbc0cdbc":"#### 2.2 Over Sampling with ADASYN\nADAptive SYNthetic is based on the idea of generating minority data samples according to their distributions using K nearest neighbour. <br>\nDifference between SMOTE and ADASYN is that SMOTE generates equal number of synthetic samples for each minority sample. <br>\nWhere ADASYN can adaptevely change the weight for minority sample so that it can compensate for the skewed distribution.","94164db1":"### 3. Combined Over and Under Sampling\nIn this scenario we will take adavantage of both Over and Under sampling methods by combining them.","0ba87576":"### 1. Under Sampling with imbalanced learn","dd0dffc4":"### Training Base Model\nNow let's train a base model on these datasets and check how well they are performing. <br>\nTo avoid the effect of hyperparameters over the sampling methods, we will use grid search to find the optimal hyper parameters.","e5562782":"#### 1.3 Under Sampling with Nearmiss","f9d83064":"#### 2.1 Oversampling with SMOTE\nSynthetic Minority Oversampling TEchnique. <br>\nIn which you will select random points from minority class, and computing the K nearest neighbours for that. <br>\nSynthetic points are added between the selected point and it's neighbours. <br>\n\n <img src=\"https:\/\/drive.google.com\/uc?export=view&id=19uM7CtuawkidIfBmW6U07ql6bbJoyNV4\" alt=\"\" width=\"700\" \/>\n","a9b26cf0":"Load libraries and dataset","3fd53249":"### Evaluation Metrices\nSelecting evaluation metrices is very crucial while handling class imbalance. <br>\nSuppose you have a class imbalance like <b> major_class : minority_class = 98:2<\/b>. Then the accuracy for a program which simply generates the major class will be 98%. <br>\nSo we need to use other metrices like <b>precision, recall, F-1 score, AUC, etc<\/b>. F-1 score is prefered as it is the weighted sum of precision and recall.","69e7ddfc":"#### 1.2 Down Sampling with Cluster Centroids\nHere we will compute the centers of the clusters. And we will save it as new dataset.<br>\nWe can specify the number of centroids (if 10, then 10 centroids will be saved from class-0 and class-1)\n","34da4811":"yes, it is.","d346df16":"\n <img src=\"https:\/\/drive.google.com\/uc?export=view&id=1tnnZBY1N2opO-QLoPdHdAHOcnL4oi-Lx\" alt=\"\" style=\"width: 250px;\"\/>\n\nWhen it comes to machine learning, handling class imbalance is very critical. Even if you get good accuracy over the test set, the model will be misleading. So handling class imbalance and selecting right metrices to evaluate them is important. Let's take a look at the pupular methods to handle the problem.\n\n\n#### Index\n\n* Imabalanced Datasets\n* Resampling Overview\n* Resampling with Pandas\n* Resampling with Imbalanced learn\n* Training base model\n* K-fold validation is the right way\n* Evaluation metrices\n* Findings","f0c66ffa":"### Resampling with Pandas\nLet's use the built in methods of pandas to resample the dataset.","d7a422c7":"#### 1. Under Sampling with Pandas\nRandom under sampling of major class.","bd201623":"### Resampling\nFrom the above results it is clear that class imbalance is present in the target column. <br>\nSo any model that is created on top of it will be misleading. Hence we need to resample the dataset. <br> <br>\n\nTo resample the dataset, there are two approaches available. Which are,\n1. <b>Under Sampling<\/b> : delete random data points from the major class.\n2. <b>Over Sampling<\/b> : add or replicate the sample data points from the minor class.\n\n <img src=\"https:\/\/drive.google.com\/uc?export=view&id=1TUS-mnR1AyKPyXzrpKkqN8KaseKbd3OC\" alt=\"\" width=\"600\" \/>\n","c2c81ac2":"The above results shows that the presence of class imbalance.","1cff9228":"The Target column.\nIs the 'target' variable balanced?","f182ac35":"### Imbalanced Dataset\nLet us explore the dataset in detail to check the distribution and by what extend the imbalance is present.[](http:\/\/)","ab90e47c":"### Let's Train and Evaluate our models","309b5d93":"Let's do some EDA","399026d8":"### 2. Over Sampling Methods","a83496ea":"#### 3.1 SMOTEENN\nSMOTE Edited Nearest Neighbour. <br>\nRemoves any example whose class label differs from the class label of atleast two of its three nearest neighbours.<br>\nRemoves more links than Tomek links. <br>\nSo providing more indepth data cleaning.","f5e69148":"#### 1.1 Down Sampling with Tomek Links\n The Tomek Links are the pairs of data points at the borders of the classes. <br>\n So removing the majority class elements from these instances (pairs) will increase the seperation between them.\n \n<img src=\"https:\/\/drive.google.com\/uc?export=view&id=1LlE-6dcgT2krMLxlkAI1gfROi2oZxhTR\" alt=\"\" width=\"700\" \/>","7429550c":"### Resample datasets\nLet's resample and append datasets to a common variable for the simulation of models.","3e358e52":"And in 2d-space it looks like.","b899e4f7":"#### Train-Test split\nSplit the dataset into test and train.","71d3deb1":"### Findings\n* Out of all the models <b>RandomOverSampling<\/b> is the better performing one.\n* We can see that f1-score is a better metric that can explain the perfomance of a model.","5c4a9c30":"So let's neglect the part of data imputation","3c94df48":"Let's select the most important 3 features with dimensionality reduction. So we will get a better visulalization of class distribution."}}