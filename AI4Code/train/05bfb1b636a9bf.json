{"cell_type":{"ac71f221":"code","73061aa3":"code","78f5532a":"code","2ea0e07f":"code","7740404b":"code","7d82d4a7":"code","25066f44":"code","345f56a1":"code","db81c4b8":"code","6eb05f69":"code","34a8d00b":"code","d5db3aba":"code","fa424f70":"code","b1092dd9":"code","8cf1ee99":"code","d5f7a44c":"code","6e57061b":"code","388f2d24":"code","45785fe2":"code","cb28683f":"code","14f6ee50":"code","0c0e523a":"code","7ec184da":"code","f1610f75":"code","fd1c850c":"code","c1a777e8":"code","222316e4":"code","13c30aa4":"code","e00f4d0f":"code","8eb76696":"code","554867d6":"code","49066007":"code","7a8e8e54":"code","e4fcae96":"code","8002c081":"code","9f1570ff":"code","e0086fe7":"code","b3478087":"code","a2283a35":"code","1ff2624d":"code","1a380601":"code","4d14d7e3":"code","1471c0e7":"code","bd24de12":"code","e620c1e6":"code","853aaee1":"code","fb26e30f":"code","5115e252":"code","911a7c56":"code","4305df6f":"code","10949089":"code","2bf8b914":"code","c4dcd9a4":"code","86eab633":"code","88e00cdf":"code","9c42723b":"code","8f4010b6":"code","23895ba0":"code","4a2b689d":"code","ca12ba6b":"code","d915d6c1":"code","ef3b4d41":"code","de66d6c5":"code","2d5f25d9":"code","d89c3e3f":"code","2233167b":"code","e5805817":"code","1b51473b":"code","43ac82ae":"code","1c233581":"code","7ae18518":"code","5e6fc803":"code","0c1295f4":"code","d1a5173c":"code","707b79b0":"code","c4958e1d":"code","ad95747a":"code","34115216":"code","c4fbc829":"code","c07b2df7":"code","8b23ced6":"code","7fe4abaf":"code","fa8b7196":"code","a6940d18":"code","21ab22af":"code","d1a0ace1":"code","f97c65c7":"code","efe9290e":"code","96c31896":"code","29928d71":"code","78e240ac":"code","ce7ae7bb":"code","e33ed9f4":"code","51108d1b":"code","7d5fce1b":"code","7308359c":"code","943cde2e":"code","798daf6a":"code","a94164ed":"code","01a4d8c2":"code","4a0c9a04":"code","1f37a073":"code","cc0e5b82":"code","7d35c0f0":"code","4355d63a":"code","aa32ce6c":"code","0600953b":"code","0594ab51":"code","d31540c1":"code","cf5cb1c9":"code","e63c74ba":"code","d67ad81a":"code","258c4dcf":"code","841cfad4":"code","41a691ee":"code","beb9488f":"code","0426b946":"code","5ab2d06b":"code","0f22cf5d":"code","d5504bf7":"code","7875f877":"code","b496b907":"markdown","3d359679":"markdown","c6066212":"markdown","3a08ea8e":"markdown","d94f3c3f":"markdown","72320d5d":"markdown","5a19af64":"markdown","b50007d1":"markdown","5b73344b":"markdown","eefd4df5":"markdown","a574c969":"markdown","7d24a720":"markdown","a7458344":"markdown","260877f4":"markdown","987608dc":"markdown","52807044":"markdown","29e32e16":"markdown","85a1099e":"markdown","978ea9b5":"markdown","f6cfce42":"markdown","4b1afae3":"markdown","369d5afe":"markdown","aabeb0c5":"markdown","bc3fd251":"markdown","165659b7":"markdown","ba45620c":"markdown"},"source":{"ac71f221":"import pandas as pd\nimport warnings\nfrom sklearn.model_selection import GridSearchCV\nfrom xgboost.sklearn import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline\n#from sklearn.ensemble import StackingClassifier\n\n\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\n\nfrom nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize \n\nimport re\nimport string\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.neural_network import MLPClassifier\n\nfrom sklearn.metrics import f1_score, make_scorer\nf1 = make_scorer(f1_score , average='macro')\n\nwarnings.simplefilter('ignore')","73061aa3":"data_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nprint(len(data_train))\ndata_train.head()","78f5532a":"data_train['target'].value_counts()","2ea0e07f":"print(len(data_test))\ndata_test.head()","7740404b":"data_test.isnull().sum()","7d82d4a7":"#I concatenate both training and testing data to avoid doing the same operations twice plus we need it for vectorization\ndata_combined = pd.concat([data_train,data_test],ignore_index=True)  \nprint(len(data_combined))\ndata_combined.head()","25066f44":"data_combined.drop(['id','target'],axis=1,inplace=True)","345f56a1":"#Total Nan Values\ndata_combined.isnull().sum()","db81c4b8":"def clean_text_round1(text):\n    '''Make text lowercase, remove text in square brackets, remove punctuation, remove words containing numbers and removing weird characters from tweets'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('[()[\\]{}\\''',.``?:;!&^]','',text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('#','',text)\n    text = re.sub('\u00fb*','',text)\n    text = re.sub('\u00fb\u00f3*','',text)\n    text = re.sub('\u00f2*','',text)\n    text = re.sub(r'[^\\w]', ' ', text)\n    text = re.sub('[^a-zA-Z]+', ' ', text)\n    return text\nround1 = lambda x: clean_text_round1(x)","6eb05f69":"data_clean = pd.DataFrame(data_combined.text.apply(round1))\ndata_clean.head()","34a8d00b":"stop_words = set(stopwords.words('english'))\ndata_clean['text_tokenized']= data_clean['text'].map(lambda x : word_tokenize(x))\ndata_clean.head()","d5db3aba":"def removing_stop_words(text_tokenized) :\n    text_tokenized = [w for w in text_tokenized if w not in stop_words]\n    for w in text_tokenized :\n        w = w.strip()\n    return text_tokenized","fa424f70":"data_clean['text_tokenized']= data_clean['text_tokenized'].map(lambda x : removing_stop_words(x))\ndata_clean.head()","b1092dd9":"#special thanks to this kernel for the amazing slang fix :\n#https:\/\/www.kaggle.com\/nmaguette\/up-to-date-list-of-slangs-for-text-preprocessing\n\nabbreviations = {\n    \"$\" : \" dollar \",\n    \"\u20ac\" : \" euro \",\n    \"4ao\" : \"for adults only\",\n    \"a.m\" : \"before midday\",\n    \"a3\" : \"anytime anywhere anyplace\",\n    \"aamof\" : \"as a matter of fact\",\n    \"acct\" : \"account\",\n    \"adih\" : \"another day in hell\",\n    \"afaic\" : \"as far as i am concerned\",\n    \"afaict\" : \"as far as i can tell\",\n    \"afaik\" : \"as far as i know\",\n    \"afair\" : \"as far as i remember\",\n    \"afk\" : \"away from keyboard\",\n    \"app\" : \"application\",\n    \"approx\" : \"approximately\",\n    \"apps\" : \"applications\",\n    \"asap\" : \"as soon as possible\",\n    \"asl\" : \"age, sex, location\",\n    \"atk\" : \"at the keyboard\",\n    \"ave.\" : \"avenue\",\n    \"aymm\" : \"are you my mother\",\n    \"ayor\" : \"at your own risk\", \n    \"b&b\" : \"bed and breakfast\",\n    \"b+b\" : \"bed and breakfast\",\n    \"b.c\" : \"before christ\",\n    \"b2b\" : \"business to business\",\n    \"b2c\" : \"business to customer\",\n    \"b4\" : \"before\",\n    \"b4n\" : \"bye for now\",\n    \"b@u\" : \"back at you\",\n    \"bae\" : \"before anyone else\",\n    \"bak\" : \"back at keyboard\",\n    \"bbbg\" : \"bye bye be good\",\n    \"bbc\" : \"british broadcasting corporation\",\n    \"bbias\" : \"be back in a second\",\n    \"bbl\" : \"be back later\",\n    \"bbs\" : \"be back soon\",\n    \"be4\" : \"before\",\n    \"bfn\" : \"bye for now\",\n    \"blvd\" : \"boulevard\",\n    \"bout\" : \"about\",\n    \"brb\" : \"be right back\",\n    \"bros\" : \"brothers\",\n    \"brt\" : \"be right there\",\n    \"bsaaw\" : \"big smile and a wink\",\n    \"btw\" : \"by the way\",\n    \"bwl\" : \"bursting with laughter\",\n    \"c\/o\" : \"care of\",\n    \"cet\" : \"central european time\",\n    \"cf\" : \"compare\",\n    \"cia\" : \"central intelligence agency\",\n    \"csl\" : \"can not stop laughing\",\n    \"cu\" : \"see you\",\n    \"cul8r\" : \"see you later\",\n    \"cv\" : \"curriculum vitae\",\n    \"cwot\" : \"complete waste of time\",\n    \"cya\" : \"see you\",\n    \"cyt\" : \"see you tomorrow\",\n    \"dae\" : \"does anyone else\",\n    \"dbmib\" : \"do not bother me i am busy\",\n    \"diy\" : \"do it yourself\",\n    \"dm\" : \"direct message\",\n    \"dwh\" : \"during work hours\",\n    \"e123\" : \"easy as one two three\",\n    \"eet\" : \"eastern european time\",\n    \"eg\" : \"example\",\n    \"embm\" : \"early morning business meeting\",\n    \"encl\" : \"enclosed\",\n    \"encl.\" : \"enclosed\",\n    \"etc\" : \"and so on\",\n    \"faq\" : \"frequently asked questions\",\n    \"fawc\" : \"for anyone who cares\",\n    \"fb\" : \"facebook\",\n    \"fc\" : \"fingers crossed\",\n    \"fig\" : \"figure\",\n    \"fimh\" : \"forever in my heart\", \n    \"ft.\" : \"feet\",\n    \"ft\" : \"featuring\",\n    \"ftl\" : \"for the loss\",\n    \"ftw\" : \"for the win\",\n    \"fwiw\" : \"for what it is worth\",\n    \"fyi\" : \"for your information\",\n    \"g9\" : \"genius\",\n    \"gahoy\" : \"get a hold of yourself\",\n    \"gal\" : \"get a life\",\n    \"gcse\" : \"general certificate of secondary education\",\n    \"gfn\" : \"gone for now\",\n    \"gg\" : \"good game\",\n    \"gl\" : \"good luck\",\n    \"gmt\" : \"greenwich mean time\",\n    \"gmta\" : \"great minds think alike\",\n    \"gn\" : \"good night\",\n    \"g.o.a.t\" : \"greatest of all time\",\n    \"goat\" : \"greatest of all time\",\n    \"goi\" : \"get over it\",\n    \"gps\" : \"global positioning system\",\n    \"gr8\" : \"great\",\n    \"gratz\" : \"congratulations\",\n    \"gyal\" : \"girl\",\n    \"h&c\" : \"hot and cold\",\n    \"hp\" : \"horsepower\",\n    \"hr\" : \"hour\",\n    \"hrh\" : \"his royal highness\",\n    \"ht\" : \"height\",\n    \"ibrb\" : \"i will be right back\",\n    \"ic\" : \"i see\",\n    \"icq\" : \"i seek you\",\n    \"icymi\" : \"in case you missed it\",\n    \"idc\" : \"i do not care\",\n    \"idgadf\" : \"i do not give a damn fuck\",\n    \"idgaf\" : \"i do not give a fuck\",\n    \"idk\" : \"i do not know\",\n    \"ie\" : \"that is\",\n    \"i.e\" : \"that is\",\n    \"ifyp\" : \"i feel your pain\",\n    \"IG\" : \"instagram\",\n    \"iirc\" : \"if i remember correctly\",\n    \"ilu\" : \"i love you\",\n    \"ily\" : \"i love you\",\n    \"imho\" : \"in my humble opinion\",\n    \"imo\" : \"in my opinion\",\n    \"imu\" : \"i miss you\",\n    \"iow\" : \"in other words\",\n    \"irl\" : \"in real life\",\n    \"j4f\" : \"just for fun\",\n    \"jic\" : \"just in case\",\n    \"jk\" : \"just kidding\",\n    \"jsyk\" : \"just so you know\",\n    \"l8r\" : \"later\",\n    \"lb\" : \"pound\",\n    \"lbs\" : \"pounds\",\n    \"ldr\" : \"long distance relationship\",\n    \"lmao\" : \"laugh my ass off\",\n    \"lmfao\" : \"laugh my fucking ass off\",\n    \"lol\" : \"laughing out loud\",\n    \"ltd\" : \"limited\",\n    \"ltns\" : \"long time no see\",\n    \"m8\" : \"mate\",\n    \"mf\" : \"motherfucker\",\n    \"mfs\" : \"motherfuckers\",\n    \"mfw\" : \"my face when\",\n    \"mofo\" : \"motherfucker\",\n    \"mph\" : \"miles per hour\",\n    \"mr\" : \"mister\",\n    \"mrw\" : \"my reaction when\",\n    \"ms\" : \"miss\",\n    \"mte\" : \"my thoughts exactly\",\n    \"nagi\" : \"not a good idea\",\n    \"nbc\" : \"national broadcasting company\",\n    \"nbd\" : \"not big deal\",\n    \"nfs\" : \"not for sale\",\n    \"ngl\" : \"not going to lie\",\n    \"nhs\" : \"national health service\",\n    \"nrn\" : \"no reply necessary\",\n    \"nsfl\" : \"not safe for life\",\n    \"nsfw\" : \"not safe for work\",\n    \"nth\" : \"nice to have\",\n    \"nvr\" : \"never\",\n    \"nyc\" : \"new york city\",\n    \"oc\" : \"original content\",\n    \"og\" : \"original\",\n    \"ohp\" : \"overhead projector\",\n    \"oic\" : \"oh i see\",\n    \"omdb\" : \"over my dead body\",\n    \"omg\" : \"oh my god\",\n    \"omw\" : \"on my way\",\n    \"p.a\" : \"per annum\",\n    \"p.m\" : \"after midday\",\n    \"pm\" : \"after midday\",\n    \"poc\" : \"people of color\",\n    \"pov\" : \"point of view\",\n    \"pp\" : \"pages\",\n    \"ppl\" : \"people\",\n    \"prw\" : \"parents are watching\",\n    \"ps\" : \"postscript\",\n    \"pt\" : \"point\",\n    \"ptb\" : \"please text back\",\n    \"pto\" : \"please turn over\",\n    \"qpsa\" : \"what happens\", #\"que pasa\",\n    \"ratchet\" : \"rude\",\n    \"rbtl\" : \"read between the lines\",\n    \"rlrt\" : \"real life retweet\", \n    \"rofl\" : \"rolling on the floor laughing\",\n    \"roflol\" : \"rolling on the floor laughing out loud\",\n    \"rotflmao\" : \"rolling on the floor laughing my ass off\",\n    \"rt\" : \"retweet\",\n    \"ruok\" : \"are you ok\",\n    \"sfw\" : \"safe for work\",\n    \"sk8\" : \"skate\",\n    \"smh\" : \"shake my head\",\n    \"sq\" : \"square\",\n    \"srsly\" : \"seriously\", \n    \"ssdd\" : \"same stuff different day\",\n    \"tbh\" : \"to be honest\",\n    \"tbs\" : \"tablespooful\",\n    \"tbsp\" : \"tablespooful\",\n    \"tfw\" : \"that feeling when\",\n    \"thks\" : \"thank you\",\n    \"tho\" : \"though\",\n    \"thx\" : \"thank you\",\n    \"tia\" : \"thanks in advance\",\n    \"til\" : \"today i learned\",\n    \"tl;dr\" : \"too long i did not read\",\n    \"tldr\" : \"too long i did not read\",\n    \"tmb\" : \"tweet me back\",\n    \"tntl\" : \"trying not to laugh\",\n    \"ttyl\" : \"talk to you later\",\n    \"u\" : \"you\",\n    \"u2\" : \"you too\",\n    \"u4e\" : \"yours for ever\",\n    \"utc\" : \"coordinated universal time\",\n    \"w\/\" : \"with\",\n    \"w\/o\" : \"without\",\n    \"w8\" : \"wait\",\n    \"wassup\" : \"what is up\",\n    \"wb\" : \"welcome back\",\n    \"wtf\" : \"what the fuck\",\n    \"wtg\" : \"way to go\",\n    \"wtpa\" : \"where the party at\",\n    \"wuf\" : \"where are you from\",\n    \"wuzup\" : \"what is up\",\n    \"wywh\" : \"wish you were here\",\n    \"yd\" : \"yard\",\n    \"ygtr\" : \"you got that right\",\n    \"ynk\" : \"you never know\",\n    \"zzz\" : \"sleeping bored and tired\"\n}","8cf1ee99":"def convert_abbrev(text):\n    for word in text : \n        a = word\n        word = abbreviations[word.lower()] if word.lower() in abbreviations.keys() else word\n        l = word.split(' ')\n        text = text + l\n        text.remove(a)\n    return text","d5f7a44c":"data_clean['text_tokenized']=data_clean['text_tokenized'].map(lambda x: convert_abbrev(x))\ndata_clean.head()","6e57061b":"words_cloud_data=data_clean.copy()\nwords_cloud_data[\"word_cloud_text\"]=words_cloud_data[\"text_tokenized\"].map(lambda x : ' '.join(x))\nwords_cloud_data = words_cloud_data[0:7612]\nwords_cloud_data['target']=data_train['target']\nwords_cloud_data.drop(['text','text_tokenized'],axis=1,inplace=True)","388f2d24":"from wordcloud import WordCloud\nimport matplotlib.pyplot as plt\n\nwc = WordCloud(stopwords=stop_words, background_color=\"white\", colormap=\"Dark2\",\n               max_font_size=150, random_state=42)\n\nplt.rcParams['figure.figsize'] = [16, 6]\n\ntarget_1 =\"\"\ntarget_0 =\"\"\n\nfor i in range(len(words_cloud_data)) :\n    if words_cloud_data[\"target\"][i]== 1 :\n        target_1 = target_1 + words_cloud_data[\"word_cloud_text\"][i]+\" \"\n    if words_cloud_data[\"target\"][i] == 0 :\n        target_0 = target_0 + words_cloud_data[\"word_cloud_text\"][i]+ \" \"\n\n\nwc.generate(target_1)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","45785fe2":"plt.rcParams['figure.figsize'] = [16, 6]\nwc.generate(target_0)\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","cb28683f":"#Some words are very frequent in both target classes, removing them would improve model's score\ndef remove_confusing_words(text) :\n    for i in text :\n        if i == 'amp' or i == 'new' or i=='people' :\n            text.remove(i)\n    return text\ndata_clean['text_tokenized']=data_clean['text_tokenized'].map(lambda x: remove_confusing_words(x))\ndata_clean.head()","14f6ee50":"data_clean['location']=data_combined['location']\ndata_clean['keyword']=data_combined['keyword']\ndata_clean.head()","0c0e523a":"#dataset containing a good number of cities and their countries that will be usefull for us\ncities_data = pd.read_csv('..\/input\/worldcities\/worldcities.csv')","7ec184da":"cities_data.head()","f1610f75":"cities_data.drop(['city','lat','lng','admin_name','capital','population','id','iso2','iso3'],axis=1,inplace=True)","fd1c850c":"cities_data.head()","c1a777e8":"cities_dict = {}\n\nfor i in range(len(cities_data)) :\n    if cities_data['country'][i] in cities_dict :\n        cities_dict[cities_data['country'][i]].append(cities_data['city_ascii'][i].lower())\n    else :\n        cities_dict[cities_data['country'][i]] = list()\n        cities_dict[cities_data['country'][i]].append(cities_data['city_ascii'][i].lower())","222316e4":"cities_to_countries = {}\nfor i,j in cities_dict.items() :\n    for element in j :\n        cities_to_countries[element]=i.lower()    \nprint(cities_to_countries)        ","13c30aa4":"countries_names = list(cities_dict.keys())\ncountries_names_min = [] \nfor i in countries_names :\n    countries_names_min.append(i.lower())    \nprint(countries_names_min)    ","e00f4d0f":"cities  = [] \nfor j in cities_dict.values() : \n    cities = cities + j\nprint(cities)    ","8eb76696":"keywords=list(data_clean['keyword'].value_counts().index) # creating a list of existing keywords\nlocations=list(data_clean['location'].value_counts().index) #creating a list of existing locations\ncountries_names_min = countries_names_min+locations #adding existing locations to the ones from the dataset","554867d6":"def isNaN(num):\n    return num != num","49066007":"data_clean['Location_not_Nan']=0\ndata_clean['Keyword_not_Nan']=0","7a8e8e54":"for i in range(len(data_clean)) :\n    if isNaN(data_clean['location'][i]) :\n        data_clean['Location_not_Nan'][i]=0\n    else :\n        data_clean['Location_not_Nan'][i]=1\n    if isNaN(data_clean['keyword'][i]) :\n        data_clean['Keyword_not_Nan'][i]=0\n    else :\n        data_clean['Keyword_not_Nan'][i]=1","e4fcae96":"data_clean.head()","8002c081":"for i in range(len(data_clean)):\n    if isNaN(data_clean['location'][i]) :\n        for j in data_clean['text_tokenized'][i] :\n            if j in countries_names_min :\n                data_clean['location'][i]=j\n            elif j in cities :\n                data_clean['location'][i]=cities_to_countries[j]\n            else :\n                data_clean['location'][i]=\"NoLocation\"\n            if j in keywords :\n                data_clean['keyword'][i] = j","9f1570ff":"data_clean['location'].fillna(data_clean['location'].mode()[0],inplace=True)","e0086fe7":"data_clean['location']=data_clean['location'].map(lambda x : x.lower())","b3478087":"data_clean['keyword'].fillna(data_clean['keyword'].mode()[0],inplace=True)","a2283a35":"def changing_location(text) :\n    if text == 'usa' or text == 'new york' or text == 'gainesville\/tampa, fl' or text =='glendale, ca' or text == 'harbour heights, fl' or text =='new jersey' :\n        text = 'united states'\n    elif text == 'london' or text == 'brentwood uk' :\n        text = 'uk'\n    return text\n\ndef changing_keyword(text) :\n    text = re.sub('^.*fire.*$','fire',text)\n    text = re.sub('^.*storm.*$','storm',text)\n    text = re.sub('^.*emergency.*$','emergency',text)\n    text = re.sub('^.*disaster.*$','disaster',text)\n    text = re.sub('^.*collapse.*$','collapse',text)\n    text = re.sub('^.*bombing.*$','bombing',text)\n    text = re.sub('^.*bomb.*$','bomb',text)\n    text = re.sub('^.*zone.*$','zone',text)\n    text = re.sub('^.*bagging.*$','bagging',text)\n\n    return text\n","1ff2624d":"data_clean['location']=data_clean['location'].map(lambda x : changing_location(x))\ndata_clean['keyword']=data_clean['keyword'].map(lambda x: changing_keyword(x))","1a380601":"frequency_map_location = data_clean['location'].value_counts().to_dict()\nfrequency_map_keyword = data_clean['keyword'].value_counts().to_dict()\n\nprint(frequency_map_location)\n\nprint(frequency_map_keyword)","4d14d7e3":"data_clean['location'] = data_clean['location'].map(frequency_map_location)\ndata_clean['keyword'] = data_clean['keyword'].map(frequency_map_keyword)\ndata_clean.head()","1471c0e7":"#I chose lancaster stemmer because it's the heaviest stemmer between porter stemmer and snowballstemmer\nfrom nltk.stem import LancasterStemmer\nlancaster = LancasterStemmer()","bd24de12":"def lancaster_stemming(text_tokenized) :\n    text_tokenized = [lancaster.stem(w) for w in text_tokenized]\n    return ' '.join(text_tokenized)","e620c1e6":"data_clean['text_tokenized']=data_clean['text_tokenized'].map(lambda x:lancaster_stemming(x))\ndata_clean.head()","853aaee1":"def words_in_tweet(text) :\n    l = text.split(' ')\n    return len(l)\ndata_clean['tweet_length']=data_clean['text_tokenized'].map(lambda x: words_in_tweet(x))","fb26e30f":"def avg_length(text) :\n    l = text.split(' ')\n    len1 = 0\n    for i in l :\n        len1 += len(i)\n    return len1 \/ len(l) \ndata_clean['avg_word_length'] = data_clean['text_tokenized'].map(lambda x:avg_length(x))","5115e252":"data_clean['unique_words']=data_clean['text_tokenized'].map(lambda x:len(set(x.split(' '))))","911a7c56":"data_clean.head()","4305df6f":"def count_vect(data_clean):\n    cv = CountVectorizer(stop_words='english')\n    data_cv = cv.fit_transform(data_clean.text_tokenized)\n    data_dtm = pd.DataFrame(data_cv.toarray(), columns=cv.get_feature_names())\n    data_dtm.index = data_clean.index\n    return data_dtm\n\ndata_vectorized = count_vect(data_clean[['text_tokenized']])\ndata_vectorized.head() ","10949089":"data_clean1 = pd.concat([data_clean,data_vectorized],axis=1)","2bf8b914":"data_clean1.shape","c4dcd9a4":"data_clean_train1 = data_clean1[0:7613]\ndata_clean_test1 = data_clean1[7613:]","86eab633":"data_clean_train1['prediction_target']=data_train['target']","88e00cdf":"data_clean_train1.drop(['text','text_tokenized'],axis=1,inplace=True)\ndata_clean_test1.drop(['text','text_tokenized'],axis=1,inplace=True)","9c42723b":"features_to_encode = ['keyword','location','tweet_length','unique_words']\n\nfor col in features_to_encode :\n    means = data_clean_train1.groupby(col)['prediction_target'].mean()\n    data_clean_train1[col] = data_clean_train1[col].map(means)\n    data_clean_test1[col] = data_clean_test1[col].map(means)   ","8f4010b6":"data_clean_train1.head()","23895ba0":"y= data_clean_train1['prediction_target']\nX= data_clean_train1.drop('prediction_target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.30, random_state=30)","4a2b689d":"import numpy as np\ndef select_model(x,y,model):\n    scores=cross_val_score(model,x,y,cv=5,scoring='f1')\n    acc=np.mean(scores)\n    return acc","ca12ba6b":"gaus_NB = GaussianNB()\nprint(select_model(X,y,gaus_NB))","d915d6c1":"multi_NB = MultinomialNB()\nprint(select_model(X,y,multi_NB))","ef3b4d41":"data_clean_test1['tweet_length'].fillna(data_clean_test1['tweet_length'].mean(),inplace=True)","de66d6c5":"multi_NB.fit(X,y)\npredictions = multi_NB.predict(data_clean_test1)","2d5f25d9":"nb_pred = pd.DataFrame(predictions,columns=['target'])\nnb_pred.insert(0,'id',data_test['id'])\nnb_pred.to_csv(\"MysubmissionNB.csv\",index=False)","d89c3e3f":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))\nprint('Accuracy = {:.2f}'.format(logreg.score(X_test, y_test)))","2233167b":"mlp = MLPClassifier()\nmlp = mlp.fit(X_train,y_train)\nmlp_pred = mlp.predict(X_test)\nprint(classification_report(y_test, mlp_pred))\nprint('Accuracy = {:.2f}'.format(mlp.score(X_test, y_test)))","e5805817":"rfc = RandomForestClassifier()\nrfc = rfc.fit(X_train,y_train)\nrfc_pred = rfc.predict(X_test)\nprint(classification_report(y_test, rfc_pred))","1b51473b":"ada = AdaBoostClassifier()\nada = ada.fit(X_train,y_train)\nada_pred = ada.predict(X_test)\nprint(classification_report(y_test, ada_pred))","43ac82ae":"gbc = GradientBoostingClassifier()\ngbc= gbc.fit(X_train,y_train)\ngbc_pred = gbc.predict(X_test)\nprint(classification_report(y_test, gbc_pred))","1c233581":"etc = ExtraTreesClassifier()\netc = etc.fit(X_train,y_train)\netc_pred = etc.predict(X_test)\nprint(classification_report(y_test, etc_pred))","7ae18518":"#from sklearn.ensemble import StackingClassifier\nfrom mlxtend.classifier import StackingClassifier\n\n\n#estimators = [\n #  ('rf', rfc),\n #   ('mlp', make_pipeline(StandardScaler(),\n #                      mlp  )),\n #   ('multi_nb',multi_NB),\n #   ('ada',ada),\n #   ('etc',etc)\n #]","5e6fc803":"clf = StackingClassifier(classifiers = [ada,etc,mlp,rfc,multi_NB],meta_classifier=logreg)","0c1295f4":"#clf = StackingClassifier(\n#    estimators=estimators, final_estimator=logreg)\nclf.fit(X,y)\npredictions = clf.predict(data_clean_test1)","d1a5173c":"stacking_pred = pd.DataFrame(predictions,columns=['target'])\nstacking_pred.insert(0,'id',data_test['id'])\nstacking_pred.to_csv(\"MysubmissionStack.csv\",index=False)\n#0.8036","707b79b0":"from sklearn.model_selection import RandomizedSearchCV\n\nmodel = XGBClassifier()\n\nparameters = {\n    'n_estimators' : [600],\n    'learning_rate' : [0.4],\n    'max_depth' : [6],\n    'booster' : ['gbtree'],\n    'n_jobs' : [-1],\n    'objective' : ['binary:logistic']\n}","c4958e1d":"random_cv = RandomizedSearchCV(estimator = model ,\n                               param_distributions = parameters,\n                               cv = 5,n_iter=50,scoring=f1,verbose=5,return_train_score=True)","ad95747a":"random_cv.fit(X,y)","34115216":"random_cv.best_estimator_","c4fbc829":"predictions = random_cv.predict(data_clean_test1)","c07b2df7":"xgbpred = pd.DataFrame(predictions,columns=['target'])\nxgbpred.insert(0,'id',data_test['id'])\nxgbpred.to_csv(\"MysubmissionXGB.csv\",index=False)","8b23ced6":"model = CatBoostClassifier()\nprint(select_model(X,y,model))","7fe4abaf":"model = LGBMClassifier(n_estimators=300)\nprint(select_model(X,y,model))","fa8b7196":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout\nfrom keras.preprocessing import text, sequence\nfrom keras import utils\nimport tensorflow as tf","a6940d18":"from keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras import regularizers\n\nmodel = Sequential()\nmodel.add(Dense(512, activation='relu', input_dim=(12359),\n                kernel_regularizer=regularizers.l2(0.01),\n                activity_regularizer=regularizers.l1(0.01)))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(256, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation='sigmoid'))\n\nadam = Adam(lr=1e-4, beta_1=0.9, beta_2=0.999, epsilon=10**-8, decay=0.0001, amsgrad=False)\nmodel.compile(optimizer= adam,\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\nprint(model.summary())","21ab22af":"hist = model.fit(X, y,\n                    batch_size=16,\n                    epochs=20,\n                    verbose=1)","d1a0ace1":"%matplotlib inline\n\nhistory = pd.DataFrame(hist.history)\nplt.figure(figsize=(12,12))\nplt.plot(history[\"loss\"],label='Train Loss')\nplt.plot(history[\"val_loss\"],label='Validation Loss')\nplt.title(\"Loss as function of epoch\");\nplt.xlabel('epoch')\nplt.ylabel('loss')\nplt.show()","f97c65c7":"predictions_val = model.predict(data_clean_test1)\npredictions_val = np.where(predictions_val>0.5, 1, 0)","efe9290e":"df_predictions = pd.DataFrame(predictions_val,columns=['target'])\ndf_predictions.insert(0,'id',data_test['id'])\ndf_predictions.to_csv(\"MysubmissionDNN.csv\",index=False)","96c31896":"from nltk.tokenize import word_tokenize\nimport gensim\nfrom tqdm.notebook import tqdm\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.models import Model , Sequential\n\nfrom tensorflow.keras.layers import Embedding , LSTM , Dense , SpatialDropout1D , Dropout\nfrom tensorflow.keras.initializers import Constant \nfrom tensorflow.keras.optimizers import Adam","29928d71":"data_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndata_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","78e240ac":"data_train.drop(['id','location'],axis=1,inplace=True)\ndata_test.drop(['id','location'],axis=1,inplace=True)","ce7ae7bb":"data_train.head()","e33ed9f4":"data_train['text'] = data_train['text'].map(round1)\ndata_test['text'] = data_test['text'].map(round1)","51108d1b":"data_train['text_tokenized']= data_train['text'].map(lambda x : word_tokenize(x))\ndata_test['text_tokenized']= data_test['text'].map(lambda x : word_tokenize(x))","7d5fce1b":"data_train['text_tokenized']=data_train['text_tokenized'].map(lambda x: convert_abbrev(x))\ndata_test['text_tokenized']=data_test['text_tokenized'].map(lambda x: convert_abbrev(x))","7308359c":"data_train['text']=data_train['text_tokenized'].map(lambda x : ' '.join(x))\ndata_test['text']=data_test['text_tokenized'].map(lambda x : ' '.join(x))","943cde2e":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text) \n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","798daf6a":"data_train['text'] = data_train['text'].map(lambda x:remove_URL(x))\ndata_train['text'] = data_train['text'].map(lambda x:remove_emoji(x))\ndata_train['text'] = data_train['text'].map(lambda x:remove_punct(x))\ndata_train['text'] = data_train['text'].map(lambda x:remove_html(x))","a94164ed":"data_test['text'] = data_test['text'].map(lambda x:remove_URL(x))\ndata_test['text'] = data_test['text'].map(lambda x:remove_emoji(x))\ndata_test['text'] = data_test['text'].map(lambda x:remove_punct(x))\ndata_test['text'] = data_test['text'].map(lambda x:remove_html(x))","01a4d8c2":"df = pd.concat([data_train,data_test])","4a0c9a04":"def create_corpus(df) :\n    corpus = []\n    for tweet in tqdm(df['text']) :\n        words = [word.lower() for word in word_tokenize(tweet) ]\n        corpus.append(words)\n    return corpus    ","1f37a073":"corpus = create_corpus(df)","cc0e5b82":"embedding_dict = {}\n\nwith open('..\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt','r') as f :\n    for line in f :\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:],'float32')\n        embedding_dict[word] = vectors\nf.close()","7d35c0f0":"MAX_LEN = 50\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(corpus)\nsequences = tokenizer.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences , maxlen=MAX_LEN,truncating = 'post' , padding='post')","4355d63a":"word_index = tokenizer.word_index\nprint('Number of unique words :',len(word_index))","aa32ce6c":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words,100))\n\nfor word , i in tqdm(word_index.items()) :\n    if i < num_words :\n        emb_vec = embedding_dict.get(word)\n        if emb_vec is not None :\n            embedding_matrix[i] = emb_vec","0600953b":"model = Sequential()\n\nembedding = Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),input_length=MAX_LEN , trainable=False)\nmodel.add(embedding)\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(LSTM(100,dropout=0.2,recurrent_dropout=0.2))\nmodel.add(Dense(1,activation='sigmoid'))\n\noptimizer = Adam(lr=3e-4)\n\nmodel.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])","0594ab51":"train = tweet_pad[:data_train.shape[0],:]\ntest = tweet_pad[data_train.shape[0]:,:]","d31540c1":"history = model.fit(train,data_train['target'],epochs=10,batch_size=4)","cf5cb1c9":"predictions_glove = model.predict(test)\nsub['target'] = predictions_glove.round().astype(int)\nsub.to_csv('submission_Glove.csv',index=False)","e63c74ba":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","d67ad81a":"from tensorflow.keras.layers import Input\nfrom tensorflow.keras.callbacks import ModelCheckpoint \n\nimport tensorflow_hub as hub\nimport tokenization","258c4dcf":"def bert_encoder(texts,tokenizer,max_len=512) :\n    \n    all_tokens = [] \n    all_masks = []\n    all_segments = []\n    \n    for text in texts :\n        \n        text = tokenizer.tokenize(text)\n        text = text[:max_len-2] # so that we can add cls and sep tokens\n        input_sequence = [\"[CLS]\"]+text+[\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        \n        pad_mask = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_mask)\n        all_segments.append(segment_ids)\n        \n        \n    return np.array(all_tokens) , np.array(all_masks) , np.array(all_segments)","841cfad4":"def bert_model(bert_layer , max_len=512) :\n    \n    input_word_ids = Input(shape=(max_len,) ,dtype=tf.int32 , name = 'input_word_ids' )\n    input_word_masks = Input(shape=(max_len,),dtype=tf.int32,name='input_mask')\n    input_word_segments = Input(shape=(max_len,),dtype=tf.int32,name='input_segments')\n    \n    _,sequence_outputs = bert_layer([input_word_ids,input_word_masks,input_word_segments])\n    \n    \n    clf_output = sequence_outputs[:,0,:]\n    dense_layer1 = Dense(256,activation='relu')(clf_output)\n    dense_layer1 = Dropout(0.3)(dense_layer1)\n    \n    if Dropout_num == 0 :\n        out = Dense(1,activation = 'sigmoid')(dense_layer1)\n    else :\n        X = Dropout(Dropout_num)(dense_layer1)\n        out = Dense(1, activation='sigmoid')(X)\n        \n    model = Model(inputs = [input_word_ids,input_word_masks,input_word_segments] , outputs = out)   \n    \n    model.compile(optimizer=Adam(lr=learning_rate),loss='binary_crossentropy',metrics=['accuracy'])\n    \n    return model","41a691ee":"#https:\/\/tfhub.dev\/tensorflow\/albert_en_xxlarge\/1     #\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"   \nbert_layer = hub.KerasLayer(module_url, trainable=True)","beb9488f":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","0426b946":"train_input = bert_encoder(data_train.text.values,tokenizer ,max_len = 160)\ntest_input = bert_encoder(data_test.text.values,tokenizer, max_len = 160)\ntrain_labels = data_train.target.values","5ab2d06b":"Dropout_num = 0.3\nlearning_rate = 6e-6\n\nmodel_bert = bert_model(bert_layer,max_len = 160)\nmodel_bert.summary()","0f22cf5d":"checkpoint = ModelCheckpoint('model_BERT.h5' , monitor='val_loss',save_best_only=True)\n\nepochs = 3\nbatch_size = 16\n\nhistory = model_bert.fit(\n    train_input,train_labels,\n    validation_split = 0.2,\n    epochs = epochs,\n    batch_size = batch_size,\n    callbacks = [checkpoint]\n\n)","d5504bf7":"model_bert.load_weights('model_BERT.h5')\npredictions = model_bert.predict(test_input)\nsub['target'] = predictions.round().astype(int)\nsub.to_csv('submission_bert.csv',index=False)","7875f877":"from transformers import TFAutoModel, AutoTokenizer","b496b907":"# Modeling","3d359679":"I've tried many models as well such as ADAboostClassifier, MLPClassifier, RandomForestClassifier , CatBoost and LightGBM\ni still need a lot of computational power to fine tune them even though RandomizedSearch was more helpful than GridSearch","c6066212":"# Dealing with missing locations and keywords","3a08ea8e":"=> next up i'll try to use BERT or AutoML\nLeave your comments below and let me know what you think !","d94f3c3f":"## Tokenizing and removing stop words","72320d5d":"Finally i would like to thank many existing notebooks that have helped me a lot specially with the DNN architecture since i'm still a beginner any advice or question is welcomed !","5a19af64":"# Adding 2 features: whether location or keyword exist or not ","b50007d1":"# Model LSTM + GloVe :","5b73344b":"## Using Word clouds to figure out most frequent words in Tweets that are real or not","eefd4df5":"# I'm back ROBERTAAAAAAAA !!!","a574c969":"After all attempts location and keywords still have missing values I decided to fill them with mode","7d24a720":"Removing redundant categories from both keyword and location","a7458344":"# Target Encoding some features","260877f4":"# Data Preprocessing","987608dc":"# Reading Data","52807044":"# Stemming","29e32e16":"# Stacking","85a1099e":"Features such as tweet length, unique words and average word length that improved the score.","978ea9b5":"# Importing libraries needed","f6cfce42":"# Vecotorizing","4b1afae3":"Features such as no location and no keyword which indicate whether the location or keyword existed didn't improve the models","369d5afe":"# Bert with tf-hub","aabeb0c5":"# Adding 3 features Number of words in the tweet, number of unique words in the tweet and average word length","bc3fd251":"# Summary","165659b7":"# Slangs Fix","ba45620c":"During this competition i learned a lot, i've tried to do my best to get the best f1-score i can get. I've tried several other things that are not in this notebook such as TF-IDF sadly it didn't improve model's score i've also had trouble with computational power despite using google colab's GPU for that i applied PCA for dimentionality reduction."}}