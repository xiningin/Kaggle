{"cell_type":{"942ee1f7":"code","ac76d88e":"code","9b2e6cd6":"code","6ea6b30f":"code","f7d92618":"code","af834ae5":"code","b09d20cc":"code","8f34f968":"code","1d2842f4":"code","9a49b28c":"code","e8a860fd":"code","30781ce0":"code","fa332de7":"code","eeff199f":"code","1fda9ea4":"code","ee6e0971":"code","4b637ee8":"code","82c56583":"code","b7c2f2e7":"code","ce6da3d1":"code","feea1005":"code","9e58513b":"code","4d824b2c":"code","1bff5697":"code","b824ce5c":"code","14556bdf":"code","1eaad50e":"code","0ec0d0f7":"code","6d184db5":"code","9a3b9b47":"code","d1ee02b7":"code","e6ef5cc8":"code","89b3eb9d":"code","80627486":"code","fbfdd097":"code","1ff8d1d4":"code","4b2d8056":"code","261b21fd":"markdown","8cf96fed":"markdown","0a7dc3c9":"markdown","3d09d31a":"markdown","3073ce57":"markdown","f1e58cf3":"markdown","137a482f":"markdown","99413852":"markdown","8c2f9b27":"markdown","0cbfd4f8":"markdown","3ff5f50d":"markdown","e7080a22":"markdown","ca9b40ba":"markdown","4106b13f":"markdown","31ace880":"markdown","7961c504":"markdown","e82546ca":"markdown","5dc7f7c0":"markdown","105d538e":"markdown","97eba0c4":"markdown","a646c5e8":"markdown","87b710bb":"markdown","6c1330e6":"markdown","4b011f9d":"markdown","04f6c0d3":"markdown","98410083":"markdown","3812cf3e":"markdown","dd837f4c":"markdown","c623054b":"markdown","29d07a8e":"markdown","b711b869":"markdown","ab1ed4e3":"markdown","dcbf0e0a":"markdown","9bd4c636":"markdown","a558cac2":"markdown","cf7be7de":"markdown","8c2b87bb":"markdown","c4f0c5c8":"markdown","0cd160e3":"markdown","d705e8c5":"markdown"},"source":{"942ee1f7":"import numpy as np\nimport pandas as pd\n\nimport os\nimport random\nfrom operator import itemgetter\nimport copy\nimport time\n\nimport torch\nimport torchvision\nimport torchvision.transforms as transform\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader, Dataset, ConcatDataset\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.utils import make_grid\nimport torch.nn.functional as F\n\nfrom mlxtend.plotting import plot_confusion_matrix\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom matplotlib.image import imread\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice= torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')","ac76d88e":"example = '..\/input\/flowers-recognition\/flowers\/flowers\/daisy'\npath = '..\/input\/flowers-recognition\/flowers\/flowers'","9b2e6cd6":"img = mpimg.imread(example + '\/100080576_f52e8ee070_n.jpg')\nprint('Shape:', img.shape)\nplt.imshow(img);","6ea6b30f":"def plotHist(img):\n  plt.figure(figsize=(10,5))\n  plt.subplot(1,2,1)\n  plt.imshow(img)\n  plt.axis('off')\n  histo = plt.subplot(1,2,2)\n  histo.set_ylabel('Count')\n  histo.set_xlabel('Pixel Intensity')\n  plt.hist(img.flatten(), bins=10, lw=0, alpha=0.5, color='r')\n\nplotHist(img)","f7d92618":"transformer = {\n    'original': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.ToTensor(), \n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))\n]), \n   'dataset1': transform.Compose([\n                           transform.Resize((220, 220)),\n                           transform.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n                           transform.RandomRotation(5),\n                           transform.RandomAffine(degrees=11, translate=(0.1,0.1), scale=(0.8,0.8)),\n                           transform.ToTensor(),\n                           transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                               (0.3268945515155792, 0.29282665252685547, 0.29053378105163574)),\n]), \n   'dataset2': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.RandomHorizontalFlip(),\n                                 transform.RandomRotation(10),\n                                 transform.RandomAffine(translate=(0.05,0.05), degrees=0),\n                                 transform.ToTensor(),\n                                 transform.RandomErasing(inplace=True, scale=(0.01, 0.23)),\n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))]),\n   'dataset3': transform.Compose([\n                                 transform.Resize((220, 220)),\n                                 transform.RandomHorizontalFlip(p=0.5),\n                                 transform.RandomRotation(15),\n                                 transform.RandomAffine(translate=(0.08,0.1), degrees=15),\n                                 transform.ToTensor(),\n                                 transform.Normalize((0.4124234616756439, 0.3674212694168091, 0.2578217089176178), \n                                                     (0.3268945515155792, 0.29282665252685547, 0.29053378105163574))\n                                                     \n])\n       }","af834ae5":"bs = 50\n\noriginal = ImageFolder(path, transform=transformer['original'])\n\n#all_set = train_val + test\ntrain_val, test = train_test_split(original, test_size=0.2, shuffle=True, random_state=43)\n\n#train_val = train + val + dataset1 + dataset2 + dataset3\ntrain_val = ConcatDataset([train_val, \n                           ImageFolder(path, transform=transformer['dataset1']),\n                           ImageFolder(path, transform=transformer['dataset2']),\n                           ImageFolder(path, transform=transformer['dataset3'])]) \n\ntrain, val = train_test_split(train_val, test_size=0.1, shuffle=True, random_state=43)\n\nloaders = {\n    'train': DataLoader(train, batch_size=bs, num_workers=4, pin_memory=True),\n    'val': DataLoader(val, batch_size=bs, num_workers=4, pin_memory=True),\n    'test': DataLoader(test, batch_size=bs, num_workers=4, pin_memory=True)\n}\n\ndataset_sizes = {\n    'train': len(train),\n    'val': len(val), \n    'test': len(test),\n}","b09d20cc":"exampleset = ImageFolder(path, transform=transform.Compose([transform.ToTensor(),\n                                                            transform.CenterCrop(255),]))\n\nx, y = next(iter(DataLoader(exampleset)))\n\nchannels = ['Red', 'Green', 'Blue']\ncmaps = [plt.cm.Reds_r, plt.cm.Greens_r, plt.cm.Blues_r]\n\nfig, ax = plt.subplots(1, 4, figsize=(15, 10))\n\nfor i, axs in enumerate(fig.axes[:3]):\n    axs.imshow(x[0][i,:,:], cmap=cmaps[i])\n    axs.set_title(f'{channels[i]} Channel')\n    axs.set_xticks([])\n    axs.set_yticks([])\n    \nax[3].imshow(x[0].permute(1,2,0))\nax[3].set_title('Three Channels')\nax[3].set_xticks([])\nax[3].set_yticks([]);","8f34f968":"channels = 3\n\nfor channel in range(channels):\n    for x in ['train', 'val', 'test']:\n        #number of pixels in the dataset = number of all pixels in one object * number of all objects in the dataset\n        num_pxl = dataset_sizes[x]*220*220\n    \n        #we go through the butches and sum up the pixels of the objects, \n        #which then divide the sum by the number of all pixels to calculate the average\n        total_sum = 0\n        for batch in loaders[x]:\n            layer = list(map(itemgetter(channel), batch[0]))\n            layer = torch.stack(layer, dim=0)\n            total_sum += layer.sum()\n        mean = total_sum \/ num_pxl\n\n        #we calculate the standard deviation using the formula that I indicated above\n        sum_sqrt = 0\n        for batch in loaders[x]: \n            layer = list(map(itemgetter(channel), batch[0]))\n            sum_sqrt += ((torch.stack(layer, dim=0) - mean).pow(2)).sum()\n        std = torch.sqrt(sum_sqrt \/ num_pxl)\n        \n        print(f'|channel:{channel+1}| {x} - mean: {mean}, std: {std}')","1d2842f4":"x, y = next(iter(loaders['train']))\nx.mean(),  x.std()","9a49b28c":"x, y = next(iter(loaders['train']))\nimg_norm = x[0].permute(1,2,0).numpy()\nplotHist(img_norm)","e8a860fd":"print('Classes:', original.classes)\nprint('Number of classes:', len(original.classes))","30781ce0":"dic = {}\n\nfor classes in original.classes:\n  dic[classes] = [len([os.path.join(path+'\/'+classes, filename) for filename in os.listdir(path+'\/'+classes)])]\n\nsamplesize = pd.DataFrame.from_dict(dic)","fa332de7":"samplesize","eeff199f":"figure_size = plt.rcParams['figure.figsize']\nfigure_size[0] = 40\nfigure_size[1] = 20\nplt.rcParams['figure.figsize'] = figure_size\n\nsns.barplot(data=samplesize)\n\nindex = np.arange(len(original.classes))\n\nplt.xlabel('Flowers', fontsize=25)\nplt.ylabel('Count of flowers', fontsize=25)\nplt.xticks(index, original.classes, fontsize=25)\nplt.title('Class Distrubution', fontsize=35)\nplt.show()","1fda9ea4":"# Function for plotting samples\ndef plot_samples(samples):  \n    fig, ax = plt.subplots(nrows=5, ncols=5, figsize=(15,12))\n    i = 0\n    for row in range(5):\n         for col in range(5):\n                img = mpimg.imread(samples[i][0][0])\n                ax[row][col].imshow(img)\n                ax[row][col].axis('off')\n                ax[row][col].set_title(samples[i][1], fontsize=15)\n                i+=1\n  \n\nrand_samples = [] \nfor _ in range(25): \n    classes = random.choice(original.classes)\n    rand_samples.append([random.sample([os.path.join(path+'\/'+classes, filename) for filename in os.listdir(path+'\/'+classes)], 1), classes]) \nrand_samples[0]\nplot_samples(rand_samples)\nplt.suptitle('Samples', fontsize=30)\nplt.show()","ee6e0971":"def show_batch(dl):\n    for images, labels in dl:\n        fig, ax = plt.subplots(figsize=(25, 25))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(images[:60], nrow=10).permute(1, 2, 0))\n        ax.set_title('Images with augmentation', fontsize=40)\n        break\n        \nshow_batch(loaders['train'])","4b637ee8":"def accuracy(outputs, labels):\n    _, preds = torch.max(outputs, dim=1) \n    return torch.tensor(torch.sum(preds == labels).item() \/ len(preds)), preds","82c56583":"#save the losses for further visualization\nlosses = {'train':[], 'val':[]}\naccuracies = {'train':[], 'val':[]}\nlr = []","b7c2f2e7":"def train(seed, epochs, model):\n    \n  print('Creating a model {}...'.format(seed))\n\n  model.to(device)  \n  criterion = nn.CrossEntropyLoss()\n  if seed==2 or seed==3:\n    optimizer = torch.optim.Adam(model.fc.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay = 1e-5)\n  else:\n    optimizer = torch.optim.Adam(model.classifier.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)\n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=3, verbose=True)\n  #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.1, epochs=epochs, steps_per_epoch=len(loaders['train']), cycle_momentum=True)\n  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)\n  since = time.time()\n  best_model = copy.deepcopy(model.state_dict())\n  best_acc = 0.0\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n#             lr.append(scheduler.get_lr())\n#             scheduler.step()\n\n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n      if phase == 'train':\n          acc = 100. * running_corrects.double() \/ dataset_sizes[phase]\n          scheduler.step(acc)\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double()\/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}\/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n      lr.append(scheduler._last_lr)\n        \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n        print('=='*31)\n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    #scheduler.step() \n  time_elapsed = time.time() - since\n  print('CLASSIFIER TRAINING TIME {}m {}s'.format(time_elapsed\/\/60, time_elapsed%60))\n  print('=='*31)\n\n\n  model.load_state_dict(best_model)\n\n  for param in model.parameters():\n        param.requires_grad=True\n\n  optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0)  \n  scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n  #scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 3, gamma=0.1)\n  #scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, 0.001, epochs=epochs, steps_per_epoch=len(loaders['train']), cycle_momentum=True)\n  for epoch in range(epochs):\n    for phase in ['train', 'val']:\n      if phase == 'train':\n        model.train()\n      else:\n        model.eval()\n      \n      running_loss = 0.0\n      running_corrects = 0.0\n\n      for inputs, labels in loaders[phase]:\n        inputs, labels = inputs.to(device), labels.to(device)\n\n        optimizer.zero_grad()\n\n        with torch.set_grad_enabled(phase=='train'):\n          outp = model(inputs)\n          _, pred = torch.max(outp, 1)\n          loss = criterion(outp, labels)\n        \n          if phase == 'train':\n            loss.backward()\n            optimizer.step()\n#             lr.append(scheduler.get_lr())\n#             scheduler.step()\n\n        running_loss += loss.item()*inputs.size(0)\n        running_corrects += torch.sum(pred == labels.data)\n\n      if phase == 'train':\n        acc = 100. * running_corrects.double() \/ dataset_sizes[phase]\n        scheduler.step(acc)\n\n      epoch_loss = running_loss \/ dataset_sizes[phase]\n      epoch_acc = running_corrects.double()\/dataset_sizes[phase]\n      losses[phase].append(epoch_loss)\n      accuracies[phase].append(epoch_acc)\n      if phase == 'train':\n        print('Epoch: {}\/{}'.format(epoch+1, epochs))\n      print('{} - loss:{}, accuracy{}'.format(phase, epoch_loss, epoch_acc))\n      lr.append(scheduler._last_lr)\n    \n      if phase == 'val':\n        print('Time: {}m {}s'.format((time.time()- since)\/\/60, (time.time()- since)%60))\n        print('=='*31)    \n      if phase == 'val' and epoch_acc > best_acc:\n        best_acc = epoch_acc\n        best_model = copy.deepcopy(model.state_dict())\n    #scheduler.step() \n  time_elapsed = time.time() - since\n  print('ALL NET TRAINING TIME {}m {}s'.format(time_elapsed\/\/60, time_elapsed%60))\n  print('=='*31)\n\n  model.load_state_dict(best_model)\n  return model","ce6da3d1":"densenet121_0 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_0.parameters():\n  param.requires_grad=False\n\ndensenet121_0.classifier = nn.Linear(in_features=densenet121_0.classifier.in_features, out_features=len(original.classes), bias=True)","feea1005":"densenet121_1 = torchvision.models.densenet121(pretrained=True)\nfor param in densenet121_1.parameters():\n  param.requires_grad=False\n\ndensenet121_1.classifier = nn.Linear(in_features=densenet121_1.classifier.in_features, out_features=len(original.classes), bias=True)","9e58513b":"googlenet = torchvision.models.googlenet(pretrained=True)\nfor param in googlenet.parameters():\n  param.grad_requires = False\n\ngooglenet.fc = nn.Linear(in_features=googlenet.fc.in_features, out_features=len(original.classes), bias=True)","4d824b2c":"resnet101 = torchvision.models.resnet101(pretrained=True)\nfor param in resnet101.parameters():\n  param.grad_requires = False\n\nresnet101.fc = nn.Linear(in_features=resnet101.fc.in_features, out_features=len(original.classes), bias=True)","1bff5697":"vgg19_bn = torchvision.models.vgg19_bn(pretrained=True)\nfor param in vgg19_bn.parameters():\n  param.grad_requires = False\n\nvgg19_bn.classifier[6] = nn.Linear(4096, len(original.classes), bias=True)","b824ce5c":"num_models = 5\nepochs = 10\n\nmodels = [densenet121_0, densenet121_1, googlenet, resnet101, vgg19_bn]\n\nfor seed in range(num_models):\n   train(seed=seed, epochs=epochs, model=models[seed])","14556bdf":"fig, ax = plt.subplots(5, 2, figsize=(15, 15))\nmodelname = ['DenseNet_0', 'DenseNet_1', 'GooglNet', 'ResNet101', 'VGG16 with BN']\n\ni=0\n\nfor row in range(5):\n\n  epoch_list = list(range(1,epochs*2+1))\n\n  ax[row][0].plot(epoch_list, accuracies['train'][i:20+i], '-o', label='Train Accuracy')\n  ax[row][0].plot(epoch_list, accuracies['val'][i:20+i], '-o', label='Validation Accuracy')\n  ax[row][0].plot([epochs for x in range(20)],  np.linspace(min(accuracies['train'][i:20+i]).cpu(), max(accuracies['train'][i:20+i]).cpu(), 20), color='r', label='Unfreeze net')\n  ax[row][0].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][0].set_ylabel('Accuracy Value')\n  ax[row][0].set_xlabel('Epoch')\n  ax[row][0].set_title('Accuracy {}'.format(modelname[row]))\n  ax[row][0].legend(loc=\"best\")\n\n  ax[row][1].plot(epoch_list, losses['train'][i:20+i], '-o', label='Train Loss')\n  ax[row][1].plot(epoch_list, losses['val'][i:20+i], '-o',label='Validation Loss')\n  ax[row][1].plot([epochs for x in range(20)], np.linspace(min(losses['train'][i:20+i]), max(losses['train'][i:20+i]), 20), color='r', label='Unfreeze net')\n  ax[row][1].set_xticks(np.arange(0, epochs*2+1, 5))\n  ax[row][1].set_ylabel('Loss Value')\n  ax[row][1].set_xlabel('Epoch')\n  ax[row][1].set_title('Loss {}'.format(modelname[row]))\n  ax[row][1].legend(loc=\"best\")\n  fig.tight_layout()\n  fig.subplots_adjust(top=1.5, wspace=0.3)\n\n  i+=20","1eaad50e":"class Ensemble(nn.Module):\n    def __init__(self, device):\n        super(Ensemble,self).__init__()\n        # you should use nn.ModuleList. Optimizer doesn't detect python list as parameters\n        self.models = nn.ModuleList(models)\n        \n    def forward(self, x):\n        # it is super simple. just forward num_ models and concat it.\n        output = torch.zeros([x.size(0), len(original.classes)]).to(device)\n        for model in self.models:\n            output += model(x)\n        return output","0ec0d0f7":"model =  Ensemble(device)","6d184db5":"def validation_step(batch):\n        images,labels = batch\n        images,labels = images.to(device),labels.to(device)\n        out = model(images)                                      \n        loss = F.cross_entropy(out, labels)                    \n        acc,preds = accuracy(out, labels)                       \n        \n        return {'val_loss': loss.detach(), 'val_acc':acc.detach(), \n                'preds':preds.detach(), 'labels':labels.detach()}","9a3b9b47":"def test_prediction(outputs):\n        batch_losses = [x['val_loss'] for x in outputs]\n        epoch_loss = torch.stack(batch_losses).mean()           \n        batch_accs = [x['val_acc'] for x in outputs]\n        epoch_acc = torch.stack(batch_accs).mean()             \n        # combine predictions\n        batch_preds = [pred for x in outputs for pred in x['preds'].tolist()] \n        # combine labels\n        batch_labels = [lab for x in outputs for lab in x['labels'].tolist()]  \n        \n        return {'test_loss': epoch_loss.item(), 'test_acc': epoch_acc.item(),\n                'test_preds': batch_preds, 'test_labels': batch_labels}","d1ee02b7":"@torch.no_grad()\ndef test_predict(model, test_loader):\n    model.eval()\n    # perform testing for each batch\n    outputs = [validation_step(batch) for batch in test_loader] \n    results = test_prediction(outputs)                          \n    print('test_loss: {:.4f}, test_acc: {:.4f}'\n          .format(results['test_loss'], results['test_acc']))\n    \n    return results['test_preds'], results['test_labels']","e6ef5cc8":"model.to(device)\npreds,labels = test_predict(model, loaders['test'])","89b3eb9d":"def norm_out(img):\n    \n    img = img.permute(1,2,0)\n    mean = torch.FloatTensor([0.4124234616756439, 0.3674212694168091, 0.2578217089176178])\n    std = torch.FloatTensor([0.3268945515155792, 0.29282665252685547, 0.29053378105163574])\n    \n    img = img*std + mean\n        \n    return np.clip(img,0,1)","80627486":"fig, ax = plt.subplots(figsize=(8,12), ncols=2, nrows=4)\n\nfor row in range(4):\n    i = np.random.randint(0, high=len(test))\n    img,label = test[i]\n    \n    m = nn.Softmax(dim=1)\n    percent = m(model(img.to(device).unsqueeze(0)))\n    predmax3percent = torch.sort(percent[0])[0]\n    predmax3inds = torch.sort(percent[0])[1]\n    classes = np.array([original.classes[predmax3inds[-5]], original.classes[predmax3inds[-4]], original.classes[predmax3inds[-3]], original.classes[predmax3inds[-2]],original.classes[predmax3inds[-1]]])\n    class_name = original.classes\n\n    ax[row][0].imshow(norm_out(img))\n    ax[row][0].set_title('Real : {}'.format(class_name[label]))\n    ax[row][0].axis('off')\n    ax[row][1].barh(classes, predmax3percent.detach().cpu().numpy())\n    ax[row][1].set_aspect(0.1)\n    ax[row][1].set_yticks(classes)\n    ax[row][1].set_title('Predicted Class: {} ({}%)'.format(original.classes[predmax3inds[-1]], round((predmax3percent[-1]*100).item(), 2)))\n    ax[row][1].set_xlim(0, 1.)\n    plt.tight_layout()","fbfdd097":"report = classification_report(labels, preds,\n                               output_dict=True,\n                               target_names=original.classes)\nreport_df = pd.DataFrame(report).transpose()","1ff8d1d4":"pd.set_option(\"display.max_rows\", None)\nreport_df.head(134)","4b2d8056":"# Plot confusion matrix\ncm  = confusion_matrix(labels, preds)\nplt.figure()\nplot_confusion_matrix(cm,figsize=(12,8),cmap=plt.cm.Blues)\nplt.xticks(range(len(original.classes)), original.classes, fontsize=16)\nplt.yticks(range(len(original.classes)), original.classes, fontsize=16)\nplt.xlabel('Predicted Label',fontsize=18)\nplt.ylabel('True Label',fontsize=18)\nplt.show()","261b21fd":"**Launching training**","8cf96fed":"# 7. Metrics","0a7dc3c9":"**This might be helpful:**\n\nPredicting pneumonia by X-ray: https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-x-ray-transfer-learning-densenet\n\nFruit prediction for 131 classes(!!!): https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-fruits-transferlearing-ensemble-test99-18\n\nFashionMNIST: https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-fashionmnist-acc-0-94\n\nA similar solution, but with a submission(MNIST top 5%): https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-mnist-transferlearning-ensemble-99-714","3d09d31a":"**This is how we can look at our classes. There are only five of them, which is not much**","3073ce57":"# 3. Training and Test\n\n**Idea:** I will use an ensemble of pre-trained models. I first train only the classifier on 10 epochs, then unfreeze the network and train all together for another 10 epochs","f1e58cf3":"**The function below will normalize the image back to its original. It simply multiplies the tensor by the standard deviation and adds the mean**","137a482f":"**As you can see from the graphs, the unfreeze idea worked. We can see that after the red lines, the performance improves!**","99413852":"In order not to count the accuracy many times, we write the function","8c2f9b27":"**Let's write augmentation and normalization right away. Why are there four datasets?**\n\n**(1)** The \"original\" dataset is the original dataset(wow), which we will split into two parts: test (20%) and training (80%).\n\n**(2)** The dataset \"dataset1\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n\n**(3)** The dataset \"dataset2\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n\n**(4)** The dataset \"dataset3\" is a dataset with augmentation, which we will add to the training part of the original dataset to expand the data.\n","0cbfd4f8":"**4. ResNet**","3ff5f50d":"# 6. Predictions in individual images","e7080a22":"Train function structure:\n\n1. **Classifier Training**\n2. **Network-wide Training**","ca9b40ba":"**Let's take a quick look at the data. I don't know about you, but the first thing I always want to do is look at what our data looks like :)**","4106b13f":"**pin_memory:** You know how sometimes your GPU memory shows that it\u2019s full but you\u2019re pretty sure that your model isn\u2019t using that much? That overhead is called pinned memory. ie: this memory has been reserved as a type of \u201cworking allocation.\u201d\nWhen you enable pinned_memory in a DataLoader it \u201cautomatically puts the fetched data Tensors in pinned memory, and enables faster data transfer to CUDA-enabled GPUs\u201d\n\n**num_workers:** PyTorch allows loading data on multiple processes simultaneously. A good rule: ***num_worker = 4 * num_GPU***","31ace880":"# **1. Data Loading**","7961c504":"**These functions will help us when calculating the accuracy**","e82546ca":"# **0. Importing Libraries**\n","5dc7f7c0":"**The imbalance is small and we do not need to handle it in any way, since the ratio of the largest class to the smallest is 1.47, which is not much**","105d538e":"# MODELS","97eba0c4":"# 4. Loss and Accuracy Plots","a646c5e8":"**And this is how images with augmentation look like**","87b710bb":"**First, let's write an ensemble class. It's very easy!**","6c1330e6":"**In random pictures, the network shows 100% accuracy, this is the most confident answer, so you can make the assumption that the networks have learned well**","4b011f9d":"**We have already normalized the data, but at this stage I would like to dwell in more detail, because this is very important.**\n\nIn datasets, we have three-channel images, that is, we need to normalize for each channel separately (!!!). Because of the unnormalized data, problems may appear, for example, regularization during training can work to the detriment, but we do not want this at all. The task of normalization is to make the mean as close to zero as possible, and the standard deviation around 1. \n\nHow each channel looks separately can be seen below:","04f6c0d3":"**Let's take a look at the pixel distribution after normalization. Compared to the distribution at the beginning, the difference is large**","98410083":"**5. VGG19**","3812cf3e":"**Let's take a batch from the training dataset and see its mean and standard deviation:**","dd837f4c":"**1. DenseNet(1)**","c623054b":"**It seems to me that we have achieved good enough accuracy**","29d07a8e":"**Let's check how imbalanced our data is. Since we are working with images, first we need to make a pandas table, and then render the table.**","b711b869":"# 5. Test set predictions","ab1ed4e3":"**2. DenseNet(2)**","dcbf0e0a":"***I hope you enjoyed it and found something new for yourself!*\n*I am always happy to receive any feedback. What do you think can be changed and what can be removed?***","9bd4c636":"**Paths**","a558cac2":"# **2. Data preparation**\n","cf7be7de":"**From the image above, you can understand that they are not normalized (which is very expected), how strong the spread can be seen using the script below:**","8c2b87bb":"**3. GoogleNet**","c4f0c5c8":"**Now let's check how well we managed to normalize the data for each channel for the test, training and validation datasets:**","0cd160e3":"**Let's see how the images from the original dataset look like without changes:**","d705e8c5":"**This is where we will record the history of learning, so that we can make visualization later. We need visualization to evaluate learning, for example, overfitting or underfitting. Of course, we can analyze with numbers, but it is much easier to perceive information visually**"}}