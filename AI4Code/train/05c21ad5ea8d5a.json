{"cell_type":{"352774d6":"code","649c90b9":"code","0804d90b":"code","55ed5204":"code","be90a7b8":"code","9ad578b1":"code","9e548438":"code","85aabedc":"code","35d12478":"code","fd111ee3":"code","d01f88bc":"code","57ecafbd":"code","56a41691":"code","30581233":"code","c332a9e5":"code","b1923f18":"code","483209c3":"code","19e5c936":"code","7836778b":"code","ac21a882":"code","d978cf6b":"code","16f1ca15":"code","7f62e133":"code","b8f650c7":"code","eade28bc":"code","057af610":"code","b69e3a46":"code","08c492a9":"code","6d7d3e95":"code","b9312101":"markdown","d12d7861":"markdown","05f82bcf":"markdown","85034cca":"markdown","3af43081":"markdown","b493d17e":"markdown","ada2d5d5":"markdown","7d5c9c39":"markdown","e6c15076":"markdown","d6fa8e03":"markdown","03826802":"markdown","2c493fbb":"markdown","adc7fa26":"markdown","c522d2be":"markdown","5fef5954":"markdown","be71592f":"markdown","75635b78":"markdown","b7561afb":"markdown","5e1c41d8":"markdown","0f1d9649":"markdown","bda2c870":"markdown","9451c206":"markdown","dd190c31":"markdown","6020b5cd":"markdown","59ef12b3":"markdown","e9b140a6":"markdown","d1de0fc4":"markdown","8ddcec71":"markdown","68a6340d":"markdown","9c4a6efd":"markdown","610eff20":"markdown","44aef274":"markdown","47b7cae2":"markdown","f8dfc04a":"markdown","ce3f96a2":"markdown","ad6edc3a":"markdown","20936676":"markdown","4e4540ea":"markdown","97cd1d4a":"markdown","b3a0b543":"markdown","2a012416":"markdown","6f744a73":"markdown","8afa032c":"markdown","e1a5e0ef":"markdown","dea79451":"markdown","3990b66f":"markdown","d29917a1":"markdown","bd9df94e":"markdown","57806baf":"markdown","1346dd52":"markdown","1f368ec3":"markdown","e64dee6e":"markdown","49334a92":"markdown","50e73df1":"markdown","894805c4":"markdown","727643ec":"markdown","ec8f259e":"markdown","a134fdc4":"markdown","34385347":"markdown","97663704":"markdown"},"source":{"352774d6":"# @title Tutorial slides\n\n# @markdown These are the slides for the videos in all tutorials today\nfrom IPython.display import IFrame\nIFrame(src=f\"https:\/\/mfr.ca-1.osf.io\/render?url=https:\/\/osf.io\/4ye56\/?direct%26mode=render%26action=download%26mode=render\", width=854, height=480)","649c90b9":"# @title Install dependencies\n!pip install git+https:\/\/github.com\/NeuromatchAcademy\/evaltools --quiet\nfrom evaltools.airtable import AirtableForm\n\n# generate airtable form\natform = AirtableForm('appn7VdPRseSoMXEG','W1D3_T1','https:\/\/portal.neuromatchacademy.org\/api\/redirect\/to\/9c55f6cb-cdf9-4429-ac1c-ec44fe64c303')","0804d90b":"# Imports\nimport random\n\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch.nn as nn\nimport torch.optim as optim\nfrom tqdm.auto import tqdm\nfrom IPython.display import display\nfrom torch.utils.data import DataLoader, TensorDataset","55ed5204":"# @title Figure settings\nimport ipywidgets as widgets       # interactive display\n%config InlineBackend.figure_format = 'retina'\nplt.style.use(\"https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/content-creation\/main\/nma.mplstyle\")","be90a7b8":"# @title Plotting functions\n\ndef imshow(img):\n  img = img \/ 2 + 0.5     # unnormalize\n  npimg = img.numpy()\n  plt.imshow(np.transpose(npimg, (1, 2, 0)))\n  plt.axis(False)\n  plt.show()\n\n\ndef plot_function_approximation(x, relu_acts, y_hat):\n  fig, axes = plt.subplots(2, 1)\n\n  # Plot ReLU Activations\n  axes[0].plot(x, relu_acts.T);\n  axes[0].set(xlabel='x',\n              ylabel='Activation',\n              title='ReLU Activations - Basis Functions')\n  labels = [f\"ReLU {i + 1}\" for i in range(relu_acts.shape[0])]\n  axes[0].legend(labels, ncol = 2)\n\n  # Plot function approximation\n  axes[1].plot(x, torch.sin(x), label='truth')\n  axes[1].plot(x, y_hat, label='estimated')\n  axes[1].legend()\n  axes[1].set(xlabel='x',\n              ylabel='y(x)',\n              title='Function Approximation')\n\n  plt.tight_layout()\n  plt.show()","9ad578b1":"# @title Set random seed\n\n# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n\n# for DL its critical to set the random seed so that students can have a\n# baseline to compare their results to expected results.\n# Read more here: https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\n\n# Call `set_seed` function in the exercises to ensure reproducibility.\n#import random\n# import torch\n\ndef set_seed(seed=None, seed_torch=True):\n  if seed is None:\n    seed = np.random.choice(2 ** 32)\n  random.seed(seed)\n  np.random.seed(seed)\n  if seed_torch:\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.benchmark = False\n    torch.backends.cudnn.deterministic = True\n\n  print(f'Random seed {seed} has been set.')\n\n\n# In case that `DataLoader` is used\ndef seed_worker(worker_id):\n  worker_seed = torch.initial_seed() % 2**32\n  np.random.seed(worker_seed)\n  random.seed(worker_seed)","9e548438":"# @title Set device (GPU or CPU). Execute `set_device()`\n# especially if torch modules used.\n\n# inform the user if the notebook uses GPU or CPU.\n\ndef set_device():\n  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n  if device != \"cuda\":\n    print(\"GPU is not enabled in this notebook. \\n\"\n          \"If you want to enable it, in the menu under `Runtime` -> \\n\"\n          \"`Hardware accelerator.` and select `GPU` from the dropdown menu\")\n  else:\n    print(\"GPU is enabled in this notebook. \\n\"\n          \"If you want to disable it, in the menu under `Runtime` -> \\n\"\n          \"`Hardware accelerator.` and select `None` from the dropdown menu\")\n\n  return device","85aabedc":"SEED = 2021\nset_seed(seed=SEED)\nDEVICE = set_device()","35d12478":"# @title Video 0: Introduction\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1E3411r7TL\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"Gh0KYl7ViAc\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n#add event to airtable\natform.add_event('Video 0: Introduction')\n\ndisplay(out)","fd111ee3":"# @title Video 1: Universal Approximation Theorem\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1SP4y147Uv\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"tg8HHKo1aH4\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 1: Universal Approximation Theorem')\n\ndisplay(out)","d01f88bc":"def approximate_function(x_train, y_train):\n\n  ####################################################################\n  # Fill in missing code below (...),\n  # then remove or comment the line below to test your function\n  ## raise NotImplementedError(\"Complete approximate_function!\")\n  ####################################################################\n\n  # Number of relus\n  n_relus = x_train.shape[0] - 1\n\n  # x axis points (more than x train)\n  x = torch.linspace(torch.min(x_train), torch.max(x_train), 1000)\n\n  ## COMPUTE RELU ACTIVATIONS\n\n  # First determine what bias terms should be for each of `n_relus` ReLUs\n  # take 9 elements\n  b = -x_train[:-1]\n\n  # Compute ReLU activations for each point along the x axis (x)\n  relu_acts = torch.zeros((n_relus, x.shape[0]))\n\n  for i_relu in range(n_relus):\n    relu_acts[i_relu, :] = torch.relu(x + b[i_relu])\n\n  ## COMBINE RELU ACTIVATIONS\n\n  # Set up weights for weighted sum of ReLUs\n  combination_weights = torch.zeros((n_relus, ))\n\n  # Figure out weights on each ReLU\n  prev_slope = 0\n  for i in range(n_relus):\n    delta_x = x_train[i+1] - x_train[i]\n    slope = (y_train[i+1] - y_train[i]) \/ delta_x\n    combination_weights[i] = slope - prev_slope\n    prev_slope = slope\n\n  # Get output of weighted sum of ReLU activations for every point along x axis\n  y_hat = combination_weights @ relu_acts\n\n  return y_hat, relu_acts, x\n\n# add event to airtable\natform.add_event('Coding Exercise 1: Function approximation with ReLU')\n\n\n# Make training data from sine function\nN_train = 10\nx_train = torch.linspace(0, 2*np.pi, N_train).view(-1, 1)\ny_train = torch.sin(x_train)\n\n## Uncomment the lines below to test your function approximation\ny_hat, relu_acts, x = approximate_function(x_train, y_train)\nplot_function_approximation(x, relu_acts, y_hat)","57ecafbd":"# @title Video 2: Building MLPs in PyTorch\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1zh411z7LY\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"XtwLnaYJ7uc\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 2: Building MLPs in PyTorch')\n\ndisplay(out)","56a41691":"class Net(nn.Module): # nn.Module creates backward function and the graph\n  def __init__(self, actv, input_feature_num, hidden_unit_nums, output_feature_num):\n    super(Net, self).__init__()\n    self.input_feature_num = input_feature_num # save the input size for reshapinng later\n    self.mlp = nn.Sequential() # Initialize layers of MLP (container)\n\n    in_num = input_feature_num # initialize the temporary input feature to each layer\n    for i in range(len(hidden_unit_nums)): # Loop over layers and create each one\n\n      ####################################################################\n      # Fill in missing code below (...),\n      # then remove or comment the line below to test your function\n      # raise NotImplementedError(\"Create MLP Layer\")\n      ####################################################################\n\n      out_num = hidden_unit_nums[i] # assign the current layer hidden unit from list\n      layer = nn.Linear(in_num, out_num) # inputs outputs use nn.Linear to define the layer\n      in_num = out_num # assign next layer input using current layer output\n      self.mlp.add_module('Linear_%d'%i, layer) # append layer to the model with a name\n\n      actv_layer = eval('nn.%s'%actv) # Assign activation function (eval allows us to instantiate object from string)\n      self.mlp.add_module('Activation_%d'%i, actv_layer) # append activation to the model with a name\n\n    out_layer = nn.Linear(in_num, output_feature_num) # Create final layer\n    self.mlp.add_module('Output_Linear', out_layer) # append the final layer\n\n  def forward(self, x):\n    # reshape inputs to (batch_size, input_feature_num)\n    # just in case the input vector is not 2D, like an image!\n    x = x.view(-1, self.input_feature_num)\n\n    ####################################################################\n    # Fill in missing code below (...),\n    # then remove or comment the line below to test your function\n    # raise NotImplementedError(\"Run MLP model\")\n    ####################################################################\n\n    logits = self.mlp.forward(x) # forward pass of MLP\n    return logits\n\n# add event to airtable\natform.add_event('Coding Exercise 2: Implement a general-purpose MLP in Pytorch')\n\n\ninput = torch.zeros((100, 2))\n## Uncomment below to create network and test it on input\nnet = Net(actv='LeakyReLU(0.1)', input_feature_num=2, hidden_unit_nums=[100, 10, 5], output_feature_num=1).to(DEVICE)\ny = net(input.to(DEVICE))\nprint(f'The output shape is {y.shape} for an input of shape {input.shape}')","30581233":"# @title Video 3: Cross Entropy\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1Ag41177mB\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"N8pVCbTlves\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 3: Cross Entropy')\n\ndisplay(out)","c332a9e5":"def cross_entropy_loss(x, labels):\n  # x is the model predictions we'd like to evaluate using lables\n  x_of_labels = torch.zeros(len(labels))\n  ####################################################################\n  # Fill in missing code below (...),\n  # then remove or comment the line below to test your function\n  # raise NotImplementedError(\"Cross Entropy Loss\")\n  ####################################################################\n  # 1. prediction for each class corresponding to the label\n  for idx, label in enumerate(labels):\n    x_of_labels[idx] = x[idx, label]\n  # 2. loss vector for the batch\n  losses = -x_of_labels + torch.log(torch.sum(torch.exp(x), axis=1))\n  # 3. Return the average of the loss vector\n  avg_loss = torch.mean(losses)\n\n  return avg_loss\n\n# add event to airtable\natform.add_event('Coding Exercise 2.1: Implement Batch Cross Entropy Loss')\n\n\nlabels = torch.tensor([0, 1])\nx = torch.tensor([[10.0, 1.0, -1.0, -20.0],  # correctly classified\n                  [10.0, 10.0, 2.0, -10.0]])  # Not correctly classified\nCE = nn.CrossEntropyLoss()\npytorch_loss = CE(x, labels).item()\n## Uncomment below to test your function\nour_loss = cross_entropy_loss(x, labels).item()\nprint(f'Our CE loss: {our_loss:0.8f}, Pytorch CE loss: {pytorch_loss:0.8f}')\nprint(f'Difference: {np.abs(our_loss - pytorch_loss):0.8f}')","b1923f18":"def create_spiral_dataset(K, sigma, N):\n\n  # Initialize t, X, y\n  t = torch.linspace(0, 1, N)\n  X = torch.zeros(K*N, 2)\n  y = torch.zeros(K*N)\n\n  # Create data\n  for k in range(K):\n    X[k*N:(k+1)*N, 0] = t*(torch.sin(2*np.pi\/K*(2*t+k)) + sigma*torch.randn(N))\n    X[k*N:(k+1)*N, 1] = t*(torch.cos(2*np.pi\/K*(2*t+k)) + sigma*torch.randn(N))\n    y[k*N:(k+1)*N] = k\n\n  return X, y\n\n\n# Set parameters\nK = 4\nsigma = 0.16\nN = 1000\n\nset_seed(seed=SEED)\nX, y = create_spiral_dataset(K, sigma, N)\nplt.scatter(X[:, 0], X[:, 1], c = y)\nplt.show()","483209c3":"# @title Video 4: Training and Evaluating an MLP\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1QV411p7mF\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"DfXZhRfBEqQ\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 4: Training and Evaluating an MLP')\n\ndisplay(out)","19e5c936":"def shuffle_and_split_data(X, y, seed):\n  # set seed for reproducibility\n  torch.manual_seed(seed)\n  # Number of samples\n  N = X.shape[0]\n  ####################################################################\n  # Fill in missing code below (...),\n  # then remove or comment the line below to test your function\n  # raise NotImplementedError(\"Shuffle & split data\")\n  ####################################################################\n  # Shuffle data\n  shuffled_indices = torch.randperm(N)   # get indices to shuffle data, could use torch.randperm\n  X = X[shuffled_indices]\n  y = y[shuffled_indices]\n\n  # Split data into train\/test\n  test_size = int(N*.2)   # assign test datset size using 20% of samples\n  X_test = X[:test_size]\n  y_test = y[:test_size]\n  X_train = X[test_size:]\n  y_train = y[test_size:]\n\n  return X_test, y_test, X_train, y_train\n\n# add event to airtable\natform.add_event('Coding Exercise 2.3: Implement for a classfication task')\n\n\n## Uncomment below to test your function\nX_test, y_test, X_train, y_train = shuffle_and_split_data(X, y, seed=SEED)\nplt.scatter(X_test[:, 0], X_test[:, 1], c=y_test)\nplt.title('Test data')\nplt.show()","7836778b":"g_seed = torch.Generator()\ng_seed.manual_seed(SEED)\n\nbatch_size = 128\ntest_data = TensorDataset(X_test, y_test)\ntest_loader = DataLoader(test_data, batch_size=batch_size,\n                         shuffle=False, num_workers=2,\n                         worker_init_fn=seed_worker,\n                         generator=g_seed)\n\ntrain_data = TensorDataset(X_train, y_train)\ntrain_loader = DataLoader(train_data, batch_size=batch_size, drop_last=True,\n                        shuffle=True, num_workers=2,\n                         worker_init_fn=seed_worker,\n                         generator=g_seed)","ac21a882":"def train_test_classification(net, criterion, optimizer, train_loader,\n                              test_loader, num_epochs=1, verbose=True,\n                              training_plot=False, device='cpu'):\n\n  net.train() # signifies when training starts \n  training_losses = []\n  for epoch in tqdm(range(num_epochs)):  # loop over the dataset multiple times\n    running_loss = 0.0\n    for i, data in enumerate(train_loader, 0):\n      # get the inputs; data is a list of [inputs, labels]\n      inputs, labels = data\n      inputs = inputs.to(device).float()\n      labels = labels.to(device).long()\n\n      # zero the parameter gradients\n      optimizer.zero_grad()\n\n      # forward + backward + optimize\n      outputs = net(inputs)\n\n      loss = criterion(outputs, labels)\n      loss.backward()\n      optimizer.step()\n\n      # print statistics\n      if verbose:\n        training_losses += [loss.item()]\n\n  net.eval() # signifies when evaluations\n  def test(data_loader):\n    correct = 0\n    total = 0\n    for data in data_loader:\n      inputs, labels = data\n      inputs = inputs.to(device).float()\n      labels = labels.to(device).long()\n\n      outputs = net(inputs)\n      _, predicted = torch.max(outputs, 1)\n      total += labels.size(0)\n      correct += (predicted == labels).sum().item()\n\n    acc = 100 * correct \/ total\n    return total, acc\n\n  train_total, train_acc = test(train_loader)\n  test_total, test_acc = test(test_loader)\n\n  if verbose:\n    print(f\"Accuracy on the {train_total} training samples: {train_acc:0.2f}\")\n    print(f\"Accuracy on the {test_total} testing samples: {test_acc:0.2f}\")\n\n  if training_plot:\n    plt.plot(training_losses)\n    plt.xlabel('Batch')\n    plt.ylabel('Training loss')\n    plt.show()\n\n  return train_acc, test_acc","d978cf6b":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q1', text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","16f1ca15":"set_seed(SEED)\nnet = Net('ReLU()', X_train.shape[1], [128], K).to(DEVICE)\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(net.parameters(), lr=1e-3)\nnum_epochs = 100\n\n_, _ = train_test_classification(net, criterion, optimizer, train_loader,\n                                 test_loader, num_epochs=num_epochs,\n                                 training_plot=True, device=DEVICE)","7f62e133":"def sample_grid(M=500, x_max=2.0):\n  ii, jj = torch.meshgrid(torch.linspace(-x_max, x_max, M),\n                          torch.linspace(-x_max, x_max, M))\n  X_all = torch.cat([ii.unsqueeze(-1),\n                     jj.unsqueeze(-1)],\n                     dim=-1).view(-1, 2)\n  return X_all\n\n\ndef plot_decision_map(X_all, y_pred, X_test, y_test,\n                      M=500, x_max=2.0, eps=1e-3):\n  decision_map = torch.argmax(y_pred, dim=1)\n\n  for i in range(len(X_test)):\n    indices = (X_all[:, 0] - X_test[i, 0])**2 + (X_all[:, 1] - X_test[i, 1])**2 < eps\n    decision_map[indices] = (K + y_test[i]).long()\n\n  decision_map = decision_map.view(M, M)\n  plt.imshow(decision_map, extent=[-x_max, x_max, -x_max, x_max], cmap='jet')\n  plt.show()","b8f650c7":"X_all = sample_grid()\ny_pred = net(X_all)\nplot_decision_map(X_all, y_pred, X_test, y_test)","eade28bc":"# @title Student Response\nfrom ipywidgets import widgets\n\n\ntext=widgets.Textarea(\n   value='Type your answer here and click on `Submit!`',\n   placeholder='Type something',\n   description='',\n   disabled=False\n)\n\nbutton = widgets.Button(description=\"Submit!\")\n\ndisplay(text,button)\n\ndef on_button_clicked(b):\n   atform.add_answer('q2' , text.value)\n   print(\"Submission successful!\")\n\n\nbutton.on_click(on_button_clicked)","057af610":"# @title Airtable Submission Link\nfrom IPython import display as IPydisplay\nIPydisplay.HTML(\n   f\"\"\"\n <div>\n   <a href= \"{atform.url()}\" target=\"_blank\">\n   <img src=\"https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/static\/AirtableSubmissionButton.png?raw=1\"\n alt=\"button link to Airtable\" style=\"width:410px\"><\/a>\n   <\/div>\"\"\" )","b69e3a46":"# @title Video 5: Biological to Artificial Neurons\nfrom ipywidgets import widgets\n\nout2 = widgets.Output()\nwith out2:\n  from IPython.display import IFrame\n  class BiliVideo(IFrame):\n    def __init__(self, id, page=1, width=400, height=300, **kwargs):\n      self.id=id\n      src = \"https:\/\/player.bilibili.com\/player.html?bvid={0}&page={1}\".format(id, page)\n      super(BiliVideo, self).__init__(src, width, height, **kwargs)\n\n  video = BiliVideo(id=f\"BV1mf4y157vf\", width=730, height=410, fs=1)\n  print(\"Video available at https:\/\/www.bilibili.com\/video\/{0}\".format(video.id))\n  display(video)\n\nout1 = widgets.Output()\nwith out1:\n  from IPython.display import YouTubeVideo\n  video = YouTubeVideo(id=f\"ELAbflymSLo\", width=730, height=410, fs=1, rel=0)\n  print(\"Video available at https:\/\/youtube.com\/watch?v=\" + video.id)\n  display(video)\n\nout = widgets.Tab([out1, out2])\nout.set_title(0, 'Youtube')\nout.set_title(1, 'Bilibili')\n\n# add event to airtable\natform.add_event('Video 5: Biological to Artificial Neurons')\n\ndisplay(out)","08c492a9":"def run_LIF(I, T=50, dt=0.1, t_ref=10,\n            Rm=1, Cm=10, Vth=1, V_spike=0.5):\n  \"\"\"\n  Simulate the LIF dynamics with external input current\n\n  Args:\n    I          : input current (mA)\n    T          : total time to simulate (msec)\n    dt         : simulation time step (msec)\n    t_ref      : refractory period (msec)\n    Rm         : resistance (kOhm)\n    Cm         : capacitance (uF)\n    Vth        : spike threshold (V)\n    V_spike    : spike delta (V)\n\n  Returns:\n    time       : time points\n    Vm         : membrane potentials\n  \"\"\"\n\n  # Set up array of time steps\n  time = torch.arange(0, T+dt, dt)\n\n  # Set up array for tracking Vm\n  Vm = torch.zeros(len(time))\n\n  # Iterate over each time step\n  t_rest = 0\n  for i, t in enumerate(time):\n\n    # If t is after refractory period\n    if t > t_rest:\n      Vm[i] = Vm[i-1] + 1\/Cm*(-Vm[i-1]\/Rm + I)  * dt\n\n    # If Vm is over the threshold\n    if Vm[i] >= Vth:\n\n      # Increase volatage by change due to spike\n      Vm[i] += V_spike\n\n      # Set up new refactory period\n      t_rest = t + t_ref\n\n  return time, Vm\n\n\nsim_time, Vm = run_LIF(1.5)\n# Plot the membrane voltage across time\nplt.plot(sim_time, Vm)\nplt.title('LIF Neuron Output')\nplt.ylabel('Membrane Potential (V)')\nplt.xlabel('Time (msec)')\nplt.show()","6d7d3e95":"# @title\n\n# @markdown Make sure you execute this cell to enable the widget!\nmy_layout = widgets.Layout()\n\n@widgets.interact(Rm=widgets.FloatSlider(1., min=1, max=100.,\n                                         step=0.1, layout=my_layout),\n                  t_ref=widgets.FloatSlider(1., min=1, max=100.,\n                                            step=0.1, layout=my_layout)\n                  )\n\n\ndef plot_IF_curve(Rm, t_ref):\n  T = 1000 # total time to simulate (msec)\n  dt = 1 # simulation time step (msec)\n  Vth = 1 # spike threshold (V)\n  Is_max = 2\n  Is = torch.linspace(0, Is_max, 10)\n  spike_counts = []\n  for I in Is:\n    _, Vm = run_LIF(I, T=T, dt=dt, Vth=Vth, Rm=Rm, t_ref=t_ref)\n    spike_counts += [torch.sum(Vm > Vth)]\n\n  plt.plot(Is, spike_counts)\n  plt.title('LIF Neuron: Transfer Function')\n  plt.ylabel('Spike count')\n  plt.xlabel('I (mA)')\n  plt.xlim(0, Is_max)\n  plt.ylim(0, 80)\n  plt.show()","b9312101":"---\n# Section 2: MLPs in Pytorch\n\n*Time estimate: ~1hr and 20 mins*","d12d7861":"## Section 2.2: Spiral classification dataset\nBefore we could start optimizing these loss functions, we need a dataset!\n\nLet's turn this fancy-looking equation into a classification dataset","05f82bcf":"The main loss function we could use out of the box for multi-class classification for `N` samples and `C` number of classes is:\n* CrossEntropyLoss:\nThis criterion expects a batch of predictions `x` with shape `(N, C)` and class index in the range $[0, C-1]$ as the target (label) for each `N` samples, hence a batch of `labels` with shape `(N, )`. There are other optional parameters like class weights and class ignores. Feel free to check the PyTorch documentation [here](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html) for more detail. Additionally, [here](https:\/\/sparrow.dev\/cross-entropy-loss-in-pytorch\/) you can learn where is appropriate to use the CrossEntropyLoss.\n\nTo get CrossEntropyLoss of a sample $i$, we could first calculate $-\\log(\\text{softmax(x}))$ and then take the element corresponding to $\\text { labels }_i$ as the loss. However, due to numerical stability, we implement this more stable equivalent form,\n\n\\begin{equation}\n\\operatorname{loss}(x_i, \\text { labels }_i)=-\\log \\left(\\frac{\\exp (x[\\text { labels }_i])}{\\sum_{j} \\exp (x[j])}\\right)=-x_i[\\text { labels }_i]+\\log \\left(\\sum_{j=1}^C \\exp (x_i[j])\\right)\n\\end{equation}","85034cca":" These are the slides for the videos in all tutorials today\n","3af43081":"---\n# Section 0: Introduction to MLPs","b493d17e":"```\nOur CE loss: 0.34672737, Pytorch CE loss: 0.34672749\nDifference: 0.00000012\n```","ada2d5d5":"## Section 2.1: Classification with MLPs","7d5c9c39":"##  Set device (GPU or CPU). Execute `set_device()`\n","e6c15076":"####  Student Response\n","d6fa8e03":"And we need to make a Pytorch data loader out of it. Data loading in PyTorch can be separated in 2 parts:\n* Data must be wrapped on a Dataset parent class where the methods __getitem__ and __len__ must be overrided. Note that at this point the data is not loaded on memory. PyTorch will only load what is needed to the memory. Here `TensorDataset` does this for us directly.\n* Use a Dataloader that will actually read the data in batches and put into memory. Also, the option of `num_workers > 0` allows multithreading, which prepares multiple batches in the queue to speed things up.","03826802":"As you see in the top panel, we obtain 10 shifted ReLUs with the same slope. These are the basis functions that MLP uses to span the functional space, i.e., MLP finds a linear combination of these ReLUs.","2c493fbb":"##  Video 0: Introduction\n","adc7fa26":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_51988471.py)\n\n","c522d2be":"##  Video 2: Building MLPs in PyTorch\n","5fef5954":"##  Figure settings\n","be71592f":"## Coding Exercise 2: Implement a general-purpose MLP in Pytorch\nThe objective is to design an MLP with these properties:\n* works with any input (1D, 2D, etc.)\n* construct any number of given hidden layers using `nn.Sequential()` and `add_module()` function\n* use the same given activation function (i.e., [Leaky ReLU](https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.LeakyReLU.html)) in all hidden layers.\n\n**Leaky ReLU** is described by the following mathematical formula:\n\n\\begin{equation}\n\\text{LeakyReLU}(x) = \\text{max}(0,x) + \\text{negative_slope} \\cdot \\text{min}(0, x) =\n\\left\\{\n  \\begin{array}{ll}\n  x & ,\\; \\text{if} \\; x \\ge 0 \\\\\n  \\text{negative_slope} \\cdot x & ,\\;  \\text{otherwise}\n  \\end{array}\n\\right.\n\\end{equation}","75635b78":" Executing `set_seed(seed=seed)` you are setting the seed\n","b7561afb":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_b0675c60.py)\n\n","5e1c41d8":"---\n# Section 1: The Need for MLPs\n\n*Time estimate: ~35 mins*","0f1d9649":"##  Airtable Submission Link\n","bda2c870":"```\nThe output shape is torch.Size([100, 1]) for an input of shape torch.Size([100, 2])\n```","9451c206":"# Tutorial 1: Biological vs. Artificial Neural Networks\n\n**Week 1, Day 3: Multi Layer Perceptrons**\n\n**By Neuromatch Academy**\n\n__Content creators:__ Arash Ash, Surya Ganguli\n\n__Content reviewers:__ Saeed Salehi, Felix Bartsch, Yu-Fang Yang, Antoine De Comite, Melvin Selim Atay, Kelson Shilling-Scrivo\n\n__Content editors:__ Gagana B, Kelson Shilling-Scrivo, Spiros Chavlis\n\n__Production editors:__ Anoop Kulkarni, Kelson Shilling-Scrivo, Spiros Chavlis","dd190c31":"**Our 2021 Sponsors, including Presenting Sponsor Facebook Reality Labs**\n\n<p align='center'><img src='https:\/\/github.com\/NeuromatchAcademy\/widgets\/blob\/master\/sponsors.png?raw=True'\/><\/p>","6020b5cd":"#### \n","59ef12b3":"### Coding Exercise 2.1: Implement Batch Cross Entropy Loss\n\nTo recap, since we will be doing batch learning, we'd like a loss function that given:\n* a batch of predictions `x` with shape `(N, C)` \n* a batch of `labels` with shape `(N, )` that ranges from `0` to `C-1`\n\nreturns the average loss $L$ calculated according to:\n\n\\begin{align}\nloss(x_i, \\text { labels }_i) &= -x_i[\\text { labels }_i]+\\log \\left(\\sum_{j=1}^C \\exp (x_i[j])\\right) \\\\\nL &= \\frac{1}{N} \\sum_{i=1}^{N}{loss(x_i, \\text { labels }_i)}\n\\end{align}\n\nSteps:\n\n1.   Use indexing operation to get predictions of class corresponding to the labels (i.e., $x_i[\\text { labels }_i]$)\n2.   Compute $loss(x_i, \\text { labels }_i)$ vector (`losses`) using `torch.log()` and `torch.exp()` without Loops!\n3. Return the average of the loss vector\n\n","e9b140a6":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_e09a57e7.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=1120.0 height=832.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W1D3_MultiLayerPerceptrons\/static\/W1D3_Tutorial1_Solution_e09a57e7_0.png>\n\n","d1de0fc4":"---\n# Summary\n\nIn this tutorial we have explored the Multi-leayer Perceptrons (MLPs). More specifically, we have discuss the similarities of artificial and biological neural networks (for more information see the Bonus section), we have learned the Universal Approximation Theorem, and we have implemented MLPs in PyTorch.","8ddcec71":"###  Video 3: Cross Entropy\n","68a6340d":"##  Video 1: Universal Approximation Theorem\n","9c4a6efd":"What would be your suggestions to increase models ability to generalize? Think about it and discuss with your pod.","610eff20":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_083a4d77.py)\n\n*Example output:*\n\n<img alt='Solution hint' align='left' width=1120.0 height=832.0 src=https:\/\/raw.githubusercontent.com\/NeuromatchAcademy\/course-content-dl\/main\/tutorials\/W1D3_MultiLayerPerceptrons\/static\/W1D3_Tutorial1_Solution_083a4d77_0.png>\n\n","44aef274":"####  Student Response\n","47b7cae2":"##  Install dependencies\n","f8dfc04a":"<a href=\"https:\/\/colab.research.google.com\/github\/NeuromatchAcademy\/course-content-dl\/blob\/main\/tutorials\/W1D3_MultiLayerPerceptrons\/student\/W1D3_Tutorial1.ipynb\" target=\"_blank\"><img alt=\"Open In Colab\" src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\"\/><\/a>","ce3f96a2":"And finally, let's visualize the learned decision-map. We know you're probably running out of time, so we won't make you write code now! But make sure you have reviewed it since we'll start with another visualization technique next time.","ad6edc3a":"### Think! 2.3.2: Does it generalize well?\nDo you think this model is performing well outside its training distribution? Why?","20936676":"## Simulating an LIF Neuron\n\nIn the cell below is given a function for LIF neuron model with it's arguments described.\n\nNote that we will use Euler's method to make a numerical approximation to a derivative. Hence we will use the following implementation of the model dynamics,\n\n\\begin{equation}\nV_n=\\left\\{\\begin{array}{cc}\nV_{n-1} + \\frac{1}{C}\\left(-\\frac{V}{R}+I \\right) \\Delta t & t>t_{r e s t} \\\\\n0 & \\text { otherwise }\n\\end{array}\\right.\n\\end{equation}","4e4540ea":"\\begin{equation}\n\\begin{array}{c}\nX_{k}(t)=t\\left(\\begin{array}{c}\n\\sin \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right) \\\\\n\\cos \\left[\\frac{2 \\pi}{K}\\left(2 t+k-1\\right)\\right]+\\mathcal{N}\\left(0, \\sigma\\right) \n\\end{array}\\right)\n\\end{array}, \\quad 0 \\leq t \\leq 1, \\quad k=1, \\ldots, K\n\\end{equation}","97cd1d4a":"###  Video 4: Training and Evaluating an MLP\n","b3a0b543":"## Section 2.3: Training and Evaluation","2a012416":"---\n# Bonus: Neuron Physiology and Motivation to Deep Learning\n\nThis section will motivate one of the most popular nonlinearities in deep learning, the ReLU nonlinearity, by starting from the biophysics of neurons and obtaining the ReLU nonlinearity through a sequence of approximations. We will also show that neuronal biophysics sets a time scale for signal propagation speed through the brain. This time scale implies that neural circuits underlying fast perceptual and motor processing in the brain may not be excessively deep. \n\nThis biological motivation for deep learning is not strictly necessary to follow the rest of this course, so this section can be safely skipped and **is a bonus section**.","6f744a73":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_d90d9e4b.py)\n\n","8afa032c":"##  Plotting functions\n","e1a5e0ef":"---\n# Setup\n\nThis is a GPU free notebook!","dea79451":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_f3376d8f.py)\n\n","3990b66f":"In the previous segment, we implemented a function to approximate any smooth function using MLPs. We saw that using Lipschitz continuity; we can prove that our approximation is mathematically correct. MLPs are fascinating, but before we get into the details on designing them, let's familiarize ourselves with some basic terminology of MLPs- layer, neuron, depth, width, weight, bias, and activation function. Armed with these ideas, we can now design an MLP given its input, hidden layers, and output size.","d29917a1":"##  Video 5: Biological to Artificial Neurons\n","bd9df94e":"Let's write a general-purpose training and evaluation code and keep it in our pocket for next tutorial as well. So make sure you review it to see what it does.\n\nNote that `model.train()` tells your model that you are training the model. So layers like dropout, batchnorm etc. which behave different on the train and test procedures know what is going on and hence can behave accordingly. And to turn off training mode we set `model.eval()`","57806baf":"### Think! 2.3.1: What's the point of .eval() and .train()?\n\nIs it necessary to use `net.train()` and `net.eval()` for our MLP model? why?","1346dd52":"##  Set random seed\n","1f368ec3":"### Coding Exercise 2.3: Implement it for a classfication task\nNow that we have the Spiral dataset and a loss function, it's your turn to implement a simple train\/test split for training and validation.\n\nSteps to follow: \n  * Dataset shuffle\n  * Train\/Test split (20% for test)\n  * Dataloader definition\n  * Training and Evaluation","e64dee6e":"## Coding Exercise 1: Function approximation with ReLU\nWe learned that one hidden layer MLPs are enough to approximate any smooth function! Now let's manually fit a sine function using ReLU activation. \n\nWe will approximate the sine function using a linear combination (a weighted sum) of ReLUs with slope 1. We need to determine the bias terms (which determines where the ReLU inflection point from 0 to linear occurs) and how to weight each ReLU. The idea is to set the weights iteratively so that the slope changes in the new sample's direction.\n\nFirst, we generate our \"training data\" from a sine function using `torch.sine` function.\n\n```python\n>>> import torch\n>>> torch.manual_seed(2021)\n<torch._C.Generator object at 0x7f8734c83830>\n>>> a = torch.randn(5)\n>>> print(a)\ntensor([ 2.2871,  0.6413, -0.8615, -0.3649, -0.6931])\n>>> torch.sin(a)\ntensor([ 0.7542,  0.5983, -0.7588, -0.3569, -0.6389])\n```\n\nThese are the points we will use to learn how to approximate the function. We have 10 training data points so we will have 9 ReLUs (we don't need a ReLU for the last data point as we don't have anything to the right of it to model). \n\nWe first need to figure out the bias term for each ReLU and compute the activation of each ReLU where:\n\n\\begin{equation}\ny(x) = \\text{max}(0,x+b)\n\\end{equation}\n\nWe then need to figure out the correct weights on each ReLU so the linear combination approximates the desired function.","49334a92":"### Think!: Real and Artificial neuron similarities\n\nWhat happens at infinite membrane resistance ($R_m$) and small refactory time ($t_{ref}$)? Why?\n\nTake 10 mins to discuss the similarity between a real neuron and an artificial one with your pod.","50e73df1":"### Interactive Demo: Neuron's transfer function explorer for different $R_m$ and $t_{ref}$\nWe know that real neurons communicate by modulating the spike count meaning that more input current causes a neuron to spike more often. Therefore, to find an input-output relationship, it makes sense to characterize their spike count as a function of input current. This is called the neuron's input-output transfer function. Let's plot the neuron's transfer function and see how it changes with respect to the **membrane resistance** and **refractory time**? ","894805c4":"Now let's put everything together and train your first deep-ish model!","727643ec":"[*Click for solution*](https:\/\/github.com\/NeuromatchAcademy\/course-content-dl\/tree\/main\/\/tutorials\/W1D3_MultiLayerPerceptrons\/solutions\/W1D3_Tutorial1_Solution_d58d2933.py)\n\n","ec8f259e":"##  Tutorial slides\n","a134fdc4":" Make sure you execute this cell to enable the widget!\n","34385347":"## Leaky Integrate-and-fire (LIF)\nThe basic idea of LIF neuron was proposed in 1907 by Louis \u00c9douard Lapicque, long before we understood the electrophysiology of a neuron (see a translation of [Lapicque's paper](https:\/\/pubmed.ncbi.nlm.nih.gov\/17968583\/) ). More details of the model can be found in the book [**Theoretical neuroscience**](http:\/\/www.gatsby.ucl.ac.uk\/~dayan\/book\/) by Peter Dayan and Laurence F. Abbott.\n\nThe model dynamics is defined with the following formula,\n\n\n\\begin{equation}\n\\frac{d V}{d t}=\\left\\{\\begin{array}{cc}\n\\frac{1}{C}\\left(-\\frac{V}{R}+I \\right) & t>t_{r e s t} \\\\\n0 & \\text { otherwise }\n\\end{array}\\right.\n\\end{equation}\n\n\nNote that $V$, $C$, and $R$ are the membrane voltage, capacitance, and resitance of the neuron respectively and $-\\frac{V}{R}$ is the leakage current. When $I$ is sufficiently strong such that $V$ reaches a certain threshold value $V_{\\rm th}$, it momentarily spikes and then $V$ is reset to $V_{\\rm reset}< V_{\\rm th}$, and voltage stays at $V_{\\rm reset}$ for $\\tau_{\\rm ref}$ ms, mimicking the refractoriness of the neuron during an action potential (note that $V_{\\rm reset}$ and $\\tau_{\\rm ref}$ is assumed to be zero in the lecture):\n\n\n\\begin{eqnarray}\nV(t)=V_{\\rm reset} \\text{  for } t\\in(t_{\\text{sp}}, t_{\\text{sp}} + \\tau_{\\text{ref}}]\n\\end{eqnarray}\n\n\nwhere $t_{\\rm sp}$ is the spike time when $V(t)$ just exceeded $V_{\\rm th}$.\n\nThus, the LIF model captures the facts that a neuron:\n- performs spatial and temporal integration of synaptic inputs \n- generates a spike when the voltage reaches a certain threshold\n- goes refractory during the action potential\n- has a leaky membrane \n\nFor in-depth content on computational models of neurons, follow the [NMA](https:\/\/www.neuromatchacademy.org\/) tutorial 1 of *Biological Neuron Models*. Specifically, for NMA-CN 2021 follow this [Tutorial](https:\/\/github.com\/NeuromatchAcademy\/course-content\/blob\/master\/tutorials\/W2D3_BiologicalNeuronModels\/W2D3_Tutorial1.ipynb).\n","97663704":"---\n# Tutorial objectives\n\nIn this tutorial, we will explore the Multi-layer Perceptrons (MLPs). MLPs are arguably one of the most tractable models (due to their flexibility) that we can use to study deep learning fundamentals. Here we will learn why they are: \n\n* similar to biological networks\n* good at function approximation\n* implemented the way they are in PyTorch"}}