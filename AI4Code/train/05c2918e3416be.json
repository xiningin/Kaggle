{"cell_type":{"f2be912f":"code","42762be0":"code","6f21e2e5":"code","1087868f":"code","a1c49c54":"code","a554207b":"code","0249d0bf":"code","b3e56de9":"code","1eff8a1d":"code","84880a98":"code","258fb87d":"code","ae6561fa":"code","c12d18e2":"code","d32fd32d":"code","f0f5f56b":"code","cf8bd1e5":"code","9be687f2":"code","78e83177":"code","211be1eb":"code","3539beda":"code","335ac8d6":"code","c9cdae47":"code","a20590c0":"code","57f4e299":"code","a3cacf03":"code","f8589831":"code","4ae5c93e":"code","e8cbb8ab":"code","9b36bb82":"code","d1d6afc7":"code","d238db40":"code","45541c1e":"code","40b117ac":"code","0fb712a9":"code","bc8f1e67":"code","9d78aac3":"code","32b0988e":"code","b4f694a5":"markdown","d3f25432":"markdown","2c6e37ca":"markdown","e52160a7":"markdown","401c15e5":"markdown","f7075409":"markdown","a2dea8a2":"markdown","78ea4744":"markdown","eef76384":"markdown","13e8ad35":"markdown","3a8e135a":"markdown","cc9f1623":"markdown","a5168bf9":"markdown","b784905b":"markdown","83c71c4b":"markdown","aba1cd9a":"markdown","8370deba":"markdown","b79fb363":"markdown","0cba177a":"markdown","5cc5ace7":"markdown","1e29360d":"markdown","2243f85a":"markdown","756bb861":"markdown","3c58aea7":"markdown","3df9b720":"markdown","60653ff8":"markdown","92e528f6":"markdown","275629f6":"markdown","08483b63":"markdown","44d12198":"markdown","5974750e":"markdown","d858f61c":"markdown","cba5f10b":"markdown","29fe1609":"markdown","e8b41d5c":"markdown"},"source":{"f2be912f":"import warnings\nwarnings.filterwarnings('ignore')\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.offline as plty\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n\nimport pywt\n\n%matplotlib inline","42762be0":"INPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncalendar_df = pd.read_csv(INPUT_DIR+'\/calendar.csv')\nsell_prices_df = pd.read_csv(INPUT_DIR+'\/sell_prices.csv')\nsales_train_validation_df = pd.read_csv(INPUT_DIR+'\/sales_train_validation.csv')","6f21e2e5":"d_columns = sales_train_validation_df.columns[6:]\n\nsample_1 = sales_train_validation_df.iloc[1][d_columns]\nsample_2 = sales_train_validation_df.iloc[11][d_columns]\nsample_3 = sales_train_validation_df.iloc[21][d_columns]\n","1087868f":"fig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(sample_1)),\n                        y=sample_1,showlegend=False, mode='lines',\n                        name='First Sample',marker_color='#0C629F'), \n              row=1, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(sample_2)),\n                        y=sample_2,showlegend=False, mode='lines',\n                        name='Second Sample',marker_color='#F36F62'), \n              row=2, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(sample_3)),\n                        y=sample_3,showlegend=False, mode='lines',\n                        name='Third Sample',marker_color='#68D9D6'), \n              row=3, col=1)\n\nfig.update_layout(height=1200, width=1000, title_text=\"Sample sales\", template='plotly_white')\n\nfig.show()","a1c49c54":"fig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(len(sample_1[200:300])),\n                        y=sample_1[200:300],showlegend=False, mode='lines+markers',\n                        name='First Sample',marker_color='#64B8F3'), \n              row=1, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(sample_2[200:300])),\n                        y=sample_2[200:300],showlegend=False, mode='lines+markers',\n                        name='Second Sample',marker_color='#F36F62'), \n              row=2, col=1)\nfig.add_trace(go.Scatter(x=np.arange(len(sample_3[200:300])),\n                        y=sample_3[200:300],showlegend=False, mode='lines',\n                        name='Third Sample',marker_color='#68D9D6'), \n              row=3, col=1)\n\nfig.update_layout(height=900, width=900, title_text=\"Sample sales\", template='plotly_white')\n\nfig.show()","a554207b":"def maddest(d, axis=None):\n    return np.mean(np.absolute(d - np.mean(d, axis)), axis)\n\ndef denoise_signal(x, wavelet='db4', level=1):\n    coeff = pywt.wavedec(x, wavelet, mode=\"per\")\n    sigma = (1\/0.6745) * maddest(coeff[-level])\n\n    uthresh = sigma * np.sqrt(2*np.log(len(x)))\n    coeff[1:] = (pywt.threshold(i, value=uthresh, mode='hard') for i in coeff[1:])\n\n    return pywt.waverec(coeff, wavelet, mode='per')","0249d0bf":"denoised_1 = denoise_signal(sample_1[200:300])\ndenoised_2 = denoise_signal(sample_2[200:300])\ndenoised_3 = denoise_signal(sample_3[200:300])\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_1[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#64B8F3'), \n              row=1, col=1)\nfig.add_trace(go.Scatter(x=np.arange(200),\n                        y=denoised_1,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#0C629F'), \n              row=1, col=1)\n\n\n\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_2[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#F2B963'), \n              row=2, col=1)\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=denoised_2,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#F36F62'), \n              row=2, col=1)\n\n\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_3[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#A9E9E7'), \n              row=3, col=1)\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=denoised_3,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#20817D'), \n              row=3, col=1)\n\nfig.update_layout(height=900, width=900, title_text=\"Original Signal vs Denoised\", template='plotly_white')\n\nfig.show()","b3e56de9":"def average_smoothing(signal, kernel_size=3, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample)","1eff8a1d":"denoised_1 = average_smoothing(sample_1[200:300])\ndenoised_2 = average_smoothing(sample_2[200:300])\ndenoised_3 = average_smoothing(sample_3[200:300])\n\nfig = make_subplots(rows=3, cols=1)\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_1[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#64B8F3'), \n              row=1, col=1)\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=denoised_1,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#0C629F'), \n              row=1, col=1)\n\n\n\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_2[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#F2B963'), \n              row=2, col=1)\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=denoised_2,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#F36F62'), \n              row=2, col=1)\n\n\n\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=sample_3[200:300],showlegend=False, mode='lines',\n                        name='Original',marker_color='#A9E9E7'), \n              row=3, col=1)\nfig.add_trace(go.Scatter(x=np.arange(100),\n                        y=denoised_3,showlegend=False, mode='lines',\n                        name='Denoised',marker_color='#20817D'), \n              row=3, col=1)\n\nfig.update_layout(height=900, width=900, title_text=\"Original Signal vs Denoised\", template='plotly_white')\n\nfig.show()","84880a98":"store_ids = set(sales_train_validation_df['store_id'])\nsales_df = sales_train_validation_df.set_index('id')[d_columns] \\\n    .T \\\n    .merge(calendar_df.set_index('d')['date'],\n           left_index=True,\n           right_index=True,\n            validate='1:1') \\\n    .set_index('date')\n\nsales_df.index = pd.to_datetime(sales_df.index)","258fb87d":"fig = go.Figure()\nfor store in store_ids:\n    store_columns = [c for c in sales_df.columns if store in c]\n    df = sales_df[store_columns].sum(axis=1)\n    df.columns = ['date','sum']\n    df = df.groupby(pd.Grouper(freq=\"M\")).mean()\n    fig.add_trace(go.Scatter(x=df.index, y=df, name=store))\n\nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Monthy Average Sales (per store)\", template='plotly_white',\n                     xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(count=12, label='12m', step='month', stepmode='backward'),\n                dict(count=18, label='18m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date'\n    ))","ae6561fa":"fig = go.Figure()\nfor store in store_ids:\n    store_columns = [c for c in sales_df.columns if store[:2] in c]\n    df = sales_df[store_columns].sum(axis=1)\n    df.columns = ['date','sum']\n    df = df.groupby(pd.Grouper(freq=\"M\")).mean()\n    fig.add_trace(go.Scatter(x=df.index, y=df, name=store[:2]))\n\nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Monthy Average Sales CA vs TX vs WI\", template='plotly_white',\n                     xaxis=dict(\n        rangeselector=dict(\n            buttons=list([\n                dict(count=6, label='6m', step='month', stepmode='backward'),\n                dict(count=12, label='12m', step='month', stepmode='backward'),\n                dict(count=18, label='18m', step='month', stepmode='backward'),\n                dict(step='all')\n            ])\n        ),\n        rangeslider=dict(visible = True),\n        type='date'\n    ))\nfig.update_traces(showlegend=True)","c12d18e2":"fig = go.Figure()\nfor store in store_ids:\n    store_columns = [c for c in sales_df.columns if store in c]\n    data = sales_df[store_columns].sum(axis=1).rolling(90).mean()\n    fig.add_trace(go.Scatter(x=np.arange(len(data)), y=data, name=store))\n\nfig.update_layout(yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\", template='plotly_white')","d32fd32d":"fig = go.Figure()\nfor store in store_ids:\n    store_columns = [c for c in sales_df.columns if store in c]\n    data = sales_df[store_columns].sum(axis=1).rolling(90).mean()\n    fig.add_trace(go.Box(x=[store]*1913, y=data, name=store))\n\nfig.update_layout(height=500, width=800,yaxis_title=\"Sales\", xaxis_title=\"Time\", title=\"Rolling Average Sales vs. Time (per store)\", template='plotly_white')","f0f5f56b":"train_df = sales_df[-100:-30]\nval_df = sales_df[-30:]","cf8bd1e5":"fig = make_subplots(rows=3, cols=1)\n\nfor idx in range(3):\n    fig.add_trace(go.Scatter(x=train_df.index, y=train_df[train_df.columns[idx]]), row=idx+1, col=1)\n    fig.add_trace(go.Scatter(x=val_df.index, y=val_df[val_df.columns[idx]]), row=idx+1, col=1)\n\nfig.update_layout(height=900, width=900, title_text=\"Train & Validation\", template='plotly_white')","9be687f2":"from statsmodels.tsa.stattools import adfuller\n\nresult = adfuller(train_df[train_df.columns[0]])\nprint('Test Statistic : ',result[0])\nprint('P-value : ',np.round(result[1],decimals=15))","78e83177":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\n_ = plot_acf(train_df[train_df.columns[0]], lags=10, zero=False, alpha=0.05)\n_ = plot_pacf(train_df[train_df.columns[0]], lags=10, zero=False, alpha=0.05)","211be1eb":"from statsmodels.tsa.statespace.sarimax import SARIMAX\n\norder_aic_bic = []\n# Loop over AR order\nfor p in range(3):\n    #Loop over MA order\n    for q in range(3):\n        try:\n            model = SARIMAX(train_df[train_df.columns[0]],order=(p,0,q))\n            results = model.fit()\n            order_aic_bic.append((p, q, results.aic, results.bic))\n        except:\n            order_aic_bic.append((p, q, None, None))\n\norder_df = pd.DataFrame(order_aic_bic, columns=['p','q','aic','bic'])\n\norder_df","3539beda":"model = SARIMAX(train_df[train_df.columns[0]],order=(1,0,1))\nresults = model.fit()\n\nresiduals = results.resid\n# Mean Absolute Error\nmae = np.mean(np.abs(residuals))\nprint('Mean Absoulte Error : ', mae)\n\n\nresults.plot_diagnostics(figsize=(18,10))\n_ = plt.show()","335ac8d6":"results.summary()","c9cdae47":"predictions = []\n\nfor idx in range(3):\n    model = SARIMAX(train_df[train_df.columns[idx]],order=(1,0,1))\n    results = model.fit()\n    predictions.append(results.forecast(30))\n\npredictions = np.array(predictions).reshape((-1, 30))","a20590c0":"fig = make_subplots(rows=3, cols=1)\n\nfor idx in range(3):\n    fig.add_trace(go.Scatter(x=train_df.index, y=train_df[train_df.columns[idx]]), row=idx+1, col=1)\n    fig.add_trace(go.Scatter(x=val_df.index, y=val_df[val_df.columns[idx]]), row=idx+1, col=1)\n    fig.add_trace(go.Scatter(x=val_df.index, y=predictions[idx]), row=idx+1, col=1)\n    \nfig.update_layout(height=900, width=900, title_text=\"Predictions ARIMA\", template='plotly_white')\n","57f4e299":"# Identifying seasonal periods\n_ = plot_acf(train_df[train_df.columns[0]], zero=False, lags=20)","a3cacf03":"from statsmodels.tsa.seasonal import seasonal_decompose\n\nres = seasonal_decompose(train_df[train_df.columns[0]], freq=11)\n\nfig, (ax1,ax2,ax3) = plt.subplots(3,1, figsize=(15,8))\nres.trend.plot(ax=ax1)\nres.resid.plot(ax=ax2)\nres.seasonal.plot(ax=ax3)","f8589831":"!pip install -q pmdarima","4ae5c93e":"\nimport pmdarima as pm\n\n\nresults = pm.auto_arima(train_df[train_df.columns[0]],\n                           d=0, # Non-Seasonal difference order\n                           start_p=1, #Initial guess for p\n                           start_q=1, #Intial guess for q\n                           max_p=3, \n                           max_q=3,\n                           seasonal=True , #Times series is seasonal\n                           m=11, #Seasonal Period\n                           D=1, #Seasonal Difference order\n                           start_P=1, #Initial guess for P\n                           start_Q=1, #Intial guess for Q\n                           max_P=2, \n                           max_Q=2,\n                           information_criterion='aic', #To select Best model\n                           trace = True, #print results while training\n                           error_action = 'ignore', #ignore orders that don't work\n                           stepwise=True #Apply intelligent order search\n                           )\n","e8cbb8ab":"predictions = []\n\nfor idx in range(3):\n    model = SARIMAX(train_df[train_df.columns[idx]],order=(0,0,0), seasonal_order=(1,1,1,11), trend='c')\n    results = model.fit()\n    predictions.append(results.forecast(30))\n\npredictions = np.array(predictions).reshape((-1, 30))","9b36bb82":"fig = make_subplots(rows=3, cols=1)\n\nfor idx in range(3):\n    fig.add_trace(go.Scatter(x=train_df.index, y=train_df[train_df.columns[idx]]), row=idx+1, col=1)\n    fig.add_trace(go.Scatter(x=val_df.index, y=val_df[val_df.columns[idx]]), row=idx+1, col=1)\n    fig.add_trace(go.Scatter(x=val_df.index, y=predictions[idx]), row=idx+1, col=1)\n    \nfig.update_layout(height=900, width=900, title_text=\"Predictions: Seasonal ARIMA\", template='plotly_white')","d1d6afc7":"# Creating monthly average for CA\nstore_columns = [c for c in sales_df.columns if 'CA' in c]\ndf = sales_df[store_columns].sum(axis=1)\ndf.columns = ['date','sum']\ndf = df.groupby(pd.Grouper(freq=\"M\")).mean()","d238db40":"train_df = df.loc['2012-12-31':'2015-12-31']\nval_df = df.loc['2015-12-31':]","45541c1e":"fig = make_subplots(rows=1, cols=1)\n\nfig.add_trace(go.Scatter(x=train_df.index, y=train_df, name='train'), row=1, col=1)\nfig.add_trace(go.Scatter(x=val_df.index, y=val_df, name='validation'), row=1, col=1)\n\nfig.update_layout(height=500, width=700, title_text=\"Train & Validation\", template='plotly_white')","40b117ac":"_ = plot_acf(train_df, lags=30, zero=False)","0fb712a9":"train_df_diff","bc8f1e67":"import pmdarima as pm\n\n\nresults = pm.auto_arima(train_df,\n                           d=0, # Non-Seasonal difference order\n                           start_p=1, #Initial guess for p\n                           start_q=1, #Intial guess for q\n                           max_p=3, \n                           max_q=3,\n                           seasonal=True , #Times series is seasonal\n                           m=12, #Seasonal Period\n                           D=2, #Seasonal Difference order\n                           start_P=1, #Initial guess for P\n                           start_Q=1, #Intial guess for Q\n                           max_P=2, \n                           max_Q=2,\n                           information_criterion='aic', #To select Best model\n                           trace = True, #print results while training\n                           error_action = 'ignore', #ignore orders that don't work\n                           stepwise=True #Apply intelligent order search\n                           )","9d78aac3":"predictions = []\n\n\nmodel = SARIMAX(train_df,order=(1,0,1), seasonal_order=(2,2,0,12), trend='c')\nresults = model.fit()\npredictions = results.forecast(11)","32b0988e":"fig = make_subplots(rows=1, cols=1)\n\ndates = pd.date_range(start='2015-12-31', periods=12, freq='M')\nfig.add_trace(go.Scatter(x=train_df.index, y=train_df, name='train'), row=1, col=1)\nfig.add_trace(go.Scatter(x=val_df.index, y=val_df, name='validation'), row=1, col=1)\nfig.add_trace(go.Scatter(x=dates, y=predictions, name='prediction'), row=1, col=1)\n    \nfig.update_layout(height=500, width=700, title_text=\"Predictions: Seasonal ARIMA\", template='plotly_white')","b4f694a5":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observation :<\/li>\n  <\/ol>\n<\/nav>\n\nBoth ACF & PACF plots tail-off and represent a sinusoidal wave. Hence our data best represents ARMA model.","d3f25432":"<center><h3><span style=\"color:#7F5D51;\">California, Texas, and Wisconsin.<\/span><\/h3><\/center>","2c6e37ca":"<h1 style=\"font-family:verdana;\"> <center>\ud83d\udcda Introduction \ud83d\udcda<\/center> <\/h1>\n\n***\n![image.png](attachment:image.png)\n<br>\n<br>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:15px; font-family:verdana; line-height: 1.4em;\">\n    <center>\n        \ud83d\udccc In this kernel,we will forecast future sales at <span style=\"color:crimson;\">Walmart<\/span> based on heirarchical sales in the states of <span style=\"color:crimson;\">California, Texas, and Wisconsin.<span><\/center> \n\n\n<br>\n    <center>\nForecasting sales, revenue, and stock prices is a classic application of machine learning in economics, and it is important because it allows investors to make guided decisions based on forecasts made by algorithms.\n    <\/center>\n<br> \n    <center>\nIn this kernel, brief explaination of the structure of dataset will be provided. Then, we will visualize the dataset using Matplotlib and Plotly. And finally, I will demonstrate how this problem can be approached with a variety of forecasting algorithms. \n    <\/center> \n<\/div>\n\n***\n\n<blockquote>\n    <p style=\"font-size:16px; color:#159364; font-family:verdana;\">\n    If you find this kernel interesting, please give an upvote.\ud83d\ude03\n    <\/p>\n<\/blockquote>\n","e52160a7":"We could observe 11 seasonal periods from the acf function","401c15e5":"<center> <h2> Creating Time Series Sales Data <\/h2><\/center>","f7075409":"<h2>The Augmented Dicky-Fuller test<\/h2>\n<li>Tests for trend non-stationary<\/li>\n<li>Null hypothesis is time series is non-stationary<\/li>\n","a2dea8a2":"<center> <h3 style='color:#757575;'>Create samples<\/h3> <\/center>","78ea4744":"<center> <h2> Creating Train and Validation sets <\/h2><\/center>","eef76384":"Almost every sales curve has \"linear oscillation\" trend at the macroscopic level. Basically, the sales oscillate like a sine wave about a certain mean value, but this mean value has an upward linear trend. This implies that the sales are oscillating at a higher and higher level every few months.\n\nThis trend is reminiscent of the business cycle, where economies have short-term oscillatory fluctuations but grow linearly in the long run.\n\n![image.png](attachment:image.png)","13e8ad35":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observations :<\/li>\n  <\/ol>\n<\/nav>\n<p>\n\n<li>Sales from California have been the highest<\/li>\n<li>Texas and Winston have shown roughly the same amount of sales <\/li>\n<li>The rise and fall in sales have been similar for the three states in 5 years<\/li>\n    \n<\/p>","3a8e135a":"<center><h3 style='color:#5665EF'> Wavelet denoising<\/h3><\/center>\n\n<p style='color:#646464'>Wavelet denoising (usually used with electric signals) is a way to remove the unnecessary noise from a time series. This method calculates coefficients called the \"wavelet coefficients\". These coefficients decide which pieces of information to keep (signal) and which ones to discard (noise).\n    <br>We make use of the <b>MAD<\/b> (mean absolute deviation) value to understand the randomness in the sales and accordingly decide the minimum threshold for the wavelet coefficients in the time series. We filter out the low coefficients from the wavelets and reconstruct the sales data from the remaining coefficients and that's it; we have successfully removed noise from the sales data.<\/p>\n","cc9f1623":"<center><h2>Predictions<\/h2><\/center>","a5168bf9":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Conclusion :<\/li>\n  <\/ol>\n<\/nav>\n\nBoth AIC & BIC values were found to be lowest for AR order(p)=1 and MA order(q)=1","b784905b":"<center><h2>Auto-Correlation & Partial Auto-Correlation functions<\/h2><\/center>\n<p>\n     After a time series has been stationarized, the next step in fitting an ARIMA model is to determine whether AR or MA terms are needed to correct any autocorrelation that remains in the differenced series.\n<\/p>\n\n![image.png](attachment:image.png)\n","83c71c4b":"<h3>\nClick on buttons <button type=\"button\" class=\"btn btn-secondary\">6m<\/button> <button type=\"button\" class=\"btn btn-secondary\">12m<\/button> <button type=\"button\" class=\"btn btn-secondary\">18m<\/button> to get 6,12 & 18 months detailed report above\n<\/h3>\n<hr>\n<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observations :<\/li>\n  <\/ol>\n<\/nav>\n<p>\n\n<li>There is obvious depression in sales in the months of November or December.<\/li>\n<li>After fall in sales, in the coming new year months (Jan & Feb) the sales seems to have climbed up <\/li>\n<li>The peak sales for each year is observed between July and September\n    \n<\/p>","aba1cd9a":"\n<center><h3 class=\"text-muted\"> Sample Data <span class=\"badge badge-pill badge-info\" style='font-size:20px'>Zoomed<\/span><\/h3> <\/center>\n","8370deba":"### Find Seasonal Order","b79fb363":"<center><h3>Fitting ARIMA model with order(1,1)<\/h3><\/center>","0cba177a":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observations :<\/li>\n  <\/ol>\n<\/nav>\nIn the above plots, we can clearly see that the sales data is very erratic and volatile. Sometimes, the sales are zero for a few days in a row, and at other times, it remains at its peak value for a few days. Therefore, we need some sort of \"denoising\" techniques to find the underlying trends in the sales data and make forecasts\n","5cc5ace7":"<center><h3 style='color:#5665EF'>Average smoothing<\/h3><\/center>\n<p style='color:#646464'>\nAverage smoothing is a relatively simple way to denoise time series data. In this method, we take a \"window\" with a fixed size (like 10). <br>We first place the window at the beginning of the time series (first ten elements) and calculate the mean of that section. We now move the window across the time series in the forward direction by a particular \"stride\", calculate the mean of the new window and repeat the process, until we reach the end of the time series. All the mean values we calculated are then concatenated into a new time series, which forms the denoised sales data.","1e29360d":"<center><h2>Modelling on Entire one year of data of California<\/h2><\/center>","2243f85a":"### Searching over model orders to find the best one","756bb861":"<center> <h2>Load Data<\/h2> <\/center>","3c58aea7":"Prob(Q) = 0.22(>0.05) , Hence we accept the null hypothesis that residuals are not correlated.(Observed the same in the above Correlogram plot)\n<br>\nProb(JB) = 0 (<0.05) , indicates that the residuals are not normally distributed, ","3df9b720":"**We have confirmed from ACF plot that the seasonal period is of <span style=\"color:crimson;\">12 months<\/span>(number of lines between two peaks including the two)**","60653ff8":"## Forecasting next 12 months Data for California","92e528f6":"<center><h2>Using the ARIMA Model<\/h2><\/center>\nThe name ARMA is short for Autoregressive Moving Average. It comes from merging two simpler models \u2013 the Autoregressive, or AR, and the Moving Average, or MA. In analysis, we tend to put the residuals at the end of the model equation, so that\u2019s why the \u201cMA\u201d part comes second. Of course, this will become apparent once we examine the equation.\n<br><br>\n<b>What does a simple ARMA model look like?<\/b>\n<br><br>\nLet\u2019s suppose that \u201cY\u201d is some random time-series variable. Then, a simple Autoregressive Moving Average model would look something like this:\n\n<br><br>\n<center><b>yt = c + \u03d51 yt-1 + \u03b81 \u03f5 t-1 + \u03f5 t<\/b><\/center>\n<br><br>\n<h3>Searching model order over AIC and BIC<\/h3>\n<b>AIC : Akaike Information Criterion<\/b> \n<li>Lower AIC indicates a better model<\/li>\n<br>\n<b>BIC : Bayesian Information Criterion<\/b> \n<li>Similar to AIC<\/li>\n<li>Lower BIC indicates a better model<\/li>","275629f6":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observations :<\/li>\n  <\/ol>\n<\/nav>\n\n<li><b>Standardize Residuals: <\/b> There is no obvious structure in the residuals, hence our model has worked correctly<\/li>\n<li><b>Histogram plus estimated Density: <\/b> The green and orange lines are almost the same, hence our model is good <\/li>\n<li><b>Normal Q-Q: <\/b> The dots are not aligned with the red line, which states that our model residuals are not normally distributed<\/li>\n<li><b>Correlogram: <\/b> There is no significant correlation in the residuals, it means that our model has captured most of the information in the data that is required<\/li>","08483b63":"<h2> Seasonal Arima <\/h2>\n\n![image.png](attachment:image.png)","44d12198":"<center><h2>Seasonal Decomposition<\/h2><\/center>\n<br>\n<b>Time Series = Trend + Season + Residual","5974750e":"<center> <h3 class=\"text-muted\">Plot Samples<\/h3> <\/center>","d858f61c":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Conclusion :<\/li>\n  <\/ol>\n<\/nav>\n\n<li  style=\"font-size:16px\">T-statistic = -7.2, More negative means likey to be stationary<\/li>\n<li style=\"font-size:16px\">P-value < 0.05, Hence we reject the Null hypothesis that Time series is non-Stationary, which concludes that our data is stationary <\/li>\n","cba5f10b":"<nav aria-label=\"breadcrumb\">\n  <ol class=\"breadcrumb\">\n    <li class=\"breadcrumb-item active\" aria-current=\"page\">Observations :<\/li>\n  <\/ol>\n<\/nav>\n<p>\nWe can see that average smoothing is not as effective as Wavelet denoising at finding macroscopic trends and pattersns in the data. A lot of the noise in the original sales persists even after denoising. Therefore, wavelet denoising is clearly more effective at finding trends in the sales data. Nonetheless, average smoothing or \"rolling mean\" can also be used to calculate useful features for modeling.\n<\/p>","29fe1609":"<center> <h2>Import Libraries<\/h2> <\/center>","e8b41d5c":"<center><h2> Modelling with SARIMA<\/h2><\/center>"}}