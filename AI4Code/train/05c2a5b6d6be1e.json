{"cell_type":{"4525dda7":"code","2e88cfee":"code","767cdba1":"code","7a41753e":"code","84a00ab4":"code","31b3e880":"code","cff539c0":"code","96557509":"code","fd94cc5c":"code","a8741d6b":"code","37b23e48":"code","3e28dc79":"code","dfadc1b3":"code","dc6294b5":"code","a610fb5d":"code","b03332c1":"code","db6af627":"code","fd81b73e":"code","4c929e52":"code","9f754c99":"code","977717ee":"code","e826d917":"code","f9928b22":"code","5e6f652a":"code","7f417eb3":"code","9e3a06a0":"code","f6a03394":"code","c57a4fdc":"code","6f038cba":"code","e45abc60":"code","1d9efe77":"code","3d783ec8":"code","97b260f0":"code","55298ba4":"code","61f6dbe4":"code","2d41fccb":"code","f70cd9b1":"code","08c10023":"code","d592103f":"code","47ad37e5":"code","a7f5c1a6":"code","21caad35":"code","d9a5c785":"code","20cb16c8":"code","2866b150":"code","958780a1":"code","4e2747fd":"code","28f59c65":"code","9f77cf15":"code","8fae0d19":"code","4fc3cbc9":"code","088896a6":"code","b74663f2":"code","471b56b2":"code","bdd0e55b":"code","e19321fe":"code","3151f44b":"code","2116592e":"code","580d596e":"code","21587969":"markdown","5e229d7f":"markdown","f418eb3a":"markdown","756abc7e":"markdown","0b8c78be":"markdown","4bcd2bf1":"markdown","9a825b71":"markdown","59b73853":"markdown","11ce1236":"markdown","8db85031":"markdown","795d6200":"markdown","3b56a7c8":"markdown","6ef9064d":"markdown","3f684e5c":"markdown","ddceec25":"markdown","7e5c721a":"markdown","1e5034fc":"markdown","ab3b9a36":"markdown"},"source":{"4525dda7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e88cfee":"#import required libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas_profiling as pp\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier  \nfrom sklearn.metrics import accuracy_score,f1_score,auc,roc_curve,classification_report,confusion_matrix \n","767cdba1":"#read the csv files\n\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\n\ntrain.shape , test.shape","7a41753e":"train.head()","84a00ab4":"#number of passangers who survived \nsdf = train.Survived.value_counts()\n(sdf\/len(train.Survived))*100","31b3e880":"#check for null\/missing values\ndef  check_null(df):\n    null_df = train.isnull().sum()\n    null_df = null_df[null_df!=0]\n    return null_df\n\nprint(check_null(train)) \nprint(\"----\"*5)\nprint(check_null(test))","cff539c0":"pp.ProfileReport(train)","96557509":"#rate of men and women who survived\nmale = train[train.Sex=='male']\nrate_male = sum(male['Survived'])\/len(male['PassengerId'])*100\n\nfemale = train[train.Sex=='female']\nrate_female = sum(female['Survived'])\/len(female['PassengerId'])*100\n\nprint(\"Rate Male who survived :\",rate_male)\nprint(\"Rate Felame who survived :\",rate_female)","fd94cc5c":"g = sns.FacetGrid(train, col=\"Sex\",row='Survived', height=3.5);\ng.map(sns.histplot, \"Age\");\n","a8741d6b":"sns.countplot( x='Survived', data=train, hue=\"Pclass\");","37b23e48":"sns.countplot( x='Survived', data=train, hue=\"Embarked\");","3e28dc79":"train.groupby('Survived')['SibSp'].plot.hist(histtype= 'bar');\ntrain[['SibSp','Survived']].groupby('SibSp').sum().sort_values(by='Survived',ascending=False)\nplt.legend();","dfadc1b3":"train.groupby('Survived')['Parch'].plot.hist(histtype= 'bar');\ntrain[['Parch','Survived']].groupby('Parch').sum().sort_values(by='Survived',ascending=False)\nplt.legend();","dc6294b5":"g = sns.FacetGrid(train, col='Survived', height=3.5);\ng.map(sns.histplot, \"Fare\");","a610fb5d":"g = sns.FacetGrid(train, col='Embarked',size=4.5, aspect=1.6);\ng.map(sns.pointplot, 'Pclass','Survived','Sex');\ng.add_legend();","b03332c1":"sns.boxplot(data = train['Age'], orient = \"h\")","db6af627":"sns.boxplot(data = train['Fare'], orient = \"h\")","fd81b73e":"#drop unused cols\nprint(\"Before : \",train.shape,test.shape)\n\ntrain = train.drop(['PassengerId','Ticket','Cabin'],axis=1)\ntest = test.drop(['PassengerId','Ticket','Cabin'],axis=1)\n\nprint(\"After : \",train.shape,test.shape)","4c929e52":"train['family_count'] = train['SibSp']+train['Parch']\ntest['family_count'] = test['SibSp']+test['Parch']\n","9f754c99":"data = [train, test]\nfor dataset in data:\n    dataset.loc[dataset['family_count'] > 0, 'isalone'] = 0\n    dataset.loc[dataset['family_count'] == 0, 'isalone'] = 1\naxes = sns.factorplot('family_count','Survived', \n                      data=train, aspect = 2.5, );","977717ee":"train.isalone = train.isalone.astype('int')\ntest.isalone = test.isalone.astype('int')","e826d917":"train.head()","f9928b22":"#Extract Title feature\n\nfor dataset in [train,test]:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","5e6f652a":"#We can replace many titles with a more common name or classify them as Rare.\n\nfor dataset in [train,test]:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss') #Mlle is a traditional alternative for an unmarried woman aka Miss.\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss') #Ms for unmarrid woman aka Miss\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs') #Mme is used before the name of a married woman \n    \ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","7f417eb3":"g = sns.FacetGrid(train, col=\"Title\",row='Survived', height=3.5);\ng.map(sns.histplot, \"Age\");","9e3a06a0":"g = sns.FacetGrid(train, col=\"Title\",hue='Survived', height=3.5);\ng.map(sns.histplot, \"family_count\");","f6a03394":"#drop Name col\ntrain = train.drop(['Name'],axis=1)\ntest = test.drop(['Name'],axis=1)","c57a4fdc":"train.head()","6f038cba":"print(check_null(train)) \nprint(\"----\"*5)\nprint(check_null(test))","e45abc60":"train[train.Embarked.isnull()]","1d9efe77":"num_cols = train.select_dtypes(exclude='O').columns\nnum_cols = [col for col in num_cols if col!='Survived']\nnum_cols","3d783ec8":"num_cols = train.select_dtypes(exclude='O').columns\nnum_cols = [col for col in num_cols if col!='Survived']\ncat_cols = train.select_dtypes(include='O').columns\n\nprint(\"Numerical cols :\",num_cols,\"\\n\",\"---\"*25)\nprint(\"Categorical cols :\",cat_cols)","97b260f0":"#Impute missing values using simple imputer\n\n#numeric col\nimputer = SimpleImputer(strategy='mean')\ntrain[num_cols] = imputer.fit_transform(train[num_cols])\ntest[num_cols] = imputer.fit_transform(test[num_cols])\n\n#categorical col\nimputer = SimpleImputer(strategy='most_frequent')\ntrain[cat_cols] = imputer.fit_transform(train[cat_cols])\ntest[cat_cols] = imputer.fit_transform(test[cat_cols])","55298ba4":"print(check_null(train)) \nprint(\"----\"*5)\nprint(check_null(test))","61f6dbe4":"#train.iloc[[61,829]] ","2d41fccb":"#corelation plot\nplt.figure(figsize=(12,9))\nmatrix = np.triu(train.corr())\nsns.heatmap(train.corr(), annot=True, mask=matrix)","f70cd9b1":"print('Before : ',train.shape,test.shape)\ntrain_dummies = pd.get_dummies(train[cat_cols],prefix=cat_cols,drop_first=True)\ntest_dummies = pd.get_dummies(test[cat_cols],prefix=cat_cols,drop_first=True)\n\ntrain = pd.concat([train,train_dummies],axis=1)\ntest = pd.concat([test,test_dummies],axis=1)\n\ntrain = train.drop(cat_cols,axis=1)\ntest = test.drop(cat_cols,axis=1)\n\nprint('After : ',train.shape,test.shape)","08c10023":"train.describe()","d592103f":"scaler = StandardScaler()\ntrain[num_cols] = scaler.fit_transform(train[num_cols])\ntest[num_cols] = scaler.fit_transform(test[num_cols])","47ad37e5":"train.describe()","a7f5c1a6":"X = train.copy()\ny = X.Survived\ndel X['Survived']\n\nX.shape , y.shape , test.shape","21caad35":"X_train, X_test, y_train, y_test = train_test_split(X,y,random_state=42,test_size=0.3,stratify=y)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","d9a5c785":"#Baseline accuracy\nb = y_train.value_counts()\n(b\/sum(b))*100","20cb16c8":"#Fitting Random Forest Classification to the training set to get important features\n\nclassifier = RandomForestClassifier(n_estimators = 100,random_state=42)\nclassifier.fit(X,y)\n#importances = classifier.feature_importances_","2866b150":"plt.figure(figsize=(10,10))\n\nfeat_importances = pd.Series( classifier.feature_importances_, index=X.columns)\nfeat_importances.sort_values(ascending=True)\nfeat_importances.nlargest(20).plot(kind='barh');","958780a1":"X_filtered_features = feat_importances.nlargest(7).index #select first 7 features\nX_filtered_features","4e2747fd":"#Logistic Regression\n\nlogreg = LogisticRegression() #create instance \nlogreg.fit(X_train[X_filtered_features], y_train) #fit the model\n\ny_pred = logreg.predict(X_test[X_filtered_features]) #prediction\n\n#accuracy score\ntrain_acc = round(logreg.score(X_train[X_filtered_features], y_train) * 100, 2)\ntest_acc = round(accuracy_score(y_test,y_pred) * 100, 2)\nprint(\"Train Acc : \",train_acc)\nprint(\"Test Acc : \",test_acc)","28f59c65":"print(\"Confusion matrix :\\n\",confusion_matrix (y_test,y_pred))\n\nprint(\"\\nClassification_report : \\n\",classification_report (y_test,y_pred))\n\nfpr,tpr,t = roc_curve(y_test,y_pred)\nprint(\"\\nAUC : \",auc(fpr,tpr))","9f77cf15":"rf_model = RandomForestClassifier(n_estimators =100,random_state=42)\nrf_model.fit(X_train[X_filtered_features],y_train)\n\ny_pred = rf_model.predict(X_test[X_filtered_features]) #prediction\n\n#accuracy score\ntrain_acc = round(rf_model.score(X_train[X_filtered_features], y_train) * 100, 2)\ntest_acc = round(accuracy_score(y_test,y_pred) * 100, 2)\nprint(\"Train Acc : \",train_acc)\nprint(\"Test Acc : \",test_acc)","8fae0d19":"print(\"Confusion matrix :\\n\",confusion_matrix (y_test,y_pred))\n\nprint(\"\\nClassification_report : \\n\",classification_report (y_test,y_pred))\n\nfpr,tpr,t = roc_curve(y_test,y_pred)\nprint(\"\\nAUC : \",auc(fpr,tpr))","4fc3cbc9":"from sklearn.model_selection import GridSearchCV \n#Use the GridSearch CV to search for best hyperparameters\n\nparameters = {\n    \"max_depth\":[1,2,3],\n    \"n_estimators\":[25,50,75,100],\n    \"max_features\":[5,6,7]\n    \n}\n\n#First create the base model to tune\nrf = RandomForestClassifier(random_state=42)\n\n#Grid search of parameters, using 5 fold cross validation, \n\nrf_random = GridSearchCV(estimator=rf, param_grid=parameters, cv=5)\n\n#Fit the random search model\nrf_random.fit(X_train[X_filtered_features], y_train)","088896a6":"#best parameters\nrf_random.best_params_","b74663f2":"rf_model = RandomForestClassifier(n_estimators =25,max_depth=3,max_features=5,random_state=42)\nrf_model.fit(X_train[X_filtered_features],y_train)\n\ny_pred = rf_model.predict(X_test[X_filtered_features]) #prediction\n\n#accuracy score\ntrain_acc = round(rf_model.score(X_train[X_filtered_features], y_train) * 100, 2)\ntest_acc = round(accuracy_score(y_test,y_pred) * 100, 2)\nprint(\"Train Acc : \",train_acc)\nprint(\"Test Acc : \",test_acc)","471b56b2":"print(\"Confusion matrix :\\n\",confusion_matrix (y_test,y_pred))\n\nprint(\"\\nClassification_report : \\n\",classification_report (y_test,y_pred))\n\nfpr,tpr,t = roc_curve(y_test,y_pred)\nprint(\"\\nAUC : \",auc(fpr,tpr))","bdd0e55b":"xgb = GradientBoostingClassifier (random_state=42)\nxgb.fit(X_train[X_filtered_features],y_train)\n\ny_pred = xgb.predict(X_test[X_filtered_features]) #prediction\n\n#accuracy score\ntrain_acc = round(xgb.score(X_train[X_filtered_features], y_train) * 100, 2)\ntest_acc = round(accuracy_score(y_test,y_pred) * 100, 2)\nprint(\"Train Acc : \",train_acc)\nprint(\"Test Acc : \",test_acc)","e19321fe":"parameters = {\n    \"max_depth\":[1,2,3,4,5],\n    \"n_estimators\":[25,50,75,100],\n    \"learning_rate\":[0.001,0.01,0.1]\n    \n}\n\n#First create the base model to tune\nxgb = GradientBoostingClassifier(random_state=42,max_features='sqrt')\n\n#Grid Search of parameters, using 5 fold cross validation, \n\nxgb_model = GridSearchCV(estimator=xgb,param_grid=parameters, cv=5)\n\n#Fit the random search model\nxgb_model.fit(X_train[X_filtered_features], y_train)\nxgb_model.best_params_","3151f44b":"xgb = GradientBoostingClassifier (learning_rate=0.1 ,max_depth = 4,max_features='sqrt',n_estimators=100 ,random_state=42)\nxgb.fit(X_train[X_filtered_features],y_train)\n\ny_pred = xgb.predict(X_test[X_filtered_features]) #prediction\n\n#accuracy score\ntrain_acc = round(xgb.score(X_train[X_filtered_features], y_train) * 100, 2)\ntest_acc = round(accuracy_score(y_test,y_pred) * 100, 2)\nprint(\"Train Acc : \",train_acc)\nprint(\"Test Acc : \",test_acc)","2116592e":"print(\"Confusion matrix :\\n\",confusion_matrix (y_test,y_pred))\n\nprint(\"\\nClassification_report : \\n\",classification_report (y_test,y_pred))\n\nfpr,tpr,t = roc_curve(y_test,y_pred)\nprint(\"\\nAUC : \",auc(fpr,tpr))","580d596e":"model = RandomForestClassifier(n_estimators =25,max_depth=3,max_features=5,random_state=42)\nmodel.fit(X[X_filtered_features],y)\npredictions = model.predict(test[X_filtered_features])\n\nsubmission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nsubmission['Survived'] = predictions\nsubmission.to_csv('Submission_rf.csv', index=False)","21587969":"### Logistic Regression","5e229d7f":"Overfitted","f418eb3a":"# Handle Missing Values","756abc7e":"**Hyperparameter Tunning**","0b8c78be":"**Extract Title feature**","4bcd2bf1":"Compared to Logistic Regression Model and GradientBoostingClassifier , RandomForestClassifier performed well which is more generalized model ","9a825b71":"## Public Score : 0.78468","59b73853":"### GradientBoosting","11ce1236":"# Feature Engineering","8db85031":"\n\n<html>\n<h3>Data Dictionary<\/h3>\n<table border=1>\n<tbody>\n<tr><th><b>Variable<\/b><\/th><th><b>Definition<\/b><\/th><th><b>Key<\/b><\/th><\/tr>\n<tr>\n<td>survival<\/td>\n<td>Survival<\/td>\n<td>0 = No, 1 = Yes<\/td>\n<\/tr>\n<tr>\n<td>pclass<\/td>\n<td>Ticket class<\/td>\n<td>1 = 1st, 2 = 2nd, 3 = 3rd<\/td>\n<\/tr>\n<tr>\n<td>sex<\/td>\n<td>Sex<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>Age<\/td>\n<td>Age in years<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>sibsp<\/td>\n<td># of siblings \/ spouses aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>parch<\/td>\n<td># of parents \/ children aboard the Titanic<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>ticket<\/td>\n<td>Ticket number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>fare<\/td>\n<td>Passenger fare<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>cabin<\/td>\n<td>Cabin number<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>embarked<\/td>\n<td>Port of Embarkation<\/td>\n<td>C = Cherbourg, Q = Queenstown, S = Southampton<\/td>\n<\/tr>\n<\/tbody>\n<\/table>\n\n<h3>Variable Notes<\/h3><h6>\n<p><b>pclass<\/b>: A proxy for socio-economic status (SES)<br> 1st = Upper<br> 2nd = Middle<br> 3rd = Lower<br><br> <b>age<\/b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5<br><br> <b>sibsp<\/b>: The dataset defines family relations in this way...<br> Sibling = brother, sister, stepbrother, stepsister<br> Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)<br><br> <b>parch<\/b>: The dataset defines family relations in this way...<br> Parent = mother, father<br> Child = daughter, son, stepdaughter, stepson<br> Some children travelled only with a nanny, therefore parch=0 for them.<\/p><h6>\n\n<\/html>","795d6200":"## Hey Kagglers , this is my first kernel on Kaggle \n## Please do upvote if this kernel finds helpful and\n## do leave comment\/feedback if you can suggest improvements.\n## \n## Thank-You!!","3b56a7c8":"### RandomForestClassifier","6ef9064d":"# Feature Selection Using RandomForestClassifier ","3f684e5c":"# Convert Categorical cols","ddceec25":"# Titanic - Machine Learning from Disaster","7e5c721a":"**Extarct isalone feature**","1e5034fc":"**Hyperparameter Tunning**","ab3b9a36":"Passanger survival rate is less"}}