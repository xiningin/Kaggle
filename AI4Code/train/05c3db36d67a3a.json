{"cell_type":{"55eac43d":"code","f951b07b":"code","be99599a":"code","5ef65069":"code","e967bc5b":"code","54f2638e":"code","de145e3f":"code","85fbf0a4":"code","8850f78f":"code","2e180eb0":"code","15d5c258":"code","a2cfdc6a":"code","4cb3bdaf":"code","c751a0c4":"code","24b90629":"code","ca83ee15":"code","5e1a73cf":"code","e87707ef":"code","d94acd88":"code","dfc64a2a":"code","c516a4d1":"code","c495cc7f":"code","19b4b0b0":"code","91a0cf2a":"code","1d4ca6a9":"code","b6480e27":"code","97229f8d":"code","c3afecb7":"code","bc4ef98a":"code","da898ba3":"code","eeacbd2a":"code","900ee4af":"code","4c199f83":"code","e69fd691":"code","10ed5d41":"code","419cac83":"code","63c2eec9":"code","4a45aff1":"code","5046456a":"code","d13d3599":"code","be481aa8":"code","1278c43f":"code","7688f661":"code","92e9cae9":"code","75f6d454":"code","34e79e3c":"code","dcc57e7b":"code","a87a58c8":"markdown","3cfbf52d":"markdown","d71130c7":"markdown","50fe3e78":"markdown","b6dcb4f4":"markdown","f9b7f0a8":"markdown","3e7aed22":"markdown","712c53cc":"markdown","26a1b27d":"markdown","cb99521e":"markdown","f8179a53":"markdown","f85d7bc3":"markdown","09bcc411":"markdown","20e226d9":"markdown","c89ddae3":"markdown","468b22b3":"markdown","6fadc837":"markdown","8edab1c2":"markdown","55bb485e":"markdown","9f22d63d":"markdown","032e65cc":"markdown","c1f51c4b":"markdown","98acdbdb":"markdown","4856ef0d":"markdown","fc1f854d":"markdown","8e7a8203":"markdown","b28e2c17":"markdown","f7189ea0":"markdown","9fca26d9":"markdown","568d72e4":"markdown","30d358ae":"markdown","7e5bd806":"markdown","b0f2291f":"markdown"},"source":{"55eac43d":"risk_factor_terms = {\n    'age': ['age', 'children', 'elder', 'infant*', 'old', 'older', 'senior', 'year old', 'years old', 'young', 'younger'],\n    'cancer': ['cancer', 'cancer of the blood', 'carcinoma', 'leucaemia', 'leukemia', 'lung cancer', 'lymphoma', 'malignant neoplasm', 'malignant neoplastic disease', 'malignant tumor', 'metastatic tumor'],\n    'cardiovascular disease': ['abnormal heart rhytmns', 'aorta disease', 'angina', 'cardiomyopathy', 'cardiovascular', 'cardiovascular disease', 'congenital heart disease', 'congestive heart failure', 'coronary', 'coronary arterial disease', 'coronary artery disease', 'deep vein thrombosis', 'heart attack', 'heart failure', 'heart valve disease', 'ischemic heart disease', 'Marfan syndrome', 'pericardial disease', 'peripheral vascular disease', 'pulmonary embolism', 'rheumatic heart disease', 'specified heart arrhytmia', 'vascular disease'],\n    'cerebrovascular disease': ['aneurysm', 'cerebr*', 'embolism', 'haemorragic stroke', 'hemorrhagic stroke', 'ischemic stroke', 'sybarachnoid hemorrhage', 'TIA', 'unspecified stroke', 'stroke'],\n    'chronic kidney disease': ['chronic kidney disease', 'kidney'],\n    'chronic liver disease': ['chronic liver disease', 'cirrhosis', 'hepatocellular carcinoma', 'hepatitis', 'liver'],\n    'COPD': ['chronic obstructive pulmonary disease', 'COPD'],\n    'diabetes': ['diabetes'],\n    'drinking': ['alcohol*', 'drinking'],\n    'hypertension': ['high blood pressure', 'hyperpiesia', 'hyperpiesis', 'hypertensive', 'hypertension'],\n    'male gender': ['gender', 'male', 'man', 'men', 'sex'],\n    'obesity': ['morbidly obese', 'morbidly overweight', 'obes*', 'overweight'],\n    'respiratory system disease': ['apnea', 'asbestosis', 'asthma', 'bronciectasis', 'bronchiolitis', 'bronchitis', 'chest infection', 'chronic cough', 'common cold', 'cough', 'cystic fibrosis', 'emphysema', 'hantavirus', 'idiopathic pulmonary fibrosis', 'influenza', 'laryngitis', 'lung infection', 'pertussis', 'pleural effusion', 'pleurisy', 'pneumonia', 'pulmonary embolism', 'pulmonary hypertension', 'pulmonary nodules', 'respiratory', 'respiratory disease', 'respiratory syncytial virus', 'respiratory system disease', 'sarcoidosis', 'sinus infection', 'sinusitis', 'sleep apnea', 'tonsilitis', 'tuberculosis', 'work-related asthma'],\n    'smoking': ['cigarette*', 'smok*', 'tobacco']\n}\n\ncovid19_synonyms = [\"coronavirus disease 19\", \"sars cov 2\", \"2019 ncov\", \"2019ncov\", \"coronavirus 2019\", \"wuhan pneumonia\", \"wuhan virus\", \"wuhan coronavirus\", \"covid19\", \"covid-19\"]\n\n# Max documents to search per Lucene query (set it to e.g. 1000000 to return all possible matches)\nMAX_SEARCH_RESULTS = 1000000\n# Min grammar match score required to keep a match, the lesser the score the more tokens we allow between the ocurrences of a risk factor and a severe measure\n# Each token between the risk factor and the severe measure penalizes the overall score by 15 points\n# We allow for at least 40 tokens in between (roughly the lenght of 2 sentences), which will add -15*40=-600 points to the overall match score\nMIN_GRAMMAR_SCORE = -600\n# Left and right context size in characters to extract upon a grammar match (roughly 10 words, assuming 5 chars per word on average)\nCONTEXT_SIZE = 50\n# Styles for highlighting the matched risk factor and severe measures within the full text of the papers\nRISK_FACTOR_STYLE = \"background-color: #EC1163\"\nSEVERE_STYLE = \"background-color: #80FF32\"","f951b07b":"# Install and import relevant libraries\n!python -m easy_install ..\/input\/compiledlucene\/bk\/lucene-8.1.1-py3.6-linux-x86_64.egg\n!cp -r ..\/input\/compiledlucene\/bk\/JCC-3.7-py3.6-linux-x86_64.egg \/opt\/conda\/lib\/python3.6\/site-packages\/\nimport sys\nsys.path\nsys.path.append('\/opt\/conda\/lib\/python3.6\/site-packages\/JCC-3.7-py3.6-linux-x86_64.egg')\nsys.path.append('\/opt\/conda\/lib\/python3.6\/site-packages\/lucene-8.1.1-py3.6-linux-x86_64.egg')","be99599a":"!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp_2.8.0-0ubuntu1_xenial_amd64.deb\n!dpkg -i ..\/input\/libgrapenlp\/libgrapenlp-dev_2.8.0-0ubuntu1_xenial_amd64.deb\n!pip install pygrapenlp","5ef65069":"import sys, os, lucene, threading, time, html\nfrom datetime import datetime\nfrom java.nio.file import Paths\nfrom org.apache.lucene.analysis.miscellaneous import LimitTokenCountAnalyzer\nfrom org.apache.lucene.document import Document, Field, FieldType\nfrom org.apache.lucene.index import FieldInfo, IndexWriter, IndexWriterConfig, IndexOptions\nfrom org.apache.lucene.analysis.standard import StandardAnalyzer\nfrom org.apache.lucene.index import DirectoryReader\nfrom org.apache.lucene.queryparser.classic import QueryParser\nfrom org.apache.lucene.store import SimpleFSDirectory\nfrom org.apache.lucene.search import IndexSearcher\n\nLUCENE_INDEX_DIR = \"documentLevel\"\nLUCENE_BASE_DIR = \"\/kaggle\/working\"\nCOVID_FULLTEXT_DF = \"..\/input\/covidfulltext\/metadata_and_fulltext_2020-04-17.csv\"","e967bc5b":"from collections import OrderedDict\nfrom pygrapenlp import u_out_bound_trie_string_to_string\nfrom pygrapenlp.grammar_engine import GrammarEngine","54f2638e":"from tqdm.auto import tqdm\nimport numpy as np\nimport pandas as pd\nimport numpy as np\nimport pandas as pd\nimport bs4 as bs\nimport urllib.request\nfrom time import sleep\nimport os\nfrom IPython.display import display, Image, HTML","de145e3f":"# Load indexed metadata \nmetadata = pd.read_csv('..\/input\/covidfulltext\/metadata_and_fulltext_2020-04-17.csv')\nmetadata.shape","85fbf0a4":"%%capture --no-display\n# This section loads the indexs and takes about 4 minutes to run\nclass Ticker(object):\n\n    def __init__(self):\n        self.tick = True\n\n    def run(self):\n        while self.tick:\n            sys.stdout.write('.')\n            sys.stdout.flush()\n            time.sleep(1.0)\n\nclass IndexFiles(object):\n    \"\"\"Usage: python IndexFiles <doc_directory>\"\"\"\n\n    def __init__(self, root, storeDir, analyzer):\n        ##print(\"before store\")\n        if not os.path.exists(storeDir):\n            os.mkdir(storeDir)\n        ##print(\"after store\")\n\n        store = SimpleFSDirectory(Paths.get(storeDir))\n        ##print(storeDir)\n        analyzer = LimitTokenCountAnalyzer(analyzer, 1048576)\n        ##print(\"after analyzer \")\n\n        config = IndexWriterConfig(analyzer)\n        ##print(\"after config\")\n\n        config.setOpenMode(IndexWriterConfig.OpenMode.CREATE)\n        ##print(\"before writer\")\n        writer = IndexWriter(store, config)\n        ##print(\"after writer\")\n        self.indexDocs(root, writer)\n        ticker = Ticker()\n        ##print ('commit index')\n        threading.Thread(target=ticker.run).start()\n        writer.commit()\n        writer.close()\n        ticker.tick = False\n        ##print ('done')\n\n    def indexDocs(self, root, writer):\n\n        t1 = FieldType()\n        t1.setStored(True)\n        t1.setTokenized(False)\n        t1.setStoreTermVectors(True)\n        t1.setStoreTermVectorOffsets(True)\n        t1.setStoreTermVectorPositions(True)\n        t1.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n        \n\n        t2 = FieldType()\n        t2.setStored(True)\n        t2.setTokenized(True)\n        t2.setStoreTermVectors(True)\n        t2.setStoreTermVectorOffsets(True)\n        t2.setStoreTermVectorPositions(True)\n        t2.setIndexOptions(IndexOptions.DOCS_AND_FREQS_AND_POSITIONS_AND_OFFSETS)\n                \n        i = 1\n        for index, row in tqdm(metadata.iterrows(), desc='Indexing: ', total=len(metadata.index)):\n            print (\"adding \", i , \"th document:\", row['paper_id'])\n            try :\n                doc = Document()\n                doc.add(Field(\"paper_id\", row['paper_id'], t1))\n                doc.add(Field(\"title\", row['title'], t2))\n                doc.add(Field(\"doi\",row['doi'], t1))\n                doc.add(Field(\"pmcid\", row['pmcid'], t1))\n                doc.add(Field(\"publish_time\", row['publish_time'], t1))\n                doc.add(Field(\"journal\", row['journal'], t1))\n                doc.add(Field(\"url\", row['url'], t1))\n                \n                if len(row['text']) > 0:\n                    doc.add(Field(\"full_text\", row['text'], t2))\n                else :\n                    print (\"warning: no fulltext available in %s\", row['title'])\n                    \n                if len(row['abstract_y']) > 0:\n                    doc.add(Field(\"abstract\", row['abstract_y'], t2))\n                else :\n                    print (\"warning: no abstract available in %s\", row['title'])\n                writer.addDocument(doc)\n            except (RuntimeError, TypeError, NameError):\n                pass\n            i=i+1\n            \n\nlucene.initVM()\nstart = datetime.now()\ntry:\n    IndexFiles(LUCENE_BASE_DIR, os.path.join(LUCENE_BASE_DIR, LUCENE_INDEX_DIR),StandardAnalyzer())\n    end = datetime.now()\n    print (end - start)\nexcept (RuntimeError, TypeError, NameError):\n    print (\"Failed: \")\n    raise ","8850f78f":"def make_or_term_query(terms):\n    quoted_terms = terms.copy()\n    for i in range(len(quoted_terms)):\n        if ' ' in quoted_terms[i]:\n            quoted_terms[i] = '\"' + quoted_terms[i] + '\"'\n    return ' OR '.join(quoted_terms)\n\ndef make_and_query(subqueries):\n    return \"(\" + \") AND (\".join(subqueries) + \")\"\n\ndef search(searcher, analyzer, query_expression):\n    query = QueryParser(\"full_text\", analyzer).parse(query_expression)\n    scoreDocs = searcher.search(query, MAX_SEARCH_RESULTS).scoreDocs\n    results = []\n    for scoreDoc in scoreDocs:\n        doc = searcher.doc(scoreDoc.doc)\n        result = {\n            'date': doc.get(\"publish_time\"),\n            'study': doc.get(\"title\"),\n            'study_link': doc.get(\"url\"),\n            'journal': doc.get(\"journal\"),\n            'paper_id': doc.get('paper_id'),\n            'paper_full_text': doc.get('full_text'),\n            'pmcid': doc.get(\"pmcid\")\n        }\n        results.append(result)\n    return pd.DataFrame(results)","2e180eb0":"directory = SimpleFSDirectory(Paths.get(os.path.join(LUCENE_BASE_DIR, LUCENE_INDEX_DIR)))\nsearcher = IndexSearcher(DirectoryReader.open(directory))\nanalyzer = StandardAnalyzer()","15d5c258":"clinical_expression = make_or_term_query(['clinical', 'epidem*', 'virological', 'host susceptibility to serve'])\ncovid19_expression = make_or_term_query(covid19_synonyms)\nquery_expression = make_and_query([clinical_expression, covid19_expression])\nprint('query: ', query_expression)\ndf_relevant_papers = search(searcher, analyzer, query_expression)\nprint(\"{} documents found\".format(len(df_relevant_papers.index)))\n","a2cfdc6a":"# Examine results \ndf_relevant_papers.head()","4cb3bdaf":"# Only use papers that have a PMCID\ndf_relevant_papers = df_relevant_papers[~df_relevant_papers['pmcid'].str.contains('NaN')]","c751a0c4":"# This list will be used to pull publication data from PubMed servers\nrelevant_papers = df_relevant_papers.pmcid.values","24b90629":"PMC_FOLDER = os.path.join('..', 'input', 'pmc-tables')\n\ndef download_article_tables(pmcid, print_tables=True, use_cached_tables=True):\n#     print(\"\", end = '.')\n    source = None\n    from_cache = 0\n    from_pubmed = 0\n\n    # If requested to use cached tables and the table is in the cache, load it from the cache\n    if use_cached_tables:\n        cached_table_file = os.path.join(PMC_FOLDER, pmcid + '.xml')\n        if os.path.exists(cached_table_file):\n            with open(cached_table_file, 'rb') as fp:\n                source = fp.read()\n            from_cache += 1\n\n    # If requested not to use the cached tables or the needed table was not in the cache, download it\n    if not source:\n        sleep(0.5) # This line is needed as the NCBI server will block if there are too many frequent requests\n        url = 'https:\/\/eutils.ncbi.nlm.nih.gov\/entrez\/eutils\/efetch.fcgi?db=pmc&id={0}'.format(pmcid)\n        #beautiful soup\n        source = urllib.request.urlopen(url).read()\n        from_pubmed += 1\n\n    soup = bs.BeautifulSoup(source,'lxml')\n    df = None\n    cols = None\n    dfs = []\n    title = ''\n    abstract = ''\n    journal = ''\n    title = soup.findAll('article-title')[0].find(text=True)# title\n    try:\n        abstract = soup.find(\"abstract\").text\n    except:\n        abstract = ''\n    #getting the table\n    tables = soup.findAll('table')  # Note: \n    i = 1\n    for table in tables:\n        if(print_tables):\n            print(\"\\nTable {0}  for paper id {1}: \".format(i, pmcid))\n        table_rows = table.find_all('tr')\n        #generate df\n        df = pd.DataFrame()\n        for tr in table_rows:\n            td = tr.find_all(['td'])\n            row = [i.text for i in td]\n            \n            if(len(row) > 0):\n                s = pd.Series(row)\n                df = df.append(s, ignore_index=True)\n            else: # it is a header\n                th = tr.find_all(['th'])\n                cols = [i.text for i in th]\n        \n        if(cols): #assign header\n            if(len(df.columns) == len(cols)+1):\n                #add one column for the title\n                cols.insert(0,'')\n            if(len(df.columns) == len(cols)):\n                df.columns = cols\n        \n        dfs.append( df )\n        if(print_tables):\n            print(title)\n            print(abstract)\n            display(HTML(df.to_html()))\n        i = i + 1\n\n    return dfs, from_cache, from_pubmed","ca83ee15":"unique_pmcids = df_relevant_papers[~df_relevant_papers['pmcid'].str.contains('NaN')].pmcid.unique().tolist()\nunique_pmcids.sort()\n# Save needed PMCIDs for offline download of corresponding tables\nwith open('pmcids.txt', 'wt', encoding='UTF-8') as fp:\n    for pmcid in unique_pmcids:\n        fp.write(pmcid)\n        fp.write('\\n')","5e1a73cf":"%%capture --no-display\ntables_by_pmcid = {}\nfrom_cache = 0\nfrom_pubmed = 0\nfor pmcid in tqdm(unique_pmcids, desc='Downloading:'):\n    try:\n        df_list, from_cache_inc, from_pubmed_inc = download_article_tables(pmcid, print_tables=False)\n        from_cache += from_cache_inc\n        from_pubmed += from_pubmed_inc\n        if len(df_list) > 0:\n            tables_by_pmcid[pmcid] = df_list\n    except Exception as e:\n        display(HTML(\"<p>Unable to download tables for PMCID {}; error message: {}<\/p>\".format(pmcid, str(e))))\ndisplay(HTML(\"<p>Cached papers\/downloaded papers: {}\/{}\".format(from_cache, from_pubmed)))","e87707ef":"# Print some stats on tables \ntable_count = 0\npmcid_count = len(unique_pmcids)\npmcid_with_tables_count = 0\nfor pmcid, tables in tables_by_pmcid.items():\n    table_count += len(tables)\n    pmcid_with_tables_count += 1\nprint('Total PMCID papers: {}'.format(pmcid_count))\nprint('Number of PMCID papers with tables: {} ({:.2%})'.format(pmcid_with_tables_count, pmcid_with_tables_count \/ pmcid_count))\nprint('Total tables downloaded: {}'.format(table_count))\nprint('Average number of tables per PMCID paper: {}'.format(1.0 * table_count \/ pmcid_count))","d94acd88":"def get_statistical_info(severity_numbers):\n#severity_number pattern: non-severity, non-severity_n, severity, severity_n\n    OR, lower_CI, higher_CI, CI, p_value = 0.0, 0.0, 0.0, 0.0, 0.0\n    ##Cases with positive (bad) outcome\n    a = severity_numbers[2]     # Number in exposed group\n    b = severity_numbers[3] - a # Number in control group\n\n    ##Cases with negative (good) outcome\n    c = severity_numbers[0]     # Number in exposed group\n    d = severity_numbers[1] - c # Number in control group\n    \n    if(a == 0 or b <= 0 or c == 0 or d <=0): # cannot calculate division by 0 or sqrt < 0\n        return OR, lower_CI, higher_CI, CI, p_value\n    \n    \n    OR = (a*d)\/(b*c)\n    std_error  = np.sqrt(1\/a + 1\/b + 1\/c +1\/d) \n    \n    lower_CI = np.exp(np.log(OR)-1.96*std_error)\n    higher_CI = np.exp(np.log(OR)+1.96*std_error)\n    \n    CI = \"95% CI: {:.2f}-{:.2f}\".format(lower_CI, higher_CI)\n    \n    #P-value: \u03c6(z) = (1 \/ \u221a2\u03c0) \u00d7 e -z2\/2\n    z = np.log(OR) \/ std_error\n    \n    p_value = (1\/(np.sqrt(2*np.pi))) * np.exp(-(z**2)\/2)\n    \n    #return \"OR: {:.2f} ({:}) p={:.2f}\".format(OR, CI, p_value)\n    return OR, lower_CI, higher_CI, CI, p_value\n    ","dfc64a2a":"#Input: the dataframe you are looking for and the set of synonyms to search in the header\n#Output: the indexes where the non severe and severe cases are described, in this order.\ndef get_column_index_severities(df, severe_synonyms_set):\n    \n    synonyms_non = set([\"non\", \"non \",\"non-\",\"no \",\"dead\",\"death\",\"died\",\"3\",\"mort\"])\n\n    row = list(df.columns)\n    \n    severe_index = None\n    non_severe_index = None\n    \n    severe_value = \"\"\n    non_severe_value = \"\"\n\n    for i, r in enumerate(row):\n        r = str(r).lower().replace('\u2010', '-')\n        #print(\"r is\",r)\n        for sv in severe_synonyms_set:\n            if(sv in r):   \n                #print(\"Checking\",sv)\n                #check if its not severe\n                severe_value = sv\n                for n in synonyms_non:\n                    #print(\"Checking\",sv,\" and\",n)\n                    if(n+sv in r) or (n in r):\n                        non_severe_index = i\n                        non_severe_value = n\n                        #print(\"n\"+n)\n                        break\n                \n                if(not non_severe_index):\n                    severe_index = i\n\n                if(severe_index and non_severe_index):\n                    #print(\"Found index for \" + severe_value + \" and \" + non_severe_value)\n                    return [non_severe_index, severe_index]                \n                \n                break #not necessary to look for more synonyms on severe set\n    return [non_severe_index, severe_index]","c516a4d1":"def get_only_number(val):\n    if isinstance(val, str):\n        i = val.find(\"(\")\n        if(i > 0):\n            val = val[0:i]\n        i = val.find(\"\/\")\n        if(i > 0):\n            val = val[0:i]\n    try:\n        f = float(val)\n        return f\n    except:\n        return 0\n\ndef get_total_number(header):\n    \n    val = header[header.find(\"=\")+1:header.find(\")\")].strip() #pattern (n = 12)\n    if(val.isdigit()):\n        return int(val)\n    return 0\n    \n\ndef get_numbers_by_risk_factor(df, risk_factor_synonyms, header_synonyms):\n    # Remove Lucene's query wilcard * from synonyms and lowercase them all\n    synonyms = [risk_factor_synonym.lower().replace('*', '') for risk_factor_synonym in risk_factor_synonyms]\n    # Search for risk factor synonyms in row headers starting with the longest ones\n    synonyms.sort(key=lambda s: len(s), reverse=True)\n    non_severe_index, severe_index = get_column_index_severities(df, header_synonyms)\n    \n    if(not non_severe_index or not severe_index):\n        # print(\"Info: Severity not found on header\")\n        return\n        \n    # Search for a risk factor synonym\n    for synonym in synonyms:\n        row = df[df.iloc[:,0].str.lower().str.contains(synonym)].values\n        if len(row) > 0: # if found synonym, stop search\n            break\n    if len(row) == 0: # no synonym not found \n        # print(\"Info: Risk factor not found\")\n        return None\n    row = row[0]\n    \n    header = list(df.columns)\n\n    if non_severe_index:\n        non_severe = get_only_number(row[non_severe_index])\n        non_severe_n = get_total_number(header[non_severe_index])\n\n    if severe_index:\n        severe = get_only_number(row[severe_index])\n        severe_n = get_total_number(header[severe_index])\n\n    return [non_severe, non_severe_n, severe, severe_n]\n\n\ndef get_severity_numbers(df, risk_factor_synonyms):\n    severe_synonyms = set([\"severe\", \"critical\", \"icu\", \"picu\",\"poor\",\"mild\",\"ecmo\", \"mv\",\"gi\",\"corticosteroids\",\"2\"])\n    return get_numbers_by_risk_factor(df, risk_factor_synonyms, severe_synonyms)\n\ndef get_fatality_numbers(df, risk_factor_synonyms):\n    fatality_synonyms = set([\"surviv\"])\n    return get_numbers_by_risk_factor(df, risk_factor_synonyms, fatality_synonyms)","c495cc7f":"def extract_from_tables(tables_by_pmcid, risk_factor, risk_factor_synonyms, metadata):\n# Extracts from the paper tables the target datapoints for the given risk factor\n    result_df = pd.DataFrame(columns=['Risk','Date', 'Study','Study Link','Journal','Severe','Severe lower bound','Severe upper bound','Severe p-value','Severe Significant', 'Score', 'Excerpt'])\n    for pmcid, tables in tqdm(tables_by_pmcid.items(), desc=risk_factor + ': '):\n        paper_data_dict = {}\n        #get information from the metadata\n        paper_info = metadata[metadata.pmcid == pmcid][['title', 'publish_time', 'source_x', 'url', 'pmcid','journal']]\n        paper_info = paper_info.iloc[0]\n        for df in tables:\n            df.columns = df.iloc[0]\n            severe_numbers = get_severity_numbers(df, risk_factor_synonyms)\n         \n            if severe_numbers: \n                severe_OR, severe_lower_CI, severe_higher_CI, severe_CI, severe_p_value = get_statistical_info(severe_numbers)\n                if severe_OR:\n                    if (float(severe_p_value)<0.05):\n                        significant = \"Significant\"\n                    else:\n                        significant = \"Not Significant\"\n                    date = None\n                    if paper_info.publish_time:\n                        date = html.escape(paper_info.publish_time)\n                    study = None\n                    if paper_info.title:\n                        study = html.escape(paper_info.title)\n                    study_link = None\n                    if paper_info.url:\n                        study_link = '<a href=\"' + paper_info.url + '\">' + html.escape(paper_info.url) + '<\/a>'\n                    journal = None\n                    if paper_info.journal:\n                        journal = html.escape(str(paper_info.journal)),\n                    \n                    paper_data_dict = {\n                        'Risk': html.escape(risk_factor),\n                        'Date': date,\n                        'Study': study,\n                        'Study Link': study_link,\n                        'Journal': journal,\n                        'Severe': severe_OR,\n                        'Severe lower bound': severe_lower_CI,\n                        'Severe upper bound': severe_higher_CI,\n                        'Severe p-value': severe_p_value,\n                        'Severe Significant': significant,\n                        'Score': None, # Not used here, used in the grammar approach for the match score\n                        'Excerpt': None # Not used here, used in the grammar approach to show the matched text with context\n                    }\n                    result_df = result_df.append(paper_data_dict, ignore_index=True)\n                    break\n\n    return result_df","19b4b0b0":"%%capture --no-display\n# Generate target tables\ntables_by_risk_factor_part_1 = {}\nfor risk_factor, risk_factor_synonyms in tqdm(risk_factor_terms.items(), desc=\"Extracting: \"):\n    #print(\"\\nTable for risk factor {}\".format(risk_factor)\n    tables_by_risk_factor_part_1[risk_factor] = extract_from_tables(tables_by_pmcid, risk_factor, risk_factor_synonyms, metadata)","91a0cf2a":"%%capture --no-display\nfor risk_factor, table in tables_by_risk_factor_part_1.items():\n    display(HTML(table.to_html(escape=False)))","1d4ca6a9":"covid19_expression = make_or_term_query(covid19_synonyms)\nrisk_factor_queries = {risk_factor: make_and_query([make_or_term_query(synonyms), covid19_expression]) for risk_factor, synonyms in risk_factor_terms.items()}\npapers_by_risk_factor = {}\n\nfor risk_factor, query_expression in tqdm(risk_factor_queries.items(), desc='Searching: '):\n    results = search(searcher, analyzer, query_expression)\n    papers_by_risk_factor[risk_factor] = results\n    print(\"{}: {} documents found\".format(risk_factor, len(results.index)))","b6480e27":"Image(\"..\/input\/severe-grammar\/axiom_severe.png\")","97229f8d":"Image(\"..\/input\/severe-grammar\/null_insert.png\")","c3afecb7":"Image(\"..\/input\/severe-grammar\/penalizing_insert.png\")","bc4ef98a":"Image(\"..\/input\/severe-grammar\/risk_factor_alone_or_in_list.png\")","da898ba3":"Image(\"..\/input\/severe-grammar\/risk_factor.png\")","eeacbd2a":"Image(\"..\/input\/severe-grammar\/target_risk_factor.png\")","900ee4af":"Image(\"..\/input\/severe-grammar\/risk_factor_respiratory_system_disease.png\")","4c199f83":"Image(\"..\/input\/severe-grammar\/severe.png\")","e69fd691":"Image(\"..\/input\/severe-grammar\/metric.png\")","10ed5d41":"Image(\"..\/input\/severe-grammar\/gap.png\")","419cac83":"Image(\"..\/input\/severe-grammar\/comparison_operator.png\")","63c2eec9":"Image(\"..\/input\/severe-grammar\/value.png\")","4a45aff1":"Image(\"..\/input\/severe-grammar\/float_interval.png\")","5046456a":"Image(\"..\/input\/severe-grammar\/p_value.png\")","d13d3599":"grammar_dir = os.path.join('..', 'input', 'severe-grammar')\nbin_delaf_pathname = os.path.join(grammar_dir, 'dictionary.bin')\nsevere_grammar_pathname = os.path.join(grammar_dir, 'grammar_severe.fst2')\nsevere_grammar_engine = GrammarEngine(severe_grammar_pathname, bin_delaf_pathname)","be481aa8":"def grapenlp_results_to_python_dic(sentence, native_results):\n    top_segments = OrderedDict()\n    score = -sys.maxsize - 1 # Minimum integer value\n    if not native_results.empty():\n        top_native_result = native_results.get_elem_at(0)\n        score = top_native_result.w\n        top_native_result_segments = top_native_result.ssa\n        for i in range(0, top_native_result_segments.size()):\n            native_segment = top_native_result_segments.get_elem_at(i)\n            native_segment_label = native_segment.name\n            segment_label = u_out_bound_trie_string_to_string(native_segment_label)\n            segment = OrderedDict()\n            segment['value'] = sentence[native_segment.begin:native_segment.end]\n            segment['start'] = native_segment.begin\n            segment['end'] = native_segment.end\n            top_segments[segment_label] = segment\n    return top_segments, score","1278c43f":"def parse_segments(text, segments):\n    metric = ''\n    metric_and_value = None\n    lower_bound = None\n    upper_bound = None\n    p_expression = None\n    p_value = None\n    significant = None\n    excerpt = None\n\n    if 'HR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'HR'\n    elif 'OR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'OR'\n    elif 'RR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'RR'\n    elif 'AHR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'AHR'\n    elif 'AOR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'AOR'\n    elif 'ARR' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'ARR'\n    elif 'WDM' in segments:\n        if metric:\n            metric += '\/'\n        metric += 'WDM'\n\n    if metric and 'value' in segments:\n        metric_and_value = metric\n        if 'less' in segments:\n            metric_and_value += ' < '\n        elif 'less_or_equal' in segments:\n            metric_and_value += ' \u2264 '\n        elif 'greater' in segments:\n            metric_and_value += ' > '\n        elif 'greater_or_equal' in segments:\n            metric_and_value += ' \u2265 '\n        else:\n            metric_and_value += ' = '\n        metric_and_value += segments['value']['value'].replace('\u00b7', '.')            \n\n    if 'lower_bound' in segments:\n        lower_bound = float(segments['lower_bound']['value'].replace('\u00b7', '.'))\n    if 'upper_bound' in segments:\n        upper_bound = float(segments['upper_bound']['value'].replace('\u00b7', '.'))\n    if 'p' in segments:\n        p_expression = segments['p']['value']\n    if 'p_value' in segments:\n        p_value = float(segments['p_value']['value'].replace('\u00b7', '.'))\n        if (p_value <= 0.05):\n            significant = 'Significant'\n        else:\n            significant = 'Not Significant'\n\n    risk_factor = '<span style=\"' + RISK_FACTOR_STYLE +'\">' + html.escape(segments['risk_factor']['value']) + '<\/span>'\n    risk_factor_start = segments['risk_factor']['start']\n    risk_factor_end = segments['risk_factor']['end']\n\n    severe_start = segments['severe']['start']\n    severe_end = segments['severe']['end']\n\n    insert = ' '\n    if 'insert' in segments:\n        insert += html.escape(segments['insert']['value']) + ' '\n\n    # If risk factor inside severe:\n    if severe_start < risk_factor_start and risk_factor_end < severe_end:\n        left_context = html.escape(text[severe_start - CONTEXT_SIZE:severe_start])\n        right_context = html.escape(text[severe_end:severe_end + CONTEXT_SIZE])\n        left_severe = html.escape(text[severe_start:risk_factor_start])\n        right_severe = html.escape(text[risk_factor_end:severe_end])\n        excerpt = '<span style=\"' + SEVERE_STYLE + '\">' + left_severe + '<\/span>' + \\\n                  risk_factor + \\\n                  '<span style=\"' + SEVERE_STYLE + '\">' + right_severe + '<\/span>'\n    else:\n        severe = '<span style=\"' + SEVERE_STYLE + '\">' + segments['severe']['value'] + '<\/span>'\n        # If risk factor appeared before severe measure\n        if risk_factor_end < severe_start:\n            left_context = html.escape(text[risk_factor_start - CONTEXT_SIZE:risk_factor_start])\n            right_context = html.escape(text[severe_end:severe_end + CONTEXT_SIZE])\n            excerpt = left_context + risk_factor + insert + severe + right_context\n        # Else severe measure appeared before risk factor\n        else:\n            left_context = html.escape(text[severe_start - CONTEXT_SIZE:severe_start])\n            right_context = html.escape(text[risk_factor_end:risk_factor_end + CONTEXT_SIZE])\n            excerpt = left_context + severe + insert + risk_factor + right_context\n\n    return metric_and_value, lower_bound, upper_bound, p_expression, significant, excerpt","7688f661":"def extract_from_text(papers, risk_factor, grammar_engine):\n# Extracts from the paper full text the target datapoints for the given risk factor\n    result_df = pd.DataFrame(columns=['Risk','Date', 'Study','Study Link','Journal','Severe','Severe lower bound','Severe upper bound','Severe p-value','Severe Significant', 'Score', 'Excerpt'])\n    context = {'risk_factor': risk_factor.replace(' ', '_')}\n    matched_papers = 0\n    for id, paper in tqdm(papers.iterrows(), desc=risk_factor, total=len(papers.index)):\n        text = paper['paper_full_text']\n        native_matches = severe_grammar_engine.tag(text, context)\n        segments, score = grapenlp_results_to_python_dic(text, native_matches)\n        if segments:\n            matched_papers += 1\n            if score >= MIN_GRAMMAR_SCORE:\n                metric_and_value, lower_bound, upper_bound, p_expression, significant, excerpt = parse_segments(text, segments)\n                if metric_and_value:\n                    date = None\n                    if paper.date:\n                        date = html.escape(paper.date)\n                    study = None\n                    if paper.study:\n                        study = html.escape(paper.study)\n                    study_link = None\n                    if paper.study_link:\n                        study_link = '<a href=\"' + paper.study_link + '\">' + html.escape(paper.study_link) + '<\/a>'\n                    journal = None\n                    if paper.journal:\n                        journal = html.escape(str(paper.journal)),\n                    \n                    paper_data_dict = {\n                        'Risk': html.escape(risk_factor),\n                        'Date': date,\n                        'Study': study,\n                        'Study Link': study_link,\n                        'Journal': journal,\n                        'Severe': metric_and_value,\n                        'Severe lower bound': lower_bound,\n                        'Severe upper bound': upper_bound,\n                        'Severe p-value': p_expression,\n                        'Severe Significant': significant,\n                        'Score': score,\n                        'Excerpt': excerpt\n                    }\n                    result_df = result_df.append(paper_data_dict, ignore_index=True)\n    matches_above_threshold = len(result_df.index)\n    total_papers = len(papers.index)\n    total_papers_ratio = '?'\n    matched_papers_ratio = '?'\n    matches_above_threshold_ratio = '?'\n    if total_papers > 0:\n        total_papers_ratio = 100\n        matched_papers_ratio = round(100 * matched_papers \/ total_papers, 2)\n        matches_above_threshold_ratio = round(100 * matches_above_threshold \/ total_papers, 2)\n    display(HTML(\"<p>total papers\/total matches\/matches above threshold: {}\/{}\/{} ({}%\/{}%\/{}%)<\/p>\".format(total_papers, matched_papers, matches_above_threshold, total_papers_ratio, matched_papers_ratio, matches_above_threshold_ratio)))\n    return result_df","92e9cae9":"%%capture --no-display\n# For each risk factor\ntables_by_risk_factor_part_2 = {}\nfor risk_factor, papers in tqdm(papers_by_risk_factor.items(), desc='Extracting: '):\n    table = extract_from_text(papers, risk_factor, severe_grammar_engine)\n    tables_by_risk_factor_part_2[risk_factor] = table","75f6d454":"%%capture --no-display\nfor risk_factor, table in tables_by_risk_factor_part_2.items():\n    display(HTML(table.to_html(escape=False)))","34e79e3c":"%%capture --no-display\n# Merge tables of part 1 & 2\ntables_by_risk_factor_merged = {}\nfor risk_factor in tqdm(risk_factor_terms, desc='Merging: '):\n    table1 = tables_by_risk_factor_part_1[risk_factor]\n    table2 = tables_by_risk_factor_part_2[risk_factor]\n    table = pd.concat([table1, table2], ignore_index=True)\n    table.sort_values(by='Study', inplace=True)\n    tables_by_risk_factor_merged[risk_factor] = table\n    part1_count = len(table1.index)\n    part2_count = len(table2.index)\n    total_count = part1_count + part2_count\n    total_ratio = '?'\n    part1_ratio = '?'\n    part2_ratio = '?'\n    if total_count > 0:\n        total_ratio = 100\n        part1_ratio = round(100 * part1_count \/ total_count, 2)\n        part2_ratio = round(100 * part2_count \/ total_count, 2)\n    display(HTML('<p>{} result count from tables\/text\/total: {}\/{}\/{} ({}%\/{}%\/{}%)<\/p>'.format(risk_factor, part1_count, part2_count, total_count, total_ratio, part1_ratio, part2_ratio)))","dcc57e7b":"%%capture --no-display\n#Display and save tables\nfor risk_factor, table in tqdm(tables_by_risk_factor_merged.items(), desc='Saving'):\n    display(HTML(table.to_html(escape=False)))\n    table.to_csv(risk_factor + \".csv\")","a87a58c8":"We include in the *severe_grammar* dataset the full list of GRF grammar files, which can be viewed and edited with the Unitex Corpus Processing suite (available at https:\/\/unitexgramlab.org\/). Using this intuitive graphical user interface tool, users can reconfigure the grammars provided or add new grammars. For the efficient application of the grammar to a text, the GRF files are first converted into a single FST2 file (grammar_severe.fst2) that stores an equivalent and pseudo-minimized recursive transition network with output, a kind of push-down automaton. This conversion can be done with the Grf2Fst2 command included in Unitex. More information can be found on [this notebook](https:\/\/www.kaggle.com\/javiersastre\/grapenlp-grammar-engine-in-a-kaggle-notebook).","3cfbf52d":"The GrapeNLP grammar engine ingests both an input text and a set of key\/value pairs (the context variables), which can be checked in a grammar box by means of meta codes <@key=value> in order to only enable certain grammar paths.\n\nEach **risk_factor_XXX** defines the possible synonyms and expressions for each risk factor type, for instance grammar **risk_factor_respiratory_system_disease**:","d71130c7":"# Download Tables <a class=\"anchor\" id=\"download-tables-header\"><\/a>\n![](http:\/\/)\nThis section will retrieve tables from the publications that have a PMCID, as these publications store tabular data on PubMed. To avoid a timeout due to the long time required for downloading tables of thousands of papers, we have pre-downloaded the tables of more than 71K papers and saved them into the pmc-tables dataset. In case the tables of a given paper are not in the dataset, they will be downloaded from PubMed.","50fe3e78":"The **penalizing_insert** grammar extracts the matched text for subsequent generation of the publication excerpt for later inspection (note the use of output tag).\n\nThe **risk_factor_alone_or_in_list** grammar detects different terms relative to each risk factor category, either alone or in a list of risk factors:","b6dcb4f4":"Note the <E> meta code represents the empty string, which can be used to make a box optional. Note as well that some risk factor synonyms are shorter versions of others, such as cough and dry cough. Since lexical masks matching specific words or symbols are given 14 points by default, the longest match will be selected.\n    \nThe **severe** grammar detects the actual datapoints we want to extract, which are expressed in the papers in a variety of forms:","f9b7f0a8":"# Introduction <a class=\"anchor\" id=\"intro-header\"><\/a>\nThis notebook provides extensive search and information extraction functionality for risk factors related to Sars-CoV-2, Covid-19, including the extraction and generation of odds ratios, confidence intervals and relevant excerpts. It facilitates search queries using an Apache Lucene based search engine over an index compiled on the full text of publications in the Kaggle competition metadata. It performs table extraction from publications of interest, along with extensive text mining of unstructured text through a robust [GrapeNLP](https:\/\/github.com\/GrapeNLP) grammar with fuzzy matching capabilities.\n\nThe indexed metadata is searched using the Lucene search engine to uncover papers of interest and a PubMed online service is queried to find tabular data related to cohort and epidemiological studies. [PubMed](https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/) is a free search engine accessing primarily the MEDLINE database of references and abstracts on life sciences and biomedical topics. Tables are automatically downloaded and relevant information is extracted. Odds ratios and confidence intervals are computed, where the relevant data is available.\n\nA powerful grammar engine, [GrapeNLP](https:\/\/github.com\/GrapeNLP), is also used to rapidly and robustly search unstructured publication text, retrieving the various statistics related to the significance of numerous risk factors on the fatality and severity of Covid-19. Summary tables are generated and are exported as CSV files.\n\nIt has successfully generated numerous table entries for the following risk factors: \n\n* Hypertension\n* Diabetes \n* Male gender \n* Heart Disease \n* COPD \n* Smoking Status \n* Age \n* Cerebrovascular disease \n* Cardio- and cerebrovascular disease \n* Cancer \n* Respiratory system diseases \n* Chronic kidney disease \n* Chronic respiratory diseases \n* Drinking \n* Overweight or obese \n* Chronic liver disease \n\nThe techniques provided here would be of interest to researchers and policy makers seeking to automatically find answers to questions within, and connect insights across the dataset provided, to aid ongoing COVID-19 response efforts worldwide. These techniques can also be used by other teams to supplement their data analysis. \n\nThis submission is able to: (1) recreate the target tables; (2) create new summary tables; and\/or (3) append new rows to the old tables in order to add: (A) newly published articles; or (B) previously overlooked articles.\n\nIt will use the following workflow:\n* [Notebook Parameters](#parameters-header) - Set the parameters to apply to the entire notebook\n* [Install Libraries and Load Metadata](#install-header) - Install the necessary components and load the CORD-19 metadata.\n* [Load Lucene Index and Searcher](#load-lucene-header) - Load the CORD-19 Lucene index and instantiate the Lucene searcher\n* [Search for Publications with Relevant Tables](#search-tables-header) - Search for papers that may contain tables relevant to risk factors. To implement this component, we use PyLucene, which is a Python extension for accessing Java Lucene.\n* [Download Tables](#download-tables-header) Download XML tables of PubMed papers found.\n* [Compute Odds Ratios](#odds-header) Define functions to calculate Odds Ratios and Confidence Intervals from the data to extract.\n* [Extract Information from Tables](#extract-from-tables-header) Extract datapoints from the downloaded tables.\n* [Search for Publications with Relevant Text](#search-text-header) - Search for papers that may mention relevant information about risk factors in the full text. Again, we use PyLucene for accessing Java Lucene.\n* [Extraction Grammar Description](#grammar-description-header) - Overview of the GrapeNLP grammar used to find and extract the target datapoints.\n* [Load Grammar](#load-grammar-header) - Instantiation of the grammar engine.\n* [Extract Information from Text](#extract-from-text-header) - Extract datapoints from full text of papers.\n* [Merge and Display Results](#display-header) Aggregate results from table and text extraction approaches, save the target CSV tables and display them.\n* [Conclusion](#conclusion-header) Conclusions and future directions.\n\nThe image below shows sample output for target tables. \n\n![summary_table.png](attachment:summary_table.png)\n\nPlease feel free to reuse, cite contribution and up vote please.\n","3e7aed22":"Define utility functions to make Lucene query expressions and perform a search in Lucene. The latter method takes a search query and returns a set of documents ranked by their relevancy\/ similarity to the query. QueryParser provides a highly configurable hybrid form of search on search topics such as \u201cphrasal search\u201d , \u201cBoolean search\u201d , \u201cproximity search\u201d and etc. Also, search topics can be field specific because Lucene indexes terms as a composition of a field name and a token.","712c53cc":"# Notebook parameters<a class=\"anchor\" id=\"parameters-header\"><\/a>\n![](http:\/\/)\nWe set here the global parameters to be used accross the entire notebook.","26a1b27d":"# Merge and Display Results <a class=\"anchor\" id=\"display-header\"><\/a>\n![](http:\/\/)\nMerge the summary tables found for both the table- and text-based information extraction approaches, display the results and save findings in CSV files. To know which rows come from which approach, look at the 'Score' and 'Excerpt' columns: these have values when the result comes from the full publication text.","cb99521e":"# Conclusion <a class=\"anchor\" id=\"conclusion-header\"><\/a>\n\nWe set out to provide a reproducible pipeline to accurately search and extract valuable quantitative data from Covid-19 research studies in order to derive risk ratios, odds ratios, confidence intervals and p-values for relevant risk factors. We have demonstrated the ability to automatically extract numerous tables for all of the risk factors outlined in the Kaggle task description. This pipeline enables researchers, clinicians, policy makers and many others to rapidly mine tens of thousands of publications in order to find highly relevant quantitative data from studies related to Covid-19. This pipeline is fully automatic and is highly configurable and would have utility for many other aspects of Covid-19 research and indeed research into many other diseases.\n\nOur inspection of the result show that they are highly relevant and we can tune precision and recall by tuning the grammar threshold for accepting a match.\n\n### Pros and Cons of this Approach\n\n#### Pros\nOur approach provides the following features:\n* We combine high performance indexing and search features with both online web services and GrapeNLP grammar engine to extract quantitative data related to Covid-19.\n* The generated results are highly relevant.\n* We can tune precision and recall by tuning the grammar threshold for accepting a match.\n* We automate all of the steps needed to extract this crucial information in one reusable pipeline.\n* The end result is a set of risk ratios, odds ratios, confidence interval and p-values that provide statistical insight into Covid-19 risk factors.\n* The collection of data from multiple risk factor studies, can lead to a higher statistical power and more robust estimates than is possible from any individual study.\n* The system is highly configurable, allowing researchers to target other factors and issues.\n* The analysis is highly reproducible, allowing other researchers to replicate and reuse the pipeline.\n* Our approach is highly explainable unlike ML black box approaches\n* The system can be re-run as more data becomes available.\n    \n#### Cons \nThe following are the shortfalls of our approach:\n* We are pulling some tables automatically where they have been provided in XML. We can expand the technique to HTML and PDF tables in the future. However, we are also extracting the relevant information for the full text, which mitigates this issue.\n* The papers in the corpus could suffer from publication bias, which is a type of bias that occurs when publishers favour results that show significant findings, potentially allowing the results to be generalized to a larger population. We can address these issues in future work using statistical techniques such as meta-analysis funnel plots.\n\n### Future Work\nThe grammar provided is flexible and we have targeted risk factors related to severe outcomes, as our goal was to prove the approach we are using. In future work we can:\n* Isolate as well data-points specific to fatality and other relevant aspects.\n* Expand the technique to HTML and PDF tables and process more table types.\n* Add meta-analysis to improve insight to Covid-19 risk factors.\n* Add funnel plots to detect publication bias.\n* Inconsistency of results across studies can be quantified and analyzed.\n* Add sentence delimiter tags to the paper texts and modify the grammar so it favours matches of risk factors and severe metrics within the same sentence.\n* Further refinements of the grammar to improve precision and recall.\n* Add pre-downloaded tables to Lucene index for fast table retrieval.\n* Run several instances of the grammar engine concurrently to take advantage of multiple cores and reduce execution time.\n* Manually select true positives found by the grammar in order to build a training set for implementing a machine learning approach to information extraction","f8179a53":"# Extract Information from Text <a class=\"anchor\" id=\"extract-from-text-header\"><\/a>\n![](http:\/\/)\nWe apply here the loaded grammar to the full text of each paper found, for each risk factor defined in the first section of this notebook.","f85d7bc3":"Grammar **p_value** extracts the entire p-value expression (e.g. p < 0.05) as well as the value only in order to determine significance:","09bcc411":"# Extraction Grammar Description <a class=\"anchor\" id=\"grammar-description-header\"><\/a>\n![](http:\/\/)\nGrammars have greater expressive power compared to regular expressions, so we have developed a [GrapeNLP](https:\/\/github.com\/GrapeNLP) grammar for the extraction of severe measures that are within a maximum distance from a mention of a given risk factor synonym. This grammar performs a similar operation than a regular expression with extraction groups: it will try to match the entire text of each paper, then upon a match it will extract segments of the matched text that are bounded in the grammar by XML tags. The XML tags act as parenthesis in regular expressions for defining extraction groups, allowing later retrieval of the matched portions of text in each group, using the tag names instead of group indexes. We extract the following segments from the text:\n* the severity metric (OR, HR, RR, WDM, AOR, AHR or AOR)\n* confidence interval bounds, lower and upper\n* p-value\n* the original excerpt of text mentioning both the risk factor and severe measure, for manual reviewing\n\nGrapeNLP is an efficient grammar engine based on push-down automata with output, where the outputs are used both for generating the XML tags and for assigning scores to each matched token, depending on the specificity degree of the lexical mask used to match the token: by default, the most restrictive lexical masks (the ones matching a specific word or symbol only) get 14 points, while the *<TOKEN>* mask, which matches any token, gets 0 points. In case the entire text can be matched through different paths in the grammar, the grammar engine efficiently selects the one with the greatest overall score using a Viterbi-like algorithm in order to later extract the corresponding segments.\n    \nThe grammars can be structured into reusable components (equivalent to non-terminal symbols of context-free grammars) as illustrated by the grammar axiom:","20e226d9":"The **comparison_operator** grammar generates empty XML tags to identify the comparison operator used, regardless of the expression used:","c89ddae3":"# Search for Publications with Relevant Text <a class=\"anchor\" id=\"search-text-header\"><\/a>\n![](http:\/\/)\nWe reuse the previously loaded Lucene index and search engine to find papers whose text may contain the relevant information extract for each risk factor and its corresponding synonyms, which are defined in this notebook's parameters section.","468b22b3":"This grammar matches 0, 1 or more arbitrary tokens, giving 0 points to each match. To match the arbitrary text that may appear between the risk factor mention and the severity measure, we use the **penalizing_insert** grammar, which overrides the default 0 point score in order to penalize longer inserts and consequently favor the closest *risk factor*\/*severe* pair:","6fadc837":"# Extract Information from Tables <a class=\"anchor\" id=\"manual-header\"><\/a>\n![](http:\/\/)\nWe are relying on the search above to identify the most relevant tables.\nThis section will get information from the extracted tables related to a given risk factor. \n\n** However, this works best for a fixed set of consistent table styles.** The system can be updated in the future to accommodate more diverse tables.","8edab1c2":"The grammar engine is implemented in C++, and the corresponding native match object accessed from Python using SWIG. We use the following function to convert this native object into a Python dictionary which is easier to process. Upon multiple matches of the same expression, we retrieve the top scored match. The grammar not only matches expressions but tags the fragmets to extract (what we call \"segments\" of the input text) For the top scored match, this method returns a Python dictionary for all the tagged text segments, using as key the label of the tag and as value the text segment itself. The total match score is also returned in order to be able to enforce a minimum score.","55bb485e":"# Search for Publications with Relevant Tables <a class=\"anchor\" id=\"search-tables-header\"><\/a>\n![](http:\/\/)\nThe following section searches for publications that may contain tables relevant to risk factors, based on Apache Lucene.","9f22d63d":"Instantiate the Lucene objects needed to perform searches","032e65cc":"# Load GrapeNLP grammar <a class=\"anchor\" id=\"load-grammar-header\"><\/a>\n![](http:\/\/)","c1f51c4b":"To identify the provided metric independently of the form used (e.g. \"OR\" or \"odds ratio\"), we consistently generate the same empty tag for each set of metric synonyms:","98acdbdb":"# Compute Odds Ratio <a class=\"anchor\" id=\"odds-header\"><\/a>\n![](http:\/\/)\nAs we are analysing count data, such as the number of severe outcomes versus the number of non-severe outcomes, we will compute the summary statistics using an odds ratio. \nThe term \u201codds\u201d refers to the probability of an event occurring versus the probability of the event not occurring.\n\nThese can be illustrated with the following example, based on [Ranganathan](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC4640017):\n\n| | Fatal | Survive | Total |\n| --- | --- | --- | --- |\n| A | 18 | 46 | 64 |\n| B | 29 | 36 | 65 |\n| Totals | 47 | 82 | 129 |\n\nOdds is defined as:\n\n$$Odds = \\frac{Fatal}{Survive}$$\n\nThe odds of death in group A is 18\/46 (0.39) and in group B it is 29\/36 (0.81).\n\nThe odds *ratio* (OR) is the ratio of the odds of an event in group A to the odds of an event in group B. \n\nOur Covid-19 data is mainly from retrospective cohort studies, which are studies that examine existing data to identify risk factors for particular conditions, therefore we will utilise  odds  ratio.","4856ef0d":"# Load Lucene Index and Searcher<a class=\"anchor\" id=\"load-lucene-header\"><\/a>\n![](http:\/\/)\nLoad the Lucene index using IndexWriter. This component manages an index over a dynamic collection of documents and provides very rapid updates to the index as documents are added and deleted from the collection. This index provides a mapping from terms to documents, which is called an \u201cinverted index. Document indexing consists of first constructing a Lucene Document that contains the fields to be indexed, then adding that Document to the inverted index\u201d, see figure below. The index is maintained as a set of segments in a storage abstraction called SimpleFSDirectory which provides an interface similar to an OS file system.\n\n![index.png](attachment:index.png)\n\nIn this kernel, we define each paper as a \u201cDocument\u201d and the corresponding metadata of the paper as \u201cFields\u201d of the Document. Also, we store the frequency of terms as well as their position and offsets in document and this option gives us an opportunity to do more complicated and flexible querying such as \u201cphrasal querying\u201d.","fc1f854d":"Download the tables of all PMCID papers \/ load them from the cache in the pmc-tables dataset when available","8e7a8203":"Grammars **value** and **float_interval** are used to extract the measure value and confidence interval:","b28e2c17":"Given a list of papers, a risk factor and a grammar engine instance, this method returns a DataFrame for the target summary tables containing an entry per paper, where the severe metric was found close to a mention of the risk factor. The table includes the target datapoints, choosing the severe measures that are closest to the corresponding risk factor mention, along with the excerpt of text containing both the risk factor mention and severity measures. Including the relevant excerpt in the table facilitates human inspection of the output.","f7189ea0":"The following function interprets the XML tags found in the grammar output in order to return the severe metric, value, confidence interval bounds and p-value.","9fca26d9":"This grammar looks for pairs of mentions *risk factor*\/*severe measure* that are close enough. There are 3 possible combinations: 1) the risk factor precedes the severe measure, 2) the risk factor is embedded inside the severe measure (e.g. OR 3.4 59 years olds; 95% CI 1.4-9.5), or the risk factor appears after the severe measure. Since the grammar performs exact match of the entire paper, it uses the *null_insert* grammar to match the text that may appear before and after the text we are looking for, but only extracts the relevant datapoints.","568d72e4":"Between the mention of the metric and its value we allow an arbitrary amount of words, numbers and punctuation symbols, but not a full stop. Each aditional token in this gap is penalized with -15 points to priorize the smallest **gap**:","30d358ae":"It makes use of 2 other risk factor grammars, **risk_factor** and **target_risk_factor**, the difference being that **risk_factor** accepts any risk factor while **target_risk_factor** accepts only the type of risk factor we are looking for (e.g. age only, or cancer only, etc.). In case there is a list of risk factors, the entire segment of text from the first to the last target risk factors will be captured. Note in the previous section we have compiled lists of papers for each one of the considered risk factors. We apply the same grammar to all the papers, but for each risk factor we use a grammar context variable to specify which type of risk factor the **target_risk_factor** grammar should look for:","7e5bd806":"# Install Libraries and Load Metadata<a class=\"anchor\" id=\"install-header\"><\/a>\n![](http:\/\/)\nA number of external components are used, including Apache Lucene with pre-compiled indexes and the GrapeNLP grammar engine for information extraction. In this kernel, we provide an installation\/ configuration\/ compilation package for PyLucene 8.1.1 as an external data called \u201ccompiledlucene\u201d, which provides all the required software dependencies for installation and deployment of PyLucene. For the installation of GrapeNLP, we reuse the libgrapenlp dataset, which provides the required Debian packages, then we install from Pypi the GrapeNLP Python interface package: pygrapenlp. More details on how to install and use the GrapeNLP grammar engine in a Kaggle notebook can be found here: https:\/\/www.kaggle.com\/javiersastre\/grapenlp-grammar-engine-in-a-kaggle-notebook","b0f2291f":"[](http:\/\/)Filtering articles that have a PMCID, as these publications store tabular data on PubMed.\n"}}