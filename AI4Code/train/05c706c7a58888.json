{"cell_type":{"e4d23822":"code","b7eab121":"code","72c05f5e":"code","60d27583":"code","c75181a0":"code","1fb9b9cb":"code","640c5b7c":"code","3737faf2":"code","928ed16c":"code","5d035764":"code","65b47488":"code","4ac51682":"code","0fef8ceb":"code","db3b3e6b":"code","bf5c516c":"code","b02086c8":"code","3dc9006f":"code","8c89f624":"code","b4cf6832":"code","d5a74bd1":"code","f6ddce35":"code","8f6d5172":"code","414ffb38":"code","b0319439":"code","880636aa":"code","f0d32e8d":"code","63ea9d4b":"code","b04873af":"code","ce36bd25":"code","6c200bc4":"code","7bca3073":"code","62f0cc4c":"code","8295297e":"code","23efc418":"code","6e4a0375":"code","28de1c87":"code","cdee5204":"code","a6a688a5":"code","df7d602a":"markdown","c7574588":"markdown","b0ff99a5":"markdown","e30e8d0c":"markdown","5f15c766":"markdown","77cb3443":"markdown","47da7af5":"markdown","0dff831e":"markdown","34471700":"markdown","088e2d0e":"markdown","29cfec7d":"markdown","556e8735":"markdown","33382c46":"markdown","fb591944":"markdown","f211b705":"markdown","e443cfe8":"markdown","116c4da4":"markdown","7c624083":"markdown","e9379efa":"markdown","7e61f9ad":"markdown","9b03c12e":"markdown","6f856448":"markdown","5a884e6d":"markdown","37e755f9":"markdown","d875a233":"markdown","72f5f97c":"markdown","2c225105":"markdown","252e2a50":"markdown","6a9b87b6":"markdown","102a7f2a":"markdown","b3b908ba":"markdown","d8ac42ee":"markdown","9a855d9b":"markdown","ec4c6da8":"markdown","f83529a8":"markdown","33676535":"markdown","662cfa7f":"markdown","3eb77521":"markdown"},"source":{"e4d23822":"\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom scipy.stats import norm\nfrom skopt import gp_minimize,space\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom skopt.utils import use_named_args\nfrom hyperopt import hp,Trials,tpe,fmin\nfrom hyperopt.pyll.base import scope\nfrom hyperopt.plotting import main_plot_history\nimport optuna\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nfrom scipy.stats import skew\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import KFold,cross_val_score\nfrom sklearn.preprocessing import PolynomialFeatures","b7eab121":"train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","72c05f5e":"print(\"Dimensions of train set are:-\",train.shape[0],\",\",train.shape[1])\nprint(\"Dimensions of test set are:-\",test.shape[0],\",\",test.shape[1])","60d27583":"pd.set_option(\"display.max_columns\",81)","c75181a0":"train.head()","1fb9b9cb":"corr = train.corr()","640c5b7c":"corr[\"SalePrice\"].sort_values(ascending=False)","3737faf2":"x=corr[\"SalePrice\"].sort_values(ascending=False)[:11].index\nmask = np.zeros_like(corr.loc[x,x])\nmask[np.triu_indices_from(mask)]=True\n\nsns.heatmap(corr.loc[x,x],mask=mask,annot=True,cmap=\"coolwarm\")","928ed16c":"sns.pairplot(train[x[0:6]],x_vars=list(x[1:6]),y_vars=[x[0]],diag_kind=\"kde\")","5d035764":"f,ax = plt.subplots(1,2,figsize=(15,8))\nsns.boxplot(\"OverallQual\",\"SalePrice\",data=train,ax=ax[0])\nsns.boxplot(\"GarageCars\",\"SalePrice\",data=train,ax=ax[1])","65b47488":"index = len(train)\ny=train[\"SalePrice\"]\ntrain.drop(\"SalePrice\",axis=1,inplace=True)\ndataset = pd.concat([train,test]).reset_index(drop=True)","4ac51682":"dataset.isnull().sum()[dataset.isnull().sum()>0].sort_values(ascending=False)","0fef8ceb":"df_mszon=dataset.groupby([\"Neighborhood\",\"MSZoning\"])[\"Id\"].count().reset_index().groupby([\"Neighborhood\"])\nmaximum_mszon = df_mszon.max()\nmaximum_mszon= maximum_mszon.drop(\"Id\",axis=1)\n\nmax_dict_mszon = maximum_mszon.to_dict()\ndef mapper_mszon(x):\n    for index,val in zip(max_dict_mszon,max_dict_mszon.values()):\n        for index1,val1 in val.items():\n            if(x==index1):\n                return val1\n\ndataset.loc[dataset[\"MSZoning\"].isnull(),\"MSZoning\"] = dataset.loc[dataset[\"MSZoning\"].isnull(),\"Neighborhood\"].apply(lambda x: mapper_mszon(x))","db3b3e6b":"df_lotf=dataset.groupby([\"Neighborhood\",\"LotConfig\"])[\"LotFrontage\"].mean()\n\nmax_dict_lotf = df_lotf.to_dict()\ndef mapper_lotf(x1,x2):\n    for index,val in zip(max_dict_lotf,max_dict_lotf.values()):\n        if((x1==index[0]) & (x2==index[1])):\n            return val\n\ndataset.loc[dataset[\"LotFrontage\"].isnull(),\"LotFrontage\"] = dataset.loc[dataset[\"LotFrontage\"].isnull(),[\"Neighborhood\",\"LotConfig\"]].apply(lambda x: mapper_lotf(x[0],x[1]),axis=1)\ndataset[\"LotFrontage\"] = dataset.groupby([\"Neighborhood\"])[\"LotFrontage\"].transform(lambda x: x.median())","bf5c516c":"dataset[\"Alley\"].fillna(\"NA\",inplace=True)\ndataset[\"Utilities\"].fillna(\"NA\",inplace=True)\ndataset[\"Exterior1st\"].fillna(\"NA\",inplace=True)\ndataset[\"Exterior2nd\"].fillna(\"NA\",inplace=True)\ndataset[\"MasVnrType\"].fillna(\"NA\",inplace=True)\ndataset[\"MasVnrArea\"].fillna(0,inplace=True)\ndataset[\"BsmtQual\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtCond\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtExposure\"].fillna(\"No\",inplace=True)\ndataset[\"BsmtFinType1\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtFinSF1\"].fillna(0,inplace=True)\ndataset[\"BsmtFinType2\"].fillna(\"NA\",inplace=True)\ndataset[\"BsmtFinSF2\"].fillna(0,inplace=True)\ndataset[\"BsmtUnfSF\"].fillna(0,inplace=True)\ndataset[\"TotalBsmtSF\"].fillna(0,inplace=True)\ndataset[\"BsmtFullBath\"].fillna(0,inplace=True)\ndataset[\"BsmtHalfBath\"].fillna(0,inplace=True)\ndataset[\"FireplaceQu\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageType\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageYrBlt\"].fillna(0,inplace=True)\ndataset[\"GarageFinish\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageCars\"].fillna(0,inplace=True)\ndataset[\"GarageArea\"].fillna(0,inplace=True)\ndataset[\"GarageQual\"].fillna(\"NA\",inplace=True)\ndataset[\"GarageCond\"].fillna(\"NA\",inplace=True)\ndataset[\"PoolQC\"].fillna(\"NA\",inplace=True)\ndataset[\"Fence\"].fillna(\"NA\",inplace=True)\ndataset[\"MiscFeature\"].fillna(\"NA\",inplace=True)","b02086c8":"dataset[\"Functional\"].fillna(\"Typ\",inplace=True)\ndataset[\"Electrical\"].fillna(dataset[\"Electrical\"].mode()[0],inplace=True)\ndataset[\"KitchenQual\"].fillna(dataset[\"KitchenQual\"].mode()[0],inplace=True)\ndataset[\"SaleType\"].fillna(\"Oth\",inplace=True)","3dc9006f":"dataset[\"MSSubClass\"]=dataset[\"MSSubClass\"].astype(\"category\")","8c89f624":"x=sns.distplot(y)\nx.set_title(\"Distribution plot for Sale Price\")","b4cf6832":"x=sns.distplot(np.log1p(y),fit=norm)\nx.set_title(\"Distribution plot for Sale Price\")","d5a74bd1":"y= np.log1p(y)","f6ddce35":"dataset = dataset.replace({\"Alley\" : {\"Grvl\" : 1, \"Pave\" : 2,\"NA\":0},\n                       \"BsmtCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"FireplaceQu\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"NA\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageFinish\" : {\"Fin\" : 3, \"RFn\" : 2, \"Unf\" : 1, \"NA\" : 0},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \"PoolQC\" : {\"NA\" : 0, \"Fa\" : 1, \"TA\" : 2, \"Gd\" : 3, \"Ex\" : 4},\n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2, \"NA\":0},\n                       \"Utilities\" : {\"ELO\" : 1, \"NASeWa\" : 2, \"NASewr\" : 3, \"AllPub\" : 4}}\n                     )","8f6d5172":"cats = [\"basement_flag\",\"fire_flag\",\"wooddeck_flag\",\"garage_flag\",\"pool_flag\",\"fence_flag\"]\ndataset[\"basement_flag\"]=np.where(dataset[\"TotalBsmtSF\"]>0,1,0)\ndataset[\"fire_flag\"]=np.where(dataset[\"Fireplaces\"]>0,1,0)\ndataset[\"wooddeck_flag\"]=np.where(dataset[\"WoodDeckSF\"]>0,1,0)\ndataset[\"garage_flag\"]=np.where(dataset[\"GarageArea\"]>0,1,0)\ndataset[\"porch_flag\"]=np.where((dataset[\"OpenPorchSF\"]+dataset[\"EnclosedPorch\"]+dataset[\"3SsnPorch\"]\\\n                                +dataset[\"ScreenPorch\"])>0,1,0)\ndataset[\"pool_flag\"] = np.where(dataset[\"PoolArea\"]>0,1,0)\ndataset[\"fence_flag\"] = np.where(dataset[\"Fence\"]==\"NA\",1,0)\ndataset[cats]=dataset[cats].astype(\"category\")","414ffb38":"dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']","b0319439":"outlier_features = [\"GrLivArea\",\"TotalSF\",\"GarageArea\"]\n\nmeans,sds = np.mean(dataset[outlier_features]),np.std(dataset[outlier_features])\nlower,upper = means - 2.5*sds , means + 2.5*sds\ndef compare(x):\n    count=0\n    for col in outlier_features:\n        if x[col]>lower[col] and x[col]<upper[col]:\n            count=count+1\n    if count==len(outlier_features):\n        return True\n    else: \n        return False\n\ny= y.array\nsub_data = dataset.loc[:index-1]  \ntest_1 = dataset.loc[index:,:]\n\ndata_train_new = sub_data.loc[sub_data.apply(lambda x: compare(x),axis=1)]\ny= y[sub_data.apply(lambda x: compare(x),axis=1)]\n\nindex = len(data_train_new)\ndataset = pd.concat([data_train_new,test_1]).reset_index(drop=True)","880636aa":"numeric_feats = dataset.dtypes[(dataset.dtypes != \"object\") & (dataset.dtypes != \"category\")].index\nskewed_feats = dataset[numeric_feats].apply(lambda x: skew(x)).sort_values(ascending=False)\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness = skewness[abs(skewness) > 0.5]\n\nskewed_features = skewness.index\ndataset[skewed_features] = np.log1p(dataset[skewed_features])","f0d32e8d":"dataset.drop(\"Id\",axis=1,inplace=True)","63ea9d4b":"dataset = pd.get_dummies(dataset, drop_first= True)\ntrain_1 = dataset.loc[:index-1,:]\ntest_1 = dataset.loc[index:,:]","b04873af":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(train_1,y,test_size=0.2, random_state=42)","ce36bd25":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX_train= rs.fit_transform(X_train)\nX_test = rs.transform(X_test)\nX_submit = rs.transform(test_1.values)","6c200bc4":"param_space_hopt = {\n    \"max_depth\":scope.int(hp.quniform(\"max_depth\",3,10,1)),\n              \"n_estimators\":scope.int(hp.quniform(\"n_estimators\",50,500,1)),\n               \"criterion\":hp.choice(\"criterion\",[\"mse\"]),\n               \"max_features\":hp.uniform(\"max_features\",0.1,1),\n               \"min_samples_leaf\":scope.int(hp.quniform(\"min_samples_leaf\",2,10,1))\n              }\n\n\ndef objective_hopt(params_hopt):\n    model_hopt = RandomForestRegressor(**params_hopt)\n    skf = KFold(n_splits=5,random_state=42)\n    scores = -np.mean(cross_val_score(model_hopt,X_train,y_train,cv=skf,scoring=\"neg_mean_squared_error\"))\n    return scores\n\ntrial_hopt = Trials()\nhyopt = fmin(fn=objective_hopt,space = param_space_hopt, algo=tpe.suggest,max_evals=25,trials=trial_hopt) ","7bca3073":"hyopt","62f0cc4c":"main_plot_history(trial_hopt)","8295297e":"model_hopt =RandomForestRegressor(n_estimators= int(hyopt[\"n_estimators\"]),criterion=\"mse\",max_depth=int(hyopt[\"max_depth\"]),min_samples_leaf=int(hyopt[\"min_samples_leaf\"]),max_features=hyopt[\"max_features\"],random_state=42)\nmodel_hopt.fit(X_train,y_train)\ny_pred_hyopt = model_hopt.predict(X_test)\nhyopt_score = np.sqrt(mean_squared_error(y_test,y_pred_hyopt))\nhyopt_score","23efc418":"def optimization_optuna(trial_optuna):\n    \n    n_estimators = trial_optuna.suggest_int(\"n_estimators\",50,1000)\n    max_depth = trial_optuna.suggest_int(\"max_depth\",3,10)\n    criterion = trial_optuna.suggest_categorical(\"criterion\",[\"mse\"])\n    min_samples_split = trial_optuna.suggest_int(\"min_samples_leaf\",2,10)\n    max_features = trial_optuna.suggest_uniform(\"max_features\",0.1,1)\n    \n\n    model_optuna = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth,criterion=criterion,\n                                         min_samples_split=min_samples_split,max_features=max_features)\n    skf = KFold(n_splits=5)\n    score = cross_val_score(model_optuna,X_train,y_train,cv=skf,scoring=\"neg_mean_squared_error\")\n    return -np.mean(score)","6e4a0375":"study = optuna.create_study(direction=\"minimize\")\nresult = study.optimize(optimization_optuna,n_trials=25)","28de1c87":"study.best_params","cdee5204":"model_optuna =RandomForestRegressor(n_estimators= study.best_params[\"n_estimators\"],criterion=study.best_params[\"criterion\"],max_depth=study.best_params[\"max_depth\"],min_samples_leaf=study.best_params[\"min_samples_leaf\"],max_features=study.best_params[\"max_features\"],random_state=42)\nmodel_optuna.fit(X_train,y_train)\ny_pred_optuna = model_optuna.predict(X_test)\noptuna_score = np.sqrt(mean_squared_error(y_test,y_pred_optuna))\noptuna_score","a6a688a5":"optuna.visualization.plot_optimization_history(study)","df7d602a":"1. Exploratory Data Analysis\nTo get a sense of which features are important and from where do we start we can create a correlation matrix. By default it uses the pearson correlation coefficent.","c7574588":"# Automated Hyperparameter Tuning & EDA\n\n### I am sure all of us at some point of time have found it difficult to tune the hyperparamters of the model to our liking. Grid search is effective but can take a long time especially if we are low on processing power.\n\n### In my opinion once your data set is clean and all ready to be used, hyper parameter tuning is the next critical step when you are trying out different models.\n\nIn this notebook we would see two techniques which help us get around this issue:-\n### 1. Hyperopt\n### 2. Optuna\n\n![](https:\/\/www.oreilly.com\/library\/view\/hands-on-q-learning-with\/9781789345803\/assets\/f704ea35-39ed-47f6-be62-815e893ee5fd.png)\n[Image Source](https:\/\/www.oreilly.com\/library\/view\/hands-on-q-learning-with\/9781789345803\/480edc86-3794-406f-93c0-869866b5f5d3.xhtml\n)\n\nDocumentation for the libraries are below:-\n\nhttp:\/\/hyperopt.github.io\/hyperopt\/\n\nhttps:\/\/optuna.readthedocs.io\/en\/stable\/\n\nScikit Optimize is also a useful technique but I have not been able to cover it here due to some compatibility issue.\n\nhttps:\/\/scikit-optimize.github.io\/stable\/auto_examples\/hyperparameter-optimization.html\n\nHere I would build on one of my earlier notebooks and we will apply these techniques while modelling.\n","b0ff99a5":"Let us log transform it and recheck. Post log transform it is close to a normal distribution.","e30e8d0c":"In optuna we can give the direction in which we evaluate the objective function. Earlier we used -ve since those objective functions evaluated for minimizing.\n\nHere we can define the direction and we choose maximize since it we use accuracy score. We haven't negated the score in the objective function.","5f15c766":"# Optuna Hyperparamter Tuning\nWe define the objective function below.","77cb3443":"Let us start of by importing all the necessary packages.","47da7af5":"We have created flags for different feature of the house to check if it is available or not.","0dff831e":"We one hot encode the dataset and then split it to the orginal train set (with outliers removed) and the test set(all records intact).","34471700":"For features that can't be missing we have taken the mode and the default value as per data description.","088e2d0e":"Let's check the best parameters.","29cfec7d":"Since the SubClass are categories and not of numeric data type we covert the feature to category type.","556e8735":"Let us check the distribution of our dependent variable. It doesnt seem to be a normal distribution and leans towards a positive skewed distribution.","33382c46":"We log transform SalePrice as we have seen earlier it helps in getting a normal distribution.","fb591944":"Now we can zoom in to the 10 most important variables according to the pearson correlation coefficient and check the matrix.","f211b705":"Let's expand our column view since we have 81 columns in the train set.","e443cfe8":"We import both the train and test sets into different dataframes","116c4da4":"3. Feature Engineering\nWe have seen quite a few of the non numerical features have ordinal nature. We have transformed them below to numeric type to maintain the ordinal relation.","7c624083":"OverallQual and GarageCards have a positive correlation with the Sale Price. Price for houses with 4 garages are lower which seems to be an anomaly or there might be some other feature which is impacting the . We have only 5 records with 4 Garage cars which can cause such cases.","e9379efa":"We visualize the movement of scores according to the calls to the objective functions.","7e61f9ad":"Let us pair the 5 most important variables according to our matrix with sale price.\\n We can see some outliers, which we will take care of later.\n\nThe OverallQual and GarageCars plot with Sale Price will be better respresented with box plots, since they are ordinal features.","9b03c12e":"We should check the dimensions of these dataframes before proceeding. We have 1460 and 1459 rows in the train and test set respectively. We have 80 features in both the train and test set. The extra column in the train set would be the prediction.","6f856448":"We impute the missing values for zone using neighborhood as an indicator.","5a884e6d":"2. Handling missing values\nChecking for null values in our new combined dataset.","37e755f9":"Since we have a high number of variables and is prone to have outliers as we worked only on a few features we should opt for Robust Scaler transformation to handle the outliers. Below is a good read on different scaling methods:- https:\/\/scikit-learn.org\/stable\/auto_examples\/preprocessing\/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py","d875a233":"We are dropping the Id feature since it would not add any useful information to the model.","72f5f97c":"Let us now split out train set further in to train and test sets for validatio","2c225105":"Similarly we impute the values for LotFrontage using Neighborhood and LotConfig as indicators. We taken the median of similar Neighborhood and LotFrontage values.","252e2a50":"Let's check the best parameters","6a9b87b6":"We evaluate the best parameters on the test dat","102a7f2a":"Having a peek at the train set we can see all features along with the value we have to predict - SalePrice","b3b908ba":"Now it is time to combine our train and test sets since we need to preprocess it the same way so that we can feed it later into our model.\n\nAlso we would drop the SalePrice column before merging and copy it to another series - y.","d8ac42ee":"We plot the scores against the calls to the objective function.","9a855d9b":"Having a look at the correlation of different numerical feature with SalePrice.","ec4c6da8":"We create a TotalSF feature which includes the surface area for basement, 1st floor and the 2nd floor.","f83529a8":"4. Outlier Handling\nOutlier handling is done using the mean and the standard deviation. We have taken three features which have highest correlation to the sale price based on which outliers have been removed.","33676535":"# Hyperopt Hyperparameter Tuning\nBelow we defind the parameter space and the objective function.","662cfa7f":"For all the numeric features we have checked for skew and have log transformed those features which have high skew (greater than 0.5 in our case) to get a normal distribution.","3eb77521":"Base on the definition of these features give in the data description we impute the rest of the values to NA or 0 which signifies absence of that feature."}}