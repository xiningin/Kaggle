{"cell_type":{"9e6b8f56":"code","f0ed4a7a":"code","38b6274c":"code","b4ced5c6":"code","1c9e061d":"code","bdccf8d1":"code","1d7abcb2":"code","28a45028":"code","dddb4621":"code","c2e6590c":"code","026beec8":"code","f47cac89":"code","da31d82e":"code","5dbdbe37":"code","ea92d6bc":"code","960ee9c4":"code","3fe7e39b":"code","933e2c06":"code","d17f737b":"code","79ab944b":"code","7fa4abbd":"code","7680403e":"code","165df67c":"code","fa5cc2a8":"code","1b676614":"code","6608340a":"code","718a9f1e":"code","1a30cf53":"code","2cb78c2a":"markdown","c82d5f58":"markdown","3a2c891c":"markdown","5864c4e3":"markdown","1dd98f64":"markdown","5375bdf5":"markdown","00dd7265":"markdown","18bb68f7":"markdown","430d85b7":"markdown","0972ca95":"markdown","5cb556b8":"markdown","0fd35171":"markdown","df334a6d":"markdown","5e3e6f61":"markdown","1a8df9b0":"markdown","cee04f8f":"markdown","717bdbdc":"markdown","76968b0a":"markdown","784088ca":"markdown","6abbf3c0":"markdown","18210a10":"markdown","ef33d37c":"markdown"},"source":{"9e6b8f56":"#Main Library Imports \nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nplt.style.use('ggplot')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f0ed4a7a":"#import data and show first 5 rows *units are kg\/m^3, days, and MPa*\nconcCompStrength = pd.read_csv('\/kaggle\/input\/concrete-compressive-strength\/Concrete Compressive Strength.csv')\n#rename the columns to access them easier \nconcCompStrength.columns = ['cement','slag','fly ash','water','superplasticizer','coarse agg','fine agg','age','strength']\n#add a column for W\/C ratio \nconcCompStrength.insert(7,'wc ratio',concCompStrength['water'] \/ concCompStrength['cement'])\nconcCompStrength.head()","38b6274c":"#number of rows and columns\noriginalRows = concCompStrength.shape[0]\nconcCompStrength.shape","b4ced5c6":"#check if any values are null\nconcCompStrength.isnull().any()","1c9e061d":"#check the descriptive statistics\nconcCompStrength.describe()","bdccf8d1":"#remove rows with age less than 28 days \nconcCompStrength = concCompStrength.drop(concCompStrength[concCompStrength.age < 28].index)\nconcCompStrength.head(10)","1d7abcb2":"#check the descriptive statistics\nconcCompStrength.describe()","28a45028":"#create boxplot to visualize outliers\nplt.figure(figsize = (15,10))\nbox = sns.boxplot(data=concCompStrength)","dddb4621":"#remove outliers defined as values greater than 3 STD from the mean\nz_scores = stats.zscore(concCompStrength) #z-score = (value-mean)\/STD\n\nabs_z_scores = np.abs(z_scores)\nfiltered_entries = (abs_z_scores < 3).all(axis=1)\nconcCompStrength = concCompStrength[filtered_entries]\n\nconcCompStrength.head(10)","c2e6590c":"#check the descriptive statistics\nconcCompStrength.describe()","026beec8":"#define the weak mixes are less than 15MPa strength (usually these are for non-structural purposes)\n#is there any feature that makes a mix weaker? \nweakVal = 15\nweakMixes = concCompStrength[concCompStrength.strength < weakVal]\nnormalMixes = concCompStrength[concCompStrength.strength >= weakVal]\nweakOrNorm = np.where(concCompStrength.strength < weakVal,'Weak','Normal')\nfrom collections import Counter\ncounts = Counter(weakOrNorm)\nplt.bar(counts.keys(),counts.values())\nprint(counts)","f47cac89":"#visualize all weak mix data versus strength\nplt.figure(figsize = (15, 15))\nfor idx,col in enumerate(weakMixes.columns,start=1):\n    if idx>9:\n        break\n    plt.subplot(3,3,idx)\n    sns.scatterplot(data=weakMixes, x=col, y=\"strength\")","da31d82e":"#Check the correlation of the weak mix variables\nplt.figure(figsize = (8, 8))\nsns.heatmap(weakMixes.corr(),annot=True).set_title(f'Weak Mixes (<{weakVal}MPa)')","5dbdbe37":"#visualize the distribution of the Normal Strength mixes \nplt.figure(figsize = (15, 15))\nfor idx,col in enumerate(normalMixes.columns,start=1):\n    if idx==9:\n        continue\n    if idx==10:\n        idx=9\n    plt.subplot(3,3,idx)\n    sns.histplot(data=normalMixes, x=col)","ea92d6bc":"#visualize all normal mix data (strength > 15MPa) versus strength\nplt.figure(figsize = (15, 15))\nfor idx,col in enumerate(normalMixes.columns,start=1):\n    if idx>9:\n        break\n    plt.subplot(3,3,idx)\n    sns.scatterplot(data=normalMixes, x=col, y=\"strength\")","960ee9c4":"#Check the correlation of the normal mix variables\nplt.figure(figsize = (8, 8))\nsns.heatmap(normalMixes.corr(),annot=True).set_title(f'Normal Mixes (>{weakVal}MPa)')","3fe7e39b":"#normalize the data to use for ML algorithms \n#use standard scaler to make means of all distributions 0\nfrom sklearn.preprocessing import StandardScaler\nstrength = normalMixes['strength']\nvariables = normalMixes.drop(columns='strength')\n\nsc=StandardScaler()\nscaledVars = sc.fit_transform(variables)\nscaledVars = pd.DataFrame(scaledVars,columns=variables.columns)\nscaledVars.head()","933e2c06":"#split the data into an 70\/30 train\/test set\nfrom sklearn.model_selection import train_test_split\ntrainVars,testVars,trainStrength,testStrength=train_test_split(scaledVars,strength,test_size=.30,random_state=0)","d17f737b":"#define the ML regression models to use \nfrom sklearn.linear_model import SGDRegressor,GammaRegressor,Lasso,GammaRegressor,ElasticNet,Ridge,LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, ExtraTreesRegressor  \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.neighbors import KNeighborsRegressor\n# Import model evaluation Tools \nfrom sklearn.model_selection import learning_curve, validation_curve, GridSearchCV \n\nlr=LinearRegression()\nknn=KNeighborsRegressor()\nrf=RandomForestRegressor()\ndt=DecisionTreeRegressor()\nlasso=Lasso()\nsgd=SGDRegressor()\nridge=Ridge()\ngboost=GradientBoostingRegressor()\nbagging=BaggingRegressor()\nadboost=AdaBoostRegressor()\netr=ExtraTreesRegressor()","79ab944b":"#loop through the algortihms to train them all \nmodels=[lr,knn,rf,dt,lasso,sgd,ridge,gboost,bagging,adboost,etr]\nmodelNames = ['LR','KNN','RF','DT','Lasso','SGD','Ridge','GBoost','Bagging','ADBoost','ETR']\n\ntrainAccuracy=[]\ntestAccuracy=[]\nfor model in models:\n    #fit the model to the data\n    model.fit(trainVars,trainStrength)\n    #get the r-squared score\n    trainAccuracy.append(model.score(trainVars,trainStrength))\n    testAccuracy.append(model.score(testVars,testStrength))\n    \nmod=pd.DataFrame([modelNames,trainAccuracy,testAccuracy]).T\nmod.columns=['model','train score','test score']","7fa4abbd":"plt.figure(figsize = (20, 8))\nbars = sns.barplot(x='model',y='test score',data=mod)\nfor i,score in enumerate(mod['test score']):\n    bars.text(i,score,round(score,4),ha='center')\nplt.ylim(0.50, 1)\nplt.show()","7680403e":"#remove coarse aggregate from the variables \n#variables2 = variables.drop(columns='coarse agg')\nvariables2 = variables.drop(columns=['water', 'cement','coarse agg'])\n\nscaledVars2 = sc.fit_transform(variables2)\nscaledVars2 = pd.DataFrame(scaledVars2,columns=variables2.columns)","165df67c":"#split the data into an 70\/30 train\/test set\ntrainVars2,testVars2,trainStrength2,testStrength2=train_test_split(scaledVars2,strength,test_size=.30,random_state=0)\n\ntrainAccuracy2=[]\ntestAccuracy2=[]\nfor model in models:\n    #fit the model to the data\n    model.fit(trainVars2,trainStrength2)\n    #get the r-squared score\n    trainAccuracy2.append(model.score(trainVars2,trainStrength2))\n    testAccuracy2.append(model.score(testVars2,testStrength2))\n    \nmod2=pd.DataFrame([modelNames,trainAccuracy2,testAccuracy2]).T\nmod2.columns=['model','train score','test score']","fa5cc2a8":"plt.figure(figsize = (20, 8))\nbars2 = sns.barplot(x='model',y='test score',data=mod2)\nfor i,score in enumerate(mod2['test score']):\n    bars2.text(i,score,round(score,4),ha='center')\nplt.ylim(0.50, 1)\nplt.show()","1b676614":"# hyper parameter tuning of gradient boost regressor \ngrid_params = {\n    'loss': ['ls', 'lad', 'huber', 'quantile'],\n    'n_estimators': [1,2,5,10,20,50,100],\n    'criterion' : ['friedman_mse', 'mse', 'mae'],\n    'min_samples_split' : [1,2,3,4,5],\n    'min_samples_leaf' : [1,2,3,4,5]\n}\n\ngrid_search = GridSearchCV(gboost, grid_params, cv = 5, n_jobs = -1)\ngrid_search.fit(trainVars2, trainStrength2)\n\n# best parameters and best score\nprint(f'Best Parameters: {grid_search.best_params_}')\nprint(f'Best Score: {grid_search.best_score_}')","6608340a":"# hyper parameter tuning of random forest regressor \ngrid_params = {\n    'n_estimators': [1,2,5,10,20,50,100],\n    'max_depth' : [None,3,5,7,9,10,20],\n    'min_samples_split' : [1,2,3,4,5],\n    'min_samples_leaf' : [1,2,3,4,5]\n}\n\ngrid_search = GridSearchCV(rf, grid_params, cv = 5, n_jobs = -1)\ngrid_search.fit(trainVars2, trainStrength2)\n\n# best parameters and best score\nprint(f'Best Parameters: {grid_search.best_params_}')\nprint(f'Best Score: {grid_search.best_score_}')","718a9f1e":"strengthPredictionModel = GradientBoostingRegressor(criterion='friedman_mse', loss='huber', min_samples_leaf=4, min_samples_split=3, n_estimators=100)\nstrengthPredictionModel.fit(trainVars2, trainStrength2)\nfinalTestScore = model.score(testVars2,testStrength2)\nprint(f'Final Test Score: {finalTestScore}')","1a30cf53":"from numpy import arange\nfrom sklearn.metrics import r2_score, mean_squared_error, accuracy_score\nplt.figure(figsize = (20, 8))\nactual = testStrength2\npredictions = strengthPredictionModel.predict(testVars2)\n\nrowsToPlot = 30\nplt.scatter(arange(len(predictions[:rowsToPlot])),predictions[:rowsToPlot])\nplt.scatter(arange(len(actual[:rowsToPlot])),actual[:rowsToPlot],marker='^')\nplt.legend(['prediction','actual'])\nplt.ylabel('Strength (kPa)')\n\nr2 = r2_score(actual,predictions)\nrmse = mean_squared_error(actual,predictions,squared=False)\nprint(f'r2:{r2}')\nprint(f'rmse:{rmse}')","2cb78c2a":"# Setting up Numerical Model ","c82d5f58":"None of the values are missing which is good","3a2c891c":"The Extra Trees Regressor is the most accurate but Random Forest, Gradient Boost, and Bagging are also intriguing. However, to further simplify the model, it may be useful to remove some variables such as water & cement since the w\/c ratio is already included. Also since coarse aggregate has a very low correlation with strength (-0.06) it is probably okay to try and remove this variable as well.","5864c4e3":"Note for the variables of slag, fly ash, and superplasticizer, we have a lot of mixes with a very small amount of them (i.e close to 0). In general, these components are the most expensive in concrete mixes and can be thought of as \"add-ons\" so it is reasonable that most mixes will not contain a lot of these components. I notice also that for fly ash there is not much data between 25-75 kg\/m^3 and so I expect predictions for mixes wtih fly ash content in this range will not be very good.","1dd98f64":"# Train All The Models","5375bdf5":"# Explore The Data ","00dd7265":"# Structural Concrete Strength Prediction Model with 85% Accuracy ","18bb68f7":"The water, superplasricizer, fine aggregate, wc ratio, and age variables all contain some outliers so I decided to remove those outliers with values of 3 standard deviations away from the mean.","430d85b7":"I added one new column to the data based on my experience with concrete mix design. Often the ratio of water to cement is a better indicator of the strength compared with just water content or cement content alone. ","0972ca95":"The RMSE indicates that the average error in the prediction is about 5.6 MPa which is not bad. The final accuracy is 85%.","5cb556b8":"At this point I was concerned with the minimum strength of the samples being only 8 MPa. Structural Concrete typically has a strength greater than 15MPa and anything less that that is mainly used as backfill or for purposes where the strength is not so important. In this dataset there are 17 weak mixes with strength less than 15 MPa. ","0fd35171":"Remiving outliers got rid of 32 samples. However, most of the descriptive statistics for strength remain the same. Also, some rows with very high age were removed. This is okay because an older concrete will not be as representative of its original strength. ","df334a6d":"We have 1030 concrete mix samples and 9 variables to predict the strength","5e3e6f61":"In general, for normal strength mixes, the WC ratio is correlated most with the strength at -0.6. We can see here that the new variable of WC ratio is probably more useful than cement or water content alone. Coarse aggregate seems to be mostly irrelevant in normal mix strength. In weak mixes this was not the case.","1a8df9b0":"The Gradient Boost regressor has a higher accuracy of 84.2% so this model will be trained with those selected parameters from the grid search.","cee04f8f":"It is clear that all the weak mixes have no slag content. Although 15 samples is not really enough to make any conclusion from this, it is still an interesting find that could be investigated with more data. Another note to make here is that the w\/c ratio are all greater than 1 except for one sample. This is expected since having more water than cement typically weakens a mix.","717bdbdc":"The slag content and age do not show up here vbecuase their values do not change. Every weak mix has the same age ans same slag content. Also, water content has a high negative correlation. This means that more water = weaker mix. Coarse aggregate has a pretty high positive correlation meaning more coarse agg = stronger mix.","76968b0a":"# Exploratory Data Analysis ","784088ca":"I noticed that the minimum age here is 1 day which is not nearly enough for structural concrete to gain its full strength. \n\"Concrete gains 16% strength in one day, 40% in 3 days, 65% in 7 days, 90% in 14 days, and 99% strength in 28 days\" (from theconstructor.org). Based on this I decided to remove all samples with strength measured before 28 days.","6abbf3c0":"# Model Optimization and Validation","18210a10":"When I removed the mixes with age less than 28 days, this got rid of 324 samples. The min strength is now a bit more reasonable and the variation in the overall strength is lower ","ef33d37c":"After removing the water, cement, and coarse aggregate variables, the test score accuracy does not change significantly (less than 1%) so it is better to use this simpler model. Since RF and GBoost performed the best in this quick test, I will use a grid search and cross validation to validate results and optimize parametrs."}}