{"cell_type":{"beb19d5c":"code","beb4e603":"code","1fe7f5f7":"code","6deb387d":"code","c22d0f23":"code","201b75db":"code","85795942":"code","4c5a7fb9":"code","bd50143f":"code","6252c271":"code","c2560e22":"code","dc8ed934":"code","9992a764":"code","5ec21262":"code","586960f3":"code","882b485b":"code","18f1fcb6":"code","89110a8d":"code","bd34081c":"markdown","e7f68d5f":"markdown"},"source":{"beb19d5c":"# Data processing\nimport numpy as np\nimport pandas as pd\nimport datetime as dt\n\n# Visualization libaries\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Machine Learning\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n\n# Read data\n#n = 55000000 # Number of total rows\n#s = 100000 # Desired sample size\n#skip = sorted(np.random.choice(range(n), n-s, replace=False))\n#skip[0] = 1\n#train = pd.read_csv('..\/input\/train.csv', skiprows=skip, header=0)\ntrain = pd.read_csv('..\/input\/train.csv', nrows=10_000_000)\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.dtypes","beb4e603":"# Let's start by checking for NaN values\nprint('Sum of NaN values for each column')\nprint(train.isnull().sum())\n\n# It seems like we lost some data for the dropoff. There are several ways of handling this, but I just go with removing the rows.\ntrain = train.dropna()\nprint('Sum of NaN values for each column after dropping NaN')\nprint(train.isnull().sum())","1fe7f5f7":"# Let's have a look at the data\ntrain.describe()","6deb387d":"# So there seem to be a lot of outliers.\n\n# Manually picking reasonable levels until I find a smarter way\ntrain = train.loc[(train['fare_amount'] > 0) & (train['fare_amount'] < 200)]\ntrain = train.loc[(train['pickup_longitude'] > -300) & (train['pickup_longitude'] < 300)]\ntrain = train.loc[(train['pickup_latitude'] > -300) & (train['pickup_latitude'] < 300)]\ntrain = train.loc[(train['dropoff_longitude'] > -300) & (train['dropoff_longitude'] < 300)]\ntrain = train.loc[(train['dropoff_latitude'] > -300) & (train['dropoff_latitude'] < 300)]\n#train = train.loc[train[columns_to_select] < ]\n# Let's assume taxa's can be mini-busses as well, so we select up to 8 passengers.\ntrain = train.loc[train['passenger_count'] <= 8]\ntrain.describe()","c22d0f23":"print('Sum of NaN values for each column')\nprint(test.isnull().sum())","201b75db":"combine = [test, train]\nfor dataset in combine:\n    # Distance is expected to have an impact on the fare\n    dataset['longitude_distance'] = dataset['pickup_longitude'] - dataset['dropoff_longitude']\n    dataset['latitude_distance'] = dataset['pickup_latitude'] - dataset['dropoff_latitude']\n    \n    # Straight distance\n    dataset['distance_travelled'] = (dataset['longitude_distance'] ** 2 + dataset['latitude_distance'] ** 2) ** .5\n    dataset['distance_travelled_sin'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n    dataset['distance_travelled_cos'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5)\n    dataset['distance_travelled_sin_sqrd'] = np.sin((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n    dataset['distance_travelled_cos_sqrd'] = np.cos((dataset['longitude_distance'] ** 2 * dataset['latitude_distance'] ** 2) ** .5) ** 2\n    \n    # Haversine formula for distance\n    # Haversine formula:\ta = sin\u00b2(\u0394\u03c6\/2) + cos \u03c61 \u22c5 cos \u03c62 \u22c5 sin\u00b2(\u0394\u03bb\/2)\n    R = 6371e3 # Metres\n    phi1 = np.radians(dataset['pickup_latitude'])\n    phi2 = np.radians(dataset['dropoff_latitude'])\n    phi_chg = np.radians(dataset['pickup_latitude'] - dataset['dropoff_latitude'])\n    delta_chg = np.radians(dataset['pickup_longitude'] - dataset['dropoff_longitude'])\n    a = np.sin(phi_chg \/ 2) ** .5 + np.cos(phi1) * np.cos(phi2) * np.sin(delta_chg \/ 2) ** .5\n    c = 2 * np.arctan2(a ** .5, (1-a) ** .5)\n    d = R * c\n    dataset['haversine'] = d\n    \n    # Bearing\n    # Formula:\t\u03b8 = atan2( sin \u0394\u03bb \u22c5 cos \u03c62 , cos \u03c61 \u22c5 sin \u03c62 \u2212 sin \u03c61 \u22c5 cos \u03c62 \u22c5 cos \u0394\u03bb )\n    y = np.sin(delta_chg * np.cos(phi2))\n    x = np.cos(phi1) * np.sin(phi2) - np.sin(phi1) * np.cos(phi2) * np.cos(delta_chg)\n    dataset['bearing'] = np.arctan2(y, x)\n    \n    # Maybe time of day matters? Obviously duration is a factor, but there is no data for time arrival\n    # Features: hour of day (night vs day), month (some months may be in higher demand) \n    dataset['pickup_datetime'] = pd.to_datetime(dataset['pickup_datetime'])\n    dataset['hour_of_day'] = dataset.pickup_datetime.dt.hour\n    dataset['day'] = dataset.pickup_datetime.dt.day\n    dataset['week'] = dataset.pickup_datetime.dt.week\n    dataset['month'] = dataset.pickup_datetime.dt.month\n    dataset['day_of_year'] = dataset.pickup_datetime.dt.dayofyear\n    dataset['week_of_year'] = dataset.pickup_datetime.dt.weekofyear\n    \n\n# Remove rows with zero distance from training set\ntrain = train.loc[train['haversine'] != 0]\ntrain = train.dropna()\n\n    \ntrain.head()","85795942":"print('Train data: Sum of NaN values for each column')\nprint(train.isnull().sum())\nprint('Test data: Sum of NaN values for each column')\nprint(test.isnull().sum())","4c5a7fb9":"# So we have a lot of NaN values for the Haversine for the test set.\n# This is probably because python cannot work with to short distances.\n# This is not good, since there's a lot more NaN values than actual values.\n# So maybe Haversine isn't such a good feature afterall?\n# If used, it should be fixed somehow. Ideas?\n# One way could be by using the median or mean. However, this is not accurate.\nmedian = test['haversine'].median()\ntest['haversine'] = test['haversine'].fillna(median)","bd50143f":"# Let's check how the features correlate\ncolormap = plt.cm.RdBu\nplt.figure(figsize=(20,20))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(train.corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","6252c271":"# Let's drop all the irrelevant features\ntrain_features_to_keep = ['haversine', 'fare_amount']\ntrain.drop(train.columns.difference(train_features_to_keep), 1, inplace=True)\n\ntest_features_to_keep = ['haversine', 'key']\ntest.drop(test.columns.difference(test_features_to_keep), 1, inplace=True)","c2560e22":"# Let's experiment with different models.\n# Process:\n# 1. Get predictions using Linear Regression, Random Forest and XGBoost.\n# 2. Check how prediction correlate.\n# 3. Take a weighted average of predictions\n# 4. Submit and fingers crossed \\X\/","dc8ed934":"# Step 1:\n# Let's combine the training set again\nx_train = train.drop('fare_amount', axis=1)\ny_train = train['fare_amount']\nx_test = test.drop('key', axis=1)\n\n# Set up the models.\n# Linear Regression Model\nfrom sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(x_train, y_train)\nregr_pred = regr.predict(x_test)\n\n# Random Forest Regressor\nfrom sklearn.ensemble import RandomForestRegressor\nrfr = RandomForestRegressor()\nrfr.fit(x_train, y_train)\nrfr_pred = rfr.predict(x_test)","9992a764":"# Let's prepare the test set\nx_pred = test.drop('key', axis=1)\n\n# Let's run XGBoost and predict those fares!\nx_train,x_test,y_train,y_test = train_test_split(train.drop('fare_amount',axis=1),train.pop('fare_amount'),random_state=123,test_size=0.2)\n\ndef XGBmodel(x_train,x_test,y_train,y_test):\n    matrix_train = xgb.DMatrix(x_train,label=y_train)\n    matrix_test = xgb.DMatrix(x_test,label=y_test)\n    model=xgb.train(params={'objective':'reg:linear','eval_metric':'rmse'}\n                    ,dtrain=matrix_train,num_boost_round=200, \n                    early_stopping_rounds=20,evals=[(matrix_test,'test')],)\n    return model\n\nmodel=XGBmodel(x_train,x_test,y_train,y_test)\nxgb_pred = model.predict(xgb.DMatrix(x_pred), ntree_limit = model.best_ntree_limit)","5ec21262":"regr_pred, rfr_pred, xgb_pred","586960f3":"# Assigning weights. More precise models gets higher weight.\nregr_weight = 1\nrfr_weight = 1\nxgb_weight = 3\nprediction = (regr_pred * regr_weight + rfr_pred * rfr_weight + xgb_pred * xgb_weight) \/ (regr_weight + rfr_weight + xgb_weight)","882b485b":"prediction","18f1fcb6":"# Add to submission\nsubmission = pd.DataFrame({\n        \"key\": test['key'],\n        \"fare_amount\": prediction.round(2)\n})\n\nsubmission.to_csv('sub_fare.csv',index=False)","89110a8d":"submission","bd34081c":"# Things to do:\n- Check if whether outliers are correctly removed\n- Come up with new features\n- Increase or randomize the train loading","e7f68d5f":"# Seems like we have a few outliers. Let's visualize the data and see if we can spot the outliers.\ncolumns_to_plot = ['fare_amount', 'passenger_count']\nsns.pairplot(train.loc[:, train.columns != 'pickup_datetime'])"}}