{"cell_type":{"db9800fd":"code","b9a3f0a8":"code","9c433c9f":"code","a4e98f6b":"code","b84c676d":"code","6a1b083a":"code","bf42d5f1":"code","6b3a6885":"code","1e5e552a":"code","da0e24c2":"code","8d6ab0ac":"code","f060c9e7":"code","c325d7ca":"code","e366d538":"code","86af3f02":"code","cee4be9d":"code","4a79ed5a":"code","301786df":"code","1a5ae17f":"code","d38e1d4d":"code","071cac85":"code","05a1778b":"code","a35cf280":"code","e0adeb35":"code","5d9b20d6":"code","fe32f697":"code","1a25130f":"code","9c808be0":"code","8ff09eb0":"code","1a916b63":"markdown","7fae96df":"markdown","9e9e5965":"markdown","d89868b1":"markdown","128b202e":"markdown","6d17521d":"markdown","c1f7a3d7":"markdown","1bb93ab6":"markdown","ba26e023":"markdown","315263f1":"markdown","3faaedc7":"markdown","3605dbc5":"markdown"},"source":{"db9800fd":"import tensorflow as tf\nfrom keras.layers import Input, Lambda, Dense, Flatten,GlobalAveragePooling2D,BatchNormalization,Dropout,Activation\nfrom keras.models import Model\n# from keras.applications.densenet201 import DenseNet121\nfrom keras.applications.resnet50 import ResNet50\n# from keras.applications.densenet201 import DenseNet201\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nimport numpy as np\nfrom glob import glob\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split","b9a3f0a8":"train_dir = '..\/input\/reshaped-images\/Reshaped_images\/Train'\ntest_dir = '..\/input\/reshaped-images\/Reshaped_images\/Validation'","9c433c9f":"classes=[]\nfor file in os.listdir(train_dir):\n    classes+=[file]\nprint(classes)\nprint(len(classes))","a4e98f6b":"\nN=[]\nfor i in range(len(classes)):\n    N+=[i]\n    \nmapping=dict(zip(classes,N)) \nreverse_mapping=dict(zip(N,classes)) \n\n\ndef mapper(value):\n    return reverse_mapping[value]","b84c676d":"brownspot = [train_dir + '\/Brown Spot\/' + img for img in os.listdir(train_dir + '\/Brown Spot')[:9]]\nhealthy = [train_dir  + '\/Healthy\/' + img for img in os.listdir(train_dir + '\/Healthy')[:9]]\nhispa = [train_dir  + '\/Hispa\/' + img for img in os.listdir(train_dir + '\/Hispa')[:9]]\nleafblast = [train_dir  + '\/LeafBlast\/' + img for img in os.listdir(train_dir + '\/LeafBlast')[:9]]","6a1b083a":"from PIL import Image\nplt.figure(figsize=(16,16))\nfor i,k  in enumerate(brownspot):\n    image = Image.open(k)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image)\n    plt.title(\"Brown Spot\")","bf42d5f1":"plt.figure(figsize=(16,16))\nfor i,k  in enumerate(hispa):\n    image = Image.open(k)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image)\n    plt.title(\"Hispa\")","6b3a6885":"plt.figure(figsize=(16,16))\nfor i,k  in enumerate(leafblast):\n    image = Image.open(k)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image)\n    plt.title(\"Leaf Blast\")","1e5e552a":"plt.figure(figsize=(16,16))\nfor i,k  in enumerate(healthy):\n    image = Image.open(k)\n    plt.subplot(3,3,i+1)\n    plt.imshow(image)\n    plt.title(\"Healthy\")","da0e24c2":"dataset=[]\nshape=(224,224)\ncount=0\n#FOR EACH OF THE IMAGE\n# 1. WE READ THE IMAGE IN RGB FORMAT\n# 2. CONVERT IT INTO AN ARRAY \n# 3. SCALE DOWN THE PIXELS\n# 4. STORE THE PIXEL INFORMATION\nfor file in os.listdir(train_dir):\n    path=os.path.join(train_dir,file)\n    t=0\n    for im in os.listdir(path):\n        image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=shape)\n        image=img_to_array(image)\n        image=image\/255.0\n        dataset+=[[image,count]]\n        t+=1\n    count=count+1","8d6ab0ac":"#WE DO THE SAME THING FOR THE TEST DATASET\ntestset=[]\ncount=0\nfor file in os.listdir(test_dir):\n    path=os.path.join(test_dir,file)\n    t=0\n    for im in os.listdir(path):\n        image=load_img(os.path.join(path,im), grayscale=False, color_mode='rgb', target_size=shape)\n        image=img_to_array(image)\n        image=image\/255.0\n        testset+=[[image,count]]\n        t+=1\n    count=count+1","f060c9e7":"#WE CREATE THE LABELS AND THE FEATURES FROM THE DATASET FOR BOTH TRAIN AND TEST\ndata,labels0=zip(*dataset)\ntest,testlabels0=zip(*testset)","c325d7ca":"labels1=to_categorical(labels0)\nlabels=np.array(labels1)\n","e366d538":"data=np.array(data)\ntest=np.array(test)","86af3f02":"#SPLIT THE DATASET INTO 80-20,TRAIN-VALIDATION RATIO\ntrainx,testx,trainy,testy=train_test_split(data,labels,test_size=0.2,random_state=42)","cee4be9d":"print(trainx.shape)\nprint(testx.shape)\nprint(trainy.shape)\nprint(testy.shape)","4a79ed5a":"#WE CREATE A BATCH OF IMAGES AFTER AUGMENTING THEM\ndatagen = ImageDataGenerator(horizontal_flip=True,vertical_flip=True,rotation_range=30,zoom_range=0.2,\n                        width_shift_range=0.1,height_shift_range=0.2,shear_range=0.2)","301786df":"# model = tf.keras.Sequential([\n#     tf.keras.layers.Conv2D(32,(3,3),activation = 'relu',input_shape = (224,224,3)),\n#     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n#     tf.keras.layers.Conv2D(64,(3,3),activation = 'relu'),\n#     tf.keras.layers.Conv2D(128,(3,3),activation = 'relu'),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Conv2D(256,(3,3),activation = 'relu'),\n# #     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Conv2D(512,(3,3),activation = 'relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Conv2D(512,(3,3),activation = 'relu'),\n#     tf.keras.layers.Conv2D(1024,(3,3),activation = 'relu'),\n#     tf.keras.layers.MaxPooling2D(2,2),\n#     tf.keras.layers.Flatten(),\n#     tf.keras.layers.Dense(1024,activation = 'relu'),\n#     tf.keras.layers.Dropout(0.5),\n#     tf.keras.layers.Dense(512,activation = 'relu'),\n#     tf.keras.layers.Dropout(0.3),\n#     tf.keras.layers.Dense(256,activation = 'relu'),\n#     tf.keras.layers.Dropout(0.2),\n#     tf.keras.layers.Dense(4,activation = 'softmax')\n\n# ],    name = 'Conv2D_Model')\n\n# model.summary()","1a5ae17f":"# LEARNING_RATE = 0.001 #@param {type:\"number\"}\n\n# model.compile(optimizer = tf.keras.optimizers.Adam(),\n#               loss = 'categorical_crossentropy',\n#               metrics = ['categorical_accuracy'])","d38e1d4d":" #@param {type:\"integer\"}\n\n# his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=10)","071cac85":"# get_acc = his.history['categorical_accuracy']\n# value_acc = his.history['val_categorical_accuracy']\n# get_loss = his.history['loss']\n# validation_loss = his.history['val_loss']\n\n# epochs = range(len(get_acc))\n# plt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\n# plt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\n# plt.title('Training vs validation accuracy')\n# plt.legend(loc=0)\n# plt.figure()\n# plt.show()","05a1778b":"pretrained_model3 = tf.keras.applications.InceptionV3(input_shape=(224,224,3),include_top=False,weights='imagenet')\n ","a35cf280":"inputs3 = pretrained_model3.input\nx3 = tf.keras.layers.Flatten()(pretrained_model3.output)\nx3 = tf.keras.layers.Dense(1024, activation='relu')(x3)\nx3 = tf.keras.layers.BatchNormalization()(x3)\nx3 = tf.keras.layers.Dropout(0.2)(x3)\n# # x3 = GlobalAveragePooling2D()(pretrained_model3.output)\n# x3 = tf.keras.layers.Dense(512, activation='relu')(x3)\n# # x3 = tf.keras.layers.BatchNormalization()(x3)\n# x3 = tf.keras.layers.Dropout(0.1)(x3)\n# x3 = tf.keras.layers.Dense(128, activation='relu')(x3)\n# x3 = tf.keras.layers.Dropout(0.3)(x3)\n# x3 = tf.keras.layers.Dense(64, activation='relu')(x3)\n# # x3 = tf.keras.layers.Dropout(0.2)(x3)\n# x3 = tf.keras.layers.Dense(32, activation='relu')(x3)\n# x3 = tf.keras.layers.Dropout(0.2)(x3)\n# x3 = tf.keras.layers.Dense(16, activation='relu')(x3)\noutputs3 = tf.keras.layers.Dense(4, activation='softmax')(x3)\nmodel = tf.keras.Model(inputs=inputs3, outputs=outputs3)","e0adeb35":"from tensorflow.keras.optimizers import Adam\noptimizer = Adam(learning_rate = 0.0001)\nmodel.compile(optimizer=optimizer,loss='categorical_crossentropy',metrics=['accuracy'])","5d9b20d6":"his=model.fit(datagen.flow(trainx,trainy,batch_size=32),validation_data=(testx,testy),epochs=40)","fe32f697":"y_pred=model.predict(testx)\npred=np.argmax(y_pred,axis=1)\nground = np.argmax(testy,axis=1)\nprint(classification_report(ground,pred))","1a25130f":"get_acc = his.history['accuracy']\nvalue_acc = his.history['val_accuracy']\nget_loss = his.history['loss']\nvalidation_loss = his.history['val_loss']\n\nepochs = range(len(get_acc))\nplt.plot(epochs, get_acc, 'r', label='Accuracy of Training data')\nplt.plot(epochs, value_acc, 'b', label='Accuracy of Validation data')\nplt.title('Training vs validation accuracy')\nplt.legend(loc=0)\nplt.figure()\nplt.show()\n","9c808be0":"epochs = range(len(get_loss))\nplt.plot(epochs, get_loss, 'r', label='Loss of Training data')\nplt.plot(epochs, validation_loss, 'b', label='Loss of Validation data')\nplt.title('Training vs validation loss')\nplt.legend(loc=0)\nplt.figure()\nplt.show()","8ff09eb0":"model.save('ResNetForPlant.h5')","1a916b63":"# Preparing the Data For Out Model\n","7fae96df":"# Brown Spot","9e9e5965":"# Reading The Image","d89868b1":"# Hispa","128b202e":"# Predictions And Results ","6d17521d":"# **CHOOSING AND BUILDING A MODEL**","c1f7a3d7":"# Blast","1bb93ab6":"# 1. Simple Convolutional Neural Network ","ba26e023":"# **Visualizing the Images**","315263f1":"# Healthy","3faaedc7":"# Using Transfer Learning Technique","3605dbc5":"**THERE ARE 4 CLASSES OF WHICH 3 OF THEM ARE DISEASE AND 1 IS HEALTHY**"}}