{"cell_type":{"1cb077b7":"code","590ff839":"code","a01c76d1":"code","a102c0a5":"code","5cb531f0":"code","c0d6aa54":"code","a0f06c3a":"code","d4dbf2a0":"code","248c135b":"code","c4d4637b":"code","7a650e32":"code","59e8e40f":"code","4a02d3c0":"code","293c7eee":"code","d7e8c235":"code","f43348df":"code","4666751a":"code","2fd27b95":"code","ac32011e":"code","ba63b564":"code","aecd7aae":"code","2df45652":"code","af7e53cc":"code","d3bb33db":"code","38735e72":"code","ebac77d7":"code","1f5a4c4e":"code","c4c9e869":"code","07bcc519":"code","ea070684":"code","b2b1ef52":"markdown","43256426":"markdown","7de5a30f":"markdown","e4848a64":"markdown","5d2625e8":"markdown","4fa9d13b":"markdown","9c42a183":"markdown","1b9bb680":"markdown","58015237":"markdown","0f9962a0":"markdown","8b240797":"markdown","06e9c95e":"markdown","c415cc45":"markdown","34929d68":"markdown","4c2dfe0d":"markdown","9f1f3ab7":"markdown","e5e7df27":"markdown","2d12bf44":"markdown"},"source":{"1cb077b7":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\nimport tensorflow as tf \nfrom tensorflow.keras.layers.experimental.preprocessing import TextVectorization\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras import layers","590ff839":"df_train = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ndf_test = pd.read_csv(\"..\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')","a01c76d1":"df_train.head(7) # check data (first 7 rows)","a102c0a5":"df_test.head(7) # check data (first 7 rows)","5cb531f0":"df_train.shape # check data","c0d6aa54":"df_test.shape # check data","a0f06c3a":"def plot_pie(df, column, axes, amount=-1):\n    \"\"\" PIE PLOT \"\"\"\n    count_classes = df[column].value_counts()\n    if (amount != -1):\n        count_classes = count_classes[:amount]\n    n_classes = len(count_classes)\n    explode = (0.1,) * n_classes # explode for 0.1 each slice\n    colors = sns.color_palette('pastel') # pastel colors\n    axes.pie(count_classes, \n             labels=count_classes.index, \n             explode=explode,\n             colors=colors,\n             autopct='%.0f%%',\n             shadow=True, \n             startangle=90)\n\nfig, axes = plt.subplots(nrows=1, ncols=2, figsize=(20,8))\nplot_pie(df_train, \"Sentiment\", axes[0])\nplot_pie(df_train, \"Location\", axes[1], 10) # only first 10 locations\nplt.show()","d4dbf2a0":"y = df_train[\"Sentiment\"].map({\"Negative\":0,\n                               \"Positive\": 1,\n                               \"Neutral\": 2,\n                               \"Extremely Positive\": 3,\n                               \"Extremely Negative\": 4\n                               })\n\ny_test = df_test[\"Sentiment\"].map({\"Negative\":0,\n                                   \"Positive\": 1,\n                                   \"Neutral\": 2,\n                                   \"Extremely Positive\": 3,\n                                   \"Extremely Negative\": 4\n                                                })","248c135b":"# get the number of classes\nn_classes = df_train[\"Sentiment\"].nunique()\nn_classes","c4d4637b":"# extract text data from dataframe \nX = df_train['OriginalTweet'].to_numpy()\nX_test = df_test['OriginalTweet'].to_numpy()","7a650e32":"# TextVectorization transforms a batch of strings into either a list of token indices or a dense representation\ntext_vectorizer = TextVectorization(max_tokens=10000, \n                                    standardize=\"lower_and_strip_punctuation\", \n                                    output_sequence_length=15)","59e8e40f":"# adapt dataset\ntext_vectorizer.adapt(X)","4a02d3c0":"# example\ntext_vectorizer([\"Computer vision and deep learning\"])","293c7eee":"# the get_vocabulary() function provides \n# the vocabulary to build a metadata file \n# with one token per line\nwords = text_vectorizer.get_vocabulary()\n# The vocabulary contains the padding \n# token ('') and OOV token ('[UNK]') \n# as well as the passed tokens\nwords[:10] # check data","d7e8c235":"# turns positive integers (indexes) \n# into dense vectors of fixed size\nembedding = Embedding(input_dim=10000, \n                      output_dim=128, \n                      input_length=15, \n                      name = 'embeding_1')\nembedding","f43348df":"# example\nsample_embed = embedding(text_vectorizer([\"Computer vision and deep learning\"]))\nsample_embed","4666751a":"def calculate_results(y_true, y_pred):\n    # Calculate model accuracy\n    model_accuracy = accuracy_score(y_true, y_pred) * 100\n    # Calculate model precision, \n    # recall and f1 score using \"weighted\" average\n    model_precision, model_recall, model_f1, _ = precision_recall_fscore_support(y_true, \n                                                                                 y_pred, \n                                                                                 average=\"weighted\")\n    model_results = {\"accuracy\": model_accuracy,\n                     \"precision\": model_precision,\n                     \"recall\": model_recall,\n                     \"f1\": model_f1}\n    return model_results\n","2fd27b95":"def plot_NN_history(model_history, suptitle):\n    # plot data\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(15,6))\n    fig.suptitle(suptitle, fontsize=18)\n    \n    axes[0].plot(model_history.history['accuracy'], \n                 label='train accuracy', \n                 color='g', \n                 axes=axes[0])\n    axes[0].plot(model_history.history['val_accuracy'], \n                 label='val accuracy', \n                 color='r', \n                 axes=axes[0])\n    axes[0].set_title(\"Model Accuracy\", fontsize=16) \n    axes[0].legend(loc='upper left')\n\n    axes[1].plot(model_history.history['loss'], \n                 label='train loss', \n                 color='g', \n                 axes=axes[1])\n    axes[1].plot(model_history.history['val_loss'], \n                 label='val loss', \n                 color='r', \n                 axes=axes[1])\n    axes[1].set_title(\"Model Loss\", fontsize=16) \n    axes[1].legend(loc='upper left')\n\n    plt.show()","ac32011e":"def run_model(inputs, outputs, name, epochs, NN_name, suptitle):\n    \"\"\" GENERAL FUNCTION FOR RUNNING NEURAL NETWORK MODELS\"\"\"\n    \n    # create model\n    model = tf.keras.Model(inputs, outputs, name=name)\n    # compile model\n    model.compile(loss='sparse_categorical_crossentropy', \n                  optimizer=tf.keras.optimizers.Adam(), \n                  metrics=[\"accuracy\"])\n    # check model\n    model.summary()\n    print()\n    # train model\n    print(\"...training model...\")\n    model_history = model.fit(X, \n                              y, \n                              epochs=epochs, \n                              validation_data=(X_test, y_test),\n                              verbose=True)\n    print()\n    # check on test data\n    print(\"...evaluating model...\")\n    model.evaluate(X_test, y_test)\n    print()\n    \n    # check shape\n    print(\"y_test.shape = \", y_test.shape)\n    print()\n    \n    # get the probabilities\n    y_prob = model.predict(X_test)\n    # get the classes\n    y_hat = y_prob.argmax(axis=-1) \n    # see the test labels\n    print(\"y_test =\\n\", y_test)\n    print()\n    # check results\n    res = calculate_results(y_test, y_hat)\n    res = pd.DataFrame([res])\n    res.insert(0, \"model\", NN_name)\n    # visualize NN history\n    plot_NN_history(model_history, suptitle)\n    return model, res","ba63b564":"# setting inputs and outputs of NN\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.GRU(64)(x)\noutputs = layers.Dense(units = n_classes, activation = 'softmax')(x)","aecd7aae":"model_1, res1 = run_model(inputs, outputs, \"model_1_GRU\", 10, \"GRU\", \"GRU TRAIN HISTORY\")\nres1","2df45652":"# setting inputs and outputs of NN\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.Bidirectional(layers.LSTM(64))(x)\noutputs = layers.Dense(units = n_classes, activation = 'softmax')(x)","af7e53cc":"model_2, res2 = run_model(inputs, outputs, \"model_2_Bidirectional_LSTM\", 10, \"B_LSTM\", \"B_LSTM TRAIN HISTORY\")\nres2","d3bb33db":"# setting inputs and outputs of NN\ninputs = layers.Input(shape=(1,), dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.Bidirectional(layers.GRU(64))(x)\noutputs = layers.Dense(units = n_classes, activation = 'softmax')(x)","38735e72":"model_3, res3 = run_model(inputs, outputs, \"model_3_Bidirectional_GRU\", 10, \"B_GRU\", \"B_GRU TRAIN HISTORY\")\nres3","ebac77d7":"# setting inputs and outputs of NN\ninputs = layers.Input(shape=(1,),dtype=\"string\")\nx = text_vectorizer(inputs)\nx = embedding(x)\nx = layers.Conv1D(filters=32, kernel_size=5, activation=\"relu\")(x)\nx = layers.GlobalMaxPool1D()(x)\noutputs = layers.Dense(units = n_classes, activation = 'softmax')(x)","1f5a4c4e":"model_4, res4 = run_model(inputs, outputs, \"model_4_Conv1D\", 12, \"Conv1D\", \"Conv1D TRAIN HISTORY\")\nres4","c4c9e869":"results = [res1, res2, res3, res4]\nresults = pd.concat(results, ignore_index=True)\nresults.index = results['model']\nresults = results.drop(columns=['model'])\n\nresults # check data","07bcc519":"def plot_results(results, ax1, ax2, ax3, ax4):\n    \"\"\"VISUALIZE RESULTS\"\"\"\n    # color = ['green', 'cyan', 'blue', 'magenta']\n    color1 = plt.cm.spring(np.linspace(0, 1, len(results)))\n    color2 = plt.cm.viridis(np.linspace(0, 1, len(results)))\n    color3 = plt.cm.winter(np.linspace(0, 1, len(results)))\n    color4 = plt.cm.cool(np.linspace(0, 1, len(results)))\n    \n    ax1 = results['accuracy'].plot.bar(ax=ax1, color=color1, legend=False)\n    ax1.set_title(\"Accuracy\", fontsize=16)\n    \n    ax2 = results['precision'].plot.bar(ax=ax2, color=color2, legend=False)\n    ax2.set_title(\"Precision\", fontsize=16)   \n    \n    ax3 = results['recall'].plot.bar(ax=ax3, color=color3, legend=False)\n    ax3.set_title(\"Recall\", fontsize=16) \n    \n    ax4 = results['f1'].plot.bar(ax=ax4, color=color4, legend=False)\n    ax4.set_title(\"F1\", fontsize=16) \n    \n    print()","ea070684":"# plot data\nfig, (ax1, ax2, ax3, ax4) = plt.subplots(nrows=1, ncols=4, figsize=(20,8))\nplt.suptitle(\"Performance of the LSTM, GRU, B-LSTM, B-GRU\", fontsize=18) \nplot_results(results, ax1, ax2, ax3, ax4)\nplt.show()","b2b1ef52":"# 1. Import libraries","43256426":"Also, we'll write some helpful functions to plot visualizations:","7de5a30f":"So, next steps are similar to the steps, which are written in [the 1st part](https:\/\/www.kaggle.com\/maricinnamon\/coronavirus-tweets-classification-nlp-lstm).","e4848a64":"# Coronavirus tweets classification (part-2)\nHello everyone! It is my second part of the \"Coronavirus tweets classification\". Here we'll explore some more models: GRU and Bidirectional GRU, Bidirectional LSTM and simple Convolutional NN.\n\nHere [\ud83d\udc49Coronavirus tweets classification | NLP | LSTM\ud83d\udc48](https:\/\/www.kaggle.com\/maricinnamon\/coronavirus-tweets-classification-nlp-lstm) you can read the first part (LSTM Model was used). Also, first part includes some EDA (Exploratory Data Analysis).\n","5d2625e8":"# 8. Compare models","4fa9d13b":"# 7.2 Bidirectional LSTM\nIn bidirectional LSTM, instead of training a single model, we introduce two. The first model learns the sequence of the input provided, and the second model learns the reverse of that sequence.","9c42a183":"# 7.3 Bidirectional GRU\nBidirectional wrapper for GRU.","1b9bb680":"After that, we count **unique values** to get the number of classes.","58015237":"# 2. Read data","0f9962a0":"# 7.4 Convolutional NN","8b240797":"# 7. Train our models\nHere we are going to train our models. But, before training, we'll define some helpful functions. ","06e9c95e":"# 4. Prepare data\nHere we convert text labels to **numeric labels** (e.g. \"Sentiment\" = {0,1,..4}).","c415cc45":"# 6. Embedding\nEmbedding turns positive integers (indexes) into dense vectors of fixed size.","34929d68":"After that, we can plot our data:","4c2dfe0d":"# 9. Conclusion\nThank you for reading my new article! **If you liked it, please, make an upvote \u2764\ufe0f**\n \n* The first part of this project: [Coronavirus tweets classification | NLP | LSTM](https:\/\/www.kaggle.com\/maricinnamon\/coronavirus-tweets-classification-nlp-lstm)\n\n*Other notebooks:*\n* [Contradictory, My Dear Watson | nlp | tensorflow](https:\/\/www.kaggle.com\/maricinnamon\/contradictory-my-dear-watson-nlp-tensorflow)\n* [SMS spam with NBC | NLP | sklearn](https:\/\/www.kaggle.com\/maricinnamon\/sms-spam-with-nbc-nlp-sklearn)\n* [House Prices Regression sklearn](https:\/\/www.kaggle.com\/maricinnamon\/house-prices-regression-sklearn)\n* [Automobile Customer Clustering (K-means & PCA)](https:\/\/www.kaggle.com\/maricinnamon\/automobile-customer-clustering-k-means-pca)\n* [Credit Card Fraud detection sklearn](https:\/\/www.kaggle.com\/maricinnamon\/credit-card-fraud-detection-sklearn)\n* [Market Basket Analysis for beginners](https:\/\/www.kaggle.com\/maricinnamon\/market-basket-analysis-for-beginners)\n* [Neural Network for beginners with keras](https:\/\/www.kaggle.com\/maricinnamon\/neural-network-for-beginners-with-keras)\n* [Fetal Health Classification for beginners sklearn](https:\/\/www.kaggle.com\/maricinnamon\/fetal-health-classification-for-beginners-sklearn)\n* [Retail Trade Report Department Stores (LSTM)](https:\/\/www.kaggle.com\/maricinnamon\/retail-trade-report-department-stores-lstm)","9f1f3ab7":"# 7.1 GRU\nGated recurrent units (GRUs) are a gating mechanism in recurrent neural networks, introduced in 2014 by Kyunghyun Cho et al. The GRU is like a long short-term memory (LSTM) with a forget gate, but has fewer parameters than LSTM, as it lacks an output gate. GRU's performance on certain tasks of polyphonic music modeling, speech signal modeling and natural language processing was found to be similar to that of LSTM. GRUs have been shown to exhibit better performance on certain smaller and less frequent datasets.","e5e7df27":"# 3. Visualize data\nAs it is said earlier, you can see some plots in [the first part (3. Visualize data)](https:\/\/www.kaggle.com\/maricinnamon\/coronavirus-tweets-classification-nlp-lstm\/).\nHere I'll build some more visualisations.\n\nFirstly, let's build some pie charts:","2d12bf44":"# 5. TextVectorization\nTextVectorization transforms a batch of strings into either a list of token indices or a dense representation."}}