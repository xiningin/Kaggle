{"cell_type":{"b6f323fa":"code","f55114b3":"code","f76a89c6":"code","94ab54d5":"code","2d762705":"code","c8382175":"code","bd4b9310":"code","8adbdb51":"code","0b60e0b3":"code","6f868c3b":"code","b78f342b":"code","aeb22f1e":"code","9753693f":"code","697e051f":"code","8ee0345b":"code","961664f9":"code","bd26d227":"code","589c3241":"code","1c16659a":"code","8a771f96":"code","e55ec8e3":"code","bf31983b":"code","af64bf51":"code","9a2d3946":"code","94cc50fd":"code","f08e4557":"code","776fdbba":"code","b6416176":"code","28e03ce6":"code","29840971":"code","175e0e64":"code","7137ac02":"markdown","bf8b4dfc":"markdown","179fb2c2":"markdown","3a0e8b4a":"markdown","189a633d":"markdown","720f9121":"markdown","e92b6d34":"markdown","7933a34f":"markdown"},"source":{"b6f323fa":"import json\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom tqdm import tqdm\n\npd.set_option('display.max_colwidth', -1)","f55114b3":"# def reduce_mem_usage(props, log=False):\n#     start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n#     print(\"Memory usage of properties dataframe is :\", round(start_mem_usg, 2), \" MB\")\n#     NAlist = [] # Keeps track of columns that have missing values filled in. \n#     for col in props.columns:\n#         if props[col].dtype != object:  # Exclude strings and timestamps\n            \n#             # Print current column type\n#             if log: print(\"******************************\")\n#             if log: print(\"Column: \",col)\n#             if log: print(\"dtype before: \",props[col].dtype)\n            \n#             # make variables for Int, max and min\n#             IsInt = False\n#             mx = props[col].max()\n#             mn = props[col].min()\n            \n#             # Integer does not support NA, therefore, NA needs to be filled\n#             if not np.isfinite(props[col]).all(): \n#                 NAlist.append(col)\n#                 props[col].fillna(mn-1,inplace=True)            \n\n#             # test if column can be converted to an integer\n#             asint = props[col].fillna(0).astype(np.int64)\n#             result = (props[col] - asint)\n#             result = result.sum()\n#             if result > -0.01 and result < 0.01:\n#                 IsInt = True\n\n            \n#             # Make Integer\/unsigned Integer datatypes\n#             if IsInt:\n#                 if mn >= 0:\n#                     if mx < 255:\n#                         props[col] = props[col].astype(np.uint8)\n#                     elif mx < 65535:\n#                         props[col] = props[col].astype(np.uint16)\n#                     elif mx < 4294967295:\n#                         props[col] = props[col].astype(np.uint32)\n#                     else:\n#                         props[col] = props[col].astype(np.uint64)\n#                 else:\n#                     if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n#                         props[col] = props[col].astype(np.int8)\n#                     elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n#                         props[col] = props[col].astype(np.int16)\n#                     elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n#                         props[col] = props[col].astype(np.int32)\n#                     elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n#                         props[col] = props[col].astype(np.int64)    \n            \n#             # Make float datatypes 32 bit\n#             else:\n#                 props[col] = props[col].astype(np.float32)\n            \n#             # Print new column type\n#             if log: print(\"dtype after: \",props[col].dtype)\n#             if log: print(\"******************************\")\n    \n#     mem_usg = props.memory_usage().sum() \/ 1024**2 \n#     print(\"Memory usage is now: \", round(mem_usg, 2), \" MB\")\n#     print(\"This is \",round(100 * mem_usg \/ start_mem_usg, 2),\"% of the initial size\")\n#     return props\n\n\n# https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else: df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n","f76a89c6":"%%time\nN_ROWS = int(1e4)\ntrain = reduce_mem_usage(pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/train.csv\",\n#                     parse_dates=[\"timestamp\"],infer_datetime_format=True\n                   )#, nrows=N_ROWS)\n                        )\ntest = pd.read_csv(\"\/kaggle\/input\/data-science-bowl-2019\/test.csv\", nrows=N_ROWS)\ntrain.head()","94ab54d5":"train[[\"event_id\",\"game_session\",\"installation_id\"]].nunique()","2d762705":"## parse datetime\n# train.timestamp = pd.to_datetime(train.timestamp,infer_datetime_format=True)\n# test.timestamp = pd.to_datetime(test.timestamp,infer_datetime_format=True)\n\n# train.set_index(\"timestamp\",inplace=True)","c8382175":"train.dtypes","bd4b9310":"start_mem_usg = train.memory_usage().sum() \/ 1024**2 \nprint(\"Memory usage of the train is : {:.1f} MB for now\".format(start_mem_usg))\nstart_mem_usg = test.memory_usage().sum() \/ 1024**2 \nprint(\"Memory usage of the test is : {:.1f} MB for now\".format(start_mem_usg))","8adbdb51":"labels = pd.read_csv('\/kaggle\/input\/data-science-bowl-2019\/train_labels.csv')\nprint(labels.nunique())\nprint(labels.shape)\nprint(labels.dtypes)\nlabels.head()","0b60e0b3":"## https:\/\/www.kaggle.com\/carlossouza\/cleaning-useless-data-to-load-train-csv-faster\n\nprint(train.shape)\nuseful_installation_ids = labels['installation_id'].unique()\ntrain = train.loc[train.installation_id.isin(labels.installation_id)]\nprint(train.shape)","6f868c3b":"train.head(3)","b78f342b":"train.memory_usage().sum() \/ 1024**2 ","aeb22f1e":"# train.timestamp = pd.to_datetime(train.timestamp,infer_datetime_format=True)\n# train.memory_usage().sum() \/ 1024**2 ","9753693f":"train.dtypes","697e051f":"train['event_data'] = train['event_data'].apply(lambda x: json.loads(x))\ntest['event_data'] = test['event_data'].apply(lambda x: json.loads(x))","8ee0345b":"event_data = train['event_data'].tolist()\nunique_keys = list()\nfor my_json in event_data:\n    unique_keys += my_json.keys()\n    \nunique_keys = list(set(unique_keys))\nprint('event_data contains {} new columns'.format(len(unique_keys)))\nprint('Some new columns are:', unique_keys[:5])","961664f9":"event_data[0:5]","bd26d227":"for ky in tqdm(unique_keys):\n    def give_me_keys(x):\n        try:\n            return x[ky]\n        except KeyError:\n            return np.nan\n    train[ky] = train['event_data'].apply(give_me_keys)\n    test[ky] = test['event_data'].apply(give_me_keys)\n    \n    \nprint('Train shape is:', train.shape)\nprint('Test shape is:', test.shape)\nstart_mem_usg = train.memory_usage().sum() \/ 1024**2 \nprint(\"Memory usage of the train dataframe is : {:.1f} MB for now\".format(start_mem_usg))","589c3241":"# Now that we've the information contained in event_data, we can drop it\ntry:\n    train.drop('event_data', axis=1, inplace=True)\n    test.drop('event_data', axis=1, inplace=True)\nexcept:\n    pass\nprint(train.shape)\ntrain.head()","1c16659a":"%%time\ntrain = reduce_mem_usage(train)","8a771f96":"VAR_THRESH = .015 # .1\nNAN_THRESH = .991 #.99\n\nnan_dict = train.isna().mean() >= NAN_THRESH\n# cols_to_drop += [k for k, v in nan_dict.items() if v]\ncols_to_drop = [k for k, v in nan_dict.items() if v]\n\n# drop twice to save on second check\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)\n\nprint(\"less nans, train size:\", train.shape)\nvar_dict = train.std() <= VAR_THRESH\ncols_to_drop = [k for k, v in var_dict.items() if v]\n\ncols_to_drop = list(set(cols_to_drop))\ntrain.drop(cols_to_drop, axis=1, inplace=True)\ntest.drop(cols_to_drop, axis=1, inplace=True)\n\nprint('We dropped {} columns'.format(len(cols_to_drop)))\nprint('Train shape is: ', train.shape)\nprint('Test shape is: ', test.shape)","e55ec8e3":"train.head()","bf31983b":"train.tail()","af64bf51":"train.tail()","9a2d3946":"print(train.shape)\ndf = train.groupby([\"installation_id\",\"game_session\"],as_index=False).last() # \"game_session\",\nprint(\"df.shape\",df.shape)\ndf = df.merge(labels,on=[\"installation_id\",\"game_session\"],how=\"inner\")\nprint(\"df.merge(labels,on=[installation_id,game_session],how=inner)\", df.shape)\ndf.head()","94cc50fd":"# df = df.T.drop_duplicates().T ## can't drop duplicate cols, data has lists\/dicts = unhashable\n# df.shape","f08e4557":"df[[\"event_id\",\"game_session\",\"installation_id\"]].nunique()","776fdbba":"df.to_csv(\"dsb_train_v1.csv.gz\",compression=\"gzip\")\n\ntrain.to_csv(\"dsb_context_tr_v1.csv.gz\",compression=\"gzip\")","b6416176":"# col_to_label_encode = list()\n# for col in train.columns:\n#     try:\n#         if len(train[col].unique()) < 10:\n#             col_to_label_encode.append(col)\n#     except:\n#         pass","28e03ce6":"# correspondance_dict = dict()\n\n# for col in col_to_label_encode:\n#     try:\n#         le = LabelEncoder()\n#         train[col] = le.fit_transform(train[col])\n#         test[col] = le.transform(test[col])\n\n#         keys = le.classes_\n#         values = le.transform(le.classes_)\n#         dictionary = dict(zip(keys, values))\n#         correspondance_dict[col] = dictionary\n\n#     except:    # the variable is not label encodable\n#         pass\n\n# correspondance_dict","29840971":"# train = reduce_mem_usage(train, log=False)","175e0e64":"# train.to_pickle('train.pkl')\n# test.to_pickle('test.pkl')","7137ac02":"### The goal of this notebook is to offer a first preprocessing step so that you can manipulate this huuuuuuge dataset easily. Can save as a .pkl, which allows you to load it quickly!\n\n* Also get data in prediction ready format\n* `For each installation_id represented in the test set, you must predict the accuracy_group of the last assessment for that installation_id.`\n","bf8b4dfc":"* Now we will Label Encode some variable to stock them as small int (instead of objects)","179fb2c2":"# Dataset Minification","3a0e8b4a":"## Exploring event_data column","189a633d":"### `event_data` seems interesting. I think it is the main source of information.\n### The data is given in json format, so we'll parse it to be able to create columns","720f9121":"#### Loading\n\n###### We also remove rows from train with no labelled data.\n* https:\/\/www.kaggle.com\/carlossouza\/cleaning-useless-data-to-load-train-csv-faster","e92b6d34":"### Add labels with data for predicting\n* take last event per session and join with label. \n\n* get last event per game session for attaching to the label\n* we would need this for test as well","7933a34f":"### Remove columns which variance is very low or with too many missing values\n\n* Modify the 2 thresholds to fit your needs"}}