{"cell_type":{"aa757fa3":"code","3742c8eb":"code","1231809e":"code","e444b54d":"code","a991de0f":"code","7cec355a":"code","3e58a156":"code","b63a55c3":"code","f517a718":"code","3191c530":"code","61a3434e":"code","2b5466b9":"code","2c0da9e0":"code","156f628f":"markdown","1fc029b2":"markdown","fa97f908":"markdown","e5e50bc0":"markdown","b19f8325":"markdown"},"source":{"aa757fa3":"# They are for data manipulation\/\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \nimport pandas as pd #data processing, CSV file I\/O (e.g. pd.read_csv)\nimport numpy as np #linear algebra\n\n# For Visualization \/ \uc2dc\uac01\ud654\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')\nimport missingno\n\n# Ignore warnings \/ \uacbd\uace0 \uc81c\uac70 (Pandas often makes warnings)\nimport sys\nimport warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","3742c8eb":"# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1231809e":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()\n# train_data.describe()","e444b54d":"# Giving my correlation matrix a heatmap, annot=true gives me the correlation values\nsns.heatmap(train_data.corr(), annot = True)","a991de0f":"# Visualize the missing row for each columns\nmissingno.matrix(train_data, figsize = (15,8)) #The age is missing for some passengers, their cabin too are missing for the majority of them","7cec355a":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()\n","3e58a156":"# older55 = train_data.loc[train_data.Age > 55][\"Survived\"]\n# rate_older55 = sum(older55)\/len(older55)\n\n# print(\"% of people older than 55 who survived:\", rate_older55)\n# younger55 = train_data.loc[train_data.Age < 55][\"Survived\"]\n# rate_younger55 = sum(younger55)\/len(younger55)\n\n# print(\"% of people young than 55 who survived:\", rate_younger55)","b63a55c3":"from sklearn.ensemble import RandomForestClassifier #Random Forest Model\nfrom sklearn.model_selection import train_test_split #To split my training sample\nfrom sklearn.metrics import f1_score #A metrics for measuring accuracy on my split test sample\n\n\n#The whole results as training data\ny = train_data[\"Survived\"]\n\n#For the moment these features are fine, as I don't know yet how to deal with colmuns of data with missing cells\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\", \"Embarked\", \"Fare\"]\n\n#The features of the training sample\nX = pd.get_dummies(train_data[features])\n\n#The ultimate testing set\nX_test = pd.get_dummies(test_data[features])\n\n#Here I will split the data between a training sample and a test sample (whith 80\/20 ratio)\ntrain_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.20, random_state=1)\n","f517a718":"\n#Let's try and find a good number of estimators with a max tree-depth of 5\ndef get_f1(n_estimators, train_X, val_X, train_y, val_y):\n    model = RandomForestClassifier(n_estimators=n_estimators, max_depth=5, random_state=1)\n    model.fit(train_X, train_y)\n    preds_val = model.predict(val_X)\n    f1 = f1_score(val_y, preds_val, pos_label=1, average='binary')\n    return(f1)\n\n\nscores = {n_estimators: get_f1(n_estimators, train_X, val_X, train_y, val_y) for n_estimators in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n\nbest_n_estimators = max(scores, key=scores.get)\nprint(best_n_estimators)\nprint(scores)\n#Turns out around 400 is the best we can\n","3191c530":"# Let's import the imputer constructor and create one\nfrom sklearn.impute import SimpleImputer\nmy_imputer = SimpleImputer()\n\n# Let's have a new training set with the age in it \nnew_X = pd.get_dummies(train_data[features + [\"Age\"]])\n\n#Let's impute the missing values for a new_X\nimputed_X = pd.DataFrame(my_imputer.fit_transform(new_X))\n#Because imputing gets rid of the columns\nimputed_X.columns = new_X.columns\n\n# Now let's do the same thing for our testing set\nnew_test_X = pd.get_dummies(test_data[features + [\"Age\"]])\nimputed_test_X = pd.DataFrame(my_imputer.fit_transform(new_test_X))\nimputed_test_X.columns = new_test_X.columns\n\n# Visualize the missing row for each columns\nmissingno.matrix(imputed_X, figsize = (15,8)) #The age is missing for some passengers, their cabin too are missing for the majority of them\n\n\n#Now let's split the training dataset and do a little test\nimputed_X_train, imputed_X_val,imputed_y_train, imputed_y_val = train_test_split(imputed_X, y, train_size=0.80, test_size=0.20, random_state=1)\n\n# We will try and find the best number of estimators using get_f1 again\n# scores_imputed = {n_estimators: get_f1(n_estimators, imputed_X_train, imputed_X_val,imputed_y_train, imputed_y_val) for n_estimators in [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\n# best_n_estimators = max(scores_imputed, key=scores_imputed.get)\n# print(best_n_estimators)\n# print(scores_imputed)\n# Seems like 100 is the best we can\n\n#IMPUTATION SET TESTING\n#Let's create a Random Forest model to begin with and look at the predictions on our split test sample with imputed values\nimpute_model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nimpute_model.fit(imputed_X_train, imputed_y_train)\ntest_sample_predictions = impute_model.predict(imputed_X_val)\nfirst_f1_score = f1_score(imputed_y_val, test_sample_predictions, pos_label=1, average='binary')\nprint(first_f1_score)","61a3434e":"# BEFORE IMPUTATION\n# Let's creat a Random Forest model to begin with and look at the predictions on our split test sample\nmodel = RandomForestClassifier(n_estimators=200, max_depth=5, random_state=1)\nmodel.fit(train_X, train_y)\ntest_sample_predictions = model.predict(val_X)\nfirst_f1_score = f1_score(val_y, test_sample_predictions, pos_label=1, average='binary')\nprint(first_f1_score)","2b5466b9":"# #Let's print the categorical values (i.e obect datatypes)\n# categ_cols = [col for col in ]","2c0da9e0":"#This is for when I did all the training and testing and I only have to make predictions for test_data \npredictions = impute_model.predict(imputed_test_X)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","156f628f":"<font size=\"4\">For some reasons (???) eventhough I drawed the conclusion that imputing my age values lead to worse results, my score actually improved nonetheless! Now either the way I get to test my results for the split dataset (using the f1_score) is truly flawed or maybe true randomness is involved, either I am too ignorant to know yet!<\/font>","1fc029b2":"<font size=4>Now let's try and handle categorical values with a One-Hot encoder<\/font>","fa97f908":"# We can clearly see that imputing the missing data when it comes to the age actually doesn't give us a predicting advantage, unfortunately for me :(","e5e50bc0":"<font size=\"5\">So this is a new try, here I will recreate the datasets but this time I will replace the missing values for the age with the mean of the age using SimpleImputer to impute those values<\/font>","b19f8325":"****Still need to learn differents types of models, and learn about merging models and other techniques to get a better score****"}}