{"cell_type":{"84c0dbb2":"code","1efadf47":"code","60ebf18e":"code","b54108b7":"code","05c61dd3":"code","bd14e5e1":"code","83139941":"code","021baea8":"code","b907a02e":"code","267ca3a2":"code","a3965261":"code","b9fd55e1":"code","d3c05e0f":"code","0ca4f2a6":"code","d56ae967":"code","84cb246c":"code","3c1166a7":"code","bdf2e25c":"code","42d3233a":"code","fabfe35c":"code","c49ecf6a":"code","bdc9aebe":"code","fd764f81":"code","2c5e946e":"code","51cb666b":"code","f9ede67d":"code","4cdcb8ba":"code","33a481a8":"code","6140e609":"code","3f0e611d":"code","72e566ec":"code","38067075":"code","c66d9010":"code","cf379c0f":"code","16947ba8":"code","eea7ad6b":"code","c3fcd038":"code","5b59f0d6":"code","3a8144cc":"code","b2bc004a":"code","c5f41f8b":"code","552ff1fc":"code","485c5e8e":"code","8df1975f":"code","27f402be":"code","6b9c4779":"code","7ee46c12":"code","7f62f1d5":"code","244491e1":"code","098a92a0":"code","babaec62":"code","a3e674ec":"code","0316449c":"markdown","49889c6d":"markdown","4da34fb7":"markdown","85f2f882":"markdown","005b83d4":"markdown","6750e777":"markdown","03b639f8":"markdown","f6ce2c63":"markdown","06b9d638":"markdown","ee3a66bc":"markdown","63718f6b":"markdown","12abf0fa":"markdown","4d6f218d":"markdown","9413b11b":"markdown","6f281787":"markdown","bdfa604c":"markdown","3e8b9ce9":"markdown","11c4e0bd":"markdown","8868ffb5":"markdown","363328a3":"markdown"},"source":{"84c0dbb2":"from __future__ import print_function\nimport pandas as pd # data analysis\nimport numpy as np # linear algebra\n\n#import libraries for data visualization\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\n\n\nfrom sklearn.metrics import classification_report\nfrom sklearn import metrics\nfrom sklearn import tree\nfrom sklearn.model_selection import cross_val_score\nimport warnings\nwarnings.filterwarnings('ignore')","1efadf47":"crop = pd.read_csv('..\/input\/crop-recommendation-dataset\/Crop_recommendation.csv')\ncrop.head(5)","60ebf18e":"crop.info()","b54108b7":"crop.describe()","05c61dd3":"crop.columns","bd14e5e1":"crop.shape","83139941":"crop['label'].unique()","021baea8":"crop['label'].nunique()","b907a02e":"crop['label'].value_counts()","267ca3a2":"sns.heatmap(crop.isnull(),cmap=\"coolwarm\")\nplt.show()","a3965261":"plt.figure(figsize=(12,5))\nplt.subplot(1, 2, 1)\n# sns.distplot(df_setosa['sepal_length'],kde=True,color='green',bins=20,hist_kws={'alpha':0.3})\nsns.distplot(crop['temperature'],color=\"red\",bins=15,hist_kws={'alpha':0.5})\nplt.subplot(1, 2, 2)\nsns.distplot(crop['ph'],color=\"green\",bins=15,hist_kws={'alpha':0.5})","b9fd55e1":"sns.pairplot(crop,hue = 'label')","d3c05e0f":"sns.jointplot(x=\"rainfall\",y=\"humidity\",data=crop[(crop['temperature']<40) & \n                                                  (crop['rainfall']>40)],height=10,hue=\"label\")","0ca4f2a6":"sns.set_theme(style=\"whitegrid\")\nfig, ax = plt.subplots(figsize=(30,15))\nsns.boxplot(x='label',y='ph',data=crop)","d56ae967":"fig, ax = plt.subplots(1, 1, figsize=(15, 9))\nsns.heatmap(crop.corr(), annot=True,cmap='viridis')\nax.set(xlabel='features')\nax.set(ylabel='features')\n\nplt.title('Correlation between different features', fontsize = 15, c='black')\nplt.show()","84cb246c":"crop_summary = pd.pivot_table(crop,index=['label'],aggfunc='mean')\ncrop_summary.head()","3c1166a7":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=crop_summary.index,\n    y=crop_summary['N'],\n    name='Nitrogen',\n    marker_color='mediumvioletred'\n))\nfig.add_trace(go.Bar(\n    x=crop_summary.index,\n    y=crop_summary['P'],\n    name='Phosphorous',\n    marker_color='springgreen'\n))\nfig.add_trace(go.Bar(\n    x=crop_summary.index,\n    y=crop_summary['K'],\n    name='Potash',\n    marker_color='dodgerblue'\n))\n\nfig.update_layout(title=\"N-P-K values comparision between crops\",\n                  plot_bgcolor='white',\n                  barmode='group',\n                  xaxis_tickangle=-45)\n\nfig.show()","bdf2e25c":"features = crop[['N', 'P','K','temperature', 'humidity', 'ph', 'rainfall']]\ntarget = crop['label']","42d3233a":"acc = []\nmodel = []","fabfe35c":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(features,target,test_size = 0.2,random_state =2)","c49ecf6a":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\n\nknn.fit(x_train,y_train)\n\npredicted_values = knn.predict(x_test)\n\nx = metrics.accuracy_score(y_test, predicted_values)\nacc.append(x)\nmodel.append('K Nearest Neighbours')\nprint(\"KNN Accuracy is: \", x)\n\nprint(classification_report(y_test,predicted_values))","bdc9aebe":"score = cross_val_score(knn,features,target,cv=5)\nprint('Cross validation score: ',score)","fd764f81":"#Print Train Accuracy\nknn_train_accuracy = knn.score(x_train,y_train)\nprint(\"knn_train_accuracy = \",knn.score(x_train,y_train))\n#Print Test Accuracy\nknn_test_accuracy = knn.score(x_test,y_test)\nprint(\"knn_test_accuracy = \",knn.score(x_test,y_test))","2c5e946e":"y_pred = knn.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_knn = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cm_knn, annot=True, linewidth=0.5, fmt=\".0f\",cmap='viridis', ax = ax)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Predicted vs actual')\nplt.show()","51cb666b":"mean_acc = np.zeros(20)\nfor i in range(1,21):\n    #Train Model and Predict  \n    knn = KNeighborsClassifier(n_neighbors = i).fit(x_train,y_train)\n    yhat= knn.predict(x_test)\n    mean_acc[i-1] = metrics.accuracy_score(y_test, yhat)\n\nmean_acc","f9ede67d":"loc = np.arange(1,21,step=1.0)\nplt.figure(figsize = (10, 6))\nplt.plot(range(1,21), mean_acc)\nplt.xticks(loc)\nplt.xlabel('Number of Neighbors ')\nplt.ylabel('Accuracy')\nplt.show()","4cdcb8ba":"from sklearn.model_selection import GridSearchCV","33a481a8":"grid_params = { 'n_neighbors' : [12,13,14,15,16,17,18],\n               'weights' : ['uniform','distance'],\n               'metric' : ['minkowski','euclidean','manhattan']}","6140e609":"gs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose = 1, cv=3, n_jobs = -1)","3f0e611d":"g_res = gs.fit(x_train, y_train)","72e566ec":"g_res.best_score_","38067075":"g_res.best_params_","c66d9010":"# Using the best hyperparameters\nknn_1 = KNeighborsClassifier(n_neighbors = 12, weights = 'distance',algorithm = 'brute',metric = 'manhattan')\nknn_1.fit(x_train, y_train)","cf379c0f":"# Training & Testing accuracy after applying hyper parameter\nknn_train_accuracy = knn_1.score(x_train,y_train)\nprint(\"knn_train_accuracy = \",knn_1.score(x_train,y_train))\n#Print Test Accuracy\nknn_test_accuracy = knn_1.score(x_test,y_test)\nprint(\"knn_test_accuracy = \",knn_1.score(x_test,y_test))","16947ba8":"from sklearn.tree import DecisionTreeClassifier\nDT = DecisionTreeClassifier(criterion=\"entropy\",random_state=2,max_depth=5)\n\nDT.fit(x_train,y_train)\n\npredicted_values = DT.predict(x_test)\nx = metrics.accuracy_score(y_test, predicted_values)\nacc.append(x)\nmodel.append('Decision Tree')\nprint(\"Decision Tree's Accuracy is: \", x*100)\n\nprint(classification_report(y_test,predicted_values))","eea7ad6b":"score = cross_val_score(DT, features, target,cv=5)\nprint('Cross validation score: ',score)","c3fcd038":"#Print Train Accuracy\ndt_train_accuracy = DT.score(x_train,y_train)\nprint(\"Training accuracy = \",DT.score(x_train,y_train))\n#Print Test Accuracy\ndt_test_accuracy = DT.score(x_test,y_test)\nprint(\"Testing accuracy = \",DT.score(x_test,y_test))","5b59f0d6":"y_pred = DT.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_dt = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cm_dt, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='viridis', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()","3a8144cc":"from sklearn.ensemble import RandomForestClassifier\n\nRF = RandomForestClassifier(n_estimators=20, random_state=0)\nRF.fit(x_train,y_train)\n\npredicted_values = RF.predict(x_test)\n\nx = metrics.accuracy_score(y_test, predicted_values)\nacc.append(x)\nmodel.append('RF')\nprint(\"Random Forest Accuracy is: \", x)\n\nprint(classification_report(y_test,predicted_values))","b2bc004a":"score = cross_val_score(RF,features,target,cv=5)\nprint('Cross validation score: ',score)","c5f41f8b":"#Print Train Accuracy\nrf_train_accuracy = RF.score(x_train,y_train)\nprint(\"Training accuracy = \",RF.score(x_train,y_train))\n#Print Test Accuracy\nrf_test_accuracy = RF.score(x_test,y_test)\nprint(\"Testing accuracy = \",RF.score(x_test,y_test))","552ff1fc":"y_pred = RF.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_rf = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cm_rf, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='viridis', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()","485c5e8e":"from sklearn.naive_bayes import GaussianNB\nNaiveBayes = GaussianNB()\n\nNaiveBayes.fit(x_train,y_train)\n\npredicted_values = NaiveBayes.predict(x_test)\nx = metrics.accuracy_score(y_test, predicted_values)\nacc.append(x)\nmodel.append('Naive Bayes')\nprint(\"Naive Bayes Accuracy is: \", x)\n\nprint(classification_report(y_test,predicted_values))","8df1975f":"score = cross_val_score(NaiveBayes,features,target,cv=5)\nprint('Cross validation score: ',score)","27f402be":"#Print Train Accuracy\nnb_train_accuracy = NaiveBayes.score(x_train,y_train)\nprint(\"Training accuracy = \",NaiveBayes.score(x_train,y_train))\n#Print Test Accuracy\nnb_test_accuracy = NaiveBayes.score(x_test,y_test)\nprint(\"Testing accuracy = \",NaiveBayes.score(x_test,y_test))","6b9c4779":"y_pred = NaiveBayes.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cm_nb, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='viridis', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()","7ee46c12":"import xgboost as xgb\nXB = xgb.XGBClassifier()\nXB.fit(x_train,y_train)\n\npredicted_values = XB.predict(x_test)\n\nx = metrics.accuracy_score(y_test, predicted_values);\nacc.append(x)\nmodel.append('XGBoost')\nprint(\"XGBoost Accuracy is: \", x)\n\nprint(classification_report(y_test,predicted_values))","7f62f1d5":"score = cross_val_score(XB,features,target,cv=5)\nprint('Cross validation score: ',score)","244491e1":"#Print Train Accuracy\nXB_train_accuracy = XB.score(x_train,y_train)\nprint(\"Training accuracy = \",XB.score(x_train,y_train))\n#Print Test Accuracy\nXB_test_accuracy = XB.score(x_test,y_test)\nprint(\"Testing accuracy = \",XB.score(x_test,y_test))","098a92a0":"y_pred = XB.predict(x_test)\ny_true = y_test\n\nfrom sklearn.metrics import confusion_matrix\n\ncm_nb = confusion_matrix(y_true,y_pred)\n\nf, ax = plt.subplots(figsize=(15,10))\nsns.heatmap(cm_nb, annot=True, linewidth=0.5, fmt=\".0f\",  cmap='viridis', ax = ax)\nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title('Predicted vs actual')\nplt.show()","babaec62":"plt.figure(figsize=[14,7],dpi = 100, facecolor='white')\nplt.title('Accuracy Comparison')\nplt.xlabel('Accuracy')\nplt.ylabel('ML Algorithms')\nsns.barplot(x = acc,y = model,palette='viridis')\nplt.savefig('plot.png', dpi=300, bbox_inches='tight')","a3e674ec":"label = ['KNN', 'Decision Tree','Random Forest','Naive Bayes','XG Boost']\nTest = [knn_test_accuracy, dt_test_accuracy,rf_test_accuracy,\n        nb_test_accuracy, XB_test_accuracy]\nTrain = [knn_train_accuracy,  dt_train_accuracy, rf_train_accuracy,\n         nb_train_accuracy, XB_train_accuracy]\n\nf, ax = plt.subplots(figsize=(20,7)) # set the size that you'd like (width, height)\nX_axis = np.arange(len(label))\nplt.bar(X_axis - 0.2,Test, 0.4, label = 'Test', color=('midnightblue'))\nplt.bar(X_axis + 0.2,Train, 0.4, label = 'Train', color=('mediumaquamarine'))\n\nplt.xticks(X_axis, label)\nplt.xlabel(\"ML algorithms\")\nplt.ylabel(\"Accuracy\")\nplt.title(\"Testing vs Training Accuracy\")\nplt.legend()\n#plt.savefig('train vs test.png')\nplt.show()","0316449c":"<img src='https:\/\/i.imgur.com\/qpOUo9R.gif'>","49889c6d":" <a id=\"5.5\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\"> eXtreme Gradient Boosting (XGBoost)<\/p>","4da34fb7":" <a id=\"5.2\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Decision Tree<\/p>","85f2f882":"<div style=\"font-size:18px; font-family:verdana;\">We will use three hyperparamters- n-neighbors, weights and metric:<br>\n\n* n_neighbors: Decide the best k based on the values we have computed earlier.\n* weights: Check whether adding weights to the data points is beneficial to the model or not. 'uniform' assigns no weight, while 'distance' weighs points by the inverse of their               distances meaning nearer points will have more weight than the farther points.\n* metric: The distance metric to be used will calculating the similarity\n\n<\/div>\n","005b83d4":"<img src=\"https:\/\/i.imgur.com\/MQcqLwg.gif\">","6750e777":" <a id=\"5.3\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Random Forest<\/p>","03b639f8":"<div style=\"font-size:15px; font-family:verdana;\">One of the challenges in a KNN algorithm is finding the best 'k' i.e. the number of neighbors to be used in the majority vote while deciding the class. Generally, it is advisable to test the accuracy of your model for different values of k and then select the best one from them.<br>\n<\/div>","f6ce2c63":"<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Machine learning is an important decision support tool for crop yield prediction, including supporting decisions on what crops to grow and what to do during the growing season of the crops.<\/p>\n<p style=\"font-size:18px; font-family:verdana; line-height: 1.7em\">Machine learning is everywhere throughout the whole growing and harvesting cycle. It begins with a crop prediction \u2014 from the soil preparation, seeds breeding and water feed measurement \u2014 and it ends when robots pick up the harvest determining the ripeness with the help of computer vision.<\/p><br>","06b9d638":"<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n    \n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Table Of Contents<\/p>\n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. Exploratory Data Analysis](#3) \n \n* [4. Feature selection](#4) \n    \n* [5. Modeling Classification algorithms](#5)\n    \n    * [5.1 K-Nearest Neighbors](#5.1)\n         * [5.1.1 Hyperparameter Tuning](#5.1.1)\n    \n    * [5.2 Decision Tree](#5.2)\n    * [5.3 Random Forest](#5.3)\n    * [5.4 Naive Bayes Classifier](#5.4)\n    * [5.5 XGBoost](#5.5)    \n    \n* [6. Accuracy comparison](#6) \n     ","ee3a66bc":" <a id=\"3\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Exploratory Data Analysis<\/p>","63718f6b":" <a id=\"5.4\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Naive Bayes Classifier<\/p>","12abf0fa":" <a id=\"5\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Modeling Classification algorithms<\/p>","4d6f218d":"<p style=\"background-color:#44d180;font-family:newtimeroman;font-size:18px;line-height:1.7em;text-align:center;border-radius:5px 5px\">In the crop recommendation, the user can provide the parameters like N-P-K, temperature, humidity, pH value, rainfall, crop from their side and the application will predict which crop should the user grow.<\/p>","9413b11b":" <a id=\"5.1\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:120%;text-align:center;border-radius:60px 40px;\">K-Nearest Neighbors<\/p>","6f281787":" <a id=\"5.1.1\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:120%;text-align:center;border-radius:60px 40px;\">Hyperparameter Tuning<\/p>","bdfa604c":" <a id=\"1\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Importing Libraries<\/p>","3e8b9ce9":" <a id=\"6\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Accuracy comparison<\/p>","11c4e0bd":" <a id=\"4\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Feature Selection<\/p>","8868ffb5":"<blockquote><p style=\"font-size:18px; color:#159364; font-family:verdana;\">\n<code>Nitrogen (N)<\/code> \u2013 Nitrogen is largely responsible for the growth of leaves on the plant.<\/br>\n<code>Phosphorus (P)<\/code> \u2013 Phosphorus is largely responsible for root growth and flower and fruit development.<\/br>\n<code>Potassium (K)<\/code> \u2013 Potassium is a nutrient that helps the overall functions of the plant perform correctly.<\/br><\/blockquote>","363328a3":" <a id=\"2\"><\/a>\n# <p style=\"background-color:#44d180;font-family:roboto;color:#0a0a0b;font-size:150%;text-align:center;border-radius:60px 40px;\">Loading data<\/p>"}}