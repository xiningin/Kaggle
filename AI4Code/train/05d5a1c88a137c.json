{"cell_type":{"d5f2e3bc":"code","17969c25":"code","075f97dc":"code","f7795142":"code","660b5b73":"code","1752c880":"code","53f98791":"code","6a7cfc30":"code","039e7ae4":"code","73e46f09":"code","b55dd0d6":"code","e94caf98":"code","70ecc03f":"code","7d93bcfb":"code","9e51de2f":"code","2d09c627":"code","0fe4e103":"code","fb3a78a3":"code","454d6800":"code","5d51acf2":"code","760f5c34":"code","11fcf009":"code","f947517e":"code","e44e47b5":"code","28eb5cbf":"markdown"},"source":{"d5f2e3bc":"## libraries\nimport numpy as np\nimport pandas as pd\nfrom fastai.text import *\nfrom pathlib import Path","17969c25":"## create directory and path for models\nif not os.path.isdir('..\/model'):\n    os.makedirs('..\/model')\n    \npath_model = Path(\"..\/model\")","075f97dc":"## read in datasets\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/sample_submission.csv')","f7795142":"## create databunch with both train and test text and label for language model\nbs = 48\ndata_lm = (TextList.from_df(pd.concat([train[['text']], test[['text']]], ignore_index=True, axis=0), path_model)\n           .split_by_rand_pct(0.1)\n           .label_for_lm()\n           .databunch(bs=bs))","660b5b73":"## check tokenisation looks ok on training set\ndata_lm.show_batch()","1752c880":"## create lm learner with pre-trained model\nlearn = language_model_learner(data_lm, AWD_LSTM, drop_mult=0.3)","53f98791":"## run lr finder\nlearn.lr_find()","6a7cfc30":"## plot lr finder\nlearn.recorder.plot(skip_end=15)","039e7ae4":"## train for one epoch frozen\nlearn.fit_one_cycle(1, 1.3e-2, moms=(0.8,0.7))","73e46f09":"## unfreeze and train for four further cycles unfrozen\nlearn.unfreeze()\nlearn.fit_one_cycle(4, 1e-3, moms=(0.8,0.7))","b55dd0d6":"## save model and encoder\nlearn.save('fine_tuned')\nlearn.save_encoder('fine_tuned_enc')","e94caf98":"## training set with text and target\ndf = train[['text', 'target']]","70ecc03f":"## test set with text\ndf_test = test[['text']]","7d93bcfb":"## create databunch for classification task, \n## including randomly selected validation set, and test set\nbs = 16\ndata_clas = (TextList.from_df(df, path_model, vocab=data_lm.vocab)\n             #.split_none()\n             .split_by_rand_pct(0.1)\n             .label_from_df('target')\n             .add_test(TextList.from_df(df_test, path_model, vocab=data_lm.vocab))\n             .databunch(bs=bs))","9e51de2f":"## check test set looks ok\ndata_clas.show_batch(ds_type=DatasetType.Test)","2d09c627":"## create classification learning, including f1 score in metrics, and add encoder\nlearn = text_classifier_learner(data_clas, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, FBeta(beta=1)])\nlearn.load_encoder('fine_tuned_enc')","0fe4e103":"## run lr finder\nlearn.lr_find()","fb3a78a3":"## plot lr finder\nlearn.recorder.plot()","454d6800":"## train for 1 cycle frozen\nlearn.fit_one_cycle(1, 2e-2, moms=(0.8,0.7))","5d51acf2":"## unfreeze the last 2 layers and train for 1 cycle\nlearn.freeze_to(-2)\nlearn.fit_one_cycle(1, slice(1e-2\/(2.6**4),1e-2), moms=(0.8,0.7))","760f5c34":"## unfreeze the last 3 layers and train for 1 cycle\nlearn.freeze_to(-3)\nlearn.fit_one_cycle(1, slice(5e-3\/(2.6**4),5e-3), moms=(0.8,0.7))","11fcf009":"## unfreeze all and train for 2 cycles\nlearn.unfreeze()\nlearn.fit_one_cycle(2, slice(1e-3\/(2.6**4),1e-3), moms=(0.8,0.7))","f947517e":"## get test set predictions and ids\npreds, _ = learn.get_preds(ds_type=DatasetType.Test,  ordered=True)\npreds = preds.argmax(dim=-1)\n\nid = test['id']","e44e47b5":"my_submission = pd.DataFrame({'id': id, 'target': preds})\nmy_submission.to_csv('submission.csv', index=False)","28eb5cbf":"I have used the [ULMFiT](https:\/\/arxiv.org\/abs\/1801.06146) method on the task. It comprises the following training steps:\n\n- Take a language model (LM) pre-trained on a large general domain corpus, in this case an AWD-LSTM trained on WikiText-103\n- Fine-tune the LM on the task text (train and test) using the various fastai training tricks including traingular slanted learning rates\n- Further fine-tune on the classification task using gradual unfreezing of layers\n\nI've managed to get up to 0.818 F1, but this varies significantly between runs."}}