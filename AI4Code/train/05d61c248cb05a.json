{"cell_type":{"5c15a0f2":"code","484d42af":"code","bfd30d99":"code","a6544f77":"code","e961f1e3":"code","7de6a295":"code","478d0fa6":"code","eaa0cee4":"code","0430c710":"code","e9c2e550":"markdown","aeb3d6f7":"markdown","700b6e7a":"markdown","13d34956":"markdown","706c7b36":"markdown","a5b968ed":"markdown"},"source":{"5c15a0f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","484d42af":"from sklearn.metrics import f1_score","bfd30d99":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","a6544f77":"df_train.target.value_counts(normalize=True)","e961f1e3":"len(df_train)","7de6a295":"f1_score(df_train.target.values, np.ones(len(df_train)))","478d0fa6":"df_test['prediction'] = 1\ndf_test = df_test[['qid', 'prediction']]\n\ndf_test.to_csv('submission.csv', index=False)","eaa0cee4":"positive_ratio = 0.113 \/ (2 - 0.113)\n\nprint(positive_ratio)","0430c710":"0.11653 \/ (2 - 0.11653)","e9c2e550":"Let's now do the same for the test set. I submitted this in version 1 of this kernel","aeb3d6f7":"phew :)","700b6e7a":"This gets a score of `0.113` - which obviously means the distribution of target values is different between the test and training sets, but how different?\n\nwell since the f1 score is defined as\n\n$$f1 = {{2 * precision * recall}\\over{precision + recall}}$$\n\nwhich is the same as\n\n$$ f1 = {{2 TP}\\over{2TP + FP + FN}}$$\n\nbecause we labelled everything as positive, we know $FP = \\sum - TP$ (where $\\sum$ is the total number of results) and $FN = 0$, this becomes\n\n$$ f1 = {{2 TP}\\over{TP + \\sum}}$$\n\nafter some rearraging we get\n\n$${TP\\over{\\sum}} = {{f1}\\over{2 - f1}}$$\n\nie we can relate the ratio of positive labells to the f1 score\n\nDoing this we get","13d34956":"# Submission distributions\n\nOne of the interesting things I've noticed about this competition is how different my prediction of f1 score is from the LB score, in this kernel I wanted to examine this a bit further\n\n**TL; DR:** the training set and test set have slightly different distrubutions of target values. You should take this into account when you train your model","706c7b36":"the training set positive ratio is `0.06187` as shown above\n\nThis isn't a huge difference, but if there's a similar difference between the test set and the private test set it could really mix up the leaderboard!\n\n\n## Appendix: sanity check :)","a5b968ed":"First, lets do the dumbest thing and label everything as positive in our training set, then see what f1 score we get"}}