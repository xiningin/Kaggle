{"cell_type":{"8c3fd864":"code","e067ea09":"code","adbb1b31":"code","b2679eee":"code","f8fde5bb":"code","a499db89":"code","27cf905c":"code","3ef0e2da":"code","b3da7ec8":"code","01718b28":"code","9dc4bc42":"code","22f9d1cc":"code","68fd0294":"code","e704999c":"code","3894b906":"code","20f89c8d":"code","da985972":"code","d16be18c":"code","c48c5961":"code","276460f5":"code","5e214fd4":"code","6af04961":"code","5de290f0":"code","5e4d7556":"code","95766f46":"code","c3bedb50":"code","b6571434":"code","05711709":"code","785ee0a5":"code","339e15ed":"code","d9bde622":"code","55e1c822":"code","7c6883d9":"code","b8720781":"code","eae0f34a":"code","af5240b4":"code","5bd71bb5":"code","984f7d2e":"code","b0390983":"code","71b2315c":"code","6b2e36c7":"code","1ac0194f":"code","2e897bbf":"code","dc9d9939":"code","2de4135c":"code","caef8714":"code","ad22094d":"code","79e940ea":"code","6406e168":"code","08758d5a":"code","ef922911":"code","a99cb2c8":"code","c05022bb":"code","3f87365c":"code","5fbd76be":"code","74e15120":"code","d9e3d483":"code","aa7a1ba4":"code","faecd190":"code","72a7caea":"code","20673622":"code","22e7e22f":"code","531bf0fa":"code","b1501f6b":"code","445a5f3f":"code","9f361aed":"code","c721545a":"code","4230b44a":"code","fe320e65":"code","cc087ef0":"code","784ff78a":"code","f7961f98":"code","776c01d1":"code","3ea1ed47":"code","67297c30":"code","9923be8b":"code","f286e7fa":"code","1ccc7238":"code","25622d04":"code","6298950f":"code","748f37a2":"code","d1945423":"code","6f9cc83c":"code","eb5dde38":"code","462eb982":"code","840d272d":"code","bb83f7a0":"code","b77365f4":"code","034a12c4":"code","eb49d069":"code","c135ba1c":"code","fe85ad1e":"code","447ef358":"code","2e3fb47c":"code","e15423cc":"code","d8358faf":"code","b93e50d3":"code","ca3ce807":"code","ce889fcf":"code","7fc9a9ec":"code","acd198f7":"code","74b1f457":"code","c1cdba17":"code","3d0b1e5b":"code","f60d444e":"code","397bd19b":"code","533f72dc":"code","d1e7e12c":"code","54e8b7cd":"code","0e385b66":"code","a6364376":"code","f58c142d":"code","d31274ae":"code","45537d91":"code","fdfced4e":"code","8dd15d17":"code","18fe51c6":"code","21054804":"code","5fcf7ba9":"code","c481e817":"code","7bb9e471":"code","65e5c1ec":"code","9c9b1581":"code","95e143f6":"code","9a2438ad":"code","b994d373":"code","b97ca718":"code","44295e61":"code","d9b9bc15":"code","6dfc1066":"code","f66d4438":"code","e4638ffc":"code","4da02930":"code","a29a4001":"code","b98116fa":"code","4c6d8721":"code","03fdd9b7":"code","07f1681f":"code","5b9d1511":"code","ded3cb9b":"code","7be8808b":"code","55228451":"code","eb4b52d7":"code","656dd36d":"code","ac3130cf":"code","4792166c":"code","cbebdce5":"code","9fb14a3e":"code","64cfd53f":"code","0389b016":"code","cdfc1ec1":"code","129ebf2d":"code","8ab4612d":"code","b240f5d9":"code","8c4f4332":"code","2f9f6f39":"code","d7cc30ac":"code","0c163f12":"code","e9666e2b":"code","111edf83":"code","f49eff9b":"code","a70adeb4":"code","a0297968":"code","2e5d8cea":"code","25f5adea":"code","19138861":"code","70d6c9ec":"code","9dafa3b5":"code","042047af":"code","83e0ada7":"code","935d7a75":"code","3a13fb80":"code","1f804e3f":"code","5ae10f87":"code","c55a368f":"code","bc41c27d":"code","f20f91f1":"code","3605b57f":"code","d2a4fca7":"code","110e40d0":"code","32d6302f":"code","e7437360":"code","aeeb78df":"code","7c811ec4":"code","1b049322":"code","64ef3e90":"code","a22d5a38":"code","954fcffe":"code","bcab4240":"code","321eddc2":"code","3982014f":"code","a617e1d7":"code","8682f3d6":"code","9ebb92f9":"code","c6be505b":"code","4ca85d57":"code","e418942d":"code","c720b88c":"code","9cd9b39a":"code","72835afe":"code","6dc5d50c":"code","5bf506de":"code","ec24721d":"code","9a4c4e0e":"code","2aa48c23":"code","c6b38efe":"code","1efb42f6":"code","b7886370":"code","1a4430df":"code","0af98c11":"code","fd373e6a":"code","134b13fe":"code","f5457d72":"code","850cf2db":"code","3af464ed":"code","68f05d8c":"code","5a5c0fcc":"code","606848c0":"code","e3ef8e07":"code","91998953":"code","46ebe44b":"code","716d5341":"code","bbc4d27e":"code","df570c8e":"markdown","69ee0677":"markdown","06e2d301":"markdown","969a09c1":"markdown","8fa8845c":"markdown","937d3bbc":"markdown","798f4325":"markdown","3f39f4be":"markdown","1315fd91":"markdown","127ae55c":"markdown","ce07afc4":"markdown","35fb5a56":"markdown","93ee0276":"markdown","72241b29":"markdown","945f290b":"markdown","25f880ce":"markdown","e8e8e108":"markdown","48a0936b":"markdown","e74bc1d6":"markdown","264c07e9":"markdown","08b3bd97":"markdown","87e4f0cd":"markdown","9e320ed0":"markdown","b684a101":"markdown","933b876a":"markdown","b657df6f":"markdown","50deb68c":"markdown","557b5a93":"markdown","4b18cde7":"markdown","71dddd14":"markdown","84669644":"markdown","38e8983b":"markdown","a6865a8f":"markdown","4b69f66b":"markdown","7167d39a":"markdown","f1de1a47":"markdown","c8382c8a":"markdown","714719c9":"markdown","5414d12a":"markdown","9385b839":"markdown","70a296e5":"markdown","9e737774":"markdown","0c268e0c":"markdown","d103391f":"markdown","d0753b70":"markdown","eba96e92":"markdown","100109e3":"markdown","4cc2f4f7":"markdown","5f5f9572":"markdown","1dde80de":"markdown","c6720449":"markdown","dc7c427e":"markdown","c88bc27c":"markdown","ffa8e01f":"markdown","d9d2cc5b":"markdown","3efefbe3":"markdown","a235c7bf":"markdown","da790215":"markdown","abbf5a64":"markdown","8969c380":"markdown","ad766199":"markdown","4476efd8":"markdown","a8a67945":"markdown","72d8ea75":"markdown","0e0f8942":"markdown","fc17773c":"markdown","f91d4165":"markdown","672742fb":"markdown","3a026477":"markdown","764c4ef7":"markdown","ab4780ab":"markdown","b3949290":"markdown","b3646757":"markdown","8df01e1e":"markdown","c84e4625":"markdown","ed8f5171":"markdown","e3cea45f":"markdown","08180f61":"markdown","c531d82d":"markdown","ee1a7f87":"markdown","337ef6a6":"markdown","b5c1badf":"markdown","ae7bdb57":"markdown"},"source":{"8c3fd864":"import pandas as pd \npd.set_option('display.float_format', lambda x: '%.5f' % x)\nimport numpy as np\nfrom matplotlib import pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom IPython.display import Image\nfrom catboost import CatBoostClassifier, Pool\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.simplefilter(action='ignore', category=DeprecationWarning)","e067ea09":"approved = pd.read_csv('..\/input\/lending-club\/accepted_2007_to_2018q4.csv\/accepted_2007_to_2018Q4.csv')\ndescr = pd.read_csv('..\/input\/lcdatadictionary\/LCDataDictionary.csv', index_col = 'columns')","adbb1b31":"approved.iloc[:, [0,19,49,59,118,129,130,131,134,135,136,139,145,146,147]].columns","b2679eee":"def meaning(column_name):\n    assert column_name in descr.index, 'There is no such column in the dataset!'\n    return descr.loc[column_name].description\n\ndef gini(fpr, tpr):\n    \"\"\"\n    Function calculates Gini coefficient.\n    fpr - the vector of defaults;\n    tpr - the vector of variable values.\n    \"\"\"\n    return -(2 * roc_auc_score(fpr, tpr) - 1)\n\ndef check_gini(data, cols, target):\n    gini_df = {}\n    for i in cols:\n        if data[i].dtype == 'object':\n            vals = data[i].astype('category').cat.codes.values\n        else:\n            vals = data[i].values\n        gini_df[i] = gini(data[target].values, vals)\n    gini_df = pd.DataFrame.from_dict(gini_df, orient = 'index',columns = ['gini'])\n    gini_df['gini_abs'] = abs(gini_df['gini'])\n    gini_df = gini_df.sort_values('gini_abs', ascending = False)\n    return gini_df","f8fde5bb":"print(f'Dataset consists of {approved.shape[0]} records and {approved.shape[1]} features.')","a499db89":"cols_to_drop = ['id', 'member_id', 'funded_amnt_inv', 'pymnt_plan', 'url', 'initial_list_status', 'out_prncp_inv', 'total_pymnt',\n                'total_pymnt_inv', 'last_pymnt_d', 'last_pymnt_amnt', 'next_pymnt_d', 'last_credit_pull_d', 'last_fico_range_high', \n                'last_fico_range_low', 'policy_code', 'earliest_cr_line', 'hardship_flag', 'hardship_type', 'hardship_reason', 'hardship_status', \n                'deferral_term', 'hardship_amount', 'hardship_start_date', 'hardship_end_date', 'payment_plan_start_date', 'hardship_length', \n                'hardship_dpd', 'hardship_loan_status', 'orig_projected_additional_accrued_interest', 'hardship_payoff_balance_amount', \n                'hardship_last_payment_amount',   'debt_settlement_flag', 'debt_settlement_flag_date', 'settlement_status', 'settlement_date',\n                'settlement_amount', 'settlement_percentage', 'settlement_term', 'verification_status', 'verification_status_joint', 'disbursement_method']","27cf905c":"meaning('id')","3ef0e2da":"approved[~approved.member_id.isna()].shape","b3da7ec8":"len(approved.id.unique()) == approved.shape[0]","01718b28":"df = approved[approved.columns[~approved.columns.isin(cols_to_drop)]].copy()","9dc4bc42":"df = df[~df.term.isna()] # droping records with missed term\ndf['term'] = df['term'].map({' 36 months': 36, ' 60 months': 60}).astype('int32')","22f9d1cc":"received = (\n    df['total_rec_prncp']\n    + df['total_rec_int']\n    + df['total_rec_late_fee']\n    + df['recoveries']\n)\nexpected = df['installment'] * df['term']\ndf['fraction_recovered'] = received \/ expected\ndf.groupby('loan_status')['fraction_recovered'].describe()","68fd0294":"cols_utils = ['out_prncp', 'total_rec_prncp', 'total_rec_int', 'total_rec_late_fee', 'recoveries', \\\n      'collection_recovery_fee', 'funded_amnt', 'fraction_recovered', 'issue_d', 'loan_status']","e704999c":"# converting issue date to date format\ndf['issue_d'] = pd.to_datetime(df['issue_d'])","3894b906":"df.groupby('issue_d')['loan_amnt'].count().plot(title = 'Loans volume in time', figsize = (15, 8));","20f89c8d":"df.pivot_table(index = 'issue_d', columns = 'loan_status', values = 'loan_amnt', aggfunc = 'count',).plot(figsize = (15, 8), \\\n                                                                                        title = 'Loans volume in time by loan_status');","da985972":"# drop records with unknown issue date\ndf = df[~df.issue_d.isna()]\n\n# drop records with loan_status null\ndf = df[~df.loan_status.isna()]","d16be18c":"df['def_flag'] = df['loan_status'].map({\n    'Fully Paid': 0,\n    'Current': 0,\n    'Charged Off': 1,\n    'Late (31-120 days)': 1,\n    'In Grace Period': 0,\n    'Late (16-30 days)': 0,\n    'Does not meet the credit policy. Status:Fully Paid': 0,\n    'Does not meet the credit policy. Status:Charged Off': 1,\n    'Default': 1\n    \n})","c48c5961":"df.loan_status.value_counts(normalize = True)","276460f5":"df[df.def_flag.isna()]","5e214fd4":"df.groupby('issue_d')['def_flag'].mean().plot(figsize = (15, 8), title ='Default Rate');","6af04961":"print(df.def_flag.value_counts(normalize = True))\nsns.countplot(df.def_flag);","5de290f0":"status_to_drop = ['Current', 'Late (16-30 days)', 'In Grace Period']\ndf = df[~df.loan_status.isin(status_to_drop)]","5e4d7556":"df.groupby('issue_d')['def_flag'].mean().plot(figsize = (15, 8), title ='Default Rate');","95766f46":"print(df.def_flag.value_counts(normalize = True))\nsns.countplot(df.def_flag);","c3bedb50":"df.groupby('issue_d')['loan_amnt'].count().plot(figsize = (15, 8), title = 'Loans volume in time');","b6571434":"missings = df.isnull().sum()\/df.shape[0]\nmissings = list(missings[missings.values > 0.1].index)","05711709":"informative = ['mths_since_last_record', 'mths_since_last_major_derog', 'mths_since_recent_bc_dlq', 'mths_since_recent_revol_delinq',\\\n              'mths_since_rcnt_il', 'all_util', 'inq_last_12m', 'open_acc_6m', 'open_il_24m', 'total_bal_il', 'open_rv_12m', 'open_rv_24m',\\\n              'max_bal_bc', 'open_il_12m', 'open_act_il', 'inq_fi', 'mths_since_last_delinq', 'mths_since_recent_inq']\ndrop_miss = [x for x in missings if x not in informative]\nlen(drop_miss)","785ee0a5":"df = df[[x for x in df.columns if x not in drop_miss]]","339e15ed":"grade_related = ['grade', 'sub_grade', 'int_rate', 'installment']\n\nfacility = ['loan_amnt', 'term', 'purpose', 'title', 'application_type']\n\nscoring = ['fico_range_low', 'fico_range_high',  'pct_tl_nvr_dlq']\n\ncust_rel = ['emp_title', 'emp_length', 'home_ownership', 'annual_inc', 'dti', 'zip_code', 'addr_state', 'tax_liens']\n\ndelinq = ['delinq_2yrs', 'mths_since_last_delinq', 'mths_since_last_major_derog', 'acc_now_delinq', \\\n            'tot_coll_amt', 'chargeoff_within_12_mths', 'delinq_amnt', 'mths_since_recent_bc_dlq', 'mths_since_recent_revol_delinq',\\\n            'num_accts_ever_120_pd', 'num_tl_120dpd_2m', 'num_tl_30dpd', 'num_tl_90g_dpd_24m', 'pub_rec_bankruptcies', 'pub_rec', \\\n            'collections_12_mths_ex_med']\n\ninq_rel = ['inq_last_6mths', 'inq_fi', 'inq_last_12m', 'mths_since_recent_inq', 'mths_since_last_record']\n           \nacc_rel = ['open_acc', 'open_acc_6m', 'total_acc', 'open_act_il', 'open_il_12m', 'open_il_24m', 'open_rv_12m', 'open_rv_24m',\\\n           'acc_open_past_24mths', 'mo_sin_old_il_acct', 'mo_sin_old_rev_tl_op', 'mo_sin_rcnt_rev_tl_op', 'mort_acc', 'mo_sin_rcnt_tl',\\\n            'mths_since_recent_bc', 'num_actv_bc_tl', 'num_actv_rev_tl', 'num_sats', 'num_bc_sats', 'num_bc_tl', 'num_il_tl', 'num_op_rev_tl',\\\n           'num_rev_accts', 'num_rev_tl_bal_gt_0', 'num_tl_op_past_12m', 'mths_since_rcnt_il']\n\nbal_rel = ['tot_cur_bal', 'total_bal_il', 'max_bal_bc', 'avg_cur_bal', 'percent_bc_gt_75', 'tot_hi_cred_lim', 'total_bal_ex_mort',\\\n            'total_bc_limit', 'total_il_high_credit_limit', 'revol_bal', 'revol_util', 'all_util', 'total_rev_hi_lim', 'bc_util', 'bc_open_to_buy']","d9bde622":"categorical = []\nordinal = []\nto_drop = []","55e1c822":"df[facility].dtypes","7c6883d9":"df.purpose.unique()","b8720781":"df.title.unique()","eae0f34a":"facility.remove('title')\nto_drop.append('title')","af5240b4":"fig, axes = plt.subplots(2, 2, figsize = (10, 10))\nsns.distplot(df.loan_amnt, ax = axes[0][0])\nsns.countplot(df.term, ax = axes[0][1])\nsns.distplot(df.int_rate, ax = axes[1][0])\nsns.distplot(df.installment, ax = axes[1][1]);","5bd71bb5":"df[facility].isnull().sum()","984f7d2e":"categorical.append('term')","b0390983":"chart = sns.countplot(df.purpose)\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90);","71b2315c":"purpose_map = {\n    'debt_consolidation': 'debt_consolidation', \n    'small_business': 'small_business', \n    'home_improvement': 'home_improvement',\n    'major_purchase': 'major_purchase', \n    'credit_card': 'credit_card', \n    'other': 'other', \n    'house': 'home_improvement', \n    'vacation': 'other',\n    'car': 'other', \n    'medical': 'other', \n    'moving': 'other', \n    'renewable_energy': 'other', \n    'wedding': 'other',\n    'educational': 'other'\n}","6b2e36c7":"df['purpose'] = df.purpose.map(purpose_map)","1ac0194f":"chart = sns.countplot(df.purpose)\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90);","2e897bbf":"categorical.append('purpose')","dc9d9939":"df.groupby('application_type').def_flag.mean().plot(kind = 'bar');","2de4135c":"df.application_type.value_counts(normalize=True)","caef8714":"categorical.append('application_type')","ad22094d":"check_gini(df, facility, 'def_flag')","79e940ea":"df[facility].corr()","6406e168":"df[scoring].dtypes","08758d5a":"df[grade_related].dtypes","ef922911":"df[grade_related].isnull().sum()","a99cb2c8":"df[scoring].isnull().sum()","c05022bb":"fig, axes = plt.subplots(3, 2, figsize = (15, 16))\nsns.countplot(sorted(df.grade), ax = axes[0][0])\nsns.countplot(sorted(df.sub_grade), ax = axes[0][1])\nsns.distplot(df[df.def_flag == 0].fico_range_high, ax = axes[1][0])\nsns.distplot(df[df.def_flag == 1].fico_range_high, ax = axes[1][0])\nsns.distplot(df[df.def_flag == 0].fico_range_low, ax = axes[1][1])\nsns.distplot(df[df.def_flag == 1].fico_range_low, ax = axes[1][1])\nsns.distplot(df[df.def_flag == 0].pct_tl_nvr_dlq, ax = axes[2][0])\nsns.distplot(df[df.def_flag == 1].pct_tl_nvr_dlq, ax = axes[2][0])\naxes[0][1].set_xticklabels(axes[0][1].get_xticklabels(), fontsize = 6);","3f87365c":"df.groupby('grade').def_flag.mean().plot(kind = 'bar');","5fbd76be":"df.groupby('sub_grade').def_flag.mean().plot(kind = 'bar');","74e15120":"df[scoring + grade_related].corr()","d9e3d483":"scoring.remove('fico_range_high')\nto_drop.append('fico_range_high')","aa7a1ba4":"meaning('pct_tl_nvr_dlq')","faecd190":"df['pct_tl_nvr_dlq'] = df['pct_tl_nvr_dlq'].fillna(0)","72a7caea":"check_gini(df, scoring, 'def_flag')","20673622":"df[cust_rel].isnull().sum()","22e7e22f":"len(df.emp_title.unique())","531bf0fa":"cust_rel.remove('emp_title')\nto_drop.append('emp_title')","b1501f6b":"df.emp_length.value_counts()","445a5f3f":"emp_length_map = {\n    '< 1 year': 0,\n    '1 year': 1,\n    '2 years': 2,\n    '3 years': 3,\n    '4 years': 4,\n    '5 years': 5,\n    '6 years': 6,\n    '7 years': 7,\n    '8 years': 8,\n    '9 years': 9,\n    '10+ years': 10,\n}","9f361aed":"df.emp_length = df.emp_length.map(emp_length_map).fillna(0).astype('int32')","c721545a":"fig, axes = plt.subplots(1, 2, figsize = (12, 6))\nsns.countplot(df.emp_length, ax = axes[0])\ndf.groupby('emp_length')['def_flag'].mean().plot(title = 'Default rate train', ax = axes[1])\naxes[0].set_title('Employment length in years')\nplt.tight_layout();","4230b44a":"ordinal.append('emp_length')","fe320e65":"print(df['home_ownership'].value_counts())\nsns.countplot(df['home_ownership']);","cc087ef0":"home_ownership_map = {\n    'MORTGAGE': 'MORTGAGE',\n    'OWN': 'OWN',\n    'RENT': 'RENT',\n    'ANY': 'RENT',\n    'NONE': 'RENT',\n    'OTHER': 'RENT'\n}","784ff78a":"df['home_ownership'] = df.home_ownership.map(home_ownership_map)","f7961f98":"categorical.append('home_ownership')","776c01d1":"fig, axes = plt.subplots(1, 2, figsize = (10, 5))\nsns.distplot(df.annual_inc, ax = axes[0])\nsns.distplot(df.dti, ax = axes[1])\naxes[0].set_title('Annual income distribution')\naxes[1].set_title('Debt-to-income ratio')\nplt.tight_layout();","3ea1ed47":"df[['annual_inc', 'dti']].describe()","67297c30":"df.sort_values('annual_inc', ascending = False)[['emp_title', 'annual_inc']].head(20)","9923be8b":"len(df.zip_code.unique())","f286e7fa":"cust_rel.remove('zip_code')\nto_drop.append('zip_code')","1ccc7238":"len(df.addr_state.unique())","25622d04":"fig = plt.figure(figsize = (10, 8) )\nchart = sns.countplot(df.addr_state.sort_values())\nchart.set_xticklabels(chart.get_xticklabels(), rotation = 90, fontsize = 8);","6298950f":"cust_rel.remove('addr_state')\nto_drop.append('addr_state')","748f37a2":"df.tax_liens.describe([.85, .9, .95, .99])","d1945423":"cust_rel.remove('tax_liens')\nto_drop.append('tax_liens')","6f9cc83c":"df[cust_rel].isnull().sum()","eb5dde38":"df[cust_rel].corr()","462eb982":"(df[delinq].isnull().sum()\/df.shape[0]).sort_values(ascending = False)","840d272d":"df[delinq].dtypes","bb83f7a0":"meaning('chargeoff_within_12_mths')","b77365f4":"df.loc[:, 'mths_since_recent_dlq'] = df[['mths_since_recent_bc_dlq', 'mths_since_last_major_derog', \\\n                                         'mths_since_recent_revol_delinq', 'mths_since_last_delinq']].min(axis = 1)","034a12c4":"meaning('mths_since_last_major_derog')","eb49d069":"for i in ['mths_since_recent_bc_dlq', 'mths_since_last_major_derog', 'mths_since_recent_revol_delinq', 'mths_since_last_delinq']:\n    delinq.remove(i)\n    to_drop.append(i)","c135ba1c":"delinq.append('mths_since_recent_dlq')","fe85ad1e":"ordinal.append('mths_since_recent_dlq')","447ef358":"delinq.remove('num_tl_120dpd_2m')\ndelinq.remove('num_tl_30dpd')\nto_drop += ['num_tl_120dpd_2m', 'num_tl_30dpd']","2e3fb47c":"ordinal.append('num_tl_90g_dpd_24m')\nordinal.append('num_accts_ever_120_pd')\nordinal.append('delinq_2yrs')","e15423cc":"df[['tot_coll_amt', 'collections_12_mths_ex_med', 'chargeoff_within_12_mths', 'delinq_amnt',\\\n         'pub_rec_bankruptcies', 'pub_rec', 'acc_now_delinq']].describe([.01,.1,.2,.3,.4,.5,.6,.7,.8,.9, .95, .99])","d8358faf":"sns.boxplot(df.tot_coll_amt);","b93e50d3":"for i in ['tot_coll_amt', 'collections_12_mths_ex_med', 'chargeoff_within_12_mths', 'delinq_amnt', 'acc_now_delinq']:\n    delinq.remove(i)\n    to_drop.append(i)","ca3ce807":"df['flag_pub_rec_bankruptcies'] = (df['pub_rec_bankruptcies'] > 0).astype('int32').fillna(1)","ce889fcf":"delinq.append('flag_pub_rec_bankruptcies')\ncategorical.append('flag_pub_rec_bankruptcies')\ndelinq.remove('pub_rec_bankruptcies')\nto_drop.append('pub_rec_bankruptcies')","7fc9a9ec":"df['flag_pub_rec'] = (df['pub_rec'] > 0).astype('int32').fillna(1)","acd198f7":"delinq.append('flag_pub_rec')\ncategorical.append('flag_pub_rec')\ndelinq.remove('pub_rec')\nto_drop.append('pub_rec')","74b1f457":"fig, axes = plt.subplots(2, 2, figsize = (10, 8))\nsns.countplot(df.flag_pub_rec_bankruptcies, ax = axes[0][0])\ndf.sort_values('flag_pub_rec_bankruptcies').groupby('flag_pub_rec_bankruptcies')['def_flag'].mean().plot(kind = 'bar', \\\n                                                                                                               title = 'Default rate', ax = axes[0][1])\nsns.countplot(df.flag_pub_rec, ax = axes[1][0])\ndf.sort_values('flag_pub_rec').groupby('flag_pub_rec')['def_flag'].mean().plot(kind = 'bar', \\\n                                                                                            title = 'Default rate', ax = axes[1][1])\nplt.tight_layout();","c1cdba17":"df[delinq].corr()","3d0b1e5b":"(df[inq_rel].isnull().sum()\/df.shape[0]).sort_values(ascending = False)","f60d444e":"df[inq_rel].dtypes","397bd19b":"df[~df.inq_last_12m.isna()].groupby('issue_d').loan_amnt.count().plot();","533f72dc":"df[~df.inq_fi.isna()].groupby('issue_d').loan_amnt.count().plot();","d1e7e12c":"to_drop += ['inq_last_12m', 'inq_fi']\ninq_rel.remove('inq_last_12m')\ninq_rel.remove('inq_fi')","54e8b7cd":"df['inq_last_6mths'] = df['inq_last_6mths'].fillna(0).astype('int32')","0e385b66":"ordinal.append('inq_last_6mths')","a6364376":"ordinal.append('mths_since_recent_inq')","f58c142d":"df['flag_last_record'] = (df['mths_since_last_record'].isna() == False).astype('int32')","d31274ae":"to_drop.append('mths_since_last_record')\ninq_rel.remove('mths_since_last_record')\ncategorical.append('flag_last_record')\ninq_rel.append('flag_last_record')","45537d91":"df[inq_rel].corr()","fdfced4e":"(df[acc_rel].isnull().sum()\/df.shape[0]).sort_values(ascending = False)","8dd15d17":"meaning('mths_since_rcnt_il')","18fe51c6":"for i in ['open_acc_6m', 'open_act_il', 'open_il_12m', 'open_il_24m', 'open_rv_12m', 'open_rv_24m']:\n    to_drop.append(i)\n    acc_rel.remove(i)","21054804":"df[acc_rel].dtypes","5fcf7ba9":"df[acc_rel].describe()","c481e817":"ordinal += acc_rel","7bb9e471":"(df[bal_rel].isnull().sum()\/df.shape[0]).sort_values(ascending = False)","65e5c1ec":"for i in ['all_util', 'max_bal_bc', 'total_bal_il']:\n    to_drop.append(i)\n    bal_rel.remove(i)","9c9b1581":"df[bal_rel].describe()","95e143f6":"df[bal_rel] = df[bal_rel].fillna(0)","9a2438ad":"df[bal_rel].dtypes","b994d373":"check_gini(df, bal_rel, 'def_flag')","b97ca718":"df2 = df[df.columns[~df.columns.isin(to_drop)]].copy()","44295e61":"features = [x for x in df2.columns if x not in cols_utils and x not in grade_related]","d9b9bc15":"features.remove('def_flag')\nto_use = features + cols_utils + grade_related","6dfc1066":"train_val = df2.loc[df2['issue_d'] <  df2['issue_d'].quantile(0.9)].copy()\ntest_df =  df2.loc[df2['issue_d'] >= df2['issue_d'].quantile(0.9)].copy()","f66d4438":"train_val = train_val[(train_val.annual_inc >= 10000) & (train_val.annual_inc <= 500000)]\ntrain_val = train_val[(train_val.dti >= 0) & (train_val.dti <= 80)]","e4638ffc":"train_df = train_val.loc[train_val['issue_d'] <  train_val['issue_d'].quantile(0.8)].copy()\nval_df =  train_val.loc[train_val['issue_d'] >= train_val['issue_d'].quantile(0.8)].copy()","4da02930":"print(test_df.shape, train_df.shape, val_df.shape)\nprint(test_df.shape[0]\/df2.shape[0], train_df.shape[0]\/df2.shape[0], val_df.shape[0]\/df2.shape[0])","a29a4001":"print(test_df.issue_d.min(), test_df.issue_d.max())\nprint(train_df.issue_d.min(), train_df.issue_d.max())\nprint(val_df.issue_d.min(), val_df.issue_d.max())","b98116fa":"X_train = train_df[to_use].copy()\ny_train = train_df['def_flag'].copy()\nX_val = val_df[to_use].copy()\ny_val = val_df['def_flag'].copy()\nX_test = test_df[to_use].copy()\ny_test = test_df['def_flag'].copy()","4c6d8721":"print(X_test.shape, X_train.shape, X_val.shape)","03fdd9b7":"(X_train[features].isnull().sum()\/X_train[features].shape[0]).sort_values(ascending = False)","07f1681f":"X_train['annual_inc'] = X_train['annual_inc'].fillna(X_train['annual_inc'].median())\nX_train['dti'] = X_train['dti'].fillna(X_train['dti'].median())\nX_val['annual_inc'] = X_val['annual_inc'].fillna(X_train['annual_inc'].median())\nX_val['dti'] = X_val['dti'].fillna(X_train['dti'].median())\nX_test['annual_inc'] = X_test['annual_inc'].fillna(X_train['annual_inc'].median())\nX_test['dti'] = X_test['dti'].fillna(X_train['dti'].median())","5b9d1511":"X_train[[x for x in features if 'num' in x]].describe()","ded3cb9b":"X_train[[x for x in features if 'num' in x]] = X_train[[x for x in features if 'num' in x]].fillna(0)\nX_test[[x for x in features if 'num' in x]] = X_test[[x for x in features if 'num' in x]].fillna(0)\nX_val[[x for x in features if 'num' in x]] = X_val[[x for x in features if 'num' in x]].fillna(0)","7be8808b":"for i in [x for x in features if 'mo_' in x or 'mths_' in x]:\n    X_train[i] = X_train[i].fillna(X_train[i].max())\n    X_test[i] = X_test[i].fillna(X_train[i].max())\n    X_val[i] = X_val[i].fillna(X_train[i].max())","55228451":"X_train[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']]\\\n                        = X_train[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']].fillna(0)\n\nX_test[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']]\\\n                        = X_test[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']].fillna(0)\n\nX_val[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']]\\\n                        = X_val[['mort_acc', 'acc_open_past_24mths', 'total_acc', 'delinq_2yrs', 'open_acc']].fillna(0)","eb4b52d7":"print(X_train.isnull().values.any())\nprint(X_test.isnull().values.any())\nprint(X_val.isnull().values.any())","656dd36d":"X_train[categorical].dtypes","ac3130cf":"X_train[ordinal].dtypes","4792166c":"for i in ordinal:\n    X_train[i] = X_train[i].astype('int32')\n    X_test[i] = X_test[i].astype('int32')\n    X_val[i] = X_val[i].astype('int32')","cbebdce5":"X_train[[x for x in features if x not in categorical and x not in ordinal]].dtypes","9fb14a3e":"X_train['lti'] = X_train['loan_amnt']*(12\/X_train['term'])\/X_train['annual_inc']\nX_train['lti'] = X_train['lti'].clip(upper = 2)","64cfd53f":"X_val['lti'] = X_val['loan_amnt']*(12\/X_val['term'])\/X_val['annual_inc']\nX_val['lti'] = X_val['lti'].clip(upper = 2)","0389b016":"X_test['lti'] = X_test['loan_amnt']*(12\/X_test['term'])\/X_test['annual_inc']\nX_test['lti'] = X_test['lti'].clip(upper = 2)","cdfc1ec1":"X_train['lti'].describe()","129ebf2d":"fig, ax = plt.subplots(1, 3, figsize = (15, 5))\nsns.distplot(X_train['lti'], ax = ax[0])\nsns.distplot(X_val['lti'], ax = ax[1])\nsns.distplot(X_test['lti'], ax = ax[2])\nax[0].set_title('Train')\nax[1].set_title('Val')\nax[2].set_title('Test')\nplt.tight_layout();","8ab4612d":"features.append('lti')\nto_use.append('lti')","b240f5d9":"gini_df = check_gini(pd.concat([X_train[features], y_train], axis = 1), \\\n                     features, 'def_flag')\ngini_df.head()","8c4f4332":"# removing variables with low disriminatory power.\nfeatures = [x for x in features if x not in gini_df[gini_df['gini_abs'] < 0.02].index]","2f9f6f39":"gini_df = gini_df[gini_df['gini_abs'] >= 0.02]","d7cc30ac":"fig, ax = plt.subplots(figsize=(10,10))\nsns.heatmap(X_train[gini_df.index].corr(), ax=ax)\nplt.tight_layout();","0c163f12":"corr_matrix = X_train[gini_df.index].corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\nhigh_corr = [column for column in upper.columns if any(upper[column] > 0.70)]","e9666e2b":"len(high_corr)","111edf83":"features = [x for x in features if x not in high_corr]","f49eff9b":"to_use = [x for x in to_use if x not in high_corr]","a70adeb4":"categorical = [x for x in categorical if x not in high_corr and x in features]\nordinal = [x for x in ordinal if x not in high_corr and x in features]","a0297968":"len(features)","2e5d8cea":"len([x for x in features if x not in categorical and x not in ordinal]) + len(categorical) + len(ordinal)","25f5adea":"X_train = X_train[to_use]\nX_val = X_val[to_use]\nX_test = X_test[to_use]","19138861":"to_norm = [x for x in features if x not in categorical and x not in ordinal]\nto_norm","70d6c9ec":"sc = StandardScaler()\nX_train.loc[:, to_norm] = sc.fit_transform(X_train[to_norm])\nX_val.loc[:, to_norm] = sc.transform(X_val[to_norm])\nX_test.loc[:, to_norm] = sc.transform(X_test[to_norm])","9dafa3b5":"cat_params = {'random_seed': 17, 'eval_metric':'AUC', 'custom_loss':['AUC'], 'silent': True}\ncat = CatBoostClassifier(**cat_params)","042047af":"categ = [int(np.where(X_train[features].columns ==i)[0]) for i in categorical]\ncateg","83e0ada7":"train_data = Pool(data=X_train[features], label = y_train, cat_features=categ)\nval_data = Pool(data=X_val[features], label = y_val, cat_features=categ)","935d7a75":"%%time\ncat.fit(train_data)","3a13fb80":"confusion_matrix(y_val, cat.predict(X_val[features]))","1f804e3f":"roc_auc_score(y_val, cat.predict(X_val[features]))","5ae10f87":"gini(y_val, cat.predict_proba(val_data)[:,1])","c55a368f":"print(classification_report(y_val, cat.predict(X_val[features])))","bc41c27d":"roc_auc = []\nthresholds = []\nfor i in range(1, 100, 1):\n    preds = [int(x[1] > i\/100) for x in cat.predict_proba(train_data)]\n    roc_auc.append(roc_auc_score(y_train, preds))\n    thresholds.append(i\/100)\nsns.lineplot(x = thresholds, y = roc_auc);","f20f91f1":"cat_tr = thresholds[np.argmax(roc_auc)]","3605b57f":"preds = [int(x[1] > cat_tr) for x in cat.predict_proba(val_data)]","d2a4fca7":"confusion_matrix(y_val, preds)","110e40d0":"roc_auc_score(y_val, preds)","32d6302f":"print(classification_report(y_val, preds))","e7437360":"X_train_lr = X_train[features].copy()\nX_val_lr = X_val[features].copy()\nX_test_lr = X_test[features].copy()\nfor i in categorical:\n    X_train_lr = X_train_lr.join(pd.get_dummies(X_train_lr[i], prefix = i, drop_first = True))\n    X_train_lr = X_train_lr.drop([i], axis = 1)\n    X_val_lr = X_val_lr.join(pd.get_dummies(X_val_lr[i], prefix = i, drop_first = True))\n    X_val_lr = X_val_lr.drop([i], axis = 1)\n    X_test_lr = X_test_lr.join(pd.get_dummies(X_test_lr[i], prefix = i, drop_first = True))\n    X_test_lr = X_test_lr.drop([i], axis = 1)","aeeb78df":"[x for x in X_train_lr.columns if x not in X_test_lr.columns]","7c811ec4":"params = {\n    'random_state': 17,\n    'n_jobs': 12,\n    'penalty': 'l2'\n}\nclf = LogisticRegression(**params)","1b049322":"%%time\nclf.fit(X_train_lr, y_train)","64ef3e90":"confusion_matrix(y_val, clf.predict(X_val_lr))","a22d5a38":"roc_auc_score(y_val, clf.predict(X_val_lr))","954fcffe":"gini(y_val, clf.predict_proba(X_val_lr)[:,1])","bcab4240":"print(classification_report(y_val, clf.predict(X_val_lr)))","321eddc2":"roc_auc = []\nthresholds = []\nfor i in range(1, 100, 1):\n    preds = [int(x[1] > i\/100) for x in clf.predict_proba(X_train_lr)]\n    roc_auc.append(roc_auc_score(y_train, preds))\n    thresholds.append(i\/100)\nsns.lineplot(x = thresholds, y = roc_auc);","3982014f":"lr_tr = thresholds[np.argmax(roc_auc)]","a617e1d7":"preds = [x[1] > lr_tr for x in clf.predict_proba(X_val_lr)]","8682f3d6":"confusion_matrix(y_val, preds)","9ebb92f9":"roc_auc_score(y_val, preds)","c6be505b":"y_pred_cat = cat.predict_proba(X_val[features])[::,1]\ny_pred_lr = clf.predict_proba(X_val_lr)[::,1]\nfpr, tpr, _ = roc_curve(y_val,  y_pred_cat)\nfpr_lr, tpr_lr, _ = roc_curve(y_val,  y_pred_lr)\nauc_cat = roc_auc_score(y_val, y_pred_cat)\nauc_lr = roc_auc_score(y_val, y_pred_lr)\nplt.plot(fpr,tpr, label=\"catboost, auc=\"+str(auc_cat))\nplt.plot(fpr_lr, tpr_lr, label=\"lr, auc=\"+str(auc_lr))\nplt.legend(loc=4)\nplt.show();","4ca85d57":"preds = [x[1] > cat_tr for x in cat.predict_proba(X_test[features])]","e418942d":"confusion_matrix(y_test, preds)","c720b88c":"roc_auc_score(y_test, preds)","9cd9b39a":"gini(y_test, cat.predict_proba(X_test[features])[:,1])","72835afe":"fig = plt.figure(figsize = (6, 6))\nsns.distplot(cat.predict_proba(X_test[features])[:,1][y_test == 1], label = 'defaults')\nsns.distplot(cat.predict_proba(X_test[features])[:,1][y_test == 0], label = 'paid-off')\nplt.title('Predicted default probability by status.')\nplt.legend(loc=\"best\");","6dc5d50c":"X_test.loc[:, 'pred'] = cat.predict_proba(X_test[features])[:, 1]","5bf506de":"fig = plt.figure(figsize = (8, 8))\nX_test.groupby('grade').pred.plot(kind = 'kde', legend = True, title = 'Predicted probability of default by grade');","ec24721d":"summary_train = pd.DataFrame(columns = ['grade', 'avg_DR', 'ROI'])\nsummary_train['grade'] = train_df.groupby('grade').def_flag.mean().index\nsummary_train['avg_DR'] = train_df.groupby('grade').def_flag.mean().values","9a4c4e0e":"for i, g in enumerate(summary_train['grade'].values):\n    train = train_df[train_df.grade == g].copy()\n    return_amnt = train['total_rec_prncp'].sum() + train['total_rec_int'].sum() + train['total_rec_late_fee'].sum() + train['recoveries'].sum() \n    investment = train['funded_amnt'].sum()\n    summary_train.loc[i, 'ROI'] = round(return_amnt\/investment, 2)\n\nsummary_train","2aa48c23":"summary = pd.DataFrame(columns = ['grade', 'avg_DR', 'ROI'])\nsummary['grade'] = test_df.groupby('grade').def_flag.mean().index\nsummary['avg_DR'] = test_df.groupby('grade').def_flag.mean().values","c6b38efe":"for i, g in enumerate(summary['grade'].values):\n    test = test_df[test_df.grade == g].copy()\n    return_amnt = test['total_rec_prncp'].sum() + test['total_rec_int'].sum() + test['total_rec_late_fee'].sum() + test['recoveries'].sum() \n    investment = test['funded_amnt'].sum()\n    summary.loc[i, 'ROI'] = round(return_amnt\/investment, 2)\n\nsummary","1efb42f6":"train_df.loc[:, 'pred'] = cat.predict_proba(X_train[features])[:, 1]","b7886370":"kmeans = KMeans(n_clusters = 7, random_state = 17)","1a4430df":"%%time\nkmeans.fit(train_df.sort_values('pred')[['pred']])","0af98c11":"train_df['alt_grade'] = kmeans.predict(train_df[['pred']])","fd373e6a":"train_df.groupby('alt_grade').pred.mean()","134b13fe":"alt_grade_map  = {\n    4: 'A',\n    1: 'B',\n    2: 'C',\n    5: 'D',\n    0: 'E',\n    3: 'F',\n    6: 'G'\n}","f5457d72":"train_df['alt_grade'] = train_df['alt_grade'].map(alt_grade_map)","850cf2db":"fig, ax = plt.subplots(1, 2, figsize = (12, 6))\nsns.countplot(train_df.sort_values('alt_grade').alt_grade, ax = ax[0])\nsns.countplot(train_df.sort_values('grade').grade, ax = ax[1]);","3af464ed":"summary_train['alt_grade'] = train_df.groupby('alt_grade').def_flag.mean().index\nsummary_train['alt_avg_DR'] = train_df.groupby('alt_grade').def_flag.mean().values","68f05d8c":"for i, g in enumerate(summary_train['alt_grade'].values):\n    train = train_df[train_df.alt_grade == g].copy()\n    return_amnt = train['total_rec_prncp'].sum() + train['total_rec_int'].sum() + train['total_rec_late_fee'].sum() + train['recoveries'].sum() \n    investment = train['funded_amnt'].sum()\n    summary_train.loc[i, 'alt_ROI'] = round(return_amnt\/investment, 2)\n\nsummary_train","5a5c0fcc":"test_df.loc[:, 'pred'] = cat.predict_proba(X_test[features])[:, 1]\ntest_df['alt_grade'] = kmeans.predict(test_df[['pred']])\ntest_df['alt_grade'] = test_df['alt_grade'].map(alt_grade_map)","606848c0":"fig, ax = plt.subplots(1, 2, figsize = (12, 6))\nsns.countplot(test_df.sort_values('alt_grade').alt_grade, ax = ax[0])\nsns.countplot(test_df.sort_values('grade').grade, ax = ax[1]);","e3ef8e07":"summary['alt_grade'] = test_df.groupby('alt_grade').def_flag.mean().index\nsummary['alt_avg_DR'] = test_df.groupby('alt_grade').def_flag.mean().values","91998953":"for i, g in enumerate(summary['alt_grade'].values):\n    test = test_df[test_df.alt_grade == g].copy()\n    return_amnt = test['total_rec_prncp'].sum() + test['total_rec_int'].sum() + test['total_rec_late_fee'].sum() + test['recoveries'].sum() \n    investment = test['funded_amnt'].sum()\n    summary.loc[i, 'alt_ROI'] = round(return_amnt\/investment, 2)\n\nsummary","46ebe44b":"print(f'Worst grade volume new: {test_df[test_df.alt_grade == \"G\"].shape[0]}.')\nprint(f'Worst grade volume old: {test_df[test_df.grade == \"G\"].shape[0]}.')","716d5341":"fig, ax = plt.subplots(1, 2, figsize = (12, 6))\ntest_df.sort_values('alt_grade').groupby('alt_grade').def_flag.mean().plot(kind = 'bar', ax = ax[0])\ntest_df.sort_values('grade').groupby('grade').def_flag.mean().plot(kind = 'bar', ax = ax[1]);","bbc4d27e":"pd.pivot_table(test_df, index = 'grade', columns = 'alt_grade', values = 'loan_amnt', aggfunc = lambda x: len(x))","df570c8e":"## EDA and data preprocessing","69ee0677":"Below I define supportive functions for Gini index calculation and variables meaning checking.","06e2d301":"Variable `tot_coll_amt` has many outliers and relatively high standard deviation. `collections_12_mths_ex_med`, `chargeoff_within_12_mths`, `delinq_amnt` till 99 percentile don't have any value except 0. It's decided to drop these 5 variables. For variables `pub_rec_bankruptcies` and `pub_rec distribution` of values is more diverse, still there are not many records with value above zero. Creating flags for these two.","969a09c1":"`num_tl_120dpd_2m` and `num_tl_30dpd` updated ones for 2 months, not enough appearances, decided to drop. For `num_tl_90g_dpd_24m`, `num_accts_ever_120_pd` and `delinq_2yrs` missings imputation and data type convertion needed.","8fa8845c":"### filtering out features with high uninfomative missing rate\n\nThere are two types of missings: informative and non-informative. Informative are the ones which gives some information, like missing in DPD meaning no delinquency ever. Non-informative missing don't contain any information. So, I remove features with high rate of non-informative missings.","937d3bbc":"Gini = 39%.","798f4325":"## Data normalization\n\nStandard normalization for continuous variables.","3f39f4be":"All client ids are missing. All ids are unique.","1315fd91":"As promised dropping outliers for `dti` and `annual_ind`, but only for development sample.","127ae55c":"Imputation of missings will be done later based on training sample.","ce07afc4":"Filling missings for `annual_income` and `dti` with medians from train sample.","35fb5a56":"## Introduction","93ee0276":"### variables categorization\n\nLet's split variables on few groups: \n* facility level variables\n* scoring variables\n* customer related variables\n* delinquency related variables\n* inqueries related variables\n* accounts related variables\n* balances related\n\n`grade_related` is the group for variables dependent on grade. I will not use this variables for prediction, because these four values calculated by Lending Club. The goal is to show, that using data, to which Lending Club has access before grading we're able to distinguish the most risky group of borrowers and provide better grading.","72241b29":"## Problem definition","945f290b":"## Conclusion and business gains\n\nIn this notebook I tried to show, that grading policy used by Lending Club is not optimal. Considering the fact, that based on grades interest rates are set, grading plays key role in Lending Club's interaction with clients. Better scoring gives more robust evaluation of risk by Lending Club and helps investors make more adequate decisions. From aboves analysis underestimation of risk in many cases may be noticed. Below there is a table showing movements between standard grades and alternative ones for test sample, built based on the new model. New model seems to be more restrictive, as it puts more loans to worse buckets, but as I shew in the previous section default rate is the same in comparison to old `G` grade.","25f880ce":"We may merge three least numerous groups to one, but this group still will be too small. So the proposal is to merge it in the most conservative way, which means with category with the highest DR, to not underestimate the risk. The riskiest category amongst three biggest is RENT.","e8e8e108":"For `mths_since_recent_inq` missings imputation and type convertion needed.","48a0936b":"I'll drop loans with the following statuses:\n* In Grace Period\n* Late (16-30 days)\n* Current\n\nI made this because we're unable to identify which of them are defaults\/non-defaults. Assigning them to non-defaults proably will lead to overestimation of True Negative rate.","e74bc1d6":"## Alternative grading\n\nLet's check ROI and default rate for each Lending Club's grade on train and test samples.","264c07e9":"We may notice, that even for fully paid loans recovered fraction doesn't always reach 100%, sometimes even reach higher values.","08b3bd97":"Creating new variable based on months since variables to avoid such a high missing rate.","87e4f0cd":"As may be seen from the aboves, our model prediction gives more robust grading comparing to standard one used by Lending Club. We're able more efficiently distinguish better loans from worse. Having comparable share of each grade on the unseen data by the model we're able to reach higher ROI for better grades and distinguish loans of worse quality which are in alternative grade G, which by the way has much higher share in the portfolio comparing to initial grade.","9e320ed0":"### accounts related variables\n\nSimilar missing rate for several variables looks strange. It looks like this variables were collected from some point in time, exclusion may be not the best solution in this case, still on the purpose of this task I assume some simplification and remove these variables from further analysis.","b684a101":"Categories encoded correctly.","933b876a":"Some mergings may be managed, for simplification dropping `addr_state`.","b657df6f":"Due to high missing rate creating a flag.","50deb68c":"### delinquency related variables","557b5a93":"## Missing treatment","4b18cde7":"Seems like the old rating policy underestimates risk level. Lending Club classifies considerably more loans to B and C grades, while new model classify many more loans to worse grades.","71dddd14":"24 features left.","84669644":"Too low diversification.","38e8983b":"## Sample split\n\nI will split samples on three: train, validation and test.","a6865a8f":"Numerous companies from financial indutry often invest considerable resources to improve their predictive models with the aim of having better insights into their customers. Such an interest in model improvement has intensified in recent years mostly because of fast development of machine learning and artificial intelligence. For standard lending institution default predictive model with high performance helps to considerably minimize Credit Loss, resulting in higher revenue and profits. Usually the better predictive model the more efficient is the underwriting policy and collection process. A well-functioning model should distinguish creditworthy customers from those that are credit risks. Often, more-predictive credit-decisioning model can identify a greater number of customers within an institution\u2019s specified risk tolerance, which should expand revenues as well.\n\nIn given task the goal is to increase detection of defaulted loans before the loan is issued\/offered by P2P lending company - Lending Club. Peer-to-peer lending differs from traditional financial institutions like banks or commercial lending companies. Below I'll try to analyze possible business gains from better detection of defaults for Lending Club.\n\nRefering to [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Lending_Club):\n> LendingClub made money by charging borrowers an origination fee and investors a service fee. The size of the origination fee depended on the credit grade and ranges to be 1.1\u20135.0% of the loan amount. The size of the service fee was 1% on all amounts the borrower pays.\n\nSo, Lending Club is a mediator between investors and borrowers, earning money by charging both. The main Lending Club interest is to attract more clients and maintain protfolio size. The motivation of borrowers is clear, they want to find as cheap capital as possible, so they're seeking for the best offer at the market, which is available for them. In case of investors the motivation is obvious as well. Investors look for high ROI (return of investments), but remembering that returns are proportional to risks, we may formalize saying, that investors look for appropriate returns\/risks ratio. If investors experience losses it may cause churn rate growth. \n\nThe underwriting process for Lending Club looks like this. Borrower applies for the loan, then if he\/she meets all the basic requirements - Lending Club using their scoring model assigns client to respective grade. There are 7 grades and 35 sub-grades. Interest rate is dependent on sub-grade. Please see more info [here](https:\/\/www.lendingclub.com\/foliofn\/rateDetail.action). After that, Lending Club gives access to the loan for investors with information about the loan and the borrower (incl. grade and sub-grade) and investors decide whether or not to invest money in this loan. The lower the grade the higher the interest rate, which means, that investors may take higher risks to gain potentially higher returns.\n\nSeeking for default rate reduction we can end up with too restrictive underwriting policy which does not neccessary correlate with higher ROI for investors, because we'll not let investors choose risky loans, which means lower interests. For Lending Club it probably means the loss of investors with high risk appetite and borrowers with weak credit history, or in case of Lending Club those who need higher loan amount.\n\nThe main goal of this notebook is to show, that there is a space for improvement for Lending Clubs's grading policy. Grading is a base for interest rate assignment, so adequate risk evaluation should help investors to make more perceived investment decisions which is in line with ther risk appetite. Besides, probably it will result lower churn rate for platform's clients.","4b69f66b":"Let's map alternative grades to respective names. Less risky by predicted default probability will be named A, second B and so on.","7167d39a":"Preventing underestimation of risk should bring following benefits for Lending Club:\n* better identification of riskier clients should result in higher gains from originatiot fee;\n* identification of greater number of customers within an institution\u2019s specified risk tolerance;\n* reduction of investors losses, as a result lower churn rate;\n* lower interest rates for less risky clients;\n* model outputs may help improve underwriting policy.\n\nModel can be improved as well:\n* deeper data insights;\n* additional external data;\n* model tuning;\n* more model candidates;\n* broader feature engineering.","f1de1a47":"Now, comparing results for test sample. The first part of the table shows statistics for standard grading policy. The second one - for the new one. Notice, DR for grade G almost didn't change, but new model predicted 4 times more loans to be in the worst grade. Additionally, higher ROI transfered from more risky grades to ones with expected better quality. For grades A-E ROI is higher comparing to previous results.","c8382c8a":"Merging some purpose groups.","714719c9":"If you find this Notebook useful, please upvote.","5414d12a":"As can be noticed, ROI for four first classes is almost the same, we avoid ROI disproportion as it was previously. Now, doesn't matter in which loans from A-D grades investors have invested, expected return is almost the same. In other words, seems, like risk compensation is more accurate. Again, new grading identifies the riskiest loans more effectively than the old policy. We may see more loans in G grade with higher DR than the old one. But that's results for train sample. Now, let's try it for the test sample.","9385b839":"### inqueries related variables","70a296e5":"Alternative grades have almost the same distribution as a standard ones. We can notice, that new model classifies more loans to A grade and to F,G grades, which means it's more confident with predicting the best and the worst loans in terms of risk level.","9e737774":"## Feature engineering\n\nHere I create only one additional variable. Loan-to-income shows the relation between annual principal due to annual income.","0c268e0c":"The best threshold.","d103391f":"ROC_AUC equals 54%.","d0753b70":"Some incomes are not consistent with employment position. Still, filtering dataset by income we can notice that some of borrowers with high income are on the high position. Nevertheless, it's decided to provide an upper bound for annual income and exclude records with very high income to avoid impact of such records on estimation. Lower bound provided as well, as it's hard to believe financed loan for client with annual income less than 10000 dollars annually. Negative DTI is for sure an error. Besides, seems like DTI given in %. DTI greater than 80% is probably may be considered as outlier because it's very high debt share in monthly income. The probability of approval of such loan is extremely low. Those exclusions will be performed later on train and validation samples.","eba96e92":"## Contents\n\n1. [Introduction](#Introduction)\n2. [Problem definition](#Problem-definition)\n3. [Data import](#Data-import)\n4. [EDA and data preprocessing](#EDA-and-data-preprocessing)\n    * [filtering out variables not available before loan origination](#filtering-out-variables-not-available-before-loan-origination)\n    * [portfolio insights](#portfolio-insights)\n    * [target definition](#target-definition)\n    * [filtering out features with high uninfomative missing rate](#filtering-out-features-with-high-uninfomative-missing-rate)\n    * [variables categorization](#variables-categorization)\n    * [facility level variables](#facility-level-variables)\n    * [scoring variables](#scoring-variables)\n    * [customer related variables](#customer-related-variables)\n    * [delinquency related variables](#delinquency-related-variables)\n    * [inqueries related variables](#inqueries-related-variables)\n    * [accounts related variables](#accounts-related-variables)\n    * [balances related variables](#balances-related-variables)\n5. [Sample split](#Sample-split)\n6. [Missing treatment](#Missing-treatment)\n7. [Data Types convertions](#Data-Types-convertions)\n8. [Feature Engineering](#Feature-Engineering)\n9. [Correlation analysis](#Correlation-analysis)\n10. [Data normalization](#Data-normalization)\n11. [Cat-Boost model](#Cat-Boost-model)\n12. [Logistic Regression model](#Logistic-Regression-model)\n13. [Best model](#Best-model)\n14. [Alternative grading](#Alternative-grading)\n15. [Conclusion and business gains](#Conclusion-and-business-gains)","100109e3":"## Logistic Regression model\n\nTo build Logistic Regression categorical variables should be encoded.","4cc2f4f7":"Addtionaly let's create three lists to categorize variables by type.","5f5f9572":"We're able to achive almost 64% of ROC-AUC and almost 40% Gini for test sample.","1dde80de":"## Data Types convertions\n\nData types for categorical variables are ok. Binary variables have int type, others will be encoded.","c6720449":"Probably for `num` variables 0 may be used for imputation.","dc7c427e":"For continuous variables everything is ok.","c88bc27c":"Let's check different thresholds and dependency of chosen metric on it. ","ffa8e01f":"### facility level variables","d9d2cc5b":"# <center> Efficient grading for Lending Club with CatBoost","3efefbe3":"### target definition\n\nFor the purpose of this task we assume, that loan has default status if it has one of the following loan_statuses:\n* Charged off\n* Default\n* Does not meet the credit policy. Status: Charged Off\n* Late (31-120 days)","a235c7bf":">[LendingClub](https:\/\/www.lendingclub.com\/) was an American peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission (SEC), and to offer loan trading on a secondary market. \nLending Club enables borrowers to create unsecured personal loans between 1,000 USD and 40,000 USD. The standard loan period is three years. Investors can search and browse the loan listings on Lending Club website and select loans that they want to invest in based on the information supplied about the borrower, amount of loan, loan grade, and loan purpose. Investors make money from interest. Lending Club makes money by charging borrowers an origination fee and investors a service fee.\n\nLending Club Dataset contains a comprehensive list of features and allows building predictive models. The dataset includes detailed information for every loan issued by Lending Club from 2007 to Q4-2018. In given notebook I built default predictive model and provided alternative grading approach based on this model. Newly created grading policy shows better results in terms of default identification. Model performance was measured based on Default Rate statistics and ROI for each bucket. Model was built based on features available only before loan origination to prevent data leakage. Only closed loans conisdered, which means Current and a bit late loans were excluded from the analysis.","da790215":"### portfolio insights","abbf5a64":"ROC-AUC 53%, Gini = 33%","8969c380":"Sanity check.","ad766199":"No missings left.","4476efd8":"As warning shows, there are numerous columns in the data set with mixed data types. Looking ahead those features will be removed and will not be used during modelling, as most of them are not available before loan orgination.","a8a67945":"## Best model\n\nSo, I'm picking Cat Boost model for higher Gini on validation sample. Plot below shows, that CatBoost model is slightly better than LR in terms of ROC AUC.","72d8ea75":"At the aboves plot we may see that for recent dates there are not many closed loans, because minumum loan term is 3 years.","0e0f8942":"`fico_range_low` is ideally correlated with `fico_range_high`. Dropping one of them.","fc17773c":"Here we can impute 0 for missings, assuming no inqueries made.","f91d4165":"### filtering out variables not available before loan origination\n\nI tried to filter out variables which can not be available before loan origination to prevent data leakage. Some variables which are not available prior to data origination are needed for calculations of returns.","672742fb":"### scoring variables","3a026477":"Let's see how predicted probabilty for test set distributed by grades.","764c4ef7":"For the rest of variables I impute 0.","ab4780ab":"Too many unique values.","b3949290":"Now, I'll try to create new grading. For this I'll use KMeans classifier. We'll have 7 grades as Lending Club does. Notice, that training sample is used to fit classifier.","b3646757":"## Data import","8df01e1e":"`inq_fi` and `inq_last_12m` are collected since the end of 2015. Moreover there is strong correlation with time. I'm dropping them.","c84e4625":"Seems like title is a subcategory of purpose or more detailed description. For the purpose of this task it's decided to drop title column.","ed8f5171":"## Correlation analysis\n\nChosen approach is to check pairwise correlation and Gini index, threshold for correlation value is 70%. I leave only variable with highest Gini amongst correlated. `grade_related` variables will not be used. Features with Gini lower than 0.02 were removed as well.","e3cea45f":"Conservative assumption - if `emp_length` is missing, than employment length is 0.","08180f61":"The best threshold.","c531d82d":"Variables `months since`. Zero will not be a good choice. Probably in this case we may impute max, as features show a distance to some event. Missings in this case probably means no event, so the distance should be maximum. Notice for each sample we take max from training sample.","ee1a7f87":"Term seems to be have relatively high descriminatory power.","337ef6a6":"## Cat-Boost model","b5c1badf":"### balances related variables\n\nAgain, the same logic, drop variables with high missing rate since they were collected from 2015. As previously, numerous outliers, this may be seen from provided below statistics even without plotting a boxplot. Filling missings with zeros seems as an adequate measure in this case too.","ae7bdb57":"### customer related variables"}}