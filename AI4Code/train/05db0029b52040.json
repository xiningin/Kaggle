{"cell_type":{"00da8379":"code","5e66bb0b":"code","5a098a13":"code","9b0d3047":"code","92077a9e":"code","d5f8de99":"code","524687f7":"code","a3a67a14":"code","157f26b0":"code","124e94dd":"code","d5ec2461":"code","5eb18e62":"code","9f6cd1e2":"code","8a2f7e9f":"code","a4a6505d":"code","56bae7ff":"code","55d713b5":"code","f3b3879d":"markdown","9a94b282":"markdown","777f87c5":"markdown","2aa349bd":"markdown","bf24a082":"markdown","73a62276":"markdown","b0a455c7":"markdown","ae1c15e7":"markdown","9de7e6c0":"markdown","044264f3":"markdown","c52c0466":"markdown","83c94ab6":"markdown","1720861c":"markdown"},"source":{"00da8379":"# modeling\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\n# result\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('seaborn-whitegrid')          # graph settings\nplt.rcParams['figure.figsize'] = (12,5)    # graph settings\n\n# data preprocessing\nfrom sklearn.model_selection import train_test_split\n\n# data wrangling\nimport numpy as np \nimport pandas as pd\n\n# corpus\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\n\n# string manipulation\nimport re\nimport spacy\nimport collections","5e66bb0b":"import nltk \nnltk.download(\"stopwords\") ","5a098a13":"df = pd.read_csv(\"..\/input\/toxic-tweets-dataset\/FinalBalancedDataset.csv\")\ndf = df.drop(columns = ['Unnamed: 0'])\ndf.head()","9b0d3047":"import wordcloud\nfrom wordcloud import WordCloud\nallWords = ' '.join([twts for twts in df['tweet']])\nwordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)\n\nplt.figure(figsize = (10, 8))\nplt.imshow(wordCloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","92077a9e":"allWords = ' '.join([twts for twts in df[df['Toxicity'] == 1]['tweet']])\nwordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)\n\nplt.figure(figsize = (10, 8))\nplt.imshow(wordCloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","d5f8de99":"allWords = ' '.join([twts for twts in df[df['Toxicity'] == 0]['tweet']])\nwordCloud = WordCloud(width=500, height=300, random_state=21, max_font_size=110).generate(allWords)\n\nplt.figure(figsize = (10, 8))\nplt.imshow(wordCloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","524687f7":"X = df['tweet'].copy()\ny = df['Toxicity'].copy()","a3a67a14":"def data_cleaner(tweet):\n    tweet = tweet.lower()\n    tweet = tweet.replace(\":(\",\" sedih\")\n    tweet = tweet.replace(\":)\",\" senang\")\n    tweet = tweet.replace(\":3\",\" lucu\")\n    tweet = tweet.replace(\":d\",\" senang\")\n    tweet = tweet.replace(\":-)\",\" senang\")\n    tweet = tweet.replace(\"=)\",\" senang\")\n    tweet = re.sub(r'http\\S+', ' ', tweet)   # remove urls\n    tweet = re.sub(r'<.*?>',' ', tweet)      # remove html tags\n    tweet = re.sub(r'\\d+',' ', tweet)        # remove digits\n    tweet = re.sub(r'#\\w+',' ', tweet)       # remove hashtags\n    tweet = re.sub(r'@\\w+',' ', tweet)       # remove mentions\n    tweet = re.sub(r'[^\\w\\s]',' ', tweet)    # remove punctuation\n    tweet = re.sub('[^A-Za-z0-9 ]+', '', tweet) # remove characters that are not a letters or numbers\n    tweet = \" \".join([word for word in tweet.split() if not word in stop_words])   # remove stop words\n    tweet = stemmer.stem(tweet)\n    tweet = tweet.strip()\n    return tweet","157f26b0":"stop_words = stopwords.words('english')\nstemmer = SnowballStemmer(\"english\")\n\n#X.apply(data_cleaner)\nX_cleaned = X.apply(data_cleaner)\nX_cleaned","124e94dd":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_cleaned)\nX = tokenizer.texts_to_sequences(X_cleaned)\nvocab_size = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary size: {}\".format(vocab_size))\nprint(\"\\nExample:\\n\")\nprint(\"Sentence:\\n{}\".format(X_cleaned[0]))\nprint(\"\\nAfter tokenizing :\\n{}\".format(X[0]))\n\nX = pad_sequences(X, padding='post')\nprint(\"\\nAfter padding :\\n{}\".format(X[0]))","d5ec2461":"sns.countplot(x = \"Toxicity\",data = df)","5eb18e62":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)","9f6cd1e2":"tf.keras.backend.clear_session()\n\n# hyperparameters\nEPOCHS = 2\nBATCH_SIZE = 64\nembedding_dim = 64\nunits = 256\n\nmodel = tf.keras.Sequential([\n    L.Embedding(vocab_size, embedding_dim, input_length=X.shape[1]),\n    L.Bidirectional(L.GRU(units, return_sequences=True)),\n    L.GlobalMaxPool1D(),\n    L.Dropout(0.4),\n    L.Dense(512, activation=\"sigmoid\"),\n    L.Dropout(0.4),\n    L.Dense(2, activation = \"softmax\")\n])\n\nmodel.compile(\n    loss=SparseCategoricalCrossentropy(from_logits=True),\n    optimizer='adam',\n    metrics=['accuracy']\n)\n\nmodel.summary()","8a2f7e9f":"history = model.fit(X_train, y_train, epochs=2, validation_data = (X_test,y_test), batch_size=BATCH_SIZE)","a4a6505d":"predicted = model.predict_classes(X_test)\nloss, acc = model.evaluate(X_test, y_test, verbose=0)\nprint('Test loss: {}'.format(loss))\nprint('Test Accuracy: {}'.format(acc))","56bae7ff":"conf = confusion_matrix(y_test, predicted)\n\nlabels = ['Non-Toxic','Toxic']\n\ncm = pd.DataFrame(\n    conf, index = [i for i in labels],\n    columns = [i for i in labels]\n)\n\nsns.heatmap(cm, annot=True, fmt=\"d\")\nplt.show()","55d713b5":"print(classification_report(y_test, predicted, target_names=labels))","f3b3879d":"## Model Building and Training","9a94b282":"## Check Distribution of Class","777f87c5":"After 2 epochs, we will get overfitting model","2aa349bd":"## Train Test Split","bf24a082":"## Import Data","73a62276":"### Classification Report","b0a455c7":"## Predict Data Test","ae1c15e7":"## Visualize Toxic Tweets","9de7e6c0":"## Data Visualization (ALL)","044264f3":"## Visualize Non-Toxic Tweets","c52c0466":"## Cleaning Data","83c94ab6":"## Tokenizing","1720861c":"### Confusion Matrix"}}