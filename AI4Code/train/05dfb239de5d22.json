{"cell_type":{"397ce56d":"code","db9281c5":"code","219f6f4b":"code","de02222d":"code","a810625d":"code","379ffbd9":"code","ceffc60b":"code","024f2ab4":"code","c81bf98f":"code","b6fc59c4":"code","948e0eb2":"code","483140ec":"code","6fd5cbd2":"code","eaf3d664":"code","d0c9b95c":"code","5c679e55":"code","9eb04331":"code","95ead068":"code","ae5eec4a":"code","39bd18b1":"code","311bbf7d":"code","4911103b":"code","cc386543":"code","631275ae":"markdown","71e2920d":"markdown","7f46259d":"markdown","2049c26a":"markdown","6d22096a":"markdown","bc643c97":"markdown","86acae79":"markdown","e1ae0caf":"markdown","e03c3ca1":"markdown","a4dd3263":"markdown","c25e5a05":"markdown","c8e4e7e1":"markdown","721ef7c9":"markdown","2e1a9dd7":"markdown"},"source":{"397ce56d":"import numpy as np\nfrom scipy.stats import mode\nimport pandas as pd\nfrom matplotlib import pyplot as plt\n\n# plotting and coding utils\nplt.style.use('fivethirtyeight')\n%config IPCompleter.use_jedi = False\n%matplotlib inline\nplt.rcParams[\"figure.figsize\"] = (12, 8)","db9281c5":"in_train = '\/kaggle\/input\/wine-m\/train.csv'\nin_test = '\/kaggle\/input\/wine-m\/test.csv'\n\ntrain_df = pd.read_csv(in_train)\ntest_df = pd.read_csv(in_test)","219f6f4b":"train_df.head()","de02222d":"train_df.shape, test_df.shape","a810625d":"(train_df.target.value_counts() \/ train_df.shape[0]).plot(kind = 'bar')\n\nplt.title('Distribution of class labels')\nplt.xlabel(\"Class labels\")\nplt.ylabel(\"% Records\")\nplt.xticks(rotation = 0);","379ffbd9":"train_df.isna().sum()","ceffc60b":"train_df[train_df.duplicated()]","024f2ab4":"train_df.describe()","c81bf98f":"from sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import make_pipeline","b6fc59c4":"X = train_df.drop(\"target\", axis = 1)\ny = train_df[\"target\"]\nts = 0.2\nrs = 99\n\nX_train, X_test, y_train, y_true = train_test_split(X, y, test_size= ts, random_state=rs)","948e0eb2":"clf = DecisionTreeClassifier(random_state=rs)\nclf.fit(X_train, y_train)\nfig, ax = plt.subplots(figsize = (18,12))\nplot_tree(clf, ax=ax, fontsize = 8, feature_names= X.columns);","483140ec":"y_pred = clf.predict(X_test)\nprint(classification_report(y_true, y_pred))","6fd5cbd2":"\nclass MyRandomForestClassifier():\n    \"\"\" Customized Random forest classifier\n    Parameters\n    ----------\n    n_estimators: int, # of trees in forest\n    max_features: string, subset of records to train a single decision tree\n    random_state: int, random state of decision trees to reproduce the results\n    \"\"\"\n    def __init__(self, n_estimators = 10, max_features = \"sqrt\", random_state = None):\n        self.n_estimators = n_estimators\n        self.max_features = max_features\n        self.random_state = random_state\n        self.trees = []\n        self.results = []\n        \n    # train the trees of this random forest using subsets of X (and y)\n    def fit(self, X, y):\n        \"\"\" Train the model\n        Parameters\n        ----------\n        X: features matrix\n        y: class labels\n        \"\"\"\n        N = X.shape[0] # Total n. of records in fm\n        for _ in range(self.n_estimators): # Grow trees\n            subset_ind = np.random.choice(N, N, replace=True) # choose N rows with replacement\n            self.trees.append( \\\n                DecisionTreeClassifier( \\\n                     max_features= self.max_features) \\\n                .fit(X.iloc[subset_ind], y.iloc[subset_ind]))\n        \n    \n    # predict the label for each point in X\n    def predict(self, X):\n        self.results = [tree.predict(X) for tree in self.trees] # Predictions of each tree\n        return mode(self.results, axis=0)[0][0] # uniform voting for final labeling\n    \n    def get_params(self, **kwarg):\n        return {\n            \"n_estimators\": self.n_estimators,\n            \"max_features\": self.max_features,\n            \"random_state\": self.random_state\n        }","eaf3d664":"rfc = MyRandomForestClassifier(n_estimators=100, random_state=rs)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(classification_report(y_true, y_pred))","d0c9b95c":"rfc = MyRandomForestClassifier(n_estimators=100, random_state=rs)\nacc = cross_val_score(rfc, X, y, cv = 5, scoring='accuracy')\nacc.mean()","5c679e55":"rfc = RandomForestClassifier(n_estimators=100, random_state=rs)\nrfc.fit(X_train, y_train)\ny_pred = rfc.predict(X_test)\nprint(classification_report(y_true, y_pred))","9eb04331":"params = {\n    \"n_estimators\": [80,100,120,150],\n    \"max_depth\": [2, 4, 8],\n    \"max_features\": [\"auto\", \"sqrt\"],\n    \"criterion\": [\"gini\", \"entropy\"]    \n}\n\nrfc = RandomForestClassifier(random_state=rs)\nsearch_rfc = GridSearchCV(rfc, param_grid=params, n_jobs = -1, scoring = 'accuracy')\nsearch_rfc.fit(X_train, y_train)\ny_pred = search_rfc.predict(X_test)\nprint(classification_report(y_true, y_pred))","95ead068":"rfc = RandomForestClassifier(** search_rfc.best_params_)\nacc = cross_val_score(rfc, X, y, cv = 5, scoring='accuracy')\nacc.mean()","ae5eec4a":"est = search_rfc.best_estimator_\ny_out = est.predict(test_df)","39bd18b1":"pd.DataFrame(y_out, columns= ['target']).to_csv('from_rfc.csv',index_label = 'id')","311bbf7d":"clf = make_pipeline(StandardScaler(), SVC(random_state=rs))\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_test)\nprint(classification_report(y_true, y_pred))","4911103b":"clf = make_pipeline(StandardScaler(), SVC(random_state=rs))\nclf.fit(X, y)\ny_out = clf.predict(test_df)","cc386543":"pd.DataFrame(y_out, columns= ['target']).to_csv('from_svc.csv',index_label = 'id')","631275ae":"SVC is predicting the labels with maximum accuracy.  ","71e2920d":"#### The results are imporved from the predictions made by a single decision tree.\n#### Let's now compare the results with RandomForest provided by Sklearn module.","7f46259d":"### In this report, I predict the class labels of wine dataset via different ML models.\nThe problem dataset contains three kinds of wines.  \nEach record in the dataset contains information about unique characteristics of the wine sample.  \n\nTo perform the classification task we will:\n1. Use Decision tree classifiers\n1. Create custom RandomForest classifier model\n1. Use Support vector machines to make predictions","2049c26a":"#### Dataset is perfectly clean as we don't have any missing values or duplicates.","6d22096a":"#### The distribution of class labels shows that the problem is not perfectly balanced (approximately 33%\/40%\/27%)","bc643c97":"### Load datasets","86acae79":"# Model building","e1ae0caf":"#### We can use hyperparameters tuning to find optimal set of settings","e03c3ca1":"# Support Vector Classifier\n\nSupport vector machines are known for their higher accuracy.  We cannot use this dataset directly on svm because the distribution of feature values is not same.  We will use Standard scaler to standardize input dataset.","a4dd3263":"## Decision Tree Classifier","c25e5a05":"## Dataset split for cross validation","c8e4e7e1":"#### The accuracy score for this simple problem is very good.  We are able to achieve a score much higher than baseline for this competition.","721ef7c9":"## Sklearn's Random Forest Classifier\n","2e1a9dd7":"## Customized RandomForest Classifier"}}