{"cell_type":{"c4ebfa33":"code","d5b67323":"code","14aac91d":"code","1eb9b62e":"code","9bc34c62":"code","f4bb9948":"code","511a2bf8":"code","d79927d3":"code","8d2c561e":"code","f1927edb":"code","52248369":"code","c063000c":"code","1c2289b7":"code","b4e5f920":"code","ffd83289":"code","5cda0607":"code","91decfd4":"code","f20b3fd0":"code","f93a781c":"code","4e08cdab":"code","c82b2e5d":"code","b2bdd6c8":"code","9819c723":"code","04ebd231":"code","a19f0347":"code","d67af056":"code","7ed2b9d2":"code","408b949b":"code","e5d046b4":"code","be503499":"code","a7cdb086":"code","fdcd4762":"code","69b7ca28":"code","cde61257":"code","9feb9cc4":"code","3fc97066":"code","99d10565":"code","ed4f2bbb":"code","9b711634":"code","08556634":"code","4dc69ee0":"code","079ecbe2":"code","02753959":"code","64548e52":"code","7447d197":"code","b985e38a":"code","28d1aafe":"code","68dffaaf":"code","395852a6":"code","4bc6e646":"code","7a4944bb":"code","bb5a3a01":"code","31ce2676":"code","10c9d450":"code","2be97428":"code","c6ad44b4":"code","e3dab303":"code","d3133623":"code","58a980ed":"code","95f0de2c":"code","c06ef044":"code","d77f4fa7":"code","3984348a":"code","d9c43d8c":"code","56a289af":"code","e5271815":"code","a79a9664":"code","8d7ef2cd":"code","33e28f6b":"code","c45c0b07":"code","fcfdd01d":"code","d8e7806b":"code","ec33c6c9":"code","2c9e0503":"code","6f385220":"code","e6334784":"code","8a7a7cbd":"code","cbeefae6":"code","309687dc":"code","d33cc8e4":"code","64863b40":"code","22080c44":"code","7083481c":"code","ee658872":"code","0e6eb7e9":"code","fde43b7b":"code","5eff7d74":"code","b64f9373":"code","eca27e12":"code","ae92ab17":"code","3116a10b":"code","7ef1f88a":"code","a1c7627c":"code","faf855a0":"code","9beb9846":"code","75a6eadb":"code","99a01220":"code","e0616aab":"code","8f3d4ccd":"code","f02d3b10":"code","5fdc0c53":"code","5435d1d9":"code","dcc5ef99":"code","123416a2":"code","7d15369e":"code","6c909fb5":"code","14092d29":"code","6667381b":"code","78c657dd":"code","edb53e5a":"code","c2cfaa33":"code","0552c3a3":"code","64138790":"code","40b65864":"code","05f60a0b":"code","82938817":"code","dc32e07d":"code","24910659":"code","65f90a8d":"code","538fbbb8":"code","aec06b76":"code","304f58d8":"code","2f30fe6b":"markdown","37e80bda":"markdown","4c4b7762":"markdown","1cf291ea":"markdown","f53efa58":"markdown","55b93cd6":"markdown","65c32b79":"markdown","eded7b44":"markdown","2b3f3c43":"markdown","a25eb0a8":"markdown","b1a6c4f2":"markdown","44441eda":"markdown","9f75ee48":"markdown","6db86cc4":"markdown","3694e58e":"markdown","550282e6":"markdown","33578f78":"markdown","7b63974f":"markdown","e0ec5a8e":"markdown","56008b42":"markdown","670fcc6f":"markdown","df232420":"markdown","72258e84":"markdown","dbc8cc38":"markdown","7b73d646":"markdown","cd8e6948":"markdown","c0288b08":"markdown","de74a684":"markdown","ec03f090":"markdown","1b1e8334":"markdown","b7c2e512":"markdown","972aef32":"markdown","da44dd8f":"markdown","4039d38b":"markdown","ea078ec4":"markdown","b6a4cd30":"markdown","0d72532c":"markdown","82ad5c84":"markdown","e57f67ce":"markdown","b1431727":"markdown","c9785f45":"markdown","00776965":"markdown","edabd9f6":"markdown","8bb34bae":"markdown","39cffaeb":"markdown","0184f0ce":"markdown","4c57dafd":"markdown","14b0ddbb":"markdown","cd085233":"markdown","74ee3d7c":"markdown","0216c15b":"markdown","47c00ab2":"markdown","f8a64e3d":"markdown","1396fb14":"markdown","80d76565":"markdown","8b92a822":"markdown","20118c40":"markdown","8144f708":"markdown","af89b8de":"markdown","de892302":"markdown","742b0151":"markdown","9b0006c8":"markdown","d74aaa99":"markdown","8a50fba6":"markdown","aec3a6e9":"markdown","7f8b61f8":"markdown","2ca42d33":"markdown","018ce309":"markdown","3995d8ef":"markdown","32eca2dd":"markdown","3b0e8511":"markdown","ecb41413":"markdown","24216fcf":"markdown","0066dc4c":"markdown","afc2e51a":"markdown","6be561c2":"markdown","60cc1172":"markdown","4c7fd3a8":"markdown"},"source":{"c4ebfa33":"!pip install comet_ml","d5b67323":"import comet_ml\nfrom comet_ml import Experiment","14aac91d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1eb9b62e":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n#import seaborn as sns\nimport string\nimport re\nimport spacy\n\n#The Natural Language Toolkit library\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk.probability import FreqDist\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import brown \nfrom nltk import bigrams, trigrams\n\n#Machine learning library\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n#library for oversampling \nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n","9bc34c62":"!pip install -U spacy","f4bb9948":"!python -m spacy download en_core_web_md","511a2bf8":"##### Setting the API key (saved as environment variable)\nexperiment = Experiment(api_key='e03Z32Ilv0BBSwULP9xqfZwkO',\n                       project_name = \"general\",workspace=\"lee-roy\")","d79927d3":"lemmatizer = WordNetLemmatizer() \nnltk.download('wordnet')\nnltk.download('punkt')\n#nlp = spacy.load('en_core_web_md')\ntokenizer = RegexpTokenizer(r'\\w+')\nnltk.download('averaged_perceptron_tagger')","8d2c561e":"train_df = pd.read_csv('..\/input\/climate-change-belief-analysis\/train.csv')\ntest_df = pd.read_csv('..\/input\/climate-change-belief-analysis\/test.csv')","f1927edb":"def removing_stopwords(post):\n    \"\"\"\n    This function gets all the words in the tweets tokenizes, removes stopwords\n    and lemmatizes all the words in a sentence\n    \n    \"\"\"\n    stop_words = set(stopwords.words('english')) \n    word_tokens = word_tokenize(post) \n    filtered_sentence = [w for w in word_tokens if not w in stop_words] \n    filtered_sentence = [] \n    for w in word_tokens: \n        if w not in stop_words: \n            filtered_sentence.append(w) \n    \n    allwords = [lemmatizer.lemmatize(w) for w in filtered_sentence]\n    return allwords","52248369":"def nouns(post):\n    \"\"\"\n    This function gets all the nouns in tweets \n    \"\"\"\n    text = word_tokenize(post)\n    nouns = set()\n    for word, pos in nltk.pos_tag(text): \n        if pos in ['NN']:\n            nouns.add(word)\n    return nouns","c063000c":"def verbs(post):\n    \"\"\"\n    This function gets all the verbs in tweets \n    \"\"\"\n    text = word_tokenize(post)\n    nouns = set()\n    for word, pos in nltk.pos_tag(text): \n        if pos in ['VB']: \n            nouns.add(word)\n    return nouns","1c2289b7":"def tweets_from_tokens(post):\n    \"\"\"\n    This function creates clean messages from lists\n    \n    \"\"\"\n    str1 = \" \"  \n    sentence = str1.join([w for w in post])\n    return sentence","b4e5f920":"def remove_punctuation_numbers(post):\n    \"\"\"\n    This function removes all puntuation marks and numbers\n    \"\"\"\n    punc_numbers = string.punctuation + '0123456789'\n    return ''.join([l for l in post if l not in punc_numbers])","ffd83289":"def remove_urls(data):\n    \"\"\"\n    This function removes url links\n    \n    \"\"\"\n    pattern_url = r'http[s]?:\/\/(?:[A-Za-z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9A-Fa-f][0-9A-Fa-f]))+'\n    subs_url = r''\n    data['message'] = data['message'].replace(to_replace = pattern_url, value = subs_url, regex = True)\n    return data\n","5cda0607":"def remove_pattern(input_txt, pattern):\n    \"\"\"\n    This function removes specified patterns\n    \n    \"\"\"\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n        \n    return input_txt","91decfd4":"def length_of_message(data):\n    \"\"\"\n    This function gets the length of a tweet\n    \n    \"\"\"\n    data['length'] = data['message'].apply(lambda x:len(x))\n    return data","f20b3fd0":"def make_bigrams(post):\n    \"\"\"\n    Tokenise and get bi-grams from the tweets\n    \n    \"\"\"\n    text = word_tokenize(post)\n    return list(bigrams(text))","f93a781c":"def make_trigrams(post):\n    \"\"\"\n    Tokenise and get bi-grams from the tweets\n    \n    \"\"\"\n    text = word_tokenize(post)\n    return list(trigrams(text))","4e08cdab":"def metric_evaluation(y_test,predictions):\n    cfn_m = confusion_matrix(y_test,predictions)\n    c_r = classification_report(y_test,predictions)\n    accuracy = metrics.accuracy_score(y_test,predictions)\n    print(cfn_m)\n    print(c_r)\n    print(accuracy)","c82b2e5d":"def scored(y_test,predictions):\n    a = accuracy_score(y_test,predictions)\n    p = precision_score(y_test,predictions,average='macro')\n    r = recall_score(y_test,predictions,average='macro')\n    f = f1_score(y_test,predictions,average='macro')\n    return a,p,r,f","b2bdd6c8":"train_df.isnull().sum()","9819c723":"train = train_df.copy()\ntrain['unclean'] =  train['message'].str.lower()\ntrain = remove_urls(train)\ntrain['message'] = np.vectorize(remove_pattern)(train['message'], \"@[\\w]*\")\ntrain['message'] = train['message'].apply(remove_punctuation_numbers)\ntrain['message'] = train['message'].str.lower()\ntrain['POS_nouns']  = train['message'].apply(nouns)\ntrain['POS_verbs']  = train['message'].apply(verbs)\ntrain['clean_tokens']  = train['message'].apply(removing_stopwords)\ntrain['message']  = train['clean_tokens'].apply(tweets_from_tokens)","04ebd231":"def convert_bigrams(post):\n    \"\"\"\n    This function converts bigrams from tuples\n    \"\"\"\n    res = ['_'.join(tups) for tups in post]\n    return res","a19f0347":"train['bigrams_text_list']  = train['message'].apply(make_bigrams).apply(convert_bigrams)\ntrain['trigrams_text_list']  = train['message'].apply(make_trigrams).apply(convert_bigrams)","d67af056":"train['bigrams_text'] = train['bigrams_text_list'].apply(tweets_from_tokens)\ntrain['trigrams_text'] = train['trigrams_text_list'].apply(tweets_from_tokens)","7ed2b9d2":"train","408b949b":"test = test_df.copy()\ntest['unclean'] =  test['message'].str.lower()\ntest = remove_urls(test)\ntest['message'] = np.vectorize(remove_pattern)(test['message'], \"@[\\w]*\")\ntest['message'] = test['message'].apply(remove_punctuation_numbers)\ntest['message'] = test['message'].str.lower()\ntest['clean_tokens']  = test['message'].apply(removing_stopwords)\ntest['message']  = test['clean_tokens'].apply(tweets_from_tokens)","e5d046b4":"test.head()","be503499":"train['sentiment'].unique()","a7cdb086":"counts =  list(train['sentiment'].value_counts())","fdcd4762":"labels = train['sentiment'].unique()\nheights = counts\nplt.bar(labels,heights,color='tab:red')\nplt.xticks(labels)\nplt.ylabel(\"# of observations\")\nplt.xlabel(\"Sentiment\")\nplt.show()","69b7ca28":"#plot a word cloud of the most common words\nall_words = ' '.join([text for text in train['message']])\n\ndef plotwordclouds(text):\n    wordcloud = WordCloud(width=1000, height=700, random_state=21,background_color=\"White\",\n                          colormap=\"Reds\", max_font_size=110).generate(text)\n\n    plt.figure(figsize=(10, 7))\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis('off')\n    plt.show()\n\nplotwordclouds(all_words)","cde61257":"def plot_most_frequent(text,title):\n    \"\"\"\n    This function plots a bar plot of the words  \n    \"\"\"\n    mostcommon_small = FreqDist(text).most_common(25)\n    x, y = zip(*mostcommon_small)\n    plt.figure(figsize=(50,30))\n    plt.margins(0.02)\n    plt.bar(x, y,color='tab:red')\n    plt.xlabel('Words', fontsize=50)\n    plt.ylabel('Frequency of Words', fontsize=50)\n    plt.yticks(fontsize=40)\n    plt.xticks(rotation=90, fontsize=40)\n    plt.title(title, fontsize=60)\n    plt.show() ","9feb9cc4":"allwords = []\nfor wordlist in train['clean_tokens']:\n    allwords += wordlist\n    \nplot_most_frequent(allwords,'Frequency of 25 Most Common Words')","3fc97066":"allnouns = []\nfor wordlist in train['POS_nouns']:\n    allnouns += wordlist\n    \nplot_most_frequent(allnouns,'Frequency of 25 Most Common nouns')","99d10565":"allverbs = []\nfor wordlist in train['POS_verbs']:\n    allverbs += wordlist\nplot_most_frequent(allverbs,'Frequency of 25 Most Common verbs')","ed4f2bbb":"all_bigrams = ' '.join([str(text) for text in train['bigrams_text']])\nplotwordclouds(all_bigrams)","9b711634":"allwords = []\nfor wordlist in train['bigrams_text_list']:\n    allwords += wordlist\n    \nplot_most_frequent(allwords,'Frequency of 25 Most Common bi-grams')","08556634":"all_bigrams = ' '.join([str(text) for text in train['trigrams_text']])\nplotwordclouds(all_bigrams)","4dc69ee0":"allwords = []\nfor wordlist in train['trigrams_text_list']:\n    allwords += wordlist\n    \nplot_most_frequent(allwords,'Frequency of 25 Most Common Words')","079ecbe2":"for i in range(5):\n    print(train[train['sentiment']==-1]['message'].iloc[i] + '\\n')","02753959":"all_words = ' '.join([text for text in train[train['sentiment']==-1]['message']])\nplotwordclouds(all_words)","64548e52":"allwords = []\nfor wordlist in train[train['sentiment']==-1]['clean_tokens']:\n    allwords += wordlist\n    train[train['sentiment']==-1]['clean_tokens']\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","7447d197":"allnouns = []\nfor wordlist in train[train['sentiment']==-1]['POS_nouns']:\n    allnouns += wordlist\n\nplot_most_frequent(allnouns,'Frequency of 25 Most Common nouns')","b985e38a":"allverbs = []\nfor wordlist in train[train['sentiment']==-1]['POS_verbs']:\n    allverbs += wordlist\n\nplot_most_frequent(allverbs,'Frequency of 25 Most Common verbs')","28d1aafe":"all_words = ' '.join([text for text in train[train['sentiment']==-1]['bigrams_text']])\nplotwordclouds(all_words)","68dffaaf":"allwords = []\nfor wordlist in train[train['sentiment']==-1]['bigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","395852a6":"all_words = ' '.join([text for text in train[train['sentiment']==-1]['trigrams_text']])\nplotwordclouds(all_words)","4bc6e646":"allwords = []\nfor wordlist in train[train['sentiment']==-1]['trigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","7a4944bb":"for i in range(5):\n    print(train[train['sentiment']==0]['message'].iloc[i] + '\\n')","bb5a3a01":"all_words = ' '.join([text for text in train[train['sentiment']==0]['message']])\nplotwordclouds(all_words)","31ce2676":"allwords = []\nfor wordlist in train[train['sentiment']==0]['clean_tokens']:\n    allwords += wordlist\n    train[train['sentiment']==-1]['clean_tokens']\n\nplot_most_frequent(allwords,'Frequency of 20 Most Common words')","10c9d450":"allnouns = []\nfor wordlist in train[train['sentiment']==0]['POS_nouns']:\n    allnouns += wordlist\n\nplot_most_frequent(allnouns,'Frequency of 20 Most Common nouns')","2be97428":"allverbs = []\nfor wordlist in train[train['sentiment']==0]['POS_verbs']:\n    allverbs += wordlist\n\nplot_most_frequent(allverbs,'Frequency of 25 Most Common verbs')","c6ad44b4":"all_words = ' '.join([text for text in train[train['sentiment']==0]['bigrams_text']])\nplotwordclouds(all_words)","e3dab303":"allwords = []\nfor wordlist in train[train['sentiment']==0]['bigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","d3133623":"all_words = ' '.join([text for text in train[train['sentiment']==0]['trigrams_text']])\nplotwordclouds(all_words)","58a980ed":"allwords = []\nfor wordlist in train[train['sentiment']==0]['trigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","95f0de2c":"for i in range(5):\n    print(train[train['sentiment']==1]['message'].iloc[i] + '\\n')\n    ","c06ef044":"all_words = ' '.join([text for text in train[train['sentiment']==1]['message']])\nplotwordclouds(all_words)","d77f4fa7":"allwords = []\nfor wordlist in train[train['sentiment']==1]['clean_tokens']:\n    allwords += wordlist\n    train[train['sentiment']==1]['clean_tokens']\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","3984348a":"allnouns = []\nfor wordlist in train[train['sentiment']==1]['POS_nouns']:\n    allnouns += wordlist\n\nplot_most_frequent(allnouns,'Frequency of 25 Most Common nouns')","d9c43d8c":"allverbs = []\nfor wordlist in train[train['sentiment']==1]['POS_verbs']:\n    allverbs += wordlist\n\nplot_most_frequent(allverbs,'Frequency of 25 Most Common verbs')","56a289af":"all_words = ' '.join([text for text in train[train['sentiment']==1]['bigrams_text']])\nplotwordclouds(all_words)","e5271815":"allwords = []\nfor wordlist in train[train['sentiment']==1]['bigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","a79a9664":"all_words = ' '.join([text for text in train[train['sentiment']==1]['trigrams_text']])\nplotwordclouds(all_words)","8d7ef2cd":"allwords = []\nfor wordlist in train[train['sentiment']==1]['trigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","33e28f6b":"for i in range(5):\n    print(train[train['sentiment']==2]['message'].iloc[i] + '\\n')","c45c0b07":"all_words = ' '.join([text for text in train[train['sentiment']==2]['message']])\nplotwordclouds(all_words)","fcfdd01d":"allwords = []\nfor wordlist in train[train['sentiment']==2]['clean_tokens']:\n    allwords += wordlist\n\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","d8e7806b":"allnouns = []\nfor wordlist in train[train['sentiment']==2]['POS_nouns']:\n    allnouns += wordlist\n\nplot_most_frequent(allnouns,'Frequency of 25 Most Common nouns')","ec33c6c9":"allverbs = []\nfor wordlist in train[train['sentiment']==2]['POS_verbs']:\n    allverbs += wordlist\n\nplot_most_frequent(allverbs,'Frequency of 25 Most Common verbs')","2c9e0503":"all_words = ' '.join([text for text in train[train['sentiment']==2]['bigrams_text']])\nplotwordclouds(all_words)","6f385220":"allwords = []\nfor wordlist in train[train['sentiment']==2]['bigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","e6334784":"all_words = ' '.join([text for text in train[train['sentiment']==2]['trigrams_text']])\nplotwordclouds(all_words)","8a7a7cbd":"allwords = []\nfor wordlist in train[train['sentiment']==2]['trigrams_text_list']:\n    allwords += wordlist\n\nplot_most_frequent(allwords,'Frequency of 25 Most Common words')","cbeefae6":"X = train['message']\ny = train['sentiment']\ntest_x = test['message']","309687dc":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=42)","d33cc8e4":"X_up = train['unclean']\ny_up = train['unclean']\ntest_up_x = test['unclean']","64863b40":"X_train_up, X_test_up, y_train_up, y_test_up=train_test_split(X_up,y_up,test_size=0.2,random_state=42)","22080c44":"vectorizer = TfidfVectorizer()\ntfidf_vect = vectorizer.fit(X_train)\nxtrain_tfidf = tfidf_vect.transform(X_train)","7083481c":"xtest_tfidf =  tfidf_vect.transform(test_x)","ee658872":"vectorizer_up = TfidfVectorizer()\ntfidf_vect_up = vectorizer_up.fit(X_train_up)\nxtrain_tfidf_up = tfidf_vect_up.transform(X_train_up)","0e6eb7e9":"xtest_tfidf_up =  tfidf_vect_up.transform(test_up_x)","fde43b7b":"clf = LinearSVC()\nclf.fit(xtrain_tfidf,y_train)\nscores = cross_val_score(clf, xtrain_tfidf, y_train, cv=10)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","5eff7d74":"from sklearn.pipeline import Pipeline\ntext_clf = Pipeline([('tfidf',TfidfVectorizer()),('clf',LinearSVC())])\ntext_clf.fit(X_train,y_train)","b64f9373":"predictions_p = text_clf.predict(X_test)","eca27e12":"metric_evaluation(y_test,predictions_p)","ae92ab17":"accuracy,precision,recall,f1 = scored(y_test,predictions_p)","3116a10b":"predictions = clf.predict(xtest_tfidf)","7ef1f88a":"predictions_df = pd.DataFrame(data=predictions, index=test['tweetid'],\n                      columns=['sentiment'])","a1c7627c":"predictions_df","faf855a0":"predictions_df.to_csv('predictions_df.csv')","9beb9846":"clf_up = LinearSVC()\nclf_up.fit(xtrain_tfidf_up,y_train_up)\nscores = cross_val_score(clf, xtrain_tfidf_up, y_train_up, cv=10)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","75a6eadb":"clf_lr = LogisticRegression(max_iter=4000)\nclf_lr.fit(xtrain_tfidf,y_train)","99a01220":"scores = cross_val_score(clf_lr, xtrain_tfidf, y_train, cv=10)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","e0616aab":"predictions_2 = clf_lr.predict(xtest_tfidf_up)\npredictions_df_2 = pd.DataFrame(data=predictions_2, index=test['tweetid'],\n                      columns=['sentiment'])\npredictions_df_2","8f3d4ccd":"#predictions_df_2.to_csv('predictions_df_2.csv')","f02d3b10":"clf_RF = RandomForestClassifier(criterion = 'entropy',random_state=0)\nclf_RF.fit(xtrain_tfidf_up,y_train)","5fdc0c53":"scores = cross_val_score(clf_RF, xtrain_tfidf_up, y_train, cv=10)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","5435d1d9":"predictions_3 = clf_RF.predict(xtest_tfidf_up)\npredictions_df_3 = pd.DataFrame(data=predictions_3, index=test['tweetid'],\n                      columns=['sentiment'])\npredictions_df_3","dcc5ef99":"sm = SMOTE(random_state=1234)\nX_res, y_res = sm.fit_resample(xtrain_tfidf_up, y_train)","123416a2":"counts =  list(y_res.value_counts())\nlabels = y_res.unique()\nheights = counts\nplt.bar(labels,heights,color='tab:red')\nplt.xticks(labels)\nplt.ylabel(\"# of observations\")\nplt.xlabel(\"Sentiment\")\nplt.show()","7d15369e":"clf1 = LinearSVC()\nclf1.fit(X_res, y_res)","6c909fb5":"scores = cross_val_score(clf1, X_res, y_res)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","14092d29":"predictions_4 = clf1.predict(xtest_tfidf_up)","6667381b":"predictions_df4 = pd.DataFrame(data=predictions_4, index=test['tweetid'],\n                      columns=['sentiment'])","78c657dd":"predictions_df4.to_csv(r'predictions_df4.csv')","edb53e5a":"rus = RandomUnderSampler(random_state=123)\nX_res_u, y_res_u = rus.fit_resample(xtrain_tfidf_up, y_train)","c2cfaa33":"counts =  list(y_res_u.value_counts())\nlabels = y_res_u.unique()\nheights = counts\nplt.bar(labels,heights,color='tab:red')\nplt.xticks(labels)\nplt.ylabel(\"# of observations\")\nplt.xlabel(\"Sentiment\")\nplt.show()","0552c3a3":"clf2 = LinearSVC()\nclf2.fit(X_res_u, y_res_u)","64138790":"from sklearn.model_selection import cross_val_score\nscores = cross_val_score(clf2, X_res_u, y_res_u, cv=10)\nprint(\"Accuracy: %0.2f (+\/- %0.2f)\" % (scores.mean(), scores.std() * 2))","40b65864":"predictions_5 = clf2.predict(xtest_tfidf_up)\npredictions_df5 = pd.DataFrame(data=predictions_5, index=test['tweetid'],\n                      columns=['sentiment'])\npredictions_df5","05f60a0b":"#predictions_df5.to_csv(r'predictions_df5.csv')","82938817":"# creating a dictionary with all the metrics\ndua = [True,False]\nc = [0.001,0.01,0.1,1,10]\nparam_grid = {'dual':dua,\n             'C':c}","dc32e07d":"# using gridsearch to choose the best parameters\ngrid_SVM = GridSearchCV(LinearSVC(),param_grid,cv=10)\ngrid_SVM.fit(xtrain_tfidf,y_train)","24910659":"# getting a list of the best parameters\ngrid_SVM.best_params_","65f90a8d":"params = {\"model_type\": \"clf\",\n         \"stratify\": True}\nmetrics = {'accuracy': scores.mean(),\n          'precision':precision,\n          'recall':recall,\n          'f1':f1}","538fbbb8":"experiment.log_parameters(params)\nexperiment.log_metrics(metrics)","aec06b76":"experiment.end()","304f58d8":"experiment.display()","2f30fe6b":"# References","37e80bda":"Verbs may indicate how insight into how people may want to act, we notice the following key words indicating feelings towards climate change:\n\n- Fight\n- stop\n- combat\n- tackle\n- change","4c4b7762":"### IMPORTING RELAVANT LIBRARIES","1cf291ea":"### Tfidf transformation for pre-processed data","f53efa58":"## Develop a logistic regression model","55b93cd6":"When comparing the metrics of this model and the previous one we find that The linear SVM model performs better than the logistic regression model","65c32b79":"We trained our data using a few classification models. After all the training we found that the best performing model was the LinearSVC model.\nAmid all the testing, we also tried to see which would perform better between the pre processed data and the data that has not been cleaned. It became clear that the minimally pre processed data performed much better, so we ran with that data set. Next ting we thought would be interesting to try, because durin EDA we noticed that there is a large imbalance in the data, so we used SMOTE to try and tackle that issue.\n\nResampling our data didn't prove effective as it actually made our predictions worse on kaggle. Both over and undersampling. Much to our surprise actually.\n\nHaving more data on our minority classes would have been great for our evaluation. Also what we could have done better, would have to be the use of xgboosting and gradient boosting. This just seemed problematic for us as it made the run time of our code extensively long","eded7b44":"### Most common nouns","2b3f3c43":"### Train and test data (tokenized, lemmatized,and cleaned)","a25eb0a8":"## For tweets in the class \"1\" ","b1a6c4f2":"### Most common nouns ","44441eda":"### Most common verbs","9f75ee48":"After selecting our best performing model, we then need to make sure that it performs at its utmost best. To do this we will be making use of gridsearchCV in order to select the best parameters for this model. We will then log the metrics of this model onto comet.","6db86cc4":"### Most common verbs","3694e58e":"### The data that has minimal pre-processing provides the best accuracy score, therefore going forward, that data is used.","550282e6":"## First for tweets in the class \"-1\" ","33578f78":"## Develop a linear Support Vector Vachine(SVM) (Minimal pre-processing)","7b63974f":"### Apply the same to the hold-out data","e0ec5a8e":"It can be seen that there are no rows with missing tweets","56008b42":"## For tweets in the class \"2\" ","670fcc6f":"### Visualise the most used verbs","df232420":"# MODEL DEVELOPMENT","72258e84":"### Most common words\n","dbc8cc38":"# Conclusion\n","7b73d646":"### Most used words ","cd8e6948":"## For tweets in the class \"0\" ","c0288b08":"The Random Forest model does not produce more accuerate predictions\nThe linear SVM model produced the best predictions","de74a684":"### Tfidf transformation for minimally pre-processed data","ec03f090":"### Visualising bi-grams","1b1e8334":"### Visualising tri-grams","b7c2e512":"## Apply pre-processing functions\n\n1) Remove puntuation\n\n2) Make lower case\n\n3) Tokenize\n\n4) Lemmatize\n\n5) Remove stop words \n\n6) Get verbs and nouns","972aef32":"### Visualise the most used bi-grams","da44dd8f":"# CLASSIFICATION PREDICT\n# Team EN1 JHB","4039d38b":"### Plot of Tweets in each Sentiment","ea078ec4":"## We take a look at the tweets by the different sentiment classes","b6a4cd30":"We managed to run tests on quite a few models. Each of them quite unique in their execution, thus in turn providing us with different metrics altogether. We made various submissions on kaggle to assess these metrics as we've noticed they are slightly different from what we calculated on our local machines. After comparing these metrics, as well as the metrics gathered from our kaggle submissions, we were drawn to the conclusion that the model which performs best is the Linear SVM model. \n\nWe thought that after running SMOTE that we would get a model with greater accuracy, seeing as the biggest problem we encountered was the fact that the data is severely inbalanced. But we were proven wrong and te Linear SVM model stilled reigned victorious.","0d72532c":"### Most common words","82ad5c84":"### Most common nouns","e57f67ce":"## Develop a linear support vector machine (Pre-processed)","b1431727":"## Check for missing tweets","c9785f45":"### Fit a linear SVM mmodel","00776965":"### Descrption\nMany companies are built around lessening one\u2019s environmental impact or carbon footprint. They offer products and services that are environmentally friendly and sustainable, in line with their values and ideals. They would like to determine how people perceive climate change and whether or not they believe it is a real threat. This would add to their market research efforts in gauging how their product\/service may be received.\n\n### Problem Statement\nDevelop a Machine Learning model that is able to classify whether or not a person believes in climate change, based on their novel tweet data.\n\n### Benefits\nProviding an accurate and robust solution to this task gives companies access to a broad base of consumer sentiment, spanning multiple demographic and geographic categories - thus increasing their insights and informing future marketing strategies.","edabd9f6":"### Visualise the most used tri-grams","8bb34bae":"## Extract some of the tweets by sentiment value\n# All tweets","39cffaeb":"### Train and test data (minimal preprocessing)\n","0184f0ce":"### Most common verbs","4c57dafd":"### Obtain and save predictions","14b0ddbb":"Unsurprisingly, most of the tweets contain information about climate change\n\nSome keys noted\n\n- A huge number of tweets were retweets\n- Notice links to Donald Trump","cd085233":"# PRE-PROCESSING","74ee3d7c":"Once again, Donald trump appears in a lot of the tweets","0216c15b":"### Visualising tri-grams","47c00ab2":"### Visualise bi-grams","f8a64e3d":"## Over Sampling using Synthetic Minority Oversampling Technique (SMOTE) \n","1396fb14":"The greatest number of tweets can be seen in the class \"1\" followed by class \"2\"","80d76565":"https:\/\/towardsdatascience.com\/random-forest-text-classification-trump-v-obama-c09f947173dc\n\nhttps:\/\/medium.com\/fuzz\/machine-learning-classification-models-3040f71e2529#:~:text=There%20are%20a%20number%20of,level%20at%20some%20of%20these.\n\nhttps:\/\/machinelearningmastery.com\/smote-oversampling-for-imbalanced-classification\/\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2017\/03\/imbalanced-data-classification\/#:~:text=Dealing%20with%20imbalanced%20datasets%20entails,as%20it%20has%20wider%20application.","8b92a822":"### Most common nouns","20118c40":"### Visualising bi-grams","8144f708":"The Linear SVM model without sampling produced the highest accuracy, however a SMOTE model may be the best option as it will more accurately predict the minority clases","af89b8de":"## Develop a Random Forest Model","de892302":"### Most common words","742b0151":"### Visualise the most used nouns","9b0006c8":"To get an idea of the most common words, we take a look at a word cloud and a bar plot depicting the most commonly used words","d74aaa99":"# Hyperparameter tuning and comet submission","8a50fba6":"There are four unique tweet classes","aec3a6e9":"## Random Undersampling","7f8b61f8":"We see some words start to surface\n\n- China?\n- links to Donald Trump\n- hoax \n- Scott Pruitt and EPA (Scott Pruitt is the Environmental Protection Agency(EPA) Administrator)","2ca42d33":"### Cross validation is used to get the average F1 score","018ce309":"### Visualising tri-grams","3995d8ef":"Even though the accuracy is high, when formal submission was made it perfomed worse than the unbalance linear SVM. This may be because the synthetic observations are causing overfitting","32eca2dd":"### Visualise tri-grams","3b0e8511":"The biagrams do not reveal much different information from the single words, we  take a look at the trigrams next","ecb41413":"# EXPLORATORY ANALYSIS","24216fcf":"### Visualising bi-grams","0066dc4c":"## Define functions to pre-process data","afc2e51a":"### Most common verbs","6be561c2":"### Most common words","60cc1172":"## Comet submission","4c7fd3a8":"# Model Evaluation"}}