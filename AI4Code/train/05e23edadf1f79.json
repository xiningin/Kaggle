{"cell_type":{"19fdafeb":"code","60b78a83":"code","7da6f19a":"code","29587fbf":"code","3c09ed70":"code","da651f9e":"code","5025185a":"code","8c19e9b1":"code","7c9b64ae":"code","60ff74b8":"code","74215861":"code","8f470ffb":"code","c1e61e29":"code","029cb18e":"code","72ec085b":"code","b364d455":"code","beb13dc2":"code","6a50a13a":"code","41aa67c5":"code","f0dd1d1a":"code","70587e2e":"code","f6b5ba3b":"code","a0295ec7":"code","173cbafd":"code","adec8de1":"code","8c9cfb43":"code","db0b2ee4":"code","4d2b0978":"code","bbaf021f":"code","ff34f61d":"code","e4610100":"markdown","7ee233c8":"markdown"},"source":{"19fdafeb":"import numpy as np \nimport pandas as pd \npd.set_option('display.max_rows', 1000)\nfrom sklearn import decomposition\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","60b78a83":"RANDOM_STATE = 42","7da6f19a":"df_target_train = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_target_train.csv')\nprint('target_train:', df_target_train.shape)\ndf_sample_submit = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_sample_submit.csv')\nprint('sample_submit:', df_sample_submit.shape)\ndf_tracks = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_tracks.csv')\nprint('tracks:', df_tracks.shape)\ndf_genres = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_genres.csv')\nprint('genres:', df_genres.shape)\ndf_features = pd.read_csv('\/kaggle\/input\/mlclass-dubai-by-ods-lecture-5-hw\/df_features.csv')\nprint('features:', df_features.shape)","29587fbf":"train_idx = df_target_train.track_id.values\ntest_idx = df_sample_submit.track_id.values\ncomb = pd.concat([df_target_train, df_sample_submit], ignore_index=True)\ncomb = pd.merge(comb, df_tracks, on='track_id', how='inner')\ncomb.set_index('track_id', inplace=True, drop=True)","3c09ed70":"#df_features.set_index('track_id', drop=True, inplace=True)\n# pca = decomposition.PCA().fit(df_features.values)\n# plt.figure(figsize=(10,7))\n# plt.plot(np.cumsum(pca.explained_variance_ratio_), color='k', lw=2, marker='.')\n# plt.xlabel('Number of components')\n# plt.ylabel('Total explained variance')\n# plt.xlim(0, 63)\n# plt.yticks(np.arange(0, 1.1, 0.1))\n# #plt.axvline(21, c='b')\n# plt.axhline(0.9, c='r')\n# plt.show();","da651f9e":"df_features.set_index('track_id', drop=True, inplace=True)\ndf_features_idx = df_features.index\npca = decomposition.PCA(n_components=0.99, random_state=RANDOM_STATE)\nX_features_pca = pca.fit_transform(df_features)","5025185a":"df_features_pca = pd.DataFrame(X_features_pca, index=df_features_idx, columns=[f'x{i}' for i in range(X_features_pca.shape[1])])","8c19e9b1":"comb = pd.merge(comb, df_features_pca, on='track_id', how='inner')","7c9b64ae":"features = comb.columns.tolist() \ndate_cols = ['album:date_created', 'album:date_released', 'artist:active_year_begin', 'artist:active_year_end', 'track:date_created', 'track:date_recorded']\nnumericals = [c for c in features if comb[c].dtype in [int, float]]\ncategoricals = [c for c in features if comb[c].dtype==object and c not in date_cols and c!='track:genres']\ntarget = 'track:genres'","60ff74b8":"for date_col in date_cols:\n    comb[date_col] = pd.to_datetime(comb[date_col])\n    comb[date_col + '_year'] = comb[date_col].dt.year.fillna(0).astype(str) \n    comb[date_col + '_month'] = comb[date_col].dt.month.fillna(0).astype(str) \n    comb[date_col + '_week'] = comb[date_col].dt.week.fillna(0).astype(str) \n    comb[date_col + '_day'] = comb[date_col].dt.day.fillna(0).astype(str) \n    #train[date_col + '_weekofyear'] = train[date_col].dt.weekofyear # same as week\n    comb[date_col + '_dayofyear'] = comb[date_col].dt.dayofyear.fillna(0).astype(str) \n    comb[date_col + '_weekday'] = comb[date_col].dt.weekday.fillna(0).astype(str) \n    comb[date_col + '_quarter'] = comb[date_col].dt.quarter.fillna(0).astype(str) \n    comb[date_col + '_is_month_start'] = comb[date_col].dt.is_month_start.fillna(0).astype(str) \n    comb[date_col + '_is_month_end'] = comb[date_col].dt.is_month_end.fillna(0).astype(str) \n    comb[date_col + '_is_quarter_start'] = comb[date_col].dt.is_quarter_start.fillna(0).astype(str) \n    comb[date_col + '_is_quarter_end'] = comb[date_col].dt.is_quarter_end.fillna(0).astype(str) \n    comb[date_col + '_is_year_start'] = comb[date_col].dt.is_year_start.fillna(0).astype(str) \n    comb[date_col + '_is_year_end'] = comb[date_col].dt.is_year_end.fillna(0).astype(str) \n    comb.drop(date_col, axis=1, inplace=True)    \n\ncategoricals += comb.columns[-13*len(date_cols):].tolist()","74215861":"features = numericals + categoricals","8f470ffb":"labels = comb.loc[train_idx, target].apply(lambda x: x.split(' ')).apply(lambda x: [int(i) for i in x]).explode().to_frame().groupby('track:genres')\\\n             .size().to_frame().rename({0:'sizes'}, axis=1)\nlabels","c1e61e29":"from collections import defaultdict\nr = defaultdict(int)\nfor _, row in df_target_train.iterrows():\n    for x in row['track:genres'].split(' '):\n        r[int(x)] += 1\n        \nlabels_ = list(sorted(r.keys()))\nsizes_ = list(sorted(r.values()))\nassert(labels.index.values==labels_).all()\nassert(labels.sort_values('sizes').sizes.values==sizes_).all()","029cb18e":"track_labels = comb.loc[train_idx, target].apply(lambda x: x.split(' ')).apply(lambda x: [int(i) for i in x]).to_frame().explode('track:genres')\\\n                   .reset_index().set_index('track:genres')\ntrack_labels","72ec085b":"comb[categoricals] = comb[categoricals].replace(np.nan, None)\ntrain = comb.loc[train_idx, features].copy()\ntest = comb.loc[test_idx, features].copy()","b364d455":"labels.sort_values('sizes', ascending=False, inplace=True)\nlabels","beb13dc2":"models = {}\n\nfor label, row in labels.iterrows():\n    size = row.sizes\n    if (size > 1000) : # FIRST MODEL IS OBTAINED WITH LABELS WITH MORE THAN 1000 APPAREANCES\n    #if (size <= 1000) & (size>250): # SECOND MODEL IS OBTAINED WITH LABELS WITH LESS THAN 1000 APPAREANCES AND MORE THEN 250 APPAREANCES\n        print('Predico label: ' + str(label) + ' di size: '+ str(size))\n\n        positive_idx = track_labels.loc[label].track_id.values\n        train_pos = train.loc[positive_idx].copy()\n        negative_idx = train.index.difference(train_pos.index)\n        train_neg = train.loc[negative_idx].copy()\n\n        train_pos['target'] = 1\n        train_neg['target'] = 0\n\n        train_temp = pd.concat([train_neg,train_pos]).sort_index()\n        assert(train.sort_index().equals(train_temp[features]))\n\n        negative_size = train_temp.groupby('target').size().loc[0]\n        positive_size = train_temp.groupby('target').size().loc[1]\n\n        #negative_weight = round(1 - negative_size\/(negative_size+positive_size),2)\n        #positive_weight = round(1 - positive_size\/(negative_size+positive_size),2)\n        negative_weight = 1\n        positive_weight = round(negative_size\/positive_size,2)\n\n        cat_dict = {c:train_temp.columns.get_loc(c) for c in train_temp[categoricals].columns}\n\n        X_train, X_validation, y_train, y_validation = train_test_split(train_temp[features], train_temp['target'], train_size=0.7, stratify=train_temp['target'], \n                                                                        random_state=RANDOM_STATE)\n\n        models[label] = {'acc': -1}\n\n        model = CatBoostClassifier(iterations=200, random_seed=RANDOM_STATE, eval_metric='F1', class_weights=[negative_weight, positive_weight])\n        model.fit(X_train, y_train, cat_features=list(cat_dict.values()), eval_set=(X_validation, y_validation), verbose=False, plot=False)\n        #print('F1 score validation; ', f1_score(y_validation, model.predict(data=X_validation)))\n        #print('F1 score train; ', f1_score(y_train, model.predict(data=X_train)))\n        preds_validation = model.predict_proba(data=X_validation)[:,1]\n\n        best_t = -1\n        best_f1 = -1\n        for t in np.linspace(0.01, 0.99, 99):\n            f1 = f1_score(y_validation, (preds_validation >= t).astype(np.float))\n            #print(t,f1)\n            if f1 > best_f1:\n                    best_f1 = f1\n                    best_t = t\n        \n        print('Best F1 score: ', round(best_f1,2))\n        \n        if models[label]['acc'] < best_f1:\n                models[label]['acc'] = best_f1\n                models[label]['t'] = best_t\n                models[label]['model'] = model","6a50a13a":"dsfdsfdrs","41aa67c5":"# import pickle\n\n# with open('.\/models2.pkl', 'wb') as f:\n#     pickle.dump(models, f)","f0dd1d1a":"with open('\/kaggle\/input\/models-hw6\/models.pkl', 'rb') as f:\n    models1 = pickle.load(f)\nwith open('\/kaggle\/input\/models-hw6\/models2.pkl', 'rb') as f:\n    models2 = pickle.load(f)","70587e2e":"models = {**models1, **models2}\nmodels","f6b5ba3b":"from tqdm import notebook\n\ndef get_test(k=1.0):\n    g_prediction = {}\n    for g_id, d in notebook.tqdm(models.items()):\n        #p = d['model'].predict_proba(df_features.loc[df_sample_submit['track_id'].values])[:, 1]\n        p = d['model'].predict_proba(data=test[features])[:, 1]\n        g_prediction[g_id] = df_sample_submit['track_id'].values[p > k*d['t']]\n\n    track2genres = defaultdict(list)\n    for g_id, tracks in g_prediction.items():\n        for t_id in tracks:\n            track2genres[t_id].append(g_id)\n            \n    return track2genres\n\ntrack2genres = get_test(k=0.6)","a0295ec7":"np.median([len(v) for v in track2genres.values()])","173cbafd":"# for k in np.linspace(1, 2, 11):\n#     track2genres = get_test(k=k)\n#     print(k, np.median([len(v) for v in track2genres.values()]))","adec8de1":"df_sample_submit['track:genres'] = df_sample_submit.apply(lambda r: ' '.join([str(x) for x in track2genres[r['track_id']]]), axis=1)","8c9cfb43":"df_sample_submit.groupby(['track:genres']).size()","db0b2ee4":"missing_idx = df_sample_submit[df_sample_submit['track:genres']==''].index\ndf_sample_submit.loc[missing_idx, 'track:genres'] = '15 38'","4d2b0978":"df_sample_submit.groupby(['track:genres']).size().sort_values(ascending=False)","bbaf021f":"df_sample_submit.to_csv('.\/submit_06.csv', index=False)","ff34f61d":"!head .\/submit_06.csv","e4610100":"## Pre-processing","7ee233c8":"## Load data"}}