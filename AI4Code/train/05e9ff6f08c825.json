{"cell_type":{"4c725490":"code","cb1cee1e":"code","50d758f8":"code","17a04595":"code","d43a4ac4":"code","2bb7f5d8":"code","ba0e1da6":"code","b3f4c55c":"code","a472bf2c":"code","3b6fd74f":"code","bcec1b37":"code","7c8a49f7":"code","df2bc53c":"code","92270817":"code","01bd924c":"code","5534231e":"code","c05b269f":"code","7f802f5b":"code","35edf02f":"code","2aa4d283":"code","af6c588d":"code","e823e4fa":"code","439a8544":"code","8f4f245e":"code","3ca64aba":"code","51f347b9":"code","1c2b237d":"code","bee338f4":"code","652dbe62":"code","8e5d2f30":"code","8f26a501":"code","b1fc9264":"code","702fbe34":"code","2aca5b78":"code","8adb5128":"code","320066fb":"code","f9afab14":"code","fdbd7f85":"code","bc25c1f3":"code","2b4bd8dc":"code","72710c37":"code","bec9615c":"code","ec0640fd":"code","d5d79922":"code","a3b4d85b":"code","21964377":"markdown","db407d26":"markdown","c44456b7":"markdown","a00cf2b2":"markdown"},"source":{"4c725490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport jieba.posseg as pseg\nimport os\nimport keras\nprint(os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/fake-news-pair-classification-challenge\"))\nprint(os.listdir(\"..\/input\/apply-jieba-tokenizer\"))\n\n# Any results you write to the current directory are saved as output.","cb1cee1e":"# ! pip install opencc-python-reimplemented","50d758f8":"# from opencc import OpenCC\nimport pandas as pd","17a04595":"if os.path.isdir(\"..\/input\/fake-news-pair-classification-challenge\"):\n    TRAIN_CSV_PATH = '..\/input\/fake-news-pair-classification-challenge\/train.csv'\n    TEST_CSV_PATH = '..\/input\/fake-news-pair-classification-challenge\/test.csv'\n    TOKENIZED_TRAIN_CSV_PATH = \"..\/input\/apply-jieba-tokenizer\/tokenized_train.csv\"\n    TOKENIZED_TEST_CSV_PATH = \"..\/input\/apply-jieba-tokenizer\/tokenized_test.csv\"\nelse:\n    TRAIN_CSV_PATH = '..\/input\/train.csv'\n    TEST_CSV_PATH = '..\/input\/test.csv'\n    TOKENIZED_TRAIN_CSV_PATH = None","d43a4ac4":"train = pd.read_csv(TRAIN_CSV_PATH, index_col='id')\ntrain.head(3)","2bb7f5d8":"cols = ['title1_zh', \n        'title2_zh', \n        'label']\ntrain = train.loc[:, cols]\ntrain.head(3)","ba0e1da6":"text = '\u6211\u662f\u53f0\u4e2d\u4eba\uff0c\u4f46\u662f\u6211\u5728\u677f\u6a4b\u4e0a\u73ed'\nwords = pseg.cut(text)\n[word for word in words]","b3f4c55c":"def jieba_tokenizer(text):\n    words = pseg.cut(text)\n    return ' '.join([\n        word for word, flag in words if flag != 'x'])","a472bf2c":"train.isna().any()","3b6fd74f":"train.title2_zh.fillna('UNKNOWN', inplace=True)\ntrain.isna().any()","bcec1b37":"def process(data):\n    res = data.apply(jieba_tokenizer)\n    return res\n\n\ndef check_merge_idx(data, res):\n    assert((data.index == res.index).all(), 'Something error when merge data')\n\ndef parallelize(data, func):\n    from multiprocessing import cpu_count, Pool\n    cores = partitions = cpu_count()\n    data_split = np.array_split(data, partitions)\n    pool = Pool(cores)\n    res = pd.concat(pool.map(func, data_split))\n    pool.close()\n    pool.join()\n    check_merge_idx(data, res)\n    return res","7c8a49f7":"np.all(train.index == train.title1_zh.index)","df2bc53c":"if os.path.exists(TOKENIZED_TRAIN_CSV_PATH):\n    print(\"Use prepared tokenized train data\")\n    train = pd.read_csv(TOKENIZED_TRAIN_CSV_PATH, index_col='id')\nelse:\n    print(\"start to training\")\n    train['title1_tokenized'] = parallelize(train.loc[:, 'title1_zh'], process)\n    train['title2_tokenized'] = parallelize(train.loc[:, 'title2_zh'], process)\n    train.to_csv('tokenized_train.csv',index=True)","92270817":"train.loc[:, [\"title1_zh\", \"title1_tokenized\"]].head(10)","01bd924c":"train.loc[:, [\"title2_zh\", \"title2_tokenized\"]].head(10)","5534231e":"train.fillna('UNKNOWN', inplace=True)","c05b269f":"\nMAX_NUM_WORDS = 10000\ntokenizer = keras \\\n    .preprocessing \\\n    .text \\\n    .Tokenizer(num_words=MAX_NUM_WORDS)","7f802f5b":"corpus_x1 = train.title1_tokenized\ncorpus_x2 = train.title2_tokenized\ncorpus = pd.concat([\n    corpus_x1, corpus_x2])\ncorpus.shape","35edf02f":"pd.DataFrame(corpus.iloc[:5],\n             columns=['title'])","2aa4d283":"corpus.isna().any()","af6c588d":"tokenizer.fit_on_texts(corpus)\nx1_train = tokenizer \\\n    .texts_to_sequences(corpus_x1)\nx2_train = tokenizer \\\n    .texts_to_sequences(corpus_x2)","e823e4fa":"len(x1_train)","439a8544":"x1_train[:1]","8f4f245e":"for seq in x1_train[:1]:\n    print([tokenizer.index_word[idx] for idx in seq])","3ca64aba":"MAX_SEQUENCE_LENGTH = 20\nx1_train = keras \\\n    .preprocessing \\\n    .sequence \\\n    .pad_sequences(x1_train, \n                   maxlen=MAX_SEQUENCE_LENGTH)\n\nx2_train = keras \\\n    .preprocessing \\\n    .sequence \\\n    .pad_sequences(x2_train, \n                   maxlen=MAX_SEQUENCE_LENGTH)","51f347b9":"x1_train[0]","1c2b237d":"for seq in x1_train + x2_train:\n    assert len(seq) == 20\n    \nprint(\"\u6240\u6709\u65b0\u805e\u6a19\u984c\u7684\u5e8f\u5217\u9577\u5ea6\u7686\u70ba 20 !\")","bee338f4":"train.label[:5]","652dbe62":"import numpy as np \n\n# \u5b9a\u7fa9\u6bcf\u4e00\u500b\u5206\u985e\u5c0d\u61c9\u5230\u7684\u7d22\u5f15\u6578\u5b57\nlabel_to_index = {\n    'unrelated': 0, \n    'agreed': 1, \n    'disagreed': 2\n}\n\n# \u5c07\u5206\u985e\u6a19\u7c64\u5c0d\u61c9\u5230\u525b\u5b9a\u7fa9\u7684\u6578\u5b57\ny_train = train.label.apply(\n    lambda x: label_to_index[x])\n\ny_train = np.asarray(y_train) \\\n            .astype('float32')\n\ny_train[:5]","8e5d2f30":"# \u57fa\u672c\u53c3\u6578\u8a2d\u7f6e\uff0c\u6709\u5e7e\u500b\u5206\u985e\nNUM_CLASSES = 3\n\n# \u5728\u8a9e\u6599\u5eab\u88e1\u6709\u591a\u5c11\u8a5e\u5f59\nMAX_NUM_WORDS = 10000\n\n# \u4e00\u500b\u6a19\u984c\u6700\u9577\u6709\u5e7e\u500b\u8a5e\u5f59\nMAX_SEQUENCE_LENGTH = 20\n\n# \u4e00\u500b\u8a5e\u5411\u91cf\u7684\u7dad\u5ea6\nNUM_EMBEDDING_DIM = 256\n\n# LSTM \u8f38\u51fa\u7684\u5411\u91cf\u7dad\u5ea6\nNUM_LSTM_UNITS = 128","8f26a501":"x1_train[:5]","b1fc9264":"train.label[:5]","702fbe34":"y_train = keras \\\n    .utils \\\n    .to_categorical(y_train)\n\ny_train[:5]","2aca5b78":"from sklearn.model_selection \\\n    import train_test_split\n\nVALIDATION_RATIO = 0.1\n# \u5c0f\u5f69\u86cb\nRANDOM_STATE = 9527\n\nx1_train, x1_val, \\\nx2_train, x2_val, \\\ny_train, y_val = \\\n    train_test_split(\n        x1_train, x2_train, y_train, \n        test_size=VALIDATION_RATIO, \n        random_state=RANDOM_STATE\n)","8adb5128":"print(\"Training Set\")\nprint(\"-\" * 10)\nprint(f\"x1_train: {x1_train.shape}\")\nprint(f\"x2_train: {x2_train.shape}\")\nprint(f\"y_train : {y_train.shape}\")\n\nprint(\"-\" * 10)\nprint(f\"x1_val:   {x1_val.shape}\")\nprint(f\"x2_val:   {x2_val.shape}\")\nprint(f\"y_val :   {y_val.shape}\")\nprint(\"-\" * 10)\nprint(\"Test Set\")","320066fb":"# \u5efa\u7acb\u5b7f\u751f LSTM \u67b6\u69cb\uff08Siamese LSTM\uff09\nfrom keras import Input\nfrom keras.layers import Embedding,LSTM, concatenate, Dense\nfrom keras.models import Model\n\n# \u5206\u5225\u5b9a\u7fa9 2 \u500b\u65b0\u805e\u6a19\u984c A & B \u70ba\u6a21\u578b\u8f38\u5165\n# \u5169\u500b\u6a19\u984c\u90fd\u662f\u4e00\u500b\u9577\u5ea6\u70ba 20 \u7684\u6578\u5b57\u5e8f\u5217\ntop_input = Input(\n    shape=(MAX_SEQUENCE_LENGTH, ), \n    dtype='int32')\nbm_input = Input(\n    shape=(MAX_SEQUENCE_LENGTH, ), \n    dtype='int32')\n\n# \u8a5e\u5d4c\u5165\u5c64\n# \u7d93\u904e\u8a5e\u5d4c\u5165\u5c64\u7684\u8f49\u63db\uff0c\u5169\u500b\u65b0\u805e\u6a19\u984c\u90fd\u8b8a\u6210\n# \u4e00\u500b\u8a5e\u5411\u91cf\u7684\u5e8f\u5217\uff0c\u800c\u6bcf\u500b\u8a5e\u5411\u91cf\u7684\u7dad\u5ea6\n# \u70ba 256\nembedding_layer = Embedding(\n    MAX_NUM_WORDS, NUM_EMBEDDING_DIM)\ntop_embedded = embedding_layer(\n    top_input)\nbm_embedded = embedding_layer(\n    bm_input)\n\n# LSTM \u5c64\n# \u5169\u500b\u65b0\u805e\u6a19\u984c\u7d93\u904e\u6b64\u5c64\u5f8c\n# \u70ba\u4e00\u500b 128 \u7dad\u5ea6\u5411\u91cf\nshared_lstm = LSTM(NUM_LSTM_UNITS)\ntop_output = shared_lstm(top_embedded)\nbm_output = shared_lstm(bm_embedded)\n\n# \u4e32\u63a5\u5c64\u5c07\u5169\u500b\u65b0\u805e\u6a19\u984c\u7684\u7d50\u679c\u4e32\u63a5\u55ae\u4e00\u5411\u91cf\n# \u65b9\u4fbf\u8ddf\u5168\u9023\u7d50\u5c64\u76f8\u9023\nmerged = concatenate(\n    [top_output, bm_output], \n    axis=-1)\n\n# \u5168\u9023\u63a5\u5c64\u642d\u914d Softmax Activation\n# \u53ef\u4ee5\u56de\u50b3 3 \u500b\u6210\u5c0d\u6a19\u984c\n# \u5c6c\u65bc\u5404\u985e\u5225\u7684\u53ef\u80fd\u6a5f\u7387\ndense =  Dense(\n    units=NUM_CLASSES, \n    activation='softmax')\npredictions = dense(merged)\n\n# \u6211\u5011\u7684\u6a21\u578b\u5c31\u662f\u5c07\u6578\u5b57\u5e8f\u5217\u7684\u8f38\u5165\uff0c\u8f49\u63db\n# \u6210 3 \u500b\u5206\u985e\u7684\u6a5f\u7387\u7684\u6240\u6709\u6b65\u9a5f \/ \u5c64\u7684\u7e3d\u548c\nmodel = Model(\n    inputs=[top_input, bm_input], \n    outputs=predictions)\n\nmodel.summary()","f9afab14":"from keras.utils import plot_model\nimport matplotlib.pyplot as plt\nplot_model(\n    model, \n    to_file='model.png', \n    show_shapes=True, \n    show_layer_names=False, \n    rankdir='LR')\n\nfrom IPython.display import SVG\nfrom keras.utils import model_to_dot\nSVG(model_to_dot(model, rankdir='LR', show_shapes=True, show_layer_names=False,).create(prog='dot', format='svg'))","fdbd7f85":"from keras.optimizers import Adam","bc25c1f3":"lr = 1e-3\nopt = Adam(lr=lr, decay=lr\/50)\nmodel.compile(\n    optimizer='adam',\n    loss='categorical_crossentropy',\n    metrics=['accuracy'])","2b4bd8dc":"x1_train[:9527].shape","72710c37":"# \u6c7a\u5b9a\u4e00\u6b21\u8981\u653e\u591a\u5c11\u6210\u5c0d\u6a19\u984c\u7d66\u6a21\u578b\u8a13\u7df4\nBATCH_SIZE = 512\n\n# \u6c7a\u5b9a\u6a21\u578b\u8981\u770b\u6574\u500b\u8a13\u7df4\u8cc7\u6599\u96c6\u5e7e\u904d\nNUM_EPOCHS = 50\n\n# \u5be6\u969b\u8a13\u7df4\u6a21\u578b\nhistory = model.fit(\n    # \u8f38\u5165\u662f\u5169\u500b\u9577\u5ea6\u70ba 20 \u7684\u6578\u5b57\u5e8f\u5217\n    x=[x1_train, x2_train], \n    y=y_train,\n    batch_size=BATCH_SIZE,\n    epochs=NUM_EPOCHS,\n    # \u6bcf\u500b epoch \u5b8c\u5f8c\u8a08\u7b97\u9a57\u8b49\u8cc7\u6599\u96c6\n    # \u4e0a\u7684 Loss \u4ee5\u53ca\u6e96\u78ba\u5ea6\n    validation_data=(\n        [x1_val, x2_val], \n        y_val\n    ),\n    # \u6bcf\u500b epoch \u96a8\u6a5f\u8abf\u6574\u8a13\u7df4\u8cc7\u6599\u96c6\n    # \u88e1\u982d\u7684\u6578\u64da\u4ee5\u8b93\u8a13\u7df4\u904e\u7a0b\u66f4\u7a69\u5b9a\n    shuffle=True\n)","bec9615c":"import pandas as pd\nif os.path.exists(TOKENIZED_TEST_CSV_PATH):\n    print(\"Use tokenized test csv\")\n    test = pd.read_csv(TOKENIZED_TEST_CSV_PATH, index_col=0)\nelse:\n    print(\"Use raw test csv\")\n    test = pd.read_csv(TEST_CSV_PATH, index_col=0)\n    test.fillna('UNKNOWN', inplace=True)\n    test['title1_tokenized'] = parallelize(test.loc[:, 'title1_zh'], process)\n    test['title2_tokenized'] = parallelize(test.loc[:, 'title2_zh'], process)\n    test.fillna('UNKNOWN', inplace=True)\ntest.head(3)","ec0640fd":"\n# \u5c07\u8a5e\u5f59\u5e8f\u5217\u8f49\u70ba\u7d22\u5f15\u6578\u5b57\u7684\u5e8f\u5217\nx1_test = tokenizer \\\n    .texts_to_sequences(\n        test.title1_tokenized)\nx2_test = tokenizer \\\n    .texts_to_sequences(\n        test.title2_tokenized)\n\n# \u70ba\u6578\u5b57\u5e8f\u5217\u52a0\u5165 zero padding\nx1_test = keras \\\n    .preprocessing \\\n    .sequence \\\n    .pad_sequences(\n        x1_test, \n        maxlen=MAX_SEQUENCE_LENGTH)\nx2_test = keras \\\n    .preprocessing \\\n    .sequence \\\n    .pad_sequences(\n        x2_test, \n        maxlen=MAX_SEQUENCE_LENGTH)    \n\n# \u5229\u7528\u5df2\u8a13\u7df4\u7684\u6a21\u578b\u505a\u9810\u6e2c\npredictions = model.predict(\n    [x1_test, x2_test])","d5d79922":"predictions[:5]","a3b4d85b":"index_to_label = {v: k for k, v in label_to_index.items()}\n\ntest['Category'] = [index_to_label[idx] for idx in np.argmax(predictions, axis=1)]\n\nsubmission = test \\\n    .loc[:, ['Category']] \\\n    .reset_index()\n\nsubmission.columns = ['Id', 'Category']\nsubmission.to_csv('submission.csv', index=False)\nsubmission.head()","21964377":"# \u6587\u672c\u5206\u8a5e\n\u6587\u672c\u5206\u8a5e\uff08Text Segmentation\uff09\u662f\u4e00\u500b\u5c07\u4e00\u9023\u4e32\u6587\u5b57\u5207\u5272\u6210\u591a\u500b\u6709\u610f\u7fa9\u7684\u55ae\u4f4d\u7684\u6b65\u9a5f\u3002\n\n\u9019\u55ae\u4f4d\u53ef\u4ee5\u662f\n\n* \u4e00\u500b\u4e2d\u6587\u6f22\u5b57 \/ \u82f1\u6587\u5b57\u6bcd\uff08Character\uff09\n* \u4e00\u500b\u4e2d\u6587\u8a5e\u5f59 \/ \u82f1\u6587\u55ae\u5b57\uff08Word\uff09\n* \u4e00\u500b\u4e2d\u6587\u53e5\u5b50 \/ \u82f1\u6587\u53e5\u5b50\uff08Sentence\uff09\n\n\u4f9d\u7167\u4e0d\u540c\u7684 NLP \u4efb\u52d9\u6703\u6709\u4e0d\u540c\u5207\u5272\u9700\u6c42\uff0c\u4f46\u5f88\u5e38\u898b\u7684\u5207\u6cd5\u662f\u4ee5\u55ae\u5b57\uff08Word\uff09\u70ba\u55ae\u4f4d\uff0c\u4e5f\u5c31\u662f Word Segmentation\u3002","db407d26":"# **Reference by[ Lee Meng's](https:\/\/leemeng.tw\/) [Post](https:\/\/leemeng.tw\/shortest-path-to-the-nlp-world-a-gentle-guide-of-natural-language-processing-and-deep-learning-for-everyone.html#%E8%B3%87%E6%96%99%E5%89%8D%E8%99%95%E7%90%86%EF%BC%9A%E8%AE%93%E6%A9%9F%E5%99%A8%E8%83%BD%E5%A4%A0%E8%99%95%E7%90%86%E6%96%87%E5%AD%97)**","c44456b7":"# \u5e8f\u5217\u7684 Zero Padding","a00cf2b2":"# \u5c07 Label \u505a One-hot Encoding"}}