{"cell_type":{"f377f16a":"code","bee89794":"code","a5d50f20":"code","0ccc9c2d":"code","6ae24062":"code","b9a39c38":"code","cbd24da0":"code","8d7f0a34":"code","f3557db8":"code","3d2de35f":"code","936ab691":"code","2021727e":"code","c8eb0d23":"code","96799925":"code","0ed0be32":"code","cd05665a":"code","6b14e378":"code","2753a5e5":"code","674c5017":"code","46464f1a":"code","70dfeeca":"code","1dabb8d1":"code","f45f1588":"code","e32528ef":"code","d83dc703":"code","5842aa84":"code","2397c8fe":"code","b9e7d996":"code","d8b6e575":"code","889d930a":"code","3b5be04d":"code","19763b03":"code","4e951ea9":"code","be3ff90f":"code","59b4af10":"code","0b19180e":"code","307e3672":"code","bf39165e":"code","a8d1d921":"code","a8d6df57":"code","9e4d2a05":"code","ff7a0cb7":"code","e2f18924":"code","a5675387":"code","f09ed206":"code","903d099a":"code","b6c04069":"code","c68207af":"code","13d97f71":"markdown","bb6274f3":"markdown","1d95deaf":"markdown","238b3149":"markdown","93280948":"markdown","470f0aa9":"markdown","7aff425c":"markdown","e489f6c3":"markdown","df60b8f8":"markdown","950806b4":"markdown","464165cf":"markdown","bd08e48f":"markdown","c94ef314":"markdown","0b74ea8b":"markdown","1acc18dd":"markdown","6e9a747b":"markdown","e58531d4":"markdown","e4751b3d":"markdown","a88a8b9e":"markdown","34576797":"markdown","7780ef8c":"markdown","868a70e2":"markdown","c57705ca":"markdown","882e1f47":"markdown","5c12bf5c":"markdown","1001a5e3":"markdown","b7fe5e47":"markdown","65af30eb":"markdown","e8805002":"markdown","b476297d":"markdown","a9c70016":"markdown","a6d4e9db":"markdown","580d169f":"markdown","b8a005db":"markdown","873a6ae7":"markdown","49ca26d2":"markdown","dbf7b6f9":"markdown","1402d5ae":"markdown","4c1d9274":"markdown","59589ef3":"markdown","476377bc":"markdown","2464485d":"markdown","bbfd1a00":"markdown","cd3b4fca":"markdown","366cc9f7":"markdown","d822d8d2":"markdown","e5d5eccd":"markdown","1aae8dbf":"markdown","0ca50e2d":"markdown","c686b237":"markdown"},"source":{"f377f16a":"import numpy as np\nimport pylab as pl\nimport pandas as pd\nimport matplotlib.pyplot as plt \n%matplotlib inline\nimport seaborn as sns\nfrom sklearn.utils import shuffle\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.model_selection import cross_val_score, GridSearchCV\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","bee89794":"data = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndata","a5d50f20":"display(data[[\"diagnosis\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]].groupby([\"diagnosis\"]).agg([\"max\",'mean',\"min\"]).style.background_gradient(cmap=\"Oranges\"))","0ccc9c2d":"bmbc= data['diagnosis'].value_counts()  \nplt.figure(figsize=(6,4))\nsns.barplot(bmbc.index, bmbc.values, alpha=0.8)\nplt.ylabel('Number of Data', fontsize=12)\nplt.xlabel('diagnosis', fontsize=9)\nplt.xticks(rotation=90)\nplt.show();","6ae24062":"f, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"radius_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of radius_mean\")","b9a39c38":"f1, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"texture_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of texture_mean\")","cbd24da0":"f2, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"perimeter_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of perimeter_mean\")","8d7f0a34":"f3, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"area_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of area_mean\")","f3557db8":"f4, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"smoothness_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of smoothness_mean\")","3d2de35f":"f5, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"compactness_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of compactness_mean\")","936ab691":"f6, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"concavity_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of concavity_mean\")","2021727e":"f7, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"concave points_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of concave points_mean\")","c8eb0d23":"f8, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"symmetry_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of symmetry_mean\")","96799925":"f9, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"fractal_dimension_mean\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of fractal_dimension_mean\")","0ed0be32":"f11, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"radius_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of radius_se \")","cd05665a":"f12, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"texture_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of texture_se\")","6b14e378":"f13, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"perimeter_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of perimeter_se \")","2753a5e5":"f14, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"area_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of area_se\")","674c5017":"f15, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"smoothness_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of smoothness_se\")","46464f1a":"f16, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"compactness_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of compactness_se\")","70dfeeca":"f17, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"concavity_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of concavity_se\")","1dabb8d1":"f18, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"symmetry_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of symmetry_se\")","f45f1588":"f19, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"fractal_dimension_se\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of fractal_dimension_se\")","e32528ef":"f20, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"radius_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of radius_worst\")","d83dc703":"f21, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"texture_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of texture_worst\")","5842aa84":"f22, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"perimeter_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of perimeter_worst\")","2397c8fe":"f23, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"area_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of area_worst\")","b9e7d996":"f24, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"smoothness_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of smoothness_worst\")","d8b6e575":"f25, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"compactness_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of compactness_worst\")","889d930a":"f26, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"concavity_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of concavity_worst\")","3b5be04d":"f27, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"concave points_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of concave points_worst\")","19763b03":"f28, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"symmetry_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of symmetry_worst\")","4e951ea9":"f29, axes = plt.subplots(1,1, figsize = (16, 5))\ng1 = sns.distplot(data[\"fractal_dimension_worst\"], color=\"red\",ax = axes)\nplt.title(\"Distributional of fractal_dimension_worst\")","be3ff90f":"## Print the categorical columns\nprint([c for c in data.columns if (1<data[c].nunique()) & (data[c].dtype != np.number)& (data[c].dtype != int) ])","59b4af10":"def diagnosis(x):\n    if x=='M'              :   return 1\n    if x=='B'              :   return 0\ndata['diagnosis'] = data['diagnosis'].apply(diagnosis)\ndata","0b19180e":"\nprint(\"Any missing sample in data set:\",data.isnull().values.any(), \"\\n\")","307e3672":"#replace missing value with mean\ndata = data.replace([np.inf, -np.inf], np.nan)\ndata= data.fillna(data.mean())\ndata","bf39165e":"#drop this column because it's not necessary (null)\ndata= data.drop(columns=[\"Unnamed: 32\"])","a8d1d921":"corr=data.corr()[\"diagnosis\"]\ncorr[np.argsort(corr, axis=0)[:-1]]","a8d6df57":"#plotting correlations\nnum_feat=data.columns[data.dtypes!=object]\nnum_feat=num_feat [:-1]\nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(data[col].values, data.diagnosis.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(8,15))\nrects = ax.barh(ind, np.array(values), color='magenta')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients each feature with target\");","9e4d2a05":"corrMatrix=data[[\"diagnosis\",\"radius_mean\",\"texture_mean\",\"perimeter_mean\",\"area_mean\",\"smoothness_mean\",\"compactness_mean\",\n                # \"concavity_mean\",\"concave points_mean\",\"symmetry_mean\",\"fractal_dimension_mean\",\"radius_se\",\"texture_se\",\n                # \"perimeter_se\",\"area_se\",\"smoothness_se\",\"compactness_se\",\"concavity_se\",\"concave points_se\",\"symmetry_se\",\n               #  \"fractal_dimension_se\",\"radius_worst\",\"texture_worst\",\"perimeter_worst\",\"area_worst\",\"smoothness_worst\",\n                 \"compactness_worst\",\"concavity_worst\",\"concave points_worst\",\"symmetry_worst\",\"fractal_dimension_worst\"]].corr()\n\nsns.set(font_scale=1.10)\nplt.figure(figsize=(15, 10))\n\nsns.heatmap(corrMatrix, vmax=.8, linewidths=0.01,\n            square=True,annot=True,cmap='viridis',linecolor=\"white\")\nplt.title('Correlation between features');","ff7a0cb7":"from sklearn.model_selection import train_test_split\nY = data['diagnosis']\nX = data.drop(columns=['diagnosis'])\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=9)\nprint('X train shape: ', X_train.shape)\nprint('Y train shape: ', Y_train.shape)\nprint('X test shape: ', X_test.shape)\nprint('Y test shape: ', Y_test.shape)","e2f18924":"from sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nGBR = GradientBoostingClassifier()\n\nparameters = {'learning_rate': [0.01,0.02,0.03,0.04],\n                  'subsample'    : [0.9, 0.5, 0.2, 0.1],\n                  'n_estimators' : [100,500,1000, 1500],\n                  'max_depth'    : [4,6,8,10]\n                 }\nmodel = GridSearchCV(estimator=GBR, param_grid = parameters, cv = 2, n_jobs=-1)\nmodel.fit(X_train,Y_train)","a5675387":"# We predict test values\nY_predict = model.predict(X_test)","f09ed206":"# The confusion matrix\nmodel_cm = confusion_matrix(Y_test, Y_predict)\nf, ax = plt.subplots(figsize=(5,5))\nsns.heatmap(model_cm, annot=True, linewidth=0.7, linecolor='red', fmt='g', ax=ax, cmap=\"BuPu\")\nplt.title(' GradientBoostingClassifier Confusion Matrix')\nplt.xlabel('Y predict')\nplt.ylabel('Y test')\nplt.show()","903d099a":"test_acc = round(model.fit(X_train,Y_train).score(X_test, Y_test)* 100, 2)\ntrain_acc = round(model.fit(X_train, Y_train).score(X_train, Y_train)* 100, 2)","b6c04069":"model = pd.DataFrame({\n    'Model': [' GradientBoostingClassifier'],\n    'Train Score': [train_acc],\n    'Test Score': [test_acc]\n})\nmodel.sort_values(by='Test Score', ascending=False)","c68207af":"from sklearn.metrics import average_precision_score\naverage_precision = average_precision_score(Y_test, Y_predict)\n\nprint('Average precision-recall score: {0:0.2f}'.format(\n      average_precision))","13d97f71":"## Distributional of texture_mean","bb6274f3":"## Distributional of texture_se","1d95deaf":"## Distributional of concavity_se","238b3149":"## Distributional of texture_worst","93280948":"# Precision and Recall\n\nPrecision is a description of random errors, a measure of statistical variability. In simpler terms, given a set of data points from repeated measurements of the same quantity, the set can be said to be accurate if their average is close to the true value of the quantity being measured, while the set can be said to be precise if the values are close to each other. While Recall is defined as the fraction of relevant documents retrieved compared to the total number of relevant documents (true positives divided by true positives+false negatives)","470f0aa9":"## Distributional of symmetry_mean","7aff425c":"## Distributional of area_mean","e489f6c3":"# Accuracy\n\nis closeness of the measurements to a specific value. Accuracy has two definitions:\n\nMore commonly, it is a description of systematic errors, a measure of statistical bias; low accuracy causes a difference between a result and a \"true\" value. ISO calls this trueness.\nAlternatively, ISO defines[[2](https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision)] accuracy as describing a combination of both types of observational error above (random and systematic), so high accuracy requires both high precision and high trueness.","df60b8f8":"# <center> Prediction of benign and malignant breast cancer wisconsin<\/center>","950806b4":"## Distributional of smoothness_mean","464165cf":"## Distributional of fractal_dimension_worst","bd08e48f":"# Prepocessing","c94ef314":"## Compare the features\n\nI did compare the features that group by diagnosis and get the max, min, and mean of the features","0b74ea8b":"# Check Missing values ","1acc18dd":"## Distributional of concavity_mean","6e9a747b":"## Distributional of smoothness_worst","e58531d4":"## Distributional of area_se","e4751b3d":"## Distributional of compactness_worst","a88a8b9e":"# Measurement","34576797":"## Distributional of fractal_dimension_mean","7780ef8c":"## Distributional of radius_worst","868a70e2":"## Distributional of symmetry_worst","c57705ca":"## Distributional of area_worst","882e1f47":"## Distributional of smoothness_se","5c12bf5c":"## Distributional of perimeter_se","1001a5e3":"# Visualization","b7fe5e47":"# Model\n\n# GradientBoostingClassifier \n\nGB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions. In each stage n_classes_ regression trees are fit on the negative gradient of the binomial or multinomial deviance loss function. Binary classification is a special case where only a single regression tree is induced. Detail see : https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html","65af30eb":"## Distributional of concavity_worst","e8805002":"## Distributional of compactness_se","b476297d":"## Distributional of radius_se","a9c70016":"## Distributional of concave points_mean","a6d4e9db":"# Load Dataset","580d169f":"  <center>\n  \n  <img src=\"https:\/\/storage.googleapis.com\/kaggle-datasets-images\/180\/384\/3da2510581f9d3b902307ff8d06fe327\/dataset-cover.jpg\" width=\"900\">\n    \n    <\/center>","b8a005db":"# Import Library","873a6ae7":"# About Dataset\n\n\n\n\n\nFeatures are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass. They describe characteristics of the cell nuclei present in the image.\nn the 3-dimensional space is that described in: [K. P. Bennett and O. L. Mangasarian: \"Robust Linear Programming Discrimination of Two Linearly Inseparable Sets\", Optimization Methods and Software 1, 1992, 23-34].\n\nThis database is also available through the UW CS ftp server:\nftp ftp.cs.wisc.edu\ncd math-prog\/cpo-dataset\/machine-learn\/WDBC\/\n\nAlso can be found on UCI Machine Learning Repository: https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Wisconsin+%28Diagnostic%29\n\n\n\n## Attribute Information:\n\n1) ID number\n\n2) Diagnosis (M = malignant, B = benign)\n\n3-32)\n\nTen real-valued features are computed for each cell nucleus:\n\na) radius (mean of distances from center to points on the perimeter)\n\nb) texture (standard deviation of gray-scale values)\n\nc) perimeter\n\nd) area\n\ne) smoothness (local variation in radius lengths)\n\nf) compactness (perimeter^2 \/ area - 1.0)\n\ng) concavity (severity of concave portions of the contour)\n\nh) concave points (number of concave portions of the contour)\n\ni) symmetry\n\nj) fractal dimension (\"coastline approximation\" - 1)\n\n\nThe mean, standard error and \"worst\" or largest (mean of the three\nlargest values) of these features were computed for each image,\nresulting in 30 features. For instance, field 3 is Mean Radius, field\n13 is Radius SE, field 23 is Worst Radius.\n\n\nAll feature values are recoded with four significant digits.\n\n\nMissing attribute values: none\n\n\nClass distribution: 357 benign, 212 malignant","49ca26d2":"## Distributional of perimeter_mean","dbf7b6f9":"# Plotting correlations","1402d5ae":"# Confusion Matrix\nis commonly used for a summarization of prediction results on a classification problem.The number of correct and incorrect predictions is summarized with counting values and each value broken down for each class. Each of them is the key to the confusion matrix. It shows the classification model is confused when it makes predictions, at this point in here it gives us insight not only into the errors being made by a classifier but also show the types of errors that are being made [[1](https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/)].","4c1d9274":"As you can see above, we have a missing value. Missing values in the training data set can affect the prediction or classification of a model negatively. Also, some machine learning algorithms can't accept missing data eg. SVM. So in this study, I tried to handle missing values with mean.\n\nBut filling missing values with mean\/median\/mode or using another predictive model to predict missing values is also a prediction that may not be 100% accurate, instead, you can use models like Decision Trees and Random Forest which handle missing values very well.  For more information to handle missing value, check out this wonderful article: https:\/\/machinelearningmastery.com\/handle-missing-data-python\/","59589ef3":"## Distributional of radius_mean","476377bc":"# Heatmap","2464485d":"## Distributional of perimeter_worst","bbfd1a00":"# SPLIT TRAINING TEST\n\n","cd3b4fca":"Data for training and testing. To select a set of training data that will be input in the Machine Learning algorithm, to ensure that the classification algorithm training can be generalized well to new data. For this study using a sample size of 20%, assumed it ideal ratio between training and testing","366cc9f7":"## Distributional of fractal_dimension_se","d822d8d2":"## Distributional of compactness_mean","e5d5eccd":"# Correlation in Data\n\nIn this work, I try to measure correlation in data using Correlation coefficients.\n\nCorrelation coefficientsare used to measure how strong a relationship is between two variables.Correlation coefficient formulas are used to find how strong a relationship is between data. The formulas return a value between -1 and 1, where:\n\n1 indicates a strong positive relationship. -1 indicates a strong negative relationship.\n\nA result of zero indicates no relationship at all.\n\n<img src=\"https:\/\/www.statisticshowto.com\/wp-content\/uploads\/2012\/10\/pearson-2-small.png\" widhth=\"400\">","1aae8dbf":"## Distributional of concave points_worst","0ca50e2d":"## Distributional of symmetry_se","c686b237":"# References:\n\n1. https:\/\/www.geeksforgeeks.org\/confusion-matrix-machine-learning\/\n1. https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision\n1. https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html\n1. https:\/\/machinelearningmastery.com\/handle-missing-data-python\/"}}