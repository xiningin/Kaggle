{"cell_type":{"05ae7fda":"code","a38c2399":"code","64787661":"code","49b3c8fe":"code","1b3dedb2":"code","806d5420":"code","3c82c3a5":"code","089aa04e":"code","dcec5183":"code","8beb67ff":"code","523aedcd":"code","79f1bdc7":"code","35cc0466":"code","e8bdb2f8":"code","1c8b2115":"code","d703a311":"code","6d0c7864":"code","0f2ebdd8":"code","1439a3ac":"code","a1e67e71":"code","69c361ce":"code","b6abd732":"code","5593e7e9":"code","315813d4":"code","cbca0280":"code","e49a239d":"code","6df22b63":"code","5950f157":"code","d7aa2760":"code","1f17e8f2":"code","aeab2248":"code","506e4b49":"code","05185ca2":"code","c18a9976":"code","40a30e4d":"code","ae37ce80":"code","85451469":"code","42b7f5f2":"code","670c504e":"code","e2aa1234":"code","07ab0b0c":"markdown","3ee44157":"markdown","2903f941":"markdown","fd59d70c":"markdown","fedad5b2":"markdown","7a129da2":"markdown","1c25a345":"markdown","dc2753a3":"markdown","81add0ce":"markdown","02bec658":"markdown","78526961":"markdown","70b7951b":"markdown","9d79dc29":"markdown"},"source":{"05ae7fda":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","a38c2399":"train_df=pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_df=pd.read_csv(\"..\/input\/titanic\/test.csv\")","64787661":"train_df.head()","49b3c8fe":"def missingdata(data):\n    total = data.isnull().sum().sort_values(ascending = False)\n    percent = (data.isnull().sum()\/data.isnull().count()*100).sort_values(ascending = False)\n    ms=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    ms= ms[ms[\"Percent\"] > 0]\n    f,ax =plt.subplots(figsize=(8,6))\n    plt.xticks(rotation='90')\n    fig=sns.barplot(ms.index, ms[\"Percent\"],color=\"green\",alpha=0.8)\n    plt.xlabel('Features', fontsize=15)\n    plt.ylabel('Percent of missing values', fontsize=15)\n    plt.title('Percent missing data by feature', fontsize=15)\n    return ms","1b3dedb2":"missingdata(train_df)","806d5420":"missingdata(test_df)","3c82c3a5":"test_df['Age'].mean()","089aa04e":"train_df['Embarked'].fillna(train_df['Embarked'].mode()[0], inplace = True)","dcec5183":"test_df['Fare'].fillna(test_df['Fare'].median(), inplace = True)","8beb67ff":"drop_column = ['Cabin']# with more missing values \ntrain_df.drop(drop_column, axis=1, inplace = True)\ntest_df.drop(drop_column,axis=1,inplace=True)","523aedcd":"test_df['Age'].fillna(test_df['Age'].median(), inplace = True)\ntrain_df['Age'].fillna(train_df['Age'].median(), inplace = True)","79f1bdc7":"print('check the nan value in train data')\nprint(train_df.isnull().sum())\nprint('___'*30)\nprint('check the nan value in test data')\nprint(test_df.isnull().sum())","35cc0466":"## combine test and train as single to apply some function\nall_data=[train_df,test_df]","e8bdb2f8":"# Create new feature FamilySize as a combination of SibSp and Parch\nfor dataset in all_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1","1c8b2115":"import re\n# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n# Create a new feature Title, containing the titles of passenger names\nfor dataset in all_data:\n    dataset['Title'] = dataset['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nfor dataset in all_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', \n                                                 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')","d703a311":"## create bin for age features\nfor dataset in all_data:\n    dataset['Age_bin'] = pd.cut(dataset['Age'], bins=[0,12,20,40,120], labels=['Children','Teenage','Adult','Elder'])","6d0c7864":"## create bin for fare features\nfor dataset in all_data:\n    dataset['Fare_bin'] = pd.cut(dataset['Fare'], bins=[0,7.91,14.45,31,120], labels=['Low_fare','median_fare',\n                                                                                      'Average_fare','high_fare'])","0f2ebdd8":"### for our reference making a copy of both DataSet start working for copy of dataset\ntraindf=train_df\ntestdf=test_df","1439a3ac":"all_dat=[traindf,testdf]","a1e67e71":"for dataset in all_dat:\n    drop_column = ['Age','Fare','Name','Ticket']\n    dataset.drop(drop_column, axis=1, inplace = True)","69c361ce":"drop_column = ['PassengerId']\ntraindf.drop(drop_column, axis=1, inplace = True)","b6abd732":"testdf.head(2)","5593e7e9":"traindf = pd.get_dummies(traindf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])\n","315813d4":"testdf = pd.get_dummies(testdf, columns = [\"Sex\",\"Title\",\"Age_bin\",\"Embarked\",\"Fare_bin\"],\n                             prefix=[\"Sex\",\"Title\",\"Age_type\",\"Em_type\",\"Fare_type\"])","cbca0280":"traindf.head()","e49a239d":"g = sns.pairplot(data=train_df, hue='Survived', palette = 'seismic',\n                 size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\ng.set(xticklabels=[])","6df22b63":"from sklearn.model_selection import train_test_split #for split the data\nfrom sklearn.metrics import accuracy_score  #for accuracy_score\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nall_features = traindf.drop(\"Survived\",axis=1)\nTargeted_feature = traindf[\"Survived\"]\nX_train,X_test,y_train,y_test = train_test_split(all_features,Targeted_feature,test_size=0.3,random_state=42)","5950f157":"from sklearn.linear_model import LogisticRegression # Logistic Regression\n\nmodel = LogisticRegression()\nmodel.fit(X_train,y_train)\nprediction_lr=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Logistic Regression is',round(accuracy_score(prediction_lr,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_lr=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Logistic REgression is:',round(result_lr.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","d7aa2760":"from sklearn.svm import SVC, LinearSVC\n\nmodel = SVC()\nmodel.fit(X_train,y_train)\nprediction_svm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Support Vector Machines Classifier is',round(accuracy_score(prediction_svm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_svm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Support Vector Machines Classifier is:',round(result_svm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","1f17e8f2":"from sklearn.tree import DecisionTreeClassifier\nmodel= DecisionTreeClassifier(criterion='gini', \n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto')\nmodel.fit(X_train,y_train)\nprediction_tree=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the DecisionTree Classifier is',round(accuracy_score(prediction_tree,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_tree=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Decision Tree classifier is:',round(result_tree.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","aeab2248":"from sklearn.ensemble import GradientBoostingClassifier\nmodel= GradientBoostingClassifier()\nmodel.fit(X_train,y_train)\nprediction_gbc=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Gradient Boosting Classifier is',round(accuracy_score(prediction_gbc,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_gbc=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for AdaBoostClassifier is:',round(result_gbc.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","506e4b49":"from sklearn.ensemble import RandomForestClassifier\nmodel = RandomForestClassifier(criterion='gini', n_estimators=700,\n                             min_samples_split=10,min_samples_leaf=1,\n                             max_features='auto',oob_score=True,\n                             random_state=1,n_jobs=-1)\nmodel.fit(X_train,y_train)\nprediction_rm=model.predict(X_test)\nprint('--------------The Accuracy of the model----------------------------')\nprint('The accuracy of the Random Forest Classifier is',round(accuracy_score(prediction_rm,y_test)*100,2))\nkfold = KFold(n_splits=10, random_state=22) # k=10, split the data into 10 equal parts\nresult_rm=cross_val_score(model,all_features,Targeted_feature,cv=10,scoring='accuracy')\nprint('The cross validated score for Random Forest Classifier is:',round(result_rm.mean()*100,2))\ny_pred = cross_val_predict(model,all_features,Targeted_feature,cv=10)","05185ca2":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines','Logistic Regression',  \n              'Gradient Decent','Decision Tree','Random forest'],\n    'Score': [result_svm.mean(),result_lr.mean(),result_gbc.mean(), result_tree.mean(),result_rm.mean()]})\nmodels.sort_values(by='Score',ascending=False)","c18a9976":"train_X = traindf.drop(\"Survived\", axis=1)\ntrain_Y=traindf[\"Survived\"]\ntest_X  = testdf.drop(\"PassengerId\", axis=1).copy()\ntrain_X.shape, train_Y.shape, test_X.shape","40a30e4d":"import xgboost as xgb\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\nmodel = GradientBoostingClassifier()\nparam_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300,400],\n              'learning_rate': [0.1, 0.05, 0.01,0.001],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.2,0.1] \n              }\n\nmodelf = GridSearchCV(model,param_grid = param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\nmodelf.fit(train_X,train_Y)\nmodelf.best_score_\nmodelf.best_estimator_","ae37ce80":"modelf.best_score_","85451469":"model= SVC()\nparam_grid = {'kernel': ['rbf','linear'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\nmodelsvm = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodelsvm.fit(train_X,train_Y)\n\nprint(modelsvm.best_estimator_)\n\n# Best score\nprint(modelsvm.best_score_)","42b7f5f2":"model = RandomForestClassifier()\nn_estim=range(100,1000,100)\nparam_grid = {\"n_estimators\" :n_estim}\nmodel_rf = GridSearchCV(model,param_grid = param_grid, cv=5, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\nmodel_rf.fit(train_X,train_Y)\n\nprint(model_rf.best_score_)\nmodel_rf.best_estimator_","670c504e":"# Random Forests\nfrom sklearn.ensemble import RandomForestClassifier\nrandom_forest = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=None, max_features='auto', max_leaf_nodes=None,\n            min_impurity_decrease=0.0, min_impurity_split=None,\n            min_samples_leaf=1, min_samples_split=2,\n            min_weight_fraction_leaf=0.0, n_estimators=400, n_jobs=1,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\nrandom_forest.fit(train_X, train_Y)\nY_pred_rf = random_forest.predict(test_X)\nrandom_forest.score(train_X,train_Y)\nacc_random_forest = round(random_forest.score(train_X, train_Y) * 100, 2)\n\nprint(\"Important features\")\npd.Series(random_forest.feature_importances_,train_X.columns).sort_values(ascending=True).plot.barh(width=0.8)\nprint('__'*30)\nprint(acc_random_forest)","e2aa1234":"submission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": Y_pred_rf})","07ab0b0c":"lets understand all features in this \n\nsurvival Survival 0 = No, 1 = Yes\n\npclass Ticket class 1 = 1st, 2 = 2nd, 3 = 3rd\n\nsex Male or Female\n\nAge Age in years\n\nsibsp # of siblings \/ spouses aboard the Titanic\n\nparch # of parents \/ children aboard the Titanic\n\nticket Ticket number\n\nfare Passenger fare\n\ncabin Cabin number\n\nembarked Port of Embarkation C = Cherbourg, Q = Queenstown, S = Southampton\n\nVariable Notes pclass: A proxy for socio-economic status (SES) 1st = Upper 2nd = Middle 3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\n\nSibling = brother, sister, stepbrother, stepsister\n\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\n\nParent = mother, father\n\nChild = daughter, son, stepdaughter, stepson","3ee44157":"lets  handel the missing missing values ","2903f941":"we are with the same .. \n\nlets check with svm","fd59d70c":"it seem good but we need somthing more .. ok then lets use we use **Hyper-Parameters Tuning** i.e the machine learning models are like a Black-Box. There are some default parameter values for this Black-Box, which we can tune or change the learning rate of the algorithm and get a better model. This is known as Hyper-Parameter Tuning","fedad5b2":"**If you like this notebook just upvote which can keep me movitvation ...:)**","7a129da2":"**Support Vector Machines******","1c25a345":"lets see how it varys ","dc2753a3":"**LogisticRegression **","81add0ce":"The Objective of this notebook is to give an idea how is the workflow in any predictive modeling problem. How do we check features, how do we add new features and some Machine Learning Concepts. I have tried to keep the notebook as basic as possible so that even newbies can understand every phase of it.","02bec658":"for the first timr when i started data science i took this dataste just becase of it is related to **Titanic**...:)","78526961":"**Load the DataSet :**","70b7951b":"Feature analysis Feature engineering Modeling are the three main imp works we have ","9d79dc29":"as it is a classification model we have so many ways to fit it but lets chose the best one "}}