{"cell_type":{"6f450cea":"code","8d95447c":"code","35c59a9e":"code","0f11f303":"code","ed6bb25f":"code","acecd862":"code","bb133a81":"code","8b255a45":"code","ecdacbe9":"code","1daa85df":"code","48bb7e3d":"code","7b336f3d":"code","64e82af5":"code","c3595a74":"code","c62e5e4b":"code","3b3286cf":"code","8cc23139":"code","42aee997":"code","423671ae":"code","0f3adbe5":"code","7fe01da2":"code","3dd9a0ed":"markdown","884d0b81":"markdown","0b6c1cc2":"markdown","5cc75d7a":"markdown","8f5ba028":"markdown","fdb2ac63":"markdown","95f559d3":"markdown","f6ac971c":"markdown","5c4b06e2":"markdown","97139c12":"markdown","43416fee":"markdown","83831903":"markdown"},"source":{"6f450cea":"import datetime\nimport re\nimport gc\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport lightgbm as lgb\nimport xgboost as xgb\n\nfrom sklearn.model_selection import cross_validate, GridSearchCV, validation_curve,RandomizedSearchCV\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import KFold, StratifiedKFold,TimeSeriesSplit, GroupKFold\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, RobustScaler\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, \\\n    roc_auc_score, confusion_matrix, classification_report, plot_roc_curve\nfrom sklearn.model_selection import train_test_split\n","8d95447c":"train_identity= pd.read_csv(\"..\/input\/ieee-fraud-detection\/train_identity.csv\")\ntrain_transaction= pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')                           ","35c59a9e":"df_train=pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n\ndf_train.columns = [col.lower() for col in df_train.columns]\ndf_train.head(20)","0f11f303":"test_identity= pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_identity=test_identity.set_axis(['TransactionID', 'id_01', 'id_02', 'id_03', 'id_04', 'id_05', 'id_06', 'id_07', 'id_08', 'id_09', 'id_10', 'id_11', 'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo'], axis=1)\ntest_transaction= pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')\n\ndf_test=pd.merge(test_transaction, test_identity, on=\"TransactionID\", how=\"left\")\n\ndf_test.columns = [col.lower() for col in df_test.columns]\ndf_test.head(20)","ed6bb25f":"sample_sub = pd.read_csv('..\/input\/ieee-fraud-detection\/sample_submission.csv')","acecd862":"#function of missing values\ndef missing_values_table(dataframe, na_name=False):\n    na_columns = [col for col in dataframe.columns if dataframe[col].isnull().sum() > 0]\n    n_miss = dataframe[na_columns].isnull().sum().sort_values(ascending=False)\n    ratio = (dataframe[na_columns].isnull().sum() \/ dataframe.shape[0] * 100).sort_values(ascending=False)\n    missing_df = pd.concat([n_miss, np.round(ratio, 1)], axis=1, keys=['n_miss', 'ratio'])\n    print(missing_df , end=\"\\n\")\n    if na_name:\n        return na_columns\n\n#function of one hot encoding\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first)\n    return dataframe    \n","bb133a81":"#deal with missing values\nmissing_values_table(df_train, na_name=False)\n","8b255a45":"#Reducing Vxx Columns\n#Determining which columns are related by the number of NANs present\n#Finding groups of Vs with similar NAN structure\n\nnans_df= df_train.isna()\nnans_group={}\ni_cols=[\"v\" + str(i) for i in range (1,340)]\nfor col in df_train.columns:\n    cur_group=nans_df[col].sum()\n    try:\n        nans_group[cur_group].append(col)\n    except:\n        nans_group[cur_group]=[col]\ndel nans_df; x=gc.collect()\n\nfor k, v in nans_group.items():\n    print(\"##NAN count=\", k)\n    print(v)","ecdacbe9":"#Finding subsets within the groups that are highly correlated\n##NAN count= 1269\n#['d1', 'v281', 'v282', 'v283', 'v288', 'v289', 'v296', 'v300', 'v301', 'v313', 'v314', 'v315']\n#Replacing all groups with one column from each subset.\n\nvs=nans_group[1269]\nvtitle=\"v281-v315, d1 \"\n\ndef make_corr(vs, vtitle=\"\"):\n    cols= [\"transactiondt\"] + vs\n    plt.figure(figsize=(15,15))\n    sns.heatmap(df_train[cols].corr(), cmap=\"RdBu_r\", annot=True, center=0.0)\n    if vtitle!=\"\" : plt.title(vtitle, fontsize=14)\n    else: plt.title(vs[0]+\"\"+vs[-1], fontsize= 14)\n    plt.show()\nmake_corr(vs,vtitle)\n\n","1daa85df":"grps = [[281],[282,283],[288,289],[296],[300,301],[313,314,315]]\n\ndef reduce_group(grps, c=\"v\"):\n    use=[]\n    for g in grps:\n        mx=0; vx=g[0]\n        for gg in g:\n            n=df_train[c+ str(gg)].nunique()\n            if n> mx:\n                mx=n\n                vx=gg\n        use.append(vx)\n    print(\"Use these\", use)\n    \nreduce_group(grps)\n\n","48bb7e3d":"#Vcols after reducing\n\nV=  [1, 2, 5, 6, 8, 11]\nV+= [13, 14, 16, 20, 24, 26, 27, 30]\nV+= [36, 37, 40, 41, 45, 47, 49]\nV+= [54, 56, 60, 62, 65, 67, 68, 70]\nV+= [76, 77, 80, 82, 86, 88, 89, 91]\nV+= [96, 98, 99, 104, 107, 114, 111, 115, 117, 120, 121, 123, 124, 127, 129, 130, 136]\nV+= [138, 140, 142, 147, 156, 162]\nV+= [165, 160, 166]\nV+= [178, 176, 173, 182, 190, 203, 205, 207, 215]\nV+= [169, 171, 175, 180, 185, 188, 198, 210, 209]\nV+= [218, 223, 224, 226, 228, 229, 235, 240, 258, 246, 253, 252, 260, 261, 264, 266, 267, 274, 277]\nV+= [220, 222, 234, 238, 250, 271]\nV+= [294, 284, 285, 286, 291, 297, 303, 305, 307, 309, 310, 320]\nV+= [332, 325, 335, 338]\nV+= [281, 283, 289, 296, 300, 314]\n\nprint(\"reduced set has\", len (V), \"columns\")","7b336f3d":"#New df_train and df_test\nv_cols=[\"v\" + str(i) for i in range (1,340)]\nnew_v_cols = ['v'+str(x) for x  in V]\ndroped_v_cols = [col for col in v_cols if col not in new_v_cols]\ndf_train.drop(df_train[droped_v_cols], inplace=True, axis=1)\ndf_test.drop(df_test[droped_v_cols], inplace=True, axis=1)\n","64e82af5":"null_cols= df_train[['id_01',\"id_03\",\"id_04\",'id_07','id_08',\"id_09\",\"id_10\",'id_21', 'id_22', 'id_23',\n                     'id_24', 'id_25', 'id_26', 'id_27','id_33', 'id_13',\"id_34\",\n                     'id_14','id_17', 'id_18','id_19', 'id_20', 'id_32',\"dist2\",\"d7\", \"d13\", \"d14\",\"d12\"]]\n\ndf_train.drop(null_cols, axis=1, inplace=True)\ndf_test.drop(null_cols, axis=1, inplace=True)","c3595a74":"#p_emaildomain_train\n\ndf_train[\"p_emaildomain\"].unique()\ndf_train[\"p_emaildomain\"].value_counts()\n\ndf_train.loc[df_train[\"p_emaildomain\"].isin(['gmail.com', \"gmail\"]), \"p_emaildomain\"]= \"Google\"\ndf_train.loc[df_train[\"p_emaildomain\"].isin(['yahoo.com', \"yahoo.com.mx\", 'yahoo.fr', 'yahoo.de','yahoo.es','yahoo.co.uk','yahoo.co.jp']), \"p_emaildomain\"]= \"Yahoo Mail\"\ndf_train.loc[df_train[\"p_emaildomain\"].isin(['outlook.com', 'msn.com', 'live.com','live.com.mx', 'outlook.es','live.fr','hotmail.com','hotmail.es', 'hotmail.fr', 'hotmail.de', 'hotmail.co.uk' ]), \"p_emaildomain\"]= \"Microsoft\"\ndf_train.loc[df_train[\"p_emaildomain\"].isin(['aol.com' ]), \"p_emaildomain\"]= \"Aol\"\ndf_train.loc[df_train[\"p_emaildomain\"].isin(['anonymous.com' ]), \"p_emaildomain\"]= \"Anonymous\"\ndf_train.loc[df_train[\"p_emaildomain\"].isin(df_train[\"p_emaildomain\"].value_counts()[df_train[\"p_emaildomain\"].value_counts()<=8000].index), \"p_emaildomain\"]= \"Others\"\n#others belirlemek i\u00e7in girdi\u011fimiz de\u011fer ald\u0131\u011f\u0131m\u0131z sample a g\u00f6re de\u011fi\u015fiyor\ndf_train[\"p_emaildomain\"].fillna(\"NoInf\", inplace=True)\ndf_train[\"p_emaildomain\"].value_counts()\np_email_domain_ratio = (df_train[\"p_emaildomain\"].value_counts() \/ df_train.shape[0] * 100).sort_values(ascending=False)\n\n\n\n#p_emaildomain_test\n\ndf_test[\"p_emaildomain\"].unique()\ndf_test[\"p_emaildomain\"].value_counts()\n\ndf_test.loc[df_test[\"p_emaildomain\"].isin(['gmail.com', \"gmail\"]), \"p_emaildomain\"]= \"Google\"\ndf_test.loc[df_test[\"p_emaildomain\"].isin(['yahoo.com', \"yahoo.com.mx\", 'yahoo.fr', 'yahoo.de','yahoo.es','yahoo.co.uk','yahoo.co.jp']), \"p_emaildomain\"]= \"Yahoo Mail\"\ndf_test.loc[df_test[\"p_emaildomain\"].isin(['outlook.com', 'msn.com', 'live.com','live.com.mx', 'outlook.es','live.fr','hotmail.com','hotmail.es', 'hotmail.fr', 'hotmail.de', 'hotmail.co.uk' ]), \"p_emaildomain\"]= \"Microsoft\"\ndf_test.loc[df_test[\"p_emaildomain\"].isin(['aol.com' ]), \"p_emaildomain\"]= \"Aol\"\ndf_test.loc[df_test[\"p_emaildomain\"].isin(['anonymous.com' ]), \"p_emaildomain\"]= \"Anonymous\"\ndf_test.loc[df_test[\"p_emaildomain\"].isin(df_test[\"p_emaildomain\"].value_counts()[df_test[\"p_emaildomain\"].value_counts()<=21000].index), \"p_emaildomain\"]= \"Others\"\n#others belirlemek i\u00e7in girdi\u011fimiz de\u011fer ald\u0131\u011f\u0131m\u0131z sample a g\u00f6re de\u011fi\u015fiyor\ndf_test[\"p_emaildomain\"].fillna(\"NoInf\", inplace=True)\np_email_domain_ratio = (df_test[\"p_emaildomain\"].value_counts() \/ df_test.shape[0] * 100).sort_values(ascending=False)\n\n#r_emaildomain_train\n\ndf_train[\"r_emaildomain\"].unique()\ndf_train[\"r_emaildomain\"].value_counts()\n\ndf_train.loc[df_train[\"r_emaildomain\"].isin(['gmail.com', \"gmail\"]), \"r_emaildomain\"]= \"Google\"\ndf_train.loc[df_train[\"r_emaildomain\"].isin(['yahoo.com', \"yahoo.com.mx\", 'yahoo.fr', 'yahoo.de','yahoo.es','yahoo.co.uk','yahoo.co.jp']), \"r_emaildomain\"]= \"Yahoo Mail\"\ndf_train.loc[df_train[\"r_emaildomain\"].isin(['outlook.com', 'msn.com', 'live.com','live.com.mx', 'outlook.es','live.fr','hotmail.com','hotmail.es', 'hotmail.fr', 'hotmail.de', 'hotmail.co.uk' ]), \"r_emaildomain\"]= \"Microsoft\"\ndf_train.loc[df_train[\"r_emaildomain\"].isin(['aol.com']), \"r_emaildomain\"]= \"Aol\"\ndf_train.loc[df_train[\"r_emaildomain\"].isin(['anonymous.com']), \"r_emaildomain\"]= \"Anonymous\"\ndf_train.loc[df_train[\"r_emaildomain\"].isin(df_train[\"r_emaildomain\"].value_counts()[df_train[\"r_emaildomain\"].value_counts()<=2000].index), \"r_emaildomain\"]= \"Others\"\ndf_train[\"r_emaildomain\"].fillna(\"NoInf\", inplace=True)\nr_email_domain_ratio = (df_train[\"r_emaildomain\"].value_counts() \/ df_train.shape[0] * 100).sort_values(ascending=False)\n\n\n#r_emaildomain_test\n\ndf_test[\"r_emaildomain\"].unique()\ndf_test[\"r_emaildomain\"].value_counts()\n\ndf_test.loc[df_test[\"r_emaildomain\"].isin(['gmail.com', \"gmail\"]), \"r_emaildomain\"]= \"Google\"\ndf_test.loc[df_test[\"r_emaildomain\"].isin(['yahoo.com', \"yahoo.com.mx\", 'yahoo.fr', 'yahoo.de','yahoo.es','yahoo.co.uk','yahoo.co.jp']), \"r_emaildomain\"]= \"Yahoo Mail\"\ndf_test.loc[df_test[\"r_emaildomain\"].isin(['outlook.com', 'msn.com', 'live.com','live.com.mx', 'outlook.es','live.fr','hotmail.com','hotmail.es', 'hotmail.fr', 'hotmail.de', 'hotmail.co.uk' ]), \"r_emaildomain\"]= \"Microsoft\"\ndf_test.loc[df_test[\"r_emaildomain\"].isin(['aol.com']), \"r_emaildomain\"]= \"Aol\"\ndf_test.loc[df_test[\"r_emaildomain\"].isin(['anonymous.com']), \"r_emaildomain\"]= \"Anonymous\"\ndf_test.loc[df_test[\"r_emaildomain\"].isin(df_test[\"r_emaildomain\"].value_counts()[df_test[\"r_emaildomain\"].value_counts()<=3000].index), \"r_emaildomain\"]= \"Others\"\ndf_test[\"r_emaildomain\"].fillna(\"NoInf\", inplace=True)\nr_email_domain_ratio = (df_test[\"r_emaildomain\"].value_counts() \/ df_test.shape[0] * 100).sort_values(ascending=False)\n\n#id_30 train\ndf_train[\"id_30\"].unique()\n\ndf_train.loc[df_train[\"id_30\"].str.contains(\"Windows\", na=False), \"id_30\"] = \"Windows\"\ndf_train.loc[df_train[\"id_30\"].str.contains(\"iOS\", na=False), \"id_30\"] = \"iOS\"\ndf_train.loc[df_train[\"id_30\"].str.contains(\"Mac OS\", na=False), \"id_30\"] = \"Mac\"\ndf_train.loc[df_train[\"id_30\"].str.contains(\"Android\", na=False), \"id_30\"] = \"Android\"\ndf_train[\"id_30\"].fillna(\"NAN\", inplace=True)\n\n\n#id_30 test\ndf_test[\"id_30\"].unique()\n\ndf_test.loc[df_test[\"id_30\"].str.contains(\"Windows\", na=False), \"id_30\"] = \"Windows\"\ndf_test.loc[df_test[\"id_30\"].str.contains(\"iOS\", na=False), \"id_30\"] = \"iOS\"\ndf_test.loc[df_test[\"id_30\"].str.contains(\"Mac OS\", na=False), \"id_30\"] = \"Mac\"\ndf_test.loc[df_test[\"id_30\"].str.contains(\"Android\", na=False), \"id_30\"] = \"Android\"\ndf_test[\"id_30\"].fillna(\"NAN\", inplace=True)\n\n\n#id_31 train\ndf_train[\"id_31\"].unique()\ndf_train[\"id_31\"].value_counts()\n\ndf_train.loc[df_train[\"id_31\"].str.contains(\"chrome\", na=False), \"id_31\"] = \"Chrome\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"firefox\", na=False), \"id_31\"] = \"Firefox\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"safari\", na=False), \"id_31\"] = \"Safari\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"edge\", na=False), \"id_31\"] = \"Edge\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"ie\", na=False), \"id_31\"] = \"IE\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"samsung\", na=False), \"id_31\"] = \"Samsung\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"opera\", na=False), \"id_31\"] = \"Opera\"\ndf_train.loc[df_train[\"id_31\"].str.contains(\"google\", na=False), \"id_31\"] = \"Google Search Application\"\ndf_train[\"id_31\"].fillna(\"NAN\", inplace=True)\ndf_train.loc[df_train[\"id_31\"].isin(df_train[\"id_31\"].value_counts()[df_train[\"id_31\"].value_counts()< 500].index), \"id_31\"]= \"Others\"\n\n\n#id_31 test\n\ndf_test[\"id_31\"].unique()\ndf_test[\"id_31\"].value_counts()\n\ndf_test.loc[df_test[\"id_31\"].str.contains(\"chrome\", na=False), \"id_31\"] = \"Chrome\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"firefox\", na=False), \"id_31\"] = \"Firefox\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"safari\", na=False), \"id_31\"] = \"Safari\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"edge\", na=False), \"id_31\"] = \"Edge\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"ie\", na=False), \"id_31\"] = \"IE\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"samsung\", na=False), \"id_31\"] = \"Samsung\"\ndf_test.loc[df_test[\"id_31\"].str.contains(\"opera\", na=False), \"id_31\"] = \"Opera\"\ndf_test[\"id_31\"].fillna(\"NAN\", inplace=True)\ndf_test.loc[df_test[\"id_31\"].isin(df_test[\"id_31\"].value_counts()[df_test[\"id_31\"].value_counts()< 2001].index), \"id_31\"]= \"Others\"\n\n\n#deviceinfo train\n\ndf_train[\"deviceinfo\"].unique()\ndf_train.loc[df_train['deviceinfo'].str.contains('SM', na=False), 'deviceinfo'] = 'Samsung'\ndf_train.loc[df_train['deviceinfo'].str.contains('Moto', na=False), 'deviceinfo'] = 'Motorola'\ndf_train.loc[df_train['deviceinfo'].str.contains('moto', na=False), 'deviceinfo'] = 'Motorola'\ndf_train.loc[df_train['deviceinfo'].str.contains('HUAWEI', na=False), 'deviceinfo'] = 'Huawei'\ndf_train.loc[df_train['deviceinfo'].str.contains('LG', na=False), 'deviceinfo'] = 'LG'\ndf_train.loc[df_train['deviceinfo'].str.contains('GT-', na=False), 'deviceinfo'] = 'Samsung'\ndf_train.loc[df_train['deviceinfo'].str.contains('Trident', na=False), 'deviceinfo'] = 'Trident'\ndf_train.loc[df_train['deviceinfo'].str.contains('BLADE', na=False), 'deviceinfo'] = 'ZTE'\ndf_train.loc[df_train['deviceinfo'].isin(df_train['deviceinfo'].value_counts()[df_train['deviceinfo'].value_counts() < 7570].index), 'deviceinfo'] = \"Others\"\ndf_train[\"deviceinfo\"].fillna(\"NAN\", inplace=True)\n\n#deviceinfo test\n\ndf_test[\"deviceinfo\"].unique()\ndf_test.loc[df_test['deviceinfo'].str.contains('SM', na=False), 'deviceinfo'] = 'Samsung'\ndf_test.loc[df_test['deviceinfo'].str.contains('Moto', na=False), 'deviceinfo'] = 'Motorola'\ndf_test.loc[df_test['deviceinfo'].str.contains('moto', na=False), 'deviceinfo'] = 'Motorola'\ndf_test.loc[df_test['deviceinfo'].str.contains('HUAWEI', na=False), 'deviceinfo'] = 'Huawei'\ndf_test.loc[df_test['deviceinfo'].str.contains('LG', na=False), 'deviceinfo'] = 'LG'\ndf_test.loc[df_test['deviceinfo'].str.contains('GT-', na=False), 'deviceinfo'] = 'Samsung'\ndf_test.loc[df_test['deviceinfo'].str.contains('Trident', na=False), 'deviceinfo'] = 'Trident'\ndf_test.loc[df_test['deviceinfo'].str.contains('BLADE', na=False), 'deviceinfo'] = 'ZTE'\ndf_test.loc[df_test['deviceinfo'].isin(df_test['deviceinfo'].value_counts()[df_test['deviceinfo'].value_counts() < 6000].index), 'deviceinfo'] = \"Others\"\ndf_test[\"deviceinfo\"].fillna(\"NAN\", inplace=True)\n\n","c62e5e4b":"START_DATE = '2017-12-01'\nstartdate = datetime.datetime.strptime(START_DATE, \"%Y-%m-%d\")\n\n#train\ndf_train[\"Date\"] = df_train['transactiondt'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_train['New_months'] = df_train['Date'].dt.month\ndf_train['New_weekdays'] = df_train['Date'].dt.dayofweek\ndf_train['New_hours'] = df_train['Date'].dt.hour\ndf_train['New_days'] = df_train['Date'].dt.day\ndf_train['New_is_month_end'] = df_train[\"Date\"].dt.is_month_end.astype(int)\ndf_train[\"New_is_wknd\"] = df_train[\"Date\"].dt.weekday \/\/ 4\ndf_train['New_is_month_start'] = df_train[\"Date\"].dt.is_month_start.astype(int)\ndf_train['New_day_of_year'] = df_train[\"Date\"].dt.dayofyear\n\n#test\ndf_test[\"Date\"] = df_test['transactiondt'].apply(lambda x: (startdate + datetime.timedelta(seconds=x)))\n\ndf_test['New_months'] = df_test['Date'].dt.month\ndf_test['New_weekdays'] = df_test['Date'].dt.dayofweek\ndf_test['New_hours'] = df_test['Date'].dt.hour\ndf_test['New_days'] = df_test['Date'].dt.day\ndf_test['New_is_month_end'] = df_test[\"Date\"].dt.is_month_end.astype(int)\ndf_test[\"New_is_wknd\"] = df_test[\"Date\"].dt.weekday \/\/ 4\ndf_test['New_is_month_start'] = df_test[\"Date\"].dt.is_month_start.astype(int)\ndf_test['New_day_of_year'] = df_test[\"Date\"].dt.dayofyear\n\ndf_train.drop(\"Date\", axis=1, inplace=True)\ndf_test.drop(\"Date\", axis=1, inplace=True)\n","3b3286cf":"cat_cols_train= df_train[['productcd',  'card4', 'card6','p_emaildomain', 'r_emaildomain',\n              'devicetype', 'deviceinfo','id_12','id_15','id_16','id_28',\n              'id_29',\"id_30\", 'id_31', 'id_35', 'id_36', 'id_37', 'id_38',\"m4\",'m1', 'm2', 'm3'\n             , 'm5', 'm6', 'm7', 'm8', 'm9',\"New_months\", \"New_hours\"]]\n\ncat_cols_test= df_test[['productcd',  'card4', 'card6','p_emaildomain', 'r_emaildomain',\n              'devicetype', 'deviceinfo','id_12','id_15','id_16','id_28',\n              'id_29',\"id_30\", 'id_31', 'id_35', 'id_36', 'id_37', 'id_38',\"m4\",'m1', 'm2', 'm3'\n            , 'm5', 'm6', 'm7', 'm8', 'm9']]\n","8cc23139":"# ONE-HOT ENCODING TRAIN\n\nohe_cols_train=cat_cols_train.columns\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first, dtype=\"float\")\n    return dataframe\n\ndf_train=one_hot_encoder(df_train, ohe_cols_train, drop_first=True)\n\n#you should rename columns name after encoding since columns include characteristic name like :\/ \n\ncolumns= [df_train.columns]\ndf_train= df_train.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\n# ONE-HOT ENCODING TEST\n\nohe_cols_test=cat_cols_test.columns\n\ndef one_hot_encoder(dataframe, categorical_cols, drop_first=False):\n    dataframe = pd.get_dummies(dataframe, columns=categorical_cols, drop_first=drop_first, dtype=\"float\")\n    return dataframe\n\ndf_test=one_hot_encoder(df_test, ohe_cols_test, drop_first=True)\n\ncolumns= [df_test.columns]\ndf_test= df_test.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n\ndf_train.drop(\"card6_debitorcredit\", axis=1, inplace=True)\n\n\n","42aee997":"#Train- Test Split#\ntrain_index=df_train.index[:450000]\ntest_index=df_train.index[450000:]\n\ntrain_cols=df_train.columns\nX_train=df_train.loc[train_index,train_cols]\nX_train.drop([\"transactionid\", \"isfraud\", \"transactiondt\"], axis=1, inplace=True)\n\nX_test=df_train.loc[test_index,train_cols]\nX_test.drop([\"transactionid\", \"isfraud\", \"transactiondt\"], axis=1, inplace=True)\n\ny_train1 = df_train[\"isfraud\"]\ny_train=y_train1[train_index]\ny_test=y_train1[test_index]\n\n","423671ae":"model =xgb.XGBClassifier(\n    n_estimators=500,\n    max_depth=9,\n    learning_rate=0.05,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    tree_method='gpu_hist'\n     )\nmodel.fit(X_train, y_train, eval_metric=[\"error\", \"logloss\"], verbose=True)\n\ny_pred = model.predict(X_test)\ntrain_y= model.predict(X_train)","0f3adbe5":"print('Train Accuracy score:',accuracy_score(y_train,train_y))\nprint('Train F1 score:',f1_score(y_train,train_y))\nprint('Train Precision score:',precision_score(y_train,train_y))\nprint('Train Recall score:',recall_score(y_train,train_y)) \n\nprint('Test Accuracy score:',accuracy_score(y_test,y_pred))\nprint('Test F1 score:',f1_score(y_test,y_pred))\nprint('Test Precision score:',precision_score(y_test,y_pred))\nprint('Test Recall score:',recall_score(y_test,y_pred)) \n","7fe01da2":"def plot_confusion_matrix(y_test, y_pred):\n    acc = round(accuracy_score(y_test, y_pred), 2)\n    cm = confusion_matrix(y_test, y_pred)\n    sns.heatmap(cm, annot=True, fmt=\".0f\")\n    plt.xlabel('y_pred')\n    plt.ylabel('y_test')\n    plt.title('Accuracy Score: {0}'.format(acc), size=10)\n    plt.show()\n\nplot_confusion_matrix(y_test, y_pred)","3dd9a0ed":"After one hot encoding rename cols name and control the shape of train and test dataframes.","884d0b81":"**Feature Extraction**\nTaking the start date \u20182017-12-01\u2019, constructed time variables. In discussions tab you should read an excellent solutions.","0b6c1cc2":"Define categorical cols for both train and test dataframes. Since the types of features are not object and have many categories, we define cat_cols manuel. Also you can add other time variables in cat_cols.","5cc75d7a":"**Re-grouping of categorical features**","8f5ba028":"**Data Description**\n\n**Transaction Table**\n* TransactionDT: timedelta from a given reference datetime (not an actual timestamp)\n* TransactionAMT: transaction payment amount in USD\n* ProductCD: product code, the product for each transaction\n* card1 - card6: payment card information, such as card type, card category, issue bank, country, etc.\n* addr: address\n* dist: distance\n* P_ and (R__) emaildomain: purchaser and recipient email domain\n* C1-C14: counting, such as how many addresses are found to be associated with the payment card, etc. The actual meaning is masked.\n* D1-D15: timedelta, such as days between previous transaction, etc.\n* M1-M9: match, such as names on card and address, etc.\n* Vxxx: Vesta engineered rich features, including ranking, counting, and other entity relations.\n* Categorical Features:\nProductCD,\ncard1 - card6,\naddr1, addr2,\nP_emaildomain,\nR_emaildomain,\nM1 - M9\n\n**Identity Table**\nVariables in this table are identity information \u2013 network connection information (IP, ISP, Proxy, etc) and digital signature (UA\/browser\/os\/version, etc) associated with transactions.\nThey're collected by Vesta\u2019s fraud protection system and digital security partners.\n(The field names are masked and pairwise dictionary will not be provided for privacy protection and contract agreement)\n\n* Categorical Features:\nDeviceType,\nDeviceInfo,\nid_12 - id_38","fdb2ac63":"**Drop missing features which have 85% missing value ratio**","95f559d3":" **IEEE-CIS Fraud Detection**\n\nThe data comes from Vesta's real-world e-commerce transactions and contains a wide range of features from device type to product features.\n\n**What is fraud detection?**:  Fraud detection\u00a0protects person information, assets, accounts and transactions through the real-time, near-real-time analysis of activities by users and other defined entities. It uses background server-based processes that examine users\u2019 and other defined entities\u2019 access and behavior patterns, and typically compares this information to a profile of what\u2019s expected.\u00a0\n","f6ac971c":"**Importing Libraries**","5c4b06e2":"Vxx features and id_xx features have high nulity ratio.Since we do not know what these features exactly present, we reduce of Vxx cols. \nreference: https:\/\/www.kaggle.com\/cdeotte\/eda-for-columns-v-and-id\nAlso PCA is usefull method for Vxx features.","97139c12":"**Importing Train and Test datasets**","43416fee":"**Data Preprocessing**","83831903":"**There is three problems related to datatsets.***\n1. Columns name are masked \n2. Imbalanced dataset\n3. Time series dataset"}}