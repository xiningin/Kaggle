{"cell_type":{"a23ce0e2":"code","4fae7e10":"code","8789e54d":"code","46042ee6":"code","a8fd8d35":"code","d7f5dd8b":"code","303f060d":"code","c3bb60d4":"code","afbe017d":"code","abdfc6d9":"code","de2faad6":"code","b6c04f29":"code","057159a6":"code","ebc47db0":"code","a568bc7b":"code","553bf128":"code","15ff6e87":"code","61c3d3b8":"code","23a02636":"markdown","6ca36fd1":"markdown","769dd251":"markdown","94ff29d2":"markdown","0d91479a":"markdown","c5f31a7b":"markdown","d00e7652":"markdown","468a447c":"markdown","8bff0171":"markdown"},"source":{"a23ce0e2":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nsns.set_style('whitegrid')\nimport os\nimport time\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nimport sklearn","4fae7e10":"print(os.listdir(\"..\/input\"))","8789e54d":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\nprint('Rows: ',train_df.shape[0],'Columns: ',train_df.shape[1])\ntrain_df.info()","46042ee6":"train_df.head()","a8fd8d35":"train_df['target'].value_counts()","d7f5dd8b":"sns.countplot(train_df['target'])\nsns.set_style('whitegrid')","303f060d":"X_test = test_df.drop('ID_code',axis=1)","c3bb60d4":"X = train_df.drop(['ID_code','target'],axis=1)\ny = train_df['target']","afbe017d":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents\n             , columns = ['principal component 1', 'principal component 2'])","abdfc6d9":"pca.explained_variance_ratio_","de2faad6":"finalDf = pd.concat([principalDf, train_df[['target']]], axis = 1)","b6c04f29":"import matplotlib.pyplot as plt\nfig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1.0, 0.0]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 1']\n               , finalDf.loc[indicesToKeep, 'principal component 2']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","057159a6":"fig = plt.figure(figsize = (8,8))\nax = fig.add_subplot(1,1,1) \nax.set_xlabel('Principal Component 1', fontsize = 15)\nax.set_ylabel('Principal Component 2', fontsize = 15)\nax.set_title('2 component PCA', fontsize = 20)\ntargets = [1.0]\ncolors = ['r']\nfor target, color in zip(targets,colors):\n    indicesToKeep = finalDf['target'] == target\n    ax.scatter(finalDf.loc[indicesToKeep, 'principal component 2']\n               , finalDf.loc[indicesToKeep, 'principal component 1']\n               , c = color\n               , s = 50)\nax.legend(targets)\nax.grid()","ebc47db0":"#pca = PCA(n_components=2)\npca = PCA(.95)\nP_X = pca.fit_transform(X)\nP_X_TEST = pca.transform(X_test)\n","a568bc7b":"pca.n_components_","553bf128":"pca.explained_variance_ratio_","15ff6e87":"from catboost import CatBoostClassifier,Pool\ntrain_pool = Pool(P_X,y)\nm = CatBoostClassifier(iterations=3000,eval_metric=\"AUC\", objective=\"Logloss\",learning_rate=0.003)\nm.fit(P_X,y,silent=True)\ny_pred1 = m.predict_proba(P_X_TEST)[:,0]\n","61c3d3b8":"sub1 = pd.DataFrame({\"ID_code\": test_df.ID_code.values})\nsub1[\"target\"] = y_pred1\nsub1.to_csv(\"submission1.csv\", index=False)","23a02636":"# Please upvote, if you find this kernel interesting","6ca36fd1":"Another way to use PCA. \nBy specifying PCA(.95) we mean that we need 95% of variance to be covered. So out might be multiple principal, instead just 2.","769dd251":"\"explained_variance_ratio_\" gives you percentage of variance covered by the first 2 principal component","94ff29d2":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nP_X_T=scaler.fit_transform(P_X)\nP_X_TEST_T=scaler.transform(P_X_TEST)","0d91479a":"Shows number principal components required to cover 95% of variance","c5f31a7b":"# Please upvote, if you find this kernel interesting","d00e7652":"Running a model by taking the output of PCA(111 principal components).","468a447c":"n_components=2 (Final output will be converted to 2 Dimension)","8bff0171":"# PCA\n\n**Dimensionality reduction:** If we want to visualize 2 variables(2 dimensional) we can do it using any type of plot, 1 variable at x-axis and another in y-axis. But, what if want to visualize 200 variables (200 dimension)? we can use dimensionality reduction techniques like PCA,t-SNE, UMAP for the same. This techniques intelligently summarizes\/group information related to multi dimension to the required low dimension. Unfortunately this techniques didn't help much in this competetion."}}