{"cell_type":{"a0c7fd6a":"code","cba474e6":"code","453706d6":"code","194d8393":"code","894f00a0":"code","965a3307":"code","66e1308e":"code","529791a1":"code","65255f99":"code","7a93d94d":"code","a22487d4":"code","eaa73315":"code","f2a96ab7":"markdown"},"source":{"a0c7fd6a":"import numpy as np\nimport pandas as pd\nimport os\nfrom pathlib import Path\nimport joblib\nimport random\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm import tqdm\nfrom warnings import simplefilter\nsimplefilter('ignore')\n","cba474e6":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.core.lightning import LightningModule\nfrom transformers import (PreTrainedModel, RobertaModel, RobertaTokenizerFast, RobertaConfig,\n                          get_constant_schedule_with_warmup, AdamW)","453706d6":"model_name = 'roberta_v7'\ndata_dir = Path('..\/input\/commonlitreadabilityprize')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\npretrained_path = '..\/input\/roberta-base\/'\nbuild_dir = Path('.\/build')\noutput_dir = build_dir \/ model_name\ntrn_encoded_file = output_dir \/ 'trn.enc.joblib'\ntokenizer_file = output_dir \/ 'tokenizer.joblib'\nval_predict_file = output_dir \/ f'{model_name}.val.txt'\nsubmission_file = 'submission.csv'\n\n# Config\nid_col = 'id'\ntarget_col = 'target'\ntext_col = 'excerpt'\nmax_len = 256\nn_fold = 5\nn_est = 20\nn_stop = 2\nbatch_size = 16\nseed = 42","194d8393":"output_dir.mkdir(parents=True, exist_ok=True)\nseed_everything(seed)","894f00a0":"if torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"GPU is available\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"GPU not available, CPU used\")","965a3307":"trn = pd.read_csv(train_file, index_col=id_col)\ntst = pd.read_csv(test_file, index_col=id_col)\ny = trn[target_col].values\nprint(trn.shape, y.shape)\ntrn.head()","66e1308e":"#Tokenization Using RoBERTa\ntokenizer = RobertaTokenizerFast.from_pretrained(pretrained_path, do_lower_case=True)\nmodel_config = RobertaConfig.from_pretrained(pretrained_path)\nmodel_config.output_hidden_states = True","529791a1":"class Data(Dataset):\n    def __init__(self, df):\n        super().__init__()\n        self.df = df\n        self.labeled = target_col in df\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        texts = self.df[text_col][idx]\n        token = tokenizer(texts, max_length=max_len, truncation=True, padding='max_length', \n                          return_tensors='pt', add_special_tokens=True)\n        ids = torch.tensor(token['input_ids'], dtype=torch.long).squeeze()\n        mask = torch.tensor(token['attention_mask'], dtype=torch.long).squeeze()\n        if self.labeled:\n            target = torch.tensor(self.df[target_col][idx], dtype=torch.float)\n        \n        return (ids, mask, target) if self.labeled else (ids, mask)","65255f99":"#Model Training with Cross-Validation\nclass ReadabilityModel(LightningModule):\n    \n    def __init__(self, conf):\n        super().__init__()\n        self.config = conf\n        self.model = RobertaModel.from_pretrained(pretrained_path, config=self.config)\n        self.dropout = nn.Dropout(0.1)\n        self.num_targets = 1\n        self.clf = nn.Linear(768, self.num_targets)\n        torch.nn.init.normal_(self.clf.weight, std=0.02)\n    \n    def forward(self, inputs):\n        ids, mask = inputs\n        out = self.model(ids, attention_mask=mask)\n        out = out['hidden_states']\n        x = out[-1]\n        x = self.dropout(x)\n        x = torch.mean(x, 1, True)\n        preds = self.clf(x)\n        preds = preds.squeeze(-1).squeeze(-1)\n\n        return preds\n    \n    def training_step(self, batch, batch_idx):\n        ids, mask, y = batch\n        p = self([ids, mask])\n        loss = self.loss_fn(p, y)\n        self.log('train_loss', loss)\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        ids, mask, y = batch\n        p = self([ids, mask])\n        loss = self.loss_fn(p, y)\n        self.log('val_loss', loss)\n        \n    def configure_optimizers(self):\n        optimizer = AdamW(model.parameters(), lr=1e-5, weight_decay=0.01)\n        lr_scheduler = get_constant_schedule_with_warmup(optimizer, 100)\n        return [optimizer], [lr_scheduler]\n    \n    def loss_fn(self, p, y):\n        return torch.sqrt(nn.MSELoss()(p, y))","7a93d94d":"cv = KFold(n_splits=n_fold, shuffle=True, random_state=seed)\n\np = np.zeros_like(y, dtype=float)\np_tst = np.zeros((tst.shape[0],), dtype=float)\nfor i_cv, (i_trn, i_val) in enumerate(cv.split(trn), 1):\n    model = ReadabilityModel(model_config)\n    trn_loader = DataLoader(Data(trn.iloc[i_trn]), shuffle=True, batch_size=batch_size)\n    val_loader = DataLoader(Data(trn.iloc[i_val]), shuffle=False, batch_size=batch_size * 8)\n\n    trainer = Trainer(max_epochs=n_est, gpus=-1, logger=False, checkpoint_callback=False,\n                      callbacks=[EarlyStopping(monitor='val_loss', mode='min', patience=n_stop)])\n    trainer.fit(model, trn_loader, val_loader)\n\n    val_loader = DataLoader(Data(trn.iloc[i_val].drop(target_col, axis=1)), shuffle=False, \n                            batch_size=batch_size * 8)\n    tst_loader = DataLoader(Data(tst), shuffle=False, batch_size=batch_size * 8)\n    p[i_val] = np.concatenate(trainer.predict(model, val_loader))\n    p_tst += np.concatenate(trainer.predict(model, tst_loader)) \/ n_fold\n    \n    trainer.save_checkpoint(f'{model_name}_cv{i_cv}.ckpt')","a22487d4":"#Print CV RMSE and Save CV Predictions\nprint(f'CV RMSE: {mean_squared_error(y, p, squared=False):.6f}')\nnp.savetxt(val_predict_file, p, fmt='%.6f')","eaa73315":"#Submission\nsub = pd.read_csv(sample_file, index_col=id_col)\nsub[target_col] = p_tst\nsub.to_csv(submission_file)\nsub.head()","f2a96ab7":"<h1><Center>CommonLit Readability Prize<\/center><\/h1>\n\nRefer to my previous [notebook](https:\/\/www.kaggle.com\/harshsharma511\/one-stop-understanding-eda-bert) for Competition Understanding, EDA and Baseline BERT Model. "}}