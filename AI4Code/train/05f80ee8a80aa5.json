{"cell_type":{"11082ba8":"code","9e4a9147":"code","fe4163ba":"code","2941702e":"code","b7cea3e9":"code","f3772f28":"code","24293c64":"code","4e13066e":"code","69755c3a":"code","9570efe0":"code","eb93ebfd":"code","1035d34b":"code","1543cade":"code","f4a3a9ef":"code","e72b2573":"code","04475bdc":"code","2196506b":"code","0fd5009f":"code","ed76309b":"code","6ba6de36":"code","b9ec39e0":"code","4761c4dd":"code","b326db54":"code","e7ac0e5f":"code","0639efed":"code","30f8a9f0":"code","47af68a7":"code","cdc122b6":"code","18d6351e":"code","9f380f66":"code","829d43e7":"code","6db5dfde":"code","ec66f8b5":"code","bba6af56":"code","329deb4d":"code","f994cc1b":"code","2825b231":"code","a9fad3ba":"code","e9186285":"code","c31beaf2":"code","e44744f5":"code","41da5c95":"code","364ef89c":"code","57fa9903":"code","fc4706e5":"code","73abf552":"code","28e3eed7":"code","9c50aebd":"code","c31e748b":"code","216619f8":"code","75f771ac":"code","d3b64c51":"code","c99d84d1":"code","16d3eeed":"code","696c8451":"code","8e345e43":"code","67fa10a8":"code","08771d3e":"code","5913c677":"code","029b369f":"code","2b4e999a":"code","dcec6943":"code","240cd262":"code","b78fdc01":"code","4fdbe0c8":"code","34c99b79":"code","88f723a0":"code","09bafd83":"code","0d2332d7":"markdown","cd6d90a8":"markdown","9a98d43c":"markdown","2616f701":"markdown","b5f5e21c":"markdown","0e61a27d":"markdown","54546b04":"markdown","60ffd588":"markdown","23091ec8":"markdown","b9a405bb":"markdown","2593cb1a":"markdown","5d561030":"markdown","e1fa6aef":"markdown","bd22617c":"markdown","3fb83387":"markdown","72b2b2d0":"markdown","2db1dbd0":"markdown","2c5c6365":"markdown","251ab04b":"markdown","c33adee4":"markdown","bd5859af":"markdown","613d328e":"markdown","58632822":"markdown","d71efc44":"markdown","4ec45165":"markdown","01bf96eb":"markdown","d18e5fe9":"markdown"},"source":{"11082ba8":"#installation of libraries\nimport numpy as np\nimport pandas as pd \nimport statsmodels.api as sm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import scale, StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.metrics import confusion_matrix, accuracy_score, mean_squared_error, r2_score, roc_auc_score, roc_curve, classification_report\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold","9e4a9147":"#any warnings that do not significantly impact the project are ignored.\nimport warnings\nwarnings.simplefilter(action = \"ignore\") ","fe4163ba":"#reading the dataset\ndf = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n#selection of the first 5 observations\ndf.head() ","2941702e":"#return a random sample of items from an axis of object\ndf.sample(3) ","b7cea3e9":"#makes random selection from dataset at the rate of written value\ndf.sample(frac = 0.01) ","f3772f28":"#size information\ndf.shape","24293c64":"#dataframe's index dtype and column dtypes, non-null values and memory usage information\ndf.info()","4e13066e":"#explanatory statistics values of the observation units corresponding to the specified percentages\ndf.describe([0.10,0.25,0.50,0.75,0.90,0.95,0.99]).T\n#transposition of the df table. This makes it easier to evaluate.","69755c3a":"#correlation between variables\ndf.corr()","9570efe0":"#get a histogram of the Glucose column for both classes\n\ncol = 'Glucose'\nplt.hist(df[df['Outcome']==0][col], 10, alpha=0.5, label='non-diabetes')\nplt.hist(df[df['Outcome']==1][col], 10, alpha=0.5, label='diabetes')\nplt.legend(loc='upper right')\nplt.xlabel(col)\nplt.ylabel('Frequency')\nplt.title('Histogram of {}'.format(col))\nplt.show()","eb93ebfd":"for col in ['BMI', 'BloodPressure']:\n    plt.hist(df[df['Outcome']==0][col], 10, alpha=0.5, label='non-diabetes')\n    plt.hist(df[df['Outcome']==1][col], 10, alpha=0.5, label='diabetes')\n    plt.legend(loc='upper right')\n    plt.xlabel(col)\n    plt.ylabel('Frequency')\n    plt.title('Histogram of {}'.format(col))\n    plt.show()","1035d34b":"def plot_corr(df,size = 9): \n    corr = df.corr() #corr = variable, where we assign the correlation matrix to a variable\n    fig, ax = plt.subplots(figsize = (size,size)) \n    #fig = the column to the right of the chart, subplots (figsize = (size, size)) = determines the size of the chart\n    ax.matshow(corr) # prints the correlation, which draws the matshow matrix directly\n    cax=ax.matshow(corr, interpolation = 'nearest') #plotting axis, code that makes the graphic like square or map\n    fig.colorbar(cax) #plotting color\n    plt.xticks(range(len(corr.columns)),corr.columns,rotation=65) \n    # draw xticks, rotation = 17 is for inclined printing of expressions written for each top column\n    plt.yticks(range(len(corr.columns)),corr.columns) #draw yticks","1543cade":"#we draw the dataframe using the function.\nplot_corr(df) ","f4a3a9ef":"#correlation matrix in seaborn library\nimport seaborn as sb\nsb.heatmap(df.corr());","e72b2573":"#this way we can see the correlations\nsb.heatmap(df.corr(),annot =True); ","04475bdc":"#proportions of classes 0 and 1 in Outcome\ndf[\"Outcome\"].value_counts()*100\/len(df)","2196506b":"#how many classes are 0 and 1\ndf.Outcome.value_counts()","0fd5009f":"#histogram of the Age variable\ndf[\"Age\"].hist(edgecolor = \"black\");","ed76309b":"#Age, Glucose and BMI means according to Outcome variable\ndf.groupby(\"Outcome\").agg({\"Age\":\"mean\",\"Glucose\":\"mean\",\"BMI\":\"mean\"})","6ba6de36":"#no missing data in dataset\ndf.isnull().sum()","b9ec39e0":"#zeros in the corresponding variables mean NA, so 0 is assigned instead of NA\ndf[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = df[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0, np.NaN)","4761c4dd":"#exclusive values\ndf.isnull().sum()","b326db54":"def median_target(var):   \n    \n    temp = df[df[var].notnull()] \n    temp = temp[[var, 'Outcome']].groupby(['Outcome'])[[var]].median().reset_index() #reset_index; solved problems in indices\n    \n    return temp\n#Non-nulls are selected from within df and assigned to a dataframe named temp, ignoring the observation units filled.","e7ac0e5f":"#median of glucose taken according to Outcome's value of 0 and 1\nmedian_target(\"Glucose\")","0639efed":"#median values of diabetes and non-diabetes were given for incomplete observations.\n\ncolumns = df.columns\n\ncolumns = columns.drop(\"Outcome\")\n\nfor col in columns:\n    \n    df.loc[(df['Outcome'] == 0 ) & (df[col].isnull()), col] = median_target(col)[col][0]\n    df.loc[(df['Outcome'] == 1 ) & (df[col].isnull()), col] = median_target(col)[col][1]\n    #select the outcome value 0 and the relevant variable blank, select the relevant variable\n#It refers to pre-comma filtering operations, it is used for column selection after comma.","30f8a9f0":"#according to BMI, some ranges were determined and categorical variables were assigned.\nNewBMI = pd.Series([\"Underweight\", \"Normal\", \"Overweight\", \"Obesity 1\", \"Obesity 2\", \"Obesity 3\"], dtype = \"category\")\n\ndf[\"NewBMI\"] = NewBMI\n\ndf.loc[df[\"BMI\"] < 18.5, \"NewBMI\"] = NewBMI[0]\n\ndf.loc[(df[\"BMI\"] > 18.5) & (df[\"BMI\"] <= 24.9), \"NewBMI\"] = NewBMI[1]\ndf.loc[(df[\"BMI\"] > 24.9) & (df[\"BMI\"] <= 29.9), \"NewBMI\"] = NewBMI[2]\ndf.loc[(df[\"BMI\"] > 29.9) & (df[\"BMI\"] <= 34.9), \"NewBMI\"] = NewBMI[3]\ndf.loc[(df[\"BMI\"] > 34.9) & (df[\"BMI\"] <= 39.9), \"NewBMI\"] = NewBMI[4]\ndf.loc[df[\"BMI\"] > 39.9 ,\"NewBMI\"] = NewBMI[5]","47af68a7":"df.head()","cdc122b6":"#categorical variable creation according to the insulin value\ndef set_insulin(row):\n    if row[\"Insulin\"] >= 16 and row[\"Insulin\"] <= 166:\n        return \"Normal\"\n    else:\n        return \"Abnormal\"     ","18d6351e":"df.head()","9f380f66":"#NewInsulinScore variable added with set_insulin\ndf[\"NewInsulinScore\"] = df.apply(set_insulin, axis=1)","829d43e7":"df.head()","6db5dfde":"#some intervals were determined according to the glucose variable and these were assigned categorical variables.\n\nNewGlucose = pd.Series([\"Low\", \"Normal\", \"Overweight\", \"Secret\", \"High\"], dtype = \"category\")\n\ndf[\"NewGlucose\"] = NewGlucose\n\ndf.loc[df[\"Glucose\"] <= 70, \"NewGlucose\"] = NewGlucose[0]\n\ndf.loc[(df[\"Glucose\"] > 70) & (df[\"Glucose\"] <= 99), \"NewGlucose\"] = NewGlucose[1]\n\ndf.loc[(df[\"Glucose\"] > 99) & (df[\"Glucose\"] <= 126), \"NewGlucose\"] = NewGlucose[2]\n\ndf.loc[df[\"Glucose\"] > 126 ,\"NewGlucose\"] = NewGlucose[3]","ec66f8b5":"df.head()","bba6af56":"#categorical variables were converted into numerical values by making One Hot Encoding transform\n#it is also protected from the Dummy variable trap\ndf = pd.get_dummies(df, columns =[\"NewBMI\",\"NewInsulinScore\", \"NewGlucose\"], drop_first = True)","329deb4d":"df.head()","f994cc1b":"#categorical variables\ncategorical_df = df[['NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret']]","2825b231":"#categorical variables deleted from df\ny = df[\"Outcome\"]\nX = df.drop([\"Outcome\",'NewBMI_Obesity 1','NewBMI_Obesity 2', 'NewBMI_Obesity 3', 'NewBMI_Overweight','NewBMI_Underweight',\n                     'NewInsulinScore_Normal','NewGlucose_Low','NewGlucose_Normal', 'NewGlucose_Overweight', 'NewGlucose_Secret'], axis = 1)\ncols = X.columns\nindex = X.index","a9fad3ba":"y.head()","e9186285":"X.head()","c31beaf2":"#by standardizing the variables in the dataset, the performance of the models is increased.\nfrom sklearn.preprocessing import RobustScaler\ntransformer = RobustScaler().fit(X)\nX = transformer.transform(X)\nX = pd.DataFrame(X, columns = cols, index = index)","e44744f5":"X.head()","41da5c95":"#combining non-categorical and categorical variables\nX = pd.concat([X, categorical_df], axis = 1)","364ef89c":"X.head()","57fa9903":"models = []\nmodels.append(('LR', LogisticRegression(random_state = 12345)))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier(random_state = 12345)))\nmodels.append(('RF', RandomForestClassifier(random_state = 12345)))\nmodels.append(('SVM', SVC(gamma='auto', random_state = 12345)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345)))\n\n#evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n        \n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n#boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results,\n            vert=True, #vertical box alignment\n            patch_artist=True) #fill with color\n                         \nax.set_xticklabels(names)\nplt.show()","fc4706e5":"rf_params = {\"n_estimators\" :[100,200,500,1000], \n             \"max_features\": [3,5,7], \n             \"min_samples_split\": [2,5,10,30],\n            \"max_depth\": [3,5,8,None]}","73abf552":"rf_model = RandomForestClassifier(random_state = 12345)","28e3eed7":"gs_cv = GridSearchCV(rf_model, \n                    rf_params,\n                    cv = 10,\n                    n_jobs = -1,\n                    verbose = 2).fit(X, y)","9c50aebd":"gs_cv.best_params_","c31e748b":"rf_tuned = RandomForestClassifier(**gs_cv.best_params_)","216619f8":"rf_tuned = rf_tuned.fit(X,y)","75f771ac":"cross_val_score(rf_tuned, X, y, cv = 10).mean()","d3b64c51":"feature_imp = pd.Series(rf_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index, palette=\"Blues_d\")\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Severity Levels\")\nplt.show()","c99d84d1":"xgb = GradientBoostingClassifier(random_state = 12345)","16d3eeed":"xgb_params = {\n    \"learning_rate\": [0.01, 0.1, 0.2, 1],\n    \"min_samples_split\": np.linspace(0.1, 0.5, 3),\n    \"max_depth\":[3,5,8],\n    \"subsample\":[0.5, 0.9, 1.0],\n    \"n_estimators\": [100,500]}","696c8451":"xgb_cv = GridSearchCV(xgb,xgb_params, cv = 10, n_jobs = -1, verbose = 2).fit(X, y)","8e345e43":"xgb_cv.best_params_","67fa10a8":"xgb_tuned = GradientBoostingClassifier(**xgb_cv.best_params_).fit(X,y)","08771d3e":"cross_val_score(xgb_tuned, X, y, cv = 10).mean()","5913c677":"feature_imp = pd.Series(xgb_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index, palette=\"Blues_d\")\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Severity Levels\")\nplt.show()","029b369f":"lgbm = LGBMClassifier(random_state = 12345)","2b4e999a":"lgbm_params = {\"learning_rate\": [0.01, 0.03, 0.05, 0.1, 0.5],\n              \"n_estimators\": [500, 1000, 1500],\n              \"max_depth\":[3,5,8]}","dcec6943":"gs_cv = GridSearchCV(lgbm, \n                     lgbm_params, \n                     cv = 10, \n                     n_jobs = -1, \n                     verbose = 2).fit(X, y)","240cd262":"gs_cv.best_params_","b78fdc01":"lgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X,y)","4fdbe0c8":"cross_val_score(lgbm_tuned, X, y, cv = 10).mean()","34c99b79":"feature_imp = pd.Series(lgbm_tuned.feature_importances_,\n                        index=X.columns).sort_values(ascending=False)\n\nsns.barplot(x=feature_imp, y=feature_imp.index, palette=\"Blues_d\")\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Feature Severity Levels\")\nplt.show()","88f723a0":"models = []\n\nmodels.append(('RF', RandomForestClassifier(random_state = 12345, max_depth = 8, max_features = 7, min_samples_split = 2, n_estimators = 500)))\nmodels.append(('XGB', GradientBoostingClassifier(random_state = 12345, learning_rate = 0.1, max_depth = 5, min_samples_split = 0.1, n_estimators = 100, subsample = 1.0)))\nmodels.append((\"LightGBM\", LGBMClassifier(random_state = 12345, learning_rate = 0.01,  max_depth = 3, n_estimators = 1000)))\n\nresults = []\nnames = []","09bafd83":"for name, model in models:\n    \n        cv_results = cross_val_score(model, X, y, cv = 10, scoring= \"accuracy\")\n        results.append(cv_results)\n        names.append(name)\n        msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n        print(msg)\n        \n# boxplot algorithm comparison\nfig = plt.figure(figsize=(15,10))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results,\n            vert=True, #vertical box alignment\n            patch_artist=True) #fill with color\n                         \nax.set_xticklabels(names)\nplt.show()","0d2332d7":"## Feature Engineering","cd6d90a8":"# Subject of the Project\n\nThe dataset is primarily used for predicting the onset of diabetes within five years in females of Pima Indian heritage over the age of 21 given medical details about their bodies. The dataset is meant to correspond with a binary (2-class) classification machine learning problem. \n\nWe have a dependent variable that indicates the state of having diabetes. Our goal is to model the relationship between other variables and whether or not they have diabetes.\n\nWhen the various features of the people are entered, we want to establish a machine learning model that will make a prediction about whether these people will have diabetes or not. This is a classification problem.\n\n## Dataset Information\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\nWe have 9 columns and 768 instances (rows). The column names are provided as follows (in order):\n\n    - Pregnancies: Number of times pregnant\n    - Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n    - BloodPressure: Diastolic blood pressure (mm Hg)\n    - SkinThickness: Triceps skinfold thickness (mm)\n    - Insulin: 2-Hour serum insulin measurement (mu U\/ml)\n    - BMI: Body mass index (weight in kg\/(height in m) 2 )\n    - DiabetesPedigreeFunction: Diabetes pedigree function\n    - Age: Age (years)\n    - Outcome: Class variable (0 or 1, 0 = non-diabetic, 1 = diabetic)","9a98d43c":"With model tune operations, better estimation was made compared to base models.","2616f701":"Our eventual goal is to exploit patterns in our data in order to predict the onset of diabetes. Visualize some of the differences between those that developed diabetes and those that did not. ","b5f5e21c":"# Modeling","0e61a27d":"# Model Optimization","54546b04":"# Data Pre-Processing","60ffd588":"# Data Understanding","23091ec8":"### Final Model Installation","b9a405bb":"## One-Hot Encoding","2593cb1a":"## Variable Standardization","5d561030":"# Resources\n\n    - https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.sample.html\n    \n    - https:\/\/github.com\/omarozt\/MachineLearningWorkshop\n    \n    - https:\/\/www.kaggle.com\/ibrahimyildiz\/pima-indians-diabetes-pred-0-9078-acc\n    \n    - https:\/\/seaborn.pydata.org\/examples\/color_palettes.html\n        \n    - Feature Engineering Made Easy, Sinan Ozdemir and Divya Susarla ","e1fa6aef":"It seems that there is no missing value in the data set, but when the variables are examined, the zeros in these variables represent NA.","bd22617c":"# Comparison of Final Models","3fb83387":"Conclusion: The highest correlations with Outcome were observed between Glucose, BMI, Age and Pregnancies.","72b2b2d0":"### Final Model Installation","2db1dbd0":"### XGBoost Tuning","2c5c6365":"Independent and dependent variable selected from dataframe, groupby operation is applied to the dependent variable then the independent variable is selected and the median of this variable is taken.","251ab04b":"## Missing Data Analysis","c33adee4":"### LightGBM Tuning","bd5859af":"## Model Tuning","613d328e":"These histograms show us the distributions of 'BMI', 'BloodPressure', 'Glucose' for the two class variables (non-diabetes and diabetes).\n\nThere seems to be a large jump in 'Glucose' for those who will eventually develop diabetes. To solidify this, we can visualize correlation matrix in an attempt to quantify the relationship between these variables. ","58632822":"\nIt seems that this histogram is showing us a pretty big difference between Glucose and two prediction classes.","d71efc44":"### Random Forests Tuning","4ec45165":"# Conclusion    \n    \n    - Machine learning models were established to predict whether people will have diabetes with varying variables.\n\n    - The 3 classification models that best describe the dataset were selected and these models were compared according to their success rates. Compared models are Random Forests, XGBoost, LightGBM.\n\n    - As a result of this comparison; It is determined that the model that best describes and gives the best results is LightGBM.","01bf96eb":"### Final Model Installation","d18e5fe9":"RF, XGB and LightGBM gave good results. We focused on optimizing these models"}}