{"cell_type":{"6bdf8b14":"code","2e23fa32":"code","5f45b3b1":"code","16c77b8f":"code","7c183e94":"code","f07df34a":"code","7480eaba":"code","ed676275":"code","d62fd0e8":"code","627940f1":"code","93ff745c":"code","c38ed8b5":"code","b25641c1":"code","10bf19ba":"code","2f5cb719":"code","2d56370f":"code","79c1a497":"code","c07f678d":"code","70f6eaa5":"code","55c5b219":"code","8049529e":"code","3630c7cb":"code","b53a02b8":"code","72d16871":"code","e934c143":"code","0a14546a":"code","94e9111c":"code","468ae55a":"code","af337328":"code","2022a2c3":"code","c698d4dc":"code","4335c42e":"code","7f70f78f":"code","362445fb":"code","2f522a78":"code","ffd3bd96":"markdown","56181377":"markdown","3ac33354":"markdown","9f721ec3":"markdown","90614b0c":"markdown","f0062801":"markdown","b2a6eaf9":"markdown","d7a64db1":"markdown","66796a6f":"markdown","d7c90f01":"markdown","90c9b85b":"markdown","2306a8df":"markdown","6068b92f":"markdown","29706385":"markdown","b04ac333":"markdown","08430b24":"markdown","6ae5c2e4":"markdown","82621fa6":"markdown","3dbf6c02":"markdown","48768b70":"markdown","58de0bc6":"markdown","7b95310f":"markdown","12880c37":"markdown","3ca63075":"markdown","e94de915":"markdown","b5f5ce11":"markdown","30c2102a":"markdown","bb865183":"markdown","5a5098bf":"markdown","e1736de9":"markdown","bb6af6e3":"markdown","49ade12a":"markdown"},"source":{"6bdf8b14":"\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport warnings\nwarnings.filterwarnings('ignore')\n\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import plot_roc_curve\n\n%matplotlib inline","2e23fa32":"heart_dataset = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\nheart_dataset.head()","5f45b3b1":"heart_dataset.target.value_counts()","16c77b8f":"heart_dataset.target.value_counts(normalize=True)","7c183e94":"heart_dataset.target.value_counts().plot(kind = 'bar', color = [\"blue\", 'red'])\nplt.tight_layout()\nplt.show()","f07df34a":"pd.crosstab(heart_dataset.target, heart_dataset.sex)","7480eaba":"pd.crosstab(heart_dataset.target, heart_dataset.sex).plot(kind ='bar',\n                                                         figsize = (10, 8),\n                                                         color = [\"red\",\"blue\"])\nplt.title(\"Hear Disease Frequency for Sex attribute\")\nplt.xlabel(\"0 = No Disease \\t 1 = Disease\")\nplt.ylabel(\"Target Amount\")\nplt.legend([\"Male\", \"Female\"])\nplt.tight_layout()\nplt.show()","ed676275":"# Create another figure\nplt.figure(figsize=(10,6))\n\n# Start with positve examples\nplt.scatter(heart_dataset.age[heart_dataset.target==1], \n            heart_dataset.thalach[heart_dataset.target==1], \n            c=\"salmon\") # define it as a scatter figure\n\n# Now for negative examples, we want them on the same plot, so we call plt again\nplt.scatter(heart_dataset.age[heart_dataset.target==0], \n            heart_dataset.thalach[heart_dataset.target==0], \n            c=\"lightblue\") # axis always come as (x, y)\n\n# Add some helpful info\nplt.title(\"Heart Disease in function of Age and Max Heart Rate\")\nplt.xlabel(\"Age\")\nplt.legend([\"Disease\", \"No Disease\"])\nplt.ylabel(\"Max Heart Rate\");","d62fd0e8":"# Create a new crosstab and base plot\npd.crosstab(heart_dataset.cp, heart_dataset.target).plot(kind=\"bar\", \n                                   figsize=(10,6), \n                                   color=[\"blue\", \"salmon\"])\n\n# Add attributes to the plot to make it more readable\nplt.title(\"Heart Disease Frequency Per Chest Pain Type\")\nplt.xlabel(\"Chest Pain Type\")\nplt.ylabel(\"Frequency\")\nplt.legend([\"No Disease\", \"Disease\"])\nplt.xticks(rotation = 0);","627940f1":"corr_matrix = heart_dataset.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, \n            annot=True, \n            linewidths=0.5, \n            fmt= \".2f\", \n            cmap=\"YlGnBu\");","93ff745c":"X = heart_dataset.drop('target', axis = 1)\n","c38ed8b5":"y = heart_dataset.target.values","b25641c1":"# Random seed for reproducibility\nnp.random.seed(42)\n\n# Split into train & test set\nX_train, X_test, y_train, y_test = train_test_split(X, # independent variables \n                                                    y, # dependent variable\n                                                    test_size = 0.2) # percentage of data to use for test set","10bf19ba":"# Put models in a dictionary\nmodels = {\"KNN\": KNeighborsClassifier(),\n          \"Logistic Regression\": LogisticRegression(), \n          \"Random Forest\": RandomForestClassifier()}\n\n# Create function to fit and score models\ndef fit_and_score(models, X_train, X_test, y_train, y_test):\n    \"\"\"\n    Fits and evaluates given machine learning models.\n    models : a dict of different Scikit-Learn machine learning models\n    X_train : training data\n    X_test : testing data\n    y_train : labels assosciated with training data\n    y_test : labels assosciated with test data\n    \"\"\"\n    # Random seed for reproducible results\n    np.random.seed(42)\n    # Make a list to keep model scores\n    model_scores = {}\n    # Loop through models\n    for name, model in models.items():\n        # Fit the model to the data\n        model.fit(X_train, y_train)\n        # Evaluate the model and append its score to model_scores\n        model_scores[name] = model.score(X_test, y_test)\n    return model_scores","2f5cb719":"model_scores = fit_and_score(models=models,\n                             X_train=X_train,\n                             X_test=X_test,\n                             y_train=y_train,\n                             y_test=y_test)\nmodel_scores","2d56370f":"model_compare = pd.DataFrame(model_scores, index=['accuracy'])\nmodel_compare.T.plot.bar();","79c1a497":"# Create a list of train scores\ntrain_scores = []\n\n# Create a list of test scores\ntest_scores = []\n\n# Create a list of different values for n_neighbors\nneighbors = range(1, 21) # 1 to 20\n\n# Setup algorithm\nknn = KNeighborsClassifier()\n\n# Loop through different neighbors values\nfor i in neighbors:\n    knn.set_params(n_neighbors = i) # set neighbors value\n    \n    # Fit the algorithm\n    knn.fit(X_train, y_train)\n    \n    # Update the training scores\n    train_scores.append(knn.score(X_train, y_train))\n    \n    # Update the test scores\n    test_scores.append(knn.score(X_test, y_test))","c07f678d":"plt.plot(neighbors, train_scores, label=\"Train score\")\nplt.plot(neighbors, test_scores, label=\"Test score\")\nplt.xticks(np.arange(1, 21, 1))\nplt.xlabel(\"Number of neighbors\")\nplt.ylabel(\"Model score\")\nplt.legend()\n\nprint(f\"Maximum KNN score on the test data: {max(test_scores)*100:.2f}%\")","70f6eaa5":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Different RandomForestClassifier hyperparameters\nrf_grid = {\"n_estimators\": np.arange(10, 1000, 50),\n           \"max_depth\": [None, 3, 5, 10],\n           \"min_samples_split\": np.arange(2, 20, 2),\n           \"min_samples_leaf\": np.arange(1, 20, 2)}","55c5b219":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for LogisticRegression\nRandomSearch_model = RandomizedSearchCV(LogisticRegression(),\n                                param_distributions=log_reg_grid,\n                                cv=5,\n                                n_iter=20,\n                                verbose=True)\n\n# Fit random hyperparameter search model\nRandomSearch_model.fit(X_train, y_train);","8049529e":"RandomSearch_model.best_params_","3630c7cb":"RandomSearch_model.score(X_test, y_test)","b53a02b8":"# Setup random seed\nnp.random.seed(42)\n\n# Setup random hyperparameter search for RandomForestClassifier\nRFC = RandomizedSearchCV(RandomForestClassifier(),\n                           param_distributions=rf_grid,\n                           cv=5,\n                           n_iter=20,\n                           verbose=True)\n\n# Fit random hyperparameter search model\nRFC.fit(X_train, y_train);","72d16871":"RFC.score(X_test, y_test)","e934c143":"# Different LogisticRegression hyperparameters\nlog_reg_grid = {\"C\": np.logspace(-4, 4, 20),\n                \"solver\": [\"liblinear\"]}\n\n# Setup grid hyperparameter search for LogisticRegression\ngs_log_reg = GridSearchCV(LogisticRegression(),\n                          param_grid=log_reg_grid,\n                          cv=5,\n                          verbose=True)\n\n# Fit grid hyperparameter search model\ngs_log_reg.fit(X_train, y_train);","0a14546a":"# Make preidctions on test data\ny_preds = gs_log_reg.predict(X_test)","94e9111c":"# Import ROC curve function from metrics module\nfrom sklearn.metrics import plot_roc_curve\n\n# Plot ROC curve and calculate AUC metric\nplot_roc_curve(gs_log_reg, X_test, y_test);","468ae55a":"# Import Seaborn\nimport seaborn as sns\nsns.set(font_scale=1.5) # Increase font size\n\ndef plot_conf_mat(y_test, y_preds):\n    \"\"\"\n    Plots a confusion matrix using Seaborn's heatmap().\n    \"\"\"\n    fig, ax = plt.subplots(figsize=(3, 3))\n    ax = sns.heatmap(confusion_matrix(y_test, y_preds),\n                     annot=True, # Annotate the boxes\n                     cbar=False)\n    plt.xlabel(\"true label\")\n    plt.ylabel(\"predicted label\")\n    \nplot_conf_mat(y_test, y_preds)","af337328":"print(classification_report(y_test, y_preds))","2022a2c3":"# Import cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\n# Instantiate best model with best hyperparameters (found with GridSearchCV)\nclf = LogisticRegression(C=0.23357214690901212,\n                         solver=\"liblinear\")","c698d4dc":"cross_validation_score_accuracy = cross_val_score(clf,\n                                                  X,\n                                                  y,\n                                                  cv = 5,\n                                                  scoring = 'accuracy')","4335c42e":"cross_validation_score_precision = cross_val_score(clf,\n                                                  X,\n                                                  y,\n                                                  cv = 5,\n                                                  scoring = 'precision')","7f70f78f":"cross_validation_score_recall = cross_val_score(clf,\n                                                  X,\n                                                  y,\n                                                  cv = 5,\n                                                  scoring = 'recall')","362445fb":"cross_validation_score_f1 = cross_val_score(clf,\n                                                  X,\n                                                  y,\n                                                  cv = 5,\n                                                  scoring = 'f1')","2f522a78":"# Visualizing cross-validated metrics\ncv_metrics = pd.DataFrame({\"Accuracy\": cross_validation_score_accuracy,\n                            \"Precision\": cross_validation_score_precision,\n                            \"Recall\": cross_validation_score_recall,\n                            \"F1\": cross_validation_score_f1})\ncv_metrics.T.plot.bar(title=\"Cross-Validated Metrics\", legend=False);","ffd3bd96":"# 1.1 Classifications Explained: \n\nThe Central Statistics Office has a statutory role in the co-ordination of official statistics in Ireland. In particular, it has the authority to ensure that appropriate standard classifications are used for this purpose. To this end, the Statistics Act 1993, 10(2), states that \u2019The Office (viz. Central Statistics Office) shall have authority to co-ordinate official statistics compiled by public authorities to ensure, in particular, adherence to statistical standards and the use of appropriate classifications.\n\nA classification is an ordered set of related categories used to group data according to its similarities. It consists of codes and descriptors and allows survey responses to be put into meaningful categories in order to produce useful data. A classification is a useful tool for anyone developing statistical surveys. It is a framework which both simplifies the topic being studied and makes it easy to categorise all data or responses received.","56181377":"## <b style=\"color:blue\">what does pd.crosstab() do?<\/b>\n### crosstab. Compute a simple cross tabulation of two (or more) factors. By default computes a frequency table of the factors unless an array of values and an aggregation function are passed.","3ac33354":"## 7.2 Confusion Metrics","9f721ec3":"# 5. Comparison The All Models Score","90614b0c":"## 7.3.3 Corss validation Recall score","f0062801":"# 3. Correlation\n\n## <b style=\"color:blue\">3.1 Definition<\/b>\nCorrelation is used to test relationships between quantitative variables or categorical variables. In other words, it\u2019s a measure of how things are related. The study of how variables are correlated is called correlation analysis.\n\n## <b style=\"color:blue\">3.2 Some examples of data that have a high correlation:<\/b>\n\n   * Your caloric intake and your weight.\n   * Your eye color and your relatives\u2019 eye colors.\n   * The amount of time your study and your GPA.\n   \n## <b style=\"color:blue\">3.3 Some examples of data that have a low correlation (or none at all):<\/b>\n\n   * Your sexual preference and the type of cereal you eat.\n   * A dog\u2019s name and the type of dog biscuit they prefer.\n   * The cost of a car wash and how long it takes to buy a soda inside the station.\n   \n## <b style=\"color:blue\">Types<\/b>\nThe most common correlation coefficient is the Pearson Correlation Coefficient. It\u2019s used to test for linear relationships between data. In AP stats or elementary stats, the Pearson is likely the only one you\u2019ll be working with. However, you may come across others, depending upon the type of data you are working with. For example, Goodman and Kruskal\u2019s lambda coefficient is a fairly common coefficient. It can be symmetric, where you do not have to specify which variable is dependent, and asymmetric where the dependent variable is specified.","b2a6eaf9":"## 7.3 Classification Report","d7a64db1":"# 2.2 Target of the Dataset","66796a6f":"# 1. Classification","d7c90f01":"## 4.3 Train - Test Split ","90c9b85b":"## 7.1 ROC curve","2306a8df":"## 7.3.1 Corss validation Accuracy score","6068b92f":"## 4.1 Independent Variables ","29706385":"###  <u><b>More specifically, we'll look at the following topics.<\/b><\/u>\n\n   ### <b style=\"color:blue\">1. Exploratory data analysis (EDA)<\/b> - the process of going through a dataset and finding out more about it.\n   \n   ### <b style=\"color:blue\">2. Model training<\/b>  - create model(s) to learn to predict a target variable based on other variables.\n   \n   ### <b style=\"color:blue\">3. Model evaluation<\/b>  - evaluating a models predictions using problem-specific evaluation metrics. \n   \n   ### <b style=\"color:blue\">4. Model comparison<\/b>  - comparing several different models to find the best one.\n   \n   ### <b style=\"color:blue\">5. Model fine-tuning<\/b>  - once we've found a good model, how can we improve it?\n   \n   ### <b style=\"color:blue\">6. Feature importance<\/b>  - since we're predicting the presence of heart disease, are there some things which are more important for prediction?\n   \n   ### <b style=\"color:blue\">7. Cross-validation<\/b>  - if we do build a good model, can we be sure it will work on unseen data?\n   \n   ### <b style=\"color:blue\">8. Reporting what we've found<\/b>  - if we had to present our work, what would we show someone?\n","b04ac333":"# 2. Import lnecessary ibraries python","08430b24":"<img src=\"https:\/\/img.grepmed.com\/uploads\/6940\/management-hfref-dapagliflozin-sglt2inhibitor-cardiology-32-original.png\" alt=\"CLS\">","6ae5c2e4":"# 7. Evaluate accuracy","82621fa6":"# 2.2.1 Normalize the target value","3dbf6c02":"## 7.4 Visualizing cross-validated metrics","48768b70":"## 7.3 Corss validation Score","58de0bc6":"## 4.2 Dependent Varialbes","7b95310f":"# 2.1 Load DataSet","12880c37":"# 1.3 Heart Disease Data Dictionary\n\nA data dictionary describes the data you're dealing with. Not all datasets come with them so this is where you may have to do your research or ask a **subject matter expert** (someone who knows about the data) for more.\n\nThe following are the features we'll use to predict our target variable (heart disease or no heart disease).\n\n   <b style=\"color:blue\">1. age<\/b> - age in years \n   \n   <b style=\"color:blue\">2. sex<\/b> - (1 = male; 0 = female) \n   \n   <b style=\"color:blue\">3. cp<\/b> - chest pain type \n     * 0: Typical angina: chest pain related decrease blood supply to the heart\n     * 1: Atypical angina: chest pain not related to heart\n     * 2: Non-anginal pain: typically esophageal spasms (non heart related)\n     * 3: Asymptomatic: chest pain not showing signs of disease\n     \n   <b style=\"color:blue\">4. trestbps<\/b> - resting blood pressure (in mm Hg on admission to the hospital)\n     * anything above 130-140 is typically cause for concern\n     \n   <b style=\"color:blue\">5. chol<\/b> - serum cholestoral in mg\/dl \n     * serum = LDL + HDL + .2 * triglycerides\n     * above 200 is cause for concern\n     \n   <b style=\"color:blue\">6. fbs<\/b> - (fasting blood sugar > 120 mg\/dl) (1 = true; 0 = false) \n     * '>126' mg\/dL signals diabetes\n     \n   <b style=\"color:blue\">7. restecg<\/b> - resting electrocardiographic results\n     * 0: Nothing to note\n     * 1: ST-T Wave abnormality\n        - can range from mild symptoms to severe problems\n        - signals non-normal heart beat\n     * 2: Possible or definite left ventricular hypertrophy\n        - Enlarged heart's main pumping chamber\n        \n   <b style=\"color:blue\">8. thalach<\/b> - maximum heart rate achieved \n   \n   <b style=\"color:blue\">9. exang<\/b> - exercise induced angina (1 = yes; 0 = no) \n   \n   <b style=\"color:blue\">10. oldpeak<\/b> - ST depression induced by exercise relative to rest \n     * looks at stress of heart during excercise\n     * unhealthy heart will stress more\n     \n   <b style=\"color:blue\">11. slope<\/b> - the slope of the peak exercise ST segment\n     * 0: Upsloping: better heart rate with excercise (uncommon)\n     * 1: Flatsloping: minimal change (typical healthy heart)\n     * 2: Downslopins: signs of unhealthy heart\n     \n   <b style=\"color:blue\">12. ca<\/b> - number of major vessels (0-3) colored by flourosopy \n     * colored vessel means the doctor can see the blood passing through\n     * the more blood movement the better (no clots)\n     \n   <b style=\"color:blue\">13. thal<\/b> - thalium stress result\n     * 1,3: normal\n     * 6: fixed defect: used to be defect but ok now\n     * 7: reversable defect: no proper blood movement when excercising \n     \n   <b style=\"color:blue\">14. target<\/b> - have disease or not (1=yes, 0=no) (= the predicted attribute)\n","3ca63075":"# 2.3 Compare Target columns with Sex column","e94de915":"# 4. Create Heart Disease Diagnosis Model","b5f5ce11":"# 1.2 Features of a classification\nIt is generally accepted that a standard classification will usually meet a number of requirements which are outlined below.\n\nExhaustive categories  - All survey responses need to fit into the classification structure somewhere\nPrecise and meaningful descriptors for categories - The content of each category in the classification should be clearly defined\nConceptually sound - The classification should have a conceptual basis and a logical structure\nStatistically balanced - In general, survey responses should not fall heavily into one category and sparsely into the other categories\nOperationally feasible - There is no point in having a classification that cannot be implemented in practice\nStatistically robust - The classification should be able to be used for a number of years without revision\nInternationally comparable - The classification should be comparable with any international standard classification","30c2102a":"## 4.4 Model Selection","bb865183":"## 7.3.4 Corss validation F1 score","5a5098bf":"## 7.3.2 Corss validation Precision score","e1736de9":"## 4.5 Find out the model score","bb6af6e3":"## 6. Apply RandomizeSearchCV","49ade12a":"<img src=\"https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S1386505617303684-gr2.jpg\" alt=\"Process\">"}}