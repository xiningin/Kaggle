{"cell_type":{"ed099425":"code","37144ccd":"code","efd0eda0":"code","f78592be":"code","2fc8e77c":"code","6bd29e45":"code","191a5247":"code","ec79e2e9":"code","8d461bb2":"code","404cb9e5":"code","64e20509":"code","aa8d8360":"code","81d934a8":"code","bc1031f9":"code","217f9ee0":"markdown","cfd6387a":"markdown"},"source":{"ed099425":"from kaggle.competitions import twosigmanews\nimport time\nimport numpy as np\nimport pandas as pd\nimport sklearn\nimport matplotlib.pyplot as plt","37144ccd":"env = twosigmanews.make_env()\ndays = env.get_prediction_days()","efd0eda0":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n(market_train, news_train) = env.get_training_data()","f78592be":"news_train","2fc8e77c":"def process(market_df, newsdf, train=False):\n    market_df['time'] = market_df.time.dt.strftime(\"%Y%m%d\").astype(int)\n    \n    cat_cols = ['assetCode']\n    num_cols = ['volume', 'close', 'open', 'returnsClosePrevRaw1', 'returnsOpenPrevRaw1', 'returnsClosePrevMktres1',\n                'returnsOpenPrevMktres1', 'returnsClosePrevRaw10', 'returnsOpenPrevRaw10', 'returnsClosePrevMktres10',\n                'returnsOpenPrevMktres10']\n\n    market_df['bartrend'] = market_df['close'] \/ market_df['open']\n\n    market_df['average'] = (market_df['close'] + market_df['open'])\/2\n    market_df['pricevolume'] = market_df['volume'] * market_df['close']\n    \n    # See Raba in https:\/\/www.kaggle.com\/rabaman\/0-64-in-100-lines\n    newsdf['time'] = newsdf.time.dt.strftime(\"%Y%m%d\").astype(int)\n    newsdf['assetCode'] = newsdf['assetCodes'].map(lambda x: list(eval(x))[0])\n    newsdf['position'] = newsdf['firstMentionSentence'] \/ newsdf['sentenceCount']\n    newsdf['coverage'] = newsdf['sentimentWordCount'] \/ newsdf['wordCount']\n    \n    droplist = ['sourceTimestamp','firstCreated','sourceId','headline','takeSequence','provider','firstMentionSentence',\n                'sentenceCount','bodySize','headlineTag','marketCommentary','subjects','audiences','sentimentClass',\n                'assetName', 'assetCodes','urgency','wordCount','sentimentWordCount']\n    newsdf.drop(droplist, axis=1, inplace=True)\n    market_df.drop(['assetName', 'volume'], axis=1, inplace=True)\n    \n    # combine multiple news reports for same assets on same day\n    newsgp = newsdf.groupby(['time','assetCode'], sort=False).aggregate(np.mean).reset_index()\n    \n    # join news reports to market data, note many assets will have many days without news data\n    market_train = pd.merge(market_df, newsgp, how='left', on=['time', 'assetCode'], copy=False)\n    num_cols = [x for x in market_train.columns if x not in cat_cols +['returnsOpenNextMktres10', 'universe']]\n    if train:\n        print(f\"In {(market_train['bartrend'] >= 1.2).sum()} lines price increased by 20% or more...dropping\")\n        market_train = market_train[market_train['bartrend'] <= 1.2]\n        print(f\"In {(market_train['bartrend'] <= 0.8).sum()} lines price decreased by 20% or more...dropping\")\n        market_train = market_train[market_train['bartrend'] >= 0.8]\n\n    market_train[num_cols] = market_train[num_cols].fillna(0)\n    \n    return market_train, cat_cols, num_cols\n\n\nmarket_train, cat_cols, num_cols = process(market_train, news_train, train=True)\n\nscaler = StandardScaler()\nmarket_train[num_cols] = scaler.fit_transform(market_train[num_cols])\n\ntrain_indices, val_indices = train_test_split(market_train.index.values,test_size=0.25, random_state=23)\n\ndef encode(encoder, x):\n    len_encoder = len(encoder)\n    try:\n        id = encoder[x]\n    except KeyError:\n        id = len_encoder\n    return id\n\nencoders = [{} for cat in cat_cols]\n\n\nfor i, cat in enumerate(cat_cols):\n    print('encoding %s ...' % cat, end=' ')\n    encoders[i] = {l: id for id, l in enumerate(market_train.loc[train_indices, cat].astype(str).unique())}\n    market_train[cat] = market_train[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n    print('Done')\n\nembed_sizes = [len(encoder) + 1 for encoder in encoders] #+1 for possible unknown assets\n\nmarket_train[num_cols] = market_train[num_cols].fillna(0)\nprint('scaling numerical columns')","6bd29e45":"from keras.models import Model\nfrom keras.layers import Input, Dense, Embedding, Concatenate, Flatten, BatchNormalization\nfrom keras.losses import binary_crossentropy, mse\n\ncategorical_inputs = []\nfor cat in cat_cols:\n    categorical_inputs.append(Input(shape=[1], name=cat))\n\ncategorical_embeddings = []\nfor i, cat in enumerate(cat_cols):\n    categorical_embeddings.append(Embedding(embed_sizes[i], 10)(categorical_inputs[i]))\n\n#categorical_logits = Concatenate()([Flatten()(cat_emb) for cat_emb in categorical_embeddings])\ncategorical_logits = Flatten()(categorical_embeddings[0])\ncategorical_logits = Dense(32,activation='relu')(categorical_logits)\n\nnumerical_inputs = Input(shape=(31,), name='num')\nnumerical_logits = numerical_inputs\nnumerical_logits = BatchNormalization()(numerical_logits)\n\nnumerical_logits = Dense(512,activation='sigmoid')(numerical_logits)\nnumerical_logits = Dense(256,activation='sigmoid')(numerical_logits)\nnumerical_logits = Dense(128,activation='sigmoid')(numerical_logits)\nnumerical_logits = Dense(64,activation='sigmoid')(numerical_logits)\n\nlogits = Concatenate()([numerical_logits,categorical_logits])\nlogits = Dense(64,activation='sigmoid')(logits)\nout = Dense(1, activation='sigmoid')(logits)\n\nmodel = Model(inputs = categorical_inputs + [numerical_inputs], outputs=out)\nmodel.compile(optimizer='adam',loss=binary_crossentropy)","191a5247":"def get_input(market_train, indices):\n    X_num = market_train.loc[indices, num_cols].values\n    X = {'num':X_num}\n    for cat in cat_cols:\n        X[cat] = market_train.loc[indices, cat_cols].values\n    y = (market_train.loc[indices,'returnsOpenNextMktres10'] >= 0).values\n    r = market_train.loc[indices,'returnsOpenNextMktres10'].values\n    u = market_train.loc[indices, 'universe']\n    d = market_train.loc[indices, 'time']\n    return X,y,r,u,d\n\n# r, u and d are used to calculate the scoring metric\nX_train,y_train,r_train,u_train,d_train = get_input(market_train, train_indices)\nX_valid,y_valid,r_valid,u_valid,d_valid = get_input(market_train, val_indices)","ec79e2e9":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\ncheck_point = ModelCheckpoint('model.hdf5',verbose=True, save_best_only=True)\nearly_stop = EarlyStopping(patience=3,verbose=True)\nmodel.fit(X_train,y_train.astype(int),\n          validation_data=(X_valid,y_valid.astype(int)),\n          epochs=15,\n          verbose=True,\n          callbacks=[early_stop,check_point]) ","8d461bb2":"from sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\n\n# distribution of confidence that will be used as submission\nmodel.load_weights('model.hdf5')\nconfidence_valid = model.predict(X_valid)[:,0]*2 -1\nprint(accuracy_score(confidence_valid>0,y_valid))\nplt.hist(confidence_valid, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","404cb9e5":"n_days = 0\nprep_time = 0\nprediction_time = 0\npackaging_time = 0\npredicted_confidences = np.array([])\nfor (market_obs_df, news_obs_df, predictions_template_df) in days:\n        \n    t = time.time()\n    \n    market_obs_df, cat_cols, num_cols = process(market_obs_df, news_obs_df)\n    market_obs_df[num_cols] = scaler.transform(market_obs_df[num_cols])\n    \n    market_obs_df['assetCode_encoded'] = market_obs_df[cat].astype(str).apply(lambda x: encode(encoders[i], x))\n\n    X_num_test = market_obs_df[num_cols].values\n    X_test = {'num':X_num_test}\n    X_test['assetCode'] = market_obs_df['assetCode_encoded'].values\n    \n    prep_time += time.time() - t\n    \n    t = time.time()\n    market_prediction = model.predict(X_test)[:,0]*2 -1\n    predicted_confidences = np.concatenate((predicted_confidences, market_prediction))\n    prediction_time += time.time() -t\n    \n    t = time.time()\n    preds = pd.DataFrame({'assetCode':market_obs_df['assetCode'],'confidence':market_prediction})\n    # insert predictions to template\n    predictions_template_df = predictions_template_df.merge(preds,how='left').drop('confidenceValue',axis=1).fillna(0).rename(columns={'confidence':'confidenceValue'})\n    env.predict(predictions_template_df)\n    packaging_time += time.time() - t\n\ntotal = prep_time + prediction_time + packaging_time\nprint(f'Preparing Data: {prep_time:.2f}s')\nprint(f'Making Predictions: {prediction_time:.2f}s')\nprint(f'Packing: {packaging_time:.2f}s')\nprint(f'Total: {total:.2f}s')","64e20509":"[x for x in market_train.columns if x not in market_obs_df.columns]","aa8d8360":"#env.predict(predictions_template_df)","81d934a8":"# distribution of confidence as a sanity check: they should be distributed as above\nplt.hist(predicted_confidences, bins='auto')\nplt.title(\"predicted confidence\")\nplt.show()","bc1031f9":"env.write_submission_file()","217f9ee0":"> # Improving on Baseline NN\n\nIn previous iterations I did some basic unpacking for the model and contest, and some baseline modeling. I'll add some of the news metrics to the model in this iteration.\n\nI used code or referred to findings from the following kernels and I recommend them:\n\nhttps:\/\/www.kaggle.com\/artgor\/eda-feature-engineering-and-everything\n\nhttps:\/\/www.kaggle.com\/marketneutral\/the-fallacy-of-encoding-assetcode\n\nhttps:\/\/www.kaggle.com\/rabaman\/0-64-in-100-lines (note the feature engineering).\n\nhttps:\/\/www.kaggle.com\/christofhenkel\/market-data-nn-baseline\n\n## Previous versions and scores:\n\nV1 -- Do Nothing: 0.0\n\nV2 -- Feature Engineering: 0.0\n\nV4 -- Linear Regression and Baseline NN: 0.609\n\nV6 -- Improving on Baseline NN 0.602","cfd6387a":"## Adding to baseline NN"}}