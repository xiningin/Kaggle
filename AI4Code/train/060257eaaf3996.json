{"cell_type":{"c33824bb":"code","71b1f70b":"code","000a0300":"code","7c42f84a":"code","a04c922f":"code","d5bceeae":"code","c1aec438":"code","341fa0cc":"code","58157134":"code","fea13699":"code","cdf1346e":"code","fa93bbcd":"code","19b1b42e":"code","d838eb88":"code","4398cb26":"code","ff6f8ce9":"code","6aa522dc":"code","f782f2c2":"code","8a5979a5":"code","b573c6ab":"code","b8cdf73d":"code","73536a2a":"code","3c43fb73":"code","af04aebc":"code","0e947710":"code","60767297":"code","6016dd3a":"code","e01029e9":"code","ede91158":"code","94652368":"code","7beb782c":"code","71a7c52a":"code","dbe519e5":"code","125a9e9e":"code","2d0d4224":"markdown","fdff19fd":"markdown","5b6c49d8":"markdown"},"source":{"c33824bb":"!pip install evalml\n!pip install h2o","71b1f70b":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom evalml.automl import AutoMLSearch\nimport pandas as pd\nimport numpy as np\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.ensemble import ExtraTreesRegressor\n#from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import OrdinalEncoder","000a0300":"df = pd.read_csv('..\/input\/jobathon-analytics-vidhya\/train.csv')\ntest = pd.read_csv('..\/input\/jobathon-analytics-vidhya\/test.csv')","7c42f84a":"#Since ID is not to be used|\ndf = df.set_index('ID')\ntest = test.set_index('ID')","a04c922f":"x_train,x_test,y_train,y_test = train_test_split(df.drop(['Response'],axis=1).copy(),df['Response'],test_size=0.2,random_state=42)","d5bceeae":"from evalml.automl import AutoMLSearch\nautoml = AutoMLSearch(X_train=x_train, y_train=y_train, problem_type='binary')\nautoml.search()","c1aec438":"!pip uninstall pandas -y\n!pip install pandas","341fa0cc":"automl.rankings","58157134":"automl.describe_pipeline(2) #We can then follow the steps and build our model or directly use it","fea13699":"#instantiate both packages to use\nencoder = OrdinalEncoder()\nimputer = IterativeImputer(ExtraTreesRegressor())\n# create a list of categorical columns to iterate over\ncat_cols_df = df.columns\ncat_cols_test = test.columns\ndef encode(data):\n    '''function to encode non-null data and replace it in the original data'''\n    #retains only non-null values\n    nonulls = np.array(data.dropna())\n    #reshapes the data for encoding\n    impute_reshape = nonulls.reshape(-1,1)\n    #encode date\n    impute_ordinal = encoder.fit_transform(impute_reshape)\n    #Assign back encoded values to non-null values\n    data.loc[data.notnull()] = np.squeeze(impute_ordinal)\n    return data\n\n#create a for loop to iterate through each column in the data\nfor columns in cat_cols_df:\n    encode(df[columns])\nfor columns in cat_cols_test:\n    encode(test[columns])","cdf1346e":"test.info(),df.info()","fa93bbcd":"imputer.fit_transform(df)","19b1b42e":"encode_data = pd.DataFrame(np.round(imputer.transform(df)),columns = df.columns)","d838eb88":"encode_data","4398cb26":"test = pd.DataFrame(np.round(imputer.fit_transform(test)),columns = test.columns)","ff6f8ce9":"test","6aa522dc":"test.to_csv('my_new_test.csv')\nencode_data.to_csv('my_new_data.csv')#making a backup","f782f2c2":"import h2o\nprint(h2o.__version__)\nfrom h2o.automl import H2OAutoML\n\nh2o.init(max_mem_size='16G')","8a5979a5":"train = h2o.H2OFrame(encode_data)\ntrain.head()","b573c6ab":"test = h2o.import_file('.\/my_new_test.csv')\n# test = test.drop(['C1','ID'])\ntest.head()","b8cdf73d":"from h2o.estimators import H2OTargetEncoderEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators import H2OStackedEnsembleEstimator,H2OXGBoostEstimator\nfrom h2o.grid.grid_search import H2OGridSearch","73536a2a":"train['Response'] = train['Response'].asfactor()","3c43fb73":"x = train.columns\ny = 'Response'\nx.remove(y)\nx","af04aebc":"aml = H2OAutoML(max_models=None, seed=42, max_runtime_secs=300,exploitation_ratio=0.2,nfolds=5,balance_classes=True,stopping_metric='AUC')\naml.train(x=x, y=y, training_frame=train)\nlb = aml.leaderboard\nlb.head(rows=lb.nrows)","0e947710":"preds = aml.predict(test)","60767297":"test_ = h2o.H2OFrame(pd.read_csv('..\/input\/jobathon-analytics-vidhya\/test.csv'))","6016dd3a":"preds = preds.cbind(test_['ID'])","e01029e9":"preds = preds.drop(['predict','p0'])","ede91158":"preds['Response'] = preds['p1']\npreds = preds.drop(['p1'])","94652368":"preds\n","7beb782c":"h2o.export_file(preds,path='.\/submission_f.csv',force=True)","71a7c52a":"len(test_)","dbe519e5":"preds = pd.read_csv('.\/submission_f.csv')","125a9e9e":"preds","2d0d4224":"The task of the competition was to find the best model which has maximum AUC. I used EvalMl and H2O AI platform for selecting the best model and parameters for training my model. Hope U guys enjoy and learn something insightful to you.\nThanks. Please Upvote :D .","fdff19fd":"# Installing and Importing relevant libraries","5b6c49d8":"**Please Upvote If you Liked it!!!**"}}