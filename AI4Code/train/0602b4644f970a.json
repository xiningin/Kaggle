{"cell_type":{"1ce94506":"code","1ab861e2":"code","343265b0":"code","d1dc676a":"code","efbef0e4":"code","f1934491":"code","bca73fb6":"code","15d4a67e":"code","762d28af":"code","fb3bfabc":"code","9a8d3d20":"code","09b2c276":"code","3a8c0aa9":"code","57086c0e":"code","b2139e1a":"markdown","acea67df":"markdown","cc4adc5b":"markdown","42d5c06e":"markdown","f0e82c49":"markdown","97a2dd6b":"markdown","87e02137":"markdown","d5894584":"markdown","3d009bc7":"markdown"},"source":{"1ce94506":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom scipy.interpolate import interp1d\nfrom sklearn.preprocessing import RobustScaler\n\nfiles = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'train' in filename:\n            files['train'] = os.path.join(dirname, filename)\n        if 'valid' in filename:\n            files['valid'] = os.path.join(dirname, filename)\n        if 'test' in filename:\n            files['test'] = os.path.join(dirname, filename)","1ab861e2":"class2id = {\n    'None': 0,\n    'D0': 1,\n    'D1': 2,\n    'D2': 3,\n    'D3': 4,\n    'D4': 5,\n}\nid2class = {v: k for k, v in class2id.items()}","343265b0":"dfs = {\n    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n    for k in files.keys()\n}","d1dc676a":"def interpolate_nans(padata, pkind='linear'):\n    \"\"\"\n    see: https:\/\/stackoverflow.com\/a\/53050216\/2167159\n    \"\"\"\n    aindexes = np.arange(padata.shape[0])\n    agood_indexes, = np.where(np.isfinite(padata))\n    f = interp1d(agood_indexes\n               , padata[agood_indexes]\n               , bounds_error=False\n               , copy=False\n               , fill_value=\"extrapolate\"\n               , kind=pkind)\n    return f(aindexes)","efbef0e4":"def date_encode(date):\n    if isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\")\n    return (\n        np.sin(2 * np.pi * date.timetuple().tm_yday \/ 366),\n        np.cos(2 * np.pi * date.timetuple().tm_yday \/ 366),\n    )\n\ndef loadXY(\n    df,\n    random_state=42, # keep this at 42\n    window_size=180, # how many days in the past (default\/competition: 180)\n    target_size=6, # how many weeks into the future (default\/competition: 6)\n    fuse_past=True, # add the past drought observations? (default: True)\n    return_fips=False, # return the county identifier (do not use for predictions)\n    encode_season=True, # encode the season using the function above (default: True) \n    use_prev_year=False, # add observations from 1 year prior?\n):\n    df = dfs[df]\n    soil_df = pd.read_csv(\"\/kaggle\/input\/soil_data.csv\")\n    time_data_cols = sorted(\n        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n    )\n    static_data_cols = sorted(\n        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n    )\n    count = 0\n    score_df = df.dropna(subset=[\"score\"])\n    X_static = np.empty((len(df) \/\/ window_size, len(static_data_cols)))\n    X_fips_date = []\n    add_dim = 0\n    if use_prev_year:\n        add_dim += len(time_data_cols)\n    if fuse_past:\n        add_dim += 1\n        if use_prev_year:\n            add_dim += 1\n    if encode_season:\n        add_dim += 2\n    X_time = np.empty(\n        (len(df) \/\/ window_size, window_size, len(time_data_cols) + add_dim)\n    )\n    y_past = np.empty((len(df) \/\/ window_size, window_size))\n    y_target = np.empty((len(df) \/\/ window_size, target_size))\n    if random_state is not None:\n        np.random.seed(random_state)\n    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n        if random_state is not None:\n            start_i = np.random.randint(1, window_size)\n        else:\n            start_i = 1\n        fips_df = df[(df.index.get_level_values(0) == fips)]\n        X = fips_df[time_data_cols].values\n        y = fips_df[\"score\"].values\n        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n            if use_prev_year:\n                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n                    continue\n                X_time[count, :, -len(time_data_cols) :] = X[\n                    i - 365 : i + window_size - 365\n                ]\n            if not fuse_past:\n                y_past[count] = interpolate_nans(y[i : i + window_size])\n            else:\n                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n                    y[i : i + window_size]\n                )\n            if encode_season:\n                enc_dates = [\n                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n                ]\n                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n            temp_y = y[i + window_size : i + window_size + target_size * 7]\n            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n            X_static[count] = X_s\n            count += 1\n    print(f\"loaded {count} samples\")\n    results = [X_static[:count], X_time[:count], y_target[:count]]\n    if not fuse_past:\n        results.append(y_past[:count])\n    if return_fips:\n        results.append(X_fips_date)\n    return results","f1934491":"scaler_dict = {}\nscaler_dict_static = {}\nscaler_dict_past = {}\n\n\ndef normalize(X_static, X_time, y_past=None, fit=False):\n    for index in tqdm(range(X_time.shape[-1])):\n        if fit:\n            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n        X_time[:, :, index] = (\n            scaler_dict[index]\n            .transform(X_time[:, :, index].reshape(-1, 1))\n            .reshape(-1, X_time.shape[-2])\n        )\n    for index in tqdm(range(X_static.shape[-1])):\n        if fit:\n            scaler_dict_static[index] = RobustScaler().fit(\n                X_static[:, index].reshape(-1, 1)\n            )\n        X_static[:, index] = (\n            scaler_dict_static[index]\n            .transform(X_static[:, index].reshape(-1, 1))\n            .reshape(1, -1)\n        )\n    index = 0\n    if y_past is not None:\n        if fit:\n            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n        y_past[:, :] = (\n            scaler_dict_past[index]\n            .transform(y_past.reshape(-1, 1))\n            .reshape(-1, y_past.shape[-1])\n        )\n        return X_static, X_time, y_past\n    return X_static, X_time","bca73fb6":"X_static_train, X_time_train, y_target_train = loadXY(\"train\")\nprint(\"train shape\", X_time_train.shape)\nX_static_valid, X_time_valid, y_target_valid, valid_fips = loadXY(\"valid\", return_fips=True)\nprint(\"validation shape\", X_time_valid.shape)\nX_static_train, X_time_train = normalize(X_static_train, X_time_train, fit=True)\nX_static_valid, X_time_valid = normalize(X_static_valid, X_time_valid)","15d4a67e":"batch_size = 128\noutput_weeks = 6\nuse_static = True\nhidden_dim = 512\nn_layers = 2\nffnn_layers = 2\ndropout = 0.1\none_cycle = True\nlr = 7e-5\nepochs = 10\nclip = 5","762d28af":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(\n    torch.tensor(X_time_train),\n    torch.tensor(X_static_train),\n    torch.tensor(y_target_train[:, :output_weeks]),\n)\ntrain_loader = DataLoader(\n    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n)\nvalid_data = TensorDataset(\n    torch.tensor(X_time_valid),\n    torch.tensor(X_static_valid),\n    torch.tensor(y_target_valid[:, :output_weeks]),\n)\nvalid_loader = DataLoader(\n    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n)","fb3bfabc":"import torch\nfrom torch import nn\nfrom sklearn.metrics import f1_score, mean_absolute_error\n\nclass DroughtNetLSTM(nn.Module):\n    def __init__(\n        self,\n        output_size,\n        num_input_features,\n        hidden_dim,\n        n_layers,\n        ffnn_layers,\n        drop_prob,\n        static_dim=0,\n    ):\n        super(DroughtNetLSTM, self).__init__()\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n\n        self.lstm = nn.LSTM(\n            num_input_features,\n            hidden_dim,\n            n_layers,\n            dropout=drop_prob,\n            batch_first=True,\n        )\n        self.dropout = nn.Dropout(drop_prob)\n        self.fflayers = []\n        for i in range(ffnn_layers - 1):\n            if i == 0:\n                self.fflayers.append(nn.Linear(hidden_dim + static_dim, hidden_dim))\n            else:\n                self.fflayers.append(nn.Linear(hidden_dim, hidden_dim))\n        self.fflayers = nn.ModuleList(self.fflayers)\n        self.final = nn.Linear(hidden_dim, output_size)\n\n    def forward(self, x, hidden, static=None):\n        batch_size = x.size(0)\n        x = x.to(dtype=torch.float32)\n        if static is not None:\n            static = static.to(dtype=torch.float32)\n        lstm_out, hidden = self.lstm(x, hidden)\n        lstm_out = lstm_out[:, -1, :]\n\n        out = self.dropout(lstm_out)\n        for i in range(len(self.fflayers)):\n            if i == 0 and static is not None:\n                out = self.fflayers[i](torch.cat((out, static), 1))\n            else:\n                out = self.fflayers[i](out)\n        out = self.final(out)\n\n        out = out.view(batch_size, -1)\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        weight = next(self.parameters()).data\n        hidden = (\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n            weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n        )\n        return hidden","9a8d3d20":"is_cuda = torch.cuda.is_available()\nif is_cuda:\n    device = torch.device(\"cuda\")\n    print(\"using GPU\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"using CPU\")\nstatic_dim = 0\nif use_static:\n    static_dim = X_static_train.shape[-1]\nmodel = DroughtNetLSTM(\n    output_weeks,\n    X_time_train.shape[-1],\n    hidden_dim,\n    n_layers,\n    ffnn_layers,\n    dropout,\n    static_dim,\n)\nmodel.to(device)\nloss_function = nn.MSELoss()\nif one_cycle:\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.OneCycleLR(\n        optimizer, max_lr=lr, steps_per_epoch=len(train_loader), epochs=epochs\n    )\nelse:\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\ncounter = 0\nvalid_loss_min = np.Inf\ntorch.manual_seed(42)\nnp.random.seed(42)\nfor i in range(epochs):\n    h = model.init_hidden(batch_size)\n\n    for k, (inputs, static, labels) in tqdm(\n        enumerate(train_loader),\n        desc=f\"epoch {i+1}\/{epochs}\",\n        total=len(train_loader),\n    ):\n        model.train()\n        counter += 1\n        if len(inputs) < batch_size:\n            h = model.init_hidden(len(inputs))\n        h = tuple([e.data for e in h])\n        inputs, labels, static = (\n            inputs.to(device),\n            labels.to(device),\n            static.to(device),\n        )\n        model.zero_grad()\n        if use_static:\n            output, h = model(inputs, h, static)\n        else:\n            output, h = model(inputs, h)\n        loss = loss_function(output, labels.float())\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        if one_cycle:\n            scheduler.step()\n\n        with torch.no_grad():\n            if k == len(train_loader) - 1 or k == (len(train_loader) - 1) \/\/ 2:\n                val_h = model.init_hidden(batch_size)\n                val_losses = []\n                model.eval()\n                labels = []\n                preds = []\n                raw_labels = []\n                raw_preds = []\n                for inp, stat, lab in valid_loader:\n                    if len(inp) < batch_size:\n                        val_h = model.init_hidden(len(inp))\n                    val_h = tuple([each.data for each in val_h])\n                    inp, lab, stat = inp.to(device), lab.to(device), stat.to(device)\n                    if use_static:\n                        out, val_h = model(inp, val_h, stat)\n                    else:\n                        out, val_h = model(inp, val_h)\n                    val_loss = loss_function(out, lab.float())\n                    val_losses.append(val_loss.item())\n                    for labs in lab:\n                        labels.append([int(l.round()) for l in labs])\n                        raw_labels.append([float(l) for l in labs])\n                    for pred in out:\n                        preds.append([int(p.round()) for p in pred])\n                        raw_preds.append([float(p) for p in pred])\n                # log data\n                labels = np.array(labels)\n                preds = np.clip(np.array(preds), 0, 5)\n                raw_preds = np.array(raw_preds)\n                raw_labels = np.array(raw_labels)\n                for i in range(output_weeks):\n                    log_dict = {\n                        \"loss\": float(loss),\n                        \"epoch\": counter \/ len(train_loader),\n                        \"step\": counter,\n                        \"lr\": optimizer.param_groups[0][\"lr\"],\n                        \"week\": i + 1,\n                    }\n                    # w = f'week_{i+1}_'\n                    w = \"\"\n                    log_dict[f\"{w}validation_loss\"] = np.mean(val_losses)\n                    log_dict[f\"{w}macro_f1\"] = f1_score(\n                        labels[:, i], preds[:, i], average=\"macro\"\n                    )\n                    log_dict[f\"{w}micro_f1\"] = f1_score(\n                        labels[:, i], preds[:, i], average=\"micro\"\n                    )\n                    log_dict[f\"{w}mae\"] = mean_absolute_error(\n                        raw_labels[:, i], raw_preds[:, i]\n                    )\n                    print(log_dict)\n                    for j, f1 in enumerate(\n                        f1_score(labels[:, i], preds[:, i], average=None)\n                    ):\n                        log_dict[f\"{w}{id2class[j]}_f1\"] = f1\n                    model.train()\n                if np.mean(val_losses) <= valid_loss_min:\n                    torch.save(model.state_dict(), \".\/state_dict.pt\")\n                    print(\n                        \"Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...\".format(\n                            valid_loss_min, np.mean(val_losses)\n                        )\n                    )\n                    valid_loss_min = np.mean(val_losses)","09b2c276":"def predict(x, static=None):\n    if static is None:\n        out, _ = model(torch.tensor(x), val_h)\n    else:\n        out, _ = model(torch.tensor(x), val_h, static)\n    return out","3a8c0aa9":"dict_map = {\n    \"y_pred\": [],\n    \"y_pred_rounded\": [],\n    \"fips\": [],\n    \"date\": [],\n    \"y_true\": [],\n    \"week\": [],\n}\ni = 0\nfor x, static, y in tqdm(\n    valid_loader,\n    desc=\"validation predictions...\",\n):\n    val_h = tuple([each.data.cpu() for each in model.init_hidden(len(x))])\n    with torch.no_grad():\n        if use_static:\n            pred = predict(x, static).clone().detach()\n        else:\n            pred = predict(x).clone().detach()\n    for w in range(output_weeks):\n        dict_map[\"y_pred\"] += [float(p[w]) for p in pred]\n        dict_map[\"y_pred_rounded\"] += [int(p.round()[w]) for p in pred]\n        dict_map[\"fips\"] += [f[1][0] for f in valid_fips[i : i + len(x)]]\n        dict_map[\"date\"] += [f[1][1] for f in valid_fips[i : i + len(x)]]\n        dict_map[\"y_true\"] += [float(item[w]) for item in y]\n        dict_map[\"week\"] += [w] * len(x)\n    i += len(x)\ndf = pd.DataFrame(dict_map)","57086c0e":"for w in range(6):\n    wdf = df[df['week']==w]\n    mae = mean_absolute_error(wdf['y_true'], wdf['y_pred']).round(3)\n    f1 = f1_score(wdf['y_true'].round(),wdf['y_pred'].round(), average='macro').round(3)\n    print(f\"Week {w+1}\", f\"MAE {mae}\", f\"F1 {f1}\")\nmae = mean_absolute_error(df['y_true'], df['y_pred']).round(3)\nf1 = f1_score(df['y_true'].round(),df['y_pred'].round(), average='macro').round(3)\nprint(f\"Overall\", f\"MAE {mae}\", f\"F1 {f1}\")","b2139e1a":"Below we use PyTorch to load the data.","acea67df":"Now we add a helper to normalise the data.","cc4adc5b":"We load the json files for training, validation and testing into the ``files`` dictionary.","42d5c06e":"We also add a helper function to interpolate the drought values.","f0e82c49":"Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted.","97a2dd6b":"> ## US Drought & Meteorological Data Starter Notebook\nThis notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors.","87e02137":"## Loading & Visualizing the Data\nIn this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes.","d5894584":"We encode the day of year using sin\/cos and add the data loading function `loadXY`.","3d009bc7":"The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\nThis could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes."}}