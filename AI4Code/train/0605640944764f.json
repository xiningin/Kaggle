{"cell_type":{"5c139737":"code","1899c49b":"code","ee20cc13":"code","7b34feaf":"code","f125361f":"code","415fe7e5":"code","490f1a0f":"code","3825fc3f":"code","638f71f5":"code","2c6bd98f":"code","bc7f8dc5":"code","b5338c2a":"code","3bc8d1ae":"code","162c1ee1":"code","b6d5d544":"code","ad46f4e6":"code","3c0996b0":"markdown","a52a4c66":"markdown","e4d75a74":"markdown","790b10ca":"markdown","8574cc02":"markdown","9f90bda3":"markdown","f592a878":"markdown","ab5cec7d":"markdown","61ea46fe":"markdown","9dec41aa":"markdown","9521cb18":"markdown","1f56acbc":"markdown","2c34edff":"markdown","975402d3":"markdown","8a57f02a":"markdown"},"source":{"5c139737":"import pandas as pd\n\n# Read in track metadata with genre labels : CSV File\ntracks = pd.read_csv(\"..\/input\/fmarockvshiphop\/fma-rock-vs-hiphop.csv\")\n\n# Read in echonest_metrics file : JASON File\n\nechonest_metrics = pd.read_json(\"..\/input\/echonestmetricsjson\/echonest-metrics.json\", precise_float=True)\n\n","1899c49b":"# Read in track metrics with the features\n\n# Merge the relevant columns of tracks and echonest_metrics\necho_tracks = echonest_metrics.merge(tracks[['genre_top', 'track_id']], on='track_id')\n","ee20cc13":"\n# Inspect the resultant dataframe\necho_tracks.info()\n\nechonest_metrics.head()","7b34feaf":"# Create a correlation matrix\ncorr_metrics = echo_tracks.corr()\ncorr_metrics.style.background_gradient()","f125361f":"# Import the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Define our X\nX = echo_tracks.drop(columns=['genre_top', 'track_id']) \n\n# Define our labels\ny = echo_tracks['genre_top']\n\n# Scale the features and set the values to a new variable called X_scaled\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)","415fe7e5":"pd.DataFrame(X).corr().style.background_gradient()","490f1a0f":"pd.DataFrame(X_scaled).corr().style.background_gradient()","3825fc3f":"# This is just to make plots appear in the notebook\n%matplotlib inline\n\nimport numpy as np\n\n# Import our plotting module, and PCA class\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import PCA\n\n# Get our explained variance ratios from PCA using all features\npca = PCA()\npca.fit(X_scaled)\nexp_variance = pca.explained_variance_ratio_\n\n# plot the explained variance using a barplot\nfig, ax = plt.subplots()\n\nax.bar(range(pca.n_components_), exp_variance)\nax.set_xlabel('Principal Component #')\n\nPC_values = np.arange(pca.n_components_) + 1\nplt.plot(PC_values, pca.explained_variance_ratio_, 'ro-', linewidth=2)\nplt.title('Scree Plot')\nplt.xlabel('Principal Component')\nplt.ylabel('Proportion of Variance Explained')\nplt.show()","638f71f5":"\n# Calculate the cumulative explained variance\ncum_exp_variance = np.cumsum(exp_variance)\n\n# Plot the cumulative explained variance and draw a dashed line at 0.85.\nfig, ax = plt.subplots()\nax.plot(cum_exp_variance)\nax.axhline(y=0.85, linestyle='--')\n\n# choose the n_components where about 85% of our variance can be explained\nn_components = 6\n\n# Perform PCA with the chosen number of components and project data onto components\npca = PCA(n_components, random_state=10)\npca.fit(X_scaled)\nX_pca = pca.transform(X_scaled)","2c6bd98f":"# Import train_test_split function and Decision tree classifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier\n\n# Split our data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, random_state=10)\n\n# Train our decision tree\ntree = DecisionTreeClassifier(random_state=10)\ntree.fit(X_train, y_train)\n\n# Predict the labels for the test data\ny_pred_tree = tree.predict(X_test)","bc7f8dc5":"y_pred_tree","b5338c2a":"# Import LogisticRegression\nfrom sklearn.linear_model import LogisticRegression\n\n# Train our logisitic regression\nlogreg = LogisticRegression(random_state=10)\nlogreg.fit(X_train, y_train)\ny_pred_logit = logreg.predict(X_test)\n\n# Create the classification report for both models\nfrom sklearn.metrics import classification_report\nclass_rep_tree = classification_report(y_test, y_pred_tree)\nclass_rep_log = classification_report(y_test, y_pred_logit)\n\nprint(\"Decision Tree: \\n\", class_rep_tree)\n\nprint(\"=======================================\")\n\nprint(\"Logistic Regression: \\n\", class_rep_log)","3bc8d1ae":"# Subset a balanced proportion of data points\nhop_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Hip-Hop']\nrock_only = echo_tracks.loc[echo_tracks['genre_top'] == 'Rock']\n\n# subset only the rock songs, and take a sample the same size as there are hip-hop songs\nrock_only = rock_only.sample(hop_only.shape[0], random_state=10)\n\n# concatenate the dataframes hop_only and rock_only\nrock_hop_bal = pd.concat([rock_only, hop_only]) # This dataset is now balanced\n\n# The features, labels, and pca projection are now created for the balanced dataframe\nX = rock_hop_bal.drop(['genre_top', 'track_id'], axis=1) \ny = rock_hop_bal['genre_top']\nX_pca = pca.fit_transform(scaler.fit_transform(X))\n\n\n# Redefine the train and test set with the X_pca from the balanced data\nX_train, X_test, y_train, y_test = train_test_split(\n    X_pca, y, random_state=10)","162c1ee1":"# Train our decision tree on the balanced data\ntree = DecisionTreeClassifier(random_state=10)\ntree.fit(X_train, y_train)\ny_pred_tree = tree.predict(X_test)\n\n# Train our logistic regression on the balanced data\nlogreg = LogisticRegression(random_state=10)\nlogreg.fit(X_train, y_train)\ny_pred_logit = logreg.predict(X_test)\n\n# compare the models\nprint(\"Decision Tree: \\n\", classification_report(y_test, y_pred_tree))\nprint(\"Logistic Regression: \\n\", classification_report(y_test, y_pred_logit))","b6d5d544":"from sklearn.model_selection import KFold, cross_val_score\n\n# Set up our K-fold cross-validation\nkf = KFold(10)\n\ntree = DecisionTreeClassifier(random_state=10)\nlogreg = LogisticRegression(random_state=10)\n\n# Train our models using KFold cv\ntree_score = cross_val_score(tree, X_pca, y, cv=kf)\nlogit_score = cross_val_score(logreg, X_pca, y, cv=kf)\n\n# Print the mean of each array o scores\nprint(\"Decision Tree:\", np.mean(tree_score), \"Logistic Regression:\", np.mean(logit_score))","ad46f4e6":"# The features, labels, and pca projection are now created for the balanced dataframe\n\n# Import the StandardScaler\nfrom sklearn.preprocessing import StandardScaler\n\n# Define our X\nX = echo_tracks.drop(columns=['genre_top', 'track_id']) \n\n# Define our labels\ny = echo_tracks['genre_top']\n\n# Scale the features and set the values to a new variable called X_scaled\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n\n# Set up our K-fold cross-validation\nkf = KFold(10)\n\ntree = DecisionTreeClassifier(random_state=10)\nlogreg = LogisticRegression(random_state=10)\n\n\n# Scale the features and set the values to a new variable called X_scaled\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\n\n# # Get our explained variance ratios from PCA using all features\n# pca = PCA()\n# pca.fit(X_scaled)\n\n\n# Perform PCA with the chosen number of components and project data onto components\npca = PCA(n_components, random_state=10)\npca.fit(X_scaled)\nX_pca = pca.transform(X_scaled)\n\n\n# Train our models using KFold cv\ntree_score = cross_val_score(tree, X_pca, y, cv=kf)\nlogit_score = cross_val_score(logreg, X_pca, y, cv=kf)\n\n# Print the mean of each array o scores\nprint(\"Decision Tree:\", np.mean(tree_score), \"Logistic Regression:\", np.mean(logit_score))","3c0996b0":"# Cross-validation \nThe accuracy above is only based on one particular training sample. The result will probably with diferent if we choose a diferent training set. For that reason, let's use cross-validation to ensure the model is unbiased based on the training set split","a52a4c66":"## **Let's Check the Correlations between features**\n**Correlation** between features, is **NOT desired.** In fact, strong correlation between features means **redondant variables,**, no added values to the problem, and can lead to: \n1. **overfiting** and to \n2. unecessarly **slow the process**. \n\nLet's quickly check the correlation between the features and **drop redondant features**","e4d75a74":"No clear elbow in this scree plot, So, not straightforward to find the number of intrinsic dimensions using this method.\n\nLet's investigate the **cumulative explained variance** plot to determine how many features are required to explain, a given % of the variance.","790b10ca":"# **Inspect the resultant dataframe**\n","8574cc02":"# Compare Descision Tree VS Logistic Regresstion\n","9f90bda3":"# **Project Title: Classify Song Genres from Audio Data**","f592a878":"# Unbalanced Data\n\nFrom the report Results, we observe that Rock songs are fairly well classified, as compared to hip-hop songs. It might be because of unbalanced data. In fact, we have far more data points for the rock (3892) than for hip-hop (910). \n\nThis also suggests that most of the model's accuracy is driven by its ability to classify just rock songs, which is biased. To avoid this issue, we need to balanced the dataset and have about the same number of samples for eah class.","ab5cec7d":"Now we have balanced precision for all the two classes, this is a better model than before.\nN.B. For other Critical projects were the model needs to predict critical issues such as health diagnosis of Spam, unbiased classification can be acceptable. For ezample, False Positive (False Alarm) is less dangeous thant False Negative.","61ea46fe":"Notice that we now have a more accurate pre","9dec41aa":"# Cross Validation (Unbalanced Data)\nThis section is just to emphasize how important it is, to balanced the data before applying any model. \nAs shown by the results below, if we applied the cross balidation on the unbalanced data, we would have found that the two models have higher precision than it should be. ","9521cb18":"The objectif of this project is to compare two models for classifying Music (Song) genre based on Song Metadeta.\nTwo Machine Learning Model are compared here : Decision Tree and Logistic Regression.\n\n**Knowlege Gain from this project:**\n\n\n1. Import and Joint datasets\n2. Check Correlations between dataset Features\n3. Normalize Dataset\n4. Apply Principal Component Analysis (PCA) to transform the Dataset, and if possible Reduce the dimension of the Data\n5. Be aware that unbalanced dataset tends to falsely attribute higher precision to the  class with the biggest number of elements\n6. Balanced data classes to have good and unbiased model\n7. Apply cross_validation in order to avoid bias coming from Data Split","1f56acbc":"# **Import the dataset**\nFrom Kaggle Workspace, click **Add data** on the top right corner, then browse to look for the data you are interrested in\n","2c34edff":"\n# # **Normalize the Data**\nIf we had obvious STRONG correlation between the features, we could have just decided to drop the redondant features without any transformation.\nSince there is no STRONG correlation between the features, we will use other techniques to reduce the features of the dataset. We will use the **PCA, Principal Component Analysis** here.\nBut, before performing the PCA, the data **MUST BE NORMALIZED!**\n","975402d3":"# Corr(Original) = Corr(Scaled). \nMake Sure that the Original Features and the Scaled Features have the same correlation Matrix.\nIn fact, the scale and transformation of the data **DOES Not Change** the **relationship** between variables of the data","8a57f02a":"# **Data Merging\/Joining**"}}