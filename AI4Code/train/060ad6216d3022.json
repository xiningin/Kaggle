{"cell_type":{"030051b6":"code","0136e4b4":"code","f874d9e2":"code","bfdb5a21":"code","9a041a2d":"code","149af14d":"code","ca869def":"code","db05d1a1":"code","4f6b12d8":"code","88df77e4":"code","0332585b":"code","d26f2208":"code","9a918fe6":"code","bd79a399":"code","f48bef3c":"code","25d24700":"code","59be02c7":"code","b0fb43c3":"code","1cecae70":"code","47a4b2ac":"code","48c29266":"code","dc70b58c":"code","0ed9c2a7":"code","81693c53":"code","c71ec5bd":"code","a04b16da":"code","21682447":"markdown","a78155c9":"markdown","c16fd5e1":"markdown","f8824132":"markdown","ba98fe9b":"markdown","19eb78c1":"markdown","42c45d69":"markdown","154d7693":"markdown","ead510de":"markdown","8eb3ed2f":"markdown","b5309d5c":"markdown"},"source":{"030051b6":"# Let's import what we gonna to use firstly.\nimport nltk\nimport pandas as pd","0136e4b4":"# read a tabular dataset.\ndataset = pd.read_csv('..\/input\/Reviews.csv')","f874d9e2":"# the function `head(n: int)` will show the first n rows of data. The default value of n is 5.\ndataset.head()","bfdb5a21":"dataset['Score'].value_counts()","9a041a2d":"for ele in dataset['Score'].values:\n    mapping(ele)","149af14d":"# For easy-understanding, we modify this task to fine-grained estimation to binary classification.\ndef mapping(x):\n    if x > 4: return 1\n    if x <= 4: return 0\n\ndataset['label']= dataset['Score'].apply(mapping)\n\n# We would only use texts and labels, so just keep it simple.\ndataset = dataset[['Text', 'label']]","ca869def":"# Check the dataset again.\ndataset.head()","db05d1a1":"s = 'a b c'\ns.split(' ')","4f6b12d8":"def split_str(x):\n    return x.split(' ')","88df77e4":"# We can tokenize a sentence using a tokenizer or splitting by space simply.\ndataset['tokenized_sentences_nltk'] = dataset['Text'].apply(nltk.word_tokenize) # it take some times\ndataset['tokenized_sentences_naive'] = dataset['Text'].apply(lambda s: s.split(' '))","0332585b":"stopwords = set(nltk.corpus.stopwords.words())\nprint(stopwords)","d26f2208":"def clean_stopwords(sentence):\n    res = []\n    for word in sentence:\n        if word not in stopwords:\n            res.append(word)\n    return res\n    # return [w if w not in stopwords for w in sentence]\n\ndataset['tokenized_sentences_nltk_remove_stopwords'] = dataset['tokenized_sentences_nltk'].apply(clean_stopwords)","9a918fe6":"pd.set_option('display.max_colwidth', 300)","bd79a399":"dataset[['Text', 'tokenized_sentences_nltk', 'tokenized_sentences_naive', 'tokenized_sentences_nltk_remove_stopwords']].head()","f48bef3c":"# Extract the data\ncleaned_texts = dataset['tokenized_sentences_nltk_remove_stopwords'].values\nlabels = dataset['label'].values","25d24700":"# Calculate the word frequencies and word energies\nword_frequency = {} \nword_energy = {}\n\n# To handle missing key, you can use defaultdict which would initialize a item that miss it's key with a default value. \n# Or you can just use the `Counter` class in this situation.\n# ---\n# from collections import defaultdict, Counter\n# ...\n# ---\n\nfor text, label in zip(cleaned_texts, labels):\n    for word in text:\n        if word in word_frequency:\n            word_frequency[word] += 1\n        else:\n            word_frequency[word] = 1\n            \n        if label == 1:\n            if word not in word_energy:\n                word_energy[word] = 1\n            else:\n                word_energy[word] += 1\n        else:\n            if word not in word_energy:\n                word_energy[word] = -1\n            else:\n                word_energy[word] -= 1","59be02c7":"for word in word_energy:\n    word_energy[word] \/= word_frequency[word]","b0fb43c3":"reliable_word_energy = {}\nfor word in word_energy:\n    # we assume that the energies would be reliable only for words that appear more than 500 times in our corpus. \n    if word_frequency[word] > 500: \n        reliable_word_energy[word] = word_energy[word]","1cecae70":"top_30_positive_words = [v[0] for v in sorted(reliable_word_energy.items(), key=lambda x: x[1], reverse=True)[:30]]\ntop_30_negative_words = [v[0] for v in sorted(reliable_word_energy.items(), key=lambda x: x[1], reverse=False)[:30]]","47a4b2ac":"from wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\n%matplotlib inline\n\ndef plot_word_clouds(keywords):\n    wordcloud = WordCloud().generate(' '.join(keywords))\n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")","48c29266":"plot_word_clouds(top_30_positive_words)","dc70b58c":"plot_word_clouds(top_30_negative_words)","0ed9c2a7":"def calc_energies(sentence):\n    score = 0\n    for word in sentence:\n        if word in reliable_word_energy:\n            score += reliable_word_energy[word]\n    score \/= len(sentence) # normalization\n    return score\n\ndataset['sentiment_energy'] = dataset['tokenized_sentences_nltk_remove_stopwords'].apply(calc_energies)","81693c53":"import numpy as np\n\ndef threshold_searching(left=-1, right=1, num_thresholds=101):\n    record = []\n    for i in np.linspace(left, right, num_thresholds):\n        record.append(((dataset['sentiment_energy'] > i).astype('int') == dataset['label']).sum() \/ len(dataset))\n    plt.title('Threshold Searching')\n    plt.xlabel('Threshold Value')\n    plt.ylabel('Accuracy')\n    plt.plot(np.linspace(left, right, num_thresholds), record)\n    best_train_threshold = np.linspace(left, right, num_thresholds)[record.index(max(record))]\n    best_train_accuracy = max(record)\n    return best_train_threshold, best_train_accuracy\n\nbest_train_threshold, best_train_accuracy = threshold_searching()\nprint(best_train_threshold, best_train_accuracy)","c71ec5bd":"def our_sentiment_classifier(sentence, threshold=best_train_threshold):\n    energy = calc_energies(sentence)\n    if energy > threshold:\n        return 1\n    else:\n        return 0","a04b16da":"print('prediction', our_sentiment_classifier(['i', 'will', 'not', 'buy', 'it', 'again']))\nprint('prediction', our_sentiment_classifier(['it', 'is', 'really', 'delicious']))","21682447":"# Visualization","a78155c9":"# A simple algorithm\n\nLet's assume words in a sentence are totally independent which means we can consider each single word as a feature. So, for a sentence with 10 words, it would has 10 features and what we're going to do is predicting the sentiments using the words tendency.\n\n![](https:\/\/i.imgur.com\/StdAo5H.png)","c16fd5e1":"# Search the best threshold","f8824132":"# Load the dataset","ba98fe9b":"# Introduction to Text Analysis\n\n![](https:\/\/www.kdnuggets.com\/images\/sentiment-fig-1-689.jpg)\n\nText analysis is a process for parsing texts in order to extract some machine-readable facts. In this tutorial, we will play with sentiment analysis using Food Reviews dataset from Amazon. \n\n### What will you learn?\n\nSome basic operations of the following libraries:\n* `NLTK`: a leading platform for building Python programs to work with human language data.\n* `pandas`: a library for tabular data, provides fast, flexible, and expressive data structures designed to make working with structured.\n* `matplotlib`:  a plotting library, produces publication quality figures in a variety of hardcopy formats.","19eb78c1":"# Calculate the energies","42c45d69":"# Tokenization\n![](https:\/\/i.imgur.com\/0E4JLiZ.png)","154d7693":"> # Overview","ead510de":"# Remove the unreliable results","8eb3ed2f":"## Remove the stopwords","b5309d5c":"# Normalization"}}