{"cell_type":{"f66744cf":"code","716ec233":"code","a58936f0":"code","71e7f42a":"code","77d267eb":"code","3907c9d2":"code","3eb72d23":"code","71cb895a":"code","e5b64f04":"code","5e608438":"code","2b0bf711":"code","c945a984":"code","114726a5":"code","a03e3ddb":"code","ca26d99e":"code","52e01904":"code","383ab598":"code","de1e367c":"code","a1176200":"code","841a4dfb":"code","a2194b91":"code","842c3bbc":"code","418617a7":"code","f9c00a91":"code","2ae8e509":"code","25cb24a6":"code","cab2bea8":"code","4066cf1b":"code","8bae719a":"code","96bdc1a1":"code","6af66590":"code","2cb819ad":"code","24ee25fa":"code","6eed00f7":"code","97579310":"code","84564f40":"code","2a30ed0d":"code","e608d12c":"code","cbd32b2c":"code","cba0e1de":"code","49c7d7ca":"markdown","c9f30afc":"markdown","71022597":"markdown","39b0919c":"markdown","8d82cb75":"markdown","e45388f8":"markdown","c662d3cf":"markdown","afb295de":"markdown","d196ca6d":"markdown","8ce13a78":"markdown","692a0410":"markdown","a890d6c5":"markdown"},"source":{"f66744cf":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import OrdinalEncoder\n\nfrom scipy.special import boxcox, inv_boxcox\nfrom scipy import stats\n\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score\n\nimport xgboost as xgb\nimport lightgbm\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n","716ec233":"train = pd.read_csv(\"\/kaggle\/input\/mercedes-benz-greener-manufacturing\/train.csv.zip\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/mercedes-benz-greener-manufacturing\/sample_submission.csv.zip\")\ntest = pd.read_csv(\"\/kaggle\/input\/mercedes-benz-greener-manufacturing\/test.csv.zip\")","a58936f0":"frame = pd.DataFrame({\"Max Values\":list(train.max()),\"Min Values\": list(train.min())},index=train.columns)\nframe = frame.loc[frame[\"Max Values\"]==frame[\"Min Values\"]]","71e7f42a":"train.drop(list(frame.index),axis=1,inplace=True)","77d267eb":"train","3907c9d2":"enc = OrdinalEncoder()","3eb72d23":"encoded = pd.DataFrame(enc.fit_transform(train[train.dtypes.loc[train.dtypes==\"object\"].index]),\n                       columns=list(train.dtypes.loc[train.dtypes==\"object\"].index))\n\nfor column in encoded.columns:\n    train[column] = encoded[column]","71cb895a":"train","e5b64f04":"fig = plt.figure(figsize=(8,4))\nax1 = fig.add_subplot(211)\nprob = stats.probplot(train[\"y\"], dist=stats.norm,plot=ax1)\nax2 = fig.add_subplot(212)\nxt, lambda_param = stats.boxcox(train[\"y\"])\nprob = stats.probplot(xt, dist=stats.norm, plot=ax2)\nax2.set_title('Post Box-Cox transformation')\nplt.subplots_adjust(bottom=-1) ","5e608438":"fig1 = plt.figure(figsize=(12,6))\nsns.histplot(train[\"y\"])\nplt.title(\"Original Target Distribution\")","2b0bf711":"fig1 = plt.figure(figsize=(12,6))\nsns.histplot(xt)\nplt.title(\"Post Box-Cox Transformation Target Distribution\")","c945a984":"\"\"\" Parameters for training models with before Box-Cox and After Box Cox transformation \"\"\" \n\ny_ = train[\"y\"]\ny_boxcox,lambda_param = stats.boxcox(y_)\n\nprint(y_.shape)\nprint(y_boxcox.shape)\nprint(lambda_param)","114726a5":"X = train[train.columns[2:]]","a03e3ddb":"dt_model = DecisionTreeRegressor()\n\nrf_model = RandomForestRegressor(max_depth=6, criterion=\"mae\")\n\nlgbm_model = lightgbm.LGBMRegressor(boosting_type='gbdt',\n                                    learning_rate=0.06, \n                                    max_depth=5, \n                                    n_estimators = 100,random_state=1)\n\nxgb_model = xgb.XGBRegressor(\n                             learning_rate=0.06,\n                             max_depth=5,\n                             n_estimators=400,random_state=1)","ca26d99e":"CV_RESULTS = pd.DataFrame()","52e01904":"cv_dt = cross_val_score(dt_model,X,y_,cv=10,scoring=('r2'))\ncv_dt_boxcox = cross_val_score(dt_model,X,y_boxcox,cv=10,scoring=('r2'))\nCV_RESULTS[\"DecisionTrees\"] = cv_dt\nCV_RESULTS[\"DecisionTrees After Box-Cox\"] = cv_dt_boxcox","383ab598":"cv_rf = cross_val_score(rf_model,X,y_,cv=10,scoring=('r2'))\ncv_rf_boxcox = cross_val_score(rf_model,X,y_boxcox,cv=10,scoring=('r2'))\nCV_RESULTS[\"RandomForest\"] = cv_rf\nCV_RESULTS[\"RandomForest After Box-Cox\"] = cv_rf_boxcox","de1e367c":"cv_lgbm = cross_val_score(lgbm_model,X,y_,cv=10,scoring=('r2'))\ncv_lgbm_boxcox = cross_val_score(lgbm_model,X,y_boxcox,cv=10,scoring=('r2')) \nCV_RESULTS[\"LightGBM\"] = cv_lgbm\nCV_RESULTS[\"LightGBM After Box-Cox\"] = cv_lgbm_boxcox","a1176200":"cv_xgb = cross_val_score(xgb_model,X,y_,cv=10,scoring=('r2'))\ncv_xgb_boxcox = cross_val_score(xgb_model,X,y_boxcox,cv=10,scoring=('r2'))\nCV_RESULTS[\"XGBoost\"] = cv_xgb\nCV_RESULTS[\"XGBoost After Box-Cox\"] = cv_xgb_boxcox","841a4dfb":"CV_RESULTS","a2194b91":"X_train_, X_test_, y_train_, y_test_ = train_test_split(X,y_,test_size=0.1,random_state=101)\nX_train_bc, X_test_bc, y_train_bc, y_test_bc = train_test_split(X,y_boxcox,test_size=0.1,random_state=101)","842c3bbc":"\"\"\"RandomForest - Standard Target\"\"\"\nrf_model.fit(X_train_,y_train_)\nrf_predict = rf_model.predict(X_test_)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_,rf_predict)))","418617a7":"\"\"\"RandomForest - Box Cox Target\"\"\"\nrf_model.fit(X_train_bc,y_train_bc)\nrf_predict_bc = rf_model.predict(X_test_bc)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_bc,rf_predict_bc)))","f9c00a91":"\"\"\"LightGBM - Standard Target\"\"\"\nlgbm_model.fit(X_train_,y_train_)\nlgbm_predict = lgbm_model.predict(X_test_)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_,lgbm_predict)))","2ae8e509":"\"\"\"LightGBM - Box Cox Target\"\"\"\nlgbm_model.fit(X_train_bc,y_train_bc)\nlgbm_predict_bc = lgbm_model.predict(X_test_bc)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_bc,lgbm_predict_bc)))","25cb24a6":"\"\"\"XGBoost - Standard Target\"\"\"\nxgb_model.fit(X_train_,y_train_)\nxgb_predict = xgb_model.predict(X_test_)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_,xgb_predict)))","cab2bea8":"\"\"\"XGBoost - Box Cox Target\"\"\"\nxgb_model.fit(X_train_bc,y_train_bc)\nxgb_predict_bc = xgb_model.predict(X_test_bc)\nprint(\"R2 Score: {0}\".format(r2_score(y_test_bc,xgb_predict_bc)))","4066cf1b":"stacked_ = (xgb_predict + lgbm_predict + rf_predict)\/3\nprint(\"Stacked R2 Score: {0}\".format(r2_score(y_test_,stacked_)))","8bae719a":"stacked_bc = (xgb_predict_bc + lgbm_predict_bc + rf_predict_bc)\/3\nprint(\"Stacked R2 Score: {0}\".format(r2_score(y_test_bc,stacked_bc)))","96bdc1a1":"test_input = test[X.columns]\ntest_id = test[\"ID\"]","6af66590":"test_encoded = pd.DataFrame(enc.fit_transform(test_input[test_input.dtypes.loc[test_input.dtypes==\"object\"].index]),\n                       columns=list(test_input.dtypes.loc[test_input.dtypes==\"object\"].index))","2cb819ad":"for column in test_encoded.columns:\n    test_input[column] = test_encoded[column]","24ee25fa":"rf_model.fit(X,y_boxcox)\nlgbm_model.fit(X,y_boxcox)\nxgb_model.fit(X,y_boxcox)","6eed00f7":"prediction_1 = rf_model.predict(test_input)\nprediction_2 = lgbm_model.predict(test_input)\nprediction_3 = xgb_model.predict(test_input)","97579310":"prediction_final = (prediction_1+prediction_2+prediction_3)\/3","84564f40":"prediction_inverse = inv_boxcox(prediction_final,lambda_param)","2a30ed0d":"submission[\"y\"]=prediction_inverse","e608d12c":"submission[\"y\"]=submission[\"y\"].apply(lambda x: np.round(x)+1)","cbd32b2c":"submission","cba0e1de":"#submission.to_csv(\"submission_x_1.csv\",index=False)","49c7d7ca":"## Data\n**About dataset**\nThis dataset contains an anonymized set of variables, each representing a custom feature in a Mercedes car. For example, a variable could be 4WD, added air suspension, or a head-up display.\n\nThe ground truth is labeled \u2018y\u2019 and represents the time (in seconds) that the car took to pass testing for each variable.\n\nFile descriptions\nVariables with letters are categorical. Variables with 0\/1 are binary values.\n\ntrain.csv - the training set\ntest.csv - the test set, you must predict the 'y' variable for the 'ID's in this file\nsample_submission.csv - a sample submission file in the correct format\n\n## Objective of the notebook\n**From Notebook Author** <br>\n![image.png](attachment:6b974f6c-8c66-4376-9dcd-ca0f729d12fb.png)\n\nAs we can see dimension of dataset is quite large. Majority of the features are binary features, there are some features that have the same min value as max value which means that there is no change in any of data samples. I would assume these features should be dropped to reduce dimensionality of dataset. <br>\nAnother thing to consider are categorical features. We can encode those but since basically all of the features are sort of binary then I feel it would be beneficial to pick model that does not require things like scaling or normalization. In this case Tree based\/ensemble models would be our first choice, so in this case I will go with DecisionTree or Random Forest as our baseline model and LightGBM and XGBoost as potential upgrade.\nIn terms of encoding I will go with OrdinalEncoder (I don't need to care about scale and I don't want to increase dimension with One-Hot Encoder)\n\nAs for stacking, I will not stack models. The reason for this is, well this is potentially production environment and stacking is not the best practice in deployment conditions (If I am wrong, then correct me on this).\nBesides its been couple of years since this competition has ended so there is nothing to win. \n","c9f30afc":"## Target Distribution\n\nHere we are going to analyze our target variable distribution and as we can see in fig1, you target variable is maybe not most normally distributed however box cox transformation does not change much. In that case we can stick to original target. \nHowever for sake of experiment we will try to do it with post boxcox transformation and then with use of Lambda parameter from transformation inverse it and see our predictions.\n\nGenerally speaking, models that we are using are Non Parametric models (Decision trees) so we don't have to make assumptions about target distibution, however it doesn't mean that we can't see if that makes a difference.","71022597":"### Random Forest","39b0919c":"## Modeling\nWe will try 4 models.\n1. Decision Trees Regressor\n2. Random Forest Regressor\n3. LightGBM Regressor\n4. XGB Regressor\n\nWe will cross validate these models to see predictive ability of each of those models (and dataset)\n","8d82cb75":"### Stacked Models","e45388f8":"![image.png](attachment:522a77c8-7b0f-4f9a-94bb-816182a1224b.png)\n\nProbably you are going to ask me about last layer which is round(x)+1.\nWell generally we want to predict amount of seconds so I have figured that it would be nice to round values and I have noticed small performance boost with that. Secondly I have noticed that my model underpredict a little bit, therefore I added 1 spare second to our prediction.\nAlso from industrial perspective (real life) it would be good to add some time margin to our predictions, much like when we forecast supply chain and we use confidence interval to deliever slightly more supply than demanded.\n\nGenerally I know it is pretty old competition but I had blast doing it and I feel car manufacturing companies are still years ahead of many other manufacturing plants that would also benefits from improvements like this one.","c662d3cf":"### LightGBM","afb295de":"As we can see, stacked models had overall better performance than any of models alone. In this case I will use 100% of avaiable dataset to train our models and make the predictions using dataset with boxcox transformation. Later I will use lambda parameter to perform inverse box cox transformation to obtain real predictions.\n\n## Submission","d196ca6d":"# Mercedes-Benz - Test Bed Time Reduction\n\n**Competition Description**\nSince the first automobile, the Benz Patent Motor Car in 1886, Mercedes-Benz has stood for important automotive innovations. These include, for example, the passenger safety cell with crumple zone, the airbag and intelligent assistance systems. Mercedes-Benz applies for nearly 2000 patents per year, making the brand the European leader among premium car makers. Daimler\u2019s Mercedes-Benz cars are leaders in the premium car industry. With a huge selection of features and options, customers can choose the customized Mercedes-Benz of their dreams.\n\nTo ensure the safety and reliability of each and every unique car configuration before they hit the road, Daimler\u2019s engineers have developed a robust testing system. But, optimizing the speed of their testing system for so many possible feature combinations is complex and time-consuming without a powerful algorithmic approach. As one of the world\u2019s biggest manufacturers of premium cars, safety and efficiency are paramount on Daimler\u2019s production lines.\n\nIn this competition, Daimler is challenging Kagglers to tackle the curse of dimensionality and reduce the time that cars spend on the test bench. Competitors will work with a dataset representing different permutations of Mercedes-Benz car features to predict the time it takes to pass testing. Winning algorithms will contribute to speedier testing, resulting in lower carbon dioxide emissions without reducing Daimler\u2019s standards.","8ce13a78":"## Encoding","692a0410":"It seems like Box Cox transformation really made a difference here. Our best models are RandomForest, XGBoost and then LightGBM, which is quite suprising. I will separate them into train and test set to see their performance in random sampled datasets. For submission I will prepare 4 submission files. One for each model + one for stacked predictions.\nSince we have lambda parameter from Box Cox transformation, I can use models trained on box cox dataset to get my predictions.","a890d6c5":"### XGBoost"}}