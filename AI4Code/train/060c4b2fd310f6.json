{"cell_type":{"c6b2d767":"code","e1f968ae":"code","918f636a":"code","f37395b1":"code","01c2a8ad":"code","16d63ecb":"code","bbaa3e57":"code","51b588d7":"code","a0918e81":"code","7dca9c53":"code","3bcb13e5":"code","75dd6e54":"code","a3b89ec6":"code","54f1b7fd":"code","d5473479":"code","5ef4a850":"markdown","6716575b":"markdown","557c79c6":"markdown","bfc9c505":"markdown","00dfcbe9":"markdown","fa430dad":"markdown","05cd09dd":"markdown","4b301f2d":"markdown","560e1b8e":"markdown"},"source":{"c6b2d767":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n\n# load packages\nimport sys #access to system parameters https:\/\/docs.python.org\/3\/library\/sys.html\nprint(\"Python version: {}\". format(sys.version))\n\nimport pandas as pd #collection of functions for data processing and analysis modeled after R dataframes with SQL like features\nprint(\"pandas version: {}\". format(pd.__version__))\n\nimport matplotlib as plt #collection of functions for scientific and publication-ready visualization\nprint(\"matplotlib version: {}\". format(plt.__version__))\n\nimport numpy as np #foundational package for scientific computing\nprint(\"NumPy version: {}\". format(np.__version__))\n\nimport scipy as sp #collection of functions for scientific computing and advance mathematics\nprint(\"SciPy version: {}\". format(sp.__version__)) \n\nimport IPython\nfrom IPython import display #pretty printing of dataframes in Jupyter notebook\nprint(\"IPython version: {}\". format(IPython.__version__)) \n\nimport sklearn #collection of machine learning algorithms\nprint(\"scikit-learn version: {}\". format(sklearn.__version__))\n\n# misc libraries\nimport random\nimport time\n\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","e1f968ae":"# Common Model Algorithms\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n# Common Model Helpers\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n# Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.plotting import scatter_matrix\n\n# Configure Visualization Defaults\n#%matplotlib inline = show plots in Jupyter Notebook browser\n%matplotlib inline\n# Set style parameters\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8","918f636a":"# All NaN are listed as \"na\"\n\n#import data from file: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.read_csv.html\ndata_raw = pd.read_csv('..\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_training_set.csv', na_values = \"na\")\n\n#a dataset should be broken into 3 splits: train, test, and (final) validation\n#the test file provided is the validation file for competition submission\n#we will split the train set into train and test data in future sections\ndata_val  = pd.read_csv('..\/input\/aps-failure-at-scania-trucks-data-set\/aps_failure_test_set.csv', na_values = \"na\")\n\n#to play with our data we'll create a copy\n#remember python assignment or equal passes by reference vs values, so we use the copy function: https:\/\/stackoverflow.com\/questions\/46327494\/python-pandas-dataframe-copydeep-false-vs-copydeep-true-vs\ndata1 = data_raw.copy(deep = True)\n\n#however passing by reference is convenient, because we can clean both datasets at once\ndata_cleaner = [data1, data_val]\n\n#preview data\nprint (data_raw.info()) #https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.info.html\n#data_raw.head() #https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.head.html\n#data_raw.tail() #https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.tail.html\ndata_raw.sample(10) #https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sample.html","f37395b1":"print(\"No. of columns not containing null values\")\nprint(len(data1.columns[data1.notna().all()]))\nprint(\"-\"*10)\n\nprint(\"Total no. of columns in the dataframe\")\nprint(len(data1.columns))\nprint(\"-\"*10)\n\ndata1.describe(include = 'all')","01c2a8ad":"missing = data1.isna().sum().div(data1.shape[0]).mul(100).to_frame().sort_values(by=0, ascending = False)\nfig = missing.plot(kind='bar', figsize=(50,20), fontsize=24)\nfig.set_title(\"Plot:  Sorted Proportion of Missing Values by Column for Training Set\", fontsize=36)\nfig.set_xlabel(\"Columns\", fontsize=36)\nfig.set_ylabel(\"Proportion Missing (%)\", fontsize=36)","16d63ecb":"###COMPLETING: complete or delete missing values in train and test\/validation dataset\n\nfrom sklearn.preprocessing import LabelEncoder\nlb = LabelEncoder() \n\n# Select columns from the training set with more than 30% missing values\ndrop_column = data1.columns[data1.isnull().mean() > 0.3]\n\n# Process both datasets\nfor dataset in data_cleaner:\n    dataset.rename(columns={\"class\":\"Class\"}, inplace=True) #to avoid name collision with the class\n    # Encode class labels\n    dataset['Class'] = lb.fit_transform(dataset['Class'])\n    \n    # Delete selected features with too many missing values from both training sets\n    dataset.drop(drop_column, axis=1, inplace = True)\n    \n    for col in dataset:\n        # Complete missing col with mode\n        dataset[col].fillna(dataset[col].mean(), inplace = True)","bbaa3e57":"print(\"No. of columns not containing null values\")\nprint(len(data1.columns[data1.notna().all()]))\nprint(\"-\"*10)\n\nprint(\"Total no. of columns in the dataframe\")\nprint(len(data1.columns))\nprint(\"-\"*10)\n\ndata1.describe(include = 'all')","51b588d7":"#CONVERT: convert objects to category using Label Encoder for train and test\/validation dataset\n\n# define y variable aka target\/outcome\nTarget = ['Class']\n\n# define x variables for original features aka feature selection\ndata1_x = data1.columns.drop('Class')","a0918e81":"# split train and test data with function defaults\n# random_state -> seed or control random number generator: https:\/\/www.quora.com\/What-is-seed-in-random-number-generation\ntrain1_x, test1_x, train1_y, test1_y = model_selection.train_test_split(data1[data1_x], data1[Target], random_state = 0)\n\nprint(\"Data1 Shape: {}\".format(data1.shape))\nprint(\"Train1 Shape: {}\".format(train1_x.shape))\nprint(\"Test1 Shape: {}\".format(test1_x.shape))\n\ntrain1_x.head()","7dca9c53":"# Discrete Variable Correlation by Failure using\n# group by aka pivot table: https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.groupby.html\nfor x in data1_x:\n    if data1[x].dtype != 'float64' :\n        print('Failure Correlation by:', x)\n        print(data1[[x, Target[0]]].groupby(x, as_index=False).mean())\n        print('-'*10, '\\n')","3bcb13e5":"# https:\/\/www.kaggle.com\/percevalve\/scania-dataset-eda-for-histograms\n\nfrom collections import Counter\n\ndef get_tag(name):\n    return name.split(\"_\")[0]\n\ncolumns_list = train1_x.columns\n\nall_columns_with_tags = [a for a in columns_list if \"_\" in a]\nall_tags = [get_tag(a) for a in all_columns_with_tags]\nhists = [k for k, v in Counter(all_tags).items() if v == 10]\nhists_columns = [k for k in all_columns_with_tags if get_tag(k) in hists]\nhists_dict = {k:[col for col in hists_columns if k in col] for k in hists if get_tag(k) in hists}\ncounter_columns = [k for k in all_columns_with_tags if get_tag(k) not in hists]","75dd6e54":"# https:\/\/www.kaggle.com\/percevalve\/scania-dataset-eda-for-histograms\n    \nfor hist in hists:\n    data1[f\"{hist}_total\"] = sum(data1[col] for col in hists_dict[hist])\ndata1[\"system_age\"] = data1[[f\"{hist}_total\" for hist in hists]].max(axis=1)\n\nplt.figure(figsize=(15,5));\nfor_plotting = data1[data1.system_age>=0]\n_,bins,_ = plt.hist(np.log(for_plotting[for_plotting.Class==0].system_age+1),bins=100,density=True,alpha=0.5,label=\"Class 0\");\nplt.hist(np.log(for_plotting[for_plotting.Class==1].system_age+1),bins=bins,density=True,alpha=0.5,label=\"Class 1\");\nplt.legend();\nplt.ylabel(\"Percentage per categorie\");\nplt.xlabel(\"Number of measurements (a.k.a. System Age) for physical (log scale)\");\nplt.xlim(0,21);","a3b89ec6":"from sklearn.metrics.scorer import make_scorer\nfrom sklearn.metrics import confusion_matrix\n\ndef my_scorer(y_true,y_pred):\n    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n    cost = 10*fp+500*fn\n    return cost\n\nmy_func = make_scorer(my_scorer, greater_is_better=False)","54f1b7fd":"from sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom pprint import pprint\n\n# Number of components for pca\nn_components = [int(x) for x in np.linspace(start = 5, stop = 26, num = 1)]\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 1000, num = 10)]\n\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10, 50]\n\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4, 8]\n\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\nclf = RandomForestClassifier(class_weight=\"balanced\", random_state=0)\npca = PCA()\n\npipe = Pipeline(steps=[(\"pca\", pca),(\"clf\", clf)])\n\n# Create the random grid\nrandom_grid = {'pca__n_components': n_components,\n               'clf__n_estimators': n_estimators,\n               'clf__max_features': max_features,\n               'clf__max_depth': max_depth,\n               'clf__min_samples_split': min_samples_split,\n               'clf__min_samples_leaf': min_samples_leaf,\n               'clf__bootstrap': bootstrap\n              }\n\n# Look at parameters used by random forest\nprint('Parameters currently in use:\\n')\npprint(clf.get_params())\n\nsearch = RandomizedSearchCV(pipe, random_grid, iid = False, cv = 5, return_train_score = True, scoring = my_func, n_jobs = -1, verbose=3)\nsearch.fit(train1_x, train1_y)\n\n# Printing best classificator\nprint(\"Best parameter (CV score=%0.3f):\" % search.best_score_)\nprint(search.best_params_)","d5473479":"# Printing best classificator on test set\ntest_score = - search.score(test1_x, test1_y)\ntest_score_per_truck = test_score\/test1_x.shape[0]\n\nprint(\"Best model on test set (Cost = $ %0.2f):\" % test_score)\nprint(\"Best model cost per truck on test set (Cost = $ %0.2f)\" % test_score_per_truck)","5ef4a850":"As mentioned previously, the test file provided is really validation data for competition submission. So, we will use sklearn function to split the training data in two datasets; 75\/25 split. This is important, so we don't overfit our model. Meaning, the algorithm is so specific to a given subset, it cannot accurately generalize another subset, from the same dataset. It's important our algorithm has not seen the subset we will use to test, so it doesn't \"cheat\" by memorizing the answers. We will use sklearn's train_test_split function. In later sections we will also use sklearn's cross validation functions, that splits our dataset into train and test for data modeling comparison.","6716575b":"Forked from A Data Science Framework: To Achieve 99% Accuracy.\n\n# Table of Contents\n1. [Chapter 1 - How a Data Scientist Beat the Odds](#ch1)\n1. [Chapter 2 - A Data Science Framework](#ch2)\n1. [Chapter 3 - Step 1: Define the Problem and Step 2: Gather the Data](#ch3)\n1. [Chapter 4 - Step 3: Prepare Data for Consumption](#ch4)\n1. [Chapter 5 - The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting](#ch5)\n1. [Chapter 6 - Step 4: Perform Exploratory Analysis with Statistics](#ch6)\n1. [Chapter 7 - Step 5: Model Data](#ch7)\n1. [Chapter 8 - Evaluate Model Performance](#ch8)\n1. [Chapter 9 - Tune Model with Hyper-Parameters](#ch9)\n1. [Chapter 10 - Tune Model with Feature Selection](#ch10)\n1. [Chapter 11 - Step 6: Validate and Implement](#ch11)\n1. [Chapter 12 - Conclusion and Step 7: Optimize and Strategize](#ch12)\n1. [Change Log](#ch90)\n1. [Credits](#ch91)","557c79c6":"<a id=\"ch3\"><\/a>\n# Step 1: Define the Problem\nCreate a model which accurately predicts and minimizes the cost of failures.\n\nTotal_cost = 10 x No_Instances + 500 x No_Instances.\n\n......\n\n**Project Summary:**\n\n**Context**\nThe dataset consists of data collected from heavy Scania trucks in everyday usage. The system in focus is the Air Pressure system (APS) which generates pressurized air that is utilized in various functions in a truck, such as braking and gear changes. The datasets' positive class consists of component failures for a specific component of the APS system. The negative class consists of trucks with failures for components not related to the APS. The data consists of a subset of all available data, selected by experts.\n\n**Content**\nThe training set contains 60000 examples in total in which 59000 belong to the negative class and 1000 positive class. The test set contains 16000 examples. There are 171 attributes per record.\n\nThe attribute names of the data have been anonymized for proprietary reasons. It consists of both single numerical counters and histograms consisting of bins with different conditions. Typically the histograms have open-ended conditions at each end. For example, if we measuring the ambient temperature \"T\" then the histogram could be defined with 4 bins where:\n\nThe attributes are as follows: class, then anonymized operational data. The operational data have an identifier and a bin id, like \"Identifier_Bin\". In total there are 171 attributes, of which 7 are histogram variables. Missing values are denoted by \"na\".\n\n**Cost Function**\nThe total cost of a prediction model the sum of Cost_1 multiplied by the number of Instances with type 1 failure and Cost_2 with the number of instances with type 2 failure, resulting in a Total_cost. In this case Cost_1 = 10 refers to the cost that an unnecessary check needs to be done by an mechanic at an workshop, while Cost_2 = 500 refer to the cost of missing a faulty truck, which may cause a breakdown.\n* Total_cost = Cost_1 x No_Instances + Cost_2 x No_Instances.\n* Total_cost = 10 x No_Instances + 500 x No_Instances.\n\nCreate a model which accurately predicts and minimizes the cost of failures.\n\nPractice Skills\n* Multi-class classification\n* Python\n\n# Step 2: Gather the Data\n\nThe dataset is also given to us on a golden plater with test and train data at [Kaggle: Air pressure system failures in Scania trucks](https:\/\/www.kaggle.com\/uciml\/aps-failure-at-scania-trucks-data-set)","bfc9c505":"<a id=\"ch6\"><\/a>\n# Step 4: Perform Exploratory Analysis with Statistics\nNow that our data is cleaned, we will explore our data with descriptive and graphical statistics to describe and summarize our variables. In this stage, you will find yourself classifying features and determining their correlation with the target variable and each other.","00dfcbe9":"<a id=\"ch5\"><\/a>\n## 3.21 The 4 C's of Data Cleaning: Correcting, Completing, Creating, and Converting\nIn this stage, we will clean our data by 1) correcting aberrant values and outliers, 2) completing missing information, 3) creating new features for analysis, and 4) converting fields to the correct format for calculations and presentation.\n\n1. **Correcting:** Reviewing the data, there does not appear to be any aberrant or non-acceptable data inputs. In addition, we see we may have potential outliers in age and fare. However, since they are reasonable values, we will wait until after we complete our exploratory analysis to determine if we should include or exclude from the dataset. It should be noted, that if they were unreasonable values, for example age = 800 instead of 80, then it's probably a safe decision to fix now. However, we want to use caution when we modify data from its original value, because it may be necessary to create an accurate model.\n2. **Completing:** There are null values or missing data in the age, cabin, and embarked field. Missing values can be bad, because some algorithms don't know how-to handle null values and will fail. While others, like decision trees, can handle null values. Thus, it's important to fix before we start modeling, because we will compare and contrast several models. There are two common methods, either delete the record or populate the missing value using a reasonable input. It is not recommended to delete the record, especially a large percentage of records, unless it truly represents an incomplete record. Instead, it's best to impute missing values. A basic methodology for qualitative data is impute using mode. A basic methodology for quantitative data is impute using mean, median, or mean + randomized standard deviation. An intermediate methodology is to use the basic methodology based on specific criteria; like the average age by class or embark port by fare and SES. There are more complex methodologies, however before deploying, it should be compared to the base model to determine if complexity truly adds value. For this dataset, age will be imputed with the median, the cabin attribute will be dropped, and embark will be imputed with mode. Subsequent model iterations may modify this decision to determine if it improves the model\u2019s accuracy.\n3. **Creating:**  Feature engineering is when we use existing features to create new features to determine if they provide new signals to predict our outcome. For this dataset, we will create a title feature to determine if it played a role in survival.\n4. **Converting:** Last, but certainly not least, we'll deal with formatting. There are no date or currency formats, but datatype formats. Our categorical data imported as objects, which makes it difficult for mathematical calculations. For this dataset, we will convert object datatypes to categorical dummy variables.","fa430dad":"<a id=\"ch4\"><\/a>\n# Step 3: Prepare Data for Consumption\nSince step 2 was provided to us on a golden plater, so is step 3. Therefore, normal processes in data wrangling, such as data architecture, governance, and extraction are out of scope. Thus, only data cleaning is in scope.\n\n## 3.1 Import Libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks. The idea is why write ten lines of code, when you can write one line. ","05cd09dd":"## 3.2 Meet and Greet Data\n\nThis is the meet and greet step. Get to know your data by first name and learn a little bit about it. What does it look like (datatype and values), what makes it tick (independent\/feature variables(s)), what's its goals in life (dependent\/target variable(s)). Think of it like a first date, before you jump in and start poking it in the bedroom.\n\nTo begin this step, we first import our data. Next we use the info() and sample() function, to get a quick and dirty overview of variable datatypes (i.e. qualitative vs quantitative). Click here for the [Source Data Dictionary](https:\/\/www.kaggle.com\/uciml\/aps-failure-at-scania-trucks-data-set).\n\n1. The *class* variable is our outcome or dependent variable. It is a string datatype. \npositive classification or class 1 means there is component failures of specific components of the APS system.\nnegative classification or class 0 means there is a failure but not related to the APS\n\nAll other variables are potential predictor or independent variables. All the feature are positive integers : \n* 100 are \"counters\" that counts the occurrence of events known by a two-letter code. \n* 7 \"histograms\" of 10 bins, which means that here this is a counting of a particular physical value being in a particular range (also known as \"bucket\").\n**It's important to note, more predictor variables do not make a better model, but the right variables.** There is a heavy class imbalance.\n","4b301f2d":"## 3.22 Clean Data\n\nNow that we know what to clean, let's execute our code.\n\n** Developer Documentation: **\n* [pandas.DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/indexing.html)\n* [pandas.isnull](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.isnull.html)\n* [pandas.DataFrame.sum](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sum.html)\n* [pandas.DataFrame.mode](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.mode.html)\n* [pandas.DataFrame.copy](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.copy.html)\n* [pandas.DataFrame.fillna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.fillna.html)\n* [pandas.DataFrame.drop](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.drop.html)\n* [pandas.Series.value_counts](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.Series.value_counts.html)\n* [pandas.DataFrame.loc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.loc.html)","560e1b8e":"# Data Science Framework\n1. **Define the Problem:** If data science, big data, machine learning, predictive analytics, business intelligence, or any other buzzword is the solution, then what is the problem? As the saying goes, don't put the cart before the horse. Problems before requirements, requirements before solutions, solutions before design, and design before technology. Too often we are quick to jump on the new shiny technology, tool, or algorithm before determining the actual problem we are trying to solve.\n2. **Gather the Data:** John Naisbitt wrote in his 1984 (yes, 1984) book Megatrends, we are \u201cdrowning in data, yet staving for knowledge.\" So, chances are, the dataset(s) already exist somewhere, in some format. It may be external or internal, structured or unstructured, static or streamed, objective or subjective, etc. As the saying goes, you don't have to reinvent the wheel, you just have to know where to find it. In the next step, we will worry about transforming \"dirty\" data to \"clean\" data.\n3. **Prepare Data for Consumption:** This step is often referred to as data wrangling, a required process to turn \u201cwild\u201d data into useful data. Data wrangling includes implementing data architectures for storage and processing, developing data governance standards for quality and control, data extraction (i.e. ETL and web scraping), and data cleaning to identify aberrant, missing, or outlier data points.\n4. **Perform Exploratory Analysis:** Anybody who has ever worked with data knows, garbage-in, garbage-out (GIGO). Therefore, it is important to deploy descriptive and graphical statistics to look for potential problems, patterns, classifications, correlations and comparisons in the dataset. In addition, data categorization (i.e. qualitative vs quantitative) is also important to understand and select the correct hypothesis test or data model.\n5. **Model Data:** Like descriptive and inferential statistics, data modeling can either summarize the data or predict future outcomes. Your dataset and expected results, will determine the algorithms available for use. It's important to remember, algorithms are tools and not magical wands or silver bullets. You must still be the master craft (wo)man that knows how-to select the right tool for the job. An analogy would be asking someone to hand you a Philip screwdriver, and they hand you a flathead screwdriver or worst a hammer. At best, it shows a complete lack of understanding. At worst, it makes completing the project impossible. The same is true in data modelling. The wrong model can lead to poor performance at best and the wrong conclusion (that\u2019s used as actionable intelligence) at worst.\n6. **Validate and Implement Data Model:** After you've trained your model based on a subset of your data, it's time to test your model. This helps ensure you haven't overfit your model or made it so specific to the selected subset, that it does not accurately fit another subset from the same dataset. In this step we determine if our [model overfit, generalize, or underfit our dataset](http:\/\/docs.aws.amazon.com\/machine-learning\/latest\/dg\/model-fit-underfitting-vs-overfitting.html).\n7. **Optimize and Strategize:** This is the step where you iterate back through the process to make it better, stronger, faster than it was before. As a data scientist, your strategy should be to outsource developer operations and application plumbing, so you have more time to focus on recommendations and design."}}