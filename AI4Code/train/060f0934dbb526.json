{"cell_type":{"5a3042a4":"code","0bb4fb13":"code","4e8c8aab":"code","c6a477cc":"code","dab7a502":"code","6242d90c":"code","78635809":"code","70b010d6":"code","1f65c794":"code","e5b12085":"code","78ac89fe":"code","eb5d09da":"code","365b069a":"code","2a6ed3fd":"code","1f98413d":"markdown","1f889155":"markdown","96c4f5ad":"markdown","73e707bf":"markdown","82379917":"markdown","15c893a1":"markdown","3a5d665f":"markdown","679cafac":"markdown","39560c74":"markdown","6d9bbfe5":"markdown","0d47884e":"markdown","66c00913":"markdown","d2fdb45b":"markdown","67b5d27b":"markdown","f8be1e04":"markdown","196794af":"markdown","d116d1fa":"markdown","aee14673":"markdown","2fa0d330":"markdown","5a84c439":"markdown"},"source":{"5a3042a4":"import sys\n!cp ..\/input\/rapids\/rapids.0.18.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","0bb4fb13":"import cudf as pd # pandas on GPU\nimport cupy as np # numpy on GPU\nfrom cuml.decomposition import PCA # scikit-learn on GPU\n!pip install sentence-transformers\nfrom sentence_transformers import SentenceTransformer # PyTorch supported\nimport gc\nimport torch\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl","4e8c8aab":"df_train = pd.read_csv('..\/input\/actuarial-loss-estimation\/train.csv')\ndf_test  = pd.read_csv('..\/input\/actuarial-loss-estimation\/test.csv')","c6a477cc":"n0 = df_train.shape[0]\n# text is upper case in source data, but models are lower case\ntxt = [t.lower() for df in (df_train, df_test) for t in df.ClaimDescription.to_array()]\ntxt[:3]","dab7a502":"if torch.cuda.is_available(): # check if GPU enabled kernel\n    print('Cuda !')","6242d90c":"model = SentenceTransformer('paraphrase-distilroberta-base-v1', device='cuda')\nprint(f'Initial sequence length in paraphrase distilroberta : {model.max_seq_length}')\nprint(f'First sentence : {txt[0]}\\nCorresponding tokens : {model.tokenizer(txt[0])}')\nprint(f\"Maximal sequence length in our text data : {max([len(model.tokenizer(t)['input_ids']) for t in txt])}\")\n# resizing model max_seq_length for faster computations (remove a loooot of unuseful <PAD> tokens)\nmodel.max_seq_length = 25\nprint(f'Resized sequence length in paraphrase distilroberta : {model.max_seq_length}')","78635809":"txt_encoded = np.array(model.encode(txt, normalize_embeddings=True))\ntxt_encoded.shape","70b010d6":"plt.hist(np.var(txt_encoded, axis=0).get(), bins=100)\nplt.title('Variance on the 768 Embedding Coordinates')\nplt.show()","1f65c794":"pca = PCA(n_components=2, copy=True, random_state=0, svd_solver='jacobi', whiten=True, verbose=True)\ntxt_encoded_pca = pca.fit_transform(txt_encoded)","e5b12085":"pca.explained_variance_","78ac89fe":"fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(txt_encoded_pca[:, 0].get(), txt_encoded_pca[:, 1].get(), c='r')\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\nplt.show()","eb5d09da":"def quantile(array, threshold):\n    array_ = np.sort(array.flatten())\n    return array_[int(threshold * array_.shape[0])]","365b069a":"x, y = txt_encoded_pca[:n0, 0].get(), txt_encoded_pca[:n0, 1].get()\n# take only train data as we want to explain target, here ultimate cost\nc = df_train.UltimateIncurredClaimCost.values\ntrunc = quantile(c, 0.8)\n# for more visibility, as there are some very extreme values, I have to truncate too big values\ncriteria = np.where(c <= trunc)[0].get()\nx, y, c = x[criteria], y[criteria], c[criteria]\nfig, ax = plt.subplots(figsize=(10,10))\nax.scatter(x, y, c=c.get(), cmap='jet')\nax.set_xlabel('First Principal Component')\nax.set_ylabel('Second Principal Component')\n\ncmap = mpl.cm.jet\nnorm = mpl.colors.Normalize(vmin=min(c), vmax=max(c))\n\ncbar = fig.colorbar(mpl.cm.ScalarMappable(norm=norm, cmap=cmap),\n             ax=ax, orientation='vertical', label='Ultimate Cost')\n\nplt.savefig('Ultimate_cost_based_on_text.png', dpi=100)\nplt.show()","2a6ed3fd":"n_comp = 20\npca = PCA(n_components=n_comp, copy=True, random_state=0, svd_solver='jacobi', whiten=True, verbose=True)\ntxt_encoded_pca = pca.fit_transform(txt_encoded)\nembeddings_train, embeddings_test = pd.DataFrame(txt_encoded_pca[:n0, :], columns=[f'X_{i}' for i in range(n_comp)]), pd.DataFrame(txt_encoded_pca[n0:, :], columns=[f'X_{i}' for i in range(n_comp)])\nembeddings_train.to_csv(f'embeddings_train_{n_comp}.csv', index=False)\nembeddings_test.to_csv(f'embeddings_test_{n_comp}.csv', index=False)","1f98413d":"How much of the total variance are explained by these 2 first axes ?","1f889155":"Indeed, we can see that the variance on each embedding axis is very low (for comparison, the expected variance had the coordinates been sampled from a $\\mathcal{U}([0,1])$ would have been of $\\frac{1}{12} \\approx 0.0833$ along each of the 768 axes). It means, as expected, that the data is highly colinear, all embedding coordinates being centered on the same values.","96c4f5ad":"[RAPIDS](https:\/\/rapids.ai) enable you to perform every numpy, pandas or sklearn manipulation & modeling, entirely on GPU for higher performance.","73e707bf":"We get 768-dimensional vectorized and normalized representations of our 90_000 sentences.That's quite big, and not very efficient : as our sentences are from the same writing-style, they are probably highly correlated, that's not great for building a model that would be able to differentiate injury severity. Moreover, we would like the coordinates to bear explainability power, and ideally to have them orthonormal.","82379917":"## Summary :\n1. Obtaining Sentence Embeddings from Transformers\n2. Principal Components Analysis with RAPIDS\n2. Data Analysis and Vizualization","15c893a1":"This notebook is intended on providing [**optimal text embeddings**](https:\/\/www.kaggle.com\/louise2001\/embeddings-actuarial-loss-competition) for the claim descriptions of the [actuarial loss competition](https:\/\/www.kaggle.com\/c\/actuarial-loss-estimation). I will detail the whole procedure with additional illustrations of the obtained vectorized representations.","3a5d665f":"### Model Preparation","679cafac":"We will concatenate text data from train and test database in order to process them globally. That will provide better fitting in the PCA as well as prevent any unpleasant surprises.","39560c74":"### Clustering on Target Value","6d9bbfe5":"Isn't that cute ? Now, let's see how these axes bear explainability towards our problem. We will try to see how the accidents scatter along these 2 axes, based on the target ultimate cost that we wish to predict.","0d47884e":"Why did I choose the model trained on the paraphrase-scoring task ? We are trying to extract the global meaning of the description, in order to get an idea of the gravity or lasting consequences of an accident. It therefore seemed to me to be a related and adequate model to use for getting my raw embeddings.","66c00913":"### Installing RAPIDS and other requirements","d2fdb45b":"### Reading and Processing Text Data","67b5d27b":"# PCA on Sentence Embeddings with RAPIDS","f8be1e04":"We notice a very clear separation of the two halves of the heart into 2 subgroups, the left half with lower ultimate costs, the right half with higher ultimate costs.","196794af":"That's not much... Let's get a visualization of how our claim descriptions are scattered based on the first 2 explainability axes.","d116d1fa":"As a conclusion, the data provided in the [Embeddings dataset](https:\/\/www.kaggle.com\/louise2001\/embeddings-actuarial-loss-competition) has been generated on the first 20 principal orthonormal components. I provide the source code hereunder.","aee14673":"### Raw Roberta embeddings","2fa0d330":"### Principal Component Analysis","5a84c439":"Transformers are a very efficient way of getting optimal text embeddings.\nI will compute raw sentence embeddings based on the paraphrase-trained DistilRoberta. You can see more on this model [here](https:\/\/github.com\/UKPLab\/sentence-transformers) or [here](https:\/\/www.sbert.net)."}}