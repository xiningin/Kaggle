{"cell_type":{"d2aeedde":"code","1ac36549":"code","626c2d31":"code","e7213968":"code","ba894a44":"code","810ff0e7":"code","37fb8f3b":"code","12e4ef34":"code","9438e0a4":"code","7f142cd9":"code","ee6d8504":"code","bd4c1c8b":"code","4706853b":"code","1aacfc64":"code","0c00e41d":"code","d4242911":"code","febdc8e5":"code","1cbeb05d":"code","2e053483":"code","20aaa342":"code","92313173":"code","dd7b6d08":"code","3a9b6496":"code","018e1140":"code","2b22e8fe":"code","44ac477b":"code","d4fe3da2":"code","bdaf067d":"code","97c5c805":"code","9c73901c":"code","a0055c5e":"code","7c024a5a":"code","26667765":"code","03e6afde":"code","3a0b5510":"code","cb4b5a6a":"code","bee4f4ae":"code","122ce06f":"code","c4924396":"code","d2f97a27":"code","82405226":"code","ed6f3bca":"markdown","4044a347":"markdown","34e02357":"markdown","3ab15669":"markdown","d6c83a38":"markdown","585e76bd":"markdown","e3779de1":"markdown","a12cb0e1":"markdown","92192429":"markdown","b217d474":"markdown","a016e7b2":"markdown","1f7bdf60":"markdown","e508cc3b":"markdown","94ebeeb9":"markdown","66d9da78":"markdown","c79e7a7e":"markdown","e6bf04a1":"markdown","c5c30f42":"markdown","f824d989":"markdown","7d122c82":"markdown"},"source":{"d2aeedde":"# target store \nSTORE = 'CA_1'\n\n# first and last days which are considered for training data\nFIRST_TRAIN_DAY=600\nLAST_TRAIN_DAY= 1941\n\n# set cross validation conditions\nVALID_SET_LENGTH = 28\nCV_STEPS = 3\n\n# path for data import\nPATH_TO_FILE = \"..\/input\/m5-forecasting-accuracy\"","1ac36549":"import pandas as pd\nimport numpy as np\nimport gc\nimport os\nfrom typing import Union\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()\n\nfrom sklearn.metrics import mean_squared_error\nfrom scipy.sparse import csr_matrix\n\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\n\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate, Flatten, BatchNormalization\nfrom tensorflow.keras.models import Model\n","626c2d31":"# credit to https:\/\/www.kaggle.com\/ragnar123\/very-fst-model\ndef reduce_memory_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df.loc[:,col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df.loc[:,col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df.loc[:,col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df.loc[:,col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df.loc[:,col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df.loc[:,col] = df[col].astype(np.float32)\n                else:\n                    df.loc[:,col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df  \n","e7213968":"def load_sales_data(path=PATH_TO_FILE):\n    print('Load sales data ...')\n    sales_all = pd.read_csv(os.path.join(path, \"sales_train_evaluation.csv\"))\n    sales=sales_all[sales_all.store_id==STORE].copy()\n    return sales, sales_all\n\ndef load_calendar_data(path=PATH_TO_FILE):\n    print('Load calendar data ...')\n    calendar_all = pd.read_csv(os.path.join(path, \"calendar.csv\"))\n    if 'CA' in STORE:\n        calendar=calendar_all.drop(['snap_TX','snap_WI'], axis=1)    \n    elif 'TX' in STORE:\n        calendar=calendar_all.drop(['snap_CA','snap_WI'], axis=1) \n    elif 'WI' in STORE:\n        calendar=calendar_all.drop(['snap_CA','snap_TX'], axis=1)\n    return calendar, calendar_all\n\n\ndef load_price_data(path=PATH_TO_FILE):\n    print('Load price data ...')\n    prices_all = pd.read_csv(os.path.join(path, \"sell_prices.csv\"))\n    prices=prices_all[prices_all.store_id==STORE]\n    return prices, prices_all\n","ba894a44":"%%time\n# load all data\nsales, sales_all = load_sales_data()\ncalendar, calendar_all = load_calendar_data()\nprices, prices_all = load_price_data()\n\n# data needed for evaluation process\nsample_submission = pd.read_csv(os.path.join(PATH_TO_FILE, \"sample_submission.csv\"))\nsales_copy = sales.copy()","810ff0e7":"class WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, \n                 calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 'all'  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')]\\\n                     .columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')]\\\n                               .columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], \n                                 axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id']\n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)\\\n                    [valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns]\\\n                    .set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index()\\\n                   .rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left',\n                                    on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd'])\\\n                    .unstack(level=2)['value']\\\n                    .loc[zip(self.train_df.item_id, self.train_df.store_id), :]\\\n                    .reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns],\n                               weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt) \n    \n    def score(self, valid_preds: Union[pd.DataFrame, \n                                       np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape \\\n               == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, \n                                       columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], \n                                 valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n\n            valid_preds_grp = valid_preds.groupby(group_id)[self.valid_target_columns].sum()\n            setattr(self, f'lv{i + 1}_valid_preds', valid_preds_grp)\n            \n            lv_scores = self.rmsse(valid_preds_grp, i + 1)\n            setattr(self, f'lv{i + 1}_scores', lv_scores)\n            \n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, \n                                  sort=False).prod(axis=1)\n            \n            all_scores.append(lv_scores.sum())\n            \n        self.all_scores = all_scores\n\n        return np.mean(all_scores)","37fb8f3b":"def prepare_store_prediction_for_WRMSSE_dashboard(pred, sales_all=sales_all): \n    # prepare template with ground truth according to validation step\n    sub = sales_all[['id']+[f'd_{d}' for d in range(pred.d.min(), pred.d.max()+1)]]\n    sub = sub[sub.id.str.endswith('_evaluation')].copy()\n    sub = sub.set_index(['id'])\n\n    # transfom prediction df for score evaluation\n    pred = pred.assign(id=pred.id, d_=\"d_\" + (pred.d).astype(\"str\"))\n    pred = pred.pivot(index='id', columns=\"d_\", values=\"sales\")\n\n    # update respective store with predicted values\n    sub.update(pred)\n\n    # reindex\n    new_index=sales_all[['id','item_id','store_id']].copy()\n    sub = sub.reset_index().merge(new_index, how='left', on=['id']).set_index(['item_id','store_id'])\n    sub = sub.drop(['id'], axis=1)\n\n    return sub","12e4ef34":"%%time\nWRMSSE_evaluators = {}\nfor CV_step in range(1,CV_STEPS+1):\n    print(f'Build WRMSSE_evaluator for CV step {CV_step} ...')\n    \n    # define last CV step: same 28 days as for prediction period but of the previous year\n    CV_start = FIRST_TRAIN_DAY\n    if CV_step == CV_STEPS:\n        CV_end = LAST_TRAIN_DAY - 364 + VALID_SET_LENGTH   \n    else:\n        CV_end = LAST_TRAIN_DAY - (CV_step-1)*VALID_SET_LENGTH \n        \n    print(f'validation start day: {CV_end-VALID_SET_LENGTH+1}')\n    print(f'validation end day: {CV_end}')\n    \n    # prepare train and valid data for current CV step\n    train_df = sales_all.drop([f'd_{i}' for i in range(1,CV_start)]\n                              +[f'd_{i}' for i in range(CV_end - VALID_SET_LENGTH +1,LAST_TRAIN_DAY+1)], axis=1)\n    #train_df = train_df.drop([f'd_{i}' for i in range(CV_train_end,LAST_TRAIN_DAY)], axis=1)\n    valid_df = sales_all[[f'd_{i}' for i in range(CV_end - VALID_SET_LENGTH +1,CV_end +1)]]\n    \n    # prepare calendar for current CV step\n    calendar_n = calendar_all.iloc[ CV_start-1 : CV_end, : ]\n\n    # prepare prices for current CV step\n    prices_start = calendar_all['wm_yr_wk'][CV_start-1]\n    prices_end = calendar_all['wm_yr_wk'][CV_end-1]\n    prices_n = prices_all.loc[(prices_all.wm_yr_wk>=prices_start) & (prices_all.wm_yr_wk<=prices_end), :]\n    \n    # inizialize WRMSSE_evaluator objects for current CV step\n    WRMSSE_evaluators[f'eval_CV_{CV_step}'] = WRMSSEEvaluator(train_df, valid_df, calendar_n, prices_n)","9438e0a4":"# visualization of WRMSSE\n\ndef create_viz_df(df,lv):\n    \n    df = df.T.reset_index()\n    if lv in [6,7,8,9,11,12]:\n        df.columns = [i[0] + '_' + i[1] if i != ('index','') \\\n                      else i[0] for i in df.columns]\n    df = df.merge(calendar_all.loc[:, ['d','date']], how='left', \n                  left_on='index', right_on='d')\n    df['date'] = pd.to_datetime(df.date)\n    df = df.set_index('date')\n    df = df.drop(['index', 'd'], axis=1)\n    \n    return df\n\ndef create_dashboard(CV_step):\n    \n    evaluator = WRMSSE_evaluators[f'eval_CV_{CV_step}']\n    wrmsses = [np.mean(evaluator.all_scores)] + evaluator.all_scores\n    labels = ['Overall'] + [f'Level {i}' for i in range(1, 13)]\n\n    \n        \n    # configuration array for the charts\n    n_rows = [1, 1, 4, 1, 3, 3, 3, 3, 3, 3, 3, 3]\n    n_cols = [1, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3]\n    width = [7, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14, 14]\n    height = [4, 3, 12, 3, 9, 9, 9, 9, 9, 9, 9, 9]\n    \n    for i in [3,8]:\n        \n        scores = getattr(evaluator, f'lv{i}_scores')\n        weights = getattr(evaluator, f'lv{i}_weight')\n        \n        if i > 1 and i < 9:\n            if i < 7:\n                fig, axs = plt.subplots(1, 2, figsize=(12, 3))\n            else:\n                fig, axs = plt.subplots(2, 1, figsize=(12, 8))\n                \n            ## RMSSE plot\n            scores.plot.bar(width=.8, ax=axs[0], color='g')\n            axs[0].set_title(f\"RMSSE\", size=14)\n            axs[0].set(xlabel='', ylabel='RMSSE')\n            if i >= 4:\n                axs[0].tick_params(labelsize=8)\n            for index, val in enumerate(scores):\n                axs[0].text(index*1, val+.01, round(val,4), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n            \n            ## Weight plot\n            weights.plot.bar(width=.8, ax=axs[1])\n            axs[1].set_title(f\"Weight\", size=14)\n            axs[1].set(xlabel='', ylabel='Weight')\n            if i >= 4:\n                axs[1].tick_params(labelsize=8)\n            for index, val in enumerate(weights):\n                axs[1].text(index*1, val+.01, round(val,2), color='black', \n                            ha=\"center\", fontsize=10 if i == 2 else 8)\n                    \n            fig.suptitle(f'Level {i}: {evaluator.group_ids[i-1]}', size=24 ,\n                         y=1.1, fontweight='bold')\n            plt.tight_layout()\n            plt.show()\n\n    # PLOT SALES\n    # actual data\n    sales_cum= data[(data.d<=data_val[f'CV_{CV_step}'].d.max())&(data.d>=data_val[f'CV_{CV_step}'].d.max()-60)].groupby('d')['sales'].sum()\n\n    # NN prediction \n    sales_cum_NN_pred = data_val[f'CV_{CV_step}'].groupby('d')['sales'].sum()\n\n    ##seasonal naive baseline\n    #shift=364\n    #sales_cum_baseline = data[(data.d<=data_val[f'CV_{CV_step}'].d.max()-shift)&(data.d>data_val[f'CV_{CV_step}'].d.max()-shift-VALID_SET_LENGTH)].groupby('d')['sales'].sum()\n    #sales_cum_baseline.index = sales_cum_baseline.index +shift\n\n    polt = sales_cum.plot(x ='d', y='sales', kind = 'line',figsize=(16,8), label='actual sales')\n    polt = sales_cum_NN_pred.plot(x ='d', y='sales', kind = 'line',figsize=(16,8), label='predicted sales: NN')\n    #polt = sales_cum_baseline.plot(x ='d', y='sales', kind = 'line',figsize=(16,8), label='predicted sales: seasonal naive')\n    plt.legend(loc=\"upper left\")\n    plt.title(f'All sales for store {STORE}')\n    plt.show()\n","7f142cd9":"sales.head()","ee6d8504":"sales.info()","bd4c1c8b":"def prepare_sales_data(df, first_train_day=FIRST_TRAIN_DAY):\n    \n    # select rows for selected store\n    df = df.drop(['state_id', 'store_id'], axis=1)\n    \n    # add day columns for test data - this way both train and test data are prepared together\n    last_train_day = int(df.columns[-1].replace('d_',''))\n    for day in range(last_train_day+1, last_train_day+ 28 +1):\n        df[f\"d_{day}\"] = np.nan\n    \n    # reshape sales days 'd_XXXX' from wide to long format.\n    if first_train_day != None:\n        df = df.drop([\"d_\" + str(i + 1) for i in range(first_train_day)], axis=1).copy()\n    id_vars = [col for col in df.columns if not col.startswith(\"d_\")]\n    df = df.melt(id_vars = id_vars, var_name = \"d\", value_name = \"sales\")\n    \n    # convert feature 'd' to integer\n    df = df.assign(d=df.d.str[2:].astype(\"int16\"))\n    \n    # reduce data size\n    df = reduce_memory_usage(df)\n    \n    return df\n","4706853b":"sales = prepare_sales_data(sales)\ngc.collect()\nsales.head()","1aacfc64":"                                                                             \ndef create_sales_features(df, params):\n    if params['activate']:\n        lag = params['lag']\n        print(f'shift_lag: {lag}')\n        if params['lag_feature'] == True:\n            df.loc[:,f'lag_l{lag}'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag))\n        if params['roll_mean_win'] != []:\n            for win in params['roll_mean_win']:\n                print(f'rolling_mean: {lag}\/{win}')                                                                       \n                df[f'rolling_mean_l{lag}_w{win}'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag).rolling(win).mean())\n        if params['roll_median_win'] != []:\n            for win in params['roll_median_win']:\n                print(f'rolling_median: {lag}\/{win}')                                                                       \n                df[f'rolling_median_l{lag}_w{win}'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag).rolling(win).median())\n        if params['roll_std_win'] != []:\n            for win in params['roll_std_win']:\n                print(f'rolling_std: {lag}\/{win}')                                                                       \n                df[f'rolling_std_l{lag}_w{win}'] = df.groupby(['id'])['sales'].transform(lambda x: x.shift(lag).rolling(win).std())\n    return df   ","0c00e41d":"# set parameters for sales features and activate for use\n\nparams_lag7 = {'activate': True,\n               'lag': 7,\n               'lag_feature':True,\n               'roll_mean_win':[7],\n               'roll_median_win':[7],\n               'roll_std_win':[7]}\n\nparams_lag14 = {'activate': False,\n               'lag': 14,\n               'lag_feature':True,\n               'roll_mean_win':[7],\n               'roll_median_win':[7],\n               'roll_std_win':[7]}\n\nparams_lag21 = {'activate': False,\n               'lag': 21,\n               'lag_feature':True,\n               'roll_mean_win':[7],\n               'roll_median_win':[7],\n               'roll_std_win':[7]}\n\nparams_lag28 = {'activate': True,\n               'lag': 28,\n               'lag_feature':True,\n               'roll_mean_win':[7,28],\n               'roll_median_win':[7,28],\n               'roll_std_win':[7,28]}\n\nparams_lag182 = {'activate': False,\n               'lag': 182,\n               'lag_feature':True,\n               'roll_mean_win':[28],\n               'roll_median_win':[],\n               'roll_std_win':[]}\n\nparams_lag364 = {'activate': False,\n               'lag': 364,\n               'lag_feature':True,\n               'roll_mean_win':[7,28],\n               'roll_median_win':[],\n               'roll_std_win':[]}\n\n\n# create features\nsales = create_sales_features(sales, params_lag7)\nsales = create_sales_features(sales, params_lag14)\nsales = create_sales_features(sales, params_lag21)\nsales = create_sales_features(sales, params_lag28)\nsales = create_sales_features(sales, params_lag182)\nsales = create_sales_features(sales, params_lag364)\n\n","d4242911":"def return_df_with_lag_and_win_per_column(df):\n    df_shift = pd.DataFrame({'lag+win': [0 for col in df.columns]}, index = df.columns)\n    for col in df.columns:\n        if '_l' in col:\n            pre = col.split('_l')[1]\n            if '_w' in pre:\n                lag = int(pre.split('_w')[0])\n                win = int(pre.split('_w')[1])             \n                total = lag + win\n            else: \n                lag = int(pre)\n                win = 0\n                total = lag\n            df_shift.loc[col, 'lag'] = lag\n            df_shift.loc[col, 'win'] = win\n            df_shift.loc[col, 'lag+win'] = total\n    return df_shift.loc[df_shift['lag+win']!=0]\n        ","febdc8e5":"# remove rows where NaNs originate from feature engineering\nmask = (return_df_with_lag_and_win_per_column(sales)['lag+win']).idxmax()\nsales = sales[(sales.d >= 1914) | (pd.notna(sales[mask]))].copy()\n\n","1cbeb05d":"sales.info()","2e053483":"calendar.head()","20aaa342":"calendar.info()","92313173":"def prepare_calendar_data(df):\n    #drop further useless features  \n    df = df.drop(['date', 'weekday'], axis=1)\n    \n    # convert feature 'd' to integer\n    df = df.assign(d = df.d.str[2:].astype(int))\n    \n    # substitute NaN in event_name and event_type by 'undefined'\n    df = df.fillna('undefined')\n    \n    # reduce data size\n    df = reduce_memory_usage(df)\n    \n    return df","dd7b6d08":"calendar = prepare_calendar_data(calendar)\ngc.collect()\ncalendar.head()","3a9b6496":"calendar.info()","018e1140":"prices.head()","2b22e8fe":"prices.info()","44ac477b":"def prepare_price_data(df):\n    #drop useless features  \n    df = df.drop(['store_id'], axis=1)\n    \n    # reduce data size\n    df = reduce_memory_usage(df)\n    \n    return df","d4fe3da2":"prices = prepare_price_data(prices)","bdaf067d":"prices.sell_price.isnull().value_counts()","97c5c805":"def create_price_features(df):\n    \n    # percentage change between the current and a prior element\n    df[\"sell_price_rel_diff\"] = df.groupby([\"item_id\"])[\"sell_price\"].pct_change()\n    \n    # rolling std of prices\n    df[\"sell_price_roll_sd7\"] = df.groupby([\"item_id\"])[\"sell_price\"].transform(lambda x: x.rolling(7).std())\n    \n    # relative cumulative price \n    grouped = df.groupby([\"item_id\"])[\"sell_price\"]\n    df[\"sell_price_cumrel\"] = (grouped.shift(0) - grouped.cummin()) \/ (1 + grouped.cummax() - grouped.cummin())\n    \n    # reduce data size\n    df = reduce_memory_usage(df)\n    \n    return df\n","9c73901c":"#create new price features\nprices = create_price_features(prices)","a0055c5e":"prices.info()","7c024a5a":"data = sales.merge(calendar, how=\"left\", on=\"d\")\ndata = data.merge(prices, how=\"left\", on=[\"wm_yr_wk\", \"item_id\"])\ndata.drop([\"wm_yr_wk\"], axis=1, inplace=True)\ndel sales, calendar, prices\ngc.collect()\ndata.head()","26667765":"cat_features = [\"item_id\", \"dept_id\", \"cat_id\", \n                'wday', 'month', 'year', \n                'event_name_1', 'event_type_1', 'event_name_2', 'event_type_2']\nbool_features = [f\"snap_{STORE.split('_')[0]}\"]\nnum_features = [e for e in data.columns if e not in cat_features + bool_features + ['d','sales','id']]\ndense_features = num_features + bool_features\n\n\n# encode numerical features \nfor i, feature in enumerate(num_features):\n    data[feature] = data[feature].fillna(data[feature].median())\n    #data[feature] = StandardScaler().fit_transform(data[[feature]])\n    \n\n# encode categorical features\nfor i, feature in enumerate(cat_features):\n    data[feature] = OrdinalEncoder(dtype=\"int\").fit_transform(data[[feature]])\n\n\ndata = reduce_memory_usage(data)\n\n","03e6afde":"def check_number_of_rows_with_NaNs(df, column='', last_train_day=LAST_TRAIN_DAY):\n    if column == '':\n        NaN_rows_all = len(df[(df.d<=last_train_day) & df.isna().any(axis=1)])\n        print('rows containing NaN values: '+ str(NaN_rows_all))\n    else:\n        NaN_rows_all = len(df[(df.d<=last_train_day) & df.isna().any(axis=1)])\n        NaN_rows_col = len(df[(df.d<=last_train_day) & df[column].isna()])\n        print('rows containing NaN values (column\/total): '+ str(NaN_rows_col)+'\/'+ str(NaN_rows_all))\n        \ncheck_number_of_rows_with_NaNs(data)","3a0b5510":"# further functions for data preparation which will be called individualy \n# before the respective training\/prediction run (see below)\n\ndef create_input_dictionary_for_model(df):\n    X = {\"dense_features\": df[dense_features].to_numpy()}\n    for i, feature in enumerate(cat_features):\n        X[feature] = df[[feature]].to_numpy()\n    return X\n\ndef prepare_data_for_current_validation_step(data, current_CV_step):\n    # determine column with max number of NaN caused by lag features\n    df_lag_win = return_df_with_lag_and_win_per_column(data)\n    max_NaN_rows = max(df_lag_win['lag+win'])\n        \n    # determin start and end days of training and validation data for current validation step\n    if current_CV_step == CV_STEPS: \n        first_day = LAST_TRAIN_DAY - max_NaN_rows - 364 -1\n        last_day = LAST_TRAIN_DAY + VALID_SET_LENGTH - 364\n        pred_start_day =  last_day - VALID_SET_LENGTH +1\n    else:\n        first_day = LAST_TRAIN_DAY - current_CV_step*VALID_SET_LENGTH - max_NaN_rows -1\n        last_day = LAST_TRAIN_DAY - (current_CV_step-1)*VALID_SET_LENGTH\n        pred_start_day =  last_day - VALID_SET_LENGTH +1\n   \n    print(f'Validation range: {pred_start_day} - {last_day}\\n')\n    print('Prepare data ...\\n')\n\n    # prepare training data\n    mask = data.d < pred_start_day\n    X_train = create_input_dictionary_for_model(data[mask])\n    y_train = data[\"sales\"][mask]\n    \n    # prepare validation data for model\n    mask = (data.d >= pred_start_day) & (data.d <= last_day) \n    X_valid = create_input_dictionary_for_model(data[mask])\n    y_valid = data[\"sales\"][mask]\n\n   # prepare validation data for realistic day-to-day prediction (to avoid data leakage)  \n    df_val = data[(data.d >= first_day) & (data.d <= last_day)].copy()\n    df_val.loc[df_val.d >= pred_start_day, 'sales']=0\n    for col, lag in df_lag_win.lag.items():   \n        df_val.loc[df_val.d > pred_start_day + lag, col]=0\n\n    return X_train, y_train, X_valid, y_valid, df_val","cb4b5a6a":"gc.collect()","bee4f4ae":"def create_model(loss=keras.losses.mean_squared_error,\n                 metrics=['mse'],\n                 optimizer=keras.optimizers.Nadam(learning_rate=0.0002)\n                ):\n    \n    tf.random.set_seed(42)\n    tf.keras.backend.clear_session()\n    gc.collect()\n\n    # Numerical features \n    dense_input = Input(shape=(len(dense_features), ), name='dense_features')\n\n    # Categorical embeddings\n    item_id_input = Input(shape=(1,), name='item_id')\n    item_id_emb = Flatten()(Embedding(len(data['item_id'].unique()), 3)(item_id_input))\n\n    dept_id_input = Input(shape=(1,), name='dept_id')\n    dept_id_emb = Flatten()(Embedding(len(data['dept_id'].unique()), 1)(dept_id_input))\n\n    cat_id_input = Input(shape=(1,), name='cat_id')\n    cat_id_emb = Flatten()(Embedding(len(data['cat_id'].unique()), 1)(cat_id_input))\n\n    wday_input = Input(shape=(1,), name='wday')\n    wday_emb = Flatten()(Embedding(len(data['wday'].unique()), 1)(wday_input))\n\n    month_input = Input(shape=(1,), name='month')\n    month_emb = Flatten()(Embedding(len(data['month'].unique()), 1)(month_input))\n\n    year_input = Input(shape=(1,), name='year')\n    year_emb = Flatten()(Embedding(len(data['year'].unique()), 1)(year_input))\n\n    event_name_1_input = Input(shape=(1,), name='event_name_1')\n    event_name_1_emb = Flatten()(Embedding(len(data['event_name_1'].unique()), 1)(event_name_1_input))\n\n    event_type_1_input = Input(shape=(1,), name='event_type_1')\n    event_type_1_emb = Flatten()(Embedding(len(data['event_type_1'].unique()), 1)(event_type_1_input))\n\n    event_name_2_input = Input(shape=(1,), name='event_name_2')\n    event_name_2_emb = Flatten()(Embedding(len(data['event_name_2'].unique()), 1)(event_name_2_input))\n\n    event_type_2_input = Input(shape=(1,), name='event_type_2')\n    event_type_2_emb = Flatten()(Embedding(len(data['event_type_2'].unique()), 1)(event_type_2_input))\n\n    inputs = [dense_input, wday_input, month_input,year_input, event_name_1_input, event_type_1_input,\n              event_name_2_input, event_type_2_input, item_id_input, dept_id_input, cat_id_input]\n    \n    # Combine numerical inputs and embedding \n    x = concatenate([dense_input, wday_emb, month_emb, year_emb, event_name_1_emb, event_type_1_emb, \n                     event_name_2_emb, event_type_2_emb, item_id_emb, dept_id_emb,cat_id_emb])\n\n    # Define dense layers \n    x = BatchNormalization()(x)\n    x = Dense(30, activation=\"elu\", kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = Dense(30, activation=\"elu\", kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = Dense(30, activation=\"elu\", kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    x = Dense(10, activation=\"elu\", kernel_initializer='he_normal')(x)\n    x = BatchNormalization()(x)\n    output = Dense(1, activation=\"linear\", name='output')(x)\n\n    # Connect input and output\n    model = Model(inputs, output)\n    \n    # compile model\n    model.compile(loss=loss, metrics=metrics, optimizer=optimizer)\n    \n    return model","122ce06f":"\ndef predict_validation_data_for_current_validation_step(data, current_CV_step):\n    \n    # prepare data\n    X_train, y_train, X_valid, y_valid, data_val = prepare_data_for_current_validation_step(data, current_CV_step)\n\n    \n    # compile model\n    print('Compile model ...\\n')\n    model = create_model(loss=keras.losses.mean_squared_error,\n                         metrics=['mse'],\n                         optimizer=keras.optimizers.Nadam(learning_rate=0.0002))\n\n    \n    # train model\n    print('Train model ...\\n')\n    history = model.fit(X_train, \n                        y_train,\n                        batch_size=10000,\n                        epochs=20,\n                        shuffle=True,\n                        validation_data=(X_valid,y_valid))\n\n\n    # day-by-day prediction\n    print('\\n Predict validation data ...\\n')\n    lag = 7 # min lag used for features\n    interval = 1\n\n    if current_CV_step == CV_STEPS: \n        start_day = LAST_TRAIN_DAY -364 +1\n    else:\n        start_day = LAST_TRAIN_DAY - current_CV_step*VALID_SET_LENGTH +1\n\n    for i in range(start_day, data_val.d.max() +1):\n        print(i)\n        if i== start_day + interval*lag:\n            data_val = create_sales_features(data_val, params_lag7)\n            interval+=1  \n        if i== start_day + 2*lag:\n            data_val = create_sales_features(data_val, params_lag14)\n        if i== start_day + 3*lag:\n            data_val = create_sales_features(data_val, params_lag21)\n\n        X = create_input_dictionary_for_model(data_val[data_val.d == i])\n        y_pred = model.predict(X, batch_size=10000)\n\n        data_val.loc[data_val.d == i, \"sales\"] = y_pred.clip(0)\n    \n    print('\\n')\n\n    # calculate store-WRMSSE for CV step\n    pred = prepare_store_prediction_for_WRMSSE_dashboard(data_val[data_val.d>=start_day]).values\n    W = WRMSSE_evaluators[f'eval_CV_{current_CV_step}'].score(pred) \n    print(f'store-WRMSSE for CV_{current_CV_step}: ' + str(W)+'\\n\\n\\n')\n    \n    # summarize history for accuracy\n    plt.plot(history.history['mse'], color='red')\n    plt.plot(history.history['val_mse'], color='green')\n    plt.plot(history.history['loss'], color='red')\n    plt.plot(history.history['val_loss'], color='green')\n    plt.title('model accuracy')\n    plt.ylabel('accuracy')\n    plt.xlabel('epochs')\n    plt.legend(['train accuracy\/loss', 'validation accuracy\/loss',], loc='best')\n    plt.show()\n       \n    return data_val[data_val.d>=start_day], [W]\n","c4924396":"%%time\ndata_val={}\nCV_scores=pd.DataFrame()\n\nfor CV_step in range(1,CV_STEPS+1):\n    print(f'VALIDATION STEP {CV_step}\\n')\n    \n    # training and prediction for CV step\n    data_val[f'CV_{CV_step}'], CV_scores[f'{STORE}_CV{CV_step}'] = predict_validation_data_for_current_validation_step(data,CV_step)\n","d2f97a27":"# show store-WRMSSE of the different CV steps (store-WRMSSE is the contribution of the respective store to the total\/final WRMSSE)\nCV_scores[f'{STORE}_CV_mean'] = CV_scores.loc[0,:].mean()\nCV_scores.to_csv(f\"wrmsse_{STORE}.csv\", index=True)\nCV_scores","82405226":"# select CV step that you want to display\nCV_step = 2\ncreate_dashboard(CV_step)","ed6f3bca":"[back to Table of Contents](#TOC)\n## 7. Conclusions <a id=\"Conclusions\"><\/a>","4044a347":"<a id=\"TOC\"><\/a>\n## Table of contents\n* **1. [Set global parameters](#global)** <br>\n* **2. [Import libraries and data](#Import)** <br>\n* **3. [Initialize WRMSSE](#WRMSSE)** <br>\n* **4. [Data analysis and feature engineering](#Ana)** <br>\n    * 4.1 [Sales](#Ana1) <br>\n    * 4.2 [Calendar](#Ana2) <br>\n    * 4.3 [Prices](#Ana3) <br>\n* **5. [Data preparation for modeling](#Prep)** <br>\n    * 5.1 [Merging Sales, Calendar, and Prices data sets](#Prep1) <br>\n    * 5.2 [Encoding](#Prep2) <br>   \n    * 5.3 [Perpare data for CV and final submission](#Prep3) <br>\n* **6. [Modeling](#Model)** <br>\n    * 5.1 [The Model](#Model1) <br>\n    * 5.2 [Cross-validation: training and prediction](#Model2) <br>\n* **7. [Conclusions](#Conclusions)** <br>\n\n","34e02357":"### 5.2 Encoding <a id='Prep2'><\/a>","3ab15669":"### 4.3 Price data <a id=\"Ana3\"><\/a>","d6c83a38":"In order to predict the whole data set (all stores), you can optimize the model for each store in an individual notebook and combine the predictions in one submission file (not implemented here). Making predictions on store level and running the different notebooks in parallel allows a quick optimization process and solves the problem with RAM limitation here on kaggle. Note that for this version neither the features nor the model have been opimized. For further ideas for FE see the references below in the comments.\n","585e76bd":"### 4.2 Calendar data <a id=\"Ana2\"><\/a>","e3779de1":"[back to Table of Contents](#TOC)\n## 4. Data analysis and feature engineering <a id=\"Ana\"><\/a>","a12cb0e1":"[back to Table of Contents](#TOC)\n## 5. Data preparation for modeling <a id=\"Prep\"><\/a>","92192429":"### 5.1 Merge Sales, calendar and prices data sets <a id='Prep1'><\/a>","b217d474":"Credit mainly goes to https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834 and https:\/\/www.kaggle.com\/tnmasui\/m5-wrmsse-evaluation-dashboard","a016e7b2":"### 6.2 Cross-validatoion: Training and prediction <a id='Model2'><\/a>","1f7bdf60":"In the following cell you can set the parameters for the sales features: \nSelect **'activate': True** to use the respective lag feature and fill the list with whatever windows you want to use. If the list is empty, it will not be used.\nThe code below allows to easily try a lot of different lag features and always be sure that the missing values are calculated in the day-to-day prediction for lags smaller than 28 days (more infos on this issue [here](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/141515))","e508cc3b":"[back to Table of Contents](#TOC)\n## 3. Calculate weights for WRMSSE <a id=\"WRMSSE\"><\/a>","94ebeeb9":"[back to Table of Contents](#TOC)\n## 2. Import libraries and data <a id=\"Import\"><\/a>","66d9da78":"### 5.3 Prepare data for cross-validation and final submission <a id='Prep3'><\/a>","c79e7a7e":"## 1. Set global parameters <a id=\"global\"><\/a>\n\n","e6bf04a1":"### 4.1 Sales data <a id=\"Ana1\"><\/a>","c5c30f42":"### 6.1 The Model <a id='Model1'><\/a>","f824d989":"[back to Table of Contents](#TOC)\n## 6. Modeling <a id='Model'><\/a>","7d122c82":"This notebook shows how to use a **neural network with categorical embeddings** and **day-to-day prediction** to forecast the sales on **store level**. To optimize the model I implemented a **3-fold cross-validation** taking into account the previous two month and the same month of the previous year as validation periods. I implemented a **store-WRMSSE** as validation metric, which is simply the error which the respective store contributes to the overall WRMSSE (regarding all data). Note that for this version the features and the model have not yet been opimized, so there is plenty of room for improvement.\n\nThe same code using a **light gradient bossting model** instead of a neural network can be found [here](https:\/\/www.kaggle.com\/dantefilu\/lgbm-on-store-level-with-3-fold-cv-store-wrmsse).\n\nHere are some very useful notebooks and discussions that inspired me or where I borrowed some code. Thanks a lot!\n\n* https:\/\/www.kaggle.com\/headsortails\/back-to-predict-the-future-interactive-m5-eda by @headsortails\n* https:\/\/www.kaggle.com\/nxrprime\/preprocessing-fe by @nxrprime\n* https:\/\/www.kaggle.com\/ragnar123\/very-fst-model by @ragnar123\n* https:\/\/www.kaggle.com\/kneroma\/m5-forecast-v2-python by @kneroma\n* https:\/\/www.kaggle.com\/tnmasui\/m5-wrmsse-evaluation-dashboard by @tnmasui\n* https:\/\/www.kaggle.com\/mayer79\/m5-forecast-keras-with-categorical-embeddings-v2 by @mayer79\n* https:\/\/www.kaggle.com\/nxrprime\/transformer-xl-intro-baseline by @nxrprime\n\n* https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/141515 by @amedprof\n* https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/142129 by @kneroma\n* https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/143077 by @hengck23\n* https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/138881 by @kyakovlev\n* https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/144067 by @kyakovlev\n"}}