{"cell_type":{"8c0ff163":"code","df8c6326":"code","772e7e1d":"code","9a8ec7a1":"code","76096630":"code","d689e07f":"code","6f60b59a":"code","087afb32":"code","b6238c36":"code","26b8f8f4":"code","eced349c":"code","d655526a":"code","c97d5120":"code","157616e2":"code","76897a1a":"code","2db0762b":"code","25de794a":"code","cfe751f8":"code","81e7776a":"code","753ebf8a":"code","3bf4ff89":"code","08b89f90":"code","26d2ba49":"code","5c155961":"code","a7565078":"code","d03c01ad":"code","8d27dcff":"code","7e01a992":"code","fa877465":"code","e346bc22":"code","2c7f2c1e":"code","2d9bc51c":"code","08b093d3":"code","ccad0f74":"code","fa42dcd5":"code","7aa8c932":"code","99b3d3ac":"code","319505c7":"code","0625971a":"code","331d517e":"code","b76a16a4":"code","540015d9":"code","2a004842":"code","a896b0f1":"code","3101b011":"code","305eb4d4":"code","7967891e":"code","f8aa55b2":"code","388d00b2":"code","0e70d995":"code","fc0df89e":"code","11c2f879":"code","07475f06":"code","9b972937":"code","41f54563":"code","6dba4095":"code","02c03e76":"code","5016161e":"code","a52f82fe":"code","1b5dc4b2":"code","de7b03c9":"code","df01fb93":"code","e2e8d1ab":"code","e0b86f6b":"code","1d646c83":"code","b1b86bfc":"code","9238c6c7":"code","c977ac50":"code","179eaa26":"markdown","c7235061":"markdown","b50fb0f1":"markdown","e9a449b4":"markdown","92979182":"markdown","9bed81cb":"markdown","05fece10":"markdown","9e046ca0":"markdown","b2420740":"markdown","12fd7431":"markdown","8f77b148":"markdown","95489369":"markdown","5af0f537":"markdown","574cd490":"markdown","1baad5e0":"markdown","7062c4f7":"markdown","e42a4307":"markdown","145497d6":"markdown","5fb78801":"markdown","6772ae19":"markdown","dafebf8f":"markdown","0ca9a489":"markdown","48d012aa":"markdown","aecc22bf":"markdown","7981b4e4":"markdown"},"source":{"8c0ff163":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","df8c6326":"from sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler \n\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV\n\nfrom xgboost.sklearn import XGBRegressor","772e7e1d":"from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\nfrom sklearn.ensemble import RandomForestRegressor","9a8ec7a1":"import time\nimport gc\nfrom scipy.stats import uniform\nimport calendar","76096630":"from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","d689e07f":"pd.set_option('display.max_columns', 100)","6f60b59a":"train = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/train.csv\")\ntrain.head()","087afb32":"store = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/store.csv\")\nstore.head()","b6238c36":"train.isnull().sum()","26b8f8f4":"store.isnull().sum()","eced349c":"#store.shape\n#store.dropna(inplace=True)\n#store.shape","d655526a":"store['StoreType'].value_counts()\nstore['Assortment'].value_counts()","c97d5120":"store['StoreType']= store['StoreType'].map({'a':1, 'b' : 2, 'c': 3, 'd' : 4})\nstore['Assortment'] = store['Assortment'].map({'a':1, 'b' : 2, 'c': 3})","157616e2":"store.head()","76897a1a":"data = pd.merge(train, store,on = 'Store', how='left')\ndata.head()","2db0762b":"data.shape","25de794a":"data['StateHoliday'].value_counts()","cfe751f8":"#Tried with reducing the dataset by taking only the records have Sales > 0.\n#still system is getting longer time for procesing the model\n\n#data = data[data['Sales'] > 0]\n\ndata.dropna(inplace = True)","81e7776a":"data.shape","753ebf8a":"# credits to kaggle link on specifying how to handle date and month values\n# https:\/\/www.kaggle.com\/rohinigarg\/random-forest-and-xgboost-parameter-tuning\n\ndef checkpromomonth(row):\n if (row['MonthName'] in row['PromoInterval']):\n    return 1\n else:\n    return 0\n","3bf4ff89":"def ProcessData(data):\n    data[\"CompetitionDistance\"].fillna(data[\"CompetitionDistance\"].mean(), inplace = True)\n    \n    data['StateHoliday']= data['StateHoliday'].map({'0':0, 0: 0,'a':1, 'b' : 2, 'c': 3})\n    \n    data['Date']=pd.to_datetime(data['Date'])\n    data['Year']=data['Date'].dt.year\n    data['MonthNumber']=data['Date'].dt.month\n    data['MonthName']=data['MonthNumber'].apply(lambda x: calendar.month_abbr[x])\n    data['Day']=data['Date'].dt.day\n    data['WeekNumber']=data['Date'].dt.weekofyear\n\n    data['CompetitionOpen'] = 12 * (data['Year'] - data['CompetitionOpenSinceYear']) + (data['MonthNumber'] - data['CompetitionOpenSinceMonth'])\n    data['CompetitionOpen'] = data['CompetitionOpen'].apply(lambda x: x if x > 0 else 0)\n\n    data['Promo2Open'] = 12 * (data['Year'] - data['Promo2SinceYear']) + (data['WeekNumber'] - data['Promo2SinceWeek']) \/ float(4)\n    data['Promo2Open'] = data['Promo2Open'].apply(lambda x: x if x > 0 else 0)\n\n    data['PromoInterval']=data['PromoInterval'].astype(str)\n    \n    data['IsPromoMonth'] =  data.apply(lambda row: checkpromomonth(row),axis=1)\n\n    data.drop(['CompetitionOpenSinceMonth', 'CompetitionOpenSinceYear'], axis = 1,  inplace = True)\n    data.drop(['Promo2SinceYear', 'Promo2SinceWeek'], axis = 1,  inplace = True)\n    data.drop(['Date', 'MonthName','PromoInterval'], axis = 1,  inplace = True)","08b89f90":"ProcessData(data)","26d2ba49":"data.head()","5c155961":"data.isnull().sum()\ndata.shape","a7565078":"data.min().min()           \ndata.max().max() ","d03c01ad":"data = data.astype('int32')","8d27dcff":"data.info()","7e01a992":"y = data['Sales']\ndata.drop(['Sales','Customers'], axis = 1,  inplace = True)","fa877465":"data.nunique()","e346bc22":"num_columns = data.columns[data.nunique() > 12]\ncat_columns = data.columns[data.nunique() <= 12]\nnum_columns\ncat_columns","2c7f2c1e":"plt.figure(figsize=(15,10))\nsns.distributions._has_statsmodels=False\nfor i in range(len(num_columns)):\n    plt.subplot(2,3,i+1)\n    sns.distplot(data[num_columns[i]])\n    \nplt.tight_layout()","2d9bc51c":"ct=ColumnTransformer([\n    ('rs',RobustScaler(),num_columns),\n    ('ohe',OneHotEncoder(),cat_columns),\n    ],\n    remainder=\"passthrough\"\n    )\nct.fit_transform(data)","08b093d3":"X=data","ccad0f74":"X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.30)","fa42dcd5":"X_train.shape\nX_test.shape\ny_train.shape\ny_test.shape","7aa8c932":"steps_xg = [('sts', StandardScaler() ),\n            ('pca', PCA()),\n            ('xg',  XGBRegressor(objective='reg:squarederror',silent = False, n_jobs=3, reg_lambda=1,gamma=0))\n            ]\n\npipe_xg = Pipeline(steps_xg)\n\npipe_xg.get_params()","99b3d3ac":"#credit : https:\/\/www.kaggle.com\/tushartilwankar\/sklearn-rf\ndef ToWeight(y):\n    w = np.zeros(y.shape, dtype=float)\n    ind = y != 0\n    w[ind] = 1.\/(y[ind]**2)\n    return w\n\ndef RMSPE(y, yhat):\n    w = ToWeight(y)\n    rmspe = np.sqrt(np.mean( w * (y - yhat)**2 ))\n    return rmspe ","319505c7":"#from sklearn.metrics import make_scorer, r2_score, mean_squared_error","0625971a":"#Randomized Search\nparameters = {'xg__learning_rate':  uniform(0, 1),\n              'xg__n_estimators':   range(50,300),\n              'xg__max_depth':      range(3,10),\n              'pca__n_components' : range(10,17)}\n\nrs = RandomizedSearchCV(pipe_xg,\n                        param_distributions=parameters,\n                        #scoring=make_scorer(mean_squared_error, squared=False),\n                        #scoring= RMSPE,\n                        n_iter=15,    \n                        verbose = 1,\n                        #refit = RMSPE,\n                        n_jobs = 3,\n                        cv = 3              \n                        )","331d517e":"start = time.time()\nrs.fit(X_train, y_train)\nend = time.time()\n(end - start)\/60 ","b76a16a4":"rs.best_estimator_.named_steps[\"xg\"].feature_importances_\nrs.best_estimator_.named_steps[\"xg\"].feature_importances_.shape","540015d9":"# Model with parameters of random search\nmodel_rs = XGBRegressor(objective='reg:squarederror',silent = False, n_jobs=3, reg_lambda=1,gamma=0,\n                    learning_rate = rs.best_params_['xg__learning_rate'],\n                    max_depth = rs.best_params_['xg__max_depth'],\n                    n_estimators=rs.best_params_['xg__max_depth']\n                    )\n\n\nmodel_rs.fit(X_train, y_train)","2a004842":"y_pred_rs = model_rs.predict(X_test)","a896b0f1":"RMSPE(y_test,y_pred_rs)\n\nrs.best_score_","3101b011":"import math","305eb4d4":"accuracy_rs =  math.sqrt(sum((y_test - y_pred_rs)**2)\/y_test.count())\nprint(\"Accuracy with Random search XGB model:\",accuracy_rs*100)","7967891e":"X_test_df = X_test.reset_index()\ny_test_df = y_test.reset_index()\ny_pred_df  = pd.DataFrame(y_pred_rs)\n\nfinal = X_test_df\n#final\nfinal = final.merge(y_test_df, left_index=True, right_index=True)\nfinal = final.merge(y_pred_df, left_index=True, right_index=True)\nfinal","f8aa55b2":"test = pd.read_csv(\"\/kaggle\/input\/rossmann-store-sales\/test.csv\")\ntest.head()\n\ntest.isnull().sum()","388d00b2":"test.shape","0e70d995":"test.Open.fillna(0, inplace= True)","fc0df89e":"test.isnull().sum()","11c2f879":"store.head()","07475f06":"data = pd.merge(test, store,on = 'Store', how='left')\ndata.head()\n\ndata.shape","9b972937":"ProcessData(data)\n\ndata.head()","41f54563":"submission = data['Id']\ndata=data.drop('Id',axis=1)","6dba4095":"data.head()","02c03e76":"data.info()","5016161e":"data.min().min()           \ndata.max().max()","a52f82fe":"data = data.astype('int32')","1b5dc4b2":"data.nunique() ","de7b03c9":"num_columns = data.columns[data.nunique() > 12]\ncat_columns = data.columns[data.nunique() <= 12]","df01fb93":"num_columns\ncat_columns","e2e8d1ab":"plt.figure(figsize=(15,10))\nsns.distributions._has_statsmodels=False\nfor i in range(len(num_columns)):\n    plt.subplot(2,3,i+1)\n    graph = sns.distplot(data[num_columns[i]])\n    \nplt.tight_layout()\n\n","e0b86f6b":"ct=ColumnTransformer([\n    ('rs',RobustScaler(),num_columns),\n    ('ohe',OneHotEncoder(),cat_columns),\n    ],\n    remainder=\"passthrough\"\n    )\nct.fit_transform(data)\n","1d646c83":"y_pred_rs = model_rs.predict(data)","b1b86bfc":"y_pred_rs","9238c6c7":"final = submission.reset_index()\ny_pred_df  = pd.DataFrame(y_pred_rs)\n\nfinal = final.merge(y_pred_df, left_index=True, right_index=True)\nfinal","c977ac50":"final.to_csv('submission.csv')","179eaa26":"drop both sales and customer column in train data set since in test data set sales and customer fields are not there. before that use sales as target variable from train data set as we will be predicting sales in test data set","c7235061":"Replace numerical values for the columns which have labels","b50fb0f1":"Based on the graph, we will use RobustScaler for the numerical column","e9a449b4":"Check the data set for any null values and correct (fill with some values or drop them)","92979182":"Since the data size is more (almost 10 lakhs) and system is hanging if I run fulle data set, I am trying to minimize the data set before processing.","9bed81cb":"We will process the dataset for changing the date field to numerical fields and StateHoliday field as well","05fece10":"Now we use the organized data set for regression problem","9e046ca0":"## Rossmann Store Data Analysis and Sales Prediction\n### Created by : Ezhilarasan \nTo analyse Rossmann Store Data and prodict the future Sales using XGB Regressor along with Parameter Tuning (using Randomized Search)\n","b2420740":"We will consider 0 where the Store field has null value","12fd7431":"StateHoliday field has label values. we will convert the values to numeric group","8f77b148":"Creating the Model with the best parameters from RandomSearch Results","95489369":"We will load test dat given in the kaggle merge with store data, process it and predict the sales in our randomized search model","5af0f537":"Import necessary libraries","574cd490":"Read given datasets (train & store)","1baad5e0":"Conclusion : We could predict the Sales using XGB Regressor (upon parameter tuning using Random Search)","7062c4f7":"Below function is for evluating the RMSPE (Root Mean Square Percentage Error).","e42a4307":"We will check minimum and maximum values of the data set and try to reduce the memory size","145497d6":"Merge train and store data","5fb78801":"Date column has to be converted to numerical fields","6772ae19":"We will use the scaled data split it internally to train and test, create our model and predict the test data (before going to predict the Sales for the actual test data given in the kaggle link)","dafebf8f":"We will check the predicted sales values from randomized search with target data as below","0ca9a489":"Predict our test data in the final model created","48d012aa":"Based on the above table, last 2 columns (Actual Sales & Predicted Sales) are relatively matching.","aecc22bf":"Pipeline creation for XGB Regressor. ","7981b4e4":"We will use Random Search for tuning the pipeline parameters"}}