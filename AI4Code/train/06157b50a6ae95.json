{"cell_type":{"eb787557":"code","bddf3e62":"code","2cfb850b":"code","acc73149":"code","ca68fb7f":"code","4027be6c":"code","a0b82dac":"code","8256bb27":"code","4e93e023":"code","9becb3bc":"code","7c0d6577":"code","7c5f46dd":"code","d2a88ed4":"code","3c4eb43d":"code","ddff0144":"code","a746bfcd":"code","09d344ed":"code","ad25c0ab":"code","9e23f98d":"code","6f64c192":"code","eae21837":"code","ff83e850":"code","93d04bac":"code","9497ac12":"code","b704aecd":"code","0a2b8ece":"code","d96d6e33":"code","93490d58":"code","08018ae1":"code","55099931":"code","94cd497f":"code","1c995ce9":"code","9d8d115c":"code","119069f0":"code","d21e154a":"code","075c56c9":"code","1d411972":"code","814b2b42":"code","fad09b07":"code","ed306923":"code","ed6afbd9":"code","96008abd":"code","56e3eb30":"code","cff7c8f5":"code","b45c98de":"code","4e27e874":"code","b3c471b4":"code","2f502379":"markdown","8c7097eb":"markdown","e6c3bd0e":"markdown","4dcf88a7":"markdown","209e3f01":"markdown","5e27e520":"markdown","882be0d5":"markdown","419554d6":"markdown","02516c61":"markdown","51af50c7":"markdown","b6df98c2":"markdown","8e70c8b4":"markdown","d49953b5":"markdown","8cb41de1":"markdown","d27662d9":"markdown","beeb2bdc":"markdown","f2df0ca2":"markdown","12b38a64":"markdown","bde02e87":"markdown","241db0aa":"markdown","278e2688":"markdown","9b7bbd8e":"markdown","5c22b645":"markdown","0a8e0031":"markdown","8597a3d2":"markdown","5056b1d8":"markdown","193e3c53":"markdown","b99b3a5d":"markdown","1d93edd8":"markdown","14791882":"markdown","41eb2add":"markdown","78e65b73":"markdown","1ae0d6c8":"markdown","c92c767f":"markdown","1eb3704f":"markdown","10328fef":"markdown","d2212def":"markdown","bc6f836e":"markdown","2fc79f8d":"markdown","23bb8f44":"markdown"},"source":{"eb787557":"import matplotlib.pyplot as plt \nimport PIL\nimport tensorflow as tf\nimport os\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\n\nimport pandas as pd\nimport requests # to get image from the web\nimport shutil # to save it locally\nimport time\nimport numpy as np","bddf3e62":"def getFileNameFromUrl(url):\n  firstpos=url.rindex(\"\/\")\n  lastpos=len(url)\n  filename=url[firstpos+1:lastpos]\n  print(f\"url={url} firstpos={firstpos} lastpos={lastpos} filename={filename}\")\n  return filename\n\n\ndef downloadImage(imageUrl, destinationFolder):\n  filename = getFileNameFromUrl(imageUrl)\n  # Open the url image, set stream to True, this will return the stream content.\n  r = requests.get(imageUrl, stream = True)\n\n  # Check if the image was retrieved successfully\n  if r.status_code == 200:\n      # Set decode_content value to True, otherwise the downloaded image file's size will be zero.\n      r.raw.decode_content = True\n      \n      # Open a local file with wb ( write binary ) permission.\n      filePath = os.path.join(destinationFolder, filename)\n      if not os.path.exists(filePath):\n        with open(filePath,'wb') as f:\n            shutil.copyfileobj(r.raw, f)\n        print('Image sucessfully Downloaded: ',filename)\n        print(\"Sleeping for 1 seconds before attempting next download\")\n        time.sleep(1)\n      else:\n        print(f'Skipping image {filename} as it is already Downloaded: ')\n\n  else:\n      print(f'Image url={imageUrl} and filename={filename} Couldn\\'t be retreived. HTTP Status={r.status_code}')\n\n\n","2cfb850b":"df = pd.read_csv(\"..\/input\/nativity-paintings-dataset\/nativity_dataset.csv\")\n\n# create directory to which we download if it doesn't exist\ndestinationFolder = \".\/dataset\/nativity\"\nos.makedirs(destinationFolder, exist_ok=True)\n\nfor i, row in df.iterrows():\n  print(f\"Index: {i}\")\n  print(f\"{row['Image URL']}\\n\")\n\n  downloadImage(row[\"Image URL\"], destinationFolder)\n  ","acc73149":"df = pd.read_csv(\"..\/input\/nativity-paintings-dataset\/other_dataset.csv\")\n\n# create directory to which we download if it doesn't exist\ndestinationFolder = \".\/dataset\/others\"\nos.makedirs(destinationFolder, exist_ok=True)\n\nfor i, row in df.iterrows():\n  print(f\"Index: {i}\")\n  print(f\"{row['Image URL']}\\n\")\n\n  downloadImage(row[\"Image URL\"], destinationFolder)\n  ","ca68fb7f":"df = pd.read_csv(\"..\/input\/nativity-paintings-dataset\/test_dataset.csv\")\n\n# create directory to which we download if it doesn't exist\ndestinationFolder = \".\/test_dataset\/\"\nos.makedirs(destinationFolder, exist_ok=True)\n\nfor i, row in df.iterrows():\n  labelName = row[\"Label\"]\n  print(f\"Index: {i}\")\n  print(f\"{row['Image URL']}\\n\")\n  destinationFolderLabel= os.path.join(destinationFolder, labelName)\n  os.makedirs(destinationFolderLabel, exist_ok=True)\n  downloadImage(row[\"Image URL\"], destinationFolderLabel)\n  ","4027be6c":"!apt install -y imagemagick","a0b82dac":"def resizeImages(sourceFolder, destinationFolder, maxPixels=1048576):\n  os.makedirs(destinationFolder, exist_ok=True)\n  for path, subdirs, files in os.walk(sourceFolder):\n      relativeDir=path.replace(sourceFolder, \"\")\n      destinationFolderPath = destinationFolder + relativeDir\n      os.makedirs(destinationFolderPath,exist_ok=True)\n      for fileName in files:\n          sourceFilepath=os.path.join(path,fileName)\n          destinationFilepath=os.path.join(destinationFolderPath, fileName)\n          print(f\"sourceFilepath={sourceFilepath} destinationFilepath={destinationFilepath}\")\n          os.system(f\"convert {sourceFilepath} -resize {maxPixels}@\\> {destinationFilepath}\")","8256bb27":"# resize training images\nsourceFolder=\".\/dataset\"\ndestinationFolder = \".\/resized\/dataset\"\nresizeImages(sourceFolder, destinationFolder, maxPixels=90000)\n\n# resize testing images\nsourceFolder=\".\/test_dataset\"\ndestinationFolder = \".\/resized\/test_dataset\"\nresizeImages(sourceFolder, destinationFolder, maxPixels=90000)\n\n","4e93e023":"!mv .\/resized\/dataset\/nativity .\/resized\/dataset\/1\n!mv .\/resized\/dataset\/others .\/resized\/dataset\/0\n\n!mv .\/resized\/test_dataset\/nativity .\/resized\/test_dataset\/1\n!mv .\/resized\/test_dataset\/others .\/resized\/test_dataset\/0\n","9becb3bc":"import pathlib\ndata_dir = pathlib.Path(\".\/resized\/dataset\")\ntest_data_dir = pathlib.Path(\".\/resized\/test_dataset\")\n\nimage_count = len(list(data_dir.glob('*\/*')))\nprint(image_count)","7c0d6577":"nativity_label=\"1\"\nnativity = list(data_dir.glob(f'{nativity_label}\/*'))\n\nPIL.Image.open(str(nativity[0]))","7c5f46dd":"PIL.Image.open(str(nativity[1]))","d2a88ed4":"others_label=\"0\"\nothers = list(data_dir.glob(f'{others_label}\/*'))\nPIL.Image.open(str(others[1]))","3c4eb43d":"PIL.Image.open(str(others[2]))","ddff0144":"batch_size = 32\nimg_height = 300\nimg_width = 300\n","a746bfcd":"train_ds = tf.keras.preprocessing.image_dataset_from_directory(data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size, label_mode='binary')\n\n","09d344ed":"val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size,label_mode='binary')","ad25c0ab":"#Retrieve a batch of images from the test set\ntest_data_dir = pathlib.Path(\".\/resized\/test_dataset\")\ntest_batch_size=37\ntest_ds = tf.keras.preprocessing.image_dataset_from_directory(\n  test_data_dir,\n  seed=200,\n  image_size=(img_height, img_width),\n  batch_size=test_batch_size,label_mode='binary')","9e23f98d":"class_names = train_ds.class_names\nprint(class_names)","6f64c192":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    if labels[i] == 1.0:\n      title = \"Nativity\"\n    else:\n      title = \"Others\"\n\n    plt.title(title)\n    plt.axis(\"off\")","eae21837":"for image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","ff83e850":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n","93d04bac":"labelMappings={\"0\":\"Others\",\"1\":\"Nativity\",\n               0.0:\"Others\",1.0 :\"Nativity\"}\n\ndef predictWithTestDataset(model):\n  image_batch, label_batch = test_ds.as_numpy_iterator().next()\n  predictions = model.predict_on_batch(image_batch).flatten()\n  print(f\"After sigmoid:{predictions}\")\n  predictions = tf.where(predictions < 0.5, 0, 1)\n\n  #print('Predictions:\\n', predictions.numpy())\n  #print('Labels:\\n', label_batch)\n  correctPredictions=0\n  plt.figure(figsize=(20, 20))\n  print(f\"number predictions={len(predictions)}\")\n  for i in range(len(predictions)):\n    ax = plt.subplot(8, 5, i +1)\n    plt.imshow(image_batch[i].astype(\"uint8\"))\n    prediction = class_names[predictions[i]]\n    predictionLabel = labelMappings[prediction]\n    gtLabel = labelMappings[label_batch[i][0]]\n    if gtLabel == predictionLabel:\n      correctPredictions += 1\n    plt.title(f\"P={predictionLabel} GT={gtLabel}\")\n    plt.axis(\"off\")\n  \n  accuracy = correctPredictions\/len(predictions)\n  print(f\"Accuracy:{accuracy}\")","9497ac12":"normalization_layer = layers.experimental.preprocessing.Rescaling(1.\/255)","b704aecd":"normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nimage_batch, labels_batch = next(iter(normalized_ds))\nfirst_image = image_batch[0]\n# Notice the pixels values are now in `[0,1]`.\nprint(np.min(first_image), np.max(first_image))","0a2b8ece":"\n\nmodel = Sequential([\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(1, activation='sigmoid')\n])\n","d96d6e33":"model.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(from_logits=True), metrics=[keras.metrics.BinaryAccuracy()])\n","93490d58":"model.summary()","08018ae1":"epochs=10\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","55099931":"print(history.history)\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\n# acc = history.history['accuracy']\n# val_acc = history.history['val_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","94cd497f":"predictWithTestDataset(model)","1c995ce9":"data_augmentation = keras.Sequential(\n  [\n    layers.experimental.preprocessing.RandomFlip(\"horizontal\", \n                                                 input_shape=(img_height, \n                                                              img_width,\n                                                              3)),\n    layers.experimental.preprocessing.RandomRotation(0.1) ,\n   #layers.experimental.preprocessing.RandomZoom(0.1)\n   \n    ]\n\n\n)\n","9d8d115c":"plt.figure(figsize=(10, 10))\nfor images, _ in train_ds.take(1):\n  for i in range(9):\n    augmented_images = data_augmentation(images)\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n    plt.axis(\"off\")","119069f0":"\nmodel = Sequential([\n  data_augmentation,\n  layers.experimental.preprocessing.Rescaling(1.\/255, input_shape=(img_height, img_width, 3)),\n  layers.Conv2D(16, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(32, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Conv2D(64, 3, padding='same', activation='relu'),\n  layers.MaxPooling2D(),\n  layers.Dropout(0.2),\n  layers.Flatten(),\n  layers.Dense(128, activation='relu'),\n  layers.Dense(1, activation='sigmoid')\n])\n","d21e154a":"from tensorflow import optimizers\nmodel.compile(loss=keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer=optimizers.RMSprop(lr=1e-4),\n                  metrics=[keras.metrics.BinaryAccuracy()])","075c56c9":"model.summary()","1d411972":"epochs = 25\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","814b2b42":"print(history.history)\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","fad09b07":"predictWithTestDataset(model)","ed306923":"base_model = keras.applications.Xception(\n    weights=\"imagenet\",  # Load weights pre-trained on ImageNet.\n    input_shape=(img_height, img_width, 3),\n    include_top=False,\n)  # Do not include the ImageNet classifier at the top.\n\n# Freeze the base_model\nbase_model.trainable = False\n\n# Create new model on top\ninputs = keras.Input(shape=(img_height, img_width, 3))\nx = data_augmentation(inputs)  # Apply random data augmentation\n\n# Pre-trained Xception weights requires that input be normalized\n# from (0, 255) to a range (-1., +1.), the normalization layer\n# does the following, outputs = (inputs - mean) \/ sqrt(var)\nnorm_layer = keras.layers.experimental.preprocessing.Normalization()\nmean = np.array([127.5] * 3)\nvar = mean ** 2\n# Scale inputs to [-1, +1]\nx = norm_layer(x)\nnorm_layer.set_weights([mean, var])\n\n# The base model contains batchnorm layers. We want to keep them in inference mode\n# when we unfreeze the base model for fine-tuning, so we make sure that the\n# base_model is running in inference mode here.\nx = base_model(x, training=False)\nx = keras.layers.GlobalAveragePooling2D()(x)\nx = keras.layers.Dropout(0.2)(x)  # Regularize with dropout\noutputs = keras.layers.Dense(1, activation='sigmoid')(x)\nmodel = keras.Model(inputs, outputs)\n\nmodel.summary()\n\n","ed6afbd9":"# model.compile(optimizer=keras.optimizers.Adam(),\n#               loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n#               metrics=['accuracy'])\nmodel.compile(\n    optimizer=keras.optimizers.Adam(),\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nepochs = 25\nhistory = model.fit(\n  train_ds,\n  validation_data=val_ds,\n  epochs=epochs\n)","96008abd":"print(history.history)\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","56e3eb30":"predictWithTestDataset(model)","cff7c8f5":"# Unfreeze the base_model. Note that it keeps running in inference mode\n# since we passed `training=False` when calling it. This means that\n# the batchnorm layers will not update their batch statistics.\n# This prevents the batchnorm layers from undoing all the training\n# we've done so far.\nbase_model.trainable = True\nmodel.summary()\n\nmodel.compile(\n    optimizer=keras.optimizers.Adam(1e-5),  # Low learning rate\n    loss=keras.losses.BinaryCrossentropy(from_logits=True),\n    metrics=[keras.metrics.BinaryAccuracy()],\n)\n\nepochs = 2\nhistory = model.fit(train_ds, epochs=epochs, validation_data=val_ds)","b45c98de":"print(history.history)\nacc = history.history['binary_accuracy']\nval_acc = history.history['val_binary_accuracy']\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs_range = range(epochs)\n\nplt.figure(figsize=(8, 8))\nplt.subplot(1, 2, 1)\nplt.plot(epochs_range, acc, label='Training Accuracy')\nplt.plot(epochs_range, val_acc, label='Validation Accuracy')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(1, 2, 2)\nplt.plot(epochs_range, loss, label='Training Loss')\nplt.plot(epochs_range, val_loss, label='Validation Loss')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.show()","4e27e874":"predictWithTestDataset(model)","b3c471b4":"nativity_url = \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/e\/e4\/Leonardo_Da_Vinci_-_Vergine_delle_Rocce_%28Louvre%29.jpg\"\nnativity_path = tf.keras.utils.get_file('NativityPaiting', origin=nativity_url)\n\nimg = keras.preprocessing.image.load_img(\n    nativity_path, target_size=(img_height, img_width)\n)\nimg_array = keras.preprocessing.image.img_to_array(img)\nimg_array = tf.expand_dims(img_array, 0) # Create a batch\n\npredictions = model.predict(img_array)\nprint(predictions)\npredictions = tf.where(predictions < 0.5, 0, 1)\npredictedLabel = labelMappings[str(predictions[0].numpy()[0])]\nprint(predictedLabel)\n\n","2f502379":"<a href=\"https:\/\/colab.research.google.com\/github\/armindocachada\/tensorflow-custom-classification-art-nativity\/blob\/main\/custom_image_classification_detect_if_a_painting_is_from_the_nativity.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","8c7097eb":"Here are some paintings of the nativity:","e6c3bd0e":"# Visualize the data\nHere are the first 9 images from the training dataset.","4dcf88a7":"After downloading, you should now have a copy of the dataset available. There are 429 total images:","209e3f01":"# Conclusion\n\nWe started with training a model from scratch using a simple architecture.\n\nWe have been able to get some pretty good results with a limited dataset. This is indeed very promising! There are many ways to further improve these results, from gathering more images, experimenting with different image sizes and even trying new model architectures.\n\nHope you found this notebook useful. And hope to see you for the next one. Happy coding!","5e27e520":"We define an utility function","882be0d5":"# Compile and train the model","419554d6":"The image_batch is a tensor of the shape (32, 300, 300, 3). This is a batch of 32 images of shape 300x300x3 (the last dimension refers to color channels RGB). The label_batch is a tensor of the shape (32,), these are corresponding labels to the 32 images.\n\nYou can call .numpy() on the image_batch and labels_batch tensors to convert them to a numpy.ndarray.\n\n","02516c61":"# Data augmentation\nOverfitting generally occurs when there are a small number of training examples. Data augmentation takes the approach of generating additional training data from your existing examples by augmenting them using random transformations that yield believable-looking images. This helps expose the model to more aspects of the data and generalize better.\n\nYou will implement data augmentation using the layers from tf.keras.layers.experimental.preprocessing. These can be included inside your model like other layers, and run on the GPU.","51af50c7":"# Overfitting\n\nIn the plots above, the training accuracy is increasing linearly over time, whereas validation accuracy stalls around 60% in the training process. Also, the difference in accuracy between training and validation accuracy is noticeable\u2014a sign of overfitting.\n\nWhen there are a small number of training examples, the model sometimes learns from noises or unwanted details from training examples\u2014to an extent that it negatively impacts the performance of the model on new examples. This phenomenon is known as overfitting. It means that the model will have a difficult time generalizing on a new dataset.\n\nThere are multiple ways to fight overfitting in the training process. In this tutorial, you'll use data augmentation and add Dropout to your model.\n\n","b6df98c2":"# Visualize training results\nAfter applying data augmentation and Dropout, there is less overfitting than before, and training and validation accuracy are closer aligned.","8e70c8b4":"Test model against test dataset","d49953b5":"# Predict on new data\nFinally, let's use our model to classify an image that wasn't included in the training or validation sets.\n\n","8cb41de1":"And some random paintings:","d27662d9":"Looking at the plots, we are seeing a typical sign of **overfitting**. **Overfitting** happens when the model fits a bit too much with the training data but does poorly against the validation data. Notice that the accuracy increases along with the epochs for the training accuracy but with the validation data, the accuracy doesn't increase and in this case the loss increasses.","beeb2bdc":"# Configure the dataset for performance\n\nLet's make sure to use buffered prefetching so you can yield data from disk without having I\/O become blocking. These are two important methods you should use when loading data.\n\nDataset.cache() keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n\nDataset.prefetch() overlaps data preprocessing and model execution while training.\n\nInterested readers can learn more about both methods, as well as how to cache data to disk in the data performance guide.","f2df0ca2":"We inspect the image_batch and labels_batch variables. ","12b38a64":"# Map image labels to numeric values\n\nWe are using Binary cross-entropy for our classification so we need to make sure our labels are either a 0 or a 1. Nativity = 1 and Others = 0\n\nWe will rename the folders to a 0 and a 1 since that is what **tf.keras.preprocessing.image_dataset_from_directory** uses to create the labels for our data set.","bde02e87":"There are two ways to use this layer. You can apply it to the dataset by calling map:","241db0aa":"# Train the model","278e2688":"# Standardize the data\nThe RGB channel values are in the [0, 255] range. This is not ideal for a neural network; in general you should seek to make your input values small. Here, you will standardize values to be in the [0, 1] range by using a Rescaling layer.","9b7bbd8e":"# Import TensorFlow and other libraries","5c22b645":"You will use data augmentation to train a model in a moment.\n\n# Dropout\nAnother technique to reduce overfitting is to introduce Dropout to the network, a form of regularization.\n\nWhen you apply Dropout to a layer it randomly drops out (by setting the activation to zero) a number of output units from the layer during the training process. Dropout takes a fractional number as its input value, in the form such as 0.1, 0.2, 0.4, etc. This means dropping out 10%, 20% or 40% of the output units randomly from the applied layer.\n\nLet's create a new neural network using layers.Dropout, then train it using augmented images.","0a8e0031":"# Load using keras.preprocessing\n\nKeras provides a bunch of really convenient functions to make our life easier when working with Tensorflow. **tf.keras.preprocessing.image_dataset_from_directory** is one of them. It loads images from the files into **tf.data.DataSet** format. \n\n","8597a3d2":"Some more utility functions just to help download the images from our image dataset. Notice that **getFileNameFromUrl()** does some very basic **cleanup** and **extraction** of the **filename** in the url.","5056b1d8":"Or, you can include the layer inside your model definition, which can simplify deployment. Let's use the second approach here.","193e3c53":"The Virgin of the rocks","b99b3a5d":"# Create the model\nThe model consists of three convolution blocks with a max pool layer in each of them. There's a fully connected layer with 128 units on top of it that is activated by a relu activation function. This model has not been tuned for high accuracy, the goal of this tutorial is to show a standard approach.","1d93edd8":"# Transfer Learning\n\nWe have already used image augmentation to try and get better results from our model, and I have to say the results were not bad at all. We were able to get a model with 65% accuracy. Surely, we can do better than that, if we are able to collect hundreds more, perhaps thousands more images for our training and validation data set.\n\nYou can certainly do that, but there is another way that doesn't involve the tedious and expensive process of collecting more training data: **Transfer Learning**.\n\nWith transfer learning we can borrow a model that is already trained against thousands of images and re-train it for our use case, but with much fewer images than it would have been possible to if we trained a model from scratch.\n\nTo do so we can use Keras to download a pre-trained model with the Xception architecture already trained on Imagenet. \n\nTo perform transfer learning we need to freeze the weights of the base model and perform the training as we normally would. You will notice that we still do the image augmentation, and the regularization.  \n","14791882":"I am a little bit disappointed. Our Virgin of the Rocks painting is not recognised as a nativity painting. But is it really?","41eb2add":"You can find the class names in the class_names attribute on these datasets. These correspond to the directory names in alphabetical order.\n\n\n","78e65b73":"# Tensorflow Object Detection API - Custom Image Classification - Can I detect if a painting is a nativity painting?\n\nCan I teach an AI model using Tensorflow Object Detection API, how to identify the theme of a painting?\n\nThe only way to find out is by trying! And I think you might be surprised by the outcome of this experiment.\n\nBut, I am going to narrow down my problem. I want to identify if a painting is about the **Nativity** or not.\nWhich is simple binary classification problem.\n\nBut first let me explain what a nativity painting is. A nativity painting is a painting where the subject is the birth of Jesus Christ, very revered in Christianity. \nDuring the last two thousand years, many famous artists like, for example, Leonardo Da Vinci, were commissioned to create paintings for Churches, so there should be plenty of paintings to pick about the nativity.\n\n\n![The Nativity By Boticelli](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/ab\/Botticelli%2C_adorazione_dei_magi_uffizi.jpg\/600px-Botticelli%2C_adorazione_dei_magi_uffizi.jpg \"The Nativity By Boticelli\")\n\nWhen trying to identify a painting about the **nativity** you will almost always see **Baby Jesus** in the center. And, almost always, you will see his **Mother Mary** and his **Father Joseph**. You will see sometimes **animals** around **Baby Jesus**, because he was born in a stable. Sometimes you will see him in a cave.\nAnd sometimes he is outside and surrounded by shepherds, and so on. \n\nI hope you can agree with me. There is NO WAY that we can write any code that can take account of all these different variations. \n\nRemember what I just said? **Almost always** you will have Baby Jesus in the painting. But surely, a **nativity** painting without **Baby Jesus** makes no sense?\nRight. Apparently **Leonardo Da Vinci**, didn't get that message and created a **nativity** painting without its central character. Mind that the **Virgin of the Rocks** is a very famous painting, but for me this is a clear **outlier**.  \n\n\n![Virgin of the Rocks](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/f\/fb\/Leonardo_da_Vinci_-_Virgin_of_the_Rocks_%28National_Gallery_London%29.png\/311px-Leonardo_da_Vinci_-_Virgin_of_the_Rocks_%28National_Gallery_London%29.png)\n\nTherefore for the sake of not confusing our **Deep Neural Network**, I am not going to include the **Virgin on the Rocks** in the training or validation while building our model. But just for fun at the end we will see if our model really can be smarter than what we think.\n\nOk. Now that you know what I am trying to build, for the sake of learning Tensorflow. You will quite rightly think. There is no way this will ever work?!\n\nBefore you rush into judgement, hold on to your paper! Let's find out!\n\n\n# Before Starting\n\nI created this notebook with code from the \"Tensorflow tutorial on **Image Classification**. You can find the original tutorial in the link below: \n\nhttps:\/\/www.tensorflow.org\/tutorials\/images\/classification\n\n# Video\n\nCheck my youtube video series to see more detais about this notebook:\n\nhttps:\/\/www.youtube.com\/watch?v=LiMH5lyjEEA&list=PL3OV2Akk7XpCr4nykYF22aTrLrqnoOw1T\n\n\n\n\n\n\n","1ae0d6c8":"Now we define the utility function **resizeImages** to resize images and copy from a **sourceFolder** to a **destinationFolder**.","c92c767f":"# Visualize training results","1eb3704f":"In general it is advised to split data into training data and validation data using a 80% 20% split. Remember, this is not a hard and fast rule.","10328fef":"Let's visualize what a few augmented examples look like by applying data augmentation to the same image several times:\n\n","d2212def":"# Compile the model\nFor this tutorial, choose the optimizers.Adam optimizer and losses.SparseCategoricalCrossentropy loss function. To view training and validation accuracy for each training epoch, pass the metrics argument.\n\n\n","bc6f836e":"# Resize All images to be no bigger than 90000 pixels(width x height)\n\nSome of the images in our dataset are over 80MB in size. If I try to resize an image with Python, it will try to load the image into memory. Not a great idea. So we are going to use Imagemick to do the job super fast.","2fc79f8d":"# Model summary\n\nView all the layers of the network using the model's summary method:","23bb8f44":"# Download Training, Validation and Test Image Data Sets\n\nIn order to train an image classifier we need to have a **training** image dataset, **validation** dataset and a **test** dataset.\n\nSince we are training a binary image classifier, we will have images for two different classes:\n\n\n*   Nativity\n*   Others\n\nDuring the training of our model we will use the training dataset to teach the model how to classify a painting: either a **nativity** painting or not(**other**).\n\nAt the end of each training cycle(epoch) we will use the validationg data-set to score how well the model is doing by calculating the **accuracy** and the **loss**. The **accuracy** measures how many times our model got the right answer. **Higher** is better. The **loss** measures the delta, i.e. the difference between the predicted value and the actual value. **Lower** is better.\n\nIt is important that the **validation** data set is **separate** from the **training** dataset because the AI model is very good at cheating. If you don't separate the two, the model, will simply memorise the answers instead of learning the intrinsic characteristics of what we are trying to teach it.\n\nAt the end of the training we will also use a separate **test** dataset from the **training** and **validation** dataset, to do an independent benchmark of the model performance. \n\nYou will notice that we are downloading three files:\n- nativity_dataset.csv - contains all nativity paintings\n- other_dataset.csv - contains many paintings except nativity paintings\n- test_dataset.csv - contains labeled paintings\n\nWait a moment! Did I not just say that the training data set should be separate from the validation data set, so why keep it in the same files?\n\nYes, but because we are doing data exploration, it is a good thing to have some flexibility. Typically you are advised to have 80% of the training data and 20% of the validation data. But, this is not a hard and fast rule. We might want to change these percentages and see what gives us better results as part of our experimentation. This is also known as **Hyperparameter tuning**. On the other hand the test data set should be fixed, so we can compare different models with different architectures in a consistent way.\n"}}