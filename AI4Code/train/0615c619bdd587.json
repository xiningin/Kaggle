{"cell_type":{"ef8100fd":"code","a96cb66a":"code","c849f050":"code","cc42faeb":"code","17bed5a4":"code","c2f45152":"code","019580c9":"code","63d0a79a":"code","9b43e60b":"code","ceec3365":"code","603a4059":"code","687e621b":"markdown","ed0b1554":"markdown","bb579a0e":"markdown","bbdf0368":"markdown"},"source":{"ef8100fd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ntest_data = []\ntrain_data = []\n\n# check is this path is correct in your version\nDATA_PATH = \"..\/input\/s1-pytorch-speaker-verification-data-prep\"\n\n# feel free to pick you own model weight\nMODEL_RESTORE_FROM = \"..\/input\/s2-pytorch-speaker-verification-train-model\/chk_pts\/ckpt_epoch_8000.pth\"\n\n\n# creating a list of files for training and testing\nfor dirname, _, filenames in os.walk(DATA_PATH):\n    for filename in filenames:\n        str = os.path.join(dirname, filename)\n        if \"test\" in str:\n            test_data.append(str)\n        elif \"train\" in str:\n            train_data.append(str)\n            \n\n","a96cb66a":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.autograd as grad\n\nimport time\n\n\nimport random","c849f050":"# you can change these as you feel fit. These govern the input\/output shapes to the model\ndef get_hyper_parameters():\n    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n    print(f\"Device type available {device}\")\n    hparam_dict = {\n\n        # genereal parameters\n        \"is_training_mode\": True,\n        \"device\": device,\n\n        \"model_path\": \"pre_trained_models\",\n        \"log_file\": \"logs\/log.txt\",\n        \"checkpoint_dir\": \"chk_pts\",\n\n        \"is_data_preprocessed\": True,\n        \"sr\": 16000,\n\n        # For mel spectrogram preprocess\n        \"nfft\": 512,\n        \"window\": 0.025,  # (s)\n        \"hop\": 0.01,  # (s)\n        \"n_mels\": 40,  # Number of mel energies\n        \"tisv_frame\": 180,  # Max number of time steps in input after preprocess\n\n        # model hyper parameters\n        \"hidden\": 768,  # Number of LSTM hidden layer units\n        \"num_layer\": 3,  # Number of LSTM layers\n        \"proj\": 256,  # Embedding size\n\n        # train:\n        \"training_N\": 4,  # Number of speakers in batch\n        \"training_M\": 5,  # Number of utterances per speaker\n        \"training_num_workers\": 0,  # number of workers for dataloader\n        \"lr\": 0.01,\n        \"training_epochs\": 16000,  # Max training speaker epoch\n        \"log_interval\": 30,  # Epochs before printing progress\n        \"checkpoint_interval\": 2000,  # Save model after x speaker epochs\n        \"restore_existing_model\": False,  # Resume training from previous model path\n        \"verbose\": True,\n\n        # test:\n        \"test_N\": 4,  # Number of speakers in batch\n        \"test_M\": 6,  # Number of utterances per speaker\n        \"test_num_workers\": 8,  # number of workers for data laoder\n        \"test_epochs\": 10,  # testing speaker epochs\n\n        # small error\n        \"small_err\": 1e-6,\n\n\n    }\n    return hparam_dict\n\n\nclass DictWithDotNotation(dict):\n    \"\"\"\n    a dictionary that supports dot notation \n    as well as dictionary access notation \n    usage: d = DotDict() or d = DotDict({'val1':'first'})\n    set attributes: d.val2 = 'second' or d['val2'] = 'second'\n    get attributes: d.val2 or d['val2']\n    \"\"\"\n    __getattr__ = dict.__getitem__\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\n\n    def __init__(self, dct=None):\n        dct = dict() if not dct else dct\n        for key, value in dct.items():\n            if hasattr(value, 'keys'):\n                value = Dict_with_dot_notation(value)\n            self[key] = value\n\n\nclass HyperParameters(DictWithDotNotation):\n\n    def __init__(self, hp_dict=None):\n        super(DictWithDotNotation, self).__init__()\n\n        if hp_dict is None:\n            hp_dict = get_hyper_parameters()\n\n        hp_dotdict = DictWithDotNotation(hp_dict)\n        for k, v in hp_dotdict.items():\n            setattr(self, k, v)\n\n    __getattr__ = DictWithDotNotation.__getitem__\n    __setattr__ = DictWithDotNotation.__setitem__\n    __delattr__ = DictWithDotNotation.__delitem__\n\n\nhp = HyperParameters()\nhp","cc42faeb":"# dataset class for data retrieval\nclass SpeakerDatasetPreprocessed(Dataset):\n\n    def __init__(self, data_source, utter_num, shuffle=True, utter_start=0):\n\n        self.lst_train_data = data_source\n        self.utter_num = utter_num #hp.training_M\n\n        self.file_list = data_source #os.listdir(self.path)\n        self.shuffle = shuffle\n        self.utter_start = utter_start\n\n    def __len__(self):\n        return len(self.file_list)\n\n    def __getitem__(self, idx):\n\n        np_file_list = self.file_list\n\n        if self.shuffle:\n            # select random speaker\n            selected_file = random.sample(np_file_list, 1)[0]\n        else:\n            selected_file = np_file_list[idx]\n\n#         print(selected_file)\n        # load utterance spectrogram of selected speaker\n        utters = np.load(selected_file)\n        if self.shuffle:\n            # select M utterances per speaker\n            utter_index = np.random.randint(0, utters.shape[0], self.utter_num)\n            utterance = utters[utter_index]\n        else:\n            # utterances of a speaker [batch(M), n_mels, frames]\n            utterance = utters[self.utter_start: self.utter_start + self.utter_num]\n\n        # TODO implement variable length batch size\n        utterance = utterance[:, :, :160]\n\n        utterance = torch.tensor(np.transpose(utterance, axes=(0, 2, 1)))  # transpose [batch, frames, n_mels]\n        return utterance","17bed5a4":"# creating the test data loader\ntest_dataset = SpeakerDatasetPreprocessed(test_data, hp.test_M,\n                                          shuffle=False)\ntest_loader = DataLoader(test_dataset,\n                          batch_size=hp.test_N,  # number of speakers\n                          shuffle=True,\n                          num_workers=hp.test_num_workers,\n                          drop_last=True)","c2f45152":"# testing the data loader\nfor mel_db_batch in test_loader:\n    print(mel_db_batch.shape)","019580c9":"class GE2ELoss(nn.Module):\n\n    def __init__(self, device):\n        super(GE2ELoss, self).__init__()\n        self.w = nn.Parameter(torch.tensor(10.0).to(device), requires_grad=True)\n        self.b = nn.Parameter(torch.tensor(-5.0).to(device), requires_grad=True)\n        self.device = device\n\n    def forward(self, embeddings):\n        # Clamps all elements in input into the range [ min, max ]\n        # https:\/\/pytorch.org\/docs\/stable\/generated\/torch.clamp.html\n        torch.clamp(self.w, hp.small_err)\n        centroids = GE2ELoss.get_centroids(embeddings)\n        cos_similarity = GE2ELoss.get_cos_sim(embeddings, centroids)\n\n        # this is eq (5) from the paper https:\/\/arxiv.org\/abs\/1710.10467\n        sim_matrix = self.w * cos_similarity.to(self.device) + self.b\n\n        loss, _ = GE2ELoss.calc_loss(sim_matrix)\n        return loss\n\n    # eq. (1)\n    @staticmethod\n    def get_centroids(embeddings):\n        # calculating centroid of all the utterances  as per eq. (1)\n        # centroid for neg similarity\n        centroids = embeddings.mean(dim=1)\n        return centroids\n\n    # calculates cos similarities of eq. (5)\n    @staticmethod\n    def get_cos_sim(embeddings, centroids):\n        # number of utterances per speaker\n        num_utterances = embeddings.shape[1]\n        utterance_centroids = GE2ELoss.get_utterance_centroids(embeddings)\n\n        # flatten the embeddings and utterance centroids to just utterance,\n        # so we can do cosine similarity\n        utterance_centroids_flat = utterance_centroids.view(\n            utterance_centroids.shape[0] * utterance_centroids.shape[1],\n            -1)\n        embeddings_flat = embeddings.view(embeddings.shape[0] * num_utterances, -1)\n        # the cosine distance between utterance and the associated centroids\n        # for that utterance\n        # this is each speaker's utterances against his own centroid, but each\n        # comparison centroid has the current utterance removed\n        cos_same = F.cosine_similarity(embeddings_flat, utterance_centroids_flat)\n\n        # now we get the cosine distance between each utterance and the other speakers' centroids\n        # to do so requires comparing each utterance to each centroid. To keep the\n        # operation fast, we vectorize by using matrices L (embeddings) and\n        # R (centroids) where L has each utterance repeated sequentially for all\n        # comparisons and R has the entire centroids frame repeated for each utterance\n        centroids_expand = centroids.repeat((num_utterances * embeddings.shape[0], 1))\n        embeddings_expand = embeddings_flat.unsqueeze(1).repeat(1, embeddings.shape[0], 1)\n        embeddings_expand = embeddings_expand.view(\n            embeddings_expand.shape[0] * embeddings_expand.shape[1],\n            embeddings_expand.shape[-1]\n        )\n        cos_diff = F.cosine_similarity(embeddings_expand, centroids_expand)\n        cos_diff = cos_diff.view(\n            embeddings.size(0),\n            num_utterances,\n            centroids.size(0)\n        )\n        # assign the cosine distance for same speakers to the proper idx\n        same_idx = list(range(embeddings.size(0)))\n        cos_diff[same_idx, :, same_idx] = cos_same.view(embeddings.shape[0], num_utterances)\n        cos_diff = cos_diff + hp.small_err\n        return cos_diff\n\n    # eq (8)\n    @staticmethod\n    def get_centroid(embeddings, speaker_num, utterance_num):\n        # c(j\u2212i) = 1  # mmX=1  # ejm, (8)\n        # eq 8\n        centroid = 0\n        for utterance_id, utterance in enumerate(embeddings[speaker_num]):\n            if utterance_id == utterance_num:\n                continue\n            centroid = centroid + utterance\n        centroid = centroid \/ (len(embeddings[speaker_num]) - 1)\n        return centroid\n\n    @staticmethod\n    def get_utterance_centroids(embeddings):\n        \"\"\"\n        Returns the centroids for each utterance of a speaker, where\n        the utterance centroid is the speaker centroid without considering\n        this utterance\n\n        Shape of embeddings should be:\n            (speaker_ct, utterance_per_speaker_ct, embedding_size)\n        \"\"\"\n        sum_centroids = embeddings.sum(dim=1)\n        # we want to subtract out each utterance, prior to calculating the\n        # the utterance centroid\n        sum_centroids = sum_centroids.reshape(sum_centroids.shape[0], 1, sum_centroids.shape[-1])\n        # we want the mean but not including the utterance itself, so -1\n        num_utterances = embeddings.shape[1] - 1\n        centroids = (sum_centroids - embeddings) \/ num_utterances\n        return centroids\n\n    @staticmethod\n    def calc_loss(sim_matrix):\n        same_idx = list(range(sim_matrix.size(0)))\n\n        # eq. (6)\n        pos = sim_matrix[same_idx, :, same_idx]\n        neg = (torch.exp(sim_matrix).sum(dim=2) + hp.small_err).log_()\n        per_embedding_loss = -1 * (pos - neg)  # this is the loss as per eq. (6)\n\n        # eq. (10) final loss\n        # the final GE2E loss LG is\n        # the sum of all losses over the similarity matrix (1 \u2264 j \u2264 N, and 1 \u2264 i \u2264 M)\n        loss = per_embedding_loss.sum()\n        return loss, per_embedding_loss","63d0a79a":"class SpeechEmbedModel(nn.Module):\n\n    def __init__(self):\n        super(SpeechEmbedModel, self).__init__()\n\n        # this creates a three stacks (hp.num_layer) of LSTM\n        self.LSTM_stack = nn.LSTM(hp.n_mels, hp.hidden, num_layers=hp.num_layer, batch_first=True)\n\n        for name, param in self.LSTM_stack.named_parameters():\n            if 'bias' in name:\n                nn.init.constant_(param, 0.0)\n            elif 'weight' in name:\n                nn.init.xavier_normal_(param)\n\n        # feed forward layer\n        self.projection = nn.Linear(hp.hidden, hp.proj)\n\n    def forward(self, x):\n        x, _ = self.LSTM_stack(x.float())  # (batch, frames, n_mels)\n        # only use last frame\n        x = x[:, x.size(1) - 1]\n        x = self.projection(x.float())\n\n        # The embedding vector (d-vector) is defined as the L2 normalization of the network output\n        x = x \/ torch.norm(x, dim=1).unsqueeze(1)\n        return x","9b43e60b":"# testing the untrained model\ntmp_model = SpeechEmbedModel()\ntotal_utterances = hp.test_N * hp.test_M\n\nfor mel_db_batch in test_loader:\n    # mel is returned as 4x5x160x40 (batchxnum_speakerxutterlenxn_mel)and we will reshape it to 20x160x40\n    new_shape = (total_utterances, mel_db_batch.size(2), mel_db_batch.size(3))\n    mel_db_batch = torch.reshape(mel_db_batch, new_shape)\n    res = tmp_model(mel_db_batch)\n    break\n\n# although the output is pure random, but it confirms that the model works.\nres.shape, res","ceec3365":"# creating a model instance\nmodel = SpeechEmbedModel()\n\n#loading the pre-trained weights to the model\nmodel.load_state_dict(torch.load(MODEL_RESTORE_FROM))\n\n# setting the model to testing mode. i.e. switch off the learning\nmodel.eval()","603a4059":"# calculating the total sentences\/utterances\ntotal_utterances = hp.test_N * hp.test_M\n\n# initializing w and b place holders\nw = grad.Variable(torch.tensor(1.0))\nb = grad.Variable(torch.tensor(0.0))\n\nfor mel_db_batch in test_loader:\n    # mel_db_batch is returned as 4x5x160x40 (batchxnum_speakerxutterlenxn_mel) and we will reshape it to 20x160x40\n    new_shape = (total_utterances, mel_db_batch.size(2), mel_db_batch.size(3))\n    mel_db_batch = torch.reshape(mel_db_batch, new_shape)\n    \n    # pass mel_db_batch through the pre-trained model\n    embeddings = model(mel_db_batch)\n    \n    # output of the model is (NxM, utterance_size), we reshape it to NxMxUtter_size\n    embeddings = torch.reshape(embeddings, (hp.test_N, hp.test_M, embeddings.size(1)))\n    \n    # calculating centroids and similarity matrix\n    centroids = GE2ELoss.get_centroids(embeddings)\n    cos_sim = GE2ELoss.get_cos_sim(embeddings, centroids)\n    sim_matrix = w * cos_sim + b\n    \n    S = sim_matrix.detach().numpy()    \n    print(S.shape, type(S), \"\\n\", S)\n    \n    # calculating EER\n    diff = 1\n    EER = 0\n    EER_thres = 0 \n    EER_FAR = 0\n    EER_FRR = 0\n    \n    # through thresholds calculate false acceptance ratio (FAR) and false reject ratio (FRR)\n    thres_lst = [0.01*i + 0.5 for i in range(50)]\n    for thres in thres_lst:\n        S_thres = S > thres\n    \n        # False acceptance ratio = false acceptance \/ mismatched population (enroll speaker != verification speaker)\n        # sum of number of times there were a TRUE in the fist axis of the matrix\n        # sum of number of times there was a TRUE for the same person\n        # e.g.\n        # t[0] =\n        # array([[ True, False, False, False],\n        #        [ True, False, False, False],\n        #        [ True, False, False, False],\n        #        [ True, False, False, False],\n        #        [ True, False, False, False],\n        #        [ True, False, False, False]])\n        # hence np.sum(S_thres[i]) = 6\n        \n        # and t[0, :, 0]\n        # array([ True,  True,  True,  True,  True,  True]) \n        # hence np.sum(S_thres[i, :, i] = 6\n        # hence 6 - 6 = 0 for i = 0\n        # and [0, 0, 0, 0] for all the speakers\n        # which means false acceptance ratio was 0.. which is awesome\n        \n        denominator = (hp.test_N - 1) \/ hp.test_M \/ hp.test_N\n        lst = [np.sum(S_thres[i]) - np.sum(S_thres[i, :, i]) for i in range(hp.test_N)]\n        FAR = sum(lst) \/ denominator\n\n        # False reject ratio = false reject \/ matched population (enroll speaker = verification speaker)\n        # this same but in reverse order.\n        # we have M number of speakers and for given number, how many did we reject?\n        denominator = hp.test_M \/ hp.test_N\n        lst = [hp.test_M - np.sum(S_thres[i][:, i]) for i in range(hp.test_N)]\n        FRR = sum(lst) \/ denominator\n        \n        # Save threshold when FAR = FRR (=EER)\n        if diff > abs(FAR-FRR):\n            diff = abs(FAR-FRR)\n            EER = (FAR+FRR)\/2\n            EER_thres = thres\n            EER_FAR = FAR\n            EER_FRR = FRR\n        \n    print(\"\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)\"%(EER, EER_thres, EER_FAR, EER_FRR))\n","687e621b":"# Step3 - Speaker verification\n**Following are the steps to verify the model**\n\n1. Download the data for testing from the S1 ([here](https:\/\/www.kaggle.com\/gaurav41\/s1-pytorch-speaker-verification-data-prep))\n2. Download the pretrained model from the S2 ([here](https:\/\/www.kaggle.com\/gaurav41\/s2-pytorch-speaker-verification-train-model\/data))\n3. Create data pipeline\n4. Create the model (untrained)\n5. Load the weights and learnings from the pretrained model\n6. Generate embeddings using the model\n7. Calculate 'sim_matrix' using the GE2E class\n8. Calculate Errors\n\n**Depending upon the data selected by the DataLoader, you might see an error of .33 or 0**","ed0b1554":"# Classes for GE2E loss and Model\n\n**Note that these are exactly the same classes as we had in S2**","bb579a0e":"# Testing the model","bbdf0368":"# DataLoader"}}