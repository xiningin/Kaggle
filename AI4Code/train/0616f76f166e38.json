{"cell_type":{"1afabb87":"code","1504c2af":"code","2ab706cb":"code","2dd583fe":"code","377be49f":"code","0c1bd79d":"code","6da9ccb6":"code","e9638125":"code","3a93a694":"code","4efeda76":"code","4d6af57b":"code","9dd6a6f8":"code","66e398a7":"code","4c506029":"code","d155224f":"code","31def956":"code","c341aa33":"markdown","cb8a2009":"markdown","84bed282":"markdown","f6545c2e":"markdown","ac1bb629":"markdown","f8e1d974":"markdown","66053f8e":"markdown","07144397":"markdown","812cea42":"markdown","760ec43b":"markdown","2808fe30":"markdown","3550a74c":"markdown","4bb3b28b":"markdown","572849ea":"markdown","c796a1f5":"markdown"},"source":{"1afabb87":"# Import all modules\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n\n# ML model: RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\n\n#Cross Validation\nfrom sklearn.model_selection import cross_val_score\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\n\n# Preprocessing\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\n\n# Hyperparameter Tuning\nfrom sklearn.model_selection import GridSearchCV\n\n# Visualization\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nprint('Import completed')","1504c2af":"# Load csv files\ntrain_data = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\ntest_data = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv')\n","2ab706cb":"# First look at the data\nprint(train_data.shape)\ntrain_data.head()","2dd583fe":"# Set target which will be predicted later on\ntarget = train_data['SalePrice']\n\n# Splitting data in numerical and categorical subsets\nnum_attr = train_data.select_dtypes(exclude='object').drop('SalePrice', axis=1).copy()\ncat_attr = train_data.select_dtypes(include='object').copy()","377be49f":"# Finding outliers by graphing numerical attributes to SalePrice\nplots = plt.figure(figsize=(12,20))\n\nprint('Loading 35 plots ...')\nfor i in range(len(num_attr.columns)-1):\n    plots.add_subplot(9, 4, i+1)\n    sns.regplot(num_attr.iloc[:,i], target)\n    \nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=1.0)","0c1bd79d":"cat_attr.columns","6da9ccb6":"sns.countplot(x='SaleCondition', data=cat_attr)","e9638125":"# Missing values for numerical attributes\nnum_attr.isna().sum().sort_values(ascending=False).head()","3a93a694":"# Missing values for categorical attributes\ncat_attr.isna().sum().sort_values(ascending=False).head(16)","4efeda76":"# Copy the data to prevent changes to original data\ndata_copy = train_data.copy()\n\ndata_copy.MasVnrArea = data_copy.MasVnrArea.fillna(0)\n\n# Columns which can be filled with 'None'\ncat_cols_fill_none = ['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtFinType2', 'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond',\n                     'MasVnrType']\nfor cat in cat_cols_fill_none:\n    data_copy[cat] = data_copy[cat].fillna(\"None\")\n    \ndata_copy.isna().sum().sort_values(ascending=False).head()","4d6af57b":"# Dropping outliers found when visualizing the numerical subset of our dataset\ndata_copy = data_copy.drop(data_copy['LotFrontage'][data_copy['LotFrontage']>200].index)\ndata_copy = data_copy.drop(data_copy['LotArea'][data_copy['LotArea']>100000].index)\ndata_copy = data_copy.drop(data_copy['BsmtFinSF1'][data_copy['BsmtFinSF1']>4000].index)\ndata_copy = data_copy.drop(data_copy['TotalBsmtSF'][data_copy['TotalBsmtSF']>6000].index)\ndata_copy = data_copy.drop(data_copy['1stFlrSF'][data_copy['1stFlrSF']>4000].index)\ndata_copy = data_copy.drop(data_copy.GrLivArea[(data_copy['GrLivArea']>4000) & (target<300000)].index)\ndata_copy = data_copy.drop(data_copy.LowQualFinSF[data_copy['LowQualFinSF']>550].index)\n\nX = data_copy.drop('SalePrice', axis=1)\n\ny = data_copy.SalePrice\n\n\nnumerical_transformer = Pipeline([\n    ('imputer', SimpleImputer(strategy='median')),\n    ('std_scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, num_attr.columns),\n        ('cat', categorical_transformer, cat_attr.columns)\n    ])","9dd6a6f8":"xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05)\n\nxgb_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', xgb_model)\n                             ])\n\n\nparam_grid = [\n    {'model__n_estimators': [50, 100, 150, 200, 250, 300, 500], 'model__learning_rate': [0.01, 0.02, 0.05, 0.1, 0.3]}\n]\n\ngrid_search = GridSearchCV(xgb_pipeline, param_grid, cv=5, scoring='neg_mean_squared_error', return_train_score=True)\n\n\ngrid_search.fit(X, y)\n\n\n","66e398a7":"best_model = grid_search.best_estimator_","4c506029":"# Applying the same data cleaning we used for the training data to the test data\n\ntest_X = test_data.copy()\ntest_X.MasVnrArea = test_X.MasVnrArea.fillna(0)\ntest_X = test_X.drop('Id', axis=1)\n","d155224f":"test_preds = best_model.predict(test_X)\ntest_preds","31def956":"output = pd.DataFrame({'Id': test_data.Id,\n                      'SalePrice': test_preds})\noutput.to_csv('submission.csv', index=False)\nprint('Submitted')","c341aa33":"# 1.2 Analyzing categorical attributes","cb8a2009":"This array contains all predictions. To create the submission.csv file, we will run the next code cell:","84bed282":"Parameters for XGBRegressor seem to yield good results. (Future addition: tweaking parameters to fine-tune algorithm)<br>\nSame applys to RandomForestRegressor.","f6545c2e":"# 3.1 XGBRegressor","ac1bb629":"There are a lot of missing values here.<br>\nWe can drop nearly all of them as one hot encoding them won't be useful with this high amount of missing values. (It would dramatically increase the amount of columns in the dataset)\n<br><br>\n* MasVnrType and MasVnrArea both have 8 missing values\n* Electrical --> One hot encoding (pd.get_dummies())\n","f8e1d974":"259 missing values for LotFrontage --> we will use SimpleImputer() to fill them with averaged values.<br>","66053f8e":"# 3. Building two models (RandomForestRegressor and XGBRegressor)","07144397":"# 1.1 Analyzing numerical attributes","812cea42":"Missing values left in the dataset: LotFrontage (259), GarageYrBuilt (81), Electrical (1)","760ec43b":"# Predict House Pricing\nwork in progres\n<br><br><br>\nFuture additions:\n* Cross-validation\n* Method to calculate best parameters for the model\n* better data cleaning (Experimenting with SimpleImputer(), one hot encoding for more columns)\n* more visualizations for categorical data\n","2808fe30":"# 1. Exploratory data analysis ","3550a74c":"# 4. Final predictions and submission","4bb3b28b":"As you can see we have a dataset containing 80 columns (with both numerical and categorical values) and 1460 rows (houses).\n<br>The first step for now is to split the dataset in numerical and categorical subsets.\n<br>We will also set our target to be 'SalePrice' since it is the attribute we will want to predict.","572849ea":"# 2. Data Cleaning","c796a1f5":"**Outliers (don't follow regression line):**\n* LotFrontage > 200\n* LotArea > 100,000\n* BsmtFinSF1 > 4000\n* TotalBsmtSF > 6000\n* GrLivArea > 4000 + SalePrics < 300,000\n* LowQualFinSF > 550"}}