{"cell_type":{"8dbf37ca":"code","c90630fa":"code","a496ff6e":"code","1242cade":"code","3e99971b":"code","e9099f0a":"code","d5733384":"code","5e764bb0":"code","2f4eb6ce":"code","c8b1c59d":"code","df8d1f64":"code","6d80af72":"code","e94ced57":"code","bb3184b7":"code","0fcf1226":"code","73b2ab80":"code","007e646f":"code","110b7005":"code","66a38970":"code","7ef0b43f":"code","ee44b406":"code","a74e2d6a":"code","f524d05a":"code","eeef896a":"code","40baeb34":"markdown","276c946c":"markdown","edb43300":"markdown","738817c4":"markdown","787064ca":"markdown","1bed2b00":"markdown","6c251a96":"markdown","3f3cf34f":"markdown","5b4af25c":"markdown","abca45d4":"markdown","bf87b31d":"markdown","76164692":"markdown"},"source":{"8dbf37ca":"import os\nimport zipfile\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Model\nfrom keras.layers import Dense, Input, Dropout, Activation\nfrom keras.layers import Bidirectional, LSTM, Embedding, GlobalMaxPool1D\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras import backend as K\nfrom keras import callbacks\nfrom sklearn.model_selection import train_test_split","c90630fa":"samplesub_zip = '..\/input\/jigsaw-toxic-comment-classification-challenge\/sample_submission.csv.zip'\ntest_zip = '..\/input\/jigsaw-toxic-comment-classification-challenge\/test.csv.zip'\ntest_labels_zip = '..\/input\/jigsaw-toxic-comment-classification-challenge\/test_labels.csv.zip'\ntrain_zip = '..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv.zip'\n\nfor file_dir in [samplesub_zip, test_zip, test_labels_zip, train_zip]:\n  zip_ref = zipfile.ZipFile(file_dir, 'r')\n  zip_ref.extractall('.\/jigsawtoxic\/')\n  zip_ref.close()\n\nbase_dir = '.\/jigsawtoxic\/'\nos.listdir('.\/jigsawtoxic\/')","a496ff6e":"train = pd.read_csv('.\/jigsawtoxic\/train.csv')\ntrain","1242cade":"test = pd.read_csv('.\/jigsawtoxic\/test.csv')\ntest","3e99971b":"test_labels = pd.read_csv('.\/jigsawtoxic\/test_labels.csv')\ntest_labels","e9099f0a":"sample_submission = pd.read_csv('.\/jigsawtoxic\/sample_submission.csv')\nsample_submission","d5733384":"for list_columns in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(train[list_columns].value_counts())","5e764bb0":"for list_columns in ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']:\n    print(test_labels[list_columns].value_counts())","2f4eb6ce":"train = train.drop(columns=[\"id\"])","c8b1c59d":"print(train.isnull().any(), \"\\n\")\nprint(test_labels.isnull().any())","df8d1f64":"data_train = train['comment_text']\ndata_labels = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\ny_train = train[data_labels].values\ndata_test = test['comment_text']","6d80af72":"embed_size = 50             # how big is each word vector\nmax_features = 20000        # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 200                # max number of words in a comment to use\n\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(data_train))\n\nlist_tokenized_train = tokenizer.texts_to_sequences(data_train)\nlist_tokenized_test = tokenizer.texts_to_sequences(data_test)\n\nX_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\nX_test = pad_sequences(list_tokenized_test, maxlen=maxlen)","e94ced57":"glove_embedding = '..\/input\/glove6b50dtxt\/glove.6B.50d.txt'\n\ndef get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(glove_embedding))","bb3184b7":"all_embs = np.stack(embeddings_index.values())\nemb_mean, emb_std = all_embs.mean(), all_embs.std()\nemb_mean, emb_std","0fcf1226":"word_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","73b2ab80":"total_num_words = [len(one_comment) for one_comment in list_tokenized_train]","007e646f":"plt.hist(total_num_words, bins = np.arange(0, 500, 10))\nplt.show()","110b7005":"model = tf.keras.Sequential([\n    tf.keras.layers.Embedding(input_dim=max_features, output_dim=embed_size, \n                              input_length=maxlen, weights=[embedding_matrix]),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(60, return_sequences=True, \n                                                       dropout=0.1, recurrent_dropout=0.1)),\n    tf.keras.layers.GlobalMaxPool1D(),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(50, activation='relu'),\n    tf.keras.layers.Dropout(0.1),\n    tf.keras.layers.Dense(6, activation='sigmoid')])    # only 2 value in each labels\n\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n# using binary crossentropy because each labels or the features only have 2 value, 0 or 1\n\nmodel.summary()","66a38970":"early_stopping_cb = callbacks.EarlyStopping(monitor='val_loss', patience=5)","7ef0b43f":"batch_size = 2048\nepochs = 100\n\nhistory = model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs, \n                    validation_split=0.2,     # validation set is 20% of dataset\n                    callbacks=[early_stopping_cb], verbose=1)","ee44b406":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","a74e2d6a":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epochs')\nplt.legend(['train', 'test'])\nplt.show()","f524d05a":"y_test = model.predict([X_test], batch_size=512, verbose=1) ","eeef896a":"sample_submission[data_labels] = y_test\nsample_submission.to_csv('submission.csv', index=False)\nsample_submission","40baeb34":"*Note: if false, then there are no null value in dataset","276c946c":"# Reference:\n* [GloVe Web](https:\/\/nlp.stanford.edu\/projects\/glove\/)","edb43300":"We're gonna use embedding from other pre-trained word vector dataset to make it faster and not training Jigsaw Toxic Commment from beginning.\n\nGloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\n\nIn this notebook, we're gonna use version 6B50d","738817c4":"# Tokenization","787064ca":"from dataset page description value of -1 indicates it was not used for scoring; (Note: file added after competition close and this notebook was made after the competition over)","1bed2b00":"# Preface","6c251a96":"Drop column that are not needed for training","3f3cf34f":"# EDA (Exploratory Data Analysis)\nWe take a look inside of the dataset, train set and test set, also what the submission be like","5b4af25c":"make prediction using trained model for test set and submit the submission","abca45d4":"Using callbacks.EarlyStopping with val_loss as monitor to make training stopped if validation loss not increasing, and the training will stop since after 5 epoch","bf87b31d":"## Import Dataset","76164692":"# Import \n## Import Library\nFirst, import library needed to solve the problem"}}