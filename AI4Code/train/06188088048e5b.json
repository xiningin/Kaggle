{"cell_type":{"53f9d73d":"code","e3eea816":"code","8d6b297b":"code","1432fd7a":"code","4efe28c5":"code","4f46a723":"code","0ad6e564":"code","82cce8ef":"code","e0c4bc31":"code","3f4d9f32":"markdown","e0944147":"markdown","c2081ff1":"markdown","e00add55":"markdown","a9e7e974":"markdown","f94ae29c":"markdown","b97fdcf3":"markdown"},"source":{"53f9d73d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# data visualization\nimport matplotlib.pyplot as plt \n\n# Model Selection\nfrom sklearn.model_selection import train_test_split\n\n# Model Libraries\nfrom keras.models import Model,Sequential\nfrom keras.layers import Input, Dense\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","e3eea816":"data = pd.read_csv(\"\/kaggle\/input\/digit-recognizer\/train.csv\")","8d6b297b":"img_size = 28\n\ndata_piksel = np.array(data.drop(\"label\",axis=1))\n\nplt.subplot(1,2,1)\nplt.imshow(data_piksel[1].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(data_piksel[15].reshape(img_size, img_size))\nplt.axis('off')\nplt.show()","1432fd7a":"data_piksel = data_piksel.astype(\"float32\") \/ 255.0","4efe28c5":"X_train, X_test, _, _ = train_test_split(data_piksel, data.loc[:,\"label\"], test_size=0.2, random_state=42)","4f46a723":"input_img = Input(shape=(784,))\nencoded_1 = Dense(32, activation=\"relu\")(input_img)\nencoded_2 = Dense(16, activation=\"relu\")(encoded_1)\ndecoded_1 = Dense(32, activation=\"relu\")(encoded_2)\noutput = Dense(784, activation=\"sigmoid\")(decoded_1)\n\nautoencoder = Model(input_img, output)\nautoencoder.compile(optimizer=\"adam\",loss=\"binary_crossentropy\")\nhist = autoencoder.fit(X_train,X_train,epochs=50,batch_size=128,validation_data=(X_test,X_test))","0ad6e564":"plt.figure(figsize=(7,7))\nplt.plot(hist.history[\"loss\"],label=\"Train Loss\")\nplt.legend()\nplt.show()","82cce8ef":"plt.figure(figsize=(7,7))\nplt.plot(hist.history[\"val_loss\"],label=\"Validation Loss\")\nplt.legend()\nplt.show()","e0c4bc31":"prediction = autoencoder.predict(X_test)\n\nplt.subplot(1,2,1)\nplt.imshow(prediction[1].reshape(img_size, img_size))\nplt.axis('off')\nplt.subplot(1, 2, 2)\nplt.imshow(prediction[15].reshape(img_size, img_size))\nplt.axis('off')\nplt.show()","3f4d9f32":"## Conclusion\n\nIn this kernal I explained:\n\n* What is the Autoencoder Model\n* Where is the Autoencoder Model\n* Autoencoder Struct and Working Principle\n* I coded a example model.\n\nThank you so much for analyzing my notebook. If it is benefit for you, please upvote my notebook. You can comment your ideas or questions. I'm looking forward your ideas.\n\nEnjoy with Deep Learning !","e0944147":"## Creating Model\n\nNumber of input neuron and output neuron is equal. Because Autoencoder generates from input to output. Number of hidden layer and neuron is hyper parameter. We can design according to our model. Output activation function is sigmoid if we work to image dataset. ","c2081ff1":"## Model Evaluation","e00add55":"# What is Autoencoder Network?\n\nAutoencoder is a algorithm which input datas reconstruct again. We can say to compy from input to output. Autoencoder consists input, output and hidden layer. Number of hidden layer changes according to model. Hidden layers are used to find important feature. Autoencoder is a unsupervised learning model. It doesn't need to anything label. Autoencoder is used to dimention reduction(or feature extraction) and denoising. Autoencoder can be used to combine some algorith as hybrid. In this notebook, I'm going to explain in basic shape. For this, I used a example model. Let's analyze it.\n\n![image.png](attachment:image.png)","a9e7e974":"## Preprocessing","f94ae29c":"## Spliting Data","b97fdcf3":"## Libraries"}}