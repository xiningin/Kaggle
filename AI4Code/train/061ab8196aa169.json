{"cell_type":{"bc5ef9e8":"code","4eb66f57":"code","9b7519fe":"code","a71b5c64":"code","30791741":"code","1c31e18a":"code","fee93e3e":"code","d8aff127":"code","a219f3e8":"code","b759c367":"code","33051a77":"code","7ea27545":"code","a060b310":"code","7ea2fbce":"code","53f03c0d":"code","e1ac2c24":"code","ced34be7":"code","44458d1e":"code","9f710222":"code","67cac75c":"code","ebfe8422":"code","2ab13990":"code","6f485cbd":"code","2764ddbb":"code","89920c29":"code","bc67f875":"code","e8dd988b":"code","b8b75b4b":"code","005fcd3e":"code","32806ba8":"code","fb70d6a4":"code","cda6dd28":"code","bb3426d5":"code","7060ce93":"code","9a4a05c0":"code","d3111bf6":"code","2b5ef7ee":"code","64a3d35b":"code","a5cd27c1":"code","874c0ea8":"markdown","26a03d2a":"markdown","f2bb4488":"markdown","d0a8955a":"markdown"},"source":{"bc5ef9e8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","4eb66f57":"data = pd.read_csv('\/kaggle\/input\/creditcardfraud\/creditcard.csv')","9b7519fe":"data.head()","a71b5c64":"data.describe()","30791741":"data.Class.value_counts(normalize=True)","1c31e18a":"import matplotlib.pyplot as plt\ndata.hist(figsize= (20,18))\nplt.show()","fee93e3e":"X = data.iloc[:,:30]\nY = data.iloc[:,-1]","d8aff127":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X,Y,test_size= 0.3 , random_state= 42,stratify= Y,shuffle = True)\n","a219f3e8":"y_train.value_counts()","b759c367":"y_test.value_counts()","33051a77":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import accuracy_score,f1_score,precision_score,precision_recall_curve,recall_score,roc_auc_score,plot_precision_recall_curve,confusion_matrix","7ea27545":"def Model_comparision_report(x_train,x_test,y_train,y_test):\n    algorithms = []\n\n    algorithms.append((\"Stochastic Gradient Descent \",SGDClassifier()))\n    algorithms.append((\"Random Forest \", RandomForestClassifier()))\n    algorithms.append((\"Xtreme Gradient Boost \", XGBClassifier()))\n    algorithms.append((\"Bernoulli Naive Bayes \", BernoulliNB()))\n    algorithms.append((\"AdaBoost \", AdaBoostClassifier()))\n    algorithms.append((\"Extra Tress Classfier \", ExtraTreesClassifier()))\n    algorithms.append((\"Gradient Boosting Classifier\" ,GradientBoostingClassifier()))\n    algorithms.append((\"Bagging Classifier \", BaggingClassifier()))\n    algorithms.append((\"Multi-layer Preceptron\", MLPClassifier()))\n\n    names = []\n\n    train_f1 = []\n    test_f1 = []\n\n    train_recall = []\n    test_recall = []\n\n    train_precision = []\n    test_precision = []\n\n    train_auc = []\n    test_auc = []\n\n    train_acc = []\n    test_acc = []\n\n\n    for name , clf in algorithms:\n        clf.fit(x_train,y_train)\n        train_f1.append(f1_score(y_train,clf.predict(x_train)))\n        test_f1.append(f1_score(y_test,clf.predict(x_test)))\n        train_precision.append(precision_score(y_train,clf.predict(x_train)))\n        test_precision.append(precision_score(y_test,clf.predict(x_test)))\n        train_recall.append(recall_score(y_train,clf.predict(x_train)))\n        test_recall.append(recall_score(y_test,clf.predict(x_test)))\n        names.append(name)\n        train_auc.append(roc_auc_score(y_train,clf.predict(x_train)))\n        test_auc.append(roc_auc_score(y_test,clf.predict(x_test)))\n        train_acc.append(accuracy_score(y_train,clf.predict(x_train)))\n        test_acc.append(accuracy_score(y_test,clf.predict(x_test)))\n\n\n    clf_comparision = pd.DataFrame({\"Algorithms\": names, \"Train_Precision\" : train_precision,\n                                  \"Test_Precision\" : test_precision, \"Train_recall\" : train_recall,\n                                  \"Test_recall\": test_recall,\"Train_F1\" : train_f1,\"Test_F1\": test_f1,\n                                   \"Train_AUC\": train_auc, \"Test_AUC\": test_auc,\n                                   \"Train Accuracy\": train_acc, \"Test_Accuracy\":test_acc})\n    return clf_comparision","a060b310":"first_report = Model_comparision_report(x_train,x_test,y_train,y_test)\nfirst_report","7ea2fbce":"oversample = pd.concat([x_train,y_train],axis = 1)\nfraud = oversample.loc[oversample[\"Class\"] == 1]\nnot_fraud = oversample.loc[oversample[\"Class\"] == 0]","53f03c0d":"from sklearn.utils import resample\n\nfraud_samples = resample(fraud,n_samples = len(not_fraud),random_state = 27)","e1ac2c24":"fraud_samples.shape,not_fraud.shape","ced34be7":"new_data = pd.concat([fraud_samples,not_fraud], axis = 0)\nnew_data.shape","44458d1e":"x_train = new_data.iloc[:,:30]\ny_train = new_data.iloc[:,-1]\nx_train.shape,y_train.shape","9f710222":"x_test.shape,y_test.shape","67cac75c":"oversample_report = Model_comparision_report(x_train,x_test,y_train,y_test)\noversample_report","ebfe8422":"# replace = False because we reducing the majority class \nnfraud_samples = resample(not_fraud,replace= False,n_samples = len(fraud),random_state = 27)\n\nnfraud_samples.shape,fraud.shape,not_fraud.shape","2ab13990":"under_data = pd.concat([nfraud_samples,fraud],axis = 0)\nx_train = under_data.iloc[:,:30]\ny_train = under_data.iloc[:,30]\nx_train.shape,y_train.shape","6f485cbd":"undersample_report = Model_comparision_report(x_train,x_test,y_train,y_test)\nundersample_report","2764ddbb":"from imblearn.over_sampling import SMOTE","89920c29":"x_train,x_test,y_train,y_test = train_test_split(X,Y,test_size= 0.3 , random_state= 42,stratify= Y,shuffle = True)\nsm = SMOTE(random_state=27,k_neighbors=5)\nx_train , y_train = sm.fit_sample(x_train,y_train)","bc67f875":"\nx_train.shape,y_train.shape","e8dd988b":"y_train.value_counts()","b8b75b4b":"y_test.value_counts()","005fcd3e":"smote_report = Model_comparision_report(x_train,x_test,y_train,y_test)\nsmote_report","32806ba8":"xtree_clf = ExtraTreesClassifier()\nxtree_clf.fit(x_train,y_train)\nplot_precision_recall_curve(xtree_clf,x_test,y_test)\nplt.show()","fb70d6a4":"train_prob = xtree_clf.predict_proba(x_test)\np,r,t = precision_recall_curve(y_test,train_prob[:,1])","cda6dd28":"prt = pd.DataFrame({'precision':p[:-1],'recall':r[:-1],'threshold':t},columns=['precision','recall','threshold'])","bb3426d5":"prt.loc[(prt['recall'] > 0.8) & (prt['precision'] > 0.8)].sort_values(by=['recall'],ascending = False)","7060ce93":"y_pred_threshold = (train_prob[:,1] >=  0.37)","9a4a05c0":"precision_score(y_test,y_pred_threshold)","d3111bf6":"recall_score(y_test,y_pred_threshold)","2b5ef7ee":"accuracy_score(y_test,y_pred_threshold)","64a3d35b":"f1_score(y_test,y_pred_threshold)","a5cd27c1":"confusion_matrix(y_test,y_pred_threshold)","874c0ea8":"# Fraud Prediction with Skewed Dataset\n1. Here will see how to deal with imbalanced dataset\n2. What performance metric we should choose\n3. How to deal with highly skewed dataset\n4. Perform Precision-Recall trade off\n5. Chosing best model from all - with better visulaisation of model performance","26a03d2a":"# Conclusion\n* When we have imbalanced dataset - like this highly skewed then Oversampling and SMOTE really helps our model in terms of F1 score\n* We should intially split our dataset and apply sampling techniques only on train set\n* We should always test our model in original test dataset which does not have samples created by us\n* We have seen that RandomForest Classifier\/Extra Trees classifier has the best results among the other powerful classifier\n* We see the accuracy is not a good meteric for evaluation for imbalanced dataset\n* F1 score and Precision-recall trade off or curve is better perfoamce metric for highly skewed data\n* For Extra Trees 0.37 is the threshold which gives highest F1 score of 0.8243\n* We can further play around and optimize the RandomForest Classifier\/Extra Tree clasifier as both gave same results\n* Sometimes Neural Network model does not perform well as compared to other ensemble models","f2bb4488":"# Three techniques:\n1. Oversampling : Generating more minority samples\n2. Undersampling : Reducing Majority class samples to number of minority samples\n3. SMOTE : Synthetic data generation","d0a8955a":"# ** ****Base Model - without any sampling**"}}