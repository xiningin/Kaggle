{"cell_type":{"0048665b":"code","2dbe9fe9":"code","28053999":"code","78d3e1c4":"code","5c3d75bc":"code","146a329b":"code","31635aee":"code","bbfd8547":"code","e717dc38":"code","557621af":"code","c9b981cb":"code","f552a6a4":"code","17f2c1af":"code","0a32090e":"code","4be536f1":"code","da164085":"code","7aefb823":"code","6d46d1d1":"code","ec9c3d00":"code","a639d342":"markdown","ad4f7f3d":"markdown","5a95a867":"markdown","61e5b539":"markdown","4eedb014":"markdown","d10bf78c":"markdown","88bdee71":"markdown","325ec8b4":"markdown","c716f934":"markdown","edaa4866":"markdown","2191c33d":"markdown","5e28685a":"markdown","421de894":"markdown","16989b62":"markdown"},"source":{"0048665b":"## Make sure internet is enabled!\n!pip install pyspark==2.4","2dbe9fe9":"# Imports\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# PySpark imports\nfrom pyspark.sql import SparkSession\nimport pyspark.sql.functions as F","28053999":"# 'local[*]' means use all available cores in the local machine\nspark = SparkSession \\\n    .builder \\\n    .appName(\"Hello spark.ml\") \\\n    .master('local[*]') \\\n    .getOrCreate()","78d3e1c4":"spark","5c3d75bc":"train_fname = r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_fname = r'\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\n\ndf = spark.read.csv(train_fname, header=True, inferSchema=True, nullValue='NA').repartition(4).persist()\ndf_test = spark.read.csv(test_fname, header=True, inferSchema=True, nullValue='NA').repartition(4).persist()","146a329b":"df.count(), df_test.count()","31635aee":"\ndf.select('SalePrice').summary(\"count\", \"mean\", \"stddev\", \n                               \"min\", \"5%\", \"25%\", \"50%\", \"75%\", \"95%\", \"max\").show()","bbfd8547":"df.select('SalePrice').sample(False, 0.5).toPandas().hist()","e717dc38":"# Convert MSSubClass to a string because it is a categorical field\ndf = df.withColumn('MSSubClass', F.col('MSSubClass').cast('string'))\ndf_test = df_test.withColumn('MSSubClass', F.col('MSSubClass').cast('string'))","557621af":"nominal_fields = ['MSSubClass',\n 'MSZoning',\n 'Street',\n 'Alley',\n 'LandContour',\n 'Utilities',\n 'LotConfig',\n 'Neighborhood',\n 'Condition1',\n 'Condition2',\n 'BldgType',\n 'HouseStyle',\n 'RoofStyle',\n 'RoofMatl',\n 'Exterior1st',\n 'Exterior2nd',\n 'MasVnrType',\n 'Foundation',\n 'Heating',\n 'CentralAir',\n 'Electrical',\n 'Functional',\n 'GarageType',\n 'PavedDrive',\n 'MiscFeature',\n 'SaleType',\n 'SaleCondition']\n\nordinal_fields = [\n 'LotShape',\n 'LandSlope',\n 'OverallQual',\n 'OverallCond',\n 'ExterQual',\n 'ExterCond',\n 'BsmtQual',\n 'BsmtCond',\n 'BsmtExposure',\n 'BsmtFinType1',\n 'BsmtFinType2',\n 'HeatingQC',\n 'KitchenQual',\n 'FireplaceQu',\n 'GarageFinish',\n 'GarageQual',\n 'GarageCond',\n 'PoolQC',\n 'Fence'   \n]\n\ncategorical_fields = [\n 'MSSubClass'\n]\n\ncount_fields = [\n 'BsmtFullBath',\n 'BsmtHalfBath',\n 'FullBath',\n 'HalfBath',\n 'BedroomAbvGr',\n 'KitchenAbvGr',\n 'TotRmsAbvGrd',\n 'Fireplaces',\n 'GarageCars'\n]\n\ntimeseries_fields = [\n 'MoSold',\n 'YrSold'\n]\n\ncontinuous_fields = [\n 'LotFrontage',\n 'LotArea',\n 'YearBuilt',\n 'YearRemodAdd',\n 'MasVnrArea',\n 'BsmtFinSF1',\n 'BsmtFinSF2',\n 'BsmtUnfSF',\n 'TotalBsmtSF',\n '1stFlrSF',\n '2ndFlrSF',\n 'LowQualFinSF',\n 'GrLivArea',\n 'GarageYrBlt',\n 'GarageArea',\n 'WoodDeckSF',\n 'OpenPorchSF',\n 'EnclosedPorch',\n '3SsnPorch',\n 'ScreenPorch',\n 'PoolArea',\n 'MiscVal',\n ]\n\ntarget_fields = [\n 'SalePrice'\n]\n\nid_fields = [\n    'Id'\n]","c9b981cb":"def describe_categorical(df, column, target='SalePrice', numRows=20):\n    df.groupby(column).agg(F.count('ID').alias('count'), \n                           F.round(F.mean(target)).alias('mean'), \n                           F.round(F.stddev(target)).alias('stddev'),\n                           F.min(target).alias('min'), \n                           F.max(target).alias('max')\n                          ).orderBy('count', ascending=False).show(numRows)\n    ","f552a6a4":"for column, typ in df.dtypes:\n    print(column)\n    describe_categorical(df, column)","17f2c1af":"def scatter_plot(df, x, y='SalePrice', sampling_rate=1.0):\n    df.select(x, y).sample(False, sampling_rate).toPandas().plot.scatter(x=x, y=y, title=\"{} vs {}\".format(x,y))","0a32090e":"for field in continuous_fields:\n    scatter_plot(df, field)","4be536f1":"def box_plot(df, x, y='SalePrice', sampling_rate=1.0):\n    pdf = df.select(x, y).sample(sampling_rate).toPandas()\n    sns.catplot(x=x, y=y, kind=\"box\", data=pdf)\n    plt.gca().set_title(\"{} vs {}\".format(x,y))","da164085":"for field in ordinal_fields:\n    box_plot(df, field)","7aefb823":"# Handle Outliers","6d46d1d1":"#df = df.withColumn('IsNew', F.col('SaleType') == 'New')","ec9c3d00":"# Drop fields with more than 10% missing values","a639d342":"## Analyze the Target Variable\n\nThe target variable is `SalePrice`","ad4f7f3d":"#### Handle missing values","5a95a867":"# Hello spark.ml\nThis is a notebook that shows an example of using pyspark for data analysis and modeling. This notebook will use regression methods to predict home prices. It is not practical to use pyspark for this dataset because it is so small, but hopefully it should serve as a guide on how to complete a simple machine learning project with spark.\n![Spark Logo](https:\/\/spark.apache.org\/images\/spark-logo-trademark.png)","61e5b539":"## Feature Engineering","4eedb014":"Importing libraries","d10bf78c":"## Read in Data\nData can be read in with an inferred schema. For big data it would be recommended to define the schema explicitly because inferring the schema requires two passes reading the data.\n\nThe dataframe is read into only one partition by default because it is so small. The data is repartitioned into four partitions for demonstation purposes.","88bdee71":"## Analyze Input Variables","325ec8b4":"## Data Prep","c716f934":"#### Single Variable Analysis with Target","edaa4866":"## Setup\nThe following code will set up the environment with the required software\n* Java 8 (already installed)\n* Spark (already installed)\n* pyspark (to be installed, [make sure internet is enabled](https:\/\/www.kaggle.com\/questions-and-answers\/36982))","2191c33d":"Compute basic summary statistics with the `summary` function","5e28685a":"#### Plot a histogram of the variable.\nThis is an example of converting a subset of the pyspark dataframe to a pandas dataframe to do a statistical function. Although it is possible to compute histograms in a pyspark dataframe, it is not as simplistic as using pandas.","421de894":"#### Create spark session using local resources","16989b62":"## Data Cleaning"}}