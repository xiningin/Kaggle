{"cell_type":{"22379bd8":"code","32da64d4":"code","7ad4741b":"code","b8cc70f6":"code","66b98301":"code","07bbb971":"code","470e2236":"code","7324be0b":"code","13f1ee12":"code","ebaeff3d":"code","45abe082":"code","7ed45bc9":"code","a9a13b41":"code","e62975e0":"code","828669ca":"code","2e9c2aca":"code","da8c6519":"code","d2aa7f4a":"code","d9be9b3b":"markdown","e761972b":"markdown","01bfb08e":"markdown","751d46ba":"markdown","f928e3cf":"markdown","c858b23a":"markdown","7116c9a1":"markdown","b08a2526":"markdown"},"source":{"22379bd8":"import numpy as np\nimport pandas as pd\nimport skimage.io\nimport matplotlib.pyplot as plt","32da64d4":"#import training data\ntrain = pd.read_csv(\"..\/input\/train.csv\")","7ad4741b":"train.head()","b8cc70f6":"path_to_train = '..\/input\/train\/'\ndef load_image(file):\n    image_red_ch = skimage.io.imread(path_to_train+file+'_red.png')\n    image_yellow_ch = skimage.io.imread(path_to_train+file+'_yellow.png')\n    image_green_ch = skimage.io.imread(path_to_train+file+'_green.png')\n    image_blue_ch = skimage.io.imread(path_to_train+file+'_blue.png')\n    image = np.stack((image_green_ch, image_red_ch, image_blue_ch, image_yellow_ch))\n    return image","66b98301":"from skimage.filters import threshold_otsu","07bbb971":"def threshod_image(img):\n    bw_img = np.zeros_like(img, dtype=bool)\n    for i, arr in enumerate(img):\n        bw_img[i] = arr > threshold_otsu(arr)\n    return bw_img","470e2236":"def mask_green(bw_img):\n    mask_img_red = bw_img[0] & bw_img[1]\n    mask_img_blue = bw_img[0] & bw_img[2]\n    mask_img_yellow = bw_img[0] & bw_img[3]\n    return np.stack((bw_img[0], mask_img_red, mask_img_blue, mask_img_yellow))","7324be0b":"def compute_ratios(mask_img):\n    ratios = []\n    for i in range(1,mask_img.shape[0]):\n        ratios.append(mask_img[i].sum()\/mask_img[0].sum())\n    return ratios","13f1ee12":"def transform(file):\n    a = load_image(file)\n    bw_img = threshod_image(a)\n    mask_img = mask_green(bw_img)\n    return compute_ratios(mask_img)","ebaeff3d":"from sklearn.preprocessing import MultiLabelBinarizer","45abe082":"def dataset(size=100):\n    targets = []\n    features = []\n    c = 0\n    for i, row in train.sample(size, random_state=1).iterrows():\n        c+=1\n        targets.append([int(x) for x in row[1].split(' ')])\n        features.append(transform(row[0]))\n        if c % 10 == 0:\n            print(\"Processing %.2f\" % ((c*100)\/size), end='\\r')\n    return np.array(features), MultiLabelBinarizer().fit_transform(targets)","7ed45bc9":"features, targets = dataset(1000)","a9a13b41":"features.shape","e62975e0":"targets.shape","828669ca":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score","2e9c2aca":"neigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(features[:700], targets[:700]) ","da8c6519":"preds = neigh.predict(features[700:])","d2aa7f4a":"f1_score(targets[700:], preds, average='macro')","d9be9b3b":"### Multilabel classifier\n","e761972b":"### Import 4 channels into numpy array","01bfb08e":"### Mask the green channel","751d46ba":"### Threshold","f928e3cf":"### Feature engineering\nAs a baseline model, I would like to perform some basic feature engeneering to predict the localization of the protein of interest. The features will be the overlaping pixels of the green channel (the protein of interest) vs the other channels (red,blue, yellow).","c858b23a":"### Define Target","7116c9a1":"For each files:\n* import the 4 channels.\n* threshold of the channels\n* convert to black and white pictures\n* mask the green picture with other channels\n* compute the ratio of unmasked green vs total green.","b08a2526":"### Ratios"}}