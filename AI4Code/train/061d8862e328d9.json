{"cell_type":{"333c8d56":"code","5f842b27":"code","2aade976":"code","47914488":"code","411e7878":"code","721af8cd":"code","b5f29759":"code","0af13c43":"code","9629ef3a":"code","b18d26a7":"code","8ed5ff20":"code","e1653a46":"code","1bb35f24":"code","37e085da":"markdown","54ee6ab0":"markdown"},"source":{"333c8d56":"from  __future__ import print_function, division\nfrom builtins import range","5f842b27":"import os\nimport sys\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, GlobalMaxPool1D\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding\nfrom keras.models import Model\nfrom sklearn.metrics import roc_auc_score","2aade976":"MAX_SEQUENCE_LENGTH = 100\nMAX_VOCAB_SIZE = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SLIT = 0.2\nBATCH_SIZE = 128\nEPOCHS = 10","47914488":"word2vec = {}\nwith open(\"..\/input\/glove.6B.100d.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vec = np.asarray(values[1:], dtype='float32')\n        word2vec[word] = vec\nprint('Found %s word vectors.' % len(word2vec))","411e7878":"train = pd.read_csv(\"..\/input\/jigsaw-toxic-comment-classification-challenge\/train.csv\")\nsentences = train['comment_text'].fillna('DUMMY_VALUE').values\npossible_label = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\ntargets = train[possible_label].values\n\nprint(\"max sequence lenght\", max(len(s) for s in sentences))\nprint(\"min sequence lenght\", min(len(s) for s in sentences))\ns = sorted(len(s) for s in sentences)\nprint(\"median sequence length\", s[len(s) \/\/ 2])","721af8cd":"tokenizer = Tokenizer(num_words = MAX_VOCAB_SIZE)\ntokenizer.fit_on_texts(sentences)\nsequences = tokenizer.texts_to_sequences(sentences)","b5f29759":"word2idx = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word2idx))","0af13c43":"data  = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nprint(\"Shape of data tensor: \", data.shape)","9629ef3a":"print('Pre-trained embedings')\nnum_words = min(MAX_VOCAB_SIZE, len(word2idx) + 1)\nembedding_matrix = np.zeros((num_words, EMBEDDING_DIM))\nfor word, i in word2idx.items():\n    if i < MAX_VOCAB_SIZE:\n        embedding_vector = word2vec.get(word)\n        if embedding_vector is not None:\n            embedding_matrix[i] = embedding_vector","b18d26a7":"embedding_layer = Embedding(\n    num_words,\n    EMBEDDING_DIM,\n    weights = [embedding_matrix],\n    input_length = MAX_SEQUENCE_LENGTH,\n    trainable = False\n)","8ed5ff20":"print('start building ...')\ninput_ = Input(shape=(MAX_SEQUENCE_LENGTH, ))\nx = embedding_layer(input_)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = MaxPooling1D(3)(x)\nx = Conv1D(128, 3, activation='relu')(x)\nx = GlobalMaxPool1D()(x)\nx = Dense(128, activation='relu')(x)\noutput = Dense(len(possible_label), activation= 'sigmoid')(x)","e1653a46":"model = Model(input_, output)\nmodel.compile(\n    loss=\"binary_crossentropy\",\n    optimizer='rmsprop',\n    metrics=['accuracy']\n)","1bb35f24":"print('Training model...')\nr = model.fit(\n    data,\n    targets,\n    batch_size=BATCH_SIZE,\n    epochs=EPOCHS,\n    validation_split=VALIDATION_SLIT\n)","37e085da":"### get word -> integer mapping","54ee6ab0":"### covert the sentence from string into integer"}}