{"cell_type":{"8517a0a5":"code","a9e7ed6b":"code","9c0b0eae":"code","12de28cc":"code","a55a10bb":"code","71b07b19":"code","a82c5ed3":"code","add2c7e8":"code","004a65b2":"code","2aaca0e2":"code","7fe102a1":"code","33367145":"code","af481263":"code","73aa6ec6":"code","291bd44a":"code","619c0e44":"code","50bb929a":"code","627bea93":"code","9552e938":"code","6f77cdf5":"code","6a373fba":"code","302839d8":"code","2d8b7b1d":"code","54fa353a":"code","439cd8c3":"code","8c12bffb":"code","ff403c3f":"code","cdddf672":"code","890c0a1f":"code","7d64177e":"code","c1ba8b74":"code","40a6c6ab":"code","90dd0687":"code","8308ef97":"code","2f61f050":"code","9b9586f1":"code","22adca9a":"code","f2e653cd":"code","064cd0ca":"code","ade5e25d":"code","3c048185":"code","9879232f":"code","1d54e22f":"code","6f18df19":"code","6f649a3b":"code","5b81aa20":"code","a811014c":"code","524e9b77":"code","58389824":"code","ea4a79f0":"code","1f72aa0b":"code","37baa468":"code","2ffed7f1":"code","28921dec":"code","6877b180":"code","fdea7d4e":"code","de2cedc0":"code","7c7cecab":"code","ef984140":"code","b490a77d":"code","b3b24356":"code","a9997849":"code","f44f1b30":"code","08d328fa":"markdown","ddb3c97d":"markdown","32035172":"markdown","26ea52fa":"markdown","49e8e685":"markdown","a0f7d081":"markdown","a5069b9c":"markdown","1e58bedb":"markdown","13151a7e":"markdown","d40936d7":"markdown","c36af20a":"markdown","256dbee9":"markdown","0b0eecfa":"markdown","0f6aeb8a":"markdown","d133a6a2":"markdown","c97bebca":"markdown","af668328":"markdown","b1a46938":"markdown","11d68f60":"markdown"},"source":{"8517a0a5":"# IMPORTING PACKAGES\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\n#from scipy.stats import norm\nimport seaborn as sns\nsns.set(context = 'paper', palette = 'winter_r', style = 'darkgrid',\nrc= {'figure.facecolor': 'gray',}, font_scale=1.5)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.svm import SVC","a9e7ed6b":"# LOADING THE DATASET\ndf = pd.DataFrame\ndf = pd.read_csv('..\/input\/m1-data\/M1_data.csv',delimiter=\",\")","9c0b0eae":"df","12de28cc":"# RENAMING THE DATAFRAME\napple_df =df","a55a10bb":"# LOOKING FOR UNIQUE VALUE ON STATUS\napple_df['status'].unique()","71b07b19":"# LOOKING FOR UNIQUE VALUE ON TYPE OF USER\napple_df['user_pcmac'].unique()","a82c5ed3":"# LOOKING FOR UNIQUE DOMAIN OF USERS\napple_df['domain'].unique()","add2c7e8":"# CHECKING FOR NULL VALUES\napple_df.isnull().sum()","004a65b2":"# LABEL ENCODING\n\napple_df['trust_apple'] = apple_df['trust_apple'].replace(['Yes','No'],['1','0'])\napple_df['user_pcmac'] = apple_df['user_pcmac'].replace(['PC','Apple','Hp','Other'],['1','0','2','2'])\napple_df['familiarity_m1'] = apple_df['familiarity_m1'].replace(['Yes','No'],['1','0'])\napple_df['m1_purchase'] = apple_df['m1_purchase'].replace(['Yes','No'],['1','0'])\napple_df['gender'] = apple_df['gender'].replace(['Male','Female'],['1','0'])\napple_df['status'] = apple_df['status'].replace(['Male','Female'],['1','0'])\napple_df['gender'] = apple_df['gender'].replace(['Male','Female'],['1','0'])\napple_df['status'] = apple_df['status'].replace(['Student','Employed','Retired','Student ant employed','Unemployed','Self-Employed'],['0','1','2','1','3','4'])\napple_df['domain'] = apple_df['domain'].replace(['Science','Finance','IT & Technology','Arts & Culture','Hospitality','Politics','Social Sciences','Administration & Public Services','Education','Engineering','Marketing','Healthcare','Business','Retired','Economics','Law','Agriculture','Communication ','Realestate','Logistics','Consulting ','Retail'],['0','1','2','3','4','5','6','7','8','9','10','11','12','21','13','14','15','16','17','18','29','20'])\n\napple_df","2aaca0e2":"# FIST LOOK AT THE DAT TYPES\napple_df.info()","7fe102a1":"# COVERING DTYPES \n\napple_df['trust_apple'] = pd.to_numeric(apple_df['trust_apple'])\napple_df['user_pcmac'] = pd.to_numeric(apple_df['user_pcmac'])\napple_df['familiarity_m1'] = pd.to_numeric(apple_df['familiarity_m1'])\napple_df['m1_purchase'] = pd.to_numeric(apple_df['m1_purchase'])\napple_df['gender'] = pd.to_numeric(apple_df['gender'])\napple_df['status'] = pd.to_numeric(apple_df['status'])\napple_df['domain'] = pd.to_numeric(apple_df['domain'])\n\napple_df.dtypes","33367145":"# PERFORMING STATISTICS\napple_df.describe()","af481263":"# OVERVIEW OF CORRELATION \napple_df.corr()","73aa6ec6":"apple_df.shape","291bd44a":"# VISUALIZING CORRELATION MATRIX (HEATMAP 1)\n\nplt.figure(figsize=(16, 6))\nheatmap = sns.heatmap(apple_df.corr(), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':18}, pad=25);\n# save heatmap as .png file\n# dpi - sets the resolution of the saved image in dots\/inches\n# bbox_inches - when set to 'tight' - does not allow the labels to be cropped\n# plt.savefig('heatmap.png', dpi=300, bbox_inches='tight')","619c0e44":"# FIRST LOOK AT CORRELATION MATRIX (HEATMAP 2)\n\ncorrelation = apple_df.corr()\nplt.figure(1,figsize = (18,18))\nsns.heatmap(correlation,annot=True)","50bb929a":"# VISUALIZING DEPENDENT VARIABLE\n\ncount_Class= pd.value_counts(apple_df[\"m1_purchase\"], sort= True)\ncount_Class.plot(kind= 'bar')","627bea93":"# VISUALIZING PRICE\n\ncount_Class= pd.value_counts(apple_df[\"f_price\"], sort= True)\ncount_Class.plot(kind= 'bar')","9552e938":"# VISUALIZING AGE GROUP\n\ncount_Class= pd.value_counts(apple_df[\"age_group\"], sort= True)\ncount_Class.plot(kind= 'bar')","6f77cdf5":"# VISUALIZING AGE GROUP\n\nsns.distplot(apple_df['age_group'] , bins = 10)","6a373fba":"# VISUALIZING GENDER \n\ncount_Class= pd.value_counts(apple_df[\"gender\"], sort= True)\ncount_Class.plot(kind= 'bar')","302839d8":"# VISUALIZING PRODUCT TRUST\n\ncount_Class= pd.value_counts(apple_df[\"trust_apple\"], sort= True)\ncount_Class.plot(kind= 'bar')","2d8b7b1d":"# view column names again\n\napple_df.columns","54fa353a":"# check distribution of target_class column\n\ndf['m1_purchase'].value_counts()","439cd8c3":"# view the percentage distribution of target_class column\n\ndf['m1_purchase'].value_counts()\/np.float(len(df))","8c12bffb":"apple_df.columns","ff403c3f":"# Purchase correlation\n\napple_df.sort_values(by=[\"m1_purchase\"],ascending=False).iloc[0].sort_values(ascending=False)","cdddf672":"# plot histogram for top 8 correlated features to check distribution\n\n\nplt.figure(figsize=(24,20))\n\n\nplt.subplot(4, 2, 1)\nfig = apple_df['age_computer'].hist(bins=20)\nfig.set_xlabel('age_computer')\nfig.set_ylabel('m1_purchase')\n\n\nplt.subplot(4, 2, 2)\nfig = df['f_batterylife'].hist(bins=20)\nfig.set_xlabel('f_batterylife')\nfig.set_ylabel('m1_purchase')\n\n\nplt.subplot(4, 2, 3)\nfig = df['f_multitasking'].hist(bins=20)\nfig.set_xlabel('f_multitasking')\nfig.set_ylabel('m1_purchase')\n\n\n\nplt.subplot(4, 2, 4)\nfig = df['interest_computers'].hist(bins=20)\nfig.set_xlabel('interest_computers')\nfig.set_ylabel('m1_purchase')\n\n\n\nplt.subplot(4, 2, 5)\nfig = df['f_noise'].hist(bins=20)\nfig.set_xlabel('f_noise')\nfig.set_ylabel('m1_purchase')\n\n\n\nplt.subplot(4, 2, 6)\nfig = df['f_price'].hist(bins=20)\nfig.set_xlabel('f_price')\nfig.set_ylabel('m1_purchase')\n\n\n\nplt.subplot(4, 2, 7)\nfig = df['f_size'].hist(bins=20)\nfig.set_xlabel('f_size')\nfig.set_ylabel('m1_purchase')\n\n\nplt.subplot(4, 2, 8)\nfig = df['age_group'].hist(bins=20)\nfig.set_xlabel('age_group')\nfig.set_ylabel('m1_purchase')\n\n\n\n","890c0a1f":"# plot histogram for other correlated features to check distribution\n\nplt.figure(figsize=(24,20))\n\nplt.subplot(4, 2, 1)\nfig = df['f_neural'].hist(bins=20)\nfig.set_xlabel('f_neural')\nfig.set_ylabel('m1_purchase')\n\nplt.subplot(4, 2, 2)\nfig = df['f_performance'].hist(bins=20)\nfig.set_xlabel('f_performance')\nfig.set_ylabel('m1_purchase')\n\nplt.subplot(4, 2, 3)\nfig = df['income_group'].hist(bins=20)\nfig.set_xlabel('income_group')\nfig.set_ylabel('m1_purchase')\n\n\nplt.subplot(4, 2, 4)\nfig = df['user_pcmac'].hist(bins=20)\nfig.set_xlabel('user_pcmac')\nfig.set_ylabel('m1_purchase')\n\nplt.subplot(4, 2, 5)\nfig = df['f_synergy'].hist(bins=20)\nfig.set_xlabel('f_synergy')\nfig.set_ylabel('m1_purchase')\n\n\nplt.subplot(4, 2, 6)\nfig = df['f_performanceloss'].hist(bins=20)\nfig.set_xlabel('f_performanceloss')\nfig.set_ylabel('m1_purchase')\n\nplt.subplot(4, 2, 7)\nfig = df['m1_consideration'].hist(bins=20)\nfig.set_xlabel('m1_consideration')\nfig.set_ylabel('m1_purchase')\n\nplt.subplot(4, 2, 8)\nfig = df['gender'].hist(bins=20)\nfig.set_xlabel('gender')\nfig.set_ylabel('m1_purchase')\n\n","7d64177e":"# Selecting feature vector and target variable\n\nX = apple_df.drop(['m1_purchase'], axis=1)\ny = apple_df['m1_purchase']\n","c1ba8b74":"# split X and y into training and testing sets\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\n# check the shape of X_train and X_test\n\nX_train.shape, X_test.shape","40a6c6ab":"# SELECTING ALL COLUMNS\n\ncols = X_train.columns","90dd0687":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","8308ef97":"X_train = pd.DataFrame(X_train, columns=[cols])\n","2f61f050":"X_test = pd.DataFrame(X_test, columns=[cols])","9b9586f1":"X_train.describe()","22adca9a":"# import SVC classifier\nfrom sklearn.svm import SVC\n\n\n# import metrics to compute accuracy\nfrom sklearn.metrics import accuracy_score\n\n\n# instantiate classifier with default hyperparameters\nsvc=SVC() \n\n\n# fit classifier to training set\nsvc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with default hyperparameters: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","f2e653cd":"# instantiate classifier with linear kernel and C=1.0\nlinear_svc=SVC(kernel='linear', C=1.0) \n\n\n# fit classifier to training set\nlinear_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred_test=linear_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with linear kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred_test)))","064cd0ca":"# Compare the train-set and test-set accuracy\u00b6\n\ny_pred_train = linear_svc.predict(X_train)\n\ny_pred_train","ade5e25d":"# print the scores on training and test set\n\nprint('Training set score: {:.4f}'.format(linear_svc.score(X_train, y_train)))\n\nprint('Test set score: {:.4f}'.format(linear_svc.score(X_test, y_test)))","3c048185":"# instantiate classifier with polynomial kernel and C=1.0\n\npoly_svc=SVC(kernel='poly', C=1.0) \n\n\n# fit classifier to training set\npoly_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","9879232f":"# instantiate classifier with polynomial kernel and C=100.0\npoly_svc100=SVC(kernel='poly', C=100.0) \n\n\n# fit classifier to training set\npoly_svc100.fit(X_train, y_train)\n\n\n# make predictions on test set\ny_pred=poly_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with polynomial kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","1d54e22f":"# instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n\n\n# fit classifier to training set\nsigmoid_svc.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","6f18df19":"# instantiate classifier with sigmoid kernel and C=100.0\nsigmoid_svc100=SVC(kernel='sigmoid', C=100.0) \n\n\n# fit classifier to training set\nsigmoid_svc100.fit(X_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc100.predict(X_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=100.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","6f649a3b":"# Printing the Confusion Matrix and slice it into four pieces\n\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test, y_pred_test)\n\nprint('Confusion matrix\\n\\n', cm)\n\nprint('\\nTrue Positives(TP) = ', cm[0,0])\n\nprint('\\nTrue Negatives(TN) = ', cm[1,1])\n\nprint('\\nFalse Positives(FP) = ', cm[0,1])\n\nprint('\\nFalse Negatives(FN) = ', cm[1,0])","5b81aa20":"# visualize confusion matrix with seaborn heatmap\n\ncm_matrix = pd.DataFrame(data=cm, columns=['Actual Positive:1', 'Actual Negative:0'], \n                                 index=['Predict Positive:1', 'Predict Negative:0'])\n\nsns.heatmap(cm_matrix, annot=True, fmt='d', cmap='YlGnBu')","a811014c":"from sklearn.metrics import classification_report\n\nprint(classification_report(y_test, y_pred_test))","524e9b77":"TP = cm[0,0]\nTN = cm[1,1]\nFP = cm[0,1]\nFN = cm[1,0]","58389824":"# print classification accuracy\n\nclassification_accuracy = (TP + TN) \/ float(TP + TN + FP + FN)\n\nprint('Classification accuracy : {0:0.4f}'.format(classification_accuracy))","ea4a79f0":"# print classification error\n\nclassification_error = (FP + FN) \/ float(TP + TN + FP + FN)\n\nprint('Classification error : {0:0.4f}'.format(classification_error))","1f72aa0b":"#running feature importance with DecisionTreeClassifier\n#entropy is used to measure disorder in the columns\n#max_depth defines the maximum depth of the tree\/max features\n\nfrom sklearn.tree import DecisionTreeClassifier\ndt = DecisionTreeClassifier(random_state = 15, criterion = 'entropy', max_depth = 10)\ndt.fit(X,y)","37baa468":"#creating empty lists to append the values \ncol = []\nfi = []\n\n#the parameter axis=1 refer to columns, while 0 refers to rows\nfor i, column in enumerate(df.drop('m1_purchase', axis = 1)):\n  col.append(column)\n  fi.append(dt.feature_importances_[i])\n\n#displaying fi_df as a new dataframe displaying the feature importances \nfi_df = zip(col, fi)\nfi_df = pd.DataFrame(fi_df, columns = ['feature', 'feature_importance'])\n\n#sorting the feature importance in ascending order\nfi_df = fi_df.sort_values('feature_importance', ascending = False).reset_index()\n\n#the last 5 columns have no relation to y at all\nfi_df","2ffed7f1":"#keep our 10 most important features (x)\nimp_col = fi_df['feature'][0:10]\n\n#original dataframe \nprint(apple_df.shape)\n\n#new dataframe with only the features we want (x)\nprint(apple_df[imp_col].shape)","28921dec":"#displaying 10 'most important' features that we decided to use\nimp_col","6877b180":"#displaying the 10 most important features with data\nm1_df = df[imp_col]","fdea7d4e":"X = m1_df.values\nX.astype(int)\n\ny = np.array(apple_df['m1_purchase']).reshape(-1,1)\ny = y.astype(int)\n\nprint(X.shape)\nprint(y.shape)","de2cedc0":"#creating a training and test set \n#the train and test size is set to the same size as it gave the best accuracy score\nx_train, x_test, y_train, y_test = train_test_split(X, y, train_size = 0.20, test_size = 0.20,random_state=25)\n\n#scaling and normalizing the data (this also imporved the score)\nsc = StandardScaler() \nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)","7c7cecab":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\nX_train = scaler.fit_transform(X_train)\n\nX_test = scaler.transform(X_test)","ef984140":"#checking the shape of the train and test set\nprint(y_train.shape)\nprint(x_train.shape)\nprint(y_test.shape)\nprint(x_test.shape)","b490a77d":"# instantiate classifier with sigmoid kernel and C=1.0\nsigmoid_svc=SVC(kernel='sigmoid', C=1.0) \n\n\n# fit classifier to training set\nsigmoid_svc.fit(x_train,y_train)\n\n\n# make predictions on test set\ny_pred=sigmoid_svc.predict(x_test)\n\n\n# compute and print accuracy score\nprint('Model accuracy score with sigmoid kernel and C=1.0 : {0:0.4f}'. format(accuracy_score(y_test, y_pred)))","b3b24356":"print(f'\\nPredicted m1 purchases: {y_pred}')\nprint(f'Real m1 purchases: {y_test.ravel()}')","a9997849":"#confusion materix \nconfmtrx = np.array(confusion_matrix(y_test, y_pred))\n\n\ncm = pd.DataFrame(confmtrx, index=['Actual Not Buyers', 'Actual Buyers'],\ncolumns=['Predicted Not Buyers', 'Predicted Buyers'])\ncm","f44f1b30":"#calculating False Positives (FP), False Negatives (FN), True Positives (TP) & True Negatives (TN)\n\nFP = confmtrx.sum(axis=0) - np.diag(confmtrx)\nFN = confmtrx.sum(axis=1) - np.diag(confmtrx)\nTP = np.diag(confmtrx)\nTN = confmtrx.sum() - (FP + FN + TP)\n\n#true positive rate (sensitivity)\nTPR = TP \/ (TP + FN)\n\n#true negative (precision)\nTNR = TP \/ (TP + FP)\n\n#false positive rate or False alarm rate\nFPR = FP \/ (FP + TN)\n\n#false negative rate or Miss Rate\nFNR = FN \/ (FN + TP)\n\n#the averages\nprint(f'The average True Positive Rate is: {round((TPR.sum()\/2)*100, 2)} %')\nprint(f'The average True Negative Rate is: {round((TNR.sum()\/2)*100, 2)} %')\nprint(f'The average False Positive Rate is: {round((FPR.sum()\/2)*100, 2)} %')\nprint(f'The average False Negative Rate is: {round((FNR.sum()\/2)*100, 2)} %')\n","08d328fa":"## Classification metrices\n\n\n#### Classification Report\n\n- Classification report is another way to evaluate the classification model performance. It displays the precision, recall, f1 and support scores for the model. I have described these terms in later.\n\nWe can print a classification report as follows:","ddb3c97d":"#### From the above models,we can see that sigmoid kernel performance is by far the best in terms of model accuracy when \n\n- When C = 1 an accuracy of 81 % is achieved \n- when C = 100 an accuract of 85 % is achieved\n\n","32035172":"## Feature Importance & Tuning","26ea52fa":"# Final Preciction","49e8e685":"# Final Confusion Matrix","a0f7d081":"## Finally Model Preparation","a5069b9c":"##  SVM with linear kernel ","1e58bedb":"# Confusion Matrix Score","13151a7e":"## SVM with sigmoid kernel with C = 1.1","d40936d7":"In the small training and testing sets used, the algorithm classified the data instance wrong 4\/27 and correct 23\/27 times as shown in the confusion matrix. To further increase the accuracy of this model, hyper parameter tuning would need to be applied. It is also worth noting that even with optimal parameters the model might not perform better due to the data being insufficient.\n\nHope you enjoyed the noteboook!\n\n\nTo compare accuracy scores kindly have a look into - https:\/\/www.kaggle.com\/hedvig\/predictive-marketing-classification-on-apple-data\/comments\n\n\nFeel free to comment and hit the \ud83d\udc46\ud83c\udffb button if you enjoyed my work","c36af20a":"## Feature Scaling","256dbee9":"#  PREDICT SALE OF M1 MACBOOK\n\n@Author : Rupesh Kumar\n\nDate : 22\/03\/2021\n\n\n##### 1. PROJECT OBJECTIVES:\n\n- The primary objective of the project is to determine the clients that would most likely purchase Apple's newly released M1 Macbook.\n\n##### 2. DATA COLLECTION:\n\n- The data was collected through a survey sent across different techno savvy groups over 1 week period of time.\n\n##### 3. DATA PREPARATION & ANALYSIS :\n\n- Collected data is prepared by converting columns with appropriate labels.\n- Collected data is prepared by encoding Boolean value and converting dtypes.\n- Datasets are linearly separable using all 22 input features.\n\n##### 4. CORRELATION & FEATURE SELECTION :\n\n- The dataset consists of multiple suitable features to predit the profile of the clients who would purchase the new Apple's M1 Macbook.\n\n- With the help of correlation matrix a strong relationship will be drawn between multiple feature which are then labelled and categorized as independent variable (x) used along with a target variable 'm1_purchase' (y) to fit a model that can predict the profiles of the clients who would likely purchse the new Apple's M1 Macbook. \n\n##### 5. MODEL CHOISE:\n\n- A Support Vector Machine (SVM) will be used in this notebook as SVM is a very powerful and versatile Machine Learning model, capable of performing linear or nonliner classification, regression, and even outlier detection from the scikit-learn package.","0b0eecfa":"## SVM with polynomial kernel ","0f6aeb8a":"## Confusion matrix","d133a6a2":"The confusion matrix shows 5 + 14 = 19 correct predictions and 4 + 4 = 8 incorrect predictions.\n\nIn this case, we have\n\n- True Positives (Actual Positive:1 and Predict Positive:1) - 5\n- True Negatives (Actual Negative:0 and Predict Negative:0) - 14\n- False Positives (Actual Negative:0 but Predict Positive:1) - 4 (Type I error)\n- False Negatives (Actual Positive:1 but Predict Negative:0) - 4 (Type II error)\n\n\n\n","c97bebca":"## Split data into separate training and test set ","af668328":"## SVM with default hyperparameters\n\nDefault hyperparameter means C=1.0, kernel=rbf and gamma=auto among other parameters.\n\n","b1a46938":"## SVM with sigmoid kernel ","11d68f60":"## Check for overfitting and underfitting ( Accuracy)"}}