{"cell_type":{"af1f8526":"code","cff1c416":"code","e472e53a":"code","06dd7a13":"code","aee88211":"code","9ff63ea3":"code","3788e1a5":"code","4fa2517b":"code","6d7e15ed":"code","8fe9d821":"code","b9c32e98":"code","d892f11a":"code","aeb43d2c":"code","39b159ee":"code","55573630":"code","865c4116":"code","84881c1b":"code","98a90afd":"code","cd40cbbf":"code","2f06d060":"code","25fe7bca":"code","064a54f9":"code","927c19fc":"code","3d5983d4":"code","2aec75eb":"code","d5bcadf4":"code","e1042bb9":"code","3a734931":"code","302bc6de":"code","7a5e9107":"markdown","2eb17273":"markdown","762df372":"markdown","767b4b3e":"markdown","51cf5187":"markdown","a3ecf7dc":"markdown","f66c4470":"markdown","aa309efd":"markdown","cf37f9a5":"markdown","d6bf2f4d":"markdown","7e7d7b7d":"markdown","93a3682e":"markdown","6e7976db":"markdown","60473279":"markdown","7f8b744e":"markdown","a4ddec99":"markdown"},"source":{"af1f8526":"data_path = \"..\/input\/tabular-playground-series-oct-2021\"","cff1c416":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport gc\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","e472e53a":"train_full = pd.read_csv(data_path+'\/train.csv')\ntest_full = pd.read_csv(data_path+'\/test.csv')\nsample_submission_full = pd.read_csv(data_path+'\/sample_submission.csv')\n\ntrain = train_full.sample(10000)","06dd7a13":"train_full.shape","aee88211":"train.head()","9ff63ea3":"train.isna().sum().sum()","3788e1a5":"import matplotlib.pyplot as plt\n\nunique_values = train.drop(['target'], axis=1).nunique()\n\nunique_values.hist(bins=100, figsize=(15, 7))\nplt.title('Distribution du nombre de valeurs uniques par variable')","4fa2517b":"unique_values[unique_values < 1000].hist(bins=10, figsize=(5, 5))\nplt.title('Distribution du nombre de valeurs uniques par variable (moins de 1000 valeurs uniques)')\nprint(\"Nombre de valeurs uniques maximal pour les s\u00e9ries contenant moins de 1000 valeurs uniques :\",\\\n      unique_values[unique_values < 1000].max())\nprint(\"Nombre de valeurs uniques minimal pour les s\u00e9ries contenant plus de 1000 valeurs uniques :\",\\\n      unique_values[unique_values >= 1000].min())","6d7e15ed":"cat_cols = unique_values[unique_values < 3].index\nnum_cols = unique_values[unique_values >= 3].index\n\nprint(\"Categorical columns : \", len(cat_cols))\nprint(\"Numerical columns : \", len(num_cols))","8fe9d821":"recap_cat = []\n\nfor col in train[cat_cols]:\n    recap_cat.append([\n        col,\n        train[col].nunique(),\n        train[col].mean(),\n        train[col].min(),\n        train[col].max(),\n        train[col].corr(train['target'])\n    ])\n\nrecap_cat = pd.DataFrame(recap_cat, columns=['Col','Num Unique','Mean','Min','Max','Corr'])\nrecap_cat= recap_cat.sort_values(by=['Corr'], ascending=False, key=abs)\n\nrecap_cat.head(5)","b9c32e98":"print(recap_cat.Min.max())\nprint(recap_cat.Max.min())","d892f11a":"train.iloc[:, [3, 7, 10, 23]].hist()","aeb43d2c":"round((train[cat_cols].f22 & ~train['target']).sum() \/ len(train), 3) * 100","39b159ee":"recap_num = []\n\nfor col in train[num_cols]:\n    recap_num.append([\n        col,\n        train[col].nunique(),\n        train[col].mean(),\n        train[col].min(),\n        train[col].max(),\n        train[col].corr(train['target'])\n    ])\n\nrecap_num = pd.DataFrame(recap_num, columns=['Col','Num Unique','Mean','Min','Max','Corr'])\nrecap_num = recap_num.sort_values(by=['Corr'], ascending=False, key=abs)\n\nrecap_num.head(5)","55573630":"from sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain[num_cols] = scaler.fit_transform(train[num_cols])\ntest_full[num_cols] = scaler.transform(test_full[num_cols])","865c4116":"import time\ntime_ = time.time()\ntime.sleep(1)\nprint('Time to run this cell :', round(time.time()-time_, 2), 's')","84881c1b":"from sklearn.linear_model import LogisticRegression\n\ntime_ = time.time()\n\nkf = StratifiedKFold(n_splits=3,random_state=1998,shuffle=True)\n\ntest_pred_lo = 0\nfold = 1\ntotal_auc_lr = 0\n\ncols = list(num_cols)+list(cat_cols)\n\nfor train_idx, test_idx in kf.split(train[cols],train['target']):\n    X_tr,X_val=train[cols].iloc[train_idx],train[cols].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[train_idx],train['target'].iloc[test_idx]\n    \n    lr = LogisticRegression(n_jobs = -1, random_state = 42, C = 5, max_iter = 2000)\n    lr.fit(X_tr, y_tr)\n    \n    valid_pred_lo = lr.predict_proba(X_val)[:, 1]\n    auc = roc_auc_score(y_val, valid_pred_lo)\n    total_auc_lr += auc \/ 3\n    print('Fold', fold, 'AUC :', auc)\n    fold += 1\n    \nprint('Total AUC score :', total_auc_lr)\nt_lr = round(time.time()-time_, 2)\nprint('Time to run this cell :', t_lr, 's')","98a90afd":"lr.predict_proba(X_tr)[:,1]","cd40cbbf":"time_ = time.time()\n\n\nparams={'objective' : 'binary',\n    'eval_metric' : ['auc', 'error'],\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'random_state' : 2021}\n\ncols = list(num_cols) + list(cat_cols)\n\npreds_lgbm = np.zeros(test_full.shape[0])\nkf = StratifiedKFold(n_splits=3,random_state=1998,shuffle=True)\nauc=[]  # list contains auc for each fold\nn=0\nfor train_idx, test_idx in kf.split(train[cols],train['target']):\n\n    X_tr,X_val=train[cols].iloc[train_idx],train[cols].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[train_idx],train['target'].iloc[test_idx]\n\n    eval_metric = [\"auc\",\"logloss\"]\n\n    model = LGBMClassifier(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_tr,y_tr),(X_val,y_val)],early_stopping_rounds=100,eval_metric=eval_metric,verbose=False)\n    preds_lgbm += model.predict_proba(test_full[cols])[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print('Fold', n+1, 'AUC :', auc[n])\n    n+=1  \n\ntotal_auc_lgbm = sum(auc)\/3\nprint('Total AUC score :', total_auc_lgbm)\n\nt_lgbm = round(time.time()-time_, 2)\nprint('Time to run this cell :', t_lgbm, 's')","2f06d060":"preds_lgbm","25fe7bca":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['valid_1']['auc'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n# plot auc\nprint(len(x_axis), len(results['training']['binary_logloss']))\nax[0].plot(x_axis, results['training']['auc'], label='Train')\nax[0].plot(x_axis, results['valid_1']['auc'], label='Test')\nax[0].legend()\nax[0].set_title('LGBM AUC-ROC')\nax[0].set_ylabel('AUC-ROC')\nax[0].set_xlabel('N estimators')\n# plot classification error\nax[1].plot(x_axis, results['training']['binary_logloss'], label='Train')\nax[1].plot(x_axis, results['valid_1']['binary_logloss'], label='Test')\nax[1].legend()\nax[1].set_title('LGBM Binary Logloss')\nax[1].set_ylabel('Classification Error')\nax[1].set_xlabel('N estimators')\nplt.show()\nplt.tight_layout()","064a54f9":"time_ = time.time()\n\nxgb_params = {\n    'objective': 'binary:logistic',\n    'eval_metric': 'auc',\n    'use_label_encoder': False,\n    'n_jobs': -1,\n    'n_estimators': 10000,\n    'max_depth': 3,\n    'subsample': 0.5,\n    'colsample_bytree': 0.5,\n    'learning_rate': 0.01187,\n    'random_state': 2021\n}\n\ncols = list(num_cols) + list(cat_cols)\n\npreds = np.zeros(test_full.shape[0])\nkf = StratifiedKFold(n_splits=3,random_state=1998,shuffle=True)\nauc=[]  # list contains auc for each fold\nn=0\nfor train_idx, test_idx in kf.split(train[cols],train['target']):\n    X_tr,X_val=train[cols].iloc[train_idx],train[cols].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[train_idx],train['target'].iloc[test_idx]\n\n    model = XGBClassifier(**xgb_params)\n\n    eval_metric = [\"auc\",\"error\"]\n\n    model.fit(X_tr,y_tr,eval_set=[(X_tr,y_tr),(X_val,y_val)],early_stopping_rounds=200,eval_metric=eval_metric,verbose=False)\n    preds += model.predict_proba(test_full[cols])[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print('Fold', n+1, 'AUC :', auc[n])\n    n+=1  \n\ntotal_auc_xgb = sum(auc)\/3\nprint('Total AUC score :', total_auc_xgb)\n\nt_xgb = round(time.time()-time_, 2)\nprint('Time to run this cell :', t_xgb, 's')","927c19fc":"# retrieve performance metrics\nresults = model.evals_result()\nepochs = len(results['validation_0']['error'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n# plot auc\nax[0].plot(x_axis, results['validation_0']['auc'], label='Train')\nax[0].plot(x_axis, results['validation_1']['auc'], label='Test')\nax[0].legend()\nax[0].set_title('XGBoost AUC-ROC')\nax[0].set_ylabel('AUC-ROC')\nax[0].set_xlabel('N estimators')\n# plot classification error\nax[1].plot(x_axis, results['validation_0']['error'], label='Train')\nax[1].plot(x_axis, results['validation_1']['error'], label='Test')\nax[1].legend()\nax[1].set_title('XGBoost Classification Error')\nax[1].set_ylabel('Classification Error')\nax[1].set_xlabel('N estimators')\nplt.show()\nplt.tight_layout()","3d5983d4":"time_ = time.time()\n\ncb_params = {\n    'loss_function' : 'CrossEntropy',\n    'eval_metric' : 'AUC',\n    'iterations' : 10000,\n    'grow_policy' : 'SymmetricTree',\n    'use_best_model' : True,\n    'depth' : 5,\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'verbose' : 0,\n    'random_state': 2021\n}\n\ncols = list(num_cols) + list(cat_cols)\n\npreds = np.zeros(test_full.shape[0])\nkf = StratifiedKFold(n_splits=3,random_state=1998,shuffle=True)\nauc=[]  # list contains auc for each fold\nn=0\nfor train_idx, test_idx in kf.split(train[cols],train['target']):\n    X_tr,X_val=train[cols].iloc[train_idx],train[cols].iloc[test_idx]\n    y_tr,y_val=train['target'].iloc[train_idx],train['target'].iloc[test_idx]\n\n    model = CatBoostClassifier(**cb_params)\n\n    eval_metric = [\"auc\",\"error\"]\n\n    model.fit(X_tr, y_tr, eval_set=[(X_tr,y_tr),(X_val,y_val)],early_stopping_rounds=200)\n    preds += model.predict_proba(test_full[cols])[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print('Fold', n+1, 'AUC :', auc[n])\n    n+=1  \n\ntotal_auc_cb = sum(auc)\/3\nprint('Total AUC score :', total_auc_cb)\n\nt_cb = round(time.time()-time_, 2)\nprint('Time to run this cell :', t_cb, 's')","2aec75eb":"# retrieve performance metrics\nresults = model.evals_result_\nepochs = len(results['validation_0']['CrossEntropy'])\nx_axis = range(0, epochs)\nfig, ax = plt.subplots(1, 2, figsize=(15,5))\n# plot auc\nax[0].plot(x_axis, results['validation_0']['AUC'], label='Train')\nax[0].plot(x_axis, results['validation_1']['AUC'], label='Test')\nax[0].legend()\nax[0].set_title('CatBoost AUC-ROC')\nax[0].set_ylabel('AUC-ROC')\nax[0].set_xlabel('N estimators')\n# plot classification error\nax[1].plot(x_axis, results['validation_0']['CrossEntropy'], label='Train')\nax[1].plot(x_axis, results['validation_1']['CrossEntropy'], label='Test')\nax[1].legend()\nax[1].set_title('CatBoost CrossEntropy')\nax[1].set_ylabel('CrossEntropy Error')\nax[1].set_xlabel('N estimators')\nplt.show()\nplt.tight_layout()","d5bcadf4":"fig, ax = plt.subplots()\nplt.title('AUC \/ Training Time Ratio')\n\nmodels = ['Log Reg', 'LGBM', 'XGBoost', 'CatBoost']\ntimes = [t_lr, t_lgbm, t_xgb, t_cb]\naucs = [total_auc_lr, total_auc_lgbm, total_auc_xgb, total_auc_cb]\ncolor = ['tab:blue', 'tab:orange', 'tab:green', 'tab:red']\n\nfor idx, model in enumerate(models):\n    \n    x, y = times[idx], aucs[idx]\n    #scale = 200.0 * np.random.rand(n)\n    ax.scatter(x, y, c=color[idx], label=model,\n               alpha=1, edgecolors='none')\n\nax.set_xlabel('Training Time (s)')\nax.set_ylabel('AUC')\nax.legend()\nax.grid(True)\n\nplt.show()","e1042bb9":"train_full = pd.read_csv(data_path+'\/train.csv')\ntest_full = pd.read_csv(data_path+'\/test.csv')\nsample_submission_full = pd.read_csv(data_path+'\/sample_submission.csv')\n\nunique_values = train_full.drop(['target'], axis=1).nunique()\n\ncat_cols = unique_values[unique_values < 3].index\nnum_cols = unique_values[unique_values >= 3].index\n\nfrom sklearn.preprocessing import RobustScaler\n\nscaler = RobustScaler()\ntrain_full[num_cols] = scaler.fit_transform(train_full[num_cols])\ntest_full[num_cols] = scaler.transform(test_full[num_cols])","3a734931":"import time\n\ntime_ = time.time()\n\n\nparams={'objective' : 'binary',\n    'eval_metric' : ['auc', 'error'],\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'random_state' : 2021}\n\ncols = list(num_cols) + list(cat_cols)\n\npreds_lgbm = np.zeros(test_full.shape[0])\nkf = StratifiedKFold(n_splits=3,random_state=1998,shuffle=True)\nauc=[]  # list contains auc for each fold\nn=0\nfor train_idx, test_idx in kf.split(train_full[cols],train_full['target']):\n\n    X_tr,X_val=train_full[cols].iloc[train_idx],train_full[cols].iloc[test_idx]\n    y_tr,y_val=train_full['target'].iloc[train_idx],train_full['target'].iloc[test_idx]\n\n    eval_metric = [\"auc\",\"logloss\"]\n\n    model = LGBMClassifier(**params)\n    model.fit(X_tr,y_tr,eval_set=[(X_tr,y_tr),(X_val,y_val)],early_stopping_rounds=100,eval_metric=eval_metric,verbose=False)\n    preds_lgbm += model.predict_proba(test_full[cols])[:,1]\/kf.n_splits\n    auc.append(roc_auc_score(y_val, model.predict_proba(X_val)[:, 1]))\n    gc.collect()\n    print('Fold', n+1, 'AUC :', auc[n])\n    n+=1  \n\ntotal_auc_lgbm = sum(auc)\/3\nprint('Total AUC score :', total_auc_lgbm)\n\nt_lgbm = round(time.time()-time_, 2)\nprint('Time to run this cell :', t_lgbm, 's')","302bc6de":"submit_df = sample_submission_full\nsubmit_df['target'] = preds_lgbm.ravel()\nsubmit_df.to_csv(\"submission.csv\", index=False)\nsubmit_df.head()","7a5e9107":"Les variables sont d\u00e9j\u00e0 normalis\u00e9es entre 0 et 1.","2eb17273":"# Traitement de la donn\u00e9e\n","762df372":"# Import de la donn\u00e9e","767b4b3e":"## CatBoostClassifier","51cf5187":"# Mod\u00e8le final","a3ecf7dc":"On souhaite isoler la barre basse de cette distribution pour voir s'il existe un seuil trivial pour diff\u00e9rencier les variables num\u00e9riques (continues) et cat\u00e9goriques (discr\u00e8tes). D'autre part, on observe un outlier qui correspond \u00e0 l'id unique.","f66c4470":"On note la pr\u00e9sence d'une variable cat\u00e9gorielle tr\u00e8s anti-corr\u00e9l\u00e9e avec la target.","aa309efd":"#### Variables num\u00e9riques","cf37f9a5":"### Identification des variables num\u00e9riques \/ cat\u00e9gorielles\n","d6bf2f4d":"## LGBMClassifier","7e7d7b7d":"#### Variables cat\u00e9gorielles\n","93a3682e":"## XGBClassifier","6e7976db":"# M\u00e9thodes ensemblistes\n","60473279":"# Mod\u00e8le Standard\n","7f8b744e":"On oberve donc une diff\u00e9rence nette entre les variables num\u00e9riques, pr\u00e9sentants au moins 50632 valeurs uniques, et les variables cat\u00e9goriques qui poss\u00e8dent au maximum 2 valeurs uniques.","a4ddec99":"### Corr\u00e9lations\n"}}