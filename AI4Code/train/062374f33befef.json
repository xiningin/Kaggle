{"cell_type":{"89bb3f7c":"code","96bf0e56":"code","3eea949c":"code","71702def":"code","85cc8f37":"code","477c07fa":"code","ed7339af":"code","8eed58f6":"code","48b1ac60":"code","17bede01":"code","27f25115":"code","20828040":"code","1361ab53":"code","6beaaeb0":"code","b9dd7e09":"code","e8dabfd7":"code","5294c07f":"code","b792ed8e":"code","328766c5":"code","67a424c9":"code","276cf739":"code","192f53c0":"code","9fdd2c88":"code","b238ae08":"code","f95a2bd0":"code","fb1da5df":"code","bdf467c3":"code","efd14341":"code","6e4ce770":"code","9945af61":"code","d3ca3414":"code","dab07fb7":"code","10fd813b":"code","42c300e5":"code","f54a3f04":"code","86e7937e":"code","10ac3641":"code","9143f7e2":"code","8dfa3526":"code","cc81d4ae":"code","be76d193":"code","9e607507":"code","3debad28":"code","756a9949":"code","d6e8a391":"code","4aa99503":"markdown","50b98786":"markdown","d750d74d":"markdown","28ca3ae1":"markdown","b9041dfb":"markdown","fc4c9154":"markdown","1dad429a":"markdown","5d5f3238":"markdown","06253036":"markdown","48e71e7b":"markdown","2def9521":"markdown","664f1fc0":"markdown","19c19a95":"markdown","1cd0b019":"markdown","42ed2ed5":"markdown","a8850cf7":"markdown","208210a9":"markdown","6aa541d4":"markdown","667f5d2c":"markdown","72b1ef9f":"markdown","ab9a9932":"markdown","abc02b56":"markdown","2ca5aed9":"markdown","65cd0cc6":"markdown","a3644340":"markdown","735816a4":"markdown","a23ac198":"markdown","d7a2b29d":"markdown","b591fb94":"markdown","eff70b77":"markdown","b35b16f1":"markdown","73618a79":"markdown","92948b2a":"markdown","aaf8af7e":"markdown","cb98f70f":"markdown","2c021ef9":"markdown","87ccad28":"markdown","80f59539":"markdown","833d1280":"markdown","a728c501":"markdown","80826528":"markdown","16be897a":"markdown","32c57a06":"markdown","eb0ae190":"markdown","f84c8686":"markdown","59515154":"markdown","73fbaa8e":"markdown","7b4dc569":"markdown","c79eb286":"markdown","4d96ce43":"markdown","78e76183":"markdown"},"source":{"89bb3f7c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nsns.set(color_codes = True)\n\n# used to supress display of warnings\nimport warnings\n\nimport itertools\n\n# pre-processing methods\nfrom sklearn.preprocessing import StandardScaler\n\n# evaluation methods\nfrom sklearn.pipeline import Pipeline","96bf0e56":"# suppress display of warnings\nwarnings.filterwarnings('ignore')\n\n# display all dataframe columns\npd.options.display.max_columns = None\n\n# to set the limit to 3 decimals\npd.options.display.float_format = '{:.7f}'.format\n\n# display all dataframe rows\npd.options.display.max_rows = None","3eea949c":"INPUT = '..\/input\/tabular-playground-series-sep-2021\/'\n\ntrain = pd.read_csv(INPUT + 'train.csv')\ntest = pd.read_csv(INPUT + 'test.csv')\nsubmission = pd.read_csv(INPUT + 'sample_solution.csv')","71702def":"print('Number of rows = {0} and Number of columns = {1} in train set'.format(train.shape[0], train.shape[1]))\nprint('Number of rows = {0} and Number of columns = {1} in test set'.format(test.shape[0], test.shape[1]))\nprint('Number of rows = {0} and Number of columns = {1} in sample solution set'.format(submission.shape[0], submission.shape[1]))","85cc8f37":"# Get train set - top 5 rows\ntrain.head()","477c07fa":"# Get test set - top 5 rows\ntest.head()","ed7339af":"# Get submission set - top 5 rows\nsubmission.head()","8eed58f6":"# Check train set info\ntrain.info()","48b1ac60":"# Reference - https:\/\/www.kaggle.com\/maksymshkliarevskyi\/tps-sep-all-for-start-eda-xgb-catboost-baseline\nmy_colors = ['#DC143C', '#FF1493', '#FF7F50', '#FFD700', '#32CD32', '#4ddbff', '#1E90FF', '#663399', '#708090']\n\ndtypes = train.dtypes.value_counts().reset_index()\n\nplt.figure(figsize = (12, 1))\nplt.title('Data types\\n')\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[0, 1], label = str(dtypes.iloc[0, 0]), color = my_colors[4])\nplt.barh(str(dtypes.iloc[0, 0]), dtypes.iloc[1, 1], left = dtypes.iloc[0, 1], label = str(dtypes.iloc[1, 0]), color = my_colors[5])\nplt.legend(loc = 'upper center', ncol = 3, fontsize = 13, bbox_to_anchor = (0.5, 1.45), frameon = False)\nplt.yticks('')\nplt.text(110, -0.9, '', size = 8)\nplt.show()","17bede01":"# Check test set info\ntest.info()","27f25115":"# Drop id column in train and test set\ntrain.drop('id', axis = 1, inplace = True)\ntest.drop('id', axis = 1, inplace = True)","20828040":"# Check duplicates in train set\ntrain.duplicated().sum()","1361ab53":"# Check duplicates in test set\ntest.duplicated().sum()","6beaaeb0":"# Number of missing values in each column in train set\ntrain.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","b9dd7e09":"# Reference - https:\/\/www.kaggle.com\/snikhil17\/making-basic-eda-attractive\n\nmissing = (train.isnull().sum()).to_frame().reset_index().rename({0:'Counts'}, axis = 1)\nplt.figure(figsize=(20, 5))\nax = sns.barplot(missing['index'], missing['Counts'], palette  = 'magma_r')\nplt.title(\"Number of Missing Values in Train set\", fontsize = 20)\nplt.xticks(fontsize = 7, rotation = 90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Number of Missing Values\")\nplt.show();","e8dabfd7":"# % of missing values in each column in Train set\ndisplay(round((train.isnull().sum() \/ (len(train.index)) * 100) , 2).sort_values(ascending = False).to_frame().rename({0:'Percentage(%)'}, axis = 1).T.style.background_gradient('magma_r'))","5294c07f":"missing = (train.isnull().sum() \/ (len(train.index)) * 100).to_frame().reset_index().rename({0:'Percentage(%)'}, axis = 1)\nplt.figure(figsize=(20, 5))\nax = sns.barplot(missing['index'], missing['Percentage(%)'], palette  = 'viridis')\nplt.title(\"% of Missing Values in Train set\", fontsize = 20)\nplt.xticks(fontsize =7, rotation = 90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"% of Missing Values\")\nplt.show();","b792ed8e":"# Number of missing values in each column in test set\ntest.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","328766c5":"missing = (test.isnull().sum()).to_frame().reset_index().rename({0:'Counts'}, axis = 1)\nplt.figure(figsize=(20, 5))\nax = sns.barplot(missing['index'], missing['Counts'], palette  = 'ch:start=.2,rot=-.3')\nplt.title(\"Number of Missing Values in Test set\", fontsize = 20)\nplt.xticks(fontsize = 7, rotation = 90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"Number of Missing Values\")\nplt.show();","67a424c9":"# % of missing values in each column in Test set\ndisplay(round((test.isnull().sum() \/ (len(test.index)) * 100) , 2).sort_values(ascending = False).to_frame().rename({0:'Percentage(%)'}, axis = 1).T.style.background_gradient('magma_r'))","276cf739":"missing = (train.isnull().sum() \/ (len(train.index)) * 100).to_frame().reset_index().rename({0:'Percentage(%)'}, axis = 1)\nplt.figure(figsize=(20, 5))\nax = sns.barplot(missing['index'], missing['Percentage(%)'], palette  = 'flare')\nplt.title(\"% of Missing Values in Test set\", fontsize = 20)\nplt.xticks(fontsize =7, rotation = 90)\nplt.xlabel(\"Variables\")\nplt.ylabel(\"% of Missing Values\")\nplt.show();","192f53c0":"# Replace missing missing numerical values with it's mean value in train set\nfor i in train.columns[(train.columns != 'id') & (train.columns != 'claim')]:\n    train[i].fillna(train[i].mean(), inplace = True)","9fdd2c88":"# Number of missing values in each column in train set\ntrain.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","b238ae08":"# Replace missing missing numerical values with it's mean value in test set\nfor i in test.columns[(test.columns != 'id')]:\n    test[i].fillna(test[i].mean(), inplace = True)","f95a2bd0":"# Number of missing values in each column in test set\ntest.isnull().sum().sort_values(ascending = False).to_frame().rename({0:'Counts'}, axis = 1).T.style.background_gradient('crest')","fb1da5df":"# Reference: https:\/\/www.kaggle.com\/suharkov\/sep-2021-playground-eda-no-model-for-now\ndf_plot = ((train - train.min())\/(train.max() - train.min()))\n\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = df_plot.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 90:120], ax = ax[3])","bdf467c3":"# Reference: https:\/\/www.kaggle.com\/suharkov\/sep-2021-playground-eda-no-model-for-now\ndf_plot = ((test - test.min())\/(test.max() - test.min()))\n\nfig, ax = plt.subplots(4, 1, figsize = (25,25))\nsns.boxplot(data = df_plot.iloc[:, 1:30], ax = ax[0])\nsns.boxplot(data = df_plot.iloc[:, 30:60], ax = ax[1])\nsns.boxplot(data = df_plot.iloc[:, 60:90], ax = ax[2])\nsns.boxplot(data = df_plot.iloc[:, 90:120], ax = ax[3])","efd14341":"num_cols = train.drop('claim', axis = 1).columns","6e4ce770":"cols = num_cols[0:110]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","9945af61":"cols = num_cols[110:119]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\"]\nfig = plt.figure(figsize=(20, 8))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(2,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(train[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","d3ca3414":"train[num_cols].skew().sort_values(ascending = False).to_frame().rename({0:'Skewness'}, axis = 1).T.style.background_gradient('crest')","dab07fb7":"train[num_cols].kurtosis().sort_values(ascending = False).to_frame().rename({0:'Kurtosis'}, axis = 1).T.style.background_gradient('crest_r')","10fd813b":"plt.figure(figsize = (10,6))\ntrain['claim'].value_counts().plot.pie(autopct = '%.1f', colors = ['powderblue', 'slateblue'])\nplt.title(\"Claim vlaue distribution\", pad = 20, fontdict = {'size' : 15, 'color' : 'brown', 'weight' : 'bold'})\nplt.show()","42c300e5":"claim_yes = len(train.loc[train['claim'] == 1])\nclaim_no = len(train.loc[train['claim'] == 0])\n\nprint(\"Number of claimed insurance policies: {0} ({1:2.2f}%)\".format(claim_yes, (claim_yes \/ (claim_yes + claim_no)) * 100 ))\nprint(\"Number of disclaimed insurance policies: {0} ({1:2.2f}%)\".format(claim_no, (claim_no \/ (claim_yes + claim_no)) * 100))","f54a3f04":"cols = num_cols[0:110]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\",\"b\",\"r\"]*11\nfig = plt.figure(figsize=(20, 100))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(22,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","86e7937e":"cols = num_cols[110:119]\nlength = len(cols)\ncs = [\"b\",\"r\",\"g\",\"c\",\"m\",\"k\",\"lime\",\"c\"]\nfig = plt.figure(figsize=(20, 8))\n\nfor i,j,k in itertools.zip_longest(cols, range(length),cs):\n    plt.subplot(2,5,j+1)\n    fig.tight_layout(pad=2.0)\n    ax = sns.kdeplot(test[i], color=k)\n    ax.set_facecolor(\"w\")\n    plt.axvline(train[i].mean(), linestyle=\"dashed\", label=\"mean\", color=\"k\")\n    plt.legend(loc=\"best\")\n    plt.title(i,color=\"navy\")\n    plt.xlabel(\"\")","10ac3641":"# Summary statistics for Train set\ntrain.describe().style.background_gradient(cmap='magma_r')","9143f7e2":"# Summary statistics for Test set\ntest.describe().style.background_gradient(cmap='inferno_r')","8dfa3526":"# Create a copy of Train set\ntrain_corr = train.copy()\n\n# Transform Train set independent features\nscaler_X = StandardScaler()\npipeline = Pipeline(steps=[('s', scaler_X)])\n\ntrain_corr[num_cols] = pipeline.fit_transform(train_corr[num_cols])","cc81d4ae":"corr = abs(train_corr.corr()) # correlation matrix\nlower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\nmask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\nplt.figure(figsize = (140, 40))\nsns.heatmap(lower_triangle, cmap = 'coolwarm', annot= True, xticklabels = corr.index, yticklabels = corr.columns, cbar= True, linewidths= 1, \n            mask = mask)\nplt.show()","be76d193":"train_corr.corr()[['claim']].T.style.background_gradient('magma_r')","9e607507":"# Delete train_corr\ndel train_corr","3debad28":"# Create a copy of Train set\ntest_corr = test.copy()\n\n# Transform Test set independent features\ntest_corr[num_cols] = pipeline.fit_transform(test_corr[num_cols])","756a9949":"corr = abs(test_corr.corr()) # correlation matrix\nlower_triangle = np.tril(corr, k = -1)  # select only the lower triangle of the correlation matrix\nmask = lower_triangle == 0  # to mask the upper triangle in the following heatmap\n\nplt.figure(figsize = (140, 40))\nsns.heatmap(lower_triangle, cmap = 'coolwarm', annot= True, xticklabels = corr.index, yticklabels = corr.columns, cbar= True, linewidths= 1, \n            mask = mask)\nplt.show()","d6e8a391":"# Delete test_corr\ndel test_corr","4aa99503":"<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Here are my other notebooks, please have a look and definitely you will find it useful. Happy reading \ud83d\ude42<\/strong><\/p>\n<ol>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/industrial-safety-complete-solution\">Industrial Safety - Complete Solution<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/eda-statistical-analysis-hypothesis-testing\">EDA - Statistical Analysis - Hypothesis Testing<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/random-forest-with-bootstrap-sampling-for-beginner\">Random Forest with Bootstrap Sampling for beginner<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/amazon-electronics-eda-recommender-system\">Amazon Electronics - EDA - Recommender System<\/a><\/li>\n<li><a href =\"https:\/\/www.kaggle.com\/vinayakshanawad\/personal-loan-logistic-reg-accuracy-90-41\">Personal Loan - Logistic Reg - Accuracy = 90.41%<\/a><\/li>\n<\/ol>","50b98786":"**Observations**\n\n* **f3, f8, f9, f10, f12, f21, f26, f27, f28, f32, f33, f35, f62, f65, f73, f74, f77, f78, f82, f84, f86, f92, f96, f98, f102, f103, f104, f108, f114, f116 features are having very high values compared to other features.**","d750d74d":"<a id = '5.8'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.8 EDA (Exploratory Data Analysis) Summary <\/strong><\/p>\n\n**1. Univariate analysis in both Train and Test set.**\n* f34, f57 and f97 features - Normally distributed.\n* f2, f31, f58, f60, f91, f94, f110 - Left skewed distribution - all these features are skewed to lower values.\n* f1, f17, f22, f46, f79 - seems to normally distributed.\n* f23, f55, f66   - Moderately left skewed distribution.\n* f36, f48, f69, f90, f99  - Moderately right skewed distribution.\n* Except all above features, rest of the features are skewed to higher values.\n* f113 feature - Normally distributed.\n* Except f113 features, rest of the features are skewed to higher values.\n\n\t**Note: Train and Test set distribution looks similar, that means they come from same population distribution.**\n\n**2. We have 49.85% customers claimed the insurance policies and 50.15% customers did not claim. And data is balanced as well.**\n**3. Skewness details:**\n* f34, f57, f97, f2, f31, f58, f60, f91, f94, f110, f1, f17, f22, f46, f79, f23, f55, f66, f36, f48, f69, f90, f99 - Except these features, rest of the features are skewed to higher values.\n* f1, f65, f60, f11, f50, f31, f55, f5, f24, f66, f94, f23, f2, f13, f110, f58, f91, f46 - These features are skewed to lower values.\n**4. Kurtosis details:**\n* f10, f22, f46, f104, f17, f92, f96, f43, f76, f53, f3, f6, f36, f69, f32, f51, f21, f116, f114, f26, f115, f73 - These features are having more peaked distribution which is Leptokurtic.\n* f62, f33, f112, f4, f64, f83, f118, f71, f15, f18, f106, f113, f44, f38, f30, f63, f11, f82, f79, f48, f52, f74, f103, f68, f39, f87, f110, f1, f91, f89, f90, f13, f14, f99, f102, f8, f101, f80 - These features are having less peaked distribution which is Platykurtic.\n\n**5. Summary statistics:**\n* f3, f8, f9, f10, f12, f21, f26, f27, f28, f32, f33, f35, f62, f65, f73, f74, f77, f78, f82, f84, f86, f92, f96, f98, f102, f103, f104, f108, f114, f116 features are having very high values compared to other features.\n\n**6. Correlation analysis:**\n* All independent featuers are having negligible correlation with other features.\n* Even target feature - claim is having negligible correlation with all independent features.","28ca3ae1":"<a id = '5.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.6 Study Summary Statistics <\/strong><\/p>","b9041dfb":"**Observations**\n\n* Except these features - f5, f7, f12, f25, f28, f29, f34, f37, f40, f42, f50, f56, f57, f60, f61, f65, f67, f70, f72, f75, f81, f88, f93, f97, f100, f105, f109, f117 all other features are having outliers in test set.","fc4c9154":"<a id = '6.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 6. Future work <\/h2> \n\n1. Data preprocessing.\n2. Feature engineering.\n3. Model building.\n4. Hyper parameter tuning.\n5. Final submission.\n\n<p style = \"font-size:30px; color: #007580 ;background-color:  ; text-align: left; border-radius: 5px 5px; padding: 5px\" ><strong> Finally, please let me know your suggestions. Thanks for reading \ud83d\ude42<\/strong><\/p>","1dad429a":"<a id = '5.7'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.7 Study Correlation <\/strong><\/p>","5d5f3238":"<a id = '4.6'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.6 Data Cleaning Summary <\/strong><\/p>\n\n1. No duplicate instances in both train and test set.\n2. % of missing values in both train and test set is 1.57% to 1.63%.\n3. Replaced missing missing numerical values with it's mean value in both train set and test set..\n3. Except these features - f5, f7, f12, f25, f28, f29, f34, f37, f40, f42, f50, f56, f57, f60, f61, f65, f67, f70, f72, f75, f81, f88, f93, f97, f100, f105, f109, f117 all other features are having outliers in train set.\n4. Except these features - f5, f7, f12, f25, f28, f29, f34, f37, f40, f42, f50, f56, f57, f60, f61, f65, f67, f70, f72, f75, f81, f88, f93, f97, f100, f105, f109, f117 all other features are having outliers in test set.\n\n    **Note: Need to handle Outliers in both Train and Test set.**","06253036":"<a id = '5.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.4 Univariate analysis - Target (Categorical) column <\/strong><\/p>","48e71e7b":"<a id = '4.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.3 Handle Missing Values <\/strong><\/p> ","2def9521":"* **Above plot shows, claim distribution is almost equal.**","664f1fc0":"<a id = '5.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.1 Univariate Analysis - Train set - Numerical columns<\/strong><\/p> ","19c19a95":"### Calculate claim ratio from target variable","1cd0b019":"<a id = '5.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.5 Univariate analysis - Test set - Numerical columns <\/strong><\/p>","42ed2ed5":"<a id = '5.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 5. EDA (Data Analysis and Preparation) <\/h2> ","a8850cf7":"* **So we have 49.85% customers claimed the insurance policies and 50.15% customers did not claim. And data is balanced as well.**","208210a9":"<a id = '4.5'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.5 Check Outliers in Test Set <\/strong><\/p> ","6aa541d4":"**Observations**\n\n* **All independent featuers are having negligible correlation with other features.**","667f5d2c":"**Observations**\n\n* **f113 feature - Normally distributed.**\n* **Except f113 feature, rest of the features are skewed to higher values.**","72b1ef9f":"<a id = '2.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 2. Import the necessary libraries <\/h2> ","ab9a9932":"**Observations**\n\n* **f10, f22, f46, f104, f17, f92, f96, f43, f76, f53, f3, f6, f36, f69, f32, f51, f21, f116, f114, f26, f115, f73 - These features are having more peaked distribution which is Leptokurtic.**\n* **f62, f33, f112, f4, f64, f83, f118, f71, f15, f18, f106, f113, f44, f38, f30, f63, f11, f82, f79, f48, f52, f74, f103, f68, f39, f87, f110, f1, f91, f89, f90, f13, f14, f99, f102, f8, f101, f80 - These features are having less peaked distribution which is Platykurtic.**","abc02b56":"[![EDA-Image.png](https:\/\/i.postimg.cc\/Nj1tLsHM\/EDA-Image.png)](https:\/\/postimg.cc\/v4HkNdXF)","2ca5aed9":"**Observations**\n\n* Except id and claim columns, all columns are decimals (float)\n* Id column is numerical column\n* Claim column is categorical (binary) column","65cd0cc6":"#### Looking at the Test set, some of the features have very high values while other very low values, so let's scale\/normalize the features to check the correlation between all features.","a3644340":"### Files\n\n- train.csv - the training data with the target claim column\n- test.csv - the test set; you will be predicting the claim for each row in this file\n- sample_submission.csv - a sample submission file in the correct format","735816a4":"<a id = '3.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 3. Data Collection <\/h2> ","a23ac198":"**Observations**\n\n* **f113 feature - Normally distributed.**\n* **Except f113 features, rest of the features are skewed to higher values.**\n\n**Note: Train and Test set distribution looks similar, that means they come from same population distribution.**","d7a2b29d":"<a id = '0'><\/a>\n<h2 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spcaing: 3px; background-color: #007580; color : #fed049; border-radius: 5px 5px; text-align:center; font-weight: bold\" >Table of Contents<\/h2> \n\n1. [Overview](#1.0)\n2. [Import the necessary libraries](#2.0)\n3. [Data Collection](#3.0)\n4. [Data Cleaning](#4.0)\n\t- [4.1 Check Duplicates](#4.1)\n\t- [4.2 Check Missing Values](#4.2)\n    - [4.3 Handle Missing Values](#4.3)\n\t- [4.4 Check Outliers in Train Set](#4.4)\n\t- [4.5 Check Outliers in Test Set](#4.5)\n\t- [4.6 Data Cleaning Summary](#4.6)\n5. [EDA (Data Analysis and Preparation)](#5.0)\n\t- [5.1 Univariate Analysis - Train set - Numerical columns](#5.1)\n\t- [5.2 Check Skewness](#5.2)\n\t- [5.3 Check Kurtosis](#5.3)\n\t- [5.4 Univariate analysis - Target (Categorical) column](#5.4)\n\t- [5.5 Univariate analysis - Test set - Numerical columns](#5.5)\n    - [5.6 Study Summary Statistics](#5.6)\n\t- [5.7 Study Correlation](#5.7)\n\t- [5.8 EDA (Exploratory Data Analysis) Summary](#5.8)\n6. [Future work](#5.0)","b591fb94":"**Observations**\n\n* **f34, f57 and f97 features - Normally distributed.**\n* **f2, f31, f58, f60, f91, f94, f110 - Left skewed distribution - all these features are skewed to lower values.**\n* **f1, f17, f22, f46, f79 - seems to normally distributed.**\n* **f23, f55, f66   - Moderately left skewed distribution.**\n* **f36, f48, f69, f90, f99  - Moderately right skewed distribution.**\n* **Except all above features, rest of the features are skewed to higher values.**","eff70b77":"**Observations**\n\n* **f34, f57 and f97 features - Normally distributed.**\n* **f2, f31, f58, f60, f91, f94, f110 - Left skewed distribution - all these features are skewed to lower values.**\n* **f1, f17, f22, f46, f79 - seems to normally distributed.**\n* **f23, f55, f66   - Moderately left skewed distribution.**\n* **f36, f48, f69, f90, f99  - Moderately right skewed distribution.**\n* **Except all above features, rest of the features are skewed to higher values.**","b35b16f1":"<p style = \"font-size:20px; color: #007580 \"><strong> Study Correlation in Train Set <\/strong><\/p>","73618a79":"<a id = '1.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 1. Overview <\/h2> ","92948b2a":"#### Data type of each attribute","aaf8af7e":"**Observations**\n\n* Except these features - f5, f7, f12, f25, f28, f29, f34, f37, f40, f42, f50, f56, f57, f60, f61, f65, f67, f70, f72, f75, f81, f88, f93, f97, f100, f105, f109, f117 all other features are having outliers in train set.","cb98f70f":"#### Setting Options","2c021ef9":"<p style = \"font-size:20px; color: #007580 \"><strong> Data Collection Summary <\/strong><\/p>\n\n1. There are about 957919 rows and 120 columns in train dataset.\n2. There are about 493474 rows and 119 columns in test dataset.\n3. We see there are total 118 float64 types and 2 int64 types variables present in train dataset.\n4. We see there are total 118 float64 types and 1 int64 types variables present in test dataset.\n\n* **Binary Categorical columns** - claim\n\n* **Numerical columns** - From f1 to f118","87ccad28":"**Observations**\n\n* Except id and claim columns, all columns are decimals (float)\n* Id column is numerical column","80f59539":"<p style = \"font-size:20px; color: #007580 \"><strong> Study Correlation in Test Set <\/strong><\/p>","833d1280":"#### Looking at the train set, some of the features have very high values while other very low values, so let's scale\/normalize the features to check the correlation between all features.","a728c501":"<a id = '5.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.2 Check Skewness <\/strong><\/p>","80826528":"<a id = '4.2'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.2 Check Missing Values <\/strong><\/p> ","16be897a":"**Observations**\n\n* **All independent featuers are having negligible correlation with other features.**\n* **Even target feature - claim is having negligible correlation with all independent features.**","32c57a06":"<a id = '4.4'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.4 Check Outliers in Train Set <\/strong><\/p> ","eb0ae190":"### Evaluation\n\nSubmissions are evaluated on area under the ROC curve between the predicted probability and the observed target.","f84c8686":"<a id = '5.3'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 5.3 Check Kurtosis <\/strong><\/p>","59515154":"<a id = '4.1'><\/a>\n<p style = \"font-size:20px; color: #007580 \"><strong> 4.1 Check Duplicates <\/strong><\/p> ","73fbaa8e":"<h2 style = \"font-size:40px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> TPS Sep 2021 - EDA<\/h2> \n","7b4dc569":"<a id = '4.0'><\/a>\n<h2 style = \"font-size:35px; font-family:Garamond ; font-weight : normal; background-color: #007580; color :#fed049   ; text-align: center; border-radius: 5px 5px; padding: 5px\"> 4. Data Cleaning <\/h2> ","c79eb286":"**Observations**\n\n* **f3, f8, f9, f10, f12, f21, f26, f27, f28, f32, f33, f35, f62, f65, f73, f74, f77, f78, f82, f84, f86, f92, f96, f98, f102, f103, f104, f108, f114, f116 features are having very high values compared to other features.**","4d96ce43":"**Observations**\n\n* **f34, f57, f97, f2, f31, f58, f60, f91, f94, f110, f1, f17, f22, f46, f79, f23, f55, f66, f36, f48, f69, f90, f99 - Except these features, rest of the features are skewed to higher values.**\n* **f1, f65, f60, f11, f50, f31, f55, f5, f24, f66, f94, f23, f2, f13, f110, f58, f91, f46 - These features are skewed to lower values.**","78e76183":"### Data Description:\n\nFor this competition, we will predict whether a customer made a claim upon an insurance policy. The ground truth claim is binary valued, but a prediction may be any number from 0.0 to 1.0, representing the probability of a claim. The features in this dataset have been anonymized and may contain missing values."}}