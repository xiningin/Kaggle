{"cell_type":{"0cb76bf7":"code","eb279c0a":"code","09850c43":"code","75749e59":"code","544b0f9f":"code","d6908db9":"code","9be2a9a1":"code","f49a4e4a":"code","1618231e":"code","b02eb1f7":"code","7c2401d0":"code","d63415ff":"code","920dbd34":"code","197c7980":"code","54abdaae":"code","faf93b06":"code","63efefef":"code","cd937515":"markdown","731489c2":"markdown","164550ff":"markdown","d64c6cc4":"markdown","284aa4a7":"markdown","b587c483":"markdown","f9940c9a":"markdown","1697bd9d":"markdown"},"source":{"0cb76bf7":"# In this notebook is demonstrated an proposal for treatment of imbalanced data\n# import of needed libraries\nimport numpy as np\nimport pandas as pd\nfrom scipy.sparse import *\nfrom sklearn.metrics.pairwise import pairwise_distances\nfrom sklearn import preprocessing\nfrom imblearn.over_sampling import SMOTE # pip install -U imbalanced-learn\nimport seaborn as sns\n\n# evaluation\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom sklearn.metrics import confusion_matrix, accuracy_score, f1_score\n\n# classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestClassifier","eb279c0a":"# read dataset\ndata_origin = pd.read_csv(\"..\/input\/weight-lifting-exercises\/example_wearablecomputing_weight_lifting_exercises_biceps_curl_variations.csv\")\nprint(data_origin.shape)\ndata_origin.head()","09850c43":"# show % of samples per classe\ndef show_classes_distribution(y):\n    n_samples = len(y)\n    labels = np.unique(y)\n    counts = pd.value_counts(y) \/ n_samples\n\n    df = pd.DataFrame({'classe': labels, 'percent': counts})\n    # counts.plot.bar(x='counts', y='classe', rot=0)\n    ax = sns.barplot(x='classe', y=\"percent\", data=df)","75749e59":"def evaluate_model(model, model_name, X, y):\n    X_train, X_test, y_train, y_true = train_test_split(X, y, test_size = 0.25, random_state=1)\n    \n    X_train_scaled = preprocessing.scale(X_train)\n    model.fit(X_train_scaled, y_train)\n    \n    y_pred = model.predict(X_test)\n    \n    acc = accuracy_score(y_true, y_pred)\n    f_measure = f1_score(y_true, y_pred, average='macro')\n    mat = confusion_matrix(y_true, y_pred)\n    \n    print('************************************************************************************************')\n    print(model_name, ' accuracy: ', acc)\n    print(model_name, ' f-measure: ', f_measure)\n    print(model_name, ' matrix confusion: ')\n    print(mat)\n    \n    cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n    scores = cross_val_score(model, X, y, cv=cv, scoring='f1_macro')\n    \n    print(model_name, ' scores by cv: ', scores)\n    print(model_name, ' scores (mean)', np.average(scores))","544b0f9f":"def get_encoded_data(data):\n    vars_to_dummy = np.array(data.select_dtypes(include=['object', 'bool']).columns)\n    new_data = pd.get_dummies(data, columns=vars_to_dummy, dummy_na=True)\n    new_data = new_data.fillna(0)\n    \n    return new_data","d6908db9":"def get_oversampled_data(X, y):    \n    sm = SMOTE(sampling_strategy='auto', random_state=7)\n\n    # Fit the model to generate the data.\n    X_new, y_new = sm.fit_sample(X, y)\n    \n    return X_new, y_new","9be2a9a1":"def construct_W(X, **kwargs):\n    \"\"\"\n    Construct the affinity matrix W through different ways\n    Notes\n    -----\n    if kwargs is null, use the default parameter settings;\n    if kwargs is not null, construct the affinity matrix according to parameters in kwargs\n    Input\n    -----\n    X: {numpy array}, shape (n_samples, n_features)\n        input data\n    kwargs: {dictionary}\n        parameters to construct different affinity matrix W:\n        y: {numpy array}, shape (n_samples, 1)\n            the true label information needed under the 'supervised' neighbor mode\n        metric: {string}\n            choices for different distance measures\n            'euclidean' - use euclidean distance\n            'cosine' - use cosine distance (default)\n        neighbor_mode: {string}\n            indicates how to construct the graph\n            'knn' - put an edge between two nodes if and only if they are among the\n                    k nearest neighbors of each other (default)\n            'supervised' - put an edge between two nodes if they belong to same class\n                    and they are among the k nearest neighbors of each other\n        weight_mode: {string}\n            indicates how to assign weights for each edge in the graph\n            'binary' - 0-1 weighting, every edge receives weight of 1 (default)\n            'heat_kernel' - if nodes i and j are connected, put weight W_ij = exp(-norm(x_i - x_j)\/2t^2)\n                            this weight mode can only be used under 'euclidean' metric and you are required\n                            to provide the parameter t\n            'cosine' - if nodes i and j are connected, put weight cosine(x_i,x_j).\n                        this weight mode can only be used under 'cosine' metric\n        k: {int}\n            choices for the number of neighbors (default k = 5)\n        t: {float}\n            parameter for the 'heat_kernel' weight_mode\n        fisher_score: {boolean}\n            indicates whether to build the affinity matrix in a fisher score way, in which W_ij = 1\/n_l if yi = yj = l;\n            otherwise W_ij = 0 (default fisher_score = false)\n        reliefF: {boolean}\n            indicates whether to build the affinity matrix in a reliefF way, NH(x) and NM(x,y) denotes a set of\n            k nearest points to x with the same class as x, and a different class (the class y), respectively.\n            W_ij = 1 if i = j; W_ij = 1\/k if x_j \\in NH(x_i); W_ij = -1\/(c-1)k if x_j \\in NM(x_i, y) (default reliefF = false)\n    Output\n    ------\n    W: {sparse matrix}, shape (n_samples, n_samples)\n        output affinity matrix W\n    \"\"\"\n\n    # default metric is 'cosine'\n    if 'metric' not in kwargs.keys():\n        kwargs['metric'] = 'cosine'\n\n    # default neighbor mode is 'knn' and default neighbor size is 5\n    if 'neighbor_mode' not in kwargs.keys():\n        kwargs['neighbor_mode'] = 'knn'\n    if kwargs['neighbor_mode'] == 'knn' and 'k' not in kwargs.keys():\n        kwargs['k'] = 5\n    if kwargs['neighbor_mode'] == 'supervised' and 'k' not in kwargs.keys():\n        kwargs['k'] = 5\n    if kwargs['neighbor_mode'] == 'supervised' and 'y' not in kwargs.keys():\n        print ('Warning: label is required in the supervised neighborMode!!!')\n        exit(0)\n\n    # default weight mode is 'binary', default t in heat kernel mode is 1\n    if 'weight_mode' not in kwargs.keys():\n        kwargs['weight_mode'] = 'binary'\n    if kwargs['weight_mode'] == 'heat_kernel':\n        if kwargs['metric'] != 'euclidean':\n            kwargs['metric'] = 'euclidean'\n        if 't' not in kwargs.keys():\n            kwargs['t'] = 1\n    elif kwargs['weight_mode'] == 'cosine':\n        if kwargs['metric'] != 'cosine':\n            kwargs['metric'] = 'cosine'\n\n    # default fisher_score and reliefF mode are 'false'\n    if 'fisher_score' not in kwargs.keys():\n        kwargs['fisher_score'] = False\n    if 'reliefF' not in kwargs.keys():\n        kwargs['reliefF'] = False\n\n    n_samples, n_features = np.shape(X)\n\n    # choose 'knn' neighbor mode\n    if kwargs['neighbor_mode'] == 'knn':\n        k = kwargs['k']\n        if kwargs['weight_mode'] == 'binary':\n            if kwargs['metric'] == 'euclidean':\n                # compute pairwise euclidean distances\n                D = pairwise_distances(X)\n                D **= 2\n                # sort the distance matrix D in ascending order\n                dump = np.sort(D, axis=1)\n                idx = np.argsort(D, axis=1)\n                # choose the k-nearest neighbors for each instance\n                idx_new = idx[:, 0:k+1]\n                G = np.zeros((n_samples*(k+1), 3))\n                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n                G[:, 1] = np.ravel(idx_new, order='F')\n                G[:, 2] = 1\n                # build the sparse affinity matrix W\n                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n                bigger = np.transpose(W) > W\n                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n                return W\n\n            elif kwargs['metric'] == 'cosine':\n                # normalize the data first\n                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n                for i in range(n_samples):\n                    X[i, :] = X[i, :]\/max(1e-12, X_normalized[i])\n                # compute pairwise cosine distances\n                D_cosine = np.dot(X, np.transpose(X))\n                # sort the distance matrix D in descending order\n                dump = np.sort(-D_cosine, axis=1)\n                idx = np.argsort(-D_cosine, axis=1)\n                idx_new = idx[:, 0:k+1]\n                G = np.zeros((n_samples*(k+1), 3))\n                G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n                G[:, 1] = np.ravel(idx_new, order='F')\n                G[:, 2] = 1\n                # build the sparse affinity matrix W\n                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n                bigger = np.transpose(W) > W\n                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n                return W\n\n        elif kwargs['weight_mode'] == 'heat_kernel':\n            t = kwargs['t']\n            # compute pairwise euclidean distances\n            D = pairwise_distances(X)\n            D **= 2\n            # sort the distance matrix D in ascending order\n            dump = np.sort(D, axis=1)\n            idx = np.argsort(D, axis=1)\n            idx_new = idx[:, 0:k+1]\n            dump_new = dump[:, 0:k+1]\n            # compute the pairwise heat kernel distances\n            dump_heat_kernel = np.exp(-dump_new\/(2*t*t))\n            G = np.zeros((n_samples*(k+1), 3))\n            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n            G[:, 1] = np.ravel(idx_new, order='F')\n            G[:, 2] = np.ravel(dump_heat_kernel, order='F')\n            # build the sparse affinity matrix W\n            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            bigger = np.transpose(W) > W\n            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n            return W\n\n        elif kwargs['weight_mode'] == 'cosine':\n            # normalize the data first\n            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n            for i in range(n_samples):\n                    X[i, :] = X[i, :]\/max(1e-12, X_normalized[i])\n            # compute pairwise cosine distances\n            D_cosine = np.dot(X, np.transpose(X))\n            # sort the distance matrix D in ascending order\n            dump = np.sort(-D_cosine, axis=1)\n            idx = np.argsort(-D_cosine, axis=1)\n            idx_new = idx[:, 0:k+1]\n            dump_new = -dump[:, 0:k+1]\n            G = np.zeros((n_samples*(k+1), 3))\n            G[:, 0] = np.tile(np.arange(n_samples), (k+1, 1)).reshape(-1)\n            G[:, 1] = np.ravel(idx_new, order='F')\n            G[:, 2] = np.ravel(dump_new, order='F')\n            # build the sparse affinity matrix W\n            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            bigger = np.transpose(W) > W\n            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n            return W\n\n    # choose supervised neighborMode\n    elif kwargs['neighbor_mode'] == 'supervised':\n        k = kwargs['k']\n        # get true labels and the number of classes\n        y = kwargs['y']\n        label = np.unique(y)\n        n_classes = np.unique(y).size\n        # construct the weight matrix W in a fisherScore way, W_ij = 1\/n_l if yi = yj = l, otherwise W_ij = 0\n        if kwargs['fisher_score'] is True:\n            W = lil_matrix((n_samples, n_samples))\n            for i in range(n_classes):\n                class_idx = (y == label[i])\n                class_idx_all = (class_idx[:, np.newaxis] & class_idx[np.newaxis, :])\n                W[class_idx_all] = 1.0\/np.sum(np.sum(class_idx))\n            return W\n\n        # construct the weight matrix W in a reliefF way, NH(x) and NM(x,y) denotes a set of k nearest\n        # points to x with the same class as x, a different class (the class y), respectively. W_ij = 1 if i = j;\n        # W_ij = 1\/k if x_j \\in NH(x_i); W_ij = -1\/(c-1)k if x_j \\in NM(x_i, y)\n        if kwargs['reliefF'] is True:\n            # when xj in NH(xi)\n            G = np.zeros((n_samples*(k+1), 3))\n            id_now = 0\n            for i in range(n_classes):\n                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n                D = pairwise_distances(X[class_idx, :])\n                D **= 2\n                idx = np.argsort(D, axis=1)\n                idx_new = idx[:, 0:k+1]\n                n_smp_class = (class_idx[idx_new[:]]).size\n                if len(class_idx) <= k:\n                    k = len(class_idx) - 1\n                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n                G[id_now:n_smp_class+id_now, 2] = 1.0\/k\n                id_now += n_smp_class\n            W1 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            # when i = j, W_ij = 1\n            for i in range(n_samples):\n                W1[i, i] = 1\n            # when x_j in NM(x_i, y)\n            G = np.zeros((n_samples*k*(n_classes - 1), 3))\n            id_now = 0\n            for i in range(n_classes):\n                class_idx1 = np.column_stack(np.where(y == label[i]))[:, 0]\n                X1 = X[class_idx1, :]\n                for j in range(n_classes):\n                    if label[j] != label[i]:\n                        class_idx2 = np.column_stack(np.where(y == label[j]))[:, 0]\n                        X2 = X[class_idx2, :]\n                        D = pairwise_distances(X1, X2)\n                        idx = np.argsort(D, axis=1)\n                        idx_new = idx[:, 0:k]\n                        n_smp_class = len(class_idx1)*k\n                        G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx1, (k, 1)).reshape(-1)\n                        G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx2[idx_new[:]], order='F')\n                        G[id_now:n_smp_class+id_now, 2] = -1.0\/((n_classes-1)*k)\n                        id_now += n_smp_class\n            W2 = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            bigger = np.transpose(W2) > W2\n            W2 = W2 - W2.multiply(bigger) + np.transpose(W2).multiply(bigger)\n            W = W1 + W2\n            return W\n\n        if kwargs['weight_mode'] == 'binary':\n            if kwargs['metric'] == 'euclidean':\n                G = np.zeros((n_samples*(k+1), 3))\n                id_now = 0\n                for i in range(n_classes):\n                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n                    # compute pairwise euclidean distances for instances in class i\n                    D = pairwise_distances(X[class_idx, :])\n                    D **= 2\n                    # sort the distance matrix D in ascending order for instances in class i\n                    idx = np.argsort(D, axis=1)\n                    idx_new = idx[:, 0:k+1]\n                    n_smp_class = len(class_idx)*(k+1)\n                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n                    G[id_now:n_smp_class+id_now, 2] = 1\n                    id_now += n_smp_class\n                # build the sparse affinity matrix W\n                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n                bigger = np.transpose(W) > W\n                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n                return W\n\n            if kwargs['metric'] == 'cosine':\n                # normalize the data first\n                X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n                for i in range(n_samples):\n                    X[i, :] = X[i, :]\/max(1e-12, X_normalized[i])\n                G = np.zeros((n_samples*(k+1), 3))\n                id_now = 0\n                for i in range(n_classes):\n                    class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n                    # compute pairwise cosine distances for instances in class i\n                    D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n                    # sort the distance matrix D in descending order for instances in class i\n                    idx = np.argsort(-D_cosine, axis=1)\n                    idx_new = idx[:, 0:k+1]\n                    n_smp_class = len(class_idx)*(k+1)\n                    G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n                    G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n                    G[id_now:n_smp_class+id_now, 2] = 1\n                    id_now += n_smp_class\n                # build the sparse affinity matrix W\n                W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n                bigger = np.transpose(W) > W\n                W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n                return W\n\n        elif kwargs['weight_mode'] == 'heat_kernel':\n            G = np.zeros((n_samples*(k+1), 3))\n            id_now = 0\n            for i in range(n_classes):\n                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n                # compute pairwise cosine distances for instances in class i\n                D = pairwise_distances(X[class_idx, :])\n                D **= 2\n                # sort the distance matrix D in ascending order for instances in class i\n                dump = np.sort(D, axis=1)\n                idx = np.argsort(D, axis=1)\n                idx_new = idx[:, 0:k+1]\n                dump_new = dump[:, 0:k+1]\n                t = kwargs['t']\n                # compute pairwise heat kernel distances for instances in class i\n                dump_heat_kernel = np.exp(-dump_new\/(2*t*t))\n                n_smp_class = len(class_idx)*(k+1)\n                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_heat_kernel, order='F')\n                id_now += n_smp_class\n            # build the sparse affinity matrix W\n            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            bigger = np.transpose(W) > W\n            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n            return W\n\n        elif kwargs['weight_mode'] == 'cosine':\n            # normalize the data first\n            X_normalized = np.power(np.sum(X*X, axis=1), 0.5)\n            for i in range(n_samples):\n                X[i, :] = X[i, :]\/max(1e-12, X_normalized[i])\n            G = np.zeros((n_samples*(k+1), 3))\n            id_now = 0\n            for i in range(n_classes):\n                class_idx = np.column_stack(np.where(y == label[i]))[:, 0]\n                # compute pairwise cosine distances for instances in class i\n                D_cosine = np.dot(X[class_idx, :], np.transpose(X[class_idx, :]))\n                # sort the distance matrix D in descending order for instances in class i\n                dump = np.sort(-D_cosine, axis=1)\n                idx = np.argsort(-D_cosine, axis=1)\n                idx_new = idx[:, 0:k+1]\n                dump_new = -dump[:, 0:k+1]\n                n_smp_class = len(class_idx)*(k+1)\n                G[id_now:n_smp_class+id_now, 0] = np.tile(class_idx, (k+1, 1)).reshape(-1)\n                G[id_now:n_smp_class+id_now, 1] = np.ravel(class_idx[idx_new[:]], order='F')\n                G[id_now:n_smp_class+id_now, 2] = np.ravel(dump_new, order='F')\n                id_now += n_smp_class\n            # build the sparse affinity matrix W\n            W = csc_matrix((G[:, 2], (G[:, 0], G[:, 1])), shape=(n_samples, n_samples))\n            bigger = np.transpose(W) > W\n            W = W - W.multiply(bigger) + np.transpose(W).multiply(bigger)\n            return W","f49a4e4a":"# in this function the feature selection is performed by\n# laplacian score, so the top features will be selected\ndef lap_score(X):\n    \"\"\"\n    This function implements the laplacian score feature selection, steps are as follows:\n    1. Construct the affinity matrix W if it is not specified\n    2. For the r-th feature, we define fr = X(:,r), D = diag(W*ones), ones = [1,...,1]', L = D - W\n    3. Let fr_hat = fr - (fr'*D*ones)*ones\/(ones'*D*ones)\n    4. Laplacian score for the r-th feature is score = (fr_hat'*L*fr_hat)\/(fr_hat'*D*fr_hat)\n    Input\n    -----\n    X: {numpy array}, shape (n_samples, n_features)\n        input data\n    kwargs: {dictionary}\n        W: {sparse matrix}, shape (n_samples, n_samples)\n            input affinity matrix\n    Output\n    ------\n    score: {numpy array}, shape (n_features,)\n        laplacian score for each feature\n    Reference\n    ---------\n    He, Xiaofei et al. \"Laplacian Score for Feature Selection.\" NIPS 2005.\n    \"\"\"\n\n    # construct the affinity matrix W\n    W = construct_W(X)\n    \n    # build the diagonal D matrix from affinity matrix W\n    D = np.array(W.sum(axis=1))\n    L = W\n    tmp = np.dot(np.transpose(D), X)\n    D = diags(np.transpose(D), [0])\n    Xt = np.transpose(X)\n    t1 = np.transpose(np.dot(Xt, D.todense()))\n    t2 = np.transpose(np.dot(Xt, L.todense()))\n    # compute the numerator of Lr\n    D_prime = np.sum(np.multiply(t1, X), 0) - np.multiply(tmp, tmp)\/D.sum()\n    # compute the denominator of Lr\n    L_prime = np.sum(np.multiply(t2, X), 0) - np.multiply(tmp, tmp)\/D.sum()\n    # avoid the denominator of Lr to be 0\n    D_prime[D_prime < 1e-12] = 10000\n\n    # compute laplacian score for all features\n    score = 1 - np.array(np.multiply(L_prime, 1\/D_prime))[0, :]\n    return np.transpose(score)","1618231e":"def get_reduced_data(X, top):\n    feat_scores = lap_score(X.values)\n    idx = np.argsort(feat_scores, 0)\n\n    return X.iloc[:, idx[0:top]]","b02eb1f7":"# the strategy used is the oversampling (resampling the minority classes)\n# the technique used is: SMOTE (Synthetic Minority Over-sampling Technique)\n# ref: https:\/\/towardsdatascience.com\/having-an-imbalanced-dataset-here-is-how-you-can-solve-it-1640568947eb\n\npd.set_option('display.max_columns', 10)\n# encoding categorical variables\ny_raw = data_origin['classe']\nX_raw = data_origin.drop('classe', axis=1)\n\nX_raw_encoded = get_encoded_data(X_raw)\n# X_encoded\n\n# X_encoded = pd.get_dummies(X)\n# X_encoded.dtypes\nX_new, y_new = get_oversampled_data(X_raw_encoded, y_raw)\nprint(X_new.shape)\n\ntop_features = 50\nX_new_reduced = get_reduced_data(X_new, top_features)","7c2401d0":"print('unbalanced data')\nshow_classes_distribution(y_raw)","d63415ff":"print('balanced data')\nshow_classes_distribution(y_new)","920dbd34":"knn = KNeighborsClassifier(n_neighbors = 3)\nevaluate_model(knn, 'knn - balanced', X_new, y_new)\nevaluate_model(knn, 'knn - balanced and reduced', X_new_reduced, y_new)\nevaluate_model(knn, 'knn - unbalance', X_raw_encoded, y_raw)","197c7980":"# decision tree\ndt = tree.DecisionTreeClassifier()\nevaluate_model(dt, 'Decision Tree - balanced', X_new, y_new)\nevaluate_model(dt, 'Decision Tree - balanced and reduced', X_new_reduced, y_new)\nevaluate_model(dt, 'Decision Tree - unbalance', X_raw_encoded, y_raw)","54abdaae":"# Random Forest\nrf = RandomForestClassifier(max_depth=2, random_state=0)\nevaluate_model(rf, 'RandomForest - balanced', X_new, y_new)\nevaluate_model(rf, 'RandomForest - balanced and reduced', X_new_reduced, y_new)\nevaluate_model(rf, 'RandomForest - unbalance', X_raw_encoded, y_raw)","faf93b06":"# SVM\nsvm = svm.SVC()\nevaluate_model(svm, 'SVM - balanced', X_new, y_new)\nevaluate_model(svm, 'SVM - balanced and reduced', X_new_reduced, y_new)\nevaluate_model(svm, 'SVM - unbalance', X_raw_encoded, y_raw)","63efefef":"# MLP\nmlp = MLPClassifier(hidden_layer_sizes=(50, 20, 10), random_state=1)\nevaluate_model(mlp, 'MLP - balanced', X_new, y_new)\nevaluate_model(mlp, 'MLP - balanced and reduced', X_new_reduced, y_new)\nevaluate_model(mlp, 'MLP - unbalance', X_raw_encoded, y_raw)","cd937515":"# Dimensionality Reduction by Feature Selection","731489c2":"# Testing models","164550ff":"# Unbalance Strategy - Oversampling (SMOTE - Synthetic Minority Over-sampling Technique)","d64c6cc4":"# Conclusions\n\nAfter the experiments it's possible to observes that: \n1) the RandomForest get the best performance\n2) the balance of classes improve the performance\n3) it's necessary try others strategies to increase de performance\n4) the objects are classified in the C and E classe","284aa4a7":"# Evaluation Strategy","b587c483":"The code used for feature selection is: https:\/\/github.com\/jundongl\/scikit-feature\/blob\/master\/skfeature\/function\/similarity_based\/lap_score.py\nthe method scores the features, so it's necessary choose the m-best features in the ranking\nin this experiment the ideal (empirical) top of features is 50","f9940c9a":"# Generate dummy variables","1697bd9d":"# Get Reduced and Balanced Data"}}