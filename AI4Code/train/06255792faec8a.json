{"cell_type":{"c965914e":"code","adc2f605":"code","8d060a1f":"code","8f4925de":"code","d78825c4":"code","ea3afaeb":"code","d6215b89":"code","ba7e55ed":"code","cbf08186":"code","ac3f421a":"code","8981bec2":"code","c61aa609":"code","20a0b4d9":"code","d586af0e":"code","ad72b199":"code","d61a1cfb":"code","b45d26f1":"code","76e6d5c5":"code","21be1609":"code","31c7d82b":"code","66aaafae":"code","63d0123a":"code","2bc17e6d":"code","b89e958b":"code","7360371e":"code","53be213b":"code","0e6214f5":"code","eaa76e02":"code","2718e193":"code","4abdad69":"code","ef976240":"code","eefb9dd4":"code","63bcfa07":"code","80b364e3":"code","33a20c1c":"code","3eaf0200":"code","c372cb16":"code","a1835d5c":"code","ad096b63":"code","5c160789":"code","a5139d5f":"code","abae84c4":"code","d0005c1d":"code","88964764":"code","b0f70379":"code","20d09afe":"code","47e1b8f0":"code","1c484996":"code","a9df66d7":"code","5a7b183b":"code","a0fd57ae":"code","ac2f81aa":"code","a8818830":"code","bf0f8adf":"code","3648cf9e":"code","c7a2d123":"code","5af6e0fa":"markdown","3f6674d9":"markdown","3766ac8a":"markdown","4979580c":"markdown","205cecfb":"markdown","e3c04d41":"markdown","79846f25":"markdown","8ab24aa3":"markdown","cd93e6e9":"markdown","fadf5177":"markdown","a28b97b1":"markdown","43d04d50":"markdown","ef6898a2":"markdown"},"source":{"c965914e":"!pip install GoogleNews\n!pip install pycaret","adc2f605":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas_profiling\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","8d060a1f":"from GoogleNews import GoogleNews\ngooglenews = GoogleNews()\ngooglenews.clear()","8f4925de":"googlenews = GoogleNews(lang='en')\ngooglenews.get_news('British Columbia accident')\ngooglenews.search('British Columbia accident')","d78825c4":"result_0 = googlenews.page_at(1)\n\ndesc_1 = googlenews.get_texts()\nlink_1 = googlenews.get_links()\n\nfor i in list(range(2, 10)):\n\n    result = googlenews.page_at(i)\n    desc = googlenews.get_texts()\n    link = googlenews.get_links()\n\n    desc_1 = desc_1 + desc\n    link_1 = link_1 + link\n\nimport pandas as pd\n\ncolumn_names = [\"description_text\", 'link']\ndf = pd.DataFrame(columns = column_names)\n\ndf['description_text'] = desc_1\ndf['link'] = link_1\ndisplay(df)\n\ndf.to_csv('google_news.csv', index = False)\n\ndf = df.reset_index()\n\ndie = df[df[\"description_text\"].str.contains(\"die\", na=False)]\nkill = df[df[\"description_text\"].str.contains(\"kill\", na=False)]\nkilled = pd.concat([die, kill])\nkilled['Killed'] = 'Yes'\nkilled = killed.reset_index(drop=True)\n\ndf = pd.merge(df,killed,on='index',how='left')\ndf = df[['description_text_x','Killed']]\n\ndf.Killed = df.Killed.replace(np.NaN, 'No')\ndf","ea3afaeb":"df.profile_report()","d6215b89":"df_ = df.copy()","ba7e55ed":"# One hot encoding\n\ndf = pd.get_dummies(df,columns=['Killed'], drop_first=True)\ndisplay(df.head())\ndisplay(df.tail())","cbf08186":"df2 = df\ndf2 = df2[['Killed_Yes','description_text_x']]\ndisplay(df2.head())\ndisplay(df2.tail())","ac3f421a":"import re\nfrom bs4 import BeautifulSoup","8981bec2":"# Remove HTTP tags\n%time df2['news_processed'] = df2['description_text_x'].map(lambda x : ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)\",\" \",x).split()))\ndf2.head()","c61aa609":"#Lower Case\n%time df2['news_processed'] = df2['news_processed'].map(lambda x: x.lower())\ndf2.head()","20a0b4d9":"#Remove punctuations\n%time df2['news_processed'] = df2['news_processed'].map(lambda x: re.sub(r'[^\\w\\s]', '', x))\ndf2.head()","d586af0e":"#Remove unicodes\n%time df2['news_processed'] = df2['news_processed'].map(lambda x : re.sub(r'[^\\x00-\\x7F]+',' ', x))\ndf2.head()","ad72b199":"from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords","d61a1cfb":"# Remove stopwords\nstop_words = stopwords.words('english')\n%time df2['news_processed'] = df2['news_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\ndf2.head()","b45d26f1":"# Lemmatize the text\nlemmer = WordNetLemmatizer()\n\n%time df2['news_processed'] = df2['news_processed'].map(lambda x : ' '.join([lemmer.lemmatize(w) for w in x.split() if w not in stop_words]))\ndf2.head()","76e6d5c5":"#Removing Stop words again after Lemmatize\n%time df2['news_processed'] = df2['news_processed'].map(lambda x : ' '.join([w for w in x.split() if w not in stop_words]))\ndisplay(df2.head())","21be1609":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import HashingVectorizer","31c7d82b":"#funtion to get 'top N' or 'bottom N' words\n\ndef get_n_words(corpus, direction, n):\n    vec = CountVectorizer(stop_words = 'english').fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    if direction == \"top\":\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    else:\n        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=False)\n    return words_freq[:n]","66aaafae":"#10 most common and 10 most rare words\ncommon_words = get_n_words(df2['news_processed'], \"top\", 15)\nrare_words = get_n_words(df2['news_processed'], \"bottom\", 15)","63d0123a":"common_words = dict(common_words)\nnames = list(common_words.keys())\nvalues = list(common_words.values())\nplt.subplots(figsize = (15,10))\nbars = plt.bar(range(len(common_words)),values,tick_label=names)\nplt.title('15 most common words:')\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .05, yval)\nplt.show()","2bc17e6d":"rare_words = dict(rare_words)\nnames = list(rare_words.keys())\nvalues = list(rare_words.values())\nplt.subplots(figsize = (15,10))\nbars = plt.bar(range(len(rare_words)),values,tick_label=names)\nplt.title('15 most rare words:')\nfor bar in bars:\n    yval = bar.get_height()\n    plt.text(bar.get_x(), yval + .001, yval)\nplt.show()","b89e958b":"# BOW-TF Embedding\n\nno_features = 800\ntf_vectorizer = CountVectorizer(min_df=.015, max_df=.8, max_features=no_features, ngram_range=[1, 3])\n\n%time tpl_tf = tf_vectorizer.fit_transform(df2['news_processed'])\ndisplay(\"Bow-TF :\", tpl_tf.shape)\ndf_tf = pd.DataFrame(tpl_tf.toarray(), columns=tf_vectorizer.get_feature_names())\ndisplay(df_tf.head())","7360371e":"#Preparing processed and BoW-TF embedded data for Classification\ndf_tf_m = pd.concat([df2, df_tf], axis = 1)\ndf_tf_m.drop(columns=['description_text_x', 'news_processed'], inplace = True)\nprint(df_tf_m.shape)\ndisplay(df_tf_m.head())\ndisplay(df_tf_m.tail())","53be213b":"# BoW-TF:IDF Embedding\ntfidf_vectorizer = TfidfVectorizer(min_df=.02, max_df=.7, ngram_range=[1,3])\n\n%time tpl_tfidf = tfidf_vectorizer.fit_transform(df2['news_processed'])\ndisplay(\"Bow-TF:IDF :\", tpl_tfidf.shape)\ndf_tfidf = pd.DataFrame(tpl_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names(), index=df2.index)\ndisplay(df_tfidf.head())","0e6214f5":"#Preparing processed and BoW-TF:IDF embedded data for Classification\ndf_tfidf_m = pd.concat([df2, df_tfidf], axis = 1)\ndf_tfidf_m.drop(columns=['description_text_x', 'news_processed'], inplace = True)\nprint(df_tfidf_m.shape)\ndisplay(df_tfidf_m.head())\ndisplay(df_tfidf_m.tail())","eaa76e02":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import StratifiedKFold, cross_validate, train_test_split, cross_val_score, KFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.metrics import roc_curve, auc, classification_report, confusion_matrix, precision_score, recall_score,  accuracy_score, precision_recall_curve","2718e193":"#function to prepare Confusion Matrix, RoC-AUC curve, and relvant statistics\n\ndef clf_report(Y_test, Y_pred, probs):\n    print(\"\\n\", \"Confusion Matrix\")\n    cm = confusion_matrix(Y_test, Y_pred)\n    #print(\"\\n\", cm, \"\\n\")\n    sns.heatmap(cm, square=True, annot=True, cbar=False, fmt = 'g', cmap='RdBu',\n                xticklabels=['Not_killed', 'killed'], yticklabels=['Not_killed', 'killed'])\n    plt.xlabel('true label')\n    plt.ylabel('predicted label')\n    plt.show()\n    print(\"\\n\", \"Classification Report\", \"\\n\")\n    print(classification_report(Y_test, Y_pred))\n    print(\"Overall Accuracy : \", round(accuracy_score(Y_test, Y_pred) * 100, 2))\n    print(\"Precision Score : \", round(precision_score(Y_test, Y_pred, average='binary') * 100, 2))\n    print(\"Recall Score : \", round(recall_score(Y_test, Y_pred, average='binary') * 100, 2))\n    preds = probs[:,1] # this is the probability for 1, column 0 has probability for 0. Prob(0) + Prob(1) = 1\n    fpr, tpr, threshold = roc_curve(Y_test, preds)\n    roc_auc = auc(fpr, tpr)\n    print(\"AUC : \", round(roc_auc * 100, 2), \"\\n\")\n    #display(probs)\n    #print(\"Cutoff Probability : \", preds)\n    plt.figure()\n    plt.plot(fpr, tpr, label='Best Model on Test Data (area = %0.2f)' % roc_auc)\n    plt.plot([0.0, 1.0], [0, 1],'r--')\n    plt.xlim([-0.1, 1.1])\n    plt.ylim([-0.1, 1.1])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('RoC-AUC on Test Data')\n    plt.legend(loc=\"lower right\")\n    plt.savefig('Log_ROC')\n    plt.show()\n    print(\"--------------------------------------------------------------------------\")","4abdad69":"#function to prepare different Classification models\n\ndef model_dvt(df):\n    Y = df['Killed_Yes']\n    X = df.drop('Killed_Yes', axis = 1)\n    \n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\n    print(\"Train Data Dimensions : \", X_train.shape)\n    print(\"Test Data Dimensions : \", X_test.shape)\n    \n    print(\"\\n\", 'Random Forest Classifier')\n    clf = RandomForestClassifier(n_estimators=500, max_depth=10, random_state=21)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'AdaBoost Classifier')\n    clf = AdaBoostClassifier(n_estimators=200,random_state=21)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'Grdient Boosting Classifier')\n    clf = GradientBoostingClassifier(n_estimators=100, max_depth=1, random_state=21, learning_rate=1.0)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)\n    \n    print(\"\\n\", 'Naive Bayes Classifier')\n    clf = MultinomialNB(alpha = 1.0)\n    %time clf.fit(X_train, Y_train)\n    Y_pred = clf.predict(X_test)\n    probs = clf.predict_proba(X_test)\n    clf_report(Y_test, Y_pred, probs)","ef976240":"print('Models on Term Frequency - Bag of Words data')\n%time model_dvt(df_tf_m)","eefb9dd4":"print('Models on IDF - Bag of Words')\n%time model_dvt(df_tfidf_m)","63bcfa07":"from sklearn.model_selection import GridSearchCV","80b364e3":"Y = df_tf_m['Killed_Yes']\nX = df_tf_m.drop('Killed_Yes', axis = 1)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size = 0.85, random_state = 21)\nprint(\"Train Data Dimensions : \", X_train.shape)\nprint(\"Test Data Dimensions : \", X_test.shape)","33a20c1c":"#Creating a grid of hyperparameters\ngrid_params = {'n_estimators' : [100,200,300],\n               'learning_rate' : [1.0, 0.1, 0.05]}\n\nABC = AdaBoostClassifier()\n#Building a 10 fold CV GridSearchCV object\ngrid_object = GridSearchCV(estimator = ABC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n\n#Fitting the grid to the training data\n%time grid_object.fit(X_train, Y_train)","3eaf0200":"#Extracting the best parameters and score\nprint(\"Best Parameters : \", grid_object.best_params_)\nprint(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\nprint(\"Best model : \", grid_object.best_estimator_)\n\n#Applying the tuned parameters back to the model\nY_pred = grid_object.best_estimator_.predict(X_test)\nprobs = grid_object.best_estimator_.predict_proba(X_test)\nclf_report(Y_test, Y_pred, probs)\n\nkfold = KFold(n_splits=10, random_state=25, shuffle=True)\n%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\nresults = results * 100\nresults = np.round(results,2)\nprint(\"Cross Validation Accuracy : \", round(results.mean(), 2))\nprint(\"Cross Validation Accuracy in every fold : \", results)","c372cb16":"grid_params = {'n_estimators' : [100,200,300,400,500],\n               'max_depth' : [10, 7, 5, 3],\n               'criterion' : ['entropy', 'gini']}\n\nRFC = RandomForestClassifier()\ngrid_object = GridSearchCV(estimator = RFC, param_grid = grid_params, scoring = 'roc_auc', cv = 10, n_jobs = -1)\n\n%time grid_object.fit(X_train, Y_train)","a1835d5c":"# print(\"Best Parameters : \", grid_object.best_params_)\nprint(\"Best_ROC-AUC : \", round(grid_object.best_score_ * 100, 2))\nprint(\"Best model : \", grid_object.best_estimator_)\n\nY_pred = grid_object.best_estimator_.predict(X_test)\nprobs = grid_object.best_estimator_.predict_proba(X_test)\nclf_report(Y_test, Y_pred, probs)\n\nkfold = KFold(n_splits=10, random_state=25, shuffle=True)\n%time results = cross_val_score(grid_object.best_estimator_, X_test, Y_test, cv=kfold)\nresults = results * 100\nresults = np.round(results,2)\nprint(\"Cross Validation Accuracy : \", round(results.mean(), 2))\nprint(\"Cross Validation Accuracy in every fold : \", results)","ad096b63":"df_.head()\n\ndf_.columns = ['News', 'Kill_Flag']\ndf_ = df_[['Kill_Flag', 'News']]\ndf_","5c160789":"from nltk.corpus import stopwords\nstop_words = stopwords.words('english')","a5139d5f":"from pycaret.nlp import *\n%time su_1 = setup(data = df_, target = 'News', custom_stopwords=stop_words, session_id=21)","abae84c4":"models()","d0005c1d":"%time m1 = create_model(model='lda', multi_core=True)","88964764":"%time lda_data = assign_model(m1)","b0f70379":"lda_data.head()","20d09afe":"evaluate_model(m1)","47e1b8f0":"plot_model(m1, plot='topic_distribution')\nplot_model(m1, plot='topic_model')\nplot_model(m1, plot='wordcloud', topic_num = 'Topic 0')\nplot_model(m1, plot='frequency', topic_num = 'Topic 0')\nplot_model(m1, plot='bigram', topic_num = 'Topic 0')\nplot_model(m1, plot='trigram', topic_num = 'Topic 0')\nplot_model(m1, plot='distribution', topic_num = 'Topic 0')\nplot_model(m1, plot='sentiment', topic_num = 'Topic 0')","1c484996":"plot_model(m1, plot = 'tsne')","a9df66d7":"plot_model(m1, plot = 'umap')","5a7b183b":"lda_data = lda_data[['Kill_Flag', 'Topic_0', 'Topic_1', 'Topic_2', 'Topic_3']]\nlda_data.head()","a0fd57ae":"from pycaret.classification import *","ac2f81aa":"%time pce_1 = setup(data = lda_data, target = 'Kill_Flag', session_id = 5, train_size = 0.85)","a8818830":"%time compare_models()","bf0f8adf":"#step1 : model creation\n%time pce_1_m1 = create_model('dt')","3648cf9e":"#step2 : model tuning\n%time tuned_pce_1_m1 = tune_model(pce_1_m1)","c7a2d123":"#step3 : getting insights from model perfromance\n%time evaluate_model(tuned_pce_1_m1)","5af6e0fa":"### Embedding of Preprocessed Data","3f6674d9":"### BoW-TF:IDF Embedding","3766ac8a":"### Models on IDF - Bag of Words","4979580c":"### Looking for the best params: Grid Search","205cecfb":"### Topic Modelling for Text Classification: The PyCaret Way vs the Regular Approach\n\nCreated on: 7\/15\/2021\n\nUpdated on: 7\/20\/2021","e3c04d41":"### Models on TF - Bag of Words data","79846f25":"### Classification","8ab24aa3":"### Preprocessing of Text Data","cd93e6e9":"## Data Collection\n\n#### Scraping the news from Google related to any accidents in British Columbia. ","fadf5177":"### BOW-TF Embedding","a28b97b1":"# Topic Modelling: The Conventional Way","43d04d50":"* http:\/\/www.pycaret.org\/tutorials\/html\/NLP101.html\n\n* https:\/\/github.com\/pycaret\/pycaret\/blob\/master\/examples\/PyCaret%202%20NLP.ipynb\n\n* https:\/\/github.com\/prateek025\/SMS_Spam_Ham\/blob\/master\/Spam-Ham.ipynb","ef6898a2":"# Topic Modelling: The PyCaret Way"}}