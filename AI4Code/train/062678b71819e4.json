{"cell_type":{"b1d7b352":"code","5fbbb57f":"code","64887ab4":"code","4c56eb3e":"code","b30b4946":"code","681c5ebe":"code","654e894c":"code","d967d9f9":"code","0a046eb1":"code","f0cecbf6":"code","f1d27d64":"code","0522f3f9":"code","e4e01b37":"code","c3ed4c9c":"code","a4b16678":"code","888daddd":"code","4209ba28":"code","7369238e":"code","8a736d63":"code","bba0aeb9":"code","8bb10529":"code","a37d00c7":"code","2882e82f":"code","4fca958e":"code","55ed457d":"markdown","9ab8d7c4":"markdown","7768bd58":"markdown","f6d205c2":"markdown","589bd62e":"markdown","0939f640":"markdown","c9d9a87b":"markdown","8984c98b":"markdown","7b6be28c":"markdown","a7a96696":"markdown","8e2ca8a1":"markdown","7d029ebd":"markdown"},"source":{"b1d7b352":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5fbbb57f":"\"\"\"\"import the dataset\"\"\"\ndata_fake = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/Fake.csv\")\ndata_true = pd.read_csv(\"..\/input\/fake-and-real-news-dataset\/True.csv\")","64887ab4":"\"\"\"insert the output(target) column in dataset\"\"\"\ndata_fake['target'] = 'Fake'\ndata_true['target'] = 'True'","4c56eb3e":"data_fake.head()","b30b4946":"data_true.head()","681c5ebe":"\"\"\"concat both the dataset in one dataset\"\"\"\ndata = pd.concat([data_fake,data_true],axis=0,ignore_index=True)","654e894c":"data.head()","d967d9f9":"# count the number of fake and True news. \ndata['target'].value_counts()","0a046eb1":"sns.countplot(data['target'])","f0cecbf6":"\"\"\"checking whether any missing or empty text row is not present in dataset\"\"\"\ndata['text'].isna().sum()","f1d27d64":"\"\"\"check whether text column is full link and remove it\"\"\"\ndef remove_link_from(text):\n    txt = ''\n    text = text.split(' ')\n    for tx in text:\n        if ('http' in text) or (\".com\" in text) or ('https' in text) or ('.in' in text) or ('bit.ly' in text) or ('tiny' in text):\n            continue\n        else:\n            txt = txt+tx\n    return txt","0522f3f9":"\"\"\"appply it to text column\"\"\"\ndata['text'] = data['text'].apply(remove_link_from)","e4e01b37":"import re\n# remove digit\ndef remove_digit(data):\n    regen_data = re.sub('[^a-zA-Z]',' ',data)\n    return regen_data.lower()","c3ed4c9c":"data['text'] = data['text'].apply(remove_digit)","a4b16678":"\"\"\"now remove the punctuation marks\"\"\"\nimport string\npnc = string.punctuation\npnc+= '\\n \\n\\n \\t \\t\\t \\r \\b'\n\n\ndef remove_punctuation(txt):\n    txt = txt.split()\n    txt = [word.lower() for word in txt if word not in pnc]\n    return ' '.join(txt)\n\ndata['text'] = data['text'].apply(remove_punctuation)\n        \n        ","888daddd":"\"\"\"check number of text with empty entry\"\"\"\ndata[data['text'] == ' ']","4209ba28":"\"\"\"split independent and dependent data\"\"\"\nX = data['text']\ny = data['target']","7369238e":"\"\"\"perform label encoding\"\"\"\ny = pd.get_dummies(y,drop_first=True)","8a736d63":"\"\"\"convert multidimention array to one-dim array with reshape function\"\"\"\ny = y.values.reshape(-1,)","bba0aeb9":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,classification_report,confusion_matrix","8bb10529":"\"\"\"intialize a TfidVector\"\"\"\ntfvctor = TfidfVectorizer(stop_words='english',max_df=0.5)\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.20,random_state=10)","a37d00c7":"\"\"\"perform fit Transform\"\"\"\nX_train = tfvctor.fit_transform(X_train)\nX_test = tfvctor.transform(X_test)","2882e82f":"\"\"\"create Logistic Regression classification model \"\"\"\nlclassify = LogisticRegression()\n\"\"\"fit the model with train and test data\"\"\"\nlclassify.fit(X_train,y_train)\n\"\"\"predict x_test data\"\"\"\npred_data = lclassify.predict(X_test)\n\"\"\"calculate accuracy and classification_report\"\"\"\nacc = accuracy_score(y_test,pred_data)\ncnfm = confusion_matrix(y_test,pred_data)\ncr = classification_report(y_test,pred_data)\nprint(\"classification by Logistic Regression\")\nprint(\"Accuracy:\",acc)\nprint(\"confusion matrix: \\n\",cnfm)\nprint(\"classification report: \\n\",cr)","4fca958e":"from sklearn.naive_bayes import MultinomialNB\n\nNBclassify = MultinomialNB().fit(X_train, y_train)\npred_data = NBclassify.predict(X_test)\n\"\"\"calculate accuracy and classification_report\"\"\"\nNBacc = accuracy_score(y_test,pred_data)\nNBcnfm = confusion_matrix(y_test,pred_data)\nNBcr = classification_report(y_test,pred_data)\nprint(\"classification by Naive Bayes:\")\nprint(\"Accuracy:\",NBacc)\nprint(\"confusion matrix:\\n\",NBcnfm)\nprint(\"classification report:\\n\",NBcr)","55ed457d":"<h1>Naive Bayes<\/h1>","9ab8d7c4":"<h2>Remove Punctution<\/h2>","7768bd58":"<h4>if you all find out this notebook helpful, upvotes it<\/h4>","f6d205c2":"there is no NaN cell in text","589bd62e":"<h1>2. Data Preprocessing<\/h1>\n&nbsp<h2>Checking missing or None value<\/h2>","0939f640":"<h2>Remove Stopword and Perform TFIDFVectorize opration<\/h2>","c9d9a87b":"<h1>1. Load Dataset<\/h1>","8984c98b":"remove all stopwords from the <b>text<\/b> feature of the dataset, there are no need to stopwords in model.provide parameter in tfidfvectorizer <b>stop","7b6be28c":"<h2>Steps for classification fake and True news<\/h2>\n<ol>\n    <li>load the dataset<\/li>\n      <li>Data Preprocessing<\/li>\n    <ul>\n        <li>Checking missing values<\/li>\n         <li>Remove link from news text<\/li>\n         <li>Remove Punctuation<\/li>\n         <li>Remove Stopwords<\/li>\n         <li>Perform TFIDFVectorizer<\/li>\n    <\/ul>\n      <li>Create a Model<\/li>\n      <li>Test the Model<\/li>\n    <\/ol>","a7a96696":"<h1>3. Create Model and Test it<\/h1>","8e2ca8a1":"popuar algorithms that is used for binary classification.\n<ul>\n    <li>Logistic Regression<\/li>\n     <li>K-Nearest Neighbour<\/li>\n     <li>Decision tree classifier<\/li>\n     <li>Support Vector Machine<\/li>\n     <li>Naive bayes<\/li>\n    <\/ul>\n <h1>Logistic Regression<\/h1>","7d029ebd":"<h2>Remove links<\/h2>"}}