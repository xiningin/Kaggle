{"cell_type":{"feb64f62":"code","baed167e":"code","2f176054":"code","663d1126":"code","1ace58ea":"code","bf15d848":"code","0ab0b102":"code","4e1af742":"code","01643163":"code","f12fee39":"code","35f4551c":"code","36fc63cb":"code","4851083b":"code","a842af49":"code","c12d8bfc":"code","953f85ab":"code","dba6f6e5":"code","71dfb806":"code","38cb512c":"code","d57fb4fe":"code","86427f89":"code","dce3c5cc":"code","38de0cf4":"code","d7b61a97":"code","270382e9":"markdown","4b7c1291":"markdown","3bd41603":"markdown","52b9f412":"markdown","ed0fd913":"markdown","c1648ea9":"markdown","b2e51123":"markdown","1ffc4719":"markdown","6d88caa7":"markdown","eeb4e06c":"markdown","8be02ad5":"markdown"},"source":{"feb64f62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","baed167e":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","2f176054":"df=pd.read_csv('..\/input\/iris\/Iris.csv')\ndf.head()","663d1126":"df.shape","1ace58ea":"df.columns","bf15d848":"df.describe(percentiles=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])","0ab0b102":"df.nunique()","4e1af742":"df['Species'].value_counts()","01643163":"df.groupby('Species').size()","f12fee39":"df.columns","35f4551c":"featured_column=['SepalLengthCm', 'SepalWidthCm', 'PetalLengthCm', 'PetalWidthCm']\nX=df[featured_column].values\nY=df['Species'].values","36fc63cb":"from sklearn.preprocessing import LabelEncoder\nle=LabelEncoder()\ny=le.fit_transform(Y)","4851083b":"from sklearn.model_selection import train_test_split\nX_train, X_test,y_train, y_test=train_test_split(X,y,test_size=0.2, random_state=0)","a842af49":"from pandas.plotting import parallel_coordinates\nplt.figure(figsize=(15,10))\nparallel_coordinates(df.drop(\"Id\", axis=1), \"Species\")\nplt.title(\"Parallel Coorfinate PLot\",fontsize=20, fontweight='bold')\nplt.xlabel(\"Feature\", fontsize=15)\nplt.ylabel(\"Feature_Values\", fontsize=15)\nplt.legend(loc=1,prop={'size':15}, facecolor=\"white\", shadow=True,edgecolor=\"black\")\nplt.show()","c12d8bfc":"from pandas.plotting import andrews_curves\nplt.figure(figsize=(15,10))\nandrews_curves(df.drop(\"Id\", axis=1), \"Species\")\nplt.title(\"Parallel Coorfinate PLot\",fontsize=20, fontweight='bold')\nplt.xlabel(\"Feature\", fontsize=15)\nplt.ylabel(\"Feature_Values\", fontsize=15)\nplt.legend(loc=1,prop={'size':15}, facecolor=\"white\", shadow=True,edgecolor=\"black\")\nplt.show()","953f85ab":"plt.figure()\nsns.pairplot(df.drop(\"Id\", axis=1), hue=\"Species\", size=3,markers=[\"o\",\"s\",\"d\"])\nplt.show()","dba6f6e5":"plt.figure()\ndf.drop(\"Id\", axis=1).boxplot(by=\"Species\",figsize=(15,10))\nplt.show()","71dfb806":"from mpl_toolkits.mplot3d import Axes3D\nfig=plt.figure(1,figsize=(20,15))\nax=Axes3D(fig,elev=48,azim=134)\nax.scatter(X[:, 0], X[:, 1], X[:, 2], c=y,\n           cmap=plt.cm.Set1, edgecolor='k', s = X[:, 3]*50)\n\nfor name, label in [('Virginica', 0), ('Setosa', 1), ('Versicolour', 2)]:\n    ax.text3D(X[y == label, 0].mean(),\n              X[y == label, 1].mean(),\n              X[y == label, 2].mean(), name,\n              horizontalalignment='center',\n              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'),size=25)\n\nax.set_title(\"3D visualization\", fontsize=40)\nax.set_xlabel(\"Sepal Length [cm]\", fontsize=25)\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"Sepal Width [cm]\", fontsize=25)\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"Petal Length [cm]\", fontsize=25)\nax.w_zaxis.set_ticklabels([])\n\nplt.show()","38cb512c":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.model_selection import cross_val_score\n\n#Instantiate learning model (k = 3)\nclassifier=KNeighborsClassifier(n_neighbors=3)\n\nclassifier.fit(X_train, y_train)\n\ny_pred=classifier.predict(X_test)","d57fb4fe":"cm=confusion_matrix(y_test,y_pred)\ncm","86427f89":"accuracy=accuracy_score(y_test,y_pred)*100\nprint(\"Accuracy of the modelis equal :\"  + str(round(accuracy,2)) +  \"%\")\n","dce3c5cc":"# creating list of K for KNN\nk_list=list(range(1,50,2))\n# creating list of cv scores\ncv_score=[]\n# perform 10-fold cross validation\n\nfor k in k_list:\n    knn=KNeighborsClassifier(n_neighbors=k)\n    score=cross_val_score(knn, X_train,y_train, cv=10,scoring='accuracy')\n    cv_score.append(score.mean())","38de0cf4":"# changing to misclassification error\n\nMSE = [1 - x for x in cv_score]\n\nplt.figure()\nplt.figure(figsize=(15,10))\nplt.title('The optimal number of neighbors', fontsize=20, fontweight='bold')\nplt.xlabel('Number of Neighbors K', fontsize=15)\nplt.ylabel('Misclassification Error', fontsize=15)\nsns.set_style(\"whitegrid\")\nplt.plot(k_list, MSE)\n\nplt.show()\n","d7b61a97":"best_k=k_list[MSE.index(min(MSE))]\nprint(\"The optimal NUmber of neighbour is %d\" % best_k)","270382e9":"# Evaluating predictions","4b7c1291":"# Dividing data into features and labels\n> NOTE: As we can see dataset contain six columns: Id, SepalLengthCm, SepalWidthCm, PetalLengthCm, PetalWidthCm and Species. The actual features are described by columns 1-4. Last column contains labels of samples. Firstly we need to split data into two arrays: X (features) and y (labels).","3bd41603":"# Pairplot\nPairwise is useful when you want to visualize the distribution of a variable or the relationship between multiple variables separately within subsets of your dataset.","52b9f412":"#  Using KNN for classification","ed0fd913":"# 1.2 Distance measure\nIn the classification setting, the K-nearest neighbor algorithm essentially boils down to forming a majority vote between the K most similar instances to a given \u201cunseen\u201d observation. Similarity is defined according to a distance metric between two data points. The k-nearest-neighbor classifier is commonly based on the Euclidean distance between a test sample and the specified training samples. Let  xi  be an input sample with  p  features  (xi1,xi2,...,xip) ,  n  be the total number of input samples  (i=1,2,...,n) . The Euclidean distance between sample  xi  and  xl  is is defined as:\n\nd^2(xi,xl)=(xi1\u2212xl1)^2+(xi2\u2212xl2)^2+...+(xip\u2212xlp)^2\n \nSometimes other measures can be more suitable for a given setting and include the Manhattan, Chebyshev and Hamming distance.","c1648ea9":"# Label encoding\n> NOTE: As we can see labels are categorical. KNeighborsClassifier does not accept string labels. We need to use LabelEncoder to transform them into numbers. Iris-setosa correspond to 0, Iris-versicolor correspond to 1 and Iris-virginica correspond to 2.","b2e51123":"# Parallel Coordinates\nParallel coordinates is a plotting technique for plotting multivariate data. It allows one to see clusters in data and to estimate other statistics visually. Using parallel coordinates points are represented as connected line segments. Each vertical line represents one attribute. One set of connected line segments represents one data point. Points that tend to cluster will appear closer together.","1ffc4719":"# K Nearest Neighbor\n> 1. KNN Theory\n1.1 Type of algorithm\nKNN can be used for both classification and regression predictive problems. KNN falls in the supervised learning family of algorithms. Informally, this means that we are given a labelled dataset consiting of training observations  (x,y)  and would like to capture the relationship between  x  and  y . More formally, our goal is to learn a function  h:X\u2192Y  so that given an unseen observation  x ,  h(x)  can confidently predict the corresponding output  y .","6d88caa7":"# Andrews Curves\nAndrews curves allow one to plot multivariate data as a large number of curves that are created using the attributes of samples as coefficients for Fourier series. By coloring these curves differently for each class it is possible to visualize data clustering. Curves belonging to samples of the same class will usually be closer together and form larger structures.","eeb4e06c":"# 1.3 Algorithm steps\n> STEP 1: Cgoose the number K of neighbors\n\n> STEP 2: Take the K nearest neighbors of the new data point, according to your distance metric\n\n> STEP 3: Among these K neighbors, count the number of data points to each category\n\n> STEP 4: Assign the new data point to the category where you counted the most neighbors","8be02ad5":"# 3D visualization\nYou can also try to visualize high-dimensional datasets in 3D using color, shape, size and other properties of 3D and 2D objects. In this plot I used marks sizes to visualize fourth dimenssion which is Petal Width [cm]."}}