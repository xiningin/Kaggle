{"cell_type":{"45300636":"code","6c15997e":"code","38cf6f22":"code","e54ac0dd":"code","e416259e":"code","c1e00cf1":"code","239c3dbf":"code","fd461bd2":"code","933abcb2":"code","9a947a37":"code","3431c965":"code","8762f53c":"code","e99b9fff":"code","15a27d7a":"code","1c8f707b":"code","17155350":"code","8c29797c":"code","6d007a05":"code","bb46a94a":"code","4ee37557":"code","1a877e1f":"code","109a5a1b":"code","7ca78e0f":"code","3bf766ba":"code","125ebb22":"code","93ea40b0":"code","e22e70d0":"code","3d972dcb":"code","8c21083b":"code","0457510c":"code","934450ac":"code","8a17ad81":"code","cc6d23c1":"code","dc6c3176":"code","614492d2":"code","2315aed1":"code","10481a6f":"code","ae959a1d":"code","edff5af2":"code","2ec3040d":"code","838efa91":"code","4fe9461f":"code","74a3950c":"code","e0b71de5":"code","0e919b4e":"code","e96868c5":"code","08999a1f":"code","10b38730":"code","59c7ccc7":"code","ec16c934":"code","0ab05c07":"code","2e384410":"code","fa18d2d7":"code","c64f3a85":"code","f27761e7":"code","83480a8f":"code","60ad6c83":"code","8ebd1802":"code","f355089a":"code","91db5ceb":"code","9b66f0e7":"code","cebd3184":"code","036c431b":"code","809345cf":"code","f66a6a0f":"code","d801ea5b":"code","8e2c2f03":"code","7de59d0e":"code","c903f413":"markdown","0c486806":"markdown","42447f02":"markdown","4f7206f8":"markdown","1ee66aa0":"markdown","6f01bbe3":"markdown","94beb2ff":"markdown","6f7d8772":"markdown"},"source":{"45300636":"import warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport string\nimport keras\nimport numpy as np\nimport pandas as pd\nimport re\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom nltk.corpus import stopwords\nfrom plotly import graph_objs as go\nfrom plotly import express as px\nfrom plotly import figure_factory as ff\nfrom collections import Counter\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n! pip install bs4\nfrom bs4 import BeautifulSoup\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report,confusion_matrix,accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn import metrics\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport tensorflow as tf\nfrom keras.models import Model\nfrom keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.utils import to_categorical\nfrom keras.callbacks import EarlyStopping\nfrom keras.preprocessing import text, sequence\nfrom keras.models import Sequential\nfrom keras.layers import Dense,Embedding,LSTM,Dropout,Bidirectional,GRU\nfrom keras.callbacks import ReduceLROnPlateau\n\nfrom string import punctuation\nfrom nltk.tokenize.toktok import ToktokTokenizer\nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords","6c15997e":"primary_blue = \"#496595\";primary_blue2 = \"#85a1c1\";primary_blue3 = \"#3f4d63\";primary_grey = \"#c6ccd8\";","38cf6f22":"sms = pd.read_csv(\"\/kaggle\/input\/sms-spam-collection-dataset\/spam.csv\", encoding='latin-1')\nsms","e54ac0dd":"sms.dropna(how=\"any\", inplace=True, axis=1)\nsms.columns = ['label', 'message']\nsms.head()","e416259e":"sms.describe()","c1e00cf1":"sms.groupby('label').describe()","239c3dbf":"sms['messagelen'] = sms.message.apply(len)\nsms.head()","fd461bd2":"sms[sms.label=='ham'].describe()","933abcb2":"sms[sms.label=='spam'].describe()","9a947a37":"balance_counts = sms.groupby('label')['label'].agg('count').values\nbalance_counts","3431c965":"fig = go.Figure()\nfig.add_trace(go.Bar(\n    x=['ham'],\n    y=[balance_counts[0]],\n    name='ham',\n    text=[balance_counts[0]],\n    textposition='auto',\n    marker_color=primary_blue\n))\nfig.add_trace(go.Bar(\n    x=['spam'],\n    y=[balance_counts[1]],\n    name='spam',\n    text=[balance_counts[1]],\n    textposition='auto',\n    marker_color=primary_grey\n))\nfig.update_layout(\n    title='<span style=\"font-size:32px; font-family:Times New Roman\">Dataset distribution by target<\/span>'\n)\nfig.show()","8762f53c":"plt.figure(figsize=(12,5))\nsms[sms['label']=='spam']['messagelen'].plot(bins=35,kind='hist',color='blue',label='spam',alpha=0.5)\nplt.legend()\nplt.xlabel('message length')\nplt.show()","e99b9fff":"plt.figure(figsize=(12,5))\nsms[sms['label']=='ham']['messagelen'].plot(bins=35,kind='hist',color='red',label='spam',alpha=0.5)\nplt.legend()\nplt.xlabel('message length')\nplt.show()","15a27d7a":"df=sms.copy()\ndf.label.replace(\"ham\",0,inplace = True)\ndf.label.replace(\"spam\",1,inplace = True)\nstop = set(stopwords.words('english'))\npunctuation = list(string.punctuation)\nstop.update(punctuation)","1c8f707b":"def strip_html(text):\n    soup = BeautifulSoup(text, \"html.parser\")\n    return soup.get_text()\n\n#Removing the square brackets\ndef remove_between_square_brackets(text):\n    return re.sub('\\[[^]]*\\]', '', text)\n# Removing URL's\ndef remove_between_square_brackets(text):\n    return re.sub(r'http\\S+', '', text)\n#Removing the stopwords from text\ndef remove_stopwords(text):\n    final_text = []\n    for i in text.split():\n        if i.strip().lower() not in stop:\n            if i.strip().isalpha():\n                final_text.append(i.strip())\n    return \" \".join(final_text)\n#Removing the noisy text\ndef denoise_text(text):\n    text = strip_html(text)\n    text = remove_stopwords(text)\n    text = remove_stopwords(text)\n    return text\n#Apply function on review column\ndf['message']=df['message'].apply(denoise_text)","17155350":"plt.figure(figsize = (20,20)) # Text that is not Spam\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.label == 0].message))\nplt.imshow(wc , interpolation = 'bilinear')","8c29797c":"plt.figure(figsize = (20,20)) # Text that is Spam\nwc = WordCloud(max_words = 2000 , width = 1600 , height = 800 , stopwords = STOPWORDS).generate(\" \".join(df[df.label == 1].message))\nplt.imshow(wc , interpolation = 'bilinear')","6d007a05":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(20,10))\nword=df[df['label']==1]['message'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='red')\nax1.set_title('Spam')\nword=df[df['label']==0]['message'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='green')\nax2.set_title('Not Spam')\nfig.suptitle('Average word length in each text')","bb46a94a":"df1=df.copy()","4ee37557":"ps = PorterStemmer() # Using porterstemmer for text preprocessing\nmessage = []\nfor i in range(0, df1.shape[0]):\n    review = re.sub('[^a-zA-Z]', ' ', df1['message'][i])\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    message.append(review)","1a877e1f":"df1['clean_msg']=np.empty((len(message),1))\nfor i in range(len(message)):\n    df1['clean_msg'][i]=message[i]\ndf1['clean_msg_len']=df1['clean_msg'].apply(len)\ndf1.head()","109a5a1b":"X=df1['clean_msg']\nY=df1['label']","7ca78e0f":"cv = CountVectorizer(max_features=2500)\nX = cv.fit_transform(message).toarray()\nX","3bf766ba":"Y=np.array(Y)\ntype(Y)","125ebb22":"X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.10, random_state = 0)\n","93ea40b0":"print(\"X_train shape: {}\\n X_test shape: {}\\nY_train shape: {}\\nY_test shape: {}\".format(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape))","e22e70d0":"acc=[]","3d972dcb":"spam_detect_model = MultinomialNB().fit(X_train, Y_train)\npred=spam_detect_model.predict(X_test)\n\nprint(\"Accuracy of Naive Bayes Classifier is: {}\".format(metrics.accuracy_score(Y_test,pred)))\nacc.append(metrics.accuracy_score(Y_test,pred))","8c21083b":"LR=LogisticRegression(solver='liblinear')\nLR.fit(X_train,Y_train)\nyhat = LR.predict(X_test)\nprint(\"LogisticRegression's Accuracy:{0}\".format(metrics.accuracy_score(Y_test, yhat)))\nacc.append(metrics.accuracy_score(Y_test,yhat))","0457510c":"clf = svm.SVC(kernel='rbf')\nclf.fit(X_train, Y_train) \nyhat = clf.predict(X_test)\nprint(\"SVM's Accuracy:{0}\".format(metrics.accuracy_score(Y_test, yhat)))\nacc.append(metrics.accuracy_score(Y_test, yhat))","934450ac":"Random_forest = RandomForestClassifier(n_estimators=50)\nRandom_forest.fit(X_train,Y_train)\nrandomForest_predict = Random_forest.predict(X_test)\nrandomForest_score = metrics.accuracy_score(Y_test, randomForest_predict)\nprint(\"Random Forest Score :\",randomForest_score)\nacc.append(metrics.accuracy_score(Y_test,randomForest_predict ))","8a17ad81":"gbk = GradientBoostingClassifier(random_state=100, n_estimators=150,min_samples_split=100, max_depth=6)\ngbk.fit(X_train, Y_train)\ngbk_predict = gbk.predict(X_test)\nprint(\"Gradient Boosting Score :\",metrics.accuracy_score(Y_test,gbk_predict ))\nacc.append(metrics.accuracy_score(Y_test,gbk_predict ))","cc6d23c1":"mx=-1\nfor i in range(1,25):\n    \n    neigh=KNeighborsClassifier(n_neighbors = i).fit(X_train,Y_train)\n    yhat = neigh.predict(X_test)\n    KNN_score=metrics.accuracy_score(Y_test, yhat)\n    print(\"KNN Accuracy at {} is {}\".format(i,KNN_score))\n    mx=max(mx,KNN_score)\n    print(\"\\n\")\nacc.append(mx)\nmx","dc6c3176":"algo_name=['Naive Bayes Classifier','Logistic Regression','SVM','Random Forest Classifier','Gradient Boosting','KNN']\nacc=np.array(acc)\nfrom numpy import median\nplt.figure(figsize=(10,8))\nsns.barplot(y=acc*100,x=algo_name,estimator=median,palette=\"Blues_d\")\nplt.xlabel('Algorithm Name',size=30)\nplt.xticks(rotation=45)\nplt.ylabel('Accuracy',size=30)","614492d2":"x_train,x_test,y_train,y_test = train_test_split(df.message,df.label,random_state = 0)","2315aed1":"max_features = 4000\nmaxlen = 50\ntokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(x_train)\ntokenized_train = tokenizer.texts_to_sequences(x_train)\nx_train = sequence.pad_sequences(tokenized_train, maxlen=maxlen)","10481a6f":"tokenized_test = tokenizer.texts_to_sequences(x_test)\nx_test = sequence.pad_sequences(tokenized_test, maxlen=maxlen)","ae959a1d":"EMBEDDING_FILE = '..\/input\/twitter-embeded\/glove.twitter.27B.50d.txt'\ndef get_coefs(word, *arr): \n    return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE))","edff5af2":"all_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\n#change below line if computing normal stats is too slow\nembedding_matrix = embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, 50))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","2ec3040d":"batch_size = 64\nepochs = 5\nembed_size = 50","838efa91":"#Defining Neural Network\nmodel = Sequential()\n#Non-trainable embeddidng layer\nmodel.add(Embedding(max_features, output_dim=embed_size, weights=[embedding_matrix], input_length = maxlen, trainable=True))\n#LSTM \nmodel.add(Bidirectional(LSTM(units=128, return_sequences = True)))\nmodel.add(Bidirectional(GRU(units=32)))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","4fe9461f":"model.summary()","74a3950c":"history = model.fit(x_train, y_train, batch_size = batch_size , validation_data = (x_test,y_test) , epochs =20)","e0b71de5":"print(\"Accuracy of the model on Testing Data is - \" , model.evaluate(x_test,y_test)[1]*100 , \"%\")","0e919b4e":"pred = model.predict_classes(x_test)\npred[:5]","e96868c5":"print(classification_report(y_test, pred, target_names = ['Not Spam','Spam']))","08999a1f":"cm = confusion_matrix(y_test,pred)\ncm","10b38730":"cm = pd.DataFrame(cm , index = ['Not Spam','Spam'] , columns = ['Not Spam','Spam'])","59c7ccc7":"plt.figure(figsize = (10,10))\nsns.heatmap(cm,cmap= \"Blues\", linecolor = 'black' , linewidth = 1 , annot = True, fmt='' , xticklabels = ['Not Spam','Spam'] , yticklabels = ['Not Spam','Spam'])","ec16c934":"df2= pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',delimiter=',',encoding='latin-1')\ndf2.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'],axis=1,inplace=True)\ndf2.info()","0ab05c07":"X1= df2.v2\nY1 = df2.v1\nle = LabelEncoder()\nY1= le.fit_transform(Y)\nY1 = Y1.reshape(-1,1)","2e384410":"Xtrain,Xtest,Ytrain,Ytest = train_test_split(X1,Y1,test_size=0.15)","fa18d2d7":"max_words = 1000\nmax_len = 150\ntok = Tokenizer(num_words=max_words)\ntok.fit_on_texts(Xtrain)\nsequences = tok.texts_to_sequences(Xtrain)\nsequences_matrix = sequence.pad_sequences(sequences,maxlen=max_len)","c64f3a85":"def RNN():\n    inputs = Input(name='inputs',shape=[max_len])\n    layer = Embedding(max_words,50,input_length=max_len)(inputs)\n    layer = LSTM(64)(layer)\n    layer = Dense(256,name='FC1')(layer)\n    layer = Activation('relu')(layer)\n    layer = Dropout(0.5)(layer)\n    layer = Dense(1,name='out_layer')(layer)\n    layer = Activation('sigmoid')(layer)\n    model = Model(inputs=inputs,outputs=layer)\n    return model","f27761e7":"model = RNN()\nmodel.summary()\nmodel.compile(loss='binary_crossentropy',optimizer=RMSprop(),metrics=['accuracy'])","83480a8f":"model.fit(sequences_matrix,Ytrain,batch_size=128,epochs=10,\n          validation_split=0.2,callbacks=[EarlyStopping(monitor='val_loss',min_delta=0.0001)])","60ad6c83":"test_sequences = tok.texts_to_sequences(Xtest)\ntest_sequences_matrix = sequence.pad_sequences(test_sequences,maxlen=max_len)","8ebd1802":"accr = model.evaluate(test_sequences_matrix,Ytest)\nprint('Test set\\n  Loss: {:0.3f}\\n  Accuracy: {:0.3f}'.format(accr[0],accr[1]))","f355089a":"df1","91db5ceb":"Xtrain, Xtest, ytrain, ytest = train_test_split(df1['clean_msg'], df1['label'], test_size = 0.3, random_state=0, shuffle = True, stratify=df1['label'])","9b66f0e7":"Xtrain","cebd3184":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import TfidfVectorizer","036c431b":"def tfidf(data):\n    tfidf_vec = TfidfVectorizer()\n    tfidf_model = tfidf_vec.fit(data)\n    print(tfidf_model.dtype)\n    X_tfidf = tfidf_model.transform(data)\n    return X_tfidf","809345cf":"tfidf(Xtrain)","f66a6a0f":"clf = Pipeline([('tfidf', TfidfVectorizer()), ('clf', RandomForestClassifier(n_estimators=100, n_jobs=-1))])","d801ea5b":"clf.fit(Xtrain,ytrain)","8e2c2f03":"! pip install gradio","7de59d0e":"import gradio as gr\ncounter = 0\ndef prdict(mssg):\n    ans=clf.predict([mssg])\n    ans=list(ans);ans=ans[0];\n    if ans==0:\n        s=\"Ham\"\n        return s\n    else:\n        f=\"Spam\"\n        return f\niface = gr.Interface(fn=prdict, inputs=\"text\", outputs=\"text\")\niface.launch(share=True)","c903f413":"# EDA","0c486806":"# Data Exploration","42447f02":"**lets perform stemming or lemmetzation and vectorization** ","4f7206f8":"# Data Visulaization","1ee66aa0":"# Data Cleaning Indepth ","6f01bbe3":"# Glove method","94beb2ff":"# RNN","6f7d8772":"# Ml algorithms"}}