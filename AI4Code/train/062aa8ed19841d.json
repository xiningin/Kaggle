{"cell_type":{"1d0b1cc1":"code","81aa81b9":"code","aca6bd04":"code","23f5928f":"code","1fa859bb":"code","09367fcc":"code","144be025":"code","640c868d":"code","5bf7e838":"code","e8a36110":"code","f2c297c2":"code","b90e5ece":"code","094035c2":"code","13dc7ca0":"code","47869a3d":"code","42d38875":"code","af625e00":"code","cecf57cb":"code","dfb9c062":"code","8a1f4946":"code","8e2ab91f":"code","196cea58":"code","2e591c94":"code","13418e30":"code","31925ef5":"code","90d56889":"code","6e82c92e":"code","c27ae258":"code","ff8fbc12":"code","3b6e451b":"code","3aa5a035":"code","558f77ce":"code","ad5831fa":"code","9097d895":"code","845052ea":"code","98f155c9":"code","affe0730":"code","71927127":"code","c6d26ee7":"code","74c58dea":"markdown","e0c88694":"markdown","76b0fe72":"markdown","f8002903":"markdown","8544fd8f":"markdown","8ca5fd25":"markdown","136bc4c4":"markdown","0a5f1414":"markdown","230c4567":"markdown","552aa144":"markdown","975d4a3e":"markdown","072c1e7b":"markdown","b9562316":"markdown","714e9399":"markdown","2eef7b1d":"markdown","a35f1dee":"markdown","f8d243ec":"markdown","7a5c9e69":"markdown","00f57ce5":"markdown","6e5d1fa8":"markdown","f76bc305":"markdown","b225ba4e":"markdown","2d2798db":"markdown","1f13d71a":"markdown","fadf644a":"markdown","63ec1635":"markdown","2ecf4e10":"markdown","b573c83b":"markdown","baaf72c1":"markdown","3eb995b8":"markdown","17f7d628":"markdown"},"source":{"1d0b1cc1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.linear_model import LinearRegression","81aa81b9":"data = pd.read_csv('..\/input\/real-estate-price-prediction\/Real estate.csv')","aca6bd04":"data.head()","23f5928f":"data.sample(frac=0.05)","1fa859bb":"data.shape","09367fcc":"data.columns","144be025":"data.describe()","640c868d":"data.info()","5bf7e838":"data.isnull().sum()","e8a36110":"sns.displot(data, x = 'Y house price of unit area', kind = 'kde')","f2c297c2":"q = data['Y house price of unit area'].quantile(0.99)\ndata_1 = data[data['Y house price of unit area']<q]","b90e5ece":"sns.displot(data_1, x = 'Y house price of unit area', kind = 'kde')","094035c2":"sns.displot(data_1, x= 'X2 house age', kind = 'kde')","13dc7ca0":"sns.displot(data_1, x= 'X3 distance to the nearest MRT station', kind = 'kde')","47869a3d":"sns.displot(data_1, x='X4 number of convenience stores', kind = 'kde')","42d38875":"data_2 = data_1.drop(['No'], axis = 1)\ndata_2.columns","af625e00":"data_cleaned = data_2.reset_index(drop=True)","cecf57cb":"data_cleaned","dfb9c062":"cor = data_cleaned.corr()\nplt.figure(figsize = (12,8))\nsns.heatmap(cor, annot=True)","8a1f4946":"cor_target = abs(cor['Y house price of unit area'])\ncor_target","8e2ab91f":"df = data_cleaned.iloc[:,0:6]\ncor1 = df.corr()\nplt.figure(figsize = (10,6))\nsns.heatmap(cor1, annot = True)","196cea58":"from statsmodels.stats.outliers_influence import variance_inflation_factor","2e591c94":"X = data_cleaned[['X1 transaction date', 'X2 house age', 'X3 distance to the nearest MRT station', 'X4 number of convenience stores', 'X5 latitude', 'X6 longitude']]\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\nprint(vif_data)","13418e30":"X = data_cleaned[['X2 house age', 'X3 distance to the nearest MRT station', 'X4 number of convenience stores']]\nvif_data = pd.DataFrame()\nvif_data[\"feature\"] = X.columns\nvif_data[\"VIF\"] = [variance_inflation_factor(X.values, i)\n                          for i in range(len(X.columns))]\nprint(vif_data)","31925ef5":"features_selected = vif_data['feature']\nfeatures_selected","90d56889":"final_data = data_cleaned[['X2 house age','X3 distance to the nearest MRT station','X4 number of convenience stores','Y house price of unit area']]\nfinal_data.columns","6e82c92e":"g = sns.PairGrid(final_data, y_vars=[\"Y house price of unit area\"], x_vars=[\"X2 house age\", \"X3 distance to the nearest MRT station\", \"X4 number of convenience stores\"], height=4)\ng.map(sns.regplot, color=\"b\", line_kws={'color': 'red'})","c27ae258":"targets = final_data['Y house price of unit area']\ninputs = final_data.drop(['Y house price of unit area'],axis=1)","ff8fbc12":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(inputs, targets, test_size=0.33, random_state=365)","3b6e451b":"reg = LinearRegression()\nreg.fit(x_train,y_train)\ny_hat = reg.predict(x_train)","3aa5a035":"reg.score(x_train,y_train).round(3)","558f77ce":"y_resid = y_train - y_hat\n\nfig, ax = plt.subplots(1,2, constrained_layout = True)\n    \nsns.regplot(x=y_hat, y=y_train, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Observed vs. Predicted Values', fontsize=10)\nax[0].set(xlabel='Predicted', ylabel='Observed')\n\nsns.regplot(x=y_hat, y=y_resid, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Residuals vs. Predicted Values', fontsize=10)\nax[1].set(xlabel='Predicted', ylabel='Residuals')","ad5831fa":"y_resid.hist()\nplt.show()","9097d895":"intercept = reg.intercept_.round(3)","845052ea":"coeff = reg.coef_.round(3)\ncoeff","98f155c9":"reg_summary = pd.DataFrame(inputs.columns.values, columns=['Features'])\nreg_summary['Weights'] = coeff\nreg_summary","affe0730":"y_hat_test = reg.predict(x_test)","71927127":"reg.score(x_test,y_test).round(3)","c6d26ee7":"y_resid_test = y_test - y_hat_test\n\nfig, ax = plt.subplots(1,2, constrained_layout = True)\n    \nsns.regplot(x=y_hat_test, y=y_test, ax=ax[0], line_kws={'color': 'red'})\nax[0].set_title('Observed vs. Predicted Values', fontsize=10)\nax[0].set(xlabel='Predicted', ylabel='Observed')\n\nsns.regplot(x=y_hat_test, y=y_resid_test, ax=ax[1], line_kws={'color': 'red'})\nax[1].set_title('Residuals vs. Predicted Values', fontsize=10)\nax[1].set(xlabel='Predicted', ylabel='Residuals')","74c58dea":"# Check for Linearity","e0c88694":"# Train Test Split","76b0fe72":"**We will use filter method for feature selection. In this method, filtering is done using correlation matrix and it is most commonly done using Pearson correlation and VIF.**","f8002903":"*We can see high correlation of 0.71, 0.61, 0.57, 0.56 between X3 & Y, X4 & Y, X5 & Y, X6 & Y respectively.*\n\n*Now, we will check for correlation among the features or independent variables.*","8544fd8f":"*Obviously, there are some outliers present. We will use percentile based approach to remove the outliers from our data.*","8ca5fd25":"# Declare the Inputs and the Targets","136bc4c4":"**Variance Inflation Factor**","0a5f1414":"*65.10% of the data fit the regression model.*","230c4567":"# Testing","552aa144":"**Pearson Correlation**","975d4a3e":"> **Our linear model is not appropriate as it violates the OLS assumptions of Normality and homogeneity. There are ways to fix these issues which requires detailed study on my part as I am a beginner in ML. So far I enjoyed performing this analysis, and my understanding on regression technique has increased. You learn by doing.**","072c1e7b":"*Our mathematical model is yhat = 41.646 - 0.258X2 house age - 0.005X3 distance to the nearest MRT station + 1.506X4 number of convenience stores*","b9562316":"# Finding the Weights and Bias","714e9399":"*From the above plots, we can see linear relationship but not a perfect one. As the house age increases, the price decreases. As the distance to the nearest MRT station increases, the price decreases. And, as the number of convenience stores increases, the price increases.*","2eef7b1d":"*The residuals are skewed on the right or we can say the distribution of residuals is non-normal. So, it further violates the OLS assumptions*\n\n*Without considering these violations, I will test the model.*","a35f1dee":"# Create the Regression","f8d243ec":"# Explore the Dataset","7a5c9e69":"*So by removing the features with high values of VIF, we have brought down the values of other variables closer to 1.*","00f57ce5":"*The points are not symmetrically distributed around the diagonal line neither around the horizontal line. Hence, there is violation of the linearity and homogeneity assumption.*","6e5d1fa8":"*The Correlation matrix above shows high correlation among some features. Hence, there is possibility of multicollinearity. We will check for multicollinearity among the independent varibales by calculating the VIF's and drop those features with values>5.*","f76bc305":"# Check for Missing Values","b225ba4e":"*Thus, we have removed some outliers from our data and we will proceed further with feature selection.*\n\n*I will drop the 'No' column from the dataset as it's not needed further.*","2d2798db":"# Import the relevant libraries","1f13d71a":"*Let's plot PDF's with our new data (data_1) and see whether we have removed some outliers or not.*","fadf644a":"# Descriptive Statistics of the Dataset","63ec1635":"# Load the Dataset","2ecf4e10":"# Exploring the PDF and dealing with the outliers","b573c83b":"*There are no missing values in the dataset.*","baaf72c1":"# Feature Selection","3eb995b8":"*The points are not symmetrically distributed around the diagonal line neither around the horizontal line. Hence, there is violation of the linearity and homogeneity assumption.*","17f7d628":"*Finally, we will keep those features whose VIF is less than 5. (VIF < 5)*\n\n*We will again calculate the VIF's after removing those with value>5.*"}}