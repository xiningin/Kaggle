{"cell_type":{"663fd10d":"code","434a0a59":"code","778c2246":"code","0269909c":"code","736eb86f":"code","bf8d57c4":"code","3ff305c0":"code","bc673ba2":"code","2b609403":"code","409f926f":"code","94df2ca0":"code","f0b25113":"code","3a812cab":"code","ff78e8b0":"code","0458f298":"code","9383aad5":"code","d65eefd7":"code","e7740d99":"code","72589566":"code","c675b91b":"code","6c3b48d7":"code","80256103":"code","cc752b63":"markdown","eaadb8d2":"markdown","42d23e32":"markdown","c8a4de3f":"markdown","5fcf0639":"markdown","245b1804":"markdown","20ef81e2":"markdown"},"source":{"663fd10d":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","434a0a59":"df = pd.read_csv('..\/input\/purchasedata\/Data.csv')\ndf","778c2246":"# Check if there are any duplicates\ndf.duplicated()","0269909c":"# Drop Duplicates\ndf.drop_duplicates(inplace=True)\ndf","736eb86f":"# Check if there is any missing data\ndf.isnull()","bf8d57c4":"# Fill missing data with 0\ndf.fillna(0)","3ff305c0":"# Or Drop entries with missing data\ndf.dropna()","bc673ba2":"# Or fill the missing data with the mean of corresponding column\navg_age = df['Age'].mean()\navg_salary = df['Salary'].mean()\nprint(avg_age)\nprint(avg_salary)","2b609403":"df['Age'].replace(np.nan,avg_age,inplace = True)\ndf['Salary'].replace(np.nan,avg_salary, inplace = True)\ndf","409f926f":"df2 = pd.read_csv('..\/input\/purchasedata\/Data.csv')\ndf2.drop_duplicates(inplace = True)\nX = df2.iloc[:,:-1].values\nY = df2.iloc[:,-1].values\nprint(X)\nprint(Y)\ndf2","94df2ca0":"# Using Scikit-Learn to deal with missing data\nfrom sklearn.impute import SimpleImputer\nimp = SimpleImputer(missing_values = np.nan, strategy = 'mean')     \nimp.fit(X[:,1:3])\nX[:, 1:3] = imp.transform(X[: , 1:3])\nX","f0b25113":"from sklearn.preprocessing import StandardScaler        #MinMaxScaler can also be used\nsc = StandardScaler()\nX[:,1:] = sc.fit_transform(X[:,1:])\nX","3a812cab":"# Encoding independent variable\ndummy1 = pd.get_dummies(df['Country'])\ndummy1","ff78e8b0":"df = pd.concat([df,dummy1], axis=1)\ndf","0458f298":"dummy = df.iloc[:,4:].values\ndummy","9383aad5":"np.concatenate([dummy,X[:,1:]],axis = 1)","d65eefd7":"#Encoding dependent variable\ndummy2 = pd.get_dummies(df['Purchased'])\ndummy2","e7740d99":"#Encoding independent variable\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nct = ColumnTransformer(transformers=[('encoder', OneHotEncoder(), [0])], remainder='passthrough')\nX = np.array(ct.fit_transform(X))\nX","72589566":"#Encoding Dependent variable\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(Y)\nY","c675b91b":"print(X)\nprint(Y)","6c3b48d7":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = 0.2, random_state = 0)\nprint(X_train)\nprint(y_train)","80256103":"print(X_test, y_test)","cc752b63":"# Data Preprocessing","eaadb8d2":"# 4. Splitting the data into training and test sets","42d23e32":"### Using Scikit-Learn","c8a4de3f":"## 2.Feature scaling(normalisation)","5fcf0639":"#### Using Scikit-Learn","245b1804":"## 3.Handling categorical data","20ef81e2":"## 1.Handling missing and duplicate data"}}