{"cell_type":{"570ef3a1":"code","a3f42380":"code","0fe8a5ee":"code","2af16489":"code","bc323bc6":"code","ccf55dd7":"code","5cd25bd3":"code","2ebf70d2":"code","5a78550a":"code","89122e3d":"code","dbb5a48a":"code","3607c6be":"code","85cd9a3d":"code","fde95d98":"code","a0262b64":"code","c95d0e83":"code","f23c1e9e":"code","0868066f":"code","2710a10c":"code","8eea7264":"code","be63da38":"code","a4e825ca":"code","3517bfd5":"code","9b346ec7":"code","63484518":"code","11b644e1":"code","2e358710":"code","af951c17":"code","8df57e4b":"code","7db8689c":"code","2b98f45b":"code","28918362":"code","a0788f73":"code","c1ec0b76":"code","2a180d5b":"code","4a85ad67":"code","cd0adc88":"code","5bdd2c60":"code","a418e3b2":"code","701fa72c":"code","465f98b2":"code","84c94b49":"code","fab06f7d":"markdown","f6ccadf6":"markdown","9de9036a":"markdown","ef293c51":"markdown","300ff073":"markdown","a3e4494c":"markdown","f33934b8":"markdown","30bedccb":"markdown","18e0634c":"markdown","dfd747d8":"markdown","49d941b7":"markdown","a2c175f1":"markdown","df1cfbbb":"markdown","766696aa":"markdown","1b22d046":"markdown","0fb6e8dc":"markdown","91678ab2":"markdown","735c71ba":"markdown","7ee702af":"markdown","73ab2dca":"markdown","64fddfd3":"markdown","400ef56d":"markdown","9b5f17d4":"markdown","669d7153":"markdown","0da8b1a2":"markdown","6399c10e":"markdown","2cb88c0d":"markdown","131b8802":"markdown","9e9b1a65":"markdown"},"source":{"570ef3a1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pylab as plt\nimport seaborn as sns\nfrom sklearn import metrics\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nplt.style.use('ggplot')\ncolor_pal = [x['color'] for x in plt.rcParams['axes.prop_cycle']]","a3f42380":"# Show how the files appear in the input folder\n!ls -GFlash --color ..\/input","0fe8a5ee":"train_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')","2af16489":"train_df.head()","bc323bc6":"print('The training set has shape {}'.format(train_df.shape))\nprint('The test set has shape {}'.format(test_df.shape))","ccf55dd7":"# Distribution of the target\ntrain_df['scalar_coupling_constant'].plot(kind='hist', figsize=(20, 5), bins=1000, title='Distribution of the target scalar coupling constant')\nplt.show()","5cd25bd3":"# Number of of atoms in molecule\nfig, ax = plt.subplots(1, 2)\ntrain_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[6],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Train Set)',\n                                                                      ax=ax[0])\ntest_df.groupby('molecule_name').count().sort_values('id')['id'].plot(kind='hist',\n                                                                       bins=25,\n                                                                       color=color_pal[2],\n                                                                      figsize=(20, 5),\n                                                                      title='# of Atoms in Molecule (Test Set)',\n                                                                     ax=ax[1])\nplt.show()","2ebf70d2":"ss = pd.read_csv('..\/input\/sample_submission.csv')\nss.head()","5a78550a":"test_df.head()","89122e3d":"train_df.head(1)","dbb5a48a":"! cat ..\/input\/structures\/dsgdb9nsd_000001.xyz","3607c6be":"structures = pd.read_csv('..\/input\/structures.csv')\nstructures.head()","85cd9a3d":"# 3D Plot!\nfrom mpl_toolkits.mplot3d import Axes3D\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nexample = structures.loc[structures['molecule_name'] == 'dsgdb9nsd_000001']\nax.scatter(xs=example['x'], ys=example['y'], zs=example['z'], s=100)\nplt.suptitle('dsgdb9nsd_000001')\nplt.show()","fde95d98":"dm = pd.read_csv('..\/input\/dipole_moments.csv')\ndm.head()","a0262b64":"mst = pd.read_csv('..\/input\/magnetic_shielding_tensors.csv')\nmul = pd.read_csv('..\/input\/mulliken_charges.csv')\npote = pd.read_csv('..\/input\/potential_energy.csv')\nscc = pd.read_csv('..\/input\/scalar_coupling_contributions.csv')","c95d0e83":"mst.head()","f23c1e9e":"mul.head()","0868066f":"# Plot the distribution of mulliken_charges\nmul['mulliken_charge'].plot(kind='hist', figsize=(15, 5), bins=500, title='Distribution of Mulliken Charges')\nplt.show()","2710a10c":"pote.head()","8eea7264":"# Plot the distribution of potential_energy\npote['potential_energy'].plot(kind='hist',\n                              figsize=(15, 5),\n                              bins=500,\n                              title='Distribution of Potential Energy',\n                              color='b')\nplt.show()","be63da38":"scc.head()","a4e825ca":"scc.groupby('type').count()['molecule_name'].sort_values().plot(kind='barh',\n                                                                color='grey',\n                                                               figsize=(15, 5),\n                                                               title='Count of Coupling Type in Train Set')\nplt.show()","3517bfd5":"fig, ax = plt.subplots(2, 2, figsize=(20, 10))\nscc['fc'].plot(kind='hist', ax=ax.flat[0], bins=500, title='Fermi Contact contribution', color=color_pal[0])\nscc['sd'].plot(kind='hist', ax=ax.flat[1], bins=500, title='Spin-dipolar contribution', color=color_pal[1])\nscc['pso'].plot(kind='hist', ax=ax.flat[2], bins=500, title='Paramagnetic spin-orbit contribution', color=color_pal[2])\nscc['dso'].plot(kind='hist', ax=ax.flat[3], bins=500, title='Diamagnetic spin-orbit contribution', color=color_pal[3])\nplt.show()","9b346ec7":"scc = scc.merge(train_df)","63484518":"# Downsample to speed up plot time.\nsns.pairplot(data=scc.sample(5000), hue='type', vars=['fc','sd','pso','dso','scalar_coupling_constant'])\nplt.show()","11b644e1":"atom_count_dict = structures.groupby('molecule_name').count()['atom_index'].to_dict()","2e358710":"train_df['atom_count'] = train_df['molecule_name'].map(atom_count_dict)\ntest_df['atom_count'] = test_df['molecule_name'].map(atom_count_dict)","af951c17":"train_df.sample(1000).plot(x='atom_count',\n                           y='scalar_coupling_constant',\n                           kind='scatter',\n                           color=color_pal[0],\n                           figsize=(20, 5),\n                           alpha=0.5)\nplt.show()","8df57e4b":"train_df.groupby('type')['scalar_coupling_constant'].mean().plot(kind='barh',\n                                                                 figsize=(15, 5),\n                                                                title='Average Scalar Coupling Constant by Type')\nplt.show()\n# THIS IS A MODEL!!! This is a model??\ntype_mean_dict = train_df.groupby('type')['scalar_coupling_constant'].mean().to_dict()\ntest_df['scalar_coupling_constant'] = test_df['type'].map(type_mean_dict)\ntest_df[['id','scalar_coupling_constant']].to_csv('super_simple_submission.csv', index=False)","7db8689c":"def metric(df, preds):\n    df[\"prediction\"] = preds\n    maes = []\n    for t in df.type.unique():\n        y_true = df[df.type==t].scalar_coupling_constant.values\n        y_pred = df[df.type==t].prediction.values\n        mae = np.log(metrics.mean_absolute_error(y_true, y_pred))\n        maes.append(mae)\n    return np.mean(maes)","2b98f45b":"def map_atom_info(df, atom_idx):\n    df = pd.merge(df, structures, how = 'left',\n                  left_on  = ['molecule_name', f'atom_index_{atom_idx}'],\n                  right_on = ['molecule_name',  'atom_index'])\n    \n    df = df.drop('atom_index', axis=1)\n    df = df.rename(columns={'atom': f'atom_{atom_idx}',\n                            'x': f'x_{atom_idx}',\n                            'y': f'y_{atom_idx}',\n                            'z': f'z_{atom_idx}'})\n    return df\n\ntrain_df = map_atom_info(train_df, 0)\ntrain_df = map_atom_info(train_df, 1)\n\ntest_df = map_atom_info(test_df, 0)\ntest_df = map_atom_info(test_df, 1)","28918362":"# https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark\ntrain_p_0 = train_df[['x_0', 'y_0', 'z_0']].values\ntrain_p_1 = train_df[['x_1', 'y_1', 'z_1']].values\ntest_p_0 = test_df[['x_0', 'y_0', 'z_0']].values\ntest_p_1 = test_df[['x_1', 'y_1', 'z_1']].values\n\ntrain_df['dist'] = np.linalg.norm(train_p_0 - train_p_1, axis=1)\ntest_df['dist'] = np.linalg.norm(test_p_0 - test_p_1, axis=1)","a0788f73":"# make categorical variables\natom_map = {'H': 0,\n            'C': 1,\n            'N': 2}\ntrain_df['atom_0_cat'] = train_df['atom_0'].map(atom_map).astype('int')\ntrain_df['atom_1_cat'] = train_df['atom_1'].map(atom_map).astype('int')\ntest_df['atom_0_cat'] = test_df['atom_0'].map(atom_map).astype('int')\ntest_df['atom_1_cat'] = test_df['atom_1'].map(atom_map).astype('int')","c1ec0b76":"# One Hot Encode the Type\ntrain_df = pd.concat([train_df, pd.get_dummies(train_df['type'])], axis=1)\ntest_df = pd.concat([test_df, pd.get_dummies(test_df['type'])], axis=1)","2a180d5b":"color_index = 0\naxes_index = 0\nfig, axes = plt.subplots(8, 1, figsize=(20, 20), sharex=True)\nfor mtype, d in train_df.groupby('type'):\n    d['dist'].plot(kind='hist',\n                  bins=1000,\n                  title='Distribution of Distance Feature for {}'.format(mtype),\n                  color=color_pal[color_index],\n                  ax=axes[axes_index])\n    if color_index == 6:\n        color_index = 0\n    else:\n        color_index += 1\n    axes_index += 1\nplt.show()","4a85ad67":"train_df['dist_to_type_mean'] = train_df['dist'] \/ train_df.groupby('type')['dist'].transform('mean')\ntest_df['dist_to_type_mean'] = test_df['dist'] \/ test_df.groupby('type')['dist'].transform('mean')","cd0adc88":"# Configurables\nFEATURES = ['atom_index_0', 'atom_index_1',\n            'atom_0_cat',\n            'x_0', 'y_0', 'z_0',\n            'atom_1_cat', \n            'x_1', 'y_1', 'z_1', 'dist', 'dist_to_type_mean',\n            'atom_count',\n            '1JHC', '1JHN', '2JHC', '2JHH', '2JHN', '3JHC', '3JHH', '3JHN'\n           ]\nTARGET = 'scalar_coupling_constant'\nCAT_FEATS = ['atom_0','atom_1']\nN_ESTIMATORS = 2000\nVERBOSE = 500\nEARLY_STOPPING_ROUNDS = 200\nRANDOM_STATE = 529\n\nX = train_df[FEATURES]\nX_test = test_df[FEATURES]\ny = train_df[TARGET]","5bdd2c60":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_absolute_error\nimport lightgbm as lgb\n\nlgb_params = {'num_leaves': 128,\n              'min_child_samples': 64,\n              'objective': 'regression',\n              'max_depth': 6,\n              'learning_rate': 0.1,\n              \"boosting_type\": \"gbdt\",\n              \"subsample_freq\": 1,\n              \"subsample\": 0.9,\n              \"bagging_seed\": 11,\n              \"metric\": 'mae',\n              \"verbosity\": -1,\n              'reg_alpha': 0.1,\n              'reg_lambda': 0.4,\n              'colsample_bytree': 1.0\n         }\n\nRUN_LGB = False\n\nif RUN_LGB:\n    n_fold = 5\n    folds = KFold(n_splits=n_fold, shuffle=True, random_state=RANDOM_STATE)\n\n    # Setup arrays for storing results\n    oof = np.zeros(len(X))\n    prediction = np.zeros(len(X_test))\n    scores = []\n    feature_importance = pd.DataFrame()\n\n    # Train the model\n    for fold_n, (train_idx, valid_idx) in enumerate(folds.split(X)):\n        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n        model = lgb.LGBMRegressor(**lgb_params, n_estimators = N_ESTIMATORS, n_jobs = -1)\n        model.fit(X_train, y_train,\n                  eval_set=[(X_train, y_train), (X_valid, y_valid)],\n                  eval_metric='mae',\n                  verbose=VERBOSE,\n                  early_stopping_rounds=EARLY_STOPPING_ROUNDS)\n\n        y_pred_valid = model.predict(X_valid)\n        y_pred = model.predict(X_test, num_iteration=model.best_iteration_)\n\n        # feature importance\n        fold_importance = pd.DataFrame()\n        fold_importance[\"feature\"] = FEATURES\n        fold_importance[\"importance\"] = model.feature_importances_\n        fold_importance[\"fold\"] = fold_n + 1\n        feature_importance = pd.concat([feature_importance, fold_importance], axis=0)\n\n        prediction \/= folds.n_splits\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        print('CV mean score: {0:.4f}, std: {1:.4f}.'.format(np.mean(scores), np.std(scores)))\n        oof[valid_idx] = y_pred_valid.reshape(-1,)\n        scores.append(mean_absolute_error(y_valid, y_pred_valid))\n        prediction += y_pred","a418e3b2":"if RUN_LGB:\n    # Save Prediction and name appropriately\n    submission_csv_name = 'submission_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    oof_csv_name = 'oof_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n    fi_csv_name = 'fi_lgb_{}folds_{}CV.csv'.format(n_fold, np.mean(scores))\n\n    print('Saving LGB Submission as:')\n    print(submission_csv_name)\n    ss = pd.read_csv('..\/input\/sample_submission.csv')\n    ss['scalar_coupling_constant'] = prediction\n    ss.to_csv(submission_csv_name, index=False)\n    ss.head()\n    # OOF\n    oof_df = train_df[['id','molecule_name','scalar_coupling_constant']].copy()\n    oof_df['oof_pred'] = oof\n    oof_df.to_csv(oof_csv_name, index=False)\n    # Feature Importance\n    feature_importance.to_csv(fi_csv_name, index=False)","701fa72c":"if RUN_LGB:\n    # Plot feature importance as done in https:\/\/www.kaggle.com\/artgor\/artgor-utils\n    feature_importance[\"importance\"] \/= folds.n_splits\n    cols = feature_importance[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(\n        by=\"importance\", ascending=False)[:50].index\n\n    best_features = feature_importance.loc[feature_importance.feature.isin(cols)]\n\n    plt.figure(figsize=(15, 20));\n    ax = sns.barplot(x=\"importance\",\n                y=\"feature\",\n                hue='fold',\n                data=best_features.sort_values(by=\"importance\", ascending=False));\n    plt.title('LGB Features (avg over folds)');","465f98b2":"from catboost import Pool, cv\n\nRUN_CATBOOST_CV = False\n\nif RUN_CATBOOST_CV:\n    labels = train_df['scalar_coupling_constant'].values\n    cat_features = ['type','atom_count','atom_0','atom_1']\n    cv_data = train_df[['type','atom_count','atom_0','atom_1',\n                        'x_0','y_0','z_0','x_1','y_1','z_1','dist']]\n    cv_dataset = Pool(data=cv_data,\n                      label=labels,\n                      cat_features=cat_features)\n\n    ITERATIONS = 100000\n    params = {\"iterations\": ITERATIONS,\n              \"learning_rate\" : 0.02,\n              \"depth\": 7,\n              \"loss_function\": \"MAE\",\n              \"verbose\": False,\n              \"task_type\" : \"GPU\"}\n\n    scores = cv(cv_dataset,\n                params,\n                fold_count=5, \n                plot=\"True\")\n    \n    scores['iterations'] = scores['iterations'].astype('int')\n    scores.set_index('iterations')[['test-MAE-mean','train-MAE-mean']].plot(figsize=(15, 5), title='CV (MAE) Score by iteration (5 Folds)')","84c94b49":"from catboost import CatBoostRegressor, Pool\n\nITERATIONS = 200000\n\nFEATURES = [#'atom_index_0',\n            'atom_index_1',\n            'atom_0',\n            'x_0', 'y_0', 'z_0',\n            'atom_1', \n            'x_1', 'y_1', 'z_1',\n            'dist', 'dist_to_type_mean',\n            'atom_count',\n            'type']\nTARGET = 'scalar_coupling_constant'\nCAT_FEATS = ['atom_0','atom_1','type']\n\ntrain_dataset = Pool(data=train_df[FEATURES],\n                  label=train_df['scalar_coupling_constant'].values,\n                  cat_features=CAT_FEATS)\n\ncb_model = CatBoostRegressor(iterations=ITERATIONS,\n                             learning_rate=0.2,\n                             depth=7,\n                             eval_metric='MAE',\n                             random_seed = 529,\n                             task_type=\"GPU\")\n\n# Fit the model\ncb_model.fit(train_dataset, verbose=1000)\n\n# Predict\ntest_data = test_df[FEATURES]\n\ntest_dataset = Pool(data=test_data,\n                    cat_features=CAT_FEATS)\n\nss = pd.read_csv('..\/input\/sample_submission.csv')\nss['scalar_coupling_constant'] = cb_model.predict(test_dataset)\nss.to_csv('basline_catboost_submission.csv', index=False)","fab06f7d":"# Predicting Molecular Properties\nCan you measure the magnetic interactions between a pair of atoms?\n\nIn this competition, you will develop an algorithm that can predict the magnetic interaction between two atoms in a molecule (i.e., the scalar coupling constant).\n\n![](http:\/\/www.chem.ucalgary.ca\/courses\/350\/Carey5th\/Ch13\/coupling04.gif)\n\n**NOTE : Some (but not all) of the text in this kernel was taken from the competition details. I do this to show exactly what the description and rules are for the competition alongside some exploritory code. Be sure to read the competition details yourself directly from the website here: https:\/\/www.kaggle.com\/c\/champs-scalar-coupling\/overview\/description. **","f6ccadf6":"## magnetic_shielding_tensors.csv\n- contains the magnetic shielding tensors for all atoms in the molecules. The first column (molecule_name) contains the molecule name, the second column (atom_index) contains the index of the atom in the molecule, the third to eleventh columns contain the XX, YX, ZX, XY, YY, ZY, XZ, YZ and ZZ elements of the tensor\/matrix respectively.\n\n## mulliken_charges.csv\n- contains the mulliken charges for all atoms in the molecules. The first column (molecule_name) contains the name of the molecule, the second column (atom_index) contains the index of the atom in the molecule, the third column (mulliken_charge) contains the mulliken charge of the atom.\n\n## potential_energy.csv\n- contains the potential energy of the molecules. The first column (molecule_name) contains the name of the molecule, the second column (potential_energy) contains the potential energy of the molecule.","9de9036a":"The target distribution is pretty interesting! Spikes near zero, and -20. There is also a good bit around 80. We will return to these files later.","ef293c51":"## Look at the xyz file for this example\n- The first number `5` is the number of atoms in this molecule.\n- A blank line....\n- Each following line is the element and their cartesian coordinates. So in this examples we have one Carbon atom and four Hydrogen atoms!","300ff073":"## Structures.csv\nThis file contains the same information as the individual xyz structure files, but in a single file.\n\nAt first glance - this csv seems a lot more useable than the `xyz` files","a3e4494c":"The training set is larger than the test set.\n","f33934b8":"## Target vs. Atom Count","30bedccb":"We can see in the test set each molicule name is associated with an id.","18e0634c":"# ...more to come","dfd747d8":"# Super Simple Baseline Model [1.239 Public LB]\nThe simplest thing we can do as a model is predict that the target is the **average** value that we observe for that **type** in the training set!","49d941b7":"Evaluation metric is important to understand as it determines how your model will be scored. Ideally we will set the loss function of our machine learning algorithm to use this metric so we can minimize the specific type of error.\n\nCheck out this kernel by `@abhishek` with code for the evaluation metric: https:\/\/www.kaggle.com\/abhishek\/competition-metric\n","a2c175f1":"# Baseline Models\n- using **atom_count** and **type** as categorical features\n- Run this in a notebook to see the interactive plot of training and test error metrics.","df1cfbbb":"## train.csv and test.csv\nThe training set, where the first column `molecule_name` is the name of the molecule where the coupling constant originates (the corresponding XYZ file is located at .\/structures\/.xyz), the second `atom_index_0` and third column `atom_index_1` is the atom indices of the atom-pair creating the coupling and the fourth column `scalar_coupling_constant` is the scalar coupling constant that we want to be able to predict","766696aa":"## dipole_moments.csv\n- contains the molecular electric dipole moments. These are three dimensional vectors that indicate the charge distribution in the molecule. The first column (molecule_name) are the names of the molecule, the second to fourth column are the X, Y and Z components respectively of the dipole moment.","1b22d046":"We can see each observation provides the:\n- molecule_name\n- atom_index_0 - index of the first atom pair\n- atom_index_1 - index of the second atom pair\n- scalar_coupling_constant (target)","0fb6e8dc":"# Additional Data\n*NOTE: additional data is provided for the molecules in Train only!*\n\n","91678ab2":"## sample_submission file\nWe are predicting for a given `id` found in the test set. The id corresponds with a given `molecule_name`","735c71ba":"## scalar_coupling_contributions.csv\n- The scalar coupling constants in train.csv (or corresponding files) are a sum of four terms. scalar_coupling_contributions.csv contain all these terms.\n    - The first column (molecule_name) are the **name of the molecule**,\n    - the second **(atom_index_0)** and\n    - third column **(atom_index_1)** are the atom indices of the atom-pair,\n    - the fourth column indicates the **type of coupling**,\n    - the fifth column (fc) is the **Fermi Contact contribution**,\n    - the sixth column (sd) is the **Spin-dipolar contribution**,\n    - the seventh column (pso) is the **Paramagnetic spin-orbit contribution** and\n    - the eighth column (dso) is the **Diamagnetic spin-orbit contribution**.","7ee702af":"When we look at the target `scalar_coupling_constant` in relation to the `atom_count` - there visually appears to be a relationship. We notice the gap in coupling constant values, between ~25 and ~75. It is rare to see a value within this range. Could this be a good case for a classification problem between the two clusters?","73ab2dca":"# The Data\n\nIn this competition, you will be predicting the scalar_coupling_constant between atom pairs in molecules, given the two atom types (e.g., C and H), the coupling type (e.g., 2JHC), and any features you are able to create from the molecule structure (xyz) files.\n\nFor this competition, you will not be predicting all the atom pairs in each molecule rather, you will only need to predict the pairs that are explicitly listed in the train and test files. For example, some molecules contain Fluorine (F), but you will not be predicting the scalar coupling constant for any pair that includes F.\n\nThe training and test splits are by molecule, so that no molecule in the training data is found in the test data.","64fddfd3":"## Save LGB Results, OOF, and Feature Importance\nIt's always a good idea to save your OOF, predictions and feature importances. You never know when they will come in handy in the future.\nI like to save the Number of folds and CV score in the filename.","400ef56d":"These plots are beautiful. It's a shame we don't have this data for the test set.","9b5f17d4":"# Catboost","669d7153":"## structures.zip annd structures csv files.\nfolder containing molecular structure (xyz) files, where: \n- the first line is the number of atoms in the molecule,\n- followed by a blank line\n- and then a line for every atom, where the first column contains the atomic element (H for hydrogen, C for carbon etc.) and the remaining columns contain the X, Y and Z cartesian coordinates (a standard format for chemists and molecular visualization programs)\n\n\n...lets have a look at the first example from the training set!","0da8b1a2":"# Evaluation Metric\n\nSubmissions are evaluated on the Log of the Mean Absolute Error, calculated for each scalar coupling type, and then averaged across types, so that a 1% decrease in MAE for one type provides the same improvement in score as a 1% decrease for another type.\n\n![Eval Metric](https:\/\/i.imgur.com\/AK6z3Dn.png)\n\nWhere:\n\n- `T` is the number of scalar coupling types\n- `nt` is the number of observations of type t\n- `yi` is the actual scalar coupling constant for the observation\n- `yi^` is the predicted scalar coupling constant for the observation\n\nFor this metric, the MAE for any group has a floor of 1e-9, so that the minimum (best) possible score for perfect predictions is approximately -20.7232.","6399c10e":"# Feature Creation\nThis feature was found from `@inversion` 's kernel here: https:\/\/www.kaggle.com\/inversion\/atomic-distance-benchmark\/output\nThe code was then made faster by `@seriousran` here: https:\/\/www.kaggle.com\/seriousran\/just-speed-up-calculate-distance-from-benchmark","2cb88c0d":"## Relationship between Target and Features\n** Keep in mind these features are provided for the training data ONLY**","131b8802":"Using the this feature from `@artgor`'s amazing kernel - https:\/\/www.kaggle.com\/artgor\/molecular-properties-eda-and-models","9e9b1a65":"# LightGBM - 5 Fold Cross Valified"}}