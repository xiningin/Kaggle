{"cell_type":{"6930a78a":"code","46088632":"code","d312ea48":"code","d6872c03":"code","93c39744":"code","26cc8f3d":"code","73eb51fa":"code","fe7633a8":"code","99fe41c9":"code","b7c52705":"code","4e8afd49":"code","6852a567":"code","655e915b":"code","ee48f163":"markdown","12d02a52":"markdown","befa5416":"markdown"},"source":{"6930a78a":"%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nimport matplotlib.pyplot as plt\n\nimport torch\nfrom torch import nn\nfrom torch import optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms, models\nimport numpy as np","46088632":"data_dir = '..\/input\/flower_data\/flower_data'\n\n# TODO: Define transforms for the training data and testing data\ntrain_transforms = transforms.Compose([transforms.RandomRotation(30),\n                                       transforms.RandomResizedCrop(224),\n                                       transforms.RandomHorizontalFlip(),\n                                       transforms.ToTensor(),\n                                       transforms.Normalize([0.485, 0.456, 0.406],\n                                                            [0.229, 0.224, 0.225])])\n\ntest_transforms = transforms.Compose([transforms.Resize(255),\n                                      transforms.CenterCrop(224),\n                                      transforms.ToTensor(),\n                                      transforms.Normalize([0.485, 0.456, 0.406],\n                                                           [0.229, 0.224, 0.225])])\n\n# Pass transforms in here, then run the next cell to see how the transforms look\ntrain_data = datasets.ImageFolder(data_dir + '\/train', transform=train_transforms)\ntest_data = datasets.ImageFolder(data_dir + '\/valid', transform=test_transforms)\n\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=64)","d312ea48":"model = models.densenet121(pretrained=True)\nmodel","d6872c03":"# Freeze parameters so we don't backprop through them\nfor param in model.parameters():\n    param.requires_grad = False\n\nfrom collections import OrderedDict\nclassifier = nn.Sequential(OrderedDict([\n    ('fc1', nn.Linear(1024, 512)),\n    ('relu', nn.ReLU()),\n    ('dropout', nn.Dropout(0.4)),\n    ('fc2', nn.Linear(512, 102)),\n    ('output', nn.LogSoftmax(dim=1))\n]))\n    \nmodel.classifier = classifier\nmodel.classifier","93c39744":"criterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.001)","26cc8f3d":"# check if CUDA is available\ntrain_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available.  Training on CPU ...')\nelse:\n    print('CUDA is available!  Training on GPU ...')","73eb51fa":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\nepochs = 50\n\nvalid_loss_min = np.Inf\n\n# Some lists to keep track of loss and accuracy during each epoch\nepoch_list = []\ntrain_loss_list = []\nval_loss_list = []\ntrain_acc_list = []\nval_acc_list = []\n# Start epochs\nfor epoch in range(epochs):\n    \n    #adjust_learning_rate(optimizer, epoch)\n    \n    # monitor training loss\n    train_loss = 0.0\n    val_loss = 0.0\n    \n    ###################\n    # train the model #\n    ###################\n    # Set the training mode ON -> Activate Dropout Layers\n    model.train() # prepare model for training\n    # Calculate Accuracy         \n    correct = 0\n    total = 0\n    \n    # Load Train Images with Labels(Targets)\n    for data, target in train_loader:\n        \n        if train_on_gpu:\n            data, target = data.to(device), target.to(device)\n        \n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # forward pass: compute predicted outputs by passing inputs to the model\n        output = model(data)\n        \n        if type(output) == tuple:\n            output, _ = output\n        \n        # Calculate Training Accuracy \n        predicted = torch.max(output.data, 1)[1]        \n        # Total number of labels\n        total += len(target)\n        # Total correct predictions\n        correct += (predicted == target).sum()\n        \n        # calculate the loss\n        loss = criterion(output, target)\n        # backward pass: compute gradient of the loss with respect to model parameters\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss += loss.item()*data.size(0)\n    \n    # calculate average training loss over an epoch\n    train_loss = train_loss\/len(train_loader.dataset)\n    \n    # Avg Accuracy\n    accuracy = 100 * correct \/ float(total)\n    \n    # Put them in their list\n    train_acc_list.append(accuracy)\n    train_loss_list.append(train_loss)\n    \n        \n    # Implement Validation like K-fold Cross-validation \n    \n    # Set Evaluation Mode ON -> Turn Off Dropout\n    model.eval() # Required for Evaluation\/Test\n\n    # Calculate Test\/Validation Accuracy         \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n\n\n            if train_on_gpu:\n                data, target = data.cuda(), target.cuda()\n\n            # Predict Output\n            output = model(data)\n            if type(output) == tuple:\n                output, _ = output\n\n            # Calculate Loss\n            loss = criterion(output, target)\n            val_loss += loss.item()*data.size(0)\n            # Get predictions from the maximum value\n            predicted = torch.max(output.data, 1)[1]\n\n            # Total number of labels\n            total += len(target)\n\n            # Total correct predictions\n            correct += (predicted == target).sum()\n    \n    # calculate average training loss and accuracy over an epoch\n    val_loss = val_loss\/len(test_loader.dataset)\n    accuracy = 100 * correct\/ float(total)\n    \n    # Put them in their list\n    val_acc_list.append(accuracy)\n    val_loss_list.append(val_loss)\n    \n    # Print the Epoch and Training Loss Details with Validation Accuracy   \n    print('Epoch: {} \\tTraining Loss: {:.4f}\\t Val. acc: {:.2f}%'.format(\n        epoch+1, \n        train_loss,\n        accuracy\n        ))\n    # save model if validation loss has decreased\n    if val_loss <= valid_loss_min:\n        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n        valid_loss_min,\n        val_loss))\n        # Save Model State on Checkpoint\n        torch.save(model.state_dict(), 'umodel.pt')\n        valid_loss_min = val_loss\n    # Move to next epoch\n    epoch_list.append(epoch + 1)","fe7633a8":"model.load_state_dict(torch.load('umodel.pt'))","99fe41c9":"# Training \/ Validation Loss\nplt.plot(epoch_list,train_loss_list)\nplt.plot(val_loss_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.title(\"Training\/Validation Loss vs Number of Epochs\")\nplt.legend(['Train', 'Valid'], loc='upper right')\nplt.show()","b7c52705":"# Train\/Valid Accuracy\nplt.plot(epoch_list,train_acc_list)\nplt.plot(val_acc_list)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Training\/Validation Accuracy\")\nplt.title(\"Accuracy vs Number of Epochs\")\nplt.legend(['Train', 'Valid'], loc='best')\nplt.show()","4e8afd49":"val_acc = sum(val_acc_list[:]).item()\/len(val_acc_list)\nprint(\"Validation Accuracy of model = {} %\".format(val_acc))","6852a567":"!git clone https:\/\/github.com\/GabrielePicco\/deep-learning-flower-identifier\n!pip install airtable\nimport sys\nsys.path.insert(0, 'deep-learning-flower-identifier')","655e915b":"from test_model_pytorch_facebook_challenge import calc_accuracy\ncalc_accuracy(model, input_image_size=224, use_google_testset=False)","ee48f163":"**By [Droid(kaggle- droid021)](https:\/\/www.linkedin.com\/in\/v3nvince)**","12d02a52":"# Udacity PyTorch Scholarship Final Lab Challenge","befa5416":"## Links Here:  \n**Model State Checkpoint File: [umodel.pt](.\/umodel.pt)**   (Preferred)  "}}