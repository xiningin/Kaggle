{"cell_type":{"f6a6c57e":"code","cafc78d4":"code","8520c43d":"code","9d958afe":"code","b97d93bd":"code","b8ce8ebb":"code","3198d44b":"code","76c33b0f":"code","0c825c43":"code","e00360f3":"code","aafabfdf":"code","9873241f":"code","16805b2b":"code","838098ac":"code","433c6e96":"code","722908e6":"code","efb8bce8":"code","40ef2a6d":"code","0dff10b6":"code","f73abee8":"code","5fffb6f7":"code","e9c13b24":"code","9cc37f0e":"code","af675dfe":"code","30618116":"code","a9b42f2a":"code","12314e8f":"code","56394296":"code","48ef873d":"code","ea6a9f36":"code","5a3ea919":"code","7eecdcbc":"code","053b4a69":"code","87d65191":"code","70e3ae54":"code","4f6f1ad5":"code","1e9d2207":"code","e7cde68f":"code","f5295ef6":"code","9485029e":"code","eb8cc9fd":"code","6c83e4b4":"code","fce2201d":"code","b0078d47":"code","d8cb1a28":"code","c06bcedd":"code","946f1c02":"markdown","6b579ad0":"markdown","5146f1ef":"markdown","9c41f84c":"markdown","42bc9afe":"markdown","2f7eb3ca":"markdown","ca3c7e6c":"markdown","7aa926f9":"markdown","b1920e1c":"markdown","70b72ddc":"markdown","f0c19197":"markdown","8c4f15b1":"markdown","f8b9e09f":"markdown","d31bab28":"markdown","57c14b26":"markdown","8aa1e613":"markdown","be8a155a":"markdown","f439292f":"markdown","272a5b87":"markdown","e1e325eb":"markdown","f0ece2fe":"markdown","bf239613":"markdown","9d55dd68":"markdown","531a5f21":"markdown","121c81d4":"markdown","e2992262":"markdown","32e043ac":"markdown","6a1d1fcc":"markdown","6eba53ba":"markdown","2285cd00":"markdown","ceb562ab":"markdown","0413267f":"markdown","fab4e42a":"markdown","773b2f21":"markdown","b1100f49":"markdown","6427c136":"markdown","02c0f00f":"markdown"},"source":{"f6a6c57e":"# importing necessary libraries:\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\nimport os\nprint(os.listdir(\"..\/input\"))","cafc78d4":"from sklearn.metrics import accuracy_score,recall_score, precision_score\nfrom sklearn.model_selection import GridSearchCV","8520c43d":"# importing dataset\ndataset = pd.read_csv('..\/input\/WA_Fn-UseC_-Telco-Customer-Churn.csv', na_values = [' ','','#NA','NA','NULL','NaN', 'nan', 'n\/a'], \n                      dtype = {'TotalCharges':np.float32, 'MonthlyCharges': np.float32} )","9d958afe":"print(dataset.shape)\ndataset.head(3)","b97d93bd":"dataset.describe()","b8ce8ebb":"# Dropping column not having any significance in predicting the customer decision so we will drop it\ndataset.drop(columns = ['customerID', 'PaperlessBilling', 'PaymentMethod']\n             , axis = 1, inplace = True)","3198d44b":"#checking if any column in the data contain the na_values\ndataset.isna().any()","76c33b0f":"total_rows_with_na_values = sum((dataset['TotalCharges'].isna())*1)\nprint(total_rows_with_na_values\/dataset.shape[0])","0c825c43":"dataset['TotalCharges'].fillna(dataset['TotalCharges'].mean(), inplace = True)\nprint(\"Is there any na value left in dataset:\",dataset['TotalCharges'].isna().any())","e00360f3":"X = dataset.iloc[:,:-1].values\ny = (dataset.iloc[:,-1].values == 'Yes')*1","aafabfdf":"# encoding of labels of dataset\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\n\nfor i in range(X.shape[1]):\n    # for encoding of all columns having unique values lower than 5\n    if len(np.unique(X[:,i])) < 5:\n        X[:,i] = label_encoder.fit_transform(X[:,i])","9873241f":"# Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(X)","16805b2b":"from statsmodels.stats.outliers_influence import variance_inflation_factor    \n\ndef calculate_vif_(X, thresh=5.0):\n    variables = list(range(X.shape[1]))\n    dropped = True\n    while dropped:\n        dropped = False\n        vif = [variance_inflation_factor(X[:, variables], i) for i in range(X[:, variables].shape[1])]\n        maxloc = vif.index(max(vif))\n        print(max(vif))\n        \n        if max(vif) > thresh:\n            del variables[maxloc]\n            dropped = True\n\n    print('Remaining variables:')\n    print(variables)\n    return X[:, variables]","838098ac":"X = calculate_vif_(X, 5)","433c6e96":"import statsmodels.regression.linear_model as sm","722908e6":"def backwardElimination(x,y, SL):\n    numVars = len(x[0])\n    temp = np.zeros((x.shape[0],numVars)).astype(int)\n    \n    for i in range(0, numVars):\n        regressor_OLS = sm.OLS(y, x).fit()\n        \n        maxVar = max(regressor_OLS.pvalues).astype(float)\n        \n        adjR_before = regressor_OLS.rsquared_adj.astype(float)\n        \n        if maxVar > SL:\n            \n            for j in range(0, numVars - i):\n                \n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n                    temp[:,j] = x[:, j]\n                    x = np.delete(x, j, 1)\n                    \n                    tmp_regressor = sm.OLS(y, x).fit()\n                    adjR_after = tmp_regressor.rsquared_adj.astype(float)\n                    \n                    if (adjR_before >= adjR_after):\n                        x_rollback = np.hstack((x, temp[:,[0,j]]))\n                        x_rollback = np.delete(x_rollback, j, 1)\n                        print (regressor_OLS.summary())\n                        return x_rollback\n                    else:\n                        continue\n    regressor_OLS.summary()\n    return x\n \n","efb8bce8":"X = np.append(arr = np.ones((X.shape[0], 1)).astype(int), values = X, axis = 1)","40ef2a6d":"SL = 0.05\nX_opt = X[:, [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16]]\nX_Modeled = backwardElimination(X_opt,y, SL)","0dff10b6":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_Modeled, y, test_size = 0.4, random_state = 32)","f73abee8":"import seaborn as sns","5fffb6f7":"# choosing only 1000 tuple from each class for better visualization\nchurn1 = dataset[dataset['Churn'] == 'Yes'][:1000]\nchurn0 = dataset[dataset['Churn'] == 'No'][:1000]","e9c13b24":"plt.figure(figsize = (10,5))\nsns.scatterplot('TotalCharges', 'MonthlyCharges',data=churn1)\nsns.scatterplot('TotalCharges', 'MonthlyCharges',data=churn0)\nplt.show()","9cc37f0e":"sns.scatterplot('TotalCharges', 'tenure',data=churn1)\nsns.scatterplot('TotalCharges', 'tenure',data=churn0)\nplt.show()","af675dfe":"sns.scatterplot('MonthlyCharges', 'tenure',data=churn1)\nsns.scatterplot('MonthlyCharges', 'tenure',data=churn0)\nplt.show()","30618116":"from mpl_toolkits.mplot3d import Axes3D","a9b42f2a":"from numpy import linalg as LA\n\nprint(X_test.shape)\nsigma = (1\/X_test.shape[0])*np.matmul(X_test.T, X_test)\nu,s,v = np.linalg.svd(sigma)","12314e8f":"# Reducing the X_test from 13 dimension to 2 dimension\nz2 = np.matmul(X_test, u[:,:2])\n\nchurn1 = z2[y_test == 1]\nchurn0 = z2[y_test == 0]\n\nplt.figure(figsize = (8,6))\nsns.scatterplot(churn1[:,0], churn1[:,1], color = 'blue', s = 20)\nsns.scatterplot(churn0[:,0],churn0[:,1], color = 'red', s = 20)\nplt.show()","56394296":"# Reducing the X_test from 13 dimension to 2 dimension\nz = np.matmul(X_test, u[:,:3])\n\nchurn1 = z[y_test == 1]\nchurn0 = z[y_test == 0]\n\n\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d', navigate =True)\nax.scatter(churn0[:,0], churn0[:,1], churn0[:,2], c='blue', s=10)\nax.scatter(churn1[:,0], churn1[:,1], churn1[:,2],  c='red', s=10)\nax.view_init(69, 175)\nplt.show()","48ef873d":"def model_evaluation(classifier, X, y):\n    y_pred = classifier.predict(X)\n    print(\"accuracy score:\",accuracy_score(y, y_pred))\n    print(\"precision score:\",precision_score(y, y_pred))\n    print(\"recall score\",recall_score(y, y_pred))","ea6a9f36":"from sklearn.naive_bayes import GaussianNB\nnaiveBayesClassifier = GaussianNB()\nnaiveBayesClassifier.fit(X_train,y_train)","5a3ea919":"print(\"***Training***\")\nmodel_evaluation(naiveBayesClassifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(naiveBayesClassifier,X_test,y_test)","7eecdcbc":"from sklearn.linear_model import LogisticRegression\nlogistic_classifier = LogisticRegression(random_state = 32, solver = 'lbfgs')\nlogistic_classifier.fit(X_train, y_train)","053b4a69":"print(\"***Training***\")\nmodel_evaluation(logistic_classifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(logistic_classifier,X_test,y_test)","87d65191":"from sklearn.neighbors import KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors = 9, p = 1, weights = 'uniform', leaf_size = 15, algorithm = 'ball_tree')\nknn_classifier.fit(X_train, y_train)","70e3ae54":"# it will take some time but will return the best perimeter to run on the classifier\n# then again run the classifier with the new parameters and fit on training set\nparameters = [{'n_neighbors': [5,7,9], 'weights':['uniform','distance'],\n              'algorithm':['ball_tree', 'kd_tree', 'brute'],\n              'leaf_size':[15,30, 45], 'p':[1,2], }]\n\n\ngrid_search = GridSearchCV(estimator = knn_classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\n\ngrid_search = grid_search.fit(X_train, y_train)\n\nbest_parameters = grid_search.best_params_\nprint(\"best score on the parameter on 10 folds is :\",grid_search.best_score_)\nprint(best_parameters)\n# We are taking these best parameters and updating the parameters in the classifier above and fit on training set again","4f6f1ad5":"print(\"***Training***\")\nmodel_evaluation(knn_classifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(knn_classifier,X_test,y_test)","1e9d2207":"from sklearn.svm import SVC\nSvm_classifier = SVC(kernel = 'linear', random_state = 32, C = 1)\nSvm_classifier.fit(X_train, y_train)","e7cde68f":"# This block will take some time to find best perimeter of the classifier\nparameters = [{'C': [1, 10, 100, 1000], 'kernel': ['linear']},\n              {'C': [1, 10, 100, 1000], 'kernel': ['rbf'], 'gamma': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]}]\ngrid_search = GridSearchCV(estimator = Svm_classifier,\n                           param_grid = parameters,\n                           scoring = 'accuracy',\n                           cv = 10,\n                           n_jobs = -1)\ngrid_search = grid_search.fit(X_train, y_train)\nbest_parameters = grid_search.best_params_\n\nprint(\"best score on the parameter on 10 folds is :\",grid_search.best_score_)\nprint(best_parameters)\n# We are taking these best parameters and updating the parameters in the classifier above and fit on training set again","f5295ef6":"print(\"***Training***\")\nmodel_evaluation(Svm_classifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(Svm_classifier,X_test,y_test)","9485029e":"from sklearn.tree import DecisionTreeClassifier\ndt_classifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\ndt_classifier.fit(X_train, y_train)","eb8cc9fd":"print(\"***Training***\")\nmodel_evaluation(dt_classifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(dt_classifier,X_test,y_test)","6c83e4b4":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nrdt_classifier = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\nrdt_classifier.fit(X_train, y_train)\n","fce2201d":"print(\"***Training***\")\nmodel_evaluation(rdt_classifier, X_train,y_train)\nprint(\"***Testing***\")\nmodel_evaluation(rdt_classifier,X_test,y_test)","b0078d47":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense","d8cb1a28":"# Initialising the ANN\nclassifier = Sequential()\n\nclassifier.add(Dense(output_dim = 64, init = 'glorot_uniform', activation = 'relu', input_dim = 13))\n\nclassifier.add(Dense(output_dim = 128, init = 'glorot_uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 64, init = 'glorot_uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 32, init = 'glorot_uniform', activation = 'relu'))\n\nclassifier.add(Dense(output_dim = 1, init = 'glorot_uniform', activation = 'sigmoid'))\n\nclassifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n# Fitting the ANN to the Training set\nclassifier.fit(X_train, y_train, batch_size = 32, epochs = 100)","c06bcedd":"print(\"***Training***\")\ny_train_pred = classifier.predict_classes(X_train)\nprint(\"accuracy: \", accuracy_score(y_train,y_train_pred))\nprint(\"precision: \", precision_score(y_train,y_train_pred))\nprint(\"recall: \", recall_score(y_train,y_train_pred))\n\nprint(\"***Testing***\")\ny_test_pred = classifier.predict_classes(X_test)\nprint(\"accuracy: \", accuracy_score(y_test,y_test_pred))\nprint(\"precision: \", precision_score(y_test,y_test_pred))\nprint(\"recall: \", recall_score(y_test,y_test_pred))","946f1c02":"tenure and TotalCharges can also not be used for classification task.","6b579ad0":"After 100 epochs on the training set the model is showing an accuracy of approx. 88.8% on training set and approx 75% on test set. Neural networks requires a large amount but here is scarcity of it. But, even then it has out performed all the previous conventional models.","5146f1ef":"## KNN","9c41f84c":"The adjusted R-squared is a modified version of R-squared that has been adjusted for the number of predictors in the model. The adjusted R-squared increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance.<br>\n The p-value is a number between 0 and 1 and interpreted in the following way: A small p-value (typically \u2264 0.05) indicates strong evidence against the null hypothesis, so you reject the null hypothesis.<br>\nA null hypothesis is a type of hypothesis used in statistics that proposes that no statistical significance exists in a set of given observations. The null hypothesis attempts to show that no variation exists between variables or that a single variable is no different than its mean.","42bc9afe":"From above operation it can be observed that TotalCharges contains **na_values**..","2f7eb3ca":"## Random Forest Classification","ca3c7e6c":"The variance inflation factor is a measure for the increase of the\nvariance of the parameter estimates if an additional variable, given by\nexog_idx is added to the linear regression. It is a measure for\nmulticollinearity of the design matrix, exog.","7aa926f9":"## Now Splitting Dataset into training and test set","b1920e1c":"## Dividing the data in X and y","70b72ddc":"## Decision Tree Classification","f0c19197":"## Artificial Neural Network","8c4f15b1":"# Implementing Machine Learning Models","f8b9e09f":"# Dimensionality reduction to visualize the data","d31bab28":"## Total Charges VS Monthly Charges","57c14b26":"Here, 0.15 % of total data contains **na** values so removing it will not have any significant affect on value of our data.<br>\nBut, here we are going to keep it in the data by *replacing it with the mean of the column*.","8aa1e613":"## Dealing multi-collinearity","be8a155a":"## Logistic Regression","f439292f":"We could use model with higher recall or may be with higher precision according to the need and situation.","272a5b87":"# Conclusion","e1e325eb":"# Importing the data","f0ece2fe":"From above graph it can be observed that we couldn't classify our data on the basis of Monthly Charges and Total Charges.","bf239613":"The precision is the ratio tp \/ (tp + fp) where tp is the number of true positives and fp the number of false positives. The precision is intuitively the ability of the classifier not to label as positive a sample that is negative. The best value is 1 and the worst value is 0.<br><br>\nThe recall is the ratio tp \/ (tp + fn) where tp is the number of true positives and fn the number of false negatives. The recall is intuitively the ability of the classifier to find all the positive samples. The best value is 1 and the worst value is 0.<br>","9d55dd68":"Artificial Neural Network out performed all the other classification technique. With large amount of data, bigger and deeper network the problem of vanishing and exploding gradient can be solved by using LSTM (i.e. Long Short Term Memory) we could have a model which could achieve state of art performance.<br><br>\n \n**Neural Network accuracy:**<br>\nOn the training set : 0.8883564075257366<br>\n    On the test set : 0.7473385379701917<br>","531a5f21":"## Tenure VS Total Charges","121c81d4":"You could try grid search method on decision tree and random forest also but since it is taking 5-10 minutes approx. so we are not using it later.","e2992262":"## Encoding the values and feature scaling","32e043ac":"# Understanding data and manipulation","6a1d1fcc":"## Dealing with na values##","6eba53ba":"Here we can easily observe cluster of red dots i.e. Churn 'Yes' and blue dot i.e. Churn 'No'.","2285cd00":"Tenure and Monthly can also not be used for classification.","ceb562ab":"Here it is not quite clear but we can say that blue is more dense at the bottom and red dots are more dense at the top.","0413267f":"# Visualization","fab4e42a":"Decision Tree and Random Forest Classifier both having huge difference between their train set accuracy and test set accuracy, so their is surely overfitting. That's why we will not consider any further. Till now logistic Regression and \nSVM classifier is out performing others considering accuracy, precision and recall score combined instead Naive bayes has the best recall score.","773b2f21":"## Building optimal model using backward elimination","b1100f49":"## Naive Bayes Classifier","6427c136":"## Tenure VS Monthly Charges","02c0f00f":"## SVM"}}