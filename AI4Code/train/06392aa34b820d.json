{"cell_type":{"ce86ddab":"code","491f418e":"code","0ba4a17e":"code","0ae2b040":"code","8e53cf2e":"code","9b0d23d8":"code","343c1285":"code","fd3abaa2":"code","0f86c298":"code","0f77badc":"code","b6e182bb":"code","61aff1c0":"code","c856972b":"code","6d16d047":"code","b913f4e3":"code","c3037f22":"code","b5815b36":"code","00f61873":"code","18a81f9c":"code","075a857d":"code","31557c81":"code","ef2ddc40":"code","f408fd57":"code","10e45fa2":"code","077eb77d":"code","68ffad68":"code","2ad46647":"code","d8226c17":"code","54e14d99":"code","73ad56e7":"markdown","aaa19e17":"markdown","ff46f168":"markdown","13ba9d6a":"markdown","fb8b89af":"markdown","7cdb8a3e":"markdown","002d69d1":"markdown","53573c13":"markdown","e9f75790":"markdown","3923d351":"markdown","b40f430c":"markdown","1adca6b9":"markdown","33a6b5e0":"markdown","a5b9c4dc":"markdown","6fb5265a":"markdown","97b7bc17":"markdown"},"source":{"ce86ddab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","491f418e":"class Brain():\n    def __init__(self, n_neurons=100, n_inputs=10, n_outputs=10, n_connections=10,\n                 low=-1, resting=0, threshold=1, peak=5, decay=0.5, seed=2020):\n        \"\"\" Create a brain with hidden, input, and output neurons\n            Args: \n            - n_neurons [int]: total number of neurons\n            - n_inputs [int]: number of neurons having input\n            - n_outputs [int]: number of neurons having output\n            - n_connections [int]: number of connections per neuron\n            - low: minimum potential of neuron\n            - resting: resting potential of neuron\n            - threshold: threshold potential of neuron\n            - peak: peak fire potential (action potential) of neuron\n            - decay [float]: decay rate of neuron potential\n            - seed [int]: random seed for numpy\n            Return: Brain object\n        \"\"\"\n        assert n_neurons >= n_outputs + n_inputs\n        assert n_neurons >= n_connections\n        self.n_neurons = n_neurons\n        self.n_inputs = n_inputs\n        self.n_outputs = n_outputs\n        self.n_connections = n_connections\n        self.low = low\n        self.resting = resting     \n        self.threshold = threshold\n        self.peak = peak\n        self.decay = decay\n        np.random.seed(seed)\n        self.mask = self.generate_mask()\n        self.initialize_weights()\n        self.initialize_neurons()        \n\n    def generate_mask(self):\n        \"\"\" Determine how the neurons are connected. It will try to generate on average n_connections per neurons.\n            No self-connection are allowed.\n        \"\"\"\n        threshold = (self.n_connections + 1) \/ self.n_neurons\n        return (np.random.rand(self.n_neurons, self.n_neurons) < threshold) & ~np.identity(self.n_neurons, dtype='bool')\n    \n    def initialize_weights(self, seed=None):\n        \"\"\" Initialize the weights of conected neurons to be between -1 and 1. \"\"\"\n        if seed:\n            np.random.seed(seed)\n        self.weights = (2 * np.random.rand(self.n_neurons, self.n_neurons).astype('float32') - 1) * self.mask\n    \n    def initialize_neurons(self):\n        self.neurons = np.ones(self.n_neurons).astype('float32') * self.resting  # potentials of the neurons\n        self.neurons_fire = np.zeros(self.n_neurons, dtype='bool')               # whether the neurons fires or not\n    \n    def forward_pass(self):\n        # compute input potential\n        neurons_next = np.dot(self.weights, self.neurons_fire * self.peak) + self.decay * self.neurons\n        # If the neuron has just fired, then suppress its potential to the lowest potential\n        neurons_next = np.where(self.neurons_fire, self.low, neurons_next)\n        neurons_next = np.clip(neurons_next, a_min=self.low, a_max=None)\n        # If input potential (x) is greater than threshold, then fire at peak potential, otherwise no change.\n        self.neurons = neurons_next\n        self.neurons_fire = self.neurons >= self.threshold\n    \n    def train_hebbian_one_step(self, learning_rate=0.05):\n        \"\"\" Train with Hebbian like method (unsupervised learning)\n            dW_ij = lr * (next_fire_i * M_ij * prev_fire_j - prev_fire_i * M_ij * next_fire_j)\n                  = lr * M_ij * (next_fire_i * prev_fire_j - prev_fire_i * next_fire_j)\n        \"\"\"\n        prev_fire = self.neurons_fire\n        self.forward_pass()\n        next_fire = self.neurons_fire\n        # dW = learning_rate * next_fire.reshape(-1, 1) * self.weights * next_fire\n        dW = learning_rate * self.mask * (\n            next_fire.reshape(-1, 1) * prev_fire - 1 * prev_fire.reshape(-1, 1) * next_fire\n        )\n        self.weights = np.clip(self.weights + dW, a_min=-1, a_max=1)\n    \n    def set_inputs(self, inputs):\n        self.neurons[:self.n_inputs] = inputs \/ self.decay\n    \n    def get_outputs(self):\n        return self.neurons[self.n_inputs : self.n_inputs + self.n_outputs]\n    \n    def predict(self, inputs, steps=10, constant_input=False):\n        self.set_inputs(inputs)\n        for step in range(steps):\n            if constant_input: \n                self.set_inputs(inputs)\n            self.forward_pass()\n        return self.get_outputs()\n    \n    def predict_history(self, inputs, steps=10, constant_input=False):\n        self.set_inputs(inputs)\n        hist = np.zeros((steps, self.n_outputs), dtype='float32')\n        for step in range(steps):\n            if constant_input: \n                self.set_inputs(inputs)\n            self.forward_pass()\n            hist[step] = self.get_outputs()\n        return hist\n\n    def predict_brain_history(self, inputs, steps=10, constant_input=False):\n        self.set_inputs(inputs)\n        hist = np.zeros((steps, self.n_neurons), dtype='float32')\n        for step in range(steps):\n            if constant_input: \n                self.set_inputs(inputs)\n            self.forward_pass()\n            hist[step] = self.neurons\n        return hist\n    \n    def train_hebbian_brain_history(self, inputs, steps=10, lr=0.05, constant_input=False):\n        self.set_inputs(inputs)\n        hist = np.zeros((steps, self.n_neurons), dtype='float32')\n        for step in range(steps):\n            if constant_input: \n                self.set_inputs(inputs)\n            self.train_hebbian_one_step(lr)\n            hist[step] = self.neurons\n        return hist\n    \n    def train_hebbian_brain_batch(self, X, total_steps=1000, steps_per_sample=10, lr=0.05, constant_input=True):\n        \"\"\" Train Hebbian brain with an array of data X \"\"\"\n        n_samples_train = np.ceil(total_steps \/ steps_per_sample).astype('int')\n        total_steps = n_samples_train * steps_per_sample\n        if n_samples_train > len(X):\n            X = np.vstack([X] * np.ceil(n_samples_train \/ len(X)).astype('int'))\n        X = X[:n_samples_train]\n        print('training steps:', n_samples_train, 'x', steps_per_sample)\n        for x in tqdm(X):\n            self.set_inputs(x)\n            for _ in range(steps_per_sample):\n                if constant_input: \n                    self.set_inputs(x)\n                self.train_hebbian_one_step(lr)","0ba4a17e":"# Create our brain\nbrain = Brain(n_neurons=100, n_inputs=10, n_outputs=10, n_connections=10)","0ae2b040":"brain.initialize_neurons()\ndisplay(brain.predict(np.array([5,]+[0]*9), 9, constant_input=False))\n\nbrain.initialize_neurons()\ndisplay(brain.predict(np.array([0]*9+[1]), 9, constant_input=False))\n\nbrain.initialize_neurons()\ndisplay(brain.predict(np.array([0]*9+[1]), 9, constant_input=True))","8e53cf2e":"plt.figure(figsize=(10, 4))\nplt.subplot(121).set_title('weights')\nplt.imshow(brain.weights)\nplt.subplot(122).set_title('mask')\nplt.imshow(brain.mask)\nplt.show()","9b0d23d8":"# History of output neuron fire\nbrain.initialize_neurons()\nplt.imshow(brain.predict_history(np.array([1,]+[0]*9), 30, constant_input=False)); plt.show()","343c1285":"# History of output neuron fire with constant input\nbrain.initialize_neurons()\nplt.imshow(brain.predict_history(np.array([1,]+[0]*9), 30, constant_input=True)); plt.show()","fd3abaa2":"# History of all neurons\nbrain.initialize_neurons()\nplt.imshow(brain.predict_brain_history(np.array([1,]+[0]*9), 90, constant_input=False)); plt.show()","0f86c298":"# History of all neurons with constant input\nbrain.initialize_neurons()\nplt.imshow(brain.predict_brain_history(np.array([1,]+[0]*9), 90, constant_input=True)); plt.show()","0f77badc":"# non-constant inputs vs outputs at various timesteps\nplt.figure(figsize=(12, 6))\nsteps = [9, 19, 39]\nplt_subplot = 101 + 10 * len(steps)\nfor step in steps:\n    pred = np.zeros((10, brain.n_outputs), dtype='float32')\n    for i in range(10):\n        brain.initialize_neurons()\n        pred[i] = brain.predict(np.array([0]*i+[1]+[0]*(9 - i)), step, constant_input=False)\n    display(pd.DataFrame(pred))\n    plt.subplot(plt_subplot).set_title(f'steps = {step}')\n    plt.imshow(pred)\n    plt.ylabel('input neuron location'); plt.xlabel('output neuron fire location'); \n    plt_subplot += 1\nplt.show()","b6e182bb":"# constant inputs vs outputs at various timesteps\nplt.figure(figsize=(12, 6))\nsteps = [9, 19, 39]\nplt_subplot = 101 + 10 * len(steps)\nfor step in steps:\n    pred = np.zeros((10, brain.n_outputs), dtype='float32')\n    for i in range(10):\n        brain.initialize_neurons()\n        pred[i] = brain.predict(np.array([0]*i+[1]+[0]*(9 - i)), step, constant_input=True)\n    display(pd.DataFrame(pred))\n    plt.subplot(plt_subplot).set_title(f'steps = {step}')\n    plt.imshow(pred)\n    plt.ylabel('input neuron location'); plt.xlabel('output neuron fire location'); \n    plt_subplot += 1\nplt.show()","61aff1c0":"# constant inputs vs outputs at various timesteps\nplt.figure(figsize=(16, 8))\nplt_subplot = [2, 5, 1]\nfor i in range(10):\n    brain.initialize_neurons()\n    pred = brain.predict_history(np.array([0]*i+[1]+[0]*(9 - i)), steps=10, constant_input=True)\n    #print(pred)\n    plt.subplot(*plt_subplot).set_title(f'input = {i}')\n    plt.imshow(pred)\n    plt.ylabel('timesteps'); plt.xlabel('output neuron fire location'); \n    plt_subplot[2] += 1\nplt.show()","c856972b":"# non-constant inputs vs outputs at various timesteps\nplt.figure(figsize=(16, 8))\nplt_subplot = [2, 5, 1]\nfor i in range(10):\n    brain.initialize_neurons()\n    pred = brain.predict_history(np.array([0]*i+[1]+[0]*(9 - i)), steps=10, constant_input=False)\n    #print(pred)\n    plt.subplot(*plt_subplot).set_title(f'input = {i}')\n    plt.imshow(pred)\n    plt.ylabel('timesteps'); plt.xlabel('output neuron fire location'); \n    plt_subplot[2] += 1\nplt.show()","6d16d047":"print('Non-constant input')\nplt.figure(figsize=(10, 5))\n# with training\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.subplot(1, 2, 1).set_title('with training')\nplt.imshow(brain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=False))\n\n# without training\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.subplot(1, 2, 2).set_title('without training')\nplt.imshow(brain.predict_brain_history(np.array([1,]+[0]*9), 90, constant_input=False))\nplt.show()","b913f4e3":"print('Constant input')\nplt.figure(figsize=(10, 5))\n# with training\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.subplot(1, 2, 1).set_title('with training')\nplt.imshow(brain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=True))\n\n# without training\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.subplot(1, 2, 2).set_title('without training')\nplt.imshow(brain.predict_brain_history(np.array([1,]+[0]*9), 90, constant_input=True))\nplt.show()","c3037f22":"# Weights\nplt_bins = np.arange(-1, 1.02, 0.02)\nplt.figure(figsize=(12, 4))\nplt.title('Weight distributions after training with constant input')\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.hist(brain.weights.reshape(-1)[brain.mask.reshape(-1)], bins=plt_bins, label='before training', alpha=0.7)\nbrain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=True)\nplt.hist(brain.weights.reshape(-1)[brain.mask.reshape(-1)], bins=plt_bins, label='after training', alpha=0.7)\nplt.xlabel('weight'); plt.ylabel('N weights')\nplt.legend(); plt.grid(); plt.show()","b5815b36":"# Weights\nplt_bins = np.arange(-1, 1.02, 0.02)\nplt.figure(figsize=(12, 4))\nplt.title('Weight distributions after training with non-constant input')\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nplt.hist(brain.weights.reshape(-1)[brain.mask.reshape(-1)], bins=plt_bins, label='before training', alpha=0.7)\nbrain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=False)\nplt.hist(brain.weights.reshape(-1)[brain.mask.reshape(-1)], bins=plt_bins, label='after training', alpha=0.7)\nplt.xlabel('weight'); plt.ylabel('N weights')\nplt.legend(); plt.grid(); plt.show()","00f61873":"# Weights change\n# plt.figure(figsize=(12, 4))\nplt.title('Weight change after training with constant input')\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nW_before = brain.weights\nbrain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=True)\ndW = brain.weights - W_before\nplt.imshow(dW)\nplt.xlabel('input'); plt.ylabel('output')\nplt.show()","18a81f9c":"# Weights change\n# plt.figure(figsize=(12, 4))\nplt.title('Weight change after training with non-constant input')\nbrain.initialize_neurons()\nbrain.initialize_weights(2030)\nW_before = brain.weights\nbrain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=False)\ndW = brain.weights - W_before\nplt.imshow(dW)\nplt.xlabel('input'); plt.ylabel('output')\nplt.show()","075a857d":"# weight change for various constant inputs\nplt.figure(figsize=(16, 8))\nplt_subplot = [2, 5, 1]\nfor i in range(10):\n    brain.initialize_neurons()\n    brain.initialize_weights(2030)\n    W_before = brain.weights\n    brain.train_hebbian_brain_history(np.array([1,]+[0]*9), 90, lr=0.05, constant_input=False)\n    dW = brain.weights - W_before\n    plt.subplot(*plt_subplot).set_title(f'input = {i}')\n    plt.imshow(dW)\n    plt.ylabel('output'); plt.xlabel('input'); \n    plt_subplot[2] += 1\nplt.show()","31557c81":"df_train = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\n\ncols_pixels = [c for c in df_train.columns if c != 'label']\n\ny_train = df_train['label']\nx_train = df_train[cols_pixels].values\nx_test = df_test[cols_pixels].values\n\nx_train = x_train \/ 255\nx_test = x_test \/ 255\n\nprint(x_train.shape, x_test.shape, y_train.shape)\nprint(x_train.min(), x_train.max())","ef2ddc40":"# Create our brain\nbrain_mnist = Brain(n_neurons=2000, n_inputs=784, n_outputs=10, n_connections=100)\n\nplt.figure(figsize=(10, 4))\nplt.subplot(121).set_title('weights')\nplt.imshow(brain_mnist.weights)\nplt.subplot(122).set_title('mask')\nplt.imshow(brain_mnist.mask)\nplt.show()","f408fd57":"plt.imshow(brain_mnist.predict_history(x_train[0], constant_input=True)); plt.show()","10e45fa2":"# before training\nplt.figure(figsize=(10, 5))\nbrain_mnist.initialize_neurons()\nbrain_mnist.initialize_weights(2030)\nplt.title('without training')\nplt.imshow(brain_mnist.predict_brain_history(x_train[0], 20, constant_input=True))\nplt.show()\n\nweights_before = brain_mnist.weights.reshape(-1)[brain_mnist.mask.reshape(-1)]","077eb77d":"%%time\n# training\nbrain_mnist.train_hebbian_brain_batch(x_train, 1000)","68ffad68":"# Weights\nplt_bins = np.arange(-1, 1.02, 0.02)\nplt.figure(figsize=(12, 4))\nplt.title('Weight distributions change')\nplt.hist(weights_before, bins=plt_bins, label='before training', alpha=0.7)\nplt.hist(brain_mnist.weights.reshape(-1)[brain_mnist.mask.reshape(-1)], bins=plt_bins, label='after training', alpha=0.7)\nplt.xlabel('weight'); plt.ylabel('N weights')\nplt.legend(); plt.grid(); plt.show()","2ad46647":"plt.imshow(brain_mnist.predict_history(x_train[0], constant_input=True)); plt.show()","d8226c17":"plt.imshow(brain_mnist.predict_history(x_train[333], constant_input=True)); plt.show()","54e14d99":"plt.imshow(brain_mnist.predict_history(x_train[333], constant_input=False)); plt.show()","73ad56e7":"# Train with Hebbian like theory\nhttps:\/\/en.wikipedia.org\/wiki\/Hebbian_theory","aaa19e17":"Neurons seems to stuck at weird pattern...","ff46f168":"The reason why the firing is alternating between timestep is because the neuron can't fire right after it just fire in previous timestep. Also, we observed that the neurons usually fire at odd number of timesteps since the signals are input at t=0 (even) timestep.","13ba9d6a":"# On real image","fb8b89af":"# Brain response to different inputs","7cdb8a3e":"#### non-constant inputs vs outputs at various timesteps","002d69d1":"# Spiking neural network\nThis project trying to build and reseawrch a simplified spiking neural network. The final goal is to use this prototype spiking neural network to do prediction on MNIST.\n\nLouis Yang 2020","53573c13":"### Brain connections","e9f75790":"## Read data","3923d351":"# Reference\n- Bozeman Science - The Action Potential https:\/\/www.youtube.com\/watch?v=HYLyhXRp298\n- Synaptic plasticity https:\/\/www.youtube.com\/watch?v=tfifTUYuAYU\n- Khan Academy - The synapse https:\/\/www.khanacademy.org\/science\/biology\/human-biology\/neuron-nervous-system\/a\/the-synapse\n- Spike-timing dependent plasticity http:\/\/www.scholarpedia.org\/article\/Spike-timing_dependent_plasticity","b40f430c":"TODO: Try simple optimization method","1adca6b9":"### Simple inputs and outputs\nSee what the ouput patterns will be given a simplified one-hot input.","33a6b5e0":"# Spiking neural net","a5b9c4dc":"## Brain predict history","6fb5265a":"Looks like the innputs doesn't really affect the weight changes. ","97b7bc17":"#### constant inputs vs outputs at various timesteps"}}