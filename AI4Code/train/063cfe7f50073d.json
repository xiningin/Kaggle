{"cell_type":{"7cc6233c":"code","f3f5933b":"code","f45333b8":"code","35ced803":"code","4f0dcfb7":"code","f9cf119a":"code","5818d5aa":"code","be148fe0":"code","4a3b09a0":"code","933feee8":"code","da59003d":"code","f38b700f":"code","5bd548ee":"code","ad8808b7":"code","ef64b910":"code","8e755446":"code","cfa86326":"code","07310136":"code","5b4fce58":"code","7722c828":"code","3ba7e7ef":"code","a5602fec":"markdown","f809fd2a":"markdown","320f615b":"markdown","4502896b":"markdown","949ff923":"markdown","ed8dbdca":"markdown","90b319c6":"markdown","ddabcd73":"markdown","062b7df8":"markdown","eb06778e":"markdown"},"source":{"7cc6233c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f3f5933b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_iris\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import metrics\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold","f45333b8":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('fivethirtyeight')  \nimport warnings\nwarnings.filterwarnings('ignore')  #this will ignore the warnings.it wont display warnings in notebook","35ced803":"from sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import recall_score\ndef printreport(exp, pred):\n    print(classification_report(exp, pred))\n    print(\"recall score\")\n    print(recall_score(exp,pred,average = 'macro'))","4f0dcfb7":"gr = pd.read_csv('..\/input\/greedata\/featureO.csv')","f9cf119a":"for column in gr.columns[0:-1]:\n    for spec in gr[\"label\"].unique():\n        selected_spec = gr[gr[\"label\"] == spec]\n        selected_column = selected_spec[column]\n        \n        std = selected_column.std()\n        avg = selected_column.mean()\n        \n        three_sigma_plus = avg + (3 * std)\n        three_sigma_minus =  avg - (3 * std)\n        \n        outliers = selected_column[((selected_spec[column] > three_sigma_plus) | (selected_spec[column] < three_sigma_minus))].index\n        gr.drop(outliers, inplace=True)\n        print(column, spec, outliers)","5818d5aa":"x = gr.iloc[:,0:3].values \ny = gr.label.values\nprint(x)\nprint(y)\n","be148fe0":"from sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.3, random_state=0, stratify=y)","4a3b09a0":"scaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","933feee8":"xtrain = x_train\nytrain = y_train\nxtest = x_test\nytest = y_test","da59003d":"k_range = list(range(1,11))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(xtrain, ytrain)\n    y_pred = knn.predict(xtest)\n    scores.append(metrics.accuracy_score(ytest, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.show()\n","f38b700f":"p1 = pd.DataFrame(scores , k_range)\ns1 = p1.loc[:,0]\ns1_argmax = s1[s1 == s1.max()].index.values","5bd548ee":"k = s1_argmax[0]","ad8808b7":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=k, metric='minkowski')\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_test)\n\nprint('KNN')\ncm = confusion_matrix(y_test,y_pred)\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\nprintreport(y_test, y_pred)\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","ef64b910":"x = gr.iloc[:,0:3].values \ny = gr.label.values\nprint(x)\nprint(y)\n\n\nfrom sklearn.model_selection import train_test_split\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.2, random_state=0, stratify=y)\n\nscaler = StandardScaler()\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)","8e755446":"xtrain = x_train\nytrain = y_train\nxtest = x_test\nytest = y_test","cfa86326":"k_range = list(range(1,11))\nscores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(xtrain, ytrain)\n    y_pred = knn.predict(xtest)\n    scores.append(metrics.accuracy_score(ytest, y_pred))\n    \nplt.plot(k_range, scores)\nplt.xlabel('Value of k for KNN')\nplt.ylabel('Accuracy Score')\nplt.title('Accuracy Scores for Values of k of k-Nearest-Neighbors')\nplt.show()\n","07310136":"p1 = pd.DataFrame(scores , k_range)\n# print(p1)\n# print(p1.loc[:,0].max())\n\ns1 = p1.loc[:,0]\ns1_argmax = s1[s1 == s1.max()].index.values\n\nk = s1_argmax[0]","5b4fce58":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=k, metric='minkowski')\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_test)\n\nprint('KNN')\ncm = confusion_matrix(y_test,y_pred)\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\nprintreport(y_test, y_pred)\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()","7722c828":"kfold = KFold(n_splits=5, shuffle=True)\nresults = cross_val_score(knn, x, y, cv=kfold)\nprint(results)\nprint(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))","3ba7e7ef":"import xgboost as xgb\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom xgboost import plot_importance\n\nxgb_cls = xgb.XGBClassifier(objective=\"multi:softmax\", num_class=5)\nxgb_cls.fit(x_train, y_train)\ny_pred = xgb_cls.predict(x_test)\n\nprint('XGboost')\ns = accuracy_score(y_test, y_pred)\nprint('accury')\nprint(s)\ncm = confusion_matrix(y_test, y_pred)\nprintreport(y_test, y_pred)\n\n\n\n#print(cm)\nimport seaborn as sns\nf, ax = plt.subplots(figsize =(5,5))\nsns.heatmap(cm,annot = True,linewidths=0.5,linecolor=\"red\",fmt = \".0f\",ax=ax)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nplt.show()\n\nplot_importance(xgb_cls)","a5602fec":"## Using the best k","f809fd2a":"# Standard Scaler","320f615b":"# K-Fold Validation","4502896b":"# KNN Classification","949ff923":"# Dropping outlier","ed8dbdca":"# Increasing the amount of the train set and the accuracy will increase","90b319c6":"# VS XGboost Classification","ddabcd73":"# Split Data","062b7df8":"## Finding the best k","eb06778e":"# Quota Fuction"}}