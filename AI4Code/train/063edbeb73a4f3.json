{"cell_type":{"819cec4e":"code","76278ca6":"code","8049a278":"code","c0b08ea3":"code","a21c5f0a":"code","2f1e58da":"code","c2d4e454":"code","92d222df":"code","dccd1d81":"code","b76b2c74":"code","7aac4c94":"code","51236fa8":"code","c81d0812":"code","9b6d1d68":"code","a69b974c":"code","c1f9fe58":"code","7428e849":"code","e46f2180":"code","f47e67fe":"code","bd53b400":"code","b3fedaf9":"code","4ea29ea3":"code","f81711e9":"code","6688a083":"code","089822ab":"code","3d85cf48":"code","a4639129":"code","0b65238c":"code","c9b12f5d":"code","1f5d1b06":"code","5b75f805":"code","7f255a5a":"code","e1319a56":"code","b1eb9edb":"code","3fc4c85a":"code","161677a3":"code","4b390483":"code","d8968dd6":"code","b46c91c4":"code","95307281":"code","2d9d57e3":"code","d3e7e847":"code","bae36860":"code","392a684a":"code","09b44bc3":"code","34983b74":"code","0e6aca32":"code","fe07e965":"code","7742e375":"code","ec079907":"code","7a6136d4":"code","de3594a9":"code","f1d3fce0":"code","76eca71e":"code","797397ff":"code","b8388680":"code","bdf7edd6":"code","b3e624a4":"markdown","03f25831":"markdown","0014076e":"markdown","9bed4255":"markdown","881f24ed":"markdown","1e167018":"markdown","85d7789b":"markdown","c32605f3":"markdown","ad4aba4a":"markdown","5b110f31":"markdown","204420cf":"markdown","301abbdb":"markdown","b2dd8471":"markdown","9d29cc41":"markdown","1fded2a0":"markdown","06cba2ff":"markdown","42efa6bb":"markdown","18ed035e":"markdown","cafe9d5b":"markdown","5b81aa19":"markdown","f467f21b":"markdown"},"source":{"819cec4e":"import os\nimport numpy as np \nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport spacy\n\nimport dask.dataframe as dd\nimport dask.array as da","76278ca6":"train_df = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")\ndf = dd.from_pandas(train_df, npartitions=5)\ndf.head()","8049a278":"df.visualize()","c0b08ea3":"plt.figure(figsize = (10,7))\nsns.countplot(df.iloc[:,5].compute())\nplt.title(\"Barplot of is_duplicate\")","a21c5f0a":"print(\"Total number of qustion pairs : \", len(df))\nprint(\"% of question pairs which are similar : \", (len(df[df[\"is_duplicate\"]==1]) \/ len(df))*100)\nprint(\"% of question pairs which are not similar : \", (len(df[df[\"is_duplicate\"]==0]) \/ len(df))*100)","2f1e58da":"distinct_qus = len(set(df['qid1'].compute().tolist() + df['qid2'].compute().tolist()))\nprint(\"Total number of distinct questions : \", distinct_qus)\n\nappended_series = df['qid1'].append(train_df['qid2']).compute()\nqus_freq_more_than_one = sum(appended_series.value_counts()>1)\nprint(\"Repeated questions(Number of questions having frequency more than one time) : \", qus_freq_more_than_one)\nprint(\"Highest repeat frequency : \", max(appended_series.value_counts()))","c2d4e454":"plt.figure(figsize = (10,7))\nsns.barplot([\"Distinct\", \"Repeated\"], [distinct_qus, qus_freq_more_than_one])\nplt.title(\"Barplot indicating distinct and repeated questions\")","92d222df":"print(\"Number of duplicate question pairs : \", df[['qid1','qid2']].compute().duplicated().sum())","dccd1d81":"plt.figure(figsize = (20,12))\nsns.distplot(appended_series.value_counts(),bins = 200, kde = False, color = \"blue\")\nplt.yscale('log', nonposy='clip')","b76b2c74":"df.isna().compute().sum()","7aac4c94":"df = df.fillna('')","51236fa8":"df['freq_qid1'] = df.groupby('qid1')['qid1'].transform('count').compute()\ndf['freq_qid2'] = df.groupby('qid2')['qid2'].transform('count').compute() ","c81d0812":"df['q1len'] = df['question1'].str.len().compute()\ndf['q2len'] = df['question2'].str.len().compute()","9b6d1d68":"df['q1_n_words'] = df.apply(lambda row: len(row.question1.split(\" \")),axis=1).compute()\ndf['q2_n_words'] = df.apply(lambda row: len(row.question2.split(\" \")),axis=1).compute()","a69b974c":"def stripped_common_words(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)\ndf['word_Common'] = df.apply(stripped_common_words, axis=1).compute()","c1f9fe58":"def stripped_word_total(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * (len(set1) + len(set2))\ndf['word_Total'] = df.apply(stripped_word_total, axis=1).compute()","7428e849":"def stripped_word_share(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)\/(len(set1) + len(set2))\ndf['word_share'] = df.apply(stripped_word_share, axis=1).compute()","e46f2180":"df['freq_q1+q2'] = df['freq_qid1']+df['freq_qid2'].compute()\ndf['freq_q1-q2'] = abs(df['freq_qid1']-df['freq_qid2']).compute()","f47e67fe":"df.head()","bd53b400":"print (\"Minimum number of words in question1 : \" , df['q1_n_words'].min().compute())\nprint (\"Minimum number of words in question2 : \" , df['q2_n_words'].min().compute())\nprint (\"Number of Questions with minimum words [question1] :\", len(df[df['q1_n_words']== 1]))\nprint (\"Number of Questions with minimum words [question2] :\", len(df[df['q2_n_words']== 1]))","b3fedaf9":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_share across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['word_share'].compute(),ax=ax1)\nax2.set_title(\"Distribution of word_share across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_share'].compute() , label = \"1\", ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_share'].compute() , label = \"0\" , ax=ax2)\nplt.show()","4ea29ea3":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_Common across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['word_Common'].compute(),ax=ax1)\nax2.set_title(\"Distribution of word_Common across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['word_Common'].compute() , label = \"1\", color = 'red',ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['word_Common'].compute() , label = \"0\" , color = 'blue' ,ax=ax2)","f81711e9":"!pip install distance","6688a083":"import distance","089822ab":"stop_words = stopwords.words(\"english\")\ndef text_preprocess(txt):\n    txt = str(txt).lower()\n    txt = txt.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n          .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n          .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n          .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n          .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n        .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n        .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    txt = re.sub(r\"([0-9]+)000000\", r\"\\1m\", txt)\n    txt = re.sub(r\"([0-9]+)000\", r\"\\1k\", txt)\n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    if type(txt) == type(''):\n        txt = re.sub(pattern, ' ', txt)\n    if type(txt) == type(''):\n        txt = porter.stem(txt)\n        example1 = BeautifulSoup(txt)\n        txt = example1.get_text()\n               \n    return txt","3d85cf48":"safe_div = 0.0001\ndef fetch_token_features(q1, q2):\n    token_features = [0.0]*10\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    q1_words = set([word for word in q1_tokens if word not in stop_words])\n    q2_words = set([word for word in q2_tokens if word not in stop_words])\n    q1_stops = set([word for word in q1_tokens if word in stop_words])\n    q2_stops = set([word for word in q2_tokens if word in stop_words])\n    common_word_count = len(q1_words & q2_words)\n    common_stop_count = len(q1_stops & q2_stops)\n    common_token_count = len(set(q1_tokens) & set(q2_tokens))\n    token_features[0] = common_word_count \/ (min(len(q1_words), len(q2_words)) + safe_div)\n    token_features[1] = common_word_count \/ (max(len(q1_words), len(q2_words)) + safe_div)\n    token_features[2] = common_stop_count \/ (min(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[3] = common_stop_count \/ (max(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[4] = common_token_count \/ (min(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[5] = common_token_count \/ (max(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[6]= int(q1_tokens[-1] == q2_tokens[-1])\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))\/2\n    return token_features","a4639129":"# fetch the Longest Common sub string\ndef fetch_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) \/ (min(len(a), len(b)) + 1)","0b65238c":"def extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].apply(text_preprocess).compute()\n    df[\"question2\"] = df[\"question2\"].apply(text_preprocess).compute()\n    token_features = df.apply(lambda row: fetch_token_features(row.question1, row.question2), axis=1).compute()\n    cwc_min = dd.from_array(np.array(list(map(lambda i: i[0], token_features)))).compute()\n    cwc_min.name = \"cwc_min\"\n    df.merge(cwc_min.to_frame())\n    cwc_max = dd.from_array(np.array(list(map(lambda i: i[1], token_features)))).compute()\n    cwc_max.name = \"cwc_max\"\n    df.merge(cwc_max.to_frame())\n    csc_min = dd.from_array(np.array(list(map(lambda i: i[2], token_features)))).compute()\n    csc_min.name = \"csc_min\"\n    df.merge(csc_min.to_frame())\n    csc_max = dd.from_array(np.array(list(map(lambda i: i[3], token_features)))).compute()\n    csc_max.name = \"csc_max\"\n    df.merge(csc_max.to_frame())\n    ctc_min = dd.from_array(np.array(list(map(lambda i: i[4], token_features)))).compute()\n    ctc_min.name = \"ctc_min\"\n    df.merge(ctc_min.to_frame())\n    ctc_max = dd.from_array(np.array(list(map(lambda i: i[5], token_features)))).compute()\n    ctc_max.name = \"ctc_max\"\n    df.merge(ctc_max.to_frame())\n    last_word_eq = dd.from_array(np.array(list(map(lambda i: i[6], token_features)))).compute()\n    last_word_eq.name = \"last_word_eq\"\n    df.merge(last_word_eq.to_frame())\n    first_word_eq = dd.from_array(np.array(list(map(lambda i: i[7], token_features)))).compute()\n    first_word_eq.name = \"first_word_eq\"\n    df.merge(first_word_eq.to_frame())\n    abs_len_diff = dd.from_array(np.array(list(map(lambda i: i[8], token_features)))).compute()\n    abs_len_diff.name = \"abs_len_diff\"\n    df.merge(abs_len_diff.to_frame())\n    mean_len = dd.from_array(np.array(list(map(lambda i: i[9], token_features)))).compute()\n    mean_len.name = \"mean_len\"\n    df.merge(mean_len.to_frame())\n    df[\"token_set_ratio\"] = df.apply(lambda row: fuzz.token_set_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"token_sort_ratio\"] = df.apply(lambda row: fuzz.token_sort_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"fuzz_ratio\"] = df.apply(lambda row: fuzz.QRatio(row.question1, row.question2), axis=1).compute()\n    df[\"fuzz_partial_ratio\"] = df.apply(lambda row: fuzz.partial_ratio(row.question1, row.question2), axis=1).compute()\n    df[\"longest_substr_ratio\"]  = df.apply(lambda row: fetch_longest_substr_ratio(row.question1, row.question2), axis=1).compute()\n    return df","c9b12f5d":"dff = dd.from_pandas(train_df, npartitions=5)\ndff.head()","1f5d1b06":"df = extract_features(df)","5b75f805":"df.head(2)","7f255a5a":"df_duplicate = df[df['is_duplicate'] == 1].compute()\ndf_nonduplicate = df[df['is_duplicate'] == 0].compute()","e1319a56":"np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()","b1eb9edb":"df_duplicate = df[df['is_duplicate'] == 1].compute()\ndf_nonduplicate = df[df['is_duplicate'] == 0].compute()\n\nduplicate_flatten = np.dstack([df_duplicate[\"question1\"], df_duplicate[\"question2\"]]).flatten()\nnonduplicate_flatten = np.dstack([df_nonduplicate[\"question1\"], df_nonduplicate[\"question2\"]]).flatten()\nprint (\"Number of questions in duplicate pairs set(class 1) : \",duplicate_flatten.shape[0])\nprint (\"Number of questions in non-duplicate pairs set(class 0) : \",nonduplicate_flatten.shape[0])","3fc4c85a":"os.chdir(\"\/kaggle\/working\/\")\nnp.savetxt('train_duplicate.txt', duplicate_flatten, delimiter=' ', fmt='%s')\nnp.savetxt('train_nonduplicate.txt', nonduplicate_flatten, delimiter=' ', fmt='%s')\n#Reading the text files\nduplicate_w = open(path.join(\"\/kaggle\/working\/\", 'train_duplicate.txt')).read()\nnonduplicate_w = open(path.join(\"\/kaggle\/working\/\", 'train_nonduplicate.txt')).read()\nprint (\"Total number of words in duplicate pair set :\",len(duplicate_w))\nprint (\"Total number of words in non duplicate pair set :\",len(nonduplicate_w))","161677a3":"stop_words = set(STOPWORDS)\nstop_words.add(\"said\")\nstop_words.add(\"br\")\nstop_words.add(\" \")\nstop_words.remove(\"not\")\nstop_words.remove(\"no\")\nstop_words.remove(\"like\")","4b390483":"wc = WordCloud(background_color=\"white\", max_words=len(duplicate_w), stopwords=stop_words)\nwc.generate(duplicate_w)\nplt.figure(figsize =(10,8))\nplt.title(\"Word cloud for duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","d8968dd6":"wc = WordCloud(background_color=\"white\", max_words=len(nonduplicate_w), stopwords=stop_words)\nwc.generate(nonduplicate_w)\nplt.figure(figsize =(10,8))\nplt.title(\"Word cloud for non-duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","b46c91c4":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of token_sort_ration across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['token_sort_ratio'].compute(), ax=ax1)\nax2.set_title(\"Distribution of token_sort_ration across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['token_sort_ratio'].compute() , label = \"1\",ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['token_sort_ratio'].compute() , label = \"0\" , ax=ax2)","95307281":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of token_sort_ration across both the duplicacy level\")\nsns.violinplot(x = df['is_duplicate'].compute(), y = df['fuzz_ratio'].compute(), ax=ax1)\nax2.set_title(\"Distribution of token_sort_ration across both the duplicacy level\")\nsns.distplot(df[df['is_duplicate'] == 1.0]['fuzz_ratio'].compute() , label = \"1\",ax=ax2)\nsns.distplot(df[df['is_duplicate'] == 0.0]['fuzz_ratio'].compute() , label = \"0\" , ax=ax2)","2d9d57e3":"ddf = dd.from_pandas(train_df, npartitions=5)","d3e7e847":"ddf['question1'] = ddf.apply(lambda row: str(row.question1), axis=1).compute()\nddf['question2'] = ddf.apply(lambda row: str(row.question2), axis=1).compute()","bae36860":"merge_questions = list(ddf['question1'].compute()) + list(ddf['question2'].compute())\ntfidf = TfidfVectorizer(lowercase=False)\ntfidf.fit_transform(merge_questions)","392a684a":"word_to_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","09b44bc3":"w2v = spacy.load('en_core_web_sm')","34983b74":"w2v_vec_q1 = []\nfor qus1 in tqdm(list(ddf['question1'].compute())):\n    doc_q1 = w2v(qus1)\n    mean_vec_q1 = np.zeros([len(doc_q1), len(doc_q1[0].vector)])\n    for word in doc_q1:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q1 += vec * idf\n    mean_vec_q1 = mean_vec_q1.mean(axis=0)\n    w2v_vec_q1.append(mean_vec_q1)\nq1_feats_m = dd.from_array(np.array(list(w2v_vec_q1))).compute()","0e6aca32":"w2v_vec_q2 = []\nfor qus2 in tqdm(list(ddf['question2'].compute())):\n    doc_q2 = w2v(qus2)\n    mean_vec_q2 = np.zeros([len(doc_q2), len(doc_q2[0].vector)])\n    for word in doc_q2:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q2 += vec * idf\n    mean_vec_q2 = mean_vec_q2.mean(axis=0)\n    w2v_vec_q2.append(mean_vec_q2)\nq2_feats_m = dd.from_array(np.array(list(w2v_vec_q2))).compute()","fe07e965":"q1_feats_m[\"id\"] = df[\"id\"]\nq2_feats_m[\"id\"] = df[\"id\"]\ndf_q = q1_feats_m.merge(q2_feats_m,on =\"id\",how = \"left\")","7742e375":"df = df.drop([\"qid1\", \"qid2\", \"question1\",\"question2\"], axis=1).compute()\ndf_final = df.merge(df_q,on =\"id\",how = \"left\")","ec079907":"df_final = dd.from_pandas(df_final, npartitions=5)\ndf_final.head()","7a6136d4":"from dask_ml.model_selection import train_test_split","de3594a9":"y = df_final[\"is_duplicate\"].compute()\ndf_final = df_final.drop(['id', 'is_duplicate'],axis=1).compute()\nX_train,X_test, y_train, y_test = train_test_split(df_final, y, test_size=0.3,random_state = 42)","f1d3fce0":"print(\"Training data size :\",X_train.shape)\nprint(\"Test data size :\",X_test.shape)","76eca71e":"from dask_ml.linear_model import LogisticRegression","797397ff":"clf = LogisticRegression(random_state = 42)\nclf.fit(X_train.values, y_train.values)","b8388680":"from dask_ml.metrics import accuracy_score, log_loss\ny_pred = clf.predict_proba(X_test.values)\nprint(\"Log loss of the model : \", log_loss(y_test, y_pred))","bdf7edd6":"y_pred = clf.predict(X_test.values)\nprint(\"Accuracy of the model : \", accuracy_score(da.from_array(y_test, chunks = 5),da.from_array(y_pred, chunks = 5)))","b3e624a4":"#### 5.1.b Univariate analysis of feature word_common","03f25831":"#### 6.1.c Distribution of the fuzz_ratio ","0014076e":"### Splitting into train and test set with 70:30 ratio","9bed4255":"### 7. Featurization through weighted tf-idf based word vectors","881f24ed":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data using dask**](#2.-Loading-data-using-dask)   \n3. [**Basic Data Analysis**](#3.-Basic-Data-Analysis)  \n    3.1 [**Checking for class imbalance**](#3.1-Checking-for-class-imbalance)  \n    3.2 [**Number of distinct questions**](#3.2-Number-of-distinct-questions)  \n4. [**Data preprocessing**](#4.-Data-preprocessing)  \n    4.1 [**Checking for duplicates**](#4.1-Checking-for-duplicates)  \n    4.2 [**Checking for missing values**](#4.2-Checking-for-missing-values)  \n5. [**Basic Feature Extraction**](#5.-Basic-Feature-Extraction)  \n    5.1 [**Analysis on few extracted features**](#5.1-Analysis-on-few-extracted-features)  \n6. [**Text preprocessing**](#6.-Text-preprocessing)   \n    6.1 [**Analysing extracted features **](#6.1-Analysing-extracted-features )  \n7. [**Featurization through weighted tf-idf based word vectors**](#7.-Featurization-through-weighted-tf-idf-based-word-vectors) \n8. [**Merging all the extacted features**](#8.-Merging-all-the-extacted-features)\n9. [**Machine Learning models**](#9.-Machine-Learning-models)  \n    9.1 [**Fitting Logistic Regression**](#9.1-Fitting-Logistic-Regression-model)  ","1e167018":"### 9. Machine Learning models\n\n#### 9.1 Fitting Logistic Regression model \n","85d7789b":"#### Frequency of each question","c32605f3":"### 6.1 Analysing extracted features \n\n#### 6.1.a Word cloud formation","ad4aba4a":"### 3. Basic Data Analysis\n\n#### 3.1 Checking for class imbalance","5b110f31":"#### 5.1 Analysis on few extracted features","204420cf":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","301abbdb":"### 2. Loading data using dask","b2dd8471":"#### 5.1.a Univariate analysis of feature word_share","9d29cc41":"### 1. Importing necessary libraries","1fded2a0":"### 4. Data preprocessing\n\n#### 4.1 Checking for duplicates","06cba2ff":"#### 3.2 Number of distinct questions","42efa6bb":"### 5. Basic Feature Extraction\n\n- freq_qid1 = Frequency of qid1's\n- freq_qid2 = Frequency of qid2's\n- q1len = Length of q1\n- q2len = Length of q2\n- q1_n_words = Number of words in Question 1\n- q2_n_words = Number of words in Question 2\n- word_Common = (Number of common unique words in Question 1 and Question 2)\n- word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n- word_share = (word_common)\/(word_Total)\n- freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n- freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2","18ed035e":"#### 6.1.b Distribution of the token_sort_ratio ","cafe9d5b":"### 8. Merging all the extacted features","5b81aa19":"### 6. Text preprocessing\n\nIt involves - \n- Removing html tags\n- Removing Punctuations\n- Removing Stopwords\n- Performing stemming\n- Expanding contractions etc.","f467f21b":"#### 4.2 Checking for missing values"}}