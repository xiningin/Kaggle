{"cell_type":{"11111c55":"code","000838c2":"code","209af066":"code","bc7b41f4":"code","ec58a87f":"code","5b28367c":"code","f4c366d0":"code","a3856e71":"code","84f10aa5":"code","7ff179f5":"code","9b5b0ed6":"code","88c620a1":"code","8e4c236d":"code","15d3572f":"code","dd4dc984":"code","08909482":"code","13fdc0d4":"code","ea9426a1":"code","e2918a5d":"code","3561caa0":"code","ac9fb97c":"markdown","f88f70bb":"markdown","222cc816":"markdown","99379737":"markdown","00407852":"markdown","2d1147e5":"markdown","3c5f5ccd":"markdown","d59633cb":"markdown"},"source":{"11111c55":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics.pairwise import euclidean_distances\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport string\nfrom nltk.corpus import stopwords","000838c2":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","209af066":"print(\"Shape of train data : \", train.shape)\nprint(\"Shape of test data : \", test.shape)","bc7b41f4":"train.head()","ec58a87f":"print(\"Missing value in train data :\\n\",train.isna().sum())\nprint(\"\\nMissing value in test data :\\n\",test.isna().sum())","5b28367c":"#getting number of relevant and irrelevent tweets out of total 7,613 tweets in train dataset\ntrain[['id','target']].groupby('target').count()","f4c366d0":"counts = pd.DataFrame(train[\"target\"].value_counts())\ncounts.rename(columns={\"target\": \"Samples\"}, index={0: \"Not Real\", 1: \"Real\"}, inplace=True)\nax = sns.barplot(x=counts.index, y=counts.Samples)\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(\n        x=p.get_x()+(p.get_width()\/2),\n        y=height,\n        s=round(height),\n        ha=\"center\"\n    )\n","a3856e71":"train['tweet_len'] = train.apply(lambda row : len(row['text']), axis = 1)\ntest['tweet_len'] = test.apply(lambda row : len(row['text']), axis = 1)","84f10aa5":"test.head()","7ff179f5":"#Distribution of tweets length based on relevant\/irrelevant fact\nplt.figure(figsize=(10, 6))\n\ntrain[train.target== 0].tweet_len.plot(bins=40, kind='hist', color='blue', \n                                       label='irrelevant', alpha=0.6)\ntrain[train.target==1].tweet_len.plot(bins=40,kind='hist', color='red', \n                                      label='relevant', alpha=0.6)\nplt.legend()\nplt.xlabel(\"Length of text\")\nplt.show()","9b5b0ed6":"def text_cleaning_process(text):\n    STOPWORDS = stopwords.words('english')\n    nopunc = [char for char in text if char not in string.punctuation]\n    nopunc = ''.join(nopunc)\n    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])","88c620a1":"train['clean_text'] = train.apply(lambda row : text_cleaning_process(row['text']), axis = 1)\ntest['clean_text'] = test.apply(lambda row : text_cleaning_process(row['text']), axis = 1)","8e4c236d":"train.head()","15d3572f":"# defining X (input) and y (label) from the dataframe columns for later use in COUNTVECTORIZER\nX_train = train['clean_text'].values\ny_train = train['target'].values\nX_test = test['clean_text'].values\n# y_test = df_tweet_test.target.values\n#shape and dimension of X and y arrays\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)","dd4dc984":"xtrain, xvalid, ytrain, yvalid = train_test_split(X_train, y_train, \n                                                  stratify=y_train, \n                                                  random_state=42, \n                                                  test_size=0.3, shuffle=True)","08909482":"print (xtrain.shape)\nprint (xvalid.shape)","13fdc0d4":"# Always start with these features. They work (almost) everytime!\ntfv = TfidfVectorizer(min_df=3,  max_features=None, \n            strip_accents='unicode', analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), use_idf=1,smooth_idf=1,sublinear_tf=1,\n            stop_words = 'english')\n\n# Fitting TF-IDF to both training and test sets (semi-supervised learning)\ntfv.fit(list(xtrain) + list(xvalid))\nxtrain_tfv =  tfv.transform(xtrain) \nxvalid_tfv = tfv.transform(xvalid)","ea9426a1":"# Fitting a simple Logistic Regression on TFIDF\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_tfv, ytrain)\npredictions = clf.predict_proba(xvalid_tfv)\n\nprint (\"logloss: %0.3f \" % log_loss(yvalid, predictions))","e2918a5d":"ctv = CountVectorizer(analyzer='word',token_pattern=r'\\w{1,}',\n            ngram_range=(1, 3), stop_words = 'english')\n\n# Fitting Count Vectorizer to both training and test sets (semi-supervised learning)\nctv.fit(list(xtrain) + list(xvalid))\nxtrain_ctv =  ctv.transform(xtrain) \nxvalid_ctv = ctv.transform(xvalid)","3561caa0":"# Fitting a simple Logistic Regression on Counts\nclf = LogisticRegression(C=1.0)\nclf.fit(xtrain_ctv, ytrain)\npredictions = clf.predict_proba(xvalid_ctv)\n\nprint (\"logloss: %0.3f \" % log_loss(yvalid, predictions))","ac9fb97c":"As we can see there are missing values in keyword and Location features.","f88f70bb":"### Building Basic Models\nLet's start building our very first model.\n\nOur very first model is a simple TF-IDF (Term Frequency - Inverse Document Frequency) followed by a simple Logistic Regression.","222cc816":"As we can see above it is an imbalanced dataset, as number of irrelevant tweets is considerably higher than relevant ones.","99379737":"And there we go. We have our first model with a logloss of 0.490.\n\nBut we are greedy and want a better score. Lets look at the same model with a different data.\n\nInstead of using TF-IDF, we can also use word counts as features. This can be done easily using CountVectorizer from scikit-learn.","00407852":"Let's check how data looks like,","2d1147e5":"We just improved our first model by 0.027!!!\n","3c5f5ccd":"## 2. Looking at Class Imbalance\nIt looks like we have 7,613 training samples. Let's see how many tweets we have that are examples of disaster versus those that are not. What we're looking at is whether or not we have a balance between samples that are both real examples of disasters, and those that are not.","d59633cb":"    Takes in a string of text, then performs the following:\n    Vectorization1. Remove all punctuation\n    2. Remove all stopwords\n    3. Returns a list of the cleaned text"}}