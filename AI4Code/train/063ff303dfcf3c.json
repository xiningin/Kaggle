{"cell_type":{"e9557742":"code","5b435623":"code","d0c181dc":"code","a6247616":"code","8c989ec8":"code","9ea36cfc":"code","747f236e":"code","c75bf865":"code","809d0bb7":"code","d02515d3":"code","e4ce249f":"code","61674df7":"code","16466cc0":"code","c15c8529":"code","3619060d":"code","9fd7f255":"code","e221aae1":"code","33543db7":"code","c933c204":"code","6572c99f":"code","d2ee0c25":"code","704ace55":"code","a73287aa":"code","97429a45":"code","f8aa16c4":"code","68812d70":"code","3b323e63":"code","8c33c0bd":"code","8515c11a":"code","fc4f93d8":"code","524087e8":"code","7826728b":"code","d3f1f238":"code","c776ade0":"code","5452ac64":"code","64537e42":"code","cbc99eef":"code","6b14b723":"code","f413f224":"code","de81fde2":"code","a77dd229":"code","e35cbd92":"code","ed09715a":"code","4f31ce58":"code","e15f449a":"code","06b73d40":"code","2e0a2607":"code","22895b73":"code","a8a42d0d":"code","3066e28b":"code","dd972cac":"code","7ae816e9":"code","8a524e69":"code","cd7ff1dc":"code","2145d5d2":"code","a528d1af":"code","a7542b7b":"code","c6c15272":"code","3baee159":"code","cf388aff":"code","d5d874e7":"code","adf3002d":"code","272d78de":"code","99e28557":"code","7897d5a8":"code","5edcb8c2":"code","89c857f5":"code","6ed412d0":"code","5672cfe8":"code","37cf6bf2":"code","84db09b7":"code","935b295c":"code","0d6aa713":"code","88875065":"code","bd1e7107":"code","b5ba14f2":"code","fb8cac22":"code","02743af2":"code","38dc1fd5":"markdown","5a89463a":"markdown","9d627c64":"markdown","75fdc419":"markdown","503548a0":"markdown","39e541bc":"markdown","02d1402d":"markdown","290e3ba4":"markdown","80be6884":"markdown","75b0993a":"markdown","92d19eff":"markdown","c058230d":"markdown","317675e3":"markdown","e13158b3":"markdown","d5c3d305":"markdown","6c25bbdc":"markdown","b5ec8d78":"markdown","657c65e8":"markdown","cebe9b3f":"markdown","9687f049":"markdown","fe7994e2":"markdown","5ec0b155":"markdown","917439b0":"markdown","ed95ffdf":"markdown","4aaf3790":"markdown","bd14de1b":"markdown","d343910a":"markdown","60fefae2":"markdown","de84930c":"markdown"},"source":{"e9557742":"#computation import \nimport pandas as pd\nimport numpy as np\nimport os\nimport sklearn.metrics as metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom scipy import stats\n\n#plot imports\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib as mpl\n\n#management import\nimport gc\nimport time\n\n%matplotlib inline","5b435623":"%time\ndir = '..\/input\/digit-recognizer\/'\nz = '.csv'\ntrain = pd.read_csv(dir+'train'+z)\ntest = pd.read_csv(dir+'test'+z)","d0c181dc":"#ENV SETUP OF THE NOTEBOOK\nTARGET = 'label'\nFOLD = 5\nSEED = 42","a6247616":"print('Train Shape: ',train.shape)\ntrain.head(10)","8c989ec8":"print('Test Shape: ',test.shape)\ntest.head(10)","9ea36cfc":"#seeing for missing values\nprint(f'''Train Null Count: {train.isnull().sum().sum()}\nTest Null Count: {test.isnull().sum().sum()}''')","747f236e":"y = train[TARGET]\ntrain.drop(TARGET,axis=1,inplace=True)\n\n#check if all columns in train is in test\nassert train.columns.to_list() == test.columns.to_list()","c75bf865":"#function to plot multiple digits\ndef plot_digits(instances, images_per_row=5, **options):\n    size = 28\n    images_per_row = min(len(instances), images_per_row)\n    images = [instance.reshape(size,size) for instance in instances]\n    n_rows = (len(instances) - 1) \/\/ images_per_row + 1\n    row_images = []\n    n_empty = n_rows * images_per_row - len(instances)\n    images.append(np.zeros((size, size * n_empty)))\n    for row in range(n_rows):\n        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n        row_images.append(np.concatenate(rimages, axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap = mpl.cm.binary, **options)\n    plt.axis(\"off\")","809d0bb7":"#single digit plot\nsome_digit = train.iloc[0].values\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap=mpl.cm.binary)\nplt.axis(\"off\")\nplt.show()","d02515d3":"#multiple digit plot\nexample_images = train.iloc[:100].values\nplot_digits(example_images, images_per_row=10)","e4ce249f":"#plot the count of labels in the training dataset\n_x = y.value_counts().index\n_y = y.value_counts().values\n\nsns.barplot(_x,_y)\n\ndel _x, _y\ngc.collect()","61674df7":"from sklearn.ensemble import RandomForestClassifier\nX_train, X_test, y_train, y_test = train_test_split(train,y,test_size=0.2,\n                                                    random_state=SEED)\n\nstart_time = time.time()\nrf = RandomForestClassifier(n_estimators=100, random_state=SEED)\nrf.fit(X_train,y_train)\n\nexecution_time = time.time() - start_time\n\nprint(f\"Training took {execution_time:.2f}s\")","16466cc0":"y_hat = rf.predict(X_test)\nround(accuracy_score(y_test,y_hat)*100,2)","c15c8529":"cv = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\nstart_time = time.time()\nfor fold,(train_indx,test_indx) in enumerate(cv.split(train,y)):\n    X_train, X_test = train.iloc[train_indx].reset_index(drop=True),train.iloc[test_indx].reset_index(drop=True)\n    y_train, y_test = y.iloc[train_indx].reset_index(drop=True),y.iloc[test_indx].reset_index(drop=True)\n    \n    rf = RandomForestClassifier(n_estimators=100,random_state=fold)\n    rf.fit(X_train,y_train)\n    \n    y_hat = rf.predict(X_test)\n    print(f'Fold {fold+1}: Accuracy Score: {round(accuracy_score(y_test,y_hat)*100,2)}%')\n    print('='*30)\nexecution_time = time.time() - start_time\n\ndel X_train, X_test, y_train, y_test, rf, cv, y_hat\ngc.collect()\n\nprint(f\"5 Folds Took: {execution_time:.2f}s\")","3619060d":"from sklearn.decomposition import PCA\n\nX_train, X_test, y_train, y_test = train_test_split(train,y,test_size=0.2,\n                                                    random_state=SEED)\n\npca = PCA(n_components=0.95)\nX_train_reduced = pca.fit_transform(X_train)\nprint('Shape of train-set After PCA:',X_train_reduced.shape)","9fd7f255":"'''\nThe following code compresses the MNIST dataset down to 154 dimensions, then\nuses the inverse_transform() method to decompress it back to 784 dimensions:\n'''\n\nX_recovered = pca.inverse_transform(X_train_reduced)\n\n#single digit plot\nsome_digit = X_recovered[0]\nsome_digit_image = some_digit.reshape(28, 28)\nplt.imshow(some_digit_image, cmap=mpl.cm.binary)\nplt.axis(\"off\")\nplt.show()","e221aae1":"start_time = time.time()\nrf = RandomForestClassifier(n_estimators=100, random_state=SEED)\nrf.fit(X_train_reduced,y_train)\n\nexecution_time = time.time() - start_time\n\nprint(f\"Training took {execution_time:.2f}s\")","33543db7":"X_test_reduced = pca.transform(X_test)\n\ny_hat = rf.predict(X_test_reduced)\nround(accuracy_score(y_test,y_hat)*100,2)","c933c204":"from sklearn.linear_model import LogisticRegression\n\nlog_clf = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", random_state=SEED)\n\nstart_time = time.time()\nlog_clf.fit(X_train, y_train)\nend_time = time.time()\n\nex_time =  end_time - start_time\nprint(f\"Training took {ex_time:.2f}s\")","6572c99f":"y_pred = log_clf.predict(X_test)\nround(accuracy_score(y_test, y_pred),2)*100","d2ee0c25":"log_clf2 = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", random_state=SEED)\n\nstart_time = time.time()\nlog_clf2.fit(X_train_reduced, y_train)\nend_time = time.time()\n\nex_time =  end_time - start_time\nprint(f\"Training took {ex_time:.2f}s\")","704ace55":"y_pred = log_clf.predict(X_test)\nround(accuracy_score(y_test, y_pred),2)*100","a73287aa":"np.random.seed(SEED) #setting up the seed\n\nm = 10000\nidx = np.random.permutation(42000)[:m]\n\nX = train.iloc[idx,:]\ny = y[idx]","97429a45":"from sklearn.manifold import TSNE\nstart_time = time.time()\ntsne = TSNE(n_components=2, random_state=SEED)\nX_reduced = tsne.fit_transform(X)\nend_time = time.time()\n\nprint(end_time - start_time)","f8aa16c4":"plt.figure(figsize=(13,10))\nplt.scatter(X_reduced[:, 0], X_reduced[:, 1],s=12, c=y, cmap=\"tab10\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","68812d70":"#function to plot scatter plot with digits on it.\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.offsetbox import AnnotationBbox, OffsetImage\n\ndef plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n    # Let's scale the input features so that they range from 0 to 1\n    X_normalized = MinMaxScaler().fit_transform(X)\n    # Now we create the list of coordinates of the digits plotted so far.\n    # We pretend that one is already plotted far away at the start, to\n    # avoid `if` statements in the loop below\n    neighbors = np.array([[10., 10.]])\n    # The rest should be self-explanatory\n    plt.figure(figsize=figsize)\n    cmap = mpl.cm.get_cmap(\"jet\")\n    digits = np.unique(y)\n    for digit in digits:\n        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1],s=12, c=[cmap(digit \/ 9)])\n    plt.axis(\"off\")\n    ax = plt.gcf().gca()  # get current axes in current figure\n    for index, image_coord in enumerate(X_normalized):\n        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n        if closest_distance > min_distance:\n            neighbors = np.r_[neighbors, [image_coord]]\n            if images is None:\n                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n                         color=cmap(y[index] \/ 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n            else:\n                image = images[index].reshape(28, 28)\n                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n                ax.add_artist(imagebox)","3b323e63":"plot_digits(X_reduced, y, images=X.values, figsize=(22, 22))","8c33c0bd":"from sklearn.decomposition import PCA\nstart_time = time.time()\ntsne = PCA(n_components=2, random_state=SEED)\nX_reduced = tsne.fit_transform(X)\nend_time = time.time()\n\nprint(f'{round(end_time - start_time,2)}s')","8515c11a":"plot_digits(X_reduced, y, images=X.values, figsize=(22, 22))","fc4f93d8":"from sklearn.manifold import LocallyLinearEmbedding\nstart_time = time.time()\ntsne = LocallyLinearEmbedding(n_components=2, random_state=SEED)\nX_reduced = tsne.fit_transform(X)\nend_time = time.time()\n\nprint(f'{round(end_time - start_time,2)}s')","524087e8":"plot_digits(X_reduced, y, images=X.values, figsize=(22, 22))","7826728b":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nstart_time = time.time()\ntsne = LinearDiscriminantAnalysis(n_components=2)\nX_reduced = tsne.fit_transform(X,y)\nend_time = time.time()\n\nprint(f'{round(end_time - start_time,2)}s')","d3f1f238":"plot_digits(X_reduced, y, images=X.values, figsize=(22, 22))","c776ade0":"import umap\nstart_time = time.time()\ntsne = umap.UMAP(n_components=2, random_state=SEED)\nX_reduced = tsne.fit_transform(X)\nend_time = time.time()\n\nprint(f'{round(end_time - start_time,2)}s')","5452ac64":"from sklearn.pipeline import Pipeline\npca_tsne = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=42)),\n    (\"tsne\", TSNE(n_components=2, random_state=42)),\n])\nstart_time = time.time()\nX_pca_tsne_reduced = pca_tsne.fit_transform(X)\nend_time = time.time()\nprint(\"PCA+t-SNE took {:.1f}s.\".format(end_time - start_time))\nplot_digits(X_pca_tsne_reduced, y.values)\nplt.show()","64537e42":"plot_digits(X_pca_tsne_reduced, y, images=X.values, figsize=(22, 22))","cbc99eef":"plot_digits(X_pca_tsne_reduced, y, images=X.values, figsize=(22, 22))","6b14b723":"pca_umap = Pipeline([\n    (\"pca\", PCA(n_components=0.95, random_state=SEED)),\n    (\"umap\", umap.UMAP(n_components=2, random_state=SEED)),\n])\nstart_time = time.time()\nX_pca_umap_reduced = pca_umap.fit_transform(X)\nend_time = time.time()\nprint(\"PCA+UMAP took {:.1f}s.\".format(end_time - start_time))\nplot_digits(X_pca_umap_reduced, y.values)\nplt.show()","f413f224":"plot_digits(X_pca_umap_reduced, y, images=X.values, figsize=(22, 22))","de81fde2":"_X = train[:5000]\n_y = y[:5000]","a77dd229":"import numpy as np\nfrom sklearn.linear_model import Perceptron, LogisticRegression\n\n#starified kfold\ncv = StratifiedKFold(n_splits=5,shuffle=True,random_state=SEED)\nstart_time = time.time()\nfor fold,(train_indx,test_indx) in enumerate(cv.split(_X,_y)):\n    X_train, X_test = _X.iloc[train_indx].reset_index(drop=True),_X.iloc[test_indx].reset_index(drop=True)\n    y_train, y_test = _y.iloc[train_indx].reset_index(drop=True),_y.iloc[test_indx].reset_index(drop=True)\n    \n    #perceptron and logistic regression comparision\n    per = Perceptron(tol=1e-3, random_state=SEED)\n    logi = LogisticRegression(solver='saga',random_state=SEED,multi_class='multinomial')\n    \n    per.fit(X=X_train,y=y_train)\n    logi.fit(X=X_train,y=y_train)\n    \n    y_hat = per.predict(X_test)\n    y_hat_lr = logi.predict(X_test)\n    \n    print(f'Fold {fold+1}:Perceptron Accuracy Score: {round(accuracy_score(y_test,y_hat)*100,2)}%')\n    print(f'Fold {fold+1}:Logistic Regression Accuracy Score: {round(accuracy_score(y_test,y_hat_lr)*100,2)}%')\n\n    print('='*30)\nexecution_time = time.time() - start_time\n\ndel X_train, X_test, y_train, y_test, cv, y_hat, y_hat_lr, _X, _y\ngc.collect()","e35cbd92":"import os\n\nroot_dir = os.path.join(os.curdir,'my_logs')\n\ndef get_run_log():\n    import time\n    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n    return os.path.join(root_dir,run_id)\n","ed09715a":"import tensorflow as tf\nfrom tensorflow import keras\n\ntf.__version__, keras.__version__","4f31ce58":"train.loc[0].min(), train.loc[0].max() #we need to scale down the greyscale value to 0-1 (normalize)","e15f449a":"dir = '..\/input\/digit-recognizer\/'\nz = '.csv'\ntrain = pd.read_csv(dir+'train'+z)\ntest = pd.read_csv(dir+'test'+z)\n\ny = train[TARGET]\ntrain.drop(TARGET,axis=1,inplace=True)\n\n#normalizing the values between 0 and 1\nX_valid, X_train = train[:5000] \/ 255., train[5000:] \/ 255.\ny_valid, y_train = y[:5000], y[5000:]\nX_test = test \/ 255.","06b73d40":"plt.imshow(X_train.iloc[0,:].values.reshape(28,28), cmap=\"binary\")\nprint(y_train.iloc[0])\nplt.axis('off')\nplt.show() #ploting the first row of all the columns to plot the digit","2e0a2607":"train.shape","22895b73":"#clearing previous states and seeding\nkeras.backend.clear_session()  #Resets all state generated by Keras.\nnp.random.seed(42) #seed for reuseablity\ntf.random.set_seed(42)\n#initializing model\nmodel = keras.models.Sequential()\nmodel.add(keras.layers.Dense(300, activation=\"relu\",input_shape=(784,))) #first layer with 300 neurons and we also mentioned the input shape\nmodel.add(keras.layers.Dense(100, activation=\"relu\")) #secound layer with relu activation \nmodel.add(keras.layers.Dense(10, activation=\"softmax\")) #output layer with softmax function as this is the multiclass classification problem","a8a42d0d":"model.layers #we can see all the layers as list of object (only if mentioned the input shape above)","3066e28b":"model.summary() #it shows all the information about the model like layer name, input\/output shape etc. for more info please refer tensorflow documents","dd972cac":"keras.utils.plot_model(model, \"simple_keras_sequential.png\", show_shapes=True)","7ae816e9":"hidden1 = model.layers[1]\nhidden1.name","8a524e69":"model.get_layer(hidden1.name) is hidden1","cd7ff1dc":"weights, biases = hidden1.get_weights()\nweights","2145d5d2":"biases","a528d1af":"weights.shape, biases.shape","a7542b7b":"#compiling model (mentioning the loss and metrics for the model)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])","c6c15272":"#using earlystopping callback to use best model and avoid overfitting \ncallback = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n\n#tensorboard\nrun_log = get_run_log()\ntensorboard_cb = keras.callbacks.TensorBoard(run_log)\n\nhistory = model.fit(X_train, y_train, epochs=100,validation_data=(X_valid, y_valid), callbacks= [callback,tensorboard_cb])","3baee159":"history.params","cf388aff":"print(history.epoch)\nhistory.history.keys()","d5d874e7":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","adf3002d":"model.evaluate(X_valid, y_valid)","272d78de":"sample_submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\npreds = model.predict(test)\nnp.argmax(preds, axis=-1)\n\nsample_submission.Label = np.argmax(preds, axis=-1)\nsample_submission.to_csv('keras_Squesntial_model.csv',index=False)","99e28557":"skf = StratifiedKFold(n_splits=5,random_state=SEED, shuffle=True)\n\nprediction = {}\nprediction['sequential'] = []\n\nfor fold, (train_indx,valid_indx) in enumerate(skf.split(train,y)):\n    X_train, X_valid = train.loc[train_indx,:].reset_index(drop=True) \/255, train.loc[valid_indx,:].reset_index(drop=True) \/255\n    y_train, y_valid = y.loc[train_indx].reset_index(drop=True), y.loc[valid_indx].reset_index(drop=True)\n    \n    #clearing previous states and seeding\n    keras.backend.clear_session()  #Resets all state generated by Keras.\n    np.random.seed(42) #seed for reuseablity\n    tf.random.set_seed(42)\n\n    #initializing model\n    model = keras.models.Sequential()\n    model.add(keras.layers.Dense(300, activation=\"relu\",input_shape=(784,))) #first layer with 300 neurons and we also mentioned the input shape\n    model.add(keras.layers.Dense(100, activation=\"relu\")) #secound layer with relu activation \n    model.add(keras.layers.Dense(10, activation=\"softmax\")) #output layer with softmax function as this is the multiclass classification problem\n\n    #compiling model (mentioning the loss and metrics for the model)\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])\n    \n    #using earlystopping callback to use best model and avoid overfitting \n    callback = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n    history = model.fit(X_train, y_train, epochs=100,\n                        validation_data=(X_valid, y_valid), callbacks=[callback])\n    \n    #ploting the model history for every fold\n    pd.DataFrame(history.history).plot(figsize=(8, 5))\n    plt.grid(True)\n    plt.gca().set_ylim(0, 1)\n    plt.show()\n    \n    #make prediction on the test set\n    preds = model.predict_proba(test)\n    \n    #save the prediction of every fold in dictionary\n    prediction['sequential'].append(preds)\n    \n    #cleaning up memory\n    del X_train, y_train, X_valid, y_valid, history, model, preds\n    gc.collect()","7897d5a8":"#taking mean of each columns and then converting to class label\nsample_submission.Label = np.argmax(np.mean(prediction['sequential'],axis=0),axis=1)\nsample_submission.to_csv('Sequentialkeras_5fold.csv',index=False)","5edcb8c2":"dir = '..\/input\/digit-recognizer\/'\nz = '.csv'\ntrain = pd.read_csv(dir+'train'+z)\ntest = pd.read_csv(dir+'test'+z)\n\ny = train[TARGET]\ntrain.drop(TARGET,axis=1,inplace=True)\n\n#normalizing the values between 0 and 1\nX_valid, X_train = train[:5000] \/ 255., train[5000:] \/ 255.\ny_valid, y_train = y[:5000], y[5000:]\nX_test = test \/ 255.","89c857f5":"#clearing previous states and seeding\nkeras.backend.clear_session()  #Resets all state generated by Keras.\nnp.random.seed(42) #seed for reuseablity\ntf.random.set_seed(42)\n#initializing model\n\ninput_ = keras.layers.Input(shape=(784,))\nhidden1 = keras.layers.Dense(300, activation=\"relu\")(input_) #first layer with 300 neurons and we also mentioned the input shape\nhidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1) #secound layer with relu activation \nconcat = keras.layers.Concatenate()([input_, hidden2])\noutput_ = keras.layers.Dense(10, activation=\"softmax\")(concat) #output layer with softmax function as this is the multiclass classification problem\n\nmodel = keras.Model(inputs=[input_], outputs=[output_])","6ed412d0":"model.summary()","5672cfe8":"keras.utils.plot_model(model, \"simple_keras_functional.png\", show_shapes=True)","37cf6bf2":"#compiling model (mentioning the loss and metrics for the model)\nmodel.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])","84db09b7":"#using earlystopping callback to use best model and avoid overfitting \ncallback = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n\n\n#tensorboard\nrun_log = get_run_log()\ntensorboard_cb = keras.callbacks.TensorBoard(run_log)\n\nhistory = model.fit(X_train, y_train, epochs=100,validation_data=(X_valid, y_valid), callbacks= [callback,tensorboard_cb])","935b295c":"pd.DataFrame(history.history).plot(figsize=(8, 5))\nplt.grid(True)\nplt.gca().set_ylim(0, 1)\nplt.show()","0d6aa713":"model.evaluate(X_valid, y_valid)","88875065":"sample_submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')\n\npreds = model.predict(test)\nnp.argmax(preds, axis=-1)\n\nsample_submission.Label = np.argmax(preds, axis=-1)\nsample_submission.to_csv('keras_Functional_model.csv',index=False)","bd1e7107":"skf = StratifiedKFold(n_splits=5,random_state=SEED, shuffle=True)\n\nprediction = {}\nprediction['Functional'] = []\n\nfor fold, (train_indx,valid_indx) in enumerate(skf.split(train,y)):\n    X_train, X_valid = train.loc[train_indx,:].reset_index(drop=True) \/255, train.loc[valid_indx,:].reset_index(drop=True) \/255\n    y_train, y_valid = y.loc[train_indx].reset_index(drop=True), y.loc[valid_indx].reset_index(drop=True)\n    \n    #clearing previous states and seeding\n    keras.backend.clear_session()  #Resets all state generated by Keras.\n    np.random.seed(42) #seed for reuseablity\n    tf.random.set_seed(42)\n    \n    #initializing model\n    input_ = keras.layers.Input(shape=(784,))\n    hidden1 = keras.layers.Dense(300, activation=\"relu\")(input_) #first layer with 300 neurons and we also mentioned the input shape\n    hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1) #secound layer with relu activation \n    concat = keras.layers.Concatenate()([input_, hidden2])\n    output_ = keras.layers.Dense(10, activation=\"softmax\")(concat) #output layer with softmax function as this is the multiclass classification problem\n\n    model = keras.Model(inputs=[input_], outputs=[output_])\n    \n    #compiling model (mentioning the loss and metrics for the model)\n    model.compile(loss=\"sparse_categorical_crossentropy\",\n              optimizer=\"sgd\",\n              metrics=[\"accuracy\"])\n    \n    #using earlystopping callback to use best model and avoid overfitting \n    callback = keras.callbacks.EarlyStopping(patience=10,restore_best_weights=True)\n    history = model.fit(X_train, y_train, epochs=100,\n                        validation_data=(X_valid, y_valid), callbacks=[callback])\n    \n    #ploting the model history for every fold\n    pd.DataFrame(history.history).plot(figsize=(8, 5))\n    plt.grid(True)\n    plt.gca().set_ylim(0, 1)\n    plt.show()\n    \n    #make prediction on the test set\n    preds = model.predict(test)\n    \n    #save the prediction of every fold in dictionary\n    prediction['Functional'].append(preds)\n    \n    #cleaning up memory\n    del X_train, y_train, X_valid, y_valid, history, model, preds\n    gc.collect()","b5ba14f2":"#taking mode of each columns and then converting to class label\nsample_submission.Label = np.argmax(stats.mode(prediction['Functional'],axis=0)[0][0],axis=1)\nsample_submission.to_csv('Functionalkeras_5fold.csv',index=False)","fb8cac22":"# Download Ngrok to tunnel the tensorboard port to an external port\n!wget https:\/\/bin.equinox.io\/c\/4VmDzA7iaHb\/ngrok-stable-linux-amd64.zip\n!unzip ngrok-stable-linux-amd64.zip\n\n# Run tensorboard as well as Ngrox (for tunneling as non-blocking processes)\nimport os\nimport multiprocessing\n\n\npool = multiprocessing.Pool(processes = 10)\nresults_of_processes = [pool.apply_async(os.system, args=(cmd, ), callback = None )\n                        for cmd in [\n                        f\"tensorboard --logdir .\/my_logs --host 0.0.0.0 --port 6006 &\",\n                        \".\/ngrok http 6006 &\"\n                        ]]","02743af2":"! curl -s http:\/\/localhost:4040\/api\/tunnels | python3 -c \\\n    \"import sys, json; print(json.load(sys.stdin)['tunnels'][0]['public_url'])\"","38dc1fd5":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#E74C3C;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Oh no!<\/h2>Training is actually more than twice slower now! How can that be? Well, as we saw in this chapter, dimensionality reduction does not always lead to faster training time: it depends on the dataset, the model and the training algorithm.\n<\/div>\n\n","5a89463a":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n10. Building an Image Classifier Using Keras Simple Functional Class\n<\/div>\n\n> The Keras functional API is a way to create models that are more flexible than the tf.keras.Sequential API. The functional API can handle models with non-linear topology, shared layers, and even multiple inputs or outputs.\n\n* [Jump Top](#0)\n<a id=10><\/a>\n\n","9d627c64":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#27AE60;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Exercise<\/h2>Train a new Random Forest classifier on the reduced dataset and see how long it takes. Was training much faster?\n<\/div>","75fdc419":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#008080;font-size:120%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nLet's see if it helps when using softmax regression:\n<\/div>","503548a0":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#800080;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Poor<\/h2>It is common for performance to drop slightly when reducing dimensionality, because we do lose some useful signal in the process. However, the performance drop is rather severe in this case. So PCA really did not help: it slowed down training and reduced performance. :(\n<\/div>","39e541bc":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#CD5C5C;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Exercise<\/h2>Train a Random Forest classifier on the dataset and time how long it takes, then evaluate the resulting model on the test set.\n<\/div>","02d1402d":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n11. TensorBoard on Kaggle\n<\/div>\n\n> Kaggle New Feature: [How to use TensorBoard in Kaggle Kernel](https:\/\/www.kaggle.com\/product-feedback\/89671)\n\n\n> [TensorBoard on Kaggle](https:\/\/www.kaggle.com\/shivam1600\/tensorboard-on-kaggle)\n\n\n* [Jump Top](#0)\n<a id=11><\/a>\n\n","290e3ba4":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n5. PCA + t-SNE\n<\/div>\n\n* [Jump Top](#0)\n<a id=5><\/a>\n\n","80be6884":"## Using Earlystopping callback on simple sequential class","75b0993a":"## **Start Tensorboard**","92d19eff":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n6. PCA + UMAP\n<\/div>\n\n* [Jump Top](#0)\n<a id=6><\/a>\n\n","c058230d":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:170%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nNote: Upvote is Free!!!\n<\/div>","317675e3":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#008000;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Okay<\/h2>so softmax regression takes much longer to train on this dataset than the random forest classifier, plus it performs worse on the test set. But that's not what we are interested in right now, we want to see how much PCA can help softmax regression. Let's train the softmax regression model using the reduced dataset:\n<\/div>\n","e13158b3":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n8. Logistic Regression Vs Perceptron\n<\/div>\n\nLogistic Regression perform better than perceptron. see this document for more:\n[Connections between Perceptron and Logistic Regression (and SVM)](https:\/\/www.cs.utexas.edu\/~gdurrett\/courses\/fa2021\/perc-lr-connections.pdf)\n\n* [Jump Top](#0)\n<a id=8><\/a>\n\n","d5c3d305":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#A569BD;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Exercise<\/h2>Next, use PCA to reduce the dataset's dimensionality, with an explained variance ratio of 95%.\n<\/div>","6c25bbdc":"## **Get the url to access tensorboad**\nNote: This block of code might throw an error while running all blocks at time. So run blocks one by one till the block below","b5ec8d78":"<img src=\"https:\/\/i.ibb.co\/JHDc1Tn\/tree.png\" alt=\"tree\" border=\"0\">","657c65e8":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n7. My First Artificial Neural networks (ANN) Model\n<\/div>\n\n# ANNs\nANNs are at the very core of Deep Learning. They are versatile, powerful, and scala\u2010\nble, making them ideal to tackle large and highly complex Machine Learning tasks\nsuch as classifying billions of images (e.g., Google Images), powering speech recogni\u2010\ntion services (e.g., Apple\u2019s Siri), recommending the best videos to watch to hundreds\nof millions of users every day (e.g., YouTube), or learning to beat the world champion\nat the game of Go (DeepMind\u2019s AlphaGo).\n\n# Perceptron\n\nThe Perceptron is one of the simplest ANN architectures, invented in 1957 by Frank\nRosenblatt. It is based on a slightly different artificial neuron (see Figure below) called\na threshold logic unit (TLU), or sometimes a linear threshold unit (LTU). The inputs\nand output are numbers (instead of binary on\/off values), and each input connection\nis associated with a weight. The TLU computes a weighted sum of its inputs (z = w1 x1 + w2 x2 ... + wn xn = x\u22ba w), then applies a step function to that sum and outputs the\nresult: hw(x) = step(z), where z = x\u22ba w.\n\n<img src=\"data:image\/png;base64,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\">\n\n> Threshold logic unit: an artificial neuron which computes a weighted sum\nof its inputs then applies a step function\n\n# Step Function\n\n<img src=\"https:\/\/www.simplilearn.com\/ice9\/free_resources_article_thumb\/Perceptron\/Perceptron_9.jpg\">\n\nPerceptrons are trained using a variant of this rule that takes\ninto account the error made by the network when it makes a prediction\nPerceptron learning rule reinforces connections that help reduce the error. More\nspecifically, the Perceptron is fed one training instance at a time, and for each\ninstance it makes its predictions. For every output neuron that produced a wrong\nprediction, it reinforces the connection weights from the inputs that would have con\u2010\ntributed to the correct prediction.\n\n\nThe decision boundary of each output neuron is linear, so Perceptrons are incapable\nof learning complex patterns (just like Logistic Regression classifiers). However, if the\ntraining instances are linearly separable, Rosenblatt demonstrated that this algorithm\nwould converge to a solution.This is called the [**Perceptron convergence theorem**](http:\/\/web.mit.edu\/course\/other\/i2course\/www\/vision_and_learning\/perceptron_notes.pdf).\n\n* [Jump Top](#0)\n<a id=7><\/a>","cebe9b3f":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n9. Building an Image Classifier Using Keras Simple Sequential Class\n<\/div>\n\n> First let's import TensorFlow and Keras.\n\n* [Jump Top](#0)\n<a id=9><\/a>\n\n","9687f049":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n3. PCA\n<\/div>\n\n* [Jump Top](#0)\n<a id=3><\/a>\n\n","fe7994e2":"## **Using TensorBoard for Visualization**","5ec0b155":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n2. Digit Plots\n<\/div>\n\n* [Jump Top](#0)\n<a id=2><\/a>\n\n","917439b0":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#CD5C5C;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nUse t-SNE to reduce the MNIST dataset down to two dimensions and plot the result using Matplotlib. You can use a scatterplot using 10 different colors to represent each image's target class.  \nDimensionality reduction on the full 42,000 images takes a very long time, so let's only do this on a random subset of 10,000 images:","ed95ffdf":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#808000;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Nice!<\/h2>\nReducing dimensionality led to over 2\u00d7 speedup. :)\nA very slight drop in performance, which might be a reasonable price to pay for a 2\u00d7 speedup, depending on the application.\nSo there you have it: PCA can give you a formidable speedup... but not always!\n<\/div>","4aaf3790":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n1. Import Data\n<\/div>\n\n* [Jump Top](#0)\n<a id=1><\/a>","bd14de1b":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nDigit Recogniser | Dimension Reduction PCA, t-SNE, UMAP\n<\/div>","d343910a":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#283747;font-size:100%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n<h2>Exercise<\/h2>Next evaluate the classifier on the test set: how does it compare to the previous classifier?\n<\/div>","60fefae2":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\n4. Dimension Reduction\n<\/div>\n\n* [Jump Top](#0)\n<a id=4><\/a>\n\n","de84930c":"<div style=\"color:White; display:fill; border-radius:5px;background-color:#dd4124;font-size:270%;font-family:sans-serif;letter-spacing:0.5px;text-align: center\">\nTable of Contents:\n<\/div>\n\n<a id=0><\/a>\n\n* [1. Import Data](#1)\n* [2. Digit Plots](#2)\n* [3. PCA](#3)\n* [4. Dimension Reduction](#4)\n* [5. PCA + t-SNE](#5)\n* [6. PCA + UMAP](#6)\n* [7. My First Artificial Neural networks (ANN) Model](#7)\n* [8. Logistic Regression vs Perceptron](#8)\n* [9. Building an Image Classifier Using Keras Simple Sequential Class](#9)\n* [10.Building an Image Classifier Using Keras Simple Functional Class](#10)\n* [11. TensorBoard on Kaggle](#11)"}}