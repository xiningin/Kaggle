{"cell_type":{"dc6102c4":"code","494234f5":"code","a7b4f857":"code","020def75":"code","21a1dfa0":"code","02b8fa92":"code","30ca4135":"code","2919277c":"code","47ff03a6":"code","9f8d8e24":"code","f8d935bc":"code","af6ae1bf":"code","81028640":"code","c4ae4f46":"code","c1351477":"code","05ef7936":"code","94e73c8f":"code","1742110d":"code","1689a5b6":"code","984d5bf4":"code","59dae629":"code","f9e996ed":"code","587efe9a":"code","d368c8ab":"code","aecfd986":"code","6617a5ea":"markdown","779bf1a7":"markdown","66901e08":"markdown","975d1356":"markdown","811f29d2":"markdown","d7340c35":"markdown","01d18a2e":"markdown","fc14d74e":"markdown","d97920f9":"markdown","e9902de7":"markdown","d02d4a8f":"markdown","d2f3333b":"markdown","a8addd19":"markdown","2daf9384":"markdown","7c1e1857":"markdown","a224efe9":"markdown","593dbb85":"markdown","c2b2ffdd":"markdown","6723c4a0":"markdown","c109804e":"markdown","13206e86":"markdown","40063449":"markdown","5aec5d84":"markdown","e0e0af1c":"markdown","4fd56dbf":"markdown","ceb5b00c":"markdown","5d0cad20":"markdown","58d58d32":"markdown","767da22e":"markdown","5832ec01":"markdown","f74b3711":"markdown","37c251e5":"markdown","7ae0b013":"markdown"},"source":{"dc6102c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","494234f5":"data = pd.read_csv(\"\/kaggle\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv\")","a7b4f857":"data.head()","020def75":"data.columns","21a1dfa0":"data.info()","02b8fa92":"data.describe()","30ca4135":"data.corr()","2919277c":"data[\"Gender\"].value_counts()","47ff03a6":"sns.countplot(data[\"Gender\"], palette=\"bone\")\nplt.show()","9f8d8e24":"plt.figure(figsize=(10,6))\nsns.distplot(data[\"Annual Income (k$)\"])\nplt.title(\"Distribution of Annual Income\")\nplt.xlabel(\"Annual Income (k$)\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nsns.distplot(data[\"Age\"])\nplt.title(\"Distribution of Age\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Frequency\")\nplt.show()\n\nplt.figure(figsize=(10,6))\nsns.distplot(data[\"Spending Score (1-100)\"])\nplt.title(\"Distribution of Spending Score (1-100)\")\nplt.xlabel(\"Spending Score (1-100)\")\nplt.ylabel(\"Frequency\")\nplt.show()","f8d935bc":"sns.pairplot(data.drop('CustomerID', axis=1), hue='Gender', aspect=1.5)\nplt.show()","af6ae1bf":"from sklearn.cluster import KMeans\nx_data1 = data[['Annual Income (k$)' , 'Spending Score (1-100)']].iloc[: , :].values \nwcss = []\nfor k in range(1,15):\n    kmean = KMeans(n_clusters=k, random_state=0)\n    kmean.fit(x_data1)\n    wcss.append(kmean.inertia_)\n    \nplt.plot(range(1,15), wcss)\nplt.xlabel(\"Number of k (cluster) values\")\nplt.ylabel(\"Inertia - WCSS\")\nplt.title(\"Finding Optimum Number of K\")\nplt.show()","81028640":"kmean_x_data1 = KMeans(n_clusters=5, random_state=0)\nclusters = kmean_x_data1.fit_predict(x_data1)\ndata[\"Label1\"] = clusters","c4ae4f46":"data.head(2)","c1351477":"plt.figure(figsize=(15,8))\nplt.scatter(x_data1[clusters == 0,0 ],x_data1[clusters == 0,1 ],color = \"green\", label = \"High income-Low Spanding\")\nplt.scatter(x_data1[clusters == 1,0 ],x_data1[clusters == 1,1 ],color = \"red\", label = \"Middle income-Middle Spandingg\")\nplt.scatter(x_data1[clusters == 2,0 ],x_data1[clusters == 2,1 ],color = \"purple\", label = \"High income-High Spanding\")\nplt.scatter(x_data1[clusters == 3,0 ],x_data1[clusters == 3,1 ],color = \"cyan\", label = \"Low income-High Spanding\")\nplt.scatter(x_data1[clusters == 4,0 ],x_data1[clusters == 4,1 ],color = \"orange\", label = \"Low income-Low Spanding\")\nplt.scatter(kmean_x_data1.cluster_centers_[:,0], kmean_x_data1.cluster_centers_[:,1], color=\"black\", label=\"Centroids\",s =75)#0,1 x and y axis\nplt.xlabel(\"Annual Income k$\")\nplt.ylabel(\"Spending Score\")\nplt.legend()\nplt.title(\"Segmentation According to Income and Spending Score\")\nplt.show()","05ef7936":"data[(data[\"Label1\"]<3) & (data[\"Label1\"]>1)].groupby(\"CustomerID\").head()","94e73c8f":"from sklearn.cluster import KMeans\nx_data2 = data[['Age', 'Spending Score (1-100)']].iloc[: , :].values \nwcss2 = []\nfor k in range(1,15):\n    kmean2 = KMeans(n_clusters=k, random_state=0)\n    kmean2.fit(x_data2)\n    wcss2.append(kmean2.inertia_)\n    \nplt.plot(range(1,15), wcss2)\nplt.xlabel(\"Number of k (cluster) values\")\nplt.ylabel(\"Inertia - WCSS\")\nplt.title(\"Finding Optimum Number of K\")\nplt.show()","1742110d":"kmean_x_data2 = KMeans(n_clusters=4, random_state=0)\nclusters2 = kmean_x_data2.fit_predict(x_data2)\ndata[\"Label2\"] = clusters2","1689a5b6":"data.head(2)","984d5bf4":"plt.figure(figsize=(15,8))\nplt.scatter(x_data2[clusters2 == 1,0 ],x_data2[clusters2 == 1,1 ],color = \"red\", label = \"A group\")\nplt.scatter(x_data2[clusters2 == 2,0 ],x_data2[clusters2 == 2,1 ],color = \"purple\", label = \"B group\")\nplt.scatter(x_data2[clusters2 == 3,0 ],x_data2[clusters2 == 3,1 ],color = \"cyan\", label = \"C group\")\nplt.scatter(x_data2[clusters2 == 0,0 ],x_data2[clusters2 == 0,1 ],color = \"green\", label = \"D group\")\nplt.scatter(kmean_x_data2.cluster_centers_[:,0], kmean_x_data2.cluster_centers_[:,1], color=\"black\", label=\"Centroids\", s=75)\nplt.xlabel(\"Age\")\nplt.ylabel(\"Spending Score\")\nplt.legend()\nplt.title(\"Segmentation According to Age and Spending Score\")\nplt.show()","59dae629":"data[(data[\"Label1\"]<3) & (data[\"Label1\"]>1)].groupby(\"CustomerID\").head()","f9e996ed":"from scipy.cluster.hierarchy import linkage, dendrogram\nmerg = linkage(x_data1, method=\"ward\") # ward : mathematical expression\ndendrogram(merg, leaf_rotation=90)\n\nplt.xlabel(\"Data Points\")\nplt.ylabel(\"Euclidean Distance\")\nplt.title(\"Dendrogram\")\nplt.show()","587efe9a":"from sklearn.cluster import AgglomerativeClustering\nhc = AgglomerativeClustering(n_clusters=5 ,affinity=\"euclidean\", linkage=\"ward\")\ncluster3 = hc.fit_predict(x_data1)\ndata[\"Label3\"] = cluster3","d368c8ab":"data.head(2)","aecfd986":"plt.figure(figsize=(15,8))\nplt.scatter(x_data1[cluster3 == 0,0 ],x_data1[cluster3 == 0,1 ],color = \"green\", label = \"High income-Low Spanding\")\nplt.scatter(x_data1[cluster3 == 1,0 ],x_data1[cluster3 == 1,1 ],color = \"red\", label = \"Middle income-Middle Spandingg\")\nplt.scatter(x_data1[cluster3 == 2,0 ],x_data1[cluster3 == 2,1 ],color = \"purple\", label = \"High income-High Spanding\")\nplt.scatter(x_data1[cluster3 == 3,0 ],x_data1[cluster3 == 3,1 ],color = \"cyan\", label = \"Low income-High Spanding\")\nplt.scatter(x_data1[cluster3 == 4,0 ],x_data1[cluster3 == 4,1 ],color = \"orange\", label = \"Low income-Low Spanding\")\nplt.scatter(kmean_x_data1.cluster_centers_[:,0], kmean_x_data1.cluster_centers_[:,1], color=\"black\", label=\"Centroids\",s =75)#0,1 x and y axis\nplt.xlabel(\"Annual Income k$\")\nplt.ylabel(\"Spending Score\")\nplt.legend()\nplt.title(\"Segmentation According to Income and Spending Score\")\nplt.show()","6617a5ea":"#### Not much corrolations between columns.","779bf1a7":"#### First check how many male and female :","66901e08":"#### So, for the data which include income and spending score, the optimum k value is 5. Lets try","975d1356":" <a id = '6'><\/a><br>\n### Income and Spending Score","811f29d2":"#### So what is dendrogram:\n#### * We can use dendrogram for estimate how many clusters do we need.\n#### * Basic aproach is like that : \n#### * Find the longest vertical line that is not intersected by any line horizontally. Draw a threshold from there. The number of lines crossed by the boundary gives the number of clusters.","d7340c35":" <a id = '7'><\/a><br>\n### Age and Spending Score","01d18a2e":"#### According to dendrogram for this data (x_data1 : income and spending score) number of clusters should be 5 (As KMeans)","fc14d74e":" <a id = '3'><\/a><br>\n# Visualization","d97920f9":"# Content\n1. [Clustering and Customer Segmentation](#1)\n1. [Read Data](#2)\n1. [Visualization](#3)\n1. [Clustering](#4)\n    1. [KMeans Clustering](#5)\n        * [Income and Spending Score](#6)\n        * [Age and Spending Score](#7)\n    1. [Hierarchical Clustering](#8)\n        * [Dendrogram](#9)\n        * [Income and Spending Score](#10)\n1. [Conclusion](#11)","e9902de7":"#### After these, it is easy to extrapolate something from data. Specific treatments etc ... ","d02d4a8f":" <a id = '1'><\/a><br>\n# Clustering and Customer Segmentation","d2f3333b":" <a id = '10'><\/a><br>\n### Income and Spending Score","a8addd19":" <a id = '9'><\/a><br>\n### Dendrogram","2daf9384":"![image.png](attachment:48559491-5294-4565-805b-fe61d537fe3c.png)!","7c1e1857":"![kmeans3.png](attachment:edd1fd5a-2229-4e55-876f-d09d10850425.png)","a224efe9":"#### As you see, we clustered same datas (x_data1: income and spending score) with different clustering algorithms (KMeans and Hierarchial). At first sight, they are looking same or very familiar. ","593dbb85":"#### If you interested the people in purple group :","c2b2ffdd":" <a id = '8'><\/a><br>\n# Hierarchical Clustering","6723c4a0":"#### Basically, Hierarchical Clustering can be considered in 4 steps :\n#### 1. Cluster each data point\n#### 2. Make the two closest data points a cluster\n#### 3. Make two nearest clusters a cluster\n#### 4. Repeat 3rd step.","c109804e":"![Customer segmentation.png](attachment:b775012b-344a-473f-bf07-e5236b7418d3.png)","13206e86":" <a id = '5'><\/a><br>\n# K Means Clustering","40063449":"#### You may ask how the distance measured ?:\n#### With euclidian distance with different methods:\n#### 1. By the nearest two points\n#### 2. By the furthest two points\n#### 3. By the mean\n#### 4. By the centroid","5aec5d84":"![abcd.png](attachment:32f71c31-9875-442c-8641-2176b1d47f21.png)","e0e0af1c":"#### Basically, K-Means can be considered in 5 steps :\n#### 1. Choose k values (eg:3)\n#### 2. Centroids occur randomly (blank middle circles)\n#### 3. Group data points by distence from centroid. (Distances are measured with euclidian distance as default)\n#### 4. Avaraged over data points for each group and lines drawn. (This could be reply several times.)\n#### 5. Find new centroid location until centroids location do not change. (Each centroid approaches where the number of data point is higher for own segment)","4fd56dbf":" <a id = '11'><\/a><br>\n# Conclusion\n#### In this tutorial, we mentioned what clustering and segemntation is, than learned and coded KMeans & Hierarchial Clustering. ","ceb5b00c":"#### Distributions","5d0cad20":"#### * Generally, machine learning is splitted as supervised learning (regression & classification), unsupervised learning (clustering) and reinforcement learning.\n#### * If you a bit familiar with classification you may ask what is the difference ? (I asked :) \n#### * Supervised learning is learning through our eyes. Borders are created according to our requests. Man-Woman ? or Cat-Dog ? ...\n#### * In unsupervised learning, the computer interprets the data itself (no labels) and defines its limits freely. No human eyes.\n#### * It is generally using in customer segmentation, market segmentation, health and image proccesing.\n#### * So what is customer segmentation:\n#### * Which product to which customer ?, which product to specific customer ? (Collabration Filtering) \n#### * Know customers or create demagogical classes from historical data, show the same advertisements to the same class instead of showing them individually.\n#### * For example : High income, middle income, low income. Those in the same segment are likely to show the same behaivor.\n#### * If subway riders exhibit bag-buying behavior then place bag advertisements in subway stations.\n#### * Basically, clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group and dissimilar to the data points in other groups.\n#### * Logically, our aim should be keeping the distance of data within the same cluster to a minimum and the distance between clusters to a maximum.\n","58d58d32":"#### Lastly, when label1 == 2, label2 == 1 (1 : red) (except for 1 data) ","767da22e":"![image.png](attachment:124034f3-8ac1-4422-998e-c375dfe78c50.png)","5832ec01":"![image.png](attachment:11e71887-1b54-4501-a7ed-0d493a17615a.png)","f74b3711":" <a id = '4'><\/a><br>\n# Clustering","37c251e5":"#### There are some concetpts here that need to be explained here like wcss, inertia and elbow rule:\n#### 1. What is wcss : Within Cluster Sum of Square --> This is a mathematical method \n#### 2. Inertia : Inertia measures how well a dataset was clustered by K-Means. It is calculated by measuring the distance between each data point and its centroid, squaring this distance, and summing these squares across one cluster. A good model is one with low inertia and a low number of clusters ( K ). In other words inertia measuring by wcss.\n#### 3. Elbow point : The theory says that the number where breaking point occur is optimum number of k. ","7ae0b013":" <a id = '2'><\/a><br>\n# Read Data"}}