{"cell_type":{"ddd8b7f9":"code","5455749e":"code","728cdca6":"code","6226f9de":"code","6033e527":"code","457c6f77":"code","04976e44":"code","6fe4329e":"code","8ab33e36":"code","ad1bc27c":"code","263dbc1f":"code","59c716a1":"code","2b30f107":"code","ab7b3490":"code","d0e3d087":"code","fdd983c6":"code","2bcbb07c":"code","17937930":"code","ff419211":"code","84690a97":"code","c6ed2534":"code","f3981c28":"code","2e4303ac":"code","df7c59c1":"code","fdccf704":"markdown","9a683c0d":"markdown","ea0e063c":"markdown","043d045f":"markdown","5a690fae":"markdown","e3167df4":"markdown","67a52c2c":"markdown","6e7bd487":"markdown","e4aaf608":"markdown","97593d18":"markdown","f885ddbf":"markdown","763d9c9f":"markdown","95b5edef":"markdown","83dc3d1b":"markdown","ac987646":"markdown"},"source":{"ddd8b7f9":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nfrom keras.preprocessing.text import text_to_word_sequence\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, LSTM, Conv1D, MaxPool1D, Dropout\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import EarlyStopping","5455749e":"train = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip', sep = '\\t')\ntest = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/test.tsv.zip', sep = '\\t')","728cdca6":"print(train.shape, test.shape)","6226f9de":"train.head()","6033e527":"test.head()","457c6f77":"train.info()","04976e44":"test.info()","6fe4329e":"train.drop(['PhraseId','SentenceId'], inplace = True, axis = 'columns')\n\nsubmission = pd.DataFrame()\nsubmission['PhraseId'] = test['PhraseId']\ntest.drop(['PhraseId','SentenceId'], inplace = True, axis = 'columns')","8ab33e36":"train.head()","ad1bc27c":"test.head()","263dbc1f":"num_classes = len(train['Sentiment'].unique())","59c716a1":"for i in range(len(train['Phrase'])):\n    train['Phrase'][i] = text_to_word_sequence(train['Phrase'][i])","2b30f107":"for i in range(len(test['Phrase'])):\n    test['Phrase'][i] = text_to_word_sequence(test['Phrase'][i])","ab7b3490":"tokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(train['Phrase'])\ntrain['Phrase'] = tokenizer.texts_to_sequences(train['Phrase'])\n\ntokenizer = Tokenizer(num_words=5000)\ntokenizer.fit_on_texts(test['Phrase'])\ntest['Phrase'] = tokenizer.texts_to_sequences(test['Phrase'])","d0e3d087":"max_length = 100\n\ntrain_copy = train['Phrase']\ntrain_copy = pad_sequences(train['Phrase'],maxlen = max_length)\n\ntest_copy = test['Phrase']\ntest_copy = pad_sequences(test['Phrase'],maxlen = max_length)\n\nvocab_size = len(tokenizer.word_index) + 1","fdd983c6":"X = train_copy\ny = pd.get_dummies(train['Sentiment'])","2bcbb07c":"X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.3)","17937930":"# Model constants\nembedding_vector_length = 32\n\n# RNN Model\nmodel = Sequential()\n\n# Embedding layer\nmodel.add(Embedding(input_dim=vocab_size, \n                    output_dim=embedding_vector_length, \n                    input_length=max_length))\n\n# Convolutional layer(1D)\nmodel.add(Conv1D(filters = 16,\n                 kernel_size = 3,\n                 padding = 'same',\n                 activation = 'relu'))\n\n# MaxPool(1D) - Reduce to half\nmodel.add(MaxPool1D(pool_size = 2))\n\n# LSTM layers\nmodel.add(LSTM(32, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = True))\nmodel.add(LSTM(16, dropout = 0.2, recurrent_dropout = 0.2, return_sequences = False))\n\n# Dense layers\nmodel.add(Dense(32, activation = 'relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(num_classes, activation = 'softmax'))\n\nmodel.compile(loss = 'categorical_crossentropy',\n              optimizer = 'adam',\n              metrics = ['accuracy'])\n\nmodel.summary()","ff419211":"early_stopping = EarlyStopping(min_delta = 0.001,\n                               mode = 'max',\n                               monitor = 'val_acc',\n                               patience = 2)\ncallback = [early_stopping]","84690a97":"train_history = model.fit(x = X_train,\n                          y = y_train,\n                          batch_size = 1024,\n                          epochs = 20,\n                          verbose = 1,\n                          validation_data = (X_val, y_val),\n                          callbacks = callback)","c6ed2534":"plt.plot(train_history.history['accuracy'], label='Training accuracy')\nplt.plot(train_history.history['val_accuracy'], label='Validation accuracy')\nplt.legend()","f3981c28":"prediction = model.predict(test_copy)\nfinal_prediction = [np.argmax(i) for i in prediction]","2e4303ac":"submission['Sentiment'] = final_prediction\nsubmission.head()","df7c59c1":"submission.to_csv('..\/working\/submission.csv', index=False)","fdccf704":"### Predicting and submitting","9a683c0d":"### Splitting data for training and testing","ea0e063c":"### Final look at the data","043d045f":"### Visualizing data","5a690fae":"### Preparing data","e3167df4":"### Importing required packages","67a52c2c":"### Tokenizing sentences to words","6e7bd487":"### Plotting the accuracy","e4aaf608":"### Model","97593d18":"### Total classes of sentiments","f885ddbf":"### Loading data","763d9c9f":"### Converting tokenized words to numeric form","95b5edef":"### Dealing with the padding","83dc3d1b":"### Drop unnecessary columns","ac987646":"### Fitting model"}}