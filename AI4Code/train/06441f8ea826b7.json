{"cell_type":{"521313e6":"code","19c13c37":"code","0a784c53":"code","5994f9fe":"code","eb725f87":"code","0cdf7fb7":"code","d5a152b2":"code","961bf05a":"code","f24e3b01":"code","9884da77":"code","79bf1019":"code","ac83c139":"code","c12a8370":"code","0dd562aa":"code","0f6bad57":"code","2889fa36":"code","3bcff88f":"code","dcc9321f":"code","828933db":"code","63ceb78d":"code","18fd0d58":"code","4285ac2d":"code","bef3fbe6":"code","93e32bec":"code","c5b4e0bd":"code","e74cddf0":"code","27ae5853":"code","8105382a":"code","46e33c9e":"code","92eafc38":"code","44c20536":"code","f1df5339":"code","6be8cd44":"code","94233eca":"code","1481ff16":"code","befcacc4":"code","82fe331d":"code","bc20b31f":"code","a8a38ac1":"code","44b7429d":"code","cc2e7a5c":"code","6d36ccaa":"code","e9203ad2":"code","9e7e2a64":"code","cc9f6d98":"code","c5005b6f":"code","c707bb62":"code","281bf46c":"code","570c5b9a":"code","01ab875a":"code","70cc61ab":"code","b473c575":"code","692f1b79":"code","40f1763b":"code","bfdf76b7":"code","45321e09":"code","51965681":"code","3efcfa44":"code","b9262124":"code","162e9ab8":"code","938e01a0":"code","4f632e40":"code","428b1373":"code","1109d2ef":"code","b7c5d4a2":"code","83b4b604":"code","21d348ce":"code","8e116def":"code","8745c645":"code","fe28d766":"code","78ba2691":"code","e7950a0e":"code","36d3ed12":"code","40474f49":"code","cf188b36":"code","687d379f":"code","32a4e46a":"code","481b3c9b":"code","0c987cd5":"markdown","41c6878b":"markdown","f5cd4028":"markdown","4198fed5":"markdown","faa2f4eb":"markdown","cdd272a3":"markdown","63a83209":"markdown","1aa8c710":"markdown","6aeca3e7":"markdown","96a5a59d":"markdown","643e025c":"markdown","674e02e5":"markdown","b8dc1fc4":"markdown","e41f56ea":"markdown","3a470f9f":"markdown","e52faeb2":"markdown","a73828bc":"markdown","7452d844":"markdown","19babdee":"markdown","a5bdf1d1":"markdown","23984541":"markdown","42c27198":"markdown","085cbe85":"markdown","6633585d":"markdown","4d4c24b3":"markdown","197eecde":"markdown","dd144ee0":"markdown","fa0f39e1":"markdown","db1f1c28":"markdown","7163aa45":"markdown","add9430f":"markdown","8a66670c":"markdown","ca6468c6":"markdown","da8dcaad":"markdown","a87cfaab":"markdown","6148ec19":"markdown","d3847e63":"markdown","e778e073":"markdown","0358683e":"markdown","c4faa716":"markdown","db5e34d4":"markdown","425cd9c6":"markdown","36437f88":"markdown","ea225d30":"markdown","7fb6682e":"markdown","a401ea13":"markdown","744cbb5e":"markdown","d7c75ba1":"markdown","5d67959c":"markdown","98e293f7":"markdown","ca673de6":"markdown","9d61fddb":"markdown","cd0c8e42":"markdown","0dc88fa8":"markdown","0153ef37":"markdown","e3466426":"markdown"},"source":{"521313e6":"import warnings\nwarnings.filterwarnings('ignore')\nimport warnings\nwarnings.filterwarnings(\"ignore\", module=\"lightgbm\")\n\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom lightgbm import LGBMRegressor\nimport matplotlib.pyplot as plt\nimport datetime \nimport lightgbm as lgbm\n\npd.set_option('display.max_colwidth',None)","19c13c37":"items = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/items.csv')\nitem_cat = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nshops = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/shops.csv')\n\ntrain = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv')\ntest_dataset = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/test.csv')","0a784c53":"train.head()","5994f9fe":"train.shape","eb725f87":"train.isnull().sum()","0cdf7fb7":"train_dataset = train.copy()","d5a152b2":"train_dataset","961bf05a":"train_dataset[train_dataset['item_cnt_day'] == 2169.0]","f24e3b01":"monthly_sales=train_dataset.groupby([\"date_block_num\",\"shop_id\",\"item_id\"])[\n    \"date\",\"item_price\",\"item_cnt_day\"].agg({\"date\":[\"min\",'max'],\"item_price\":\"mean\",\"item_cnt_day\":\"sum\"})","9884da77":"monthly_sales","79bf1019":"monthly_sales.columns","ac83c139":"sales_by_month = train_dataset.groupby(['date_block_num'])['item_cnt_day'].sum()\nsales_by_month.plot()","c12a8370":"corr = train_dataset.corr()\nfig = plt.figure()\nax = fig.add_subplot(111)\ncax = ax.matshow(corr,cmap='coolwarm', vmin=-1, vmax=1)\nfig.colorbar(cax)\nticks = np.arange(0,len(train_dataset.columns),1)\nax.set_xticks(ticks)\nplt.xticks(rotation=90)\nax.set_yticks(ticks)\nax.set_xticklabels(train_dataset.columns)\nax.set_yticklabels(train_dataset.columns)\nplt.show()","0dd562aa":"items.head()","0f6bad57":"plt.rcParams['figure.figsize'] = (24, 9)\nsns.barplot(items['item_category_id'], items['item_id'], palette = 'colorblind')\nplt.title('Number of Item Sold Per Category', fontsize = 30)\nplt.xlabel('Item Categories', fontsize = 15)\nplt.ylabel('Items', fontsize = 15)\nplt.show()","2889fa36":"plt.rcParams['figure.figsize'] = (24, 9)\nsns.countplot(train_dataset['date_block_num'], palette = 'colorblind')\nplt.title('Number of Item Sold Per Month Over 2013 - 2015', fontsize = 30)\nplt.xlabel('Month', fontsize = 15)\nplt.ylabel('Items Count', fontsize = 15)\nplt.show()","3bcff88f":"# item_cat['item_category_name'].count()\nprint(item_cat['item_category_name'].nunique())\nprint(shops['shop_name'].nunique())","dcc9321f":"from wordcloud import WordCloud\nfrom wordcloud import STOPWORDS\n\nplt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'pink',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(shops['shop_name']))\n\n\nplt.title('Wordcloud for Shop Names', fontsize = 25)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","828933db":"plt.rcParams['figure.figsize'] = (15, 12)\nstopwords = set(STOPWORDS)\nwordcloud = WordCloud(background_color = 'lightyellow',\n                      max_words = 200, \n                      stopwords = stopwords,\n                     width = 1200,\n                     height = 800,\n                     random_state = 42).generate(str(item_cat['item_category_name']))\n\n\nplt.title('Wordcloud for Item Category Names', fontsize = 24)\nplt.axis('off')\nplt.imshow(wordcloud, interpolation = 'bilinear')","63ceb78d":"train_dataset['date'] = pd.to_datetime(train_dataset['date'], errors='coerce')","18fd0d58":"days = []\nmonths = []\nyears = []\n\nfor day in train_dataset['date']:\n    days.append(day.day)\nfor month in train_dataset['date']:\n    months.append(month.month)    \nfor year in train_dataset['date']:\n    years.append(year.year)","4285ac2d":"plt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(days, palette= 'pastel')\nplt.title('The busiest days for the shops', fontsize = 24)\nplt.xlabel('Days', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()","bef3fbe6":"# busy month\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(months, palette= 'rocket')\nplt.title('The busiest months for the shops', fontsize = 24)\nplt.xlabel('Months', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()\n\n# busy year\nplt.rcParams['figure.figsize'] = (15, 7)\nsns.countplot(years, palette= 'cubehelix')\nplt.title('The busiest years for the shops', fontsize = 24)\nplt.xlabel('Years', fontsize = 12)\nplt.ylabel('Frequency', fontsize = 12)\n\nplt.show()","93e32bec":"train_dataset['day'] = days\ntrain_dataset['month'] = months\ntrain_dataset['year'] = years","c5b4e0bd":"train_dataset","e74cddf0":"sns.countplot(train_dataset[(train_dataset.month == 2) & (train_dataset.year == 2013)]['shop_id'], palette='pastel')","27ae5853":"train_dataset.describe()","8105382a":"plt.figure(figsize=(10,4))\nplt.xlim(train_dataset.item_price.min(), train_dataset.item_price.max()*1.1)\nsns.boxplot(x=train_dataset.item_price)","46e33c9e":"plt.figure(figsize=(10,4))\nplt.xlim(train_dataset.item_cnt_day.min(), train_dataset.item_cnt_day.max()*1.1)\nsns.boxplot(x=train_dataset.item_cnt_day)","92eafc38":"train_dataset = train_dataset[(train_dataset[\"item_price\"] > 0) & (train_dataset[\"item_price\"] < 50000)]\ntrain_dataset = train_dataset[(train_dataset[\"item_cnt_day\"] > 0) & (train_dataset[\"item_cnt_day\"] < 1000)]","44c20536":"train_dataset.shape","f1df5339":"train_dataset[train_dataset['item_price'] < 0]","6be8cd44":"median = train_dataset[(train_dataset.shop_id==32)&(train_dataset.item_id==2973)&(train_dataset.date_block_num==4)&(train_dataset.item_price>0)].item_price.median()\nmedian","94233eca":"train_dataset[\"item_price\"] = train_dataset[\"item_price\"].map(lambda x: median if x<0 else x)","1481ff16":"train_dataset[train_dataset['item_price'] < 0]","befcacc4":"train_dataset[train_dataset['item_cnt_day'] < 0]","82fe331d":"train_dataset[\"item_cnt_day\"] = train_dataset[\"item_cnt_day\"].map(lambda x: 0 if x<0 else x)","bc20b31f":"train_dataset[train_dataset['item_cnt_day'] < 0]","a8a38ac1":"train_dataset.head(2)","44b7429d":"print(\"total unique items: \", items['item_id'].nunique())\nprint(\"total unique items in train dataset: \", train_dataset['item_id'].nunique())\nprint(\"total unique items in test dataset: \", test_dataset['item_id'].nunique())\n\nprint(\"total unique shops: \", shops['shop_id'].nunique())\nprint(\"total unique shops in train dataset: \", train_dataset['shop_id'].nunique())\nprint(\"total unique shops in test dataset: \", test_dataset['shop_id'].nunique())","cc2e7a5c":"test_item_list = [x for x in (np.unique(test_dataset['item_id']))]\ntrain_item_list = [x for x in (np.unique(train_dataset['item_id']))]\n\nmissing_item_ids_ = [element for element in test_item_list if element not in train_item_list]\nlen(missing_item_ids_)","6d36ccaa":"shops","e9203ad2":"# getting rid of \"!\" before shop_names\nshops['shop_name'] = shops['shop_name'].map(lambda x: x.split('!')[1] if x.startswith('!') else x)\nshops['shop_name'] = shops[\"shop_name\"].map(lambda x: '\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"' if x == '\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"' else x)","9e7e2a64":"shops['city'] = shops['shop_name'].map(lambda x: x.split(\" \")[0])\n# lets assign code to these city names too\nshops['city_code'] = shops['city'].factorize()[0]","cc9f6d98":"shops.head(2)","c5005b6f":"for shop_id in shops['shop_id'].unique():\n    shops.loc[shop_id, 'num_products'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_id'].nunique()\n    shops.loc[shop_id, 'min_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].min()\n    shops.loc[shop_id, 'max_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].max()\n    shops.loc[shop_id, 'mean_price'] = train_dataset[train_dataset['shop_id'] == shop_id]['item_price'].mean()","c707bb62":"shops.head(2)","281bf46c":"item_cat","570c5b9a":"cat_list = []\nfor name in item_cat['item_category_name']:\n    cat_list.append(name.split('-'))","01ab875a":"item_cat['split'] = (cat_list)\nitem_cat['cat_type'] = item_cat['split'].map(lambda x: x[0])\nitem_cat['cat_type_code'] = item_cat['cat_type'].factorize()[0]\nitem_cat['sub_cat_type'] = item_cat['split'].map(lambda x: x[1] if len(x)>1 else x[0])\nitem_cat['sub_cat_type_code'] = item_cat['sub_cat_type'].factorize()[0]","70cc61ab":"item_cat.head(2)","b473c575":"item_cat.drop('split', axis = 1, inplace=True)\nitem_cat.head(2)","692f1b79":"train_dataset = train_dataset[train_dataset[\"item_cnt_day\"]>0]\ntrain_dataset = train_dataset[[\"month\", \"date_block_num\", \"shop_id\", \"item_id\", \"item_price\", \"item_cnt_day\"]].groupby(\n    [\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n    {\"item_price\": \"mean\",\"item_cnt_day\": \"sum\", \"month\": \"min\"}).reset_index()\ntrain_dataset.rename(columns={\"item_cnt_day\": \"item_cnt_month\"}, inplace=True)\ntrain_dataset = pd.merge(train_dataset, items, on=\"item_id\", how=\"inner\")\ntrain_dataset = pd.merge(train_dataset, shops, on=\"shop_id\", how=\"inner\")\ntrain_dataset = pd.merge(train_dataset, item_cat, on=\"item_category_id\", how=\"inner\")","40f1763b":"train_dataset.head(2)","bfdf76b7":"train_dataset.drop(['item_name', 'shop_name', 'city', 'item_category_name', 'cat_type', 'sub_cat_type'], axis = 1, inplace=True)","45321e09":"train_dataset.head(1)","51965681":"test_dataset.head()","3efcfa44":"test_dataset.shape","b9262124":"train_dataset.shape","162e9ab8":"train_dataset = train_dataset[train_dataset['shop_id'].isin(test_dataset['shop_id'].unique())]\ntrain_dataset = train_dataset[train_dataset['item_id'].isin(test_dataset['item_id'].unique())]","938e01a0":"train_dataset.shape","4f632e40":"train_dataset.head(2)","428b1373":"final_train_dataset = train_dataset.copy()\nfinal_test_dataset = test_dataset.copy()","1109d2ef":"def data_preprocess(sales_train, test=None):\n    indexlist = []\n    for i in sales_train.date_block_num.unique():\n        x = itertools.product(\n            [i],\n            sales_train.loc[sales_train.date_block_num == i].shop_id.unique(),\n            sales_train.loc[sales_train.date_block_num == i].item_id.unique(),\n        )\n        indexlist.append(np.array(list(x)))\n    df = pd.DataFrame(\n        data=np.concatenate(indexlist, axis=0),\n        columns=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    # Adding new revenue column\n    sales_train[\"item_revenue_day\"] = sales_train[\"item_price\"] * sales_train[\"item_cnt_month\"]\n    # Aggregate item_id \/ shop_id item_cnts and revenue at the month level\n    sales_train_grouped = sales_train.groupby([\"date_block_num\", \"shop_id\", \"item_id\"]).agg(\n        item_cnt_month=pd.NamedAgg(column=\"item_cnt_month\", aggfunc=\"sum\"),\n        item_revenue_month=pd.NamedAgg(column=\"item_revenue_day\", aggfunc=\"sum\"),\n    )\n    #print(sales_train_grouped)\n    # Merge the grouped data with the index\n    df = df.merge(\n        sales_train_grouped, how=\"left\", on=[\"date_block_num\", \"shop_id\", \"item_id\"],\n    )\n\n    if test is not None:\n        test[\"date_block_num\"] = 34\n        test[\"date_block_num\"] = test[\"date_block_num\"].astype(np.int8)\n        test[\"shop_id\"] = test.shop_id.astype(np.int8)\n        test[\"item_id\"] = test.item_id.astype(np.int16)\n        test = test.drop(columns=\"ID\")\n\n        df = pd.concat([df, test[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n\n    # Fill empty item_cnt entries with 0\n    df.item_cnt_month = df.item_cnt_month.fillna(0)\n    df.item_revenue_month = df.item_revenue_month.fillna(0)\n\n    return df\n\ndataset_final = data_preprocess(final_train_dataset, final_test_dataset)","b7c5d4a2":"dataset_final = pd.merge(dataset_final, items, on=\"item_id\", how=\"inner\")\ndataset_final = pd.merge(dataset_final, shops, on=\"shop_id\", how=\"inner\")\ndataset_final = pd.merge(dataset_final, item_cat, on=\"item_category_id\", how=\"inner\")\ndataset_final.head(3)","83b4b604":"dataset_final.drop(['item_name', 'shop_name', 'city', 'item_category_name', 'cat_type', 'sub_cat_type'], axis = 1, inplace=True)\ndataset_final.head(2)","21d348ce":"dataset_final.shape","8e116def":"def lag_feature(matrix, lag_feature, lags):\n    for lag in lags:\n        newname = lag_feature + f\"_lag_{lag}\"\n        print(f\"Adding feature {newname}\")\n        targetseries = matrix.loc[:, [\"date_block_num\", \"item_id\", \"shop_id\"] + [lag_feature]]\n        targetseries[\"date_block_num\"] += lag\n        targetseries = targetseries.rename(columns={lag_feature: newname})\n        matrix = matrix.merge(\n            targetseries, on=[\"date_block_num\", \"item_id\", \"shop_id\"], how=\"left\"\n        )\n#     print(matrix)\n    return matrix\n\ndataset_final = lag_feature(dataset_final, 'item_cnt_month', lags=[1,2,3])\ndataset_final = lag_feature(dataset_final, 'item_revenue_month', lags=[1])\nprint(\"Lag features created..\")\nprint(dataset_final.columns)","8745c645":"mean = dataset_final.groupby(['date_block_num']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_item_cnt']\nmean = mean.reset_index()\ndataset_final = pd.merge(dataset_final, mean, on=['date_block_num'], how='left')\ndel(mean)\n#Lagging\ndataset_final = lag_feature(dataset_final, \"avg_by_month_item_cnt\", [1,2,3,6])\ndataset_final.drop(columns = ['avg_by_month_item_cnt'], axis = 1, inplace = True)\nprint(\"_____________________________________________________________________________\")\n\n#Let's make a mean by month \/ id\nmean = dataset_final.groupby(['date_block_num', 'item_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_item_id_item_cnt']\nmean = mean.reset_index()\ndataset_final = pd.merge(dataset_final, mean, on=['date_block_num', 'item_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset_final = lag_feature( dataset_final, \"avg_by_month_item_id_item_cnt\" ,[1,2,3] )\ndataset_final.drop(columns = ['avg_by_month_item_id_item_cnt'], axis = 1, inplace = True)\nprint(\"_____________________________________________________________________________\")\n\n#Now a mean by month \/ shop\nmean = dataset_final.groupby(['date_block_num', 'shop_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_shop_item_cnt']\nmean = mean.reset_index()\ndataset_final = pd.merge(dataset_final, mean, on=['date_block_num', 'shop_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset_final = lag_feature( dataset_final, \"avg_by_month_shop_item_cnt\", [1,2,3])\ndataset_final.drop(columns = ['avg_by_month_shop_item_cnt'], axis = 1, inplace = True)\nprint(\"_____________________________________________________________________________\")\n\n#Now a mean by month \/ city\nmean = dataset_final.groupby(['date_block_num', 'city_code']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_city_item_cnt']\nmean = mean.reset_index()\ndataset_final = pd.merge(dataset_final, mean, on=['date_block_num', 'city_code'], how='left')\ndel(mean)\n\n#Lagging\ndataset_final = lag_feature( dataset_final, \"avg_by_month_city_item_cnt\", [1])\ndataset_final.drop(columns = ['avg_by_month_city_item_cnt'], axis = 1, inplace = True)\nprint(\"_____________________________________________________________________________\")\n\n#Now a mean by month \/ category\nmean = dataset_final.groupby(['date_block_num', 'item_category_id']).agg({'item_cnt_month': ['mean']})\nmean.columns = ['avg_by_month_cat_item_cnt']\nmean = mean.reset_index()\ndataset_final = pd.merge(dataset_final, mean, on=['date_block_num', 'item_category_id'], how='left')\ndel(mean)\n\n#Lagging\ndataset_final = lag_feature( dataset_final, \"avg_by_month_cat_item_cnt\" ,[1])\ndataset_final.drop(columns = ['avg_by_month_cat_item_cnt'], axis = 1, inplace = True)\nprint(\"_____________________________________________________________________________\")","fe28d766":"dataset_final.fillna(0, inplace= True)\ndataset_final.head(2)","78ba2691":"matrix = dataset_final[dataset_final.date_block_num>=12] \nmatrix.reset_index(drop=True, inplace=True)","e7950a0e":"matrix.head(2)","36d3ed12":"matrix.columns","40474f49":"# # final_train_df = train_dataset[['date_block_num','item_id','shop_id','item_cnt_month']]\n# final_train_df = train_dataset.copy()\n# final_train_df = pd.concat([final_train_df, test_copy[[\"date_block_num\", \"shop_id\", \"item_id\"]]])\n# # final_train_df = final_train_df.pivot_table(index = ['shop_id','item_id'],values = ['item_cnt_month'],columns = ['date_block_num'],fill_value = 0,aggfunc='sum')\n# # final_train_df.reset_index(inplace = True)\n# final_train_df = final_train_df.pivot_table(index=['item_id','shop_id'], columns = 'date_block_num', values = 'item_cnt_month', fill_value = 0).reset_index()\n\n# # final_train_df = pd.merge(test_dataset,final_train_df,on = ['item_id','shop_id'],how = 'left')\n# # final_train_df.fillna(0,inplace = True)\n# final_train_df","cf188b36":"def fit_booster(\n    X_train,\n    y_train,\n    X_test=None,\n    y_test=None,\n    params=None,\n    test_run=False,\n    categoricals=[],\n    dropcols=[],\n    early_stopping=True,\n):\n    if params is None:\n        params = {\"learning_rate\": 0.1, \"subsample_for_bin\": 300000, \"n_estimators\": 50}\n\n    early_stopping_rounds = None\n    if early_stopping == True:\n        early_stopping_rounds = 50\n\n    if test_run:\n        eval_set = [(X_train, y_train)]\n    else:\n        eval_set = [(X_train, y_train), (X_test, y_test)]\n\n    booster = lgbm.LGBMRegressor(**params)\n\n    categoricals = [c for c in categoricals if c in X_train.columns]\n\n    booster.fit(\n        X_train,\n        y_train,\n        eval_set=eval_set,\n        eval_metric=[\"rmse\"],\n        verbose=100,\n        categorical_feature=categoricals,\n        early_stopping_rounds=early_stopping_rounds,\n    )\n\n    return booster\n\n\n\nkeep_from_month = 2  # The first couple of months are dropped because of distortions to their features (e.g. wrong item age)\ntest_month = 33\n# dropping this will reduce overfitting\ndropcols = [\n    \"shop_id\",\n    \"item_id\"\n] \nvalid = matrix.drop(columns=dropcols).loc[matrix.date_block_num == test_month, :]\ntrain__ = matrix.drop(columns=dropcols).loc[matrix.date_block_num < test_month, :]\ntrain__ = train__[train__.date_block_num >= keep_from_month]\nX_train = train__.drop(columns=\"item_cnt_month\")\ny_train = train__.item_cnt_month\nX_valid = valid.drop(columns=\"item_cnt_month\")\ny_valid = valid.item_cnt_month\n\n\n\nparams = {\n    \"num_leaves\": 966,\n    \"cat_smooth\": 45.01680827234465,\n    \"min_child_samples\": 27,\n    \"min_child_weight\": 0.021144950289224463,\n    \"max_bin\": 214,\n    \"learning_rate\": 0.01,\n    \"subsample_for_bin\": 300000,\n    \"min_data_in_bin\": 7,\n    \"colsample_bytree\": 0.8,\n    \"subsample\": 0.6,\n    \"subsample_freq\": 5,\n    \"n_estimators\": 8000,\n}\n\ncategoricals = [\n    \"item_category_id\",\n    \"month\",\n]  # These features will be set as categorical features by LightGBM and handled differently\n\nlgbooster = fit_booster(\n    X_train,\n    y_train,\n    X_valid,\n    y_valid,\n    params=params,\n    test_run=False,\n    categoricals=categoricals,\n)","687d379f":"matrix['item_cnt_month'] = matrix['item_cnt_month'].clip(0,20)\nkeep_from_month = 2\ntest_month = 34\ntest__ = matrix.loc[matrix.date_block_num==test_month, :]\nX_test = test__.drop(columns=\"item_cnt_month\")\ny_test = test__.item_cnt_month\n\nX_test[\"item_cnt_month\"] = lgbooster.predict(X_test.drop(columns=dropcols)).clip(0, 20)","32a4e46a":"X_test[\"item_cnt_month\"] ","481b3c9b":"testing = test_dataset.merge(\n    X_test[[\"shop_id\", \"item_id\", \"item_cnt_month\"]],\n    on=[\"shop_id\", \"item_id\"],\n    how=\"inner\",\n    copy=True,\n)\n# Verify that the indices of the submission match the original\nassert test_dataset.equals(testing[[\"ID\", \"shop_id\", \"item_id\"]])\ntesting[[\"ID\", \"item_cnt_month\"]].to_csv(\".\/submission.csv\", index=False)","0c987cd5":"Since we want to predict the sale for Nov, 2015 i.e. `date_block_num = 34` so we will have to add that to our `test_dataset` to be able to make sale prediciton.","41c6878b":"we can see that in Feb of 2013 `shop_id` `31` has the highest number of sales","f5cd4028":"checking the number of unique snop names and item category names","4198fed5":"##### <a name=\"nulls\"><\/a>there are no nulls.","faa2f4eb":"# <a name=\"final\"><\/a>Preparing our final DF\n\nWe will also prepare our `train` and `test` datasets.\n\nNow we will split `train_dataset` into `train_set` and `validation_set`.\n","cdd272a3":"Lets drop the following columns since it may not be necessary while building models","63a83209":"### <a name=\"soldpcat\"><\/a>Checking how many items sold per category","1aa8c710":"extracting the city names","6aeca3e7":"No `item_price` less than 0 remaining","96a5a59d":"# <a name=\"model\"><\/a>Model Creation\n\n[Reference kernel](https:\/\/github.com\/angliu-bu\/Kaggle-Predict-Future-Sales\/blob\/main\/predict-future-sales.ipynb)","643e025c":"Also, from the diagram we can see that there is a price point that is less than zero. We will fill that price with median value.\n","674e02e5":"By looking at the plot above we can say that the sale is decreasing over months. However, some peaks are spotted during November.","b8dc1fc4":"\ud83e\udd17 \n\nAfter assigning the median value we can no longer find any record with negative pricing.","e41f56ea":"We can also see from the (2nd outlier)`item_cnt_day` diagram that there are some negative values. ","3a470f9f":"### Below we plot the outliers\n\n[this](https:\/\/www.kaggle.com\/homiarafarhana\/predict-future-sales#Exploratory-Data-Analysis) kernel has helped me understand this concept.","e52faeb2":"In this competition, we are given sale information spanning over the year 2013 to 2015. Our goal is to predict the future sales.\n\nWe have data from Jan 2013 to Oct 2015, We will predict the sale for Nov 2015 by analysing the given data.\n\nThe kernels that helped me and I took inpirations from:\n\n- [kernel 1](https:\/\/www.kaggle.com\/sanjayar\/step-by-step-guide-for-sales-data-prediction-lstm)\n- [kernel 2](https:\/\/github.com\/sharmaroshan\/Predict-Future-Sales\/blob\/master\/Predicting_Future_Sales.ipynb)\n- [kernel 3](https:\/\/www.kaggle.com\/homiarafarhana\/predict-future-sales)\n- [kernel 4](https:\/\/www.kaggle.com\/stefanschulmeister87\/extensive-eda-and-data-preparation)\n\n\nContents:\n\n- [Imports](#imports)\n    - [data description](#data-desc)\n- [EDA](#eda)\n    - [checking for nulls](#nulls)\n    - [monthly sale](#month)\n    - [correlation](#correlation)\n    - [item sold per category](#soldpcat)\n    - [item sold per month](#soldpmonth)\n    - [wordcloud](#wordcloud)\n    - [busiest days\/months\/years for the shops](#busy)\n    - [dealing with outliers](#outlier)\n- [Data Processing](#data)\n    - [Checking for missing columns](#miss)\n    - [processing shop data](#shopp)\n    - [processing item category data](#catdatap)\n- [Preparing final DF for modeling](#final)\n- [Model Creation](#model)\n- [Prediction](#prediction)\n- [Submission](#submission)","a73828bc":"First lets create `test` and `train` dataset copies","7452d844":"# <a name=\"eda\"><\/a>EDA | Exploratory Data Analysis\n\n- ID - an Id that represents a (Shop, Item) tuple within the test set\n- shop_id - unique identifier of a shop\n- item_id - unique identifier of a product\n- item_category_id - unique identifier of item category\n- item_cnt_day - number of products sold. You are predicting a monthly amount of this measure\n- item_price - current price of an item\n- date - date in format dd\/mm\/yyyy\n- date_block_num - a consecutive month number, used for convenience. January 2013 is 0, February 2013 is 1,..., October 2015 is 33\n- item_name - name of item\n- shop_name - name of shop\n- item_category_name - name of item category","19babdee":"shape reduced","a5bdf1d1":"Lets find out which `item_ids` are in test_set but not in train_set","23984541":"As shown above that not all the shop and item IDs from `test` are present in our `train_dataset` so lets only keep the IDs that are present in the `test_dataset`","42c27198":"### <a name=\"catdatap\"><\/a>Processing Item Category data\nThe Item category name is designed like below:\n\n\n- Item category name = type of the category + sub types\n\nfor example: an item category name **\u0421\u043b\u0443\u0436\u0435\u0431\u043d\u044b\u0435 - \u0411\u0438\u043b\u0435\u0442\u044b\t** is tranlated as **Service - Tickets**\n\nwhere the `type` of this category is **Service** and `subtype` is **Tickets** (what kind of service)..\n\nwe will now add these new features to our dataset","085cbe85":"### <a name=\"outlier\"><\/a>Outliers","6633585d":"from the description above, when we look at the max and min values we can see that there is an outlier for `item_price` and `item_cnt_day`","4d4c24b3":"no < 0 `item_cnt_day` value remaining","197eecde":"Note that the list of shops and products slightly changes every month.","dd144ee0":"# <a name=\"submission\"><\/a>Submission","fa0f39e1":"### <a name=\"correlation\"><\/a>checking for correlation\n\nthere are no noticeably strong pos\/neg correlation in sight ","db1f1c28":"creating a column`split` after `item_category_name` at '-'","7163aa45":"Adding some `mean` value calculation to the `DF`","add9430f":"### <a name=\"shopp\"><\/a>Processing shop data\n\nLets Look at all the Shops now. Every shop_name is designed like this:\nshop_name = city + kind of shop.\n\nWe attempt to extract the city feature out of the shop_name to add more diversity to our dataset.\n\nThe first two row shows that a city name is starting with '!', so we will get rid of the '!'.\n\nIndex 46 gives us a shop_name as **\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434 \u0422\u0426 \"7\u042f\"** \n\nIDK Russian but analyzing different kernels it seem like the city name is actually **\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434** not **\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434**, there is an extra \" \". So we will get rid of that too.","8a66670c":"##### <a name=\"miss\"><\/a>checking to see if all `shop_id` and `item_id` from `test dataset` is also present in the `train dataset`","ca6468c6":"lets take a look at our `test_dataset`","da8dcaad":"with `city_code` we are assigning a unique label to each `city`","a87cfaab":"wordcloud for item categories","6148ec19":"363 items are not found in `train_dataset` so predicting sales for these items is not easy since we do not have the prices for these items.","d3847e63":"lets add few more features to our shop dataset like below:\n\n``` \n\"num_products\"\n\"min_price\"\n\"max_price\"\n\"mean_price\"\n```","e778e073":"plotting the monthly sales","0358683e":"# <a name=\"prediction\"><\/a>Prediction","c4faa716":"we can see that item numbers in test and train sets are not equal. So making prediction for the missing items is going to be difficult.","db5e34d4":"-----------------------------------------------------------------------------------------------\n","425cd9c6":"Making a copy of the `train_dataset`, It is a good thing to so because we do not want to mess up the original content while exploring the data.","36437f88":"##### <a name=\"data-desc\"><\/a>Provided data description\n\n- sales_train.csv - the training set. Daily historical data from January 2013 to October 2015.\n- test.csv - the test set. You need to forecast the sales for these shops and products for November 2015.\n- sample_submission.csv - a sample submission file in the correct format.\n- items.csv - supplemental information about the items\/products.\n- item_categories.csv  - supplemental information about the items categories.\n- shops.csv- supplemental information about the shops.","ea225d30":"# <a name=\"datap\"><\/a>Data Preprocessing","7fb6682e":"Busiest months and years for shops","a401ea13":"converting the date into datetimelike format\n\ni.e. 01.02.2013    ==>    2013-02-01","744cbb5e":"By judging from the above  outlier diagram above we see a price point further than the other points. So we can get rid of that point.\n\nAlso for `item_cnt_day` there is a point further than other point we will get rid of that point too.\n\nThe demonstration has been shown below:\n","d7c75ba1":"`item_cnt_day` < 0 or -1 probably means that those items were returned. If the Items are returned then there are no sales involved as well. So, we can get rid of negative values and set it as 0.","5d67959c":"**2169** pieces of item ID **1173** were sold on **28\/10\/2015**","98e293f7":"# <a name=\"imports\"><\/a>Imports\n","ca673de6":"leaving behind the sale record of year 2013","9d61fddb":"Adding the lag feature for column names `item_cnt_month` and `item_revenue_month`","cd0c8e42":"#### <a name=\"wordcloud\"><\/a>WordCloud for shop name","0dc88fa8":"#### <a name=\"busy\"><\/a>Busiest days for the shop","0153ef37":"##### <a name=\"month\"><\/a>Monthly sales","e3466426":"### <a name=\"soldpmonth\"><\/a>Checking how many items sold per per month i.e. (jan 2013 ~ Oct 2015)"}}