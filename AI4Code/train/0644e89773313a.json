{"cell_type":{"3d4af12f":"code","1c39a39c":"code","5fbe6524":"code","4d654872":"code","ccd8934d":"code","bf3cbf7d":"code","ca9cf17f":"code","f518e738":"code","8118a238":"code","7e4ca296":"code","7fd52cd9":"code","3fd2d969":"code","0920f751":"code","3a376933":"code","380bc476":"code","2762eceb":"code","6f8f894f":"code","853ab902":"code","89fe9c7e":"code","499b6edc":"code","0fd7f187":"code","78c1d8a3":"code","16aba4fe":"code","a18f938d":"code","3d2e0ec4":"code","4a542153":"code","f6ec319f":"code","05cb1ddf":"code","634a015d":"code","16b38af5":"code","b3ca12a1":"code","b3ccea47":"code","1e562add":"code","ff4fded2":"code","9ff4b250":"code","e426f295":"code","f0a802db":"code","c40130fa":"markdown","9cd3d238":"markdown","b5cdc44d":"markdown","e97a0965":"markdown","2ed920b2":"markdown","462166a3":"markdown","57ca0b2f":"markdown","81b84d71":"markdown","be82a50f":"markdown","167d7d92":"markdown","f58342d4":"markdown","d99d81d0":"markdown"},"source":{"3d4af12f":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\nfrom nltk.corpus import stopwords\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import roc_auc_score\nimport warnings \nwarnings.filterwarnings('ignore')","1c39a39c":"df=pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv',encoding='latin-1')","5fbe6524":"df.head()","4d654872":"df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1,inplace=True)","ccd8934d":"df.head()","bf3cbf7d":"sns.countplot(df['v1'])","ca9cf17f":"# lets make a new column for the message length and observe if we can see some useful insights\n\nlength=[]\nfor message in df['v2']:\n    msg_lst=message.split(' ')\n    length.append(len(msg_lst))","f518e738":"df['Length']=length","8118a238":"df.head()","7e4ca296":"g=df.groupby('v1')\nfor name,df1 in g:\n    sns.histplot(df1['Length'])\n    plt.show()","7fd52cd9":"def preprocess(df1):\n    v2=[]\n    for message in df1['v2']:\n        # converting all characters to lowercase\n        lowered=message.lower()\n        \n        # replacing all the non alphabets with space\n        new=re.sub(r'[^A-Za-z]',' ',message)\n        \n        #splitting the message into words wherever the whitespace occurs\n        splitted=new.split()\n        \n        #removing stopwords followed by stemming \n        stemmed=[nltk.stem.PorterStemmer().stem(word) for word in splitted if word not in stopwords.words('english')]\n        \n        #finally joining back the splitted message\n        final=' '.join(stemmed)\n        v2.append(final)\n        \n    return v2","3fd2d969":"# replacing old v2 column with new processed column\ndf['v2']=preprocess(df)","0920f751":"df.head()","3a376933":"tfidf=TfidfVectorizer()\nfeatures=tfidf.fit_transform(df['v2']).todense()","380bc476":"# making a dataframe of new vector's matrix\ndf1=pd.DataFrame(features,columns=tfidf.get_feature_names())","2762eceb":"from sklearn.preprocessing import LabelEncoder","6f8f894f":"le=LabelEncoder()\ndf1['v1']=le.fit_transform(df['v1'])","853ab902":"#adding length column\ndf1['Length']=df['Length']","89fe9c7e":"df1","499b6edc":"from sklearn.model_selection import train_test_split","0fd7f187":"# be careful that the data is unbalanced and we need to use the stratify parameter\nX_train, X_test, y_train, y_test=train_test_split(df1.drop('v1',axis=1),df1['v1'],test_size=0.2,stratify=df1['v1'])","78c1d8a3":"X_train","16aba4fe":"X_test","a18f938d":"y_train","3d2e0ec4":"y_test","4a542153":"nb=MultinomialNB()","f6ec319f":"nb.fit(X_train,y_train)","05cb1ddf":"nb_pred=nb.predict(X_test)","634a015d":"from sklearn.metrics import confusion_matrix","16b38af5":"confusion_matrix(y_test,nb_pred)","b3ca12a1":"roc_auc_score(y_test,nb_pred)","b3ccea47":"sns.heatmap(confusion_matrix(y_test,nb_pred),annot=True)","1e562add":"# linear kernel works best for the text classification\nsvc=SVC(kernel='linear')","ff4fded2":"svc.fit(X_train,y_train)","9ff4b250":"svc_pred=svc.predict(X_test)\nconfusion_matrix(y_test,svc_pred)","e426f295":"roc_auc_score(y_test,svc_pred)","f0a802db":"sns.heatmap(confusion_matrix(y_test,svc_pred),annot=True)","c40130fa":"### Dropping unneccesary columns","9cd3d238":"### 1. Naive Bayes  ","b5cdc44d":"# SVM worked better ","e97a0965":"# Naive bayes and SVM classifier works well with the text data","2ed920b2":"not bad!!","462166a3":"# Train test test split ","57ca0b2f":"### 2. Lets try with SVC model","81b84d71":"# vectorizing using tfidf ","be82a50f":"### text preprocessing ","167d7d92":"# Label encoding of target column","f58342d4":"# unbalanced dataset (we need to use stratify technique while train test split)","d99d81d0":"### the above histogram says that if the message is lenghty , it is more likely to be a spam message (it adds some information to our analysis and thus this is useful for model creation)"}}