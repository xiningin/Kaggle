{"cell_type":{"d1afb37e":"code","0860c990":"code","6fe25e9f":"code","08899642":"code","d00cfcd4":"code","06f9b112":"code","a3f065d9":"code","629de54a":"code","41b9a1a7":"code","d585c187":"code","5631b619":"code","12cf2b4f":"code","07f37a88":"code","b2c3b446":"code","9afff24e":"code","03ad4314":"code","889c96c1":"code","7ecdb4d5":"code","237fbda7":"code","31a1b016":"code","61995569":"code","ea82d667":"code","67be19c9":"markdown","db5687c9":"markdown","e3b2fe8c":"markdown","cf8e4fd9":"markdown","cb5465f9":"markdown","49a80467":"markdown","9e57445c":"markdown","a9b808a9":"markdown","33208956":"markdown","2a62c9a1":"markdown","91e01699":"markdown","37141f64":"markdown","9387396f":"markdown","e803dd63":"markdown","23b0ecc7":"markdown","900148bb":"markdown","66780c57":"markdown","99720575":"markdown"},"source":{"d1afb37e":"# Import der ben\u00f6tigten Bibliotheken.\n\nimport os\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport time\nimport random\nimport torch\nimport torch.nn as nn\nimport torchvision\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nfrom matplotlib.image import imread\n%matplotlib inline","0860c990":"# Pr\u00fcfung ob GPU verf\u00fcgbar ist.\n\nTrainOnGPU = torch.cuda.is_available()\nif TrainOnGPU:\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n(print('GPU is available :)\\n') if TrainOnGPU else print('GPU is not available. Training on CPU ...\\n'))","6fe25e9f":"root = '..\/input\/chest-xray-pneumonia\/chest_xray'\nos.listdir(root)","08899642":"dirs = [os.path.join(root, 'train'), os.path.join(root, 'val'), os.path.join(root, 'test')]\nclasses = ['NORMAL', 'PNEUMONIA']","d00cfcd4":"# Ausgabe wieviele Daten verf\u00fcgbar sind.\n\nprint('vorhandene Bilder im Datensatz:')\nprint('Training: {} Normale, {} Pneumonia'.format(len(os.listdir(os.path.join(dirs[0], classes[0]))), len(os.listdir(os.path.join(dirs[0], classes[1])))))\nprint('Validierung: {} Normale, {} Pneumonia'.format(len(os.listdir(os.path.join(dirs[1], classes[0]))), len(os.listdir(os.path.join(dirs[1], classes[1])))))\nprint('Test: {} Normale, {} Pneumonia'.format(len(os.listdir(os.path.join(dirs[2], classes[0]))), len(os.listdir(os.path.join(dirs[2], classes[1])))))","06f9b112":"train_samplesize = pd.DataFrame.from_dict(\n    {'Normal': [len([os.path.join(root+'\/train\/NORMAL', filename) \n                     for filename in os.listdir(root+'\/train\/NORMAL')])], \n     'Pneumonia': [len([os.path.join(root+'\/train\/PNEUMONIA', filename) \n                        for filename in os.listdir(root+'\/train\/PNEUMONIA')])]})\n\nsns.barplot(data=train_samplesize).set_title('Training Set Data Inbalance', fontsize=16)\nplt.show()","a3f065d9":"def plot_samples(samples):  \n    fig, ax = plt.subplots(nrows=2, ncols=5, figsize=(30,8))\n    for i in range(len(samples)):\n        image = cv2.cvtColor(imread(samples[i]), cv2.COLOR_BGR2RGB)\n        ax[i\/\/5][i%5].imshow(image)\n        if i<5:\n            ax[i\/\/5][i%5].set_title(\"Normal\", fontsize=20)\n        else:\n            ax[i\/\/5][i%5].set_title(\"Pneumonia\", fontsize=20)\n        ax[i\/\/5][i%5].axis('off')\n        \nrand_samples = random.sample([os.path.join(root+'\/train\/NORMAL', filename) \n                              for filename in os.listdir(root+'\/train\/NORMAL')], 5) + \\\n               random.sample([os.path.join(root+'\/train\/PNEUMONIA', filename) \n                              for filename in os.listdir(root+'\/train\/PNEUMONIA')], 5)\n\nplot_samples(rand_samples)\nplt.suptitle('Training Set Samples', fontsize=30)\nplt.show()","629de54a":"DataTransforms = {'train': torchvision.transforms.Compose([\n                           torchvision.transforms.Resize((224, 224)),                                            # Resize auf eine Gr\u00f6\u00dfe von 224 x 224\n                           torchvision.transforms.CenterCrop(224),                                               # Bildmittelpunkt\n                           torchvision.transforms.RandomHorizontalFlip(),                                        # Um vertikale Achse spiegeln\n                           torchvision.transforms.RandomRotation(10),                                            # Drehung um bis zu 10\u00b0\n                           torchvision.transforms.ToTensor(),                                                    # In Tensor speichern. Bild nun von 0-1.\n                           torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),]),    # Normalisierung in Mean und Standard Deviation\n                   \n                'testval': torchvision.transforms.Compose([\n                           torchvision.transforms.Resize((224,224)),\n                           torchvision.transforms.CenterCrop(224),                                               # Validation und Test Daten m\u00fcssen ebenfalls einem Rezise,\n                           torchvision.transforms.ToTensor(),                                                    # einer Tensor Transformation und einer Normalisierung \n                           torchvision.transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),]),}   # unterzogen werden.","41b9a1a7":"DataSets = {\n    'train': torchvision.datasets.ImageFolder(dirs[0], DataTransforms['train']),\n    'val':   torchvision.datasets.ImageFolder(dirs[1], DataTransforms['testval']),\n    'test':  torchvision.datasets.ImageFolder(dirs[2], DataTransforms['testval']),}","d585c187":"print(DataSets['train'])\nprint(DataSets['val'])","5631b619":"random_seed = 2020\ntorch.manual_seed(random_seed);\n\ntrain_ds, val_ds = train_test_split(DataSets['train'], test_size=0.3, random_state=random_seed)\nprint('L\u00e4nge Trainingsdaten: {}'.format(len(train_ds)))\nprint('L\u00e4nge Validierungsdaten: {}'.format(len(val_ds)))\n\nDataSets['train'] = train_ds\nDataSets['val'] = val_ds","12cf2b4f":"BatchSize = 32\n\nDataLoaders = {\n    'train': torch.utils.data.DataLoader(DataSets['train'], batch_size=BatchSize, shuffle=True, num_workers=2),\n    'val':   torch.utils.data.DataLoader(DataSets['val'],   batch_size=BatchSize, shuffle=True, num_workers=2),\n    'test':  torch.utils.data.DataLoader(DataSets['test'],  batch_size=BatchSize, shuffle=True, num_workers=2)}","07f37a88":"model = torchvision.models.vgg16_bn(pretrained=True)\n\nfor param in model.features.parameters():\n    param.requires_grad = False\n\nnumf = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(numf, 2)\nmodel = model.to(device)","b2c3b446":"NumEpochs = 10\nLearnRate = 0.001\noptimizer = torch.optim.Adam(model.parameters(), lr=LearnRate)\nscheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\ncriterion = nn.CrossEntropyLoss()","9afff24e":"#from torchsummary import summary\n#summary(model, input_size=(3, 224, 224), batch_size=32)","03ad4314":"def train(model, TrainData, ValData, NumEpochs, optimizer, criterion, scheduler = None, TRAINONGPU = False):\n    print('Training\\n------------------------------------------')\n    LearnRate = optimizer.param_groups[0][\"lr\"]\n    TrainStartTime = time.perf_counter()\n    BestValLoss = np.Inf\n\n    if TRAINONGPU:\n        model.cuda()\n\n    history = {'trainloss': [], 'valloss': [], 'valacc': [],\n               'trainacc': [], 'learnrate': []}\n\n    # ----------------------------------------\n    # Start der Epochen\n\n    for epoch in range(1, NumEpochs + 1):\n        print('Epoche {}\/{}:'.format(epoch, NumEpochs))\n        EpochStartTime = time.perf_counter()\n        \n        # ----------------------------------------\n        # Training\n\n        model.train(True)\n\n        TrainLoss, TrainAcc = 0.0, 0.0\n        ClassCorrect, ClassTotal = [0.0, 0.0], [0.0, 0.0]\n\n        for data, target in TrainData:\n\n            if TRAINONGPU:\n                data, target = data.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            output = model(data)\n            _, pred = torch.max(output.data, 1)\n            loss = criterion(output, target)\n            loss.backward()\n            optimizer.step()\n\n            TrainLoss += loss.item() * data.size(0)\n            TrainAcc += torch.sum(pred == target.data)\n\n            CorrectTensor = pred.eq(target.data.view_as(pred))\n            Correct = np.squeeze(CorrectTensor.numpy()) if not TRAINONGPU else np.squeeze(CorrectTensor.cpu().numpy())\n\n            for i in range(data.shape[0]):\n                label = target.data[i]\n                ClassCorrect[label] += Correct[i].item()\n                ClassTotal[label] += 1\n\n            del data, target, output, pred\n            torch.cuda.empty_cache()\n\n        # ----------------------------------------\n        # Training Metrics berechnen\n\n        LearnRate = optimizer.param_groups[0][\"lr\"]\n        TrainLoss = TrainLoss \/ len(TrainData.dataset)\n        TrainAcc = 100 * TrainAcc \/ len(TrainData.dataset)\n        NormalAcc = (100 * ClassCorrect[0] \/ ClassTotal[0])\n        PneumoniaAcc = (100 * ClassCorrect[1] \/ ClassTotal[1])\n        \n        print('Learn Rate: {}'.format(LearnRate))\n        print('Training Loss: {:.6f} \\n         Accuracy: {:.2f}%'.format(TrainLoss, TrainAcc))\n        print('         Normal: {:.2f}% [{:.0f}\/{:.0f}]'.format(NormalAcc, np.sum(ClassCorrect[0]), np.sum(ClassTotal[0])))\n        print('         Pneumo: {:.2f}% [{:.0f}\/{:.0f}]'.format(PneumoniaAcc, np.sum(ClassCorrect[1]), np.sum(ClassTotal[1])))\n        del ClassCorrect, ClassTotal\n\n        # ----------------------------------------\n        # Validierung\n\n        model.train(False)\n        model.eval()\n\n        ValLoss, ValAcc = 0.0, 0.0\n        ClassCorrect, ClassTotal = [0.0, 0.0], [0.0, 0.0]\n\n        for data, target in ValData:\n\n            if TRAINONGPU:\n                data, target = data.cuda(), target.cuda()\n            \n            optimizer.zero_grad()\n            output = model(data)\n            _, pred = torch.max(output.data, 1)\n            loss = criterion(output, target)\n\n            ValLoss += loss.item() * data.size(0)\n            ValAcc += torch.sum(pred == target.data)\n\n            CorrectTensor = pred.eq(target.data.view_as(pred))\n            Correct = np.squeeze(CorrectTensor.numpy()) if not TRAINONGPU else np.squeeze(CorrectTensor.cpu().numpy())\n\n            for i in range(data.shape[0]):\n                label = target.data[i]\n                ClassCorrect[label] += Correct[i].item()\n                ClassTotal[label] += 1\n\n            del data, target, output, pred\n            torch.cuda.empty_cache()\n\n        # ----------------------------------------\n        # Validierungs Metrics berechnen\n\n        ValLoss = ValLoss \/ len(ValData.dataset)\n        ValAcc = 100 * ValAcc \/ len(ValData.dataset)\n        NormalAcc = (100 * ClassCorrect[0] \/ ClassTotal[0])\n        PneumoniaAcc = (100 * ClassCorrect[1] \/ ClassTotal[1])\n\n        print('Validation Loss: {:.6f} \\n           Accuracy: {:.2f}%'.format(ValLoss, ValAcc))\n        print('           Normal: {:.2f}% [{:.0f}\/{:.0f}]'.format(NormalAcc, np.sum(ClassCorrect[0]), np.sum(ClassTotal[0])))\n        print('           Pneumo: {:.2f}% [{:.0f}\/{:.0f}]'.format(PneumoniaAcc, np.sum(ClassCorrect[1]), np.sum(ClassTotal[1])))\n        del ClassCorrect, ClassTotal\n\n        # ----------------------------------------\n        # History aktualisieren\n\n        history['trainloss'].append(TrainLoss)\n        history['valloss'].append(ValLoss)\n        history['valacc'].append(ValAcc)\n        history['trainacc'].append(TrainAcc)\n        history['learnrate'].append(LearnRate)\n\n        # ----------------------------------------\n        # Scheduler Step\n\n        scheduler.step() if (scheduler != None) else None\n\n        # ----------------------------------------\n        # Modell speichern\n\n        logdir = 'model.pth'\n        if ValLoss <= BestValLoss:\n            torch.save(model.state_dict(), logdir)\n            print('Validation loss decreased ({:.6f} -> {:.6f}). Modell sichern.'.format(BestValLoss, ValLoss))\n            BestValLoss = ValLoss\n\n        print('{:.2f} seconds elapsed in this epoch.\\n'.format(time.perf_counter()-EpochStartTime))\n    print('------------------------------------------')\n    print('{:.2f} total seconds elapsed.\\n'.format(time.perf_counter()-TrainStartTime))\n\n    model.to('cpu')\n    model.load_state_dict(torch.load(logdir))\n\n    return model, history","889c96c1":"model, history = train(model, DataLoaders['train'], DataLoaders['val'], NumEpochs, optimizer, criterion, scheduler, TrainOnGPU)","7ecdb4d5":"def test(model, TestData, criterion, TRAINONGPU):\n\n    DATALOADER = TestData\n    print('\\nTesting\\n------------------------------------------')\n\n    if TRAINONGPU:\n        model.cuda()\n    \n    TestStartTime = time.perf_counter()\n\n    # ----------------------------------------\n    # Test\n\n    model.train(False)\n    model.eval()\n\n    TestLoss, TestAcc = 0.0, 0.0\n    ClassCorrect, ClassTotal = [0.0, 0.0], [0.0, 0.0]\n\n    for data, target in DATALOADER:\n      \n        if TRAINONGPU:\n            data, target = data.cuda(), target.cuda()\n        \n        output = model(data)\n        loss = criterion(output, target)\n        TestLoss += loss.item() * data.size(0)\n\n        _, pred = torch.max(output.data, 1)\n        CorrectTensor = pred.eq(target.data.view_as(pred))\n        Correct = np.squeeze(CorrectTensor.numpy()) if not TRAINONGPU else np.squeeze(CorrectTensor.cpu().numpy())\n\n        for i in range(data.shape[0]):\n            label = target.data[i]\n            ClassCorrect[label] += Correct[i].item()\n            ClassTotal[label] += 1\n\n        del data, target, output, pred\n        torch.cuda.empty_cache()\n\n    # ----------------------------------------\n    # Test Results\n\n    TestLoss = TestLoss \/ len(TestData.dataset)\n    TestAcc = (100 * np.sum(ClassCorrect) \/ np.sum(ClassTotal))\n    NormalAcc = (100 * ClassCorrect[0] \/ ClassTotal[0])\n    PneumoniaAcc = (100 * ClassCorrect[1] \/ ClassTotal[1])\n\n    print('Loss: {:.6f} \\nAccuracy: {:.2f}% [{:.0f}\/{:.0f}]'.format(TestLoss, TestAcc, np.sum(ClassCorrect), np.sum(ClassTotal)))\n    print('Accuracy Normal: {:.2f}% [{:.0f}\/{:.0f}]'.format(NormalAcc, np.sum(ClassCorrect[0]), np.sum(ClassTotal[0])))\n    print('Accuracy Pneumo: {:.2f}% [{:.0f}\/{:.0f}]'.format(PneumoniaAcc, np.sum(ClassCorrect[1]), np.sum(ClassTotal[1])))\n\n    # ----------------------------------------\n    # Confusion Matrix\n\n    confusion = np.array([[np.sum(ClassCorrect[0]), np.sum(ClassTotal[0])-np.sum(ClassCorrect[0])],\n                          [np.sum(ClassTotal[1])-np.sum(ClassCorrect[1]), np.sum(ClassCorrect[1])]])\n\n    print('------------------------------------------')\n    print('{:.2f} seconds elapsed for testing.\\n'.format(time.perf_counter()-TestStartTime))\n\n    return confusion","237fbda7":"# confusion = test(model, DataLoaders['test'], criterion, TrainOnGPU)\nconfusion = test(model, DataLoaders['val'], criterion, TrainOnGPU)","31a1b016":"def LossPlot(history):\n    if len(history['valloss']) <= 2:\n        print('Zu wenige Daten f\u00fcr Plot.')\n        return\n    epochs = np.linspace(1, len(history['valloss']), len(history['valloss']))\n    fig, ax = plt.subplots(figsize=(8,5))\n\n    LT = ax.plot(epochs, history['trainloss'], 'b-', label='Train Loss')\n    LV = ax.plot(epochs, history['valloss'], 'r-', label='Val Loss')\n\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Loss')\n\n    ax.set_xlim([1, (len(history['valloss']))])\n    if len(history['valloss']) >= 30:\n        ax.set_xticks(range(1, (len(history['valloss'])+1), 5))\n    elif len(history['valloss']) >= 20:\n        ax.set_xticks(range(1, (len(history['valloss'])+1), 2))\n    elif len(history['valloss']) < 20:\n        ax.set_xticks(range(1, (len(history['valloss'])+1)))\n\n    lns = LT+LV\n    labs = [l.get_label() for l in lns]\n    ax.legend(lns, labs, loc='upper right')\n    ax.grid('on')\n\n    plt.savefig('LossPlot.pdf')\n\nLossPlot(history)","61995569":"def AccuracyPlot(history):\n    if len(history['valloss']) <= 2:\n        print('Zu wenige Daten f\u00fcr Plot.')\n        return\n    epochs = np.linspace(1, len(history['valloss']), len(history['valloss']))\n    fig, ax = plt.subplots(figsize=(8,5))\n\n    AT = ax.plot(epochs, history['trainacc'], 'b-', label='Train Acc')\n    AV = ax.plot(epochs, history['valacc'], 'r-', label='Val Acc')\n    ax.set_xlabel('Epochs')\n    ax.set_ylabel('Accuracy')\n\n    ax.set_xlim([1, (len(history['valloss']))])\n    if len(history['valloss']) >= 30:\n        ax.set_xticks(range(1, (len(history['valloss'])+1), 5))\n    elif len(history['valloss']) >= 20:\n        ax.set_xticks(range(1, (len(history['valloss'])+1), 2))\n    elif len(history['valloss']) < 20:\n        ax.set_xticks(range(1, (len(history['valloss'])+1)))\n\n    lns = AT+AV\n    labs = [l.get_label() for l in lns]\n    ax.legend(lns, labs, loc='upper left')\n    ax.grid('on')\n\n    plt.savefig('AccPlot.pdf')\n\nAccuracyPlot(history)","ea82d667":"def ConfusionMatrix(confusion):\n    cmtx = confusion\n    if np.sum(cmtx) == 0:\n        print('Confusion Matrix erst nach erfolgreichem Test verf\u00fcgbar.')\n        return\n    cmtx = cmtx.astype('float') \/ cmtx.sum(axis=1)[:, np.newaxis]\n\n    cmap = plt.get_cmap('Blues')\n    plt.figure(figsize=(8,6))\n    plt.imshow(cmtx, interpolation='nearest', cmap = cmap)\n    cb = plt.colorbar()\n    cb.set_ticks(np.linspace(0, 1, 11))\n\n    plt.xticks(np.arange(2), ['Normal', 'Pneumonia'])\n    plt.yticks(np.arange(2), ['Normal', 'Pneumonia'])\n\n    for i in range(cmtx.shape[0]):\n        for j in range(cmtx.shape[1]):\n            plt.text(j,i, '{:.4f}'.format(cmtx[i,j]), ha = 'center', color= \"white\" if cmtx[i, j] > 0.5 else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n    plt.savefig('ConfusionMatrix.pdf')\n\nConfusionMatrix(confusion)","67be19c9":"In den folgenden Funktionen sollen die Trainingsergebnisse visuell dargestellt werden. Dabei wird zum einen der Loss und die Accuracy \u00fcber den Epochen als Graph dargestellt. Zus\u00e4tzlich wird f\u00fcr die Testdaten eine Confusion Matrix erzeugt, welche die endg\u00fcltigen Ergebnisse verdeutlichen soll.","db5687c9":"## Vorbereitung","e3b2fe8c":"Die Dataloaders vereinen die Directories der Datensets Train, Val und Test sowie deren Augmentierung. Ebenso wird die Batch Size der Daten festgelegt. Das Programm hat nun Zugriff auf alle Daten \u00fcber die Angabe eines DataLoaders.","cf8e4fd9":"## Initialisierung des Neuronalen Netzes","cb5465f9":"## Augmentierung","49a80467":"## Test","9e57445c":"Das CNN wird mit Transfer Learning aufgebaut. Die Struktur wird dabei von dem Netz VGG16 mit Batch Normalisierung geliefert. Angepasst wird dabei nur der letzte Layer mit den jeweiligen Ausgabe Schichten.","a9b808a9":"# Detect Pneumonia from Chest X-Rays\n\nDieses Notebook habe ich f\u00fcr eine Projektarbeit w\u00e4hrend meines Masterstudiums erstellt. Es handelt sich um eine Transfer Learning L\u00f6sung mit einem VGG16 Netzwerk.","33208956":"## Dataset","2a62c9a1":"Um den Datensatz effektiv zu erlernen, muss eine erneute Datenaufteilung stattfinden. Im Datensatz exisiteren nur 16 Validierungsbilder. Dies ist f\u00fcr eine effektive Performance Einsch\u00e4tzung zu wenig. Aus diesem Grund werden den Trainingsdaten Bilder entnommen und als Validierungsbilder zur Verf\u00fcgung gestellt.","91e01699":"Bei der Betrachtung der Datenaufteilung f\u00e4llt auf, dass diese eine hohe Inbalance aufweist. (Es sind viel mehr R\u00f6ntgenbilder mit Pneumonia als ohne vorhanden.)","37141f64":"Die Augmentierung dient dazu, die gegebenen Datens\u00e4tze mit leichten Ver\u00e4nderungen der Bilder zu vervielf\u00e4ltigen. Dadurch wird k\u00fcnstlich eine gr\u00f6\u00dfere Menge an Trainings-Daten erzeugt. Die Test und Validierungsdaten werden ebenfalls leicht angepasst um dem Netz die gleichen Vorraussetzungen zu geben. Somit werden alle Daten einem Resize, einr Tensor Transformation und einer Normalisierung unterzogen.","9387396f":"## Results","e803dd63":"## Training","23b0ecc7":"Die Vorbereitung des Programms umfasst den Import der ben\u00f6tigten Bibliotheken f\u00fcr das Neuronale Netz und die weiteren Python Module. Ebenso wird gepr\u00fcft, ob das Programm Zugriff auf eine CUDA f\u00e4hige Grafikkarte hat. Wenn dem nicht so ist, wird das Programm standardm\u00e4\u00dfig auf der CPU ausgef\u00fchrt. (Hat den Nachteil, dass das Training um einiges mehr Zeit in Anspruch nehmen wird.)","900148bb":"Der Datensatz \"Chest X-Ray Images (Pneumonia)\" ist ein 2018 von Paul Mooney kreierter Datensatz. Er dient zur Lehre und zu \u00f6ffentlichen Competitions auf Kaggle. Er besteht aus 5863 Bildern in zwei Kategorien. Es gibt eine gesunde (Normal) und eine kranke (Pneumonia) Klasse, welche jeweils aus R\u00f6ntgenbildern des Brustbereichs bestehen. Ziel ist die Unterscheidung zwischen gesunden Personen und denjenigen mit einer Lungenentz\u00fcndung (Pneumonia).\n\nQuelle: https:\/\/www.kaggle.com\/paultimothymooney\/chest-xray-pneumonia","66780c57":"## Data Loaders","99720575":"Um die Rohdaten zu betrachten, wird nun mit einer Funktion ein Plot erzeugt, welcher von jeder Sorte (Normal, Pneumonia) je 5 Bilder ausgibt. Die Detektion mit blo\u00dfem Auge f\u00e4llt sehr schwer und ist ohne fachliche medizinische Kenntnis nicht durchf\u00fchrbar."}}