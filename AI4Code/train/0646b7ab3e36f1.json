{"cell_type":{"8360b35a":"code","2f2b424e":"code","8b6ce2b7":"code","83b2f544":"code","8dd5f9c2":"code","9dc622bd":"code","e1f74591":"code","1aef4e42":"code","72ea364b":"code","09524548":"code","c01073cd":"code","0fe2439a":"code","de87f3bf":"code","4712b64b":"code","c00c1bfb":"code","6a1e67ab":"code","d1ad8c5d":"code","87f0bb57":"code","b9867b8c":"code","9ecc83d5":"code","d5f5c9ad":"code","29dd73a7":"code","228434a6":"markdown","a4d6cf53":"markdown"},"source":{"8360b35a":"import numpy as np\nimport torch\nimport torch.nn as nn\n\n'''Based on https:\/\/github.com\/WangYueFt\/dgcnn\/blob\/master\/pytorch\/model.py.'''\n\n\ndef knn(x, k):\n    inner = -2 * torch.matmul(x.transpose(2, 1), x)\n    xx = torch.sum(x ** 2, dim=1, keepdim=True)\n    pairwise_distance = -xx - inner - xx.transpose(2, 1)\n    idx = pairwise_distance.topk(k=k + 1, dim=-1)[1][:, :, 1:]  # (batch_size, num_points, k)\n    return idx\n\n\n# v1 is faster on GPU\ndef get_graph_feature_v1(x, k, idx):\n    batch_size, num_dims, num_points = x.size()\n\n    idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n    idx = idx + idx_base\n    idx = idx.view(-1)\n\n    fts = x.transpose(2, 1).reshape(-1, num_dims)  # -> (batch_size, num_points, num_dims) -> (batch_size*num_points, num_dims)\n    fts = fts[idx, :].view(batch_size, num_points, k, num_dims)  # neighbors: -> (batch_size*num_points*k, num_dims) -> ...\n    fts = fts.permute(0, 3, 1, 2).contiguous()  # (batch_size, num_dims, num_points, k)\n    x = x.view(batch_size, num_dims, num_points, 1).repeat(1, 1, 1, k)\n    fts = torch.cat((x, fts - x), dim=1)  # ->(batch_size, 2*num_dims, num_points, k)\n    return fts\n\n\n# v2 is faster on CPU\ndef get_graph_feature_v2(x, k, idx):\n    batch_size, num_dims, num_points = x.size()\n\n    idx_base = torch.arange(0, batch_size, device=x.device).view(-1, 1, 1) * num_points\n    idx = idx + idx_base\n    idx = idx.view(-1)\n\n    fts = x.transpose(0, 1).reshape(num_dims, -1)  # -> (num_dims, batch_size, num_points) -> (num_dims, batch_size*num_points)\n    fts = fts[:, idx].view(num_dims, batch_size, num_points, k)  # neighbors: -> (num_dims, batch_size*num_points*k) -> ...\n    fts = fts.transpose(1, 0).contiguous()  # (batch_size, num_dims, num_points, k)\n\n    x = x.view(batch_size, num_dims, num_points, 1).repeat(1, 1, 1, k)\n    fts = torch.cat((x, fts - x), dim=1)  # ->(batch_size, 2*num_dims, num_points, k)\n\n    return fts\n\n\nclass EdgeConvBlock(nn.Module):\n    r\"\"\"EdgeConv layer.\n    Introduced in \"`Dynamic Graph CNN for Learning on Point Clouds\n    <https:\/\/arxiv.org\/pdf\/1801.07829>`__\".  Can be described as follows:\n    .. math::\n       x_i^{(l+1)} = \\max_{j \\in \\mathcal{N}(i)} \\mathrm{ReLU}(\n       \\Theta \\cdot (x_j^{(l)} - x_i^{(l)}) + \\Phi \\cdot x_i^{(l)})\n    where :math:`\\mathcal{N}(i)` is the neighbor of :math:`i`.\n    Parameters\n    ----------\n    in_feat : int\n        Input feature size.\n    out_feat : int\n        Output feature size.\n    batch_norm : bool\n        Whether to include batch normalization on messages.\n    \"\"\"\n\n    def __init__(self, k, in_feat, out_feats, batch_norm=True, activation=True, cpu_mode=False):\n        super(EdgeConvBlock, self).__init__()\n        self.k = k\n        self.batch_norm = batch_norm\n        self.activation = activation\n        self.num_layers = len(out_feats)\n        self.get_graph_feature = get_graph_feature_v2 if cpu_mode else get_graph_feature_v1\n\n        self.convs = nn.ModuleList()\n        for i in range(self.num_layers):\n            self.convs.append(nn.Conv2d(2 * in_feat if i == 0 else out_feats[i - 1], out_feats[i], kernel_size=1, bias=False if self.batch_norm else True))\n\n        if batch_norm:\n            self.bns = nn.ModuleList()\n            for i in range(self.num_layers):\n                self.bns.append(nn.BatchNorm2d(out_feats[i]))\n\n        if activation:\n            self.acts = nn.ModuleList()\n            for i in range(self.num_layers):\n                self.acts.append(nn.ReLU())\n\n        if in_feat == out_feats[-1]:\n            self.sc = None\n        else:\n            self.sc = nn.Conv1d(in_feat, out_feats[-1], kernel_size=1, bias=False)\n            self.sc_bn = nn.BatchNorm1d(out_feats[-1])\n\n        if activation:\n            self.sc_act = nn.ReLU()\n\n    def forward(self, points, features):\n\n        topk_indices = knn(points, self.k)\n        x = self.get_graph_feature(features, self.k, topk_indices)\n\n        for conv, bn, act in zip(self.convs, self.bns, self.acts):\n            x = conv(x)  # (N, C', P, K)\n            if bn:\n                x = bn(x)\n            if act:\n                x = act(x)\n\n        fts = x.mean(dim=-1)  # (N, C, P)\n\n        # shortcut\n        if self.sc:\n            sc = self.sc(features)  # (N, C_out, P)\n            sc = self.sc_bn(sc)\n        else:\n            sc = features\n\n        return self.sc_act(sc + fts)  # (N, C_out, P)\n\n\nclass ParticleNet(nn.Module):\n\n    def __init__(self,\n                 input_dims,\n                 num_classes,\n                 conv_params=[(7, (32, 32, 32)), (7, (64, 64, 64))],\n                 fc_params=[(128, 0.1)],\n                 use_fusion=True,\n                 use_fts_bn=True,\n                 use_counts=True,\n                 for_inference=False,\n                 for_segmentation=False,\n                 **kwargs):\n        super(ParticleNet, self).__init__(**kwargs)\n\n        self.use_fts_bn = use_fts_bn\n        if self.use_fts_bn:\n            self.bn_fts = nn.BatchNorm1d(input_dims)\n\n        self.use_counts = use_counts\n\n        self.edge_convs = nn.ModuleList()\n        for idx, layer_param in enumerate(conv_params):\n            k, channels = layer_param\n            in_feat = input_dims if idx == 0 else conv_params[idx - 1][1][-1]\n            self.edge_convs.append(EdgeConvBlock(k=k, in_feat=in_feat, out_feats=channels, cpu_mode=for_inference))\n\n        self.use_fusion = use_fusion\n        if self.use_fusion:\n            in_chn = sum(x[-1] for _, x in conv_params)\n            out_chn = np.clip((in_chn \/\/ 128) * 128, 128, 1024)\n            self.fusion_block = nn.Sequential(nn.Conv1d(in_chn, out_chn, kernel_size=1, bias=False), nn.BatchNorm1d(out_chn), nn.ReLU())\n\n        self.for_segmentation = for_segmentation\n\n        fcs = []\n        for idx, layer_param in enumerate(fc_params):\n            channels, drop_rate = layer_param\n            if idx == 0:\n                in_chn = out_chn if self.use_fusion else conv_params[-1][1][-1]\n            else:\n                in_chn = fc_params[idx - 1][0]\n            if self.for_segmentation:\n                fcs.append(nn.Sequential(nn.Conv1d(in_chn, channels, kernel_size=1, bias=False),\n                                         nn.BatchNorm1d(channels), nn.ReLU(), nn.Dropout(drop_rate)))\n            else:\n                fcs.append(nn.Sequential(nn.Linear(in_chn, channels), nn.ReLU(), nn.Dropout(drop_rate)))\n        if self.for_segmentation:\n            fcs.append(nn.Conv1d(fc_params[-1][0], num_classes, kernel_size=1))\n        else:\n            fcs.append(nn.Linear(fc_params[-1][0], num_classes))\n        self.fc = nn.Sequential(*fcs)\n\n        self.for_inference = for_inference\n\n    def forward(self, points, features, mask=None):\n#         print('points:\\n', points)\n#         print('features:\\n', features)\n        if mask is None:\n            mask = (features.abs().sum(dim=1, keepdim=True) != 0)  # (N, 1, P)\n        points *= mask\n        features *= mask\n        coord_shift = (mask == 0) * 1e9\n        if self.use_counts:\n            counts = mask.float().sum(dim=-1)\n            counts = torch.max(counts, torch.ones_like(counts))  # >=1\n\n        if self.use_fts_bn:\n            fts = self.bn_fts(features) * mask\n        else:\n            fts = features\n        outputs = []\n        for idx, conv in enumerate(self.edge_convs):\n            pts = (points if idx == 0 else fts) + coord_shift\n            fts = conv(pts, fts) * mask\n            if self.use_fusion:\n                outputs.append(fts)\n        if self.use_fusion:\n            fts = self.fusion_block(torch.cat(outputs, dim=1)) * mask\n\n#         assert(((fts.abs().sum(dim=1, keepdim=True) != 0).float() - mask.float()).abs().sum().item() == 0)\n        \n        if self.for_segmentation:\n            x = fts\n        else:\n            if self.use_counts:\n                x = fts.sum(dim=-1) \/ counts  # divide by the real counts\n            else:\n                x = fts.mean(dim=-1)\n\n        output = self.fc(x)\n        if self.for_inference:\n            output = torch.softmax(output, dim=1)\n        # print('output:\\n', output)\n        return output\n\n\nclass FeatureConv(nn.Module):\n\n    def __init__(self, in_chn, out_chn, **kwargs):\n        super(FeatureConv, self).__init__(**kwargs)\n        self.conv = nn.Sequential(\n            nn.BatchNorm1d(in_chn),\n            nn.Conv1d(in_chn, out_chn, kernel_size=1, bias=False),\n            nn.BatchNorm1d(out_chn),\n            nn.ReLU()\n            )\n\n    def forward(self, x):\n        return self.conv(x)\n\n\nclass ParticleNetTagger(nn.Module):\n\n    def __init__(self,\n                 pf_features_dims,\n                 sv_features_dims,\n                 num_classes,\n                 conv_params=[(7, (32, 32, 32)), (7, (64, 64, 64))],\n                 fc_params=[(128, 0.1)],\n                 use_fusion=True,\n                 use_fts_bn=True,\n                 use_counts=True,\n                 pf_input_dropout=None,\n                 sv_input_dropout=None,\n                 for_inference=False,\n                 **kwargs):\n        super(ParticleNetTagger, self).__init__(**kwargs)\n        self.pf_input_dropout = nn.Dropout(pf_input_dropout) if pf_input_dropout else None\n        self.sv_input_dropout = nn.Dropout(sv_input_dropout) if sv_input_dropout else None\n        self.pf_conv = FeatureConv(20, 32)\n        self.sv_conv = FeatureConv(11, 32)\n        self.pn = ParticleNet(input_dims=32,\n                              num_classes=num_classes,\n                              conv_params=conv_params,\n                              fc_params=fc_params,\n                              use_fusion=use_fusion,\n                              use_fts_bn=use_fts_bn,\n                              use_counts=use_counts,\n                              for_inference=for_inference)\n\n    def forward(self, pf_points, pf_features, pf_mask, sv_points, sv_features, sv_mask):\n        if self.pf_input_dropout:\n            pf_mask = (self.pf_input_dropout(pf_mask) != 0).float()\n            pf_points *= pf_mask\n            pf_features *= pf_mask\n        if self.sv_input_dropout:\n            sv_mask = (self.sv_input_dropout(sv_mask) != 0).float()\n            sv_points *= sv_mask\n            sv_features *= sv_mask\n\n        points = torch.cat((pf_points, sv_points), dim=2)\n        features = torch.cat((self.pf_conv(pf_features * pf_mask) * pf_mask, self.sv_conv(sv_features * sv_mask) * sv_mask), dim=2)\n        mask = torch.cat((pf_mask, sv_mask), dim=2)\n        return self.pn(points, features, mask)","2f2b424e":"!pip install torchsummary","8b6ce2b7":"import torch\n\ndef get_model(data_config, **kwargs):\n    conv_params = [\n        (8, (64, 64, 64)),\n        (8, (96, 96, 96)),\n        (8, (128, 128, 128)),\n        ]\n    fc_params = [(128, 0.1)]\n    use_fusion = True\n\n    pf_features_dims = 32\n    sv_features_dims = 32\n    num_classes = 6\n    model = ParticleNetTagger(pf_features_dims, sv_features_dims, num_classes,\n                              conv_params, fc_params,\n                              use_fusion=use_fusion,\n                              use_fts_bn=kwargs.get('use_fts_bn', True),\n                              use_counts=kwargs.get('use_counts', True),\n                              pf_input_dropout=kwargs.get('pf_input_dropout', 0.1),\n                              sv_input_dropout=kwargs.get('sv_input_dropout', 0.1),\n                              for_inference=kwargs.get('for_inference', False)\n                              )\n\n    return model\n\n\ndef get_loss(data_config, **kwargs):\n    return torch.nn.CrossEntropyLoss()","83b2f544":"model = get_model(1)","8dd5f9c2":"from torchsummary import summary\n\npf_points, pf_features, pf_mask, sv_points, sv_features, sv_mask = (2,50),(20,50),(1,50),(2,5),(11,5),(1,5) \n#(2,50),(20,50),(1,50),(2,5),(11,5),(1,5) \n#(50,2),(50,20),(50,1),(5,2),(5,11),(5,1)\n\nsummary(model, [pf_points, pf_features, pf_mask, sv_points, sv_features, sv_mask])","9dc622bd":"import tensorflow as tf\nfrom tensorflow import keras\n\n\n# A shape is (N, P_A, C), B shape is (N, P_B, C)\n# D shape is (N, P_A, P_B)\ndef batch_distance_matrix_general(A, B):\n    with tf.name_scope('dmat'):\n        r_A = tf.reduce_sum(A * A, axis=2, keepdims=True)\n        r_B = tf.reduce_sum(B * B, axis=2, keepdims=True)\n        m = tf.matmul(A, tf.transpose(B, perm=(0, 2, 1)))\n        D = r_A - 2 * m + tf.transpose(r_B, perm=(0, 2, 1))\n        return D\n\n\ndef knn(num_points, k, topk_indices, features):\n    # topk_indices: (N, P, K)\n    # features: (N, P, C)\n    with tf.name_scope('knn'):\n        queries_shape = tf.shape(features)\n        batch_size = queries_shape[0]\n        batch_indices = tf.tile(tf.reshape(tf.range(batch_size), (-1, 1, 1, 1)), (1, num_points, k, 1))\n        indices = tf.concat([batch_indices, tf.expand_dims(topk_indices, axis=3)], axis=3)  # (N, P, K, 2)\n        return tf.gather_nd(features, indices)\n\n\ndef edge_conv(points, features, num_points, K, channels, with_bn=True, activation='relu', pooling='average', name='edgeconv'):\n    \"\"\"EdgeConv\n    Args:\n        K: int, number of neighbors\n        in_channels: # of input channels\n        channels: tuple of output channels\n        pooling: pooling method ('max' or 'average')\n    Inputs:\n        points: (N, P, C_p)\n        features: (N, P, C_0)\n    Returns:\n        transformed points: (N, P, C_out), C_out = channels[-1]\n    \"\"\"\n\n    with tf.name_scope('edgeconv'):\n\n        # distance\n        D = batch_distance_matrix_general(points, points)  # (N, P, P)\n        _, indices = tf.nn.top_k(-D, k=K + 1)  # (N, P, K+1)\n        indices = indices[:, :, 1:]  # (N, P, K)\n\n        fts = features\n        knn_fts = knn(num_points, K, indices, fts)  # (N, P, K, C)\n        knn_fts_center = tf.tile(tf.expand_dims(fts, axis=2), (1, 1, K, 1))  # (N, P, K, C)\n        knn_fts = tf.concat([knn_fts_center, tf.subtract(knn_fts, knn_fts_center)], axis=-1)  # (N, P, K, 2*C)\n\n        x = knn_fts\n        for idx, channel in enumerate(channels):\n            x = keras.layers.Conv2D(channel, kernel_size=(1, 1), strides=1, data_format='channels_last',\n                                    use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_conv%d' % (name, idx))(x)\n            if with_bn:\n                x = keras.layers.BatchNormalization(name='%s_bn%d' % (name, idx))(x)\n            if activation:\n                x = keras.layers.Activation(activation, name='%s_act%d' % (name, idx))(x)\n\n        if pooling == 'max':\n            fts = tf.reduce_max(x, axis=2)  # (N, P, C')\n        else:\n            fts = tf.reduce_mean(x, axis=2)  # (N, P, C')\n\n        # shortcut\n        sc = keras.layers.Conv2D(channels[-1], kernel_size=(1, 1), strides=1, data_format='channels_last',\n                                 use_bias=False if with_bn else True, kernel_initializer='glorot_normal', name='%s_sc_conv' % name)(tf.expand_dims(features, axis=2))\n        if with_bn:\n            sc = keras.layers.BatchNormalization(name='%s_sc_bn' % name)(sc)\n        sc = tf.squeeze(sc, axis=2)\n\n        if activation:\n            return keras.layers.Activation(activation, name='%s_sc_act' % name)(sc + fts)  # (N, P, C')\n        else:\n            return sc + fts\n\n\ndef _particle_net_base(points, features=None, mask=None, setting=None, name='particle_net'):\n    # points : (N, P, C_coord)\n    # features:  (N, P, C_features), optional\n    # mask: (N, P, 1), optinal\n\n    with tf.name_scope(name):\n        if features is None:\n            features = points\n\n        if mask is not None:\n            mask = tf.cast(tf.not_equal(mask, 0), dtype='float32')  # 1 if valid\n            coord_shift = tf.multiply(1000000000.0, tf.cast(tf.equal(mask, 0), dtype='float32'))  # make non-valid positions to 99\n\n        fts = tf.squeeze(keras.layers.BatchNormalization(name='%s_fts_bn' % name)(tf.expand_dims(features, axis=2)), axis=2)\n        fusion = []\n        for layer_idx, layer_param in enumerate(setting.conv_params):\n            K, channels = layer_param\n            pts = tf.add(coord_shift, points) if layer_idx == 0 else tf.add(coord_shift, fts)\n            fts = edge_conv(pts, fts, setting.num_points, K, channels, with_bn=True, activation='relu',\n                            pooling=setting.conv_pooling, name='%s_%s%d' % (name, 'EdgeConv', layer_idx))\n            if mask is not None:\n                fts = tf.multiply(fts, mask)\n            fusion.append(fts)\n            \n        fts = tf.concat(fusion, axis = -1)\n        fts = keras.layers.Conv1D(256,1, strides=1, data_format='channels_last',\n                                    kernel_initializer='glorot_normal', name='fusion_block_conv')(fts)\n        fts = keras.layers.BatchNormalization(name='fusion_block_bn')(fts)\n        fts = keras.layers.Activation('relu', name='fusion_block_activation')(fts)\n        pool = tf.reduce_mean(fts, axis=1)  # (N, C)\n\n        if setting.fc_params is not None:\n            x = pool\n            for layer_idx, layer_param in enumerate(setting.fc_params):\n                units, drop_rate = layer_param\n                x = keras.layers.Dense(units, activation='relu')(x)\n                if drop_rate is not None and drop_rate > 0:\n                    x = keras.layers.Dropout(drop_rate)(x)\n            out = keras.layers.Dense(setting.num_class, activation='softmax')(x)\n            return out  # (N, num_classes)\n        else:\n            return pool\n\n\nclass _DotDict:\n    pass\n\ndef PNet_starter():\n    \n    points1 = keras.Input(name='points1', shape=(25,2))\n    features1 = keras.Input(name='features1', shape=(25,16)) \n    mask1 = keras.Input(name='mask1', shape=(25,1)) \n\n    points2 = keras.Input(name='points2', shape=(25,2))\n    features2 = keras.Input(name='features2', shape=(25,8)) \n    mask2 = keras.Input(name='mask2', shape=(25,1))\n    \n    points3 = keras.Input(name='points3', shape=(5,2))\n    features3 = keras.Input(name='features3', shape=(5,18)) \n    mask3 = keras.Input(name='mask3', shape=(5,1))\n    \n    points = tf.concat([points3,points1,points2], axis = 1)\n    mask = tf.concat([mask3,mask1,mask2], axis = 1)\n    \n    features1 = keras.layers.BatchNormalization()(features1)\n    features1 = keras.layers.Conv1D(32,1)(features1)\n    features1 = keras.layers.BatchNormalization()(features1)\n    features1 = keras.layers.Activation('relu')(features1)\n    \n    features2 = keras.layers.BatchNormalization()(features2)\n    features2 = keras.layers.Conv1D(32,1)(features2)\n    features2 = keras.layers.BatchNormalization()(features2)\n    features2 = keras.layers.Activation('relu')(features2)\n    \n    features3 = keras.layers.BatchNormalization()(features3)\n    features3 = keras.layers.Conv1D(32,1)(features3)\n    features3 = keras.layers.BatchNormalization()(features3)\n    features3 = keras.layers.Activation('relu')(features3)\n    \n    features = tf.concat([features3,features1,features2], axis = 1)\n    \n    return points,features,mask\n     \ndef get_particle_net(num_classes, input_shapes):\n    r\"\"\"ParticleNet model from `\"ParticleNet: Jet Tagging via Particle Clouds\"\n    <https:\/\/arxiv.org\/abs\/1902.08570>`_ paper.\n    Parameters\n    ----------\n    num_classes : int\n        Number of output classes.\n    input_shapes : dict\n        The shapes of each input (`points`, `features`, `mask`).\n    \"\"\"\n    setting = _DotDict()\n    setting.num_class = 6\n    # conv_params: list of tuple in the format (K, (C1, C2, C3))\n    setting.conv_params = [\n        (8, (64, 64, 64)),\n        (8, (96, 96, 96)),\n        (8, (128, 128, 128)),\n        ]\n    # conv_pooling: 'average' or 'max'\n    setting.conv_pooling = 'average'\n    # fc_params: list of tuples in the format (C, drop_rate)\n    setting.fc_params = [(128, 0.1)]\n    setting.num_points = 55\n    \n    points1 = keras.Input(name='points1', shape=(50,2))\n    features1 = keras.Input(name='features1', shape=(50,20)) \n    mask1 = keras.Input(name='mask1', shape=(50,1)) \n\n    points2 = keras.Input(name='points2', shape=(5,2))\n    features2 = keras.Input(name='features2', shape=(5,11)) \n    mask2 = keras.Input(name='mask2', shape=(5,1))\n    \n    points = tf.concat([points1,points2], axis = 1)\n    mask = tf.concat([mask1,mask2], axis = 1)\n    \n    features11 = keras.layers.BatchNormalization()(features1)\n    features11 = keras.layers.Conv1D(32,1)(features11)\n    features11 = keras.layers.BatchNormalization()(features11)\n    features11 = keras.layers.Activation('relu')(features11)\n    \n    features21 = keras.layers.BatchNormalization()(features2)\n    features21 = keras.layers.Conv1D(32,1)(features21)\n    features21 = keras.layers.BatchNormalization()(features21)\n    features21 = keras.layers.Activation('relu')(features21)\n    \n    features = tf.concat([features11,features21], axis = 1)\n\n    outputs = _particle_net_base(points, features, mask, setting, name='ParticleNet')\n\n    return keras.Model(inputs=[points1, features1, mask1, points2, features2, mask2], outputs=outputs, name='ParticleNet')\n\n\ndef get_particle_net_lite(num_classes, input_shapes):\n    r\"\"\"ParticleNet-Lite model from `\"ParticleNet: Jet Tagging via Particle Clouds\"\n    <https:\/\/arxiv.org\/abs\/1902.08570>`_ paper.\n    Parameters\n    ----------\n    num_classes : int\n        Number of output classes.\n    input_shapes : dict\n        The shapes of each input (`points`, `features`, `mask`).\n    \"\"\"\n    setting = _DotDict()\n    setting.num_class = num_classes\n    # conv_params: list of tuple in the format (K, (C1, C2, C3))\n    setting.conv_params = [\n        (7, (32, 32, 32)),\n        (7, (64, 64, 64)),\n        ]\n    # conv_pooling: 'average' or 'max'\n    setting.conv_pooling = 'average'\n    # fc_params: list of tuples in the format (C, drop_rate)\n    setting.fc_params = [(128, 0.1)]\n    setting.num_points = input_shapes['points'][0]\n\n    points = keras.Input(name='points', shape=input_shapes['points'])\n    features = keras.Input(name='features', shape=input_shapes['features']) if 'features' in input_shapes else None\n    mask = keras.Input(name='mask', shape=input_shapes['mask']) if 'mask' in input_shapes else None\n    outputs = _particle_net_base(points, features, mask, setting, name='ParticleNet')\n\n    return keras.Model(inputs=[points, features, mask], outputs=outputs, name='ParticleNet')","e1f74591":"PNet = get_particle_net(1,2)\n\nPNet.summary()","1aef4e42":"def roc_auc_score(y_true, y_pred):\n    \"\"\" ROC AUC Score.\n    Approximates the Area Under Curve score, using approximation based on\n    the Wilcoxon-Mann-Whitney U statistic.\n    Yan, L., Dodier, R., Mozer, M. C., & Wolniewicz, R. (2003).\n    Optimizing Classifier Performance via an Approximation to the Wilcoxon-Mann-Whitney Statistic.\n    Measures overall performance for a full range of threshold levels.\n    Arguments:\n        y_pred: `Tensor`. Predicted values.\n        y_true: `Tensor` . Targets (labels), a probability distribution.\n    \"\"\"\n    with tf.name_scope(\"RocAucScore\"):\n        pos = tf.boolean_mask(y_pred, tf.cast(y_true, tf.bool))\n        neg = tf.boolean_mask(y_pred, ~tf.cast(y_true, tf.bool))\n        pos = tf.expand_dims(pos, 0)\n        neg = tf.expand_dims(neg, 1)\n        # original paper suggests performance is robust to exact parameter choice\n        gamma = 0.2\n        p     = 2\n        difference = tf.zeros_like(pos * neg) + pos - neg - gamma\n        masked = tf.boolean_mask(difference, difference < 0.0)\n        return tf.reduce_sum(tf.pow(-masked, p))","72ea364b":"pred = [[0.0, 0.6, 0.3]]\ntrue = [[0, 0, 0]]\ny_pred = tf.cast(pred, dtype = tf.float32)\ny_true = tf.cast(true, dtype = tf.float32)\n\na = tf.keras.losses.CategoricalCrossentropy()\na(y_true, y_pred).numpy()","09524548":"roc_auc_score(y_true, y_pred)","c01073cd":"a = np.array([1,2,3,4,5,6,7])\nb = np.array([[1,1],[1,1],[1,1],[1,4],[1,4],[1,4],[1,4]])","0fe2439a":"b[a == 3] = [6,9]","de87f3bf":"b","4712b64b":"def getBin(value, bins):\n    \"\"\"\n    Get the bin of \"values\" in axis \"bins\".\n    Not forgetting that we have more bin-boundaries than bins (+1) :)\n    \"\"\"\n    for index, bin in enumerate (bins):\n        # assumes bins in increasing order\n        if value < bin:\n            return index-1            \n    #print (' overflow ! ', value , ' out of range ' , bins)\n    return -1","c00c1bfb":"value = 2\nx_axis = [15, 20, 26, 35, 46, 61, 80, 106, 141, 186, 247, 326, 432, 571, 756, 1000]","6a1e67ab":"getBin(value, x_axis)","d1ad8c5d":"l1 = np.array([False, False, True])\nl2 = np.array([False, True, True])","87f0bb57":"np.sum((l1 + l2)*1)","b9867b8c":"e = np.exp(np.linspace(np.log(0.01), np.log(2.5), 8))\ne","9ecc83d5":"np.flip((e*-1))","d5f5c9ad":"classes = ['isB','isBB','isLeptonicB','isC','isUDS','isG']\nglobal_branches = ['jet_pt', 'jet_eta',\n                                'nCpfcand','nNpfcand',\n                                'nsv','npv',\n                                'TagVarCSV_trackSumJetEtRatio',\n                                'TagVarCSV_trackSumJetDeltaR',\n                                'TagVarCSV_vertexCategory',\n                                'TagVarCSV_trackSip2dValAboveCharm',\n                                'TagVarCSV_trackSip2dSigAboveCharm',\n                                'TagVarCSV_trackSip3dValAboveCharm',\n                                'TagVarCSV_trackSip3dSigAboveCharm',\n                                'TagVarCSV_jetNSelectedTracks',\n                                'TagVarCSV_jetNTracksEtaRel']","29dd73a7":"pt_min = 15\npt_max = 1000\neta_bound = 2.5\n\nfor n, i in enumerate(classes):\n    print('Verification for '+ i)\n    print(n)\n    \n    #flavor_pt1 = df1['jet_pt'][df1[classes] == 1]\n    #flavor_eta1 = df1['jet_eta'][df1[classes] == 1]\n    #flavor_pt2 = df2['jet_pt'][df2[classes] == 1]\n    #flavor_eta2 = df2['jet_eta'][df2[classes] == 1]\n    \n    for j in global_branches:\n        print('Evaluating the variable '+ j)\n        #flavor_branch1 = df1[global_branches][df1[classes] == 1]\n        #flavor_branch2 = df2[global_branches][df2[classes] == 1]\n        \n        #flavor_branch1 = flavor_branch1[(flavor_pt1 > pt_min) & (flavor_pt1 < pt_max) & (abs(flavor_eta1) < eta_bound)]\n        #flavor_branch2 = flavor_branch2[(flavor_pt2 > pt_min) & (flavor_pt2 < pt_max) & (abs(flavor_eta2) < eta_bound)]\n        \n        #KS_test = scipy.stats.ks_2samp(flavor_branch1, flavor_branch2)\n        #print(KS_test)","228434a6":"# Comparison between PyTorch and Tensorflow implementation of ParticleNet\n\nThis notebook is a toolbox used for showing my final Tensorflow implementation of ParticleNet. For more information about the model, read \"ParticleNet: Jet Tagging via Particle Clouds\" (https:\/\/arxiv.org\/abs\/1902.08570).\n\nThe PyTorch\/Torch implementation comes from the autors (see https:\/\/github.com\/hqucms\/weaver). In this context, I compare the AK4 jets version only.\n\nThe number of trainable weights for Torch is 232,292 and for Tensorflow 232,612. The diffence might come from a different definition of the Batch Normalization or the usage of biais. I might check this in the future but the difference is negligable.","a4d6cf53":"# Mini-toolbox\n\nDetails coming soon... ;)"}}