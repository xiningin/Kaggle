{"cell_type":{"7d56966b":"code","1e10dc29":"code","ed7f998f":"code","643fa53e":"code","fe8716e5":"code","0045e209":"code","9dd1886c":"code","d5fdd3a6":"code","ada34f47":"code","7df22f65":"code","e417eb54":"code","a4518633":"code","909c517d":"code","ce3127b8":"code","b919d370":"code","2c58e6f7":"code","83c7c22d":"code","992fcb69":"code","d671be76":"code","b306f617":"code","edb884df":"code","cc0adc0a":"code","f90d53e7":"code","ce86b77b":"code","a1841b97":"code","c97a3fc1":"code","6bc4298c":"code","c3c1e749":"code","cec71803":"code","90ac83a9":"code","e9e06f5c":"code","f6066f37":"code","4e3ebf86":"code","e694b68a":"code","a2522989":"code","3fffac07":"code","a962253f":"code","058ca926":"code","0a6eea24":"code","f5d46b91":"code","1ae739cd":"code","74ab4323":"code","fc3665d1":"code","e5f9391d":"code","5d92ed99":"code","0c25a1d4":"code","326f98a1":"code","8fcd7055":"code","4fe37415":"code","289dfac7":"code","fcaffb0f":"code","ecc348b0":"code","5f29ca7c":"code","dd10d20a":"code","70116719":"code","9879585e":"code","bf80c699":"code","744e2bb0":"code","78929b07":"code","155f7c58":"code","f52d682b":"code","772992e6":"code","a00189bd":"code","84ccfea4":"code","33365d20":"code","18878dd6":"code","cbd25b11":"code","0f5e6277":"code","4690e99b":"code","518d03ac":"code","55a652da":"code","5300e7b5":"code","03efce54":"code","ffe773bd":"code","8d11be8a":"code","92236f71":"code","f35ca4fa":"code","e43f647a":"code","d431a2a2":"code","dc3b41f3":"code","5c61c36b":"code","6672c0e4":"code","d35df9da":"code","ffc68b17":"markdown","31450e6b":"markdown","58c67664":"markdown","d04a8ef9":"markdown","05710664":"markdown","b167ad1b":"markdown","d4c86f98":"markdown","337f8ee8":"markdown","5019ab40":"markdown","e1d64ed2":"markdown","7d142f8c":"markdown","22b2f6e1":"markdown","0f29b4e7":"markdown","7ed33439":"markdown","c7a46791":"markdown","1edb9312":"markdown","ba54b451":"markdown","53492527":"markdown","3692e262":"markdown","f8319905":"markdown","0cf3b0f6":"markdown","68106b9c":"markdown","dc5d15dd":"markdown","7bcc0711":"markdown","ebf833c4":"markdown","4884f002":"markdown","85091691":"markdown","35039422":"markdown","476d613c":"markdown","903249ee":"markdown","55d19fd6":"markdown","0b02d9a0":"markdown","68c871ec":"markdown","365b548e":"markdown","46acb63b":"markdown","0deee548":"markdown","11bf9abe":"markdown","a35f7a19":"markdown","dfeafef5":"markdown","e050cc07":"markdown","8897cc0a":"markdown","f7bbc227":"markdown"},"source":{"7d56966b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))\n\n# Any results you write to the current directory are saved as output.","1e10dc29":"from pandas import Series,DataFrame\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nfrom sklearn.cross_validation import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nsns.set_style('whitegrid')\n\n%matplotlib inline","ed7f998f":"###Loading the data\ntitanic_df = pd.read_csv('..\/input\/train.csv')\n","643fa53e":"titanic_df.head(10)","fe8716e5":"titanic_df.info()","0045e209":"titanic_df.shape","9dd1886c":"titanic_df['PassengerId'].nunique()","d5fdd3a6":"###Gender Plot\nsns.factorplot('Sex',data=titanic_df,kind='count')\n\n","ada34f47":"### Class plot\nsns.factorplot('Pclass',data=titanic_df,kind='count')","7df22f65":"sns.factorplot('Pclass',data=titanic_df,hue='Sex',kind='count')\n","e417eb54":"# Set style of scatterplot\nsns.set_context(\"notebook\", font_scale=1.1)\nsns.set_style(\"ticks\")\n\n# Create scatterplot of dataframe\nsns.lmplot('Age', # Horizontal axis\n           'Fare', # Vertical axis\n           data=titanic_df, # Data source\n           fit_reg=False, # Don't fix a regression line\n           hue=\"Survived\", # Set color\n           scatter_kws={\"marker\": \"D\",\"s\": 50}) # S marker size\n\n# Set title\nplt.title('Fare ')\n\n# Set x-axis label\nplt.xlabel('Age')\n\n# Set y-axis label\nplt.ylabel('Fare')","a4518633":"#Creating Child as a feature\ndef titanic_children(passenger):\n    \n    age , sex = passenger\n    if age <16:\n        return 'child'\n    else:\n        return sex\n\ntitanic_df['person'] = titanic_df[['Age','Sex']].apply(titanic_children,axis=1)\n        ","909c517d":"### Plotting a graph to check the ratio of male,female and children in each category of class\n\nsns.factorplot('Pclass',data=titanic_df,hue='person',kind='count')","ce3127b8":"###Mean age of the passengers\ntitanic_df['Age'].mean()","b919d370":"as_fig = sns.FacetGrid(titanic_df,hue='Pclass',aspect=5)\n\nas_fig.map(sns.kdeplot,'Age',shade=True)\n\noldest = titanic_df['Age'].max()\n\nas_fig.set(xlim=(0,oldest))\n\nas_fig.add_legend()","2c58e6f7":"titanic_df['Age'] = titanic_df['Age'].fillna(titanic_df['Age'].mean())","83c7c22d":"sns.factorplot('Embarked',data=titanic_df,hue='Pclass',kind='count')","992fcb69":"\ntitanic_df['Alone'] = titanic_df.Parch + titanic_df.SibSp\n","d671be76":"## if Alone value is >0 then they are with family else they are Alone\n\ntitanic_df['Alone'].loc[titanic_df['Alone']>0] = 'With Family'\ntitanic_df['Alone'].loc[titanic_df['Alone'] == 0] = 'Without Family'\n","b306f617":"#Let us visualise the Alone column\n\nsns.factorplot('Alone',kind='count',data=titanic_df)","edb884df":"# let us see who are alone according to class\nsns.factorplot('Alone',kind='count',data=titanic_df,hue='Pclass')","cc0adc0a":"sns.factorplot('Survived',data=titanic_df,kind='count')","f90d53e7":"sns.factorplot('Survived',data=titanic_df,kind='count',hue='Pclass')","ce86b77b":"sns.factorplot('Survived',data=titanic_df,kind='count',hue='Sex')","a1841b97":"sns.factorplot('Pclass','Survived',data=titanic_df,hue='Sex')","c97a3fc1":"sns.factorplot('Pclass','Survived',data=titanic_df,hue='Alone')","6bc4298c":"sns.lmplot('Age','Survived',data=titanic_df,logistic=True)","c3c1e749":"sns.lmplot('Age','Survived',data=titanic_df,hue='Pclass',logistic=True)","cec71803":"sns.lmplot('Age','Survived',data=titanic_df,hue='Sex',logistic=True)","90ac83a9":"sns.lmplot('Age','Survived',data=titanic_df,hue='Alone',logistic=True)","e9e06f5c":"sns.lmplot('Age','Survived',data=titanic_df,hue='Embarked',logistic=True)","f6066f37":"AgeBucket = ['less2', '2-18', '18-35','35-55','55-65', '65plus']\nbins = [-1, 2, 18, 35,55, 65, np.inf]\n    #combined['AgeBin'] = pd.cut(combined['Age'], bins, labels=names)\n    \ntitanic_df['AgeBucket'] = pd.cut(titanic_df['Age'],bins, labels = AgeBucket)\nsns.factorplot('Survived',data=titanic_df,kind='count',hue='AgeBucket')\n","4e3ebf86":"bins = [-1, 20, 40, 80, 120, 200,300, np.inf]\nFareBucket = ['0-20',' 20-40', '40-80', '80-120', '120-200','200-300','300+']\ntitanic_df['FareBucket'] = pd.cut(titanic_df['Fare'], bins,\n                                 labels=FareBucket).astype('str')\nsns.factorplot('Survived',data=titanic_df,kind='count',hue='FareBucket')\n","e694b68a":"# importing pandas, numpy\nimport pandas as pd\nimport numpy as np\nimport random","a2522989":"# importing packages for plots \nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nfrom pandas import read_csv\n#from sklearn.preprocessing import Imputer\nimport numpy","3fffac07":"# reading the training dataset\ntitanic_df = pd.read_csv(\"..\/input\/train.csv\")","a962253f":"# analyzing missing values\n\ndef missing_value_analysis(data,columns):\n    df = data[columns]\n    missing_value_perc = df.isnull().sum() *100 \/df.shape[0] \n    missing_value_perc = pd.DataFrame({'id':missing_value_perc.index, 'missing_value_perc':missing_value_perc.values})\n\n    missing_data = missing_value_perc[missing_value_perc.missing_value_perc > 0]\n    sns.set(style=\"whitegrid\")\n    plt.figure(figsize=(10,8))\n    ax = sns.barplot(x=\"id\",y=\"missing_value_perc\", data=missing_data, color='steelblue')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=90)\n    print(missing_data.shape)\n    return missing_value_perc","058ca926":"missing_value_analysis(titanic_df,columns=['PassengerId', 'Survived', 'Pclass', 'Name', 'Sex', 'Age', 'SibSp',\n       'Parch', 'Ticket', 'Fare', 'Cabin', 'Embarked'])","0a6eea24":"# filling mising values of age with mean of columns\n\ntitanic_df.iloc[:,5]=titanic_df.iloc[:,5].fillna(titanic_df.mean(axis=0)[3])","f5d46b91":"# filling nans of embarked with S, as it is the mostly used value\n\ntitanic_df['Embarked']=titanic_df['Embarked'].fillna('S')\n\n# dropping cabin column as it ahs 77% of nas\n\ntitanic_df = titanic_df.drop([\"Cabin\"],axis=1)","1ae739cd":"# lets check for any missing values\ntitanic_df.isnull().any()","74ab4323":"# dropping the columns which are not useful as they are customer specific \n\ntitanic_df = titanic_df.drop([\"Ticket\"],axis=1)\ntitanic_df = titanic_df.drop([\"Name\"],axis=1)\n","fc3665d1":"# encoding to convert string columns to numeric\ntitanic_df['Sex']=titanic_df[\"Sex\"].map({'female':1, 'male':0})\ntitanic_df['Embarked']=titanic_df['Embarked'].map({'S':0, 'C':1,'Q':2})","e5f9391d":"# Importing kmeans pacjage for clustering\nfrom sklearn import preprocessing\nfrom sklearn.cluster import KMeans\n\nrandom.seed(30)","5d92ed99":"X = np.array(titanic_df.drop(['Survived'], 1).astype(float))\nX = preprocessing.scale(X)\ny = np.array(titanic_df['Survived'])","0c25a1d4":"X","326f98a1":"# applying clustering algorithm for four clusters\n\nclf = KMeans(n_clusters=4,random_state=30)\nclf.fit(X)\n","8fcd7055":"# convering labels to dataframe\npp1=clf.fit(X)\npredicted_class1=pp1.labels_\npredicted_class1=pd.DataFrame(predicted_class1)","4fe37415":"predicted_class1=predicted_class1.rename(columns={0: 'output'})\npredicted_class1=predicted_class1.reset_index()\ntitanic_df=titanic_df.reset_index()\ndata1=titanic_df.merge(predicted_class1,on='index')","289dfac7":"# Getting the number of passengers in different clusters\ndata1['output'].value_counts()","fcaffb0f":"# Getting the surivival rates in various clusters\ndata1[['output','Survived']].groupby('output').agg({'Survived':'mean','output':'count'})","ecc348b0":"def distplot(data, columns, title=None, xlabel = None, ylabel = None, \n             num_bins = 30, edge_color = 'SteelBlue', fill_color = None,\n             line_width = 1.0, fig_size = (8,6), title_font = 14, adjust_top = 0.85, adjust_wspace = 0.3):\n    df = data\n    fig = plt.figure(figsize = fig_size)\n    if title != None:\n        title = fig.suptitle(title, fontsize = title_font)\n    fig.subplots_adjust(top = adjust_top, wspace = adjust_wspace)\n\n    ax1 = fig.add_subplot(1, 1, 1)\n    if xlabel != None:\n        ax1.set_xlabel(xlabel)\n    if ylabel != None:\n        ax1.set_ylabel(ylabel) \n    sns.kdeplot(df[columns], ax=ax1, shade=True, color = edge_color)","5f29ca7c":"# lets visualize teh cluster across the survival rates\ndistplot(data=data1,columns=['output','Survived'],xlabel = 'clusters',ylabel='survival')","dd10d20a":"cluster0=data1[data1['output']==0]\ncluster1=data1[data1['output']==1]\ncluster2=data1[data1['output']==2]\ncluster3=data1[data1['output']==3]","70116719":"# Cluster0\ncluster0['Sex'].value_counts(normalize=True)\n","9879585e":"cluster0['Pclass'].value_counts(normalize=True)\n","bf80c699":"sns.distplot(cluster0['Age'], color=\"r\",)  \n","744e2bb0":"kwargs={'cumulative': True}\nsns.distplot(class2['Age'], color=\"r\",hist_kws=kwargs,bins=10)  ","78929b07":"# Cluster1\ncluster1['Sex'].value_counts(normalize=True)\n","155f7c58":"cluster1['Pclass'].value_counts(normalize=True)\n","f52d682b":"sns.distplot(cluster1['Age'], color=\"r\",)  \n","772992e6":"kwargs={'cumulative': True}\nsns.distplot(class1['Age'], color=\"r\",hist_kws=kwargs,bins=10)  ","a00189bd":"# Cluster2\ncluster2['Sex'].value_counts(normalize=True)\n","84ccfea4":"cluster2['Pclass'].value_counts(normalize=True)\n","33365d20":"sns.distplot(cluster2['Age'], color=\"r\",)  \n","18878dd6":"kwargs={'cumulative': True}\nsns.distplot(class2['Age'], color=\"r\",hist_kws=kwargs,bins=10)  ","cbd25b11":"# Cluster3\ncluster3['Sex'].value_counts(normalize=True)\n","0f5e6277":"cluster3['Pclass'].value_counts(normalize=True)\n","4690e99b":"sns.distplot(cluster3['Age'], color=\"r\",)  \n","518d03ac":"kwargs={'cumulative': True}\nsns.distplot(class3['Age'], color=\"r\",hist_kws=kwargs,bins=10)  ","55a652da":"from sklearn.preprocessing import Imputer","5300e7b5":"from sklearn.preprocessing import scale","03efce54":"def clean_data(data):\n    data[\"Fare\"] = data[\"Fare\"].fillna(data[\"Fare\"].dropna().median())\n    data[\"Age\"] = data[\"Age\"].fillna(data[\"Age\"].dropna().median())\n    \n    data.loc[data[\"Sex\"] == \"male\", \"Sex\"] = 0\n    data.loc[data[\"Sex\"] == \"female\", \"Sex\"] = 1\n    \n    data[\"Embarked\"] = data[\"Embarked\"].fillna(\"S\")\n    data.loc[data[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n    data.loc[data[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n    data.loc[data[\"Embarked\"] == \"Q\", \"Embarked\"] = 2","ffe773bd":"\ntitanic_df['Alone'] = titanic_df.Parch + titanic_df.SibSp\n","8d11be8a":"## if Alone value is >0 then they are with family else they are Alone\n\ntitanic_df['Alone'].loc[titanic_df['Alone']>0] = 'With Family'\ntitanic_df['Alone'].loc[titanic_df['Alone'] == 0] = 'Without Family'\n","92236f71":"# Check score with Decision Tree Model\nimport pandas as pd\nfrom sklearn import tree\ntrain = pd.read_csv(\"\/kaggle\/input\/train.csv\")\ntrain['Alone'] = train.Parch + train.SibSp\ntrain['Alone'].loc[train['Alone']>0] = 1\ntrain['Alone'].loc[train['Alone'] == 0] = 0\nclean_data(train)","f35ca4fa":"target = train[\"Survived\"].values\nfeatures = train[[\"Pclass\", \"Age\", \"Fare\", \"Embarked\", \"Sex\", \"SibSp\", \"Parch\",'Alone']].values\ndecision_tree = tree.DecisionTreeClassifier(random_state = 42)\ndecision_tree_ = decision_tree.fit(features, target)\nprint(decision_tree_.score(features, target)) ","e43f647a":"# Making the Decision Tree more generalized to reduce overfitting\nfrom sklearn import model_selection\ngeneralized_tree = tree.DecisionTreeClassifier(\n                    random_state = 1,\n                    max_depth = 7,\n                    min_samples_split = 2)\ngeneralized_tree_ = generalized_tree.fit(features, target)\nscores = model_selection.cross_val_score(generalized_tree, features, target, scoring = 'accuracy', cv = 50)\nprint(scores)\nprint(scores.mean())","d431a2a2":"import graphviz\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn.preprocessing import StandardScaler as scaler\n\ndata = export_graphviz(DecisionTreeClassifier(max_depth=3).fit(features, target), out_file=None, \n                       feature_names = ['Pclass', 'Age', 'Fare', 'Embarked', 'Sex', 'SibSp', 'Parch','Alone'],\n                       class_names = ['Survived (0)', 'Survived (1)'], \n                       filled = True, rounded = True, special_characters = True)\n# we have intentionally kept max_depth short here to accommodate the entire visual-tree\ngraph = graphviz.Source(data)\ngraph","dc3b41f3":"#Perform Grid Search to tune hyperparameters of the Random Forest model\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state = 1)\nn_estimators = [1740]\nmax_depth = [6]\nmin_samples_split = [4 ]\nmin_samples_leaf = [5] \noob_score = ['True']\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth, min_samples_split = min_samples_split, min_samples_leaf = min_samples_leaf, oob_score = oob_score)\n\ngridF = GridSearchCV(forest, hyperF, verbose = 1, n_jobs = 4)\nbestF = gridF.fit(features, target)","5c61c36b":"print(bestF)","6672c0e4":"# Check score with Random Forest Model having the best hyperparameters\nfrom sklearn.ensemble import RandomForestClassifier\n\nr_forest = RandomForestClassifier(criterion='gini',bootstrap=True,\n                                    n_estimators=1745,\n                                    max_depth=9,\n                                    min_samples_split=6,\n                                    min_samples_leaf=6,\n                                    max_features='auto',\n                                    oob_score=True,\n                                    random_state=123,\n                                    n_jobs=-1,\n                                    verbose=0)\nrf_clf = r_forest.fit(features, target)\nprint(rf_clf.score(features, target)) ","d35df9da":"rf_clf.oob_score_","ffc68b17":"Visualizing cluster 4\n\n* This cluster has 38% of survival rate and it presents 16% passengers of titanic\n* Majority of male population,62% of passengers in cluster 2 are male and 38% female\n* No passenger from class 1\n* Majority of class 3 passengers,89% of passengers are from class 3 and remaining from class 2\n* Majority of people from 30\u2019s","31450e6b":"We observe that the overall score has decreased but this generalized tree avoids overfitting. We can visualize the tree picture using **graphviz**.","58c67664":"Class really affects the survival rate. There is a huge difference in the proportion of the survived people within the different class. Class 1 pepole are more probable to survive.","d04a8ef9":"This gives us an insight that there are quite a few males than females in 3rd class. ","05710664":"Lets try Clustering on the Data given to understand if we can use some unsupervised methods to get inferences out of the data","b167ad1b":"Soem Feature Engineering: creating features to find more pattern using given features","d4c86f98":"Now, we will use the above function for finding the score of prediction for survival. Below code takes \u201cSurvived\u201d as dependent variable and rest of the features as independent variables to fit the entire thing into regression model and finds the accuracy of the model.","337f8ee8":"So, the decision tree produces 97.98% accuracy which is a better-fit model. We can make this decision tree model more generalized by using a generalized decision tree and using cross validation (50 fold). The model improvement also indicates at reducing overfitting, apart from just enhancing accuracy.\n\n**(2B) Decision Tree Model with 50-Fold Cross Validation :**","5019ab40":"# Modelling\nUsing Simple Decision Tree Model to learn and predict\n","e1d64ed2":"# Data Analysis","7d142f8c":"If you use Cluster 0 for predicting survived pepople, cluster 1 for predicting them as not survived and Cluster 2 as Not survived Pepole and CLuster 3 as too Not survived Poeple - Overall Accuracy of this Unsupervised Model would be around- (0.62*221+0.78*397+0.51*133+0.62*140)\/891 = 67.5 % Accuracy","22b2f6e1":"Highest Survival among age group 18-35. Among older srvival is good between 55-65 but almost no survival for ****65+ ","0f29b4e7":"It is intereting to see that most of the passengers boarded at Queenstown are from 3rd class. And many passengers boarded at Southhampton. Will this help in making predictions? ","7ed33439":"Let's Deep dive to understand the Titanic data set\n","c7a46791":"From the above graphs, we can infer that there are more number of passengers with a age group of 20 to 40 in all the three classes. There is also a shift in distribution of age when we move from class 1 to class 3.","1edb9312":"Discussion Point: Overfitting ?","ba54b451":"S= SouthAmption, C = Charbour , Q= Queenstown","53492527":"# Supervised Modelling","3692e262":"The survival rates are higher if they are with family.  Let us check how Age playes a role in the survival rate.","f8319905":"The above graphs shows that  older the passenger, lesser the chance of survival. ","0cf3b0f6":"Let's dig deeper into data and find out what factors helped survival.","68106b9c":"Some Feature Engineering!!\nLet's check who are with family and who are alone\nThis can be found by adding Parch and Sibsp columns","dc5d15dd":"# Explain decision path: \n\nLet see the root nodes where most of the population lies and explain the decision paths-\n\nSex= Male ---> Age >6.5 ----> PClass>1 ----> Not Survived   (Correctly Classified - 383, Incorrectly Classified- 50) \n\n\n* Sex= Female ---> PClass =1 or 2 ----> Age>=2.5 ---->  Survived  (Correctly Classified - 160, Incorrectly Classified- 8) \n\n","7bcc0711":"The above graph shows that the survival rate for male is very low nevertheless of the class. And, the survival rate is less for the 3rd class passengers.","ebf833c4":"**Understanding Basics about the data set **\n1. What are the attributes in the data set ?\n2. What is the shape (number of rows and columns) of data set ? \n3. What are data types for the different attributes?\n4. What a sample looks like in Dataset?\n5. Investigating the different attributes ?","4884f002":"Shows more male passengers than female ","85091691":"# Unsupervised Modelling","35039422":"0= Not Survived, 1= Survived","476d613c":"Distribution of passengers across the 4 cluster\n\ncluster 0- 24.8%\n\nCluster 1- 44.5%\n\nCluster 2- 14.9%\n\nCluster 3- 15.7%\n\nSurvival Rate amongst the 4 clusters:\n\ncluster 0- 62%\n\ncluster 1- 22%\n\ncluster 2- 49%\n\ncluster 3- 38%","903249ee":"**(2) Random Forest :**\n\nNext, we will try fitting the problem with a Random Forest model to improve the accuracy of the result. Since Random Forest is a ensemble model consisting of multiple decision trees, it will improve the performance of the model. We will first tune the hyperparameters and then will use the model with best hyperparameters.","55d19fd6":"More number of males, females and children in the class three.\n","0b02d9a0":"Checking if the class had any effect in the survival rate\n","68c871ec":"Visualizing cluster 1\n\n\n* This cluster has 22% of survival rate and it presents 44% passengers of titanic\n* Majority of male population,80.5% of passengers in cluster 0 are male and 19.4% female\n* No passenger from class 1\n* Majority of class 3 passengers, 70% of passengers are from class 3 and remaining from class 2\n* Population distributed across the age\n","365b548e":"Visualizing cluster 2\n\n\n*This cluster has 48% of survival rate and it presents 15% passengers of titanic\n\n*63% of passengers of cluster 1 are female\n\n*Passengers from all classes\n\n*Majority of class 3 passengers\n\n*This class contains passenger below 20\u2019s\n\n*65% of passengers are from class 3, 27 % from class 2 and less than 1% from class 1\n","46acb63b":"Who were the passengers on the titanic? (What age, gender, class etc)\n","0deee548":"Visualizing cluster 0\n\n* This cluster has 62% of survival rate and it presents 25% passengers of titanic\n* 45% of passengers of cluster 1 are female\n* Almost all passengers of class 1. No Class 3 passengers\n* 80% of the passengers below 35 years of age and 60 % of them below 20.","11bf9abe":"Now checking the Age distribution in various class","a35f7a19":"Shockingly, the number of passengers boarded at Southhampton are more compared to Cherbourg and Queenstown but the survival rate is high for Cherbour passengers than Southhampton. So there is a chance that Embarked  helps in prediction.","dfeafef5":"Interesting! More passengers are from class Three.","e050cc07":"***Importing missing values of column- age,embarked and cabin***","8897cc0a":"Very good survival rate for passengers paid fare between GBP 80+ ","f7bbc227":"Data Pre-Processing for Modelling\n\nWe will use below function for data pre-processing. It finds the cells in the data set where \u201cFare\u201d and \u201cAge\u201d have no value (blank cells) and fills them with median values of those columns respectively. For convenience, it marks \u201cmale\u201d passengers with \u201c0\u201d and female passengers with \u201c1\u201d. Also, it finds the blank cells in \u201cEmbarked\u201d column and fills them with \u201cS\u201d (Southampton by default). For convenience, it marks \u201cS\u201d (Sothampton embarked) with \u201c0\u201d, \u201cC\u201d with 1 and \u201cQ\u201d with 2 respectively."}}