{"cell_type":{"2c88c795":"code","ab4dbd27":"code","5f3fab66":"code","fb7144aa":"code","9304b322":"code","80f61bbf":"code","7136aa20":"code","dfe9f051":"code","f664ac05":"code","e3a80713":"code","505f1c3b":"code","00923ff8":"code","1dc10dd6":"code","d347eefb":"code","280969c3":"code","a306f8de":"code","01999807":"code","ae872ddc":"code","89c568b9":"code","b2825280":"code","4f12cdc2":"code","9560b720":"code","97135dc9":"code","61b6b431":"code","ad95267e":"code","24e9d63e":"code","a9fc2462":"code","d6f95ee7":"code","0af6d03a":"code","cf435df6":"code","9764d047":"code","b198e05c":"code","0c80ef61":"code","8309de21":"code","31663a06":"code","2cd75f52":"code","224d4ef3":"code","3b03972d":"code","03394a2f":"code","582b9c18":"code","3e2152cb":"code","08196b8b":"code","6d5b519c":"code","818607d2":"code","7c71a120":"code","1955271e":"code","561652ab":"code","d7aac130":"code","f7c63e40":"code","85f13a00":"code","98f38342":"code","5ba6ed28":"code","4ac8b19c":"code","e508ce46":"code","43fed297":"code","e03681ff":"code","ac8642c7":"code","a8afcfac":"code","b8f9dea4":"code","925c7660":"code","362a44c0":"code","6bf2dcad":"code","b54536da":"code","8a5d3d7b":"code","ea4a29a4":"code","08c4dfff":"code","a3c32b1a":"code","d6848a83":"code","09871ff4":"code","5e4a1339":"code","d2566797":"code","6fbe7d9b":"code","6b79326f":"code","1ef33272":"code","ecb172ff":"code","3f0cdc2a":"code","c24aeb21":"code","6e5e63b8":"code","4523ba7f":"code","3b6e9e17":"code","e7f1a387":"code","cff94829":"code","9030ce11":"code","e95bc4c5":"code","f7c4068b":"code","3354431e":"code","19dd1b09":"code","44878e40":"code","82186aa4":"code","74fa828d":"code","99d0e8d5":"code","9e5b239f":"code","f5f9c7e8":"code","596d2871":"code","c2366109":"code","7150cb93":"code","90b40132":"code","21120112":"code","777c7176":"code","a6a98cf2":"code","1ed4556b":"code","d5cde458":"code","36794db2":"code","ae4e67dd":"code","2c53d263":"code","ad39d7f1":"code","279eeea2":"markdown","25aaba8a":"markdown","f9cdcbd9":"markdown","5805d7fe":"markdown","e17b1b4a":"markdown","5425e009":"markdown","1c442e21":"markdown","ef0e6ee2":"markdown","afe6cc11":"markdown","85e0a2bf":"markdown","8d463976":"markdown","229dcfbd":"markdown","f0dbda25":"markdown","65678e28":"markdown","a1ccd1c1":"markdown","b19ccd8f":"markdown","c57c5d6c":"markdown","438f3e69":"markdown","83f72e39":"markdown","6a9afcc7":"markdown","2fd7739c":"markdown","033ae227":"markdown","b9baeb3a":"markdown","9203379f":"markdown","a539a4cf":"markdown","1f4774d4":"markdown","5e5ccf02":"markdown","d54824f5":"markdown","e826bba2":"markdown","e96112fe":"markdown","71e14901":"markdown","642349d5":"markdown","0792e083":"markdown","d1096dfe":"markdown","d2c3591c":"markdown","ff0187c1":"markdown","c5bf58d4":"markdown","1b421d99":"markdown","b5c7bf36":"markdown"},"source":{"2c88c795":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ab4dbd27":"# reading dataset\ndf_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_train.head()","5f3fab66":"# get insights of data\nprint('Train set: ', df_train.shape)\nprint('Test set: ', df_test.shape)","fb7144aa":"# get info of training data sets\ndf_train.info()","9304b322":"# Define numerical feature and categoriacal Features\nnum_var = ['Age','Fare']\ncat_var = ['PassengerId','Survived','Pclass','Name','Sex','SibSp','Parch','Embarked']","80f61bbf":"df_train[cat_var] = df_train[cat_var].astype('category')\ndf_train.info()","7136aa20":"# describe numerical variables\ndf_train.describe()","dfe9f051":"# describe categorical variables\ndf_train.describe(exclude=['float64'])","f664ac05":"# get percentage null values\ndf_train.isnull().mean()*100","e3a80713":"# get percentage null values fro test set\ndf_test.isnull().mean()*100","505f1c3b":"# Describe\ndf_train.Cabin.describe()","00923ff8":"# Look for null values\ndf_train[~df_train['Cabin'].isnull()]['Cabin'].head()","1dc10dd6":"# Missing value imputaion in train set\n# fill null values with 'N'\ndf_train['Cabin'].fillna('N', inplace = True)\n\n# get he 1st character in the cabin column\ndf_train['Cabin'] = df_train['Cabin'].astype(str).str[0]\ndf_train['Cabin'].value_counts()","d347eefb":"# Missing value imputaion in test set\n# fill null values with 'N'\ndf_test['Cabin'].fillna('N', inplace = True)\n\n# get he 1st character in the cabin column\ndf_test['Cabin'] = df_test['Cabin'].astype(str).str[0]\ndf_test['Cabin'].value_counts()","280969c3":"# Missing value imputaion with median\ndf_train['Age']= df_train['Age'].fillna(df_train['Age'].median())\ndf_test['Age']= df_test['Age'].fillna(df_test['Age'].median())","a306f8de":"# check for missing values % in Age and Cabin columns (Train Set)\ndf_train[['Age','Cabin']].isnull().mean()*100","01999807":"# check for missing values % in Age and Cabin columns (Test Set)\ndf_test[['Age','Cabin']].isnull().mean()*100","ae872ddc":"# Look for null values\ndf_train['Embarked'].isnull().sum()","89c568b9":"df_train['Embarked'].value_counts(normalize = True)","b2825280":"# Missing value imputaion with mode\ndf_train['Embarked'] = df_train['Embarked'].fillna(df_train['Embarked'].mode()[0])","4f12cdc2":"# check missing value %\nprint('Missing value in Embarked of train set:', df_train['Embarked'].isnull().mean()*100)","9560b720":"# Missing value imputaion with median\ndf_test['Fare'].fillna(df_test['Fare'].median(), inplace = True)","97135dc9":"# check for missing values % in Age and Cabin columns (Test Set)\nprint('Missing value in Fare of test set:',df_test['Fare'].isnull().mean()*100)","61b6b431":"df_train.head()","ad95267e":"type(df_train)","24e9d63e":"# lets check missing values after all treatments\nprint('Missing values train set: ')\nprint(df_train.isnull().mean()*100)\nprint('\\n Missing values test set:')\nprint(df_test.isnull().mean()*100)","a9fc2462":"\nplt.figure(figsize=(15,4))\nplt.subplot(1,2,1)\nsns.countplot(x = 'Pclass', data = df_train, hue = 'Survived')\n\nplt.subplot(1,2,2)\nsns.countplot(x = 'Sex', data = df_train, hue = 'Survived')\n\nplt.show()","d6f95ee7":"#plot for Age\nplt.figure(figsize=(8,6))\nsns.violinplot(x= 'Survived', y= 'Age', data = df_train)\nplt.show()\n\n","0af6d03a":"#plot for exploration of fare \nplt.figure(figsize=(10,5))\nax = sns.kdeplot(df_train[\"Fare\"][df_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(df_train[\"Fare\"][df_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Not- Survived'])\nplt.title('Density Plot of Fare for Surviving Population and Deceased Population')\nax.set(xlabel='Fare')\nplt.xlim(-20,200)\nplt.show()","cf435df6":"plt.figure(figsize= (15,5))\nplt.subplot(1,2,1)\nsns.countplot(x = 'SibSp', data = df_train, hue = 'Survived')\n\nplt.subplot(1,2,2)\nsns.countplot(x = 'Parch', data = df_train, hue = 'Survived')\nplt.show()","9764d047":"df_train[['SibSp','Parch']] = df_train[['SibSp','Parch']].astype('int64')\ndf_test[['SibSp','Parch']] = df_test[['SibSp','Parch']].astype('int64')","b198e05c":"# Lets make a derived feature of family size\ndf_train['Family_Size']= df_train['SibSp']+df_train['Parch']+1\ndf_test['Family_Size']= df_test['SibSp']+df_test['Parch']+1","0c80ef61":"# count plot for Family_Size\nplt.figure(figsize = (10,6))\nsns.countplot(x = 'Family_Size', data = df_train, hue = 'Survived')\nplt.show()","8309de21":"# Plot for Embarker\nsns.countplot(x='Embarked', data=df_train, palette='Set1')\nplt.show()","31663a06":"df_train.info()","2cd75f52":"df_train.info()","224d4ef3":"# Converting family size into data type categoty\ndf_train['Family_Size'] = df_train['Family_Size'].astype('category')\ndf_test['Family_Size'] = df_test['Family_Size'].astype('category')","3b03972d":"# Creating dummy variables for the variable\ndf = pd.get_dummies(df_train, columns =['Pclass','Cabin','Embarked','Family_Size'], drop_first=True)\ndf.head()","03394a2f":"df.columns","582b9c18":"# Creating dummy variables for the variable\ndf_test = pd.get_dummies(df_test, columns =['Pclass','Cabin','Embarked','Family_Size'], drop_first=True)\ndf_test.head()","3e2152cb":"# drop columns\ndf.drop(['Name','SibSp', 'Parch','Ticket'], axis =1, inplace = True)\ndf_test.drop(['Name','SibSp', 'Parch','Ticket'], axis =1, inplace = True)","08196b8b":"# imputing Sex Column\n\ndf['Sex'] = df['Sex'].replace({'male': 1, 'female':0})\ndf_test['Sex'] = df_test['Sex'].replace({'male': 1, 'female':0})","6d5b519c":"df[df.columns] = df[df.columns].apply(pd.to_numeric)\ndf_test[df_test.columns] = df_test[df_test.columns].apply(pd.to_numeric)","818607d2":"### Checking the survival Rate\nprint('Survival Rate: ',  round(df['Survived'].mean()*100, 2))","7c71a120":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(df.corr(),annot = True)\nplt.show()","1955271e":"# Creating feature variables of X\nX = df.drop(['PassengerId','Survived'],1)\nX.head()","561652ab":"# Creating feature variables of y\ny = df['Survived']\ny.head()","d7aac130":"# Splitting the dataset into test and train not required as data already splited\nX_train, X_test, y_train, y_test = train_test_split(X,y, train_size =0.8, test_size =0.2, random_state = 100)","f7c63e40":"df_train.head()","85f13a00":"df_test.head()","98f38342":"from sklearn.preprocessing import StandardScaler","5ba6ed28":"df.head()","4ac8b19c":"# scale X_train\nscaler = StandardScaler()\nX_train[['Age','Fare']] = scaler.fit_transform(X_train[['Age','Fare']])\nX_train.head()","e508ce46":"# scale X_test\nX_test[['Age','Fare']] = scaler.transform(X_test[['Age','Fare']])\nX_test.head()","43fed297":"# scale df_test\ndf_test[['Age','Fare']] = scaler.transform(df_test[['Age','Fare']])\ndf_test.head()","e03681ff":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","ac8642c7":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 15)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","a8afcfac":"rfe.support_","b8f9dea4":"col = X_train.columns[rfe.support_]\ncol","925c7660":"X_train_rfe = X_train[col]","362a44c0":"import statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import accuracy_score, classification_report, precision_score, recall_score\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, roc_curve, auc","6bf2dcad":"# create function for stats logistic model \ndef sm_logregmodel(X_train_sm):\n    #Add constant\n    X_train_sm = sm.add_constant(X_train_sm)\n\n    # create a fitted model\n    logm = sm.GLM(y_train, X_train_sm, family = sm.families.Binomial())\n    res = logm.fit()\n    return res","b54536da":"# Function to calculate VIF\n# calculate VIF\ndef vif_calc(X):\n    vif = pd.DataFrame()\n    vif['Features'] = X.columns\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    vif['VIF'] = round(vif['VIF'],2)\n    vif = vif.sort_values(by='VIF', ascending = False)\n    return vif","8a5d3d7b":"# build 1st starts model\nlogm1= sm_logregmodel(X_train_rfe)\nprint(logm1.summary())","ea4a29a4":"# Loop to remove P value variables >0.05 in bstep mannen and update model\npvalue = logm1.pvalues[1:]\nwhile(max(pvalue)>0.05):\n    maxp_var = pvalue[pvalue == pvalue.max()].index\n    print('Removed variable:' , maxp_var[0], '    P value: ', round(max(pvalue),3))\n    \n    # drop variable with high p value\n    X_train_rfe = X_train_rfe.drop(maxp_var, axis = 1)\n    logm1 = sm_logregmodel(X_train_rfe)\n    pvalue = logm1.pvalues[1:]","08c4dfff":"# build model and check \nlogm2 = sm_logregmodel(X_train_rfe)\nprint(logm2.summary())\n\n# Check for VIF\nprint(vif_calc(X_train_rfe))","a3c32b1a":"# drop variable with high p value and update the model\nX_train_rfe.drop('Family_Size_5', axis = 1, inplace = True)\n#update model\nlogm3 = sm_logregmodel(X_train_rfe)\nprint(logm3.summary())\n\n# check VIF\nprint(vif_calc(X_train_rfe))","d6848a83":"# List down final model varibales and its coefficients\n\n# assign final model to lm_final\nlog_final = logm3\n\n# list down and check variables of final model\nvar_final = list(log_final.params.index)\nvar_final.remove('const')\nprint('Final Selected Variables:', var_final)\n\n# Print the coefficents of final varible\nprint('\\033[1m{:10s}\\033[0m'.format('\\nCoefficent for the variables are:'))\nprint(round(log_final.params,3))","09871ff4":"# getting the predicted values on the train set\nX_train_sm = sm.add_constant(X_train[var_final])\ny_train_pred = log_final.predict(X_train_sm)\ny_train_pred[:10]","5e4a1339":"# Reshaping the numpy array containing predicted values\ny_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","d2566797":"# Create a new dataframe containing the actual conversion flag and the probabilities predicted by the model\ny_train_pred_final = pd.DataFrame({'Converted':y_train.values, 'Conversion_Prob': y_train_pred})\ny_train_pred_final.head()","6fbe7d9b":"# function for ROC curve\ndef draw_roc( actual, probs ):\n    from sklearn.metrics import roc_curve\n    from sklearn.metrics import roc_auc_score \n    \n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs, drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score, color ='red' )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic curve')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n    \n    # fpr, tpr, thresholds = roc_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)\n    auc_score = roc_auc_score(actual, probs)\n    print('ROC AUC : ',round(auc_score,2))\n    optimal_idx = np.argmax(tpr - fpr)\n    optimal_threshold = thresholds[optimal_idx]\n    print(\"Threshold value is:\", round(optimal_threshold,2))\n\n#     return fpr,tpr, thresholds","6b79326f":"# plot Roc Curve\ndraw_roc(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)","1ef33272":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.Conversion_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","ecb172ff":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.Converted, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","3f0cdc2a":"# validate Optimal cut off point\nsns.set_style(\"whitegrid\")\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'], figsize=(14,6))\n# plot x axis limits\nplt.xticks(np.arange(0, 1, step=0.05), size = 12)\nplt.yticks(size = 12)\nplt.show()","c24aeb21":"cut_off = 0.37\ny_train_pred_final['final_predicted'] = y_train_pred_final.Conversion_Prob.map( lambda x: 1 if x > cut_off else 0)\n\ny_train_pred_final.head()","6e5e63b8":"# Precision and recall tradeoff\np, r, thresholds = precision_recall_curve(y_train_pred_final.Converted, y_train_pred_final.Conversion_Prob)\n\n# Slightly alter the figure size to make it more horizontal.\nplt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","4523ba7f":"# function to predict and get classification summary\n\ndef classification_model_metrics(logm, X, y, cut_off):\n    \n    # check variables of model\n    X_cols = list(logm.params.index)\n    X_cols.remove('const')\n    \n    # getting the predicted values on the train set\n#     var_final = X[log_final.params.index[1:]]\n    X_sm = sm.add_constant(X[X_cols])\n    y_pred = logm.predict(X_sm)\n    \n    # Reshaping the numpy array containing predicted values\n    y_pred = y_pred.values.reshape(-1)\n\n    # Create a new dataframe containing the actual conversion flag and the probabilities predicted by the model\n    y_pred_final = pd.DataFrame({'Converted':y.values, 'Conversion_Prob': y_pred})\n\n    # Prediction at cutoff\n    y_pred_final['predicted'] = y_pred_final.Conversion_Prob.map(lambda x: 1 if x > cut_off else 0)\n\n    # Classification Summary\n    from sklearn.metrics import classification_report\n    classification_summary = classification_report(y_pred_final.Converted, y_pred_final.predicted, digits = 2)\n    \n    return classification_summary","3b6e9e17":"# Model Metric Evaluation at optimum cut off\nmodel = logm3\ncut_off = 0.37\n\nprint('model metrics of train set @', cut_off  )\nmodel_metrics = classification_model_metrics(model,X_train, y_train, cut_off)\nprint(model_metrics)\nprint('--------------------------------------------------------')\nprint('model metrics of test set @', cut_off  )\nmodel_metrics = classification_model_metrics(model,X_test, y_test, cut_off)\nprint(model_metrics)\n","e7f1a387":"feature_importance = model.params[1:]\nfeature_importance","cff94829":"#Getting a relative coeffient value for all the features wrt the feature with the highest coefficient\nfeature_importance = model.params[1:]\nfeature_importance = 100.0 * (feature_importance \/ abs(feature_importance).max())\nfeature_importance.sort_values()","9030ce11":"feature_importance = feature_importance.sort_values(ascending=False)\nplt.figure(figsize=(6,6))\nfeature_importance.plot.barh(align='center', color = 'tab:red',alpha=0.8, fontsize = 12)\nplt.title('Relative Feature Importance', fontsize=14)\nplt.show()","e95bc4c5":"# getting the predicted values on test df\nX = df_test[var_final]\nX_sm = sm.add_constant(X[var_final])\ny_pred = log_final.predict(X_sm)\ny_pred[:10]","f7c4068b":"# create submmission df\nsubmission = df_test[['PassengerId']]\n# submission['Survived'] = round(y_pred*100,2)\nsubmission['Survived'] = y_pred.map(lambda x: 1 if x > cut_off else 0)\nsubmission.head()","3354431e":"submission.to_csv(\"submission_logreg.csv\", index=False)","19dd1b09":"from sklearn.ensemble import RandomForestClassifier\nRF= RandomForestClassifier(random_state=42,n_jobs=-1)","44878e40":"df.columns","82186aa4":"df_test.columns","74fa828d":"df_test['Cabin_T']= 0","99d0e8d5":"X_train_RF = df.iloc[:,2:]\ny_train_RF = df['Survived']","9e5b239f":"# RF.fit(X_train_RF,y_train_RF)","f5f9c7e8":"# define parameters for grid search\nparams = {\n    \"n_estimators\": [5, 10, 15, 20, 25,50], \n    \"max_depth\": [3, 5, 7, 9, 11, 13],\n    \"min_samples_leaf\": [5,10,20,50,100,500]\n}","596d2871":"grid_search = GridSearchCV(estimator=RF,\n                           param_grid=params,\n                           cv = 4,\n                           n_jobs=-1, verbose=1, scoring=\"accuracy\")","c2366109":"%%time\ngrid_search.fit(X_train, y_train)","7150cb93":"print(f'Best parameters {grid_search.best_params_}')\nprint(f'Mean cross-validated accuracy score of the best_estimator: {grid_search.best_score_:.3f}')","90b40132":"rf_best = grid_search.best_estimator_ ","21120112":"from sklearn.metrics import plot_roc_curve\nplot_roc_curve(rf_best, X_train_RF, y_train_RF)\nplt.show()","777c7176":"# Making Predictions on train set\ny_pred_train = rf_best.predict(X_train_RF)\n\n# Calculating the accuracy\nprint(\"Accuracy:\",round(rf_best.score(X_train_RF, y_train_RF)*100,2))","a6a98cf2":"feature_imp = pd.Series(rf_best.feature_importances_, index = X_train_RF.columns).sort_values(ascending=False)\nfeature_imp.head()","1ed4556b":"feature_imp = feature_imp.head().sort_values()\nplt.figure(figsize=(6,6))\nfeature_imp.plot.barh(align='center', color = 'tab:red',alpha=0.8, fontsize = 12)\nplt.title('Feature Importance', fontsize=14)\nplt.show()","d5cde458":"df_test.info()","36794db2":"# df_test_RF = df_test.drop(['PassengerId'],axis = 1)","ae4e67dd":"# FinalPrediction\ny_pred_test = rf_best.predict(df_test.iloc[:,1:])\ny_pred_test[:10]","2c53d263":"submission = pd.DataFrame(\n    { \n        'PassengerId': df_test['PassengerId'], \n        'Survived': y_pred_test\n    }\n)\nsubmission.to_csv(\"submission_RF.csv\", index=False)","ad39d7f1":"submission.head()","279eeea2":"## Data Preparation:","25aaba8a":"The age distribution for survived and Not-Servived is actually very similar. \n- The survivors, a larger proportion were children. \n- The passengers evidently made an attempt to save children by giving them a place on the life rafts. ","f9cdcbd9":"### Ploting ROC Curve\nAn ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","5805d7fe":"The distributions are clearly different for the fares of survivors vs. deceased\n- Its' likely that this would be a significant predictor in our final model.  \n- Passengers who paid lower fare appear to have been less likely to survive.  ","e17b1b4a":"## 2) Random Forest Classifier","5425e009":"## Handing missing values:\n\nMissing value handling: \n- Cabin: `70%` passenger were not having cabin number. We have to check further insights of the data.\n- age and Fare: impute with median \n- Embarked: impute with mode (as categorical feature)","1c442e21":"**No null values obseved. Now the dataset is good for analysis**","ef0e6ee2":"### Prediction on test DataFrame","afe6cc11":"### Data Prepration:","85e0a2bf":"**Missing value handing Age:**","8d463976":"- We could obsrve high number of missing values in `Cabin` and `age` columns in both train and test dataset\n- There are some minor missing value in `Embarked` column of train set\n- There are some minor missing value in `Fare` column of train set\n","229dcfbd":"**Logistic Regression**\n\nAccuracy: train  80.0, test 0.81\n<br>Sensitivity: train 0.81, test 0.79\n\n**Random forrest**\n\nAccuracy =  0.824\n\n\n**Final Score : 0.7703**","f0dbda25":"### Step 8: Feature Selection Using RFE","65678e28":"### Background:\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n\n### Data Dictionary:\nThe data has already been splited into two groups:\n- training set (train.csv)\n- test set (test.csv)\n\n**Features details:**\n\n| Variable | Definition | Key |\n| :- | :- | :-: |\n| survival | Survival | 0 = No; 1 = Yes |\n| class | Passenger Class | 1 = 1st; 2 = 2nd; 3 = 3rd\n| name | Name\n| sex | Sex\n| age | Age\n| sibsp | Number of Siblings\/Spouses aboard\n| parch | Number of Parents\/Children aboard\n| ticket | Ticket Number\n| fare | Passenger Fare\n| cabin | Cabin\n| embarked | Port of Embarkation | C = Cherbourg; Q = Queenstown; S = Southampton","a1ccd1c1":"**Metric Evaluation: (Train and Test Set)**","b19ccd8f":"The sex column is boolean category so we can impute this columns to 0-1. ","c57c5d6c":"## Reading and Understanding Data","438f3e69":"- As we have considered family size derived from from 'SibSp' and 'Parch' we will drop these two columns\n- We will drop `Name`,`Ticket` column as this does not add value to model","83f72e39":"**Missing value handing Fare:**","6a9afcc7":"- It seems that the 1st alphabet is deck level and the numeric value is the cabin number.\n- So to make use of the data we will exclude the cabin number and keep the alphabet as deck level. Also we will assign `N` for those passengers not having cabin.","2fd7739c":"### Important Features and coefficients:","033ae227":"### Dummy Creation for Categorical Variable","b9baeb3a":"## Environment setup","9203379f":"## Feature Scaling","a539a4cf":"There are number of categorical varibales is observed as numeric. lets change those variables to data type `category`","1f4774d4":"**missing value handing Cabin:**","5e5ccf02":"Insights:\n-  This shows that the persons traveling alone or having family size less had higher chace for survival. Probably the watent to stick to their family\n- In `Parch` we could see higher number of childern travelled with only with a nanny. Nanny made attempt to save those children making space in raft.\n- `SibSp` and `Parch` shows similar survival rate, We can combine these two features to get a derived feature Family_Size and get more insight from it.","d54824f5":"## Train - Test Split","e826bba2":"**Missing value handing Embarked:**","e96112fe":"### Evaluating model Metrics for train set","71e14901":"## Visualisation & Exploratory Data Analysis:","642349d5":"# <font color=blue> Titanic - Machine Learning from Disaster <font>","0792e083":"## Concluson:","d1096dfe":"### Step 7: Model Building","d2c3591c":"Insights:\n- The most common boarding port of embarkation is S ( The most passengers boarded in Southhampton)","ff0187c1":"Insight:\n- Lower Family size has got higher chance of survival","c5bf58d4":"#### Running Your First Training Model","1b421d99":"### Prediction:","b5c7bf36":"**Precision and recall tradeoff**"}}