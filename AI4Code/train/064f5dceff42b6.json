{"cell_type":{"548500f8":"code","5450bcb6":"code","703bf769":"code","be1a28ce":"code","5f3f810b":"code","5fc78ec2":"code","45e3954d":"code","2396fc0a":"code","106b80be":"code","c4abe5df":"code","478ac685":"code","79066848":"code","e9616583":"code","c7f9208f":"code","b6741afa":"code","a7de6436":"code","ea6222fb":"code","fa6a598d":"code","91f35896":"code","e78eee14":"code","6b2abeba":"code","7eaafe58":"code","281657a9":"code","594c7393":"code","1bf98088":"code","143b3a38":"code","4e7b6545":"code","140e9b4c":"code","a830f85e":"code","7de25f42":"code","07959673":"code","f92a8366":"code","4bcc4105":"code","8d02def3":"code","89112b2f":"code","fcc8d2b5":"code","f5380ece":"code","189bea55":"code","5dde742c":"code","769f0bff":"code","448cf8fc":"code","dc1f784d":"code","cfa9c605":"code","cb4a1640":"code","7f333eb1":"code","5cd98225":"code","b44e7734":"code","4d933442":"code","77b9a13b":"code","ec3b2b6c":"code","ae44abf1":"code","cd172fd9":"code","c1cac78b":"code","74de8d2a":"code","7e428bc1":"code","b13ccc27":"markdown","f38e41e5":"markdown","4b862060":"markdown","d4d62161":"markdown","e1f10fbb":"markdown","8944252a":"markdown","60214979":"markdown","449471cb":"markdown","f2788c78":"markdown","102b2d40":"markdown","d33742a9":"markdown","2961c693":"markdown","dd3f3878":"markdown","fe3d819e":"markdown","20a14308":"markdown","1dcdd8b7":"markdown","1b1c717c":"markdown","f87a52cf":"markdown","a40d87a7":"markdown","d8078be3":"markdown"},"source":{"548500f8":"##bibliotecas \nimport matplotlib.pyplot as plt\nimport matplotlib.patches as mpatches\nimport seaborn as sns\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import plot_confusion_matrix, plot_roc_curve, plot_precision_recall_curve, classification_report, confusion_matrix\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nimport scikitplot as skplt\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import resample\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n## visualiza\u00e7\u00e3o\npd.set_option('max_columns', 140)\npd.set_option('max_colwidth', 5000)\npd.set_option('display.max_rows', 140)\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12,8)\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5450bcb6":"## carregando e juntando datasets\ntrain = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/test.csv')\ncodebook = pd.read_csv('\/kaggle\/input\/costa-rican-household-poverty-prediction\/codebook.csv')\n\ndf = train.append(test)\n\ntrain.shape,test.shape, df.shape\n","703bf769":"## procurando valores com grande quantidade de nulos\ndf.isna().sum().sort_values(ascending=False).head(10)","be1a28ce":"## verificando vari\u00e1veis que apresentaram muitos nulos e n\u00e3o s\u00e3o a Target\nprint(codebook[codebook['Variable name']=='rez_esc'])\nprint('----------------------------------')\nprint(codebook[codebook['Variable name']=='v18q1'])\nprint('----------------------------------')\nprint(codebook[codebook['Variable name']=='v2a1'])\nprint('----------------------------------')\nprint(codebook[codebook['Variable name']=='meaneduc'])\nprint('----------------------------------')\nprint(codebook[codebook['Variable name']=='SQBmeaned'])","5f3f810b":"df.v18q1.value_counts()","5fc78ec2":"df.rez_esc.value_counts()","45e3954d":"df.v2a1.value_counts()","2396fc0a":"df.meaneduc.value_counts()","106b80be":"df.SQBmeaned.value_counts()","c4abe5df":"#realizando a imputa\u00e7\u00e3o e dropando coluna\ndf['v2a1'].fillna(-1, inplace=True)\ndf['v18q1'].fillna(0, inplace=True)\ndf['SQBmeaned'].fillna(-1, inplace=True)\ndf['rez_esc'].fillna(-1, inplace=True)\ndf.meaneduc.fillna(df.meaneduc.median(), inplace=True)\n## realizando o drop de outras colunas que assim como SQBmeaned s\u00e3o provenientes de outras\n# df.drop(['SQBescolari'], axis=1, inplace=True)\n# df.drop(['SQBage'], axis=1, inplace=True)\n# df.drop(['SQBhogar_total'], axis=1, inplace=True)\n# df.drop(['SQBedjefe'], axis=1, inplace=True)\n# df.drop(['SQBhogar_nin'], axis=1, inplace=True)\n# df.drop(['SQBovercrowding'], axis=1, inplace=True)\n# df.drop(['SQBdependency'], axis=1, inplace=True)\n# df.drop(['agesq'], axis=1, inplace=True)\n","478ac685":"df.isna().sum().sort_values(ascending=False).head(10)","79066848":"##verificando se existem colunas do tipo objeto\ndf.select_dtypes('object').describe()","e9616583":"df.dependency.value_counts().head(5)","c7f9208f":"df.edjefe.value_counts().head(5)","b6741afa":"df.edjefa.value_counts().head(5)","a7de6436":"#realizando um replace\nvalores_replace = {'yes': 1, 'no': 0}\n#df.drop(['Id'], axis=1, inplace=True)\n#df.drop(['idhogar'], axis=1, inplace=True)\ndf['dependency'] = df['dependency'].replace(valores_replace).astype(float)\ndf['edjefe'] = df['edjefe'].replace(valores_replace).astype(int)\ndf['edjefa'] = df['edjefa'].replace(valores_replace).astype(int)","ea6222fb":"df.select_dtypes('object')","fa6a598d":"## verificando a correla\u00e7\u00e3o entre as vari\u00e1veis e a Target\n\ndf[df['Target'].notnull()].corr()['Target'].sort_values(ascending=False)","91f35896":"## ap\u00f3s a cria\u00e7\u00e3o das colunas ocorreu uma leve melhoria no score, as colunas comentadas foram testadas e diminuiram o score geral.\n\n#df[\"telperpessoa\"]=df[\"qmobilephone\"]\/df[\"tamviv\"]\ndf[\"m2perpessoa\"]=df[\"tamhog\"]\/df[\"tamviv\"]\n# df['tabletsperpessoa'] = df['v18q1'] \/ df['tamviv']\n# df['roomsperpessoa'] = df['rooms'] \/ df['tamviv']\ndf['rentperpessoa'] = df['v2a1'] \/ df['tamviv']\n# df['hsizeperpessoa'] = df['hhsize'] \/ df['tamviv']\n\n","e78eee14":"## m\u00e9dia de aluguel paga por quantidade de quartos \ndf[df['tipovivi3']==1].groupby(['rooms'])['v2a1'].mean().plot(kind='bar')","6b2abeba":"#df['v2a1'].value_counts.plot.bar()","7eaafe58":"## quantidade de moradias com teto (1) e sem teto (0)\ndf.groupby(['cielorazo'])['idhogar'].nunique().plot(kind='bar')","281657a9":"## porcentagem de moradias sem teto\nprint(round(df[df['cielorazo']==0]['idhogar'].nunique()\/df[df['cielorazo']==1]['idhogar'].nunique()*100,2),'%')","594c7393":"#abastaguano\nplt.figure(figsize=(15,9))\nfig, axes = plt.subplots(nrows=2, ncols=2)\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=None, hspace=0.3)\nfig.suptitle('0 (possuem) 1 (n\u00e3o possuem)')\n\nax=axes[0,0].title.set_text('Distribui\u00e7\u00e3o \u00e1gua.')\nax=axes[0,1].title.set_text('Distribui\u00e7\u00e3o energia.')\nax=axes[1,0].title.set_text('Possui Sanitario.')\nax=axes[1,1].title.set_text('Possui piso.')\n\ndf.groupby(['abastaguano'])['idhogar'].nunique().plot(kind='bar',ax=axes[0,0])\ndf.groupby(['noelec'])['idhogar'].nunique().plot(kind='bar',ax=axes[0,1])\ndf.groupby(['pisonotiene'])['idhogar'].nunique().plot(kind='bar',ax=axes[1,0])\ndf.groupby(['sanitario1'])['idhogar'].nunique().plot(kind='bar',ax=axes[1,1])\n\nquantidade = mpatches.Patch(label='Quantidade')\nplt.legend(fancybox=True, framealpha=1,handles=[quantidade] ,shadow=True, borderpad=1 )\n\nplt.show()","1bf98088":"\nprint('** M\u00e9todos de se jogar lixo fora **')\nprint('Tanker truck:',df[df['elimbasu1']==1].groupby(['elimbasu1'])['idhogar'].nunique().iloc[0])\nprint('Botan hollow or buried:',df[df['elimbasu2']==1].groupby(['elimbasu2'])['idhogar'].nunique().iloc[0])\nprint('Burning:',df[df['elimbasu3']==1].groupby(['elimbasu3'])['idhogar'].nunique().iloc[0])\nprint('Throwing in an unoccupied space:',df[df['elimbasu4']==1].groupby(['elimbasu4'])['idhogar'].nunique().iloc[0])\nprint('Throwing in river,  creek or sea:',df[df['elimbasu5']==1].groupby(['elimbasu5'])['idhogar'].nunique().iloc[0])\nprint('Other:',df[df['elimbasu6']==1].groupby(['elimbasu6'])['idhogar'].nunique().iloc[0])\n","143b3a38":"## zonas de moradia\nrural = round(df[df['area2']==1].groupby(['area2'])['idhogar'].nunique().iloc[0]\/df[df['area1']==1].groupby(['area1'])['idhogar'].nunique().iloc[0],2)\nurbana = 1-rural\nplt.figure(figsize=(15,9))\nlabels = [r'Urbana('+str(urbana)+')', r'Rural ('+str(rural)+')']\nsizes = [88.4, 10.6, 0.7, 0.3]\ncolors = ['orange', 'blue']\npatches, texts = plt.pie(sizes, colors=colors, startangle=90)\nplt.legend(patches, labels, loc=\"best\")\nplt.pie(df.groupby('area1')['idhogar'].nunique())\nplt.show()","4e7b6545":"## contando a quantidade de lideres de familia masculinos e femininos\nprint('quantidade de lideres de fam\u00edlia homens:',df[(df['parentesco1']==1) & (df['male']==1)].shape[0])\nprint('quantidade de lideres de fam\u00edlia mulheres:',df[(df['parentesco1']==1) & (df['female']==1)].shape[0])","140e9b4c":"## verificando a distribui\u00e7\u00e3o da classe a ser predita\ndf[df['Target'].notnull()].groupby(['Target'])['idhogar'].nunique().plot(kind='bar', title='1 = extreme poverty 2 = moderate poverty 3 = vulnerable households 4 = non vulnerable households')","a830f85e":"## separando datasets em treino e teste para aplica\u00e7\u00e3o no modelo\nfeats = [c for c in df.columns if c not in ['Id', 'idhogar', 'Target']]\ntrain, test = df[~df['Target'].isnull()], df[df['Target'].isnull()]\ntrain.shape, test.shape","7de25f42":"#Criando v\u00e1rios par\u00e2metros  afim de buscar o melhor caso\nparam_grid = {\n    'criterion' : ['gini', 'entropy'],\n    'class_weight' : ['balanced','balanced_subsample'],\n     'n_estimators': [100, 200, 300, 400]\n }\nrf = RandomForestClassifier()\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                         cv = 3, n_jobs = -1, verbose = 2)","07959673":"## comentado para n\u00e3o rodar toda vez j\u00e1 que demora mais de 10 minutos\n# grid_search.fit(train[feats], train['Target'])\n# grid_search.best_params_","f92a8366":"## mesmo ap\u00f3s aplicar o grid_search, o resultado ainda n\u00e3o ficou melhor que apenas com o n_estimators 200, por isso foi mantido ele.\nrf = RandomForestClassifier(n_jobs=-1, n_estimators=200 ,random_state=42)\nrf.fit(train[feats], train['Target'])","4bcc4105":"## realizando a predi\u00e7\u00e3o de valores\ntest['Target'] = rf.predict(test[feats]).astype(int)\n## Demonstrando a import\u00e2ncia das vari\u00e1veis e seus nomes \nprint(rf.feature_importances_)\nprint(train.columns)","8d02def3":"#distribui\u00e7\u00e3o da vari\u00e1vel \ntest['Target'].value_counts(normalize=True)","89112b2f":"pd.Series(rf.feature_importances_, index=feats).sort_values()","fcc8d2b5":"skplt.metrics.plot_confusion_matrix(train['Target'], rf.predict(train[feats])) ## Matriz de confus\u00e3o\naccuracy_score(train['Target'], rf.predict(train[feats])) ## score do modelo","f5380ece":"# Criando o arquivo para submiss\u00e3o\ntest[['Id', 'Target']].to_csv('submission1.csv', index=False)","189bea55":"## pegando chefe de fam\u00edlia\nheads2 = train[train['parentesco1'] == 1]\n## treinando modelo\n# max_depth : profundidade de \u00e1rvore, utilizando none para ir expandido at\u00e9 todas estarem puras\n# n_jobs : quantidade de tarefas rodando em paralelo\n# n_estimators : quantidade de \u00e1rvores na floresta, 700 foi o melhor valor entre os testados\n# min_impurity_decrease : o n\u00f3 vai se dividir se a impureza da divis\u00e3o for maior ou igual ao valor utilizado\n# min_samples_leaf : quantidade minima de amostras para ser considerado um n\u00f3 folha\n# verbose : demonstrar menos informa\u00e7\u00f5es na hora de rodar o modelo\n# class_weight : Foi utilizado balanced, balanceando o peso das classes\n#\n#\nrf4 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\nrf4.fit(heads2[feats], heads2['Target'])\n## predizendo\ntest['Target'] = rf4.predict(test[feats]).astype(int)\n# criando arquivo\ntest[['Id', 'Target']].to_csv('submission.csv', index=False)","5dde742c":"skplt.metrics.plot_confusion_matrix(heads2['Target'], rf4.predict(heads2[feats])) ## Matriz de confus\u00e3o\naccuracy_score(heads2['Target'], rf4.predict(heads2[feats])) ## Score do modelo","769f0bff":"xgb = XGBClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\nxgb.fit(heads2[feats], heads2['Target'])\ntest['Target'] = xgb.predict(test[feats]).astype(int)\ntest[['Id', 'Target']].to_csv('submission2.csv', index=False)\n","448cf8fc":"skplt.metrics.plot_confusion_matrix(heads2['Target'], xgb.predict(heads2[feats])) ## Matriz de confus\u00e3o\naccuracy_score(heads2['Target'], xgb.predict(heads2[feats])) ## Score do modelo","dc1f784d":"## learning_rate : controla quanto vai ser a contribui\u00e7\u00e3o para o novo modelo, utilizando o atual\nabc = AdaBoostClassifier(random_state=42, n_estimators=700,learning_rate=1.0)\nabc.fit(heads2[feats], heads2['Target'])\ntest['Target'] = abc.predict(test[feats]).astype(int)\ntest[['Id', 'Target']].to_csv('submission3.csv', index=False)","cfa9c605":"skplt.metrics.plot_confusion_matrix(heads2['Target'], abc.predict(heads2[feats])) ## Matriz de confus\u00e3o\naccuracy_score(heads2['Target'], abc.predict(heads2[feats])) ## Score do modelo","cb4a1640":"# Criando um modelo de RF Classifier e usando o Cross Validation\nrfc = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\nscores = cross_val_score(rfc, heads2[feats], heads2['Target'], cv=5, n_jobs=-1)\nscores","7f333eb1":"scores.mean()","5cd98225":"train['Target'].value_counts()\n","b44e7734":"df_1 = train[train['Target'] == 1]\ndf_2 = train[train['Target'] == 2]\ndf_3 = train[train['Target'] == 3]\ndf_4 = train[train['Target'] == 4]\ndf_1.shape,df_2.shape,df_3.shape,df_4.shape,","4d933442":"## aumentando as classes menores at\u00e9 igualar a quantidade com df_4\n\ndf_1 = resample(df_1, \n                       replace=True,\n                       n_samples=len(df_4),\n                       random_state=42)\n\ndf_2 = resample(df_2, \n                       replace=True,\n                       n_samples=len(df_4),\n                       random_state=42)\n\ndf_3 = resample(df_3, \n                       replace=True,\n                       n_samples=len(df_4),\n                       random_state=42)","77b9a13b":"df_1.shape,df_2.shape,df_3.shape,df_4.shape,","ec3b2b6c":"## juntando as 4 tabelas\ntrain = pd.concat([df_1, df_2, df_3, df_4])","ae44abf1":"train.shape","cd172fd9":"## aplicando o mesmo treinamento que alcan\u00e7ou o maior score\nheads2 = train[train['parentesco1'] == 1]\nrf4 = RandomForestClassifier(max_depth=None, random_state=42, n_jobs=4, n_estimators=700,\n                            min_impurity_decrease=1e-3, min_samples_leaf=2,\n                            verbose=0, class_weight='balanced')\nrf4.fit(heads2[feats], heads2['Target'])\n## predizendo\ntest['Target'] = rf4.predict(test[feats]).astype(int)\n# criando arquivo\ntest[['Id', 'Target']].to_csv('submission4.csv', index=False)","c1cac78b":"skplt.metrics.plot_confusion_matrix(heads2['Target'], rf4.predict(heads2[feats])) ## Matriz de confus\u00e3o\naccuracy_score(heads2['Target'], rf4.predict(heads2[feats])) ## Score do modelo","74de8d2a":"## verificando a melhoria do primeiro modelo do testo, ap\u00f3s aplica\u00e7\u00e3o de balanceamento \nrf4 = RandomForestClassifier(n_jobs=-1, n_estimators=200 ,random_state=42)\nrf4.fit(heads2[feats], heads2['Target'])\n## predizendo\ntest['Target'] = rf4.predict(test[feats]).astype(int)\n# criando arquivo\ntest[['Id', 'Target']].to_csv('submission5.csv', index=False)","7e428bc1":"skplt.metrics.plot_confusion_matrix(heads2['Target'], rf4.predict(heads2[feats])) ## Matriz de confus\u00e3o\naccuracy_score(heads2['Target'], rf4.predict(heads2[feats])) ## Score do modelo","b13ccc27":"1. # > Treinando modelo 0.36746 (Submiss\u00e3o Kaggle)","f38e41e5":"# > Cross Validation avaliando modelo, utilizando o melhor resultado que foi RandomForest","4b862060":"O balanceamento da classe target n\u00e3o alterou os 100% de acerto do primeiro modelo, ainda apresentando caracter\u00edsticas de um modelo com overfit. ","d4d62161":"# O balancemento da classe Target embora tenha melhorado o resultado do primeiro modelo realizando no trabalho, acabou diminuindo em  aproximadamente 0.006 o modelo que obteve o melhore score. Isso demonstra uma certa import\u00e2ncia do balanceamento no modelo inicial e que uma poss\u00edvel altera\u00e7ao nos par\u00e2metros passados na hora do treinamento do modelo com maior score, pode elevar mais ainda o score obtido.  ","e1f10fbb":"* As vari\u00e1veis que obtiveram as maiores correla\u00e7\u00f5es positivas foram meaneduc, cielorazo, escolari. meaneduc e escolari s\u00e3o vari\u00e1veis voltadas ao tempo de educa\u00e7\u00e3o dos moradores, j\u00e1 cielorazo se a casa possui teto. \n* J\u00e1 as vari\u00e1veis que obtiveram as maiores correla\u00e7\u00f5es negativas foram hogar_nin, r4t1, overcrowding. hogar_nin informa a quantidade de crian\u00e7as presentes na casa, ou seja, se voc\u00ea deseja possuir uma boa casa, n\u00e3o tenha filhos. r4t1 s\u00e3o pessoas mais jovens de 12 anos e overcrowding s\u00e3o quantidade de pessoas por quarto.","8944252a":"# > > Explorando dados e realizando o pr\u00e9-processamento","60214979":"O ADABoost alcan\u00e7ou aproximadamente 65% de score, \u00e9 importante observar que existe uma grande dispers\u00e3o na matriz de confus\u00e3o, com os acertos 4 representando a maior parte dentro o score obtido. Isso mostra que esse modelo tem facilida apenas para acertar resultados 4.","449471cb":"# > Feature engineering","f2788c78":"# > Balanceamento Over-Sampling 0.43312 (Submiss\u00e3o Kaggle melhor modelo) 0.40099 (Submiss\u00e3o Kaggle primeiro modelo do trabalho)","102b2d40":"Utilizando os mesmos par\u00e2metros do melhor modelo, o XGBoost tamb\u00e9m alcan\u00e7ou 100% de acerto nos scores, demonstrando comportamento de um modelo com overfit","d33742a9":"# > Treinamento Chefe de fam\u00edlia com XGBoost utilizando os mesmos par\u00e2metros do RandomForest alcan\u00e7ou 0.40326 (Submiss\u00e3o Kaggle)","2961c693":"# Conclus\u00e3o:\n\n* Logo no in\u00edcio do trabalho foi realizado a limpeza e visualiza\u00e7\u00f5es de determinadas vari\u00e1veis existentes, a fim de ter uma melhor compreens\u00e3o do dataset como um todo. Tamb\u00e9m foi realizada a cria\u00e7\u00e3o de diversas colunas novas, e testadas 1 a 1, buscando a combina\u00e7\u00e3o que mais contribuia para um score positivo.\n\n* Ap\u00f3s essa etapa, foi realizado o treinamento dos modelo, assim observando a import\u00e2ncia de se aplicar diversos modelos e par\u00e2metros diferentes. Durante o treinamento e predi\u00e7\u00e3o dos modelos de treino, diversos modelos alcan\u00e7aram 100% de acertos, causando overfit desses modelos, j\u00e1 o modelo que se saiu melhor na submiss\u00e3o do kaggle, foi o modelo que alcan\u00e7ou 84% de acerto, nesse modelo foi utilizado diversos par\u00e2metros presentes no RandomForest que ao longo do trabalho foram modificados diversas vezes a fim de escontrar melhores combina\u00e7\u00f5es.\n\n* O estudo de diferentes modelos e suas infinidades de par\u00e2metros, \u00e9 de extrema import\u00e2ncia, e considero essa a parte de maior dificuldade do trabalho. Entender quais os melhores modelos e par\u00e2metros para determinado treinamento exige muito estudo e experi\u00eancia. Por isso durante o trabalho o ambiente do Kaggle foi de grande ajuda, proporcionando a visualiza\u00e7\u00e3o de c\u00f3digo de outras pessoas, facilitando assim o entendimento de passos, modelos e par\u00e2metros utilizados.\n\n* A etapa de tratamento tamb\u00e9m foi um desafio, diversos testes foram realizados, desde o drop de colunas, at\u00e9 a imputa\u00e7\u00e3o por m\u00e9dia,mediana ou valores bem distintos, como -1. Ao fim a imputa\u00e7\u00e3o de -1 nos modelos foi a que obteve melhores resultados durante o modelo, por isso parte do trabalho foi comentada ou apagada, utilizando apenas o tratamento com melhores resultados.\n\nEsse trabalho demonstrou a import\u00e2ncia de se realizar diversos testes e mudan\u00e7as ao longo de todo o processo, e s\u00f3 refor\u00e7ou a afirma\u00e7\u00e3o da n\u00e3o exist\u00eancia de um tipo especifico de pr\u00e9-processamento e treinamento que sirva para todos os casos\n\nobs : o melhor modelo \u00e9 o \u00fanico que gera o arquivo submission.csv, que \u00e9 o \u00fanico que pode ser rodado pelo Kaggle","dd3f3878":"# a m\u00e9dia do Cross Validation deu 0.59, bem acima do melhor score ap\u00f3s submeter o c\u00f3digo ao Kaggle que foi de 0.43912. De todos os modelos aplicados, o RandomForest com os par\u00e2metros escolhidos foi o que alcan\u00e7ou o maior score. XGBoost vem logo atr\u00e1s com 0.40326, e o ADABoost na \u00faltima posi\u00e7\u00e3o com 0.37155.","fe3d819e":"O modelo acertou 100% dos dados colocados de treinamento, demonstrando claramente um overfit","20a14308":"Diferentemente do primeiro modelo, esse modelo que foi o que alcan\u00e7ou o melhor resultado dentre todos do trabalho, n\u00e3o acertou 100% das predi\u00e7\u00f5es do modelo de treino, demonstrando que a escolha de pegar apenas chefe de familia, e o tratamento dos par\u00e2metros, realmente melhoraram o modelo","1dcdd8b7":"# > Treinamento Chefe de fam\u00edlia com ADABoost alcan\u00e7ou 0.37155 (Submiss\u00e3o Kaggle)","1b1c717c":"Ap\u00f3s o balanceamento, e a aplica\u00e7\u00e3o dos mesmos par\u00e2metros utilizados no melhor modelo, a matriz de confus\u00e3o demonstra um resultado de 86% de acerto. Esse resultado se encontra um pouco acima dos 84% de acerto do melhor modelo.","f87a52cf":"# > Treinamento apenas com chefe de fam\u00edlia RandomForest alcan\u00e7ou 0.43912 (Submiss\u00e3o Kaggle)","a40d87a7":"1. # > An\u00e1lisando os dados","d8078be3":"* A vari\u00e1vel v18q1 (number of tablets household owns) n\u00e3o possui em seus valores distintos o 0, o que leva a crer que os valores missings presentes s\u00e3o de casas que n\u00e3o possuem tabletes - Candidata a imputa\u00e7\u00e3o de dados\n* A vari\u00e1vel rez_esc  (Years behind in school) Possui 0 em seus valores distintos, o que leva a crer que s\u00e3o realmente dados faltantes. - Por representar mais de 50% da base de valores nulos, utilizaremos o n\u00famero -1 nos valores nulos.\n* A vari\u00e1vel v2a1  (Monthly rent payment) s\u00f3 \u00e9 preenchida em caso de casas alugadas, para isso utilizaremos o n\u00famero -1 nos valores nulos\n* A vari\u00e1vel meaneduc (average years of education for adults (18+)) \u00e9 candidata a ter seus valores nulos substituidos pela mediana"}}