{"cell_type":{"ad48726f":"code","9fb4350c":"code","d2985b96":"code","96863979":"code","a938d2d7":"code","4ebf3207":"code","cc23e9ad":"code","545bda45":"code","439a8254":"code","3fb9a294":"code","8c136710":"markdown","db679cfc":"markdown","4c59b4b0":"markdown","637cb714":"markdown","98f5a3af":"markdown","4ab6b112":"markdown","ffd70d56":"markdown","cfb88314":"markdown"},"source":{"ad48726f":"!pip uninstall fsspec -qq -y\n!pip install --no-index --find-links ..\/input\/hf-datasets\/wheels datasets -qq","9fb4350c":"import pandas as pd\nfrom datasets import Dataset\nfrom sklearn.metrics import mean_squared_error\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n%env WANDB_DISABLED=True","d2985b96":"model_checkpoint = '..\/input\/distilbertbaseuncased'\nbatch_size = 16\nmax_length = 256","96863979":"df = pd.read_csv('..\/input\/step-1-create-folds\/train_folds.csv') # https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\ndf = df.rename(columns={'target':'label'}) # HF expects this column name to pick up the target column in trainer\n\ntrain_dataset = Dataset.from_pandas(df[df.kfold != 0].reset_index(drop=True))\nvalid_dataset = Dataset.from_pandas(df[df.kfold == 0].reset_index(drop=True))\n\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=True)\n\ndef tokenize(batch): return tokenizer(batch['excerpt'], padding='max_length', truncation=True, max_length=max_length)\n\ntrain_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\nvalid_dataset = valid_dataset.map(tokenize, batched=True, batch_size=len(valid_dataset))","a938d2d7":"def model_init():\n    return AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=1) # note this is actually a regression model\n\ndef compute_metrics(pred):\n    targs = pred.label_ids\n    preds = pred.predictions\n    rmse = mean_squared_error(targs, preds, squared=False)\n    return {\n        'rmse': rmse,\n    }\n\nargs = TrainingArguments(\n    \"outputs_dir\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    fp16=True,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    num_train_epochs=1,\n    weight_decay=0.01,\n    load_best_model_at_end=False, # change this to True after hyperparameter tuning\n    save_strategy='no', # remove this after hyperparamenter tuning\n)\n\ntrainer = Trainer(\n    model_init=model_init,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)","4ebf3207":"def hp_space(trial):\n    return {\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True),\n        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 5),\n        \"seed\": trial.suggest_int(\"seed\", 1, 40),\n        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16, 32, 64]),\n        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-3, 1e-1, log=True),\n    }","cc23e9ad":"best_run = trainer.hyperparameter_search(n_trials=20, direction=\"minimize\", hp_space=hp_space)","545bda45":"best_run","439a8254":"for n, v in best_run.hyperparameters.items():\n    print(f'{n}: {v}')","3fb9a294":"for n, v in best_run.hyperparameters.items():\n    setattr(trainer.args, n, v)\n\ntrainer.train()","8c136710":"### Please upvote if you find this helpful :) ","db679cfc":"## Model and Training with HF transformers","4c59b4b0":"## Loading and preprocessing training data with HF datasets","637cb714":"## Config","98f5a3af":"We can now replicate the results of the best run like this. ","4ab6b112":"## Hyperparameter tuning\n\nHuggingFace makes it easy for us:","ffd70d56":"## Hyperparameter tuning with HF Trainer\n\nThis notebook shows how to tune the training of a HF transformer model with HF trainer. \n\n### Please upvote if you find this helpful :) ","cfb88314":"Let's see the parameters of the best run. "}}