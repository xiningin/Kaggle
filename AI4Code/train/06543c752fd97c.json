{"cell_type":{"d5156c6d":"code","f30f920c":"code","84f069af":"code","335b9088":"code","1eb446d1":"code","935e1de4":"code","9ee4f0c5":"code","d0901403":"code","6dd836ab":"code","9377d976":"code","6a0a526b":"code","4ba68990":"code","ff3fd247":"code","45d90433":"code","1fb27b5b":"code","713ba4cd":"code","b44cc60c":"code","d04907a8":"code","26c340ed":"code","32a7f3d7":"code","5fa1cdf3":"code","cfea7926":"code","7d1fa4c5":"code","260c0dfe":"code","06945263":"code","bdc8e6f8":"code","53a74013":"code","25ae9e9a":"code","66dce71d":"code","ca31aebe":"code","c73172d7":"code","a966e70e":"code","926019d3":"code","e6bc8cee":"code","71ba5bec":"code","e9d35d31":"code","537cc4fd":"code","571933b3":"code","8035e84d":"code","293cc4a9":"code","ba6a9f0c":"markdown","0a51b49c":"markdown","83411b22":"markdown","88c07b17":"markdown","d7715394":"markdown","532d8015":"markdown","20a0f69b":"markdown","df44a01a":"markdown","815d0b9a":"markdown","9d5f6543":"markdown","e2422862":"markdown","2b069629":"markdown","1b622c49":"markdown","4d5c5ffe":"markdown","1945a369":"markdown","cba7cbd2":"markdown"},"source":{"d5156c6d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom scipy import stats\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler  \n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, BaggingClassifier, VotingClassifier, RandomTreesEmbedding\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split, KFold, cross_validate\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\n\nfrom sklearn.metrics import roc_auc_score as auc\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f30f920c":"test_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\ntrain_data = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\n\n\nprint(\"Shape of test_data \"+str(test_data.shape))\n\nprint(\"Shape of train_data \"+str(train_data.shape))\n\n","84f069af":"y_train = train_data['target']\ntrain_data_id = train_data['id']\n\ntest_data_id = test_data['id']\n\ntrain_data.drop(['target','id'], axis=1,inplace=True)\ntest_data.drop('id', axis=1,inplace=True)\n\nprint(\"New shape of test_data \"+str(test_data.shape))\n\nprint(\"New shape of train_data \"+str(train_data.shape))\n\n","335b9088":"print(\"Contents of train_data\\n\")\n\n\ntrain_data.head()","1eb446d1":"print(\"Contents of test_data\\n\")\n\n\ntest_data.head()","935e1de4":"train_data.columns","9ee4f0c5":"test_data.columns","d0901403":"missing_val_count_by_col = train_data.isnull().sum()\n\nprint(\"Columns in train_data with missing values, and the number of missing values\")\nprint(missing_val_count_by_col[missing_val_count_by_col > 0])","6dd836ab":"X_train = train_data\n\nprint(\"Shape of y_train before train\/validation split is\"+str(y_train.shape))\nprint(\"Shape of X_train before train\/validation split is\"+str(X_train.shape))\nprint(\"\\n\")\n#Split training set into a training set and validation set\n\nX_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, train_size=0.9, test_size=0.1, random_state=0)\n\nprint(\"Shape of y_train is\"+str(y_train.shape))\nprint(\"Shape of X_train is\"+str(X_train.shape))\n\nprint(\"Shape of y_valid is\"+str(y_valid.shape))\nprint(\"Shape of X_valid is\"+str(X_valid.shape))","9377d976":"#Printing out the number of unique values for each column in the training data\nfor col_name in X_train.keys():\n    print(\"Column \" + col_name + \" has \" + str( len(X_train[col_name].unique()) ) + \" unique values\")","6a0a526b":"print(X_train[\"nom_6\"].value_counts().sort_values(ascending=False))\nprint(X_train[\"nom_7\"].value_counts().sort_values(ascending=False))\nprint(X_train[\"nom_8\"].value_counts().sort_values(ascending=False))\nprint(X_train[\"nom_9\"].value_counts().sort_values(ascending=False))","4ba68990":"%%time\n\nlabel_X_train = X_train.copy()\nlabel_X_valid = X_valid.copy()\nlabel_test_data = test_data.copy()\n\nsk_label_encoder = LabelEncoder()\n\nfor mycol in [\"ord_0\",\"ord_1\",\"ord_2\",\"ord_2\",\"ord_3\",\"ord_4\",\"ord_5\"]:\n    label_X_train[mycol] = sk_label_encoder.fit_transform(label_X_train[mycol])\n    label_X_valid[mycol] = sk_label_encoder.transform(label_X_valid[mycol])\n    label_test_data[mycol] = sk_label_encoder.transform(label_test_data[mycol])\n    ","ff3fd247":"%%time\nlabel_X_train.head()","45d90433":"%%time\nlabel_X_valid.head()","1fb27b5b":"%%time\nlabel_test_data.head()","713ba4cd":"low_cardinality_nom_cols = []\nhigh_cardinality_nom_cols = []\n\n\nfor nom_col in range(10):\n    nom_col_name = \"nom_\"+str(nom_col)\n    if label_X_train[nom_col_name].nunique() < 10:\n        low_cardinality_nom_cols.append(nom_col_name)\n    else:\n        high_cardinality_nom_cols.append(nom_col_name)\n\nprint(\"Nominal columns low cardinality (<=10):\", low_cardinality_nom_cols)\nprint(\"Nominal columns with high cardinality (>10):\", high_cardinality_nom_cols)","b44cc60c":"#combining everything into a single data frame so as to apply a uniform encoding across train, validation, test data sets\n#If this is not OK please provide your feedback, with references as to why (tyvm)\nlabel_X_train[\"kind\"] = \"train\"\nlabel_X_valid[\"kind\"] = \"valid\"\nlabel_test_data[\"kind\"] = \"test\"\n\nbig_df = pd.concat([label_X_train, label_X_valid, label_test_data], sort=False ).reset_index(drop=True)\n\nprint(\"big_df shape is \"+str(big_df.shape))","d04907a8":"%%time\nfor col in low_cardinality_nom_cols:\n    temp_df_to_concat = pd.get_dummies(big_df[col], prefix=col)\n    big_df = pd.concat([big_df, temp_df_to_concat], axis=1)\n    big_df.drop([col],axis=1, inplace=True)\n\n\nfor col in high_cardinality_nom_cols:\n        big_df[f\"hash_{col}\"] = big_df[col].apply( lambda x: hash(str(x)) % 5000)\n        \n\n#Not sure if I can run this over all of big_df. In the example the coder runs it over df_train only\n\n#Just modify training or validation data set\n\nbig_df_train_valid = big_df.loc[ (big_df[\"kind\"] == \"train\") | (big_df[\"kind\"]==\"valid\") ]\nbig_df_test = big_df.loc[big_df[\"kind\"] == \"test\"]\n\nfor col in high_cardinality_nom_cols:\n    enc_nom_1 =  (big_df_train_valid.groupby(col).size() ) \/ len(big_df_train_valid)\n    big_df_train_valid[f\"freq_{col}\"] = big_df_train_valid[col].apply( lambda x : enc_nom_1[x])\n\nfor col in high_cardinality_nom_cols:\n    enc_nom_1 =  (big_df_test.groupby(col).size() ) \/ len(big_df_test)\n    big_df_test[f\"freq_{col}\"] = big_df_test[col].apply( lambda x : enc_nom_1[x])\n    \nlabel_X_train = big_df_train_valid.loc[ big_df[\"kind\"]==\"train\" ]\nlabel_X_valid = big_df_train_valid.loc[ big_df[\"kind\"]==\"valid\" ]\nlabel_test_data = big_df_test.loc[ big_df[\"kind\"]==\"test\" ]\n\nlabel_X_train.drop(\"kind\", axis=1, inplace=True)\nlabel_X_valid.drop(\"kind\", axis=1, inplace=True)\nlabel_test_data.drop(\"kind\", axis=1, inplace=True)","26c340ed":"label_X_train.head()","32a7f3d7":"label_X_valid.head()","5fa1cdf3":"label_test_data.head()","cfea7926":"print(\"shape of label_test_data \"+str(label_test_data.shape))\nprint(\"shape of label_X_train \"+str(label_X_train.shape))\nprint(\"shape of label_X_valid \"+str(label_X_valid.shape))\n\ndel big_df\ndel big_df_test\ndel big_df_train_valid","7d1fa4c5":"%%time\n\n#More encoding. Borrowed idea from another notebook. Trying other things were too slow\n\nbinary_dict = {\"T\":1, \"F\":0, \"Y\":1, \"N\":0}\n\n\nlabel_X_train[\"bin_3\"] = label_X_train[\"bin_3\"].map(binary_dict)\nlabel_X_train[\"bin_4\"] = label_X_train[\"bin_4\"].map(binary_dict)\n\nlabel_X_valid[\"bin_3\"] = label_X_valid[\"bin_3\"].map(binary_dict)\nlabel_X_valid[\"bin_4\"] = label_X_valid[\"bin_4\"].map(binary_dict)\n\nlabel_test_data[\"bin_3\"] = label_test_data[\"bin_3\"].map(binary_dict)\nlabel_test_data[\"bin_4\"] = label_test_data[\"bin_4\"].map(binary_dict)\n\n","260c0dfe":"label_X_train.drop(high_cardinality_nom_cols, axis=1, inplace=True)\nlabel_X_valid.drop(high_cardinality_nom_cols, axis=1, inplace=True)\nlabel_test_data.drop(high_cardinality_nom_cols, axis=1, inplace=True)\n\n","06945263":"label_X_train.head()","bdc8e6f8":"print(\"Rows of label_X_train \"+str(label_X_train.shape[0]))\nprint(\"Rows of y_train \"+str(y_train.shape[0]))\nprint(\"Rows of label_X_valid \"+str(label_X_valid.shape[0]))\nprint(\"Rows of y_valid \"+str(y_valid.shape[0]))\n\n","53a74013":"ada_boost_model = AdaBoostClassifier(n_estimators=100, random_state=0, learning_rate=0.05, base_estimator=DecisionTreeClassifier(max_depth=10))","25ae9e9a":"#using a StandardScaler as the sklearn documents suggest scaling the inputs\nneural_model = MLPClassifier(hidden_layer_sizes=(96,96,48,48,24,12,6,3,1), \n                             solver=\"adam\", \n                             batch_size=\"auto\", \n                             #learning_rate=\"adaptive\",\n                             learning_rate_init=0.002,\n                             max_iter=200,\n                             n_iter_no_change=10,\n                             random_state=1,\n                             verbose=True\n                            )\n\nNNPipeline = Pipeline([(\"scaler\",StandardScaler()), (\"NN\",neural_model)])","66dce71d":"gradient_boost_model = GradientBoostingClassifier(n_estimators=50)\n\n","ca31aebe":"#I was thinking of doing my own train\/valid split\n#but realized with the cross_val_score() function, I need to\n#recombine my train\/validation sets and let the internal functionality of cross_val_score()\n#do this splitting for me. So that's why I'm recombining them below :\\\nnew_X_train = pd.concat([label_X_train, label_X_valid], axis=0)\nnew_y_train = pd.concat([y_train, y_valid], axis=0)\nnew_X_train_scaled = pd.DataFrame()\nmy_columns= new_X_train.columns","c73172d7":"%%time\nn_folds = 7\n\nkfold = KFold(n_splits=n_folds, shuffle=False, random_state=42)\n\ncv_results = cross_val_score(gradient_boost_model, new_X_train.values, new_y_train,\n                            cv=kfold, scoring='roc_auc', n_jobs=-1)\n\nprint(\"gradient_boost_model average results\",cv_results.mean())\n\ncv_results = cross_val_score(ada_boost_model, new_X_train.values, new_y_train,\n                            cv=kfold, scoring='roc_auc', n_jobs=-1)\n\nprint(\"ada_boost_model average results\",cv_results.mean())\n\n#cv_results = cross_val_score(NNPipeline, new_X_train.values, new_y_train,\n#                            cv=kfold, scoring='roc_auc', n_jobs=-1)\n\n#print(\"NNPipeline average results\",cv_results.mean())","a966e70e":"%%time\ngradient_boost_model.fit(new_X_train, new_y_train)","926019d3":"y_test_pred = gradient_boost_model.predict(label_test_data)\n\nmyscore = gradient_boost_model.score(label_test_data)","e6bc8cee":"label_test_data.head()","71ba5bec":"y_test_pred","e9d35d31":"y_test_pred.shape","537cc4fd":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.hist(y_test_pred, density=True, bins=2)\n#plt.xticks(x+0.5,['0','1'])\nplt.ylabel(\"number of predictions\")\nplt.xlabel(\"values\")\n\n","571933b3":"submission = pd.DataFrame({'id':test_data_id, 'target':y_test_pred})\n","8035e84d":"submission.head()","293cc4a9":"submission.to_csv('submission.csv', index=False)","ba6a9f0c":"Trying a [MLPClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.neural_network.MLPClassifier.html)","0a51b49c":"## Handling nominal valued features\n\nFor features that have high cardinality (>=10) will do hashing\/frequency encoding\nFor features that have low cardinality (<10) will do one-hot encoding\n","83411b22":"So now I'm going to drop old features\n\nin label_X_train I will drop these:\n\n* `high_cardinality_nom_cols`\n\nin label_X_valid I will drop these:\n* `high_cardinality_nom_cols`\n\nin label_test_data I will drop these:\n* `high_cardinality_nom_cols`\n\n*Note*: In the cell above I dropped the `low_cardinality_nom_cols` as I one-hot encoded them.","88c07b17":"Fortunately there are none","d7715394":"## Build a Model\n[SKLearn AdaBoostClassifier FTW!](https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html#adaboost)","532d8015":"Checking for null items in the data set","20a0f69b":"Looking at nominal variables with high unique value counts","df44a01a":"Going to try this person's data engineering: https:\/\/www.kaggle.com\/asimandia\/let-s-try-some-feature-engineering","815d0b9a":"Trying a [GradientBoostingClassifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingClassifier.html)","9d5f6543":"## Transform ordinal features to numeric labels\nJust going to transform all ordinal features to numeric labels","e2422862":"### Convert low cardinality nominal variables to one-hot encoded variables\n","2b069629":"# Feedback, please\nPlease give me feedback.\n\nI'm especially wondering about my neural network classifier which scores 50% accuracy :**(\n\nThank you very much,\n","1b622c49":"So I've massaged the training a validation data\nThere should be an equal number of rows in the y_train and label_X_train pairs and the y_val, label_X_val pairs","4d5c5ffe":"So going to test each one out by doing the following:\n\nGoing to do k-fold cross validation (TODO: add reference) with `n_folds` folds\n\nPer the competition will do ROC AUC scoring.\n\n","1945a369":"performing train\/validation set split here","cba7cbd2":"# Number of unique values for each feature"}}