{"cell_type":{"c71e9c90":"code","c1e58a09":"code","b2d61c31":"code","6312363c":"code","b7f0a58d":"code","f47b5e16":"code","2a22f00f":"code","bc9aa4a0":"code","d7712450":"code","1e11dd5f":"code","0b97d68c":"code","9b21baa9":"code","560a108f":"code","561f239b":"code","5ee19cc7":"code","b7b6aa21":"code","df1665b1":"code","438bb577":"code","8bd4e4f9":"code","0cc9123f":"code","c2b72f7c":"code","70764930":"code","a7f0c789":"code","de965867":"code","790a269c":"code","b0f03925":"code","edc721df":"code","14982352":"code","047fe7ef":"code","959579f0":"code","78e09bd4":"code","1d92c8d7":"code","87e33c16":"code","fb55a585":"code","0cb1f3da":"code","71abeff6":"code","75ec2f3f":"code","f70347ac":"code","ff109cfa":"markdown","9a7f65f8":"markdown","07335b6f":"markdown","f83f2382":"markdown","50b52784":"markdown","35177b72":"markdown","067b9557":"markdown","e7c65eef":"markdown","832c63d1":"markdown","2b33b631":"markdown","a2425f50":"markdown","4122ea6a":"markdown","0fd04af5":"markdown","a95572fb":"markdown","fab66e10":"markdown"},"source":{"c71e9c90":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom sklearn.preprocessing import StandardScaler, PowerTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression, SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, roc_auc_score\n\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c1e58a09":"train_set = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_set = pd.read_csv('..\/input\/titanic\/test.csv')","b2d61c31":"train_set.info()\ntest_set.info()","6312363c":"train_set.describe()","b7f0a58d":"train_set.head(10)","f47b5e16":"def initial_exploration(aSet, numericColumns, categoricalColumns, v):\n    print('NUMERIC EXPLORATION\\n')\n    num_cols = aSet[numericColumns]\n    numeric_exploration(num_cols)\n    pd.pivot_table(aSet, index = 'Survived', values = numericColumns)\n    \n    print('CATEGORIC EXPLORATION\\n')\n    cat_cols = aSet[categoricalColumns]\n    categorical_exploration(cat_cols)\n    print(pd.pivot_table(aSet, index = 'Survived', columns = 'Pclass', \n                         values = v, aggfunc='count'))\n    print()\n    print(pd.pivot_table(aSet, index = 'Survived', columns = 'Sex', \n                         values = v, aggfunc='count'))\n    print()\n    print(pd.pivot_table(aSet, index = 'Survived', columns = 'Embarked', \n                         values = v, aggfunc='count'))\n    \ndef numeric_exploration(numset):\n    for i in numset:\n        sns.histplot(numset[i])\n        plt.show()\n    print(numset.corr())\n    sns.heatmap(numset.corr())\n    plt.show()\n\ndef categorical_exploration(catset):\n    for i in catset.columns:\n        X = catset[i].value_counts().index\n        y = catset[i].value_counts()\n        sns.barplot(x = X, y = y)\n        plt.show()","2a22f00f":"#splitting the data into groups to more easily analyze features of the same type\nnumeric = ['Age', 'SibSp', 'Parch', 'Fare']\ncategoric = ['Pclass', 'Sex', 'Ticket', 'Cabin', 'Embarked']\ntruth = ['Survived']\npassengerId = ['PassengerId']\n\ninitial_exploration(train_set, numeric, categoric+truth, 'Ticket')","bc9aa4a0":"def transformations(aSet, trainSet):\n    #dropping cabin\n    aSet.drop('Cabin', inplace=True, axis = 1)\n    \n    #transforming ticket data\n    aSet['Ticket_num'] = aSet['Ticket'].apply(lambda row: getTicketNumber(row))\n    aSet['Ticket_num'] = aSet['Ticket_num'].fillna(trainSet['Ticket_num'].mean())\n    aSet['Ticket_num'] = aSet['Ticket_num'].astype(int)\n    aSet.drop('Ticket', inplace = True, axis = 1)\n    \n    #deal with missing data\n    attendMissingData(aSet, trainSet)\n    \ndef getTicketNumber(ticket):\n    #gets any number after a space, if there is no space we keep the original ticket number\n    pattern = re.compile(\"(?<= )\\d+\")\n    ticketNum = re.search(pattern, ticket)\n    if ticketNum is not None:\n        return int(ticketNum.group(0))\n    if ticket.isnumeric():\n        return int(ticket)\n    else:\n        return np.nan\n    \ndef attendMissingData(aSet, trainSet):\n    #dropping all missing values of Embarked, since there are very few\n    trainSet.dropna(axis = 0, subset = ['Embarked'], inplace = True)\n    \n    #filling in all missing values of age, since there are fewer missing values,\n    #and we can reasonably assume that the age of any person is around the mean\n    aSet['Age'] = aSet['Age'].fillna(trainSet['Age'].mean())\n    aSet['Fare'] = aSet['Fare'].fillna(trainSet['Fare'].mean())","d7712450":"#performing the transformations on the training dataset\ntransformations(train_set, train_set)\ntransformations(test_set, train_set)","1e11dd5f":"#checking to see the result of our transformations\ntrain_set.info()\ntest_set.info()","0b97d68c":"#re analyzing the dataset\ntrain_set.describe()","9b21baa9":"train_set.head(10)","560a108f":"#adding the new feature to the features list and removing the old ones\nnumeric.append('Ticket_num')\ncategoric.remove('Ticket')\ncategoric.remove('Cabin')","561f239b":"#doing more exploratory analysis with new features\ninitial_exploration(train_set, numeric, categoric+truth, 'Ticket_num')","5ee19cc7":"#finds outliers multi standard deviations from the mean\ndef findOutliers(aSet, trainSet, col, mult):\n    mean = trainSet[col].mean()\n    std = trainSet[col].std()\n    cut_off = std * mult\n    lower, upper = mean - cut_off, mean + cut_off\n    \n    return aSet[(aSet[col] < upper) & (aSet[col] > lower)]","b7b6aa21":"trimmed_train = pd.DataFrame(findOutliers(train_set, train_set, 'Fare', 5))\ntrimmed_train_2 = pd.DataFrame(findOutliers(trimmed_train, trimmed_train, 'Ticket_num', 3))","df1665b1":"sns.histplot(trimmed_train['Fare'])\nplt.show()\nsns.histplot(trimmed_train_2['Ticket_num'])\nplt.show()","438bb577":"def normalizeCol(aSet, trainSet, col):\n    log = PowerTransformer()\n    log.fit(trainSet[[col]])\n    return log.transform(aSet[[col]])","8bd4e4f9":"trimmed_train_2['norm_fare'] = normalizeCol(trimmed_train_2, trimmed_train_2, 'Fare')\nsns.histplot(trimmed_train_2['norm_fare'])\nplt.show()\n\nnumeric.append('norm_fare')\nprint(trimmed_train_2[numeric].corr())\nsns.heatmap(trimmed_train_2[numeric].corr())\nplt.show()\n\ntest_set['norm_fare'] = normalizeCol(test_set, trimmed_train_2, 'Fare')","0cc9123f":"#looks better\nnumeric.remove('Fare')","c2b72f7c":"trimmed_train_2['Pclass'] = trimmed_train_2['Pclass'].astype(str)\ntest_set['Pclass'] = test_set['Pclass'].astype(str)\n\ndum_trim_train = pd.get_dummies(trimmed_train_2[numeric + categoric + truth + passengerId])\ntest_set_dum = pd.get_dummies(test_set[numeric + categoric + passengerId])","70764930":"def standardizeCol(aSet, trainSet, col):\n    scaler = StandardScaler()\n    scaler.fit(trainSet[[col]])\n    return scaler.transform(aSet[[col]])","a7f0c789":"X = dum_trim_train.loc[:, dum_trim_train.columns != 'Survived']\ny = dum_trim_train.loc[:, dum_trim_train.columns == 'Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y)\ny_train = y_train['Survived']\n\nscaler = StandardScaler()\nX_train_scaled = X_train.copy()\nX_train_scaled[numeric] = scaler.fit_transform(X_train_scaled[numeric])\nX_test_scaled = X_test.copy()\nX_test_scaled[numeric] = scaler.fit_transform(X_test_scaled[numeric])\n\n\ntest_set_scaled = test_set_dum.copy()\ntest_set_scaled[numeric] = scaler.fit_transform(test_set_scaled[numeric])","de965867":"#linreg with uncaled data\nlinreg = LinearRegression()\ncv = cross_val_score(linreg, X_train, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","790a269c":"#linreg with scaled data\nlinregSC = LinearRegression()\ncv = cross_val_score(linregSC, X_train_scaled, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","b0f03925":"#KNN\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","edc721df":"#KNN\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn, X_train_scaled, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","14982352":"#decision tree\ndt = DecisionTreeClassifier()\ncv = cross_val_score(dt, X_train, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","047fe7ef":"#decision tree\ndt = DecisionTreeClassifier()\ncv = cross_val_score(dt, X_train_scaled, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","959579f0":"#SGD\nsgd = SGDClassifier()\ncv = cross_val_score(sgd, X_train, y_train, cv = 20)\nprint(cv)\nprint(cv.mean())","78e09bd4":"#in order to evalute the models\ndef performance(classifier, model_type):\n    print(model_type)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Params: ' + str(classifier.best_params_))","1d92c8d7":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors': np.arange(3, 30),\n                 'weights': ['uniform', 'distance'],\n                 'algorithm': ['auto', 'ball_tree', 'kd_tree'],\n                 'p': [1, 2]}\n\ngrid_knn = GridSearchCV(estimator = knn,\n                       param_grid = param_grid,\n                       cv = 8, \n                       n_jobs = -1,\n                       refit = True,\n                       return_train_score = True)\nknn_result = grid_knn.fit(X_train_scaled, y_train)\nperformance(knn_result, 'KNN')","87e33c16":"dt = DecisionTreeClassifier()\nparam_grid = {'criterion': ['gini', 'entropy'],\n             'max_depth': np.arange(1, 11),\n             'max_features': ['auto', 'sqrt', 'log2', None]}\n\ngrid_dt = GridSearchCV(estimator = dt,\n                       param_grid = param_grid,\n                       cv = 8, \n                       n_jobs = -1,\n                       refit = True,\n                       return_train_score = True)\ndt_result = grid_dt.fit(X_train, y_train)\nperformance(dt_result, 'Decision Tree')","fb55a585":"sgd = SGDClassifier()\nparam_grid = {'loss': ['hinge', 'log', 'perceptron', 'squared_loss', 'huber', 'epsilon_insensitive'],\n             'penalty': ['l1', 'l2', 'elasticnet'],\n             'alpha': [0.01, 0.1, 0.25],\n             'max_iter': [10000, 15000, 20000],\n             'shuffle': [False, True],\n             'early_stopping': [False, True]}\n\ngrid_sgd = GridSearchCV(estimator = sgd,\n                       param_grid = param_grid,\n                       cv = 8, \n                       n_jobs = -1,\n                       refit = True,\n                       return_train_score = True)\nsgd_result = grid_sgd.fit(X_train_scaled, y_train)\nperformance(sgd_result, 'Decision Tree')","0cb1f3da":"knn_predictions = knn_result.best_estimator_.predict(X_test_scaled).astype(int)\nprint(knn_predictions[0:10])\nprint(confusion_matrix(y_test, knn_predictions))\nknn_prob_pred = knn_result.best_estimator_.predict_proba(X_test_scaled)[:,1]\nprint(roc_auc_score(y_test, knn_prob_pred))\nprint()\n\ndt_predictions = dt_result.best_estimator_.predict(X_test).astype(int)\nprint(dt_predictions[0:10])\nprint(confusion_matrix(y_test, dt_predictions))\ndt_prob_pred = dt_result.best_estimator_.predict_proba(X_test)[:,1]\nprint(roc_auc_score(y_test, dt_prob_pred))\nprint()\n\nsgd_predictions = sgd_result.best_estimator_.predict(X_test_scaled).astype(int)\nprint(sgd_predictions[0:10])\nprint(roc_auc_score(y_test, sgd_predictions))","71abeff6":"#knn_finalP = knn_result.best_estimator_.predict(test_set_scaled).astype(int)\ndt_finalP = dt_result.best_estimator_.predict(test_set_dum).astype(int)\n#sgd_finalP = sgd_result.best_estimator_.predict(test_set_scaled).astype(int)","75ec2f3f":"#knn_final = {'PassengerId': test_set_scaled.PassengerId,\n#            'Survived': knn_finalP}\n#submission = pd.DataFrame(data = knn_final)\n\ndt_final = {'PassengerId': test_set_dum.PassengerId,\n           'Survived': dt_finalP}\nsubmission2 = pd.DataFrame(data = dt_final)\n\n#sgd_final = {'PassengerId': test_set_scaled.PassengerId,\n#           'Survived': sgd_finalP}\n#submission3 = pd.DataFrame(data = sgd_final)\n","f70347ac":"#submission.to_csv('submission_knn.csv', index = False)\nsubmission2.to_csv('submission_dtd.csv', index = False)\n#submission3.to_csv('submission_sgd.csv', index = False)","ff109cfa":"_Ticket and Cabin are not useful as categorical variables. They need to be transformed somehow to be readable._ \n\n- Cabin: Because of the large amount of missing values present in this column, I will be dropping it all together\n- Ticket: I will extract the ticket number from the ticket column, and reanalyze it as a numeric variable.","9a7f65f8":"_ANALYZING EXPLORATORY AND CATEGORICAL DATA_\n\n- printing histograms and correlation matrices for numeric data\n- printing bar plots for categoric data\n- printing pivot tables for both","07335b6f":"**LOADING DATA INTO TRAIN AND TEST SETS**","f83f2382":"_Dropping Cabin_\n\n_Extracting Ticket Numbers from Tickets_\n- using a pattern to get only the number after the beginnings\n- leaving tickets without prefixes alone\n- filling in bad data with NANs\n\n_Dealing with Missing Data_\n- dropping rows with missing Embarkment Data\n- filling in missing values of Age with training age mean\n- filling in missing values of Fare with training fare mean","50b52784":"**GENERATING THE SUBMISSION DATA WITH THE BEST MODEL**\n- getting the test set prediction from the decision tree model\n- turning those predictions into a dataFrame\n- turning the dataFrame into a csv","35177b72":"_It looks like the best model is the decision tree_","067b9557":"_Now we will create dummy varaibles for all categoric features_","e7c65eef":"_It looks like Ticket_num and Fare have some pretty serious outliers_","832c63d1":"_Looks better_","2b33b631":"_now that the data has been split and scaled, we can construct each of the following models_\n- Linear Regression\n- Nearest Neighbors\n- Decision Trees\n- SGDClassifier","a2425f50":"_Now we will normalize fare, as it is heavily skewed_","4122ea6a":"**IMPORT STATEMENTS**\n\n- numpy used for general calculations\n- pandas used for data reading, processing, and output\n- seaborn for plots\n- matplotlib.pyplot for plots\n- re for transforming text data\n- StandardScaler and PowerTransformer for standardization and normalization\n- train_test_split, cross_val_score, GridSearchCV for testing and analyzing models\n- LinearRegression, SGDClassifier, KNeighborsClassifier, DecisionTreeClassifier for models\n- confusion_matrix, accuracy_score for analysis of results","0fd04af5":"_Selecting the best models to pursue_\n- Linear regression did poorly, which is not strange, as this is a classification problem\n- The other models give a good start for tuning","a95572fb":"**BUILDING THE MODELS \/ EVALUATING THE MODELS**\n- Creating a train and test set from the training set\n- Initial testing of different models\n- Further testing of best models\n- Picking the best model","fab66e10":"**EXPLORATORY DATA ANALYSIS**"}}