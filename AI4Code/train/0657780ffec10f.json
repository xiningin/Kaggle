{"cell_type":{"1e0845c2":"code","39c16cd0":"code","8c8da15d":"code","f1abf9c7":"code","33c0b3f0":"code","a3bf5de3":"code","25059908":"code","bb541871":"code","0f20c589":"code","6d2cfdc3":"code","1c176bd8":"code","5d20f366":"code","ec86a0a7":"code","16143857":"code","10f0fe9c":"code","96efab6c":"code","a2f8f7d8":"code","20b962d2":"code","3fa0f0de":"markdown","c9ecee45":"markdown","5d9f4ee8":"markdown","c5187b80":"markdown","c8c7504f":"markdown","bf565379":"markdown","f785549d":"markdown","14d3bb51":"markdown","4698dca5":"markdown","fb9b8886":"markdown","e7ae8afd":"markdown","9872f0a3":"markdown","82bd5757":"markdown","a0c43f6f":"markdown","afc8b86c":"markdown","1bfd99c1":"markdown","9f43b5c3":"markdown"},"source":{"1e0845c2":"import os                                   # os package\nimport time                                 # time package\nimport torch                                # root package\nimport torch.nn as nn                       # neural networks\nimport torch.optim as optim                 # Optimizers e.g. gradient descent, ADAM, etc. \nimport torchvision.datasets as dset         # dataset representation\nfrom torch.utils.data import DataLoader     # dataset loading\nimport torchvision.transforms as transforms # composable transforms\nfrom torchvision.utils import make_grid, save_image\nfrom torch.autograd import Variable\n\n%matplotlib inline\nimport random                               # random package\nimport numpy as np                          # package for scientific computing \nimport matplotlib.pyplot as plt             # visualization package\nimport matplotlib.image as mpimg\nimport matplotlib.animation as animation\nimport warnings                             # supress warnings\nwarnings.filterwarnings('ignore')\nfrom tqdm import tqdm_notebook as tqdm      # progres package\n\n# random seed everything\ndef random_seed(seed_value, use_cuda):\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu vars\n    random.seed(seed_value) # Python\n    if use_cuda: \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False","39c16cd0":"random_seed(17, True)","8c8da15d":"!unzip ..\/input\/generative-dog-images\/all-dogs.zip -d \/kaggle\/working\/ > \/dev\/null","f1abf9c7":"PATH = '\/kaggle\/working\/all-dogs\/'\nimages = os.listdir(PATH)\nprint(f'There are {len(images)} pictures of dogs.')\n\nfig, axes = plt.subplots(nrows=3, ncols=3, figsize=(12,10))\n\nfor indx, axis in enumerate(axes.flatten()):\n    rnd_indx = np.random.randint(0, len(os.listdir(PATH)))\n    img = plt.imread(PATH + images[rnd_indx])\n    imgplot = axis.imshow(img)\n    axis.set_title(images[rnd_indx])\n    axis.set_axis_off()\nplt.tight_layout(rect=[0, 0.03, 1, 0.95])","33c0b3f0":"batch_size = 32\nimage_size = 64\n\nrandom_transforms = [transforms.ColorJitter(), transforms.RandomRotation(degrees=20)]\ntransform = transforms.Compose([transforms.Resize(64),\n                                transforms.CenterCrop(64),\n                                transforms.RandomHorizontalFlip(p=0.5),\n                                transforms.RandomApply(random_transforms, p=0.2),\n                                transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_data = dset.ImageFolder('\/kaggle\/working\/', transform=transform)\ntrain_loader = torch.utils.data.DataLoader(train_data, shuffle=True, batch_size=batch_size)\n                                           \nimgs, label = next(iter(train_loader))\nimgs = imgs.numpy().transpose(0, 2, 3, 1)","a3bf5de3":"def weights_init(m):\n    \"\"\"\n    Takes as input a neural network m that will initialize all its weights.\n    \"\"\"\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        m.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        m.weight.data.normal_(1.0, 0.02)\n        m.bias.data.fill_(0)","25059908":"class Generator(nn.Module):\n    def __init__(self, nz=128, channels=3):\n        super(Generator, self).__init__()\n        \n        self.nz = nz\n        self.channels = channels\n        \n        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0):\n            block = [\n                nn.ConvTranspose2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False),\n                nn.BatchNorm2d(n_output),\n                nn.ReLU(inplace=True),\n            ]\n            return block\n\n        self.model = nn.Sequential(\n            *convlayer(self.nz, 1024, 4, 1, 0), # Fully connected layer via convolution.\n            *convlayer(1024, 512, 4, 2, 1),\n            *convlayer(512, 256, 4, 2, 1),\n            *convlayer(256, 128, 4, 2, 1),\n            *convlayer(128, 64, 4, 2, 1),\n            nn.ConvTranspose2d(64, self.channels, 3, 1, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, z):\n        z = z.view(-1, self.nz, 1, 1)\n        img = self.model(z)\n        return img","bb541871":"class Discriminator(nn.Module):\n    def __init__(self, channels=3):\n        super(Discriminator, self).__init__()\n        \n        self.channels = channels\n\n        def convlayer(n_input, n_output, k_size=4, stride=2, padding=0, bn=False):\n            block = [nn.Conv2d(n_input, n_output, kernel_size=k_size, stride=stride, padding=padding, bias=False)]\n            if bn:\n                block.append(nn.BatchNorm2d(n_output))\n            block.append(nn.LeakyReLU(0.2, inplace=True))\n            return block\n\n        self.model = nn.Sequential(\n            *convlayer(self.channels, 32, 4, 2, 1),\n            *convlayer(32, 64, 4, 2, 1),\n            *convlayer(64, 128, 4, 2, 1, bn=True),\n            *convlayer(128, 256, 4, 2, 1, bn=True),\n            nn.Conv2d(256, 1, 4, 1, 0, bias=False),  # FC with Conv.\n        )\n\n    def forward(self, imgs):\n        logits = self.model(imgs)\n        out = torch.sigmoid(logits)\n    \n        return out.view(-1, 1)","0f20c589":"!mkdir results\n\nbatch_size = 32\nLR_G = 0.001\nLR_D = 0.0005\n\nbeta1 = 0.5\nepochs = 100\n\nreal_label = 0.9\nfake_label = 0\nnz = 128\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","6d2cfdc3":"netG = Generator(nz).to(device)\nnetD = Discriminator().to(device)\n\ncriterion = nn.BCELoss()\n\noptimizerD = optim.Adam(netD.parameters(), lr=LR_D, betas=(beta1, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=LR_G, betas=(beta1, 0.999))\n\nfixed_noise = torch.randn(25, nz, 1, 1, device=device)\n\nG_losses = []\nD_losses = []\nepoch_time = []","1c176bd8":"def plot_loss (G_losses, D_losses, epoch):\n    plt.figure(figsize=(10,5))\n    plt.title(\"Generator and Discriminator Loss - EPOCH \"+ str(epoch))\n    plt.plot(G_losses,label=\"G\")\n    plt.plot(D_losses,label=\"D\")\n    plt.xlabel(\"iterations\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()","5d20f366":"def show_generated_img(n_images=5):\n    sample = []\n    for _ in range(n_images):\n        noise = torch.randn(1, nz, 1, 1, device=device)\n        gen_image = netG(noise).to(\"cpu\").clone().detach().squeeze(0)\n        gen_image = gen_image.numpy().transpose(1, 2, 0)\n        sample.append(gen_image)\n    \n    figure, axes = plt.subplots(1, len(sample), figsize = (64,64))\n    for index, axis in enumerate(axes):\n        axis.axis('off')\n        image_array = sample[index]\n        axis.imshow(image_array)\n        \n    plt.show()\n    plt.close()","ec86a0a7":"for epoch in range(epochs):\n    \n    start = time.time()\n    for ii, (real_images, train_labels) in tqdm(enumerate(train_loader), total=len(train_loader)):\n        ############################\n        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n        ###########################\n        # train with real\n        netD.zero_grad()\n        real_images = real_images.to(device)\n        batch_size = real_images.size(0)\n        labels = torch.full((batch_size, 1), real_label, device=device)\n\n        output = netD(real_images)\n        errD_real = criterion(output, labels)\n        errD_real.backward()\n        D_x = output.mean().item()\n\n        # train with fake\n        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n        fake = netG(noise)\n        labels.fill_(fake_label)\n        output = netD(fake.detach())\n        errD_fake = criterion(output, labels)\n        errD_fake.backward()\n        D_G_z1 = output.mean().item()\n        errD = errD_real + errD_fake\n        optimizerD.step()\n\n        ############################\n        # (2) Update G network: maximize log(D(G(z)))\n        ###########################\n        netG.zero_grad()\n        labels.fill_(real_label)  # fake labels are real for generator cost\n        output = netD(fake)\n        errG = criterion(output, labels)\n        errG.backward()\n        D_G_z2 = output.mean().item()\n        optimizerG.step()\n        \n        # Save Losses for plotting later\n        G_losses.append(errG.item())\n        D_losses.append(errD.item())\n        \n        if (ii+1) % (len(train_loader)\/\/2) == 0:\n            print('[%d\/%d][%d\/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f \/ %.4f'\n                  % (epoch + 1, epochs, ii+1, len(train_loader),\n                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2))\n            \n    plot_loss (G_losses, D_losses, epoch)\n    G_losses = []\n    D_losses = []\n    if epoch % 10 == 0:\n        show_generated_img()\n\n    epoch_time.append(time.time()- start)","16143857":"print (\">> average EPOCH duration = \", np.mean(epoch_time))","10f0fe9c":"if not os.path.exists('..\/output_images'):\n    os.mkdir('..\/output_images')\n    \nim_batch_size = 50\nn_images=10000\n\nfor i_batch in tqdm(range(0, n_images, im_batch_size)):\n    gen_z = torch.randn(im_batch_size, nz, 1, 1, device=device)\n    gen_images = netG(gen_z)\n    images = gen_images.to(\"cpu\").clone().detach()\n    images = images.numpy().transpose(0, 2, 3, 1)\n    for i_image in range(gen_images.size(0)):\n        save_image(gen_images[i_image, :, :, :], os.path.join('..\/output_images', f'image_{i_batch+i_image:05d}.png'))","96efab6c":"fig = plt.figure(figsize=(25, 16))\n# display 10 images from each class\nfor i, j in enumerate(images[:32]):\n    ax = fig.add_subplot(4, 8, i + 1, xticks=[], yticks=[])\n    plt.imshow(j)","a2f8f7d8":"import shutil\nshutil.make_archive('images', 'zip', '..\/output_images')","20b962d2":"torch.save(netG.state_dict(), 'generator.pth')\ntorch.save(netD.state_dict(), 'discriminator.pth')","3fa0f0de":"### Generator","c9ecee45":"#### Save models","5d9f4ee8":"### Initialize models and optimizers","c5187b80":"### Plot Loss per EPOCH","c8c7504f":"### Training Loop","bf565379":"#### Generated Images","f785549d":"**Importing the libraries**","14d3bb51":"<center>\nGenerative Adversarial Networks (GANs)\n---\n<\/center>\n![](https:\/\/espresso-jobs.com\/conseils-carriere\/wp-content\/uploads\/2019\/05\/monalisa.gif)\n<br>","4698dca5":"### Look at some dogs\nThe Stanford Dogs dataset contains images of 120 breeds of dogs from around the world.","fb9b8886":"### Submission","e7ae8afd":"### Show generated images","9872f0a3":"### Weights\nDefining the `weights_init` function","82bd5757":"### Training","a0c43f6f":"### Image Preprocessing","afc8b86c":"# Introduction\n\n- Generative Adversarial Networks (GANs)\n- How GANs Work\n- GANs Process\n- Examples\n\n### Generative Adversarial Networks (GANs)\n\nGenerative Adversarial Networks are used to generate images that never existed before. They learn about the world (objects, animals and so forth) and create new versions of those images that never existed.\n\nThey have two components:\n\n- A **Generator** - this creates the images.\n- A **Discriminator** - this assesses the images and tells the generator if they are similar to what it has been trained on. These are based off real world examples.\n\nWhen training the network, both the generator and discriminator start from scratch and learn together.\n\n### How GANs Work\n\n\n**G** for **Generative** - this is a model that takes an input as a random noise singal and then outputs an image.\n\n![](https:\/\/camo.githubusercontent.com\/a2c5a0db812c0ade199e5ccacf86c6cff4db1685\/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f63762f67656e657261746976652e706e67)\n\n**A** for **Adversarial** - this is the discriminator, the opponent of the generator. This is capable of learning about objects, animals or other features specified. For example: if you supply it with pictures of dogs and non-dogs, it would be able to identify the difference between the two.\n\n![](https:\/\/camo.githubusercontent.com\/96c8ccb9a91b8789106c1b3dfc9d62dde9d3cbe1\/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f63762f6469736372696d696e61746f722d6578616d706c652e706e67)\n\nUsing this example, once the discriminator has been trained, showing the discriminator a picture that isn't a dog it will return a 0. Whereas, if you show it a dog it will return a 1.\n\n![](https:\/\/camo.githubusercontent.com\/8b5978b05b5ab4cd9bfba4819a0f0e09a12c8068\/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f63762f6469736372696d696e61746f722d73636f7265732e706e67)\n\n**N** for **Network** - meaning the generator and discriminator are both neural networks.\n\n\n### GANs Process\n\n**Step 1** - we input a random noise signal into the generator. The generator creates some images which is used for training the discriminator. We provide the discriminator with some features\/images we want it to learn and the discriminator outputs probabilities. These probabilities can be rather high as the discriminator has only just started being trained. The values are then assessed and identified. The error is calculated and these are backpropagated through the discriminator, where the weights are updated.\n\n![](https:\/\/camo.githubusercontent.com\/a26a06e2437514df1bbd736480f06a86aabebef8\/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f63762f73746570312d6469736372696d696e61746f722e706e67)\n\nNext we train the generator. We take the batch of images that it created and put them through the discriminator again. We do not include the feature images. The generator learns by tricking the discriminator into it outputting false positives.\n\nThe discriminator will provide an output of probabilities. The values are then assessed and compared to what they should have been. The error is calculated and backpropagated through the generator and the weights are updated.\n\n![](https:\/\/camo.githubusercontent.com\/07a68fab0dbea632b29d6186e298a6c05333497c\/68747470733a2f2f61636975732e636f2e756b2f77702d636f6e74656e742f7468656d65732f61636975732f6d616368696e655f6c6561726e696e672f696d67732f63762f73746570312d67656e657261746f722e706e67)\n\n**Step 2** - This is the same as step 1 but the generator and discriminator are trained a little more. Through backpropagation the generator understands its mistakes and starts to make them more like the feature.\n\nThis is created through a *Deconvolutional Neural Network*.\n\n### Examples\n\n**GANs** can be used for the following:\n\n- Generating Images\n- Image Modification\n- Super Resolution\n- Assisting Artists\n- Photo-Realistic Images\n- Speech Generation\n- Face Ageing\n\n<br>","1bfd99c1":"### Discriminator","9f43b5c3":"### Parameters"}}