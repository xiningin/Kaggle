{"cell_type":{"a3e0385d":"code","1f508f07":"code","49a115cf":"code","a08be96b":"code","f54a59f7":"code","419c9bb3":"code","60d4e916":"code","baed7e1d":"code","948aa9bf":"code","da415c84":"code","fcfdd99d":"code","362d2dcc":"code","6fb7c248":"code","843543a8":"code","6103b96c":"code","e4d19454":"code","9d9818b8":"code","16410a2c":"code","eca2b157":"code","2c2f89b1":"code","397fffe7":"code","ae161785":"code","d59dec38":"code","3fab5f9a":"code","7fa5bd9a":"code","0af3cbd0":"code","2010a680":"code","64247838":"code","9f9d68a0":"code","27e77a69":"markdown","8c4e0979":"markdown","f5f3345a":"markdown","91236684":"markdown","b4b481e6":"markdown","2e54879f":"markdown","90694fb0":"markdown","ad024be4":"markdown","0bfe4548":"markdown","eb512ade":"markdown","a17c1310":"markdown","e4a866ab":"markdown","afd6b2fd":"markdown","a4fc7813":"markdown","239ae668":"markdown","577568cd":"markdown","b61757ea":"markdown","b1002654":"markdown","93def560":"markdown","a424e521":"markdown","7b7f20e6":"markdown","e553420d":"markdown","b9988f53":"markdown","9f0bc630":"markdown","ed45d58e":"markdown","10902da6":"markdown","041b439c":"markdown","423ec306":"markdown","9246bfa8":"markdown","9a70d737":"markdown","8a1b8ed6":"markdown","5d227eee":"markdown","92a9018b":"markdown","62602a61":"markdown","c544add3":"markdown","bd9d405e":"markdown","5858bba4":"markdown","a47f3106":"markdown","7921d70c":"markdown","777be385":"markdown","44d6963c":"markdown","ec6d511a":"markdown","dfe43599":"markdown","169937ea":"markdown","28eba03d":"markdown","8486d97c":"markdown","3ae78f2f":"markdown"},"source":{"a3e0385d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n","1f508f07":"data_cancer= pd.read_csv(\"\/kaggle\/input\/breast-cancer\/breast-cancer_csv.csv\")\ndata_cancer.head()","49a115cf":"data_cancer.info()","a08be96b":"data_cancer.isna().sum()","f54a59f7":"data_cancer.dropna(inplace=True)","419c9bb3":"from sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\ndataset_cancer = data_cancer.values\nX_bc = dataset_cancer[:, :-1]\ny_bc = dataset_cancer[:,-1]\nX_bc = X_bc.astype(str)\n\ndef prepare_inputs(X):\n  oe = OrdinalEncoder()\n  oe.fit(X)\n  X_enc = oe.transform(X)\n  return X_enc\n# prepare target\ndef prepare_targets(y):\n  le = LabelEncoder()\n  le.fit(y)\n  y_enc = le.transform(y)\n  return y_enc\n# prepare input data\nX_bc_enc = prepare_inputs(X_bc)\n# prepare output data\ny_bc_enc = prepare_targets(y_bc)\n\n# split into train and test sets\nX_train_bc_enc, X_test_bc_enc, y_train_bc_enc, y_test_bc_enc = train_test_split(X_bc_enc, y_bc_enc, test_size=0.33, random_state=1)\n# summarize\nprint('Train', X_train_bc_enc.shape, y_train_bc_enc.shape)\nprint('Test', X_test_bc_enc.shape, y_test_bc_enc.shape)","60d4e916":"from sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\n# feature selection\ndef select_features_chi(X_train, y_train, X_test): \n    fs = SelectKBest(score_func=chi2, k='all') \n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n# feature selection\nX_train_bc_fs, X_test_bc_fs, fs_bc = select_features_chi(X_train_bc_enc, y_train_bc_enc, X_test_bc_enc)\n# what are scores for the features\nfor i in range(len(fs_bc.scores_)):\n    print('Feature %d: %f' % (i, fs_bc.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs_bc.scores_))], fs_bc.scores_) \nplt.show()","baed7e1d":"from sklearn.feature_selection import mutual_info_classif\n# feature selection\ndef select_features_mutual(X_train, y_train, X_test):\n    fs = SelectKBest(score_func= mutual_info_classif, k='all') \n    fs.fit(X_train, y_train)\n    X_train_fs = fs.transform(X_train)\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n# feature selection\nX_train_bc_fs, X_test_bc_fs, fs_bc = select_features_mutual(X_train_bc_enc, y_train_bc_enc, X_test_bc_enc)\n# what are scores for the features\nfor i in range(len(fs_bc.scores_)): \n    print('Feature %d: %f' % (i, fs_bc.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs_bc.scores_))], fs_bc.scores_)\nplt.show()","948aa9bf":"data_db = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndata_db.head()","da415c84":"data_db.info()","fcfdd99d":"dataset_db = data_db.values\n# split into input and output variables\nX_db = dataset_db[:, :-1]\ny_db = dataset_db[:,-1]\n# split into train and test sets\nX_train_db, X_test_db, y_train_db, y_test_db = train_test_split(X_db, y_db, test_size=0.33, random_state=1) # summarize\nprint('Train', X_train_db.shape, y_train_db.shape)\nprint('Test', X_test_db.shape, y_test_db.shape)","362d2dcc":"from sklearn.feature_selection import f_classif\n# feature selection\ndef select_features_anova(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=f_classif, k='all') # learn relationship from training data \n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n# feature selection\nX_train_db_fs, X_test_db_fs, fs_db = select_features_anova(X_train_db, y_train_db, X_test_db)\n# what are scores for the features\nfor i in range(len(fs_db.scores_)):\n    print('Feature %d: %f' % (i, fs_db.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs_db.scores_))], fs_db.scores_) \nplt.show()","6fb7c248":"# feature selection\nX_train_db_fs, X_test_db_fs, fs_db = select_features_mutual(X_train_db, y_train_db, X_test_db)\n# what are scores for the features\nfor i in range(len(fs_db.scores_)):\n    print('Feature %d: %f' % (i, fs_db.scores_[i]))\n# plot the scores\nplt.bar([i for i in range(len(fs_db.scores_))], fs_db.scores_) \nplt.show()","843543a8":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\ndata = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\n# retrieve numpy array\ndataset = data.values\n# split into input (X) and output (y) variables\nX = dataset[:, :-1]\ny = dataset[:,-1]\n# define the evaluation method\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# define the pipeline to evaluate\nmodel = LogisticRegression(solver='liblinear')\nfs = SelectKBest(score_func=f_classif)\npipeline = Pipeline(steps=[('anova',fs), ('lr', model)])\n# define the grid\ngrid = dict()\ngrid['anova__k'] = [i+1 for i in range(X.shape[1])]\n# define the grid search\nsearch = GridSearchCV(pipeline, grid, scoring='accuracy', n_jobs=-1, cv=cv) # perform the search\nresults = search.fit(X, y)\n# summarize best\nprint('Best Mean Accuracy: %.3f' % results.best_score_)\nprint('Best Config: %s' % results.best_params_)","6103b96c":"from sklearn.datasets import make_regression\n# generate regression dataset\nX, y = make_regression(n_samples=1000, n_features=100, n_informative=10, noise=0.1,\n    random_state=1)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# summarize\nprint('Train', X_train.shape, y_train.shape) \nprint('Test', X_test.shape, y_test.shape)","e4d19454":"from sklearn.feature_selection import f_regression\n# feature selection\ndef select_features(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=f_regression, k='all') # learn relationship from training data \n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(10): \n    print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.figure(figsize=(10,8))\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","9d9818b8":"from sklearn.feature_selection import mutual_info_regression\n# feature selection\ndef select_features_reg(X_train, y_train, X_test):\n    # configure to select all features\n    fs = SelectKBest(score_func=mutual_info_regression, k='all') # learn relationship from training data\n    fs.fit(X_train, y_train)\n    # transform train input data\n    X_train_fs = fs.transform(X_train)\n    # transform test input data\n    X_test_fs = fs.transform(X_test)\n    return X_train_fs, X_test_fs, fs\n\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# what are scores for the features\nfor i in range(10): print('Feature %d: %f' % (i, fs.scores_[i]))\n# plot the scores\nplt.figure(figsize=(10,8))\nplt.bar([i for i in range(len(fs.scores_))], fs.scores_)\nplt.show()","16410a2c":"from sklearn.datasets import make_classification\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.feature_selection import RFE\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# create pipeline\nrfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5) \nmodel = DecisionTreeClassifier()\npipeline = Pipeline(steps=[('s',rfe),('m',model)])\n# evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","eca2b157":"from sklearn.model_selection import RepeatedKFold\nfrom sklearn.tree import DecisionTreeRegressor\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# create pipeline\nrfe = RFE(estimator=DecisionTreeRegressor(), n_features_to_select=5) \nmodel = DecisionTreeRegressor()\npipeline = Pipeline(steps=[('s',rfe),('m',model)])\n# evaluate model\ncv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(pipeline, X, y, scoring='neg_mean_absolute_error', cv=cv,\nn_jobs=-1)\n# report performance\nprint('MAE: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","2c2f89b1":"# get the dataset\ndef get_dataset():\n  X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n      random_state=1)\n  return X, y\n# get a list of models to evaluate\ndef get_models():\n  models = dict()\n  for i in range(2, 10):#number features to select\n    rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=i) \n    model = DecisionTreeClassifier()\n    models[str(i)] = Pipeline(steps=[('s',rfe),('m',model)])\n  return models\n# evaluate a given model using cross-validation\ndef evaluate_model(model, X, y):\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1) \n    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1) \n    return scores\n# define dataset\nX, y = get_dataset()\n# get the models to evaluate\nmodels = get_models()\n# evaluate the models and store results\nresults, names = list(), list()\nfor name, model in models.items():\n    scores = evaluate_model(model, X, y)\n    results.append(scores)\n    names.append(name)\n    print('>%s %.3f (%.3f)' % (name, np.mean(scores), np.std(scores)))\n# plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","397fffe7":"from sklearn.feature_selection import RFECV\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# create pipeline\nrfe = RFECV(estimator=DecisionTreeClassifier()) \nmodel = DecisionTreeClassifier()\npipeline = Pipeline(steps=[('s',rfe),('m',model)]) # evaluate model\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\nn_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1) # report performance\nprint('Accuracy: %.3f (%.3f)' % (np.mean(n_scores), np.std(n_scores)))","ae161785":"rfe = RFE(estimator=DecisionTreeClassifier(), n_features_to_select=5)\n# fit RFE\nrfe.fit(X, y)\n# summarize all features\nfor i in range(X.shape[1]):\n    print('Column: %d, Selected=%s, Rank: %d' % (i, rfe.support_[i], rfe.ranking_[i]))","d59dec38":"from sklearn.linear_model import LinearRegression\n# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# define the model\nmodel = LinearRegression()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.coef_\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance) \nplt.show()","3fab5f9a":"from sklearn.linear_model import LogisticRegression\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# define the model\nmodel = LogisticRegression()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.coef_[0]\n# summarize feature importance\nfor i,v in enumerate(importance): print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","7fa5bd9a":"# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1)\n# define the model\nmodel = DecisionTreeRegressor()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance): print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","0af3cbd0":"# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# define the model\nmodel = DecisionTreeClassifier()\n# fit the model\nmodel.fit(X, y)\n# get importance\nimportance = model.feature_importances_\n# summarize feature importance\nfor i,v in enumerate(importance): print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance)\nplt.show()","2010a680":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.inspection import permutation_importance\n# define dataset\nX, y = make_regression(n_samples=1000, n_features=10, n_informative=5, random_state=1) # define the model\nmodel = KNeighborsRegressor()\n# fit the model\nmodel.fit(X, y)\n# perform permutation importance\nresults = permutation_importance(model, X, y, scoring='neg_mean_squared_error')\n# get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance) \nplt.show()","64247838":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.inspection import permutation_importance\n# define dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# define the model\nmodel = KNeighborsClassifier()\n# fit the model\nmodel.fit(X, y)\n# perform permutation importance\nresults = permutation_importance(model, X, y, scoring='accuracy') # get importance\nimportance = results.importances_mean\n# summarize feature importance\nfor i,v in enumerate(importance):\n    print('Feature: %0d, Score: %.5f' % (i,v))\n# plot feature importance\nplt.bar([x for x in range(len(importance))], importance) \nplt.show()","9f9d68a0":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectFromModel\n# feature selection\ndef select_features(X_train, y_train, X_test):\n  # configure to select a subset of features\n  fs = SelectFromModel(RandomForestClassifier(n_estimators=1000), max_features=5)\n  # learn relationship from training data\n  fs.fit(X_train, y_train)\n  # transform train input data\n  X_train_fs = fs.transform(X_train)\n  # transform test input data\n  X_test_fs = fs.transform(X_test)\n  return X_train_fs, X_test_fs, fs\n# define the dataset\nX, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5,\n    random_state=1)\n# split into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# feature selection\nX_train_fs, X_test_fs, fs = select_features(X_train, y_train, X_test)\n# fit the model\nmodel = LogisticRegression(solver='liblinear') \nmodel.fit(X_train_fs, y_train)\n# evaluate the model\nyhat = model.predict(X_test_fs)\n# evaluate predictions\naccuracy = accuracy_score(y_test, yhat) \nprint('Accuracy: %.2f' % (accuracy*100))","27e77a69":"\n1. ColumnTransformer class \n\nTo separately select numerical input variables and categorical input variables using appropriate metrics.\n\n2. Wrapper method\n* Tree-Searching Methods (depth-first, breadth-first, etc.).\n* Stochastic Global Search (simulated annealing, genetic algorithm).\n* Step-Wise Models.\n* RFE.\n\n3. Intrinsic\n* Classification and Regression Trees (CART).\n* Random Forest\n* Bagged Decision Trees\n* Gradient Boosting\n\n4. Filter\n\nUse statistical measures to score the correlation or dependence between input variables that can be filtered to choose the most relevant features.\n* Select the top k variables: SelectKBest.\n","8c4e0979":"The plot clearly shows 8 to 10 features are a lot more important than the other features. We could set k = 10 When configuring the SelectKBest to select these top features.\n","f5f3345a":"## Feature Selection with Importance<a class=\"anchor\" id=\"section_5_4\"><\/a>","91236684":"> First, a model is fit on the dataset, such as a model that does not support native feature importance scores. Then the model is used to make predictions on a dataset, although the values of a feature (column) in the dataset are scrambled. This is repeated for each feature in the dataset. Then this whole process is repeated 3, 5, 10 or more times. The result is a mean importance score for each input feature (and distribution of scores given the repeats).","b4b481e6":"### Table of Contents\n\n* [1.What is Feature Selection](#chapter1)\n* [2.How to Select Categorical Input Features](#chapter2)\n    * [Chi-Squared Feature Selection](#section_2_1)\n    * [Mutual Information Feature Selection](sSection_2_2)\n* [2.How to Select Numerical Input Features](#chapter3)\n    * [ANOVA F-test Feature Selection](#section_3_1)\n    * [Mutual Information Feature Selection](#section_3_2)\n* [3.How to Select Features for Numerical Output](#chapter3)\n    * [Correlation Feature Selection ](#section_3_1)\n    * [Mutual Information Feature Selection](#section_3_2)\n* [4.How to Use RFE for Feature Selection ](#chapter4)\n    * [RFE for Classification](#section_4_1)\n    * [RFE for Regression](#section_4_2)\n    * [RFE Hyperparameters](#section_4_3)\n* [5.How to Use Feature Importance](#chapter5)\n    * [CoefficientsasFeatureImportance ](#section_5_1)\n    * [Decision Tree Feature Importance ](#section_5_2)\n    * [Permutation Feature Importance](#section_5_3)   \n    * [Feature Selection with Importance](#section_5_4) \n    ","2e54879f":"**Supervised feature selection methods may further be classified into three groups, including intrinsic, wrapper, filter methods.**\n\n* Intrinsic: Algorithms that perform automatic feature selection during training.\n\n* Filter: Select subsets of features based on their relationship with the target.\n\n* Wrapper: Search subsets of features that perform according to a predictive model.","90694fb0":"# 5.How to Use Feature Importance <a class=\"anchor\" id=\"chapter5\"><\/a>\n","ad024be4":"## ANOVA F-test Feature Selection<a class=\"anchor\" id=\"section_3_1\"><\/a>","0bfe4548":"## RFE for Classification <a class=\"anchor\" id=\"section_4_1\"><\/a>","eb512ade":"**Which Features Were Selected**","a17c1310":"# 2.How to Select Numerical Input Features #","e4a866ab":"# 3.How to Select Features for Numerical Output <a class=\"anchor\" id=\"chapter3\"><\/a>","afd6b2fd":"## Decision Tree Feature Importance<a class=\"anchor\" id=\"section_5_2\"><\/a>","a4fc7813":"# 4.How to Use RFE for Feature Selection <a class=\"anchor\" id=\"chapter4\"><\/a>","239ae668":"*Decision tree algorithms like classification and regression trees (CART) offer importance scores based on the reduction in the criterion used to select split points, like Gini or entropy. This same approach can be used for ensembles of decision trees, such as the random forest and stochastic gradient boosting algorithms.*","577568cd":"We can use the SelectFromModel class to define both the model we wish to calculate importance scores, RandomForestClassifier in this case, and the number of features to select, 5 in this case.","b61757ea":"## Tune the Number of Selected Features ##","b1002654":"## RFE for Regression<a class=\"anchor\" id=\"section_4_2\"><\/a>","93def560":"![Screen Shot 2021-11-05 at 21.50.01.png](attachment:d63d0b25-925d-4fa3-b2de-6588575984d1.png)","a424e521":"## Mutual Information Feature Selection <a class=\"anchor\" id=\"section_2_2\"><\/a>","7b7f20e6":"## Permutation Feature Importance<a class=\"anchor\" id=\"section_5_3\"><\/a>","e553420d":"*Feature importance refers to techniques that assign a score to input features based on how useful they are at predicting a target variable.*","b9988f53":"*Correlation is a measure of how two variables change together.*","9f0bc630":"* Numerical Output: Regression predictive modeling problem.\n* Categorical Output: Classification predictive modeling problem.\n","ed45d58e":"# 1. What is Feature Selection? <a class=\"anchor\" id=\"chapter1\"><\/a>\n\n> **Feature selection is the process of reducing the number of input variables when developing a predictive model.**","10902da6":"## Mutual Information Feature Selection <a class=\"anchor\" id=\"section_3_2\"><\/a>","041b439c":"This clearly shows that feature 3 might be the most relevant (according to chi-squared) and that perhaps four of the nine input features are the most relevant. We could set k = 4 when configuring the SelectKBest to select these top four features.","423ec306":"\n* Unsupervised Selection: Do not use the target variable (e.g. remove redundant variables).\n\n* Supervised Selection: Use the target variable (e.g. remove irrelevant variables).\n","9246bfa8":"1. Numerical Input, Numerical Output\n* Pearson\u2019s correlation coefficient (linear).\n* Spearman\u2019s rank coefficient (nonlinear).\n* Mutual Information.\n2. Numerical Input, Categorical Output\n* ANOVA correlation coefficient (linear).\n* Kendall\u2019s rank coefficient (nonlinear). \n* Mutual Information.\n3. Categorical Input, Categorical Output\n* Chi-Squared test (contingency tables).\n* Mutual Information.","9a70d737":"**Automatically Select the Number of Features**","8a1b8ed6":"*Linear machine learning algorithms fit a model where the prediction is the weighted sum of the input values. Examples include linear regression, logistic regression, and extensions that add regularization, such as ridge regression, LASSO, and the elastic net.*","5d227eee":"This clearly shows that feature 1 might be the most relevant (according to test statistic) and that perhaps six of the eight input features are the most relevant. We could set k=6 when configuring the SelectKBest to select these six four features.","92a9018b":"![Screen Shot 2021-11-05 at 22.04.32.png](attachment:7298fb43-5683-4501-b610-26c83fbd3b8f.png)","62602a61":"*Recursive Feature Elimination, or RFE for short, is a popular feature selection algorithm. RFE is popular because it is easy to configure and use, and because it is effective at selecting those features (columns) in a training dataset that are more or most relevant in predicting the target variable.*\n\n*RFE is a wrapper-type feature selection algorithm. This means that a different machine learning algorithm is given and used in the core of the method, is wrapped by RFE, and used to help select features.*","c544add3":"## Coefficients as Feature Importance<a class=\"anchor\" id=\"section_5_1\"><\/a>","bd9d405e":"## RFE Hyperparameters<a class=\"anchor\" id=\"section_4_3\"><\/a>","5858bba4":"## Feature Selection With Any Data Type ##","a47f3106":"## Chi-Squared Feature Selection <a class=\"anchor\" id=\"section_2_1\"><\/a>","7921d70c":"*ANOVA is an acronym for analysis of variance and is a parametric statistical hypothesis test for determining whether the means from two or more samples of data (often three or more) come from the same distribution or not.*\n\n*ANOVA is used when one variable is numeric and one is categorical, such as numerical input variables and a classification target variable in a classification task.*","777be385":"# 2.How to Select Categorical Input Features <a class=\"anchor\" id=\"chapter2\"><\/a>","44d6963c":"## Correlation Feature Selection <a class=\"anchor\" id=\"section_3_1\"><\/a>","ec6d511a":"*Mutual information from the field of information theory is the application of information gain (typically used in the construction of decision trees) to feature selection.* \n\n*Mutual information is calculated between two variables and measures the reduction in uncertainty for one variable given a known value of the other variable.*","dfe43599":"## Mutual Information Feature Selection <a class=\"anchor\" id=\"section_3_2\"><\/a>","169937ea":"*Pearson\u2019s chi-squared (Greek letter squared, e.g. \u03c72, pronounced kai) statistical hypothesis test is an example of a test for independence between categorical variables*","28eba03d":"* Permutation feature importance is a technique for calculating relative importance scores that is independent of the model used.\n\n* This approach can be used for regression or classification and requires that a performance metric be chosen as the basis of the importance score, such as the mean squared error for regression and accuracy for classification.\n","8486d97c":"## Statistics for Feature Selection","3ae78f2f":"* RFE requires a nested algorithm that is used to provide the feature importance scores, such as a decision tree. The class is configured with the chosen algorithm specified via the estimator argument.\n* The number of features to select via the n features to select argument."}}