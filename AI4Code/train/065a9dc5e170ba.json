{"cell_type":{"3ff714d8":"code","55dbbadd":"code","bcaa4b97":"code","57f0cb33":"code","fe6150f8":"code","1af4f817":"code","af7446c9":"code","2d2a5519":"code","ded66390":"code","9c90439e":"code","6785e96a":"code","964e5c8a":"code","3e75e5e0":"markdown","9b146d26":"markdown","2852f3d0":"markdown","0363c581":"markdown","1ea64a43":"markdown","8b2821df":"markdown","5def5931":"markdown","cd2cb0ca":"markdown","ffb5fe8c":"markdown","c94032ce":"markdown","392bedb6":"markdown","2c741d6a":"markdown","c3ab0d63":"markdown","2ad9ec39":"markdown"},"source":{"3ff714d8":"import numpy as np\nimport pandas as pd\nfrom dask_ml.preprocessing import StandardScaler\nimport gc\nimport time\nimport dask.dataframe as dask\nfrom dask.distributed import Client, progress ","55dbbadd":"# set workers\nclient = Client(n_workers=2, threads_per_worker=2, memory_limit='2GB')\nclient # work locally only ","bcaa4b97":"# setting the number of rows for the CSV file\nstart_time = time.time()\n\nN = 5_000_000\ncolumns = 257\n\n# create DF \ndf = pd.DataFrame(np.random.randint(999, 999999, size=(N, columns)), columns=['level_%s' % i for i in range(0, columns)])\n\nprint('%s seconds' % (time.time() - start_time))","57f0cb33":"display(df.head(2))","fe6150f8":"print(f'shape of generated data is {df.shape}')","1af4f817":"# # save df to csv \n\n# start_time = time.time()\n\n# df.to_csv('random.csv', sep=',')\n\n# print('%s seconds' % (time.time() - start_time)) # 877.5422155857086 seconds, 8.9 G","af7446c9":"test = '..\/input\/test.tsv'\ntrain = '..\/input\/train.tsv'","2d2a5519":"class LoadBigCsvFile:\n    \n    '''load data from tsv, transform, scale, add two columns\n    Input .csv, .tsv files\n    Output transformed file ready to save in .csv, .tsv format\n    '''\n    def __init__(self, train, test, scaler=StandardScaler(copy=False)):\n\n        self.train = train\n        self.test = test\n        self.scaler = scaler # here we use StandartScaler of Dask. We can use sklearn one\n\n    def read_data(self):\n\n        # use dask and load with smallest possible format - int16 using 'C'\n        try:\n            data_train = dask.read_csv(self.train, \\\n                                     dtype={n:'int16' for n in range(1, 300)}, engine='c').reset_index()\n            data_test = dask.read_csv(self.test, \\\n                                    dtype={n:'int16' for n in range(1, 300)}, engine='c').reset_index()\n        except: (IOError, OSError), 'can not open file'\n\n        #if any data?\n        assert len(data_test) != 0 and len(data_train) != 0, 'No data in files'\n\n        # fit train and transform test\n        self.scaler.fit(data_train.iloc[:,1:])\n        del data_train # del file that we do not need\n        test_transformed = self.scaler.transform(data_test.iloc[:,1:])\n\n        # compute  values and add columns\n        test_transformed['max_feature_2_abs_mean_diff'] = abs(test_transformed.mean(axis=1) - test_transformed.max(axis=1))\n        test_transformed['max_feature_2_index'] = test_transformed.idxmin(axis=1)\n        test_transformed['job_id'] = data_test.iloc[:,0] # add first column (it is not numerical)\n\n        del data_test # del file that we do not need\n\n        return test_transformed","ded66390":"start_time = time.time()\ndata = LoadBigCsvFile(train, test).read_data()\ngc.collect()\nprint('class loaded in %s seconds' % (time.time() - start_time))","9c90439e":"# save to hdf for later use or modification\nstart_time = time.time()\ndata.to_hdf('test_proc.hdf',  key='df1')\nprint('file saved in hdf in %s seconds' % (time.time() - start_time))","6785e96a":"start_time = time.time()\nhdf_read = dask.read_hdf('test_proc.hdf', key='df1', mode='r', chunksize=10000)\nprint('file load into system in %s seconds' % (time.time() - start_time))","964e5c8a":"display(hdf_read.head(3))","3e75e5e0":"## You can run LOCALY Dask Dashboard to track the perfomance","9b146d26":"# 3. Class to load, transform data with Dask","2852f3d0":"### Motivation\n\nData in the form of tables has become a standard in Data Science. It is easy to process and analyse it.\n\n**But what if you have really a huge data set for every day work, and you need to process tons of files?\n**\n\nImagine, you have 5 million records generated every day and you need to process them for production. You may also need to work with them throughout the day - to formulate new hypotheses or analysis. \n\nThis task was set for me to pass one of the stages of the interview - a test task. \nI need to open file in the .tsv format (analogous to .csv) and carry out the following operations:\n\n1. Scale test data using train data. Process some columns.\n2. Find the maximum index value and write the index in a new column.\n\nTo simplify I will show results of my experiments immediately \n\n","0363c581":"# 8. Solution for large cvs files:\n\n## The combination of Dask + HDF format gives the best ratio of time \/ size of archives \/ functions of calculations.\n\nIf necessary, you can convert HDF to csv \/ tsc format, including specifying the sizes of batches.\n\nI hope my experiments will help you quickly process large files and solve tasks in an interview. )\n\nThank you for your time! \n\nI would be glad if you tell me other options for solving this problem.","1ea64a43":"# 4. Let run class instance and track the time","8b2821df":"# 6. Load created file again into the system.","5def5931":"# Big dataset in .csv or .tsv processing with [Dask](http:\/\/https:\/\/docs.dask.org\/en\/latest\/) library","cd2cb0ca":"# 1. Huge csv file generation.\n## Skip this stage and use files in the Kaggle input folder unless you want to repeate my experiment\n\nHere we are preparing a test file (random.csv) of size 9G. This is 5 million records of random numbers (from 100 to 1 million) with 257 columns. Finally we get a table of 5 million for 257 random integers = 1 billion 285 million values.\n\n**Creating a DataFrame took 5 seconds **, uploading it to .csv format using Pandas (to_csv) - 14 minutes!\n**\n\nAn attempt to open this file using the system or Pandas failed. ( So use files in Kaggle input directory to run the code.","ffb5fe8c":"## Results (emulation of a 2G RAM server, on my comp with 30G RAM, i7):\n\n### 1. Processing files takes 53-56 seconds. It is ok\n\n### 2. Saving the processed file in .tcv\n\n* a. Dask file to Pandas -> case memory error !!!\n* b. Dask file with Dask_to_csv took 836 seconds and created 139 file archives 25 G in size!\n* c. Dask file to Json took 6850 seconds and created 139 files with a total size of 8.2 G!\n* d. Dask fle to Parket took 190 seconds and creared 141 files with a total size of 13 G!\n* e. Dask file to HDF took 152 seconds and created 1 file 10.4 G in size\n\n### 3. Downloading a file of options b and d will take about the same amount of time and server RAM.\n\n* -> d. Parket option will require additional time to download and convert to a format convenient for Data Science tasks.\n\n> *** -> e. HDF format allowed to open 5G file in 0.035 seconds! and allow you to perform the necessary calculations directly**","c94032ce":"## Set our small size files path","392bedb6":"### The process of converting a data to tcv format takes the most time and memory, than transforming data itself!\n\n\n### With small files this is not critical, but for files larger than 1 gigabyte this becomes a problem.","2c741d6a":"# 7. As we see with a small file (1 megabyte), everything came quickly.\n\n## So here I used a file for the same class but for 5 GigaByte file (code to generate is above) and results are next:","c3ab0d63":"# 5. Data is ready to save. But what format to choose?\n\n## Here we use hdf format - why? please the final test results","2ad9ec39":"# 2. Uncomment and run you want to wait more that 15 minutes.\n# Strongly recommend to Use files in Kaggle input directory!"}}