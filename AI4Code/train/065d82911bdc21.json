{"cell_type":{"e25b0253":"code","3407ab80":"code","5738ebd6":"code","71c3d644":"code","57344338":"code","d37dbd57":"code","3c60c3f8":"code","5b658d14":"code","b01e6d42":"code","da78c8bd":"code","b4fc2afc":"code","97def200":"code","9f7a680a":"code","84bfc970":"code","a464a8cf":"code","e58903ae":"code","4febd7fd":"code","a45ed167":"code","c0330b5b":"code","11fcde0d":"code","85e1cf08":"code","0d79ec99":"code","8425dce8":"code","5c07c7dd":"code","63f41037":"code","169f7068":"code","7117c4da":"code","84b54012":"code","a2c9b8d5":"code","9fd061ae":"code","639b1e85":"code","27ac6e8d":"code","a511e115":"code","ffbe354f":"code","5b0d5240":"code","f2ab8acf":"code","1a47af0f":"code","fbb3b6b6":"code","efd66b24":"code","c2560f88":"code","d73b6896":"markdown","27e74647":"markdown","38c63f76":"markdown","71de68b6":"markdown","bd2d6e08":"markdown","c35bbfd1":"markdown","f31210f0":"markdown"},"source":{"e25b0253":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n","3407ab80":"# load data set\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_train.head()","5738ebd6":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test.head()","71c3d644":"# remove the \"Survived\" column in \"df_train\" and join bind two sets\ndf = pd.concat([df_train.drop(\"Survived\", axis=1), df_test])\ndf.head()","57344338":"# remove all string columns: \"Name\", \"Ticket\", \"Cabin\"\ndf = df.drop([\"Name\", \"Ticket\", \"Cabin\"], axis = 1)\ndf.head()","d37dbd57":"# check missing value\ndf.isna().sum()","3c60c3f8":"# deal with NA in Age\ndf[df.Age.isna()]\n# there are many rows contain NA in Age columns and I use the median Age to replace the NA\ndf.Age = df.Age.fillna(df.Age.median())\n\n# deal with NA in Fare\ndf[df.Fare.isna()]\n# it is a 3th class passenger and there is strongly relationship between Fare and Class,so I use the mean(Fare) in 3th class to replace the missing value\ndf.Fare = df.Fare.fillna(df.Fare[df.Pclass ==3].median())\n\n# deal with NA in Embarked\ndf[df.Embarked.isna()]\n# the Embarked is independent, so I use mode to replace the NA\ndf.Embarked = df.Embarked.fillna(df.Embarked.mode()[0])","5b658d14":"# check missing value again\ndf.isna().sum()","b01e6d42":"# update the df_train\ndf_train.Age = df.Age[0:len(df_train)]\ndf_train.Fare = df.Fare[0:len(df_train)]\ndf_train.Embarked = df.Embarked[0:len(df_train)]","da78c8bd":"# Pclass\nind = list(set(df.Pclass))\ncount = df.groupby(\"Pclass\").count().Sex\ncount0 = df_train[df_train.Survived == 0].groupby(\"Pclass\").count().Sex\ncount1 = df_train[df_train.Survived == 1].groupby(\"Pclass\").count().Sex\n\n# distrubition\nplt.bar(ind, count)\nplt.ylim(0,750)\nplt.show()\n\n# relationship\np0 = plt.bar(ind, count0)\np1 = plt.bar(ind, count1, bottom = count0)\nplt.ylim(0,750)\nplt.legend((p0[0], p1[0]), ('Not Survived','Survived'))\nplt.show()","b4fc2afc":"# Sex\nind = sorted(list(set(df.Sex)))\ncount = df.groupby(\"Sex\").count().Pclass\ncount0 = df_train[df_train.Survived == 0].groupby(\"Sex\").count().Survived\ncount1 = df_train[df_train.Survived == 1].groupby(\"Sex\").count().Survived\n\n# distrubition\nplt.bar(ind, count)\nplt.ylim(0,900)\nplt.show()\n\n# relationship\np0 = plt.bar(ind, count0)\np1 = plt.bar(ind, count1, bottom = count0)\nplt.ylim(0,900)\nplt.legend((p0[0], p1[0]), ('Not Survived','Survived'))\nplt.show()","97def200":"# Age\n#distribution\nplt.boxplot(df.Age)\nplt.show()\n\n#relationship\np0 = plt.hist(df_train.Age[df_train.Survived == 0], width = 5, alpha = 0.5)\np1 = plt.hist(df_train.Age[df_train.Survived == 1], width = 5, alpha = 0.5)\nplt.legend(('Not Survived','Survived'))\nplt.show()","9f7a680a":"# SibSp\nind = list(set(df.SibSp))\ncount = df.groupby(\"SibSp\").count().Pclass\n\n# distrubition\nplt.bar(ind, count)\nplt.ylim(0,900)\nplt.show()\n\n# relationship\nsib = df_train.groupby(['SibSp','Survived']).agg(total=('Pclass', 'size'))\nsib[\"Percentage\"] = sib \/ sib.groupby(level=0).sum() *100\nsib","84bfc970":"# Parch\nind = list(set(df.Parch))\ncount = df.groupby(\"Parch\").count().Pclass\n\n# distrubition\nplt.bar(ind, count)\nplt.ylim(0,900)\nplt.show()\n\n# relationship\npar = df_train.groupby(['Parch','Survived']).agg(total=('Pclass', 'size'))\npar[\"Percentage\"] = par \/ par.groupby(level=0).sum() *100\npar","a464a8cf":"# Fare\n#distribution\nplt.boxplot(df.Fare)\nplt.show()\n\n#relationship\np1 = plt.hist(df_train.Fare[df_train.Survived == 0], bins = 20, alpha = 0.5)\np2 = plt.hist(df_train.Fare[df_train.Survived == 1], bins = 20, alpha = 0.5)\nplt.legend(('Not Survived','Survived'))\nplt.show()","e58903ae":"# Embarked\nind = sorted(list(set(df.Embarked)))\ncount = df.groupby(\"Embarked\").count().Pclass\ncount0 = df_train[df_train.Survived == 0].groupby(\"Embarked\").count().Survived\ncount1 = df_train[df_train.Survived == 1].groupby(\"Embarked\").count().Survived\n\n# distrubition\nplt.bar(ind, count)\nplt.ylim(0,900)\nplt.show()\n\n# relationship\np0 = plt.bar(ind, count0)\np1 = plt.bar(ind, count1, bottom = count0)\nplt.ylim(0,900)\nplt.legend((p0[0], p1[0]), ('Not Survived','Survived'))\nplt.show()","4febd7fd":"#Encode Features\n#Sex 0: female, 1: male\ndf.Sex = df.Sex.replace(\"female\", 0)\ndf.Sex = df.Sex.replace(\"male\", 1)\n\n#Embarked 0:C, 1:Q, 2:S\ndf.Embarked = df.Embarked.replace(\"C\",0)\ndf.Embarked = df.Embarked.replace(\"Q\",1)\ndf.Embarked = df.Embarked.replace(\"S\",2)","a45ed167":"#genearate dummy variables for Pclass, SibSp, Parch, and Embarked\ndf_dummy = pd.get_dummies(df, columns = ['Pclass', 'SibSp', 'Parch', 'Embarked'])","c0330b5b":"# create train and test sets\ntrain_x = df_dummy[0:len(df_train)]\ntrain_y = df_train.Survived\ntrain_x","11fcde0d":"# check the feature inportance by randomforest\nmodel_rf = RandomForestRegressor()\nmodel_rf.fit(train_x, train_y)\n\nind =  model_rf.feature_importances_.argsort()\nplt.barh(train_x.columns[ind], model_rf.feature_importances_[ind])","85e1cf08":"# some Prach and SibSp dummy variables are low importance and only exist in the train set or test set. So, I plan to encode them and reduce the number of feature\n# in SibSp, 0: 0 ,1: 1, 2: 2+\n# in Parch, 0: 0, 1: 1, 2: 2+\ndf.loc[df.SibSp >= 2, 'SibSp'] = 2\ndf.loc[df.Parch >= 2, 'Parch'] = 2\n\ndf.head()","0d79ec99":"#repeat. geneate dummy variables and create train and test sets\ndf_dummy = pd.get_dummies(df, columns = ['Pclass', 'SibSp', 'Parch', 'Embarked'])\n\n# create train and test sets\ntrain_x = df_dummy[0:len(df_train)]\ntrain_y = df_train.Survived\ntest_x = df_dummy[len(df_train):]","8425dce8":"# model list used in this case\nmodels = [\"Random Forest\", \"Xgboost\", \"Extra Randomized Trees\"]\nscores = []","5c07c7dd":"# Grid Search\n# Random Forset\ngrid_rf = {\n    \"n_estimators\": np.linspace(100,1000,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [2,3,4,5,6,7,8,9],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020,2021]\n}\n\nmodel_rf = RandomForestClassifier()\nsearch_rf = GridSearchCV(estimator = model_rf, \n                         param_grid = grid_rf, \n                         cv = 5,\n                         n_jobs = -1,\n                         verbose = 2)","63f41037":"# do not run this code in your local PC, it is time consuming\nsearch_rf.fit(train_x, train_y)","169f7068":"# find the best parameters\nbest_rf = search_rf.best_params_\nbest_rf","7117c4da":"# it seems RandomForestRegressor cannot accept the dict type parameters, so I have to rewrite the best parameters\nfinal_rf =  RandomForestClassifier(**best_rf)\nfinal_rf.fit(train_x, train_y)\npred_rf = final_rf.predict(test_x)\npred_rf","84b54012":"# Xgboost\ngrid_xgb = {\n    'booster': ['gbtree', 'gblinear'],\n    'objective': ['binary:logistic'],\n    'subsample': [0.6,0.7,0.8,0.9],\n    'colsample_bytree': [0.6,0.7,0.8,0.9],\n    'eta': [0.05,0.1,0.2,0.3],\n    'max_depth': [3,5,7],\n    'seed': [2021,2022],\n    'eval_metric': ['logloss']\n}\n\n\nmodel_xgb = xgb.XGBClassifier()\nsearch_xgb = GridSearchCV(estimator = model_xgb, \n                         param_grid = grid_xgb, \n                         cv = 5,\n                         n_jobs = -1,\n                         verbose = 2)","a2c9b8d5":"# do not run this code in your local PC, it is time consuming\nsearch_xgb.fit(train_x, train_y)","9fd061ae":"# find the best parameters\nbest_xgb = search_xgb.best_params_\nbest_xgb","639b1e85":"# final xgboost model\nfinal_xgb = xgb.XGBClassifier(**best_xgb)\nfinal_xgb.fit(train_x, train_y)\npred_xgb = final_xgb.predict(test_x)\npred_xgb","27ac6e8d":"# Extra Randomized Trees. this part is very simular with random forest, so I copy my code to here\ngrid_ert = {\n    \"n_estimators\": np.linspace(100,1000,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [2,3,4,5,6,7,8,9],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020,2021]\n}\n\nmodel_ert = ExtraTreesClassifier()\nsearch_ert = GridSearchCV(estimator = model_ert, \n                         param_grid = grid_ert, \n                         cv = 5,\n                         n_jobs = -1,\n                         verbose = 2)","a511e115":"# do not run this code in your local PC, it is time consuming\nsearch_ert.fit(train_x, train_y)","ffbe354f":"# find the best parameters\nbest_ert = search_ert.best_params_\nbest_ert","5b0d5240":"# final Extra Randomized Trees model\nfinal_ert =  ExtraTreesClassifier(**best_ert)\nfinal_ert.fit(train_x, train_y)\npred_ert = final_ert.predict(test_x)\npred_ert","f2ab8acf":"# a majority vote ensemble\ndef ensumble_majority(trainx, trainy, testx):\n    final_rf =  RandomForestClassifier(**best_rf)\n    final_rf.fit(trainx, trainy)\n    pred_rf = final_rf.predict(testx)\n    \n    final_ert =  ExtraTreesClassifier(**best_ert)\n    final_ert.fit(trainx, trainy)\n    pred_ert = final_rf.predict(testx)\n    \n    final_xgb = xgb.XGBClassifier(**best_xgb)\n    final_xgb.fit(trainx, trainy)\n    pred_xgb = final_xgb.predict(testx)\n    \n    collection = pd.DataFrame({'pred_rf':pred_rf,'pred_ert':pred_ert,'pred_xgb':pred_xgb})\n    collection['sum'] = collection.sum(axis = 1)\n    collection.loc[collection['sum'] > 1.5, 'pred'] = 1\n    collection.loc[collection['sum'] < 1.5, 'pred'] = 0\n    \n    return(collection.pred)\n    ","1a47af0f":"pred = ensumble_majority(train_x, train_y, test_x)\n\nres = pd.DataFrame({'PassengerId':test_x.PassengerId, 'Survived': pred})\nres.to_csv('result.csv', index=False)","fbb3b6b6":"# a simple ensemble can use the majority vote rule, but here I try to build a simple stacking ensemble.\n# I use random forest and Extra Randomized Trees as basic models, and choose xgboost as the second layer models\n\ndef ensemble_stacking(trainx, trainy, testx):\n    '''\n    this function is to ensemble three models' predictions: random forest, extra randomized trees, and xgboost\n    '''\n    # first, split the train set into two equal parts.\n    px1,px2, py1, py2 = train_test_split(trainx, trainy, random_state= 2021, test_size=0.5)\n    \n    # second, train basic models by the two sets.\n    # random forest\n    final_rf =  RandomForestClassifier(**best_rf)\n    final_rf.fit(px1,py1)\n    pred_rf = final_rf.predict(px2)\n    \n    \n    # extra randomize trees\n    final_ert =  ExtraTreesClassifier(**best_ert)\n    final_ert.fit(px2,py2)\n    pred_ert = final_ert.predict(px1)\n    \n    # third, combind the two basic models predictions and as a new feature add into the original train set.\n    # it means to create a new train setand the new set = original train set + tow basic models' predictions\n    \n    new_x = pd.concat([px1, px2])\n    new_y = pd.concat([py1, py2])\n    feature = np.concatenate([pred_ert,pred_rf])\n    new_x['feature'] = feature\n    \n    # finally, train xgboost model by the new set and make the final prediction.\n    \n    final_xgb = xgb.XGBClassifier(**best_xgb)\n    final_xgb.fit(train_x, train_y)\n    pred_xgb = final_xgb.predict(test_x)\n    pred_xgb\n    \n    return(pred_xgb)","efd66b24":"# call ensemble function\npred = ensemble_stacking(train_x, train_y, test_x)","c2560f88":"res = pd.DataFrame({'PassengerId':test_x.PassengerId, 'Survived': pred})\nres.to_csv('result_stacking.csv', index=False)","d73b6896":"Hi, guys.\n\nThis is my first join the Kaggle competition and write code in python (I used R before, and my best score in this competition also is made by R). So, I hope you all can help me to correct my error and misunderstanding of any concept and process (especially the stacking ensemble part).\n\nIn this note book, I try to roughly implement the whole workflow. here is the composition of my code:\n\n[1]. import and load data\n\n[2]. Data Cleansing \n\n[3]. EDA\n\n[4]. Feature Engineering\n\n[5]. Modeling selection \n\n[6]. Ensemble Generation\n\n\nHere is a wired question I find: in the grid search part, I use 'random_state' or 'seed' to control the Randomness, but I find the best parameter is lightly different each time.","27e74647":"# Model Selection","38c63f76":"# Feature Engineering","71de68b6":"Thank you for viewing my notebook!\nI want to share some of my experiences here.\n\n[1] more try, more findings. I check every model I used in this competition. I find some models can get better scores and parts of models' predictions are better than the ensemble result. (In R, I try more than 10 models, but in Python, I just choose 3 models.)\n\n[2] string columns are very important. In my code, I delete all string columns because I don't know how to deal with them. But I notice that many high score notebooks extract information from the string columns and generate features finally. So, I guess that is why I cannot reach the top 10% :(\n\n[3] Generating FEATURES! I think the most important step in the whole process is to determine features. The quality of features determines your score's mean value, and the rest parts, like grid search and ensemble, are the sd of your score. Therefore, if you want to get a higher score, pay more attention to your features.","bd2d6e08":"# EDA\nIn the EDA part, I have two tasks:\n\nFirst, check all variables' distribution.\n\nSecond, check the relationship between survived and other variables in df_train","c35bbfd1":"# Ensemble Generation","f31210f0":"# Data Cleansing"}}