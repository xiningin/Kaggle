{"cell_type":{"605f1368":"code","7f45e547":"code","c6646056":"code","47e8da41":"code","7719100f":"code","334ca8c2":"code","e0afad7b":"code","17c831c6":"code","1c9565d7":"code","54e65ff3":"markdown","d337262d":"markdown","d63e9a78":"markdown","702de9c6":"markdown","6a4d6818":"markdown","b4254853":"markdown","92102f60":"markdown"},"source":{"605f1368":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sb\nimport os\nimport datetime as dt\n%matplotlib inline\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n# Any results you write to the current directory are saved as output.\n\n","7f45e547":"main_dataset = pd.read_csv(r'\/kaggle\/input\/us-accidents\/US_Accidents_Dec19.csv')\nmain_dataset.shape","c6646056":"main_dataset.dtypes","47e8da41":"main_dataset.describe()","7719100f":"corr = main_dataset.corr(method='pearson') \nfig, ax = plt.subplots(figsize=(9,9)) \nsb.heatmap(corr, ax = ax,cmap='coolwarm',  robust=True)\nax.set_title('Correlation')\nplt.show()","334ca8c2":"mask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nfig, ax = plt.subplots(figsize=(9,9)) \ncmap = sb.diverging_palette(240, 10, as_cmap=True)\nsb.heatmap(corr, mask=mask, linewidths=.5, cmap=cmap, center=0, ax=ax)\nax.set_title('Correlation')\nplt.show()","e0afad7b":"main_dataset['date'] = main_dataset['Start_Time'].str.split(n=1).str[0]\nmain_dataset['Date'] = pd.to_datetime(main_dataset['date'], errors='coerce')\nmain_dataset['Week'] = main_dataset['Date'] .dt.week\nmain_dataset['Year'] = main_dataset['Date'] .dt.year","17c831c6":"plt.style.use('ggplot')\nfig, ax = plt.subplots(figsize=(15,7))\nmain_dataset.groupby(['Year','Week','Severity']).count()['ID'].unstack().plot(ax=ax)\nax.set_xlabel('Week')\nax.set_ylabel('Number of Accidents')","1c9565d7":"main_dataset['timestamp'] = pd.to_datetime(main_dataset['Weather_Timestamp'], errors='coerce')\nmain_dataset['Hour'] = main_dataset['timestamp'] .dt.hour\nmain_dataset['Minute'] = main_dataset['timestamp'] .dt.minute\nhours = [hour for hour, df in main_dataset.groupby('Hour')]\nplt.plot(hours, main_dataset.groupby(['Hour'])['ID'].count())\nplt.xticks(hours)\nplt.xlabel('Hour')\nplt.ylabel('Numer of accidents')\nplt.grid(True)\nplt.show()","54e65ff3":"# Searching correlation between columns","d337262d":"# Describe data","d63e9a78":"# Add new columns","702de9c6":"# Visualize US Accidents Dataset group by Year, Week and Severity","6a4d6818":"# Checking the shape of dataset ","b4254853":"# Checking datatpyes and columns","92102f60":"# At what time do accidents happen?"}}