{"cell_type":{"ccf9a228":"code","136d5670":"code","be24af3f":"code","ac472fb0":"code","8407e5e3":"code","a5b3fa19":"code","e99dfec9":"code","6aa200d2":"code","27f44b1d":"code","03f74761":"code","65208f05":"code","dd37eb69":"code","6f7144b9":"code","2e974637":"code","d1a2d53d":"code","48e4a57a":"code","ac24e02a":"code","3b53295c":"code","28266489":"code","48b6fa8e":"code","0a722d0e":"code","45a271d1":"code","a124cd65":"code","e2045f0b":"code","39794f17":"code","953acde4":"code","12f7d8c6":"code","bebe941b":"code","da0bdf8c":"code","c821585e":"code","04f432a0":"code","f3c335cc":"code","1569c695":"code","2df0a86e":"code","594a9f19":"code","51585685":"code","47522ce0":"code","d56197ae":"code","699e78be":"code","dc7e56cd":"code","f43fe6b1":"code","7b0a50d4":"code","a74b03bf":"code","d09ef25b":"code","cd8d4134":"code","2a88586c":"code","5b4f7060":"code","042af9c0":"markdown","a8fc269e":"markdown","a76a97ff":"markdown","eb98b5fd":"markdown","c488cd11":"markdown","f2e8b878":"markdown","cf3d71f8":"markdown","1539afa7":"markdown","d0b38188":"markdown","04768aee":"markdown","25aa482e":"markdown","9ecb34d5":"markdown","237fee09":"markdown","4b6f06f6":"markdown","1c28a54d":"markdown","6048adab":"markdown","f26510d4":"markdown","dd2d10f3":"markdown","033bbd00":"markdown","8cddda27":"markdown","652976b7":"markdown","7fcc5d60":"markdown","e15a75b9":"markdown","4fe2a803":"markdown","6fc1557d":"markdown","00782eda":"markdown","67ca7888":"markdown","95636cb8":"markdown","512d3176":"markdown","8b261272":"markdown","24f2dfaa":"markdown","412b79ff":"markdown","8b10ba11":"markdown","9b8269ff":"markdown","0a9fe0f6":"markdown","cb427d59":"markdown","4901098f":"markdown"},"source":{"ccf9a228":"import numpy as np\nimport pandas as pd \n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings('ignore')","136d5670":"dataset = pd.read_csv(\"..\/input\/groceries\/groceries.csv\", sep=\";\", header= None)\n\n# when we set header=None it will consider as csv file which has no header.","be24af3f":"dataset.head()","ac472fb0":"unique_items_list = []\n\n# for each index it will iter row by row\nfor index, row in dataset.iterrows():  \n    \n    # splitting items with , and creating a new list for row & it will going add it agian \n    # ...item_series list for each iteration..so item_series will be list of lists..\n    items_series = list(row.str.split(','))\n    \n    \n    # agian reading each list elements from item_Series which is big list as mentioned above code\n    for each_row_list in items_series:\n        \n        # iterating each item from each_row_lists\n        for item in each_row_list:\n            \n            # for first iteration..unique_items_list is empty so first item directly append to it.\n            #...from next onwards..it will start to check condition 'not in'\n            #....& if item not found in unique_items_list list then it will append to it.\n            #......finally we will get one unique item list..\n            if item not in unique_items_list:\n                unique_items_list.append(item)","8407e5e3":"df_apriori = pd.DataFrame(columns=unique_items_list)","a5b3fa19":"df_apriori","e99dfec9":"dataset1 =df_apriori.copy()","6aa200d2":"## If for the item names obesrved w.r.t. each list will be assigned as number 1 & those items are not in \n##...row number iterating will be assigned with nuber 0.\n\nfor index, row in dataset.iterrows():\n    items = str(row[0]).split(',')\n    one_hot_encoding = np.zeros(len(unique_items_list),dtype=int)\n    for item_name in items:\n        for i,column in enumerate(dataset1.columns):\n            if item_name == column:\n                one_hot_encoding[i] = 1\n    dataset1.at[index] = one_hot_encoding\n\n# Transction encoder is fastest method to do all this.","27f44b1d":"dataset1.head()","03f74761":"# shape of the dataset1\n\ndataset1.shape","65208f05":"dataset1.info()","dd37eb69":"# Sinced efault datatype saved as 'object'. Converting in 'integer' datatype\n\ndataset1 = dataset1.astype('uint8')\ndataset1.info()","6f7144b9":"dataset1.head()","2e974637":"zero =[]\none = []\nfor i in df_apriori.columns:\n    zero.append(list(dataset1[i].value_counts())[0])\n    one.append(list(dataset1[i].value_counts())[1])","d1a2d53d":"count_df = pd.DataFrame([zero,one], columns=df_apriori.copy().columns)","48e4a57a":"count_df.head()","ac24e02a":"# Changing row names...\n\ncount_df.index = ['Not_Purchased', 'Purchased']\ncount_df","3b53295c":"# CHECKING WHICH PRODUCTE\n\nprint('maximum purchased item:',count_df.idxmax(axis = 1)[1],':',count_df.loc['Purchased'].max())\nprint('minimum purchased item:',count_df.idxmax(axis = 1)[0],':',count_df.loc['Not_Purchased'].max())","28266489":"# Simplest way to sort elements..\n\nsorted_df = pd.DataFrame(count_df.sort_values(by=['Purchased'],axis=1,ascending=False).transpose())\nsorted_df.head(20)","48b6fa8e":"# adding Purchased% table into the dataset1.\n\nsorted_df['Purchased%']= sorted_df.Purchased\/sum(sorted_df.Purchased)\nsorted_df.head()","0a722d0e":"# Finding out avergae of the total purchased% so that we get idea about min_support value setting.\n\nnp.mean(sorted_df['Purchased%'])","45a271d1":"# Plotting sorted top purchased products..\n\nfig = plt.subplots(figsize=(20,10))\npurchased = sorted_df.head(50).xs('Purchased' ,axis = 1)\npurchased.plot(kind='bar',fontsize=16)\nplt.title('Purchased top Count',fontsize=30)\nplt.xlabel('Products', fontsize=20)\nplt.ylabel('total qty. purchased', fontsize=20)\nplt.show()","a124cd65":"from mlxtend.frequent_patterns import apriori, association_rules\n\nfreq_items = apriori(dataset1, min_support=0.02, use_colnames=True, max_len=5)\n\n# min_support value can be choose by the user\/business need\n# max_len is item combinations..here i have taken as 5. total items in combination formed should not be more than 5","e2045f0b":"freq_items.shape","39794f17":"# checking first 10 rows\n\nfreq_items.head(10)","953acde4":"# checking last 10 rows \n\nfreq_items.tail(10)","12f7d8c6":"# for this we need support value dataframe..that is fre_items from measure1.\n\nconfidence_association = association_rules(freq_items, metric='confidence', min_threshold=0.2)\n\n# min_threshold is nothing but setting min % crieteria. In this case i have choosen 20% \n#...confidence should be minimum 20%.","bebe941b":"# checking combination in first 10 rows from dataset\n\nconfidence_association.head(10)","da0bdf8c":"0.028978*100","c821585e":"# checking combination in last 10 rows from dataset\n\nconfidence_association.tail(10)","04f432a0":"lift_association = association_rules(freq_items, metric=\"lift\", min_threshold=1)","f3c335cc":"lift_association.shape","1569c695":"lift_association.head(10)","2df0a86e":"lift_association.tail(5)","594a9f19":"# As per above output observation, it is clear that when same items repeated..\n#...(for ex: in first row: A-->B, and in next row B-->A) gives same leverage & lift but confidence is different.\n#...this is known as redudency when same item set shuffled as ancedents & consequent.\n#.... so to eliminates in easist way..will sort n the basis of leverage & confidence.\n\nredundancy = lift_association.sort_values(by=['leverage','confidence'],axis=0, ascending=False).reset_index()\nredundancy = redundancy.drop(['index'], axis=1)\nredundancy.shape\nredundancy.head()","51585685":"redundancy.tail()","47522ce0":"# Now check output of above cells, when leverage and lift are same for consequent rows..then compare with the value of confidence\n#...if confidence of middle cell found less than two side cells..drop it.\n# dropping odd index rows..since it contains less confidence\n# ultimately this will help us to elminate repeated combination..which has low lift & confidence..\n\nunique_rules = redundancy.iloc[::2]\nunique_rules.shape","d56197ae":"unique_rules.head(10)","699e78be":"top_20 = unique_rules[unique_rules['lift']>1.5]","dc7e56cd":"top_20_sort = top_20.sort_values(by='lift', ascending=False)\ntop_20_sort.reset_index(inplace=True)","f43fe6b1":"top_20_sort = top_20_sort.drop(['index'],axis=1)","7b0a50d4":"top_20_sort.head()","a74b03bf":"x = top_20_sort[['antecedents','consequents']]","d09ef25b":"item_list = []\nfor i in x.antecedents.to_list():\n    for j in list(set(i)):\n        item_list .append(j)","cd8d4134":"for p in x.consequents.to_list():\n    for q in list(set(p)):\n        item_list.append(q)","2a88586c":"def unique(list1): \n    # insert the list to the set \n    list_set = set(list1) \n    # convert the set to the list \n    unique_list = (list(list_set))\n    top_items =[]\n    for m in unique_list:\n        top_items.append(m)\n    print(top_items)","5b4f7060":"unique(item_list)","042af9c0":"# Step 2: Dataset Loading & Preprocessing","a8fc269e":" ### Finding out support for each possible products or diff. product sets present in transction dataframe(dataset1)","a76a97ff":"# Step 4: Apriori Rule\n***\n\nref. used: https:\/\/www.kdnuggets.com\/2016\/04\/association-rules-apriori-algorithm-tutorial.html","eb98b5fd":" ### Checking for Purchased and not purchased item qty. details to get insights","c488cd11":" This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears. In Table 1 below, the support of {apple} is 4 out of 8, or 50%. Itemsets can also contain multiple items. For instance, the support of {apple, beer, rice} is 2 out of 8, or 25%.\n","f2e8b878":" ### Unique item names from to 20 lift combinations..","cf3d71f8":" ### Top 20 combinations w.r.t. Lift more than 1.5","1539afa7":"<p algin='justify'> This says how popular an itemset is, as measured by the proportion of transactions in which an itemset appears.<\/p> \n\nFor example:\n\nIn Table sorted_df, the support of {whole milk} is 2513 out of total 9835 row tranctions. i.e. 25.55%. \n\n\nItemsets can also contain multiple items. For instance, the support of {bottled water, soda} is 285 out of 9835, or 2.89%","d0b38188":" ### Understanding terminologies:\n ***\n \n [Basic Terminology](https:\/\/michael.hahsler.net\/research\/recommender\/associationrules.html#:~:text=Leverage%20measures%20the%20difference%20of,expected%20from%20the%20independent%20sells)","04768aee":" ### Sorting items from main dataset agian & assigning in respective column. ","25aa482e":" This says how likely item Y is purchased when item X is purchased, expressed as {X -> Y}. This is measured by the proportion of transactions with item X, in which item Y also appears. In Table 1, the confidence of {apple -> beer} is 3 out of 4, or 75%.\n\n\n\n<img src=\"https:\/\/github.com\/ShrikantUppin\/Association_Rules\/blob\/main\/measure2.png?raw=true\" >\n\n***\n* Drawbacks of Confidence measure:\n***\n\n* it might misrepresent the importance of an association. \n\n* This is because it only accounts for how popular apples are, but not beers. If beers are also very popular in general, there will be a higher chance that a transaction containing apples will also contain beers, thus inflating the confidence measure. \n\n\nNote: To account for the base popularity of both constituent items, we use a third measure called lift.","9ecb34d5":"Grocery shop contains total 169 numbers of items.","237fee09":"# Step 5: Generated Rules analysis\/Processing","4b6f06f6":"'whole milk', 'other vegetables', 'rolls\/buns', 'soda', 'yogurt','bottled water', 'root vegetables', 'tropical fruit'","1c28a54d":" ### Finding out all unique items available at grocery.","6048adab":" ### Now, Generating empty Dataframe with unique_items_list elements as column names.","f26510d4":" ### Building Association rules using confidence metrics.","dd2d10f3":"![](https:\/\/github.com\/ShrikantUppin\/Association_Rules\/blob\/main\/measure1_formula.png?raw=true)\n\n<img src=\"https:\/\/github.com\/ShrikantUppin\/Association_Rules\/blob\/main\/measure1.png?raw=true\" width=\"300\" height=\"300\">\n","033bbd00":" #### Concept:","8cddda27":" ### Summary:\n \n \n * freq_items = apriori(dataset1, min_support=0.02, use_colnames=True, max_len=5)\n \n \n\n * confidence_association = association_rules(freq_items, metric='confidence', min_threshold=0.2)\n \n \n * lift_association = association_rules(freq_items, metric=\"lift\", min_threshold=1)\n ","652976b7":"for above output:\n\n* Each row is one transction.\n\n* Products in each row is nothing but items purchased by buyer\/customer.","7fcc5d60":"# Step 1: Importing Required Libraries..","e15a75b9":"## 4.1 Measure 1: Support \n***","4fe2a803":"***","6fc1557d":" #### 1 . Antecedent and Consequent\n \nThe IF component of an association rule is known as the antecedent. The THEN component is known as the consequent. The antecedent and the consequent are disjoint; they have no items in common.\n\n\n #### 2. antecedent support\n \n It is antecedent support with all transction numbers.\n \n \n #### 3. consequent support\n\n It is consequent  support with all transction numbers.\n \n \n #### 4. Support:\n \n Here support is considered for antecedent+consequent combination.\n \n \n #### 5. confidence\n \n Confidence is related to 'consequent item' or 'consequent item combination' w.r.t. antecedent item  or item set.\n \n \n #### 6. lift\n \nLift measures how many times more often X and Y occur together than expected if they where statistically independent. Lift is not down-ward closed and does not suffer from the rare item problem.\n \n In short firm possibilities of buying consequent whenever Antecedent item is purchaed by customer\n \n \n #### 7. Leverage\n \n Leverage measures the difference of X and Y appearing together in the data set and what would be expected if X and Y where statistically dependent. The rational in a sales setting is to find out how many more units (items X and Y together) are sold than expected from the independent sells.\n \n leverage also can suffer from the rare item problem.\n \n leverage(X -> Y) = P(X and Y) - (P(X)P(Y))\n \n \n #### 8. conviction\n \n conviction(X -> Y) = P(X)P(not Y)\/P(X and not Y)=(1-sup(Y))\/(1-conf(X -> Y))\n\nConviction compares the probability that X appears without Y if they were dependent with the actual frequency of the appearance of X without Y. In that respect it is similar to lift (see section about lift on this page), however, it contrast to lift it is a directed measure. Furthermore, conviction is monotone in confidence and lift.\n\n\n#### 9. Coverage\n\ncoverage(X) = P(X) = sup(X)\n\nA simple measure of how often a item set appears in the data set.","00782eda":"***\n***","67ca7888":"above are the top 20 products items & the shuffled combination gives top lift result.","95636cb8":"We have obtained unique_rules with metric='lift'. Now, this unique_rules dataframe will be used for analysis..just filtering as per threshold value set\/required & obtaining diff. pairs of item sets.\n\n\nNote: lift is set to 1. in previous code. Since if lift is equal to or greater than 1..that means chances to pick consequents items by customer is more..!!","512d3176":" ### People purchased more is daily need items & transction for all them is above 1000 nos.","8b261272":"If you discover that sales of items beyond a certain proportion tend to have a significant impact on your profits, you might consider using that proportion as your support threshold. You may then identify itemsets with support values above this threshold as significant itemsets.\n\n***\n***","24f2dfaa":" #### Concept:","412b79ff":" ### Building Association rules using confidence metrics.","8b10ba11":"## 4.2 Measure 2: Confidence\n***\n***","9b8269ff":" #### Concept:\n    \nThis says how likely item Y is purchased when item X is purchased, while controlling for how popular item Y is. In Table 1, the lift of {apple -> beer} is 1,which implies no association between items. A lift value greater than 1 means that item Y is likely to be bought if item X is bought, while a value less than 1 means that item Y is unlikely to be bought if item X is bought.\n\n\n<img src=\"https:\/\/github.com\/ShrikantUppin\/Association_Rules\/blob\/main\/measure3.png?raw=true\" >","0a9fe0f6":"## 4.4 Eliminating redudancy sets...\n ","cb427d59":"# Step 3: EDA","4901098f":"## 4.3 Measure 3: Lift \n***"}}