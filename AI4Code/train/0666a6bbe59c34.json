{"cell_type":{"00160547":"code","0a3a5cf5":"code","28b6f248":"code","dd0b6d38":"code","f12d2e32":"code","dffcc667":"code","2bcf35c4":"code","69536003":"code","5c179f50":"code","493dc51c":"code","eb7b9ec5":"code","3be347cb":"code","6ca4a5d9":"code","7a83453f":"code","9a82fce0":"code","5f309d39":"code","f9c35c2d":"code","68a2d866":"code","4ac90290":"code","14619a24":"code","478d002d":"code","d27790b8":"code","a8d80c10":"code","c6e67c21":"code","55ccf395":"code","cae5c4cf":"code","da37be5e":"code","12232be4":"code","5379568f":"code","d8299061":"code","fe81a553":"code","ac633c37":"code","00cd8ee4":"code","a365e3b6":"code","9564874f":"code","4ff47a2a":"code","608aab1b":"code","698280ac":"code","78b42098":"code","fed5c901":"code","bbae2b6a":"code","3126c435":"code","2ebadbbf":"code","e9963c70":"code","3109673f":"code","2bfecb50":"code","aaaa58cb":"code","7d0739e6":"code","b17b2eb1":"code","cc9283dd":"code","5a59f439":"code","bbbe1ae0":"code","4d50e1bd":"code","a722af94":"code","1e42b16f":"code","20ab72ee":"markdown","9c88211e":"markdown","25a0d26e":"markdown","a804c4b0":"markdown","4c166b0a":"markdown","aed005f6":"markdown","91d557fa":"markdown","deaf41bf":"markdown","f45ec98c":"markdown","c48b0182":"markdown","c11e2a21":"markdown","6ad30369":"markdown","e4ba8b13":"markdown","3ed5a2e6":"markdown","d1e1c503":"markdown","03ca9d30":"markdown","8a939419":"markdown","99444fee":"markdown","3a3eddaa":"markdown","7a77e3ff":"markdown","fe202659":"markdown","377eac29":"markdown","40b8b465":"markdown","a3fd4fc8":"markdown","f4ed52da":"markdown","6c8ccd62":"markdown","7529b961":"markdown","6117c891":"markdown","3c8d1bd5":"markdown","ef21a776":"markdown","77336824":"markdown","c928ea37":"markdown","028eac08":"markdown","73fd267b":"markdown","3797747a":"markdown","c8922678":"markdown","ad5e5188":"markdown","cb555eba":"markdown","13f90afd":"markdown","45d2b0b5":"markdown","49a07cff":"markdown","bbf5aed2":"markdown","1b6f22cd":"markdown","4fd2611c":"markdown","3f6c0dce":"markdown","e985a041":"markdown"},"source":{"00160547":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","0a3a5cf5":"house_prices = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","28b6f248":"house_prices.head(10)","dd0b6d38":"house_prices.shape()","f12d2e32":"house_prices.columns","dffcc667":"#SalePrice - the property's sale price in dollars. This is the target variable that you're trying to predict.\nhouse_prices['SalePrice'].describe()","2bcf35c4":"#bar plot \nfig = plt.figure()\nax = fig.add_axes([0,0,1,1])\nax.bar(house_prices['Street'],house_prices['SalePrice'])\nax.set_ylabel('Price')\nax.set_xlabel('Street')\n\nplt.show()","69536003":"#histogram plot \nplt.hist(house_prices['SalePrice'],50)\nplt.ylabel('Count')\nplt.xlabel('Price')\nplt.show()","5c179f50":" house_prices['SalePrice'][0:4]","493dc51c":"labels = house_prices['LotShape'][0:4]#'Frogs', 'Hogs', 'Dogs', 'Logs'\nsizes = house_prices['SalePrice'][0:4] #[15, 30, 45, 10]\nexplode = (0, 0.1, 0, 0)  # only \"explode\" the 2nd slice (i.e. 'Hogs')\n\nfig1, ax1 = plt.subplots()\nax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n        shadow=True, startangle=90)\nax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n\nplt.show()","eb7b9ec5":"# univariate distribution of observations\nsns.distplot(house_prices['SalePrice']);","3be347cb":"#Deviate from the normal distribution.Have appreciable positive skewness.Show peakedness.\nprint(\"Skewness: %f\" % house_prices['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % house_prices['SalePrice'].kurt())","6ca4a5d9":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea' #GrLivArea: Above grade (ground) living area square feet\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","7a83453f":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'#TotalBsmtSF: Total square feet of basement area\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","9a82fce0":"#box plot overallqual\/saleprice \n# OverallQual: Overall material and finish quality examples\n\nvar = 'OverallQual'\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","5f309d39":"var = 'YearBuilt'\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","f9c35c2d":"#correlation matrix\ncorrmat = house_prices.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","68a2d866":"corrmat","4ac90290":"#saleprice correlation matrix zoomed in\n\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncols","14619a24":"cm = np.corrcoef(house_prices[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","478d002d":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(house_prices[cols], size = 2.5)\nplt.show();","d27790b8":"my_filepath = \"..\/input\/world-happiness\/2019.csv\"\nmy_data=pd.read_csv(my_filepath, index_col=\"Overall rank\")\nmy_data.head(5)\n","a8d80c10":"my_data.describe()\n","c6e67c21":"my_data.info()\n","55ccf395":"# num_bins=10\n# plt.xlabel('Generosity')\n# plt.ylabel('values')\n# plt.hist(my_data['Generosity'], num_bins)\nsns.set_style('darkgrid')\nsns.distplot(my_data['Generosity'])","cae5c4cf":"num_bins=10\nplt.xlabel('Healthy life expectancy')\nplt.ylabel('count')\nplt.hist(my_data['Healthy life expectancy'], num_bins)","da37be5e":"num_bins=10\nplt.xlabel('GDP per capita')\nplt.ylabel('count')\nplt.hist(my_data['GDP per capita'], num_bins)","12232be4":"plt.figure(figsize=(20,50))\nsns.barplot(x=my_data[\"Score\"], y=my_data['Country or region'])","5379568f":"sns.heatmap(data=my_data.corr(), annot=True)\nplt.show()","d8299061":"sns.regplot(x=my_data['Healthy life expectancy'], y=my_data['Social support'])","fe81a553":"sns.regplot(x=my_data['Healthy life expectancy'], y=my_data['Freedom to make life choices'])","ac633c37":"#.isnull().sum()\nmissing_count = my_data.isnull().sum()\nmissing_count","00cd8ee4":"#missing data\ntotal = house_prices.isnull().sum().sort_values(ascending=False)\npercent = (house_prices.isnull().sum()\/house_prices.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","a365e3b6":"house_prices.shape","9564874f":"#dealing with missing data\nhouse_prices = house_prices.drop((missing_data[missing_data['Total'] > 1]).index,1)\nhouse_prices = house_prices.drop(house_prices.loc[house_prices['Electrical'].isnull()].index)\nhouse_prices.isnull().sum().max() #just checking that there's no missing data missing...","4ff47a2a":"house_prices.shape","608aab1b":"# normalize the exponential data with boxcox\nnormalized_data = stats.boxcox(original_data)\n\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(normalized_data[0], ax=ax[1])\nax[1].set_title(\"Normalized data\")","698280ac":"import datetime\n\n# read in our data\nlandslides = pd.read_csv(\"..\/input\/landslide-events\/catalog.csv\")\nnp.random.seed(0)\nprint(landslides['date'].head())\nlandslides['date'].dtype\n","78b42098":"# create a new column, date_parsed, with the parsed dates\nlandslides['date_parsed'] = pd.to_datetime(landslides['date'], format = \"%m\/%d\/%y\")\n# print the first few rows\nlandslides['date_parsed'].head()","fed5c901":"# try to get the day of the month from the date column\nday_of_month_landslides = landslides['date'].dt.day","bbae2b6a":"# get the day of the month from the date_parsed column\nday_of_month_landslides = landslides['date_parsed'].dt.day\nday_of_month_landslides","3126c435":"# for min_max scaling\nfrom mlxtend.preprocessing import minmax_scaling\n\noriginal_data = np.random.exponential(size = 1000)\noriginal_data[0:10]","2ebadbbf":"# mix-max scale the data between 0 and 1\nscaled_data = minmax_scaling(original_data, columns = [0])\nscaled_data[0:10]","e9963c70":"\n# plot both together to compare\nfig, ax=plt.subplots(1,2)\nsns.distplot(original_data, ax=ax[0])\nax[0].set_title(\"Original Data\")\nsns.distplot(scaled_data, ax=ax[1])\nax[1].set_title(\"Scaled data\")","3109673f":"import pandas as pd\nhouse_prices = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","2bfecb50":"#deleting points\nhouse_prices.sort_values(by = 'GrLivArea', ascending = False)[:2]\nhouse_prices = house_prices.drop(house_prices[house_prices['Id'] == 1299].index)\nhouse_prices = house_prices.drop(house_prices[house_prices['Id'] == 524].index)","aaaa58cb":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([house_prices['SalePrice'], house_prices[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","7d0739e6":"#histogram and normal probability plot with SalePrice\nsns.distplot(house_prices['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(house_prices['SalePrice'], plot=plt)\n","b17b2eb1":"#applying log transformation\nhouse_prices['SalePrice'] = np.log(house_prices['SalePrice'])","cc9283dd":"#transformed histogram and normal probability plot\nsns.distplot(house_prices['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(house_prices['SalePrice'], plot=plt)","5a59f439":"#histogram and normal probability plot for GrLivArea\nsns.distplot(house_prices['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(house_prices['GrLivArea'], plot=plt)","bbbe1ae0":"#data transformation\nhouse_prices['GrLivArea'] = np.log(house_prices['GrLivArea'])","4d50e1bd":"#transformed histogram and normal probability plot\nsns.distplot(house_prices['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(house_prices['GrLivArea'], plot=plt)","a722af94":"#scatter plot\nplt.scatter(house_prices['GrLivArea'], house_prices['SalePrice']);","1e42b16f":"world_bank_Data = pd.read_csv('..\/input\/world-bank-data-1960-to-2016')\nworld_bank_Data.head()","20ab72ee":"## Missing values\nData intuition also comes into play here -  why column has missing values - was it not recorded or does it not exist? \nData Scientist or not, I urge you to think about the bigger picture always, think about the process, how you reached where you are at, and why you are doing something. \nSteps: \n1. See how many missing data points we have\n2. Figure out why the data is missing\n3. Drop missing values. Keep track of just how much data is lost.\n4. Filling in missing values automatically aka Imputation (A Better Option)\n","9c88211e":"Since there are several variables to keep finding a relationship with our target variable, \nlets first create a heatmap using seahorse itself.\n","25a0d26e":"Successful companies not only capture and have access to data, but they\u2019re also able to derive insights that drive better decisions, which result in better customer service, competitive differentiation, and higher revenue growth. \n\nThe process of understanding the data plays a key role in the process of choosing the right algorithm for the right problem. \n\nSome algorithms can work with smaller sample sets while others require tons and tons of samples. Certain algorithms work with categorical data while others like to work with numerical input.","a804c4b0":"## Understanding correlation. \n","4c166b0a":" 'TotalBsmtSF' and 'GrLiveArea' ---> the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area.\n\n 'SalePrice' and 'YearBuilt' ---> 'dots cloud'---> a exponential function. same tendency in the upper limit of the 'dots cloud'. Also, notice how the set of dots regarding the last years tend to stay above this limit ---> prices are increasing faster now.\n\n","aed005f6":"We're getting this error because the dt.day() function doesn't know how to deal with a column with the dtype \"object\". Even though our dataframe has dates in it, because they haven't been parsed we can't interact with them in a useful way.","91d557fa":"\nhttps:\/\/medium.com\/@dataakkadian?source=post_page-----295d0b0c7f60--------------------------------\n\nOnce you have analyzed the data, understood the problem well enough, you create the model either from scratch or\nusing a known named algorithm tuning it for your solution. \n\n#### Implement machine learning algorithms.\n* Set up a machine learning pipeline that compares the performance of each algorithm on the dataset using a set of carefully selected evaluation criteria.\n* Another approach is to use the same algorithm on different subgroups of datasets. \n* The best solution for this is to do it once or have a service running that does this in intervals when new data is added.\n\n\n#### Questions to ask yourself when you decide on the model\/algorithm to use \n\n- The accuracy of the model,  interpretability, complexity,  scalability of the model.\n- How long does it take to build, train, and test the model?\n- How long does it take to make predictions using the model?\n- Does the model meet the business goal?\n\n### Validation strategies. Will be taken in detail with examples by Swetha. \n So no one strategy would work for all scenarios. There could be data leakage or not. \n* Train\/test split - usually we have a 70% training and 30 % for testing our model but there could a sampling bias. \nTo avoid this there are various strategies \n* Holdout set \n* k-Fold Cross-Validation\n* Leave-one-out Cross-Validation\n* Leave-one-group-out Cross-Validation\n\nThere are more ways mentioned in this article\n\nhttps:\/\/towardsdatascience.com\/validating-your-machine-learning-model-25b4c8643fb7\n\n\n#### Optimize hyperparameters.\u00a0\nThere are three options for optimizing hyperparameters, \n* grid search, random search, and Bayesian optimization.\n\n\n#### Model deployment\nIf you have answered all of the above and are satisfied (within the stipulated time given to you) - Finally deploy\n","deaf41bf":"Skewness is a measure of symmetry, or more precisely, the lack of symmetry. A distribution, or data set, is symmetric if it looks the same to the left and right of the center point.\n\nKurtosis is a measure of whether the data are heavy-tailed or light-tailed relative to a normal distribution. That is, data sets with high kurtosis tend to have heavy tails, or outliers. Data sets with low kurtosis tend to have light tails, or lack of outliers. A uniform distribution would be the extreme case.\n\n\nhttps:\/\/www.itl.nist.gov\/div898\/handbook\/eda\/section3\/eda35b.htm\nMore details on Skewness and kurtosis\n","f45ec98c":"# **Categorize by the input:**\n\n### 1. a labeled data ---> supervised learning problem.\nhuman supervises\/guides the algorithm on what conclusions it should come up with. For this we need to know algorithm\u2019s possible outputs beforehand. \n![image.png](attachment:image.png)\n\n","c48b0182":"Some practical strategies.\n\n* Correlation matrix (heatmap style).\n* 'SalePrice' correlation matrix.\n* Scatter plots between the most correlated variables","c11e2a21":"## Normality","6ad30369":"Practise","e4ba8b13":"### **Definitions**\n* Median\nThe median (middle quartile) marks the mid-point of the data and is shown by the line that divides the box into two parts. Half the scores are greater than or equal to this value and half are less.\n\n* Inter-quartile range\nThe middle \u201cbox\u201d represents the middle 50% of scores for the group. The range of scores from lower to upper quartile is referred to as the inter-quartile range. The middle 50% of scores fall within the inter-quartile range.\n\n* Upper quartile\nSeventy-five percent of the scores fall below the upper quartile. \n* Lower quartile\nTwenty-five percent of scores fall below the lower quartile.\n\n* Whiskers\nThe upper and lower whiskers represent scores outside the middle 50%. Whiskers often (but not always) stretch over a wider range of scores than the middle quartile groups.\n\nExample to understand how to interpret these box plots\n\nhttps:\/\/www.khanacademy.org\/math\/statistics-probability\/summarizing-quantitative-data\/box-whisker-plots\/a\/box-plot-review\n\nhttps:\/\/www.wellbeingatschool.org.nz\/information-sheet\/understanding-and-interpreting-box-plots","3ed5a2e6":"In the search for writing 'homoscedasticity' right at the first attempt\nThe best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\n","d1e1c503":"# **Categorize by output:**\n\n1. a number ---> regression problem. (supervised)\n2. a class ---> classification problem. (supervised)\n3. a set of input groups ---> it\u2019s a clustering problem.\n![image.png](attachment:image.png)\n\nBriefly - Linear Regression: \n y = mx\nrelationships between two continuous (quantitative) variables.\n X - the independent variable. \n y - the dependent variable. \nWe use X to explain or predict Y.\n\n![fig2.1.png](attachment:fig2.1.png)\n\nMultiple Linear Regression:\n\ny = b1x1 + b2x2 + \u2026 + bnxn + c.\n\n\nMultiple regression generally explains the relationship between multiple independent or predictor variables and one dependent or criterion variable.  A dependent variable is modeled as a function of several independent variables with corresponding coefficients, along with the constant term.  Multiple regression requires two or more predictor variables, and this is why it is called multiple regression.\n\n\n\nHere, bi\u2019s (i=1,2\u2026n) are the regression coefficients, which represent the value at which the criterion variable changes when the predictor variable changes.\n\nWorks well even with huge datasets. But it can be unstable in case features are redundant.\n\n\n2-Logistic Regression\nThis performs binary classification, ---> outputs are binary. \nIts a special case of linear regression --->  output variable is categorical, where we are using a log of odds as the dependent variable. \nFun fact - Uses a linear combination of features, applies a nonlinear function (sigmoid) to it, ---> tiny instance of the neural network!\n\n![linear_vs_logistic_regression_h8voek.jpg](attachment:linear_vs_logistic_regression_h8voek.jpg)\n\n\n","03ca9d30":"## Outliers\nAlmost every data would have some or the other outlier that doesn't follow the patterns of the rest of the variables. Anomalies. Their detections are super important. Eugene.ai is a company that detect anomalies or outliers from any data and warns you about it. \n","8a939419":"https:\/\/www.geeksforgeeks.org\/heteroscedasticity-in-regression-analysis\/ \n\n<!-- ![image.png](attachment:image.png) -->","99444fee":"# Relationship with categorical features\n\n* 1 continuous and 1 categorical box and whiskers plot\n* heatmap","3a3eddaa":"## Scaling - Normalisation\/Standardisation\n\nhttps:\/\/towardsdatascience.com\/everything-you-need-to-know-about-min-max-normalization-in-python-b79592732b79","7a77e3ff":"* GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n* We just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).","fe202659":"### If the heatmap looks confusing to you with those colors. (color blindness is way more common than you think) lets see the numbers first! \n","377eac29":"* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them! We can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twins.\n* 'TotRmsAbvGrd' and 'GrLivArea', twins again.","40b8b465":"#### Bivariate analysis\n* 2 Continuous variables ---> Scatter plot","a3fd4fc8":"2 types of variables (column names) - continuous & Categorical \n* continuous - variables that have numerical data values\n* categorical - ordinal values, binary values, grades, ","f4ed52da":"A dataset along with the business problem helps us decide what approach to take.\n\nThe next step is to categorize the problem.\n* a) By Output \n* b) By Input","6c8ccd62":"# Dates","7529b961":"> how to choose the \"best\" number of bins is an interesting one, and there's actually a fairly vast literature on the subject. There are some commonly-used rules-of-thumb that have been proposed (e.g. the Freedman-Diaconis Rule, Sturges' Rule, Scott's Rule, the Square-root rule, etc.) each of which has its own strengths and weaknesses. https:\/\/stackoverflow.com\/questions\/33458566\/how-to-choose-bins-in-matplotlib-histogram","6117c891":"# Why EDA\n\ndeveloped back in the 1970s by John Turkey \u2013 the same scientist who coined the word \u201cBit\u201d (short for Binary Digit). \nNo hard-and-fast rules for approaching it. \n\nHelps in: \n* Spotting missing and erroneous data\n* Mapping and understanding the underlying structure of your data\n* Identifying the most important variables in your dataset\n* Testing a hypothesis or checking assumptions related to a specific model\n* Establishing a parsimonious model (one that can explain your data using minimum variables)\n\u00a0","3c8d1bd5":"### strong linear relationship but becomes exponential ","ef21a776":"## How to read a box and whiskers plot\n","77336824":"\nThanks to [Pedro Marcelino](https:\/\/www.kaggle.com\/pmarcelino), PhD whose comprehension strategies helped me add a lot of interpretation to the Data Visualisations","c928ea37":"# Relationship with Numerical features","028eac08":"#  Agenda for the day\n * ML lifecycle. Problem solving techniques.  \n * Why is EDA important ?\n * Univariate and bivariate data analysis along with different visualizations - with Case studies.\n * Dataset cleaning strategies\n ","73fd267b":"### 2. unlabeled data ---> to find structure ---> unsupervised learning problem. \n\nApparently true artificial intelligence as a computer learns to identify patterns without a human.\nFor this we have very little information about objects and try to come up with clusters by observing some similarities between groups of objects. Those that don't fit in any of the clusters ---> anomalies.\n\n![image.png](attachment:image.png)\n\n#### K-means\nThis is a  clustering algorithm used to automatically divide a large group into smaller groups. K number of groups. First round of clustering when compared to a second round of clustering might have different points, so we have to take the average. \nIf None of the points change groups, so you\u2019re finished. Otherwise, try again.\n","3797747a":"# Data Cleaning \nhttps:\/\/www.kaggle.com\/getting-started\/52652\n* Day 1: Handling missing values\n* Day 2: Data scaling and normalization\n* Day 3: Cleaning and parsing dates\n* Day 4: Character encoding errors (no more messed up text fields!)\n* Day 5: Fixing inconsistent data entry & spelling errors","c8922678":"# Neural Network","ad5e5188":"As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.","cb555eba":"![image.png](attachment:image.png)","13f90afd":"## Few Assumptions to check for\n1. Normality - Our data should look like a normal distribution. This is important because several statistic tests rely on this (e.g. t-statistics). \n    - check univariate normality for 'SalePrice' (which is a limited approach). \n    - Note: univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. \n    - in big samples (>200 observations) normality is not such an issue. \n    - if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity).\n\n2. Homoscedasticity - assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' (Hair et al., 2013).\n   - desirable; we want the error term to be the same across all values of the independent variables.\n\n3. Linearity- The most common way to assess linearity is to examine scatter plots and search for linear patterns. \n   - If patterns are not linear, it would be worthwhile to explore data transformations. ","45d2b0b5":"![image.png](attachment:image.png)","49a07cff":"### 3. optimize a function by interacting with an environment ---> reinforcement learning problem.\n\nFor this we refer to goal-oriented algorithms, aiming to maximize along a particular dimension over many steps. eg: maximize the points won in a game over many moves. Since there is no training dataset, the reinforcement agent learns from experience and decides what to do on ths spot. \n\n\n![image.png](attachment:image.png)\nAs you graduate you will understand this video better\n<!-- ![1_qVq3H2Lln3WB1lb5hBInfg.png](attachment:1_qVq3H2Lln3WB1lb5hBInfg.png)\n -->\nhttps:\/\/www.youtube.com\/watch?v=Lu56xVlZ40M ","bbf5aed2":"# Univariate and BiVariate \n\nUnivariate visualisations are essentially probability distributions of each and every field in the raw dataset \u2013 with summary statistics. \n\n**Univariate visualisations use**\n\n* frequency distribution tables, \n* bar charts, \n* histograms, or \n* pie charts for the graphical representation.\n\n**Bivariate Analysis depend on the type of variable in question**\n\n* For instance, if you\u2019re dealing with **two continuous** variables, **a scatter plot** should be the graph of your choice. \n* If one is categorical and the other is continuous, a **box plot** is preferred and \n* when both the variables are categorical, a **mosaic plot** is chosen.\n\nMultivariate visualizations help in understanding the interactions between different data-fields. It involves observation and analysis of more than one statistical outcome variable at any given time.","1b6f22cd":"# \"Correlation Is Not Causation\"\n* for eg: The correlation between Sunglasses and Ice Cream sales is high\n\nDoes this mean that sunglasses make people want ice cream?","4fd2611c":"\n![image.png](attachment:image.png)\nImage Source : https:\/\/medium.com\/analytics-vidhya\/machine-learning-development-life-cycle-dfe88c44222e","3f6c0dce":"### that's a linear relationship! Price of the property increases as the living area increases. ","e985a041":"Since from the  first one heatmap look at 'TotalBsmtSF' and '1stFlrSF' variables,  & 'GarageX' variables. \nsignificant the correlation is between these variables. \n### Its so strong ---> we could have a situation of  multicollinearity.\n(When more than two explanatory variables in a multiple regression model are highly linearly related. perfect multicollinearity the correlation between two independent variables is equal to 1 or \u22121)\n\n'SalePrice' has high correlations with 'GrLivArea', 'TotalBsmtSF', and 'OverallQual'. But there are definitely other factors. "}}