{"cell_type":{"083def0a":"code","65778c84":"code","7723c353":"code","f94d78f1":"code","13045f02":"code","a0a98bde":"code","0d99f140":"code","bc59c17a":"code","f9ad02d5":"markdown","37b31792":"markdown","3dc1f4e6":"markdown","de94f713":"markdown","206e35a0":"markdown","73bb5d53":"markdown","49fb3094":"markdown","7419674d":"markdown","d5d9770a":"markdown","9a8cf2d0":"markdown","eb96c9b7":"markdown","f9cefdd9":"markdown","84ce2c0a":"markdown","7fa6c62a":"markdown"},"source":{"083def0a":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV, SGDClassifier\nfrom sklearn.model_selection import validation_curve, learning_curve\nfrom matplotlib import pyplot as plt","65778c84":"data = pd.read_csv('..\/input\/telecom_churn.csv').drop('State', axis=1)\ndata['International plan'] = data['International plan'].map({'Yes': 1, 'No': 0})\ndata['Voice mail plan'] = data['Voice mail plan'].map({'Yes': 1, 'No': 0})\n\ny = data['Churn'].astype('int').values\nX = data.drop('Churn', axis=1).values","7723c353":"alphas = np.logspace(-2, 0, 20)\nsgd_logit = SGDClassifier(loss='log', n_jobs=-1, random_state=17, max_iter=5)\nlogit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=2)), \n                       ('sgd_logit', sgd_logit)])\nval_train, val_test = validation_curve(logit_pipe, X, y,\n                                       'sgd_logit__alpha', alphas, cv=5,\n                                       scoring='roc_auc')","f94d78f1":"def plot_with_err(x, data, **kwargs):\n    mu, std = data.mean(1), data.std(1)\n    lines = plt.plot(x, mu, '-', **kwargs)\n    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n                     facecolor=lines[0].get_color(), alpha=0.2)\n\nplot_with_err(alphas, val_train, label='training scores')\nplot_with_err(alphas, val_test, label='validation scores')\nplt.xlabel(r'$\\alpha$'); plt.ylabel('ROC AUC')\nplt.legend()\nplt.grid(True);","13045f02":"def plot_learning_curve(degree=2, alpha=0.01):\n    train_sizes = np.linspace(0.05, 1, 20)\n    logit_pipe = Pipeline([('scaler', StandardScaler()), ('poly', PolynomialFeatures(degree=degree)), \n                           ('sgd_logit', SGDClassifier(n_jobs=-1, random_state=17, alpha=alpha, max_iter=5))])\n    N_train, val_train, val_test = learning_curve(logit_pipe,\n                                                  X, y, train_sizes=train_sizes, cv=5,\n                                                  scoring='roc_auc')\n    plot_with_err(N_train, val_train, label='training scores')\n    plot_with_err(N_train, val_test, label='validation scores')\n    plt.xlabel('Training Set Size'); plt.ylabel('AUC')\n    plt.legend()\n    plt.grid(True);","a0a98bde":"plot_learning_curve(degree=2, alpha=10)","0d99f140":"plot_learning_curve(degree=2, alpha=0.05)","bc59c17a":"plot_learning_curve(degree=2, alpha=1e-4)","f9ad02d5":"Una situaci\u00f3n t\u00edpica: para una peque\u00f1a cantidad de datos, los errores entre la capacitaci\u00f3n y los conjuntos de validaci\u00f3n cruzada son bastante diferentes, lo que indica un exceso de adaptaci\u00f3n. Para ese mismo modelo, pero con una gran cantidad de datos, los errores \"convergen\", lo que indica un defecto de adaptaci\u00f3n.\n\u00a0\nSi agregamos m\u00e1s datos, el error en el conjunto de entrenamiento no crecer\u00e1. Por otro lado, el error en los datos de prueba no se reducir\u00e1.\n\u00a0\nPor lo tanto, vemos que los errores \"convergieron\" y la adici\u00f3n de nuevos datos no ayudar\u00e1. En realidad este caso es el m\u00e1s interesante para los negocios. Es posible que aumentemos el tama\u00f1o del conjunto de datos en 10x, pero, sin cambiar la complejidad del modelo, es posible que estos datos adicionales no ayuden. Por lo tanto, la estrategia de \"establecer una vez, luego usar 10 veces\" podr\u00eda no funcionar.\n\u00a0\n\u00bfQu\u00e9 pasa si reducimos el coeficiente de regularizaci\u00f3n a 0.05?\n\u00a0\nVemos una buena tendencia: las curvas convergen gradualmente, y si nos movemos m\u00e1s hacia la derecha, es decir, agregamos m\u00e1s datos al modelo, podemos mejorar la calidad del conjunto de validaci\u00f3n a\u00fan m\u00e1s.","37b31792":"Trabajaremos nuestros datos en la rotaci\u00f3n de clientes del operador de telecomunicaciones.","3dc1f4e6":"**Entrenaremos la regresi\u00f3n log\u00edstica con pendiente de gradiente estoc\u00e1stico. M\u00e1s adelante en el curso, tendremos un art\u00edculo separado sobre este tema.**","de94f713":"# <center>Tema 4. Clasificaci\u00f3n lineal y regresi\u00f3n.\n## <center> Parte 5. Validaci\u00f3n y curvas de aprendizaje.","206e35a0":"### Recursos \u00fatiles\n- Medium [\"historia\"](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-4-linear-classification-and-regression-44a41b9b5220) basado en este cuaderno\n- Materiales del curso como un [conjunto de datos de Kaggle](https:\/\/www.kaggle.com\/kashnitsky\/mlcourse)\n- Si lees ruso: un [art\u00edculo] (https:\/\/habrahabr.ru\/company\/ods\/blog\/323890\/) en Habrahabr con ~ el mismo material. Y una [conferencia](https:\/\/youtu.be\/oTXGQ-_oqvI) en YouTube\n- En el libro [\"Aprendizaje profundo\"](http:\/\/www.deeplearningbook.org) (I. Goodfellow, Y. Bengio y A. Courville) se ofrece una descripci\u00f3n general agradable y concisa de los modelos lineales.\n- Los modelos lineales est\u00e1n cubiertos pr\u00e1cticamente en todos los libros de ML. Recomendamos \u201cReconocimiento de patrones y aprendizaje autom\u00e1tico\u201d (C. Bishop) y \u201cAprendizaje autom\u00e1tico: una perspectiva probabil\u00edstica\u201d (K. Murphy).\n- Si prefiere una visi\u00f3n general del modelo lineal desde el punto de vista de un estad\u00edstico, mire \"Los elementos del aprendizaje estad\u00edstico\" (T. Hastie, R. Tibshirani y J. Friedman).\n- El libro \u201cAprendizaje de m\u00e1quinas en acci\u00f3n\u201d (P. Harrington) lo guiar\u00e1 a trav\u00e9s de las implementaciones de algoritmos ML cl\u00e1sicos en Python puro.\n- [Scikit-learn](http:\/\/scikit-learn.org\/stable\/documentation.html) biblioteca. Estos chicos trabajan duro para escribir documentaci\u00f3n muy clara.\n- Scipy 2017 [tutorial de scikit-learn](https:\/\/github.com\/amueller\/scipy-2017-sklearn) por Alex Gramfort y Andreas Mueller.\n- Uno m\u00e1s [curso de ML](https:\/\/github.com\/diefimov\/MTH594_MachineLearning) con muy buenos materiales.\n- [Implementaciones](https:\/\/github.com\/rushter\/MLAlgorithms) de muchos algoritmos ML. B\u00fasqueda de regresi\u00f3n lineal y regresi\u00f3n log\u00edstica.","73bb5d53":"Veamos qu\u00e9 obtenemos para el modelo lineal. Estableceremos el coeficiente de regularizaci\u00f3n como bastante grande.","49fb3094":"**Como primer paso, construiremos curvas de validaci\u00f3n que muestren c\u00f3mo la calidad (ROC-AUC) en los conjuntos de entrenamiento y pruebas var\u00eda con el par\u00e1metro de regularizaci\u00f3n.**","7419674d":"Ahora, \u00bfqu\u00e9 sucede si hacemos que el modelo sea a\u00fan m\u00e1s complejo al establecer alfa = 10-4?\n\nSe observa un ajuste excesivo: el AUC disminuye tanto en el entrenamiento como en los conjuntos de validaci\u00f3n.","d5d9770a":"Ahora que tenemos una idea de validaci\u00f3n de modelos, validaci\u00f3n cruzada y regularizaci\u00f3n. Consideremos la pregunta m\u00e1s grande:\n\n**\u00bfQu\u00e9 hacer si la calidad del modelo no es satisfactoria? **\n\n- \u00bfDebemos hacer el modelo m\u00e1s complicado o m\u00e1s sencillo?\n- \u00bfDeber\u00edamos a\u00f1adir m\u00e1s caracter\u00edsticas?\n- \u00bfSimplemente necesitamos m\u00e1s datos para el entrenamiento?\n\nLas respuestas a estas preguntas no son obvias. En particular, a veces un modelo m\u00e1s complejo puede llevar a un deterioro en el rendimiento. Otras veces, agregar nuevas observaciones no traer\u00e1 cambios notables. De hecho, la capacidad de tomar la decisi\u00f3n correcta y elegir el m\u00e9todo correcto para mejorar el modelo distingue a un buen profesional de uno malo.","9a8cf2d0":"La construcci\u00f3n de estas curvas puede ayudar a comprender qu\u00e9 camino tomar y c\u00f3mo ajustar adecuadamente la complejidad del modelo para los nuevos datos.","eb96c9b7":"<center>\n<img src=\"https:\/\/mlcourse.ai\/notebooks\/blob\/master\/img\/ods_stickers.jpg\" \/>\n    \n## [mlcourse.ai](mlcourse.ai), open Machine Learning course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated and edited by [Christina Butsko](https:\/\/www.linkedin.com\/in\/christinabutsko\/), [Nerses Bagiyan](https:\/\/www.linkedin.com\/in\/nersesbagiyan\/), [Yulia Klimushina](https:\/\/www.linkedin.com\/in\/yuliya-klimushina-7168a9139), and [Yuanyuan Pao](https:\/\/www.linkedin.com\/in\/yuanyuanpao\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose.","f9cefdd9":"**Conclusiones sobre las curvas de aprendizaje y validaci\u00f3n:**\n\n- El error en el conjunto de entrenamiento no dice nada sobre la calidad del modelo.\n- El error de validaci\u00f3n cruzada muestra qu\u00e9 tan bien el modelo se ajusta a los datos (la tendencia existente en los datos) al tiempo que conserva la capacidad de generalizar a nuevos datos.\n- **Curva de validaci\u00f3n** es un gr\u00e1fico que muestra los resultados de los conjuntos de capacitaci\u00f3n y validaci\u00f3n seg\u00fan la **complejidad del modelo**:\n\u00a0\u00a0\u00a0\u00a0 + Si las dos curvas est\u00e1n cerca una de la otra y ambos errores son grandes, es un signo de *underfitting*\n\u00a0\u00a0\u00a0\u00a0 + Si las dos curvas est\u00e1n alejadas una de la otra, es un signo de *overfitting*\n- **Curva de aprendizaje** es un gr\u00e1fico que muestra los resultados de los conjuntos de capacitaci\u00f3n y validaci\u00f3n seg\u00fan el n\u00famero de observaciones:\n\u00a0\u00a0\u00a0\u00a0 + Si las curvas convergen, agregar datos nuevos no ayudar\u00e1, y es necesario cambiar la complejidad del modelo.\n\u00a0\u00a0\u00a0\u00a0 + Si las curvas no han convergido, agregar nuevos datos puede mejorar el resultado.\n","84ce2c0a":"La tendencia es bastante visible y es muy com\u00fan.\n\n- Para modelos simples, los errores de entrenamiento y validaci\u00f3n son cercanos y grandes. Esto sugiere que el modelo **underfitted**, lo que significa que no tiene un n\u00famero suficiente de par\u00e1metros.\n\n- Para modelos altamente sofisticados, los errores de entrenamiento y validaci\u00f3n difieren significativamente. Esto puede explicarse por **overfitting**. Cuando hay demasiados par\u00e1metros o la regularizaci\u00f3n no es lo suficientemente estricta, el ruido en los datos puede \"distraer\" el algoritmo y perder el rastro de la tendencia general.\n","7fa6c62a":"### \u00bfCu\u00e1ntos datos se necesitan?\n\nCuantos m\u00e1s datos utilice el modelo, mejor. Pero, \u00bfc\u00f3mo entendemos si los nuevos datos ser\u00e1n \u00fatiles en una situaci\u00f3n determinada? Por ejemplo, \u00bfes racional gastar $N$ para que los asesores dupliquen el conjunto de datos?\n\nComo los nuevos datos pueden no estar disponibles, es razonable variar el tama\u00f1o del conjunto de capacitaci\u00f3n y ver c\u00f3mo la calidad de la soluci\u00f3n depende de la cantidad de datos de capacitaci\u00f3n. As\u00ed es como obtenemos **curvas de aprendizaje**.\n\nLa idea es simple: mostramos el error en funci\u00f3n de la cantidad de ejemplos utilizados en la capacitaci\u00f3n. Los par\u00e1metros del modelo son fijados de antemano."}}