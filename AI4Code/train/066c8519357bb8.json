{"cell_type":{"60f6ec29":"code","912a9dfc":"code","87d56c86":"code","332a3a4b":"code","3d2ae4bc":"code","3e896dd3":"code","72302eb6":"code","632bd589":"code","f0752e81":"code","da7d75a4":"code","554d8aa8":"code","c47c427e":"code","8a66684a":"code","9c42e0ed":"code","dc4f6f6b":"code","16de5411":"code","6e5267c2":"code","447114db":"code","b0dcf1c8":"code","21dfca7a":"code","5bc9d8a1":"code","3293d599":"code","12ccf420":"code","6ba275a1":"code","6171bcea":"code","7ed72dae":"code","6a5fbba3":"code","b3ff8a05":"code","8c7a1af3":"code","26d115f1":"code","1592aad2":"code","77e329f9":"code","8f1a8117":"code","b1f64fe1":"code","13353b85":"code","f6e07ca2":"code","7679b15a":"code","5ac4adc5":"code","2e0f627a":"code","e03f1134":"code","7924b0ea":"code","574868fa":"code","0df64d0e":"code","d047dca1":"code","28361c54":"code","b16c8b0b":"code","59712bc4":"code","84f7cd0a":"code","cf7d6636":"code","83a1f22f":"markdown","ee4bdbaf":"markdown","0645eee6":"markdown","5e4c3463":"markdown","5feee8e0":"markdown","64fe1203":"markdown","d0312c42":"markdown","a4a440d2":"markdown","9d459b89":"markdown","de930d6a":"markdown","a5733ec9":"markdown","7a68242f":"markdown","546a14d1":"markdown","3fec5454":"markdown","0009f86b":"markdown","565d1af3":"markdown","2ab048db":"markdown","68a1d0d1":"markdown","1a8df85f":"markdown","a2a0d3fe":"markdown","ab2be09d":"markdown","9c80ae47":"markdown","4753af87":"markdown","6215a89d":"markdown","4149efa2":"markdown","fc4e027e":"markdown","0fc6bb78":"markdown","3aa2ecb0":"markdown","017d0abc":"markdown","99f5acc5":"markdown","13c8dbf8":"markdown","de70ae13":"markdown","7df0b598":"markdown","d7026218":"markdown","f3311723":"markdown","624ab71e":"markdown","a559f529":"markdown","eeab157c":"markdown","efd19b65":"markdown"},"source":{"60f6ec29":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import KFold, cross_val_score, cross_val_predict, train_test_split\nfrom sklearn.model_selection import StratifiedKFold, learning_curve, ShuffleSplit, GridSearchCV, RandomizedSearchCV\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn import metrics\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, roc_curve, accuracy_score\nfrom sklearn.metrics import classification_report, confusion_matrix, precision_recall_curve, average_precision_score\nfrom scipy import stats\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport matplotlib.patches as mpatches\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline","912a9dfc":"txn = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","87d56c86":"txn","332a3a4b":"txn.describe().T","3d2ae4bc":"txn.info()","3e896dd3":"txn_type = txn['Class'].apply(lambda x: 'Fraud' if x==1 else 'Not Fraud').value_counts()\nprint('There are {} fraud transactions ({:.2%})'.format(txn_type['Fraud'], txn_type['Fraud']\/txn.shape[0]))\nprint('There are {} safe transactions ({:.2%})'.format(txn_type['Not Fraud'], txn_type['Not Fraud']\/txn.shape[0]))","72302eb6":"fig, axes = plt.subplots(7,4,figsize=(14,14))\nfeats = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10','V11', 'V12', 'V13', 'V14', 'V15',\n         'V16', 'V17', 'V18', 'V19', 'V20','V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28']\nfor i, ax in enumerate(axes.flatten()):\n    ax.hist(txn[feats[i]], bins=25, color='green')\n    ax.set_title(str(feats[i])+' Distribution', color='brown')\n    ax.set_yscale('log')\nplt.tight_layout()\n\nmax_val = np.max(txn[feats].values)\nmin_val = np.min(txn[feats].values)\nprint('All values range: ({:.2f}, {:.2f})'.format(min_val, max_val))","632bd589":"plt.figure(figsize=(14,6))\nsns.distplot(txn['Time'])","f0752e81":"plt.figure(figsize=(14,6))\nsns.distplot(txn['Amount'], hist=False, rug=True)","da7d75a4":"txn['Amount'] = RobustScaler().fit_transform(txn['Amount'].values.reshape(-1,1))\ntxn['Time'] = RobustScaler().fit_transform(txn['Time'].values.reshape(-1,1))\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(14,4))\nsns.distplot(txn['Time'], ax=ax1)\nsns.distplot(txn['Amount'], hist=False, rug=True, ax=ax2)","554d8aa8":"X = txn.drop(['Class'], axis=1)\ny = txn['Class']\n\nfinal_Xtrain, final_Xtest, final_ytrain, final_ytest = train_test_split(X,\n                                    y, test_size=0.2, stratify=y, random_state=42)\n\nfinal_Xtrain = final_Xtrain.values\nfinal_Xtest = final_Xtest.values\nfinal_ytrain = final_ytrain.values\nfinal_ytest = final_ytest.values\n\ntrain_unique_label, train_counts_label = np.unique(final_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(final_ytest, return_counts=True)\n\nprint()\nprint('Proportions [Safe vs Fraud]')\nprint('Training %: '+ str(100*train_counts_label\/len(final_ytrain)))\nprint('Testing %: '+ str(100*test_counts_label\/len(final_ytest)))","c47c427e":"# shuffle data first, so it's random\ntxn = txn.sample(frac=1)\n\ntxn_fraud = txn.loc[txn['Class']==1]\ntxn_safe = txn.loc[txn['Class']==0][:492]\n\ntxn_under = pd.concat([txn_fraud, txn_safe])\ntxn_under = txn_under.sample(frac=1, random_state=41)\n\ntxn_under.shape","8a66684a":"txn_type = txn_under['Class'].apply(lambda x: 'Fraud' if x==1 else 'Not Fraud').value_counts()\nprint('Randomly Undersampled Dataset (txn_under):')\nprint('There are {} fraud transactions ({:.2%})'.format(txn_type['Fraud'], txn_type['Fraud']\/txn_under.shape[0]))\nprint('There are {} safe transactions ({:.2%})'.format(txn_type['Not Fraud'], txn_type['Not Fraud']\/txn_under.shape[0]))","9c42e0ed":"fig, axes = plt.subplots(2, 1, figsize=(20,16))\n\nsns.heatmap(txn_under.corr(), annot=True, fmt=\".2f\", cmap = 'RdYlGn', ax=axes[0])\naxes[0].set_title(\"Balanced Correlation Matrix (Reference This One)\", fontsize=20, fontweight='bold')\n\nsns.heatmap(txn.corr(), annot=True, fmt=\".2f\", cmap = 'RdYlGn', ax=axes[1])\naxes[1].set_title('Imbalanced Correlation Matrix', fontsize=20, fontweight='bold')\n\nplt.tight_layout()","dc4f6f6b":"fig, axes = plt.subplots(2,3,figsize=(14,8))\n\nhigh_corr_feats = ['V14', 'V12', 'V10', 'V11', 'V4']\n\nfor i, ax in enumerate(axes.flatten()):\n    if i == 5:\n        ax.axis('off')\n        break\n    sns.boxplot(x='Class', y=high_corr_feats[i], data=txn_under, ax=ax, palette=sns.color_palette('magma_r', 2))\n    ax.set_ylabel(None)\n    ax.set_title(label=high_corr_feats[i], fontsize=16, fontweight='bold')\nplt.tight_layout()","16de5411":"fig, axes = plt.subplots(2,3,figsize=(14,7))\nfig.suptitle('    Fraud Transaction Distributions', fontsize=20, fontweight='bold')\n\nfor i, ax in enumerate(axes.flatten()):\n    if i == 5:\n        ax.axis('off')\n        break\n    v_fraud = txn_under[txn_under['Class']==1][high_corr_feats[i]].values\n    sns.distplot(v_fraud, ax=ax, fit=stats.norm)\n    ax.set_title(str(high_corr_feats[i]), fontsize=12)","6e5267c2":"len(txn_under)","447114db":"high_corr_feats2 = ['V14', 'V12', 'V10', 'V11', 'V4']\n\nfor i in high_corr_feats2:\n    v_fraud = txn_under[txn_under['Class']==1][i]\n\n    q75 = np.percentile(v_fraud, 75)\n    q25 = np.percentile(v_fraud, 25)\n    iqr = q75 - q25\n\n    v_lower, v_upper = q25-1.5*iqr, q75+1.5*iqr\n    outliers = [x for x in v_fraud if x > v_upper or x < v_lower]\n\n    print(str(len(outliers))+' '+str(i)+' fraud outliers: '+str(outliers)+'\\n')\n\n    txn_under = txn_under.drop(txn_under.index[txn_under[i].isin(outliers) & \n                                     txn_under['Class']==1])","b0dcf1c8":"len(txn_under)","21dfca7a":"fig, axes = plt.subplots(2, 3, figsize=(20,12))\nfig.suptitle('    Outlier Reduction', fontsize=20, fontweight='bold')\n\nloc1 = [(0.98, -17.5), (0.98, -17.3), (0.98, -14.5), (0.98, 9.2), (0.98, 10.8)]\nloc2 = [(0, -12), (0, -12), (0, -12), (0, 6), (0, 8)]\n\nfor i, ax in enumerate(axes.flatten()):\n    if i == 5:\n        ax.axis('off')\n        break\n    sns.boxplot(x=\"Class\", y=high_corr_feats[i], data=txn_under, ax=ax, palette=sns.color_palette('magma_r', 2))\n    ax.set_title(str(high_corr_feats[i]), fontsize=16, fontweight='bold')\n    ax.annotate('Fewer extreme\\n     outliers', xy=loc1[i], xytext=loc2[i],\n                arrowprops=dict(facecolor='Red'), fontsize=14)\n    ax.set_ylabel('')","5bc9d8a1":"X = txn_under.drop('Class', axis=1)\ny = txn_under['Class']\n\n# Implement dimensionality reductions\nX_pca = PCA(n_components=2, random_state=38).fit_transform(X.values)\nX_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=37).fit_transform(X.values)\nX_tsne = TSNE(n_components=2, random_state=39).fit_transform(X.values)","3293d599":"f, axes = plt.subplots(1, 3, figsize=(24,6))\n# labels = ['No Fraud', 'Fraud']\nf.suptitle('    Dimensionality Reductions', fontsize=20, fontweight='bold')\n\ngreen_patch = mpatches.Patch(color='darkgreen', label='No Fraud')\nred_patch = mpatches.Patch(color='darkred', label='Fraud')\n\ndim_red = [X_pca, X_svd, X_tsne]\ntitles = ['PCA', 'Truncated SVD', 't-SNE']\n\nfor i, ax in enumerate(axes):\n    ax.scatter(dim_red[i][:,0], dim_red[i][:,1], c=(y == 0), cmap='RdYlGn', label='No Fraud', linewidths=2)\n    ax.scatter(dim_red[i][:,0], dim_red[i][:,1], c=(y == 1), cmap='RdYlGn', label='Fraud', linewidths=2)\n    ax.set_title(titles[i], fontsize=20)\n    ax.grid(True)\n    ax.legend(handles=[green_patch, red_patch])","12ccf420":"X = txn_under.drop('Class', axis=1)\ny = txn_under['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","6ba275a1":"models = {\"Log Reg\": LogisticRegression(), \"KNN\": KNeighborsClassifier(), \"SVC\": SVC(),\n          \"D Tree\": DecisionTreeClassifier()}\n\nprint('Mean cv accuracy on undersampled data. \\n')\nfor name, model in models.items():\n    training_acc = cross_val_score(model, X_train, y_train, cv=5)\n    print(name+\":\", str(round(training_acc.mean()*100, 2))+\"%\")","6171bcea":"# Use GridSearchCV to find the best parameters.\n\nprint('Mean cv scores on undersampled data after tuning hyperparameters. \\n')\n\n# Logistic Regression \nlog_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 200, 300, 400, 500, 600, 700, 800, 900, 1000]}\ngrid_log_reg = GridSearchCV(LogisticRegression(max_iter=10000), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Log Reg: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\n# K Nearest Neighbors\nknn_params = {\"n_neighbors\": list(range(2,6,1)), 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\ngrid_knn = GridSearchCV(KNeighborsClassifier(), knn_params)\ngrid_knn.fit(X_train, y_train)\nknn = grid_knn.best_estimator_\nknn_score = cross_val_score(knn, X_train, y_train, cv=5)\nprint('KNN:     ', round(knn_score.mean() * 100, 2).astype(str) + '%')\n\n# SVC\nsvc_params = {'C': [0.5, 0.6, 0.7, 0.8, 0.9, 1], 'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\nsvc = grid_svc.best_estimator_\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('SVC:     ', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\n# DescisionTree\ntree_params = {\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": list(range(2,4,1)), \n              \"min_samples_leaf\": list(range(3,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree = grid_tree.best_estimator_\ntree_score = cross_val_score(tree, X_train, y_train, cv=5)\nprint('D Tree:  ', str(round(tree_score.mean() * 100, 2)) + '%')","7ed72dae":"cv = ShuffleSplit(n_splits=100, test_size=0.2, random_state=42)\n\nfig, axes = plt.subplots(2,2, figsize=(18,12), sharey=True)\nclassifier = [log_reg, knn, svc, tree]\ntitles = [\"Logistic Regression Learning Curve\", \"K Nearest Neighbors Learning Curve\",\n         \"Support Vector Classifier Learning Curve\", \"Decision Tree Classifier Learning Curve\"]\n\nfor i, ax in enumerate(axes.flatten()):\n    train_sizes, train_acc, test_acc = learning_curve(\n        classifier[i], X_train, y_train, cv=cv, n_jobs=4, train_sizes=np.linspace(.1, 1.0, 10))\n    train_acc_mean = np.mean(train_acc, axis=1)\n    train_acc_std = np.std(train_acc, axis=1)\n    test_acc_mean = np.mean(test_acc, axis=1)\n    test_acc_std = np.std(test_acc, axis=1)\n    ax.fill_between(train_sizes, train_acc_mean - train_acc_std, train_acc_mean + train_acc_std, alpha=0.3, color=\"#b2b8b7\")\n    ax.fill_between(train_sizes, test_acc_mean - test_acc_std, test_acc_mean + test_acc_std, alpha=0.3, color=\"#46d448\")\n    ax.plot(train_sizes, train_acc_mean, 'o-', color=\"#b2b8b7\", label=\"Training accuracy\")\n    ax.plot(train_sizes, test_acc_mean, 'o-', color=\"#46d448\", label=\"Cross-validation accuracy\")\n    ax.set_title(titles[i], fontsize=14)\n    ax.set_xlabel('Training size')\n    ax.set_ylabel('Accuracy')\n    ax.grid(True)\n    ax.legend(loc='upper right')\n    \nplt.ylim(0.86, 1.01);","6a5fbba3":"print ('Model ROC AUC \\n')\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5, method=\"decision_function\")\nprint('Log Reg: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n\nknn_pred = cross_val_predict(knn, X_train, y_train, cv=5)\nprint('KNN: {:.4f}'.format(roc_auc_score(y_train, knn_pred)))\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5, method=\"decision_function\")\nprint('SVC: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n\ntree_pred = cross_val_predict(tree, X_train, y_train, cv=5)\nprint('D Tree: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))","b3ff8a05":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknn_fpr, knn_tpr, knn_threshold = roc_curve(y_train, knn_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\nplt.figure(figsize=(12,6))\nplt.title('ROC Curves', fontsize=18)\nplt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\nplt.plot(knn_fpr, knn_tpr, label='K Nearest Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knn_pred)))\nplt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\nplt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\nplt.plot([0, 1], [0, 1], 'k--')\nplt.axis([-0.01, 1, 0, 1])\nplt.xlabel('False Positive Rate', fontsize=16)\nplt.ylabel('True Positive Rate', fontsize=16)\nplt.annotate('Minimum Possible ROC Score (50%)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n            arrowprops=dict(facecolor='Red', shrink=0.05))\nplt.legend()","8c7a1af3":"# Predict on X_test\nlog_reg_pred2 = log_reg.predict(X_test)\nknn_pred2 = knn.predict(X_test)\nsvc_pred2 = svc.predict(X_test)\ntree_pred2 = tree.predict(X_test)\n\nlog_reg_cf = confusion_matrix(y_test, log_reg_pred2)\nknn_cf = confusion_matrix(y_test, knn_pred2)\nsvc_cf = confusion_matrix(y_test, svc_pred2)\ntree_cf = confusion_matrix(y_test, tree_pred2)\n\nfig, axes = plt.subplots(2, 2,figsize=(18,10))\ntitles = ['Logistic Regression', 'K Nearest Neighbors', 'Suppor Vector Classifier', 'DecisionTree Classifier']\nconf_matrix = [log_reg_cf, knn_cf, svc_cf, tree_cf]\n\nfig.suptitle('Confusion Matrices (NearMiss Undersampling)     ', fontsize=20, fontweight='bold')\n\nfor i, ax in enumerate(axes.flatten()):\n    sns.heatmap(conf_matrix[i], ax=ax, annot=True, fmt='.0f', cmap='magma')\n    ax.set_title(titles[i], fontsize=14)\n    ax.set_xticklabels(['Predicted\\nSafe', 'Predicted\\nFraud'], fontsize=10)\n    ax.set_yticklabels(['Safe', 'Fraud'], fontsize=10)","26d115f1":"print('Logistic Regression:')\nprint(classification_report(y_test, log_reg_pred2))\n\nprint('K Nearest Neighbors:')\nprint(classification_report(y_test, knn_pred2))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, svc_pred2))\n\nprint('DecisionTree Classifier:')\nprint(classification_report(y_test, tree_pred2))","1592aad2":"accuracy_undersample = []\nprecision_undersample = []\nrecall_undersample = []\nf1_undersample = []\nauc_undersample = []\n\n# Cross-Validating correctly to determine real-world performance\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\nfor train, test in sss.split(final_Xtrain, final_ytrain):\n    pipeline_undersample = imbalanced_make_pipeline(NearMiss(), log_reg)\n    model_undersample = pipeline_undersample.fit(final_Xtrain[train], final_ytrain[train])\n    prediction_undersample = model_undersample.predict(final_Xtrain[test])\n    \n    accuracy_undersample.append(pipeline_undersample.score(final_Xtrain[test], final_ytrain[test]))\n    precision_undersample.append(precision_score(final_ytrain[test], prediction_undersample))\n    recall_undersample.append(recall_score(final_ytrain[test], prediction_undersample))\n    f1_undersample.append(f1_score(final_ytrain[test], prediction_undersample))\n    auc_undersample.append(roc_auc_score(final_ytrain[test], prediction_undersample))","77e329f9":"precision, recall, threshold = precision_recall_curve(y_train, log_reg_pred)\n\ny_pred = log_reg.predict(X_train)\n\n# Overfit\nprint('Cross-Validating on Undersampled\/Balanced Data: \\n')\nprint('Accuracy Score: {:.4f}'.format(accuracy_score(y_train, y_pred)))\nprint('Precision Score: {:.4f}'.format(precision_score(y_train, y_pred)))\nprint('Recall Score: {:.4f}'.format(recall_score(y_train, y_pred)))\nprint('F1 Score: {:.4f}'.format(f1_score(y_train, y_pred)))\nprint('---' * 20)\n\n# True\nprint('Cross-Validating on Original\/Imbalanced Data: \\n')\nprint(\"Accuracy Score: {:.4f}\".format(np.mean(accuracy_undersample)))\nprint(\"Precision Score: {:.4f}\".format(np.mean(precision_undersample)))\nprint(\"Recall Score: {:.4f}\".format(np.mean(recall_undersample)))\nprint(\"F1 Score: {:.4f}\".format(np.mean(f1_undersample)))","8f1a8117":"y_score_under = log_reg.decision_function(final_Xtest)\nundersample_average_precision = average_precision_score(final_ytest, y_score_under)\n\nfig = plt.figure(figsize=(14,5))\n\nprecision, recall, _ = precision_recall_curve(final_ytest, y_score_under)\n\nplt.step(recall, precision, color='Green', alpha=0.3, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='Green')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('Undersampled Precision-Recall Curve (Avg Score: {0:0.4f})'.format(undersample_average_precision), fontsize=16);","b1f64fe1":"accuracy_oversample = []\nprecision_oversample = []\nrecall_oversample = []\nf1_oversample = []\nauc_oversample = []\n\n# Classifier with optimal parameters - we use RandomizedSearch instead of GridSearch, given large sample size.\nlog_reg_params = {'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\nrand_log_reg = RandomizedSearchCV(LogisticRegression(random_state=4, max_iter=1000), log_reg_params, n_iter=4)\n\nfor train, test in sss.split(final_Xtrain, final_ytrain):\n    # Apply SMOTE during training, not cross-validation.\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg)\n    model = pipeline.fit(final_Xtrain[train], final_ytrain[train])\n    log_reg_sm = rand_log_reg.best_estimator_\n    prediction = log_reg_sm.predict(final_Xtrain[test])\n\n    accuracy_oversample.append(pipeline.score(final_Xtrain[test], final_ytrain[test]))\n    precision_oversample.append(precision_score(final_ytrain[test], prediction))\n    recall_oversample.append(recall_score(final_ytrain[test], prediction))\n    f1_oversample.append(f1_score(final_ytrain[test], prediction))\n    auc_oversample.append(roc_auc_score(final_ytrain[test], prediction))\n\nprint('Cross-Validation on Original\/Imbalanced Data (Correct Approach, SMOTE)')\nprint('')\nprint(\"accuracy: {:.4f}\".format(np.mean(accuracy_oversample)))\nprint(\"precision: {:.4f}\".format(np.mean(precision_oversample)))\nprint(\"recall: {:.4f}\".format(np.mean(recall_oversample)))\nprint(\"f1: {:.4f}\".format(np.mean(f1_oversample)))\nprint(\"auc: {:.4f}\".format(np.mean(auc_oversample)))","13353b85":"# Predict on X_test\nlog_reg_sm_pred = log_reg_sm.predict(X_test)\n\nlog_reg_sm_cf = confusion_matrix(y_test, log_reg_sm_pred)\n\nfig, axes = plt.subplots(1, 2,figsize=(18,6))\ntitles = ['SMOTE Oversampling', 'NearMiss Undersampling']\n\nconf_matrix = [log_reg_sm_cf, log_reg_cf]\n\nfig.suptitle('Logistic Regression Confusion Matrices     ', fontsize=20, fontweight='bold')\n\nfor i, ax in enumerate(axes.flatten()):\n    sns.heatmap(conf_matrix[i], ax=ax, annot=True, fmt='.0f', cmap='magma')\n    ax.set_title(titles[i], fontsize=14)\n    ax.set_xticklabels(['Predicted\\nSafe', 'Predicted\\nFraud'], fontsize=10)\n    ax.set_yticklabels(['Safe', 'Fraud'], fontsize=10)","f6e07ca2":"labels = ['No Fraud', 'Fraud']\n\nprint('Performance on Undersampled Test Data \\n')\nprint('Logistic Regression, SMOTE Oversampling:')\nprint(classification_report(y_test, log_reg_sm_pred, target_names=labels))\n\nprint('Logistic Regression, NearMiss Undersampling:')\nprint(classification_report(y_test, log_reg_pred2, target_names=labels))","7679b15a":"print('Logistic Regression Performance, Final Testing:\\n')\n# Logistic regression trained on undersampled data\ny_pred = log_reg.predict(final_Xtest)\nundersample_accuracy = accuracy_score(final_ytest, y_pred)\nprint('Undersampling Accuracy: {:.4f}'.format(undersample_accuracy))\n\n# Logistic regression trained on oversampled data\ny_pred_sm = log_reg_sm.predict(final_Xtest)\noversample_accuracy = accuracy_score(final_ytest, y_pred_sm)\nprint('Oversampling Accuracy: {:.4f}'.format(oversample_accuracy))","5ac4adc5":"labels = ['No Fraud', 'Fraud']\n\nprint('Logistic Regression Performance, Final Testing: \\n')\n\nprint('Logistic Regression, NearMiss Undersampling:')\nprint(classification_report(final_ytest, y_pred, target_names=labels))\n\nprint('Logistic Regression, SMOTE Oversampling:')\nprint(classification_report(final_ytest, y_pred_sm, target_names=labels))","2e0f627a":"y_score_over = log_reg_sm.decision_function(final_Xtest)\noversample_average_precision = average_precision_score(final_ytest, y_score_over)\n\nfig = plt.figure(figsize=(14,5))\n\nprecision, recall, _ = precision_recall_curve(final_ytest, y_score_over)\n\nplt.step(recall, precision, color='Red', alpha=0.3, where='post')\nplt.fill_between(recall, precision, step='post', alpha=0.3, color='Orange')\n\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.title('Oversampled Precision-Recall Curve (Avg Score: {0:0.4f})'.format(oversample_average_precision), fontsize=16);","e03f1134":"import itertools\nimport keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy","7924b0ea":"NN_undersample = Sequential([Dense(X_train.shape[1], input_shape=(X_train.shape[1], ), activation='relu'),\n                             Dense(32, activation='relu'),\n                             Dense(2, activation='softmax')])\nNN_undersample.summary()","574868fa":"NN_undersample.compile(Adam(lr=0.001), metrics=['accuracy'], loss='sparse_categorical_crossentropy')\n\nNN_undersample.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose=2)\n\nundersample_pred = NN_undersample.predict_classes(final_Xtest)","0df64d0e":"def plot_cm(cm, classes, normalize=False, title='Confusion matrix', cmap='Blues'):\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n    plt.ylabel('Actual')\n    plt.xlabel('Predicted')\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()","d047dca1":"undersample_cm = confusion_matrix(final_ytest, undersample_pred)\nlabels = ['Safe', 'Fraud']\n\nplt.figure(figsize=(6,5))\nplot_cm(undersample_cm, labels, title=\"Random Undersample\\nConfusion Matrix\")","28361c54":"print(classification_report(final_ytest, undersample_pred, target_names=labels, digits=4))","b16c8b0b":"sm = SMOTE(sampling_strategy='minority', random_state=49)\nXsm_train, ysm_train = sm.fit_sample(final_Xtrain, final_ytrain)\n\nNN_oversample = Sequential([Dense(Xsm_train.shape[1], input_shape=(Xsm_train.shape[1], ), activation='relu'),\n                            Dense(32, activation='relu'),\n                            Dense(2, activation='softmax')])\nNN_oversample.summary()","59712bc4":"NN_oversample.compile(Adam(lr=0.001), metrics=['accuracy'], loss='sparse_categorical_crossentropy')\n\nNN_oversample.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=2)\n\noversample_pred = NN_oversample.predict_classes(final_Xtest)","84f7cd0a":"oversample_smote = confusion_matrix(final_ytest, oversample_pred)\n\nplt.figure(figsize=(6,5))\nplot_cm(oversample_smote, labels, title=\"SMOTE Oversample\\nConfusion Matrix \", cmap=plt.cm.Greens)","cf7d6636":"print(classification_report(final_ytest, oversample_pred, target_names=labels, digits=4))","83a1f22f":"Awesome! Through random undersampling, we now have a balanced dataset, **txn_under**, with which we can predict fraudulent transactions without overfitting.\n\nNext, let's create a correlation matrix to get a clear view of which features are most heavily correlated with fraud. Note that we should only use a balanced dataset (i.e. **txn_under**) to determine correlations, as per our discussion above, an imbalanced dataset (i.e. **txn**) overfits to the majority class. Let's create correlation matrices for both to illustrate the difference (but make sure you only reference the balanced matrix to determine correlations!).","ee4bdbaf":"Awesome! Just as we should expect -- fraud transactions have lower values for features that are negatively correlated (V14, V12, V10) and higher values for features that are positively correlated (V11, V4).\n\nNote the presence of outliers in some of these features. We want to remove **extreme fraud outliers** from the most **highly correlated features**. This is an important preprocessing step for maximizing model performance on recognizing fraud. Addressing outliers in features with the highest correlations will help ensure we are addressing only the most impactful outliers, and removing only the most extreme outliers will help us prevent unnecessary loss of information.\n\nWe will use the IQR to identify outliers. But first, let's get a better view of the distributions for each feature, to ensure they are roughly normal, and thus our outlier removal approach makes sense.","0645eee6":"Awesome! With the highest ROC_AUC score and undersampled cross-validation accuracy, logistic regression looks like it could be our best model...\n\nWe're finally ready to test our models with our X_test\/y_test data to determine which model does the best at recognizing fraud (keep in mind, this is still not the original\/imbalanced data).","5e4c3463":"# Neural Networks\nNow let's implement two simple neural networks (NNs) to see how they perform!\n\nTo create our NNs, let's have one input layer with the same number of nodes as features plus a bias node, a second hidden layer with 32 nodes, and one output node classifying the transaction as 0 (safe) or 1 (fraud).\n\nFirst, we'll build our undersampled model.","5feee8e0":"No nulls!","64fe1203":"Wow! Oversampling with SMOTE greatly improved accuracy on the original test data. Let's compare precision and recall as well:","d0312c42":"# Logistic Regression: Oversampling (SMOTE) Performance\nNow let's proceed with our oversampling approach! We'll be using the **SMOTE** technique to oversample.\n\n#### SMOTE\nUnlike with the undersampling approach in which we randomly remove data from the majority class, with SMOTE we create new synthetic points for the minority class to achieve class balance. Specifically, SMOTE creates synthetic points between closest-neighbors of the minority class. This approach retains more information than undersampling, as we aren't removing any data from the original dataset. However, because we have to train on more data with SMOTE, the approach is more time\/resource-intensive.\n\n#### Cross-Validation\nAgain, while we should train our model on the oversampled\/balanced data, it's important that we cross-validate on the original data. Overfitting can result if we cross-validate on the oversampled data, as this population contains additional synthetic samples and does not represent the true imbalance between classes. As with undersampling, we want to cross-validate on the original\/imbalanced data to get an objective view of model performance.","a4a440d2":"These all look normal enough to proceed. We're ready to remove outliers:","9d459b89":"### Scaling\nOur next step in preparing the data for predictive modeling is to scale our features appropriately. We can infer that the **V__** features of this dataset are already scaled, as they have already undergone PCA (in which scaling would have occurred). However, we still need to scale **Amount** and **Time**. To do this, we'll use RobustScaler(), which performs better on datasets with significant outliers (note the outliers in the **Amount** distribution above).","de930d6a":"Awesome! We were able to improve our cross-validation scores.\n\nLet's plot learning curves to get a sense of whether our models are over\/underfitting. Note that the wider the gap between our training and cross-validation scores, the more likely we are overfitting:","a5733ec9":"Now, let's create a function to plot the confusion matrix...","7a68242f":"Interesting! We tend to see higher cross-validation scores (precision, accuracy, f1) when training with oversampled data than when training with undersampled data from earlier in our analysis... which makes sense.\n\nFor example, we see significantly higher precision when training on the oversampled data. The reason for this is that we don't lose any information on the safe transaction set during training (e.g. by randomly removing data) in our oversampling approach, which allows our model to more thoroughly learn the characteristics of safe transactions, and thus missclassify them less often, increasing precision.\n\nNote, however, that recall remained relatively unchanged. This is a testament to how well SMOTE is able to add new synthetic fraud transactions to the training data while maintaining the original distribution\/characteristics of fraud.","546a14d1":"Awesome! While we tend to see lower scores when cross-validating on the original\/imbalanced data, these differences make sense. \n\nFor example, we would expect to see much lower precision when cross-validating on the original\/imbalanced data than the undersampled\/balanced data, simply because the imbalanced data consists of far more safe transactions. Thus, we would expect a higher number to be misclassified as fraud, lowering precision.\n\nNote, however, that recall remains relatively unchanged. This also makes sense, as we're cross-validating on the same (i.e. unsampled) set of fraud transactions in both scenarios.\n\nNow we're ready to test our logistic regression model (trained on NearMiss undersampled data) with our final_Xtest and final_ytest data. We'll use both precision and recall to evaluate performance:","3fec5454":"# Credit Card Fraud Detection\n\nFirst, I'd like to thank Janio Martinez & Pavan Sanagapati for inspiring me to take on this project. Both have provided excellent, indepth analyses of this [Kaggle Dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud), and I've learned a great deal from both.\n\nI'd also like to thank Marco Altini for his article [Dealing with Imbalanced Data: Undersampling, Oversampling, and Proper Cross-Validation](https:\/\/www.marcoaltini.com\/blog\/dealing-with-imbalanced-data-undersampling-oversampling-and-proper-cross-validation), which helped me better understand a lot of the concepts covered in this notebook.\n\nIf you like my work, please upvote or shoot me a comment. And please let me know if you have any questions or if you have suggestion for improving upon my approach - having a conversation is the best way for everyone involved to learn & grow :)\n\n# Introduction\nIn this kernel, our overall goal is to develop the best model for classifying both fraud and safe transactions correctly, working with Kaggle's **Credit Card Fraud Detection** dataset.\n\n### The Data\nWe are given a [dataset](https:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud) of roughly 285K transactions to work with, for which we have the following features: **Time**, **Amount**, and 28 anonymized features (**V1**-**V28**) which are the result of a previously performed PCA. We know that these anonymized features have already been scaled, but Time and Amount have not. We also have our target variable **Class**, which tells us if transactions are fraudulent.\n\n### Goals\n- Perform inital Exploratory Data Analysis.\n- Balance the data through random undersampling, and using this balanced data to determine which features are most highly correlated with fraud, attempt to address the most influential extreme outliers.\n- Train, cross-validate & test a variety of different baseline classifiers on our undersampled balanced data to determine which best recognizes fraud.\n- Cross-validate and test our best baseline model on the original data to determine its true performance.\n- Investigate if we can improve our baseline model by training it on oversampled (SMOTE) balanced data, rather than undersampled balanced data.\n- Train Neural Networks with both undersampling and oversampling approaches to test which peforms better.\n- Identify which model performed best overall.\n\n\nOur approach includes:\n\n1. Exploratory Data Analysis & Preprocessing\n - View imbalance between fraud & safe transactions\n - Identify null values\n - View feature distributions\n - Perform feature scaling where needed\n2. Random Undersampling\n - Identify true correlations\n - Remove extreme outliers\n3. Dimensionality Reduction Gutcheck\n - Perform PCA, T-SNE, Truncated SVD\n - Infer how well our classifiers might performance\n4. Baseline Classification Models\n - Training & cross-validation on undersampled data\n - Optimize hyperparameters with GridSearchCV\n - View learning curves to check if models are under\/overfitting\n - Check ROC curve performance\n - Test models with undersampled data, view confusion matrices\n - Determine which model best classifies fraud (logistic regression)\n5. Logistic Regression: Undersampling (NearMiss) Performance\n - Train logistic regression using undersampled (NearMiss) data\n - Cross-Validate on the original data, compare performance to cross-validation on undersampled data\n - Test performance on final test data, look at precision-recall score\n6. Logistic Regression: Oversampling (SMOTE) Performance\n - Train logistic regression using oversampled (SMOTE) data\n - Cross-Validate on the original data, compare performance to undersampled model\n - Cross-Validate on the undersampled test data, compare performance to undersampled model\n - Test performance on final test data, compare both under & oversampled logistic regression models\n - Using accuracy, precision-recall score, and confusion matrices, determine whether undersampling (NearMiss) or oversampling (SMOTE) gives better results for logistic regression\n7. Neural Networks\n - Create two simple neural network classifiers\n - Train the first using NearMiss undersampling, test performance with original test data\n - Train the second using SMOTE oversampling, test performance with original test data\n - Using confusion matrices, compare results\n - Determine which model performed best\n\nLet's begin by loading the following libraries:","0009f86b":"Awesome! This gives us a helpful insight: with only 0.17% fraud rate, we have a very imbalanced dataset, which we will need to account for when developing our predictive models.\n\nLet's take a closer look at the distribution of each feature.","565d1af3":"Awesome, SMOTE slightly improved logistic regression's performance on the undersampled test data!\n\nLet's now see which model performs best on the original test data (final_Xtest, final_ytest). We'll start with accuracy:","2ab048db":"Our average precision-recall score on our test data is objectively low. This may seem suspiciously low after seeing our ROC curves above, but remember, we're now evaluating our model on the original\/imbalanced data. As we can see in the graph, our model allows many more safe transactions in (i.e. missclassifying them) as we relax our decision boundary.","68a1d0d1":"# Gutcheck - Dimensionality Reduction\nBefore we proceed with creating classification models, let's first get a sense of how effective our models might be through performing dimensionality reduction on our data. \n\nSpecifically, we'll use three dimensionality reduction methods (t-SNE, PCA & Truncated SVD) to reduce the number of our features to just two. We will then graph the results on the xy-plane, highlighting fraud and safe transactions differently. If we're able to see a clear separation between classes in the graphs, that will give us an indication that further predictive models may perform well at classifying fraud.","1a8df85f":"Awesome! Based on our undersampled test data, logistic regression performed the best. We will continue our analysis with just logistic regression.","a2a0d3fe":"\n# Findings \/ Conclusion\nWhile the plan is to continue to iterate on this project, below is a summary of what we've learned so far.\n\n\n## EDA & Preprocessing\nIt's important to always explore the distributions of features in a dataset before beginning any predictive modeling. This helps with many things, including identifying which features need to be scaled, uncovering imabalances in our data, and locating outliers, nulls, and other important values.\n\nWhen determining feature correlations in a highly imbalanced dataset, it's important to first balance our data through a method like random sampling. If we don't first balance our data, any correlations we derive will overfit to the majority class, and we won't have a clear view of which features most influence the minoirty class.\n\nIn our analysis, we balanced our data through **random undersampling** of safe transactions. We were then able to accurately determine correlations and identify which features would most influence our predictions. From those highly correlated features, we were able to identify and remove the most impactful extreme outliers.\n\n\n## Anticipating Model Effectiveness with Dimensionality Reduction\nWe can use dimensionality reduction techniques like T-SNE, PCA & Truncated SVD to get a sense of how well our classifiers might perform.\n\nTo do this, we first reduced the features of our dataset to just two, and visualized the results in the xy-plane. Highlighting the different classes in this visualization, we saw how easily seperable or \"clustered\" each class was, which gave us an indication that our classification models would perform well down the line.\n\n\n## Cross-Validation & Imbalanced Data\nIn dealing with this imbalanced dataset, it was important that we first balance our data before training our classification models. Otherwise, our models would have overfit to the majority class, assuming that practically all transactions are safe.\n\nWhen cross-validating, however, we took two approaches:\n\n1. First, we cross-validated on the undersampled (balanced) dataset. This allowed us to see which model best recognized fraud transactions. The performance metrics from this cross-validation approach (accuracy, precision, recall, etc.) were not representative of what we would expect to see in production, however, because the balanced data did not represent the imbalance we would expect to see in production.\n\n2. Second, to get a true view of our model's performances, we cross-validated on the original imbalanced data. This dataset was representative of the imbalance we would expect to see in production, thus cross-validating on this dataset gave an accurate view of our performance metrics.\n\n\n## Baseline Models - Random Undersampling\nWe began by evaluating the performance of four different models: Logistic Regression, K Nearest Neighbors Classifier, Support Vector Classifier, and the Decision Tree Classifier. We made sure to optimize the hyperparameters for each model using GridSearchCV, trained and cross-validated our models on the undersampled (balanced) data, and checked learning curves to confirm our models were not over\/underfitting.\n\nTo evaluate which model performed best at identifying fraud, we looked at both cross-validation accuracy and ROC_AUC on our undersampled (balanced) training data. We further evaluated each model on our undersampled test data, creating confusion matrices for each.\n\nIn the end, logistic regression performed the best at identifying fraud, thus we chose to focus on this model moving forward.\n\n\n## Logistic Regression Performance - Random Undersampling\nProceeding with our logistic regression model trained on undersampled data, we cross-validated the model using the original (imbalanced) training data to get a true view of its performance. We saw that our true performance metrics were lower than when we cross-validated on the undersampled data, especially precision (which made sense, as the imbalanced data consists of far more safe transactions with the potential to be misclassified as fraud).\n\nUsing our final test data, we looked at our model's precision-recall curve. We found relatively low scores overall, further indicating that our model did not do a great job of keeping safe transactions from being missclassified as the decision boundary was relaxed. \n\n\n## Logistic Regression Performance - Oversampling (SMOTE)\nNext, we went back to the drawing board and trained logistic regression on oversampled data using the SMOTE technique. We optimized hyperparameters with RandomizedSearchCV this time, due to the large size of the training data. Upon cross-validating with the original (imbalanced) training data, we saw significantly higher precision and accuracy than from our previous logistic regression model trained on undersampled data. The reason for this is that we don't lose information about safe transction when oversampling (SMOTE), thus we can better classify them.\n\nWe saw that while both models performed similarly on the undersampled (balanced) test data, the oversampled logistic regression model produced much higher accuracy on the final (imbalanced) test data. Furthermore, the oversampled logistic regression model far outperformed the undersampled model in terms of precision-recall.\n\n\n## Neural Network\nFinally, we put our baseline model aside, and investigated the performance of neural networks. We built two simple NNs: the first was trained on the undersampled data, the second was trained on the oversampled data.\n\nAfter evaluating both NNs on our original test data, we found that the oversampled NN also outperformed the undersampled NN (assuming 1 fraud is less important than ~150 blocked accounts).\n\n\n## Key Takeaways \/ Next Steps\nBalancing our dataset using oversampling (SMOTE) helped us improve our logistic regression and Neural Network models over random undersampling. While it's possible that undersampling could outperform oversampling in certain scenarios (e.g. depending on the tradeoff between false positives and false negatives), in general oversampling preserves more of the original data, and thus models trained with oversampling perform more accurate classification. Of course, the tradeoff with oversampling is that it uses more time\/resources, given it consists of more training data.\n\nNext Steps: Recall that we removed outliers from our undersampled data before training our models. We should also do the same for our oversampled data to see if our performance improves further.","ab2be09d":"Now let's print our performance metrics for both cross-validation approaches (balanced vs imbalanced data) to see how they compare.","9c80ae47":"Great! We successfully removed the most extreme outliers from our most highly-correlated features.","4753af87":"Great! As we can see, balancing our data gives very different (true) correlation values. Referencing the balanced output, we see fraud is most highly correlated with the following features:\n\n**Negative Correlations**: V14, V12, V10\n\n**Positive Correlations**: V11, V4\n\nLet's view these features a bit more closely...","6215a89d":"## Undersampled NN","4149efa2":"Awesome! SMOTE oversampling gives much higher accuracy overall, as well as a higher precision and f1 score for fraud transactions. Additionally, as we can see in our precision-recall graph directly above, training with SMOTE resulted in a much better precision-recall score overall. In comparison to our undersampling precision-recall curve, we now let far fewer safe transactions in (i.e. missclassifying them) as we relax our decision boundary. Clearly, the information loss through undersampling negatively affected the model's ability to correctly classify safe transactions.","fc4e027e":"Awesome! The undersampled NN performed pretty well, with recall and precision similar to what we saw from our oversampled logistic regression model earlier.\n\nLet's see how our performance changes when we train our NN using oversampled data.\n\n## Oversampled NN (SMOTE)","0fc6bb78":"Great! Now, let's train this network on the undersampled data, then we'll predict on the final test data.","3aa2ecb0":"# Exploratory Data Analysis & Preprocessing\nFirst, let's take a high-level view of our data to get a good sense of what we're working with. In particular, let's look at summary statistics for each feature, the balance between fraud & safe transactions, and null values.","017d0abc":"Awesome! Per our output, both our train & test populations have 99.83% safe and 0.17% fraud transactions.","99f5acc5":"The anonymous **V__** features tend to exhibit various distributions and ranges. Overall, values across all features fall within the range (-114, 121). Note we're using a logarithmic y-axis to help visualize the data more clearly.\n\nLet's also look at the distributions for **Time** & **Amount**:","13c8dbf8":"# Logistic Regression: Undersampling (NearMiss) Performance\n\nFirst, let's come back to our previous conversation on the proper way to cross-validate for estimating true performance. So far, we've only cross-validated using balanced\/undersampled data. Let's now cross-validate logistic regression using the original, imbalanced data.\n\nRecall that we still have to train our model on balanced data. To do this, we'll implement the NearMiss() algorithm, which is an undersampling technique that works by randomly removing samples of the majority class that are close to samples of the minority class, thus clarifying the boundary between both classes and improving model classification.","de70ae13":"Awesome! The oversampled NN performed very well, with high f1 scores on both safe and fraud transactions, and exceptionally high fraud precision.\n\nWhile the oversampled NN miscalssified a handful more fraud transactions than the undersampled NN, the undersampled NN missclassified almost 200x more safe transactions. It's important to recognize that the cost of missclassifying a safe transaction is non-zero. Missclassifying a safe transaction blocks the cardholder from making additional transactions until that cardholder can verfiy their account, which costs the cardholder and the financial institution. Overall, it's probably safe to assume that the cost of missclassifying 1 fraud transactions does not outweight the cost of blocking ~150 safe transactions. Thus, we assume the oversampled NN is the best model overall.","7df0b598":"Out of curiosity, let's compare how our logistic regression models trained on undersampled data (log_reg) vs oversampled data (log_reg_sm) perform on the undersampled test data (X_test & y_test).","d7026218":"Success! (Note the change in scale on the x-axis)\n\nNow let's split our data into our final train and test populations. While we will train & cross-validate our models on various samples of the the training data (**final_Xtrain, final_ytrain** - more on this later), we will preserve this test data (**final_Xtest, final_ytest**) to evaluate the final performance of our models at the end of our analysis. Note that we make sure to stratify by our target variable, **Class**, in order to ensure both populations are representative.","f3311723":"Great! We don't appear to be overfitting given the lack of a large gap between curves as training size increases, and we don't appear to be underfitting given how high our scores are.\n\nLet's take a look at the Receiver Operating Characteristic (ROC) curve for each of our models, to get a better sense of their performance. ROC curves are used to show in a graphical way the connection\/trade-off between False Positive Rate (the percent of safe transactions our model classifies incorrectly) and True Positive Rate (the percentage of fraud transactions our model classifies correctly) for every possible decision boundary in our models. In other words, it shows the tradeoff between our incorrect classification of safe transactions and our correct classification of fraud transactions as we move through every possible decision boundary.\n\nTo evaluate model performance, we'll calculate the area under the ROC curve (AUC). This tells us which model does the best job of distinguishing between fraud and safe transactions overall.","624ab71e":"# Baseline Classification Models\nWe're finally ready to create our classification models! We'll start with four types of classifiers: **Logistic Regression**, **K Nearest Neighbors**, **SVC**, and **Decision Tree**. \n\nFirst, we'll train and cross-validate our models on the **txn_under** dataset to get a sense of which model does the best job of recognizing fraud. Once we have an opinion on which model does best, we'll cross-validate that model on the unsampled (original) data to get an objective view of its true performance.\n\n#### A Note on Cross-Validating Imbalanced Data:\nBefore we proceed further, let's digress for a moment to talk about a common mistake made when dealing with imbalanced datasets. While we should train our models on balanced data, we should cross-validate on imbalanced (i.e. original) data to best evaluate the objective performance of our trained models. Cross-validating on the original data gives us the most accurate view of our model's performance in production, because the original data preserves and is representative of the imbalance we would expect to see in production.\n\nOur initial goal here, however, isn't to determine objective performance, but rather to determine which model does the best job of identifying the characteristics of fraud. In this case, it is okay for us to cross-validate with our undersampled data, as long as we are consistent in our approach across all models. We will still be able to rank models' abilities to identify fraud by cross-validating on undersampled data, even if the scores we derive are not representative of what we would expect to see in production.\n\nLet's start by splitting our undersampled data, **txn_under**, into train and test sets:","a559f529":"Fantastic! We see a clear separation between fraud & safe transactions in all 3 graphs, especially in PCA and Truncated SVD! This gives us a good indication that our predictive models will be able to effectively classify fraud.","eeab157c":"Despite the fact that these scores are not representative of the true production accuracy, we do see that all four models do a very good job (~90% or more) of identifying fraud... which is good, and inline with what we might expect given what we saw from our dimensionality reduction earlier. Let's see how much we can improve these scores further by optimizing our hyperparameters with GridSearchCV. Again, we'll perform cross-validation on the balanced data so that we can compare to the baseline scores above:","efd19b65":"# Random Undersampling\nNow that we've explored and scaled our data, we can focus on \"balancing\" our data.\n\nBalancing our data means resampling our data such that we have a 50\/50 split between fraud and safe transactions (as we just saw, we're pretty far from that at the moment with 99.83% safe vs 0.17% fraud). It is import that we create this 50\/50 split before training our models because otherwise our models will overfit to the majority class, assuming that essentially all transactions are safe. We want to train our model to recognize the characteristics of fraud, not assume that most transactions aren't fraud. Thus we must create a 50\/50 split to remove the effect of the imbalance during training.\n\nLet's consider two ways to create this 50\/50 split:\n\n**Random Undersampling**:  Here, we reduce the number of transactions in the majority class (i.e. safe transactions) randomly, until the counts of both the majority and minority classes are equal.\n\n**SMOTE Oversampling**: Here, we increase the number of transactions in the minority class (i.e. fraud transactions) through creating synthetic transactions in between existing ones that are in close proximity. Synthetic transactions continue to be created until the counts of both the majority and minority classes are equal.\n\nWe will perform both sampling approaches to determine which performs better. Let's start with random undersampling:"}}