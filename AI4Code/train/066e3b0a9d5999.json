{"cell_type":{"3d04976f":"code","00e2d7db":"code","997bf9e6":"code","e0371846":"code","9d5663ad":"code","2cc3339f":"code","ab459009":"code","6deeebb8":"code","a4eb7f69":"markdown","cfb24d6e":"markdown","b1193a55":"markdown","271d0d21":"markdown","2d3ca5fe":"markdown","d032feec":"markdown","7a893b67":"markdown"},"source":{"3d04976f":"import pandas as pd\nimport numpy as np\nimport tensorflow as tf\nnp.random.seed(123)\ntf.random.set_seed(123)\n\npath = \"..\/input\/ml2021spring-hw1\/\"\ntrain_file = path + \"covid.train.csv\"\ntest_file = path + \"covid.test.csv\"\n\ndef read_file(filename,mode,dayType):\n    df = pd.read_csv(filename)\n    cols = df.columns\n    state_cols = list(cols[1:41])\n    day1_cols = list(cols[41:59])\n    day2_cols = list(cols[59:77])\n    day3_cols = list(cols[77:])\n    res = []\n    if dayType == 1:\n        train_cols = state_cols + day1_cols\n        day1_df = df[state_cols+day1_cols]\n        day2_df = df[state_cols+day2_cols]\n        day3_df = df[state_cols+day3_cols]\n        day1_df.columns = train_cols\n        day2_df.columns = train_cols\n        if(\"test\" == mode):\n            day3_df.columns = train_cols[:-1]\n            train_df = pd.concat([day1_df,day2_df])\n            res.append(train_df)\n            res.append(day3_df)\n        elif(\"train\" == mode):\n            day3_df.columns = train_cols\n            train_df = pd.concat([day1_df,day2_df,day3_df])\n            res.append(train_df)\n    elif dayType == 2:\n        train_cols = state_cols + day1_cols + day2_cols\n        day1_df = df[state_cols+day1_cols+day2_cols]\n        day2_df = df[state_cols+day2_cols+day3_cols]\n        day1_df.columns = train_cols\n        if(\"test\" == mode):\n            day2_df.columns = train_cols[:-1]\n            res.append(day1_df)\n            res.append(day2_df)\n        elif(\"train\" == mode):\n            day2_df.columns = train_cols\n            train_df = pd.concat([day1_df,day2_df])\n            res.append(train_df)\n    elif dayType == 3:\n        train_cols = state_cols + day1_cols + day2_cols + day3_cols\n        day1_df = df[train_cols]\n        if(\"test\" == mode):\n#             select_cols = ['cli', 'ili', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'tested_positive', 'cli.1', 'hh_cmnty_cli.1', 'nohh_cmnty_cli.1', 'tested_positive.1', 'cli.2', 'hh_cmnty_cli.2', 'nohh_cmnty_cli.2']\n#             day1_df = df[state_cols+select_cols]\n            res.append(pd.DataFrame())\n            res.append(day1_df)\n        elif(\"train\" == mode):\n#             select_cols = ['cli', 'ili', 'hh_cmnty_cli', 'nohh_cmnty_cli', 'tested_positive', 'cli.1', 'hh_cmnty_cli.1', 'nohh_cmnty_cli.1', 'tested_positive.1', 'cli.2', 'hh_cmnty_cli.2', 'nohh_cmnty_cli.2','tested_positive.2']\n#             day1_df = df[state_cols+select_cols]\n            res.append(day1_df)\n    return res\n\ndef read_train_test(train_file,test_file,dayType):\n    train_res = read_file(train_file,\"train\",dayType)\n    test_res = read_file(test_file,\"test\",dayType)\n    train_df = pd.concat([train_res[0],test_res[0]])\n    return train_df,test_res[1]\n\ntrain_1d_df,test_1d_df = read_train_test(train_file,test_file,1)\ntrain_2d_df,test_2d_df = read_train_test(train_file,test_file,2)\ntrain_3d_df,test_3d_df = read_train_test(train_file,test_file,3)","00e2d7db":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\ndef scale_fea(sc,features,mode):\n    state = features.values[:,:40]\n    other_fea = features.values[:,40:]\n    if('train' == mode):\n        other_fea = features.values[:,40:]\n        sc_other_fea = sc.fit_transform(other_fea)\n    else:\n        sc_other_fea = sc.transform(other_fea)\n    res = np.hstack((state,sc_other_fea))\n    return res\n\ndef gen_feature(train_df):\n    train_df = train_df.sample(frac=1).reset_index(drop=True)\n    l_col = train_df.columns[-1]\n    labels = train_df[l_col]\n    features = train_df.drop(columns=[l_col])\n    train_x,test_x,train_y,test_y = train_test_split(features,labels,test_size = 0.1)\n    sc = StandardScaler()\n    train_x = scale_fea(sc,train_x,'train')\n    test_x = scale_fea(sc,test_x,'test')\n    return train_x,train_y,test_x,test_y,sc\n\ntrain_1d_x,train_1d_y,test_1d_x,test_1d_y,sc_1d = gen_feature(train_1d_df)\ntrain_2d_x,train_2d_y,test_2d_x,test_2d_y,sc_2d = gen_feature(train_2d_df)\ntrain_3d_x,train_3d_y,test_3d_x,test_3d_y,sc_3d = gen_feature(train_3d_df)","997bf9e6":"from keras.models import Sequential,load_model\nfrom keras.layers import Dense,Dropout\nfrom keras.optimizers import Adam,SGD\nfrom keras.callbacks import EarlyStopping,ModelCheckpoint\nfrom keras import regularizers\ndef build_model(featureLen,dayType):\n    if dayType == 1:\n        network = Sequential()\n        \"\"\"\n        network.add(Dense(128,activation='relu',input_shape=(featureLen,)))\n        network.add(Dense(64,activation='relu'))\n        network.add(Dense(32,activation='relu'))\n        network.add(Dense(1))\n        opt = Adam(learning_rate = 0.0001)\n        \"\"\"\n        network.add(Dense(128,activation='relu',input_shape=(featureLen,)))\n        network.add(Dense(64,activation='relu'))\n        network.add(Dense(32,activation='relu'))\n        network.add(Dense(1))\n        opt = Adam(learning_rate = 0.0005)\n        network.compile(loss='mse',optimizer=opt,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    elif dayType == 2:\n        network = Sequential()\n        \"\"\"\n        network.add(Dense(128,activation='relu',input_shape=(featureLen,)))\n        network.add(Dense(64,activation='relu'))\n        network.add(Dense(32,activation='relu'))\n        network.add(Dense(1))\n        opt = Adam(learning_rate = 0.0001)\n        \"\"\"\n        network.add(Dense(128,activation='relu',input_shape=(featureLen,)))\n        network.add(Dense(64,activation='relu'))\n        network.add(Dense(32,activation='relu'))\n        network.add(Dense(1))\n        opt = Adam(learning_rate = 0.0005)\n        network.compile(loss='mse',optimizer=opt,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    elif dayType == 3:\n        network = Sequential()\n        \"\"\"\n        network.add(Dense(32,activation='relu',input_shape=(featureLen,)))\n        network.add(Dense(16,activation='relu'))\n        network.add(Dense(8,activation='relu'))\n        network.add(Dense(4,activation='relu'))\n        network.add(Dense(2,activation='relu'))\n        network.add(Dense(1))\n        opt = Adam(learning_rate = 0.0002)\n        \"\"\"\n#         network.add(Dense(32,activation='relu',input_shape=(featureLen,),kernel_regularizer=regularizers.l2(1e-5)))\n#         network.add(Dense(4,activation='relu',kernel_regularizer=regularizers.l2(1e-5)))\n#         network.add(Dense(32,activation='relu',kernel_regularizer=regularizers.l1_l2(l1=1e-5, l2=1e-4)))\n#         network.add(Dense(1))\n#         opt = Adam(learning_rate = 0.0001)\n        network.add(Dense(256,activation=tf.nn.silu,kernel_regularizer=regularizers.l2(1e-4),input_shape=(featureLen,)))\n        network.add(Dropout(0.5))\n        network.add(Dense(128,activation=tf.nn.silu,kernel_regularizer=regularizers.l2(1e-4)))\n        network.add(Dropout(0.5))\n        network.add(Dense(1))\n        opt = SGD(learning_rate=0.001,momentum=0.8)\n        network.compile(loss='mse',optimizer=opt,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return network\n\ndef train_model(train_x,train_y,valid_size,featureLen,dayType,model_path,epoch,batch,patience,train_type='train'):\n    network = build_model(featureLen,dayType)\n    valid_x = train_x[:valid_size]\n    partial_train_x = train_x[valid_size:]\n    valid_y = train_y[:valid_size]\n    partial_train_y = train_y[valid_size:]\n    es = EarlyStopping(monitor='val_root_mean_squared_error',mode='min',verbose=1,patience=patience)\n    ckpt = ModelCheckpoint(model_path,monitor='val_root_mean_squared_error',save_best_only=True,verbose=1,mode='min')\n    if('train' == train_type):\n        history = network.fit(partial_train_x,partial_train_y,epochs=epoch,batch_size=batch,validation_data=(valid_x,valid_y),callbacks=[es,ckpt]) #\n    elif('total' == train_type):\n        history = network.fit(train_x,train_y,epochs=epoch,batch_size=batch,validation_data=(valid_x,valid_y),callbacks=[es,ckpt])#,callbacks=[es]\n    return network,history\n\ndef train_day_model(train_x,train_y,test_x,test_y,dayType,model_path,epoch,batch,patience,train_type='train'):\n    valid_size = int(train_x.shape[0]\/10)\n    featureLen = train_x.shape[1]\n    network,history = train_model(train_x,train_y,valid_size,featureLen,dayType,model_path,epoch,batch,patience,train_type)\n    test_loss, test_rmse = network.evaluate(test_x, test_y)\n    print('test_loss:%f,test_rmse:%f'% (test_loss,test_rmse))\n    return network,history\n\n\nmodel_1d_path = \"\/kaggle\/working\/model_1d\/\"\nmodel_2d_path = \"\/kaggle\/working\/model_2d\/\"\nmodel_3d_path = \"\/kaggle\/working\/model_3d\/\"\nepoch = 5000\nbatch = 200\npatience = 300\nnetwork_1d,history_1d = train_day_model(train_1d_x,train_1d_y,test_1d_x,test_1d_y,1,model_1d_path,epoch,batch,patience)\nnetwork_2d,history_2d = train_day_model(train_2d_x,train_2d_y,test_2d_x,test_2d_y,2,model_2d_path,epoch,batch,patience)\nnetwork_3d,history_3d = train_day_model(train_3d_x,train_3d_y,test_3d_x,test_3d_y,3,model_3d_path,epoch,batch,patience)","e0371846":"import matplotlib.pyplot as plt\ncolorMap = {1:'r',2:'g',3:'b'}\n\ndef plot_history(history,dayType):\n    history_dict = history.history\n    loss = history_dict['loss']\n    val_loss = history_dict['val_loss']\n    mae = history_dict['root_mean_squared_error']\n    val_mae = history_dict['val_root_mean_squared_error']\n    epochs = range(1,len(loss)+1)\n    \n    n = 50\n    end = 2000\n    plt.subplot(1,2,1)\n    plt.plot(epochs[n:end],loss[n:end],'bo',c=colorMap[dayType],label=str(dayType)+' train loss')\n    plt.plot(epochs[n:end],val_loss[n:end],'b',c=colorMap[dayType],label=str(dayType)+' valid loss')\n    plt.legend()\n    plt.subplot(1,2,2)\n    plt.plot(epochs[n:end],mae[n:end],'bo',c=colorMap[dayType],label=str(dayType)+' train root_mean_squared_error')\n    plt.plot(epochs[n:end],val_mae[n:end],'b',c=colorMap[dayType],label=str(dayType)+ ' valid root_mean_squared_error')\n    \nplt.figure(figsize=(15,5))\nplot_history(history_1d,1)\nplot_history(history_2d,2)\nplot_history(history_3d,3)\nplt.legend()\nplt.show()","9d5663ad":"def gen_merge_feature(train_3d_df):\n    part_d1_cols = cols[:40]+cols[-18:-1]\n    part_d2_cols = cols[:40]+cols[-36:-1]\n    part_d1_df = train_3d_df[part_d1_cols]\n    part_d2_df = train_3d_df[part_d2_cols]\n    part_d3_df = train_3d_df[cols[:-1]]\n    sc_d1_feature = scale_fea(sc_1d,part_d1_df,'test')\n    sc_d2_feature = scale_fea(sc_2d,part_d2_df,'test')\n    sc_d3_feature = scale_fea(sc_3d,part_d3_df,'test')\n    pred_1d = network_1d.predict(sc_d1_feature)\n    pred_2d = network_2d.predict(sc_d2_feature)\n    pred_3d = network_3d.predict(sc_d3_feature)\n    merge_feature = np.hstack([pred_1d,pred_2d,pred_3d])\n    return merge_feature\n\ncols=list(train_3d_df.columns)\nmerge_feature = gen_merge_feature(train_3d_df)\nmerge_labels = train_3d_df[cols[-1]]\ntrain_merge_x,test_merge_x,train_merge_y,test_merge_y = train_test_split(merge_feature,merge_labels,test_size = 0.1)\nprint(train_merge_x.shape)","2cc3339f":"def build_merge_model():\n    network = Sequential()\n    network.add(Dense(16,activation='relu',input_shape=(3,))) #activation='relu',\n#     network.add(Dense(4,activation='relu',kernel_regularizer=regularizers.l2(1e-5)))\n    network.add(Dense(1))\n    opt = Adam(learning_rate = 0.0001)\n    network.compile(loss='mse',optimizer=opt,metrics=[tf.keras.metrics.RootMeanSquaredError()])\n    return network\n\ndef train_merge_model(train_x,train_y,test_x, test_y,model_path,epoch,batch,patience,train_type='train'):\n    network = build_merge_model()\n    valid_size = int(train_x.shape[0]\/10)\n    valid_x = train_x[:valid_size]\n    partial_train_x = train_x[valid_size:]\n    valid_y = train_y[:valid_size]\n    partial_train_y = train_y[valid_size:]\n    es = EarlyStopping(monitor='val_root_mean_squared_error',mode='min',verbose=1,patience=patience)\n    ckpt = ModelCheckpoint(model_path,monitor='val_root_mean_squared_error',save_best_only=True,verbose=1,mode='min')\n    if('train' == train_type):\n        history = network.fit(partial_train_x,partial_train_y,epochs=epoch,batch_size=batch,validation_data=(valid_x,valid_y),callbacks=[es,ckpt]) #\n    elif('total' == train_type):\n        history = network.fit(train_x,train_y,epochs=epoch,batch_size=batch,validation_data=(valid_x,valid_y),callbacks=[es,ckpt])#,callbacks=[es]\n    test_loss, test_rmse = network.evaluate(test_x, test_y)\n    print('test_loss:%f,test_rmse:%f'% (test_loss,test_rmse))\n    return network,history\n\nmodel_merge_path = \"\/kaggle\/working\/model_merge\/\"\nmerge_epoch = 500\nmerge_batch = 10\nmerge_patience = 50\nnetwork_merge,history_merge = train_merge_model(train_merge_x,train_merge_y,test_merge_x,test_merge_y,model_merge_path,epoch,batch,patience)","ab459009":"pred_merge = network_merge.predict(test_merge_x)\ndiff = pred_merge.reshape(1,-1)[0] - test_merge_y.values\nweights = network_merge.get_weights()\nprint(np.mean(abs(diff)),np.var(diff))\nprint(weights)\nplt.figure(figsize=(15,5))\nplot_history(history_merge,1)","6deeebb8":"test_orig_file = \"..\/input\/ml2021spring-hw1\/covid.test.csv\"\nres_file = \"\/kaggle\/working\/submit.csv\"\ntest_feature = gen_merge_feature(test_3d_df)\npred = network_merge.predict(test_feature)\ntest_orig_df = pd.read_csv(test_orig_file)\nids = test_orig_df.id.values\nwith open(res_file,'w') as f_out:\n    f_out.write(\"id,tested_positive\\n\")\n    for i in range(len(ids)):\n        outline = str(ids[i])+\",\"+str(pred[i][0])+\"\\n\"\n        f_out.write(outline)\nprint(\"end\")","a4eb7f69":"## \u7ed8\u5236\u66f2\u7ebf","cfb24d6e":"### \u6a21\u578b\u8bad\u7ec3","b1193a55":"## \u8bfb\u53d6\u6570\u636e\n1.\u6bcf\u6761\u8bad\u7ec3\u6837\u672c\u753195\u5217\u6784\u6210\uff0c\u542b\u4e49\u5982\u4e0b:  \n\\[0\\]\uff1a\u6837\u672cid\uff0c\u65e0\u6709\u6548\u4fe1\u606f\uff0c\u53ef\u5220\u9664  \n\\[1,40\\]\uff1a\u6240\u5728\u5dde\u7684one-hot\u8868\u793a  \n\\[41,58\\]\uff1a2d\u524d\u7684\u7279\u5f81  \n\\[59,76\\]\uff1a1d\u524d\u7684\u7279\u5f81  \n\\[77,95\\]\uff1a\u5f53\u5929\u7279\u5f81  \n2.\u5c06\u6837\u672c\u91cd\u65b0\u7ec4\u5408\u62101d\u30012d\u30013d\u4e09\u79cd\u6570\u636e\u96c6\uff0c\u6570\u636e\u7ef4\u5ea6\u5982\u4e0b:  \n\nday | train | pred\n  - | - | -  \n1    |(9886, 58)|(893, 57)|  \n2    |(6293, 76)|(893, 75)|  \n3    |(2700, 94)|(893, 93)|\n","271d0d21":"## \u6a21\u578b\u878d\u5408  \n### \u6784\u9020\u6837\u672c","2d3ca5fe":"## \u7279\u5f81\u5de5\u7a0b  \n\u7279\u5f81\u5f52\u4e00\u5316\uff1a\u5c06\u975eone-hot\u7279\u5f81\u7f29\u653e\u81f30~1\u8303\u56f4  \n\u6837\u672cshuffle\uff1a\u539f\u59cb\u6570\u636e\u4e2d\u6837\u672c\u6309\u5dde\u6392\u5e8f\uff0c\u9700\u8981\u5c06\u6837\u672c\u968f\u673a\u6253\u6563  \n\u6570\u636e\u62c6\u5206\uff1a\u5c06\u6570\u636e\u96c6\u62c6\u5206\u4e3a\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\uff0c\u5176\u4e2d\u6d4b\u8bd5\u96c6\u5360\u6bd410%  \n\nday | train | test\n  - | - | -  \n1    |(8897, 57)|(989, 57)|  \n2    |(5663, 75)|(630, 75)|  \n3    |(2430, 93)|(270, 93)|","d032feec":"### \u6548\u679c\u68c0\u67e5","7a893b67":"## \u6a21\u578b\u6784\u9020\u4e0e\u8bad\u7ec3  \n\u4f7f\u7528dnn\u4e3a\u4e09\u79cd\u6570\u636e\u96c6\u6784\u5efa\u4e09\u4e2a\u4e0d\u540c\u7684\u6a21\u578b,\u6d4b\u8bd5\u7ed3\u679c\u5982\u4e0b \n\nday | loss | rmse\n  - | - | -  \n1    |0.531236|0.728859|  \n2    |0.497631|0.705429|  \n3    |0.912615|0.955309|  \n\n  \n  \n"}}