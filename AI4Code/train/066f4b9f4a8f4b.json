{"cell_type":{"95bec713":"code","eb386b25":"code","8684017f":"code","73e83cd0":"code","d11f6bc4":"code","de068ab4":"code","4d740919":"code","8ae8dc0c":"code","7401f968":"code","2027b7ca":"code","e9bc6eef":"code","f11bae65":"code","1f2aef2d":"code","c7f2a344":"code","ae3fdfaf":"code","c37e28bb":"code","51228244":"code","70b3cfe8":"code","5f1c37a5":"code","a0009622":"code","3040adb3":"code","562ff6ca":"code","635ed652":"code","ec2e9e90":"markdown","6f63f016":"markdown","ac9ea836":"markdown","bd3fdf24":"markdown","960b6df9":"markdown"},"source":{"95bec713":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt","eb386b25":"df=pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","8684017f":"df.head(10)","73e83cd0":"df.describe()","d11f6bc4":"df.Class.value_counts(normalize=True)","de068ab4":"df.dtypes.value_counts()","4d740919":"col_miss=[var for var in df.columns if df[var].isnull().any()]","8ae8dc0c":"# correlation\nabs(df.corr()['Class']).sort_values(ascending=False).head(5)","7401f968":"# Spearman correlation\nabs(df.corr(method='spearman')['Class']).sort_values(ascending=False).head(5)","2027b7ca":"from sklearn.feature_selection import SelectKBest,chi2,f_classif\n\nX=df.drop(['Class','Time'],axis=1)\ny=df['Class']\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\n#concat two dataframes for better visualization \nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nprint(featureScores.nlargest(10,'Score'))  #print 10 best features","e9bc6eef":"from sklearn.ensemble import ExtraTreesClassifier\nimport matplotlib.pyplot as plt\nmodel = ExtraTreesClassifier()\nmodel.fit(X,y)\nprint(model.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model.feature_importances_, index=X.columns)\nfeat_importances.nlargest(10).plot(kind='barh')\nplt.show()","f11bae65":"plt.figure(figsize=(12,6))\nplt.subplot(2,2,1)\nsns.boxplot(df.Class,df.V17)\nplt.subplot(2,2,2)\nsns.boxplot(df.Class,df.V4)\nplt.subplot(2,2,3)\nsns.boxplot(df.Class,df.V14)\nplt.subplot(2,2,4)\nsns.boxplot(df.Class,df.V14)\nplt.tight_layout()","1f2aef2d":"sns.distplot(df[df['Class']==0]['Amount'])\nsns.distplot(df[df['Class']==1]['Amount'])","c7f2a344":"sns.distplot(df[df['Class']==0]['Time'],color='red',hist=False)\nsns.distplot(df[df['Class']==1]['Time'],color='blue',hist=False)","ae3fdfaf":"fig, ax = plt.subplots(1, 2, figsize=(12,6))\n\nsns.distplot(df.Amount,  ax=ax[0])\nsns.distplot(df.Time,  ax=ax[1])","c37e28bb":"#Training and Testing Data Split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix, auc,roc_auc_score","51228244":"x=df.drop('Class',axis=1)\nlist=['V17','V14','V12','V16','V10']\nx_selected=df[list]\ny=df.Class","70b3cfe8":"x_train,x_test,y_train,y_test=train_test_split(x_selected,y,test_size=0.2)","5f1c37a5":"#Without SMOTE\nxg_model=XGBClassifier()\nxg_model.fit(x_train,y_train)","a0009622":"xg_pred=xg_model.predict(x_test)\nxg_pred","3040adb3":"lg_model=LogisticRegression()\nlg_model.fit(x_train,y_train)\nlg_pred=lg_model.predict(x_test)","562ff6ca":"print(confusion_matrix(y_test,xg_pred))\nprint('\\n')\nprint('XGboost AUC: ',roc_auc_score(y_test,xg_pred))\nprint('Logistic AUC: ', roc_auc_score(y_test,lg_pred))","635ed652":"#SMOTE\nfrom imblearn.over_sampling import SMOTE\noversample = SMOTE()\nX=df.iloc[:,:-2].values\ny=df.Class.values\n\nX, y=oversample.fit_resample(X,y)\ndf_oversamper=pd.DataFrame(X)\ndf_oversamper['Class']=y","ec2e9e90":"**EDA**","6f63f016":"**All variables included:**\nXGboost AUC:  0.8832981666432221\nLogistic AUC:  0.8330695831574998\n\n\n**Only important variables included:**\nXGboost AUC:  0.907520775835047\nLogistic AUC:  0.8096858968968126\n\nMuch improved","ac9ea836":"## Feature Selection\n\n1. Univariate Selection\n2. Feature Importance\n3.Correlation Matrix with Heatmap","bd3fdf24":"## EDA","960b6df9":"## Library Loading"}}