{"cell_type":{"286a3521":"code","6577301d":"code","d75ea540":"code","46eb54c9":"code","bb89d501":"code","f9586932":"code","c408c2b3":"code","08fdb051":"code","2ff6b389":"code","81acd916":"code","0264d228":"code","d89cab0f":"code","49b97809":"code","0bce3541":"code","c730e2a2":"code","885d9bb5":"code","474758ba":"code","e50174cc":"code","42ea5a9e":"code","a71fd540":"code","a53d49f1":"code","1ea9f7f8":"code","dbd6c5ef":"code","d227d9cf":"code","e6428303":"code","50d10ab1":"code","25c4b640":"code","1c71fe1c":"code","7c8e81c6":"code","afe6a898":"code","506b2f56":"markdown","0296f639":"markdown","2b3c3996":"markdown","10a8d57d":"markdown","99408529":"markdown"},"source":{"286a3521":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","6577301d":"data = pd.read_csv('..\/input\/mengary-revenue-prediction\/train.csv', parse_dates=['delivery date', 'placement date'])\ndata.head()","d75ea540":"data.info()","46eb54c9":"data.describe()","bb89d501":"# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler().fit(data[['id', 'discount', 'price', 'no of items', 'RID']])\n# data_norm = scaler.transform(data[['id', 'discount', 'price', 'no of items', 'RID']])","f9586932":"# data = data.drop(columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# data_norm = pd.DataFrame(data_norm, columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# data = pd.concat([data_norm, data], axis=1)\n# data.head()","c408c2b3":"# # Removing outlier \/\/ Score is not improving\n# plt.figure(figsize=(15, 10))\n# data = data[data.price < 14000]\n# data['price'].plot()\n# plt.show()","08fdb051":"from sklearn.metrics import r2_score\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, StackingRegressor, BaggingRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n# from xgboost import XGBRegressor\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder\nfrom sklearn.neighbors import LocalOutlierFactor","2ff6b389":"# Handling \"delivery date\" and \"placement date\"\ndata['delivery_day'] = data['delivery date'].dt.day\ndata['delivery_month'] = data['delivery date'].dt.month\ndata['placement_day'] = data['placement date'].dt.day\ndata['placement_month'] = data['placement date'].dt.month\n\n# One hot Encoding the \"Segment\" column\ndummy_seg = pd.get_dummies(data['segment'], prefix='seg')\ndata.drop(columns=['segment'], inplace=True)\ndata = pd.concat([data, dummy_seg], axis=1)\n\n# Label Encoding of the \"delivery type\" column\ndata['delivery type'] = data['delivery type'].fillna('Standard Class')\nlabel_encoder = LabelEncoder().fit(data['delivery type'])\ndata['delivery_type'] = label_encoder.transform(data['delivery type'])\n\n# One hot Encoding the \"sub-class\" column\ndummy_type = pd.get_dummies(data['sub-class'], prefix='sclass')\ndata.drop(columns=['sub-class'], inplace=True)\ndata = pd.concat([data, dummy_type], axis=1)\n\n# One hot Encoding the \"class\" column\ndummy_type = pd.get_dummies(data['class'], prefix='class')\ndata.drop(columns=['class'], inplace=True)\ndata = pd.concat([data, dummy_type], axis=1)","81acd916":"data['new_price'] = data['price'] - data['price']*data['discount']\ndata.info()","0264d228":"data.columns","d89cab0f":"# correlation = data.corr()","49b97809":"# plt.figure(figsize=(20,10))\n# sns.heatmap(correlation, annot=True, fmt='.2f', linewidths=0.5)\n# plt.show()","0bce3541":"num_cols = ['id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'new_price', 'delivery_type', 'sclass_battery', 'sclass_charger', 'sclass_chocolates', 'sclass_colddrinks', 'sclass_fastfood', 'sclass_headset', 'sclass_hoodies', 'sclass_laptop', 'sclass_lighting', 'sclass_pants', 'sclass_phone', 'sclass_shorts', 'sclass_sweets', 'sclass_tablet', 'sclass_television', 'sclass_tshirts',   'sclass_watch', 'class_fynota', 'class_kariox', 'class_qexty', 'delivery_day', 'delivery_month', 'placement_day', 'placement_month']\n\nfeatures = data[num_cols]\ntarget = data['profit']\n\n# # Removing outlier \/\/ score is not improving\n# features = features.to_numpy()\n# lof = LocalOutlierFactor(n_neighbors=2)\n# yhat = lof.fit_predict(features)\n# mask = yhat != -1\n# features, target = features[mask, :], target[mask]\n\nX_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.25, random_state=42)\n\nprint('Shape of train set:', X_train.shape)\nprint('Shape of test set:', X_test.shape)","c730e2a2":"# # Standardization\n# from sklearn.preprocessing import StandardScaler\n\n# scaler = StandardScaler().fit(X_train)\n# X_train_norm = scaler.transform(X_train)\n# X_test_norm = scaler.transform(X_test)","885d9bb5":"# X_train = X_train.drop(columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# X_train_norm = pd.DataFrame(X_train_norm, columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# X_train = pd.concat([X_train_norm, X_train], axis=1)\n# X_train.head()","474758ba":"# X_test = X_test.drop(columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# X_test_norm = pd.DataFrame(X_test_norm, columns=['id', 'discount', 'price', 'no of items', 'RID'])\n# X_test = pd.concat([X_test_norm, X_test], axis=1)\n# X_test.head()","e50174cc":"X_test.info()","42ea5a9e":"# # get a stacking ensemble of models \/\/ not performing well on leaderboard\n# def get_stacking():\n# \t# define the base models\n# \tlevel0 = list()\n# \tlevel0.append(('br', BaggingRegressor(ExtraTreesRegressor(), random_state=0)))\n# \tlevel0.append(('rfr', RandomForestRegressor()))\n# \tlevel0.append(('etr', ExtraTreesRegressor(random_state=0)))\n# \tlevel0.append(('xgbr', XGBRegressor()))\n# \t# define meta learner model\n# \tlevel1 = ExtraTreesRegressor()\n# \t# define the stacking ensemble\n# \tmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n# \treturn model\n\n# model = get_stacking()\n# model.fit(X_train, y_train)\n# y_pred = model.predict(X_test)","a71fd540":"# regressor = ExtraTreesRegressor(n_estimators=5000, max_features=30, min_samples_split=3)\nregressor = ExtraTreesRegressor(random_state=42)\nregressor.fit(X_train, y_train)\ny_pred = regressor.predict(X_test)","a53d49f1":"print('R2 Score:', r2_score(y_test, y_pred))","1ea9f7f8":"# regressor.fit(features, target)","dbd6c5ef":"# from sklearn.model_selection import KFold, cross_val_score\n# from sklearn.metrics import make_scorer\n\n# score = make_scorer(r2_score)\n# cv = KFold(n_splits=10, random_state=1, shuffle=True)\n# scores = cross_val_score(regressor, features, target, scoring=score, cv=cv)\n\n# print('Accuracy: %.3f (%.3f)' % (np.mean(scores), np.std(scores)))","d227d9cf":"# # feature importance\n# plt.figure(figsize=(10,8))\n# features_impt = pd.Series(regressor.feature_importances_, index=features.columns)\n# features_impt.nlargest(30).plot(kind='barh')\n# plt.show()","e6428303":"ground_test = pd.read_csv('..\/input\/mengary-revenue-prediction\/test.csv', parse_dates=['delivery date', 'placement date'])\nground_test.info()","50d10ab1":"# Handling \"delivery date\" and \"placement date\"\nground_test['delivery_day'] = ground_test['delivery date'].dt.day\nground_test['delivery_month'] = ground_test['delivery date'].dt.month\nground_test['placement_day'] = ground_test['placement date'].dt.day\nground_test['placement_month'] = ground_test['placement date'].dt.month\n\n# One hot Encoding the \"Segment\" column\ndummy_seg = pd.get_dummies(ground_test['segment'], prefix='seg')\nground_test.drop(columns=['segment'], inplace=True)\nground_test = pd.concat([ground_test, dummy_seg], axis=1)\n\n# Label Encoding of the \"delivery type\" column\nground_test['delivery_type'] = label_encoder.transform(ground_test['delivery type'])\n\n# One hot Encoding the \"sub-class\" column\ndummy_type = pd.get_dummies(ground_test['sub-class'], prefix='sclass')\nground_test.drop(columns=['sub-class'], inplace=True)\nground_test = pd.concat([ground_test, dummy_type], axis=1)\n\n# One hot Encoding the \"class\" column\ndummy_type = pd.get_dummies(ground_test['class'], prefix='class')\nground_test.drop(columns=['class'], inplace=True)\nground_test = pd.concat([ground_test, dummy_type], axis=1)","25c4b640":"ground_test['new_price'] = ground_test['price'] - ground_test['price']*ground_test['discount']\nground_test.columns","1c71fe1c":"cols = ['id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'new_price', 'delivery_type', 'sclass_battery', 'sclass_charger', 'sclass_chocolates', 'sclass_colddrinks', 'sclass_fastfood', 'sclass_headset', 'sclass_hoodies', 'sclass_laptop', 'sclass_lighting', 'sclass_pants', 'sclass_phone', 'sclass_shorts', 'sclass_sweets', 'sclass_tablet', 'sclass_television', 'sclass_tshirts',   'sclass_watch', 'class_fynota', 'class_kariox', 'class_qexty', 'delivery_day', 'delivery_month', 'placement_day', 'placement_month']\n\n# Standardization\n# ground_test_norm = scaler.transform(ground_test[cols])\n\npred = regressor.predict(ground_test[cols])","7c8e81c6":"submission = pd.DataFrame()\nsubmission['id'] = ground_test['id']\nsubmission['profit'] = pred","afe6a898":"# submission.to_csv('submission\/submit7.csv', index=False)","506b2f56":"# [Mengary Revenue Prediction](https:\/\/www.kaggle.com\/c\/mengary-revenue-prediction\/overview)\n\n![header.jpg](attachment:header.jpg)","0296f639":"### Problem Statement\n\nMengary is an online E-Commerce company that delivers goods to its customers all over the U.S.A (similar to Amazon). The 3 subsidiary companies under it namely Kariox, Fynota, and Qexty sell products in the Electronics, Food & Beverage, and Clothing Industry segment, respectively. The CEO and the board of directors of the parent company have been excessively worried about the profits for the present year considering their profits were not great last financial year. They wish to predict the profits on each product of the subsidiary companies so that they can adjust the discount given on the loss-making products. But since they don\u2019t know which products are going to produce loss, the management has decided to hire you and your team to help predict the profit for the upcoming financial year. It's almost the end of the financial year 2020-2021 and the data of all the order placement dates and delivery dates(including other features) are known to the company beforehand.\n\n### Objective\n\nYour task is to predict the profits for the remainder of the year 2021 (April to December). The board members have put in a lot of trust in you to accurately predict the profits for the remainder of this year so that they can increase their revenue.\n\n### Evaluation\n\nThe evaluation of the predicted outcomes is done on the basis of the R^2 score. The formula for the same is as follows:\n\n*R^2 = 1 - (RSS \/ TSS)*\n\nwhere,\nR^2 = coefficient of determination\nRSS = sum of squares of residuals\nTSS = total sum of squares","2b3c3996":"- Approach 1\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'address code'`\n        - R2 score: 0.4444, Algorithm: XGBRegressor\n        - R2 score: 0.2788, Algorithm: RandomForestRegressor\n\n- Approach 2\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'type_First Class', 'type_Same Day', 'type_Second Class', 'type_Standard Class'`\n        - R2 score: 0.3661, Algorithm: XGBRegressor\n        - R2 score: 0.3469, Algorithm: RandomForestRegressor\n\n- Approach 3\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'type_First Class', 'type_Same Day', 'type_Second Class', 'type_Standard Class', 'discounted_price'`\n        - R2 score: 0.4828, Algorithm: XGBRegressor\n        - R2 score: 0.5836, Algorithm: RandomForestRegressor\n\n- Approach 4\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'type_First Class', 'type_Same Day', 'type_Second Class', 'type_Standard Class', 'discounted_price', 'class_battery', 'class_charger', 'class_chocolates', 'class_colddrinks', 'class_fastfood', 'class_headset', 'class_hoodies', 'class_laptop', 'class_lighting', 'class_pants', 'class_phone', 'class_shorts', 'class_sweets', 'class_tablet', 'class_television', 'class_tshirts',   'class_watch'`\n        - R2 score: 0.5557, Algorithm: XGBRegressor\n        - R2 score: 0.6400, Algorithm: RandomForestRegressor\n\n- Approach 5\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'new_price', 'delivery_type', 'sclass_battery', 'sclass_charger', 'sclass_chocolates', 'sclass_colddrinks', 'sclass_fastfood', 'sclass_headset', 'sclass_hoodies', 'sclass_laptop', 'sclass_lighting', 'sclass_pants', 'sclass_phone', 'sclass_shorts', 'sclass_sweets', 'sclass_tablet', 'sclass_television', 'sclass_tshirts',   'sclass_watch', 'class_fynota', 'class_kariox', 'class_qexty'`\n        - R2 score: 0.6164, Algorithm: RandomForestRegressor\n        - R2 score: 0.6923, Algorithm: ExtraTreesRegressor (Highest Score on LeaderBoard)\n\n- Approach 6 (After hyperparameter tuning small improvement)\n    - Features: `'id', 'discount', 'price', 'no of items', 'RID', 'seg_Consumer', 'seg_Corporate', 'seg_Home Office', 'new_price', 'delivery_type', 'sclass_battery', 'sclass_charger', 'sclass_chocolates', 'sclass_colddrinks', 'sclass_fastfood', 'sclass_headset', 'sclass_hoodies', 'sclass_laptop', 'sclass_lighting', 'sclass_pants', 'sclass_phone', 'sclass_shorts', 'sclass_sweets', 'sclass_tablet', 'sclass_television', 'sclass_tshirts',   'sclass_watch', 'class_fynota', 'class_kariox', 'class_qexty'`\n        - R2 score: 0.6813, Algorithm: ExtraTreesRegressor(n_estimators=5000, max_features=30, min_samples_split=3)","10a8d57d":"## Test Data Prediction","99408529":"## Model Building and Feature Engineering"}}