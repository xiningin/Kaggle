{"cell_type":{"acb8d951":"code","3e6e3afe":"code","4fe15ddb":"code","9bc5ff1c":"code","2acb2ad2":"code","c2c64ee6":"code","04b15dc9":"code","ec02b2bf":"code","61a68491":"code","16a2a276":"code","011ad64e":"code","03fd9644":"code","b74dee5b":"code","271d000b":"code","ea82e883":"code","2b7de31c":"code","a72d7ab8":"code","2918fcf5":"code","98a233bd":"code","15cdaa30":"code","77f55455":"code","51f32d1e":"code","f3013420":"code","515452b5":"code","5f6c16bc":"code","6543c873":"code","96be2807":"markdown","c5fbb969":"markdown","aa5d78bc":"markdown","c40c8768":"markdown","f3da8d9d":"markdown","e22b9c26":"markdown","cd57aa12":"markdown","5619bd33":"markdown","9b07a603":"markdown","2c2fda6f":"markdown","0f7cc39c":"markdown","4ca4d66d":"markdown","09921794":"markdown"},"source":{"acb8d951":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\n\n# Import data and do some preprocessing\ndf_raw = pd.read_csv('..\/input\/lolport\/data_raw.csv')\ndf_raw['Total_CS'] = df_raw.loc[:, [\"CS in Team's Jungle\", \"CS in Enemy Jungle\", \"CS\"]].sum(axis = 1)\ndf_pre = df_raw.drop([\"CS in Team's Jungle\", \"CS in Enemy Jungle\", \"CS\", 'Wards placed', \n                      'Wards destroyed', 'Control Wards Purchased', 'VSPM', 'WPM', 'VWPM',\n                      'WCPM', 'VS%', 'Physical Damage', 'Magic Damage', 'True Damage', \n                      'Solo kills', 'Double kills', 'Triple kills', 'Quadra kills',\n                      'Penta kills', 'GOLD%', 'VS%', 'DMG%'], axis = 1)\ndf_pre = df_pre[['Win', 'Team', 'Player', 'Role', 'Kills', 'Deaths', 'Assists', 'KDA', 'K+A Per Minute',\n                 'KP%', 'Total damage taken', 'Total heal', 'Time ccing others', 'Damage self mitigated',\\\n                 'Total damage to Champion', 'Damage dealt to turrets', 'DPM', 'Total_CS', 'CSM', 'Golds',\\\n                 'GPM', 'Vision Score', 'GD@15', 'CSD@15', 'XPD@15', 'LVLD@15']]\nX = df_pre.drop(['Win', 'Team', 'Player', 'Role'], axis = 1)\ny = df_pre.Win","3e6e3afe":"# Heatmap\nf, ax = plt.subplots(figsize = (15, 15))\nsns.heatmap(df_pre.corr(), annot = True, linewidths = .5, fmt = '.2f', ax = ax)\nplt.show()\n# Check the correlation coefficient, some features are showing correlation","4fe15ddb":"# Pairplot\nsns.pairplot(df_pre, hue = \"Win\", height = 2.5)\nplt.tight_layout()\nplt.show()","9bc5ff1c":"# Feature boxplot\nX.boxplot(figsize = (10, 8), vert = False)\n# Some features are skewed right let's solve it","2acb2ad2":"# Checking the Skewness, Kurtosis\nnumerical_feats = X.dtypes[X.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\ncategorical_feats = X.dtypes[X.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))\n\nfor col in numerical_feats: print('{:22}'.format(col),\n                                  'Skewness: {:05.2f}'.format(X[col].skew()) , ' \\t' ,\n                                  'Kurtosis: {:06.2f}'.format(X[col].kurt()) )","c2c64ee6":"# Solving skewness\nX['log_Total damage taken'] = np.log1p(X['Total damage taken'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X['log_Total damage taken'])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Total damage taken'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Total damage taken'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Total damage taken'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Total damage taken'].kurt()))\n\nX['log_Total heal'] = np.log1p(X['Total heal'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X['log_Total heal'])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Total heal'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Total heal'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Total heal'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Total heal'].kurt()))\n\nX['log_Damage self mitigated'] = np.log1p(X['Damage self mitigated'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X[\"log_Damage self mitigated\"])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Damage self mitigated'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Damage self mitigated'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Damage self mitigated'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Damage self mitigated'].kurt()))\n\nX['log_Total damage to Champion'] = np.log1p(X['Total damage to Champion'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X[\"log_Total damage to Champion\"])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Total damage to Champion'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Total damage to Champion'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Total damage to Champion'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Total damage to Champion'].kurt()))\n\nX['log_Damage dealt to turrets'] = np.log1p(X['Damage dealt to turrets'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X['log_Damage dealt to turrets'])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Damage dealt to turrets'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Damage dealt to turrets'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Damage dealt to turrets'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Damage dealt to turrets'].kurt()))\n\nX['log_Golds'] = np.log1p(X['Golds'])\nf, ax = plt.subplots(figsize = (10, 6))\nsns.distplot(X['log_Golds'])\nprint(\"log_Skewness: {:.3f}\".format(X['log_Damage dealt to turrets'].skew()),\\\n      \"log_Kurtosis: {:.3f}\".format(X['log_Damage dealt to turrets'].kurt()))\nprint(\"Skewness: {:.3f}\".format(X['Damage dealt to turrets'].skew()),\\\n      \"Kurtosis: {:.3f}\".format(X['Damage dealt to turrets'].kurt()))\n","04b15dc9":"X = X.drop(['Total damage taken', 'Total heal', 'Damage self mitigated', 'Total damage to Champion',\n            'Damage dealt to turrets', 'Golds'], axis = 1)\nX.boxplot(figsize = (10, 8), vert = False)","ec02b2bf":"X_tra, X_test, y_tra, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0)\nX_train, X_val, y_train, y_val = train_test_split(X_tra, y_tra, test_size = 0.25, random_state = 0)\nprint(\"train X Data shape: {}\".format(X_train.shape))\nprint(\"val X Data shape: {}\".format(X_val.shape))\nprint(\"test X Data shape: {}\".format(X_test.shape))\ncol = X.columns.values","61a68491":"from sklearn.preprocessing import RobustScaler\nRBsc = RobustScaler()\nX_train_RBsc = RBsc.fit_transform(X_train)\nX_val_RBsc = RBsc.transform(X_val)\nX_test_RBsc = RBsc.transform(X_test)\nX_train_RBsc = pd.DataFrame(X_train_RBsc)\nX_train_RBsc.columns = [col]\nX_train_RBsc.boxplot(figsize=(10, 8), vert = False)","16a2a276":"from sklearn.decomposition import PCA\n# Choosing the number of components.\npca_RBn = PCA(n_components = 10).fit(X_train_RBsc)\nX_train_pca_RB = pca_RBn.transform(X_train_RBsc)\nprint('eigen_value :', pca_RBn.explained_variance_)\nprint('explained variance ratio :', pca_RBn.explained_variance_ratio_)\nprint('cum_explained variance ratio :', sum(pca_RBn.explained_variance_ratio_))\nplt.figure(figsize = (10, 8))\nplt.plot(np.cumsum(pca_RBn.explained_variance_ratio_), 'o-')\nplt.grid()\nplt.title('Cumulative Explained Variance')\nplt.xlabel('Number of Components')\nplt.ylabel('Cumulative Explained Variance');\n\nplt.figure(figsize = (10, 8))\nplt.plot([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], pca_RBn.explained_variance_ratio_, 'o-')\nplt.grid()\nplt.title('Scree Plot')\nplt.xlabel('Number of Components')\nplt.ylabel('Explained Variance Ratio');","011ad64e":"pca_RB = PCA(n_components = 4).fit(X_train_RBsc)\nX_train_pca_RB = pca_RB.transform(X_train_RBsc)\nX_val_pca_RB = pca_RB.transform(X_val_RBsc)\nX_test_pca_RB = pca_RB.transform(X_test_RBsc)\nprint(\"Original shape: {}\".format(str(X_train_RBsc.shape)))\nprint(\"Reduced shape: {}\".format(str(X_train_pca_RB.shape)))\nprint('eigen_value :', pca_RB.explained_variance_)\nprint('explained variance ratio :', pca_RB.explained_variance_ratio_)\nprint('cum_explained variance ratio :', sum(pca_RB.explained_variance_ratio_))","03fd9644":"''' # Important features '''\nimport statsmodels.api as sm\ny_traint = list(y_train)\nlogit = sm.Logit(y_traint, X_train_RBsc)\nresult= logit.fit()\nprint(result.summary())\nprint(np.exp(result.params))","b74dee5b":"''' # Perform grid search '''\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n# RB\nclf = LogisticRegression(max_iter = 2000, random_state = 0)\nh_para = [{'C': np.logspace(-4, 4, 20), 'penalty': [ 'l1', 'l2'], 'solver': ['liblinear']},\n          {'C': np.logspace(-4, 4, 20), 'penalty': ['l2'], 'solver': ['lbfgs']}]\ngs = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngs = gs.fit(X_train_RBsc, y_train)\nbest_accuracy_RB = gs.best_score_\nbest_parameters_RB = gs.best_params_\nprint(best_parameters_RB, f'{best_accuracy_RB:.4f}')\n# {'C': 29.763514416313132, 'penalty': 'l2', 'solver': 'liblinear'} 0.9723\n\n# pca_RB\nclf = LogisticRegression(max_iter = 2000, random_state = 0)\nh_para = [{'C': np.logspace(-4, 4, 20), 'penalty': [ 'l1', 'l2'], 'solver': ['liblinear']},\n          {'C': np.logspace(-4, 4, 20), 'penalty': ['l2'], 'solver': ['lbfgs']}]\ngs = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngs = gs.fit(X_train_pca_RB, y_train)\nbest_accuracy_pca_RB = gs.best_score_\nbest_parameters_pca_RB = gs.best_params_\nprint(best_parameters_pca_RB, f'{best_accuracy_pca_RB:.4f}')\n# {'C': 0.08858667904100823, 'penalty': 'l2', 'solver': 'liblinear'} 0.9220","271d000b":"''' # The score of grid search's' best_parameters '''\nfrom sklearn.linear_model import LogisticRegression\n# RB\nlr_RB = LogisticRegression(C = 4.28, penalty = 'l2', solver = 'liblinear').fit(X_train_RBsc, y_train)\nprint(\"RB_lr train: {:.4f}\".format(lr_RB.score(X_train_RBsc, y_train)))    # 0.9287\nprint(\"BR_lr val: {:.4f}\".format(lr_RB.score(X_val_RBsc, y_val)))          # 0.9326\n\n# pca_RB\nlr_pca_RB = LogisticRegression(C = 0.23, penalty = 'l2', solver = 'liblinear').fit(X_train_pca_RB, y_train)\nprint(\"pca_RB_lr train: {:.4f}\".format(lr_pca_RB.score(X_train_pca_RB, y_train)))   # 0.8566\nprint(\"pca_RB_lr val: {:.4f}\".format(lr_pca_RB.score(X_val_pca_RB, y_val)))         # 0.8814\n\n# Only a few %p have droped on the train data and remember there was a multicollinearity between few features so I chose pca_RB","ea82e883":"''' # Perform grid search '''\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n# RB\nclf = KNeighborsClassifier()\nh_para = [{'n_neighbors': range(1, 100), 'weights': ['uniform', 'distance']}]\ngrid_search = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train_RBsc, y_train)\nbest_accuracy_RB = grid_search.best_score_\nbest_parameters_RB = grid_search.best_params_\nprint(best_parameters_RB, f'{best_accuracy_RB:.4f}')\n# {'n_neighbors': 43, 'weights': 'distance'} 0.9485\n\n# pca_RB\nclf = KNeighborsClassifier()\nh_para = [{'n_neighbors': range(1, 100), 'weights': ['uniform', 'distance']}]\ngrid_search = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train_pca_RB, y_train)\nbest_accuracy_pca_RB = grid_search.best_score_\nbest_parameters_pca_RB = grid_search.best_params_\nprint(best_parameters_pca_RB, f'{best_accuracy_pca_RB:.4f}')\n# {'n_neighbors': 62, 'weights': 'distance'} 0.9215","2b7de31c":"''' # The score of grid search's' best_parameters '''\nfrom sklearn.neighbors import KNeighborsClassifier\n# RB\nKNN_RB = KNeighborsClassifier(n_neighbors = 43, weights = 'distance').fit(X_train_RBsc, y_train)\nprint(\"KNN_RB train: {:.4f}\".format(KNN_RB.score(X_train_RBsc, y_train)))     # 1.0000\nprint(\"KNN_RB val: {:.4f}\".format(KNN_RB.score(X_val_RBsc, y_val)))           # 0.8907\n\n# pca_RB\nKNN_pca_RB = KNeighborsClassifier(n_neighbors = 62, weights = 'distance').fit(X_train_pca_RB, y_train)\nprint(\"KNN_pca_RB train: {:.4f}\".format(KNN_pca_RB.score(X_train_pca_RB, y_train)))     # 1.0000\nprint(\"KNN_pca_RB val: {:.4f}\".format(KNN_pca_RB.score(X_val_pca_RB, y_val)))           # 0.8488","a72d7ab8":"''' # Find what's the problem '''\n# Something is wrong, an overfitting has occurred\ntrain_accuracy = []\nval_accuracy = []\nneighbors_settings = range(1, 50)\nfrom sklearn.neighbors import KNeighborsClassifier\nfor n_neighbors in neighbors_settings:\n    knMod = KNeighborsClassifier(n_neighbors = n_neighbors, weights = 'distance').fit(X_train_pca_RB, y_train)\n    train_accuracy.append(knMod.score(X_train_pca_RB, y_train))\n    val_accuracy.append(knMod.score(X_val_pca_RB, y_val))\nplt.plot(neighbors_settings, train_accuracy, label = \"train accuracy\")\nplt.plot(neighbors_settings, val_accuracy, label = \"val accuracy\")\nplt.ylabel(\"accuracy\")\nplt.xlabel(\"n_neighbors\")\nplt.legend()\nplt.show()\n# If it is weights = 'distance' it causes overfitting, so it has to be weights = 'uniform'","2918fcf5":"''' # Perform a gridsearch again '''\n# The defult is weights = 'uniform' so don't need to write it\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n# RB\nclf = KNeighborsClassifier()\nh_para = [{'n_neighbors': range(1, 100)}]\ngrid_search = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train_RBsc, y_train)\nbest_accuracy_RB = grid_search.best_score_\nbest_parameters_RB = grid_search.best_params_\nprint(best_parameters_RB, f'{best_accuracy_RB:.4f}')\n# {'n_neighbors': 43} 0.9472\n\n# pca_RB\nclf = KNeighborsClassifier()\nh_para = [{'n_neighbors': range(1, 100)}]\ngrid_search = GridSearchCV(clf, h_para, scoring = 'roc_auc', cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(X_train_pca_RB, y_train)\nbest_accuracy_pca_RB = grid_search.best_score_\nbest_parameters_pca_RB = grid_search.best_params_\nprint(best_parameters_pca_RB, f'{best_accuracy_pca_RB:.4f}')\n# {'n_neighbors': 61} 0.9199","98a233bd":"''' # The score of grid search's' best_parameters '''\nfrom sklearn.neighbors import KNeighborsClassifier\n# RB\nKNN_RB = KNeighborsClassifier(n_neighbors = 43).fit(X_train_RBsc, y_train)\nprint(\"KNN_RB train: {:.4f}\".format(KNN_RB.score(X_train_RBsc, y_train)))   # 0.8806\nprint(\"KNN_RB val: {:.4f}\".format(KNN_RB.score(X_val_RBsc, y_val)))         # 0.8884\n\n# pca_RB\nKNN_pca_RB = KNeighborsClassifier(n_neighbors = 61).fit(X_train_pca_RB, y_train)\nprint(\"KNN_pca_RB train: {:.4f}\".format(KNN_pca_RB.score(X_train_pca_RB, y_train)))     # 0.8496\nprint(\"KNN_pca_RB val: {:.4f}\".format(KNN_pca_RB.score(X_val_pca_RB, y_val)))           # 0.8465","15cdaa30":"''' # Perform random search '''\n# As a Korean I don't like waiting more than 2 minutes, so let's do a random search instead\n# Just joking anyway, it takes about 20 minutes when performing a grid search, \n# random search is an alternative method to optimize the hyperparameters.\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n# RB\nclf = DecisionTreeClassifier(random_state = 0)\nh_para = [{'criterion': ['entropy', 'gini'], 'max_depth': np.arange(1, 20),\n           'max_leaf_nodes': np.arange(2, 20), 'min_samples_split': np.arange(2, 20),\n           'min_samples_leaf': np.arange(1, 20)}]\nrds = RandomizedSearchCV(clf, h_para, n_iter = 10, cv = 10, \n                         scoring = 'roc_auc', n_jobs = -1).fit(X_train_RBsc, y_train)\nbest_accuracy_RB = rds.best_score_\nbest_parameters_RB = rds.best_params_\nprint(best_parameters_RB, f'{best_accuracy_RB:.4f}')\n# {'min_samples_split': 15, 'min_samples_leaf': 8, 'max_leaf_nodes': 10, 'max_depth': 4, 'criterion': 'entropy'} 0.9262\n\n# pca_RB\nclf = DecisionTreeClassifier(random_state = 0)\nh_para = [{'criterion': ['entropy', 'gini'], 'max_depth': np.arange(1, 20),\n           'max_leaf_nodes': np.arange(2, 20), 'min_samples_split': np.arange(2, 20),\n           'min_samples_leaf': np.arange(1, 20)}]\nrds = RandomizedSearchCV(clf, h_para, n_iter = 10, cv = 10, \n                         scoring = 'roc_auc', n_jobs = -1).fit(X_train_pca_RB, y_train)\nbest_accuracy_pca_RB = rds.best_score_\nbest_parameters_pca_RB = rds.best_params_\nprint(best_parameters_pca_RB, f'{best_accuracy_pca_RB:.4f}')\n# {'min_samples_split': 2, 'min_samples_leaf': 13, 'max_leaf_nodes': 15, 'max_depth': 6, 'criterion': 'entropy'} 0.8620\n\n# Everytime you run it, the result will be different","77f55455":"''' # The score of random search's' best_parameters '''\nfrom sklearn.tree import DecisionTreeClassifier\n# RB\nDTC_RB = DecisionTreeClassifier(max_depth = 4, max_leaf_nodes = 10, min_samples_leaf = 8, \n                                min_samples_split = 15, criterion = 'entropy').fit(X_train_RBsc, y_train)\nprint(\"DTC_RB train: {:.4f}\".format(DTC_RB.score(X_train_RBsc, y_train)))     # 0.8891\nprint(\"DTC_RB val: {:.4f}\".format(DTC_RB.score(X_val_RBsc, y_val)))           # 0.8860\n\n# pca_RB\nDTC_pca_RB = DecisionTreeClassifier(max_depth = 6, max_leaf_nodes = 15, min_samples_leaf = 13,\n                                    min_samples_split = 2, criterion = 'entropy').fit(X_train_pca_RB, y_train)\nprint(\"DTC_pca_RB train: {:.4f}\".format(DTC_pca_RB.score(X_train_pca_RB, y_train)))     # 0.8132\nprint(\"DTC_pca_RB val: {:.4f}\".format(DTC_pca_RB.score(X_val_pca_RB, y_val)))           # 0.8093","51f32d1e":"''' # Perform grid search '''\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n# RB\nclf = SVC()\nh_para = [{'kernel': [ 'linear' ], 'C': np.logspace(-3, 2, 6)},\n          {'kernel': ['rbf'], 'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6)}]\ngrid_search = GridSearchCV(clf, h_para, cv = 10, scoring = 'roc_auc', n_jobs = -1)\ngds = grid_search.fit(X_train_RBsc, y_train)\nbest_accuracy_gds_RB = gds.best_score_\nbest_parameters_gds_RB = gds.best_params_\nprint(best_parameters_gds_RB, f'{best_accuracy_gds_RB:.4f}')\n# {'C': 100.0, 'kernel': 'linear'} 0.9711\n\n# pca_RB\nclf = SVC()\nh_para = [{'kernel': [ 'linear' ], 'C': np.logspace(-3, 2, 6)},\n          {'kernel': ['rbf'], 'C': np.logspace(-3, 2, 6), 'gamma': np.logspace(-3, 2, 6)}]\ngrid_search = GridSearchCV(clf, h_para, cv = 10, scoring = 'roc_auc', n_jobs = -1)\ngds = grid_search.fit(X_train_pca_RB, y_train)\nbest_accuracy_gds_pca_RB = gds.best_score_\nbest_parameters_gds_pca_RB = gds.best_params_\nprint(best_parameters_gds_pca_RB, f'{best_accuracy_gds_pca_RB:.4f}')\n# {'C': 0.01, 'kernel': 'linear'} 0.9221","f3013420":"''' # The score of grid search's' best_parameters '''\nfrom sklearn.svm import SVC\n# RB\nsvm_RB = SVC(kernel = 'linear', C = 100).fit(X_train_RBsc, y_train)\nprint(\"svm_RB train: {:.4f}\".format(svm_RB.score(X_train_RBsc, y_train)))    # 0.9302\nprint(\"svm_RB val: {:.4f}\".format(svm_RB.score(X_val_RBsc, y_val)))          # 0.9302\n\n# pca_RB\nsvm_pca_RB = SVC(kernel = 'linear', C = 0.01).fit(X_train_pca_RB, y_train)\nprint(\"svm_pca_RB train: {:.4f}\".format(svm_pca_RB.score(X_train_pca_RB, y_train)))    # 0.8550\nprint(\"svm_pca_RB val: {:.4f}\".format(svm_pca_RB.score(X_val_pca_RB, y_val)))          # 0.8744","515452b5":"import matplotlib.pyplot as plt\nimport numpy as np\na = np.arange(4)\nb = [round(lr_pca_RB.score(X_val_pca_RB, y_val), 4), round(KNN_pca_RB.score(X_val_pca_RB, y_val), 4),\n     round(DTC_pca_RB.score(X_val_pca_RB, y_val), 4), round(svm_pca_RB.score(X_val_pca_RB, y_val), 4)]\nc = ['LogisticRegression', 'KNN', 'DecisionTree', 'SVM']\nplt.bar(a,b)\nfor i, v in enumerate(a):\n    plt.text(v, b[i], b[i], fontsize = 9, color='blue',\n             horizontalalignment = 'center', verticalalignment = 'bottom')\nplt.title(\"Accuracy of Models\", fontsize = 15)\nplt.ylabel(\"Accuracy\", fontsize = 15)\nplt.xticks(a, c, fontsize = 13)\nplt.show()\n# Logistic Regression has the highest accuracy","5f6c16bc":"#%% Final model test\nlr_pca_RB = LogisticRegression(C = 0.61, penalty = 'l1', solver = 'liblinear').fit(X_train_pca_RB, y_train)\nprint(\"pca_RB_lr test: {:.4f}\".format(lr_pca_RB.score(X_test_pca_RB, y_test)))      # 0.8279\n\n# confusion_matrix\nfrom sklearn.metrics import confusion_matrix\ny_test_pred = lr_pca_RB.predict(X_test_pca_RB)\ny_true = y_test\ncm = confusion_matrix(y_true, y_test_pred)\nf, ax = plt.subplots(figsize = (5, 5))\nsns.heatmap(cm, annot = True, linewidths = 0.5, fmt = '.0f', ax = ax )\nplt.xlabel('Negative')\nplt.ylabel('Positive')\nplt.show()","6543c873":"'''# Visualization '''\nfrom sklearn.linear_model import LogisticRegression\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nX_v1 = X_test_pca_RB\nY_v1 = y_test\nX_v1 = X_v1[np.logical_or(Y_v1==0,Y_v1==1)]\nY_v1 = Y_v1[np.logical_or(Y_v1==0,Y_v1==1)]\n\nclf = LogisticRegression(C = 3, penalty = 'l2', solver = 'liblinear').fit(X_train_pca_RB, y_train)\nz_v1 = lambda x_v1, y_v1:\\\n    (-clf.intercept_[0]-clf.coef_[0][0]*x_v1-clf.coef_[0][1]*y_v1)\/clf.coef_[0][2]\n\ntmp = np.linspace(-5, 5, 100)\nx_v1, y_v1 = np.meshgrid(tmp,tmp)\n\nfig = plt.figure(figsize = (10, 8))\nax  = fig.add_subplot(111, projection = '3d')\nax.plot3D(X_v1[Y_v1==0,0], X_v1[Y_v1==0,1], X_v1[Y_v1==0,2],'ob')\nax.plot3D(X_v1[Y_v1==1,0], X_v1[Y_v1==1,1], X_v1[Y_v1==1,2],'sr')\nax.plot_surface(x_v1, y_v1, z_v1(x_v1, y_v1))\nax.view_init(32, 222)\nax.set_title(\"LogisticRegression Visualization\")\nax.set_xlabel(\"1st\")\nax.w_xaxis.set_ticklabels([])\nax.set_ylabel(\"2nd\")\nax.w_yaxis.set_ticklabels([])\nax.set_zlabel(\"3rd\")\nax.w_zaxis.set_ticklabels([])\nplt.show()\n\n## You can make it animated by this code\n#for ii in np.arange(0, 360, 1):\n#    ax.view_init(elev=32, azim=ii)\n#    fig.savefig('gif_image%d.png' % ii)","96be2807":"# Split data","c5fbb969":"* Skewness a = 0 means symmetrical, if a < 0 means skewed to the right, if a > 0 means skewed to the left.\n* When the skewness is -0.5 < a < 0.5, the data is considered symmetric\n* When it's -1 < a < -0.5 or 0.5 < a < 1, it's a bit skewed\n* When it's a > |1|, it's very skewed","aa5d78bc":"* We can take a look at the Regression coefficient and the probability such as when KDA increases by 1,the probability of winning increases by 2.87.\n* And when Total damage to Champion increases by 1,the probability of winning increases by 2.69, and so on.","c40c8768":"* According to the scree plot and the cumulative variance, 4 components seems right.","f3da8d9d":"# KNN","e22b9c26":"# SVM","cd57aa12":"# PCA","5619bd33":"# Scaling\n* It looks like there's a lot of outliers, but there're meaningful observation. \n* Anyway to make the model perform better I'm going to use a RobustScaler which will minimize the affect of the outliears.","9b07a603":"# Logistic Regression","2c2fda6f":"# Decision Tree","0f7cc39c":"# Choosing final model","4ca4d66d":"* Design predict models and analyze results by using League of Legends game data\n* I've crawled the data from this site https:\/\/gol.gg\/tournament\/tournament-stats\/LCK%20Spring%202021\/\n\n\n* The observation in KDA which is \"Perfect KDA\" are replaced by this formula KDA = (K+A)*1.2","09921794":"Take a look, it got much better"}}