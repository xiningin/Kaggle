{"cell_type":{"8a9725f2":"code","4db8ae0c":"code","b2cd59ef":"code","7f4363e6":"code","49d15216":"code","f7374b6f":"code","979737ff":"code","c9f304fe":"code","8160d63a":"code","b553d725":"code","7951184e":"code","b290cfd5":"code","9c82811d":"code","a10fd7cc":"code","75549528":"code","3633719e":"code","74987423":"code","906ec135":"code","1bdfe3ce":"code","b33fc979":"code","45b80dfc":"code","6ff6cfa5":"code","3774a11e":"code","77ea2992":"code","6820765d":"code","e6391dea":"code","6ffb5bfd":"code","648aee55":"code","782676d0":"code","086acadd":"code","c85124f1":"code","90155554":"code","162f31cd":"code","df53264e":"code","1445a38c":"code","af5ff84b":"markdown","49d5f307":"markdown","9a9267eb":"markdown","5e4e9cd2":"markdown","41d46e04":"markdown","8e2279af":"markdown","035d776a":"markdown","3e46decb":"markdown","48045dd9":"markdown","5514a061":"markdown","3711416e":"markdown","2233b43a":"markdown","189c014d":"markdown","ea9b5199":"markdown"},"source":{"8a9725f2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4db8ae0c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import power_transform\nfrom sklearn.metrics import confusion_matrix\nfrom boruta import BorutaPy\nfrom sklearn.model_selection import train_test_split\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nimport pickle","b2cd59ef":"target_column='Survived'\nvisualize_column = 'Sex'\n\ndef prob_surviving(visualize_column):\n    group = train_df.groupby(by=[visualize_column])\n\n    for index, g in group:\n        print(f'value = {index}')\n        survived = g[target_column].sum()\n        total = g[target_column].count()\n        print(survived,' Survived from ',total)\n        print(f'probability of surviving knowing {index} is {round(survived\/total,2)}')\n        print(f'probability of dying knowing {index} is {round(1-survived\/total,2)}','\\n\\n')","7f4363e6":"def cluster_feature(org_df,n_clusters,column,random_state=29):\n    \n    df=org_df[org_df['trainable']==1]\n    \n    df=df[df[column].notnull()].copy()\n    classify_feature=KMeans(n_clusters=n_clusters,random_state=random_state).fit(df[column].values.reshape([-1,1]))\n\n    #df[column]=classify_feature.labels_\n    #df['Age']=df['Age'].apply(lambda x: int(x\/10)*10 if not np.isnan(x) else np.nan)\n    org_df[column]=org_df[column].apply( lambda x:\n        x if np.isnan(x) else classify_feature.predict([[x]])[0])\n    #df[column]\n    print(str(column)+' culsters are: \\n',classify_feature.cluster_centers_,'\\n')\n    return classify_feature,org_df","49d15216":"def convert_int(df,column):\n    \n    df[column]=df[column].str.extract(r'((\\s?)\\d+$)')\n    df[column]=df[column].str.replace(' ','').apply(lambda x: int(x) if type(x)==str else x)\n    \n    return df\n\ndef get_mean_unbiased(df,column,low_per,high_per):\n    \n    cal_df=df[df[column].notnull()]\n    \n    s=cal_df[column].quantile([low_per,high_per])\n    val_min,val_max=s.iloc[0],s.iloc[1]\n    \n    cal_df=cal_df[(cal_df[column]>val_min) & (cal_df[column]<val_max)]\n    \n    return cal_df[column].mean()","f7374b6f":"def fill_missing_age_wtitle(df,func=np.max):\n    #sns.countplot(x='Title',hue='Age',data=df)\n    #df['Title'].unique()\n    #sns.countplot(x='Age',hue='Title',data=df)\n    #plt.legend(bbox_to_anchor=(1.01, 1),borderaxespad=0)\n\n    #Fill age nan values with the most probable value according to the title of the person\n    df['Age']=df.apply(lambda row: func(gr_age[row['Title']].index.values) if np.isnan(row['Age']) else row['Age'],axis=1)\n    print(df['Age'])\n    return df","979737ff":"def fill_embarked_wticket(df,column='Ticket'):\n\n    df['Embarked']=df.apply(lambda row: np.max(gr_ticket[row[column]].index.values) if type(row['Embarked'])==float else row['Embarked'],axis=1)\n    \n    return df","c9f304fe":"def reorder_clusters(clusters,df,column):\n    '''\n    clusters is the output of kmean.cluster_centers_\n    '''\n    clusters=np.array(clusters).ravel()\n    print(clusters)\n    ordered_clusters=np.sort(clusters)# sort in ascending order\n    \n    reorder_dict={}\n    for cluster_id in range(len(clusters)):\n        val=clusters[cluster_id]\n        new_cluster_id=np.where(ordered_clusters==val)# get the index of the val in ordered cluster\n        reorder_dict[cluster_id]=new_cluster_id[0][0]\n    \n    #reorder dictionary maps the current value to its supposed valu if reordered.\n    #for example cluster 0 of age may have center of 53. if the clusters were ordered, then it should\n    #be cluster 3. thus, this dictionary has the current cluster id (0) as key and the cluster id if\n    #reordered as value\n    \n    df[column]=df[column].apply(lambda x: reorder_dict[int(x)])\n    \n    print(reorder_dict)\n    return ordered_clusters,df\n    ","8160d63a":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntrain_df.head()","b553d725":"test_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ntest_df.head()","7951184e":"pd.isna(train_df).sum()\/len(train_df)","b290cfd5":"pd.isna(test_df).sum()\/len(test_df)","9c82811d":"for feature in ['Sex','Pclass','Embarked']:\n    print(feature)\n    prob_surviving(feature)","a10fd7cc":"sns.countplot(x=target_column,hue='Sex',data=train_df)","75549528":"sns.countplot(x=target_column,hue='SibSp',data=train_df)","3633719e":"sns.countplot(x=target_column,hue='Parch',data=train_df)","74987423":"sns.countplot(x=target_column,hue='Pclass',data=train_df)","906ec135":"sns.countplot(x=target_column,hue='Embarked',data=train_df)","1bdfe3ce":"df=train_df.copy()","b33fc979":"Tdf=train_df.copy()\nTdf['trainable']=1\n\nTsdf=test_df.copy()\nTsdf['trainable']=0\n\ntotal_df=pd.concat([Tdf,Tsdf])\ntotal_df","45b80dfc":"random_state=29\n\ntotal_df['family_size']=total_df['Parch']+total_df['SibSp']+1\ntotal_df['last_name']=total_df['Name'].apply(lambda x: str.split(x, \",\")[0])\ntotal_df['Title']=total_df['Name'].str.extract(r'(,\\s(\\w+|\\w+\\s\\w+)\\.)')[1]\n\ndf=total_df[total_df['trainable']==1]\n\nallowed_vals=['Mr','Mrs','Miss','Master','Dr','Major']\n\ntotal_df.loc[~total_df['Title'].isin(allowed_vals), 'Title'] = 'Rare'\n\n#do KMeans on Age\nage_classify,total_df=cluster_feature(total_df,n_clusters=4,column='Age',random_state=random_state)\n\n#Fill the missing Age values using Title column\ngr_age=total_df.groupby(['Title','Age'])['PassengerId'].count()\ntotal_df=fill_missing_age_wtitle(total_df,func=np.max)\n\n#fill the missing Fare values according to the Pclass values.\n#The higher the class the more likely the fare will increase.\ndf=total_df[total_df['trainable']==1]\nclass_mean=df[['Pclass','Fare']].groupby('Pclass').mean()\ntotal_df['Fare']=total_df.apply(lambda row: class_mean.loc[row['Pclass']].values[0] if np.isnan(row['Fare']) else row['Fare'],axis=1)\n\n#next i will add 2 a great feature i found in 2 other notebooks. This feature greatly increased my\n#score.\n#generate a family information group\ndefault_survival_chance = 0.5\ntotal_df['Family_Survival'] = default_survival_chance\n\n# Grouping by last name and fare gives you groups of families\nfor grp, grp_df in total_df[['Survived','Name', 'last_name', 'Fare', 'Ticket', 'PassengerId','Sex',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['last_name', 'Fare']):\n    \n    #if the family group has more than one member, then it is a family\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            smean=grp_df.drop(ind)['Survived'].mean()\n            smean= 0.4 if np.isnan(smean) else smean\n            passID = row['PassengerId']\n            total_df.loc[total_df['PassengerId'] == passID, 'prob_survival']=smean#np.log10(smean+0.0001)\n            if (smax == 1.0):\n                total_df.loc[total_df['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                total_df.loc[total_df['PassengerId'] == passID, 'Family_Survival'] = 0\n\n#grouping by ticket gives you groups that are expected to have gone together to the trip\nfor _, grp_df in total_df.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                smean=grp_df.drop(ind)['Survived'].mean()\n                smean=0.4 if np.isnan(smean) else smean\n                passID = row['PassengerId']\n                total_df.loc[total_df['PassengerId'] == passID, 'prob_survival']=smean#np.log10(smean+0.0001)\n                if (smax == 1.0):\n                    total_df.loc[total_df['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    total_df.loc[total_df['PassengerId'] == passID, 'Family_Survival'] = 0\n\ntotal_df['prob_survival']=total_df['prob_survival'].fillna(total_df['Family_Survival'])\ntotal_df['prob_survival']=total_df['prob_survival'].apply(lambda x: 1\/(1+np.exp(-x)))\n# total_df['prob_survival']=total_df['prob_survival'].replace(0,0.000001)\n# total_df['prob_survival']=power_transform((total_df['prob_survival']).values.reshape(-1, 1), method='box-cox')\nSprob_classify,total_df=cluster_feature(total_df,n_clusters=7,column='prob_survival',random_state=random_state)\n#4\n#replace the 0 Fare values with a small number (wrt fare) and apply a log10 on the values.\ntotal_df['Fare']=total_df['Fare'].replace(0,0.00001)\ntotal_df['Fare']=total_df['Fare'].apply(lambda x: np.log(x+1))\n#one of the cited notebooks suggests that the fare is for the family. Supposing this theory stands,\n#then divide the fare(log fare to be more accurate) by the family size to have something like \n#fare\/family member\ntotal_df['fare_per_fsize']=total_df['Fare'].values\/total_df['family_size'].values\nfare_classify,total_df=cluster_feature(total_df,n_clusters=3,column='fare_per_fsize',random_state=random_state)\n\n#I tried some method to fill the missing embarked and all gave 'S' on train set \\\n#while test set does not have missing values. Thus, i just did fill missing embarked values with 'S'\ntotal_df['Embarked']=total_df['Embarked'].fillna('S')\n#This next 2 columns were taken from another notebooks\n#these new features seems to be useful\n\ntotal_df['Sex']=total_df['Sex'].apply(lambda x: 1 if x=='male' else 0)\n\ntotal_df['Embarked']=total_df['Embarked'].apply(lambda x: 0 if x=='S' else (1 if x == 'C' else 2))\n\nprint('Null values: ',pd.isna(total_df).sum()\/len(total_df))\nprint('train_df: ',len(train_df))\nprint('test_df: ',len(test_df))\nprint('total_df: ',len(total_df))\nprint('columns: ',total_df.columns)\n\ntotal_df","6ff6cfa5":"age_clusters,total_df=reorder_clusters(age_classify.cluster_centers_,total_df,'Age')\nprob_survival_clusters,total_df=reorder_clusters(Sprob_classify.cluster_centers_,total_df,'prob_survival')\nfare_clusters,total_df=reorder_clusters(fare_classify.cluster_centers_,total_df,'fare_per_fsize')","3774a11e":"sns.heatmap(total_df.corr())","77ea2992":"drop_features=['Survived','PassengerId','SibSp','Parch','Ticket','trainable','last_name','Name',\n               'Title','Cabin','Embarked','Family_Survival','family_size','Fare']\n\nX_train=total_df[total_df['trainable']==1]\nX_test=total_df[total_df['trainable']==0]\n\nX_train=X_train.drop(columns=drop_features).values\ny_train=total_df[total_df['trainable']==1]['Survived'].values.ravel()#.reshape((-1,1))\n\nX_test=X_test.drop(columns=drop_features).values\ny_test=total_df[total_df['trainable']==0][['PassengerId','Survived']]#.reshape((-1,1))","6820765d":"cv=5\nverbose=1\n# parameters={\n# #     'max_depth':np.arange(5,16,5),\n# #     'min_samples_split':np.arange(10,30,5),\n#     'max_leaf_nodes':[5,8,10,12,15],\n#     'min_samples_leaf':[3,5,8,10],\n# #     'n_estimators':[100,150,200,250],\n#     'min_samples_split':[5,10,15,20]\n# }\n\nforest_classifier=RandomForestClassifier(n_estimators=100,max_depth=10,max_leaf_nodes=12,\n                            min_samples_leaf=10,min_samples_split=5,n_jobs=-1,random_state=27)\n\n# clf=GridSearchCV(forest_classifier,parameters,verbose=verbose,cv=cv)\n# clf.fit(X_train,y_train)\n\n\n# best_estimator=clf.best_estimator_\n# print(X_train)\nforest_classifier.fit(X_train,y_train)\n\n# y_test['Survived']=best_estimator.predict(X_test).astype(np.int32)\ny_test['Survived']=forest_classifier.predict(X_test).astype(np.int32)\ny_test=y_test.set_index('PassengerId')","e6391dea":"y_test.to_csv('result.csv')","6ffb5bfd":"filename='random_forest.pickle'\npickle.dump(forest_classifier, open(filename, 'wb'))","648aee55":"X_train=total_df[total_df['trainable']==1].drop(columns=drop_features)\ny_train=total_df[total_df['trainable']==1]['Survived']\n\ntrain_X, val_X, train_y, val_y = train_test_split(X_train, y_train, random_state=27)\n\nrf_model=RandomForestClassifier(n_estimators=100,max_depth=10,max_leaf_nodes=12,\n                           min_samples_leaf=10,min_samples_split=5,n_jobs=-1,random_state=27)\nrf_model.fit(train_X,train_y)\nprint(np.sum(rf_model.predict(val_X)==val_y)\/len(val_X))","782676d0":"y_pred=rf_model.predict(val_X)\nconf_mat=confusion_matrix(val_y,y_pred)\n\nconf_df=pd.DataFrame(conf_mat,\n                     columns=['predictive negative','predictive positive'],\n                     index=['true negative','true positive'])\nconf_df['total']=conf_df.sum(axis=1)\nconf_df.loc['total']=conf_df.sum(axis=0)\n\nconf_df.loc['precision',['predictive negative']]=conf_df.loc['true negative',['predictive negative']]\/conf_df.loc['total',['predictive negative']]\nconf_df.loc['precision',['predictive positive']]=conf_df.loc['true positive',['predictive positive']]\/conf_df.loc['total',['predictive positive']]\n\nconf_df=conf_df.applymap(lambda x: int(x) if x.is_integer() else np.round(x,2))\n\n#sns.heatmap(conf_df,annot=True,linewidths=2)\nconf_df","086acadd":"#rf_model=forest_classifier\nperm = PermutationImportance(rf_model, random_state=1).fit(X_train,y_train)\neli5.show_weights(perm, feature_names = total_df.drop(columns=drop_features).columns.values)","c85124f1":"from pdpbox import pdp, get_dataset, info_plots\n\nage_classify\n#rf_model=forest_classifier\n\nfeature_to_plot='Age'\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rf_model, dataset=total_df[total_df['trainable']==1],\n                            feature=feature_to_plot,\n                            model_features=total_df.drop(columns=drop_features).columns)\n\nfor index,age in enumerate(age_clusters):\n    print(f'Age {index} is around {int(age)} years')\n\n\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","90155554":"#rf_model=forest_classifier\n\nfeature_to_plot='Sex'\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rf_model, dataset=total_df[total_df['trainable']==1],\n                            feature=feature_to_plot,\n                            model_features=total_df.drop(columns=drop_features).columns,)\n\nprint(f'Sex 0 is female')\nprint(f'Sex 1 is male')\n\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","162f31cd":"#rf_model=forest_classifier\n\nfeature_to_plot='prob_survival'\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rf_model, dataset=total_df[total_df['trainable']==1],\n                            feature=feature_to_plot,\n                            model_features=total_df.drop(columns=drop_features).columns)\n\n# print(prob_survival_clusters)\nfor index,val in enumerate(prob_survival_clusters):\n    print(f'prob_survival {index} is around {np.round(prob_survival_clusters[index],3)}')\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","df53264e":"#rf_model=forest_classifier\n\nfeature_to_plot='fare_per_fsize'\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rf_model, dataset=total_df[total_df['trainable']==1],\n                            feature=feature_to_plot,\n                            model_features=total_df.drop(columns=drop_features).columns)\n\nfor index,val in enumerate(fare_clusters):\n    print(f'fare_per_fsize {index} is around {np.round(fare_clusters[index],3)}')\n    \n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\n\nplt.show()","1445a38c":"#rf_model=forest_classifier\n\nfeature_to_plot='Pclass'\n# Create the data that we will plot\npdp_goals = pdp.pdp_isolate(model=rf_model, dataset=total_df[total_df['trainable']==1],\n                            feature=feature_to_plot,\n                            model_features=total_df.drop(columns=drop_features).columns)\n\n# plot it\npdp.pdp_plot(pdp_goals, feature_to_plot)\nplt.show()","af5ff84b":"<h1 style=\"color: #00FF00\"> Model Explainability <\/h1>\nIn this part I did some model explainability to check that the model somehow represents the visualizations above. Furthermore, it is giving prediction that actually makes sense. <br\/>\nThese ideas were taken from kaggles courses.","49d5f307":"The 2 figures above shows that the model learned the relationship between age and survival rate.<br\/>\nIn the titanic (or the movie at least), the priority was women and children. The graph and plot shows that the kids of age around 5 had a better chance of surviving. For those around 21 and 34 had less surviving chances. Yet, they were higher than people of age around 52 and higher. It is typical that the older the person the less its ability to survive and they sacrifice for the younger generation.<br\/>\nThe curves shows that as the age increases, the surviving chances decreases. I think it is logical enough.","9a9267eb":"we can treate this feature as log fare per family member. As fare increases, the richer the person. According to the plots, the richier the person, the higher the surviving chance. Sure there is an element of randomness (specially that the priority was to save women and children and then men) which can be seen between 0 and 2.","5e4e9cd2":"This plot also verifies the relation between surviving and Pclass. class 1 is higher than class 2 higher than class 3. the higher the class, the higher the surviving chances.","41d46e04":"From the first section we know females had better chance to survive than men. This relation was learned by the model.","8e2279af":"<h1 style=\"color: #00FF00\"> CONCLUSION <\/h1>\nAs an overall, the model did learn the same information we saw while visualizing the data. Thus, the model did learn useful relationships from the data. I hope this notebook helps you.\n\n<h4 style=\"color: #00FF00\">\n    Thank you for reading. Hope it was helpful.    \n<\/h4>","035d776a":"<h1 style=\"color: #00FF00\"> LOAD THE DATA <\/h1>","3e46decb":"<p>20% of the data does not contain Age. It can still be used<\/p>\n<p>While 77% of the data does not contain a Cabin value. It will not be used since there are a lot of missing values.<\/p>","48045dd9":"<h1 style=\"color: #00FF00\"> Generate the results <\/h1>","5514a061":"I want to point to these 2 notebooks for pointing out a great feature 'Family_Survival':\n\n[blood is thicker than water friendship forever](http:\/\/www.kaggle.com\/shunjiangxu\/blood-is-thicker-than-water-friendship-forever)\n\n[simple end to end ml workflow top 5 score](https:\/\/www.kaggle.com\/josh24990\/simple-end-to-end-ml-workflow-top-5-score)","3711416e":"<h1 style=\"color: #00FF00\"> Visualization <\/h1>","2233b43a":"<h1 style=\"color: #00FF00\"> Prepare The Data <\/h1>","189c014d":"<b>Save the model:<\/b>","ea9b5199":"It is expected that as probability of survival increases, the better the chance of actually surviving."}}