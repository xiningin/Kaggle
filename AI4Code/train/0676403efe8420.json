{"cell_type":{"2d941f1d":"code","10ff3d16":"code","b9aacfb4":"code","c0dc0b7d":"code","a7d32636":"code","3f1c946f":"code","b8306bae":"code","c95c49d8":"code","5bbe840e":"code","0a4eebc9":"code","540722b6":"code","20e979e1":"code","216b959d":"code","e8c0f42e":"code","5c055103":"code","4cb95187":"code","cee91806":"code","601feb12":"code","cb71b183":"code","2998c1f4":"markdown","a3baca5a":"markdown","ff3ac6a7":"markdown","80dd8f8c":"markdown","b6c03e3c":"markdown","7c45098c":"markdown","35c12ae7":"markdown","5833c6c7":"markdown","c4fa399d":"markdown"},"source":{"2d941f1d":"#Get the GPU info\ngpu_info = !nvidia-smi\ngpu_info = '\\n'.join(gpu_info)\nprint(gpu_info)","10ff3d16":"!pip install cudf\n%time\nimport cudf, io, requests\nfrom io import StringIO\n\nurl = \"https:\/\/github.com\/plotly\/datasets\/raw\/master\/tips.csv\"\ncontent = requests.get(url).content.decode('utf-8')\n\ntips_df = cudf.read_csv(StringIO(content))\nprint(tips_df.head())\ntips_df['tip_percentage'] = tips_df['tip'] \/ tips_df['total_bill'] * 100\n\n# display average tip by dining party size\nprint(tips_df.groupby('size').tip_percentage.mean())","b9aacfb4":"import os\nimport cupy as cp\nimport pandas as pd\nimport dask_cudf","c0dc0b7d":"s = cudf.Series([1,2,3,None,4])\ns","a7d32636":"ds = dask_cudf.from_cudf(s, npartitions=2) \nds.compute()","3f1c946f":"#Creating a cudf.DataFrame and a dask_cudf.DataFrame by specifying values for each column\ndf = cudf.DataFrame({'a': list(range(20)),\n                     'b': list(reversed(range(20))),\n                     'c': list(range(20))\n                    })\ndf\n#Create dask_cudf.DataFrame from cudf.DataFrame \nddf = dask_cudf.from_cudf(df, npartitions=2) \nddf.compute()","b8306bae":"#Time Series: DataFrames supports datetime typed columns, which allow users to interact with \n# and filter data based on specific timestamps.\nimport datetime as dt\nimport cupy as cp\ndate_df = cudf.DataFrame()\ndate_df['date']=pd.date_range('01\/01\/2018',periods=85, freq='D')\ndate_df['value']=cp.random.rand(len(date_df))\nsearch_date = dt.datetime.strptime('2018-11-23', '%Y-%m-%d')\ndate_df.query('date <= @search_date')\n","c95c49d8":"import os\nimport pandas as pd \nimport numpy as np\nimport datetime as dt \nimport math\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Input, Dropout, Dense,LSTM, Activation\nfrom tensorflow.keras.optimizers import Adam\nimport tensorflow.keras\nimport matplotlib.pyplot as plt \nfrom itertools import cycle\nimport plotly.graph_objects as go\nimport plotly.express as xp\nimport plotly.subplots as sp\nimport cupy as cp\ntf.test.gpu_device_name()\nfrom sklearn.metrics import mean_absolute_error\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","5bbe840e":"data=cudf.read_csv('..\/input\/google-stock-price-all-time\/google.csv')\ndata.to_pandas()\ndata.columns=data.columns.str.lower()\ndata=data.set_index('date')\ndata['date']=cudf.to_datetime(data.index)\ndata = data.drop(columns = ['date'])\ndata.shape","0a4eebc9":"target_col = 'close'\n#data split train and test\ndef split_test_train(df,split=0.2):\n    split_size=len(df) - int(test_size * len(df))\n    train =  df.iloc[:split_size,:]\n    test  =  df.iloc[split_size :,:]\n    return train_pd,test_pd\ndef extract_window_data(df, window_len=5, zero_base=True):\n    window_data = []\n    for idx in range(len(df) - window_len):\n        tmp = df[idx: (idx + window_len)].copy()\n        if zero_base:\n            tmp = normalise_zero_base(tmp)\n        window_data.append(tmp.values)\n    return cp.array(window_data)\n#window normalization\ndef normalise_zero_base(df):\n    return cudf.DataFrame(df.values \/ df.values[0] - 1)\n\n#Data preparation\ndef prepare_data(df, target_col, window_len=10, zero_base=True, test_size=0.2):\n    train_data, test_data = train_test_split(df, test_size=test_size)\n    X_train = extract_window_data(train_data, window_len, zero_base)\n    X_test = extract_window_data(test_data, window_len, zero_base)\n    y_train = train_data[target_col][window_len:].values\n    y_test = test_data[target_col][window_len:].values\n    if zero_base:\n        y_train = y_train \/ train_data[target_col][:-window_len].values - 1\n        y_test = y_test \/ test_data[target_col][:-window_len].values - 1\n\n    return train_data, test_data, X_train, X_test, y_train, y_test","540722b6":"train, test = train_test_split(data, test_size=0.2)\ntrain_pd = train.to_pandas()\ntest_pd = test.to_pandas()\ndef line_plot(line1, line2, label1=None, label2=None, title='', lw=2):\n    fig, ax = plt.subplots(1, figsize=(13, 7))\n    ax.plot(line1, label=label1, linewidth=lw)\n    ax.plot(line2, label=label2, linewidth=lw)\n    ax.set_ylabel('price [CAD]', fontsize=14)\n    ax.set_title(title, fontsize=16)\n    ax.legend(loc='best', fontsize=16);\nline_plot(train_pd[target_col], test_pd[target_col], 'training', 'test', title='')","20e979e1":"#model\ndef build_model(input_data):\n    model=Sequential()\n    model.add(LSTM(256,return_sequences=True,input_shape=(input_data.shape[0],input_data.shape[1])))\n    model.add(LSTM(256,return_sequences=True))\n    model.add(Dropout(0.2))\n    model.add(Dense(1))\n    model.add(Activation('linear'))\n\n    \n    return model\n\nnp.random.seed(42)\nwindow_len = 5\ntest_size = 0.2\nzero_base = True\nlstm_neurons = 100\nepochs = 20\nbatch_size = 32\nloss = 'mse'\ndropout = 0.2\noptimizer = 'adam'\nmodel=build_model(train_data)\ntrain, test, X_train, X_test, y_train, y_test = prepare_data(\n    data, target_col, window_len=window_len, zero_base=zero_base, test_size=test_size)\noptimizer=tf.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer='adam',loss='mse')","216b959d":"import tensorflow as tf\nsess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))\ntf.test.gpu_device_name()","e8c0f42e":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","5c055103":"arr_tf_xt = tf.experimental.dlpack.from_dlpack(X_train.toDlpack())\narr_tf_yt = tf.experimental.dlpack.from_dlpack(y_train.toDlpack())","4cb95187":"history=model.fit(arr_tf_xt,arr_tf_yt,epochs=100,verbose=1,batch_size=32,shuffle=True)","cee91806":"arr_tf_xtest = tf.experimental.dlpack.from_dlpack(X_test.toDlpack())\narr_tf_ytest = tf.experimental.dlpack.from_dlpack(y_test.toDlpack())\ntargets = test[target_col][window_len:]\npreds = model.predict(arr_tf_xtest).squeeze()\n","601feb12":"testcd = test.to_pandas()\ntargetscd = targets.to_pandas()","cb71b183":"preds = testcd[target_col].values[:-window_len] * (preds + 1)\npreds = cudf.Series(index=targets.index, data=preds)\npredscd = preds.to_pandas()\nline_plot(targetscd, predscd , 'actual', 'prediction', lw=3)","2998c1f4":"# More AI and ML libraries from RAPIDS.AI can be found [here](https:\/\/docs.rapids.ai\/api)","a3baca5a":"### This notebook demonstrates the CuDF library usage.  is a Python GPU DataFrame library (built on the Apache Arrow columnar memory format) for loading, joining, aggregating, filtering, and otherwise manipulating tabular data using a DataFrame style API.\n\n ### It provides a pandas-like API that will be familiar to data engineers & data scientists, so they can use it to easily accelerate their workflows without going into the details of CUDA programming.","ff3ac6a7":"# Get started with CuDF","80dd8f8c":"# Accelerating Python in Banking [link](chrome-extension:\/\/efaidnbmnnnibpcajpcglclefindmkaj\/viewer.html?pdfurl=https%3A%2F%2Finfo.nvidia.com%2Frs%2F156-OFN-742%2Fimages%2FFS%2520Webinar%2520Deck.pdf&clen=1977023&chunk=true)","b6c03e3c":"# EDA","7c45098c":"# Preprocesing , Data preparation & Model","35c12ae7":"# Google stock prediction with CuDF","5833c6c7":"# prediction ","c4fa399d":"## Dask :\n> ### is a flexible library for parallel computing in Python that makes scaling out your workflow smooth and simple. On the CPU, Dask uses Pandas to execute operations in parallel on DataFrame partitions.\n\n## Dask-cuDF :\n> ### extends Dask where necessary to allow its DataFrame partitions to be processed by cuDF GPU DataFrames as opposed to Pandas DataFrames. For instance, when you call dask_cudf.read_csv(...), your cluster\u2019s GPUs do the work of parsing the CSV file(s) with underlying cudf.read_csv()."}}