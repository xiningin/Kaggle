{"cell_type":{"64c0a11f":"code","6c51b127":"code","fdc62893":"code","35f4f58f":"code","d1ffe345":"code","cc0e505e":"code","7a55ed5b":"code","15ed2413":"code","379e7382":"code","d541d2da":"code","2a04a40b":"code","39a3408d":"code","1a9a4116":"code","70d23e98":"code","0aec05a6":"code","4f4d1cec":"code","8e7b5e5e":"code","35231286":"code","d4394ac9":"code","9cd333f5":"code","f8f26058":"code","788e8d38":"code","158f7db6":"code","3b1e0b6f":"code","32f9e915":"code","83be4b3d":"code","e4985b94":"code","4358b75a":"code","a1d651e0":"code","66939ed2":"code","cac1f696":"code","6969799e":"code","a24c16b3":"markdown","90624cdc":"markdown","2526ebd1":"markdown","ba757f16":"markdown","36db5b61":"markdown","cfddea72":"markdown","42e29851":"markdown","c099cf82":"markdown","244a7eca":"markdown","74613bbe":"markdown"},"source":{"64c0a11f":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nimport os\nimport re\nimport cv2\nimport math\nimport random","6c51b127":"import keras\nfrom keras import *\nfrom keras.layers import *\nimport tensorflow_addons as tfa\n\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()","fdc62893":"\nmonet_names = tf.io.gfile.glob(r\"..\/input\/gan-getting-started\/monet_tfrec\/*.tfrec\")\nprint(monet_names)\nphoto_names = tf.io.gfile.glob(r\"..\/input\/gan-getting-started\/photo_tfrec\/*.tfrec\")","35f4f58f":"def count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) \n         for filename in filenames]\n    return np.sum(n)\n\nprint(f\"Monet file tfrecords: {len(monet_names)}\")\nprint(f\"Photo file tfrecords: {len(photo_names)}\")\nprint(f\"Monet images: {count_data_items(monet_names)}\")\nprint(f\"Photo images: {count_data_items(photo_names)}\")","d1ffe345":"def prepare_image(img, dim = 256):    \n    img = tf.image.decode_jpeg(img, channels = 3)\n    img = (tf.cast(img, tf.float32) \/ 255.0) - 1\n    img = tf.reshape(img, [dim, dim, 3])\n    return img\n\ndef read_tfrecord(example):\n    tfrec_format = {\n        'image' : tf.io.FixedLenFeature([], tf.string),\n        'image_name' : tf.io.FixedLenFeature([], tf.string),\n        \"target\": tf.io.FixedLenFeature([], tf.string)\n    }   \n    \n    example = tf.io.parse_single_example(example, tfrec_format)\n    image = prepare_image(example['image'])\n    return image","cc0e505e":"def load_dataset(filenames, labeled=True, ordered=False):\n    dataset = tf.data.TFRecordDataset(filenames)\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n    return dataset","7a55ed5b":"monet_ds = load_dataset(monet_names, labeled=True).batch(1)\nphoto_ds = load_dataset(photo_names, labeled=True).batch(1)","15ed2413":"base_path = '..\/input\/gan-getting-started\/'\nmonet_path = os.path.join(base_path, 'monet_jpg')\nphoto_path = os.path.join(base_path, 'photo_jpg')","379e7382":"def batch_visualization(path, n_images, is_random=True, figsize=(16, 16)):\n    plt.figure(figsize=figsize)\n    \n    w = int(n_images ** .5)\n    h = math.ceil(n_images \/ w)\n    \n    all_names = os.listdir(path)\n    \n    image_names = all_names[:n_images]\n    if is_random:\n        image_names = random.sample(all_names, n_images)\n    \n    for ind, image_name in enumerate(image_names):\n        img = cv2.imread(os.path.join(path, image_name))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n        plt.subplot(h, w, ind + 1)\n        plt.imshow(img)\n        plt.axis('off')\n    \n    plt.show()","d541d2da":"batch_visualization(monet_path,6)","2a04a40b":"batch_visualization(photo_path,6)","39a3408d":"rand_monet = r\"..\/input\/gan-getting-started\/monet_jpg\/0260d15306.jpg\"\nrand_photo = r\"..\/input\/gan-getting-started\/photo_jpg\/000ded5c41.jpg\"","1a9a4116":"def color_hist_visualization(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    layers = ['red', 'green', 'blue']\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 4, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    for i in range(len(layers)):\n        plt.subplot(1, 4, i + 2)\n        plt.hist(\n            img[:, :, i].reshape(-1),\n            bins=25,\n            alpha=0.5,\n            color=layers[i],\n            density=True\n        )\n        plt.title(layers[i])\n        plt.xlim(0, 255)\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()","70d23e98":"print(\"Monet: \")\ncolor_hist_visualization(rand_monet)\nprint(\"\\nPhoto: \")\ncolor_hist_visualization(rand_photo)","0aec05a6":"def channels_visualization(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 4, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    for i in range(3):\n        plt.subplot(1, 4, i + 2)\n        tmp_img = np.full_like(img, 0)\n        tmp_img[:, :, i] = img[:, :, i]\n        plt.imshow(tmp_img)\n        plt.xlim(0, 255)\n        plt.xticks([])\n        plt.yticks([])\n    plt.show()","4f4d1cec":"print(\"Monet: \")\nchannels_visualization(rand_monet)\nprint(\"\\nPhoto: \")\nchannels_visualization(rand_photo)","8e7b5e5e":"def grayscale_visualization(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 3, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    plt.subplot(1, 3, 2)\n    tmp_img = np.full_like(img, 0)\n    for i in range(3):\n        tmp_img[:, :, i] = img.mean(axis=-1)\n    plt.imshow(tmp_img)\n    plt.axis('off')\n    \n    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n    plt.subplot(1,3,3)\n    plt.imshow(img)\n    plt.axis(\"off\")\n    \n    plt.show()","35231286":"print(\"Monet: \")\ngrayscale_visualization(rand_monet)\nprint(\"\\nPhoto: \")\ngrayscale_visualization(rand_photo)","d4394ac9":"def color_graph(image_path, figsize=(16, 4)):\n    plt.figure(figsize=figsize)\n    \n    img = cv2.imread(image_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) \n    plt.subplot(1, 2, 1)\n    plt.imshow(img)\n    plt.axis('off')\n    \n    chans = cv2.split(img)\n    colors = (\"b\", \"g\", \"r\")\n    plt.subplot(1, 2, 2)\n    plt.title(\"'Flattened' Color Histogram\")\n    plt.xlabel(\"Bins\")\n    plt.ylabel(\"# of Pixels\")\n    features = []\n    # loop over the image channels\n    for (chan, color) in zip(chans, colors):\n        # create a histogram for the current channel and\n        # concatenate the resulting histograms for each\n        # channel\n        hist = cv2.calcHist([chan], [0], None, [256], [0, 256])\n        features.extend(hist)\n        # plot the histogram\n        plt.plot(hist, color = color)\n        plt.xlim([0, 256])\n    \n    plt.show()","9cd333f5":"print(\"Monet: \")\ncolor_graph(rand_monet)\nprint(\"\\nPhoto: \")\ncolor_graph(rand_photo)","f8f26058":"from shutil import copyfile\ncopyfile(src = \"..\/usr\/lib\/monet_using_gan_utility\/monet_using_gan_utility.py\", dst = \"..\/working\/monet.py\")\nfrom monet import *","788e8d38":"generator_g = Generator()\ntf.keras.utils.plot_model(generator_g, dpi=48)","158f7db6":"discriminator_g = Discriminator()\ntf.keras.utils.plot_model(discriminator_g, dpi=48)","3b1e0b6f":"with strategy.scope():\n    monet_generator = Generator() # transforms photos to Monet-esque paintings\n    photo_generator = Generator() # transforms Monet paintings to be more like photos\n\n    monet_discriminator = Discriminator() # differentiates real Monet paintings and generated Monet paintings\n    photo_discriminator = Discriminator() # differentiates real photos and generated photos","32f9e915":"class CycleGan(keras.Model):\n    def __init__(\n        self,\n        monet_generator,\n        photo_generator,\n        monet_discriminator,\n        photo_discriminator,\n        lambda_cycle=10):\n        \n        super(CycleGan, self).__init__()\n        self.m_gen = monet_generator\n        self.p_gen = photo_generator\n        self.m_disc = monet_discriminator\n        self.p_disc = photo_discriminator\n        self.lambda_cycle = lambda_cycle\n        \n    def compile(\n        self,\n        m_gen_optimizer,\n        p_gen_optimizer,\n        m_disc_optimizer,\n        p_disc_optimizer,\n        gen_loss_fn,\n        disc_loss_fn,\n        cycle_loss_fn,\n        identity_loss_fn):\n        \n        super(CycleGan, self).compile()\n        self.m_gen_optimizer = m_gen_optimizer\n        self.p_gen_optimizer = p_gen_optimizer\n        self.m_disc_optimizer = m_disc_optimizer\n        self.p_disc_optimizer = p_disc_optimizer\n        self.gen_loss_fn = gen_loss_fn\n        self.disc_loss_fn = disc_loss_fn\n        self.cycle_loss_fn = cycle_loss_fn\n        self.identity_loss_fn = identity_loss_fn\n        \n    def train_step(self, batch_data):\n        real_monet, real_photo = batch_data\n        \n        with tf.GradientTape(persistent=True) as tape:\n            # photo to monet back to photo\n            fake_monet = self.m_gen(real_photo, training=True)\n            cycled_photo = self.p_gen(fake_monet, training=True)\n\n            # monet to photo back to monet\n            fake_photo = self.p_gen(real_monet, training=True)\n            cycled_monet = self.m_gen(fake_photo, training=True)\n\n            # generating itself\n            same_monet = self.m_gen(real_monet, training=True)\n            same_photo = self.p_gen(real_photo, training=True)\n\n            # discriminator used to check, inputing real images\n            disc_real_monet = self.m_disc(real_monet, training=True)\n            disc_real_photo = self.p_disc(real_photo, training=True)\n\n            # discriminator used to check, inputing fake images\n            disc_fake_monet = self.m_disc(fake_monet, training=True)\n            disc_fake_photo = self.p_disc(fake_photo, training=True)\n\n            # evaluates generator loss\n            monet_gen_loss = self.gen_loss_fn(disc_fake_monet)\n            photo_gen_loss = self.gen_loss_fn(disc_fake_photo)\n\n            # evaluates total cycle consistency loss\n            total_cycle_loss = self.cycle_loss_fn(real_monet, cycled_monet, self.lambda_cycle) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n\n            # evaluates total generator loss\n            total_monet_gen_loss = monet_gen_loss + total_cycle_loss + self.identity_loss_fn(real_monet, same_monet, self.lambda_cycle)\n            total_photo_gen_loss = photo_gen_loss + total_cycle_loss + self.identity_loss_fn(real_photo, same_photo, self.lambda_cycle)\n\n            # evaluates discriminator loss\n            monet_disc_loss = self.disc_loss_fn(disc_real_monet, disc_fake_monet)\n            photo_disc_loss = self.disc_loss_fn(disc_real_photo, disc_fake_photo)\n\n        # Calculate the gradients for generator and discriminator\n        monet_generator_gradients = tape.gradient(total_monet_gen_loss,\n                                                  self.m_gen.trainable_variables)\n        photo_generator_gradients = tape.gradient(total_photo_gen_loss,\n                                                  self.p_gen.trainable_variables)\n\n        monet_discriminator_gradients = tape.gradient(monet_disc_loss,\n                                                      self.m_disc.trainable_variables)\n        photo_discriminator_gradients = tape.gradient(photo_disc_loss,\n                                                      self.p_disc.trainable_variables)\n\n        # Apply the gradients to the optimizer\n        self.m_gen_optimizer.apply_gradients(zip(monet_generator_gradients,\n                                                 self.m_gen.trainable_variables))\n\n        self.p_gen_optimizer.apply_gradients(zip(photo_generator_gradients,\n                                                 self.p_gen.trainable_variables))\n\n        self.m_disc_optimizer.apply_gradients(zip(monet_discriminator_gradients,\n                                                  self.m_disc.trainable_variables))\n\n        self.p_disc_optimizer.apply_gradients(zip(photo_discriminator_gradients,\n                                                  self.p_disc.trainable_variables))\n        \n        return {\n            \"monet_gen_loss\": total_monet_gen_loss,\n            \"photo_gen_loss\": total_photo_gen_loss,\n            \"monet_disc_loss\": monet_disc_loss,\n            \"photo_disc_loss\": photo_disc_loss\n        }","83be4b3d":"with strategy.scope():\n    def discriminator_loss(real, generated):\n        real_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(real), real)\n        generated_loss = tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.zeros_like(generated), generated)\n        total_disc_loss = real_loss + generated_loss\n        return total_disc_loss * 0.5","e4985b94":"with strategy.scope():\n    def generator_loss(generated):\n        return tf.keras.losses.BinaryCrossentropy(from_logits=True, reduction=tf.keras.losses.Reduction.NONE)(tf.ones_like(generated), generated)\n    \nwith strategy.scope():\n    def calc_cycle_loss(real_image, cycled_image, LAMBDA):\n        loss1 = tf.reduce_mean(tf.abs(real_image - cycled_image))\n        return LAMBDA * loss1\n\nwith strategy.scope():\n    def identity_loss(real_image, same_image, LAMBDA):\n        loss = tf.reduce_mean(tf.abs(real_image - same_image))\n        return LAMBDA * 0.5 * loss","4358b75a":"with strategy.scope():\n    monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n\n    monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n    photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","a1d651e0":"with strategy.scope():\n    cycle_gan_model = CycleGan(\n        monet_generator, photo_generator, monet_discriminator, photo_discriminator\n    )\n\n    cycle_gan_model.compile(\n        m_gen_optimizer = monet_generator_optimizer,\n        p_gen_optimizer = photo_generator_optimizer,\n        m_disc_optimizer = monet_discriminator_optimizer,\n        p_disc_optimizer = photo_discriminator_optimizer,\n        gen_loss_fn = generator_loss,\n        disc_loss_fn = discriminator_loss,\n        cycle_loss_fn = calc_cycle_loss,\n        identity_loss_fn = identity_loss\n    )","66939ed2":"from IPython.display import clear_output","cac1f696":"try:\n    cycle_gan_model.fit(\n        tf.data.Dataset.zip((monet_ds, photo_ds)),\n        epochs=30)\nexcept:\n    clear_output()","6969799e":"print(\"Above fitting gives an error:\\nInvalidArgumentError: Unable to parse tensor proto [Op:DatasetCardinality]\")","a24c16b3":"# Building the cycleGAN model\nImage-to-image translation frameworks are frequently difficult to train because of the need for perfect pairs; the CycleGAN solves this by making this an unpaired domain translation.\n\nThe CycleGAN has three losses:\n- Cycle-consistent, which measures the difference between the original image and an image translated into a different domain and back again\n- Adversarial, which ensures realistic images\n- Identity, which preserves the color space of the image\n\nPractical applications of the CycleGAN include self-driving car training and exten- sions that allow us to create different styles of images during the translation process.","90624cdc":"All the solutions are cordially invited!","2526ebd1":"From here we are declaring generator and discriminator spaces of GANs which will work the same as encoder and decoder of autoencoders. To know more about autoencoders, [click here](https:\/\/www.kaggle.com\/discussion\/240924)","ba757f16":"# Loading the data\n- ***read_tfrecord()***:     takes a file and parse to get required variables.\n- ***prepare_image()***:     takes an image in tensor datatype and returns after reshaping and normalizing it.\n- ***get_dataset()***:       reads the TFRecords file and maps with the help of read_tfrecord function.\n- _**data_augment()**_:      as the name suggests augment the image data of tensor type for better accuracy.\n- _**get_gan_dataset()**_:   returns our main datasets which we will use for the training purposes","36db5b61":"Data is stored as tfrecords for faster training and processing. `tf.io.gfile.glob` returns the list of files that manages the pattern given. For eg: \"*.tfrec\"","cfddea72":"## Individual images\n- color_hist_visualization(): Gives the RGB bar graph of photo\n- channels_visualization(): Gives the RGB channel graph of photo\n- grayscale_visualization(): Converts the RGB channel to grayscale\n- color_graph(): Gives the RGB histogram graph","42e29851":"Training","c099cf82":"# Training a CycleGAN\nIn the CycleGAN\u2019s case, the architecture is complex, and as a result, we need a structure that allows us to keep accessing the original attributes and methods that we have defined. As a result, we will write out the CycleGAN as a Python class of its own with methods to build the Generator and Discriminator, and run the training.\n\nFor the training to execute we will need a seperate Generator() and discriminator() function which we will feed to CycleGAN as methods which in turn needs the upsample() and downsample() of image.\n\n- For downsampling we are using the Conv2D() as primary layer and LeakyReLU() as activation\n- For upsampling we are using the Conv2DTranspose() as primary layer and Dropout() at 0.3, ReLU() as secondary layers\n\nFor all of these I have made a utility script as this notebook kernel size was exceeding","244a7eca":"# Dataset visualisation\n- batch_visualisation(): shows the given number of images in given path","74613bbe":"# I\u2019m Something of a Painter Myself\nI wanted to be a picasso like painter, but haven't got enought talent to paint. So using the GAN I am noe transforming the photo images in this competition into a monet. I am using the tfrecords to get the better out of the model in training. I tried to explain every step to the best of my knowledge! Please do support if found interesting \n\nAlso I took some references from the following resources \n\n[Link 1](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics)\n[Link 2](https:\/\/www.kaggle.com\/drzhuzhe\/monet-cyclegan-tutorial)\n[Link 3](https:\/\/kozodoi.me\/python\/deep%20learning\/computer%20vision\/competitions\/2020\/08\/30\/pre-training.html)"}}