{"cell_type":{"2f8c10ed":"code","955cf9f8":"code","2085606e":"code","5a5c74d1":"code","fc9bc024":"code","7cf6495f":"code","87fb6011":"code","c02da48a":"code","177a6be5":"code","1849d7b5":"code","456c0821":"code","5211bafd":"code","297fbaec":"code","31eb088d":"code","6954da56":"code","c4eb4630":"code","f61ba35b":"code","86aa8598":"code","06dcccb3":"code","e01d37d0":"code","03d8e3ed":"code","21456f3e":"code","7d335275":"markdown","3c3cfad0":"markdown","a4e2b293":"markdown","595b4593":"markdown","9435f0d6":"markdown","ea3aca9f":"markdown","3823ef42":"markdown","262bfa57":"markdown","f3a2d557":"markdown","2077ffc3":"markdown","5f4a845a":"markdown","79370adb":"markdown","40475be0":"markdown","008d05ea":"markdown","128850b8":"markdown","d4b4bb83":"markdown"},"source":{"2f8c10ed":"import pandas as pd\nimport numpy as np\nimport math\n\nimport librosa as lb # https:\/\/librosa.github.io\/librosa\/\nimport soundfile as sf # https:\/\/pysoundfile.readthedocs.io\/en\/latest\/\n\nimport os\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.cbook import boxplot_stats","955cf9f8":"!ls ..\/input\/respiratory-sound-database\/respiratory_sound_database\/Respiratory_Sound_Database","2085606e":"#load patient diagnosis.csv\n\ndiag_csv = '..\/input\/respiratory-sound-database\/respiratory_sound_database\/Respiratory_Sound_Database\/patient_diagnosis.csv'\ndiagnosis = pd.read_csv(diag_csv, names=['pId', 'diagnosis'])\ndiagnosis.head()","5a5c74d1":"ds = diagnosis['diagnosis'].unique()\nds","fc9bc024":"#get all text files\naudio_text_loc = '..\/input\/respiratory-sound-database\/respiratory_sound_database\/Respiratory_Sound_Database\/audio_and_txt_files'\nfiles = [s.split('.')[0] for s in os.listdir(path = audio_text_loc) if '.txt' in s]\nfiles","7cf6495f":"def tokenize_file(filename):\n    return filename.split('_')","87fb6011":"#read each file\n\nfiles_ = []\nfor f in files:\n    df = pd.read_csv(audio_text_loc + '\/' + f + '.txt', sep='\\t', names=['start', 'end', 'crackles', 'wheezes'])\n    df['filename'] = f\n    #get filename features\n    f_features = tokenize_file(f)\n    df['pId'] = f_features[0]\n    df['ac_mode'] = f_features[3]\n    \n    files_.append(df)\n    \nfiles_df = pd.concat(files_)\nfiles_df.reset_index()\nfiles_df.head()","c02da48a":"files_df.info()","177a6be5":"diagnosis.info()","1849d7b5":"files_df['pId'] = files_df['pId'].astype('float64')\nfiles_df.info()","456c0821":"files_df = pd.merge(files_df, diagnosis, on='pId')\nfiles_df.head()","5211bafd":"#code taken from eatmygoose https:\/\/www.kaggle.com\/eatmygoose\/cnn-detection-of-wheezes-and-crackles\ndef slice_data(start, end, raw_data,  sample_rate):\n    max_ind = len(raw_data) \n    start_ind = min(int(start * sample_rate), max_ind)\n    end_ind = min(int(end * sample_rate), max_ind)\n    return raw_data[start_ind: end_ind]","297fbaec":"files_df['len_per_slice'] = files_df['end'].sub(files_df['start'], axis = 0) \nmax_len_per_slice = max(files_df['len_per_slice'])\nmax_len_per_slice","31eb088d":"plt.scatter(files_df['len_per_slice'], y=files_df.index)","6954da56":"box = plt.boxplot(files_df['len_per_slice'])","c4eb4630":"force_max_len = math.ceil(boxplot_stats(files_df['len_per_slice'])[0]['whishi'])\nforce_max_len","f61ba35b":"def compute_len(samp_rate=22050, time=force_max_len, acquisition_mode=0):\n    '''Computes the supposed length of sliced data\n        samp_size = sample size from the data\n        samp_rate = sampling rate. by default since we're working on 24-bit files, we'll use 96kHz\n        time = length of time for the audio file. by default we'll use the max we have which is 5.48\n        acquisition_mode = either mono or stereo. 0 for mono, 1 for stereo\n    '''\n    comp_len = 0\n    if acquisition_mode == 1: #ac mode is single channel which means it's 'mono'\n        comp_len = samp_rate * time\n    else: #stereo\n        comp_len = (samp_rate * time) * 2\n\n    return comp_len","86aa8598":"#create output path\nos.makedirs('output')","06dcccb3":"for d in ds:\n    path = os.path.join('output', d)\n    os.makedirs(path)","e01d37d0":"!ls","03d8e3ed":"i = 0 #iterator for file naming\n\nfor idx, row in files_df.iterrows():\n    filename = row['filename']\n    start = row['start']\n    end = row['end']\n    diag = row['diagnosis']\n    \n    #check len and force to 6 sec if more than that\n    if force_max_len < end - start:\n        end = start + force_max_len\n    \n    aud_loc = audio_text_loc + '\/' + f + '.wav'\n    \n    if idx != 0:\n        if files_df.iloc[idx-1]['filename'] == filename:\n            i=i+1\n        else:\n            i=0\n    n_filename = filename + '_' + str(i) + '.wav'\n    path = 'output\/' + diag + '\/' + n_filename\n    \n    print('processing ' + n_filename + '...')\n\n    data, samplingrate = lb.load(aud_loc)\n    sliced_data = slice_data(start=start, end=end, raw_data=data, sample_rate=samplingrate)\n    \n    #pad audio if < forced_max_len\n    a_len = compute_len(samp_rate=samplingrate, acquisition_mode=row['ac_mode']=='sc')\n    padded_data = lb.util.pad_center(sliced_data, a_len)\n\n    sf.write(file=path, data=padded_data, samplerate=samplingrate)\n","21456f3e":"!ls output\/","7d335275":"Now that we have our files list, we have to read each one to get the crackles and wheezes information--including when in the audio file it is recorded (start and end time denoted in seconds).\n\nI've created the files_df to compile all these data (which includes the patient id and acquisition mode--stereo or mono--which will be used later).","3c3cfad0":"Next we will need to read all the unique files in our dataset. This is done by using the os.listdir function with the condition of checking only .txt files.\n\nNote: We can also use the condition to check .wav files. Eitherway we'll just check all unique files. Condition is needed because if we're checking all kinds of files, we may get replicates. Our dataset consists of .txt and its equivalent .wav files.","a4e2b293":"This is the 1st part of my mini series relating to Detecting Respiratory Disease with the use of Respiratory Audio (breathing sounds). For this kernel, we're only going to slice each audio file into subslices which is defined by the txt files. \n\n- Part 2: [Split into train and test](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-2-split-to-train-and-test)\n- Part 3: [Create spectrogram images from audio](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-3-create-spectrogram-images)\n- Part 4: [Create and train a VGG16 model with the spec images](https:\/\/www.kaggle.com\/danaelisanicolas\/cnn-part-4-training-and-modelling-with-vgg16)\n\nLet's start.\n\nHere you'll see that i'm importing librosa and soundfile which are python packages that deals with audio files.\n* [Librosa](https:\/\/librosa.github.io\/librosa\/)\n* [Soundfile](https:\/\/pysoundfile.readthedocs.io\/en\/latest\/)\n\nBoxplot_stats will be used later to see outliers. More of this later","595b4593":"We're almost ready to create new wav files based on the slices. There's one more problem: we must prepare where to store these new wav files.\n\nSo we have to create our output directory and subfolders as defined by the unique diagnosis.","9435f0d6":"16 seconds long? Is there someone who can have 1 breathe as long as 16 seconds? No, obviously.\n\nSo we try and understand our data and check the outliers and the relative maximum of the dataset.","ea3aca9f":"We know that our filenames have a certain meaning in them. We get all info (or tokens) by splitting the filename by using \"_\" as separators. We define a function to do this task.","3823ef42":"We want to slice the wav file into subslices to get the pure breathing part of the audio file. Again, this is denoted by the start and end times mentioned in the txt files. \n\nI've defined the slice_data function to do this.","262bfa57":"Then it's done! We can check using !ls again","f3a2d557":"Alright time to roll! Now that everything is set, let's start processing the files now. \n\nThe first thing you might notice is the declaration of the i = 0. This will be used in saving of files later. As you may know, each wav file will consist of different slices. When saving these slices, we'll add the ith number of slice. For example, 104_1b1_Ll_sc_Litt3200.wav might have 4 slices, so we'll save them as \n- 104_1b1_Ll_sc_Litt3200_0, \n- 104_1b1_Ll_sc_Litt3200_1, \n- 104_1b1_Ll_sc_Litt3200_2, \n- 104_1b1_Ll_sc_Litt3200_3\n\nNext, for each file \n- get the filename (which will be used for saving later)\n- get the start and end times for the processing of the sliced wav files\n- get the diagnosis so we'll know which subfolder we'll save the sliced wav file\n\nIf the slice is greater than force_max_len, force the slice to only be force_max_len long.\n\nUsing the librosa function load, load the wav file which returns raw data and the sampling rate.\nThen we feed the raw data to the compute_len function we defined earlier to get the expected length of data.\nPad the sliced audio with zeroes using the librosa function util.pad_center.\nThen finally, save this file to our destination path which includes which subfolder we'll save it.\n\nTLDR;\n- Read the audio file\n- Slice the audio file\n- Pad in case it's less than 6 sec\n- Save","2077ffc3":"Load the patient diagnosis file first and check all the unique diagnosis we have in our data.\n\nThis is necessary in how we will sort our output later.","5f4a845a":"I had to look first where the audio and text files are by using !ls","79370adb":"We want to combine the 2 dataframes we've made so far. However to do that, we have to make sure that the column where we'll combine them must have the same data type.\n\nIn this case I just changed the files_df pId column to float64 to be the same as the diagnosis pId dataframe. We can also use int32 to minimise the data allocation in our system however that'll not be our concern for now.\n\nOnce the 2 columns have the same data type, we'll use the pandas merge function to combine the 2 dataframes.","40475be0":"Which we can now check using !ls","008d05ea":"And define the forced maximum length force_max_len as round up of the relative max (5.48 sec round up to 6 sec) length of time.","128850b8":"Next is we have to compute the length of the raw data of the slices. I defined a function compute_len to do this.\n\nRemember we had to store the acquisition mode? It's because it has different computation for stereo and mono.\n- Stereo: (Sampling rate * time) * 2\n- Mono: (Sampling rate * time)","d4b4bb83":"So.. sure we can slice the audio files, but there are things we need to consider as well\n- We need to make sure that they will have the same length (this is in preparation for feeding them into the model for training later)\n- If they're not the same length, then we have to pad the audio with silent (or zeroes) sounds.\n- For the length, we have to know what is the optimal length of time we should use.\n\nFor the next part I tried to get the max length per slice that we currently have in our dataframe"}}