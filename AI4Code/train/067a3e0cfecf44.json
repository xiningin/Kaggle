{"cell_type":{"185eb251":"code","37689840":"code","17ede716":"code","3390c88f":"code","45dfcfd1":"code","3382f74e":"code","225ca9fd":"code","554c1760":"code","58929135":"code","98440643":"code","5d677b0c":"code","d81f648e":"code","a94e1d9c":"code","5375391a":"code","8f3c803e":"code","f82630c2":"code","9e47a0d4":"code","ff9b2cf7":"code","2ee6a9a8":"code","39d6ebc3":"code","0d4389da":"code","907bdcc6":"code","74cb9ff8":"code","b55918ef":"code","8ea01c29":"code","421d254d":"code","4ff4eaa2":"code","91a4045d":"code","e7a49985":"code","6b46cd37":"code","3af747fb":"code","5a15099b":"code","ca85a252":"code","d02520da":"code","f88b3935":"code","affc9333":"code","139cdf33":"code","3b5d0ac2":"code","1aa95dff":"code","94e9bbb9":"code","a21c3144":"code","8b05c4cb":"code","80a99d27":"code","d8c6324d":"code","32d2586c":"code","26acaeec":"code","947e5436":"code","42ae5959":"code","cf8924c2":"code","b61aeabe":"code","69f46611":"code","a1dfa9ba":"code","10a3e959":"code","e0ead4c9":"code","db70a5ba":"code","649bfe9a":"code","79728b76":"code","096bade7":"code","4501701e":"code","c3f8c1f1":"code","42ffc3b0":"code","e2c7e289":"code","a39eeb54":"code","61528c04":"code","331f1d77":"code","40314c20":"code","d28af1f1":"code","7d55313b":"code","2f61f0c6":"code","67214041":"code","cbc4698e":"code","deeb59e2":"code","4f8ad797":"code","93014045":"code","cf3d7964":"code","014e512f":"code","1dcfbcaa":"code","42204a4e":"code","f5094dc8":"code","017e8dcf":"code","587d5f61":"code","1b1e701d":"code","ae246293":"code","de68cfa6":"code","9eea714f":"code","d58bd2f4":"code","17303b4e":"code","c14a9344":"code","1774b0e0":"code","fd494ef3":"code","16740687":"code","54eb7aa0":"code","2b52d029":"code","65dd905f":"code","5c3d758b":"code","3242a1e5":"code","603232d5":"code","d9d2fd4c":"code","76dcbd6d":"code","1a0b2d99":"code","5ffae228":"code","1d4621bb":"code","ff5414b0":"code","e6410e83":"code","8366c5ec":"code","d6fc062f":"code","240380e9":"code","d20873c4":"code","4d992f76":"code","ae57862f":"code","f8b37408":"code","eabff8f3":"code","93b28fca":"code","95a6165a":"code","0d804e51":"code","65fbb80f":"code","e7d3bf81":"code","eb5fec1f":"code","de129c46":"code","7c267bda":"code","414f5ff7":"code","f6a491bb":"code","f589c757":"code","21388b03":"code","3f2eb321":"code","33ade2b2":"markdown","c1894fb4":"markdown","9003a88f":"markdown","c9dafd08":"markdown","9b6c79e3":"markdown","236d3ed9":"markdown","6339ae62":"markdown","473138c0":"markdown","12c2f1dd":"markdown","35357053":"markdown","eb310a56":"markdown","7446647d":"markdown","37110616":"markdown","2c1ff22e":"markdown","ff6f6b91":"markdown","402e499a":"markdown","0f0d95b2":"markdown","6f565075":"markdown","b12be55a":"markdown","44031f98":"markdown","34150eac":"markdown","60ec6007":"markdown","8f947979":"markdown","6b76acba":"markdown","a1c43dda":"markdown","abbe33a1":"markdown","d6606644":"markdown","9ab758a7":"markdown","6038f5fa":"markdown","abb19f01":"markdown","b14630b7":"markdown","471cd376":"markdown","9d4b7645":"markdown","c0ff5f6a":"markdown","a91e6610":"markdown","ca8434e8":"markdown","c1d5fddf":"markdown","dbd39086":"markdown","ee4e9d07":"markdown","75be4c05":"markdown","f42b9c82":"markdown","85bbfff1":"markdown"},"source":{"185eb251":"# K\u00fct\u00fchaneleri y\u00fckleme\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\n#\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom matplotlib import pyplot \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import KFold \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import r2_score\n#\nfrom sklearn.metrics import classification_report \nfrom sklearn.metrics import confusion_matrix \nfrom sklearn.metrics import accuracy_score \n#\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.svm import SVC\n#\nfrom sklearn.linear_model import Lasso \nfrom sklearn.linear_model import ElasticNet \nfrom sklearn.model_selection import GridSearchCV \nfrom sklearn.linear_model import LinearRegression \nfrom sklearn.tree import DecisionTreeRegressor \nfrom sklearn.neighbors import KNeighborsRegressor \nfrom sklearn.svm import SVR \n#\nfrom sklearn.ensemble import RandomForestRegressor \nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom sklearn.ensemble import ExtraTreesRegressor \nfrom sklearn.ensemble import AdaBoostRegressor \nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import Pipeline \nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.neighbors import KNeighborsClassifier \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis \nfrom sklearn.naive_bayes import GaussianNB \nfrom sklearn.svm import SVC \nfrom sklearn.ensemble import AdaBoostClassifier \nfrom sklearn.ensemble import GradientBoostingClassifier \nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.ensemble import ExtraTreesClassifier\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","37689840":"data = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\")\ndata.head()","17ede716":"# regions\ndata.region.unique()","3390c88f":"# regions of counts\nprint(data[\"region\"].value_counts(dropna=False))","45dfcfd1":"# data shape:\nrow, columns = data.shape\nprint(\"Data Row:\", row)\nprint(\"Data Columns:\", columns)","3382f74e":"# column names:\ndata.columns","225ca9fd":"# descriptions \ndisplay(data.describe().T)","554c1760":"# class distribution \nprint(\"Data is not balanced:\",data.groupby('type').size())","58929135":"# Dataset Correlation\ndata.corr()","98440643":"data.isnull().sum()","5d677b0c":"f,ax = plt.subplots(figsize = (10,7))\nax = sns.countplot(x=data.type,label=\"Count\",palette=\"viridis\")\nplt.xlabel('Type of Avocado',fontsize = 15,color='blue')\nplt.ylabel('Count',fontsize = 15,color='blue')\nplt.title('Avocado',fontsize = 20,color='blue')\n#total = len(data['year'])\n# how to show counts:\nfor p in ax.patches:\n        ax.annotate('{:.0f}'.format(p.get_height()), (p.get_x()+0.35, p.get_height()+75))","d81f648e":"f,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=\"type\", y=\"AveragePrice\",data=data,palette=\"viridis\");\nplt.title(\"Compare Average Prices & Observe Outliers\",fontsize = 20,color='blue')\nplt.xlabel('Type of Avocado',fontsize = 15,color='blue')\nplt.ylabel('Average Price',fontsize = 15,color='blue')","a94e1d9c":"f,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=\"year\", y=\"AveragePrice\",hue=\"type\",data=data,palette=\"viridis\");\nplt.title(\"Compare Average Prices of Years & Observe Outliers\",fontsize = 20,color='blue')\nplt.xlabel('Years',fontsize = 15,color='blue')\nplt.ylabel('Average Price',fontsize = 15,color='blue')","5375391a":"f,ax = plt.subplots(figsize = (10,7))\nsns.violinplot(x=\"year\", y=\"AveragePrice\", hue=\"type\", data=data,split=True, inner=\"quart\",palette=\"viridis\")\nplt.xticks(rotation=90)\nplt.title(\"Compare Average Prices of Years\",fontsize = 20,color='blue')\nplt.xlabel('Years',fontsize = 15,color='blue')\nplt.ylabel('Average Price',fontsize = 15,color='blue')","8f3c803e":"f,ax = plt.subplots(figsize = (10,7))\nsns.barplot(x=\"year\", y=\"AveragePrice\",hue=\"type\",data=data,palette=\"viridis\")\nplt.tight_layout() # grafikler daha d\u00fczg\u00fcn g\u00f6z\u00fckecek\nplt.title(\"Compare Average Prices of Years\",fontsize = 20,color='blue')\nplt.xlabel('Years',fontsize = 15,color='blue')\nplt.ylabel('Average Price',fontsize = 15,color='blue')","f82630c2":"f,ax = plt.subplots(figsize = (10,7))\n#data.drop(\"Unnamed: 0\",axis=1,inplace=True)\nsns.heatmap(data.corr(), annot=True,cmap = 'Greens', linewidths=0.5,linecolor=\"black\", fmt= '.2f',ax=ax)","9e47a0d4":"# Split Dataset, \"conventional & organic\"\ndata_con = data[data[\"type\"] == \"conventional\"]\ndata_org = data[data[\"type\"] == \"organic\"]","ff9b2cf7":"f,ax = plt.subplots(figsize = (12,7))\nplt.subplot(2,1,1) # ikiye birlik d\u00fczlemde ilk grafik\nsns.distplot(data_con.AveragePrice,color=\"green\",label=\"Average Price\");\nplt.title(\"Average Price of Conventional\",fontsize = 20,color='blue')\nplt.xlabel('Average Price',fontsize = 15,color='blue')\nplt.legend()\nplt.grid()\n#\nplt.subplot(2,1,2)\nsns.distplot(data_org.AveragePrice,color=\"darkblue\",label=\"Average Price\");\nplt.title(\"Average Price of Organic\",fontsize = 20,color='blue')\nplt.xlabel('Average Price',fontsize = 15,color='blue')\nplt.tight_layout() # grafikler daha d\u00fczg\u00fcn g\u00f6z\u00fckecek\nplt.legend()\nplt.grid()","2ee6a9a8":"# Avocado Average Price\nf,ax = plt.subplots(figsize = (17,9))\nsns.barplot(x=\"region\", y=\"AveragePrice\",hue=\"type\",data=data,palette=\"viridis\")\nplt.xticks(rotation=90)\nplt.tight_layout()","39d6ebc3":"#conda install -c conda-forge wordcloud\n#from wordcloud import WordCloud \n# how many times using regions in dataset\nfrom wordcloud import WordCloud \ndata_region = data.region\nplt.subplots(figsize=(10,10))\nwordcloud = WordCloud(\n                          background_color='white',\n                          width=512,\n                          height=384\n                         ).generate(\"*\".join(data_region))\nplt.imshow(wordcloud)\nplt.title(\"Wordcloud for Regions\")\nplt.axis('off')","0d4389da":"# Sum(ounce) of Avocados for per year\ndata_2015 = data[data.year==2015]\nsum_2015 = data_2015[\"Total Volume\"].sum()\ndata_2016 = data[data.year==2016]\nsum_2016 = data_2016[\"Total Volume\"].sum()\ndata_2017 = data[data.year==2017]\nsum_2017 = data_2017[\"Total Volume\"].sum()\ndata_2018 = data[data.year==2018]\nsum_2018 = data_2018[\"Total Volume\"].sum()\n#**********************************************************************************\n# Sum(ounce) of Avocados for per year in conventional\ndata_con_2015 = data_con[data.year==2015]\nsum_con_2015 = data_con_2015[\"Total Volume\"].sum()\ndata_con_2016 = data_con[data.year==2016]\nsum_con_2016 = data_con_2016[\"Total Volume\"].sum()\ndata_con_2017 = data_con[data.year==2017]\nsum_con_2017 = data_con_2017[\"Total Volume\"].sum()\ndata_con_2018 = data_con[data.year==2018]\nsum_con_2018 = data_con_2018[\"Total Volume\"].sum()\n#**********************************************************************************\n# Sum(ounce) of Avocados for per year in organic\ndata_org_2015 = data_org[data.year==2015]\nsum_org_2015 = data_org_2015[\"Total Volume\"].sum()\ndata_org_2016 = data_org[data.year==2016]\nsum_org_2016 = data_org_2016[\"Total Volume\"].sum()\ndata_org_2017 = data_org[data.year==2017]\nsum_org_2017 = data_org_2017[\"Total Volume\"].sum()\ndata_org_2018 = data_org[data.year==2018]\nsum_org_2018 = data_org_2018[\"Total Volume\"].sum()\n\nlabels = data.year.value_counts().index\ncolors = ['grey','blue','red','yellow']\nfracs = [15, 30, 45, 10]\nsizes_1 = [sum_con_2015,sum_con_2016,sum_con_2017,sum_con_2018]#for con\nfig = plt.figure(figsize = (9,9))\n#\nsizes_2 = [sum_org_2015,sum_org_2016,sum_org_2017,sum_org_2018]#for org\nax1 = fig.add_axes([0, -0.1, .5, .5], aspect=1)\nax1.pie(sizes_1, labels=labels, radius = 1.2,colors=colors,autopct='%1.2f%%')\n#\nax2 = fig.add_axes([0.7, -0.1, .5, .5], aspect=1)\nax2.pie(sizes_2, labels=labels, radius = 1.2,colors=colors,autopct='%1.2f%%')\n#\nsizes_0 = [sum_2015,sum_2016,sum_2017,sum_2018]\nax3 = fig.add_axes([.35, 0, .5, 1.5], aspect=1)\nax3.pie(sizes_0, labels=labels, radius = 1.2,colors=colors,autopct='%1.2f%%')\n#\nax1.set_title('Avocado Consumption in Conventional',color = 'blue',fontsize = 15)\nax2.set_title('Avocado Consumption in Organic',color = 'blue',fontsize = 15)\nax3.set_title('Avocado Consumption ',color = 'blue',fontsize = 15)\nplt.show()\nplt.tight_layout() # grafikler daha d\u00fczg\u00fcn g\u00f6z\u00fckecek","907bdcc6":"data.head()","74cb9ff8":"# split date: day,month,year \nliste = []\nfor date in data.Date:\n    liste.append(date.split(\"-\"))\n    \n# month and day adding to lists\nmonth = []\nday = []\nfor i in range(len(liste)):\n    month.append(liste[i][1])\n    day.append(liste[i][2])\n    \n# adding to dataset\ndata[\"month\"] = month\ndata[\"day\"] = day\n\n# delete old date column\ndata.drop([\"Date\"],axis=1,inplace=True)\n\n#convert objects to int\ndata.month = data.month.values.astype(int)\ndata.day = data.day.values.astype(int)","b55918ef":"# drop unnecessary features\ndata.drop([\"Unnamed: 0\",\"region\"],axis=1,inplace=True)","8ea01c29":"# find dummy variables\ndata[\"type\"] = pd.get_dummies(data.type,drop_first=True)","421d254d":"data.head()","4ff4eaa2":"# Y\ny = data[[\"type\"]][:]","91a4045d":"# X\nx = data.drop([\"type\"],axis=1,inplace=True)\nx = data.iloc[:,:]","e7a49985":"# Scale the data to be between -1 and 1\nsc = StandardScaler()\nx = sc.fit_transform(x)","6b46cd37":"# Creating Train and Test Datasets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","3af747fb":"# Model List\nmodels = [] \nmodels.append(('LR', LogisticRegression())) \nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('SVM', SVC())) \nmodels.append(('NB', GaussianNB()))\nmodels.append(('DTC', DecisionTreeClassifier()))","5a15099b":"# evaluate models using cross validation score:\nresults = [] \nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=42) \n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy') \n    results.append(cv_results) \n    names.append(name) \n    print(\"Model Name:{} Model Acc:{:.3f} Model Std:{:.3f}\".format(name, cv_results.mean(), cv_results.std()))","ca85a252":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=names, y=results,palette=\"viridis\");\nplt.title(\"Compare Model's Accuracies\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Accuracies',fontsize = 15,color='blue')","d02520da":"# Tuning Decision Tree Model\ncriterions = [\"gini\",\"entropy\"]\nparam_grid = dict(criterion=criterions) ","f88b3935":"dtc = DecisionTreeClassifier()\ngs = GridSearchCV(estimator=dtc,param_grid=param_grid,scoring=\"accuracy\", cv=10)\ngrid_search = gs.fit(x_train,y_train)\nbest_score = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Score:\",best_score)\nprint(\"Best Parameters:\",best_parameters)","affc9333":"# Time to use DFC for dataset:\ndtc = DecisionTreeClassifier(criterion=\"entropy\")\ndtc.fit(x_train,y_train)\ny_pred = dtc.predict(x_test)","139cdf33":"#confussion matrix: \nfrom sklearn import metrics\ndtc_cm = confusion_matrix(y_test,y_pred)\ndtc_cross = pd.crosstab(y_test[\"type\"], y_pred,rownames=['Actual Values'], colnames=['Predicted Values'])\ndtc_acc = metrics.accuracy_score(y_test, y_pred)\nprint(dtc_cross)\nprint(dtc_acc)","3b5d0ac2":"# Feature Importance\n#print(rfc.feature_importances_)\n# You can see that we are given an importance score \n# for each attribute where the larger the score, \n# the more important the attribute.\nfeature = pd.DataFrame(data=[dtc.feature_importances_], columns=data.columns)\nfeature.head()","1aa95dff":"# Classi\ufb01cation Report\nfrom sklearn.metrics import classification_report \nreport = classification_report(y_test, y_pred) \nprint(report)","94e9bbb9":"# ROC E\u011frisi:\nimport sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = dtc.predict_proba(x_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n# method I: plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.3f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","a21c3144":"# ensembles \nensembles = [] \nensembles.append(('ABC', AdaBoostClassifier()))\nensembles.append(('GBC', GradientBoostingClassifier()))\nensembles.append(('RFC', RandomForestClassifier()))\nensembles.append(('ETC', ExtraTreesClassifier()))","8b05c4cb":"# evaluate models using cross validatiob score:\nresults_ensemble = [] \nnames_ensemble = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=42) \n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy') \n    results_ensemble.append(cv_results) \n    names_ensemble.append(name) \n    print(\"Model Name:{} Model Acc:{:.3f} Model Std:{:.3f}\".format(name, cv_results.mean(), cv_results.std()))","80a99d27":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=names_ensemble, y=results_ensemble,palette=\"viridis\");\nplt.title(\"Compare Ensemble Model's Accuracies\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Accuracies',fontsize = 15,color='blue')","d8c6324d":"# Tuning Extra Trees Class. Model\nestimators = [1,3,5,7,9,11,13,15,17,19,21,23,25,27,29] \ncriterions = [\"gini\",\"entropy\"]\nparam_grid = dict(n_estimators=estimators,criterion=criterions) ","32d2586c":"etc = ExtraTreesClassifier()\ngs = GridSearchCV(estimator=etc,param_grid=param_grid,scoring=\"accuracy\", cv=10)\ngrid_search = gs.fit(x_train,y_train)\nbest_score = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Score:\",best_score)\nprint(\"Best Parameters:\",best_parameters)","26acaeec":"# Time to use ETC for dataset:\netc = ExtraTreesClassifier(n_estimators=27,criterion=\"gini\")\netc.fit(x_train,y_train)\ny_pred = etc.predict(x_test)","947e5436":"#confussion matrix: \nfrom sklearn import metrics\netc_cm = confusion_matrix(y_test,y_pred)\netc_cross = pd.crosstab(y_test[\"type\"], y_pred,rownames=['Actual Values'], colnames=['Predicted Values'])\netc_acc = metrics.accuracy_score(y_test, y_pred)\nprint(etc_cross)\nprint(etc_acc)","42ae5959":"# Classi\ufb01cation Report\nfrom sklearn.metrics import classification_report \nreport = classification_report(y_test, y_pred,digits=3) \nprint(report)","cf8924c2":"#model\nfrom xgboost import XGBClassifier\nxgb = XGBClassifier()\nxgb.fit(x_train,y_train)\ny_pred = xgb.predict(x_test)","b61aeabe":"#confussion matrix: \nfrom sklearn import metrics\nxgb_cm = confusion_matrix(y_test,y_pred)\nxgb_cross = pd.crosstab(y_test[\"type\"], y_pred,rownames=['Actual Values'], colnames=['Predicted Values'])\nxgb_acc = metrics.accuracy_score(y_test, y_pred)\nprint(xgb_cross)\nprint(xgb_acc)","69f46611":"# Tuning XGBOOST Model\nlearning_rates = [0.1,0.01,0.001] \nliste = list(range(250))\nestimators = liste\ngammas = [1,0.5,0.1,0.01,0.001,0]\nboosters = [\"gbtree\",\"gblinear\",\"dart\"]\nparam_grid = dict(n_estimators=estimators,learning_rate=learning_rates,gamma=gammas,booster=boosters) ","a1dfa9ba":"# Tuning is taking time...\n#xgb = XGBClassifier()\n#gs = GridSearchCV(estimator=xgb,param_grid=param_grid,scoring=\"accuracy\", cv=10)\n#grid_search = gs.fit(x_train,y_train)\n#best_score = grid_search.best_score_\n#best_parameters = grid_search.best_params_\n#print(\"Best Score:\",best_score)\n#print(\"Best Parameters:\",best_parameters)","10a3e959":"from sklearn.neural_network import MLPClassifier\nmlpc = MLPClassifier(verbose=False)\nmlpc.fit(x_train, y_train)    \n#mlpc.max_iter \n#mlpc.hidden_layer_sizes#node say\u0131s\u0131 \ny_pred = mlpc.predict(x_test)","e0ead4c9":"mlpc.get_params()","db70a5ba":"#confussion matrix: \nfrom sklearn import metrics\nmlpc_cm = confusion_matrix(y_test,y_pred)\nmlpc_cross = pd.crosstab(y_test[\"type\"], y_pred,rownames=['Actual Values'], colnames=['Predicted Values'])\nmlpc_acc = metrics.accuracy_score(y_test, y_pred)\nprint(mlpc_cross)\nprint(mlpc_acc)","649bfe9a":"models = [\"dtc\",\"etc\",\"xgb\",\"nn\"]\nvalues = [0.986,0.997,0.991,0.982]","79728b76":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.barplot(x=models, y=values,palette=\"viridis\");\nplt.title(\"Compare Ensemble Model's Accuracies\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Accuracies',fontsize = 15,color='blue')","096bade7":"data = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\")\ndata.head()","4501701e":"# Split Dataset, \"conventional & organic\"\ndata_con = data[data[\"type\"] == \"conventional\"]\ndata_org = data[data[\"type\"] == \"organic\"]","c3f8c1f1":"f,ax = plt.subplots(figsize = (20,7))\ndata_con = data_con.sort_values(\"Date\")\nplt.plot(data_con['Date'], data_con['AveragePrice'])","42ffc3b0":"f,ax = plt.subplots(figsize = (20,7))\ndata_org = data_org.sort_values(\"Date\")\nplt.plot(data_org['Date'], data_org['AveragePrice'])","e2c7e289":"# split date: day,month,year \nliste = []\nfor date in data.Date:\n    liste.append(date.split(\"-\"))\n    \n# month and day adding to lists\nmonth = []\nday = []\nfor i in range(len(liste)):\n    month.append(liste[i][1])\n    day.append(liste[i][2])\n    \n# adding to dataset\ndata[\"month\"] = month\ndata[\"day\"] = day\n\n# delete old date column\ndata.drop([\"Date\"],axis=1,inplace=True)\n\n#convert objects to int\ndata.month = data.month.values.astype(int)\ndata.day = data.day.values.astype(int)","a39eeb54":"# drop unnecessary features\ndata.drop([\"Unnamed: 0\"],axis=1,inplace=True)","61528c04":"data.head()","331f1d77":"# Split Dataset, \"conventional & organic\"\ndata_con = data[data[\"type\"] == \"conventional\"]\ndata_org = data[data[\"type\"] == \"organic\"]","40314c20":"# find dummy variables\ndata_con = pd.get_dummies(data_con,drop_first=True)\ndata_org = pd.get_dummies(data_org,drop_first=True)","d28af1f1":"data_con.head()","7d55313b":"data_org.head()","2f61f0c6":"# For Conventional\nimport statsmodels.api as sm\nexog_con = data_con.iloc[:,1:].values\nendog_con = data_con.iloc[:,[0]].values\nr_ols_con = sm.OLS(endog_con,exog_con) #ba\u011f\u0131ml\u0131 de\u011fi\u015fken, X_l:ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlerimiz.\nr_con = r_ols_con.fit()\nprint(r_con.summary())","67214041":"# For Organic\nimport statsmodels.api as sm\nexog_org = data_org.iloc[:,1:].values\nendog_org = data_org.iloc[:,[0]].values\nr_ols_org = sm.OLS(endog_org,exog_org) #ba\u011f\u0131ml\u0131 de\u011fi\u015fken, X_l:ba\u011f\u0131ms\u0131z de\u011fi\u015fkenlerimiz.\nr_org = r_ols_org.fit()\nprint(r_org.summary())","cbc4698e":"# Y\ny = data_con[[\"AveragePrice\"]][:]","deeb59e2":"# X\nx = data_con.drop([\"AveragePrice\"],axis=1,inplace=True)\nx = data_con.iloc[:,1:]","4f8ad797":"# Scale the data to be between -1 and 1\nsc_x = StandardScaler()\nsc_y = StandardScaler()\nx = sc_x.fit_transform(x)\ny = sc_y.fit_transform(y)","93014045":"# Creating Train and Test Datasets\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=42)","cf3d7964":"classic_models = [] \nclassic_models.append(('LR', LinearRegression())) \nclassic_models.append(('LASSO', Lasso())) \nclassic_models.append(('EN', ElasticNet())) \nclassic_models.append(('KNN', KNeighborsRegressor())) \nclassic_models.append(('DTR', DecisionTreeRegressor())) \nclassic_models.append(('SVR', SVR()))","014e512f":"# evaluate models using cross validation score:\nclassic_results = [] \nclassic_names = []\nfor name, model in classic_models:\n    kfold = KFold(n_splits=10, random_state=42) \n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='r2') \n    classic_results.append(cv_results) \n    classic_names.append(name) \n    print(\"Model Name:{} Model Score:{:.3f} Model Std:{:.3f}\".format(name, cv_results.mean(), cv_results.std()))","1dcfbcaa":"# Compare Model's Scores\nf,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=classic_names, y=classic_results,palette=\"viridis\");\nplt.title(\"Compare Model's Scores\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Scores',fontsize = 15,color='blue')","42204a4e":"# Tuning Decision Tree Model\ncriterions = [\"mse\",\"mae\"]\nparam_grid = dict(criterion=criterions) ","f5094dc8":"dtr = DecisionTreeRegressor()\ngs = GridSearchCV(estimator=dtr,param_grid=param_grid,scoring=\"r2\", cv=kfold)\ngrid_search = gs.fit(x_train,y_train)\nbest_score = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"Best Score:\",best_score)\nprint(\"Best Parameters:\",best_parameters)","017e8dcf":"means = grid_search.cv_results_['mean_test_score'] \nstds = grid_search.cv_results_['std_test_score'] \nparams = grid_search.cv_results_['params'] \nfor mean, stdev, param in zip(means, stds, params): \n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","587d5f61":"# Time to use DTR for dataset:\ndtr = DecisionTreeRegressor(criterion=\"mse\")\ndtr.fit(x_train,y_train)\ny_pred = dtr.predict(x_test)","1b1e701d":"result_DFR = r2_score(y_test, y_pred)\nprint(\"{:.2f}\".format(result_DFR))","ae246293":"results_models = []\nresults_models.append(result_DFR)","de68cfa6":"# ensembles \nensembles = [] \nensembles.append(('ABR', AdaBoostRegressor()))\nensembles.append(('GBR', GradientBoostingRegressor()))\nensembles.append(('RFR', RandomForestRegressor()))\nensembles.append(('ETR', ExtraTreesRegressor()))","9eea714f":"# evaluate models using cross validatiob score:\nresults_ensemble = [] \nnames_ensemble = []\nfor name, model in ensembles:\n    kfold = KFold(n_splits=10, random_state=42) \n    cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='r2') \n    results_ensemble.append(cv_results) \n    names_ensemble.append(name) \n    print(\"Model Name:{} Model Score:{:.3f} Model Std:{:.3f}\".format(name, cv_results.mean(), cv_results.std()))","d58bd2f4":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.boxplot(x=names_ensemble, y=results_ensemble,palette=\"viridis\");\nplt.title(\"Compare Ensemble Model's Accuracies\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Scores',fontsize = 15,color='blue')","17303b4e":"# Tuning Extra Trees Regressior Model\nestimators = list(range(25,301,25))\ncriterions = [\"mse\",\"mae\"]\nparam_grid = dict(n_estimators=estimators,criterion=criterions) ","c14a9344":"# Note: Taking time\n# Applying Extra Trees Regressior\n#etr = ExtraTreesRegressor()\n#gs = GridSearchCV(estimator=etr,param_grid=param_grid,scoring=\"r2\", cv=kfold)\n#grid_search = gs.fit(x_train,y_train)\n#best_score = grid_search.best_score_\n#best_parameters = grid_search.best_params_\n#print(\"Best Score:\",best_score)\n#print(\"Best Parameters:\",best_parameters)","1774b0e0":"# Time to use ETR for dataset:\netr = ExtraTreesRegressor(n_estimators=100,criterion=\"mse\")\netr.fit(x_train,y_train)\ny_pred = etr.predict(x_test)","fd494ef3":"result_ETR = r2_score(y_test, y_pred)\nprint(\"{:.2f}\".format(result_ETR))","16740687":"results_models.append(result_ETR)","54eb7aa0":"from xgboost import XGBRegressor\nxgbr = XGBRegressor(silent=True)  # silent: close to warnings  \nxgbr.fit(x_train,y_train)\ny_pred = xgbr.predict(x_test)   ","2b52d029":"result_XGB = r2_score(y_test, y_pred)\nprint(\"{:.2f}\".format(result_XGB))","65dd905f":"results_models.append(result_XGB)","5c3d758b":"from sklearn.neural_network import MLPRegressor\nmlp = MLPRegressor()\nmlp.fit(x_train,y_train)    \n#mlp.max_iter \n#mlp.hidden_layer_sizes#node say\u0131s\u0131 \ny_pred = mlp.predict(x_test)   ","3242a1e5":"result_MLP = r2_score(y_test, y_pred)\nprint(\"{:.2f}\".format(result_MLP))","603232d5":"results_models.append(result_MLP)","d9d2fd4c":"models = [\"DTR\",\"ETR\",\"XGB\",\"NN\"]","76dcbd6d":"# Compare Model's Acc\nf,ax = plt.subplots(figsize = (10,7))\nsns.barplot(x=models, y=results_models,palette=\"viridis\");\nplt.title(\"Compare Ensemble Model's Scores\",fontsize = 20,color='blue')\nplt.xlabel('Models',fontsize = 15,color='blue')\nplt.ylabel('Scores',fontsize = 15,color='blue')","1a0b2d99":"data_con.head()","5ffae228":"# Note that input data must be normalized\nx_test_sample = np.array([[78992.15, 1132.00,  71976.41, 72.58, 5811.16, 5000, 133.76, 0, 2015, 8, 17, 1,0,0,0,0,0,0,0,0,0,0,\n                           0,0,0,0,0,0,0,0,0,0,\n                           0,0,0,0,0,0,0,0,0,0,\n                           0,0,0,0,0,0,0,0,0,0,\n                           0,0,0,0,0,0,0,0,0,0,0]])\n                          \ny_predict_sample = etr.predict(x_test_sample)\nprint('Expected Purchase Amount=', y_predict_sample)\ny_predict_sample_orig = sc_y.inverse_transform(y_predict_sample)\nprint('Expected Purchase Amount=', y_predict_sample_orig)","1d4621bb":"from fbprophet import Prophet","ff5414b0":"data = pd.read_csv(\"\/kaggle\/input\/avocado-prices\/avocado.csv\")\ndata.head()","e6410e83":"prophet_df = data.iloc[:,[1,2]]\nprophet_df.head()","8366c5ec":"prophet_df = prophet_df.sort_values(\"Date\")","d6fc062f":"prophet_df = prophet_df.rename(columns={'Date':'ds', 'AveragePrice':'y'})","240380e9":"prophet_df.tail()","d20873c4":"m = Prophet()\nm.fit(prophet_df)","4d992f76":"# Forcasting into the future\nfuture = m.make_future_dataframe(periods=365)\nfuture.tail()","ae57862f":"forecast = m.predict(future)\nforecast[['ds', 'yhat', 'yhat_lower', 'yhat_upper']].tail()","f8b37408":"# You can plot the forecast\nfigure1 = m.plot(forecast, xlabel='Date', ylabel='Price')","eabff8f3":"# If you want to see the forecast components\nfigure2 = m.plot_components(forecast)","93b28fca":"# pip install h2o\n# or \n# pip install http:\/\/h2o-release.s3.amazonaws.com\/h2o\/rel-weierstrass\/2\/Python\/h2o-3.14.0.2-py2.py3-none-any.whl\nimport h2o\nfrom h2o.automl import H2OAutoML\nh2o.init()","95a6165a":"data = pd.read_csv(\"\/kaggle\/input\/avocado\/avocado_2.csv\")\ndata.head()","0d804e51":"# Load data into H2O\ndf = h2o.import_file(\"\/kaggle\/input\/avocado\/avocado_2.csv\")","65fbb80f":"df.describe()","e7d3bf81":"y = \"C3\"","eb5fec1f":"# Parse Df\nsplits = df.split_frame([0.6, 0.2], seed = 1)","de129c46":"splits","7c267bda":"# Parse Df\ntrain = splits[0]\nvalid = splits[1]\ntest  = splits[2]","414f5ff7":"# Run AutoML\naml = H2OAutoML(max_runtime_secs = 300, seed = 1, project_name = \"avocado_price\")\naml.train(y = y, training_frame = train, leaderboard_frame = valid)","f6a491bb":"aml.leaderboard.head()","f589c757":"pred = aml.predict(test)\npred","21388b03":"test[\"C3\"]","3f2eb321":"perf = aml.leader.model_performance(test)\nperf","33ade2b2":"<a id = \"3\"><\/a><br>\n## Load Dataset","c1894fb4":"<a id = \"9\"><\/a><br>\n### Classic Models","9003a88f":"<a id = \"12\"><\/a><br>\n### Neural Network","c9dafd08":"### Prepare Dataset For Regression","9b6c79e3":"<a id = \"6\"><\/a><br>\n## Data Visualization","236d3ed9":"## Tuning for Extra Trees Classifier","6339ae62":"<a id = \"16\"><\/a><br>\n### Ensemble Models","473138c0":"## Per Year Consumption","12c2f1dd":"* Prophet is open source software released by Facebook\u2019s Core Data Science team.\n* Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. \n* Prophet works best with time series that have strong seasonal effects and several seasons of historical data. ","35357053":"<a id = \"18\"><\/a><br>\n### Neural Network","eb310a56":"## Price of Regions","7446647d":"### Predict Using Leader Model","37110616":"<a id = \"4\"><\/a><br>\n## Descriptive Analysis","2c1ff22e":"<a id = \"19\"><\/a><br>\n### Compare Models","ff6f6b91":"<a id = \"10\"><\/a><br>\n### Ensemble Models","402e499a":"### Paremeters of NN","0f0d95b2":"<a id = \"20\"><\/a><br>\n## Let's Make Some Price Predictions via ETR","6f565075":"### Organic Avocado Price","b12be55a":"<a id = \"14\"><\/a><br>\n## Regression Modeling\n### Avocado Price Prediction","44031f98":"## Classification Report","34150eac":"<a id = \"15\"><\/a><br>\n### Classic Models","60ec6007":"### Observations:\n* The small P values indicate that we can reject the null hypothesis that Quantity has no effect on Price.\n* Hight R-squared indicates that our model explains a lot of the response variability.\n* In regression analysis, we\u2019d like our regression model to have significant variables and to produce a high R-squared value.\n* We will show graphs to help interpret regression analysis results more intuitively.","8f947979":"## Feature Descriptions:\n* Date - The date of the observation\n* AveragePrice - the average price of a single avocado\n* type - conventional or organic\n* year - the year\n* Region - the city or region of the observation\n* Total Volume - Total number of avocados sold\n* 4046 - Total number of avocados with PLU 4046 sold\n* 4225 - Total number of avocados with PLU 4225 sold\n* 4770 - Total number of avocados with PLU 4770 sold","6b76acba":"<a id = \"17\"><\/a><br>\n### XGBOOST Model","a1c43dda":"## Performance of Model","abbe33a1":"<font color = 'red'>\n<h1>Data Analysis of Avocado<h1>","d6606644":"<a id = \"13\"><\/a><br>\n### Compare Models","9ab758a7":"# Introduction\n<br>\n<br>\n<font color = 'blue'>\n<b>Content: <\/b>\n\n1. [Prepare Problems](#1)\n    * [Load Libraries](#2)\n    * [Load Dataset](#3)    \n1. [Basic Data Analysis](#4)\n1. [Missing Value](#5)\n1. [Visualization](#6)\n1. [Feature Engineering](#7)\n1. [Classification Modeling](#8)\n    * [Classic Models](#9)\n    * [Ensemble Models](#10)\n    * [XGBOOST Model](#11)\n    * [Neural Network](#12)\n    * [Compare Models](#13)\n1. [Regression Modeling](#14)\n    * [Classic Models](#15)\n    * [Ensemble Models](#16)\n    * [XGBOOST Model](#17)\n    * [Neural Network](#18)\n    * [Compare Models](#19)\n    * [Let's Make Some Price Predictions via ETR](#20)\n    * [Facebook Prophet](#21)\n    * [H20.ai AutoML](#22)\n   ","6038f5fa":"<a id = \"5\"><\/a><br>\n## Missing Values","abb19f01":"<a id = \"21\"><\/a><br>\n# Facebook Prophet","b14630b7":"### Tuning Decision Tree Model","471cd376":"### Conventional Avocado Price","9d4b7645":"<a id = \"11\"><\/a><br>\n### XGBOOST Model","c0ff5f6a":"* 60% for training\n* 20% for validation (hyper parameter tuning)\n* 20% for final testing, will be withheld until the end","a91e6610":"## Ordinary Least Squares (OLS) Estimation","ca8434e8":"<a id = \"8\"><\/a><br>\n## Classification Modeling\n### Is Your Avocado Organic or Conventional","c1d5fddf":"## Tuning Decision Tree Classifier","dbd39086":"<a id = \"7\"><\/a><br>\n## Feature Engineering For Classification","ee4e9d07":"<font color = 'red'>\n<a id = \"1\"><\/a><br>\n<h2>Prepare Problems<h2>\n<font color = 'blue'>\n      1) Avocado Price Prediction<br><br>\n      2) Is Your Avocado \"Organic or Not\" Clasification","75be4c05":"<a id = \"22\"><\/a><br>\n# H20.ai AutoML","f42b9c82":"### Future Engineering","85bbfff1":"## Regression Analysis"}}