{"cell_type":{"2851a2f8":"code","3e43932c":"code","59d18c37":"code","b06955d7":"code","67473e5b":"code","4f04913b":"code","3984bb0f":"code","30e6abb4":"code","0e68a678":"code","fce5c09a":"code","fe35d58d":"code","35b5305d":"code","9a74f346":"code","0fa97592":"code","76b4e587":"code","00c93cb7":"code","a503a4cb":"code","4fcc6726":"code","b6d01d86":"code","f7e30b49":"code","0a4e2599":"code","af565f58":"code","0d0c731d":"code","070d9378":"code","a61b3fd2":"code","4f097c10":"code","3e576775":"code","1e6d72e7":"code","352ccd26":"code","2721b8e2":"code","b635acac":"code","f64ec54f":"code","d37bec60":"code","5e2f3786":"code","76e817c6":"code","a4585d84":"code","1c98834b":"code","b614b8f8":"code","399555a6":"code","87af852d":"code","43416a3e":"code","4bd175f8":"code","6815c917":"code","82d4d8f3":"code","ab6e6f0c":"code","a07482f6":"code","d770ec56":"code","b413b1ea":"code","ea8f55bf":"code","4ad747f5":"code","c7f6e835":"code","3abf51a5":"code","a9646efb":"code","96ddf729":"code","5b57ff35":"code","3a4122e2":"code","7e8a0d9a":"code","e0155aeb":"code","1f706de7":"code","4997bb21":"code","cd0171bf":"code","cca3e261":"code","02af4663":"code","fc5da235":"code","8ef0dd6a":"code","433e43d0":"code","566d2bc2":"code","7f15e296":"code","af0617d0":"code","af1744b0":"code","42262281":"code","226e83a4":"code","1d177cd6":"code","5d285d75":"code","aba6fbdf":"code","4c9b4be8":"code","b697c4ab":"code","6e464888":"code","a4095299":"code","a11de324":"code","63119651":"code","a6da7b06":"code","44aa9448":"code","dfbc0d14":"code","a5836c2e":"code","9071b669":"code","56b9b23d":"code","695e4e70":"code","710bf0aa":"code","32ff9ac7":"code","43e6b803":"code","b25dae9a":"code","ef990ab8":"markdown","f0eb1a7a":"markdown","244e3d0a":"markdown","8df442b0":"markdown","4357a2a9":"markdown","bfd681ab":"markdown","321346ec":"markdown","864f7aeb":"markdown","3d025019":"markdown","9f5194f0":"markdown","5ff61abb":"markdown","80376213":"markdown","9f83b8c1":"markdown","f7c72c33":"markdown"},"source":{"2851a2f8":"# Importing the libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","3e43932c":"# Importing the dataset\ntrain = pd.read_csv('..\/input\/train.csv').drop('Unnamed: 0', axis = 1)","59d18c37":"# For each column heading we replace \"-\" and convert the heading in lowercase \ncleancolumn = []\nfor i in range(len(train.columns)):\n    cleancolumn.append(train.columns[i].replace('-', '').lower())\ntrain.columns = cleancolumn\n","b06955d7":"train.head()","67473e5b":"train[train.columns[1:]].describe()","4f04913b":"train[1:].isna().tail(10)","3984bb0f":"# This give you the calulation of the target lebels. Which category of the target lebel is how many percentage.\ntotal_len = len(train['seriousdlqin2yrs'])\npercentage_labels = (train['seriousdlqin2yrs'].value_counts()\/total_len)*100\npercentage_labels","30e6abb4":"# Graphical representation of the target label percentage.\nsns.set()\nsns.countplot(train.seriousdlqin2yrs).set_title('Data Distribution')\nax = plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2.,\n            height + 2,\n            '{:.2f}%'.format(100*(height\/total_len)),\n            fontsize=14, ha='center', va='bottom')\nsns.set(font_scale=2)\nsns.set(rc={'figure.figsize':(8,8)})\nax.set_xlabel(\"Labels for seriousdlqin2yrs attribute\")\nax.set_ylabel(\"Numbers of records\")\nplt.show()\n\n","0e68a678":"# You will get to know which column has missing value and it's give the count that how many records are missing \ntrain.isnull().sum()","fce5c09a":"# Graphical representation of the missing values.\nx = train.columns\ny = train.isnull().sum()\nsns.set()\nsns.barplot(x,y)\nax = plt.gca()\nfor p in ax.patches:\n    height = p.get_height()\n    ax.text(p.get_x() + p.get_width()\/2.,\n            height + 2,\n            int(height),\n            fontsize=14, ha='center', va='bottom')\nsns.set(font_scale=1.5)\nsns.set(rc={'figure.figsize':(8,8)})\nax.set_xlabel(\"Data Attributes\")\nax.set_ylabel(\"count of missing records for each attribute\")\nplt.xticks(rotation=90)\nplt.show()","fe35d58d":"# Actual replacement of the missing value using mean value.\ntrain_mean = train.fillna((train.mean()))\ntrain_mean.head()","35b5305d":"train_mean.isnull().sum()","9a74f346":"# Actual replacement of the missing value using median value.\ntrain_median = train.fillna((train.median()))\ntrain_median.head()","0fa97592":"train_median.isnull().sum()","76b4e587":"train.fillna((train.median()), inplace=True)\n# Get the correlation of the training dataset\ncorrelation=train[train.columns[1:]].corr()\ncorrelation","00c93cb7":"sns.set()\nsns.set(font_scale=1.25)\nsns.heatmap(train[train.columns[1:]].corr(),annot=True,fmt=\".1f\")\nsns.set(rc={'figure.figsize':(10,10)})\nplt.show()","a503a4cb":"train.columns","4fcc6726":"X=train.drop('seriousdlqin2yrs',axis=1)\ny=train.seriousdlqin2yrs","b6d01d86":"train.columns[1:]","f7e30b49":"features_label = train.columns[1:]","0a4e2599":"#Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 200, criterion = 'entropy', random_state = 0)\nclassifier.fit(X, y)\nimportances = classifier.feature_importances_\nindices = np. argsort(importances)[::-1]\nfor i in range(X.shape[1]):\n    print (\"%2d) %-*s %f\" % (i + 1, 30, features_label[i],importances[indices[i]]))","af565f58":"plt.title('Feature Importances')\nplt.bar(range(X.shape[1]),importances[indices], color=\"green\", align=\"center\")\nplt.xticks(range(X.shape[1]),features_label, rotation=90)\nplt.xlim([-1, X.shape[1]])\nplt.show()","0d0c731d":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score","070d9378":"X = train.drop('seriousdlqin2yrs', axis=1)\ny = train.seriousdlqin2yrs","a61b3fd2":"# Splitting the dataset into the Training set and Test set\nfrom sklearn.model_selection  import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 42)","4f097c10":"# Fitting Random Forest Classification to the Training set\nfrom sklearn.ensemble import RandomForestClassifier\nRandom = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=None, min_samples_split=2,\n                               min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto',\n                               max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, \n                               random_state=None, verbose=0)\nRandom.fit(X_train, y_train)","3e576775":"# Predicting the Test set results\ny_pred = Random.predict(X_test)\n\n# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\n","1e6d72e7":"cm","352ccd26":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","2721b8e2":"test_labels=Random.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","b635acac":"import matplotlib.pyplot as plt\nimport itertools\n\nfrom sklearn import svm, datasets\nfrom sklearn.metrics import confusion_matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()\n","f64ec54f":"# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()\n","d37bec60":"# Predicting the Test set results\ny_pred = Random.predict(X)\n\n# Making the Confusion Matrix w\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred.round())\nnp.set_printoptions(precision=2)","5e2f3786":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","76e817c6":"# Fitting Logistic Regression to the Training set\nfrom sklearn.linear_model import LogisticRegression\nLogistic = LogisticRegression(penalty='l1', dual=False, tol=0.0001, C=1.0, fit_intercept=True,\n                            intercept_scaling=1, class_weight=None, \n                            random_state=None, solver='liblinear', max_iter=100,\n                            multi_class='ovr', verbose=2)\nLogistic.fit(X_train, y_train)","a4585d84":"# Predicting the Test set results\ny_pred = Logistic.predict(X_test)\n\n# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)","1c98834b":"cm","b614b8f8":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","399555a6":"test_labels=Logistic.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","87af852d":"# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","43416a3e":"# Predicting the Test set results\ny_pred = Logistic.predict(X)\n\n# Making the Confusion Matrix w\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred.round())\nnp.set_printoptions(precision=2)","4bd175f8":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","6815c917":"# Fitting K-NN to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nKNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', leaf_size=30, p=2,\n                             metric='minkowski', metric_params=None)\nKNN.fit(X_train, y_train)","82d4d8f3":"# Predicting the Test set results\ny_pred = KNN.predict(X_test)\n\n# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)","ab6e6f0c":"cm","a07482f6":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","d770ec56":"test_labels=KNN.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","b413b1ea":"# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","ea8f55bf":"# Predicting the Test set results\ny_pred = KNN.predict(X)\n\n# Making the Confusion Matrix w\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y, y_pred.round())\nnp.set_printoptions(precision=2)","4ad747f5":"# Plot non-normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","c7f6e835":"# Fitting Ada-boost to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nADA = AdaBoostClassifier(base_estimator=None, n_estimators=200, learning_rate=1.0)\nADA.fit(X_train, y_train)","3abf51a5":"# Predicting the Test set results\ny_pred = ADA.predict(X_test)\n\n# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\ncm","a9646efb":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","96ddf729":"test_labels=ADA.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","5b57ff35":"# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","3a4122e2":"# Fitting GradientBoosting to the Training set\nfrom sklearn.neighbors import KNeighborsClassifier\nGradientBoo = GradientBoostingClassifier(loss='deviance', learning_rate=0.1, n_estimators=200, subsample=1.0,\n                                   min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, \n                                   max_depth=3,\n                                   init=None, random_state=None, max_features=None, verbose=0)\nGradientBoo.fit(X_train, y_train)","7e8a0d9a":"# Predicting the Test set results\ny_pred = GradientBoo.predict(X_test)\n\n# Making the Confusion Matrix \nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\nnp.set_printoptions(precision=2)\ncm","e0155aeb":"#Let's see how our model performed\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test, y_pred))","1f706de7":"test_labels=GradientBoo.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","4997bb21":"# Plot normalized confusion matrix\nplot_confusion_matrix(cm,classes=[0,1])\nsns.set(rc={'figure.figsize':(6,6)})\nplt.show()","cd0171bf":"test_labels=classifier.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","cca3e261":"from sklearn.model_selection import cross_val_score\ndef cvDictGen(functions, scr, X_train=X, y_train=y, cv=5, verbose=1):\n    cvDict = {}\n    for func in functions:\n        cvScore = cross_val_score(func, X_train, y_train, cv=cv, verbose=verbose, scoring=scr)\n        cvDict[str(func).split('(')[0]] = [cvScore.mean(), cvScore.std()]\n    \n    return cvDict\n\ndef cvDictNormalize(cvDict):\n    cvDictNormalized = {}\n    for key in cvDict.keys():\n        for i in cvDict[key]:\n            cvDictNormalized[key] = ['{:0.2f}'.format((cvDict[key][0]\/cvDict[cvDict.keys()[0]][0])),\n                                     '{:0.2f}'.format((cvDict[key][1]\/cvDict[cvDict.keys()[0]][1]))]\n    return cvDictNormalized\n","02af4663":"cvD = cvDictGen(functions=[Random, Logistic, KNN, ADA, GradientBoo], scr='roc_auc')\ncvD","fc5da235":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import randint","8ef0dd6a":"adaHyperParams = {'n_estimators': [10,50,100,200,420]}\ngridSearchAda = RandomizedSearchCV(estimator=ADA, param_distributions=adaHyperParams, n_iter=5,\n                                   scoring='roc_auc', fit_params=None, cv=None, verbose=2).fit(X_train, y_train)\ngridSearchAda.best_params_, gridSearchAda.best_score_","433e43d0":"gbHyperParams = {'loss' : ['deviance', 'exponential'],\n                 'n_estimators': randint(10, 500),\n                 'max_depth': randint(1,10)}\ngridSearchGB = RandomizedSearchCV(estimator=GradientBoo, param_distributions=gbHyperParams, n_iter=10,\n                                   scoring='roc_auc', fit_params=None, cv=None, verbose=2).fit(X_train, y_train)\ngridSearchGB.best_params_, gridSearchGB.best_score_","566d2bc2":"#Fitting both ADA and Gradient\nbestGbModFitted = gridSearchGB.best_estimator_.fit(X_train, y_train)\nbestAdaModFitted = gridSearchAda.best_estimator_.fit(X_train, y_train)","7f15e296":"cvDictbestpara = cvDictGen(functions=[bestGbModFitted, bestAdaModFitted], scr='roc_auc')\ncvDictbestpara","af0617d0":"test_labels=bestGbModFitted.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","af1744b0":"test_labels=bestAdaModFitted.predict_proba(np.array(X_test.values))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","42262281":"import numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\n\ntransformer = FunctionTransformer(np.log1p)\nX_train_1 = np.array(X_train)\nX_train_transform = transformer.transform(X_train_1)","226e83a4":"bestGbModFitted_transformed = gridSearchGB.best_estimator_.fit(X_train_transform, y_train)\nbestAdaModFitted_transformed = gridSearchAda.best_estimator_.fit(X_train_transform, y_train)\n\ncvDictbestpara_transform = cvDictGen(functions=[bestGbModFitted_transformed, bestAdaModFitted_transformed],\n                                     scr='roc_auc')\ncvDictbestpara_transform","1d177cd6":"import numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\n\ntransformer = FunctionTransformer(np.log1p)\nX_test_1 = np.array(X_test)\nX_test_transform = transformer.transform(X_test_1)\nX_test_transform","5d285d75":"test_labels=bestGbModFitted_transformed.predict_proba(np.array(X_test_transform))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","aba6fbdf":"test_labels=bestAdaModFitted_transformed.predict_proba(np.array(X_test_transform))[:,1]\nroc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","4c9b4be8":"from sklearn.ensemble import VotingClassifier\nvotingMod = VotingClassifier(estimators=[('gb', bestGbModFitted_transformed), \n                                         ('ada', bestAdaModFitted_transformed)], voting='soft',weights=[2,1])\nvotingMod = votingMod.fit(X_train_transform, y_train)","b697c4ab":"test_labels=votingMod.predict_proba(np.array(X_test_transform))[:,1]\nvotingMod.score(X_test_transform, y_test)","6e464888":"roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","a4095299":"#without transform voting\nfrom sklearn.ensemble import VotingClassifier\nvotingMod_old = VotingClassifier(estimators=[('gb', bestGbModFitted), ('ada', bestAdaModFitted)], \n                                 voting='soft',weights=[2,1])\nvotingMod_old = votingMod.fit(X_train, y_train)","a11de324":"test_labels=votingMod_old.predict_proba(np.array(X_test.values))[:,1]","63119651":"roc_auc_score(y_test,test_labels , average='macro', sample_weight=None)","a6da7b06":"# Read Training dataset as well as drop the index column\ntest = pd.read_csv('..\/input\/test.csv').drop('Unnamed: 0', axis = 1)\n# For each column heading we replace \"-\" and convert the heading in lowercase \ncleancolumn = []\nfor i in range(len(test.columns)):\n    cleancolumn.append(test.columns[i].replace('-', '').lower())\ntest.columns = cleancolumn","44aa9448":"\ntest.head()","dfbc0d14":"test.drop(['seriousdlqin2yrs'], axis=1, inplace=True)","a5836c2e":"test.fillna((train_median.median()), inplace=True)\n","9071b669":"test.head()","56b9b23d":"test_labels_votingMod_old = votingMod_old.predict_proba(np.array(test.values))[:,1]\nprint (len(test_labels_votingMod_old))","695e4e70":"\noutput = pd.DataFrame({'ID':test.index, 'probability':test_labels_votingMod_old})","710bf0aa":"output.to_csv(\".\/predictions.csv\", index=False)","32ff9ac7":"import numpy as np\nfrom sklearn.preprocessing import FunctionTransformer\n\ntransformer = FunctionTransformer(np.log1p)\ntest_data_temp = np.array(test)\ntest_data_transform = transformer.transform(test_data_temp)","43e6b803":"\ntest_labels_votingMod = votingMod.predict_proba(np.array(test.values))[:,1]\nprint (len(test_labels_votingMod_old))","b25dae9a":"output = pd.DataFrame({'ID':test.index, 'probability':test_labels_votingMod})\noutput.to_csv(\".\/predictions_voting_Feature_transformation.csv\", index=False)","ef990ab8":"# Feature Selection\n","f0eb1a7a":"# Introducing the problem statement\nFirst of all, let's try to understand the application that we want to develop or the problem that we are trying to solve. Once we understand the problem statement and it's use case, it will be much easier for us to develop the application. So let's begin!\n\nHere, we want to help financial companies, such as banks, NBFS, lenders, and so on. We will make an algorithm that can predict to whom financial institutes should give loans or credit. Now you may ask what is the significance of this algorithm? Let me explain that in detail. When a financial institute lends money to a customer, they are taking some kind of risk. So, before lending, financial institutes check whether or not the borrower will have enough money in the future to pay back their loan. Based on the customer's current income and expenditure, many financial institutes perform some kind of analysis that helps them decide whether the borrower will be a good customer for that bank or not. This kind of analysis is manual and time-consuming. So, it needs some kind of automation. If we develop an algorithm, that will help financial institutes gauge their customers efficiently and effectively.Your next question may be what is the output of our algorithm? Our algorithm will generate probability. This probability value will indicate the chances of borrowers defaulting. Defaulting means borrowers cannot repay their loan in a certain amount of time. Here, probability indicates the chances of a customer not paying their loan EMI on time, resulting in default. So, a higher probability value indicates that the customer would be a bad or inappropriate borrower (customer) for the financial institution, as they may default in the next 2 years. A lower probability value indicates that the customer will be a good or appropriate borrower (customer) for the financial institution and will not default in the next 2 years.\n\n\n\n\n","244e3d0a":"# Testing on Real Test Dataset","8df442b0":"# Voting based ensamble model","4357a2a9":"#The algorithms that we are going to choose are as follows (this selection is based on intuition):\n\nRandomForest\n\nLogistic Regression\n\nK-Nearest Neighbor (KNN)\n\nAdaBoost\n\nGradientBoosting\n","bfd681ab":"Gradient Boosting","321346ec":"# Basic data analysis or exploratory data analysis (EDA)","864f7aeb":"## Missing Values","3d025019":"# Train models with help of new hyper parameter","9f5194f0":"# Hyper parameter optimization using Randomized search","5ff61abb":"\n# Train and build baseline model","80376213":"ADA Boosting\n","9f83b8c1":"# Correlation","f7c72c33":"# Feature Transformation"}}