{"cell_type":{"bf7c5ded":"code","f155ef69":"code","82b608c9":"code","115dad03":"code","7444de30":"code","51aedfa9":"code","fbc55e28":"code","b86b0d0d":"code","c5ad9975":"code","2fce09b0":"code","5331b602":"code","a8c3ecf8":"code","605719b9":"code","d56d9c1c":"code","1bcf0156":"code","6925eb5d":"code","631218b0":"code","9da1a30f":"code","4e84a0e1":"code","eb19cf3e":"code","2a4a9785":"code","5973b6b8":"code","79f4e36c":"code","d2dcee47":"code","088fdf82":"code","f7bf137c":"code","cb26cae3":"code","57051e97":"code","8f4c3d17":"code","032cad58":"code","2a7c6499":"code","74df2736":"code","1a798404":"code","95c2bf86":"code","f7dbbd6d":"code","6b3d3944":"code","d05fd110":"code","50ee5773":"code","a6e9fea8":"code","751d33f7":"code","19fd3530":"code","b1861cb2":"code","5b5fab05":"code","532922d2":"code","a6aa354c":"code","7341a937":"code","1f063820":"code","7dcf895c":"code","13abb78f":"code","261d6a08":"code","238a7221":"code","fdf586b9":"code","6000d013":"code","65586e0f":"code","292f8509":"code","c2bea410":"code","1795f8ef":"code","ae902ab3":"code","e53839b3":"code","91f456f5":"code","fba0bb60":"code","afb90636":"code","e7592fe6":"code","6160d10c":"code","ee992e74":"code","33c78cb0":"code","0a3690dc":"code","92544b97":"code","2af3d09d":"code","a8cafa6e":"code","fed9c94a":"code","0a15ac6b":"code","7c7553f5":"code","b5eef1f3":"code","4f8ec206":"code","17d359d2":"code","038242f0":"code","86455ee9":"code","6c3155a0":"code","75734133":"code","bc0ef7d5":"code","14383f19":"markdown","584c275f":"markdown","479568a1":"markdown","ec45be87":"markdown","4d99c38e":"markdown","1cb7d76d":"markdown","1a7cbaaa":"markdown","9d6d20e9":"markdown","d9f239e4":"markdown","d7830394":"markdown","0cea8780":"markdown","9152767d":"markdown","2d9df6b7":"markdown","7cf75655":"markdown","1a66684f":"markdown","a5db7bca":"markdown","63fd409f":"markdown","8b60ceb5":"markdown","faf99cbc":"markdown","ada30c02":"markdown","577cabc5":"markdown","0bbdcf31":"markdown","2f2c41e6":"markdown","58fdb52b":"markdown"},"source":{"bf7c5ded":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","f155ef69":"#reading the csv data file\ndf =  pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\ndf.head()","82b608c9":"df.shape","115dad03":"df.columns","7444de30":"df.info()","51aedfa9":"df = df.drop(['Unnamed: 32'],axis=1)","fbc55e28":"df.shape","b86b0d0d":"df.describe()","c5ad9975":"df.corr()","2fce09b0":"# PLOTTING HEATMAP FOR VISUALISING CORRELATION BETWEEN FEATURES\nplt.figure(figsize=(20, 10))\nheatmap = sb.heatmap(df.corr(),cmap='BrBG',annot=True)\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);","5331b602":"#check the balance in deendent feature\nplt.figure(figsize=(10, 8))\nsb.scatterplot(y = df.index , x= df.diagnosis,palette = 'BrBe')","a8c3ecf8":"#CHECKING DISTRIBUTION OF DATA IN FEATURES\nfig, axes = plt.subplots(2,3,figsize=(20,8))\nsb.distplot(df['area_mean'],ax = axes[0,0])\nsb.distplot(df['radius_mean'],ax = axes[0,1])\nsb.distplot(df['texture_mean'],ax = axes[0,2])\nsb.distplot(df['perimeter_mean'],ax = axes[1,0])\nsb.distplot(df['smoothness_mean'],ax = axes[1,1])\nsb.distplot(df['concavity_mean'],ax = axes[1,2])","605719b9":"#CONVERTING THE CATEGORICAL DATA TO NUMERICAL\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()","d56d9c1c":"df['diagnosis'] = le.fit_transform(df['diagnosis'])","1bcf0156":"df['diagnosis']","6925eb5d":"from sklearn.preprocessing import StandardScaler","631218b0":"scaler = StandardScaler()\nscaler.fit(df)","9da1a30f":"scaled_data = scaler.transform(df)","4e84a0e1":"from sklearn.decomposition import PCA\npca = PCA(n_components=2)","eb19cf3e":"pca.fit(scaled_data)","2a4a9785":"x_pca = pca.transform(scaled_data)","5973b6b8":"scaled_data.shape, x_pca.shape","79f4e36c":"pca_df = pd.DataFrame(data = x_pca, columns = ['principal component 1', 'principal component 2'])\npca_df","d2dcee47":"plt.figure(figsize=(16,8))\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=14)\nplt.xlabel('Principal Component - 1',fontsize=20)\nplt.ylabel('Principal Component - 2',fontsize=20)\nplt.title(\"Principal Component Analysis of Breast Cancer\",fontsize=20)\ntargets = [0,1]\ncolors = ['r', 'g']\nfor target, color in zip(targets,colors):\n    indicesToKeep = df['diagnosis'] == target\n    plt.scatter(pca_df.loc[indicesToKeep, 'principal component 1']\n               , pca_df.loc[indicesToKeep, 'principal component 2'], c = color, s = 50)\n\nplt.legend(targets,prop={'size': 15})","088fdf82":"from sklearn.model_selection import train_test_split","f7bf137c":"X = df[['radius_mean', 'texture_mean', 'perimeter_mean','area_mean', 'smoothness_mean', 'compactness_mean', 'concavity_mean',\n       'concave points_mean', 'symmetry_mean', 'fractal_dimension_mean','radius_se', 'texture_se', 'perimeter_se', 'area_se', 'smoothness_se','compactness_se', 'concavity_se', 'concave points_se', 'symmetry_se',\n       'fractal_dimension_se', 'radius_worst', 'texture_worst','perimeter_worst', 'area_worst', 'smoothness_worst',\n       'compactness_worst', 'concavity_worst', 'concave points_worst','symmetry_worst', 'fractal_dimension_worst']]\nY = df[['diagnosis']]","cb26cae3":"X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size = 0.3)","57051e97":"X_train.shape, Y_train.shape, X_test.shape, Y_test.shape","8f4c3d17":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","032cad58":"from sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()","2a7c6499":"LR.fit(X_train,Y_train)","74df2736":"Y_LR = LR.predict(X_test)","1a798404":"from sklearn.metrics import accuracy_score,confusion_matrix\nacc_LR = accuracy_score(Y_test, Y_LR)\nprint('ACCURACY SCORE: ',acc_LR)\ncm_LR = confusion_matrix(Y_test,Y_LR)\nprint('CONFUSION MATRIX: \\n',cm_LR)","95c2bf86":"from sklearn.neighbors import KNeighborsClassifier","f7dbbd6d":"#Finding best possible number of neighbors\nno_of_neighbors_and_accuracies = {}\nfor i in range(1,15):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,Y_train)\n    Y_knn = knn.predict(X_test)\n    score = accuracy_score(Y_knn,Y_test)\n    no_of_neighbors_and_accuracies[i] = score","6b3d3944":"no_of_neighbors_and_accuracies","d05fd110":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train,Y_train)","50ee5773":"Y_knn = knn.predict(X_test)","a6e9fea8":"acc_knn = accuracy_score(Y_test, Y_knn)\nprint('ACCURACY SCORE: ',acc_knn)\ncm_knn = confusion_matrix(Y_test,Y_knn)\nprint('CONFUSION MATRIX: \\n',cm_knn)","751d33f7":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(criterion='gini')","19fd3530":"dtc.fit(X_train,Y_train)","b1861cb2":"Y_dtc = dtc.predict(X_test)","5b5fab05":"acc_dtc = accuracy_score(Y_test, Y_dtc)\nprint('ACCURACY SCORE: ',acc_dtc)\ncm_dtc = confusion_matrix(Y_test,Y_dtc)\nprint('CONFUSION MATRIX: \\n',cm_dtc)","532922d2":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier(criterion='entropy')","a6aa354c":"rfc.fit(X_train, Y_train)","7341a937":"Y_rfc = rfc.predict(X_test)","1f063820":"acc_rfc = accuracy_score(Y_test, Y_rfc)\nprint('ACCURACY SCORE: ',acc_rfc)\ncm_rfc = confusion_matrix(Y_test,Y_rfc)\nprint('CONFUSION MATRIX: \\n',cm_rfc)","7dcf895c":"from sklearn.svm import SVC\nsvc = SVC()","13abb78f":"svc.fit(X_train,Y_train)","261d6a08":"Y_svc = svc.predict(X_test)","238a7221":"acc_svc = accuracy_score(Y_test, Y_svc)\nprint('ACCURACY SCORE: ',acc_svc)\ncm_svc = confusion_matrix(Y_test,Y_svc)\nprint('CONFUSION MATRIX: \\n',cm_svc)","fdf586b9":"from sklearn.naive_bayes import GaussianNB  \ngnb = GaussianNB() ","6000d013":"gnb.fit(X_train, Y_train)","65586e0f":"Y_gnb = gnb.predict(X_test) ","292f8509":"acc_gnb = accuracy_score(Y_test, Y_gnb)\nprint('ACCURACY SCORE: ',acc_gnb)\ncm_gnb = confusion_matrix(Y_test,Y_gnb)\nprint('CONFUSION MATRIX: \\n',cm_gnb)","c2bea410":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier()","1795f8ef":"gbc.fit(X_train,Y_train)","ae902ab3":"Y_gbc = gbc.predict(X_test)","e53839b3":"acc_gbc = accuracy_score(Y_test, Y_gbc)\nprint('ACCURACY SCORE: ',acc_gbc)\ncm_gbc = confusion_matrix(Y_test,Y_gbc)\nprint('CONFUSION MATRIX: \\n',cm_gbc)","91f456f5":"from sklearn.linear_model import SGDClassifier\nsgdc = SGDClassifier()","fba0bb60":"Y_sgdc = sgdc.fit(X_train,Y_train)","afb90636":"Y_sgdc = sgdc.predict(X_test)","e7592fe6":"acc_sgdc = accuracy_score(Y_test, Y_sgdc)\nprint('ACCURACY SCORE: ',acc_sgdc)\ncm_sgdc = confusion_matrix(Y_test,Y_sgdc)\nprint('CONFUSION MATRIX: \\n',cm_sgdc)","6160d10c":"from sklearn.ensemble import AdaBoostClassifier\nadb = AdaBoostClassifier()","ee992e74":"adb.fit(X_train,Y_train)","33c78cb0":"Y_adb = adb.predict(X_test)","0a3690dc":"acc_adb = accuracy_score(Y_test, Y_adb)\nprint('ACCURACY SCORE: ',acc_adb)\ncm_adb = confusion_matrix(Y_test,Y_adb)\nprint('CONFUSION MATRIX: \\n',cm_adb)","92544b97":"from xgboost import XGBClassifier\nxgb = XGBClassifier()","2af3d09d":"xgb.fit(X_train,Y_train)","a8cafa6e":"Y_xgb = xgb.predict(X_test)","fed9c94a":"acc_xgb = accuracy_score(Y_test, Y_xgb)\nprint('ACCURACY SCORE: ',acc_xgb)\ncm_xgb = confusion_matrix(Y_test,Y_xgb)\nprint('CONFUSION MATRIX: \\n',cm_xgb)","0a15ac6b":"from catboost import CatBoostClassifier\ncb = CatBoostClassifier()","7c7553f5":"cb.fit(X_train,Y_train)","b5eef1f3":"Y_cb = cb.predict(X_test)","4f8ec206":"acc_cb = accuracy_score(Y_test, Y_cb)\nprint('ACCURACY SCORE: ',acc_cb)\ncm_cb = confusion_matrix(Y_test,Y_cb)\nprint('CONFUSION MATRIX: \\n',cm_cb)","17d359d2":"from lightgbm import LGBMClassifier","038242f0":"lg = LGBMClassifier()","86455ee9":"lg.fit(X_train,Y_train)","6c3155a0":"Y_lg = lg.predict(X_test)","75734133":"acc_lg = accuracy_score(Y_test, Y_lg)\nprint('ACCURACY SCORE: ',acc_lg)\ncm_lg = confusion_matrix(Y_test,Y_lg)\nprint('CONFUSION MATRIX: \\n',cm_lg)","bc0ef7d5":"models = pd.DataFrame({\n    'Model': ['Logistic Regression','KNN','Decision Tree','Random Forest','Support Vector Machines',\n              'Naive Bayes','Gradient Boosting','Stochastic gradient decent','AdaBoost','XGboost','Catboost','LightGBM'],\n    'Score': [acc_LR, acc_knn, acc_dtc,acc_rfc, acc_svc, acc_gnb, acc_gbc, acc_sgdc, acc_adb, acc_xgb, acc_cb, acc_lg]})\nmodels.sort_values(by='Score', ascending=False)","14383f19":"Principal Component Analysis, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.\nPCA is a very flexible tool and allows analysis of datasets that may contain, for example, multicollinearity, missing values, categorical data, and imprecise measurements. The goal is to extract the important information from the data and to express this information as a set of summary indices called principal components.\n    So to sum up, the idea of PCA is simple \u2014 reduce the number of variables of a data set, while preserving as much information as possible.\n- Read more about PCA in details from [here](https:\/\/builtin.com\/data-science\/step-step-explanation-principal-component-analysis)","584c275f":"## Exploring the data","479568a1":"## Visualising PCA","ec45be87":"![](https:\/\/ars.els-cdn.com\/content\/image\/1-s2.0-S2214784516300147-gr1.jpg)","4d99c38e":"- Here we can see that 3 gives highest accuracy, so we'll choose n_neighbors = 3","1cb7d76d":"### 2)KNN Classification","1a7cbaaa":"## Feature Scaling","9d6d20e9":"### 9)XGBoost","d9f239e4":"### 4)Random Forest","d7830394":"### 3)Decision Tree ","0cea8780":"### 8)Stochastic Gradient Decent","9152767d":"### 1) Logisic Regression","2d9df6b7":"# <center> BREAST CANCER PREDICTION\n    \n![](https:\/\/www.dadberg.com\/wp-content\/uploads\/2021\/04\/f-958x575.png)\n    \n    - Breast cancer is the most common invasive cancer in women and the second leading cause of cancer death in women after lung cancer.\n    -  The Wisconsin Breast Cancer dataset is obtained from a prominent machine learning database named UCI machine learning database. Using the Breast Cancer Wisconsin (Diagnostic) Database, we can create a classifier that can help diagnose patients and predict the likelihood of a breast cancer.\n    - In this Notebbok I have used almost all useful classifiers for classification of breast cancer being benign or malignant.\n    - Before feeding the data into Classifying Model, I have preprocessed it by dimension reducing technique of PCA(Principle Component Analysis).","7cf75655":"### 6)Naive Bayes","1a66684f":"## Importing Libraries","a5db7bca":"## Split data into train and test","63fd409f":"### 8)AdaBoost","8b60ceb5":"### 5)Support Vector Machine","faf99cbc":"### 10)CatBoost","ada30c02":"### 7)Gradient Boosting","577cabc5":"### Thank you. Consider **UPVOTING** if you find it useful :)","0bbdcf31":"## We can conclude that Logistic Regression & SVM gives the highest possible accuracy which is 98.8%","2f2c41e6":"![](https:\/\/miro.medium.com\/max\/2000\/1*KdvxqXIOkb9JY_BeUWvpxg.jpeg)","58fdb52b":"### 11)Light GBM"}}