{"cell_type":{"0bf36b04":"code","62cd2659":"code","e37d9340":"code","0dcd111c":"code","36d9ce2e":"code","99adfd53":"code","1fb2f212":"code","10f1a687":"code","780ec117":"code","e6a3d198":"code","1450f789":"code","5ed966d5":"code","c1e40d57":"code","942958a9":"code","a3ad49d5":"code","ac421ed7":"code","c9fe3332":"code","9fd9aa29":"code","fb0be864":"markdown","45b7786a":"markdown","5bd76c6a":"markdown","bbfdefd7":"markdown","fffc73f8":"markdown","054380b4":"markdown","619fc974":"markdown","e78e25d5":"markdown","bebc520f":"markdown","775d0795":"markdown","ab855ea0":"markdown","c393a08c":"markdown","d9ddb907":"markdown","c5e52bd4":"markdown","b8f7da4c":"markdown","4f84a583":"markdown","76e73789":"markdown","8baa10bb":"markdown","1bf2a736":"markdown","4cf6fbeb":"markdown","886c3141":"markdown","1929bc99":"markdown","02f4db2d":"markdown","dc0d8874":"markdown"},"source":{"0bf36b04":"!pip install -q wandb --upgrade\n!pip install -q transformers\n!pip install -q accelerate","62cd2659":"# Hide warnings\nimport warnings\nwarnings.filterwarnings('ignore', category=DeprecationWarning)\nwarnings.filterwarnings('ignore', category=FutureWarning)\n\n# Python\nimport os\nimport random\nfrom collections import defaultdict\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\n\n# Utilities\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\n# Pytorch for Deep Learning\nimport torch\nimport transformers\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.cuda import amp\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.optim as optim\nfrom torch.optim.optimizer import Optimizer\nfrom torch.optim import lr_scheduler\n\n#HuggingFace Libraries\nimport transformers\nfrom transformers import SqueezeBertTokenizer, SqueezeBertModel\nfrom accelerate import Accelerator\n\naccelerator = Accelerator()\n\n# Weights and Biases Tool\nimport wandb","e37d9340":"params = {\n    'seed': 42,\n    'model' : 'squeezebert\/squeezebert-uncased',\n    'name': 'squeezebert-uncased',\n    'tokenizer' : SqueezeBertTokenizer.from_pretrained('squeezebert\/squeezebert-uncased'),\n    'device': accelerator.device,\n    'lr': 1e-4,\n    'weight_decay': 1e-6,\n    'batch_size': 32,\n    'num_workers' : 8,\n    'epochs': 3,\n    'max_len': 205,\n    'nfolds': 5,\n    'gradient_accumulation_steps': 2\n}","0dcd111c":"def seed_everything(seed=42):\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \nseed_everything(params['seed'])","36d9ce2e":"train_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/train.csv\")\ntrain_df.head()","99adfd53":"test_df = pd.read_csv(\"..\/input\/commonlitreadabilityprize\/test.csv\")\ntest_df.head()","1fb2f212":"class BERTDataset(Dataset):\n    def __init__(self, review, target=None, is_test = False):\n        self.review = review\n        self.target = target\n        self.is_test = is_test\n        self.tokenizer = params['tokenizer']\n        self.max_len = params['max_len']\n    \n    def __len__(self):\n        return len(self.review)\n    \n    def __getitem__(self, idx):\n        review = str(self.review[idx])\n        review = ' '.join(review.split())\n        \n        inputs = self.tokenizer.encode_plus(\n            review,\n            None,\n            truncation=True,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True\n        )   \n        \n        ids = torch.tensor(inputs['input_ids'], dtype=torch.long)\n        mask = torch.tensor(inputs['attention_mask'], dtype=torch.long)\n        token_type_ids = torch.tensor(inputs['token_type_ids'], dtype=torch.long)\n  \n  \n        targets = torch.tensor(self.target[idx], dtype=torch.float)\n        return {\n            'ids': ids,\n            'mask': mask,\n            'token_type_ids': token_type_ids,\n            'targets': targets\n                }","10f1a687":"class MetricMonitor:\n    def __init__(self, float_precision=3):\n        self.float_precision = float_precision\n        self.reset()\n\n    def reset(self):\n        self.metrics = defaultdict(lambda: {\"val\": 0, \"count\": 0, \"avg\": 0})\n\n    def update(self, metric_name, val):\n        metric = self.metrics[metric_name]\n\n        metric[\"val\"] += val\n        metric[\"count\"] += 1\n        metric[\"avg\"] = metric[\"val\"] \/ metric[\"count\"]\n\n    def __str__(self):\n        return \" | \".join(\n            [\n                \"{metric_name}: {avg:.{float_precision}f}\".format(\n                    metric_name=metric_name, avg=metric[\"avg\"],\n                    float_precision=self.float_precision\n                )\n                for (metric_name, metric) in self.metrics.items()\n            ]\n        )\n    \ndef use_rmse_score(output, target):\n    return np.sqrt(mean_squared_error(target, output))","780ec117":"def get_loader(train_data, valid_data):\n    \n    train_set = BERTDataset(\n        review = train_data['excerpt'].values,\n        target = train_data['target'].values\n    )\n\n    valid_set = BERTDataset(\n        review = valid_data['excerpt'].values,\n        target = valid_data['target'].values\n    )\n\n    train_loader = DataLoader(\n        train_set,\n        batch_size = params['batch_size'],\n        shuffle = True,\n        num_workers=params['num_workers'],\n        pin_memory = True\n    )\n\n    valid_loader = DataLoader(\n        valid_set,\n        batch_size = params['batch_size'],\n        shuffle = False,\n        num_workers=params['num_workers'],\n        pin_memory = True\n    )\n    \n    return train_loader, valid_loader","e6a3d198":"class SqueezeBert(nn.Module):\n    def __init__(self):\n        super(SqueezeBert, self).__init__()\n        self.bert = SqueezeBertModel.from_pretrained(params['model'])\n        self.drop = nn.Dropout(0.3)\n        self.fc = nn.Linear(768, 1)\n    \n    def forward(self, ids, mask, token_type_ids):\n        _, output = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n        output = self.drop(output)\n        output = self.fc(output)\n        return output","1450f789":"import math\nfrom typing import TYPE_CHECKING, Any, Callable, Optional\n\nif TYPE_CHECKING:\n    from torch.optim.optimizer import _params_t\nelse:\n    _params_t = Any\n\nclass MADGRAD(Optimizer):\n\n    def __init__(\n        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, weight_decay: float = 0, eps: float = 1e-6,\n    ):\n        if momentum < 0 or momentum >= 1:\n            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n        if lr <= 0:\n            raise ValueError(f\"Learning rate {lr} must be positive\")\n        if weight_decay < 0:\n            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n        if eps < 0:\n            raise ValueError(f\"Eps must be non-negative\")\n\n        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n        super().__init__(params, defaults)\n\n    @property\n    def supports_memory_efficient_fp16(self) -> bool:\n        return False\n\n    @property\n    def supports_flat_params(self) -> bool:\n        return True\n\n    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        if 'k' not in self.state:\n            self.state['k'] = torch.tensor([0], dtype=torch.long)\n        k = self.state['k'].item()\n\n        for group in self.param_groups:\n            eps = group[\"eps\"]\n            lr = group[\"lr\"] + eps\n            decay = group[\"weight_decay\"]\n            momentum = group[\"momentum\"]\n\n            ck = 1 - momentum\n            lamb = lr * math.pow(k + 1, 0.5)\n\n            for p in group[\"params\"]:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data\n                state = self.state[p]\n\n                if \"grad_sum_sq\" not in state:\n                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n                    state[\"s\"] = torch.zeros_like(p.data).detach()\n                    if momentum != 0:\n                        state[\"x0\"] = torch.clone(p.data).detach()\n\n                if momentum != 0.0 and grad.is_sparse:\n                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n\n                grad_sum_sq = state[\"grad_sum_sq\"]\n                s = state[\"s\"]\n\n                # Apply weight decay\n                if decay != 0:\n                    if grad.is_sparse:\n                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n\n                    grad.add_(p.data, alpha=decay)\n\n                if grad.is_sparse:\n                    grad = grad.coalesce()\n                    grad_val = grad._values()\n\n                    p_masked = p.sparse_mask(grad)\n                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n                    s_masked = s.sparse_mask(grad)\n\n                    # Compute x_0 from other known quantities\n                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 \/ 3).add_(eps)\n                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n\n                    # Dense + sparse op\n                    grad_sq = grad * grad\n                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n\n                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 \/ 3).add_(eps)\n\n                    s.add_(grad, alpha=lamb)\n                    s_masked._values().add_(grad_val, alpha=lamb)\n\n                    # update masked copy of p\n                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n                    # Copy updated masked p to dense p using an add operation\n                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n                    p.data.add_(p_masked, alpha=-1)\n                else:\n                    if momentum == 0:\n                        # Compute x_0 from other known quantities\n                        rms = grad_sum_sq.pow(1 \/ 3).add_(eps)\n                        x0 = p.data.addcdiv(s, rms, value=1)\n                    else:\n                        x0 = state[\"x0\"]\n\n                    # Accumulate second moments\n                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n                    rms = grad_sum_sq.pow(1 \/ 3).add_(eps)\n\n                    # Update s\n                    s.data.add_(grad, alpha=lamb)\n\n                    # Step\n                    if momentum == 0:\n                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n                    else:\n                        z = x0.addcdiv(s, rms, value=-1)\n\n                        # p is a moving average of z\n                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n\n\n        self.state['k'] += 1\n        return loss","5ed966d5":"def get_criterion(outputs, targets):\n    return torch.sqrt(nn.MSELoss()(outputs, targets))","c1e40d57":"def get_scheduler(optimizer, nb_train_steps):\n    return  transformers.get_linear_schedule_with_warmup(\n                                                            optimizer,\n                                                            num_warmup_steps=0,\n                                                            num_training_steps=nb_train_steps\n                                                        )\n","942958a9":"model = SqueezeBert()\nmodel = model.to(params['device'])\noptimizer = MADGRAD(model.parameters(), lr=params['lr'], weight_decay=params['weight_decay'])","a3ad49d5":"def train(train_loader, model, optimizer, epoch, params, scheduler):\n    metric_monitor = MetricMonitor()\n    model.train()\n    stream = tqdm(enumerate(train_loader), total=len(train_loader))\n    scaler = amp.GradScaler()   \n    for idx, inputs in stream:\n\n        ids = inputs['ids'].to(params['device'], dtype=torch.long)\n        mask = inputs['mask'].to(params['device'], dtype=torch.long)\n        ttis = inputs['token_type_ids'].to(params['device'], dtype=torch.long)\n        target = inputs['targets'].to(params['device'], dtype=torch.float)\n\n        # AMP with Gradient Scaling\n        with amp.autocast(enabled=True):\n            output = model(ids=ids, mask=mask, token_type_ids=ttis).view(-1)\n            loss = get_criterion(output, target)\n            loss = loss \/ params['gradient_accumulation_steps']\n\n        accelerator.backward(scaler.scale(loss))\n\n        # Gradient Accumulation\n        if idx % params['gradient_accumulation_steps'] == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n            scheduler.step()\n\n            \n                \n        rmse_score = use_rmse_score(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n        metric_monitor.update('RMSE', rmse_score)\n        wandb.log({\"Train RMSE\":rmse_score})\n        \n\n        stream.set_description(\n            \"Epoch: {epoch}. Train.      {metric_monitor}\".format(\n                epoch=epoch,\n                metric_monitor=metric_monitor)\n        )","ac421ed7":"def validate(val_loader, model,epoch, params):\n    metric_monitor = MetricMonitor()\n    model.eval()\n    stream = tqdm(enumerate(val_loader), total=len(val_loader))\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for idx, inputs in stream:\n            ids = inputs['ids'].to(params['device'], dtype=torch.long)\n            mask = inputs['mask'].to(params['device'], dtype=torch.long)\n            ttis = inputs['token_type_ids'].to(params['device'], dtype=torch.long)\n            target = inputs['targets'].to(params['device'], dtype=torch.float)\n            \n            output = model(ids=ids, mask=mask, token_type_ids=ttis).view(-1)\n            loss = get_criterion(output, target)\n            \n            rmse_score = use_rmse_score(output.detach().cpu().numpy(), target.detach().cpu().numpy())\n            \n            metric_monitor.update('RMSE', rmse_score)\n            wandb.log({\"Valid RMSE\":rmse_score})\n            stream.set_description(\n                \"Epoch: {epoch}. Validation. {metric_monitor}\".format(\n                    epoch=epoch,\n                    metric_monitor=metric_monitor)\n            )\n            \n            targets = target.detach().cpu().numpy().tolist()\n            outputs = output.detach().cpu().numpy().tolist()\n            \n            final_targets.extend(targets)\n            final_outputs.extend(outputs)\n    return final_outputs, final_targets","c9fe3332":"def create_folds(train_df):\n    \n    data = train_df.sample(frac=1).reset_index(drop=True)\n    data = data[['excerpt', 'target']]\n    kf = StratifiedKFold(n_splits=params['nfolds'])\n    nb_bins = int(np.floor(1 + np.log2(len(data))))\n    data.loc[:, 'bins'] = pd.cut(data['target'], bins=nb_bins, labels=False)\n    \n    return kf, data","9fd9aa29":"best_rmse = 100\nbest_epoch = -np.inf\nbest_model_name = None\n\nkf, data = create_folds(train_df)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X=data, y=data['bins'].values)):\n    \n    run = wandb.init(project='CommonLit', \n             config= {'competetion': 'CommonLit-Readability', '_wandb_kernel':'tang'}, \n             group = 'SqueezeBert',\n             job_type='train',\n             name = f'Fold{fold}')\n    \n    print(f\"{'='*36} Fold: {fold} {'='*36}\")\n\n    train_data = data.loc[train_idx]\n    valid_data = data.loc[valid_idx]\n    \n    train_loader, valid_loader = get_loader(train_data, valid_data)\n    nb_train_steps = int(len(train_data) \/ params['batch_size'] * params['epochs'])\n    scheduler = get_scheduler(optimizer, nb_train_steps)\n    model, optimizer, train_loader, valid_loader = accelerator.prepare(model, optimizer, train_loader, valid_loader)\n\n    for epoch in range(1, params['epochs'] + 1):\n\n        train(train_loader, model, optimizer, epoch, params, scheduler)\n        predictions, valid_targets = validate(valid_loader, model, epoch, params)\n        rmse = round(use_rmse_score(valid_targets, predictions), 3)\n        \n        torch.save(model.state_dict(),f\"{params['name']}_{epoch}_epoch_{rmse}_rmse.pth\")\n\n        if rmse < best_rmse:\n            best_rmse = rmse\n            best_epoch = epoch\n            best_model_name = f\"{params['name']}_{epoch}_epoch_{rmse}_rmse.pth\"\n            \n        \n    wandb.log({\"Best RMSE\":best_rmse})        \n    print(f\"Best RMSE in fold: {fold} was: {best_rmse:.4f}\")\n    print(f\"Final RMSE in fold: {fold} was: {rmse:.4f}\")\n\nprint(f\"Best RMSE of {best_rmse:.4f} was found in fold: {fold}\")","fb0be864":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Validation Loop<\/p>","45b7786a":"<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Accelerate provides an easy API to make your scripts run with mixed precision and on any kind of distributed setting (multi-GPUs, TPUs etc.) while still letting you write your own training loop. The same code can then runs seamlessly on your local machine for debugging or your training environment.\nIn 5 Lines of code we can run our scripts on any distributed setting!<\/p>","5bd76c6a":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Import Libraries<\/p>","bbfdefd7":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">MadGrad Optimizer<\/p>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Introducing MADGRAD, a novel optimization method in the family of AdaGrad adaptive gradient methods. MADGRAD shows excellent performance on deep learning optimization problems from multiple fields, including classification and image-to-image tasks in vision, and recurrent and bidirectionally-masked models in natural language processing. For each of these tasks, MADGRAD matches or outperforms both SGD and ADAM in test set performance, even on problems for which adaptive methods normally perform poorly.<br><br>\nMADGRAD is a general purpose optimizer that can be used in place of SGD or Adam may converge faster and generalize better. Currently GPU-only. Typically, the same learning rate schedule that is used for SGD or Adam may be used. The overall learning rate is not comparable to either method and should be determined by a hyper-parameter sweep.<\/p>\n\n![](https:\/\/warehouse-camo.ingress.cmh1.psfhosted.org\/9f23964e71fdb90e6464c7f643b6dff2a3ededb6\/68747470733a2f2f6769746875622e636f6d2f66616365626f6f6b72657365617263682f6d6164677261642f626c6f622f6d61737465722f666967757265732f6e6c702e706e673f7261773d74727565)","fffc73f8":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Class for Monitoring Loss and ROC<\/p>","054380b4":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\"><a href = 'https:\/\/wandb.ai\/tanishqgautam\/CommonLit?workspace=user-tanishqgautam'>Check out the Weights and Biases Dashboard here $\\rightarrow$ <\/a><\/p>","619fc974":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Train and Valid Loader<\/p>\n","e78e25d5":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Squeeze BERT Tokenizer and Model<\/p>\n\n<center><img src=\"https:\/\/i.imgur.com\/T6C6HG1.png\" width=\"1500\" alt=\"tokenizer\" \/><\/center><br><br>\n\n<center><img src=\"https:\/\/i.imgur.com\/sRXgv3p.png\" width=\"1500\" alt=\"tokenizer\" \/><\/center>","bebc520f":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">KFold Metrics Visualization<\/p>\n<center><img src=\"https:\/\/i.imgur.com\/zIrMSbF.png\" width=\"2000\" alt=\"Weights & Biases\" \/><\/center><br>","775d0795":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Cross Validation Results<\/p>\n\n<p style = \"font-family: garamond; font-size: 25px; font-style: normal; border-radius: 10px 10px; text-align:center\">We are able to achieve a RMSE score of 0.21 from the 5th Fold!<br><br> Weights & Biases provides us with such easy to use interface and tools to keep a track of our Evaluation metrics like training and validation RMSE along with other resources like Best fold RMSE and Gpu usage.<br><br> Let's take a look at some of our training graphs<\/p>","ab855ea0":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">W&B Initialization for K-FOLD CV<\/p>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">K-Fold CV gives a model with less bias compared to other methods. In K-Fold CV, we have a paprameter \u2018k\u2019. This parameter decides how many folds the dataset is going to be divided. Every fold gets chance to appears in the training set (k-1) times, which in turn ensures that every observation in the dataset appears in the dataset, thus enabling the model to learn the underlying data distribution better.<br><br>Another approach is to shuffle the dataset just once prior to splitting the dataset into k folds, and then split, such that the ratio of the observations in each class remains the same in each fold. Also the test set does not overlap between consecutive iterations. This approach is called Stratified K-Fold CV. This approach is useful for imbalanced datasets.<\/p>\n","c393a08c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Configurations\/Parameters<\/p>","d9ddb907":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Training with Gradient Accumulation<\/p>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">The idea behind gradient accumulation is very simple. It calculates the loss and gradients after each mini-batch, but instead of updating the model parameters, it waits and accumulates the gradients over consecutive batches. And then ultimately updates the parameters based on the cumulative gradient after a specified number of batches. It serves the same purpose as having a mini-batch with higher number of images.<\/p>","c5e52bd4":"<p p style = \"font-family: garamond; font-size:40px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">What are we discussing today? <\/p>\n <p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#006699; border-radius: 10px 10px; text-align:center\">Squeeze BERT <br>\n MadGrad optimizer <br>\n Gradient Accumulation <br>\n HuggingFace Accelerate <br>\n Weights and Biases for Experiment Tracking","b8f7da4c":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Loss Function, Optimizer and Scheduler<\/p>","4f84a583":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Accelerate by HuggingFace \ud83e\udd17<\/p>","76e73789":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Best Fold RMSE<\/p>\n\n<center><img src=\"https:\/\/i.imgur.com\/zyoth0U.png\" width=\"2000\" alt=\"Weights & Biases\" \/><\/center><br>","8baa10bb":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Custom Dataset<\/p>","1bf2a736":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">GPU Utilization<\/p>\n<center><img src=\"https:\/\/i.imgur.com\/ZoudAs7.png\" width=\"2000\" alt=\"Weights & Biases\" \/><\/center><br>","4cf6fbeb":"# <center><img src=\"https:\/\/i.imgur.com\/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" \/><\/center><br>\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Wandb is a developer tool for companies turn deep learning research projects into deployed software by helping teams track their models, visualize model performance and easily automate training and improving models.\nWe will use their tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues.<br><br>We'll be using this to train our K Fold Cross Validation and gain better insights about our training. <br><br><\/p>\n\n![img](https:\/\/i.imgur.com\/BGgfZj3.png)","886c3141":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Seed for Reproducibility<\/p>","1929bc99":"![](https:\/\/image4.owler.com\/logo\/commonlit-org_owler_20170203_094053_original.png)\n<center><img src=\"https:\/\/i.imgur.com\/iywFvlD.png\" width=\"2000\" alt=\"Weights & Biases\" \/><\/center><br>\n\n<h1 style = \"font-family: garamond; font-size: 40px; font-style: normal; letter-spacing: 1px; background-color: #f6f5f5; color :#6666ff; border-radius: 200px 200px; text-align:center\">Squeeze BERT<\/h1>\n\n<p style = \"font-family: garamond; font-size: 20px; font-style: normal; border-radius: 10px 10px; text-align:center\">Humans read and write hundreds of billions of messages every day. Further, due to the availability of large datasets, large computing systems, and better neural network models, natural language processing (NLP) technology has made significant strides in understanding, proofreading, and organizing these messages. Thus, there is a significant opportunity to deploy NLP in myriad applications to help web users, social networks, and businesses. <br><br>\nIn particular, we consider smartphones and other mobile devices as crucial platforms for deploying NLP models at scale. However, today\u2019s highly-accurate NLP neural network models such as BERT and RoBERTa are extremely computationally expensive, with BERT-base taking 1.7 seconds to classify a text snippet on a Pixel 3 smartphone. In this work, we observe that methods such as grouped convolutions have yielded significant speedups for computer vision networks, but many of these techniques have not been adopted by NLP neural network designers.<br><br>\nSqueeze BERT demonstrates how to replace several operations in self-attention layers with grouped convolutions, and we use this technique in a novel network architecture called SqueezeBERT, which runs 4.3x faster than BERT-base on the Pixel 3 while achieving competitive accuracy on the GLUE test set. The SqueezeBERT code will be released.<\/p>","02f4db2d":"<p p style = \"font-family: garamond; font-size:30px; font-style: normal;background-color: #f6f5f5; color :#6666ff; border-radius: 10px 10px; text-align:center\">Define Train and Test<\/p>","dc0d8874":"<p p style = \"font-family: garamond; font-size:35px; font-style: normal;background-color: #f6f5f5; color :#ff0066; border-radius: 10px 10px; text-align:center\">Upvote the kernel if you find it insightful!<\/p>"}}