{"cell_type":{"bd704983":"code","edf53b9b":"code","a8dfcde1":"code","713df79b":"code","698c2fca":"code","2101b14c":"code","0ebeb7d5":"code","abe970ae":"code","c46fac14":"code","08615210":"code","e60b7a00":"code","c230c3de":"code","5916e290":"code","5e97bd58":"code","c042c361":"code","2ebec8a5":"code","6561fbff":"code","dc202441":"code","1251d605":"code","0df74337":"code","4ac27cd8":"code","ce8683b6":"code","8ba47cb9":"code","8bc54926":"code","5d3b8e09":"code","4565e28b":"code","41a72482":"code","64623c28":"code","80d3d253":"code","afc66a4c":"code","984928be":"code","fc2158ea":"code","a65b9063":"code","8721ff3d":"code","854046b6":"code","8684150a":"code","3faa440d":"code","385c0475":"code","abd3a7ce":"code","65f13b73":"code","404422ab":"code","1734bd03":"code","4d877c6d":"code","c070067a":"code","1153b732":"code","65567427":"code","9cd631ac":"code","b74701b8":"code","df43a690":"code","15c22bde":"code","8bf725e2":"code","b482a965":"code","4297aa48":"code","53138e1d":"code","22547736":"code","6158413b":"code","0362f51f":"code","ea4d0608":"code","f01fffb6":"code","101d113f":"code","b14f11f9":"code","9e867106":"code","45e96997":"code","ddd67c2d":"code","b0fa6bb1":"code","7748fd38":"code","f3f9e63e":"code","f9a1c7ab":"code","ff231045":"code","fc49099f":"code","331e1a5d":"code","28580796":"code","b367ca16":"markdown","2a20bb8b":"markdown","b3dbac53":"markdown","43413aa1":"markdown","eb4b5f74":"markdown","effa8831":"markdown","d7a97183":"markdown","e2aaeb97":"markdown","f4417d83":"markdown","84b45b97":"markdown","5fcb5c21":"markdown","0b8db7fb":"markdown","44bd50b6":"markdown","b8d390d7":"markdown","e4a2cc20":"markdown","0a50aa60":"markdown","1e67103a":"markdown","ad9ae2fa":"markdown","bfe422dc":"markdown","06f43c12":"markdown","a12c21ac":"markdown","7c813ec7":"markdown","a0fa13f0":"markdown","e5ffbf59":"markdown","5e08231b":"markdown","999dd3fd":"markdown","a493fc69":"markdown","f7f5aeec":"markdown","c7fe3f72":"markdown","300ff061":"markdown","9748ab54":"markdown","808ce0f2":"markdown","f20e3793":"markdown"},"source":{"bd704983":"# This Python 3 environment comes with many helpful analytics libraries installed\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom gensim.summarization import keywords\nfrom gensim.summarization.summarizer import summarize\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","edf53b9b":"!pip install PyPDF2","a8dfcde1":"!pip install textract","713df79b":"!pip install autocorrect","698c2fca":"pip install gensim","2101b14c":"import PyPDF2\nimport textract\nimport glob\n\nfrom autocorrect import Speller\nfrom nltk.tokenize import word_tokenize\nimport warnings\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","0ebeb7d5":"def read_allfiles(file):\n    # myFile = PdfFileReader(open(\"..\/input\/accident\/test1.pdf\", 'rb'))\n    open_filename = open(file, 'rb')\n\n    fileAccid = PyPDF2.PdfFileReader(open_filename)\n    total_pages = fileAccid.numPages\n    count = 0\n    text  = ''\n    while(count < total_pages):\n        mani_page  = fileAccid.getPage(count)\n        count += 1\n        text += mani_page.extractText().replace('\\n','')\n    \n    if text != '':\n        text = text  \n    else:\n        textract.process(open_filename, method='tesseract', encoding='utf-8', langauge='eng' )  \n\n    return text\n    text = read_allfiles(file)\n","abe970ae":"path = glob.glob(r'..\/input\/accident\/*.pdf') \n\nprint(path)","c46fac14":"from textblob import TextBlob\ndef get_unique_sentences(text):\n    unique_sentences = []\n    for sentence in [sent.raw for sent in TextBlob(text).sentences]:\n        if sentence not in unique_sentences:\n            unique_sentences.append(sentence)\n    return ' '.join(unique_sentences)","08615210":"#  pour sauver le nom de file sans extension: os.path.splitext(file)[0]\n# name = os.path.basename(file)\n\nappended_data = []\nfor file in path:\n    text = read_allfiles(file)\n    text = summarize(text, ratio=0.4)\n    text = get_unique_sentences(text) \n    df = pd.DataFrame([text])\n    df.columns = ['script']\n    df.index = ['doc']\n    appended_data.append(df)\n    \ndf = pd.concat(appended_data)\n  \nprint(df)","e60b7a00":"df['script'] = df.replace('\\\\n','', regex=True)\ndf[\"script\"] = df[\"script\"].str.lower()\ndf[\"script\"] = df[\"script\"].str.replace('\\d+', '')\ndf","c230c3de":"import nltk\nimport string\nimport re\nfrom nltk.corpus import stopwords, brown\nfrom nltk.tokenize import word_tokenize, sent_tokenize, RegexpTokenizer\nfrom nltk.stem import WordNetLemmatizer\nfrom autocorrect import spell","5916e290":"# Create a list of all the words we wish to remove\nmots = [\"blablabla\"]\n # mots = [\"information\", \"likely\", \"probably\", \"taken\", \"use\", \"following\", \"follows\", \"remove\",\"aircraft\"]\nprint(mots)","5e97bd58":"stop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer()\n\ndef clean_text(text):\n    text = re.sub(r'[^\\w\\s]','',text, re.UNICODE)\n    shortword = re.compile(r'\\W*\\b\\w{1,3}\\b')\n    text = shortword.sub('', text)\n    text = text.lower()\n    text = [lemmatizer.lemmatize(token) for token in text.split(\" \")]\n    text = [lemmatizer.lemmatize(token, \"v\") for token in text]\n    text = [word for word in text if not word in stop_words]\n    text = \" \".join(text)\n    return text\n\ndf['script'] = df.script.apply(lambda x: clean_text(x))\ndf","c042c361":"#  Counting the occurrences of tokens and building a sparse matrix of documents x tokens.\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport numpy as np\n\ncorpus = df.script\nvect = CountVectorizer(stop_words='english')\n\n# Transforms the data into a bag of words\ndata_vect = vect.fit_transform(corpus)","2ebec8a5":"feature_names = vect.get_feature_names()\ndata_vect_feat = pd.DataFrame(data_vect.toarray(), columns=feature_names)\ndata_vect_feat.index = df.index\ndata_vect_feat\n","6561fbff":"data_cat = data_vect_feat.transpose()\ndata_cat","dc202441":"# accident = 1  ncident  = 0\ndata_vect_feat['categorie'] = np.where(data_vect_feat['accident'] != 0, 1, 0)\ndata_vect_feat ","1251d605":"df['categoria'] = data_vect_feat['categorie']\ndf","0df74337":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nTF_IDF_matrix = vectorizer.fit_transform(df['script'])\nTF_IDF_matrix = TF_IDF_matrix.T\n\nprint('Vocabulary Size : ', len(vectorizer.get_feature_names()))\nprint('Shape of Matrix : ', TF_IDF_matrix.shape)","4ac27cd8":"# Global Variables \nK = 2 # number of components","ce8683b6":"import numpy as np\n\n# Applying SVD\nU, s, VT = np.linalg.svd(TF_IDF_matrix.toarray()) # .T is used to take transpose and .toarray() is used to convert sparse matrix to normal matrix\n\nTF_IDF_matrix_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n\n# Getting document and term representation\nterms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\ndocs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix ","8ba47cb9":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.scatter(docs_rep[:,0], docs_rep[:,1], c=df['categoria'])\nplt.title(\"Document Representation\")\nplt.show()","8bc54926":"data_vect_feat.to_excel(\"dataframe.xlsx\", index = False, header=True)","5d3b8e09":"data1 = data_vect_feat.loc[:, data_vect_feat.columns != 'categorie']","4565e28b":"data = data1.transpose()\ndata","41a72482":"import nltk\nfrom nltk import bigrams\nimport itertools\n\ndef generate_co_occurrence_matrix(corpus):\n    vocab = set(corpus)\n    vocab = list(vocab)\n    vocab_index = {word: i for i, word in enumerate(vocab)}\n \n    # Create bigrams from all words in corpus\n    bi_grams = list(bigrams(corpus))\n \n    # Frequency distribution of bigrams ((word1, word2), num_occurrences)\n    bigram_freq = nltk.FreqDist(bi_grams).most_common(len(bi_grams))\n \n    # Initialise co-occurrence matrix\n    # co_occurrence_matrix[current][previous]\n    co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n \n    # Loop through the bigrams taking the current and previous word,\n    # and the number of occurrences of the bigram.\n    for bigram in bigram_freq:\n        current = bigram[0][1]\n        previous = bigram[0][0]\n        count = bigram[1]\n        pos_current = vocab_index[current]\n        pos_previous = vocab_index[previous]\n        co_occurrence_matrix[pos_current][pos_previous] = count\n    co_occurrence_matrix = np.matrix(co_occurrence_matrix)\n \n    # return the matrix and the index\n    return co_occurrence_matrix, vocab_index\n \n \ntext_data = [['Where', 'Python', 'is', 'used'],\n             ['What', 'is', 'Python' 'used', 'in'],\n             ['Why', 'Python', 'is', 'best'],\n             ['What', 'companies', 'use', 'Python']]\n \n# Create one list using many lists\ndata = list(itertools.chain.from_iterable(text_data))\nmatrix, vocab_index = generate_co_occurrence_matrix(data)\n \n \ndata_matrix = pd.DataFrame(matrix, index=vocab_index,\n                             columns=vocab_index)\nprint(data_matrix)","64623c28":"from sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import TfidfVectorizer","80d3d253":"tfidf = TfidfVectorizer(stop_words = 'english', max_features=30)\nx_fitted =tfidf.fit(df.script)\nX_text = x_fitted.transform(df.script)","afc66a4c":"voc_dim = tfidf.vocabulary_","984928be":"len(set(voc_dim))","fc2158ea":"tfidf.vocabulary_","a65b9063":"print(X_text)","8721ff3d":"idf = tfidf.idf_\nprint(dict(zip(x_fitted.get_feature_names(), idf)))","854046b6":"rr = dict(zip(x_fitted.get_feature_names(), idf))","8684150a":"token_weight = pd.DataFrame.from_dict(rr, orient='index').reset_index()\ntoken_weight.columns=('token','weight')\ntoken_weight = token_weight.sort_values(by='weight', ascending=False)\ntoken_weight \n\nsns.barplot(x='token', y='weight', data=token_weight)            \nplt.title(\"Inverse Document Frequency(idf) per token\")\nfig=plt.gcf()\nfig.set_size_inches(20,5)\nplt.show()","3faa440d":"# get feature names\nfeature_names = np.array(tfidf.get_feature_names())\nsorted_by_idf = np.argsort(tfidf.idf_)\nprint(\"Features with lowest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[:20]]))\nprint(\"\\nFeatures with highest idf:\\n{}\".format(\n       feature_names[sorted_by_idf[-20:]]))","385c0475":"def cloud(text, title):\n    # Setting figure parameters\n    mpl.rcParams['figure.figsize']=(10.0,10.0)    #(6.0,4.0)\n    #mpl.rcParams['font.size']=12                #10 \n    mpl.rcParams['savefig.dpi']=100             #72 \n    mpl.rcParams['figure.subplot.bottom']=.1 \n    \n    # Processing Text\n    stopwords = set(STOPWORDS) # Redundant\n    wordcloud = WordCloud(width=1400, height=800,\n                          background_color='black',\n                          max_words = 50\n                         ).generate(\" \".join(text))\n    \n    # Output Visualization\n    plt.figure(figsize=(20,10), facecolor='k')\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.tight_layout(pad=0)\n    plt.title(\"Top most common 50 words from the incident report\",fontsize=24,color='orange')\n    plt.tight_layout(pad=1)\n    plt.savefig('Incident_top.jpeg')\n    #plt.imshow(plt.recolor( colormap= 'Pastel1_r' , random_state=17), alpha=0.98)\n    #fig.savefig(\"wordcloud.png\", dpi=900)","abd3a7ce":"import matplotlib as mpl\nfrom subprocess import check_output\nfrom wordcloud import WordCloud, STOPWORDS\n\nx = 0\nprint(cloud(df[df.categoria == x]['script'].values,x))","65f13b73":"x = 1\nprint(cloud(df[df.categoria == x]['script'].values,x))","404422ab":"def resume_df(text):\n    try:\n        return summarize(text).script\n    except:\n        return None\n\ndf['resume'] = df['script'].apply(resume_df)\n\nprint(df)","1734bd03":"# Find the top 50 words written in the Accident Rapport\ntop_dict = {}\nfor c in data.columns:\n    top = data[c].sort_values(ascending=False)\n    top_dict[c]= list(zip(top.index, top.values))\n\n    \nfor x in list(top_dict)[0:50]:\n    print(\"key {}, value {} \".format(x,  top_dict[x]))","4d877c6d":"# Look at the most common top words --> add them to the stop word list\nfrom collections import Counter\n\n# Let's first pull out the top 100 words for incident\nwords = []\nfor incident in data:\n    top = [word for (word, count) in top_dict[incident]]\n    for t in top:\n        words.append(t)\n\nprint(words[:10])","c070067a":"from wordcloud import WordCloud, STOPWORDS\nimport imageio\n\n# Image used in which our world cloud output will be\nimg1 = imageio.imread(\"..\/input\/incident\/avion.jpeg\")\nhcmask1 = img1\n\n# Get 100 words based on the \nwords_except_stop_dist = nltk.FreqDist(w for w in words[:50]) \nwordcloud = WordCloud(stopwords=set(STOPWORDS),background_color='black',mask=hcmask1).generate(\" \".join(words_except_stop_dist))\nplt.imshow(wordcloud,interpolation = 'bilinear')\nfig=plt.gcf()\nfig.set_size_inches(10,12)\nplt.axis('off')\nplt.title(\"Top most common 50 words from the incident report\",fontsize=24,color='orange')\nplt.tight_layout(pad=0)\nplt.savefig('Incident_top_50.jpeg')","1153b732":"data.shape","65567427":"blob = TextBlob(df['script'])\nblob.sentiment","9cd631ac":"# Gensim\nimport gensim\nimport gensim.corpora as corpora\nfrom gensim.utils import simple_preprocess\nfrom gensim.models import CoherenceModel\n\n# Plotting tools\nimport pyLDAvis\n#import graphlab as gl\n#import pyLDAvis.graphlab\nimport pyLDAvis.gensim  # don't skip this\n\n# Enable logging for gensim - optional\nimport logging\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.ERROR)\n","b74701b8":"data  = []\ndata.append(clean_text(lower_case1))","df43a690":"import spacy\n\n# Second lemmatization of our data\ndef lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n    \"\"\"https:\/\/spacy.io\/api\/annotation\"\"\"\n    texts_output = []\n    for sent in data:\n        doc = nlp(\" \".join(sent)) \n        texts_output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n    return texts_output\n\nnlp = spacy.load('en', disable=['parser', 'ner'])\n\n# Lemmatize keeping only noun, adj, vb, adv\ndata_lemmatized = lemmatization(data, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n","15c22bde":"# Create Dictionary\nid2word = corpora.Dictionary(data_lemmatized)\n\n# Create Corpus\ntexts = data_lemmatized\n\n# Term Document Frequency\ncorpus = [id2word.doc2bow(text) for text in texts]\n\n\n# View\nprint(corpus[:1])","8bf725e2":"from pprint import pprint\n# number of topics\nnum_topics = 10\n# Build LDA model\nlda_model = gensim.models.LdaMulticore(corpus=corpus,\n                                       id2word=id2word,\n                                       num_topics=num_topics)\n# Print the Keyword in the 10 topics\npprint(lda_model.print_topics())\ndoc_lda = lda_model[corpus]","b482a965":"import pyLDAvis.gensim\nimport pickle \nimport pyLDAvis\n\npyLDAvis.enable_notebook()\nvis_topics = pyLDAvis.gensim.prepare(lda_model, corpus, id2word)\n","4297aa48":"vis_topics","53138e1d":"# Print model perplexity\nprint('\\nPerplexity:', lda_model.log_perplexity(corpus))\n\n\n# Coherence Score\n\ncoherence_model_lda = CoherenceModel(lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('Coherence Score:', coherence_lda)","22547736":"!pip install git+https:\/\/github.com\/syasini\/NLOOP.git@master","6158413b":"from nloop import Text","0362f51f":"text = Text(words, fast=False)","ea4d0608":"text.lda.run(num_topics=10)","f01fffb6":"# extract keywords and their ranks (NLOOP uses the pytextrank pipeline from spacy)\nkeywords_list = [x for item in text.keywords.texts for x in item]\nranks_list = [x for item in text.keywords.ranks for x in item]","101d113f":"import networkx as nx\n\n%load_ext autoreload\n%autoreload 2","b14f11f9":"# only keep top keywords that pass the rank_cutoff threshold\nrank_cutoff = 0.1\n\ntop_keywords = []\nfor keywords, ranks in zip(text.keywords.texts , text.keywords.ranks ):\n    top_keywords.append((np.array(keywords)[np.array(ranks)>rank_cutoff]).tolist())","9e867106":"print(top_keywords)","45e96997":"from itertools import combinations, chain\nfrom collections import Counter\n\n# form pairs of keywords from each document\nkeyword_pair = list(chain(\n                    *[list(combinations(kw_list,2)) for kw_list in top_keywords])\n                   )","ddd67c2d":"# count all the unique pairs\npair_counter = Counter(keyword_pair).items()","b0fa6bb1":"print(len(pair_counter))","7748fd38":"def get_pair_graph(pair_counter, weight_times=1, degree_cutoff=50):\n    \"\"\"construct the co-ocurrence graph from the keyword pairs\n    \n    Parameters\n    ----------\n    pair_counter: Counter dictionary\n        consists of keyword pairs as keys and their counts as values\n    weight_times: int or float (scalar)\n        multplicative factor for weights of edges in the graph\n    degree_cutoff: int\n        nodes with degrees below this number will be ignored\n        \n    Return\n    ------\n    G: networkx graph instance\n    \n    \"\"\"\n    \n    G = nx.Graph()\n\n    #construct the graph from the edges\n    for pair, weight in pair_counter: \n        \n        G.add_edge(*pair, weight=weight_times*(weight))\n    \n    # remove nodes with degrees smaller than the cutoff\n    node_list = []\n    for node in np.copy(G.nodes):\n        if G.degree(node)<degree_cutoff:\n            \n            G.remove_node(node) \n    \n    return G","f3f9e63e":"# get the keyword pair graph\nG = get_pair_graph(pair_counter, degree_cutoff=50)","f9a1c7ab":"# calculate the node sizes using arbitrary transformation \nnode_sizes= [20*G.degree[node]**2+100 for node in G.nodes]\n\n# construct the label dictionary\nlabels = {i:i for i in list(G.nodes)}","ff231045":"print(len(G.nodes))","fc49099f":"# draw the graph\nplt.figure(figsize=(10,10),dpi=100)\n\npos = nx.spring_layout(G, k=3, \n                       fixed=[\"viruses\"], pos={\"viruses\":(0,0)}, \n                       dim=2, iterations=50)\n\n\nnx.draw_networkx_nodes(G, pos, \n                       #with_labels=True, \n                       node_color=\"tab:orange\",\n                       node_size=node_sizes, \n                       node_shape=\"8\", \n                       edgecolors=\"tab:red\",\n                      )\n\nnx.draw_networkx_edges(G, pos, \n                       #with_labels=True, \n                       edgecolors=\"grey\",\n                       alpha=0.1,\n                      )\n\n_= nx.draw_networkx_labels(G, pos, \n                        labels=labels, \n                        )","331e1a5d":"from pandas import DataFrame\ndf = pd.DataFrame(data)","28580796":"coo_dict=build_co_occurrence_matrix(df,window_size=1)\n\ncoo_dict","b367ca16":"**TextBlob** - retourne deux propri\u00e9t\u00e9s\n\n*Polarit\u00e9* : une valeur flottante qui varie de [-1.0 \u00e0 1.0] o\u00f9 0 indique neutre, +1 indique l\u2019\u00e9nonc\u00e9 le plus positif et -1 rindicate l\u2019\u00e9nonc\u00e9 le plus n\u00e9gatif.\n\n*Subjectivit\u00e9* : valeur flottante qui varie de [0,0 \u00e0 1,0] o\u00f9 0,0 est le plus objectif alors que 1,0 est le plus subjectif. La phrase subjective exprime des opinions personnelles, des opinions, des croyances, des \u00e9motions, des all\u00e9gations, des d\u00e9sirs, des croyances, des soup\u00e7ons et des sp\u00e9culations o\u00f9, comme objectif, fait r\u00e9f\u00e9rence \u00e0 l\u2019information factuelle.","2a20bb8b":"\n# **# Text Mining pour analyser les donn\u00e9es d'Accidents de l'Aviation**\n","b3dbac53":"On a appliqu\u00e9 la matrice document-terme qui est une matrice math\u00e9matique qui d\u00e9crit la fr\u00e9quence des mots.","43413aa1":"Comment donner un sens \u00e0 tout \u00e7a ?\n\ncela signifie que les 10 premiers mots cl\u00e9s qui font partie de ce sujet sont : mask, lens, smoke, oxygen, crew, emergency, surface, internal, pressure et commander. \nLes chiffres avant les mots repr\u00e9sentent le poids du mot sp\u00e9cifique sur ce sujet, par exemple mask dans le sujet 0 p\u00e8sent 0.003 et c\u2019est beaucoup pour tous les mots les 10 premiers mots dans le sujet 0.","eb4b5f74":"TfidfVectorizer : \nLe but de tf-idf est de r\u00e9duire l\u2019impact des jetons qui se produisent tr\u00e8s fr\u00e9quemment dans un corpus donn\u00e9 et qui sont donc empiriquement moins informatifs que les caract\u00e9ristiques qui se produisent dans une petite fraction du corpus de formation","effa8831":"La mod\u00e9lisation th\u00e9matique peut \u00eatre consid\u00e9r\u00e9e comme un type de mod\u00e9lisation statistique permettant de d\u00e9couvrir les 'sujets' abstraits qui sont pr\u00e9sent\u00e9s dans une myriade de documents (il peut s\u2019agir d\u2019un seul document). Un sujet est consid\u00e9r\u00e9 comme un ensemble de mots cl\u00e9s courants qui sont des repr\u00e9sentants typiques. Son par des mots cl\u00e9s dans lesquels on d\u00e9termine ce que le sujet est tout au sujet.","d7a97183":"Il s\u2019agit d\u2019un ensemble de techniques de traitement du langage naturel (PNL) qui consiste \u00e0 analyser, \u00e0 identifier et \u00e0 cat\u00e9goriser les opinions exprim\u00e9es dans un texte afin de d\u00e9terminer si l\u2019attitude de l\u2019auteur \u00e0 l\u2019\u00e9gard d\u2019un sujet, d\u2019un produit, de la politique, des services, des marques, etc. est positive. n\u00e9gatif, ou neutre.","e2aaeb97":"# ****9 - Mod\u00e9lisation des sujets \u00e0 l\u2019aide de LDA","f4417d83":"formule: tf-idf(d, t) = tf(t) * idf(d, t)\n\n* tf(t)= la fr\u00e9quence du terme est le nombre de fois que le terme appara\u00eet dans le document\n* idf(d, t) = la fr\u00e9quence du document est le nombre de documents' qui contiennent le terme 't\u2019","84b45b97":"Repr\u00e9sentation graphique de la fr\u00e9quence inverse des documents","5fcb5c21":"On va utiliser la fonction PdfFileReader() de PyPDF2 qui est un paquet pour extraire des informations de document telles que titre, auteur, nombre de pages,.... , diviser les documents page par page, fusionner page par page, etc..","0b8db7fb":"On va diviser nos phrases de texte en mots individuels en utilisant la fonction word_tokenize() de la bo\u00eete \u00e0 outils Natural Language Toolkit (nltk).","44bd50b6":"# **8 - R\u00e9sum\u00e9 de texte**\n\nIl y a deux Types de r\u00e9sum\u00e9 de texte Extractif et Abstractif.\nDans la synth\u00e8se extractive, la machine va extraire un sous-ensemble de phrases qui d\u00e9crivent le texte.\nC'est une approche plus mature pour r\u00e9sumer, cette approche a \u00e9t\u00e9 introduite pour la premi\u00e8re fois en 1958\n\nParmi les diff\u00e9rentes m\u00e9thodes de synthese on a choisi d'utiliser Gensim:\nLe r\u00e9sum\u00e9 de texte automatique de Gensim utilise TextRank, un algorithme non supervis\u00e9 bas\u00e9 sur des graphiques pond\u00e9r\u00e9s. Il repose sur l'algorithme populaire PageRank que Google a utilis\u00e9 pour classer les pages Web.","b8d390d7":"*Indice de perplexit\u00e9 et de coh\u00e9rence du mod\u00e8le:*\n\n\u00c9valuons notre mod\u00e8le en calculant la perplexit\u00e9 et la coh\u00e9rence du sujet. \nLa perplexit\u00e9 est une mesure d\u2019\u00e9valuation de la probabilit\u00e9 (probabilit\u00e9 pr\u00e9dictive) que de nouvelles donn\u00e9es invisibles re\u00e7oivent le mod\u00e8le qui a \u00e9t\u00e9 appris plus t\u00f4t. \nLa coh\u00e9rence des sujets mesure une note sur un seul sujet en mesurant le degr\u00e9 de similitude s\u00e9mantique de tous les mots ayant obtenu une note \u00e9lev\u00e9e dans un sujet. La perplexit\u00e9 du mod\u00e8le et la note du sujet fournissent une mesure commode pour juger de la qualit\u00e9 d\u2019un mod\u00e8le de sujet donn\u00e9.","e4a2cc20":"Les BoW sont des m\u00e9thodes utilis\u00e9es pour extraire les features d\u2019un texte. Un BoW est une repr\u00e9sentation de texte qui d\u00e9crit l\u2019occurrence de mots dans un texte, en mesurant la pr\u00e9sence de mots connus dans un corpus. Elle ne prend pas en compte l\u2019ordre ni la structure des mots dans le document.\n\nOn va utiliser countVectorizer pour calculer l'occurrence des mots de notre vocabulaire.","0a50aa60":"(1) On peut s\u00e9lectionner manuellement chaque sujet pour afficher ses termes les plus fr\u00e9quents et\/ou \u00ab pertinents \u00bb, en utilisant diff\u00e9rentes valeurs du param\u00e8tre \u03bb. Cela peut nous aider lorsque on essaye d\u2019assigner un nom interpr\u00e9table ou un \u00ab sens \u00bb humain \u00e0 chaque sujet.\n(2) l\u2019exploration du graphique de distance interth\u00e9matique peut nous aider \u00e0 apprendre comment les sujets se rapportent les uns aux autres, y compris la structure potentielle de niveau sup\u00e9rieur entre les groupes de sujets.","1e67103a":"# ****7 - Analyse du sentiment du rapport","ad9ae2fa":"# ****5 - Pr\u00e9process avec Bag of Words model","bfe422dc":"Nous pouvons voir que la polarit\u00e9 est de 0,105, ce qui signifie que le document est neutre et que 0,37 subjectivit\u00e9 renvoie \u00e0 des renseignements presque factuels plut\u00f4t qu\u2019\u00e0 des opinions publiques.","06f43c12":"# **2 - la lecture du document**","a12c21ac":"Cette mesure statistique permet d'\u00e9valuer l'importance d'un terme contenu dans un document, relativement \u00e0 une collection ou un corpus.\nOn peut enlever les mots rares ou les mots tr\u00e8s fr\u00e9quents. td-idf est une technique qui vient des moteurs de recherche. Elle construit le m\u00eame type de matrice mais associe \u00e0 chaque couple (document - mot) un poids qui d\u00e9pend de la fr\u00e9quence d\u2019un mot globalement et du nombre de documents contenant ce mot.","7c813ec7":"Nous essaierons d\u2019obtenir les 100 mots les plus courants de notre document et de les inclure dans un nuage de mots(wordcloud).","a0fa13f0":"Max_df : \nLors de la construction du vocabulaire, il ignore les termes qui ont une fr\u00e9quence de document strictement sup\u00e9rieure au seuil donn\u00e9. Cela pourrait \u00eatre utilis\u00e9 pour exclure les termes qui sont trop fr\u00e9quents et qui sont peu susceptibles d\u2019aider \u00e0 pr\u00e9dire l\u2019\u00e9tiquette","e5ffbf59":"Conversion en minuscules","5e08231b":"Analyse des r\u00e9sultats du mod\u00e8le LDA\nMaintenant que nous avons un mod\u00e8le, visualisons les sujets pour l\u2019interpr\u00e9tabilit\u00e9. Pour ce faire, nous utiliserons un package de visualisation populaire, pyLDAvis, con\u00e7u pour aider interactivement a:\nMieux comprendre et interpr\u00e9ter les sujets individuels;\nMieux comprendre les relations entre les sujets.","999dd3fd":"# ****TF-IDF","a493fc69":"# 1 - Import des libraries","f7f5aeec":"Sauvegardons nos donn\u00e9es dans une data frame pour que nous puissions faire notre analyse","c7fe3f72":"Gensim cr\u00e9e un identifiant unique pour chaque mot du document. Le corpus produit ci-dessus est une cartographie de l\u2019id de mot et de la fr\u00e9quence des mots dans le document, c.-\u00e0-d. [0,1] word_id 0 signifie les appers de l\u2019id de mot en premier dans le document et la fr\u00e9quence des mots qu\u2019il appara\u00eet une fois dans le document. ","300ff061":"# ****6 - Obtenir les 50 mots les plus fr\u00e9quents","9748ab54":"*Construire le mod\u00e8le th\u00e9matique avec LDA*\n\nEn dehors de cela, alpha et b\u00eata sont des hyperparam\u00e8tres qui affectent sparsity des sujets. Les deux avec des valeurs par d\u00e9faut \u00e0 1.0\/num_topics pr\u00e9c\u00e9dent.\n\nalpha est la distribution th\u00e9matique par document, en termes simples c\u2019est une matrice o\u00f9 chaque ligne est un document et chaque colonne est un sujet. beta est la distribution de mots par sujet, \u00e9galement en termes simples c\u2019est une matrice o\u00f9 chaque ligne repr\u00e9sente un sujet et chaque colonne repr\u00e9sente un mot.","808ce0f2":"# **4 - Nettoyage du texte et transformation en liste de mots cl\u00e9s**","f20e3793":"La lemmatisation est le processus de regroupement des diff\u00e9rentes formes inflect\u00e9es d\u2019un mot afin qu\u2019elles puissent \u00eatre analys\u00e9es comme un seul \u00e9l\u00e9ment."}}