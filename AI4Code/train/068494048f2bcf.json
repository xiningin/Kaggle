{"cell_type":{"66ecb5ab":"code","deb24b01":"code","1a0017e5":"code","6cacf87a":"code","cc707658":"code","864ff023":"code","4d3be9e5":"code","6e3d5aad":"code","ae0011c2":"code","23bf7581":"code","594b4c5a":"code","8d57fd49":"code","26529d65":"code","48be8ddb":"code","f402dfeb":"code","97988955":"code","30faed85":"code","bc8f0940":"code","5b06329f":"code","dc7f516f":"code","39f9088c":"code","e9e50ca2":"code","28ffb1df":"code","c7a35a30":"code","2bf2258d":"code","2baa4e04":"code","f6679906":"code","baf0f4dd":"code","abe83a7f":"code","adad8f05":"code","ae27e18d":"code","731b6595":"code","73c91f1f":"code","b52dea45":"code","fa158177":"markdown","5f9bd203":"markdown","dbca6078":"markdown","f19afbe0":"markdown","990571a0":"markdown","75af4f18":"markdown","39bb43ad":"markdown","6aec1c7c":"markdown","be1e0b5a":"markdown"},"source":{"66ecb5ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","deb24b01":"import tensorflow  as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Device:', tpu.master())\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nexcept:\n    strategy = tf.distribute.get_strategy()\nprint('Number of replicas:', strategy.num_replicas_in_sync)","1a0017e5":"# General Purpose Packages\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nfrom tqdm import tqdm\n\n# Data Processing\nimport re\nimport emoji\nimport string\nimport nltk\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Transformers\nfrom transformers import BertTokenizerFast\nfrom transformers import TFBertModel","6cacf87a":"train_data = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin-1')\ntest_data = pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin-1')","cc707658":"train_data = train_data.ffill(axis = 0)\nprint(train_data.shape,test_data.shape)","864ff023":"def change_sen(sentiment):\n    if sentiment == \"Extremely Positive\":\n        return 2\n    elif sentiment == \"Extremely Negative\":\n        return 0\n    elif sentiment == \"Negative\":\n        return 0\n    elif sentiment == \"Positive\":\n        return 2\n    else:\n        return 1\n    \ntrain_data['Sentiment'] = train_data['Sentiment'].apply(lambda x : change_sen(x))\ntest_data['Sentiment'] = test_data['Sentiment'].apply(lambda x : change_sen(x))\n\ncount_dict = train_data['Sentiment'].value_counts().to_dict()\nLabels = list(count_dict.keys())\nvalues = list(count_dict.values())","4d3be9e5":"df_plot = train_data.groupby(by = 'TweetAt').count()\npx.bar(x = df_plot.index,y =df_plot['Sentiment'],color = df_plot['Sentiment'])","6e3d5aad":"px.pie(values = values,names = Labels,hole=.5,title = \"Label Counts In Percentage\")","ae0011c2":"df_plot = train_data.groupby(by = \"Sentiment\").count()\npx.bar(df_plot,x = df_plot.index,y = df_plot['OriginalTweet'],color = [\"Negative\",\"Neural\",\"Positive\"],title = 'Count of Classes')","23bf7581":"# Substituting Emoji from text\ndef remove_emoji(text):\n    return re.sub(emoji.get_emoji_regexp(),r\"\",text)\n\n# non_nece = Removing punctuation,links,mentions and \\r\\n new line characters\ndef remove_non_nece(text):\n    text = text.replace(\"\\r\",\"\").replace(\"\\n\",\"\").lower()\n    text = re.sub(r\"(?:\\@|https?\\:\/\/)\\S+\",\"\",text)\n    text = re.sub(r\"[^\\x00-\\x7f]\",r\"\",text)\n    banned_list = string.punctuation + '\u00c3'+'\u00b1'+'\u00e3'+'\u00bc'+'\u00e2'+'\u00bb'+'\u00a7'\n    table = str.maketrans('', '', banned_list)\n    text = text.translate(table)\n    return text\n\ndef clean_hashtags(tweet):\n    new_tweet = \" \".join(word.strip() for word in re.split('#(?!(?:hashtag)\\b)[\\w-]+(?=(?:\\s+#[\\w-]+)*\\s*$)', tweet)) #remove last hashtags\n    new_tweet2 = \" \".join(word.strip() for word in re.split('#|_', new_tweet)) #remove hashtags symbol from words in the middle of the sentence\n    return new_tweet2\n\n#Filter special characters such as & and $ present in some words\ndef filter_chars(a):\n    sent = []\n    for word in a.split(' '):\n        if ('$' in word) | ('&' in word):\n            sent.append('')\n        else:\n            sent.append(word)\n    return ' '.join(sent)\n\ndef remove_mult_spaces(text): # remove multiple spaces\n    return re.sub(\"\\s\\s+\" , \" \", text)\n","594b4c5a":"# Cleaning Train data\ntexts_new = []\nfor t in tqdm(train_data.OriginalTweet):\n    texts_new.append(remove_mult_spaces(filter_chars(clean_hashtags(remove_non_nece(remove_emoji(t))))))\n\n# Clearning Test Data\ntexts_new_test = []\nfor t in tqdm(test_data.OriginalTweet):\n    texts_new_test.append(remove_mult_spaces(filter_chars(clean_hashtags(remove_non_nece(remove_emoji(t))))))\ntrain_data['clean_text'] = texts_new\ntest_data['clean_text'] = texts_new_test","8d57fd49":"# Text len for train data\ntext_len_train = list()\nfor text in tqdm(train_data['clean_text']):\n    text_len_train.append(len(text.split()))\ntrain_data['text_len'] = text_len_train    \n\n# Text len for test data\ntext_len_test = list()\nfor text in tqdm(test_data['clean_text']):\n    text_len_test.append(len(text.split()))\ntest_data['text_len'] = text_len_test ","26529d65":"train_df = train_data[train_data['text_len'] > 4]\ntest_df  = test_data[test_data['text_len'] >4 ]\nprint(\"Train Shape : \",train_df.shape)\nprint(\"Test Shape : \",test_df.shape)","48be8ddb":"tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')","f402dfeb":"token_len = list()\nfor txt in tqdm(train_data['clean_text'].values):\n    tokens = tokenizer.encode(txt,max_length = 512,truncation = True)\n    token_len.append(len(tokens))\ntrain_data['token_len'] = token_len\nmax_token_len = np.max(token_len)\nprint(f\"Maximun Token Sentence Length : {max_token_len}\")","97988955":"for idx,txt in enumerate(train_data['clean_text'].values):\n    tokens = tokenizer.encode(txt,max_length = 512,truncation = True)\n    if len(tokens) > 65:\n        print(f\"Index : {idx}, Text : {txt}\")\n        print(\"--\"*50)\n        break\n        \nprint(\"The Text is Not In English Language, So We have to Drop Those Rows\")","30faed85":"train_data = train_data.sort_values(by = 'token_len',ascending = False).iloc[13:,:]\ntrain_data = train_data.sample(frac = 1).reset_index(drop = True)","bc8f0940":"test_token_len = list()\nfor idx,txt in enumerate(tqdm(test_data['clean_text'].values)):\n    token = tokenizer.encode(txt,max_length = 512,truncation = True)\n    test_token_len.append(len(token))\ntest_data['token_len'] = test_token_len","5b06329f":"# Check whether Other language are present in text\nfor idx,txt in tqdm(enumerate(test_data['clean_text'].values)):\n    token = tokenizer.encode(txt,max_length = 512,truncation = True)\n    if len(token) > 80:\n        print(f\"Index : {idx}, Text : {txt}\")","dc7f516f":"test_data = test_data.sort_values(by = 'token_len',ascending = False).iloc[5:,:]\ntest_data = test_data.sample(frac = 1).reset_index(drop = True)","39f9088c":"X = train_data['clean_text']\ny = train_data['Sentiment']\n\nX_train,X_valid,y_train,y_valid = train_test_split(X,y,test_size = 0.2,stratify = y,random_state = 42)\n\nX_test = test_data['clean_text']\ny_test = test_data['Sentiment']","e9e50ca2":"one_enc = OneHotEncoder()\ny_train = one_enc.fit_transform(np.array(y_train).reshape(-1,1)).toarray()\ny_valid = one_enc.fit_transform(np.array(y_valid).reshape(-1,1)).toarray()\ny_test = one_enc.fit_transform(np.array(y_test).reshape(-1,1)).toarray()","28ffb1df":"print(f\"TRAINING DATA : {X_train.shape[0]}\\nVALIDATION DATA : {X_valid.shape[0]}\\nTESTING DATA : {X_test.shape[0]}\")","c7a35a30":"MAX_LEN = 128","2bf2258d":"def tokenize(data,max_len = MAX_LEN):\n    input_ids = list()\n    attention_mask = list()\n    for i in tqdm(range(len(data))):\n        encoded = tokenizer.encode_plus(data[i],\n                                        add_special_tokens = True,\n                                        max_length = MAX_LEN,\n                                        padding = 'max_length',\n                                        return_attention_mask = True\n                                       )\n        input_ids.append(encoded['input_ids'])\n        attention_mask.append(encoded['attention_mask'])\n    return np.array(input_ids),np.array(attention_mask)","2baa4e04":"train_input_ids, train_attention_masks = tokenize(X_train.values, MAX_LEN)\nval_input_ids, val_attention_masks = tokenize(X_valid.values, MAX_LEN)\ntest_input_ids, test_attention_masks = tokenize(X_test.values, MAX_LEN)","f6679906":"bert_model = TFBertModel.from_pretrained('bert-base-uncased')","baf0f4dd":"def create_model(bert_model,max_len = MAX_LEN):\n    #params\n    opt = tf.keras.optimizers.Adam(learning_rate=0.0001)\n    loss = tf.keras.losses.CategoricalCrossentropy()\n    accuracy = tf.keras.metrics.CategoricalAccuracy()\n    \n    input_ids = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n    attention_masks = tf.keras.Input(shape = (max_len,),dtype = 'int32')\n    \n    embeddings = bert_model([input_ids,attention_masks])[1]\n    \n    x = tf.keras.layers.Dense(16,activation = 'relu',kernel_regularizer = 'l2')(embeddings)\n    x = tf.keras.layers.Dropout(0.5)(x)\n    output = tf.keras.layers.Dense(3,activation = 'softmax')(x)\n    \n    \n    model = tf.keras.models.Model(inputs = [input_ids,attention_masks],outputs = output)\n    \n    model.compile(opt,loss=loss,metrics = accuracy)\n    return model\n    \nmodel = create_model(bert_model,MAX_LEN)\nmodel.summary()","abe83a7f":"tf.keras.utils.plot_model(model)","adad8f05":"with strategy.scope():\n    bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n    model = create_model(bert_model,MAX_LEN)   ","ae27e18d":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(mode='min',patience=5)\nhistory_bert = model.fit([train_input_ids,train_attention_masks],y_train,validation_data = ([val_input_ids,val_attention_masks],y_valid),epochs = 25,batch_size = 30*4,callbacks = early_stopping)","731b6595":"history_bert.history.keys()","73c91f1f":"plt.plot(history_bert.history['categorical_accuracy'])\nplt.plot(history_bert.history['val_categorical_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","b52dea45":"plt.plot(history_bert.history['loss'])\nplt.plot(history_bert.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","fa158177":"# **Trim Train and Test Data**","5f9bd203":"# **Look At the Data**","dbca6078":"# **BERT Modelling**","f19afbe0":"# **BERT Tokenizer**","990571a0":"# One Hot Encoding ","75af4f18":"Now The data clearning is completed. You can clean data furture, If you have any ideas. ","39bb43ad":"# **Padding**","6aec1c7c":"# **Cleaning Train & Test Data**","be1e0b5a":"# Train - Validation - Test Split"}}