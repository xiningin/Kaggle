{"cell_type":{"cb0924b7":"code","a6c13d7a":"code","5a863db7":"code","658300cd":"code","dcb39c2e":"code","d2131bf1":"code","3290ebca":"code","05dac2f6":"code","fdd082e8":"code","6306adb6":"code","a5ceee35":"code","31b42996":"code","5b631ae9":"code","30d13a37":"code","24c95d99":"code","e93eac03":"code","5f85ebcf":"code","7a289f21":"code","5b45b4ca":"code","9fc5486f":"code","04b7056b":"code","d025868d":"code","2cd2c41e":"code","ddad4f47":"code","e0c4a280":"code","5059d498":"code","d41269d5":"code","39021a60":"code","58e7dea8":"code","7940ea3e":"code","a330de4c":"code","5e4db70e":"code","18334850":"markdown","1fe69e7e":"markdown","23e940cc":"markdown","68006351":"markdown","3b26c590":"markdown","15599e11":"markdown","adb55d51":"markdown","fe924a8f":"markdown","98b7df6e":"markdown","6bfe10ba":"markdown","57a41a4b":"markdown","f96b09c1":"markdown","6ac7f33f":"markdown","f4d59502":"markdown","b9fb29a0":"markdown","848b0dcc":"markdown","6626325b":"markdown","f0188c89":"markdown","f4b9665c":"markdown","fc29b193":"markdown","3d45597f":"markdown","38db652a":"markdown","9a9dee83":"markdown","0ee1507c":"markdown","23285d5f":"markdown","2739b63d":"markdown","a512b93a":"markdown","563b1387":"markdown","73df9366":"markdown","f10355a5":"markdown","9abb155b":"markdown","0a6b09d4":"markdown","2605b7cd":"markdown"},"source":{"cb0924b7":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n#import train and test data.\ntrain=pd.read_csv('..\/input\/titanic\/train.csv')\ntest=pd.read_csv('..\/input\/titanic\/test.csv')\nname=train.Name\ntrain.head()","a6c13d7a":"## Lets againgtake a quick glance of what we are dealing with.\ntrain.head(5)","5a863db7":"train.shape# Means 891 rows and 12 columns.","658300cd":"train.isnull().sum()","dcb39c2e":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import Imputer\nImp=Imputer(missing_values='NaN',strategy='median',axis=1)\nnew=Imp.fit_transform(train.Age.values.reshape(1,-1))\ntrain['Age2']=new.T\n#Lets drop the old one age Column.\n","d2131bf1":"train.drop('Age',axis=1,inplace=True)\n","3290ebca":"train.isnull().sum()","05dac2f6":"train.set_index('PassengerId',inplace=True)\n## get dummy variables for Column sex and embarked since they are categorical value.\ntrain = pd.get_dummies(train, columns=[\"Sex\"], drop_first=True)\ntrain = pd.get_dummies(train, columns=[\"Embarked\"],drop_first=True)\n\n\n#Mapping the data.\ntrain['Fare'] = train['Fare'].astype(int)\ntrain.loc[train.Fare<=7.91,'Fare']=0\ntrain.loc[(train.Fare>7.91) &(train.Fare<=14.454),'Fare']=1\ntrain.loc[(train.Fare>14.454)&(train.Fare<=31),'Fare']=2\ntrain.loc[(train.Fare>31),'Fare']=3\n\ntrain['Age2']=train['Age2'].astype(int)\ntrain.loc[ train['Age2'] <= 16, 'Age2']= 0\ntrain.loc[(train['Age2'] > 16) & (train['Age2'] <= 32), 'Age2'] = 1\ntrain.loc[(train['Age2'] > 32) & (train['Age2'] <= 48), 'Age2'] = 2\ntrain.loc[(train['Age2'] > 48) & (train['Age2'] <= 64), 'Age2'] = 3\ntrain.loc[train['Age2'] > 64, 'Age2'] = 4","fdd082e8":"# In our data the Ticket and Cabin,Name are the base less,leds to the false prediction so Drop both of them.\ntrain.drop(['Ticket','Cabin','Name'],axis=1,inplace=True)\ntrain.head()\nprint(type(train.Age2))","6306adb6":"train.shape\n# 891 rows and 9 columns.","a5ceee35":"train.Survived.value_counts()\/len(train)*100\n#This signifies almost 61% people in the ship died and 38% survived.","31b42996":"train.describe()","5b631ae9":"train.groupby('Survived').mean()","30d13a37":"train.groupby('Sex_male').mean()","24c95d99":"train.corr()","e93eac03":"plt.subplots(figsize = (15,8))\nsns.heatmap(train.corr(), annot=True,cmap=\"PiYG\")\nplt.title(\"Correlations Among Features\", fontsize = 20);","5f85ebcf":"plt.subplots(figsize = (15,8))\nsns.barplot(x = \"Sex_male\", y = \"Survived\", data=train, edgecolor=(0,0,0), linewidth=2)\nplt.title(\"Survived\/Non-Survived Passenger Gender Distribution\", fontsize = 25)\nlabels = ['Female', 'Male']\nplt.ylabel(\"% of passenger survived\", fontsize = 15)\nplt.xlabel(\"Gender\",fontsize = 15)\nplt.xticks(sorted(train.Sex_male.unique()), labels)\n\n# 1 is for male and 0 is for female.","7a289f21":"sns.set(style='darkgrid')\nplt.subplots(figsize = (15,8))\nax=sns.countplot(x='Sex_male',data=train,hue='Survived',edgecolor=(0,0,0),linewidth=2)\ntrain.shape\n## Fixing title, xlabel and ylabel\nplt.title('Passenger distribution of survived vs not-survived',fontsize=25)\nplt.xlabel('Gender',fontsize=15)\nplt.ylabel(\"# of Passenger Survived\", fontsize = 15)\nlabels = ['Female', 'Male']\n#Fixing xticks.\nplt.xticks(sorted(train.Survived.unique()),labels)\n## Fixing legends\nleg = ax.get_legend()\nleg.set_title('Survived')\nlegs=leg.texts\nlegs[0].set_text('No')\nlegs[1].set_text('Yes')\n","5b45b4ca":"train.head(4)","9fc5486f":"plt.subplots(figsize = (10,10))\nax=sns.countplot(x='Pclass',hue='Survived',data=train)\nplt.title(\"Passenger Class Distribution - Survived vs Non-Survived\", fontsize = 25)\nleg=ax.get_legend()\nleg.set_title('Survival')\nlegs=leg.texts\n\nlegs[0].set_text('No')\nlegs[1].set_text(\"yes\")","04b7056b":"plt.subplots(figsize=(10,8))\nsns.kdeplot(train.loc[(train['Survived'] == 0),'Pclass'],shade=True,color='r',label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Pclass'],shade=True,color='b',label='Survived' )\n\nlabels = ['First', 'Second', 'Third']\nplt.xticks(sorted(train.Pclass.unique()),labels)","d025868d":"plt.subplots(figsize=(15,10))\n\nax=sns.kdeplot(train.loc[(train['Survived'] == 0),'Fare'],color='r',shade=True,label='Not Survived')\nax=sns.kdeplot(train.loc[(train['Survived'] == 1),'Fare'],color='b',shade=True,label='Survived' )\nplt.title('Fare Distribution Survived vs Non Survived',fontsize=25)\nplt.ylabel('Frequency of Passenger Survived',fontsize=20)\nplt.xlabel('Fare',fontsize=20)","2cd2c41e":"#fig,axs=plt.subplots(nrows=2)\nfig,axs=plt.subplots(figsize=(10,8))\nsns.set_style(style='darkgrid')\nsns.kdeplot(train.loc[(train['Survived']==0),'Age2'],color='r',shade=True,label='Not Survived')\nsns.kdeplot(train.loc[(train['Survived']==1),'Age2'],color='b',shade=True,label='Survived')\n","ddad4f47":"X=train.drop('Survived',axis=1)\ny=train['Survived'].astype(int)\n","e0c4a280":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis \nfrom xgboost import XGBClassifier\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    XGBClassifier(),\n    RandomForestClassifier(n_estimators=100, max_features=3),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n    \n\n\nlog_cols = [\"Classifier\", \"Accuracy\"]\nlog= pd.DataFrame(columns=log_cols)\n","5059d498":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\n\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.3, random_state=0)\nacc_dict = {}\n\nfor train_index, test_index in sss.split(X, y):\n    \n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n    \n    for clf in classifiers:\n        name = clf.__class__.__name__\n    \n        clf.fit(X_train,y_train)\n        predict=clf.predict(X_test)\n        acc=accuracy_score(y_test,predict)\n        if name in acc_dict:\n            acc_dict[name]+=acc\n        else:\n            acc_dict[name]=acc","d41269d5":"log['Classifier']=acc_dict.keys()\nlog['Accuracy']=acc_dict.values()\nlog.set_index([[0,1,2,3,4,5,6,7,8,9,10]])\n%matplotlib inline\nsns.set_color_codes(\"muted\")\nax=plt.subplots(figsize=(10,8))\nax=sns.barplot(y='Classifier',x='Accuracy',data=log,color='b')\nax.set_xlabel('Accuracy',fontsize=20)\nplt.ylabel('Classifier',fontsize=20)\nplt.grid(color='r', linestyle='-', linewidth=0.5)\nplt.title('Classifier Accuracy',fontsize=20)\n","39021a60":"train.head()","58e7dea8":"classifier=SVC()\nxtrain=train.iloc[:,1:]\nytrain=train.iloc[:,:1]\nytrain=ytrain.values.ravel()\nclassifier.fit(xtrain,ytrain)\n#we need to convert the test data ","7940ea3e":"testIm=Imputer(missing_values='NaN',strategy='most_frequent',axis=1)\nAge1=testIm.fit_transform(test.Age.values.reshape(1,-1))\nFare2=testIm.fit_transform(test.Fare.values.reshape(1,-1))\ntest.drop(['Name','Cabin','Age','Ticket','Fare'],axis=1,inplace=True)\ntest['Age1']=Age1.T\ntest['Fare2']=Fare2.T\ntest.set_index('PassengerId',inplace=True)\n#test[test.Fare.isnull()]#this will tell us which row have null so we can drop that particular row.\n#test.drop([1044],axis=0,inplace=True)#drop the row which NaN .\n#test.isnull().sum()","a330de4c":"## get dummy variables for Column sex and embarked since they are categorical value.\ntest = pd.get_dummies(test, columns=[\"Sex\"], drop_first=True)\ntest = pd.get_dummies(test, columns=[\"Embarked\"],drop_first=True)\n\n\n#Mapping the data.\ntest['Fare2'] = test['Fare2'].astype(int)\ntest.loc[test.Fare2<=7.91,'Fare2']=0\ntest.loc[(test.Fare2>7.91) &(test.Fare2<=14.454),'Fare2']=1\ntest.loc[(test.Fare2>14.454)&(test.Fare2<=31),'Fare2']=2\ntest.loc[(test.Fare2>31),'Fare2']=3\n\ntest['Age1']=test['Age1'].astype(int)\ntest.loc[ test['Age1'] <= 16, 'Age1']= 0\ntest.loc[(test['Age1'] > 16) & (test['Age1'] <= 32), 'Age1'] = 1\ntest.loc[(test['Age1'] > 32) & (test['Age1'] <= 48), 'Age1'] = 2\ntest.loc[(test['Age1'] > 48) & (test['Age1'] <= 64), 'Age1'] = 3\ntest.loc[test['Age1'] > 64, 'Age1'] = 4","5e4db70e":"Result=classifier.predict(test)\nprint(Result)\nprint(len(Result))","18334850":"There is nothing out of the ordinary of about this plot, except the very left part of the distribution. It shows that\n\nchildren and infants were the priority.","1fe69e7e":"From the above barplot, we can clearly see that the accuracy of the SVC classifier is best out of all other classifiers..\n\nLets apply this to our test data.","23e940cc":"**Fare and Survived**","68006351":"**Summary**\n\nFirst class passenger had the upper hand during the tragedy than second and third class passengers. You can probably agree with me more on this, when we look at the distribution of ticket fare and survived column.","3b26c590":"**Negative Correlation Features:**\n- Fare and Pclass: -0.55\n - This relationship can be explained by saying that first class passenger(1) paid more for fare then second class passenger(2), similarly second class passenger paid more than the third class passenger(3). \n- Gender and Survived: -0.54\n - Basically is the info of whether the passenger was male or female.\n- Pclass and Survived: -0.34","15599e11":"I will train the data with the following models:\n- Logistic Regression\n- Gaussian Naive Bayes\n- Support Vector Machines\n- Decision Tree Classifier\n- K-Nearest Neighbors(KNN)\n -  and many other.....","adb55d51":"**Positive Correlation Features:**\n- Fare and Survived: 0.26.\n\nThere is a positive correlation between Fare and Survived rated. This can be explained by saying that, the passenger who paid more money for their ticket were more likely to survive. ","fe924a8f":"The data has been split into two groups:\n- training set (train.csv)\n- test set(test.csv)\n<br>\n\nThe training set includes passengers survival status(also know as the ground truth from the titanic tragedy) which along with other features like gender, class, fare and pclass is used to create machine learning model.\n<br><br>\nThe test set should be used to see how well my model performs on unseen data. The test set does not provide passengers survival status. We are going to use our model to predict passenger survival status.\n<br><br>\n\nLets describe whats the meaning of the features given the both train & test datasets.\n<h4>Variable Definition Key.<\/h4>\n- Survival\n - 0= No\n - 1= Yes\n- pclass (Ticket class)\n - 1=1st\n - 2=2nd\n - 3=3rd\n \n- sex\n<br>\n\n- age\n\n\n- sibsp (# of siblings \/ spouses aboard the Titanic)\n<br>\n- parch (# of parents \/ children aboard the Titanic)\n<br>\n- tickets\n<br>\n- fare\n<br>\n- cabin\n- embarked Port of Embarkation.\n - C = Cherbourg,\n - Q = Queenstown,\n - S = Southampton\n- pclass: A proxy for socio-economic status (SES)\n<br>\n<h4>This is important to remember and will come in handy for later analysis.<\/h4>\n - 1st = Upper\n - 2nd = Middle\n - 3rd = Lower\n\n","98b7df6e":"**Age and Survived**","6bfe10ba":"**Modeling the Data**","57a41a4b":"There are a couple of points that should be noted from the statistical overview. They are..\n- About the survival rate, only 38% passenger survived during that tragedy.\n- About the survival rate for genders, 74% female passengers survived, while only 19% male passengers survived.","f96b09c1":"<h2 >Part 2.Exploratory data analysis<\/h2>.\n\n**Exploratory data analysis (EDA)** is an approach to analyzing data sets to summarize their main characteristics, often with visual methods.\n\n![](http:\/\/media.giphy.com\/media\/m3UHHYejQ4rug\/giphy.gif)","6ac7f33f":"**Correlation Matrix and Heatmap**","f4d59502":"### Importing the data","b9fb29a0":"**We use the Method 2 i.e(sklearn.preprocessing.Imputer)**\n\nJust because it is easy to use....\n","848b0dcc":"This bar plot above shows the distribution of female and male survived. The x_label shows gender and the y_label shows % of passenger survived. This bar plot shows that 74% female passenger survived while only ~19% male passenger survived.","6626325b":"It looks like this dataset is quite organized, however, before using this dataset for analyzing and visualizing we need to deal with ..\n- Different variables\n- Null values","f0188c89":"## Different variables present in the datasets.\n - **There are four type of variables**\n  - **Numerical Features**: Age, Fare, SibSp and Parch\n  - **Categorical Features**: Sex, Embarked, Survived and Pclass\n  - **Alphanumeric Features**: Ticket and Cabin(Contains both alphabets and the numeric value)\n  - **Text Features**: Name\n\n** We really need to tweak these features so we get the desired form of input data**","f4b9665c":"\n\n## Part 1. Cleaning the data.","fc29b193":"This kde plot is pretty self explanatory with all the labels and colors. Something I have noticed that some readers might find questionable is that in, the plot; the third class passengers have survived more than second class passnegers. It is true since there were a lot more third class passengers than first and second.\n\n","3d45597f":"<h2>Result<\/h2>\n\nThe final result is","38db652a":"**Gender and Survived**\n","9a9dee83":"<h1 style='text-align:center'>Titanic.<\/h1>\n<br>\n\n![](http:\/\/media.giphy.com\/media\/1Nk9bIidJVTy0\/giphy.gif)\n\n**Titanic** is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the **Titanic** sank after colliding with an iceberg, killing *1502* out of *2224* passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.<br><br>\n\n**What particularly we need do in this challange ?**\n\nIn this challenge, we need to complete the analysis of what sorts of people were likely to survive. In particular,  we apply the tools of machine learning to predict which passengers survived the tragedy?.\n","0ee1507c":"This count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive.","23285d5f":"<h2>Classifier Comparision<\/h2>\n\nBy Classifier Comparison we choose which model best for the given data.","2739b63d":"<h2>Prediction<\/h2>\n\nLets use the SVC classifier to predict our data.","a512b93a":"**Pclass and Survived**","563b1387":"**Summary**\n- As we suspected, female passengers have survived at a much better rate than male passengers.\n- It seems about right since females and children were the priority.","73df9366":"So it clearly seems that,The survival of the people belong to 3rd class is very least.\nIt looks like ...\n-  63% first class passenger survived titanic tragedy, while\n-  48% second class and\n-  only 24% third class passenger survived.","f10355a5":"**Hope you find it useful.** \n\n**If this notebook helped you in anyway, please do upvote!**\n","9abb155b":"<h3>What does this data set mean.<\/h3>\n____","0a6b09d4":"**This kernal is still under  process for further imporvement.**\n\nI will always incorporate new concepts of data science as I master them. This journey of learning is worth sharing as well as collaborating. Therefore any comments about further improvements would be genuinely appreciated.","2605b7cd":"We see Age  and Cabin have a lot of missing value.So First we need to deal with all these NaN values.\n- As in Cabin column about 1\\3rd of the values are missing.So we get rid of this column. \n<br>\n\n## Why missing values treatment is required?\nMissing data in the training data set can reduce the power \/ fit of a model or can lead to a biased model because we have not analysed the behavior and relationship with other variables correctly. It can lead to wrong prediction or classification.\n- Here the methods to deal with missing values.\n\n### KNN Imputation. \n------\nIn this method of imputation, the missing values of an attribute are imputed using the given number of attributes that are most similar to the attribute whose values are missing.\n\nFor more...\n<br>\n**Method 1**\n- [KNN Imputation](https:\/\/www.analyticsvidhya.com\/blog\/2016\/01\/guide-data-exploration\/)\n- [Blog](https:\/\/towardsdatascience.com\/the-use-of-knn-for-missing-values-cf33d935c637)\n\n**Method 2**\n- [sklearn.preprocessing.Imputer](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.Imputer.html)\n"}}