{"cell_type":{"71488e3f":"code","38b13935":"code","71076e2b":"code","3210ec65":"code","e6d999d3":"code","364f154a":"code","cdbc7fbe":"code","faa11c4a":"code","d1ab9d0e":"code","efa677d5":"code","a25dbf8d":"code","cecc1545":"code","b36f60e5":"code","71be0b44":"code","37ab118c":"code","bf3dc912":"code","96eb4cac":"code","0531f57d":"code","1eaacefe":"code","a5d6355d":"code","58c2bbab":"code","da27619e":"code","84a412c5":"code","2c40776e":"code","003f3d7c":"code","ba46cad3":"code","ea4dad5a":"code","d4c9ef65":"code","139178f5":"code","272335b1":"code","4ee1921d":"code","cedd6177":"code","c3e98b5d":"code","6247bd4d":"code","84ced5c3":"code","8dae24f2":"code","2ddf3453":"code","e4117b18":"code","9597388c":"code","8dedcbfc":"code","18ef4c6e":"code","fcce81e5":"code","10e6af39":"code","7053316b":"code","e911e7d3":"code","998e1f66":"code","24ab23c4":"code","f523844a":"code","72b0c261":"code","39c029a2":"code","8a9bb52f":"code","ac1ba685":"code","71c1a50f":"code","589bf5e2":"code","8c2ee859":"code","5e2daf0a":"code","7b7e57bc":"code","68bd2153":"code","009510bc":"code","b63d0b93":"code","f96bea36":"code","94b854bf":"code","fe66cfeb":"code","ce1a9bdb":"code","75608566":"code","2254dd79":"code","5eff2a66":"code","b6cec478":"code","6126f897":"code","43b5556d":"code","f0307797":"code","cf7dc5c1":"code","655bd734":"code","8c753b61":"code","ad9b712f":"code","180cd572":"code","9cfba305":"code","c4ec01b7":"code","03b239ff":"code","af1fa3c6":"code","fcd059b9":"code","e9dd19bf":"code","732f01e5":"code","6cfb8dff":"code","b064b4d5":"markdown","cc525051":"markdown","5ae90613":"markdown","162e3a46":"markdown","7fd939c9":"markdown","e9e86a02":"markdown","099a5d7a":"markdown","76a6bf00":"markdown","573b3740":"markdown","f12e2643":"markdown","8b45d41e":"markdown","2cd04783":"markdown","9287151d":"markdown","639cf86b":"markdown","1ca5b656":"markdown","417319eb":"markdown","11750bd3":"markdown","557e856a":"markdown","d7eebe22":"markdown","94b7561d":"markdown","c8a67d9a":"markdown","a99727c9":"markdown","5cbbbb1e":"markdown","8400d55f":"markdown","ec2ebfb9":"markdown","1430ee2c":"markdown","c5919871":"markdown","cbcf1c0c":"markdown","ae2b985a":"markdown"},"source":{"71488e3f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","38b13935":"#Import packages\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n!pip install openpyxl\n!pip install xlrd\n%matplotlib inline","71076e2b":"#Display all columns and 10 rows no shrink\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', 10)","3210ec65":"#Import dataset\nanz = pd.read_excel(\"..\/input\/anz-synthesised-transaction-dataset\/ANZ synthesised transaction dataset.xlsx\", engine=\"openpyxl\")","e6d999d3":"anz.head(10)","364f154a":"anz.info()","cdbc7fbe":"anz.describe()","faa11c4a":"anz.isnull().sum()","d1ab9d0e":"###locate NaN value###\n\n#find rows which are all features(columns) are null \nanz[anz.isnull()]\n\n#target on certain column\nanz[anz['merchant_state'].isnull()]","efa677d5":"##### Create consumer long lat #####\n#create new dataframe with split value columns\nlong_lat = anz['long_lat'].str.split(\" \", n=1, expand=True) #expand parameter is set to True, which means it will return a data frame with all separated strings in different columns\n                                                            #n=1 means separated at the first occurrence of \u201ct\u201d and not on the later occurrence since the n parameter was set to 1 (Max 1 separation in a string)\nlong_lat.head()\n#create new column to add to the anz dataframe from new created dataframe\nanz['consumer_longitude'] = long_lat[0] \nanz['consumer_latitude'] = long_lat[1]\nanz.head()\n\n\n\n##### seperate merchant long lat #####\nmerchant_lg_lt = anz['merchant_long_lat'].str.split(\" \", n=1, expand=True)\nmerchant_lg_lt.head()\nanz['merchant_longitude'] = merchant_lg_lt[0] \nanz['merchant_latitude'] = merchant_lg_lt[1]\nanz.head()\n\n#anz['consumer_longitude'].unique()\n#anz['consumer_latitude'].unique()\n#anz['merchant_longitude'].unique()\n#anz['merchant_latitude'].unique()\n\n\n#convert columns to numeric\nanz[['consumer_longitude', 'consumer_latitude', 'merchant_longitude', 'merchant_latitude']] = anz[['consumer_longitude', 'consumer_latitude', 'merchant_longitude', 'merchant_latitude']].apply(pd.to_numeric, errors='coerce')\nanz.info()","a25dbf8d":"#the dataset only contain records for 91 days, one day is missing\n\nprint(len(anz['date'].unique()))\n\n\nmin_date = anz['date'].min()\nmax_date = anz['date'].max()\ndate_range = pd.date_range(start=min_date, end=max_date)\n\n# print(date_range)\n\nfor x in anz['date'].unique():\n    if x not in date_range:\n        print(f'the missing date is {x}')\n    \nprint(f'the missing date is {x}')","cecc1545":"#confirm the one-to-one link of account_id and customer_id\n\n#create new column concat customer id and account id\nanz['cust_id_acc_id_compiled'] = anz['account'] + \"-\" + anz['customer_id']\nanz.head()\n#check unique account\nprint(len(anz['cust_id_acc_id_compiled'].unique()))\n#the length return to be 100, which means there are 100 unique combination of cust_id and acc_id\n#if more than one customers have different acc, the combination will exceed 100","b36f60e5":"##check the range of customer location\n#filtering out transactions for those who dont reside in Australia\n\ncondition_1 = ((anz['consumer_longitude'] <113) & (anz['consumer_longitude'] >154))\ncondition_2 = ((anz['consumer_latitude'] <(-44)) & (anz['consumer_latitude'] >(-10)))\n\nanz[condition_1 & condition_2]","71be0b44":"#visualise the overall transaction amount\nanz_purchase_overall = anz['amount']\nplt.hist(anz_purchase_overall, bins=[0,10,20,30,40,50,60,70,80,90,100,110], rwidth= 0.95)\nplt.xlabel('Overall Amount')\nplt.ylabel('Frequency')","37ab118c":"#visualise customers' average monthly transaction volume\n\ncustomer = anz.groupby('customer_id')\n#print group \n#for x, y in customer:\n#    print(x)\n#    print(y)\ncustomer['amount'].count()","bf3dc912":"anz_avg_transac_vol = anz.groupby('customer_id')['amount'].count()","96eb4cac":"plt.hist(anz_avg_transac_vol, bins=20, rwidth=0.95)\nplt.xlabel('Monthly Transaction Volume')\nplt.ylabel('No of Customers')","0531f57d":"#segment the dataset by transaction date and time\n#visualise transaction volume over an average week\nday_order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\nto_date = anz['date'].dt.day_name()\nanz['Weekday'] = to_date\n#anz_transac_vol_weekday = \nww = anz.groupby('Weekday')['amount'].count().reset_index()\nww2= ww.set_index(\"Weekday\").reindex(day_order).reset_index()\nprint(ww2)\nplt.plot(ww2['Weekday'], ww2['amount'])\nplt.xlabel('Weekday')\nplt.ylabel('Transaction Volume')\nplt.title('Weekday Transaction Volume')\n","1eaacefe":"to_date = anz['date'].dt.day_name()\nanz['Weekday'] = to_date\n#ww = anz.groupby('Weekday')['amount'].count()\n\n#ww = anz.groupby('Weekday')['amount'].count()\n#yy = anz.groupby('Weekday')['date'].count()\n#print(yy)\n#print(f'{ww} is ww')\n\nday_order=[\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\n\n#First way of converting groupby series object to dataframe\nanz_date = anz.groupby(['date', 'Weekday'])['amount'].count().reset_index()\n# print(anz_date)\n#print(type(anz_date))\nanz_weekday = anz_date.groupby(['Weekday'])['amount'].count().reset_index()                 #note here\nprint(type(anz_weekday))\nprint(anz_weekday)\n\nanz_amount = anz_date.groupby('Weekday')['amount'].sum().reset_index()                    #note here\n#print(anz_amount)\n\n#success version\nanz_weekday['average_vol'] = anz_amount['amount']\/anz_weekday['amount']\n# print(anz_weekday)\n\n\n#new column adding into the existing dataframe using transform (but by using this method the dataframe size will still maintain the same, just that avg_col column, the results for each row depends on the weekday column, eg. all wednesday will have the same result)\nanz['avg_vol'] = anz.groupby(['date', 'Weekday'])['amount'].transform('count')\n#print(anz)\n#print(anz[['date', 'Weekday', 'avg_vol']])\n#print(type(anz))\n\n\n#perfect solution\nanz_test = anz_date.groupby('Weekday')['amount'].count().reset_index(name='total_day')                 \n#print(anz_test)\n\nanz_test['total_transac'] = anz_date.groupby('Weekday')['amount'].transform('sum')  #note here\nprint(anz_test)\n\n#calculation can be performed with other dataframes also\nanz_test['avg_transac'] = anz_date.groupby('Weekday')['amount'].transform('sum')\/anz_test['total_day']  #note here\n#print(anz_test)\n\n#rearrange\n\n#anz_test['Weekday'] = anz_test['Weekday'].astype('category', categories= day_order, ordered= True)\n\nanz_test = anz_test.set_index('Weekday').reindex(day_order).reset_index()\nprint(type(anz_test))\n\"\"\"\n#notes\nthere are 3 types of object to take note\n\n1. dataframe groupby object\n ->    anz_date.groupby('Weekday')['amount']\n  \n2. dataframe series\n ->    anz_date.groupby('Weekday')['amount'].count()\n \n3. dataframe\n ->    anz_date.groupby('Weekday')['amount'].count().reset_index(name='total_day')\n \n ->    anz_weekday = anz_date.groupby(['Weekday'])----this is still a dataframe, just that weekday became the index\n--> tranform can be used to add new column to a dataframe\n\n\"\"\"\n#this method not working as in anz_weekday, the amount column already stated .count(), so the code stated below will work in the way 13 sum()\/1 count()\n# anz_weekday['avg_vol'] = anz_weekday['amount'].sum()\/anz_weekday['amount'].count()\n# print(f'anz_weekday compiled = {anz_weekday}' )\n\n\n#Plot Graph in order\nplt.plot(anz_test['Weekday'], anz_test['avg_transac'])\nplt.xlabel('Weekday')\nplt.ylabel('Average Transaction')\nplt.title('Average Weekday Transaction Volume')","a5d6355d":"anz.head()","58c2bbab":"print(anz['extraction'].dtypes)","da27619e":"print(type(anz['extraction']))","84a412c5":"#convert object to datetime\nanz['extraction_time'] =anz['extraction'].astype('datetime64[ns]')\n\nanz.head()\nanz['extraction'].dtypes\nanz.head()","2c40776e":"anz['hour'] = anz['extraction_time'].dt.strftime('%H')","003f3d7c":"anz.head()","ba46cad3":"anz_hour_summary = anz.groupby('hour')['Weekday'].count().reset_index(name='transac_vol')\nanz_hour_summary.info()","ea4dad5a":"anz['hour'].unique()","d4c9ef65":"anz_hour_summary['avg_transaction'] = anz_hour_summary['transac_vol'] \/ anz_hour_summary['hour'].count()\nanz_hour_summary","139178f5":"#Plot Graph in order\nplt.plot(anz_hour_summary['hour'], anz_hour_summary['avg_transaction'])\nplt.xlabel('hour')\nplt.ylabel('Average Transaction')\nplt.title('Hourly Average Transaction Volume')","272335b1":"#Exploring location information\n#We could firstly see the distribution of distance betwwen a customer and the merchant he\/she trades with\n#Calculate distance based on longitude and latitude \nfrom numpy import cos, sin, arcsin, sqrt\nfrom math import radians\n\ndef haversine(row):\n    lon1 = row['consumer_longitude']\n    lat1 = row['consumer_latitude']\n    lon2 = row['merchant_longitude']\n    lat2 = row['merchant_latitude']\n    lon1, lat1, lon2, lat2 = map(radians, [lon1, lat1, lon2, lat2])\n    dlon = lon2 - lon1 \n    dlat = lat2 - lat1 \n    a = sin(dlat\/2)**2 + cos(lat1) * cos(lat2) * sin(dlon\/2)**2\n    c = 2 * arcsin(sqrt(a)) \n    km = 6367 * c\n    return km\n\nanz['distance(km)'] = anz.apply(lambda row: haversine(row), axis=1)\nanz.head()","4ee1921d":"plt.hist(anz['distance(km)'], rwidth= 0.95, bins=20)\nplt.xlabel('Distance(km)')\nplt.ylabel('Frequency')\nplt.title('Distance between customer and merchants')\nplt.show()","cedd6177":"anz['distance(km)'].max()","c3e98b5d":"#create df with merchant latitude & longitude no NaNs\n\nanz_merchant = anz[pd.notnull(anz['merchant_latitude'])]\nanz_merchant.head(5)","6247bd4d":"!pip install folium\nimport folium","84ced5c3":"aus_map = folium.Map(location=[-37.8136, 144.96], zoom_start=5)\n\n#display map\naus_map","8dae24f2":"#instantiate a feature group for the incidents in the dataframe\nmerchants= folium.map.FeatureGroup()\n\n\n\n#loop through the merchants and add each to the merchants feature group\nfor lat, lng in zip(anz_merchant.merchant_latitude, anz_merchant.merchant_longitude):\n    merchants.add_child(\n        folium.features.CircleMarker(\n            [lat, lng],\n            radius=5,\n            color='red',\n            fill= True,\n            fill_color='blue',\n            fill_opacity=0.6\n        )\n    )\n\n    \n#add pop out text to eacg marker on the map\nlatitudes = list(anz_merchant['merchant_latitude'])\nlongitudes = list(anz_merchant['merchant_longitude'])\nlabels = list(anz_merchant['txn_description'])    \n\nfor lat, lng, label in zip(latitudes, longitudes, labels):\n    folium.Marker([lat, lng], popup=label).add_to(aus_map)\n    \n\n#add merchants to map\naus_map.add_child(merchants)\n","2ddf3453":"#Identify the annual salary for each customer\n#firstly check salary payment frequency of each customer\nanz_cust_id = pd.DataFrame(anz['customer_id'].unique(), columns=['customer_id']) #create dataframe to store result from numpyarray\n\nprint(type(anz_cust_id))\nprint(len(anz_cust_id))\nprint(anz_cust_id)\n","e4117b18":"anz.head()","9597388c":"#find out pay\/salary row\n\n# anz['txn_description'].unique()\nanz_w_pay = anz[anz['txn_description'] == 'PAY\/SALARY']\n# display(anz_w_pay)\n# print(type(anz_w_pay))\n#883 rows\n\n\n#create two dataframes and join together\nanz_cust_salary = anz_w_pay.groupby('customer_id')['amount'].count().reset_index(name='frequency')\nanz_cust_salary_total = anz_w_pay.groupby('customer_id')['amount'].sum().reset_index(name='total_salary')\n\nanz_cust_salary = pd.merge(anz_cust_salary, anz_cust_salary_total[['customer_id', 'total_salary']], on='customer_id', how='left')\ndisplay(anz_cust_salary)\n\n\n\n\n\"\"\"\n#####tried to create a dataframe, then use transform to add the series into the dataframe created, but then the results turned out to be NaN on the total salary column#####\n\n anz_cust_salary['total_salary_received'] = anz_w_pay.groupby('customer_id')['amount'].transform('sum')\n\n condidtion_1 = (anz_cust_salary['customer_id'] == 'CUS-1005756958')\n anz_cust_salary[condition_1]\n\nanz_cust_salary\nprint(type(anz_cust_salary['frequency']))\n\n#working code\n anz_w_pay.groupby('customer_id')['amount'].sum()\nanz_cust_salary['total'] = \nanz_w_pay.groupby('customer_id')['amount'].sum()\n\nanz_cust_salary_total['total_salary']\n\n\"\"\"\n\n\n\"\"\"\n92 days total period\nWeekly pay   ==> 12-14 pay received in the period \nFortnightly  ==> 6- 7 pay received in the period\nMonthly      ==> 2 - 5 pay received in the period\n\"\"\"\n","8dedcbfc":"# for i in anz_cust_salary['frequency']:\n#     print(i)\n\n\"\"\"\nanz_cust_salary['Annual_Sal'] = 0\nfor i in anz_cust_salary['frequency']:\n    if anz_cust_salary['frequency'][i] >= 12:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/7*365.25\n    elif anz_cust_salary['frequency'][i]<= 5:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] *12\n    else:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/14*365.25\n\ndisplay(anz_cust_salary)\n            \n\n\"\"\"\nanz_cust_salary['Annual_Sal'] = 0\nfor i in range(0, len(anz_cust_salary['frequency'])):\n    if anz_cust_salary['frequency'][i] >=12:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/7*365.25\/1000\n    elif anz_cust_salary['frequency'][i] <=5:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] *12\/1000\n    else:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/14*365.25\/1000\n\ndisplay(anz_cust_salary)        \n        \n\n    \nplt.hist(anz_cust_salary['Annual_Sal'], bins=5)\nplt.xlabel('Income')\nplt.ylabel('Frequncy')\nplt.title(\"Customers' Annual Salary(k)\")","18ef4c6e":"\"\"\"test on groupby vs groupby transform\"\"\"\nanz_w_pay\n# display(anz_w_pay)\nanz_new_test = anz_w_pay.groupby('gender')['amount'].sum().reset_index(name='amount')\n\n# anz_new_test['balance'] = anz_w_pay.groupby('gender')['balance'].sum()\n\n\n# anz_new_test['balance'] = \nanz_balance_wo_transform = anz_w_pay.groupby('gender')['balance'].sum().reset_index(name='xx')                      #working 2rows\nanz_balance_transform = anz_w_pay.groupby('gender')['balance'].transform('sum')              #working 883 rows\n\nanz_new_test['balance'] = 0\nanz_new_test['balance'] = anz_balance_wo_transform['xx']\n\n# print(type(anz_balance_wo_transform))\n# display(anz_balance_wo_transform)\n# print(type(anz_balance_transform))\n# display(anz_balance_transform)\n\n\nanz_new_test2 = anz_w_pay.groupby('gender')['balance'].sum().reset_index(name='balance')\n\n\n# display(anz_new_test)\n# display(anz_new_test2)\n\n\"\"\"\nconclusion\ncannot directly use tranform to aggregate data into new dataframe.....index not matching\ntransform only can be used back to the same dataset as it creates a scalar of same length\nmust create a new dataframe and equalized the column of new dataframe to targeted dataframe\n\n\n\n######final conclusion######\nuse groupby, then apply aggregation then .values to turn the result into numpy array\nthen can directly merge to targeted dataframe as a column without losing information\n\n\n\n\n###not working###\nanz_test_balance['amount'] = anz_w_pay.groupby('gender')['balance'].sum()\n\n\n####working#####\nanz_test_balance['amount'] = anz_w_pay.groupby('gender')['balance'].sum().values\nanz_test_balance['amount'] = anz_w_pay.groupby('gender')['balance'].sum().to_numpy()\n\n\nhttps:\/\/www.codegrepper.com\/code-examples\/python\/add+column+to+dataframe+pandas+from+another+dataframe\n\"\"\"\n\n\n\nanz_test_balance = anz_w_pay.groupby('gender')['balance'].sum().reset_index(name='balance')\n\n\n\n\n\"\"\"either way will do (.values), .to_numpy()\"\"\"\nanz_test_balance['amount'] = anz_w_pay.groupby('gender')['balance'].sum().values\n# anz_test_balance['amount'] = anz_w_pay.groupby('gender')['balance'].sum().to_numpy()\n\n\n\nprint(type(anz_test_balance['amount']))\ndisplay(anz_test_balance)\n\n","fcce81e5":"#create a dataframe to store relevant features for customers\nanz_cust_behaviour = anz[['customer_id', 'gender', 'amount', 'date', 'balance']]\ndisplay(anz_cust_behaviour)\nprint(type(anz_cust_behaviour))\n\n#add weekly transac amount, max_amount, no_large_transac, use_no_day, avg_transac_amount, median_balance\n\n\nanz_cust_behaviour_summary = anz_cust_behaviour.groupby('customer_id')['amount'].sum().reset_index(name='total_amount')\n\n#display(anz_cust_behaviour_summary)\n\n#create new column\nanz_cust_behaviour_summary['weekly_average_transaction'] = 0\nanz_cust_behaviour_summary['max_amount'] = 0\nanz_cust_behaviour_summary['no_large_transaction'] = 0\nanz_cust_behaviour_summary['use_no_day'] = 0\nanz_cust_behaviour_summary['avg_transac_amount'] = 0 \nanz_cust_behaviour_summary['median_balance'] = 0\n\n\"\"\"\nbest solution\ncreate a new dataframe of the aggregated\/grouped data, then equalized the columns of both the new dataframe and targeted dataframe\n\"\"\"\n#max_amount\nanz_max = anz_cust_behaviour.groupby('customer_id')['amount'].max().reset_index(name='max_max')\nanz_max\n\nanz_cust_behaviour_summary['max_amount'] = anz_max['max_max']\nanz_testtest = anz_cust_behaviour.groupby('customer_id')['amount'].max().reset_index(name='max')\n\n\n#avg_transac_amount\navg_amount = anz_cust_behaviour.groupby('customer_id')['amount'].mean().reset_index(name='avg_amount')\nanz_cust_behaviour_summary['avg_transac_amount'] = avg_amount['avg_amount']\n\n\n#median balance\nanz_median = anz_cust_behaviour.groupby('customer_id')['balance'].median().reset_index(name='median_balance')\nanz_cust_behaviour_summary['median_balance'] = anz_median['median_balance']\n\n#no_of_day transaction made\nanz_date = anz_cust_behaviour.groupby('customer_id')['date'].nunique().reset_index(name='unique_date')\ndisplay(anz_date)\nprint(f'anz is {type(anz_date)}')\nanz_cust_behaviour_summary['use_no_day'] = anz_date['unique_date']\n\n\n#weekly transaction\n\"\"\"\nalthough cannot directly transform from other dataset, \nbut calculation based on other datasets with same scalar still can be performed without any error\nsee \n------anz_cust_behaviour_summary['weekly_average_transaction'] = round(anz_weekly_transaction['total_transaction_count']\/anz_date['unique_date']*7)-------\nthe abovementioned code comprised of 3 datasets where the targeted dataset is depending on the calculations from the other 2 datasets.\n\"\"\"\n\nanz_weekly_transaction = anz_cust_behaviour.groupby('customer_id')['amount'].count().reset_index(name='total_transaction_count')\nanz_weekly_transaction['count_date'] = anz_date['unique_date']\nanz_weekly_transaction['avg_transac'] = round(anz_weekly_transaction['total_transaction_count']\/anz_date['unique_date']*7 )\ndisplay(anz_weekly_transaction)\nanz_cust_behaviour_summary['weekly_average_transaction'] = round(anz_weekly_transaction['total_transaction_count']\/anz_date['unique_date']*7)\n\n\n\n\n#no of large transaction\n\n# condition = anz_cust_behaviour[anz_cust_behaviour['amount']>=100]\n# df = anz_cust_behaviour[anz_cust_behaviour['amount']>=100].groupby('customer_id')['amount'].count().reset_index(name='large_transaction')\n# anz_cust_behaviour_summary['no_large_transaction'] = df['large_transaction']\n\n\nanz_cust_behaviour_summary['no_large_transaction'] = anz_cust_behaviour[anz_cust_behaviour['amount']>=100].groupby('customer_id')['amount'].count().values\n\ndisplay(anz_cust_behaviour_summary)\n\n\n","10e6af39":"anz_cust_behaviour_summary.info()","7053316b":"anz_cust_behaviour_summary.head()","e911e7d3":"anz_cust_behaviour_summary['annual _salary'] = 0\nanz_cust_behaviour_summary['gender'] = 0\nanz_cust_behaviour_summary['age'] = 0\nanz_cust_behaviour_summary['state'] = 0\n\n\nanz_cust_behaviour_summary.head()\n\n","998e1f66":"anz_cust_behaviour_summary['annual_salary'] = anz_cust_salary['Annual_Sal']","24ab23c4":"anz_cust_behaviour_summary.head()","f523844a":"anz.head()","72b0c261":"anz_cust_behaviour_summary['age'] = anz.groupby('customer_id')['age'].max().values","39c029a2":"anz_cust_behaviour_summary.head()","8a9bb52f":"anz_cust_behaviour_summary['gender'] = anz.groupby('customer_id')['gender'].max().values","ac1ba685":"anz_cust_behaviour_summary.head()","71c1a50f":"anz.head()","589bf5e2":"anz_salary2 = anz[anz['txn_description'] == 'PAY\/SALARY'].groupby('customer_id')['txn_description'].count().reset_index(name= 'pay_frequency')","8c2ee859":"anz_salary2['total_payment_received'] = 0\nanz_salary2['annual_salary'] = 0\nanz_salary2.head()","5e2daf0a":"column_name = list(anz_salary2.columns.values)\ncolumn_name\nanz_salary2 = anz_salary2[['customer_id', 'total_payment_received', 'pay_frequency', 'annual_salary']]\nanz_salary2.head()\n","7b7e57bc":"anz_salary2['total_payment_received']= anz[anz['txn_description'] == 'PAY\/SALARY'].groupby('customer_id')['amount'].sum().values\nanz_salary2.head()","68bd2153":"%%timeit\n##normal looping, result shows that in average, it takes 9.35 ms to process the loop, for 100 rows, for 10,000 rows, it takes 9.35s, for 100,000 it takes 93.5s, for 1,000,000 rows it takes 935s, so efficiency is a bit low. \n##iterating cell by cell.\n\nanz_cust_salary['Annual_Sal'] = 0\nfor i in range(0, len(anz_cust_salary['frequency'])):\n    if anz_cust_salary['frequency'][i] >=12:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/7*365.25\/1000\n    elif anz_cust_salary['frequency'][i] <=5:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] *12\/1000\n    else:\n        anz_cust_salary['Annual_Sal'][i] = anz_cust_salary['total_salary'][i] \/14*365.25\/1000\n\nanz_cust_salary","009510bc":"%%timeit\n\n#customize function for lambda\n#https:\/\/codeforests.medium.com\/ppicpandas-tricks-pass-multiple-columns-to-lambda-e0c16312fb50                      how to use customized function in apply, lambda\n#result shows that by using apply and loop (vectorize) through pandas series(columns), we get 1.65ms for looping over 100 rows. for 10,000 rows, it takes 1.65s, 100,000 rows it takes 16.5s, 1,000,000 rows for 165s.\ndef annual_salary(pay_frequency, total_payment_received):\n    annual_salary = 0\n    if pay_frequency >= 12:\n        annual_salary = total_payment_received\/7*365.25\n    elif pay_frequency <= 5:\n        annual_salary = total_payment_received*12\n    else:\n        annual_salary = total_payment_received\/14*365.25\n    return annual_salary\n    \n\n\n\nanz_salary2['annual_salary'] = anz_salary2.apply(lambda x: annual_salary(x['pay_frequency'], x['total_payment_received']), axis= 1)\n# anz_salary2['annual_salary'] = anz_salary2.apply(lambda x: x['total_payment_received']\/7*20 if x['pay_frequency'] == 7 else 0, axis =1)","b63d0b93":"%%timeit\n# https:\/\/engineering.upside.com\/a-beginners-guide-to-optimizing-pandas-code-for-speed-c09ef2c6a4d6        ##speed comparison of normal looping, looping through apply, verctorization of pandas series, vectorization of numpy array\n#result shows that by using apply and loop (vectorize) through numpy array, we get xxms for looping over 100 rows. for 10,000 rows, it takes 1.65s, 100,000 rows it takes 16.5s, 1,000,000 rows for 165s.\n#https:\/\/www.dataquest.io\/blog\/tutorial-add-column-pandas-dataframe-based-on-if-else-condition\/\n###########################################np.select###########################################################\nconditions = [\n    \n    (anz_salary2['pay_frequency'] >= 12),\n    (anz_salary2['pay_frequency'] <= 5),\n    (anz_salary2['pay_frequency'] < 12) & (anz_salary2['pay_frequency'] > 5),   \n    ]\n\n\nvalues = [anz_salary2['total_payment_received']\/7*365.25, anz_salary2['total_payment_received']*12, anz_salary2['total_payment_received']\/14*365.25 ]\n\n\nanz_salary2['annual_salary'] = np.select(conditions, values)\n\n# anz_salary2 = annual_salary_cal(anz_salary2['pay_frequency'], anz_salary2['total_payment_received'])\nanz_salary2.head()\n\n\n","f96bea36":"%%timeit\n###############np.vectorize######################\n#https:\/\/stackoverflow.com\/questions\/39109045\/numpy-where-with-multiple-conditions\ndef annual_salary(pay_frequency, total_payment_received):\n    annual_salary = 0\n    if pay_frequency >= 12:\n        annual_salary = total_payment_received\/7*365.25\n    elif pay_frequency <= 5:\n        annual_salary = total_payment_received*12\n    else:\n        annual_salary = total_payment_received\/14*365.25\n    return annual_salary\n\n\n\nanz_salary2['annual_salary'] = np.vectorize(annual_salary)(anz_salary2['pay_frequency'], anz_salary2['total_payment_received'])\n\n\n","94b854bf":"anz_salary2.head()","fe66cfeb":"anz_cust_behaviour_summary['annual_salary']= anz_salary2['annual_salary']\nanz_cust_behaviour_summary.drop(anz_cust_behaviour_summary.columns[8], axis= 1, inplace= True)\nanz_cust_behaviour_summary.head()","ce1a9bdb":"anz_cust_behaviour_summary['age_below20'] = 0\nanz_cust_behaviour_summary['age_btw20n40'] = 0\nanz_cust_behaviour_summary['age_btw40n60'] = 0\nanz_cust_behaviour_summary.head()\n","75608566":"##create additional features\n\n\"\"\"\n%%timeit\nanz_cust_behaviour_summary['age_below20'] = anz_cust_behaviour_summary.apply(lambda x: age_20(x['age']), axis= 1)\nanz_cust_behaviour_summary['age_btw20n40'] = anz_cust_behaviour_summary.apply(lambda x: age_40(x['age']), axis= 1)                                                                              \nanz_cust_behaviour_summary['age_btw40n60'] = anz_cust_behaviour_summary.apply(lambda x: age_60(x['age']), axis= 1)   \nanz_cust_behaviour_summary.head()   \n\n4.83 ms \u00b1 553 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each\n\n\"\"\"\n\n\"\"\"\n558 \u00b5s \u00b1 13.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each \n\"\"\"\n\ndef age_20(age):\n    if age <= 20:\n        return 1\n    return 0\n\ndef age_40(age):\n    if 20 <age <= 40:\n        return 1\n    return 0\n\ndef age_60(age):\n    if 40 <age <= 60:\n        return 1\n    return 0\n\nanz_cust_behaviour_summary['age_below20'] = np.vectorize(age_20)(anz_cust_behaviour_summary['age'])\nanz_cust_behaviour_summary['age_btw20n40'] = np.vectorize(age_40)(anz_cust_behaviour_summary['age'])\nanz_cust_behaviour_summary['age_btw40n60'] = np.vectorize(age_60)(anz_cust_behaviour_summary['age'])\n\nanz_cust_behaviour_summary.head()\n\n\n","2254dd79":"anz.head()","5eff2a66":"#investigate the state where customers live\n#assume they live where most transactions occured (indicated by merchant_state)\nmerchant_state = anz.groupby(['customer_id', 'merchant_state'])['merchant_state'].count().reset_index(name='trans_count')\nmerchant_state.info()\nmerchant_state.head()\n\nmerchant_state_max = merchant_state[merchant_state.groupby('customer_id')['trans_count'].transform(max) == merchant_state['trans_count']]\nmerchant_state_max.info()\nmerchant_state_max.head()\n\nmerchant_state_max_update = merchant_state_max.reset_index()\n\n","b6cec478":"print(merchant_state_max_update[merchant_state_max_update['customer_id']== 'CUS-2650223890'])\nprint(merchant_state_max_update[merchant_state_max_update['customer_id']== 'CUS-495599312'])","6126f897":"#for equal number of transactions btw multiple states, pick the most likely state\nmerchant_state_max[merchant_state_max.duplicated('customer_id')]\n# merchant_state_max.groupby('customer_id').filter(lambda x: x['customer_id'].value_counts()<2)\n# merchant_state_max.groupby('customer_id').filter(lambda x: x['customer_id'].is_unique)\n\n#215, 356\nmerchant_state_max[merchant_state_max['customer_id']== 'CUS-2650223890']\nmerchant_state_max[merchant_state_max['customer_id']== 'CUS-495599312']\n#find out which states gt more transaction record\nmerchant_state.groupby('merchant_state')['trans_count'].count()\n\n\"\"\"\n#result shows that \nACT    18\nVIC    90\n--NSW    89\n--SA     69\nQLD    66\nWA     65\nTAS    10\n--NT      7\nhence, assume that those duplicate ones staying at merchant state with less transaction count\nas higher transaction count maybe occured in popular state for shopping\n\"\"\"\nmerchant_state_max.info()\nmerchant_state_max.head()\n\nmerchant_state_update = merchant_state_max_update.drop([merchant_state_max_update.index[52], merchant_state_max_update.index[88]])\n\n                        \n                         \n\nmerchant_state_update.info()\nmerchant_state_update.head()\n","43b5556d":"anz_cust_behaviour_summary.head()","f0307797":"anz_cust_behaviour_summary['state'] = merchant_state_update['merchant_state']\nanz_cust_behaviour_summary.head()","cf7dc5c1":"#explore correlations between annual salary and various customer attributes and features, visualise using scatter plot.\n\nfrom pandas.plotting import scatter_matrix\nscatter_matrix(anz_cust_behaviour_summary, figsize=(15,15))\nplt.show()\n","655bd734":"#start with the model that includes all the features created\n# from sklearn.linear_model import LinearRegression, Lasso\nimport math\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\n","8c753b61":"#define dependent(y) and independent(x) variables\ny= anz_cust_behaviour_summary['annual_salary']\nx=anz_cust_behaviour_summary.drop(columns=['annual_salary', 'customer_id','state','gender'], axis=1)\n\n#train test split \nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size= 0.2, random_state=42)","ad9b712f":"#check for shape\nx_train.shape, y_train.shape, x_test.shape, y_test.shape\n","180cd572":"#apply standard scaling to get optimized results\n\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test = sc.transform(x_test)\n","9cfba305":"#create and train linear model \nLR = LinearRegression()\nLR.fit(x_train, y_train)\n","c4ec01b7":"#Use model to predict on test data\nprediction = LR.predict(x_test)\n","03b239ff":"#print model performance\nprint('Coefficients:', LR.coef_)\nprint('Intercept', LR.intercept_)\nprint('Mean squared error (MSE): %.2f' % mean_squared_error(y_test, prediction)) #%.2f=roundoff to 2 decimal places\nprint('Coefficient of determination (R^2): %.2f' % r2_score(y_test, prediction)) #equivalent of model.score()        #LR.score(x_test, y_test)\nprint(f'RMSE: %.2f'% math.sqrt(mean_squared_error(y_test,prediction)))\n\n# print(f'adjusted R2 ={1-(1-r2_score(y_test,prediction))*((len(x_test)-1)\/(len(x_test)-len(x_test[0])-1))}')\nprint(f'adjusted r2 ={1-(1-r2_score(y_test, prediction))*(len(x_test)-1)\/(len(x_test)-len(x_test[0])-1)}')\n\n","af1fa3c6":"\"\"\"\n###coefficient \ncoefficients are the weight of the features in the x dataframe\nprint(x.columns) #output ['total_amount', 'weekly_average_transaction', 'max_amount', 'no_large_transaction', 'use_no_day', 'avg_transac_amount','median_balance', 'age', 'age_below20', 'age_btw20n40', 'age_btw40n60']\ny(prediction) = 345761.17257798*(total_amount) + 18198.25908356*(weekly_average_transaction)+ 183847.03493676*(max_amount)...\nso if coefficient of feature is not zero, keep adding features will increase the coefficient of determination (R^2)\n\n\n\"\"\"\n\n\"\"\"\n###root mean squared error (RMSE)\nAround 0.001 is great, 1.0 - 2.0 means you should tune your model, greater than that means if tuning doesn't work, try another model.\n\"\"\"\n\n\"\"\"\n###Coefficient of determination (R^2)    #closer to 1 is better\n\nR\u00b2 gives us a measure of how well the actual outcomes are replicated by the model or the regression line. \nThis is based on the total variation of prediction explained by the model. R\u00b2 is always between 0 and 1 or between 0% to 100%.\nAn R2 of 0 means that the dependent variable cannot be predicted from the independent variable.\nAn R2 of 1 means the dependent variable can be predicted without error from the independent variable.\nAn R2 between 0 and 1 indicates the extent to which the dependent variable is predictable. \nAn R2 of 0.10 means that 10 percent of the variance in Y is predictable from X; an R2 of 0.20 means that 20 percent is predictable; and so on.\n\"\"\"\n\"\"\"\nintercept is the predicted y value when x=0\n\"\"\"\n\"\"\"\nadjusted r2\nhttps:\/\/stackoverflow.com\/questions\/49381661\/how-do-i-calculate-the-adjusted-r-squared-score-using-scikit-learn\/49381947\n\n\"\"\"\n\n\"\"\"\nref:\nhttps:\/\/stattrek.com\/statistics\/dictionary.aspx?definition=coefficient_of_determination\nhttps:\/\/www.youtube.com\/watch?v=2AQKmw14mHM\nhttps:\/\/corporatefinanceinstitute.com\/resources\/knowledge\/other\/coefficient-of-determination\/#:~:text=The%20coefficient%20of%20determination%20(R%C2%B2,that%20is%20changed%20in%20order\n\"\"\"\n\n\n\n","fcd059b9":"from sklearn.tree import DecisionTreeRegressor\nfrom sklearn import tree\nmodel = DecisionTreeRegressor(random_state=0)\nmodel = model.fit(x_train, y_train)","e9dd19bf":"tree_prediction = model.predict(x_test)","732f01e5":"#plot tree\nfig= plt.figure(figsize=(25,20), dpi=100)\ngg = tree.plot_tree(model, filled=True, feature_names=x.columns, max_depth= 2)\nplt.savefig('tree.pdf', bbox_inches='tight')\n\n","6cfb8dff":"#check accuracy\n#compute test-set MSE\nmse_tree = mean_squared_error(y_test, tree_prediction)\n#compute test-set RMSE\nrmse_tree = mse_tree**(1\/2)\nprint(rmse_tree)","b064b4d5":"**a. Overall Transaction Amount**","cc525051":"#### 6.2.1 Model Building ","5ae90613":"#### 4.1 Data Preparation\n","162e3a46":"### 3.0 Description of All Data Sources Used\n\n#### 3.1 Source, Liscensing, Privacy\n\n1. The dataset is provided ANZ based on a synthesised transaction dataset containing 3 months\u2019 worth of transactions for 100 hypothetical customers. It contains purchases, recurring transactions, and salary transactions.\n\n2. License: N\/A\n\n3. Source: https:\/\/www.theforage.com\/virtual-internships\/prototype\/ZLJCsrpkHo9pZBJNY\/ANZ-Virtual-Internship?inv=8jeCcsp6NDcp4juED\n\n4. Privacy: The dataset was generated from 100 hypothetical customers, hence no privacy issues involved.\n\n#### 3.2 Data Credibility Checking Using ROCCC Method\nN\/A: Not Applicable as the dataset was generated from hypothetical customers.\n\n1. Reliability: N\/A \n2. Original: N\/A\n3. Comprehensive: N\/A\n4. Current: N\/A\n5. Cited: N\/A","7fd939c9":"#### 6.1.2 Measure Model Performance","e9e86a02":"### 2.0 Summary of Business Task\n\n\n#### Task 1: Exploratory Data Analysis\n\nSegment the dataset and draw unique insignts, including visualisation of the transaction volume and assessing the effect of any outliers.\n\n#### Task 2: Predictive Analytics\n\nExplore correlations between customer attributes, build a regression and a decision-tree prediction model based on your findings.\n\n","099a5d7a":"To validate, we could further plot the location of the customer and the merchants he\/she trades with on a map\n","76a6bf00":"#### 5.2.2 Explore correlations between annual salary and various customer attributes","573b3740":"**d. Average Transaction Volume By Hour**","f12e2643":"### 5.0 Data Analysis\n\nGather some interesting overall insights about the data","8b45d41e":"#### 5.2.1 Identify the annual salary for each customer","2cd04783":"#### 6.2.2 Visualization of Tree","9287151d":"#### 5.1 DA-Virtual Internship Task 1","639cf86b":"### 5.2 DA-Vitual Internship Task 2","1ca5b656":"### 6.0 Build a simple regression model to predict the annual salary for each customer","417319eb":"**c. Average Transaction Volume By Weekday**","11750bd3":"#### 3.3 Data Preparation\n\nThe purpose of doing so is to\n\nMake sure that each column feature is in its correct datatype (ex. date in datetime, age in integer etc..),otherwise, then we need to tranform the wrong datatype to the correct datatype.\nLocate missing values\nGather & understand information in the datset. -> The dataset contains 12043 transactions for 100 customers who have one bank account each.\n-> Transaction period is from 01\/08\/2018-31\/10\/2018 (92 days duration)\n\n-> The data entries are unique and have consistent formats for analysis.\n\n-> For each row, information is complete for majority of columns. Some columns contain missing data, which is likely due to the nature of transaction (i.e merchants are not involved for Interbank transfer or salary payments)\n\n-> It is also noticed that there is only 91 unique dates in the dataset, suggesting the transaction records for one day are missing (2018-08-16)\n\n-> The range of each feature should also be examined which shows that there is one customer that resides outside Australia.","557e856a":"#### 5.1.1 Data Visualization","d7eebe22":"#### 6.1.1 Model Building","94b7561d":"### 4.0 Documentation of Cleaning or Manipulation of Data","c8a67d9a":"#### 6.1.3 Summary and Observation on Performance of Regression Model\nThe RMSE of the model over the whole dataset is over 177k, which indicates the inaccurary of the model. The model\u2019s adjusted R^2\/ R^2 also shows that it only explains about 47% of variation in customers\u2019annual salary. It is thus risky to use this linear model to predict customer's income bracket. More data is required to develop a more reliable model.","a99727c9":"#### 6.1 Regression Model","5cbbbb1e":"**b. Monthly Transaction Volume**","8400d55f":"#### 5.1.2 Summary of Data Analysis-Task 1\n\n#a. overall transaction amount\n1. The histogram shows that most of the transaction amount is around 10-30 AUD.\n\n#b. customers' monthly transaction volume(time)\n1. The histogram shows that mostly of the customers do a montly transaction of 0-150 times monthly.\n\n#c. average transaction volume by weekday\n1. The line graph shows that Wednesday and Friday have the highest transaction volume, while transaction volume is the lowest on Monday.\n\n#d. average transaction volume by hour\n1. Hourly transaction volume is highest during 7-8 am.and lowest during midnight.","ec2ebfb9":"#### 6.2.3 Measure Model Performance","1430ee2c":"### 7.0 Conclusion and Recommendation\nThe model is not abla to predict the annual salary of the customer accurately as shown by the model perfomance of the Regression Model and Decision Tree Regressor. More data is required to develop a more reliable model.","c5919871":"### 1.0 Introduction\nThis is ANZ virtual internship. This internship comprised of two parts:\n\nTask 1: Exploratory Data Analysis\n\nTask 2: Predictive Analytics\n\n\n\n\n##### Project completed by: Kee Kia Yun dated 27 May 2021","cbcf1c0c":"#### 6.2.4 Summary and Observation on Performance of Decision Tree Model \nThe RMSE of the model over the whole dataset is over 214k, which indicates the inaccurary of the model.","ae2b985a":"### 6.2 Decision-tree"}}