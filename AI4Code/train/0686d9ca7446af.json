{"cell_type":{"9cdf8a93":"code","80ffae9c":"code","af49656a":"code","60b79ba2":"code","e56f7dc4":"code","c9fb8e60":"code","e9a74602":"code","8604c39e":"code","433363b6":"code","5a8c8d9d":"code","b40e59f7":"code","93e9141d":"code","a3f1a8b3":"code","01ded825":"code","4a1f4a7c":"code","05688699":"code","da088a00":"code","a93aaf20":"code","63db3ad1":"code","1e568c92":"code","74157ec0":"code","b8d8fd8d":"code","5f857340":"code","add81101":"code","4f10ba83":"code","bdcd5be1":"code","3f67c87c":"code","8883cfe4":"code","c5860734":"code","730cb442":"code","fc4ba47d":"code","929261c1":"code","dab2a1c7":"code","33a8a16d":"code","335f9edd":"code","48624198":"code","e9edf9ae":"code","93b599eb":"code","a9dc67d8":"code","63484e23":"code","50178e29":"code","56a69520":"code","13d7a3d0":"code","ed9c2f7a":"code","87c24e58":"code","cc275230":"code","00536a3e":"code","ba48a8d3":"code","123664b2":"code","5e7a47c9":"code","3f4ca2c6":"code","3d9b3ff5":"code","69caed78":"code","89778dfb":"code","fb2da3e6":"code","b64b3924":"code","0ca1e079":"code","8ed0ea57":"code","f484d08a":"markdown","97fba533":"markdown","22171345":"markdown","87d9d97d":"markdown","f2e9346c":"markdown","a83d84f4":"markdown","76094e55":"markdown","4ef796cc":"markdown","0b591f31":"markdown","87aa634b":"markdown","bf755a4f":"markdown","d919a379":"markdown","df876fc5":"markdown","572249aa":"markdown","b2f4bf4e":"markdown","4af41afc":"markdown","0cade821":"markdown","75962179":"markdown","a9646e2e":"markdown","6b175585":"markdown","7e39a2fd":"markdown","ad4551f5":"markdown","9f379c05":"markdown","fbc619b8":"markdown","9ae5e5a7":"markdown","3a7e1c0f":"markdown","ecc4fc9a":"markdown","4752241a":"markdown","57148dbd":"markdown"},"source":{"9cdf8a93":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns","80ffae9c":"#load the csv file and make the data frame\nbank_df = pd.read_csv('\/kaggle\/input\/bank-loan\/Bank_Personal_Loan_Modelling.csv')","af49656a":"#display the first 5 rows of data frame\nbank_df.head()","60b79ba2":"print(\"The dataframe has {} rows and {} columns\".format(bank_df.shape[0],bank_df.shape[1]))","e56f7dc4":"#display information of data frame\nbank_df.info()","c9fb8e60":"#another way to check if null values are there or not\nbank_df.apply(lambda x:sum(x.isnull()))","e9a74602":"#5 point summary of dataframe\nbank_df.describe().transpose()","8604c39e":"#display histogram plot of each attribute\/column\nfor i in bank_df.columns:\n    plt.hist(bank_df[i])\n    plt.xlabel(i)\n    plt.ylabel('frequency')\n    plt.show()","433363b6":"bank_df['Personal Loan'].value_counts()","5a8c8d9d":"print(\"Percentage of customer accept personal loan is {}%\".format((480\/5000)*100))\nprint(\"Percentage of customer not accept personal loan is {}%\".format((4520\/5000)*100))","b40e59f7":"sns.distplot(bank_df['Personal Loan'],kde=False)\nplt.show()","93e9141d":"new_bank_df = bank_df.copy()","a3f1a8b3":"print(\"the total customers whose experience is in negative is {}\".format((new_bank_df[new_bank_df['Experience']<0]).shape[0]))","01ded825":"#converting negative experience values into positive\nnew_bank_df['Experience'] = new_bank_df['Experience'].apply(lambda x : abs(x) if(x<0) else x)","4a1f4a7c":"print(\"now after manipulation total customers whose experience is in negative is {}\".format((new_bank_df[new_bank_df['Experience']<0]).shape[0]))","05688699":"#dropping ID and ZIP Code columns from new_bank_df dataframe\nnew_bank_df.drop(['ID','ZIP Code'],axis=1,inplace=True)","da088a00":"#display first 5 rows of dataframe.\nnew_bank_df.head()","a93aaf20":"#display pair plot\nsns.pairplot(data=new_bank_df,hue='Personal Loan')\nplt.show()","63db3ad1":"X = new_bank_df.drop('Personal Loan',axis=1)\ny = new_bank_df['Personal Loan']","1e568c92":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.30,random_state=1)","74157ec0":"print(\"The training feature are {} % of dataset and training labels are {} % of dataset\".format(((X_train.shape[0]\/5000)*100),((y_train.shape[0]\/5000)*100)))\nprint(\"The test feature are {} % of dataset and test labels are {} % of dataset\".format(((X_test.shape[0]\/5000)*100),((y_test.shape[0]\/5000)*100)))","b8d8fd8d":"#importing the library\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score,confusion_matrix,recall_score,precision_score,f1_score ","5f857340":"lr = LogisticRegression() #Instantiate the LogisticRegression object\nlr.fit(X_train,y_train) #call the fit method of logistic regression to train the model or to learn the parameters of model","add81101":"y_predict = lr.predict(X_test) #predicting the result of test dataset and storing in a variable called y_predict","4f10ba83":"print(accuracy_score(y_test,y_predict))#printing overall accuracy score","bdcd5be1":"print(\"Confusion matrix\")\nprint(confusion_matrix(y_test,y_predict))#creating confusion matrix","3f67c87c":"#displaying precision,recall and f1 score.\ndf_table = confusion_matrix(y_test,y_predict)\na = (df_table[0,0] + df_table[1,1]) \/ (df_table[0,0] + df_table[0,1] + df_table[1,0] + df_table[1,1])\np = df_table[1,1] \/ (df_table[1,1] + df_table[0,1])\nr = df_table[1,1] \/ (df_table[1,1] + df_table[1,0])\nf = (2 * p * r) \/ (p + r)\n\nprint(\"accuracy : \",round(a,2))\nprint(\"precision: \",round(p,2))\nprint(\"recall   : \",round(r,2))\nprint(\"F1 score : \",round(f,2))","8883cfe4":"#another way of displaying precision,recall and f1 score\nprint(\"precision:\",precision_score(y_test,y_predict))\nprint(\"recall   :\",recall_score(y_test,y_predict))\nprint(\"f1 score :\",f1_score(y_test,y_predict))","c5860734":"for idx, col_name in enumerate(X_train.columns):\n    print(\"The coefficient for {} is {}\".format(col_name, lr.coef_[0][idx]))","730cb442":"print(\"The intercept is {}\".format(lr.intercept_))","fc4ba47d":"#importing the library\nfrom sklearn.neighbors import KNeighborsClassifier","929261c1":"knn = KNeighborsClassifier(n_neighbors=5) #Initialize the object\nknn.fit(X_train,y_train)  #call the fit method of knn classifier to train the model","dab2a1c7":"knn_y_predict = knn.predict(X_test) #predicting the result of test dataset and storing in a variable called knn_y_predict","33a8a16d":"print(accuracy_score(y_test,knn_y_predict)) #printing overall accuracy score","335f9edd":"print(\"Confusion matrix\")\nprint(confusion_matrix(y_test,knn_y_predict)) #creating confusion matrix","48624198":"#displaying precision,recall and f1 score\nprint(\"precision:\",precision_score(y_test,knn_y_predict))\nprint(\"recall   :\",recall_score(y_test,knn_y_predict))\nprint(\"f1 score :\",f1_score(y_test,knn_y_predict))","e9edf9ae":"#importing the library\nfrom sklearn.naive_bayes import GaussianNB","93b599eb":"nb = GaussianNB() #Initialize the object\nnb.fit(X_train,y_train)  #call the fit method of gaussian naive bayes to train the model or to learn the parameters of model","a9dc67d8":"nb_y_predict = nb.predict(X_test)  #predicting the result of test dataset and storing in a variable called nb_y_predict","63484e23":"print(accuracy_score(y_test,nb_y_predict))  #printing overall accuracy score","50178e29":"print(\"Confusion matrix\")\nprint(confusion_matrix(y_test,nb_y_predict))  #printing confusion matrix","56a69520":"#displaying precision,recall and f1 score\nprint(\"precision:\",precision_score(y_test,nb_y_predict))\nprint(\"recall   :\",recall_score(y_test,nb_y_predict))\nprint(\"f1 score :\",f1_score(y_test,nb_y_predict))","13d7a3d0":"#importing the library\nfrom sklearn.svm import SVC","ed9c2f7a":"svc = SVC()  #Initialize the object\nsvc.fit(X_train,y_train)  #call the fit method of support vector machine to train the model or to learn the parameters of model","87c24e58":"svc_y_predict = svc.predict(X_test)  #predicting the result of test dataset and storing in a variable called svc_y_predict","cc275230":"print(accuracy_score(y_test,svc_y_predict))  #printing overall accuracy score","00536a3e":"print(\"Confusion matrix\")\nprint(confusion_matrix(y_test,svc_y_predict))#printing confusion matrix","ba48a8d3":"#displaying precision,recall and f1 score\nprint(\"precision:\",precision_score(y_test,svc_y_predict))\nprint(\"recall   :\",recall_score(y_test,svc_y_predict))\nprint(\"f1 score :\",f1_score(y_test,svc_y_predict))","123664b2":"#Earlier we select k randomly as 5 now we will see which k value will give least misclassification error\n# creating odd list of K for KNN\nmyList = list(range(1,20))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))","5e7a47c9":"# empty list that will hold accuracy scores\nac_scores = []\n\n# perform accuracy metrics for values from 1,3,5....19\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred_var = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred_var)\n    ac_scores.append(scores)\n\n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","3f4ca2c6":"# plot misclassification error vs k\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.show()","3d9b3ff5":"knn_opt = KNeighborsClassifier(n_neighbors=9) #Initialize the object\nknn_opt.fit(X_train,y_train)#call the fit method of knn classifier to train the model","69caed78":"knn_opt_y_predict = knn_opt.predict(X_test)#predicting the result of test dataset and storing in a variable called knn_opt_y_predict","89778dfb":"print(accuracy_score(y_test,knn_opt_y_predict))#printing overall accuracy score","fb2da3e6":"print(\"Confusion matrix\")\nprint(confusion_matrix(y_test,knn_opt_y_predict))#creating confusion matrix","b64b3924":"#displaying precision,recall and f1 score\nprint(\"precision:\",precision_score(y_test,knn_opt_y_predict))\nprint(\"recall   :\",recall_score(y_test,knn_opt_y_predict))\nprint(\"f1 score :\",f1_score(y_test,knn_opt_y_predict))","0ca1e079":"lr_scores = []\nthresh = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\nfor i in range(0,len(thresh)):\n    preds = np.where(lr.predict_proba(X_test)[:,1] >=thresh[i], 1, 0)\n    accurcy_scores = accuracy_score(y_test, preds)\n    lr_scores.append(accurcy_scores)\n\ndf = pd.DataFrame(data={'thresh':thresh,'accuracy_scores':lr_scores})\nprint(df)","8ed0ea57":"plt.plot(thresh,lr_scores)\nplt.xlabel('Threshold')\nplt.ylabel('Accuracy_scores')\nplt.show()","f484d08a":"#Overview <a><\/a>\nWelcome to my Kernel! In this kernel, I use various classification models and try to classify whether customer will accept Personal Loan or not. As you can guess, there are various methods to suceed this and each method has pros and cons.\nIf you have a question or feedback, do not hesitate to write.","97fba533":"so from above we can see that there are no null values in the dataframe and all columns are numeric type.","22171345":"#Importing Modules and Exploratory Data Analysis(EDA) <a><\/a>","87d9d97d":"so in our test dataset total 1500 entities\/customers are there. so (1315+36)=1351, means 1351 customers out of 1500 customers in real not accept personal loan but our model predict 1315\/1351 not accept personal loan and for 36 customers it did wrong prediction. likewise, (99+50)=149, means 149 customers out of 1500 customers accept personal loan but our model predict 50\/149 accept personal loan and for 99 customers it did wrong prediction.","f2e9346c":"so in logistic regression our hypothesis is h(z) = 1\/(1+e^(-z)). where z = -0.40*Age +  0.40*Experience + 0.048*Income + 0.63*family + 0.16*CCAvg + 1.62*Education + 0.000782*Mortgage + -0.86*Securities Account + 3.2*CD Account + -0.59*Online + -1.01*CreditCard + -2.37.\nso above hypothesis give probability if probability is>=0.5 then we will classify as 1 else 0.","a83d84f4":"->As per the data provided CreditCard attribute means does the customer use a credit card issued by universal bank so from above it looks like most customer not using credit card (i.e.,frequency of customer not using credit card is high)\n\n->As per the data provided Online attribute means does the customer use internet banking facilities so from above it looks like most customer using internet facility(i.e.,frequency of customer using online facility is high)\n\n->As per the data provided CD Account attribute means does the customer have a certificate of deposit(CD) account with the bank so from above it looks like most customer not having CD Account(i.e.,frequency of customer not having CD Account is high)\n\n->As per the data provided Securities Account attribute means does the customer have a securities account with the bank so from above it looks like most customer not having Securities Account(i.e.,frequency of customer not having securities Account is high)\n\n->As per the data provided Personal Loan attribute means did this customer accept the personal loan offered in the last campaign so from above it looks like most customer not accept Personal Loan(i.e.,frequency of customer not accept Personal Loan is high) and this is our Target Variable because our objective is to predict the probability that a customer will accept a personal loan or not.\n\n->As per the data provided Mortgage attribute means Value of house mortgage if any.($000) so from above we can see that there is a rigt-skewness in Mortgage column because long tail is at right side(Mean>median) and for more than 50% customer value of Mortgage is 0\n\n->As per the data provided Education attribute means Education Level. 1: Undergrad; 2: Graduate; 3: Advanced\/Professional so from above it looks like most customers are undergraduate after that Advanced\/Professional and last Graduate.\n\n->As per the data provided CCAvg attribute means Avg. spending on credit cards per month(in thousnad dollar)\nso from above we can see that there is a right\u2212skewness in CCAvg column because long tail is at right side(Mean>median) and maximum Avg. spending on credit card per month is 10000.\n\n->As per the data provided Family attribute means Family size of the customer so from above it looks like most customer are whose family size is 1 and least customers are those whose family size is 3.\n\n->As per the data provided ZIP code attribute means Home Address ZIP code.\n\n->As per the data provided Income attribute means Annual income of the customer(in thousand dollar)\nso from above we can see that there is a right\u2212skewness in Income column because long tail is at right side (Mean>median) and maximum Income is 224000\n\n->As per the data provided Experience attribute means #years of professional experience so from above we can see that Experience is quite normally distributed but we have experience in negative(-) also. and maximum experience is of 43 years.\n\n->As per the data provided Age attribute means Customer's age in completed years so from above we can see that Age is quite normally distributed and maximum age of customer is of 67 years and minimum age of customer is 23 years.\n\n->As per the data provided Id attribute means Customer ID it's looks like it is just a serial number(1 to 5000)","76094e55":"so from above we can see that we have Experience in negative which is not feasible so we will fix that one may be it's a (-) sign by mistake.\nso we will make new data frame and make changes in that dataframe.","4ef796cc":"From above pair plot it looks like whose customer income is high they more likely to accept Personal Loan","0b591f31":"**Objective::**\nour objective is to predict the probability that a customer will accept a personal loan or not.","87aa634b":"so in our test dataset total 1500 entities\/customers are there. so (1228+123)=1351, means 1351 customers out of 1500 customers in real not accept personal loan but our model predict 1228\/1351 not accept personal loan and for 123 customers it did wrong prediction. likewise, (65+84)=149, means 149 customers out of 1500 customers accept personal loan but our model predict 84\/149 accept personal loan and for 65 customers it did wrong prediction.","bf755a4f":"so from above we can see that there are 0 null values in each column.","d919a379":"so in our test dataset total 1500 entities\/customers are there. so (1306+45)=1351, means 1351 customers out of 1500 customers in real not accept personal loan but our model predict 1306\/1351 not accept personal loan and for 45 customers it did wrong prediction. likewise, (94+55)=149, means 149 customers out of 1500 customers accept personal loan but our model predict 55\/149 accept personal loan and for 94 customers it did wrong prediction","df876fc5":"so from above we can see that 90.4% customer not accept personal loan. so we dont have proper distribution of target column means majority of customers not accept personal loan. This means that without any model building if i say for any random customer that it will not accept personal loan than i am 90% true in claiming that thing.\n\nso our model cannot predict very well that it will accept personal loan.","572249aa":"#Personal Loan Prediction <a><\/a>\n\n1. Overview\n2. Importing Modules and Exploratory Data Analysis(EDA) \n3. Logistic Regression\n4. K-Nearest Neighbour\n5. Naive Bayes'\n6. Support Vector Machine\n7. Optimization","b2f4bf4e":"#Logistic Regression <a><\/a>","4af41afc":"so in our test dataset total 1500 entities\/customers are there. so (1350+1)=1351, means 1351 customers out of 1500 customers in real not accept personal loan but our model predict 1350\/1351 not accept personal loan and for 1 customer it did wrong prediction. likewise, (142+7)=149, means 149 customers out of 1500 customers accept personal loan but our model predict 7\/149 accept personal loan and for 142 customers it did wrong prediction.","0cade821":"Education and Family can be sorted based on educational level and family size respectively so no need to apply one-hot encoding. ","75962179":"#K-Nearest Neighbour <a><\/a>","a9646e2e":"so from above we can see that only 480 customer accept personal loan and 4520 customer not accept personal loan.","6b175585":"In logistic regression we can change the threshold and check what is the accuracy","7e39a2fd":"#Optimization <a><\/a>","ad4551f5":"so now we have new dataframe called new_bank_df","9f379c05":"confusion matrix is a square matrix which will help us to know the class level accuracy so in our test dataset total 1500 entities\/customers are there. so (1334+17)=1351, means 1351 customers out of 1500 customers in real not accept personal loan but our model predict 1334\/1351 not accept personal loan and for 17 customers it did wrong prediction. likewise, (65+84)=149, means 149 customers out of 1500 customers accept personal loan but our model predict 84\/149 accept personal loan and for 65 customers it did wrong prediction.","fbc619b8":"**SPLITTING OF DATA INTO TRAINING AND TEST SET WITH 70:30 ratio**","9ae5e5a7":"#Support Vector Machine <a><\/a>","3a7e1c0f":"so from above we can see that ID and Zip code is no more required in model building. so we can remove these two feature columns.","ecc4fc9a":"so from above we can see that at threshold 0.5 we have maximum accuracy score(0.94533)","4752241a":"#Naive Bayes' <a><\/a>","57148dbd":"These are the basic model of logistic regression,knn,naive bayes',support vector machine and if we will see from above than we can conclude that Logistic Regression performs well among all. now we will see how we can optimize thses models"}}