{"cell_type":{"c57f99e1":"code","bea18466":"code","afc63241":"code","db41e877":"code","f7860d38":"code","40179a66":"code","6596ae50":"code","ca630f47":"code","b32e5139":"code","65d2d42c":"code","b70ab9de":"code","963ec3d7":"code","5a362d31":"code","bc4fa036":"code","574c346d":"code","58d7bea8":"code","9aaf3edb":"code","def1d060":"code","b0b31e70":"code","b60bd6ce":"code","369afe75":"code","3cf80085":"code","ca5f746b":"code","0010b5a9":"code","71abc97d":"code","07a9a7d4":"code","f44d91b7":"code","d304b1b4":"code","57281bfb":"code","9f7b6b70":"code","9f710977":"code","81f34fc9":"code","a9ca3c21":"code","190dd086":"code","b0adab5d":"code","a6ebda08":"markdown","49097880":"markdown","074b204b":"markdown","14b19304":"markdown","04b32844":"markdown","e6df2f95":"markdown","d3c0863f":"markdown","bbec69a4":"markdown","db66d21b":"markdown","dc9366a7":"markdown","a18f002e":"markdown","fd776ca1":"markdown","239877b4":"markdown","cc73df1e":"markdown","7e8512c0":"markdown","fe901a8e":"markdown","3c37c5e9":"markdown","d5287b3b":"markdown","0171d9e2":"markdown","9a8f26c7":"markdown","3c183407":"markdown"},"source":{"c57f99e1":"!pip install strsimpy pytorch-lightning\n\n!cp ..\/input\/payment-detection\/utils.py .\/utils.py\n\n#!pip install --upgrade torchvision\n\n#!pip install --upgrade torchtext\n\nimport torch\nimport pandas as pd\n\n\nfrom PIL import Image\nimport cv2\nimport gc\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Adagrad, SGD\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import OneCycleLR, StepLR, ReduceLROnPlateau\n\nimport random\n\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\n\n\nimport torchvision\nimport pickle\nimport json\n\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\nimport pytorch_lightning as pl\n\nfrom itertools import chain\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\n\nfrom string import digits, ascii_uppercase\nfrom collections import defaultdict\n\nimport math \nimport utils\n\nfrom strsimpy.levenshtein import Levenshtein","bea18466":"torch.cuda.is_available()","afc63241":"SEED = 1489\n\n\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)","db41e877":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","f7860d38":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\n\n#\u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c\nTEST_PATH = \"..\/input\/payment-detection\/test\/test\" \nTRAIN_PATH = \"..\/input\/payment-detection\/train\/train\"\nSUBMISSION_PATH = \"..\/input\/payment-detection\/submission.csv\"\nTRAIN_INFO = \"..\/input\/payment-detection\/train.csv\"\n#\u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\u044b \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430\nOS_MANY = 1\nOS_UY = 3\nOS_EX = 2\nOS_ST = 2\nOS_PC = 0\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435\nTHRESHOLD = 0.7\nHOT_THRESHOLD = 0.975 #0.7#0.7#0.6\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435 \u0432 \u0441\u043b\u0443\u0447\u0430\u0435 \u043f\u043e\u043b\u043d\u043e\u0433\u043e \u043e\u0442\u0441\u0443\u0442\u0441\u0442\u0432\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\nEMPTY_STRING_THRESHOLD = 0.5\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nIMAGE_WIDTH = 412#800#800#412  \nIMAGE_HEIGHT = 412#800#800#412  \n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nVAL_SIZE = 0.2\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nN_ITER = 12\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nBATCH_SIZE = 8\nBATCH_SIZE_VAL = 8\n\n\nLR = 1e-4\ncpu_device = torch.device(\"cpu\")","40179a66":"train_df  = pd.read_csv(TRAIN_INFO)","6596ae50":"test_df = pd.read_csv(SUBMISSION_PATH)","ca630f47":"valid_images = np.random.choice(train_df.image.unique(), size=int(VAL_SIZE * train_df.image.nunique()), replace=False)\n\nvalid_set = train_df[train_df.image.isin(valid_images)]\n\ntrain_set = train_df[~train_df.image.isin(valid_images)]","b32e5139":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')\n  \ndef get_dict(df):\n    train_dict = defaultdict(list)\n    for i, row in df.iterrows():\n        train_dict[row.image].append([int(row.x1 *(IMAGE_WIDTH\/ row.width)), #xmin\n                                     int(row.y1 * (IMAGE_HEIGHT\/ row.height)),#ymin\n                                     int((row.x1 + row.w)*(IMAGE_WIDTH\/ row.width)), #xmax\n                                     int((row.y1 + row.h)*(IMAGE_HEIGHT\/ row.height)), #ymax\n                                     row.label]) #label\n    for k in train_dict.keys():\n        train_dict[k].sort(key = lambda x: (x[0], x[1]))\n    return train_dict","65d2d42c":"train_dict = get_dict(train_set)\nval_dict = get_dict(valid_set)","b70ab9de":"#\u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0438\u0437\u043d\u0430\u0447\u0430\u043b\u044c\u043d\u043e\u0435 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445\ndict_ph = []\nMa = 0\nEx = 0\nPc = 0\nUy = 0\nVi = 0\nSt = 0\nfor photo in train_dict.keys():\n    for card in train_dict[photo]:\n        if card[4] == 'MA':\n            Ma += 1\n        if card[4] == 'EX':\n            Ex += 1\n        if card[4] == 'PC':\n            Pc += 1\n        if card[4] == 'UY':\n            Uy += 1\n        if card[4] == 'VI':\n            Vi += 1\n        if card[4] == 'ST':\n            St += 1\nprint('MA -',Ma,',EX -', Ex,',PC -', Pc,',UY -', Uy,',VI -', Vi, ',ST -',St)","963ec3d7":"#\u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043c \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043f\u043e \u043f\u0440\u0438\u043d\u0446\u0438\u043f\u0443 \u043f\u0440\u0438\u043c\u0435\u0440\u043e\u0432 \u0441 \u0431\u043e\u043b\u044c\u0448\u0438\u043c \u0447\u0438\u0441\u043b\u043e\u043c \u043a\u0430\u0440\u0442\ntrain_dat = [item for item in train_dict.items()]\nfor i in range(len(train_dat)):\n    train_dat[i] = list(train_dat[i])\n\nfor oversampling_iteration in range(OS_MANY):\n    num = 0\n    for i in range(len(train_dat)):\n        vow = train_dat[i]\n        if len(vow[1]) >= 7:\n            vow[0] = vow[0]+'_'+str(num)\n            train_dat.append(vow)\n            num+=1\n    new_strings = []\n    for i in range(len(train_dat)):\n        if '_' in train_dat[i][0]:\n            new_strings.append(train_dat[i])\n    for elem in new_strings:\n        train_dict.update({elem[0]:elem[1]})","5a362d31":"#\u043f\u0440\u043e\u0432\u0435\u0434\u0451\u043c \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043f\u043e \u0440\u0435\u0434\u043a\u0438\u043c \u043a\u0430\u0440\u0442\u0430\u043c \u0441\u0440\u0435\u0434\u0438 \u0432\u0441\u0435\u0433\u043e \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430.\nfor oversampling_iteration in range(OS_UY):\n    num = 0\n    for i in range(len(train_dat)):\n        vow = train_dat[i]\n        for card in vow[1]:\n            if card[4] == 'UY':\n                vow[0] = vow[0]+'_'+str(num)\n                train_dat.append(vow)\n                num+=1\n                break\n    new_strings = []\n    for i in range(len(train_dat)):\n        if '_' in train_dat[i][0]:\n          #  print(train_dat[i][0])\n            new_strings.append(train_dat[i])\n    for elem in new_strings:\n        train_dict.update({elem[0]:elem[1]})\n        \nfor oversampling_iteration in range(OS_EX):\n    for i in range(len(train_dat)):\n        vow = train_dat[i]\n        for card in vow[1]:\n            if card[4] == 'EX':\n                vow[0] = vow[0]+'_'+str(num)\n                train_dat.append(vow)\n                num += 1\n                break\n    new_strings = []\n    for i in range(len(train_dat)):\n        if '_' in train_dat[i][0]:\n          #  print(train_dat[i][0])\n            new_strings.append(train_dat[i])\n    for elem in new_strings:\n        train_dict.update({elem[0]:elem[1]})\nfor oversampling_iteration in range(OS_ST):\n    for i in range(len(train_dat)):\n        vow = train_dat[i]\n        for card in vow[1]:\n            if card[4] == 'ST':\n                vow[0] = vow[0]+'_'+str(num)\n                train_dat.append(vow)\n                num += 1\n                break\n    new_strings = []\n    for i in range(len(train_dat)):\n        if '_' in train_dat[i][0]:\n          #  print(train_dat[i][0])\n            new_strings.append(train_dat[i])\n    for elem in new_strings:\n        train_dict.update({elem[0]:elem[1]})\nfor oversampling_iteration in range(OS_PC):\n    num = 0\n    for i in range(len(train_dat)):\n        vow = train_dat[i]\n        for card in vow[1]:\n            if card[4] == 'PC':\n                vow[0] = vow[0]+'_'+str(num)\n                train_dat.append(vow)\n                num+=1\n                break\n    new_strings = []\n    for i in range(len(train_dat)):\n        if '_' in train_dat[i][0]:\n          #  print(train_dat[i][0])\n            new_strings.append(train_dat[i])\n    for elem in new_strings:\n        train_dict.update({elem[0]:elem[1]})","bc4fa036":"#\u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0432 \u0434\u0430\u043d\u043d\u044b\u0445 \u043f\u043e\u0441\u043b\u0435 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u0438 \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430\ndict_ph = []\nMa = 0\nEx = 0\nPc = 0\nUy = 0\nVi = 0\nSt = 0\nfor photo in train_dict.keys():\n    for card in train_dict[photo]:\n        if card[4] == 'MA':\n            Ma += 1\n        if card[4] == 'EX':\n            Ex += 1\n        if card[4] == 'PC':\n            Pc += 1\n        if card[4] == 'UY':\n            Uy += 1\n        if card[4] == 'VI':\n            Vi += 1\n        if card[4] == 'ST':\n            St += 1\nprint('MA -',Ma,',EX -', Ex,',PC -', Pc,',UY -', Uy,',VI -', Vi, ',ST -',St)","574c346d":"\"\"\"\nhashes = []\ntrain_df_ = train_df.drop_duplicates(subset = ['image'])\nfor i in tqdm.tqdm(range(len(train_df_))):\n  image_name = train_df_.iloc[i].image\n  img = Image.open(os.path.join(TRAIN_PATH, image_name))\n  hashes.append(imagehash.phash(img).hash.reshape(64))\n\nhashes = torch.Tensor(np.array(hashes).astype(int)).cuda()\nsims = np.array([(hashes[i] == hashes).sum(dim=1).cpu().numpy()\/64 for i in range(hashes.shape[0])])\ntrain_df_['image_'] = train_df_['image'].apply(lambda x: os.path.join(TRAIN_PATH, x))\n\nthreshold = 0.92\npaths = train_df_.image_.values\nduplicates = np.where((sims > threshold))\n\n\ntrain_df_['duplicates'] = ''\nfor i in tqdm.tqdm(range(len(sims))):\n  for j in range(len(sims)):\n    if i == j:\n        continue\n    if sims[i][j] > threshold:\n      train_df_['duplicates'].iloc[i] = f'duplicate_{i}'\n      train_df_['duplicates'].iloc[j] = f'duplicate_{i}'\na = train_df_.drop_duplicates(subset = ['duplicates']).query(\"duplicates != ''\")\nb = train_df_.query(\"duplicates == ''\")\ndata_without = pd.concat([a, b])\ntrain_df = train_df[train_df.image.isin(data_without.image)]\n\"\"\"","58d7bea8":"import albumentations as A\n\n#\u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0438 \u0438 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438\nclass ShapeDataset(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(), \n                                               \n                                               \n                                               #Pad(10, fill=0, padding_mode='constant'),\n                                               #RandomApply([RandomRotation(90)], p=0.5),\n                                               #random_rotation(20-30 degrees) apply 40%\n                                               \n                                               \n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        \n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n\n    def num_classes(self):\n        return 7\n\n    def __len__(self, ):\n        return len(self.data.keys())\n    \n    def __getitem__(self, idx):\n              \n        def get_boxes(obj):\n            boxes = []\n            for row in obj:\n                boxes.append(row[:4])\n            return torch.as_tensor(boxes, dtype=torch.float)\n\n        def get_areas(obj):\n            areas = []\n            for row in obj:\n                areas.append((row[2] - row[0]) * (row[3] - row[1]))\n            return torch.as_tensor(areas, dtype=torch.int64)\n        \n        def get_class(obj):\n            \"\"\"\n            \u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0447\u0438\u0441\u043b\u043e\u043c\n            \"\"\"\n            cards = {\n                'VI':1,\n                'MA':2,\n                'EX':3, \n                'PC':4, \n                'ST':5,\n                'UY':6\n                    }\n            classes = []\n            for row in obj:\n                classes.append(cards[row[4]])\n            return torch.as_tensor(classes, dtype=torch.int64)\n        \n        img_name = list(self.data.keys())[idx]\n        \n        #\u043a\u043e\u0434 \u0432 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u0442\u0440\u043e\u043a \u043d\u0438\u0436\u0435 \u043d\u0443\u0436\u0435\u043d \u0434\u043b\u044f \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430\n        \n        check = img_name.find('_')   #  \u041d\u0430\u0445\u043e\u0434\u0438\u0442 \u043d\u0430 \u043a\u0430\u043a\u043e\u043c \u043c\u0435\u0441\u0442\u0435 \u0432\u043f\u0435\u0440\u0432\u044b\u0435 \u0432\u0441\u0442\u0440\u0435\u0447\u0430\u0435\u0442\u0441\u044f '_'\n        \n        if check != -1:   \n            img_name_for_photo = img_name[0 : (check)]\n        else:\n            img_name_for_photo = img_name\n            \n        path = os.path.join(self.IMAGE_DIR, img_name_for_photo)\n\n        \n    \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n        \n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n            \n        obj = self.data[img_name]\n        \n        #\u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0438 \u043e\u0442 albumentations \u0441 \u0443\u0447\u0451\u0442\u043e\u043c \u0441\u043c\u0435\u0449\u0435\u043d\u0438\u0439 bounding box\u043e\u0432\n        transforms_a = A.Compose([\n                                               A.HorizontalFlip(p=0.5),\n                                               A.VerticalFlip(p=0.5),\n                                               A.RandomBrightness(limit=0.1, always_apply=False, p=0.5)\n                                               #A.RandomBrightnessContrast(p=0.2),\n                                               ], bbox_params=A.BboxParams(format='pascal_voc'\n                                                                        ))\n       \n        image = transforms_a(image=img, bboxes=self.data[img_name])\n        \n        obj = image['bboxes']\n        \n        img = image['image']\n        \n        target = {}\n        target['boxes'] = get_boxes(obj)\n        target['labels'] = get_class(obj)\n        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n        target['area'] = get_areas(obj)\n        target['iscrowd'] = torch.zeros((len(obj),), dtype=torch.int64)\n        \n        if self.transform:\n            image = self.transform(img)\n        \n\n        return image, target\n\n#\u0434\u0430\u0442\u0430\u0441\u0435\u0442 \u0434\u043b\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nclass ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(),\n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               ) ])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return 7\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n        \n        \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n            \n        return image \n","9aaf3edb":"#\u0438\u043d\u0438\u0446\u0438\u0430\u0446\u0438\u044f \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u043e\u0432\ntrain_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_dict)\nvalid_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, val_dict)\ntest_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test_df)","def1d060":"#\u0438\u043d\u0438\u0446\u0438\u0430\u0446\u0438\u044f \u0437\u0430\u0433\u0440\u0443\u0437\u0447\u0438\u043a\u043e\u0432 \u0434\u0430\u043d\u043d\u044b\u0445\ndataloader_train = DataLoader(\n    train_data, batch_size=BATCH_SIZE,\n    shuffle=True, num_workers=0, collate_fn=collate_fn)\ndataloader_valid = DataLoader(\n    valid_data, batch_size=BATCH_SIZE_VAL, \n    shuffle=False, num_workers=0, collate_fn=collate_fn)","b0b31e70":"def imshow(inp, title=None, plt_ax=plt, default=False):\n    \"\"\"Imshow \u0434\u043b\u044f \u0442\u0435\u043d\u0437\u043e\u0440\u043e\u0432\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt_ax.imshow(inp)\n    if title is not None:\n        plt_ax.set_title(title)\n    plt_ax.grid(False)","b60bd6ce":"#\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f bounding box. \u041f\u0440\u044f\u043c\u043e\u0443\u0433\u043e\u043b\u044c\u043d\u0438\u043a \u0441 \u0434\u0430\u043d\u043d\u044b\u043c\u0438 \u043f\u0440\u0430\u0432\u043e\u0439 \u043d\u0438\u0436\u043d\u0435\u0439 \u0438 \u043b\u0435\u0432\u043e\u0439 \u0432\u0435\u0440\u0445\u043d\u0435 \u0442\u043e\u0447\u043a\u043e\u0439 \u0438 \u0435\u0441\u0442\u044c \u043d\u0430\u0448 BB\nfas = train_data[544]\nx1, y1, x2, y2 = fas[1]['boxes'][0][0],fas[1]['boxes'][0][1],fas[1]['boxes'][0][2],fas[1]['boxes'][0][3]\n#plt.rectangle((x1,y1), (x2,y2))\nplt.scatter((x1,y1), (x2,y2))\n#plt.scatter((x1,y2), (x2,y1))\n\ninp = fas[0].numpy().transpose((1, 2, 0))\n\n\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\n\n\ninp = std * inp + mean\ninp = np.clip(inp, 0, 1)\nplt.imshow(inp)\n","369afe75":"#\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,1000))\n    \n    im_val = train_data[random_characters][0]\n    \n    card = train_data[random_characters][1]['labels'].numpy()\n    \n    cards = {\n                'VISA':1,\n                'MASTER CARD':2,\n                'AMERICAN EXPRESS':3, \n                'MIR':4, \n                'MAESTRO':5,\n                'UNION PAY':6\n                    }\n    \n    def get_key(d, value):\n        res = []\n        for k, v in d.items():\n            for value in card:\n                if v == value:\n                    res.append(k)\n        return(res)\n\n    items = get_key(cards, card[0])\n    \n    \n    img_label = \" \".join(map(lambda x: x.capitalize(),\\\n                items))\n    \n    imshow(im_val.data.cpu(), title=img_label, plt_ax=fig_x)","3cf80085":"#\u0434\u0435\u043c\u043e\u043d\u0441\u0442\u0440\u0430\u0446\u0438\u044f \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445\nfig, ax = plt.subplots(nrows=3, ncols=3,figsize=(12, 12), \\\n                        sharey=True, sharex=True)\nfor fig_x in ax.flatten():\n    random_characters = int(np.random.uniform(0,100))\n    \n    im_val = test_data[random_characters]\n    \n    \n    imshow(im_val.data.cpu(), plt_ax=fig_x)","ca5f746b":"device = get_device()","0010b5a9":"import torchvision\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\n\n# load a pre-trained model for classification and return\n# only the features\n\n\nbackbone = torchvision.models.densenet169(pretrained=True).features # 169 \u0431\u044b\u043b\u043e\n#print(len(list(backbone.children())))   #169 \u0431\u044b\u043b\u043e\n#for p in nn.Sequential(*list(backbone.children())[:7]).parameters():\n\n\n\n# FasterRCNN needs to know the number of\n# output channels in a backbone. For mobilenet_v2, it's 1280\n# so we need to add it here\nbackbone.out_channels = 1664 #1024 - 121 #2208 -161\n\n# let's make the RPN generate 5 x 3 anchors per spatial\n# location, with 5 different sizes and 3 different aspect\n# ratios. We have a Tuple[Tuple[int]] because each feature\n# map could potentially have different sizes and\n# aspect ratios\nanchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),),\n                                   aspect_ratios=((0.5, 1.0, 2.0),))\n\n# let's define what are the feature maps that we will\n# use to perform the region of interest cropping, as well as\n# the size of the crop after rescaling.\n# if your backbone returns a Tensor, featmap_names is expected to\n# be [0]. More generally, the backbone should return an\n# OrderedDict[Tensor], and in featmap_names you can choose which\n# feature maps to use.\nroi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=['0'],\n                                                output_size=7,\n                                                sampling_ratio=2)\n\n# put the pieces together inside a FasterRCNN model\nmodel_densenet = FasterRCNN(backbone,\n                   num_classes=7,\n                   rpn_anchor_generator=anchor_generator,\n                   box_roi_pool=roi_pooler)\n\nmodel_densenet = model_densenet.to(device)","71abc97d":"def lev_dist(preds, df_dict):\n    \n    \n    def c_sort(sub_li):\n        \"\"\"\n        \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0431\u043e\u043a\u0441\u043e\u0432 \u043f\u043e \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c. \u0421\u043b\u0435\u0432\u0430 \u043d\u0430\u043f\u0440\u0430\u0432\u043e \u0441\u0432\u0435\u0440\u0445\u0443 \u0432\u043d\u0438\u0437.\n        \"\"\"\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n        \n        \"\"\"\n        \u0414\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430.\n        \"\"\"\n        cards = {\n                1:'VI',\n                2:'MA',\n                3:'EX', \n                4:'PC', \n                5:'ST',\n                6:'UY'\n                    }\n        return cards[number]\n    \n    levenshtein = Levenshtein()     \n            \n    imgs = defaultdict(list)\n    \n    \n    for key in df_dict.keys():\n        \n        #\u0438\u0434\u0451\u043c \u043f\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        imgs[key] += df_dict[key]\n            \n            \n    labels = {}\n    \n    for i in imgs.keys():\n        \n        #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443\n        \n        labels[i] = \" \".join([j[4] for j in imgs[i]])\n    \n    preds_list = []\n    \n    cnt = 0\n    \n    labels_pred = {}\n    labels_pred_HOT = {}\n    imgs_name = list(df_dict.keys())\n    for i in preds:\n        for j in i:\n            #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b\n            _temp_boxes = i[j]['boxes'].cpu().detach().numpy().tolist()\n            _temp_label = i[j]['labels'].cpu().detach().numpy().tolist()\n            _temp_confidence = i[j]['scores'].cpu().detach().numpy().tolist()\n               \n            for index, _ in enumerate(_temp_boxes):\n                # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430\n                _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n                _temp_boxes[index].append(_temp_confidence[index])\n                \n                \n            _temp_boxes = c_sort(_temp_boxes)\n\n            # \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u044b\u0448\u0435 THRESHOLD\n            \n            labels_pred[imgs_name[j]] = \" \".join([k[4] for k in _temp_boxes if k[5] > THRESHOLD])\n            \n            labels_pred_HOT[imgs_name[j]] = \" \".join([k[4] for k in _temp_boxes if k[5] > HOT_THRESHOLD])\n            \n            cnt+=1\n\n    assert len(labels) == len(labels_pred)\n    assert len(labels) == len(labels_pred_HOT)\n    \n    lev_dist = 0\n    lev_dist_HOT = 0\n    \n    for i in imgs.keys():\n        # \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        \n        lev_dist += levenshtein.distance(labels[i], labels_pred[i])\n        lev_dist_HOT += levenshtein.distance(labels[i], labels_pred_HOT[i])\n        \n    print(\"Levenstein distance {0}\".format(lev_dist\/len(imgs)))\n    print(\"Levenstein distance HOT {0}\".format(lev_dist_HOT\/len(imgs)))\n    \n    return lev_dist\/len(imgs)","07a9a7d4":"train_losses = []\ntest_losses = []\nminimum_dist = 5\nminimum_val_loss = 100","f44d91b7":"\nclass LitModule(pl.LightningModule):\n    \n  def __init__(self, model, hparams, minimum_dist, minimum_val_loss):\n    super().__init__()\n    self.model = model\n    self.hparams = hparams\n    self.minimum_dist = minimum_dist\n    self.minimum_val_loss = minimum_val_loss\n  def forward(self, batch):\n    \n    images, targets = batch\n    \n    images = list(image.to(device) for image in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    loss_dict = self.model(images, targets)\n    return loss_dict\n\n  def training_step(self, batch, batch_nb):\n    loss_dict = utils.reduce_dict(self(batch))\n    loss = sum([v for v in loss_dict.values()])\/4 \n    \n    self.log('loss_classifier', loss_dict['loss_classifier'], on_step=True, on_epoch=True, prog_bar=True,)\n    self.log('loss_box_reg', loss_dict['loss_box_reg'], on_step=True, on_epoch=True, prog_bar=True,)\n    self.log('loss_objectness', loss_dict['loss_objectness'], on_step=True, on_epoch=True, prog_bar=True,)\n    self.log('loss_rpn_box_reg', loss_dict['loss_rpn_box_reg'], on_step=True, on_epoch=True, prog_bar=True,)\n    self.log('loss', loss, on_step=True, on_epoch=True, prog_bar=True,)\n    return loss\n\n  def validation_step(self, batch, batch_nb):\n    images, targets = batch\n    images = list(img.to(device) for img in images)\n    targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n    \n    self.eval()\n    outputs = self.model(images)\n    outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n    res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n    \n    self.train()\n    loss_dict = utils.reduce_dict(self.forward(batch))\n    loss = sum([v for v in loss_dict.values()])\/4\n    \n    return res, loss\n\n\n  def validation_epoch_end(self, validation_step_outputs):\n    dist = lev_dist([output[0] for output in validation_step_outputs], val_dict)\n    if dist <= self.minimum_dist:\n        torch.save(self.model.state_dict(), 'best_model_levenstein')\n        self.minimum_dist = dist\n    self.log('levenstein', dist, on_epoch=True)\n    \n    val_loss = torch.stack([x[1] for x in validation_step_outputs]).mean()\n    if val_loss <= self.minimum_val_loss:\n        torch.save(self.model.state_dict(), 'best_model_loss')\n        self.minimum_val_loss = val_loss\n    self.log('val_loss', val_loss, on_epoch=True, prog_bar=True)\n    \n    \n    return dist\n\n  def configure_optimizers(self):\n    optimizer = torch.optim.Adam(self.model.parameters(), lr = self.hparams.lr, weight_decay= self.hparams.weight_decay)\n    scheduler = {\"scheduler\": ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2),\n                 \"monitor\": \"loss\",\n                 \"name\": \"reduce_lr\",\n                  }\n    return [optimizer], [scheduler]","d304b1b4":"from pytorch_lightning import seed_everything\nfrom argparse import Namespace\nfrom pytorch_lightning.callbacks import ModelCheckpoint\nfrom pytorch_lightning.plugins import DDPPlugin\nfrom pytorch_lightning.callbacks import LearningRateMonitor\n\n\nseed_everything(42)\nargs = {\n    'lr': 1e-4,\n    'weight_decay': 1e-4,\n}\n\n\nhparams = Namespace(**args)","57281bfb":"torch.cuda.empty_cache()","9f7b6b70":"params = [p for p in model_densenet.parameters() if p.requires_grad]\n\nsegm_model = LitModule(model_densenet , hparams, minimum_dist, minimum_val_loss)\n\nsegm_model.cuda()\n\n\n\n#N_ITER\ntrainer = pl.Trainer(gpus=1, max_epochs=N_ITER, num_sanity_val_steps=0, precision=16)\ntrainer.fit(segm_model, dataloader_train, dataloader_valid)","9f710977":"torch.save(model_densenet.state_dict(), 'densenet_epoch_last')","81f34fc9":"model_densenet.load_state_dict(torch.load('densenet_epoch_last'))","a9ca3c21":"def get_prediction(dataset,  model):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    \n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = []\n    scores = []\n     #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n        \n        labels_pred = []\n        names_pred = []\n        \n    imgs_name = test_df.image.unique().tolist()\n\n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for ind, i in enumerate(preds):\n        \n        _temp_boxes = i[0]['boxes'].cpu().detach().numpy().tolist()\n        _temp_label = i[0]['labels'].cpu().detach().numpy().tolist()\n        _temp_confidence = i[0]['scores'].cpu().detach().numpy().tolist()\n\n        for index, _ in enumerate(_temp_boxes):\n            _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n            _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(imgs_name[ind])\n        scores.append([\" \".join(str([j[4] for j in _temp_boxes])),[j[5] for j in _temp_boxes]])\n        labels_pred.append( \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD]))\n             \n    d = {'image': names_pred, 'payment': labels_pred}\n    \n    t = {'payment-score': scores}\n    df = pd.DataFrame(data=d)\n    t_df = pd.DataFrame(data=t)\n        \n    return df , t_df","190dd086":"metric_logger = utils.MetricLogger(delimiter=\"  \")\n\nheader = \"Test:\"\n\n\ndataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\nmodel_densenet.cuda()\n\ng_predictions = get_prediction(dataloader_test, model_densenet)\npredictions = g_predictions[0]\nscores = g_predictions[1]","b0adab5d":"predictions.to_csv(\"submission_07.csv\", index=None)","a6ebda08":"\u0427\u0430\u0441\u0442\u0438\u0447\u043d\u043e \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430\nhttps:\/\/www.kaggle.com\/kaggleuser983\/notebook3c64cdafe5","49097880":"## \u0423\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u0438 \u0438\u043c\u043f\u043e\u0440\u0442 \u043d\u0435\u043e\u0431\u0445\u043e\u0434\u0438\u043c\u044b\u0445 \u0431\u0438\u0431\u043b\u0438\u043e\u0442\u0435\u043a","074b204b":"## \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f","14b19304":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 \u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","04b32844":"\u0418\u0437 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439 \u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b \u043f\u043e\u0447\u0442\u0438 \u0432\u0441\u0451, \u0447\u0442\u043e \u0441\u0443\u043c\u0435\u043b \u043d\u0430\u0439\u0442\u0438. \u041f\u0440\u043e\u0431\u043e\u0432\u0430\u043b \u043f\u043e\u0432\u043e\u0440\u043e\u0442\u044b \u0441 \u043f\u0430\u0434\u0434\u0438\u043d\u0433\u043e\u043c - \u043d\u0435 \u0437\u0430\u0448\u043b\u0438, \u0432\u0430\u0440\u0438\u0430\u0446\u0438\u0438 \u0441 \u044f\u0440\u043a\u043e\u0441\u0442\u044c\u044e \u0438 \u043a\u043e\u043d\u0442\u0440\u0430\u0441\u0442\u043e\u043c \u0437\u0430\u0445\u043e\u0434\u0438\u043b\u0438 \u0447\u0435\u0440\u0435\u0437 \u0440\u0430\u0437, \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 random brightness. \u0414\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0445\u043e\u0440\u043e\u0448\u0438 \u043e\u043a\u0430\u0437\u0430\u043b\u0438\u0441\u044c \u0444\u043b\u0438\u043f\u044b, \u043d\u043e \u0441 \u043d\u0438\u043c\u0438 \u0432\u0430\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0441\u043e\u0445\u0440\u0430\u043d\u0438\u0442\u044c \u0432\u0438\u0434 bounding box \u043f\u043e\u0441\u043b\u0435 \u043f\u0435\u0440\u0435\u0432\u043e\u0440\u043e\u0442\u043e\u0432, \u0432\u043e\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0441\u044f albumentations.","e6df2f95":"\u041f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u043b\u0441\u044f Pytorch Lightning. \u0421\u043e\u0445\u0440\u0430\u043d\u044f\u043b \u0432\u0435\u0441\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439, \u043b\u0443\u0447\u0448\u0438\u0445 \u043f\u043e \u043b\u043e\u0441\u0441\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0438 \u043f\u043e levenshtein \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438.","d3c0863f":"\u0412 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u0438\u0442\u043e\u0433\u0435 \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433 \u043c\u043d\u0435 \u043d\u0435 \u043e\u0447\u0435\u043d\u044c \u0441\u0438\u043b\u044c\u043d\u043e \u043f\u043e\u043c\u043e\u0433, \u043d\u043e \u0432\u0441\u0451 \u0436\u0435 \u043d\u0435\u0441\u043a\u043e\u043b\u044c\u043a\u043e \u0441\u043e\u0442\u044b\u0445 \u0443 \u043c\u0435\u043d\u044f \u043e\u043d \u043f\u043e\u043c\u043e\u0433\u0430\u043b \u0441\u044a\u0435\u0441\u0442\u044c \u043d\u0430 LB. \u0412\u0430\u0436\u043d\u0435\u0435 \u0431\u044b\u043b\u043e \u0441 \u043d\u0438\u043c \u043d\u0435 \u043f\u0435\u0440\u0435\u0431\u043e\u0440\u0449\u0438\u0442\u044c, \u0438\u0431\u043e \u044d\u0444\u0444\u0435\u043a\u0442 \u043e\u0442 \u0441\u043b\u0438\u0448\u043a\u043e\u043c \u0431\u043e\u043b\u044c\u0448\u043e\u0433\u043e \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430 \u043e\u0447\u0435\u043d\u044c \u0437\u0430\u043c\u0435\u0442\u0435\u043d \u0438 \u043d\u0435\u043f\u0440\u0438\u044f\u0442\u0435\u043d \u043e\u043a\u0430\u0437\u0430\u043b\u0441\u044f, \u043a\u0430\u043a \u044f \u0443\u0431\u0435\u0436\u0434\u0430\u043b\u0441\u044f.","bbec69a4":"## OVERSAMPLING. \u0414\u0443\u0431\u043b\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u043d\u043d\u044b\u0445 \u0441 \u0440\u0435\u0434\u043a\u0438\u043c\u0438 \u0442\u0430\u0440\u0433\u0435\u0442\u043d\u044b\u043c\u0438 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f\u043c\u0438","db66d21b":"## \u0421\u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430. \u041f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u0438\u0435 \u0430\u0443\u0433\u043c\u0435\u043d\u0442\u0430\u0446\u0438\u0439.","dc9366a7":"\u0415\u0449\u0451 \u044f \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b, \u0447\u0442\u043e \u043d\u0430 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0447\u0435\u043d\u044c \u0433\u0440\u044f\u0437\u043d\u0430\u044f \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0430, \u0430 \u0442\u0430\u043a\u0436\u0435 \u0434\u043e\u0441\u0442\u0430\u0442\u043e\u0447\u043d\u043e \u043c\u043d\u043e\u0433\u043e \u0444\u043e\u0442\u043e-\u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432, \u0443 \u043c\u0435\u043d\u044f \u0431\u044b\u043b\u0430 \u0438\u0434\u0435\u044f \u0443\u0431\u0440\u0430\u0442\u044c \u0432\u0441\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u044b, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e \u043a\u043e\u0434 \u0434\u043b\u044f \u0438\u0437\u0431\u0430\u0432\u043b\u0435\u043d\u0438\u044f \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u0447\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445 \u043e\u0442 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0438\u0436\u0435, \u043d\u043e \u043d\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u0443\u044e \u0435\u0433\u043e, \u0442\u0430\u043a \u043a\u0430\u043a, \u0432 \u043a\u043e\u043d\u0435\u0447\u043d\u043e\u043c \u0438\u0442\u043e\u0433\u0435, \u044d\u0442\u043e\u0442 \u043f\u043e\u0434\u0445\u043e\u0434 \u043d\u0435 \u043f\u0440\u0438\u043d\u0451\u0441 \u043c\u043d\u0435 \u0443\u0441\u043f\u0435\u0445\u043e\u0432, \u0443\u0434\u0430\u043b\u0435\u043d\u0438\u0435 \u0434\u0443\u0431\u043b\u0438\u043a\u0430\u0442\u043e\u0432 \u043d\u0435 \u043f\u043e\u0432\u043b\u0438\u044f\u043b\u043e \u043d\u0430 \u0443\u043b\u0443\u0447\u0448\u0435\u043d\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439 \u043c\u043e\u0435\u0439 \u0441\u0435\u0442\u0438. \n(\u044d\u0442\u0430 \u043e\u043f\u0435\u0440\u0430\u0446\u0438\u044f \u043f\u0440\u043e\u0432\u043e\u0434\u0438\u0442\u0441\u044f, \u043a\u043e\u043d\u0435\u0447\u043d\u043e, \u0434\u043e \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0430, \u043f\u0440\u043e\u0441\u0442\u043e \u0432\u044b\u043d\u0435\u0441 \u0432 \u043a\u043e\u043d\u0435\u0446 \u0440\u0430\u0437\u0434\u0435\u043b\u0430, \u043a\u0430\u043a \u043d\u0435\u0441\u0443\u0449\u0435\u0441\u0442\u0432\u0435\u043d\u043d\u043e\u0435 \u0432 \u0438\u0442\u043e\u0433\u0435.)","a18f002e":"\u0420\u0430\u0431\u043e\u0442\u0443 \u0432\u044b\u043f\u043e\u043b\u043d\u0438\u043b: \u0414\u043e\u0440\u043e\u043d\u044c\u043a\u0438\u043d \u041c\u0430\u043a\u0441\u0438\u043c \u0412\u044f\u0447\u0435\u0441\u043b\u0430\u0432\u043e\u0432\u0438\u0447, \u0441\u0442\u0443\u0434\u0435\u043d\u0442 3 \u043a\u0443\u0440\u0441\u0430 \u0444\u0430\u043a\u0443\u043b\u044c\u0442\u0435\u0442\u0430 \u0444\u0438\u0437\u0438\u043a\u0438 \u041d\u0418\u0423 \u0412\u0428\u042d (\u041c\u043e\u0441\u043a\u0432\u0430).","fd776ca1":"## \u0424\u0438\u043a\u0441\u0430\u0446\u0438\u044f random seeds \u0438 \u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","239877b4":"### \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445","cc73df1e":"\u041f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044f\u044e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u044b \u0441\u0432\u043e\u0438\u0445 \u0442\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043e\u043a \u0432 google \u0442\u0430\u0431\u043b\u0438\u0446\u0430\u0445:\n\nhttps:\/\/docs.google.com\/spreadsheets\/d\/14iNcTqGMZRhfFsfW-xtYruhaPDeaB24K0iB-u0x6G0k\/edit?usp=sharing","7e8512c0":"\u041c\u043e\u0434\u0435\u043b\u0435\u0439 \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u043b \u043e\u0447\u0435\u043d\u044c \u043c\u043d\u043e\u0433\u043e. \u041d\u0430\u0447\u0438\u043d\u0430\u043b \u0441 \u0431\u0430\u0437\u043e\u0432\u043e\u0433\u043e resnet50 Faster_RCNN, \u043b\u0443\u0447\u0448\u0435\u0433\u043e \u0447\u0435\u043c 0.6 \u043d\u0435 \u0434\u043e\u0431\u0438\u0432\u0430\u043b\u0441\u044f \u043d\u0430 LB, \u0434\u0430\u043b\u0435\u0435 \u0440\u0435\u0448\u0438\u043b \u043f\u0440\u0438\u0434\u0435\u0440\u0436\u0438\u0432\u0430\u0442\u044c\u0441\u044f \u0441\u0442\u0440\u0430\u0442\u0435\u0433\u0438\u0438 faster_RCNN \u0438 \u043f\u043e\u0434\u0431\u0438\u0440\u0430\u043b \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u043f\u0440\u0435\u0434\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u044b\u0435 backbone, \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 densenet, \u043e\u043d\u0438 \u043e\u0431\u0443\u0447\u0430\u044e\u0442\u0441\u044f \u043b\u0443\u0447\u0448\u0435 \u043e\u0441\u0442\u0430\u043b\u044c\u043d\u044b\u0445 \u043d\u0430 \u043d\u0430\u0448\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445, \u0432\u0435\u0440\u043e\u044f\u0442\u043d\u043e \u043f\u043e\u0442\u043e\u043c\u0443 \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u0434\u0440\u0443\u0433\u0438\u0445 \u0440\u0430\u0431\u043e\u0442\u0430\u044e\u0442 \u0441 \u043d\u0435\u0431\u043e\u043b\u044c\u0448\u0438\u043c\u0438 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430\u043c\u0438. \u041f\u044b\u0442\u0430\u043b\u0441\u044f \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c Efficient Det, \u043d\u043e \u0431\u044b\u0441\u0442\u0440\u043e \u043f\u043e\u043d\u044f\u043b, \u0447\u0442\u043e \u0434\u043b\u044f \u0435\u0451 \u043e\u0431\u0443\u0447\u0435\u043d\u0438\u044f \u0442\u043e\u0447\u043d\u043e \u043d\u0443\u0436\u043d\u043e \u0431\u044b\u043b\u043e \u0431\u044b \u0431\u043e\u043b\u044c\u0448\u0435 \u0434\u0430\u043d\u043d\u044b\u0445. \n\u0422\u0430\u043a\u0436\u0435 \u0437\u0430\u043c\u0435\u0442\u0438\u043b, \u0447\u0442\u043e trainable backbone layers \u043b\u0443\u0447\u0448\u0435 \u043e\u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0442\u044c \u043a\u0430\u043a \u043c\u043e\u0436\u043d\u043e \u0431\u043e\u043b\u044c\u0448\u0435, \u0442\u0430\u043a \u0447\u0442\u043e \u044f \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0435 \u0441\u0442\u0430\u043b \u0437\u0434\u0435\u0441\u044c \u0437\u0430\u043c\u043e\u0440\u0430\u0436\u0438\u0432\u0430\u0442\u044c \u0441\u043b\u043e\u0438 backbone, \u0445\u043e\u0442\u044f \u0447\u0430\u0441\u0442\u043e \u0442\u0430\u043a \u0434\u0435\u043b\u0430\u044e\u0442. \u0412 \u0442\u0430\u0431\u043b\u0438\u0446\u0435 \u043d\u0438\u0436\u0435 \u044f \u043e\u043f\u0438\u0441\u0430\u043b \u0441\u043b\u0443\u0447\u0430\u0439 \u0441 \u0437\u0430\u043c\u043e\u0440\u043e\u0436\u0435\u043d\u043d\u044b\u043c\u0438 \u0432\u0435\u0441\u0430\u043c\u0438 backbone. \u0417\u0430\u043c\u043e\u0440\u043e\u0437\u043a\u0430 \u043d\u0435 \u0437\u0430\u0448\u043b\u0430 \u0434\u043b\u044f DenseNet","fe901a8e":"\u041e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b, \u0447\u0442\u043e \u0434\u0430\u043d\u043d\u0430\u044f \u043c\u043e\u0434\u0435\u043b\u044c \u043b\u0443\u0447\u0448\u0435 \u0432\u0441\u0435\u0433\u043e \u0431\u044b\u043b\u0430 \u0432\u0441\u0435\u0433\u0434\u0430 \u0438\u043c\u0435\u043d\u043d\u043e \u043d\u0430 \u044d\u043f\u043e\u0445\u0430\u0445 10-13, \u0430 \u043b\u0443\u0447\u0448\u0430\u044f \u043f\u043e \u043b\u043e\u0441\u0441\u0443 \u043a\u0430\u043a \u043d\u0438 \u0441\u0442\u0440\u0430\u043d\u043d\u043e \u043e\u043a\u0430\u0437\u044b\u0432\u0430\u043b\u0430\u0441\u044c \u043d\u0435\u0434\u043e\u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u0434\u043b\u044f \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0442\u0435\u0441\u0442\u0430, \u0442\u0430\u043a \u0447\u0442\u043e \u044f, \u0443\u0436\u0435 \u0437\u043d\u0430\u044f \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0430\u044f \u043f\u043e \u043b\u0435\u0432\u0435\u043d\u0448\u0442\u0430\u0439\u043d\u0443 \u043d\u0430 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u0438 \u0438 \u043f\u043e \u043b\u043e\u0441\u0441\u0443 \u043d\u0435 \u043f\u043e\u0434\u043e\u0439\u0434\u0443\u0442, \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u044e \u0432 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u0437\u0430\u0433\u0440\u0443\u0437\u043a\u0443 \u043c\u043e\u0434\u0435\u043b\u0438 12 \u044d\u043f\u043e\u0445\u0438. \n(\u043a\u0430\u043a \u0432\u043f\u043e\u0441\u043b\u0435\u0434\u0441\u0442\u0432\u0438\u0438 \u0432\u044b\u044f\u0441\u043d\u0438\u043b\u043e\u0441\u044c, \u044d\u0442\u043e \u0431\u044b\u043b\u043e \u043e\u0448\u0438\u0431\u043a\u043e\u0439, \u0441\u0430\u0431\u043c\u0438\u0442 \u0441 \u0442\u0430\u043a\u043e\u0439 \u0436\u0435 \u043c\u043e\u0434\u0435\u043b\u044c\u044e, \u043e\u0431\u0443\u0447\u0435\u043d\u043d\u043e\u0439 \u043d\u0430 20 \u044d\u043f\u043e\u0445\u0430\u0445, \u043f\u043e\u043a\u0430\u0437\u0430\u043b \u043d\u0430 private \u0441\u043a\u043e\u0440 0.39, \u0447\u0442\u043e \u043b\u0443\u0447\u0448\u0435 \u043c\u043e\u0435\u0433\u043e) \u041d\u043e, \u043a \u0441\u043e\u0436\u0430\u043b\u0435\u043d\u0438\u044e, \u044f \u0435\u0433\u043e \u043d\u0435 \u0432\u044b\u0431\u0440\u0430\u043b \u0434\u043b\u044f \u0444\u0438\u043d\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0441\u043a\u043e\u0440\u0438\u043d\u0433\u0430 :(\n\n![image.png](attachment:d71fd002-ce3b-402a-9087-551bbb0368c9.png)","3c37c5e9":"## \u041c\u0435\u0442\u0440\u0438\u043a\u0430","d5287b3b":"# \"Look at the data\"","0171d9e2":"\u041f\u0440\u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u043c\u043d\u043e\u0433\u043e \u0441\u043c\u043e\u0442\u0440\u0435\u043b \u043d\u0430 \u0442\u0435\u0441\u0442\u043e\u0432\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u0438 \u0438\u0433\u0440\u0430\u043b\u0441\u044f \u0441 threshold. \u0411\u044b\u043b\u0430 \u0438\u0434\u0435\u044f \u0441\u0442\u0430\u0432\u0438\u0442\u044c \u0438\u043d\u043e\u0439 threshold, \u043d\u0430\u0437\u0432\u0430\u043d\u043d\u044b\u0439 \u043c\u043d\u043e\u044e EMPTY_STRING_THRESHOLD, \u043d\u0430 \u0442\u0435 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0438, \u0433\u0434\u0435 \u0441\u0435\u0442\u044c \u0441\u043e\u0432\u0441\u0435\u043c \u043d\u0438\u0447\u0435\u0433\u043e \u043d\u0435 \u043e\u0431\u043d\u0430\u0440\u0443\u0436\u0438\u043b\u0430, \u0442\u0430\u043a \u043a\u0430\u043a \u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c, \u0447\u0442\u043e \u0432 \u0442\u0435\u0441\u0442\u0435 \u043f\u043e\u0447\u0442\u0438 \u043d\u0430 \u043a\u0430\u0436\u0434\u043e\u0439 \u043a\u0430\u0440\u0442\u0438\u043d\u043a\u0435 \u0435\u0441\u0442\u044c \u0445\u043e\u0442\u044f \u0431\u044b \u043e\u0434\u043d\u0430 \u0431\u0430\u043d\u043a\u043e\u0432\u0441\u043a\u0430\u044f \u043a\u0430\u0440\u0442\u0430, \u043d\u043e \u044d\u0442\u0430 \u0438\u0434\u0435\u044f \u0432 \u0438\u0442\u043e\u0433\u0435 \u043d\u0435 \u0437\u0430\u0448\u043b\u0430 \u0438 \u0442\u043e\u043b\u044c\u043a\u043e \u043f\u043e\u0440\u0442\u0438\u043b\u0430 \u0441\u043a\u043e\u0440 \u043d\u0430 LB, \u0442\u0430\u043a \u0447\u0442\u043e \u0442\u0443\u0442 \u0443\u0436\u0435 \u0435\u0451 \u0443\u0431\u0438\u0440\u0430\u044e.","9a8f26c7":"\u041f\u0430\u0440\u0430 \u0441\u043b\u043e\u0432 \u043e \u043f\u043e\u0434\u0431\u043e\u0440\u0435 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0437\u0430\u0445\u043e\u0434\u0438\u043b\u0438 \u0438 \u043d\u0435 \u0437\u0430\u0445\u043e\u0434\u0438\u043b\u0438 \u0443 \u043c\u0435\u043d\u044f. \u0414\u043e\u0432\u043e\u043b\u044c\u043d\u043e \u0441\u043a\u043e\u0440\u043e \u044f \u043f\u043e\u043d\u044f\u043b, \u0447\u0442\u043e \u0432\u0430\u0436\u043d\u0443\u044e \u0440\u043e\u043b\u044c \u0432 \u043f\u043e\u0431\u0435\u0434\u0435 \u0438\u0433\u0440\u0430\u0435\u0442 Threshold, \u043a\u0430\u043a \u0432 \u044d\u0442\u043e\u043c \u0438 \u0443\u0431\u0435\u0434\u0438\u043b\u0441\u044f \u043f\u043e\u0441\u043b\u0435 \u043e\u043a\u043e\u043d\u0447\u0430\u043d\u0438\u044f \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u044f, \u044f \u043f\u0440\u0438\u043c\u0435\u043d\u044f\u043b \u0440\u0430\u0437\u043b\u0438\u0447\u043d\u044b\u0435 \u0438 \u0441\u043c\u043e\u0442\u0440\u0435\u043b \u043d\u0430 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u0438 \u0432 \u043e\u0442\u0432\u0435\u0442\u0430\u0445 \u0441\u0435\u0442\u0438. \u0412\u0432\u043e\u0434\u0438\u043b \u0434\u0432\u0430 \u043f\u043e\u0440\u043e\u0433\u0430, \u043e\u0434\u0438\u043d \u043e\u0447\u0435\u043d\u044c \u0432\u044b\u0441\u043e\u043a\u0438\u0439, \u043c\u043d\u0435 \u0431\u044b\u043b\u043e \u0438\u043d\u0442\u0435\u0440\u0435\u0441\u043d\u043e \u043f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u043a\u0430\u043a \u0445\u043e\u0440\u043e\u0448\u043e \u0441\u043e\u0432\u043f\u0430\u0434\u0430\u044e\u0442 \u0441 \u0440\u0435\u0430\u043b\u044c\u043d\u044b\u043c\u0438 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f \u0441\u0435\u0442\u0438 \u0441 \u0432\u044b\u0441\u043e\u043a\u043e\u0439 \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c\u044e. \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430 \u0441\u043f\u0435\u0440\u0432\u0430 \u0441\u0442\u0430\u0432\u0438\u043b \u0440\u0430\u0432\u043d\u044b\u043c 16 \u0438 12, \u043d\u043e \u0437\u0430\u0442\u0435\u043c \u044d\u043a\u0441\u043f\u0435\u0440\u0438\u043c\u0435\u043d\u0442\u0430\u043b\u044c\u043d\u043e \u043f\u043e\u043d\u044f\u043b, \u0447\u0442\u043e \u0447\u0443\u0442\u044c \u043c\u0435\u043d\u044c\u0448\u0438\u0439 \u0437\u0430\u0445\u043e\u0434\u0438\u0442 \u043b\u0443\u0447\u0448\u0435, \u043e\u0441\u0442\u0430\u043d\u043e\u0432\u0438\u043b\u0441\u044f \u043d\u0430 8. \u0414\u0430\u043b\u0435\u0435 \u044f \u043c\u043d\u043e\u0433\u043e \u0432\u0440\u0435\u043c\u0435\u043d\u0438 \u043f\u043e\u0441\u0432\u044f\u0442\u0438\u043b \u043e\u0432\u0435\u0440\u0441\u0435\u043c\u043f\u043b\u0438\u043d\u0433\u0443, \u043a\u043e\u0442\u043e\u0440\u044b\u0439 \u0434\u0435\u0442\u0430\u043b\u044c\u043d\u043e \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u044e \u0447\u0443\u0442\u044c \u043d\u0438\u0436\u0435. \u041f\u043e\u0434\u0431\u043e\u0440 \u0433\u0438\u043f\u0435\u0440\u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432, \u0434\u0430\u0432\u0448\u0438\u0445 \u043c\u043d\u0435 \u043c\u043e\u0451 \u043b\u0443\u0447\u0448\u0435\u0435 \u0440\u0435\u0448\u0435\u043d\u0438\u0435, \u043d\u0438\u0436\u0435:","3c183407":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430"}}