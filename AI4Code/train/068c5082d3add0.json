{"cell_type":{"b5ce8cb4":"code","b998af2a":"code","bad23641":"code","193bdd5b":"code","a11e7d06":"code","64a3b64c":"code","e5fe208d":"code","0518836c":"code","1a7cdaac":"code","e54b40d7":"code","1b7d2f0b":"code","0e29ece1":"code","88b23858":"code","1bdca590":"code","ecc5f81c":"code","95d24fee":"code","17f697c6":"code","55a46c0a":"code","711b419a":"code","4dfb4f65":"code","7f080804":"markdown","b256fef6":"markdown","ab4d5a00":"markdown","037e0bb2":"markdown","9cca2085":"markdown","0defb4a2":"markdown","7607dc9e":"markdown","0a620126":"markdown","bbb5ca87":"markdown","57b8e56f":"markdown"},"source":{"b5ce8cb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\nimport lightgbm as lgbm\nimport datetime\nimport sklearn.model_selection","b998af2a":"date_cols = [\n    'cdc_report_dt',\n    'pos_spec_dt',\n    'onset_dt',\n]\n\npd_training = pd.read_csv(\n    r'\/kaggle\/input\/health-hackathon-prep\/training.csv',\n    parse_dates=date_cols,\n)\npd_prediction = pd.read_csv(\n    r'\/kaggle\/input\/health-hackathon-prep\/testing.csv',\n    parse_dates=date_cols,\n)","bad23641":"pd_training.head()","193bdd5b":"pd_training_no_label = pd_training.drop('death_yn', axis='columns')\nlabels = pd_training['death_yn'].map({'Yes': 1, 'No': 0})\nprint('Records:', labels.size, '\\nDeaths:', labels.sum())","a11e7d06":"pd_training_naive = pd_training_no_label.fillna({\n    'cdc_report_dt': datetime.datetime(2020, 1, 1),\n    'pos_spec_dt': datetime.datetime(2020, 1, 1),\n    'onset_dt': datetime.datetime(2020, 1, 1),\n    'sex': 'Unknown',\n    'age_group': 'Unknown',\n    'Race and ethnicity (combined)': 'Unknown',\n})\npd_training_naive['cdc_report_dt'] = pd_training_naive['cdc_report_dt'].dt.strftime('%Y%m%d')\npd_training_naive['pos_spec_dt'] = pd_training_naive['pos_spec_dt'].dt.strftime('%Y%m%d')\npd_training_naive['onset_dt'] = pd_training_naive['onset_dt'].dt.strftime('%Y%m%d')\npd_training_naive = pd_training_naive.astype({\n    'cdc_report_dt': 'int',\n    'pos_spec_dt': 'int',\n    'onset_dt': 'int',\n})\n\nmap_yn_cols = {\n    'Yes': 1,\n    'No': 0,\n    'Unknown': -1,\n}\npd_training_naive['hosp_yn'] = pd_training_naive['hosp_yn'].map(map_yn_cols)\npd_training_naive['icu_yn'] = pd_training_naive['icu_yn'].map(map_yn_cols)\npd_training_naive['medcond_yn'] = pd_training_naive['medcond_yn'].map(map_yn_cols)\n\npd_training_naive['sex'] = pd_training_naive['sex'].map({'Male': 0, 'Female': 1, 'Other': 2, 'Unknown': 3})\npd_training_naive['current_status'] = pd_training_naive['current_status'].map({'Laboratory-confirmed case': 0, 'Probable Case': 1})\npd_training_naive['age_group'] = pd_training_naive['age_group'].map({\n    '0 - 9 Years': 0,\n    '10 - 19 Years': 1,\n    '20 - 29 Years': 2,\n    '30 - 39 Years': 3,\n    '40 - 49 Years': 4,\n    '50 - 59 Years': 5,\n    '60 - 69 Years': 6,\n    '70 - 79 Years': 7,\n    '80+ Years': 8,\n    'Unknown': -1,\n})\npd_training_naive['Race and ethnicity (combined)'] = pd_training_naive['Race and ethnicity (combined)'].map({\n    'American Indian\/Alaska Native, Non-Hispanic': 0,\n    'Asian, Non-Hispanic': 1,\n    'Black, Non-Hispanic': 2,\n    'Hispanic\/Latino': 3,\n    'Multiple\/Other, Non-Hispanic': 4,\n    'Native Hawaiian\/Other Pacific Islander, Non-Hispanic': 5,\n    'Unknown': 6,\n    'White, Non-Hispanic': 7,\n})\npd_training_naive.head()","64a3b64c":"training, holdout, training_labels, holdout_labels = sklearn.model_selection.train_test_split(\n    pd_training_naive,\n    labels,\n    train_size=0.8,\n    random_state=42,\n)\nprint('Records in training:', len(training), '\\nRecords in holdout:', len(holdout))","e5fe208d":"training_dset = lgbm.Dataset(\n    training,\n    training_labels,\n)\nvalid_dset = lgbm.Dataset(\n    holdout,\n    holdout_labels,\n)\nlgbm_params = {\n    'task': 'train',\n    'objective': 'binary',\n    'metric': 'binary_logloss',\n    'learning_rate': 0.03,\n    'num_leaves': 128,\n    'seed': 42,\n}\nevals_naive = {}\nhistory = lgbm.train(\n    lgbm_params,\n    training_dset,\n    num_boost_round=800,\n    verbose_eval=50,\n    valid_sets=[valid_dset],\n    valid_names=['valid_naive'],\n    evals_result=evals_naive,\n)","0518836c":"all_features = pd.concat(\n    [\n        pd_training_no_label.assign(train_test='Train'),\n        pd_prediction.assign(train_test='Test'),\n    ],\n    axis='rows',\n)\nprint('Records in stacked features:', len(all_features))","1a7cdaac":"import sklearn.preprocessing\n\ncategorical_features = [\n    'current_status',\n    'sex',\n    'age_group',\n    'Race and ethnicity (combined)',\n    'hosp_yn',\n    'icu_yn',\n    'medcond_yn',\n]\nall_features = all_features.fillna({\n    cat_var: 'Missing'\n    for cat_var in categorical_features\n})\nenc = sklearn.preprocessing.OneHotEncoder()\nohe_features = enc.fit_transform(all_features[categorical_features])","e54b40d7":"import re\nohe_vars = []\nfor cat_var, cat_var_categories in zip(categorical_features, enc.categories_):\n    for category in cat_var_categories:\n        colname = f'{cat_var}_{category}'\n        colname_cleaner = lambda x:re.sub('[^A-Za-z0-9_]+', '_', x)\n        ohe_vars.append(\n            colname_cleaner(colname)\n        )\nprint(ohe_vars)","1b7d2f0b":"all_features[ohe_vars] = ohe_features.toarray()\nall_features.head()","0e29ece1":"date_features = [\n    'cdc_report_dt',\n    'pos_spec_dt',\n    'onset_dt',\n]\nall_features = all_features.fillna({\n    date_feature: datetime.datetime(2020, 1, 1)\n    for date_feature in date_features\n})\ndate_first_us_case = datetime.datetime(2020, 1, 21)\nfor date_feature in date_features:\n    all_features[f'{date_feature}_day'] = all_features[date_feature].dt.day\n    all_features[f'{date_feature}_month'] = all_features[date_feature].dt.month\n    all_features[f'{date_feature}_week'] = all_features[date_feature].dt.isocalendar().week\n    all_features[f'{date_feature}_dayofweek'] = all_features[date_feature].dt.dayofweek\n    all_features[f'{date_feature}_dayofyear'] = all_features[date_feature].dt.dayofyear\n    all_features[f'{date_feature}_days_since_first_case'] = (all_features[date_feature] - date_first_us_case).dt.days\n    \n# Novel date features\n\nall_features['days_diff_cdc_and_pos_spec'] = (all_features['cdc_report_dt'] - all_features['pos_spec_dt']).dt.days\nall_features['days_diff_cdc_and_onset'] = (all_features['cdc_report_dt'] - all_features['onset_dt']).dt.days\nall_features['days_diff_pos_spec_and_onset'] = (all_features['pos_spec_dt'] - all_features['onset_dt']).dt.days\n\n\nall_features.head()","88b23858":"# Add case counts\nnum_cases_by_date = all_features.groupby('cdc_report_dt_days_since_first_case').size()\ncumul_cases = num_cases_by_date.cumsum()\ncumul_cases.name = 'cumulative_cases'\n\nnum_cases_by_week = all_features.groupby('cdc_report_dt_week').size()\nnum_cases_by_week.name = 'cases_this_week'\n\nall_features = all_features.join(\n    cumul_cases,\n    on='cdc_report_dt_days_since_first_case',\n).join(\n    num_cases_by_week,\n    on='cdc_report_dt_week',\n)\nall_features.head()","1bdca590":"# Add some simple PCA\n\nimport sklearn.decomposition\nimport sklearn.pipeline\n\nfeatures_for_pca = all_features.drop(\n    categorical_features + date_features + ['id','train_test'],\n    axis='columns',\n)\npipeline = sklearn.pipeline.Pipeline(\n    [\n        ('normalizer', sklearn.preprocessing.StandardScaler()),\n        ('pca', sklearn.decomposition.PCA(n_components=10)),\n    ]\n)\npca_features = pipeline.fit_transform(features_for_pca)\npca_cols = [f'pca_{num}' for num in range(1, 11)]\nall_features[pca_cols] = pca_features\nall_features.head()","ecc5f81c":"pca_fit = pipeline.named_steps['pca']\nprint(pca_fit.explained_variance_ratio_)","95d24fee":"# Drop our un-trainable features\n\nall_features = all_features.drop(\n    categorical_features + date_features + ['cdc_report_dt_week', 'pos_spec_dt_week', 'onset_dt_week'],\n    axis='columns',\n)","17f697c6":"# Split back into train\/test\ntrain_engineered_features = all_features.loc[lambda df: df.train_test == 'Train'].drop('train_test', axis='columns')\ntest_engineered_features = all_features.loc[lambda df: df.train_test == 'Test'].drop('train_test', axis='columns')\n\ntrain_engineered_features.head()","55a46c0a":"# Make sure we didn't break any row ordering in our feature engineering pipeline\nassert (train_engineered_features.id == pd_training.id).all()\n\ntraining_engineered, holdout_engineered = sklearn.model_selection.train_test_split(\n    train_engineered_features,\n    train_size=0.8,\n    random_state=42,\n)\n\ntraining_engineered_dset = lgbm.Dataset(\n    training_engineered,\n    training_labels,\n)\nvalid_engineered_dset = lgbm.Dataset(\n    holdout_engineered,\n    holdout_labels,\n)\nevals_engineered = {}\nengineered_history = lgbm.train(\n    lgbm_params,\n    training_engineered_dset,\n    num_boost_round=800,\n    verbose_eval=50,\n    valid_sets=[valid_engineered_dset],\n    valid_names=['valid_engineered'],\n    evals_result=evals_engineered,\n)","711b419a":"evals = {**evals_naive, **evals_engineered}\nlgbm.plot_metric(evals, figsize=(10, 8))","4dfb4f65":"lgbm.plot_importance(history, figsize=(8, 6), title='Feature Importance - Naive Features')\nlgbm.plot_importance(engineered_history, figsize=(8, 6), max_num_features=15, title='Feature Importance - Engineered Features')","7f080804":"## Next, we split off the prediction target (labels) from the features","b256fef6":"## Now let's see if these engineered features help our algorithm learn better","ab4d5a00":"### Let's fit a very simple, untuned LightGBM model on our simple features","037e0bb2":"## Now that we've fit a simple model, let's try to see if we can extract more value through feature engineering\n\nWe'll start by stacking our training and testing data together, so that we can apply feature engineering to both consistently","9cca2085":"### Next, let's work on our date columns","0defb4a2":"### Now let's one hot encode all of our categorical features","7607dc9e":"### Let's close it out with some novel feature construction","0a620126":"## The GBM algorithm that we will be using, LightGBM, requires that all feature information is either integers or floats.\n\nThe section below represents the bare minimum amount of work that we could do to get all of the features in a state where they can be passed to LightGBM","bbb5ca87":"## First, we read the data into Pandas","57b8e56f":"### Let's hold out 20% of our data for assessing the fit of our LightGBM model on simple features"}}