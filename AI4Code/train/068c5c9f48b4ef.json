{"cell_type":{"9092e6dd":"code","a04d07c5":"code","b80ea6b5":"code","a74cc07c":"code","7291437c":"code","0874809f":"code","3c0b4a55":"code","b464be2f":"code","4929b5e2":"code","814c342b":"code","2b122d04":"code","c79d3261":"code","8ef86b42":"code","4277b60f":"code","26bf88db":"code","17209855":"code","c67b267a":"code","99ce097b":"code","3ed3c1bb":"code","ef578f85":"code","0c3d2e15":"code","1f76e35a":"code","a98b42b8":"code","af8311b8":"code","6b25edf1":"code","64dabe06":"code","1a448889":"code","47de1e4f":"code","94f831bd":"code","42e3fb92":"code","c9fc97a1":"code","ba46a980":"code","1e8e506f":"code","2a369a40":"code","bf907837":"code","0c7c27f0":"code","f51b8cd4":"code","c1ff5a1b":"code","11d44aa6":"code","ab96a1fa":"code","c327b113":"code","6d782210":"code","4ff76ded":"code","9e826566":"code","56b5b98a":"code","cc5a7a12":"code","f8fd5f95":"code","86b6e984":"code","55290bba":"code","65964d06":"code","c30f9296":"code","df9d766c":"code","003edeb1":"code","d97006fb":"code","78b7728a":"code","fc8ca902":"code","cc41c902":"code","081cbd83":"code","9bc5e2a3":"code","f66174e4":"code","3cb80c51":"code","d1878bda":"code","3032493f":"code","85421150":"code","99db94c1":"code","453e8781":"code","f4db0fe9":"code","e74c9ca1":"code","d34013e8":"code","17160dc0":"code","16bbad4c":"code","778939b8":"code","4e655985":"code","b7eb8361":"code","f3af6c4b":"code","c284d0fc":"code","4e386328":"code","2f6b7e56":"code","f71b9f55":"code","505e67ea":"code","fb3d83e7":"code","853c0d29":"code","8ad4cfbd":"code","56f58e99":"code","23bf3b92":"code","4aff7ece":"code","96e3f964":"code","d2345176":"code","668493c5":"code","39f957f0":"code","3745dcc0":"code","e4fb4783":"code","de2f4135":"code","85ba0e28":"code","16062caf":"code","d4354f78":"code","19fa70ca":"code","8e0b8eb2":"code","96898e27":"code","04f20ac1":"code","2e924046":"code","06c2eec0":"code","6165742a":"code","5f0084dd":"code","65bbd9a5":"code","78ba83a4":"markdown","b30d9362":"markdown","7f4d52ee":"markdown","4f8559d7":"markdown","8935dfa4":"markdown","4b9f228a":"markdown","4b10714e":"markdown","f9646c82":"markdown","1167ba83":"markdown","b865d4a1":"markdown","0103d8b5":"markdown","21872b99":"markdown","8e5c9a4e":"markdown","8c3174a8":"markdown","5b92370f":"markdown","b494a0b8":"markdown","9e18b5e3":"markdown","cd28f779":"markdown","a7932266":"markdown","abc11958":"markdown","cc662914":"markdown","785f1d8b":"markdown","1b7e8d24":"markdown","fd871fcb":"markdown","0ead0f75":"markdown","7f3cea1f":"markdown","c12d6f6a":"markdown","50f4ce2d":"markdown","30a35fc2":"markdown","2aa95166":"markdown","5da060a5":"markdown","a6cf164c":"markdown","23cc7f69":"markdown","74b1e46a":"markdown","e8a838d3":"markdown","ee19ed35":"markdown","125a29bf":"markdown","b7690cbe":"markdown","61cad477":"markdown","e1739bdc":"markdown","c8909a04":"markdown","98d3c7f9":"markdown","8cdd12e1":"markdown","95024cea":"markdown","614e1e9e":"markdown","0db38c14":"markdown","543d138e":"markdown","532b80c3":"markdown","d21c4566":"markdown","5ae12ad1":"markdown","d54e1afd":"markdown","9cc49f30":"markdown","6c958727":"markdown","dee0f608":"markdown","2d3cea56":"markdown","7b993cb0":"markdown","991c9945":"markdown","d9333253":"markdown","438c6678":"markdown","b1ec634b":"markdown","04fcce08":"markdown","639413f2":"markdown","18853354":"markdown","1cd69c5e":"markdown","dd346ec0":"markdown","5a255533":"markdown","a51445bf":"markdown","73eab0c2":"markdown","17617628":"markdown","c5837981":"markdown","8043f7bb":"markdown","99c4f3ab":"markdown","81d44ec6":"markdown","0febf92b":"markdown","6ccdbaf8":"markdown","cdba4706":"markdown","63e39903":"markdown","f8701d32":"markdown","ac70c01b":"markdown","e790af43":"markdown","854b8a61":"markdown","5b4ef1f6":"markdown","8e3e71cb":"markdown","bd8ec384":"markdown","496663ce":"markdown","9667c0d2":"markdown","97a86eae":"markdown","d76f7269":"markdown","8d0db65f":"markdown","d61bfa45":"markdown","e484fce1":"markdown","459e1785":"markdown","9e8c7426":"markdown","7fefde37":"markdown","733c6355":"markdown","6e84feaa":"markdown"},"source":{"9092e6dd":"# import pandas library \nimport pandas as pd\ndf = pd.read_csv('..\/input\/no-show-appointment\/data.csv')","a04d07c5":"print('Number of samples:', len(df))","b80ea6b5":"# exploring data\nlist(df. columns)","a74cc07c":"df.head()","7291437c":"df.describe()","0874809f":"#Renaming the No-show column as Output_label \ndf = df.rename(columns={\"No-show\": \"OUTPUT_LABEL\"})\n\ndf.OUTPUT_LABEL = df.OUTPUT_LABEL.map({ 'No': 0, 'Yes': 1 })","3c0b4a55":"df[['OUTPUT_LABEL']].head(10)","b464be2f":"def calc_prevalence(y_actual):\n    # this function calculates the prevalence of the positive class (label = 1)\n    return (sum(y_actual)\/len(y_actual))","4929b5e2":"print('prevalence of the positive class: %.3f'%calc_prevalence(df['OUTPUT_LABEL'].values))","814c342b":"#number of columns\nprint('Number of columns:',len(df.columns))","2b122d04":"df[list(df.columns)[:14]].head()","c79d3261":"for c in list(df.columns):\n    \n    # get a list of unique values\n    n = df[c].unique()\n    \n    # if number of unique values is less than 30, print the values. Otherwise print the number of unique values\n    if len(n)<30:\n        print(c)\n        print(n)\n    else:\n        print(c + ': ' +str(len(n)) + ' unique values')","8ef86b42":"#Seeing this Handcap, Gender are the variable to be made to be changed into binary\nimport numpy as np     \n# replace ? with nan\ndf = df.replace('?',np.nan) ","4277b60f":"df.Age.describe()","26bf88db":"df = df[~(df['Age'] < 1)]    \ndf.Age.describe()","17209855":"#Change to the date format\ndf['ScheduledDay'] = pd.to_datetime(df['ScheduledDay'],format='%Y-%m-%d %H:%M:%S')\ndf['AppointmentDay'] = pd.to_datetime(df['AppointmentDay'],format='%Y-%m-%d %H:%M:%S')\ndf['num_days'] = (df['AppointmentDay']-df['ScheduledDay']).dt.days\ndf.num_days.head(10)","c67b267a":"df.num_days = np.where(df.num_days<0, 0, df.num_days)\ndf.num_days.head(10)","99ce097b":"df[\"day_of_week\"] = df[\"ScheduledDay\"].dt.dayofweek\ndf[\"month\"] = df[\"ScheduledDay\"].dt.month\ndf[\"week\"] = df[\"ScheduledDay\"].dt.week","3ed3c1bb":"cols_num = ['Scholarship','Hipertension', 'Diabetes', 'Alcoholism',\n       'SMS_received', 'Age', 'num_days', 'day_of_week', 'month','week']","ef578f85":"df[cols_num].isnull().sum()","0c3d2e15":"#pure categorical variables\ncols_cat = ['Gender']","1f76e35a":"pd.get_dummies(df[cols_cat]).head()","a98b42b8":"df.Handcap.describe()","af8311b8":"cols_cat_num = ['Handcap']","6b25edf1":"df[cols_cat_num] = df[cols_cat_num].astype('str')\npd.get_dummies(df[cols_cat_num],drop_first = False).head()","64dabe06":"df_cat = pd.get_dummies(df[cols_cat + cols_cat_num],drop_first = True)","1a448889":"df_cat.head()","47de1e4f":"# Joining all the categorical variables and saving them\ndf = pd.concat([df,df_cat], axis = 1)\ncols_all_cat = list(df_cat.columns)","94f831bd":"#Engineering Features Summary\nprint('Total number of features:', len(cols_num + cols_all_cat))\nprint('Numerical Features:',len(cols_num))\nprint('Categorical Features:',len(cols_all_cat))\n","42e3fb92":"#Let's check if we are missing any data.\ndf[cols_num + cols_all_cat].isnull().sum().sort_values(ascending = False).head(10)","c9fc97a1":"cols_input = cols_num + cols_all_cat\ndf_data = df[cols_input + ['OUTPUT_LABEL']]","ba46a980":"df_data.head()","1e8e506f":"# check for duplicated columns in cols_input\ndup_cols = set([x for x in cols_input if cols_input.count(x) > 1])\nprint(dup_cols)\nassert len(dup_cols) == 0,'you have duplicated columns in cols_input'","2a369a40":"# check for duplicated columns in df_data\ncols_df_data = list(df_data.columns)\ndup_cols = set([x for x in cols_df_data if cols_df_data.count(x) > 1])\nprint(dup_cols)\nassert len(dup_cols) == 0,'you have duplicated columns in df_data'","bf907837":"# check the size of df_data makes sense\nassert (len(cols_input) + 1) == len(df_data.columns), 'issue with dimensions of df_data or cols_input'","0c7c27f0":"# shuffle the samples\ndf_data = df_data.sample(n = len(df_data), random_state = 42)\ndf_data = df_data.reset_index(drop = True)","f51b8cd4":"# Save 30% of the data as validation and test data \ndf_valid_test=df_data.sample(frac=0.30,random_state=42)\nprint('Split size: %.3f'%(len(df_valid_test)\/len(df_data)))","c1ff5a1b":"#And now split into test and validation using 50% fraction\ndf_test = df_valid_test.sample(frac = 0.5, random_state = 42)\ndf_valid = df_valid_test.drop(df_test.index)","11d44aa6":"# use the rest of the data as training data\ndf_train_all=df_data.drop(df_valid_test.index)","ab96a1fa":"# check the prevalence of each \nprint('Test prevalence(n = %d):%.3f'%(len(df_test),calc_prevalence(df_test.OUTPUT_LABEL.values)))\nprint('Valid prevalence(n = %d):%.3f'%(len(df_valid),calc_prevalence(df_valid.OUTPUT_LABEL.values)))\nprint('Train all prevalence(n = %d):%.3f'%(len(df_train_all), calc_prevalence(df_train_all.OUTPUT_LABEL.values)))","c327b113":"print('all samples (n = %d)'%len(df_data))\nassert len(df_data) == (len(df_test)+len(df_valid)+len(df_train_all)),'math didnt work'","6d782210":"# split the training data into positive and negative\nrows_pos = df_train_all.OUTPUT_LABEL == 1\ndf_train_pos = df_train_all.loc[rows_pos]\ndf_train_neg = df_train_all.loc[~rows_pos]\n\nn = np.min([len(df_train_pos), len(df_train_neg)])\n\n# merge the balanced data\ndf_train = pd.concat([df_train_pos.sample(n = n, random_state = 42), \n                      df_train_neg.sample(n = n, random_state = 42)],axis = 0, \n                     ignore_index = True)\n\n# shuffle the order of training samples \ndf_train = df_train.sample(n = len(df_train), random_state = 42).reset_index(drop = True)\n\nprint('Train balanced prevalence(n = %d):%.3f'%(len(df_train), calc_prevalence(df_train.OUTPUT_LABEL.values)))","4ff76ded":"df_train_all.to_csv('df_train_all.csv',index=False)\ndf_train.to_csv('df_train.csv',index=False)\ndf_valid.to_csv('df_valid.csv',index=False)\ndf_test.to_csv('df_test.csv',index=False)","9e826566":"import pickle\npickle.dump(cols_input, open('cols_input.sav', 'wb'))","56b5b98a":"def fill_my_missing(df, df_mean, col2use):\n    # This function fills the missing values\n\n    # check the columns are present\n    for c in col2use:\n        assert c in df.columns, c + ' not in df'\n        assert c in df_mean.col.values, c+ 'not in df_mean'\n    \n    # replace the mean \n    for c in col2use:\n        mean_value = df_mean.loc[df_mean.col == c,'mean_val'].values[0]\n        df[c] = df[c].fillna(mean_value)\n    return df","cc5a7a12":"# your code here\ndf_mean = df_train_all[cols_input].mean(axis = 0)\n# save the means\ndf_mean.to_csv('df_mean.csv',index=True)","f8fd5f95":"# load the means so we know how to do it for the test data\ndf_mean_in = pd.read_csv('df_mean.csv', names =['col','mean_val'])\ndf_mean_in.head()","86b6e984":"# create the X and y matrices\nX_train = df_train[cols_input].values\nX_train_all = df_train_all[cols_input].values\nX_valid = df_valid[cols_input].values\n\ny_train = df_train['OUTPUT_LABEL'].values\ny_valid = df_valid['OUTPUT_LABEL'].values\n\nprint('Training All shapes:',X_train_all.shape)\nprint('Training shapes:',X_train.shape, y_train.shape)\nprint('Validation shapes:',X_valid.shape, y_valid.shape)","55290bba":"from sklearn.preprocessing import StandardScaler\nscaler  = StandardScaler()\nscaler.fit(X_train_all)","65964d06":"scalerfile = 'scaler.sav'\npickle.dump(scaler, open(scalerfile, 'wb'))","c30f9296":"# load it back\nscaler = pickle.load(open(scalerfile, 'rb'))","df9d766c":"# transform our data matrices\nX_train_tf = scaler.transform(X_train)\nX_valid_tf = scaler.transform(X_valid)","003edeb1":"from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\ndef calc_specificity(y_actual, y_pred, thresh):\n    # calculates specificity\n    return sum((y_pred < thresh) & (y_actual == 0)) \/sum(y_actual ==0)\n\ndef print_report(y_actual, y_pred, thresh):\n    \n    auc = roc_auc_score(y_actual, y_pred)\n    accuracy = accuracy_score(y_actual, (y_pred > thresh))\n    recall = recall_score(y_actual, (y_pred > thresh))\n    precision = precision_score(y_actual, (y_pred > thresh))\n    specificity = calc_specificity(y_actual, y_pred, thresh)\n    print('AUC:%.3f'%auc)\n    print('accuracy:%.3f'%accuracy)\n    print('recall:%.3f'%recall)\n    print('precision:%.3f'%precision)\n    print('specificity:%.3f'%specificity)\n    print('prevalence:%.3f'%calc_prevalence(y_actual))\n    print(' ')\n    return auc, accuracy, recall, precision, specificity ","d97006fb":"thresh = 0.5","78b7728a":"# k-nearest neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\nknn=KNeighborsClassifier(n_neighbors = 100)\nknn.fit(X_train_tf, y_train)","fc8ca902":"y_train_preds = knn.predict_proba(X_train_tf)[:,1]\ny_valid_preds = knn.predict_proba(X_valid_tf)[:,1]\n\nprint('KNN')\nprint('Training:')\nknn_train_auc, knn_train_accuracy, knn_train_recall, \\\n    knn_train_precision, knn_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nknn_valid_auc, knn_valid_accuracy, knn_valid_recall, \\\n    knn_valid_precision, knn_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","cc41c902":"import warnings\nwarnings.filterwarnings('ignore')","081cbd83":"from sklearn.linear_model import LogisticRegression\nlr=LogisticRegression(random_state = 42)\nlr.fit(X_train_tf, y_train)","9bc5e2a3":"y_train_preds = lr.predict_proba(X_train_tf)[:,1]\ny_valid_preds = lr.predict_proba(X_valid_tf)[:,1]\n\nprint('Logistic Regression')\nprint('Training:')\nlr_train_auc, lr_train_accuracy, lr_train_recall, \\\n    lr_train_precision, lr_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nlr_valid_auc, lr_valid_accuracy, lr_valid_recall, \\\n    lr_valid_precision, lr_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","f66174e4":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)\nsgdc.fit(X_train_tf, y_train)","3cb80c51":"y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = sgdc.predict_proba(X_valid_tf)[:,1]\n\nprint('Stochastic Gradient Descend')\nprint('Training:')\nsgdc_train_auc, sgdc_train_accuracy, sgdc_train_recall, sgdc_train_precision, sgdc_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nsgdc_valid_auc, sgdc_valid_accuracy, sgdc_valid_recall, sgdc_valid_precision, sgdc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","d1878bda":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(X_train_tf, y_train)","3032493f":"y_train_preds = nb.predict_proba(X_train_tf)[:,1]\ny_valid_preds = nb.predict_proba(X_valid_tf)[:,1]\n\nprint('Naive Bayes')\nprint('Training:')\nnb_train_auc, nb_train_accuracy, nb_train_recall, nb_train_precision, nb_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nnb_valid_auc, nb_valid_accuracy, nb_valid_recall, nb_valid_precision, nb_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","85421150":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(max_depth = 10, random_state = 42)\ntree.fit(X_train_tf, y_train)","99db94c1":"y_train_preds = tree.predict_proba(X_train_tf)[:,1]\ny_valid_preds = tree.predict_proba(X_valid_tf)[:,1]\n\nprint('Decision Tree')\nprint('Training:')\ntree_train_auc, tree_train_accuracy, tree_train_recall, tree_train_precision, tree_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ntree_valid_auc, tree_valid_accuracy, tree_valid_recall, tree_valid_precision, tree_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","453e8781":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)","f4db0fe9":"y_train_preds = rf.predict_proba(X_train_tf)[:,1]\ny_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n\nprint('Random Forest')\nprint('Training:')\nrf_train_auc, rf_train_accuracy, rf_train_recall, rf_train_precision, rf_train_specificity =print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nrf_valid_auc, rf_valid_accuracy, rf_valid_recall, rf_valid_precision, rf_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","e74c9ca1":"from sklearn.ensemble import GradientBoostingClassifier\ngbc =GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=3, random_state=42)\ngbc.fit(X_train_tf, y_train)","d34013e8":"y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n\nprint('Gradient Boosting Classifier')\nprint('Training:')\ngbc_train_auc, gbc_train_accuracy, gbc_train_recall, gbc_train_precision, gbc_train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\ngbc_valid_auc, gbc_valid_accuracy, gbc_valid_recall, gbc_valid_precision, gbc_valid_specificity = print_report(y_valid,y_valid_preds, thresh)","17160dc0":"df_results = pd.DataFrame({'classifier':['KNN','KNN','LR','LR','SGD','SGD','NB','NB','DT','DT','RF','RF','GB','GB'],\n                           'data_set':['train','valid']*7,\n                          'auc':[knn_train_auc, knn_valid_auc,lr_train_auc,lr_valid_auc,sgdc_train_auc,sgdc_valid_auc,nb_train_auc,nb_valid_auc,tree_train_auc,tree_valid_auc,rf_train_auc,rf_valid_auc,gbc_train_auc,gbc_valid_auc,],\n                          'accuracy':[knn_train_accuracy, knn_valid_accuracy,lr_train_accuracy,lr_valid_accuracy,sgdc_train_accuracy,sgdc_valid_accuracy,nb_train_accuracy,nb_valid_accuracy,tree_train_accuracy,tree_valid_accuracy,rf_train_accuracy,rf_valid_accuracy,gbc_train_accuracy,gbc_valid_accuracy,],\n                          'recall':[knn_train_recall, knn_valid_recall,lr_train_recall,lr_valid_recall,sgdc_train_recall,sgdc_valid_recall,nb_train_recall,nb_valid_recall,tree_train_recall,tree_valid_recall,rf_train_recall,rf_valid_recall,gbc_train_recall,gbc_valid_recall,],\n                          'precision':[knn_train_precision, knn_valid_precision,lr_train_precision,lr_valid_precision,sgdc_train_precision,sgdc_valid_precision,nb_train_precision,nb_valid_precision,tree_train_precision,tree_valid_precision,rf_train_precision,rf_valid_precision,gbc_train_precision,gbc_valid_precision,],\n                          'specificity':[knn_train_specificity, knn_valid_specificity,lr_train_specificity,lr_valid_specificity,sgdc_train_specificity,sgdc_valid_specificity,nb_train_specificity,nb_valid_specificity,tree_train_specificity,tree_valid_specificity,rf_train_specificity,rf_valid_specificity,gbc_train_specificity,gbc_valid_specificity,]})","16bbad4c":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")","778939b8":"ax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=df_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\nplt.show()","4e655985":"import warnings\nwarnings.filterwarnings('ignore')","b7eb8361":"import time\n\nmy_params = [2,4,6,8,10]# fill this in your list of parameters\n\n# initialize arrays for storing the results\ntrain_metrics = np.zeros(len(my_params))\nvalid_metrics = np.zeros(len(my_params))\n\n# train a model for each param in a list\nt1 = time.time()\nfor jj in range(len(my_params)):\n    my_param = my_params[jj]\n    for i in my_params:\n    # fit model\n     model= RandomForestClassifier(max_depth = i)                                \n     model.fit(X_train_tf, y_train)\n    # get predictions\n    y_train_preds = model.predict_proba(X_train_tf)[:,1]\n    y_valid_preds = model.predict_proba(X_valid_tf)[:,1]\n\n    # calculate auc\n    metric_train = roc_auc_score(y_train, y_train_preds) # fill this in\n    metric_valid = roc_auc_score(y_valid, y_valid_preds) # fill this in\n\n    # save aucs\n    train_metrics[jj] = metric_train\n    valid_metrics[jj] = metric_valid\n    \n    # print the time\n    t2 = time.time()\n    print(my_param, t2-t1)\n    t1 = time.time()","f3af6c4b":"import matplotlib.pyplot as plt\n\nplt.plot(my_params, train_metrics,'o-',label = 'train')\nplt.plot(my_params, valid_metrics,'o-',label = 'valid')\n\nplt.xlabel('max_depth') # fill this in\nplt.ylabel('AUC')# fill this in\nplt.title('Effect of max_depth on AUC') # fill this in\nplt.legend()\nplt.show()","c284d0fc":"import numpy as np\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"\n    Generate a simple plot of the test and training learning curve.\n\n    Parameters\n    ----------\n    estimator : object type that implements the \"fit\" and \"predict\" methods\n        An object of that type which is cloned for each validation.\n\n    title : string\n        Title for the chart.\n\n    X : array-like, shape (n_samples, n_features)\n        Training vector, where n_samples is the number of samples and\n        n_features is the number of features.\n\n    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n        Target relative to X for classification or regression;\n        None for unsupervised learning.\n\n    ylim : tuple, shape (ymin, ymax), optional\n        Defines minimum and maximum yvalues plotted.\n\n    cv : int, cross-validation generator or an iterable, optional\n        Determines the cross-validation splitting strategy.\n        Possible inputs for cv are:\n          - None, to use the default 3-fold cross-validation,\n          - integer, to specify the number of folds.\n          - An object to be used as a cross-validation generator.\n          - An iterable yielding train\/test splits.\n\n        For integer\/None inputs, if ``y`` is binary or multiclass,\n        :class:`StratifiedKFold` used. If the estimator is not a classifier\n        or if ``y`` is neither binary nor multiclass, :class:`KFold` is used.\n\n        Refer :ref:`User Guide <cross_validation>` for the various\n        cross-validators that can be used here.\n\n    n_jobs : integer, optional\n        Number of jobs to run in parallel (default 1).\n    \"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"AUC\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes, scoring = 'roc_auc')\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"b\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt","4e386328":"# Cross validation with 5 iterations to get smoother mean test and train\n# score curves, each time with 20% data randomly selected as a validation set.\n\n\ntitle = \"Learning Curves (Random Forest)\"                                        # fill this in\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=42)\nestimator = RandomForestClassifier(max_depth = 6, random_state = 42)                                       # fill this in\nplot_learning_curve(estimator, title, X_train_tf, y_train, ylim=(0.2, 1.01), cv=cv, n_jobs=4)\n\nplt.show()","2f6b7e56":"from sklearn.ensemble import RandomForestClassifier\nrf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)","f71b9f55":"feature_importances = pd.DataFrame(rf.feature_importances_,\n                                   index = cols_input,\n                                    columns=['importance']).sort_values('importance',\n                                                                        ascending=False)\nfeature_importances.head()","505e67ea":"num = np.min([50, len(cols_input)])\nylocs = np.arange(num)\n# get the feature importance for top num and sort in reverse order\nvalues_to_plot = feature_importances.iloc[:num].values.ravel()[::-1]\nfeature_labels = list(feature_importances.iloc[:num].index)[::-1]\n\nplt.figure(num=None, figsize=(8, 15), dpi=80, facecolor='w', edgecolor='k');\nplt.barh(ylocs, values_to_plot, align = 'center')\nplt.ylabel('Features')\nplt.xlabel('Importance Score')\nplt.title('Feature Importance Score - Random Forest')\nplt.yticks(ylocs, feature_labels)\nplt.show()","fb3d83e7":"rf.get_params()","853c0d29":"from sklearn.model_selection import RandomizedSearchCV\n\n# number of trees\nn_estimators = range(200,1000,200)\n# maximum number of features to use at each split\nmax_features = ['auto','sqrt']\n# maximum depth of the tree\nmax_depth = range(2,20,2)\n# minimum number of samples to split a node\nmin_samples_split = range(2,10,2)\n# criterion for evaluating a split\ncriterion = ['gini','entropy']\n\n# random grid\n\nrandom_grid = {'n_estimators':n_estimators,\n              'max_features':max_features,\n              'max_depth':max_depth,\n              'min_samples_split':min_samples_split,\n              'criterion':criterion}\n\nprint(random_grid)","8ad4cfbd":"from sklearn.metrics import make_scorer, roc_auc_score\nauc_scoring = make_scorer(roc_auc_score)","56f58e99":"# create a baseline model\nrf = RandomForestClassifier()\n\n# create the randomized search cross-validation\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, \n                               n_iter = 20, cv = 2, \n                               scoring=auc_scoring,verbose = 1, random_state = 42)","23bf3b92":"import time\n# fit the random search model (this will take a few minutes)\nt1 = time.time()\nrf_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","4aff7ece":"rf_random.best_params_","96e3f964":"rf=RandomForestClassifier(max_depth = 6, random_state = 42)\nrf.fit(X_train_tf, y_train)\n\ny_train_preds = rf.predict_proba(X_train_tf)[:,1]\ny_valid_preds = rf.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline Random Forest')\nrf_train_base_auc = roc_auc_score(y_train, y_train_preds)\nrf_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(rf_train_base_auc))\nprint('Validation AUC:%.3f'%(rf_valid_base_auc))\n\nprint('Optimized Random Forest')\ny_train_preds_random = rf_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = rf_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\n\nrf_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\nrf_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(rf_train_opt_auc))\nprint('Validation AUC:%.3f'%(rf_valid_opt_auc))","d2345176":"from sklearn.linear_model import SGDClassifier\nsgdc=SGDClassifier(loss = 'log',alpha = 0.1,random_state = 42)\nsgdc.fit(X_train_tf, y_train)","668493c5":"penalty = ['none','l2','l1']\nmax_iter = range(200,1000,200)\nalpha = [0.001,0.003,0.01,0.03,0.1,0.3]\nrandom_grid_sgdc = {'penalty':penalty,\n              'max_iter':max_iter,\n              'alpha':alpha}\n# create the randomized search cross-validation\nsgdc_random = RandomizedSearchCV(estimator = sgdc, param_distributions = random_grid_sgdc, n_iter = 20, cv = 2, scoring=auc_scoring,verbose = 0, random_state = 42)\n\nt1 = time.time()\nsgdc_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","39f957f0":"sgdc_random.best_params_","3745dcc0":"y_train_preds = sgdc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = sgdc.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline sgdc')\nsgdc_train_base_auc = roc_auc_score(y_train, y_train_preds)\nsgdc_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(sgdc_train_base_auc))\nprint('Validation AUC:%.3f'%(sgdc_valid_base_auc))\n\nprint('Optimized sgdc')\ny_train_preds_random = sgdc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = sgdc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\nsgdc_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\nsgdc_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(sgdc_train_opt_auc))\nprint('Validation AUC:%.3f'%(sgdc_valid_opt_auc))","e4fb4783":"from sklearn.ensemble import GradientBoostingClassifier\ngbc =GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n     max_depth=3, random_state=42)\ngbc.fit(X_train_tf, y_train)","de2f4135":"# number of trees\nn_estimators = range(50,200,50)\n\n# maximum depth of the tree\nmax_depth = range(1,5,1)\n\n# learning rate\nlearning_rate = [0.001,0.01,0.1]\n\n# random grid\n\nrandom_grid_gbc = {'n_estimators':n_estimators,\n              'max_depth':max_depth,\n              'learning_rate':learning_rate}\n\n# create the randomized search cross-validation\ngbc_random = RandomizedSearchCV(estimator = gbc, param_distributions = random_grid_gbc, n_iter = 20, cv = 2, scoring=auc_scoring,verbose = 0, random_state = 42)\n\nt1 = time.time()\ngbc_random.fit(X_train_tf, y_train)\nt2 = time.time()\nprint(t2-t1)","85ba0e28":"gbc_random.best_params_","16062caf":"y_train_preds = gbc.predict_proba(X_train_tf)[:,1]\ny_valid_preds = gbc.predict_proba(X_valid_tf)[:,1]\n\nthresh = 0.5\n\nprint('Baseline gbc')\ngbc_train_base_auc = roc_auc_score(y_train, y_train_preds)\ngbc_valid_base_auc = roc_auc_score(y_valid, y_valid_preds)\n\nprint('Training AUC:%.3f'%(gbc_train_base_auc))\nprint('Validation AUC:%.3f'%(gbc_valid_base_auc))\nprint('Optimized gbc')\ny_train_preds_random = gbc_random.best_estimator_.predict_proba(X_train_tf)[:,1]\ny_valid_preds_random = gbc_random.best_estimator_.predict_proba(X_valid_tf)[:,1]\ngbc_train_opt_auc = roc_auc_score(y_train, y_train_preds_random)\ngbc_valid_opt_auc = roc_auc_score(y_valid, y_valid_preds_random)\n\nprint('Training AUC:%.3f'%(gbc_train_opt_auc))\nprint('Validation AUC:%.3f'%(gbc_valid_opt_auc))","d4354f78":"df_results = pd.DataFrame({'classifier':['SGD','SGD','RF','RF','GB','GB'],\n                           'data_set':['baseline','optimized']*3,\n                          'auc':[sgdc_valid_base_auc,sgdc_valid_opt_auc,\n                                 rf_valid_base_auc,rf_valid_opt_auc,\n                                 gbc_valid_base_auc,gbc_valid_opt_auc],\n                          })","19fa70ca":"df_results","8e0b8eb2":"import seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set(style=\"darkgrid\")\n\nax = sns.barplot(x=\"classifier\", y=\"auc\", hue=\"data_set\", data=df_results)\nax.set_xlabel('Classifier',fontsize = 15)\nax.set_ylabel('AUC', fontsize = 15)\nax.tick_params(labelsize=15)\n# Put the legend out of the figure\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0., fontsize = 15)\n\nplt.show()","96898e27":"pickle.dump(rf_random.best_estimator_, open('best_classifier.pkl', 'wb'),protocol = 4)","04f20ac1":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pickle\n\n# load the model, columns, mean values, and scaler\nbest_model = pickle.load(open('best_classifier.pkl','rb'))\ncols_input = pickle.load(open('cols_input.sav','rb'))\ndf_mean_in = pd.read_csv('df_mean.csv', names =['col','mean_val'])\nscaler = pickle.load(open('scaler.sav', 'rb'))","2e924046":"# load the data\ndf_train = pd.read_csv('df_train.csv')\ndf_valid= pd.read_csv('df_valid.csv')\ndf_test= pd.read_csv('df_test.csv')","06c2eec0":"# fill missing\ndf_train = fill_my_missing(df_train, df_mean_in, cols_input)\ndf_valid = fill_my_missing(df_valid, df_mean_in, cols_input)\ndf_test = fill_my_missing(df_test, df_mean_in, cols_input)\n\n# create X and y matrices\nX_train = df_train[cols_input].values\nX_valid = df_valid[cols_input].values\nX_test = df_test[cols_input].values\n\ny_train = df_train['OUTPUT_LABEL'].values\ny_valid = df_valid['OUTPUT_LABEL'].values\ny_test = df_test['OUTPUT_LABEL'].values\n\n# transform our data matrices \nX_train_tf = scaler.transform(X_train)\nX_valid_tf = scaler.transform(X_valid)\nX_test_tf = scaler.transform(X_test)","6165742a":"y_train_preds = best_model.predict_proba(X_train_tf)[:,1]\ny_valid_preds = best_model.predict_proba(X_valid_tf)[:,1]\ny_test_preds = best_model.predict_proba(X_test_tf)[:,1]","5f0084dd":"thresh = 0.5\n\nprint('Training:')\ntrain_auc, train_accuracy, train_recall, train_precision, train_specificity = print_report(y_train,y_train_preds, thresh)\nprint('Validation:')\nvalid_auc, valid_accuracy, valid_recall, valid_precision, valid_specificity = print_report(y_valid,y_valid_preds, thresh)\nprint('Test:')\ntest_auc, test_accuracy, test_recall, test_precision, test_specificity = print_report(y_test,y_test_preds, thresh)","65bbd9a5":"from sklearn.metrics import roc_curve \n\nfpr_train, tpr_train, thresholds_train = roc_curve(y_train, y_train_preds)\nauc_train = roc_auc_score(y_train, y_train_preds)\n\nfpr_valid, tpr_valid, thresholds_valid = roc_curve(y_valid, y_valid_preds)\nauc_valid = roc_auc_score(y_valid, y_valid_preds)\n\nfpr_test, tpr_test, thresholds_test = roc_curve(y_test, y_test_preds)\nauc_test = roc_auc_score(y_test, y_test_preds)\n\nplt.plot(fpr_train, tpr_train, 'r-',label ='Train AUC:%.3f'%auc_train)\nplt.plot(fpr_valid, tpr_valid, 'b-',label ='Valid AUC:%.3f'%auc_valid)\nplt.plot(fpr_test, tpr_test, 'g-',label ='Test AUC:%.3f'%auc_test)\nplt.plot([0,1],[0,1],'k--')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.legend()\nplt.show()","78ba83a4":"Calculate prediction probabilities","b30d9362":"Create a training (df_train_all), validation (df_valid) and test (df_test) set. ","7f4d52ee":"# Building Training\/Validation\/Test Samples","4f8559d7":"Plot the feature importance for random forest model.","8935dfa4":"get_dummies function does not work on numerical data. To trick get_dummies, we can convert the numerical data into strings and then it will work properly. ","4b9f228a":"Prevalence regarding this project suggests that 20.2% of the people will not show up for the scheduled appointment.  ","4b10714e":"Looking at the above graph, we can notice that the most important feature is num_days which we have created to know the number of days between the appointment day and the scheduled day. Similarly, week, in particular, month and age seem to be more important features. Seeing the important features it conveys that features like Handicap, Alcoholic, Gender don't have much of impact on the modelling.","f9646c82":"Gap between days looks fine.\n3. Calculate week of the day, month, week respectively when the appointment has been scheduled.","1167ba83":"To use the RandomizedSearchCV function, we need something to score or evaluate a set of hyperparameters. Here we will use the auc","b865d4a1":"It is a classification technique which follows the rules of Bayes theorem. Classifier performs best with the categorical variables and for numerical variables with assumption to be normally distributed. The basic principle of na\u00efve Bayes classifier works on the assumption that for a feature in a class is not related to any other feature. It means every single feature contributes independently to the probability of the output variable.","0103d8b5":"### K nearest neighbors (KNN)","21872b99":"Plot the ROC curve","8e5c9a4e":"Optimize Gradient Boosting Tree Classifier","8c3174a8":"Optimization SGDClassifier","5b92370f":"# Feature Engineering","b494a0b8":"My current best model is: Random Forest","9e18b5e3":"Training random forest model and evaluate the performance","cd28f779":"Selecting performace parameter as AUC. Bar plot of this performance metric below to demonstrate the baseline performance. ","a7932266":"# Conclusion","abc11958":"Evaluate the Performance","cc662914":"### Decision Tree Classifier","785f1d8b":"Now we are going to evaluate the performance of the best model on our test set. Note that to execute the code below. We need to execute the cells for the functions we defined above calc_prevalence, fill_my_missing, calc_specificity,print_report","1b7e8d24":"With this hospital appointment data, analyzing whether the people who made the appointment will no show.","fd871fcb":"Let's analyze the performance of the best model compared to the baseline model.","0ead0f75":"Steps to follow for data exploration \n\n1) load dataset into a dataframe (df)\n\n2) Defining an OUTPUT_LABEL\n\n3) Calculating the prevalence of the positive class","7f3cea1f":"Make a new dataframe that only has the columns of interest. Double check that the columns used to define your OUTPUT_LABEL are not part of cols_input! ","c12d6f6a":"Picking up hyperparameter max_depth for range of values and analyzing it.","50f4ce2d":"Trainig naive bayes model and evaluate the performance","30a35fc2":"Create a scalar, save it, and scale the X matrices","2aa95166":"#Numerical categorical variables\nHandcap is one of the feature which says whether the patient is handicapped or not. If patient is handicapped, till what degree(0,1,2,3,4)","5da060a5":"Calculate the mean value from the training data.","a6cf164c":"#Categorical Features","23cc7f69":"Using RandomizedSearchCV, optimize a few of your baseline models. \nNote that GradientBoosting Classifier may take a while so you might need to adjust the number of iterations or specific parameters. If this takes too long on your computer, feel free to take that one out. \n    \n    \nRandom Forest Optimaization","74b1e46a":"Explore the columns and unique values of your dataset. ","e8a838d3":"Training dataset is basically used for train the dataset and fit various models before the validation and test dataset.\nValidation dataset is used for used to improve the modelling and tuning model hyperparameters.\nTest dataset is used to implement the final model and get to the final conclusion .It provides unbaised evaluation of final model fit on the training dataset.","ee19ed35":"Balancing is a nececessary part of modelling otherwise imbalance modelling can lead to accurarte results which are suspicious and faulty.\nWe're going to use sub-sample method for the more dominant class: use a random subset of the negatives. As we have a significant count of positive samples. We'll be creating the 50-50 negative and positive ratio. Take df_train_all and create a balanced dataset","125a29bf":"### Naive Bayes","b7690cbe":"Chekcing for the Unique values across each column for each column","61cad477":"The above learning curve suggests that there is high bias as training and validation curves are very closer to each other. Hence, it is the case of underfitting. We can do listed below techniques to overcome high bias\nHigh Bias:\n- Add new features\n- Increase model complexity\n- Reduce regularization\n- Change model architecture ","e1739bdc":"Random forest is a collaboration of many decision trees combined on a platform or bag. The concept of bagging is done to make a decision tree for different features all together. Concept of controlling the best split is by sub setting when features are higher in number. Through this, more unique trees are formed and reduce the correlation between trees.","c8909a04":"Gradient boost classifier is one of the highest rated machine learning technique and more frequently used technique. The algorithm used to improve the errors of the earlier used decision tree, which is included in the next tree to improve the learning rate. Learning rate decides how quick the error is fixed in every decision tree.","98d3c7f9":"Filling any missing values with the mean value","8cdd12e1":"# Introduction","95024cea":"Training stochastic gradient descent model and evaluate the performance","614e1e9e":"This dataset id obtained from kaggle with 100k observations and 14 variables. https:\/\/www.kaggle.com\/joniarroba\/noshowappointments\nDataset has different types of features\n1. Appointment Id - unique,\n2. Patient Id- unique,\n3. Date - Appointment - datetime,\n4. Date - Scheduled - datetime,\n5. Age - numerical, \n6. Gender - categorical,\n7. Alchololic - categorical,\n8. Hypertension - categorical,\n9. Diabetes - categorical,\n10. Handicap - ordinal,\n11. SMS recieved - categorical,\n12. Neighbourhood - ordinal,\n13. Scholarship - categorical,\n14. No-show - target,","0db38c14":"Since we balanced our training data, let's set our threshold at 0.5 to label a predicted sample as positive. ","543d138e":"Age looks fine now. Moving on to the next feature.\n2. Number of days between Appointment made and Appointment scheduled. Thinking logically it can be impactful feature as if the gap between the appointment and scheduled days is high, it can result in no show.","532b80c3":"Healthcare industry is one of the biggest industries over the globe. Everyday there are enthusiastic individuals are working to make healthcare better. Our objective should be in the same direction  to make earth a better place.\nMy objective is to analyze the a hospital data for the appointments made over a period of time in year 2016\nThe chosen data is from a particular hospital in Brazil which contains all the appointments made by the people from May-June 2016. An AppointmentId is generated with all the details of the patient like age, gender, recieved SMS or not and more variables.We'll attempt to mine insights and produce a predictive model that can be used as the first step to solve the problem. Analyzing what can be reason behind the No-show for appointments by the people. \n","d21c4566":"Note that in this case, we donot get a boost on the Validation AUC. \nIt looks like this Optimized Random Forest was better. Let's try another model, Gradient Boosting.","5ae12ad1":"After analyzing all the models, we selected Random forest as our best model and optimized it. It predicted 80% of the no-show appointments correctly.\nModel correctly identified that 30% of the time patients will not show up for appointment. It is 1.5 times better than randomly guessing.\nPeople who do not show up for the appointment beacuse of several factors like long waiting period for appointment day, confirmation of SMS, Age factor. If more features are provided like longitude and lattitude, alogrithm have given more better result. ","d54e1afd":"Let's pick the AUC and compare performance of different models on the basis of that.","9cc49f30":"Three important parameters of `RandomizedSearchCV` are\n- scoring = evaluation metric used to pick the best model\n- n_iter = number of different combinations\n- cv = number of cross-validation splits\n\nincreasing the last two of these will increase the run-time, but will decrease chance of overfitting. Note that the number of variables and grid size also influences the runtime. Cross-validation is a technique for splitting the data multiple times to get a better estimate of the performance metric. For the purposes of this tutorial, we will restrict to 2 CV to reduce the time","6c958727":"# Data Exploration","dee0f608":"# Model Selection ","2d3cea56":"## Analyze results baseline models","7b993cb0":"Make a plot comparing the performance of the optimized models to the baseline models. ","991c9945":"### Stochastic Gradient Descent","d9333253":"Selecting the current best model as Random Forest Classifier as it gives AUC of 0.724 for training and 0.722 for test data. ","438c6678":"We have AUC of 0.688 that catches 63.5% of the no show appointments when we used threshold of 0.5","b1ec634b":"# Dataset","04fcce08":"Training KNN and evaluate performance","639413f2":"## Feature Importance","18853354":"### Logistic Regression","1cd69c5e":"It doesn't seems right. Difference cannot be negative, it was just because of same day appointment. So, making -1 values to zero.","dd346ec0":"# Model Evaluation","5a255533":"Note that in this case, we do get a boost of 0.07 on the Validation AUC. It looks like this Optimized Random Forest has some underfitting. Let's try some other models like SGD and Gradient Boosting.","a51445bf":"Training logistic regression and evaluate the performance","73eab0c2":"It seems both the models(Random Forest and Gradient BoostingClassifier) performed almost equal but RF Classifier has an edge in validation AUC over GB Classsifier. My best model pick is Random Forest, let's save the model using the package pickle.","17617628":"### Random Forest","c5837981":"From the above dataset we can describe that there are couple of categorical features, ordinal features and some features like PatientId, AppointmentId, Neighbourhood are not that meaningful to consider for modelling.","8043f7bb":"### Gradient Boosting Classifier","99c4f3ab":"K-Nearest Neighbors is classification technique which computes between the data points with training data point. It picks up the nearest datapoint and does most of the class selection with the most common data points. It selects the value of optimal k. \n","81d44ec6":"Training gradient boosting model and evaluate the performance","0febf92b":"The decision tree is a type of supervised learning method which works on a tree-like structure while making decisions considering the different features. Each node of the tree is subdivided into two more categories of Yes and No which represents some feature conclusion until there is a dead end. The simplest way to reach the decision which is mostly relatable to real life. ","6ccdbaf8":"Calculate the prevalence of the positive class","cdba4706":"Let's check if there are any missing values in the numerical data.","63e39903":"See the best parameters","f8701d32":"Create a column called OUTPUT_LABEL based on your data that is 0 for your negative class and 1 for your positive class","ac70c01b":"Stochastic gradient descent is an enhanced version of logistic regression which uses the same gradient descent functionality. Here is contradictory, unlike the batch gradient descent which calculates errors only after every training data is executed. The stochastic model calculates error after every training sample and updates the next one by one. It is fast, and the rate of improvement is good.   ","e790af43":"Check a few things to catch known bugs. ","854b8a61":"## Model Selection: baseline models","5b4ef1f6":"#Numerical Features\n1. Age feature","8e3e71cb":"## Hyperparameter tuning","bd8ec384":"To add the one-hot encoding columns to the dataframe we can use concat function. Make sure to use axis = 1 to indicate add the columns.","496663ce":"Age cannot be negative. So, removing the negative values.","9667c0d2":"Let's verify that we used all the data.","97a86eae":"Training decision tree model and evaluate the performance","d76f7269":"## Learning Curves","8d0db65f":"Let's check if we are missing any data. ","d61bfa45":"Save all 4 dataframes to csv and the cols_input","e484fce1":"# Project Definition","459e1785":"Logistic regression is a machine learning modelling method to predict when the outcome variable is categorical. The basic principle is to maximize the likelihood and minimize the log cost function through the gradient descent with iterations. This helps in predicting the likelihood that the incident is going to happen or not with different parameters like AUC, prevalence, precision, recall.","9e8c7426":"Let's make a dataframe with these results and plot the outcomes using a package called seaborn.","7fefde37":"Using your baseline model that has the best performance on the validation set, plot a learning curve for that model. we will make use of the learning curve code from scikit-learn's website with a small change of plotting the AUC instead of accuracy.","733c6355":"Finally combining all the numerical features as cols_num","6e84feaa":"## Pick your best model"}}