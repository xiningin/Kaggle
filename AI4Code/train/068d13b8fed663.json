{"cell_type":{"b9da2e6a":"code","5e316e20":"code","87cc8cd0":"code","9f017657":"code","866be95c":"code","4ec4726e":"code","064aa7d5":"code","c8a07c17":"code","d3e6e6b7":"code","f8443062":"code","9e57976b":"code","0c80772c":"code","66843fb3":"code","5ee41c97":"code","3e359d6d":"code","b88cdd1d":"code","19ed7e5a":"code","a81809a5":"code","f2a517a7":"code","2dd0859b":"code","f2dbe9a4":"code","5771748c":"code","971e081d":"code","02fd0778":"code","aa069c55":"code","47605fd4":"code","dde40998":"code","600469d3":"code","daf74e39":"code","e7eb11fc":"markdown","d382e9b0":"markdown","13d04ee4":"markdown","51c42576":"markdown","edb821c2":"markdown"},"source":{"b9da2e6a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n#import data\ndata = pd.read_csv('\/kaggle\/input\/questions.csv')","5e316e20":"#check slice of data\ndata.head()","87cc8cd0":"# distribution of data for different class\ndata.is_duplicate.value_counts()","9f017657":"print('length of data: ', len(data))\nprint('shape of data: ', data.shape)\n#no of entries in data is 405k which is quiet high \n#so we can perform analysis on subset of data to save excecution time.","866be95c":"# taking subset of data \ndf = data[:50000] \n","4ec4726e":"# checking Nan values if any\ndf.isna().sum()\n#we dont have any Nan values in subset of data","064aa7d5":"#printing few question pairs in the datset\nfor i in range(0,20):\n    print(df.question1[i])\n    print(df.question2[i])\n    print()","c8a07c17":"from string import punctuation\nimport scipy #library for scientific calculations\nimport datetime\nimport nltk\nfrom sklearn import re\nfrom sklearn import pipeline\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report, f1_score, accuracy_score","d3e6e6b7":"# Cleaning text in question\n\n# This list is so extensive that we can look back at it as a reference when \n# needed for other regular expression related work.\n\nSPECIAL_TOKENS = {'non-ascii': 'non_ascii_word'}\n\ndef normalized_text(text, stem_words=True):\n    def pad_str(s):\n        return ' '+s+' '\n    \n    if pd.isnull(text):  #If null\n        return ''\n\n    # Empty question\n    \n    if type(text) != str or text=='':  #if text type is not string\n        return ''\n\n    # Clean the text\n    text = re.sub(\"\\'s\", \" \", text) # we have cases like \"Sam is\" or \"Sam's\" (i.e. his) these two cases aren't separable, I choose to compromise are kill \"'s\" directly\n    text = re.sub(\" whats \", \" what is \", text, flags=re.IGNORECASE) # replace whats by what is and ignore case\n    text = re.sub(\"\\'ve\", \" have \", text) # replace 've by have\n    text = re.sub(\"can't\", \"can not\", text) # replace can't by can not\n    text = re.sub(\"n't\", \" not \", text) # replace n't by not\n    text = re.sub(\"i'm\", \"i am\", text, flags=re.IGNORECASE) # replace i'm by i am and ignore case\n    text = re.sub(\"\\'re\", \" are \", text) # replace 're by are\n    text = re.sub(\"\\'d\", \" would \", text) # replace 'd by would\n    text = re.sub(\"\\'ll\", \" will \", text) # replace 'll by will\n    text = re.sub(\"e\\.g\\.\", \" eg \", text, flags=re.IGNORECASE) # replace e.g. by eg and ignore case\n    text = re.sub(\"b\\.g\\.\", \" bg \", text, flags=re.IGNORECASE) # replace b.g. by bg and ignore case\n    text = re.sub(\"(\\d+)(kK)\", \" \\g<1>000 \", text) \n    text = re.sub(\"e-mail\", \" email \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?U\\.S\\.A\\.\", \" America \", text, flags=re.IGNORECASE)\n    text = re.sub(\"(the[\\s]+|The[\\s]+)?United State(s)?\", \" America \", text, flags=re.IGNORECASE)\n    text = re.sub(\"\\(s\\)\", \" \", text, flags=re.IGNORECASE)\n    text = re.sub(\"[c-fC-F]\\:\\\/\", \" disk \", text)\n    \n    # remove comma between numbers, i.e. 15,000 -> 15000\n    text = re.sub('(?<=[0-9])\\,(?=[0-9])', \"\", text)\n    \n    # add padding to punctuations and special chars, we still need them later\n    text = re.sub('\\$', \" dollar \", text)\n    text = re.sub('\\%', \" percent \", text)\n    text = re.sub('\\&', \" and \", text)\n    \n    text = re.sub('[^\\x00-\\x7F]+', pad_str(SPECIAL_TOKENS['non-ascii']), text) \n    \n    # Indian Currency\n    text = re.sub(\"(?<=[0-9])rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(\" rs(?=[0-9])\", \" rs \", text, flags=re.IGNORECASE)\n    \n    # cleaning text rules from : https:\/\/www.kaggle.com\/currie32\/the-importance-of-cleaning-text\n    text = re.sub(r\" (the[\\s]+|The[\\s]+)?US(A)? \", \" America \", text)\n    text = re.sub(r\" UK \", \" England \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" india \", \" India \", text)\n    text = re.sub(r\" switzerland \", \" Switzerland \", text)\n    text = re.sub(r\" china \", \" China \", text)\n    text = re.sub(r\" chinese \", \" Chinese \", text) \n    text = re.sub(r\" imrovement \", \" improvement \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" intially \", \" initially \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" quora \", \" Quora \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dms \", \" direct messages \", text, flags=re.IGNORECASE)  \n    text = re.sub(r\" demonitization \", \" demonetization \", text, flags=re.IGNORECASE) \n    text = re.sub(r\" actived \", \" active \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" kms \", \" kilometers \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" cs \", \" computer science \", text, flags=re.IGNORECASE) \n    text = re.sub(r\" upvote\", \" up vote\", text, flags=re.IGNORECASE)\n    text = re.sub(r\" iPhone \", \" phone \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" \\0rs \", \" rs \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" calender \", \" calendar \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" ios \", \" operating system \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" gps \", \" GPS \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" gst \", \" GST \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" programing \", \" programming \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" bestfriend \", \" best friend \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" dna \", \" DNA \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" III \", \" 3 \", text)\n    text = re.sub(r\" banglore \", \" Banglore \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J K \", \" JK \", text, flags=re.IGNORECASE)\n    text = re.sub(r\" J\\.K\\. \", \" JK \", text, flags=re.IGNORECASE)\n    \n    # replace the float numbers with a random number\n    \n    text = re.sub('[0-9]+\\.[0-9]+', \" 87 \", text)\n  \n    #Removing Punctuations\n    text = [word for word in text if word not in punctuation]\n    text = ''.join(text)\n    text = text.lower()\n       \n    # Return a list of words\n    return text","f8443062":"#applying text cleaning function to question text\ndf['question1'] = df['question1'].apply(normalized_text)\ndf['question2'] = df['question2'].apply(normalized_text)","9e57976b":"#checking the cleaned text to see changes made\nfor i in range(0,20):\n    print(df.question1[i])\n    print(df.question2[i])\n    print()","0c80772c":"#data head\ndf.head()","66843fb3":"#using word vector of word_count and frequency withpout capturing the meaning of word\n#r'\\w{1,}' indiactes 1 or more word\n\nCV = CountVectorizer(analyzer='word', stop_words='english', token_pattern=r'\\w{1,}')\nq1_trans = CV.fit_transform(df['question1'].values)\nq2_trans = CV.fit_transform(df['question2'].values)\n    ","5ee41c97":"#scipy.sparse.hstack will stack sparse matrix columnwise, and stacking them side by side\n\nX = scipy.sparse.hstack((q1_trans, q2_trans))\ny = df.is_duplicate.values    ","3e359d6d":"#splitting traing set and test set for training and validating model for classification\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)","b88cdd1d":"#gradient Boosting Model used\n#start time\nst = datetime.datetime.now()\n\nclassifier1 = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \nobjective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n#fitting the model\nprint(classifier1.fit(X_train, y_train))\n#predicting if pair is duplicate or not\nprediction_CV = classifier1.predict(X_test)\n\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_CV))\nprint(\"Accuracy score: \\n\", accuracy_score(y_test, prediction_CV))\nprint(\"Classification report:\\n\", classification_report(y_test, prediction_CV))\nprint(\"F1 Score:\\n \",f1_score(y_test, prediction_CV))\n\net = datetime.datetime.now()\nprint(\"Code run-time: \", et-st)\n","19ed7e5a":"#TF-IDF vectorizer \n#5000 features were used for tfidf vectorizer\n#r'\\w{1,}'  indicates more than 1 word\n\ntfidf = TfidfVectorizer(analyzer='word', max_features=5000, token_pattern=r'\\w{1,}')\n\nq1word_trans = tfidf.fit_transform(df['question1'].values)\nq2word_trans = tfidf.fit_transform(df['question2'].values)\n\nX = scipy.sparse.hstack((q1word_trans,q2word_trans))\ny = df.is_duplicate.values\n","a81809a5":"# train-test-split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)","f2a517a7":"# Xg Boost classifier for word level vectorizer\n\nst = datetime.datetime.now()\n\nclassifier2 = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \nobjective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n#fitting the model with traing data\nprint(classifier2.fit(X_train, y_train))\n\n#predicting the test data\nprediction_tfidf = classifier2.predict(X_test)\n\n#Performance evaluation\nprint(\"Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\nprint(\"Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\nprint(\"Classification report:\\n\", classification_report(y_test, prediction_tfidf))\nprint(\"F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n\net = datetime.datetime.now()\nprint(\"Code run-time: \", et-st)","2dd0859b":"#TF-IDF ngram level vectorizer \n#5000 features were used for tfidf vectorizer\n#r'\\w{1,}'  indicates more than 1 word\n#ngram_range = (1,3) means 2 and 3 features are used\n\ntfidf = TfidfVectorizer(analyzer='word',ngram_range=(1,3), max_features=5000, token_pattern=r'\\w{1,}')\n\nq1ngram_trans = tfidf.fit_transform(df['question1'].values)\nq2ngram_trans = tfidf.fit_transform(df['question2'].values)\n\nX = scipy.sparse.hstack((q1ngram_trans,q2ngram_trans))\ny = df.is_duplicate.values","f2dbe9a4":"# train-test-split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)","5771748c":"# Xg Boost classifier for ngram_range=(1,3) level vectorizer\n\nst = datetime.datetime.now()\n\nclassifier3 = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \nobjective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n#fitting the model with traing data\nprint(classifier3.fit(X_train, y_train))\n\n#predicting the test data\nprediction_tfidf = classifier3.predict(X_test)\n\n#Performance evaluation\nprint(\"ngram_range Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\nprint(\"ngram_range Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\nprint(\"ngram_range Classification report:\\n\", classification_report(y_test, prediction_tfidf))\nprint(\"ngram_range F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n\net = datetime.datetime.now()\nprint(\"Code run-time: \", et-st)","971e081d":"#TF-IDF ngram level vectorizer \n#5000 features were used for tfidf vectorizer\n#r'\\w{1,}'  indicates more than 1 word\n#ngram_range = (2,3) means 2 and 3 features are used\n\ntfidf = TfidfVectorizer(analyzer='word',ngram_range=(2,3), max_features=5000, token_pattern=r'\\w{1,}')\n\nq1ngram_trans = tfidf.fit_transform(df['question1'].values)\nq2ngram_trans = tfidf.fit_transform(df['question2'].values)\n\nX = scipy.sparse.hstack((q1ngram_trans,q2ngram_trans))\ny = df.is_duplicate.values","02fd0778":"# train-test-split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)","aa069c55":"# Xg Boost classifier for ngram_range=(1,3) level vectorizer\n\nst = datetime.datetime.now()\n\nclassifier4 = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \nobjective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n#fitting the model with traing data\nprint(classifier4.fit(X_train, y_train))\n\n#predicting the test data\nprediction_tfidf = classifier4.predict(X_test)\n\n#Performance evaluation\nprint(\"ngram_range(2,3) Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\nprint(\"ngram_range(2,3) Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\nprint(\"ngram_range(2,3) Classification report:\\n\", classification_report(y_test, prediction_tfidf))\nprint(\"ngram_range(2,3) F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n\net = datetime.datetime.now()\nprint(\"Code run-time: \", et-st)","47605fd4":"#TF-IDF ngram level vectorizer \n#5000 features were used for tfidf vectorizer\n#r'\\w{1,}'  indicates more than 1 word\n#ngram_range = (1,3) means 2 and 3 features are used\n#char level analyzer is used \n\ntfidf = TfidfVectorizer(analyzer='char',ngram_range=(1,3), max_features=5000, token_pattern=r'\\w{1,}')\n\nq1char_trans = tfidf.fit_transform(df['question1'].values)\nq2char_trans = tfidf.fit_transform(df['question2'].values)\n\nX = scipy.sparse.hstack((q1char_trans,q2char_trans))\ny = df.is_duplicate.values","dde40998":"# train-test-split\nX_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.30, random_state = 42)","600469d3":"# Xg Boost classifier for char level vectorizer\n\nst = datetime.datetime.now()\n\nclassifier5 = XGBClassifier(max_depth=50, n_estimators=80, learning_rate=0.1, colsample_bytree=.7, gamma=0, reg_alpha=4, \nobjective='binary:logistic', eta=0.3, silent=1, subsample=0.8)\n#fitting the model with traing data\nprint(classifier5.fit(X_train, y_train))\n\n#predicting the test data\nprediction_tfidf = classifier5.predict(X_test)\n\n#Performance evaluation\nprint(\"char level Confusion Matrix:\\n\", confusion_matrix(y_test, prediction_tfidf))\nprint(\"char level Accuracy score: \\n\", accuracy_score(y_test, prediction_tfidf))\nprint(\"char level Classification report:\\n\", classification_report(y_test, prediction_tfidf))\nprint(\"char level F1 Score:\\n \",f1_score(y_test, prediction_tfidf))\n\net = datetime.datetime.now()\nprint(\"Code run-time: \", et-st)","daf74e39":"### Challenge was to create the model which can classify between similar question asked on quora##\n\n### text processing used is so extensive that we can use it as a reference when \n# needed for other regular expression related work.###\n\n### Initially Bag of Words model is used for vectorization with XGBOOST for classification\n\n### TFIDF vectorizer at word level, n-Gram level, Char level is used.\n\n### Without sentiment capturing, best accuracy of 75.73% is achieved using \n###extreme gradient boosting on character level TF-IDF....","e7eb11fc":"*** ****TF-IDF (word level) + XGBOOST\n**","d382e9b0":"* ******ngram level TFIDF + XGBOOST","13d04ee4":"**Conclusion**","51c42576":"* 1. * *** Bag of Words + XGBOOST**\n","edb821c2":"*** Char level TFIDF + XGBOOST****"}}