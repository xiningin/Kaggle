{"cell_type":{"7fea53c5":"code","d7c70fd5":"code","6a130539":"code","d97a8913":"code","3033435d":"code","6e462517":"code","f6c7028d":"code","9beda544":"code","98263ebb":"code","e5c2f8a2":"code","911ae7c5":"code","32f7a68e":"code","7f2678f7":"code","3813fe8c":"code","9dab7e10":"code","06ac3f53":"code","8bbf8b4b":"code","1f1c4e88":"code","733438db":"code","7b104686":"code","657d390e":"code","a2a280ff":"code","4285dde5":"code","d02ddac9":"code","86b39c8a":"code","3368a49a":"code","ca30c320":"code","ac7edb5c":"code","aea1a62a":"code","e400e958":"code","2f2632b0":"code","805728ae":"code","516b1438":"code","5f3e0b01":"code","1da3f195":"code","16a32cd3":"code","50526da7":"code","fffdd76c":"code","4bbbf3e9":"code","5ab4b499":"code","5a51e198":"code","dec46ef8":"code","98c6dcc5":"code","be9cf70f":"code","8eac0cb8":"code","f60f31e0":"code","d6688918":"code","56db4387":"code","adaa405a":"code","5d8f6c87":"markdown","db3dcab7":"markdown","a29f4eb2":"markdown","83ecf0e4":"markdown","6004549b":"markdown","261ab240":"markdown","273e6343":"markdown","2b874ac8":"markdown","c6023aa6":"markdown","4f61c944":"markdown"},"source":{"7fea53c5":"pip install textblob","d7c70fd5":"pip install wordcloud","6a130539":"pip install nltk","d97a8913":"from warnings import filterwarnings\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom nltk.corpus import stopwords\nfrom nltk.sentiment import SentimentIntensityAnalyzer\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, cross_validate\nfrom sklearn.preprocessing import LabelEncoder\nfrom textblob import Word, TextBlob\nfrom wordcloud import WordCloud\n\n\nfilterwarnings('ignore')\npd.set_option('display.max_columns', None)\npd.set_option('display.width', 200)\npd.set_option('display.float_format', lambda x: '%.2f' % x)","3033435d":"df = pd.read_csv(\"..\/input\/amazon-reviews\/amazon_reviews.csv\", sep=\",\")\ndf.head()","6e462517":"df.info()","f6c7028d":"# Normalizing Case Folding\ndf['reviewText'] = df['reviewText'].str.lower()\n\n# Punctuations\ndf['reviewText'] = df['reviewText'].str.replace('[^\\w\\s]', '')\n\n# Numbers\ndf['reviewText'] = df['reviewText'].str.replace('\\d', '')","9beda544":"# Stopwords\n\n# nltk.download('stopwords')\nsw = stopwords.words('english')\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in str(x).split() if x not in sw))","98263ebb":"# Rarewords\n\ndrops = pd.Series(' '.join(df['reviewText']).split()).value_counts()[-1000:]\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join(x for x in x.split() if x not in drops))","e5c2f8a2":"# Tokenization\n\n# nltk.download(\"punkt\")\ndf[\"reviewText\"].apply(lambda x: TextBlob(x).words).head()","911ae7c5":"# Lemmatization\n\n# Kelimeleri k\u00f6klerine ay\u0131rma i\u015flemidir.\n# nltk.download('wordnet')\ndf['reviewText'] = df['reviewText'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()]))\n\ndf['reviewText'].head(10)","32f7a68e":"# Terim Frekanslar\u0131n\u0131n Hesaplanmas\u0131\n\ntf = df[\"reviewText\"].apply(lambda x: pd.value_counts(x.split(\" \"))).sum(axis=0).reset_index()\n\ntf.columns = [\"words\", \"tf\"]\ntf.head()","7f2678f7":"tf.shape","3813fe8c":"tf[\"words\"].nunique()","9dab7e10":"tf[\"tf\"].describe([0.05, 0.10, 0.25, 0.50, 0.75, 0.80, 0.90, 0.95, 0.99]).T","06ac3f53":"# Barplot\n\ntf[tf[\"tf\"] > 500].plot.bar(x=\"words\", y=\"tf\")\nplt.show()","8bbf8b4b":"# Wordcloud\n\ntext = \" \".join(i for i in df.reviewText)\nwordcloud = WordCloud().generate(text)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()","1f1c4e88":"# daha a\u00e7\u0131k renkli bir grafik\nwordcloud = WordCloud(max_font_size=50,\n                      max_words=100,\n                      background_color=\"white\").generate(text)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n\nwordcloud.to_file(\"wordcloud.png\")","733438db":"# NLTK already has a built-in, pretrained sentiment analyzer\n# called VADER (Valence Aware Dictionary and sEntiment Reasoner).\n\ndf.head()","7b104686":"# nltk.download('vader_lexicon')\nsia = SentimentIntensityAnalyzer()","657d390e":"sia.polarity_scores(\"The film was awesome\")","a2a280ff":"sia.polarity_scores(\"I liked this music but it is not good as the other one\")","4285dde5":"# mesela review'lar\u0131 b\u00fcy\u00fcltmek istersek:\ndf[\"reviewText\"].apply(lambda x: x.upper())","d02ddac9":"# \u015fimdi skorlar\u0131 hesaplayal\u0131m mesela 10 tanesi i\u00e7in\ndf[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x))","86b39c8a":"# peki bu s\u00f6zl\u00fck i\u00e7erisinden sadece bir bile\u015feni se\u00e7mek istersek ne yapaca\u011f\u0131z?\ndf[\"reviewText\"][0:10].apply(lambda x: sia.polarity_scores(x)[\"compound\"])","3368a49a":"# i\u015flemi kal\u0131c\u0131 olarak yapal\u0131m:\ndf[\"polarity_score\"] = df[\"reviewText\"].apply(lambda x: sia.polarity_scores(x)[\"compound\"])\ndf.head()","ca30c320":"# Feature Engineering\n\n# Target'\u0131n Olu\u015fturulmas\u0131\ndf[\"reviewText\"][0:10].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")","ac7edb5c":"# \u015fimdi t\u00fcm veri i\u00e7in ayn\u0131 i\u015flemi yap\u0131p veri setinin i\u00e7ine sentiment_label ad\u0131nda bir de\u011fi\u015fken ekleyelim:\ndf[\"sentiment_label\"] = df[\"reviewText\"].apply(lambda x: \"pos\" if sia.polarity_scores(x)[\"compound\"] > 0 else \"neg\")\ndf.head(20)","aea1a62a":"# dengesiz veri problemimiz var m\u0131 bir s\u0131n\u0131f da\u011f\u0131l\u0131m\u0131na bakal\u0131m\ndf[\"sentiment_label\"].value_counts()","e400e958":"# bir soru daha merak etti\u011fim \u015fey \u015fu verilen puanlar a\u00e7\u0131s\u0131ndan neg-pos labelleri aras\u0131nda fark var m\u0131?\ndf.groupby(\"sentiment_label\")[\"overall\"].mean()","2f2632b0":"# target'\u0131n encode edilmesi\ndf[\"sentiment_label\"] = LabelEncoder().fit_transform(df[\"sentiment_label\"])\n\nX = df[\"reviewText\"]\ny = df[\"sentiment_label\"]","805728ae":"\n# ngram\na = \"\"\"Bu \u00f6rne\u011fi anla\u015f\u0131labilmesi i\u00e7in daha uzun bir metin \u00fczerinden g\u00f6sterece\u011fim.\nN-gram'lar birlikte kullan\u0131lan kelimelerin kombinasyolar\u0131n\u0131 g\u00f6sterir ve feature \u00fcretmek i\u00e7in kullan\u0131l\u0131r\"\"\"\n\nTextBlob(a).ngrams(3)","516b1438":"# Count Vectors\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ncorpus = ['This is the first document.',\n          'This document is the second document.',\n          'And this is the third one.',\n          'Is this the first document?']","5f3e0b01":"# word frekans\nvectorizer = CountVectorizer()\nX_c = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\nX_c.toarray()","1da3f195":"# n-gram frekans\nvectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\nX_n = vectorizer2.fit_transform(corpus)\nvectorizer2.get_feature_names()\nX_n.toarray()","16a32cd3":"# Veriye uygulanmas\u0131:\nvectorizer = CountVectorizer()\nX_count = vectorizer.fit_transform(X)\n\nvectorizer.get_feature_names()[10:15]\nX_count.toarray()[10:15]","50526da7":"# word tf-idf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer='word')\nX_w = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\nX_w.toarray()","fffdd76c":"# n-gram tf-idf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(ngram_range=(2, 3))\nX_n = vectorizer.fit_transform(corpus)\nvectorizer.get_feature_names()\nX_n.toarray()","4bbbf3e9":"# Veriye uygulanmas\u0131:\ntf_idf_word_vectorizer = TfidfVectorizer()\nX_tf_idf_word = tf_idf_word_vectorizer.fit_transform(X)","5ab4b499":"# Logistic Regression\n\nlog_model = LogisticRegression().fit(X_tf_idf_word, y)\n\ncross_val_score(log_model,\n                X_tf_idf_word,\n                y, scoring=\"accuracy\",\n                cv=5).mean()\n\nyeni_yorum = pd.Series(\"this product is great\")\nyeni_yorum = pd.Series(\"look at that shit very bad\")\nyeni_yorum = pd.Series(\"it was good but I am sure that it fits me\")\n\nyeni_yorum = CountVectorizer().fit(X).transform(yeni_yorum)\nlog_model.predict(yeni_yorum)","5a51e198":"# orjinal yorumlardan modele sorabilir miyiz?\nrandom_review = pd.Series(df[\"reviewText\"].sample(1).values)\nrandom_review","dec46ef8":"yeni_yorum = CountVectorizer().fit(X).transform(random_review)\nlog_model.predict(yeni_yorum)","98c6dcc5":"# Count Vectors\nrf_model = RandomForestClassifier().fit(X_count, y)\ncross_val_score(rf_model, X_count, y, cv=5, n_jobs=-1).mean()","be9cf70f":"# TF-IDF Word-Level\nrf_model = RandomForestClassifier().fit(X_tf_idf_word, y)\ncross_val_score(rf_model, X_tf_idf_word, y, cv=5, n_jobs=-1).mean()","8eac0cb8":"# TF-IDF N-GRAM\n#rf_model = RandomForestClassifier().fit(X_tf_idf_ngram, y)\n#cross_val_score(rf_model, X_tf_idf_ngram, y, cv=5, n_jobs=-1).mean()","f60f31e0":"# Hiperparametre Optimizasyonu\n\nrf_model = RandomForestClassifier(random_state=17)\n\nrf_params = {\"max_depth\": [5, 8, None],\n             \"max_features\": [5, 7, \"auto\"],\n             \"min_samples_split\": [2, 5, 8, 20],\n             \"n_estimators\": [100, 200, 500]}\n\nrf_best_grid = GridSearchCV(rf_model,\n                            rf_params,\n                            cv=5,\n                            n_jobs=-1,\n                            verbose=True).fit(X_count, y)\n\nrf_best_grid.best_params_\n\nrf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X_count, y)\n\ncv_results = cross_validate(rf_final, X_count, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","d6688918":"rf_best_grid.best_params_","56db4387":"rf_final = rf_model.set_params(**rf_best_grid.best_params_, random_state=17).fit(X_count, y)","adaa405a":"cv_results = cross_validate(rf_final, X_count, y, cv=3, scoring=[\"accuracy\", \"f1\", \"roc_auc\"])","5d8f6c87":"# TF-IDF","db3dcab7":"# 5. Modeling","a29f4eb2":"# 4. Sentiment Modeling","83ecf0e4":"# 1. Text Preprocessing","6004549b":"<div style=\"display:fill;\n            border-radius: false;\n            border-style: solid;\n            border-color:#000000;\n            border-style: false;\n            border-width: 2px;\n            color:#CF673A;\n            font-size:15px;\n            font-family: Georgia;\n            background-color:#E8DCCC;\n            text-align:center;\n            letter-spacing:0.1px;\n            padding: 0.1em;\">\n\n**<h2>\u2661 Thank you for taking the time \u2661**","261ab240":"# 3. Sentiment Analysis","273e6343":"# 2. Text Visualization","2b874ac8":"# Random Forests","c6023aa6":"![1TURvrYWSTRQLGF6sJ025gw.png](attachment:0d2a7c72-9699-4220-b328-4bf30bb8515a.png)","4f61c944":"# Introduction to Text Mining and Natural Language Processing"}}