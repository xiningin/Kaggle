{"cell_type":{"83ba2cd1":"code","cc9a3839":"code","448d17ca":"code","27f0b1d5":"code","040522c8":"code","149b4883":"code","aaca7492":"code","03992535":"code","7ed3c81f":"code","42fd92a0":"code","5669ec10":"code","565edd10":"code","10bc0aa9":"code","4d401767":"code","c3ecb97c":"code","d4cbfa99":"code","acd0cb73":"code","c63569f0":"code","b45ba79e":"code","40b4c3d7":"code","20df20da":"code","29ea9139":"code","bb806c06":"code","4b548185":"code","a245d076":"code","0dc0c982":"markdown","aa355c4a":"markdown","634ce3ba":"markdown","7801f1ad":"markdown","ddc7b9e4":"markdown","2cbeff89":"markdown","28beaf89":"markdown","ebc12abe":"markdown","0b24e7d2":"markdown","0fca9981":"markdown"},"source":{"83ba2cd1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nimport re # Regular expression\nimport nltk # text processing\nfrom collections import namedtuple, defaultdict\nimport numbers\nfrom wordcloud import WordCloud\n\n# Plotting library\nimport matplotlib.pyplot as plt\n% matplotlib inline\n\nfrom sklearn.feature_extraction import stop_words\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer # text processing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression,SGDClassifier\nfrom sklearn.metrics import roc_curve, roc_auc_score, auc, accuracy_score, log_loss\n\nimport scipy.sparse as sp\nfrom nltk.stem import WordNetLemmatizer \n\nimport gc\n\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cc9a3839":"class BoW(object):\n    '''\n    Calculates frequencies of tokens across documents using Bag Of Words from scikit-learn\n    '''                 \n    def __init__(self):\n        self.vect = None\n        self.bow = None\n        \n    def sort_vocabulary(self, order=-1):\n        '''\n        Sorts the vocabulary in a given order\n        \n        Parameters\n        -------------\n        sort_order: {integer}, sorts the vocabulary.If order = -1, sort in descending\n        '''\n        tfs = np.array(self.bow.sum(axis=0)).ravel()\n        sort_indx = (order * tfs).argsort()\n        \n        labels = list()\n        values = list()\n        terms = list(self.vect.vocabulary_.keys())\n        indices = list(self.vect.vocabulary_.values())\n        for index in sort_indx:\n            labels.append(terms[indices.index(index)])\n            values.append(tfs[index])\n            \n        return labels, values\n        \n    def fit(self, corpus, token_pattern):\n        self.vect = CountVectorizer(token_pattern=token_pattern)\n        self.bow = self.vect.fit_transform(corpus)\n        \n    def clean(self):\n        self.vect = None\n        self.bow = None\n        \ndef search_pattern(token_pattern, corpus):\n    pat = re.compile(token_pattern)\n    matches = corpus.apply(lambda doc: re.findall(pat, doc))\n    return matches\n        \ndef create_vocab(tokens, ascending=None):\n    '''\n    Creates a vocabulary (key, value) pairs of words to frequencies\n    \n    Parameters\n    --------------\n    token_list : {array of array} - list of list of tokens\n    '''\n    \n    counter = defaultdict()\n    counter.default_factory = counter.__len__\n\n    for doc in tokens:\n        for token in doc:\n            counter[token] += 1\n            \n    counter = dict(counter)\n    if ascending is None:\n        return counter\n    \n    if ascending is False:\n        return sorted(counter.items(), key=lambda x: x[1], reverse=True)\n    return sorted(counter.items(), key=lambda x: x[1])\n\ndef plot_barchart(labels, values, chart_params, figsize=(10, 4), horizontal=False):\n    '''\n    Plots a barchart\n    \n    Parameters\n    -----------------\n    labels : {array}, labels on the x-axis\n    values : {array}, values on the y-axis\n    chart_params : {namedtuple}, chart configurations\n    '''\n    fig, ax = plt.subplots(figsize=figsize)\n    y_pos = np.arange(len(values))\n    if horizontal is False:\n        ax.bar(y_pos, values, chart_params.width, color=chart_params.colors)\n        ax.set_xticks(y_pos)\n        ax.set_xticklabels(labels)\n    else:\n        ax.barh(y_pos, values, chart_params.width, color=chart_params.colors)\n        ax.set_yticks(y_pos)\n        ax.set_yticklabels(labels)\n    \n    ax.set_title(chart_params.title, fontdict={'size': chart_params.title_size})\n    plt.show()\n\ndef gen_cloud_data(corpus):\n    '''\n    Generates a word cloud data\n    \n    Parameters\n    ------------\n    corpus : {array} - Array of array of tokens\n    '''\n    digit_cloud = ' '\n    for row in corpus:\n        digit_cloud = digit_cloud + '_' + re.sub(r'\\s+', '_', row) + ' '\n    return digit_cloud\n\ndef plot_wordcloud(cloud_data, stops, params, figsize=(9, 7)):\n    '''\n    Plots a word cloud from array of documents\n    '''\n    all_stops = stop_words.ENGLISH_STOP_WORDS.union(stops)\n    wordcloud = WordCloud(background_color ='white',\n                    stopwords = all_stops,\n                    max_words = 400,\n                    max_font_size = 120, \n                    random_state = 42).generate(cloud_data)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(wordcloud)\n    plt.title(params.title, fontdict={\n        'size': params.title_size,\n        'color': 'green'\n    })\n    plt.axis(\"off\")\n    plt.show()\n    \ndef print_statements_from_index(indx_list, N=5):\n    for i in range(N):\n        indx = np.random.choice(indx_list)\n        print('Index -> {}'.format(indx))\n        print(train_df.loc[indx, 'question_text'])\n    \n# Declare namedtuples\n\nChartParams = namedtuple('ChartParams', ['title', 'title_size'])\n\nBarChartParams = namedtuple('BarChart', ['title', 'title_size', 'width', 'colors'])","448d17ca":"def get_uppers(tokens):\n    return tokens.apply(lambda row: np.array(row)[np.where([token.isupper() for token in row])[0]])\n\ndef get_digits(tokens):\n    return tokens.apply(lambda row: np.array(row)[np.where([token.isdigit() for token in row])[0]])\n\ndef get_alphanumerics(tokens):\n    return tokens.apply(lambda row: [token for token in row if re.match(r\"([a-zA-Z]+\\d+|\\d+[a-zA-Z]+)\", token) is not None])\n\ndef get_currency(tokens):\n    return tokens.apply(lambda row: [token for token in row if re.match(r\"[$]\\d\\d+[a-z]+$\", token) is not None])\n\ndef get_hyperlinks(tokens):\n    return tokens.apply(lambda row: [token for token in row if re.match(r'\\bhttps?[:\/\/].*[a-zA-Z0-9-&_.]$\\b', token) is not None])\n\ndef get_masked_tokens(tokens, low=None, high=None):\n    if high is None and low is None:\n        return tokens\n    \n    masked = np.zeros(len(tokens), dtype=bool)\n    \n    if low is not None:\n        masked = tokens.apply(len) > low\n    if high is not None:\n        masked &= tokens.apply(len) <= high\n        \n        \n    return tokens[np.where(masked)[0]]","27f0b1d5":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\nprint(\"Total records in train data = {0}\".format(train_df.shape[0]))\nprint(\"Total records in test data = {0}\".format(test_df.shape[0]))","040522c8":"tokens = train_df.question_text.apply(lambda doc: doc.split())\nuppers = get_uppers(tokens)\ndigits = get_digits(tokens)\nalphanums = get_alphanumerics(tokens)\ncurrencies = get_currency(tokens)","149b4883":"N = 10\nnum_pat = r'\\b19\\d{2}|20\\d{2}\\b'\nbow = BoW()\nbow.fit(train_df.question_text, num_pat)\nlabels, values = bow.sort_vocabulary()\nplot_barchart(labels[:N], values[:N], BarChartParams('Top {} numbers'.format(N), 20, 0.5, 'm'))\n\nbow = None\ngc.collect()","aaca7492":"cloud_data = gen_cloud_data(np.hstack(get_masked_tokens(digits)))\nplot_wordcloud(cloud_data, set(), ChartParams('Most common numbers', 30))\ngc.collect()","03992535":"N = 10\nalphaVocab = create_vocab(alphanums, ascending=True)[:N]\nplot_barchart([i[0] for i in alphaVocab], [i[1] for i in alphaVocab], BarChartParams('Top {} alphanumerics'.format(N), 20, 0.5, 'r'))","7ed3c81f":"N = 10\nbar_data = create_vocab(get_masked_tokens(currencies, 0), ascending=True)[:10]\nplot_barchart([i[0] for i in bar_data], [i[1] for i in bar_data], BarChartParams('Top {} currencies'.format(N), 20, 0.5, '#d8db2e'))\n","42fd92a0":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(15, 7))\n\nax[0][0].hist(get_masked_tokens(tokens, 3, 10).apply(len) , color='#4f2951')\nax[0][1].hist(get_masked_tokens(uppers, 4, 10).apply(len), color='#13b230')\nax[1][0].hist(get_masked_tokens(digits, 1, 10).apply(len), color='#3c2ed3')\nax[1][1].hist(get_masked_tokens(alphanums, 1, 10).apply(len), color='#d8db2e')\nplt.show()","5669ec10":"hyperlinks_series = get_hyperlinks(tokens)\nhyperlinks = get_masked_tokens(hyperlinks_series, 0)","565edd10":"hyperlinks_df = pd.DataFrame({\n    'links': hyperlinks\n})\nhyperlinks_df['domain'] = hyperlinks_df.links.apply(lambda links: [re.match(r\"(https?\\:\\\/\\\/).*\", link).group(1) for link in links])\nhyperlinks_df['url_first'] = hyperlinks_df.links.apply(lambda links: [re.match(r\"https?\\:\\\/\\\/([a-zA-Z0-9.-]+)\\\/?.*\", link).group(1) for link in links if re.match(r\"https?\\:\\\/\\\/([\\w.])\\\/?\", link) is not None])\nhyperlinks_df['query_params'] = hyperlinks_df.links.apply(lambda links: [re.match(r\"\\bhttps?\\:\\\/\\\/.*[?](.*)\\b\", link).group(1) for link in links if re.match(r\"\\bhttps?\\:\\\/\\\/.*[?](.*)\\b\", link) is not None])\n","10bc0aa9":"N = 60\nbar_data = create_vocab(hyperlinks_df['url_first'], ascending=True)[30:N]\nplot_barchart([i[0] for i in bar_data], [i[1] for i in bar_data], BarChartParams(''.format(N), 20, 0.5, '#d8db2e'), figsize=(10, 8), horizontal=True)\n","4d401767":"def replace_urls(doc, url_pattern):\n    return re.sub(url_pattern, r'\\1', doc)\n\ndef replace_months(doc):\n    return re.sub(r'(jan|feb|march|april|may|june|july|august|september|october|november|december)', r'month', doc)\n\ndef preprocess(doc):\n    doc = replace_months(doc)\n    doc = re.sub(r'(youtu)[.](be)', r'\\1\\2', doc)\n    doc = re.sub(r'i[.](imgur)', r'\\1', doc)\n    doc = re.sub(r'\\b\\s{2}\\b', r'', doc)\n    return doc","c3ecb97c":"train_df['preprocessed_text'] = train_df.question_text.apply(lambda doc: preprocess(doc))","d4cbfa99":"# Custom stop words list\n\nX = train_df.preprocessed_text\ny = train_df.target\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.33, random_state=42)\n","acd0cb73":"STOPS = set(['to'])\n\ndef filter_tokens(tokens):\n    \n#     tokens = [t for t in tokens if t not in STOPS]\n    original_tokens = list(tokens)\n    return tokens\n\nclass CustomVectorizer(TfidfVectorizer):\n    def build_tokenizer(self):\n        tokenize = super(CustomVectorizer, self).build_tokenizer()\n        return lambda doc: list(filter_tokens(tokenize(doc)))","c63569f0":"MAX_FEATURES = 10000\ncount_vect = CustomVectorizer(min_df=5, max_features=MAX_FEATURES, lowercase=False)\ncount_vect.fit(X_train)\ndtm = count_vect.transform(X_train)\n\nclf = LogisticRegression()\nclf.fit(dtm, y_train)","b45ba79e":"def sort_bow_vocab(vect, dtm, order=-1):\n    tfs = np.asarray(dtm.sum(axis=0)).ravel()\n    sorted_indices = (order * tfs).argsort()\n\n    terms = list(vect.vocabulary_.keys())\n    indices = list(vect.vocabulary_.values())\n    \n    labels = []\n    values = []\n    for i in sorted_indices:\n        values.append(tfs[i])\n        labels.append(terms[indices.index(i)])\n        \n    return labels, values\n\nlabels, values = sort_bow_vocab(count_vect, dtm, order=1)\n\nplot_barchart(labels[:10], values[:10], BarChartParams('Top {} words'.format(10), 20, 0.5, 'm'))","40b4c3d7":"X_test_vectors = count_vect.transform(X_test)\npredictions = clf.predict(X_test_vectors)\n\nfalse_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, predictions)\nroc_auc = auc(false_positive_rate, true_positive_rate)\nprint('AUC: ', roc_auc_score(y_test, predictions))\nprint('Accuracy: ', accuracy_score(y_test, predictions))","20df20da":"fig, ax = plt.subplots(figsize=(8, 4))\nax.plot(false_positive_rate, true_positive_rate, color='green', lw=2, label='ROC curve (area = %0.5f)' % roc_auc)\nax.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.show()","29ea9139":"# AUC:  0.7077320472912008\n# Accuracy:  0.9541437656169885","bb806c06":"!ls -l '..\/input\/embeddings'","4b548185":"# from gensim.models import KeyedVectors\n\n# EMBEDDINGS = '..\/input\/embeddings\/'\n# embeddings_index = KeyedVectors.load_word2vec_format(os.path.join(EMBEDDINGS, 'GoogleNews-vectors-negative300\/GoogleNews-vectors-negative300.bin'), binary=True)","a245d076":"# test_predictions.loc[:, 'qid'] = test_df.loc[:, 'qid']\n# test_predictions.to_csv('submission.csv', index=False)","0dc0c982":"## Define functions ","aa355c4a":"\n\n\n## Load data","634ce3ba":"## 2. Alphanumerics","7801f1ad":"## Feature extraction functions","ddc7b9e4":"># Keras","2cbeff89":"## Extract features from text","28beaf89":"## 4. Bar plots for extracted features\n>* Token lengths\n>* Uppercase tokens\n>* Digits\n>* Alphanumerics","ebc12abe":"## 1.  Numbers","0b24e7d2":">As our dataset contains sentences we have to convert them in a format that can be learned by machine learning models.One such format is the well known bow(bag of words) model where each document is a vector of frequency count of words in the document.\n\n>>There are different ways to calculate the frequency distribution of words in text.One such library is [scikit-learn](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html).\n\n> <strong><font size=\"3\">[Bag-of-Words](https:\/\/scikit-learn.org\/stable\/modules\/feature_extraction.html#the-bag-of-words-representation)<\/font><\/strong>","0fca9981":"## 3. Currency "}}