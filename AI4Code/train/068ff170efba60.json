{"cell_type":{"e83cc7f8":"code","729943f3":"code","63d63875":"code","21f6e4ec":"code","41f5b884":"code","9e82b226":"code","22bcd9b9":"code","68db42f0":"markdown","029452cf":"markdown","28689792":"markdown","7033a73f":"markdown","9b62c449":"markdown","a049d342":"markdown"},"source":{"e83cc7f8":"from gensim.corpora import Dictionary\nfrom gensim.models import TfidfModel, LsiModel\nfrom multiprocessing import Pool\nimport sqlite3 as sql\nimport pandas as pd\nimport numpy as np\nimport logging\nimport time\nimport re\n\ndb = '..\/input\/english-wikipedia-articles-20170820-sqlite\/enwiki-20170820.db'\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)","729943f3":"def get_query(select, db=db):\n    '''\n    1. Connects to SQLite database (db)\n    2. Executes select statement\n    3. Return results and column names\n    \n    Input: 'select * from analytics limit 2'\n    Output: ([(1, 2, 3)], ['col_1', 'col_2', 'col_3'])\n    '''\n    with sql.connect(db) as conn:\n        c = conn.cursor()\n        c.execute(select)\n        col_names = [str(name[0]).lower() for name in c.description]\n    return c.fetchall(), col_names\n\ndef tokenize(text, lower=True):\n    '''\n    1. Strips apostrophes\n    2. Searches for all alpha tokens (exception for underscore)\n    3. Return list of tokens\n\n    Input: 'The 3 dogs jumped over Scott's tent!'\n    Output: ['the', 'dogs', 'jumped', 'over', 'scotts', 'tent']\n    '''\n    text = re.sub(\"'\", \"\", text)\n    if lower:\n        tokens = re.findall('''[a-z_]+''', text.lower())\n    else:\n        tokens = re.findall('''[A-Za-z_]''', text)\n    return tokens\n\ndef get_article(article_id):\n    '''\n    1. Construct select statement\n    2. Retrieve all section_texts associated with article_id\n    3. Join section_texts into a single string (article_text)\n    4. Tokenize article_text\n    5. Return list of tokens\n    \n    Input: 100\n    Output: ['the','austroasiatic','languages','in',...]\n    '''\n    select = '''select section_text from articles where article_id=%d''' % article_id\n    docs, _ = get_query(select)\n    docs = [doc[0] for doc in docs]\n    doc = '\\n'.join(docs)\n    tokens = tokenize(doc)\n    return tokens\n       \nclass Corpus():\n    def __init__(self, article_ids, dictionary, tfidf):\n        self.article_ids = article_ids\n        self.len = len(article_ids)\n        self.dictionary = dictionary\n        self.tfidf = tfidf\n\n    def __iter__(self):\n        article_ids = np.random.choice(self.article_ids, self.len, replace=False)\n        with Pool(processes=4) as pool:\n            docs = pool.imap_unordered(get_article, article_ids)\n            for doc in docs:\n                yield tfidf[dictionary.doc2bow(doc)]\n\n    def __len__(self):\n        return self.len","63d63875":"select = '''select distinct article_id from articles'''\narticle_ids, _ = get_query(select)\narticle_ids = [article_id[0] for article_id in article_ids]","21f6e4ec":"dictionary = Dictionary.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_trimmed.dict')\ntfidf = TfidfModel(dictionary=dictionary)","41f5b884":"start = time.time()\n# To keep training time reasonable, let's just look at a random 10K section text sample.\nsample_article_ids = np.random.choice(article_ids, 10000, replace=False)\ndocs = Corpus(sample_article_ids, dictionary, tfidf)\nlsa = LsiModel(docs, num_topics=200, id2word=dictionary)\nend = time.time()\nprint('Time to train LSA from generator: %0.2fs' % (end - start))","9e82b226":"for i in range(10):\n    print('Topic %d' % i)\n    print(lsa.print_topic(i))","22bcd9b9":"lsa = LsiModel.load('..\/input\/english-wikipedia-articles-20170820-models\/enwiki_2017_08_20_lsa.model')","68db42f0":"# Tutorial: Latent Semantic Indexing\nThis is a basic guide to efficiently training a LSI\/LSA model on the English Wikipedia dump using Gensim.","029452cf":"Now let's train a LSA model. We'll do a single pass, and feed tokens from full articles to build the topic model.","28689792":"Gensim offers built-in functionality for examining topics. Note that we only trained on a subset of the articles, so these topics are likely to be a little incoherent.","7033a73f":"First step, grab the index we'll be iterating over. In this case, we want to use section text, so let's use the implicit column: **rowid**.","9b62c449":"Instead, let's look at a pre-trained LSA model.","a049d342":"Now we'll load the pre-trained\/trimmed dictionary from earlier and also create a TF-IDF model."}}