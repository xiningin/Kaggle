{"cell_type":{"fe76b121":"code","304c0fef":"code","d8253098":"code","abb653eb":"code","ff04c86a":"code","5ca4a7d6":"code","1e34b0d5":"code","f65ff6dd":"code","03db6252":"code","d8b52fde":"code","c1d3c219":"code","0f2d3021":"code","3645d542":"code","0a584244":"markdown","c6864d18":"markdown","6ede32eb":"markdown","26df15b9":"markdown","288df415":"markdown"},"source":{"fe76b121":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","304c0fef":"import numpy as np\nimport pandas as pd\nimport statistics\nimport matplotlib.pyplot as plt\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, KFold,cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport os\nprint(os.listdir(\"..\/input\"))\n\nhousing_data = pd.read_csv('..\/input\/housing.csv',sep=\",\")\nhousing_data.head()","d8253098":"housing_data.shape\nhousing_data.isnull().any()","abb653eb":"housing_data.describe()","ff04c86a":"# Applying mean normalization on features\ndef meannormalize(x):\n    return (x - x.mean())\/x.std()\n\nhousing_data = housing_data.apply(meannormalize)","5ca4a7d6":"housing_data.describe()","1e34b0d5":"corr_df = housing_data.corr()\n\nprint(corr_df)\nplt.matshow(corr_df)","f65ff6dd":"#plt.scatter(housing_data['LSTAT'],housing_data['MEDV'],c=\"b\")\n#plt.show()\nplt.figure(figsize=(20, 5))\nfeatures = ['LSTAT','PTRATIO','RM']\nfor i, col in enumerate(features):\n    plt.subplot(1, len(features) , i+1)\n    x = housing_data[col]\n    y = housing_data['MEDV']\n    plt.scatter(x, y, marker='o')\n    plt.title(col)\n    plt.xlabel(col)\n    plt.ylabel('MEDV')","03db6252":"X = housing_data[['LSTAT','RM','PTRATIO']]\ny = housing_data['MEDV'].values\n\n#X = X.reshape(-1,1)\ny = y.reshape(-1,1) \n","d8b52fde":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=5)\n\n# Create linear regression object\nregr = LinearRegression()\n\n# Train the model using the training sets\nregr.fit(X_train, y_train)","c1d3c219":"# Make predictions using the training set first\ndf_y_train_pred = regr.predict(X_train)\n\n# The mean squared error, lower the value better it is. 0 means perfect prediction\nprint(\"Mean squared error of training set: %.2f\"%\n      mean_squared_error(y_train, df_y_train_pred))\n\n# Explained variance score: 1 is perfect prediction\nprint('R2 variance score of training set: %.2f' % r2_score(y_train, df_y_train_pred))\n\n\n\n# Make predictions using the testing set\ndf_y_pred = regr.predict(X_test)\n\n# print('Coefficients: \\n', regr.coef_)\n# print('Intercept: \\n', regr.intercept_)\n\n# The mean squared error, lower the value better it is. 0 means perfect prediction\nprint(\"Mean squared error of testing set: %.2f\"%\n      mean_squared_error(y_test, df_y_pred))\n\n# Explained variance score: 1 is perfect prediction\nprint('R2 Variance score of testing set: %.2f' % r2_score(y_test, df_y_pred))\n\n#calculating adjusted r2\nN = y_test.size\np = X_train.shape[1]\nadjr2score = 1 - ((1-r2_score(y_test, df_y_pred))*(N - 1))\/ (N - p - 1)\nprint(\"Adjusted R^2 Score %.2f\" % adjr2score)\n\n# Plot outputs\n#ax = plt.scatter(X_test['RM'], y_test,color='DarkBlue', label='Group 1');\n\n#plt.scatter(X_test['LSTAT'], y_test, color='DarkGreen', label='Group 2');\n\n# plt.scatter(X_test, y_test,  color='black')\n# plt.plot(X_test, df_y_pred, color='red', linewidth=1)\n\n# plt.xticks(())\n# plt.yticks(())\n\n# plt.show()","0f2d3021":"from sklearn.preprocessing import PolynomialFeatures\n\ndef create_polynomial_regression_model(degree):\n    #\"Creates a polynomial regression model for the given degree\"\n    poly_features = PolynomialFeatures(degree=degree)\n    # transforms the existing features to higher degree features.\n    X_train_poly = poly_features.fit_transform(X_train)\n    # fit the transformed features to Linear Regression\n    poly_model = LinearRegression()\n    poly_model.fit(X_train_poly, y_train)\n    # predicting on training data-set\n    y_train_predicted = poly_model.predict(X_train_poly)\n    \n    # predicting on test data-set\n    y_test_predict = poly_model.predict(poly_features.fit_transform(X_test))\n    \n    # evaluating the model on training dataset\n    \n    rmse_train = np.sqrt(mean_squared_error(y_train, y_train_predicted))\n    r2_train = r2_score(y_train, y_train_predicted)\n    \n    # evaluating the model on test dataset\n    rmse_test = np.sqrt(mean_squared_error(y_test, y_test_predict))\n    r2_test = r2_score(y_test, y_test_predict)\n    \n    print(\"\\n\")\n    print(\"The model performance for the training set\")\n    print(\"-------------------------------------------\")\n    print(\"RMSE of training set is {}\".format(rmse_train))\n    print(\"R2 score of training set is {}\".format(r2_train))\n    \n    print(\"\\n\")\n    \n    print(\"The model performance for the test set\")\n    print(\"-------------------------------------------\")\n    print(\"RMSE of test set is {}\".format(rmse_test))\n    print(\"R2 score of test set is {}\".format(r2_test))\n    print(\"\\n\")","3645d542":"create_polynomial_regression_model(3)","0a584244":"# Applying Mean Normalization on feature set","c6864d18":"# describing housing_data for basic statistic metrics","6ede32eb":"# Applying Scikit learn Linear Regression based on 3 independent columns 'RM','LSAT','PTRATIO' to predict value of dependent variable 'MEDV'","26df15b9":"**Applying Polynomial Regression on dataset**","288df415":"# Checking if any columns have null data"}}