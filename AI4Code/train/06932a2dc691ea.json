{"cell_type":{"c06abff0":"code","3234c06e":"code","d31a4e24":"code","6530340c":"code","24f503e3":"code","61a4f285":"code","6ad293c3":"markdown","e20b157e":"markdown","ff7b0171":"markdown","947b3758":"markdown","a6d98aaf":"markdown"},"source":{"c06abff0":"import pandas as pd\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n\ndataset = pd.read_csv(\"\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndataset.head()","3234c06e":"# divido i dati relativi agli features (attributi) dalle classi (Outcome)\nfeature_cols = ['Pregnancies', 'Insulin', 'BMI', 'Age','Glucose','BloodPressure','DiabetesPedigreeFunction']\nx = dataset[feature_cols] # features\ny = dataset.Outcome # classi\n# divido il dataset in training set e test set\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) # 70% training and 30% test","d31a4e24":"# creo il classificatore DecisionTreeClassifier\ndtc = DecisionTreeClassifier()\n# training del classificatore\ndtc = dtc.fit(x_train,y_train)\n# eseguo la predizione dei dati del test set\ny_pred = dtc.predict(x_test)\n# calcolo l'accuracy\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","6530340c":"from sklearn.tree import export_graphviz\nfrom sklearn.externals.six import StringIO  \nfrom IPython.display import Image \n!pip install pydotplus\nimport pydotplus\n\ndot_data = StringIO()\nexport_graphviz(dtc, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True,feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('diabetes.png')\nImage(graph.create_png())","24f503e3":"# creo il classificatore DecisionTreeClassifier\ndtc_optimized = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n# training del classificatore\ndtc_optimized = dtc_optimized.fit(x_train,y_train)\n# eseguo la predizione dei dati del test set\ny_pred = dtc_optimized.predict(x_test)\n# calcolo l'accuracy per il classificatore ottimizzato\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))","61a4f285":"\ndot_data = StringIO()\nexport_graphviz(dtc_optimized, out_file=dot_data,  \n                filled=True, rounded=True,\n                special_characters=True, feature_names = feature_cols,class_names=['0','1'])\ngraph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \ngraph.write_png('diabetes.png')\nImage(graph.create_png())","6ad293c3":"Possiamo notare come l'albero ottenuto sia molto esteso e di difficile comprensione, ora procederemo ad ottimizzare le performance del classificatore.","e20b157e":"# Classificazione con alberi di decisione\n\nNel seguente notebook verr\u00e0 creato un albero di decisione per eseguire una classificazione tra pazienti diabetici e non.\n\n## Dataset\nIl dataset utilizzato \u00e8 Pima Indians Diabetes Database, contiene dati relativi a 768 pazienti, per ognuno dei quali sono presenti 8 diversi attributi:\n- Pregnancies\t\n- Glucose\t\n- BloodPressure\t\n- SkinThickness\t\n- Insulin\t\n- BMI\t\n- DiabetesPedigreeFunction\n- Age\n- Outcome: la classificazione, 1 per i pazienti diabetici 0 altrimenti\n","ff7b0171":"## Visualizzazione dell'albero ottenuto\nIl seguente codice si occupa di mostrare l'albero di decisione creato. La funzione export_graphviz si occupa di traformare il classificatore (dtc) in un file dot che succesivamente viene convertito in un immagine grazie al package pydotplus.","947b3758":"## Ottimizzazione\nPossiamo incrementare le performance del classificatore cambiando alcuni parametri, analizzando la [documentazione](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.tree.DecisionTreeClassifier.html), troviamo i seguneti parametri per il classificatore:\n\n- criterion : consente di utilizzare un criterio differente per la selezione degli attributi. Sono disponibili i valori \u201cgini\u201d per scegliere gli attributi in base al Gini index e \u201centropy\u201d per scegliere gli attributi in base all'information gain.\n\n- splitter : consente di scegliere la strategia di divisione (split strategy). I valori disponibili sono \u201cbest\u201d per scegliere una strategia ottima o \u201crandom\u201d per scegliere una strategia random.\n\n- max_depth : Questo paramtro permette di impostare un limite alla profondit\u00e0 massima dell'albero. Se viene settato a None, allora i nodi vengono espansi fino a trovare foglie pure. Utilizzando valori alti, quindi sviluppando l'abero in profondit\u00e0 rischiamo l'overfitting, mentre con valori troppo bassi rischiamo di ottenere un classificatore sottospecializzato (underfitting).","a6d98aaf":"Possiamo notare come sia migliorata la situazione, potando l'albero abbiamo incrementato l'accuracy circa del 10%. "}}