{"cell_type":{"b85326ef":"code","9222d1ad":"code","d052b3bf":"code","512d543f":"code","1de51160":"code","8865878c":"code","363465b5":"code","120620cd":"code","eb674add":"code","0aa5aaf1":"code","a87162b9":"code","141ea2af":"code","2fde08e5":"code","bddcc5b2":"code","4157a913":"code","2d7c40e3":"code","2164b107":"code","ca9649cf":"code","68c6e13d":"code","ca317c82":"code","8efed538":"code","38adcf14":"code","666555cc":"code","6b9cd23a":"code","fd153fc4":"code","c24ae6d8":"code","fe116327":"code","d5ed3e5b":"code","36ab66d6":"code","483020da":"code","29508820":"code","4ce5dbaf":"code","9a53e96f":"code","7657ab75":"code","aa7a01f4":"code","bb8a6c9e":"code","e7764041":"code","fc0c1a76":"code","41cbee4a":"code","9cb1b912":"code","66d72d09":"code","5add1739":"code","a2564ccc":"code","dfe12c8b":"code","54ce542b":"code","e72999da":"code","7480202c":"code","57d8737e":"code","d7c4944c":"code","f53fdfcc":"code","d29de0eb":"markdown","65b68ad6":"markdown","765202e2":"markdown","0887d02a":"markdown","745bbe4b":"markdown","fcb24e3e":"markdown","8e9ad388":"markdown"},"source":{"b85326ef":"import numpy as np\nimport pandas as pd\nimport pydicom\nimport os\nimport random\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import KFold","9222d1ad":"import tensorflow as tf\nimport tensorflow.keras.backend as K\nimport tensorflow.keras.layers as L\nimport tensorflow.keras.models as M","d052b3bf":"import warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom plotly.subplots import make_subplots\nimport plotly.express as px\nimport plotly.graph_objects as go","512d543f":"def seed_everything(seed=2020):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n    \nseed_everything(42)","1de51160":"ROOT = \"..\/input\/osic-pulmonary-fibrosis-progression\"\nBATCH_SIZE= 128","8865878c":"train = pd.read_csv(f\"{ROOT}\/train.csv\")\ntrain.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\ntest = pd.read_csv(f\"{ROOT}\/test.csv\")\n\nprint(\"add infos\")\nsub = pd.read_csv(f\"{ROOT}\/sample_submission.csv\")\nsub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\nsub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\nsub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\nsub = sub.merge(test.drop('Weeks', axis=1), on=\"Patient\")","363465b5":"train.head()","120620cd":"test.head()","eb674add":"train['WHERE'] = 'train'\ntest['WHERE'] = 'val'\nsub['WHERE'] = 'test'\ndata = train.append([test, sub])","0aa5aaf1":"print('Training Data:',train.info(), end = \"\\n\\n\\n\")\n\nprint('Testing Data:',test.info())","a87162b9":"# Visualising Train DataSet \nfig = px.histogram(train, x=\"Sex\")\nfig.update_layout(title_text= \"Patient Count in Training Dataset\")\nfig.show()","141ea2af":"fig = px.histogram(train, x=\"SmokingStatus\")\nfig.update_layout(title_text= \"Ex-Smoker , Never Smoked, Present Smoker\")\nfig.show()","2fde08e5":"# Age Distribution\n\nfig = px.histogram(train, x=\"Age\")\nfig.update_layout(title_text= \"Patient Count in Training Dataset\")\nfig.show()","bddcc5b2":"fig = px.histogram(train, y=\"Sex\" , color = \"Age\")\nfig.update_layout(title_text= \"Affected Patient wr Age\")\nfig.show()","4157a913":"fig = px.histogram(train, x=\"Age\" , color = \"SmokingStatus\")\nfig.update_layout(title_text= \"Age wr Smoking Status\")\nfig.show()","2d7c40e3":"print(train.shape, test.shape, sub.shape, data.shape)\nprint(train.Patient.nunique(), test.Patient.nunique(), sub.Patient.nunique(), \n      data.Patient.nunique())","2164b107":"df = px.data.gapminder()\nfig = px.area(train, x=\"Weeks\", y=\"Percent\", color = \"SmokingStatus\")\nfig.update_layout(title_text= \"Percent Affected wr Weeks and Smoking Status\")\nfig.show()","ca9649cf":"data['min_week'] = data['Weeks']\ndata.loc[data.WHERE=='test','min_week'] = np.nan\ndata['min_week'] = data.groupby('Patient')['min_week'].transform('min')","68c6e13d":"fig = px.scatter(x = train[\"Weeks\"] , y = train[\"Percent\"])\n\nfig.update_layout(title_text= \"Weeks vs Percent\")\n\nfig.show()","ca317c82":"fig = px.histogram(train, x=\"FVC\", color = \"Sex\")\nfig.update_layout(title_text= \"FVC wr Gender\")\nfig.show()","8efed538":"fig = px.histogram(train, x=\"FVC\", color = \"SmokingStatus\")\nfig.update_layout(title_text= \"FVC wr Smoking Status\")\nfig.show()","38adcf14":"fig = px.histogram(train, x=\"FVC\", color = \"SmokingStatus\")\nfig.update_layout(title_text= \"FVC wr Smoking Status\")\nfig.show()","666555cc":"train.columns","6b9cd23a":"parallel_diagram = train[['Weeks', 'Patient', 'FVC', 'Percent', 'Age', 'Sex', 'SmokingStatus']]\n\nfig = px.parallel_categories(parallel_diagram, color_continuous_scale=px.colors.sequential.Inferno)\nfig.update_layout(title='Parallel category diagram on trainset')\nfig.show()","fd153fc4":"def individual_patient_detail(patient_id):\n    patient_df = train[train['Patient'] == patient_id]\n\n    \n    fig = make_subplots(specs=[[{\"secondary_y\": True}]])\n    \n    fig.add_trace(go.Scatter(x=patient_df['Weeks'], y=patient_df['FVC'], mode='lines+markers+text', text=patient_df['FVC'], name='FVC'), secondary_y=False)\n    fig.add_trace(go.Scatter(x=patient_df['Weeks'], y=patient_df['Percent'], mode='markers', text=round(patient_df['Percent'], 2), name='Percent'), secondary_y=True)\n    fig.update_traces(textposition='top center')\n    fig.update_layout(title_text=f'Forced Vital Capacity and Percent of {patient_id}',\n                      xaxis_title=\"Weeks\",\n                      width=1000,\n                      height=500)\n    fig.update_yaxes(title_text=\"Forced vital capacity\", secondary_y=False)\n    fig.update_yaxes(title_text=\"Percent\", secondary_y=True)\n    \n    fig.show()","c24ae6d8":"for ss in train['SmokingStatus'].unique():\n    for sample in random.sample(train[train['SmokingStatus'] == ss]['Patient'].tolist(), 2):\n        individual_patient_detail(sample)","fe116327":"base = data.loc[data.Weeks == data.min_week]\nbase = base[['Patient','FVC']].copy()\nbase.columns = ['Patient','min_FVC']\nbase['nb'] = 1\nbase['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\nbase = base[base.nb==1]\nbase.drop('nb', axis=1, inplace=True)","d5ed3e5b":"data = data.merge(base, on='Patient', how='left')\ndata['base_week'] = data['Weeks'] - data['min_week']\ndel base","36ab66d6":"COLS = ['Sex','SmokingStatus']\nFE = []\nfor col in COLS:\n    for mod in data[col].unique():\n        FE.append(mod)\n        data[mod] = (data[col] == mod).astype(int)","483020da":"data['age'] = (data['Age'] - data['Age'].min() ) \/ ( data['Age'].max() - data['Age'].min() )\ndata['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) \/ ( data['min_FVC'].max() - data['min_FVC'].min() )\ndata['week'] = (data['base_week'] - data['base_week'].min() ) \/ ( data['base_week'].max() - data['base_week'].min() )\ndata['percent'] = (data['Percent'] - data['Percent'].min() ) \/ ( data['Percent'].max() - data['Percent'].min() )\nFE += ['age','percent','week','BASE']","29508820":"train = data.loc[data.WHERE=='train']\ntest = data.loc[data.WHERE=='val']\nsub = data.loc[data.WHERE=='test']\ndel data","4ce5dbaf":"train.shape, test.shape, sub.shape","9a53e96f":"C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n#=============================#\ndef score(y_true, y_pred):\n    tf.dtypes.cast(y_true, tf.float32)\n    tf.dtypes.cast(y_pred, tf.float32)\n    sigma = y_pred[:, 2] - y_pred[:, 0]\n    fvc_pred = y_pred[:, 1]\n    \n    #sigma_clip = sigma + C1\n    sigma_clip = tf.maximum(sigma, C1)\n    delta = tf.abs(y_true[:, 0] - fvc_pred)\n    delta = tf.minimum(delta, C2)\n    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n    metric = (delta \/ sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n    return K.mean(metric)\n#============================#\ndef qloss(y_true, y_pred):\n    # Pinball loss for multiple quantiles\n    qs = [0.2, 0.50, 0.8]\n    q = tf.constant(np.array([qs]), dtype=tf.float32)\n    e = y_true - y_pred\n    v = tf.maximum(q*e, (q-1)*e)\n    return K.mean(v)\n#=============================#\ndef mloss(_lambda):\n    def loss(y_true, y_pred):\n        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n    return loss\n#=================\ndef make_model():\n    z = L.Input((9,), name=\"Patient\")\n    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n    #x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n                     name=\"preds\")([p1, p2])\n    \n    model = M.Model(z, preds, name=\"CNN\")\n    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n    model.compile(loss=mloss(0.8375), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n    return model","7657ab75":"net = make_model()\nprint(net.summary())\nprint(net.count_params())","aa7a01f4":"y = train['FVC'].values\nz = train[FE].values\nze = sub[FE].values\npe = np.zeros((ze.shape[0], 3))\npred = np.zeros((z.shape[0], 3))","bb8a6c9e":"NFOLD = 5\nkf = KFold(n_splits=NFOLD)","e7764041":"%%time\ncnt = 0\nfor tr_idx, val_idx in kf.split(z):\n    cnt += 1\n    print(f\"FOLD {cnt}\")\n    net = make_model()\n    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=800, \n            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n    print(\"predict val...\")\n    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n    print(\"predict test...\")\n    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) \/ NFOLD","fc0c1a76":"sigma_opt = mean_absolute_error(y, pred[:, 1])\nunc = pred[:,2] - pred[:, 0]\nsigma_mean = np.mean(unc)\nprint(sigma_opt, sigma_mean)","41cbee4a":"idxs = np.random.randint(0, y.shape[0], 100)\nplt.plot(y[idxs], label=\"ground truth\")\nplt.plot(pred[idxs, 0], label=\"q25\")\nplt.plot(pred[idxs, 1], label=\"q50\")\nplt.plot(pred[idxs, 2], label=\"q75\")\nplt.legend(loc=\"best\")\nplt.show()","9cb1b912":"print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())","66d72d09":"plt.hist(unc)\nplt.title(\"uncertainty in prediction\")\nplt.show()","5add1739":"sub.head()","a2564ccc":"sub['FVC1'] = 0.996  * pe[:, 1]\nsub['Confidence1'] = pe[:, 2] - pe[:, 0]","dfe12c8b":"subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()","54ce542b":"subm.loc[~subm.FVC1.isnull()].head(10)","e72999da":"subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\nif sigma_mean<70:\n    subm['Confidence'] = sigma_opt\nelse:\n    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']","7480202c":"subm.head()","57d8737e":"subm.describe().T","d7c4944c":"otest = pd.read_csv('..\/input\/osic-pulmonary-fibrosis-progression\/test.csv')\nfor i in range(len(otest)):\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1","f53fdfcc":"subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission.csv\", index=False)","d29de0eb":"- epochs, callbacks","65b68ad6":"## Visualising Dataset","765202e2":"### Predict Test - X Test\n\npe","0887d02a":"**OSIC - Baseline ElasticNet + EDA**\n\nhttps:\/\/www.kaggle.com\/jagadish13\/osic-baseline-elasticnet-eda\n\n**Data provided**\n- train.csv : Baseline CT Scan and entire history of FVC\n- test.csv : Baseline CT and Initial FVC Measurement\n- train\/ : Baseline CT scan in DICOM format\n- test\/ : Baseline CT Scan in DICOM format","745bbe4b":"### Predict Val - Y Test\n\npred[val_idx]","fcb24e3e":"Model + Optimizers\n\n- Using Adam optimizer from adamax","8e9ad388":"### Mean Absolute Error MAE\n\n- Try reducing the values"}}