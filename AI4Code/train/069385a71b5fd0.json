{"cell_type":{"5c96967a":"code","a8c91c40":"code","2175d7f3":"code","41904b5b":"code","5599d90a":"code","48d9be93":"code","7e0e183d":"code","033a2c3f":"code","65c60c00":"code","79dc6f9b":"code","8b8cc9a5":"code","bc9fa2d2":"code","a0e3de81":"code","8004e539":"code","0c48cb85":"code","2fd41eea":"code","5021704f":"code","9328f4eb":"code","620bc7f5":"code","596349dd":"code","b8271bb9":"code","3b808c4c":"code","90c84acc":"code","6c76dca3":"code","28933dfb":"code","ee6c0fe4":"code","54a367f8":"code","1c355969":"code","8608e480":"code","1d57913f":"code","f577c3f9":"code","e3b38af2":"code","cf7b06cd":"code","c13c4634":"code","b7d0f692":"code","eca1d622":"code","59d2ff8f":"markdown","a0928573":"markdown","e32eab4a":"markdown","6161b7f7":"markdown","f76fc333":"markdown","8a66f5ba":"markdown","c054c27b":"markdown","6203d3c9":"markdown","fe3f52a2":"markdown","763c7eb4":"markdown","93d3a7a0":"markdown","7239ad00":"markdown","a4f24303":"markdown","dcf0a061":"markdown","cd69662c":"markdown","d8f2176f":"markdown","d37a9255":"markdown","60b72a4f":"markdown","53967c91":"markdown","64eb82be":"markdown","53c28236":"markdown","fe3e1a79":"markdown","8e15cc0e":"markdown","a136cdde":"markdown","093c32e3":"markdown","085f07ce":"markdown","b6fc2b6e":"markdown","c420a746":"markdown","f9ccddc1":"markdown","7bcc06cf":"markdown","4fc96cab":"markdown","a8959a08":"markdown","9fa740dd":"markdown","93f635cc":"markdown","d69e959d":"markdown","cf43c449":"markdown","3650bc14":"markdown","296d11f4":"markdown","d2adeeb3":"markdown","96162717":"markdown","9143a87a":"markdown","8aa0dd09":"markdown","fd577274":"markdown","5ad8528d":"markdown","df516286":"markdown","a1e4d894":"markdown","45fc26d0":"markdown","44e8753c":"markdown","92b946c3":"markdown","afc367d6":"markdown","8c765e90":"markdown","3164e021":"markdown","2d900c0d":"markdown","fd3ee4fa":"markdown","d4053abc":"markdown","72c5cd0a":"markdown","b3b0babd":"markdown","101ca9a9":"markdown","07c4ec2a":"markdown","4bb4529d":"markdown","397b499a":"markdown","bea0d7f8":"markdown","ceb6b585":"markdown","f252815e":"markdown","ead1a211":"markdown","7d33cc56":"markdown","7e04dc58":"markdown","11cdb98a":"markdown"},"source":{"5c96967a":"import numpy as np\nnp.random.seed(40643)\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import cm\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import Matern #,DotProduct, WhiteKernel\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy.interpolate import interp2d, interp1d","a8c91c40":"def branin(x,y,a=1,b=5.1\/(4*np.pi**2),c=5\/np.pi,r=6,s=10,t=1\/(8*np.pi)):\n    return a*(y-b*x**2+c*x-r)**2+s*(1-t)*np.cos(x)+s","2175d7f3":"X=np.linspace(-5,10,1500)\nY=np.linspace(0,15,1500)\npoints=[(x,y) for y in Y for x in X]","41904b5b":"Z=[branin(x,y) for (x,y) in points]\nm=max(Z)\ncolors=cm.rainbow([z\/m for z in Z])","5599d90a":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\nax.scatter([p[0] for p in points],[p[1] for p in points],Z, zdir='z', s=0.1, c=colors, depthshade=True)","48d9be93":"coordinates=[(x,y,z) for x,y,z in zip([p[0] for p in points],[p[1] for p in points],Z)]\ninit_points=np.random.choice(len(coordinates),100)\nX,y=np.asarray([points[i] for i in init_points]),[Z[i] for i in init_points]\nX_test,y_test=np.asarray([points[i] for i in range(len(coordinates)) if i not in init_points]),[Z[i] for i in range(len(coordinates)) if i not in init_points]\n# small=np.random.choice([i for i in range(len(coordinates)) if i not in init_points],100)\n# X_test_small,y_test_small=np.asarray([points[i] for i in small]),[Z[i] for i in small]","7e0e183d":"def mse(true,estim):\n    loss=0\n    for t,e in zip(true,estim):\n        loss+=(t-e)**2\n    loss=loss\/len(true)\n    return loss","033a2c3f":"dic_errors={'branin':0}","65c60c00":"def interpol_2d(kind,X=X,y=y):\n    fun=interp2d([x[0]for x in X],[x[1] for x in X],y,kind=kind)\n    return fun","79dc6f9b":"linear=interpol_2d('linear')\nZ_linear=[linear(x[0],x[1]) for x in X_test[::2]]\nZ_linear=np.concatenate(Z_linear)","8b8cc9a5":"dic_errors['linear']=mse(Z_linear,y_test[::2])\ndic_errors","bc9fa2d2":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_linear])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_linear, zdir='z', s=0.1, c=colors, depthshade=True)","a0e3de81":"cubic=interpol_2d('cubic')\nZ_cubic=[cubic(x[0],x[1]) for x in X_test[::2]]\nZ_cubic=np.concatenate(Z_cubic)\ndic_errors['cubic_splines']=mse(Z_cubic,y_test[::2])\ndic_errors","8004e539":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_cubic])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_cubic, zdir='z', s=0.1, c=colors, depthshade=True)","0c48cb85":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_cubic])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_cubic, zdir='z', s=0.1, c=colors, depthshade=True)\nax.set_zlim(0, 300)","2fd41eea":"neigh = KNeighborsRegressor(n_neighbors=2)\nneigh.fit(X, y)\nZ_knn=neigh.predict(X_test[::2])\ndic_errors['knn'] = mse(Z_knn,y_test[::2])\ndic_errors","5021704f":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_knn])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_knn, zdir='z', s=0.1, c=colors, depthshade=True)","9328f4eb":"regr = RandomForestRegressor(n_estimators=100, criterion='mse', max_depth=100, random_state=0)\nregr.fit(X, y)\nZ_randforest=regr.predict(X_test[::2])\ndic_errors['random_forest'] = mse(Z_randforest,y_test[::2])\ndic_errors","620bc7f5":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_randforest])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_randforest, zdir='z', s=0.1, c=colors, depthshade=True)","596349dd":"kernel=Matern(length_scale=1,nu=2.5)\ngpr = GaussianProcessRegressor(kernel=kernel,random_state=0).fit(X, y)\nprint(gpr.score(X, y))","b8271bb9":"Z_krig=gpr.predict(X_test[::2], return_std=False)\ndic_errors['kriging'] = mse(Z_krig,y_test[::2])\ndic_errors","3b808c4c":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_krig])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_krig, zdir='z', s=0.1, c=colors, depthshade=True)","90c84acc":"Z_error=gpr.predict(X_test[::2], return_std=True)","6c76dca3":"Z_error=Z_error[1]","28933dfb":"fig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111, projection='3d')\nmm=max(Z_error)\ncolors=cm.gist_earth([z\/mm for z in Z_error])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_error, zdir='z', s=0.1, c=colors, depthshade=True)","ee6c0fe4":"dic_errors_montecarlo=dic_errors\ndic_errors_grid={'branin' : 0}","54a367f8":"def ackley(x,y=0):\n    return -20*np.exp(-0.2*np.sqrt(0.5*(x**2 + y**2))) - np.exp(0.5*(np.cos(2*np.pi*x) + np.cos(2*np.pi*y))) + np.exp(1) + 20","1c355969":"ackley_x=np.linspace(-5,5,100000)\nackley_y=[ackley(x) for x in ackley_x]\nackley_m=max(ackley_y)\ncolors=cm.rainbow([y\/ackley_m for y in ackley_y])\nplt.scatter(ackley_x,ackley_y,s=0.1,c=colors)","8608e480":"ackley_grid_x=np.linspace(-5,5,11)\nackley_grid_y=[ackley(x) for x in ackley_grid_x]\ncolors_2=cm.rainbow([y\/ackley_m for y in ackley_grid_y])\nplt.scatter(ackley_x,ackley_y,s=0.1,c=colors)\nfor x,y,c in zip(ackley_grid_x,ackley_grid_y,colors_2):\n    plt.axvline(x=x,linestyle='--',linewidth=0.3,c=c)\n#     plt.axhline(y=y,linestyle='--',linewidth=0.3,c=c)\n    plt.plot(x,y,'o',c=c)","1d57913f":"ackley_interp_linear=[interp1d(ackley_grid_x,ackley_grid_y,kind='linear')(x) for x in ackley_x]\nackley_interp_quadratic=[interp1d(ackley_grid_x,ackley_grid_y,kind='quadratic')(x) for x in ackley_x]\nlabels=['Linear Interpolation','Quadratic Interpolation','Original Ackley Function']\nplt.figure(figsize=(8,8))\nplt.scatter(ackley_x,ackley_y,s=0.1,c=colors)\nplt.plot(ackley_x,ackley_interp_linear,'k',linewidth=0.4)\nplt.plot(ackley_x,ackley_interp_quadratic,'slategray',linewidth=0.4)\nfor x,y,c in zip(ackley_grid_x,ackley_grid_y,colors_2):\n    plt.axvline(x=x,linestyle='--',linewidth=0.3,c=c)\n    plt.plot(x,y,'o',c=c)\nplt.legend(labels)","f577c3f9":"grid_x=np.linspace(-5,10,10)\ngrid_y=np.linspace(0,15,10)\npoints_grid = [(x,y) for x in grid_x for y in grid_y]\nbranin_grid=[branin(x,y) for x in grid_x for y in grid_y]","e3b38af2":"linear=interpol_2d('linear',X=points_grid,y=branin_grid)\nZ_linear=[linear(x[0],x[1]) for x in X_test[::2]]\nZ_linear=np.concatenate(Z_linear)\ndic_errors_grid['linear'] = mse(Z_linear,y_test[::2])\ndic_errors_grid","cf7b06cd":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_linear])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_linear, zdir='z', s=0.1, c=colors, depthshade=True)","c13c4634":"kernel=Matern(length_scale=1,nu=2.5)\ngpr = GaussianProcessRegressor(kernel=kernel,random_state=0).fit(points_grid, branin_grid)\nkrig=gpr.predict(X_test[::2], return_std=True)\nZ_krig,Z_error=krig[0],krig[1]\ndic_errors_grid['kriging'] = mse(Z_krig,y_test[::2])\ndic_errors_grid","b7d0f692":"fig = plt.figure(figsize=(10,10))\nax = fig.add_subplot(111, projection='3d')\ncolors=cm.rainbow([z\/m for z in Z_krig])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_krig, zdir='z', s=0.1, c=colors, depthshade=True)","eca1d622":"fig = plt.figure(figsize=(8,8))\nax = fig.add_subplot(111, projection='3d')\nmax_error=max(Z_error)\ncolors=cm.gist_earth([z\/max_error for z in Z_error])\nax.scatter([x[0] for x in X_test[::2]],[x[1] for x in X_test[::2]],Z_error, zdir='z', s=0.1, c=colors, depthshade=True)","59d2ff8f":"Wow, so beautiful, so smooth ! And definitely much alike the original function. However, if you watch carefully, you can still notice areas where it is not very well estimated (for example the back corner). It still seems to be, by far, the best method so far.","a0928573":"Here at last, I come to the most interesting part of this notebook ! For those of you who never heard about kriging, I recommend you to start with some theory.","e32eab4a":"## Conclusion","6161b7f7":"## Introduction","f76fc333":"Warning : throughout this notebook, the graphical and numerical results may vary a lot depending on the random seed. It is important to set it once and for all before starting working. You can try to change it to see the impact (it is very immediate). However, it will not change the global conclusions of this work.","8a66f5ba":"The context is the following : suppose we have a model we try to estimate by Kriging. However, an exact simulation of the value of a point is costly (in money, in case of physical experiences, or in time, in case of computing). For simplicity, we will keep the Branin function in this part.","c054c27b":"The error is very symetrical, contrary to the previous one we had on random points.","6203d3c9":"### One-shot strategy","fe3f52a2":"Oh oh... Why is this error so massive compared to the linear error ? Let's take a look...","763c7eb4":"Here I present 2 methods who are not strictly speaking interpolation methods, but still widely used for space estimation.","93d3a7a0":"Oh oh... Definitely not a good idea, those splines. What is impressive when you think twice about it, is that the squared error on points at the edges must be (and is indeed) MASSIVE. However, interestingly, if you look closely we can see the actual polynomial curves between the interpolation points.","7239ad00":"### Random Forest Regression","a4f24303":"## Classical interpolation methods","dcf0a061":"Kriging is a very interesting and powerful interpolation methods, that can be used for various purposes : interpolation, smoothing (like presented here), but also in sequential experiments, in contexts of fixed calculation budgets (more detailed notebook on this subject coming soon), since it presents the advantage to estimate its own standard error in every point of space.","cd69662c":"https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.gaussian_process.GaussianProcessRegressor.html#sklearn.gaussian_process.GaussianProcessRegressor","d8f2176f":"# Sequential Kriging","d37a9255":"Interestingly, we notice here a property of trees : we can see the presence of kind of strays, which are in fact linked to tree branches used in the attribution of value. They define \"ranges\" of values into which the attributed target is constant.","60b72a4f":"In this notebook, I will try to present various spatial estimation methods in multi-dimensions (here, for graphical needs, in dimension 2). My goal is to illustrate clearly how each works, his advantages and disadvantages. Lastly, I will present a method that I like very much : Kriging.","53967c91":"### K-nearest-neighbors","64eb82be":"Well, not so bad ! But it still lacks some smoothness. Looking at the edges of the surface, we can easily guess where the grid lines here (angles). Now trying with Kriging :","53c28236":"Oh oh... What happened here ? Well, actually, this is a classical problem of polynomial interpolation (especially with a high degree, like cubic splines) : it explodes at the edges. If we set a maximum $z$ limit, we will see better :","fe3e1a79":"Some of you might have recognized the 2D-Ackley function, projected in the plane $y=0$. Let's plot it to get a quick idea.","8e15cc0e":"Let's just suppose for a moment that, instead of our Branin function, we are trying to estimate the following function :","a136cdde":"Let us keep track of errors :","093c32e3":"Let's now take advantage of a fundamental property of Kriging : getting access to the model error (std to 95%). This comes from the fact that Gaussian Process Regression is a *probabilitic method*, originally used to estimate some well behaved random variables.","085f07ce":"Funny, the linear model seems to work better on a regular grid than on random points !","b6fc2b6e":"Suppose we have a 3D surface with the following equation :","c420a746":"Better than with linear interpolation... Let's now take a look at the error :","f9ccddc1":"We see here very well the fundamental property of k-nn : you can only take 100 Z-values, the ones from the initial points. You cannot get a continuous prediction. We very clearly see the patches surrounding the 100 original points, into which the data is approximated by the value at the central point.","7bcc06cf":"### Polynomial interpolation","4fc96cab":"Be careful with Kriging : it is a gaussian process regressor, and has a random component. Luckily, scikit-learn uses the same seed as numpy, but don't forget to set it if you want your results to be reproducible.","a8959a08":"We will also keep a dictionnary of the calculated errors, that will be useful for future comparisons.","9fa740dd":"Let's start with the simplest of all methods : linear interpolation. It is very straightforward to implement using scipy (see here : https:\/\/docs.scipy.org\/doc\/scipy-0.14.0\/reference\/generated\/scipy.interpolate.interp2d.html).","93f635cc":"Wow, a lot better ! Let's take a look.","d69e959d":"### Linear interpolation","cf43c449":"Now, we will try various interpolation methods, to see which ones seem to approximate this function to the best. Let's randomly sample 100 points from our calculated points, and use them for fitting our interpolation. then we will try to recalculate all the others, and plot them.","3650bc14":"There are so many different types of polynomial interpolation that I will restrict myself to cubic splines (every piece is a polynomial of degree $\\leq 3$).","296d11f4":"Like expected, the loss is particularly high in the back corner, but globally when you look at its range of values, it stays rather low compared to other methods.","d2adeeb3":"I am trying to rely as few as possible on fancy packages, whose underlying code I am not sure of. Here are the tools you will need, which are some very common and widely used packages.","96162717":"Kriging also seems to work better on a regular grid. It is still better than the linear model.","9143a87a":"## Introduction","8aa0dd09":"Here as well, you can try various values for the number of trees, the maximal depth, or the metric.","fd577274":"I don't want you to conclude from this notebook that you should always use Kriging, and that the other methods never work. Actually, there isn't one perfect method for every estimation problem. Please note that the function I have chosen, the Branin function, is a very peculiar function, often studied for its uncommon properties, that was a perfect example for this notebook. But on most simpler functions, the other methods can work as well, and sometimes even better than Kriging !","5ad8528d":"Throughout this notebook, in addition to the graphics, we will need a suitable metrics to compare the different methods (that is to say, the \"error\" of each one with respect to the true values). Thereare a lot of possible metrics. I will choose the average mean squared error :","df516286":"I encourage you to test different values of n_neighbors, to see the difference in results. However, the conclusion remaines the same.","a1e4d894":"## Kriging","45fc26d0":"Let's plot it to get a better idea. Please note that, throughout all this notebook, everytime I want to plot something I use the scatter function *on purpose*. This is because with continuous plotting, matplotlib *interpolates* between your points, and since we are trying to study different estimation methods, that would be very untrustworthy. However, if you put enough points, it will always appear to be continuous to the viewer, so that is not a problem.","44e8753c":"Note : this notebooks skips the entire theory behind each method. If you are curious, feel free to do some additional research.","92b946c3":"This example helps me come to my point : with a regular grid, you often (not to say always) lose a loooot of information about your function. Only a random sampling can help you capture a better idea of the real behaviour of your function. To sums things up, you should never, ever, use a regular grid to estimate a function you are not sure of (or at least, you should always add random points in addition to the regular grid).","afc367d6":"Now let's come back to our 2D-Branin function. What happens if we estimate it using a 100-points-regular-grid ? Let's do just a simple linear interpolation first :","8c765e90":"On this base, looking at the sampled points, anyone would easily do a simple linear or polynomial interpolation to estimate this model, and feel quite confident about it, whereas he is actually missing a lot of information about the true model.","3164e021":"What is the error of this method ?","2d900c0d":"We notice very well the \"straightness\" of linear interpolation : we can easily figure out where were the original points, and the curve is anything but smooth. Notice a big problem in this representation : we have some \"layers\", on top of each other, instead of a continuous surface.That is because we are plotting a lot of points, and sometimes the model doesn't choose well between which known points to interpolate. We will see int he second part of this notebook that, when you use less points, you don't meet this problem and the result is actually better. Anyway, it doesn't seem, in our case, a very appropriate interpolation method.","fd3ee4fa":"Not as good as k-nn, but still a lot better than linear and splines interpolation.","d4053abc":"Some of you who have read a bit about optimization (https:\/\/en.wikipedia.org\/wiki\/Test_functions_for_optimization) will recognize it as the Branin function. It is commonly used, as many other functions, to test some optimization algorithms, since it is very strangely shaped with a lot of local minima everywhere, and three global minima (http:\/\/www.sfu.ca\/~ssurjano\/branin.html).","72c5cd0a":"## Other methods","b3b0babd":"The question is : how can you spend your fixed budget of simulations (let's say *100*, like we did previously) in the most intelligent way possible, that is to say, resulting in the best estimation possible of your model ? We will study and compare two strategies : sampling all the 100 points from the beginning and spending it all in one-shot, and spending them one point after another, carefully choosing the next point to simulate based on your most recent estimate of your model.","101ca9a9":"# Why you should use Kriging","07c4ec2a":"A spline is a special function defined piecewise by polynomials. In interpolating problems, spline interpolation is often preferred to polynomial interpolation because it yields similar results, even when using low degree polynomials (from https:\/\/en.wikipedia.org\/wiki\/Spline_(mathematics)).","4bb4529d":"We get a score of 1, which is natural : kriging is an exact interpolation method. Like previously, I encourage you to try different kernels to see the results. There is not one perfect kernel for every function : the choice of kernel depends on what you are trying to estimate.","397b499a":"#### Grid-sampling","bea0d7f8":"Wow, great ! that's the model with the lowest error so far ! And by far !","ceb6b585":"If you want to choose all your points from the beginning, you have several strategies available : drawing a grid in your input space to sample your points, or taking them random (there are, of course, several levels of randomness available). I will first present the grid-sampling strategy and explain why it is a bad idea, then go to the random sampling strategies.","f252815e":"https:\/\/scikit-learn.org\/stable\/modules\/gaussian_process.html#gaussian-process","ead1a211":"I you didn't know (I didn't until quite recently), in lots of packages Kriging is actually called Gaussian Process Regressor.","7d33cc56":"Now, let's suppose that we want to estimate this function with a regular grid between -5 and 5, of step 1.","7e04dc58":"Useful links :","11cdb98a":" Let's take advantage of the kriging property of giving access to error, to see in which area the estimation is poorer :"}}