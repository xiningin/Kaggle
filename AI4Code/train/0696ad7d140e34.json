{"cell_type":{"c7521029":"code","465c3fe7":"code","8803bcdb":"code","77fb7adc":"code","531f2afd":"code","e356583a":"code","5aa792be":"code","ebdfaa00":"code","810d6f5e":"code","a17d0aa4":"code","a392d544":"code","b623d236":"code","be0a8103":"code","338768dd":"code","4af4902b":"code","b9754cdf":"code","efc36cdb":"code","2ad5a47f":"code","74fd5dd0":"code","cda1c0b9":"code","fb59e6f1":"code","9891487e":"code","78a82b69":"code","29b206a2":"markdown","473dab56":"markdown","1b5e956b":"markdown","ba567377":"markdown","bb270c91":"markdown","a31edf3e":"markdown","333f3547":"markdown","97c843d0":"markdown","6690070b":"markdown","c338786c":"markdown","227782c3":"markdown","b1f1622d":"markdown","14890d95":"markdown","992600af":"markdown","80984493":"markdown","6339321a":"markdown","6a7eca4e":"markdown","9d704442":"markdown","62a896dc":"markdown","e3b31e76":"markdown","67925af3":"markdown","a475a9f2":"markdown","7997cfa5":"markdown","9cbe3414":"markdown"},"source":{"c7521029":"import pandas as  pd \nimport numpy as np \nimport seaborn as sns \nimport plotly.express as px\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import StratifiedKFold\nfrom imblearn.pipeline import make_pipeline\nfrom collections import Counter\nfrom sklearn.metrics import RocCurveDisplay, roc_curve, auc\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import *\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\nfrom imblearn.pipeline import Pipeline\n\nimport pandas as pd\npd.options.mode.chained_assignment = None  # default='warn'\n%matplotlib inline\nsns.set_style(\"whitegrid\")","465c3fe7":"path = '..\/input\/hr-analytics-job-change-of-data-scientists\/aug_train.csv'\n# Loading data\ndf = pd.read_csv(path)\ndf.describe(include= 'object')","8803bcdb":"### Fuctions\n\ndef inputing_data_major (cols):\n    level,enrolled,major,exp = cols[0],cols[1],cols[2],cols[3]\n    \n    if pd.isnull(major):\n        if all([level in ['Primary School','High School'], exp  > 5]):\n            return 'No Major'\n        elif all([level in['Graduate','Masters','High School','Phd'], exp > 1]):\n            return 'Other'\n        else:\n            return major\n    return major\n\ndef inputing_data (cols):\n    level,enrolled,major,exp = cols[0],cols[1],cols[2],cols[3]\n    \n    if pd.isnull(enrolled):\n        if all([level in ['Primary School','High School'], exp  > 0]):\n            return 'no_enrollment'\n            #When other can be he \/she could go to university\n        else: \n            return enrolled \n    else:\n        return enrolled\n    \n    \n# Based on my studies in MBA a start-up usually have lot of challeging 'money wise' \n# Usually they look for MAster or phd for intership \n#therefore i asume that if you are MAster\/phd with more than 10 years your salsary will be a good one \n#ine that a start up wont pay becaue the financial difficulties and if you stay ther emore thant 3 years means that it is not a start up anymore \n\n\ndef inputing_data_company_type (cols):\n    comp,level,last_job,exp = cols[0],cols[1],cols[2],cols[3]\n    \n    if pd.isnull(comp):\n        if all([level in ['Mastersl','Phd','Graduate'], last_job in ['3','4','>4'],exp  > 10]):\n            return 'Pvt Ltd'\n            #When other can be he \/she could go to university\n        else: \n            return comp \n    else:\n        return comp\n    \n    \ndef company_size (cols):\n    comp_size,comp = cols[0],cols[1]\n    \n    comps = {'Early Stage Startup':'<10','Funded Startup':'50-99','NGO':'100-500',\n            'Other':'100-500','Public Sector':'1000-4999','Pvt Ltd':'50-99'}\n    \n    if pd.isnull(comp_size):\n        if comp in comps.keys():\n            return  comps[comp]\n            #When other can be he \/she could go to university\n        else: \n            return comp_size \n    else:\n        return comp_size \n    \n    \nfrom IPython.display import display_html\nfrom itertools import chain,cycle\ndef display_side_by_side(*args,titles=cycle([''])):\n    html_str=''\n    for df,title in zip(args, chain(titles,cycle(['<\/br>'])) ):\n        html_str+='<th style=\"text-align:center\"><td style=\"vertical-align:top\">'\n        html_str+=f'<h2>{title}<\/h2>'\n        html_str+=df.to_html().replace('table','table style=\"display:inline\"')\n        html_str+='<\/td><\/th>'\n    display_html(html_str,raw=True)\n    ","77fb7adc":"# I will use this information to create bin \/ segmentation to Fill nan Values\ndf.company_size.groupby(df['company_type']).apply(lambda g: g.mode()).reset_index()[['company_type', 'company_size']].values\nimport missingno as msno\nmsno.matrix(df, sparkline=True, figsize=(10,5), sort='ascending', fontsize=12, labels=True, color=(0.25, 0.45, 0.6));\nprint(\"Missing values\")","531f2afd":"experiences_values = {'<1':0.99,'>20':21}\ndf.experience.replace(experiences_values, inplace=True)\ndf.experience = df.experience.astype(float)","e356583a":"df['gender'].fillna('Other', inplace = True)","5aa792be":"comps = {'Early Stage Startup':'<10','Funded Startup':'50-99','NGO':'100-500',\n            'Other':'100-500','Public Sector':'1000-4999','Pvt Ltd':'50-99','middle_size_st':'10\/49'}\n\nfor key,values in comps.items():\n    df.loc[((df['company_type'].isna())&(df['company_size'] ==values)),'company_type'] = key\n    \ndf.loc[( (df['company_type'].isna()) & (df['company_size'].isna()) & (df['last_new_job']=='never') ),['company_type','company_size'] ]= 'NW'","ebdfaa00":"df.major_discipline = df[['education_level','enrolled_university','major_discipline','experience']].apply(inputing_data_major,axis=1)\ndf.enrolled_university = df[['education_level','enrolled_university','major_discipline','experience']].apply(inputing_data,axis=1)\ndf.company_type = df[['company_type','education_level','last_new_job','experience']].apply(inputing_data_company_type,axis=1)\ndf.company_size = df[['company_size','company_type']].apply(company_size,axis=1)\nmsno.matrix(df, sparkline=True, figsize=(10,5), sort='ascending', fontsize=12, labels=True, color=(0.25, 0.45, 0.6));\nprint(\"After feature Engineering\")","810d6f5e":"qty_of_nuls =1\ndf.drop(df.iloc[df[(df.isnull().sum(axis=1) >= qty_of_nuls)].index].index, inplace=True)\nmsno.matrix(df, sparkline=True, figsize=(10,5), sort='ascending', fontsize=12, labels=True, color=(0.25, 0.45, 0.6))","a17d0aa4":"# Frist Visualizatoin \nvs = df.copy()\n\nfig,ax = plt.subplots(1,3,figsize = (18,5))\n\n#training time vs target \nsns.boxenplot(y='training_hours' , x='gender', data= vs , hue= 'target', ax=ax[0] )\nsns.countplot(y='gender'  ,data= vs, hue='target', ax=ax[1])\nsns.kdeplot(data = vs , x='training_hours', hue = 'gender',multiple=\"stack\", ax=ax[2])\nprint(\"The average Training required to be change\/stay in a job seems to be the same by gender\")\nprint(\"Male are more intersted in staying in the same job\")\nprint('the difference between training time is obvious, male are the one with more training time - experience')","a392d544":"from IPython.display import display\nfrom PIL import Image\npath=('..\/input\/diagram\/diagram.png')\ndisplay(Image.open(path))\n\nprint(\"\\nIn here a small diagram to better understand how I filled those nan Values\")","b623d236":"df.drop(['enrollee_id','city'], axis = 1, inplace=True)\ndf.reset_index(drop=True,inplace=True)","be0a8103":"target = df['target']\nnumerical = df[['city_development_index','training_hours']] \nordinal = df[['education_level','experience','company_size','last_new_job']] \nenco= df[['gender','relevent_experience','major_discipline','enrolled_university']]","338768dd":"# For Experiences \nbins,labels = [0,3,6,9,12,15,18,21,22],[0,1,2,3,4,5,6,7]\nordinal.loc[:,'experience'] = pd.cut(ordinal.loc[:,'experience'], bins = bins, labels=labels, right = True).values.astype(int)\n\n# For Education Level\nlevels_ = {'Primary School':0,'High School':1,'Graduate':2 ,'Masters':3,'Phd':4}\nordinal.loc[:,'education_level'].replace(levels_, inplace= True)\n\n# For Company Size - \nc_size = {'50-99':3, 'NW':0, '<10':1, '10000+':8, '5000-9999':7, '1000-4999':6, '10\/49':2,'100-500':4, '500-999':5}\nordinal.loc[:,'company_size'].replace(c_size, inplace= True)\n\n#For Last New Job\nlast_job = {'>4':3, 'never':0, '4':2, '1':1, '3':2, '2':1}\nordinal.loc[:,'last_new_job'].replace(last_job, inplace= True)\n\ndisplay_side_by_side(ordinal.head(5), df[['education_level','experience','company_size','last_new_job']].head(5))","4af4902b":"from sklearn.preprocessing import OneHotEncoder\ndf_enc = enco.copy()\nOHC = OneHotEncoder()\n\nfinal = pd.DataFrame()\nfor col in enco:\n    new_data = OHC.fit_transform(df_enc[[col]])\n    df_enc = df_enc.drop(col, axis =1)\n    cats = OHC.categories_\n    new_cols = [\"_\".join([col,cat]) for cat in cats[0]]\n    new_df = pd.DataFrame(new_data.toarray(), columns = new_cols) \n    final = pd.concat([final,new_df], axis=1)\n\nfinal","b9754cdf":"from sklearn.neighbors import LocalOutlierFactor\n\n#Joining Data set\ndata = pd.concat([numerical, ordinal,final,target], axis=1)\n\n#Identifying and Ensembling to Outliers-data set\nLOF = LocalOutlierFactor(novelty=False)\ndata['Outliers'] = LOF.fit_predict(data)\n\n#Finding Ourliers\noutliers = data[data['Outliers'] ==-1].index\nprint(f'Number of outliers {len(outliers)}')\n\n#Visualizations\nfig, ax = plt.subplots(1,3 ,figsize=(15,5))\nout_liers = data.iloc[outliers]\n\nsns.boxenplot(x='target' ,y ='experience' , data=data, ax=ax[0])\nsns.scatterplot(x='target' ,y ='experience' , data=out_liers, color ='r',ax =ax[0])\n\nsns.boxenplot(x='target' ,y ='city_development_index' , data=data, ax=ax[1])\nsns.scatterplot(x='target' ,y ='city_development_index' , data=out_liers, color ='r', ax=ax[1])\n\nsns.boxenplot(x='target' ,y ='training_hours' , data=data, ax=ax[2])\nsns.scatterplot(x='target' ,y ='training_hours' , data=out_liers, color ='r', ax=ax[2])\n\nprint(\"Let's Visualize the outliers in Training hours, Experience and City\")\nprint(\"Notice that the 'red' dotes are the outliers, consider LocalOutlierFactor is finding those outliers \\\nin high dimmensional space, and we are visualizing them in 2D\")\ndata.drop('Outliers', axis = 1 , inplace = True)\ndata.drop(outliers , inplace=True)","efc36cdb":"f,ax = plt.subplots(2,4, figsize = (17,10))\nsns.color_palette(\"rocket\", as_cmap=True)\n\n#------Numerical values \nnumerical = data[['city_development_index','training_hours']]\nx='city_development_index'\nx1= 'training_hours'\ntitle ='Training hours Distribution Min, Mean, Max'\n\n# Before transforming \nsns.histplot(x=x, data= numerical, ax = ax[0,0])\nsns.boxplot(y=x, data =numerical , orient ='v' , ax=ax[0,1],palette=\"Paired\").set(title=title)\nsns.histplot(x=x1, data= numerical, ax = ax[1,0])\nsns.boxplot(y=x1, data =numerical , orient ='v' , ax=ax[1,1],palette=\"flare\").set(title=title)\n\n# After Transforming \nfor i in numerical.columns:\n    data[i] = np.log1p(data[i]).astype('float')\n    \nsns.histplot(x=x, data= data, ax = ax[0,2], element=\"step\",color ='orange')\nsns.boxenplot(y=x, data =data , orient ='v' , ax=ax[0,3], palette=\"viridis\").set(title=title)\nsns.histplot(x=x1, data= data, ax = ax[1,2],element=\"poly\", color ='orange')\nsns.boxenplot(y=x1, data =data , orient ='v' , ax=ax[1,3],palette=\"rocket\").set(title=title);","2ad5a47f":"#Data Set\ndata.reset_index(drop=True, inplace=True)\n\n#Algorithms Default Parameters\nKNN = KNeighborsClassifier(n_neighbors=3)\nRF = RandomForestClassifier(max_depth = 30) \nGB = GradientBoostingClassifier(n_estimators=100, random_state=42)\nSV = SVC()\nLSV = LinearSVC() #doesnt give the Probability \nLR = LogisticRegression()","74fd5dd0":"# Class Relation\n\nsns.relplot(y='training_hours', x = 'city_development_index', hue='target',size='education_level',\n            data =data ,palette='viridis', sizes=(10, 200),alpha=.6,height=6);\n\nprint(\"Before choosing the best way to handle the data set , we have to get familiar with its distributions \\\nin here we can notice that out target -the chances looking\/ no looking for a job - will not be easy to classify\")\n\nprint(\"So the first question would be : which model will help us to do this job \\\ndo I want to Downsample, oversample , use a technique that combines both or just go with the skelarn approach, there is not a durect answer but we had\\\nsome clues : 1. the target are mixed , if we use SMOTE to create Synthetic samples and this kind of data set is perfect for that algorithm, 2. We can use Cross validation and AUC to choose the best strategy\")","cda1c0b9":"from imblearn.over_sampling import ADASYN, RandomOverSampler, SMOTE\n\n# Splitting\nX = data.drop(['target'], axis = 1)\ny = data[['target']]\ncv = StratifiedKFold(n_splits=3)\npos_label = y.target.value_counts(normalize=True).idxmin()","fb59e6f1":"# Creating List to Update Values\ndisp_default = [] # <- default Values\ndisp_var = [] #<- OverSampling \nscore = []  #<-Scores\n\n#Creating And Initiating\npipeline = [\n    # NO oversampling Techniques\n    make_pipeline(GB),\n    make_pipeline(RF),\n    make_pipeline(KNN),\n    make_pipeline(StandardScaler(),LR), \n    \n    ## oversampling Techniques\n    make_pipeline(SMOTE(random_state=42,sampling_strategy ='minority'), GB),\n    make_pipeline(ADASYN(random_state=42), RF),\n    make_pipeline(SMOTE(random_state=42), RF),\n    make_pipeline(ADASYN(random_state=42), KNN),\n    make_pipeline(SMOTE(random_state=42), KNN),\n    make_pipeline(ADASYN(random_state=42),StandardScaler(), LR),\n    make_pipeline(SMOTE(random_state=42),StandardScaler(), LR)]\n\n# Starting \nfor model in pipeline:\n    #Mean ROC curve\n    mean_tpr, mean_fpr = 0.0, np.linspace(0, 1, 100)\n    for train, test in cv.split(X, y):\n        \n        #Training Models - < Probability and Final Outputs\n        model.fit(X.iloc[train], y.iloc[train].values.ravel())\n        \n        y_proba = model.predict_proba(X.iloc[test])\n        y_pred = model.predict(X.iloc[test])\n\n        \n        #Creating Dimmensions\n        pos_label_idx = np.flatnonzero(model.classes_ == pos_label)[0]\n        fpr, tpr, thresholds = roc_curve(y.iloc[test], y_proba[:, pos_label_idx], pos_label=pos_label)\n        mean_tpr += np.interp(mean_fpr, fpr, tpr)\n        mean_tpr[0] = 0.0\n\n    mean_tpr \/= cv.get_n_splits(X, y)\n    mean_tpr[-1] = 1.0\n    mean_auc = auc(mean_fpr, mean_tpr)\n    \n    #------------IF AUC > 63 Pass\n    if (model[0].__class__.__name__ =='SMOTE') or (model[0].__class__.__name__ =='ADASYN'):\n        if mean_auc > .63:\n            disp_var.append(RocCurveDisplay(\n                    fpr=mean_fpr,tpr=mean_tpr,\n                    roc_auc=mean_auc,estimator_name=f\"{model[0].__class__.__name__}\"+\"_\"+f\"{model.steps[-1][0]}\"))\n            try:\n                model['smote'].fit_resample(X,y)\n                print(f\"{model[0].__class__.__name__}\"+\"_\"+f\"{model.steps[-1][0]}\")\n                print('Previous dataset shape %s' % Counter(data['target']))\n                print('New Data dataset shape %s' % Counter(y_res['target']), \"<-------Here new data\")\n                print(\"\\n\")\n            except: \n                pass\n    else:\n        disp_default.append(RocCurveDisplay(\n                fpr=mean_fpr,tpr=mean_tpr,\n                roc_auc=mean_auc, estimator_name=f\"{model[0].__class__.__name__}\"))\n        \n    # ----------Scores\n    y_true = y.iloc[test]\n    Pre_ = precision_score(y_true,y_pred,average='macro')\n    Rec_ = recall_score(y_true,y_pred,average='macro')\n    F1 = f1_score(y_true, y_pred,average='macro')   \n    name =f\"{model.steps[0][0]}\"+\"_\"+f\"{model.steps[-1][0]}\"\n    score.append(pd.Series({'name':name ,'Pre':Pre_,'Rec_':Rec_,'F1':F1}))\n    ","9891487e":"# ---- Plotting\nfig, ax = plt.subplots(1,3,figsize=(20, 10))\nfig.suptitle(\"Comparison of over-sampling methods with a KNN, RandomForest, Logistical Regression\")\nax[0].set_xlim([0, 1]) ,ax[1].set_xlim([0, 1]) \n\n\n#Visualization of classes\nsns.countplot(x='target', data=data, ax=ax[0])\nax[0].plot([0, 1], [0, 1])\n\nfor i in disp_default:\n    sns.despine(offset=10, ax=ax[0])\n    i.plot(ax=ax[1], linestyle=\"--\")\n    ax[1].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\")\n    \nfor j in disp_var:\n    sns.despine(offset=10, ax=ax[0])\n    j.plot(ax=ax[2], linestyle=\"--\")\n    ax[2].plot([0, 1], [0, 1], linestyle=\"--\", color=\"k\")\n    \n\n    \nprint(\"As we can observed KNN was eliminated because the AUC wasnt greater than 65 , another observation is that gradient boosting has better performance \\\nwihout Oversampling but we MUST analize Recall, Precision and that is exactly what you are going to see in the next section\\\nRemember each model is unique and sometime accuracy is no the best measurement\")\n","78a82b69":"final = pd.concat(score, axis=1).T\nfinal.sort_values(['Pre','Rec_','F1'], ascending=False)","29b206a2":"###### Experienced \nI will use this columns as condition to fill some nan values that i consider can be fill out","473dab56":"# Model","1b5e956b":"###### Company size\nOther columns - Please check Fuctions Section","ba567377":"# Conclusion","bb270c91":"###### Company size\nThose companies between 10-99 are start up\nThose row with univeristy_enrollment  = nan , experience = nan , last_new_job = never are equal to a new categories NW = 'Never Work' \nwe dont have that info not because was a mistake but because they never work","a31edf3e":"##### 1. Identify and Transform data into ML standard input - No string, No nan, No-Null","333f3547":"###### Oversampling\n\nthis consist on create\/copy \/ repeat samples from the category that has less observation in this case \"1\" I will use ADASYN, RandomOverSampler, SMOTE + Cross validation \nThere is a good example in Imblanaced-Sklearn\n\n1. Visualize\n2. Stratified\n3. Choose the model\n4. Plot curve","97c843d0":"###### Dealing with gender\nI assumed that 'nan values' in gender are those candidate that didnt want to provide that info, there fore fo to Others","6690070b":"#####  5. Define the best strategy to deal with imbalanced data set","c338786c":"### Segmented data set into Categorical and Numerical ","227782c3":"####  Analysis, Consideration and Transformation.\n\n1. Identify and Transform data into ML standard input - No string, No nan, No-Null\n2. Identify Outliers - Use unsupervised ML OUTLIERS\n3. Transform numerical Variables if they are skewed\n3. Define the best algorithms and Strategies\n4. Define the best strategy to deal with imbalanced data set\n5. Hyperparameter\n\n***Categorical Ordinal*** \n* ***education_level***,***experience***,***company_size***,***last_new_job***\n- --For Experiences-- \nI asuumed that the more experienced you are the higher the rank you get - but sure I also Understand that is not always like that - \nI create 8 bin that goes from [[0, 3) < [3, 6) < [6, 9) < [9, 12) < [12, 15) < [15, 18) < [18, 21) < [21, 22)] and 0 - 8 respectively\n\n- --For Education Level-- \nThe Higher Education the Higher Rank \n\n- --For Company Size-- \nThis is a bit controversial because there is not a studies that support my theory but based on my studies in MBA and after consulting with   @Mauricio - https:\/\/www.linkedin.com\/in\/mauricio-rodr%C3%ADguez-082170109\/-   expert in Human Resources and Recruitment \nWe concluded that the average attrition rate known as  \"turnover\" in Start-up  is higher than in well-established company which suggest that the chances of quitting and go to other companies is higher, and because this study is about that i assigned values from  -1-8 where 8  is the best one which means your chances of leave the company are lower\n\n***Categorical Nominal*** \n* ***city_development_index***,***training_hours*** \nfind their skewness and apply tranformation if it is needed\n\n***Enconding*** \n* ***gender***,***relevent_experience***,***major_discipline***,***enrolled_university*** \nI will use OneHotEncoder to transformed this variables into Categorical to avoid any preferences \n\n***Final notes*** : I could use LabelEnconder from scikit-learn but it doesnt have a way to define the order of importances -- actually it has but was easier do it manually -","b1f1622d":"###### Nominal","14890d95":"# Obtain Data","992600af":"---------------No more Nan, Null Values ---------------------","80984493":"Welcome to this new post, these are the most important thing in this notebook\n1. Feature Engineering techniques to fill nan Values\n2. Unsupervise Machine Learning for Outlier detection\n3. Pipeline + Cross_validation\n4. Technique to handle Imbalanced Data\n5. Avance Pipeline + AUR Curve\n\n\nThanks for passing by... I am really gld to share this with the community , ***please leave a comment or vote up \nI am one medal away to be notebook Expert and I am working on my CV \/ Potofolio etc ***","6339321a":"##### 3.Transform numerical Variables if they are skewed","6a7eca4e":"### Dropping columns\n\nI strongly believe that city and city development  are related , therefore I can choose the numerical value (index) to avoid high dimensionality","9d704442":"###### Ordinal","62a896dc":"##### 4. Define the best algorithms and Strategies \n* I chose the following algorithms because the nature of the data , classification task, training time , the ability of classifying data in medium - high dimensionality space","e3b31e76":"# Scrub Data\n\nBased on my studies in MBA a start-up usually have lot of challeging 'money wise' \nUsually they look for MAster or phd for intership \ntherefore i asume that if you are MAster\/phd with more than 10 years your salsary will be a good one \nine that a start up wont pay becaue the financial difficulties and if you stay ther emore thant 3 years means that it is not a start up anymore ","67925af3":"This is the most crucial part , as I said before accuracy is not always the best way to measure  a model \nin this example we have imbalanced model bias to 1 \"Looking for a job change\" so the model itself would be great at\nidentifying people who is looking for a job change, as the description said\n***wants to hire data scientists among people who successfully pass some courses which conduct by the company***, there fore precision is the most relevant measurement , because it can predict correctly 71% of the time , but how about if it were in the other way around let say the company XXX end people to this courses and they want to make sure they wont leave the company for this new one , in this case the most important measurement in rec_ that can be understood as \"the ability of the model to identify those who doesn't want to change their job\"\n\nBut this alway up to the Team who is taking care of this issue.\nand how about you..!! what would you do? and how would you explain this to your manager?\n\n\n*** Another Observations ***\n- Downsampling technique can be implemented as well - \"I will do in few weeks\"\n- we can also find better parameters for gradientboostingclassifier such as Learning rate\n- In real life would be better to collect more data and make sure about people response.\n\nHope you can learn something from this notebook ...!!!\nany feedback \/ Comment is more than welcome ","a475a9f2":"# Explore","7997cfa5":"###### others\nThis are the row that have more than 4 nan values so it is Imposible to make any connection between columns","9cbe3414":"##### 2. Identify Outliers"}}