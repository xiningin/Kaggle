{"cell_type":{"3f780e45":"code","5cc3d319":"code","9f980a14":"code","6d3b2a71":"code","e55ec874":"code","474d6c08":"code","e5722443":"code","63677e4c":"code","866b6036":"code","48d41abd":"code","85ace208":"code","a9de0902":"code","36c36844":"code","272531cf":"code","1faf6759":"code","4e70a45e":"code","b2e1d429":"code","e41058fe":"code","6c81fb34":"code","78812f17":"code","c9201859":"code","84625e2b":"code","6d66bec3":"code","e3b96eed":"code","da0c68d5":"code","ff2a5b6a":"code","ec1d572e":"code","f9296163":"code","a5684bfc":"code","eb3b5690":"code","fe6f66f4":"code","db3aecd3":"code","f120b3c8":"code","f183573c":"code","710f61c5":"code","ba66e813":"code","ed6dca1f":"code","310261c9":"code","ba3e64bd":"code","7524547f":"code","db2d40d1":"code","c856b53b":"code","59682468":"code","818a88b1":"code","90dacf05":"code","0520088a":"code","5325391e":"markdown","fa2959f5":"markdown","e2d85ecc":"markdown","25e16069":"markdown","c1ca38e5":"markdown","9cfaee4b":"markdown","71f9e4c2":"markdown","3f9704d5":"markdown","1af443c6":"markdown","ce5d1aa9":"markdown","ab1ffa7f":"markdown","e28bed8d":"markdown","ec26e4fa":"markdown","a7827cff":"markdown","0a098968":"markdown","aca930be":"markdown","288d95d2":"markdown","1e3728a8":"markdown"},"source":{"3f780e45":"#import libraries\nimport pandas as pd\nfrom scipy import stats\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","5cc3d319":"#import data\ncc_data = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')","9f980a14":"#look at distributions\ncc_data.describe()","6d3b2a71":"#look at formatting\ncc_data.head()","e55ec874":"#look for null values and at dtypes\ncc_data.info()","474d6c08":"#look at null values for minimum_payments\ncc_data[cc_data.MINIMUM_PAYMENTS.isna()]","e5722443":"#look at null values for credit_limit\ncc_data[cc_data.CREDIT_LIMIT.isna()]","63677e4c":"#look at CUST_ID tail\ncc_data.tail()","866b6036":"#look at data 3 std from mean\ncc_data[np.abs(stats.zscore(cc_data.loc[:, (cc_data.columns != 'CUST_ID')])) >= 3]","48d41abd":"#histograms of all columns except cust_id\nfor i in cc_data.loc[:, cc_data.columns != 'CUST_ID']:\n    plt.hist(cc_data[i], edgecolor='black')\n    plt.xticks()\n    plt.xlabel(i)\n    plt.ylabel('number of people')\n    plt.show()","85ace208":"#heat map to find extreme positive and negative correlations\nplt.figure(figsize=(16, 6))\nsns.heatmap(cc_data.loc[:, (cc_data.columns != 'CUST_ID')].corr(), annot=True)\nplt.title('Correlation Heatmap for Numerical Variables', fontdict={'fontsize':12}, pad=12);","a9de0902":"#pairplot to show plots against each variable\nsns.pairplot(cc_data.loc[:, (cc_data.columns != 'CUST_ID')])\nplt.show()","36c36844":"#use simpleimputer to impute using the median\nfrom sklearn.impute import SimpleImputer\n\nimputer = SimpleImputer(missing_values=np.nan, strategy='median')\ncc_data['MINIMUM_PAYMENTS'] = imputer.fit_transform(cc_data['MINIMUM_PAYMENTS'].values.reshape(-1,1))\ncc_data['CREDIT_LIMIT'] = imputer.fit_transform(cc_data['CREDIT_LIMIT'].values.reshape(-1,1))","272531cf":"#select all data except CUST_ID\ncc_data_for_PCA = cc_data.loc[:, (cc_data.columns != 'CUST_ID')]\n\n#standardize\ncc_data_for_PCA_scaled = (cc_data_for_PCA - cc_data_for_PCA.mean(axis=0)) \/ cc_data_for_PCA.std(axis=0)\n\nfrom sklearn.decomposition import PCA\n\n#create principal components (2 axes based on elbow method below)\npca = PCA(2)\ncc_data_pca = pca.fit_transform(cc_data_for_PCA_scaled)\n\n#convert to dataframe\ncomponent_names = [f\"PC{i+1}\" for i in range(cc_data_pca.shape[1])]\ncc_data_pca = pd.DataFrame(cc_data_pca, columns=component_names)","1faf6759":"#plot data using principal components\nsns.scatterplot(x=cc_data_pca.loc[:,'PC1'],y=cc_data_pca.loc[:,'PC2'])\nplt.show()","4e70a45e":"#determine loadings\nloadings = pd.DataFrame(\n    pca.components_.T,  # transpose the matrix of loadings\n    columns=component_names,  # so the columns are the principal components\n    index=cc_data.loc[:, (cc_data.columns != 'CUST_ID')].columns,  # and the rows are the original features\n)\nloadings\n\n#PC1 is characterized by how much the card owner purchases using card\n#PC2 is characterized by how much the card owner takes cash advances using card","b2e1d429":"#determine % explained variance and use % cumulative variance for elbow method to determine number of PCs\n\ndef plot_variance(pca, width=8, dpi=100):\n    # Create figure\n    fig, axs = plt.subplots(1, 2)\n    n = pca.n_components_\n    grid = np.arange(1, n + 1)\n    # Explained variance\n    evr = pca.explained_variance_ratio_\n    axs[0].bar(grid, evr)\n    axs[0].set(\n        xlabel=\"Component\", title=\"% Explained Variance\", ylim=(0.0, 1.0)\n    )\n    # Cumulative Variance\n    cv = np.cumsum(evr)\n    axs[1].plot(np.r_[0, grid], np.r_[0, cv], \"o-\")\n    axs[1].set(\n        xlabel=\"Component\", title=\"% Cumulative Variance\", ylim=(0.0, 1.0)\n    )\n    # Set up figure\n    fig.set(figwidth=8, dpi=100)\n    return axs\n\nplot_variance(pca);\n\n#about 50% of the variance is explained by these principal components\n#2 PCA chosen based on the elbow method","e41058fe":"cc_data['AVG_PURCHASE_TRX_PRICE'] = cc_data.loc[:,'PURCHASES']\/cc_data.loc[:,'PURCHASES_TRX']","6c81fb34":"cc_data['AVG_PURCHASE_TRX_PRICE'] = cc_data.AVG_PURCHASE_TRX_PRICE.replace(np.NaN, 0)\ncc_data['AVG_PURCHASE_TRX_PRICE'] = cc_data.AVG_PURCHASE_TRX_PRICE.replace(np.inf, 0)","78812f17":"cc_data['BALANCE_TO_CREDIT_LIMIT'] = cc_data.loc[:,'BALANCE']\/cc_data.loc[:,'CREDIT_LIMIT']","c9201859":"cc_data['AVG_CASH_ADVANCE_TRX_AMOUNT'] = cc_data.loc[:,'CASH_ADVANCE']\/cc_data.loc[:,'CASH_ADVANCE_TRX']","84625e2b":"cc_data['AVG_CASH_ADVANCE_TRX_AMOUNT'] = cc_data.AVG_CASH_ADVANCE_TRX_AMOUNT.replace(np.NaN, 0)\ncc_data['AVG_CASH_ADVANCE_TRX_AMOUNT'] = cc_data.AVG_CASH_ADVANCE_TRX_AMOUNT.replace(np.inf, 0)","6d66bec3":"#standardize for kmeans\nfrom sklearn.preprocessing import StandardScaler\n\nkmeans_columns1 = ['AVG_PURCHASE_TRX_PRICE', 'ONEOFF_PURCHASES','INSTALLMENTS_PURCHASES']\ncc_data_kmeans1 = cc_data.loc[:, kmeans_columns1]\n\nstandardize = StandardScaler()\ncc_data_kmeans1 = standardize.fit_transform(cc_data_kmeans1)","e3b96eed":"from sklearn.cluster import KMeans\n\nkmeans_models = [KMeans(n_clusters=k, random_state=1).fit(cc_data_kmeans1) for k in range (1, 10)]\ninnertia = [model.inertia_ for model in kmeans_models]\n\nplt.plot(range(1, 10), innertia)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","da0c68d5":"#generate clusters to determine different clusters to advertise to\n\nkmeans = KMeans(n_clusters=4, random_state = 1)\ncc_data[\"Ad_Groups_Purchases\"] = kmeans.fit_predict(cc_data_kmeans1)\n\ncc_data[\"Ad_Groups_Purchases\"] = cc_data[\"Ad_Groups_Purchases\"].astype(\"category\")","ff2a5b6a":"#plot variables against clusters\n\nfor i in ['BALANCE', 'BALANCE_FREQUENCY', 'BALANCE_TO_CREDIT_LIMIT','AVG_PURCHASE_TRX_PRICE', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']:\n    sns.stripplot(x ='Ad_Groups_Purchases',y=i,data=cc_data)\n    plt.show()","ec1d572e":"#standardize for kmeans\nkmeans_columns2 = ['BALANCE', 'AVG_CASH_ADVANCE_TRX_AMOUNT']\ncc_data_kmeans2 = cc_data.loc[:, kmeans_columns2]\n\nstandardize = StandardScaler()\ncc_data_kmeans2 = standardize.fit_transform(cc_data_kmeans2)","f9296163":"kmeans_models = [KMeans(n_clusters=k, random_state=1).fit(cc_data_kmeans2) for k in range (1, 10)]\ninnertia = [model.inertia_ for model in kmeans_models]\n\nplt.plot(range(1, 10), innertia)\nplt.title('Elbow method')\nplt.xlabel('Number of Clusters')\nplt.ylabel('WCSS')\nplt.show()","a5684bfc":"#generate clusters to determine different clusters to advertise to\nkmeans = KMeans(n_clusters=4, random_state = 1)\ncc_data[\"Ad_Groups_Cash_Advance\"] = kmeans.fit_predict(cc_data_kmeans2)\n\ncc_data[\"Ad_Groups_Cash_Advance\"] = cc_data[\"Ad_Groups_Cash_Advance\"].astype(\"category\")","eb3b5690":"#plot variables against clusters\nfor i in ['BALANCE', 'BALANCE_FREQUENCY', 'BALANCE_TO_CREDIT_LIMIT', 'AVG_PURCHASE_TRX_PRICE', 'PURCHASES',\n       'ONEOFF_PURCHASES', 'INSTALLMENTS_PURCHASES', 'AVG_CASH_ADVANCE_TRX_AMOUNT','CASH_ADVANCE',\n       'PURCHASES_FREQUENCY', 'ONEOFF_PURCHASES_FREQUENCY',\n       'PURCHASES_INSTALLMENTS_FREQUENCY', 'CASH_ADVANCE_FREQUENCY',\n       'CASH_ADVANCE_TRX', 'PURCHASES_TRX', 'CREDIT_LIMIT', 'PAYMENTS',\n       'MINIMUM_PAYMENTS', 'PRC_FULL_PAYMENT', 'TENURE']:\n    sns.stripplot(x ='Ad_Groups_Cash_Advance',y=i,data=cc_data)\n    plt.show()","fe6f66f4":"#create X and y variables\nX1 = cc_data.loc[:,kmeans_columns1]\ny1 = cc_data['Ad_Groups_Purchases']\n\nX2 = cc_data.loc[:,kmeans_columns2]\ny2 = cc_data['Ad_Groups_Cash_Advance']","db3aecd3":"#import libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","f120b3c8":"#create dummy variables for y\ny1 = pd.get_dummies(y1)\n\n# train\/test split with stratify making sure classes are evenlly represented across splits\nX_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, stratify=y1, train_size=0.75, random_state=1)\n\n#define scaler\nscaler=MinMaxScaler()\n\n#apply preprocessing to split data with scaler\nX_train1 = scaler.fit_transform(X_train1)\nX_test1 = scaler.transform(X_test1)","f183573c":"#create dummy variables for y\ny2 = pd.get_dummies(y2)\n\n# train\/test split with stratify making sure classes are evenlly represented across splits\nX_train2, X_test2, y_train2, y_test2 = train_test_split(X2, y2, stratify=y2, train_size=0.75, random_state=1)\n\n#define scaler\nscaler=MinMaxScaler()\n\n#apply preprocessing to split data with scaler\nX_train2 = scaler.fit_transform(X_train2)\nX_test2 = scaler.transform(X_test2)","710f61c5":"#import machine learning libraries\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom numpy import mean, std","ba66e813":"#KNeighborsClassifier with five-fold cross-validation\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train1,y_train1,cv=5)\nprint(mean(cv), '+\/-', std(cv))","ed6dca1f":"#random forest classifier with five-fold cross validation\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train1,y_train1,cv=5)\nprint(mean(cv), '+\/-', std(cv))","310261c9":"#ml algorithm tuner\nfrom sklearn.model_selection import GridSearchCV \n\n#performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: {} +\/- {}'.format(str(classifier.best_score_),str(classifier.cv_results_['std_test_score'][classifier.best_index_])))\n    print('Best Parameters: ' + str(classifier.best_params_))","ba3e64bd":"#KNeighborsClassifier tuning with five-fold cross-validation\nknn = KNeighborsClassifier()\nparam_grid = {\n              'n_neighbors' : np.arange(5,12,1),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]\n             }\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train1,y_train1)\nclf_performance(best_clf_knn,'KNN')","7524547f":"#RandomForest tuning with five-fold cross-validation\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {\n                'n_estimators': np.arange(5,10,1), \n                'bootstrap': [True,False], #bagging (T) vs. pasting (F)\n                #'max_depth': [1],\n                'max_features': ['auto','sqrt'],\n                'min_samples_leaf': np.arange(1,5,1),\n                'min_samples_split': np.arange(1,5,1)\n              }\nclf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train1,y_train1)\nclf_performance(best_clf_rf_rnd,'Random Forest')","db2d40d1":"from sklearn.metrics import accuracy_score\n\nrf = RandomForestClassifier(random_state = 1, bootstrap= False, max_features= 'auto', min_samples_leaf= 3, min_samples_split= 2, n_estimators= 7)\nrf.fit(X_train1,y_train1)\ny_pred1 = rf.predict(X_test1)\n\n#assess accuracy\nprint('RandomForestClassifier test accuracy: {}'.format(accuracy_score(y_test1, y_pred1)))","c856b53b":"#KNeighborsClassifier with five-fold cross-validation\nknn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train2,y_train2,cv=5)\nprint(mean(cv), '+\/-', std(cv))","59682468":"#random forest classifier with five-fold cross validation\nrf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train2,y_train2,cv=5)\nprint(mean(cv), '+\/-', std(cv))","818a88b1":"#KNeighborsClassifier tuning with five-fold cross-validation\nknn = KNeighborsClassifier()\nparam_grid = {\n              'n_neighbors' : np.arange(5,12,1),\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]\n             }\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = False, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train2,y_train2)\nclf_performance(best_clf_knn,'KNN')","90dacf05":"#RandomForest tuning with five-fold cross-validation\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {\n                'n_estimators': np.arange(9,15,1), \n                'bootstrap': [True,False], #bagging (T) vs. pasting (F)\n                #'max_depth': [1],\n                'max_features': ['auto','sqrt'],\n                'min_samples_leaf': np.arange(1,5,1),\n                'min_samples_split': np.arange(4,10,1)\n              }\nclf_rf_rnd = GridSearchCV(rf, param_grid = param_grid, cv = 5, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train2,y_train2)\nclf_performance(best_clf_rf_rnd,'Random Forest')","0520088a":"knn = KNeighborsClassifier(algorithm= 'auto', n_neighbors= 5, p= 1, weights= 'distance')\nknn.fit(X_train2,y_train2)\ny_pred2 = knn.predict(X_test2)\n\n#assess accuracy\nprint('KNN test accuracy: {}'.format(accuracy_score(y_test2, y_pred2)))","5325391e":"# Purchases Clusters ML","fa2959f5":"**Baseline**","e2d85ecc":"# EDA","25e16069":"# Cash Advance Clusters ML","c1ca38e5":"**Hyperparameter tuning**","9cfaee4b":"# Cluster Analysis","71f9e4c2":"**Purchases**","3f9704d5":"**Hyperparameter tuning**","1af443c6":"**Principal Component Analysis (PCA)**","ce5d1aa9":"**Final model**","ab1ffa7f":"**Cash Advances**","e28bed8d":"**Baseline**","ec26e4fa":"**Import libraries and data**","a7827cff":"# Feature Engineering","0a098968":"**Impute null values**","aca930be":"**Correlation heatmap and plotting correlated variables**","288d95d2":"# Preprocessing","1e3728a8":"**Final model**"}}