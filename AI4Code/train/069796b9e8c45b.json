{"cell_type":{"92e3cc3d":"code","4fb85b8c":"code","df509dfd":"code","799ff492":"code","5926ace8":"code","132b3ff6":"code","5c3385d2":"code","d751a3eb":"code","66a9837b":"code","3535eed7":"code","ead73382":"code","c97deba5":"code","e3163094":"code","876bafe4":"code","f2ba48d0":"code","780f6996":"code","5f6afbb3":"code","ffc0d2e6":"code","5f099561":"code","fa67df2b":"code","42aac8b3":"code","026d31b3":"code","268e599e":"code","86874f34":"code","256e9dc8":"code","97c310f3":"code","26ebeb76":"code","4539a394":"markdown","1c2582cb":"markdown","193e9789":"markdown","54046887":"markdown","60bee9d4":"markdown","90e3e665":"markdown","5959337b":"markdown","4bee0e4c":"markdown"},"source":{"92e3cc3d":"\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, f1_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')\nwarnings.filterwarnings('ignore', module='IPython')","4fb85b8c":"train_df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv');\ncombined_df= pd.concat([train_df, test_df]) ","df509dfd":"train_df.head()\n","799ff492":"train_df.info()\n","5926ace8":"test_df.info()\n","132b3ff6":"for col in train_df.columns:\n     if (train_df[col].nunique() < 10):\n        print(f\"{col}: {train_df[col].unique()}\")","5c3385d2":"train_df[\"Cabin_is_NaN\"]=np.array(train_df[\"Cabin\"].isna())\ntrain_df[\"Age_is_NaN\"]=np.array(train_df[\"Age\"].isna())\ntrain_df.loc[train_df['Age'] <= 9, 'Child'] = 1\ntrain_df.loc[train_df['Age'] > 9, 'Child'] = 0\n\ntest_df[\"Cabin_is_NaN\"]=np.array(test_df[\"Cabin\"].isna())\ntest_df[\"Age_is_NaN\"]=np.array(test_df[\"Age\"].isna())\ntest_df.loc[test_df['Age'] <= 9, 'Child'] = 1\ntest_df.loc[test_df['Age'] > 9, 'Child'] = 0","d751a3eb":"\nplt.figure(figsize = (12, 10))\n\nmask = np.zeros_like(train_df.corr())\nmask[np.triu_indices_from(mask)] = True\nfor i in range(8):\n    mask[i][i]=0\nsns.heatmap(train_df.corr(), mask=mask ,annot = True, linewidths=.5, cmap = 'Blues')","66a9837b":"train_df=train_df.drop(['Cabin','Age','Ticket','Name','PassengerId'], axis =1)\ntest_df=test_df.drop(['Cabin','Age','Ticket','Name'], axis =1)\n","3535eed7":"train_df_1 = pd.get_dummies(train_df,drop_first=True)\ntest_df_1 = pd.get_dummies(test_df,drop_first=True)","ead73382":"train_df_1['Child'].fillna(train_df_1['Child'].mean(), inplace=True)\ntest_df_1['Child'].fillna(test_df_1['Child'].mean(), inplace=True)\n","c97deba5":"\nplt.figure(figsize = (12, 10))\n\nmask = np.zeros_like(train_df_1.corr())\nmask[np.triu_indices_from(mask)] = True\nfor i in range(11):\n    mask[i][i]=0\n    \nsns.heatmap(train_df_1.corr(),mask=mask,annot=True, fmt=\".2f\", cmap=\"Blues\",)\n    \n\n","e3163094":"train_df_1.dtypes","876bafe4":"train_df_1['Cabin_is_NaN']=train_df_1[\"Cabin_is_NaN\"].apply(lambda x: int(x))\ntrain_df_1['Age_is_NaN']=train_df_1[\"Age_is_NaN\"].apply(lambda x: int(x))\n\ntest_df_1['Cabin_is_NaN']=train_df_1[\"Cabin_is_NaN\"].apply(lambda x: int(x))\ntest_df_1['Age_is_NaN']=train_df_1[\"Age_is_NaN\"].apply(lambda x: int(x))\n\ntrain_df_1.head()","f2ba48d0":"train_df_1.columns\n","780f6996":"Feature_Cols= [ 'Pclass', 'SibSp', 'Parch', 'Fare', 'Cabin_is_NaN',\n       'Age_is_NaN', 'Child', 'Sex_male', 'Embarked_Q', 'Embarked_S']\nPrediction_Col= ['Survived']","5f6afbb3":"strat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(train_df_1[Feature_Cols], train_df_1.Survived))\n\n# Create the dataframes\nX_train = train_df_1.loc[train_idx, Feature_Cols]\ny_train = train_df_1.loc[train_idx, 'Survived']\n\nX_test  = train_df_1.loc[test_idx, Feature_Cols]\ny_test  = train_df_1.loc[test_idx, 'Survived']\nlen(X_test), len(X_train)\n","ffc0d2e6":"metrics = pd.DataFrame()\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nprecision_lr, recall_lr = (round(float(x),2) for x in list(score(y_test,\n                                                                    y_pred_lr,\n                                                                    average='weighted'))[:-2])\n# adding lr stats to metrics DataFrame\nlr_stats = pd.Series({'precision':precision_lr,\n                      'recall':recall_lr,\n                      'accuracy':round(accuracy_score(y_test, y_pred_lr), 2),\n                      'f1score':round(f1_score(y_test, y_pred_lr), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_lr),2)},\n                     name='Logistic Regression')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_lr, output_dict=True)).iloc[:3,:2]","5f099561":"# Estimate KNN model and report outcomes\nknn = KNeighborsClassifier(n_neighbors=3, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\nprecision_knn, recall_knn = (round(float(x),2) for x in list(score(y_test,\n                                                                      y_pred_knn,\n                                                                      average='weighted'))[:-2])\n# adding KNN stats to metrics DataFrame\nknn_stats = pd.Series({'precision':precision_knn,\n                      'recall':recall_knn,\n                      'accuracy':round(accuracy_score(y_test, y_pred_knn), 2),\n                      'f1score':round(f1_score(y_test, y_pred_knn), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_knn),2)}, name='KNN')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_knn, output_dict=True)).iloc[:3,:2]","fa67df2b":"dt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)\ndt.tree_.node_count, dt.tree_.max_depth","42aac8b3":"y_train_pred = dt.predict(X_train)\ny_pred_dt = dt.predict(X_test)\n\nprecision_dt, recall_dt = (round(float(x),2) for x in list(score(y_test,\n                                                                y_pred_dt,\n                                                                average='weighted'))[:-2])\n# adding dt stats to metrics DataFrame\ndt_stats = pd.Series({'precision':precision_dt,\n                      'recall':recall_dt,\n                      'accuracy':round(accuracy_score(y_test, y_pred_dt), 2),\n                      'f1score':round(f1_score(y_test, y_pred_dt), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_dt),2)}, name='Decision Tree')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_dt, output_dict=True)).iloc[:3,:2]","026d31b3":"\n# Initialize the random forest estimator\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\n\n# initialise list for out of bag error\noob_list = list()\n\n# Iterate through all of the possibilities for number of trees\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    \n    # Use this to set the number of trees\n    RF.set_params(n_estimators=n_trees)\n    \n    # Fit the model\n    RF.fit(X_train, y_train)\n    \n    # Get the out of bag error and store it\n    oob_error = 1 - RF.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')","268e599e":"sns.set_context('talk')\nsns.set_style('white')\n\nax = rf_oob_df.plot(legend=False, marker='o', color=\"orange\", figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error');","86874f34":"rf = RF.set_params(n_estimators=100)\n\ny_pred_rf = rf.predict(X_test)\nprecision_rf, recall_rf = (round(float(x),2) for x in list(score(y_test,\n                                                                    y_pred_rf,\n                                                                    average='weighted'))[:-2])\nrf_stats = pd.Series({'precision':precision_rf,\n                      'recall':recall_rf,\n                      'accuracy':round(accuracy_score(y_test, y_pred_rf), 2),\n                      'f1score':round(f1_score(y_test, y_pred_rf), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_rf),2)}, name='Random Forest')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_rf, output_dict=True)).iloc[:3,:2]","256e9dc8":"fig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\n\nmodels = coeff_labels = ['lr', 'knn', 'dt', 'rf']\ncm = [confusion_matrix(y_test, y_pred_lr),\n      confusion_matrix(y_test, y_pred_knn),\n      confusion_matrix(y_test, y_pred_dt),\n      confusion_matrix(y_test, y_pred_rf)]\nlabels = ['False', 'True']\n\nfor ax,model, idx in zip(axList, models, range(0,4)):\n    sns.heatmap(cm[idx], ax=ax, annot=True, fmt='d', cmap='summer');\n    ax.set(title=model);\n    ax.set_xticklabels(labels, fontsize=20);\n    ax.set_yticklabels(labels[::-1], fontsize=20);\n    ax.set_ylabel('Prediction', fontsize=25);\n    ax.set_xlabel('Ground Truth', fontsize=25)\n    \nplt.tight_layout()","97c310f3":"metrics.append([lr_stats, knn_stats, dt_stats, rf_stats])\n","26ebeb76":"test_df_1.fillna(test_df_1.mean(), inplace=True)\n\npredictions = RF.predict(test_df_1[Feature_Cols])\noutput = pd.DataFrame({'PassengerId': test_df_1.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\n","4539a394":"#### Pclass : Ticket class (1 = 1st, 2 = 2nd, 3 = 3rd)\n#### SibSp : # of siblings \/ spouses aboard the Titanic\n#### Parch : # of parents \/ children aboard the Titanic\n#### Embarked : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","1c2582cb":"# Peer review assignment","193e9789":"### We have missing values in Age and Cabin and one missing value for Fare in the test_df.\n","54046887":"## Logistic regression\n","60bee9d4":"## KNN\n","90e3e665":"## Decision Tree","5959337b":"## 1. Dataset\n#### This is the legendary Titanic ML competition \u2013 the best, first challenge for you to dive into ML competitions and familiarize yourself with how the Kaggle platform works.\n#### The competition is simple: use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n#### I have already made a submission using simple linear regression and was curious about performance improvements that could be made.","4bee0e4c":"## Results\n### The classsification report of each classifier shows that I am able to predict consistent classification, with an F1 score of 0.84 for RandomForestClassifier model. Similar result can be achieved using any of the model above.\n"}}