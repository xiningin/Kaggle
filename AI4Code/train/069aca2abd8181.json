{"cell_type":{"67d2c8b9":"code","2ddc3cd1":"code","8df67211":"code","c0bea875":"code","01e8e382":"code","0ef7545b":"code","e97ba959":"code","12a05b6f":"code","8b641c87":"code","f811efd0":"code","5029a1da":"code","9a6328af":"code","f925b730":"code","cc3ab0b6":"code","900cbfab":"code","fe2df577":"code","d1c716a3":"code","1f585086":"code","b16432e0":"code","751f95a2":"code","38aaad43":"code","2e949f39":"code","4df62a7c":"code","8ed2cf3d":"code","fded5f38":"code","090dac3a":"code","abb167bf":"code","506785bc":"code","d60875d8":"code","7598f953":"code","b2841475":"code","2866f917":"code","0083ec4a":"code","f53b44ee":"code","1d79cb45":"code","f71c0cfa":"code","2d1a0282":"code","c9214688":"code","69eb54c1":"code","15d606bb":"code","74e19762":"code","cac5f750":"code","347ca8f0":"code","c3abb20c":"code","53f18d46":"markdown","ed2edffc":"markdown","98f689c6":"markdown","57107729":"markdown","76f8645f":"markdown","e3196203":"markdown","aaddbcf3":"markdown","44ab04da":"markdown","2891ba97":"markdown","ec890c83":"markdown","96a296bd":"markdown","6c4ab864":"markdown","ab070121":"markdown","08faf576":"markdown","26a25a61":"markdown","0407fc95":"markdown","188b07e2":"markdown","80f19576":"markdown","adaf3467":"markdown","9510bc08":"markdown","be3f3e31":"markdown","c81a4976":"markdown","04cbaa56":"markdown","536abe34":"markdown","418638a9":"markdown","2c00631b":"markdown","1c2292b0":"markdown","c0f7e99f":"markdown","b13b5239":"markdown"},"source":{"67d2c8b9":"import os\nimport random\nimport platform\nimport itertools\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\npd.set_option('display.width', None)\npd.set_option('display.max_column', None)\n\nSEED = 42\n\nos.environ['PYTHONHASHSEED']=str(SEED)\nrandom.seed(SEED)\nnp.random.seed(SEED)\n\nprint('Python version:', platform.python_version())\nprint('Numpy version:', np.__version__)\nprint('Pandas version:', pd.__version__)\nprint('Matplotlib version:', matplotlib.__version__)\nprint('Seaborn version:', sns.__version__)","2ddc3cd1":"basedir = '..\/input\/shopee-code-league-20\/_DA_Marketing_Analytics'\n\n# converting to dataframe\ndf_train = pd.read_csv(os.path.join(basedir,'train.csv'))\ndf_test = pd.read_csv(os.path.join(basedir,'test.csv'))\ndf_users = pd.read_csv(os.path.join(basedir,'users.csv'))","8df67211":"df_train.dtypes","c0bea875":"df_test.dtypes","01e8e382":"df_users.dtypes","0ef7545b":"df_train.isnull().sum()","e97ba959":"df_test.isnull().sum()","12a05b6f":"df_users.isnull().sum()","8b641c87":"print(np.sort(df_train['country_code'].unique()))\nprint(np.sort(df_test['country_code'].unique()))","f811efd0":"print(np.sort(df_users['attr_1'].unique()))\nprint(np.sort(df_users['attr_2'].unique()))\nprint(np.sort(df_users['attr_3'].unique()))\nprint(np.sort(df_users['domain'].unique()))","5029a1da":"df_train.corr()","9a6328af":"list_unique = df_users['domain'].unique()\ndict_unique = {list_unique[i]: i for i in range(len(list_unique))}\ndf_users['domain'] = df_users['domain'].apply(lambda d: dict_unique[d])","f925b730":"def convert(day):\n    try:\n        return np.float(day)\n    except:\n        return np.nan","cc3ab0b6":"df_train['last_open_day'] = df_train['last_open_day'].apply(convert)\ndf_train['last_login_day'] = df_train['last_login_day'].apply(convert)\ndf_train['last_checkout_day'] = df_train['last_checkout_day'].apply(convert)\n\ndf_test['last_open_day'] = df_test['last_open_day'].apply(convert)\ndf_test['last_login_day'] = df_test['last_login_day'].apply(convert)\ndf_test['last_checkout_day'] = df_test['last_checkout_day'].apply(convert)","900cbfab":"df_train = df_train.join(df_users, on='user_id', rsuffix='_unused')\ndf_test = df_test.join(df_users, on='user_id', rsuffix='_unused')","fe2df577":"del df_train['user_id']\ndel df_train['user_id_unused']\ndel df_train['row_id']\n\ndel df_test['user_id']\ndel df_test['user_id_unused']\ndel df_test['row_id']","d1c716a3":"df_train['day'] = pd.to_datetime(df_train['grass_date']).dt.dayofweek.astype('category')\ndf_test['day'] = pd.to_datetime(df_test['grass_date']).dt.dayofweek.astype('category')","1f585086":"del df_train['grass_date']\ndel df_test['grass_date']","b16432e0":"def fix_age(age):\n    if age < 18 or age >= 100:\n        return np.nan\n    else:\n        return age\n    \ndf_train['age'] = df_train['age'].apply(fix_age)\ndf_test['age'] = df_test['age'].apply(fix_age)","751f95a2":"# domain\n# 1 -> 'other' domain from previous preprocessing\n# df_train['domain_nan'] = df_train['domain'].isnull()\ndf_train['domain'] = df_train['domain'].fillna(1)\n\n# df_test['domain_nan'] = df_test['domain'].isnull()\ndf_test['domain'] = df_test['domain'].fillna(1)","38aaad43":"df_train","2e949f39":"df_test","4df62a7c":"df_users","8ed2cf3d":"df_train.to_csv('train_processed.csv', index=False)\ndf_test.to_csv('test_processed.csv', index=False)\ndf_users.to_csv('users_processed.csv', index=False)","fded5f38":"df_train.to_parquet('train_processed.parquet', engine='pyarrow')\ndf_test.to_parquet('test_processed.parquet', engine='pyarrow')\ndf_users.to_parquet('users_processed.parquet', engine='pyarrow')","090dac3a":"import sklearn\nimport lightgbm as lgbm\nimport scipy\n\nprint('Scikit-Learn version:', sklearn.__version__)\nprint('LightGBM version:', lgbm.__version__)\nprint('Scipy version:', scipy.__version__)","abb167bf":"X = df_train.copy()\ndel X['open_flag']\n\nX_test = df_test.copy()\n\ny = df_train['open_flag'].to_numpy()","506785bc":"cat_feature = [\n    'country_code','attr_1', 'attr_2', 'attr_3',\n    'domain','day',\n#     'last_open_day_nan', 'last_login_day_nan',\n#     'last_checkout_day_nan', 'attr_1_nan', 'attr_2_nan',\n#     'attr_3_nan', 'age_nan', 'domain_nan',\n    \n]\ncat_feature_idx = [X.columns.get_loc(ct) for ct in cat_feature]\ncat_feature_idx","d60875d8":"from sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import matthews_corrcoef\n\nK = [3, 5, 10]","7598f953":"param_dict = {\n    'learning_rate': [0.0075, 0.01, 0.0125],\n    'min_data_in_leaf': [20, 50],\n    'max_bin': [16, 102, 255],\n    'lambda': [\n        # l1, l2\n        [0.0, 0.0],\n        [0.001, 0.01],\n        [0.01, 0.1],\n        [1.0, 0.01],\n    ],\n    'n_estimators': [100, 125, 150]\n}\nparam_key = list(param_dict.keys())\nparam_item = list(param_dict.values())\nparam_item","b2841475":"param_list = list(itertools.product(*param_item))\nparam_list[:10]","2866f917":"len(param_list)","0083ec4a":"df_model = pd.DataFrame(columns=[*param_key, *[f'model_{i}' for i in range(sum(K))], *[f'model_{i}_mcc' for i in range(sum(K))], 'average_mcc'])\ndf_model","f53b44ee":"skf_list = [StratifiedKFold(n_splits=k, shuffle=True, random_state=SEED) for k in K]\n\nfor param in param_list:\n    ctr = 0\n    model = []\n    mcc_score = []\n    for skf in skf_list:\n        for train_idx, val_idx in skf.split(X, y):\n            X_train, X_val = X.loc[train_idx], X.loc[val_idx]\n            y_train, y_val = y[train_idx], y[val_idx]\n\n            model.append(\n                lgbm.LGBMClassifier(\n                    # fixed\n                    is_unbalance=True,\n                    seed=SEED,\n                    extra_trees=True,\n\n                    min_data_per_group=1,\n                    boosting_type='goss',\n                    num_leaves=63,\n                    feature_fraction=0.9,\n                    # variable\n                    learning_rate=param[0],\n                    min_data_in_leaf=param[1],\n                    max_bin=param[2], \n                    lambda_l1=param[3][0],\n                    lambda_l2=param[3][1],\n                    n_estimators=param[4],\n                )\n            )\n            model[ctr].fit(\n                X_train, y_train,\n                categorical_feature=cat_feature_idx\n            )\n\n            y_val_pred = model[ctr].predict(X_val)\n            mcc_score.append(matthews_corrcoef(y_val, y_val_pred))\n\n            ctr += 1\n    df_model.loc[ df_model.shape[0] ] = [\n        *param,\n        *model,\n        *mcc_score,\n        sum(mcc_score) \/ len(mcc_score)\n    ]","1d79cb45":"df_model = df_model.sort_values(by=['average_mcc', 'learning_rate'], ascending=[False, True]).reset_index(drop=True)\ndf_model.loc[:1000].to_pickle('model.pkl')\n!ls -lah","f71c0cfa":"from sklearn.metrics import classification_report, f1_score, confusion_matrix, precision_score, matthews_corrcoef\n\ndef predict(X, mode='best_mean'):\n    if mode == 'best_mode':\n        y_preds = []\n        for i in range(sum(K)):\n            y_preds.append(df_model.loc[0, f'model_{i}'].predict(X))\n        y_preds = np.array(y_preds)\n        y_preds = scipy.stats.mode(y_preds)\n        y_preds = y_preds[0]\n        y_preds = y_preds.reshape(-1)\n    elif mode == 'best_mean':\n        y_preds = []\n        for i in range(sum(K)):\n            y_preds.append(df_model.loc[0, f'model_{i}'].predict_proba(X))\n        y_preds = np.mean(np.array(y_preds), axis=0)\n        y_preds = np.argmax(y_preds, axis=-1)\n    elif mode == 'ensemble_mode':\n        y_preds = []\n        for i in df_model.index:\n            for j in range(sum(K)):\n                y_preds.append(df_model.loc[i, f'model_{j}'].predict(X))\n        y_preds = np.array(y_preds)\n        y_preds = scipy.stats.mode(y_preds)\n        y_preds = y_preds[0]\n        y_preds = y_preds.reshape(-1)\n    elif mode == 'ensemble_mean':\n        y_preds = []\n        for i in df_model.index:\n            for j in range(sum(K)):\n                y_preds.append(df_model.loc[i, f'model_{j}'].predict_proba(X))\n        y_preds = np.mean(np.array(y_preds), axis=0)\n        y_preds = np.argmax(y_preds, axis=-1)\n    elif mode == 'weighted_ensemble_mean':\n        y_preds = []\n#         model_weight = df_model['average_mcc'].apply(lambda a: a\/df_model['average_mcc'].sum())\n        model_weight = []\n        for i in df_model.index:\n            model_weight.append(1 + np.log10(df_model.shape[0] - i + 1))\n        print(model_weight[:10])\n        for i in df_model.index:\n            for j in range(sum(K)):\n                y_preds.append(\n                    df_model.loc[i, f'model_{j}'].predict_proba(X) *\n                    model_weight[i]\n                )\n        y_preds = np.array(y_preds)\n        y_preds = np.mean(y_preds, axis=0)\n        y_preds = np.argmax(y_preds, axis=-1)\n    else:\n        raise ValueError(\"Mode isn't supported\")\n    \n    return y_preds\n\ndef metrics(y_true, y_pred):\n    print('Weighted F1 Score :', f1_score(y_true, y_pred, average='weighted'))\n    print('MCC Score :', matthews_corrcoef(y_true, y_pred))\n    cm = confusion_matrix(y_true, y_pred)\n    cm = pd.DataFrame(cm, [0, 1], [0, 1])\n\n    sns.heatmap(cm, annot=True, cmap=\"YlGnBu\", fmt=\"d\")\n    plt.show()","2d1a0282":"y_train_pred = predict(X_train, mode='best_mode')\nmetrics(y_train, y_train_pred)","c9214688":"y_train_pred2 = predict(X_train, mode='best_mean')\nmetrics(y_train, y_train_pred2)","69eb54c1":"y_train_pred3 = predict(X_train, mode='ensemble_mode')\nmetrics(y_train, y_train_pred3)","15d606bb":"y_train_pred4 = predict(X_train, mode='ensemble_mean')\nmetrics(y_train, y_train_pred4)","74e19762":"y_train_pred5 = predict(X_train, mode='weighted_ensemble_mean')\nmetrics(y_train, y_train_pred5)","cac5f750":"pred_modes = ['best_mode','best_mean','ensemble_mode','ensemble_mean','weighted_ensemble_mean']\n\nfor mdx in pred_modes:\n    y_test_pred = predict(X_test, mode=mdx)\n    df_submission = pd.concat([pd.Series(list(range(0, len(X_test))), name='row_id', dtype=np.int32), pd.Series(y_test_pred, name='open_flag')], axis=1)\n    df_submission.to_csv('submission_'+mdx+'.csv', index=False)","347ca8f0":"lgbm.plot_importance(df_model.loc[0, 'model_0'], ignore_zero=False, figsize=(16,9))","c3abb20c":"lgbm.plot_split_value_histogram(df_model.loc[0, 'model_0'], 2)","53f18d46":"### 5. datetime","ed2edffc":"### 6. Anomaly","98f689c6":"### 3. User ID","57107729":"### 2. last{  }day","76f8645f":"### 1. best mode","e3196203":"## Submit","aaddbcf3":"### 7. NaN values","44ab04da":"### 2. null values","2891ba97":"## Visualize","ec890c83":"### Credits to [Sandy Khosasi](https:\/\/www.kaggle.com\/ilosvigil)","96a296bd":"### 3. ensemble mode","6c4ab864":"### 5. weighted ensemble mean","ab070121":"### 4. ensemble mean","08faf576":"### 2. best mean","26a25a61":"## Evaluate","0407fc95":"### 1. Domain","188b07e2":"## Preparing training","80f19576":"### 4. correlation","adaf3467":"### 1. dtypes","9510bc08":"# Version History\n## each version has its own unique way to build prediction model \n\n#5, #6: Preprocess dataset -> Save\/Load (optional) -> Light GBM Model (**VERY SLOW** *_~4h_*)\n#4: the visualizations and thorough analysis supporting version #3 (**VERY FAST** *_<10m_*)\n#3: using H2O.AI -> LGBM model (**SLOW** *_~2h_*)\n#2, #1: conventional technique with Sklearn Gradient Boosting Classifier (**VERY FAST** *_<1h_*)","be3f3e31":"## Saving preprocessed dataset (Optional)","c81a4976":"## Model","04cbaa56":"### 3. unique values","536abe34":"# Data Preparation","418638a9":"## Skim thru datasets","2c00631b":"## Importing Libraries","1c2292b0":"### 4. drop unused","c0f7e99f":"## Preprocessing","b13b5239":"## Inspect changes"}}