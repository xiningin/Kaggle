{"cell_type":{"54cdc6e5":"code","4f13cb45":"code","4e0d7f3c":"code","a7652afc":"code","bf779751":"code","7b65b81a":"code","1c4bff1a":"code","484198db":"code","4d7f56db":"code","306c9681":"code","f889886f":"code","ef921f8f":"code","1c10ed2e":"markdown"},"source":{"54cdc6e5":"!pip install split-folders","4f13cb45":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport cv2\nimport matplotlib.pyplot as plt\nimport splitfolders\nimport os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n'''import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))'''\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4e0d7f3c":"# Import relavent libraries\nimport tensorflow as tf\nfrom tensorflow.keras.applications.mobilenet_v2 import MobileNetV2\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Flatten, Dense\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","a7652afc":"# Extract train\/test\/validation splits using the facemask dataset\n# Split original 10,000 image dataset into a training and testing dataset\n\ntrain_dir = '..\/output\/split_data\/train'\ntest_dir = '..\/output\/split_data\/test'\nval_dir = '..\/output\/split_data\/val'\n\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255, horizontal_flip=True, zoom_range=0.2,shear_range=0.2)\ntrain_generator = train_datagen.flow_from_directory(directory=train_dir,target_size=(128,128),class_mode='categorical',batch_size=32)\n\nval_datagen = ImageDataGenerator(rescale=1.0\/255)\nval_generator = train_datagen.flow_from_directory(directory=val_dir,target_size=(128,128),class_mode='categorical',batch_size=32)\n\ntest_datagen = ImageDataGenerator(rescale=1.0\/255)\ntest_generator = train_datagen.flow_from_directory(directory=test_dir,target_size=(128,128),class_mode='categorical',batch_size=32)","bf779751":"# Adding layers to the MobileNet model. Sigmoid\n# Chosen for output layer as it performed better than softmax\nmobilenet = MobileNetV2(weights='imagenet',include_top=False,input_shape=(128,128,3))\n\nfor layer in mobilenet.layers:\n    layer.trainable = True\n    \nmodel = tf.keras.Sequential([\n    mobilenet,\n    Flatten(),\n    Dense(2,activation='softmax')\n])\n\n# Print the model layer and parameter summary\nmodel.summary()","7b65b81a":"# Compile and set up the model. Binary cross entropy chosen,\n# adam optimizer is known as best optimizer. Then train the model using fit_generator\nmodel.compile(optimizer=\"adam\",loss=\"binary_crossentropy\",metrics=\"accuracy\")\nhistory = model.fit_generator(generator=train_generator,steps_per_epoch=len(train_generator)\/\/32,epochs=2,validation_data=val_generator,validation_steps=len(val_generator)\/\/32)","1c4bff1a":"# Evaluate on the training dataset\nmodel.evaluate_generator(test_generator)","484198db":"# Test model on a known masked image\nsample_mask_img = cv2.imread('..\/input\/face-mask-detection\/images\/maksssksksss1.png')\nsample_mask_img = cv2.resize(sample_mask_img,(128,128))\nplt.imshow(sample_mask_img)\nsample_mask_img = np.reshape(sample_mask_img,[1,128,128,3])\nsample_mask_img = sample_mask_img\/255.0","4d7f56db":"model.predict(sample_mask_img)","306c9681":"#loading haarcascade_frontalface_default.xml\nface_model = cv2.CascadeClassifier('..\/input\/haar-cascades-for-face-detection\/haarcascade_frontalface_default.xml')\n\nimg = cv2.imread('..\/input\/face-mask-detection\/images\/maksssksksss100.png')\n\nimg = cv2.cvtColor(img, cv2.IMREAD_GRAYSCALE)\n\n#faces = face_model.detectMultiScale(img,) #returns a list of (x,y,w,h) tuples\nfaces = face_model.detectMultiScale(\n    img,\n    scaleFactor=1.1,\n    minNeighbors=3,\n    minSize=(30, 30)\n)\n\n\nout_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #colored output image\n\n#plotting\nfor (x,y,w,h) in faces:\n    cv2.rectangle(out_img,(x,y),(x+w,y+h),(0,0,255),2)\nplt.figure(figsize=(10,10))\nplt.imshow(out_img)","f889886f":"mask_label = {0:'MASK',1:'NO MASK'}\ndist_label = {0:(0,255,0),1:(255,0,0)}\n\n\nlabel = [0 for i in range(len(faces))]\nnew_img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR) #colored output image\nfor i in range(len(faces)):\n    (x,y,w,h) = faces[i]\n    crop = new_img[y:y+h,x:x+w]\n    crop = cv2.resize(crop,(128,128))\n    crop = np.reshape(crop,[1,128,128,3])\/255.0\n    mask_result = model.predict(crop)\n    cv2.putText(new_img,mask_label[mask_result.argmax()],(x, y-10),cv2.FONT_HERSHEY_SIMPLEX,0.5,dist_label[label[i]],2)\n    cv2.rectangle(new_img,(x,y),(x+w,y+h),dist_label[label[i]],2)\nplt.figure(figsize=(10,10))\nplt.imshow(new_img)","ef921f8f":"model.save_weights('ewan_model_weights.model')","1c10ed2e":"## Start building model using google ai Mobile Net V2"}}