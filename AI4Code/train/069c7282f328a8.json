{"cell_type":{"b851b83b":"code","b814fa3f":"code","d75bf785":"code","fc7bbe57":"code","c0760019":"code","6c955bb5":"code","3a37c085":"code","486afab4":"code","1c144aeb":"code","2b273e7a":"code","8a23af79":"code","56507887":"code","ae818bc3":"code","9ddb11eb":"code","e0a6524b":"code","d0c3c418":"code","754aaaa0":"code","25dc5028":"code","3a5ba59e":"code","70f55c0b":"code","25a4cf2e":"code","c2371afd":"code","61a70a19":"code","39c12abb":"code","e7dfa659":"code","a8091afa":"code","dc6947f0":"code","462a3d4e":"code","df21bad1":"code","ff877911":"code","fcdd3bc6":"code","07066f96":"code","d1e7b49b":"code","667c7f21":"code","29388d9b":"code","b0a36442":"markdown","892feefc":"markdown","828fb412":"markdown","2c50b6d5":"markdown","6c434d4e":"markdown","ba3790e5":"markdown","58de2a62":"markdown","1b371ed4":"markdown","92c4d6c0":"markdown","359f0876":"markdown","07826609":"markdown","68fee98f":"markdown","fb4074f4":"markdown","3bbbeeba":"markdown","4f91eeb9":"markdown","820b5230":"markdown","e8b0ad4b":"markdown","57cf7299":"markdown","3ec4cc32":"markdown","5164041d":"markdown","9b023dae":"markdown","bc2da927":"markdown","09815348":"markdown","7e600d97":"markdown","22ab5553":"markdown","910104e4":"markdown","63d30e2a":"markdown"},"source":{"b851b83b":"#Import important libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nimport math\nwarnings.filterwarnings('ignore')\n%matplotlib inline","b814fa3f":"#read the csv train file\ntrain = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntrain.head()","d75bf785":"#get a quick statistical description of the target value\ntrain['SalePrice'].describe()","fc7bbe57":"k=1\nsns.set_theme()\nsns.set_style('darkgrid')\n#check the distribution\nsns.distplot(train['SalePrice'], color = 'purple')\nplt.xticks(rotation=90);\nplt.title('Figure {}'.format(k), y=-0.5, fontsize = 16)\n\nk=k+1","c0760019":"fig, axes = plt.subplots(1, 2, figsize=(17, 5))\nfig.suptitle('Figure {}'.format(k), y=-0.1, fontsize = 16)\nsns.scatterplot(ax=axes[0], x='GrLivArea', y=\"SalePrice\", color = 'purple',data=train, alpha =0.5)\nsns.boxplot(ax=axes[1], x='OverallQual', y=\"SalePrice\",hue='OverallQual', data=train, color = 'purple',dodge=False)\n\naxes[1].legend('')\n","6c955bb5":"#correlation matrix\ncorrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nheatm = sns.heatmap(corrmat, vmax=0.9, square=True, cmap = 'mako_r');\nheatm.text(15, 50, 'Figure {}'.format(k),fontsize=16)\nk=k+1\n","3a37c085":"#saleprice correlation matrix\n\nn = 10 #number of parameters for heatmap\ncols = corrmat.nlargest(n, 'SalePrice')['SalePrice'].index\ncols2 = np.array(list(reversed(cols)))\ncm = train[cols2].corr()\n# mask to get only the upper triangle of the matrix, as the lower one is just repeated information\nmask = np.triu(np.ones_like(cm, dtype=np.bool))\n# adjust mask and df\n\ncm2 = cm.iloc[1:,:-1].copy()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.set(font_scale=1.3)\nheatmap = sns.heatmap(cm,mask=mask, cbar=False, annot=True, square=True, cmap = 'mako_r', fmt='.2f', vmin=.1, vmax=.85,linewidth=0.3, annot_kws={'size': 10}, yticklabels=cols2, xticklabels=cols2)\n#heatmap.text(4, 13, 'Figure {}'.format(k),fontsize=16)\nk=k+1\n\n\n","486afab4":"\nf, ax = plt.subplots(figsize=(24, 8))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", hue='YearBuilt', data=train, color = 'purple',dodge=False)\nfig.legend('')\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);\nplt.title('Figure {}'.format(k), y=-0.25, fontsize = 16)\nk=k+1","1c144aeb":"#seaborn pairplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\npairp = sns.pairplot(train[cols], size = 1.5, )\nplt.show();","2b273e7a":"#measuring missing data\ntotal = train.isnull().sum().sort_values(ascending=False)\npercentage = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percentage], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)\n","8a23af79":"test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","56507887":"for dataset in (train, test):\n    #NA means 'none' for these categorical values\n    for parameter in (\"PoolQC\",\"MiscFeature\",\"Alley\",\"Fence\",\"FireplaceQu\",\n                      'GarageType', 'GarageFinish', 'GarageQual', 'GarageCond',\n                     'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n                      'BsmtFinType2',\"MasVnrType\",'MSSubClass'):\n        dataset[parameter] = dataset[parameter].fillna(\"None\")\n       \n    #Basement and garage numerical missing values means non-existent, so \n    #can be filled with zeroes\n    for parameter in ('GarageYrBlt', 'GarageArea', 'GarageCars','BsmtFinSF1',\n                      'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', \n                      'BsmtHalfBath', \"MasVnrArea\"):\n        dataset[parameter] = dataset[parameter].fillna(0)\n    \n    #categorical with very few missing values can be filled with the most frequent value:\n    for parameter in ('MSZoning','Electrical','KitchenQual','Exterior1st','Exterior2nd',\n                     'SaleType','Utilities'):\n        dataset[parameter] = dataset[parameter].fillna(dataset[parameter].mode()[0])\n    \n    #data description says NA means typical\n    dataset['Functional'] = dataset[\"Functional\"].fillna(\"Typ\")\n    \n    #Fill area of street with the median of the area of the street of the houses\n    #in the same neighborhood\n    dataset[\"LotFrontage\"] =dataset.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","ae818bc3":"#check for any missing values\ntrain.isnull().sum().max()","9ddb11eb":"test.isnull().sum().max()","e0a6524b":"train.drop(train[train.GrLivArea > 4500].index, inplace = True)\ntrain.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0,800000), c ='purple',alpha = 0.5);\nplt.title('Figure {}'.format(k), y=-0.35, fontsize = 16)\nk=k+1","d0c3c418":"for dataset in (train, test):\n    dataset['TotalSF'] = dataset['TotalBsmtSF'] + dataset['1stFlrSF'] + dataset['2ndFlrSF']","754aaaa0":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","25dc5028":"sns.distplot(train['SalePrice'], color= 'purple',fit=norm);\nplt.xticks(rotation=90);\nplt.title('Figure {}'.format(k), y=-0.5, fontsize = 16)\nk=k+1","3a5ba59e":"#applying log transformation\ntrain['SalePrice'] = np.log(train['SalePrice'])","70f55c0b":"sns.distplot(train['SalePrice'], color= 'purple', fit=norm);\nplt.title('Figure {}'.format(k), y=-0.5, fontsize = 16)\nprint(\"Skewness: %f\" % train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train['SalePrice'].kurt())","25a4cf2e":"    #Get numerical columns\n    num_data = train.dtypes[train.dtypes != \"object\"].index\n    # Check the skew of all numerical features\n    skewed_data = train[num_data].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewness = pd.DataFrame({'Skew' :skewed_data})\n    skewness.head(10)\n    posSkew = skewness[skewness['Skew'] > 0.75]\n    print(\"There are {} skewed numerical features to log-transform\".format(posSkew.shape[0]))\n    posSkew","c2371afd":"#Make a list of skewed features\nskewed_feat= list(posSkew.index)","61a70a19":"#Apply logarithm transformation\nfor dataset in (train, test):\n    \n    for parameter in skewed_feat:\n        dataset[parameter] = np.log1p(dataset[parameter])","39c12abb":"#Assure the same order of columns in the test and train datasets\n\ntest = test[train.drop('SalePrice', axis = 1).columns]","e7dfa659":"from sklearn.preprocessing import OneHotEncoder\nohe = OneHotEncoder()\n#Get the categorical columns\ntrain_object = train.select_dtypes('object')\ntest_object = test.select_dtypes('object')\nstring_column_names = list(train_object.columns)\n#Fit the OneHotEncoder with the train data\nohe.fit(train_object)\n#Transform categorical data and store them in separate arrays\ntrain_encoded = ohe.transform(train_object).toarray()\ntest_encoded = ohe.transform(test_object).toarray()\nfeature_names = ohe.get_feature_names(string_column_names)\n#Get numerical data from the dataset\ntrain_num = train.select_dtypes(exclude='object')\ntrain_num = train_num.reset_index(drop=True)\ntest_num = test.select_dtypes(exclude='object')\ntest_num = test_num.reset_index(drop=True)\n#Create Dataframe with encoded data\ntrain_converted =pd.DataFrame(train_encoded,columns=feature_names).astype(int)\ntrain_converted = train_converted.reset_index(drop=True)\ntest_converted =pd.DataFrame(test_encoded,columns=feature_names).astype(int)\ntest_converted = test_converted.reset_index(drop=True)\n#Merge original numerical data with new encoded data, for each dataset\ntrain = pd.concat([train_num, train_converted], axis=1)\ntest = pd.concat([test_num, test_converted], axis=1)","a8091afa":"#Prepare train and test data for modelling\ny = train[\"SalePrice\"]\nX = train.drop(columns=['SalePrice','Id'])","dc6947f0":"#split data to test the model\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)","462a3d4e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","df21bad1":"from sklearn.linear_model import Lasso\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nlas = Lasso()\ngrid_values = {'alpha': [0.0001,0.0002,0.0004,0.0006,0.0008,0.001,0.01]}\ngrid_las_mse = GridSearchCV(las, param_grid = grid_values, scoring = 'neg_mean_squared_error',n_jobs = -1)\ngrid_las_mse.fit(X_train_scaled, y_train)\n\nprint('Grid best parameter (min. mse): ', grid_las_mse.best_params_)","ff877911":"#now we employ the best alpha parameter found and check the results\nlas = Lasso( alpha = 0.0004, random_state = 0).fit(X_train_scaled,y_train)\nprint('Accuracy on train data: {:.4f}'.format(las.score(X_train_scaled,y_train)))\nprint('Accuracy on test data: {:.4f}'.format(las.score(X_test_scaled,y_test)))\nprint('RMSE on test data: {:.4f}'.format(math.sqrt((mean_squared_error(y_test, las.predict(X_test_scaled))))))","fcdd3bc6":"prediction = pd.Series(las.predict(X_test_scaled))\ny_test = y_test.reset_index().drop('index', axis=1)\nytest = pd.Series(y_test['SalePrice'])\npred =pd.DataFrame({ 'Predicted price':np.exp(prediction),'Real price': np.exp(ytest)} ).reset_index()\nsns.set_style(\"white\")\n#sns.scatterplot(data=pred, x=\"predi\", y=\"test\")\nfig, axes = plt.subplots(1, 1, figsize=(6,6))\nsns.scatterplot(x='Predicted price', y='Real price', data=pred, color='purple', alpha =0.7)\nplt.plot([0, 600000], [0,600000], linewidth=2, c ='darkblue', alpha =0.8)\naxes.set(ylim=(0, 500000),xlim=(0, 500000))\n\nplt.title('Figure {}'.format(k), y=-0.3, fontsize = 16)\nplt.rcParams[\"axes.labelsize\"] = 20","07066f96":"scaler = MinMaxScaler()\nX_scaled = scaler.fit_transform(X)\nlas = Lasso( alpha = 0.0004, random_state = 0).fit(X_scaled,y)","d1e7b49b":"#We Separate the ID column and scale the test data for predicting\nId = test['Id']\ntest = test.drop('Id',axis = 1)\ntest_scaled = scaler.transform(test)","667c7f21":"#Preparing the submission file\nresultslog = las.predict(test_scaled)\nresults = np.exp(resultslog)\noutput = pd.DataFrame({'Id': Id, 'SalePrice': results})\noutput","29388d9b":"output.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was saved!\")","b0a36442":"### Missing data","892feefc":"# Feature Engineering","828fb412":"# Exploratory Data Analysis (EDA)\n\nLet's start by getting a sense of the dataset available, looking at the shape of the target value (SalePrice), and which numerical parameters have the the strongest correlation with it.","2c50b6d5":"Now let's take care of the two outliers identified at the beginning","6c434d4e":"Surprisingly, \"YearBuilt\" bearly makes the cut, so I plotted this feature to visually confirm the trend.\n\nIndeed, the price increases for newer houses, although it does not have an impact as noticeable as I would have imagined:","ba3790e5":"Now, we can create a plot showing the trends between these variables. However, it is not necessary to plot them all, as there are three pairs highly (and logically) correlated:\n\n-GrLivArea and TotRmsAbvGrd\n\n-garageCars and GarageArea\n\n-TotalBsmtSF and 1stFlrSF\n\nSo we only take one of each pair.","58de2a62":"# Simple, efficient and comprehensive housing prices analysis\n\nOn this notebook, a simple and efficient model for predicting housing prices is done, going through the following steps:\n* Exploratory data analysis: Looking at target value (SalePrice) and more important features.\n* Feature Engineering: treating missing data, outliers, generating new features, encoding data.\n* Simple and efficient Model to predict prices: Reaching high accuracy, low RMSE, avoiding overfitting, and keeping it simple (only one parameter tuning).\n\nI will justify each of the important steps I make, so the notebook works as a comprehensive guideline. ","1b371ed4":"#### Seeking Alpha\nWe'll use the Alpha parameter to control how much regularization we want on our model. To do so, we'll use GridSearchCV to find the best Alpha value in terms of RMSE.","92c4d6c0":"#### Scaling\nFor linear regressions with regularization, such as the Lasso regression model, it is important to scale the data as these models are pretty sensitive to features weights. ","359f0876":"Now, let's add a new feature which I think could help summarize an important metric: the total area of the house.","07826609":"A close to 94% accuracy is pretty good! We can also see that a similar high accuracy value both on the train and the test data, so we should be safe from having a overfitting model. In terms of kaggle competition, we also have a low RMSE value. Now let's fit the model to the entire train dataset.","68fee98f":"Guided by the documentation, we can fill the missing values.\n\n**important: We shouldn't drop the categorical features with missing values, since its relation with the target value is not measured by the pearson correlation (this only measure relation between numeric features). Hence, we shouldn't assume they are not important.**","fb4074f4":"So there it is, a simple model with only one parameter tuning. Hope you got something useful out of this notebook, I would gladly hear any feedback!","3bbbeeba":"We'll try keep it simple and efficient.\n\nIt is possible to produce a greatly developed model that iterates 10.000 times and is finely adjusted to (nearly) perfectly predict the kaggle test data, and set you on the top 1% of the leaderboard. There are various good notebooks available that employ does models, although some of them don't explain their reasoning on choosing this overcomplicated models.\n\nHowever, I think it's more valuable to have a simpler model that is easy to understand and explain, and still produce some great predictions. Furthermore, you can get fairly good results without too much CPU requirements.\n\nFor this particular dataset I chose the Lasso linear regression, as L1 penalization bodes well when there are few medium\/large effect features, which I think is the case on this dataset.","4f91eeb9":"# Modelling","820b5230":"So there it is! we have quickly grasp a basic idea of the target value and how it is related to the most important numerical parameters. This exploratory data analysis is important and could be the main objective on its own.\n\nNevertheless, we are going to continue and dive deeper into the dataset: have a look a the missing data and how to deal with it, take into account the categorical variables that were not considered in the correlation process, and train a model to predict the final SalePrice of a house.\n\nLet's go!","e8b0ad4b":"It can be seen (last row) that every parameter has a positive (or slightly negative at most) correlation with SalePrice. Apart from this, the matrix is too overcrowded to easily grab useful data. Thus, let's make a correlation matrix with only the parameters that have the largest correlation with the target value","57cf7299":"Remember from Figure 1, that the target value presented positive skewness and high peakness (kurtosis)? This is a problem since it means the parameter doesn't follow a normal distribution, and much of the calculations we will make assume we are working with a normal distribution. Luckily, we can solve this problem by applying a logarithm transformation to the values, is positive\/right skewed.\n\nMore info at: https:\/\/medium.com\/@TheDataGyan\/day-8-data-transformation-skewness-normalization-and-much-more-4c144d370e55\n","3ec4cc32":"As expected, the price increases as the size and quality of the house gets higher. \n\nOn the other hand, the left plot shows that there are a couple of outliers that should be taken care of...more on that later.","5164041d":"#### Loading the test set\n\nwe can now load the test file to apply the same transformation on both dataset\n\n**Important**: note that we are never merging nor using the test data to model the evaluation method, as that would lead to data leakage. We are simply applying the same transformation to each dataset with a ***for*** loop, to avoid coding twice.","9b023dae":"### Correlation Matrix\n\nWe could go on to plot each possible parameter against the SalePrice. However, it is more productive to check which have the largest influence and then have a look of the most important, all at once. Then, a detailed analysis can be done on whichever parameter we are interested in.","bc2da927":"It can be seen that SalePrice has a positive skewness and high peakness, so it doesn't follow a normal distribution. We will attend this matter later.","09815348":"## Encoding\n\nFor machine learning, I always recommend OneHotEncoder (OHE) over get_dummies. We could be fine using get_dummies only if we are doing a quick exploratory data analysis. For deeper analysis, OHE does the same things as get dummies but in addition, OHE saves the exploded categories into its object. Saving exploded categories is extremely useful when you want to apply the same data pre-processing on future test sets, which may have new or missing values when compared to the train data.\n\nFor the kaggle test set there might not be any problem using get_dummies if there are no new values in this particular test set. However, in real world it's ideal to prepare for dataset with poorer quality, and OHE makes the code more robust.\n\nMore info at: https:\/\/albertum.medium.com\/preprocessing-onehotencoder-vs-pandas-get-dummies-3de1f3d77dcc","7e600d97":"## Important attributes\nLooking at the dataset, there are various attributes that I might expect to have a notable impact on the sale price: Above ground living area square feet (GrLivArea), the overall quality (OverallQual), the year it was built (YearBuilt), and so on... \n\nLet's plot some of these attributes against the target value (SalePrice) and have a quick glampse at the trends. \n\nNotice that by previously using the \"describe\" function, we can see the max value and\nset a proper \"ylim\" value to ensure a proper visibility of every test point","22ab5553":"Much better now! It will help to make the same process for every other skewed feature:","910104e4":"### Log Transformations","63d30e2a":"We can confirm it by comparing it with a normal distribution:"}}