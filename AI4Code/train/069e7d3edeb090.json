{"cell_type":{"e3190e5f":"code","91296b7d":"code","e1c7358b":"code","fc19ad2c":"code","910549bc":"code","b64552a1":"code","223e1fdd":"code","157f9dcc":"code","76acb4b6":"code","620ecdd1":"code","8cfbdcf2":"code","d5b0424d":"code","65087a58":"code","f77b7cd0":"code","090aa0be":"markdown","8b592956":"markdown","63977bde":"markdown","ad379549":"markdown","0a712e73":"markdown","30d2d91f":"markdown","58bb578d":"markdown","4698add8":"markdown","9aed117f":"markdown","1c2626d3":"markdown","6518fa12":"markdown","6be3d391":"markdown","c380ddc3":"markdown","0b02c170":"markdown"},"source":{"e3190e5f":"import numpy as np \nimport pandas as pd \n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\nimport tensorflow as tf\nimport keras\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras import models\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.callbacks import LearningRateScheduler,EarlyStopping\n# Reproductibility\nfrom numpy.random import seed\nseed(84)\nimport gc\n# tf.random.set_seed(84)\nimport os\nos.listdir('..\/input\/titanic')","91296b7d":"####################################\n# Importing data and merging\n####################################\n\n# Reading dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# Adding a column in each dataset before merging\ntrain['Type'] = 'train'\ntest['Type'] = 'test'\n\n# Merging train and test\ndata = train.append(test)\n\n####################################\n# Missing values and new features\n####################################\n\n# Title\ndata['Title'] = data['Name']\n\n# Cleaning name and extracting Title\nfor name_string in data['Name']:\n    data['Title'] = data['Name'].str.extract('([A-Za-z]+)\\.', expand=True)\n    \n# Replacing rare titles \nmapping = {'Mlle': 'Miss', 'Ms': 'Miss', 'Mme': 'Mrs', 'Major': 'Other', \n           'Col': 'Other', 'Dr' : 'Other', 'Rev' : 'Other', 'Capt': 'Other', \n           'Jonkheer': 'Royal', 'Sir': 'Royal', 'Lady': 'Royal', \n           'Don': 'Royal', 'Countess': 'Royal', 'Dona': 'Royal'}\n           \ndata.replace({'Title': mapping}, inplace=True)\ntitles = ['Miss', 'Mr', 'Mrs', 'Royal', 'Other', 'Master']\n\n# Replacing missing age by median\/title \nfor title in titles:\n    age_to_impute = data.groupby('Title')['Age'].median()[titles.index(title)]\n    data.loc[(data['Age'].isnull()) & (data['Title'] == title), 'Age'] = age_to_impute\n    \n# New feature : Family_size\ndata['Family_Size'] = data['Parch'] + data['SibSp'] + 1\ndata.loc[:,'FsizeD'] = 'Alone'\ndata.loc[(data['Family_Size'] > 1),'FsizeD'] = 'Small'\ndata.loc[(data['Family_Size'] > 4),'FsizeD'] = 'Big'\n\n# Replacing missing Fare by median\/Pclass \nfa = data[data[\"Pclass\"] == 3]\ndata['Fare'].fillna(fa['Fare'].median(), inplace = True)\n\n#  New feature : Child\ndata.loc[:,'Child'] = 1\ndata.loc[(data['Age'] >= 18),'Child'] =0\n\n# New feature : Family Survival (https:\/\/www.kaggle.com\/konstantinmasich\/titanic-0-82-0-83)\ndata['Last_Name'] = data['Name'].apply(lambda x: str.split(x, \",\")[0])\nDEFAULT_SURVIVAL_VALUE = 0.5\n\ndata['Family_Survival'] = DEFAULT_SURVIVAL_VALUE\nfor grp, grp_df in data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n                               \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin == 0.0):\n                data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n                \nfor _, grp_df in data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin == 0.0):\n                    data.loc[data['PassengerId'] == passID, 'Family_Survival'] = 0\n                    \n####################################\n# Encoding and pre-modeling\n####################################                  \n\n# dropping useless features\ndata = data.drop(columns = ['Age','Cabin','Embarked','Name','Last_Name',\n                            'Parch', 'SibSp','Ticket', 'Family_Size'])\n\n# Encoding features\ntarget_col = [\"Survived\"]\nid_dataset = [\"Type\"]\ncat_cols   = data.nunique()[data.nunique() < 12].keys().tolist()\ncat_cols   = [x for x in cat_cols ]\n# numerical columns\nnum_cols   = [x for x in data.columns if x not in cat_cols + target_col + id_dataset]\n# Binary columns with 2 values\nbin_cols   = data.nunique()[data.nunique() == 2].keys().tolist()\n# Columns more than 2 values\nmulti_cols = [i for i in cat_cols if i not in bin_cols]\n# Label encoding Binary columns\nle = LabelEncoder()\nfor i in bin_cols :\n    data[i] = le.fit_transform(data[i])\n# Duplicating columns for multi value columns\ndata = pd.get_dummies(data = data,columns = multi_cols )\n# Scaling Numerical columns\nstd = StandardScaler()\nscaled = std.fit_transform(data[num_cols])\nscaled = pd.DataFrame(scaled,columns = num_cols)\n# dropping original values merging scaled values for numerical columns\ndf_data_og = data.copy()\ndata = data.drop(columns = num_cols,axis = 1)\ndata = data.merge(scaled,left_index = True,right_index = True,how = \"left\")\ndata = data.drop(columns = ['PassengerId'],axis = 1)\n\n# Target = 1st column\ncols = data.columns.tolist()\ncols.insert(0, cols.pop(cols.index('Survived')))\ndata = data.reindex(columns= cols)\n\n# Cutting train and test\ntrain = data[data['Type'] == 1].drop(columns = ['Type'])\ntest = data[data['Type'] == 0].drop(columns = ['Type'])\n\n# X and Y\nX_train = train.iloc[:, 1:20].as_matrix()\ny_train = train.iloc[:,0].as_matrix()","e1c7358b":"train.head()","fc19ad2c":"def make_model():\n    model = models.Sequential()\n#     model.add(Dense(16, activation='relu',input_shape=(X_train.shape[1],)))\n#     model.add(Dropout(0.3))\n#     model.add(BatchNormalization())\n#     model.add(Dense(8,activation='relu'))\n#     model.add(Dropout(0.2))\n#     model.add(BatchNormalization())\n#     model.add(Dense(4,activation='relu'))\n#     model.add(Dropout(0.2))\n#     model.add(BatchNormalization())\n#     model.add(Dense(1,activation='sigmoid'))\n    model.add(Dense(13, input_dim = 18, activation = 'relu'))\n    model.add(Dropout(0.2))\n    model.add(BatchNormalization())\n    model.add(Dense(8, activation = 'relu'))\n    model.add(BatchNormalization())\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(loss='binary_crossentropy', optimizer='adam',metrics=['accuracy'])\n    return model\n","910549bc":"def mixup_data(x, y, alpha=1.0):\n    if alpha > 0:\n        lam = np.random.beta(alpha, alpha)\n    else:\n        lam = 1\n\n    sample_size = x.shape[0]\n    index_array = np.arange(sample_size)\n    np.random.shuffle(index_array)\n    \n    mixed_x = lam * x + (1 - lam) * x[index_array]\n    mixed_y = (lam * y) + ((1 - lam) * y[index_array])\n\n    return mixed_x, mixed_y\n\ndef make_batches(size, batch_size):\n    nb_batch = int(np.ceil(size\/float(batch_size)))\n    return [(i*batch_size, min(size, (i+1)*batch_size)) for i in range(0, nb_batch)]\n\n\ndef batch_generator(X,y,batch_size=128,shuffle=True,mixup=False):\n    sample_size = X.shape[0]\n    index_array = np.arange(sample_size)\n    \n    while 1:\n        if shuffle:\n            np.random.shuffle(index_array)\n        batches = make_batches(sample_size, batch_size)\n        for batch_index, (batch_start, batch_end) in enumerate(batches):\n            batch_ids = index_array[batch_start:batch_end]\n            X_batch = X[batch_ids]\n            y_batch = y[batch_ids]\n            \n            if mixup:\n                X_batch,y_batch = mixup_data(X_batch,y_batch,alpha=1.0)\n                \n            yield X_batch,y_batch","b64552a1":"def step_decay(epoch):\n   initial_lrate = 0.01\n   drop = 0.5\n   epochs_drop = 10.0\n#    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)\/epochs_drop))\n   lrate = initial_lrate * np.power(drop, np.floor((1+epoch)\/epochs_drop))\n   return lrate\nlrate = LearningRateScheduler(step_decay)","223e1fdd":"class LossHistory(keras.callbacks.Callback):\n    def on_train_begin(self, logs={}):\n       self.losses = []\n       self.lr = []\n \n    def on_epoch_end(self, batch, logs={}):\n       self.losses.append(logs.get('loss'))\n       self.lr.append(step_decay(len(self.losses)))","157f9dcc":"def auc(y_true, y_pred):\n    try:\n        return tf.py_func(metrics.roc_auc_score, (y_true, y_pred), tf.double)\n    except:\n        return 0.5","76acb4b6":"batch_size = 32\nloss_history = LossHistory()\nlrate = LearningRateScheduler(step_decay)\nannealer = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** x)\ncallbacks_list = [EarlyStopping(monitor='val_loss', patience=10,mode='min'),loss_history, annealer]\nsss = StratifiedShuffleSplit(n_splits=5)\nhold_models = []\nhold_models_no_mixup = []\nfor train_index, test_index in sss.split(X_train, y_train):\n    X_train_hold, X_val_hold = X_train[train_index], X_train[test_index]\n    y_train_hold, y_val_hold = y_train[train_index], y_train[test_index]\n    \n    ### train with mixup ###\n    tr_gen = batch_generator(X_train_hold,y_train_hold,batch_size=batch_size,shuffle=True,mixup=True)\n    model = make_model()\n    model.fit_generator(\n            tr_gen, \n            steps_per_epoch=np.ceil(float(len(X_train_hold)) \/ float(batch_size)),\n            nb_epoch=1000, \n            verbose=1, \n            callbacks=callbacks_list, \n            validation_data=(X_val_hold,y_val_hold),\n            max_q_size=10,\n            )\n    hold_models.append(model)\n    \n    ### train without mixup ###\n    tr_gen_no_mixup = batch_generator(X_train_hold,y_train_hold,batch_size=batch_size,shuffle=True,mixup=False)\n    model_no_mixup = make_model()\n    model_no_mixup.fit_generator(\n            tr_gen, \n            steps_per_epoch=np.ceil(float(len(X_train_hold)) \/ float(batch_size)),\n            nb_epoch=1000, \n            verbose=1, \n            callbacks=callbacks_list, \n            validation_data=(X_val_hold,y_val_hold),\n            max_q_size=10,\n            )\n    hold_models_no_mixup.append(model_no_mixup)\n    \n    del X_train_hold, X_val_hold, y_train_hold, y_val_hold\n    gc.collect()","620ecdd1":"import matplotlib.pyplot as plt\n%matplotlib inline\n\ndef append_last_value(target_list, length):\n    if len(target_list) >= length:\n        return target_list\n    for i in range(length - len(target_list)):\n        target_list.append(target_list[-1])\n    return target_list\n\n\nmodel_index = 1\ntrained_epochs = max(len(hold_models[model_index].history.history['loss']), len(hold_models_no_mixup[model_index].history.history['loss']))\n\nloss_list = append_last_value(hold_models[model_index].history.history['loss'], trained_epochs)\nval_acc_list = append_last_value(hold_models[model_index].history.history['val_accuracy'], trained_epochs)\n\nloss_list_no_mixup = append_last_value(hold_models_no_mixup[model_index].history.history['loss'], trained_epochs)\nval_acc_list_no_mixup = append_last_value(hold_models_no_mixup[model_index].history.history['val_accuracy'], trained_epochs)\n\nplt.plot(range(1, trained_epochs+1), loss_list, label=\"loss\")\nplt.plot(range(1, trained_epochs+1), val_acc_list, label=\"val_acc\")\nplt.plot(range(1, trained_epochs+1), loss_list_no_mixup, label=\"loss_no_mixup\")\nplt.plot(range(1, trained_epochs+1), val_acc_list_no_mixup, label=\"val_accuracy_no_mixup\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()","8cfbdcf2":"def get_pred(model_list, test_df):\n    preds = [model.predict(test_df.drop('Survived', axis=1)) for model in model_list]\n    model_count = len(preds)\n    ensemble_pred = 0\n    for i in range(model_count):\n        ensemble_pred = ensemble_pred + preds[i]\n    ensemble_pred = ensemble_pred \/ model_count\n    pred_ = ensemble_pred[:,0]\n    return pred_\n\npred = get_pred(hold_models, test)\npred_no_mixup = get_pred(hold_models_no_mixup, test)","d5b0424d":"# fig = plt.figure()\n# ax = fig.add_subplot(1,1,1)\n\n# tmp_pred = hold_models[model_index].predict(test.drop('Survived', axis=1))\n# tmp_pred_no_mixup = hold_models_no_mixup[model_index].predict(test.drop('Survived', axis=1))\n\n# # ax.hist(tmp_pred, bins=10, density=True, color='red', alpha=0.5)\n# # ax.hist(tmp_pred_no_mixup, bins=10, density=True, color='blue',alpha=0.5)\n# ax.hist(tmp_pred, bins=10, color='red', alpha=0.5)\n# ax.hist(tmp_pred_no_mixup, bins=10, color='blue',alpha=0.5)\n# ax.set_title('histogram of pred score')\n# ax.set_xlabel('pred')\n# ax.set_ylabel('count')\n# ax.set_ylim(0,0.1)\n# fig.show()","65087a58":"train_survived = train['Survived'].sum()\ntrain_len = train.shape[0]\nprint('train dataset : ' + str(train_survived) + '\/' + str(train_len) + ' : ' + str(train_survived \/ train_len))\n\nthresholds = [0.4, 0.45, 0.5, 0.55, 0.6, 0.7, 0.75, 0.8]\nfor threshold in thresholds:\n    pred_survived = pred[pred > threshold].shape[0]\n    pred_len = pred.shape[0]\n    print('threshold : ' + str(threshold) + ' -> ' + str(pred_survived) + '\/' + str(pred_len) + ' : ' + str(pred_survived \/ pred_len))","f77b7cd0":"submit_threshold = 0.4\nsubmit_df = pd.read_csv('..\/input\/titanic\/test.csv')\nsubmit_df['Survived'] = np.where(pred > submit_threshold, 1, 0)\nsubmit_df = submit_df[['PassengerId', 'Survived']]\nsubmit_df.to_csv('submission.csv', index=False)","090aa0be":"Let\u2019s see how to use mixup for table dataset.","8b592956":"## Mixup generator\nThis is based on <a href=\"https:\/\/www.kaggle.com\/qqgeogor\/keras-nn-mixup\">[kernel link]<\/a>","63977bde":"### import","ad379549":"Mixup saves your time for data augmentation, because mixup doesn't have to knowledge of dataset. For example, in image data, you have to check images before implement audumentation. If your your test data has same bright, change image brightness meanless. I know this is rare case, all picture were taken same factory etc... But when you use mixup, you don't have to check data before mixup.","0a712e73":"## Introduction to Mixup\nMixup is a kind of data augmation. This is simple but powerful technique in image classification and sometimes used in table data. Although this competition is not image classification, mixup may help you when you use Neural Net solution.","30d2d91f":"By the way, if you feel you should chose 2 data from different label, I recommend check BC-learning. This is similer idea of data augumentation for image or sound. I never heard this tips fit table data, but I think it worth trying like mixup.  \nresearch link : https:\/\/arxiv.org\/abs\/1711.10284","58bb578d":"## Train","4698add8":"## What is Mixup?\nCreate new data from randomly selected 2 data. All you have to do is sampling 2 data and sum like this. \n$$\nx_{new} = \\lambda x_i + \\left( 1 - \\lambda \\right) x_j \\\\\ny_{new} = \\lambda y_i + \\left( 1 - \\lambda \\right) y_j \\\\\n\\lambda \\in [0,1] , \\lambda \\sim Beta\\left( \\alpha , \\alpha \\right) \\alpha \\in \\left(0, \\infty \\right)\n$$\nx is tensor like data (vector in table data, tensor in image data), y is lavel (one hot)","9aed117f":"![image.png](attachment:image.png)","1c2626d3":"## Simple keras model\nTitanic dataset is small. Simple and shallow model may help you to prevent overfitting.","6518fa12":"Show mixup sample in image dataset helps you to understand what caluculation do for data. Mixup 2 cv2 sample images.  \nIf you want to see mixuped table data, <a href=\"https:\/\/www.inference.vc\/mixup-data-dependent-data-augmentation\/\">this article<\/a> visialize mixup result for 2 dims dataset. And also you can read very interesting infomation about mixup!\n![image.png](attachment:image.png)","6be3d391":"## Compare two models\nLet's see loss and acc of 2 models. Acctually, sometimes nomixup model is better than mixup... Titanic dataset may be too small to use Neural Net. So I think split data distribution has a big influence.","c380ddc3":"## Load data and feature Engineering\nThis is based <a href=\"https:\/\/www.kaggle.com\/vincentlugat\/titanic-neural-networks-keras-81-8\">[Titanic - Neural Networks [KERAS] - 81.8%]<\/a>","0b02c170":"Highlight of this kernel is\n* Explane what mixup is\n* Show how to implement in keras (not for image, but for table data)\n* Compare results of models train with mixup or without mixup."}}