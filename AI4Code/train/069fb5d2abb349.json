{"cell_type":{"cf2a355f":"code","d6ea87c5":"code","39fa2cc6":"code","c7c228f8":"code","44ab5e59":"code","764088ec":"code","45fbbfd1":"code","61bdb29e":"code","bd033d43":"code","d56c362e":"code","255b0793":"code","82d4b31a":"code","5892836a":"code","19f037a6":"code","35de00bd":"code","6773de37":"code","279846f3":"code","5b653db9":"code","03f832bf":"code","b42e3de3":"code","dd74c114":"code","0fca373b":"code","b9da6b50":"code","d6c11d1f":"code","3aeefae8":"code","3c23fa80":"code","44b50599":"code","0850d815":"code","a88028cf":"code","c612baeb":"code","67bb5a0b":"code","85fbcc1b":"code","3fd73500":"code","5f278ab6":"code","e88be8e9":"code","bf3d5ecd":"code","e231fbc6":"code","1718930b":"code","99890bc0":"markdown","1f63585b":"markdown","30bc24a2":"markdown","ef0c9cdd":"markdown","53c258f3":"markdown","22499dc4":"markdown","cde33bcc":"markdown","381cbe46":"markdown","b07bea8e":"markdown","406176e6":"markdown","af70b1dd":"markdown","93fa8899":"markdown","2fa073db":"markdown","729cbb58":"markdown","d2f4635c":"markdown","4089c33a":"markdown","3969c451":"markdown"},"source":{"cf2a355f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6ea87c5":"import seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats as st\nfrom scipy.stats import skew\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_squared_log_error, r2_score\nfrom sklearn.linear_model import Lasso, LassoCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import Ridge\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import ensemble\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom xgboost import XGBRegressor\n","39fa2cc6":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","c7c228f8":"Submission = df_test['Id']","44ab5e59":"df_train.describe().T","764088ec":"#Get the data types of each variable\ndf_types = df_train.dtypes","45fbbfd1":"corr = df_train.corr()\ncorr.sort_values([\"SalePrice\"], ascending = False, inplace = True)#\nprint(corr.SalePrice)\n\nax = plt.subplots(ncols=1, figsize=(10,10))\ncorr_matrix = df_train.corr()\nmask = np.zeros_like(corr_matrix)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr_matrix, mask=mask, vmin = -1, vmax = 1, center = 0);\nplt.show()","61bdb29e":"#Deep dive into one of our correlation pairs\nsns.scatterplot(x= df_train['1stFlrSF'], y=df_train['TotalBsmtSF'])","bd033d43":"#Distribution plot of SalePrice\nsns.distplot(df_train['SalePrice'], color=\"b\");\nplt.show()\n\n#Log transformed distribution plot of SalePrice\nLog_Y = df_train['SalePrice']\nsns.distplot(np.log10(Log_Y), color=\"c\");\nplt.show()","d56c362e":"\"\"\"for i in df_train.select_dtypes(include='object').columns:\n    sns.boxplot(x=df_train[i], y = df_train['SalePrice'])\n    plt.xticks(rotation=90)\n    #plt.show()\n    \nfor i in df_train.select_dtypes(exclude='object').columns:\n    sns.scatterplot(x=df_train[i], y=df_train['SalePrice'])\n    #plt.show()\"\"\"","255b0793":"#Finding all the outliers and their corresponding rows\nprint(df_train[(df_train['LotFrontage']>300)].index)\nprint(df_train[(df_train['LotArea']>200000)].index)\nprint(df_train[(df_train['BsmtFinSF1']>5000)].index)\nprint(df_train[(df_train['TotalBsmtSF']>5000)].index)\nprint(df_train[(df_train['1stFlrSF']>4000)].index)\nprint(df_train[(df_train['GrLivArea']>4500)].index)\nprint(df_train[(df_train['EnclosedPorch']>500)].index)\nprint(df_train[(df_train['MiscVal']>8000)].index)","82d4b31a":"#I noticed that not dropping 934, 313 and 346 improved the RMSE score, but decided to exclude them anyway.\ndf_train = df_train.drop([523, 1298, 934, 313, 346, 197])","5892836a":"#Create new columns to identify the train and test dataset when it will be requried to split again\ndf_train['Train']=1\ndf_test['Train']=0\n\n#Join datasets\ndf = pd.concat([df_train, df_test], axis=0)","19f037a6":"#Change type of variables\ndf['MSSubClass'] = df['MSSubClass'].apply(str)\ndf['YrSold'] = df['YrSold'].astype(str)\ndf['MoSold'] = df['MoSold'].astype(str)\ndf['YearRemodAdd'] = df['YearRemodAdd'].astype(str)","35de00bd":"df = df.drop([\"Id\"], axis=1)","6773de37":"quantitative = [f for f in df.columns if df.dtypes[f] != 'object']\nqualitative = [f for f in df.columns if df.dtypes[f] == 'object']\n\n#Fill missing values for quantitative variables\nfor i in quantitative:\n    df.fillna(df.median(), inplace = True)\n    #print(i, df[i].median())\n    \n#Fill missing values for special variables\nspec_categ_col =['PoolArea', 'Fence', 'MiscFeature', 'Alley','FireplaceQu']\nfor i in spec_categ_col:\n    df[i] = df[i].fillna('None')\n    \n#Fill missing values for categorical variables\nfor i in qualitative:\n    df[i].fillna(df[i].mode()[0], inplace = True)\n    \n#Check missing values for all variables\ndf.isnull().sum().sum()","279846f3":"df.skew(axis = 0, skipna = True).sort_values(ascending=False)","5b653db9":"#Log Transform variables to make them better fitted to the regression model\n#df.skew(axis = 0, skipna = True)\nif Num <= 1:\n    df[\"LotArea\"] = np.log1p(df[\"LotArea\"])\n    df[\"LotFrontage\"] = np.log1p(df[\"LotFrontage\"])\n    df[\"GrLivArea\"] = np.log1p(df[\"GrLivArea\"])\n    df[\"3SsnPorch\"] = np.log1p(df[\"3SsnPorch\"])\n    df[\"LotArea\"] = np.log1p(df[\"LotArea\"])\nelse:\n    num_feats = df.dtypes[df.dtypes != \"object\"].index\n    skewed_feats =df[num_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\n    skewed_feats = skewed_feats[skewed_feats > 0.75]\n    skewed_feats = skewed_feats.index\n    skewed_feats\n    df[skewed_feats] = np.log1p(df[skewed_feats])\n\n#df.to_csv('\/Users\/shyamsondagar\/Desktop\/Python\/House Prices Advanced Regression Techniques\/Export2.csv')","03f832bf":"#Feature Engineering\n#Total Floor area of entire house\ndf['TotalSF']=df['TotalBsmtSF']+ df['1stFlrSF']+ df['2ndFlrSF']\n#Total number of baths\ndf['TotalBath'] = (df['FullBath'] + (0.5 * df['HalfBath']) + df['BsmtFullBath'] + (0.5 * df['BsmtHalfBath']))\n#Total porch area\ndf['TotalPorchSF'] = df['OpenPorchSF'] + df['3SsnPorch'] + df['EnclosedPorch'] + df['ScreenPorch']+ df['WoodDeckSF']","b42e3de3":"#Create dummy variables for categorical columns.\ndf = pd.get_dummies(data=df)","dd74c114":"#Split into training and test dataset from the original data\ndf_train = df[df[\"Train\"] ==1]\ndf_test = df[df[\"Train\"] ==0]","0fca373b":"#Drop the unwanted columns\ndf_train = df_train.drop([\"Train\"], axis=1)\ndf_test = df_test.drop([\"Train\"], axis=1)\ndf_test = df_test.drop([\"SalePrice\"], axis=1)","b9da6b50":"if Num <= 1:\n    y = np.log1p(df_train[\"SalePrice\"]).values\nelse:\n    y = df_train.SalePrice\n","d6c11d1f":"x = df_train.drop([\"SalePrice\"], axis=1)","3aeefae8":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nscaler = MinMaxScaler()\nscaler.fit(x)\n\nscaled_x_train = scaler.transform(x)\nscaled_x_test = scaler.transform(df_test)\n\nx = pd.DataFrame(scaled_x_train, columns = x.columns)\ndf_test = pd.DataFrame(scaled_x_test, index = df_test.index, columns = df_test.columns)","3c23fa80":"Lasso_model = LassoCV(alphas = [1, 0.1,0.05, 0.001, 0.0005], selection='random', max_iter=15000).fit(x, y)\nLasso_train = Lasso_model.predict(x)\nLasso_Test = Lasso_model.predict(df_test)\n\nprint(np.sqrt(mean_squared_error(y,Lasso_train)))\nprint(r2_score(y, Lasso_train))\n#print(np.sqrt(mean_squared_error(Validate,Lasso_Test)))","44b50599":"coef = pd.Series(Lasso_model.coef_, index = x.columns)\nimp_coef = pd.concat([coef.sort_values().head(10), coef.sort_values().tail(10)])\nimp_coef.plot(kind=\"barh\")\nplt.show()","0850d815":"#XGB Boosting Regressor\nXGB_model = XGBRegressor( n_estimators=3000, \n                          learning_rate=0.01,\n                          max_depth=10, \n                          max_features='sqrt',\n                          min_samples_leaf=15, \n                          criterion='mse', \n                          min_samples_split=10, \n                          random_state =10).fit(x,y)\n\nXGB_train = gbr_model.predict(x)\nXGB_Test = gbr_model.predict(df_test)\n\nprint(np.sqrt(mean_squared_error(y,XGB_train)))\nprint(r2_score(y, XGB_train))\n#print(np.sqrt(mean_squared_error(Validate,XGB_Test)))","a88028cf":"#Random Forest Regressor\nforest_model = RandomForestRegressor(\n    random_state=10, \n    n_estimators=3000,\n    max_depth=10, \n    max_features='sqrt',\n    min_samples_leaf=15, \n    criterion='mse', \n    min_samples_split=10)\n\nforest_model.fit(x,y)\nforest_train = forest_model.predict(x)\nForest_Test = forest_model.predict(df_test)\n\nprint(np.sqrt(mean_squared_error(y,forest_train)))\nprint(r2_score(y, forest_train))\n#print(np.sqrt(mean_squared_error(Validate,Forest_Test)))","c612baeb":"#Random Forest Regressor2\n\nforest_model2 = RandomForestRegressor(random_state=1)\nforest_model2.fit(x,y)\nforest_train2 = forest_model.predict(x)\nForest_Test2 = forest_model.predict(df_test)\nprint(np.sqrt(mean_squared_error(y,forest_train2)))\nprint(r2_score(y, forest_train2))","67bb5a0b":"#Gradient Boosting Regressor\ngbr_model = ensemble.GradientBoostingRegressor(\n    n_estimators=3000, \n    learning_rate=0.01,\n    max_depth=10, \n    max_features='sqrt',\n    min_samples_leaf=15, \n    criterion='mse', \n    min_samples_split=10, \n    random_state =10).fit(x,y)\n\ngbr_train = gbr_model.predict(x)\ngbr_Test = gbr_model.predict(df_test)\n\nprint(np.sqrt(mean_squared_error(y,gbr_train)))\nprint(r2_score(y, gbr_train))\n#print(np.sqrt(mean_squared_error(Validate,gbr_Test)))","85fbcc1b":"#Ridge Regression\nRidge_model = Ridge(alpha=0.05)\nRidge_model.fit(x, y)\nRidge_train = Ridge_model.predict(x)\nRidge_Test = Ridge_model.predict(df_test)\n\n#print(pd.Series(Ridge_model.coef_, index = x.columns))\n\nprint(np.sqrt(mean_squared_error(y,Ridge_train)))\nprint(r2_score(y, Ridge_train))\n#print(np.sqrt(mean_squared_error(Validate,Ridge_Test)))","3fd73500":"#Gradient Boosting Regressor\ngbr_model = ensemble.GradientBoostingRegressor(n_estimators=3000, learning_rate=0.01,\n                                   max_depth=10, max_features='sqrt',\n                                   min_samples_leaf=15, criterion='mse', min_samples_split=10, random_state =10).fit(x,y)\ngbr_train = gbr_model.predict(x)\ngbr_Test = gbr_model.predict(df_test)\n\n#print(np.sqrt(mean_squared_error(Validate,gbr_Test)))\nprint(np.sqrt(mean_squared_error(y,gbr_train)))\nprint(r2_score(y, gbr_train))","5f278ab6":"#Ridge Regression\nRidge_model = Ridge(alpha=0.05)\nRidge_model.fit(x, y)\nRidge_train = Ridge_model.predict(x)\nRidge_Test = Ridge_model.predict(df_test)\n\n#print(pd.Series(Ridge_model.coef_, index = x.columns))\n\nprint(np.sqrt(mean_squared_error(y,Ridge_train)))\nprint(r2_score(y, Ridge_train))\n#print(np.sqrt(mean_squared_error(Validate,Ridge_Test)))","e88be8e9":"Hybrid_train = 0.335*Lasso_train + 0.565*gbr_train + 0.1*XGB_train\nprint(np.sqrt(mean_squared_error(y,Hybrid_train)))\nprint(r2_score(y, Hybrid_train))","bf3d5ecd":"fig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(y,Ridge_train, label ='Ridge')\nax1.scatter(y,gbr_train, label ='GBR')\nax1.scatter(y,Lasso_train, label ='Lasso')\nax1.scatter(y,forest_train, label ='RnDForest')\nax1.scatter(y,XGB_train, label ='XGB')\nax1.scatter(y,Hybrid_train, label ='Hybrid')\nplt.legend(loc='upper left')\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"black\")\nplt.show()","e231fbc6":"#Residuals\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.scatter(y,Ridge_train-y, label ='Ridge')\nax1.scatter(y,gbr_train-y, label ='GBR')\nax1.scatter(y,Lasso_train-y, label ='Lasso')\nax1.scatter(y,forest_train2-y, label ='RnDForest')\nax1.scatter(y,XGB_train-y, label ='XGB')\nax1.scatter(y,Hybrid_train-y, label = 'Hybrid')\nplt.legend(loc='upper left')\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"black\")\nplt.show()","1718930b":"df = pd.DataFrame({'Predicted': Lasso_Test})\ndf = np.exp(df)\nSub = pd.DataFrame()\nSub['Id'] = Submission\nSub['SalePrice'] = df\nSub.to_csv('Input URL Here!')\n\n","99890bc0":"**Key Takeaway**\n1. The distribution of the target variable is right-skewed\n","1f63585b":"**Key Takeaways**\n\nStrongly Negative Correlated Variables\n* BsmtUnffSF and BsmtFinfF1\n> Makes sense\n* BsmtFullBath and BsmtUnffSF\n> Requires investigating\n* Enclosed Porch and YearBuilt\n> Requires investigating\n\nStrongly Positive Correlated Variables\n* GrLivArea and TotRmsAbvGrd\n> > Requires investigating\n* 1stFlrSF and TotalBsmtSF\n> If there is more 1st floor space, then you would expect a similar sized basement floor too. However, its rare that the basement floor would be bigger than the first floor. A Home building article shows evidence that no further foundations would be required to support the house.\n* GarageYrBlt and YearBuilt \n> makes sense given that the house and garage are normally built at the same time.*\n* GarageArea and GarageCars\n> May be a multicolinear factor that shows the same relationship.*","30bc24a2":"# Explore the Target Variable\n\nWe need to do some exploratory analysis into the relationship between the target and exploratory variables. Showing boxplots of all the categorical variables (i.e object) and scatterplots for all those that are continuous in a good start.\n","ef0c9cdd":"## Data Exploration\n\nExplore the data by looking at the summary statistics to get a feel of the data. Alternatively, looking at the original dataset also helps you to understand the structure of the data.","53c258f3":"# Feature Engineering\n","22499dc4":"# Missing Values\n\nHere we impute the missing values for all:\n1. Numeric columns with the median\n> I chose the median over the mean because it generally performs better on skewed distributions. Also the median is less sensitive to outliers.\n2. Special columns where NA is not missing but instead not present \n> Need to rename \"NA\" otherwise they would be treated as missing data.\n3. All categorical columns with the mode","cde33bcc":"# **Prepare Data for Modelling**","381cbe46":"# Data Cleansing\n\nDrop all the outliers. These rows have been removed because they do not represent the \"normal population\" and may skew the final result.\n\n**Key Takeaways**\n1. All of the variables below had at least one-two values which were away from the norm. \n2. One row had a very large lotArea and floor space \n> Perhaps this could have been a very large estate or a ranch?\n","b07bea8e":"# Transform variables\n\nInitially, I only transformed the variables with high positive skew. In the future, I would probably find a way to automate those with a skew > a set value.","406176e6":"**Model Evaluation**\n\n1. R-Squared to evaluate the validity of the model\n2. Root Mean Square Error (RMSE) to measure the variation between the actual and the forecast\n\nThe best performing model was the hybrid model with an overall better R2 value and RMSE.\n","af70b1dd":"**Submission File**","93fa8899":"Using a correlation matrix, we can identify the exploratory variables which are strongly correlated with SalePrice. These insights may shed light on which variables help predict the SalePrice.\n\nAdditionally, the correlation between pairs of variables will identifty any multicolinear factors which may distort the model.","2fa073db":"**Key Takeaways**\n1. Average SalePrice is $180,921\n2. Potential outliers on LotArea and MiscVal with a large difference between 75% to Max.","729cbb58":"This step splits the ID column from the test dataset into a new dataframe. This will later be used in the submission file.","d2f4635c":"# **Modelling **","4089c33a":"# Hybrid Models - combine different models","3969c451":"# Normalise through Scaling"}}