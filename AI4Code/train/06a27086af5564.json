{"cell_type":{"59bfe897":"code","067bfc31":"code","d15c6348":"code","22bb9408":"code","8d079f82":"code","c9e3d9e3":"code","a06f2e6f":"code","78ce4dda":"code","6b7251a7":"code","9da11a6a":"code","c97eb864":"code","89055a45":"code","6bf46c50":"code","f2f7b931":"code","f3f1aeeb":"code","e19d75f6":"code","ca27a44f":"markdown","2864cf48":"markdown","e19cb530":"markdown","32396a8b":"markdown","7cb39b3f":"markdown","592721a4":"markdown","118bc62f":"markdown","d66ec155":"markdown","31e2ea0e":"markdown","795d0734":"markdown","f42971ab":"markdown","a7eb9850":"markdown","7c8d498d":"markdown","d91a259b":"markdown"},"source":{"59bfe897":"import numpy as np\nimport pandas as pd\n\nfrom sklearn.datasets import make_blobs\nfrom scipy.stats import multivariate_normal","067bfc31":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\n# matplotlib\nplt.rc('font', size=15)\nplt.rc('axes', titlesize=18)  \nplt.rc('xtick', labelsize=10)  \nplt.rc('ytick', labelsize=10)\n\n# seaborn\nsns.set(font_scale = 1.2)\nsns.set_style('whitegrid')","d15c6348":"# Configuration\nRANDOM_SEED = 1\nN_SAMPLE_SIZE = 1500\nN_CLUSTER = 3","22bb9408":"def plot_clusters(X, y=None, ax=None, **kwargs):\n    \"\"\"Displays the given data as a scatter plot.\"\"\"\n    if ax is None:\n        ax = plt.gca()\n        \n    sns.scatterplot(\n        x=X[:, 0], \n        y=X[:, 1],\n        palette='Set2',\n        hue=y, \n        ax=ax)\n        \n    return ax","8d079f82":"from matplotlib.patches import Ellipse\n\ndef plot_ellipse(x, covar, alpha=0.3, ax=None, **kwargs):\n    \"\"\"Displays for the given covarence matrix an ellipse that\n       which represents of the highest spread. \n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n    \n    for f, a in zip([1, 2, 3], [1., 0.75, 0.5]):\n        U, s, Vt = np.linalg.svd(covar)\n        angle = np.degrees(np.arctan2(U[1, 0], U[0, 0]))\n        width, height = f * 2 * np.sqrt(s)\n\n        e = Ellipse(\n            x, \n            width, \n            height, \n            angle=angle,\n            facecolor='gray',\n            **kwargs)\n\n        e.set_alpha(a*alpha)\n        ax.add_artist(e)\n    \n    return ax","c9e3d9e3":"def plot_center(x, ax=None, **kwargs):\n    \"\"\"Displays of a center point for the specified coordinate.\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    \n    sns.scatterplot(\n        x=[x[0]], \n        y=[x[1]],\n        color='red', \n        s=80, \n        alpha=0.8, \n        ax=ax)\n    \n    return ax","a06f2e6f":"X, y = make_blobs(\n    n_samples=N_SAMPLE_SIZE, \n    centers=N_CLUSTER,\n    n_features=2,\n    random_state=RANDOM_SEED\n)","78ce4dda":"plt.subplots(1, 1, figsize=(8, 8))\n\nplot_clusters(X)\nplt.show()","6b7251a7":"from typing import NamedTuple\n\nclass GaussianMixture(NamedTuple):\n    \"\"\"Gaussian mixture parameter set.\"\"\"\n    # (K, dim) array - each row corresponds to a gaussian component mean\n    mu: np.ndarray\n    \n    # (K, ) array - each row corresponds to the variance of a component\n    var: np.ndarray\n    \n    # (K, ) array = each row corresponds to the weight of a component\n    p: np.ndarray  ","9da11a6a":"def init(\n    X:np.ndarray, \n    num_clusters:int=3, \n    seed=None) -> GaussianMixture:\n    \"\"\"Initializes the gaussian mixture parameters.\n    \n    Parameters\n    ----------\n        X: np.ndarray, (N, dim) numpy array of observed data points\n        num_clusters: int, number of clusters, optional, default 3\n        seed: int, random state, optional, default None\n\n    Returns\n    -------\n        gm: initialized gaussian mixture parameters\n    \"\"\"\n    # data size (N) and dimension (d)\n    N, dim = X.shape \n    \n    # init mixture component means (\u03bc's)\n    sigma = [[1, 0], [0, 1]]\n    mu = multivariate_normal.rvs(\n        [0, 0], \n        sigma, \n        size=num_clusters, random_state=seed)\n        \n    # init covariances (\u03a3's)\n    var = [np.identity(dim) for k in range(0, num_clusters)]\n    \n    # init weights of mixture components\n    p = np.ones(num_clusters) \/ num_clusters\n    \n    return GaussianMixture(mu, var, p)","c97eb864":"def e_step(X:np.ndarray, gm:GaussianMixture) -> np.ndarray:\n    \"\"\"Performing E-Step.\n    \n    Parameters\n    ----------\n        X: np.ndarray, (N, dim) numpy array of observed data points\n        gm: the current gaussian mixture parameters\n\n    Returns\n    -------\n        post: the posterior probability p(i|k) that \n              the data point x_i belongs in cluster k.\n    \"\"\"\n    N, _ = X.shape\n    K, dim = gm.mu.shape # number of clusters (K)\n    \n    # init posteriors probabilities\n    post = np.zeros((N, K))\n    \n    for k in range(K):\n        rv = multivariate_normal(gm.mu[k], gm.var[k])\n        post[:, k] = gm.p[k] * rv.pdf(X)\n            \n    # normalize\n    c = np.sum(post, axis=1)[:, np.newaxis]\n    post \/= c\n    \n    return post","89055a45":"def m_step(X:np.ndarray, gm:GaussianMixture, post:np.ndarray) -> GaussianMixture:\n    \"\"\"Performing M-Step.\n    \n    Parameters\n    ----------\n        X: np.ndarray, (N, dim) numpy array of observed data points.\n        gm: the current gaussian mixture parameters.\n        post: the posterior probability p(i|k).\n\n    Returns\n    -------\n        gm: the updated gaussian mixture parameters.\n    \"\"\"\n    N, _ = X.shape\n    K, dim = gm.mu.shape # number of clusters (K)\n    \n    # update weights\n    w = np.sum(post, axis=0)\n    p = w \/ N\n\n    # update mixture component means (\u03bc's)\n    c = np.sum(post, axis=1)[:, np.newaxis]\n    mu = np.dot(post.T, X) \/ w[:, np.newaxis]\n\n    # update covariances (\u03a3's)\n    var = [np.identity(dim) for k in range(0, K)]\n    for k in range(K):\n        xs = X - gm.mu[k, :]\n\n        p_diag = np.diag(post[:, k])\n        p_diag = np.matrix(p_diag)\n\n        sigma = xs.T * p_diag * xs\n        var[k] = (sigma) \/ w[:, np.newaxis][k]    \n    \n    return GaussianMixture(mu, var, p)","6bf46c50":"def get_labels(post:np.ndarray) -> np.ndarray:\n    \"\"\"Returns the cluster belonging to the data point.\"\"\"\n    return np.argmax(post, axis=1)","f2f7b931":"def plot_gaussian_mixture(\n    X:np.ndarray, \n    gm:GaussianMixture, \n    post:np.ndarray, \n    ax=None,\n    **kwargs):\n    \"\"\"Displays the current state of the \n       Gaussian mixture parameter set.\"\"\"\n    if ax is None:\n        ax = plt.gca()\n    \n    y = get_labels(post)\n    plot_clusters(X, y=y, ax=ax)\n\n    for k in range(N_CLUSTER):\n        \u03bc = gm.mu[k]\n        \u03a3 = gm.var[k]\n\n        plot_center(\u03bc, ax=ax)\n        plot_ellipse(\u03bc, \u03a3, alpha=0.3, ax=ax)\n    \n    return ax","f3f1aeeb":"gm = init(X, num_clusters=N_CLUSTER, seed=RANDOM_SEED)","e19d75f6":"nrows = 2\nncols = 4\n\nfig, axs = plt.subplots(nrows, ncols, figsize=(30, 15))\n\nfor (step, ax) in zip(range(nrows*ncols), axs.ravel()):\n    post = e_step(X, gm)\n    gm = m_step(X, gm, post)\n    \n    plot_gaussian_mixture(X, gm, post, ax=ax)\n    \n    ax.set_title(f'Step {step+1}')\n    \nplt.legend()\nplt.show()","ca27a44f":"##  Model\n\nA Gaussian mixture model is parameterized by\n\n* the mixture component weights: $p_1, \\dots, p_K$, with $\\sum_k p_k =1$,\n\n* the component means: $\\mu_1, \\dots, \\mu_K$, \n\n* and the covariances : $\\Sigma_1, \\dots, \\Sigma_K$. ","2864cf48":"### M-Step\n\n* Update the mixing probabilities $p_k$ (weights):\n$$\n\\large \\hat{p}_k = \\frac{1}{N} \\sum_{i=1}^N p(i|k)\n$$\n\n\n* Update $\\mu_k$:\n$$\n\\large \\hat{\\mu}_k = \\frac{\\sum_{i=1}^N p(i|k) x_i}{\\sum_{i=1}^N p(i|k)}\n$$\n\n* Update covariances $\\Sigma_k$:\n$$\n\\large \\hat{\\Sigma}_k = \\frac{\\sum_{i=1}^N p(i|k)\\, (x_i-\\mu_k)(x_i-\\mu_k)^\\top}{\\sum_{i=1}^N p(i|k)}\n$$\n\n","e19cb530":"$$\n\\large p(x) = \\sum_{k=1}^K p_k \\, f(x | \\mu_k, \\Sigma_k )\n$$\n\n$$\n    \\large \\sum_{k=1}^K p_k = 1\n$$\n\n\nThe pdf of the multivariate Gaussian distribution for dimension $d$ is given by the formular:\n\n$$\n\\large f(x | \\mu, \\Sigma) = \\frac{1}{(2\\pi)^{d\/2} \\sqrt{|\\Sigma|}}\\,\\exp \\Big(-\\frac{1}{2}(x-\\mu)^{\\top}\\Sigma^{-1}(x-\\mu) \\Big)\n$$","32396a8b":"## Setup\n\nFirst, we will define some constant values and functions for displaying our results.","7cb39b3f":"### E-Step\n\nFor each data point $x_i$ calculate the posterior probability $p(i|k)$ that the data point $x_i$ belongs in cluster $k$:\n\n$$\n\\large p(i|k) := \\frac{p_k f(x_i|\\mu_k, \\Sigma_k)}{\\sum_{j=1}^n p_j f(x_i|\\mu_j, \\Sigma_j)}\n$$","592721a4":"## Overview\n\nIn the notebook, we give an implementation for a Gaussian mixture model (GMM) for computing clustering problems.","118bc62f":"## Generate data","d66ec155":"### M-Step","31e2ea0e":"## Visualization","795d0734":"<h3>Thanks for reading and please vote up for this notebook.<\/h3>","f42971ab":"## Expectation Maximization (EM)","a7eb9850":"## Implementation","7c8d498d":"### Initialization","d91a259b":"### E-Step"}}