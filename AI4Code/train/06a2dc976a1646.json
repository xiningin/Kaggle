{"cell_type":{"d443a1e5":"code","cb2d3dbc":"code","e006672e":"code","fa33d569":"code","24054e26":"code","5e9011b0":"code","34d19503":"code","1f6e3a72":"code","97946488":"code","ea28fe72":"code","3f64b745":"code","6df02806":"code","783067ef":"code","9e37830d":"code","f0342f2f":"code","3608b03d":"code","9a6b8064":"code","aa47bf72":"code","cd6d8d4e":"code","d1ff749b":"code","8bdd1950":"code","a9b9f90d":"code","de7a1425":"code","2866b0c9":"code","881eb806":"code","afc8dd6e":"code","67eab7a6":"code","cc3bfbca":"markdown","09b54354":"markdown","39bdc54e":"markdown","f09179d2":"markdown","f6bc9076":"markdown","f6774347":"markdown","8753a567":"markdown"},"source":{"d443a1e5":"import numpy as np\nimport pandas as pd \nimport tensorflow as tf\nimport nltk\nimport seaborn as sns\nimport re\nimport matplotlib.pyplot as plt\n\nfrom tensorflow.keras.layers import Embedding, Dense, LSTM, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot, Tokenizer\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix","cb2d3dbc":"df=pd.read_csv('..\/input\/emotions-in-text\/Emotion_final.csv') #Text data\nEMBEDDING_FILE= f'..\/input\/glove6b100dtxt\/glove.6B.100d.txt' #GloVe file path\ndf.head()","e006672e":"#Target Classes\nsns.countplot(df['Emotion']) ","fa33d569":"df=df.dropna() #Drop columns with NA values\nX=df.drop('Emotion',axis=1) #Input\ny=df['Emotion'] #Output","24054e26":"messages=X.copy()\nmessages.reset_index(inplace=True) #Drop NA may cause inconsistency in index","5e9011b0":"nltk.download('stopwords')\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    review = re.sub('[^a-zA-Z]', ' ', messages['Text'][i]) #Remove Special Characters\n    review = review.lower() #Lower case \n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] #Remove stopwords\n    review = ' '.join(review)\n    corpus.append(review)","34d19503":"corpus[:10]","1f6e3a72":"#Creating the dictionary with word as key and pretrained-value array as value\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.strip().split()) for o in open(EMBEDDING_FILE))\n\n#Calculate mean and std for the pretrained weights\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nprint(emb_mean,emb_std)","97946488":"voc_size=10000 # Vocabulary size\nembed_size=100 #word vector size\n\ntokenizer = Tokenizer(num_words=voc_size)\ntokenizer.fit_on_texts(list(corpus))\nword_index = tokenizer.word_index #Total words in the corpus\nnb_words = min(voc_size, len(word_index))\n\n#Initialize weight matrix for embedding layer\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size)) \n\nfor word, i in word_index.items():\n    if i >= voc_size: continue #Skip the words if vocab size is reached\n    embedding_vector = embeddings_index.get(word) #Extract the pretrained values from GloVe\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","ea28fe72":"#Contains the pretrained GloVe weights for the words\nlen(embedding_matrix)","3f64b745":"#One hot representation for input\nonehot_repr=[one_hot(words,voc_size)for words in corpus]\n\n#Finding max words\nl = 0\nfor x in corpus:\n    l = max(l,len(x.split(' ')))\n\n#Padding the sequences for input\nsent_length= l\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\nprint(embedded_docs)","6df02806":"#Encoding the target outputs to integers\nlabel_encoder = preprocessing.LabelEncoder()\n\nX_final=np.array(embedded_docs) #input to array\ny = label_encoder.fit_transform(y)\ny_final=np.array(y)\nprint(y_final)","783067ef":"X_final.shape,y_final.shape","9e37830d":"#Train-Test split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, \n                                                    test_size=0.2, random_state=42) \n#Train-Validation split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, \n                                                  test_size=0.1, random_state=21) ","f0342f2f":"# Creating model\nmodel=Sequential()\nmodel.add(Embedding(voc_size, embed_size, weights=[embedding_matrix]))\nmodel.add(Dropout(0.3))\nmodel.add(Bidirectional(LSTM(64)))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(64, activation='relu',kernel_regularizer=tf.keras.regularizers.l1(0.01))) #L1 regularization\nmodel.add(Dropout(0.3))\nmodel.add(Dense(6,activation='softmax'))\nmodel.compile(loss='sparse_categorical_crossentropy',optimizer= tf.keras.optimizers.Adam(learning_rate=0.001),\n              metrics=['accuracy'])\nmodel.summary()","3608b03d":"model_save = ModelCheckpoint('weights.h5', save_best_only = True, save_weights_only = True, monitor = 'val_loss', \n                             mode = 'min', verbose = 1)\nhistory = model.fit(X_train,y_train,validation_data=(X_val,y_val),epochs=40,batch_size=64,callbacks = [model_save])","9a6b8064":"print(history.history.keys())\n#  \"Accuracy\"\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n# \"Loss\"\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","aa47bf72":"#Load the best weights\nmodel.load_weights('weights.h5')","cd6d8d4e":"y_pred=model.predict_classes(X_test)\nprint(y_pred)","d1ff749b":"#Accuracy score\nprint(accuracy_score(y_test,y_pred))","8bdd1950":"#Classification report\nprint(classification_report(y_test, y_pred, digits=5))","a9b9f90d":"#Confusion Matrix\nprint('Confusion Matrix')\nprint(sns.heatmap(confusion_matrix(y_test, y_pred),annot=True,fmt=\"d\"))","de7a1425":"#Mapping of target classes using label-encoder\nle_name_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\nprint(le_name_mapping)","2866b0c9":"#Example\ndef predict_emotion(stri):\n    review = re.sub('[^a-zA-Z]', ' ', stri)\n    review = review.lower()\n    review = review.split()\n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')]\n    review = ' '.join(review)\n    onehot_repr = [one_hot(review,voc_size)] \n    embed = pad_sequences(onehot_repr,padding='pre',maxlen=sent_length)\n    predicti = model.predict(embed)\n    return label_encoder.classes_[np.argmax(predicti)]","881eb806":"predict_emotion('I am very happy and joyful today')","afc8dd6e":"predict_emotion('He is an arrogant and rude person')","67eab7a6":"predict_emotion('The teacher is intimidating and scary')","cc3bfbca":"# Test on own","09b54354":"# Embedding layer using GloVe ","39bdc54e":"# Process Input-Output data","f09179d2":"# Read and clean data","f6bc9076":"# Import libraries","f6774347":"# Analysis and visualization of output","8753a567":"# Create and train model"}}