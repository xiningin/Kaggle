{"cell_type":{"9cce4f43":"code","29158488":"code","001e27ee":"code","da06eb63":"code","e6752403":"code","b54c1f8c":"code","637a30c3":"code","668408fe":"code","71085913":"code","e5f4685c":"code","312bfba5":"code","37ef664d":"code","ce243064":"code","a2ac5f17":"code","fe33058b":"code","c7474135":"code","42721a48":"code","b9e8990d":"code","63ebb6f8":"code","d84c6d9e":"code","cbde24ff":"code","14730feb":"code","16a7ed80":"code","1e5371a8":"code","f6f717cc":"code","b67bc024":"code","6cc18df2":"code","79c97eeb":"code","ff9c9903":"code","9832563a":"code","9e39b857":"code","006575d0":"code","d9e66cb5":"code","f3c752d9":"code","8c138e23":"code","66e1447f":"code","11bed4a1":"code","2c8d1356":"code","d2405e5b":"code","f5c34217":"code","b1962514":"code","5b8c78dd":"code","4dc2f061":"code","c5c7c4ae":"code","85f2872b":"code","19a3158e":"code","559a5f5b":"code","8722f0c0":"code","4bfe6da1":"code","03db77e3":"code","874f606f":"code","1f413e06":"code","e4ca7d70":"markdown","6268dbb1":"markdown","ea280e2c":"markdown","34c19ac3":"markdown","ea9802bc":"markdown","3e886361":"markdown","1bc14320":"markdown","1c8c42f6":"markdown","fb6515e4":"markdown","16fffd2d":"markdown","3f8bda32":"markdown","5f3f395e":"markdown","49296acb":"markdown","19544e37":"markdown","0d62c574":"markdown","41e6c27c":"markdown","5995d18f":"markdown","dc8ca63c":"markdown","da07b155":"markdown","7430ca90":"markdown","66eb89b9":"markdown","4d97ef9a":"markdown","49362070":"markdown","5ea5989b":"markdown","dba1095f":"markdown","0976c8ee":"markdown","eedf40d2":"markdown","1a9727b6":"markdown","c78f5009":"markdown","05153254":"markdown","292d6010":"markdown","fa9bb3dc":"markdown"},"source":{"9cce4f43":"#Here, I load the necessary libraries that I will need to visualize the distribution \n# of the features to the target attribute \n# and to build my classification models.\n#Important mathematical and dataframe libraries and html univariate report\nimport numpy as np \nimport pandas as pd\nimport pandas_profiling as pf\n# For visualization and ploting\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\npd.options.display.max_rows = None\npd.options.display.max_columns = None\n\n# Support functions\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom scipy.stats import uniform\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn import svm\nfrom imblearn.over_sampling import ADASYN, SMOTE\n\n# Scoring functions\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import recall_score\n\n","29158488":"#Here, I read the data and lookhttps:\/\/www.geeksforgeeks.org\/python-pandas-df-size-df-shape-and-df-ndim\/ at their first 10 raws.\nchurnData = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')\nchurnData.head(10)","001e27ee":"# checking the dimension of my dataframe\nchurnData.info()","da06eb63":"# Generated a profile report about the data set using pandas profiling\n# report to be added in html format\nchurn_data_report = pf.ProfileReport(churnData)\nchurn_data_report\n# churn_data_report.to_file('churn_data_report.html')","e6752403":"#Let me change the columns names of my dataset to lower case.\nchurnData.columns = map(str.lower, churnData.columns)","b54c1f8c":"\n# Here, I am goint to check if there is a missing data.\nchurnData.isnull().sum()","637a30c3":"#Here, I am going to remove the first three columns.\nchurnData =  churnData.iloc[:, 3:14]\n# churnData.drop(columns = [list of columns], axis =1,inplace = True)","668408fe":"#The proportion pie chart of the churned and retained customers.\nlabels = 'churned', 'stayed'\nsizes = [churnData.exited[churnData['exited']==1].count(), churnData.exited[churnData['exited']==0].count()]\nfig1, ax1 = plt.subplots(figsize=(10, 8))\nax1.pie(sizes, labels=labels, autopct='%1.1f%%')\nax1.axis('equal')\nplt.title(\"Percentage of the customers churn\", size = 15)\nplt.legend()\nplt.show()\n","71085913":"#Visualizing how customers churn based on their gender group\nprint(pd.crosstab(churnData['gender'],churnData['exited']))\ngender = pd.crosstab(churnData['gender'],churnData['exited'])\ngender.div(gender.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize=(9,5))\nplt.xlabel('gender')\np = plt.ylabel('Percentage')\n","e5f4685c":"#Visualizing how customers churn based on their Geo-Location\nprint(pd.crosstab(churnData['geography'],churnData['exited']))\nLocation = pd.crosstab(churnData['geography'],churnData['exited'])\nLocation.div(Location.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize=(9,5))\nplt.xlabel('geography')\np = plt.ylabel('Percentage')\n","312bfba5":"#Visualizing how customers churn based on having credit card information\nprint(pd.crosstab(churnData['hascrcard'],churnData['exited']))\ncreditCard = pd.crosstab(churnData['hascrcard'],churnData['exited'])\ncreditCard.div(creditCard.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize=(9,5))\nplt.xlabel('credit card holding')\np = plt.ylabel('Percentage')\n","37ef664d":"#Visualizing how customers churn based on being active information\nprint(pd.crosstab(churnData['isactivemember'],churnData['exited']))\nactives = pd.crosstab(churnData['isactivemember'],churnData['exited'])\nactives.div(actives.sum(1).astype(float), axis=0).plot(kind=\"bar\", stacked=False, figsize=(9,5))\nplt.xlabel('active members')\np = plt.ylabel('Percentage')\n","ce243064":"# The contribution of the continuous attributes to the customers churn (exited) column.\n# Relations based on the continuous data attributes\ncontinuous = ['age', 'tenure', 'creditscore', 'estimatedsalary', 'balance','numofproducts']\nfig = plt.subplots(figsize = (15,15))\nfor i,j in enumerate(continuous):\n  plt.subplot(3,2,i+1)\n  sns.boxplot(x='exited', y = j , data=churnData)\n  plt.title(\"Boxplot of exited vrs {}\".format(j))\nplt.show()\n ","a2ac5f17":"#Selecting the continuous columns and view the correlation between them.\nchurnData_cont = churnData[[\"age\",\"tenure\",\"creditscore\",\"estimatedsalary\",\"balance\",\"numofproducts\"]]\nplt.figure(figsize=(13,8), dpi=100)\nsns.heatmap(churnData_cont.corr(), xticklabels=churnData_cont.corr().columns, yticklabels=churnData_cont.corr().columns, cmap=\"viridis\", annot=True)\nplt.title(\"Correlation of the continuous features\")\nplt.xticks(fontsize=12)\nplt.yticks(fontsize=12)\nplt.show()","fe33058b":"# Encoding the categorical features into numeric\n#Copy the churnData dataframe to data_copy to keep the orginal data before features engineering process\ndata_copy = churnData.copy()\ncatfeat = ['geography', 'gender']\nencode_dict = {}\nfor i in catfeat:\n  t = data_copy.groupby([i])['balance'].mean().sort_values(ascending =True).index\n  encode_dict[i] = {k:i for i,k in enumerate(t,0)}\nfor i in catfeat:\n  data_copy[i] = data_copy[i].map(encode_dict[i])  ","c7474135":"data_copy.head()","42721a48":"churnData['geography'].unique()","b9e8990d":"data_copy['geography'].unique()","63ebb6f8":"#Here, I am going to view the proportion of our target before performing the overbalancing or underbalancing\nfrom collections import Counter\nCounter(churnData['exited'])\n\nprint(churnData['exited'].value_counts(normalize=True))\ntotal = float(len(churnData))\nplt.figure(figsize=(5,5))\nax = sns.countplot(churnData['exited'],palette='cubehelix')\nfor p in ax.patches:\n   height = p.get_height()\n   ax.text(p.get_x()+p.get_width()\/2.,\n           height + 3,\n           '{:.2f}%'.format((height\/total)*100),\n           ha=\"center\")\n# plt.savefig('targ.jpg')\nplt.legend()\nplt.show()","d84c6d9e":"# I split the data into predictors and target columns to overbalance the minority\nfeatures = data_copy[['creditscore', 'geography', 'gender', 'age', 'tenure', 'balance',\n       'numofproducts', 'hascrcard', 'isactivemember', 'estimatedsalary']]\ntarget = data_copy[['exited']]","cbde24ff":"#Resempling with SMOTE to have the same class proportion in our data\nfrom imblearn.over_sampling import SMOTE\nsmote = SMOTE(\n    sampling_strategy='minority',\n    random_state=None,\n    k_neighbors=5,\n    n_jobs=None,\n)\nfeature_smote, target_smote = smote.fit_sample(features, target)","14730feb":"#Convert the dataframe into series to plot it.\nSmotingData=target_smote.iloc[0,:]\ntype(SmotingData)","16a7ed80":"#To see how the data has become balenced after performing smote operation\nfrom collections import Counter\nCounter(churnData['exited'])\n\nprint(SmotingData.value_counts(normalize=True))\nprint('+-+'*38)\nprint('The target variable is balanced with a 50% between the number of churned customers and \\\nthose non churn customers')\nprint('+-+'*38)\ntotal = float(len(SmotingData))\nplt.figure(figsize=(5,5))\nax = sns.countplot(SmotingData,palette='Set1')\nfor p in ax.patches:\n   height = p.get_height()\n   ax.text(p.get_x()+p.get_width()\/2.,\n           height + 3,\n           '{:.2f}%'.format((height\/total)*100),\n           ha=\"center\")\n# plt.savefig('targ.jpg')\nplt.legend()\nplt.show()","1e5371a8":"# Split data into train and test\n# Data splitting section, here we get the training data to pass into our model and the test data to evaluate the performance of the model \n\nfrom sklearn.model_selection import train_test_split\nxtrain, xtest, ytrain, ytest = train_test_split(feature_smote, target_smote, test_size=0.2, random_state=1)\nxtrain,xtest,ytrain,ytest = train_test_split(features,target,test_size = .2,random_state = 2)\nprint(xtrain.shape)\nprint(ytrain.shape)\nprint(xtest.shape)\nprint(ytest.shape)\n\n","f6f717cc":"#The data need to be standardized to decrease ambiguity and guessworkengineering\nfrom sklearn.preprocessing import RobustScaler,MinMaxScaler,StandardScaler\nscal = StandardScaler()\nxtrain = scal.fit_transform(xtrain)\nxtest = scal.transform(xtest)","b67bc024":"#Imoprting several machine learning classify algorithms \nlogreg = LogisticRegression()\nlogreg = logreg.fit(xtrain,ytrain)\npred = logreg.predict(xtest)\n\nprint('Classification Report')\nprint('+-+'*15)\nprint(classification_report(ytest,pred))\nprint('+-+'*15)\nprint('Confusion Matrix')\nprint('+-+'*15)\ncm = confusion_matrix(ytest,pred)\ndf_cm = pd.DataFrame(cm, index=['Not Churn','Churn'], columns=['Not Churn','Churn'])\nplt.figure(figsize=(8,5))\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\",cmap='Set1')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix for Logistic Regression')\nplt.show()\n","6cc18df2":"radreg = GradientBoostingClassifier(n_estimators=600,max_depth =5,random_state=4)\ngradreg = radreg.fit(xtrain,ytrain)\npred = gradreg.predict(xtest)\nprint(classification_report(ytest,pred))\nprint('')\nprint(recall_score(ytest,pred))\nprint('')\nprint(roc_auc_score(ytest,pred))","79c97eeb":"randreg = RandomForestClassifier()\nrandreg = randreg.fit(xtrain,ytrain)\npred = randreg.predict(xtest)\nprint(classification_report(ytest,pred))\nprint('')\nprint(recall_score(ytest,pred))\nprint('')\nprint(roc_auc_score(ytest,pred))","ff9c9903":"#Training the model using catBoostClassified algorithm\ncatb = CatBoostClassifier(iterations=1000,learning_rate=0.001,\n                          depth = 5,use_best_model=True,eval_metric='AUC',\n                          early_stopping_rounds=10,\n                          verbose = 100)\n\ncatb = catb.fit(xtrain,ytrain,eval_set=(xtest,ytest), plot=True)\npred = catb.predict(xtest)\nprint(classification_report(ytest,pred))\nprint('')\n\n# plotting feature importance\nplt.figure(figsize=(12,8))\nfeat_importances = pd.Series(catb.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh',color = 'crimson')\nplt.title('Feature Importance from  Cat Boost Classifier')\nplt.show()\n","9832563a":"#Here, I laod the library which deals with the imbalanced dataset\n#Create an object of the classifier using Decision Tree algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=DecisionTreeClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print out the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","9e39b857":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"DecisionTree Average Recall\", crossRecall.mean())\nprint(\"DecisionTree Average Accuracy\", crossScore.mean())","006575d0":"\n#Create an object of the classifier Rondom Forest algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=RandomForestClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#print the performance of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","d9e66cb5":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"RandomForest Average Recall\", crossRecall.mean())\nprint(\"RandomForest Average Accuracy\", crossScore.mean())","f3c752d9":"#Create an object of the classifier using Gradient Boosting algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=GradientBoostingClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\ngrb = bbc.fit(xtrain, ytrain)\npreds = grb.predict(xtest)\n#Print the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","8c138e23":"#Create an object of the classifier using Logistic Regression.\nbbc = BalancedBaggingClassifier(base_estimator=LogisticRegression(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","66e1447f":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"LogisticRegression Average Recall\", crossRecall.mean())\nprint(\"LogisticRegression Average Accuracy\", crossScore.mean())","11bed4a1":"#Create an object of the classifier using catBoostClassifier algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=CatBoostClassifier(verbose= 0),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","2c8d1356":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"CatBoostClassifier Average Recall\", crossRecall.mean())\nprint(\"CatBoostClassifier Average Accuracy\", crossScore.mean())","d2405e5b":"#Average of the Recall across 10-folds on the CatBoostClassifier\ncrossRecall.mean()","f5c34217":"\n# plotting feature importance, the features that are contributing more in catBoostClassifier\nplt.figure(figsize=(12,8))\nfeat_importances = pd.Series(catb.feature_importances_, index=features.columns)\nfeat_importances.nlargest(10).sort_values().plot(kind='barh',color = 'crimson')\nplt.title('Feature Importance from  Catboost Classifier')\nplt.show()","b1962514":"# We look for the best parameters to traing XGBClassifier model\nm_dep = [5,6,7,8]\ngammas = [0.01,0.001,0.001]\nmin_c_wt = [1,5,10]\nl_rate = [0.05,0.1, 0.2, 0.3]\nn_est = [5,10,20,100]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=3, n_jobs=-1, refit=True)\nxgb_cv.fit(xtrain,ytrain)\n\nprint(\"tuned hpyerparameters :(best parameters) \",xgb_cv.best_params_)\nprint(\"accuracy :\",xgb_cv.best_score_)\nprint(xgb_cv.best_estimator_)\n","5b8c78dd":"#Traning the XGBClassifier model with the best parameters and check its performance without balanced algorithm\nxgb = XGBClassifier( n_estimators = 100, min_child_weight= 5, max_depth= 5, learning_rate= 0.1, gamma=0.001)\nxgb = xgb.fit(xtrain,ytrain)\npred = xgb.predict(xtest)\n\nprint('Classification Report')\nprint('+-+'*15)\nprint(classification_report(ytest,pred))\nprint('+-+'*15)\nprint('Confusion Matrix')\nprint('+-+'*15)\ncm = confusion_matrix(ytest,pred)\ndf_cm = pd.DataFrame(cm, index=['Not Churn','Churn'], columns=['Not Churn','Churn'])\nplt.figure(figsize=(8,5))\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\",cmap='Set1')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix for XGBoost')\nplt.show()","4dc2f061":"# Plotting the confusion Matrix of the XGBoost model\nm_dep = [7]\ngammas = [0.01]\nmin_c_wt = [1]\nl_rate = [0.2]\nn_est = [100]\n\nparam_grid = {'n_estimators': n_est, 'gamma': gammas, 'max_depth': m_dep,\n              'min_child_weight': min_c_wt, 'learning_rate': l_rate}\n\nxgb_cv_10 = RandomizedSearchCV(estimator = XGBClassifier(), n_iter=100, param_distributions =  param_grid, random_state=51, cv=10, n_jobs=-1, refit=True)\nxgb_cv_10.fit(xtrain,ytrain)\n\npred = xgb_cv_10.predict(xtest)\nroc_auc_score(ytest, pred)\nprint('Classification Report')\nprint('+-+'*15)\nprint(classification_report(ytest,pred))\nprint('+-+'*15)\nprint('Confusion Matrix')\nprint('+-+'*15)\ncm = confusion_matrix(ytest,pred)\ndf_cm = pd.DataFrame(cm, index=['Not Churn','Churn'], columns=['Not Churn','Churn'])\nplt.figure(figsize=(8,5))\nheatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\",cmap='Set1')\nheatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=15)\nheatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=15)\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\nplt.title('Confusion Matrix for XGBClassifier algorithm')\nplt.show()","c5c7c4ae":"# Checking the area under the curve of XGBClassifier with 10 folds cross validation\ny_pred_prob = xgb_cv_10.predict_proba(xtest)[:,1]\nfpr, tpr, thresholds = roc_curve(ytest, y_pred_prob)\nplt.plot([0,1], [0,1], 'k--')\nplt.plot(fpr, tpr, label='xgb')\nplt.legend()","85f2872b":"# View the area under the curve score\nroc_auc_score(ytest, y_pred_prob)","19a3158e":"#Here, I laod the library which deals with the imbalanced dataset\n#Create an object of the classifier using XGBoost algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=XGBClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print out the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","559a5f5b":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"XGBoost Average Recall\", crossRecall.mean())\nprint(\"XGboost Average Accuracy\", crossScore.mean())","8722f0c0":"#Create an object of the classifier using K-Neighbors Classier.\nbbc = BalancedBaggingClassifier(base_estimator=KNeighborsClassifier(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print out the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))\n","4bfe6da1":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"KNeighbors Average Recall\", crossRecall.mean())\nprint(\"KNeighbors Average Accuracy\", crossScore.mean())","03db77e3":"#Here, I laod the library which deals with the imbalanced dataset\n#Create an object of the classifier using Support vector machine algorithm.\nbbc = BalancedBaggingClassifier(base_estimator=svm.SVC(),\n                                sampling_strategy='auto',\n                                replacement=False,\n                                random_state=0)\n#Train the classifier.\nbbc.fit(xtrain, ytrain)\npreds = bbc.predict(xtest)\n#Print out the results of the model\nprint(classification_report(ytest,preds))\nprint('')\nprint(recall_score(ytest,preds))\nprint('')\nprint(roc_auc_score(ytest,preds))","874f606f":"# Average accuracy and Recall across 10-folds cross-validation\ncrossRecall = cross_val_score(bbc, features, target, cv=10, scoring='recall')\ncrossScore = cross_val_score(bbc, features, target, cv=10, scoring='accuracy')\nprint(\"SVM Average Recall\", crossRecall.mean())\nprint(\"SVM Average Accuracy\", crossScore.mean())","1f413e06":"#Perform the cross validation data splitting to see the model improvement on many testing set.\n# Here, I used k-fold cross validation with 10 folds\ncrossRecallg = cross_val_score(grb, features, target, cv=10, scoring='recall')\ncrossScoreg = cross_val_score(grb, features, target, cv=10, scoring='accuracy')\n# The recall on all 10-folds as an arrary\nprint(\"Recall Average:\", crossRecallg.mean())\n#The avarega of the recalls across all 10- folds with gradient boosting model\nprint(\"Accuracy Average\", crossScoreg.mean())","e4ca7d70":"### Features engineering\nIn the step, I performed the features engineering process by transforming the categorical columns into encoding, later on, I will do features standardization to improve the model performance.","6268dbb1":"### Cross Validation on Gradient Boost model with Balanced Algorithm\n\n","ea280e2c":"### Random Forest Classifier","34c19ac3":"Not surprisingly, in the above histogram, the zero in legend above shows the inactive this means that the inactive customes churn more than the active ones.\n\nIn this descriptive analysis, we note the following:\n\n * Majority of the data is from persons from France. However, the proportion of churned customers is with inversely related to the population of customers, now the bank needs to put more effort to the more churning location.\n * The proportion of female customers churning is also greater than that of male customers and the majority of the female is less then the male majority. Here the bank needs to investigate why the female are churning more.\n \n * Interestingly, majority of the customers that churned are those with credit cards. Given that majority of the customers have credit cards could prove this to be just a coincidence.\n \n  * Unsurprisingly the inactive members have a greater churn. Worryingly is that the overall proportion of inactive members is quite higher than the active ones, like a data scientist, this is a good indication to suggest the bank turn those inactive customers into an active group as they will increase the bank's revenue.\n\n\n\n        The analysis of the contribution of the continuous attributes to customers churn situation.","ea9802bc":"## 2. **Data Manipulation**\n\nThe data features names are mixed in small and capital letters, I will make them all small letters.<br \/>\nChecking if there is a missing data for data cleaning<br \/>\nDropping three first Irrelevant attributes for features selection.","3e886361":"### XGBoost with the balanced algorithm","1bc14320":"### Univariate analysis","1c8c42f6":"### Cat Boost with Balanced Algorithm","fb6515e4":"### Conclusion\n\nIn this project of classification of the customers' churn situation, I have explored the univariate, bivariate and modeling analysis. the univariate analysis is available in the provided Html file. After using the several classifier machine learning algorithms, balancing the data using a machine learning algorithm and using 10-folds cross-validation, I have come up with the model which performs better than others on our classification problem.\nOur interested matrices are <u> Recall<\/u> and\u00a0Accuracy,\u00a0Recall\u00a0because I\u00a0need\u00a0the\u00a0model\u00a0with\u00a0less\u00a0False\u00a0Negative\u00a0*(FN)* meaning\u00a0that\u00a0the\u00a0model\u00a0which\u00a0will\u00a0less\u00a0predict\u00a0that \nthe\u00a0customers\u00a0will\u00a0not\u00a0churn\u00a0but \ntruly\u00a0the\u00a0customers\u00a0will\u00a0churn.\n\nBy comparing those several algorithm the Gradient Boosting predict better than others with *74.76%* of Recall metric, this means that it has the probability of *74.76%* to predict the customers who are going to churn. The accuracy of the Gradient Boosting is *80%*.","16fffd2d":"Lucky me, there is zero missing values in my data,so, *data cleaning* becomes much easy to me.<br \/>\nNow, the *features selection* is my next step, according to my target, I am going to drop the first three columns.","3f8bda32":"### Decision Tree with Balanced Algorithm","5f3f395e":"### Logistic Regression with Balanced Algorithm","49296acb":"### Support Vector Machine with balanced algorithm","19544e37":"## Modeling","0d62c574":"### Hyper-Parameter search on XGBClassifier","41e6c27c":"### Random Forest with Balenced Algorithm","5995d18f":"## 3. Descriptive Data Analysis\n\nIn this section, I will do the distribution of the features according to my target column.<br \/>\n* Distribution of the target column(exited) itself.\n* Distribution of the continuous attributes concerning the target column.\n* Distribution of the categorical columns according to the target column.\n<br \/>I will add the comments to show clearly how the features are contributing to the customer's churn.\n\n      Proportion of the churned and stayed customers.","dc8ca63c":"### K- Nearest Neighbors Classifier","da07b155":"From the above visualization, we can see that Customers located in Germany churn more than those located from France and Spain.\n\n#### Here, I need to check how the attribute called <u>hascrcard<\/u> ('which means if the customer has a credit card or not') is affecting the customers churn. ","7430ca90":"### Catboost Classifier","66eb89b9":"### Data leakage \nData\u00a0leakage\u00a0refers\u00a0to\u00a0a\u00a0mistake\u00a0made\u00a0by\u00a0the\u00a0creator\u00a0of\u00a0a\u00a0machine\u00a0learning\u00a0model\u00a0in\u00a0which\u00a0they\u00a0accidentally\u00a0share\u00a0information\u00a0between\u00a0the\u00a0test\u00a0and\u00a0training\u00a0data-sets.\u00a0Typically,\u00a0when\u00a0splitting\u00a0a\u00a0data-set\u00a0into\u00a0testing\u00a0and\u00a0training\u00a0sets,\u00a0the\u00a0goal\u00a0is\u00a0to\u00a0ensure\u00a0that\u00a0no\u00a0data\u00a0is\u00a0shared\u00a0between\u00a0the\u00a0two.\u00a0Here\u00a0I\u00a0split\u00a0the\u00a0data\u00a0before\u00a0performing\u00a0the\u00a0standardization\u00a0or\u00a0normalization\u00a0to\u00a0avoid\u00a0the\u00a0data\u00a0leakage\u00a0issue.","4d97ef9a":"### Logistic Regression","49362070":"### Gradient Boosting Classifier","5ea5989b":"From the above histogram chart, it shows that the female churned more than the male gender.\n\n\n ##### Now, I am going to check the variation of the customers churn based on their geographical location\n","dba1095f":"## Imbalanced Dataset\nOne\u00a0of\u00a0the\u00a0common\u00a0issues\u00a0found \nin\u00a0datasets\u00a0that\u00a0are\u00a0used\u00a0for\u00a0classification \nis an imbalanced class issue.The\u00a0imbalanced\u00a0usually\u00a0reflects\u00a0an\u00a0unequal\u00a0distribution\u00a0of\u00a0classes\u00a0within\u00a0a\u00a0dataset,\u00a0in\u00a0bank's\u00a0customers\u00a0churn\u00a0data\u00a0we\u00a0have\u00a0less\u00a0exited\u00a0data\u00a0than\u00a0the\u00a0stayed\u00a0one.\u00a0By\u00a0building\u00a0the\u00a0classification\u00a0model,\u00a0the\u00a0probability\u00a0of\u00a0the\u00a0model\u00a0to\u00a0recognize\u00a0the\u00a0not\u00a0churn\u00a0customers\u00a0will\u00a0be\u00a0high\u00a0since\u00a0the\u00a0algorithm\u00a0has\u00a0more\u00a0data\u00a0in\u00a0the\u00a0training\u00a0set\u00a0and\u00a0to\u00a0recognize\u00a0the\u00a0churned\u00a0customers\u00a0will\u00a0be\u00a0difficult\u00a0to\u00a0the\u00a0model.\u00a0To\u00a0overcome\u00a0this\u00a0issue,\u00a0we\u00a0use\u00a0***imbalanced-learn***\u00a0library to improve our\u00a0model\u00a0performance.","0976c8ee":"\nAfter we encode the categorical data, we can see the geagraghy attribute becomes France=1, Spain=0, Germany=2.\nThe gender attribute becomes Female=0 and Male=1 ","eedf40d2":"From the above visualization, we can see that Customers with the credit cards (1 in legeng) churn more than those who do not have the credit cards.\n\n#### Let me check also how the situation of the customer being active or not is affecting the customers churn too.\n\n","1a9727b6":"From the above subplot where 0 means stayed and 1  means the churned customers. We note the following:\n\n\n\n   *  Worryingly, the bank is losing customers with significant bank balances which is likely to hit their available capital for lending.\n   \n   *  The product and the salary have an insignificant effect on the likelihood to churn.\n   \n   * There is insignificant difference in the credit score distribution between retained(0) and churned(1) customers.\n   * The older customers are churning at more than the younger ones. The bank may need to review their target market or review the strategy for retention between the different age groups. An idea, can the bank approaches the retired people to increase the older customers' group? \n   \n   * In the tenure, the clients who spent long time with the bank are more likely to leave compared to those that are of average time.\n   \n##### Now,\u00a0we\u00a0are\u00a0going\u00a0to\u00a0visualize\u00a0the\u00a0correlation\u00a0between\u00a0the\u00a0continuous\u00a0features\u00a0and\u00a0here\u00a0we\u00a0can\u00a0see\u00a0that\u00a0our\u00a0features\u00a0are\u00a0nor\u00a0strongly\u00a0correlated,\u00a0so,\u00a0they\u00a0won't\u00a0cause\u00a0any redundancy effect\u00a0in\u00a0our\u00a0model\u00a0building.\n  \n  \n","c78f5009":" After building the model, we can visualize which attributes are contributing the most in customers churning\n.","05153254":"## 1. *Bachground*\nThe situation of the customers churn in a given organization is defined as the loss of the customers or customers stop using the company's services. Companies like banks, telecommunication, insurance and many more need to use customers churn analysis as it has proved that, it is less expensive to retain existing customers than to acquire the new ones. The existing customers purchase more than the new ones too. Introducing the new products or services to existing customers is easier than to the new customers. So, the companies need to use the data scientist to analyze the historical company's data to support the decision-makers by giving them an indication of the customers who are about to churn and the cause of the customer's decline.\n         \n   ###### The objectives of this project are:\n* To investigate and visualize the predictors which contribute the most in customers leaving,\n* To build the machine leaning algorithms which classify the customers churn based on the available historical data,\n* To select the algorithm which has the best performance(high Recall,Accuracy) compare to the others, and that algorithm can be used to preduct the new data.\n\n*Let's get started!*\n\n<u>Explanaition of the dataset<\/u> <br \/>The data has 14 features(columns or attributes), there are categorical columns, continuous columns and target column.\n","292d6010":"### Gradient Boosting with Balanced Algorithm","fa9bb3dc":"*In this above pie chart, it shows that in our dataset the churned customers occupy 20% of the entire population, the churned customers have the small percentage then, here I will have to find the prediction model which has the high accuracy in order to be able to track those churned customers.*\n    \n\n### Bivariate Analysis    \n     Now, I going to evaluate the contribution of the categorical attributes to the target (exited) column\n   ##### Visualizing of how customers churn change based on their gender group"}}