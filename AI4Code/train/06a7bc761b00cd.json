{"cell_type":{"09b5a373":"code","1a178ca7":"code","621a08ac":"code","c28a021a":"code","7bdf80fb":"code","c56632c7":"code","0de17d85":"code","70b2fa2f":"code","afb8f1b4":"code","c78e50db":"code","9bab06d5":"code","28f04c88":"code","4a87f0c4":"code","ac77f9eb":"code","aacce158":"code","74177e3b":"code","d2785af5":"code","b423e7d0":"code","f0996b2c":"code","a81fd2a0":"code","1d7cfed1":"code","ce4df68e":"code","d563231a":"code","44ef16b3":"code","8a476bf5":"markdown","bfa3cd5f":"markdown","52134b31":"markdown","6be77f40":"markdown","c76b01f0":"markdown","a1adbe89":"markdown","0978c308":"markdown","2158e5f2":"markdown","d998958a":"markdown","e06bfffd":"markdown","ccbe9ab6":"markdown","d6f72819":"markdown","cb1b9706":"markdown","289606e4":"markdown","4030af60":"markdown","4111bbb6":"markdown","ed68e00b":"markdown","09a576ea":"markdown"},"source":{"09b5a373":"import pandas as pd\nimport numpy as np\n\n# What do we say to python warnings? NOT TODAY\nimport warnings\nwarnings.filterwarnings('ignore')","1a178ca7":"def select_features(df, target, th):\n    \"\"\"\n    Select features.\n    \"\"\"\n    # Select rows with our target value\n    proc_df = df[df[target].isna() == False]\n    \n    # Remove useless columns\n    to_drop = [col for col in proc_df.columns if ((\"(F)\" in col) or (\"(fpm)\" in col) or (\"Thermal\" in col) or (\"Air movement\" in col)) and (col != target)] + [\"Database\", \"Publication (Citation)\", \"Data contributor\"]\n    proc_df = proc_df.drop(to_drop, axis=1)\n    \n    # Remove columns with a lot of missing values\n    # Get columns with less than 30% of data missing\n    s = (proc_df.isna().sum() \/ len(proc_df) * 100 < th)\n    to_keep = list(s[s].index)\n    proc_df = proc_df[to_keep]\n    \n    return proc_df","621a08ac":"def group_koppen_categories(category):\n    if \"A\" in category:\n        return \"A\"\n    elif \"B\" in category:\n        return \"B\"\n    elif \"C\" in category:\n        return \"C\"\n    elif \"D\" in category:\n        return \"D\"\n    elif \"E\" in category:\n        return \"E\"\n    else:\n        return None\n    \ndef handle_categorical_features(df):\n\n    from sklearn.impute import SimpleImputer\n    from sklearn.preprocessing import OneHotEncoder\n    \n    # Drop climate\n    df.drop(\"Climate\", axis=1, inplace=True)\n    # Group koppen climate\n    df[\"Koppen climate classification\"] = [group_koppen_categories(category) for category in df[\"Koppen climate classification\"]]\n    # Fill missing cooling strategy in city Harbin\n    df.loc[df.City == \"Harbin\", \"Cooling startegy_building level\"] = \"Naturally Ventilated\"\n    # Fill missing city in Malaysia\n    df.loc[df.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n    \n    # Input mode by city in Season and Cooling strategy\n    # Get list of categorical variables with missing values\n    cols = [\"Season\", \"Cooling startegy_building level\"]\n    # Input mode for each city\n    for city in df.City.unique():\n        # Filter data of selected city\n        temp = df.loc[df.City == city, cols]\n        # Create imputer\n        imputer = SimpleImputer(strategy='most_frequent')\n        # Input missing values with mode\n        imputed = pd.DataFrame(imputer.fit_transform(temp))\n        # Rename columns and index\n        imputed.columns = temp.columns\n        imputed.index = temp.index\n        # Replace in dataframe\n        df.loc[df.City == city, cols] = imputed\n        \n    # Encode\n    # Get list of categorical variables\n    s = (df.dtypes == 'object')\n    cols = list(s[s].index)\n    # One Hot Encoder\n    for col in cols:\n        # Create encoder\n        OH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\n        # Transform\n        OH_cols = pd.DataFrame(OH_encoder.fit_transform(df[[col]]))\n        # Get categories names and rename\n        names = OH_encoder.categories_\n        OH_cols.columns = names\n        OH_cols.index = df.index\n        # Add encoded columns\n        df[list(names[0])] = OH_cols\n\n    # Drop un-encoded column\n    df.drop(cols, axis=1, inplace=True)\n    \n    return df\n    \ndef get_balance_dataset_index(df, target):\n    \"\"\"\n    df: the dataset to balance\n    target: the name of the target column\n    \"\"\"\n    \n    # Get count for each category\n    value_counts = df.value_counts(target).to_dict()\n    \n    # List comprehension to find the key with the minimum value (count)\n    min_category = [k for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    min_count = [v for k,v in value_counts.items() if v == min(value_counts.values())][0]\n    \n    # For each category in your target\n    dfs = []\n    for key in value_counts:\n        if key == min_category:\n            df1 = df[df[target] == min_category]\n        else:\n            df1 = df[df[target] == key].sample(min_count, random_state=55)\n        dfs.append(df1)\n    \n    dfb = pd.concat(dfs)\n    print(f\"Your balance dataset: {dfb.value_counts(target).to_dict()}\")\n    \n    return dfb.index","c28a021a":"def handle_numerical_features(df, df_raw):\n    \"\"\"\n    Input mean by city.\n    \"\"\"\n    \n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)',\n             'City',\n             'Country']\n\n    df1 = df_raw[cols]\n\n    # Input mode in missing Malaysia city\n    df1.loc[df1.Country == \"Malaysia\", \"City\"] = \"Kota Kinabalu\"\n\n    cols = ['Clo',\n             'Met',\n             'Air temperature (C)',\n             'Relative humidity (%)',\n             'Air velocity (m\/s)',\n             'Outdoor monthly air temperature (C)']\n\n    # Input mean by city in numerical features\n    # Input mode for each city\n    for city in df1.City.unique():\n        # Filter data of selected city\n        temp = df1.loc[df1.City == city, cols]\n        # Serie with the mean per column\n        means = temp.mean()\n        # Fill the missing values with the mean\n        temp = temp.fillna(means)\n        # Replace in dataframe\n        df1.loc[df1.City == city, cols] = temp\n\n    # there are cities with all null\n    df1 = df1.fillna(df1.mean())\n\n    # Add to dataset\n    df = df.drop(cols, axis=1)\n    df[cols] = df1.drop([\"City\",\"Country\"], axis=1) # we don't need the column City anymore, we have it encoded\n    \n    return df","7bdf80fb":"def simple_fe(df, df_raw):\n    \"\"\"\n    Add simple features to data frame.\n    \"\"\"\n    # New categorical features\n    # Load countries data\n    countries = pd.read_csv(\"..\/input\/latitude-and-longitude-for-every-country-and-state\/world_country_and_usa_states_latitude_and_longitude_values.csv\",\n                           usecols=[\"country\",\"latitude\"])\n    # Rename column\n    countries = countries.rename(columns={\"country\":\"Country\"})\n    # Merge\n    df_country = pd.merge(df_raw[[\"Country\"]], countries, how=\"left\", on=\"Country\")\n    # Add feature\n    df_country[\"is_northern\"] = [1 if lat > 0 else 0 for lat in df_country.latitude]\n    # Fill missings\n    df_country.loc[df_country[\"Country\"].isin([\"USA\", \"UKA\"]), [\"latitude\"]] = 1\n    # Join\n    df = df.join(df_country[[\"is_northern\"]])\n    \n    # New numerical feature\n    df[\"clo_met_ratio\"] = df.Clo \/ df.Met\n    \n    # Group features\n    mean_air_temp = pd.DataFrame(df_raw.groupby([\"City\",\"Season\"])[\"Air temperature (C)\"].mean()).reset_index().rename(columns={\"Air temperature (C)\":\"mean_air_temp\"})\n    mean_rel_humidity = pd.DataFrame(df_raw.groupby([\"City\",\"Season\"])[\"Relative humidity (%)\"].mean()).reset_index().rename(columns={\"Relative humidity (%)\":\"mean_rel_humidity\"})\n    means_df = pd.merge(df_raw, mean_air_temp, how=\"left\", on=[\"City\",\"Season\"]).merge(mean_rel_humidity, how=\"left\", on=[\"City\",\"Season\"])[[\"mean_air_temp\", \"mean_rel_humidity\"]]\n    means_df.fillna(means_df.mean(), inplace=True)\n    df = df.join(means_df)\n    \n    return df","c56632c7":"cols = [\"Clo\",               \n        \"Met\",               \n        \"Air temperature (C)\",\n        \"Relative humidity (%)\",\n        \"Air velocity (m\/s)\",\n        \"Outdoor monthly air temperature (C)\",\n        \"clo_met_ratio\",\n        \"mean_air_temp\",\n        \"mean_rel_humidity\"]\n\ndef pca_and_clustering(df, cols):\n    \"\"\"\n    Applies PCA and clustering to add features to data set.\n    \n    df: dataset\n    cols: list of continuous columns\n    \"\"\"\n    # Import packages\n    from sklearn.preprocessing import MinMaxScaler\n    from sklearn.decomposition import PCA\n    from sklearn.cluster import KMeans\n    \n    # Scale data\n    # Select the columns to scale\n    X = df[cols].copy()\n    # Create the scaler\n    scaler = MinMaxScaler()\n    # Fit your data (returns an array, we are creating a dataframe here)\n    X_scaled = pd.DataFrame(scaler.fit_transform(X))\n    # Add the columns names (where removed in previous step)\n    X_scaled.columns = X.columns\n\n    #\u00a0PCA\n    # Create principal components object\n    pca = PCA(random_state=55)\n    # Fit your data\n    X_pca = pca.fit_transform(X_scaled)\n    # Create PCA dataframe\n    component_names = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n    X_pca = pd.DataFrame(X_pca,columns=component_names)\n    X_pca.index = df.index\n    # Add them to the data set\n    df[[f\"PC{i+1}\" for i in range(X_pca.shape[1])]] = X_pca\n    # Drop\n    df.drop(cols, axis=1, inplace=True)\n\n    #\u00a0Clustering\n    # Create cluster objet\n    kmeans = KMeans(n_clusters=2, random_state=55)\n    # Fit your data\n    X_scaled[\"Cluster\"] = kmeans.fit_predict(X_scaled)\n    X_scaled.index = df.index\n    # Add column with the clusters found\n    df[\"Cluster\"] = X_scaled[\"Cluster\"].astype(\"int\")\n    \n    return df","0de17d85":"target = \"Thermal sensation acceptability\"\n\n# Load original data\ndf_raw = pd.read_csv(\"\/kaggle\/input\/ashrae-global-thermal-comfort-database-ii\/ashrae_db2.01.csv\", low_memory=False)\n# Select features\ndf = select_features(df_raw, target, 25)\n# Handle categorical features\ndf = handle_categorical_features(df)\n#get indexes from balanced dataset, this is to use always the same rows to predict\nidx = get_balance_dataset_index(df, target)\n# Handle numerical features\ndf = handle_numerical_features(df, df_raw)\n# Add simple features\ndf = simple_fe(df, df_raw)\n# Clustering and pca\ndf = pca_and_clustering(df, cols)","70b2fa2f":"df.head()","afb8f1b4":"df.info()","c78e50db":"from sklearn.model_selection import train_test_split","9bab06d5":"dfb = df.loc[idx]","28f04c88":"X = dfb.drop(\"Thermal sensation acceptability\", axis=1)\ny = dfb[\"Thermal sensation acceptability\"]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=55)","4a87f0c4":"from category_encoders import MEstimateEncoder","ac77f9eb":"df_raw.info()","aacce158":"X_train.loc[:,\"City\"] = df_raw.loc[X_train.index,\"City\"]\nX_train.loc[:,\"City\"] = X_train[\"City\"].fillna(\"Kota Kinabalu\")\nX_train.loc[:,\"Climate\"] = df_raw.loc[X_train.index,\"Climate\"]\n\nX_test.loc[:,\"City\"] = df_raw.loc[X_test.index,\"City\"]\nX_test.loc[:,\"City\"] = X_test[\"City\"].fillna(\"Kota Kinabalu\")\nX_test.loc[:,\"Climate\"] = df_raw.loc[X_test.index,\"Climate\"]","74177e3b":"# Create the encoder instance. Choose m to control noise.\nencoder = MEstimateEncoder(cols=[\"City\", \"Climate\"], m=5.0)","d2785af5":"# Fit the encoder on the encoding split.\nencoder.fit(X_train, y_train)\n\n# Encode the column to create the final training data\nX_train = encoder.transform(X_train)\nX_test = encoder.transform(X_test)","b423e7d0":"X_train.head()","f0996b2c":"from xgboost import XGBClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split, cross_val_score","a81fd2a0":"# Define the model\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, \n                         objective=\"binary:logistic\", eval_metric=\"auc\", use_label_encoder=False)\n\n# Perform cross validation with 5 folds\nprint(\"Training...\")\nscores = cross_val_score(my_model, \n                          X_train, y_train,\n                          cv=5,\n                          scoring='roc_auc')\nprint(\"...done.\")","1d7cfed1":"print(f\"Scores: {scores}\")\nprint(f\"Mean scores: {np.mean(scores)}\")","ce4df68e":"# This is the model we are going to train (a simple one)\nmy_model = XGBClassifier(n_estimators=50, learning_rate=0.1, n_jobs=4, random_state=55, objective=\"binary:logistic\")\n\n# Train\nprint(\"Training...\")\nmy_model.fit(X_train, y_train, verbose=False)\nprint(\"...done\")","d563231a":"# And we predict\nprediction = pd.DataFrame({\"y_pred\": my_model.predict(X_test), \"y_real\": y_test})\nroc_auc_score(prediction.y_real, prediction.y_pred)","44ef16b3":"print(classification_report(prediction.y_real, prediction.y_pred))","8a476bf5":"# Split data\nFor target encoding the value has to be calculated with the train data only (to avoid overfitting). Let's balance and split it the same way we were doing for the models:","bfa3cd5f":"# Load data\nThe functions used here were defined in previous notebooks, they replicate the process followed there to use always the same dataset, you can ignore them.","52134b31":"All hope is lost :(","6be77f40":"Our dataset is already encoded, but we can get from the raw data the features we want. We will be using the city and the climate; the last one is a feature we excluded at the beginning and now is coming back to be target-encoded ;) You could apply the target encoding to any of the categorical features in our data set, feel free to try it out for other columns.","c76b01f0":"...maybe there is some hope here? to beat that nasty first model?","a1adbe89":"Let's add it to our data set. Remember we have the `idx` variable with the rows selected for the model.","0978c308":"# Final considerations on feature engineering and training models\nJokes aside, this could happens: you add features to your data, you fill the missings in the best way possible and the performance keeps stuck there, in the same number. In general, at some point is really difficult to get better and the increase in performance are at the very last decimals.\n\nBut in our case, we tried to focus on the techniques and not on the performance so there is a lot of stuff we didn't explore. Here are some thing you could try it out, and we really encourage you to do it:\n\n- Her we only applied PCA and K-means, but there are a lot of other options for dimensionality reduction and clustering. [SciKitLearn package offers some dimensionaliy reduction methods](https:\/\/scikit-learn.org\/stable\/modules\/unsupervised_reduction.html) and a lot of [clustering](https:\/\/scikit-learn.org\/stable\/modules\/clustering.html).\n- We didn't cover the parameter optimization, which is a huuuge part of machine learning. When you are trying to improve a model is, maybe, one of the most important parts.\n- Why don't try out another predictive model? Random Forest? A neural network?\n- This is a more advanced technique but you could try it. Is common to make a prediction with a weaker model (a decision tree, a logistic regression) and use that prediction as a feature.\n- Remember that developing a model is an iterative process, you have to try the model, evaluate it and go back to make changes. During this process is usefull to inspect the feature importance of your model: you can try models with only a few features (the more important ones) an optimize that, which will be faster.","2158e5f2":"# Introduction\n\nThis is a serie of notebooks thar should be visited in order, they are all linked in the table of content. In this notebook we are going to create new features with target encoding and run a classification model.\n\n### Content table\n- [Preprocessing pt. 1: data transformation & EDA](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-1)\n- [Preprocessing pt. 2: encoding categorical variables](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-2)\n- [Preprocessing pt. 3: handling missing values](https:\/\/www.kaggle.com\/ponybiam\/classification-preprocessing-pt-3) \n- [Feature engineering pt. 1: simple features](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-1)\n- [Feature engineering pt. 2: clustering & PCA](https:\/\/www.kaggle.com\/ponybiam\/classification-feature-engineering-pt-2)\n- **Feature engineering pt. 3: target encoding** (you are here)\n    - [Load data](#Load-data)\n    - [Target encoding](#Target-encoding)\n    - [Model](#Model)\n        - [Balance dataset](#Balance-dataset)\n        - [Split and cross validation](#Split-and-cross-validation)\n        - [Train and predict](#Train-and-predict)","d998958a":"# Notebook goal\nThe goal of this notebook is to create new features with target encoding. At the end, we will train a predictive model. Here will be shown only one of the many possibilities in machine learning, feel free to experiment and play around with the data on your own.  We will be focusing on the techniques and not on the model performance.","e06bfffd":"# Model\nNow is time to train the model. We are going to follow the remaining steps:\n\n3. 5-folds cross validation training\n4. Training with whole training data set\n5. Prediction of test data set","ccbe9ab6":"## Train and predict\nLet's see what happens when we train with the whole training data set.","d6f72819":"And we transform our data:","cb1b9706":"# Target encoding\nA target encoding is used to encode categorical data; the difference with the encoder we previosuly used is that the encoding is performed with data from the target (that's why we have to split the data in train and validation at the beginning). It can be applied to categorical or continous targets.\n\nAs explained in this [target encoding class](https:\/\/www.kaggle.com\/ryanholbrook\/target-encoding), rare or less frequent categories can be a problem. That's why an smoothing is added to this method, to assing a smaller weight to rare categories.\n\nIn our data set, geographical and climate features may holds important information for a predictive model. We are going to encode the city and de climate, replacing each id number with the -smoothed- mean class of that building. We are going to use [M-estimate](https:\/\/contrib.scikit-learn.org\/category_encoders\/mestimate.html) from [category encoders](https:\/\/contrib.scikit-learn.org\/category_encoders\/) package. Check out the docs to ge familiar with this library.","289606e4":"This is what we got:","4030af60":"We already split our data (and added the new features), we cross-validate with the training data:","4111bbb6":"Now that our train and test data have the columns we want to encode we create the encoder.","ed68e00b":"| Notebook           | Categorical features | Missing values in categorical | Missing values in numerical | Feature engineering                    | ROC-AUC score |\n|--------------------|----------------------|-------------------------------|-----------------------------|----------------------------------------|---------------|\n| Preprocessing pt.2 | One Hot Encoding     | Mode input                    | 0 input                     | -                                      |0.6591         |\n| Preprocessing pt.3 | One Hot Encoding     | Mode Input                    | Mean input                  | -                                      |0.6466         |\n| FE pt.1            | One Hot Encoding     | Mode Input                    | Mean input                  | Simple                                 |0.6507         |\n| FE pt.2            | One Hot Encoding     | Mode Input                    | Mean input                  | Simple + PCA & Clustering              |0.6481         |\n| FE pt.3            | One Hot Encoding     | Mode Input                    | Mean input                  | Simple + PCA & Clustering + Target encoding              |0.6486         |","09a576ea":"Lets select the indexes from the balanced data:"}}