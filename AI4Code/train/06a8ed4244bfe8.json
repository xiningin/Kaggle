{"cell_type":{"6c32e316":"code","eeebc3a2":"code","f0eda356":"code","5448c715":"code","d20144c9":"code","f178978d":"code","de3bc0fa":"code","de6fa021":"code","0ae92690":"code","8dafa852":"code","883df3e7":"code","35ddf387":"code","c0881bdf":"markdown","8c41d880":"markdown"},"source":{"6c32e316":"import os\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nimport numpy as np\nimport re\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","eeebc3a2":"df = pd.read_csv(\"..\/input\/isekai-light-novel-titles-and-descriptions\/light-novel-titles.csv\")\ndf[\"desc_word_count\"] = df.descriptions.apply(lambda x: len(str(x).split()))\ndf.head()","f0eda356":"text_df = df.descriptions[df[\"desc_word_count\"] > 10] # get rid of short titles\ntext = text_df.to_numpy() ","5448c715":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nvocab_size = 100000\ntokenizer = Tokenizer(num_words=vocab_size)\ntokenizer.fit_on_texts(text)","d20144c9":"total_words = len(tokenizer.word_index) + 1","f178978d":"input_sequences = []\nfor title in text:\n    token_list = tokenizer.texts_to_sequences([title])[0]\n    for i in range(1, len(token_list)):\n        n_gram_sequence = token_list[:i+1]\n        input_sequences.append(n_gram_sequence)\n\n# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\nxs, labels = input_sequences[:,:-1],input_sequences[:,-1]\n\nys = tf.keras.utils.to_categorical(labels, num_classes=total_words)","de3bc0fa":"model = keras.Sequential()\nmodel.add(keras.layers.Embedding(total_words, 95, input_length=max_sequence_len-1))\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(128)))\nmodel.add(keras.layers.Dense(total_words, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=['accuracy'])\nhistory = model.fit(xs, ys, epochs=45, verbose=1)","de6fa021":"def generate_text(seed_text, next_words=10):\n    for _ in range(next_words):\n        token_list = tokenizer.texts_to_sequences([seed_text])[0]\n        token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n        #predicted = model.predict_classes(token_list, verbose=0)\n        predicted = np.argmax(model.predict(token_list), axis=1)\n        output_word = \"\"\n        for word, index in tokenizer.word_index.items():\n            if index == predicted:\n                output_word = word\n                break\n        seed_text += \" \" + output_word\n    return(seed_text)","0ae92690":"generate_text(\"the incompetent hero\", 30)","8dafa852":"generate_text(\"i\", 25)","883df3e7":"generate_text(\"living a slow life in\", 30)","35ddf387":"generate_text(\"living\", 30)","c0881bdf":"Code modified from [here](https:\/\/colab.research.google.com\/github\/lmoroney\/dlaicourse\/blob\/master\/TensorFlow%20In%20Practice\/Course%203%20-%20NLP\/Course%203%20-%20Week%204%20-%20Lesson%202%20-%20Notebook.ipynb) and [here](https:\/\/github.com\/somvirs57\/text_generation_tensorflow\/blob\/master\/poem_text_generator_py.py)","8c41d880":"For data analysis see [analysis-on-data](https:\/\/www.kaggle.com\/andy8744\/analysis-on-data)"}}