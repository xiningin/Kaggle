{"cell_type":{"4e1c1e23":"code","58e1c2c7":"code","0d5e8310":"code","0971e5c1":"code","263341dd":"code","6a4a8f84":"code","51ba5440":"code","1769ee04":"code","5e7746b5":"code","292892f4":"code","a08e057f":"code","6f2d70cf":"code","803b725d":"code","07723145":"code","6550c575":"code","bb970ab3":"code","e1d488b8":"code","87484d11":"code","01d40895":"code","636f153f":"code","cf1eee20":"code","ac3d1124":"code","7b75caed":"code","9a8d75d7":"code","4035b98a":"code","3a4e9330":"code","55ee0d9e":"code","11983bd9":"code","f8408915":"code","554bd7d5":"code","a7592b6b":"code","efc14051":"code","a342a344":"code","62a230d7":"code","42f9e00a":"code","0158b112":"code","d0a7012f":"code","6cc95f05":"code","69430184":"code","244bd99a":"code","3c3595a1":"code","748a3499":"code","28273bc1":"code","18e8a2de":"code","7a8e0c45":"code","d4cb16cd":"code","fd9dfdff":"code","e9e8ced0":"markdown","02be36fc":"markdown","bf093790":"markdown","bbc89fd6":"markdown","08200c98":"markdown","a9b85b9d":"markdown","e25cf19c":"markdown","4087be85":"markdown","a8499732":"markdown"},"source":{"4e1c1e23":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV,RandomizedSearchCV\nfrom sklearn.linear_model import Ridge,LinearRegression,Lasso\nfrom sklearn import svm,preprocessing\nfrom sklearn.ensemble import RandomForestRegressor\nimport xgboost\nfrom sklearn.feature_selection import SelectFromModel","58e1c2c7":"train=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\npd.pandas.set_option(\"display.max_columns\",None)\ntrain.shape\ntest.shape\n#test dataset doesnt contain SalePrice column","0d5e8310":"train.isnull().any().sum()","0971e5c1":"#digging out the features with nan values and percentage of nan values in train and test dataset\ntrain.isnull().any().sum()\nfeatures_nan=[features for features in train.columns if train[features].isnull().sum()>0]\nfeatures_nan_test=[features for features in test.columns if test[features].isnull().sum()>0]\nprint(features_nan)\nfor features in features_nan:\n    print(features+\" \"+str(np.round(train[features].isnull().mean(),4))+\"% of nan values\")\nprint(\"\\n\\n\")\nfor features in features_nan_test:\n    print(features+\" \"+str(np.round(test[features].isnull().mean(),4))+\"% of nan values\")","263341dd":"for feature in features_nan:\n    data = train.copy()\n    \n    # let's make a variable that indicates 1 if the observation was missing or zero otherwise\n    data[feature] = np.where(data[feature].isnull(), 1, 0)\n    \n    # let's calculate the mean SalePrice where the information is missing or present\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","6a4a8f84":"#digging out numerical features\nfeatures_num=[feature for feature in train.columns if train[feature].dtype!='O']\nfeatures_num_test=[feature for feature in test.columns if test[feature].dtype!='O']\nprint(len(features_num),len(features_num_test))\ntrain[features_num]","51ba5440":"#digging out temporal features\n#features based on time\nfeatures_temp=[feature for feature in train.columns if 'Yr' in feature or 'Year' in feature]\nprint(features_temp)","1769ee04":"for feature in features_temp:\n    train.groupby(feature)['SalePrice'].median().plot()\n    plt.xlabel(feature)\n    plt.ylabel('Median House Price')\n    plt.show()","5e7746b5":"#we are comparing relation between all year features and saleprice\nfor feature in features_temp:\n    if feature !='YrSold':\n        data=train.copy()\n        data[feature]=data['YrSold']-data[feature]\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('Saleprice')\n        plt.show()","292892f4":"#numerical feature is of two types, discrete and continuous\n\n#digging discrete features\n\ndis_features=[feature for feature in features_num if len(train[feature].unique())<25 and feature not in features_temp+['Id']]\ndis_features_test=[feature for feature in features_num_test if len(test[feature].unique())<25 and feature not in features_temp+['Id']]\nprint(len(dis_features),len(dis_features_test))","a08e057f":"for feature in dis_features:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.show()","6f2d70cf":"#digging out continuous numerical features\ncon_features=[feature for feature in features_num if feature not in dis_features+features_temp+['Id']]\ncon_features_test=[feature for feature in features_num_test if feature not in dis_features_test+features_temp+['Id']]\ncon_features","803b725d":"for feature in con_features:\n    data=train.copy()\n    plt.scatter(data[feature],data['SalePrice'])\n    plt.xlabel(feature)\n    plt.ylabel('SalesPrice')\n    plt.title(feature)\n    plt.show()","07723145":"for feature in con_features:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data['SalePrice']=np.log(data['SalePrice'])\n        plt.scatter(data[feature],data['SalePrice'])\n        plt.xlabel(feature)\n        plt.ylabel('SalesPrice')\n        plt.title(feature)\n        plt.show()","6550c575":"for feature in con_features:\n    data=train.copy()\n    if 0 in data[feature].unique():\n        pass\n    else:\n        data[feature]=np.log(data[feature])\n        data.boxplot(column=feature)\n        plt.ylabel(feature)\n        plt.title(feature)\n        plt.show()\n","bb970ab3":"cat_features=[feature for feature in train.columns if train[feature].dtypes=='O']\ncat_features_test=[feature for feature in test.columns if test[feature].dtypes=='O']\nlen(cat_features)-len(cat_features_test)","e1d488b8":"for feature in cat_features:\n    data=train.copy()\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.xlabel(feature)\n    plt.ylabel('SalePrice')\n    plt.title(feature)\n    plt.show()\n","87484d11":"#missing values of cat_features\nfeatures_nan_cat=[feature for feature in train.columns if train[feature].isnull().sum()>0 and train[feature].dtype=='O']\n\nfor feature in features_nan_cat:\n    print(\"{}: {}% missing values\".format(feature,np.round(train[feature].isnull().mean(),4)))","01d40895":"def replace_cat_feature(train,features_nan):\n    data=train.copy()\n    data[features_nan]=data[features_nan].fillna(\"Missing\")\n    return data\ntrain=replace_cat_feature(train,features_nan_cat)\ntest=replace_cat_feature(test,features_nan_cat)\ntest[features_nan_cat]","636f153f":"## Now lets check for numerical variables the contains missing values\nnumerical_with_nan=[feature for feature in train.columns if train[feature].isnull().sum()>0 and train[feature].dtype!='O']\nnumerical_with_nan_test=[feature for feature in test.columns if test[feature].isnull().sum()>0 and test[feature].dtype!='O']\n## We will print the numerical nan variables and percentage of missing values\n\nfor feature in numerical_with_nan:\n    print(\"{}: {}% missing value\".format(feature,np.around(train[feature].isnull().mean(),4)))","cf1eee20":"#filling nan with median of that column as there are ouliers in continues features\nfor feature in numerical_with_nan:\n    med=train[feature].median()\n    #train[feature+'nan']=np.where(train[feature].isnull(),1,0)\n    train[feature]=train[feature].fillna(med)\nfor feature in numerical_with_nan_test:\n    med=test[feature].median()\n    #test[feature+'nan']=np.where(test[feature].isnull(),1,0)\n    test[feature]=test[feature].fillna(med)\ntrain[numerical_with_nan].isnull().sum()","ac3d1124":"## Temporal Variables (Date Time Variables)\n\nfor feature in ['YearBuilt','YearRemodAdd','GarageYrBlt']:\n       \n    train[feature]=train['YrSold']-train[feature]\n    test[feature]=test['YrSold']-test[feature]\ntrain[features_temp]","7b75caed":"#normalizaing continuous features\nfor feature in con_features:\n    train[feature]=np.log(train[feature]+1)\nfor feature in con_features_test:\n    test[feature]=np.log(test[feature]+1)\ntrain[con_features]","9a8d75d7":"def handle_non_numerical_data(train,cat_features):\n    for column in cat_features:\n        target_values={}\n        def convert_to_int(val):\n            return target_values[val]\n  \n        x=2\n        unique_values=train[column].unique()\n        for unique in unique_values:\n            if unique not in target_values.keys():\n                target_values[unique]=x\n                x+=1\n        train[column]=list(map(convert_to_int,train[column]))\n    return train\ntrain=handle_non_numerical_data(train,cat_features)\ntest=handle_non_numerical_data(test,cat_features_test)\ntest[cat_features]","4035b98a":"for feature in cat_features:\n    data=train.copy()\n    data[feature].hist(bins=5)\n    plt.title(feature)\n    plt.show()","3a4e9330":"# since the features or not normally distributed we will use log normal distribution\nfor feature in cat_features:\n    train[feature]=np.log(train[feature])\n    test[feature]=np.log(test[feature])\ntest[cat_features]","55ee0d9e":"colnames=[features for features in train.columns if features not in ['SalePrice','Id']]\nlen(colnames)","11983bd9":"\ny_train=train['SalePrice']\nX_train=train.drop(['SalePrice','Id'],axis=1)\nX_test=test.drop(['Id'],axis=1)\nX_train=preprocessing.scale(X_train)\nX_test=preprocessing.scale(X_test)","f8408915":"X_train=pd.DataFrame(X_train)\nX_train.columns=colnames\nX_train","554bd7d5":"X_test=pd.DataFrame(X_test)\nX_test.columns=colnames\nfeatures_na=[feature for feature in X_test.columns if X_test[feature].isnull().sum()>0]\nfeatures_na","a7592b6b":"### Apply Feature Selection\n# first, I specify the Lasso Regression model, and I\n# select a suitable alpha (equivalent of penalty).\n# The bigger the alpha the less features that will be selected.\n\n# Then I use the selectFromModel object from sklearn, which\n# will select the features which coefficients are non-zero\n\nfeature_sel_model = SelectFromModel(Lasso(alpha=0.005, random_state=0)) # remember to set the seed, the random state in this function\nfeature_sel_model.fit(X_train, y_train)","efc14051":"feature_sel_model.get_support()","a342a344":"\n# this is how we can make a list of the selected features\nselected_feat = X_train.columns[feature_sel_model.get_support()]\n# let's print some stats\nprint('total features: {}'.format((X_train.shape[1])))\nprint('selected features: {}'.format(len(selected_feat)))","62a230d7":"X_train[selected_feat]\nX_test[selected_feat]","42f9e00a":"X_train,X_eval,y_train,y_eval=train_test_split(X_train,y_train,test_size=0.2)","0158b112":"clf=LinearRegression()\nclf.fit(X_train,y_train)\naccuracy=clf.score(X_eval,y_eval)\nprint(accuracy)\npredictions=clf.predict(X_test)\npredictions=np.exp(predictions)\nprint(list(predictions))\nplt.hist(predictions)\nplt.show()","d0a7012f":"ridge_params={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\nclf=Ridge()\nridge_tun=GridSearchCV(clf,ridge_params,scoring='neg_mean_squared_error',cv=5)\nridge_tun.fit(X_train,y_train)\nprint(ridge_tun.best_params_)\nprint(ridge_tun.best_score_)","6cc95f05":"clf=Ridge(alpha=100)\nclf.fit(X_train,y_train)\naccuracy=clf.score(X_eval,y_eval)\nprint(accuracy)\npredictions=clf.predict(X_test)\npredictions=np.exp(predictions)\nprint(list(predictions))\nplt.hist(predictions)\nplt.show()","69430184":"svm_params={'C':[1.0,2.0,3.0,5.0,10.0,7.0],\n        'epsilon':[0.1,0.2,0.2,0.25,0.15],\n        'cache_size':[200,300,250,350,150]\n       }\nclf=svm.SVR()\nsvm_tun=RandomizedSearchCV(clf,param_distributions=svm_params,scoring='neg_mean_squared_error',n_jobs=-1,cv=5)\nsvm_tun.fit(X_train,y_train)\nprint(svm_tun.best_params_)\nprint(svm_tun.best_score_)","244bd99a":"clf=svm.SVR(epsilon= 0.1, cache_size= 300, C= 5.0)\nclf.fit(X_train,y_train)\naccuracy=clf.score(X_eval,y_eval)\nprint(accuracy)\npredictions=clf.predict(X_test)\npredictions=np.exp(predictions)\nprint(list(predictions))\nplt.hist(predictions)\nplt.show()","3c3595a1":"clf=Lasso()\nparameters={'alpha':[1e-15,1e-10,1e-8,1e-3,1e-2,1,5,10,20,30,35,40,45,50,55,100]}\nlasso_tun=GridSearchCV(clf,parameters,scoring='neg_mean_squared_error',cv=5)\n\nlasso_tun.fit(X_train,y_train)\nprint(lasso_tun.best_params_)\nprint(lasso_tun.best_score_)","748a3499":"clf=Lasso(alpha=0.01)\nclf.fit(X_train,y_train)\naccuracy=clf.score(X_eval,y_eval)\nprint(accuracy)\npredictions=clf.predict(X_test)\npredictions=np.exp(predictions)\nprint(list(predictions))\nplt.hist(predictions)\nplt.show()","28273bc1":"xgb_params={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","18e8a2de":"clf=xgboost.XGBRegressor()\nxgb_tun=RandomizedSearchCV(clf,param_distributions=xgb_params,n_iter=5,scoring='neg_mean_squared_error',n_jobs=-1,cv=5,verbose=3)\nxgb_tun.fit(X_train,y_train)\nprint(xgb_tun.best_params_)\nprint(xgb_tun.best_score_)","7a8e0c45":"clf=xgboost.XGBRegressor(min_child_weight=7, max_depth= 5, learning_rate= 0.1, gamma =0.1, colsample_bytree= 0.5)\nclf.fit(X_train,y_train)\naccuracy=clf.score(X_eval,y_eval)\nprint(accuracy)\npredictions=clf.predict(X_test)\npredictions=np.exp(predictions)\nprint(list(predictions))\nplt.hist(predictions)\nplt.show()","d4cb16cd":"final_preds=pd.DataFrame(predictions)\nfinal_preds.index=range(1461,1461+1459)\nfinal_preds.columns=['SalePrice']","fd9dfdff":"final_preds","e9e8ced0":"**EDA**","02be36fc":"**FEATURE ENGINEERING**","bf093790":"**Linear Regression**","bbc89fd6":"**XGBoost Regression**","08200c98":"We will explore every feature with nan values and determine its nan values weightage on SalePrice","a9b85b9d":"**FEATURE SCALING**","e25cf19c":"**Ridge Regression**","4087be85":"**SVM**","a8499732":"**Lasso Regression**"}}