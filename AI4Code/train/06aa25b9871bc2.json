{"cell_type":{"468a4945":"code","daddf403":"code","4fcaa01e":"code","8f7c1314":"code","cf028804":"code","c94c3528":"code","f5a457fb":"code","32fc4857":"code","d20a0f09":"code","ceabc8ab":"code","84626688":"code","6a563c04":"code","45b515f5":"code","d5c85076":"code","dc874cfb":"code","b640b8e0":"code","90bd0d14":"code","5f9cfa3c":"code","b7de493e":"code","10dab9c3":"code","e7c98a7f":"code","2745b075":"code","01132e45":"code","b0952bff":"code","eb48f32e":"code","121bed61":"code","eda1a3b5":"code","d36145fa":"code","8bca95c3":"code","d5654207":"code","21f43738":"code","c6bc59c1":"code","c36b0988":"code","721d765a":"code","cc82ff2c":"code","ce445e9d":"code","2e299e79":"code","004eb9aa":"code","9232c4de":"code","e4fca094":"code","d8c3f12f":"code","12bc4aba":"code","8d07c2b4":"code","0aee1d3b":"code","bd217327":"code","9e60bb94":"code","c4b5c78c":"code","a1856235":"code","6ea74829":"code","937e725e":"code","c685e4d3":"code","1f5aae95":"code","659c1f26":"code","49e1b51b":"code","4fc52cc2":"code","5ccb9a9c":"code","400021ad":"code","cd46e487":"code","97bc0e2e":"code","86f5044b":"code","de2af0b3":"code","47d181cf":"code","aba80bdd":"code","b5f7fc84":"code","dadda1f5":"code","ff55891c":"code","b4a30ed9":"code","dfe21950":"code","0430dfb3":"code","2cc01ff6":"code","62b83f92":"code","d10707cc":"code","1b17fe60":"code","0028c125":"code","53d842f7":"code","b64a4bdb":"code","7db998c5":"code","cfc58a26":"code","f9ce535f":"code","11497ef6":"code","0fa39317":"code","291373e9":"code","aedbf597":"code","4e5d5a21":"code","6b74d418":"code","a0f86af9":"code","6304b5f3":"markdown","cfb49162":"markdown","e93fe29e":"markdown","f13c44cd":"markdown","d48d5cc8":"markdown","787d2e04":"markdown","420bd9c1":"markdown","f7e00f00":"markdown","be61f095":"markdown","6863c2cd":"markdown","3343e0a6":"markdown","d2af2679":"markdown","989b7f79":"markdown","9a66dc0c":"markdown","74b69b3c":"markdown","1b398f06":"markdown","430a381c":"markdown","dcbed5a4":"markdown","41354cd1":"markdown","c984c6cc":"markdown","6358a799":"markdown","76c74bcb":"markdown","4d184c14":"markdown","4fa8394c":"markdown","c4d8532a":"markdown","ed41a915":"markdown","d55e6e01":"markdown","4fae2275":"markdown","aff44c5d":"markdown","ed39fb3c":"markdown","235f6dc8":"markdown","3b36a19d":"markdown","8ca1f94c":"markdown","fe8e6fb0":"markdown","8dbd3345":"markdown","a37f5c03":"markdown","af85c999":"markdown","f1c78e0c":"markdown","66b66e73":"markdown","dcbf2bda":"markdown","44d5ec75":"markdown","942eadff":"markdown","69d79751":"markdown","109b197c":"markdown","942dcf4a":"markdown","344c47bf":"markdown","78b9a807":"markdown","f8970eb9":"markdown","8491c513":"markdown","2fecd69e":"markdown","6d59937b":"markdown","4910757e":"markdown","4db31201":"markdown","79acc3bb":"markdown","b135cd48":"markdown","10eb5172":"markdown","5994ce2e":"markdown","fdc3c20e":"markdown","af68f066":"markdown","b89f9039":"markdown","38d4ce88":"markdown","18001b0f":"markdown","b3187b37":"markdown","2a150019":"markdown","7873213d":"markdown","ebcba0a8":"markdown","b7423bb3":"markdown","25b3a69b":"markdown","98517da2":"markdown","1fdaa717":"markdown","81575518":"markdown","05db4ec4":"markdown","606f1f2b":"markdown","221384ed":"markdown","5a9a99c9":"markdown","b20da277":"markdown","f530f3e7":"markdown","3dda7bef":"markdown","c3ee9137":"markdown","7f0cae53":"markdown","c387b3c6":"markdown","61f846e6":"markdown","a1c631cf":"markdown","1e67c42b":"markdown","ce058116":"markdown","ce35c5eb":"markdown","d6cdc377":"markdown","5551255b":"markdown","3fe551a5":"markdown","1c3f8731":"markdown","0cadc69c":"markdown","1bf340ea":"markdown","40027bda":"markdown","492244e4":"markdown","f54175bc":"markdown","d1ae3237":"markdown","dca73fdf":"markdown","2ded1f3c":"markdown","906d8ce5":"markdown","68dea194":"markdown","fa9862cf":"markdown","b86a7263":"markdown","15aa98d1":"markdown","0a9bf37d":"markdown","38e7cebb":"markdown","ba510b17":"markdown","9624fc0e":"markdown","4c7374e9":"markdown","b64311cd":"markdown","bf6ec7c1":"markdown","7b50a711":"markdown","9c14200f":"markdown","3f2a417f":"markdown","95bd4af3":"markdown","dcae46dc":"markdown","42e4379f":"markdown","0dafeba3":"markdown","8fcf56f7":"markdown","f12f2367":"markdown","c86c0748":"markdown"},"source":{"468a4945":"# Importing libraries we will use.\nimport warnings\nimport numpy as np\nimport pandas as pd\n\nimport scipy.stats as stat\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\n\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport scipy.stats as sci\n\nfrom statsmodels.graphics.mosaicplot import mosaic\nfrom statsmodels.graphics.gofplots import ProbPlot\nimport matplotlib.pyplot as plt\nimport matplotlib.ticker as tck\nimport seaborn as sns\n\n# Limiting floats output to 2 decimal points and adding the thousands operator that places a comma between all thousands\npd.set_option('display.float_format', lambda x: '{:,.2f}'.format(x))\n\n# Ignoring warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\n# set style and color palette we will use for Seaborn visualization package\nsns.set(style=\"ticks\", palette=\"bright\")\nfrom IPython.display import Image\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","daddf403":"labelled =  pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv') #labelled data is the provided training data.\nunlabelled = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv') #unlabelled data is the test data.","4fcaa01e":"#Concate both sets for the sake of data exploration and wrangling\ndata = pd.concat([labelled, unlabelled], axis = 0, ignore_index = True)","8f7c1314":"data.shape","cf028804":"categorical = data.dtypes[data.dtypes == 'object'].index.tolist()\nnumerical = data.dtypes[data.dtypes != 'object'].index.drop(['Id','SalePrice']).tolist()","c94c3528":"print('Categorical variables are:\\n\\n', categorical,'\\n\\nNumerical variables are:\\n\\n',numerical)","f5a457fb":"print('No. of categorical variables:', len(categorical), ' ,  No. of numerical variables:', len(numerical))","32fc4857":"data[categorical].describe().transpose()","d20a0f09":"data[numerical].describe().transpose()","ceabc8ab":"data_org = data.copy()","84626688":"#Function for formating y-label to $(amount)k or $(amount)M\ndef currency(x,pos):\n    if x >= 10**6 :\n        return '${:1.2f}M'.format(x*1e-6)\n    return '${:1.0f}k'.format(x*1e-3)","6a563c04":"f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, \n                                    figsize=(10,7), gridspec_kw={\"height_ratios\": (.15, .85)})\nplt.suptitle('Distribution plot for \"SalePrice\"', fontsize = 20)\nformater = tck.FuncFormatter(currency)\nax_hist.xaxis.set_major_formatter(formater)\nsns.boxplot(data['SalePrice'].dropna(), ax= ax_box)\nsns.distplot(data['SalePrice'].dropna(), ax= ax_hist)\nax_box.set(xlabel='')","45b515f5":"print('\"SalePrice\" kurosis = ', np.round(data['SalePrice'].kurtosis(),2),\n     '\\n\"SalePrice\" skewness = ', np.round(data['SalePrice'].skew(),2)), \ndata['SalePrice'].describe()","d5c85076":"#List of varaibles having 'int64' type and we want to  make distplots for them rather than countplots.\nint_as_float = ['LotArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', \n        '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n        'EnclosedPorch', '3SsnPorch', 'ScreenPorch','PoolArea', 'MiscVal',\"YearBuilt\", \"YearRemodAdd\"]","dc874cfb":"allcolumns = np.array(data.columns.drop(['Id','SalePrice']).tolist()+[None]).reshape(20, 4)\nfig, ax = plt.subplots(figsize=(15, 70), ncols=4, nrows=20)\nfig.suptitle('All Features Distribution plots', fontsize=20)\nplt.subplots_adjust(\n    left=0.125,\n    right=0.9,\n    bottom=0.1,\n    top=0.97,\n    wspace=0.275,\n    hspace= 0.8)\n\nprob_formater = tck.FuncFormatter(\n    lambda x, p: np.format_float_scientific(x, precision=0, exp_digits=0))\n\nfor i in range(20):\n    for j in range(4):\n        if i == 19 and j == 3:\n            continue\n        ax[i][j].set_title(allcolumns[i][j], fontsize=14)\n        for tick in ax[i][j].get_xticklabels():\n            if data[allcolumns[i][j]].nunique() >= 5 and data[allcolumns[i][j]].dtype == 'object':\n                tick.set_rotation(90)\n        if (data[allcolumns[i][j]].dtypes == 'float64') | (allcolumns[i][j] in int_as_float):\n            ax[i][j].yaxis.set_major_formatter(prob_formater)\n            sns.distplot(data[allcolumns[i][j]].dropna(), ax=ax[i, j], kde=True)\n        else:\n            sns.countplot(data[allcolumns[i][j]].fillna('NA'), ax=ax[i, j])","b640b8e0":"fig, ax = plt.subplots(figsize=(15, 70), ncols=4, nrows=20)\nfig.suptitle('All Features Distribution plots', fontsize=20)\nplt.subplots_adjust(\n    left=0.125,\n    right=0.9,\n    bottom=0.1,\n    top=0.97,\n    wspace=0.275,\n    hspace= 0.8)\n\nprob_formater = tck.FuncFormatter(\n    lambda x, p: np.format_float_scientific(x, precision=0, exp_digits=0))\n\nfor i in range(20):\n    for j in range(4):\n        if i == 19 and j == 3:\n            continue\n        ax[i][j].set_title(allcolumns[i][j], fontsize=14)\n        for tick in ax[i][j].get_xticklabels():\n            if data[allcolumns[i][j]].nunique() >= 5 and data[allcolumns[i][j]].dtype == 'object':\n                tick.set_rotation(90)\n        if (data[allcolumns[i][j]].dtypes == 'float64') | (allcolumns[i][j] in int_as_float):\n            ax[i][j].yaxis.set_major_formatter(prob_formater)\n            sns.distplot(data.loc[(data[allcolumns[i][j]].isna() == False) & (data[allcolumns[i][j]] != 0),\n                                      allcolumns[i][j]], ax=ax[i, j], kde=True)\n        else:\n            sns.countplot(data[allcolumns[i][j]].fillna('NA'), ax=ax[i, j])","90bd0d14":"data.loc[data.YearBuilt == data.YearRemodAdd, 'YearRemodAdd'].count()","5f9cfa3c":"data.loc[data.YearBuilt == data.YearRemodAdd, 'YearRemodAdd'] = np.nan\nsns.distplot(data['YearRemodAdd'].dropna(), kde= True)","b7de493e":"#Plotting the two distributions on the same axis.\nsns.set(rc={'figure.figsize':(8,5)})\nsns.distplot( data['YearBuilt'].dropna() , color=\"red\", label=\"Year Built\",  kde= False, )\nsns.distplot( data['YearRemodAdd'].dropna() , color=\"blue\", label=\"Year Remodelled\", kde= False)\nplt.legend()","10dab9c3":"#The houses remodeled before 1960\ndata.loc[data.YearRemodAdd < 1960]['YearRemodAdd'].value_counts()","e7c98a7f":"#Calculating the difference between 1950 and 'YearBuilt'\ndata['difference'] = data.loc[data.YearRemodAdd == 1950, 'YearBuilt'].apply(lambda x: 1950 - x)","2745b075":"# Houses whose remodeling date will be changed\n# We want to keep record of the original data in the sack of checking our work later\nYearRem_bfr_change = data.loc[data.YearRemodAdd== 1950, [\n    'Id', 'YearRemodAdd', 'YearBuilt']].rename(columns = {'YearRemodAdd':'YearRemodAdd_bfr'})\n\nFrstOpt_YearRemodAdd = pd.DataFrame(data.YearRemodAdd)\n\n# Assigning estimated 'YearRemodAdd' to houses with YearRemodAdd == 1950 and keep the new values in a separate df\nFrstOpt_YearRemodAdd.loc[data.YearRemodAdd== 1950,'YearRemodAdd'] = data.loc[data.YearRemodAdd== 1950, [\n    'YearBuilt', 'difference']].apply(lambda x: np.ceil(x[0] + 0.5*x[1]), axis=1)\n\n#A comparison between the \"YearBuilt\" and modified \"YearRemodAdd\" to check if our constraints aren't violated\npd.concat([YearRem_bfr_change, FrstOpt_YearRemodAdd.loc[YearRem_bfr_change.index]],\n          axis=1).rename(columns={'YearRemodAdd': 'YearRemodAdd_aftr'})","01132e45":"SecndOpt_YearRemodAdd = pd.DataFrame(data.YearRemodAdd)\nSecndOpt_YearRemodAdd.loc[data.YearRemodAdd == 1950, 'YearRemodAdd'] = np.nan","b0952bff":"fig, ax = plt.subplots(ncols=2, nrows=1, figsize= (15,5.5))\nplt.subplots_adjust(top = 0.825)\n\nfig.suptitle('Comparison between the two options', fontsize=20)\nax[0].set_title('1st Option: Estimate YearRemodelled', fontsize = 14)\nax[1].set_title('2nd Option: Replace YearRemodelled = 1950 by NA', fontsize = 14)\n\nsns.distplot( data['YearBuilt'].dropna() , color=\"red\", label=\"Year Built\",  kde= False, ax= ax[0])\nsns.distplot(FrstOpt_YearRemodAdd.YearRemodAdd.dropna(), color=\"blue\", label=\"Year Remodelled\",kde= False, ax=ax[0])\nax[0].legend()\n\nsns.distplot( data['YearBuilt'].dropna() , color=\"red\", label=\"Year Built\",  kde= False, ax= ax[1] )\nsns.distplot(SecndOpt_YearRemodAdd.YearRemodAdd.dropna(), color=\"blue\", label=\"Year Remodelled\", kde= False, ax= ax[1])\nax[1].legend()\n","eb48f32e":"data.loc[data.YearRemodAdd== 1950,'YearRemodAdd'] = FrstOpt_YearRemodAdd\n#Drop the column 'difference' we created earlier\ndata.drop(columns=['difference'], inplace= True)","121bed61":"data.duplicated(subset= data.columns.drop('Id')).sum()","eda1a3b5":"Image(\"..\/input\/pic123\/Filling missing values flowchart.jpg\")","d36145fa":"# Identify the number of NAs in each feature and select only those having NAs\ntotal_NA = data.drop(columns='SalePrice').isnull().sum()[data.isnull().sum() != 0]\n\n# Calculate the percentage of NA in each feature\npercent_NA = data.drop(columns='SalePrice').isnull().sum()[data.isnull().sum() != 0]\/data.shape[0]\n\n# Summarize our findings in a dataframe\nmissing = pd.concat([total_NA, percent_NA], axis=1, keys=['Total NAs', 'Percentage']).sort_values('Total NAs', ascending=False)\nmissing","8bca95c3":"# Filling NAs for categorical features in group A.\ndata.loc[(data.Alley.isnull()) | (data.Fence.isnull()),\n         ['Alley', 'Fence']] = 'Not exist'\n\ndata.loc[data.Electrical.isnull(), 'Electrical'] = data.loc[data.CentralAir ==\n                                                            data.loc[data.Electrical.isnull(), 'CentralAir'].values[0], 'Electrical'].describe()['top']\n\ndata.loc[data.MSZoning.isnull(), 'MSZoning'] = data.loc[(data.Condition1 == data.loc[data.MSZoning.isnull(), 'Condition1'].values[0]) & (\n    data.Condition2 == data.loc[data.MSZoning.isnull(), 'Condition2'].values[0]), 'MSZoning'].describe()['top']\n\ndata.loc[data.Utilities.isnull(), 'Utilities'] = data.loc[data.Functional ==\n                                                          data.loc[data.Utilities.isnull(), 'Functional'].values[0], 'Utilities'].describe()['top']\n\ndata.loc[data.Functional.isnull(), 'Functional'] = data.loc[data.OverallCond ==\n                                                            data.loc[data.Functional.isnull(), 'OverallCond'].values[0], 'Functional'].describe()['top']","d5654207":"sns.set(rc={'figure.figsize': (3, 9)})\nsns.heatmap(data=data.corr()[['LotFrontage']].sort_values('LotFrontage', ascending=False)[\n            1:], linewidths=0.025, annot= True).set_title('Correlation Coefficient\\n vs. LotFrontage')","21f43738":"cnddt_vars = ['LotShape', 'GrLivArea', 'BldgType', 'LotArea', 'LotConfig', 'HouseStyle']\ntemp_vars = data[cnddt_vars].dropna(axis = 0)\ntemp_resp = data.iloc[temp_vars.index][['LotFrontage']]\nsubset = pd.concat([temp_vars, temp_resp], axis = 1)","c6bc59c1":"fig, ax = plt.subplots(figsize=(12, 15), ncols=2, nrows=4)\nfig.suptitle('Distribution plots for features and response variable', fontsize=20)\nplt.subplots_adjust(\n    left=0.125,\n    right=0.9,\n    bottom=0.1,\n    top=0.93,\n    wspace=0.275,\n    hspace= 0.35)\n\nallcolumns = np.array(subset.columns.tolist()+['']).reshape(4,2)\nfor i in range(4):\n    for j in range(2):\n        if i == 3 and j == 1:\n            continue\n        ax[i][j].set_title(allcolumns[i][j], fontsize=14)\n        if (data[allcolumns[i][j]].dtypes == 'float64') | (allcolumns[i][j] in int_as_float):\n            sns.distplot(data.loc[(data[allcolumns[i][j]].isna() == False) & (data[allcolumns[i][j]] != 0),\n                                      allcolumns[i][j]], ax=ax[i, j], kde=True)\n        else:\n            sns.countplot(data[allcolumns[i][j]].fillna('NA'), ax=ax[i, j])","c36b0988":"sns.pairplot(data= subset[['LotArea', 'GrLivArea', 'LotFrontage']])","721d765a":"subset.loc[((subset.LotFrontage > 300) | (subset.LotArea > 100000))]","cc82ff2c":"sns.set(rc={'figure.figsize': (4,4)})\nsns.heatmap(subset.corr(), annot= True)","ce445e9d":"fig, ax = plt.subplots(figsize=(15, 25), ncols=2, nrows=4)\nfig.suptitle('Bivariate relationships: Categorical variable vs. Numeric variable', fontsize=20)\nplt.subplots_adjust(\n    left=0.125,\n    right=0.9,\n    bottom=0.1,\n    top=0.95,\n    wspace=0.2,\n    hspace= 0.25)\n\nsns.boxplot(data= subset, x= 'BldgType', y= 'LotFrontage', ax= ax[0][0])\nsns.boxplot(data= subset, x= 'LotShape', y= 'LotFrontage', ax= ax[0][1])\nsns.boxplot(data= subset, x= 'LotConfig', y= 'LotFrontage', ax= ax[1][0])\nsns.boxplot(data= subset, x= 'HouseStyle', y= 'LotFrontage', ax= ax[1][1])\n\nsns.boxplot(data= subset, x= 'BldgType', y= 'LotArea', ax= ax[2][0])\nsns.boxplot(data= subset, x= 'LotShape', y= 'LotArea', ax= ax[2][1])\nsns.boxplot(data= subset, x= 'LotConfig', y= 'LotArea', ax= ax[3][0])\nsns.boxplot(data= subset, x= 'HouseStyle', y= 'LotArea', ax= ax[3][1])","2e299e79":"fig, ax = plt.subplots(figsize=(10, 40), ncols=1, nrows=6)\nfig.suptitle('Bivariate relationships: Categorical variable vs. Categorical variable', fontsize=20)\nplt.subplots_adjust(\n    left=0.125,\n    right=0.9,\n    bottom=0.1,\n    top=0.96,\n    wspace=0.275,\n    hspace= 0.2)\n\n#Find all possible combination of categorical features without repeation\nctg = subset.dtypes[subset.dtypes == 'object'].index.tolist()\nx = []\nfor i in range(len(ctg)-1):\n    j = i +1\n    while j != len(ctg):\n        x.append([ctg[i],ctg[j]])\n        j += 1\n\nfor i in range(len(x)):\n    title = '{0} vs. {1}'.format(x[i][0],x[i][1])\n    mosaic(data=subset, index=[x[i][0], x[i][1]], axes_label=True,\n       statistic=True, gap=0.005, title= title, ax= ax[i])\n        \nplt.show()","004eb9aa":"missing_LF = subset.loc[subset.LotFrontage.isnull()].drop(columns=['LotFrontage']) #Rows having LotFrontage to be imputed.\nhave_LF = subset.loc[~subset.LotFrontage.isnull()].reset_index()","9232c4de":"lm1 = ols('LotFrontage ~ GrLivArea + LotArea + LotShape + BldgType +  LotConfig + HouseStyle',\n         data=have_LF).fit()\nlm1.summary()","e4fca094":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,9))\nfig.suptitle('Diagnostic Plots', fontsize=20)\n\nax[0][1].axhline(np.mean(lm1.resid), color='red', ls='--', lw = 0.75, label='Residuals Avg.')\nax[1][1].axhline(0, color='black', ls='--', lw = 0.75)\n\nax[0][0].set_title('Normal Probability Plot')\nprobplot = ProbPlot(lm1.resid, fit= True)\nprobplot.ppplot(line= 'r', ax= ax[0][0])\n\nsns.scatterplot(x= lm1.fittedvalues, y= lm1.resid, ax= ax[0][1]).set_title('Residuals vs. Fitted values')\nsns.distplot(lm1.resid, ax= ax[1][0])\nsns.scatterplot(x= have_LF.index, y= lm1.resid, ax= ax[1][1]).set_title('Residuals vs. Observation order')\nplt.show()","d8c3f12f":"#The data point that could be influential\nhave_LF.iloc[[lm1.resid.sort_values().head(1).index[0]]]","12bc4aba":"k = have_LF.shape[1] - 1 #No. of predictors\nn = have_LF.shape[0] #No. of observations\ndiffits_ref = 3*np.sqrt((k+2)\/(n-k-2)) #Reference value of DIFFITS\ninfluence = lm1.get_influence()\nprint(diffits_ref)","8d07c2b4":"influence.summary_frame().loc[(influence.summary_frame(\n).cooks_d > 0.5), influence.summary_frame().columns[-6:]]","0aee1d3b":"have_LF.drop([260], inplace= True)","bd217327":"lm2 = ols('np.log(LotFrontage) ~ np.log(GrLivArea) + np.log(LotArea) + LotShape + BldgType +  LotConfig + HouseStyle',\n         data=have_LF).fit()\nlm2.summary()","9e60bb94":"fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10,8))\nfig.suptitle('Diagnostic Plots after omitting influential data points', fontsize=20)\n\nax[0][1].axhline(0, color='black', ls='--', lw = 0.75)\nax[1][1].axhline(0, color='black', ls='--', lw = 0.75)\n\nax[0][0].set_title('Normal Probability Plot')\nprobplot = ProbPlot(lm2.resid, fit= True)\nprobplot.ppplot(line= 'r', ax= ax[0][0])\n\nsns.scatterplot(x= lm2.fittedvalues, y= lm2.resid, ax= ax[0][1]).set_title('Residuals vs. Fitted values')\nsns.distplot(lm2.resid, ax= ax[1][0])\nsns.scatterplot(x= have_LF.index, y= lm2.resid, ax= ax[1][1]).set_title('Residuals vs. Observation order')\nprobplot = ProbPlot(lm2.resid, fit= True)","c4b5c78c":"subset.loc[subset.LotFrontage.isnull(), 'LotFrontage'] = np.exp(lm2.predict(missing_LF))\ndata['LotFrontage'] = subset.LotFrontage","a1856235":"# Find rows with consistent NAs in all features describing the basement\ndata.loc[(data.BsmtQual.isnull() & data.BsmtCond.isnull() & data.BsmtExposure.isnull() & data.BsmtFinType1.isnull() & data.BsmtFinType2.isnull()) & (data.BsmtFinSF1 == 0) & (data.BsmtFinSF2 == 0) & (data.BsmtUnfSF == 0) & (data.TotalBsmtSF == 0) & (data.BsmtFullBath == 0) & (data.BsmtHalfBath == 0),\n         ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']].shape[0]","6ea74829":"idx = data.loc[(data.BsmtQual.isnull() & data.BsmtCond.isnull() & data.BsmtExposure.isnull() & data.BsmtFinType1.isnull() & data.BsmtFinType2.isnull()) & (data.BsmtFinSF1 == 0) & (data.BsmtFinSF2 == 0) & (data.BsmtUnfSF == 0) & (data.TotalBsmtSF == 0) & (data.BsmtFullBath == 0) & (data.BsmtHalfBath == 0),\n         ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']].index","937e725e":"data.loc[idx, ['BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2']] = 'Not exist'\n\ndata.loc[idx, ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = 0","c685e4d3":"# Find rows with non-consistent NAs across all features describing the basement.\ndata.loc[data.BsmtQual.isnull() | data.BsmtCond.isnull() | data.BsmtExposure.isnull() | data.BsmtFinType1.isnull() | data.BsmtFinType2.isnull() | (data.BsmtFinSF1.isnull()) | (data.BsmtFinSF2.isnull()) | (data.BsmtUnfSF.isnull()) | (data.TotalBsmtSF.isnull()) | (data.BsmtFullBath.isnull()) | (data.BsmtHalfBath.isnull()),\n             ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtFinType2',\n          'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']].reset_index()","1f5aae95":"fig, ax = plt.subplots(ncols=2, nrows=1, figsize= (15,5))\n\nax[0].set_title('Basement Finish Type 2 by Basement Overall Condition', fontsize= 16)\nax[1].set_title('Basement Finish Type 2 by Basement Exposure', fontsize= 16)\n\nsns.countplot(x='BsmtCond', hue='BsmtFinType2', data=data, ax= ax[0])\nsns.countplot(x='BsmtExposure', hue='BsmtFinType2', data=data, ax= ax[1])\n\nax[0].legend(loc = [0.75,0.55])\nax[1].legend(loc = [0.75,0.55])","659c1f26":"data.BsmtFinType2.iloc[[332]] = 'Rec'","49e1b51b":"data.iloc[[948, 1487, 2348]][['LandContour','LandSlope','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1',\n              'BsmtFinType2','TotalBsmtSF']]","4fc52cc2":"fig, ax = plt.subplots(ncols=2, nrows=1, figsize= (15,5))\n\nax[0].set_title('Basement Exposure by Land Contour', fontsize= 16)\nax[1].set_title('Basement Exposure by Land Slope', fontsize= 16)\n\nsns.countplot(x='LandContour', hue='BsmtExposure', data=data.fillna('NA'), ax= ax[0])\nsns.countplot(x='LandSlope', hue='BsmtExposure', data=data.fillna('NA'), ax= ax[1])\n\nax[0].legend(loc = 'best')\nax[1].legend(loc = (0.77, 0.625))","5ccb9a9c":"data.BsmtExposure.iloc[[948, 1487, 2348]] = 'No'","400021ad":"for i in [2040, 2185, 2524]:\n    data.BsmtCond.iloc[i] = data.loc[(data.BsmtQual == data.BsmtQual.iloc[i]) & (\n        data.BsmtFinType1 == data.BsmtFinType1.iloc[i]), 'BsmtCond'].describe()['top']","cd46e487":"for i in [2217, 2218]:\n    data.BsmtQual.iloc[i] = data.loc[(\n        data.BsmtCond == data.BsmtCond.iloc[i]), 'BsmtQual'].describe()['top']","97bc0e2e":"data.loc[2120 , ['BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2']] = 'Not exist'\n\ndata.loc[2120 , ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = 0","86f5044b":"data.loc[2188 , ['BsmtQual', 'BsmtCond', 'BsmtExposure',\n                'BsmtFinType1', 'BsmtFinType2']] = 'Not exist'\n\ndata.loc[2188 , ['BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF',\n                'TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath']] = 0","de2af0b3":"data.loc[data.FireplaceQu.isnull(), ['Fireplaces']].sum()","47d181cf":"data.FireplaceQu.fillna('Not exist', inplace= True)","aba80bdd":"data.loc[data.GarageType.isnull() & data.GarageYrBlt.isnull() & data.GarageFinish.isnull() & data.GarageQual.isnull() & data.GarageCond.isnull() & (data.GarageArea == 0) & (data.GarageCars == 0),\n         ['GarageType', 'GarageArea', 'GarageYrBlt', 'GarageFinish', 'GarageQual',\n                                                                'GarageCond', 'GarageCars']].shape[0]","b5f7fc84":"data.loc[data.GarageType.isnull() & data.GarageFinish.isnull() & data.GarageQual.isnull() & data.GarageCond.isnull() & (data.GarageArea == 0) & (data.GarageCars == 0),\n         ['GarageType', 'GarageArea', 'GarageFinish', 'GarageQual',\n                                                    'GarageCond']] = 'Not exist'","dadda1f5":"data.loc[data.GarageType.isnull() | data.GarageFinish.isnull() | data.GarageQual.isnull() | data.GarageCond.isnull() | data.GarageCars.isnull() | data.GarageArea.isnull(),\n         ['GarageType', 'GarageArea', 'GarageYrBlt', 'GarageFinish', 'GarageQual',\n                                                    'GarageCond', 'GarageCars']]","ff55891c":"# Imputing GarageYrBlt, GarageFinish, GarageQual, and GarageCond for row no. 2126\ndata.at[2126,'GarageYrBlt'] = data.iloc[2126]['YearBuilt'] + data.loc[(data.GarageType == 'Detchd') & (data.Neighborhood == 'OldTown'), [\n    'GarageYrBlt', 'YearBuilt']].apply(lambda x: x[0] - x[1], axis=1).mean()\n\ndata.at[2126, 'GarageFinish'] = data.loc[(data.GarageType == 'Detchd') & (\n    data.GarageCars == 1)].GarageFinish.describe()['top']\n\ndata.at[2126, 'GarageQual'] = data.loc[(data.GarageType == 'Detchd') & (\n    data.GarageCars == 1)].GarageQual.describe()['top']\n\ndata.at[2126, 'GarageCond'] = data.loc[(data.GarageType == 'Detchd') & (\n    data.GarageCars == 1)].GarageCond.describe()['top']","b4a30ed9":"data.loc[2576, ['GarageType','GarageFinish','GarageQual','GarageCond']] = 'Not exist'\n\ndata.loc[2576, ['GarageArea', 'GarageCars']] = 0","dfe21950":"data.loc[data.PoolQC.isnull(), ['PoolArea']].sum()","0430dfb3":"data.loc[(data.PoolQC.isnull()) & (data.PoolArea != 0), ['PoolQC', 'PoolArea']]","2cc01ff6":"data.loc[[2420,2503,2599],'PoolQC'] = data.PoolQC.describe()['top']","62b83f92":"data.loc[(data.PoolQC.isnull()), 'PoolQC'] = 'Not exist'","d10707cc":"data.loc[data.MiscFeature.isnull(), ['MiscVal']].sum()","1b17fe60":"data.loc[(data.MiscFeature.isnull()) & (data.MiscVal != 0), ['MiscFeature', 'MiscVal']]","0028c125":"data.loc[2549, 'MiscFeature'] = 'Othr'","53d842f7":"data.MiscFeature.fillna('Not exist', inplace= True)","b64a4bdb":"indx = data.loc[data.MasVnrArea.isnull() & data.MasVnrType.isnull()].index\ndata.loc[indx, 'MasVnrType'] = 'None'\ndata.loc[indx, 'MasVnrArea'] = 0","7db998c5":"data.loc[data.MasVnrType.isnull(), 'MasVnrType'] = data.MasVnrType.value_counts().index[1]","cfc58a26":"data.loc[data.Exterior1st.isnull(), ['Exterior1st', 'Exterior2nd']] = 'Other'","f9ce535f":"data.loc[data.KitchenQual.isnull(), 'KitchenQual'] = data.loc[data.OverallQual == data.loc[data.KitchenQual.isnull(), 'OverallQual'].values[0],\n         'KitchenQual'].describe()['top']","11497ef6":"data.loc[data.SaleType.isnull(), 'SaleType'] = data.loc[data.SaleCondition == data.loc[data.SaleType.isnull(), 'SaleCondition'].values[0],\n         'SaleType'].describe()['top']","0fa39317":"data.drop(columns='SalePrice').isnull().sum()[data.isnull().sum() != 0]","291373e9":"data.groupby(['YrSold'])[['SalePrice']].describe()","aedbf597":"sns.distplot(data.SalePrice.dropna())","4e5d5a21":"sns.scatterplot(data= data.dropna(), x= 'GrLivArea', y='SalePrice')","6b74d418":"data.drop(index= data.loc[(data.GrLivArea > 4000)&(data.SalePrice.notnull())].index, inplace= True)","a0f86af9":"data.to_csv('modified_data.csv', index= True)","6304b5f3":"For the remaining houses with NAs in **MiscFeature**, we can conclude that these houses don't have miscellaneous features. Hence, NAs are a category value not a missing value and we'll replace them by 'Not exist'.","cfb49162":"We have two rows (indices: 2126, and 2576) that don't have consistent NAs across all features describing the garage.<br\/>\nThe first one, with index 2126, seems to have real missing values in features: **GarageYrBlt**, **GarageFinish**, **GarageQual**, **GarageCond**. We'll impute them as follows:<br\/>\n * **GarageYrBlt** = House **YearBuilt** + Average difference between(YearBuilt, GarageYrBlt) for the same **GarageType** in the same **Neighborhood**. \n * **GarageFinish**, **GarageQual**, **GarageCond**: will be filled by the most frequent values for Garages of the same type and the same car capacity (i.e. **GarageCars**).","e93fe29e":"Now, let's find rows that don't have consistent NAs across all features describing the garage.","f13c44cd":"The reasoning for this option is that, the **YearRemodAdd** may have better explanatory power without those values at 1950 that seems inaccurate. Let's replace them by NA and plot the distribution again to check the modality.","d48d5cc8":"The normal probability plot and the histogram show the residuals became almost normal.\nThe residuals vs. fitted values plot shows that residuals variance became more likely to be equal such that the residuals became more evenly distributed around the zero.<br\/>\nNow, we can use our model to predict the missing values in **LotFrontage**.","787d2e04":"Now, we have imputed all NAs except those of two features that will be handled during the modeling phase. It's time to move on to the last step in this first phase, the *Data assessment*.","420bd9c1":"There's one house with missing value in the **MiscFeature**. Since, we have no clue to guess what type of **MiscFeature** that may exist, we will it by 'Othr'.","f7e00f00":"The probability of the F-statistic is less than 0.01, which means that the model containing at least one of the features we have is useful in predicting the response variable. Also, the Adj. R-squared is moderate (0.542).","be61f095":"By checking the **\"YearRemodAdd\"** feature we note the minimum value is 1950. And no prior records are available. Also, there are 339 houses that are supposed to be remodeled in 1950 alone while there's no single house remodeled in the next year. Since we don't have more information about how was this data collected? We suspect that the process of keeping records for remodeling date started only in 1950 and all houses that have ever remodeled at that time are assumed to be remodeled in 1950.","6863c2cd":"F) Features describing Masonry veneer:<br\/> \n\nFor all houses having NAs in both **MasVnrArea** and **MasVnrType**, We'll replace NAs in **MasVnrType** by 'None' and those in **MasVnrArea** by 0.","3343e0a6":"*Note*: I found it useful to use these forms (<a href= 'https:\/\/developers.google.com\/machine-learning\/problem-framing\/try-it\/framing-exercise'>Form 1<\/a>, <a href='https:\/\/developers.google.com\/machine-learning\/problem-framing\/try-it\/formulate-exercise'>Form 2<\/a>) developed by Google in order to frame a machine learning problem.","d2af2679":"#### 1.3.2.2 Data compatibility","989b7f79":"Now, it's clear that the distribution peak is around 2000 and there's something unusual before 1960. Let's compare the distributions of **YearBuilt** and **YearRemodAdd** and try to figure out what's wrong. Initially, we expect **YearRemodAdd** distribution to lag **YearBuilt** distribution and to have almost the same shape.","9a66dc0c":"Houses with **GrLivArea** seems to be potential outliers, so would remove them.","74b69b3c":"5) One house with index 2120: has NA values in all features describing the basement. Since we have no clue to guess the true values, we will consider this house as if it has no basement. The categorical features will be filled by 'Not exist' and numeric features will be filled by 0.","1b398f06":"**Group B: Non-exclusive features**<br\/>\nAnd we found 27 features that are non-exclusive:<br\/>\n( **MasVnrType**, **MasVnrArea**, **BsmtQual**, **BsmtCond**, **BsmtExposure**, **BsmtFinType1**, **BsmtFinType2**, **TotalBsmtSF**, **BsmtFinSF1**, **BsmtFinSF2**, **BsmtUnfSF**, **BsmtFullBath**, **BsmtHalfBath**, **FireplaceQu**, **GarageType**, **GarageYrBlt**, **GarageFinish**, **GarageQual**, **GarageCond**, **GarageCars**, **PoolQC**, **MiscFeature**, **SaleType**, **GarageArea**, **KitchenQual**, **Exterior1st**, **Exterior2nd**).","430a381c":"+ 6) Select the final model:","dcbed5a4":"## 1.4 Data assessment","41354cd1":"We have a total of 35 features having NA values.","c984c6cc":"+ Is the data relevant and sufficient?\n\n<p> The available data includes almost every aspect of a housing, the location, available facilities, quality of interior and exterior, materials used on the interior and exterior, space related characteristics and many other characteristics. So the data we have is relevant.<\/p>\n<p>The training data includes 1460 record which is sufficiently large compared to the 79 feature we have. As a rule of thumb, the training data should be at least one order of magnitude more than trainable parameters.<\/p>","6358a799":"A mosaic plot is a graphical display that allows you to examine the relationship among two or more categorical variables. The mosaic plot starts as a square with length one. The square is divided first into horizontal bars whose widths are proportional to the probabilities associated with the first categorical variable. Then each bar is split vertically into bars that are proportional to the conditional probabilities of the second categorical variable.<br\/>\nThe color of tiles in mosaic plot is coded as follows:\n+ *Green*: if the observed frequency is within two standard deviations of the expected frequency under independence hypothesis.\n+ *Red*: if the observed frequency is more than two standard deviations in the positive direction.\n+ *Blue*: if the observed frequency is more than two standard deviations in the negative direction.\n+ *Hatching*: if the observed frequency is more than three standard deviations from the expected frequency in either direction.\n\nFor more on Mosaic plots:\n+ <a href=\"https:\/\/towardsdatascience.com\/mosaic-plot-and-chi-square-test-c41b1a527ce4\">Mosaic Plot and Chi-Square Test<\/a>.\n+ <a href=\"https:\/\/www.jstor.org\/stable\/2291215?seq=1#page_scan_tab_contents\">Mosaic Displays for Multi-Way Contingency Tables<\/a>\n+ <a href=\"https:\/\/www.youtube.com\/watch?v=VYKbIudYVvI\">Core Two categorical variables Mosaic Plots <\/a>","76c74bcb":"> CAUTION: Make a copy of the original data before applying any changes during this step and subsequent steps.","4d184c14":"During my journey to learn data science, I came across many interesting MOOCs, tutorials, blogs, and books that helped me develop an understanding of the data science methodology.\nOne of those great books is  <a href=\"https:\/\/amzn.to\/2EqvUy7\" target=\"_blank\">\"Think Like a Data Scientist: Tackle the data science process step-by-step\"<\/a> by Brian Godsey. This book is process-oriented: it introduces the data science work flow in an abstract way without delving into technical details of neither statistics nor programming. This helped me to focus on the big picture of the data science project without getting lost in the technical details of statistic, machine learning and coding.\n\n<p> Godsey divided the data science process into three phases: preparing, building and finishing. Each phase consists of subprocesses.","4fa8394c":"E) Feature describing miscellaneous features (**MiscFeature**):\n\nLet's check whether all houses with *NA* **MiscFeature** don't have miscellaneous features using the **MiscVal** feature. We will do so by calculating the total value of miscellaneous features for these houses.","c4d8532a":"From the above data frame, we can notice features whose maximum value is less than or equal 10 (e.g. **\"OverallQual\"**, **\"OverallCond\"**, **\"BsmtFullBath\"**, **\"BsmtHalfBath\"**) while other features may have a maximum value of tens of thousands (e.g. **\"LotArea\"**). This large difference in scales between features can negatively affect any prediction model we build later, so we should consider standardization or normalization first. <br\/>\n","ed41a915":"After omitting the influential data point and applying transformations for both response and numeric predictors, We can notice that the Adj. R-squared has increased from 0.568 to 0.751. We have to check model assumptions again.","d55e6e01":"At this point, we have cleaned our data, imputed missing values, and removed potential outliers. We would save our data to be used in the second phase, *Build*.","4fae2275":"For this data point, the **LotArea** is unusually large (215245 SF).","aff44c5d":"### 1.4.1 Descriptive statistics","ed39fb3c":"*2nd option:* Replace **YearRemodAdd** = 1950 by NA","235f6dc8":"3) Three houses with indices 2040, 2185, and 2524: have NA values in **BsmtCond** feature. Since these houses have a basement, their NAs are missing values not a category value. We fill these NAs by the most common **BsmtCond** for houses having the same **BsmtQual** and **BsmtFinType1**.","3b36a19d":"h) Features describing kitchen (**KitchenQual**):<br\/>\n\nWe will fill this NA the most frequent **KitchenQual** for houses having the same **OverallQual**.","8ca1f94c":"So, there are 1560 house that have never been remodeled and their **YearRemodAdd** will be set to NA.","fe8e6fb0":"## 1.3 Data Wrangling","8dbd3345":"+ **Type III**: Categorical variable vs. Categorical variable: we'll use mosaic plots for this type.","a37f5c03":"B) The feature describing the fireplace (**FireplaceQu**):\n\nLet's check whether all houses with *NA* **FireplaceQu** don't have fireplace using the **Fireplaces** feature. We will do so by calculating the total number of fireplaces in these houses.","af85c999":"##### Step 4: Non-exclusive features\nThis group includes features which describe a house object for which there are several features describing different aspect of it:(**BsmtQual**, **BsmtCond**, **BsmtExposure**, **BsmtFinType1**, **BsmtFinType2**, **FireplaceQu**, **GarageType**, **GarageYrBlt**, **GarageFinish**, **GarageQual**, **GarageCond**, **GarageCars**,**PoolQC**, and **MiscFeature**).<br\/>\n   \n   For this subgroup, we will check whether NAs are persistent across all categorical features describing the same house object and the numerical features have zeros. For example, houses that don't have a garage will have a value of NA for the features: **GarageType**, **GarageYrBlt**, **GarageFinish**, **GarageQual**, and **GarageCond**. And other numeric features describing the same house object: **BsmtFinSF1**, **BsmtFinSF2**, **BsmtUnfSF**, **GarageArea**, **GarageCars**, and **TotalBsmtSF** will equal zero. If NAs are not persistent across all features describing the same house object and corresponding numeric features don't equal zero, then it will be considered as a missing value.<br\/>\n\nIn the following, we will consider each group of features describing the same house object.","f1c78e0c":"+ b) Study bivariate relationships to reveal other outliers, to suggest possible transformations, and to identify possible multicollinearity.","66b66e73":"## 1.1 Set goals:\nWhile setting goals for a data science project consider the following points:\n+ Listen to the customer:\n - What questions need to be answered?\n   * What are the factors that strongly influence the final house price? and describe their relationship.\n - What is the tangible final product?\n   * A machine learning model to predict the house price based on input features.","dcbf2bda":"Now we can state the **goal of the project as follows:**\n**<p>\"to build a machine learning regression model to accurately predict the final price of each home given its characteristics\".<p\/>**","44d5ec75":"The Second row, with index 2576, have NAs in all features describing the garage except the **GarageType**. It's more likely to be data entry error. So, we will replace the **GarageType** and other categorical features by 'Not exist' and we will replace numeric features by 0  as if the house doesn't have a garage. ","942eadff":"Imputing missing values using the linear regression method is best suited when the only missing value in the row is the value to be imputed. In doing so, we'll follow these steps:\n+ 1) Define candidate features that may related to the **LotFrontage**.\n+ 2) Select rows for which no column can have missing values except the **LotFrontage**.\n+ 3) Explore data:\n  * a) On a univariate basis: to check for gross data errors.\n  * b) Study bivariate relationships to reveal other outliers, to suggest possible transformations, and to identify possible multicollinearity.\n+ 3) Split the selected rows into three sets: train\/valid\/test such that:\n  * train\/valid: are 70%, and 30% of rows whose **LotFrontage** is not NA, respectively.\n  * test: are the rows whose **LotFrontage** is NA. And these are the NAs to be imputed.\n+ 4) Use training data to identify several candidate models using the best subset feature selection method.\n+ 5) Select and evaluate a few good models:\n  * Select the models based on the adjusted-R-Sq and the number of features.\n  * Check the selected models for compliance with model assumptions (LINE assumptions).\n+ 6) Select the final model:\n  * Compare the competing models by cross-validation against the valid set.\n  * Consider residual plots, outliers, parsimony, and relevance while comparing competing models.\n  \nFor more on regression model building, check this <a href=\"https:\/\/newonlinecourses.science.psu.edu\/stat462\/node\/89\/\">link<\/a> . Let's begin!<br\/>\n\n","69d79751":"From the above influence summary, we found the suspected data point has very high Cooks's distance. It also has large absolute studentized residuals (11.84), and large absolute difference of fits (13.15). So, we can consider this data point as influential and we will refit the model after omitting it.","109b197c":"Now, we verified that all NAs are a category value not a missing value. Let's replace those NAs by *Not exist*.","942dcf4a":"3) Explore data:\n  + a) On a univariate basis: to check for gross data errors.","344c47bf":"Our estimation will be based on the mid difference between **YearBuilt** and 1950 for each **Neighborhood**. We will add this mid difference to **YearBuilt** such that:<br\/>\n\nEstimated **YearRemodAdd**  =  **YearBuilt**  +  (1950 - **YearBuilt**)\/2\n\nThis is because our assumption about the true **YearRemodAdd** implies the following constraints:<br\/>\n1- **YearRemodAdd** should be smaller than 1950<br\/>\n2- **YearRemodAdd** should be larger than **YearBuilt** <br\/>\n\nSo, we can't use a summary statistic such as: mean or median in our estimation because we will end up violating one of these two constraints.","78b9a807":"## 1.2 Explore","f8970eb9":"One can notice many features have bimodal distribution (e.g. **\"BsmtFinSF1\"**, and **\"WoodDeckSF\"**) and the first peak is almost at the value of zero. This is because a zero is used rather than NA to represent the absence of this feature. For example, **\"2ndFlrSF\"** (Second floor square feet), which is a continuous variable by nature, has two peaks one at zero (which corresponds to no 2nd floor) and the other is around 800. Here, an NA should be used rather than zero. So, we will exclude all zeros from our distplots.","8491c513":"**Group A: Exclusive features**<br\/>\nBy referring to data description file, we found 8 features that are exclusive:<br\/> (**Alley**, **YearRemodAdd**, **Fence**, **LotFrontage**, **Electrical**, **MSZoning**, **Utilities**, **Functional**)","2fecd69e":"Since the total value of miscellaneous features for these houses is not zero, there are one or more houses having missing values in the **MiscFeature**. Let's find out the indices of these houses.","6d59937b":"<p> Data wrangling or data munging is the process of collecting data and information in difficult, unstructured formats and converting it into something that's suitable for software usage.<\/p>\n\nIt consists mainly of two steps:<br\/>\n\n 1)  Additional data collection which may include one or more of the following:\n   - Data hunting (APIs, government data sources, academic datasets, and manual data collection).\n   - Scraping.\n   - Logging.\n   \n   \n 2)  Data cleaning which includes:\n   - Correcting errors: which are made by mistake during raw data collection and primary processing.\n   - Data compatibility assurance: this refers to data comparability issues that arise when datasets are merged (e.g. units of measurement, numerical representations, spelling of names, data and time formats, and currency).\n   - Dealing with missing data by different imputation techniques (e.g. heuristic based techniques, mean\/median value, most frequent value, interpolation, and nearest neighbor).\n   - Outliers detection.","4910757e":"**Influential points**<br\/>we can notice one point that has very large absolute residual. This point can be an influential point such that it excessively affect our model. We will use three quantitative measures to investigate that data point as follows:\n+ **Studentized residuals**: a data point with |studentized residual| > 3 and very different from other points can be considered as influential.\n+ **Difference of fits (DFFITS)**: any point have |DIFFITS| > 2(k+2\/n-k-2)^0.5 and very different from other points can be considered influential.\n+ **Cook's distance**: a data point with Cook's distance > 0.5 and very different from other data points is an influential.\n\nFor more on Influential Points, check this <a href=\"https:\/\/newonlinecourses.science.psu.edu\/stat462\/node\/87\/\">link<a\/>.","4db31201":"g) Features describing the Exterior covering on house (**Exterior1st**, and **Exterior2nd**):<br\/>\n\nWe will fill these NAs with 'Other'.","79acc3bb":"We will follow this data science method to investigate the questions we stated in the previous section.<br\/>\nBut we will first start by checking the features we have, and the data types of these features.","b135cd48":"We should also check the other variables for potential errors. The continuous numeric variables are best described by distplots while discrete numeric variables and categorical variables are best described by countplots. *But be aware of some variables which are actually continuous having an integer data type rather than float (e.g. **GrLivArea**, **GarageArea**, **WoodDeckSF**).*","10eb5172":"After correcting errors, and checking for ensure data compatibility the next step is to deal with missing values. The is step is somewhat tricky for this dataset specially for categorical features because some features have a category value named *NA*. For these features, *NA* represents a meaningful piece of data which means this feature doesn't exist in this house. On the contrary, the null values *NA* means that we don't now whether this feature exists in this house or not. To tackle this problem, we will follow these steps:<br\/>\n\n1- Identify which features have NA values, then<br\/>\n2- Check if the feature is exclusive such that there's no other feature is describing the same house object.<br\/>\n3- For exclusive features:\n  * A) If the feature is categorical, check whether these features have a category value called *NA* (by referring to data description file), if yes, fill by 'Not exist'. Otherwise, fill by the most common value.\n  * B) If the feature is numeric, impute the missing value by the appropriate imputation method. \n\n4- For non-exclusive features: try to infer whether it's a null value or a category value from other features describing the same house object (e.g. **GarageType**, **GarageYrBlt**, **GarageFinish**, **GarageCars**, **GarageArea**, **GarageQual**, and **GarageCond** describe different aspects of the garage).","5994ce2e":"The box plots shows that some features (**BdgType**, and **LotShape**) have a relationship with the **LotFrontage**. On the other hand, no relationship seems to exist between the other categorical variables and numeric variables.","fdc3c20e":"D) Feature describing the pool (**PoolQC**):\n\nLet's check whether all houses with *NA* **PoolQC** don't have pool using the **PoolArea** feature. We will do so by calculating the total pool area for these houses.","af68f066":"**Numerical Features at first glance**<br\/>\n<p>Let's have another look on the numerical features we have; try to make sense of the shape of distribution by comparing the mean and median for each feature, and the range of values each feature has.<p\/>","b89f9039":"So, we have 43 categorical features and 36 numerical features. The methods of graphical and quantitative analysis differs for both types.","38d4ce88":"### 1.3.2 Data cleaning\n\nNow, it's time for the second step. Let's go directly to **data cleaning:**","18001b0f":"### 1.4.2 Check outliers.","b3187b37":"### 1.3.1 Data collection\n\n<p>Fortunately, we can find more real state related data for the City of Ames, Iowa in the <a href=\"https:\/\/www.cityofames.org\/home\">official website<\/a>. This includes maps and different types of reports. But, we think we have all data needed to start building our sale price prediction model. We may make use of the additional data available online in later projects which may include geographical visualizations.<\/p>","2a150019":"We have 79 explanatory features. 1 feature for the house ID and 1 response variable. <br\/>Next, check the data types of features and capture them in separate variables so we can access them easily later. This is useful because when use the *describe* method later, the output will be much helpful.","7873213d":"From these scatter plots, we can notice two houses have unusually large **LotFrontage** (> 300 ft.) and four houses that have very large **LotArea** (> 100000 sq.ft.). These points are candidate outliers. Let's find out more about them.","ebcba0a8":"#### 1.3.2.3 Dealing with missing values","b7423bb3":"Now, we can see that features like **\"BsmtFinSF1\"**, **\"WoodDeckSF\"**, and **\"2ndFlrSF\"** have only one peak after excluding the zeros.<br\/>\nBut, the **\"YearRemodAdd\"** feature (Remodel date) is bimodal. By referring to data description file, we can find this feature was set to be equal to **\"YearBuilt\"** if there's no remodeling or additions. We suggest setting **\"YearRemodAdd\"** to NA, temporarily, rather than the **\"YearBuilt\"** and plot the displot again to check if two peaks still exist.","25b3a69b":"+ B) For numeric features (i.e. **YearRemodAdd** and **LotFrontage**):\n   * **YearRemodAdd**: these are the NAs we have set earlier for houses that have never remodeled. So, we will leave this feature as it is now and the NAs will be handled later during the modeling phase. \n   * **LotFrontage**: we will impute the missing value using the linear regression method.","98517da2":"+ **Type II**: Categorical variable vs. Numeric variable: we'll use box plots for this type.","1fdaa717":"**Features exploration**<br\/>\n","81575518":"The **LandContour** for these houses is *Lvl* (Near Flat\/Level) and **LandSlope** is *Gtl* (Gentle slope). We conclude that the land is almost flat and the house is not built into a hill. So, we can conclude that true **BsmtExposure** is *No* rather than *NA* and there's a data entry error.<br\/> Now, let's plot a countplot for **BsmtExposure** by **LandContour** and **LandSlope** to verify our assumption.","05db4ec4":"1) Define candidate features that may related to the LotFrontage:<br\/>\nFrom the 79 features available, we think the most relevant features are: **LotShape**, **GrLivArea**, **BldgType**, **LotArea**, **LotConfig**, and **HouseStyle**.<br\/>\n\n**Note**: Candidate features can be identified by means of both graphical methods such as scatter plots and quantitative methods such as Pearson correlation coefficient and its variants.","606f1f2b":"i) Feature describing the sale (i.e. **SaleType**):<br\/>\n\nWe will fill this NA by the most frequent **SaleType** for houses having the same **SaleCondition**.\n","221384ed":"We will try to follow this road map, as far as we can, to work on this project of Ames houses prices prediction. This notebook will cover the first phase (Prepare) and the remaining two phases (Build and Finish) will be covered in subsequent notebooks.","5a9a99c9":"# 1. Prepare\n\nIn this phase, we will identify what we know? What we have? What we can get? Where we are? and where we would like to go?<br\/>\nThe first and most important step is to set goals for our project.","b20da277":"So, there are 157 rows having NAs across all categorical features describing the garage and zeros in all numeric features. For these rows, we can conclude that all NAs are actually a category value not missing value. So, we will replace NAs in these features by 'Not exist' except **GarageYrBlt** we will leave NAs to be handled during the modeling phase.","f530f3e7":"Here, we have the number of duplicated rows is zero. Hence, we don't have any duplicate values.<br\/>","3dda7bef":"This step starts with the calculation of some descriptive statistics. ","c3ee9137":"#### 1.3.2.1 Correcting errors\n\n<p>The first step to detect errors is to make an expectation about distribution of the variable of interest, how should it look like? For example, if we think of distribution of annual income for individuals around the world, we would expect the distribution to be right skewed and heavy tailed: since very rich people are much less than very poor people. On the contrary, if we think of the distribution of death age, we would expect the distribution to be left skewed.<\/p>\n\nWe'll start with the **\"SalePrice\"** variable. I expect its distribution to be right skewed because it seems, at least to me, there's no upper limit on how luxurious a house could be but of course there's a lower limit. We will use both graphical methods (e.g. box plot and histogram) and quantitative measures (e.g. coefficient of skewness and kurtosis) to assess the modality, symmetry and spread of sale price distribution.<br\/> \n\nErrors can appear as something that's unexpected or unusual such as a bimodal distribution or an unusual max\/min values. ","7f0cae53":"##### Step2: \nCheck which of these 35 features is exclusive such that there's no other feature is describing the same house object.","c387b3c6":"A) Features describing the house basement (**BsmtQual**, **BsmtCond**, **BsmtFinSF1**, **BsmtFinSF2**, **BsmtUnfSF**, **TotalBsmtSF**, **BsmtExposure**, **BsmtFinType1**, **BsmtFinType2**, **BsmtHalfBath**, **BsmtFullBath**):","61f846e6":"As illustrated by below Venn diagram, one key component of data science is *domain expertise*. So, it would very helpful if you familiarize yourself with basic real estate terms and concepts. During my research, I found some useful resources: <br\/>\n1) <a href=\"https:\/\/www.proptiger.com\/guide\/real-estate-glossary\/a\">Real estate glossary<\/a>. <br\/>\n2) <a href=\"http:\/\/www.cityofames.org\/home\/showdocument?id=35641\">Ames' rules of construction and definitions<\/a>. <br\/>\n3) <a href=\"http:\/\/www.cityofames.org\/home\/showdocument?id=648\">Ames' developement standards<\/a>. <br\/> \n4) <a href=\"https:\/\/www.planning.org\/pas\/reports\/report165.htm\">Illustrating the Zoning Ordinance<\/a>. <br\/>\n\n\n<img src=\"http:\/\/social-metrics.org\/wp-content\/uploads\/2018\/10\/venn.png\" style= height:300px alt=\"alt text\" title=\"Data Science Venn Diagram\" \/>","a1c631cf":"We can't notice something unusual such as bimodal distributions. The only thing is that **LotArea**, **GrLivArea** and **LotFrontage** are right skewed. But, we will deal with this problem in a later step.","1e67c42b":"6) One house with index 2188: has NA values in all categorical features and some of the numeric features describing the basement. But other numeric features describing basement area are zeros. So, we can conclude that this house has no basement. The categorical features will be filled by 'Not exist' and numeric features will be filled by 0.","ce058116":"*1st option:* YearRemodAdd estimation\n","ce35c5eb":"+ Ask good questions based on concrete assumptions that are well defined and can be tested.<br\/>\n **Example questions:**\n - Q1. Is the type of foundation strongly correlated with the sale price?\n - Q2. Are the houses located in a commercial area more expensive than houses other in locations?\n - Q3. What's the relationship between exterior covering on house and the overall quality of the house?\n - Q4. Is the type of dwelling strongly related to the general zoning classification (e.g. Agricultural, industrial ... etc.)?\n - Q5. Is proximity to a arterial street strongly strongly correlated with the final price of the house?\n - Q6. Is proximity to more than one condition (e.g. arterial street, feeder street ... etc.) strongly related to sales price?\n - Q7. What's the relationship between the dwelling type and the confronting area? Are houses with larger confronting area more expensive than those with smaller confronting area?\n - Q8. Is there one type of dwelling that's generally more expensive than others?\n","d6cdc377":"Also, there are four features (**\"YearBuilt\"** , **\"YearRemodAdd\"** , **\"GarageYrBlt\"** , **\"YrSold\"** ) which are actually dates not numeric values. So, we should be careful while handling these features.","5551255b":"The first option has reduced the peak at 1950 and resulted in a shape that's closer to what we expect. On the other hand, the second options seems to greatly change the distribution of **YearRemodAdd**. So, we will go with the first option and update the original data frame.","3fe551a5":"So, there are 77 rows having NAs across all categorical features and zeros across all numeric features describing the basement. So, we can conclude that these houses don't have a basement and we will fill these categorical features with 'Not exist' and numeric features with 0.","1c3f8731":"From the above comparison, we can verify that no **YearRemodAdd** is neither less than **YearBuilt** nor larger than 1950. Hence, our constraints are satisfied. Now, let's plot the distribution for **YearRemodAdd** again to check the modality.","0cadc69c":"**Data Science project phases and subprocesses:<br\/>**\n\n  + _Phase 1: Prepare<br\/>_\n      - Set goals.<br\/>\n      - Explore.<br\/>\n      - Wrangle.<br\/>\n      - Assess.<br\/>\n  + _Phase 2: Build<br\/>_\n      - Plan<br\/>\n      - Analyze<br\/>\n      - Engineer<br\/>\n      - Optimize<br\/>\n      - Execute<br\/>\n  + _Phase 3: Finish<br\/>_\n      - Deliver<br\/>\n      - Revise<br\/>\n      - Wrap up","1bf340ea":"+ Summaries of prior research and similar projects.\n     * <a href=\"https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python#2.-First-things-first:-analysing-'SalePrice'\">Pedro Marcelino's kernel on Kaggle<\/a>:<br\/>\n       * Created three segments of the 79 explanatory variables (Building, Space,and Location).\n       * Stated prior expectations (e.g. high, medium, low) about the importance of each variable by asking questions from the view point of a potential customer.\n       * Used scatter plots and box plots to study the relation ship between variables with high expectation and the sales price.\n       * Used heat maps and correlation matrix to further discover more variables that might be correlated with the sales price.\n       * Checked variables with missing data and excluded variables with more than 15% missing.\n       * Standardized the sales price to detect unusual observation points, then used scatter plots to further investigate those suspected outliers.\n       * Tested the assumptions of multivariate analysis (i.e. normality, heteroscedasticity, linearity, and absence of correlated errors).\n       * Most important variables : ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF']","40027bda":"**\"MiscVal\"** feature also has a third quartile of zero, which makes sense because most houses don't have **\"MiscFeature\"**. But we can't discard this feature before checking the correlation with the **\"SalePrice\"** because it indicates that some houses have **\"MiscFeature\"** that worth as much as $15,500.","492244e4":"Here, we want to plot three types of relationships:\n+ **Type I**: Numeric variable vs. Numeric variable: we'll use scatter plots for this type.","f54175bc":"**Categorical features at first glance**<br\/>\n\n\nLet's have a look on the categorical features we have; number of categories in each, and the most frequent category.","d1ae3237":"> **\u201cNumbers have an important story to tell. They rely on you to give them a voice.\u201d** -- Stephen Few\n","dca73fdf":"##### Step1:\nIdentify which features have NA values, the number of NA values in each feature, and the percentage of NA.","2ded1f3c":"2) Select rows for which no column can have missing values except the **LotFrontage**.","906d8ce5":"From the countplot, we can notice that houses having same **BsmtExposure** (No) and **BsmtCond** (TA) have *Unf* **BsmtFinType2**. But as we stated earlier for this house the **BsmtFinType2** is unlikely to be *Unf*. The second most likely **BsmtFinType2** is *Rec*.\n\nSo, we will replace this *NA* by *Rec*.","68dea194":"The linear regression model has four assumptions that need to be satisfied before drawing any useful conclusions based on the model. These assumptions are called (**LINE** assumptions):\n+ 1) Linearity: the mean response is a linear function of the predictors. In other words, the residuals have a mean of zero.\n+ 2) Independence of residuals.\n+ 3) Normality of residuals.\n+ 4) Equal variance: the residuals are assumed to have equal variance.\n\nFor more on Linear Regression model assumptions, check this <a href=\"https:\/\/newonlinecourses.science.psu.edu\/stat462\/node\/145\/\">link<\/a>.","fa9862cf":"Data scientist as explorer should access interesting areas, recognize patterns and find new interesting things. Also, a data scientist should evaluate new and unfamiliar things, and draw connections between the familiar and unfamiliar things.<br\/>\nData scientist should make use of **the data science method:**\n+ Ask a question.\n+ State a hypothesis about the answer to the question.\n+ Make a testable prediction that would provide an evidence in favor of the hypothesis if it's correct.\n+ Test the prediction via an experiment involving data.\n+ Draw the appropriate conclusions through analyses of the experimental results.<br\/>\n","b86a7263":"There is only one house remaining with NA in **MasVnrType**. We will fill this NA by the most frequent **MasVnrType** except the 'None' because it does have a Masonry veneer.","15aa98d1":"##### Step3 : Exclusive features\n  \n+ A) For categorical features:\n  * i) Features having NA as a category level (by referring to data description file): **Alley**, and **Fence**. \n  We have no option but to assume all NAs are a category value. So, we would replace them by a more meaningful value such as: 'Not exist'. \n  \n  * ii) Features that don't have a category level called NA: **Electrical**, **MSZoning**, **Utilities**, and **Functional**.\n  We will fill these NAs as follows:\n     - **Electrical**: most common **Electrical** for houses having the same **CentralAir**.\n     - **MSZoning**: most common **MSZoning** for houses having the same **Condition1** and **Condition2**.\n     - **Utilities**: most common **Utilities** for houses having the same **Functional**.\n     - **Functional**: most common **Functional** for houses having the same **OverallCond**.","0a9bf37d":"The remaining rows that have NAs in **PoolQC**, also have zeros in **PoolArea** which means these houses don't have pool. We will fill these NAs by 'Not exist'.","38e7cebb":"We have two options to deal with this issue:<br\/>\n1st: Estimate the true **\"YearRemodAdd\"** for houses with **\"YearRemodAdd\"** = 1950.<br\/>\n2nd: to replace the **\"YearRemodAdd\"** = 1950 by NA.\n\nLet's try both options and check which is better.","ba510b17":"The data frame above shows that some features (e.g. **\"Neighborhood\"**) have as much as 25 category! This may not be useful and we may consider grouping similar categories together so that we end up with smaller number of categories that are significantly different from each other. Here, we mean by similar is that the sale price doesn't vary significantly between categories. We will use the box plots and statistical significance tests later to do this.<br\/>\n\nWe can also notice some features (e.g. **\"Utilities\"**) have a very prominent category: *AllPub* is repeated 1459 out of 1460! In this case, we don't expect this feature to have any explanatory power since it have almost one category. This feature and similar features (i.e. **\"Street\"**, **\"RoofMatl\"**, **\"Heating\"**, **\"GarageQual\"**, and **\"GarageCond\"**) are good candidates for feature engineering: some of them can be combined in one feature that has more explanatory power for example.<br\/>\n\nOther features have relatively small number of not-null values (e.g. **\"Alley\"**, **\"FireplaceQu\"**, **\"PoolQC\"**, **\"Fence\"**, and **\"MiscFeature\"**). These features would be better converted to binary features with only two categories rather than multiple categories with rare occurrences.","9624fc0e":"Now, let's plot the results of two options side-by-side and decide which one has a more intuitive result.","4c7374e9":"From the countplot, we can notice that the vast majority of houses having the same **LandContour** (Lvl) and **LandSlope** (Gtl) have no basement exposure. Hence, our assumption seems rational. So, we'll replace these *NAs* with *No*.","b64311cd":"From the above dataframe, we can find 11 houses that have missing values as follows:<br\/>\n1) One house with index 332: has one missing value in **BsmtFinType2** feature.<br\/>\n\nThis house has **TotalBsmtSF** of 3206. The basement has two types of finished area: the first type is a GLQ (Good Living Quarters) with **BsmtFinSF1** of 1124, second type is missing with **BsmtFinSF2** of 479, and unfinished area with **BsmtUnfSF** of 1603. We will try to impute this value, but we should use our perception of the problem to judge the correctness of imputed value. We are almost sure that **BsmtFinType2** is NOT:\n+ *NA*: because the house do have a basement.\n+ *Unf*: because the **BsmtFinSF2** doesn't equal **BsmtUnfSF**.\n+ *GLQ*: beacuse it would the same type as **BsmtFinType1**.\n+ *ALQ* or *BLQ*: because <a href=\"http:\/\/rockfordtownshipassessor.net\/faq\/69\/what-is-the-difference-between-rec-room-area-and-finished-basement-living-area\">it can't be a living area since the basement is not exposed<\/a>.<br\/>\nThere's only two possibilities: *Rec* and *LwQ*. Let's figure out which one is more likely by plotting the count of occurrences for each **BsmtFinType2** for houses having the same **BsmtCond** and **BsmtExposure**.","bf6ec7c1":"Data compatibility refers to the issues of comparability that arise from merging two or more datasets. Since we have only one dataset, we may not suffer from common comparability issues such as: different units of measurement, different spelling of names, different currency ... etc. That said, we may suffer from duplicate records. So, let's check if we have any duplicates.","7b50a711":"We will fill these rows by the most common **PoolQC**.","9c14200f":"Since the total pool area for these houses doesn't equal zero, we can conclude that there are one or more houses having missing values in the **PoolQC**. Let's find out the indices of these houses.","3f2a417f":"Now let's give it a try and fit a linear regression model to all features we have and check the residual plots to detect any potential outliers. We need also to select best features for our model and check if data transformation is needed.","95bd4af3":"C) Features describing the garage (**GarageType**, **GarageArea**, **GarageYrBlt**, **GarageFinish**, **GarageQual**, **GarageCond**, and **GarageCars**):\n\nAll these features have 159 NA values except: **GarageType** has 157 NA values, **GarageCars** and **GarageArea** have only 1 NA value.<br\/>\nLet's check whether NAs are consistent across all categorical features and numeric features have zeros. We'll do so by summing up all NAs across each feature and check whether it equals zero or not.","dcae46dc":"4) Two houses with indices 2217, and 2218: have NA values in **BsmtQual** feature. We fill these NAs by the most common  **BsmtQual** for houses having the same **BsmtCond**.","42e4379f":"The residuals vs. fitted values scatter plot shows that the average of residuals, *red line*, is very close to zero, so the linearity assumption is satisfied. But the vertical spread of residuals is not approximately constant. So, the constant variance assumption is also violated.<br\/> The normal probability plot along with the histogram show that the residuals slightly right skewed and have long tails. So, the normality assumption is violated.","0dafeba3":"+ 4) Use training data to identify several candidate models using the best subset feature selection method.\n+ 5) Select and evaluate a few good models:","8fcf56f7":"The probability distribution of the sale price (response variable) is unimodal, right-skewed (having skewness coefficient of 1.88 while skewness of normal distribution is zero), and heavy-tailed (having kurtosis of 6.54 while kurtosis of normal distribution is zero). Heavy-tailed distributions tend to have outliers. It seems there's nothing unexpected: the distribution is right skewed as we expected and the max\/min values are not unusual. So, we can conclude there's no errors to be corrected for this variable.","f12f2367":"Features describing different porch areas (**\"OpenPorchSF\"**, **\"EnclosedPorch\"**, **\"3SsnPorch\"**, **\"ScreenPorch\"**) and pool area (**\"PoolArea\"**) have a third quartile of Zero which means that at least 75% of houses in the dataset don't have neither porch nor pool. So, these features are better combined into fewer number of features or converted into binary features or even discarded at all.","c86c0748":"2) Three houses with index 948, 1487, and 2348: has one missing value in **BsmtExposure** feature. But before we rush to impute this values, let's have a closer look at that feature and try to figure out if something is wrong. According to data description file, the **BsmtExposure** feature is described as: *refers to walkout or garden level walls*. Generally, <a href=\"https:\/\/www.gimme-shelter.com\/basement-type-50085\/\">house with a walkout basement is built into a hill<\/a>. So, inspecting the **LandContour** and **LandSlope** features may help us guess the level of **BsmtExposure**."}}