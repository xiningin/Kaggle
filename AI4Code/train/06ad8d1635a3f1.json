{"cell_type":{"6a85d013":"code","7035b900":"code","4c1992ec":"code","8ce0a231":"code","8439f0f5":"code","e8e23793":"code","e2dd3a25":"code","6405ae72":"code","201f6669":"code","cadb23f6":"code","6c05f97d":"code","342a0f1e":"code","b5fdaa4b":"code","38d4d849":"code","8a5e0013":"code","e2d04667":"code","b3608fd5":"code","0fa32d57":"code","c3ead075":"code","b23ed22b":"code","219c5be0":"code","d50c1b4b":"code","1c45bf7f":"code","848965b7":"code","31897603":"code","5b25e167":"code","0c311194":"code","c411eb19":"code","23f880e2":"code","1f143a77":"code","27ea70f3":"code","69397eb3":"code","bd7883f3":"code","97f69ce9":"code","fca0dadd":"code","8c6593e5":"code","4c0db188":"code","e738fc8a":"code","0f1a54ee":"code","f1ef9360":"code","74926e7a":"code","e642bb38":"code","03c6397a":"code","9141c80b":"code","08a22b10":"code","76891788":"code","b61f3b28":"code","d37f21c6":"code","f35b1dc7":"code","fc184b13":"code","3e9e5d76":"code","5a598169":"code","5df029da":"code","f39506c6":"code","7dd5821e":"code","ebc307f0":"code","e8371f04":"code","952aa2af":"code","9f4e1cfd":"code","a998409e":"code","91a36858":"code","8f2df4f7":"code","5b6a74c9":"code","19510da9":"code","5074efa0":"code","d1bd35c7":"code","1d8d93eb":"code","a98d71ba":"code","922abaa1":"code","34799629":"code","8a7e6fd6":"code","9692882c":"code","dac7024e":"code","9575104f":"code","c5e005f8":"code","456974ea":"code","a9a63335":"code","726afeb4":"code","2bcecef0":"code","c5dd9575":"code","cadeb101":"code","1e57a149":"code","718faf65":"code","d29b650b":"code","8a1590f4":"code","5dd2d2a4":"markdown","3161edbf":"markdown","b86feea4":"markdown","4c1c77b6":"markdown","6dd442bc":"markdown","272f7d2f":"markdown","08b44a0c":"markdown","9b06a7bc":"markdown","327e3555":"markdown","6051332e":"markdown","674cc96c":"markdown","71bedefe":"markdown","049baa15":"markdown","0d6e56bd":"markdown","e8e16056":"markdown","ff9cf7b4":"markdown","12186a16":"markdown","f3158a4f":"markdown","6c46cc06":"markdown","ee6888a7":"markdown","29b45cfe":"markdown","c7f96c78":"markdown","757adb18":"markdown","f2386084":"markdown","d0d46ae6":"markdown","7e31d91f":"markdown","1ca81ebc":"markdown","84c88688":"markdown","a081bcd1":"markdown","d0b89774":"markdown","841008c0":"markdown","4e5d3414":"markdown","047786ee":"markdown","97738c57":"markdown","8d4018b2":"markdown","bdc1fef9":"markdown","7b722a8b":"markdown","c5d12bb1":"markdown","1ef89c72":"markdown","dd35c74e":"markdown","f5f1cf1d":"markdown","f7f04fef":"markdown","b8a260ca":"markdown","38abb9b5":"markdown","b6427fa0":"markdown","fb1241b5":"markdown","729b40ea":"markdown","60a1ca1f":"markdown","591f58e2":"markdown","37d49ae1":"markdown","270e7c50":"markdown","b11de5c1":"markdown","1ff4e2ea":"markdown","7cb0ce5d":"markdown","1b3b108c":"markdown","fd525b81":"markdown","6b6c2159":"markdown","02631e9a":"markdown","9e9db795":"markdown","d765d828":"markdown","08318379":"markdown","475ed9ce":"markdown","e9a5981f":"markdown","26526b3c":"markdown","5dbad43b":"markdown","487d058a":"markdown","85dc03af":"markdown","81504ecf":"markdown","72e5c743":"markdown","18a3e37f":"markdown","ecd7eab2":"markdown","bde25bf7":"markdown","a91c5fd3":"markdown","d32ab3be":"markdown","0072c18b":"markdown","9fbc31f5":"markdown","ecf8ffef":"markdown","d5a56ccb":"markdown","b06c89fe":"markdown","576c3ea4":"markdown","7d9f8eae":"markdown","c89e6f92":"markdown","4a81695f":"markdown"},"source":{"6a85d013":"# Data Analysis\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import boxcox\n\n# Data Visualisation\npd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# Data Modeling\nfrom sklearn.cluster import KMeans\nfrom sklearn.linear_model import LogisticRegression,Perceptron,SGDClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import cross_val_score,RandomizedSearchCV,GridSearchCV\nfrom sklearn.feature_selection import mutual_info_classif,chi2,SelectKBest\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nprint(\"Setup Complete\")","7035b900":"train_data = pd.read_csv('..\/input\/titanic\/train.csv',index_col='PassengerId')\ntest_data = pd.read_csv('..\/input\/titanic\/test.csv',index_col='PassengerId')\ncombine = (train_data,test_data)","4c1992ec":"train_data.head()","8ce0a231":"train_data.info()","8439f0f5":"train_data.isna().sum()\/train_data.shape[0]*100","e8e23793":"test_data.isna().sum()\/test_data.shape[0]*100","e2dd3a25":"missing_feat = ['Age','Cabin','Embarked']\nfor col in missing_feat:\n    data = train_data.copy()\n    # Filling the 1 or 0 depending upon is it null or not\n    data[col] = np.where(data[col].isnull(),1,0)\n    data.groupby(col)['Survived'].median().plot()\n    plt.title(col)\n    plt.show()","6405ae72":"train_data.describe()","201f6669":"train_data.describe(include=['O'])","cadb23f6":"discrete_feat = [col for col in train_data.columns if train_data[col].dtype != 'O' and \n                 train_data[col].nunique()<25]\ndiscrete_feat","6c05f97d":"for feature in discrete_feat:\n    print(feature,' Has ',train_data[feature].unique(),' Unique Values.')","342a0f1e":"for feature in discrete_feat:\n    data = train_data.copy()\n    data.groupby(feature)['Survived'].mean().plot.bar()\n    plt.title(feature)\n    plt.xlabel(feature)\n    plt.show()","b5fdaa4b":"continous_feat = [col for col in train_data.select_dtypes(exclude='O') if col not in discrete_feat]\ncontinous_feat","38d4d849":"for feature in continous_feat:\n    data  = train_data.copy()\n    data[feature].hist()\n    plt.title(feature)\n    plt.show()\n    print(data[feature].skew())","8a5e0013":"for feature in continous_feat:\n    data = train_data.copy()\n    sns.regplot(x=data[feature],y=data['Survived'])\n    plt.title(feature)\n    plt.show()","e2d04667":"# copy the master data\ndata = train_data.copy()\n\n# Filling missing values wth median \ndata['Age'].fillna(data['Age'].median(),inplace=True)\n\n# printing the skewness\nprint('Skewness :',data['Age'].skew())\n\n# Converting <=0 value to 1 for log operation\ndata.loc[data['Age']<=0,'Fare'] = 1\n\n# finding the interquatile range for detecting outliers\nior = data['Age'].quantile(.75)-data['Age'].quantile(.25)\n\n# setting the value lower bound and upper bound\nlower_b = data[\"Age\"].quantile(0.25)-ior*1.5\nlower_b = 0 if lower_b < 0 else lower_b\nupper_b = data['Age'].quantile(0.75)+ior*1.5\n\n# printing bounds\nprint('Lower bound :',lower_b,'\\nUpper bound :',upper_b)\n\n# count outliers in data\ndata['Age'] = np.where((data['Age'] > lower_b) & (data['Age'] < upper_b),0,1 )\nprint('Outliers in Age',len(data[data['Age']==1]))\nprint('Overview of outliers\\n',train_data.loc[data['Age'] == 1,'Age'])","b3608fd5":"# copy the master data\ndata = train_data.copy()\n\n# fill nan values with median\ndata['Fare'].fillna(data['Fare'].median(),inplace=True)\n\n# Converting <=0 value to 1 for log operation\ndata.loc[data['Fare']<=0,'Fare'] = 1\n\n# printing the before using log\nprint('before log :',data['Fare'].skew())\n\n# converting the data using log\ndata.Fare = np.log(data.Fare)  # pd.Series(boxcox(data.Fare)[0])\n\n# printing the after using log\nprint('after log :',data['Fare'].skew())\n\n# finding the interquatile range for detecting outliers\nior = data['Fare'].quantile(.75)-data['Fare'].quantile(.25)\n\n# setting the lower and upper bounds\nlower_b = data['Fare'].quantile(0.25)-ior*1.5\nlower_b = 0 if lower_b < 0 else lower_b\nupper_b = data['Fare'].quantile(0.75)+ior*1.5\n\n# printing lower and upper bounds\nprint('Lower bound :',lower_b,'\\nUpper bound :',upper_b)\n\n# counting the outliers in data\ndata['Fare'] = np.where((data['Fare'] > lower_b) & (data['Fare'] < upper_b),0,1 )\nprint('Outliers in Fare',len(data[data['Fare']==1]))\nprint('Overview of outliers\\n',train_data.loc[data['Fare'] == 1,'Fare'])","0fa32d57":"categical_feat = [col for col in train_data.select_dtypes(include='O')]\ntrain_data[categical_feat].head()","c3ead075":"print('Number of Unique values')\nfor col in categical_feat:\n    print(col,' : ',train_data[col].nunique())","b23ed22b":"# coping the master data\ndata = train_data.copy()\n\n# saving the index number of the unique Tickets\nunique_tickets = pd.concat(i for _, i in data.groupby(\"Ticket\") if len(i) == 1).index\n\n# Create new features\ndata['UniqueTicket'] = 0\n\n# set 1 to those have unique tickets\ndata.loc[unique_tickets,'UniqueTicket'] = 1\n\n# ploting the data and see the correlation\nsns.regplot(y=data.UniqueTicket,x=data.Survived)\nplt.show()\nSu = data.Survived\ncorr = data.drop('Survived',axis=1).corrwith(Su)\nax = corr.plot.bar(rot=40)","219c5be0":"data=train_data.copy()\n# Extracting out the title like Mr. Miss. etc.\ndata['Name'] = data['Name'].apply(lambda x: x.split(',')[1].split('.')[0])\nfor feature in categical_feat :\n    if data[feature].nunique()< 25 :\n        data.groupby(feature)['Survived'].mean().plot.bar()\n        plt.title(feature)\n        plt.show()","d50c1b4b":"# copy the master data\ndata = train_data.copy()\n\n# putting 1 where any one has a cabin otherwise 0\ndata.Cabin = np.where(data.Cabin.isnull() == True,0,1)\n\n# ploting the data find the correlation with survivers\nsns.regplot(x=data.Cabin,y=data.Survived)\nplt.show()\nSu = data.Survived\ncorr = data.drop('Survived',axis=1).corrwith(Su)\nax = corr.plot.bar(rot=40)","1c45bf7f":"Su = train_data.Survived\ncorr = train_data.drop('Survived',axis=1).corrwith(Su)\nax = corr.plot.bar(rot=40)","848965b7":"# finding categories who have mising values of training dataset\ntrain_mising_value = [col for col in train_data.columns if train_data[col].isnull().sum() != 0]\ntrain_mising_value","31897603":"# finding categories who have mising values of test dataset\ntest_mising_value = [col for col in test_data.columns if test_data[col].isnull().sum() != 0]\ntest_mising_value","5b25e167":"for data in combine:\n    # group by Sex And Pclass and find their mean\n    group = data.groupby(['Sex','Pclass'])['Age'].transform('mean')\n    group = pd.DataFrame(group)\n    \n    # putt every mean where Age is null\n    data.Age = np.where(data.Age.isnull() == True,group.Age,data.Age)","0c311194":"test_data.Fare.fillna(test_data.Fare.median(),inplace=True)","c411eb19":"for data in combine:\n    data['Cabin'].fillna('NA',inplace=True)","23f880e2":"train_data.Embarked.fillna('S',inplace=True) # 'S' is most frequent object in this feature","1f143a77":"test_data.isnull().sum()","27ea70f3":"train_data.isnull().sum()","69397eb3":"for data in combine:\n    # extracting Title (eg. Mr,Mrs,Miss etc)\n    data['Title'] = data.Name.apply(lambda name: name.split(',')[1].split('.')[0].strip())","bd7883f3":"# Assigning every title with a number from the observation in EDA part of Name\ntitle_map = {\n    \"Capt\":       4,\n    \"Col\":        2,\n    \"Major\":      2,          \n    \"Jonkheer\":   0, \n    \"Don\":        0,\n    \"Sir\" :       0,\n    \"Dr\":         2,\n    \"Rev\":        4,\n    \"the Countess\":0,\n    \"Dona\":       0,\n    \"Mme\":        0,\n    \"Mlle\":       0,\n    \"Ms\":         0,\n    \"Mr\" :        3,\n    \"Mrs\" :       1,\n    \"Miss\" :      1,\n    \"Master\" :    2,\n    \"Lady\" :      0\n}\n\nfor data in combine:\n    data.Title = data.Title.map(title_map)\n    print(data.Title.value_counts())\n    print('='*30)","97f69ce9":"for data in combine:\n    data['Relatives'] =data['SibSp'] + data['Parch']\n    print(data.Relatives.value_counts())\n    print('='*30)","fca0dadd":"for data in combine:\n    # assigning 1 who have no relative\n    data['Alone'] = 0\n    data.loc[data.Relatives == 0,'Alone'] = 1\n    print(data.Alone.value_counts())\n    print('='*30)","8c6593e5":"for data in combine:\n    # assigning 1 who have Parents or children\n    data['HaveParch'] = np.where(data.Parch > 0,1,0)\n    print(data.HaveParch.value_counts())\n    print('='*30)","4c0db188":"for data in combine:\n    # assigning 1 who have spouse or sibling\n    data['HaveSibSp'] = np.where(data.SibSp > 0,1,0)\n    print(data.HaveSibSp.value_counts())\n    print('='*30)","e738fc8a":"for data in combine:\n    unique_tickets = pd.concat(i for _, i in data.groupby(\"Ticket\") if len(i) == 1).index\n    data['UniqueTicket'] = 0\n    data.loc[unique_tickets,'UniqueTicket'] = 1\n    print(data.UniqueTicket.value_counts())\n    print('='*30)","0f1a54ee":"train_data['HaveCabin'] = np.where(train_data.Cabin == 'NA',0,1)\ntest_data['HaveCabin'] = np.where(test_data.Cabin == 'NA',0,1)","f1ef9360":"train_data['ZeroFare'] = np.where(train_data.Fare == 0,1,0)\ntest_data['ZeroFare'] = np.where(test_data.Fare == 0,1,0)","74926e7a":"train_data.info()","e642bb38":"# print null value count\nprint(\"Null values:\", train_data['Survived'].isnull().sum())\n\n# print list of unique values to check for anything unusual\nprint(\"Unique values:\", train_data['Survived'].unique())","03c6397a":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Pclass'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Pclass'].unique())","9141c80b":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['SibSp'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['SibSp'].unique())\n","08a22b10":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Parch'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Parch'].unique())\n","76891788":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Relatives'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Relatives'].unique())\n","b61f3b28":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['UniqueTicket'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['UniqueTicket'].unique())\n","d37f21c6":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Alone'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Alone'].unique())\n","f35b1dc7":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Title'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Title'].unique())","fc184b13":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['HaveParch'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['HaveParch'].unique())","3e9e5d76":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['HaveSibSp'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['HaveSibSp'].unique())","5a598169":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['HaveCabin'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['HaveCabin'].unique())","5df029da":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['ZeroFare'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['ZeroFare'].unique())","f39506c6":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Age'].isnull().sum())\n    \n    # printing the skewness of the data\n    print('Skewness of the data :',data.Age.skew())\n    \n    # Calculate the interquantile Range and print it\n    ior = data.Age.quantile(.75) - data.Age.quantile(.25)\n    print('Interquantile Range : ',ior)\n    \n    # Calculating lower and upper bound for age and print it\n    lower_b = 0 if data.Age.quantile(.25) - ior*1.5 < 0 else data.Age.quantile(.25) - ior*1.5\n    upper_b = data.Age.quantile(.75) + ior*1.5\n    print('Lower Bound :',lower_b,'Upper Bound :',upper_b)\n    \n    # detecting the outliers and fill them with the lower and upper bound value\n    data.Age = np.where(data.Age < lower_b,lower_b,data.Age)\n    data.Age = np.where(data.Age > upper_b,upper_b,data.Age)\n    \n    # Making continous values categical\n    data.Age = pd.cut(data.Age,bins=[0,10,20,35,55,60],labels=['infant','teenage','adult','elder','old'])\n","7dd5821e":"# making one hot encoding\ntrain_data = train_data.join(pd.get_dummies(train_data.Age,prefix='age'))\ntest_data = test_data.join(pd.get_dummies(test_data.Age,prefix='age'))","ebc307f0":"for data in [train_data,test_data]:\n    # print null value count\n    print(\"Null values:\", data['Fare'].isnull().sum())\n    \n    # Converting <=0 values to 1 for log\n    data.loc[data.Fare <= 0,'Fare'] = 1\n    \n    # printing skewness before apply log\n    print('before apply :',data.Fare.skew())\n    \n    # apply log\n    data.Fare = np.log(data.Fare)\n    \n    # printing skewness before apply log\n    print('after apply :',data.Fare.skew())\n    \n    # Calculate IOR and print it\n    ior = data.Fare.quantile(.75) - data.Fare.quantile(.25)\n    print('IOR Value : ',ior)\n    \n    # Calculate the lower and upper bond of fare and print it\n    lower_b = 0 if data.Fare.quantile(.25) - ior*1.5 < 0 else data.Fare.quantile(.25) - ior*1.5\n    upper_b = data.Fare.quantile(.75) + ior*3\n    print('Lower Bound :',lower_b,'Upper Bound :',upper_b)\n    \n    # detecting outliers and fill them with lower and upper bound values\n    data.Fare = np.where(data.Fare < lower_b,lower_b,data.Fare)\n    data.Fare = np.where(data.Fare > upper_b,upper_b,data.Fare)","e8371f04":"for data in [train_data,test_data]:\n    # making Categorical\n    a = data.Fare.quantile(0)\n    b = data.Fare.quantile(0.25)\n    c = data.Fare.quantile(0.5)\n    d = data.Fare.quantile(0.75)\n    e = data.Fare.quantile(1)\n    data.Fare = pd.cut(data.Fare,bins=[a,b,c,d,e],labels=['a','b','c','d'])","952aa2af":"# make one hot encoding\ntrain_data = train_data.join(pd.get_dummies(train_data.Fare,prefix='fare'))\ntest_data = test_data.join(pd.get_dummies(test_data.Fare,prefix='fare'))","9f4e1cfd":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data['Sex'].isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data['Sex'].unique())","a998409e":"# applying one hot encoding on sex\nx = pd.get_dummies(train_data.Sex,prefix='Cat')\ntrain_data = train_data.join(x)\nx = pd.get_dummies(test_data.Sex,prefix='Cat')\ntest_data = test_data.join(x)","91a36858":"for data in combine:\n    # print null value count\n    print(\"Null values:\", data.Embarked.isnull().sum())\n\n    # print list of unique values to check for anything unusual\n    print(\"Unique values:\", data.Embarked.unique())","8f2df4f7":"# applying one hot encoding on Embarked\ntrain_data = train_data.join(pd.get_dummies(train_data.Embarked,prefix='Cat'))\ntest_data = test_data.join(pd.get_dummies(test_data.Embarked,prefix='Cat'))","5b6a74c9":"train_data.to_csv('preprocessed_train_data.csv')\ntest_data.to_csv('preprocessed_test_data.csv')","19510da9":"train_data = pd.read_csv('preprocessed_train_data.csv')\ntest_data = pd.read_csv('preprocessed_test_data.csv')","5074efa0":"survived = train_data.Survived\nplt.figure(figsize=(10,6))\ncorr = train_data.drop('Survived',axis=1).corrwith(survived)\ncorr = abs(corr)\ncorr.sort_values(ascending=False,inplace=True)\nax = corr.plot.bar(rot = 40)\n","d1bd35c7":"X_train = train_data.drop('Survived',axis=1)\ny_train = train_data.Survived","1d8d93eb":"removed = [feature for feature in X_train.select_dtypes(include=['object','category',])]\nremoved","a98d71ba":"X_train.drop(removed,axis=1,inplace=True)\ntest_data.drop(removed,axis=1,inplace=True)","922abaa1":"select = SelectKBest(score_func=chi2,k=20)\nrank = select.fit(X_train,y_train)\ncol = pd.Series(rank.scores_,index=X_train.columns)\ncol.sort_values(ascending=False).plot.bar()\nprint(col.sort_values(ascending=False))\nprint('need to remove :',col.sort_values(ascending=True).index[0:15])","34799629":"removed =['age_elder', 'age_teenage', 'Cat_Q', 'Relatives', 'age_adult',\n       'age_old', 'SibSp', 'PassengerId', 'fare_c', 'Cat_S', 'fare_b',\n       'ZeroFare', 'HaveSibSp', 'Parch', 'age_infant','Alone']\nX_train.drop(removed,axis=1,inplace=True)\ntest_data.drop(removed,axis=1,inplace=True)","8a7e6fd6":"model = LogisticRegression()\nacc_lgr = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_lgr.mean()*100)","9692882c":"model =SVC()\nacc_svc = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_svc.mean()*100)","dac7024e":"model = KNeighborsClassifier(n_neighbors=10)\nacc_knn = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_knn.mean()*100)","9575104f":"model = GaussianNB()\nacc_gnb = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_gnb.mean()*100)","c5e005f8":"model = Perceptron()\nacc_per = acc_gnb = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_per.mean()*100)","456974ea":"model = LinearSVC()\nacc_lsvc = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_lsvc.mean()*100)","a9a63335":"model = SGDClassifier()\nacc_sgd = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_sgd.mean()*100)","726afeb4":"model = DecisionTreeClassifier(max_depth=10)\nacc_dtc = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_dtc.mean()*100)","2bcecef0":"model = RandomForestClassifier(n_estimators=300,criterion='entropy')\nacc_rfc = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_rfc.mean()*100)","c5dd9575":"model = XGBClassifier(n_estimators=1000,learning_rate=0.02,verbosity=0)\nacc_xgb = cross_val_score(model,X_train,y_train,cv=10,scoring='accuracy')\nprint(acc_xgb.mean()*100)","cadeb101":"models = pd.DataFrame(\n{\n    'Model':['Logistic Regression','Support Vector Machine','K Neighbors Classifier',\n             'Gaussian Naive Bayes','Perceptron','Linear SVC','Stochastic Gradient Descent',\n             'Decision Tree','Random Forest Classifier','Gradient Boosting'],\n    'Accuracy':[acc_lgr,acc_svc,acc_knn,acc_gnb,acc_per,acc_lsvc,acc_sgd,acc_dtc,acc_rfc,acc_xgb]\n})\nmodels.Accuracy = models.Accuracy.apply(lambda x: x.mean()*100)\nmodels.sort_values(by='Accuracy',ascending=False,inplace=True)\nmodels","1e57a149":"sns.set_style('whitegrid')\nplt.figure(figsize=(10,8))\nplot = sns.barplot(y=models.Model,x=models.Accuracy)","718faf65":"model = XGBClassifier()\nparam = [{'booster':['gbtree','dert'],\n          'n_estimators':[1,10,100,500,1000,1500],\n          'learning_rate':[0.02,0.05,0.1,0.002,0.005],\n          'max_depth':[2,3,4,5,7,9,10,20,30,50],\n          'gamma':[0.1,0.3,0.5,0.7,0.9,0.001,0.002],\n          'verbosity':[0]\n         }]\ngrd = RandomizedSearchCV(estimator=model,\n                   param_distributions=param,\n                   cv=10,\n                   scoring='accuracy'\n        )\ngrd.fit(X_train,y_train)\nprint(grd.best_score_)\nprint(grd.best_estimator_)","d29b650b":"model =XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n              colsample_bynode=1, colsample_bytree=1, gamma=0.002, gpu_id=-1,\n              importance_type='gain', interaction_constraints='',\n              learning_rate=0.005, max_delta_step=0, max_depth=50,\n              min_child_weight=1, monotone_constraints='()',\n              n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n              tree_method='exact', validate_parameters=1, verbosity=0)\nmodel.fit(X_train,y_train)\npred = model.predict(test_data)","8a1590f4":"gender = pd.read_csv('..\/input\/titanic\/gender_submission.csv')\noutput = pd.DataFrame({'PassengerId':gender.PassengerId,'Survived':pred})\noutput.to_csv('submission.csv',index=False)","5dd2d2a4":"Let's Check correlation with Name via Title","3161edbf":"### ZeroFare","b86feea4":"### Survived","4c1c77b6":"### **Numerical features**","6dd442bc":"### Parch","272f7d2f":"### **Support Vector Machines**","08b44a0c":"#### **Loading The Data**","9b06a7bc":"### **Unique Ticket**","327e3555":"### HaveParch","6051332e":"Check outliers of Age ","674cc96c":"## **Data Cleaning**","71bedefe":"## **Final Fitting and Submission**","049baa15":"Let's see continous features distribution or they have any kind of skewness","0d6e56bd":"### **Gaussian Naive Bayes**","e8e16056":"### **Perceptron**","ff9cf7b4":"## **Model the Data and try Different Algorithm**","12186a16":"#### Features need to be removed","f3158a4f":"### **Parent or children**","6c46cc06":"### **Random Forest Classifier**","ee6888a7":"##### Lets see everything is fine or not","29b45cfe":"Get each and every continous features","c7f96c78":"Get every discrete value","757adb18":"### UniqueTicket","f2386084":"### **Sex**","d0d46ae6":"Checking outliers of Fare","7e31d91f":"Getting Categorical Features","1ca81ebc":"### Pclass","84c88688":"### **Decision Tree**","a081bcd1":"### **Have Cabin**","d0b89774":"### **Title**","841008c0":"# **Titanic Assignment Solution**","4e5d3414":" Statistical description of the features","047786ee":"### **Alone**","97738c57":"Let's see overall correlation with Survivers ","8d4018b2":"\n## **Categorical Features**\n\nCategorical - Sex, Embarked\n\n##### Data cleaning requirements for continuous data can be determined by:\u00b6\n\n* count null values \n* see unique values \n* apply one hot encoding\n","bdc1fef9":"### **Relatives**","7b722a8b":"### HaveSibSp","c5d12bb1":"## **Model Ranking**","1ef89c72":"#### **Preview The Data**","dd35c74e":"### Cabin","f5f1cf1d":"### Zero Fare","f7f04fef":"#### After feature engineering, our feature set has expanded to:\n\n*     Discrete - Survived, Pclass, Sibsp, Parch, Relatives, Alone, UniqueTicket, Title, HaveParch, HaveSibSp, HaveCabin\n*     Continuous - Age, Fare\n*     Categorical - Sex, Embarked\n*     Others - Name, Tickets, Cabin  \n","b8a260ca":"### **Spouse or Sibling**","38abb9b5":"let's see each discrete features impact on the survivers","b6427fa0":"    so now, we can say there are no missing values ","fb1241b5":"### Embarked","729b40ea":"# **Feature Engineering**\n*  **Imputing Missing Values**\n*  **Temporal Variables**\n\n### **Data Cleaning**\n*  **Categorical Variable, Remove Rare Labels**\n*  **Lessen the Skewness of Continous Variable**\n*  **Standartise the values if Required**","60a1ca1f":"## **Discrete Features**\n\nDiscrete - Survived, Pclass, Sibsp, Parch, Relatives, Alone, UniqueTicket, Title, HaveParch, HaveSibSp, HaveCabin, ZeroFare\n\n##### Data cleaning requirements for discrete data can be determined by:\n* find null value\n* check there unique values","591f58e2":"### **Gradient Boosting**","37d49ae1":"    Embarked and Cabin has an effect on the survivers rate but age doesn't but we keep the until \n    \n    feture selection","270e7c50":"#### Separate target value","b11de5c1":"Checking the missing values are making any difference or not","1ff4e2ea":"# **Feature Selection**\n* Features are selected on their mutual info value","7cb0ce5d":"### ***Saving The Preprosed Data***","1b3b108c":"## loading preprocessed data","fd525b81":"Let's see Cabin correlation with Survivers","6b6c2159":"### **K Neighbors Classifier**","02631e9a":"### **Imputing Missing Values**\n* Find features who have missing value\n* Imputing the missing value one by one","9e9db795":"    As you can see in the above Age skewness is ok but Fare is skewness is not acceptable. ","d765d828":"### Fare","08318379":"\n## **Continuous Features**\n\nContinuous - Age, Fare\n\n##### Data cleaning requirements for continuous data can be determined by:\n\n*     If Skewed then apply log \n*     and detect and remove outliers\n\n","475ed9ce":"Let's see there number of unique values","e9a5981f":"### HaveCabin","26526b3c":"### Age","5dbad43b":"### Sibsp","487d058a":"### **Logistic Regression**","85dc03af":"# **Exaploratory Data Analysis**\n*  **Missing Values**\n*  **All Numerical Values**\n*  **Distribution of Numerical values**\n*  **Categorical Values**\n*  **Cardinality of Categorical Variables**\n*  **Relationship between Independent & Dependent Features**","81504ecf":"### **Linear SVC**","72e5c743":"    Clearly we can see the the plot showing the effect categorical values on survivers ","18a3e37f":"let's see each every discrete features unique values","ecd7eab2":"### Age","bde25bf7":"### **Embarked**","a91c5fd3":"    As you can see in the plots that the every feature has an effect on the survived ","d32ab3be":"#### **Basic Summary of the Data Set**","0072c18b":"### Fare","9fbc31f5":"### **Stochastic Gradient Descent**","ecf8ffef":"### Alone","d5a56ccb":"### Title","b06c89fe":" Finding % of missing values of each features","576c3ea4":"### **Temporal features**\n* We will create some Temporal features depending upon original features","7d9f8eae":"Let's check tickets correlation with survivers","c89e6f92":"Important Libaries need to import","4a81695f":"### Relatives\n"}}