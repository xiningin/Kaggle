{"cell_type":{"6b050bf9":"code","a1f67afa":"code","bf2965f6":"code","d858ff7b":"code","2bdbb41f":"code","c545ae87":"code","a6b74c63":"code","bab9bbf2":"code","d4863ff9":"code","f6a16fd7":"code","546f90b5":"code","5d37fed5":"code","f7146a94":"code","135d0d1d":"code","fa5946b2":"code","9f68d18b":"code","0d62b88c":"code","cd52dbcd":"code","0cb72ff7":"markdown","227afdef":"markdown","2da01b1b":"markdown","adc8d9eb":"markdown","8cb10e70":"markdown","e4beca29":"markdown","a57c7805":"markdown","fc7538e8":"markdown","1dfc69fb":"markdown","622361b0":"markdown","6cb3c877":"markdown","0c5b9ccc":"markdown","976d44aa":"markdown"},"source":{"6b050bf9":"!pip install scikit-learn-intelex -q --progress-bar off","a1f67afa":"from sklearnex import patch_sklearn, unpatch_sklearn","bf2965f6":"import numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport time\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","d858ff7b":"train = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/test.csv')\nsample_submission = pd.read_csv('..\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\ny_train = train['target']\nx_train = train.drop(['id','target'], axis=1)\nx_test = test.drop(['id'], axis=1)\n\nfrom sklearn.model_selection import train_test_split\nx_train_sub, x_val, y_train_sub, y_val = train_test_split(x_train, y_train, test_size=0.2, random_state=42)\nprint(x_train_sub.shape, x_val.shape)","2bdbb41f":"from sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False).fit(pd.concat([x_train, x_test]))\nx_train_sub_onehot = encoder.transform(x_train_sub)\nx_val_onehot = encoder.transform(x_val)\nx_test_onehot = encoder.transform(x_test)","c545ae87":"patch_sklearn()\n\nfrom sklearn.decomposition import PCA\npca_full = PCA(random_state=0).fit(x_train_sub_onehot)","a6b74c63":"plt.rcParams[\"figure.figsize\"] = (15,6)\n\nfig, ax = plt.subplots()\nxi = np.arange(1, x_train_sub_onehot.shape[1] + 1, step=1)\ny = np.cumsum(pca_full.explained_variance_ratio_)\n\nplt.ylim(0.0,1.1)\nplt.plot(xi, y, color='b')\n\nplt.xlabel('Number of Components')\n\nplt.title('Explained variance')\n\nplt.axhline(y=0.95, color='r', linestyle='--')\nplt.text(2250, 0.85, '95% cut-off threshold', color='red', fontsize=16)\n\nax.grid(axis='x')\nplt.show()","bab9bbf2":"pca = PCA(n_components=650, random_state=0).fit(x_train_sub_onehot)\nx_train_sub_pca = pca.transform(x_train_sub_onehot)\nx_val_pca = pca.transform(x_val_onehot)\nx_test_pca = pca.transform(x_test_onehot)","d4863ff9":"del x_train_sub_onehot\ndel x_val_onehot\ndel x_test_onehot","f6a16fd7":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler().fit(x_train_sub_pca)\nx_train_sub_norm = scaler.transform(x_train_sub_pca)\nx_val_norm = scaler.transform(x_val_pca)\nx_test_norm = scaler.transform(x_test_pca)","546f90b5":"del x_train_sub_pca\ndel x_val_pca\ndel x_test_pca","5d37fed5":"from sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import StackingClassifier\nfrom sklearn.preprocessing import QuantileTransformer\n\ndef get_stacking_classifier(C1=None,\n                            n_neighbors=None,\n                            n_estimators=None, min_samples_split=None, min_samples_leaf=None,\n                            n_quantiles=None, C2=None):\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.ensemble import RandomForestClassifier\n    from sklearn.neighbors import KNeighborsClassifier\n\n    log_reg = LogisticRegression(C=C1)\n    knn = KNeighborsClassifier(n_neighbors=n_neighbors)\n    rf = RandomForestClassifier(n_estimators=n_estimators, min_samples_split=min_samples_split,\n                                min_samples_leaf=min_samples_leaf, random_state=0)\n    log_reg_quantile = Pipeline([\n        ('quantile', QuantileTransformer(n_quantiles=n_quantiles, random_state=0)),\n        ('logreg', LogisticRegression(C=C2))\n    ])\n    \n    stacking_estimators = [\n        ('log_reg', log_reg),\n        ('knn', knn),\n        ('rf', rf)\n    ]\n    \n    return StackingClassifier(estimators=stacking_estimators, final_estimator=log_reg_quantile)","f7146a94":"best_params = {\n    'C1': 0.0031483991304676337,\n    'n_neighbors': 23,\n    'n_estimators': 448,\n    'min_samples_split': 10,\n    'min_samples_leaf': 6,\n    'n_quantiles': 3,\n    'C2': 0.9808974699196531,\n}","135d0d1d":"patch_sklearn()\n\nclassifier = get_stacking_classifier(**best_params)\nt0 = time.time()\nclassifier.fit(x_train_sub_norm, y_train_sub)\nt1 = time.time()\ny_pred = classifier.predict_proba(x_val_norm)\nt2 = time.time()","fa5946b2":"from sklearn.metrics import log_loss\nprint(f'fit time: {t1-t0} sec')\nprint(f'predict_proba time: {t2-t1} sec')\nprint(f\"Metric value: {log_loss(y_val, y_pred)}\")","9f68d18b":"unpatch_sklearn()\n\nclassifier = get_stacking_classifier(**best_params)\nt0 = time.time()\nclassifier.fit(x_train_sub_norm, y_train_sub)\nt1 = time.time()\ny_pred = classifier.predict_proba(x_val_norm)\nt2 = time.time()","0d62b88c":"print(f'fit time: {t1-t0} sec')\nprint(f'predict_proba time: {t2-t1} sec')\nprint(f\"Metric value: {log_loss(y_val, y_pred)}\")","cd52dbcd":"patch_sklearn()\n\nclassifier = get_stacking_classifier(**best_params)\nclassifier.fit(np.vstack((x_train_sub_norm, x_val_norm)), pd.concat([y_train_sub, y_val]))\ny_pred = classifier.predict_proba(x_test_norm)\n\nsample_submission[['Class_1','Class_2', 'Class_3', 'Class_4','Class_5','Class_6', 'Class_7', 'Class_8', 'Class_9']] = y_pred\nsample_submission.to_csv('submission.csv', index=False)","0cb72ff7":"# \u2699\ufe0f Best parameters\nThis set of parameters was found by the search on the grid of parameters","227afdef":"<br>\n<h1 style = \"font-size:25px ; font-weight : bold; color : #020296; text-align: center; border-radius: 10px 15px;\"> \ud83d\ude80 Fast Stacking with Intel(R) Extension for Scikit-learn  <\/h1>\n<br>\n\nFor classical machine learning algorithms, we often use the most popular Python library, Scikit-learn. We use it to fit models and search for optimal parameters, but\u202fscikit-learn\u202fsometimes works for hours, if not days. Speeding up this process is something anyone who uses Scikit-learn would be interested in.\n\nI want to show you how to get results faster without changing the code. To do this, we will use another Python library,\u202f**[Intel(R) Extension for Scikit-learn](https:\/\/github.com\/intel\/scikit-learn-intelex)**. It accelerates Scikit-learn and does not require you changing the code written for scikit-learn.\n\nI will show you how to speed up your kernel from **5 hours to 1.5 hours** without changes of code!","2da01b1b":"# \ud83d\ude9d Fit model with Intel(R) Extension for Scikit-learn","adc8d9eb":"## One-hot encoding","8cb10e70":"# \ud83d\udd0d Defining model and parameters for search optimal model\n\nThe model is a stacking classifier with logistic regression, kNN, random forest, and a pipeline of QuantileTransformer and another logistic regression as a final estimator","e4beca29":"# \ud83d\udcdc Conclusions\n\nWith Intel(R) Extension for Scikit-learn patching you can:\n\n- Use your scikit-learn code for training and inference without modification;\n- Train and predict scikit-learn models and get more time for experiments;\n- Get the same quality of predictions.\n\n*Please, upvote if you like.*","a57c7805":"# \ud83d\udd28 Installing Intel(R) Extension for Scikit-learn\n\nLet's try to use Intel(R) Extension for Scikit-learn. First, download it. Package also avaialble in conda - please refer to details https:\/\/github.com\/intel\/scikit-learn-intelex","fc7538e8":"## PCA","1dfc69fb":"# \ud83d\ude82 Fit model with original Scikit-learn","622361b0":"# \ud83c\udfaf Fit final model and submit result","6cb3c877":"## Normalization","0c5b9ccc":"# \ud83d\udccb Reading data and splitting on training and validation datasets","976d44aa":"# \ud83d\udcca Data preprocessing"}}