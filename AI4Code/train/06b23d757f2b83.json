{"cell_type":{"949f5075":"code","07956838":"code","1b4a3c69":"code","87f100ec":"code","0efcb5a5":"code","76275584":"code","010aa857":"code","07fbcb40":"code","6a70bcb9":"code","bd6359ca":"code","81d38b9d":"code","e447d9c1":"code","f365029c":"code","974c6f2f":"code","7d465f84":"code","80681690":"code","25f544e6":"code","f69bcffd":"code","c0e0767c":"code","ffa4ce52":"code","7757517e":"code","9057aa1e":"code","8a550d15":"markdown","9cce315f":"markdown","7d6b9738":"markdown","12398df1":"markdown","d1b4abcd":"markdown","2152ce39":"markdown","a1cd8b56":"markdown","d0131451":"markdown","0bb4c88c":"markdown","0b8248b8":"markdown","92206224":"markdown","4a301070":"markdown","a693c8a7":"markdown","31a869c7":"markdown","cc660161":"markdown","d5a96272":"markdown"},"source":{"949f5075":"import keras\nimport datetime\nimport tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport os\nimport matplotlib.pyplot as plt\nfrom keras import regularizers\nfrom keras.utils import plot_model\nfrom keras.callbacks import ModelCheckpoint, CSVLogger, TensorBoard, EarlyStopping, ReduceLROnPlateau\nfrom keras.preprocessing.image import ImageDataGenerator, load_img\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.optimizers import Adam, Nadam\nfrom sklearn.metrics import classification_report, confusion_matrix","07956838":"nb_classes         = 7\nimg_rows, img_cols = 48, 48\nbatch_size         = 32\n\ntrain_data_dir   = '..\/input\/fer2013\/train\/'\ntest_data_dir    = '..\/input\/fer2013\/test\/'","1b4a3c69":"train_datagen = ImageDataGenerator(\n    rescale=1.\/255,\n    zoom_range=0.3,\n    horizontal_flip=True\n)\n\ntrain_set = train_datagen.flow_from_directory(\n  train_data_dir,\n  color_mode  = 'grayscale',\n  target_size = (img_rows, img_cols),\n  batch_size  = batch_size,\n  class_mode  = 'categorical',\n  shuffle     = True\n)","87f100ec":"test_datagen = ImageDataGenerator(rescale = 1.\/255)\n\ntest_set = test_datagen.flow_from_directory(\n\ttest_data_dir,\n  color_mode  = 'grayscale',\n  target_size = (img_rows, img_cols),\n  batch_size  = batch_size,\n  class_mode  = 'categorical',\n  shuffle     = True\n)","0efcb5a5":"train_set.class_indices","76275584":"def count_exp(path, set_):\n    dict_ = {}\n    for expression in os.listdir(path):\n        dir_ = path + expression\n        dict_[expression] = len(os.listdir(dir_))\n    df = pd.DataFrame(dict_, index=[set_])\n    return df\n\ntrain_count = count_exp(train_data_dir, 'train')\ntest_count  = count_exp(test_data_dir, 'test')\n\nprint(train_count)\nprint(test_count)","010aa857":"# Plotting training data dist\ntrain_count.transpose().plot(kind = 'bar')","07fbcb40":"# Plotting testing data dist\ntest_count.transpose().plot(kind = 'bar')","6a70bcb9":"def plot_imgs(item_dir, top = 10):\n    all_item_dirs = os.listdir(item_dir)\n    item_files    = [os.path.join(item_dir, file) for file in all_item_dirs][:5]\n  \n    plt.figure(figsize = (10, 10))\n  \n    for idx, img_path in enumerate(item_files):\n        plt.subplot(5, 5, idx + 1)\n    \n        img = plt.imread(img_path)\n        plt.tight_layout()         \n        plt.imshow(img, cmap = 'gray')","bd6359ca":"plot_imgs(train_data_dir + 'happy')","81d38b9d":"plot_imgs(train_data_dir + 'angry')","e447d9c1":"plot_imgs(train_data_dir + 'sad')","f365029c":"def build_model(nb_classes, input_shape):\n\n  model= tf.keras.models.Sequential()\n  model.add(Conv2D(32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(48, 48,1)))\n  model.add(Conv2D(64,(3,3), padding='same', activation='relu' ))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n\n  model.add(Conv2D(128,(5,5), padding='same', activation='relu'))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n    \n  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n\n  model.add(Conv2D(512,(3,3), padding='same', activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(MaxPooling2D(pool_size=(2, 2)))\n  model.add(Dropout(0.25))\n\n  #Faltten the model\n  model.add(Flatten())\n    \n  model.add(Dense(256))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(Dropout(0.25))\n    \n  model.add(Dense(512))\n  model.add(BatchNormalization())\n  model.add(Activation('relu'))\n  model.add(Dropout(0.25))\n\n  model.add(Dense(nb_classes, activation='softmax'))\n\n  model.compile(\n    optimizer = Adam(lr=0.0001 , decay=1e-6), \n    loss='categorical_crossentropy', \n    metrics=['accuracy']\n  )\n\n  return model","974c6f2f":"# Creating an instance of the model and printing the summary\nmodel = build_model(nb_classes, (img_rows, img_cols, 1))\nprint(model.summary())","7d465f84":"plot_model(model, to_file = 'model.png', show_shapes = True, show_layer_names = True)","80681690":"chk_path = 'FER13.h5'\nlog_dir = \"checkpoint\/logs\/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n\ncheckpoint = ModelCheckpoint(\n    filepath       = chk_path,\n    save_best_only = True,\n    verbose        = 1,\n    mode           = 'min',\n    moniter        = 'val_loss'\n)\n\nearlystop = EarlyStopping(\n    monitor              = 'val_loss', \n    min_delta            = 0, \n    patience             = 3, \n    verbose              = 1, \n    restore_best_weights = True\n)\n                        \nreduce_lr = ReduceLROnPlateau(\n    monitor   = 'val_loss', \n    factor    = 0.2, \n    patience  = 6, \n    verbose   = 1, \n    min_delta = 0.0001\n)\n\n\ntensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\ncsv_logger = CSVLogger('training.log')\n\ncallbacks = [checkpoint, reduce_lr, csv_logger]","25f544e6":"def train_model(train, test, epochs, callbacks):\n  steps_per_epoch  = train.n \/\/ train.batch_size\n  validation_steps = test.n \/\/ test.batch_size\n\n  hist = model.fit(\n      x                = train, \n      validation_data  = test, \n      epochs           = epochs, \n      callbacks        = callbacks, \n      steps_per_epoch  = steps_per_epoch, \n      validation_steps = validation_steps\n  )\n\n  return hist","f69bcffd":"hist = train_model(train_set, test_set, 60, callbacks)","c0e0767c":"# Plotting the loss & accuracy\nplt.figure(figsize=(14,5))\nplt.subplot(1,2,2)\nplt.plot(hist.history['accuracy'])\nplt.plot(hist.history['val_accuracy'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['train', 'test'], loc='upper left')\n\nplt.subplot(1,2,1)\nplt.plot(hist.history['loss'])\nplt.plot(hist.history['val_loss'])\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","ffa4ce52":"train_loss, train_acc = model.evaluate(train_set)\ntest_loss, test_acc   = model.evaluate(test_set)\nprint(\"final train accuracy = {:.2f} , validation accuracy = {:.2f}\".format(train_acc*100, test_acc*100))","7757517e":"# Save the weights\nmodel.save_weights('fer2013_weights.h5')","9057aa1e":"y_pred = model.predict(train_set)\ny_pred = np.argmax(y_pred, axis=1)\nclass_labels = test_set.class_indices\nclass_labels = {v:k for k,v in class_labels.items()}\n\ncm_train = confusion_matrix(train_set.classes, y_pred)\n\nprint('Confusion Matrix')\nprint(cm_train)\n\nprint('Classification Report')\ntarget_names = list(class_labels.values())\nprint(classification_report(train_set.classes, y_pred, target_names=target_names))\n\nplt.figure(figsize=(8,8))\nplt.imshow(cm_train, interpolation='nearest')\nplt.colorbar()\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names, rotation=90)\n_ = plt.yticks(tick_mark, target_names)","8a550d15":"## Importing Libraries","9cce315f":"### Building the model","7d6b9738":"## Loading Dataset","12398df1":"*The following function shows the structure of our neural network, which is a convolutional neural net, and we'll be using Adam's optimizer and softmax, relu as activation functions*","d1b4abcd":"*Taking a glance at the distribution of both the training data and testing data.*","2152ce39":"## Exploratory Data Analysis","a1cd8b56":"## Model (CNN)","d0131451":"### Evaluating the model","0bb4c88c":"### Training the model","0b8248b8":"# Facial Emotion Recogination","92206224":"*Although the accuracy is not very high, we can see that there is no overfitting and the training and testing data share almost the same accuracy.*","4a301070":"*Now, we'll plot the confusion matrix and the classification report of the model*","a693c8a7":"*Now let's show a few samples of the dataset*","31a869c7":"*The funciton down below is the one responsible for training the model, and the steps of training are shown in the second next cell.*","cc660161":"*The next thing we'll do is to save the weights into our local directory in order to test them later on different new images*","d5a96272":"*Now we will define the path for the weights, the logs and set up callback functions for checkpoints. early stopping for avoiding overfitting training data and ReduceLROnPlateau for learning rate.*"}}