{"cell_type":{"54a6d8d6":"code","729d0113":"code","b491ec26":"code","54eba2b4":"code","25f853b6":"code","a885da2a":"code","051cd2d8":"code","0cb9e72c":"code","bdb114c7":"code","2421b783":"code","a2a1abdf":"code","95da87ca":"code","823accb2":"code","0bcd5843":"code","1f10e659":"code","0c783595":"code","c14aa284":"code","e93e4e5b":"code","21fedc0a":"code","fb6fa610":"code","704f164b":"code","fc038806":"code","6093fcf7":"code","a2037de6":"code","63ac6a51":"code","5eb71283":"code","a73da4e2":"code","61b3561d":"code","8d709eb0":"code","bbb42661":"code","da69fd31":"code","f81b4b59":"code","d8f66a1b":"code","1d08814e":"code","5274174e":"code","ed6b2206":"code","ef1ee425":"code","822623d1":"code","27bc1a46":"code","8bb9f4a8":"code","6b68711c":"code","868eefec":"code","54deebc6":"code","20203172":"code","d2555ba9":"code","85453bcd":"code","b906cc92":"code","37443c0b":"code","5460f1e9":"code","61f9a9c2":"code","32f56800":"code","9aaa23d8":"code","e6e4b335":"code","670b9a7d":"code","4a51ad00":"code","c32c780b":"code","8abe5240":"code","b3747e1d":"code","f6ff1230":"code","16bc2e7f":"code","aaf99252":"code","4e498a8f":"code","70722425":"code","a6980e05":"code","ac1431ff":"code","2872b541":"code","72d603bf":"markdown","06f06e05":"markdown","a831c804":"markdown","fc56051d":"markdown","1dcc6809":"markdown","aeeff25d":"markdown","f6a95ef7":"markdown","c2a57bb1":"markdown","3b8febfb":"markdown","27c0ce98":"markdown","96ac8064":"markdown","3a61421c":"markdown","d5688ca9":"markdown","6902a6de":"markdown","def40385":"markdown","34e58635":"markdown","0f92c0a8":"markdown","1690d853":"markdown","d158e792":"markdown","d3b58bfa":"markdown","2300e7fa":"markdown","db461d39":"markdown","af756b28":"markdown","e1a7ed84":"markdown","fa76d421":"markdown","0c0ac31d":"markdown"},"source":{"54a6d8d6":"#Loading The Libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import Counter\nfrom imblearn.over_sampling import SMOTE\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n#Libraries for model selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\n#Libraries for various model parameter selection.\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import precision_score,recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\nimport scikitplot as skplt\nfrom sklearn import metrics\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom numpy import mean\nfrom numpy import std","729d0113":"# df = pd.read_csv(\"C:\\\\Users\\\\ASUS\\\\Desktop\\\\Master\\\\AI Tr\u00ed tu\u1ec7 nh\u00e2n t\u1ea1o\\\\data\\\\heart.csv\")\ndf=pd.read_csv(\"\/kaggle\/input\/heart-disease-dataset\/heart.csv\")","b491ec26":"df","54eba2b4":"# Find a duplicate rows\nduplicate_df = df[df.duplicated(keep='last')]\nprint(duplicate_df)","25f853b6":"#Drop duplicate rows\ndf = df.drop_duplicates(keep='last')\ndf.reset_index(inplace=True)\ndel df['index']\ndf.shape","a885da2a":"df","051cd2d8":"counter = Counter(df['target'])\nfor k,v in counter.items():\n    per = v \/ len(df['target']) * 100\n    print('Class=%d, n=%d (%.3f%%)' % (k, v, per))\nkey = ['1','0']\n# creating the bar plot\nplt.bar(key, list(counter.values()))\nplt.ylabel(\"Count\")\nplt.title(\"S\u1ed1 l\u01b0\u1ee3ng c\u00e1c l\u1edbp\")\nplt.show()","0cb9e72c":"df.rename(columns ={'age':'Age','sex':'Sex','cp':'Chest_pain','trestbps':'Resting_blood_pressure','chol':'Cholesterol','fbs':'Fasting_blood_sugar',\n                    'restecg':'ECG_results','thalach':'Maximum_heart_rate','exang':'Exercise_induced_angina','oldpeak':'ST_depression','ca':'Major_vessels',\n                   'thal':'Thalassemia_types','target':'Heart_attack','slope':'ST_slope'}, inplace = True)","bdb114c7":"col= df.columns.tolist()[:-1]\nfor i in range(0 , len(col)) :\n    ax = df.boxplot(column= col[i], by='Heart_attack')\n    ax.set_ylabel(col[i])\n    plt.suptitle('') # Suppress the titles\n    plt.title('')","2421b783":"#First look at the number of Males and Females in the data\nprint(df.Sex.value_counts())\ndf.Sex.value_counts().plot(kind = 'pie', figsize = (8, 6))\nplt.title('Male Female ratio')\nplt.legend(['Male', 'Female']);","a2a1abdf":"#Check if the number of heart diseases compared to gender:\nsns.countplot(x = 'Heart_attack', data = df, hue = 'Sex')\nplt.title(\"Heart Disease Frequency for Sex\")\nplt.xlabel(\"0 = No heart Disease, 1 = Heart Disease\");","95da87ca":"#Check to see if the type of chest pain is influenced by gender:\npd.crosstab(df.Sex, df.Chest_pain).plot(kind = 'bar', color = ['coral', 'lightskyblue', 'plum', 'khaki'])\nplt.title('Type of chest pain for Sex')\nplt.xlabel('0 = Female,    1 = Male');","823accb2":"#Exercise causes angina in men and women:\nsns.countplot(x = 'Exercise_induced_angina', data = df, hue = 'Sex')\nplt.title('Exercise induced angina for Sex')\nplt.xlabel('exercise induced angina');","0bcd5843":"#Blood sugar index between men and women\npd.crosstab(df.Sex, df.Fasting_blood_sugar).plot(kind = 'bar', color = ['lightblue', 'salmon'])\nplt.title(\"Fasting blood sugar w.r.t Sex\")\nplt.xlabel(\"0 = Female,   1 = Male\")\nplt.ylabel(\"Count\");","1f10e659":"df.head()","0c783595":"df.isnull().sum()","c14aa284":"x = df.drop(['Heart_attack'], axis = 1)\ny = df['Heart_attack']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20, random_state = 10)\nfeature_scaler = MinMaxScaler()\nx_train = feature_scaler.fit_transform(x_train)\nx_test = feature_scaler.transform(x_test)","e93e4e5b":"reg = RandomForestRegressor()\nreg.fit(x, y)","21fedc0a":"col= df.columns.tolist()[:-1]\ndf_feature_importance = pd.DataFrame(reg.feature_importances_, index=col, columns=['feature importance']).sort_values('feature importance', ascending=False)\ndf_feature_importance.plot(kind='bar');","fb6fa610":"from sklearn.feature_selection import SelectFromModel\nfrom numpy import sort\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2,  random_state=10)\n\n# fit model on all training data\nRF = RandomForestClassifier()\nRF.fit(X_train, y_train)\n# make predictions for test data and evaluate\ny_pred = RF.predict(X_test)\npredictions = [round(value) for value in y_pred]\naccuracy = accuracy_score(y_test, predictions)\nprint(\"Accuracy: %.2f%%\" % (accuracy * 100.0))\n# Fit model using each importance as a threshold\nthresholds = sort(RF.feature_importances_)\nfor thresh in thresholds:\n    # select features using threshold\n    selection = SelectFromModel(RF, threshold=thresh, prefit=True)\n    select_X_train = selection.transform(X_train)\n    # train model\n    selection_model = RandomForestClassifier(n_estimators=50)\n    selection_model.fit(select_X_train, y_train)\n    # eval model\n    select_X_test = selection.transform(X_test)\n    y_pred = selection_model.predict(select_X_test)\n    predictions = [round(value) for value in y_pred]\n    accuracy = accuracy_score(y_test, predictions)\n    print(\"Thresh=%.3f, n=%d, Accuracy: %.2f%%\" % (thresh, select_X_train.shape[1], accuracy*100.0))","704f164b":"x = df.drop(['Heart_attack', 'Fasting_blood_sugar'], axis = 1)\ny = df['Heart_attack']\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.2, random_state = 10)\nfeature_scaler = MinMaxScaler()\nx_train = feature_scaler.fit_transform(x_train)\nx_test = feature_scaler.transform(x_test)","fc038806":"len(x_train[0])","6093fcf7":"# evaluate a model\ndef evaluate_model(X, y, model):\n    # define evaluation procedure\n    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n    # evaluate model\n    scores = cross_val_score(model, x, y, scoring='accuracy', cv=cv, n_jobs=-1)\n    return scores\ndef get_models():\n    models, names = list(), list()\n    # LogisticRegression\n    models.append(LogisticRegression())\n    names.append('Logistic')\n    # DecisionTreeClassifier\n    models.append(DecisionTreeClassifier())\n    names.append('DTree')\n    # RF\n    models.append(RandomForestClassifier())\n    names.append('RandomForest')\n    # KNeighborsClassifier\n    models.append(KNeighborsClassifier())\n    names.append('KNN')\n    # LinearSVC\n    models.append(LinearSVC())\n    names.append('SVC')              \n    return models, names\n# define models\nmodels, names = get_models()\nresults = list()\n# evaluate each model\nfor i in range(len(models)):\n    #evaluate the model and store results\n    scores = evaluate_model(x, y, models[i])\n    results.append(scores)\n    # summarize performance\n    print('>%s %.3f (%.3f)' % (names[i], mean(scores), std(scores)))\n# plot the results\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","a2037de6":"accuracy_train = []\naccuracy_test = []\nrecall_model = []","63ac6a51":"model = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\nc_values = [100, 30, 20, 10, 1.0, 0.1, 0.01]\npenalty = ['l2']\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","5eb71283":"model1 = LogisticRegression(C = 10  , solver = 'newton-cg', penalty = 'l2')\nmodel1.fit(x_train,y_train)\naccuracy_train1 = model1.score(x_train,y_train)\naccuracy_train.append(accuracy_train1)\naccuracy_test1 = model1.score(x_test,y_test)\naccuracy_test.append(accuracy_test1)\ny_pred = model1.predict(x_test)\nrecall1 = recall_score(y_test,y_pred)\nrecall_model.append(recall1)\nprint('Logistic Regression Accuracy -->',((accuracy_test1)*100))\nprint(recall1)","a73da4e2":"model = DecisionTreeClassifier()\ncriterions = ['gini','entropy']\nsplitters = ['best','random']\n# define grid search\ngrid = dict(criterion=criterions,splitter=splitters)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","61b3561d":"model2 = DecisionTreeClassifier(criterion = 'entropy',splitter = 'best' )\nmodel2.fit(x_train,y_train)\naccuracy_train2 = model2.score(x_train,y_train)\naccuracy_train.append(accuracy_train2)\naccuracy_test2 = model2.score(x_test,y_test)\naccuracy_test.append(accuracy_test2)\ny_pred = model2.predict(x_test)\nrecall2 = recall_score(y_test,y_pred)\nrecall_model.append(recall2)\nprint('Decision Tree Accuracy -->',((accuracy_test2)*100))\nprint(recall2)","8d709eb0":"model = LinearSVC()\npenaltys = ['l1','l2']\nC = [0.01,0.1,1,5,10,15,20]\nloss = ['hinge','squared_hinge']\n# define grid search\ngrid = dict(penalty=penaltys,C=C,loss =loss )\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","bbb42661":"model3 = LinearSVC(C = 5,loss = 'squared_hinge',penalty = 'l2')\nmodel3.fit(x_train,y_train)\naccuracy_train3 = model3.score(x_train,y_train)\naccuracy_train.append(accuracy_train3)\naccuracy_test3 = model3.score(x_test,y_test)\naccuracy_test.append(accuracy_test3)\ny_pred = model3.predict(x_test)\nrecall3 = recall_score(y_test,y_pred)\nrecall_model.append(recall3)\nprint('SVM Classifier Accuracy -->',((accuracy_test3)*100))\n","da69fd31":"print(recall3)","f81b4b59":"# model = RandomForestClassifier()\n# n_estimators = [250,500,750,1000]\n# criterion = ['gini','entropy']\n# max_features = ['auto','sqrt','log2']\n# random_state = [5]\n# # define grid search\n# grid = dict(n_estimators=n_estimators,criterion=criterion,max_features =max_features,random_state = random_state )\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\n# grid_result = grid_search.fit(x_train, y_train)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# # means = grid_result.cv_results_['mean_test_score']\n# # stds = grid_result.cv_results_['std_test_score']\n# # params = grid_result.cv_results_['params']\n# # for mean, stdev, param in zip(means, stds, params):\n# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))","d8f66a1b":"model4 = RandomForestClassifier(criterion = 'entropy',max_features = 'auto',n_estimators = 750, random_state = 5)\nmodel4.fit(x_train,y_train)\naccuracy_train4 = model4.score(x_train,y_train)\naccuracy_train.append(accuracy_train4)\naccuracy_test4 = model4.score(x_test,y_test)\naccuracy_test.append(accuracy_test4)\ny_pred = model4.predict(x_test)\nrecall4 = recall_score(y_test,y_pred)\nrecall_model.append(recall4)\nprint('Random Forest Classifier Accuracy -->',((accuracy_test4)*100))\nprint(recall4)","1d08814e":"model = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","5274174e":"model5 = KNeighborsClassifier(metric ='manhattan',  n_neighbors = 19, weights = 'uniform')\nmodel5.fit(x_train,y_train)\naccuracy_train5 = model5.score(x_train,y_train)\naccuracy_train.append(accuracy_train5)\naccuracy_test5 = model5.score(x_test,y_test)\naccuracy_test.append(accuracy_test5)\ny_pred = model5.predict(x_test)\nrecall5 = recall_score(y_test,y_pred)\nrecall_model.append(recall5)\n# accuracy.append(accuracy8)\nprint('Gradient Boosting Classifier Accuracy -->',((accuracy_test5)*100))\nprint(recall5)","ed6b2206":"Models = ['Logistic Regression','Decision Tree','SVC','Random Forest Classifier','K-Nearest Neighbors']\ntotal_v1 = list(zip(Models,accuracy_train,accuracy_test,recall_model))\noutput_v1 = pd.DataFrame(total_v1, columns = ['Models','Accuracy_train','Accuracy_test','Recall'])","ef1ee425":"output_v1","822623d1":"s_v1 = output_v1.groupby(['Models'])['Accuracy_train','Accuracy_test','Recall'].mean().reset_index().sort_values(by='Accuracy_test',ascending=False)\ns_v1.head(10).style.background_gradient(cmap='Reds')","27bc1a46":"dummy_variable1 = pd.get_dummies(df.Chest_pain)\ndummy_variable2 = pd.get_dummies(df.Thalassemia_types)\ndummy_variable3 = pd.get_dummies(df.ECG_results)\ndummy_variable4 = pd.get_dummies(df.ST_slope)\ndummy_variable5 = pd.get_dummies(df.Major_vessels)\ndf_after_dummy = pd.concat([df,dummy_variable1,dummy_variable2,dummy_variable3,dummy_variable4,dummy_variable5],axis = 'columns')","8bb9f4a8":"dummy_variable1","6b68711c":"df_after_dummy","868eefec":"final = df_after_dummy.drop(['Chest_pain','Thalassemia_types','ECG_results','ST_slope','Major_vessels'],axis = 1)\nfinal.head()","54deebc6":"final","20203172":"x = final.drop(['Heart_attack'], axis = 1)\ny = final['Heart_attack']","d2555ba9":"x_train, x_test, y_train, y_test = train_test_split(x,y,test_size = 0.20, random_state = 5)","85453bcd":"x_train","b906cc92":"feature_scaler = MinMaxScaler()\nx_train = feature_scaler.fit_transform(x_train)\nx_test = feature_scaler.transform(x_test)","37443c0b":"x_train","5460f1e9":"accuracy_train = []\naccuracy_test = []\nrecall_model = []","61f9a9c2":"model = LogisticRegression()\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\nc_values = [100, 30, 20, 10, 1.0, 0.1, 0.01]\npenalty = ['l2']\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","32f56800":"model1 = LogisticRegression(C = 1.0  , solver = 'newton-cg', penalty = 'l2')\nmodel1.fit(x_train,y_train)\naccuracy_train1 = model1.score(x_train,y_train)\naccuracy_train.append(accuracy_train1)\naccuracy_test1 = model1.score(x_test,y_test)\naccuracy_test.append(accuracy_test1)\ny_pred = model1.predict(x_test)\nrecall1 = recall_score(y_test,y_pred)\nrecall_model.append(recall1)\nprint('Logistic Regression Accuracy -->',((accuracy_test1)*100))\nprint(recall1)","9aaa23d8":"model = DecisionTreeClassifier()\ncriterions = ['gini','entropy']\nsplitters = ['best','random']\n# define grid search\ngrid = dict(criterion=criterions,splitter=splitters)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","e6e4b335":"model2 = DecisionTreeClassifier(criterion = 'gini',splitter = 'random' )\nmodel2.fit(x_train,y_train)\naccuracy_train2 = model2.score(x_train,y_train)\naccuracy_train.append(accuracy_train2)\naccuracy_test2 = model2.score(x_test,y_test)\naccuracy_test.append(accuracy_test2)\ny_pred = model2.predict(x_test)\nrecall2 = recall_score(y_test,y_pred)\nrecall_model.append(recall2)\nprint('Decision Tree Accuracy -->',((accuracy_test2)*100))\nprint(recall2)","670b9a7d":"model = LinearSVC()\npenaltys = ['l1','l2']\nC = [0.01,0.1,1,5,10,15,20]\nloss = ['hinge','squared_hinge']\n# define grid search\ngrid = dict(penalty=penaltys,C=C,loss =loss )\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))","4a51ad00":"model3 = LinearSVC(C =0.1,loss = 'squared_hinge',penalty = 'l2')\nmodel3.fit(x_train,y_train)\naccuracy_train3 = model3.score(x_train,y_train)\naccuracy_train.append(accuracy_train3)\naccuracy_test3 = model3.score(x_test,y_test)\naccuracy_test.append(accuracy_test3)\ny_pred = model3.predict(x_test)\nrecall3 = recall_score(y_test,y_pred)\nrecall_model.append(recall3)\nprint('SVC Classifier Accuracy -->',((accuracy_test3)*100))\n","c32c780b":"# model = RandomForestClassifier()\n# n_estimators = [250,500,750,1000]\n# criterion = ['gini','entropy']\n# max_features = ['auto','sqrt','log2']\n# random_state = [5]\n# # define grid search\n# grid = dict(n_estimators=n_estimators,criterion=criterion,max_features =max_features,random_state = random_state )\n# cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n# grid_search = GridSearchCV(estimator=model, param_grid=grid, cv=cv, scoring='accuracy')\n# grid_result = grid_search.fit(x_train, y_train)\n# # summarize results\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# # means = grid_result.cv_results_['mean_test_score']\n# # stds = grid_result.cv_results_['std_test_score']\n# # params = grid_result.cv_results_['params']\n# # for mean, stdev, param in zip(means, stds, params):\n# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))","8abe5240":"model4 = RandomForestClassifier(criterion = 'entropy',max_features = 'log2',n_estimators = 1000, random_state = 5)\nmodel4.fit(x_train,y_train)\naccuracy_train4 = model4.score(x_train,y_train)\naccuracy_train.append(accuracy_train4)\naccuracy_test4 = model4.score(x_test,y_test)\naccuracy_test.append(accuracy_test4)\ny_pred = model4.predict(x_test)\nrecall4 = recall_score(y_test,y_pred)\nrecall_model.append(recall4)\nprint('Random Forest Classifier Accuracy -->',((accuracy_test4)*100))\nprint(recall4)","b3747e1d":"model = KNeighborsClassifier()\nn_neighbors = range(1, 21, 2)\nweights = ['uniform', 'distance']\nmetric = ['euclidean', 'manhattan', 'minkowski']\n# define grid search\ngrid = dict(n_neighbors=n_neighbors,weights=weights,metric=metric)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(x_train, y_train)\n# summarize results\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n# means = grid_result.cv_results_['mean_test_score']\n# stds = grid_result.cv_results_['std_test_score']\n# params = grid_result.cv_results_['params']\n# for mean, stdev, param in zip(means, stds, params):\n#     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n","f6ff1230":"model5 = KNeighborsClassifier(metric ='euclidean',  n_neighbors = 13, weights = 'distance')\nmodel5.fit(x_train,y_train)\naccuracy_train5 = model5.score(x_train,y_train)\naccuracy_train.append(accuracy_train5)\naccuracy_test5 = model5.score(x_test,y_test)\naccuracy_test.append(accuracy_test5)\ny_pred = model5.predict(x_test)\nrecall5 = recall_score(y_test,y_pred)\nrecall_model.append(recall5)\n# accuracy.append(accuracy8)\nprint('Gradient Boosting Classifier Accuracy -->',((accuracy_test5)*100))\nprint(recall5)","16bc2e7f":"Models = ['Logistic Regression','Decision Tree','SVC','Random Forest Classifier','K-Nearest Neighbors']\ntotal_v2 = list(zip(Models,accuracy_train,accuracy_test,recall_model))\noutput_v2 = pd.DataFrame(total_v2, columns = ['Models','Accuracy_train','Accuracy_test','Recall'])","aaf99252":"output_v2","4e498a8f":"s_v2 = output_v2.groupby(['Models'])['Accuracy_train','Accuracy_test','Recall'].mean().reset_index().sort_values(by='Accuracy_test',ascending=False)\ns_v2.head(10).style.background_gradient(cmap='Reds')","70722425":"result_s1s2 = pd.concat([s_v1,s_v2],axis = 'columns')","a6980e05":"result_s1s2","ac1431ff":"s_v1","2872b541":"s_v2","72d603bf":"# # Random Forest Classifier","06f06e05":"# Logistic Regression","a831c804":"# Split data ","fc56051d":"# # K-Nearest Neighbors","1dcc6809":"Select Recall as the model's standard.","aeeff25d":"Create dummy variables for data to increase accuracy","f6a95ef7":"# Visualazition","c2a57bb1":"T\u1eeb m\u1ed9t s\u1ed1 bi\u1ec3u \u0111\u1ed3 ch\u00fang ta c\u00f3 th\u1ec3 th\u1ea5y vi\u1ec7c kh\u1ea3 n\u0103ng b\u1ecb b\u1ec7nh tim c\u00f3 \u1ea3nh h\u01b0\u1edfng nhi\u1ec1u \u0111\u1ebfn gi\u1edbi t\u00ednh, c\u00f3 v\u1ebb nh\u01b0 nam d\u1ec5 b\u1ecb b\u1ec7nh tim h\u01a1n n\u1eef ( Nam v\u1eadn \u0111\u1ed9ng nhi\u1ec1u h\u01a1n, nhi\u1ec1u \u00e1p l\u1ef1c h\u01a1n,...)","3b8febfb":"# Check the suitability of the algorithms to the data","27c0ce98":"# Check importan of features","96ac8064":"# Helps predict the likelihood of cardiovascular disease.","3a61421c":"# Decision Trees","d5688ca9":"# SVC","6902a6de":"RandomForest and Decision Tree overfitting","def40385":"# PreProcess","34e58635":"Nh\u1eadn th\u1ea5y c\u00f3 s\u1ef1 ph\u00e2n lo\u1ea1i r\u00f5 r\u1ec7t gi\u1eefa gi\u1edbi t\u00ednh Nam v\u00e0 N\u1eef, em s\u1ebd xem li\u1ec7u gi\u1edbi t\u00ednh c\u00f3 \u1ea3nh h\u01b0\u1edfng g\u00ec \u0111\u1ebfn kh\u1ea3 n\u0103ng b\u1ecb b\u1ec7nh tim m\u1ec7ch hay kh\u00f4ng:","0f92c0a8":"# K-Nearest Neighbors","1690d853":"Boxplot, importan => delete \"Fasting_blood_sugar\"","d158e792":"Random Forest and Decision Tree : overfiting","d3b58bfa":"# Select algorithm","2300e7fa":"# Model Selection and Training","db461d39":"# Random Forest Classifier","af756b28":"# Overall accuracies obtained by the models","e1a7ed84":"# # Decision Trees","fa76d421":"# Tuning Logistic Regression","0c0ac31d":"# Support Vector Classification."}}