{"cell_type":{"82817e87":"code","f1daf571":"code","c7721b2d":"code","75b17034":"code","aaf192e3":"code","0dc07e68":"code","72aee484":"code","a530418a":"code","5538714b":"code","190c1f64":"code","b5d5ce22":"code","031254ca":"code","7aec667d":"code","8aa74fd7":"code","f9d06984":"code","afdadf47":"code","30d0d7e3":"code","1008c7dd":"code","8658f26e":"code","5f5eecc5":"code","62dd0cdd":"code","e2b3d254":"code","2ac5b66c":"code","44abb473":"code","83aff120":"code","3ceb45b7":"code","ea3552a5":"markdown","fbf7e513":"markdown","e362de37":"markdown","9714c27a":"markdown","2f34df77":"markdown","5a1c0594":"markdown","15cb1d39":"markdown","5b5da2ee":"markdown","afd950d2":"markdown","13f493ce":"markdown","dea24181":"markdown","45111113":"markdown","643b4f7e":"markdown","b0c181f0":"markdown","f27be98d":"markdown","77bea877":"markdown","47119eff":"markdown","bf90a17a":"markdown"},"source":{"82817e87":"!pip install ..\/input\/sacremoses\/sacremoses-master\/ > \/dev\/null\n!pip install ..\/input\/transformers\/transformers-2.3.0\/ > \/dev\/null\n!pip install ..\/input\/iterative-stratification\/iterative-stratification-master\/ > \/dev\/null","f1daf571":"import re\nimport random\nimport logging\nfrom typing import List, Dict, Tuple, Any","c7721b2d":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OneHotEncoder, MinMaxScaler\nfrom sklearn.model_selection import GroupKFold\nimport joblib\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential, Model, load_model\nfrom tensorflow.keras.layers import Dense, Activation, Dropout, Input, GlobalAveragePooling1D, Lambda\nfrom tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\nimport tensorflow.keras.backend as K\nimport tensorflow_hub\nimport transformers\nfrom transformers import TFBertModel, BertTokenizer\nfrom scipy.stats import spearmanr\nfrom tqdm import tqdm\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\nfrom sklearn.linear_model import MultiTaskElasticNet","75b17034":"pd.options.display.max_columns = None\ntf.get_logger().setLevel(logging.ERROR)\n\nrandom.seed(31)\nnp.random.seed(31)\ntf.random.set_seed(31)","aaf192e3":"train = pd.read_csv('..\/input\/google-quest-challenge\/train.csv')\ntarget_cols = pd.read_csv('..\/input\/google-quest-challenge\/sample_submission.csv').columns[1:].tolist()\nfeatures_train, targets_train = train.drop(columns=target_cols), train.loc[:, target_cols]\ndel train\n\nfeatures_test = pd.read_csv('..\/input\/google-quest-challenge\/test.csv')","0dc07e68":"class BaseTransformer(BaseEstimator, TransformerMixin):\n    \n    def fit(self, x: pd.DataFrame, y = None):\n        return self\n    \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        return x\n\n\nclass ColumnTransformer(BaseTransformer):\n    \n    def __init__(self, defs: Dict[str, BaseTransformer]):\n        self.defs = defs\n    \n    def fit(self, x: pd.DataFrame, y: np.ndarray = None):\n        for col, transformer in self.defs.items():\n            transformer.fit(x[col], y)\n        return self\n        \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        xp = x.copy()\n        for col, transformer in self.defs.items():\n            xp[col] = transformer.transform(x[col])\n        return xp\n    \n    def fit_transform(self, x: pd.DataFrame, y: np.ndarray = None) -> pd.DataFrame:\n        xp = x.copy()\n        for col, transformer in self.defs.items():\n            if hasattr(transformer, 'fit_transform'):\n                xp[col] = transformer.fit_transform(x[col], y)\n            else:\n                xp[col] = transformer.fit(x[col], y).transform(x[col])\n        return xp\n\n\nclass WrappedOneHotEncoder(BaseTransformer):\n    \n    def __init__(self, col: str):\n        self.col = col\n        self.oe = OneHotEncoder(drop='first', sparse=False)\n    \n    def fit(self, x: pd.DataFrame, y = None):\n        self.oe.fit(x.loc[:, [self.col]])\n        return self\n    \n    def transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        sparse_matrix = self.oe.transform(x.loc[:, [self.col]])\n        columns = ['{0}_onehot_{1}'.format(self.col, i) for i in range(sparse_matrix.shape[1])]\n        onehot = pd.DataFrame(sparse_matrix, index=x.index, columns=columns)\n        return pd.concat([x, onehot], axis=1)\n\n\nclass WrappedMinMaxScaler(BaseTransformer):\n    \n    def __init__(self, minmax: Tuple[float, float] = (1e-5, 1-1e-5)):\n        self.mms = MinMaxScaler(minmax)\n    \n    def fit_transform(self, x: pd.DataFrame) -> pd.DataFrame:\n        array = self.mms.fit_transform(x)\n        return pd.DataFrame(array, index=x.index, columns=x.columns)","72aee484":"def colwise_spearmanr(y_true: np.ndarray, y_pred: np.ndarray, cols: List[str]) -> Dict[str, float]:\n    return {c: spearmanr(y_true[:, i], y_pred[:, i]).correlation for i, c in enumerate(cols)}\n\n\ndef average_spearmanr(y_true: np.ndarray, y_pred: np.ndarray) -> float:\n    return np.average([\n        spearmanr(y_t, y_p).correlation for y_t, y_p in zip(y_true.T, y_pred.T)\n    ])\n\n\ndef mini_batch(l: List, batch_size):\n    for i in range(0, len(l), batch_size):\n        yield l[i:i + batch_size]\n\n\ndef rank_average(arrays: List[np.ndarray]) -> np.ndarray:\n    rank_sum = np.sum([pd.DataFrame(a).rank().values for a in arrays], axis=0)\n    return rank_sum \/ (len(arrays) * arrays[0].shape[0])\n\n\ndef pd_average(df_ys: List[pd.DataFrame]) -> pd.DataFrame:\n    return pd.DataFrame(\n        np.average([df_y.values for df_y in df_ys], axis=0),\n        columns=df_ys[0].columns,\n        index=df_ys[0].index,\n    )\n\n\ndef pd_rank_average(df_ys: List[pd.DataFrame]) -> pd.DataFrame:\n    return pd.DataFrame(\n        rank_average([df_y.values for df_y in df_ys]),\n        columns=df_ys[0].columns,\n        index=df_ys[0].index,\n    )","a530418a":"class SecondLevelDomainExtracter(BaseTransformer):\n    \n    def transform(self, s_in: pd.Series) -> pd.Series:\n        s = s_in.str.extract(r'(^|.*\\.)([^\\.]+\\.[^\\.]+$)').iloc[:, 1]\n        s.name = s_in.name\n        return s\n\n\nclass UniversalSentenceEncoderEncoder(BaseTransformer):\n    \n    def __init__(self, col: str, model, batch_size: int = 16):\n        self.col = col\n        self.model = model\n        self.batch_size = batch_size\n    \n    def transform(self, df: pd.DataFrame) -> pd.DataFrame:\n        texts = df.loc[:, self.col].str.replace('?', '.').str.replace('!', '.').values\n        pbar_total = len(df) \/\/ self.batch_size + 1\n        pbar_desc = '{0} -> USE'.format(self.col)\n        use_features = np.vstack([\n            self.model(texts_batch)['outputs'].numpy() for texts_batch in tqdm(\n                mini_batch(texts, self.batch_size),\n                total=pbar_total,\n                desc=pbar_desc\n            )\n        ])\n        columns = ['{0}_use{1}'.format(self.col, i) for i in range(use_features.shape[1])]\n        return pd.concat([df, pd.DataFrame(use_features, index=df.index, columns=columns)], axis=1)\n\n    \nclass DistanceEngineerer(BaseTransformer):\n    \n    def __init__(self, col1: str, col2: str):\n        self.col1 = col1\n        self.col2 = col2\n    \n    @staticmethod\n    def extract_matrix(df: pd.DataFrame, base_col: str) -> np.ndarray:\n        columns = [c for c in df.columns if re.fullmatch(base_col + r'\\d+', c) is not None]\n        return df.loc[:, columns].values\n    \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:\n        df = df_in.copy()\n        a1 = self.extract_matrix(df, self.col1)\n        a2 = self.extract_matrix(df, self.col2)\n        assert a1.shape == a2.shape\n        l2_distance = np.power(a1 - a2, 2).sum(axis=1)\n        cos_distance = (a1 * a2).sum(axis=1)\n        df.loc[:, 'l2dist_{0}-{1}'.format(self.col1, self.col2)] = l2_distance\n        df.loc[:, 'cosdist_{0}-{1}'.format(self.col1, self.col2)] = cos_distance\n        return df\n\n\nclass QATokenizer(BaseTransformer):\n    \n    def __init__(self, tokenizer, max_len: int = 512):\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n        \n    def tokenize(self, question_title: str, question_body: str, answer: str) -> Tuple[List[int], List[int], List[int]]:\n        question = '{0}[SEP]{1}'.format(question_title, question_body)\n        return self.tokenizer.encode_plus(question, answer, max_length=self.max_len, pad_to_max_length=True)\n    \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:        \n        tokenized = [self.tokenize(\n            row['question_title'],\n            row['question_body'],\n            row['answer']\n        ) for _, row in df_in.iterrows()]\n        input_ids = [d['input_ids'] for d in tokenized]\n        attention_mask = [d['attention_mask'] for d in tokenized]\n        token_type_ids = [d['token_type_ids'] for d in tokenized]\n        \n        return pd.DataFrame(np.hstack([input_ids, attention_mask, token_type_ids]), index=df_in.index)\n\n\nclass ColumnDropper(BaseTransformer):\n    \n    def __init__(self, cols: List[str]):\n        self.cols = cols\n        \n    def transform(self, df_in: pd.DataFrame) -> pd.DataFrame:\n        return df_in.drop(columns=self.cols)","5538714b":"class BaseEnsembleCV:\n    \n    def __init__(self, n_splits: int = 5, verbose: bool = False, name: str = 'base_ecv'):\n        self.n_splits = n_splits\n        self.verbose = verbose\n        self.name = name\n    \n    def fit_fold(self, x_train, y_train, x_val, y_val, **params):\n        raise NotImplementedError()\n        \n    def save_fold(self, model, i_fold, dist):\n        raise NotImplementedError()\n    \n    def load_fold(self, i_fold, src):\n        raise NotImplementedError()\n        \n    def fit(self, df_x: pd.DataFrame, df_y: pd.DataFrame, groups = None, **params):\n        \n        x = df_x.values\n        y = df_y.values\n\n        if groups is None:\n            # thanks to [Neuron Engineer's kernel](https:\/\/www.kaggle.com\/ratthachat\/quest-cv-analysis-on-different-splitting-methods)\n            kfold = MultilabelStratifiedKFold(n_splits=self.n_splits, shuffle=True, random_state=31).split(x, y)\n        else:\n            kfold = GroupKFold(n_splits=self.n_splits).split(x, y, groups=groups)\n                    \n        iterator = enumerate(kfold)\n        if self.verbose:\n            iterator = tqdm(iterator, total=self.n_splits, desc='k-fold')\n\n        self.models = []\n        _y_oof = np.zeros(y.shape)\n\n        for i_fold, (i_train, i_val) in iterator:\n\n            x_train, x_val = x[i_train], x[i_val]\n            y_train, y_val = y[i_train], y[i_val]\n            model = self.fit_fold(x_train, y_train, x_val, y_val, i_fold, **params)\n            self.models.append(model)\n            _y_oof[i_val] = model.predict(x_val)\n        _score = colwise_spearmanr(y, _y_oof, df_y.columns)\n        self.y_oof = pd.DataFrame(_y_oof, index=df_y.index, columns=df_y.columns)\n        self.score = pd.Series(_score)            \n        self.save()\n\n        return self\n    \n    def predict(self, df_x: pd.DataFrame) -> pd.DataFrame:\n        x = df_x.values\n        y = np.average([\n            m.predict(x) for m in self.models\n        ], axis=0)\n        return pd.DataFrame(y, index=df_x.index, columns=target_cols)\n    \n    def save(self, dist: str = '.'):\n        for i_fold, m in enumerate(self.models):\n            self.save_fold(m, i_fold, dist)\n        self.y_oof.to_csv('{0}\/{1}.y_oof.csv'.format(dist, self.name), index=False)\n        self.score.to_csv('{0}\/{1}.score.csv'.format(dist, self.name), header=True)\n    \n    def load(self, src: str = '.'):\n        self.models = []\n        for i_fold in range(self.n_splits):\n            self.models.append(self.load_fold(i_fold, src))\n        self.y_oof = pd.read_csv('{0}\/{1}.y_oof.csv'.format(src, self.name))\n        self.score = pd.read_csv('{0}\/{1}.score.csv'.format(src, self.name), index_col=0).iloc[:, 0]\n        return self","190c1f64":"class ProgressBar(Callback):\n    \n    def __init__(self, pbar):\n        self.pbar = pbar\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.pbar.update()\n        self.pbar.set_postfix(logs)","b5d5ce22":"class DenseECV(BaseEnsembleCV):\n    \n    def __init__(self, name: str = 'dense_ecv'):\n        super().__init__(name=name)\n    \n    def fit_fold(self, x_train, y_train, x_val, y_val, i_fold: int, **in_params):\n        default_parmas = dict(\n            lr=0.0001,\n            epochs=50,\n            batch_size=32,\n        )\n        params = {**default_parmas, **in_params}\n        K.clear_session()\n        model = Sequential([\n            Dense(512, input_shape=(x_train.shape[1],)),\n            Activation('relu'),\n            Dropout(0.2),\n            Dense(y_train.shape[1]),\n            Activation('sigmoid'),\n        ])\n        es = EarlyStopping(patience=5)\n        model.compile(\n            optimizer=Adam(lr=params['lr']),\n            loss='binary_crossentropy',\n        )\n        with tqdm(desc='k-fold {0}\/{1}'.format(i_fold+1, self.n_splits), total=params['epochs']) as pbar:\n            model.fit(\n                x_train, y_train,\n                epochs=params['epochs'],\n                batch_size=params['batch_size'],\n                validation_data=(x_val, y_val),\n                callbacks=[es, ProgressBar(pbar)],\n                verbose=0,\n            )\n        return model\n    \n    def save_fold(self, model, i_fold, dist):\n        model.save('{0}\/{1}.model_{2}.h5'.format(dist, self.name, i_fold))\n    \n    def load_fold(self, i_fold, src):\n        return load_model('{0}\/{1}.model_{2}.h5'.format(src, self.name, i_fold))","031254ca":"class ElasticNetECV(BaseEnsembleCV):\n    \n    def __init__(self, verbose: bool = True, name: str = 'elasticnet_ecv'):\n        super().__init__(verbose=verbose, name=name)\n        \n    def fit_fold(self, x_train, y_train, *_, **in_params):\n        default_params = dict(\n            alpha=0.001,\n            l1_ratio=0.5,\n            selection='random',\n        )\n        params = {**default_params, **in_params}\n        model = MultiTaskElasticNet(random_state=31, **params)\n        return model.fit(x_train, y_train)\n    \n    def save_fold(self, model, i_fold, dist):\n        joblib.dump(model, '{0}\/{1}.model_{2}.joblib'.format(dist, self.name, i_fold))\n    \n    def load_fold(self, i_fold, src):\n        return joblib.load('{0}\/{1}.model_{2}.joblib'.format(src, self.name, i_fold))","7aec667d":"class BertFineTuningECV(BaseEnsembleCV):\n    \n    def __init__(self, pretrained_path: str, seq_len: int = 512, name: str = 'bert_ecv'):\n        super().__init__(name=name)\n        self.pretrained_path = pretrained_path\n        self.seq_len = seq_len\n    \n    def build_model(self, params) -> Model:\n        s = self.seq_len\n        with tf.device('\/cpu:0'):  # avoid OOM error\n            x = Input(self.seq_len * 3, dtype=tf.int32, name='x')\n            input_word_ids = Lambda(lambda x: x[:, :s], output_shape=(s,))(x)\n            attention_mask = Lambda(lambda x: x[:, s:s*2], output_shape=(s,))(x)\n            token_type_ids = Lambda(lambda x: x[:, s*2:], output_shape=(s,))(x)\n            bert_layer = TFBertModel.from_pretrained(self.pretrained_path)\n            last_hidden_layer, _ = bert_layer([input_word_ids, attention_mask, token_type_ids])\n            pooled = GlobalAveragePooling1D()(last_hidden_layer)\n            pooled = Dropout(0.2)(pooled)\n            y = Dense(30, activation='sigmoid', name='output')(pooled)\n            model = Model(inputs=x, outputs=y)\n        model.compile(\n            optimizer=Adam(lr=params['lr']),\n            loss='binary_crossentropy',\n        )\n        return model\n\n    def fit_fold(self, x_train, y_train, x_val, y_val, i_fold, **in_params):\n        default_params = dict(\n            lr=0.00003,\n            epochs=5,\n            batch_size=8,\n        )\n        params = {**default_params, **in_params}\n        es = EarlyStopping(patience=5)\n        cp = ModelCheckpoint('{0}.model_{1}.h5'.format(self.name, i_fold), save_best_only=True, save_weights_only=True)\n\n        K.clear_session()        \n        model = self.build_model(params)\n        h = model.fit(\n            x_train, y_train,\n            epochs=params['epochs'],\n            batch_size=params['batch_size'],\n            validation_data=(x_val, y_val),\n            callbacks=[es, cp],\n            verbose=1,\n        )\n        pd.DataFrame(h.history).to_csv('{0}_history_{1}.csv'.format(self.name, i_fold), index=False)\n        return model\n    \n    def save_fold(self, model, i_fold, dist):\n        return\n        \n    def load_fold(self, i_fold, src):\n        K.clear_session()\n        model = self.build_model(dict(lr=0.00003))\n        model.load_weights('{0}\/{1}.model_{2}.h5'.format(src, self.name, i_fold))\n        return model","8aa74fd7":"use_model = tensorflow_hub.load('..\/input\/universalsentenceencoderlarge4')\nbert_tokenizer = BertTokenizer.from_pretrained('..\/input\/transformers\/bert-base-uncased')","f9d06984":"use_preprocess = Pipeline(steps=[\n    \n    # Universal Sentence Encoder\n    ('USE_question_title', UniversalSentenceEncoderEncoder('question_title', use_model)),\n    ('USE_question_body', UniversalSentenceEncoderEncoder('question_body', use_model)),\n    ('USE_answer', UniversalSentenceEncoderEncoder('answer', use_model)),\n    \n    # distance\n    ('distance_use_question_title-question_body', DistanceEngineerer('question_title_use', 'question_body_use')),\n    ('distance_use_question_title-answer', DistanceEngineerer('question_title_use', 'answer_use')),\n    ('distance_use_question_body-answer', DistanceEngineerer('question_body_use', 'answer_use')),\n    \n    # one-hot encode & drop columns\n    ('onehost_encode_and_drop_columns', Pipeline(steps=[\n\n        # abc.example.com -> example.com\n        ('extrace_sld', ColumnTransformer({\n            'host': SecondLevelDomainExtracter(),\n        })),\n\n        # one-hot encode\n        ('onehot_encode_host', WrappedOneHotEncoder('host')),\n        ('onehot_encode_category', WrappedOneHotEncoder('category')),\n\n    ]).fit(features_train)),\n    \n    ('drop_columns', ColumnDropper([\n        'qa_id', 'category', 'host', 'question_title', 'question_body', 'question_user_name', 'question_user_page',\n        'answer', 'answer_user_name', 'answer_user_page', 'url',\n    ])),\n])\n\nuse_densenn = Pipeline(steps=[\n    ('preprocess', use_preprocess),\n    ('estimate', DenseECV()),\n])\n\nuse_elasticnet = Pipeline(steps=[    \n    ('preprocess', use_preprocess),\n    ('estimate', ElasticNetECV())\n])\n\nbert_finetuning = Pipeline(steps=[    \n    ('tokenize', QATokenizer(bert_tokenizer)),\n    ('estimate', BertFineTuningECV('..\/input\/transformers\/bert-base-uncased')),\n])","afdadf47":"question_title = features_train['question_title']","30d0d7e3":"# _ = bert_finetuning.fit(features_train, targets_train, estimate__groups=question_title)\n_ = bert_finetuning['estimate'].load('..\/input\/google-quest-challenge-trained-models')","1008c7dd":"bert_finetuning['estimate'].score.mean()","8658f26e":"# _ = use_densenn.fit(features_train, targets_train, estimate__groups=question_title)\n_ = use_densenn['estimate'].load('..\/input\/google-quest-challenge-trained-models')","5f5eecc5":"use_densenn['estimate'].score.mean()","62dd0cdd":"# _ = use_elasticnet.fit(features_train, targets_train, estimate__groups=question_title)\n_ = use_elasticnet['estimate'].load('..\/input\/google-quest-challenge-trained-models')","e2b3d254":"use_elasticnet['estimate'].score.mean()","2ac5b66c":"average_spearmanr(\n    pd_average([\n        bert_finetuning['estimate'].y_oof,\n        use_densenn['estimate'].y_oof,\n        use_elasticnet['estimate'].y_oof,        \n    ]).values,\n    targets_train.values\n)","44abb473":"average_spearmanr(\n    pd_rank_average([\n        bert_finetuning['estimate'].y_oof,\n        use_densenn['estimate'].y_oof,\n        use_elasticnet['estimate'].y_oof,        \n    ]).values,\n    targets_train.values\n)","83aff120":"def to_submission(y: pd.DataFrame) -> pd.DataFrame:\n    return pd.concat([features_test.loc[:, 'qa_id'], y], axis=1)","3ceb45b7":"WrappedMinMaxScaler().fit_transform(pd_average([\n    bert_finetuning.predict(features_test),\n    use_densenn.predict(features_test),\n    use_elasticnet.predict(features_test),\n])).pipe(to_submission).to_csv('submission.csv', index=False)","ea3552a5":"# Reusable class","fbf7e513":"### MultiTask ElasticNet Estimatorhistory","e362de37":"# Estimators","9714c27a":"# BERT Fine Tuning","2f34df77":"# Magic word","5a1c0594":"ensembling 3 models\n\n- BERT fine-tuned model\n- Universal Sentence Encoder + Dense NN\n- Universal Sentence Encoder + ElasticNet","15cb1d39":"# Load CSV","5b5da2ee":"# UniversalSentenceEncder -> ElasticNet","afd950d2":"# Reusable functions","13f493ce":"# Install libraries from kaggle dataset\n\nFirst of all, we install `transformers`, `iterstrats` and its dependencies from **Kaggle dataset**, not from PyPI.\nThis is because the internet connection is forbidden in this competition rule.","dea24181":"# Ensemble","45111113":"# Import","643b4f7e":"# Prediction","b0c181f0":"# Transformer","f27be98d":"# Pipeline","77bea877":"### BERT Fine Tuning Estimator","47119eff":"# UniversalSentenceEncder -> Dense NN","bf90a17a":"### Dense NN Estimator"}}