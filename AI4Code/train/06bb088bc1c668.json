{"cell_type":{"d214831b":"code","a887e125":"code","e384636f":"code","812caf45":"code","44ed6781":"code","1129bd48":"code","a232104a":"code","29ab54f2":"code","21f9c9fd":"code","c0a28429":"code","4fa1c7e9":"code","95bd693d":"code","8e191723":"code","14ef594a":"code","f41a7c10":"code","f9aa08de":"code","67ffe53f":"code","13967a9c":"code","46356a6b":"code","0d3d0632":"code","a4012cb2":"code","a6e8fb64":"code","9d3004bc":"code","21132dcb":"markdown","cdc62dac":"markdown","653eada1":"markdown","e119d3cb":"markdown","8dfc5c4f":"markdown","cab39ecf":"markdown","f8186c4f":"markdown"},"source":{"d214831b":"KAGGLE_KEY = \"\"","a887e125":"class Utils:\n  @staticmethod\n  def install(module, pypi_name=None, path=None):\n    if path != None:\n      if os.path.isdir(path):\n        if path not in sys.path:\n          sys.path.append(path)\n          return\n    pypi_name = module if pypi_name == None else pypi_name\n    if module not in sys.modules:\n      !pip install {pypi_name}\n\n  @staticmethod\n  def get_basepath(project, test_file='train.csv', key=''):\n    kaggle_base_path = f'..\/input\/{project}\/'\n    if os.path.isdir(kaggle_base_path): # We are in kaggle\n      return kaggle_base_path\n    colab_base_path = '.\/'\n    if not os.path.isfile(f'{colab_base_path}{test_file}'):\n      Utils.download_kaggle(project, key)\n    return colab_base_path\n\n  @staticmethod\n  def download_kaggle(project, key, type='competitions'):\n    os.environ['KAGGLE_USERNAME'] = \"sudhanshuraheja\"\n    os.environ['KAGGLE_KEY'] = key\n    !kaggle {type} download -c {project}\n    !unzip \\*.zip  && rm *.zip\n\n  @staticmethod\n  def get_strategy(enable_mixed_precision=False, enable_xla_accelerate=True):\n    device = 'TPU'\n    replicas = 0\n    # Detect hardware, return appropriate distribution strategy\n    try:\n      tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n      # TPU detection. No parameters necessary if TPU_NAME environment variable is set.\n      # On Kaggle this is always the case.\n      device = f'TPU {tpu.master()}'\n    except ValueError:\n      tpu = None\n\n    if tpu:\n      tf.config.experimental_connect_to_cluster(tpu)\n      tf.tpu.experimental.initialize_tpu_system(tpu)\n      strategy = tf.distribute.experimental.TPUStrategy(tpu)\n    else:\n      strategy = tf.distribute.get_strategy()\n      # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n      device = 'GPU' if len(tf.config.list_physical_devices('GPU')) > 0 else 'CPU'\n    replicas = strategy.num_replicas_in_sync\n\n    if enable_mixed_precision:\n      from tensorflow.keras.mixed_precision import experimental as mixed_precision\n      if tpu: policy = tf.keras.mixed_precision.experimental.Policy('mixed_bfloat16')\n      else: policy = tf.keras.mixed_precision.experimental.Policy('mixed_float16')\n      mixed_precision.set_policy(policy)\n      tf.config.optimizer.set_experimental_options({\"auto_mixed_precision\": True})\n\n    if enable_xla_accelerate:\n      tf.config.optimizer.set_jit(True)\n\n    print(f'Running on {device} with {replicas} replicas, mixed_precision: {enable_mixed_precision}, accelerate: {enable_xla_accelerate}')\n\n    return strategy\n\n  @staticmethod\n  def seed_everything(seed=0):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n","e384636f":"# The usual shebang\nimport os\nimport gc\nimport sys\nimport json\nimport math\nimport time\nimport random\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Data sciency stuff\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import display\nsns.set()\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', 60)\npd.set_option('display.memory_usage', True)\npd.set_option('show_dimensions', True)\n\n# Tensorflow\nimport tensorflow as tf\nimport tensorflow_addons as tfa\n\n# Sklearn\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import *\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import VarianceThreshold\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestClassifier\n\n# iterstrat\nUtils.install(\n    'iterstrat', \n    pypi_name='iterative-stratification', \n    path='..\/input\/iterative-stratification\/iterative-stratification-master')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold, RepeatedMultilabelStratifiedKFold\n\nif KAGGLE_KEY != \"\":\n    %load_ext google.colab.data_table","812caf45":"Utils.seed_everything()\nstrategy = Utils.get_strategy()","44ed6781":"class Metrics:\n  @staticmethod\n  def log_loss(y_true, y_pred):\n    return tf.keras.losses.BinaryCrossentropy().__call__(y_true, y_pred).numpy()\n\n  @staticmethod\n  def f1_weighted(y_true, y_pred, threshold=0.5):\n    return f1_score(y_true, (y_pred > threshold).astype('int'), average='weighted', zero_division=0)\n\n  @staticmethod\n  def classification_report(y_true, y_pred, cols, threshold=0.5):\n    return classification_report(\n      y_true, \n      (y_pred > threshold).astype('int'),\n      output_dict=True,\n      zero_division=0,\n      target_names=cols,\n    )\n\n  @staticmethod\n  def best_f1_weighted(y_true, y_pred, splits=10):\n    start = time.time()\n    best_threshold = 0\n    best_score = 0\n    for threshold in np.linspace(0, 1, num=splits+1):\n      score = Metrics.f1_weighted(y_true, y_pred, threshold=threshold)\n      if score > best_score:\n        best_score = score\n        best_threshold = threshold\n    return best_score, best_threshold\n\n  @staticmethod\n  def display_f1(classification_report):\n    scores = pd.DataFrame.from_dict(classification_report).T.sort_values(by='support', ascending=False)\n    scores = scores.drop(['samples avg', 'macro avg', 'weighted avg'], axis=0)\n    scores['TP'] = round(scores['recall'] * scores['support'])\n    scores['FN'] = scores['support'] - scores['TP']\n    scores['FP'] = (scores['TP'] \/ scores['precision']) - scores['TP']\n    scores = scores.fillna(0)\n    scores = scores.iloc[:25,:]\n    return scores\n\n  @staticmethod\n  def display_column_log_loss(y_true_df, y_pred):\n    y_true = y_true_df.values\n    y_pred = np.clip(y_pred.astype('float32'), 1e-7, 1 - 1e-7)\n    total_loss = 0\n    scores = []\n    for i in range(y_pred.shape[1]):\n      col_loss = - np.mean(y_true[:, i] * np.log(y_pred[:, i]) + (1 - y_true[:, i]) * np.log(1 - y_pred[:, i]))\n      scores.append([y_true[:,i].sum(), col_loss])\n      total_loss += col_loss\n    total_loss = total_loss \/ y_pred.shape[1]\n    df = pd.DataFrame(np.array(scores), index=y_true_df.columns, columns=['support', 'loss'])\n    df['delta'] = df['loss'] - total_loss\n    df = df.sort_values(by='support', ascending=False)\n    return df\n","1129bd48":"BASE_PATH = Utils.get_basepath('lish-moa', test_file='train_features.csv', key=KAGGLE_KEY)\n\n# Read files\ntrain_features = pd.read_csv(BASE_PATH + 'train_features.csv')\ntest_features = pd.read_csv(BASE_PATH + 'test_features.csv')\n\ntrain_targets = pd.read_csv(BASE_PATH + 'train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv(BASE_PATH + 'train_targets_nonscored.csv')\n\nsubmission = pd.read_csv(BASE_PATH + 'sample_submission.csv')\n\n# Get cp types\ntrain_cptypes = train_features['cp_type']\ntest_cptypes = test_features['cp_type']","a232104a":"display(train_features.head())","29ab54f2":"class Features:\n  def __init__(self):\n    self.length = 0\n    self.hist = []\n  def update(self, length, why='Unknown reason'):\n    initial_length = self.length\n    self.length = length\n    message = f'Features updated: {initial_length}+{self.length-initial_length}={self.length} because:{why}'\n    print(message)\n    self.hist.append(message)\n  def len(self): return self.length\n  def history(self):\n    for x in self.hist:\n      print(x)\n\nfeatures = Features()","21f9c9fd":"class Preprocess_:\n  @staticmethod\n  def fit(X): pass\n  @staticmethod\n  def transform(X):\n    remove = ['sig_id']  \n    for col in X.columns:\n      if 'cp_type' == col: X['cp_type'] = X['cp_type'].replace({ 'ctl_vehicle': 0, 'trt_cp': 1 })\n      if 'cp_dose' == col: X['cp_dose'] = X['cp_dose'].replace({ 'D1': -0.5, 'D2': 0.5 })\n      if 'cp_time' == col: X['cp_time'] = (X['cp_time']\/24)-2\n      if 'g-' in col: X[col] = X[col]\/10\n      if 'c-' in col: X[col] = X[col]\/10\n      if col in remove: X = X.drop([col], axis=1)\n    return X","c0a28429":"USE_PCA = True\nADD_FEATURES = True\nREMOVE_INVARIANT = False\nUSE_IMPORTANT_FEATURES = True\n\n# Preprocess\ntrain_features = Preprocess_.transform(train_features)\ntest_features = Preprocess_.transform(test_features)\ntrain_targets = Preprocess_.transform(train_targets)\nfeatures.update(train_features.shape[1], 'using all features')","4fa1c7e9":"train_features = train_features.astype('float32')\ntrain_targets = train_targets.astype('float32')\ntest_features = test_features.astype('float32')","95bd693d":"def CNNModel(layers, lr=3e-4, inputs=786, outputs=206, lookahead=False, initial_dropout=0):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Input(inputs))\n    model.add(tf.keras.layers.Reshape(best_split(inputs)))\n    # model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Dropout(initial_dropout))\n    \n    for layer in layers:\n        model.add(tf.keras.layers.Conv2D(layer[0], layer[1], padding='same', activation='relu', input_shape=(5, 157, 1)))\n        model.add(tf.keras.layers.MaxPooling2D((2, 2), padding='same'))\n        # model.add(tf.keras.layers.BatchNormalization()),\n        if layer[2] > 0:\n            model.add(tf.keras.layers.Dropout(layer[2])),\n        \n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(outputs, activation='sigmoid'))\n    \n    optim = tf.keras.optimizers.Adam(learning_rate=lr)\n    if lookahead:\n        optim = tfa.optimizers.Lookahead(tf.optimizers.Adam(learning_rate=lr), sync_period=6)\n    \n    model.compile(\n        optimizer=optim,\n        loss=tf.keras.losses.BinaryCrossentropy(),\n        metrics=['AUC', 'binary_crossentropy', tf.keras.metrics.FalseNegatives()],\n    )\n    return model\n\ndef best_split(number):\n    f = []\n    for i in range(1, math.ceil(number\/2)):\n        if number % i == 0:\n            f.append(i)\n            f.append(int(number\/i))\n    factors = sorted(list(dict.fromkeys(f)))\n\n    if len(factors) == 0:\n        return (0, 0, 1)\n    if len(factors) % 2 == 0:\n        middle = int(len(factors)\/2)\n        return (factors[middle - 1], factors[middle], 1)\n    else:\n        middle = int((len(factors)-1)\/2)\n        return (factors[middle], factors[middle], 1)\n","8e191723":"USE_ELI5 = False\n\nif USE_ELI5:\n  # eli5\n  Utils.install('eli5')\n  import eli5\n  from eli5.permutation_importance import get_score_importances\n\nclass Eli5:\n\n  def __init__(self, shape, seeds, batch):\n    self.importance = np.zeros(shape)\n    self.batch = batch\n    self.seeds = seeds\n\n  def score_fn(self):\n    def score(X, y):\n      return Metrics.log_loss(y, self.model.predict(X, batch_size=self.batch))\n    return score\n\n  def get_importance(self, model, X, y, n_iter=2, random_state=0):\n    # Called for each fold of a seed\n    if not USE_ELI5:\n      return\n    self.model = model\n    start_feature_importance = time.time()\n    print(f'Starting to get importance')\n    base_score, local_importance = get_score_importances(\n        score_func=self.score_fn(), \n        X=X, y=y, n_iter=n_iter, random_state=random_state)\n    time_feature_importance = (time.time() - start_feature_importance)\/60\n    print(f'Calculated importance in {time_feature_importance:.1f}m')\n    self.importance += np.mean(local_importance, axis=0)\n\n  def save(self, model_name, columns):\n    if not USE_ELI5:\n      return\n    self.importance = self.importance \/ (self.seeds)\n    importance_df = pd.DataFrame([self.importance], columns=columns).transpose().sort_values(by=0)\n    print(importance_df)\n\n    top_feats = np.argwhere(self.importance < 0).flatten()\n    with open(f'{model_name}_top_features.json', 'w') as outfile:\n      json.dump(top_feats.tolist(), outfile)\n","14ef594a":"class LearningRates:\n  @staticmethod\n  def ExponentialDecay(init=1e-3, decay_steps=10, decay_rate=0.5, staircase=False):\n    return tf.keras.optimizers.schedules.ExponentialDecay(\n      init, decay_steps, decay_rate, staircase=staircase, name='exp_decay'\n    )\n\n  @staticmethod\n  def InverseTimeDecay(init=1e-3, decay_steps=10, decay_rate=0.5, staircase=False):\n    return tf.keras.optimizers.schedules.InverseTimeDecay(\n      init, decay_steps, decay_rate, staircase=staircase, name='inv_time_decay'\n    )\n\n  @staticmethod\n  def PiecewiseConstantDecay(boundaries=[20,40,60,80], values=[1e-3, 1e-4, 1e-5, 1e-6, 1e-7]):\n    # lr = 1.0 for first 100001 steps, 0.5 for next 10000 steps, and 0.1 for additional steps\n    # boundaries = [100000, 110000]; values = [1.0, 0.5, 0.1]\n    return tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n      boundaries, values, name='piece_const_decay'\n    )\n\n  @staticmethod\n  def PolynomialDecay(init=1e-3, decay_steps=100, end_lr=0.0001, power=0.5, cycle=False):\n    return tf.keras.optimizers.schedules.PolynomialDecay(\n      init, decay_steps, end_learning_rate=end_lr, power=power, cycle=cycle, name='poly_decay'\n    )\n\n  @staticmethod\n  def CosineDecay(init=1e-3, decay_steps=100, alpha=0):\n    # https:\/\/arxiv.org\/abs\/1608.03983\n    return tf.keras.experimental.CosineDecay(\n      init, decay_steps, alpha=alpha, name='cos_decay'\n    )\n\n  @staticmethod\n  def CosineDecayRestarts(init=1e-3, decay_steps=20, t_mul=2.0, m_mul=1.0, alpha=0.0):\n    return tf.keras.experimental.CosineDecayRestarts(\n      init, decay_steps, t_mul=t_mul, m_mul=m_mul, alpha=alpha, name='cos_decay_restarts'\n    )\n\n  @staticmethod\n  def LinearCosineDecay(init=1e-3, decay_steps=100, periods=3, alpha=0.0, beta=0.001):\n    return tf.keras.experimental.LinearCosineDecay(\n      init, decay_steps, num_periods=periods, alpha=alpha, beta=beta, name='linear_cos_decay'\n    )\n\n  @staticmethod\n  def NoisyLinearCosineDecay(init=1e-3, decay_steps=100, init_var=0.4, var_decay=0.6, periods=3, alpha=0.0, beta=0.001):\n    return tf.keras.experimental.NoisyLinearCosineDecay(\n      init, decay_steps, initial_variance=init_var, variance_decay=var_decay,\n      num_periods=periods, alpha=alpha, beta=beta, name='noisy_linear_cos_decay'\n    )\n\nclass LearningRateCallbacks:\n\n  @staticmethod\n  def ReduceOnPlateau(metrics, factor=0.25, patience=3, verbose=0, mode='auto', min_delta=0.0001, cooldown=1, min_lr=0):\n    return tf.keras.callbacks.ReduceLROnPlateau(\n      monitor=metrics, factor=factor, patience=patience, verbose=verbose, mode=mode,\n      min_delta=min_delta, cooldown=cooldown, min_lr=min_lr,\n    )\n\n  @staticmethod\n  def Scheduler(schedule, verbose=0):\n    # schedule takes epoch(int), lr(float) as inputs, returns lr\n    return tf.keras.callbacks.LearningRateScheduler(\n      schedule, verbose=verbose\n    )\n\n  @staticmethod\n  def RampUpExponentialDecay(init=0.001, max=0.005, min=0.000001, ramp=5, sustain=2, decay=0.7, verbose=0, return_func=False):\n    def lr_function(epoch, lr=0):\n      if epoch < ramp:\n          lr = (max - init) \/ ramp * epoch + init\n      elif epoch < ramp + sustain:\n          lr = max\n      else:\n          lr = (max - min) * decay**(epoch - ramp - sustain) + min\n      return lr\n    if return_func:\n      return lr_function\n    else:\n      return LearningRateCallbacks.Scheduler(lr_function, verbose=verbose)\n\ndef plot_all():\n  epochs = [i for i in range(100)]\n  plt.figure(figsize=(20,15))\n  rows = 4\n  cols = 3\n  count = 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.ExponentialDecay().__call__(i) for i in epochs])\n  plt.xlabel('ExponentialDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.ExponentialDecay(staircase=True).__call__(i) for i in epochs])\n  plt.xlabel('ExponentialDecay[Staircase]'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.InverseTimeDecay().__call__(i) for i in epochs])\n  plt.xlabel('InverseTimeDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.InverseTimeDecay(staircase=True).__call__(i) for i in epochs])\n  plt.xlabel('InverseTimeDecay[Staircase]'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.PiecewiseConstantDecay().__call__(i) for i in epochs])\n  plt.xlabel('PiecewiseConstantDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.PolynomialDecay().__call__(i) for i in epochs])\n  plt.xlabel('PolynomialDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.CosineDecay().__call__(i) for i in epochs])\n  plt.xlabel('CosineDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.CosineDecayRestarts().__call__(i) for i in epochs])\n  plt.xlabel('CosineDecayRestarts'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.LinearCosineDecay().__call__(i) for i in epochs])\n  plt.xlabel('LinearCosineDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRates.NoisyLinearCosineDecay().__call__(i) for i in epochs])\n  plt.xlabel('NoisyLinearCosineDecay'); count += 1\n  plt.subplot(rows, cols, count)\n  plt.plot(epochs, [LearningRateCallbacks.RampUpExponentialDecay(return_func=True).__call__(i) for i in epochs])\n  plt.xlabel('RampUpExponentialDecay')\n  plt.show()\n\nplot_all()","f41a7c10":"class Engine:\n    def __init__(self, X, y, X_test, submission):\n        self.X = X\n        self.y = y\n        self.X_test = X_test\n        self.submission = submission\n        \n    def train(self, name, model_details, folds=10, epochs=200, batch_size=1024, seeds=[0,1,2], verbose=False, early_stopping=6):\n        self.folds = folds\n        self.epochs = epochs\n        self.batch = batch_size\n\n        started = time.time()\n        plt.figure(figsize=(21, 7))\n        verboseInt = 1 if verbose else 0\n        \n        result = self.y.copy()\n        result.loc[:, self.y.columns] = 0\n        submit = self.submission.copy()\n        submit.loc[:, self.y.columns] = 0\n\n        loadModel(model_details, summary=True)\n\n        losses = []\n        print(f'Model:{name}')\n        for scount, seed in enumerate(seeds):\n            kf = MultilabelStratifiedKFold(n_splits=self.folds, random_state=seed, shuffle=True)\n            for fold, (train_idx, val_idx) in enumerate(kf.split(self.X, self.y)):\n\n                model = loadModel(model_details)\n                eli = Eli5(self.X.shape[1], len(seeds), self.batch)\n\n                callbacks = [\n                  tf.keras.callbacks.EarlyStopping(monitor='val_binary_crossentropy', patience=early_stopping, verbose=verboseInt, mode='min', restore_best_weights=True,),\n                  # tf.keras.callbacks.ModelCheckpoint(filepath=f'{name}_{fold}.h5', monitor='val_binary_crossentropy', verbose=verboseInt, save_best_only=True, save_weights_only=False, mode='min',),\n                ]\n                if 'lr_callback' in model_details: callbacks = callbacks + [model_details['lr_callback']]\n\n                history = model.fit(\n                    x=self.X.iloc[train_idx,:].values,\n                    y=self.y.iloc[train_idx].values,\n                    validation_data=(self.X.iloc[val_idx,:].values, self.y.iloc[val_idx].values),\n                    batch_size=self.batch, epochs=self.epochs, verbose=verboseInt,\n                    callbacks=callbacks,\n                )\n                plt.subplot(1, len(seeds)+2, scount+1) # an extra one for learning_rate\n                plt.plot(history.epoch, history.history['val_binary_crossentropy'], color='blue')\n                plt.plot(history.epoch, history.history['binary_crossentropy'], linestyle='--', color='green')\n                plt.xlabel('Epochs')\n                plt.ylabel('BCE')\n\n                y_pred = model.predict(\n                    self.X.iloc[val_idx,:].values, \n                    batch_size=self.batch\n                )\n\n                loss = Metrics.log_loss(self.y.iloc[val_idx].values, y_pred)\n                losses.append(loss)\n\n                result.loc[val_idx, self.y.columns] += y_pred\n                submit.loc[:, self.y.columns] += model.predict(self.X_test.values, batch_size=self.batch)\n\n                time_taken = time.time() - started\n                print(f'Model:{name};Fold:{fold}_{scount};LL:{loss:.8f};CV:{np.array(losses).mean():.8f};Time:{time_taken:.0f}s')\n\n                eli.get_importance(model, self.X.iloc[val_idx,:].values, self.y.iloc[val_idx].values)\n\n            temp_result = result.copy()\n            temp_result.loc[:, self.y.columns] \/= (scount+1)\n            cv = Metrics.log_loss(self.y.values, temp_result.values)\n            temp_result.loc[train_cptypes == 'ctl_vehicle', self.y.columns] = 0\n            ll = Metrics.log_loss(self.y.values, temp_result.values)\n            print(f'Model:{name};LLwC:{ll:.8f};LL:{cv:.8f}\\n')\n\n        lr_data = history.history['lr'] if 'lr' in history.history else [model_details['params']['lr'].__call__(i).numpy() for i in history.epoch]\n        plt.subplot(1, len(seeds)+2, scount+2) # an extra one for learning_rate\n        plt.plot(history.epoch, lr_data, color='red')\n        plt.xlabel('Epochs')\n        plt.ylabel('Learning Rate')\n        plt.show()\n\n        eli.save(name, self.X.columns)\n\n        submit.loc[:, self.y.columns] \/= (self.folds * len(seeds))\n        submit.loc[test_cptypes == 'ctl_vehicle', self.y.columns] = 0\n        submit.to_csv(f'{name}_submission.csv', index=False)\n\n        result.loc[:, self.y.columns] \/= len(seeds)\n        result.loc[train_cptypes == 'ctl_vehicle', self.y.columns] = 0\n        result.to_csv(f'{name}_result.csv', index=False)\n\n        time_taken = (time.time() - started)\/60\n        report = self.report(result.values, np.array(losses))\n        report['y_pred'] = result\n        report['time'] = time_taken\n        print(f'Model:{name};CV:{report[\"cv_mean\"]:.8f};CVStd:{report[\"cv_std\"]:.8f};LLwC:{report[\"log_loss\"]:.8f};F1W:{report[\"f1_weighted\"]:.8f};BF1W:{report[\"best_f1_weighted\"]:.8f};BF1T:{report[\"best_threshold\"]:.6f};Time:{time_taken:.1f}m\\n')\n        return report\n    \n    def report(self, y_pred, losses):\n        y_true = self.y.values\n        return {\n            'log_loss': Metrics.log_loss(y_true, y_pred),\n            'f1_weighted': Metrics.f1_weighted(y_true, y_pred),\n            'best_f1_weighted': 0,\n            'best_threshold': 0,\n            'cv_mean': losses.mean(),\n            'cv_std': losses.std(),\n            'losses': losses,\n            'classification': Metrics.classification_report(y_true, y_pred, self.y.columns),\n        }\n\n    def show_weights_for_dense_layers(self, name):\n        for fold in range(self.folds):\n            filename = f'{name}_{fold}.h5'\n            print(filename)\n            model = tf.keras.models.load_model(filename)\n            for layer in model.layers:\n                config = layer.get_config()\n                if 'dense' in config['name']:\n                    plt.plot(sorted(layer.get_weights()[1]))\n                    plt.xlabel(config['name'] + ' units ' + str(config['units']))\n                    plt.show()\n\nengine = Engine(\n    X=train_features,\n    y=train_targets,\n    X_test=test_features,\n    submission=submission,\n)","f9aa08de":"class Ensemble:\n    def __init__(self, df, y_true, splits=100):\n        self.df = df\n        self.y_true = y_true.values\n        self.splits = splits\n\n    def set_splits(self, splits):\n      self.splits = splits\n        \n    def improve(self):\n        name = self.df.iloc[0]['model']\n        ens_result = self.read_result(name)\n        ens_submit = self.read_submission(name)\n        ens_loss = Metrics.log_loss(self.y_true, ens_result)\n        orig_loss = ens_loss.copy()\n        print(f'Starting loss with {name}: {ens_loss:.8f}')\n        self.remove_row(0)\n        \n        while(self.df.shape[0] > 0):\n            ## Find the best match\n            best_id, best_name, best_weight, best_loss = self.find_best(ens_loss, ens_result)\n            \n            if best_id == None:\n                break\n\n            ## Calculate result ensemble\n            result = self.read_result(best_name)\n            ens_result = ((1 - best_weight) * ens_result) + (best_weight * result)\n            ens_loss = Metrics.log_loss(self.y_true, ens_result)\n            improvement_temp = (1 - (ens_loss \/ orig_loss))*100\n            print(f'Ensemble loss updated to {ens_loss:.8f}, about {improvement_temp:.2f}%')\n\n            ## Calculate submission ensemble\n            submit = self.read_submission(best_name)\n            ens_submit = ((1 - best_weight) * ens_submit) + (best_weight * submit)\n\n            ## Remove best row\n            self.remove_row(best_id)\n\n        improvement = (1 - (ens_loss \/ orig_loss))*100\n        print(f'Final loss: {ens_loss:.8f}, about {improvement:.2f}%')\n        return ens_submit, ens_result\n            \n    def find_best(self, ens_loss, ens_result):\n        best_id = None\n        best_name = ''\n        best_weight = 0\n        best_loss = ens_loss\n        \n        for count in range(0, self.df.shape[0]):\n            name = self.df.iloc[count]['model']\n            loss = self.df.iloc[count]['log_loss']\n            result = self.read_result(name)\n            print(f'Checking {name} with loss {loss:.8f}', end='')\n    \n            loop_best_name = name\n            loop_best_weight = 0\n            loop_best_loss = best_loss\n            \n            for w in np.linspace(0, 1, num=self.splits+1):\n                temp_result = ((1 - w) * ens_result) + (w * result)\n                temp_loss = Metrics.log_loss(self.y_true, temp_result)\n                if temp_loss < loop_best_loss:\n                    loop_best_weight = w\n                    loop_best_loss = temp_loss\n                    print('+', end='')\n                else:\n                    print('.', end='')\n            \n            if loop_best_loss < best_loss:\n                best_id = count\n                best_name = loop_best_name\n                best_weight = loop_best_weight\n                best_loss = loop_best_loss\n                print(f'{best_loss:.8f} ({best_weight:.2f})', end='')\n            print('')\n            \n        print(f'Best Match;Idx:{best_id};Model:{best_name};Wt:{best_weight};Loss:{best_loss:.8f}\\n')\n        return best_id, best_name, best_weight, best_loss\n    \n    def read_result(self, name):\n        return pd.read_csv(f'{name}_result.csv').values\n    \n    def read_submission(self, name):\n        return pd.read_csv(f'{name}_submission.csv').drop(['sig_id'], axis=1).values\n    \n    def remove_row(self, idx):\n        self.df = self.df.drop([idx]).reset_index(drop=True)","67ffe53f":"lr = 1e-3\n\nMODELS = {\n    # CNN\n    'cnn_32_rlr': { 'type': 'CNN', 'params': {\n       'layers': [\n            (32, (3,3), 0.25),\n        ],\n        'lr': lr,\n    },  'lr_callback': LearningRateCallbacks.ReduceOnPlateau('val_binary_crossentropy') },\n    'cnn_16': { 'type': 'CNN', 'params': {\n       'layers': [\n            (16, (3,3), 0.25),\n        ],\n        'lr': LearningRates.CosineDecayRestarts(),\n    }},\n    'cnn_32_i': { 'type': 'CNN', 'params': {\n       'layers': [\n            (32, (3,3), 0.25),\n        ],\n        'lr': LearningRates.CosineDecayRestarts(), 'initial_dropout': 0.25,\n    }},\n    'cnn_32_16': { 'type': 'CNN', 'params': {\n       'layers': [\n            (32, (3,3), 0.25),\n            (16, (3,3), 0.25),\n        ],\n        'lr': LearningRates.CosineDecayRestarts(),\n    }},\n}\n\ndef loadModel(detail, save=False, summary=False):\n  with strategy.scope():\n    detail['params']['inputs'] = features.len()\n    detail['params']['outputs'] = train_targets.shape[1]\n    model_type = detail['type']\n    model = None\n    if model_type == 'CNN': model = CNNModel(**detail['params'])\n    if (summary == True) and ('summary' in dir(model)): model.summary()\n    if (save == True) and ('save' in dir(model)): model.save(f'{m}_base.h5')\n    return model","13967a9c":"VERBOSE = False\nFOLDS = 5\nEPOCHS = 100\nBATCH = 128\nSEEDS = [3,4,5,6] #3,4,5,6 \/ 4,5\nENSEMBLE = True\nENSEMBLE_SPLITS = 100\nEARLY_STOPPING = 6\n\nTRAIN = True\nUSE_OPTUNA = not TRAIN\n\nfeatures.history()\n\ndef train():\n  results = pd.DataFrame({ 'model': [], 'log_loss': [], 'time': [], 'f1_weighted': [], 'best_f1_weighted': [], 'best_threshold': [], 'cv_mean': [], 'cv_std': [], 'losses': [], 'classification': [], 'y_pred': [] })\n  # Test details for folds - https:\/\/docs.google.com\/spreadsheets\/d\/1gGsfor70pD9vZDUHn504cAZP_jGlAeT7X95m1XVPMdY\/edit#gid=0\n  for m in MODELS:\n    detail = MODELS[m]\n    epochs = EPOCHS if 'epochs' not in detail else detail['epochs']\n    batch = BATCH if 'batch' not in detail else detail['batch']\n    folds = FOLDS if 'folds' not in detail else detail['folds']\n    seeds = SEEDS if 'seeds' not in detail else detail['seeds']\n    early_stopping = EARLY_STOPPING if 'early_stopping' not in detail else detail['early_stopping']\n    report = engine.train(\n        name=m,\n        model_details=detail,\n        folds=folds,\n        epochs=epochs,\n        batch_size=batch * strategy.num_replicas_in_sync,\n        verbose=VERBOSE,\n        seeds=seeds,\n        early_stopping=early_stopping,\n    )\n    results = results.append(pd.DataFrame(\n        [[m, report[\"log_loss\"], report[\"time\"], report[\"f1_weighted\"], report[\"best_f1_weighted\"], report[\"best_threshold\"], report[\"cv_mean\"], report[\"cv_std\"], report[\"losses\"], report[\"classification\"], report['y_pred']]],\n        columns=['model', 'log_loss', 'time', 'f1_weighted', 'best_f1_weighted', 'best_threshold', 'cv_mean', 'cv_std', 'losses', 'classification', 'y_pred']\n    ), ignore_index=True)\n#     display(Metrics.display_column_log_loss(train_targets, report['y_pred'].values).iloc[:25,:])\n#     display(Metrics.display_f1(report[\"classification\"]))\n    gc.collect()\n  \n  return pd.DataFrame(results).sort_values(by=['log_loss']).reset_index(drop=True)\n\nif TRAIN:\n  results = train()","46356a6b":"results.drop(['losses', 'classification', 'y_pred'], axis=1)","0d3d0632":"ensemble = Ensemble(\n    results,\n    train_targets,\n    splits=ENSEMBLE_SPLITS,\n)\n\ndef get_ensemble():\n    print('Getting ensemble')\n    start = time.time()\n    ensemble_submission, ensemble_result = ensemble.improve()\n    print(f'Time: {time.time() - start}')    \n    submitted = pd.DataFrame(ensemble_submission, columns=train_targets.columns)\n    submitted.insert(0, column='sig_id', value=submission['sig_id'])\n    submitted.loc[test_cptypes == 'ctl_vehicle', train_targets.columns] = 0\n    submitted.to_csv('submission.csv', index=False)\n    return submitted, ensemble_result\n\ndef get_best(best_model):\n    print('Getting best model')\n    submitted = pd.read_csv(f'{best_model}_submission.csv')\n    submitted.to_csv('submission.csv', index=False)\n    return submitted, None\n\nbest_model = results.iloc[0]['model']\nsubmitted, result = get_ensemble() if ENSEMBLE else get_best(best_model)","a4012cb2":"display(Metrics.display_column_log_loss(train_targets, result))","a6e8fb64":"display(Metrics.display_f1(Metrics.classification_report(train_targets.values, result, train_targets.columns)))","9d3004bc":"submitted.head()","21132dcb":"# Create Models","cdc62dac":"# Visualise Data","653eada1":"# Load Data","e119d3cb":"# Create Engine","8dfc5c4f":"# Pre Process","cab39ecf":"# Post training","f8186c4f":"# Train and Evaluate"}}