{"cell_type":{"d4c253d6":"code","475b84b1":"code","21aea6be":"code","d8e61f52":"code","70d605ad":"code","e1c747bd":"code","b12cc786":"code","1437f7bb":"code","3b93e6fd":"code","a5e8a0e8":"code","7a80fb25":"code","6b18bf25":"code","f2700d84":"code","23642ba1":"code","bae30670":"code","27df3147":"code","9fa04cc5":"code","108ce3a4":"code","f6c61822":"code","24402dee":"code","c13ef1ea":"code","e34c5669":"code","2dced63e":"code","c836f858":"code","7ecd4e40":"code","4eed600c":"code","0fcc8de2":"code","61ecf3ab":"code","cb537c7a":"code","71e0667f":"code","daa6f90a":"code","679b0793":"code","8d6242d2":"code","329d6a89":"code","282fff05":"code","b47eb65e":"code","91e50792":"code","781f9ad1":"code","12552eeb":"code","d954c780":"code","e3d65f8f":"code","008b8d6b":"code","c70ab4c7":"code","05fe90f6":"code","562ea50c":"markdown","10ada13b":"markdown","13aae2dc":"markdown","d848b667":"markdown","4a13e552":"markdown","e13c3aa9":"markdown","d662b5f8":"markdown","23d5af3d":"markdown","bca32483":"markdown","8aaa85eb":"markdown","fb1e2a07":"markdown","c9daec39":"markdown","576341d7":"markdown","d40e919e":"markdown","1e2f302b":"markdown","6c5c5b29":"markdown","1dec0e2e":"markdown","4ae4d62c":"markdown","78cf73ad":"markdown","45663e91":"markdown","e1f8a6ae":"markdown"},"source":{"d4c253d6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport matplotlib.pyplot as plt # side-stepping mpl backend\nimport matplotlib.gridspec as gridspec # subplots\nimport mpld3 as mpl\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\nfrom sklearn.model_selection import cross_validate\n#Import models from scikit learn module:\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n   #For K-fold cross validation\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier, export_graphviz\nfrom sklearn import metrics\nfrom pandas.plotting import scatter_matrix\nfrom sklearn import svm  #for Support Vector Machine (SVM) Algorithm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import MinMaxScaler","475b84b1":"data = pd.read_csv(\"..\/input\/breast-cancer-wisconsin-data\/data.csv\")","21aea6be":"data.head(20)","d8e61f52":"data.columns","70d605ad":"data.isnull().sum()","e1c747bd":"data.drop(['Unnamed: 32',\"id\"], axis=1, inplace=True)\ndata.diagnosis = [1 if each == \"M\" else 0 for each in data.diagnosis]\ny = data.diagnosis.values\nx_data = data.drop(['diagnosis'], axis=1)\n","b12cc786":"sns.countplot(data['diagnosis'],label=\"Count\")","1437f7bb":"scaler = MinMaxScaler(feature_range=(0, 1))\nx = scaler.fit_transform(x_data)\ntype(x)","3b93e6fd":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.15, random_state=42)","a5e8a0e8":"logr = LogisticRegression()\nlogr.fit(x_train,y_train)\ny_predict_lr = logr.predict(x_test)\nacc_log = metrics.accuracy_score(y_predict_lr,y_test)*100\nprint('LogisticRegression accuracy(in %):', acc_log)","7a80fb25":"sv = SVC() #select the algorithm\nsv.fit(x_train,y_train) # we train the algorithm with the training data and the training output\ny_predict_svm = sv.predict(x_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_predict_svm,y_test)*100\nprint('SVM accuracy(in %):', acc_svm)","6b18bf25":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\ny_predict_dt = dt.predict(x_test)\nacc_dt = metrics.accuracy_score(y_predict_dt,y_test)*100\nprint('Decision Tree accuracy(in %):', acc_dt)","f2700d84":"from sklearn.metrics import confusion_matrix\nrf = RandomForestClassifier(n_estimators = 40)\nrf.fit(x_train,y_train)\nrf_pred = rf.predict(x_test)\nacc_rfc = metrics.accuracy_score(rf_pred,y_test)*100\nprint(\"RandomForestClassifier accuracy(in %):\", acc_rfc)","23642ba1":"from sklearn.naive_bayes import GaussianNB \ngnb = GaussianNB() \ngnb.fit(x_train, y_train) \ngnb_pred = gnb.predict(x_test)\nacc_gnb = metrics.accuracy_score(gnb_pred,y_test)*100\nprint(\"Gaussian Naive Bayes model accuracy(in %):\", acc_gnb)","bae30670":"knc = KNeighborsClassifier(n_neighbors=5)\nknc.fit(x_train,y_train)\ny_predict_knn = knc.predict(x_test)\nacc_knn = metrics.accuracy_score(y_predict_knn,y_test)*100\nprint(\"KNN model accuracy(in %):\", acc_knn)","27df3147":"models = pd.DataFrame({\n    'Model': ['Logistic Regression', 'Support Vector Machines','Decision Tree',\n              'Random Forest Classifier','Gaussian Naive Bayes',\n              'K-Nearest Neighbours'],\n    'Score': [acc_log,acc_svm,acc_dt,acc_rfc,acc_gnb,acc_knn]})\nmodels","9fa04cc5":"vecTest = y_test.reshape((-1))\nresult = pd.DataFrame({'ActualValues':vecTest,'LogisticRegression':y_predict_lr,'SVM':y_predict_svm,'DecisionTree':y_predict_dt,'KNN':y_predict_knn,'GausianBayes':gnb_pred,'RandomForest':rf_pred})\nresult","108ce3a4":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_recall_fscore_support","f6c61822":"plt.figure(figsize=(10,15))\n# Logistic Regression\nplt.subplot(4,2,1)\ncm = confusion_matrix(y_test, y_predict_lr) \ncm_lr = pd.DataFrame(cm)\nsns.heatmap(cm_lr,cmap=\"Oranges\",annot = True)\nplt.title('Logistic Regression \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_lr)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#Support Machine Vector\nplt.subplot(4,2,2)\ncm = confusion_matrix(y_test, y_predict_svm) \ncm_svm = pd.DataFrame(cm)\nsns.heatmap(cm_svm,cmap=\"Blues\",annot = True)\nplt.title('SVM \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_svm)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#Decision Tree\nplt.subplot(4,2,3)\ncm = confusion_matrix(y_test, y_predict_dt) \ncm_dt = pd.DataFrame(cm)\nsns.heatmap(cm_dt,cmap=\"Purples\",annot = True)\nplt.title('Decision Tree \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_dt)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#KNN\nplt.subplot(4,2,4)\ncm = confusion_matrix(y_test, y_predict_knn) \ncm_knn = pd.DataFrame(cm)\nsns.heatmap(cm_dt,cmap=\"Greens\",annot = True)\nplt.title('KNN \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, y_predict_knn)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#Gausian Bayes\nplt.subplot(4,2,5)\ncm = confusion_matrix(y_test, gnb_pred) \ncm_gb = pd.DataFrame(cm)\nsns.heatmap(cm_gb,cmap=\"Greys\",annot = True)\nplt.title('Gausian Bayes \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, gnb_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n#Random Forest Classifier\nplt.subplot(4,2,6)\ncm = confusion_matrix(y_test, rf_pred) \ncm_rf = pd.DataFrame(cm)\nsns.heatmap(cm_rf,cmap=\"Reds\",annot = True)\nplt.title('Random Forest \\nAccuracy:{0:.3f}'.format(accuracy_score(y_test, rf_pred)))\nplt.ylabel('True label')\nplt.xlabel('Predicted label')\n\nplt.tight_layout()\nplt.show()","24402dee":"from sklearn.metrics import recall_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import f1_score","c13ef1ea":"print(\"Logistic Regression Precision score: {}\".format(precision_score(y_test,y_predict_lr)))\nprint(\"Logistic Regression Recall score: {}\".format(recall_score(y_test,y_predict_lr)))\nprint(\"Logistic Regression F1 Score: {}\".format(f1_score(y_test,y_predict_lr)))","e34c5669":"print(\"SVM Precision score: {}\".format(precision_score(y_test,y_predict_svm)))\nprint(\"SVM Recall score: {}\".format(recall_score(y_test,y_predict_svm)))\nprint(\"SVM F1 Score: {}\".format(f1_score(y_test,y_predict_svm)))","2dced63e":"print(\"Decision Tree Precision score: {}\".format(precision_score(y_test,y_predict_dt)))\nprint(\"Decision Tree Recall score: {}\".format(recall_score(y_test,y_predict_dt)))\nprint(\"Decision Tree F1 Score: {}\".format(f1_score(y_test,y_predict_dt)))","c836f858":"print(\"KNN Precision score: {}\".format(precision_score(y_test,y_predict_knn)))\nprint(\"KNN Recall score: {}\".format(recall_score(y_test,y_predict_knn)))\nprint(\"KNN F1 Score: {}\".format(f1_score(y_test,y_predict_knn)))","7ecd4e40":"print(\"GNB Precision score: {}\".format(precision_score(y_test,gnb_pred)))\nprint(\"GNB Recall score: {}\".format(recall_score(y_test,gnb_pred)))\nprint(\"GNB F1 Score: {}\".format(f1_score(y_test,gnb_pred)))","4eed600c":"print(\"Random Forest Precision score: {}\".format(precision_score(y_test,rf_pred)))\nprint(\"Random Forest Recall score: {}\".format(recall_score(y_test,rf_pred)))\nprint(\"Random Forest F1 Score: {}\".format(f1_score(y_test,rf_pred)))","0fcc8de2":"\nplt.bar(models['Model'],models['Score'])\nplt.xticks(rotation=90)","61ecf3ab":"from sklearn.model_selection import cross_val_score","cb537c7a":"cvs_lr = cross_val_score(LogisticRegression(),x,y,cv=5)\nmax_lr = np.amax(cvs_lr)\ncvs_svm = cross_val_score(SVC(),x,y,cv=5)\nmax_svm = np.amax(cvs_svm)\ncvs_gnb = cross_val_score(GaussianNB(),x,y,cv=5)\nmax_gnb = np.amax(cvs_gnb)\ncvs_rf = cross_val_score(RandomForestClassifier(n_estimators = 40),x,y,cv=5)\nmax_rf = np.amax(cvs_rf)\n\nscores = pd.DataFrame({\n    \n    'Name' : {0:'LogisticRegression',1:'Support Vector Machines',2:'Gaussian Naive Bayes',3:'Random Forest Classifier'} ,  \n    'Cross_val' : {0:cvs_lr,1:cvs_svm,2:cvs_gnb,3:cvs_rf}\n    \n})\nscores","71e0667f":"print('Max Cross Validation Score of Logistic Regression {}'.format(max_lr))\nprint('Max Cross Validation Score of Support Machine Vector {}'.format(max_svm))\nprint('Max Cross Validation Score of Gaussian Naive Bayes {}'.format(max_gnb))\nprint('Max Cross Validation Score of Random Forest Classifier {}'.format(max_rf))\n\n","daa6f90a":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(x_data.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","679b0793":"X_train, X_validation, Y_train, Y_validation = train_test_split(x, y, test_size=0.20)\nX_val, X_test, Y_val, Y_test = train_test_split(X_validation, Y_validation, test_size=0.40)\n","8d6242d2":"import tensorflow as tf\nfrom tensorflow.keras import layers\nfrom keras.regularizers import l2","329d6a89":"Y_train = Y_train.reshape((-1,1))","282fff05":"Y_train.shape","b47eb65e":"learning_rate = 0.001\nepochs = 200\nbatch_size = 256\n\nmodel = tf.keras.models.Sequential()\n    \nmodel.add(tf.keras.layers.Dense(units=8,input_shape=(X_train.shape[1],),activation= tf.nn.relu,\n                                    kernel_regularizer=l2(0.001),name='Input'))\nmodel.add(tf.keras.layers.Dense(units = 16,activation = tf.nn.relu,\n                                    kernel_regularizer=l2(0.001),name='HiddenLayer-1'))\nmodel.add(tf.keras.layers.Dense(units = 16,activation = tf.nn.relu,\n                                    kernel_regularizer=l2(0.001),name='HiddenLayer-2'))\nmodel.add(tf.keras.layers.Dense(units = 32,activation = tf.nn.relu,\n                                    kernel_regularizer=l2(0.001),name='HiddenLayer-3'))\nmodel.add(tf.keras.layers.Dense(Y_train.shape[1],activation=tf.nn.sigmoid,name='Output'))\n\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=learning_rate),                                                   \n                loss=tf.keras.losses.BinaryCrossentropy(),\n                metrics=['accuracy'])","91e50792":"history = model.fit(x=x, y=y,validation_data = (X_test,Y_test),batch_size=batch_size,epochs=epochs)","781f9ad1":"model.evaluate(X_test, Y_test, verbose=1,batch_size=batch_size)","12552eeb":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='upper right')\nplt.show()","d954c780":"plt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('Model accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Val'], loc='lower right')\nplt.show()","e3d65f8f":"pred = model.predict(X_test)\npred_values = pd.DataFrame(pred,columns = ['label'])\npred_values['Model_Prediction'] = (pred_values.label >0.5).astype('int')\npred_values['Actual_values'] = Y_test\npred_values.drop('label', axis = 1)","008b8d6b":"from tensorflow.keras.utils import plot_model\nplot_model(model)","c70ab4c7":"# sns.pairplot(data[0:10], hue='diagnosis',palette='cubehelix',kind = 'reg')","05fe90f6":"clear","562ea50c":"* SVM is the algorithm that optimally separates between two different classes. SVM is a nonparametric algorithm and it is setting with the condition that a line is farthest from the elements of two different classes. SVM is generally used when classifying data that is linear.","10ada13b":"* Naive Bayes algorithm is one of the Machine Learning classification algorithms. It is most commonly used text classification problems. Naive Bayes is the simplest shape of Bayesian network, all attributes are unbiased given the value of the elegance variable. This is known as conditional independence. Itis apparent that the conditional independence assumption is rarely actual in maximum real-world applications","13aae2dc":"### Confusion matrix\nConfusion matrix is the performance measurement of the algorithm. Confusion matrix is a 2x2 matrix and it includes 4 parameters. True Positive , False Negative , False Positive, True Negative. It can also be defined as a summary of the algorithm.\n- True Positive = It is a result of the model where positive classes correctly predicted.\n- True Negative = It is a result of the model where negative classes correctly predicted.\n- False Positive = It is a result of the model where positive classes incorrectly predicted.\n- False Negative = It is a result of the model where negative classes incorrectly predicted.","d848b667":"### Random Forest Classifier","4a13e552":"#### Mix-max scaler\n- Mix-max normalization was used to reduce or eliminate redundancy. Min-max normalization is used to scale the values of properties between 0 and 1.","e13c3aa9":"### Deep Learning","d662b5f8":"### Support Vector Machines","23d5af3d":"### K-Nearest Neighbours","bca32483":"### Decision Tree","8aaa85eb":"Table of algorithm result and actual values.","fb1e2a07":"* The k nearest neighbors algorithm (KNN) is a algorithm that used for regression and classification. KNN is a nonparametric, lazy learning algorithm. KNN is a nonparametric, lazy learning algorithm. In both cases, the input consists of the closest training examples in the feature area. KNN is an instance based learning, where the function only approaches locally and all calculations are delayed until classification.\n* KNN tries to guess which group they belong to by calculating the distance of each entry from other inputs.","c9daec39":"###  Hyper Parameters\n\n#### Learning Rate\n* Gradient descent algorithms multiply the gradient by a scalar known as the learning rate to determine the next point.\n\n##### Batch size\n* Total number of training examples present in a single batch.\n\n##### Epoch\n* One Epoch is when an entire dataset is passed forward and backward through the neural network only once.\n\n#### L2 Regularizer\n* L2 penalizes weight2.\n* The derivative of L2 is 2 * weight\n\n#### Binary Crossentorpy \n* Binary crossentropy measures how far away from the true value. penalizes the probability based on the distance from the expected value.\n\n##### RMSprop \n* Maintain a moving (discounted) average of the square of gradients\n\n* Divide the gradient by the root of this average\n\n* The RMSprop optimizer restricts the oscillations in the vertical direction.    \n\n* Idea is to reach the global minima where the cost function attains the least possible value","576341d7":"Deep learning grants computational models occurring of more than one processing layers to learn the likeness of data with multiple levels of abstraction. Through these methods, technology in many other areas such as speech recognition, visual object recognition, object detection, drug discovery and genomics has been significantly improved.","d40e919e":"* Random Forest classifier is selects random sub sets from data set and with that method decision trees are generated. Every decision tree makes guesses and random forest chooses most voted one. It gives grate result without any hyper parameter. It is also one of the most used algorithms because it provides simplicity and usability for both classification and regression problems.","1e2f302b":"### Gaussian Naive Bayes","6c5c5b29":"### Pre-processing","1dec0e2e":"* Preprocessing is a stage that steps reduce the runtime before the model is run, eliminate contradictions in the dataset, and eliminate features that increase complexity, so the model works better. With pre-processing, the data is made available to be used in later stages.\n* Also normalization is one of the pre-processing steps. In some data sets, the difference in value between data can be very large, which leads to redundancy. Normalization is used to prevent this problem. Z-transform normalization and min-max normalization are just a few of the processes used.","4ae4d62c":"### Logistic Regression","78cf73ad":"* The DTC is an approach to multistage decision making. The main idea is any multistage approach is to split complex decisions into simpler and smaller ones up to gain the desired solution.\n* Decision tree classifiers are commonly used in signal classification, image recognition, medical diagnosis, speech recognition, and more. \n* It calculates the probability of a given value to each of the categories or classifying the values to assign the most likely class.","45663e91":"#### Precision\n\n- Precision shows the proportion how many of the values we estimate as Positive are truly correct.\n\n#### Recall\n- Recall shows the proportion of how many of the actual positives are correctly identified. It also called True Positive Rate. (TPR)\n\n#### F1 Score\n- F1 score is the harmonic mean of precision and recall evaluation matrices.","e1f8a6ae":"* Logistic regression generates a probability not specific value (0 or 1).\n* Logistic regression models use as their convergence criterion the maximization of a probability function. p coefficients can effortlessly converted into the corresponding odds ratios by rearing e to the coefficient if variables are represented by a single linear label and one caninterpret the magnitude of importance of a predictor."}}