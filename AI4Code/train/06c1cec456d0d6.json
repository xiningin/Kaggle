{"cell_type":{"f4aa054b":"code","180d5a1c":"code","6ba62ff4":"code","18af0930":"code","dd10b8e1":"code","32ebebb9":"code","6ad906a0":"code","07c6424e":"code","68498b61":"code","b706d03d":"code","6cd80fe8":"code","56671bf5":"code","7d3264b3":"code","2c6f02ff":"code","83542a9c":"code","7bff7626":"code","079fd39e":"code","d022fe10":"code","65cea048":"code","14dfdf62":"code","3434799d":"code","ef6af660":"code","6fbfc386":"code","91e45d7d":"code","afae4021":"code","5c637e7b":"code","64ff93dc":"code","a3a952cc":"code","4a2ee32b":"code","6d06c4b6":"code","e424302f":"code","e7627e78":"code","e8b824d1":"code","441b189f":"code","3ff8f930":"code","f3286361":"code","aa444dfb":"code","46338d55":"code","44a0572e":"code","f5af4505":"code","97324122":"code","eed1a161":"code","78939d7a":"code","716c3207":"code","6167516e":"code","a4d48557":"code","3cd811f3":"code","eb2d02a5":"code","5fe427cc":"code","3f71ae7c":"code","c5ffbf18":"code","a15d5b6f":"code","7e06c718":"code","b6683384":"code","820a91f9":"markdown","194c343e":"markdown","b4e445db":"markdown","e694ed9a":"markdown","ee56a706":"markdown","b7ea5585":"markdown","a32ee50a":"markdown","8938827a":"markdown","00448a79":"markdown","c4f477ec":"markdown","36e3844e":"markdown","909165c2":"markdown","6e67eff5":"markdown","242d9acb":"markdown","4d73d2c0":"markdown","3d215f64":"markdown","dd049836":"markdown","aec96276":"markdown","70635b60":"markdown","1aae4c1e":"markdown"},"source":{"f4aa054b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(\"ignore\")","180d5a1c":"df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndf.head()","6ba62ff4":"df.shape","18af0930":"# Looking for missing values in the dataset\ndf.isna().sum()","dd10b8e1":"df.dtypes","32ebebb9":"df.describe()","6ad906a0":"# Having a look at the correlation matrix\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.heatmap(df.corr(), annot=True, fmt='.1g', cmap=\"viridis\", cbar=False);","07c6424e":"plt.style.use(\"seaborn\")\nfig, ax = plt.subplots(figsize=(7,7))\nplt.pie(x=df[\"Outcome\"].value_counts(), \n        colors=[\"seagreen\",\"firebrick\"], \n        labels=[\"Non-Diabetic\",\"Diabetic\"], \n        shadow = True, \n        explode = (0, 0.1)\n        )\nplt.show()","68498b61":"df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']] = df[['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']].replace(0, np.nan)\ndf.isnull().sum()","b706d03d":"# Filling null values with the median\n\nfor col in [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"BMI\", \"Insulin\"]:\n    df[col] = df[col].fillna(df[col].median())","6cd80fe8":"plt.style.use(\"seaborn\")\nfig, ax = plt.subplots(3,3, figsize=(20,25)) \n\n\nsns.histplot(df[\"Age\"], ax=ax[0,0], color=\"darkorange\", kde=True); \nax[0,0].set_xlabel(\"Age\",fontsize=15)\n\nsns.histplot(df[\"Pregnancies\"], ax=ax[0,1], color=\"darkorange\",kde=True); \nax[0,1].set_xlabel(\"Pregnancies\",fontsize=15)\n\nsns.histplot(df[\"BloodPressure\"], ax=ax[0,2], color=\"darkorange\",kde=True); \nax[0,2].set_xlabel(\"BloodPressure\",fontsize=15)\n\n\n\nsns.histplot(x = df[\"BMI\"], ax=ax[1,0], color=\"mediumblue\",kde=True); \nax[1,0].set_xlabel(\"BMI\",fontsize=15)\n\nsns.histplot(x = df[\"Glucose\"], ax=ax[1,1], color=\"mediumblue\",kde=True); \nax[1,1].set_xlabel(\"Glucose\",fontsize=15)\n\nsns.histplot(x = df[\"Insulin\"], ax=ax[1,2], color=\"mediumblue\",kde=True); \nax[1,2].set_xlabel(\"Insulin\",fontsize=15)\n\n\nsns.histplot(x = df[\"DiabetesPedigreeFunction\"], ax=ax[2,0], color=\"darkgreen\",kde=True); \nax[2,0].set_xlabel(\"DiabetesPedigreeFunction\",fontsize=15)\n\nsns.histplot(x = df[\"SkinThickness\"], ax=ax[2,1], color=\"darkgreen\",kde=True); \nax[2,1].set_xlabel(\"SkinThickness\",fontsize=15)\n\nsns.histplot(x = df[\"Outcome\"], ax=ax[2,2], color=\"darkgreen\",kde=True); \nax[2,2].set_xlabel(\"Outcome\",fontsize=15);","56671bf5":"plt.style.use(\"seaborn\")\nfig, ax =plt.subplots(4,2, figsize=(20,25)) \n\n\nsns.histplot(x = df[\"Age\"], hue = df[\"Outcome\"], palette=\"rocket\", kde=True, ax=ax[0,0]);\nax[0,0].set_xlabel(\"Age\",fontsize=15)\n\nsns.histplot(x = df[\"Pregnancies\"], hue = df[\"Outcome\"], palette=\"rocket\", kde=True, ax=ax[0,1]);\nax[0,1].set_xlabel(\"Pregnancies\",fontsize=15)\n\n\nsns.histplot(x = df[\"Insulin\"], hue = df[\"Outcome\"], palette=\"dark\", kde=True, ax=ax[1,0]);\nax[1,0].set_xlabel(\"Insulin\",fontsize=15)\n\nsns.histplot(x = df[\"Glucose\"], hue = df[\"Outcome\"], palette=\"dark\", kde=True, ax=ax[1,1]);\nax[1,1].set_xlabel(\"Glucose\",fontsize=15)\n\n\nsns.histplot(x = df[\"BMI\"], hue = df[\"Outcome\"], palette=\"flare\", kde=True, ax=ax[2,0]);\nax[2,0].set_xlabel(\"BMI\",fontsize=15)\n\nsns.histplot(x = df[\"BloodPressure\"], hue = df[\"Outcome\"], palette=\"flare\", kde=True, ax=ax[2,1]);\nax[2,1].set_xlabel(\"BloodPressure\",fontsize=15)\n\n\nsns.histplot(x = df[\"SkinThickness\"], hue = df[\"Outcome\"], palette=\"viridis\", kde=True, ax=ax[3,0]);\nax[3,0].set_xlabel(\"SkinThickness\",fontsize=15)\n\nsns.histplot(x = df[\"DiabetesPedigreeFunction\"], hue = df[\"Outcome\"], palette=\"viridis\", kde=True, ax=ax[3,1]);\nax[3,1].set_xlabel(\"DiabetesPedigreeFunction\",fontsize=15);","7d3264b3":"plt.style.use(\"seaborn\")\nfig, ax =plt.subplots(2,2, figsize=(20,15)) \n\nsns.scatterplot(x = df['Glucose'], y = df['Age'], hue = df['Outcome'], palette='viridis', legend=True, ax=ax[0,0])\nplt.legend(title='Result', loc='upper left', labels=['Healthy' , 'Diabetic']);\n\nsns.scatterplot(x = df['BloodPressure'], y = df['Age'], hue = df['Outcome'], palette='spring_r', legend=True, ax=ax[0,1])\nplt.legend(title='Result', loc='upper left',labels=['Healthy' , 'Diabetic']);\n\nsns.scatterplot(x = df['BMI'], y = df['Age'], hue = df['Outcome'], palette='flare_r', legend=True, ax=ax[1,0])\nplt.legend(title='Result', labels=['Healthy' , 'Diabetic']);\n\nsns.scatterplot(x = df['Insulin'], y = df['Age'], hue = df['Outcome'], palette='hls', legend=True, ax=ax[1,1])\nplt.legend(title='Result', labels=['Healthy' , 'Diabetic']);","2c6f02ff":"df.head()","83542a9c":"# X data\nX = df.drop(\"Outcome\", axis=1)\nX.head()","7bff7626":"# y data\ny = df[\"Outcome\"]\ny.head()","079fd39e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","d022fe10":"len(X_train), len(X_test)","65cea048":"# Scaling the data \n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","14dfdf62":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)","3434799d":"LogisticRegressionScore = lr.score(X_test, y_test)\nprint(\"Accuracy obtained by Logistic Regression model:\",LogisticRegressionScore*100)","ef6af660":"# Having a look at the confusion matrix\n\nfrom sklearn.metrics import confusion_matrix, classification_report\n\ny_pred = lr.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for Logistic Regression\", fontsize=14, fontname=\"Helvetica\", y=1.03);","6fbfc386":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators = 100)\nrf.fit(X_train,y_train)","91e45d7d":"RandomForestClassifierScore = rf.score(X_test, y_test)\nprint(\"Accuracy obtained by Random Forest Classifier model:\",RandomForestClassifierScore*100)","afae4021":"# Having a look at the confusion matrix\ny_pred = rf.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for Random Forest Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","5c637e7b":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(4)\nknn.fit(X_train,y_train)","64ff93dc":"KNeighborsClassifierScore = knn.score(X_test, y_test)\nprint(\"Accuracy obtained by K Neighbors Classifier model:\",KNeighborsClassifierScore*100)","a3a952cc":"# Having a look at the confusion matrix\ny_pred = knn.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for K Neighbors Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","4a2ee32b":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree.fit(X_train, y_train)","6d06c4b6":"DecisionTreeClassifierScore = tree.score(X_test,y_test)\nprint(\"Accuracy obtained by Decision Tree Classifier model:\",DecisionTreeClassifierScore*100)","e424302f":"# Confusion matrix\ny_pred = tree.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for Decision Tree Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","e7627e78":"from catboost import CatBoostClassifier\ncat = CatBoostClassifier(iterations=40)\ncat.fit(X_train, y_train);","e8b824d1":"CatBoostClassifierScore = cat.score(X_test,y_test)\nprint(\"Accuracy obtained by CatBoost Classifier model:\",CatBoostClassifierScore*100)","441b189f":"# Confusion matrix\ny_pred = cat.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for CatBoost Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","3ff8f930":"from sklearn.ensemble import GradientBoostingClassifier\ngb = GradientBoostingClassifier()\ngb.fit(X_train, y_train)","f3286361":"GradientBoostingClassifierScore = gb.score(X_test,y_test)\nprint(\"Accuracy obtained by Gradient Boosting Classifier model:\",GradientBoostingClassifierScore*100)","aa444dfb":"# Confusion matrix\ny_pred = gb.predict(X_test)\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, cmap=\"Spectral\")\nplt.title(\"Confusion Matrix for Gradient Boosting Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","46338d55":"plt.style.use(\"seaborn\")\n\nx = [\"LogisticRegression\", \n     \"Decision Tree Classifier\", \n     \"RandomForestClassifier\", \n     \"KNeighborsClassifier\", \n     \"CatBoost Classifier\", \n     \"Gradient Boosting Classifier\"]\n\ny = [LogisticRegressionScore, \n     DecisionTreeClassifierScore, \n     RandomForestClassifierScore, \n     KNeighborsClassifierScore, \n     CatBoostClassifierScore, \n     GradientBoostingClassifierScore]\n\nfig, ax = plt.subplots(figsize=(8,6))\nsns.barplot(x=x,y=y, palette=\"crest\");\nplt.ylabel(\"Model Accuracy\")\nplt.xticks(rotation=40)\nplt.title(\"Model Comparison - Model Accuracy\", fontsize=14, fontname=\"Helvetica\", y=1.03);","44a0572e":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'criterion' : ['gini', 'entropy'],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\ngrid_search = GridSearchCV(estimator = rf, \n                           param_grid = param_grid, \n                           cv = 3, n_jobs = -1, verbose = 2)","f5af4505":"grid_search.fit(X_train, y_train)","97324122":"grid_search.best_params_","eed1a161":"grid_search.best_score_","78939d7a":"grid_search_rfc_predict = grid_search.predict(X_test)","716c3207":"print('Improvement in Random Forest Classifier after GridSearchCV: {:0.2f}%.'.format(100 * (grid_search.best_score_ - RandomForestClassifierScore) \/ RandomForestClassifierScore))","6167516e":"# Comparing Random Forest Classifier and The GridSearched Random Forest Classifier\n\nplt.style.use(\"seaborn\")\n\nx = [\"Random Forest Classifier\",  \n     \"GridSearch-RandomForestClassifier\"]\n\ny = [RandomForestClassifierScore,  \n     grid_search.best_score_]\n\nfig, ax = plt.subplots(figsize=(7,7))\nsns.barplot(x=x,y=y, palette=\"crest\");\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=45)\nplt.title(\"Random Forest Classifier  vs  GridSearched Random Forest Classifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","a4d48557":"param_grid = [\n    {\n      \"penalty\": [\"l1\", \"l2\", \"elastic\", \"none\" ],\n      \"C\" : np.logspace(-4, 4, 20),\n      \"solver\" : [\"sag\", \"saga\", \"lbfgs\", \"liblinear\", \"newton-cg\"],\n      \"max_iter\" : [100, 1000, 2500, 5000]\n    }\n]\n\ngrid_search_lr = GridSearchCV(estimator = lr, \n                              param_grid = param_grid, \n                              cv = 5, n_jobs = -1, verbose = True)","3cd811f3":"grid_search_lr.fit(X_train, y_train)","eb2d02a5":"grid_search_lr.best_params_","5fe427cc":"grid_search_lr.best_score_","3f71ae7c":"grid_search_lr_predict = grid_search_lr.predict(X_test)","c5ffbf18":"print('Improvement in Logistic Regression after GridSearchCV: {:0.2f}%.'.format(100 * (grid_search_lr.best_score_ - LogisticRegressionScore) \/ LogisticRegressionScore))","a15d5b6f":"# Comparing Logistic Regression and The GridSearched Logistic Regression\n\nplt.style.use(\"seaborn\")\n\nx = [\"Logistic Regression\",\n     \"GridSearch-LogisticRegression\"]\n\ny = [LogisticRegressionScore,\n     grid_search_lr.best_score_]\n\nfig, ax = plt.subplots(figsize=(7,7))\nsns.barplot(x=x,y=y, palette=\"crest\");\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=45)\nplt.title(\"LogisticRegression  vs  GridSearched LogisticRegression\", fontsize=14, fontname=\"Helvetica\", y=1.03);","7e06c718":"# Comparing both the improved models\n\nplt.style.use(\"seaborn\")\n\nx = [\"GridSearch LogisticRegression\", \"GridSearch RandomForestClassifier\"]\n\ny = [grid_search_lr.best_score_,  grid_search.best_score_]\n  \nfig, ax = plt.subplots(figsize=(7,7))\nsns.barplot(x=x,y=y, palette=\"viridis\");\nplt.ylabel(\"Accuracy\")\nplt.xticks(rotation=45)\nplt.title(\"GridSearched LogisticRegression  vs  GridSearched RandomForestClassifier\", fontsize=14, fontname=\"Helvetica\", y=1.03);","b6683384":"# Classification Report of Random Forest Classifier\n\nprint(classification_report(y_test, grid_search_rfc_predict))","820a91f9":"## Random Forest Classifier","194c343e":"## Gradient Boosting Classifier","b4e445db":"## Decision Tree Classifier","e694ed9a":"####  `Random Forest Classifier` and `Logistic Regression` perform best on the test set","ee56a706":"## Splitting the data into training and test datasets\nHere, we are trying to predict whether the patient has diabetes or not using the given data. Hence, the `Outcome` will be the y label and rest of the data will be the X or the input data.","b7ea5585":"#### After Hyperparameter tuning Logistic Regression and Random Forest Classifier, the `Random Forest Classifier model performs better among the two!`","a32ee50a":"## Loading up the data","8938827a":"### Comparing the results after improvement in Logistic Regression","00448a79":"## K Neighbors Classifier","c4f477ec":"## Comparing performance of the models","36e3844e":"## Logistic Regression","909165c2":"* **Pregnancies** : Number of Pregnancies\n\n* **Glucose** : Plasma glucose concentration\n\n* **BloodPressure** : Diastolic blood pressure (mm Hg)\n\n* **SkinThickness** : Triceps skin fold thickness (mm)\n\n* **Insulin** : 2-Hour serum insulin (mu U\/ml)\n\n* **BMI** : Body Mass Index (weight in kg\/(height in m)^2)\n\n* **DiabetesPedigreeFunction** : Diabetes pedigree function (a function which scores likelihood of diabetes based on family history).\n\n* **Age** : Age (years)\n\n* **Outcome** : Whether the patient is diabetic or not, 0 represents the person is not diabetic and 1 represents that the person is diabetic.","6e67eff5":"## Hyperparameter Tuning on Logistic Regression","242d9acb":"# Predicting Diabetes \ud83e\ude7a","4d73d2c0":"### Comparing the results after improvement in Random Forest Classifier","3d215f64":"## CatBoost Classifier","dd049836":"#### If you like my work, It will be really great of you to upvote this notebook!\n#### If not then you leaving a comment on what do I need to work on and improve will be really helpful!","aec96276":"### Now, comparing both `Logistic Regression` and `Random Forest Classifier` to note which performed better after hyperparameter tuning on them","70635b60":"## Hyperparameter Tuning on Random Forest Classifier","1aae4c1e":"## Importing Libraries"}}