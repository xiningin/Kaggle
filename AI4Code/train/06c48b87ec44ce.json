{"cell_type":{"a87d7b57":"code","ecbdb667":"code","a8380321":"code","2ec63274":"code","c1307c5a":"code","e19d4e4e":"code","18a0c382":"code","e3c4da20":"code","496bbddd":"code","0b8f2b19":"code","f462cfb9":"code","f5b6f32e":"code","8107048a":"code","a6b4cdf5":"code","6a08f2bb":"code","46a54e47":"code","63402c9f":"code","088a3e92":"code","8c2dc886":"code","e2b433cc":"code","6e631519":"code","492e8afa":"code","d4aad980":"code","dce189f1":"code","a426545d":"code","44b51796":"code","cb5fc84d":"code","ac9b7f9c":"code","b75e548b":"code","8e3c25d9":"code","51a1c1cf":"code","8dd3a094":"code","9755ac79":"code","32669bd7":"code","d59bb2ba":"code","58adefcd":"code","37f99b52":"code","058a040b":"code","5f7357bf":"code","8c900b98":"code","f952c01c":"code","74a5fec2":"code","90a1b52e":"code","4e4cf55d":"code","3a8114d9":"code","ee4de40a":"code","da609316":"code","6cc08417":"code","4353b189":"code","9abcb06d":"code","e41a9991":"code","62b55beb":"code","958f35a0":"code","aa233b42":"code","8156f5d2":"code","84128bbc":"code","0b9023af":"code","e9c1c949":"code","044b23ab":"code","ed8495f3":"code","e12a7a7e":"code","a7a7786d":"code","e6380924":"code","853b07dd":"code","6dc49bc9":"code","f3ce2d50":"code","b1a1b426":"code","e5306e4a":"code","f8edf61b":"code","37bb2e91":"code","2cd9fe85":"code","3e98799e":"code","44d1f79e":"code","2d93a0a3":"code","73177d15":"code","a31f8e99":"code","278b1b1d":"code","c08bd6f6":"code","492179c9":"code","c372bb38":"code","179473e6":"code","be4f2964":"code","19b7dbcf":"code","d73e5bb0":"code","7b4c816d":"code","35a7c8c2":"code","169673e6":"code","8fbedd7e":"code","4ff88441":"code","4e30af51":"code","da57e16f":"code","41c78aff":"code","6560d419":"code","475f66d5":"code","ea0a641b":"code","747548a5":"code","16d73bd2":"code","965c456a":"code","6d051085":"code","99710660":"code","7d7a896c":"code","f851e008":"code","1e0ebc40":"code","c41c5e12":"code","96c539d3":"code","7bc8654e":"code","e75fc427":"code","60ec782c":"code","bdaec00c":"code","e00fa14b":"code","49cc294b":"code","3bbfeaec":"code","80e1c7d9":"code","3dc5c35b":"code","e5a45fb7":"code","58dd0151":"code","a372b63d":"code","a6556a1e":"code","ebf49b81":"code","3dbd7a18":"code","b52ed2d1":"code","cb0be7e3":"code","633ca4cc":"code","521906d1":"code","f5f399c3":"code","6d618851":"code","9dccabf6":"code","c1b9ecc2":"code","c0436438":"code","be1066ec":"code","a13bacf4":"code","97fea765":"code","60571612":"code","a7750bc5":"code","fc23e165":"code","1bdaf2f0":"code","7ab50bb4":"code","dbcf4d88":"code","25c71818":"code","9e4fbc0c":"code","423e209a":"code","3e6bd450":"code","7fa4dbba":"code","ffc1b588":"code","94587b50":"code","490f11dd":"code","3098a31b":"code","98466dd8":"code","3b238510":"code","9699c8de":"code","37fe907d":"code","c38120f8":"code","48afe91a":"code","1961caa2":"code","a3835034":"code","02a969e5":"code","59540da5":"code","f1054b16":"code","2481ae54":"code","49d32bd1":"code","17e2520e":"code","95785861":"code","abefc03e":"code","3f20bb51":"code","0a115be9":"code","e7b83615":"code","32764560":"code","60918dbb":"code","a50d23c7":"code","c83a9f49":"code","0a0b81f8":"code","bdd0a68c":"code","1e3494c9":"code","41e9e795":"code","4af13799":"code","62333f80":"code","a84591e8":"code","aed6121e":"code","57b4b0b3":"code","d3404f10":"code","1ba38db5":"code","26b6b8f6":"code","ae238aef":"code","cc260ace":"code","de36b4ab":"code","96eb3bd1":"code","5503cc98":"code","0b0a5f54":"code","53b91687":"code","ba9a5155":"code","eda70952":"code","22c5282c":"code","c2b50807":"code","6ab6b8b7":"code","cb016046":"code","84dbff10":"code","3a018ac1":"code","6a1e9487":"code","a3291653":"code","b717b5bc":"code","444662d5":"code","92771d12":"code","00b09f68":"code","4d9926e1":"code","4551ab45":"code","1b8707d4":"code","ffdcdfaf":"code","488a9c64":"code","89d5bf83":"code","5df718f9":"code","19e9cf8a":"code","3f59f62b":"code","fcf1ab50":"code","5b73e832":"code","4e7a2ba1":"code","b1af677a":"code","c65a63c8":"code","77ea23f1":"code","206b889b":"code","64ee269a":"code","9f1136c6":"code","9e64c2b1":"code","0059e8e6":"code","1f8b65a7":"code","220c072e":"code","7a5f997b":"code","fc05d57b":"code","69aaca37":"code","d38e98ef":"code","460c00ce":"code","3c2152a8":"code","8ab96a81":"code","96241738":"code","d5f2642a":"code","1e90970f":"code","208f24f6":"code","afbfb97c":"code","72e2ee83":"code","e644c023":"code","6e389259":"code","cdebf95e":"code","31c1a819":"code","b88dc9a5":"code","32add1a6":"code","1d1d3e1b":"code","9934f857":"code","c579134e":"code","57293a55":"code","bf389857":"code","aa2532db":"code","78899c19":"code","33c6f998":"code","a24ff0cb":"code","02c87ad4":"code","58390038":"code","a75bb727":"code","120a85ed":"code","0be0945f":"code","92eee940":"code","435db04b":"code","148fa20f":"code","6b362209":"code","57b9e2d1":"code","912891ab":"code","5b487a98":"code","10d64b61":"code","523853ff":"code","4e01a9c6":"code","1400987e":"code","047ab9e1":"code","0c6ef8e2":"code","54b1e1d5":"code","9807a814":"code","76b407b6":"code","6e3862da":"code","dd66fd1b":"code","51548ae5":"code","ef89dc53":"code","399f9fc8":"code","501060fc":"code","ed8b9164":"code","3da42a36":"markdown","849fa16c":"markdown","79b73dd0":"markdown","b7ec701f":"markdown","5156ed61":"markdown","da4b3ec2":"markdown","37e3f6fa":"markdown","70aaff2e":"markdown","a114006e":"markdown","5e946b85":"markdown","dcd34271":"markdown","8c0f5695":"markdown","66da12d5":"markdown","d43e50e7":"markdown","6b598d0d":"markdown","0debedc5":"markdown","7f19c4eb":"markdown","2a2f1d30":"markdown","1e7cf607":"markdown","55c015a9":"markdown","7fa2abc0":"markdown","6afec125":"markdown","adf096b2":"markdown","915d5a90":"markdown","8ebe0618":"markdown","349e841d":"markdown","0a34c60c":"markdown","63d318bc":"markdown","f200d7d6":"markdown","5673858f":"markdown"},"source":{"a87d7b57":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import confusion_matrix\nfrom imblearn.metrics import sensitivity_specificity_support\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier\nimport seaborn as sns\nfrom datetime import timedelta, date\nimport warnings\nwarnings.filterwarnings('ignore')\nimport re\npd.set_option('display.float_format', lambda x: '%.8f' % x)","ecbdb667":"df = pd.read_csv(\"..\/input\/loan-data-set\/DR_Demo_Lending_Club.csv\")","a8380321":"df1 = df.copy()","2ec63274":"# There are 10000 data points and 28 features\ndf1.shape","c1307c5a":"df1.info()","e19d4e4e":"df1.describe()","18a0c382":"df1.dtypes","e3c4da20":"# 7% values are null\n100*(df1.isnull().sum().sum())\/(df1.notnull().sum().sum())","496bbddd":"# Dropping ID columns as it is just an unique id assigned to the records\ndf1.drop(columns ='Id',inplace = True)","0b8f2b19":"df1.duplicated().sum()","f462cfb9":"df1.isnull().sum()","f5b6f32e":"# Dropping the columns which have more than 80% of NA values\nfor i in df1.columns:\n    if (100*df1[i].isnull().sum()\/len(df1.index))>62:\n        print(\"Dropped column is : {} and % of null values in this col is : {}\".format(i,100*df1[i].isnull().sum()\/len(df1)))\n        df1.drop(columns=i,inplace = True)","8107048a":"#There are 12.95% loans which are bad\n100*df1['is_bad'].value_counts()[1]\/(sum(df1['is_bad'].value_counts()))","a6b4cdf5":"sns.countplot(x='is_bad',data = df1)","6a08f2bb":"# Dropping emp_title as this is not useful for analysis\ndf1.drop('emp_title',axis=1,inplace=True)","46a54e47":"df1.initial_list_status.value_counts()\n#There are l 0.17% m values and this does not contain much information hence dropping this feature","63402c9f":"df1.Notes.isnull().sum()\n#There are 3230 null values in Notes feature and this feature contains text which is provided by user at the time\n# of applying the loan. As category is already captured hence dropping this feature","088a3e92":"df1.drop('Notes',axis=1,inplace=True)\ndf1.drop('initial_list_status',axis=1,inplace=True)","8c2dc886":"df1.isnull().sum()","e2b433cc":"df1.isnull().sum()","6e631519":"# There are many feature where there are 4 NA values. Let's check if it all belong to same ID.\n# 'purpose', 'delinq_2yrs','earliest_cr_line','inq_last_6mths','open_acc','pub_rec','total_acc'\n# only purpose have NA values in different rows and rest above feature have NULL values in same rows hence dropping \n# the NA rows of above feature\n\ndf1.drop(df1[df1['delinq_2yrs'].isnull()].index, inplace = True) ","492e8afa":"df1.isnull().sum()","d4aad980":"# Replacing NA values of revol_util with median\n\ndf1['revol_util'] = df1['revol_util'].fillna(df1['revol_util'].median())","dce189f1":"# Assuming na values are representing as not available and replacing this with 0\ndf1['emp_length'] = [c.replace('na','0') for c in df1['emp_length']]","a426545d":"df1.isnull().sum()","44b51796":"df1.collections_12_mths_ex_med.value_counts()","cb5fc84d":"# Dropping collections_12_mths_ex_med as this feature does not have pattern to learn\n\ndf1.drop(columns=\"collections_12_mths_ex_med\",inplace=True,axis =1)","ac9b7f9c":"df1.isnull().sum()","b75e548b":"# Dropping Purpose as purpose category is containing the category of the purpose\ndf1.drop(columns=\"purpose\",inplace=True,axis =1)","8e3c25d9":"df1.columns","51a1c1cf":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(20,15))\ncorr = df1.corr(method = 'spearman')\n\nsns.heatmap(corr, annot = True)\n\nplt.show()","8dd3a094":"pd.crosstab(df1['purpose_cat'], df1['is_bad'], margins = True)","9755ac79":"for i in df1.purpose_cat.unique():\n    if re.search('small', i) and len(i)>14:\n        print(i)\n        df1['purpose_cat'] = df1['purpose_cat'].replace(i,\"Other_Small_Business\")","32669bd7":"df1['purpose_cat'] = df1['purpose_cat'].replace(regex=['other','wedding','major purchase','moving','renewable energy','vacation'], value='Other')","d59bb2ba":"from scipy.stats import pearsonr,chi2_contingency\nfrom itertools import combinations","58adefcd":"num_feat = df1.select_dtypes('number').columns.values\ncomb_num_feat = np.array(list(combinations(num_feat, 2)))\ncorr_num_feat = np.array([])\nfor comb in comb_num_feat:\n    corr = pearsonr(df1[comb[0]], df1[comb[1]])[0]\n    corr_num_feat = np.append(corr_num_feat, corr)","37f99b52":"high_corr_num = comb_num_feat[np.abs(corr_num_feat) >= 0.6]\nhigh_corr_num","058a040b":"#Dropping open account as it is highly correlated with total_acc \ndf1.drop(columns = 'open_acc',axis=1,inplace=True)","5f7357bf":"cat_feat = df1.select_dtypes('object').columns.values\ncomb_cat_feat = np.array(list(combinations(cat_feat, 2)))\ncorr_cat_feat = np.array([])\nfor comb in comb_cat_feat:\n    table = pd.pivot_table(df1, index=comb[0], columns=comb[1], aggfunc='count').fillna(0)\n    corr = np.sqrt(chi2_contingency(table)[0] \/ (table.values.sum() * (np.min(table.shape) - 1) ) )\n    corr_cat_feat = np.append(corr_cat_feat, corr)","8c900b98":"high_corr_cat = comb_cat_feat[corr_cat_feat >= 0.5]\nhigh_corr_cat","f952c01c":"corr = df1.corr()['is_bad'].sort_values()\n\n# Display correlations\nprint('Most Positive Correlations:\\n', corr.tail(10))\nprint('\\nMost Negative Correlations:\\n', corr.head(10))","74a5fec2":"# Dropping Zip code as it is in encrypted form and we cant extract insight from this feature\ndf1.drop(columns=\"zip_code\",inplace=True,axis =1)","90a1b52e":"df1.columns","4e4cf55d":"'''\nThere are just 2 records of y category for pymnt_plan feature.\nAs this feature contains less information hence dropping this feature\n'''\ndf1.drop(columns='pymnt_plan',inplace = True)","3a8114d9":"behav_var = ['delinq_2yrs',\n             'earliest_cr_line',\n             'inq_last_6mths',\n             'pub_rec',\n             'revol_bal',\n             'revol_util',\n             'total_acc',\n             'mths_since_last_major_derog',\n             'is_bad'\n            ]","ee4de40a":"Demo_var = ['emp_length',\n            'home_ownership',\n            'annual_inc',\n            'verification_status',\n            'Notes',\n            'addr_state',\n            'initial_list_status',\n            'debt_to_income',\n            'purpose_cat',\n            'policy_code',\n           ]","da609316":"df1[behav_var].dtypes","6cc08417":"df1['verification_status'] = [i.replace('VERIFIED - income source', 'Verified_Inc_Source') for i in df1['verification_status']]\ndf1['verification_status'] = [i.replace('not verified', 'Not_Verified') for i in df1['verification_status']]\ndf1['verification_status'] = [i.replace('VERIFIED - income', 'Verified_Inc') for i in df1['verification_status']]","4353b189":"#Converting variable to category and numeric values data type\ndf1['is_bad']  =  df1['is_bad'].astype('category')\ndf1['home_ownership']  =  df1['home_ownership'].astype('category')\ndf1['verification_status']  =  df1['verification_status'].astype('category')\ndf1['purpose_cat']  =  df1['purpose_cat'].astype('category')\ndf1['policy_code']  =  df1['policy_code'].astype('category')\ndf1['inq_last_6mths']  =  df1['inq_last_6mths'].astype('int')\ndf1['pub_rec']  =  df1['pub_rec'].astype('int')\ndf1['pub_rec']  =  df1['pub_rec'].astype('category')\ndf1['total_acc']  =  df1['total_acc'].astype('int')\ndf1['emp_length'] = df1['emp_length'].astype('int')","9abcb06d":"df1.dtypes","e41a9991":"plt.figure(figsize=(20, 6))\nsns.distplot(df1['annual_inc'],kde=True,color='blue')","62b55beb":"pd.crosstab(df1['annual_inc'], df1['is_bad'], margins = True)","958f35a0":"def agg_cal(feature):\n    '''\n    This function will create bivariate plot\n    Accept user input and plot the barplot of user input Vs is_bad\n    '''\n    tot_count = df1.groupby([feature])['is_bad'].count().reset_index(name = 'total_count')\n    bad_loan_count = df1[df1['is_bad']==1].groupby([feature])['is_bad'].count().reset_index(name = 'bad_count')\n    agg_data = tot_count.merge(bad_loan_count, on=feature)\n    agg_data['bad_rate'] = 100*(agg_data['bad_count']\/agg_data['total_count']).round(3)\n    plt.figure(figsize=(20, 6))\n    ax = sns.barplot(x=agg_data[feature], y='bad_rate',order=agg_data.sort_values('bad_rate')[feature],data=agg_data)\n    ax.set_xticklabels(ax.get_xticklabels(),rotation=60)\n    plt.show()","aa233b42":"df1['ann_income_bin']=np.nan\nfor i in range(df1.shape[0]):\n    if df1['annual_inc'].iloc[i]>=0 and df1['annual_inc'].iloc[i]<31000:\n        df1['ann_income_bin'].iloc[i]= \"Lowest_Income\"\n    elif df1['annual_inc'].iloc[i]>=31000 and df1['annual_inc'].iloc[i]<42000:\n        df1['ann_income_bin'].iloc[i]= \"Lower_middle_inc\"\n    elif df1['annual_inc'].iloc[i]>=42000 and df1['annual_inc'].iloc[i]<126000:\n        df1['ann_income_bin'].iloc[i]= \"Middle_income\"\n    elif df1['annual_inc'].iloc[i]>=126000 and df1['annual_inc'].iloc[i]<188000:\n        df1['ann_income_bin'].iloc[i]= \"Upper_middle_inc\"\n    elif df1['annual_inc'].iloc[i]>=188000:\n        df1['ann_income_bin'].iloc[i]= \"Higher_income\"\n    else:\n        df1['ann_income_bin'].iloc[i]='NA'","8156f5d2":"agg_cal('ann_income_bin')","84128bbc":"agg_cal(feature=\"home_ownership\")","0b9023af":"agg_cal('verification_status')\n","e9c1c949":"agg_cal('purpose_cat')","044b23ab":"agg_cal('addr_state')","ed8495f3":"agg_cal('delinq_2yrs')","e12a7a7e":"agg_cal('inq_last_6mths')","a7a7786d":"agg_cal('pub_rec')","e6380924":"agg_cal('policy_code')","853b07dd":"from datetime import datetime\ncol = 'earliest_cr_line'\ndf1[col] = pd.to_datetime(df1[col])\nfuture = df1[col] > pd.to_datetime(date(year=2050,month=1,day=1))\ndf1.loc[future, col] -= timedelta(days=365.25*100)","6dc49bc9":"df1['earl_cr_line_year'] = pd.DatetimeIndex(df1['earliest_cr_line']).year","f3ce2d50":"df1['earl_cr_line_month'] = pd.DatetimeIndex(df1['earliest_cr_line']).month","b1a1b426":"df1['earl_cr_line_day'] = pd.DatetimeIndex(df1['earliest_cr_line']).day","e5306e4a":"df1['length_cr_line'] = datetime.now().year -  df1['earl_cr_line_year']","f8edf61b":"# dropping earl_cr_line_day as all the records belong to day 1\ndf1.drop(columns='earl_cr_line_day',inplace=True)","37bb2e91":"plt.figure(figsize=(20, 6))\nsns.countplot(df1.length_cr_line,color='blue')","2cd9fe85":"agg_cal('length_cr_line')","3e98799e":"agg_cal('earl_cr_line_month')","44d1f79e":"def ecdf(data):\n    \"\"\"Compute ECDF for a one-dimensional array of measurements.\"\"\"\n    # Number of data points: n\n    n = len(data)\n\n    # x-data for the ECDF: x\n    x = np.sort(data)\n\n    # y-data for the ECDF: y\n    y = np.arange(1, n+1) \/ n\n\n    return x, y\n","2d93a0a3":"x,y = ecdf(df1.length_cr_line)\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('length_cr_line')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()","73177d15":"x,y = ecdf(df1.earl_cr_line_month)\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('earl_cr_line_month')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n","a31f8e99":"# Compute ECDF for annual_income data: x, y\nx, y = ecdf(df1.annual_inc)\n\n# Generate plot\n\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('Annual Income')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n","278b1b1d":"# Compute ECDFs\nx,y = ecdf(df1.annual_inc)\nx_not_bad,y_not_bad = ecdf(df1[df1['is_bad']==0]['annual_inc'])\nx_bad,y_bad = ecdf(df1[df1['is_bad']==1]['annual_inc'])\n\n# Plot all ECDFs on the same plot\nplt.plot(x,y,marker ='.',linestyle ='none')\nplt.plot(x_not_bad,y_not_bad,marker ='.',linestyle ='none')\nplt.plot(x_bad,y_bad,marker ='.',linestyle ='none')\n\n# Annotate the plot\nplt.legend(('Overall', 'Not bad loan', 'Bad loan'), loc='lower right')\n_ = plt.xlabel('Annual Income')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()","c08bd6f6":"# Compute ECDF for emp_length data: x, y\nx, y = ecdf(df1.emp_length)\n\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('Emp Length')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n","492179c9":"agg_cal('emp_length')","c372bb38":"# Compute ECDF for emp_length data: x, y\nx, y = ecdf(df1.debt_to_income)\n\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('Debt to Income')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n\n","179473e6":"# Compute ECDF for emp_length data: x, y\nx, y = ecdf(df1.revol_util)\n\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('Revol Util')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n\n\n","be4f2964":"# Compute ECDF for emp_length data: x, y\nx, y = ecdf(df1.revol_bal)\n\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('Revol Balance')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n\n\n\n","19b7dbcf":"len(df1[df1['debt_to_income']<=25])\/df1.shape[0]","d73e5bb0":"# Compute ECDF for emp_length data: x, y\nx, y = ecdf(df1.total_acc)\n\n# Generate plot\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('total account')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n\n\n\n\n","7b4c816d":"#df['emp_length'] = pd.to_numeric(df['emp_length'])\ndf1['emp_length'] = df1['emp_length'].astype('int')","35a7c8c2":"df1.dtypes","169673e6":"df1.addr_state.unique()","8fbedd7e":"def detect_outlier(data):\n    '''\n    This function will return number of outlier in entire data frame\n    This will store the outlier features and it's value in the form\n    of key value pair and once it's called it will return dictionary\n    '''\n    outlier_dict = dict()\n    for i in data.columns:\n        if data[i].dtypes==\"int64\" or data[i].dtypes==\"float64\":\n            Q1 = data[i].quantile(0.25)\n            Q3 = data[i].quantile(0.75)\n            IQR = Q3-Q1 \n            temp_len = len(data[((data[i] < (Q1 - 1.5 * IQR)) | (data[i] > (Q3 + 1.5 * IQR)))])\n            if temp_len!=0:\n                outlier_dict[i] = len(data[((data[i] < (Q1 - 1.5 * IQR)) | (data[i] > (Q3 + 1.5 * IQR)))])\n            else:\n                \"\"\n        else:\n            \"\"\n    return outlier_dict","4ff88441":"detect_outlier(df1)","4e30af51":"df1['emp_length'].quantile([0.91,.92,.93,.94,.95,.96,.97,.98,.99,1.0])\n","da57e16f":"#capping the value of emp_length on .99 quantile\ndf1['emp_length'][df1['emp_length'] >= np.array(df1['emp_length'].quantile([.99]))[0]] = np.array(df1['emp_length'].quantile([.99]))[0]","41c78aff":"df1['delinq_2yrs'].quantile([0.89,.92,.93,.94,.95,.96,.97,.98,.99,1.0])\n\n","6560d419":"# Compute ECDF for annual_income data: x, y\nx, y = ecdf(df1.delinq_2yrs)\n\n# Generate plot\n\n_ = plt.plot(x, y, marker='.', linestyle='none')\n\n# Label the axes\n_ = plt.xlabel('delinq_2yrs')\n_ = plt.ylabel('ECDF')\n\n# Display the plot\nplt.show()\n\n","475f66d5":"len(df1[df1['delinq_2yrs']<=2])\/df1.shape[0]","ea0a641b":"#capping the value of delinq_2yrs on .97 quantile\ndf1['delinq_2yrs'][df1['delinq_2yrs'] >= np.array(df1['delinq_2yrs'].quantile([.89]))[0]] = np.array(df1['delinq_2yrs'].quantile([.89]))[0]","747548a5":"detect_outlier(df1)","16d73bd2":"df1['annual_inc'].quantile([0.89,.92,.93,.94,.95,.96,.97,.98,.99,1.0])\n","965c456a":"#capping the value of annual_inc on .97 quantile\ndf1['annual_inc'][df1['annual_inc'] >= np.array(df1['annual_inc'].quantile([.97]))[0]] = np.array(df1['annual_inc'].quantile([.97]))[0]","6d051085":"df1['inq_last_6mths'].quantile([0.89,.92,.93,.94,.95,.96,.97,.98,.99,1.0])\n","99710660":"#capping the value of inq_last_6mths on .98 quantile\ndf1['inq_last_6mths'][df1['inq_last_6mths'] >= np.array(df1['inq_last_6mths'].quantile([.98]))[0]] = np.array(df1['inq_last_6mths'].quantile([.98]))[0]","7d7a896c":"detect_outlier(df1)","f851e008":"detect_outlier(df1)","1e0ebc40":"df1['revol_bal'].quantile([0.89,.92,.93,.94,.95,.96,.97,.98,.99,1.0])","c41c5e12":"#capping the value of revol_bal on .97 quantile\ndf1['revol_bal'][df1['revol_bal'] >= np.array(df1['revol_bal'].quantile([.97]))[0]] = np.array(df1['revol_bal'].quantile([.97]))[0]","96c539d3":"detect_outlier(df1)","7bc8654e":"df1['total_acc'].quantile([0.85,.92,.93,.94,.95,.96,.97,.98,.99,1.0])","e75fc427":"sns.boxplot(df1.total_acc)","60ec782c":"#capping the value of revol_bal on .98 quantile\ndf1['total_acc'][df1['total_acc'] >= np.array(df1['total_acc'].quantile([0.98]))[0]] = np.array(df1['total_acc'].quantile([0.98]))[0]","bdaec00c":"df1['earl_cr_line_year'].quantile([0.08,.92,.93,.94,.95,.96,.97,.98,.99,1.0])","e00fa14b":"sns.boxplot(df1.earl_cr_line_year)","49cc294b":"plt.figure(figsize = (20,6))\nsns.distplot(df1.earl_cr_line_year)","3bbfeaec":"#capping the value of earl_cr_line_year \ndf1['earl_cr_line_year'][df1['earl_cr_line_year'] <=1985] = 1985","80e1c7d9":"detect_outlier(df1)","3dc5c35b":"sns.boxplot(df1.earl_cr_line_year)","e5a45fb7":"detect_outlier(df1)","58dd0151":"df1['length_cr_line'].quantile([0.08,.92,.93,.94,.95,.96,.97,.98,.99,1.0])","a372b63d":"sns.boxplot(df1['length_cr_line'])","a6556a1e":"#capping the value of length_cr_line \ndf1['length_cr_line'][df1['length_cr_line'] >=35] = 35","ebf49b81":"detect_outlier(df1)","3dbd7a18":"detect_outlier(df1)","b52ed2d1":"#from scipy.cluster.vq import kmeans\n#from scipy.cluster.vq import vq\n#data = df1.mths_since_last_delinq.astype('float64')\n#data_raw = data.values\n#centroids, avg_distance = kmeans(data_raw, 4)\n#groups, cdist = vq(data_raw, centroids)\n#y = np.arange(0,9995)\n#plt.scatter(data_raw,  y , c=groups)\n#plt.xlabel('mths_since_last_delinq')\n#plt.ylabel('Indices')\n#plt.show()","cb0be7e3":"# Let's convert addr_state feature to real valued feature using probability\n# We will replace the state with the probaility of is_bad (for a given state)\ncountry_prob = dict()\nfor i in df1.addr_state:\n    c1 = df1.loc[(df1['addr_state'] == i) & (df1['is_bad'] == 1)]['addr_state'].count()\n    c2 = df1.loc[(df1['addr_state'] == i)]['addr_state'].count()\n    t = c1\/c2\n    country_prob[i]=t\ndf1.replace({\"addr_state\": country_prob},inplace=True)","633ca4cc":"# Assiggning 2 levels categorical variable to 0 and 1\n\n#Ignore Target variable\n\nfor i in df1.columns[1:]:\n    if len(df1[i].value_counts())<=2 and df1[i].dtypes!='int64' and df1[i].dtypes!='float64':\n        df1[i] = df1[i].map({df1[i].value_counts().index[0]: 0, df1[i].value_counts().index[1]: 1})\n        print(i)","521906d1":"# Dropping ann_income_bin and earliest_cr_line \ndf1.drop(columns=['ann_income_bin','earliest_cr_line'],inplace = True)","f5f399c3":"#Identifying categorical variables which have more than 2 levels\ncat_var_g_2level = [i for i in df1.columns if len(df1[i].value_counts())>2 and df1[i].dtypes!='int64' and df1[i].dtypes!='float64' and df1[i].dtypes!='object']\ncat_var_g_2level","6d618851":"# Creating a dummy variable for some of the categorical variables and dropping the first one.\ndummy1 = pd.get_dummies(df1[cat_var_g_2level], drop_first=True)\n\n# Adding the results to the master dataframe\ndf1 = pd.concat([df1, dummy1], axis=1)","9dccabf6":"df1.drop(columns=cat_var_g_2level,inplace=True)","c1b9ecc2":"df1.shape","c0436438":"df1.dtypes","be1066ec":"scaling_features = ['emp_length','annual_inc','debt_to_income','inq_last_6mths','revol_bal','revol_util','total_acc','mths_since_last_major_derog','earl_cr_line_year','earl_cr_line_month','length_cr_line']","a13bacf4":"df1[scaling_features]","97fea765":"df1.head()","60571612":"df2 = df1.copy()\ndf2['is_bad'] = df2.is_bad.astype('int64')\ndef iv_woe(data, target, bins=10, show_woe=False):\n    \n    #Empty Dataframe\n    newDF = pd.DataFrame()\n    \n    #Extract Column Names\n    cols = [i for i in data.columns if i!=target]\n    \n    #Run WOE and IV on all the independent variables\n    for ivars in cols:\n        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n        else:\n            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n\n        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n        d.columns = ['Cutoff', 'N', 'Events']\n        d['% of Events'] = d['Events'] \/ d['Events'].sum()\n        d['Non-Events'] = d['N'] - d['Events']\n        d['% of Non-Events'] = d['Non-Events'] \/ d['Non-Events'].sum()\n        d.loc[d['% of Non-Events'] == 0.0,'% of Non-Events'] = 1e-312\n        d['WoE'] = np.log(d['% of Events']\/d['% of Non-Events'])\n        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n        print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n        temp =pd.DataFrame({\"Variable\" : [ivars], \n                            \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n        newDF=pd.concat([newDF,temp], axis=0)\n\n        #Show WOE Table\n        if show_woe == True:\n            print(d)\n            \n    return newDF\niv_table = iv_woe(data = df2, target = 'is_bad')","a7750bc5":"\ndef indicator(value):\n    if value < 0.02 :\n        return 'Not useful for prediction'\n    elif value >= 0.02 and value < 0.1:\n        return 'Weak predictive Power'\n    elif value >=0.1 and value < 0.3:\n        return 'Medium predictive Power'\n    elif value >=0.3 and value < 0.5:\n        return 'Strong predictive Power'\n    else:\n        return 'Suspicious Predictive Power or too good to be true'\n    \niv_table['indicator'] = iv_table['IV'].apply(indicator)\niv_table.reset_index(drop=True)\niv_table","fc23e165":"from sklearn.model_selection import train_test_split","1bdaf2f0":"\nX = df1.drop(['is_bad'], axis=1)\n\nX.head()","7ab50bb4":"Y = df1['is_bad']\n\nY.head()","dbcf4d88":"scaler = StandardScaler()\n\nX[scaling_features] = scaler.fit_transform(X[scaling_features])\n\nX.head()","25c71818":"# Splitting the data into train and test\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, train_size=0.7, test_size=0.3, random_state=100)","9e4fbc0c":"# Tried SMOTE but it did not perform well\n#Performing Oversampling on the data using SMOTE function\n#from imblearn.over_sampling import SMOTE\n#from collections import Counter\n#sm = SMOTE(random_state=42)\n#X_res, y_res = sm.fit_resample(X_train, Y_train)\n# Splitting the data into train and test\n#X_train, X_test, Y_train, Y_test = train_test_split(X_res, y_res, train_size=0.7, test_size=0.3, random_state=100)","423e209a":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler","3e6bd450":"import statsmodels.api as sm","7fa4dbba":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","ffc1b588":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 20)             # running RFE with 20 variables as output\nrfe = rfe.fit(X_train, Y_train)\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","94587b50":"col = X_train.columns[rfe.support_]","490f11dd":"X_train.columns[~rfe.support_]","3098a31b":"X_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","98466dd8":"# Dropping highest insignificant feature(pub_rec_3) and train the model with remaining variables\ncol = col.drop('pub_rec_3', 1)\ncol","3b238510":"X_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","9699c8de":"#Checking VIF\n# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","37fe907d":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\n","c38120f8":"vif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","48afe91a":"# Dropping the variable purpose_cat_Other_Small_Business and training the model with remianing feature\ncol = col.drop('purpose_cat_Other_Small_Business', 1)\ncol","1961caa2":"X_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","a3835034":"# Check VIF before dropping the insignificant features\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","02a969e5":"# Dropping the variable purpose_cat_home improvement and training the model with remianing feature\ncol = col.drop('purpose_cat_home improvement', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","59540da5":"# Check VIF before dropping the insignificant features\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","f1054b16":"# Dropping the variable pub_rec_2 and training the model with remianing feature\ncol = col.drop('pub_rec_2', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","2481ae54":"# Check VIF before dropping the insignificant features\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","49d32bd1":"# Dropping the purpose_cat_medical and training the model with remianing feature\ncol = col.drop('purpose_cat_medical', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","17e2520e":"# Check VIF before dropping the insignificant features\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","95785861":"# Dropping the home_ownership_OTHER and training the model with remianing feature\ncol = col.drop('home_ownership_OTHER', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","abefc03e":"# Check VIF before dropping the insignificant features\n# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].astype(float).values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","3f20bb51":"# Dropping the policy_code_PC3 and training the model with remianing feature\ncol = col.drop('policy_code_PC3', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","0a115be9":"# Dropping the policy_code_PC3 and training the model with remianing feature\ncol = col.drop('home_ownership_OWN', 1)\nX_train_sm = sm.add_constant(X_train[col]).astype(float)\nlogm2 = sm.GLM(Y_train.astype(float),X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","e7b83615":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","32764560":"y_train_pred_final = pd.DataFrame({'is_bad':Y_train.values, 'is_bad_Prob':y_train_pred})\ny_train_pred_final['Id'] = Y_train.index","60918dbb":"y_train_pred_final['is_bad_Prob'] = y_train_pred","a50d23c7":"from sklearn import metrics\ny_train_pred_final['is_bad_Prob'] = y_train_pred\n\n# Creating new column 'predicted' with 1 if is_bad_Prob > 0.5 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.is_bad_Prob.map(lambda x: 1 if x > 0.5 else 0)\n# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.is_bad, y_train_pred_final.predicted))","c83a9f49":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.is_bad, y_train_pred_final.predicted )\nconfusion","0a0b81f8":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.is_bad, y_train_pred_final.predicted)","bdd0a68c":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","1e3494c9":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","41e9e795":"# Let us calculate specificity\nTN \/ float(TN+FP)","4af13799":"# Calculate false postive rate\nprint(FP\/ float(TN+FP))","62333f80":"# positive predictive value \nprint (TP \/ float(TP+FP))","a84591e8":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","aed6121e":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","57b4b0b3":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.is_bad, y_train_pred_final.is_bad_Prob, drop_intermediate = False )","d3404f10":"draw_roc(y_train_pred_final.is_bad, y_train_pred_final.is_bad_Prob)","1ba38db5":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.is_bad_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","26b6b8f6":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.is_bad, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","ae238aef":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","cc260ace":"y_train_pred_final['final_predicted'] = y_train_pred_final.is_bad_Prob.map( lambda x: 1 if x > 0.12 else 0)\n\ny_train_pred_final.head()","de36b4ab":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.is_bad, y_train_pred_final.final_predicted)","96eb3bd1":"confusion2 = metrics.confusion_matrix(y_train_pred_final.is_bad, y_train_pred_final.final_predicted )\nconfusion2","5503cc98":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","0b0a5f54":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","53b91687":"# Let us calculate specificity\nTN \/ float(TN+FP)","ba9a5155":"# Calculate false postive rate\nprint(FP\/ float(TN+FP))","eda70952":"# Positive predictive value \nprint (TP \/ float(TP+FP))","22c5282c":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","c2b50807":"confusion = metrics.confusion_matrix(y_train_pred_final.is_bad, y_train_pred_final.predicted )\nconfusion","6ab6b8b7":"from sklearn.metrics import precision_score, recall_score","cb016046":"precision_score(y_train_pred_final.is_bad, y_train_pred_final.predicted)\n","84dbff10":"recall_score(y_train_pred_final.is_bad, y_train_pred_final.predicted)","3a018ac1":"from sklearn.metrics import precision_recall_curve\np, r, thresholds = precision_recall_curve(y_train_pred_final.is_bad, y_train_pred_final.is_bad_Prob)","6a1e9487":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","a3291653":"X_test_sm = X_test[col]\n","b717b5bc":"X_test_sm = sm.add_constant(X_test_sm)","444662d5":"y_test_pred = res.predict(X_test_sm)\ny_test_pred[:10]\n# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)\n\n# Converting y_test to dataframe\ny_test_df = pd.DataFrame(Y_test)\n# Putting ID to index\ny_test_df['Id'] = y_test_df.index\n\n# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)\n\n# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)\n\n# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'Is_bad_Prob'})","92771d12":"y_pred_final.sort_values(by ='Is_bad_Prob',ascending=False)","00b09f68":"y_pred_final['final_predicted'] = y_pred_final.Is_bad_Prob.map(lambda x: 1 if x > 0.12 else 0)","4d9926e1":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.is_bad, y_pred_final.final_predicted)","4551ab45":"confusion2 = metrics.confusion_matrix(y_pred_final.is_bad, y_pred_final.final_predicted )\nconfusion2","1b8707d4":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","ffdcdfaf":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","488a9c64":"# Let us calculate specificity\nTN \/ float(TN+FP)","89d5bf83":"#from sklearn.preprocessing import StandardScaler\n#z_scaler = StandardScaler(copy=True, with_mean=True, with_std=True)\n#X_train_z = z_scaler.fit_transform(X_train)\n#X_test_z = z_scaler.transform(X_test)","5df718f9":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.model_selection import GridSearchCV","19e9cf8a":"pipeline_sgdlogreg = Pipeline([\n    ('imputer', SimpleImputer(copy=False)), \n    ('scaler', StandardScaler(copy=False)),\n    ('model', SGDClassifier(loss='log',random_state=1))\n])","3f59f62b":"param_grid_sgdlogreg = {\n    'model__alpha': [10**-6, 10**-2, 10**1],\n    'model__penalty': ['l1', 'l2']\n}","fcf1ab50":"grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)","5b73e832":"grid_sgdlogreg.fit(X_train, Y_train)","4e7a2ba1":"grid_sgdlogreg.best_score_","b1af677a":"grid_sgdlogreg.best_params_","c65a63c8":"from sklearn.ensemble import RandomForestClassifier","77ea23f1":"pipeline_rfc = Pipeline([\n    ('imputer', SimpleImputer(copy=False)),\n    ('model', RandomForestClassifier(n_jobs=-1, random_state=1))\n])","206b889b":"param_grid_rfc = {\n    'model__n_estimators': [50,100,200,300] # The number of randomized trees to build\n}","64ee269a":"grid_rfc = GridSearchCV(estimator=pipeline_rfc, param_grid=param_grid_rfc, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)","9f1136c6":"grid_rfc.fit(X_train, Y_train)","9e64c2b1":"grid_rfc.best_score_","0059e8e6":"print('Cross-validated AUCROC scores')\nprint(grid_sgdlogreg.best_score_, '- Logistic regression')\nprint(grid_rfc.best_score_, '- Random forest')","1f8b65a7":"param_grid_sgdlogreg = {\n    'model__alpha': np.logspace(-4.5, 0.5, 11),\n    'model__penalty': ['l1', 'l2']\n}\n\nprint(param_grid_sgdlogreg)","220c072e":"grid_sgdlogreg = GridSearchCV(estimator=pipeline_sgdlogreg, param_grid=param_grid_sgdlogreg, scoring='roc_auc', n_jobs=1, pre_dispatch=1, cv=5, verbose=1, return_train_score=False)","7a5f997b":"grid_sgdlogreg.fit(X_train, Y_train)","fc05d57b":"grid_sgdlogreg.best_score_","69aaca37":"grid_sgdlogreg.best_params_","d38e98ef":"from sklearn.metrics import roc_auc_score","460c00ce":"y_score = grid_sgdlogreg.predict_proba(X_test)[:,1]\nroc_auc_score(Y_test, y_score)","3c2152a8":"y_pred = grid_sgdlogreg.predict(X_test)","8ab96a81":"from imblearn.metrics import sensitivity_specificity_support\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')","96241738":"pca = PCA()\npca.fit(X_train)\nis_bad_pca = pca.fit_transform(X_train)","d5f2642a":"# plot feature variance\nfeatures = range(pca.n_components_)\ncumulative_variance = np.round(np.cumsum(pca.explained_variance_ratio_)*100, decimals=4)\nplt.figure(figsize=(175\/20,100\/20)) # 100 elements on y-axis; 175 elements on x-axis; 20 is normalising factor\nplt.plot(cumulative_variance)","1e90970f":"# create pipeline\nPCA_VARS = 15\nsteps = [(\"pca\", PCA(n_components=PCA_VARS)),\n         (\"logistic\", SGDClassifier(class_weight='balanced',loss='log',penalty='l2'))\n        ]\npipeline = Pipeline(steps)","208f24f6":"# fit model\npipeline.fit(X_train, Y_train)\n\n# check score on train data\npipeline.score(X_train, Y_train)","afbfb97c":"# predict bad loan on test data\ny_pred = pipeline.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = pipeline.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(Y_test, y_pred_prob),2))","72e2ee83":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","e644c023":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","6e389259":"X_train.shape","cdebf95e":"y_train_pred_final = pd.DataFrame({'is_bad':Y_train.values, 'is_bad_prob':y_train_pred})\ny_train_pred_final['ID'] = Y_train.index\ny_train_pred_final.head()","31c1a819":"{'model__alpha': 0.0031622776601683794, 'model__penalty': 'l2'}","b88dc9a5":"# Final model\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nnum_trees = 200\nmax_depth = 3\nclf = RandomForestClassifier(n_estimators=num_trees, max_depth=max_depth, n_jobs = 4, random_state=5)\nclf.fit(X_train,Y_train)\npred_prob = clf.predict_proba(X_test)\npred = clf.predict(X_test)\nauc = roc_auc_score(y_true = Y_test, y_score = pred_prob[:,1])\nprint('num_trees =',num_trees,'; depth=',max_depth,'; auc =',auc)\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')","32add1a6":"print(X_test.columns)","1d1d3e1b":"#Final Model\n# PCA\npca = PCA()\n\n# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\nlogistic = LogisticRegression(class_weight='balanced')\n\n# create pipeline\nsteps = [(\"pca\", pca),\n         (\"logistic\", logistic)\n        ]\n\n# compile pipeline\npca_logistic = Pipeline(steps)\n\n# hyperparameter space\nparams = {'pca__n_components': [16,17,18,19,20], 'logistic__C': [0.1,0.2,0.5, 1, 2, 3, 4, 5, 10], 'logistic__penalty': ['l1', 'l2']}\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nlr_model = GridSearchCV(estimator=pca_logistic, cv=folds, param_grid=params, scoring='roc_auc')","9934f857":"# fit model\nlr_model.fit(X_train, Y_train)","c579134e":"# print best hyperparameters\nprint(\"Best AUC: \", lr_model.best_score_)\nprint(\"Best hyperparameters: \", lr_model.best_params_)","57293a55":"from sklearn.metrics import roc_curve\n# predict bad loan on test data\ny_pred = lr_model.predict(X_test)\n\n# create onfusion matrix\ncm = confusion_matrix(Y_test, y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\ny_pred_prob = lr_model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(Y_test, y_pred_prob),2))\n\nlr_accuracy = metrics.accuracy_score(Y_test, y_pred)\nprint(\"Accuracy:    \\t\", round(lr_accuracy,2))\n\nlr_fpr, lr_tpr, thresholds = roc_curve(Y_test, y_pred_prob)\n\nlr_roc_auc = metrics.auc(lr_fpr, lr_tpr)\n\n","bf389857":"#RF hyperparameter tuning\n# PCA\npca = PCA()\n\n# logistic regression - the class weight is used to handle class imbalance - it adjusts the cost function\nRF = RandomForestClassifier(class_weight='balanced')\n\nsteps = [(\"pca\", pca),\n         (\"RF\", RF)\n        ]\n\n# compile pipeline\npca_RF = Pipeline(steps)\n\n# hyperparameter space\nparams = {'RF__n_estimators': [50,100], 'RF__max_depth': [20,30],\n    'RF__max_features': [5, 7]\n         }\n\n# create 5 folds\nfolds = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 4)\n\n# create gridsearch object\nrf_model = GridSearchCV(estimator=pca_RF, cv=folds, param_grid=params, scoring='roc_auc')","aa2532db":"rf_model.fit(X_train,Y_train)","78899c19":"# print best hyperparameters\nprint(\"Best AUC: \", rf_model.best_score_)\nprint(\"Best hyperparameters: \", rf_model.best_params_)","33c6f998":"# predict bad loan on test data\nrf_y_pred = rf_model.predict(X_test)\n\n# create onfusion matrix\nrf_cm = confusion_matrix(Y_test, rf_y_pred)\nprint(cm)\n\n# check sensitivity and specificity\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, rf_y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')\n\n# check area under curve\nrf_y_pred_prob = rf_model.predict_proba(X_test)[:, 1]\nprint(\"AUC:    \\t\", round(roc_auc_score(Y_test, rf_y_pred_prob),2))\n\n#print(metrics.accuracy_score(Y_test, rf_y_pred))\n\n\nRF_accuracy = metrics.accuracy_score(Y_test, rf_y_pred)\nprint(\"Accuracy:    \\t\", round(RF_accuracy,2))\n\nrf_fpr, rf_tpr, thresholds = roc_curve(Y_test, rf_y_pred_prob)\n\nrf_roc_auc = metrics.auc(rf_fpr, rf_tpr)\n\nprint(\"RF AUCROC:    \\t\", round(rf_roc_auc,2))","a24ff0cb":"# Doing parameter tuning,\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nada_boost = AdaBoostClassifier(n_estimators = 20, learning_rate = 0.2, random_state = 123)\ngridparam ={\n        'n_estimators': [100,200,500],\n        'learning_rate': [0.2,0.5,1.0],\n},\nab = GridSearchCV(ada_boost,cv=3,n_jobs=3, param_grid=gridparam)\nab.fit(X_train, Y_train)\nab_Y_pred = ab.predict(X_test)\nab_predict_proba =ab.predict_proba(X_test)[:,1] \n","02c87ad4":"#Checking the accuracy,\nab_accuracy=ab.score(X_test, Y_test)\nprint(\" Accuracy of AdaBoost  classifier: \",ab_accuracy )\n\nsensitivity, specificity, _ = sensitivity_specificity_support(Y_test, ab_Y_pred, average='binary')\nprint(\"Sensitivity: \\t\", round(sensitivity, 2), \"\\n\", \"Specificity: \\t\", round(specificity, 2), sep='')","58390038":"##Computing false and true positive rates\nfrom sklearn.metrics import roc_curve \nimport sklearn.metrics as metrics\nab_fpr, ab_tpr, thresholds = roc_curve(Y_test, ab_predict_proba) #AdaBoost Classifier\nab_roc_auc = metrics.auc(ab_fpr, ab_tpr)\nimport matplotlib.pyplot as plt\nplt.figure()\n##Creating the ROC,\nplt.plot(ab_fpr, ab_tpr, color='blue',lw=2, label='ROC curve')\n##Finding FPR and TPR,\nplt.plot([0, 1], [0, 1], color='red', lw=2, linestyle='--')\n##Splecifying the label and title,\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplot_title = 'ROC curve AUC: {0}'.format(ab_roc_auc)\nplt.title(plot_title, size=15)\nplt.show()","a75bb727":"#compare the ROC curve between different models\nplt.figure(figsize=(8,8))\nplt.plot(lr_fpr, lr_tpr, label='Logistic regression')\nplt.plot(ab_fpr, ab_tpr, label='Adaboost Classifier')\nplt.plot(rf_fpr, rf_tpr, label='Randomforest Classifier')\n#plt.plot(dt_fpr, dt_tpr, label='Decision Tree')\nplt.plot([0, 1], [0, 1], linestyle='--', lw=2, color='r',label='random', alpha=.8)\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.xticks(np.arange(0,1.1,0.1))\nplt.yticks(np.arange(0,1.1,0.1))\nplt.grid()\nplt.legend()\nplt.axes().set_aspect('equal')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')","120a85ed":"score = [    (lr_accuracy, lr_roc_auc) ,\n             (RF_accuracy, rf_roc_auc) ,\n             #(dt_accuracy, dt_roc_auc) ,\n             (ab_accuracy, ab_roc_auc)     ]\ndf_score = pd.DataFrame(score, columns = ['Accuracy' , 'AUC'], index=['LogisticRegression','RandomForest','AdaBoost']) ","0be0945f":"df_score","92eee940":"y_prob = lr_model.predict_proba(X_test)\ny_prob[:,1]\nY_test.head()","435db04b":"probabilities = pd.DataFrame()","148fa20f":"probabilities['prob'] = y_prob[:,1]","6b362209":"probabilities['actual'] = np.array(Y_test)","57b9e2d1":"probabilities['pred'] = np.array(y_pred)","912891ab":"from sklearn.metrics import classification_report, confusion_matrix\n\nprint(classification_report(Y_test,y_pred))","5b487a98":"print(confusion_matrix(Y_test,y_pred))","10d64b61":"probabilities = pd.DataFrame()","523853ff":"probabilities['pred_prob'] = y_prob[:,1]","4e01a9c6":"probabilities['actual'] = np.array(Y_test)","1400987e":"probabilities['predicted'] = np.array(y_pred)","047ab9e1":"decile_df = probabilities","0c6ef8e2":"decile_df['decile'] = pd.qcut(decile_df['pred_prob'], 10, labels=np.arange(10, 0, -1))","54b1e1d5":"lift_df = decile_df.groupby('decile')['pred_prob'].count().reset_index()","9807a814":"lift_df.rename({'pred_prob':'total'}, axis=1, inplace=True)","76b407b6":"lift_df_pred = decile_df[decile_df['actual']==1].groupby('decile')['actual'].count().reset_index()","6e3862da":"lift_df_final = lift_df.merge(lift_df_pred,on = 'decile')","dd66fd1b":"lift_df_final = lift_df_final.sort_values(['decile'], ascending=False)","51548ae5":"lift_df_final['cumresp'] = lift_df_final['actual'].cumsum()","ef89dc53":"lift_df_final['gain'] = 100*(lift_df_final['cumresp']\/sum(lift_df_final['actual']))","399f9fc8":"lift_df_final['cumlift'] = lift_df_final['gain']\/(lift_df_final['decile'].astype('int')*(100\/10))","501060fc":"lift_df_final.plot.line(x='decile', y=['gain'])","ed8b9164":"df_score","3da42a36":"There are three types of records we have (after dropping features which we will not be using for analysis) and they can be categorized as : Customer's Demographic and application feature and Credit information\/ Customer Behaviour variables\n\n**Customer's Demographic and application variables:**\n\n1. emp_length\n2. home_ownership\n3. annual_inc\n4. verification_status\n5. Notes\n6. addr_state\n7. initial_list_status\n8. debt_to_income\n9. purpose_cat\n10. policy_code\n\n**Credit information\/ Customer Behaviour variables**\n11. is_bad (Target variable)\n12. delinq_2yrs\n13. earliest_cr_line\n14. mths_since_last_delinq\n15. inq_last_6mths\n16. open_acc\n17. pub_rec\n18. revol_bal\n19. revol_util\n20. total_acc\n21. mths_since_last_major_derog\n","849fa16c":"# Multi Collinearity\n\nWe will check the multicolliearity using VIF also while doing modelling\n\nLets check pearson correlation also and for categorical feature we will be using chi2","79b73dd0":"Own and Rent have higher bad rate as compared to others.","b7ec701f":"# Train Test Split","5156ed61":"# Prediction on Test data","da4b3ec2":"### Plotting the ROC Curve","37e3f6fa":"# Ensemble Models (RF)","70aaff2e":"small business and other small business has highest bad rate and educational purpose category has lowest bad rate","a114006e":"# Outlier Treatment","5e946b85":"# Summary:\n\nSelected Logistic regression with PCA as final model\nas only this model is giving good sensitivity and specificity along with accuracy and AUC.","dcd34271":"Now all 12 features are statistically significant and VIF values are also not high hence we will consider this as final model","8c0f5695":"Not verified income have lower bad rate as compared to verified inc and verified inc source","66da12d5":"# PCA and Logistic Regression in the pipeline","d43e50e7":"# Hyper parameter Tuning","6b598d0d":"#### Feature Selection\n\nWe will start with autmatic feature elimination using RFE and start with 15 variables.\nAfter this step, we will check VIF and Statistics of the variable(p-value) for feature elimination ( backward elimination).\n\n","0debedc5":"### Home Ownership Vs Is_bad","7f19c4eb":"PC2 has higher loan bad rate while PC3 has lowest bad rate.","2a2f1d30":"bad rate is increasing from 0 to 7 for delinq_2yrs except value 5.0 as this has low bad rate.","1e7cf607":"### Annual Income Salary Vs is_Bad\n\nFollowing reference [US News](https:\/\/money.usnews.com\/money\/personal-finance\/family-finance\/articles\/where-do-i-fall-in-the-american-economic-class-system) is used while creatig income group","55c015a9":"Let's merge all these other type of small business category \nwhich have all values as bad loan with one category and rename it as Other_Small_Business","7fa2abc0":"There is no strong correlation among categorical variable","6afec125":"Stats model is not giving good performance.","adf096b2":"### Logistic Regression\n\nLet's start by splitting our data into a training set and a test set.","915d5a90":"## Feature Importance using Information value(IV)\n\n|Information Value|\tVariable Predictiveness  |\n|-----------------|------------------------  |\n|Less than 0.02   |\tNot useful for prediction|\n|0.02 to 0.1      |\tWeak predictive Power    |\n|0.1 to 0.3       |\tMedium predictive Power  |\n|0.3 to 0.5       |\tStrong predictive Power  |\n| >0.5\t          |Suspicious Predictive Power or too good to be true|","8ebe0618":"df1[df1['pub_rec']==3.0]\n\nbad rate is increasing when it is increasing from 0 to 2. Value 3 have just 5 records hence we are not able to understand the behaviour for value 3","349e841d":"# Model Interpretability","0a34c60c":"# Base Model \nWe will start with stats model and using p-value, RFE and VIF we will perform the backward feature \nselection and retrain the model after dropping the variable( drop variable one by one)","63d318bc":"### Finding Optimal Cutoff Point","f200d7d6":"We can clearly see if purpose category is small business of other types (like educational small business,house small busines etc) then loan bad rate is very high","5673858f":"Lowest income and lower middle income has highest bad rate while higher income has lowest bad rate of loan."}}