{"cell_type":{"3c0a8db3":"code","acbf438e":"code","18591f01":"code","0d9a604e":"code","12c67050":"code","b5bf700f":"code","ea87a3ac":"code","0d72dafb":"code","2883fe10":"code","d496d0c8":"code","695b3791":"code","5a29b012":"code","be2c53db":"code","d9d78fe1":"code","ac9ca76f":"code","1b3564f9":"code","5893d0c6":"code","c22ec41a":"code","5e83b627":"markdown","0fb79d31":"markdown","4393d727":"markdown","a3cd6420":"markdown","03a1aae5":"markdown","01c6a40e":"markdown","7c09696a":"markdown","fd3e48c3":"markdown","4bfaa1f7":"markdown","1ed2bcdd":"markdown","0e16ef39":"markdown","0b3eed50":"markdown","b312a695":"markdown","9cca3ecd":"markdown","e579c17f":"markdown","6e031b96":"markdown"},"source":{"3c0a8db3":"import pandas as pd #import pandas\nimport numpy as np #import numpy\nfrom sklearn.preprocessing import LabelEncoder  #importing LabelEncoder\n","acbf438e":"train = pd.read_csv('..\/input\/bigmart-sales-data\/Train.csv')","18591f01":"#check the head of dataset\ntrain.head(5)","0d9a604e":"#check the size of the dataset\nprint('Data has {} Number of rows'.format(train.shape[0]))\nprint('Data has {} Number of columns'.format(train.shape[1]))","12c67050":"#check the information of the dataset\ntrain.info()","b5bf700f":"#let's keep our categorical variables in one table\ncat_data = train[['Item_Identifier','Item_Fat_Content','Item_Type','Outlet_Identifier','Outlet_Size','Outlet_Location_Type','Outlet_Type']]","ea87a3ac":"cat_data.head()   #check the head of categorical data","0d72dafb":"cat_data.apply(lambda x: x.nunique()) #check the number of unique values in each column","2883fe10":"#check the top 10 frequency in Item_Identifier\ncat_data['Item_Identifier'].value_counts().head(10)","d496d0c8":"pd.get_dummies(cat_data['Item_Identifier'],drop_first=True)  #applying one hot encoding","695b3791":"#apply binary encoding on Item_Identifier\nimport category_encoders as ce                              #import category_encoders\nencoder = ce.BinaryEncoder(cols=['Item_Identifier'])        #create instance of binary enocder\ndf_binary = encoder.fit_transform(cat_data)                 #fit and tranform on cat_data\ndf_binary.head(5)\n","5a29b012":"#check the unique values \ncat_data['Item_Fat_Content'].unique()","be2c53db":"low_fat = ['LF','low fat']\ncat_data['Item_Fat_Content'].replace(low_fat,'Low Fat',inplace = True) #replace 'LF' and 'low fat' with 'Low Fat'\ncat_data['Item_Fat_Content'].replace('reg','Regular',inplace = True)   #Replace 'reg' with regular","d9d78fe1":"cat_data['Item_Fat_Content'].unique()","ac9ca76f":"#Apply LabelEncoder\nle = LabelEncoder()\ncat_data['Item_Fat_Content_temp'] = le.fit_transform(cat_data['Item_Fat_Content'])\nprint(cat_data['Item_Fat_Content'].head())\nprint(cat_data['Item_Fat_Content_temp'].head())","1b3564f9":"#prepare a dict to map\nmapping = {'Low Fat' : 0,'Regular': 1} #map Low Fat as 0 and Regular as 1\ncat_data['Item_Fat_Content_temp1'] = cat_data['Item_Fat_Content'].map(mapping)\ncat_data['Item_Fat_Content_temp1'].head()","5893d0c6":"factorized,index = pd.factorize(cat_data['Item_Fat_Content'])  #using pd.factorize it gives us factorized array and index values\nprint(factorized)\nprint(index)","c22ec41a":"#Let's look at item type column\nprint(cat_data['Item_Type'].nunique())  #check number of unique values\nprint(cat_data['Item_Type'].unique())   #check the unique values","5e83b627":"**Use Pandas pd.factorize method.**\n\nIt does the nominal encoding based on the order in which the categories apper. If Low Fat is at index 0 then it will be encoded as 0 Regular as 1 and vice versa.\n\n","0fb79d31":"The values in Item_Identifier has no ordering as we can see. These are nominal categorical variable.\n\nThe first column has 1559 unique values. If we try to do one hot encoding here we will have 1558 new features. We cannot feed in these many features in our model. It will make our model complex and it will reduce the model accuracy.","4393d727":"Now think which encoding technique can we apply here.\n\n* First thought would be to apply one hot encoding on features which has 3-5 unique categories.\n* But what if there is some kind of ordering present between them. So firstly we should identify the nominal and ordinal variable\n* Let's check one by one","a3cd6420":"**Encoding Item_Fat_Content**","03a1aae5":"\nIt is useful when we have ordering in our categories.","01c6a40e":"And we don't Have any ordering between them. So we have to apply ordinal encoding technique. i Leave it upto you to decide which technique to apply and we will have look at other techniques in our next Notebook.\n\nLink to kernel 3 : https:\/\/www.kaggle.com\/krishnaheroor\/encoding-technique-3","7c09696a":"In this Notebook we have seen 2 new encoding techniques.\n\n* Mapping\n* pd.factorize\n\nWe have seen the usage of different methods, their advantages and disadvantages.","fd3e48c3":"Binary encoder has given us 11 new feature which is way less than we were getting from one hot encoding. So we have been rescued here by Binary Encoding.\n\nWe have applied binary encoding but it doesn't provide us any intution as how these new features are made. All we know is by using binary encoding Here the labels are firstly encoded ordinal and then they are converted into binary codes. Then the digits from that binary string are converted into different features.\n\nThere are other intutive measures to reduce the features. We will look at them later.","4bfaa1f7":"Things to learn -\n\n* Indentifying data type as ordinal,nominal and continuous.\n* Applying different types of encoding.\n* Challenges with different encoding techniques.\n* Choosing the appropriate encoding techniques.","1ed2bcdd":"Here we have 5 unique values but if we look at them closely there are only 2 unique values. Low Fat and Regular, others are just short forms for them or are in small letters","0e16ef39":"\nAs we can see here, we have 7 categorical variables and 5 numeric variables. The first task is to identify these categorical variables as nominal or ordinal.","0b3eed50":"**We can use map to do ordinal encoding**","b312a695":"Here we have 2 categories in Item_Fat_Content and we have some ordering between the. Low Fat will have less Fat content than the regular Fat. So it is a ordinal variable.","9cca3ecd":"Here we only had 2 categories 'Low Fat' and 'Regular' so using LabelEncoding has worked here. It has mapped :-\n\n* Low Fat ------- 0\n* Regular ------- 1\n\nHere the natural ranking of alphabets has worked but every time you are not this lucky.","e579c17f":"As expected from a single feature now we have 1558 features. So it's a bad idea to apply one hot encoding here. We should not apply one hot encoding when there are too many categories.\n\nSo one hot encoding has failed us here. Now for rescue we move to LabelEncoding but we are very much aware that if we apply label encoding on a feature it assigns a natural ranking to the categories alphabatically. So we cannot apply Label encoding as well.\n\nSo we have 1 thing left (Binary Encoding) that we have learnt previously. Let's apply it and see what we get.","6e031b96":"\nWe are going to apply the different encoding techniques on big mart sales data kaggle.\n\nLink : https:\/\/www.kaggle.com\/brijbhushannanda1979\/bigmart-sales-data"}}