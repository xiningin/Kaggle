{"cell_type":{"3571066b":"code","cf71a038":"code","40200793":"code","4be8ce42":"code","169a0afe":"code","2c1e50ce":"code","6b7a074d":"code","b98eb98b":"code","bf1b5f06":"code","93f183ce":"code","2dd67c33":"code","4a533882":"code","578aeae9":"code","c00770b3":"code","0b92f3cc":"code","0f92a864":"code","54b9da95":"code","9d64cebd":"code","2cade08f":"code","a4ff3d87":"code","8b67d07b":"code","3d7f83d5":"code","96a0ede0":"code","d761bf30":"code","35452434":"code","abe5ed32":"code","ba3c7cfa":"code","5f920811":"code","d7429b35":"code","5a007328":"code","48b5369d":"code","1626cb67":"code","a45fc21c":"code","3acca232":"code","90d6f98f":"code","f7bd1b8f":"code","a6e98e3a":"code","bd2fc569":"code","fd44e0b1":"code","b07c0b1c":"code","e6453e24":"code","fa7897f9":"code","4bc70d57":"code","eeb478ef":"code","f36ec7d4":"code","20dfec5c":"code","ce620bfc":"code","fa52c580":"code","369c9380":"code","5c8b257f":"code","679cb0fa":"code","1e3698fe":"code","ac3571f9":"code","a0de3884":"code","92c220d3":"code","95aa815d":"code","2f1c45ad":"code","8937a8ef":"code","38722ec5":"code","bf5a3485":"code","7d68e225":"code","e268b6fe":"markdown","f0a37988":"markdown","f1dec8a3":"markdown","c1168170":"markdown","d11aee5f":"markdown","86e19668":"markdown","f3ebacbf":"markdown","51299b8c":"markdown","bed4a16d":"markdown","7ef5da2f":"markdown"},"source":{"3571066b":"# -- Importing required libraries and packages\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport json\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline","cf71a038":"# -- How the data was originally imported\n# -- Getting data from API multiple times with an offset of 50\n# -- 50 is max no. of downloadable records in one attempt\n\n'''df_combo_lst = []\nofst = 0\nfor i in range(10):\n    prms = {'term':'pub','location':'Melbourne','limit':50, 'offset':ofst}\n    resp = requests.get(url, params=prms, headers=head)\n    data_ind = resp.json()\n    data = pd.DataFrame(data_ind['businesses'])\n    df_combo_lst.append(data)\n    ofst += 50'''\n\n# -- Kaggle wouldn't let me import data directly from Yelp","40200793":"# -- Importing dataset\ncombo_df = pd.read_csv('..\/input\/pub-hopping-in-melbourne-yelp\/Yelp_500_PUBS.csv')","4be8ce42":"# -- Info on the dataset\n\ncombo_df.info()\nprint ('\\nshape of combo_df: ', str(combo_df.shape))","169a0afe":"# -- The customary 'First few rows'\n\ncombo_df.head(7)","2c1e50ce":"# -- dropping unnamed column\ncombo_df.drop(columns='Unnamed: 0', axis=1,inplace=True)","6b7a074d":"# -- Checking for missing values\ncombo_df.isna().sum()","b98eb98b":"# -- Checking for missing values\nax = plt.axes()\nsns.heatmap(combo_df.isna(),cmap=None,  yticklabels=False)\nax.set_title(label=\"(white bars means misisng values)\")\ncombo_df.shape\n\n# -- The only missing values we need to focus on is in the 'price' series","bf1b5f06":"# -- Checking for closed pubs\ncombo_df[combo_df.is_closed == True]\n\n# -- Returns no rows\n# -- No closed pubs in this dataset... no one's happy when they're closed","93f183ce":"# -- Checking for duplicated entries in 'name'\ndup_df = combo_df[combo_df.name.duplicated(keep=False)]\ndup_df.shape\n# -- We have duplicate entries","2dd67c33":"# -- Checking for duplicated entries in the 'alias' and 'id' columns\n# -- There is a possibility that these are from the same chain of pubs\n\nprint(combo_df[combo_df.id.duplicated()], combo_df[combo_df.alias.duplicated()] )\n# -- Both return no entries\n# -- So it's safe to assume that the duplicated pub names might be from same owner but at different locations   ","4a533882":"# -- Exploring 'location' column. This might have something interesting\n# -- 'location' is a deeply nested series\n\n\n# -- replace \"'\" with \"\"\" \ny = [i.replace(\"\\'\", \"\\\"\") for i in combo_df.location]\ncombo_df.location = y\n\n# -- Turning string into dictionary\ncombo_df.location = [eval(i) for i in combo_df.location]","578aeae9":"# -- each entry for location is a dictionary \ncombo_df.location.head()","c00770b3":"# -- exploring keys in location\ncombo_df.location[0].keys()\n\n# -- assigning values to the new columns zip_c and suburb\ncombo_df['suburb'] = [combo_df.location[i]['city'] for i in range(len(combo_df.location))]\ncombo_df['zip_c'] = [combo_df.location[i]['zip_code'] for i in range(len(combo_df.location))]","0b92f3cc":"combo_df.head()","0f92a864":"# -- checking for multiple zip_c for the same suburb\n\ndict_ = {}\nfor i in combo_df.groupby('suburb')['zip_c']:\n    dict_[list(i)[0]] = list(i)[1].nunique()\n\ndict_","54b9da95":"# -- Examining suburbs with multiple codes\nsub_list = []\nfor k, v in dict_.items():\n    if v > 1:\n        sub_list.append(k)\n        \nprint('Suburbs with multiple zip-codes:',sub_list)","9d64cebd":"# -- Analysing suburbs with multiple codes\nfor i in sub_list:\n    print(i,':',combo_df.loc[combo_df.suburb == i,'zip_c'].unique())\n\n# -- We see that there are blanks in zip_c for Melbourne and Melbourne_Airport\n# -- We shall revisit this if need arises ","2cade08f":"# -- Filling zip_c values\ncombo_df.loc[combo_df.suburb=='Melbourne Airport','zip_c'] = '3045'\n\n# -- Filling missing values for zip_c with the most frequent value\ncombo_df.loc[combo_df.zip_c=='',:]\ncombo_df.loc[combo_df.zip_c=='','zip_c']='3000'","a4ff3d87":"# Suburbs from most to least pubs in it\ncombo_df.suburb.value_counts()","8b67d07b":"# -- Pubs' distribution among suburbs\nvc_sub = combo_df.suburb.value_counts()\n\npd.Series(vc_sub)[0:20].plot(kind='bar')\n","3d7f83d5":"sns.set_theme()\nfig, ax = plt.subplots(figsize=(12,5))\ng = sns.ecdfplot(x='distance', data=combo_df, marker='.',linestyle='None', label='Distance from city centre')\ng.set(xlabel='Distance', title='ECDF for Distance')\nplt.legend()\n\n# -- A bit more than 70% of pubs are within 5000 m. of the Melbourne CBD","96a0ede0":"# -- Histogram of how pubs are distributed based on distance\ncombo_df.distance.hist(bins=40)","d761bf30":"# categorize suburbs based on distance\n# splitting at 2500, 5000 and 7500 distance units (meters I guess)\n# categorizing as nearby, farther and faroff\n\ndef dist_cat(i):\n    if i <= 2500:\n        val_ = 'nearby'\n    elif i <= 7500:\n        val_ = 'farther'\n    else:\n        val_ = 'faroff'\n    return(val_)   \n\nlist_ = [dist_cat(i) for i in combo_df.distance]\n\n# Adding these categories to the combo_df DF\ncombo_df['dist_cat'] = list_\n\ncombo_df['dist_cat'].value_counts().plot(kind='bar')","35452434":"combo_df.categories.head()\n# -- each entry is a list which has to be converted to a dictionary","abe5ed32":"# -- replace single qotes with double-qotes \nz = [i.replace(\"\\'\", \"\\\"\") for i in combo_df.categories]\ncombo_df.categories = z\n\ncombo_df.categories = [eval(i) for i in combo_df.categories]\nl2_=[i[0]['alias'] for i in combo_df.categories]\n\npd.Series(l2_).value_counts()[:10]","ba3c7cfa":"# -- The counts drop off quickly after 'restaurants'.\npd.Series(l2_).value_counts()[:10].plot(kind='bar')\ncombo_df['theme_cat'] = pd.Series(l2_)","5f920811":"# -- Upon close observation we see that most of the aliases are some kind of a bar or a pub (eg cocktailbar, brewpub etc). \n# -- So, these can be classified broadly classified into 'pubs','bars','restaurants' and 'others'\n\n# converting anything with pub, bar or restaurant into 'pubs','bars','restaurants' \ntheme_cat_list = ['pub','bar','restaurant'] \n\nfor i in theme_cat_list:\n    combo_df.loc[combo_df.theme_cat.str.contains(i), 'theme_cat'] = i\n    \n# -- It looks like a lot of pubs have registered themselves under the alias 'bar'\n\n# -- Plotting the counts on theme_cat series\ncombo_df.theme_cat.value_counts()[:10].plot(kind='bar')","d7429b35":"# -- Changing anything other than what's in theme_cat into 'others'\n\nfor i in combo_df['theme_cat']:\n    if i not in theme_cat_list:\n        combo_df.loc[combo_df.theme_cat==i,'theme_cat'] = 'other'\n        \nfig, ax = plt.subplots()\ncombo_df.theme_cat.value_counts().plot(kind='bar')\nax.set_xlabel('Theme Categories')\nax.set_title('Count of Theme Categories')\n","5a007328":"# CDF for review_count\nfig, ax = plt.subplots(figsize=(12,5))\nx_val = np.sort(combo_df.review_count)\ny_val = np.arange(1, len(combo_df.review_count)+1)\/len(combo_df.review_count)\nplt.plot(x_val, y_val, marker='.', linestyle='None',color='magenta')\nax.set_xlabel('Review Count')\nax.set_title('CDF for Review Count')\n\n\n# -- About 90% of pubs have about 20 reviews or less","48b5369d":"# Histogram + KDE for review_count\nfig, ax = plt.subplots(figsize=(12,5))\ng = sns.distplot(x=combo_df.review_count, bins=50, kde=True, color='skyblue', kde_kws={'color':'green'})\ng.set(xlabel='Review Count',title='Hist. + KDE of review_count series')\n\n\n# -- review_count distibuton seen with the KDE overlaid on histogram\n# -- Most pubs have what looks like 3 to 5 reviews","1626cb67":"# -- On observation, it appears that we can categorize review_count as low, med and high for <10, <20 and >20 \n\n# creating a range\nranges = [0,10,20,np.inf]\n\n# creating label names\nlab_names = ['low', 'med', 'high']\n\n# assigning categories to review count ranges\ntemp_l = pd.cut(combo_df.review_count, bins=ranges, labels=lab_names)\n\n# new column rev_cat added to combo_df\ncombo_df['rev_cat'] = temp_l\n\nfig, ax = plt.subplots()\ncombo_df['rev_cat'].value_counts().plot(kind='bar')\nax.set_xlabel('Theme Categories')\nax.set_title('Review Counts Categories')","a45fc21c":"# Count of 'rating'\nfig, ax = plt.subplots(figsize=(12,5))\nsns.countplot(x='rating', data=combo_df)\nax.set_xlabel('Pub Ratings')\nax.set_title(\"Pub Ratings' Count\")\n\n\n# -- 3.5 stars are what majority of the pubs have got closely followed by 4.0\n# -- These pubs are ahead of ratings 3 and below by a considerable margin","3acca232":"# categorizing 'rating' series from combo_df\n\ncombo_df['rat_cat'] = combo_df.rating\n\ncombo_df.loc[(combo_df.rating == 1.0),'rat_cat'] = 1.0\ncombo_df.loc[(combo_df.rating == 1.5) | (combo_df.rating == 2.0),'rat_cat'] = 2.0\ncombo_df.loc[(combo_df.rating == 2.5) | (combo_df.rating == 3.0),'rat_cat'] = 3.0\ncombo_df.loc[(combo_df.rating == 3.5) | (combo_df.rating == 4.0),'rat_cat'] = 4.0\ncombo_df.loc[(combo_df.rating == 4.5) | (combo_df.rating == 5.0),'rat_cat'] = 5.0\n\nfig, ax = plt.subplots()\ncombo_df.rat_cat.value_counts().sort_index().plot(kind='bar')\nax.set_xlabel('Ratings Cat.')\nax.set_title('Ratings Count Categories')","90d6f98f":"combo_df.info()","f7bd1b8f":"# copying combo_df to combo_df2: *****************************\n\ncombo_df2 = combo_df.copy()\ncombo_df2.head(4)","a6e98e3a":"#drop nan values from price series\ncombo_df_nn = combo_df2.dropna(axis=0)\n\n# Confirming successful nan removal\nax = plt.axes()\nsns.heatmap(combo_df_nn.isna(), yticklabels = False, cmap=None)\nax.set_title(label=\"(white bars means missing values)\")\ncombo_df_nn.shape","bd2fc569":"# A DF of missing 'price' values\ncombo_df_na = combo_df2.loc[combo_df2.price.isna(),:]\ncombo_df_na.shape","fd44e0b1":"# Converting 'price' series to type - category\n\ncombo_df_nn.price.astype('category')\n# combo_df_nn.iloc[0, 12]['city']\ncombo_df_nn.price.value_counts()","b07c0b1c":"# Turning 'price' series into something more relatable\ndict_ = {'$':'Budget','$$':'Medium','$$$':'Expensive','$$$$':'Luxury'}\nl1_ = []\n\nfor i in combo_df_nn.price:\n    for k,v in dict_.items():\n        if k == i:\n            l1_.append(v)\n\ncombo_df_nn.price = l1_\n","e6453e24":"# Pubs distance based on price categories\n\nsns.set_theme()\nfig, ax = plt.subplots(figsize=(12,5))\n#combo_df_nn.price.value_counts().plot(kind='bar')\nsns.violinplot(y='distance', data=combo_df_nn, x='price', palette='Dark2')\n\n# --Medium and Expensive category pubs seem to be farther away from the city centre\n# --Mostly, pubs of all 4 price categories are well within 5 km from the city centre  \n","fa7897f9":"fig, ax = plt.subplots(figsize=(12,5))\ng = sns.violinplot(y='review_count', x='price',data=combo_df_nn)\ng.set(xlabel='Price Category', title='Distribution of review_count values across price categories', ylabel='No. of reviews')\n\n# -- Moderately expensive -'Medium' pubs have the highest number of reviews by a huge margin\n# -- For all price categories, majority of the pubs have median review_count value of around 10\n","4bc70d57":"# KDE for review_count 'price'-wise and rating category-wise\n\nfig, (ax1, ax2) = plt.subplots(1,2,figsize=(16,5))\ng1 = sns.kdeplot(x='review_count', data=combo_df_nn, palette='bright', hue='price', ax=ax1)\ng2 = sns.kdeplot(x='review_count', data=combo_df_nn, palette='bright', hue='rat_cat', ax=ax2)\ng1.set(xlabel='Review Count', title='Price-wise KDE for reviews')\ng2.set(xlabel='Review Count', title='Rating-wise KDE for reviews')\n\n# -- 'Medium' priced pubs have the most number of reviews followed by Budget and expensive and 4.0\n","eeb478ef":"# Theme-wise review_count hist.\n\nfig, ax = plt.subplots(figsize=(12,5))\nsns.histplot(x=combo_df2.review_count, hue=combo_df2.theme_cat, palette='Dark2')","f36ec7d4":"# -- Here are KDE and scatter plots showing the distribution of distance against review_counts\n\ng = sns.pairplot(combo_df_nn, vars=['distance','review_count'], hue='price',height=3, aspect=2, diag_kind='kde', \\\n                  plot_kws={'alpha':0.8, 's':15}, palette='Dark2')","20dfec5c":"# -- Showing how the pubs are scattered along the distance\ng = sns.JointGrid(x='distance', y='review_count', data=combo_df_nn, hue='price', height=8)\ng.plot_joint(sns.scatterplot, alpha=0.8, s=25)\ng.plot_marginals(sns.histplot, kde=True, hue='price')","ce620bfc":"# -- How are the pubs, bars, restaurants and others are distributed along the distance?\ng = sns.lmplot(x=\"distance\", y=\"review_count\", hue=\"theme_cat\", data=combo_df_nn, fit_reg=False, height=7, scatter_kws={\"s\": 15,\\\n                                                                                'alpha':.6})\ng = sns.regplot(x=\"distance\", y=\"review_count\", data=combo_df_nn, scatter=False, ax=g.axes[0, 0],\\\n            line_kws={'color':'red', 'linewidth':1.5, 'linestyle':'--'}, order=2, ci=None)\n","fa52c580":"# Remove redundant columns\n# duplicate combo_df_nn DF\ncopy_df_nn2 = combo_df_nn[['price','dist_cat','theme_cat','rev_cat','rat_cat']]\ncopy_df_nn2.shape","369c9380":"# Creating dummies for categorical values\n\n# Converting dist_cat into dummy column\ndummy_df_dist = pd.get_dummies(copy_df_nn2.dist_cat, prefix='dist', prefix_sep='_', drop_first=True)\n\n# Converting theme into dummy column\ndummy_df_thm = pd.get_dummies(copy_df_nn2.theme_cat, prefix='theme', prefix_sep='_', drop_first=True)\n\n# Converting rev_cat into dummy column\ndummy_df_rev = pd.get_dummies(copy_df_nn2.rev_cat, prefix='rev', prefix_sep='_', drop_first=True)\n\n# Converting rat_cat into dummy column\ndummy_df_rat = pd.get_dummies(copy_df_nn2.rat_cat, prefix='rat', prefix_sep='_', drop_first=True)\n\n# joining dummy_df to copy_df_nn\nmodel_df = pd.concat([copy_df_nn2, dummy_df_dist, dummy_df_thm, dummy_df_rev, dummy_df_rat],\\\n                     axis=1).drop(columns=['dist_cat','theme_cat','rev_cat','rat_cat'])\n                                                                    \nmodel_df.head()","5c8b257f":"# segregating dependant and independant parameters\ny = model_df['price'] # Dependant\n\nmodel_df.drop(columns='price', inplace=True)\nX = model_df # Independant\nX","679cb0fa":"# Splitting data into test and training sets\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.33, random_state=1)","1e3698fe":"# Building a Logistic Regression model\nLR_model = LogisticRegression()\nLR_model.fit(X_train, y_train)","ac3571f9":"# predicting for X_test\npred = LR_model.predict(X_test)","a0de3884":"# Checking accuracy of predictions\naccu_ = accuracy_score(y_test, pred)\naccu_","92c220d3":"copy_df_na2 = combo_df_na[['dist_cat','theme_cat','rev_cat','rat_cat']]","95aa815d":"copy_df_na2.info()","2f1c45ad":"# Creating dummies for categorical values on \n\n# Converting dist_cat into dummy column\ndummy_df_dist2 = pd.get_dummies(copy_df_na2.dist_cat, prefix='dist', prefix_sep='_', drop_first=True)\n\n# Converting theme into dummy column\ndummy_df_thm2 = pd.get_dummies(copy_df_na2.theme_cat, prefix='theme', prefix_sep='_', drop_first=True)\n\n# Converting rev_cat into dummy column\ndummy_df_rev2 = pd.get_dummies(copy_df_na2.rev_cat, prefix='rev', prefix_sep='_', drop_first=True)\n\n# Converting rat_cat into dummy column\ndummy_df_rat2 = pd.get_dummies(copy_df_na2.rat_cat, prefix='rat', prefix_sep='_', drop_first=True)\n\n# joining dummy_df to copy_df_nn\nmodel_df2 = pd.concat([copy_df_na2, dummy_df_dist2, dummy_df_thm2, dummy_df_rev2, dummy_df_rat2],\\\n                     axis=1).drop(columns=['dist_cat','theme_cat','rev_cat','rat_cat'])\n                                                                    \nmodel_df2.head()","8937a8ef":"pred2 = LR_model.predict(model_df2)\npd.Series(pred2).unique()","38722ec5":"#replacing missing 'price' values from combo_df DF\n\nind = combo_df.loc[combo_df.price.isna(),'price'].index\n\nfor i in ind:\n    for j in pred2:\n        #print(j)\n        combo_df.iloc[i, 11] = j","bf5a3485":"# -- checking for null values in combo_df\n\nsns.heatmap(combo_df.isna(), yticklabels=False)\ncombo_df.shape\n","7d68e225":"# -- image_url, display_phone and phone columns can be ignored\/dropped\ncombo_df.drop(columns=['image_url','phone','display_phone']).head(3)","e268b6fe":"# Filling out missing 'price'","f0a37988":"# Analysing 'rating' series","f1dec8a3":"# ECDF on Distance\n","c1168170":"![_111325972_pub2.jpg](attachment:_111325972_pub2.jpg)","d11aee5f":"# **Contents**\n\n* Importing libraries and data\n* A peek into the data\n* Checking for closed pubs and missing values\n* Checking for duplicates\n* Normalizing\/ correcting few columns\n* EDA on categorical and numeric columns\n* Predicting missing 'price' values\n","86e19668":"# **\u201cTo alcohol: the cause of, and solution to, all of life's problems.\u201d - Homer J. Simpson**\n\nSmall pay-check? sucky job? bad boss? dodgy colleagues? missed promotions?\njudgy in-laws? nosy neighbors? noisy kids? \n\nAnd all you want is nothing but get away from daily dramas that you'd rather\nstep on a piece of Lego?\nDo you want to drain your pain, and shake off your sorrows \nlike a dog drenched to the bone?\n\nThen look no further lads and lasses... You are at the right place!\n\n'Pub Hopping in Melbourne' is a data driven guide to Pubs in Melbourne for all you legendary \nWednesday warriors who want to crack a cold one and be yourself.\n\n**Attribute Info:**\n* id: This is a unique field. I believe it's the registration ID\n* alias: alternate Name\n* name: Well\u2026 name\n* image_url: NA\n* is_closed: status\n* url: webpage address\n* review_count: No. of reviews\n* categories: Category under which the place is registered\n* rating: Rating as on Yelp\n* coordinates: coordinates of the Pub\n* transactions:?\n* price: How expensive is this place\n* location: Address\n* phone: Contact number\n* display_phone: Contact number\n* distance: Distance from Melbourne city centre (CBD)\n\n**Questions:**\n* Carrying out an EDA on the Dataset\n* Cleaning and categorizing the Dataset\n* Predicting missing Price values.\n\n\n**Questions:**\n* Where are these pubs located?\n* How far is the farthest pub from the CBD?\n* How are the pricing and reviews stacked up?\n\n\n\n \n\n","f3ebacbf":"# Analysing 'categories' series from combo_df DF","51299b8c":"# Predicting missing 'price' values","bed4a16d":"# Comparing numerical series","7ef5da2f":"# Analysing 'review_counts' from combo_df DF\u00b6"}}