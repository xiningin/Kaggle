{"cell_type":{"45c0ff46":"code","bb05171e":"code","6938fba8":"code","3132874e":"code","c582ec3c":"code","c5ca4e72":"code","2e4ab84a":"code","cfa77e47":"code","a0df0fc3":"code","b0bce605":"code","950403ef":"code","96dee514":"code","40b6ee5e":"code","8ac8b4b4":"code","42f08b51":"code","9dd3786a":"code","c7a13701":"code","0bec25c4":"code","92959bf9":"code","96b5199b":"code","a041eb33":"code","ae2b4c88":"code","e5a0ab07":"code","4afc7a64":"code","bae7e1d4":"code","790e364b":"code","4ac89b1b":"code","d270293c":"code","11d79768":"code","93c9cc18":"code","5f859181":"markdown","b2c99f41":"markdown","2b8f9b83":"markdown","5be44540":"markdown","292b211b":"markdown","519ed60b":"markdown","268d3abe":"markdown","29fc11f8":"markdown","e75ecc7e":"markdown","b4da620b":"markdown","4cea48a7":"markdown","6f8b6991":"markdown"},"source":{"45c0ff46":"import os\nos.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_selection import mutual_info_regression\nfrom sklearn.linear_model import LogisticRegression\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\n\nimport shap","bb05171e":"%%time\ntrain = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv', index_col=0)\ntest = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv', index_col=0)\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv\")\nfeature_cols = test.columns.tolist()","6938fba8":"train.hist(figsize=(20,15), grid=False, ylabelsize=5, xlabelsize=5, bins=30)\nplt.show()","3132874e":"sc = StandardScaler()\ntrain[feature_cols] = sc.fit_transform(train[feature_cols])\ntest[feature_cols] = sc.transform(test[feature_cols])","c582ec3c":"useful_features = [\"f34\", \"f55\", \"f43\", \"f8\", \"f91\", \"f80\", \"f71\", \"f27\", \"f50\", \"f97\", \"f41\", \"f25\", \"f57\", \"f66\", \"f22\", \"f96\", \"f82\", \"f26\", \"f81\", \"f40\"]","c5ca4e72":"# %%time\n\n## You can uncomment and run the following lines to find the elbow point\n# inertia = {}\n# for i in range(2,18):\n#     kmeans = KMeans(n_clusters=i, random_state=42)\n#     kmeans.fit_predict(train[useful_features])\n#     inertia.update({i:kmeans.inertia_})\n\n# inertia_df = pd.Series(inertia)\n# plt.plot(inertia_df,marker=\"o\")\n# plt.xticks(inertia_df.index)\n# plt.xlabel(\"Number of clusters\")\n# plt.ylabel(\"Inertia\")\n# plt.show()","2e4ab84a":"%%time\nn_clusters = 5\ncd_feature = True # cluster distance instead of cluster number  \n\nkmeans = KMeans(n_clusters=n_clusters, random_state=42)\n\nif cd_feature:\n    cluster_cols = [f\"cluster{i+1}\" for i in range(n_clusters)]\n    \n    X_cd = kmeans.fit_transform(train[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=train.index)\n    train = train.join(X_cd)\n    \n    X_cd = kmeans.transform(test[useful_features])\n    X_cd = pd.DataFrame(X_cd, columns=cluster_cols, index=test.index)\n    test = test.join(X_cd)\n\nelse:\n    cluster_cols = [\"cluster\"]  \n    train[\"cluster\"] = kmeans.fit_predict(train[useful_features])\n    test[\"cluster\"] = kmeans.predict(test[useful_features])\n    \n\nfeature_cols += cluster_cols\n\ntrain.head()","cfa77e47":"%%time\nsns.pairplot(train[cluster_cols+[\"target\"]].sample(1000, random_state=0), hue=\"target\", diag_kind='kde')\nplt.show()","a0df0fc3":"train[\"new_f1\"] = train[\"cluster1\"]-train[\"cluster4\"]\ntrain[\"new_f2\"] = train[\"cluster3\"]-train[\"cluster4\"]\ntest[\"new_f1\"]  = test[\"cluster1\"]-test[\"cluster4\"]\ntest[\"new_f2\"]  = test[\"cluster3\"]-test[\"cluster4\"]","b0bce605":"plt.figure(figsize=(10,5))\nsns.scatterplot(data=train, x=\"new_f1\", y=\"new_f2\", hue=\"target\", alpha=0.8)\nplt.show()","950403ef":"train[\"new_f3\"] = train[\"new_f1\"]-train[\"new_f2\"]\ntrain[\"new_f4\"] = train[\"new_f1\"]+train[\"new_f2\"]\ntest[\"new_f3\"]  = test[\"new_f1\"]-test[\"new_f2\"]\ntest[\"new_f4\"]  = test[\"new_f1\"]+test[\"new_f2\"]","96dee514":"plt.figure(figsize=(10,5))\nsns.scatterplot(data=train, x=\"new_f3\", y=\"new_f4\", hue=\"target\", alpha=0.8)\nplt.show()","40b6ee5e":"train.drop(\"new_f3\", axis=1, inplace=True) # since it doesn't look promising based on above plots\ntest.drop(\"new_f3\", axis=1, inplace=True)\n\nfeature_cols += [\"new_f1\", \"new_f2\", \"new_f4\"]","8ac8b4b4":"folds = 5\ntrain[\"kfold\"] = -1\nkf = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n\nfor fold, (train_indicies, valid_indicies) in enumerate(kf.split(train,train[\"target\"])):\n    train.loc[valid_indicies, \"kfold\"] = fold","42f08b51":"%%time\nscores = []\n\ntrain[\"lr\"] = 0\ntest[\"lr\"] = 0\nfor fold in range(folds):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    y_train = x_train['target']\n    y_valid = x_valid['target']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n\n    \n    lr_model = LogisticRegression()\n    lr_model.fit(x_train, y_train)\n    \n    preds_train = lr_model.predict_proba(x_train)[:,1]\n    preds_valid = lr_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(\"Fold\",fold,\", train:\", f\"{auc_train:.6f}\", \", valid:\", f\"{auc:.6f}\")\n    scores.append(auc)\n    \n    preds_test = lr_model.predict_proba(x_test)[:,1]\n    train[\"lr\"].loc[x_valid.index] = preds_valid\n    test[\"lr\"] += preds_test\n    \ntest[\"lr\"] \/= folds\nprint(\"AVG AUC:\",np.mean(scores))","9dd3786a":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=train, x=\"lr\", y=\"new_f4\", hue=\"target\", alpha=0.8)\nplt.show()","c7a13701":"feature_cols.append(\"lr\")","0bec25c4":"pca = PCA(n_components=2, random_state=42)\nX_pca = pca.fit_transform(train[feature_cols])\nT_pca = pca.transform(test[feature_cols])\n\npca_cols = [f\"PC{i+1}\" for i in range(X_pca.shape[1])]\n\nX_pca = pd.DataFrame(X_pca, columns=pca_cols, index=train.index)\nT_pca = pd.DataFrame(T_pca, columns=pca_cols, index=test.index)\n\ntrain = pd.concat([train, X_pca], axis=1)\ntest = pd.concat([test, T_pca], axis=1)\ntrain.head()","92959bf9":"loadings = pd.DataFrame(pca.components_, index=pca_cols, columns=train[feature_cols].columns)\nloadings.style.bar(align='mid', color=['#d65f5f', '#5fba7d'])","96b5199b":"plt.figure(figsize=(15,8))\nsns.scatterplot(data=train, x=\"PC1\", y=\"PC2\", hue=\"target\", alpha=0.8)\nplt.show()","a041eb33":"feature_cols += [\"PC1\", \"PC2\"]","ae2b4c88":"%%time\nx = train.iloc[:5000,:][feature_cols].copy()\ny = train.iloc[:5000,:]['target'].copy()\nmi_scores = mutual_info_regression(x, y)\nmi_scores = pd.Series(mi_scores, name=\"MI Scores\", index=x.columns)\nmi_scores = mi_scores.sort_values(ascending=False)","e5a0ab07":"top = 20\nplt.figure(figsize=(20,10))\nsns.barplot(x=mi_scores.values[:top], y=mi_scores.index[:top], palette=\"summer\")\nplt.title(f\"Top {top} Strong Relationships Between Feature Columns and Target Column\")\nplt.xlabel(\"Relationship with Target\")\nplt.ylabel(\"Feature Columns\")\nplt.show()","4afc7a64":"%%time\nfinal_test_predictions = []\nscores = []\n\nfor fold in range(folds):\n    x_train = train[train.kfold != fold].copy()\n    x_valid = train[train.kfold == fold].copy()\n    x_test  = test[feature_cols].copy()\n    \n    y_train = x_train['target']\n    y_valid = x_valid['target']\n    \n    x_train = x_train[feature_cols]\n    x_valid = x_valid[feature_cols]\n\n    xgb_params = {\n        'eval_metric': 'auc', \n        'objective': 'binary:logistic', \n        'tree_method': 'gpu_hist', \n        'gpu_id': 0, \n        'predictor': 'gpu_predictor', \n        'n_estimators': 10000, \n        'learning_rate': 0.01063045229441343, \n        'gamma': 0.24652519525750877, \n        'max_depth': 4, \n        'seed': 42,       \n        'min_child_weight': 366, \n        'subsample': 0.6423040816299684, \n        'colsample_bytree': 0.7751264493218339, \n        'colsample_bylevel': 0.8675692743597421, \n        'use_label_encoder': False,\n        'lambda': 0, \n        'alpha': 10\n    }\n    \n    xgb_model = XGBClassifier(**xgb_params)\n    xgb_model.fit(x_train, y_train, eval_set=[(x_valid, y_valid)], verbose=False)\n    \n    preds_train = xgb_model.predict_proba(x_train)[:,1]\n    preds_valid = xgb_model.predict_proba(x_valid)[:,1]\n    auc_train = roc_auc_score(y_train, preds_train)\n    auc = roc_auc_score(y_valid, preds_valid)\n    print(\"Fold\",fold,\", train:\", f\"{auc_train:.6f}\", \", valid:\", f\"{auc:.6f}\")\n    scores.append(auc)\n    \n    preds_test = xgb_model.predict_proba(x_test)[:,1]\n    final_test_predictions.append(preds_test)\n    \n    \nprint(\"AVG AUC:\",np.mean(scores))","bae7e1d4":"shap_values = shap.TreeExplainer(xgb_model).shap_values(x_valid)\nshap.summary_plot(shap_values, x_valid)","790e364b":"shap.dependence_plot(\"lr\", shap_values, x_valid)","4ac89b1b":"idx = 5\ndata_for_prediction = x_valid.iloc[idx]\ndata_for_prediction_array = data_for_prediction.values.reshape(1, -1)\n\n\nprint(xgb_model.predict_proba(data_for_prediction_array))\n\nshap.initjs()\nexplainer = shap.TreeExplainer(xgb_model)\nshap_values = explainer.shap_values(data_for_prediction_array)\nshap.force_plot(explainer.expected_value, shap_values, data_for_prediction)","d270293c":"shap.decision_plot(explainer.expected_value, shap_values, data_for_prediction)","11d79768":"plt.figure(figsize=(15,8))\nsns.histplot(x=np.mean(np.column_stack(final_test_predictions), axis=1), kde=True, color=\"blue\")\nplt.title(\"Predictions Distribution\")\nplt.xlabel(\"Prediction\")\nplt.show()","93c9cc18":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"submission.csv\", index=False)","5f859181":"# Plot Data","b2c99f41":"# Load Data","2b8f9b83":"# PCA","5be44540":"# Submission","292b211b":"# Mutual Information","519ed60b":"# KMeans","268d3abe":"# Adding New Features?","29fc11f8":"# Scale Data","e75ecc7e":"# XGBoost","b4da620b":"# Plot Prediction","4cea48a7":"# SHAP Values","6f8b6991":"# Logistic Regression"}}