{"cell_type":{"4165d349":"code","4fa76600":"code","c42b4175":"code","738f0d9b":"code","3678bb8a":"code","48dd15fe":"code","4d77bbdb":"code","4a63cb87":"code","0a4123e6":"code","5cdb14b0":"code","223afed0":"code","d3bf2935":"code","0e87c2ff":"code","1c691e87":"code","5233a897":"code","d42caf98":"code","da3fa115":"code","4e3f7b20":"code","6b99df34":"code","f3c3146a":"code","bd5d9d53":"code","c02f9f72":"code","4b781919":"code","9f1cfe7b":"code","2c68b8a6":"code","aceb1e3b":"code","5d2c33ac":"code","7b3e85b6":"code","3266fc1e":"code","e9b5909f":"code","c9020331":"code","9a53ba82":"code","d4a4dc83":"code","d95f33da":"code","6d3ee543":"code","5a83dd0a":"code","9811310b":"code","298f9c34":"code","5fe4f769":"code","63cad634":"code","e60dc0c6":"code","7bffbc0c":"code","35aa03a9":"code","08790c3f":"code","a3de518b":"code","dc4bc249":"code","9b915344":"code","b59b2ca9":"code","ef6cb227":"code","3c117c48":"code","d2f2a5be":"code","533aa5c9":"code","50675bd7":"code","6675bfe9":"code","1b2ca1a9":"code","4ce886e8":"code","052e061f":"code","37626750":"code","ea75f399":"code","5975b8ea":"code","1f204cb7":"code","e9657f1c":"code","d9b33698":"code","0ef061e9":"code","715156de":"code","99bb0b12":"code","985d00d2":"code","47fcc825":"code","7dceb545":"code","4cd4bef9":"code","6639f75a":"code","93f67d7f":"code","a715a456":"code","99ab3b01":"code","e5d24562":"code","98825971":"code","bc847f10":"code","e0d29629":"code","48d969be":"code","64c730ce":"code","89090396":"code","8785644f":"code","61b16455":"code","a3583548":"markdown","97778d68":"markdown","f9f91e87":"markdown","5c9f3429":"markdown","69e6c6d9":"markdown","928a0f24":"markdown","317de433":"markdown","75066fbc":"markdown","61a25f19":"markdown","7e083c55":"markdown","09f95564":"markdown","28c83ac3":"markdown","d214acc9":"markdown","ad08b0a7":"markdown","f35b74bc":"markdown","7a327304":"markdown","965b01e0":"markdown","dd49298c":"markdown","6ec9c666":"markdown","8c532840":"markdown","e8431300":"markdown","4e028090":"markdown","efa3da88":"markdown","6a77ff9a":"markdown","b2b1cf7c":"markdown","a19c1a07":"markdown","d4751273":"markdown","c0d6d9f9":"markdown","820414d0":"markdown","395e3bfa":"markdown","79eecb91":"markdown","51e364a9":"markdown","b1b97a92":"markdown","d94583c1":"markdown","bd5a73aa":"markdown","80d6324b":"markdown","9677e264":"markdown","39f41bfc":"markdown","1f27ac30":"markdown","030ed712":"markdown","1523db5d":"markdown","ad99e403":"markdown","addda664":"markdown","48cdd87a":"markdown","a3493fe8":"markdown","463591d4":"markdown","be4013ad":"markdown","78d6961e":"markdown","0ff7ee39":"markdown","e63cf73c":"markdown","8017151a":"markdown","558c2957":"markdown","47a0fb40":"markdown","968a2abb":"markdown","8d8b03f6":"markdown","1bf5930a":"markdown","0151dfaf":"markdown","6a9e7fb7":"markdown","39bf48b9":"markdown"},"source":{"4165d349":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='whitegrid',color_codes =True)\nsns.set(font_scale=1)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning) \n","4fa76600":"from scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\nfrom sklearn import linear_model\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","c42b4175":"train_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')","738f0d9b":"test_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","3678bb8a":"train_df.head()\n","48dd15fe":"train_df.info()","4d77bbdb":"train_df.get_dtype_counts()","4a63cb87":"train_df.describe()","0a4123e6":"\nplt.figure(figsize=(8,8))\nplt.scatter(x = train_df['GrLivArea'], y = train_df['SalePrice'],color='yellowgreen',edgecolor ='white')\n\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","5cdb14b0":"plt.figure(figsize=(7,7))\nsns.distplot(train_df['SalePrice'],color = 'hotpink',hist = 25)","223afed0":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","d3bf2935":"#Need to remove Outliers\ntrain_df = train_df[train_df.GrLivArea < 4500]\ntrain_df.reset_index(drop=True, inplace=True)\ntrain_df[\"SalePrice\"] = np.log1p(train_df[\"SalePrice\"])\ny = train_df['SalePrice'].reset_index(drop=True)","0e87c2ff":"train_df.shape","1c691e87":"#skewness and kurtosis\nprint(\"Skewness: %f\" % train_df['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % train_df['SalePrice'].kurt())","5233a897":"plt.figure(figsize=(8,8))\nsns.distplot(train_df['SalePrice'],color='chartreuse',hist=20)\n#stats.probplot(train_df['SalePrice'],plot=plt)","d42caf98":"train_df['SalePrice'].shape","da3fa115":"corre = train_df.corr()['SalePrice']\ncorre[np.argsort(corre,axis=0)[::-1]]","4e3f7b20":"num_feat=train_df.columns[train_df.dtypes!=object]\nnum_feat=num_feat[1:-1] \nlabels = []\nvalues = []\nfor col in num_feat:\n    labels.append(col)\n    values.append(np.corrcoef(train_df[col].values, train_df.SalePrice.values)[0,1])\n    \nind = np.arange(len(labels))\nwidth = 0.9\nfig, ax = plt.subplots(figsize=(7,10))\nrects = ax.barh(ind, np.array(values), color='magenta')\nax.set_yticks(ind+((width)\/2.))\nax.set_yticklabels(labels, rotation='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Sale Price\");","6b99df34":"correlations=train_df.corr()\nattrs = correlations.iloc[:-1,:-1] # all except target\n\nthreshold = 0.5\nimportant_corrs = (attrs[abs(attrs) > threshold][attrs != 1.0]) \\\n    .unstack().dropna().to_dict()\n\nunique_important_corrs = pd.DataFrame(\n    list(set([(tuple(sorted(key)), important_corrs[key]) \\\n    for key in important_corrs])), \n        columns=['Attribute Pair', 'Correlation'])\n\n    # sorted by absolute value\nunique_important_corrs = unique_important_corrs.ix[\n    abs(unique_important_corrs['Correlation']).argsort()[::-1]]\n\nunique_important_corrs","f3c3146a":"corremat = train_df.corr()\ntop_corr_features = corremat.index[(corremat['SalePrice'])> 0.5]\nplt.figure(figsize=(10,10))\ng= sns.heatmap(train_df[top_corr_features].corr(),annot=True,cmap='viridis',linewidths=.5)","bd5d9d53":"most_corr = pd.DataFrame(top_corr_features)\nmost_corr.columns = ['Most Correlated Features']\n\nmost_corr","c02f9f72":"# Overall Quality vs Sale Price\ndata = pd.concat([train_df['SalePrice'], train_df['OverallQual']], axis=1)\nf, ax = plt.subplots(figsize=(8, 8))\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data,palette=\"Greens\")\n","4b781919":"plt.figure(figsize=(8,8))\nsns.distplot(train_df[\"YearBuilt\"],color='darkorchid', kde=False,hist=10)","9f1cfe7b":"plt.figure(figsize=(8,8))\nsns.distplot(train_df[\"YearRemodAdd\"].astype(int),color='lime', kde=False);","2c68b8a6":"# Total Rooms vs Sale Price\nplt.figure(figsize=(10,10))\nsns.jointplot(x=train_df['YearRemodAdd'], y=train_df['SalePrice'],kind='reg',\n              height=8,color= 'mediumvioletred')","aceb1e3b":"# Basement Area vs Sale Price\nsns.jointplot(x=train_df['TotalBsmtSF'], y=train_df['SalePrice'], kind='reg',color='darkorange',\n             height=8)\n","5d2c33ac":"# First Floor Area vs Sale Price\nsns.jointplot(x=train_df['1stFlrSF'], y=train_df['SalePrice'], kind='reg',color='crimson',height=8)","7b3e85b6":"# Living Area vs Sale Price\nsns.jointplot(x=train_df['GrLivArea'], y=train_df['SalePrice'], kind='reg',color='chartreuse',height=8)","3266fc1e":"# Total Rooms vs Sale Price\nplt.figure(figsize=(8,8))\nsns.boxplot(x=train_df['TotRmsAbvGrd'], y=train_df['SalePrice'],color='fuchsia')","e9b5909f":"upperlimit = np.percentile(train_df.SalePrice.values, 99.5)\ntrain_df['SalePrice'].ix[train_df['SalePrice']>upperlimit] = upperlimit\nplt.figure(figsize=(10,7))\nplt.scatter(range(train_df.shape[0]), train_df[\"SalePrice\"].values,color='orangered',edgecolor='white')\nplt.title(\"Distribution of Sale Price\")\nplt.xlabel(\"Number of Occurences\")\nplt.ylabel(\"Sale Price\");","c9020331":"# Garage Area vs Sale Price\nplt.figure(figsize=(8,7))\nsns.violinplot(x=train_df['GarageCars'], y=train_df['SalePrice'],\n               palette=['fuchsia','red','yellow','lime','blue'])\n","9a53ba82":"# Removing outliers \ntrain_df = train_df.drop(train_df[(train_df['GarageCars']>3) \n                         & (train_df['SalePrice']<350000)].index).reset_index(drop=True)","d4a4dc83":"# Garage Area vs Sale Price after removing Outliers\nplt.figure(figsize=(8,7))\nsns.violinplot(x=train_df['GarageCars'], y=train_df['SalePrice'],\n               palette=['fuchsia','red','yellow','lime'])","d95f33da":"train_df.shape","6d3ee543":"# Garage Area vs Sale Price\nplt.figure(figsize=(9,6))\nsns.scatterplot(x=train_df['GarageArea'], y=train_df['SalePrice'],color='indigo',edgecolor='white')","5a83dd0a":"deploy_df =train_df[['OverallQual' ,'YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF','GrLivArea','FullBath',\n                     'TotRmsAbvGrd','GarageYrBlt','GarageCars','GarageArea','SalePrice']]","9811310b":"deploy_df","298f9c34":"# It seems we have nulls so we will use the imputer strategy later on.\nMissing = pd.DataFrame([deploy_df.isnull().sum()], ['train'])\nMissing[Missing.sum(axis=1) > 0]","5fe4f769":"deploy_df.select_dtypes(include=['object']).columns\n","63cad634":"deploy_df.select_dtypes(exclude=['object']).columns","e60dc0c6":"Categorical = len(deploy_df.select_dtypes(include=['object']).columns)\nnumerical = len(deploy_df.select_dtypes(exclude=['object']).columns)\nprint('Categorical:',Categorical)\nprint('Numerical:',numerical)","7bffbc0c":"corre = deploy_df.corr()['SalePrice']\ncorre[np.argsort(corre,axis=0)[::-1]]","35aa03a9":"corremat = deploy_df.corr()\ntop_corr_features = corremat.index[(corremat['SalePrice'])> 0.5]\nplt.figure(figsize=(10,10))\ng= sns.heatmap(deploy_df[top_corr_features].corr(),annot=True,cmap='viridis')","08790c3f":"target_feature = deploy_df['SalePrice']","a3de518b":"train_feature = deploy_df.drop(['SalePrice'], axis=1).reset_index(drop=True)\n#test_features = test_df\n#total_features = pd.concat([train_features, test_features]).reset_index(drop=True)","dc4bc249":"train_feature.head(3)","9b915344":"## Missing data in GarageYrBit most probably means missing Garage , so replace NaN with zero . \ntrain_feature['GarageYrBlt'] = train_feature['GarageYrBlt'].fillna(0)","b59b2ca9":"train_feature.isnull().sum()","ef6cb227":"#######################################################################################\n# Adding new features . Make sure that you understand this. \n#train_feature['YrBltAndRemod']=train_feature['YearBuilt']+train_feature['YearRemodAdd']\n#train_feature['TotalSF']=train_feature['TotalBsmtSF'] + train_feature['1stFlrSF'] \n########################################################################################","3c117c48":"train_feature.shape","d2f2a5be":"numeric_feats = train_feature.dtypes[train_feature.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = train_feature[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","533aa5c9":"X_train = train_feature ","50675bd7":"y_train = target_feature","6675bfe9":"y_train","1b2ca1a9":"X_train.shape,y_train.shape","4ce886e8":"X_train.head(4)","052e061f":"#Train the model\n\nlin_model = linear_model.LinearRegression()","37626750":"#Fit the model\nlin_model.fit(X_train, y_train)","ea75f399":"linear_accuracy = round(lin_model.score(X_train,y_train)*100,2)\nprint(round(linear_accuracy,2),'%')","5975b8ea":"#Train the model\n\nrandom_model = RandomForestRegressor(n_estimators=1000)","1f204cb7":"#Fit\nrandom_model.fit(X_train, y_train)","e9657f1c":"random_accuracy = round(random_model.score(X_train,y_train)*100,2)\nprint(round(random_accuracy,2),'%')","d9b33698":"#Train the model\n\nGBR_model = GradientBoostingRegressor(n_estimators=100, max_depth=4)","0ef061e9":"#Fit\nGBR_model.fit(X_train, y_train)","715156de":"GBR_accuracy = round(GBR_model.score(X_train,y_train)*100,2)\nprint(round(GBR_accuracy,2),'%')","99bb0b12":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","985d00d2":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train.values, y_train.values, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","47fcc825":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))","7dceb545":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\n","4cd4bef9":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))","6639f75a":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","93f67d7f":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","a715a456":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","99ab3b01":"score_lasso = rmsle_cv(lasso)\nprint(\"\\nLasso score: {:.4f} ({:.4f})\\n\".format(score_lasso.mean(), score_lasso.std()))","e5d24562":"score_Enet = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score_Enet.mean(), score_Enet.std()))","98825971":"score_KRR = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score_KRR.mean(), score_KRR.std()))","bc847f10":"score_GBoost = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score_GBoost.mean(), score_GBoost.std()))","e0d29629":"score_xgb = rmsle_cv(model_xgb)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score_xgb.mean(), score_xgb.std()))","48d969be":"score_lgb = rmsle_cv(model_lgb)\nprint(\"LGBM score: {:.4f} ({:.4f})\\n\" .format(score_lgb.mean(), score_lgb.std()))","64c730ce":"y_train =np.expm1(y_train)","89090396":"model_lgb.fit(X_train,y_train)","8785644f":"y_pred = model_lgb.predict(X_train)","61b16455":"y_pred","a3583548":"In ensemble learning theory, we call weak learners (or base models) models that can be used as building blocks for designing more complex models by combining several of them","97778d68":"Cross Validation is used to assess the predictive performance of the models and and to judge how they perform outside the sample to a new data set also known as test data\nThe motivation to use cross validation techniques is that when we fit a model, we are fitting it to a training dataset. Without cross validation we only have information on how does our model perform to our in-sample data. Ideally we would like to see how does the model perform when we have a new data in terms of accuracy of its predictions. In science, theories are judged by its predictive performance.  \n\nWe use the cross_val_score function of Sklearn. However this function has not a shuffle attribut, we add then one line of code, in order to shuffle the dataset prior to cross-validation","f9f91e87":"SalePrice is not uniformly distributed and is skewed towards the right . Therefore , we use log1p to remove the skewness .","5c9f3429":"**Define a cross validation strategy**","69e6c6d9":"Remodeling have very slight impact on sales price","928a0f24":"### **bagging, boosting and stacking**","317de433":"**Correlation Analysis**","75066fbc":"Base models scores <br>\nLet's see how these base models perform on the data by evaluating the cross-validation rmsle error","61a25f19":"For missing values in numerical cols , we fillNa with 0.","7e083c55":"Please not Outliers extreme right down","09f95564":"**Introduction:** <br>\nThe variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home. Great for practicing skills such as:\n\nCreative feature engineering\nAdvanced regression techniques like random forest and gradient boosting","28c83ac3":"### Most correlated Features","d214acc9":"\n2. Replacing With Mean\/Median\/Mode \n<br>\nThis strategy can be applied on a feature which has numeric data ","ad08b0a7":"**We need to check below assumtions:**\n\n**Normality** - A normal distribution is an arrangement of a data set in which most values cluster in the middle of the range and the rest taper off symmetrically toward either extreme.\n\nA graphical representation of a normal distribution is sometimes called a bell curve because of its flared shape.\n\n**Homoscedasticity** - This assumption means that the variance around the regression line is the same for all values of the predictor variable\n\n**Linearity**- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n**Absence of correlated errors(Multicollineraty)** - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.","f35b74bc":"1. Delete Row\n<br>\nThis method commonly used to handle the null values. Here, we either delete a particular row if it has a null value for a particular feature and a particular column if it has more than 70-75% of missing values.\n\n \n","7a327304":"### **Regularization Models**<br>\nWhat makes regression model more effective is its ability of regularizing. The term \"regularizing\" stands for models ability to structurally prevent overfitting by imposing a penalty on the coefficients.<br>\n\nThere are three types of regularizations.<br>\n\nRidge <br>\nLasso <br>\nElastic Net <br>\nThese regularization methods work by penalizing the magnitude of the coefficients of features and at the same time minimizing the error between the predicted value and actual observed values. This minimization becomes a balance between the error (the difference between the predicted value and observed value) and the size of the coefficients. The only difference between Ridge and Lasso is the way they penalize the coefficients. Elastic Net is the combination of these two. Elastic Net adds both the sum of the squares errors and the absolute value of the squared error. ","965b01e0":"### **visualization**","dd49298c":"5. Using Algorithms Which Support Missing Values\n<br>\nKNN is a machine learning algorithm which works on the principle of distance measure.\n<br>\nRef:     https:\/\/analyticsindiamag.com\/5-ways-handle-missing-values-machine-learning-datasets\/","6ec9c666":"**Analyse Target variable**","8c532840":"Multicollinearity can handle by following method.<br>\n1.Completely remove those variables <br>\n2.Make new feature by adding them or by some other operation.<br>\n3.Use PCA, which will reduce feature set to small number of non-collinear features.\n\n\n\n","e8431300":"Although it seems like house prices decrease with age, we can't be entirely sure. Is it because of inflation or stock market crashes? Let's leave the years alone.","4e028090":"Sales price increase based on Basement area","efa3da88":"### **Feature Engineering**","6a77ff9a":"### LightGBM :\nLight GBM is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithm, used for ranking, classification and many other machine learning tasks.","b2b1cf7c":"4. Predicting The Missing Values\n<br>\nUsing the features which do not have missing values, we can predict the nulls with the help of a machine learning algorithm","a19c1a07":"So what can we see till now.<br>\nwe have total 81 variables for train and 80 for test variable<br>\nwe don't have SalePrice variable for test variable because this will be our task to infer SalePrice for test set by learning from train set.<br>\nSo SalePrice is our target variable and rest of the variables are our predictor variables.","d4751273":"### This high accuracy may be misleading because of multicollinearity,letus consider that as well.","c0d6d9f9":"GradientBoostingRegressor","820414d0":"### Distribution of Target value","395e3bfa":"### Gradient Boosting Regression :\n\nGradient boosting is a  technique for regression and classification problems, which produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees. It builds the model in a stage-wise fashion like other boosting methods do\n","79eecb91":"1. Import Libraries\n1. Load Data\n1. Analyse Target variable\n1. Correlation Analysis\n1. Multicollinearity\n1. Most correlated Features\n1. Visulization\n1. Handling Missing value\n1. Filling missing value\n1. Feature Engineering     <br>\n   1.Box Cox Transformation<br>\n   2.Cross validation<br>\n   3.Regularization Model<br>\n   4.Boosting<br>\n   5.Stacking<br>\n1. Final Training and Prediction\n1. Submission","51e364a9":"**Plotting Correlation**","b1b97a92":"**Skewedness and Kurtosis: **<br>\nSkewedness:\n\nA skewness of zero or near zero indicates a symmetric distribution. A negative value for the skewness indicate a left skewness (tail to the left) A positive value for te skewness indicate a right skewness (tail to the right) Skewness\nis the degree of distortion from the symmetrical bell curve or the normal curve. So, a symmetrical distribution will have a skewness of \"0\". There are two types of Skewness: Positive and Negative. Positive Skewness(similar to our target variable distribution) means the tail on the right side of the distribution is longer and fatter. In positive Skewness the mean and median will be greater than the mode. This is similar to this dataset. \n\nKourtosis is a measure of how extreme observations are in a dataset. The greater the kurtosis coefficient , the more peaked the distribution around the mean .<br>\n\n\nThere are three types of Kurtosis: Mesokurtic, Leptokurtic and Platykurtic.<br>\nMesokurtic is similar to normal curve with the standard value of 3. This means that the extreme values of this distrubution is similar to that of a normal distribution.<br>\nLeptokurtic Example of leptokurtic distributions are the T-distributions with small degrees of freedom.<br>\nPlatykurtic: Platykurtic describes a particular statistical distribution with thinner tails than a normal distribution. Because this distribution has thin tails, it has fewer outliers (e.g., extreme values three or more standard deviations from the mean) than do mesokurtic and leptokurtic distributions.","d94583c1":" Handling the missing values is one of the greatest challenges faced by analysts, because making the right decision on how to handle it generates robust data models. Let us look at different ways of imputing the missing values.","bd5a73aa":"**This model has been deployed.Please find deployed URL at the end of this Notebook**","80d6324b":"Observation.\nAs we can see, the multicollinearity still exists in various features. However, we will keep them for now for the sake of learning and lets models(e.x. Regularization models such as Lasso, Ridge) do the clean up later on. Let's go through some of the correlations that still exists.<br>\n\nThere is 0.83 or 83% correlation between GarageYrBlt and YearBuilt.<br>\n83% correlation between TotRmsAbvGrd and GrLivArea.<br>\n89% correlation between GarageCars and GarageArea.<br>\nSimilarly many other features such asBsmtUnfSF, FullBath have good correlation with other independent feature but not so much with the dependent feature.\n we will be using many algorithms as scikit learn modules makes it easy to implement them and get the best possible model. Therefore, we will keep all the features for now.","9677e264":"**No or Little multicollinearity:**<br> Multicollinearity is when there is a strong correlation between independent variables. Linear regression or multilinear regression requires independent variables to have little or no similar features. Multicollinearity can lead to a variety of problems, including:\n\nThe effect of predictor variables estimated by our regression will depend on what other variables are included in our model.<br>\nPredictors can have wildly different results depending on the observations in our sample, and small changes in samples can result in very different estimated effects.<br>\nWith very high multicollinearity, the inverse matrix, the computer calculates may not be accurate.<br>\nWe can no longer interpret a coefficient on a variable as the effect on the target of a one-unit increase in that variable holding the other variables constant. The reason behind that is, when predictors are strongly correlated, there is not a scenario in which one variable can change without a conditional change in another variable.","39f41bfc":"**Linear Regression**","1f27ac30":"We need to remove the outliers the Garage area 4 as price is showing less compared to others","030ed712":"3. Assigning An Unique Category\n<br>\nA categorical feature will have a definite number of possibilities.This strategy will add more information into the dataset which will result in the change of variance. Since they are categorical, we need to find one hot encoding to convert it to a numeric form for the algorithm to understand it","1523db5d":"Suggetions to improve this kernal are always Welcome.\nIf you found this notebook helpful or you just liked it,some \ud83d\udc4dUpvotes\ud83d\udc4d would be very much appreciated \ud83d\ude0a","ad99e403":"we can say that bagging will mainly focus at getting an ensemble model with less variance than its components whereas boosting and stacking will mainly try to produce strong models less biased than their components (even if variance can also be reduced).","addda664":"## **Handling Missing Value**","48cdd87a":"### Kernel Ridge Regression :\n\nIt is essential to standardize the predictor variables before constructing the models.\nIt is important to check for multicollinearity,","a3493fe8":"Sales price increase based on OverallQual increases.","463591d4":"**Import Libraries**","be4013ad":"Not highly skewed","78d6961e":"**Unique importance of Correlation**","0ff7ee39":"Now have a list of strongly correlated values but this list is incomplete as we know that correlation is affected by outliers. So we could proceed as follow:\n\nPlot the numerical features and see which ones have very few or explainable outliers\nRemove the outliers from these features and see which one can have a good correlation without their outliers\nBtw, correlation by itself does not always explain the relationship between data so ploting them could even lead us to new insights and in the same manner, check that our correlated values have a linear relationship to the SalePrice.\n","e63cf73c":"### Elastic Net Regression :\n\nElastic net is always preferred over lasso & ridge regression because it solves the limitations of both methods, while also including each as special cases. So if the ridge or lasso solution is, indeed, the best, then any good model selection routine will identify that as part of the modeling process.","8017151a":"### XGBoost :\n\nXGBoost is an algorithm that has recently been dominating applied machine learning  for structured or tabular data.\n\nXGBoost is an implementation of gradient boosted decision trees designed for speed and performance.It is a more advanced version of the gradient boosting method. The main aim of this algorithm is to increase speed and to increase the efficiency of your competitions","558c2957":"Shows Positive skewness","47a0fb40":"#### Deployed Link   \u2af8  \nhttps:\/\/mlprojectdeployed.blogspot.com\/","968a2abb":"This shows multicollinearity.<br>\nIn regression, \"multicollinearity\" refers to predictors that are correlated with other predictors.  Multicollinearity occurs when your model includes multiple factors that are correlated not just to your response variable, but also to each other. In other words, it results when you have factors that are a bit redundant.","8d8b03f6":"### Categorical &Numerical Data","1bf5930a":"We can mention three major kinds of meta-algorithms that aims at combining weak learners:<br>\nbagging, that often considers homogeneous weak learners, learns them independently from each other in parallel and combines them following some kind of deterministic averaging process.<br>\nboosting, that often considers homogeneous weak learners, learns them sequentially in a very adaptative way (a base model depends on the previous ones) and combines them following a deterministic strategy.<br>\nstacking, that often considers heterogeneous weak learners, learns them in parallel and combines them by training a meta-model to output a prediction based on the different weak models predictions","0151dfaf":"We are considering only 11  features which are more correlated with sales price.","6a9e7fb7":"\n\nLet's take a look at how each relates to Sale Price and do some pre-cleaning on each feature if necessary.\n\n1.OverallQual: Rates the overall material and finish of the house (1 = Very Poor, 10 = Very Excellent)<br>\n2\tYearBuilt: Original construction date<br>\n3\tYearRemodAdd :Remodel date<br>\n4\tTotalBsmtSF: Total square feet of basement area<br>\n5\t1stFlrSF: First Floor square feet<br>\n6\tGrLivArea: Above grade (ground) living area square feet<br>\n7\tFullBath: Full bathrooms above grade<br>\n8\tTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)<br>\n9\tGarageCars: Size of garage in car capacity<br>\n10\tGarageArea: Size of garage in square feet<br>\n11\tSalePrice\n\n\n","39bf48b9":"### **LASSO Regression **\n\nLasso regression is a type of linear regression that uses shrinkage. Shrinkage is where data values are shrunk towards a central point, like the mean. The lasso procedure encourages simple, sparse models (i.e. models with fewer parameters). This particular type of regression is well-suited for models showing high levels of muticollinearity or when you want to automate certain parts of model selection, like variable selection\/parameter elimination.\n\nThe acronym \u201cLASSO\u201d stands for Least Absolute Shrinkage and Selection Operator."}}