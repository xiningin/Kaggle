{"cell_type":{"88e846bc":"code","26b26aaa":"code","9d12bfac":"code","7217e5d8":"code","dab9e169":"code","e3ecbdf7":"code","9962ee0b":"code","ff1a9683":"code","8e6647c0":"code","1130bc39":"code","e75366ff":"code","38fff484":"code","74759a53":"code","b4c283b1":"code","2752c318":"code","3a855d54":"code","ca94930c":"code","da219a2b":"code","c29bb2f8":"code","3913a9c8":"code","95b090c4":"code","514b22c9":"code","9b51cce4":"code","e5cf189e":"code","2fbc57e5":"code","7bd29383":"markdown","05780389":"markdown","c79c4c89":"markdown","56cb7c33":"markdown","f89dbe86":"markdown","05fa5ce6":"markdown","840b337c":"markdown","8e8d89e3":"markdown","eaeecf2a":"markdown","0b1e4bdf":"markdown"},"source":{"88e846bc":"import tensorflow as tf\nfrom tensorflow.keras.layers import Layer\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.activations import relu,sigmoid, softmax, tanh\nfrom tensorflow.keras.initializers import glorot_normal\n\n\n\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","26b26aaa":"class CustomDense(Layer):\n    '''\n    Build custom layer mimicks the Dense Layer\n    '''\n    def __init__(self,units=128,initializer='glorot_uniform',activation=None,name=None,**kwargs):\n        '''\n        Constructor of the class\n        args:\n            units: {int} number of neurons in the layer\n            initializer: {string or callable instance of tf.keras.initializers } initial weights\n            activation: {callable instance of tf.keras.activations} activation function to use\n            name: name of the layer {string}. There is already a name param in Base Class\n            kwargs: keyword arguments of the base Layer Class\n        '''\n        super(CustomDense,self).__init__(**kwargs) # constructor of Base Layer class\n        self.units = units # number of neurons\n        self.activation = activation # activation function\n        self.initializer = initializer # initializer\n        if name: # it works but but you SHOUD NOT USE IT\n            self._name = name\n        \n    def build(self,input_shape):\n        '''\n        method typically used to create the weights of Layer subclasses. During forward pass or call() model\n        will automatically call the build method to get the shape of the input tensor\n        args:\n            input_shape: a tensor describing shape of input. it'll be passed automatically as input.shape\n        '''\n        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer=self.initializer,trainable=True)\n        # add_weight is a method of Layer base class. input_shape[-1] gives number of features\n        \n        self.b = self.add_weight(shape=(self.units,),initializer=self.initializer,trainable=True)\n        # add bias and set to trainable. NOTE: never forget to add a , after self.units as\n        # (1) == int(1) but (1,) = tuple([1])\n\n        \n    def call(self,input_tensor):\n        '''\n        method to implement the forward pass\n        args:\n            input: input tensor\n        '''\n        result = tf.matmul(input_tensor,self.w)+self.b \n        # apply the formula y = wx + b in matric multiplication form\n        \n        if self.activation:\n            result = self.activation(result) # apply activation function\n            \n        return result","9d12bfac":"layer = CustomDense(8,activation=softmax,name='my_layer')\n\n# below code is to just create random tensor which will work as the input to the Layer\nini = tf.random_uniform_initializer()\ntensor = ini((4,3)) # 4 data points with 3 attributes\ntensor","7217e5d8":"print(layer(tensor)[0].numpy().sum())  \n# to check if our model is outputting correct sigmoid as sum==1 for each row\nlayer(tensor)","dab9e169":"layer.weights # weights and biases. we can access layer.b, layer.w individually too","e3ecbdf7":"layer.name  # layer._name works too. Not a Private variable as there are no private variables in Python","9962ee0b":"class HypotheticalLayer(Layer):\n    '''\n    Build a random layer\n    '''\n    def __init__(self,units=4,initializer='glorot_uniform',drop_rate=0.23,**kwargs):\n        super(HypotheticalLayer,self).__init__(**kwargs) # constructor of Base Layer class\n        self.units = units # number of neurons\n        self.initializer = initializer # initializer\n        self.drop_rate = drop_rate\n        \n        \n    def build(self,input_shape):\n        \n        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer=self.initializer,trainable=True)\n        \n        self.b = self.add_weight(shape=(self.units,),initializer=self.initializer,trainable=True)\n   \n        \n    def call(self,input_tensor,training=False):\n        '''\n        method to implement the forward pass with training parm\n        args:\n            input: input tensor\n            training: {bool} whether the layer is in training phase or not\n        '''\n        result = tf.matmul(input_tensor,self.w)+self.b \n        \n        if training:\n            result = tf.nn.dropout(result,rate=self.drop_rate) # apply dropout\n            \n        return result","ff1a9683":"hypo_layer = HypotheticalLayer(drop_rate=0.33)\nhypo_layer(tensor,training=True) # uses the call method. Apply dropout randomly to 33%","8e6647c0":"hypo_layer(tensor) # default training=False","1130bc39":"class NestedDense(Layer):\n    '''\n    Build nested layer made of multiple CustomDense layer\n    '''\n    def __init__(self,l1_unit=8,l2_unit=8,l2_units=4,**kwargs):\n        '''\n        Constructor of the class\n        args:\n            l1_units, l2_units, l3_units = no of neurons in the 3 sub layers\n            kwargs: keyword arguments of the base Layer Class\n        '''\n        super(NestedDense,self).__init__(**kwargs) # constructor of Base Layer class\n        \n        self.l1 = CustomDense(l1_unit,initializer=glorot_normal) # no activation, callable initializer\n        self.l2 = CustomDense(l2_units,initializer='ones',activation=relu) \n        # relu activation, he_uniform initializer\n        self.l3 = CustomDense(l1_unit) # no activation. default initializer\n\n        \n    def call(self,input_tensor):\n        '''\n        method to implement the forward pass\n        args:\n            input: input tensor\n        '''\n        x = self.l1(input_tensor) # layer 1 \n        x = tanh(x) # as we had no activation function in layer 1\n        \n        x = self.l2(x) # pass to layer 2. we have a default activation as relu\n        \n        x = self.l3(x) # for our hypothetical work, we do not need any activation for output\n            \n        return x","e75366ff":"nested_dense = NestedDense()\nnested_dense(tensor)","38fff484":"nested_dense.weights # layer has weights of all the layers","74759a53":"class SerializableLayer(Layer):\n    '''\n    Build custom layer mimicks the Dense Layer which can noe be serialised or saved\n    '''\n    def __init__(self,units=128,initializer='glorot_uniform',activation=None,**kwargs):\n        super(SerializableLayer,self).__init__(**kwargs) \n        self.units = units\n        self.activation = activation \n        self.initializer = initializer \n        \n    def build(self,input_shape):\n        \n        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer=self.initializer,trainable=True)\n       \n        self.b = self.add_weight(shape=(self.units,),initializer=self.initializer,trainable=True)\n        \n\n        \n    def call(self,input_tensor):\n        '''\n        method to implement the forward pass\n        args:\n            input: input tensor\n        '''\n        result = tf.matmul(input_tensor,self.w)+self.b \n        \n        if self.activation:\n            result = self.activation(result)\n        \n        return result\n            \n            \n    def get_config(self):\n        '''\n        Get the configuration of the layer so that you can serialise the layer for further use\n        '''\n        config = super(SerializableLayer,self).get_config() # get the configuration from the Base Layer\n        config.update({'units':self.units,'initializer':self.initializer})\n        return config","b4c283b1":"layer = SerializableLayer(units=12,**{'name':'s_l_default'}) \n# see the working of name. this 'name' parameter is from the base Layer class.\nprint(layer.name)\nconfig = layer.get_config()\nprint(config)","2752c318":"new_layer = SerializableLayer.from_config(config) # make an exact copy of the above layer\nnew_layer.get_config()","3a855d54":"class ClassificationModel(Model):\n    '''\n    A model that performs simple classification where each input can belong to Only 1 class\n    '''\n    def __init__(self,input_shape,num_dense_layers=1,units=8,classes=2,activation='relu',**kwargs):\n        '''\n        Constructor of the class to get initial arguments\n        args:\n            input_shape = {tuple} shape of incoming data\n            num_dense_layers: {int} number of dense layers to include in the classification\n            units: {int} units to use for each layer\n            classes: {int} Number of classes either binary or multiclass\n            activation = {string\/callable} activation function to use\n        '''\n        super(ClassificationModel,self).__init__(**kwargs)\n        \n        assert num_dense_layers>=1 , \"number of layers must be >=1\"\n        assert units>=1 , \"units must be >=1\"\n        assert classes>=2, \"classes must be >=2\"\n        \n        self.in_shape = input_shape\n        self.n_dense = num_dense_layers\n        self.units = units\n        self.classes = classes\n        self.activation = activation\n        \n        \n        self.in_layer = tf.keras.layers.Dense(self.units,input_shape=self.in_shape,\n                                          activation=self.activation,kernel_initializer='glorot_uniform',)\n        # input layer has Input_shape param\n        \n        \n        self.middle_layers = [] # middle layers do not have Input_shape\n        for i in range(1,self.n_dense-1):\n            self.middle_layers.append(tf.keras.layers.Dense(self.units,activation=self.activation,\n                                                     kernel_initializer='glorot_uniform',))\n            \n        if self.classes == 2:\n            self.out_layer = tf.keras.layers.Dense(1,activation='sigmoid',\n                                                   kernel_initializer='glorot_uniform',)\n        else:\n            self.out_layer = tf.keras.layers.Dense(self.classes,activation='softmax',\n                                            kernel_initializer='glorot_uniform')\n            \n        \n    def call(self,tensor):\n        '''\n        Perform a forward pass operation\n        '''\n        x = self.in_layer(tensor)\n\n        for layer in self.middle_layers:\n            x = layer(x)\n\n        probs = self.out_layer(x)\n\n        return probs","ca94930c":"model = ClassificationModel(tensor.shape,3,8,2) # 3 layers with 8 units each and binary classification\nmodel.layers","da219a2b":"# model.summary() won't work without a fit() or build() method. we can pass in a tensor as a workaround\nprobs = model(tensor)\nmodel.summary()","c29bb2f8":"class FullyConnected(Layer):\n    '''\n    Fully Connected or Dense layer \n    '''\n    def __init__(self,units=16,w_init='he_uniform',b_init='zeros',activation=None,**kwargs):\n        '''\n        Constructor of the class\n        args:\n            units: {int} number of neurons to use\n            w_init = {string\/callable} weight initializer\n            b_init = {string\/callable\/None} bias initializer. None if no bias is included\n            activation: {string} activation function to use\n            **kwargs: {dict} keyword arg for the parent class\n        '''\n        super(FullyConnected,self).__init__(**kwargs)\n        self.units = units\n        self.w_init = w_init\n        self.b_init = b_init\n        self.activation = tf.keras.activations.get(activation) # gives a respective callable. None gives linear\n        \n        \n    def build(self,input_shape):\n        '''\n        Assign weights to the layer dynamically. Base layer method\n        '''\n        self.w = self.add_weight(shape=(input_shape[-1],self.units),initializer=self.w_init,trainable=True)\n        if self.b_init:\n            self.b = self.add_weight(shape=(self.units,),initializer=self.b_init,trainable=True)\n        \n    \n    def call(self,input_tensor):\n        '''\n        Forward Pass. Part of Base layer\n        '''\n        result = tf.matmul(input_tensor,self.w)\n        if self.b_init:\n            result = result + self.b\n            \n        if self.activation:\n            result = self.activation(result)\n        return result\n    \n    \n    def compute_output_shape(self,input_shape):\n        '''\n        Method of base class which computes the shape of output. \n        compute_output_shape is not needed unless the Layer is Dynamic\n        args:\n            input_shape: (tuple) shape of incoming tensor\n        out:\n            out_shape: (tuple)  shape of resulting tensor\n        '''\n        out_shape = list(input_shape) # because we can not append to tuple\n        out_shape[-1] = self.units # replace the incoming feature dimension to outgoing\n        return tuple(out_shape) # a tuple is needed for shape\n    \n    \n    def get_config(self):\n        config = super(FullyConnected,self).get_config() # get config of the base Layer class\n        \n        config.update({'units':self.units,'activation':tf.keras.activations.serialize(self.activation)})\n        # you need to serialise the callable activation function\n        \n        return config  ","3913a9c8":"class ClassificationModel(Model):\n    '''\n    A model that performs simple classification where each input can belong to Only 1 class\n    '''\n    def __init__(self,input_shape,layers_units=[8,],classes=2,activation='relu',**kwargs):\n        '''\n        Constructor of the class to get initial arguments\n        args:\n            input_shape = {tuple} shape of incoming data\n            layer_units: {list} units to use for each layer\n            classes: {int} Number of classes either binary or multiclass\n            activation = {string\/callable} activation function to use\n        '''\n        super(ClassificationModel,self).__init__(**kwargs)\n        \n        assert len(layers_units)>=1 , \"units must be >=1\"\n        assert classes>=2, \"classes must be >=2\"\n        \n        self.in_shape = input_shape\n        self.units = layers_units\n        self.classes = classes\n        self.activation = activation\n        \n        \n        self.in_layer = FullyConnected(self.units[0],activation=self.activation,\n                                       name='input_layer',\n                                       input_shape=self.in_shape)\n        # input_shape is a parameter of base class\n        \n        \n        self.middle_layers = [] # middle layers do not have Input_shape\n        for i in range(1,len(self.units)):\n            self.middle_layers.append(FullyConnected(self.units[i],activation=self.activation))\n            \n            \n        if self.classes == 2:\n            self.out_layer = FullyConnected(1,activation='sigmoid',name='output_layer')\n        else:\n            self.out_layer = FullyConnected(self.classes,activation='softmax',name='output_layer')\n            \n        \n    def call(self,tensor):\n        '''\n        Perform a forward pass operation\n        '''\n        x = self.in_layer(tensor)\n\n        for layer in self.middle_layers:\n            x = layer(x)\n\n        probs = self.out_layer(x)\n\n        return probs","95b090c4":"wine = load_wine()\nX = wine['data']\ny = wine['target']\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=13)\n","514b22c9":"model = ClassificationModel(input_shape=X_train.shape,layers_units=[64,32,32],classes=3,activation='relu')\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001),\n              loss='sparse_categorical_crossentropy',\n              metrics=['accuracy'])\n# you can also pass all the parameters as callable\n\nmodel.layers","9b51cce4":"history = model.fit(X_train,y_train,epochs=300,validation_data=(X_test,y_test),batch_size=32)","e5cf189e":"model.summary()","2fbc57e5":"plt.style.use('seaborn')\nf,ax = plt.subplots(1,2,figsize=(15,6))\n\nax[0].plot(history.history['val_loss'],label='Validation Loss')\nax[0].plot(history.history['loss'],label='Training Loss')\nax[0].set_title('Losses',weight='bold',size='x-large')\nax[0].set_xlabel('Epoch',size='large')\nax[0].set_ylabel('Loss',size='large')\nax[0].legend()\n\nax[1].plot(history.history['val_accuracy'],label='Validation Accuracy')\nax[1].plot(history.history['accuracy'],label='Training Accuracy')\nax[1].set_title('Accuracies',weight='bold',size='x-large')\nax[1].set_xlabel('Epoch',size='large')\nax[1].set_ylabel('Accuracy',size='large')\nax[1].legend()\n\nplt.show()","7bd29383":"## Privilaged argument in the `Call()`\nThere is a special argument  `training`  given for the call method. You can use the Boolean {True, False} for this option to use in Custom Layer to tell if the layer is working with Training or inference (results). For example, there are some methods where you can train differently and when you have to predict, a different strategy is used. For example you can dropout in the training.","05780389":"# From Keras to tf.keras and tf to tf 2.0: The Journey\n\nKeras could be seen as a what we call the `Wrapper` of either `tensorflow`, `theano` or `CNTK` before the release of `tensorflow 2.0` but it's definition has changed since the rlease of `tf 2.0`. Keras was not in active development for quite some time and was not compatible with `python > 3.5` but a week ago or so, they have released a new version which is now compatible with `python >= 3.6`. \n\n# The Change:\n\n`Tensorflow 2.0` is IMO, a complete shift as a lots of things have changed. I'd say that it has been for better specially for those who wanted to be somewhere between high level Keras for faster developments and  tensorflow for research and very high customization as `tf 2.0` provides both. So it won't be completly wrong to say that tf 2.0 and Keras are almost same today. We'll be looking at the aspect of building a custom Model and Layer using the tf 2.0 and Keras.\n\nAprt from that tensorflow has allowed the **Eager Execution** where you can see the results right away instad of building a static graph so that was an exclusiveness `PyTorch` **USED** to have. So it has become Build **and** run (how python works) instead of build **then** run (how c\/c++ works).","c79c4c89":"# End Notes\nThere is a lot more that the `tf 2.0` offers you while building custom layers and models including **metrics**, **losses** and much more inside the layers. Please refer more at the official documentation. Hope you enjoyed reading. Good Luck with your journey.","56cb7c33":"# Custom Models\nIn general, we will use the `Layer` class to define inner computation blocks, and will use the `Model` class to define the outer **model -- the object we will train.**\n\nFor instance, in a `ResNet50` model, we have **several ResNet blocks subclassing Layer, and a single Model encompassing the entire ResNet50 network.**\n\nThe Model class has the same API as Layer, with the following differences:\n\n1. It exposes built-in training, evaluation, and prediction loops (`model.fit()`, `model.evaluate()`, `model.predict()`).\n2. It exposes the list of its inner layers, via the `model.layers` property.\n3. It exposes saving and serialization APIs (`save()`, `save_weights()`...)\n\n-[from tf documentation](https:\/\/www.tensorflow.org\/guide\/keras\/custom_layers_and_models#the_model_class)","f89dbe86":"# Custom Layers","05fa5ce6":"## Layers\nBefore we start, we need to note some important facts about layers:\n\n1. Each layer uses the `tf.keras.layers.Layer` as a parent class.\n\n2. There are weights associated with layers. These weights can be trainable or non-trainable according to your requirement.\n\n3. Every layer is initialized with initial weights and it can be any of the `tf.keras.initializers` such as `zeros`, `normal`, `ones` and the most widely used `glorot_uniform`.\n\n4. Layers can be recursively combined (inside one another). So the outer layer will depend on the weights of the inner layer.\n\n5. We need to have the understanding of the shape of Input coming to the layer and number of output neurons it'll give as an output. We usually do no know the shape so we use a method `build` to get the shape while forward pass.\n\n6. A layer may or not have the `loss function` or the `Regularization`. If there are any transformations  implemented in the layers, these have to be executed inside the `call`  (for forward pass) method.\n\n7. There are few methods that we can choose to apply. `__init__`  and `call` is a must and using the `build` method before initializing the weights is the best practice to follow in dynamic environments. `grt_config` and `from_config` are used if we want to serialize our layers (for saving).","840b337c":"## Serialising Layers\nWhen we want to save the layer and use later, we need to use the `get_config()` from the **Parent** or Super Class. `get_config()` needs some configuration such as number of units or other parameters.","8e8d89e3":"## Nested Layers\nWe'll try to build a layer composed of multiple `CustomDense` layers with activation associated with each. \nJust for the demonstration purpose, we'll cover multiple scenario of how can we initialise our `CustomLayer`","eaeecf2a":"# Training Model\nYou can train the above model with `keras.Dense()` layer or you can add `FullyConnected()` to `keras.Sequential()\/keras.Model()` API too to check how is it performing. We'll train our custom model using our custom layer to classify the wine dataset. It is a multi (3) class classification. ","0b1e4bdf":"# Custom Model with Custom Layers\nNow, We'll build a Custom Model with custom Layers in it. We'll add a few given layers such as Dropout too to get an understanding that we can mix and match the layers."}}