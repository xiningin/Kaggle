{"cell_type":{"70ad776b":"code","da8e60ae":"code","29992412":"code","eb48890c":"code","d74ffce0":"code","0acb55c0":"code","c0fa342d":"code","2ffd3c9c":"code","dd01bd34":"code","9cbcfa04":"code","0c8bb640":"code","5ab3e5d2":"code","f5a509b7":"code","b8d3d020":"code","93849954":"code","c2ce013d":"code","e6bdab9d":"code","28d792dc":"code","fda6357c":"code","9c97649a":"code","71e320d9":"code","0345c1a2":"code","300264f5":"code","4d74569f":"code","56037aa8":"code","9f3e9235":"code","1b8276ea":"code","658f7ac6":"code","d65cde33":"code","9987ea21":"code","e16ee121":"code","b7995dc3":"code","d805e187":"code","b214b009":"code","4ad67519":"code","bce6b729":"code","e215cb6b":"code","ea99ab7a":"code","51dd5d84":"code","e43896a6":"code","ff5f0312":"code","a451015c":"code","681c20fd":"code","4dbefbcf":"code","146e8568":"markdown","162e8150":"markdown","d20e86f2":"markdown","0430b5dc":"markdown","3d0b0607":"markdown","6c424726":"markdown","8d83e66b":"markdown","cdd33d26":"markdown","6ef307cd":"markdown","c451e982":"markdown","7b6c0d8c":"markdown","ee54cf97":"markdown","f34ba777":"markdown"},"source":{"70ad776b":"%matplotlib inline\n# matplotlib and seaborn for plotting\nimport matplotlib.pyplot as plt\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import auc, roc_curve\nimport lightgbm as lgb\n\nimport seaborn as sns\nimport numpy  as np\nimport pandas as pd\nN_SPLITS = 15","da8e60ae":"#read train and test input\ntrain = pd.read_csv(\"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntest = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ndel train['Unnamed: 0'] ,test['Unnamed: 0'] \n\ntarget='diabetes_mellitus'\nprint('train ' , train.shape)\nprint('test ' , test.shape)\n\ndf = pd.concat([train, test], axis = 0)\nprint('df ' , df.shape)","29992412":"# Function to calculate missing values by column# Funct \n# from https:\/\/www.kaggle.com\/parulpandey\/starter-code-with-baseline\ndef missing_values_table(df):\n        # Total missing values by column\n        mis_val = df.isnull().sum()\n        \n        # Percentage of missing values by column\n        mis_val_percent = 100 * df.isnull().sum() \/ len(df)\n        \n        # build a table with the thw columns\n        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        \n        # Rename the columns\n        mis_val_table_ren_columns = mis_val_table.rename(\n        columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n        \n        # Sort the table by percentage of missing descending\n        mis_val_table_ren_columns = mis_val_table_ren_columns[\n            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n        '% of Total Values', ascending=False).round(1)\n        \n        # Print some summary information\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n              \" columns that have missing values.\")\n        \n        # Return the dataframe with missing information\n        return mis_val_table_ren_columns\n\n# Missing values for training data\nmissing_values_train = missing_values_table(train)\nmissing_values_train[:20].style.background_gradient(cmap='Greens')","eb48890c":"columns_to_drop = missing_values_train[missing_values_train['% of Total Values']>70].index\ncolumns_to_drop","d74ffce0":"df.drop(columns_to_drop, axis=1, inplace=True)","0acb55c0":"# nan counts along row axis\nspecial_cols = ['encounter_id','hospital_id','diabetes_mellitus']\nfeatures = [col for col in df.columns if col not in special_cols]\ndf['nan_counts'] = df[features].isnull().sum(axis=1)\n\n## count missing values in d1 labs , and h1 labs\nd_cols = [c for c in df.columns if(c.startswith(\"d1\"))]\nh_cols = [c for c in df.columns if(c.startswith(\"h1\"))]\n\ndf[\"d1_row_nan_counts\"]  = df[d_cols].isnull().sum(axis=1)\ndf[\"h1_row_nan_counts\"] = df[h_cols].isnull().sum(axis=1)","c0fa342d":"df.tail()","2ffd3c9c":"for i, col_1 in enumerate(features):\n    others = features.copy()\n    others.remove(col_1)\n    for col_2 in others:\n        if df[col_1].equals(df[col_2]):\n            print(f\"{col_1} and {col_2} are identical.\")","dd01bd34":"df = df.drop(['h1_inr_max', 'h1_inr_min'], axis=1)","9cbcfa04":"train['diabetes_mellitus'].value_counts(normalize=True)","0c8bb640":"# Ethnicity vs Diabetes \ng = sns.catplot(x=\"ethnicity\", col=\"diabetes_mellitus\",\n                data=train, kind=\"count\",\n                height=5, aspect=1.5);","5ab3e5d2":"train['age'] = train.age.fillna(0.0).astype(int)\n\nplt.figure(figsize=(15, 8))\nplt.title(\"Age vs people suffering from Diabetes\")\nsns.lineplot(data=train, x=\"age\", y=\"diabetes_mellitus\", hue=\"gender\")\nplt.xlabel(\"Age\")\nplt.ylabel(\"Number of people Suffering\")","f5a509b7":"import seaborn as sns\nplt.figure(figsize=(18, 8))\nplt.title(\"Age count plot\")\nax = sns.barplot(x=\"age\", y=target, data=train)","b8d3d020":"sum(df['age']>16)\/len(df)","93849954":"# Drop data with implausible age \n# (don't touch the test dataset)\ndf = df[df['age']>16 & df[target].notna()]","c2ce013d":"df['age_strats'] = pd.qcut(df['age'],5)\n    \nmean_agg = df.groupby(['age_strats','gender'])['height','weight','bmi'].mean().reset_index()\nmean_agg.columns =['age_strats','gender','height_mean','weight_mean','bmi_mean']\ndf = df.merge(mean_agg,on=['age_strats','gender'],how='left')\ndf['weight']=df['weight'].fillna(df['weight_mean'])\ndf['height']=df['height'].fillna(df['height_mean'])\ndf = df.drop(['age_strats', 'height_mean','weight_mean','bmi_mean'], axis=1)\ndf.head()","e6bdab9d":"# fillna with recalculated bmi\ndf['bmi'] = np.where(df['bmi'].isna(),df['weight']\/((df['height']\/100)**2) , df['bmi'] )","28d792dc":"#https:\/\/www.calculator.net\/bmi-calculator.html?ctype=metric&cage=25&csex=m&cheightfeet=5&cheightinch=10&cpound=160&cheightmeter=180&ckg=65&printit=0\n\n\n#Severe Thinness\t< 16\n#Moderate Thinness\t16 - 17\n#Mild Thinness\t17 - 18.5\n#Normal\t18.5 - 25\n#Overweight\t25 - 30\n#Obese Class I\t30 - 35\n#Obese Class II\t35 - 40\n#Obese Class III\t> 40\n\ndef set_bmi_category(x):\n    if str(x)=='nan':\n        return x\n    if x<16:\n        return 'severe_thinness'\n    if x<17:\n        return 'moderate_thinnes'\n    if x<18.5:\n        return 'mild_thinnes'\n    if x<25:\n        return 'normal'\n    if x<30:\n        return 'overweight'\n    if x<35:\n        return 'obese_1'\n    if x<40:\n        return 'obese_2'\n    return 'obese_3'\n    \n\ndf['bmi_category'] = df['bmi'].apply(set_bmi_category)","fda6357c":"df[\"gender\"] = df[\"gender\"].fillna(\"Unknown\")\ndf[\"ethnicity\"] = df[\"ethnicity\"].fillna(\"Other\/Unknown\")\ndf[\"hospital_admit_source\"] = df[\"hospital_admit_source\"].fillna(\"Other\")\ndf[\"icu_admit_source\"] = df[\"icu_admit_source\"].fillna(\"Other ICU\")\ndf[\"icu_stay_type\"] = df[\"icu_stay_type\"].fillna(\"admit\")\ndf[\"icu_type\"] = df[\"icu_type\"].fillna(\"Med-Surg ICU\")","9c97649a":"\nfor d_max in [col for col in d_cols if col.endswith('_max')]:\n    d_min = d_max.replace('_max', '_min')\n    h_max = d_max.replace('d1', 'h1')\n    h_min = h_max.replace('_max', '_min')\n    \n    if d_min in df.columns:\n        new_col = 'd1_min>d1_max_'+(d_max.strip('max').strip('d1_'))+'incoherent'\n        df[new_col] = np.where(df[d_max] < df[d_min], 1 , 0)\n        \n    if h_max in df.columns:\n        new_col = 'h1_max>' + d_max +'_incoherent'\n        df[new_col] = np.where(df[d_max] < df[h_max], 1 , 0)\n        \n        if h_min in df.columns:\n            new_col = 'h1_min>h1_max_'+(h_max.strip('max').strip('h1_'))+'incoherent'\n            df[new_col] = np.where(df[h_max] < df[h_min], 1 , 0)\n            \n            new_col = 'h1_min<d1_min_'+(h_max.strip('max').strip('h1_'))+'incoherent'\n            df[new_col] = np.where(df[h_min] < df[d_min], 1 , 0)\n            ","71e320d9":"\n# count incoherent (from j\u00e9rome notebook)\nincr = [col for col in df.columns if 'incoherent' in col ]\ndf['count_inc_per_row'] = df[incr].sum(axis=1)\ndf = df.drop(incr, axis=1)","0345c1a2":"for d_max in [col for col in d_cols if col.endswith('_max')]:\n    d_min = d_max.replace('_max', '_min')\n    h_max = d_max.replace('d1', 'h1')\n    h_min = h_max.replace('_max', '_min')\n    vital_col = d_max.strip('d1_').strip('_max')\n    \n    if d_min in df.columns:\n        df[f'd1_{vital_col}_range'] = df[d_max] - df[d_min]\n        df[f'd1_{vital_col}_range'] = (df[d_max] + df[d_min])\/2\n    if (h_max in df.columns) & (h_min in df.columns):\n        df[f'h1_{vital_col}_range'] = df[h_max] - df[h_min]\n        df[f'h1_{vital_col}_range'] = (df[h_max] + df[h_min])\/2\n","300264f5":"# sum apach\ndf[\"gcs_sum_apache\"] = df[['gcs_eyes_apache', 'gcs_motor_apache', 'gcs_verbal_apache']].sum(axis=1)","4d74569f":"dtypes = df.dtypes\ndtypes[dtypes =='object']\ncats = dtypes[dtypes =='object'].index\ncats","56037aa8":"df = df.drop('readmission_status', axis=1)\n#df['readmission_status'].value_counts()","9f3e9235":"df[cats].isna().sum()","1b8276ea":"df['bmi_category'] = df['bmi_category'].fillna('unknown')","658f7ac6":"enc = OneHotEncoder(handle_unknown='ignore')\n\nohe_df = pd.get_dummies(df[cats])\ndf = pd.concat([df, ohe_df], axis=1)\ndel ohe_df","d65cde33":"skf = StratifiedKFold(n_splits=N_SPLITS, random_state=42)\n#splits = list(skf.split(train, train[target], train['hospital_id']))","9987ea21":"cats","e16ee121":"class KFoldFrequencyEncoderTrain(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, colnames, target, skf, groups=None, discard_original_col=False ):\n        \n        self.colnames = colnames\n        self.skf = skf\n        self.groups= groups\n        self.target = target\n        self.discard_original_col = discard_original_col\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        new_encoded = [col + '_fe' for col in self.colnames]\n        #initialization\n        X[new_encoded] = np.nan\n        for tr_ind, val_ind in self.skf.split(X, X[self.target], groups=X[self.groups]):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            # compute the frequency training subset (for each split)\n            # and apply it on the subset correponding to validation indexes\n            for col, enc_col in zip(self.colnames, new_encoded):\n                X.loc[X.index[val_ind], enc_col] = (X_val[col].map(X_tr.groupby(col).size()\/len(X_tr)))\n\n        if self.discard_original_col:\n            print(f\"drop : {colnames}\")\n            X = X.drop(self.colnames, axis=1)\n        return X\n    \nclass KFoldFrequencyEncoderTest(BaseEstimator, TransformerMixin):\n    def __init__(self, train, colnames):\n        self.train = train\n        self.colnames = colnames\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        new_cols_name = [col + '_fe' for col in self.colnames]\n        X = X.reset_index(drop=True)\n        X[new_cols_name] = np.nan\n        for col, enc_col in zip(self.colnames, new_cols_name):\n            X[enc_col] = train[col].map(train.groupby(col).size()\/len(train))                             \n        return X","b7995dc3":"#df['age'] = df['age'].fillna(0.0).astype(int)\nfreq_encoder = KFoldFrequencyEncoderTrain(colnames=list(cats)+['age'],target=target,skf=skf, groups='hospital_id')\ntrain = df[df[target].notna()]\ntrain_fe = freq_encoder.fit_transform(train)","d805e187":"freq_encoder = KFoldFrequencyEncoderTest(train, colnames=list(cats)+['age'])\ntest = df[df[target].isna()]\ntest_fe = freq_encoder.fit_transform(test)","b214b009":"df1 = pd.concat([train_fe, test_fe], axis=0)\ndel train_fe\ndel test_fe","4ad67519":"class KFoldTargetEncoderTrain(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, colnames, target, skf, groups=None, discard_original_col=False ):\n        \n        self.colnames = colnames\n        self.target = target\n        self.skf = skf\n        self.groups= groups\n        self.discard_original_col = discard_original_col\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        mean_of_target = X[self.target].mean()\n        \n        col_mean_name = [col + '_te' for col in self.colnames]\n        #initialization\n        X[col_mean_name] = np.nan\n        for tr_ind, val_ind in self.skf.split(X, X[self.target], groups=X[self.groups]):\n            X_tr, X_val = X.iloc[tr_ind], X.iloc[val_ind]\n            # compute the mean of target column from the traininf subset (for each split)\n            # and apply it on the subset correponding to validation indexes\n            for col, enc_col in zip(self.colnames, col_mean_name):\n                X.loc[X.index[val_ind], enc_col] = X_val[col].map(X_tr.groupby(col)[self.target].mean())\n                                                         \n            \n            #X[col_mean_name].fillna(mean_of_target, inplace = True)\n\n        if self.discard_original_col:\n            print(f\"drop : {colnames}\")\n            X = X.drop(self.colnames, axis=1)\n        return X\n    \nclass KFoldTargetEncoderTest(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, train, target, colnames):\n        self.train = train\n        self.target=target\n        self.colnames = colnames\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self,X):\n        new_cols_name = [col + '_te' for col in self.colnames]\n        X[new_cols_name] = np.nan\n        for col, enc_col in zip(self.colnames, new_cols_name):\n            X[enc_col] = train[col].map(train.groupby(col)[self.target].mean())                                         \n        return X","bce6b729":"df['age'] = df['age'].fillna(0).astype(int)\ntarget_encoder = KFoldTargetEncoderTrain(colnames=list(cats)+['age'],target=target,skf=skf, groups='hospital_id')\ntrain = df1[df1[target].notna()]\ntrain_te = target_encoder.fit_transform(train)","e215cb6b":"target_encoder = KFoldTargetEncoderTest(train, target=target, colnames=list(cats)+['age'])\ntest = df1[df1[target].isna()]\ntest_te = target_encoder.fit_transform(test)","ea99ab7a":"df1 = pd.concat([train_te, test_te], axis=0)\ndel train_te\ndel test_te","51dd5d84":"for c in cats:\n    df1[c] = df1[c].astype('category')","e43896a6":"\n\ntrain = df1.loc[~df1[target].isna()].reset_index(drop=True)\ntest  = df1.loc[ df1[target].isna()].reset_index(drop=True)\n\ntrain[target]=train[target].astype(int)","ff5f0312":"params = {'n_estimators':50000,\n          'boosting_type': 'gbdt',\n          'objective': 'binary',\n          'metric': 'auc',\n          'subsample': 0.75,\n          'early_stopping_rounds': 500,\n          'num_threads' : 28,\n          'feature_fraction': 0.540736096740662, \n          'lambda_l1': 2., \n          'lambda_l2': 4., \n          'min_data_per_group': 10 ,\n          'learning_rate': 0.01,\n          'max_depth': 7, \n          'scale_pos_weight': 0.871605894680739, \n          'subsample_freq': 4}","a451015c":"dtypes = df1.dtypes\nbinaries = dtypes[dtypes=='int64'].index\nbinaries = [ col for col in binaries if col not in special_cols and df1[col].nunique()==2]\n\nfeatures = [col for col in df1.columns if col not in special_cols]","681c20fd":"# from jerome notebook\n\ndef eval_auc(pred,real):\n    false_positive_rate, recall, thresholds = roc_curve(real, pred)\n    roc_auc = auc(false_positive_rate, recall)\n    return roc_auc \n\nfold_score = []\nstat = []\noof_pred = np.zeros((len(train), ))\ny_pred = np.zeros((len(test), )) \n\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train, train[target], train['hospital_id']) ):\n            x_train, x_val = train[features].iloc[train_idx], train[features].iloc[val_idx]\n            y_train, y_val = train[target][train_idx], train[target][val_idx]\n\n            train_set = lgb.Dataset(x_train, y_train, categorical_feature='auto')#list(cats)+binaries)\n            val_set   = lgb.Dataset(x_val, y_val, categorical_feature='auto')#list(cats)+binaries)\n\n            model = lgb.train(params, train_set, valid_sets=[train_set, val_set], verbose_eval=1000)\n            conv_x_val = x_val\n            oof_pred[val_idx] = model.predict(conv_x_val).reshape(oof_pred[val_idx].shape)\n\n            x_test = test[features]\n            y_pred += model.predict(x_test).reshape(y_pred.shape) \/ N_SPLITS\n            fold_score.append(eval_auc(oof_pred[val_idx],y_val))\n            stat.append(x_val.mean())\n\n            print('------------------------------------------Partial score of fold {} is: {}'.format(fold,eval_auc(oof_pred[val_idx],y_val) ))\n            \nloss_score = eval_auc(oof_pred,train[target].values) \nprint('------------------------------------------------------------------------------Our oof AUC score is: ', loss_score)\n","4dbefbcf":"np.save(\"schop_sub_cv15.npy\",y_pred)\nnp.save(\"schop_cv15.npy\",oof_pred)","146e8568":"### Categorical Columns\nFrom [here](https:\/\/www.kaggle.com\/iamleonie\/wids-datathon-2021-diabetes-detection)\n\n\nWe will fill the missing values for the gender, ethnicity, hospital_admit_source, and icu_admit_source already have columns indicating missing values, such as 'Other' or 'Unknown. We will be using these. For icu_stay_type and icu_type, we will be filling the missing values with the column most common value.","162e8150":"### 1. one hot encoding","d20e86f2":"### 3. Target encoding :\nThe encoding is computed by averaging the target value by category\n\n(same technique as frequency encoding to avoid overfitting)\n","0430b5dc":"## Missing Values","3d0b0607":"#### missing values counts based features","6c424726":"### Categorical features encoding","8d83e66b":"## Drop identical columns","cdd33d26":"### Modeling","6ef307cd":"### statistcal based features, applied on vital & labsfeatres:\n\n","c451e982":"### Weight and height\n1. we will fill nan values with mean values according to the gender and age group\n2. recalculate bmi= weight\/height\u00b2 use it to fill nan values","7b6c0d8c":"### 2. frequency encoding \nfor frequency encoding we will compute frequency occurence of each modality of each category using k-fold cross val encoding to avoid overfitting:\n* for training dataset : for each iteration keep a fold apart (validation dataset) and compute frequency for n_split-1 training dataset then apply the encoding on the validation dataset... until the overall dataset would be filled\n* for the test datset : compute frequencies from the overall training dataset and apply it on test subset\n\nPS: we would have to keep in mind that if we will train our models using cross validation method, we have to keep the same folds","ee54cf97":"Drop columns with high missing values rate (more than 70%)","f34ba777":"### d1\/h1 features"}}