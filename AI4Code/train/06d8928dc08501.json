{"cell_type":{"77e5cc2c":"code","9a09b189":"code","0a35daf5":"code","7a3891a2":"code","32d27e1a":"code","fca23f42":"code","72ac04a4":"code","086ace8c":"code","184f8283":"code","ed4c3c7c":"code","4c91c32d":"code","f7a9cd10":"code","c644b2e0":"code","645fc2e3":"code","ed641ee3":"code","fbb8cfe4":"code","892e2032":"code","64dffe31":"code","cfb396fa":"code","b693d5f6":"code","71418b13":"code","191d7d19":"code","5a28f28b":"code","ca4a8db8":"code","b8bdebf7":"code","e8a0c9d7":"code","424966e0":"code","d61f2317":"markdown","92fd2dd4":"markdown","7dacd43a":"markdown","56930b32":"markdown","c1bc916c":"markdown","ec55f49c":"markdown","7f6c8733":"markdown","8b906f76":"markdown","f14f6ad9":"markdown","d8f388b4":"markdown","bf978da7":"markdown","2a064ddc":"markdown","ad36564c":"markdown","d368b1b9":"markdown","2c2e2369":"markdown","ab98f93e":"markdown","59446f27":"markdown","4cdce399":"markdown","6e546c1a":"markdown","198a254c":"markdown","1ab75337":"markdown","bd543220":"markdown","c8998ca1":"markdown"},"source":{"77e5cc2c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9a09b189":"import matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler\n\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.base import BaseEstimator, TransformerMixin","0a35daf5":"# Set the paths to our data\ntest_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/test.csv\"\ntrain_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/train.csv\"\nsample_data_path = \"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\"\n\n# Define the data\ntest_data = pd.read_csv(test_data_path, index_col='Id')\ntrain_data = pd.read_csv(train_data_path, index_col='Id')\nsample_data = pd.read_csv(sample_data_path)\n\n# Create a directory to hold the scores of each part with different techniques\nscores_dict = {}\nsubmission_dict = {}\n\nprint(\"Data loaded\")","7a3891a2":"def overview_data(df):\n    print(\"Data shape: {}\".format(train_data.shape))\n    return df.head()","32d27e1a":"overview_data(train_data)","fca23f42":"overview_data(test_data)","72ac04a4":"overview_data(sample_data)","086ace8c":"train_data.info()","184f8283":"def get_missing_data_df(data):\n    percent_missing = data.isnull().sum() * 100 \/ len(data)\n    missing_value_df = pd.DataFrame({'column_name': data.columns,\n                                 'percent_missing': percent_missing})\n    print(missing_value_df[missing_value_df.percent_missing>0].shape)\n    return missing_value_df[missing_value_df.percent_missing>0]\n\ndef plot_missing_data_percent(df1, df2):\n    plt.figure(figsize=(16, 8))\n    plt.subplot(1, 2, 1)\n    plt.title('The percent of missing values in Training Data')\n    ax = sns.barplot(x=\"percent_missing\", y=\"column_name\", \n                 data=df1.sort_values(by=['percent_missing'],ascending=False))\n    plt.subplot(1, 2, 2)\n    plt.title('The percent of missing values in Testing Data')\n    ax = sns.barplot(x=\"percent_missing\", y=\"column_name\", \n                 data=df2.sort_values(by=['percent_missing'],ascending=False))\n    plt.show()\n    \ndef get_missing_values_to_drop_list(data, percent):\n    missing_df = get_missing_data_df(data)\n    return missing_df[missing_df.percent_missing>percent].column_name.to_list()","ed4c3c7c":"missing_val_train, missing_val_test = get_missing_data_df(train_data), get_missing_data_df(test_data)\nplot_missing_data_percent(missing_val_train, missing_val_test)","4c91c32d":"y = train_data.SalePrice\ny_log = y.apply(lambda x: np.log(x+1))\nn_bins = 100\nplt.figure(figsize=(20, 4))\n\nplt.subplot(1, 2, 1)\nplt.title('The distribution of SalePrice')\nplt.xlabel('Price')\nplt.ylabel('Count')\nplt.hist(y, bins=n_bins)\n\nplt.subplot(1, 2, 2)\nplt.title('Log(Price)')\nplt.title('The distribution of Log(SalePrice)')\nplt.xlabel('Log(SalePrice)')\nplt.ylabel('Count')\nplt.hist(y_log, bins=n_bins)\n\nplt.show()","f7a9cd10":"plt.title('Log(SalePrice) Boxplot')\nsns.boxplot(x=y_log)\nplt.show()","c644b2e0":"def plot_correlation_heatmap(df, is_standard_scaler=False):\n    num_cols = df.select_dtypes(exclude=['object']).columns.to_list()\n    corr_matrix = df[num_cols].corr().abs()\n    \n    f, ax = plt.subplots(figsize=(20, 15))\n    mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n    cmap = sns.diverging_palette(230, 20, as_cmap=True)\n    sns.heatmap(corr_matrix, mask=mask, cmap=cmap, vmax=1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n    plt.show()\n    \ndef get_redundant_pairs(df):\n    '''Get diagonal and lower triangular pairs of correlation matrix'''\n    pairs_to_drop = set()\n    cols = df.columns\n    for i in range(0, df.shape[1]):\n        for j in range(0, i+1):\n            pairs_to_drop.add((cols[i], cols[j]))\n    return pairs_to_drop\n\ndef get_top_abs_correlations(df, n=5):\n    num_cols = df.select_dtypes(exclude=['object']).columns.to_list()\n    au_corr = df[num_cols].corr().abs().unstack()\n    labels_to_drop = get_redundant_pairs(df[num_cols])\n    au_corr = au_corr.drop(labels=labels_to_drop).sort_values(ascending=False)\n    return au_corr[0:n]","645fc2e3":"plot_correlation_heatmap(train_data)","ed641ee3":"print(\"Top Absolute Correlations\")\nprint(get_top_abs_correlations(train_data, 10))","fbb8cfe4":"def get_unique_cnt_df(df):\n    col_names = df.select_dtypes(include=['object']).columns.to_list()\n    unique_cnt = [df[column].nunique() for column in col_names]\n    \n    return pd.DataFrame({'column_name': col_names,\n                           'unique_cnt': unique_cnt})\n\ndef plot_unique_cnt(df):\n    plt.subplots(figsize=(10, 15))\n    plt.title('The percent of missing values in Training Data')\n    ax = sns.barplot(x=\"unique_cnt\", y=\"column_name\", \n                 data=df.sort_values(by=['unique_cnt'],ascending=False))\n    plt.show()\n    \ndef get_catagorical_inconsistant_col_names(train_data, test_data):\n    col_names = train_data.select_dtypes(include=['object']).columns.to_list()\n\n    consistent_label_cols = [column for column in col_names if \n                             set(train_data[column]) == set(test_data[column])]\n    inconsistent_label_cols = list(set(col_names) - set(consistent_label_cols))\n    return inconsistent_label_cols","892e2032":"plot_unique_cnt(get_unique_cnt_df(train_data))","64dffe31":"col_names = train_data.select_dtypes(include=['object']).columns.to_list()\n\nconsistent_label_cols = [column for column in col_names if \n                   set(train_data[column]) == set(test_data[column])]\n\ninconsistent_label_cols = list(set(col_names) - set(consistent_label_cols))\n\nprint('The Number of Categorical Variables: ',len(col_names))\nprint()\nprint('Variables with consistent labels: ', len(consistent_label_cols))\nprint(consistent_label_cols)\nprint()\nprint('Variables with inconsistent labels: ', len(inconsistent_label_cols))\nprint(inconsistent_label_cols)","cfb396fa":"y = train_data.SalePrice\nX = train_data.drop(['SalePrice'], axis=1)\n\ncol_to_drop = get_missing_values_to_drop_list(train_data, 25)\n\ncategorical_cols = [cname for cname in X.columns if\n                    X[cname].nunique() < 10 and \n                    X[cname].dtype == \"object\" and\n                    cname not in col_to_drop\n                   ]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X.columns if \n                  X[cname].dtype in ['int64', 'float64'] and\n                  cname not in col_to_drop\n                 ]\n\n# Define the data that will be used for all tests\n\n\n# Define the data that will be used for all submission generation\nX_submit = test_data.copy()","b693d5f6":"class LogTransform(BaseEstimator, TransformerMixin):\n    def fit(self, X, y=None):\n        pass\n    \n    def transform(self, X, y=None):\n        X = X.astype('float32')\n        X = np.log(X+1)\n        return X\n    \n    def fit_transform(self, X, y=None):\n        return self.transform(X, y) \n    ","71418b13":"# 1. Numerical & Categorical variables transformer\n# 1.1 for Simple Transformer\nnumerical_transformer_simple = SimpleImputer()\ncategorical_transformer_simple = Pipeline(steps=[\n    ('imputer', SimpleImputer()),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n# 1.2 for PCA Transformer\nnumerical_transformer_pca = Pipeline([\n    ('imputer', SimpleImputer()),\n    ('log_transform', LogTransform()),\n    ('scaler', 'passthrough'),\n    ('reduce_dim',PCA())\n])\ncategorical_transformer_pca = categorical_transformer_simple\n\n\n# 2. Simple Transformer\ndata_transformer_simple = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer_simple, numerical_cols),\n        ('cat', categorical_transformer_simple, categorical_cols)\n    ])\n\n# 3. PCA Transformer\ndata_transformer_pca = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer_pca, numerical_cols),\n        ('cat', categorical_transformer_pca, categorical_cols)\n    ])","191d7d19":"# create a Random Forest\nrf = RandomForestRegressor(random_state=0, n_estimators=100)\n\n# parameters for simple transformer\nparam_grid_rf_simple = {\n    'data_transformer__num__strategy': ['mean', 'median'],\n    'data_transformer__cat__imputer__strategy': ['most_frequent', 'constant'],\n}\n\n# parameters for PCA transformer\nparam_grid_rf_pca = {\n    'data_transformer__num__imputer__strategy': ['mean', 'median'], \n    'data_transformer__num__scaler': [StandardScaler(), MinMaxScaler()],\n    'data_transformer__num__reduce_dim__n_components': [5, 10],\n    'data_transformer__cat__imputer__strategy': ['most_frequent', 'constant'],\n}\n\n# Random Forest with Simple Transformer pipeline\nclf_rf_simple = Pipeline(steps=[\n    ('data_transformer', data_transformer_simple),\n    ('model', rf)\n])\n\n# Random Forest with PCA Transformer pipeline\nclf_rf_pca = Pipeline(steps=[\n    ('data_transformer', data_transformer_pca),\n    ('model', rf)\n])","5a28f28b":"model_result = {}\ndef run_grid_search(name, clf, param_grid, X_train, X_test, y_train, y_test\n                    , scoring='neg_mean_absolute_error', cv=5, is_target_trans=True):\n    if(is_target_trans):\n        y_train = np.log1p(y_train)\n    grid_search = GridSearchCV(clf, param_grid, scoring=scoring, cv=5, n_jobs=-1)\n    grid_search.fit(X_train, y_train)\n    \n    model_result[name] = {\n        'best_model': grid_search.best_estimator_,\n        'best_params': grid_search.best_params_,\n        'cv_results_': grid_search.cv_results_,\n        'column_names': X_train.columns,\n        'test_score': -1\n    }\n    print('The best model:')\n    print(grid_search.best_params_)\n    print()\n    print()\n    print(\"Grid scores on validation dataset:\")\n    means = grid_search.cv_results_['mean_test_score']\n    stds = grid_search.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n        print(\"%0.5f (+\/-%0.05f) for %r\"\n              % (mean, std * 2, params))\n        print()\n    y_true, y_pred = y_test, grid_search.predict(X_test)\n    print(np.mean(y_pred))\n    if(is_target_trans):\n        y_pred = np.expm1(y_pred)\n    test_score = mean_absolute_error(y_true, y_pred)\n    model_result[name]['test_score'] = test_score\n    print()\n    print()\n    print('MAE of prices %.3f on testing dataset' % (test_score))\n    print()\n    ","ca4a8db8":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=0)","b8bdebf7":"run_grid_search('RF_simple', clf_rf_simple, param_grid_rf_simple, X_train, X_test, y_train, y_test\n               , is_target_trans=False)","e8a0c9d7":"run_grid_search('RF_pca', clf_rf_pca, param_grid_rf_pca, X_train, X_test, y_train, y_test, is_target_trans=False)","424966e0":"model_result['RF_simple']['test_score']\nmodel_result['RF_simple']['best_params']\n\ndf_summary = pd.DataFrame(data={\n    'type': ['The best Simple Transformer', 'The best PCA Transformer'],\n    'scores': [model_result['RF_simple']['test_score'], model_result['RF_pca']['test_score']],\n    'parameters': [model_result['RF_simple']['best_params'], model_result['RF_pca']['best_params']],\n})\n\nprint('The best parameters for Simple Transformer:')\nprint(model_result['RF_simple']['best_params'])\nprint()\nprint('The best parameters for PCA Transformer:')\nprint(model_result['RF_pca']['best_params'])\nprint()\ndf_summary","d61f2317":"### Import Data","92fd2dd4":"## Part 2: Find the best data transformer pipeline","7dacd43a":"#### Define multiple transformer strategies for SimpleImputer, Scaler, and PCA n_components\n- First, initiated a Random Forest to evaluate which data transformer is the best\n- Then, defined the candidate parameters for Scaler, SimpleImputer, and PCA n_components\n- In the End, combined the data transformer pipline and Random Forest together\n","56930b32":"Create common functions for GridSearch","c1bc916c":"### Import Libraries","ec55f49c":"### 1.5 Categorical variables","7f6c8733":"I use IQR method to identify outliers of Log(SalePrice). A point is an outlier if it is less than Q1\u20131.5IRQ or greater than Q3 + 1.5IQR. From th boxplot beelow, the outliers are on the both side of the box.\n\n**ACTION-ITEM:** Exclude outliers before training models.","8b906f76":"## Part 1: EDA","f14f6ad9":"### 2.2 Use GridSearch and Random Forest to find the best data transformer pipelines","d8f388b4":"## Part 0: Imports","bf978da7":"The top 10 Absolute Correlations sho\n- Correlation between the target and the features:\n    * target *SalePrice* and feature *OverallQual*, *GrLivArea* are strong correlation.\n- Correlation between piared features:\n    * (GarageCars, GarageArea), (YearBuilt, GarageYrBlt), (GrLivArea, TotRmsAbvGrd), (TotalBsmtSF, 1stFlrSF) are strong correlation.\n    \n**TO-ACTION:** If we use non tree-based algorithms, keep one feature out of the paired features with strong correlation to avoid collinearity.","2a064ddc":"Split data into training and testing set","ad36564c":"#### Create transformer piplines for numerical & categorical variables\nI designed 2 different data transformer piplines and they are\n- simple transformer: SimpleImputer for numerical variables, SimpleImputer and One-Hot Encoding for categorical variables\n- PCA transformer: PCA for numerical variables, SimpleImputer and One-Hot Encoding for categorical variables","d368b1b9":"The column names and their percentage of missing valuse in training data and testing data as illustrated below. We can find that\n1. The number of columns with missing value in testing data is larger than the number in training data.\n2. The Top 6 columns in training and testing data are the same. The columns are *PoolQC*, *MiscFeature*, *Alley*, *Fence*, *FireplaceQc* and *LotFrontage*.\n3. There are 4 columns that the percentage of missing values is over 50%. They are columns *PoolQC*, *MiscFeature*, *Alley* and *Fence*\n4. Most of columns' percentage of missing values is less than 20%.\n\n**TO-ACTION:** Delete columns which percent of missing values over 25%","2c2e2369":"The **combination of Pipeline & GridSearchCV** is always used for hyperparameter tuning, but it can also be applied to find the best data transformer. In this notebook, I experimented how to find the best data transformation strategy by using Pipeline & GridSearchCV, including:\n- Scaler to scale your data.\n- Impute strategy to fill missing values.\n- The number of components in PCA you should use.\n\nI created two data transformers: Simple Transformer & PCA Transformer. In the end of the notebook, I answered **which transformer is the best** and its **optimal parameters**(e.g., scaler, imputer parameters).\n\n-----\nThe structure of the post:\n- **Part 0: Imports**\n- **Part 1: EDA**\n    - 1.1 The overview of data\n    - 1.2 Check missing values\n    - 1.3 The exploration of target variables: distribution & outlier\n    - 1.4 Correlation heatmap with numerical columns\n    - 1.5 Categorical variables\n- **Part 2: Find the best data transformer pipeline**\n    - 2.1 Create data transformer pipelines\n    - 2.2 Use GridSearch and Random Forest to find the best data transformer pipelines\n- **Part3: Conclusion**","ab98f93e":"### 1.1 The overview of data","59446f27":"## Part3: Conclusion\n\nThis notebook focuses on how to use Pipeline and GridSearchCV to find the best parameters for data transformers. From the results of CVs above, Simple Transformer, with parameter `SimpleImputer for categorical variables: strategy = most_frequent` and `SimpleImputer for numerical variables: strategy = mean`, is the best data transformer because its score of MAE for Random Forest is the best.","4cdce399":"#### Create custom transformer","6e546c1a":"#### Simple Transformer is better than PCA Transformer\n\nThe MAE result of the two transformers are shown as below. The score of **Simple Transformer is better** and its parameters are:\n- SimpleImputer for categorical variables: strategy = 'most_frequent'\n- SimpleImputer for numerical variables: strategy = 'mean'","198a254c":"### 1.2 Check missing values\n\n#### Common functions for checking missing value","1ab75337":"### 1.3 The exploration of target variables: distribution & outlier\n\nThe distribution of *SalePrice* is right-skewness as shown in the left chart The distribution of SalePrice. This is not good for prediction models and we can improve it by performing log transformation. The distribution gets close to normal distribution after the transformation as shown in the right chart The distribution of Log(SalePrice).\n\n**ACTION-ITEM:**\nPredict log(SalePrice), not SalePrice.","bd543220":"### 1.4 Correlation heatmap with numerical columns","c8998ca1":"### 2.1 Create data transformer pipelines"}}