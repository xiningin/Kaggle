{"cell_type":{"157d1f92":"code","55a9a5c1":"code","e35436de":"code","172137c1":"code","5063db66":"code","a42db8bd":"code","af759ee9":"code","72841055":"code","a11b513c":"code","2b504150":"code","53703ab6":"code","96697070":"code","19d1e458":"code","bcc80619":"code","c13c8afd":"code","5f32d5cf":"code","28c67d2d":"code","f1e32fdf":"code","09be6db1":"code","1222a6ea":"code","bd5506c8":"code","65e409a7":"code","8ed307d4":"code","fccdd542":"code","5754338d":"code","2f4cd82d":"code","d0e4362b":"code","caed6c3e":"code","b5c0255a":"code","32229967":"code","c7c96f00":"code","36b4ca2f":"code","25e34238":"code","dfff11f0":"code","aa0f2920":"code","43e9b1bb":"code","468c4b35":"code","cfecce19":"markdown","9acd1a0f":"markdown","54f0f706":"markdown","096c35c8":"markdown","d9a46982":"markdown","3201002a":"markdown","5e7433f0":"markdown","06481a4d":"markdown","56f418df":"markdown","24c310d6":"markdown"},"source":{"157d1f92":"import numpy as np\nimport pandas as pd\nimport itertools # Functions creating iterators for efficient looping\nimport os\nimport gc # Grabage collector exposes the underlying memory management mechanism\nimport sys\nimport matplotlib.pyplot as plt\nimport seaborn as sns","55a9a5c1":"sns.set_style('darkgrid')\nsns.set_palette('bone')","e35436de":"def toTapleList(list1,list2):\n    return list(itertools.product(list1,list2))","172137c1":"# Memory saving function credit to https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.\n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                #if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                #    df[col] = df[col].astype(np.float16)\n                #el\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        #else:\n            #df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB --> {:.2f} MB (Decreased by {:.1f}%)'.format(\n        start_mem, end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","5063db66":"train = pd.read_csv('..\/input\/train_V2.csv')\ntrain = reduce_mem_usage(train)\ntest = pd.read_csv('..\/input\/test_V2.csv')\ntest = reduce_mem_usage(test)\nprint(train.shape, test.shape)","a42db8bd":"null_cnt = train.isnull().sum().sort_values()\nprint('null count:', null_cnt[null_cnt > 0])\n# dropna\ntrain.dropna(inplace=True)","af759ee9":"train.describe(include=np.number).drop('count').T","72841055":"for c in ['Id','groupId','matchId']:\n    print(f'unique [{c}] count:', train[c].nunique())","a11b513c":"fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n\ntrain.groupby('matchId')['matchType'].first().value_counts().plot.bar(ax=ax[0])\n\n'''\nsolo  <-- solo,solo-fpp,normal-solo,normal-solo-fpp\nduo   <-- duo,duo-fpp,normal-duo,normal-duo-fpp,crashfpp,crashtpp\nsquad <-- squad,squad-fpp,normal-squad,normal-squad-fpp,flarefpp,flaretpp\n'''\nmapper = lambda x: 'solo' if ('solo' in x) else 'duo' if ('duo' in x) or ('crash' in x) else 'squad'\ntrain['matchType'] = train['matchType'].apply(mapper)\ntrain.groupby('matchId')['matchType'].first().value_counts().plot.bar(ax=ax[1])","2b504150":"all_data = train.append(test, sort=False).reset_index(drop=True)\ndel train, test\ngc.collect()","53703ab6":"match = all_data.groupby('matchId')\nall_data['killsPerc'] = match['kills'].rank(pct=True).values\nall_data['killPlacePerc'] = match['killPlace'].rank(pct=True).values\nall_data['walkDistancePerc'] = match['walkDistance'].rank(pct=True).values\n#all_data['damageDealtPerc'] = match['damageDealt'].rank(pct=True).values\nall_data['walkPerc_killsPerc'] = all_data['walkDistancePerc'] \/ all_data['killsPerc']","96697070":"all_data['_totalDistance'] = all_data['rideDistance'] + all_data['walkDistance'] + all_data['swimDistance']","19d1e458":"def fillInf(df, val):\n    numcols = df.select_dtypes(include='number').columns\n    cols = numcols[numcols != 'winPlacePerc']\n    df[df == np.Inf] = np.NaN\n    df[df == np.NINF] = np.NaN\n    for c in cols: df[c].fillna(val, inplace=True)","bcc80619":"all_data['_healthItems'] = all_data['heals'] + all_data['boosts']\nall_data['_headshotKillRate'] = all_data['headshotKills'] \/ all_data['kills']\nall_data['_killPlaceOverMaxPlace'] = all_data['killPlace'] \/ all_data['maxPlace']\nall_data['_killsOverWalkDistance'] = all_data['kills'] \/ all_data['walkDistance']\n\nfillInf(all_data, 0)","c13c8afd":"all_data.drop(['boosts','heals','killStreaks','DBNOs'], axis=1, inplace=True)\nall_data.drop(['headshotKills','roadKills','vehicleDestroys'], axis=1, inplace=True)\nall_data.drop(['rideDistance','swimDistance','matchDuration'], axis=1, inplace=True)\nall_data.drop(['rankPoints','killPoints','winPoints'], axis=1, inplace=True)","5f32d5cf":"match = all_data.groupby(['matchId'])\ngroup = all_data.groupby(['matchId','groupId','matchType'])\n\n# target feature (max, min)\nagg_col = list(all_data.columns)\nexclude_agg_col = ['Id','matchId','groupId','matchType','maxPlace','numGroups','winPlacePerc']\nfor c in exclude_agg_col:\n    agg_col.remove(c)\nprint(agg_col)\n\n# target feature (sum)\nsum_col = ['kills','killPlace','damageDealt','walkDistance','_healthItems']","28c67d2d":"match_data = pd.concat([\n    match.size().to_frame('m.players'), \n    match[sum_col].sum().rename(columns=lambda s: 'm.sum.' + s), \n    match[sum_col].max().rename(columns=lambda s: 'm.max.' + s),\n    match[sum_col].mean().rename(columns=lambda s: 'm.mean.' + s)\n    ], axis=1).reset_index()\nmatch_data = pd.merge(match_data, \n    group[sum_col].sum().rename(columns=lambda s: 'sum.' + s).reset_index())\nmatch_data = reduce_mem_usage(match_data)\n\nprint(match_data.shape)","f1e32fdf":"minKills = all_data.sort_values(['matchId','groupId','kills','killPlace']).groupby(\n    ['matchId','groupId','kills']).first().reset_index().copy()\nfor n in np.arange(4):\n    c = 'kills_' + str(n) + '_Place'\n    nKills = (minKills['kills'] == n)\n    minKills.loc[nKills, c] = minKills[nKills].groupby(['matchId'])['killPlace'].rank().values\n    match_data = pd.merge(match_data, minKills[nKills][['matchId','groupId',c]], how='left')\n    #match_data[c].fillna(0, inplace=True)\nmatch_data = reduce_mem_usage(match_data)\ndel minKills, nKills\n\nprint(match_data.shape)","09be6db1":"match_data.head()","1222a6ea":"all_data = pd.concat([\n    group.size().to_frame('players'),\n    group.mean(),\n    group[agg_col].max().rename(columns=lambda s: 'max.' + s),\n    group[agg_col].min().rename(columns=lambda s: 'min.' + s),\n    ], axis=1).reset_index()\nall_data = reduce_mem_usage(all_data)\n\nprint(all_data.shape)","bd5506c8":"numcols = all_data.select_dtypes(include='number').columns.values\nnumcols = numcols[numcols != 'winPlacePerc']","65e409a7":"all_data = pd.merge(all_data, match_data)\ndel match_data\ngc.collect()\n\nall_data['enemy.players'] = all_data['m.players'] - all_data['players']\nfor c in sum_col:\n    #all_data['enemy.' + c] = (all_data['m.sum.' + c] - all_data['sum.' + c]) \/ all_data['enemy.players']\n    #all_data['p.sum_msum.' + c] = all_data['sum.' + c] \/ all_data['m.sum.' + c]\n    #all_data['p.max_mmean.' + c] = all_data['max.' + c] \/ all_data['m.mean.' + c]\n    all_data['p.max_msum.' + c] = all_data['max.' + c] \/ all_data['m.sum.' + c]\n    all_data['p.max_mmax.' + c] = all_data['max.' + c] \/ all_data['m.max.' + c]\n    all_data.drop(['m.sum.' + c, 'm.max.' + c], axis=1, inplace=True)\n    \nfillInf(all_data, 0)\nprint(all_data.shape)","8ed307d4":"match = all_data.groupby('matchId')\nmatchRank = match[numcols].rank(pct=True).rename(columns=lambda s: 'rank.' + s)\nall_data = reduce_mem_usage(pd.concat([all_data, matchRank], axis=1))\nrank_col = matchRank.columns\ndel matchRank\ngc.collect()\n\n# instead of rank(pct=True, method='dense')\nmatch = all_data.groupby('matchId')\nmatchRank = match[rank_col].max().rename(columns=lambda s: 'max.' + s).reset_index()\nall_data = pd.merge(all_data, matchRank)\nfor c in numcols:\n    all_data['rank.' + c] = all_data['rank.' + c] \/ all_data['max.rank.' + c]\n    all_data.drop(['max.rank.' + c], axis=1, inplace=True)\ndel matchRank\ngc.collect()\n\nprint(all_data.shape)","fccdd542":"killMinorRank = all_data[['matchId','min.kills','max.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','min.kills'])\nkillMinorRank['rank.minor.maxKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\nkillMinorRank = all_data[['matchId','max.kills','min.killPlace']].copy()\ngroup = killMinorRank.groupby(['matchId','max.kills'])\nkillMinorRank['rank.minor.minKillPlace'] = group.rank(pct=True).values\nall_data = pd.merge(all_data, killMinorRank)\n\ndel killMinorRank\ngc.collect()","5754338d":"# drop constant column\nconstant_column = [col for col in all_data.columns if all_data[col].nunique() == 1]\nprint('drop columns:', constant_column)\nall_data.drop(constant_column, axis=1, inplace=True)","2f4cd82d":"all_data['matchType'] = all_data['matchType'].apply(mapper)\n\nall_data = pd.concat([all_data, pd.get_dummies(all_data['matchType'])], axis=1)\nall_data.drop(['matchType'], axis=1, inplace=True)\n\nall_data['matchId'] = all_data['matchId'].apply(lambda x: int(x,16))\nall_data['groupId'] = all_data['groupId'].apply(lambda x: int(x,16))","d0e4362b":"null_cnt = all_data.isnull().sum().sort_values()\nprint(null_cnt[null_cnt > 0])","caed6c3e":"#all_data.drop([],axis=1,inplace=True)\n\ncols = [col for col in all_data.columns if col not in ['Id','matchId','groupId']]\nfor i, t in all_data.loc[:, cols].dtypes.iteritems():\n    if t == object:\n        all_data[i] = pd.factorize(all_data[i])[0]\n\nall_data = reduce_mem_usage(all_data)\nall_data.head()","b5c0255a":"X_train = all_data[all_data['winPlacePerc'].notnull()].reset_index(drop=True)\nX_test = all_data[all_data['winPlacePerc'].isnull()].drop(['winPlacePerc'], axis=1).reset_index(drop=True)\ndel all_data\ngc.collect()\n\nY_train = X_train.pop('winPlacePerc')\nX_test_grp = X_test[['matchId','groupId']].copy()\ntrain_matchId = X_train['matchId']\n\n# drop matchId,groupId\nX_train.drop(['matchId','groupId'], axis=1, inplace=True)\nX_test.drop(['matchId','groupId'], axis=1, inplace=True)\n\nprint(X_train.shape, X_test.shape)","32229967":"print(pd.DataFrame([[val for val in dir()], [sys.getsizeof(eval(val)) for val in dir()]],\n                   index=['name','size']).T.sort_values('size', ascending=False).reset_index(drop=True)[:10])\n","c7c96f00":"from sklearn.model_selection import GroupKFold\nfrom sklearn.preprocessing import minmax_scale\nimport lightgbm as lgb\n\nparams={'learning_rate': 0.1,\n        'objective':'mae',\n        'metric':'mae',\n        'num_leaves': 31,\n        'verbose': 1,\n        'random_state':42,\n        'bagging_fraction': 0.7,\n        'feature_fraction': 0.7\n       }\n\nreg = lgb.LGBMRegressor(**params, n_estimators=10000)\nreg.fit(X_train, Y_train)\npred = reg.predict(X_test, num_iteration=reg.best_iteration_)","36b4ca2f":"# Plot feature importance\nfeature_importance = reg.feature_importances_\nfeature_importance = 100.0 * (feature_importance \/ feature_importance.max())\nsorted_idx = np.argsort(feature_importance)\nsorted_idx = sorted_idx[len(feature_importance) - 30:]\npos = np.arange(sorted_idx.shape[0]) + .5\n\nplt.figure(figsize=(12,8))\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, X_train.columns[sorted_idx])\nplt.xlabel('Relative Importance')\nplt.title('Variable Importance')\nplt.show()","25e34238":"X_train.columns[np.argsort(-feature_importance)].values","dfff11f0":"X_test_grp['_nofit.winPlacePerc'] = pred\n\ngroup = X_test_grp.groupby(['matchId'])\nX_test_grp['winPlacePerc'] = pred\nX_test_grp['_rank.winPlacePerc'] = group['winPlacePerc'].rank(method='min')\nX_test = pd.concat([X_test, X_test_grp], axis=1)","aa0f2920":"fullgroup = (X_test['numGroups'] == X_test['maxPlace'])\n\n# full group (201366) --> calculate from rank\nsubset = X_test.loc[fullgroup]\nX_test.loc[fullgroup, 'winPlacePerc'] = (subset['_rank.winPlacePerc'].values - 1) \/ (subset['maxPlace'].values - 1)\n\n# not full group (684872) --> align with maxPlace\nsubset = X_test.loc[~fullgroup]\ngap = 1.0 \/ (subset['maxPlace'].values - 1)\nnew_perc = np.around(subset['winPlacePerc'].values \/ gap) * gap  # half&up\nX_test.loc[~fullgroup, 'winPlacePerc'] = new_perc\n\nX_test['winPlacePerc'] = X_test['winPlacePerc'].clip(lower=0,upper=1)","43e9b1bb":"X_test.loc[X_test['maxPlace'] == 0, 'winPlacePerc'] = 0\nX_test.loc[X_test['maxPlace'] == 1, 'winPlacePerc'] = 1  # nothing\nX_test.loc[(X_test['maxPlace'] > 1) & (X_test['numGroups'] == 1), 'winPlacePerc'] = 0\nX_test['winPlacePerc'].describe()","468c4b35":"test = pd.read_csv('..\/input\/test_V2.csv')\ntest['matchId'] = test['matchId'].apply(lambda x: int(x,16))\ntest['groupId'] = test['groupId'].apply(lambda x: int(x,16))\n\nsubmission = pd.merge(test, X_test[['matchId','groupId','winPlacePerc']])\nsubmission = submission[['Id','winPlacePerc']]\nsubmission.to_csv(\"submission.csv\", index=False)","cfecce19":"****Feature Engineering****","9acd1a0f":"**New Features**","54f0f706":"**Grouping to predict order of the places**","096c35c8":"**Encode**","d9a46982":"Percentage of Rank","3201002a":"**Time for Submission**","5e7433f0":"**Aggregating the features**","06481a4d":"**Prediction using LGBMRegression**","56f418df":"**Delete some features**","24c310d6":"**Drop the old features**"}}