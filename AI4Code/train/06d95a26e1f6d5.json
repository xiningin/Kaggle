{"cell_type":{"6ca886ca":"code","e0dda595":"code","e7a9b7c7":"code","034d2e71":"code","37e2d0fd":"code","4a4942cd":"code","d0d001f8":"code","0c6e6444":"code","578cecb5":"code","18e14c3f":"code","7cbfedae":"code","4739107c":"code","411e2c95":"code","2f130422":"code","049363d4":"code","d69faf1b":"code","8ac8ea57":"code","11adac2d":"code","5f3e99cd":"code","4253a770":"code","dc09de50":"code","52a35614":"code","71b84aca":"code","f0e6a5ef":"code","87443727":"code","8cce941f":"code","e82d13b6":"code","18929cee":"code","cf000180":"code","1e674325":"code","06cc3da8":"code","d568b7d7":"code","84ed4bba":"code","ed7298ca":"code","3ca87387":"code","96d0dc69":"code","908e1d93":"code","2c536dfe":"markdown","11c9f47c":"markdown","79066219":"markdown","8b71fa31":"markdown","52653599":"markdown","4168ddce":"markdown","1f635fa6":"markdown","f797578b":"markdown","addbee69":"markdown","3a504b54":"markdown","d2ac60e7":"markdown","eead45c0":"markdown","7d9a67be":"markdown","c0fa9c39":"markdown","4bbd99fd":"markdown","7e4e8d51":"markdown","af7156cc":"markdown","3c503fe3":"markdown","13635823":"markdown","b45e95e1":"markdown"},"source":{"6ca886ca":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0dda595":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport io\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom sklearn.metrics import confusion_matrix,classification_report\nimport plotly.figure_factory as ff\n\nfrom sklearn.model_selection import StratifiedKFold,KFold\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","e7a9b7c7":"train = pd.read_csv('\/kaggle\/input\/dataset\/Training Data.csv')\nprint(train.shape)\ntrain.head()","034d2e71":"test = pd.read_csv('\/kaggle\/input\/dataset\/Test Data.csv')\nprint(test.shape)\ntest.head()","37e2d0fd":"train.info()","4a4942cd":"print (\"Rows     : \" ,train.shape[0])\nprint (\"Columns  : \" ,train.shape[1])\nprint (\"\\nFeatures : \\n\" ,train.columns.tolist())\nprint (\"\\nMissing values :  \", train.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",train.nunique())","d0d001f8":"train_0 = train[train['Age']==0]","0c6e6444":"print(train_0.shape)\ntrain_0.head()","578cecb5":"train.describe()","18e14c3f":"temp = train.groupby('Adherence').count().reset_index().sort_values(by='patient_id',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","7cbfedae":"## Visualize the data\n#labels\nlab = train[\"Adherence\"].value_counts().keys().tolist()\n#values\nval = train[\"Adherence\"].value_counts().values.tolist()\n\ntrace = go.Pie(labels = lab ,\n               values = val ,\n               marker = dict(colors =  [ 'royalblue' ,'lime'],\n                             line = dict(color = \"white\",\n                                         width =  1.3)\n                            ),\n               rotation = 90,\n               hoverinfo = \"label+value+text\",\n               hole = .5\n              )\nlayout = go.Layout(dict(title = \"Target data distribution\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                       )\n                  )\n\ndata = [trace]\nfig = go.Figure(data = data,layout = layout)\npy.iplot(fig)","4739107c":"fig = go.Figure(go.Funnelarea(\n    text=temp.Adherence,\n    values=temp.patient_id,\n    title={'position':'top center','text':'Funnel-chart of adherence dsitribution'}))\nfig.show()","411e2c95":"train.columns","2f130422":"#Separating churn and non churn customers\nadhere     = train[train[\"Adherence\"] == \"Yes\"]\nnot_adhere = train[train[\"Adherence\"] == \"No\"]\n\ncat_feature = ['Gender','Diabetes','Alcoholism','HyperTension','Smokes','Tuberculosis','Sms_Reminder']\n\n\n#Separating catagorical and numerical columns\nId_col     = ['patient_id']\ntarget_col = [\"Adherence\"]\ncat_cols   = cat_feature\nnum_cols   = [x for x in train.columns if x not in cat_cols + target_col + Id_col]","049363d4":"\nfor features in cat_feature:\n\n    #labels\n    lab = train[features].value_counts().keys().tolist()\n    #values\n    val = train[features].value_counts().values.tolist()\n\n    trace = go.Pie(labels = lab ,\n                   values = val ,\n                   marker = dict(colors =  [ 'royalblue' ,'lime'],\n                                 line = dict(color = \"white\",\n                                             width =  1.3)\n                                ),\n                   rotation = 90,\n                   hoverinfo = \"label+value+text\",\n                   hole = .5\n                  )\n    layout = go.Layout(dict(title = \"Categorical feature distribution: {}\".format(features),\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                           )\n                      )\n\n    data = [trace]\n    fig = go.Figure(data = data,layout = layout)\n    py.iplot(fig)","d69faf1b":"def plot_pie(column) :\n    \n    trace1 = go.Pie(values  = adhere[column].value_counts().values.tolist(),\n                    labels  = adhere[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    domain  = dict(x = [0,.48]),\n                    name    = \"Adherence Patients\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    hole    = .6\n                   )\n    trace2 = go.Pie(values  = not_adhere[column].value_counts().values.tolist(),\n                    labels  = not_adhere[column].value_counts().keys().tolist(),\n                    hoverinfo = \"label+percent+name\",\n                    marker  = dict(line = dict(width = 2,\n                                               color = \"rgb(243,243,243)\")\n                                  ),\n                    domain  = dict(x = [.52,1]),\n                    hole    = .6,\n                    name    = \"Not adherence patients\" \n                   )\n\n\n    layout = go.Layout(dict(title = column + \" distribution in patient's feature \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            annotations = [dict(text = \"Adherence Patients\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .15, y = .5),\n                                           dict(text = \"Non adherence patients\",\n                                                font = dict(size = 13),\n                                                showarrow = False,\n                                                x = .88,y = .5\n                                               )\n                                          ]\n                           )\n                      )\n    data = [trace1,trace2]\n    fig  = go.Figure(data = data,layout = layout)\n    py.iplot(fig)\n\n\n#function  for histogram for customer attrition types\ndef histogram(column) :\n    trace1 = go.Histogram(x  = adhere[column],\n                          histnorm= \"percent\",\n                          name = \"Adherence Patient's\",\n                          marker = dict(line = dict(width = .5,\n                                                    color = \"black\"\n                                                    )\n                                        ),\n                         opacity = .9 \n                         ) \n    \n    trace2 = go.Histogram(x  = not_adhere[column],\n                          histnorm = \"percent\",\n                          name = \"Non adherence patients\",\n                          marker = dict(line = dict(width = .5,\n                                              color = \"black\"\n                                             )\n                                 ),\n                          opacity = .9\n                         )\n    \n    data = [trace1,trace2]\n    layout = go.Layout(dict(title =column + \" distribution in patients features \",\n                            plot_bgcolor  = \"rgb(243,243,243)\",\n                            paper_bgcolor = \"rgb(243,243,243)\",\n                            xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = column,\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                            yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                             title = \"percent\",\n                                             zerolinewidth=1,\n                                             ticklen=5,\n                                             gridwidth=2\n                                            ),\n                           )\n                      )\n    fig  = go.Figure(data=data,layout=layout)\n    \n    py.iplot(fig)\n    \n# #function  for scatter plot matrix  for numerical columns in data\ndef scatter_matrix(df)  :\n    \n    df  = df.sort_values(by = \"Adherence\" ,ascending = True)\n    classes = df[\"Adherence\"].unique().tolist()\n    classes\n    \n    class_code  = {classes[k] : k for k in range(2)}\n    class_code\n\n    color_vals = [class_code[cl] for cl in df[\"Adherence\"]]\n    color_vals\n\n    pl_colorscale = \"Portland\"\n\n    pl_colorscale\n\n    text = [df.loc[k,\"Adherence\"] for k in range(len(df))]\n    text\n\n    trace = go.Splom(dimensions = [dict(label  = \"Age\",\n                                       values = df[\"Age\"]),\n                                  dict(label  = 'Prescription_period',\n                                       values = df['Prescription_period'])],\n                     text = text,\n                     marker = dict(color = color_vals,\n                                   colorscale = pl_colorscale,\n                                   size = 3,\n                                   showscale = False,\n                                   line = dict(width = .1,\n                                               color='rgb(230,230,230)'\n                                              )\n                                  )\n                    )\n    axis = dict(showline  = True,\n                zeroline  = False,\n                gridcolor = \"#fff\",\n                ticklen   = 4\n               )\n    \n    layout = go.Layout(dict(title  = \n                            \"Scatter plot matrix for Numerical columns for customer attrition\",\n                            autosize = False,\n                            height = 800,\n                            width  = 800,\n                            dragmode = \"select\",\n                            hovermode = \"closest\",\n                            plot_bgcolor  = 'rgba(240,240,240, 0.95)',\n                            xaxis1 = dict(axis),\n                            yaxis1 = dict(axis),\n                            xaxis2 = dict(axis),\n                            yaxis2 = dict(axis),\n                            xaxis3 = dict(axis),\n                            yaxis3 = dict(axis),\n                           )\n                      )\n    data   = [trace]\n    fig = go.Figure(data = data,layout = layout )\n    py.iplot(fig)\n\n#for all categorical columns plot pie\nfor i in cat_feature :\n    plot_pie(i)\n\n#for all categorical columns plot histogram    \nfor i in num_cols :\n    histogram(i)\n    \nscatter_matrix(train)","8ac8ea57":"sns.distplot(train['Age'], color='g', bins=50, hist_kws={'alpha': 0.4});","11adac2d":"sns.distplot(train['Prescription_period'], color='b', bins=50, hist_kws={'alpha': 0.4});","5f3e99cd":"## getting features from prescription period i.e Week and Year\n\ndef _get_weeks_(number_of_days):\n    week = int((number_of_days % 365) \/ 7)\n    \n    return week\n\ndef _get_years(number_of_days):\n    year = int(number_of_days \/ 365)\n    \n    return year","4253a770":"train['Prescription_period_week'] = train['Prescription_period'].apply(lambda x: _get_weeks_(x))\ntrain['Prescription_period_year'] = train['Prescription_period'].apply(lambda x: _get_years(x))","dc09de50":"train['Gender']=train['Gender'].map({'F':0,'M':1})\ntrain['Adherence']=train['Adherence'].map({'No':1,'Yes':0})\nX = train.drop(['patient_id','Adherence'],axis=1)","52a35614":"corr = X.corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(11, 9))\n\n# Generate a custom diverging colormap\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\n# Draw the heatmap with the mask and correct aspect ratio\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})","71b84aca":"# Target class\nY = train['Adherence']","f0e6a5ef":"# Splitting data into train and test with test size of 0.2\n\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.2,stratify=Y,random_state=10)","87443727":"# Helper function\n\ndef _get_confusion_matrix(df):\n    cf_matrix = pd.DataFrame(df,[\"Positive\",\"Negative\"],\n                        [\"Positive\",\"Negative\"],\n                         dtype=int)\n    sns.heatmap(cf_matrix,annot=True,annot_kws={\"size\": 16}, fmt='g')\n    plt.title('Confusion Matrix')\n    \n    print('Classification Report\\n',classification_report(prediction,ytest))\n    \n    \n    print(\"Precision for Yes :{}%\".format((df[0][0])*100\/(df[0][0]+df[0][1])))\n    print(\"Recall for Yes : {}%\".format((df[0][0])*100\/(df[0][0]+df[1][0])))\n    print(\"Precision for No : {}%\".format((df[1][1])*100\/(df[1][1]+df[1][0])))\n    print(\"Recall for No : {}%\".format((df[1][1])*100\/(df[1][1]+df[0][1])))","8cce941f":"model = LGBMClassifier(learning_rate=0.02,\n                    boosting_type='gbdt',\n                    max_depth=4,\n                    random_state=100,\n                    n_estimators=800,\n                    reg_alpha=0,\n                    reg_lambda=1,\n                    n_jobs=-1)\nmodel.fit(X,Y)\nmodel.fit(xtrain,ytrain)\nprediction = model.predict(xtest)\nprint(\"Accuraccy score\",accuracy_score(prediction,ytest))\nco_matrix = confusion_matrix(prediction,ytest)","e82d13b6":"# Let's see feature importances\nfeat_imp = pd.Series(model.feature_importances_, index=X.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,5))","18929cee":"_get_confusion_matrix(co_matrix)","cf000180":"from sklearn.model_selection import GridSearchCV\n\nn_estimators = [50,100]\nmax_depth = [3,5, 8, 15]\nmin_samples_split = [2, 5, 10, 15]\nmin_samples_leaf = [1, 2, 5, 10] \n\nforest = RandomForestClassifier()\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\ngridF = GridSearchCV(forest, hyperF, cv = 3, verbose = 1, \n                      n_jobs = 6)\nbestF = gridF.fit(xtrain, ytrain)","1e674325":"from sklearn.metrics import accuracy_score,f1_score,confusion_matrix,classification_report\n\nmodel = RandomForestClassifier(max_depth=8,min_samples_leaf=1,min_samples_split=10,n_estimators=50)\nmodel.fit(xtrain,ytrain)\nprediction = model.predict(xtest)\nprint(\"Accuraccy score\",accuracy_score(prediction,ytest))\nco_matrix = confusion_matrix(prediction,ytest)","06cc3da8":"_get_confusion_matrix(co_matrix)","d568b7d7":"# Let's the feature importances\nfeat_imp = pd.Series(model.feature_importances_, index=xtrain.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,5))","84ed4bba":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nn_estimators = [50,100,300]\nmax_depth = [3,5, 8, 15]\nlearning_rate = [0.001,0.01,0.1,1]\n\nxgb_model = xgb.XGBClassifier(\n                      scale_pos_weight=1,\n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      reg_alpha = 0.3, \n                      gamma=10)\n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,learning_rate=learning_rate)\n\ngridF = GridSearchCV(xgb_model, hyperF, cv = 3, verbose = 1, \n                      n_jobs = 6)\nbestF = gridF.fit(xtrain, ytrain)","ed7298ca":"from sklearn.model_selection import GridSearchCV\nimport xgboost as xgb\n\nmodel = xgb.XGBClassifier( \n                      scale_pos_weight=1,\n                      learning_rate=1,  \n                      colsample_bytree = 0.4,\n                      subsample = 0.8,\n                      objective='binary:logistic', \n                      n_estimators=300, \n                      reg_alpha = 0.3,\n                      max_depth=5, \n                      gamma=10)\n\nmodel.fit(xtrain,ytrain)\nprediction = model.predict(xtest)\nprint(\"Accuraccy score\",accuracy_score(prediction,ytest))\nco_matrix = confusion_matrix(prediction,ytest)","3ca87387":"_get_confusion_matrix(co_matrix)","96d0dc69":"# Let's the feature importances\nfeat_imp = pd.Series(model.feature_importances_, index=xtrain.columns)\nfeat_imp.nlargest(30).plot(kind='barh', figsize=(8,5))","908e1d93":"from prettytable import PrettyTable\nx = PrettyTable()\nx.field_names = [\"Models\",\"Accuracy\",\"Recall\",\"precision\"]\n\nx.add_row([\"lgm\",89.64,90,90])\nx.add_row([\"Randomfores\",89.64,90,90])\nx.add_row([\"Xgboost\",89.67,90,90])\n\nprint(x)","2c536dfe":"> <h3>Insights<\/h3><ol><li><b> Here we can see Adherence features distribution which interprets as <font color='red'>Yes<\/font> if patient adherence to prescription and <font color='red'>No<\/font> if patient is not adherence to prescription<\/b><br><\/li>\n    <li><b>So figure interprets that 30.2 % patient's are adherence to prescription and 69.8% not<\/b><\/li><\/ol>","11c9f47c":"> <b> Here we can see which features have number of uniques values in the data. <\/b>","79066219":"> <h3>Insights<\/h3>\n><ol>\n    <li><b>From the figure we can interpret that in <font color='red'>Gender<\/font> feature more female are there as compared to male. i.e female contributes <font color='red'>66.2%<\/font> and male <font color='red'>33.8%<\/font><\/b><\/li>\n    <li><b>In diabetes feature we can observe more number of patient are having diabetes i.e <font color='red'>92.1% <\/font> <\/b><\/li>\n    <li><b>In Alcoholism feature we can observe more number of patient are alcholic i.e <font color='red'>97.5% <\/font> <\/b><\/li>\n    <li><b>In Hypertension feature we can observe more number of patient are having hypertension i.e <font color='red'>78.3% <\/font> <\/b><\/li>\n    <li><b>In Smokes feature we can observe more number of patient smokes i.e <font color='red'>94.7% <\/font> <\/b><\/li>\n    <li><b>In Tuberculosis feature we can observe more number of patient are having TB i.e <font color='red'>100% <\/font> <\/b><\/li>\n    <li><b>In SMS_reminder feature we can observe approx <font color='red'>56.9% <\/font> patient have sms reminder<\/b><\/li>\n<\/ol>","8b71fa31":"> <b>Here we can see our features are not coorelated to each other.<\/b>","52653599":"## Feature Engineering","4168ddce":"## Xgboost","1f635fa6":"## RandomForest","f797578b":"## Descriptive Analysis","addbee69":"## More","3a504b54":"# Modeling ","d2ac60e7":"## Data Overview","eead45c0":"## LGBM","7d9a67be":"### Lets draw a Funnel-Chart for better visualization","c0fa9c39":"> <h3>Insights<\/h3><ol><li><b> Here we can see how are features are ranging with min,mean and max values <\/b><br><\/li>\n    <li><b> We can see in the <font color='red'>Age<\/font> feature that minimum age is 0 and max is 113 and mean is around 37.<\/b><br><\/li>\n    <li><b> Similarly we can interpret for other features too.<\/b><\/li><\/ol>","4bbd99fd":"## EDA","7e4e8d51":"# Model Comparision","af7156cc":"1) We can try ensemble, Blending or else stacking to increase the performance of the model.","3c503fe3":"## Co-relation Matrix","13635823":"> <b> Here we can see we data comprises of numeric and categorical feature <\/b>","b45e95e1":"## Visualize the categorical feature data"}}