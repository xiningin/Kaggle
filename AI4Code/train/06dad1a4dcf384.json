{"cell_type":{"419498a2":"code","b161eb62":"code","c3d5692d":"code","61b8043e":"code","b990dd82":"code","1c8b3426":"code","abccf83c":"code","a12bd5d0":"code","e425408c":"code","ae12f739":"code","0dd68920":"code","c35323f6":"code","f760198e":"code","f4f9bdd6":"code","21ef1540":"code","7b931e42":"code","503603ec":"code","7dbe9336":"code","cd443be0":"markdown","ea556aa3":"markdown","eb1b6443":"markdown","66e8deab":"markdown"},"source":{"419498a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b161eb62":"import os\nimport gc\nimport time\nimport math\nimport datetime\n\nimport numpy as np\nimport pandas as pd\nfrom pathlib import Path\n\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom plotly.subplots import make_subplots\n\nimport scipy\nimport statsmodels\nfrom fbprophet import Prophet\n\nfrom multiprocessing import Pool, cpu_count\nfrom joblib import Parallel, delayed\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nstart = time.time()","c3d5692d":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","61b8043e":"INPUT_DIR = '..\/input\/m5-forecasting-accuracy'\ncalendar_data = pd.read_csv(f'{INPUT_DIR}\/calendar.csv')\nprice_data = pd.read_csv(f'{INPUT_DIR}\/sell_prices.csv')\nsubmission_data = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\nsales_data = pd.read_csv(f'{INPUT_DIR}\/sales_train_validation.csv')\nevaluation_data = pd.read_csv(f'{INPUT_DIR}\/sales_train_evaluation.csv')\n\nsales_data = reduce_mem_usage(sales_data)\nprice_data = reduce_mem_usage(price_data)\nsubmission_data = reduce_mem_usage(submission_data)\ncalendar_data = reduce_mem_usage(calendar_data)\n\nd_cols = [c for c in sales_data.columns if 'd_' in c]\n","b990dd82":"def split_train_val():\n    start_day = min(calendar_data['date'])\n    start_day = datetime.datetime.strptime('2011-01-29','%Y-%m-%d').date()\n    dates = [start_day + i*datetime.timedelta(days =1) for i in range(0,len(d_cols))]\n    train_start = 0\n    train_end = dates[-29]\n    return train_end\n\n\n\n\ndef create_snapshot(calendar_data,sales_data,id_lst):\n    #from datetime import strptime\n    start_day = datetime.datetime.strptime(min(calendar_data['date']),'%Y-%m-%d').date()\n    dates = [start_day + i*datetime.timedelta(days =1) for i in range(0,len(d_cols))]\n    gs_temp = pd.DataFrame()\n    gs_snapshot = pd.DataFrame()\n    for item in id_lst:\n        gs_temp['snapshot_date'] = dates\n        gs_temp['id'] = item\n        gs_temp['cat_id']  = sales_data.loc[sales_data['id']==item]['cat_id'].values[0]\n        gs_temp['store_id']  = sales_data.loc[sales_data['id']==item]['store_id'].values[0]\n        gs_temp['state_id'] = sales_data.loc[sales_data['id']==item]['state_id'].values[0]\n        gs_temp['item_id'] = sales_data.loc[sales_data['id']==item]['item_id'].values[0]\n        gs_temp['sales'] = sales_data.loc[sales_data['id']==item][d_cols].T.values\n        gs_snapshot = pd.concat([gs_snapshot,gs_temp])\n    return gs_snapshot\n\ndef convert_to_wm_yr_wk(df):\n    df['wm_yr_wk'] = df['snapshot_date'].apply(lambda x:str(x.isocalendar()[0])[-2:]+ \n                                               (str(x.isocalendar()[1]) if x.isocalendar()[1]>10 \n                                         else '0'+ str(x.isocalendar()[1]))+str(x.isocalendar()[1]))\n    return df\n","1c8b3426":"cpu_count()-1","abccf83c":"def merge_price_cal_data(price_data,calendar_data):\n    price_cal = price_data.merge(calendar_data, on = ['wm_yr_wk'], how = 'left')\n    price_cal.rename({'date':'snapshot_date'},\n                 inplace = True, axis =1)\n    price_cal.drop(['wm_yr_wk','weekday','month','year'],axis = 1, inplace = True)\n    price_cal['snapshot_date'] = price_cal['snapshot_date'].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d').date())\n\n    #price_cal.head()\n    return price_cal\n\ndef merge_snapshot_pricecal(price_cal, gs_snapshot):\n    gs_comb = gs_snapshot.merge(price_cal, on =['store_id','item_id','snapshot_date'],how = 'left')\n    return gs_comb\n\ndef extract_id_info(id1):\n    id_info= id1.split('_')\n    state = id_info[3]\n    category = id_info[0]\n    return state,category\n","a12bd5d0":"\ndef select_snaps(gs_comb,id1):\n    state, category = extract_id_info(id1)\n    snap_days_CA = gs_comb[gs_comb['snap_CA']==1]['snapshot_date'].unique()\n    snap_days_TX = gs_comb[gs_comb['snap_TX']==1]['snapshot_date'].unique()\n    snap_days_WI = gs_comb[gs_comb['snap_TX']==1]['snapshot_date'].unique()\n    if state =='CA':\n        return snap_days_CA\n    elif state == 'TX':\n        return snap_days_TX\n    else:\n        return snap_days_WI\n\n\ndef get_train_val_dates(split,gs_comb):\n    gs_comb['snapshot_date'] = gs_comb['snapshot_date'].apply(lambda x:datetime.datetime.strptime(x,'%Y-%m-%d').date())\n    start_day = datetime.datetime.strptime('2011-01-29','%Y-%m-%d').date()\n    dates = [start_day + i*datetime.timedelta(days =1) for i in range(0,len(d_cols))]\n    train_start = 0\n    train_start = dates[0]\n    train_end = dates[int(split*len(dates))]\n    return train_start,train_end\n    #train_end_ind = 0.9*len(dates)\n    ","e425408c":"def get_holidays(gs_comb,id1):\n    Hol1_rel = gs_comb[gs_comb['event_type_1']=='Religious']['snapshot_date'].unique()\n    Hol1_nat = gs_comb[gs_comb['event_type_1']=='National']['snapshot_date'].unique()\n    Hol1_cul = gs_comb[gs_comb['event_type_1']=='Cultural']['snapshot_date'].unique()\n    Hol1_Sp = gs_comb[gs_comb['event_type_1']=='Sporting']['snapshot_date'].unique()\n\n    #----------------------------\n    Hol2_rel = gs_comb[gs_comb['event_type_2']=='Religious']['snapshot_date'].unique()\n    Hol2_cul = gs_comb[gs_comb['event_type_2']=='Cultural']['snapshot_date'].unique()\n    \n    #train_start, train_end = get_train_val_dates(split, gs_comb)\n    snap_days1 = pd.DataFrame({\n      'holiday': 'snaps',\n      'ds': pd.to_datetime(select_snaps(gs_comb, id1)),\n      'lower_window': 0,\n      'upper_window': 0,\n    })\n\n    holiday1_rel = pd.DataFrame({\n      'holiday': 'holiday_religious',\n      'ds': pd.to_datetime(Hol1_rel),\n      'lower_window': -1,\n      'upper_window': 1,\n    })\n\n\n\n    holiday1_cul = pd.DataFrame({\n      'holiday': 'holiday_cultural',\n      'ds': pd.to_datetime(Hol1_cul),\n      'lower_window': -1,\n      'upper_window': 1,\n    })\n\n    holiday1_nat = pd.DataFrame({\n      'holiday': 'holiday_national',\n      'ds': pd.to_datetime(Hol1_nat),\n      'lower_window': -1,\n      'upper_window': 1,\n    })\n\n\n    holiday2_cul = pd.DataFrame({\n      'holiday': 'holiday_religious',\n      'ds': pd.to_datetime(Hol2_cul),\n      'lower_window': -1,\n      'upper_window': 1,\n    })\n\n\n    holiday2_rel = pd.DataFrame({\n      'holiday': 'holiday_religious',\n      'ds': pd.to_datetime(Hol2_rel),\n      'lower_window': -1,\n      'upper_window': 1,\n    })\n    holidays = pd.concat((snap_days1,holiday1_rel,holiday1_cul,holiday1_nat,holiday2_cul,holiday2_rel ))\n    return holidays\n","ae12f739":"def train_model(gs_comb,holidays, id1, train_end):\n    data = gs_comb[gs_comb['id']==id1]\n    data2 = data.rename({'snapshot_date':'ds','sales':'y'},axis=1)[['sell_price','ds','y']]\n    data2_tr = data2[data2['ds']<=train_end]\n    median =  data2_tr['sell_price'].median(axis = 0)\n    data2_tr['sell_price'] = data2_tr['sell_price'].fillna(median)\n    data2_tr['ds'] = data2_tr['ds'].astype('datetime64')\n    m2 = Prophet(holidays=holidays,weekly_seasonality = True, yearly_seasonality= True,changepoint_prior_scale = 0.7,uncertainty_samples = True)\n    m2.add_seasonality(name='monthoy', period=30.5, fourier_order=5)\n    m2.add_regressor('sell_price')\n    m2.fit(data2_tr)\n    return m2\n\n    ","0dd68920":"def make_prediction(m2,gs_comb,id1,train_end):\n      data = gs_comb[gs_comb['id']==id1]\n      data2 = data.rename({'snapshot_date':'ds','sales':'y'},axis=1)[['sell_price','ds','y']]\n      data2_tr = data2[data2['ds']<=train_end]\n      data2_val = data2[data2['ds']>train_end]\n      n_days_val = data2_val.shape[0]\n      future = m2.make_future_dataframe(periods = n_days_val)\n      future['sell_price'] = np.array(data['sell_price'])\n      median = data[data['snapshot_date']>train_end]['sell_price'].median(axis =0)\n      future['sell_price'] = future['sell_price'].fillna(median)\n      forecast2 = m2.predict(future)\n      return forecast2","c35323f6":"def make_validation_file(forecast2,id1):\n    item_id = id1\n    F_cols = np.array(['F'+str(i) for i in range(1,29)])\n    submission = pd.DataFrame(columns=F_cols)\n    submission.insert(0,'id',item_id)\n    forecast2['yhat'] = np.where(forecast2['yhat']<0,0,forecast2['yhat'])\n    forecast2.rename({'yhat':'y','ds':'ds'},inplace=True,axis = 1)\n    forecast2_T = forecast2[['ds','y']].T\n    submission.loc[1,'id'] =item_id\n    submission[F_cols] = forecast2_T.loc['y',:].values[-28:]\n    submission.head()\n    col_order = np.insert(F_cols,0,'id')\n    sub_val = submission[col_order]\n    #sub_val.to_csv('submission.csv',index = False)\n    return submission","f760198e":"#get_holidays(gs_comb,5)","f4f9bdd6":"#with Pool(cpu_count()) as p:\n        #forecast1 = p.map(run_prophet, [temp_series])","21ef1540":"def main1():\n    id_lst = sales_data['id'].unique().tolist()\n    train_end = split_train_val() \n    gs_snapshot = create_snapshot(calendar_data, sales_data)\n    price_cal = merge_price_cal_data(price_data,calendar_data)\n    gs_comb = merge_snapshot_pricecal(price_cal,gs_snapshot)\n    state,category = extract_id_info(id_lst[5])\n    print(state)\n    #a1 = get_snap_days(gs_comb,5)\n    hols = get_holidays(gs_comb,id_lst[5])\n    model = train_model(gs_comb,hols,id_lst[5],train_end)\n    forecast2 = make_prediction(model,gs_comb, id_lst[5],train_end)\n    submission = make_validation_file(forecast2,id_lst[5])\n    return submission","7b931e42":"def run_prophet(gs_comb,id1):\n    state,category = extract_id_info(id1)\n    hols = get_holidays(gs_comb,id1)\n    model = train_model(gs_comb,hols,id1,train_end)\n    forecast2 = make_prediction(model,gs_comb, id1,train_end)\n    submission = make_validation_file(forecast2,id1)\n    return submission","503603ec":"def main():\n    id_lst = sales_data['id'].unique().tolist()\n    train_end = split_train_val() \n    gs_snapshot = create_snapshot(calendar_data, sales_data,id_lst)\n    price_cal = merge_price_cal_data(price_data,calendar_data)\n    gs_comb = merge_snapshot_pricecal(price_cal,gs_snapshot)\n    #with Pool(4) as p:\n    #   submission = p.map(run_prophet(gs_comb), id_lst[0:10])\n    submission = Parallel(n_jobs=cpu_count(), verbose=3, backend=\"multiprocessing\")(\n             map(delayed(run_prophet(gs_comb)), id_lst[0:10]))\n    submission2 = pd.concat(submission,axis = 0)\n    submission2.to_csv('submission.csv',index = False)\n    end = time.time()\n    elapsed_time = end-start\n    time_taken = time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time))\n    print('time',time_taken)\n    return submission","7dbe9336":"main()","cd443be0":"# Extract snaps****","ea556aa3":"**Submission File**","eb1b6443":"# Train Model****","66e8deab":"# **Create Snapshot**"}}