{"cell_type":{"fcb713d5":"code","8f4c3b5b":"code","4757a284":"code","eb80b933":"code","cc271fbb":"code","2aefabd4":"code","1d132d28":"code","9290f26d":"code","1fec19be":"code","ec9d9bae":"code","7572d209":"code","60588d9e":"code","1cd4d5c1":"code","2fb9a47c":"code","4933f6a5":"code","d7fc2058":"code","39de0fad":"code","cc5d28f1":"code","c6faba69":"code","cd5d5387":"code","c741ec0d":"code","42ac003b":"code","205c49ea":"code","18055d6e":"code","d781fe81":"code","2eaa448d":"code","0779a9aa":"markdown","ff741010":"markdown","558c7dd7":"markdown"},"source":{"fcb713d5":"!pip install python-box\n","8f4c3b5b":"import os\nimport warnings\nfrom pprint import pprint\nfrom glob import glob\nfrom tqdm import tqdm\nimport torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.transforms as T\nfrom box import Box\nfrom sklearn.model_selection import StratifiedKFold\nfrom torchvision.io import read_image\nfrom torch.utils.data import DataLoader, Dataset\n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.utilities.seed import seed_everything\nfrom pytorch_lightning import callbacks\nfrom pytorch_lightning.callbacks.progress import ProgressBarBase\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\nfrom pytorch_lightning.loggers import TensorBoardLogger\nfrom pytorch_lightning import LightningDataModule, LightningModule\nfrom pytorch_lightning.loggers import WandbLogger\nfrom kaggle_secrets import UserSecretsClient\nfrom transformers import AutoTokenizer,AutoModel,AdamW\n\nimport wandb\nfrom datasets import load_dataset\nfrom sklearn.model_selection import train_test_split\n\n\nwarnings.filterwarnings(\"ignore\")","4757a284":"wandb.login()\n","eb80b933":"wandb_logger = WandbLogger(project=\"Contradictory_Watson\")","cc271fbb":"config = {'seed': 2021,\n          'root': '..\/input\/contradictory-my-dear-watson\/train.csv', \n          'n_splits': 3,\n          'epoch': 1,\n          'trainer': {\n              'gpus': 1,\n              'accumulate_grad_batches': 1,\n              'progress_bar_refresh_rate': 1,\n              'fast_dev_run': False,\n              'num_sanity_val_steps': 0,\n              'resume_from_checkpoint': None,\n          },\n          \n          \n          'train_loader':{\n              'batch_size': 64,\n              'shuffle': True,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': True,\n          },\n          'val_loader': {\n              'batch_size': 64,\n              'shuffle': False,\n              'num_workers': 4,\n              'pin_memory': False,\n              'drop_last': False\n         },\n          'model':{\n              'name': 'xlm-roberta-base',\n              'output_dim': 3\n          },\n          'optimizer':{\n              'name': 'optim.AdamW',\n              'params':{\n                  'lr': 1e-5\n              },\n          },\n          'scheduler':{\n              'name': 'optim.lr_scheduler.CosineAnnealingWarmRestarts',\n              'params':{\n                  'T_0': 20,\n                  'eta_min': 1e-4,\n              }\n          },\n          'loss': 'nn.BCEWithLogitsLoss',\n}\n\nconfig = Box(config)","2aefabd4":"df=pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")","1d132d28":"class Watson_Dataset(Dataset):\n    def __init__(self,df):\n        super().__init__()\n        self.tokenizer=AutoTokenizer.from_pretrained('xlm-roberta-base')\n        self.df=df  \n        \n    def __len__(self):\n        return(len(self.df))\n    \n    def __getitem__(self,idx):\n        a=self.tokenizer(self.df.loc[idx,'premise'],self.df.loc[idx,'hypothesis'],max_length=250,padding='max_length',return_tensors='pt')\n        return (a,self.df.loc[idx,'label'])\n        \n        ","9290f26d":"mydata=Watson_Dataset(df)","1fec19be":"mydata[0]","ec9d9bae":"class ConratdictoryDataModule(LightningDataModule):\n    def __init__(self,train_df,val_df):\n        super().__init__()\n        \n        self._train_df = train_df\n        \n       \n        self._val_df = val_df\n            \n        \n        \n        \n    def __create_dataset(self, train=True):\n        return (\n            Watson_Dataset(self._train_df)\n            if train\n            else Watson_Dataset(self._val_df)\n        )\n    \n    def train_dataloader(self):\n        dataset = self.__create_dataset(True)\n        return DataLoader(dataset,batch_size=8,shuffle=True)\n\n    \n   \n        \n    def val_dataloader(self):\n        dataset = self.__create_dataset(False)\n        return DataLoader(dataset,batch_size=2,shuffle=True)\n        \n        \n        ","7572d209":"train_df,val_df=train_test_split(df,test_size=0.2)","60588d9e":"train_df=train_df.reset_index()\nval_df=val_df.reset_index()","1cd4d5c1":"class XLM_Model(pl.LightningModule):\n    def __init__(self):\n        super().__init__()\n        self.model=AutoModel.from_pretrained('xlm-roberta-base').eval()\n        \n            \n            \n    def forward(self,input_ids,attention_mask,labels=None):\n        \n        out=self.model(input_ids=input_ids,attention_mask=attention_mask)\n        \n        return out\n    \n    def training_step(self,batch,batch_idx):\n        x=batch[0]\n        \n        y=batch[1]  \n        input_ids=x['input_ids'].squeeze(1)\n        \n        attention_mask=x['attention_mask'].squeeze(1)\n        labels=y\n        loss=self.model(input_ids=input_ids,attention_mask=attention_mask)\n        #self.log('train_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    def validation_step(self,batch,batch_idx):\n        x=batch[0]\n        y=batch[1]   \n        input_ids=x['input_ids'].squeeze(1)\n        attention_mask=x['attention_mask'].squeeze(1)\n        labels=y\n        loss=self.model(input_ids=input_ids,attention_mask=attention_mask,labels=labels)\n        #self.log('val_loss',loss,prog_bar=True,logger=True)\n        return loss\n    \n    \n    def configure_optimizers(self):\n        return AdamW(self.parameters(),lr=0.1)\n        \n        \n        ","2fb9a47c":"lr_monitor = callbacks.LearningRateMonitor()\nloss_checkpoint = callbacks.ModelCheckpoint(\nfilename=\"best_loss\",\nmonitor=\"val_loss\",\nsave_top_k=1,\nmode=\"min\",\nsave_last=False,\n)\n","4933f6a5":"trainer = pl.Trainer(\n    logger=wandb_logger,\n    callbacks=[lr_monitor, loss_checkpoint],\n\n    default_root_dir=\"\/checkpoints\",\n\n    max_epochs=5,\n\n    )","d7fc2058":"mymodel=XLM_Model()","39de0fad":"py_lightning_data=ConratdictoryDataModule(train_df,val_df)","cc5d28f1":"trainer.fit(mymodel,py_lightning_data)","c6faba69":"sample_dataloader = ConratdictoryDataModule(train_df, val_df).val_dataloader()\nbatch= iter(sample_dataloader).next()","cd5d5387":"x=batch[0]\ny=batch[0]","c741ec0d":"input_ids=x['input_ids'].squeeze(1)","42ac003b":"l=mymodel(x['input_ids'].squeeze(1),x['attention_mask'].squeeze(1))","205c49ea":"l.last_hidden_state.shape","18055d6e":"l=mymodel.model(input_ids,attention_mask,m)","d781fe81":"attention_mask=x['attention_mask'].squeeze(1)","2eaa448d":"attention_mask.shape","0779a9aa":"# Data","ff741010":"# Trainer","558c7dd7":"# Model"}}