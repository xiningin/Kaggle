{"cell_type":{"91fb98ad":"code","66023397":"code","594a8a82":"code","6a38b34e":"code","7d0d2f6f":"code","02ebc742":"code","329d3d2e":"code","f2aee8ae":"code","ef5e45fe":"code","0b7b9d3f":"code","93296c45":"code","9b41acb5":"code","651ed878":"code","9996cb6d":"markdown","12fe520d":"markdown"},"source":{"91fb98ad":"import gym\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time","66023397":"env = gym.make('FrozenLake-v1')\nenv.render()","594a8a82":"def eps_greedy(a, env, eps = 0.2):\n    \n    p = np.random.random()\n    \n    # Exploitation\n    if p < 1 - eps:\n        return a\n    \n    # Exploration\n    else:\n        return env.action_space.sample()\n    \n","6a38b34e":"from IPython.display import clear_output\n\ndef q_learning(env, epsilon = 0.1, gamma = 0.9, learning_rate = 0.1, num_games = 1000, max_iteration = 100, display = False):\n    \n    # Initialize Q-table\n    Q_table = np.zeros((env.observation_space.n, env.action_space.n))\n    \n    # Keep Track of How much Q values change\n    delta = []\n    \n    # Run the Game\n    for i in range(num_games):\n        \n        # initail Condition\n        state = env.reset()\n        done = False\n        \n        largest_change = 0\n        \n        # Run the Episode until either Reach Terminal node or maximum Iteration\n        for k in range(max_iteration):\n            \n            # The environment runs the chosen action(based on epsilon-greedy algo) and returns\n            # the next state and reward\n            \n            action = eps_greedy(np.argmax(Q_table[state, :]), env, epsilon)\n            next_state, reward, done, _ = env.step(action)\n            \n            if display:\n                clear_output(True)\n                print(f'Episode # {i}')\n                env.render()\n                time.sleep(.5)\n                \n            # update the Q-table using the Q-learning Formula\n            previous_Q = Q_table[state][action]\n            Q_table[state, action] = Q_table[state, action] * (1 - learning_rate) + learning_rate * (reward + gamma * max(Q_table[next_state, :]))\n            largest_change = max(largest_change, np.abs(previous_Q - Q_table[state][action]))\n            \n            # check if the episode is finished\n            if done:\n                if display:\n                    if k < max_iteration - 1:\n                        print('End of the Episode')\n                    \n                break\n            \n            state = next_state\n            \n        if display:\n            if k == max_iteration - 1:\n                print('End of the Episode')\n        \n        # Total Reward got from the Episode\n        delta.append(largest_change)\n        \n    # Calculate the value function and the final policy\n    value = np.zeros(env.observation_space.n)\n    policy = np.zeros(env.observation_space.n)\n    \n    for state in range(env.observation_space.n):\n        value[state] = np.max(Q_table[state])\n        policy[state] = np.argmax(Q_table[state])\n        \n    return value, policy, delta","7d0d2f6f":"#Run Q-Learning for 5 Episodes\ns_time = time.time()\nvalue, policy, delta = q_learning(env, epsilon = 0.1, gamma = 0.9, learning_rate = 0.1, num_games = 100, max_iteration = 10, display = True)\ne_time = time.time()\nprint('Run Time {} seconds'.format(e_time - s_time))","02ebc742":"#Run Q-Learning for 100000 Episodes\ns_time = time.time()\nvalue, policy, delta = q_learning(env, epsilon = 0.1, gamma = 0.99, learning_rate = 0.1, num_games = 150000, max_iteration = 100, display = False)\ne_time = time.time()\nprint('Run Time {} seconds'.format(e_time - s_time))","329d3d2e":"value","f2aee8ae":"plt.plot(delta)","ef5e45fe":"delta","0b7b9d3f":"env.render()","93296c45":"##### 0: left\n##### 1: down\n##### 2: right\n##### 3: up","9b41acb5":"#Run so many episodes\ndef run_episodes(env, policy, num_games = 1000):\n    \n    tot_rew = 0\n    state = env.reset()\n\n    for _ in range(num_games):\n        done = False\n        while not done:\n            #Select the action accordingly to the policy\n            next_state, reward, done, _ = env.step(policy[state])\n                \n            state = next_state\n            tot_rew += reward \n            if done:\n                state = env.reset()\n\n    print('Won {} of {} games!'.format(tot_rew, num_games))","651ed878":"run_episodes(env, policy, num_games = 1000)","9996cb6d":"## epsilon greedy ","12fe520d":"# Q_learning"}}