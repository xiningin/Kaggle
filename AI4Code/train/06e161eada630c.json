{"cell_type":{"b7211900":"code","4576458b":"code","e760e89b":"code","6833fbcf":"code","00d023e9":"code","f9118178":"code","c89811ce":"code","9c62de82":"code","0436fd73":"code","e3e3bc3a":"code","a205a6bf":"code","a11f85b6":"code","213afec8":"code","a49353d3":"code","7ffec57f":"code","0b2020e3":"code","c35011de":"code","6e7d0859":"code","40b9dc19":"code","0ff804d0":"code","dfa46d7c":"code","cd1c30d5":"code","61a58ee1":"code","e0537239":"code","8734025e":"code","af25d43f":"code","385c2ca6":"code","89b7f540":"code","775290d9":"code","2d864264":"code","b5943fca":"code","62a2caef":"code","4e2bafc4":"code","ef27e571":"code","de076a63":"code","7c1941bd":"code","1688d259":"code","28c13e44":"code","9dc55dc1":"code","022f35f4":"code","0f591281":"code","90d52295":"code","332eb19b":"code","f257ffe5":"code","9080a449":"code","f4349c04":"markdown","27e7a541":"markdown","8f79abc5":"markdown","bf3d2835":"markdown","a29b0a50":"markdown","056123a5":"markdown","c01c1c94":"markdown","41bd0193":"markdown","342f773b":"markdown","ffd16460":"markdown","09630640":"markdown","fb41f43e":"markdown","0e211e98":"markdown","e21c7b65":"markdown","ea7dea73":"markdown","b4536d8c":"markdown","ec811bf1":"markdown","10343071":"markdown"},"source":{"b7211900":"# install PEP8 package, for standard codding rules\n#!pip3 install flake8 pycodestyle_magic\n\n# Install XGBOOST model, not present in scikit learn\n!pip3 install xgboost\n!pip3 install catboost\n!pip3 install lightgbm\n\n# install encoder\n!pip3 install category_encoders\n\n# install the last scikit learn \n# package containing NA managment\n# for Ordinal encode\n!pip3 install scikit-learn==0.24","4576458b":"# Standard \nfrom pickle import Unpickler as Upck\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\nfrom itertools import combinations\n\n# Models\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn import tree\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.gaussian_process.kernels import RBF, Matern, RationalQuadratic, DotProduct\nfrom sklearn.neighbors import KNeighborsRegressor, RadiusNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.ensemble import RandomForestRegressor, BaggingRegressor, AdaBoostRegressor, GradientBoostingRegressor, VotingRegressor, StackingRegressor\nimport xgboost as xgb\nfrom xgboost import plot_importance\nfrom catboost import CatBoostRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.experimental import enable_iterative_imputer \nfrom sklearn.decomposition import PCA, KernelPCA\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn.manifold import TSNE\n\n\n# Preprocess data\nfrom sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, StandardScaler, normalize, PowerTransformer, RobustScaler, MinMaxScaler\nfrom category_encoders.cat_boost import CatBoostEncoder\nfrom category_encoders.target_encoder import TargetEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.model_selection import cross_val_score, ShuffleSplit, KFold, RepeatedKFold, LeaveOneOut, LeavePOut, StratifiedKFold, RepeatedStratifiedKFold, StratifiedShuffleSplit\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.impute import KNNImputer, IterativeImputer\nfrom sklearn.metrics import mean_squared_error, r2_score, silhouette_score\n\n# Statiscal tools\nfrom scipy.stats import normaltest, norm, gamma, chisquare, f_oneway, kruskal, spearmanr\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor  as vif \nfrom statsmodels.stats.diagnostic import acorr_ljungbox, kstest_normal\nfrom scipy.stats import normaltest, norm\n\n# Tensorflow for deep learning atempt\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, SimpleRNN, GRU\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Conv1D, Input\nfrom tensorflow.keras.layers import MaxPool1D, AveragePooling1D\nfrom tensorflow.keras.layers import TimeDistributed\nfrom tensorflow.keras.layers import Flatten, Dropout, BatchNormalization\nfrom tensorflow.keras.wrappers.scikit_learn import KerasRegressor\nfrom tensorflow.keras.callbacks import EarlyStopping\n\n# Option for dataframe display\npd.set_option('display.max_columns', 10000)\npd.set_option('display.max_rows', 10000)\n\n# PEP8 activation\n#%load_ext pycodestyle_magic\n#%pycodestyle_on","e760e89b":"# We retrieve the data\ndf_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv', sep=',')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv', sep=',')\n# we take apart the target and idx\ny = df_train[['SalePrice']]\nidx_test = df_test[['Id']]\nprint('shape of train: ',df_train.shape, 'shape of test: ', df_test.shape)","6833fbcf":"# Visualisation train\ndf_train.head()","00d023e9":"# Visualisation test\ndf_test.head()","f9118178":"# Check that there is no outlier or NA or Negative value \n# for the target 'SakePrice'\ndf_train['SalePrice'].describe()","c89811ce":"# Visualisation of 'SalePrice'\nplt.figure(figsize=(15, 5))\nplt.hist(df_train['SalePrice'].values, bins=np.linspace(0, df_train['SalePrice'].max(), 50), color=\"teal\", alpha=0.7)\nplt.xlabel(\"SalePrice\", fontsize=20)\nplt.ylabel(\"Frequency\", fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.title(\"Frequency of SalePrice\", fontsize=20)\nplt.show()","9c62de82":"# We can transform the target in order to reduce the variance\ny['SalePrice'] = np.log(df_train['SalePrice'].values + 1)\ndf_train['SalePrice'] = np.log(df_train['SalePrice'].values + 1)\n\n# test normality\npval = round(normaltest(df_train['SalePrice'].values)[1], 2)\n\n# Visualisation of 'SalePrice'\nplt.figure(figsize=(15, 5))\n_, b,_= plt.hist(df_train['SalePrice'].values, bins=np.linspace(df_train['SalePrice'].min(), df_train['SalePrice'].max(), 50), color=\"teal\", alpha=0.7, density=True)\nplt.plot(b, norm.pdf(b, loc=np.mean(df_train['SalePrice'].values), scale=np.std(df_train['SalePrice'].values)), color='r', label='Gaussian, p value={}'.format(pval))\nplt.xlabel(\"SalePrice\", fontsize=20)\nplt.ylabel(\"Frequency\", fontsize=20)\nplt.xticks(fontsize=15)\nplt.yticks(fontsize=15)\nplt.legend(fontsize=15)\nplt.title(\"Frequency of SalePrice\", fontsize=20)\nplt.show()","0436fd73":"# Normal test, Pvalue < 0.05 but we decide too suppress outliers\n'''df_train = df_train[df_train['SalePrice'] < 3]\ndf_train = df_train[df_train['SalePrice'] > -3]\ndf_train = df_train.dropna(subset=['SalePrice']).reset_index(drop=True)\ny = y[y['SalePrice'] < 3]\ny = y[y['SalePrice'] > -3]\ny = y.dropna(subset=['SalePrice']).reset_index(drop=True)'''","e3e3bc3a":"# type of columns\ndf_train.dtypes","a205a6bf":"# Count of NA values % for each columns\ndf_train.count()\/df_train.shape[0]","a11f85b6":"# We create a dict defining for each feature which \n# treatment\/transform we have to do (for categorical).\n# Ordinal or nominal\n\ndico = {'MSSubClass': ['object', 'nominal'], \n        'MSZoning': ['object', 'nominal'],\n        'LotFrontage': ['int', 'impute'], \n        'LotArea': ['int', 'impute'],\n        'Street': ['object', 'nominal'],\n        'Alley': ['object', 'NAreplace', 'nominal'], \n        'LotShape': ['object', 'nominal'], \n        'LandContour': ['object', 'ordinal', [np.array(['$$', 'Low', 'Lvl', 'Bnk', 'HLS'])]], \n        'Utilities': ['object', 'nominal'],\n        'LotConfig': ['object', 'nominal'],\n        'LandSlope': ['object', 'ordinal', [np.array(['$$','Gtl', 'Mod', 'Sev'])]],\n        'Neighborhood': ['object', 'nominal'], \n        'Condition1': ['object', 'nominal'], \n        'Condition2': ['object', 'nominal'], \n        'BldgType': ['object', 'nominal'], \n        'HouseStyle': ['object', 'nominal'],\n        'OverallQual': ['int', 'impute'], \n        'OverallCond' : ['int', 'impute'], \n        'YearBuilt': ['int', 'impute'], \n        'YearRemodAdd': ['int', 'impute'], \n        'RoofStyle': ['object', 'nominal'],\n        'RoofMatl': ['object', 'nominal'],\n        'Exterior1st': ['object', 'nominal'], \n        'Exterior2nd': ['object', 'nominal'],\n        'MasVnrType': ['object', 'nominal'], \n        'MasVnrArea': ['int', 'impute'],\n        'ExterQual': ['object', 'ordinal', [np.array(['$$','Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'ExterCond': ['object', 'ordinal', [np.array(['$$','Po', 'Fa', 'TA', 'Gd', 'Ex'])]],\n        'Foundation': ['object', 'nominal'],\n        'BsmtQual': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'BsmtCond': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'BsmtExposure': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'No', 'Mn', 'Av', 'Gd'])]],\n        'BsmtFinType1': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'])]] ,\n        'BsmtFinSF1': ['int', 'impute'],\n        'BsmtFinType2': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Unf', 'LwQ', 'Rec', 'BLQ', 'ALQ', 'GLQ'])]] ,\n        'BsmtFinSF2': ['int', 'impute'], \n        'BsmtUnfSF': ['int', 'impute'], \n        'TotalBsmtSF': ['int', 'impute'],\n        'Heating': ['object', 'nominal'], \n        'HeatingQC': ['object', 'ordinal', [np.array(['$$','Po', 'Fa', 'TA', 'Gd', 'Ex'])]],\n        'CentralAir': ['object', 'ordinal', [np.array(['$$','N', 'Y'])]], \n        'Electrical': ['object', 'nominal'], \n        '1stFlrSF': ['int', 'impute'], \n        '2ndFlrSF': ['int', 'impute'],\n        'LowQualFinSF': ['int', 'impute'], \n        'GrLivArea': ['int', 'impute'], \n        'BsmtFullBath': ['int', 'impute'], \n        'BsmtHalfBath': ['int', 'impute'],\n        'FullBath': ['int', 'impute'], \n        'HalfBath': ['int', 'impute'],\n        'BedroomAbvGr': ['int', 'impute'],\n        'KitchenAbvGr': ['int', 'impute'],\n        'KitchenQual': ['object', 'ordinal', [np.array(['$$','Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'TotRmsAbvGrd': ['int', 'impute'], \n        'Functional': ['object', 'ordinal', [np.array(['$$','Sal', 'Sev', 'Maj2', 'Maj1', 'Mod', 'Min2', 'Min1', 'Typ'])]], \n        'Fireplaces': ['int', 'impute'], \n        'FireplaceQu': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'GarageType': ['object', 'NAreplace', 'nominal'], \n        'GarageYrBlt': ['int', 'impute'], \n        'GarageFinish': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Unf', 'RFn', 'Fin'])]], \n        'GarageCars': ['int', 'impute'], \n        'GarageArea': ['int', 'impute'],  \n        'GarageQual': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Po', 'Fa', 'TA', 'Gd', 'Ex'])]],  \n        'GarageCond': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Po', 'Fa', 'TA', 'Gd', 'Ex'])]], \n        'PavedDrive': ['object', 'nominal'], \n        'WoodDeckSF': ['int', 'impute'], \n        'OpenPorchSF': ['int', 'impute'], \n        'EnclosedPorch': ['int', 'impute'], \n        '3SsnPorch': ['int', 'impute'], \n        'ScreenPorch': ['int', 'impute'],\n        'PoolArea': ['int', 'impute'], \n        'PoolQC': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'Fa', 'TA', 'Gd', 'Ex'])]],\n        'Fence': ['object', 'NAreplace', 'ordinal', [np.array(['$$','Noval', 'MnWw', 'GdWo', 'MnPrv', 'GdPrv'])]],\n        'MiscFeature': ['object', 'NAreplace', 'nominal'], \n        'MiscVal': ['int', 'impute'], \n        'MoSold': ['int', 'impute'],\n        'YrSold': ['int', 'impute'], \n        'SaleType': ['object', 'nominal'], \n        'SaleCondition': ['object', 'nominal']}","213afec8":"# We put NA to all values which are too rare \n# in each feature.\nlimit = 0.03\ndf_stat = df_train.copy()\nfor col in df_stat:\n    if df_stat[col].dtypes == 'object':\n        val_list = []\n        val = []\n        val_list = df_stat[col].value_counts().index.tolist()\n        val = df_stat[col].value_counts().values.tolist()\n        for i, v in zip(val_list, val):\n            if v\/df_stat.shape[0] <= limit:\n                df_stat[col] = df_stat[col].replace(i, np.nan)\n#check the link between categorical value and 'SalePrice'\n\n# link beteween categorical data and prices.\n# Kruskal test => we suppress feature with\n# p value > 0.05\nuseless = []\nfor col in df_stat.columns.values:\n    if df_stat[col].dtypes == 'object':\n        value = df_stat[col].value_counts().index.values\n        kruskal_data = []\n        if len(value) >= 2:\n            for val in value:\n                kruskal_data.append(df_stat[df_stat[col] == val]['SalePrice'].values)\n            _, p_value = kruskal(*kruskal_data)\n            print(col, ': ',p_value)\n            df_stat.boxplot(column='SalePrice', by=col, rot=90, fontsize=15, figsize=(15, 10))\n            \n            if p_value > 0.05:\n                useless.append(col)\n                \n# we delete col \nfor col in useless:\n    del(df_train[col])\n    del(df_test[col])","a49353d3":"useless","7ffec57f":"# we drop imediately qualitative feature with almost one unique value\n# So we delete categorial feature like \nfor col in df_train:\n    if col != 'SalePrice' and df_train[col].dtypes == 'object':\n        print(df_train[col].value_counts(), '\\n\\n')\n        \nlis = ['Street', 'Utilities', 'Condition2', 'RoofMatl', 'PoolQC']\nfor col in lis:\n    del(df_train[col])\n    del(df_test[col])","0b2020e3":"# Separate int\/float feature from cat\u00e9gorical feature.\nint_list = []\ncat_list = []\n\nfor k, v in dico.items():\n    if 'int' in v:\n        int_list.append(k)\n    else:\n        cat_list.append(k)\nint_list.append('SalePrice')\ncat_list.append('SalePrice')\n\nfor col in lis:\n    if col in int_list:\n        int_list.remove(col)\n    if col in cat_list:\n        cat_list.remove(col)\n        \nfor col in useless:\n    if col in int_list:\n        int_list.remove(col)\n    if col in cat_list:\n        cat_list.remove(col)","c35011de":"# we generate qualitative and quantitative dataframe for train and test \ndf_train_cat = pd.DataFrame(df_train[cat_list])\ndf_train_int = pd.DataFrame(df_train[int_list])\n\ncat_list_s = cat_list\ncat_list_s.remove('SalePrice')\nint_list_s = int_list\nint_list_s.remove('SalePrice')\n\ndf_test_cat = pd.DataFrame(df_test[cat_list_s])\ndf_test_int = pd.DataFrame(df_test[int_list_s])","6e7d0859":"# we start to replace NA value which are not real NA value.\n# like NA for garage doesn't mean that it's missing value\n# but only that there is no garage.\nfor col in cat_list:\n    if dico[col][1] == 'NAreplace':\n        df_train_cat[col] = df_train_cat[col].replace(np.nan, 'Noval')\n        df_test_cat[col] = df_test_cat[col].replace(np.nan, 'Noval')","40b9dc19":"## mask to record the actual NA value\n# positions\nmask_cat_train = df_train_cat.isna()\nmask_cat_test1 = df_test_cat.isna()\n\n# fill the NA value\nk1 = '$$'\nk2 =  '$$'\ndf_train_cat = df_train_cat.fillna(k1)\ndf_test_cat = df_test_cat.fillna(k2)","0ff804d0":"# Encode qualitative feature.\nfor k in df_train_cat:\n    if k != 'SalePrice':\n        if dico[k][1] == 'nominal' or dico[k][2] == 'nominal':\n\n            # Ordinal Encoder\n            X_train = df_train_cat[k].to_numpy()\n            X_test = df_test_cat[k].to_numpy()\n            # to manage unknown values on X_test\n            oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan)\n            df_train_cat[k] = oe.fit_transform(X_train.reshape(-1,1))\n            # to avoid dataleakage\n            df_test_cat[k] = oe.transform(X_test.reshape(-1,1))\n\n            \n        elif dico[k][1] == 'ordinal':\n            # Ordinal Encoder\n            X_train = df_train_cat[k].to_numpy()\n            X_test = df_test_cat[k].to_numpy()\n            # to manage unknown values on X_test\n            oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, categories=dico[k][2])\n            df_train_cat[k] = oe.fit_transform(X_train.reshape(-1,1))\n            # to avoid dataleakage\n            df_test_cat[k] = oe.transform(X_test.reshape(-1,1))\n         \n        \n        elif dico[k][2] == 'ordinal':\n            # Ordinal Encoder\n            X_train = df_train_cat[k].to_numpy()\n            X_test = df_test_cat[k].to_numpy()\n            # to manage unknown values on X_test\n            oe = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=np.nan, categories=dico[k][3])\n            df_train_cat[k] = oe.fit_transform(X_train.reshape(-1,1))\n            # to avoid dataleakage\n            df_test_cat[k] = oe.transform(X_test.reshape(-1,1))\n","dfa46d7c":"df_train_cat = df_train_cat.where(~mask_cat_train == True)\n\nmask_cat_test2 = df_test_cat.isna()\n# initial NA and NA generate on transform\nmask = np.logical_or(mask_cat_test1, mask_cat_test2)\ndf_test_cat = df_test_cat.where(~mask == True)","cd1c30d5":"# we supress the target 'SalePrice' in order to use \n# an imputer.\ndel(df_train_cat['SalePrice'])\ndel(df_train_int['SalePrice'])\n\n# we merge all data qualitive transformed and quantitative data.\ndf_train = pd.concat([df_train_int, df_train_cat], axis=1)\ndf_test = pd.concat([df_test_int, df_test_cat], axis=1)\ndf_train.shape","61a58ee1":"# Iterative Imputer, we impute missing value for the \n# quantitative features\nnum = df_train.columns.tolist()\n\n# model for imputer\nLr = LinearRegression()\nKnn = KNeighborsRegressor(n_neighbors=3)\nimputer = IterativeImputer(estimator = Lr, max_iter = 300, tol = 1e-3, verbose = 1, imputation_order = 'ascending') \n\n# fill NA value\ndf_train[num] =imputer.fit_transform(df_train[num])\ndf_test[num] = imputer.transform(df_test[num])","e0537239":"# Passage au Log\nfor col in df_train:\n    if col != 'SalePrice':\n        df_train[col] = np.log(df_train[col] + 1)\n        df_test[col] = np.log(df_test[col] + 1)","8734025e":"# we have to convert qualitative feature with nominale value.\n# Bayesian encoder.\nfor col in df_train_cat.columns:\n    if dico[col][1] == 'nominal' or dico[col][2] == 'nominal':\n        br = BayesianRidge()\n        br.fit(df_train[col].values.reshape(-1,1), y['SalePrice'].values)\n        df_train[col] = br.predict(df_train[col].values.reshape(-1,1))\n        df_test[col] = br.predict(df_test[col].values.reshape(-1,1))","af25d43f":"df_train.head()","385c2ca6":"# Visualization of correlation between quantitative feature and target.\nfor col in df_train:\n    if col != 'Id' and col != 'SalePrice':\n        r1 = np.corrcoef(df_train[col].values, y['SalePrice'].values)[0][1]\n        plt.figure(figsize=(10, 5))\n        plt.scatter(df_train[col].values, y['SalePrice'].values, marker='.', alpha=0.5, color = 'mediumseagreen', label=\"R: {}\".format(r1))\n        plt.xlabel('valeurs prises par {}'.format(col))\n        plt.ylabel('fr\u00e9quence de {}'.format(col))\n        plt.title('R\u00e9partition de {}'.format(col))\n        plt.legend()\n        plt.show()","89b7f540":"for col in ['MSSubClass', 'Neighborhood']:\n    rg = xgb.XGBRegressor()\n    rg.fit(df_train[col].values.reshape(-1,1), y['SalePrice'])\n    df_train[col] = rg.predict(df_train[col].values.reshape(-1,1))\n    df_test[col] = rg.predict(df_test[col].values.reshape(-1,1))","775290d9":"# feature selection => to suppress with corrolatin < 0.55\ncor = pd.concat([df_train, y], axis=1).corr()['SalePrice']\ncorlist = cor.where(cor < 0.55).dropna().index.tolist()\nfor col in corlist:\n    if col != 'SalePrice':\n        del(df_train[col])\n        del(df_test[col])","2d864264":"# Initial correlation matrix\ndf_train =  pd.concat([df_train, y], axis=1)\ndf_corr = df_train.corr()\ndf_corr.style.background_gradient(cmap='coolwarm').set_precision(2)","b5943fca":"del(df_train['SalePrice'])","62a2caef":"df_train.head()","4e2bafc4":"df_train.shape","ef27e571":"# prepare the data\n# Train\nX_train = df_train.values\ny_train = y['SalePrice'].values\n\n# Test\nX_test = df_test.values","de076a63":"# Cross Validation for XGBoost model only\nT0 = time.time()\n\ntuned_parameters_xg = {\"xgb__colsample_bytree\": [1],\n                       \"xgb__gamma\": [0, 0.5], \n                       \"xgb__reg_lambda\":[0, 0.3, 0.5, 1], \n                       \"xgb__reg_alpha\": [0, 0.3, 0.5, 1],\n                       \"xgb__learning_rate\": [0, 0.3, 0.5],\n                       \"xgb__max_depth\": [6],\n                       \"xgb__n_estimators\": [100, 300, 1000],\n                       \"xgb__subsample\": [1], \n                       \"xgb__booster\":['gbtree']}\n\n'''K = 3\ncv = KFold(n_splits = K, shuffle=True, random_state=0)\nreg_xg = Pipeline(steps=[('std', StandardScaler()), ('xgb', xgb.XGBRegressor())])\n\nR = GridSearchCV(estimator=reg_xg, param_grid=tuned_parameters_xg, cv = cv, scoring = 'neg_root_mean_squared_error', verbose=2)\nR.fit(X_train, y_train)\nprint(R.best_params_)\nprint(time.time()-T0)'''","7c1941bd":"params = {'colsample_bytree':1,\n          'gamma':0, \n          'reg_lambda':0,\n          'reg_alpha':1,\n          'learning_rate':0.3,\n          'max_depth':6,\n          'n_estimators':100,\n          'subsample':1}\n\nR = Pipeline(steps=[('std', StandardScaler()), ('xgb', xgb.XGBRegressor(**params))])\nR.fit(X_train, y_train)","1688d259":"# NOT USED\n'''\n# Resize input for convolutionnal compliance input shape\nX_train_m = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\nX_test_m = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\ny_train_m = y_train.reshape(y_train.shape[0], 1)\n\n# Model\nmodel_cnnnn = Sequential()\n#model.add(Input((None,X_train.shape[1], X_train.shape[2]), ragged=True))\nmodel_cnnnn.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', input_shape=(X_train_m.shape[1], X_train_m.shape[2])))\nmodel_cnnnn.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', ))\nmodel_cnnnn.add(Conv1D(filters=64, kernel_size=3, padding='same', activation='relu', ))\n#model_cnnLstm.add(Dropout(0.05))\nmodel_cnnnn.add(AveragePooling1D(pool_size=2))\n#model_cnnnn.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu', ))\n#model_cnnnn.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu', ))\n#model_cnnnn.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu', ))\n#model_cnnnn.add(Dropout(0.05))\n#model_cnnnn.add(MaxPool1D(pool_size=2))\n\nmodel_cnnnn.add(Flatten())\n#model_cnnLstm.add(LSTM(50, return_sequences=True))\nmodel_cnnnn.add(Dense(100))\nmodel_cnnnn.add(Dense(100))\n#model_cnnnn.add(Dense(50))\n#model_cnnnn.add(Dense(500))\n#model_cnnnn.add(Dense(500))\nmodel_cnnnn.add(Dense(1))\nmodel_cnnnn.compile(optimizer='sgd', loss='mean_squared_logarithmic_error')\n\nepochs = 300\n\nhistory = model_cnnnn.fit(X_train_m, y_train_m, epochs=epochs, batch_size=5)\n'''","28c13e44":"#model_cnnnn.summary()","9dc55dc1":"# prediction on X-train\n\n# windows display\nstart = 0\nend = 250\n\n# Inverse transformation\ny_pred = np.exp(R.predict(X_train)) - 1\ny_true = np.exp(y_train) - 1\n\n# Display\nX = range(start,end)\nplt.figure(figsize=(30, 10))\nplt.plot(X, y_pred[start:end], alpha=0.8, color = 'teal', lw=3, label=\"y_pred\")\n#plt.plot(X, y_pred_nn[start:end], alpha=0.8, color = 'mediumseagreen', label=\"y_pred_nn\")\nplt.plot(X, y_true[start:end], alpha=0.8, color = 'red', lw=3, label=\"y_train\")\nplt.xlabel(\"X_train\", fontsize=20)\nplt.ylabel(\"SalePrice\", fontsize=20)\nplt.title(\"Prediction on X_train\", fontsize=20)\nplt.legend(fontsize=20)\nplt.show()","022f35f4":"r2_score(y_true, y_pred)","0f591281":"# Correlation between y_true and y_pred\nr = np.corrcoef(y_pred, y_true)[0,1]\nplt.figure(figsize=(20, 10))\nplt.scatter(y_true, y_pred, marker='.', alpha=0.5, color = 'dodgerblue', label=\"corr: {0:.2f}\".format(r))\nplt.xlabel(\"y_true\", fontsize=20)\nplt.ylabel(\"y_pred\", fontsize=20)\nplt.title(\"Correlation y_pred and y_true\", fontsize=20)\nplt.legend(fontsize=20)\nplt.show()","90d52295":"# Normal test (Kolmogorov Smirnoff)\n# LjungBox test\n\n# Residuals\nresid = y_true - y_pred\n_, pval1 = kstest_normal(y_true-y_pred, dist='norm')\n_, pval2 = acorr_ljungbox(y_true - y_pred, lags=[9])\n\n# Dislplay\nplt.figure(figsize=(20,5))\n_, b, _ = plt.hist(resid, bins=np.linspace(resid.min(), resid.max(), 80), alpha=0.7, color='teal', density=True, label='Normal test pval: {0:.2f}\\nLjungBox test pval: {0:.2f}'.format(pval1, pval2))\nplt.plot(b, norm.pdf(b, loc=np.mean(resid), scale=np.std(resid)), color='r', label='Gaussian')\nplt.axvline(x=0, color='black', lw=3, ls=':')\nplt.title(\"R\u00e9partion des r\u00e9sidus\", fontsize=20)\nplt.xlabel(\"valeur r\u00e9sidus\", fontsize=20)\nplt.ylabel(\"Fr\u00e9quence\", fontsize=20)\nplt.legend(fontsize=15)\nplt.show()","332eb19b":"resid.std()","f257ffe5":"# Prediction on X_test\ny_pred = np.exp(R.predict(X_test)) - 1 \n\n# DataFrame creation with prediction\ndf = pd.DataFrame()\ndf['Id'] = idx_test['Id'].values\ndf['SalePrice'] = y_pred\ndf.head(10)","9080a449":"# Creation files.csv\ndf.to_csv('submission.csv', index=False)","f4349c04":"### train and test data","27e7a541":"### Quantitative data analysis","8f79abc5":"# We split quantitative and qualitative data for separate analysis","bf3d2835":"# GridSearchCV","a29b0a50":"# Data","056123a5":"## Categorical data\n### transform qualitative nominal data with 'catboost encoder'","c01c1c94":"#### CV for XGBoost only","41bd0193":"### Correlation matrix to select quantitative feature (we won't use it for feature selection)\n### we would prefer use model.feature_importance_ (from XGBoost model)","342f773b":"### Normality test on residuals","ffd16460":"### Other model with convolutional neural network (1D), (NOT USED)","09630640":"## Install package","fb41f43e":"### Correlation","0e211e98":"# Predictions and Residuals","e21c7b65":"### Display prediction on train (with XGBoost only)","ea7dea73":"# Submit","b4536d8c":"### transform qualitative ordinal data with 'Ordinal encoder'","ec811bf1":"### Correlation between y_true and y_pred","10343071":"### Cross validation and hyper parameter tuning for model"}}