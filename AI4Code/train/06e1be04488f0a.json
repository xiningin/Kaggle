{"cell_type":{"47b34962":"code","2d40b566":"code","97e20898":"code","9f7edfcd":"code","d666c3d2":"code","24c60258":"code","bd216da9":"code","dc76caf8":"code","6f993331":"code","c2e126c2":"code","0b53ca38":"code","bb11507a":"code","5d4d8a05":"code","46ff1e53":"code","212594db":"code","85db71e2":"code","7b51d7a4":"code","6a28eaad":"code","df7f5c2f":"markdown","9f4bee6e":"markdown","5100b6f6":"markdown","eaab1796":"markdown"},"source":{"47b34962":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2d40b566":"test_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')","97e20898":"os.listdir('\/kaggle\/input')\nmodel_path = '..\/input\/tf-roberta\/'\nweights_path = '..\/input\/tfrobertaweights\/tf_roBERTa-weights\/'","9f7edfcd":"weights_list = []\nweights_list = list(os.listdir(weights_path))\n#weights_list","d666c3d2":"weights_path_latest = '..\/input\/v18robertaweights\/'#'..\/input\/tweet-sentiment-roberta-tpu-version\/'\nweights_path_latest_list = []\nfor root, dirs, files in os.walk(weights_path_latest):\n    for file in files:\n        if file.endswith(\".h5\"):\n             #print(os.path.join(root, file))\n            weights_path_latest_list.append(file)","24c60258":"#weights_list\n#weights_list=pd.concat ( pd.DataFrame(weights_path_latest_list))\n#weights_list\n","bd216da9":"#weights_path_latest_list = list(os.listdir(weights_path_latest))\n#weights_list.append (weights_path_latest_list)\n#weights_list.sort()\n#print (weights_path_latest_list)","dc76caf8":"import tensorflow as tf\nfrom transformers import *\nimport tokenizers\nimport tensorflow.keras.backend as K\nfrom sklearn.model_selection import StratifiedKFold\nprint('TF version',tf.__version__)","6f993331":"MAX_LEN = 96\n\ntokenizer = tokenizers.ByteLevelBPETokenizer(\n    vocab_file=model_path+'vocab-roberta-base.json', \n    merges_file=model_path+'merges-roberta-base.txt', \n    lowercase=True,\n    add_prefix_space=True\n)\nsentiment_id = {'positive': 1313, 'negative': 2430, 'neutral': 7974}","c2e126c2":"ct = test_df.shape[0]\ninput_ids_t = np.ones((ct,MAX_LEN),dtype='int32')\nattention_mask_t = np.zeros((ct,MAX_LEN),dtype='int32')\ntoken_type_ids_t = np.zeros((ct,MAX_LEN),dtype='int32')\n\nfor k in range(test_df.shape[0]):\n        \n    # INPUT_IDS\n    text1 = \" \"+\" \".join(test_df.loc[k,'text'].split())\n    enc = tokenizer.encode(text1)                \n    s_tok = sentiment_id[test_df.loc[k,'sentiment']]\n    input_ids_t[k,:len(enc.ids)+5] = [0] + enc.ids + [2,2] + [s_tok] + [2]\n    attention_mask_t[k,:len(enc.ids)+5] = 1","0b53ca38":"def build_model():\n    ids = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    att = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n    tok = tf.keras.layers.Input((MAX_LEN,), dtype=tf.int32)\n\n    config = RobertaConfig.from_pretrained(model_path+'config-roberta-base.json')\n    bert_model = TFRobertaModel.from_pretrained(model_path+'pretrained-roberta-base.h5',config=config)\n    x = bert_model(ids,attention_mask=att,token_type_ids=tok)\n    \n    x1 = tf.keras.layers.Dropout(0.15)(x[0]) \n    #x1 = tf.keras.layers.Conv1D(1,1)(x1)\n    #x1 =  tf.keras.layers.Conv1D(2048,2, padding='same')(x1)\n    #x1 =  tf.keras.layers.Conv1D(1024,2, padding='same')(x1)\n    x1 =  tf.keras.layers.Conv1D(512,2, padding='same')(x1)\n    x1 = tf.keras.layers.Conv1D(256, 2,padding='same')(x1)\n    x1 = tf.keras.layers.Dense(1)(x1)\n    x1 = tf.keras.layers.Flatten()(x1)\n    x1 = tf.keras.layers.Activation('softmax')(x1)\n    \n    x2 = tf.keras.layers.Dropout(0.15)(x[0]) \n    #x2 = tf.keras.layers.Conv1D(1,1)(x2)\n    #x2 =  tf.keras.layers.Conv1D(2048,2, padding='same')(x2)\n    #x2 =  tf.keras.layers.Conv1D(1024,2, padding='same')(x2)\n    x2 =  tf.keras.layers.Conv1D(512,2, padding='same')(x2)\n    x2 = tf.keras.layers.Conv1D(256, 2,padding='same')(x2)\n    x2 = tf.keras.layers.Dense(1)(x2)\n    x2 = tf.keras.layers.Flatten()(x2)\n    x2 = tf.keras.layers.Activation('softmax')(x2)\n\n    model = tf.keras.models.Model(inputs=[ids, att, tok], outputs=[x1,x2])\n    optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n    model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n\n    return model","bb11507a":"#for i, fl in enumerate(weights_list):\n#    print  (weights_list[i])\nfor i, fl in enumerate(weights_path_latest_list):\n    print  (weights_path_latest_list[i])","5d4d8a05":"#VER = 'v18-fd'\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nfor i, fl in enumerate(weights_path_latest_list):\n    \n    K.clear_session()\n    \n    model = build_model()\n        \n    print('Loading model...')\n    #model.load_weights(weights_path + 'v13-fd%i-roberta-wt.h5'%(i+1))\n    model.load_weights(weights_path_latest + weights_path_latest_list[i])    \n    print(f'Predicting Test...using weights...{weights_path_latest_list[i]}')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n    preds_start += preds[0]\/len(weights_path_latest_list)\n    preds_end += preds[1]\/len(weights_path_latest_list)","46ff1e53":"'''\n#VER = 'v13-fd'\npreds_start = np.zeros((input_ids_t.shape[0],MAX_LEN))\npreds_end = np.zeros((input_ids_t.shape[0],MAX_LEN))\nfor i, fl in enumerate(weights_list):\n    \n    K.clear_session()\n    \n    model = build_model()\n        \n    print('Loading model...')\n    #model.load_weights(weights_path + 'v13-fd%i-roberta-wt.h5'%(i+1))\n    model.load_weights(weights_path + weights_list[i])    \n    print(f'Predicting Test...using weights...{weights_list[i]}')\n    preds = model.predict([input_ids_t,attention_mask_t,token_type_ids_t],verbose=1)\n    preds_start += preds[0]\/len(weights_list)\n    preds_end += preds[1]\/len(weights_list)\n'''","212594db":"submission = []\nfor i in range(input_ids_t.shape[0]):\n    a = np.argmax(preds_start[i,])\n    b = np.argmax(preds_end[i,])\n    if a>b: \n        st = test_df.loc[i,'text']\n    else:\n        text1 = \" \"+\" \".join(test_df.loc[i,'text'].split())\n        enc = tokenizer.encode(text1)\n        st = tokenizer.decode(enc.ids[a-1:b])\n    submission.append(st)\n","85db71e2":"test_df['selected_text'] = submission\npd.set_option('max_colwidth', 50)\n","7b51d7a4":"submission_df = test_df[['textID','selected_text']]\nsubmission_df.sample(25)","6a28eaad":"submission_df.to_csv('submission.csv',index=False)\nprint ('Submitted Successfully.....')","df7f5c2f":"### Reference kernels\n> [Chris Deotte's Tensorflow Roberta](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705)\n\n> [Discussion Forums](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/143281)","9f4bee6e":"![image.png](attachment:image.png)","5100b6f6":"### highest score CV .703   LB 0.711\n> Training version 20 :\n> seed 42, optimizer = adam,  lr = 3e-5  , loss = 'categorical_crossentropy'\n> ","eaab1796":"1. This notebook is the variation of Chris Deotte's [Tensorflow Roberta](https:\/\/www.kaggle.com\/cdeotte\/tensorflow-roberta-0-705) where I converted the code to TPU following this [discussion](https:\/\/www.kaggle.com\/c\/tweet-sentiment-extraction\/discussion\/143869)\n1. This notebook covers the inference part of the Tweet Sentiment Extraction test dataset. Please follow this [(Training) kernel](https:\/\/www.kaggle.com\/meenakshiramaswamy\/tweet-sentiment-roberta-tpu-inference) to check the training part with multiple folds.\n1. I have used the Pretrained Roberta model and vocab file that are downloaded by  [Chris Deotte](https:\/\/www.kaggle.com\/cdeotte\/tf-roberta)\n1. Here's the link to the trained multiple fold weights  [dataset](https:\/\/www.kaggle.com\/meenakshiramaswamy\/v18robertaweights)\n\n\n\n"}}