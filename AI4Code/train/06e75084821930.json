{"cell_type":{"9f3ce07a":"code","4898b3c5":"code","ecfbd93f":"code","171d9e4c":"code","6474e31f":"code","1709a5fc":"code","30603ecd":"code","180d1a72":"code","4e9dc631":"code","94fa7729":"code","a658b145":"code","f22d9971":"code","a980807d":"code","2fea9ac6":"code","7ab6b19f":"code","176b063f":"code","455288fe":"code","9c5e1282":"code","2317c864":"code","d0452a9a":"code","7f7d298e":"code","cc6e46b6":"markdown","7402875e":"markdown","69f87fd1":"markdown","1d7461eb":"markdown","3c6ad892":"markdown","579993b1":"markdown","a7067308":"markdown","c4a0bc68":"markdown","986b6ee6":"markdown","c4f75e81":"markdown","a7c96fdd":"markdown","b6d242b3":"markdown","1055fadc":"markdown","0c04e30b":"markdown"},"source":{"9f3ce07a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport sklearn\nimport seaborn as sns\nimport tensorflow as tf\nimport keras_tuner as kt\nfrom keras_tuner import RandomSearch\nfrom tensorflow import feature_column\nfrom sklearn import model_selection\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split, StratifiedKFold","4898b3c5":"train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","ecfbd93f":"train.head()","171d9e4c":"test.head()","6474e31f":"train.isnull().sum()","1709a5fc":"test.isnull().sum()","30603ecd":"train[\"Cabin\"] = train[\"Cabin\"].replace(np.NAN,  \"Unknown\")\ntrain[\"Embarked\"] = train[\"Embarked\"].replace(np.NAN, \"Unknown\")\ntrain[\"Age\"] = train[\"Age\"].replace(np.NAN, train[\"Age\"].median())\ntest[\"Cabin\"] = test[\"Cabin\"].replace(np.NAN, \"Unknown\")\ntest[\"Age\"] = test[\"Age\"].replace(np.NAN, test[\"Age\"].median())\ntest[\"Fare\"] = test[\"Fare\"].replace(np.NAN, test[\"Fare\"].median())","180d1a72":"train.head()","4e9dc631":"cabin_labels = sorted(set(list(train[\"Cabin\"].unique()) + list(test[\"Cabin\"].unique())))\nprint(cabin_labels[:30])","94fa7729":"train[\"Cabin_type\"] = train[\"Cabin\"].apply(lambda cabin: cabin[0])\ntest[\"Cabin_type\"] = test[\"Cabin\"].apply(lambda cabin: cabin[0])","a658b145":"train[\"family_member_size\"] = 1 + train[\"SibSp\"] + train[\"Parch\"]\ntest[\"family_member_size\"] = 1 + test[\"SibSp\"] + test[\"Parch\"]","f22d9971":"train.head()","a980807d":"categorical_columns = [\"Pclass\", \"Sex\", \"Embarked\", \"Cabin_type\"]\nnumeric_columns = [\"Age\", \"family_member_size\", \"Fare\"]\ncategorical_features = [\n    feature_column.indicator_column(\n        feature_column.categorical_column_with_vocabulary_list(key, sorted(list(train[key].unique())))\n    ) for key in categorical_columns\n]\nnumerical_features = [feature_column.numeric_column(key) for key in numeric_columns]\ninput_dictionary = dict()\ninputs = dict()\nfor item in numerical_features:\n    inputs[item.key] = tf.keras.layers.Input(name=item.key, shape=(), dtype=\"float64\")\nfor item in categorical_features:\n    dtype = None\n    if train[item.categorical_column.key].dtype == object:\n        dtype = tf.string\n    else:\n        dtype = tf.int64\n    inputs[item.categorical_column.key] = tf.keras.layers.Input(name=item.categorical_column.key, shape=(), dtype=dtype)","2fea9ac6":"for column in numeric_columns:\n    mean = train[column].mean()\n    std = train[column].std()\n    train[column] = (train[column] - mean) \/ std\n    test[column] = (test[column] - mean) \/ std","7ab6b19f":"class BinaryCrossEntropyWithWeights(tf.keras.losses.Loss):\n\n    def __init__(self, negative_weights, positive_weights):\n        super().__init__()\n        self.negative_weights = negative_weights\n        self.positive_weights = positive_weights\n        \n    def call(self, y_true, y_pred):\n        y_true = tf.cast(y_true, y_pred.dtype)\n        pos = self.positive_weights * y_true * tf.math.log(y_pred + tf.keras.backend.epsilon())\n        neg = self.negative_weights * (1.0 - y_true) * tf.math.log(1.0 - y_pred + tf.keras.backend.epsilon())\n        return -(pos + neg)","176b063f":"def get_weights():\n    total = train.shape[0]\n    pos_count = (train[\"Survived\"] == 1).sum()\n    neg_count = (train[\"Survived\"] == 0).sum()\n    negtive_weights =  pos_count \/ total\n    positive_weights = neg_count \/ total\n    return negtive_weights, positive_weights","455288fe":"negtive_weights, positive_weights = get_weights()","9c5e1282":"def build_model(hp):\n    bce_with_weights = BinaryCrossEntropyWithWeights(negtive_weights, positive_weights)\n    x = tf.keras.layers.DenseFeatures(numerical_features + categorical_features)(inputs)\n    for i in range(hp.Int(\"depth\", min_value=3, max_value=8)):\n        x = tf.keras.layers.Dense(hp.Choice(\"width\", values=[4, 8, 16, 32, 64]), activation=tf.nn.gelu)(x)\n        x = tf.keras.layers.Dropout(hp.Choice(\"dropout\", values=[0.1, 0.2, 0.3, 0.4, 0.5]))(x)\n    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(x)\n    model = tf.keras.Model(inputs=list(inputs.values()), outputs=output)\n    optimizer = tf.keras.optimizers.Adam(hp.Float(\"learning_rate\", min_value=1e-5, max_value=5e-3))\n    model.compile(loss=bce_with_weights, optimizer=optimizer, metrics=[\"accuracy\"])\n    return model","2317c864":"batch_size = 32\nkfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\ntuners = []\nfor index, (train_indices, valid_indices) in enumerate(kfold.split(train, train[\"Survived\"])):\n    fold = index + 1\n    train_features = train.iloc[train_indices]\n    val_features = train.iloc[valid_indices]\n    train_file_name = \"train_fold_%d.csv\"%(fold)\n    val_file_name = \"val_fold_%d.csv\"%(fold)\n    train_features.to_csv(train_file_name, index=False)\n    val_features.to_csv(val_file_name, index=False)\n    train_ds = tf.data.experimental.make_csv_dataset(train_file_name, batch_size=batch_size, label_name=\"Survived\", shuffle=True)\n    val_ds = tf.data.experimental.make_csv_dataset(val_file_name, batch_size=val_features.shape[0], label_name=\"Survived\", shuffle=False)\n    tuner = kt.BayesianOptimization(\n        build_model,\n        objective='val_accuracy',\n        max_trials=50,\n        directory=\"fold_%d\"%(fold)\n    )\n    reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.2, patience=3, min_lr=1e-5)\n    early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"val_accuracy\", patience=10)\n    checkpoint_path = \"model_fold_%d.h5\"%(fold)\n    checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path, save_best_only=True)\n    tuner.search(train_ds.take(train_features.shape[0] \/\/ batch_size + 1), epochs=10, validation_data=val_ds.take(1), callbacks=[reduce_lr, early_stop], verbose=0)\n    tuners.append(tuner)","d0452a9a":"for tuner in tuners:\n    best_hp = tuner.get_best_hyperparameters()[0]\n    print(best_hp.get_config()[\"values\"])\n    model = tuner.hypermodel.build(best_hp)\n    model.summary()\n    print(model.summary)","7f7d298e":"test_path = \"test.csv\"\ntest_file = test.to_csv(test_path, index=False)\ntest_ds = tf.data.experimental.make_csv_dataset(test_path, batch_size=test.shape[0], shuffle=False)\ny_pred = np.mean([tuner.get_best_models()[0].predict(test_ds.take(1)).reshape(-1) for tuner in tuners], axis=0)\nprint(y_pred.shape)\ny_pred = np.array(y_pred >= 0.5, dtype=int)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n})\nsubmission.to_csv(\"submission.csv\", index=False)","cc6e46b6":"Let's see the Cabin labels, there are so many of them. But I make an assmption that the First Alphabet matters, it indicated the location and class of the Passengers so it had an impact to survive.","7402875e":"## Data Wrangling","69f87fd1":"### Best parameters","1d7461eb":"## Model Development","3c6ad892":"## Hyper Parameter Tuning","579993b1":"## Data Preprocessing","a7067308":"## Submission","c4a0bc68":"### Add Family Member Size Feature","986b6ee6":"As we can see Age, Cabin and Fare information contains missing values, so we need to apply Missing Value Imputation to them. I will repalce missing categorical values with an unknown category and missing numerical value with median of the feature they belong to.","c4f75e81":"### Add Cabin Type Feature","a7c96fdd":"## Import datasets","b6d242b3":"We can indicate Family Member Size by SibSp and Parch feature: ","1055fadc":"# Titanic Prediction with KerasTuner\n## Table of Contents\n- Summary\n- Import Packages\n- Import datasets\n- Data Wrangling\n- Data Preprocessing\n    - Add Cabin Type Feature\n    - Add Family Member Size Feature\n- Model Development\n- Submission\n\n## Summary\n\nIn this Notebook I will build a DNN with the help of [KeraTuner](https:\/\/keras.io\/keras_tuner\/).","0c04e30b":"## Import Packages"}}