{"cell_type":{"3d86f0ca":"code","6803f85f":"code","80470ee5":"code","ff5f09c1":"code","ff830626":"code","3418c88c":"code","1b81e887":"code","dd3bb4d6":"code","896d51d3":"code","9377d2e3":"code","29574953":"code","340154c1":"code","e2e5d8c5":"code","476f9407":"code","8c619f73":"code","1dd81e50":"code","ec810308":"code","71dfa763":"code","5e9d633e":"code","091c431b":"code","546b4ac2":"code","38eaebd2":"code","e939d354":"code","1dbbf2d0":"code","3c0e0bb5":"code","9b65978c":"code","f9e94dd6":"code","43d5539d":"code","a89d1ef8":"code","3e1575bb":"code","db8c79a0":"code","2bae11f4":"code","a97a74d2":"code","91b6bdc6":"code","eddbcfee":"code","92587df6":"code","188c128e":"code","2bff0c86":"markdown","3fe1a7cc":"markdown","e0fe1923":"markdown","6d5b1aae":"markdown","fcc4a606":"markdown","3613bb97":"markdown","c42ac282":"markdown","4feaf2d9":"markdown","82a9161f":"markdown","982742ed":"markdown","a503ef0c":"markdown","efafd6ce":"markdown","4709356a":"markdown","1da551ae":"markdown","5ffa9f45":"markdown","17b3f0aa":"markdown","dcef419d":"markdown","e90d6665":"markdown","aaac5ff7":"markdown","fb8e9ab8":"markdown","2a3defeb":"markdown"},"source":{"3d86f0ca":"# import libraries we need for EDA\n%matplotlib inline\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n#import libraries we need for predictions\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.base import BaseEstimator, RegressorMixin\nfrom sklearn import metrics","6803f85f":"# importing our file\nstudents_df = pd.read_csv('..\/input\/StudentsPerformance.csv')","80470ee5":"# Look at the top 5 rows, uncomment the line below to run the code\n#students_df.head()","ff5f09c1":"# Look at the last 5 rows, uncomment the line below to run the code\n#students_df.tail()","ff830626":"# Let's take a look on columns, shape and descriptive information of our data set\n# uncomment the line below to run the code\n#students_df.columns","3418c88c":"# Shape of our dataset\n# uncomment the line below to run the code\n#students_df.shape","1b81e887":"students_df.info()","dd3bb4d6":"# Summary statistics of our numeric columns of entire dataset\nstudents_df.describe()","896d51d3":"categorical_features = ['gender', 'race\/ethnicity', 'parental level of education', 'lunch', 'test preparation course']","9377d2e3":"# Counts on categorical columns\nfor feature in categorical_features:\n    print(feature,':')\n    print(students_df[feature].value_counts())\n    print('----------------------------')","29574953":"fig, axes = plt.subplots(3,2, figsize=(12,12))\n\ndef get_x_labels(column):\n    # helper function to get all xlabels for all axes\n    col_dict = dict(students_df[column].value_counts())\n    return col_dict.keys()\n\nx_labels = [list(get_x_labels(feature)) for feature in categorical_features]\n\ndef get_y_ticks(column):\n    # helper function to get all heights for all axes\n    return students_df[column].value_counts()\n\ny_ticks = [list(get_y_ticks(feature)) for feature in categorical_features]\n\nfor i in range(3):\n    for j in range(2):\n        if i==1:\n            axes[i,j].bar(x_labels[i+j+1], y_ticks[i+j+1])\n            axes[i,j].set_frame_on(False)\n            axes[i,j].set_xticklabels(x_labels[i+j+1], rotation=45)\n            axes[i,j].set_title('{} Counts'.format(categorical_features[i+j+1].capitalize()))\n            axes[i,j].minorticks_off()\n        elif i==2:\n            axes[i,j].bar(x_labels[i+j+1], y_ticks[i+j+1])\n            axes[i,j].set_frame_on(False)\n            axes[i,j].set_xticklabels(x_labels[i+j+1], rotation=45)\n            axes[i,j].set_title('{} Counts'.format(categorical_features[i+j+1].capitalize()))\n        else:\n            axes[i,j].bar(x_labels[i+j], y_ticks[i+j])\n            axes[i,j].set_frame_on(False)\n            axes[i,j].set_xticklabels(x_labels[i+j], rotation=45)\n            axes[i,j].set_title('{} Counts'.format(categorical_features[i+j].capitalize()))\nplt.tight_layout()\nplt.show()","340154c1":"#To help you understand better, what we mean by figure and by axes\n#below I use a very nice way to clarify those. (I saw it first time \n#in LinkedIn by Ted Petrou).\n\nfig_new, ax = plt.subplots()\n\nfig_new.set_facecolor('tab:cyan') #our paper has cyan color, our axes in white color","e2e5d8c5":"ax.set_facecolor('tab:green') #our axes now has green color\nfig_new","476f9407":"numeric_features = ['math score', 'reading score', 'writing score']","8c619f73":"# First of all let us take a look on \n# the distribution of each numeric column\n\nfor feature in numeric_features:\n    students_df[feature].plot(kind='hist', bins=20)\n    plt.title('{} Distribution'.format(feature))\n    plt.show()","1dd81e50":"# We print all the minimum values for each numeric feature\n\nprint('The minimum score for Maths is: {}'.format(students_df['math score'].min()))\nprint('The minimum score for Reading is: {}'.format(students_df['reading score'].min()))\nprint('The minimum score for Writing is: {}'.format(students_df['writing score'].min()))","ec810308":"students_df.boxplot(column=numeric_features, by='gender', rot=45, figsize=(15,6), layout=(1,3));","71dfa763":"students_df.boxplot(column=numeric_features, by='test preparation course', rot=45, figsize=(15,6), layout=(1,3));","5e9d633e":"students_df.boxplot(column=numeric_features, by='parental level of education', rot=90, figsize=(15,6), layout=(1,3));","091c431b":"students_df.boxplot(column=numeric_features, by='race\/ethnicity', rot=45, figsize=(15,6), layout=(1,3));","546b4ac2":"students_df.boxplot(column=numeric_features, by='lunch', rot=45, figsize=(15,6), layout=(1,3));","38eaebd2":"# We are going to split our dataset to smaller,\n# one for each category, and compare their statistics\n# with the overall statistics.\n\ndf_compl = students_df[students_df['test preparation course'] == 'completed']\ndf_notcompl = students_df[students_df['test preparation course'] == 'none']","e939d354":"# A good way to decide if and how the test preparation course helped,\n# is to compare the mean values of our two subsets to the entire dataset\nprint(students_df.mean() - df_compl.mean())\nprint(students_df.mean() - df_notcompl.mean())","1dbbf2d0":"print(students_df.std() - df_compl.std())\nprint('--------------')\nprint(students_df.std() - df_notcompl.std())","3c0e0bb5":"df_BD = students_df[students_df['parental level of education'] == \"bachelor's degree\"]\ndf_MD = students_df[students_df['parental level of education'] == \"master's degree\"]\ndf_sc = students_df[students_df['parental level of education'] == 'some college']\ndf_AD = students_df[students_df['parental level of education'] == \"associate's degree\"]\ndf_hs = students_df[students_df['parental level of education'] == 'high school']\ndf_shs = students_df[students_df['parental level of education'] == 'some high school']","9b65978c":"print(students_df.mean() - df_BD.mean())\nprint('--------------')\nprint(students_df.mean() - df_MD.mean())\nprint('--------------')\nprint(students_df.mean() - df_sc.mean())\nprint('--------------')\nprint(students_df.mean() - df_shs.mean())\nprint('--------------')\nprint(students_df.mean() - df_hs.mean())\nprint('--------------')\nprint(students_df.mean() - df_AD.mean())","f9e94dd6":"print(students_df.std() - df_BD.std())\nprint('--------------')\nprint(students_df.std() - df_MD.std())\nprint('--------------')\nprint(students_df.std() - df_sc.std())\nprint('--------------')\nprint(students_df.std() - df_shs.std())\nprint('--------------')\nprint(students_df.std() - df_hs.std())\nprint('--------------')\nprint(students_df.std() - df_AD.std())","43d5539d":"df_A = students_df[students_df['race\/ethnicity'] == 'group A']\ndf_B = students_df[students_df['race\/ethnicity'] == 'group B']\ndf_C = students_df[students_df['race\/ethnicity'] == 'group C']\ndf_D = students_df[students_df['race\/ethnicity'] == 'group D']\ndf_E = students_df[students_df['race\/ethnicity'] == 'group E']","a89d1ef8":"print(students_df.mean() - df_A.mean())\nprint('--------------')\nprint(students_df.mean() - df_B.mean())\nprint('--------------')\nprint(students_df.mean() - df_C.mean())\nprint('--------------')\nprint(students_df.mean() - df_D.mean())\nprint('--------------')\nprint(students_df.mean() - df_E.mean())","3e1575bb":"print(students_df.std() - df_A.std())\nprint('--------------')\nprint(students_df.std() - df_B.std())\nprint('--------------')\nprint(students_df.std() - df_C.std())\nprint('--------------')\nprint(students_df.std() - df_D.std())\nprint('--------------')\nprint(students_df.std() - df_E.std())","db8c79a0":"students_dummies = pd.get_dummies(students_df, drop_first=True, columns=categorical_features)\nstudents_dummies.head()","2bae11f4":"features = ['reading score', 'writing score', 'gender_male',\n       'race\/ethnicity_group B', 'race\/ethnicity_group C',\n       'race\/ethnicity_group D', 'race\/ethnicity_group E',\n       'parental level of education_bachelor\\'s degree',\n       'parental level of education_high school',\n       'parental level of education_master\\'s degree',\n       'parental level of education_some college',\n       'parental level of education_some high school', 'lunch_standard',\n       'test preparation course_none']","a97a74d2":"target = ['math score']","91b6bdc6":"class ColumnLinearRegression(BaseEstimator, RegressorMixin):\n     # columns are a \"Hyperparameter\" for our estimator, \n     # so we have to pass it in, in the __inti__ method,\n     # we need to keep track of the columns, so we have to save them\n        \n    def __init__(self, columns):\n        if not isinstance(columns, list):\n            raise ValueError(\"columns must be a list\")\n        self.columns= columns\n        self.lr = LinearRegression()\n        \n    def _select(self, X):\n        return X[self.columns]\n        \n    def fit(self, X, y):\n        self.lr.fit(self._select(X), y)\n        return self\n    \n    def predict(self, X):\n        return self.lr.predict(self._select(X))","eddbcfee":"def r2_adj(y_t, X_t, feat, pred):\n    #this function calculates the r^2 adjusted\n    r2 = metrics.r2_score(y_t, pred)\n    n = len(X_t)\n    p = len(feat)\n    return 1-((1-r2)*(n-1)\/(n-p-1))","92587df6":"feat_list = []\nnew_dict = {}\ni=0\nr2_adj_max = 0\nbest_model = None\nfor feature in features:\n    feat_list.append(feature)\n    clr = ColumnLinearRegression(feat_list)\n    X_train, X_test, y_train, y_test = train_test_split(students_dummies[feat_list], students_dummies[target], \n                                                    test_size=0.3, random_state=42)\n    clr.fit(X_train, y_train)\n    predictions = clr.predict(X_test)\n    variables = len(clr.columns)\n    mse = metrics.mean_squared_error(y_test, predictions)\n    r2 = metrics.r2_score(y_test, predictions)\n    r2_adjusted = r2_adj(y_test, X_test, clr.columns, predictions)\n    if r2_adjusted > r2_adj_max:\n        best_model = clr\n        r2_adj_max = r2_adjusted\n        new_dict[i] = {'var': variables, \n                       'MSE': mse,\n                       'R^2': r2, \n                       'R^2_adjusted': r2_adj_max}\n    else:\n        feat_list.remove(feat_list[-1])\n    i+=1\nprint(best_model.columns)    ","188c128e":"df = pd.DataFrame(new_dict)\ndf.head()","2bff0c86":"> This model will keep two out of three scores and we are going to predict the third. *Reading & Writing Scores* are kept, *math score* is the target variable.","3fe1a7cc":"## Regression Evaluation Metrics\n\n\nHere are three common evaluation metrics for regression problems:\n\n**Mean Absolute Error** (MAE) is the mean of the absolute value of the errors:\n\n$$\\frac 1n\\sum_{i=1}^n|y_i-\\hat{y}_i|$$\n\n**Mean Squared Error** (MSE) is the mean of the squared errors:\n\n$$\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2$$\n\n**Root Mean Squared Error** (RMSE) is the square root of the mean of the squared errors:\n\n$$\\sqrt{\\frac 1n\\sum_{i=1}^n(y_i-\\hat{y}_i)^2}$$\n\nComparing these metrics:\n\n- **MAE** is the easiest to understand, because it's the average error.\n- **MSE** is more popular than MAE, because MSE \"punishes\" larger errors, which tends to be useful in the real world.\n- **RMSE** is even more popular than MSE, because RMSE is interpretable in the \"y\" units.\n\nAll of these are **loss functions**, because we want to minimize them.\n\nAnother common metric for regression is $R^2$, also known as the **coefficient of determination**. The $R^2$ quantifies how our model's MSE compares to a naive model in which we always predict the mean $y$ value, $\\bar{y}$.\n\n$$ 1 - \\frac{\\sum_i \\left[f(X_i) - y_i\\right]^2}{\\sum_i\\left(\\bar{y} - y_i\\right)^2} $$\n\nIf our $R^2 < 0$ we know our model is very bad, because the MSE is larger than than the MAE of the mean model.\n\nOne important consideration when choosing a metric is how they scale with the data. One note for $R^2$ is that, it seems to get better and better as long as we add more features. But this most of the time is not very good for our model. So beyond the $R^2$ we calculate the $R^2 adjusted$ which is a modified version of $R^2$ that has been adjusted for the number of predictors in the model. The $R^2 adjusted$ increases only if the new term improves the model more than would be expected by chance. It decreases when a predictor improves the model by less than expected by chance. The $R^2 adjusted$ can be negative, but it\u2019s usually not. It is always lower than the $R^2$.","e0fe1923":"Great notes!!! As we expected, Master's\/ Bachelor's\/ Associate's Degree did better than the entire dataset and a bit better also did Some College. \n\n>In conclusion I think that also Parental Level of Education affects a student's score","6d5b1aae":"Overview\n--\n\nThis data set includes scores from three exams and a variety of personal, social, and economic factors that have interaction effects upon them","fcc4a606":">As we can see out data set is very clean with no *Null values* and all columns are the correct type as we expected. There are 5 columns that contain categorical values and 3 of numeric (integers) values.","3613bb97":"Let's start our analysis. I am going to start first with the *Test Preparation Course*. A first note is that: \n- outliers in any of math\/reading\/writing course for those who completed the test preparation course(TPC) are very similar to minimum value while for those who didn't completed TPC not.\n\n- Moreover, scores for those who completed the TPC has smaller variance, the body of the box is everywhere higher than the others and the whiskers are shorter.\n\nIn my opinion if a student completed the TPC then has more chances to get a better score! So yes, TPC does affects the scores.","c42ac282":"Summary Statistics for Parental Level of Education and Test Preparation Course\n--","4feaf2d9":"- All numeric columns have almost equal *standard deviations* and *mean values* are very similar. \n- Mininmum score of all subjects is 0. It is possible to be an outlier but as long as it is not a negative number we can consider it as normal (a student it is possible to take 0 score for some reason).","82a9161f":"Linear Regression - Score Prediction\n--","982742ed":"This is very nice! For sure, the results for those who did took the preparation course are much better!","a503ef0c":"I would like to highlight the following notes:\n\n- Two thirds of our students DIDN'T took the test preparation course\n- Group A of Race\/Ethinicity column has the minimum number of representatives, it is more than 3 times smaller than the Group C\n- \u039cost counts of Parental Level of Education has the 'Some College' and the less counts has the 'Master's Degree'.\n\nWe could split our dataset into smaller to analyze each one category seperately.","efafd6ce":"I will try to explain as much simpler as I can the above code:\n\n- *fig, axes = plt.subplots(3,2, figsize=(12,12))*:\n    - creates a figure, you can think of that like a white paper onto whinch we are going to draw our plots (you can take a look the code below to get a better understanding)\n    - axes are just the horizontal axis $x$ and the vertical axis $y$\n    - this line creates a white paper with $3X2$ grid of axes $x$,$y$\n    - so we can think of axes as objects which we can add, remove, draw anything we want\n- *axes[i,j].bar(x, y, more params)*:\n    - creates a bar plot (usually we use barplots to plot categorical variables)\n- *axes[i,j].set_frame_on(False)*:\n    - removes the lines of the axes $x$ and $y$ but keeps the ticks (think of that like if you have draw the axes with a pencil you could take an eraser and remove the lines)\n- *axes[i,j].set_xticklabels(label, rotation=45, more params)*:\n    - adds ticks and labels on the x-axis rotated by $45^o$\n- *axes[i,j].set_title('{} Counts'.format(categorical_features[i+j].capitalize()))*:\n    - adds a title for our plot","4709356a":"Last I would like to talk about *Race\/Ethinicity* boxplot. \n- By far the best group is Group E, low variance, small number of outliers, short whiskers and the highest medians\n- Group C that represents the larger group of all on the other hand doesn't do very well. Large number of outliers, long whiskers\n- Group A is the worst group refer to scores, it is always lower than the others groups.\n\nI don't know how Race\/Ethinicity could affect the scores of a student. I believe that because of large difference of samples for each group we can't be sure if really affects the scores.","1da551ae":"Standard Deviation shows that again our subset that completed TPC, has less variance to their scores.\n\n>As plots shown above if a student completes the TPC has much more chances to get high score to exams.","5ffa9f45":"**Example Research Questions**\n\n------------\n\n- How effective is the test preparation course?\n- Which major factors contribute to test outcomes?\n- What would be the best way to improve student scores on each test?","17b3f0aa":"Creating Our First Model\n--","dcef419d":">We are going to plot all the categorical columns to see the differences. Counts between our categories differ a lot except for the *gender* column","e90d6665":"Another interesting plot is *Parental Level of Education*. \n- Master's Degree does a bit better than bachelor's Degree\n- Master's and Bachelor's Degree has long bodies and short whiskers, almost no outliers(only one) and always higher median than the others\n- Very steady is the level of Associate Degree it has almost the same values on all three courses\n\nI believe that Parental's Level of Education affects a student's score. But our samples aren't equally so I think that a better collected dataset could give us more distinct results.","aaac5ff7":"Next step is to visualize the performance of each category of each categorical feature.\n\nWe will make use of boxplots and groupby each category our dataset because:\n\n- We can take the minimum(lower cap) and maximum(upper cap) value,\n- We can take the median(green line),\n- We can take quartiles, 25%(bottom of the box), 50%(is the same as median), 75%(top of the box),\n- outliers (circles)","fb8e9ab8":"> This data set consists of the marks secured by the students in various subjects.","2a3defeb":"Well done!!! Group E has much better mean value compared to the entire dataset and also Group D. But, Group E standard deviation is similar the entire dataset.\n\n> Ultimately Race\/Ethnicity it very possible to affect score."}}