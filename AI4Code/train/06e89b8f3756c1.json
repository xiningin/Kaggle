{"cell_type":{"d11fe2c6":"code","fc051011":"code","ae0161fe":"code","fc143afc":"code","bd5077aa":"code","519751d0":"code","3bb905c3":"code","9378b4d0":"code","4a3e1cb7":"code","26f73951":"code","f37ed1a4":"code","d80175f2":"code","150ed415":"code","c33fe794":"code","55d082b2":"code","4161d20f":"code","239d9f40":"code","1feab787":"code","25f8704f":"code","13804b77":"code","289b6543":"code","e426e244":"code","c831350d":"code","837b913e":"code","8e645b40":"code","3e5da92c":"markdown","153864ce":"markdown","59806614":"markdown","50fab887":"markdown","930477ef":"markdown","28e3764a":"markdown","a34b8d69":"markdown","c37d0f62":"markdown","a49bf8ba":"markdown","854f7ebf":"markdown","38c3d58e":"markdown","fbe04832":"markdown","38e35114":"markdown","40aa2417":"markdown","e5bf3a7c":"markdown","71ce8e37":"markdown","661ac804":"markdown","a7e00869":"markdown","a46a82f2":"markdown"},"source":{"d11fe2c6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","fc051011":"%matplotlib inline \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport statsmodels as sm\n\nimport warnings \nwarnings.filterwarnings(\"ignore\")","ae0161fe":"dir_data = \"\/kaggle\/input\/temperature-timeseries-for-some-brazilian-cities\"\ninput_data = {}\nfor filename in os.listdir(dir_data):\n    if filename.endswith(\".csv\"):\n        variable_name = filename.split('.')[0]\n        input_data[variable_name] = pd.read_csv(os.path.join(dir_data,filename))","fc143afc":"input_data.keys()","bd5077aa":"input_data['station_vitoria'][:30]","519751d0":"input_data['station_vitoria'].replace(999.90, np.NaN).fillna(method='ffill')[:10]","3bb905c3":"for i in input_data.keys():\n    for j in input_data[i].columns:\n        input_data[i][j] = input_data[i][j].replace(999.90, np.NaN)\n        input_data[i][j] = input_data[i][j].fillna(input_data[i][j].rolling(12,1).mean())","9378b4d0":"for i in input_data.keys():\n    input_data[i].drop(['D-J-F','M-A-M','J-J-A','S-O-N'], axis=1, inplace=True)\n    df = input_data[i].T\n    df.columns = df.iloc[0]\n    df.drop(['YEAR'], axis=0, inplace=True)\n    \n    #plt.figure(figsize=(18,12))\n    df.iloc[:-1,-11:].plot(figsize=(12,5), title=i)","4a3e1cb7":"for i in input_data.keys():\n    input_data[i] = pd.melt(input_data[i], id_vars=['YEAR','metANN'], value_vars=['JAN','FEB','MAR','APR','MAY','JUN','JUL','AUG','SEP','OCT','NOV','DEC',], \n                            var_name='month',value_name='Temp')\n    input_data[i]['Date'] = pd.to_datetime(input_data[i]['YEAR'].astype(str)+'\/'+input_data[i]['month'].astype(str)+'\/01')\n    input_data[i].drop(['YEAR','month'],axis=1,inplace=True)\n    input_data[i].sort_values(by='Date',inplace=True)","26f73951":"temp_data = {}\nmetANN_data = {}\n\nfor i in input_data.keys():\n    temp_data[i] = input_data[i][['Date','Temp']]\n    temp_data[i] = temp_data[i].set_index('Date')\n    metANN_data[i] = input_data[i][['Date','metANN']]\n    metANN_data[i] = metANN_data[i].groupby(pd.Grouper(key='Date', freq='Y')).mean()","f37ed1a4":"fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(20,12), constrained_layout=True)\nfig.suptitle(\"temp of stations over the years\", fontsize=22)\n\nstations = [list(temp_data.keys())[:2], list(temp_data.keys())[2:4],list(temp_data.keys())[4:6],list(temp_data.keys())[6:8],list(temp_data.keys())[8:10],\n           list(temp_data.keys())[10:12]]\n\nfor row, s in zip(ax,stations):\n    for col,i in zip(row, s):\n        col.plot(temp_data[i])\n        col.set_title(i)\n\nplt.show()","d80175f2":"fig, ax = plt.subplots(nrows=6, ncols=2, figsize=(20,12),constrained_layout=True)\nfig.suptitle(\"metANN over the years\", fontsize=22)\n\nstations = [list(metANN_data.keys())[:2], list(metANN_data.keys())[2:4],list(metANN_data.keys())[4:6],list(metANN_data.keys())[6:8],\n            list(metANN_data.keys())[8:10],list(metANN_data.keys())[10:12]]\n\nfor row, s in zip(ax,stations):\n    for col,i in zip(row, s):\n        col.plot(metANN_data[i])\n        col.set_title(i)\n\n#fig.tight_layout()\nplt.show()","150ed415":"zero_index = list(temp_data.keys())[0]\none_index = list(temp_data.keys())[1]\ntemp_df = temp_data[zero_index].merge(temp_data[one_index], left_on=\"Date\", right_on='Date', suffixes=('_'+zero_index,'_'+one_index))\n\nfor i in list(temp_data.keys())[2:]:\n    temp_df = temp_df.merge(temp_data[i], left_on='Date', right_on='Date').rename(columns={'Temp':'Temp_'+i+''})","c33fe794":"temp_df.head()","55d082b2":"from statsmodels.tsa.seasonal import seasonal_decompose\nfor i in temp_df.columns:\n    #print(i)\n    try:\n        decomposition = seasonal_decompose(temp_df[i], model=\"additive\")\n    except Exception as e:\n        #print(e)\n        pass\n        \n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18,3), constrained_layout=True)\n    fig.subplots_adjust(wspace=0.15)\n\n    ax1= plt.subplot(121)\n    ax1.plot(decomposition.trend)\n    ax1.set_title(\"Trend--> \"+i+\"\")\n\n    ax2 = plt.subplot(122)\n    ax2.plot(decomposition.seasonal)\n    ax2.set_title(\"Seasonality--> \"+i+\"\")\n    \n\nplt.tight_layout()\nplt.show()    ","4161d20f":"#Now lets analyze the stationarity of time series :\nimport statsmodels\nimport statsmodels.api as sm\nfrom statsmodels.tsa.stattools import coint, adfuller","239d9f40":"#Rolling Mean and Standard Deviation \ndef TestStationaryPlot(ts):\n    rol_mean = ts.rolling(window=12, center=False).mean()\n    rol_std = ts.rolling(window=12, center=False).std()\n    \n    plt.figure(figsize=(18,6))\n    \n    plt.plot(ts, color=\"red\",label=\"Time Series\")\n    plt.plot(rol_mean, color=\"blue\", label=\"Rolling mean\")\n    plt.plot(rol_std, color=\"yellow\", label=\"Rolling standard deviation\")\n    plt.xticks(fontsize=15)\n    plt.yticks(fontsize=15)\n    \n    plt.xlabel(\"Time\", fontsize=15)\n    plt.ylabel(\"Consumption\", fontsize=15)\n    plt.legend(loc=\"best\", fontsize=15)\n    \n    plt.title(\"Rolling Mean and Standard Deviation of Series Data\", fontsize=15)\n    plt.show(block=True)\n    ","1feab787":"#Adfuller test\n#null hypothesis : Series has a unit root \n#True ---> Stationary\n#False ---> Non-stationary \n\ndef TestStationaryAdfuller(ts, cutoff=0.05):\n    ts_test = adfuller(ts, autolag='AIC')\n    ts_test_output = pd.Series(ts_test[0:4], index = ['Test Stats', 'p-value', '#Lags Used', 'Number of observation used'])\n    \n    for k, v in ts_test[4].items():\n        ts_test_output['Critical Value (%s)'%k] = v\n        \n    \n    if ts_test[1] <= cutoff:\n        #print(\"ADF TEST :  Weak evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n        return True\n    else:\n        #print(\"ADF TEST :  Strong evidence against null hypothesis, time series has a unit root, indicating it is non-stationary\")\n        return False","25f8704f":"#KPSS test \n#null hyposthesis : Series is trend stationary \n\nfrom statsmodels.tsa.stattools import kpss\n\ndef testKPSStationary(timeseries, cutoff=0.05):\n    kpsstest = kpss(timeseries, regression='c')\n    kpss_output = pd.Series(kpsstest[0:3], index=['Test Statistic','p-value','Lags Used'])\n    \n    for key,value in kpsstest[3].items():\n        kpss_output['Critical Value (%s)'%key] = value\n \n    if kpsstest[1] <= cutoff:\n        #print(\"KPSS TEST : Weak evidence against null hypothesis, time series has a unit root, indicating it is non-stationary\")\n        return False\n    else:\n        #print(\"KPSS TEST : Strong evidence against the null hypothesis, reject the null hypothesis. Data has no unit root, hence it is stationary\")\n        return True\n    ","13804b77":"def isStationary(timeseries, station):\n    \n    Station = station\n    adf_result = TestStationaryAdfuller(timeseries)\n    kpss_result = testKPSStationary(timeseries)\n    \n    if (adf_result==False and kpss_result==True):\n        Type = 'Trend'\n        Stationary = 'No'\n    elif(adf_result==True and kpss_result==True):\n        Type = np.NaN\n        Stationary = 'Yes'\n    elif(adf_result==True and kpss_result==False):\n        Type = 'Difference'\n        Stationary = 'No'\n    else:\n        Type = np.NaN\n        Stationary = 'No'\n    \n    return pd.DataFrame([{'Station':station, 'Adfuller_result':adf_result, 'KPSS_result':kpss_result, 'Type':Type, 'Stationary':Stationary}])","289b6543":"#Split the data into train and test \ntemp_df_train = temp_df[:430]\ntemp_df_test = temp_df[430:]","e426e244":"warnings.filterwarnings(\"ignore\")\nstationary_df = pd.DataFrame(columns=['Station','Adfuller_result', 'KPSS_result', 'Type','Stationary'])\nfor i in temp_df_train.columns:\n    try:\n        stationary_df = stationary_df.append(isStationary(temp_df_train[i], i))\n    except Exception as e:\n        print(\"This series of __\"+i+\"__ contains nans, will see it later(NaN at the start of the series we can safely drop it)\")\nprint(stationary_df)","c831350d":"for i in temp_df_train.columns:\n    for diff_order in range(1,13):\n        d = isStationary((temp_df_train[i] - temp_df_train[i].shift(diff_order)).dropna(), i)\n        if d.Stationary.values == 'Yes':\n            print(i, diff_order)\n            break","837b913e":"#This is mean absolute error:\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt \n\ndef mean_absolute_percentage_error(y_true, y_pred): \n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    return np.mean(np.abs((y_true - y_pred) \/ y_true)) * 100","8e645b40":"from statsmodels.tsa.api import ExponentialSmoothing, SimpleExpSmoothing, Holt\nfor i in temp_df_train.columns:\n    print('\\n\\n')\n    print(\"********************************************\"+i+\"*************************************************************\")\n    y_hat_avg = temp_df_test[i].copy()\n    fit1 = ExponentialSmoothing(np.asarray(temp_df_train[i].dropna()) ,seasonal_periods=12 ,trend='add', seasonal='add',).fit()\n    y_hat_avg['Holt_Winter'] = fit1.forecast(len(temp_df_test[i]))\n\n    plt.figure(figsize=(16,8))\n    plt.plot(temp_df_train[i][252:], label='Train')\n    plt.plot(temp_df_test[i], label='Test')\n    plt.plot(temp_df_test.index, y_hat_avg['Holt_Winter'], label='Holt_Winter')\n    plt.legend(loc='best')\n    plt.show()\n    print(\"---------------------------------Mean Absolute Percentage Error------------------------------------------------------\")\n    print(mean_absolute_percentage_error(temp_df_test[i], y_hat_avg.Holt_Winter))\n    print(\"---------------------------------------RMS----------------------------------------------------------------\")\n    print(sqrt(mean_squared_error(temp_df_test[i], y_hat_avg.Holt_Winter)))","3e5da92c":"Can not apply the reduce memory usuage as this changes the values a little bit for me , like it changed 999.90 to 1000.000","153864ce":"Applying holt winters model in data of every city of Brazil and checking... ","59806614":"**metANN Data**","50fab887":"Now will check for the metANN of stations over the years: ","930477ef":"It is clear from the above data that only two station has stationary series i.e. Curitiba and Rio. \nRest all are non-stationary, few are diff stationary. We can make them stationary by differencing. ","28e3764a":"Deompose the data to have look at seasonlity, trend and residulals","a34b8d69":"Note : Maximum percentage error in station \"Temp_station_sao_luiz\" because it contains NaN and \"Temp_station_curitiba\" ","c37d0f62":"Observations:\n    1. Global warming and other factors seems to be powerful as all has increasing trend majorly after 1990. ","a49bf8ba":"Imp: Another way is to merge these station's dataframe into one keeping the column as temp_fortaleza, temp_belem and so on. In this way processing time and space can be saved. \nYou can do this way too :)","854f7ebf":"Merge all temp dataframe into one for easy processing:","38c3d58e":"\n*If you like this kernal, please upvote :) \n    Thank you*","fbe04832":"Observations:\n    1.  Min recorded temperature is in station \"station_curitiba\" i.e 12.5 in so many years.\n    2.  Max recorded temp is in station \"station_manaus\" i.e. 32 around in late 20's decade.\n    3. \"station_belem\", \"station_fortaleza\" and \"station_manaus\" has increasing trend somewhat over the years.\n    4. \"station_macapa\" has trend also variarble trend. \n    5. \"station_recife\" has temp increasing trend over the years 1970 to 1990. ","38e35114":"It is clearly visible that all the station series are stationary after 1 diff only. while Curitiba and Rio were stationary without diff also ","40aa2417":"**Temperature Data**","e5bf3a7c":"Pattern over the months for given year ","71ce8e37":"Check if the training temperature data is stationary or not:","661ac804":"The same analysis can be applied to the metANN too","a7e00869":"We can go for forward fill also, I have chosen to go with mean of last 12 data points. ","a46a82f2":"A pattern of seasonlality is clearly visible in all stations "}}