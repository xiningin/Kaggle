{"cell_type":{"0dd02cc3":"code","bb168b4a":"code","cf4eb3d7":"code","69ba0c2e":"code","f47d7e9e":"code","3a1b0746":"code","8464f95d":"code","af2946cd":"code","3f7a8685":"code","0860cb2d":"code","53093b10":"code","fe4c466f":"code","2bfc2c2b":"code","ccf57fd5":"code","7e177341":"code","45f86b63":"code","cf334995":"code","d72f3e73":"code","b6d51f73":"code","9dae7f24":"code","0101c681":"code","f4900c33":"code","a9147e79":"code","9ab66adb":"code","fb7f2775":"code","d11e078c":"markdown","af106293":"markdown","46d1d08a":"markdown"},"source":{"0dd02cc3":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import roc_auc_score, r2_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np \nimport lightgbm as lgbm\nimport xgboost as xgb\nfrom sklearn import preprocessing\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bb168b4a":"submission = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv\")\ntrain = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/cat-in-the-dat\/test.csv\")\n\nlabels = train.pop('target')\nlabels = labels.values\ntrain_id = train.pop(\"id\")\ntest_id = test.pop(\"id\")","cf4eb3d7":"train.head(5)","69ba0c2e":"# Thanks to https:\/\/www.kaggle.com\/bustam\/one-hot-stratified-logistic-regression\ndef ord_to_num(df, col):\n    keys=np.sort(df[col].unique())\n    values=np.arange(len(keys))\n    map = dict(zip(keys, values))\n    df[col] = df[col].replace(map)\nord_to_num(train, 'ord_4')\nord_to_num(test,'ord_4')\nord_to_num(train, 'ord_5')\nord_to_num(test,'ord_5')\n\n# ord_4\ntrain['ord_4_band'] = pd.qcut(train['ord_4'], 6)\nbands=train.ord_4_band.unique()\nkeys_bands=np.sort(bands)\nvalues_bands=np.arange(len(keys_bands))\nmap_bands = dict(zip(keys_bands, values_bands))\ntrain['ord_4_band'] = train['ord_4_band'].replace(map_bands)\ntest['ord_4_band']=pd.cut(test.ord_4,pd.IntervalIndex(keys_bands))\ntest['ord_4_band'] = test['ord_4_band'].replace(map_bands)\n\n# ord_5\ntrain['ord_5_band'] = pd.qcut(train['ord_5'], 6)\nbands=train.ord_5_band.unique()\nkeys_bands=np.sort(bands)\nvalues_bands=np.arange(len(keys_bands))\nmap_bands = dict(zip(keys_bands, values_bands))\ntrain['ord_5_band'] = train['ord_5_band'].replace(map_bands)\ntest['ord_5_band']=pd.cut(test.ord_5,pd.IntervalIndex(keys_bands))\ntest['ord_5_band'] = test['ord_5_band'].replace(map_bands)\n\n# \"nom_7\", \"nom_8\", \"nom_9\" - x for values is absent in both train and test\nfor col in [\"nom_7\", \"nom_8\", \"nom_9\"]:\n    train_vals = set(train[col].unique())\n    test_vals = set(test[col].unique())\n   \n    ex=train_vals ^ test_vals\n    if ex:\n        train.loc[train[col].isin(ex), col]=\"x\"\n        test.loc[test[col].isin(ex), col]=\"x\"","f47d7e9e":"# Preprocessing\nacc = []\ndata = pd.concat([train, test])\n# Determination categorical features\nnumerics = ['int8', 'int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorical_columns = []\nfeatures = data.columns.values.tolist()\nfor col in features:\n    if data[col].dtype in numerics: continue\n    categorical_columns.append(col)\n    \n# Encoding categorical features\ndata2 = data\nfor col in categorical_columns:\n    if col in data.columns:\n        le = LabelEncoder()\n        le.fit(list(data[col].astype(str).values))\n        data2[col] = le.transform(list(data[col].astype(str).values))\ntrain = data2.iloc[:train.shape[0], :]\ntest = data2.iloc[train.shape[0]:, :]","3a1b0746":"# Building the feature importance diagrams and prediction by 4 models\n# Thanks to https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg\n  \n# LGBM\n#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, labels, test_size=0.2, random_state=0)\ntrain_set = lgbm.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgbm.Dataset(Xval, Zval, silent=False)\nparams = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.05,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': True,\n        'seed':0,        \n    }\n\nmodelL = lgbm.train(params, train_set = train_set, num_boost_round=1000,\n                   early_stopping_rounds=50, verbose_eval=50, valid_sets=valid_set)\nfeature_score = pd.DataFrame(train.columns, columns = ['feature'])\npred_train = pd.DataFrame()\npred_test = pd.DataFrame()\nfeature_score['score_lgb'] = modelL.feature_importance()\npred_train['lgb_pred'] = modelL.predict(train)\npred_test['lgb_pred'] = modelL.predict(test)\nacc.append(round(r2_score(labels, pred_train['lgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nlgbm.plot_importance(modelL,ax = axes,height = 0.5)\nplt.show();plt.close()","8464f95d":"#XGBM\n#%% split training set to validation set \ndata_tr  = xgb.DMatrix(Xtrain, label=Ztrain)\ndata_cv  = xgb.DMatrix(Xval   , label=Zval)\ndata_train = xgb.DMatrix(train)\ndata_test = xgb.DMatrix(test)\nevallist = [(data_tr, 'train'), (data_cv, 'valid')]\nparms = {'max_depth':8, #maximum depth of a tree\n         'objective':'reg:logistic',\n         'eta'      :0.3,\n         'subsample':0.8,#SGD will use this percentage of data\n         'lambda '  :4, #L2 regularization term,>1 more conservative \n         'colsample_bytree ':0.9,\n         'colsample_bylevel':1,\n         'min_child_weight': 10}\nmodelx = xgb.train(parms, data_tr, num_boost_round=200, evals = evallist,\n                  early_stopping_rounds=30, maximize=False, \n                  verbose_eval=10)\nfeature_score['score_xgb'] = feature_score['feature'].map(modelx.get_score(importance_type='weight'))\npred_train['xgb_pred'] = modelx.predict(data_train)\npred_test['xgb_pred'] = modelx.predict(data_test)\nacc.append(round(r2_score(labels, pred_train['xgb_pred']) * 100, 2))\n\n#Plot FI\nfig =  plt.figure(figsize = (15,15))\naxes = fig.add_subplot(111)\nxgb.plot_importance(modelx,ax = axes,height = 0.5)\nplt.show();plt.close()","af2946cd":"# Regression models\n# Standardization for regression models\ntrain2 = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(train),\n    columns=train.columns, index=train.index)\n# Logistic Regression\nlogreg = LogisticRegression()\nlogreg.fit(train2, labels)\ncoeff_logreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_logreg.columns = ['feature']\ncoeff_logreg[\"score_logreg\"] = pd.Series(logreg.coef_[0])\npred_train['log_pred'] = logreg.predict(train)\npred_test['log_pred'] = logreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['log_pred']) * 100, 2))\ncoeff_logreg.sort_values(by='score_logreg', ascending=False)","3f7a8685":"coeff_logreg[\"score_logreg\"] = coeff_logreg[\"score_logreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_logreg, on='feature')","0860cb2d":"# Linear Regression\nlinreg = LinearRegression()\nlinreg.fit(train2, labels)\ncoeff_linreg = pd.DataFrame(train2.columns.delete(0))\ncoeff_linreg.columns = ['feature']\ncoeff_linreg[\"score_linreg\"] = pd.Series(linreg.coef_)\npred_train['lin_pred'] = linreg.predict(train)\npred_test['lin_pred'] = linreg.predict(test)\nacc.append(round(r2_score(labels, pred_train['lin_pred']) * 100, 2))\ncoeff_linreg.sort_values(by='score_linreg', ascending=False)","53093b10":"coeff_linreg[\"score_linreg\"] = coeff_linreg[\"score_linreg\"].abs() # the level of importance of features is not associated with the sign\nfeature_score = pd.merge(feature_score, coeff_linreg, on='feature')","fe4c466f":"# Comparison of the all feature importance diagrams\nfeature_score = feature_score.fillna(0)\nfeature_score = feature_score.set_index('feature')\n\n# MinMax scale all importances\nfeature_score = pd.DataFrame(preprocessing.MinMaxScaler().fit_transform(feature_score),\n    columns=feature_score.columns, index=feature_score.index)\n\n# Create mean column\nfeature_score['mean'] = feature_score.mean(axis=1)\npred_train['mean'] = pred_train.mean(axis=1)\npred_test['mean'] = pred_test.mean(axis=1)\nacc.append(round(r2_score(labels, pred_train['mean']) * 100, 2))\n\n# Create total column with different weights\nfL = 0.2\nflog = 0.5\nflin = 0.15\nfx = 1-fL-flog-flin\nfeature_score['total'] = fL*feature_score['score_lgb'] + fx*feature_score['score_xgb'] \\\n                       + flog*feature_score['score_logreg'] + flin*feature_score['score_linreg']\npred_train['total'] = fL*pred_train['lgb_pred'] + fx*pred_train['xgb_pred'] + \\\n                    + flog*pred_train['log_pred'] + flin*pred_train['lin_pred']\npred_test['total'] = fL*pred_test['lgb_pred'] + fx*pred_test['xgb_pred'] + \\\n                    + flog*pred_test['log_pred'] + flin*pred_test['lin_pred']\nacc.append(round(r2_score(labels, pred_train['total']) * 100, 2))\n\n# Plot the feature importances\nfeature_score.sort_values('total', ascending=False).plot(kind='bar', figsize=(20, 10))","2bfc2c2b":"feature_score.sort_values('score_logreg', ascending=False)","ccf57fd5":"acc","7e177341":"# to_drop = feature_score[(feature_score['score_logreg'] < 0.0025)].index.tolist()\n# to_drop","45f86b63":"# data = data.drop(to_drop,axis=1)\ndata.info()","cf334995":"columns = [i for i in data.columns]\n\ndummies = pd.get_dummies(data,\n                         columns=columns,\n                         drop_first=True,\n                         sparse=True)\n\ndel data","d72f3e73":"train = dummies.iloc[:train.shape[0], :]\ntest = dummies.iloc[train.shape[0]:, :]\n\ndel dummies","b6d51f73":"train.head(5)","9dae7f24":"print(train.shape)\nprint(test.shape)","0101c681":"train = train.sparse.to_coo().tocsr()\ntest = test.sparse.to_coo().tocsr()\n\ntrain = train.astype(\"float32\")\ntest = test.astype(\"float32\")","f4900c33":"lr = LogisticRegression(C=0.12,\n                        solver=\"lbfgs\",\n                        tol=0.002,\n                        max_iter=10000)\n\nlr.fit(train, labels)\n\nlr_pred = lr.predict_proba(train)[:, 1]\nscore = roc_auc_score(labels, lr_pred)\n\nprint(\"score: \", score)","a9147e79":"submission[\"id\"] = test_id\nsubmission[\"target\"] = lr.predict_proba(test)[:, 1]","9ab66adb":"submission.head()","fb7f2775":"submission.to_csv(\"submission.csv\", index=False)","d11e078c":"## EDA & FE by the XGB, LGBM, Logistic Regression and Linear Regression","af106293":"## Apply Logistic Regression","46d1d08a":"## This kernel basic on the kernels:\n\n* [Logistic Regression](https:\/\/www.kaggle.com\/martin1234567890\/logistic-regression)\n\n* [Feature importance - xgb, lgbm, logreg, linreg](https:\/\/www.kaggle.com\/vbmokin\/feature-importance-xgb-lgbm-logreg-linreg)\n\n* [One-Hot, Stratified, Logistic Regression](https:\/\/www.kaggle.com\/bustam\/one-hot-stratified-logistic-regression)"}}