{"cell_type":{"691ed12c":"code","4cf17f0c":"code","05acea94":"code","d30be286":"code","fcbcd55e":"code","edda0139":"code","4cd997c7":"code","18f95dc8":"code","4c9cdaed":"code","821d24a0":"code","9229292f":"code","2de65cd0":"code","24fc893a":"code","5b98ec7b":"code","0594f7a0":"code","4793879c":"code","c472ff81":"code","d1b20f78":"code","78c9aaa1":"code","a1b2c6b9":"code","28cdc520":"code","1cd7c895":"code","1279b49b":"code","264cf564":"code","fb3bdaa9":"code","39c5735c":"code","2334ee37":"code","470c3831":"code","20a1e461":"code","f6bf244a":"code","8f720c4e":"code","a9393435":"code","be4eee7d":"code","a20809ce":"code","3334520c":"markdown"},"source":{"691ed12c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4cf17f0c":"import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = '1'","05acea94":"!pip install transformers\n# !pip3 -qq install emoji\n!pip -qq install datasets","d30be286":"from transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n# model_dir='distilbert-base-uncased-finetuned-sst-2-english' #training is still slow?!\nmodel_dir='distilroberta-base'\n\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\n\n# model = AutoModelForSequenceClassification.from_pretrained()","fcbcd55e":"posts=pd.read_csv('\/kaggle\/input\/rbitcoin-20102021\/posts.csv', header=None,names=['score','timestamp','selftext','title'])","edda0139":"posts=posts[posts.title.notnull()]","4cd997c7":"posts.head(2)","18f95dc8":"posts['text']=posts.title.fillna('')+' '+ posts.selftext.fillna('')","4c9cdaed":"sample=False\nif sample:\n    posts=posts.sample(512)","821d24a0":"from datasets import Dataset","9229292f":"ds=Dataset.from_pandas(posts[['text']])\ndataset=ds.train_test_split(0.1)","2de65cd0":"def tokenize_function(examples): return tokenizer(examples[\"text\"],padding=True,truncation=True,max_length=128) ","24fc893a":"tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\",\"__index_level_0__\"],batch_size=1000)","5b98ec7b":"tokenized_datasets","0594f7a0":"block_size = 128","4793879c":"def group_texts(examples):\n    # Concatenate all texts.\n    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n    total_length = len(concatenated_examples[list(examples.keys())[0]])\n    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n        # customize this part to your needs.\n    total_length = (total_length \/\/ block_size) * block_size\n    # Split by chunks of max_len.\n    result = {\n        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n        for k, t in concatenated_examples.items()\n    }\n    result[\"labels\"] = result[\"input_ids\"].copy()\n    return result","c472ff81":"lm_datasets = tokenized_datasets.map(\n    group_texts,\n    batched=True,\n    batch_size=1000)","d1b20f78":"lm_datasets","78c9aaa1":"from transformers import AutoModelForMaskedLM\nmodel = AutoModelForMaskedLM.from_pretrained(model_dir)","a1b2c6b9":"from transformers import Trainer, TrainingArguments,EarlyStoppingCallback","28cdc520":"import torch\nif torch.cuda.is_available():\n    training_args = TrainingArguments(\n        \"test-mlm\",\n        evaluation_strategy = \"epoch\",\n        learning_rate=2e-5,\n#         weight_decay=0.01,\n        report_to=\"none\",\n        num_train_epochs=7,\n        load_best_model_at_end=True,\n        warmup_ratio=0.1,\n        logging_strategy='epoch',\n        save_strategy='epoch',\n        fp16=True,\n#         deepspeed=\"ds_config_zero3.json\",\n        gradient_accumulation_steps=8,        #for having a bigger bs\n        per_device_train_batch_size=64,\n        metric_for_best_model='loss',#needed for early stopping callback to determine when to stop\n        per_device_eval_batch_size=64\n    )\nelse:\n    training_args = TrainingArguments(\n    \"test-mlm\",\n    evaluation_strategy = \"epoch\",\n    learning_rate=2e-5,\n    report_to=\"none\",\n    num_train_epochs=10,\n    load_best_model_at_end=True,\n    warmup_ratio=0.02, \n    gradient_accumulation_steps=8,        #more for speeding up\n    metric_for_best_model='loss',#needed for early stopping callback to determine when to stop\n    logging_strategy='epoch',\n    save_strategy='epoch')","1cd7c895":"from transformers import DataCollatorForLanguageModeling\ndata_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)","1279b49b":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=lm_datasets[\"train\"],\n    eval_dataset=lm_datasets[\"test\"],\n    data_collator=data_collator,\n    callbacks=[EarlyStoppingCallback(1)],\n#     optimizers=(AdamW,get_cosine_schedule_with_warmup) #https:\/\/www.kaggle.com\/rhtsingh\/guide-to-huggingface-schedulers-differential-lrs\n)","264cf564":"trainer.train()","fb3bdaa9":"import math\neval_results = trainer.evaluate()\nprint(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")","39c5735c":"# trainer.push_to_hub(\"bitcoinReddit-mlm\",use_auth_token='api_huVsWDHdFYGULvgAVSKFNiWbrpNBctkvRh')\n# tokenizer.push_to_hub(\"bitcoinReddit-mlm\",use_auth_token='api_huVsWDHdFYGULvgAVSKFNiWbrpNBctkvRh')","2334ee37":"# import torch\n# from transformers import pipeline\n# if torch.cuda.is_available(): model = pipeline(\"sentiment-analysis\",model=model,tokenizer=tokenizer, device=0,padding=True, truncation=True) #,max_length=128\n# else: model = pipeline(\"sentiment-analysis\",model=model,tokenizer=tokenizer,padding=True, truncation=True) #, device=0,max_length=128","470c3831":"# posts['selftext']=posts['selftext'].str.replace(r'[removed]','')\n# posts['selftext']=posts['selftext'].str.replace(r'[deleted]','')\n# posts['selftext']=posts['selftext'].str.replace(r'.','')\n# posts['selftext']=posts['selftext'].str.replace(r'Title','')\n# posts['selftext']=posts['selftext'].str.replace(r'','')\n# posts['selftext']=posts['selftext'].str.replace(r'\\[removed\\]','')\n# posts['selftext']=posts['selftext'].str.replace(r'title','')\n# posts['selftext']=posts['selftext'].str.replace(r'?','')\n# posts['selftext']=posts['selftext'].str.replace(r'\\[\\]','')\n# posts['selftext']=posts['selftext'].replace({r'[]':''})\n\n# posts['selftext']=posts['selftext'].replace({r\"\"\"**Welcome to the \/r\/Bitcoin daily discussion thread!**\\n\\n---\\n\\n**Thread topics include, but are not limited to:**\\n\\n* General discussion of current events related to Bitcoin\\n* Questions, thoughts and observations that do not warrant a separate post\\n* Cool stuff you bought with bitcoin recently\\n\\n**Thread Guidelines**\\n\\n* **Be excellent to each other.**\\n\"\"\":''})\n\n# posts['selftext']=posts['selftext'].replace({r\"\"\"Ask (and answer!) away! Here are the general rules:\\r\\n\\r\\n* If you'd like to learn something, ask.\\r\\n* If you'd like to share knowledge, answer.\\r\\n* Any question about Bitcoin is fair game.\\r\\n\\r\\nAnd don't forget to check out \/r\/BitcoinBeginners\\r\\n\\r\\nYou can sort by new to see the latest questions that may not be answered yet.\"\"\":''},regex=True)\n# posts['selftext']=posts['selftext'].replace({r\"\"\"Ask (and answer!) away! Here are the general rules:\\n\\n* If you'd like to learn something, ask.\\n* If you'd like to share knowledge, answer.\\n* Any question about bitcoins is fair game.\\n\\nAnd don't forget to check out \/r\/BitcoinBeginners\\n\\nYou can sort by new to see the latest questions that may not be answered yet.\\n\\n\"\"\":''},regex=True)","20a1e461":"# import datasets\n# from transformers.pipelines.base import KeyDataset\n# import tqdm","f6bf244a":"# from datasets import Dataset","8f720c4e":"# ds=Dataset.from_pandas(posts[['text']])","a9393435":"# KeyDataset (only *pt*) will simply return the item in the dict returned by the dataset item\n# as we're not interested in the *target* part of the dataset.\n# res=[]\n# for out in tqdm.tqdm(model(KeyDataset(ds, \"text\"))):\n#     res.append(out)\n    # {\"text\": \"NUMBER TEN FRESH NELLY IS WAITING ON YOU GOOD NIGHT HUSBAND\"}\n    # {\"text\": ....}\n    # ....","be4eee7d":"# res_df=pd.DataFrame(res)\n# res_df.to_csv('posts_bertweet_sent.csv',index=False)","a20809ce":"# res_df","3334520c":"for titles, the num_rows (of 128 length) actually goes down - that makes sense given the shorter lenght of titles"}}