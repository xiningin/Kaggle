{"cell_type":{"52481970":"code","e6b9becf":"code","87bf2252":"code","e88fac06":"code","a10f4350":"code","9a79f0fc":"code","0cde45c1":"code","96b3c89a":"code","13862472":"code","47b3391d":"code","30512fd1":"code","d9a13a80":"code","8fd72350":"code","abe5fd30":"code","715a6ad4":"code","58ced38a":"code","8d4f3051":"code","3422bb04":"code","66929de5":"markdown"},"source":{"52481970":"import numpy as np\nimport pandas as pd\nimport nltk","e6b9becf":"metadata = pd.read_csv(\"\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv\",\n                        na_values=[], keep_default_na=False)\nmetadata.head()","87bf2252":"metadata.info()","e88fac06":"from wordcloud import WordCloud, STOPWORDS\nimport matplotlib.pyplot as plt\nfrom nltk.corpus import stopwords \nfrom string import punctuation\n\nstop_words = set(stopwords.words('english'))\np_chars = set([char for char in punctuation])\n\ndef extract(items):\n    records = []\n    for x in items:\n        if(isinstance(x, str)):\n            records.append(x)\n    return records\n\ndef tokenize(items):\n    records = []\n    for x in items:\n        for word in nltk.word_tokenize(x):\n            word = word.lower()\n            if not word in (stop_words|p_chars):\n                records.append(word)\n    return records\n\ndef ShowCloud(text, title):\n    # build\n    wc = WordCloud(max_font_size=50, background_color=\"white\", \n                   collocations=False,\n                   max_words=100, stopwords=STOPWORDS)\n    wc.generate(\" \".join(text))\n    # plot\n    plt.figure(figsize=(20,10))\n    plt.axis(\"off\")\n    plt.title(title, fontsize=20)\n    plt.imshow(wc, interpolation=\"bilinear\")\n    plt.show()    ","a10f4350":"titles = extract(metadata.title)\ntitle_tokens = tokenize(titles)\nprint(f\"total words in title: {len(title_tokens)}\")","9a79f0fc":"ShowCloud(title_tokens, \"Frequent words in titles\")","0cde45c1":"abstracts = extract(metadata.abstract)\nabstract_tokens = tokenize(abstracts)\nprint(f\"total words in abstract: {len(abstract_tokens)}\")","96b3c89a":"ShowCloud(abstract_tokens, \"Frequent words in abstract\")","13862472":"from nltk.probability import FreqDist\n\ndef builtFreq(data):\n    porter = nltk.PorterStemmer()\n    lemma = nltk.WordNetLemmatizer()\n    stems = [porter.stem(t) for t in data]\n    words = [lemma.lemmatize(t) for t in stems]\n    return FreqDist(words)","47b3391d":"title_dist = builtFreq(title_tokens)\ntitle_dist.most_common()[:10]","30512fd1":"import matplotlib.pyplot as plt\n\ndef plotFreq(dist, count, title):\n    plt.figure(figsize=(20, 8))\n    plt.title(title, size = 40)\n    plt.xticks(size = 20)\n    plt.yticks(size = 20)\n    dist.plot(count)","d9a13a80":"plotFreq(title_dist, 50, \"word frequency in title\")","8fd72350":"abstract_dist = builtFreq(abstract_tokens)\nabstract_dist.most_common()[:10]","abe5fd30":"plotFreq(abstract_dist, 50, \"word frequency in abstract\")","715a6ad4":"import json\n\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            for entry in content['abstract']:\n                self.abstract.append(entry['text'])\n            # Body text\n            for entry in content['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data    \n    \ndef process(files, meta_df):\n    dic = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'journal': [], 'abstract_summary': []}\n    for idx, entry in enumerate(files):\n        if idx % (len(files) \/\/ 10) == 0:\n            print(f'Processing index: {idx} of {len(files)}')\n\n        content = FileReader(entry)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n        # no metadata, skip this paper\n        if len(meta_data) == 0:\n            continue\n\n        dic['paper_id'].append(content.paper_id)\n        dic['abstract'].append(content.abstract)\n        dic['body_text'].append(content.body_text)\n\n        # also create a column for the summary of abstract to be used in a plot\n        if len(content.abstract) == 0: \n            # no abstract provided\n            dic['abstract_summary'].append(\"Not provided.\")\n        elif len(content.abstract.split(' ')) > 100:\n            # abstract provided is too long for plot, take first 300 words append with ...\n            info = content.abstract.split(' ')[:100]\n            summary = get_breaks(' '.join(info), 40)\n            dic['abstract_summary'].append(summary + \"...\")\n        else:\n            # abstract is short enough\n            summary = get_breaks(content.abstract, 40)\n            dic['abstract_summary'].append(summary)\n\n        # get metadata information\n        meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n\n        try:\n            # if more than one author\n            authors = meta_data['authors'].values[0].split(';')\n            if len(authors) > 2:\n                # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n                dic['authors'].append(\". \".join(authors[:2]) + \"...\")\n            else:\n                # authors will fit in plot\n                dic['authors'].append(\". \".join(authors))\n        except Exception as e:\n            # if only one author - or Null valie\n            dic['authors'].append(meta_data['authors'].values[0])\n\n        # add the title information, add breaks when needed\n        try:\n            title = get_breaks(meta_data['title'].values[0], 40)\n            dic['title'].append(title)\n        # if title was not provided\n        except Exception as e:\n            dic['title'].append(meta_data['title'].values[0])\n\n        # add the journal information\n        dic['journal'].append(meta_data['journal'].values[0])\n    return dic","58ced38a":"import glob\n\nall_json = glob.glob('\/kaggle\/input\/CORD-19-research-challenge\/**\/*.json', recursive=True)\nlen(all_json)","8d4f3051":"# get sample\nfirst_row = FileReader(all_json[0])\nprint(first_row)","3422bb04":"dict_ = process(all_json, metadata)\ndf_covid = pd.DataFrame(dict_)\ndf_covid.head()","66929de5":"Processing data functions"}}