{"cell_type":{"a71a5cbc":"code","09bfd5db":"code","57858925":"code","31ab15b7":"code","10fa977c":"code","57a408a0":"code","2cba8f65":"code","3949313d":"code","ecd527f5":"code","7de04a4e":"code","1253ac90":"code","d14c3c51":"code","f21edc4c":"code","dcd0bd62":"code","c932e791":"code","fa03f101":"code","0d2afec0":"code","536e9877":"code","bffed5db":"code","36a6f905":"code","028ef804":"code","0c33f2d7":"code","620d88a2":"code","73327bf6":"code","842f8576":"code","75858c5e":"code","cdde8d07":"code","a793a387":"code","0bd2d07b":"code","8b9bcf39":"code","7ec5c882":"code","22eb74f5":"code","361afe80":"code","f02e6c46":"code","b0af5fc0":"code","e6da546f":"code","8d2df725":"code","1491f501":"code","4d8972e1":"code","2e75d72e":"code","f01a6967":"code","a0b6d7b0":"code","db21d594":"code","305540d6":"code","55fc55a5":"code","8c4548f8":"code","ffbd4bc6":"code","fab17bd9":"code","8cd20017":"code","463eab0f":"code","0a6aca8f":"code","6946eb5b":"code","e1a9e66d":"code","368177ec":"code","451d1198":"code","b9be3bb1":"code","52a7194c":"code","25724478":"code","08a5dc54":"code","d672b128":"code","693cee63":"code","49cea8c6":"code","20bce60f":"code","947a9d29":"code","88f1202c":"code","2a53e3e5":"code","a419374f":"code","76d3d5ff":"code","251060ad":"code","8168a81c":"code","5e2bdf76":"code","d966628f":"code","9e747644":"code","6eebe5d1":"code","b2845a85":"code","048834f7":"code","6fa15036":"code","6641ce6d":"code","9f1cf119":"code","c5ab519d":"code","3bdaa52a":"code","b4f6325d":"code","a54a716e":"code","1c358330":"code","6025af7b":"code","d3274171":"code","7b6233b1":"code","f1da6ad9":"code","6d8d55ac":"code","6b805cc0":"code","b2fb503e":"code","f194f042":"code","e982808c":"code","3f0419dc":"code","5b639c18":"code","5e77afda":"code","7c55a1e1":"code","59d4d471":"code","c4c8a317":"code","6d708b07":"code","a61c7db9":"code","0997a87c":"code","6c160171":"code","32c8f52c":"code","35a69b47":"code","fcf4b7fc":"code","4036b4af":"code","3f9f98da":"code","e121de11":"code","64de0a88":"code","8a73c49e":"code","8bcc66b9":"markdown","6fdf41d4":"markdown","49950a41":"markdown","a31890b3":"markdown","d5bf2bac":"markdown","b62c0506":"markdown","73d54b52":"markdown","c0bf0b7c":"markdown","f3d1b952":"markdown","f1d3ea46":"markdown","cbd4746b":"markdown","d18dd7b6":"markdown","0ae2d06d":"markdown","39edf344":"markdown","b480c717":"markdown","37b8df76":"markdown","5ededfe9":"markdown","25391da4":"markdown","6feb3c17":"markdown","9a1e9307":"markdown","72b8b773":"markdown","c494246d":"markdown","855a81e3":"markdown","13c31e7e":"markdown","cd369f62":"markdown","ad271b07":"markdown","c5a905a1":"markdown","62997612":"markdown","f3925a84":"markdown","7f3f75bf":"markdown","1967ad70":"markdown","baea8556":"markdown","656bef52":"markdown","5d41bbfa":"markdown","be046e0b":"markdown","f91d2ebf":"markdown","59a4f5b8":"markdown","ebf137aa":"markdown","c3c522d1":"markdown","5f836073":"markdown","c8a461b3":"markdown","d3f7ae4e":"markdown","96368096":"markdown","33c876fc":"markdown","ac4e6ba6":"markdown","86cdf806":"markdown","113a2a7d":"markdown","a5854023":"markdown","e1648aa9":"markdown","45be4eba":"markdown","416ba98e":"markdown","b64b3b2f":"markdown","fc2fd1fa":"markdown","7cdb7cf0":"markdown","036a3c40":"markdown","52d2fa1d":"markdown","10a84cd3":"markdown","8b8a2b17":"markdown","7e11f535":"markdown","255aa8bc":"markdown","610d1b59":"markdown","539e1109":"markdown","99eff156":"markdown","fea53cb4":"markdown","d482b9e3":"markdown","3a000565":"markdown","5a45c031":"markdown","9e69059b":"markdown","56f32de2":"markdown","e442f55c":"markdown","952f3336":"markdown","2ac84e0f":"markdown","9c640beb":"markdown","8f2a9a84":"markdown"},"source":{"a71a5cbc":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n# Libraries to help with reading and manipulating data\nimport pandas as pd\nimport numpy as np\nimport sys\n\n# libaries to help with data visualization\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Libraries to tune model, get different metric scores, and split data\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\n#libraries to help with model building\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import (\n    AdaBoostClassifier,\n    GradientBoostingClassifier,\n    RandomForestClassifier, BaggingClassifier, RandomForestClassifier)\nfrom xgboost import XGBClassifier\n\nfrom sklearn import set_config\nset_config(print_changed_only=False) # to get the default logarithm parameters\n\n# Removes the limit from the number of displayed columns.\n# This is so I can see the entire dataframe when I print it\npd.set_option('display.max_columns', None)# 2. Loading and exploring the data# 1. Import Libraries","09bfd5db":"df_test = pd.read_csv('..\/input\/airline-passenger-satisfaction\/test.csv')","57858925":"df_train = pd.read_csv('..\/input\/airline-passenger-satisfaction\/train.csv')","31ab15b7":"df = pd.concat([df_train, df_test])\ndf.head()","10fa977c":"print(f'Total number of rows in the dataset: {df.shape[0]}')\nprint(f'Total number of columns in the dataset: {df.shape[1]}')","57a408a0":"# Out of convenience, I will make all the column names lower case\ndf.columns = map(str.lower, df.columns)\ndf.columns","2cba8f65":"round((df['satisfaction'].value_counts()\/len(df['satisfaction']))*100,0)","3949313d":"df.info()","ecd527f5":"df.nunique()","7de04a4e":"df_test.nunique()","1253ac90":"df_train.nunique()","d14c3c51":"df.drop([\"unnamed: 0\"],axis=1,inplace=True)\ndf.drop([\"id\"],axis=1,inplace=True)","f21edc4c":"df['satisfaction'].replace('neutral or dissatisfied',0,inplace=True)\ndf['satisfaction'].replace('satisfied',1,inplace=True)","dcd0bd62":"cat_var = df.select_dtypes(['object'])\n\nfor colname in cat_var:\n    df[colname] = df[colname].astype('category')\n    \ndf.info()","c932e791":"df.describe().T","fa03f101":"df.describe(include=['category']).T","0d2afec0":"dup = df[df.duplicated()]\ndup","536e9877":"cat_col = df.select_dtypes(['category'])\n\nfor i in cat_col.columns:\n    print('Unique values in',i, 'are :')\n    print(cat_col[i].value_counts())\n    print('-'*50)","bffed5db":"df.isnull().sum().sort_values(ascending = False)","36a6f905":"a = df['arrival delay in minutes'].unique()\nnp.sort(a)","028ef804":"# What percentage of rows have missing values:\nnull_values = df['arrival delay in minutes'].isnull().sum()\nper_null = round((null_values \/ df.shape[0])*100,2)\nprint(f'The percentage of null values is: {per_null}%')","0c33f2d7":"df.dropna(axis=0, inplace=True)","620d88a2":"df.isnull().sum().sort_values(ascending = False)","73327bf6":"print(f'Total number of rows in the dataset: {df.shape[0]}')\nprint(f'Total number of columns in the dataset: {df.shape[1]}')","842f8576":"# outlier detection using boxplot\nnumerical_col = df.select_dtypes(include=np.number).columns.tolist()\nplt.figure(figsize=(20,30))\n\nfor i, variable in enumerate(numerical_col):\n                     plt.subplot(5,4,i+1)\n                     plt.boxplot(df[variable],whis=1.5)\n                     plt.tight_layout()\n                     plt.title(variable)\n\nplt.show()### Checking Outliers","75858c5e":"df.select_dtypes(['int64','float64']).columns","cdde8d07":"plt.figure(figsize=(12,7)) # makes the plot wider\nsns.histplot(df['age'], color='c', kde = False) # plots a simple histogram\nplt.title('Observations on Age')\nplt.axvline(df['age'].mean(), color='m', linewidth=1)\nplt.axvline(df['age'].median(), color='g', linestyle='dashed', linewidth=1);","a793a387":"plt.figure(figsize=(12,7)) # makes the plot wider\nsns.histplot(df['flight distance'], color='c', kde = False) # plots a simple histogram\nplt.title('Observations on Flight distance')\nplt.axvline(df['flight distance'].mean(), color='m', linewidth=1)\nplt.axvline(df['flight distance'].median(), color='g', linestyle='dashed', linewidth=1);","0bd2d07b":"fd_perc = round((len(df[df['flight distance'] < 1744])\/df.shape[0])*100,1)\nprint(f'The percentage of customers flying less than 1744 miles is {fd_perc}%')","8b9bcf39":"def perc_on_bar(feature):\n    '''\n    plot\n    feature: categorical feature\n    the function won't work if a column is passed in hue parameter\n    '''\n    #Creating a countplot for the feature\n    sns.set(rc={'figure.figsize':(15,7)})\n    ax=sns.countplot(x=feature, data=df)\n    \n    total = len(feature) # length of the column\n    for p in ax.patches:\n        percentage = '{:.1f}%'.format(100 * p.get_height()\/total) # percentage of each class of the category\n        x = p.get_x() + p.get_width() \/ 2 - 0.25 # width of the plot\n        y = p.get_y() + p.get_height()           # hieght of the plot\n        ax.annotate(percentage, (x, y), size = 14) # annotate the percantage \n        \n    plt.show() # show the plot","7ec5c882":"perc_on_bar(df['inflight wifi service'])","22eb74f5":"perc_on_bar(df['departure\/arrival time convenient'])","361afe80":"perc_on_bar(df['ease of online booking'])","f02e6c46":"perc_on_bar(df['gate location'])","b0af5fc0":"perc_on_bar(df['food and drink'])","e6da546f":"perc_on_bar(df['online boarding'])","8d2df725":"perc_on_bar(df['seat comfort'])","1491f501":"perc_on_bar(df['inflight entertainment'])","4d8972e1":"perc_on_bar(df['on-board service'])","2e75d72e":"perc_on_bar(df['leg room service'])","f01a6967":"perc_on_bar(df['baggage handling'])","a0b6d7b0":"perc_on_bar(df['checkin service'])","db21d594":"perc_on_bar(df['inflight service'])","305540d6":"perc_on_bar(df['cleanliness'])","55fc55a5":"df.select_dtypes(['category']).columns","8c4548f8":"perc_on_bar(df['gender'])","ffbd4bc6":"perc_on_bar(df['customer type'])","fab17bd9":"perc_on_bar(df['type of travel'])","8cd20017":"perc_on_bar(df['class'])","463eab0f":"plt.figure(figsize = (12,7))\nsns.heatmap(df.corr(), linewidths = 0.5, cmap= 'Reds', vmin = -1, vmax = 1, center = 0, annot = True, annot_kws={\"fontsize\":10});","0a6aca8f":"df.columns","6946eb5b":"cols_1 = df[['inflight wifi service', 'departure\/arrival time convenient', 'ease of online booking',\n       'gate location', 'food and drink', 'online boarding', 'seat comfort']].columns.tolist()\nplt.figure(figsize=(12,7))\n\nfor i, variable in enumerate(cols_1):\n                     plt.subplot(2,4,i+1)\n                     sns.boxplot(df[\"satisfaction\"],df[variable],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","e1a9e66d":"cols_2 = df[['inflight entertainment', 'on-board service', 'leg room service',\n       'baggage handling', 'checkin service', 'inflight service',\n       'cleanliness']].columns.tolist()\nplt.figure(figsize=(12,7))\n\nfor i, variable in enumerate(cols_2):\n                     plt.subplot(2,4,i+1)\n                     sns.boxplot(df[\"satisfaction\"],df[variable],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","368177ec":"cols_3 = df[['flight distance', 'departure delay in minutes', 'arrival delay in minutes']].columns.tolist()\nplt.figure(figsize=(12,7))\n\nfor i, variable in enumerate(cols_3):\n                     plt.subplot(2,4,i+1)\n                     sns.boxplot(df[\"satisfaction\"],df[variable],palette=\"PuBu\")\n                     plt.tight_layout()\n                     plt.title(variable)\nplt.show()","451d1198":"## Function to plot stacked bar chart\ndef stacked_plot(x):\n    sns.set(palette='nipy_spectral')\n    tab1 = pd.crosstab(x,df['satisfaction'],margins=True)\n    print(tab1)\n    print('-'*120)\n    tab = pd.crosstab(x,df['satisfaction'],normalize='index')\n    tab.plot(kind='bar',stacked=True,figsize=(10,5))\n    #plt.legend(loc='lower left', frameon=False)\n    plt.legend(loc=\"upper right\", bbox_to_anchor=(1.05,1))\n    plt.show()","b9be3bb1":"df.select_dtypes(['category']).columns","52a7194c":"stacked_plot(df['gender'])","25724478":"stacked_plot(df['customer type'])","08a5dc54":"stacked_plot(df['type of travel'])","d672b128":"pt_disats = round((len(df[(df['type of travel'] == 'Personal Travel') & (df['satisfaction'] == 0)]) \/ len(df[(df['type of travel'] == 'Personal Travel')]))*100,1)\nprint(f'The percentage of not satisfied customer who flew for personal reasons was: {pt_disats}%')","693cee63":"stacked_plot(df['class'])","49cea8c6":"class_disats_eco = round((len(df[(df['class'] == 'Eco') & (df['satisfaction'] == 0)]) \/ len(df[(df['class'] == 'Eco')]))*100,1)\nclass_disats_ecop = round((len(df[(df['class'] == 'Eco Plus') & (df['satisfaction'] == 0)]) \/ len(df[(df['class'] == 'Eco Plus')]))*100,1)\nprint(f'The percentage of not satisfied customer who flew Economy was: {class_disats_eco}% and Eco Plus was: {class_disats_ecop}%')","20bce60f":"class_tab = pd.crosstab(df['class'],df['type of travel'],margins=True)\nprint(class_tab)\nprint('-'*120)\nbus_class = pd.crosstab(df['class'],df['type of travel'],normalize='index')\nbus_class.plot(kind='bar',stacked=True,figsize=(10,5))\n#plt.legend(loc='lower left', frameon=False)\nplt.legend(loc=\"upper right\", bbox_to_anchor=(1.05,1))\nplt.show()","947a9d29":"sns.catplot(data = df,\n            x='satisfaction',\n           hue = 'type of travel',\n            col = 'class',\n           kind = 'count');","88f1202c":"# Category columns\ncateg_col = df.select_dtypes(include = 'category')\ncateg_col.columns","2a53e3e5":"# In case that I want to drop any dummy variables for future modelling, I prefer to use a copy of my dataset now.\ndf1 = df.copy()","a419374f":"df1 = pd.get_dummies(df,drop_first=True)\ndf1","76d3d5ff":"X = df1.drop('satisfaction',axis=1)\ny = df1['satisfaction'] ","251060ad":"# Splitting data into training and test set:\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1,stratify=y)\nprint(X_train.shape, X_test.shape)","8168a81c":"##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\ndef get_metrics_score(model,train,test,train_y,test_y,flag=True):\n    '''\n    model : classifier to predict values of X\n\n    '''\n    # defining an empty list to store train and test results\n    score_list=[] \n    \n    pred_train = model.predict(train)\n    pred_test = model.predict(test)\n    \n    train_acc = model.score(train,train_y)\n    test_acc = model.score(test,test_y)\n    \n    train_recall = metrics.recall_score(train_y,pred_train)\n    test_recall = metrics.recall_score(test_y,pred_test)\n    \n    train_precision = metrics.precision_score(train_y,pred_train)\n    test_precision = metrics.precision_score(test_y,pred_test)\n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n        \n    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n    if flag == True: \n        print(\"Accuracy on training set : \",model.score(train,train_y))\n        print(\"Accuracy on test set : \",model.score(test,test_y))\n        print(\"Recall on training set : \",metrics.recall_score(train_y,pred_train))\n        print(\"Recall on test set : \",metrics.recall_score(test_y,pred_test))\n        print(\"Precision on training set : \",metrics.precision_score(train_y,pred_train))\n        print(\"Precision on test set : \",metrics.precision_score(test_y,pred_test))\n    \n    return score_list # returning the list with train and test scores","5e2bdf76":"## Function to create confusion matrix\ndef make_confusion_matrix(model,y_actual,labels=[1, 0]):\n    '''\n    model : classifier to predict values of X\n    y_actual : ground truth  \n    \n    '''\n    y_predict = model.predict(X_test)\n    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cm, annot=labels,fmt='')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","d966628f":"lr = LogisticRegression(random_state=1, solver='liblinear')\nlr.fit(X_train,y_train)","9e747644":"kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5","6eebe5d1":"#Calculating different metrics\nscores_LR = get_metrics_score(lr,X_train,X_test,y_train,y_test)\n\n# creating confusion matrix\nmake_confusion_matrix(lr,y_test)","b2845a85":"from imblearn.over_sampling import SMOTE","048834f7":"print(\"Before UpSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before UpSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(sampling_strategy = 1 ,k_neighbors = 5, random_state=1)   #Synthetic Minority Over Sampling Technique\nX_train_over, y_train_over = sm.fit_resample(X_train, y_train)\n\n\nprint(\"After UpSampling, counts of label '1': {}\".format(sum(y_train_over==1)))\nprint(\"After UpSampling, counts of label '0': {} \\n\".format(sum(y_train_over==0)))\n\n\nprint('After UpSampling, the shape of train_X: {}'.format(X_train_over.shape))\nprint('After UpSampling, the shape of train_y: {} \\n'.format(y_train_over.shape))","6fa15036":"log_reg_over = LogisticRegression(random_state = 1, solver='liblinear')\n\n# Training the basic logistic regression model with training set \nlog_reg_over.fit(X_train_over,y_train_over)","6641ce6d":"kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5","9f1cf119":"#Calculating different metrics\nget_metrics_score(log_reg_over,X_train_over,X_test,y_train_over,y_test)\n\n# creating confusion matrix\nmake_confusion_matrix(log_reg_over,y_test)","c5ab519d":"from imblearn.under_sampling import RandomUnderSampler\nrus = RandomUnderSampler(random_state = 1)\nX_train_un, y_train_un = rus.fit_resample(X_train, y_train)","3bdaa52a":"print(\"Before Under Sampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before Under Sampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nprint(\"After Under Sampling, counts of label '1': {}\".format(sum(y_train_un==1)))\nprint(\"After Under Sampling, counts of label '0': {} \\n\".format(sum(y_train_un==0)))\n\nprint('After Under Sampling, the shape of train_X: {}'.format(X_train_un.shape))\nprint('After Under Sampling, the shape of train_y: {} \\n'.format(y_train_un.shape))","b4f6325d":"log_reg_under = LogisticRegression(random_state = 1, solver='liblinear')\nlog_reg_under.fit(X_train_un,y_train_un )","a54a716e":"kfold=StratifiedKFold(n_splits=5,shuffle=True,random_state=1)     #Setting number of splits equal to 5","1c358330":"#Calculating different metrics\nget_metrics_score(log_reg_under,X_train_un,X_test,y_train_un,y_test)\n\n# creating confusion matrix\nmake_confusion_matrix(log_reg_under,y_test)","6025af7b":"##  Function to calculate different metric scores of the model - Accuracy, Recall and Precision\ndef get_metrics_score(model,flag=True):\n    '''\n    model : classifier to predict values of X\n\n    '''\n    # defining an empty list to store train and test results\n    score_list=[] \n    \n    pred_train = model.predict(X_train)\n    pred_test = model.predict(X_test)\n    \n    train_acc = model.score(X_train,y_train)\n    test_acc = model.score(X_test,y_test)\n    \n    train_recall = metrics.recall_score(y_train,pred_train)\n    test_recall = metrics.recall_score(y_test,pred_test)\n    \n    train_precision = metrics.precision_score(y_train,pred_train)\n    test_precision = metrics.precision_score(y_test,pred_test)\n    \n    score_list.extend((train_acc,test_acc,train_recall,test_recall,train_precision,test_precision))\n        \n    # If the flag is set to True then only the following print statements will be dispayed. The default value is set to True.\n    if flag == True: \n        print(\"Accuracy on training set : \",model.score(X_train,y_train))\n        print(\"Accuracy on test set : \",model.score(X_test,y_test))\n        print(\"Recall on training set : \",metrics.recall_score(y_train,pred_train))\n        print(\"Recall on test set : \",metrics.recall_score(y_test,pred_test))\n        print(\"Precision on training set : \",metrics.precision_score(y_train,pred_train))\n        print(\"Precision on test set : \",metrics.precision_score(y_test,pred_test))\n    \n    return score_list # returning the list with train and test scores","d3274171":"## Function to create confusion matrix\ndef make_confusion_matrix(model,y_actual,labels=[1, 0]):\n    '''\n    model : classifier to predict values of X\n    y_actual : ground truth  \n    \n    '''\n    y_predict = model.predict(X_test)\n    cm=metrics.confusion_matrix( y_actual, y_predict, labels=[0, 1])\n    df_cm = pd.DataFrame(cm, index = [i for i in [\"Actual - No\",\"Actual - Yes\"]],\n                  columns = [i for i in ['Predicted - No','Predicted - Yes']])\n    group_counts = [\"{0:0.0f}\".format(value) for value in\n                cm.flatten()]\n    group_percentages = [\"{0:.2%}\".format(value) for value in\n                         cm.flatten()\/np.sum(cm)]\n    labels = [f\"{v1}\\n{v2}\" for v1, v2 in\n              zip(group_counts,group_percentages)]\n    labels = np.asarray(labels).reshape(2,2)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cm, annot=labels,fmt='')\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","7b6233b1":"dtree = DecisionTreeClassifier(criterion='gini',class_weight={0:0.43,1:0.57},random_state=1)","f1da6ad9":"dtree.fit(X_train, y_train)","6d8d55ac":"make_confusion_matrix(dtree,y_test)","6b805cc0":"dtree_score = get_metrics_score(dtree)","b2fb503e":"#base_estimator for bagging classifier is a decision tree by default\nbagging_estimator=BaggingClassifier(random_state=1)\nbagging_estimator.fit(X_train,y_train)","f194f042":"#Using above defined function to get accuracy, recall and precision on train and test set\nbagging_estimator_score=get_metrics_score(bagging_estimator)","e982808c":"make_confusion_matrix(bagging_estimator,y_test)","3f0419dc":"#Train the random forest classifier\nrf_estimator=RandomForestClassifier(random_state=1)\nrf_estimator.fit(X_train,y_train)","5b639c18":"#Using above defined function to get accuracy, recall and precision on train and test set\nrf_estimator_score=get_metrics_score(rf_estimator)","5e77afda":"make_confusion_matrix(rf_estimator,y_test)","7c55a1e1":"abc = AdaBoostClassifier(random_state=1)\nabc.fit(X_train,y_train)","59d4d471":"#Using above defined function to get accuracy, recall and precision on train and test set\nabc_score=get_metrics_score(abc)","c4c8a317":"make_confusion_matrix(abc,y_test)","6d708b07":"gbc = GradientBoostingClassifier(random_state=1)\ngbc.fit(X_train,y_train)","a61c7db9":"#Using above defined function to get accuracy, recall and precision on train and test set\ngbc_score=get_metrics_score(gbc)","0997a87c":"make_confusion_matrix(gbc,y_test)","6c160171":"xgb = XGBClassifier(random_state=1,eval_metric='logloss')\nxgb.fit(X_train,y_train)","32c8f52c":"#Using above defined function to get accuracy, recall and precision on train and test set\nxgb_score=get_metrics_score(xgb)","35a69b47":"make_confusion_matrix(xgb,y_test)","fcf4b7fc":"# defining list of models\nmodels = [dtree, bagging_estimator, rf_estimator, abc, gbc, xgb, lr,\n            log_reg_over, log_reg_under]\n\n# defining empty lists to add train and test results\nacc_train = []\nacc_test = []\nrecall_train = []\nrecall_test = []\nprecision_train = []\nprecision_test = []\n\n# looping through all the models to get the accuracy, recall and precision scores\nfor model in models:\n    j = get_metrics_score(model,False)\n    acc_train.append(np.round(j[0],2))\n    acc_test.append(np.round(j[1],2))\n    recall_train.append(np.round(j[2],2))\n    recall_test.append(np.round(j[3],2))\n    precision_train.append(np.round(j[4],2))\n    precision_test.append(np.round(j[5],2))## Comparing all models","4036b4af":"comparison_frame = pd.DataFrame({'Model':['Decision Tree', 'Bagging', 'Random Forest', 'AdaBoost', \n                                          'Gradient Boosting','XGBoost','Logistic Regression', 'Logistic Regression OverSampled',\n                                          'Logistic Regression UnderSample'], \n                                          'Train_Accuracy': acc_train,'Test_Accuracy': acc_test,\n                                          'Train_Recall':recall_train,'Test_Recall':recall_test,\n                                          'Train_Precision':precision_train,'Test_Precision':precision_test}) \ncomparison_frame","3f9f98da":"%%time \n\n#Creating pipeline\npipe=make_pipeline(StandardScaler(),XGBClassifier(random_state=1,eval_metric='logloss', n_estimators = 50))\n\n#Parameter grid to pass in GridSearchCV\nparam_grid={'xgbclassifier__n_estimators':np.arange(50,300,50),\n            'xgbclassifier__scale_pos_weight':[0,1,2,5,10],\n            'xgbclassifier__learning_rate':[0.01,0.1,0.2,0.05],\n            'xgbclassifier__gamma':[0,1,3,5],\n            'xgbclassifier__subsample':[0.7,0.8,0.9,1],\n           'xgbclassifier__max_depth':np.arange(1,10,1),\n            'xgbclassifier__reg_lambda':[0,1,2,5,10]}\n\n\n\n\n\n# Type of scoring used to compare parameter combinations\nscorer = metrics.make_scorer(metrics.recall_score)\n\n#Calling RandomizedSearchCV\nrandomized_cv = RandomizedSearchCV(estimator=pipe, param_distributions=param_grid, n_iter=50, scoring=scorer, cv=5, random_state=1)\n\n#Fitting parameters in RandomizedSearchCV\nrandomized_cv.fit(X_train,y_train)\n\nprint(\"Best parameters are {} with CV score={}:\" .format(randomized_cv.best_params_,randomized_cv.best_score_))","e121de11":"# Creating new pipeline with best parameters\nxgb_tuned2 = Pipeline(\n    steps=[\n        (\"scaler\", StandardScaler()),\n        (\n            \"XGB\",\n            XGBClassifier(\n                random_state=1,\n                n_estimators=250,\n                scale_pos_weight=10,\n                gamma=1,\n                subsample=1,\n                learning_rate= 0.05,\n                eval_metric='logloss', max_depth = 1, reg_lambda = 2\n            ),\n        ),\n    ]\n)\n# Fit the model on training data\nxgb_tuned2.fit(X_train, y_train)","64de0a88":"# Calculating different metrics\nget_metrics_score(xgb_tuned2)\n\n# Creating confusion matrix\nmake_confusion_matrix(xgb_tuned2, y_test)","8a73c49e":"feature_names = X_train.columns\nimportances = xgb.feature_importances_\nindices = np.argsort(importances)\n\nplt.figure(figsize=(12, 12))\nplt.title(\"Feature Importances\")\nplt.barh(range(len(indices)), importances[indices], color=\"violet\", align=\"center\")\nplt.yticks(range(len(indices)), [feature_names[i] for i in indices])\nplt.xlabel(\"Relative Importance\")\nplt.show()","8bcc66b9":"# 7. Model Hyperparameter Tuning","6fdf41d4":"* I cannot spot abnormal values.","49950a41":"* Although the XGBoost with RandomizedSearchCV model gives us a higher Recall score, the other two measures (Precision and Accuracy) go down quite a lot, so I prefer to use the model withouth the tunning.","a31890b3":"# 4. Graphical visualization of the data","d5bf2bac":"## Context:\n\nThis dataset contains an airline passenger satisfaction survey. What factors are highly correlated to a satisfied (or dissatisfied) passenger? Can you predict passenger satisfaction?","b62c0506":"* We will build our model using the DecisionTreeClassifier function. Using default 'gini' criteria to split.\n\n* If the frequency of 1 is 43% and the frequency of 0 is 57%, then class 0 will become the dominant class and the decision tree will become biased toward the dominant classes.\n\n* In this case, we can pass a dictionary {0:0.43,1:0.57} to the model to specify the weight of each class and the decision tree will give more weightage to class 1.\n\n* class_weight is a hyperparameter for the decision tree classifier.","73d54b52":"* We can find more satisfied passengers among those flying longer distances.","c0bf0b7c":"* The percentage of not satisfied customer who flew Economy was: 81.2% and Eco Plus was: 75.4%\n* Only 25% of the Business class customers are not satisfied.","f3d1b952":"* An almost 50-50 split between customer gender","f1d3ea46":"## 6.2 Bagging Classifier Models","cbd4746b":"### RandomizedSearchCV","d18dd7b6":"**BEST PERFORMING MODELS IS XGBOOST**\n* It is the best performing model since it has the highest Test Recall value. Recall is the performance measure that we are interested in. High Recall values mean lower False Negatives.\n* I will use this model to do Hyperparameter tuning using Random search.","0ae2d06d":"* Since **null** values represent only 0.3% of the total data, I will just drop them.","39edf344":"# 5. One-hot encoding","b480c717":"# 1. Import Libraries","37b8df76":"* 90% of customer who flew for personal reasons was not satisfied with the services provided.","5ededfe9":"* None of the columns have values that we might consider out of place, so I will not be removing any outliers.","25391da4":"### Feature importance XGBoost model","6feb3c17":"* It gives us better results than the Decision Tree, but it does not generalise well on Recall.","9a1e9307":"**Let's evaluate the model performance by using KFold and cross_val_score**\n\n- `K-Folds cross-validator` provides dataset indices to split data into train\/validation sets. Split dataset into k consecutive stratified folds (without shuffling by default). Each fold is then used once as validation while the k - 1 remaining folds form the training set.","72b8b773":"* People who were overall satisfied with the trips, rated high services such as **food and drink**, **online boarding**, **seat comfort**, **'inflight entertainment**, **on-board service**, **leg room service**, **baggage handling**, **checkin service**, **inflight service** and **cleanliness**.","c494246d":"# 3. Processing columns","855a81e3":"## 1 - Decision Tree","13c31e7e":"* 57% of customers were not satisfied with the services provided by the airline\/s.","cd369f62":"## 7.1 XGBoost","ad271b07":"* No duplicates on the dataset.","c5a905a1":"* Around 82% of customers are FF (Frequent Fliers)","62997612":"* It gives us the best results and it generalises well on Accuracy, but still overfits the Recall.","f3925a84":"## Bivariate analysis","7f3f75bf":"* Similar split between Business and Economy cabins.\n* Very few customers flew Premium economy.","1967ad70":"* In general customers did not give many zeros.","baea8556":"## Model evaluation criterion\n\n### Model can make wrong predictions as:\nFP: Predicting a customer was satisfied when they were not.  \nFN: Predicting a customer was not satisfied when they were.\n\n### Which Loss is greater ?\nWe want to make sure we identify those customers who were satisfied with the airline's services, so we can identify which one of them are working better in order to boost them.\n\n### How to reduce this loss i.e need to reduce False Negatives ?\nCompany wants Recall to be maximized, since greater the recall lesser the chances of false negatives","656bef52":"* Model generalises well and gives us even better score results than with AdaBoost.","5d41bbfa":"* Logistic Regression has given a generalized performance on training and test set.\n* Recall has improved largely and it is not overfitting much.","be046e0b":"### Checking Outliers","f91d2ebf":"### Oversampling train data using SMOTE","59a4f5b8":"## 1 - AdaBoost","ebf137aa":"## Content:\n\n* Gender: Gender of the passengers (Female, Male)\n\n* Customer Type: The customer type (Loyal customer, disloyal customer)\n\n* Age: The actual age of the passengers\n\n* Type of Travel: Purpose of the flight of the passengers (Personal Travel, Business Travel)\n\n* Class: Travel class in the plane of the passengers (Business, Eco, Eco Plus)\n\n* Flight distance: The flight distance of this journey\n\n* Inflight wifi service: Satisfaction level of the inflight wifi service (0:Not Applicable;1-5)\n\n* Departure\/Arrival time convenient: Satisfaction level of Departure\/Arrival time convenient\n\n* Ease of Online booking: Satisfaction level of online booking\n\n* Gate location: Satisfaction level of Gate location\n\n* Food and drink: Satisfaction level of Food and drink\n\n* Online boarding: Satisfaction level of online boarding\n\n* Seat comfort: Satisfaction level of Seat comfort\n\n* Inflight entertainment: Satisfaction level of inflight entertainment\n\n* On-board service: Satisfaction level of On-board service\n\n* Leg room service: Satisfaction level of Leg room service\n\n* Baggage handling: Satisfaction level of baggage handling\n\n* Check-in service: Satisfaction level of Check-in service\n\n* Inflight service: Satisfaction level of inflight service\n\n* Cleanliness: Satisfaction level of Cleanliness\n\n* Departure Delay in Minutes: Minutes delayed when departure\n\n* Arrival Delay in Minutes: Minutes delayed when Arrival\n\n* Satisfaction: Airline satisfaction level(Satisfaction, neutral or dissatisfaction)","c3c522d1":"### Checking if we have any strange value within our Categorical variables.","5f836073":"### Some further interesting analysis would be which services got the worst rates from the 2 different type of customers.","c8a461b3":"# 6.Split Data","d3f7ae4e":"## 3 - XGBoost Classifier","96368096":"### Checking Null Values","33c876fc":"## 6.1 Logistic Regression ","ac4e6ba6":"* Model generalises well and gives us high score results.","86cdf806":"* In terms of percentages, Disloyal customers were in general more disatisfied than Loyal ones.","113a2a7d":"* All columns have 129880 rows respectively, with the exception of **arrival delay in minutes**. I will also check for strange values.\n* I will transform column `satisfaction` into numerical values (0 and 1) as follows:\n    * neutral or dissatisfied - 0\n    * satisfied - 1\n* Object columns need to be made Categorical.","a5854023":"* Customers' Age spans from 7 to 85\n* 75% of the flights have a delay at either departure or arrival of 13 minutes or less.","e1648aa9":"* 69% of customers flew for business reasons.","45be4eba":"## Univaraite analysis - Categorical Variables","416ba98e":"* We get good results in training, but the model is slightly overfitting.","b64b3b2f":"## 2 - Bagging Classifier","fc2fd1fa":"# 2. Loading and exploring the data","7cdb7cf0":"* Logistic Regression has given a generalized performance on training and test set.\n* Recall is not overfitting much less than in the Oversampled model, but giving a slightly worse result.\n* Accuracy has worsened compared to the Oversampled model.","036a3c40":"### Fixing the data types","52d2fa1d":"### Undersampling train data using SMOTE","10a84cd3":"## 2 - Gradient Boosting Classifier","8b8a2b17":"* Mostly Business customers fly Business class.\n* More than 40% of the Economy-class cabin are Business customers.\n* 50-50 split for Economy Plus.","7e11f535":"### Checking for duplicates","255aa8bc":"* In general, people flying for Personal reasons seem to be disatisfied with the services received, independently of the cabin they fly.\n* The high percentages of disatisfied customers in Eco and Eco Plus mean that these two cabins need to improve some \/ most \/ all their services.","610d1b59":"* Since all the values in **id** columns are unique, we can drop them.\n* Values in column **Unnamed: 0** are 0, 1, 2 and so on. Since we merged two different datasets, values 0 through 25975 are displayed twice, but still this column has no purpose in this analysis, so we can drop it.","539e1109":"### Summary of the data","99eff156":"## 3 - Random Forest Classifier","fea53cb4":"### Logistic Regression on UNDER-SAMPLED data","d482b9e3":"### Logistic Regression on OVER-SAMPLED data","3a000565":"## Univaraite analysis - Numerical Variables","5a45c031":"### Substituting `satisfaction` values for 0 and 1","9e69059b":"### Let's check the number of unique values in each column","56f32de2":"## Comparing all models in order to choose top-3 performers","e442f55c":"* Logistic Regression has given a generalized performance on training and test set.\n* Recall is poor, so let's see if the model performance can be further improved by:\n\n  a) Oversampling - getting more data points for the minority class.\n  \n  b) Undersampling - dealing with the class imbalance.","952f3336":"* When classification problems exhibit a significant imbalance in the distribution of the target classes, it is good to use stratified sampling to ensure that relative class frequencies are approximately preserved in train and test sets. \n* This is done using the `stratify` parameter in the train_test_split function.\n* In this dataset, there is not a big imbalance in the distribution of the target classes.","2ac84e0f":"## 8. INSIGHTS\n\n* Although some hyperparameter tunning might throw better results, the XGBoost model gave us already very good ones.\n* According to the model's importance features, we can observe that the top-3 services that the airline\/s need to focus on are **Online boarding**, customers flying for **Personal reasons** and the **Inflight wifi services**. As we can see from the EDA (Exploratory Data Analysis), customers who were satisfied with the services provided, valued highly the Online boarding and the Inflight WiFi, where those not that satisfied, gave a poorer rate to these 2 services. We can also see, that 90% of customers who flew for personal reasons were not satisfied with the airline\/s, independently of the cabin they flew in.","9c640beb":"* As I was expecting, XGBoost has given us the best score results so far, and the model is not overfitting.","8f2a9a84":"## 6.3 Boosting Classifier Models"}}