{"cell_type":{"00eee00a":"code","96b011db":"code","b64c4638":"code","eb670008":"code","0c73948a":"code","092c3578":"code","7cd053b3":"code","88ea9153":"code","65a719cd":"code","549ec843":"code","3d743ed3":"code","2c51742d":"code","2cf54af2":"code","52daae06":"code","553c2510":"code","58cc7d14":"code","658df29a":"code","e14dcaed":"code","fc0b9def":"code","69413239":"code","9862d939":"code","c370be82":"code","d225b5f0":"code","7fd0570f":"code","31173dcd":"code","47c8f823":"code","df04d102":"code","05e5fb44":"code","75517868":"code","8826df1c":"markdown","6ec0099c":"markdown","27427676":"markdown","2083f61e":"markdown","18540a62":"markdown","4f59e614":"markdown","c13890e3":"markdown","5de6ee63":"markdown","a0c0dd2f":"markdown","aeb419db":"markdown","2c7d5eed":"markdown","fb78ca48":"markdown","e05a9e4b":"markdown","46a40cb3":"markdown","bc5085b7":"markdown","778490bc":"markdown","b71fe493":"markdown","902d1222":"markdown","72e4f4f4":"markdown","ef063bf3":"markdown","7f51778d":"markdown","ea879955":"markdown","8ab9dbc6":"markdown","8d8cc8f2":"markdown","85e79f72":"markdown","ef0af92a":"markdown","02edebc2":"markdown","31e4e3e0":"markdown","c0b14d99":"markdown","6da783d9":"markdown","dec05c36":"markdown","c92f2b4a":"markdown","8e25dd4f":"markdown","905b73be":"markdown"},"source":{"00eee00a":"from plotly.offline import init_notebook_mode, iplot\nimport plotly.graph_objs as go\nimport numpy as np\ninit_notebook_mode(connected=True)\n\n## generate random data\nN = 50\nrandom_x = np.linspace(2, 10, N)\nrandom_y1 = np.linspace(2, 10, N)\nrandom_y2 = np.linspace(2, 10, N)\n\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\", name=\"Actual Data\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\", name=\"Model\")\nlayout = go.Layout(title=\"2D Data Repersentation Space\", xaxis=dict(title=\"x2\", range=(0,12)), \n                   yaxis=dict(title=\"x1\", range=(0,12)), height=400, \n                   annotations=[dict(x=5, y=5, xref='x', yref='y', text='This 1D line is the Data Manifold (where data resides)',\n                   showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                   ax=-120, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8)])\nfigure = go.Figure(data = [trace1], layout = layout)\niplot(figure)","96b011db":"random_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"x1\", range=(0,12)), yaxis=dict(title=\"x2\", range=(0,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"2D Data Repersentation Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)\n\n\n\n#################\n\nrandom_y3 = [2 for i in range(100)]\nrandom_y4 = random_y2 + 1\ntrace4 = go.Scatter(x = random_x[4:24], y = random_y4[4:300], mode=\"lines\")\ntrace3 = go.Scatter(x = random_x, y = random_y3, mode=\"lines\")\ntrace1 = go.Scatter(x = random_x, y = random_y1, mode=\"markers\")\ntrace2 = go.Scatter(x = random_x, y = random_y2, mode=\"lines\")\nlayout = go.Layout(xaxis=dict(title=\"u1\", range=(1.5,12)), yaxis=dict(title=\"u2\", range=(1.5,12)), height=400,\n                   annotations=[dict(x=2, y=2, xref='x', yref='y', text='A', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, \n                                     arrowcolor='#636363', ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='orange', opacity=0.8), \n                                dict(x=6, y=6, xref='x', yref='y', text='B', showarrow=True, align='center', arrowhead=2, arrowsize=1, arrowwidth=2, arrowcolor='#636363',\n                                     ax=20, ay=-30, bordercolor='#c7c7c7', borderwidth=2, borderpad=4, bgcolor='yellow', opacity=0.8), dict(\n                                     x=4, y=5, xref='x', yref='y',text='d', ay=-40), \n                                dict(x=2, y=2, xref='x', yref='y', text='angle L', ax=80, ay=-10)], title=\"Latent Distance View Space\", showlegend=False)\ndata = [trace1, trace2, trace3, trace4]\nfigure = go.Figure(data = data, layout = layout)\niplot(figure)","b64c4638":"import matplotlib.pyplot as plt \nimport numpy as np\nfs = 100 # sample rate \nf = 2 # the frequency of the signal\nx = np.arange(fs) # the points on the x axis for plotting\ny = [ np.sin(2*np.pi*f * (i\/fs)) for i in x]\n\n% matplotlib inline\nplt.figure(figsize=(15,4))\nplt.stem(x,y, 'r', );\nplt.plot(x,y);","eb670008":"## load the libraries \nfrom keras.layers import Dense, Input, Conv2D, LSTM, MaxPool2D, UpSampling2D\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import to_categorical\nfrom numpy import argmax, array_equal\nimport matplotlib.pyplot as plt\nfrom keras.models import Model\nfrom imgaug import augmenters\nfrom random import randint\nimport pandas as pd\nimport numpy as np","0c73948a":"### read dataset \ntrain = pd.read_csv(\"..\/input\/fashion-mnist_train.csv\")\ntrain_x = train[list(train.columns)[1:]].values\ntrain_y = train['label'].values\n\n## normalize and reshape the predictors  \ntrain_x = train_x \/ 255\n\n## create train and validation datasets\ntrain_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2)\n\n## reshape the inputs\ntrain_x = train_x.reshape(-1, 784)\nval_x = val_x.reshape(-1, 784)","092c3578":"## input layer\ninput_layer = Input(shape=(784,))\n\n## encoding architecture\nencode_layer1 = Dense(1500, activation='relu')(input_layer)\nencode_layer2 = Dense(1000, activation='relu')(encode_layer1)\nencode_layer3 = Dense(500, activation='relu')(encode_layer2)\n\n## latent view\nlatent_view   = Dense(10, activation='sigmoid')(encode_layer3)\n\n## decoding architecture\ndecode_layer1 = Dense(500, activation='relu')(latent_view)\ndecode_layer2 = Dense(1000, activation='relu')(decode_layer1)\ndecode_layer3 = Dense(1500, activation='relu')(decode_layer2)\n\n## output layer\noutput_layer  = Dense(784)(decode_layer3)\n\nmodel = Model(input_layer, output_layer)","7cd053b3":"model.summary()","88ea9153":"model.compile(optimizer='adam', loss='mse')\nearly_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')\nmodel.fit(train_x, train_x, epochs=20, batch_size=2048, validation_data=(val_x, val_x), callbacks=[early_stopping])","65a719cd":"preds = model.predict(val_x)","549ec843":"from PIL import Image \nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(val_x[i].reshape(28, 28))\nplt.show()","3d743ed3":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5):\n    ax[i].imshow(preds[i].reshape(28, 28))\nplt.show()\n","2c51742d":"## recreate the train_x array and val_x array\ntrain_x = train[list(train.columns)[1:]].values\ntrain_x, val_x = train_test_split(train_x, test_size=0.2)\n\n## normalize and reshape\ntrain_x = train_x\/255.\nval_x = val_x\/255.","2cf54af2":"train_x = train_x.reshape(-1, 28, 28, 1)\nval_x = val_x.reshape(-1, 28, 28, 1)","52daae06":"# Lets add sample noise - Salt and Pepper\nnoise = augmenters.SaltAndPepper(0.1)\nseq_object = augmenters.Sequential([noise])\n\ntrain_x_n = seq_object.augment_images(train_x * 255) \/ 255\nval_x_n = seq_object.augment_images(val_x * 255) \/ 255","553c2510":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x[i].reshape(28, 28))\nplt.show()","58cc7d14":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(train_x_n[i].reshape(28, 28))\nplt.show()","658df29a":"# input layer\ninput_layer = Input(shape=(28, 28, 1))\n\n# encoding architecture\nencoded_layer1 = Conv2D(64, (3, 3), activation='relu', padding='same')(input_layer)\nencoded_layer1 = MaxPool2D( (2, 2), padding='same')(encoded_layer1)\nencoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(encoded_layer1)\nencoded_layer2 = MaxPool2D( (2, 2), padding='same')(encoded_layer2)\nencoded_layer3 = Conv2D(16, (3, 3), activation='relu', padding='same')(encoded_layer2)\nlatent_view    = MaxPool2D( (2, 2), padding='same')(encoded_layer3)\n\n# decoding architecture\ndecoded_layer1 = Conv2D(16, (3, 3), activation='relu', padding='same')(latent_view)\ndecoded_layer1 = UpSampling2D((2, 2))(decoded_layer1)\ndecoded_layer2 = Conv2D(32, (3, 3), activation='relu', padding='same')(decoded_layer1)\ndecoded_layer2 = UpSampling2D((2, 2))(decoded_layer2)\ndecoded_layer3 = Conv2D(64, (3, 3), activation='relu')(decoded_layer2)\ndecoded_layer3 = UpSampling2D((2, 2))(decoded_layer3)\noutput_layer   = Conv2D(1, (3, 3), padding='same')(decoded_layer3)\n\n# compile the model\nmodel_2 = Model(input_layer, output_layer)\nmodel_2.compile(optimizer='adam', loss='mse')","e14dcaed":"model_2.summary()","fc0b9def":"early_stopping = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=5, mode='auto')\nhistory = model_2.fit(train_x_n, train_x, epochs=20, batch_size=2048, validation_data=(val_x_n, val_x), callbacks=[early_stopping])","69413239":"f, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(val_x_n[i].reshape(28, 28))\nplt.show()","9862d939":"preds = model_2.predict(val_x_n[:10])\nf, ax = plt.subplots(1,5)\nf.set_size_inches(80, 40)\nfor i in range(5,10):\n    ax[i-5].imshow(preds[i].reshape(28, 28))\nplt.show()","c370be82":"def dataset_preparation(n_in, n_out, n_unique, n_samples):\n    X1, X2, y = [], [], []\n    for _ in range(n_samples):\n        ## create random numbers sequence - input \n        inp_seq = [randint(1, n_unique-1) for _ in range(n_in)]\n        \n        ## create target sequence\n        target = inp_seq[:n_out]\n    \n        ## create padded sequence \/ seed sequence \n        target_seq = list(reversed(target))\n        seed_seq = [0] + target_seq[:-1]  \n        \n        # convert the elements to categorical using keras api\n        X1.append(to_categorical([inp_seq], num_classes=n_unique))\n        X2.append(to_categorical([seed_seq], num_classes=n_unique))\n        y.append(to_categorical([target_seq], num_classes=n_unique))\n    \n    # remove unnecessary dimention\n    X1 = np.squeeze(np.array(X1), axis=1) \n    X2 = np.squeeze(np.array(X2), axis=1) \n    y  = np.squeeze(np.array(y), axis=1) \n    return X1, X2, y\n\nsamples = 100000\nfeatures = 51\ninp_size = 6\nout_size = 3\n\ninputs, seeds, outputs = dataset_preparation(inp_size, out_size, features, samples)\nprint(\"Shapes: \", inputs.shape, seeds.shape, outputs.shape)\nprint (\"Here is first categorically encoded input sequence looks like: \", )\ninputs[0][0]","d225b5f0":"def define_models(n_input, n_output):\n    ## define the encoder architecture \n    ## input : sequence \n    ## output : encoder states \n    encoder_inputs = Input(shape=(None, n_input))\n    encoder = LSTM(128, return_state=True)\n    encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n    encoder_states = [state_h, state_c]\n\n    ## define the encoder-decoder architecture \n    ## input : a seed sequence \n    ## output : decoder states, decoded output \n    decoder_inputs = Input(shape=(None, n_output))\n    decoder_lstm = LSTM(128, return_sequences=True, return_state=True)\n    decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n    decoder_dense = Dense(n_output, activation='softmax')\n    decoder_outputs = decoder_dense(decoder_outputs)\n    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n    \n    ## define the decoder model\n    ## input : current states + encoded sequence\n    ## output : decoded sequence\n    encoder_model = Model(encoder_inputs, encoder_states)\n    decoder_state_input_h = Input(shape=(128,))\n    decoder_state_input_c = Input(shape=(128,))\n    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n    decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n    decoder_states = [state_h, state_c]\n    decoder_outputs = decoder_dense(decoder_outputs)\n    decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)\n\n    return model, encoder_model, decoder_model\n\nautoencoder, encoder_model, decoder_model = define_models(features, features)","7fd0570f":"encoder_model.summary()","31173dcd":"decoder_model.summary()\n","47c8f823":"autoencoder.summary()","df04d102":"autoencoder.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\nautoencoder.fit([inputs, seeds], outputs, epochs=1)","05e5fb44":"def reverse_onehot(encoded_seq):\n    return [argmax(vector) for vector in encoded_seq]\n\ndef predict_sequence(encoder, decoder, sequence):\n    output = []\n    target_seq = np.array([0.0 for _ in range(features)])\n    target_seq = target_seq.reshape(1, 1, features)\n\n    current_state = encoder.predict(sequence)\n    for t in range(out_size):\n        pred, h, c = decoder.predict([target_seq] + current_state)\n        output.append(pred[0, 0, :])\n        current_state = [h, c]\n        target_seq = pred\n    return np.array(output)","75517868":"for k in range(5):\n    X1, X2, y = dataset_preparation(inp_size, out_size, features, 1)\n    target = predict_sequence(encoder_model, decoder_model, X1)\n    print('\\nInput Sequence=%s SeedSequence=%s, PredictedSequence=%s' \n          % (reverse_onehot(X1[0]), reverse_onehot(y[0]), reverse_onehot(target)))","8826df1c":"#### 2. Dataset Prepration\n\n\ub370\uc774\ud130\uc14b\uc744 \ubd88\ub7ec\uc624\uace0, \uc608\uce21\uac12\uacfc \ud0c0\uac9f\uc744 \ubd84\ub9ac\ud558\uba70, \uc785\ub825 \ub370\uc774\ud130\ub97c \uc815\uaddc\ud654\ud569\ub2c8\ub2e4.","6ec0099c":"\uc774\uc81c \uc785\ub825 \uc2dc\ud000\uc2a4\ub97c \uc774\uc6a9\ud574 \uc2dc\ud000\uc2a4\ub97c \uc608\uce21\ud558\ub294 \ud568\uc218\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.","27427676":"- \ub178\uc774\uc988 \uc801\uc6a9\ub41c \uac80\uc99d \ub370\uc774\ud130","2083f61e":"\uc774\ubc88\uc5d0\ub3c4 \uc870\uae30 \ud559\uc2b5 \uc885\ub8cc\ub85c \ud559\uc2b5\uc2dc\ucf1c\ubd05\uc2dc\ub2e4. \ub354 \uc88b\uc740 \uacb0\uacfc\ub97c \uc6d0\ud55c\ub2e4\uba74 epochs \uc218\ub97c \ub298\ub9ac\uba74 \ub429\ub2c8\ub2e4.","18540a62":"- \ub178\uc774\uc988\ub97c \uc5c6\uc564 \uc624\ud1a0\uc778\ucf54\ub529 \ud6c4 \uac80\uc99d \ub370\uc774\ud130","4f59e614":"\uac01 \ubaa8\ub378\uc744 \ud655\uc778\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.","c13890e3":"\uc5ec\uae30\uc11c\ub294 \uc801\uc740 epoch\ub85c \uc2dc\ub3c4\ud588\uae30\uc5d0 \ube44\uad50\uc801 \ubd88\ucda9\ubd84\ud55c \uacb0\uacfc\uac00 \ub098\uc62c \uc218 \uc788\uc9c0\ub9cc, 500~1000 epoch \uc815\ub3c4\ub85c \uc2dc\ub3c4\ud558\uba74 \ub354 \uc88b\uc740 \uacb0\uacfc\uac00 \ub098\uc62c \uac83\uc785\ub2c8\ub2e4.","5de6ee63":"#### 3. Create Autoencoder architecture\n\n\uc774\uc81c \uc624\ud1a0\uc778\ucf54\ub354 \uad6c\uc870\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4. \uc778\ucf54\ub529 \ubd80\ubd84\uc740 3\uac1c\uc758 \ub808\uc774\uc5b4\ub85c \uad6c\uc131\ub429\ub2c8\ub2e4. (2000, 1200, 500 \ub178\ub4dc\ub85c \uad6c\uc131\ub41c)\n\uc778\ucf54\ub529 \uad6c\uc870\ub294 \uc7a0\uc7ac \uacf5\uac04\uc5d0 10\uac1c\uc758 \ub178\ub4dc\ub85c \uc5f0\uacb0\ub418\uace0, \uc774 10\uac1c\ub294 \ub2e4\uc2dc \uac01\uac01 500, 1200, 2000\uac1c\uc758 \ub178\ub4dc\ub85c \uad6c\uc131\ub41c 3\uac1c\uc758 \ub514\ucf54\ub529 \uad6c\uc870\ub85c \uc5f0\uacb0\ub429\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \ub9c8\uc9c0\ub9c9\uc5d0 \ucc98\uc74c \uc778\ud48b\uacfc \uac19\uc740 \ub178\ub4dc\uc758 \uc218\ub85c \ub9de\ucdb0\uc90d\ub2c8\ub2e4. ","a0c0dd2f":"\uc774\uc81c \ucf00\ub77c\uc2a4\uc5d0\uc11c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4.","aeb419db":"#### Noisy Images\n\n\uc6b0\ub9ac\ub294 \uc758\ub3c4\uc801\uc73c\ub85c \uc774\ubbf8\uc9c0\uc5d0 \ub178\uc774\uc988\ub97c \ucd94\uac00\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc774\ubbf8\uc9c0\ub97c \ubcf4\uc644\ud558\ub294 imaug \ud328\ud0a4\uc9c0\ub97c \uc774\uc6a9\ud558\uc5ec, \ubc18\ub300\ub85c \uc774\ubbf8\uc9c0\uc5d0 \ub178\uc774\uc988\ub97c \ub9cc\ub4e4 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\ub178\uc774\uc988\uc5d0\ub294 \ub2e4\uc74c\uacfc \uac19\uc740 \uac83\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n\n- Salt and Pepper Noise\n- Gaussian Noise\n- Periodic Noise\n- Speckle Noise\n\n\uc5ec\uae30\uc11c\ub294 impulse noise\ub77c\uace0 \ubd88\ub9ac\ub294 Salt and Pepper Noise\ub97c \uc0ac\uc6a9\ud588\uc2b5\ub2c8\ub2e4.\n\uc774 \ub178\uc774\uc988\ub294 \uc120\uba85\ud558\uace0 \uac11\uc791\uc2a4\ub7ec\uc6b4 \ub178\uc774\uc988\ub97c \ub9cc\ub4ed\ub2c8\ub2e4. \ud76c\uc18c\ud558\uac8c \uac80\uc815\/\ud770 \ud53d\uc140\uc744 \ub9cc\ub4ed\ub2c8\ub2e4.\n\n\uc6d0\ubcf8 \ucee4\ub110\uc5d0\uc11c \ub178\uc774\uc988\uc5d0 \ub300\ud574 \uc218\uc815\ud574\uc8fc\uc2e0 @ColinMorris\uc5d0\uac8c \ub2e4\uc2dc \uac10\uc0ac\ud568\uc744 \uc804\ud569\ub2c8\ub2e4.\n\nThanks to @ColinMorris for suggesting the correction in salt and pepper noise.","2c7d5eed":"### Excellent References\n- https:\/\/www.analyticsvidhya.com\/blog\/2018\/06\/unsupervised-deep-learning-computer-vision\/\n- https:\/\/towardsdatascience.com\/applied-deep-learning-part-3-autoencoders-1c083af4d798\n- https:\/\/blog.keras.io\/building-autoencoders-in-keras.html\n- https:\/\/cs.stanford.edu\/people\/karpathy\/convnetjs\/demo\/autoencoder.html\n- https:\/\/machinelearningmastery.com\/develop-encoder-decoder-model-sequence-sequence-prediction-keras\/\n\n\n\ub9c8\uc9c0\ub9c9\uc73c\ub85c \uc774\ub807\uac8c \uc88b\uc740 \ucee4\ub110\uc744 \ub9cc\ub4e4\uc5b4\uc8fc\uc2e0 @ShivamBansal\ub2d8\uc5d0\uac8c \ub2e4\uc2dc \ud55c\ubc88 \uac10\uc0ac\uc758 \uc778\uc0ac\ub97c \ub4dc\ub9bd\ub2c8\ub2e4.\n\nFinally, thanks to @ShivamBansal for making such a good kernel.","fb78ca48":"\ud558\uc9c0\ub9cc \uc5ec\uae30\uc11c \ud575\uc2ec \uc9c8\ubb38\uc740 '\uc5b4\ub5a4 \ub17c\ub9ac\ub098 \uaddc\uce59\uc5d0 \uc758\ud574 B\uac00 A\uc640 L\uc744 \ud1b5\ud574 \ud45c\ud604\ub420 \uc218 \uc788\ub294\uac00' \uc785\ub2c8\ub2e4.\n\ub610 \ub2e4\ub978 \ub9d0\ub85c B, A, L\uac04\uc758 \ubc29\uc815\uc2dd\uc774 \uc5b4\ub5bb\uac8c \ub418\ub294\uc9c0\ub97c \uc54c\uc544\uc57c\ud569\ub2c8\ub2e4.\n\n\uace0\uc815\ub41c \ubc29\uc815\uc2dd\uc774 \uc5c6\uc5b4\ub3c4, \ube44\uc9c0\ub3c4 \ud559\uc2b5\uc5d0 \uc758\ud574 \uac00\ub2a5\ud55c \ucd5c\uc0c1\uc758 \ubc29\uc815\uc2dd\uc744 \uad6c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uac04\ub2e8\ud788 \ub9d0\ud574 \ud559\uc2b5 \uacfc\uc815\uc740 A\uc640 L\uc758 \ud615\ud0dc\ub85c B\ub97c \ubcc0\ud658\ud558\ub294 \uaddc\uce59\/\ubc29\uc815\uc2dd\uc73c\ub85c \uc815\uc758 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\uc624\ud1a0\uc778\ucf54\ub529 \uad00\uc810\uc5d0\uc11c \uc774 \uacfc\uc815\uc744 \uc774\ud574\ud574\ubd05\uc2dc\ub2e4.\n\nhidden layer\uac00 \uc5c6\ub294 \uc624\ud1a0\uc778\ucf54\ub354\ub97c \uac00\uc815\ud574\ubd05\uc2dc\ub2e4.\nx1, x2\ub294 \ub0ae\uc740 \ud45c\ud604 d\ub85c \ubc14\ub00c\uace0, \uc774 d\ub294 \ud6c4\uc5d0 \ub2e4\uc2dc x1, x2\ub85c \ud22c\uc601\ub429\ub2c8\ub2e4.(\ubcc0\ud658\ub429\ub2c8\ub2e4.)\n\n![simple autoencoding](https:\/\/i.imgur.com\/lfq4eEy.png)\n\n**Step1 : \uc7a0\uc7ac \uacf5\uac04\uc5d0 \ud3ec\uc778\ud2b8 \ud45c\ud604\ud558\uae30**\n\nA\uc640 B\ub97c \uc88c\ud45c\ub85c \ub098\ud0c0\ub0b8\ub2e4\uba74 \ub2e4\uc74c\uacfc \uac19\uc774 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- Point A : $(X1_A, X2_A)$\n- Point B : $(X1_B, X2_B)$\n\n\uc774\uc81c \uc774\ub97c \uc7a0\uc7ac \uacf5\uac04\uc5d0\uc11c\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- A : $(X1_A, X2_A) \\rightarrow (0, 0)$\n- B : $(X1_B, X2_B) \\rightarrow (u1_B, u2_B)$\n\n$ (u1_B, u2_B)$\ub294 \ub2e4\uc74c\uacfc \uac19\uc774 \uae30\uc900\uc810\uacfc\uc758 \uac70\ub9ac\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n$u1_B = X1_B - X1_A$\n$u2_B = X2_B - X2_A$\n\n**Step 2 : \uac70\ub9ac d\uc640 \uac01\ub3c4 L\ub85c \ud3ec\uc778\ud2b8 \ud45c\ud604\ud558\uae30**\n\n\uc774\uc81c $u1_B, u2_B$\ub97c \uac70\ub9ac d\uc640 \uac01\ub3c4 L\ub85c \ud45c\ud604\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uac01\ub3c4 L\ub9cc\ud07c \uae30\uc874 \ucd95\uc73c\ub85c \ud68c\uc804\ud55c\ub2e4\uba74 L\uc740 0\uc774 \ub420 \uac83\uc785\ub2c8\ub2e4. i.e \n\n$(d, L) \\rightarrow (d, 0)$ (after rotation)\n\n\uc774\uc81c \uc774 \ub370\uc774\ud130\uac00 \ucd9c\ub825\uc774 \ub418\uace0, \ub0ae\uc740 \ucc28\uc6d0\uc5d0\uc11c \ud45c\ud604\ud55c \uc785\ub825 \ub370\uc774\ud130\uc785\ub2c8\ub2e4.\n\n\ubaa8\ub4e0 \uacc4\uce35\uc758 \uac00\uc911\uce58\uc640 \ud3b8\ud5a5\uc774 \uc788\ub294 \uc2e0\uacbd\ub9dd\uc758 \uae30\ubcf8 \ubc29\uc815\uc2dd\uc744 \uc0dd\uac01\ud558\uba74 \ub2e4\uc74c\uacfc \uac19\uc740 \uacfc\uc815\uc774 \uc778\ucf54\ub529\uc774\ub77c\uace0 \uc0dd\uac01\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n= $((d, 0) = W \\cdot (u1_B, u2_B)$\n\n==> (encoding)\n\nW\ub294 hidden layer\uc758 \uac00\uc911\uce58 \ud589\ub82c\uc785\ub2c8\ub2e4. \uc774\ud6c4 \ub514\ucf54\ub529 \uacfc\uc815\uc740 \uc778\ucf54\ub529 \uacfc\uc815\uc744 \ubc18\ub300\ub85c \uc0dd\uac01\ud558\uba74 \ub429\ub2c8\ub2e4.\n\n=> $(u1_B, u2_B) =  Inverse(W) \\cdot (d, 0)$\n\n==> (decoding)\n\n\n### Different Rules for Different data\n\n\ubaa8\ub4e0 \uc720\ud615\uc758 \ub370\uc774\ud130\uac00 \ub611\uac19\uc740 \uaddc\uce59\uc774 \uc801\uc6a9\ub418\ub294 \uac83\uc740 \uc544\ub2d9\ub2c8\ub2e4. \uc608\uc2dc\ub85c \uc55e\uc5d0 \ub370\uc774\ud130\uc5d0\uc11c\ub294 1\ucc28\uc6d0 \uc120\ud615 \ub370\uc774\ud130 \ub9e4\ub2c8\ud3f4\ub4dc\uc5d0 \ud22c\uc601\ud558\uba74\uc11c \uac01\ub3c4 L\uc744 \uc5c6\uc574\uc2b5\ub2c8\ub2e4. \ud558\uc9c0\ub9cc \uc815\ud655\ud558\uac8c \ud22c\uc601 \ubd88\uac00\ub2a5\ud55c \ub370\uc774\ud130\uc5d0 \ub300\ud574\uc11c\ub294 \uc5b4\ub5a8\uae4c\uc694?\n\n\uc608\uc2dc\ub85c \ub2e4\uc74c \ub370\uc774\ud130\ub97c \uc0b4\ud3b4\ubd05\uc2dc\ub2e4.\n\n\n","e05a9e4b":"\uc774\ubc88 \uc624\ud1a0\uc778\ucf54\ub354 \ub124\ud2b8\uc6cc\ud06c\uc5d0\ub294 convolutional layer\uc744 \ucd94\uac00\ud569\ub2c8\ub2e4. \uc65c\ub0d0\ud558\uba74 convolutional networks\ub294 \uc774\ubbf8\uc9c0 \uc785\ub825\uc5d0 \ub300\ud574 \ub9e4\uc6b0 \uc798 \uc791\ub3d9\ud558\uae30 \ub54c\ubb38\uc785\ub2c8\ub2e4. \uc785\ub825 \ub370\uc774\ud130\ub97c convolutional network\uc5d0 \ub123\uae30 \uc704\ud574\uc11c\ub294 28 * 28 matrix\ub85c reshape\ud574\uc57c\ud569\ub2c8\ub2e4.","46a40cb3":"\uc774\uc81c \uc608\uce21\uc744 \ub9cc\ub4e4\uc5b4 \ubd05\uc2dc\ub2e4.\n","bc5085b7":"\uc774\uc81c \uc624\ud1a0\uc778\ucf54\ub354\ub97c \uc704\ud55c \ubaa8\ub378\uc744 \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4. \uc5b4\ub5a4 \uc885\ub958\uc758 \ub124\ud2b8\uc6cc\ud06c\uac00 \ud544\uc694\ud55c\uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4.\n\n**Encoding Architecture**\n\n\uc778\ucf54\ub529 \uad6c\uc870\ub294 3\uac1c\uc758 Convolutional Layer\uacfc 3\uac1c\uc758 Max Pooling \ub808\uc774\uc5b4\ub97c \ud558\ub098\ud558\ub098 \uc313\uc544 \uad6c\uc131\ub429\ub2c8\ub2e4.\nRelu\ub97c \ud65c\uc131\ud654\ud568\uc218\ub85c \uc0ac\uc6a9\ud558\uace0, `same` \ub9e4\uac1c\ubcc0\uc218\ub85c \uc774\ubbf8\uc9c0 \ud06c\uae30\ub97c \ud328\ub529\uc744 \ud1b5\ud574 \uc720\uc9c0\ud569\ub2c8\ub2e4.\n\nMax pooling layer\uc758 \uc5ed\ud560\uc740 \uc774\ubbf8\uc9c0 \ucc28\uc6d0\uc744 \ub2e4\uc6b4\uc0d8\ud50c\ub9c1\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ub429\ub2c8\ub2e4.\n\uc774 \ub808\uc774\uc5b4\ub294 \ucd08\uae30 \ud45c\ud604\uc758 \uacb9\uce58\uc9c0 \uc54a\ub294 \ubd80\ubd84 \uc601\uc5ed\uc5d0 \ucd5c\ub300 \ud544\ud130\ub97c \uc801\uc6a9\ud569\ub2c8\ub2e4.\n\n**Decoding Architecture**\n\n\ub514\ucf54\ub529 \uad6c\uc870\uc5d0\uc11c\ub3c4 \uac70\uc758 \uc720\uc0ac\ud558\uac8c 3\uac1c\uc758 Convolutional Layer\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud558\uc9c0\ub9cc Max Pooling layer 3\uac1c \ub300\uc2e0\uc5d0 unsampling layer 3\uac1c\ub97c \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \ud65c\uc131\ud654\ud568\uc218\uc640 \ud328\ub529\uc740 \uc778\ucf54\ub529\uacfc \ub3d9\uc77c\ud569\ub2c8\ub2e4.\n\nUnsampling layer\uc758 \uc5ed\ud560\uc740 \uc785\ub825 \ubca1\ud130\ub97c \ub354 \ub192\uc740 \ucc28\uc6d0\uc73c\ub85c \uc5c5\uc0d8\ud50c\ub9c1\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\nMax pooling \uc5f0\uc0b0\uc740 \ube44\uac00\uc5ed\uc774\uc9c0\ub9cc, \uac01 \ud480\ub9c1 \uc601\uc5ed \ub0b4\uc5d0 \ucd5c\ub300 \uac12\uc758 \uc704\uce58\ub97c \uae30\ub85d\ud568\uc73c\ub85c\uc368 \uadfc\uc0ac \uc5ed\uc744 \uad6c\ud560 \uc218\uc788\uc2b5\ub2c8\ub2e4. Umsampling \ub808\uc774\uc5b4\ub294 \uc774 \uc18d\uc131\uc744 \uc0ac\uc6a9\ud558\uc5ec \ub0ae\uc740 \ucc28\uc6d0\uc758 \ud2b9\uc9d5 \uacf5\uac04\uc5d0\uc11c \uc7ac\uad6c\uc131\ud569\ub2c8\ub2e4.\n","778490bc":"\uc774 \ub370\uc774\ud130\ub97c \ud45c\ud604\ud558\uae30 \uc704\ud574\uc11c\ub294, 2\uac1c\uc758 \ucc28\uc6d0\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. (X\ucd95\uacfc Y\ucd95) \n\ud558\uc9c0\ub9cc \ub2e4\uc74c\uacfc \uac19\uc740 \uc815\ubcf4\uac00 \uc788\ub2e4\uba74, \uc774 \ub370\uc774\ud130\ub97c \ud558\ub098\uc758 \ucc28\uc6d0, 1D\ub85c \ucd95\uc18c\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \n\n- \uae30\uc900\uc810 : A\n- \uc218\ud3c9\uc120\uacfc\uc758 \uac01\ub3c4 : L\n\n\uc704\uc5d0 \uc815\ubcf4\ub9cc \uac00\uc9c0\uace0 \uc788\ub2e4\uba74 \uc9c1\uc120 \uc704\uc758 \uc810 B\ub294 \uac70\ub9ac D\ub85c \ud45c\ud604\uac00\ub2a5\ud569\ub2c8\ub2e4.","b71fe493":"\uac80\uc99d \ub370\uc774\ud130\uc5d0 \uc608\uce21\uc744 \uc0dd\uc131\ud569\ub2c8\ub2e4.","902d1222":"- Before adding noise","72e4f4f4":"# How Autoencoders work - Understangin the math and implementation\n\n> \uc774 \ucee4\ub110\uc740 [Shivam Bansal\ub2d8\uc758 \uc624\ud1a0\uc778\ucf54\ub354 \uc608\uc2dc](https:\/\/www.kaggle.com\/shivamb\/how-autoencoders-work-intro-and-usecases)\uc744 \ubc88\uc5ed\ud55c \ucee4\ub110\uc785\ub2c8\ub2e4.\n\noriginal kernel : https:\/\/www.kaggle.com\/shivamb\/how-autoencoders-work-intro-and-usecases\n\n**Thanks to @ShivamBansal for  awesome kernel about Autoencoder.**\n\n### Contents\n- 1. Introduction\n    - 1.1 What are Autoencoders ?\n    - 1.2 How Autoencoders Work ?\n- 2. Implementation and UseCases\n    - 2.1 UseCase 1: Image Reconstruction\n    - 2.2 UseCase 2: Noise Removal\n    - 2.3 UseCase 3: Sequence to Sequence Prediction\n    \n    \n\ub354 \ub9ce\uc740 \uc88b\uc740 \uc790\ub8cc\ub97c \ub9cc\ub4e4\ub824\uace0 \ub178\ub825\ud558\uace0 \uc788\uc2b5\ub2c8\ub2e4.\n\n- **\ube14\ub85c\uadf8** : [\uc548\uc218\ube48\uc758 \ube14\ub85c\uadf8](https:\/\/subinium.github.io)\n- **\ud398\uc774\uc2a4\ubd81** : [\uc5b4\uc378\ub108\ub4dc \uc218\ube44\ub2c8\uc6c0](https:\/\/www.facebook.com\/ANsubinium)\n- **\uc720\ud29c\ube0c** : [\uc218\ube44\ub2c8\uc6c0\uc758 \ucf54\ub529\uc77c\uc9c0](https:\/\/www.youtube.com\/channel\/UC8cvg1_oB-IDtWT2bfBC2OQ)","ef063bf3":"\uc774\ub7f0 \uc720\ud615\uc758 \ub370\uc774\ud130\uc5d0\uc11c \ubb38\uc81c\uc810\uc740 \ud22c\uc601\uc744 \ud568\uacfc \ub3d9\uc2dc\uc5d0 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc190\uc2e4\uc744 \ubcf5\uad6c\ud560 \uc218 \uc5c6\ub2e4\ub294 \uac83\uc785\ub2c8\ub2e4.\n\uc544\ubb34\ub9ac \ub9ce\uc740 \uc774\ub3d9\uacfc \ud68c\uc804\uc73c\ub85c\ub3c4 \uc6d0\ubcf8 \ub370\uc774\ud130\ub294 \ubcf5\uad6c\ud560 \uc218 \uc5c6\uc2b5\ub2c8\ub2e4.\n\n\uadf8\ub807\ub2e4\uba74 \uc2e0\uacbd\ub9dd\uc740 \uc774 \ubb38\uc81c\ub97c \uc5b4\ub5bb\uac8c \ud574\uacb0\ud560\uae4c\uc694?\ndeep neural network\ub294 \uc120\ud615 \ub370\uc774\ud130\ub97c \ub9cc\ub4e4\uae30 \uc704\ud574 \uacf5\uac04\uc744 \uad6c\ubd80\ub9b4 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uadf8\ub807\uae30\uc5d0 \uc624\ud1a0\uc778\ucf54\ub354\ub294 \uc774\ub7f0 hidden layer\uc758 \uae30\ub2a5\uc744 \ud65c\uc6a9\ud558\uc5ec \uc800\ucc28\uc6d0 \ud45c\ud604\uc744 \ud559\uc2b5\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n![bend the space](https:\/\/i.imgur.com\/gKCOdiL.png)\n\n\uc774\uc81c \ucf00\ub77c\uc2a4\ub97c \uc774\uc6a9\ud558\uc5ec \uc774\ubbf8\uc9c0\uc5d0 \uc624\ud1a0\uc778\ucf54\ub354\ub97c \uc801\uc6a9\ud574\ubd05\uc2dc\ub2e4.","7f51778d":"### 1.2 How Autoencoder work\n\n\uc624\ud1a0\uc778\ucf54\ub354\uc758 \uc218\ud559\uc801 \ubc30\uacbd\uc5d0 \ub300\ud574 \uba3c\uc800 \uc54c\uc544\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n\uac00\uc7a5 \uae30\ubcf8\uc774 \ub418\ub294 \uc544\uc774\ub514\uc5b4\ub294 '\uace0\ucc28\uc6d0 \ub370\uc774\ud130\uc5d0\uc11c \uc800\ucc28\uc6d0 \ud45c\ud604\uc744 \ud559\uc2b5\ud558\ub294 \uac83' \uc785\ub2c8\ub2e4.\n\n\uc608\uc81c\ub97c \ud1b5\ud574 \uc54c\uc544\ubcf4\ub3c4\ub85d \ud558\uaca0\uc2b5\ub2c8\ub2e4.  \ub370\uc774\ud130\ub97c \ud45c\ud604\ud560 \uacf5\uac04\uacfc, \ub450 \uac1c\uc758 \ubcc0\uc218(x1, x2)\ub85c \ub098\ud0c0\ub0bc \uc218 \uc788\ub294 \ub370\uc774\ud130\ub97c \uc0dd\uac01\ud574\ubd05\uc2dc\ub2e4. \ub370\uc774\ud130 \ub9e4\ub2c8\ud3f4\ub4dc\ub294 \uc2e4\uc81c \ub370\uc774\ud130\uac00 \uc874\uc7ac\ud558\ub294 \ub370\uc774\ud130 \ud45c\ud604 \uacf5\uac04 \ub0b4\ubd80\uc758 \uacf5\uac04\uc785\ub2c8\ub2e4.","ea879955":"### 2.3 UseCase 3: Sequence to Sequence Prediction using AutoEncoders\n\n\uc774\ubc88 \ucf00\uc774\uc2a4\ub294 sequence to sequence \uc608\uce21\uc785\ub2c8\ub2e4. \uc55e\uc758 \uc608\uc2dc\uc5d0\uc11c\ub294 \uae30\ubcf8\uc801\uc73c\ub85c 2\ucc28\uc6d0 \ub370\uc774\ud130\uc600\uace0, \uc774\ubc88\uc5d0\ub294 sequence \ub370\uc774\ud130\ub294 1\ucc28\uc6d0 \ub370\uc774\ud130\uc785\ub2c8\ub2e4.\n\n\uc774\ub7f0 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130\uc758 \uc608\uc2dc\uc5d0\ub294 \uc2dc\uacc4\uc5f4 \ub370\uc774\ud130\uc640 \ubb38\uc790\uc5f4 \ub370\uc774\ud130\uac00 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \uc608\uc2dc\ub294 \uae30\uacc4 \ubc88\uc5ed \ub4f1\uc5d0 \uc801\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774\ubbf8\uc9c0\uc5d0 CNN\uc744 \uc0ac\uc6a9\ud588\ub2e4\uba74, \uc774 \ucf00\uc774\uc2a4\uc5d0\uc11c\ub294 LSTM\uc744 \uc0ac\uc6a9\ud569\ub2c8\ub2e4.\n\n\ub300\ubd80\ubd84\uc758 \ucf54\ub4dc\ub294 \uc544\ub798\uc758 \uae00\uc5d0\uc11c \ucc38\uc870\ud588\ub2e4\uace0 \ud569\ub2c8\ub2e4.\n\nMost of the code of this section is taken from the following reference shared by Jason Brownie in his blog post. Big Credits to him.\n\nReference : https:\/\/machinelearningmastery.com\/develop-encoder-decoder-model-sequence-sequence-prediction-keras\/\n\n#### Autoencoder Architecture\n\n\uc774 \ucf00\uc774\uc2a4\uc758 \uc624\ud1a0\uc778\ucf54\ub354\uc5d0\uc11c\ub3c4 \uc785\ub825\uc744 \ubcc0\ud658\ud558\ub294 \uc778\ucf54\ub354\uc640 \ud0c0\uac9f\uc73c\ub85c \ubcc0\ud658\ud558\ub294 \ub514\ucf54\ub354\uac00 \uc874\uc7ac\ud560 \uac83\uc785\ub2c8\ub2e4.\n\uc6b0\uc120 LSTM\uc774 \uc774 \uad6c\uc870\uc5d0\uc11c \uc5b4\ub5a4\uc2dd\uc73c\ub85c \uc791\ub3d9\ud558\ub294\uc9c0 \uc54c\uc544\ubd05\uc2dc\ub2e4.\n\n- Long Short-Term Memory, LSTM\uc740 \ub0b4\ubd80 \ub8e8\ud504\ub85c \uad6c\uc131\ub41c \ubc18\ubcf5\uc801 \uc2e0\uacbd\ub9dd\uc785\ub2c8\ub2e4.(RNN)\n- \ub2e4\ub978 RNN\uacfc \ub2e4\ub974\uac8c backpropagation throught time, BPTT\ub97c \ud65c\uc6a9\ud558\uc5ec \ud6a8\uacfc\uc801\uc73c\ub85c \ud6c8\ub828\ud558\uace0, \uc0ac\ub77c\uc9c0\ub294 \uadf8\ub798\ub514\uc5b8\ud2b8 \ubb38\uc81c\ub97c \ubc29\uc9c0\ud569\ub2c8\ub2e4.\n- LSTM layer\uc5d0\uc11c \uba54\ubaa8\ub9ac \uc720\ub2db\uc744 \uc815\uc758\ud560 \uc218 \uc788\uace0, layer\uc5d0 \uc18d\ud558\uc9c0 \uc54a\uc740 \uac01 \uc720\ub2db\uc740 \uc140\uc758 \uc0c1\ud0dc\ub97c \ub098\ud0c0\ub0b4\ub294 c\uc640 \uc228\uaca8\uc9c4 \uc0c1\ud0dc\uc774\uc790 \ucd9c\ub825\uc778 h \ub4f1\uc774 \uc788\uc2b5\ub2c8\ub2e4.\n- Keras\ub97c \uc0ac\uc6a9\ud558\uba74, LSTM \ub808\uc774\uc5b4\uc758 \ucd9c\ub825 \uc0c1\ud0dc\uc640 LSTM \ub808\uc774\uc5b4\uc758 \ud604\uc7ac \uc0c1\ud0dc\uc5d0 \ubaa8\ub450 \uc811\uadfc \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc774\uc81c \ud559\uc2b5\uacfc \uc0dd\uc131\uc744 \ud558\ub294 \uc624\ud1a0\uc778\ucf54\ub354 \uad6c\uc870\ub97c \ub9cc\ub4e4\uc5b4\ubd05\uc2dc\ub2e4. 2\uac00\uc9c0\uc758 \uc694\uc18c\ub85c \uc774\ub8e8\uc5b4\uc9d1\ub2c8\ub2e4.\n\n- \uc2dc\ud000\uc2a4\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544\ub4e4\uc774\uace0 LSTM\uc758 \ud604\uc7ac \uc0c1\ud0dc\ub97c \ucd9c\ub825\uc73c\ub85c \ubc18\ud658\ud558\ub294 \uc778\ucf54\ub354 \uc544\ud0a4\ud14d\ucc98\n- \uc2dc\ud000\uc2a4 \ubc0f \uc778\ucf54\ub354 LSTM \uc0c1\ud0dc\ub97c \uc785\ub825\uc73c\ub85c \ubc1b\uc544 \ub514\ucf54\ub529 \ub41c \ucd9c\ub825 \uc2dc\ud000\uc2a4\ub97c \ubc18\ud658\ud558\ub294 \ub514\ucf54\ub354 \uc544\ud0a4\ud14d\ucc98\n- LSTM\uc758 \uc228\uaca8\uc9c4 \uc0c1\ud0dc\uc640 \uba54\ubaa8\ub9ac \uc0c1\ud0dc\ub97c \uc800\uc7a5\ud558\uace0 (\uc228\uaca8\uc9c4 \uadf8\ub9ac\uace0 \uc0c1\ud0dc\ub4e4\uc744) \uc811\uadfc\ud558\ubbc0\ub85c,\ubcf4\uc774\uc9c0 \uc54a\ub294 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc608\uce21\uc744 \uc0dd\uc131\ud558\ub294 \ub3d9\uc548 LSTM\uc744 \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n\uc6b0\uc120, \uace0\uc815 \uae38\uc774\uc758 \ubb34\uc791\uc704 \uc2dc\ud000\uc2a4\ub97c \ud3ec\ud568\ud558\ub294 \uc2dc\ud000\uc2a4 \ub370\uc774\ud130 \uc138\ud2b8\ub97c \uc0dd\uc131\ud569\ub2c8\ub2e4. \uc6b0\ub9ac\ub294 \ubb34\uc791\uc704 \uc21c\uc11c\ub97c \uc0dd\uc131\ud558\ub294 \ud568\uc218\ub97c \uc0dd\uc131 \ud560 \uac83\uc785\ub2c8\ub2e4.\n\n- X1\uc740 \ub09c\uc218\ub97c \ud3ec\ud568\ud558\ub294 \uc785\ub825 \uc2dc\ud000\uc2a4 \uc758\ubbf8\ud569\ub2c8\ub2e4.\n- X2\ub294 \uc2dc\ud000\uc2a4\uc758 \ub2e4\ub978 \uc694\uc18c\ub97c \uc7ac\uc0dd\uc0b0\ud558\uae30 \uc704\ud574 \uc2dc\ub4dc\ub85c \uc0ac\uc6a9\ub418\ub294 \ud328\ub529 \ub41c \uc2dc\ud000\uc2a4\ub97c \uc758\ubbf8\ud569\ub2c8\ub2e4.\n- y\ub294 \ub300\uc0c1 \uc2dc\ud000\uc2a4 \ub610\ub294 \uc2e4\uc81c \uc2dc\ud000\uc2a4\ub97c \ub098\ud0c0\ub0c5\ub2c8\ub2e4.","8ab9dbc6":"## 2. Implementation\n\n### 2.1 UseCase 1 : Image Reconstruction\n\n1. \ud544\uc694\ud55c \ub77c\uc774\ube0c\ub7ec\ub9ac\ubd80\ud130 \ubd88\ub7ec\uc635\ub2c8\ub2e4.","8d8cc8f2":"- After adding noise","85e79f72":"## 1. Introduction\n\n### 1.1 \uc624\ud1a0\uc778\ucf54\ub354\ub780 \ubb34\uc5c7\uc778\uac00?\n\n\uc624\ud1a0\uc778\ucf54\ub354\ub294 \uc785\ub825\uc744 \uadf8\ub300\ub85c \ub611\uac19\uc774 \ucd9c\ub825\uc744 \ub9cc\ub4dc\ub294 \ud2b9\ubcc4\ud55c \uc720\ud615\uc758 \uc2e0\uacbd\ub9dd\uc785\ub2c8\ub2e4. \uc624\ud1a0\uc778\ucf54\ub354\ub294 \ube44\uc9c0\ub3c4 \ud559\uc2b5\uc73c\ub85c \uc785\ub825 \ub370\uc774\ud130\uc5d0 \ub300\ud55c low-level\uc758 \uc131\uc9c8\uc744 \ud559\uc2b5\ud558\uae30 \uc704\ud574 \uc0ac\uc6a9\ud569\ub2c8\ub2e4. \uc774\ub7f0 low-level \uc131\uc9c8\uc740 \uc2e4\uc81c \ub370\uc774\ud130\ub85c \uc7ac\uad6c\uc131\ud558\ub294\ub370 \ub3c4\uc6c0\uc744 \uc90d\ub2c8\ub2e4. \n\n\uc624\ud1a0\uc778\ucf54\ub354\ub294 \ub124\ud2b8\uc6cc\ud06c\uac00 \uc785\ub825\uc744 \uc608\uce21\ud558\ub3c4\ub85d \uc694\uad6c\ud558\ub294 \ud68c\uadc0\uc791\uc5c5\uc774\ub77c\uace0 \ubcfc \uc218 \uc788\uc2b5\ub2c8\ub2e4. \uc774 \ub124\ud2b8\uc6cc\ud06c\ub294 \uc911\uac04\uc5d0 \ubcd1\ubaa9 \ud604\uc0c1\uc774 \uc788\uace0, \uc774\ub7f0 \ubcd1\ubaa9\uc740 \ud6c4\uc5d0 \ub514\ucf54\ub354\ub85c \uc6d0\ubcf8 \ub370\uc774\ud130\ub85c \ub2e4\uc2dc \ubcc0\ud658\ud558\uae30 \uc704\ud574 \ud544\uc694\ud55c \uc785\ub825 \ub370\uc774\ud130\ub97c \ud6a8\uacfc\uc801\uc778 \ud45c\ud604\uc73c\ub85c \uc555\ucd95\ud574\uc8fc\ub294 \ud6a8\uacfc\ub97c \uac00\uc9d1\ub2c8\ub2e4.\n\n\uc774\ub7f0 \uc624\ud1a0 \uc778\ucf54\ub354\ub294 3\uac00\uc9c0 \uad6c\uc131 \uc694\uc18c\uac00 \uc788\uc2b5\ub2c8\ub2e4.\n\n- **Encoding Architecture** : \uc778\ucf54\ub354 \uad6c\uc870\ub294 \uc77c\ub828\uc758 \ub808\uc774\uc5b4\ub85c \uad6c\uc131\ub418\uace0, \uc774\ub97c \uc774\uc6a9\ud574 \ub178\ub4dc\ub97c \uac10\uc18c\uc2dc\ud0b5\ub2c8\ub2e4. \uadf8\ub9ac\uace0 \uc7a0\uc7ac \uacf5\uac04 \ud45c\ud604\uc758 \ud06c\uae30\ub97c \ub9e4\uc6b0 \uc904\uc5ec\uc90d\ub2c8\ub2e4.\n\n- **Latent View Representation** : \uc7a0\ubcf5 \uacf5\uac04\uc740 \uc904\uc5b4\ub4e0 \uc785\ub825 \uc815\ubcf4\uac00 \ubcf4\uc874\ub418\ub294 \uac00\uc7a5 \ub0ae\uc740 \ub808\ubca8 \uacf5\uac04\uc744 \ub098\ud0c0\ub0c5\ub2c8\ub2e4.\n\n- **Decoding Architecture** : \uc778\ucf54\ub529 \uad6c\uc870\uc640 \ub300\uce6d\ub418\ub294 \uac70\uc6b8\uc0c1\uc774\uc9c0\ub9cc, \ubaa8\ub4e0 \ub808\uc774\uc5b4\uc758 \ub178\ub4dc \uc218\uac00 \uc99d\uac00\ud558\uace0 \uad81\uadf9\uc801\uc73c\ub85c \uac70\uc758 \uc720\uc0ac\ud55c \uc785\ub825\uc744 \ucd9c\ub825\ud569\ub2c8\ub2e4.\n\n> Latent View\ub97c \uc800\ub294 \ub2e4\ub978 \ubb38\uc11c\uc5d0\uc11c \ub9ce\uc774 \uc0ac\uc6a9\ud558\ub294 latent space\ub77c \uc0dd\uac01\ud558\uc5ec \uc7a0\uc7ac\uacf5\uac04 \ub85c \ubc88\uc5ed\ud588\uc2b5\ub2c8\ub2e4.\n\n![autoencoder example](https:\/\/i.imgur.com\/Rrmaise.png)\n\n\ub9e4\uc6b0 \uc12c\uc138\ud558\uac8c \uc870\uc815\ub41c \uc624\ud1a0\uc778\ucf54\ub529 \ubaa8\ub378\uc740 \uccab \ubc88\uc9f8 \ub808\uc774\uc5b4\uc5d0\uc11c \uc804\ub2ec\ub41c \ub3d9\uc77c\ud55c \uc785\ub825\uc744 \uc7ac\uad6c\uc131 \ud560 \uc218 \uc788\uc5b4\uc57c\ud569\ub2c8\ub2e4. \uc774 \ucee4\ub110(\ud544\uc0ac)\uc5d0\uc11c\ub294 \uc624\ud1a0\uc778\ucf54\ub354\uc640 \uadf8 \uad6c\ud604 \ubc29\ubc95\uc744 \uc124\uba85\ud569\ub2c8\ub2e4.\n\n\uc624\ud1a0\uc778\ucf54\ub354\ub294 \uc774\ubbf8\uc9c0 \ub370\uc774\ud130\uc5d0 \ub9ce\uc774 \uc0ac\uc6a9\ub418\uace0,  \ub2e4\uc74c\uacfc \uac19\uc740 \uc0ac\ub840\uc5d0\uc11c \uc0ac\uc6a9\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\n- \ucc28\uc6d0 \ucd95\uc18c\n- \uc774\ubbf8\uc9c0 \uc555\ucd95\n- \uc774\ubbf8\uc9c0 \ub178\uc774\uc988 \uc81c\uac70\n- \uc774\ubbf8\uc9c0 \uc0dd\uc131\n- \ud2b9\uc131 \ucd94\ucd9c","ef0af92a":"\uc774\uc81c \uc624\ud1a0\uc778\ucf54\ub354 \ubaa8\ub378\uc744 \uc544\ub2f4 \uc635\ud2f0\ub9c8\uc774\uc800\uc640 Catgegorical Cross Entropy\ub97c \uc190\uc2e4\ud568\uc218\ub85c \uc0ac\uc6a9\ud558\uc5ec \ud6c8\ub828\ud574\ubd05\uc2dc\ub2e4.","02edebc2":"### 2.2 UseCase 2 - Image Denoising\n\n\uc624\ud1a0\uc778\ucf54\ub529\uc740 \ub9e4\uc6b0 \uc720\uc6a9\ud569\ub2c8\ub2e4. \uc774\uc81c \ub2e4\ub978 \uc608\uc2dc\ub97c \uc0b4\ud3b4\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. \n\n\ub9ce\uc740 \uacbd\uc6b0\uc5d0 \uc785\ub825 \uc774\ubbf8\uc9c0\ub294 \ub178\uc774\uc988\ub97c \uac00\uc9c0\uace0 \uc788\uc2b5\ub2c8\ub2e4. \uc624\ud1a0\uc778\ucf54\ub354\ub294 \uc774\ub97c \uc81c\uac70\ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4.\n\uc6b0\uc120 train_x\uc640 val_x \ub370\uc774\ud130\ub97c \uc774\ubbf8\uc9c0 \ud53d\uc140\uacfc \ud568\uaed8 \uc900\ube44\ud574\ubd05\uc2dc\ub2e4.\n\n![noiese MNIST](https:\/\/www.learnopencv.com\/wp-content\/uploads\/2017\/11\/denoising-autoencoder-600x299.jpg)","31e4e3e0":"\uc6d0\ubcf8 \uc774\ubbf8\uc9c0\uc640 \uc608\uce21 \uc774\ubbf8\uc9c0\ub97c \uadf8\ub824\ubd05\uc2dc\ub2e4.\n\n**Inputs : Actual Images**","c0b14d99":"\ubaa8\ub378 \uc815\ubcf4\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.","6da783d9":"**Predicted : Autoencoder Output**","dec05c36":"\ubaa8\ub378\uc758 \uc608\uce21 \uacb0\uacfc\ub97c \uac80\uc99d \ub370\uc774\ud130\uc5d0\uc11c \uc0b4\ud3b4\ubd05\uc2dc\ub2e4.","c92f2b4a":"\uc870\uae30 \ud559\uc2b5 \uc885\ub8cc\ub97c \uc774\uc6a9\ud558\uc5ec \ud6c8\ub828\uc744 \ud574\ubcf4\uaca0\uc2b5\ub2c8\ub2e4. (`early stopping callback`)","8e25dd4f":"\uc774\uc81c \ubaa8\ub378\uc744 \ud655\uc778\ud574\ubd05\uc2dc\ub2e4.","905b73be":"20 epochs \ub9cc\uc73c\ub85c \uc785\ub825 \uc774\ubbf8\uc9c0\ub97c \ub2e4\uc2dc \uc798 \uad6c\uc131\ud558\ub294 \uac83\uc744 \ud655\uc778\ud560 \uc218 \uc788\uc5c8\uc2b5\ub2c8\ub2e4.\n\uc774\uc81c \uc624\ud1a0\uc778\ucf54\ub354\ub85c \ub178\uc774\uc988\ub97c \uc5c6\uc560\ub294 \uc608\uc2dc\ub97c \uc0b4\ud3b4\ubd05\uc2dc\ub2e4. "}}