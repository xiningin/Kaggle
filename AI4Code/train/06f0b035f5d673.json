{"cell_type":{"bb7af95b":"code","bcd3f7be":"code","749f11bd":"code","d0946cee":"code","d77c0d51":"code","f8fb34ba":"code","8a52ab64":"code","fc28e36d":"code","225636c7":"code","e7c7df3f":"code","5b5ff20a":"code","6c520543":"code","65b47b01":"code","2f0951c2":"code","4f090ae8":"code","60f02134":"code","e0c07ff8":"code","5e77ed0e":"code","b87b00dd":"code","d4481881":"code","ed27ff39":"code","ada03629":"code","098a9836":"code","6f3633b3":"code","cadeface":"code","bbe17565":"code","62f1b019":"code","d2ce3c9c":"code","23da1ca6":"code","3f68219a":"markdown","4d13d287":"markdown","402772fd":"markdown","7b11821b":"markdown","ee580b43":"markdown","a69f587b":"markdown","a466b8f6":"markdown","561da3ad":"markdown","717b90a6":"markdown","e2872395":"markdown","0d32c252":"markdown","6cf81b88":"markdown","d8b619ee":"markdown","80a4bc0b":"markdown","0f6c3aa1":"markdown","d8ec4e6c":"markdown","9910cacf":"markdown","cc468845":"markdown","0344cb4b":"markdown","ac7ff973":"markdown","62eecf82":"markdown"},"source":{"bb7af95b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bcd3f7be":"#importing libaries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn import metrics\nfrom sklearn.cluster import KMeans \nfrom sklearn.cluster import MeanShift, estimate_bandwidth\nimport seaborn as sns\nimport os","749f11bd":"#Reading the data\n\ndf = pd.read_csv(\"\/kaggle\/input\/mall-customer\/Mall_Customers.csv\")\n#data Visualizing \ndf.head()\n","d0946cee":"# checking if there is any NULL data\n\ndf.isnull().any().any()","d77c0d51":"# GEtting the insides of the data\ndf.isnull().sum()\ndf.describe()","f8fb34ba":"df.Genre.value_counts()\nsns.countplot(x='Genre', data=df)\nplt.title('gender density')\nplt.show()\ntotalgenre = df.Genre.value_counts()\ngenrelabel = ['Male', 'Female']\nplt.axis('equal') # For perfect circle\nplt.pie(totalgenre, labels=genrelabel, radius=1.5, autopct='%0.2f%%', shadow=True, explode=[0, 0], startangle=45)\n# radius increase the size, autopct for show percentage two decimal point\nplt.title('Ratio of Male & Female')\nplt.show() ","8a52ab64":"df['Age'].describe()\nmy_bins=10\n# Histogram used by deafult 10 bins . bins like range.\narr=plt.hist(df['Age'],bins=my_bins, rwidth=0.95) \nplt.xlabel('Age Class')\nplt.ylabel('Frequency')\nplt.title('Age Class')","fc28e36d":"X = df.iloc[:, [3, 4]].values\n\n# let's check the shape of x\nprint(X.shape)","225636c7":"scores = []\nvalues = np.arange(2,12)\n\n#Iterate through the defined range\nfor num_clusters in values:\n  #Train the KMeans clustering model\n  kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n  kmeans.fit(X)\n  score = metrics.silhouette_score(X, kmeans.labels_,metric='euclidean', sample_size=len(X),random_state= 0)\n  print(\"\\n Number of clusters = \",num_clusters)\n  print(\"Silhouette score = \",score)\n  scores.append(score)\n\n#Extract best score and optimal number of clusters\nnum_clusters = np.argmax(scores) + values[0]\nprint('\\nOptimal number of clusters = ',num_clusters)\n\nplt.plot(range(1,11),scores)\nplt.title('The silhouette method', fontsize = 15)\nplt.xlabel('No of clusters')\nplt.ylabel('scores')\nplt.show()","e7c7df3f":"km = KMeans(num_clusters, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","5b5ff20a":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(num_clusters, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)\n\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_hc == 4, 0], X[y_hc == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Hierarchial Clustering', fontsize = 20)\nplt.xlabel('Annual Income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","6c520543":"from sklearn.cluster import MeanShift\nms = MeanShift(bandwidth=2)\nms.fit(X)\nms_y_pred = ms.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Mean shift clustering', fontsize = 20)\nplt.xlabel('Annual income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","65b47b01":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=5)\ngmm.fit(X)\ngmm_y_pred = gmm.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Gaussian mixture clustring', fontsize = 20)\nplt.xlabel('Annual income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","2f0951c2":"from sklearn.cluster import AffinityPropagation\nmodel_aff = AffinityPropagation(damping=0.9)\nmodel_aff.fit(X)\naff_y_pred = model_aff.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Affininty_propagation', fontsize = 20)\nplt.xlabel('Annual income')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","4f090ae8":"X = df.iloc[:, [2, 4]].values\nX.shape","60f02134":"scores = []\nvalues = np.arange(2,12)\n\n#Iterate through the defined range\nfor num_clusters in values:\n  #Train the KMeans clustering model\n  kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n  kmeans.fit(X)\n  score = metrics.silhouette_score(X, kmeans.labels_,metric='euclidean', sample_size=len(X),random_state= 0)\n  print(\"\\n Number of clusters = \",num_clusters)\n  print(\"Silhouette score = \",score)\n  scores.append(score)\n\n#Extract best score and optimal number of clusters\nnum_clusters = np.argmax(scores) + values[0]\nprint('\\nOptimal number of clusters = ',num_clusters)\n\nplt.plot(range(1,11),scores)\nplt.title('The silhouette method', fontsize = 15)\nplt.xlabel('No of clusters')\nplt.ylabel('scores')\nplt.show()","e0c07ff8":"km = KMeans(num_clusters, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('AGE')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","5e77ed0e":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(num_clusters, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)\n\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_hc == 3, 0], X[y_hc == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Hierarchial Clustering', fontsize = 20)\nplt.xlabel('AGE')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","b87b00dd":"from sklearn.cluster import MeanShift\nms = MeanShift(bandwidth=2)\nms.fit(X)\nms_y_pred = ms.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluMeanshift clusteringster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Mean shift clustering', fontsize = 20)\nplt.xlabel('Age')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","d4481881":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=5)\ngmm.fit(X)\ngmm_y_pred = gmm.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Gaussian mixture', fontsize = 20)\nplt.xlabel('AGE')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","ed27ff39":"from sklearn.cluster import AffinityPropagation\nmodel_aff = AffinityPropagation(damping=0.9)\nmodel_aff.fit(X)\naff_y_pred = model_aff.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Affininty_propagation', fontsize = 20)\nplt.xlabel('AGE')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","ada03629":"from sklearn.preprocessing import LabelEncoder\nnumber = LabelEncoder()\ndf['Genre'] = number.fit_transform(df['Genre'])","098a9836":"X = df.iloc[:, [1, 4]].values\nX.shape","6f3633b3":"scores = []\nvalues = np.arange(2,12)\n\n#Iterate through the defined range\nfor num_clusters in values:\n  #Train the KMeans clustering model\n  kmeans = KMeans(init='k-means++', n_clusters=num_clusters, n_init=10)\n  kmeans.fit(X)\n  score = metrics.silhouette_score(X, kmeans.labels_,metric='euclidean', sample_size=len(X),random_state= 0)\n  print(\"\\n Number of clusters = \",num_clusters)\n  print(\"Silhouette score = \",score)\n  scores.append(score)\n\n#Extract best score and optimal number of clusters\nnum_clusters = np.argmax(scores) + values[0]\nprint('\\nOptimal number of clusters = ',num_clusters)\n\nplt.plot(range(1,11),scores)\nplt.title('The silhouette method', fontsize = 15)\nplt.xlabel('No of clusters')\nplt.ylabel('scores')\nplt.show()","cadeface":"km = KMeans(num_clusters, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\ny_means = km.fit_predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('K Means Clustering', fontsize = 20)\nplt.xlabel('Gender')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","bbe17565":"from sklearn.cluster import AgglomerativeClustering\n\nhc = AgglomerativeClustering(num_clusters, affinity = 'euclidean', linkage = 'ward')\ny_hc = hc.fit_predict(X)\n\nplt.scatter(X[y_hc == 0, 0], X[y_hc == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_hc == 1, 0], X[y_hc == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_hc == 2, 0], X[y_hc == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 50, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Hierarchial Clustering', fontsize = 20)\nplt.xlabel('Gender')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","62f1b019":"from sklearn.cluster import MeanShift\nms = MeanShift(bandwidth=2)\nms.fit(X)\nms_y_pred = ms.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Mean shift clustering', fontsize = 20)\nplt.xlabel('GENDER')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","d2ce3c9c":"from sklearn.mixture import GaussianMixture\ngmm = GaussianMixture(n_components=5)\ngmm.fit(X)\ngmm_y_pred = gmm.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\n\nplt.style.use('fivethirtyeight')\nplt.title('Gaussian mixture', fontsize = 20)\nplt.xlabel('GENDER')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","23da1ca6":"from sklearn.cluster import AffinityPropagation\nmodel_aff = AffinityPropagation(damping=0.9)\nmodel_aff.fit(X)\naff_y_pred = model_aff.predict(X)\n\nplt.scatter(X[y_means == 0, 0], X[y_means == 0, 1], s = 100, c = 'red', label = 'cluster_1')\nplt.scatter(X[y_means == 1, 0], X[y_means == 1, 1], s = 100, c = 'orange', label = 'cluster_2')\nplt.scatter(X[y_means == 2, 0], X[y_means == 2, 1], s = 100, c = 'green', label = 'cluster_3')\nplt.scatter(X[y_means == 3, 0], X[y_means == 3, 1], s = 100, c = 'pink', label = 'cluster_4')\nplt.scatter(X[y_means == 4, 0], X[y_means == 4, 1], s = 100, c = 'blue', label = 'cluster_5')\nplt.scatter(km.cluster_centers_[:,0], km.cluster_centers_[:, 1], s = 150, c = 'black' , label = 'centeroid')\nplt.style.use('fivethirtyeight')\nplt.title('Affininty_propagation', fontsize = 20)\nplt.xlabel('GENDER')\nplt.ylabel('Spending Score')\nplt.legend()\nplt.grid()\nplt.show()","3f68219a":"# Kmeans Algorithm\n# The Average silhouette method to find the No. of Optimal Clusters","4d13d287":"# Hierarchial Clustering","402772fd":"# Kmeans Algorithm\n# The Average silhouette method to find the No. of Optimal Clusters","7b11821b":"# GaussianMixture clustering","ee580b43":"# KMEAN CLUSTERING","a69f587b":"# Kmean Clusters","a466b8f6":"# Clustering Analysis of various customers segments annual income","561da3ad":"# AffinityPropapagation","717b90a6":"# AffinityPropagation","e2872395":"# Kmeans Algorithm\n# The Average silhouette method to find the No. of Optimal Clusters","0d32c252":"# Meanshift clustering","6cf81b88":"# Meanshift clustering","d8b619ee":"# Hierarachial Clustering","80a4bc0b":"# Clusters of Customers Based on their Gender","0f6c3aa1":"# KMEAN CLUSTERING","d8ec4e6c":"# Meanshift clustering","9910cacf":"# Clusters of Customers Based on their Ages","cc468845":"# AffinityPropagation","0344cb4b":"# HIERARCHIAL CLUSTERING","ac7ff973":"# GaussianMixture","62eecf82":"# GaussianMixture"}}