{"cell_type":{"bfd726ec":"code","58bd1efd":"code","3f556b82":"code","7f260daf":"code","12c5f8b9":"code","e10869f9":"code","17cdfb80":"code","37fa38ed":"code","e413c498":"markdown","b8141f66":"markdown"},"source":{"bfd726ec":"import os\nimport numpy as np \nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import NearestNeighbors","58bd1efd":"# Read the data\nX_full = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id')\n\n#Drop a couple of useless columns\nX_full.drop('Soil_Type7', axis=1, inplace=True)\nX_test.drop('Soil_Type7',axis=1, inplace=True)\nX_full.drop('Soil_Type15', axis=1, inplace=True)\nX_test.drop('Soil_Type15',axis=1, inplace=True)\n\n# Separate target from predictors \ny_full = X_full.Cover_Type\nX_full.drop(['Cover_Type'], axis=1, inplace=True)","3f556b82":"weights = [\n3.3860658430898054,\n0.4163438499758126,\n7.35783588470092,\n1.4635508470705287,\n2.512455585483701,\n0.7879386244955993,\n2.3361452772106412,\n4.509437549105931,\n1.2565844481748276,\n0.8105744594321818,\n357.62840785739945,\n195.87206818235353]\nlen(weights)","7f260daf":"#Here is the code I used to find the weights.  You can ignore this.\n\nif False:\n    cols = list(X_full.columns.values)\n    \n    #These are my starting weights.  It reflects my view that Wilderness and Soil are very important,\n    #so I gave them 10000.  Elevation is also important but contains big numbers, so I start it with 4.\n    #Play around with other starting points.\n    weights = [4,1,1,1,1,1,1,1,1,1,10000,10000]\n    \n    best_score_ever=0\n    best_ever_wts = [i for i in weights]\n    lr = 0.5\n    \n    for step in range(1000):        \n        X_full_copy = X_full.copy()\n\n        for i in range(10):\n            c = X_full.columns[i]\n            X_full_copy[c] *= weights[i]\n        for i in range(10,14):\n            c = X_full.columns[i]\n            X_full_copy[c] *= weights[10]\n        for i in range(14,len(X_full.columns)):\n            c = X_full.columns[i]\n            X_full_copy[c] *= weights[11]\n\n        r = lr*random.random()\n        \n        \n        # Choose a random weight to change\n        train_index = random.randint(12)\n        train_col = X_full.columns[train_index]\n        \n        # We will test four factors for changing the current weight.\n        factors = [1-r,1,1+r, 1+2*r]        \n        wts = [weights[train_index] * f for f in factors]\n\n        best_score=0\n        best_wt=-1\n\n        for wt in wts:  \n            if train_index<10:            \n                X_full_copy[train_col] = wt * X_full[train_col]\n            if train_index==10: \n                for i in range(10,14):\n                    c = X_full.columns[i]\n                    X_full_copy[c] = wt*X_full[c]\n            if train_index > 10: \n                for i in range(14,len(X_full.columns)):\n                    c = X_full.columns[i]\n                    X_full_copy[c] = wt*X_full[c]\n\n            model = NearestNeighbors(n_neighbors=2, algorithm='ball_tree')\n            distances, indices  = model.fit(X_full_copy).kneighbors(X_full_copy)\n            \n            # we use the second best because the first best is itself.\n            second_best = indices[:,1]\n            labels = y_full.tolist()\n            my_labels = [labels[i] for i in second_best]\n            score = accuracy_score(labels, my_labels)        \n\n            if score > best_score_ever :\n                best_score_ever = score\n                best_ever_wts = [i for i in weights]\n                best_ever_wts[train_index] = wt\n                print(\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tnew best ever:\",best_score_ever)\n            if score > best_score:\n                best_wt=wt\n                best_score=score\n\n        old_wt =  weights[train_index] \n        \n        # Notice that I only go half-way to the new weights.  Just seemed like a good idea, but not sure.\n        weights[train_index] =  (weights[train_index]+best_wt)\/2\n        \n        print(\"step\",step,\"col\",train_index,\"best\",round(old_wt,2),\"->\",round(best_wt,2),\"\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\",best_score) \n        # lr*=.999\n    print(\"best weights\",best_ever_wts)\n    print(\"final weights\",weights)","12c5f8b9":"# Define and fit model, using the sklearn library.\nmodel = KNeighborsClassifier(n_neighbors=1, p=1)\n\nX_full_copy = X_full.copy()\nX_test_copy = X_test.copy()\n\nfor i in range(10):\n    c = X_full.columns[i]\n    X_full_copy[c] = weights[i]*X_full_copy[c]\n    X_test_copy[c] = weights[i]*X_test_copy[c]\nfor i in range(10,14):\n    c = X_full.columns[i]\n    X_full_copy[c] = weights[10]*X_full_copy[c]\n    X_test_copy[c] = weights[10]*X_test_copy[c]\nfor i in range(14,len(X_full.columns)):\n    c = X_full.columns[i]\n    X_full_copy[c] = weights[11]*X_full_copy[c]\n    X_test_copy[c] = weights[11]*X_test_copy[c]\n\n#model.fit(X_train, y_train)\nmodel.fit(X_full_copy, y_full)\npreds_full = model.predict(X_full_copy)\n\n#this should give 1.0, since each row is its own nearest neigbor\nprint(accuracy_score(y_full, preds_full))","e10869f9":"preds_test = model.predict(X_test_copy)","17cdfb80":"# For some reason this gave some tensorflow deprecation errors???  I did not use tensorflow.\n\noutput = pd.DataFrame({'Id': X_test_copy.index,'Cover_type': preds_test})\noutput.to_csv('submission.csv', index=False)","37fa38ed":"# Better test to see if the output worked.\noutput2 = pd.read_csv('submission.csv')\noutput2.head()","e413c498":"The first, most obvious thing to do is play around with what we mean by \"nearest\".\nI will use Euclidean distance with weights assigned to each column.\nBelow I will give a list of 12 weights.  The first\nten are for the first ten features.  The eleventh one is for the four \"Wilderness_Area\"\nfeatures, and the last one is for all of the \"Soil_Type\" features.\n\nThese weights were found by trial and error (similar to grid search).  The code I used for finding\nthem will be given below (commented out), but it is not that important.  I don't believe these\nare the best weights we can find.  You should experiment with finding other sets of weights.\n\nOne interesting thing I noticed is that variety of weight assignments that give similar results.\nThis suggests some ensembling possibilities.","b8141f66":"I decided to spend most of this contest on one particular algorithm.  \nI chose \"nearest neighbor\" for its simplicity, power, and possibilities\nfor innovation.  \nYou are welcome to join me in this endeavor,  keeping in touch\nthrough public kernels and discussion.  \nYou are also welcome to use these\nresults with your ensembles or feature engineering.\n\nThis should give a score of about .798"}}