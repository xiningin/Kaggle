{"cell_type":{"abbe0e4e":"code","afedad86":"code","e2e799d1":"code","18620497":"code","86db7fd3":"code","580e70ee":"code","0142ff39":"code","22091396":"code","ccac183b":"code","1fc2ded0":"code","3329a94c":"markdown"},"source":{"abbe0e4e":"# Imports\n\n# General\nimport numpy as np \nimport pandas as pd \nimport string\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom pylab import imread, subplot, imshow, show\nfrom pickle import dump\nfrom tqdm import tqdm\nimport os\n\n# Tokenizer\nfrom keras.preprocessing.text import Tokenizer\n\n# Converting image to vector of 4,096 elements\nfrom keras.applications.vgg16 import VGG16\nfrom keras.preprocessing.image import load_img\nfrom keras.preprocessing.image import img_to_array\nfrom keras.applications.vgg16 import preprocess_input\nfrom keras.models import Model\n\n# Attention model\nimport tensorflow as tf\nfrom keras import backend as K\nfrom keras import regularizers, constraints, initializers, activations\nfrom keras.layers.recurrent import Recurrent\nfrom keras.engine import InputSpec\n\n# Creating our model\nfrom pickle import load\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils import to_categorical\nfrom keras.utils import plot_model\nfrom keras.models import load_model\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Input\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding\nfrom keras.layers import Dropout\nfrom keras.layers.merge import add\nfrom keras.callbacks import ModelCheckpoint\nfrom IPython.display import display, Image                                                                                \n\nfrom nltk.translate.bleu_score import corpus_bleu","afedad86":"print(os.listdir(\"..\/input\"))","e2e799d1":"# Preparing the text dataset for storing the captions with indices as clean descriptions\n# and also grabbing the vocabulary from the captions and storing all the words in the vocabulary\n\ndef load_descriptions(filename):\n    file = open(filename, 'r')\n    doc = file.read()\n    file.close()\n\n    mapping = dict()\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        if len(line) < 2:\n            continue\n\n        image_id, image_desc = tokens[0], tokens[1:]\n        image_id = image_id.split('.')[0]\n        image_desc = ' '.join(image_desc)\n\n        if image_id not in mapping:\n            mapping[image_id] = list()\n\n        mapping[image_id].append(image_desc)\n\n    return mapping\n\ndef clean_descriptions(descriptions):\n    table = str.maketrans('', '', string.punctuation)\n    for key, desc_list in descriptions.items():\n        for i in range(len(desc_list)):\n            desc = desc_list[i]\n            desc = desc.split()\n            desc = [word.lower() for word in desc]\n            desc = [w.translate(table) for w in desc]\n            desc = [word for word in desc if len(word)>1]\n            desc = [word for word in desc if word.isalpha()]\n            desc_list[i] = ' '.join(desc)\n\ndef to_vocabulary(descriptions):\n    vocab = set()\n    for key in descriptions.keys():\n        [vocab.update(d.split()) for d in descriptions[key]]\n    return vocab\n\ndef save_vocabulary(vocab, filename):\n    file = open(filename, 'w')\n    for word in vocab:\n        file.write(word+\"\\n\")\n    file.close()\n\ndef save_descriptions(descriptions, filename):\n    lines = list()\n    for key, desc_list in descriptions.items():\n        for desc in desc_list:\n            lines.append(key + ' ' + desc)\n\n    data = '\\n'.join(lines)\n    file = open(filename, 'w')\n    file.write(data)\n    file.close()\n\nfilename = \"..\/input\/flicker8k-dataset\/flickr8k_text\/Flickr8k.token.txt\"\ndescriptions = load_descriptions(filename)\nprint(\"Loaded: \", len(descriptions))\n\nclean_descriptions(descriptions)\nvocabulary = to_vocabulary(descriptions)\nprint(\"Vocabulary size: \", len(vocabulary))\n\nsave_vocabulary(vocabulary, \"vocabulary.txt\")\nsave_descriptions(descriptions, \"descriptions.txt\")","18620497":"# tokenizer\n\ndef load_doc(filename):\n    file = open(filename, 'r')\n    text = file.read()\n    file.close()\n    return text\n\ndef load_set(filename):\n    doc = load_doc(filename)\n    dataset = list()\n    for line in doc.split('\\n'):\n        if len(line) < 1:\n            continue\n        identifier = line.split('.')[0]\n        dataset.append(identifier)\n    return set(dataset)\n\ndef load_clean_descriptions(filename, dataset):\n    doc = load_doc(filename)\n    descriptions = dict()\n    for line in doc.split('\\n'):\n        tokens = line.split()\n        image_id, image_desc = tokens[0], tokens[1:]\n        if image_id in dataset:\n            if image_id not in descriptions:\n                descriptions[image_id] = list()\n            desc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n            descriptions[image_id].append(desc)\n    return descriptions\n\ndef to_lines(descriptions):\n    all_desc = list()\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\ndef create_tokenizer(descriptions):\n    lines = to_lines(descriptions)\n    tokenizer = Tokenizer()\n    tokenizer.fit_on_texts(lines)\n    return tokenizer\n\nfilename = \"..\/input\/flicker8k-dataset\/flickr8k_text\/Flickr_8k.trainImages.txt\"\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\n\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\n\ntokenizer = create_tokenizer(train_descriptions)\n\ndump(tokenizer, open('tokenizer.pkl', 'wb'))","86db7fd3":"# Using the VGG16 network to extract features from each image. \n# The resulting feature file will have features of each image as a 1D, 4096 element array\n# Defining a function to extract features from images stored in a given directory\n\ndef extract(dirname):\n\n    model = VGG16()\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    print(model.summary())\n\n    features = dict()\n\n    for name in tqdm(os.listdir(dirname)):\n        filename = dirname + '\/' + name\n        image = load_img(filename, target_size=(224, 224))\n        image = img_to_array(image)\n        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n        image = preprocess_input(image)\n        feature = model.predict(image, verbose=0)\n        image_id = name.split('.')[0]\n        features[image_id] = feature\n\n    return features\n\ndirectory_name = \"..\/input\/flicker8k-dataset\/flickr8k_dataset\/Flicker8k_Dataset\"\nfeatures = extract(directory_name)\n\nprint(\"Extracted Features: \", len(features))\n\n# dumping features into a pickled file\ndump(features, open('features.pkl', 'wb'))","580e70ee":"def time_distributed_dense(x, w, b=None, dropout=None, input_dim=None, output_dim=None, timesteps=None):\n        '''Apply y.w + b for every temporal slice y of x.\n        '''\n        if not input_dim:\n            input_dim = K.shape(x)[2]\n        if not timesteps:\n            timesteps = K.shape(x)[1]\n        if not output_dim:\n            output_dim = K.shape(w)[1]\n\n        if dropout:\n            ones = K.ones_like(K.reshape(x[:, 0, :], (-1, input_dim)))\n            dropout_matrix = K.dropout(ones, dropout)\n            expanded_dropout_matrix = K.repeat(dropout_matrix, timesteps)\n            x *= expanded_dropout_matrix\n\n        x = K.reshape(x, (-1, input_dim))\n\n        x = K.dot(x, w)\n        if b:\n            x = x + b\n        x = K.reshape(x, (-1, timesteps, output_dim))\n        return x","0142ff39":"# Attention layer\n\ntfPrint = lambda d, T: tf.Print(input_=T, data=[T, tf.shape(T)], message=d)\n\nclass AttentionDecoder(Recurrent):\n\n    def __init__(self, units, output_dim, activation='tanh', return_probabilities=False, name='AttentionDecoder', kernel_initializer='glorot_uniform', \n                 recurrent_initializer='orthogonal', bias_initializer='zeros', kernel_regularizer=None, bias_regularizer=None, activity_regularizer=None, \n                 kernel_constraint=None, bias_constraint=None, **kwargs):\n        \"\"\"\n        Implements an AttentionDecoder that takes in a sequence encoded by an\n        encoder and outputs the decoded states\n        :param units: dimension of the hidden state and the attention matrices\n        :param output_dim: the number of labels in the output space\n\n        \"\"\"\n        self.units = units\n        self.output_dim = output_dim\n        self.return_probabilities = return_probabilities\n        self.activation = activations.get(activation)\n        self.kernel_initializer = initializers.get(kernel_initializer)\n        self.recurrent_initializer = initializers.get(recurrent_initializer)\n        self.bias_initializer = initializers.get(bias_initializer)\n\n        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n        self.recurrent_regularizer = regularizers.get(kernel_regularizer)\n        self.bias_regularizer = regularizers.get(bias_regularizer)\n        self.activity_regularizer = regularizers.get(activity_regularizer)\n\n        self.kernel_constraint = constraints.get(kernel_constraint)\n        self.recurrent_constraint = constraints.get(kernel_constraint)\n        self.bias_constraint = constraints.get(bias_constraint)\n\n        super(AttentionDecoder, self).__init__(**kwargs)\n        self.name = name\n        self.return_sequences = True  # must return sequences\n\n    def build(self, input_shape):\n\n        self.batch_size, self.timesteps, self.input_dim = input_shape\n\n        if self.stateful:\n            super(AttentionDecoder, self).reset_states()\n\n        self.states = [None, None]  # y, s\n\n        \"\"\"\n            Matrices for creating the context vector\n        \"\"\"\n\n        self.V_a = self.add_weight(shape=(self.units,), name='V_a', initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n        self.U_a = self.add_weight(shape=(self.input_dim, self.units), name='U_a', initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n        self.W_a = self.add_weight(shape=(self.units, self.units), name='W_a', initializer=self.kernel_initializer,\n                                   regularizer=self.kernel_regularizer, constraint=self.kernel_constraint)\n        self.b_a = self.add_weight(shape=(self.units,), name='b_a', initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for the r (reset) gate\n        \"\"\"\n        self.C_r = self.add_weight(shape=(self.input_dim, self.units), name='C_r', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.U_r = self.add_weight(shape=(self.units, self.units), name='U_r', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.W_r = self.add_weight(shape=(self.output_dim, self.units), name='W_r', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.b_r = self.add_weight(shape=(self.units, ), name='b_r', initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n\n        \"\"\"\n            Matrices for the z (update) gate\n        \"\"\"\n        self.C_z = self.add_weight(shape=(self.input_dim, self.units), name='C_z', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.U_z = self.add_weight(shape=(self.units, self.units), name='U_z', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.W_z = self.add_weight(shape=(self.output_dim, self.units), name='W_z', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.b_z = self.add_weight(shape=(self.units, ), name='b_z', initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for the proposal\n        \"\"\"\n        self.C_p = self.add_weight(shape=(self.input_dim, self.units), name='C_p', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.U_p = self.add_weight(shape=(self.units, self.units), name='U_p', initializer=self.recurrent_initializer, \n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.W_p = self.add_weight(shape=(self.output_dim, self.units), name='W_p', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.b_p = self.add_weight(shape=(self.units, ), name='b_p', initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n        \"\"\"\n            Matrices for making the final prediction vector\n        \"\"\"\n        self.C_o = self.add_weight(shape=(self.input_dim, self.output_dim), name='C_o', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.U_o = self.add_weight(shape=(self.units, self.output_dim), name='U_o', initializer=self.recurrent_initializer,\n                                    regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.W_o = self.add_weight(shape=(self.output_dim, self.output_dim), name='W_o',initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n        self.b_o = self.add_weight(shape=(self.output_dim, ), name='b_o', initializer=self.bias_initializer,\n                                   regularizer=self.bias_regularizer, constraint=self.bias_constraint)\n\n        # For creating the initial state:\n        self.W_s = self.add_weight(shape=(self.input_dim, self.units), name='W_s', initializer=self.recurrent_initializer,\n                                   regularizer=self.recurrent_regularizer, constraint=self.recurrent_constraint)\n\n        self.input_spec = [\n            InputSpec(shape=(self.batch_size, self.timesteps, self.input_dim))]\n        self.built = True\n\n    def call(self, x):\n        # store the whole sequence so we can \"attend\" to it at each timestep\n        self.x_seq = x\n\n        # apply the a dense layer over the time dimension of the sequence\n        # do it here because it doesn't depend on any previous steps\n        # to save computation time:\n        self._uxpb = time_distributed_dense(self.x_seq, self.U_a, b=self.b_a, input_dim=self.input_dim, timesteps=self.timesteps, output_dim=self.units)\n\n        return super(AttentionDecoder, self).call(x)\n\n    def get_initial_state(self, inputs):\n        # apply the matrix on the first time step to get the initial s0.\n        s0 = activations.tanh(K.dot(inputs[:, 0], self.W_s))\n\n        # from keras.layers.recurrent to initialize a vector of (batchsize,\n        # output_dim)\n        y0 = K.zeros_like(inputs)  # (samples, timesteps, input_dims)\n        y0 = K.sum(y0, axis=(1, 2))  # (samples, )\n        y0 = K.expand_dims(y0)  # (samples, 1)\n        y0 = K.tile(y0, [1, self.output_dim])\n\n        return [y0, s0]\n\n    def step(self, x, states):\n\n        ytm, stm = states\n\n        # repeat the hidden state to the length of the sequence\n        _stm = K.repeat(stm, self.timesteps)\n\n        # now multiplty the weight matrix with the repeated hidden state\n        _Wxstm = K.dot(_stm, self.W_a)\n\n        # calculate the attention probabilities\n        # this relates how much other timesteps contributed to this one.\n        et = K.dot(activations.tanh(_Wxstm + self._uxpb), K.expand_dims(self.V_a))\n        at = K.exp(et)\n        at_sum = K.sum(at, axis=1)\n        at_sum_repeated = K.repeat(at_sum, self.timesteps)\n        at \/= at_sum_repeated  # vector of size (batchsize, timesteps, 1)\n\n        # calculate the context vector\n        context = K.squeeze(K.batch_dot(at, self.x_seq, axes=1), axis=1)\n        # ~~~> calculate new hidden state\n        # first calculate the \"r\" gate:\n\n        rt = activations.sigmoid(K.dot(ytm, self.W_r) + K.dot(stm, self.U_r)+ K.dot(context, self.C_r) + self.b_r)\n\n        # now calculate the \"z\" gate\n        zt = activations.sigmoid(K.dot(ytm, self.W_z) + K.dot(stm, self.U_z) + K.dot(context, self.C_z) + self.b_z)\n\n        # calculate the proposal hidden state:\n        s_tp = activations.tanh(K.dot(ytm, self.W_p)+ K.dot((rt * stm), self.U_p) + K.dot(context, self.C_p) + self.b_p)\n\n        # new hidden state:\n        st = (1-zt)*stm + zt * s_tp\n\n        yt = activations.softmax(K.dot(ytm, self.W_o) + K.dot(stm, self.U_o) + K.dot(context, self.C_o) + self.b_o)\n\n        if self.return_probabilities:\n            return at, [yt, st]\n        else:\n            return yt, [yt, st]\n\n    def compute_output_shape(self, input_shape):\n        \"\"\"\n            For Keras internal compatability checking\n        \"\"\"\n        if self.return_probabilities:\n            return (None, self.timesteps, self.timesteps)\n        else:\n            return (None, self.timesteps, self.output_dim)\n\n    def get_config(self):\n        \"\"\"\n            For rebuilding models on load time.\n        \"\"\"\n        config = {\n            'output_dim': self.output_dim,\n            'units': self.units,\n            'return_probabilities': self.return_probabilities\n        }\n        base_config = super(AttentionDecoder, self).get_config()\n        return dict(list(base_config.items()) + list(config.items()))","22091396":"#Train model\n\n\ndef load_photo_features(filename, dataset):\n    all_features = load(open(filename, 'rb'))\n    features = {k: all_features[k] for k in dataset}\n    return features\n\n\ndef max_length(descriptions):\n    lines = to_lines(descriptions)\n    return max(len(d.split()) for d in lines)\n\ndef create_sequences(tokenizer, max_length, desc_list, photo):\n    X1, X2, y = list(), list(), list()\n    for desc in desc_list:\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        for i in range(1, len(seq)):\n            in_seq, out_seq = seq[:i], seq[i]\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n            X1.append(photo)\n            X2.append(in_seq)\n            y.append(out_seq)\n    return np.array(X1), np.array(X2), np.array(y)\n\ndef define_model(vocab_size, max_length):\n    # feature extractor model\n    inputs1 = Input(shape=(4096,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n    # sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n    #se4 = AttentionDecoder(se3)\n    # decoder model\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.summary()\n\n    # compile model\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n    # summarize model\n    model.summary()\n    #plot_model(model, to_file='model.png', show_shapes=True)\n    return model\n    \"\"\"\n     inputs = Input(shape=(max_length,))\n    embedding = Embedding(vocab_size, 512, mask_zero=True)(inputs)\n\n    embed_dropout = Dropout(0.5)(embedding)\n\n    model.add(LSTM(512, input_shape=(max_length, vocab_size), return_sequences=True))\n    #model.summary()\n\n    att = AttentionDecoder(512, max_length)(embedding)\n\n    lstm = LSTM(512)(att)\n\n    fc = Dense(512, activation='relu')(lstm)\n\n    output = Dense(vocab_size, activation='softmax')(fc)\n\n    model = Model(inputs = inputs, outputs = output)\n\n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n\n    model.summary()\n    plot_model(model, to_file='model_custom.png', show_shapes=True)\n    return model\n    \"\"\"\n\ndef data_generator(descriptions, photos, tokenizer, max_length):\n    while 1:\n        for key, desc_list in descriptions.items():\n            photo = photos[key][0]\n            in_img, in_seq, out_word = create_sequences(tokenizer, max_length, desc_list, photo)\n            yield [[in_img, in_seq], out_word]\n\nfilename = '..\/input\/flicker8k-dataset\/flickr8k_text\/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: ', len(train))\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train = ', len(train_descriptions))\ntrain_features = load_photo_features('features.pkl', train)\nprint('Photos: train = ', len(train_features))\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: ', vocab_size)\nmax_length = max_length(train_descriptions)\nprint('Description Length: ', max_length)\n\nmodel = define_model(vocab_size, max_length)\nepochs = 20\nsteps = len(train_descriptions)\nfor i in range(epochs):\n    generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n    model.fit_generator(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n    model.save('model_' + str(i) + '.h5')","ccac183b":"#Evaluate the model\n\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = word_for_id(yhat, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text\n\ndef evaluate_model(model, descriptions, photos, tokenizer, max_length):\n    actual, predicted = list(), list()\n    for key, desc_list in descriptions.items():\n        yhat = generate_desc(model, tokenizer, photos[key], max_length)\n        references = [d.split() for d in desc_list]\n        actual.append(references)\n        predicted.append(yhat.split())\n    print('BLEU-1: %f' % corpus_bleu(actual, predicted, weights=(1.0, 0, 0, 0)))\n    print('BLEU-2: %f' % corpus_bleu(actual, predicted, weights=(0.5, 0.5, 0, 0)))\n    print('BLEU-3: %f' % corpus_bleu(actual, predicted, weights=(0.3, 0.3, 0.3, 0)))\n    print('BLEU-4: %f' % corpus_bleu(actual, predicted, weights=(0.25, 0.25, 0.25, 0.25)))\n\"\"\"\n\n# prepare tokenizer on train set\n\nfilename = '..\/input\/flickr8k_text\/Flickr_8k.trainImages.txt'\ntrain = load_set(filename)\nprint('Dataset: %d' % len(train))\ntrain_descriptions = load_clean_descriptions('descriptions.txt', train)\nprint('Descriptions: train=%d' % len(train_descriptions))\ntokenizer = create_tokenizer(train_descriptions)\nvocab_size = len(tokenizer.word_index) + 1\nprint('Vocabulary Size: %d' % vocab_size)\nmax_length = max_length(train_descriptions)\nprint('Description Length: %d' % max_length)\n\"\"\"\n\nfilename = '..\/input\/flicker8k-dataset\/flickr8k_text\/Flickr_8k.testImages.txt'\ntest = load_set(filename)\nprint('Dataset: %d' % len(test))\ntest_descriptions = load_clean_descriptions('descriptions.txt', test)\nprint('Descriptions: test=%d' % len(test_descriptions))\ntest_features = load_photo_features('features.pkl', test)\nprint('Photos: test=%d' % len(test_features))\n\n# load the model\nfilename = 'model_19.h5' \nmodel = load_model(filename)\n# evaluate model\nevaluate_model(model, test_descriptions, test_features, tokenizer, max_length)","1fc2ded0":"#Prediction\n\ndef extract_features(filename):\n    model = VGG16()\n    model.layers.pop()\n    model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n    image = load_img(filename, target_size=(224, 224))\n    image = img_to_array(image)\n    image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n    image = preprocess_input(image)\n    feature = model.predict(image, verbose=0)\n    return feature\n\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'startseq'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        yhat = model.predict([photo,sequence], verbose=0)\n        yhat = np.argmax(yhat)\n        word = word_for_id(yhat, tokenizer)\n        if word is None:\n            break\n        in_text += ' ' + word\n        if word == 'endseq':\n            break\n    return in_text\n\ntokenizer = load(open('tokenizer.pkl', 'rb'))\nmax_length = 34\nmodel_file = 'model_19.h5'\nmodel = load_model(model_file)\n\n\nfolder = os.listdir('..\/input\/my-test-image\/')\nfor img in folder:\n    img_path = '..\/input\/my-test-image\/'+img\n    img_features = extract_features(img_path)\n    display(Image(filename=img_path))\n    \n    description = generate_desc(model, tokenizer, img_features, max_length)\n    print(description)\n\n\"\"\"\nimg_file = '..\/input\/my-test-image\/16.jpeg'\nphoto = extract_features(img_file)\n\ndisplay(Image(filename='..\/input\/my-test-image\/16.jpeg'))\n\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(description)\n\n\"\"\"","3329a94c":"The model is trained for 20 epochs with a loss of 3.1462. "}}