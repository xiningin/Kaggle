{"cell_type":{"a15a703c":"code","cfde8fd6":"code","068b92c6":"code","9f77f9fc":"code","6c85b803":"code","64c7a36e":"code","ade80198":"code","3f7e8753":"code","8bf83ba0":"code","d7379b22":"code","52355312":"code","96e98e28":"code","d393af72":"code","d7aef7cd":"code","31815ec5":"code","3c3f8c07":"code","d855b778":"code","555fea58":"code","669db238":"code","563ab021":"code","b32cb63f":"code","899a62e5":"code","2a829f6a":"code","0a58ffc3":"code","778b3856":"code","13b165fc":"code","dc11effc":"code","e7bcfca3":"code","6ee1a6d4":"code","4cb6718a":"code","93dc6a58":"code","bc5e6407":"code","0870c033":"code","1502744f":"code","246d4196":"code","ec1b329e":"code","a5b40d38":"code","3a722676":"code","bad6dd1c":"code","31447096":"code","616a3ac2":"code","d15c1419":"code","9d6c2cae":"code","55f27b56":"code","b45e8911":"code","c5a7abdc":"code","a3880820":"code","e7e16db8":"code","3a57d953":"code","808036e0":"code","3fddb753":"code","e7cd708e":"code","57cbc6f0":"code","0b0ddb8b":"code","7549dcc2":"code","ad33a775":"code","166895e7":"code","55447941":"code","6c3acbe3":"code","4ff69247":"code","2fade3b3":"code","6b5d1b5e":"code","174119bd":"code","4f8dfeba":"code","1bc08eaf":"code","675fd709":"code","7a3b21ee":"code","a4f5eb0e":"code","d8dea05a":"code","246e41e6":"code","148abb07":"code","f1b12cdc":"code","ff593562":"code","2279ce3d":"code","ccd3f913":"code","e275d087":"code","b8a8f7c8":"code","fecf915e":"code","4d77248e":"code","c05e4fda":"code","94b68333":"code","58225c5f":"code","eb2ac014":"code","67851375":"code","c17798f7":"code","bdc74d42":"code","86fb8565":"code","79e804cb":"code","6c8e44a6":"code","ca718c06":"code","1054cb13":"code","f4d4dcf4":"code","294f42bf":"code","0d349333":"code","0eb453c7":"code","43fe7034":"code","4622ecd7":"code","eeb5ace9":"code","3f03dbeb":"code","d801c6f7":"code","126ff92e":"markdown","091d4a27":"markdown","738720d7":"markdown","42735596":"markdown","1af67a0e":"markdown","70d2b463":"markdown","65de6d59":"markdown","1d6ebcc2":"markdown","23b21201":"markdown","7efe935f":"markdown","07154e5c":"markdown","71036b7c":"markdown","96bb6c1e":"markdown"},"source":{"a15a703c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n\nimport matplotlib.dates as dates\nfrom datetime import datetime\nimport matplotlib.pyplot as plt\nimport math\nimport tensorflow as tf\n     \n%matplotlib inline\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\nimport scipy\n\n\nimport random\nimport sklearn\nfrom nltk.corpus import stopwords\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom scipy.sparse.linalg import svds\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","cfde8fd6":"row_data = pd.read_csv('\/kaggle\/input\/ecommerce-events-history-in-cosmetics-shop\/2019-Nov.csv') #import the dataset\n\n\nif (row_data.isnull().values.any() == True):  # checking missing values\n    print(row_data.isnull().sum())\nelse: \n    print(\"There is not any null number\")","068b92c6":"df = row_data\nvisitors  = df['user_id'].nunique()\n# print(\"Number of visitors : {}\".format(visitors))\ndf[df['user_session'] == -1]","9f77f9fc":"date = df.loc[:,['event_time','user_id']]\ndate['event_time'] = date['event_time'].apply(lambda d: str(d)[0:10]) # changing event time value to :(2019-10-01  - 2019-10-31) format\ndf","6c85b803":"# Daily visitors number\n\nvisitor_by_date = date.groupby(['event_time'])['user_id'].nunique()\nvisitor_by_date","64c7a36e":"x = pd.Series(visitor_by_date.index.values).apply(lambda s: datetime.strptime(s, '%Y-%m-%d').date())\ny = visitor_by_date[:]\n\nprint(type(visitor_by_date))\nplt.rcParams['figure.figsize'] = (25,12)\nplt.plot(x,y)\nplt.show()","ade80198":"purchase  = df.loc[df['event_type'] == 'purchase'] #getting only purchase event type \npurchase = purchase.dropna(axis='rows') # dropping rows that have  at least one missing value\n\ntop_brands = purchase.groupby(['brand'])['brand'].agg(['count']).sort_values(by=['count'],ascending=False)\ntop_brands.head(25) # [samsung, apple, xiaomi, huawei, ...]\n\n\n#Dropping remove_from_cart event type\n\nindex = df[df['event_type'] == 'remove_from_cart'].index\ndf.drop(index=index,inplace=True)\ndf['user_session'] = df['user_session'].astype('category').cat.codes","3f7e8753":"# df_targets = df.loc[df[\"event_type\"].isin([\"cart\",\"purchase\"])].drop_duplicates(subset=['event_type', 'product_id','price', 'user_id','user_session'])\n# df_targets[\"is_purchased\"] = np.where(df_targets[\"event_type\"]==\"purchase\",1,0)\n# df_targets[\"is_purchased\"] = df_targets.groupby([\"user_session\",\"product_id\"])[\"is_purchased\"].transform(\"max\")\n# df_targets = df_targets.loc[df_targets[\"event_type\"]==\"cart\"].drop_duplicates([\"user_session\",\"product_id\",\"is_purchased\"])\n# df_targets['event_weekday'] = df_targets['event_time'].apply(lambda s: str(datetime.strptime(str(s)[0:10], \"%Y-%m-%d\").weekday()))\n# df_targets.dropna(how='any', inplace=True)\n# df_targets[\"category_code_level1\"] = df_targets[\"category_code\"].str.split(\".\",expand=True)[0].astype('category')\n# df_targets[\"category_code_level2\"] = df_targets[\"category_code\"].str.split(\".\",expand=True)[1].astype('category')\n# df_targets.head()","8bf83ba0":"# cart_purchase_users = df.loc[df[\"event_type\"].isin([\"cart\",\"purchase\"])].drop_duplicates(subset=['user_id'])\n# cart_purchase_users.dropna(how='any', inplace=True)\n# cart_purchase_users_all_activity = df.loc[df['user_id'].isin(cart_purchase_users['user_id'])]","d7379b22":"# activity_in_session = cart_purchase_users_all_activity.groupby(['user_session'])['event_type'].count().reset_index()\n# activity_in_session = activity_in_session.rename(columns={\"event_type\": \"activity_count\"})","52355312":"# del date #free memory","96e98e28":"# df_targets = df_targets.merge(activity_in_session, on='user_session', how='right')\n# df_targets['activity_count'] = df_targets['activity_count'].fillna(0)\n","d393af72":"\n# df_targets.head()","d7aef7cd":"# df_targets.to_csv('training_data.csv')","31815ec5":"# df_targets.info()","3c3f8c07":"# from sklearn.preprocessing import LabelEncoder\n# from sklearn.preprocessing import MinMaxScaler\n# from sklearn.model_selection import train_test_split\n# from xgboost import plot_importance\n# from sklearn.utils import resample\n# from sklearn import metrics\n\n\n# is_purcahase_set = df_targets[df_targets['is_purchased']== 1]\n# is_purcahase_set.shape[0]","d855b778":"# not_purcahase_set = df_targets[df_targets['is_purchased']== 0]\n# not_purcahase_set.shape[0]","555fea58":"# n_samples = 537\n# is_purchase_downsampled = resample(is_purcahase_set,\n#                                 replace = False, \n#                                 n_samples = n_samples,\n#                                 random_state = 27)\n# not_purcahase_set_downsampled = resample(not_purcahase_set,\n#                                 replace = False,\n#                                 n_samples = n_samples,\n#                                 random_state = 27)","669db238":"# downsampled = pd.concat([is_purchase_downsampled, not_purcahase_set_downsampled])\n# downsampled['is_purchased'].value_counts()","563ab021":"# features = downsampled.loc[:,['brand', 'price', 'event_weekday', 'category_code_level1', 'category_code_level2', 'activity_count']]","b32cb63f":"# features.loc[:,'brand'] = LabelEncoder().fit_transform(downsampled.loc[:,'brand'].copy())\n# features.loc[:,'event_weekday'] = LabelEncoder().fit_transform(downsampled.loc[:,'event_weekday'].copy())\n# features.loc[:,'category_code_level1'] = LabelEncoder().fit_transform(downsampled.loc[:,'category_code_level1'].copy())\n# features.loc[:,'category_code_level2'] = LabelEncoder().fit_transform(downsampled.loc[:,'category_code_level2'].copy())\n\n# is_purchased = LabelEncoder().fit_transform(downsampled['is_purchased'])\n# features.head()","899a62e5":"# print(list(features.columns))\n# X_train, X_test, y_train, y_test = train_test_split(features, \n#                                                     is_purchased, \n#                                                     test_size = 0.3, \n#                                                     random_state = 0)","2a829f6a":"# from xgboost import XGBClassifier\n# from sklearn.tree import DecisionTreeClassifier\n# from sklearn.ensemble import RandomForestClassifier\n# from catboost import CatBoostClassifier, Pool\n\n\n# dtf = DecisionTreeClassifier(criterion=\"gini\", max_depth=None)\n# xgb = XGBClassifier(learning_rate=0.1)\n# rf  = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n# cb = CatBoostClassifier(learning_rate=0.03,\n#                            eval_metric='MAE')\n","0a58ffc3":"# def print_acc(model):\n#     model.fit(X_train, y_train)\n#     y_pred = model.predict(X_test)\n#     print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred))\n#     print(\"Precision:\",metrics.precision_score(y_test, y_pred))\n#     print(\"Recall:\",metrics.recall_score(y_test, y_pred))\n#     print(\"fbeta:\",metrics.fbeta_score(y_test, y_pred, average='weighted', beta=0.5 )  ,\"\\n\")\n\n\n    \n# print_acc(dtf)\n# print_acc(xgb)\n# print_acc(rf)","778b3856":"\n\ndf3 = pd.DataFrame([[1,12,111], [1, 13,111], [1, 14,111],[2,21,112] ,[2,22 ,112], [2,23,112] ], columns=['user_id', 'product_id','user_session'])\ndf3 = df3.sort_values(by=['user_session'])\nuser_session_values_3 = df3['user_session'].values\ndf3","13b165fc":"def take_column_products(multiarr,a):\n    array1 = []\n    for i in range(len(multiarr)):\n        array1.append(multiarr[i][a])\n    return array1","dc11effc":"unique_sessions = np.unique(user_session_values)\nproducts_for_each_user = []\n\nfor i in unique_sessions:\n    products_for_each_user.append(df.loc[df['user_session'] == i, 'product_id'].values)\n     \nprint(products_for_each_user)","e7bcfca3":"\ndef create_product_cols(sessions,df):\n    products_for_each_user = []\n    \n    \n    for i in sessions:\n        products_for_each_user.append(df.loc[df['user_session'] == i, 'product_id'].values)\n     \n    \n    print(products_for_each_user)\n        \n\n        \n    column1 = take_column_products(products_for_each_user,0) #product_id1\n    column2 = take_column_products(products_for_each_user,1) #product_id2\n    column3  = take_column_products(products_for_each_user,2) #product_id3\n    column4  = take_column_products(products_for_each_user,3) #product_id3\n    column5  = take_column_products(products_for_each_user,4) #product_id3\n    column6  = take_column_products(products_for_each_user,5) #product_id3       \n    column7  = take_column_products(products_for_each_user,6) #product_id3\n    column8  = take_column_products(products_for_each_user,7) #product_id3\n    column9  = take_column_products(products_for_each_user,8) #product_id3\n    column10  = take_column_products(products_for_each_user,9) #product_id3\n    column11  = take_column_products(products_for_each_user,10) #product_id3\n    column12  = take_column_products(products_for_each_user,11) #product_id3\n   # column13  = take_column_products(products_for_each_user,12) #product_id3\n\n    \n    \n    \n    \n    data = {'product_id1': column1, \n        'product_id2': column2, \n        'product_id3': column3,\n        'product_id4': column4,\n        'product_id5': column5,\n        'product_id6': column6,\n        'product_id7': column7,\n        'product_id8': column8,\n        'product_id9': column9,\n        'product_id10': column10,\n        'product_id11': column11,\n        'product_id12': column12,\n        } \n    \n    new_df = pd.DataFrame(data)\n    \n    \n    \n    \n#     df.insert(3,'product_id1',column1 )\n#     df.insert(4,'product_id2', column2)\n#     df.insert(5, 'product_id3' , column3)\n#     df.insert(6, 'product_id4' , column4)\n#     df.insert(7, 'product_id5' , column5)\n#     df.insert(8, 'product_id6' , column6)\n#     df.insert(9, 'product_id7' , column7)\n#     df.insert(10, 'product_id8' , column8)\n#     df.insert(11, 'product_id9' , column9)\n#     df.insert(12, 'product_id10' , column10)\n#     df.insert(13, 'product_id11' , column11)\n#     df.insert(14, 'product_id12' , column12)\n   # df.insert(15, 'product_id13' , column13)\n   \n    \n        \n     \n    return new_df","6ee1a6d4":"# #products_for_each_basket_df =  create_product_cols(user_session_values_3,df3)\n# print(products_for_each_basket_df)\n\n\n# new_df = products_for_each_basket_df[['product_id1', 'product_id2', 'product_id3']] \n# unique_sessions = np.unique(user_session_values_3)\n# target_values = []\n\n# for i in unique_sessions:\n#     arr = products_for_each_basket_df[products_for_each_basket_df['user_session'] == i][['product_id1','product_id2','product_id3']].values\n#     target_values.append(arr)\n    \n    \n# target_values = np.hstack(target_values)\n\n# products_for_each_basket_df.insert(5,'target',target_values )","4cb6718a":"# products_for_each_basket_df = products_for_each_basket_df.drop(columns=['product_id'],axis=1)\n\n# products_for_each_basket_df","93dc6a58":"import numpy as np\n\ntarget_values = []\n\ndef generate_target(df , frequent_products):\n        for index, row in df.iterrows():\n            for i in range(1,13):\n                    if row['product_id'+  str(i)] in frequent_products:\n                        target_values.append(row['product_id'+  str(i)])\n        return target_values\n\n\ndef convert_product_to_Nan(df):\n    for index, row in df.iterrows():\n        for i in range(1,13):\n            if row['product_id'+  str(i)] == row['target']:\n               df.loc[index,'product_id' + str(i)] = np.nan\n            \n    return df\n\n           \n           \n        ","bc5e6407":"df = df.drop_duplicates(['user_session','product_id'])\n\nusers_interactions_count_df = df.groupby(['user_id', 'product_id' , 'user_session']).size().groupby('user_session').size()  #Ask question about event_time or product_id\nprint(\"Number of users: %d\" % len(users_interactions_count_df))\n\nusers_with_enough_interactions_df = users_interactions_count_df[users_interactions_count_df = 12].reset_index()[['user_session']]\nusers_with_enough_interactions_df\nprint(users_with_enough_interactions_df)\nprint('amount of  users with  13 interactions: %d' % len(users_with_enough_interactions_df))\nprint(round(len(users_with_enough_interactions_df) * 100 \/ len(users_interactions_count_df), 2) , \"%\")","0870c033":"interactions_from_selected_users_df = df.merge(users_with_enough_interactions_df, \n                                           how = 'right',\n                                           left_on = 'user_session',\n                                           right_on = 'user_session'\n                                          )\nprint(interactions_from_selected_users_df)\n\nprint('# of interactions: %d' % len(df))\nprint('# of interactions from users with at least 13 interactions: %d' % len(interactions_from_selected_users_df))\ninteractions_from_selected_users_df = interactions_from_selected_users_df.drop(['event_type','event_time','category_id','brand', 'price'],axis=1)\ninteractions_from_selected_users_df = interactions_from_selected_users_df.drop(['category_code'],axis=1)\n","1502744f":"interactions_from_selected_users_df['user_session'] = interactions_from_selected_users_df['user_session'].astype('category').cat.codes\ninteractions_from_selected_users_df[-13: -1]","246d4196":"\ninteractions_from_selected_users_df.drop(interactions_from_selected_users_df[interactions_from_selected_users_df['user_session'] == 0].index,inplace=True)\ninteractions_from_selected_users_df.drop(interactions_from_selected_users_df[interactions_from_selected_users_df['user_session'] ==-1].index,inplace=True)\n\nprint(interactions_from_selected_users_df[-13 : -1])\n\ninteractions_from_selected_users_df = interactions_from_selected_users_df.sort_values(by=['user_session'])\nprint(interactions_from_selected_users_df)\nuser_session_values = interactions_from_selected_users_df['user_session'].values\nuser_session_values[-14:-1]\n\n\n","ec1b329e":"df_new =interactions_from_selected_users_df.groupby(['product_id','user_session'],sort=True)['product_id'].count()\ndf_new=interactions_from_selected_users_df[['product_id']].apply(pd.Series.value_counts)\ndf_new = df_new.loc[df_new['product_id'] >=10].reset_index()\ndf_new.columns = (\"product_id\", 'count_of_products')\ndf_new","a5b40d38":"from collections import Counter\n\nfrequent_products =df_new.product_id.values\nprint(len(frequent_products))\nprint(frequent_products)\n[item for item, count in Counter(frequent_products).items() if count > 1]","3a722676":"frequent_products_sessions_df = interactions_from_selected_users_df.merge(df_new, \n                                           how = 'right',\n                                           left_on = 'product_id',\n                                           right_on = 'product_id'\n                                          )\n\n\n\nprint(frequent_products_sessions_df[:150])\n\n\nfrequent_sessions = frequent_products_sessions_df['user_session']\nfrequent_sessions = frequent_sessions.sort_values(ascending=True)\n\nfor i in frequent_sessions:\n    if len(frequent_sessions[ frequent_sessions ==i]) < 12:\n        index = frequent_sessions[frequent_sessions == i].index[0]\n        frequent_sessions.drop(index= index,inplace=True)\n        \n        \n","bad6dd1c":"print(len(frequent_sessions))","31447096":"prod_for_basket_whole_df = create_product_cols(frequent_sessions,interactions_from_selected_users_df)\n","616a3ac2":"prod_for_basket_whole_df ","d15c1419":"# product_arr= []\n# #prod_for_basket_whole_df.drop(columns=['product_id'],inplace=True)\n\n# print(len(prod_for_basket_whole_df))\n# for i in range(1,13):\n#     product_arr.append('product_id' + str(i))\n\n# unique_sessions = np.unique(user_session_values)\n\n# target_values = []\n\n# for i in unique_sessions:\n#     arr = prod_for_basket_whole_df[prod_for_basket_whole_df['user_session'] == i][product_arr].values\n#     arr = np.unique(arr)\n#     target_values.append(arr)\n\n\n# target_values = np.hstack(target_values)\n# print(len(target_values))\n\n\n","9d6c2cae":"target_values = generate_target(prod_for_basket_whole_df,frequent_products)\nprint(len(target_values))","55f27b56":"target_values = np.reshape(target_values, (468, 12))\n\ntargets = set()\nnewlist = []\nfor item in target_values:\n    t = tuple(item)\n    if t not in targets:\n        newlist.append(item)\n        targets.add(t)\n\n        \nnewlist = np.hstack(newlist)\nprod_for_basket_whole_df['target'] = newlist\n\nprod_for_basket_whole_df\n","b45e8911":"prod_for_basket_whole_df = convert_product_to_Nan(prod_for_basket_whole_df)\nprod_for_basket_whole_df","c5a7abdc":"print(target_values)\n# prod_for_basket_whole_df['target'] = target_values\n# prod_for_basket_whole_df","a3880820":"last_df.isnull().sum()","e7e16db8":"   def justify(a, invalid_val=0, axis=1, side='left'):    \n    \"\"\"\n    Justifies a 2D array\n\n    Parameters\n    ----------\n    A : ndarray\n        Input array to be justified\n    axis : int\n        Axis along which justification is to be made\n    side : str\n        Direction of justification. It could be 'left', 'right', 'up', 'down'\n        It should be 'left' or 'right' for axis=1 and 'up' or 'down' for axis=0.\n\n    \"\"\"\n\n    if invalid_val is np.nan:\n        #change to notnull\n        mask = pd.notnull(a)\n    else:\n        mask = a!=invalid_val\n    justified_mask = np.sort(mask,axis=axis)\n    if (side=='up') | (side=='left'):\n        justified_mask = np.flip(justified_mask,axis=axis)\n    #change dtype to object\n    out = np.full(a.shape, invalid_val, dtype=object)  \n    if axis==1:\n        out[justified_mask] = a[mask]\n    else:\n        out.T[justified_mask.T] = a.T[mask.T]\n    return out                    ","3a57d953":"new_df = prod_for_basket_whole_df.iloc[:, 0:12]\n\n\ndf = pd.DataFrame(justify(new_df.values, invalid_val=np.nan, side='left', axis=1), \n                  columns=new_df.columns)\n\n\ndf\n","808036e0":"df.dropna(axis=1 , inplace=True) \n\ndf = df.astype('int')\ndf\n","3fddb753":"#df.info()\nprod_for_basket_whole_df.target","e7cd708e":"from sklearn import preprocessing\n\n\n\n\nX = df.values\ny = prod_for_basket_whole_df.target.values\n\n\n\n\n","57cbc6f0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42   )\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]\nprint(number_of_train, number_of_test)\n\ntype(y)","0b0ddb8b":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\n\n\n\n","7549dcc2":"from sklearn.model_selection import validation_curve \nfrom scipy.stats import randint as sp_randint\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import roc_curve, auc\n\n\n\n\n\nscores = []\nneighbors= []\n\nfor n in range(1 , 5000, 50):\n    knn = KNeighborsClassifier(n_neighbors=n)\n    neighbors.append(n)\n    knn.fit(X_train, y_train)\n    y_pred = knn.predict(X_test)\n    score = accuracy_score(y_test,y_pred)\n    scores.append(score)\n    print(\"Accuracy in neighbor {0} : \".format(n) ,score)\n\n    \nfrom matplotlib.legend_handler import HandlerLine2D\n\nline1, = plt.plot(neighbors, scores, 'r', label=\"Test AUC\")\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"Accuracy score\")\nplt.xlabel('n_neighbor')\nplt.show()\n    \n\n","ad33a775":"import catboost as cb\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.metrics import accuracy_score\n","166895e7":"scores = []\nparams= []\n\nfor n in range(1 , 4000, 50):\n    model = DecisionTreeClassifier(max_depth=n)\n    params.append(n)\n    model.fit(X_train, y_train)\n    y_pred = model.predict(X_test)\n    score = accuracy_score(y_test,y_pred)\n    scores.append(score)\n    print(\"Accuracy in max_depth {0} : \".format(n) ,score)\n\n    \nfrom matplotlib.legend_handler import HandlerLine2D\n\nline1, = plt.plot(params, scores, 'r', label=\"Test accuracy\")\n\nplt.legend(handler_map={line1: HandlerLine2D(numpoints=2)})\nplt.ylabel(\"Accuracy score\")\nplt.xlabel('n_neighbor')\nplt.show()","55447941":"#CatBoost \n\nmodel = CatBoostClassifier(iterations=1500, learning_rate=0.01,random_seed=42)\nmodel.fit(X_train,y_train)\nprint('Model params:\\n{}'.format(model.get_params()))\nprint('Predicted values')\ntrain_predictions = model.predict(X_test)\nacc = accuracy_score(y_test, train_predictions)\nprint(\"Accuracy: {:.4%}\".format(acc))\n","6c3acbe3":"items_by_basket = df.groupby(\"user_session\")[[\"product_id\"]].count()\ndesc = items_by_basket.describe()\ndesc\n\n\n#mean = 3.60\n#product_count  = mean + std  = 13\n#at least 3","4ff69247":"# labels = ['view', 'cart','purchase']\n# size = df['event_type'].value_counts()\n# explode = [0.25, 0.1,0.1] \n\n# plt.rcParams['figure.figsize'] = (15, 12)\n# plt.pie(size, explode = explode, labels = labels, shadow = True, autopct = '%.2f%%')\n# plt.title('Percentages of event_types', fontsize = 24)\n# plt.axis('off')\n# plt.legend()\n# plt.show()","2fade3b3":"# #add ranking to event types \n\n# event_ranking   = {\n#     'view' : 1.5,\n#     'cart' : 3,\n#     'purchase': 5\n    \n# }\n\n# df['event_ranking'] = df['event_type'].apply(lambda x: event_ranking[x])\n\n# df_targets = df.loc[df[\"event_type\"].isin([\"purchase\"])].drop_duplicates(subset=['event_type', 'product_id',\n#                                                                                          'user_id',\n#                                                                                         'user_session'])\n\n# print(df_targets.shape)\n# df_targets.tail()\n# df","6b5d1b5e":"\n\n# new_df = df.drop(columns=['brand','price','event_time','category_code','category_id','event_type'])[:100]\n# # basket = new_df.groupby(['user_id','user_session','product_id'])[['event_ranking']].sum()\n# basket = new_df.groupby(['user_session']).filter(lambda x: True)\n# basket.duplicated(['user_session', 'product_id']).any()\n# basket","174119bd":"# i, r = pd.factorize(basket['user_session'].values)\n# # get integer factorization `j` and unique values `c`            \n# # for column `'col'`\n# j, c = pd.factorize(basket['product_id'].values)\n# # `n` will be the number of rows\n# # `m` will be the number of columns\n# n, m = r.size, c.size\n# # `i * m + j` is a clever way of counting the \n# # factorization bins assuming a flat array of length\n# # `n * m`.  Which is why we subsequently reshape as `(n, m)`\n# b = np.bincount(i * m + j, minlength=n * m).reshape(n, m)\n\n# basket = pd.DataFrame(b, r, c)","4f8dfeba":"pd.get_dummies(new_df['user_session']).T.dot(pd.get_dummies(new_df['product_id']))","1bc08eaf":"df_targets.drop_duplicates(subset=['event_type', 'product_id', 'user_id', 'user_session']).shape[0]\n\ndf_targets[\"purchased\"] = np.where(df_targets[\"event_type\"]==\"purchase\",1,0)\nprint(df_targets.shape)\ndf_targets[\"purchased\"] = df_targets.groupby([\"user_session\",\"product_id\"])[\"purchased\"].transform(\"max\")\ndf_targets","675fd709":"def smooth_user_preference(x):\n    return math.log(1+x, 2)\n\n\ninteractions_full_df = interactions_from_selected_users_df \\\n                    .groupby(['user_id', 'product_id'])['event_ranking'].sum() \\\n                    .apply(smooth_user_preference).reset_index()\n\nprint('# of unique user\/item interactions: %d' % len(interactions_full_df))\ninteractions_full_df.head(10)","7a3b21ee":"from sklearn.model_selection import train_test_split\nfrom sklearn.naive_bayes import GaussianNB\n\ninteractions_train_df, interactions_test_df = train_test_split(interactions_full_df,\n                                   stratify=interactions_full_df['user_id'],                       \n                                   test_size=0.20,\n                                   random_state=42)\n\nprint('# interactions on Train set: %d' % len(interactions_train_df))\nprint('# interactions on Test set: %d' % len(interactions_test_df))","a4f5eb0e":"#Indexing by user_id to speed up the searches during evaluation\ninteractions_full_indexed_df = interactions_full_df.set_index('user_id')\ninteractions_train_indexed_df = interactions_train_df.set_index('user_id')\ninteractions_test_indexed_df = interactions_test_df.set_index('user_id')\n\ninteractions_train_indexed_df.head(10)\ninteractions_test_indexed_df.head(10)","d8dea05a":"def get_items_interacted(person_id,df):\n    interacted_items = df.loc[person_id]['product_id']\n    return set(interacted_items if type(interacted_items) == pd.Series else [interacted_items])","246e41e6":"# #Top-N accuracy metrics consts\n# EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS = 100\n\n# class ModelEvaluator:\n\n\n#     def get_not_interacted_items_sample(self, user_id, sample_size, seed=42):\n#         interacted_items = get_items_interacted(user_id, interactions_full_indexed_df)\n#         all_items = set(df[:30000]['product_id'])\n#         non_interacted_items = all_items - interacted_items\n\n#         random.seed(seed)\n#         non_interacted_items_sample = random.sample(non_interacted_items, sample_size)\n#         return set(non_interacted_items_sample)\n\n#     def _verify_hit_top_n(self, item_id, recommended_items, topn):        \n#             try:\n#                 index = next(i for i, c in enumerate(recommended_items) if c == item_id)\n#             except:\n#                 index = -1\n#             hit = int(index in range(0, topn))\n#             return hit, index\n\n#     def evaluate_model_for_user(self, model, user_id):\n#         #Getting the items in test set\n#         interacted_values_testset = interactions_test_indexed_df.loc[user_id]\n        \n#         if type(interacted_values_testset['product_id']) == pd.Series:\n#             person_interacted_items_testset = set(interacted_values_testset['product_id'])\n#         else:\n#             person_interacted_items_testset = set([int(interacted_values_testset['product_id'])])  \n#         interacted_items_count_testset = len(person_interacted_items_testset) \n\n#         #Getting a ranked recommendation list from a model for a given user\n#         person_recs_df = model.recommend_items(user_id, \n#                                                items_to_ignore=get_items_interacted(user_id, \n#                                                                                     interactions_train_indexed_df), \n#                                                topn=10000000000)\n\n#         hits_at_5_count = 0\n#         hits_at_10_count = 0\n#         #For each item the user has interacted in test set\n#         for item_id in person_interacted_items_testset:\n#             #Getting a random sample (100) items the user has not interacted \n#             #(to represent items that are assumed to be no relevant to the user)\n#             non_interacted_items_sample = self.get_not_interacted_items_sample(user_id, \n#                                                                           sample_size=EVAL_RANDOM_SAMPLE_NON_INTERACTED_ITEMS, \n#                                                                           seed=item_id%(2**32))\n\n#             #Combining the current interacted item with the 100 random items\n#             items_to_filter_recs = non_interacted_items_sample.union(set([item_id]))\n\n#             #Filtering only recommendations that are either the interacted item or from a random sample of 100 non-interacted items\n#             valid_recs_df = person_recs_df[person_recs_df['product_id'].isin(items_to_filter_recs)]                    \n#             valid_recs = valid_recs_df['product_id'].values\n#             #Verifying if the current interacted item is among the Top-N recommended items\n#             hit_at_5, index_at_5 = self._verify_hit_top_n(item_id, valid_recs, 5)\n#             hits_at_5_count += hit_at_5\n#             hit_at_10, index_at_10 = self._verify_hit_top_n(item_id, valid_recs, 10)\n#             hits_at_10_count += hit_at_10\n\n#         #Recall is the rate of the interacted items that are ranked among the Top-N recommended items, \n#         #when mixed with a set of non-relevant items\n#         recall_at_5 = hits_at_5_count \/ float(interacted_items_count_testset)\n#         recall_at_10 = hits_at_10_count \/ float(interacted_items_count_testset)\n\n#         person_metrics = {'hits@5_count':hits_at_5_count, \n#                           'hits@10_count':hits_at_10_count, \n#                           'interacted_count': interacted_items_count_testset,\n#                           'recall@5': recall_at_5,\n#                           'recall@10': recall_at_10}\n#         return person_metrics\n\n#     def evaluate_model(self, model):\n#         #print('Running evaluation for users')\n#         people_metrics = []\n#         for idx, user_id in enumerate(list(interactions_test_indexed_df.index.unique().values)):\n#             #if idx % 100 == 0 and idx > 0:\n#             #    print('%d users processed' % idx)\n#             person_metrics = self.evaluate_model_for_user(model, user_id)  \n#             person_metrics['_user_id'] = user_id\n#             people_metrics.append(person_metrics)\n#         print('%d users processed' % idx)\n\n#         detailed_results_df = pd.DataFrame(people_metrics) \\\n#                             .sort_values('interacted_count', ascending=False)\n        \n#         global_recall_at_5 = detailed_results_df['hits@5_count'].sum() \/ float(detailed_results_df['interacted_count'].sum())\n#         global_recall_at_10 = detailed_results_df['hits@10_count'].sum() \/ float(detailed_results_df['interacted_count'].sum())\n        \n#         global_metrics = {'modelName': model.get_model_name(),\n#                           'recall@5': global_recall_at_5,\n#                           'recall@10': global_recall_at_10}    \n#         return global_metrics, detailed_results_df\n    \n# model_evaluator = ModelEvaluator()","148abb07":"# item_popularity_df = interactions_full_df.groupby('product_id')['event_ranking'].sum().sort_values(ascending=False).reset_index()\n# item_popularity_df.head(10)","f1b12cdc":"# class PopularityRecommender:\n    \n#     MODEL_NAME = 'Popularity'\n    \n#     def __init__(self, popularity_df, items_df=None):\n#         self.popularity_df = popularity_df[]\n#         self.items_df = items_df\n        \n#     def get_model_name(self):\n#         return self.MODEL_NAME\n        \n#     def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n#         # Recommend the more popular items that the user hasn't seen yet.\n#         recommendations_df = self.popularity_df[~self.popularity_df['product_id'].isin(items_to_ignore)] \\\n#                                .sort_values('event_ranking', ascending = False) \\\n#                                .head(topn)\n\n#         if verbose:\n#             if self.items_df is None:\n#                 raise Exception('\"items_df\" is required in verbose mode')\n\n#             recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n#                                                           left_on = 'product_id', \n#                                                           right_on = 'product_id')[['event_ranking', 'product_id']]\n\n\n#         return recommendations_df\n    \n# popularity_model = PopularityRecommender(item_popularity_df, df)","ff593562":"# print('Evaluating Popularity recommendation model...')\n# pop_global_metrics, pop_detailed_results_df = model_evaluator.evaluate_model(popularity_model)\n# print('\\nGlobal metrics:\\n%s' % pop_global_metrics)\n# pop_detailed_results_df.head(10)","2279ce3d":"\n# pivot ratings into movie features\ndf_features = interactions_train_df[:30000].pivot(\n    index='product_id',\n    columns='user_id',\n    values='event_ranking'\n).fillna(0)\n# convert dataframe features to scipy sparse matrix","ccd3f913":"df_features.transpose()","e275d087":"\ndf_features_matrix = df_features.as_matrix()\ndf_features_matrix[:10]","b8a8f7c8":"user_ids  = list(df_features.index)\nuser_ids[:10]","fecf915e":"users_item_pivot_sparse_matrix = csr_matrix(df_features_matrix)\nusers_item_pivot_sparse_matrix","4d77248e":"#The number of factors to factor the user-item matrix.\nNUMBER_OF_FACTORS_MF = 15\n#Performs matrix factorization of the original user item matrix\n#U, sigma, Vt = svds(users_items_pivot_matrix, k = NUMBER_OF_FACTORS_MF)\nU, sigma, Vt = svds(users_item_pivot_sparse_matrix, k = NUMBER_OF_FACTORS_MF)","c05e4fda":"U.shape","94b68333":"sigma = np.diag(sigma)\nsigma.shape","58225c5f":"Vt.shape","eb2ac014":"# training_dataset = (\n#     tf.data.Dataset.from_tensor_slices(\n#         (\n#             tf.cast(df_features.values, tf.float32),\n#         )\n#     )\n# )\n\n# for x in training_dataset:\n#     print (x)","67851375":"all_user_predicted_ratings = np.dot(np.dot(U, sigma), Vt) \nall_user_predicted_ratings","c17798f7":"all_user_predicted_ratings_norm = (all_user_predicted_ratings - all_user_predicted_ratings.min()) \/ (all_user_predicted_ratings.max() - all_user_predicted_ratings.min())","bdc74d42":"#Converting the reconstructed matrix back to a Pandas dataframe\ncf_preds_df = pd.DataFrame(all_user_predicted_ratings_norm, columns = df_features.columns, index=user_ids).transpose()","86fb8565":"# class CFRecommender:\n    \n#     MODEL_NAME = 'Collaborative Filtering'\n    \n#     def __init__(self, cf_predictions_df, items_df=df[:30000]):\n#         self.cf_predictions_df = cf_predictions_df\n#         self.items_df = items_df\n        \n#     def get_model_name(self):\n#         return self.MODEL_NAME\n        \n#     def recommend_items(self, user_id, items_to_ignore=[], topn=10, verbose=False):\n#         # Get and sort the user's predictions\n#         sorted_user_predictions = self.cf_predictions_df[user_id].sort_values(ascending=False) \\\n#                                     .reset_index().rename(columns={user_id: 'recStrength'})\n\n#         # Recommend the highest predicted rating movies that the user hasn't seen yet.\n#         recommendations_df = sorted_user_predictions[ sorted_user_predictions['product_id'].isin(items_to_ignore)] \\\n#                                .sort_values('recStrength', ascending = False) \\\n#                                .head(topn)\n\n#         if verbose:\n#             if self.items_df is None:\n#                 raise Exception('\"items_df\" is required in verbose mode')\n\n#             recommendations_df = recommendations_df.merge(self.items_df, how = 'left', \n#                                                           left_on = 'product_id', \n#                                                           right_on = 'product_id')[['recStrength','product_id']]\n\n\n#         return recommendations_df\n    \n# cf_recommender_model = CFRecommender(cf_preds_df, df[:10000])","79e804cb":"len(cf_preds_df.columns)","6c8e44a6":"# print('Evaluating Collaborative Filtering (SVD Matrix Factorization) model...')\n# cf_global_metrics, cf_detailed_results_df = model_evaluator.evaluate_model(cf_recommender_model)\n# print('\\nGlobal metrics:\\n%s' % cf_global_metrics)\n# cf_detailed_results_df.head(10)","ca718c06":"tensor = tl.tensor([df_features.values])","1054cb13":"from tensorly.decomposition import tucker\nfrom tensorly import unfold\nunfolded_tensor = unfold(tensor, 1)\nunfolded_tensor[:5]","f4d4dcf4":"core , factors = tucker(unfolded_tensor,ranks=[2,3]) \ncore.shape","294f42bf":"len(factors)","0d349333":"core","0eb453c7":"[f  for f in factors]","43fe7034":"from surprise import SVD\nfrom surprise import Dataset\nfrom surprise.model_selection import GridSearchCV\nfrom surprise.model_selection import train_test_split\nfrom surprise import Reader\nfrom surprise import accuracy\nfrom surprise import NormalPredictor\nfrom surprise.model_selection import cross_validate\nfrom surprise.model_selection import KFold\n\ninteractions_full_df = interactions_full_df[:1000]\n\nreader = Reader(rating_scale=(0, 5))\ndata = Dataset.load_from_df(interactions_full_df[['user_id', 'product_id', 'event_ranking']], reader)\n\ncross_validate(NormalPredictor(), data, cv=3)","4622ecd7":"kf = KFold(n_splits=60)\n\nalgo = SVD()\n\nfor trainset, testset in kf.split(data):\n\n    # train and test algorithm.\n    algo.fit(trainset)\n    predictions = algo.test(testset)\n\n    # Compute and print Root Mean Squared Error\n    accuracy.rmse(predictions, verbose=True)","eeb5ace9":"param_grid = {'n_epochs': [5, 10], 'lr_all': [0.002, 0.005],\n              'reg_all': [0.4, 0.6]}\ngs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)\n\ngs.fit(data)\n\n# best RMSE score\nprint(gs.best_score['rmse'])\n\n# combination of parameters that gave the best RMSE score\nprint(gs.best_params['rmse'])","3f03dbeb":"interactions_full_df[interactions_full_df['user_id']  == 10280338]","d801c6f7":"from collections import defaultdict\n\n\ndef get_top_n(predictions, n=5):\n    '''Return the top-N recommendation for each user from a set of predictions.\n\n    Args:\n        predictions(list of Prediction objects): The list of predictions, as\n            returned by the test method of an algorithm.\n        n(int): The number of recommendation to output for each user. Default\n            is 10.\n\n    Returns:\n    A dict where keys are user (raw) ids and values are lists of tuples:\n        [(raw item id, rating estimation), ...] of size n.\n    '''\n\n    # First map the predictions to each user.\n    top_n = defaultdict(list)\n    for uid, iid, true_r, est, _ in predictions:\n        top_n[uid].append((iid, est))\n\n    # Then sort the predictions for each user and retrieve the k highest ones.\n    for uid, user_ratings in top_n.items():\n        user_ratings.sort(key=lambda x: x[1], reverse=True)\n        top_n[uid] = user_ratings[:n]\n\n    return top_n\n\n\n# First train an SVD algorithm on the  dataset.\nreader = Reader(rating_scale=(0, 5))\ndata = Dataset.load_from_df(interactions_full_df[['user_id', 'product_id', 'event_ranking']], reader)\n\ntrainset = data.build_full_trainset()\nalgo = SVD()\nalgo.fit(trainset)\n\n# Then predict ratings for all pairs (u, i) that are NOT in the training set.\ntestset = trainset.build_anti_testset()\n# for i in testset:\n#     if i == (10280338,5875289,2.084533813998394):\n#         testset.remove(i)\npredictions = algo.test(testset)\n\ntop_n = get_top_n(predictions, n=10)\n\n# Print the recommended items for each user\nfor uid, user_ratings in top_n.items():\n    print( uid, [iid  for (iid, _) in user_ratings])","126ff92e":"In Recommender Systems, there are a set metrics commonly used for evaluation. We chose to work with Top-N accuracy metrics, which evaluates the accuracy of the top recommendations provided to a user, comparing to the items the user has actually interacted in test set.\nThis evaluation method works as follows:\n\n* For each user\n    * For each item the user has interacted in test set\n    \n    1. Sample 100 other items the user has never interacted.\n       Ps. Here we naively assume those non interacted items are not relevant to the user, which        might not be true, as the user may simply not be aware of those not interacted items.\n  \n    2.   Ask the recommender model to produce a ranked list of recommended items, from a set              composed one interacted item and the 100 non-interacted (\"non-relevant!) items\n    \n    3.   Compute the Top-N accuracy metrics for this user and interacted item from the                    recommendations ranked list\n* Aggregate the global Top-N accuracy metrics\n\n\nThe Top-N accuracy metric choosen was **Recall@N** which evaluates whether the interacted item is among the top N items (hit) in the ranked list of 101 recommendations for a user.\nPs. Other popular ranking metrics are** NDCG@N** and** MAP@N**, whose score calculation takes into account the position of the relevant item in the ranked list (max. value if relevant item is in the first position).","091d4a27":"## Smoothing the distrubition\n\n Aggregating all the interactions the user has performed in an item by a weighted sum of interaction type strength and apply a log transformation to smooth the distribution.","738720d7":" ## Preprocessing dataset\n \n*  ### Creating df_targets dataframe & creating new features\n","42735596":"## Popularity model\n\nA common (and usually hard-to-beat) baseline approach is the Popularity model. This model is not actually personalized - it simply recommends to a user the most popular items that the user has not previously consumed. As the popularity accounts for the \"wisdom of the crowds\", it usually provides good recommendations, generally interesting for most people.","1af67a0e":"## Interactions from users with  at least 5 interactions","70d2b463":"## Evaluation \n\nUsing here a simple **cross-validation** approach named **holdout**, in which a random data sample (20% in this case) are kept aside in the training process, and exclusively used for evaluation. All evaluation metrics reported here are computed using the **test set.**","65de6d59":"## Data Mugging\n\nAs there are different interactions types, we associate them with a weight or strength, assuming that, for example, purchase of an item  indicates a higher interest of the user on the item than adding to cart, or than a simple view.","1d6ebcc2":" ## Accuracy","23b21201":"## Applying to whole dataset","7efe935f":"## Last Result","07154e5c":"## Classification","71036b7c":"## Cold start problem\n\nRecommender systems have a problem known as user cold-start, in which is hard do provide personalized recommendations for users with none or a very few number of consumed items, due to the lack of information to model their preferences.\nFor this reason, we are keeping in the dataset only users with at leas 2 interactions.","96bb6c1e":"\n## Gini Impurity\n\nUsed by the CART (classification and regression tree) algorithm for classification trees, Gini impurity is a measure of how often a randomly chosen element from the set would be incorrectly labeled if it was randomly labeled according to the distribution of labels in the subset. The Gini impurity can be computed by summing the probability {\\displaystyle p_{i}}p_{i} of an item with label {\\displaystyle i}i being chosen times the probability {\\displaystyle \\sum _{k\\neq i}p_{k}=1-p_{i}}{\\displaystyle \\sum _{k\\neq i}p_{k}=1-p_{i}} of a mistake in categorizing that item. It reaches its minimum (zero) when all cases in the node fall into a single target category."}}