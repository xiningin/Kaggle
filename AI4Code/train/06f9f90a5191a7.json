{"cell_type":{"e17d64e3":"code","4e971ca5":"code","7a3d6bec":"code","c70569e4":"code","1e2bc523":"code","4d751113":"code","2b1de54d":"code","f76f9dd5":"code","cea675f1":"code","a7c345d5":"code","df746dec":"code","4f46a182":"code","1c71cee5":"code","765a4677":"code","1af5ccd2":"code","2721d85e":"code","27a1b3af":"code","2bd7ceb8":"code","2c51c230":"code","87c8c371":"code","8c3c7b87":"code","866af3b4":"code","c367a5c2":"code","69b65fb5":"code","7d39e272":"code","a3f27bb1":"code","94afe3de":"code","3149b54d":"code","6f0e8885":"code","f4051e67":"code","c321e581":"code","4bfb3db6":"code","91c946d6":"code","ffcb9cc7":"code","ede4bb90":"code","ed0ce2ed":"code","32ffb04d":"code","ec0027cb":"code","8a9ad650":"code","c61aed38":"code","632a0fc2":"code","26ecf181":"code","2fd673c5":"code","59a75373":"code","58da7b54":"code","06ea39a1":"code","146004e3":"code","14ed79fa":"code","f8faf6f6":"code","0556219b":"code","e29fdb4d":"code","081429fc":"code","283f3ac6":"code","170a56c6":"code","6df406d2":"code","e0404646":"code","3343ffaa":"code","4ed12c10":"code","7b6da47a":"code","aa8bda63":"code","99c7b226":"code","0e3c584b":"code","10bcd22f":"code","e7720578":"code","a972e908":"code","695edf0c":"code","68c6db88":"code","c5cbc2f0":"code","a4cc61e9":"code","4ae99a82":"markdown","847ad511":"markdown","ec41ad99":"markdown","6c6fdbf1":"markdown","489c5e08":"markdown","8a11505d":"markdown","24856146":"markdown","f5d6822a":"markdown","1a48a317":"markdown","47c94de9":"markdown","d552a711":"markdown","8cbd49dc":"markdown","f45edc43":"markdown","94b5c23f":"markdown","5045ac1f":"markdown","ca8e3273":"markdown","7888a80c":"markdown","f4799b21":"markdown","8252e648":"markdown","2ebf526a":"markdown","ebfbaf34":"markdown","6102b25f":"markdown","24645d38":"markdown","f2425954":"markdown","ea7c0382":"markdown","3af6b5e6":"markdown","e13ce3b4":"markdown","ab590bfd":"markdown","767f918b":"markdown","835814e8":"markdown","2e51abde":"markdown","3149de6b":"markdown","032e45da":"markdown","08646939":"markdown","1f733dce":"markdown","8d34a743":"markdown"},"source":{"e17d64e3":"# Supress Warnings\n\nimport warnings\nwarnings.filterwarnings('ignore')","4e971ca5":"# import all required packages\n\n# importing pandas and numpy\nimport pandas as pd\nimport numpy as np\n\n# importing sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n# Importing RFE and LinearRegression\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\n# r2_score for model evaluation\nfrom sklearn.metrics import r2_score\n\n# importing stastsmodels\nimport statsmodels.api as sm\n# importing VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n# Importing the required libraries for plots.\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","7a3d6bec":"# Importing day.csv\nday = pd.read_csv('..\/input\/bike-sharing\/day.csv')\nday.head()","c70569e4":"# lets see the columns information in the dataset\nday.info()","1e2bc523":"# lets see the overall description of numeric variables\nday.describe()","4d751113":"# Lets see total records and total columns\nday.shape","2b1de54d":"initialTotalRecords = (day.shape)[0]\ninitialTotalColumns = (day.shape)[1]\n\nprint(\"Total Rows : \", initialTotalRecords);\nprint(\"Total Columns : \", initialTotalColumns);","f76f9dd5":"# Lets see if there are any null data in our dataset or not\nround(((day.isnull().sum()\/initialTotalRecords)*100), 2)","cea675f1":"# So lets remove some columns which are not useful for our predictions\n# instant - Its an index no much significance of it to keep in dataset\n# dteday - Its a date column, which is a kind of redundant column because we already have yr and month columns seperately\n# casual & registered - It is also kind of redundant columns because its combined count is already mentioned in cnt column. \n#                  And we also need to work on the count of the bikes instead of category count so we can remove this colum\n# Lets make a new dataframe without these columns with name 'bike'\n\nbike = day.copy(deep=True) # deep=True, since when we change one data frame other should not get updated\/effected by other.\ncolumns_to_remove = ['instant', 'dteday', 'casual', 'registered'];\nbike.drop(columns_to_remove, axis = 1, inplace = True)\nbike.head()","a7c345d5":"updated_total_records = (bike.shape)[0]\nupdated_total_columns = (bike.shape)[1]\n\nprint(\"Updated total rows : \", updated_total_records);\nprint(\"Updated total columns : \", updated_total_columns);\n\n# Columns were reduced from 16 to 12","df746dec":"# Lets check the unique values count for the categorical variables\ncategorical_columns = ['season', 'mnth', 'weathersit', 'weekday'];\nfor categorical_col in categorical_columns:\n    print(bike[[categorical_col]].value_counts(), \"\\n\");","4f46a182":"# If we look our data set, it looks like all are numerical data, but actually columns\n# 'cnt', 'temp', 'atemp', 'hum', 'windspeed' are actual numeric variable, remaining are numerical categorical variables\n# But later on any way we will convert the, to dummy variables so, lets visualize only the actual numeric variables\n# For any kind of corelation among them and with target variable 'cnt'\n# If no linear relation observed among the cnt and any of the variable then regression model is not possible\n\nnumeric_columns = ['cnt', 'temp', 'atemp', 'hum', 'windspeed'];\nsns.pairplot(bike[numeric_columns])\n","1c71cee5":"# Visualize the categorical data wrt to target variable cnt before making the dummy data\nplt.figure(figsize=(25, 10))\nplt.subplot(2,3,1)\nsns.boxplot(x = 'season', y = 'cnt', data = bike)\nplt.subplot(2,3,2)\nsns.boxplot(x = 'mnth', y = 'cnt', data = bike)\nplt.subplot(2,3,3)\nsns.boxplot(x = 'weathersit', y = 'cnt', data = bike)\nplt.subplot(2,3,4)\nsns.boxplot(x = 'holiday', y = 'cnt', data = bike)\nplt.subplot(2,3,5)\nsns.boxplot(x = 'weekday', y = 'cnt', data = bike)\nplt.subplot(2,3,6)\nsns.boxplot(x = 'workingday', y = 'cnt', data = bike)\nplt.show()","765a4677":"# Correlatio matrix to visulaize which columns are having corelation with cnt\nplt.figure(figsize = (16, 10))\nsns.heatmap(bike.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","1af5ccd2":"# Lets change the data types of some of the columns which needs to get dummy data.\ncolumns_to_get_dummy = ['season', 'mnth', 'weathersit', 'weekday'];\n# Convert datatype form numeric to category so that we can get dummies for these columns\nbike[columns_to_get_dummy] = bike[columns_to_get_dummy].astype('category')","2721d85e":"# Lets check info wether they have converted or not\nbike[columns_to_get_dummy].info()","27a1b3af":"# Now, lets creqate dummy variables for the categorical varibles with drop_first=True \nbike = pd.get_dummies(bike, drop_first=True)\nbike","2bd7ceb8":"# Now lets check the columns available in our data set after creating dummies\nbike.info()","2c51c230":"# Lets split the data set into training and test data set of 70-30 percentages respectively.\n# Use the standard notation for them as df_train, df_test. Sicne, we are having more than one predictor features\n# train_test_split returns data frames instead of series data.\n\n# random_state: Controls the shuffling applied to the data before applying the split\n# shuffel: Deafult True, so no need to specify it explicitly\ndf_train, df_test = train_test_split(bike, train_size=0.7, random_state=333);","87c8c371":"# Lets check the shape and data in df_train, df_test\nprint(\"Training data set : \", df_train.shape);\nprint(\"Testing data set : \", df_test.shape);\n\n### Data set was splitted as per our requirement\n### 730: Actual total data\n### 510: 70% of actual data (Training set)\n### 220: 30% of actual data (Testing set)","8c3c7b87":"# Lets see how data looks like in training data set\ndf_train.head()","866af3b4":"df_train.describe()","c367a5c2":"### Need to do scaling on the training data set. Sicne it was used for training the model\n\n# Create scaler object\nscaler = MinMaxScaler();\n\n# Create Columns list required for scaling\ncolumns_req_for_scaling = ['temp', 'atemp', 'hum', 'windspeed', 'cnt'];\n\n# Now, fit and transform the data for the above columns in our dataset\ndf_train[columns_req_for_scaling] = scaler.fit_transform(df_train[columns_req_for_scaling]);","69b65fb5":"# Lets check wether the data is scaled or not\ndf_train.describe()","7d39e272":"# Lets create the X & Y variables from training data frame for training the model \ny_train = df_train.pop('cnt') # Our target varaiable is cnt, So lets assume it as y_train and rest columns as X_train\nX_train = df_train","a3f27bb1":"# Lets see what data is available in X_train\nX_train.head()\n# We can observe all the columns except cnt is available in X_train","94afe3de":"X_train.describe()","3149b54d":"# Create Linear regression object\nlm = LinearRegression()\n\n# Fit the model\nlm.fit(X_train, y_train)\n\n# Running RFE with the output number of the variable equal to 15 (50% of actual no.of columns available and it is suggestable)\n# By passing the fitted model with it\nrfe = RFE(lm, 15)\n\n# get the fitted rfe\nrfe = rfe.fit(X_train, y_train)","6f0e8885":"# List down all the 15 outcomes given by the RFE with rankings and its significant boolean\n# support_ : provides wether that column is supported for the model or not\n# ranking_ : provides the ranking of the models suitable for the model to pick\nlist(zip(X_train.columns, rfe.support_, rfe.ranking_))","f4051e67":"cols_supports_model = X_train.columns[rfe.support_]\ncols_supports_model","c321e581":"cols_not_supported = X_train.columns[~rfe.support_]\ncols_not_supported","4bfb3db6":"# Remove columns which are not supported for model and store them \n# so that it will be used while testing the model with test dataset\nX_train = X_train[cols_supports_model];","91c946d6":"Cols_deleted = []\nfor val in cols_not_supported.values:\n    Cols_deleted.append(val);","ffcb9cc7":"Cols_deleted","ede4bb90":"X_train.head()","ed0ce2ed":"### Common method to print the VIF continously\ndef printVIF(trainingDataSet, constantVariable):\n    vif = pd.DataFrame();\n    actualDataSet = trainingDataSet;\n    dataSetWithoutConstant = actualDataSet.drop(constantVariable, axis=1);\n    X = dataSetWithoutConstant;\n    vif['Features'] = X.columns;\n    vif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])];\n    vif['VIF'] = round(vif['VIF'], 2);\n    vif = vif.sort_values(by = \"VIF\", ascending = False);\n    print(vif);","32ffb04d":"# Fixed target variable for our model\nconstant_variable = 'const';","ec0027cb":"# To build the model lets add constant to the X_train so that our model soent pass through origin\n# store it in new variable say X_train_lm\nX_train_lm = sm.add_constant(X_train);\n\n# Running the linear model\nlm = sm.OLS(y_train,X_train_lm).fit();","8a9ad650":"# print the parameters given by the model\nlm.params","c61aed38":"# Lets look at the summary\nlm.summary()","632a0fc2":"# Since we cannot take the decision based on the only pf values for feature removal. \n# Lets calculate the VIF for the X_train_lm columns\nprintVIF(X_train_lm, constant_variable);","26ecf181":"# Droping feature 'atemp' from training data set\nX_train_lm = X_train_lm.drop(['atemp'], axis=1)\nX_train_lm.columns","2fd673c5":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('atemp');\nCols_deleted","59a75373":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","58da7b54":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","06ea39a1":"# Droping feature 'temp' from training data set\nX_train_lm = X_train_lm.drop(['temp'], axis=1)\nX_train_lm.columns","146004e3":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('temp');\nCols_deleted","14ed79fa":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","f8faf6f6":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","0556219b":"# Droping feature 'hum' from training data set\nX_train_lm = X_train_lm.drop(['hum'], axis=1)\nX_train_lm.columns","e29fdb4d":"# Add deleted column into our Cols_deleted array for future use\nCols_deleted.append('hum');\nCols_deleted","081429fc":"# Running the linear model again\nlm = sm.OLS(y_train,X_train_lm).fit();\n# Print summary of rebuilded model\nlm.summary()","283f3ac6":"# Lets print the VIF for the current columns available in the training data set\nprintVIF(X_train_lm, constant_variable);","170a56c6":"# Coeffficents of model\nlm.params","6df406d2":"# Get the predicted values of y from model using training data set\ny_train_pred = lm.predict(X_train_lm)\n\n# Calculate residuals \nres = (y_train - y_train_pred)","e0404646":"# Plot the residuals\nsns.distplot(res);","3343ffaa":"# Lets do pre processing on the test data set as we did it on training data set\n# we do rescaling on test dataset for columns required for sacling and do transform on it instead of fit again\n# Now, fit and transform the data for the above columns in our dataset\ndf_test[columns_req_for_scaling] = scaler.fit_transform(df_test[columns_req_for_scaling]);\ndf_test.head()","4ed12c10":"df_test.describe()","7b6da47a":"# Now lets create y_test, X_test data sets for evalution\ny_test = df_test.pop('cnt')\nX_test = df_test","aa8bda63":"# Lets add constant for X_test for fitting model on test data set\nX_test_lm = sm.add_constant(X_test);","99c7b226":"# Columns before dropping deleted columns from final model\nX_test_lm.columns","0e3c584b":"# Remove columns that are not available in the final model so that our predictions will be accurate and matches with final model\nX_test_lm = X_test_lm.drop(Cols_deleted, axis=1)\nX_test_lm.columns","10bcd22f":"# Now predict the model based on the test data set using the final model obj\ny_test_pred = lm.predict(X_test_lm)","e7720578":"### Residuals are normally distributed\n# Get the predicted values of y from model using training data set\ny_train_pred = lm.predict(X_train_lm)\n\n# Calculate residuals \nres = (y_train - y_train_pred)\n# Plot the residuals\nsns.distplot(res);","a972e908":"### There is a linear relationship between X & Y\nbike_assump = bike[[ 'temp', 'atemp', 'hum', 'windspeed','cnt']]\nsns.pairplot(bike_assump, diag_kind='kde')\nplt.show()","695edf0c":"#### There is no multicollinearity between the predictor variables in the final model\nprintVIF(X_train_lm, constant_variable);","68c6db88":"# Plotting y_test and y_pred to understand the spread\nfig = plt.figure()\nplt.scatter(y_test, y_test_pred, alpha=.5)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16) \nplt.show()","c5cbc2f0":"# Calculate the R-Square for the training set\ntrain_r2_score = r2_score(y_true = y_train, y_pred = y_train_pred);\ntrain_r2_score","a4cc61e9":"# Calculate the R-Square for the predicted set\ntest_r2_score = r2_score(y_true = y_test, y_pred = y_test_pred);\ntest_r2_score","4ae99a82":"##### We can see from above graph that there is a linear relationship between X('temp', 'atemp') & y('cnt')","847ad511":"##### If we look at the above data, all of them are numerical data.\n#####  But, most of the columns are having data either 0\/1 except the columns temp, atemp, hum, windspeed, cnt.\n##### So, we need to rescale the data. So, that our predictions gets much more reliable and accurate.\n\n#### So, lets rescale the data using `MinMaxScaling\/Normalization` method.\n\n### Why Normalization method is using?\n#### Answer:\n##### Beacuse, \n    1. Normalization methods makes the data to be present in between 0-1 which looks similar to the other columns data.\n    2. It doesnt create any effect or change in the categorical data or dummy data that we have created already.","ec41ad99":"### Residual Analysis","6c6fdbf1":"### Assumptions","489c5e08":"#### We can able to make the following insights from the above box plots wrt to target variable 'cnt'\n\n    1. Working day:\n    - Almost 69% of users books bike in working day which is closes to 5000\n    - This indicates, workingday can be a good predictor for the dependent variable\n    \n    2. Weekday:\n        - Almost all weekdays, the no.of bike users count was similar and it is around in between 3000-6000\n        - Medians of all the weekdays are around in between the 4000-6000 that means, more than 50% of people using bikes in all days of a week irrespective of the day of the week.\n        - Difference\/distance between the 25% and 75% of box is more for weekdays 3(Wednesday) & 6(Saturday) but not a  significant difference when compared with others [Considering start of week as Sunday]\n        \n    3. Holiday:\n        - Almost 97.6% of the bike booking were happening when it is not a holiday which means this data is clearly biased. This indicates, holiday CANNOT be a good predictor for the dependent variable.\n         \n    4. Weathersit:\n        - Most of the bike users are in weathersit 1(Clear, Few clouds, Partly cloudy, Partly cloudy), followed by 2(Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist) which is almost 67% of users.\n        - This was followed by weathersit2 with 30% of total booking\n        - Very less no.of bike users are available in weathersit 3(Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds).\n        - This indicates, weathersit does show some trend towards the bike bookings can be a good predictor for the dependent variable.\n   \n    5. Month:\n        - Most of the bookings are happening around the months 6, 7, 8, 9 (more than 5000 bookings are happening. Almost 10%)\n        - Where as months 1 & 2 are having less bookings (Less than 3000)\n        - In almost all months the differenct between the 25% to 75% is similar but for months 3,4,9,10 is having significantly more difference\n        - This indicates, mnth has some trend for bookings and can be a good predictor for the dependent variable.\n        \n    6. Season:\n        - For season 3 having more no.of bike users (More than or equal to 5000 users. Almost 32%) followed by season2 & season4 with 27% & 25% (greater than 4000 and near to 5000)\n        - Season 1 is having less users which is less than 3000\n        - Almost all seasons are having the difference between 25% and 75% is significantly having no difference among them.\n        - This indicates, season can be a good predictor for the dependent variable.","8a11505d":"##### Selecting features using RFE","24856146":"### Final MLR line is as follows\n\n### cnt = 0.235851 + (yr * 0.243876) + (workingday * 0.043512) - (windspeed * 0.175637) + (season_2 * 0.268363) + \n###            (season_3 * 0.316649) + (season_4 * 0.199574) + (mnth_3 * 0.056134) + (mnth_9 * 0.090393) +\n###            (mnth_10 * 0.105879) + (weekday_6 * 0.045960) - (weathersit_2 * 0.083298) - (weathersit_3 * 0.347539)","f5d6822a":"#### Now lets convert the numeric categorical to the dummy variables\n#### To do that we need to first convert the type of those columns and then convert them to dummy variables\n\n### Why to convert the data type from numeric to category?\n    Before answering that, lets look at the syntax of get_dummies.\n#### Syntax: \n####    `pandas.get_dummies(data, prefix=None, prefix_sep=\u2019_\u2019, dummy_na=False, columns=None, sparse=False, drop_first=False, \n#### dtype=None)`\n\n#### Lets see the description of some attributes which are rerquired for our question:\n#### columns: \n        This attribute specifies the columns that needs to get dummies. Default is None. If not specified the columns it by default takes object, category datatype columns to get dummies.\n        \n#### drop_first:\n        This is default false. It means that we get dummies for all categorical of all 'k' levels. If it is true, then we get the dummies for 'k-1' level which is expected for our model.","1a48a317":"### Visualizing the dataset on original data","47c94de9":"##### From the above summary and model we can derive following data\n    High VIF value is `temp`\n    No features with high p-values (>0.05)\n    \n    R-Square is still same 84.2%\n    \n##### So, now lets remove the columm `temp` which is having more VIF and rebuild our model again","d552a711":"### Conclusion","8cbd49dc":"#### We can observe that residuals are centered around the mean of 0 and it is normally distributed.","f45edc43":"#### Hence, It is observed that our R-Squares values for training and predicted data sets are similar.\n#### Our model is best fit for our data prediction.","94b5c23f":"#### Since we are not having any High VIF and High P-Values we can stop modeling here and can make this as our best fit model.\n#### With \n    1. R-Square : 77.2%\n    2. Total Coefficients : 12 + 1 Constant","5045ac1f":"#### From the above equation for the obtained model top 3 predictor variables are \n####  `season_3` , `season_2`,  `yr`  with its coefficients 0.316649, 0.268363, 0.243876 respectively\n\n#### where as `weathersit_3`, `windspeed`, `weathersit_2` are negatively related with target variable with its coefficients 0.347539, 0.175637, 0.083298","ca8e3273":"### Rescaling the features","7888a80c":"### Preparing data for model","f4799b21":"#### From the above graph we can say that spread of the y_test and y_pred are linear and high nice spread among them.","8252e648":"### Reading the data and understanding the dataset","2ebf526a":"#### Prediction and evaluation on Test data","ebfbaf34":"##### Now, we can observe that mostly all the columns are sacled in between 0-1. It can be observed by checking the min value and max value of columns\n\n##### Now, our dataset is ready for the training.","6102b25f":"### Training the Model","24645d38":"### Splitting the data set into training and test data set","f2425954":"##### From the above summary and VIF we can say that:\n    NO Feature having p-value (> 0.05)\n    No High VIF feature\n    \n    R-Square value was changed slightly(negligable) from 77.3 to 77.2 percentage","ea7c0382":"#### From the above heatmap it is clearly observed that 'cnt' column is having \n    - More Positive Correlation with the predictor variables  'atemp', 'temp' followed by 'yr'.\n    - More Negative Correlation with 'weathersit', 'windspeed'","3af6b5e6":"### Model Evaluation","e13ce3b4":"#### Since, we used drop_first=True, categorical levels of season, month, weekday, weather were reduced by 1 from their actual levels","ab590bfd":"#### Above equation interpretations\n    cnt: Total no.of biker users count and it is the target variable in our model\n    yr: Year in which bike was rented\/used. Unit change in year causes the 0.243876 units change in users count\n    workingday: Wether the bike rented day is holiday or workingday. Unit increase in workingday increases 0.043512 unit changes in users count\n    windspeed: Unit increase in windspeed decreases 0.175637 units in users count\n    season_2: Season_2 is summer. Unit increase in season_2 increases 0.268363 units in users count\n    season_3: season_3 is fall. Unit increase in season_3 increases 0.316649 units in users count\n    season_4: season_4 is winter. Unit increase in season_4 increases 0.199574 units in user count\n    mnth_3: mnth_3 is march. Unit increase in mnth_3 increases 0.056134 units in users count\n    mnth_9: mnth_9 is september. Unit increase in mnth_9 increases 0.090393 units in users count\n    mnth_10: mnth_10 is october. Unit increase in mnth_10 increases 0.105879 units in users count\n    weekday_6: weekday_6 is friday. Unit increase in weekday_6 increases 0.045960 units in users count\n    weathersit_2: unit increase in weathersit_2 decreases 0.083298 units in users count\n    weathersit_3: unit increase in weathersit_3 decreases 0.347539 units in users count","767f918b":"#### We can observe that residuals are centered around the mean of 0 and it is normally distributed.","835814e8":"##### From the above summary and VIF we can say that:\n    High P-value feature is `hum`\n    High VIF feature is `hum`\n    \n    R-Square value was changed from 84.2 to 77.3 percentage\n   \n##### S0, lets remove the colum `hum` from the model and lets rebuild model again","2e51abde":"#### From the above plots we can observe there is a kind of correlation among\n    1. cnt with temp, atemp variables\n    2. temp and atemp","3149de6b":"##### From above model summary and VIF data we can see that \n    High p-value features are `atemp`\n    High VIF value feature are `temp`\n    \n    R-Square value is 84.8%\n    \n##### Since, we follow one of the golden thumb rule is to remove the feature whic is having more p-value before removing the High VIF value.\n##### So, lets remove the colum `atemp` from the model and lets rebuild model again","032e45da":"###### Looks like there are no null data in our dataset so we are good to proceed further","08646939":"##### Building the Model with the variables supported for model","1f733dce":"##### We can observe that all of the feature variables in the final model are having VIF less than 5. Hence we can say that there is no collinearity between the predictors","8d34a743":"### Analyszing the R-Square for Test and Training data Set"}}