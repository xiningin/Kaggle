{"cell_type":{"10d65c5b":"code","ef1d13d2":"code","d5611c3e":"code","897701ab":"code","0cc3c196":"code","db99afe9":"code","99dfd771":"code","26020679":"code","2da38bfc":"code","edbbaa0a":"code","b52ccc85":"code","0dacfac3":"code","8c0216b8":"code","c591b714":"code","c37065fb":"code","4810a6db":"code","a10c6033":"code","c8b0f936":"code","9b868ea6":"code","c368e447":"code","b51df86c":"code","996409cc":"code","a7520270":"code","bb6517fd":"code","84ae79f5":"code","0fe6212f":"code","499cc919":"code","333fbd6a":"code","9cd321ae":"code","edab9384":"code","b4c00c9a":"code","5886f70a":"code","ae951513":"code","0e059bdd":"code","98721e81":"code","0f257031":"code","04bdb474":"code","b9acaab8":"code","e5d1a043":"code","54921d57":"code","f35e0b6e":"code","00e4cd8a":"code","66ed977d":"code","4197a429":"code","6cad1ec9":"code","4a39bc9b":"code","749b1acb":"code","53a84b5d":"code","08e6b35d":"code","ea259d99":"code","c317727a":"code","5d30ae56":"code","d3fd92c9":"code","1df27adf":"markdown"},"source":{"10d65c5b":"import warnings\nwarnings.simplefilter('ignore')","ef1d13d2":"# Importing the Standard Libraries here\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","d5611c3e":"from sklearn.model_selection import cross_val_score, GridSearchCV, train_test_split\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, IsolationForest, ExtraTreesRegressor\nfrom sklearn.metrics import mean_squared_error, r2_score","897701ab":"# Importing the datasets\n\ndata= pd.read_csv('..\/input\/Train_UWu5bXk.csv')","0cc3c196":"print(data.shape)\n\n# We will append the train and test for easier preprocessing","db99afe9":"print(data.shape)\n\ndata.describe()\n\n# we see that these are the numeric columns, we will verify this in a moment and make changes if needed.","99dfd771":"# we will see if the numerical columns have some relationship with eachother\n\n# before that we will check if pandas infered all the columns correctly, make changes if not.\n\ndata.info()\n\n# we see that all of them to be correct, except year column. we will check the values first.","26020679":"# before everything else we will check if there are any missing data in the dataset\n\ndata.isna().sum()\n# we see missing data in Two variables, the third ids due to appending the test set.","2da38bfc":"data['Item_Weight'].interpolate(inplace= True)\n\n# Let's check the values in there. \n\nprint(data['Item_Weight'].isna().sum())\ndata['Item_Weight'].sample(10)\n\n# we see that the value has been filled","edbbaa0a":"# We need to impute the values on this variable\n\nprint(data['Outlet_Size'].isna().sum())\n\ndata['Outlet_Size'].fillna(data['Outlet_Size'].mode()[0], inplace= True)\n\nprint(data['Outlet_Size'].isna().sum())\n\n# we imputed the missing values, now we will proceed","b52ccc85":"print(f'Number of uniqure values in \"Estd.Year\" is: {data.Outlet_Establishment_Year.nunique()}')\nprint(data.Outlet_Establishment_Year.value_counts(dropna= False))\n\n# we will convert this in to an object as there aren't many values","0dacfac3":"data['Outlet_Establishment_Year']= data.Outlet_Establishment_Year.astype('object')","8c0216b8":"print(data.Outlet_Establishment_Year.dtype)\n\n# we see that data type is now an object","c591b714":"# we will now see a pairpolot of the numerical columns\n\nnum_cols= [*data.select_dtypes(['int64', 'float64']).columns]\n\nsns.pairplot(data[num_cols])\n\n# we don't see a lot of serious relationships betweem the variables and the target variables,\n# we will plot a more meaningful plot using seaborn","c37065fb":"num_cols.remove('Item_Outlet_Sales')\nnum_cols","4810a6db":"plt.figure(figsize= (24, 9))\n\ncount= 1\n\nfor col in num_cols:\n    \n    plt.subplot(3, 2, count)\n    \n    sns.regplot(x= col, y= 'Item_Outlet_Sales', data= data)\n    \n    plt.xlabel(col)\n    \n    count+=1\n    \n    plt.subplot(3, 2, count)\n    \n    sns.distplot(data.loc[data[col].notnull(), col])\n    \n    count+= 1\n    \n    # We can't see no clear relationship in the data","a10c6033":"data.head()","c8b0f936":"# We'll check the values of categorical columns ('objects')\n\nobj_cols= [*data.select_dtypes('object').columns]\n\nobj_cols","9b868ea6":"for col in obj_cols:\n    \n    if data[col].nunique() > 10:\n        print(f'Number of unique values in {col} is {data[col].nunique()} so not printing values.')\n        print(\" \")\n    else:\n        \n        print(f'Values in {col} are: \\n {data[col].value_counts()}')\n        print(\" \")\n        \n# we see that there are duplicate values in the Item_Fat_Content. We'll work on it","c368e447":"data['Item_Fat_Content'].value_counts(dropna= False)","b51df86c":"data['Item_Fat_Content'].replace({'LF': 'Low Fat', 'low fat': 'Low Fat', 'reg': 'Regular'}, inplace= True)\n\n# We will check the values again\ndata['Item_Fat_Content'].value_counts(dropna= False)\n# seems we have replace correctly","996409cc":"# We also saw that there are 1559 unique values in Item_Identifier column and the total number of observations are close to 8k\n# let's check this out\n\nprint(data['Item_Identifier'].sample(10)) # Let's try extracting the first three letters from the variable and check if there's a pattern\n\nprint(data['Item_Identifier'].str[:3].value_counts(dropna= False)) # looks like there are 71 values\n\nprint(data['Item_Identifier'].str[:2].value_counts(dropna= False)) # looks like there are only 3 values if we extract 2 letters\n\ndata['Item_Identifier']= data['Item_Identifier'].str[:2]","a7520270":"# We see that there is a value called NC, non-consumable, we will have to change fat content\n\ndata['Item_Fat_Content']= np.where(data['Item_Identifier']== 'NC', 'Non-durable', data['Item_Fat_Content'])","bb6517fd":"data.sample(10)","84ae79f5":"plt.figure(figsize= (24, 12))\n\nfor idx, col in enumerate(obj_cols):\n    \n    plt.subplot(3, 3, idx+1)\n    \n    sns.boxplot(col, 'Item_Outlet_Sales', data= data)","0fe6212f":"data.boxplot(column= 'Item_Outlet_Sales', by= ['Item_Fat_Content', 'Item_Identifier'], figsize= (12, 4), rot= 45)","499cc919":"data.boxplot(column= 'Item_Outlet_Sales', by= ['Outlet_Location_Type', 'Outlet_Size'], figsize= (12, 4), rot= 45)","333fbd6a":"# we'll create a dummy dataframe from the preprocessed dataFrame\n\n\ndf= pd.get_dummies(data, drop_first= True)","9cd321ae":"print(df.shape)\ndf.head()","edab9384":"for col in num_cols:\n    \n    print(f'Minimum value in {col} is: {data[col].min()}')\n    print(\" \")\n    print(f'Minimum value in {col} is: {data[col].max()}')\n    print(\" \")\n    \n    # seems like there ","b4c00c9a":"df['Non-Visible']= np.where(df['Item_Visibility']==0, 1, 0)\n\ndf['Non-Visible'].value_counts(dropna= False)","5886f70a":"df.head()","ae951513":"df.isna().sum()","0e059bdd":"X, y= df.drop('Item_Outlet_Sales', axis= 1), df.Item_Outlet_Sales","98721e81":"X.shape, y.shape","0f257031":"y.head()","04bdb474":"X_train, X_test, y_train, y_test= train_test_split(X, y, test_size= 0.2, random_state= 123)","b9acaab8":"lr = LinearRegression()\n\nlr.fit(X_train, y_train)\n\nlr_pred= lr.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, lr_pred)))","e5d1a043":"rf= RandomForestRegressor(max_depth= 5)\n\nrf.fit(X_train, y_train)\n\nrf_pred= rf.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, rf_pred)))\n\n# We get a better score from a RandomForest Model","54921d57":"gbm= GradientBoostingRegressor(max_depth= 2)\n\ngbm.fit(X_train, y_train)\n\ngbm_pred= gbm.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, gbm_pred)))\n\n# we get a slightly better score.\n","f35e0b6e":"from sklearn.metrics import make_scorer","00e4cd8a":"# creating a custom scoring function for cross validation\n\ndef RMSE(y_true, y_pred):\n    \n    RMSE = np.sqrt(np.mean((y_true - y_pred) ** 2))\n    \n    return RMSE\n\nrmse= make_scorer(RMSE, greater_is_better= False)","66ed977d":"score= cross_val_score(estimator= gbm, X= X_train, y= y_train, scoring= rmse, cv= 5,\\\n                n_jobs= -1, verbose= 1)\n\nscore.mean(), score.std()\n# we get negative score as the scorer function returns negative score in cross validation","4197a429":"et= ExtraTreesRegressor()\n\net.fit(X_train, y_train)\n\net_pred= et.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, et_pred)))\n\n# we get a slightly better score.","6cad1ec9":"iso_forest= IsolationForest(contamination= 'auto', behaviour= 'New')\n\noutliers= iso_forest.fit_predict(X, y)\n\npd.Series(outliers).value_counts(dropna= False)\n\n# -1 indicate that the values are outliers","4a39bc9b":"# we will remove the outliers from the original predictor(X) and traget(y) variables\n\nout_bool= outliers == 1\n\nX_new, y_new= X[out_bool], y[out_bool]","749b1acb":"X_new.shape, y_new.shape\n\n# we'll now create new train and test values","53a84b5d":"X_train, X_test, y_train, y_test= train_test_split(X_new, y_new, random_state= 123, test_size= 0.2)\n\n# splitting data 80\/20","08e6b35d":"lr= LinearRegression()\n\nlr.fit(X_train, y_train)\n\nlr_pred= lr.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, lr_pred)))","ea259d99":"rf= RandomForestRegressor(max_depth= 5)\n\nrf.fit(X_train, y_train)\n\nrf_pred= rf.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, rf_pred)))\n\n# the score improves a little.","c317727a":"gbm= GradientBoostingRegressor(max_depth= 2)\n\ngbm.fit(X_train, y_train)\n\ngbm_pred= gbm.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, gbm_pred)))\n\n# We see a slight improvement in the socre after removing the outliers\n","5d30ae56":"# we can perform a GridSearch to see if we can improve the score further\n\ngbm_params= {'max_depth': np.arange(1, 10, 2), \"max_features\": [.7, .8, .9],\n             'max_leaf_nodes': np.arange(2, 10, 2), \"min_samples_leaf\": np.arange(1, 10, 2),\n             'min_samples_split': np.arange(2, 10, 2)}\n\ngbm_grid= GridSearchCV(gbm, gbm_params, scoring= rmse, n_jobs= -1, cv= 3, verbose= 1)\n\ngbm_grid.fit(X_train, y_train)","d3fd92c9":"\ngbm_grid_pred= gbm_grid.predict(X_test)\n\nprint(np.sqrt(mean_squared_error(y_test, gbm_grid_pred)))\n\n# We see almost the same score","1df27adf":"## Exploring the datasets"}}