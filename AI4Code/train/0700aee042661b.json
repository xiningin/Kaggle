{"cell_type":{"3af4fca8":"code","dc591638":"code","778d5990":"code","d1e28cc3":"code","fec744e4":"code","9dc6f811":"code","110769ea":"code","50b9587c":"code","49f95bc2":"code","e0f2bee2":"code","cb9b5c90":"code","3f85d850":"code","854a583a":"code","bbb37495":"code","0bc5072b":"code","a715b675":"code","399601f0":"markdown","c854fd5e":"markdown","e42ceedc":"markdown","1a4e5127":"markdown","6e00898f":"markdown","2317a9c7":"markdown","d0b61027":"markdown","751327e1":"markdown","b2449681":"markdown","e2c48011":"markdown","791352a3":"markdown"},"source":{"3af4fca8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","dc591638":"import pandas as pd\nimport numpy as np\nimport sklearn\nimport os\nfrom os.path import join","778d5990":"#DATA Load\n\nDATA_PATH = join('\/kaggle','input','21-ai-w11-p2')\npd_train = pd.read_csv(join(DATA_PATH, 'train_data.csv'))\npd_test = pd.read_csv(join(DATA_PATH, 'test_data.csv'))\n\nprint(pd_train.info(), pd_test.info())","d1e28cc3":"import librosa\nimport glob, pickle\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nimport librosa, librosa.display \n\ndef load_audiofiles(file_name, sample_rate=48000):\n    \n    result=np.array([])\n    \n    audio_signal, sample_rate = librosa.load(file_name, duration=3, offset=0.5, sr=sample_rate)\n\n    signal = np.zeros(int(sample_rate*3,))\n    signal[:len(audio_signal)] = audio_signal\n    \n    return signal\n    ","fec744e4":"#DataFlair - Load the data and extract features for each sound file\nfrom tqdm import tqdm\ndef load_data(data_info, isTrain=True):\n    \n    PATH = join('\/kaggle','input','21-ai-w11-p2')\n    if isTrain:\n        train_data = []#\uc74c\uc131 feature\ub4e4\uc744 \ub2f4\ub294 dictionary\n        train_label = []#\ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ud560 label\uc744 \ub2f4\ub294 list\n        \n        file_list = data_info['file_name']\n        emotion_list = data_info['emotion']\n        for file_name, emotion in tqdm(zip(file_list, emotion_list)):\n            \n            hi=join(PATH, 'train_data\/train_data',file_name)\n            train_data.append(load_audiofiles(hi))\n            train_label.append(emotion)\n            \n        return np.array(train_data), np.array(train_label)\n    \n    else:\n        test_data = []\n        file_list = data_info['file_name']\n    \n        for file_name in tqdm(file_list):\n\n            hi=join(PATH, 'test_data\/test_data',file_name)\n            test_data.append(load_audiofiles(hi))\n            \n        return np.array(test_data)\n\n#DataFlair - Split the dataset\ntrain_data, train_label = load_data(pd_train)\ntest_data = load_data(pd_test, isTrain=False)","9dc6f811":"from sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nle.fit(train_label)\ny_train = le.transform(train_label)","110769ea":"def Calculate_Melspectrogram(audio, sample_rate):\n    mel_spec = librosa.feature.melspectrogram(y=audio,\n                                              sr=sample_rate,\n                                              n_fft=1024,\n                                              win_length = 512,\n                                              window='hamming',\n                                              hop_length = 256,\n                                              n_mels=128,\n                                              fmax=sample_rate\/2\n                                             )\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    return mel_spec_db\n\nmel_train = []\nprint(\"Calculate mel spectrograms for train set\")\ntrain_data = np.stack(np.array(train_data),0)\ntest_data = np.stack(np.array(test_data),0)\nfor i in range(train_data.shape[0]):\n    mel_spectrogram = Calculate_Melspectrogram(train_data[i,:], sample_rate=48000)\n    mel_train.append(mel_spectrogram)\n    print(\"\\r Processed {}\/{} files\".format(i+1,train_data.shape[0]),end='')\n    \nprint('')\nmel_train = np.stack(mel_train,axis=0)\n\nmel_test = []\nfor i in range(test_data.shape[0]):\n    mel_spectrogram = Calculate_Melspectrogram(test_data[i,:], sample_rate=48000)\n    mel_test.append(mel_spectrogram)\n    print(\"\\r Processed {}\/{} files\".format(i+1,test_data.shape[0]),end='')\n    \nprint('')\nmel_test = np.stack(mel_test,axis=0)\n\nprint(f'mel_train:{mel_train.shape}, mel_test:{mel_test.shape}')","50b9587c":"from sklearn.preprocessing import StandardScaler\n\nx_train = np.expand_dims(mel_train, 1) #DataNum, 1ch, H, W\nx_test = np.expand_dims(mel_test, 1)\n\nscaler = StandardScaler()\n\nb,c,h,w = x_train.shape\nx_train = np.reshape(x_train, newshape=(b,-1))\nx_train = scaler.fit_transform(x_train)\nx_train = np.reshape(x_train, newshape=(b,c,h,w))\n\nb,c,h,w = x_test.shape\nx_test = np.reshape(x_test, newshape=(b,-1))\nx_test = scaler.transform(x_test)\nx_test = np.reshape(x_test, newshape=(b,c,h,w))","49f95bc2":"import torch\nimport torch.nn as nn\n\nseed = 1\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nclass ParallelModel(nn.Module):\n    def __init__(self,num_emotions):\n        super().__init__()\n\n            # 1. conv block\n        self.relu = nn.ReLU()\n        self.conv1= nn.Conv2d(in_channels=1,\n                   out_channels=16,\n                   kernel_size=3,\n                   stride=1,\n                   padding=1\n                  )\n        self.bn1 = nn.BatchNorm2d(16)\n\n        self.mp1 = nn.MaxPool2d(kernel_size=2, stride=2)\n        self.do = nn.Dropout2d(p=0.3)\n        # 2. conv block\n        self.conv2= nn.Conv2d(in_channels=16,\n                   out_channels=32,\n                   kernel_size=3,\n                   stride=1,\n                   padding=1\n                  )\n        self.bn2 = nn.BatchNorm2d(32)\n        self.mp2 = nn.MaxPool2d(kernel_size=4, stride=4)\n\n        # 3. conv block\n        self.conv3 = nn.Conv2d(in_channels=32,\n                   out_channels=64,\n                   kernel_size=3,\n                   stride=1,\n                   padding=1\n                  )\n        self.bn3 = nn.BatchNorm2d(64)\n\n        # 4. conv block\n        self.conv4= nn.Conv2d(in_channels=64,\n                   out_channels=128,\n                   kernel_size=3,\n                   stride=1,\n                   padding=1\n                  )\n        self.bn4= nn.BatchNorm2d(128)\n\n        # Linear softmax layer\n        self.out_linear = nn.Linear(512,num_emotions)\n        self.out_softmax = nn.Softmax(dim=1)\n        \n    def forward(self,x):\n\n        # transformer embedding\n        out = self.relu(self.bn1(self.conv1(x)))\n\n        out = self.do(self.mp1(out))\n        out = self.relu(self.bn2(self.conv2(out)))\n        out = self.do(self.mp2(out))\n        out = self.relu(self.bn3(self.conv3(out)))\n        out = self.do(self.mp2(out))\n        out = self.relu(self.bn4(self.conv4(out)))\n        out = self.do(self.mp2(out))\n\n\n        conv_embedding = torch.flatten(out, start_dim=1)\n\n        output_logits = self.out_linear(conv_embedding)\n        output_softmax = self.out_softmax(output_logits)\n        return output_logits, output_softmax  \n    \nmodel = ParallelModel(num_emotions=8).to(device)\nprint('Number of trainable params: ',sum(p.numel() for p in model.parameters()) )","e0f2bee2":"!pip install livelossplot","cb9b5c90":"EPOCHS=150\nDATASET_SIZE = x_train.shape[0]\nBATCH_SIZE = 64\n\n##baseline\uc5d0\uc11c \uc0ac\uc6a9\ud55c optimizer\ub294 SGD\uc774\uba70 \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4.\n##Learning rate = 0.01, momentum=0.9\nOPTIMIZER = torch.optim.SGD(model.parameters(),lr=0.01, momentum=0.9)","3f85d850":"def loss_fnc(predictions, targets):\n    return nn.CrossEntropyLoss()(input=predictions,target=targets)","854a583a":"from livelossplot import PlotLosses\nliveloss = PlotLosses()\n\nlogs = {}\n\nmodel.train()\n\nfor epoch in range(EPOCHS):\n    # shuffle data\n    ind = np.random.permutation(DATASET_SIZE)\n\n    x_train = x_train[ind,:,:,:]\n    y_train = y_train[ind]\n    \n    epoch_loss = 0\n    \n    iters = int(DATASET_SIZE \/ BATCH_SIZE)\n     \n    for i in range(iters):\n        ### indexing\uacfc\uc815\uc744 \ud1b5\ud574 \uc791\uc131\ub41c Dataloader \ucf54\ub4dc\n        batch_start = i * BATCH_SIZE\n        batch_end = min(batch_start + BATCH_SIZE, DATASET_SIZE)\n        actual_batch_size = batch_end-batch_start\n        \n        x = x_train[batch_start:batch_end,:,:,:]\n        y = y_train[batch_start:batch_end]\n        \n        ###--------------------\uc791\uc131\ud574\uc57c\ud560 \ubd80\ubd84--------------------------------\n        ##1.train \ub370\uc774\ud130\ub97c torch tensor\ub85c \ud0c0\uc785 \ubcc0\uacbd \ubc0f gpu \uc124\uc815\n        ##2.\ub370\uc774\ud130\ub97c \ubaa8\ub378\uc758 \uc785\ub825\uc73c\ub85c \ub123\uc5b4 \uc608\uce21 \ud6c4 loss \uacc4\uc0b0 \ubc0f optimization \n        x_tensor = torch.tensor(x,device=device).float()\n        y_tensor = torch.tensor(y, dtype=torch.long, device=device)\n        \n        # forward pass\n        output_logits, output_softmax = model(x_tensor)\n\n        # compute loss\n        loss = loss_fnc(output_logits, y_tensor)\n        \n        # compute gradients\n        OPTIMIZER.zero_grad()\n        loss.backward()\n        # update parameters\n        OPTIMIZER.step()\n\n        epoch_loss += loss.item()*actual_batch_size\/DATASET_SIZE\n\n    ###liveloss \ud568\uc218\ub97c \uc0ac\uc6a9\ud558\uae30 \uc704\ud55c \ucf54\ub4dc\ub4e4\n    logs['train_loss'] = epoch_loss\n        \n    liveloss.update(logs)\n    liveloss.draw()","bbb37495":"model.eval()\n\npredicts = []\n\nwith torch.no_grad():\n    for data in tqdm(x_test):\n        data = torch.FloatTensor(data).to(device).unsqueeze(1)\n\n        _ , output_softmax = model(data)\n        predictions = torch.argmax(output_softmax,dim=1).cpu().numpy()\n        predicts.append(predictions)\n\nprint(f'predict_len:{len(predicts)}')","0bc5072b":"###\uc800\uc7a5\ub41c \uc608\uce21\uac12\uc744 \uc704\uc5d0\uc11c \uc0ac\uc6a9\ud55c label encoder\ub97c \uc774\uc6a9\ud574 \ub2e4\uc2dc \ubb38\uc790\uc5f4\ub85c \uc5ed\ubcc0\ud658\ud569\ub2c8\ub2e4.\npredicts = le.inverse_transform(predicts)\npredicts","a715b675":"ID = np.array([i for i in range(len(predicts))]).reshape(-1,1)\n\nresults = np.hstack([ID, np.array(predicts).reshape(-1,1)])\n\nresults =pd.DataFrame(results, columns=['ID','emotion'])\nresults.to_csv(\"baseline.csv\", index=False)","399601f0":"# \ucf54\ub4dc \uc2dc\uc791 \uc804 \uc124\uba85\n1. \uc74c\uc131 \ub370\uc774\ud130\uc5d0 \ub300\ud55c \uc774\ub860 \ubc0f \uacfc\uc815\uc744 \uc798 \ubaa8\ub974\uc2dc\ub294 \ubd84\uc740 \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uae30 \uc804\uc5d0 overview\ub97c \ud55c\ubc88 \uc815\ub3c5\ud558\uc2dc\ub294 \uac83\uc744 \uad8c\ud569\ub2c8\ub2e4\ub9cc, \ud574\ub2f9 \uc2a4\ucf08\ub808\ud1a4 \ucf54\ub4dc \ub0b4\uc5d0\uc11c \uc74c\uc131 feature\ub97c \uac00\uacf5\ud558\ub294 \ucf54\ub4dc\ub97c \ubaa8\ub450 \uc81c\uacf5\ud558\uae30 \ub54c\ubb38\uc5d0 \uad73\uc774 \uc77d\uc9c0 \uc54a\uc73c\uc154\ub3c4 \uc0c1\uad00\uc5c6\uc2b5\ub2c8\ub2e4.\n\n2. \uc2a4\ucf08\ub808\ud1a4 \ucf54\ub4dc\uac00 \uc801\ud600\uc788\ub294 \ud574\ub2f9 ipynb \ud30c\uc77c\uc744 \ub2e4\uc6b4\ubc1b\uc544 \uc790\uc2e0\uc758 \uc791\uc5c5\uacf5\uac04(\ucf54\ub7a9, \uce90\uae00\ub178\ud2b8\ubd81 \ub4f1\ub4f1)\uc5d0\uc11c \ucf54\ub4dc\ub97c \uc791\uc131\ud569\ub2c8\ub2e4.\n\n3. \uac01 \ucf54\ub4dc\uc5d0\ub294 \ube48\uce78\uacfc \ud568\uaed8 \uad6c\ud604 \uac00\uc774\ub4dc \ub77c\uc778\uc774 \uc81c\uacf5\ub429\ub2c8\ub2e4. \ud574\ub2f9 \uac00\uc774\ub4dc\ub77c\uc778\uc744 \ub530\ub77c \ucf54\ub4dc\ub97c \uc791\uc131\ud558\uc5ec \ubca0\uc774\uc2a4 \ub77c\uc778 \uc131\ub2a5\uc744 \ub2ec\uc131\ud569\ub2c8\ub2e4.\n   \n4. (\uc120\ud0dd)\ubca0\uc774\uc2a4\ub77c\uc778\uc5d0 \ub3c4\ub2ec\ud558\uc600\ub2e4\uba74, \ub2e4\uc591\ud55c \ubc29\ubc95\uc73c\ub85c \uc131\ub2a5\uc744 \ub354 \uc62c\ub824\ubd05\uc2dc\ub2e4.(\uc74c\uc131 feature \uc7ac\uac00\uacf5, \ubaa8\ub378 architecture \ubcc0\uacbd, data \uc804\ucc98\ub9ac \ubc0f augmentation \ub4f1)","c854fd5e":"### Step4 3\ubc88\uc5d0\uc11c \uac00\uacf5\ud55c Mel_spectrogram\uc5d0 scale normaliztaion \uc801\uc6a9","e42ceedc":"### Step2 \ud559\uc2b5 \ub370\uc774\ud130\uac00 \ubb38\uc790\uc5f4\uc774\ubbc0\ub85c \uc2e4\uc218\ub85c \ubca1\ud130\ud654","1a4e5127":"### \ud559\uc2b5\uc5d0 \uc0ac\uc6a9\ub41c \ud558\uc774\ud37c\ud30c\ub77c\ubbf8\ud130 \ubc0f \uc635\ud2f0\ub9c8\uc774\uc800 \uc124\uc815","6e00898f":"### \ubaa8\ub378 \ud3c9\uac00","2317a9c7":"## \uc74c\uc131 \ub370\uc774\ud130 \uac00\uacf5 \ubc0f \uc804\ucc98\ub9ac \uacfc\uc815\n1. mp4 \ud30c\uc77c\uc744 windowing & sampling \uacfc\uc815\uc744 \ud1b5\ud574 \ubd88\ub7ec\uc634\n2. \ud559\uc2b5 \ub370\uc774\ud130\uac00 \ubb38\uc790\uc5f4\uc774\ubbc0\ub85c \uc2e4\uc218\ub85c \ubca1\ud130\ud654\n3. \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud574 1\ubc88\uc5d0\uc11c \uac00\uacf5\ud55c \uc74c\uc131 feature\ub97c Fourier transform \ubc0f mel filter\ub97c \ud1b5\ud55c Mel_spectrogram\uc73c\ub85c \ubcc0\ud658\n4. 3\ubc88\uc5d0\uc11c \uac00\uacf5\ud55c Mel_spectrogram\uc5d0 scale normaliztaion \uc801\uc6a9","d0b61027":"## \ubaa8\ub378 \uc124\uacc4\n\ubcf8\uaca9\uc801\uc73c\ub85c \ubca0\uc774\uc2a4\ub77c\uc778\uc744 \ub3c4\ub2ec\ud558\uae30 \uc704\ud55c\ubaa8\ub378\uc744 \uc124\uacc4\ud574\uc57c\ud569\ub2c8\ub2e4.\n\uc544\ub798 \uadf8\ub9bc\uacfc \ud45c\ub97c \uc798 \ucc38\uace0\ud558\uc5ec pytorch\ub85c \ubaa8\ub378\uc744 \uad6c\ud604\ud574\ubcf4\uc138\uc694.\n### \ubaa8\ub378 Pipeline\n![image.png](attachment:b3ac287f-4fdd-492e-9249-72ecca3fbfaa.png)\n\n### \ubaa8\ub378 \uc138\ubd80 \uc815\ubcf4\n![image.png](attachment:857dcc8a-5ce8-4672-be96-db7f6df962fc.png)","751327e1":"### Step1 mp4 \ud30c\uc77c\uc744 windowing & sampling \uacfc\uc815\uc744 \ud1b5\ud574 \ubd88\ub7ec\uc634","b2449681":"### Step3 \ubaa8\ub378 \ud559\uc2b5\uc744 \uc704\ud574 1\ubc88\uc5d0\uc11c \uac00\uacf5\ud55c \uc74c\uc131 feature\ub97c Fourier transform \ubc0f mel filter\ub97c \ud1b5\ud55c Mel_spectrogram\uc73c\ub85c \ubcc0\ud658","e2c48011":"### Loss \uc2dc\uac01\ud654 \ud234 \uc124\uce58","791352a3":"### \uc608\uce21\uac12 csv \ud30c\uc77c\ub85c \uc800\uc7a5 \ud6c4 \uc81c\ucd9c"}}