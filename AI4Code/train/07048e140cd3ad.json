{"cell_type":{"cbf3bd24":"code","e929e891":"code","ab9230b5":"code","fe8c58d5":"code","e4105da3":"code","1ae8fbc0":"code","00ce6657":"code","5abd30bb":"code","5336dd5f":"code","35461338":"code","d45f6df9":"code","86cb4579":"code","0b3c4c3e":"code","23412001":"code","28bd936f":"code","bb285134":"code","ce536c45":"code","bc1fdee3":"code","3ce0cca1":"markdown","a2a7f8a3":"markdown","697d6e56":"markdown","c2cceeff":"markdown","c53f2974":"markdown","62e0efb2":"markdown","7a4ab88a":"markdown","686af9f7":"markdown","9c4e7208":"markdown"},"source":{"cbf3bd24":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e929e891":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\nsubmission = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')","ab9230b5":"print(df_train.info())\nprint('*****')\nprint(df_test.info())","fe8c58d5":"print(df_train.shape)\nprint(df_test.shape)","e4105da3":"features = [col for col in df_train.columns if 'f' in col]","1ae8fbc0":"print(features)","00ce6657":"import seaborn as sns\n\nsns.countplot(x = 'target', data = df_train)","5abd30bb":"import matplotlib.pyplot as plt\n\nfor idx, feature in enumerate(features):\n    plt.hist(df_train[feature], bins=30, alpha=0.5, label='Train set')\n    plt.hist(df_test[feature], bins=30, alpha=0.5, label='Test set')\n    plt.title(feature + \" Train\/Test\")\n    plt.xlabel(feature)\n    plt.ylabel('Frequency')\n\n    plt.legend()\n    plt.show()","5336dd5f":"print(df_train[features].isna().sum().sum())\nprint(df_test[features].isna().sum().sum())","35461338":"X = df_train[features].copy()\ny = df_train['target'].copy()\n\nx_test = df_test[features].copy()","d45f6df9":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\nX[features] = scaler.fit_transform(X[features])\nx_test[features] = scaler.transform(x_test[features])","86cb4579":"for idx, feature in enumerate(features):\n    plt.plot(X[df_train['target']==0][feature], df_train.loc[df_train['target']==0]['target'])\n    plt.plot(X[df_train['target']==1][feature], df_train.loc[df_train['target']==1]['target'])\n    plt.xlabel(feature)\n\n    plt.legend()\n    plt.show()","0b3c4c3e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nmodel = LogisticRegression(random_state=0)\nmodel.fit(X_train,y_train)\n\ny_pred = model.predict(X_test)","23412001":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test, y_pred)\n  \nprint (\"Confusion Matrix : \\n\", cm)","28bd936f":"from sklearn.metrics import accuracy_score\nprint (\"Accuracy : \", accuracy_score(y_test, y_pred))","bb285134":"logistic_prediction = model.predict(x_test)","ce536c45":"logistic_csv = pd.DataFrame()\nlogistic_csv['id'] = df_test['id']\nlogistic_csv['target'] = logistic_prediction","bc1fdee3":"logistic_csv.to_csv('logistic', index=False)","3ce0cca1":"Let's have an overview of the data","a2a7f8a3":"### True positive + True negative = 71488 + 74826 = 146314\n### False positive + False negative = 26309 + 25377 = 51686","697d6e56":"### No missing values in train and test data. That is one less thing to worry about","c2cceeff":"### Observing the distribution of data in the train and test set","c53f2974":"The range of value for each feature is the same for target value = 0 and target value = 1","62e0efb2":"### Conclusion\n\nThe train and test data distribution are similar. This is great!!!","7a4ab88a":"### Conclusion\nThe distribution of data with target = 0  and target = 1 made are the same. This is one less thing to worry about :)","686af9f7":"### Observing the test data","9c4e7208":"## Implementing Logistic Regression model"}}