{"cell_type":{"d12f1a41":"code","d5857a24":"code","dbf31d1f":"code","6e16f596":"code","a54d8b80":"code","deadda90":"code","5d4847be":"code","397d09d3":"code","a2f490d6":"code","f8b61f4e":"code","b8536c38":"code","f47720ad":"code","eb438fbc":"code","cbab3b96":"code","264d5d81":"code","5f675f37":"code","e3a9aae4":"code","4a05e4e8":"code","30ad1fc2":"code","d8c00633":"code","c756f921":"code","7bde9fb4":"code","4f92cda8":"code","7046f206":"code","f01a7b90":"code","13dc5da4":"code","b3f67a2e":"code","5aad642c":"code","73bf9a49":"code","24ce80d9":"code","87dc0018":"code","adb8fb3d":"code","c171c546":"code","6a0b6297":"code","50a18397":"code","aac0121e":"code","fd9e30c3":"code","309fee74":"code","fdb17e40":"code","fc9c80c1":"code","b8821368":"code","b6ecd996":"code","305f30d4":"code","dc03f288":"code","94078143":"code","038d7aa0":"markdown","59b9b6f2":"markdown","e95484ce":"markdown","22f9bb22":"markdown","21e10a70":"markdown","3fd71523":"markdown","8b8e82c8":"markdown","5d1bafa5":"markdown","a110ea5e":"markdown","4c6e9895":"markdown","7c96eda5":"markdown","46e808cb":"markdown","d7f31bc0":"markdown","92fa9b12":"markdown","94cf9a47":"markdown","59aee018":"markdown"},"source":{"d12f1a41":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","d5857a24":"import numpy as np \nimport pandas as pd \nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import classification_report, plot_confusion_matrix\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score\nimport nltk\nfrom nltk.corpus import stopwords\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom string import punctuation\nimport re\n\nRANDOM_STATE = 1","dbf31d1f":"train_set = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","6e16f596":"train_set.head()","a54d8b80":"train_set.isnull().sum()","deadda90":"test_set.isnull().sum()","5d4847be":"train_set['keyword'].nunique()","397d09d3":"test_set['keyword'].nunique()","a2f490d6":"plt.figure(figsize=(10, 10))\nsns.countplot(y=\"keyword\",data=train_set, order=train_set.keyword.value_counts().iloc[:10].index)\nplt.title('Top 10 Keyword distribution')\nplt.show()","f8b61f4e":"train_set['keyword'] = train_set['keyword'].fillna('Unknown_Keyword')\ntest_set['keyword'] = test_set['keyword'].fillna('Unknown_Keyword')","b8536c38":"train_set['keyword'].value_counts()","f47720ad":"location_mapping = {\n                    'New York':'USA',\n                    'United States': 'USA',\n                    'London': 'UK',\n                    'Lincoln': 'USA',\n                    'Montr\u00cc\u00a9al, Qu\u00cc\u00a9bec': 'Canada',\n                    'Montreal':'Canada',\n                    'Los Angeles, CA':'USA',\n                    'Surulere Lagos,Home Of Swagg': 'Nigeria',\n                    'Waco, Texas': 'USA',\n                    'Chicago, IL': 'USA',\n                    'Mumbai': 'India',\n                    'Washington, DC': 'USA',\n                    'California': 'USA',\n                    'New York, NY': 'USA',\n                    'California, USA': 'USA',\n                    'United Kingdom': 'UK',\n                    'Florida': 'USA',\n                    'San Francisco':'USA',\n                    'Washington, D.C.': 'USA',\n                    'NYC':'USA',\n                    'Toronto':'Canada',\n                    'Chicago':'USA',\n                    'Seattle':'USA',\n                    'Sacramento, CA':'USA',\n                    'Atlanta, GA':'USA',\n                    'London, UK':'UK',\n                    'London, England': 'UK',\n                    'Everywhere': 'Worldwide',\n                    'Earth': 'Worldwide',\n                    'San Francisco, CA': 'USA',\n                    'Dallas, TX': 'USA',\n                    'New York City':'USA',\n                    'Texas':'USA',\n                    'Los Angeles':'USA',\n                    'Nashville, TN': 'USA',\n                    'US':'USA',\n                    'Denver, Colorado':'USA',\n                    'San Diego, CA': 'USA',\n                    'ss': 'Worldwide',\n                    'World':'Worldwide',\n                    '304':'Worldwide',\n                    'Manchester': 'UK',\n                    'Scotland':'UK',\n                    'Houston, TX': 'USA',\n                    'Tennessee':'USA',\n                    'Denver, CO': 'USA',\n                    'Memphis, TN': 'USA',\n                    'Planet Earth': 'Worldwide',\n                    'worldwide': 'Worldwide',\n                    'Pennsylvania, USA': 'USA',\n                    'Sydney': 'Australia',\n                    'Colorado': 'USA',\n                    'Portland, OR': 'USA',\n                    'Orlando, FL':'USA'\n}","eb438fbc":"train_set['location'] = train_set['location'].replace(location_mapping)\ntest_set['location'] = test_set['location'].replace(location_mapping)","cbab3b96":"#train_set['location'].value_counts().iloc[:20]\ntest_set['location'] = test_set['location'].fillna('Unknown_Location')\ntrain_set['location'] = train_set['location'].fillna('Unknown_Location')","264d5d81":"train_set","5f675f37":"def clean_text(data):\n    data['text_char_count'] = data['text'].apply(len)\n    data['text_word_count'] = data['text'].apply(lambda x: len(x.split()))\n    data['text_density'] = data['text_char_count'] \/ (data['text_word_count']+1)\n    data['text_punctuation_count'] = data['text'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation)))\n    data['title_word_count'] = data['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n    data['text_upper_case'] = data['text'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n    data['text'] = data['text'].apply(lambda x: x.translate(str.maketrans('', '', punctuation)))\n    data['text'] = data['text'].str.lower()\n    return data","e3a9aae4":"train_set = clean_text(train_set)\ntest_set = clean_text(test_set)","4a05e4e8":"from wordcloud import WordCloud\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\nwordcloud1 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_set[train_set['target']==1]['text']))\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Disaster Tweets',fontsize=40);\n\nwordcloud2 = WordCloud( background_color='white',\n                        width=600,\n                        height=400).generate(\" \".join(train_set[train_set['target']==0]['text']))\nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Non Disaster Tweets',fontsize=40);","30ad1fc2":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\ntrain_set['text'] = train_set['text'].apply(lambda x: tokenizer.tokenize(x))\ntest_set['text'] = test_set['text'].apply(lambda x: tokenizer.tokenize(x))\ntrain_set['text'].head()","d8c00633":"def remove_stopwords(text):\n    \"\"\"\n    Removing stopwords belonging to english language\n    \n    \"\"\"\n    words = [w for w in text if w not in stopwords.words('english')]\n    return words","c756f921":"train_set['text'] = train_set['text'].apply(lambda x : remove_stopwords(x))\ntest_set['text'] = test_set['text'].apply(lambda x : remove_stopwords(x))","7bde9fb4":"train_set.head()","4f92cda8":"train_set","7046f206":"def combine_text(list_of_text):\n    '''Takes a list of text and combines them into one large chunk of text.'''\n    combined_text = ' '.join(list_of_text)\n    return combined_text\n\ntrain_set['text'] = train_set['text'].apply(lambda x : combine_text(x))\ntest_set['text'] = test_set['text'].apply(lambda x : combine_text(x))","f01a7b90":"text_train = list(train_set['text'].values)\ntext_test = list(test_set['text'].values)","13dc5da4":"count_vectorizer = CountVectorizer()\ntrain_cvectors = count_vectorizer.fit_transform(text_train)\ntest_cvectors = count_vectorizer.transform(text_test)","b3f67a2e":"tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2))\ntrain_tfidf = tfidf.fit_transform(text_train)\ntest_tfidf = tfidf.transform(text_test)","5aad642c":"g = sns.countplot(train_set['target'])\ng.set_title('Distribution of Target')\nfor p in g.patches:\n    x = p.get_bbox().get_points()[:,0]\n    y = p.get_bbox().get_points()[1,1]\n    g.annotate('{:.2g}%'.format(100.*y\/len(train_set['target'])), (x.mean(), y), ha='center', va='bottom');","73bf9a49":"logistic = LogisticRegression(random_state = RANDOM_STATE)\ncross_log = cross_val_score(logistic, train_cvectors, train_set['target'], scoring=\"f1\")","24ce80d9":"cross_log","87dc0018":"cross_log_tf = cross_val_score(logistic, train_tfidf, train_set['target'], scoring=\"f1\")","adb8fb3d":"cross_log_tf","c171c546":"multi_nb = MultinomialNB()\ncross_mnb = cross_val_score(multi_nb, train_cvectors, train_set['target'], scoring=\"f1\")","6a0b6297":"cross_mnb","50a18397":"cross_mnb_tf = cross_val_score(multi_nb, train_tfidf, train_set['target'], scoring=\"f1\")\ncross_mnb_tf","aac0121e":"random = RandomForestClassifier(random_state=RANDOM_STATE)\ncross_rfr = cross_val_score(random, train_cvectors, train_set['target'], scoring=\"f1\")","fd9e30c3":"cross_rfr","309fee74":"cross_rfr_tf = cross_val_score(random, train_tfidf, train_set['target'], scoring=\"f1\")\ncross_rfr_tf","fdb17e40":"vote = VotingClassifier(estimators=[('logistic_regression', logistic), ('naive_bayes', multi_nb), ('random_forest', random)], voting='hard')\ncross_vote = cross_val_score(vote, train_cvectors, train_set['target'], scoring=\"f1\")","fc9c80c1":"cross_vote","b8821368":"cross_vote_tf = cross_val_score(vote, train_tfidf, train_set['target'], scoring=\"f1\")\ncross_vote_tf","b6ecd996":"multi_nb.fit(train_cvectors, train_set['target'])\nplot_confusion_matrix(multi_nb, train_cvectors, train_set['target']);","305f30d4":"pred = multi_nb.predict(test_cvectors)","dc03f288":"result = pd.DataFrame({\n    'id': test_set['id'],\n    'target':pred\n})","94078143":"result.to_csv('result.csv', index=False)","038d7aa0":"# Model Evaluations\n\n## Logistic Regression with Count Vectorizer\n","59b9b6f2":"## Random Forest","e95484ce":"## Target","22f9bb22":"# Natural Language Processing with Disaster Tweets\n\nThe goal of this project is to introduce NLP","21e10a70":"## TFID Vectorizer","3fd71523":"## Voting","8b8e82c8":"## Location\nMapping is done for locations and missing values are replaced with `Unknown_Location`","5d1bafa5":"## Exploratory Data Analysis","a110ea5e":"### Removing Stop words","4c6e9895":"## Text Feature","7c96eda5":"## Logistic Regression with TFIDF","46e808cb":"## Count Vectorizer","d7f31bc0":"### Tokenize Text feature","92fa9b12":"## Naive Bayes","94cf9a47":"Based on the result of cross validation, Naive Bayes will be used","59aee018":"### Keyword feature\nThere 221 unique keywords in train and test dataset. For missing keyword is replaced with `Unknown_Keyword`"}}