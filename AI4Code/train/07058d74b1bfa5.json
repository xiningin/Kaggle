{"cell_type":{"3c56ef7f":"code","ca645ee8":"code","7c791a6f":"code","96471599":"code","fb9814f1":"code","3a316fa4":"code","5a84b6be":"code","fc51c8fc":"code","ae99f9e1":"code","7e711fcc":"code","0f71a832":"code","14d85f62":"code","25bb3896":"code","2976fdc6":"code","3875f989":"code","61064846":"code","2ed8a12f":"code","46664af5":"code","1febbd2c":"code","5d1e8be9":"code","ec913ede":"code","e4cac542":"code","b05705c0":"code","a7355b1b":"code","dce5cfc1":"code","efebc932":"code","e10ee3ea":"code","941ee17e":"code","a8da7211":"code","9158db25":"code","7849ea0d":"code","6e4278df":"markdown","60a5dbc5":"markdown"},"source":{"3c56ef7f":"import pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport numpy as np","ca645ee8":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7c791a6f":"import pandas as pd\nInput_path = '\/kaggle\/input\/emg-signal-for-gesture-recognition\/EMG-data.csv'\ndf = pd.read_csv(Input_path)\nprint(df.head())\nprint(df.shape)","96471599":"print(\"class :\", df[\"class\"].unique())\nprint()\n#print(\"Labels :\",df[\"label\"].unique()) # 36 people hand gesture data\n#print()\nprint(\"Value Count :\\n\",df[\"class\"].value_counts())","fb9814f1":"features = df.drop(columns=[\"label\",\"class\",\"time\"])\ndisplay(features.head())\n#print(features.shape())","3a316fa4":"Class = df[\"class\"]\nprint(Class.unique())\n#print(Class.shape())","5a84b6be":"print(type(Class))\nprint(type(features))\n\nClass = Class.values\nfeatures = features.values\n\nprint(type(Class))\nprint(type(features))","fc51c8fc":"# split in training 70%, validation 10 %,  test 20% test \nfrom sklearn.model_selection import train_test_split\n# 80 and 20\nx_train, x_test, y_train, y_test = train_test_split(features, Class, test_size=0.2, random_state=1)","ae99f9e1":"# Normalizing data\nmean = x_train.mean(axis=0)\nstd = x_train.std(axis=0)\n\nx_train -= mean\nx_train \/= std\n\nx_test -= mean\nx_test \/= std","7e711fcc":"# one hot encoding Labels\ny_train = tf.keras.utils.to_categorical(y_train)\ny_test = tf.keras.utils.to_categorical(y_test)","0f71a832":"# creating a function for plotting\n\ndef plot(loss,val_loss,acc,val_acc):\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n\n    epochs = range(1, len(loss) + 1)\n\n    plt.plot(epochs, loss, 'bo', label='Training loss')\n    plt.plot(epochs, val_loss, 'b', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    plt.show()\n\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n\n    epochs = range(1, len(acc) + 1)\n\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc, 'b', label='Validation acc')\n    plt.title('Training and validation acc')\n    plt.xlabel('Epochs')\n    plt.ylabel('acc')\n    plt.legend()\n    plt.show()","14d85f62":"#################################","25bb3896":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model\n\ninput_tensor = Input(shape=(8,))\nx = layers.Dense(1024, activation='relu')(input_tensor)\ny = layers.Dense(512, activation='relu')(x)\nz = layers.Dense(256, activation='relu')(y)\nz = layers.Dense(128, activation='relu')(z)\nz = layers.Dense(64, activation='relu')(z)\nz = layers.Dense(32, activation='relu')(z)\nz = layers.Dense(128, activation='relu')(y) # acyclic graghs of layers\nz = layers.Dense(64, activation='relu')(z)\nz = layers.Dense(32, activation='relu')(z)\noutput_tensor = layers.Dense(8, activation='softmax')(z)\n\nmodel = Model(input_tensor, output_tensor)\n\n#SGD #RMSprop #Adam #Adadelta #Adagrad ##Adamax ###Nadam #Ftrl\nopt = optimizers.Nadam(lr=1e-3)\nmodel.compile(optimizer = opt, \n              loss = \"categorical_crossentropy\",\n              metrics = [\"accuracy\"])\n\n#model.summary()","2976fdc6":"# saving model, creating log for tensorboaed and applying few callbacks\n\ndef callbacks(Log,Dir):\n  import tensorflow as tf\n  import os\n\n  Filepath = Path\n  logdir = os.path.join(Filepath, Dir)\n  \n  callbacks_list = [tf.keras.callbacks.TensorBoard(\n                    log_dir=logdir,                 #  tensorboard log path      \n                    histogram_freq=1,),\n                    tf.keras.callbacks.EarlyStopping(   # stop if not improving\n                    monitor='val_accuracy',patience=2,),           # monitor validation accuracy\n                    #tf.keras.callbacks.ReduceLROnPlateau(\n                    #monitor='val_loss',factor=0.1,         # lr ko .1 se multiply kerdo (kam kerdo)\n                    #patience=10,),                # reduce the lrate if val loss stop improving\n                    tf.keras.callbacks.ModelCheckpoint(\n                    filepath= Filepath,             # save model path\n                    monitor='val_loss',             # only save best weights\n                    save_best_only=True,)]\n  return callbacks_list","3875f989":"Path = \"model1\"\nDir = \"my_log_dir\"   \nCall_B_Fun = callbacks(Path,Dir)\n\nbatch_size = 512            \nepochs = 200                \n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size, epochs = epochs,\n                    validation_split = 0.2, callbacks=Call_B_Fun)\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplot(loss,val_loss,acc,val_acc)","61064846":"# saving our model\nmodel.save('model1\/emg_1.h5')","2ed8a12f":"#################","46664af5":"# loading saved model\nfrom tensorflow.keras.models import load_model\nemg = 'model1\/emg_1.h5'\nemg_model = load_model(emg)\n#emg_model.summary()","1febbd2c":"# Removing layers\n\nnew_model = Model(emg_model.inputs, emg_model.layers[-2].output) # removing layers\nnew_model.summary()\n# removed all layers except conv","5d1e8be9":"from tensorflow.keras import layers, optimizers, Input, Model\n\ninput_tensor = Input(shape=(8,))\nx = new_model(input_tensor)       # this is our old model\n#z = layers.Dense(256, activation='relu')(x)\noutput_tensor = layers.Dense(8, activation='softmax')(x)\n\nmodel = Model(input_tensor, output_tensor)\n\n#SGD #RMSprop #Adam #Adadelta #Adagrad ##Adamax ###Nadam #Ftrl\nopt = optimizers.Nadam(lr=1e-3)\nmodel.compile(optimizer = opt, \n              loss = \"categorical_crossentropy\",\n              metrics = [\"accuracy\"])\n\nmodel.summary()","ec913ede":"def callbacks(Log,Dir):\n  import tensorflow as tf\n  import os\n\n  Filepath = Path\n  logdir = os.path.join(Filepath, Dir)\n  \n  callbacks_list = [tf.keras.callbacks.TensorBoard(\n                    log_dir=logdir,                 #  tensorboard log path      \n                    histogram_freq=1,),\n                    tf.keras.callbacks.EarlyStopping(   # stop if not improving\n                    monitor='val_loss',patience=2,),           # monitor validation loss\n                    tf.keras.callbacks.ReduceLROnPlateau(\n                    monitor='val_loss',factor=0.1,         # lr ko .1 se multiply kerdo (kam kerdo)\n                    patience=10,),                # reduce the lrate if val loss stop improving\n                    tf.keras.callbacks.ModelCheckpoint(\n                    filepath= Filepath,             # save model path\n                    monitor='val_loss',             # only save best weights\n                    save_best_only=True,)]\n  return callbacks_list","e4cac542":"Path = \"model2\"\nDir = \"my_log_dir\"   \nCall_B_Fun = callbacks(Path,Dir)\n\nbatch_size = 512            \nepochs = 200                \n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size, epochs = epochs,\n                    validation_split = 0.2, callbacks=Call_B_Fun)\n\nloss = history.history['loss']\nval_loss = history.history['val_loss']\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplot(loss,val_loss,acc,val_acc)","b05705c0":"# saving our model\nmodel.save('model2\/emg_2.h5')","a7355b1b":"###################","dce5cfc1":"# loading saved model\nfrom tensorflow.keras.models import load_model\nemg = 'model2\/emg_2.h5'\nemg_model = load_model(emg)\n#emg_model.summary()","efebc932":"from tensorflow.keras import layers, Sequential, optimizers, Input, Model\n\ninput_tensor = Input(shape=(8,))\nx = layers.Dense(1024, activation='relu')(input_tensor)\ny = layers.Dense(512, activation='relu')(x)\nz = layers.Dense(256, activation='relu')(y)\nz = layers.Dense(128, activation='relu')(z)\nz = layers.Dense(64, activation='relu')(z)\nz = layers.Dense(32, activation='relu')(z)\nz = layers.Dense(128, activation='relu')(y) # acyclic graghs of layers\nz = layers.Dense(64, activation='relu')(z)\nz = layers.Dense(32, activation='relu')(z)\noutput_tensor = layers.Dense(8, activation='softmax')(z)\n\nmodel = Model(input_tensor, output_tensor)\n\nopt = optimizers.Nadam(lr=1e-3)\nmodel.compile(optimizer = opt, \n              loss = \"categorical_crossentropy\",\n              metrics = [\"accuracy\"])\n\nmodel.set_weights(emg_model.get_weights())   # using pretrained model weights\n\n#model.summary()","e10ee3ea":"callbacks_list = [tf.keras.callbacks.EarlyStopping(   # stop if not improving\n                monitor='acc',patience=5,),           # monitor validation accuracy\n                tf.keras.callbacks.ModelCheckpoint(\n                filepath='my_model.h5',\n                monitor='val_loss',                   # only save best weights\n                save_best_only=True,)]                # when vall loss is improved\n\nbatch_size = 512           \nepochs = 15                \n\nhistory = model.fit(x_train, y_train,\n                    batch_size=batch_size, epochs = epochs,\n                    validation_split = 0.2,\n                    callbacks=callbacks_list)","941ee17e":"model.save('emg_3.h5')","a8da7211":"loss = history.history['loss']\nval_loss = history.history['val_loss']\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nplot(loss,val_loss,acc,val_acc)","9158db25":"evaluation = model.evaluate(x_test,  y_test,batch_size=batch_size, verbose=2)\nprint()\nprint(\"Test loss :\",evaluation[0]*100,\"%\")\nprint(\"Test accuracy :\",evaluation[1]*100,\"%\")","7849ea0d":"predict = 105\na = np.argmax(model.predict(x_test)[predict])\nprint(\"Predicted Class: \",a)\nprint(\"Actual Class: \",np.argmax(y_test[predict]))","6e4278df":"### Prediction","60a5dbc5":"# EMG Signal for gesture recognition"}}