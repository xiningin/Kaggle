{"cell_type":{"a0cabd2b":"code","72acf5bd":"code","723f632d":"code","68f95488":"code","c52bc64c":"code","ea2738ef":"code","23dc1d2c":"code","06caf818":"code","0f65fce2":"code","9142f912":"code","2807f144":"code","eb41acf9":"code","87de715d":"code","493eafbd":"markdown","4db06695":"markdown","d96ae360":"markdown","0e750fa2":"markdown","5fb7cc33":"markdown","b0be32cd":"markdown","7b18767d":"markdown"},"source":{"a0cabd2b":"import numpy as np # linear algebra\nfrom numpy.random import seed\nseed(1)\n\nimport matplotlib.pyplot as plt\nimport matplotlib.font_manager as mfm\nimport seaborn as sns\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom itertools import product\nfrom tqdm import tqdm\nfrom IPython.display import clear_output\n\nimport keras.losses\nfrom keras.layers import Dense, InputLayer, Dropout\nfrom keras.models import Sequential\nfrom keras.callbacks import EarlyStopping\nfrom keras import regularizers\nfrom keras import metrics\n\nfrom sklearn.tree import DecisionTreeClassifier\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","72acf5bd":"original_df = pd.read_csv(\"..\/input\/chinese-mnist-digit-recognizer\/chineseMNIST.csv\")\noriginal_df = original_df.replace({\"label\": {\n    1000: \"1e3\",\n    10000: \"1e4\",\n    100000000: \"1e8\"\n}})\noriginal_df.label = original_df.label.astype(\"str\")","723f632d":"df = pd.DataFrame(original_df)\ndf.head()","68f95488":"labels = df[\"label\"].sort_values().unique()\n    \n# Map large values (e.g. 1000000) to smaller values (e.g. 13)\nlabel_map = {}\nnum_classes = len(labels)\nfor i in range(num_classes):\n    label_map[labels[i]] = i\nprint(label_map)\ndf = df.replace({\"label\": label_map})\nprint(df.label.unique())\n\n# Get hanzi map (from small labels)\nhanzi_map = {}\nfor label in df.label.unique():\n    hanzi_map[label] = df.character[df.label == label].iloc[0]\nprint(hanzi_map)\n\ndef origLabelFromLabel(label):\n    orig_label = None \n    for k in label_map.keys():\n        if label_map[k] == label:\n            return k\n    return None","c52bc64c":"# Normalize input\ndef normalize(x):\n    min_val = np.min(x)\n    max_val = np.max(x)\n    \n    return (x - min_val)\/(max_val - min_val)\n\ndef convert_to_onehot(Y, numclass):\n    Yr = Y.ravel()\n    rows = Yr.shape[0]\n    new_Y = np.zeros((rows, numclass))\n    \n    for i in range(rows):\n        new_Y[i, Yr[i]] = 1\n    \n    return new_Y","ea2738ef":"# Shuffle the dataset\ndf = df.sample(frac=1).reset_index(drop=True)\n\n# Split into 3 sets\ntraining_percent = 0.9\ntotal_examples = df.shape[0]\n\nbreakpoint = int(training_percent * total_examples)\n\ntrain_df = df.iloc[None:breakpoint, :]\ntest_df = df.iloc[breakpoint:None, :]\n\n# Confirm if shapes are correct\nfor z in zip([\"train_df\", \"test_df\"], [train_df, test_df]):\n    print(f\"Shape of {z[0]}:\", z[1].shape)","23dc1d2c":"def dataframeToNumpy(df):\n    Xdf = df.iloc[:, None:-2]\n    ydf = df.iloc[:, -2]\n    return (\n        normalize(np.array(Xdf)), \n        convert_to_onehot(np.array(ydf), num_classes), \n    )\n\nXtrain, ytrain = dataframeToNumpy(train_df)\nXtest, ytest = dataframeToNumpy(test_df)\n\n# Confirm if shapes are correct\nnames = []\nfor p in product([\"train\", \"test\"], [\"X\", \"y\"]):\n    names.append(\"\".join([p[1], p[0]]))\nfor name in names:\n    vec = eval(name)\n    print(f\"Shape of {name}:\", vec.shape)","06caf818":"# Check for one-hot conversion error\nnp.sum(np.array(train_df.iloc[:, -2]) - np.array([yi.argmax() for yi in ytrain]))","0f65fce2":"img_dim = int(np.sqrt(Xtrain.shape[1]))\n\ndef draw100Examples(X, y, z=None):\n    # Create 100 random indices\n    randIdx = np.random.randint(0, X.shape[0], 100).reshape(10, 10)\n    \n    if z is not None:\n        figsize = (25, 15)\n        hspace = 0.7\n        title_fmt = \"Label: %s\\nPred: %s\"\n    else:\n        figsize = (15, 15)\n        hspace = 0.5\n        title_fmt = \"Label: %s%s\"\n        predicted_label = \"\"\n        \n    fig, ax = plt.subplots(10, 10, figsize=(15, 15))\n\n    for i in range(randIdx.shape[0]):\n        for j in range(randIdx.shape[1]):\n            example = X[randIdx[i, j], :]\n            example = example.reshape((img_dim, img_dim))\n            label = y[randIdx[i, j]].argmax()\n            orig_label = origLabelFromLabel(label)\n            \n            if z is not None:\n                lbl = z[randIdx[i, j]].argmax()\n                predicted_label = origLabelFromLabel(lbl)\n            \n            ax[i, j].set_title(title_fmt % (orig_label, predicted_label))\n            \n            ax[i, j].imshow(example)\n            ax[i, j].set_xticks([])\n            ax[i, j].set_yticks([])\n    \n    plt.subplots_adjust(hspace=hspace)\n    plt.show()\n\ndraw100Examples(Xtrain, ytrain)","9142f912":"class PlotLosses(keras.callbacks.Callback):\n    def __init__(self, epochs, loss_ylim):\n        self.epochs = epochs\n        self.loss_ylim = loss_ylim\n    \n    def on_train_begin(self, logs={}):\n        self.i = 0\n        self.x = []\n        self.losses = []\n        self.val_losses = []\n        \n        self.accu = []\n        self.val_accu = []\n        \n        self.fig = plt.figure()\n        \n        self.logs = []\n\n    def on_epoch_end(self, epoch, logs={}):\n        self.logs.append(logs)\n        self.x.append(self.i)\n        self.losses.append(logs.get('loss'))\n        self.accu.append(logs.get('accuracy'))\n        self.val_accu.append(logs.get('val_accuracy'))\n        self.val_losses.append(logs.get('val_loss'))\n        self.i += 1\n        \n        if (self.i < self.epochs\/2):  \n            self.xlim = (0, int(self.epochs\/2))\n        else:\n            self.xlim = (0, int(self.epochs + 1))\n\n        \n        clear_output(wait=True)\n        fig, (ax1, ax2) = plt.subplots(nrows=1, ncols=2, figsize=(15, 6))\n        \n        ax1.plot(self.x, self.losses, label=\"loss\")\n        ax1.plot(self.x, self.val_losses, label=\"val_loss\", color=\"r\")\n        ax1.set_xlim(self.xlim)\n        ax1.set_ylim(self.loss_ylim)\n        \n        ax2.plot(self.x, self.accu, label=\"accu\")\n        ax2.plot(self.x, self.val_accu, label=\"val_accu\", color=\"r\")\n        ax2.set_xlim(self.xlim)\n        ax2.set_ylim(0, 1)\n        \n        print(\"Epoch: %d; Training loss: %f; Validation loss: %f\" % \n              (self.i, self.losses[-1], self.val_losses[-1]))\n        print(\"Training accu: %f, Validation accu: %f\" % (self.accu[-1], self.val_accu[-1]))\n        ax1.legend();\n        ax2.legend();\n        plt.show();\nprint(\"Done\")","2807f144":"EPOCHS = 500\n\n# Train neural network\nmodel = Sequential()\nmodel.add(InputLayer(input_shape=(img_dim**2,)))\nmodel.add(Dropout(0.4, input_shape=(img_dim**2,)))\n\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dropout(0.3, input_shape=(256,)))\n\nmodel.add(Dense(512, activation='relu'))\nmodel.add(Dropout(0.4, input_shape=(512,)))\n\nmodel.add(Dense(num_classes, activation='softmax'))\n\nplot_losses = PlotLosses(EPOCHS, (0, 3))\nearlystop = EarlyStopping(\n    monitor=\"val_loss\",\n    min_delta=1e-4,\n    mode=\"min\",\n    patience=4,\n)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy, \n              optimizer=keras.optimizers.Adam(1e-4),\n              metrics=['accuracy'])\n\nprint(\"Begin training\")\nhist = model.fit(Xtrain, \n                 ytrain,\n                 epochs=EPOCHS,\n                 batch_size=10,\n                 verbose=0,\n                 validation_split=0.2, \n                 callbacks=[plot_losses, earlystop])\nprint(\"Training completed\")","eb41acf9":"model.evaluate(Xtest, ytest)","87de715d":"ypredict = model.predict(Xtest)\ndraw100Examples(Xtest, ytest, ypredict)","493eafbd":"## Split data into training set and test set","4db06695":"# 3. Train a neural network to recognize characters","d96ae360":"# 1. Extract features from CSV file\nNaming convention:\n\n$X = \\text{data matrix}$<br>\n$y = \\text{digit label vector}$<br>\n","0e750fa2":"# 5. Show random test images","5fb7cc33":"## Feature engineering","b0be32cd":"# 2. Show 100 training examples","7b18767d":"## Create numpy arrays from above dataframes"}}