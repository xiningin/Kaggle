{"cell_type":{"a0a30e6a":"code","1bbc8af9":"code","b9958ece":"code","63a1a851":"code","74c6af6f":"code","b2f63cff":"code","c1aa7fd5":"code","1a3622f9":"code","27c15bce":"code","fd8dc6b4":"code","43bf840c":"code","2b3241d2":"code","6f91f895":"code","20503449":"code","7b548855":"code","7197bc59":"code","f339c930":"code","0ac57858":"code","ecca9436":"code","6a029481":"code","fb7ad1df":"code","2f050613":"code","2e33b22d":"code","e2b2f096":"code","814f754b":"code","dfc2abb7":"code","03e6867d":"markdown","c26277dd":"markdown","0bfbaf15":"markdown","02c9760e":"markdown","f73a6520":"markdown","fc8cac2d":"markdown","0a5075f0":"markdown","04f7a68d":"markdown","5c83b03a":"markdown","f2950d2c":"markdown","46cad607":"markdown","ed81b3f0":"markdown","ecb2fc92":"markdown","ae6dea62":"markdown","b592d2e7":"markdown"},"source":{"a0a30e6a":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport json\nfrom sqlite3 import dbapi2 as sq3\nfrom pathlib import Path\nfrom collections import OrderedDict\n\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Activation\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.layers import Input, Embedding, Add, Dot, Flatten,Concatenate, Dense\nfrom tensorflow.keras import Model\nfrom tensorflow.keras.regularizers import l2\nfrom tensorflow.keras.optimizers import Adam, SGD\nfrom tensorflow.keras.utils import plot_model\nfrom sklearn.metrics import mean_squared_error as mse\n\n\nfrom time import time\nfrom IPython.display import clear_output","1bbc8af9":"# Connect to sqlite db\ndb = sq3.connect('..\/input\/yelp-project\/yelp_database.db')","b9958ece":"# Functions to work with SQLite db\ndef make_query(sel):\n    c = db.cursor().execute(sel)\n    return c.fetchall()\n\ndef make_frame(list_of_tuples, legend):\n    framelist=[]\n    for i, cname in enumerate(legend):\n        framelist.append((cname,[e[i] for e in list_of_tuples]))\n    return pd.DataFrame.from_dict(OrderedDict(framelist))","63a1a851":"%%time\n\n#Getting data to feed to model\nq = make_query(\"SELECT user_id, business_id, stars FROM reviews\")\nratings_df = make_frame(q, legend=['user_id', 'business_id', 'stars'])\ndel q\nratings_df.head()","74c6af6f":"ratings_df.shape","b2f63cff":"#Train - Val - Test\n#80% - 10% - 10%\nX_train, X, y_train, y = train_test_split(ratings_df.drop('stars', axis=1), ratings_df.stars, train_size=.8)\nX_test, X_val, y_test, y_val = train_test_split(X, y, train_size=.5)\ndel X, y\n\nprint(f\"Train Size: {round(X_train.shape[0]\/ratings_df.shape[0]*100)}%\")\nprint(\"X train shape: \", X_train.shape)\nprint(\"y train shape: \", y_train.shape)\n\nprint(f\"\\nValidation Size: {round(X_val.shape[0]\/ratings_df.shape[0]*100)}%\")\nprint(\"X val   shape: \", X_val.shape)\nprint(\"y val   shape: \", y_val.shape)\n\nprint(f\"\\nTest Size: {round(X_test.shape[0]\/ratings_df.shape[0]*100)}%\")\nprint(\"X test  shape: \", X_test.shape)\nprint(\"y test  shape: \", y_test.shape)","c1aa7fd5":"# Helper functions \ndef create_bias(name, inp, n_in, reg):\n    #x = Embedding(n_in, 1, input_length=1, embeddings_regularizer=l2(reg))(inp)\n    x = Embedding(n_in, 1, input_length=1, name=name)(inp)\n    return Flatten(name=name+'_flat')(x)\ndef embedding_input(name, n_in, n_out, reg):\n    inp = Input(shape=(1,), dtype='int64', name=name)\n    return inp, Embedding(n_in, n_out, input_length=1, name=name.split('_')[0]+'_factor', embeddings_regularizer=l2(reg))(inp)\n\ndef sigmoid_maker(low, high):\n    def custom_sigmoid(x):\n        return K.sigmoid(x)*(high - low) + low #within range\n    return custom_sigmoid","1a3622f9":"# Getting total unique restaurants and user in data\nnrestaurants = make_query(\"SELECT COUNT(*) FROM businesses\")[0][0]\nnusers = make_query(\"SELECT COUNT(*) FROM users\")[0][0]\nprint(\"Number of Restaurants:\", nrestaurants)\nprint(\"Number of Users:\", nusers)","27c15bce":"#Baseline accuracy\nmean_rating = y_train.mean()\n\ntrain_baseline = mse(y_train, [mean_rating]*y_train.shape[0])\nval_baseline = mse(y_val, [mean_rating]*y_val.shape[0])\ntest_baseline = mse(y_test, [mean_rating]*y_test.shape[0])\n\n\nprint(f\"\"\"Baseline MSE using mean rating:\\n\n          Train Data: {train_baseline:.4f},\n          Val   Data: {val_baseline:.4f},\n          Test  Data: {test_baseline:.4f}\"\"\")","fd8dc6b4":"L = 50\nREG = 1e-2\n\nuser_input, uLmat = embedding_input('user_input', nusers, L, REG)\nrestaurant_input, mLmat = embedding_input('restaurant_input', nrestaurants, L, REG)\nuser_bias = create_bias('user_bias', user_input, nusers, REG)\nrestaurant_bias = create_bias('restaurant_bias', restaurant_input, nrestaurants, REG)\n\nresidual = Dot(axes=2, name=\"residual\")([uLmat, mLmat])\nresidflat = Flatten(name=\"residual_flat\")(residual)\nregression = Add(name=\"regression\")([user_bias, restaurant_bias, residflat])\n\ncs = sigmoid_maker(0, 5.5)\noutput = Activation(cs, name=\"Sigmoid_Range\")(regression)\nmodel = Model([user_input, restaurant_input], output)\nmodel.compile(Adam(.01), loss='mse')\n\nmodel.summary()","43bf840c":"plot_model(model)","2b3241d2":"#Training Model\nmodel.optimizer.learning_rate = 5e-2\nh1 = model.fit([X_train.user_id, X_train.business_id], y_train, epochs=2, \n              validation_data=([X_val.user_id, X_val.business_id], y_val), batch_size=1024, verbose=1)\n\n# model.optimizer.learning_rate = 5e-3\n# h2 = model.fit([X_train.user_id, X_train.business_id], y_train, epochs=1, \n#               validation_data=([X_val.user_id, X_val.business_id], y_val), batch_size=512, verbose=2)","6f91f895":"# Plotting Train and Val loss\n\nloss = h1.history['loss']#+h2.history['loss']\nval_loss = h1.history['val_loss']#+h2.history['val_loss']\nplt.plot(loss, label='loss')\nplt.plot(val_loss, label='val_loss')\nplt.title('Loss Plot')","20503449":"#Saving model\nmodel.save('Linear_Model')","7b548855":"L = 50\nREG = 1e-2","7197bc59":"#Dense Layer Params\nhidden_layers = 4\nactivation = 'relu'\nn_neurons = 32\ndense_reg = 1e-2\n\n\nuser_input, uLmat = embedding_input('user_input', nusers, L, REG)\nrestaurant_input, mLmat = embedding_input('restaurant_input', nrestaurants, L, REG)\nuser_bias = create_bias('user_bias', user_input, nusers, REG)\nrestaurant_bias = create_bias('restaurant_bias', restaurant_input, nrestaurants, REG)\n\nx = Concatenate()([Flatten()(uLmat), Flatten()(mLmat)])\nfor _ in range(hidden_layers):\n    x = Dense(n_neurons, activation=activation, kernel_regularizer=l2(dense_reg))(x)\nx = Add(name=\"regression\")([user_bias, restaurant_bias, x])\noutput = Dense(1, activation=sigmoid_maker(0.5, 5.5), name=\"Sigmoid_Range\")(x)\n\nmodel2 = Model([user_input, restaurant_input], output)\nmodel2.compile(Adam(1e-2), loss='mse')\n\nmodel2.summary()","f339c930":"plot_model(model2)","0ac57858":"#training model\nmodel2.optimizer.learning_rate = 5e-2\nh1 = model2.fit([X_train.user_id, X_train.business_id], y_train, epochs=2, \n              validation_data=([X_val.user_id, X_val.business_id], y_val), batch_size=1024)\n\n# model2.optimizer.learning_rate = 5e-3\n# h2 = model2.fit([X_train.user_id, X_train.business_id], y_train, epochs=2, \n#               validation_data=([X_val.user_id, X_val.business_id], y_val), batch_size=512)\n\n# model2.optimizer.learning_rate = 1e-4\n# h3 = model2.fit([X_train.user_id, X_train.business_id], y_train, epochs=2, \n#               validation_data=([X_val.user_id, X_val.business_id], y_val), batch_size=2048)","ecca9436":"loss = h1.history['loss']#+h2.history['loss']+h3.history['loss']\nval_loss = h1.history['val_loss']#+h2.history['val_loss']+h3.history['val_loss']\nplt.plot(loss, label='loss')\nplt.plot(val_loss, label='val_loss')\nplt.title('loss plot')","6a029481":"model2.save('Nonlinear_Model')","fb7ad1df":"# Getting all the necessary Data\ntable = make_query(\"SELECT user_id, business_id, stars FROM reviews\")\nreviews_df = make_frame(table, ['user_id', 'business_id', 'rating'])\n\ntable = make_query(\"SELECT business_id, name, address, city, state, postal_code, latitude, longitude, stars\\\n                   FROM businesses\")\nbusiness_df = make_frame(table, \"business_id, name, address, city, state, postal_code, latitude, longitude, stars\".split(', '))\n\ndel table","2f050613":"# This cell has candidate generation function\n\ndef generate_candidates(user_id, friend=False):\n    \"\"\"\n    Returns list of business id's which could be good prospects for a particular user.\n    Factors in location and user's friends' choices\n    \n    Parameters:\n    user_id : id of user\n    friend : Recursive call to get user's friends. Internal calls only\n    \n    Return:\n    lst : list of restaurant ids. Is not empty\n    \"\"\"\n    lst = []\n    top_candidates_by_city = 20\n    top_candidates_by_frnd = 10\n\n    # By Location\n    #all the restaurants user has been to previously\n    user_history = reviews_df.query(f'user_id=={user_id}')\n    if user_history.shape[0]!=0:\n        # Details about those restaurants\n        visit_history = business_df.query(f'business_id in {list(user_history.business_id)}')\n        # Implying city user lives in by looking at most frequent city in history\n        #This will allow our recommendation to personalize recommendations for user\n        city_mode = visit_history.city.mode()\n        \n        if not friend:\n            for city in city_mode:\n                city_df = business_df.query(f'city == \"{city}\"').sort_values(by='stars')[:400]\n                lst.extend(city_df.business_id[:top_candidates_by_city].values)\n        \n        #If recursive call return restuarants rated highly by friend\n        else:\n            frnd_favourite =  visit_history.sort_values(by='stars', ascending=False).business_id.values[:top_candidates_by_frnd]\n            lst.extend(frnd_favourite)\n            return lst\n     \n    # Restaurants liked By Friends\n    frnd_lst = eval(make_query(f\"SELECT friends FROM users WHERE user_id={user_id}\")[0][0])\n\n    for frnd in frnd_lst:\n        # Recursive Call\n        # returns restuarnts friend has been to and has rated highly\n        frnd_recom = generate_candidates(frnd, friend=True)\n        lst.extend(frnd_recom)\n        \n    #If list is not empty\n    if lst:\n        #Return unique entries\n        return list(set(lst))\n    \n    # List is empty Hence user is a loner.\n    return loner_user()\n\ndef loner_user():\n    \"\"\"\n    A user is a loner if he has no friends and has not rated any restaurants. For such users we'll \n    generate candidates based on restaurant base ratings (restaurant biases). \n    This could be a new user on site who hasn't interacted much. \n    \n    Return:\n    lst : list of restaurant ids. Is not empty\n    \"\"\"\n    return_top = 20\n    restaurant_bias = model2.get_layer('restaurant_bias').weights[0].numpy()\n    bias_df = pd.DataFrame(restaurant_bias, columns=['restaurant_bias'])\n    bias_df.index.name='business_id'\n    bias_df = bias_df.sort_values(by='restaurant_bias', ascending=False)\n\n    return bias_df.index[:return_top].tolist()\n\n#Improvements \n#Candidate Generation can be improved by adding explicit args for city and restaurant category type\n\ngenerate_candidates(user_id=4)","2e33b22d":"# Wrapping our model to simplify use\ndef recommend(user_id, top=10):\n    \"\"\"\n    Uses Recommendation system to recommend restaurant to user.\n    \n    Parameters:\n    user_id : id of user\n    top : Number of recommendations [10 by default]\n    \n    Returns:\n    id_lst : List of business ids. List size = top \n    pred_ratings : List of ratings for corresponding business id. List size = top\n    \"\"\"\n    \n    label = f\"predicted_rating_user_id_{user_id}\"\n    candidates = generate_candidates(user_id=user_id)\n    pred_ratings = model2.predict([np.array([user_id]*len(candidates)), np.array(candidates)])\n    pred_df = pd.DataFrame({'business_id':candidates, label:pred_ratings.ravel()})\n    df = business_df.query(f'business_id in {candidates}')\n    return df.merge(pred_df, on='business_id').sort_values(by=label, ascending=False).head(top)","e2b2f096":"recommend(user_id=999, top=5)","814f754b":"recommend(user_id=19, top=5)","dfc2abb7":"#Calculating MSE \nticks = [\"Train\", \"Val\", \"Test\"]\navg_model = [train_baseline, val_baseline, test_baseline]\nlinear_model, non_linear_model = [], []\nfor data in [(X_train, y_train), (X_val, y_val), (X_test, y_test)]:\n    lin_mse = mse(data[1], model2.predict([data[0]['user_id'],data[0]['business_id']], verbose=1))\n    linear_model.append(lin_mse)\n    \n    nonlin_mse = mse(data[1], model2.predict([data[0]['user_id'],data[0]['business_id']], verbose=1))\n    non_linear_model.append(nonlin_mse)\n    \n    clear_output()\n    \n#Plotting Modelwise Error\nplt.figure(figsize=(8,8))\nax = plt.gca()\n\nax.plot(avg_model, \"o-\", label=\"Average Model\")\nax.plot(linear_model, \"o-\", label=\"Linear Model\")\nax.plot(non_linear_model, \"o-\", label=\"Non-Linear Model\")\n\nax.set(xlabel='', ylabel='MSE', xticks=[0,1,2], xticklabels=[\"Train Error\", \"Val Error\", \"Test Error\"])\nax.legend()","03e6867d":"## Improvements\n- Model performance can be improved by further training.\n- We can try Denser layers for non-linear combination of embeddings.","c26277dd":"Our recommendation system is very large in size we won't put all restaurants at once for one user, this would result in slow recommendation. \n\nWe use candidate generation, which looks at a user's history infers her location, finds best restaurants in her location and also looks at her friends' favourite restaurants. These restaurants are now candidates that user might like.\n\nThese candidates now go through our model which ranks them accordiing to predicted rating.","0bfbaf15":"Splitting Data","02c9760e":"## Conclusion\n- We made a recommendation system and improved on baseline metrics.\n- We recommended new restaurants to users.","f73a6520":"#### Links\n[1. Getting Data Ready](https:\/\/www.kaggle.com\/yashrajwani\/yelp-getting-data-ready)\n\n[2. Exploratory Data Analysis (EDA)](https:\/\/www.kaggle.com\/yashrajwani\/yelp-exploratory-data-analysis)\n\n","fc8cac2d":"### Recommendation using Collaborative Filtering with Matrix Factorization","0a5075f0":"### Importing Libraries","04f7a68d":"### Average Baseline Accuracy\nAverage model always predict average of all the ratings. This gives us a number which we'll try to improve on.","5c83b03a":"### Linear Recommendation System\nHere we use dot product of embeddings and add user and restaurant bias before putting through sigmoid.","f2950d2c":"### Comparing Models\nWe'll compare performance of all 3 models : Average, Linear, and Non-linear","46cad607":"## Aim\nAfter getting a sense of data from EDA, now we will now write our recommendation system. We will use Collaborative Filtering to recommend. We create embeddings for every user and every restaurant, every number in a L-dimensional embedding describes some property of user or restaurant. \nTo learn these embeddings we use a neural network which minimizes MSE on train set. Train set consists of user id and business id as input variables and rating given as target variable. We'll also write another and compare performance.\n","ed81b3f0":"### Deploying Non-Linear Recommendation System \nHere we'll write a function that takes user_id as input and returns personalized recommendations, just like actual websites.","ecb2fc92":"### Non-Linear Recommendation System \nHere we use non-linear combination of embeddings by stacking them together and passing through FFNN  and then add user and restaurant bias before putting through sigmoid.","ae6dea62":"# Restaurant Recommendation System","b592d2e7":"## 3. Modelling\n"}}