{"cell_type":{"22add07d":"code","f8b7a44c":"code","a8424279":"code","2be8b082":"code","16786445":"code","9f4f8bc4":"code","3f2d6790":"code","dc088e20":"code","8e2d99d1":"code","288d162e":"code","4e8f6e53":"code","3cb5b129":"code","839af8e4":"code","ad8d11af":"code","9deea393":"code","7ffe6011":"code","56b26309":"code","f4868133":"code","e590bfff":"code","713b9725":"code","08b72cd1":"code","826167bd":"code","a452caf5":"code","c7cc8b0f":"code","98a15d8a":"code","debb02df":"code","5ef1bc08":"code","2895c93d":"code","1d3ef008":"code","d9d9aa52":"code","93698cbd":"code","2d3469ac":"code","b4288181":"code","08ada8e5":"code","2f861a29":"code","70f23a3b":"code","ddae00a1":"code","6297d9b7":"code","fa587cd5":"code","ab22d567":"code","398558f3":"code","246adbbc":"code","80b54fdc":"code","5fe740a0":"markdown","00c1b4f6":"markdown","374679ce":"markdown","3d367795":"markdown","5b2d6f53":"markdown","f6cd689c":"markdown","9a45ff70":"markdown","36a0e97a":"markdown","9f8fd318":"markdown","b2e5a0ce":"markdown","4178129b":"markdown","2e3d895c":"markdown","1a210aea":"markdown","e6e8ccdb":"markdown","86d2da22":"markdown","ef23909a":"markdown","de22097b":"markdown","6845244a":"markdown","34818184":"markdown","cd18aa8a":"markdown","6022ce16":"markdown"},"source":{"22add07d":"import torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, models\nfrom torchvision.transforms import functional as F_T\nimport torch.nn.functional as F\nfrom typing import Iterable\nfrom dataclasses import dataclass\nimport matplotlib.pyplot as plt\nfrom torch.optim import lr_scheduler\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\n#import torch.nn.functional as F\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport time\nimport csv\n\nfrom PIL import Image\nfrom torch.utils.tensorboard import SummaryWriter\n\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"   \nos.environ[\"CUDA_VISIBLE_DEVICES\"]= '4'","f8b7a44c":"data_root = '\/synology_data\/Debasish\/Pytorch-OpenCV\/Project2\/'\nlabel_csv_path = os.path.join(data_root, 'train.csv')\n\ndf = pd.read_csv(label_csv_path)\n  \n\nmsk = np.random.rand(len(df)) < 0.8\ntrain = df[msk].reset_index(drop = True)\ntrain.to_csv(os.path.join(data_root, 'train_df.csv'),index = False)\n\nval = df[~msk].reset_index(drop = True)\nval.to_csv(os.path.join(data_root, 'test_df.csv'),index = False)\n\nprint(len(train['class'].unique()))\nprint(len(val['class'].unique()))","a8424279":"train.head()","2be8b082":"val.head()","16786445":"train['class'].unique()","9f4f8bc4":"class KenyanFood13Dataset(Dataset):\n    \"\"\"\n    This custom dataset class take root directory and train flag, \n    and return dataset training dataset id train flag is true \n    else is return validation dataset.\n    \"\"\"\n    \n    def __init__(self, data_root, filename, train=True, image_shape=None, transform=None):\n        \n        \"\"\"\n        init method of the class.\n        \n         Parameters:\n         \n         data_root (string): path of root directory.\n         \n         train (boolean): True for training dataset and False for test dataset.\n         \n         image_shape (int or tuple or list): [optional] int or tuple or list. Defaut is None. \n                                             It is not None image will resize to the given shape.\n                                 \n         transform (method): method that will take PIL image and transforms it.\n         \n        \"\"\"\n        \n        # get label to species mapping\n        label_csv_path = os.path.join(data_root, filename)\n        \n        self.label_df = pd.read_csv(label_csv_path, delimiter=' *, *', engine='python')\n        \n        \n        # set image_resize attribute\n        if image_shape is not None:\n            if isinstance(image_shape, int):\n                self.image_shape = (image_shape, image_shape)\n            \n            elif isinstance(image_shape, tuple) or isinstance(image_shape, list):\n                assert len(image_shape) == 1 or len(image_shape) == 2, 'Invalid image_shape tuple size'\n                if len(image_shape) == 1:\n                    self.image_shape = (image_shape[0], image_shape[0])\n                else:\n                    self.image_shape = image_shape\n            else:\n                raise NotImplementedError \n        else:\n            self.image_shape = image_shape\n            \n        # set transform attribute\n        self.transform = transform\n        \n        num_classes = 13\n        self.classes_list = ['githeri', 'ugali', 'kachumbari', 'matoke', 'mandazi', 'bhaji',\n       'sukumawiki', 'nyamachoma', 'chapati', 'kukuchoma', 'masalachips',\n       'pilau', 'mukimo']\n        \n\n\n        \n        # initialize the data dictionary\n        self.data_dict = {\n            'image_path': [],\n            'label': []\n        }\n        \n        # training data path, this will be used as data root if train = True\n        if train:\n            img_dir = os.path.join(data_root, 'images')\n            \n        # validation data path, this will be used as data root if train = False\n        else:\n            img_dir = os.path.join(data_root, 'images')\n                    \n        self.idx2class = {i: key for i, key in enumerate(self.classes_list)}\n\n        self.class_to_idx = {key: i for i, key in enumerate(self.classes_list)}\n        \n        with open('.\/labels.csv', 'w') as f:\n            for key in self.idx2class.keys():\n                f.write(\"%d,%s\\n\"%(key,self.idx2class[key]))\n        \n\n        for index, row in self.label_df.iterrows():\n            img_path = os.path.join(img_dir, '{}.jpg'.format(row[0]))\n            self.data_dict['image_path'].append(img_path)\n            self.data_dict['label'].append(self.class_to_idx[row[1]])#self.classes_list.index(row[1]))\n            #self.data_dict['is_train'].append(True)\n\n                    \n    def __len__(self):\n        \"\"\"\n        return length of the dataset\n        \"\"\"\n        return len(self.data_dict['label'])\n    \n    \n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n        \n        if self.image_shape is not None:\n            image = F_T.resize(image, self.image_shape)\n            \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        target = self.data_dict['label'][idx]\n        \n        return image, target            \n                \n        \n    def common_name(self, label):\n        \"\"\"\n        class label to common name mapping\n        \"\"\"\n        return self.idx2class[label]\n    \n\n        \n        \n        ","3f2d6790":"# # data root directory\n# data_root = '\/home\/debasish\/synology_data\/Debasish\/Pytorch-OpenCV\/Project2'\n# filename = 'train_df.csv'\n# train_dataset =  KenyanFood13Dataset(data_root, filename, train=True, image_shape=256)\n\n# print('Length of the dataset: {}'.format(len(train_dataset)))\n\n# img, trgt = train_dataset[300]\n\n# print('Label: {}, common name: {}'.format(trgt, train_dataset.common_name(trgt)))\n# plt.imshow(img)\n# plt.show()\n","dc088e20":"preprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor()\n    ])\n\nfilename = 'test_df.csv'\ntest_dataset =  KenyanFood13Dataset(data_root, filename, train=False, image_shape=None, transform=preprocess)\n\n# dataloader with dataset\ntest_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=15,\n        shuffle=True,\n        num_workers=2\n    )\n\n\n# Plot few images\nplt.rcParams[\"figure.figsize\"] = (15, 9)\nplt.figure\nfor images, labels in test_loader:\n    for i in range(len(labels)):\n        plt.subplot(3, 5, i+1)\n        img = F_T.to_pil_image(images[i])\n        plt.imshow(img)\n        plt.gca().set_title('Target: {0}'.format(test_dataset.idx2class[labels[i].item()]))\n    plt.show()\n    break","8e2d99d1":"kenyanfood_classes = ['githeri', 'ugali', 'kachumbari', 'matoke', 'mandazi', 'bhaji',\n       'sukumawiki', 'nyamachoma', 'chapati', 'kukuchoma', 'masalachips',\n       'pilau', 'mukimo']","288d162e":"def get_random_inputs_labels(inputs, targets, n=100):\n    \"\"\"\n    get random inputs and labels\n    \"\"\"\n\n    assert len(inputs) == len(targets)\n\n    rand_indices = torch.randperm(len(targets))\n    \n    data = inputs[rand_indices][:n]\n    \n    labels = targets[rand_indices][:n]\n    \n    class_labels = [kenyanfood_classes[lab] for lab in labels]\n    \n    return data, class_labels","4e8f6e53":"def add_data_embedings(dataset, tb_writer, n=100):\n    \"\"\"\n    Add a few inputs and labels to tensorboard. \n    \"\"\"\n    \n    dataloader = torch.utils.data.DataLoader(dataset, batch_size=n, num_workers=4, shuffle=True)\n    \n    images, labels = next(iter(dataloader))\n    \n    tb_writer.add_embedding(mat = images.view(-1, 3 * 224 * 224), \n                            metadata=labels, \n                            label_img=images)\n    \n    return","3cb5b129":"def image_preprocess_transforms():\n    \n    preprocess = transforms.Compose([\n        transforms.Resize(224),\n        transforms.CenterCrop(224),\n        transforms.ToTensor()\n        ])\n    \n    return preprocess","839af8e4":"def image_common_transforms(mean, std):\n    preprocess = image_preprocess_transforms()\n    \n    common_transforms = transforms.Compose([\n        preprocess,\n        transforms.Normalize(mean, std)\n    ])\n    \n    return common_transforms","ad8d11af":"def data_augmentation_preprocess(mean, std):\n    \n    initial_transform = transforms.RandomChoice([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.RandomRotation((-90,90)),\n        #transforms.RandomCrop(28, padding=4)\n        ])\n    \n    common_transforms = image_common_transforms(mean, std)\n                \n    aug_transforms = transforms.Compose([\n        initial_transform,\n        #transforms.RandomGrayscale(p=0.1),\n        common_transforms\n        ])\n    \n    return aug_transforms","9deea393":"def get_mean_std():\n    \n    mean = [0.485, 0.456, 0.406] \n    std = [0.229, 0.224, 0.225]\n    \n    return mean, std","7ffe6011":"def data_loader(data_root, filename, transform, batch_size=16, shuffle=False, num_workers=2):\n    \n    dataset = KenyanFood13Dataset(data_root, filename, train=False, image_shape=None, transform=transform)\n    \n    loader = torch.utils.data.DataLoader(dataset, \n                                         batch_size=batch_size,\n                                         num_workers=num_workers,\n                                         shuffle=shuffle)\n    \n    return loader","56b26309":"def subset_data_loader(data_root, filename, transform, batch_size=8, shuffle=False, num_workers=2, subset_size=0.05):\n    \n    dataset = KenyanFood13Dataset(data_root, filename, train=False, image_shape=None, transform=transform)\n    \n    data_subset = torch.utils.data.Subset(dataset,np.arange(0,len(dataset),1.\/subset_size).astype(int))\n\n    loader = torch.utils.data.DataLoader(data_subset, \n                                         batch_size=batch_size,\n                                         num_workers=num_workers,\n                                         shuffle=shuffle)\n    \n    return loader","f4868133":"def get_data(batch_size, data_root, tb_writer, num_workers=4, data_augmentation=False):\n       \n    #mean, std = get_mean_std(data_root = data_root, num_workers = num_workers)\n    mean = (0.485, 0.456, 0.406)\n    std = (0.229, 0.224, 0.225)\n    \n    common_transforms = image_common_transforms(mean, std)\n        \n   \n    # if data_augmentation is true \n    # data augmentation implementation\n    if data_augmentation:    \n        train_transforms = data_augmentation_preprocess(mean, std)\n        test_transforms = common_transforms\n    # else do common transforms\n    else:\n        train_transforms = common_transforms\n        test_transforms = common_transforms\n        \n        \n    # train dataloader\n    \n    train_loader = data_loader(data_root,\n                                'train_df.csv',\n                               train_transforms, \n                               batch_size=batch_size, \n                               shuffle=True, \n                               num_workers=num_workers)\n    \n    # test dataloader\n    \n    test_loader = data_loader(data_root, \n                              'test_df.csv',\n                              test_transforms, \n                              batch_size=batch_size, \n                              shuffle=True, \n                              num_workers=num_workers)\n    \n    testdata = KenyanFood13Dataset(data_root, 'test_df.csv', train=False, image_shape=None, transform=common_transforms)\n    \n    add_data_embedings(testdata, tb_writer, n=100)\n    \n    return train_loader, test_loader","e590bfff":"@dataclass\nclass SystemConfiguration:\n    '''\n    Describes the common system setting needed for reproducible training\n    '''\n    seed: int = 21  # seed number to set the state of all random number generators\n    cudnn_benchmark_enabled: bool = True  # enable CuDNN benchmark for the sake of performance\n    cudnn_deterministic: bool = True  # make cudnn deterministic (reproducible training)","713b9725":"@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 64  \n    epochs_count: int = 50 \n    init_learning_rate: float = 0.001  # initial learning rate for lr scheduler\n    decay_rate: float = 0.1  \n    log_interval: int = 500  \n    test_interval: int = 1  \n    data_root: str =  '\/synology_data\/Debasish\/Pytorch-OpenCV\/Project2\/'\n    num_workers: int = 5  \n    device: str = 'cuda'\n    ","08b72cd1":"def setup_system(system_config: SystemConfiguration) -> None:\n    torch.manual_seed(system_config.seed)\n    if torch.cuda.is_available():\n        torch.backends.cudnn_benchmark_enabled = system_config.cudnn_benchmark_enabled\n        torch.backends.cudnn.deterministic = system_config.cudnn_deterministic","826167bd":"def prediction(model, device, batch_input, max_prob=True):\n    \"\"\"\n    get prediction for batch inputs\n    \"\"\"\n    \n    # send model to cpu\/cuda according to your system configuration\n    model.to(device)\n    \n    # it is important to do model.eval() before prediction\n    model.eval()\n\n    data = batch_input.to(device)\n\n    output = model(data)\n\n    # get probability score using softmax\n    prob = F.softmax(output, dim=1)\n    \n    if max_prob:\n        # get the max probability\n        pred_prob = prob.data.max(dim=1)[0]\n    else:\n        pred_prob = prob.data\n    \n    # get the index of the max probability\n    pred_index = prob.data.max(dim=1)[1]\n    \n    return pred_index.cpu().numpy(), pred_prob.cpu().numpy()","a452caf5":"def get_target_and_prob(model, dataloader, device):\n    \"\"\"\n    get targets and prediction probabilities\n    \"\"\"\n    \n    pred_prob = []\n    targets = []\n    \n    for _, (data, target) in enumerate(dataloader):\n        \n        _, prob = prediction(model, device, data, max_prob=False)\n        \n        pred_prob.append(prob)\n        \n        target = target.numpy()\n        targets.append(target)\n        \n    targets = np.concatenate(targets)\n    targets = targets.astype(int)\n    pred_prob = np.concatenate(pred_prob, axis=0)\n    \n    return targets, pred_prob","c7cc8b0f":"def add_pr_curves_to_tensorboard(model, dataloader, device, tb_writer, epoch, num_classes=13):\n    \"\"\"\n    Add precession and recall curve to tensorboard.\n    \"\"\"\n    \n    targets, pred_prob = get_target_and_prob(model, dataloader, device)\n    \n    for cls_idx in range(num_classes):\n        binary_target = targets == cls_idx\n        true_prediction_prob = pred_prob[:, cls_idx]\n        \n        tb_writer.add_pr_curve(kenyanfood_classes[cls_idx], \n                               binary_target, \n                               true_prediction_prob, \n                               global_step=epoch)\n        \n    return","98a15d8a":"def add_wrong_prediction_to_tensorboard(model, dataloader, device, tb_writer, \n                                        epoch, tag='Wrong_Predections', max_images='all'):\n    \"\"\"\n    Add wrong predicted images to tensorboard.\n    \"\"\"\n    #number of images in one row\n    num_images_per_row = 8\n    im_scale = 3\n    \n    plot_images = []\n    wrong_labels = []\n    pred_prob = []\n    right_label = []\n    \n    mean, std = get_mean_std()\n    \n    for _, (data, target) in enumerate(dataloader):\n        \n        \n        images = data.numpy()\n        pred, prob = prediction(model, device, data)\n        target = target.numpy()\n        indices = pred.astype(int) != target.astype(int)\n        \n        plot_images.append(images[indices])\n        wrong_labels.append(pred[indices])\n        pred_prob.append(prob[indices])\n        right_label.append(target[indices])\n        \n    plot_images = np.concatenate(plot_images, axis=0).squeeze()\n    plot_images = (np.moveaxis(plot_images, 1, -1) * std) + mean\n    wrong_labels = np.concatenate(wrong_labels)\n    wrong_labels = wrong_labels.astype(int)\n    right_label = np.concatenate(right_label)\n    right_label = right_label.astype(int)\n    pred_prob = np.concatenate(pred_prob)\n    \n    \n    if max_images == 'all':\n        num_images = len(images)\n    else:\n        num_images = min(len(plot_images), max_images)\n        \n    fig_width = num_images_per_row * im_scale\n    \n    if num_images % num_images_per_row == 0:\n        num_row = num_images\/num_images_per_row\n    else:\n        num_row = int(num_images\/num_images_per_row) + 1\n        \n    fig_height = num_row * im_scale\n        \n    plt.style.use('default')\n    plt.rcParams[\"figure.figsize\"] = (fig_width, fig_height)\n    fig = plt.figure()\n    \n    for i in range(num_images):\n        plt.subplot(num_row, num_images_per_row, i+1, xticks=[], yticks=[])\n        plt.imshow((plot_images[i]*255).astype(np.uint8))\n        plt.gca().set_title('{0}({1:.2}), {2}'.format(kenyanfood_classes[wrong_labels[i]], \n                                                          pred_prob[i], \n                                                          kenyanfood_classes[right_label[i]]))\n        \n    tb_writer.add_figure(tag, fig, global_step=epoch)\n    \n    return","debb02df":"def train(\n    train_config: TrainingConfiguration, model: nn.Module, optimizer: torch.optim.Optimizer,\n    train_loader: torch.utils.data.DataLoader, epoch_idx: int, tb_writer: SummaryWriter\n) -> None:\n    \n    # change model in training mode\n    model.train()\n    \n    # to get batch loss\n    batch_loss = np.array([])\n    \n    # to get batch accuracy\n    batch_acc = np.array([])\n        \n    for batch_idx, (data, target) in enumerate(train_loader):\n        \n        # clone target\n        indx_target = target.clone()\n        # send data to device (it is mandatory if GPU has to be used)\n        data = data.to(train_config.device)\n        # send target to device\n        target = target.to(train_config.device)\n\n        # reset parameters gradient to zero\n        optimizer.zero_grad()\n        \n        # forward pass to the model\n        output = model(data)\n        \n        # cross entropy loss\n        loss = F.cross_entropy(output, target)\n        \n        # find gradients w.r.t training parameters\n        loss.backward()\n        # Update parameters using gradients\n        optimizer.step()\n        \n        batch_loss = np.append(batch_loss, [loss.item()])\n        \n        # get probability score using softmax\n        prob = F.softmax(output, dim=1)\n            \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1]  \n                        \n        # correct prediction\n        correct = pred.cpu().eq(indx_target).sum()\n            \n        # accuracy\n        acc = float(correct) \/ float(len(data))\n        \n        batch_acc = np.append(batch_acc, [acc])\n\n        if batch_idx % train_config.log_interval == 0 and batch_idx > 0:\n            \n            total_batch = epoch_idx * len(train_loader.dataset)\/train_config.batch_size + batch_idx\n            tb_writer.add_scalar('Loss\/train-batch', loss.item(), total_batch)\n            tb_writer.add_scalar('Accuracy\/train-batch', acc, total_batch)\n            \n    epoch_loss = batch_loss.mean()\n    epoch_acc = batch_acc.mean()\n    print('Epoch: {} \\nTrain Loss: {:.6f} Acc: {:.4f}'.format(epoch_idx, epoch_loss, epoch_acc))\n    return epoch_loss, epoch_acc","5ef1bc08":"def validate(\n    train_config: TrainingConfiguration,\n    model: nn.Module,\n    test_loader: torch.utils.data.DataLoader\n) -> float:\n    # \n    model.eval()\n    test_loss = 0\n    count_corect_predictions = 0\n    for data, target in test_loader:\n        indx_target = target.clone()\n        data = data.to(train_config.device)\n        \n        target = target.to(train_config.device)\n        \n        output = model(data)\n        # add loss for each mini batch\n        test_loss += F.cross_entropy(output, target).item()\n        \n        # get probability score using softmax\n        prob = F.softmax(output, dim=1)\n        \n        # get the index of the max probability\n        pred = prob.data.max(dim=1)[1] \n        \n        # add correct prediction count\n        count_corect_predictions += pred.cpu().eq(indx_target).sum()\n\n    # average over number of mini-batches\n    test_loss = test_loss \/ len(test_loader)  \n    \n    # average over number of dataset\n    accuracy = 100. * count_corect_predictions \/ len(test_loader.dataset)\n    \n    print(\n        '\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n            test_loss, count_corect_predictions, len(test_loader.dataset), accuracy\n        )\n    )\n    \n    \n    return test_loss, float(accuracy)\/100.0","2895c93d":"def add_model_weights_as_histogram(model, tb_writer, epoch):\n    for name, param in model.named_parameters():\n        tb_writer.add_histogram(name.replace('.', '\/'), param.data.cpu().abs(), epoch)\n    return","1d3ef008":"def add_network_graph_tensorboard(model, inputs, tb_writer):\n    tb_writer.add_graph(model, inputs)\n    return","d9d9aa52":"def save_model(model, device, model_dir='models', model_file_name='kenyan_food_classifier.pt'):\n    \n\n    if not os.path.exists(model_dir):\n        os.makedirs(model_dir)\n\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # make sure you transfer the model to cpu.\n    if device == 'cuda':\n        model.to('cpu')\n\n    # save the state_dict\n    torch.save(model.state_dict(), model_path)\n    \n    if device == 'cuda':\n        model.to('cuda')\n    \n    return","93698cbd":"def load_model(model, model_dir='models', model_file_name='kenyan_food_classifier.pt'):\n    model_path = os.path.join(model_dir, model_file_name)\n\n    # loading the model and getting model parameters by using load_state_dict\n    model.load_state_dict(torch.load(model_path))\n    \n    return model","2d3469ac":"def main(model, optimizer, tb_writer, scheduler=None, system_configuration=SystemConfiguration(), \n         training_configuration=TrainingConfiguration(), data_augmentation=False):\n    \n    # system configuration\n    setup_system(system_configuration)\n\n    # batch size\n    batch_size_to_set = training_configuration.batch_size\n    # num_workers\n    num_workers_to_set = training_configuration.num_workers\n    # epochs\n    epoch_num_to_set = training_configuration.epochs_count\n\n    # if GPU is available use training config, \n    # else lower batch_size, num_workers and epochs count\n    \n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 2\n\n    # data loader\n    train_loader, test_loader = get_data(\n        batch_size=batch_size_to_set,\n        data_root=training_configuration.data_root,\n        tb_writer=tb_writer,\n        num_workers=num_workers_to_set,\n        data_augmentation=data_augmentation\n    )\n    \n    \n    # Update training configuration\n    training_configuration = TrainingConfiguration(\n        device=device,\n        batch_size=batch_size_to_set,\n        num_workers=num_workers_to_set\n    )\n        \n    # send model to device (GPU\/CPU)\n    model.to(training_configuration.device)\n    \n    \n    # add network graph with inputs info\n    images, labels = next(iter(test_loader))\n    images = images.to(training_configuration.device)\n    add_network_graph_tensorboard(model, images, tb_writer)\n\n    best_loss = torch.tensor(np.inf)\n    \n    # epoch train\/test loss\n    epoch_train_loss = np.array([])\n    epoch_test_loss = np.array([])\n    \n    # epoch train\/test accuracy\n    epoch_train_acc = np.array([])\n    epoch_test_acc = np.array([])\n    \n#     add_wrong_prediction_to_tensorboard(model, test_loader, \n#                                                 training_configuration.device, \n#                                                 tb_writer, 0, max_images=300)\n\n    # Calculate Initial Test Loss\n    init_val_loss, init_val_accuracy = validate(training_configuration, model, test_loader)\n    print(\"Initial Test Loss : {:.6f}, \\nInitial Test Accuracy : {:.3f}%\\n\".format(init_val_loss, \n                                                                                   init_val_accuracy*100))\n  \n    \n    \n    # training time measurement\n    t_begin = time.time()\n    for epoch in range(training_configuration.epochs_count):\n        \n        # Traing\n        train_loss, train_acc = train(training_configuration, model, optimizer, train_loader, epoch, tb_writer)\n        \n        epoch_train_loss = np.append(epoch_train_loss, [train_loss])\n        \n        epoch_train_acc = np.append(epoch_train_acc, [train_acc])\n        \n        # add scalar (loss\/accuracy) to tensorboard\n        tb_writer.add_scalar('Loss\/Train',train_loss, epoch)\n        tb_writer.add_scalar('Accuracy\/Train', train_acc, epoch)\n\n        elapsed_time = time.time() - t_begin\n        speed_epoch = elapsed_time \/ (epoch + 1)\n        speed_batch = speed_epoch \/ len(train_loader)\n        eta = speed_epoch * training_configuration.epochs_count - elapsed_time\n        \n        # add time metadata to tensorboard\n        tb_writer.add_scalar('Time\/elapsed_time', elapsed_time, epoch)\n        tb_writer.add_scalar('Time\/speed_epoch', speed_epoch, epoch)\n        tb_writer.add_scalar('Time\/speed_batch', speed_batch, epoch)\n        tb_writer.add_scalar('Time\/eta', eta, epoch)\n        \n        print(\n            \"Elapsed {:.2f}s, {:.2f} s\/epoch, {:.2f} s\/batch, ets {:.2f}s\".format(\n                elapsed_time, speed_epoch, speed_batch, eta\n            )\n        )\n\n        \n\n        # Validate\n        if epoch % training_configuration.test_interval == 0:\n            current_loss, current_accuracy = validate(training_configuration, model, test_loader)\n            \n            epoch_test_loss = np.append(epoch_test_loss, [current_loss])\n        \n            epoch_test_acc = np.append(epoch_test_acc, [current_accuracy])\n            \n            # add scalar (loss\/accuracy) to tensorboard\n            tb_writer.add_scalar('Loss\/Validation', current_loss, epoch)\n            tb_writer.add_scalar('Accuracy\/Validation', current_accuracy, epoch)\n            \n            # add scalars (loss\/accuracy) to tensorboard\n            tb_writer.add_scalars('Loss\/train-val', {'train': train_loss, \n                                           'validation': current_loss}, epoch)\n            tb_writer.add_scalars('Accuracy\/train-val', {'train': train_acc, \n                                               'validation': current_accuracy}, epoch)\n            \n            if current_loss < best_loss:\n                best_loss = current_loss\n                print('Model Improved. Saving the Model...\\n')\n                save_model(model, device=training_configuration.device)\n        \n                \n            # add wrong predicted image to tensorboard\n            add_wrong_prediction_to_tensorboard(model, test_loader, \n                                                training_configuration.device, \n                                                tb_writer, epoch, max_images=300)\n        \n        # scheduler step\/ update learning rate\n        if scheduler is not None:\n            scheduler.step()\n            \n        # adding model weights to tensorboard as histogram\n        add_model_weights_as_histogram(model, tb_writer, epoch)\n        \n        # add pr curves to tensor board\n        add_pr_curves_to_tensorboard(model, test_loader, \n                                     training_configuration.device, \n                                     tb_writer, epoch, num_classes=13)\n        \n    print(\"Total time: {:.2f}, Best Loss: {:.3f}\".format(time.time() - t_begin, best_loss))\n    \n    \n    \n    return model, epoch_train_loss, epoch_train_acc, epoch_test_loss, epoch_test_acc","b4288181":"def pretrained_resnet18(transfer_learning=True, num_class=13):\n    resnet = models.resnet18(pretrained=True)\n    \n    if transfer_learning:\n        for param in resnet.parameters():\n            param.requires_grad = False\n            \n    last_layer_in = resnet.fc.in_features\n    resnet.fc = nn.Linear(last_layer_in, num_class)\n    \n    return resnet","08ada8e5":"def plot_loss_accuracy(train_loss, val_loss, train_acc, val_acc, colors, \n                       loss_legend_loc='upper center', acc_legend_loc='upper left', \n                       fig_size=(20, 10), sub_plot1=(1, 2, 1), sub_plot2=(1, 2, 2)):\n    \n    plt.rcParams[\"figure.figsize\"] = fig_size\n    fig = plt.figure()\n    \n    plt.subplot(sub_plot1[0], sub_plot1[1], sub_plot1[2])\n    \n    for i in range(len(train_loss)):\n        x_train = range(len(train_loss[i]))\n        x_val = range(len(val_loss[i]))\n        \n        min_train_loss = train_loss[i].min()\n        \n        min_val_loss = val_loss[i].min()\n        \n        plt.plot(x_train, train_loss[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN LOSS ({0:.4})\".format(min_train_loss))\n        plt.plot(x_val, val_loss[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID LOSS ({0:.4})\".format(min_val_loss))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('loss')\n    plt.legend(loc=loss_legend_loc)\n    plt.title('Training and Validation Loss')\n        \n    plt.subplot(sub_plot2[0], sub_plot2[1], sub_plot2[2])\n    \n    for i in range(len(train_acc)):\n        x_train = range(len(train_acc[i]))\n        x_val = range(len(val_acc[i]))\n        \n        max_train_acc = train_acc[i].max() \n        \n        max_val_acc = val_acc[i].max() \n        \n        plt.plot(x_train, train_acc[i], linestyle='-', color='tab:{}'.format(colors[i]), \n                 label=\"TRAIN ACC ({0:.4})\".format(max_train_acc))\n        plt.plot(x_val, val_acc[i], linestyle='--' , color='tab:{}'.format(colors[i]), \n                 label=\"VALID ACC ({0:.4})\".format(max_val_acc))\n        \n    plt.xlabel('epoch no.')\n    plt.ylabel('accuracy')\n    plt.legend(loc=acc_legend_loc)\n    plt.title('Training and Validation Accuracy')\n    \n    fig.savefig('sample_loss_acc_plot.png')\n    plt.show()\n    \n    return   ","2f861a29":"def get_optimizer_and_scheduler(model):\n    train_config = TrainingConfiguration()\n\n    init_learning_rate = train_config.init_learning_rate\n\n    # optimizer\n    optimizer = optim.SGD(\n        model.parameters(),\n        lr = init_learning_rate,\n        momentum = 0.9\n    )\n\n    decay_rate = train_config.decay_rate\n\n    lmbda = lambda epoch: 1\/(1 + decay_rate * epoch)\n\n    # Scheduler\n    scheduler = lr_scheduler.LambdaLR(optimizer, lr_lambda=lmbda)\n    \n    return optimizer, scheduler","70f23a3b":"# model = pretrained_resnet18(transfer_learning=True)\n# print(model)\n# # get optimizer and scheduler\n# optimizer, scheduler = get_optimizer_and_scheduler(model)\n\n# # Tensorboard summary writer\n# transfer_learning_sw = SummaryWriter('\/synology_data\/Debasish\/Pytorch-OpenCV\/Project2\/log_resnet18\/transferlearning')   \n\n# # train and validate\n# model, train_loss_exp2, train_acc_exp2, val_loss_exp2, val_acc_exp2 = main(model, \n#                                                                            optimizer,\n#                                                                            transfer_learning_sw,\n#                                                                            scheduler,\n#                                                                            data_augmentation=True)\n# transfer_learning_sw.close()","ddae00a1":"model = pretrained_resnet18(transfer_learning=False)\nprint(model)\n# get optimizer and scheduler\noptimizer, scheduler = get_optimizer_and_scheduler(model)\n\n# Tensorboard summary writer\nfine_tuning_sw = SummaryWriter('\/synology_data\/Debasish\/Pytorch-OpenCV\/Project2\/log_resnet18\/finetuning')   \n\nmodel, train_loss_exp9, train_acc_exp9, val_loss_exp9, val_acc_exp9 = main(model, \n                                                                           optimizer, \n                                                                           fine_tuning_sw,\n                                                                           scheduler,\n                                                                           data_augmentation=True)\n\nfine_tuning_sw.close()","6297d9b7":"# plot_loss_accuracy(train_loss=[train_loss_exp2], \n#                    val_loss=[val_loss_exp2], \n#                    train_acc=[train_acc_exp2], \n#                    val_acc=[val_acc_exp2], \n#                    colors=['blue'], \n#                    loss_legend_loc='upper center', \n#                    acc_legend_loc='upper left')","fa587cd5":"class KenyanFood13DatasetTest(Dataset):\n    \"\"\"\n\n    \"\"\"\n\n    def __init__(self, data_root, transform=None):\n        image_csv_path = os.path.join(data_root, 'test.csv')\n        self.image_df = pd.read_csv(image_csv_path)\n        print(\"Number test samples {}\".format(len(self.image_df)))\n        \n        # set transform attribute\n        self.transform = transform\n        \n        self.data_dict = {\n            'image_path': [],\n            'img_name': []\n        }\n        \n        img_dir = os.path.join(data_root, 'images')\n\n        for index, row in self.image_df.iterrows():\n            img_path = os.path.join(img_dir, '{}.jpg'.format(row[0]))\n            self.data_dict['image_path'].append(img_path)\n            self.data_dict['img_name'].append(row[0])\n\n    def __getitem__(self, idx):\n        \"\"\"\n        For given index, return images with resize and preprocessing.\n        \"\"\"\n        \n        image = Image.open(self.data_dict['image_path'][idx]).convert(\"RGB\")\n        \n        if self.transform is not None:\n            image = self.transform(image)\n            \n        img_name = self.data_dict['img_name'][idx]\n            \n        return image, np.float64(img_name)\n    \n    def __len__(self):\n        return len(self.data_dict['img_name'])","ab22d567":"def get_sample_prediction(model, data_root, mean, std):\n    classes_labels = {}\n    submit_data = {}\n    reader = csv.reader(open('.\/labels.csv'))\n\n\n    for row in reader:\n        key = row[0]\n        if key in classes_labels:\n            # implement your duplicate row handling here\n            pass\n        classes_labels[int(key)] = row[1]\n\n\n    batch_size = 15\n\n    if torch.cuda.is_available():\n        device = \"cuda\"\n        num_workers = 8\n    else:\n        device = \"cpu\"\n        num_workers = 2\n#     mean = (0.485, 0.456, 0.406)\n#     std = (0.229, 0.224, 0.225)\n    common_transforms = image_common_transforms(mean, std)\n    test_dataset_trans =  KenyanFood13DatasetTest(data_root, transform=common_transforms)\n\n    # original image dataset    \n    test_dataset =  KenyanFood13DatasetTest(data_root, transform=image_preprocess_transforms())\n\n    data_len = test_dataset.__len__()\n\n    interval = int(data_len\/batch_size)\n\n    imgs = []\n    inputs = []\n    img_names = []\n    for i in range(batch_size):\n        index = i * interval\n        trans_input, name = test_dataset_trans.__getitem__(index)\n        img, _ = test_dataset.__getitem__(index)\n\n        imgs.append(img)\n        inputs.append(trans_input)\n        img_names.append(int(name))\n\n    inputs = torch.stack(inputs)\n\n    cls, prob = prediction(model, device, batch_input=inputs)\n\n    plt.style.use('default')\n    plt.rcParams[\"figure.figsize\"] = (15, 9)\n    fig = plt.figure()\n\n\n    for i, img_names in enumerate(img_names):\n        submit_data[img_names] = classes_labels[cls[i]]\n        plt.subplot(3, 5, i+1)\n        img = transforms.functional.to_pil_image(imgs[i])\n        plt.imshow(img)\n        plt.gca().set_title('P:{0}({1:.2}), T:{2}'.format(cls[i], \n                                                     prob[i], \n                                                     classes_labels[cls[i]]))\n    #print(test_dataset.classes)\n    fig.savefig('sample_prediction.png')\n    plt.show()\n\n    with open('.\/submission.csv', 'w') as f:\n        for key in submit_data.keys():\n            f.write(\"%s,%s\\n\"%(key,submit_data[key]))\n\n    print(submit_data)\n\n    return\n        ","398558f3":"mean, std = get_mean_std()\nget_sample_prediction(model, data_root, mean, std)","246adbbc":"def get_batch_prediction():\n    test_dataset =  KenyanFood13DatasetTest(data_root, transform=image_preprocess_transforms())\n\n    test_loader = torch.utils.data.DataLoader(test_dataset, \n                                         batch_size=16,\n                                         num_workers=5,\n                                         shuffle=True)\n    if torch.cuda.is_available():\n        device = \"cuda\"\n    else:\n        device = \"cpu\"\n        batch_size_to_set = 16\n        num_workers_to_set = 2\n    \n    cls_labels = []\n    img_names = []\n    for batch_idx, (data, target) in enumerate(test_loader):\n        cls, prob = prediction(model, device, batch_input=data)\n\n        img_names.extend(target.tolist())\n        cls_labels.extend(cls.tolist())\n        \n    return img_names, cls_labels","80b54fdc":"img_names, cls_labels = get_batch_prediction()\n\nclasses_labels = {}\nsubmit_data = {}\nreader = csv.reader(open('.\/labels.csv'))\n\n\nfor row in reader:\n    key = row[0]\n    if key in classes_labels:\n        # implement your duplicate row handling here\n        pass\n    classes_labels[int(key)] = row[1]\n    \nfor i, img_names in enumerate(img_names):\n    submit_data[int(img_names)] = classes_labels[cls_labels[i]]\n    \nwith open('.\/submission.csv', 'w') as f:\n    for key in submit_data.keys():\n        f.write(\"%s,%s\\n\"%(key,submit_data[key]))\n        \nsubmit_data","5fe740a0":"### Data loader","00c1b4f6":"## <font style=\"color:green\">6. Utils [5 Points]<\/font>\n\nDefine your methods or classes which are not covered in the above sections.","374679ce":"## <font style=\"color:green\">5. Model [5 Points]<\/font>\n\nDefine your model in this section.\n\n**You are allowed to use any pre-trained model.**","3d367795":"# <font style=\"color:blue\">Project 2: Kaggle Competition - Classification<\/font>\n\n#### Maximum Points: 100\n\n<div>\n    <table>\n        <tr><td><h3>Sr. no.<\/h3><\/td> <td><h3>Section<\/h3><\/td> <td><h3>Points<\/h3><\/td> <\/tr>\n        <tr><td><h3>1<\/h3><\/td> <td><h3>Data Loader<\/h3><\/td> <td><h3>10<\/h3><\/td> <\/tr>\n        <tr><td><h3>2<\/h3><\/td> <td><h3>Configuration<\/h3><\/td> <td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>3<\/h3><\/td> <td><h3>Evaluation Metric<\/h3><\/td> <td><h3>10<\/h3><\/td> <\/tr>\n        <tr><td><h3>4<\/h3><\/td> <td><h3>Train and Validation<\/h3><\/td> <td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>5<\/h3><\/td> <td><h3>Model<\/h3><\/td> <td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>6<\/h3><\/td> <td><h3>Utils<\/h3><\/td> <td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>7<\/h3><\/td> <td><h3>Experiment<\/h3><\/td><td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>8<\/h3><\/td> <td><h3>TensorBoard Dev Scalars Log Link<\/h3><\/td> <td><h3>5<\/h3><\/td> <\/tr>\n        <tr><td><h3>9<\/h3><\/td> <td><h3>Kaggle Profile Link<\/h3><\/td> <td><h3>50<\/h3><\/td> <\/tr>\n    <\/table>\n<\/div>\n","5b2d6f53":"## <font style=\"color:green\">2. Configuration [5 Points]<\/font>\n\nDefine your configuration in this section.\n\nFor example,\n\n```\n@dataclass\nclass TrainingConfiguration:\n    '''\n    Describes configuration of the training process\n    '''\n    batch_size: int = 10 \n    epochs_count: int = 50  \n    init_learning_rate: float = 0.1  # initial learning rate for lr scheduler\n    log_interval: int = 5  \n    test_interval: int = 1  \n    data_root: str = \"\/kaggle\/input\/pytorch-opencv-course-classification\/\" \n    num_workers: int = 2  \n    device: str = 'cuda'  \n    \n```","f6cd689c":"## Image Transform","9a45ff70":"## Add Network Graphs","36a0e97a":"## <font style=\"color:green\">4. Train and Validation [5 Points]<\/font>\n\nWrite the methods or classes that will be used for training and validation.","9f8fd318":"## Get batch prediction","b2e5a0ce":"## <font style=\"color:green\">1. Data Loader [10 Points]<\/font>\n\nIn this section, you have to write a class or methods that will be used to get training and validation data\nloader.\n\nYou will have to write a custom dataset class to load data.\n\n**Note that there are not separate validation data, so you will have to create your validation set by dividing train data into train and validation data. Usually, in practice, we do `80:20` ratio for train and validation, respectively.** \n\nFor example,\n\n```\nclass KenyanFood13Dataset(Dataset):\n    \"\"\"\n    \n    \"\"\"\n    \n    def __init__(self, *args):\n    ....\n    ...\n    \n    def __getitem__(self, idx):\n    ...\n    ...\n    \n    \n```\n\n```\ndef get_data(args1, *agrs):\n    ....\n    ....\n    return train_loader, test_loader\n```","4178129b":"## <font style=\"color:green\">3. Evaluation Metric [10 Points]<\/font>\n\nDefine methods or classes that will be used in model evaluation, for example, accuracy, f1-score, etc.","2e3d895c":"## Adding Embeddings","1a210aea":"### Subset data loader","e6e8ccdb":"## <font style=\"color:green\">7. Experiment [5 Points]<\/font>\n\nChoose your optimizer and LR-scheduler and use the above methods and classes to train your model.","86d2da22":"## Main Function","ef23909a":"## <font style=\"color:green\">8. TensorBoard Dev Scalars Log Link [5 Points]<\/font>\n\nShare your tensorboard scalars logs link in this section. You can also share (not mandatory) your GitHub link if you have pushed this project in GitHub. \n\nFor example, [Find Project2 logs here](https:\/\/tensorboard.dev\/experiment\/kMJ4YU0wSNG0IkjrluQ5Dg\/#scalars).","de22097b":"## Submit the prediction","6845244a":"## Load and Save model","34818184":"## Add histogram of weights","cd18aa8a":"## <font style=\"color:green\">9. Kaggle Profile Link [50 Points]<\/font>\n\nShare your Kaggle profile link here with us so that we can give points for the competition score. \n\nYou should have a minimum accuracy of `75%` on the test data to get all points. If accuracy is less than `70%`, you will not get any points for the section. \n\n**You must have to submit `submission.csv` (prediction for images in `test.csv`) in `Submit Predictions` tab in Kaggle to get any evaluation in this section.**","6022ce16":"## Inference"}}