{"cell_type":{"703b7cd8":"code","e6e295ab":"code","6080b034":"code","9998f1a6":"code","05fb12d4":"code","686cc11b":"code","8aabd3ec":"code","9297fc7c":"code","fdeaebf4":"code","ac463e10":"code","6b40c548":"code","8b8a8ba1":"code","36d9456d":"code","f2d761e7":"code","aa5fa426":"code","ff414b77":"code","b2437a00":"code","da52ee2b":"code","e596230f":"code","8e3e740d":"code","1704e257":"code","5c5d2b1e":"code","cb5d621b":"code","6cb185d9":"code","e577de0d":"code","9cbd9722":"code","2904b63c":"code","b0cb5165":"code","0ea96985":"code","f53fc7ad":"code","de06adbc":"code","f8f7fbb5":"code","093e5d9e":"code","98e4d97b":"code","afa431e0":"markdown","93469a63":"markdown","021d449b":"markdown","55990f72":"markdown","5b41552a":"markdown","ae9db1c7":"markdown","67b9457b":"markdown","6176a938":"markdown","3b7c9123":"markdown","fe9c720d":"markdown","95ffd222":"markdown","2a1ece0c":"markdown","6378ef15":"markdown","3435c7d0":"markdown","0c5e17b2":"markdown","16d51902":"markdown","7b990440":"markdown","120bd855":"markdown","44fe2e13":"markdown","68309e8e":"markdown","decbba83":"markdown","a5bf9376":"markdown","f51dab28":"markdown","cb35a572":"markdown"},"source":{"703b7cd8":"import os, random\nimport itertools\nimport math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm.notebook import tqdm\nimport category_encoders as ce\n\nINDEX = 'ID'\nTARGET = 'ACTIVITY'\n\nSEED = 2021\nNUM_CLASSES = 6\nNFOLDS = 5\n\n# lightgbm\u306e\u30d1\u30e9\u30e1\u30bf\u53ca\u3073\u5b66\u7fd2\u8a2d\u5b9a\nparams = {\n    'objective': 'multiclass',\n    'boosting': 'gbdt',\n    'metric': 'multi_logloss',\n    'feature_pre_filter': False,\n    'num_leaves': 39,\n    'bagging_freq': 3,\n    'min_child_samples': 32,\n    'learning_rate': 0.0926,\n    'lambda_l1': 0.00000049,\n    'lambda_l2': 0.00000058,\n    'feature_fraction': 0.56,\n    'bagging_fraction': 0.99,\n    'max_bin': 349,\n    'verbosity': -1,\n    'num_class': NUM_CLASSES,\n    'seed': SEED\n}\n\nnum_boost_round = 10000\nearly_stopping_rounds = 100\nverbose_eval = 1000\n\ndef seed_everything(seed=42, is_tensorflow=False, is_torch=False, verbose=True):\n\n    os.environ['PYTHONHASHSEED'] = str(seed) # os\n    random.seed(seed) # random\n    np.random.seed(seed) # numpy\n\n    if is_tensorflow:\n        import tensorflow as tf\n        tf.random.set_seed(seed)\n        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n        os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n        tf.config.threading.set_inter_op_parallelism_threads(1)\n        tf.config.threading.set_intra_op_parallelism_threads(1)\n\n    if is_torch:\n        import torch\n        torch.manual_seed(seed)\n        torch.cuda.manual_seed(seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False # True: \u518d\u73fe\u6027\u306a\u304f\u306a\u308b\u304c\u3001\u8a08\u7b97\u901f\u304f\u306a\u308b False: \u518d\u73fe\u6027\u304c\u62c5\u4fdd\u3055\u308c\u308b\u304c\u3001\u8a08\u7b97\u9045\u304f\u306a\u308b\n\n    if verbose:\n        print(f'set random seed: {seed}')\n        \ndef lgb_get_feature_importance(model_array, features):\n    val_gain = model_array[0].feature_importance(importance_type='gain')\n    val_gain = pd.Series(val_gain)\n    \n    for m in model_array[1:]:\n        s = pd.Series(m.feature_importance(importance_type='gain'))\n        val_gain = pd.concat([val_gain, s], axis=1)\n        \n    val_mean = val_gain.mean(axis=1)\n    val_mean = val_mean.values\n    df = pd.DataFrame(val_mean, index=features, columns=['importance']).sort_values('importance')\n    df = df.sort_values('importance', ascending=True)\n    \n    ids = len(df.index) # \u7279\u5fb4\u91cf\u304c30\u4ee5\u4e0a\u3042\u308b\u3068\u304d\u306f\u4e0a\u4f4d30\u307e\u3067\u3092\u8868\u793a\n    if ids > 30:\n        df = df.iloc[-30:]\n    \n    return df\n\ndef display_ture_pred(y, oof_train):\n    ''' \u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\u51fa\u529b\u7528 '''\n    fig, ax = plt.subplots(figsize=(8, 8))\n    sns.histplot(oof_train, label='pred', ax=ax, color='black')\n    sns.histplot(y, label='true', ax=ax)\n    ax.legend()\n    ax.grid()\n    plt.show()\n\ndef display_cmx(y, oof_train):\n    ''' \u6df7\u540c\u884c\u5217\u306e\u30d7\u30ed\u30c3\u30c8\u51fa\u529b\u7528 '''\n    labels = sorted(list(set(y)))\n    cmx_data = confusion_matrix(y_true=y, y_pred=oof_train, labels=labels)\n    df_cmx = pd.DataFrame(cmx_data, index=labels, columns=labels)\n    plt.figure(figsize = (10,7))\n    sns.heatmap(df_cmx, annot=True)\n    plt.xlabel(\"predict\", fontsize=13)\n    plt.ylabel(\"true\", fontsize=13)\n    plt.show()\n    \ndef display_fi(fi_df):\n    ''' feature imporance\u306e\u30d7\u30ed\u30c3\u30c8\u51fa\u529b\u7528 '''\n    plt.figure(figsize = (10,7))\n    sns.barplot(data=fi_df, y=fi_df.index, x='importance', orient='h')\n    plt.show()\n    \ndef run_train(train_df, is_weight=True) -> list:\n    ''' \u5b66\u7fd2\u3068\u5404\u7a2e\u5b66\u7fd2\u7d50\u679c\u306e\u51fa\u529b '''\n    X = train_df.drop([INDEX, TARGET], axis=1)\n    features = list(X.columns)\n    \n    X = X.values\n    y = train_df[TARGET].astype('int')\n    \n    seed_everything(seed=SEED)\n    fold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\n    oof_train = np.zeros(len(X))\n    model_array = []\n    for i, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n        print(f'**************** FOLD {i+1} ****************')\n        X_train, X_val = X[train_idx], X[val_idx]\n        y_train, y_val = y.values[train_idx], y.values[val_idx]\n\n        if is_weight:\n            # train weight\n            w_tr = 1 \/ pd.DataFrame(y_train).reset_index().groupby(0).count().values\n            w_tr = w_tr[y_train].ravel()\n            w_tr \/= w_tr.sum()\n            train_weight = list(w_tr)\n\n            # test weight\n            w_vl = 1 \/ pd.DataFrame(y_val).reset_index().groupby(0).count().values\n            w_vl = w_vl[y_val].ravel()\n            w_vl \/= w_vl.sum()\n            valid_weight = list(w_vl)\n        else:\n            train_weight = None\n            valid_weight = None\n\n        lgb_train = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n        lgb_val = lgb.Dataset(X_val, label=y_val, weight=valid_weight)\n        \n        model = lgb.train(params,\n                      lgb_train,\n                      valid_names=[\"train\", \"valid\"],\n                      valid_sets=[lgb_train, lgb_val],\n                      num_boost_round=num_boost_round,\n                      early_stopping_rounds=early_stopping_rounds,\n                      verbose_eval=verbose_eval)\n\n        model_array.append(model)\n\n        # evaluation\n        oof_val = model.predict(X_val, num_iteration=model.best_iteration)\n        y_val_pred = np.argmax(oof_val, axis=1)\n        oof_train[val_idx] = y_val_pred\n        print(f'[FOLD {i+1}] OOF Accuracy: {accuracy_score(y_true=y_val, y_pred=y_val_pred):.5f}\\n')\n\n    print(f'[RESULT] Accuracy: {accuracy_score(y_true=y, y_pred=oof_train):.5f}\\n')\n    \n    print('\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\uff1a')\n    display_ture_pred(y, oof_train)\n    print('\\n')\n    \n    print('\u6df7\u540c\u884c\u5217\uff1a')\n    display_cmx(y, oof_train)\n    print('\\n')\n    \n    fi_df = lgb_get_feature_importance(model_array, features=features)\n    print('\u7279\u5fb4\u91cd\u8981\u5ea6\uff1a')\n    display_fi(fi_df)\n    \ntrain = pd.read_csv(\"..\/input\/cdleyouth01\/df_train.csv\")\ntest = pd.read_csv(\"..\/input\/cdleyouth01\/df_test.csv\")\n# train + test\ndata = pd.concat([train, test], axis=0).reset_index(drop=True)","e6e295ab":"run_train(train)","6080b034":"data.sort_values(by='TIME').head(30)","9998f1a6":"class KNNFeatureExtractor:\n    def __init__(self, n_neighbors=5, num_class=1):\n        self.n_neighbors = n_neighbors\n        self.knn = KNeighborsClassifier(self.n_neighbors + 1)\n        self.num_class = num_class\n    \n    def fit(self, X, y):\n        self.knn.fit(X, y)\n        self.y = y if isinstance(y, np.ndarray) else np.array(y)\n        return self\n    \n    def transform(self, X):\n        distances, indexes = self.knn.kneighbors(X)\n        distances = distances[:, 1:]\n        indexes = indexes[:, 1:]\n        labels = self.y[indexes]\n        score_columns = [f\"knn_score_class{c:02d}\" for c in range(self.num_class)]\n        df_knn = pd.DataFrame(\n            [np.bincount(labels_, distances_, self.num_class) for labels_, distances_ in zip(labels, 1.0 \/ (distances + 1e-8))],\n            columns=score_columns\n        )\n#         df_knn[\"max_knn_scores\"] = df_knn.max(1)\n#         for col in score_columns:\n#             df_knn[f\"sub_max_knn_scores_{col}\"] = df_knn[\"max_knn_scores\"] - df_knn[col]\n#         for col in score_columns:\n#             df_knn[f\"rate_max_knn_scores_{col}\"] = df_knn[\"max_knn_scores\"] \/ (df_knn[col] + 1e-8)\n#         for i, col1 in enumerate(score_columns):\n#             for j, col2 in enumerate(score_columns[i+1:], i+1):\n#                 if {i, j} & {8, 10}:\n#                     df_knn[f\"sub_{col1}_{col2}\"] = df_knn[col1] - df_knn[col2]\n#         df_knn[\"sum_knn_scores\"] = df_knn.sum(1)\n#         df_knn[\"mean_knn_scores\"] = df_knn.mean(1)\n        \n        return df_knn","05fb12d4":"from sklearn.preprocessing import StandardScaler\n\nX = train.drop([INDEX, TARGET], axis=1).reset_index(drop=True)\ny = train[TARGET]\n\nstd = StandardScaler()\nX_std = std.fit_transform(X)\nfe_ex = KNNFeatureExtractor(n_neighbors=14, num_class=NUM_CLASSES)\nfe_ex = fe_ex.fit(X_std, y)\nfe_ex.transform(X_std)","686cc11b":"X = train.drop([INDEX, TARGET], axis=1)\nfeatures = list(X.columns)\ny = train[TARGET].astype('int')\n\nstd = StandardScaler()\nX_std = std.fit_transform(X)\n\n# \u3053\u3053\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\nfe_ex = KNNFeatureExtractor(n_neighbors=14, num_class=NUM_CLASSES)\nfe_ex = fe_ex.fit(X_std, y)\nknn_train = fe_ex.transform(X_std)\nX = pd.concat([X, knn_train], axis=1)\n\n\nseed_everything(seed=SEED)\nfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)\noof_train = np.zeros(len(X))\nmodel_array = []\nfor i, (train_idx, val_idx) in enumerate(fold.split(X_std, y)):\n    print(f'**************** FOLD {i+1} ****************')\n#     X_train, X_val = X_std[train_idx], X_std[val_idx]\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.values[train_idx], y.values[val_idx]\n    \n    \n#     # \u3053\u3053\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\n#     fe_ex = KNNFeatureExtractor(n_neighbors=14, num_class=NUM_CLASSES)\n#     fe_ex = fe_ex.fit(X_train, y_train)\n#     knn_train = fe_ex.transform(X_train)\n#     knn_val = fe_ex.transform(X_val)\n#     # \u4f5c\u6210\u3057\u305f\u7279\u5fb4\u91cf\u306e\u8ffd\u52a0\n#     X_train = pd.concat([X.iloc[train_idx].reset_index(drop=True), knn_train], axis=1)\n#     X_val = pd.concat([X.iloc[val_idx].reset_index(drop=True), knn_val], axis=1)\n\n    # train weight\n    w_tr = 1 \/ pd.DataFrame(y_train).reset_index().groupby(0).count().values\n    w_tr = w_tr[y_train].ravel()\n    w_tr \/= w_tr.sum()\n    train_weight = list(w_tr)\n\n    # test weight\n    w_vl = 1 \/ pd.DataFrame(y_val).reset_index().groupby(0).count().values\n    w_vl = w_vl[y_val].ravel()\n    w_vl \/= w_vl.sum()\n    valid_weight = list(w_vl)\n\n    lgb_train = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n    lgb_val = lgb.Dataset(X_val, label=y_val, weight=valid_weight)\n    \n    model = lgb.train(params,\n                      lgb_train,\n                      valid_names=[\"train\", \"valid\"],\n                      valid_sets=[lgb_train, lgb_val],\n                      num_boost_round=num_boost_round,\n                      early_stopping_rounds=early_stopping_rounds,\n                      verbose_eval=verbose_eval)\n\n    model_array.append(model)\n\n    # evaluation\n    oof_val = model.predict(X_val, num_iteration=model.best_iteration)\n    y_val_pred = np.argmax(oof_val, axis=1)\n    oof_train[val_idx] = y_val_pred\n    print(f'[FOLD {i+1}] OOF Accuracy: {accuracy_score(y_true=y_val, y_pred=y_val_pred):.5f}\\n')\n\nprint(f'[RESULT] Accuracy: {accuracy_score(y_true=y, y_pred=oof_train):.5f}\\n')\n\nprint('\u6b63\u89e3\u30e9\u30d9\u30eb\u3068\u4e88\u6e2c\u30e9\u30d9\u30eb\u306e\u5206\u5e03\u30d7\u30ed\u30c3\u30c8\uff1a')\ndisplay_ture_pred(y, oof_train)\nprint('\\n')\n\nprint('\u6df7\u540c\u884c\u5217\uff1a')\ndisplay_cmx(y, oof_train)\nprint('\\n')\n\nfor col in knn_train.columns:\n    features.append(col)\nfi_df = lgb_get_feature_importance(model_array, features=features)\nprint('\u7279\u5fb4\u91cd\u8981\u5ea6\uff1a')\ndisplay_fi(fi_df)","8aabd3ec":"from sklearn.cluster import KMeans\n\ncluster = KMeans(n_clusters=14, max_iter=10000, random_state=SEED)\nX = train.drop([INDEX, TARGET], axis=1)\ncluster.fit(X)","9297fc7c":"# \u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u306e\u51fa\u529b\ncluster.predict(X)","fdeaebf4":"# \u4e88\u6e2c\u3055\u308c\u305f\u30af\u30e9\u30b9\u30bf\u9593\u8ddd\u96e2\ncluster.transform(X)","ac463e10":"train_cluster = train.copy()\ncluster = KMeans(n_clusters=14, max_iter=10000, random_state=SEED)\nX = train.drop([INDEX, TARGET, 'TIME', 'EEG', 'SL'], axis=1)\ntrain_cluster['cluster'] = cluster.fit_predict(X) # fit and predict","6b40c548":"plt.hist(train_cluster['cluster'])","8b8a8ba1":"run_train(train_cluster)","36d9456d":"train_cluster2 = train.copy()\ncluster = KMeans(n_clusters=14, max_iter=10000, random_state=SEED)\nX = train.drop([INDEX, TARGET, 'TIME', 'EEG', 'SL'], axis=1)\ndistances = cluster.fit_transform(X) # fit and transform\n\ndst_columns = [f\"distance_cluster{c:02d}\" for c in range(14)]\ndst = pd.DataFrame(distances, columns=dst_columns)\ntrain_cluster2 = pd.concat([train_cluster2, dst], axis=1)","f2d761e7":"run_train(train_cluster2)","aa5fa426":"train_cluster3 = train.copy()\ncluster = KMeans(n_clusters=14, max_iter=10000, random_state=SEED)\nX = train.drop([INDEX, TARGET, 'TIME', 'EEG', 'SL'], axis=1)\ncluster.fit(X)\ntrain_cluster3['cluster'] = cluster.predict(X)\ndistances = cluster.transform(X)\n\ndst_columns = [f\"distance_cluster{c:02d}\" for c in range(14)]\ndst = pd.DataFrame(distances, columns=dst_columns)\ntrain_cluster3 = pd.concat([train_cluster3, dst], axis=1)","ff414b77":"run_train(train_cluster3)","b2437a00":"def aggregations(df: pd.DataFrame, cat_features: list, num_features: list) -> pd.DataFrame:\n    for ccol in cat_features:\n        for ncol in num_features:\n\n            # min, max, std, mean, median, skew, kurt\n            new_colname = f'{ccol}_groupby_{ncol}_min'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('min')\n            new_colname = f'{ccol}_groupby_{ncol}_max'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('max')\n            new_colname = f'{ccol}_groupby_{ncol}_std'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('std')\n            new_colname = f'{ccol}_groupby_{ncol}_mean'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('mean')\n            new_colname = f'{ccol}_groupby_{ncol}_median'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('median')\n            new_colname = f'{ccol}_groupby_{ncol}_skew'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('skew')\n            new_colname = f'{ccol}_groupby_{ncol}_kurt'\n            tmp = df.groupby(ccol)[ncol].apply(pd.DataFrame.kurt).to_dict()\n            df[new_colname] = df[ccol].map(tmp)\n\n            # Q1, Q3\n            q25_map = df.groupby(ccol)[ncol].quantile(0.25)\n            q75_map = df.groupby(ccol)[ncol].quantile(0.75)\n            # IQ, range, ratio\n            new_colname = f'{ccol}_{ncol}_iqr'\n            df[new_colname] = df[ccol].map(q75_map) - df[ccol].map(q25_map)\n            new_colname = f'{ccol}_{ncol}_iqr_ratio'\n            df[new_colname] = df[ccol].map(q75_map) \/ (df[ccol].map(q25_map) + 1e-8)\n\n            # min-max range, ratio\n            new_colname = f'{ccol}_{ncol}_range'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('max') - df.groupby(ccol)[ncol].transform('min')\n            new_colname = f'{ccol}_{ncol}_range_ratio'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('max') \/ (df.groupby(ccol)[ncol].transform('min') + 1e-8)\n            \n            # mean variance\n            new_colname = f'{ccol}_{ncol}_mean_var'\n            df[new_colname] = df.groupby(ccol)[ncol].transform('std') \/ (df.groupby(ccol)[ncol].transform('mean') + 1e-8)\n\n            # mean absolute deviation\n            new_colname = f'{ccol}_{ncol}_mean_absolute_deviation'\n            df[new_colname] = df.groupby(ccol)[ncol].apply(lambda x: np.abs(x - x.mean()))\n\n            # median absolute deviation\n            new_colname = f'{ccol}_{ncol}_median_absolute_deviation'\n            df[new_colname] = df.groupby(ccol)[ncol].apply(lambda x: np.abs(x - x.median()))\n\n            # diff, ratio\n            new_colname = f'{ccol}_{ncol}_diff'\n            df[new_colname] = df.groupby(ccol)[ncol].apply(lambda x: x - x.mean())\n            new_colname = f'{ccol}_{ncol}_ratio'\n            df[new_colname] = df.groupby(ccol)[ncol].apply(lambda x: x \/ (x.mean() + 1e-8))\n\n            # z-score\n            new_colname = f'{ccol}_{ncol}_zscore'\n            df[new_colname] = df.groupby(ccol)[ncol].apply(lambda x: (x - x.mean()) \/ (x.std() + 1e-8))\n\n            # high low ratio\n            new_colname = f'{ccol}_{ncol}_hl_ratio'\n            _mean = df.groupby(ccol)[ncol].transform('mean')\n            high_map = df[df[ncol] >= _mean].groupby(ccol)[ncol].sum()\n            low_map = df[df[ncol] < _mean].groupby(ccol)[ncol].sum()\n            df[new_colname] = df[ccol].map(high_map \/ (low_map + 1e-8))\n\n    return df","da52ee2b":"train_kmeans_agg = train_cluster.copy()\ntrain_kmeans_agg = aggregations(df=train_kmeans_agg, \n                                cat_features=['cluster'],\n                                num_features=['SL', 'BP', 'HR', 'CIRCLUATION'])","e596230f":"run_train(train_kmeans_agg)","8e3e740d":"def count_encoding(df: pd.DataFrame, columns: list) -> pd.DataFrame:\n    print('start count encoding ...')\n    for col in tqdm(columns):\n        new_colname = f'{col}_count_enc'\n        tmp = df[col].value_counts().to_dict()\n        df[new_colname] = df[col].map(tmp)\n    return df\n\ndef target_encoding(df: pd.DataFrame, columns: list, target: str, seed=0) -> pd.DataFrame:\n    print('start target encoding ...')\n    train = df[df[target].isnull()==False].reset_index(drop=True)\n    test = df[df[target].isnull()==True].reset_index(drop=True)\n\n    for col in tqdm(columns):\n        new_colname = f'{col}_target_enc'\n        ## target enc\n        encoder = ce.CatBoostEncoder(cols=col, random_state=seed, sigma=0.01)\n        train[new_colname] = encoder.fit_transform(train[col], train[target])\n        test[new_colname] = encoder.transform(test[col])\n\n    df = pd.concat([train, test], axis=0).reset_index(drop=True)\n    return df","1704e257":"train_kmeans_enc = train_cluster.copy()\ntrain_kmeans_enc = count_encoding(train_kmeans_enc, columns=['cluster'])\ntrain_kmeans_enc = target_encoding(train_kmeans_enc, columns=['cluster'], target=TARGET, seed=SEED)","5c5d2b1e":"run_train(train_kmeans_enc)","cb5d621b":"train_cluster.sort_values(by=['TIME', 'cluster']).head()","6cb185d9":"train_lag = train_cluster.copy()\ntrain_lag = train_lag.sort_values(by=['TIME', 'cluster']).reset_index(drop=True) # TIME\u3068cluster\u3067\u30bd\u30fc\u30c8\nfor col in ['SL', 'BP', 'HR', 'CIRCLUATION']:\n    for i in range(3):\n        lag = i + 1\n        new_colname = col + '_lag' + str(lag)\n        train_lag[new_colname] = train_lag.groupby(['cluster'])[col].shift(lag)\n        \n        new_colname_diff = 'diff_' + col + '_lag' + str(lag)\n        train_lag[new_colname_diff] = train_lag[col] - train_lag[new_colname]\n        \n        new_colname_ratio = 'ratio_' + col + '_lag' + str(lag)\n        train_lag[new_colname_ratio] = train_lag[col] \/ (train_lag[new_colname] + 1e-8)\n        \ntrain_lag = train_lag.fillna(-999)","e577de0d":"run_train(train_lag)","9cbd9722":"def create_stat_feature(df, cols: list):\n    for col in cols:\n        # min, max, std, mean, median\n        new_colname = f'{col}_min_ratio'\n        df[new_colname] = df[col].map(lambda x: x \/ (np.min(x) + 1e-8))\n        new_colname = f'{col}_max_ratio'\n        df[new_colname] = df[col].map(lambda x: x \/ (np.max(x) + 1e-8))\n        new_colname = f'{col}_std_ratio'\n        df[new_colname] = df[col].map(lambda x: x \/ (np.std(x) + 1e-8))\n        new_colname = f'{col}_mean_ratio'\n        df[new_colname] = df[col].map(lambda x: x \/ (np.mean(x) + 1e-8))\n        new_colname = f'{col}_median_ratio'\n        df[new_colname] = df[col].map(lambda x: x \/ (np.median(x) + 1e-8))\n        \n    return df\n\ndef feature_engineering(df):\n    # log\n    EEG_min = df['EEG'].min()\n    df['EEG'] += abs(EEG_min)\n\n    for col in ['SL', 'EEG', 'HR', 'CIRCLUATION']:\n        new_colname = 'log_' + col\n        df[new_colname] = np.log1p(df[col])\n\n    # binning\n    df['log_SL_bins'] = pd.cut(df['log_SL'], bins=4, labels=False)\n    df['log_EEG_bins'] = pd.cut(df['log_EEG'], bins=4, labels=False)\n    df['log_HR_bins'] = pd.cut(df['log_HR'], bins=4, labels=False)\n    df['log_CIRCLUATION_bins'] = pd.cut(df['log_CIRCLUATION'], bins=4, labels=False)\n\n\n    # cliping\n    for col in ['SL', 'EEG', 'HR', 'CIRCLUATION']:\n        p01 = df[col].quantile(0.01)\n        p99 = df[col].quantile(0.99)\n        new_colname = col + '_clipped'\n        df[new_colname] = df[col].clip(p01, p99) # \u4e0a\u754c\u30fb\u4e0b\u754c1%\u3092\u30af\u30ea\u30c3\u30d7\n        \n    # \u7d71\u8a08\u91cf\u7cfb\n    df = create_stat_feature(df, cols=['SL', 'BP', 'HR', 'CIRCLUATION'])\n\n    # \u6841\u5206\u96e2\n    df['SL_float'] = df['SL'].map(lambda x: math.modf(x)[0])\n    df['SL_int'] = df['SL'].map(lambda x: math.modf(x)[1])\n    \n    # kmeans\n    cluster = KMeans(n_clusters=14, max_iter=10000, random_state=SEED)\n    df_kmeans = df[['BP', 'HR', 'CIRCLUATION']]\n    cluster.fit(df_kmeans)\n    df['cluster'] = cluster.predict(df_kmeans)\n    distances = cluster.transform(df_kmeans)\n    dst_columns = [f\"distance_cluster{c:02d}\" for c in range(14)]\n    dst = pd.DataFrame(distances, columns=dst_columns)\n    df = pd.concat([df, dst], axis=1)\n\n    # aggregation\n    df = aggregations(df=df, \n                      cat_features=['log_SL_bins', 'log_EEG_bins', 'log_CIRCLUATION_bins', 'cluster'],\n                      num_features=['SL', 'BP', 'CIRCLUATION'])\n    \n    return df","2904b63c":"train = pd.read_csv(\"..\/input\/cdleyouth01\/df_train.csv\")\ntest = pd.read_csv(\"..\/input\/cdleyouth01\/df_test.csv\")\n# train + test\ndata = pd.concat([train, test], axis=0).reset_index(drop=True)\ndata = feature_engineering(data)","b0cb5165":"train = data[data[TARGET].isnull()==False].reset_index(drop=True)\ntest = data[data[TARGET].isnull()==True].reset_index(drop=True)\n\n\nX = train.drop([INDEX, TARGET], axis=1)\ny = train[TARGET].astype('int')\ntest_id = test[INDEX].values\ntest = test.drop([INDEX, TARGET], axis=1)\n\n# \u7d71\u8a08\u91cf\u8a08\u7b97\u306e\u904e\u7a0b\u3067\u3067\u304d\u305f\u6b20\u640d\u5024\u306a\u3069\u306e\u88dc\u5b8c\uff08\u4eca\u56de\u306f-999\u3067\u57cb\u3081\u307e\u3057\u305f\uff09\nX = X.replace([np.inf, -np.inf], np.nan).fillna(-999)\ntest = test.replace([np.inf, -np.inf], np.nan).fillna(-999)\n\nfeatures = list(X.columns)\n\nseed_everything(seed=SEED)\nfold = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state=SEED)","0ea96985":"oof_train = np.zeros(len(X))\nmodel_array = []\nfor i, (train_idx, val_idx) in enumerate(fold.split(X, y)):\n    print(f'**************** FOLD {i+1} ****************')\n    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n    y_train, y_val = y.values[train_idx], y.values[val_idx]\n\n    # train weight\n    w_tr = 1 \/ pd.DataFrame(y_train).reset_index().groupby(0).count().values\n    w_tr = w_tr[y_train].ravel()\n    w_tr \/= w_tr.sum()\n    train_weight = list(w_tr)\n\n    # test weight\n    w_vl = 1 \/ pd.DataFrame(y_val).reset_index().groupby(0).count().values\n    w_vl = w_vl[y_val].ravel()\n    w_vl \/= w_vl.sum()\n    valid_weight = list(w_vl)\n\n    lgb_train = lgb.Dataset(X_train, label=y_train, weight=train_weight)\n    lgb_val = lgb.Dataset(X_val, label=y_val, weight=valid_weight)\n    \n    model = lgb.train(params,\n                      lgb_train,\n                      valid_names=[\"train\", \"valid\"],\n                      valid_sets=[lgb_train, lgb_val],\n                      num_boost_round=num_boost_round,\n                      early_stopping_rounds=early_stopping_rounds,\n                      verbose_eval=verbose_eval)\n\n    model_array.append(model)\n    \n    # evaluation\n    oof_val = model.predict(X_val, num_iteration=model.best_iteration)\n    y_val_pred = np.argmax(oof_val, axis=1)\n    oof_train[val_idx] = y_val_pred\n    print(f'[FOLD {i+1}] OOF Accuracy: {accuracy_score(y_true=y_val, y_pred=y_val_pred):.5f}\\n')\n\n    # prediction\n    if i == 0:\n        preds = model.predict(test, num_iteration=model.best_iteration)\n    else:\n        preds += model.predict(test, num_iteration=model.best_iteration)\n        \nprint(f'[RESULT] Accuracy: {accuracy_score(y_true=y, y_pred=oof_train):.5f}')\n\npreds = preds \/ NFOLDS\ntest_pred = np.argmax(preds, axis=1)","f53fc7ad":"display_ture_pred(y, oof_train)","de06adbc":"display_cmx(y, oof_train)","f8f7fbb5":"fi_df = lgb_get_feature_importance(model_array, features)\ndisplay_fi(fi_df)","093e5d9e":"sub = pd.DataFrame([])\nsub[INDEX] = test_id\nsub[TARGET] = test_pred\nsub.to_csv('submission.csv', index=False)","98e4d97b":"std = StandardScaler()\nX_std = std.fit_transform(X)\n# test_std = std.transform(test)\nfe_ex = KNNFeatureExtractor(n_neighbors=14, num_class=NUM_CLASSES)\nfe_ex = fe_ex.fit(X_std, y)\nknn_train = fe_ex.transform(X_std)\n# knn_test = fe_ex.transform(test_std)\nX = pd.concat([X, knn_train], axis=1)\n# test = pd.concat([test, knn_test], axis=1)\n\nX[TARGET] = train[TARGET]\nX[INDEX] = train[INDEX]\n\nrun_train(X)","afa431e0":"# \u4eca\u307e\u3067\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u3066\u307f\u308b\n\u73fe\u72b6\u601d\u3044\u3064\u304f\u306e\u306f\u3053\u3053\u307e\u3067\u3067\u3059\u3002\u6b8b\u308a\u6642\u9593\u5c11\u306a\u3044\u3067\u3059\u304c\u3001\u65b0\u3057\u3044\u30a2\u30a4\u30c7\u30a2\u3042\u3063\u305f\u3089\u8a66\u884c\u30fb\u5171\u6709\u3057\u307e\u3059\u3002  \n\u3053\u3053\u3067\u3001\u4eca\u307e\u3067\u4f5c\u3063\u305f\u6709\u52b9\u305d\u3046\u306a\u7279\u5fb4\u91cf\u3092\u7d44\u307f\u8fbc\u3093\u3067\u307f\u3066\u30e2\u30c7\u30eb\u3092\u4f5c\u6210\u30fb\u63d0\u51fa\u3057\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002","93469a63":"\u6bd4\u8f03\u306e\u305f\u3081\u3001\u4f55\u3082\u51e6\u7406\u3057\u306a\u3044\u30e2\u30c7\u30eb\u3092\u304a\u3044\u3066\u304a\u304d\u307e\u3059\u3002","021d449b":"# \u524d\u56de\u4f5c\u3063\u305f\u7279\u5fb4\u91cf\u306f\u3069\u3046\u3060\u3063\u305f\u304b\n### \u624b\u5143\u306e\u30b9\u30b3\u30a2\n**0.73725\u304b\u30890.74668**\u306b\u5411\u4e0a\n### LB\u30b9\u30b3\u30a2\n\u5909\u5316\u306a\u3057","55990f72":"# Kmeans\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0\u306b\u3088\u308b\u7279\u5fb4\u91cf\u4f5c\u6210\n\u30c7\u30fc\u30bf\u304b\u3089\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u500b\u4eba\u3092\u30e9\u30d9\u30ea\u30f3\u30b0\u3057\u307e\u3059\u3002","5b41552a":"\u3067\u306f\u3001\u3053\u306e\u7279\u5fb4\u91cf\u3067LightGBM\u3092\u56de\u3057\u3066\u307f\u307e\u3059\u3002","ae9db1c7":"\u6b8b\u5ff5\u306a\u304c\u3089LB\u30b9\u30b3\u30a2\u306b\u5909\u5316\u306f\u3042\u308a\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u305f\u3060\u3001\u63d0\u51fa\u30d5\u30a1\u30a4\u30eb\u3092\u3088\u304f\u898b\u308b\u3068\u5e7e\u3064\u304b\u4e88\u6e2c\u304c\u5909\u308f\u3063\u3066\u3044\u308b\u306e\u304c\u308f\u304b\u308b\u306e\u3067\u3001\u5168\u304f\u540c\u3058\u4e88\u6e2c\u3092\u3057\u305f\u308f\u3051\u3067\u306f\u306a\u3044\u3067\u3059\u3002  \n\u306a\u306e\u3067\u3001\u524d\u56de\u4f5c\u3063\u305f\u7279\u5fb4\u91cf\u304c\u30e2\u30c7\u30eb\u306e\u7cbe\u5ea6\u5411\u4e0a\u306b\u5bc4\u4e0e\u3057\u3066\u3044\u308b\u304b\u3069\u3046\u304b\u306f\u3001\u306f\u3063\u304d\u308a\u3068\u306f\u308f\u304b\u3089\u306a\u3044\u72b6\u6cc1\u3067\u3059\u3002\u7d50\u69cb\u56f0\u3063\u3066\u307e\u3059\u3002","67b9457b":"### Kmeans\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0 and TIME\u306b\u3088\u308b\u30e9\u30b0\u7279\u5fb4\u91cf\n\u540c\u4e00\u30af\u30e9\u30b9\u30bf\u5185\u3067\u306eTIME\u306b\u3088\u308b\u30e9\u30b0\u7279\u5fb4\u91cf\u3092\u53d6\u3063\u3066\u3044\u304d\u307e\u3059\u3002\u30e9\u30b0\u7279\u5fb4\u91cf\u3068\u306f\u3001\u6642\u7cfb\u5217\u30c7\u30fc\u30bf\u306a\u3069\u3092\u6271\u3046\u969b\u306b\u4f5c\u3089\u308c\u308b\u7279\u5fb4\u91cf\u3067\u3059\u3002\u6642\u9593\u65b9\u5411\u306b\u30c7\u30fc\u30bf\u3092\u305a\u3089\u3057\u305f\u308a\u3001\u305a\u3089\u3057\u305f\u30c7\u30fc\u30bf\u3068\u5404\u6642\u70b9\u3067\u306e\u30c7\u30fc\u30bf\u306e\u5dee\u5206\u3092\u53d6\u308b\u3053\u3068\u3067\u4f5c\u6210\u3057\u307e\u3059\u3002\u8a73\u3057\u304f\u306f[\u3053\u3061\u3089](https:\/\/yolo-kiyoshi.com\/2019\/03\/23\/post-1111\/)\u3092\u53c2\u7167\u304f\u3060\u3055\u3044\u3002","6176a938":"# \u7279\u5fb4\u91cf\u4f5c\u6210\n\u65e9\u901f\u4f5c\u3063\u3066\u3044\u304d\u307e\u3059\u3002\u524d\u56de\u3068\u9055\u3063\u3066\u738b\u9053\u7684\u306a\u624b\u6cd5\u304c\u3042\u308b\u308f\u3051\u3067\u306f\u306a\u3044\u306e\u3067\u3001\u30c7\u30fc\u30bf\u898b\u3064\u3064\u9069\u5f53\u306b\u30b3\u30fc\u30c9\u3092\u7d44\u3093\u3067\u3044\u304d\u307e\u3059\u3002","3b7c9123":"# KNN\u306b\u3088\u308b\u7279\u5fb4\u91cf\u4f5c\u6210\n\u30af\u30e9\u30b9\u30e9\u30d9\u30eb\u306b\u5bfe\u3057\u3066\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u306e\u4eba\u6570\u5206\u306e\u8fd1\u508d\u6570\u3092\u6307\u5b9a\u3057\u3066\u3044\u307e\u3059\u3002  \n\u3053\u308c\u306b\u3088\u3063\u3066\u3001\u30dc\u30e9\u30f3\u30c6\u30a3\u30a2\u500b\u4eba\u306e\u9055\u3044\u3092\u8868\u3059\u3088\u3046\u306a\u7279\u5fb4\u91cf\u3092\u4f55\u3068\u304b\u3057\u3066\u53d6\u308a\u305f\u3044\u3068\u601d\u3044\u307e\u3059\u3002","fe9c720d":"### Kmeans\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0+\u96c6\u7d04\u7d71\u8a08","95ffd222":"### KMeans\u306e\u30e9\u30d9\u30eb\u306e\u307f","2a1ece0c":"\u3042\u3063\u304b\u3057\u307e\u3057\u305f\u3002\u6709\u52b9\u305d\u3046\u306a\u7279\u5fb4\u91cf\u304c\u4f5c\u308c\u3066\u306a\u3044\u307f\u305f\u3044\u3067\u3059\u306d\u3002","6378ef15":"### \u4ee5\u4e0a\u306e\u3069\u3061\u3089\u3082","3435c7d0":"\u60aa\u5316\u3057\u307e\u3057\u305f\u306d\u3002\u305f\u3060\u3001HR\u3092\u8d85\u3048\u308b\u7279\u5fb4\u91cd\u8981\u5ea6\u3092\u3082\u3064\u7279\u5fb4\u91cf\u304c\u3067\u304d\u307e\u3057\u305f\u3002\u3053\u308c\u304c\u6c17\u306b\u306a\u308a\u307e\u3059\u304c\u3001\u60aa\u5316\u3057\u3066\u3044\u308b\u3093\u3067\u3059\u3088\u306d\u3002","0c5e17b2":"knn\u3092\u5165\u308c\u308b\u3068\u30b9\u30b3\u30a2\u304c\u4e0b\u304c\u3063\u3066\u3057\u307e\u3044\u307e\u3059\u304c\u3001\u5404fold\u3054\u3068\u306e\u30b9\u30b3\u30a2\u304c\u307b\u307c\u4e00\u5b9a\u3067\u3042\u308b\u3053\u3068\u304c\u898b\u3066\u53d6\u308c\u307e\u3059\u3002\u60aa\u304f\u306a\u3055\u305d\u3046\u306b\u3082\u601d\u3048\u3066\u304d\u307e\u3059\u304c\u3001\u4eca\u56de\u30b9\u30b3\u30a2\u306e\u9ad8\u304b\u3063\u305fknn\u3092\u9664\u3044\u305f\u65b9\u3067\u63d0\u51fa\u3057\u307e\u3059\u3002","16d51902":"\u4eca\u56de\u3001knn\u3067\u306e\u7279\u5fb4\u91cf\u306f\u5165\u308c\u307e\u305b\u3093\u3067\u3057\u305f\u304c\u3001\u5165\u308c\u305f\u5834\u5408\u306e\u30b9\u30b3\u30a2\u3092\u898b\u3066\u304a\u304d\u307e\u3059\u3002","7b990440":"### KMeans\u306e\u30af\u30e9\u30b9\u30bf\u9593\u8ddd\u96e2\u306e\u307f","120bd855":"\u30c7\u30fc\u30bf\u6570\u304c16382\u3082\u3042\u308b\u306b\u3082\u304b\u304b\u308f\u3089\u305a\u3001HR\u306f644\u901a\u308a\u3057\u304b\u306a\u3044\u3088\u3046\u3067\u3059\u3002CIRCLUATION\u3082\u4f3c\u305f\u3088\u3046\u306a\u611f\u3058\u3067\u3057\u305f\u3002  \nBP\u3082\u4e00\u5b9a\u306a\u3053\u3068\u304c\u591a\u304f\u3001\u76f4\u524d\u306eTIME\u3067\u306eACTIVITY\u306b\u5fdc\u3058\u3066\u5927\u304d\u304f\u306a\u3063\u305f\u308a\u3057\u3066\u305d\u3046\u3067\u3059\u3002","44fe2e13":"\u4f7f\u3044\u65b9\u306f\u3053\u3093\u306a\u611f\u3058","68309e8e":"\u3084\u308a\u65b9\u304c\u60aa\u3044\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u304c\u3001\u3053\u308c\u306f\u52b9\u304b\u306a\u3044\u307f\u305f\u3044\u3067\u3059\u306d\u3002","decbba83":"\u3060\u3044\u3076\u4e0a\u304c\u308a\u307e\u3057\u305f\u3002\u30af\u30e9\u30b9\u30bf\u30e9\u30d9\u30eb\u306b\u5bfe\u3059\u308b\u96c6\u7d04\u7d71\u8a08\u3067\u3053\u3053\u307e\u3067\u4e0a\u304c\u308b\u306a\u3089\u3001\u983b\u5ea6\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3084\u30bf\u30fc\u30b2\u30c3\u30c8\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u3082\u884c\u3051\u308b\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002\u3084\u3063\u3066\u307f\u307e\u3059","a5bf9376":"# \u3053\u306eNoteBook\u3067\u306f\u3001\u3053\u306e\u30b3\u30f3\u30da\u72ec\u81ea\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u3063\u3066\u3044\u304d\u307e\u3059\n\u4eca\u307e\u3067\u306f\u3001\u3069\u306e\u30b3\u30f3\u30da\u3067\u3082\u884c\u308f\u308c\u308b\u4e00\u822c\u7684\u306a\u65b9\u6cd5\u3067\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3057\u307e\u3057\u305f\u3002\u3067\u3059\u304c\u305d\u308c\u3060\u3051\u3067\u306f\u3088\u3044\u7279\u5fb4\u91cf\u304c\u4f5c\u308c\u308b\u3068\u306f\u9650\u3089\u306a\u3044\u3067\u3059\u3002\u305d\u3053\u3067\u3001\u4eca\u56de\u306f\u30b3\u30f3\u30da\u72ec\u81ea\u306e\u7279\u5fb4\u91cf\u3092\u4f5c\u6210\u3057\u3066\u3044\u304d\u307e\u3059\u3002\u306a\u3093\u3068\u306a\u304f\u3053\u3046\u3060\u308d\u3046\u306a\u3001\u3068\u3044\u3046\u611f\u899a\u3067\u4f5c\u3063\u3066\u3044\u304f\u306e\u3067\u7a81\u3063\u8fbc\u307f\u3069\u3053\u308d\u3042\u308b\u3068\u601d\u3044\u307e\u3059\u304c\u304a\u4ed8\u304d\u5408\u3044\u304f\u3060\u3055\u3044\u3002","f51dab28":"\u307b\u3068\u3093\u3069\u5909\u308f\u3089\u306a\u3044\u3067\u3059\u306d\u3002","cb35a572":"### kmeans\u30af\u30e9\u30b9\u30bf\u30ea\u30f3\u30b0+\u5404\u7a2eencoding"}}