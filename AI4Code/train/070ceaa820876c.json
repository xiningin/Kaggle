{"cell_type":{"420ebbc5":"code","747e4d9d":"code","09853937":"code","096c121b":"code","fa6db7e3":"code","b164cadc":"code","fb434e81":"code","876a0126":"code","344fc5d5":"code","076c6585":"code","4765194b":"markdown"},"source":{"420ebbc5":"import numpy as np\nimport pandas as pd \nimport keras\nfrom keras.layers import *\nfrom keras.callbacks import *\nfrom keras.models import *\nfrom keras.losses import *\nfrom keras.optimizers import *\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport glob\n\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n\nX = train_df.drop(['label'], axis=1).values.reshape(-1, 28,28) \/ 255\ny = train_df['label'].values\n\nX_test = test_df.values.reshape(-1, 28,28) \/ 255","747e4d9d":"latent_input = Input(shape=(100,), name='latent_input')\nlabel_input = Input(shape=(1,), name='label_input')\nx = Embedding(input_dim=10, output_dim=10)(label_input)\nx = Reshape((10,))(x)\nx = concatenate([x, latent_input], axis=-1)\nx = Dense(7*7*128)(x)\nx = LeakyReLU()(x)\nx = Reshape((7,7,128))(x)\nx = UpSampling2D()(x)\nx = Conv2D(64, kernel_size=3, strides=1, padding='same')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = UpSampling2D()(x)\nx = Conv2D(32, kernel_size=3, strides=1, padding='same')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Conv2D(1, kernel_size=3, strides=1, padding='same')(x)\nx = Activation('sigmoid')(x)\nx = Reshape((28,28,))(x)\ngenerator = Model(inputs=[latent_input, label_input], outputs=x)\ngenerator.summary()","09853937":"img_input = Input(shape=(28,28,))\nx = Reshape((28,28,1))(img_input)\nx = Conv2D(16, kernel_size=3, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Conv2D(32, kernel_size=3, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Conv2D(64, kernel_size=3, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\nx = BatchNormalization()(x)\nx = Conv2D(128, kernel_size=3, strides=2, padding='same')(x)\nx = LeakyReLU()(x)\nx = Dropout(0.3)(x)\nx = BatchNormalization()(x)\nx = Flatten()(x)\nx = Dense(11, activation='softmax')(x)\ndiscriminator = Model(inputs=img_input, outputs=x)\ndiscriminator.summary()","096c121b":"latent_input = Input(shape=(100,), name='latent_input')\nlabel_input = Input(shape=(1,), name='label_input')\nx = generator([latent_input, label_input])\nx = discriminator(x)\ngan = Model(inputs=[latent_input, label_input], outputs=x)\ndiscriminator.trainable = False\ngan.summary()","fa6db7e3":"discriminator.trainable = True\ndiscriminator.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer=Adam(0.001))\ndiscriminator.trainable = False\ngan.compile(loss='sparse_categorical_crossentropy', metrics=['acc'], optimizer=Adam(0.0003))","b164cadc":"def random_mnist_set(X, y, size=10):\n    length = len(X)\n    indices = np.random.choice(length, size)\n    return X[indices], y[indices]\n\ndef random_generated_set(generator, size=10):\n    latents = np.random.normal(0, 1, (size, 100))\n    return generator.predict_on_batch({'latent_input':latents,\n                      'label_input':np.random.randint(10, size=size)})","fb434e81":"save_interval = 300\nepochs = save_interval*10+1\nbatch_size=20\nhistory = {'dloss':[], 'gloss':[]}\nfor iteration in range(epochs):\n    discriminator.trainable=True\n    X_sample, y_sample = random_mnist_set(X, y, batch_size)\n    dloss_real = discriminator.train_on_batch(X_sample, y_sample)\n    dloss_fake = discriminator.train_on_batch(random_generated_set(generator, size=batch_size), np.full(batch_size, 10))\n    \n    discriminator.trainable=False\n    \n    gan_labels = np.random.randint(10, size=batch_size)\n    gloss = gan.train_on_batch({'latent_input':np.random.normal(0, 1, (batch_size, 100)),\n                        'label_input':gan_labels}, \n                       gan_labels)\n    \n    history['dloss'].append((dloss_real[0]+dloss_fake[0])\/2)\n    history['gloss'].append(gloss[0])\n    if iteration % save_interval == 0:\n        print('generator: loss={}. acc={}'.format(gloss[0], gloss[1]))\n        print('discriminator: loss_real={}, acc_real={}'.format(dloss_real[0],dloss_real[1]))\n        print('    loss_fake={}, acc_fake={}'.format(dloss_fake[0],dloss_fake[1]))\n        generator.save_weights('generator_{0:05d}.h5'.format(iteration))","876a0126":"plt.plot(history['dloss'], 'r')\nplt.plot(history['gloss'], 'b')","344fc5d5":"checkpoints = sorted(glob.glob('generator_*.h5'))\n\nplt.figure(figsize=(10,2*len(checkpoints)))\nfor i,cp in enumerate(checkpoints):\n    generator.load_weights(cp)\n    generated = generator.predict_on_batch({'latent_input':np.random.normal(0, 1, (10, 100)),\n                      'label_input':np.arange(10)})\n    for j, g in enumerate(generated):\n        plt.subplot(len(checkpoints),10,10*i+j+1)\n        plt.imshow(g, cmap='gray')\n        plt.axis('off')","076c6585":"generator.load_weights(checkpoints[-1])\nplt.figure(figsize=(10,10))\nfor i in range(10):\n    generated = generator.predict_on_batch({'latent_input':np.random.normal(0, 1, (10, 100)),\n                      'label_input':np.arange(10)})\n    for j, g in enumerate(generated):\n        plt.subplot(len(checkpoints),10,10*i+j+1)\n        plt.imshow(g, cmap='gray')\n        plt.axis('off')","4765194b":"* [How to Train a GAN? Tips and tricks to make GANs work](https:\/\/github.com\/soumith\/ganhacks)"}}