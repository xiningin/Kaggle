{"cell_type":{"3ec14a65":"code","a698b7ef":"code","759f11b4":"code","56608331":"code","6e9fa1fa":"code","28cdb543":"code","6648d064":"code","70b59e9a":"code","ddd0aed5":"code","4470349c":"code","3918a433":"code","5d2ffd4e":"code","4c16eeec":"code","bac37bd4":"code","56a6911b":"code","5bb30c72":"code","c6e59abe":"code","20834fca":"code","5ff6de04":"code","ef037bbc":"code","a97f4546":"code","65a502e9":"code","67b37d7d":"code","02e6e03b":"code","255c6695":"code","78c6d920":"markdown","0d80bb9c":"markdown","6de395be":"markdown","70faf104":"markdown","03cfc215":"markdown","87dfaaa3":"markdown","1bf3ca02":"markdown","e05fc160":"markdown","73a08226":"markdown","bc9c77fd":"markdown","f507e784":"markdown","713c09ed":"markdown","b4314f6d":"markdown","70ed40b7":"markdown","059eea11":"markdown","e23e69e5":"markdown"},"source":{"3ec14a65":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom plotly.offline import init_notebook_mode, iplot, plot\nimport plotly as py\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nfrom plotly.subplots import make_subplots\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt","a698b7ef":"train = pd.read_csv('\/kaggle\/input\/hr-analytics-analytics-vidya\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/hr-analytics-analytics-vidya\/test.csv')","759f11b4":"train.head()","56608331":"print(\"Train data set dtypes: \\n\")\nprint(f\"Total Cols: {len(train.columns)}\")\nprint(f\"{train.dtypes.value_counts()}\")\nprint('-'*30)","6e9fa1fa":"train.describe()","28cdb543":"data = go.Bar(\n            x=train.isnull().sum().index,\n            y=train.isnull().sum(),\n            name = \"Missing Values\"\n)\n\nlayout = go.Layout(barmode = \"group\")\nfig = go.Figure(data = data, layout = layout)\niplot(fig)","6648d064":"labels = train['is_promoted'].value_counts().index\nvalues = train['is_promoted'].value_counts()\n\nfig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=0.3)])\nfig.show()","70b59e9a":"categorical = train.select_dtypes(include='object')\nfor col in categorical.columns:\n    labels = train[col].value_counts().index\n    values = train[col].value_counts()\n    fig = go.Figure(data=[go.Pie(labels=labels, values=values,hole=0.3,name=col)])\n    fig.show()","ddd0aed5":"numeric_cols = train.select_dtypes(exclude='object')\nnumeric_cols = numeric_cols.drop('employee_id',axis=1)","4470349c":"fig = make_subplots(rows=4, cols=2)\n\ntraces = [\n    go.Histogram(\n        x=train[col], \n        nbinsx=100, \n        name=col\n    ) for col in numeric_cols.columns\n]\n\nfor i in range(len(traces)):\n    fig.append_trace(\n        traces[i], \n        (i \/\/ 2) + 1, \n        (i % 2) + 1\n    )\nfig.update_layout(\n    title_text='Numeric_Values',\n    height=900,\n    width=800\n)\nfig.show()","3918a433":"con_col = ['age','length_of_service','avg_training_score']\nfor col in con_col:\n    fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n    sns.boxplot(y=train[col], data=train, ax=axs[0])\n    sns.boxenplot(y=train[col], data=train, ax=axs[1])\n    sns.violinplot(y=train[col], data=train, ax=axs[2])\n    sns.stripplot(y=train[col], data=train, size=4, color=\".3\", linewidth=0, ax=axs[3])","5d2ffd4e":"cols = categorical.drop('region',axis=1) \nplt.figure(figsize=(20,12))\nj=0\nfor i in cols:\n    j+=1\n    plt.subplot(2,2,j)\n    ax1 = sns.countplot(data=train,x= train[i],hue=\"is_promoted\")\n    #if(j==1):\n    #    plt.xticks( rotation=90)\n    for p in ax1.patches:\n        height = p.get_height()\n        ax1.text(p.get_x()+p.get_width()\/2.,\n                height + 3,\n                '{:1.2f}'.format(height\/len(train),0),\n                ha=\"center\",rotation=0) ","4c16eeec":"plt.figure(figsize=(12,8))\ncorr = numeric_cols.corr()\nsns.heatmap(corr,cmap='Blues',linewidth=0.5,annot=True)","bac37bd4":"train = train.dropna()\ntrain = train[train['length_of_service'] < 35]\nmapp = {'Finance':'Analytics & Others','HR':'Analytics & Others','R&D':'Analytics & Others','Legal':'Analytics & Others','Procurement':'T&P','Technology':'T&P','Sales & Marketing':'S&M',\n       'Operations':'Operations','Analytics':'Analytics & Others'}\ntrain['department'] = train['department'].map(mapp)\ntest['department'] = test['department'].map(mapp)\nmapp_1= {\"Bachelor's\":'UG & Below',\"Master's & above\":\"Master's & above\",'Below Secondary':'UG & Below'}\ntrain['education'] = train['education'].map(mapp_1)\ntest['education'] = test['education'].map(mapp_1)\nmapp_2= {'other':'other','sourcing':'s&r','referred':'s&r'}\ntest['recruitment_channel'] = test['recruitment_channel'].map(mapp_2)\ntrain['age'] = pd.cut(train['age'],bins=[18,30,40,100],labels=['twenties','thirties','forty+'])\ntest['age'] = pd.cut(test['age'],bins=[18,30,40,100],labels=['twenties','thirties','forty+'])\ntrain['length_of_service']= pd.cut(train['length_of_service'],bins=[0,5,100],labels=['<5','5+'])\ntest['length_of_service']= pd.cut(test['length_of_service'],bins=[0,5,100],labels=['<5','5+'])\ntrain['no_of_trainings']= pd.cut(train['no_of_trainings'],bins=[0,1,100],labels=['1','2+'])\ntest['no_of_trainings']= pd.cut(test['no_of_trainings'],bins=[0,1,100],labels=['1','2+'])\ntrain['avg_training_score'] = pd.cut(train['avg_training_score'],bins=[0,50,60,70,80,100],labels=['5','6','7','8','9+'])\ntest['avg_training_score'] = pd.cut(test['avg_training_score'],bins=[0,50,60,70,80,100],labels=['5','6','7','8','9+'])\ntrain = train.drop('region',axis=1)\ntest = test.drop('region',axis=1)","56a6911b":"from sklearn.preprocessing import LabelEncoder\ncats = [c for c in train.columns if (train[c].dtypes =='object' ) ]\nprint('Categories', cats)\n\nfor c in cats:\n    le=LabelEncoder()\n    le.fit(list(train[c].astype('str')) + list(test[c].astype('str')))\n    train[c] = le.transform(list(train[c].astype(str))) \n    test[c] = le.transform(list(test[c].astype(str))) \ntrain.head()","5bb30c72":"train['age'] = le.fit_transform(train['age'])\ntrain['length_of_service'] = le.fit_transform(train['length_of_service'])\ntrain['no_of_trainings'] = le.fit_transform(train['no_of_trainings'])\ntrain['avg_training_score'] = le.fit_transform(train['avg_training_score'])\n\ntest['age'] = le.fit_transform(test['age'])\ntest['length_of_service'] = le.fit_transform(test['length_of_service'])\ntest['no_of_trainings'] = le.fit_transform(test['no_of_trainings'])\ntest['avg_training_score'] = le.fit_transform(test['avg_training_score'])\ntrain = train.drop('employee_id',axis=1)\ntest = test.drop('employee_id',axis=1)","c6e59abe":"print(\"Before Sampling\")\ntrain['is_promoted'].value_counts()","20834fca":"from imblearn.over_sampling import SMOTE\n\ny_train_ada =train['is_promoted']\n\noversampled_df, oversampled_trainY = SMOTE().fit_resample(train, y_train_ada)","5ff6de04":"print(\"After Sampling\")\noversampled_df['is_promoted'].value_counts()","ef037bbc":"plt.figure(figsize=(12,8))\n\nsns.heatmap(oversampled_df.corr(),cmap='Blues',linewidth=0.5,annot=True)","a97f4546":"print(\"Before Sampling\")\ntrain['is_promoted'].value_counts()","65a502e9":"randomn_df = train.sample(frac=1,random_state=4)\n\nis_promoted = randomn_df.loc[randomn_df['is_promoted'] == 1]\n\nnon_promoted = randomn_df.loc[randomn_df['is_promoted'] == 0].sample(n=4232,random_state=42)\n\nundersample_df = pd.concat([is_promoted, non_promoted])","67b37d7d":"print(\"After Sampling\")\nundersample_df['is_promoted'].value_counts()","02e6e03b":"plt.figure(figsize=(12,8))\n\nsns.heatmap(undersample_df.corr(),cmap='Blues',linewidth=0.5,annot=True)","255c6695":"Thank you!","78c6d920":"# Understanding the Data","0d80bb9c":"# Outliers","6de395be":"# Target Column","70faf104":"# Oversampling","03cfc215":"# Bi-Variate Analysis","87dfaaa3":"# Categorical","1bf3ca02":"# Undersampling","e05fc160":"# Numeric","73a08226":"### OUR dataset consists of only roughly 9 percent of positive outcome. This would cause these classifiers to ignore small classes while concentrating on classifying the large ones accurately.\nTWO METHODS TO OVERCOME\n","bc9c77fd":"![](https:\/\/cdn-images-1.medium.com\/max\/1600\/1*P93SeDGPGw0MhwvCcvVcXA.png)","f507e784":"# Data Processing","713c09ed":"Not Promoted is roughly 9 times dominating","b4314f6d":"## Correlation with is_promoted in undersample and oversample is better than the imbalanced datasets","70ed40b7":"NOTE: These are just basic process, main purpose for doing this is for the next step Oversampling and Undersampling","059eea11":"# Sampling","e23e69e5":"# Missing Values"}}