{"cell_type":{"4c79bde8":"code","d33c187f":"code","f3302b73":"code","70d4f160":"code","19f0ad12":"code","c2a78b46":"code","9ec4ffb2":"code","4608d146":"code","b257d680":"code","124269e1":"code","2794c29f":"code","4c023d31":"code","1c794ade":"code","daf07cf8":"code","7a1594df":"code","94541aeb":"code","ca238b82":"code","135def70":"code","ef086b31":"code","361fc185":"code","1368023c":"code","92c13574":"code","fc1fe5fa":"code","fdbed60a":"code","da605540":"code","9fc4d021":"code","c9857507":"code","5348cb12":"code","79ebe58d":"code","7fcc0e42":"code","022b365b":"code","fbd80159":"code","91b11762":"code","e012aefc":"code","04a009a7":"code","ff251b11":"code","c0d4846f":"markdown","3991464a":"markdown","f62e70f7":"markdown","78f171b4":"markdown","6830716b":"markdown","a6caf878":"markdown"},"source":{"4c79bde8":"pip install imutils","d33c187f":"# import the necessary packages\nfrom tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Dense,Dropout\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.models import Sequential\nfrom pandas import DataFrame\nimport matplotlib.pyplot as plt\nimport ast \nimport numpy as np\nimport pandas as pd\nimport cv2\nimport os\nimport random\nfrom imutils import paths\nfrom sklearn.preprocessing import LabelBinarizer","f3302b73":"img_name = list(map(lambda x: x, os.listdir('..\/input\/illinois-doc-labeled-faces-dataset\/front\/front\/')))","70d4f160":"df = pd.DataFrame(data = img_name, index =None, columns = ['filename'])","19f0ad12":"df.insert(1,\"criminal\",\"suclu\")","c2a78b46":"df","9ec4ffb2":"data = []\nlabels = []","4608d146":"print(\"[INFO] loading images...\")\nimagePaths = sorted(list(paths.list_images('..\/input\/illinois-doc-labeled-faces-dataset\/front\/front\/')))","b257d680":"len(imagePaths)","124269e1":"imagePaths[0:5]","2794c29f":"IMAGE_DIMS = (224, 224, 3)","4c023d31":"import random","1c794ade":"# loop over the input images\nfor i in random.sample(range(len(imagePaths)),5000):\n\t# load the image, pre-process it, and store it in the data list\n    img=imagePaths[i]\n    image = cv2.imread(f\"{img}\")\n    image1 = cv2.resize(image,(224,224))\n    image1 = img_to_array(image1)\n    data.append(image1)\n    \n\n\t# extract the class label from the image path and update the\n\t# labels list\n\t# s\u0131n\u0131f etiketini g\u00f6r\u00fcnt\u00fc yolundan \u00e7\u0131kar\u0131n ve etiket listesini g\u00fcncelleyin\n    label = \"suclu\"\n    labels.append(label)","daf07cf8":"len(data)","7a1594df":"labels[0:8]","94541aeb":"# scale the raw pixel intensities to the range [0, 1]\n# ham piksel yo\u011funluklar\u0131n\u0131 [0, 1] aral\u0131\u011f\u0131na \u00f6l\u00e7eklendirin\ndata = np.array(data, dtype=\"float\") \/ 255.0\nlabels = np.array(labels)","ca238b82":"# binarize the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)","135def70":"len(lb.classes_)","ef086b31":"# partition the data into training and testing splits using 80% of\n# the data for training and the remaining 20% for testing\n#% 80'i kullanarak verileri e\u011fitim ve test b\u00f6l\u00fcmlerine ay\u0131r\u0131n\n# e\u011fitim verileri ve kalan% 20 test i\u00e7in\n(trainX, testX, trainY, testY) = train_test_split(data,\n\tlabels, test_size=0.1, random_state=42)","361fc185":"# construct the image generator for data augmentation\n# Veri b\u00fcy\u00fctme i\u00e7in g\u00f6r\u00fcnt\u00fc olu\u015fturucuyu olu\u015fturun\naug = ImageDataGenerator(rotation_range=25, width_shift_range=0.1,\n\theight_shift_range=0.1, shear_range=0.2, zoom_range=0.2,\n\thorizontal_flip=True, fill_mode=\"nearest\")","1368023c":"from tensorflow.keras.applications import VGG16\nfrom tensorflow.keras.layers import Flatten, Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import RMSprop, SGD, Adam, Nadam\nfrom tensorflow.keras.regularizers import l1, l2, L1L2\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom tensorflow.keras.callbacks import EarlyStopping","92c13574":"# load the VGG16 network, ensuring the head FC layers are left off\nvgg = VGG16(weights=\"imagenet\", include_top=False,\n\tinput_tensor=Input(shape=(224, 224, 3)))\n# freeze all VGG layers so they will *not* be updated during the\n# training process\nvgg.trainable = False\n# flatten the max-pooling output of VGG\nflatten = vgg.output\nflatten = Flatten()(flatten)\n# construct a fully-connected layer header to output the predicted\n# bounding box coordinates\nsoftmaxHead = Dense(512, activation=\"relu\")(flatten)\nsoftmaxHead = Dropout(0.5)(softmaxHead)\nsoftmaxHead = Dense(512, activation=\"relu\")(softmaxHead)\nsoftmaxHead = Dropout(0.3)(softmaxHead)\nsoftmaxHead = Dense(len(lb.classes_), activation=\"sigmoid\")(softmaxHead)\n# construct the model we will fine-tune for bounding box regression\nmodel = Model(inputs=vgg.input, outputs=softmaxHead)\nprint(model.summary())","fc1fe5fa":"EPOCHS = 20\nINIT_LR =1e-3\nBS = 32","fdbed60a":"opt = Adam(lr=INIT_LR, decay=INIT_LR \/ EPOCHS)\nmodel.compile(loss=\"mse\", optimizer=opt,metrics=[\"accuracy\"])\n# train the network for bounding box regression\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nprint(\"[INFO] training bounding box regressor...\")\nH = model.fit(\n\tx=aug.flow(trainX, trainY, batch_size=BS),\n\tvalidation_data=(testX, testY),\n\tsteps_per_epoch=len(trainX) \/\/ BS,\n\tepochs=EPOCHS, verbose=1, callbacks = [es])","da605540":"BASE_OUTPUT = '..\/working\/'\nMODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector1.h5\"])\n# serialize the model to disk\nprint(\"[INFO] saving object detector model...\")\nmodel.save(MODEL_PATH, save_format=\"h5\")","9fc4d021":"score = model.evaluate(x=testX,y=testY,batch_size=BS)\nscore\nprint('Score Accuracy : {:.2f}%'.format(score[1]*100))","c9857507":"pred_Y = model.predict(testX, batch_size = BS, verbose = True)\npred_Y_cat = np.argmax(pred_Y, -1)","5348cb12":"from sklearn.metrics import classification_report, confusion_matrix\n\nconfusion_matrix=(confusion_matrix(np.argmax(testY,-1), pred_Y_cat))","79ebe58d":"lb.classes_[:]","7fcc0e42":"import seaborn as sns\nLABELS=['suclu']\nplt.figure(figsize=(10,8))\nsns.heatmap(confusion_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt = 'd')\nplt.title('Confusion Matrix')\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","022b365b":"# import the necessary packages\nfrom tensorflow.keras.preprocessing.image import img_to_array\nfrom tensorflow.keras.preprocessing.image import load_img\nfrom tensorflow.keras.models import load_model\nfrom keras.preprocessing import image\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport numpy as np\nimport mimetypes\nimport argparse\nimport imutils\nimport cv2\nimport os","fbd80159":"print(\"[INFO] loading object detector...\")\nmodel = load_model('.\/detector1.h5')","91b11762":"def predict(imagePath):\n    # load the input image (in Keras format) from disk and preprocess\n    # it, scaling the pixel intensities to the range [0, 1]\n    image = cv2.imread(imagePath)\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    output = image.copy()\n    image = cv2.resize(image, (224, 224))\n    image = image.astype(\"float\") \/ 255.0\n    image = img_to_array(image)\n    image = np.expand_dims(image, axis=0)\n    # make bounding box predictions on the input image\n    proba = model.predict(image)[0]\n    idx = np.argmax(proba)\n    label = lb.classes_[idx]\n    label = \"{} olma ihtimali: {:.2f}% \".format(label, proba[idx] * 100)\n    cv2.putText(output, label, (10, 25),  cv2.FONT_HERSHEY_SIMPLEX,\n        0.7, (255, 0, 0), 2)\n        \n    # show the output image\n    fig=plt.figure(figsize=(10,10))\n    plt.grid(b=None)\n    plt.axis('off')\n    return plt.imshow(output)","e012aefc":"imagePath = \"..\/input\/illinois-doc-labeled-faces-dataset\/front\/front\/A00147.jpg\"\npredict(imagePath)","04a009a7":"imagePath = \"..\/input\/illinois-doc-labeled-faces-dataset\/front\/front\/A01181.jpg\"\npredict(imagePath)","ff251b11":"imagePath = \"..\/input\/illinois-doc-labeled-faces-dataset\/front\/front\/A01759.jpg\"\npredict(imagePath)","c0d4846f":"\u00e7ekilen dosyan\u0131n ne kadar veri i\u00e7erdi\u011fine bakl\u0131yor","3991464a":"\u00e7ekti\u011fimiz isimlerle dataframe olu\u015fturuyoruz","f62e70f7":"dataframe de yeni kolon a\u00e7arak g\u00f6r\u00fcnt\u00fclere su\u00e7lu etiketi ekleniyor.","78f171b4":"burada klas\u00f6rden g\u00f6r\u00fcnt\u00fclerin yollar\u0131n\u0131 \u00e7ekiyoruz daha sonra bu yollar resimleri klas\u00f6rden okumam\u0131za yarayacak","6830716b":"burada klas\u00f6rden g\u00f6r\u00fcnt\u00fclerin isimlerini liste halinde \u00e7ekiyoruz","a6caf878":"burada g\u00f6r\u00fcnt\u00fcler \u00e7ekilerek ileride e\u011fitebilmek i\u00e7in \u00f6zelik ve giri\u015f de\u011ferleri belirleniyor."}}