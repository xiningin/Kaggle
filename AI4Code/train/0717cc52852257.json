{"cell_type":{"0e0580ca":"code","8f2ff19b":"code","955f9a4f":"code","d41fffcc":"code","b0ea16ac":"code","4241bf1c":"code","cc14c694":"code","df931721":"code","3eba1bc0":"code","ae337770":"code","781d515c":"code","a946ea86":"code","9d7b68b9":"code","54f34cc3":"code","ea0ab965":"code","122a2ce6":"code","8fb27d92":"code","ce1dee8d":"code","64a78c88":"code","bd673082":"code","2519642c":"code","834cc511":"code","9cc5a8f6":"code","11ca3abe":"code","6a8a9e42":"code","0737365f":"code","870f7494":"code","5e341169":"code","11ff0978":"code","ea17b925":"code","bd4cdcb6":"code","339d236e":"code","d020a8ec":"code","013c417f":"code","a7dbc72f":"code","cfa2de05":"code","acbfd2ce":"code","9c009005":"code","42ad9731":"code","51ec2b71":"code","e57065ed":"code","b9ca85e1":"code","53ccf56b":"code","e0791e7a":"code","620060ca":"code","f84b71f5":"code","7a6bfccd":"code","db070be8":"code","1b489cae":"code","a6277ef0":"code","77faff54":"code","318794e8":"code","87fb885c":"code","b69f9379":"code","66ccc60a":"code","15077270":"code","9cbd9021":"code","6afa517f":"code","a600a136":"code","8205fc77":"code","3e728b98":"code","4fb7e92b":"code","a9a10e37":"code","2a173e59":"code","671db208":"code","9c2f1b94":"code","373a0f2a":"code","e18260e5":"code","78b7f97e":"code","cc176993":"code","c58b7ef3":"code","a9324381":"code","101a40ad":"code","c7a7ae33":"code","e27114f1":"code","fed5de73":"code","f9182a89":"code","99f06932":"code","8ae9f9a6":"code","b23f1f8f":"code","70fa9602":"code","f9eefc3e":"code","cf79b6ad":"code","4cd77c86":"code","6d6ce568":"code","52d81518":"code","55eeccbf":"code","8745cba2":"code","1a53ba0e":"code","8749c24f":"markdown","eafd6ca0":"markdown","c19ef7f2":"markdown","60413ff0":"markdown","ee90848c":"markdown","4470acab":"markdown","cd038e8c":"markdown","fc2fc4cd":"markdown","dfd95610":"markdown","a3d0c8cb":"markdown","1300c32a":"markdown","791a1534":"markdown","b28a42d4":"markdown","b1d36075":"markdown","1fe8a7b3":"markdown","c033376d":"markdown","e5ad67d1":"markdown","a1912609":"markdown","06a6b475":"markdown","aa689384":"markdown","3f7de7f8":"markdown","edfca26c":"markdown","86673ab7":"markdown","f55b381e":"markdown","9fd8908b":"markdown","87bb686c":"markdown","ba379a00":"markdown","0b4cd35e":"markdown","51691848":"markdown","11cd3203":"markdown","e0e6cbb5":"markdown","11486184":"markdown","fbf52bd7":"markdown","b1f896f4":"markdown","c454537a":"markdown","e8a0e7e9":"markdown","af3efc51":"markdown","a0a3570c":"markdown","64f7cf9b":"markdown","f92912e6":"markdown","079d7613":"markdown","7897b8aa":"markdown","ba460704":"markdown","542bea3b":"markdown","d812dfe7":"markdown","0893a753":"markdown","d9a9cda5":"markdown"},"source":{"0e0580ca":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Reading train data\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\n\n","8f2ff19b":"train.head(5)","955f9a4f":"# Summarie and statistics\ntrain.describe()","d41fffcc":"sns.heatmap(pd.concat(objs=[train, test], axis=0).reset_index(drop=True).isnull(),yticklabels=False,cbar=False,cmap='viridis')","b0ea16ac":"# Missing data\npd.concat(objs=[train, test], axis=0).reset_index(drop=True).isnull().sum()","4241bf1c":"# Outlier detection \nfrom collections import Counter\n\ndef detect_outliers(df,n,features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","cc14c694":"train.loc[Outliers_to_drop] # Show the outliers rows","df931721":"## Join train and test data to apply to both the same data preprocessing\ntrain_len = len(train)\ndf =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)","3eba1bc0":"# Replace empty  with np.NaNs \ndf = df.fillna(np.nan)\n\n# Check for Null values\ndf.isnull().sum()","ae337770":"# Drop outliers\ntrain = train.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","781d515c":"\ndf.drop('PassengerId',axis=1,inplace=True) # Obviously PassengerId is also irrelevant.","a946ea86":"\ng1 = sns.factorplot(x=\"SibSp\",y=\"Survived\",data=train,kind=\"bar\", size = 6)\ng1.set_ylabels(\"survival probability\")","9d7b68b9":"g2 = sns.factorplot(x=\"Parch\",y=\"Survived\",data=train,kind=\"bar\", size = 6)\ng2.set_ylabels(\"survival probability\")","54f34cc3":"# Create a family size feature\ndf[\"Fsize\"] = df[\"SibSp\"] + df[\"Parch\"] + 1","ea0ab965":"# Create new categorical feature describing family size\ndf['Single'] = df['Fsize'].map(lambda s: 1 if s == 1 else 0)\ndf['SmallF'] = df['Fsize'].map(lambda s: 1 if  s == 2  else 0)\ndf['MedF'] = df['Fsize'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ndf['LargeF'] = df['Fsize'].map(lambda s: 1 if s >= 5 else 0)\ndf.head(2)","122a2ce6":"sns.FacetGrid(train, col='Survived').map(sns.distplot, \"Age\")","8fb27d92":"sns.factorplot(y=\"Age\",x=\"Sex\",data=df,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"Pclass\", data=df,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"SibSp\", data=df,kind=\"box\")\nsns.factorplot(y=\"Age\",x=\"Parch\", data=df,kind=\"box\")","ce1dee8d":"# convert Sex into categorical value 0 for male and 1 for female\ndf[\"Sex\"] = df[\"Sex\"].map({\"male\": 0, \"female\":1})\nsns.heatmap(df[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\"]].corr(),annot=True)","64a78c88":"index_NaN_age = list(df[\"Age\"][df[\"Age\"].isnull()].index)\nage_med = df[\"Age\"].median()\nfor i in index_NaN_age :    \n    age_pred = df[\"Age\"][(\n        (df['SibSp'] == df.iloc[i][\"SibSp\"]) & \n        (df['Parch'] == df.iloc[i][\"Parch\"]) & \n        (df['Pclass'] == df.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        df['Age'].iloc[i] = age_pred\n    else :\n        df['Age'].iloc[i] = age_med","bd673082":"#df['Child'] = df['Age'].map(lambda s: 1 if  s <= 8 else 0)\n","2519642c":"# Old approach\n#def impute_age(cols):\n#    Age = cols[0]\n#    Pclass = cols[1]\n#    fclass=train[train['Pclass']==1]['Age'].mean()\n#    sclass=train[train['Pclass']==2]['Age'].mean()\n#    tclass=train[train['Pclass']==3]['Age'].mean()\n#    \n#    if pd.isnull(Age):\n#\n#        if Pclass == 1:\n#            return fclass\n#\n#        elif Pclass == 2:\n#            return sclass\n#\n#        else:\n#            return tclass\n#\n#    else:\n#        return Age\n#train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)\n#train.head(6)","834cc511":"df[\"Fare\"].isnull().sum(),test[\"Fare\"].isnull().sum()","9cc5a8f6":"sns.factorplot(y=\"Fare\",x=\"Sex\",data=df,kind=\"box\")\nsns.factorplot(y=\"Fare\",x=\"Pclass\", data=df,kind=\"box\")\nsns.factorplot(y=\"Fare\",x=\"SibSp\", data=df,kind=\"box\")\nsns.factorplot(y=\"Fare\",x=\"Parch\", data=df,kind=\"box\")","11ca3abe":"df[\"Fare\"] = df[\"Fare\"].fillna(df[\"Fare\"].median())\n\nindex_NaN_age = list(df[\"Fare\"][df[\"Fare\"].isnull()].index)\nage_med = df[\"Fare\"].median()\nfor i in index_NaN_age :    \n    age_pred = df[\"Fare\"][(\n        (df['SibSp'] == df.iloc[i][\"SibSp\"]) & \n        (df['Parch'] == df.iloc[i][\"Parch\"]) & \n        (df['Pclass'] == df.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        df['Fare'].iloc[i] = age_pred\n    else :\n        df['Fare'].iloc[i] = age_med","6a8a9e42":"# Explore Fare distribution \nsns.distplot(df[\"Fare\"], label=\"Skewness : %.2f\"%(df[\"Fare\"].skew())).legend(loc=\"best\")","0737365f":"# Log transformation to reduce skewness of Fare distribution\ndf[\"Fare\"] = df[\"Fare\"].map(lambda i: np.log(i+1))","870f7494":"sns.distplot(df[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(df[\"Fare\"].skew())).legend(loc=\"best\")","5e341169":"sns.barplot(x=\"Sex\",y=\"Survived\",data=train).set_ylabel(\"Survival Probability\")","11ff0978":"g = sns.factorplot(x=\"Pclass\",y=\"Survived\",hue='Sex',data=train,kind=\"bar\", size = 6 , \npalette = \"muted\")\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","ea17b925":"df[\"Embarked\"].value_counts()","bd4cdcb6":"#Fill Embarked NaN  with the most frequent value 'S' \ndf[\"Embarked\"] = df[\"Embarked\"].fillna(\"S\")","339d236e":"g= sns.factorplot(x=\"Embarked\",y=\"Survived\",data=train,kind=\"bar\", size = 6 )\ng.despine(left=True)\ng = g.set_ylabels(\"survival probability\")","d020a8ec":"g=sns.factorplot(\"Pclass\", col=\"Embarked\",  data=train, size=6, kind=\"count\")\ng.despine(left=True)\ng = g.set_ylabels(\"Count\")","013c417f":"df.head(2)","a7dbc72f":"df = pd.get_dummies(df, prefix=\"Embarked\", columns = [\"Embarked\"])\n\n#for elem in df['Embarked'].unique()[:-1]:\n#    df['Embarked_'+str(elem)] = (df['Embarked'] == elem)\/1","cfa2de05":"df.head(2)","acbfd2ce":"df[['Cabin','Ticket']].head()","9c009005":"df[\"Cabin\"][df[\"Cabin\"].isnull()]='X'\ndf[\"Cabin\"]=df[\"Cabin\"].apply(lambda x: x[0])","42ad9731":"sns.factorplot(x=\"Cabin\",y='Survived',data=df,kind=\"bar\", size = 6).set_ylabels(\"survival probability\")","51ec2b71":"sns.countplot(x=\"Cabin\",data=df)","e57065ed":"df[\"Cabin\"]=df[\"Cabin\"].apply(lambda x: \"CA\" if x!='X' else x)","b9ca85e1":"df = pd.get_dummies(df, columns = [\"Cabin\"], prefix=\"Cabin\",drop_first=True)\n\n#for elem in df['Cabin'].unique()[1]:\n#    df['Cabin_'+str(elem)] = (df['Cabin'] == elem)\/1\n#df.drop(['Cabin'],axis=1,inplace=True)","53ccf56b":"df.head(2)","e0791e7a":"Ticket = []\nfor i in list(df.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n        #Ticket.append('Y')\n    else:\n        Ticket.append(\"X\")\n        \ndf[\"Ticket Prefix\"] = Ticket\nplt.figure(figsize=(12,6))\nsns.factorplot(x=\"Ticket Prefix\",y='Survived',data=df,kind=\"bar\", size = 6).set_ylabels(\"survival probability\")","620060ca":"Ticket = []\nfor i in list(df.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n        #Ticket.append('Y')\n    else:\n        Ticket.append(\"X\")\n        \ndf[\"Ticket Prefix\"] = Ticket\nplt.figure(figsize=(12,6))\nsns.factorplot(x=\"Ticket Prefix\",y='Survived',data=df,kind=\"bar\", size = 6).set_ylabels(\"survival probability\")","f84b71f5":"df.drop(['Ticket','Ticket Prefix'],axis=1,inplace=True)","7a6bfccd":"#Create featyre with tittle and throw name\ndf['titles']=df['Name'].apply(lambda x: x.split(' ')[1])","db070be8":"df['titles'].value_counts()","1b489cae":"# Unifyiing equivalent tittles\nequivalent_titles={'der':'Mr.','Mlle.':'Miss.','Mme.':'Mrs.','Don.':'Mr.','Ms.':'Miss.','Dr.':'Master.'}\n\ndf['titles']=df['titles'].apply(lambda x: equivalent_titles[x] if x in equivalent_titles else x)\n\nrare_dict=dict(df['titles'].value_counts()<=8)\nrare_list=[x for x in rare_dict if rare_dict[x]==True]\n\ndf['titles']=df['titles'].apply(lambda x: 'rare' if x in rare_list else x)\n","a6277ef0":"g = sns.factorplot(x=\"titles\",y=\"Survived\",data=df,kind=\"bar\")\n#g = g.set_xticklabels([\"Master\",\"Miss-Mrs\",\"Mr\",\"Rare\"])\ng = g.set_ylabels(\"survival probability\")","77faff54":"for elem in df['titles'].unique():\n    df[str(elem)] = (df['titles'] == elem)\/1","318794e8":"df.head()","87fb885c":" \ndef feat_moder(cols):\n   \n    Age = cols[0]\n    Sex = cols[1]\n    Parch= cols[2]\n    title=cols[3]\n    \n    if Age>=16 and  Parch>0 and Sex==1 and title!='Miss.':\n        return 1\n    else:\n        return 0\n\n#df['Mother'] = df[['Age','Sex', 'Parch','titles']].apply(feat_moder,axis=1)","b69f9379":"df.drop(['Name','titles'],axis=1,inplace=True)\n","66ccc60a":"df.head(2)","15077270":"#df[\"Pclass\"] = df[\"Pclass\"].astype(\"category\")\n#df = pd.get_dummies(df, columns = [\"Pclass\"],prefix=\"Pc\")","9cbd9021":"df.head(2)","6afa517f":"#df.drop(['Parch','SibSp'],axis=1,inplace=True)","a600a136":"train = df[:train_len]\ntest = df[train_len:]\ntest.drop(labels=[\"Survived\"],axis = 1,inplace=True)","8205fc77":"train[\"Survived\"] = train[\"Survived\"].astype(int)\n\nY_train = train[\"Survived\"]\n\nX_train = train.drop(labels = [\"Survived\"],axis = 1)","3e728b98":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import RidgeClassifier\nfrom xgboost.sklearn import XGBClassifier\n\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import RobustScaler","4fb7e92b":"# Create rules to create the cross validations datasets that are shuffled. \n#Cross validation also creates datasets but as far as I'm aware it does not have the shuffle option.\nkfold = StratifiedKFold(n_splits=8,shuffle=True, random_state=42)","a9a10e37":"\nrs = 42\nclrs = []\n\nclrs.append(XGBClassifier())\n\nclrs.append(make_pipeline(RobustScaler(),SVC(random_state=rs)))\nclrs.append(DecisionTreeClassifier(random_state=rs))\nclrs.append(RandomForestClassifier(random_state=rs))\nclrs.append(GradientBoostingClassifier(random_state=rs))\nclrs.append(make_pipeline(RobustScaler() ,KNeighborsClassifier()))\nclrs.append(LogisticRegression(random_state = rs))\nclrs.append(RidgeClassifier(random_state = rs))\n\ncv_results = []\nfor clr in clrs :\n    cv_results.append(cross_val_score(clr, X_train, y = Y_train, scoring = 'f1_weighted', cv = kfold, n_jobs=4))\n\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\n","2a173e59":"cv_df = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algo\":[\"XGBoost\",\"SVC\",\"DecisionTree\",\n\"RandomForest\",\"Gradient Boosting\",\"KNN\",\"Logistic Regression\",'RidgeClassifier']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algo\",data = cv_df,orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\nprint(cv_df)","671db208":"# SVC Parameters tunning \nxgb = XGBClassifier()\n\n\n\n## Search grid for optimal parameters\nxgb_param_grid ={  \n    \"n_estimators\": [90,100,110],\n    'max_depth':[5,10],\n    'min_child_weight':[5,10],\n    \"learning_rate\": [0.05,0.1],\n    'subsample':[0.8],\n    'colsample_bytree':[0.8],\n    \"gamma\": [0],\n    'reg_alpha':[1e-5, 1e-2, 0.1],\n    \"scale_pos_weight\": [1],\n}\n\n\ngsxgb=GridSearchCV(xgb,param_grid = xgb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0,refit=True)\n\ngsxgb.fit(X_train,Y_train)\n\nxgb_best = gsxgb.best_estimator_\n\n# Best score\ngsxgb.best_score_, gsxgb.best_params_","9c2f1b94":"# SVC Parameters tunning \nSVClr = SVC()\n\n\n## Search grid for optimal parameters\nSVClr_param_grid = [{'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100],'probability':[True]}\n                     ,{'C':[1,10],'kernel':['linear'],'probability':[True]}\n                   ]\n\nrobustscaler = RobustScaler().fit(X_train)\nrobustscaled_X=robustscaler.transform(X_train)\n\ngsSVClr=GridSearchCV(SVClr,param_grid = SVClr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0,refit=True)\n\n\n\ngsSVClr.fit(robustscaled_X,Y_train)\n\nSVClr_best = gsSVClr.best_estimator_\n\n# Best score\ngsSVClr.best_score_, gsSVClr.best_params_","373a0f2a":"# DesicionTree Parameters tunning \nDTC = DecisionTreeClassifier()\n\n\n## Search grid for optimal parameters\nDTC_param_grid = {\"max_depth\": [None],\n              \"min_samples_split\": [2, 3, 5],\n              \"min_samples_leaf\": [2, 5, 7],\n              \"criterion\": [\"gini\"],\n               \"random_state\":[42]}\n\n\ngsDTC = GridSearchCV(DTC,param_grid = DTC_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\ngsDTC.fit(X_train,Y_train)\n\nDTC_best = gsDTC.best_estimator_\n\n# Best score\ngsDTC.best_score_, gsDTC.best_params_","e18260e5":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [2, 3],\n              \"min_samples_split\": [7, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [True],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\n\nrf_param_grid = { \n    'max_features':['auto'], 'oob_score':[True], 'random_state':[1],\n    \"criterion\" : [\"gini\", \"entropy\"], \"min_samples_leaf\" : [1, 5], \"min_samples_split\" : [ 4, 10 ], \"n_estimators\": [ 100, 400, 700]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_, gsRFC.best_params_","78b7f97e":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_,gsGBC.best_params_","cc176993":"# KNN Parameters tunning \nKNNclr = KNeighborsClassifier()\n\n\n## Search grid for optimal parameters\nKNNclr_param_grid = {'n_neighbors':np.arange(1,20)}\n                   \n\nrobustscaler = RobustScaler().fit(X_train)\nrobustscaled_X=robustscaler.transform(X_train)\n    \ngsKNNclr=GridSearchCV(KNNclr,param_grid = KNNclr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\n\ngsKNNclr.fit(robustscaled_X,Y_train)\n\nKNNclr_best = gsSVClr.best_estimator_\n\n# Best score\ngsKNNclr.best_score_, gsKNNclr.best_params_","c58b7ef3":"# We can actually check the elbow rule, as we increase complexity (k) here is a point in which the error stabilizes! \n# In fact, I think that the elbow scatter plot of complexity vs cv error, but need to double check\nplt.scatter(gsKNNclr.param_grid['n_neighbors'], 1-gsKNNclr.cv_results_['mean_test_score'])","a9324381":"# Logistic regression Parameters tunning \nLRClr = LogisticRegression()\n\n\n## Search grid for optimal parameters\nLRClr_param_grid = {'penalty':['l2'],'C':[1,10,100],'random_state':[rs]}\n\ngsLRClr=GridSearchCV(LRClr,param_grid = LRClr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\n\n\ngsLRClr.fit(X_train,Y_train)\n\nLRClr_best = gsLRClr.best_estimator_\n\n# Best score\ngsLRClr.best_score_, gsLRClr.best_params_","101a40ad":"# Ridge classication Parameters tunning \nRClr = RidgeClassifier()\n\n\n## Search grid for optimal parameters\nRClr_param_grid = {'alpha':[10, 1,0.1,0.01,0.001],'random_state':[rs],'normalize':[True, False]}\n\ngsRClr=GridSearchCV(RClr,param_grid = RClr_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\n\n\ngsRClr.fit(X_train,Y_train)\n\nRClr_best = gsLRClr.best_estimator_\n\n# Best score\ngsRClr.best_score_, gsRClr.best_params_","c7a7ae33":"from sklearn.ensemble import AdaBoostClassifier, ExtraTreesClassifier\nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 0)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_","e27114f1":"def plot_learning_curve(estimator, title, X, y, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"b\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n\n\n\nplot_learning_curve(gsDTC.best_estimator_,\"DTrees learning curves\",X_train,Y_train,cv=kfold)\nplot_learning_curve(gsGBC.best_estimator_,\"GBoosting \",X_train,Y_train,cv=kfold)\nplot_learning_curve(gsRClr.best_estimator_,\"Logistic learning curves\",X_train,Y_train,cv=kfold)\nplot_learning_curve(gsLRClr.best_estimator_,\"Ridge learning curves\",X_train,Y_train,cv=kfold)\n","fed5de73":"plot_learning_curve(gsRFC.best_estimator_,\"RForest learning curves\",X_train,Y_train,cv=kfold)","f9182a89":"plot_learning_curve(gsxgb.best_estimator_,\"XGB learning curves\",X_train,Y_train,cv=kfold)\nplot_learning_curve(make_pipeline(RobustScaler(),gsKNNclr.best_estimator_),\"KNN learning curves\",X_train,Y_train,cv=kfold)\nplot_learning_curve(make_pipeline(RobustScaler(),gsSVClr.best_estimator_),\"SVC learning curves\",X_train,Y_train,cv=kfold)\n\nplot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\n","99f06932":"test_Survived_EX = pd.Series(ExtC_best.predict(test), name=\"ET\")\ntest_Survived_XGB = pd.Series(xgb_best.predict(test), name=\"XGB\")\ntest_Survived_DT = pd.Series(DTC_best.predict(test), name=\"DT\")\ntest_Survived_RF = pd.Series(RFC_best.predict(test), name=\"RF\")\ntest_Survived_GB = pd.Series(GBC_best.predict(test), name=\"GB\")\ntest_Survived_R = pd.Series(RClr_best.predict(test), name=\"R\")\ntest_Survived_L = pd.Series(LRClr_best.predict(test), name=\"L\")\n\ntest_Survived_KNN = pd.Series(KNNclr_best.predict(robustscaler.transform(test)), name=\"KNN\")\ntest_Survived_SVC = pd.Series(SVClr_best.predict(robustscaler.transform(test)), name=\"SVC\")\n\n\n\n\n\n# Concatenate all classifier results\nensemble_results = pd.concat([test_Survived_EX,test_Survived_XGB,test_Survived_DT,test_Survived_RF,test_Survived_GB,test_Survived_R, test_Survived_L,\n                              test_Survived_KNN,test_Survived_SVC],axis=1)\n\n\nsns.heatmap(ensemble_results.corr(),annot=True)\n","8ae9f9a6":"votingC = VotingClassifier(estimators=[\n     ('ET', ExtC_best)\n     ,('XG', xgb_best)\n    #, ('DT', DTC_best)\n    , ('RF', RFC_best)\n    , ('GB', GBC_best) \n    , ('R',RClr_best)\n    #, ('L',LRClr_best)\n    , ('SVC',make_pipeline(RobustScaler(), SVClr_best))\n    #, ('KNN',make_pipeline(RobustScaler(), KNNclr_best))\n                                      ], voting='soft', n_jobs=4)\n\n#votingC = votingC.fit(X_train, Y_train)","b23f1f8f":"\ncv_result=cross_val_score(votingC, X_train, y = Y_train, scoring = 'f1_weighted', cv = kfold, n_jobs=-1)\n\n    \ncv_result.mean(), cv_result.std()","70fa9602":"votingC = votingC.fit(X_train, Y_train)\ntest_Survived = pd.Series(votingC.predict(test), name=\"Survived\")\n\nIDtest = pd.read_csv('..\/input\/test.csv')[\"PassengerId\"]\nresults_voting = pd.concat([IDtest,test_Survived],axis=1)\n\nresults_voting.to_csv(\"python_voting.csv\",index=False)","f9eefc3e":"from sklearn.model_selection import train_test_split","cf79b6ad":"X_blending_train, X_blending_cv, Y_blending_train, Y_blending_cv = X_train[:450], X_train[450:],  Y_train[:450], Y_train[450:]","4cd77c86":"df_val=X_blending_cv\ndf_test=test\n\nmodels=[\n        ExtC_best\n        #, xgb_best\n        #, DTC_best\n        #, RFC_best\n        , GBC_best\n        , RClr_best\n        #, LRClr_best\n        , make_pipeline(RobustScaler(), SVClr_best)\n        #, make_pipeline(RobustScaler(), KNNclr_best)\n]\n\nfor i,model in enumerate(models):\n    model.fit(X_blending_train, Y_blending_train)\n    \n    val_meta_features=model.predict(X_blending_cv)\n    test_meta_features=model.predict(test)\n   \n    val_meta_features=pd.DataFrame(val_meta_features, index=range(450,450+len(val_meta_features)),columns=['mf_'+'i'])\n    test_meta_features=pd.DataFrame(test_meta_features, index=range(891,891+len(test_meta_features)),columns=['mf_'+'i'])\n    \n    df_val= pd.concat([df_val,val_meta_features],axis=1)\n    df_test= pd.concat([df_test,test_meta_features],axis=1)\n    \n    ","6d6ce568":"\nkfold_blending = StratifiedKFold(n_splits=4,shuffle=True, random_state=42)\n\nmodel = make_pipeline(RobustScaler(), SVClr_best)\n\n\ncv_result=cross_val_score(model, df_val, y = Y_blending_cv, scoring = 'f1_weighted', cv = kfold_blending, n_jobs=-1)\n\n    \ncv_result.mean(), cv_result.std()","52d81518":"model.fit(df_val,Y_blending_cv)\n\ntest_Survived_blending = pd.Series(model.predict(df_test), name=\"Survived\")\n\nresults_blending = pd.concat([IDtest,test_Survived_blending],axis=1)\n\nresults_blending.to_csv(\"python_blending.csv\",index=False)","55eeccbf":"results_voting.head(8)","8745cba2":"results_blending.head(8)","1a53ba0e":"df.head()","8749c24f":"Not quite gaussian but at least skewness was reduced. ","eafd6ca0":"Finally, we again split the train and test sets.","c19ef7f2":"Ha! Always knew that is better to rare than to be a Mr.","60413ff0":"### Embark","ee90848c":"This calls for a new categorical feature that divedes values 0-2 which have higher probs of surviving.","4470acab":"# 3. Feature analysis\/treatment","cd038e8c":"## 8.2 Blending","fc2fc4cd":"# 7. Learning curves (fitting analysis)","dfd95610":"### Parch (# of parents \/ children aboard the Titanic)","a3d0c8cb":"** My getting started competition on the Titanic desaster. It is heavily based on:**\n\n- Udemy course \"Data Science and ML Bootcamp\", by Jose Portilla. \n- The great [Yassine Ghouzam notebook](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)","1300c32a":"Age distribution is the same for both sexes. Meanwhile Pclass and SbSp decrease it and SbSp increase it. The next correlation plot show that how are these variables individually correlated!","791a1534":"## 3.2 Numerical features (Survived, SibSp, Parch, Age, Fare)","b28a42d4":"## 3.1 Drop obviously useless features","b1d36075":"## 3.2 Outlier detection \nTukey method (Tukey JW., 1977) \nSee [Yassine for details](https:\/\/www.kaggle.com\/yassineghouzam\/titanic-top-4-with-ensemble-modeling)\n\n","1fe8a7b3":"Nop, aparently, the prefix does not make a significant difference. So once and for all we thrwo this variable.","c033376d":"### Cross Validation of voting system","e5ad67d1":"### SibSp (# of siblings \/ spouses aboard the Titanic) ","a1912609":"## 2.1 Check for missing data","06a6b475":"# 5. Cross validation\n\nSome popular models:\n\n* Logistic regression\n* Support vector machines\n* Decision Tree\n* Random Forest\n* Gradient Boosting\n* KNN\n* Ridge\n","aa689384":"### Pclass","3f7de7f8":"# 8. Ensemble","edfca26c":"** We see that when it come to surviving, it is good to be a child and not being old. However, since both distributions are peaked around 25's age, age by itself does not tell you much. \n\nThe question at this point is how to fill the missing age data. For this purpose let us compare age with other features**","86673ab7":"### Fare","f55b381e":"# 6. Hyper-Parameter tunning\n\nEach model has hyper-parameter, roughtly speaking these are parameter that control the complexity of the model to avoid overfitting. \n\nAn appropiate ","9fd8908b":"## 4.1 Name","87bb686c":"Apparently, survival prob is higher from C embarked. Interestingly this is not necessary associated with passenger class as the following plots show","ba379a00":"Single ones and large families have a more difficult time surviving! The Parch survival probability distribution has esencially the same shape as the one related to SibSp. For this reason, it make sense to mixed them in so a new feature that separates singles, small and large families as follows:","0b4cd35e":"# 4. Feture engineering","51691848":"\n## 3.2 Categorical values","11cd3203":" # 0. Intro","e0e6cbb5":"## 8.1 Voting","11486184":"In absence of more info, we choose to missing Age data with the median age of similar rows according to Pclass, Parch and SibSp.","fbf52bd7":"# 2. Data preprocessing","b1f896f4":"## 4.3 Scaling data","c454537a":"[Guidelines for tunning XGBoost ](https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/)","e8a0e7e9":"The Fare distribution is very skewed. This can cause overweight high values in the model, even if it is scaled.\nIn this case, it is better to transform it with the log function to reduce this skew.\n","af3efc51":"# 1. Load and import esencial tools\n","a0a3570c":"### Ticket\n\nSeveral tickets have a non-digit prefix, it is of any importance?****","64f7cf9b":"**Blending does not seem to help significantly**","f92912e6":"Again sex is irrelevant, a reasonable strategy would to fill the single missing data with the median of similar passengers. Because this is rather a continues variable let us look at the distrubion:","079d7613":"### Age","7897b8aa":"In the next section we shall use a linear model, for it, we will trow th Parch and SibSp variables.","ba460704":"## 4.2 Create categorical values for Pclass","542bea3b":"Additionally, we create a child categorical feature, because it gives good survival changes","d812dfe7":"This shows that ticket data do not includes cabin data in an obvios way. \n\nSince must people usually did not have a cabin, the NaN calues probably mean this. Let us see is bein a cabin of having nan has any impact on survival prop","0893a753":"### Cabin","d9a9cda5":"### Sex\nNot much to do with this feature. But at least we can have a look at how it influenciates survival"}}