{"cell_type":{"9d3bbc0f":"code","b0837f48":"code","e3128525":"code","2d7c62dc":"code","bac03502":"code","3a705d57":"code","bc6ea87c":"code","af457e97":"code","6f7bb53e":"code","c73e7b78":"code","52235d7f":"code","37df2f72":"code","36b92d51":"code","0b803fb9":"code","2c010476":"code","1ce8785c":"code","6d588e33":"code","c3ca579b":"code","dc05d8bd":"code","8756a020":"markdown","9536485d":"markdown","2e70cd34":"markdown","b55847c4":"markdown","f2362059":"markdown","c9735571":"markdown","5c181da8":"markdown","75407d78":"markdown"},"source":{"9d3bbc0f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# Restrict minor warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# to display all outputs of one cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\npd.options.display.max_columns = 100\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer as CTT\nfrom sklearn.preprocessing import StandardScaler as ss, OneHotEncoder as ohe\nfrom sklearn.ensemble import ExtraTreesClassifier as etc,RandomForestClassifier as rf\nfrom sklearn.ensemble import AdaBoostClassifier as adc,BaggingClassifier as bgc\nfrom imblearn.ensemble import BalancedRandomForestClassifier as brf\nfrom sklearn.ensemble import GradientBoostingClassifier as GBC\nfrom imblearn.ensemble import EasyEnsembleClassifier\nfrom imblearn.ensemble import RUSBoostClassifier\n\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import TomekLinks\nfrom imblearn.combine import SMOTETomek\n\nfrom imblearn.metrics import classification_report_imbalanced","b0837f48":"df = pd.read_csv('..\/input\/caravan-insurance-challenge\/caravan-insurance-challenge.csv')\nbase = df[df['ORIGIN']=='train']\nvldn = df[df['ORIGIN']=='test']\n_=base.pop('ORIGIN')\n_=vldn.pop('ORIGIN')\ny_base=base.pop('CARAVAN')\ny_vldn=vldn.pop('CARAVAN')","e3128525":"base.describe()","2d7c62dc":"base.columns[base.isna().sum()>0]","bac03502":"pd.crosstab(df.ORIGIN,df.CARAVAN)","3a705d57":"pd.DataFrame(df.nunique()).T\npd.DataFrame(base.nunique()).T\npd.DataFrame(vldn.nunique()).T\n","bc6ea87c":"ax= plt.axes()\n_=sns.heatmap(df.drop(columns=['ORIGIN']).corr(),cmap='Oranges',cbar=None,ax=ax)\n_=ax.set_title('Correlation Heatmap')","af457e97":"f,axs=plt.subplots(1,2,figsize=(12,6))\nsns.heatmap(df.iloc[:,1:44].corr(),ax=axs[0],vmin=-2, vmax=2,cbar=None)\nsns.heatmap(df.iloc[:,44:-1].corr(),ax=axs[1],vmin=-2, vmax=2,cbar=None)\n","6f7bb53e":"cat_col = ['MOSTYPE','MOSHOOFD']\nnum_cols = list(base.columns.values[43:])\nctt = CTT([('ss',ss(),num_cols),\n           ('ohe',ohe(),cat_col)],remainder='passthrough')","c73e7b78":"cat_col = ['MOSTYPE','MOSHOOFD']\nnum_cols = list(base.columns.values[43:])\nctt = CTT([ ('ss',ss(),num_cols),\n           ('ohe',ohe(),cat_col)],remainder='passthrough')\n","52235d7f":"sm_trainX , sm_trainY = SMOTE(random_state=42).fit_resample(base,y_base)\ntm_trainX ,tm_trainY = TomekLinks().fit_resample(base,y_base)\ncmb_trainX, cmb_trainY = SMOTETomek(random_state=42).fit_resample(base,y_base)","37df2f72":"print('Imbalanced Sample')\ny_base.value_counts()\nprint('Over_sampled Sample')\nsm_trainY.value_counts()\nprint('Under_Sampled Sample')\ntm_trainY.value_counts()\nprint('Combine_sampled Sample')\ncmb_trainY.value_counts()","36b92d51":"from sklearn.metrics import confusion_matrix,roc_curve,roc_auc_score,classification_report\nimport itertools\ndef plot_confusion_matrix(y_true, y_pred, classes, ax=None, cmap=plt.cm.Blues):\n    \n    cm = confusion_matrix(y_true, y_pred)\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        \n    print(classification_report_imbalanced(y_true,y_pred))\n    \n    fig, ax = (plt.gcf(), ax)\n    \n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.set_title('Confusion Matrix')\n\n    tick_marks = np.arange(len(classes))\n    ax.set_xticks(tick_marks)\n    ax.set_xticklabels(classes, rotation=45)\n    ax.set_yticks(tick_marks)\n    ax.set_yticklabels(classes)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        ax.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    fig.tight_layout()\n    ax.set_ylabel('True label')\n    ax.set_xlabel('Predicted label')\ndef plot_roc(y_true, y_pred, ax=None):\n    \"\"\"Plot ROC curve\"\"\" \n    false_positive_rate, true_positive_rate, threshold = roc_curve(y_true, y_pred)\n    roc_score = roc_auc_score(y_true,y_pred)\n    \n    fig, ax = (plt.gcf(), ax) if ax is not None else plt.subplots(1,1)\n\n    ax.set_title(\"Receiver Operating Characteristic\")\n    ax.plot(false_positive_rate, true_positive_rate)\n    ax.plot([0, 1], ls=\"--\")\n    ax.plot([0, 0], [1, 0] , c=\".7\"), plt.plot([1, 1] , c=\".7\")\n    ax.annotate('ROC: {:.5f}'.format(roc_score), [0.75,0.05])\n    ax.set_ylabel(\"True Positive Rate\")\n    ax.set_xlabel(\"False Positive Rate\")\n    fig.tight_layout()\n    return roc_score\ndef feat_imps(model, X_train, plot=False, n=None):\n    \"\"\" Dataframe containing each feature with its corresponding importance in the given model\"\"\"\n    fi_df = pd.DataFrame({'feature':X_train.columns,\n                          'importance':model.feature_importances_}\n                        ).sort_values(by='importance', ascending=False)\n    if plot:\n        fi_df[:(n if n is not None else 15)].plot.bar(x='feature',y='importance')\n    else:\n        return fi_df\ndef plot_cmroc(y_true, y_pred, classes=[0,1], normalize=True):\n    \"\"\"Convenience function to plot confusion matrix and ROC curve \"\"\"\n    fig,axes = plt.subplots(1,2, figsize=(9,4))\n    plot_confusion_matrix(y_true, y_pred, classes=classes, ax=axes[0])\n    roc_score = plot_roc(y_true, y_pred, ax=axes[1])\n    fig.tight_layout()\n    plt.show()\n    return roc_score","0b803fb9":"\nInteractiveShell.ast_node_interactivity = \"last\"","2c010476":"from sklearn.metrics import recall_score,make_scorer,fbeta_score\ndef recall_1(y_true, y_pred):\n    tp,fp,fn,tn = confusion_matrix(y_true, y_pred).ravel()\n    return tn\/(tn+fp)\n\nrecall_class_1 = make_scorer(recall_1, greater_is_better=True)\n\ndef f2(y_true, y_pred):\n    return fbeta_score(y_true, y_pred,beta=2)\n\nf2_score = make_scorer(f2, greater_is_better=True)","1ce8785c":"from sklearn.metrics import fbeta_score\nfrom sklearn.model_selection import GridSearchCV\nestimators = [rf(),brf(),adc()]\nn_est = [50,55,60,65]\nrep = []\nfor e in estimators:\n    pipe = Pipeline([\n                    ('ct',ctt),\n                    ('e',e)\n                ])\n    rcv = GridSearchCV(pipe,{'e__n_estimators':n_est},scoring=recall_class_1,cv=5)\n    rcv.fit(base,y_base)\n    rep.append(['Biased data',e.__class__.__name__,rcv.best_params_['e__n_estimators'],rcv.best_score_])\n    rcv.fit(tm_trainX,tm_trainY)\n    rep.append(['Tomek Link',e.__class__.__name__,rcv.best_params_['e__n_estimators'],rcv.best_score_])\n    rcv.fit(sm_trainX,sm_trainY)\n    rep.append(['SMOTE',e.__class__.__name__,rcv.best_params_['e__n_estimators'],rcv.best_score_])\nrep  ","6d588e33":"pipe = Pipeline([\n                    ('ct',ctt),\n                    ('e',rf(random_state=0,n_jobs=-1))\n                ])\n\npipe.fit(cmb_trainX,cmb_trainY)\nplot_cmroc(y_base,pipe.predict(base))\nplot_cmroc(y_vldn,pipe.predict(vldn))","c3ca579b":"rus = RUSBoostClassifier(n_estimators=10,\n                         base_estimator=bgc(n_estimators=10,base_estimator=brf(random_state=0,n_jobs=-1)))\npipe = Pipeline([('ct',ctt),('e',rus)])\npipe.fit(cmb_trainX,cmb_trainY)\nplot_cmroc(y_base,pipe.predict(base))\nplot_cmroc(y_vldn,pipe.predict(vldn))","dc05d8bd":"eec=EasyEnsembleClassifier(base_estimator=GBC(),n_jobs=-1, random_state=0,sampling_strategy='majority')\npipe_eec = Pipeline([('ct',ctt),('e',eec)])\npipe_eec.fit(base,y_base)\nplot_cmroc(y_base,pipe_eec.predict(base))\nplot_cmroc(y_vldn,pipe_eec.predict(vldn))\n","8756a020":"## <a id='1'>Import Packages<\/a>","9536485d":"## Preprocessing\nIn last section the imabalance of data is established that warrants special treatment(Over\/Under-Sampling). The Categorical columns do need conversion like OneHotEncoding(in case of L0 and L2) or replacement with mean of brackets(in case of L1). Numrical columns may directly be scaled.","2e70cd34":"Now we can pipeline the samples and fit them.","b55847c4":"After creating the column transformer we need the dataset to perform the transformation. Here we can resample the dataset. We are creating here three types of samples - oversampled, undersampled and combined.","f2362059":"#### **OBSERVATIONS MADE**\n - No Null values\n - Data is imbalanced,too few observations for CARAVAN=1\n - Numerical columns have lesser values while categorical columns are having larger valueset.\n - Numerical & categoical features both are showing correlation with their kind only.","c9735571":"# Caravan Insurance:Identify potential purchasers\n\nThe data used here was published by P. van der Putten and M. van Someren (eds) and  has been used in the CoIL Challenge 2000 datamining competition at http:\/\/www.wi.leidenuniv.nl\/~putten\/library\/cc2000\/. For brevity the metadata of dataset is not described here although the same is available at http:\/\/kdd.ics.uci.edu\/databases\/tic\/dictionary.txt\n\nTo identify the potential purchasers classification techniques can be used and then we can tune hyper-parameters to maximize the accuracy. Here as data is *quite* imbalanced, we can utilise over-sampling or under-sampling techniques. For learning purpose we will use here all three states of data,***viz***,\n - Unbalanced\n - Over-Sampled\n - Under-Sampled\n\nAlso, to get better results we will use different classification techniques,***viz***,\n  - Extremely Random Forest\n  - Random Forest\n  - Balanced Random Forest\n  - AdaBoost\n  - Bagging Classifier\n<hr>","5c181da8":"### <a id='2.1'>Get an insight<\/a>\nNow we try to get an insight of the training dataset. Here we try to get basic datatype information, distributions, presence of nulls etc.","75407d78":"## <a id='2'>Read The data<\/a>\nIn this section data is read and divided into two parts: *One* the base dataset to train the model and *Second* the validation dataset. Since the dataset is having first column **ORIGIN** just to identify whether it's for training or for testing, it is dropped. Also, the target feature **CARAVAN** is popped for further use."}}