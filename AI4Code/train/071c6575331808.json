{"cell_type":{"7ed50982":"code","e248e134":"code","dd670736":"code","18c3c2b4":"code","96d578d6":"code","c2fe0ed3":"markdown"},"source":{"7ed50982":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e248e134":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split,GridSearchCV\nfrom sklearn.metrics import recall_score,roc_curve,roc_auc_score,plot_confusion_matrix\n%matplotlib inline\n\ndf = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf.head()","dd670736":"#cat_data\ncat_list = df.select_dtypes(include=\"object\").columns.tolist()\nfor cat in cat_list:\n    df[cat] = df[cat].apply(lambda x:cat+\"_\"+x)\n    a = pd.get_dummies(df[cat])\n    df = pd.concat([df,a],axis=1)\n    df.drop(columns=cat,inplace=True)\n#num_cat\nnum_list= [\"Age\",\"RestingBP\",\"Cholesterol\",\"FastingBS\",\"MaxHR\",\"Oldpeak\"]\nfor i in num_list:\n    mean = df[i].mean()\n    std = df[i].std()\n    df[i] = df[i].map(lambda x:(x-mean)\/std)\nY = df[\"HeartDisease\"]\nX = df.drop(columns=[\"HeartDisease\"])\nxtrain,xtest,ytrain,ytest = train_test_split(X,Y,test_size=0.25,random_state=123)\n\nmlp = MLPClassifier(hidden_layer_sizes=(100,50,10),solver=\"lbfgs\")\nmlp.fit(xtrain,ytrain)\ny_pre = mlp.predict(xtest)\nrecall_mlp = round(recall_score(ytest,y_pre),2)\nmlp_prob = mlp.predict_proba(xtest)[:,1]\nmlp_auc = round(roc_auc_score(ytest,mlp_prob),2)\nmlp_score = round(mlp.score(xtest,ytest),2)\nmlp_dic = {\"recall_score\":recall_mlp,\"score\":mlp_score,\"auc\":mlp_auc}\nprint('')\nprint('--------------------------------')\nprint(\"score:\",mlp_score)\nprint(\"recall_call:{}.\".format(recall_mlp))\nprint(\"auc_score:\",mlp_auc)\nprint('')\nprint('-------------------------------')\nplot_confusion_matrix(mlp,xtest,ytest)\nplt.show()","18c3c2b4":"logist = LogisticRegression(random_state=123)\nparam_grid = {\n    \"C\":[0.01,0.03,0.05,0.04,1],\"multi_class\":['auto','ovr','multinomial'],\"max_iter\":[100,200,500,600,1000,1500,2000]\n}\ngrid = GridSearchCV(estimator=logist,param_grid=param_grid)\ngrid.fit(xtrain,ytrain)\nlogs=grid.best_estimator_\nprint(logs)\nlogs.fit(xtrain,ytrain)\ny_pre_log = logs.predict(xtest)\nrecall_log = round(recall_score(ytest,y_pre_log),2)\nlog_prob = logs.predict_proba(xtest)[:,1]\nlog_auc = round(roc_auc_score(ytest,log_prob),2)\nlog_score = round(logs.score(xtest,ytest),2)\nlog_dic = {\"recall_score\":recall_log,\"score\":log_score,\"auc\":log_auc}\nprint('')\nprint('--------------------------------')\nprint(\"score:\",log_score)\nprint(\"recall_call:{}.\".format(recall_log))\nprint(\"auc_score:\",log_auc)\nprint('')\nprint('-------------------------------')\nplot_confusion_matrix(logs,xtest,ytest)\nplt.show()","96d578d6":"y = df[\"HeartDisease\"]\nX = df.drop(columns=[\"HeartDisease\"])\ndef split_X(model,X,y):\n    for train_index, test_index in model.split(X):\n        X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n        y_train, y_test = y[train_index], y[test_index]\n        return X_train, X_test,y_train, y_test\ndef split_X_y(model,X,y):\n    for train_index, test_index in model.split(X,y):\n        X_train, X_test = X.loc[train_index,:], X.loc[test_index,:]\n        y_train, y_test = y[train_index], y[test_index]\n        return X_train, X_test,y_train, y_test\n    \nmodel=LogisticRegression(C=0.03, multi_class='multinomial', random_state=123,max_iter=1000)\nX_train, X_test, y_train, y_test = train_test_split( X, y, test_size=0.33, random_state=42, stratify=y)\nclf=model.fit(X_train, y_train)\ny_pred=clf.predict(X_test)\nscore=clf.score(X_test, y_test)\n\nfrom sklearn.model_selection import StratifiedKFold\nskf = StratifiedKFold(n_splits=2)\nX_train, X_test,y_train, y_test=split_X_y(skf,X,y)\nclf=model.fit(X_train, y_train)\ny_pred1=clf.predict(X_test)\nscore1=clf.score(X_test, y_test)\n\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nrskf = RepeatedStratifiedKFold(n_splits=2, n_repeats=2,random_state=36851234)\nX_train, X_test,y_train, y_test=split_X_y(rskf,X,y)\nclf=model.fit(X_train, y_train)\ny_pred2=clf.predict(X_test)\nscore2=clf.score(X_test, y_test)\n\nfrom sklearn.model_selection import RepeatedKFold\nrkf = RepeatedKFold(n_splits=2, n_repeats=2, random_state=2652124)\nX_train, X_test,y_train, y_test=split_X_y(rkf,X,y)\nclf=model.fit(X_train, y_train)\ny_pred3=clf.predict(X_test)\nscore3=clf.score(X_test, y_test)\n\nfrom sklearn.model_selection import KFold\nkf = KFold(n_splits=10)\nX_train, X_test,y_train, y_test= split_X(kf,X,y)\nclf=model.fit(X_train, y_train)\ny_pred4=clf.predict(X_test)\nscore4=clf.score(X_test, y_test)\n\ndata={'score':[score,score1,score2,score3,score4],'sampling':['train_test_split','StratifiedKFold','RepeatedStratifiedKFold','RepeatedKFold','KFold']}\ndf=pd.DataFrame(data)\ndf.sort_values(by='score', axis=0, ascending=False,inplace=True)\ndf","c2fe0ed3":"# Reference\n\nhttps:\/\/www.kaggle.com\/kgxiao\/eda-logistic-regression-recall-rate-91"}}