{"cell_type":{"9fc3e77f":"code","da7bb825":"code","ef8be3e7":"code","1b873a81":"code","60204748":"code","460f6c82":"code","c87541f1":"code","140b49ce":"code","066ebd99":"code","2dff6d6b":"code","69bfb7f0":"code","f785d1d1":"code","ca2acb4d":"code","9790b02c":"code","4f340f0c":"code","f7501f22":"code","0b11feeb":"code","bdda3c5f":"code","f6ff4301":"markdown","0fb7849d":"markdown","56451b8c":"markdown","5fa16291":"markdown","600837ca":"markdown","c403777a":"markdown","6dc43442":"markdown","7f65dcbd":"markdown","27d76890":"markdown","13442573":"markdown","fea4f386":"markdown","5bbdcde6":"markdown","e6074f7a":"markdown","82e229e1":"markdown","73984a6f":"markdown"},"source":{"9fc3e77f":"# Lets load libraries\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import classification_report,confusion_matrix\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.model_selection import cross_validate, cross_val_score, train_test_split\n\nimport pickle","da7bb825":"# loading dataset\nsdata = pd.read_csv('..\/input\/star-type-classification\/Stars.csv')","ef8be3e7":"# rename columns (because I like full names :)\nsdata.rename(columns = {'L':'L\/Lo -lumin','R':'R\/Ro -rad','A_M':'Mv -magn'}, inplace = True)\nsdata.info()","1b873a81":"sdata.head(5)","60204748":"# visualize missing data\nsns.heatmap(sdata.isnull(),yticklabels=False,cbar=False,cmap='viridis')","460f6c82":"# pairplots for numeric data\nsns.pairplot(sdata, kind = 'reg')","c87541f1":"# lets calculate pairs correlation and build a heatmap (code taken from Kaggle-user ChrisX, https:\/\/www.kaggle.com\/docxian\/star-type-classification)\n\nfeatures_num = ['Temperature', 'L\/Lo -lumin', 'R\/Ro -rad', 'Mv -magn']\n\n# calc correlation matrices\ncorr_pearson = sdata[features_num].corr(method='pearson')         # Pearson's corr - shows the linear relationship \ncorr_spearman = sdata[features_num].corr(method='spearman')       # Spearman's corr - shows monotonic relationship\n\n# and plot side by side\nplt.figure(figsize=(15,5))\nax1 = plt.subplot(1,2,1)\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\n\nax2 = plt.subplot(1,2,2, sharex=ax1)\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","140b49ce":"g = sns.catplot(data=sdata, x='Color', y=\"Type\")\ng.fig.set_figwidth(7)\ng.fig.set_figheight(5)\ng.set_xticklabels(rotation = 90)","066ebd99":"# Spectral class categorical plot\ng = sns.catplot(data=sdata, x='Spectral_Class', y=\"Type\")\ng.fig.set_figwidth(7)\ng.fig.set_figheight(5)\ng.set_xticklabels(rotation = 0)","2dff6d6b":"sdata.info()","69bfb7f0":"# these are basic colors categories, we will make separated columns for them\n# the transformation done below is a controversial decision, because I reduced total numbers of colors \n# and redistributed difficult colors into these simle basic categories. This redistribution is not fully\n# based on physical approach, where we have no special spectral definition of some taken colors (like \"Pale\").\n\nbasic_colors = {'RED','ORANGE','YELLOW','GREEN','BLUE','WHITE','PALE'}\nzero_list = [0]*len(sdata)\n\nfor col in basic_colors:\n    sdata[col] =  zero_list\n\n\n# I am always lazy, so copy-pasting is my love    \nsdata.loc[sdata.Color == 'Red','RED'] = 1\nsdata.loc[sdata.Color == 'White',['WHITE']] = 1\nsdata.loc[sdata.Color == 'Blue White',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Yellowish White',['YELLOW','WHITE']] = 1\nsdata.loc[sdata.Color == 'Blue white',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Pale yellow orange',['PALE','YELLOW','ORANGE']] = 1\nsdata.loc[sdata.Color == 'Blue',['BLUE']] = 1\nsdata.loc[sdata.Color == 'Blue-white',['BLUE','WHITE']] = 1\nsdata.loc[sdata.Color == 'Whitish',['WHITE']] = 1\nsdata.loc[sdata.Color == 'yellow-white',['YELLOW','WHITE']] = 1\nsdata.loc[sdata.Color == 'Orange',['ORANGE']] = 1\nsdata.loc[sdata.Color == 'White-Yellow',['WHITE','YELLOW']] = 1\nsdata.loc[sdata.Color == 'white',['WHITE']] = 1\nsdata.loc[sdata.Color == 'yellowish',['YELLOW']] = 1\nsdata.loc[sdata.Color == 'Yellowish',['YELLOW']] = 1\nsdata.loc[sdata.Color == 'Orange-Red',['ORANGE','RED']] = 1\nsdata.loc[sdata.Color == 'Blue-White',['WHITE','BLUE']] = 1          \n          \nsdata","f785d1d1":"# replace Spectral_Class cat to numerical\ns_class = pd.get_dummies(sdata['Spectral_Class'], drop_first = True)\nsdata = pd.concat([sdata,s_class], axis = 1)\nsdata","ca2acb4d":"# make a copy of our dataset not to reload the main if we do something wrong :)))\ntdata = sdata.drop(['Color','Spectral_Class'], axis = 1).copy()","9790b02c":"# train data and target separation\nx_data = tdata.drop('Type', axis = 1)\ny_data = tdata['Type']","4f340f0c":"# model creation\n# we will take the Gradient boosting classifier\n \ngbc = GradientBoostingClassifier(loss = 'deviance', max_depth=3, n_estimators=400, learning_rate = 0.085,\n                                 min_samples_leaf = 1, max_features = 'log2')  #GradientBoosting model\n","f7501f22":"# our dataset is not big, so as I understand, there is no reason to divide it in more than 5 folds in a k-fold validation\n# we can play with this number to see how the accuracy changes\n\nmodel = gbc\nfolds_n = 5\ncv_results = cross_val_score(model, x_data, y_data, cv = folds_n, scoring=\"accuracy\",n_jobs=-1)\nprint('min accuracy= {v}'.format(v = np.min(cv_results)))\nprint('avg accuracy= {v}'.format(v = np.mean(cv_results)))\nprint('max accuracy= {v}'.format(v = np.max(cv_results)))\n","0b11feeb":"model.fit(x_data,y_data)\n\n# save the model to disk  (see https:\/\/machinelearningmastery.com\/save-load-machine-learning-models-python-scikit-learn\/)\nfilename = 'star_classifier.sav'\npickle.dump(model, open(filename, 'wb'))","bdda3c5f":"feature_importance = model.feature_importances_\nsorted_idx = np.argsort(feature_importance)\npos = np.arange(sorted_idx.shape[0]) + .5\nfig = plt.figure(figsize=(12, 6))\nplt.subplot(1, 2, 1)\nplt.barh(pos, feature_importance[sorted_idx], align='center')\nplt.yticks(pos, np.array(x_data.columns)[sorted_idx])\nplt.title('Feature Importance (MDI)')","f6ff4301":"# Thank you for attention :) Please judge me, but not strictly, I only study :)","0fb7849d":"#### Colors are a bit messy. We will replace them with numerical columns later","56451b8c":"### Common data","5fa16291":"## Exploratory data analysis","600837ca":"#### seems Type correlates with absolute magnitude","c403777a":"# Model fitting","6dc43442":"### Numeric data","7f65dcbd":"#### Now lets see feature importance in the trained model (https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_gradient_boosting_regression.html)","27d76890":"### Categorical values to numerical","13442573":"This classifier uses stars type data from https:\/\/www.kaggle.com\/brsdincer\/star-type-classification\n\nData description:\n\nTemperature -- K\nL -- L\/Lo - relative luminocity (in the model renamed to L\/Lo -lumin)\n\nR -- R\/Ro - relative radius (in the model renamed to R\/Ro -rad)\n\nAM -- Mv - magnitude (in the model renamed to Mv -magn)\n\nColor -- General Color of Spectrum\n\nSpectral_Class -- O,B,A,F,G,K,M \/ SMASS - https:\/\/en.wikipedia.org\/wiki\/Asteroid_spectral_types\n\nType -- Red Dwarf, Brown Dwarf, White Dwarf, Main Sequence , Super Giants, Hyper Giants\n\n\nTARGET:\nType\n\nfrom 0 to 5\n\nRed Dwarf - 0\n\nBrown Dwarf - 1\n\nWhite Dwarf - 2\n\nMain Sequence - 3\n\nSuper Giants - 4\n\nHyper Giants - 5\n\n\nMATH:\n\nLo = 3.828 x 10^26 Watts (Avg Luminosity of Sun)\n\nRo = 6.9551 x 10^8 m (Avg Radius of Sun)","fea4f386":"### Categorical data","5bbdcde6":"# Stars type classifier","e6074f7a":"## Loading libraries and dataset","82e229e1":"#### Looks too optimistic (at different runs). But even if we change the number of folds to 3 (biger test, smaller train) the accuracy stays the same. I was thinking, that \"fit\" could use DataFrame index as a feature and because the initial data is sorted, it could be a data leakage, but seems not (see https:\/\/stackoverflow.com\/questions\/58635398\/does-sklearn-use-pandas-index-as-a-feature)","73984a6f":"#### So lets train the model on the full dataset and save is for future generations :)"}}