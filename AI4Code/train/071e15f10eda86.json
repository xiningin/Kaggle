{"cell_type":{"6e58c4a3":"code","6193ffbc":"code","50bf4c43":"code","1c81dbe1":"code","c8ce6e97":"code","e0adf566":"code","2eb9ec48":"code","a8279f14":"code","587bfd81":"code","d7d0c225":"code","578977b4":"code","e968a960":"code","4b68ab6a":"code","85b64b67":"code","51d9cb45":"code","62c97eb8":"code","09fd0bb6":"code","16df40be":"code","f1c85082":"code","82b2f0a8":"code","1bba52ae":"code","09bb9907":"code","3f284b54":"code","d735eebd":"code","0d138fd7":"code","2e46ec34":"code","0cd63748":"code","0f68ef85":"code","ff0c46f1":"code","5a05dd86":"markdown","63866f3f":"markdown","605727db":"markdown","c89b1b7e":"markdown","c6a75b9c":"markdown","cc6ea740":"markdown","454eea41":"markdown","8875b6ab":"markdown","667d628c":"markdown","7e752a33":"markdown","68687c78":"markdown","b0a3aa25":"markdown","4885777e":"markdown","0acf7fe8":"markdown","a2124fd7":"markdown","c325ab57":"markdown","09716dcc":"markdown","db2398f5":"markdown"},"source":{"6e58c4a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6193ffbc":"import re\nfrom tqdm import tqdm\nimport transformers\n%matplotlib inline\n\nfrom sklearn.model_selection import train_test_split\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.corpus import stopwords\nwords = stopwords.words(\"english\")\nlemma = nltk.stem.WordNetLemmatizer()\n\n\nimport torch\nfrom transformers import BertTokenizer\nfrom transformers import RobertaTokenizer\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import BertModel,RobertaModel\nfrom transformers import AdamW, get_linear_schedule_with_warmup\n\nimport random\nimport time\n","50bf4c43":"train = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/janatahack-independence-day-2020-ml-hackathon\/sample_submission_UVKGLZE.csv')","1c81dbe1":"train.columns","c8ce6e97":"targets = ['ComputerScience', 'Physics', 'Mathematics','Statistics', 'QuantitativeBiology', 'QuantitativeFinance']","e0adf566":"train[\"text\"] = train[\"TITLE\"]+\"\"+train[\"ABSTRACT\"]\ntest[\"text\"] = test[\"TITLE\"]+\"\"+test[\"ABSTRACT\"]","2eb9ec48":"X = train.text.values\ny = train[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']].values\n\nX_train, X_val, y_train, y_val =train_test_split(X, y, test_size=0.1, random_state=2020)","a8279f14":"if torch.cuda.is_available():       \n    device = torch.device(\"cuda\")\n    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n    print('Device name:', torch.cuda.get_device_name(0))\n\nelse:\n    print('No GPU available, using the CPU instead.')\n    device = torch.device(\"cpu\")\n","587bfd81":"# def text_preprocessing(text):\n#     \"\"\"\n#     - Remove entity mentions (eg. '@united')\n#     - Correct errors (eg. '&amp;' to '&')\n#     @param    text (str): a string to be processed.\n#     @return   text (Str): the processed string.\n#     \"\"\"\n#     text = text.lower()\n#     text = re.sub(r\"what's\", \"what is \", text)\n#     text = re.sub(r\"won't\", \"will not \", text)\n#     text = re.sub(r\"\\'s\", \" \", text)\n#     text = re.sub(r\"\\'ve\", \" have \", text)\n#     text = re.sub(r\"can't\", \"can not \", text)\n#     text = re.sub(r\"n't\", \" not \", text)\n#     text = re.sub(r\"i'm\", \"i am \", text)\n#     text = re.sub(r\"\\'re\", \" are \", text)\n#     text = re.sub(r\"\\'d\", \" would \", text)\n#     text = re.sub(r\"\\'ll\", \" will \", text)\n#     text = re.sub(r\"\\'scuse\", \" excuse \", text)\n#     text = re.sub(r\"\\'\\n\", \" \", text)\n#     text = re.sub(r\"-\", \" \", text)\n#     text = re.sub(r\"\\'\\xa0\", \" \", text)\n#     text = re.sub('\\s+', ' ', text)\n#     text = ''.join(c for c in text if not c.isnumeric())\n    \n#     # Remove '@name'\n#     text = re.sub(r'(@.*?)[\\s]', ' ', text)\n    \n      # Remove Latex tags in the test\n#     text = re.sub(r'(\\$+)(?:(?!\\1)[\\s\\S])*\\1',' ',text)\n    \n      # Remove words with in brackets\n#     text = re.sub(r'\\([^)]*\\)', '', text)\n\n#     # Replace '&amp;' with '&'\n#     text = re.sub(r'&amp;', '&', text)\n\n#     # Remove trailing whitespace\n#     text = re.sub(r'\\s+', ' ', text).strip()\n     \n# #     tokens = word_tokenize(text)\n# #     remove_words = [word for word in tokens if not word in words]\n# #     text = [lemma.lemmatize(word) for word in remove_words]\n# #     joined_words = \" \".join(text)\n\n#     return text","d7d0c225":"def text_preprocessing(text):\n    \"\"\"\n    - Remove entity mentions (eg. '@united')\n    - Correct errors (eg. '&amp;' to '&')\n    @param    text (str): a string to be processed.\n    @return   text (Str): the processed string.\n    \"\"\"\n    \n    text = text.lower()\n    text = re.sub(r\"what's\", \"what is \", text)\n    text = re.sub(r\"won't\", \"will not \", text)\n    text = re.sub(r\"\\'s\", \" \", text)\n    text = re.sub(r\"\\'ve\", \" have \", text)\n    text = re.sub(r\"can't\", \"can not \", text)\n    text = re.sub(r\"n't\", \" not \", text)\n    text = re.sub(r\"i'm\", \"i am \", text)\n    text = re.sub(r\"\\'re\", \" are \", text)\n    text = re.sub(r\"\\'d\", \" would \", text)\n    text = re.sub(r\"\\'ll\", \" will \", text)\n    text = re.sub(r\"\\'scuse\", \" excuse \", text)\n    text = re.sub(r\"\\'\\n\", \" \", text)\n    text = re.sub(r\"-\", \" \", text)\n    text = re.sub(r\"\\'\\xa0\", \" \", text)\n    text = re.sub('\\s+', ' ', text)\n    text = ''.join(c for c in text if not c.isnumeric())\n    \n    # Remove '@name'\n    text = re.sub(r'(@.*?)[\\s]', ' ', text)\n\n    # Replace '&amp;' with '&'\n    text = re.sub(r'&amp;', '&', text)\n\n    # Remove trailing whitespace\n    text = re.sub(r'\\s+', ' ', text).strip()\n\n    return text","578977b4":"temp = train[\"text\"][1000]\nprint(temp)\nval = text_preprocessing(text=temp)\nprint(val)","e968a960":"#Load the Bert tokenizer\n# tokenizer = RobertaTokenizer.from_pretrained('roberta-base',do_lower_case=True)\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\",do_lower_case=True)\n# Create a funcition to tokenize a set of text\n\ndef preprocessing_for_bert(data):\n    \"\"\"Perform required preprocessing steps for pretrained BERT.\n    @param    data (np.array): Array of texts to be processed.\n    @return   input_ids (torch.Tensor): Tensor of token ids to be fed to a model.\n    @return   attention_masks (torch.Tensor): Tensor of indices specifying which\n                  tokens should be attended to by the model.\n    \"\"\"\n    # create empty lists to store outputs\n    input_ids = []\n    attention_masks = []\n    \n    #for every sentence...\n    \n    for sent in data:\n        # 'encode_plus will':\n        # (1) Tokenize the sentence\n        # (2) Add the `[CLS]` and `[SEP]` token to the start and end\n        # (3) Truncate\/Pad sentence to max length\n        # (4) Map tokens to their IDs\n        # (5) Create attention mask\n        # (6) Return a dictionary of outputs\n        encoded_sent = tokenizer.encode_plus(\n            text = text_preprocessing(sent),   #preprocess sentence\n            add_special_tokens = True,         #Add `[CLS]` and `[SEP]`\n            max_length= MAX_LEN  ,             #Max length to truncate\/pad\n            pad_to_max_length = True,          #pad sentence to max length \n            return_attention_mask= True        #Return attention mask \n        )\n        # Add the outputs to the lists\n        input_ids.append(encoded_sent.get('input_ids'))\n        attention_masks.append(encoded_sent.get('attention_mask'))\n        \n    #convert lists to tensors\n    input_ids = torch.tensor(input_ids)\n    attention_masks = torch.tensor(attention_masks)\n    \n    return input_ids,attention_masks","4b68ab6a":"# Before tokenizing we need to specify the maximum length of our sentences\n\n#concat the train data and test data\n\nall_text = np.concatenate([train.text.values,test.text.values])\n\n#Encode the concatenated data\nlen_sent = [len(text_preprocessing(sent)) for sent in all_text]\n\n# Find the maximum length\navg_len = np.mean(len_sent)\nprint('Avg length: ',avg_len)","85b64b67":"## Now Tokenizing the data\n\nMAX_LEN = 500\n\n# Print sentece 0 and its encoded token ids\ntoken_ids = list(preprocessing_for_bert([X[0]])[0].squeeze().numpy())\nprint('Original: ',X[0])\nprint('Token IDs: ',token_ids)","51d9cb45":"# Run function 'preprocessing_for_bert' on the train set and validation set\nprint('Tokenizing data...')\ntrain_inputs, train_masks = preprocessing_for_bert(X_train)\nval_inputs, val_masks = preprocessing_for_bert(X_val)","62c97eb8":"# Convert other data types to torch.Tensor\ntrain_labels = torch.tensor(y_train)\nval_labels = torch.tensor(y_val)\n\n## For fine-tuning Bert, the authors recommmend a batch size of 16 or 32\nbatch_size = 16\n\n# Create the DataLoader for our training set\ntrain_data = TensorDataset(train_inputs,train_masks, train_labels)\ntrain_sampler = RandomSampler(train_data)\ntrain_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n\n# Create the DataLoader for our validation set\nval_data = TensorDataset(val_inputs, val_masks, val_labels)\nval_sampler = SequentialSampler(val_data)\nval_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)","09fd0bb6":"%%time\n# Create the BertClassifier class\n\nclass BertClassifier(nn.Module):\n    \"\"\"\n        Bert Model for classification Tasks.\n    \"\"\"\n    def __init__(self, freeze_bert=False):\n        \"\"\"\n        @param   bert: a BertModel object\n        @param   classifier: a torch.nn.Module classifier\n        @param   freeze_bert (bool): Set `False` to fine_tune the Bert model\n        \"\"\"\n        super(BertClassifier,self).__init__()\n        # Specify hidden size of Bert, hidden size of our classifier, and number of labels\n        D_in, H,D_out = 768,30,6\n        \n#         self.bert = RobertaModel.from_pretrained('roberta-base')\n        self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n        \n        self.classifier = nn.Sequential(\n                            nn.Linear(D_in, H),\n                            nn.ReLU(),\n                            nn.Linear(H, D_out))\n        self.sigmoid = nn.Sigmoid()\n        # Freeze the Bert Model\n        if freeze_bert:\n            for param in self.bert.parameters():\n                param.requires_grad = False\n    \n    def forward(self,input_ids,attention_mask):\n        \"\"\"\n        Feed input to BERT and the classifier to compute logits.\n        @param    input_ids (torch.Tensor): an input tensor with shape (batch_size,\n                      max_length)\n        @param    attention_mask (torch.Tensor): a tensor that hold attention mask\n                      information with shape (batch_size, max_length)\n        @return   logits (torch.Tensor): an output tensor with shape (batch_size,\n                      num_labels)\n        \"\"\"\n        outputs = self.bert(input_ids=input_ids,\n                           attention_mask = attention_mask)\n        \n        # Extract the last hidden state of the token `[CLS]` for classification task\n        last_hidden_state_cls = outputs[0][:,0,:]\n        \n        # Feed input to classifier to compute logits\n        logit = self.classifier(last_hidden_state_cls)\n        \n#         logits = self.sigmoid(logit)\n        \n        return logit","16df40be":"def initialize_model(epochs=4):\n    \"\"\"Initialize the Bert Classifier, the optimizer and the learning rate scheduler.\n    \"\"\"\n    \n    # Instantiate Bert Classifier\n    bert_classifier = BertClassifier(freeze_bert=False)\n    \n    bert_classifier.to(device)\n    \n    # Create the optimizer\n    optimizer = AdamW(bert_classifier.parameters(),\n                     lr=5e-5, #Default learning rate\n                     eps=1e-8 #Default epsilon value\n                     )\n    \n    # Total number of training steps\n    total_steps = len(train_dataloader) * epochs\n    \n    # Set up the learning rate scheduler\n    scheduler = get_linear_schedule_with_warmup(optimizer, \n                                              num_warmup_steps=0, # Default value\n                                              num_training_steps=total_steps)\n    return bert_classifier, optimizer, scheduler","f1c85082":"# Specify loss function\n#loss_fn = nn.CrossEntropyLoss()\nloss_fn = nn.BCEWithLogitsLoss()\n\ndef set_seed(seed_value=42):\n    \"\"\"Set seed for reproducibility.\n    \"\"\"\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    torch.cuda.manual_seed_all(seed_value)\n\ndef train(model, train_dataloader, val_dataloader=None, epochs=4, evaluation=False):\n    \"\"\"Train the BertClassifier model.\n    \"\"\"\n    # Start training loop\n    print(\"Start training...\\n\")\n    for epoch_i in range(epochs):\n        # =======================================\n        #               Training\n        # =======================================\n        # Print the header of the result table\n        print(f\"{'Epoch':^7} | {'Batch':^7} | {'Train Loss':^12} | {'Val Loss':^10} | {'Val Acc':^9} | {'Elapsed':^9}\")\n        print(\"-\"*70)\n\n        # Measure the elapsed time of each epoch\n        t0_epoch, t0_batch = time.time(), time.time()\n\n        # Reset tracking variables at the beginning of each epoch\n        total_loss, batch_loss, batch_counts = 0, 0, 0\n\n        # Put the model into the training mode\n        model.train()\n\n        # For each batch of training data...\n        for step, batch in enumerate(train_dataloader):\n            batch_counts +=1\n            # Load batch to GPU\n            b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n            # Zero out any previously calculated gradients\n            model.zero_grad()\n\n            # Perform a forward pass. This will return logits.\n            logits = model(b_input_ids, b_attn_mask)\n\n            # Compute loss and accumulate the loss values\n            loss = loss_fn(logits, b_labels.float())\n            batch_loss += loss.item()\n            total_loss += loss.item()\n\n            # Perform a backward pass to calculate gradients\n            loss.backward()\n\n            # Clip the norm of the gradients to 1.0 to prevent \"exploding gradients\"\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n            # Update parameters and the learning rate\n            optimizer.step()\n            scheduler.step()\n\n            # Print the loss values and time elapsed for every 20--50000 batches\n            if (step % 50000 == 0 and step != 0) or (step == len(train_dataloader) - 1):\n                # Calculate time elapsed for 20 batches\n                time_elapsed = time.time() - t0_batch\n\n                # Print training results\n                print(f\"{epoch_i + 1:^7} | {step:^7} | {batch_loss \/ batch_counts:^12.6f} | {'-':^10} | {'-':^9} | {time_elapsed:^9.2f}\")\n\n                # Reset batch tracking variables\n                batch_loss, batch_counts = 0, 0\n                t0_batch = time.time()\n\n        # Calculate the average loss over the entire training data\n        avg_train_loss = total_loss \/ len(train_dataloader)\n\n        print(\"-\"*70)\n        # =======================================\n        #               Evaluation\n        # =======================================\n        if evaluation == True:\n            # After the completion of each training epoch, measure the model's performance\n            # on our validation set.\n            val_loss, val_accuracy = evaluate(model, val_dataloader)\n\n            # Print performance over the entire training data\n            time_elapsed = time.time() - t0_epoch\n            \n            print(f\"{epoch_i + 1:^7} | {'-':^7} | {avg_train_loss:^12.6f} | {val_loss:^10.6f} | {val_accuracy:^9.2f} | {time_elapsed:^9.2f}\")\n            print(\"-\"*70)\n        print(\"\\n\")\n    \n    print(\"Training complete!\")\n\n\ndef evaluate(model, val_dataloader):\n    \"\"\"After the completion of each training epoch, measure the model's performance\n    on our validation set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    # Tracking variables\n    val_accuracy = []\n    val_loss = []\n\n    # For each batch in our validation set...\n    for batch in val_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask, b_labels = tuple(t.to(device) for t in batch)\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n\n        # Compute loss\n        loss = loss_fn(logits, b_labels.float())\n        val_loss.append(loss.item())\n\n        # Get the predictions\n        #preds = torch.argmax(logits, dim=1).flatten()\n        \n        # Calculate the accuracy rate\n        #accuracy = (preds == b_labels).cpu().numpy().mean() * 100\n        accuracy = accuracy_thresh(logits.view(-1,6),b_labels.view(-1,6))\n        \n        val_accuracy.append(accuracy)\n\n    # Compute the average accuracy and loss over the validation set.\n    val_loss = np.mean(val_loss)\n    val_accuracy = np.mean(val_accuracy)\n\n    return val_loss, val_accuracy\n\ndef accuracy_thresh(y_pred, y_true, thresh:float=0.5, sigmoid:bool=True):\n    \"Compute accuracy when `y_pred` and `y_true` are the same size.\"\n    if sigmoid: \n        y_pred = y_pred.sigmoid()\n    return ((y_pred>thresh)==y_true.byte()).float().mean().item()\n    #return np.mean(((y_pred>thresh).float()==y_true.float()).float().cpu().numpy(), axis=1).sum()","82b2f0a8":"set_seed(42)    # Set seed for reproducibility\nbert_classifier, optimizer, scheduler = initialize_model(epochs=1)\ntrain(bert_classifier, train_dataloader, val_dataloader, epochs=1, evaluation=True)","1bba52ae":"\ndef bert_predict(model, test_dataloader):\n    \"\"\"Perform a forward pass on the trained BERT model to predict probabilities\n    on the test set.\n    \"\"\"\n    # Put the model into the evaluation mode. The dropout layers are disabled during\n    # the test time.\n    model.eval()\n\n    all_logits = []\n\n    # For each batch in our test set...\n    for batch in test_dataloader:\n        # Load batch to GPU\n        b_input_ids, b_attn_mask = tuple(t.to(device) for t in batch)[:2]\n\n        # Compute logits\n        with torch.no_grad():\n            logits = model(b_input_ids, b_attn_mask)\n        all_logits.append(logits)\n    \n    # Concatenate logits from each batch\n    all_logits = torch.cat(all_logits, dim=0)\n\n    # Apply softmax to calculate probabilities\n    #probs = F.softmax(all_logits, dim=1).cpu().numpy()\n    probs = all_logits.sigmoid().cpu().numpy()\n    \n\n    return probs\n\n#probs = all_logits.sigmoid().cpu().numpy()\n","09bb9907":"## Compute predicted probabilities on the test set\n\nprobs = bert_predict(bert_classifier,val_dataloader)\n\n# Evalueate the bert classifier\n\n# evaluate_roc(probs, y_val)","3f284b54":"# Concatenate the train set and the validation set\n\nfull_train_data = torch.utils.data.ConcatDataset([train_data, val_data])\nfull_train_sampler = RandomSampler(full_train_data)\nfull_train_dataloader = DataLoader(full_train_data, sampler=full_train_sampler, batch_size=batch_size)\n\n# Train the Bert Classifier on the entire training data\nset_seed(42)\nbert_classifier, optimizer, scheduler = initialize_model(epochs=4)\ntrain(bert_classifier, full_train_dataloader, epochs=4)","d735eebd":"test[\"text\"] = test[\"text\"].apply(text_preprocessing)\n\n## Run preprocessing_for_bert on the test set\nprint('Tokenizing data...')\ntest_inputs, test_masks = preprocessing_for_bert(test.text)\n\n# Create the DataLoader for our test set\ntest_dataset = TensorDataset(test_inputs, test_masks)\n#test_sampler = SequentialSampler(test_dataset)\n#test_dataloader = DataLoader(test_dataset, sampler=test_sampler, batch_size=32)\ntest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=16)\n\n","0d138fd7":"# Compute predicted probabilities on the test set\nprobs = bert_predict(bert_classifier, test_dataloader)\n\n# Get predictions from the probabilities\n#threshold = 0.5. ## Change depending on the accuracy you need\n#preds = np.where(probs[:, 1] > threshold, 1, 0)","2e46ec34":"submission = pd.DataFrame(probs,columns=['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance'])\ntest[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']]=submission\nfinal_sub = test[['Computer Science', 'Physics', 'Mathematics',\n       'Statistics', 'Quantitative Biology', 'Quantitative Finance']]\nfinal_sub.head()\n","0cd63748":"final_sub.to_csv(\"BertModel_proba.csv\",index=False)","0f68ef85":"def check_thresold(thresold):\n    sub[\"Computer Science\"] = [1 if n >= thresold else 0 for n in final_sub[\"Computer Science\"]]\n    sub[\"Physics\"] = [1 if n >= thresold else 0 for n in final_sub[\"Physics\"]]\n    sub[\"Mathematics\"] = [1 if n >= thresold else 0 for n in final_sub[\"Mathematics\"]]\n    sub[\"Statistics\"] = [1 if n >= thresold else 0 for n in final_sub[\"Statistics\"]]\n    sub[\"Quantitative Biology\"] = [1 if n >= thresold else 0 for n in final_sub[\"Quantitative Biology\"]]\n    sub[\"Quantitative Finance\"] = [1 if n >= thresold else 0 for n in final_sub[\"Quantitative Finance\"]]\n    \n    sub.to_csv(\"_bert_{}_hdn30_padd462_split8_text_cleaned1.csv\".format(thresold),index=False)","ff0c46f1":"check_thresold(0.36)","5a05dd86":"# Necessary Import Statement","63866f3f":"## Performing train test split on all the target column value as Y and our combined text as X Feature","605727db":"# Public LB : 15\n# Private LB : ","c89b1b7e":"0.218901\n0.152067\n0.114838\n0.086432","c6a75b9c":"Pre-Processing and creating the test data loader on Test dataset","cc6ea740":"# Calling our Model for Training","454eea41":"Train function and Evaluation function","8875b6ab":"# Simple Bert Model","667d628c":"Check which device we are using Either GPU OR CPU","7e752a33":"Function to predict on test dataset","68687c78":"# Problem Statement\nTopic Modeling for Research Articles\n\nResearchers have access to large online archives of scientific articles. As a consequence, finding relevant articles has become more difficult. Tagging or topic modelling provides a way to give token of identification to research articles which facilitates recommendation and search process.\n\nGiven the abstract and title for a set of research articles, predict the topics for each article included in the test set. \n\nNote that a research article can possibly have more than 1 topic. The research article abstracts and titles are sourced from the following 6 topics: \n\n1. Computer Science\n\n2. Physics\n\n3. Mathematics\n\n4. Statistics\n\n5. Quantitative Biology\n\n6. Quantitative Finance","b0a3aa25":"# Pre Processing the text in such a way that Bert Based Model Expects ","4885777e":"General Pre-Processing Steps to remove unwnated characters extrac spaces and other simple steps","0acf7fe8":"![](https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/jantahack_i-day-thumbnail-1200x1200-90.jpg)","a2124fd7":"## Specify the optimizer of our model with how many epochs you are going to train","c325ab57":"Combining the Title and Abstract of the given data to create a final text","09716dcc":"As it is a multilabel classification problem there will be muliple targets ","db2398f5":"# Creating the torch Data Loader for our train and validation data"}}