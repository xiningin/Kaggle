{"cell_type":{"6f64c0a3":"code","e7d08d43":"code","eab29bce":"code","e918c80e":"code","b7daed37":"code","8a9a9c31":"code","efe6c4e6":"code","49c41664":"code","7a46c1ce":"code","0298740d":"code","f781fbee":"code","06ba02dc":"code","db7e5ee1":"code","5ff2e9e1":"code","907f0f44":"code","7291d3e4":"code","ce3fbd0e":"code","1f5125e9":"code","bd8bb7ae":"code","c17f03bf":"code","40e13d55":"code","fd7b06dd":"code","b9e8dd7a":"code","74051fbb":"code","4563ac0a":"code","b12f3913":"code","8edccb01":"code","af18a1aa":"code","d5071d9f":"markdown","16a32e05":"markdown","e0f88c34":"markdown","e6a8eaea":"markdown","95495b05":"markdown"},"source":{"6f64c0a3":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip install scispacy\n    !pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz\n    !pip install whoosh","e7d08d43":"import pandas as pd\nimport numpy as np\nimport os\nfrom collections import defaultdict","eab29bce":"import whoosh\nfrom whoosh.qparser import *\nfrom whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED\nfrom whoosh.analysis import StandardAnalyzer\nfrom whoosh import index\n\nfrom whoosh.analysis import Tokenizer, Token\nfrom whoosh import highlight","e918c80e":"from IPython.core.display import display, HTML","b7daed37":"# def set_column_width(ColumnWidth, MaxRows):\n#     pd.options.display.max_colwidth = ColumnWidth\n#     pd.options.display.max_rows = MaxRows\n#     print('Set pandas dataframe column width to', ColumnWidth, 'and max rows to', MaxRows)\n\n# # this has the user interactively set the Pandas dataframe dimensions\n# def user_sets_pandas_df_dimensions():\n#     interact(set_column_width, \n#            ColumnWidth=widgets.IntSlider(min=50, max=400, step=50, value=200),\n#            MaxRows=widgets.IntSlider(min=50, max=500, step=100, value=100));","8a9a9c31":"df = pd.read_csv('..\/input\/cord-19-create-dataframe\/cord19_df.csv')\n\ndf.shape","efe6c4e6":"df.columns","49c41664":"import en_core_sci_lg\n\n# medium model\nnlp = en_core_sci_lg.load(disable=[\"tagger\", \"parser\", \"ner\"])\nnlp.max_length = 2000000\n\n# New stop words list \ncustomize_stop_words = [\n    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',\n    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n    '-PRON-', 'usually',\n    r'\\usepackage{amsbsy', r'\\usepackage{amsfonts', r'\\usepackage{mathrsfs', r'\\usepackage{amssymb', r'\\usepackage{wasysym',\n    r'\\setlength{\\oddsidemargin}{-69pt',  r'\\usepackage{upgreek', r'\\documentclass[12pt]{minimal'\n]\n\n# Mark them as stop words\nfor w in customize_stop_words:\n    nlp.vocab[w].is_stop = True","7a46c1ce":"def my_spacy_tokenizer(sentence):\n    # lowercase lemma, startchar and endchar of each word in sentence\n    return [(word.lemma_.lower(), word.idx, word.idx + len(word)) for word in nlp(sentence) \n            if not (word.like_num or word.is_stop or word.is_punct or word.is_space or len(word)==1)]","0298740d":"class SpacyTokenizer(Tokenizer):\n    \"\"\"\n    Customized tokenizer for Whoosh\n    \"\"\"\n    \n    def __init__(self, spacy_tokenizer):\n        self.spacy_tokenizer = spacy_tokenizer\n    \n    def __call__(self, value, positions = False, chars = False,\n                 keeporiginal = False, removestops = True,\n                 start_pos = 0, start_char = 0, mode = '', **kwargs):\n        \"\"\"\n        :param value: The unicode string to tokenize.\n        :param positions: Whether to record token positions in the token.\n        :param chars: Whether to record character offsets in the token.\n        :param start_pos: The position number of the first token. For example,\n            if you set start_pos=2, the tokens will be numbered 2,3,4,...\n            instead of 0,1,2,...\n        :param start_char: The offset of the first character of the first\n            token. For example, if you set start_char=2, the text \"aaa bbb\"\n            will have chars (2,5),(6,9) instead (0,3),(4,7).\n        \"\"\"\n        \n        assert isinstance(value, str), \"%r is not unicode\" % value\n        \n        spacy_tokens = self.spacy_tokenizer(value)\n        \n        t = Token(positions, chars, removestops=removestops, mode=mode)\n\n        for pos, spacy_token in enumerate(spacy_tokens):\n            t.text = spacy_token[0]\n            if keeporiginal:\n                t.original = t.text\n            t.stopped = False\n            if positions:\n                t.pos = start_pos + pos\n            if chars:\n                t.startchar = start_char + spacy_token[1]\n                t.endchar = start_char + spacy_token[2]\n            yield t","f781fbee":"#get hardcoded schema for the index\ndef get_search_schema(analyzer=StandardAnalyzer()):\n    schema = Schema(paper_id=ID(stored=True),\n                    doi = ID(stored=True),\n                    authors=TEXT(analyzer=analyzer),\n                    journal=TEXT(analyzer=analyzer),\n                    title=TEXT(analyzer=analyzer, stored=True),\n                    abstract=TEXT(analyzer=analyzer, stored=True),\n                    methods=TEXT(analyzer=analyzer)\n#                     body_text = TEXT(analyzer=analyzer)\n                   )\n    return schema\n\ndef add_documents_to_index(ix, df):\n    #create a writer object to add documents to the index\n    writer = ix.writer()\n\n    #now we can add documents to the index\n    for _, doc in df.iterrows():\n        writer.add_document(paper_id = str(doc.paper_id),\n                            doi = str(doc.doi),\n                            authors = str(doc.authors),\n                            journal = str(doc.journal),\n                            title = str(doc.title),\n                            abstract = str(doc.abstract),\n                            methods = str(doc.methods)\n#                             body_text = str(doc.body_text)                            \n                           )\n \n    writer.commit()\n\n    return\n\ndef create_search_index(search_schema):\n    if not os.path.exists('indexdir'):\n        os.mkdir('indexdir')\n        ix = index.create_in('indexdir', search_schema)\n        add_documents_to_index(ix, df)\n    else:           \n        #open an existing index object\n        ix = index.open_dir('indexdir')\n    return ix","06ba02dc":"# import shutil\n# shutil.rmtree(\"\/kaggle\/working\/indexdir\") # to delete existing index","db7e5ee1":"my_analyzer = SpacyTokenizer(my_spacy_tokenizer)","5ff2e9e1":"search_schema = get_search_schema(analyzer=my_analyzer)\nix = create_search_index(search_schema)","907f0f44":"parser = MultifieldParser(['title', 'abstract'], schema=search_schema, group=OrGroup.factory(0.9))\nparser.add_plugin(SequencePlugin())","7291d3e4":"df.set_index('paper_id', inplace=True)","ce3fbd0e":"def search(query, lower=1950, upper=2020, only_covid19=False, kValue=5):\n    query_parsed = parser.parse(query)\n    with ix.searcher() as searcher:\n        results = searcher.search(query_parsed, limit = None)\n        output_dict = defaultdict(list)\n        for result in results:\n            output_dict['paper_id'].append(result['paper_id'])\n            output_dict['score'].append(result.score)\n        \n    search_results = pd.Series(output_dict['score'], index=pd.Index(output_dict['paper_id'], name='paper_id'))\n    search_results \/= search_results.max()\n    \n    relevant_time = df.publish_year.between(lower, upper)\n\n    if only_covid19:\n        temp = search_results[relevant_time & df.is_covid19]\n\n    else:\n        temp = search_results[relevant_time]\n\n    if len(temp) == 0:\n        return -1\n\n    # Get top k matches\n    top_k = temp.nlargest(kValue)\n    \n    return top_k","1f5125e9":"def searchQuery(query, lower=1950, upper=2020, only_covid19=False, attr=['paper_id', 'title', 'abstract', 'url', 'authors'], kValue=3):\n    search_results = search(query, lower, upper, only_covid19, kValue)\n\n    if type(search_results) is int:\n        return []\n    #results = df.loc[search_results].reset_index()\n    results = pd.merge(search_results.to_frame(name='similarity'), df, on='paper_id', how='left').reset_index() \n\n    return results[attr].to_dict('records')","bd8bb7ae":"search('title:hydroxychloroquine', kValue=3)","c17f03bf":"searchQuery('authors:drosten', only_covid19=True, kValue=2, attr=['authors', 'title'])","40e13d55":"searchQuery(\"title:hydroxychloroquine AND methods:(randomized controlled trial)\", only_covid19=True, kValue=2, attr=['title', 'abstract'])","fd7b06dd":"searchQuery(\"authors:drosten NOT title:coronavirus\", only_covid19=False, kValue=2, attr=['authors', 'title'])","b9e8dd7a":"searchQuery('doi:10.1101\/2020.01.31.929042', only_covid19=True, kValue=3, attr=['doi', 'title'])","74051fbb":"searchQuery('journal:(Studies in Natural Products Chemistry)', kValue=2, attr=['title', 'journal'])","4563ac0a":"user_query = 'abstract:(sars OR \"sars-cov-2\" OR coronavirus* OR ncov OR \"covid-19\" OR mers OR \"mers-cov\") \\\nNOT abstract:(animal OR equine OR porcine OR calves OR dog*) \\\nAND abstract:incubation \\\nAND abstract:(\"symptom onset\" OR characteristics OR exposure)'","b12f3913":"searchQuery(user_query, kValue=1, attr=['title', 'abstract'])","8edccb01":"query_parsed = parser.parse('incubation period' )","af18a1aa":"with ix.searcher() as searcher:\n    results = searcher.search(query_parsed, limit = 3)\n    results.fragmenter = highlight.SentenceFragmenter(charlimit=100000, maxchars=200)\n#     results.fragmenter = highlight.WholeFragmenter()\n    for hit in results:\n        print(hit['title'])\n        display(HTML(hit.highlights(\"title\", top=2)))\n        display(HTML(hit.highlights(\"abstract\", top=2)))","d5071d9f":"# Get Search Results","16a32e05":"# Setup Code","e0f88c34":"# Load Data","e6a8eaea":"# Highlight Excerpts","95495b05":"# Create Search Index"}}