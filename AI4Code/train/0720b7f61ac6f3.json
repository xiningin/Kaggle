{"cell_type":{"d90c96a1":"code","64bdc882":"code","ddf56832":"code","f6e22a3a":"code","a003e602":"code","14a6720f":"code","f6ce5fd1":"code","19a23816":"code","e48b2138":"code","121e0273":"code","8a82899d":"code","80245c3e":"code","ac18e0d4":"code","90383903":"code","1d416ed0":"code","285f24e9":"code","4905a4fb":"code","8c91d7b9":"code","cf5e3a3a":"code","37ee4bc1":"code","4568fe3f":"code","0469cb16":"code","4ed8be88":"code","32a61f74":"code","212d6f18":"code","34fd9219":"code","669202df":"code","3eed7e75":"code","0b40096e":"code","80fe4384":"code","a9e4dc50":"code","1c642327":"code","5f8bb815":"code","f20aa883":"code","27e189bc":"code","5c3618e2":"code","90b24a78":"code","f1adeee3":"code","275e9eb1":"code","6f4e849f":"code","63f461f0":"code","262057e1":"code","e0906696":"code","82961f34":"markdown","bac544bc":"markdown","ca6dbdd8":"markdown","6653d8ae":"markdown","58b768ee":"markdown","a80132ce":"markdown","0eb5cd56":"markdown","f51c5587":"markdown","45c30069":"markdown","48076b72":"markdown","de59fcbb":"markdown"},"source":{"d90c96a1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","64bdc882":"import matplotlib.pyplot as plt\nimport seaborn as sns","ddf56832":"stu_por=pd.read_csv(\"..\/input\/studentpor\/student-por.csv\",sep=\";\")\nstu_mat=pd.read_csv(\"..\/input\/studentmat\/student-mat.csv\",sep=\";\")\n","f6e22a3a":"stu_por.head(10)","a003e602":"stu=pd.concat([stu_por,stu_mat])\nstu[\"total_grades\"]=(stu[\"G1\"]+stu[\"G2\"]+stu[\"G3\"])\/3","14a6720f":"stu=stu.drop([\"G1\",\"G2\",\"G3\"],axis=1)\nmax=stu[\"total_grades\"].max()\nmin=stu[\"total_grades\"].min()","f6ce5fd1":"#ranging the grade in three parts\ndef marks(total_grades):\n    if(total_grades<7):\n        return(\"low\")\n    elif(total_grades>=7 and total_grades<14):\n        return(\"average\")\n    elif(total_grades>=14):\n        return(\"high\")\nstu[\"grades\"]=stu[\"total_grades\"].apply(marks)","19a23816":"stu.dtypes\nstu.describe()\n","e48b2138":"#describing categorical data\nstu.describe(include=\"all\")\n","121e0273":"stu.info()\n\n#checking for null values\nstu.isnull().any()","8a82899d":"#visualizing the grades\nplt.figure(figsize=(8,6))\nsns.countplot(stu[\"grades\"], order=[\"low\",\"average\",\"high\"], palette='Set1')\nplt.title('Final Grade - Number of Students',fontsize=20)\nplt.xlabel('Final Grade', fontsize=16)\nplt.ylabel('Number of Student', fontsize=16)","80245c3e":"#describing correlation\ncorr=stu.corr()\n\nplt.figure(figsize=(20,20))\nsns.heatmap(corr, annot=True, cmap=\"Reds\")\nplt.title('Correlation Heatmap', fontsize=20)","ac18e0d4":"#comparing school with grades\nsns.boxplot(x=\"school\", y=\"total_grades\", data=stu)\n\nschool_counts=stu[\"school\"].value_counts().to_frame()\nschool_counts.rename(columns={\"school\":\"school_counts\"},inplace=True)\nschool_counts.index.name='school'\n\nschool_sns=sns.countplot(hue=stu[\"school\"],x=stu[\"grades\"],data=stu)\n\n#crosstab is expanded form of value counts the the factors inside any variables\nperc=(lambda col:col\/col.sum())\nindex=[\"average\",\"high\",\"low\"]\nschooltab1=pd.crosstab(columns=stu.school,index=stu.grades)\n\nschool_perc=schooltab1.apply(perc).reindex(index)\n\nschool_perc.plot.bar(colormap=\"PiYG_r\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By school', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n\n#so by graph we know that school has impact on grades of students","90383903":"#comparing sex with grades\nsns.boxplot(x=\"sex\", y=\"total_grades\", data=stu)\nschool_counts=stu[\"sex\"].value_counts()\n#as the graph of sex nearly overlaps so it will not have impact on grades\nstu=stu.drop([\"sex\"],axis=1)\n\n","1d416ed0":"#comparing address with grades\nsns.boxplot(x=\"address\", y=\"total_grades\", data=stu)\nindex=[\"average\",\"high\",\"low\"]\naddresstab1=pd.crosstab(columns=stu.address,index=stu.grades)\n\naddress_perc=addresstab1.apply(perc).reindex(index)\n\naddress_perc.plot.bar(colormap=\"PiYG_r\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By address', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#address is factor for the grades","285f24e9":"#comparing famsize with grades\nsns.boxplot(x=\"famsize\", y=\"total_grades\", data=stu)\nfamsizetab1=pd.crosstab(columns=stu.famsize,index=stu.grades)\n\nfamsize_perc=famsizetab1.apply(perc).reindex(index)\n\nfamsize_perc.plot.bar(colormap=\"PiYG_r\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By famsize', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#famsize has great impact on grades","4905a4fb":"#comparing pstatus with grades\nsns.boxplot(x=\"Pstatus\", y=\"total_grades\", data=stu)\nPstatustab1=pd.crosstab(columns=stu.Pstatus,index=stu.grades)\n\nPstatus_perc=Pstatustab1.apply(perc).reindex(index)\n\nPstatus_perc.plot.bar(colormap=\"PiYG_r\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By Pstatus', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#it is not a good factor","8c91d7b9":"#comparing jobs\nsns.boxplot(x=\"Mjob\", y=\"total_grades\", data=stu)\nsns.boxplot(x=\"Fjob\", y=\"total_grades\", data=stu)\nstu1=stu[[\"Fjob\",\"Mjob\",\"total_grades\"]]\njob_grp=stu1.groupby(['Mjob','Fjob'],as_index=False).mean()\njob_pivot=job_grp.pivot(index='Mjob',columns='Fjob',values='total_grades')\n\n#so father and mother jobs has great impact on grades","cf5e3a3a":"#comparing reasons\nsns.boxplot(x=\"reason\", y=\"total_grades\", data=stu)\n#it has impact on the grades","37ee4bc1":"#comparing guardians\nsns.boxplot(x=\"guardian\", y=\"total_grades\", data=stu)\n\nguardiantab1=pd.crosstab(columns=stu.guardian,index=stu.grades)\nguardian_perc=guardiantab1.apply(perc).reindex(index)\nguardian_perc.plot.bar(colormap=\"BrBG\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By guardian', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#so guardian has grat impact on grades","4568fe3f":"#support of family and school\nsns.boxplot(x=\"schoolsup\", y=\"total_grades\", data=stu)\n#it is the important factor\nsns.boxplot(x=\"famsup\", y=\"total_grades\", data=stu)\nstu[[\"famsup\",\"total_grades\"]].groupby([\"famsup\"],as_index=False).mean()\n#famsup does not have great impact on grades \nstu=stu.drop([\"famsup\"],axis=1) ","0469cb16":"#comparing paid attributes\nsns.boxplot(x=\"paid\", y=\"total_grades\", data=stu)\npaidtab1=pd.crosstab(columns=stu.paid,index=stu.grades)\npaid_perc=paidtab1.apply(perc).reindex(index)\npaid_perc.plot.bar(colormap=\"BrBG\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By paid', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#paid does not have much influence on grades so\nstu=stu.drop([\"paid\"],axis=1)\n\n\n","4ed8be88":"sns.boxplot(x=\"activities\", y=\"total_grades\", data=stu)\n#is has graet impact on student perforamnce\nsns.boxplot(x=\"nursery\", y=\"total_grades\", data=stu)\n#it does not have great impact on performance\nstu=stu.drop([\"nursery\"],axis=1)","32a61f74":"#comparing if higher educatiob of students have impact on performance\nsns.boxplot(x=\"higher\", y=\"total_grades\", data=stu)\n\nsns.boxplot(x=\"internet\", y=\"total_grades\", data=stu)\n#internet also have great impact on performance of individual\n","212d6f18":"#high school romace impact on the performance of students\nsns.boxplot(x=\"romantic\", y=\"total_grades\", data=stu)\nromantictab1=pd.crosstab(columns=stu.romantic,index=stu.grades)\nromantic_perc=romantictab1.apply(perc).reindex(index)\nromantic_perc.plot.bar(colormap=\"BrBG\",fontsize=15,figsize=(7,7))\nplt.title('Final Grade By romantic', fontsize=20)\nplt.ylabel('Percentage of Student Counts ', fontsize=16)\nplt.xlabel('Final Grade', fontsize=16)\nplt.show()\n#so high school romance leads to decline in performance of students\n#beware of that","34fd9219":"stu.columns","669202df":"stu1=pd.get_dummies(stu,columns=[\"school\",\"address\",\"famsize\",\"Pstatus\",\"Mjob\",\"Fjob\",\"reason\",\"guardian\", 'schoolsup', 'activities', 'higher', 'internet', 'romantic' ])\ntest_stu1=stu1[\"grades\"]\nteststu1=stu1[\"total_grades\"]\ntrain_stu1=stu1.drop(['total_grades','grades'],axis=1)\ntrain_stu=train_stu1.values","3eed7e75":"train_stu1","0b40096e":"teststu1","80fe4384":"from scipy import stats\n#comparing age with marks\nsns.regplot(x=\"age\",y=\"total_grades\",data=stu)\n\n","a9e4dc50":"#pearson coeffiecient\nstu[[\"age\",\"total_grades\"]].corr()\n#p-value\npearson_coef , p_value=stats.pearsonr(stu[\"age\"],stu[\"total_grades\"])\nprint(\"The Pearson Correlation Coefficient is\", pearson_coef, \" with a P-value of P =\", p_value)\n#age is not a good factor","1c642327":"#using backward elimination for finding optimal featrures\n\n#if p-value is greater than 0.6 than we will removethat feature\nimport statsmodels.api as sm\nX=np.append(arr=np.ones((1044,1)).astype(int),values=train_stu,axis=1)\nX_opt = X[:, [0, 1, 2, 3, 4,5,6,7,8,9,10,11,12,13]]\nregressor_ols=sm.OLS(endog=teststu1,exog=X_opt).fit()\nregressor_ols.summary()\n\nX_opt = X[:, [0,2,3,4,5,6,7,8,9,10,11,12,13]]\nregressor_ols=sm.OLS(endog=teststu1,exog=X_opt).fit()\nregressor_ols.summary()\n\nX_opt = X[:, [0,2,3,4,5,6,7,9,10,11,12,13]]\nregressor_ols=sm.OLS(endog=teststu1,exog=X_opt).fit()\nregressor_ols.summary()\n","5f8bb815":"stu.describe()\n","f20aa883":"stu.columns\n","27e189bc":"#now we merge our training data\ntrain_x=np.concatenate((X_opt,X[:,14:49]),axis=1)\nstu[[\"Medu\",\"total_grades\"]].corr()\nstu[[\"Fedu\",\"total_grades\"]].corr()","5c3618e2":"train_stu2=train_stu1.drop([\"age\",\"freetime\"],axis=1)\nnp1=[1 for i in range(0,1044)]\ntrain_stu2.insert(loc=0,column= \"noimprotance\", value=np1)\n#now after getting the proper features we will split the data\n","90b24a78":"train_stu2.columns\ntrain_stu2.head(30)","f1adeee3":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(train_x,test_stu1, test_size = 0.2, random_state = 0)\n","275e9eb1":"# =============================================================================\n# #random forest\n# =============================================================================\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier=RandomForestClassifier(n_estimators=80,criterion=\"entropy\",random_state=0)\nclassifier.fit(X_train,y_train)\n\n#predicting the test set re4sults\ny_pred_random=classifier.predict(X_test)\n\nimportances=classifier.feature_importances_\nfor i,features in zip(importances,[ 'noimportant','Medu', 'Fedu', 'traveltime', 'studytime', 'failures', 'famrel',\n       'goout', 'Dalc', 'Walc', 'health', 'absences',\n       'total_grades', 'grades', 'school_GP', 'school_MS', 'address_R',\n       'address_U', 'famsize_GT3', 'famsize_LE3', 'Pstatus_A', 'Pstatus_T',\n       'Mjob_at_home', 'Mjob_health', 'Mjob_other', 'Mjob_services',\n       'Mjob_teacher', 'Fjob_at_home', 'Fjob_health', 'Fjob_other',\n       'Fjob_services', 'Fjob_teacher', 'reason_course', 'reason_home',\n       'reason_other', 'reason_reputation', 'guardian_father',\n       'guardian_mother', 'guardian_other', 'schoolsup_no', 'schoolsup_yes',\n       'activities_no', 'activities_yes', 'higher_no', 'higher_yes',\n       'internet_no', 'internet_yes', 'romantic_no', 'romantic_yes']):\n    print(\"{}:{}\".format(features,i))\nindices = np.argsort(importances)\n\n# Rearrange feature names so they match the sorted feature importances\nnames = [train_stu2.columns[i] for i in indices]\n\n# Barplot: Add bars\n\nplt.figure(figsize=(20,20))\nplt.bar(range(train_x.shape[1]), importances[indices],width=0.5)\n# Add feature names as x-axis labels\nplt.xticks(range(train_x.shape[1]),names, rotation=60, fontsize = 12)\n#from here we cam see that absences is the important features for determining the grades of students\n\n# Create plot title\nplt.title(\"Feature Importance\")\n# Show plot\nplt.show()\n\n#determinnig the confusion matrix\nfrom sklearn.metrics import confusion_matrix\ncm_random=confusion_matrix(y_test,y_pred_random)\n\n#determining the precision,recall and f1-score \nfrom sklearn.metrics import classification_report\nreport_random=classification_report(y_test,y_pred_random)\n","6f4e849f":"# =============================================================================\n# # Fitting Kernel SVM to the Training set\n# =============================================================================\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)\n# Predicting the Test set results\ny_pred_SVC= classifier.predict(X_test)\ncm_SVC=confusion_matrix(y_test,y_pred_SVC)\n\n#determining the precision,recall and f1-score \nfrom sklearn.metrics import classification_report\nreport_SVC=classification_report(y_test,y_pred_SVC)\nprint(report_SVC)","63f461f0":"# =============================================================================\n# #fitting logistic regression to the training set\n# =============================================================================\nfrom sklearn.linear_model import LogisticRegression\nclassifier=LogisticRegression(random_state=0)\nclassifier.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred_logistic= classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_logistic= confusion_matrix(y_test, y_pred_logistic)\n\n#determining the precision,recall and f1-score \nfrom sklearn.metrics import classification_report\nreport_logistic=classification_report(y_test,y_pred_logistic)","262057e1":"# =============================================================================\n# #fitting the knn_calssifier to the training set\n# =============================================================================\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier=KNeighborsClassifier(n_neighbors=5,metric='minkowski',p=2)\nclassifier.fit(X_train,y_train)\n# Predicting the Test set results\ny_pred_knn= classifier.predict(X_test)\n\n# Making the Confusion Matrix\nfrom sklearn.metrics import confusion_matrix\ncm_knn= confusion_matrix(y_test, y_pred_knn)\n\n#determining the precision,recall and f1-score \nfrom sklearn.metrics import classification_report\nreport_knn=classification_report(y_test,y_pred_knn)\n\n","e0906696":"print(report_random)\nprint(report_SVC)\nprint(report_logistic)\nprint(report_knn)","82961f34":"Importing the data","bac544bc":"Dropping columns","ca6dbdd8":"TRAINING THE DATASET","6653d8ae":"#encoding categorical data","58b768ee":"ANALYZING CATEGORICAL VARIABLES","a80132ce":"ANALYZING NUMERICAL VARIABLES","0eb5cd56":"Merging two datasets","f51c5587":"SO BY CONFUSION MATRIX AND F-SCORE WE FIND OUT THAT RANDOM FOREST IS BEST CLASSIFIER FOR GIVEN PROBLEM.","45c30069":"using boxplots","48076b72":"ANALYZING THE DATA","de59fcbb":"Splitting the dataset"}}