{"cell_type":{"ee42a360":"code","cab989a9":"code","d7d14c22":"code","fad035ce":"code","f8cb14d8":"code","5fb4ba38":"code","ed982a15":"code","7dee6dd1":"code","388ee09d":"code","0792ecbe":"code","59e8c043":"code","b06605bb":"code","3c16a1cb":"code","f08c4928":"code","e27b5a30":"code","58a9e84b":"code","89c0d481":"code","7f15859f":"code","052fb6d4":"code","04fded24":"code","37232932":"code","8542199c":"code","0e35ca34":"code","5b6b9bd0":"code","2932dd53":"code","22489e37":"code","7632e37b":"code","0b6f0a8e":"code","97361db5":"code","409be81a":"code","bbe99cdc":"code","1283fb8c":"code","e9e63dd3":"code","8bd791f8":"code","032b0923":"code","ff80696d":"code","9af07a38":"code","99116b54":"code","10dad095":"code","f6415afd":"code","6a48ddf1":"code","be2cc166":"code","45b33a08":"code","5b29a91a":"code","8495e028":"code","667d1968":"code","94fbaf30":"code","566814a4":"code","0926d2e5":"code","a4d2a95e":"code","afbd780b":"code","36b0ec9d":"code","38526b81":"code","c3a64246":"code","721f40df":"code","d6deb42d":"code","1ea62035":"code","767b70aa":"code","535d9d29":"code","992054ca":"code","a86a65ff":"markdown","e6ec9676":"markdown","a7e0561e":"markdown","45779aa1":"markdown","adac729e":"markdown","22247fce":"markdown","7d7f6e74":"markdown","027228e9":"markdown","cc2933b5":"markdown","b659689b":"markdown","209c533d":"markdown","7a3a57dc":"markdown","5ad990e0":"markdown","7a114dc9":"markdown","2f638837":"markdown","50379a29":"markdown","1b821a5f":"markdown","6f979bb0":"markdown","1b56efce":"markdown","8031b966":"markdown","97495774":"markdown","2c457d61":"markdown","c78c80fc":"markdown","598e9f76":"markdown","a8d34f53":"markdown","b020041b":"markdown","1fb5ede6":"markdown","7a7e0c15":"markdown","b1c659ea":"markdown","077472e0":"markdown","78ce05cd":"markdown","c5c6f05d":"markdown","4cb46c11":"markdown","721c8c01":"markdown","287274af":"markdown","68cca932":"markdown","f17fe921":"markdown","48301206":"markdown"},"source":{"ee42a360":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport seaborn as sns\nimport warnings\nimport itertools\nwarnings.filterwarnings('ignore')\nreview =pd.read_csv(\"..\/input\/Womens-Clothing-E-Commerce-Reviews.csv\")","cab989a9":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline","d7d14c22":"df_rough=pd.read_csv(\"..\/input\/Womens-Clothing-E-Commerce-Reviews.csv\",index_col=False)\ncolumn_contain=['Clothing ID','Age','Title','Review Text','Rating','Recommended IND','Positive Feedback Count','Division Name','Department Name','Class Name']","fad035ce":"df=pd.DataFrame(data=df_rough,columns=column_contain)\ndf.info()","f8cb14d8":"df.head()","5fb4ba38":"df.describe()","ed982a15":"df.fillna(0,inplace=True)","7dee6dd1":"for col in df.columns:\n    print('{} unique element: {}'.format(col,df[col].nunique()))","388ee09d":"df.head()","0792ecbe":"# Drop rows with missing values and drop duplicate\ndf.dropna(inplace=True)\ndf.drop_duplicates(inplace=True)\n\n# Visualize pairplot of df\nsns.pairplot(df, hue='Rating');","59e8c043":"df1=df['Rating'].value_counts().to_frame()\navgdf1 = df.groupby('Class Name').agg({'Rating': np.average})\navgdf2 = df.groupby('Class Name').agg({'Age': np.average})\navgdf3 = df.groupby('Rating').agg({'Age': np.average})\n\ntrace1 = go.Bar(\n    x=avgdf1.index,\n    y=round(avgdf1['Rating'],2),\n    marker=dict(\n        color=avgdf1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace2 = go.Bar(\n    x=df1.index,\n    y=df1.Rating,\n    marker=dict(\n        color=df1['Rating'],\n        colorscale = 'RdBu')\n)\n\ntrace3 = go.Bar(\n    x=avgdf2.index,\n    y=round(avgdf2['Age'],2),\n    marker=dict(\n        color=avgdf2['Age'],\n        colorscale = 'RdBu')\n)\n\ntrace4 = go.Bar(\n    x=avgdf3.index,\n    y=round(avgdf3['Age'],2),\n    marker=dict(\n        color=avgdf3['Age'],\n        colorscale = 'Reds')\n)\n\nfig = tools.make_subplots(rows=2, cols=2)\n\nfig.append_trace(trace1, 1, 1)\nfig.append_trace(trace2, 1, 2)\nfig.append_trace(trace3, 2, 1)\nfig.append_trace(trace4, 2, 2)\n\nfig['layout']['xaxis1'].update(title='Class')\nfig['layout']['yaxis1'].update(title='Average Rating')\nfig['layout']['xaxis2'].update(title='Rating')\nfig['layout']['yaxis2'].update(title='Count')\nfig['layout']['xaxis3'].update(title='Class')\nfig['layout']['yaxis3'].update(title='Average Age of the Reviewers')\nfig['layout']['xaxis4'].update(title='Rating')\nfig['layout']['yaxis4'].update(title='Average Age of the Reviewers')\n\nfig['layout'].update(height=800, width=900,showlegend=False)\npy.iplot(fig)","b06605bb":"cv = df['Class Name'].value_counts()\n\ntrace = go.Scatter3d( x = avgdf1.index,\n                      y = avgdf1['Rating'],\n                      z = cv[avgdf1.index],\n                      mode = 'markers',\n                      marker = dict(size=10,color=avgdf1['Rating']),\n                      hoverinfo =\"text\",\n                      text=\"Class: \"+avgdf1.index+\" \\ Average Rating: \"+round(avgdf1['Rating'],2).apply(str)+\" \\ Number of Reviewers: \"+cv[avgdf1.index].apply(str)\n                      )\n\ndata = [trace]\nlayout = go.Layout(title=\"Average Rating & Class & Number of Reviewers\",\n                   scene = dict(\n                    xaxis = dict(title='Class'),\n                    yaxis = dict(title='Average Rating'),\n                    zaxis = dict(title='Number of Sales'),),\n                   margin = dict(l=30, r=30, b=30, t=30))\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig)\nplt.savefig('3D_Scatter.png')","3c16a1cb":"df['Review Text']=df['Review Text'].astype(str)\ndf['Review Length']=df['Review Text'].apply(len)","f08c4928":"g = sns.FacetGrid(data=df, col='Rating')\ng.map(plt.hist, 'Review Length', bins=50)","e27b5a30":"plt.figure(figsize=(10,10))\nsns.boxplot(x='Rating', y='Review Length', data=df)","58a9e84b":"rating = df.groupby('Rating').mean()\nrating.corr()","89c0d481":"sns.heatmap(data=rating.corr(), annot=True)","7f15859f":"df.head()","052fb6d4":"plt.figure(figsize=(12,5))\nplt.title(\"Distribution of age\")\nax = sns.distplot(df[\"Age\"], color = 'm')","04fded24":"df.groupby(['Rating', pd.cut(df['Age'], np.arange(0,100,10))])\\\n       .size()\\\n       .unstack(0)\\\n       .plot.bar(stacked=True)","37232932":"plt.figure(figsize=(15,15))\ndf.groupby(['Department Name', pd.cut(df['Age'], np.arange(0,100,10))])\\\n       .size()\\\n       .unstack(0)\\\n       .plot.bar(stacked=True)","8542199c":"plt.figure(figsize=(15,15))\ndf.groupby(['Class Name', pd.cut(df['Age'], np.arange(0,100,10))])\\\n       .size()\\\n       .unstack(0)\\\n       .plot.bar(stacked=True)","0e35ca34":"z=df.groupby(by=['Department Name'],as_index=False).count().sort_values(by='Class Name',ascending=False)\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=z['Department Name'],y=z['Class Name'], data=z)\nplt.xlabel(\"Department Name\")\nplt.ylabel(\"Count\")\nplt.title(\"Counts Vs Department Name\")","5b6b9bd0":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\nimport seaborn as sns\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n%matplotlib inline\n \ndf1 = pd.read_csv(\"..\/input\/Womens-Clothing-E-Commerce-Reviews.csv\")\ndf = df1[['Review Text','Rating','Class Name','Age']]\n#df1.info()\n#df1.describe()\ndf1.head()\n","2932dd53":"df['Review Text'] = df['Review Text'].fillna('')\nvectorizer = CountVectorizer()\nanalyzer = vectorizer.build_analyzer()\n\ndef wordcounts(s):\n    c = {}\n    if analyzer(s):\n        d = {}\n        w = vectorizer.fit_transform([s]).toarray()\n        vc = vectorizer.vocabulary_\n        for k,v in vc.items():\n            d[v]=k\n        for index,i in enumerate(w[0]):\n            c[d[index]] = i\n    return  c\n\ndf['Word Counts'] = df['Review Text'].apply(wordcounts)\ndf.head()","22489e37":"selectedwords = ['awesome','great','fantastic','extraordinary','amazing','super',\n                 'magnificent','stunning','impressive','wonderful','breathtaking',\n                 'love','content','pleased','happy','glad','satisfied','lucky',\n                 'shocking','cheerful','wow','sad','unhappy','horrible','regret',\n                 'bad','terrible','annoyed','disappointed','upset','awful','hate']\n\ndef selectedcount(dic,word):\n    if word in dic:\n        return dic[word]\n    else:\n        return 0\n    \ndfwc = df.copy()  \nfor word in selectedwords:\n    dfwc[word] = dfwc['Word Counts'].apply(selectedcount,args=(word,))\n    \nword_sum = dfwc[selectedwords].sum()\nprint('Selected Words')\nprint(word_sum.sort_values(ascending=False).iloc[:5])\n\nprint('\\nClass Names')\nprint(df['Class Name'].fillna(\"Empty\").value_counts().iloc[:5])\n\nfig, ax = plt.subplots(1,2,figsize=(20,10))\nwc0 = WordCloud(background_color='white',\n                      width=450,\n                      height=400 ).generate_from_frequencies(word_sum)\n\ncn = df['Class Name'].fillna(\" \").value_counts()\nwc1 = WordCloud(background_color='white',\n                      width=450,\n                      height=400 \n                     ).generate_from_frequencies(cn)\n\nax[0].imshow(wc0)\nax[0].set_title('Selected Words\\n',size=25)\nax[0].axis('off')\n\nax[1].imshow(wc1)\nax[1].set_title('Class Names\\n',size=25)\nax[1].axis('off')\n\nrt = df['Review Text']\nplt.subplots(figsize=(18,6))\nwordcloud = WordCloud(background_color='white',\n                      width=900,\n                      height=300\n                     ).generate(\" \".join(rt))\nplt.imshow(wordcloud)\nplt.title('All Words in the Reviews\\n',size=25)\nplt.axis('off')\nplt.show()","7632e37b":"w=df.groupby(by=['Division Name'],as_index=False).count().sort_values(by='Class Name',ascending=False)\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=w['Division Name'],y=w['Class Name'], data=w)\nplt.xlabel(\"Division Name\")\nplt.ylabel(\"Count\")\nplt.title(\"Counts Vs Division Name\")","0b6f0a8e":"from collections import Counter\nfrom nltk.tokenize import RegexpTokenizer\nfrom stop_words import get_stop_words\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk import sent_tokenize, word_tokenize\nfrom wordcloud import WordCloud, STOPWORDS\nimport re\n\ntop_N = 100\n#convert list of list into text\n#a=''.join(str(r) for v in df_usa['title'] for r in v)\n\na = df['Review Text'].str.lower().str.cat(sep=' ')\n\n# removes punctuation,numbers and returns list of words\nb = re.sub('[^A-Za-z]+', ' ', a)\n\n#remove all the stopwords from the text\nstop_words = list(get_stop_words('en'))         \nnltk_words = list(stopwords.words('english'))   \nstop_words.extend(nltk_words)\n\nword_tokens = word_tokenize(b)\nfiltered_sentence = [w for w in word_tokens if not w in stop_words]\nfiltered_sentence = []\nfor w in word_tokens:\n    if w not in stop_words:\n        filtered_sentence.append(w)\n\n# Remove characters which have length less than 2  \nwithout_single_chr = [word for word in filtered_sentence if len(word) > 2]\n\n# Remove numbers\ncleaned_data_title = [word for word in without_single_chr if not word.isnumeric()]        \n\n# Calculate frequency distribution\nword_dist = nltk.FreqDist(cleaned_data_title)\nrslt = pd.DataFrame(word_dist.most_common(top_N),\n                    columns=['Word', 'Frequency'])\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.barplot(x=\"Word\",y=\"Frequency\", data=rslt.head(7))","97361db5":"def wc(data,bgcolor,title):\n    plt.figure(figsize = (100,100))\n    wc = WordCloud(background_color = bgcolor, max_words = 1000,  max_font_size = 50)\n    wc.generate(' '.join(data))\n    plt.imshow(wc)\n    plt.axis('off')","409be81a":"wc(cleaned_data_title,'black','Most Used Words')","bbe99cdc":"from textblob import TextBlob\n\nbloblist_desc = list()\n\ndf_review_str=df['Review Text'].astype(str)\nfor row in df_review_str:\n    blob = TextBlob(row)\n    bloblist_desc.append((row,blob.sentiment.polarity, blob.sentiment.subjectivity))\n    df_polarity_desc = pd.DataFrame(bloblist_desc, columns = ['Review','sentiment','polarity'])\n \ndef f(df_polarity_desc):\n    if df_polarity_desc['sentiment'] > 0:\n        val = \"Positive Review\"\n    elif df_polarity_desc['sentiment'] == 0:\n        val = \"Neutral Review\"\n    else:\n        val = \"Negative Review\"\n    return val\n\ndf_polarity_desc['Sentiment_Type'] = df_polarity_desc.apply(f, axis=1)\n\nplt.figure(figsize=(10,10))\nsns.set_style(\"whitegrid\")\nax = sns.countplot(x=\"Sentiment_Type\", data=df_polarity_desc)","1283fb8c":"positive_reviews=df_polarity_desc[df_polarity_desc['Sentiment_Type']=='Positive Review']\nnegative_reviews=df_polarity_desc[df_polarity_desc['Sentiment_Type']=='Negative Review']","e9e63dd3":"wc(positive_reviews['Review'],'black','Most Used Words')","8bd791f8":"wc(negative_reviews['Review'],'black','Most Used Words')","032b0923":"import string\ndef text_process(review):\n    nopunc=[word for word in review if word not in string.punctuation]\n    nopunc=''.join(nopunc)\n    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]","ff80696d":"df['Review Text'].head(5).apply(text_process)","9af07a38":"# Rating of 4 or higher -> positive, while the ones with \n# Rating of 2 or lower -> negative \n# Rating of 3 -> neutral\ndf = df[df['Rating'] != 3]\ndf['Sentiment'] = df['Rating'] >=4\ndf.head()","99116b54":"train_data,test_data = train_test_split(df,train_size=0.8,random_state=0)\nX = vectorizer.fit_transform(train_data['Review Text'])\ny = train_data['Sentiment']","10dad095":"start=dt.datetime.now()\nlr = LogisticRegression()\nlr.fit(X,y)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","f6415afd":"start=dt.datetime.now()\nnb = MultinomialNB()\nnb.fit(X,y)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","6a48ddf1":"start=dt.datetime.now()\nsvm = SVC()\nsvm.fit(X,y)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","be2cc166":"start=dt.datetime.now()\nnn = MLPClassifier()\nnn.fit(X,y)\nprint('Elapsed time: ',str(dt.datetime.now()-start))","45b33a08":"pred_lr = lr.predict_proba(vectorizer.transform(test_data['Review Text']))[:,1]\nfpr_lr,tpr_lr,_ = roc_curve(test_data['Sentiment'].values,pred_lr)\nroc_auc_lr = auc(fpr_lr,tpr_lr)\n\npred_nb = nb.predict_proba(vectorizer.transform(test_data['Review Text']))[:,1]\nfpr_nb,tpr_nb,_ = roc_curve(test_data['Sentiment'].values,pred_nb)\nroc_auc_nb = auc(fpr_nb,tpr_nb)\n\npred_svm = svm.decision_function(vectorizer.transform(test_data['Review Text']))\nfpr_svm,tpr_svm,_ = roc_curve(test_data['Sentiment'].values,pred_svm)\nroc_auc_svm = auc(fpr_svm,tpr_svm)\n\npred_nn = nn.predict_proba(vectorizer.transform(test_data['Review Text']))[:,1]\nfpr_nn,tpr_nn,_ = roc_curve(test_data['Sentiment'].values,pred_nn)\nroc_auc_nn = auc(fpr_nn,tpr_nn)\n\nf, axes = plt.subplots(2, 2,figsize=(15,10))\naxes[0,0].plot(fpr_lr, tpr_lr, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_lr))\naxes[0,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Logistic Regression')\naxes[0,0].legend(loc='lower right', fontsize=13)\n\naxes[0,1].plot(fpr_nb, tpr_nb, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nb))\naxes[0,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[0,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[0,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Naive Bayes')\naxes[0,1].legend(loc='lower right', fontsize=13)\n\naxes[1,0].plot(fpr_svm, tpr_svm, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_svm))\naxes[1,0].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,0].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,0].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Support Vector Machine')\naxes[1,0].legend(loc='lower right', fontsize=13)\n\naxes[1,1].plot(fpr_nn, tpr_nn, color='darkred', lw=2, label='ROC curve (area = {:0.2f})'.format(roc_auc_nn))\naxes[1,1].plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\naxes[1,1].set(xlim=[-0.01, 1.0], ylim=[-0.01, 1.05])\naxes[1,1].set(xlabel ='False Positive Rate', ylabel = 'True Positive Rate', title = 'Neural Network')\naxes[1,1].legend(loc='lower right', fontsize=13)","5b29a91a":"df=df.dropna(axis=0,how='any')\nrating_class = df[(df['Rating'] == 1) | (df['Rating'] == 5)]\nX_review=rating_class['Review Text']\ny=rating_class['Rating']","8495e028":"from sklearn.feature_extraction.text import CountVectorizer\nbow_transformer=CountVectorizer(analyzer=text_process).fit(X_review)","667d1968":"print(len(bow_transformer.vocabulary_))","94fbaf30":"X_review = bow_transformer.transform(X_review)","566814a4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_review, y, test_size=0.3, random_state=101)","0926d2e5":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\nnb.fit(X_train, y_train)","a4d2a95e":"predict=nb.predict(X_test)","afbd780b":"from sklearn.metrics import confusion_matrix, classification_report\nprint(confusion_matrix(y_test, predict))\nprint('\\n')\nprint(classification_report(y_test, predict))","36b0ec9d":"rating_positive=df['Review Text'][3]\nrating_positive","38526b81":"rating_positive_transformed = bow_transformer.transform([rating_positive])\nnb.predict(rating_positive_transformed)[0]","c3a64246":"rating_negative=df['Review Text'][61]\nrating_negative","721f40df":"rating_negative_transformed = bow_transformer.transform([rating_negative])\nnb.predict(rating_negative_transformed)[0]","d6deb42d":"X_predict_recommend=df['Review Text']\ny_recommend=df['Recommended IND']\n\nbow_transformer=CountVectorizer(analyzer=text_process).fit(X_predict_recommend)\n\nX_predict_recommend = bow_transformer.transform(X_predict_recommend)\n\nX_train, X_test, y_train, y_test = train_test_split(X_predict_recommend, y_recommend, test_size=0.3, random_state=101)\n\nnb = MultinomialNB()\nnb.fit(X_train, y_train)\n\npredict_recommendation=nb.predict(X_test)\n\n\nprint(confusion_matrix(y_test, predict_recommendation))\nprint('\\n')\nprint(classification_report(y_test, predict_recommendation))","1ea62035":"rating_positive","767b70aa":"rating_positive_transformed = bow_transformer.transform([rating_positive])\nnb.predict(rating_positive_transformed)[0]","535d9d29":"rating_negative","992054ca":"rating_negative_transformed = bow_transformer.transform([rating_negative])\nnb.predict(rating_negative_transformed)[0]","a86a65ff":"Avant d\u2019ajuster les mod\u00e8les, j\u2019ai divis\u00e9 les donn\u00e9es en formation et en test. Ensuite, j'ai mont\u00e9 les mod\u00e8les un \u00e0 un pour savoir le mod\u00e8le que je vais travailler avec","e6ec9676":"Sur le graphique \u00e0 barres ci-dessus, nous pouvons dire que le groupe d\u2019\u00e2ge des 10 \u00e0 20 ans a attribu\u00e9 une note moins \u00e9lev\u00e9e. Il est \u00e9vident. Dans ce groupe d'\u00e2ge, les adolescents ne se soucient g\u00e9n\u00e9ralement pas des achats en ligne et des critiques. Le groupe d'\u00e2ge 20-40 ont donn\u00e9 le plus 5 points par rapport \u00e0 tous les autres groupes d'\u00e2ge. En fait, c\u2019est le groupe d\u2019\u00e2ge qui a attribu\u00e9 le plus grand nombre de critiques et d\u2019\u00e9valuations. De m\u00eame, le groupe d\u2019\u00e2ge au-dessus de 70 ans ne s\u2019int\u00e9ressait pas aux achats en ligne.","a7e0561e":"## Neural Network","45779aa1":"## On va tester maintenat le mod\u00e8le avec notre Data","adac729e":"Deuxi\u00e8mement, je veux tester avec la critique n\u00e9gative. J'ai choisi l'examen ci-dessous et sa note est 1. Apr\u00e8s \u00e9valuation, il devrait pr\u00e9dire la note 1.","22247fce":"Une fois que nous avions pr\u00e9dit les valeurs, la t\u00e2che la plus importante consiste maintenant \u00e0 v\u00e9rifier et \u00e0 \u00e9valuer notre mod\u00e8le par rapport aux cotes r\u00e9elles (stock\u00e9es dans y_test) \u00e0 l'aide de confusion_matrix et classification_report de Scikit-learn.","7d7f6e74":"Selon le graphique ci-dessus, il y a plus de critiques positives, mais l\u00e0 encore, cela d\u00e9pend de la valeur de la polarit\u00e9. Je pense que la valeur du sentiment > 0 est un avis positif.","027228e9":"Maintenant on va voir quelques attributs en fonction des autres:","cc2933b5":"## Naive Bayes","b659689b":"Le diagramme en barres ci-dessus calcule la fr\u00e9quence du mot dans la colonne Texte de r\u00e9vision. Le mot dress est apparu plus dans le texte. A c\u00f4t\u00e9 de cela, le mot Love vient en second, ce qui est un indicateur de r\u00e9vision positive.","209c533d":"## test des don\u00e9es:","7a3a57dc":"## Logistic Regression","5ad990e0":"Notre mod\u00e8le donc a atteint une efficacit\u00e9 de 95%. Cela signifie que l'entreprise peut pr\u00e9dire si les utilisateurs ont aim\u00e9 le produit ou non","7a114dc9":"Pour le moment, nous avons notre colonne de text revision sous forme de jeton (qui n\u2019a pas de ponctuation ni de mots vides). CountVectorizer de Scikit-learn peut convertir la collection de textes en une matrice de comptes de jetons. Vous pouvez imaginer cette matrice r\u00e9sultante comme une matrice 2D, o\u00f9 chaque ligne est un mot unique et chaque colonne est une r\u00e9vision.","2f638837":"Le wordcloud ci-dessus concerne les mots les plus utilis\u00e9s dans la colonne Texte de r\u00e9vision.","50379a29":"  Maintenant, je vais savoir quelques informtions sur les donn\u00e9es comme les types d'index et des colonnes, les valeurs non null et l'utilisation de la m\u00e9moire:","1b821a5f":"On remarque que ces donn\u00e9es contient 23486 entr\u00e9es et 9 colonnes. Certaines des entr\u00e9es manquent, comme Titre, Nom de la division, Nom du d\u00e9partement et Nom de la classe.","6f979bb0":"Au niveau de nos donn\u00e9es, il y a 3 divisions qui sont General, General Petite et Intimates. Les produits de la division g\u00e9n\u00e9rale ont \u00e9t\u00e9 plus vendus que les produits g\u00e9n\u00e9raux Petite et Intimates. Environ 14 000 produits ont \u00e9t\u00e9 vendus dans la division g\u00e9n\u00e9rale, 8 000 dans la division g\u00e9n\u00e9rale Petite et environ 1 600 produits ont \u00e9t\u00e9 vendus dans la division Initmates.","1b56efce":"Le diagramme en barres ci-dessus montre qu'il y a un nombre maximal d'entr\u00e9es pour Top, qui se situe autour de 10500. Ensuite, le service des robes compte environ 6000 entr\u00e9es","8031b966":"on a remarqu\u00e9 qu'il y a des texte de revue qui sont long et d'autres tr\u00e8s courts Alors il y a-t-il un corr\u00e9lation entre la longueure des commentaires \u00e9crites par les critqueurs et la note qu'ils donnent?","97495774":"Comme nous n'avons pas de colonne indiquant le sentiment positif ou n\u00e9gatif dans l'ensemble de donn\u00e9es, j'ai d\u00e9fini une nouvelle colonne. Pour ce faire, j'ai suppos\u00e9 que les commentaires dont l'\u00e9valuation \u00e9tait \u00e9gale ou sup\u00e9rieure \u00e0 4 (positive dans le nouveau cadre de donn\u00e9es) et \u00e0 l'\u00e9valuation n\u00e9gative les autres (faux dans le nouveau cadre de donn\u00e9es)","2c457d61":"La carte de corr\u00e9lation ci-dessus montre qu'il n'y a pas beaucoup de corr\u00e9lation entre les colonnes. Les colonnes telles que Review Length et positive Feedback Count sont l\u00e9g\u00e8rement corr\u00e9l\u00e9es. Et le nombre 0,93 en n\u00e9gatif indique qu'il n'y a aucune corr\u00e9lation entre positive Feedback count et age. \u00c0 mesure que l'\u00e2ge augmente, la dur\u00e9e de l'examen diminue.","c78c80fc":"Dans le graphique ci-dessus, je veux me concentrer sur le d\u00e9partement et le groupe d'\u00e2ge pour vor chaque trange d'age par quoi elle est interss\u00e9es. Les femmes \u00e2g\u00e9es de 20 \u00e0 60 ans \u00e9taient plus actives et achetaient le produit en ligne. \u00c0 partir du diagramme en barres ci-dessus, nous pouvons conclure que toutes les femmes \u00e9taient plus concentr\u00e9es sur le d\u00e9partement Tops et Dressess. Et, un peu concentr\u00e9 sur bottoms aussi mais pas tant que \u00e7a. Ils \u00e9taient moins concentr\u00e9s sur le d\u00e9partement Trend.","598e9f76":"Aujourd'hui, je vais vous pr\u00e9senter mon premier travail sur Kaggle; au niveau de cette anayse je vais faire quelques critiques positives et n\u00e9gatives sur les diff\u00e9rentes avis des visiteurs: Au niveau de ce travail, j'ai explor\u00e9 les choses suivante: -\n\nLe groupe d'\u00e2ge de la femme qui a \u00e9crit le plus, le moins et tr\u00e8s peu de critiques \n\nle type de produit achet\u00e9 par chaque tranche d'age (noms de d\u00e9partement,noms de classe ) \n\nPour chaque d\u00e9partement quel est le nombre ou la pourcentage de class name qu'il a\n\nPour chaque devision nom quel est le nombre departement name \n\nPour chaque division name quel est le nombre des name classe \n\nCompter la fr\u00e9quence des mots dans la colonne Texte de r\u00e9vision \n\nWordcloud de la colonne Review Text \n\nColonne revision text - Combien de critiques positives, n\u00e9gatives et neutres sont bas\u00e9es sur le sentiment et la valeur de polarit\u00e9. \n\nWordcloud de la colonne positive Review Text \n\nWordcloud de la colonne de texte de r\u00e9vision n\u00e9gatif \n\nUtiliser l\u2019algorithme multinomial na\u00eff Bayes pour pr\u00e9dire quel produit a la note 5 et quel a la note1? \n\nUtilisation de l'algorithme multinomial Naive Bayes pour pr\u00e9dire quel produit est recommand\u00e9 et lequel ne l'est pas?","a8d34f53":"## Construction d'un classificateur de sentiments:","b020041b":"De la bo\u00eete \u00e0 moustaches ci-dessus, nous pouvons conclure que les \u00e9valuations 3 et 4 ont plus de lenteur en revue.","1fb5ede6":"\nMaintenant, le nombre ci-dessus est la taille du vocabulaire stock\u00e9 dans le vectoriseur (bas\u00e9 sur X_review)","7a7e0c15":"## Vectorisation","b1c659ea":"On va analyser les diff\u00e9rentes ages afin de voir la tranche d'age qui domine:","077472e0":"Maintenat on va pr\u00e9dire si un produit est recommand\u00e9 ou nn?","78ce05cd":"Le wordcloud ci-dessus uniquement pour les commentaires positifs.","c5c6f05d":"Tout d'abord, je veux tester avec la critique positive. J'ai choisi l'examen ci-dessus et sa note est 5. Apr\u00e8s \u00e9valuation, il devrait pr\u00e9dire une note de 5.","4cb46c11":"Le wordcloud ci-dessus que pour les commentaires n\u00e9gatifs.","721c8c01":"Maintenat, on va donner un sens visuel des don\u00e9es: on va colorier les donn\u00e9es en fonction de la colonne \"Rating\"","287274af":"## test du mod\u00e8le:","68cca932":"Pour pr\u00e9dire les notes des avis, j'ai utilis\u00e9 l\u2019algorithme Naive Bayes Machine Learning. Depuis cela fonctionne bien avec les donn\u00e9es de texte.","f17fe921":"Voici comment fonctionne la fonction ci-dessus. En gros, la fonction ci-dessus supprime les ponctuations, convertit les mots en minuscules et supprime les mots vides de la phrase.","48301206":"## Support Vector Machine (SVM)"}}