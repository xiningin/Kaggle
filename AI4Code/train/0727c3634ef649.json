{"cell_type":{"709cb6d7":"code","55204327":"code","83ba26e1":"code","19e1d003":"code","891b78e3":"code","09ffd460":"code","405ca98b":"code","77d2ce0f":"code","13bee46b":"code","e54be3d6":"code","c5aea34e":"code","c297d404":"code","e50aa041":"code","299201df":"code","7390c950":"code","9dfbe889":"code","81aa9b6f":"code","c566df0c":"code","d21413f2":"code","f506abbd":"code","70180cce":"code","79db6324":"code","307d0f30":"code","a04c0814":"code","7c45fba6":"code","3fe5df04":"markdown","25713b74":"markdown","40b4ad8d":"markdown","0563d889":"markdown","05281d5c":"markdown","8c9a6ea4":"markdown","dc94210b":"markdown","fedb7cfe":"markdown","afcf0a4a":"markdown","a9019084":"markdown","07fe7ee5":"markdown","a2d01e5c":"markdown","afc7c2e9":"markdown","a3f9cf1b":"markdown","9af9f0b3":"markdown","f6c7f287":"markdown","0c66acfb":"markdown","b1437a78":"markdown","1d463eff":"markdown","0a09f748":"markdown","79a8149a":"markdown","97384630":"markdown","fec78f4b":"markdown","8048305e":"markdown","fb905526":"markdown"},"source":{"709cb6d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","55204327":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom pandas import set_option\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC,LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report,confusion_matrix, plot_confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.pipeline import Pipeline","83ba26e1":"dataset = pd.read_excel('\/kaggle\/input\/covid19\/dataset.xlsx')\ndataset.head()","19e1d003":"dataset.isnull().sum()","891b78e3":"dataset.drop(['Patient ID'], axis=1, inplace = True)\nnumeric_cols = []\ncategory_cols = []\nobject_cols = []","09ffd460":"for c in dataset.columns:\n    print(\"Col {} Format {}\".format(c, dataset[c].dtype ) )\n    if(dataset[c].dtype == np.float64 or dataset[c].dtype == np.int64):\n          numeric_cols.append(c)\n  \n    else:\n          category_cols.append(c)","405ca98b":"positives = dataset['SARS-Cov-2 exam result'] == 'positive'\nnegatives = dataset['SARS-Cov-2 exam result'] == 'negatives'\n\nfor nu in numeric_cols:\n    meancolpos = dataset[nu][positives].mean()\n    meancolneg = dataset[nu][negatives].mean()\n       \n    dataset[nu][positives].fillna(meancolpos, inplace = True)\n    dataset[nu][negatives].fillna(meancolpos, inplace = True)","77d2ce0f":"for ca in category_cols:\n    dataset[ca].fillna('Not tested\/Not Aplicable', inplace = True)\n    \n    onehotenc = pd.get_dummies(dataset[ca]) \n    \n    for o in onehotenc.columns:\n        onehotenc.rename(columns={ o: ca + \"-\" + str(o)}, inplace=True)\n    \n    dataset.drop([ca], axis=1,inplace = True)\n    \n    dataset = pd.concat([dataset, onehotenc], axis=1)","13bee46b":"dataset.isnull().sum()\n\nresisnull = dataset.isnull().sum()\n\nprint(resisnull)\n\nfor i, v in resisnull.items():  \n    if(v > 0):\n        print(\"Dropping Col {}\".format(i))\n        dataset.drop([i], axis=1,inplace = True)\n        \nresisnull = dataset.isnull().sum()\nprint(resisnull)","e54be3d6":"y = dataset[\"SARS-Cov-2 exam result-positive\"].values\ndataset.drop([\"SARS-Cov-2 exam result-positive\", \"SARS-Cov-2 exam result-negative\"], axis =1, inplace= True)\n","c5aea34e":"set_option('precision', 2)\ncorr = dataset.corr(method='pearson')\nsns.heatmap(corr)","c297d404":"columns = np.full((corr.shape[0],), True, dtype=bool)\nfor i in range(corr.shape[0]):\n    for j in range(i+1, corr.shape[0]):\n        if corr.iloc[i,j] >= 0.9 or corr.iloc[i,j] <= -0.9:\n            if columns[j]:\n                columns[j] = False\nselected_columns = dataset.columns[columns]\ndataset = dataset[selected_columns]\n","e50aa041":"set_option('precision', 2)\ncorr = dataset.corr(method='pearson')\nsns.heatmap(corr)","299201df":"print(dataset.shape)\n\nX = dataset.values\nseed = 7\ntest_size = 0.30\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)\nX_select = StandardScaler().fit_transform(X)","7390c950":"lsvc = LinearSVC(C=0.1, penalty=\"l1\", dual=False).fit(X_select, y)\nmodel = SelectFromModel(lsvc, prefit=True)\nX_new = model.transform(X_select)\n\nprint(X_new.shape)\n\nmask = model.get_support() #list of booleans\nnew_features = [] # The list of your K best features\nfeature_names = list(dataset.columns.values)\n\nfor bool, feature in zip(mask, feature_names):\n    if bool:\n        new_features.append(feature)\n        print(feature)","9dfbe889":"pipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('clf', SVC())\n])\n\n\nlist_C = [0.001, 0.01, 0.1, 1, 10,100]\nlist_gamma = [0.001, 0.01, 0.1, 1, 10, 100]\n\nparametros_grid = dict(clf__C=list_C, clf__gamma=list_gamma)\ngridSVM = GridSearchCV(pipe, parametros_grid, cv=5, scoring='f1_macro', verbose = 10, n_jobs  = 4)\ngridSVM.fit(X_train,y_train)\n\nprint(gridSVM.best_score_)\nprint(gridSVM.best_params_)","81aa9b6f":"y_true, y_pred = y_test, gridSVM.predict(X_test)\nprint(classification_report(y_true, y_pred))","c566df0c":"bestPipe = gridSVM.best_estimator_\nbestSVM = bestPipe['clf']\nprint(bestSVM)\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\nplot_confusion_matrix(bestSVM, X_test, y_test, normalize = 'true')","d21413f2":"X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=test_size, random_state=seed)\n\npipe = Pipeline([\n    ('scaler',StandardScaler()),\n    ('clf', SVC())\n])\n\n\nlist_C = [0.001, 0.01, 0.1, 1, 10,100]\nlist_gamma = [0.001, 0.01, 0.1, 1, 10, 100]\n\nparametros_grid = dict(clf__C=list_C, clf__gamma=list_gamma)\ngridSVM = GridSearchCV(pipe, parametros_grid, cv=5, scoring='f1_macro', verbose = 10, n_jobs  = 4)\ngridSVM.fit(X_train,y_train)\n\nprint(gridSVM.best_score_)\nprint(gridSVM.best_params_)","f506abbd":"y_true, y_pred = y_test, gridSVM.predict(X_test)\nprint(classification_report(y_true, y_pred))\n\nbestPipe = gridSVM.best_estimator_\nbestSVM = bestPipe['clf']\nprint(bestSVM)\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\nplot_confusion_matrix(bestSVM, X_test, y_test, normalize = 'true')","70180cce":"X_rf = StandardScaler().fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_xgb, y, test_size=test_size, random_state=seed)\n\n\nforest = RandomForestClassifier(random_state = 1)\n\nn_estimators = [100, 300, 500, 800]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15]\nmin_samples_leaf = [1, 2, 5] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\nGridSearchCV(forest, hyperF, cv = 5, verbose = 1, n_jobs = 10, scoring='f1_macro')\n\nbestpipe = gridF.fit(X_train, y_train)\n","79db6324":"print(gridF.best_score_)\nprint(gridF.best_params_)","307d0f30":"print(classification_report(y_true, y_pred))\n\nbestEst = gridF.best_estimator_\nprint(bestEst)\n\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\nplot_confusion_matrix(bestEst, X_test, y_test, normalize = 'true')","a04c0814":"X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=test_size, random_state=seed)\n\n\nforest = RandomForestClassifier(random_state = 1)\n\nn_estimators = [100, 300, 500, 800]\nmax_depth = [5, 8, 15, 25, 30]\nmin_samples_split = [2, 5, 10, 15]\nmin_samples_leaf = [1, 2, 5] \n\nhyperF = dict(n_estimators = n_estimators, max_depth = max_depth,  \n              min_samples_split = min_samples_split, \n             min_samples_leaf = min_samples_leaf)\n\nGridSearchCV(forest, hyperF, cv = 5, verbose = 1, n_jobs = 10, scoring='f1_macro')\n\ngridF.fit(X_train, y_train)\n","7c45fba6":"print(classification_report(y_true, y_pred))\n\nbestEst = gridF.best_estimator_\nprint(bestEst)\n\n\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)\nplot_confusion_matrix(bestEst, X_test, y_test, normalize = 'true')","3fe5df04":"Store class values:","25713b74":"Get all positives and negatives samples for COVID-19 and fill all \"na\" with the meaning of each class.","40b4ad8d":"Verify the correlation of all features:","0563d889":"Creates a list of all Categorical Cols and applies one hot enconding for these categorical features:","05281d5c":"## Experiment #2 SVM with features from LinearSVM","8c9a6ea4":"Verifing all nullable values.","dc94210b":"Verify the correlation one more time after romoves high correlations:","fedb7cfe":"Creates de Matriz X, training and test set:","afcf0a4a":"Dropping Patient ID.","a9019084":"Training the first model using SVM-RBF and Search the hyper-parameters using GridSearch, f1_macro were used because we have an unbalanced  data:","07fe7ee5":"## Data pre-processing","a2d01e5c":"## Experiment #3 Random Forest with All features","afc7c2e9":"Apply a LinearSVM in order to select the more important features:","a3f9cf1b":"## Experiment #4 Random Forest with Selected Features","9af9f0b3":"Analysing the results with training set","f6c7f287":"Creates a List of all Numeric cols and Categorical cols.","0c66acfb":"The best result was obtained using SVM with the characteristics selected through LinearSVM, since the dataset is unbalanced, we analyzed the f1_macro as an important measure to decide which is the best classification.\n\nIn the confusion matrix we can analyze that for all classifiers the model still makes a lot of wrong classifications  for positive records, however this is also due to the nature of the datset.\n\nOther methods of feature selection can be applied such as heuritics (Genetic algorithms for example) and other types of classifiers such as neural networks and essemble.","b1437a78":"## Experiment #1 SVM with all features","1d463eff":"Plot the confusion Matrix:","0a09f748":"Remove features with high correlation:","79a8149a":"## Feature Selection using Linear SVC","97384630":"Plot the confusion Matrix Experiment #2:","fec78f4b":"# Results:","8048305e":"**TASK 1**\n\nThis  notebooks show an approch with Support Vector Machines and Random Forest in order to solve the Task one of the challenge. The main steps are described as bellow:\n\n* Pre-processing\n* Feature Selecion\n* Search Paramters ans Classification with SVM and Random Forest\n* Results and analysis\n","fb905526":"Special Treatment for Urine - pH:"}}