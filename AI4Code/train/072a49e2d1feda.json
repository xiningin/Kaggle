{"cell_type":{"4a30e0dc":"code","7daa7736":"code","78343ff3":"code","21f4722a":"code","c7ed1c4c":"code","efa71d34":"code","a28a476f":"code","73393274":"markdown","2c756595":"markdown","61fa0e92":"markdown","7a83a677":"markdown","78bfcb65":"markdown","e2b0b2df":"markdown","6d27ad91":"markdown"},"source":{"4a30e0dc":"import numpy as np\nimport os\nimport tensorflow as tf\nimport librosa\nimport time\nimport random\nfrom tensorflow.contrib import rnn\n\naudio_path = \"..\/input\/vctk-corpus\/VCTK-Corpus\/wav48\"\n\nclass hyperparam:\n    train_path = \"train\"\n    test_path = \"test\"\n    model_path = \"model\"\n    \n    # spectrogram params\n    tisv_frame = 180 # max frame number of utterances of tdsv\n    hop = 0.01 # hop size (s)\n    window = 0.025 # window length (s)\n    sr = 8000 # sampling rate\n    \n    # model params\n    num_layer = 3 # number of lstm layers\n    hidden = 128 # hidden state dimension of lstm\n    proj = 64 # projection dimension of lstm\n    \n    # training params\n    N = 4 # number of speakers of batch \n    M = 5 # number of utterances per speaker\n    train = True\n    optim = 'sgd' # optimizer type\n    loss = 'softmax' # loss type (softmax or contrast)\n\n    iteration = 10000\n    lr = 1e-2\n    \n    # test params\n    tdsv = False # text dependent or not\n    model_num = 1 # number of ckpt file to load\n    \nconfig = hyperparam()\n\ntf.test.gpu_device_name()","7daa7736":"def save_spectrogram_tisv(audio_path, cutting = True, cutted = 30, cut_utter = 100):\n    \"\"\" Full preprocess of text independent utterance. The log-mel-spectrogram is saved as numpy file.\n        Each partial utterance is splitted by voice detection using DB\n        and the first and the last 180 frames from each partial utterance are saved. \n        Need : utterance data set (VTCK)\n    \"\"\"\n    print(\"start text independent utterance feature extraction\")\n    os.makedirs(config.train_path, exist_ok=True)   # make folder to save train file\n    os.makedirs(config.test_path, exist_ok=True)    # make folder to save test file\n\n    total_speaker_num = len(os.listdir(audio_path))\n    \n    if cutting:\n        train_speaker_num= (cutted\/\/10)*8 # for cutting set\n    else:\n        train_speaker_num= (total_speaker_num\/\/10)*9  # split total data 90% train and 10% test\n    \n    \n    print(\"total speaker number : %d\"%total_speaker_num)\n    if cutting:\n        print(\"train : %d, test : %d\"%(train_speaker_num, cutted+1-train_speaker_num))\n    else:\n        print(\"train : %d, test : %d\"%(train_speaker_num, total_speaker_num-train_speaker_num))\n    print(os.listdir(audio_path))\n    \n    for i, folder in enumerate(os.listdir(audio_path)):\n        speaker_path = os.path.join(audio_path, folder)     # path of each speaker\n        print(\"%d th speaker processing...\" % i)\n        utterances_spec = []\n        k=0\n        for j, utter_name in enumerate(os.listdir(speaker_path)):\n            utter_path = os.path.join(speaker_path, utter_name)         # path of each utterance\n            utter, sr = librosa.core.load(utter_path, config.sr)        # load utterance audio\n            # trim the silence in the audio. the interval of utter_trim corresponding to the non-silent region\n            utter_trim, index = librosa.effects.trim(utter, top_db=20)\n            \n            cur_slide = 0\n            mfcc_win_sample = int(config.sr*config.hop*config.tisv_frame) # the length of a mfcc window\n            while(True):\n                if(cur_slide + mfcc_win_sample > utter_trim.shape[0]):\n                    break\n                slide_win = utter_trim[cur_slide : cur_slide+mfcc_win_sample]\n\n                S = librosa.feature.mfcc(y=slide_win, sr=config.sr, n_mfcc=40)\n                utterances_spec.append(S)\n                cur_slide += int(mfcc_win_sample\/2)\n            if cutting:\n                if j > cut_utter:\n                    break\n                \n        utterances_spec = np.array(utterances_spec)\n        print('utterances_spec.shape = {}'.format(utterances_spec.shape))\n\n        if i<train_speaker_num:      # save spectrogram as numpy file\n            np.save(os.path.join(config.train_path, \"speaker%d.npy\"%i), utterances_spec)\n        else:\n            np.save(os.path.join(config.test_path, \"speaker%d.npy\"%(i-train_speaker_num)), utterances_spec)\n        \n        if cutting:\n            if i > cutted-1:\n                break\n        \n        ","78343ff3":"def random_batch(shuffle=True, noise_filenum=None, utter_start=0):\n    \"\"\" Generate 1 batch.\n        For TD-SV, noise is added to each utterance.\n        For TI-SV, random frame length is applied to each batch of utterances (140-180 frames)\n        speaker_num : number of speaker of each batch\n        utter_num : number of utterance per speaker of each batch\n        shuffle : random sampling or not\n        noise_filenum : specify noise file or not (TD-SV)\n        utter_start : start point of slicing (TI-SV)\n    :return: 1 random numpy batch (frames x batch(NM) x n_mels)\n    \"\"\"\n    speaker_num = config.N\n    utter_num = config.M \n    # data path\n    if config.train:\n        path = config.train_path\n    else:\n        path = config.test_path\n    \n    # TI-SV \u672c\u5b9e\u9a8c\u4f7f\u7528\uff1a \u4e0e\u6587\u672c\u65e0\u5173\u8bf4\u8bdd\u4eba\u9274\u522b\n    np_file_list = os.listdir(path)\n    total_speaker = len(np_file_list)\n\n    if shuffle:\n        selected_files = random.sample(np_file_list, speaker_num)  # select random N speakers\n    else:\n        selected_files = np_file_list[:speaker_num]                # select first N speakers\n            \n    utter_batch = []\n    for file in selected_files:\n        utters  =  np.load(os.path.join(path, file))        # load utterance spectrogram of selected speaker\n        if shuffle:\n            utter_index = np.random.randint(0, utters.shape[0], utter_num)   # select M utterances per speaker\n            utter_batch.append(utters[utter_index])       # each speaker's utterance [M, n_mels, frames] is appended\n        else:\n            utter_batch.append(utters[utter_start: utter_start+utter_num])\n\n    utter_batch = np.concatenate(utter_batch, axis=0)     # utterance batch [batch(NM), n_mels, frames]\n\n    if config.train:\n        frame_slice = np.random.randint(140,181)          # for train session, random slicing of input batch\n        utter_batch = utter_batch[:,:,:frame_slice]\n    else:\n        utter_batch = utter_batch[:,:,:160]               # for train session, fixed length slicing of input batch\n\n    utter_batch = np.transpose(utter_batch, axes=(2,0,1))     # transpose [frames, batch, n_mels]\n\n    return utter_batch","21f4722a":"def train(path):\n    tf.reset_default_graph()    # reset graph\n    # draw graph\n    batch = tf.placeholder(shape= [None, config.N*config.M, 40], dtype=tf.float32)  # input batch (time x batch x n_mel)\n    lr = tf.placeholder(dtype= tf.float32)  # learning rate\n    global_step = tf.Variable(0, name='global_step', trainable=False)\n    w = tf.get_variable(\"w\", initializer= np.array([10], dtype=np.float32))\n    b = tf.get_variable(\"b\", initializer= np.array([-5], dtype=np.float32))\n\n    # embedding lstm (3-layer default)\n    with tf.variable_scope(\"lstm\"):\n        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # define lstm op and variables\n        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n        embedded = normalize(embedded)                    # normalize\n    print(\"embedded size: \", embedded.shape)\n    # loss\n    sim_matrix = similarity(embedded, w, b)\n    print(\"similarity matrix size: \", sim_matrix.shape)\n    loss = loss_cal(sim_matrix, type=config.loss)\n\n    # optimizer operation\n    trainable_vars= tf.trainable_variables()                # get variable list\n    optimizer= optim(lr)                                    # get optimizer (type is determined by configuration)\n    grads, vars= zip(*optimizer.compute_gradients(loss))    # compute gradients of variables with respect to loss\n    grads_clip, _ = tf.clip_by_global_norm(grads, 3.0)      # l2 norm clipping by 3\n    grads_rescale= [0.01*grad for grad in grads_clip[:2]] + grads_clip[2:]   # smaller gradient scale for w, b\n    train_op= optimizer.apply_gradients(zip(grads_rescale, vars), global_step= global_step)   # gradient update operation\n\n    # check variables memory\n    variable_count = np.sum(np.array([np.prod(np.array(v.get_shape().as_list())) for v in trainable_vars]))\n    print(\"total variables :\", variable_count)\n\n    # record loss\n    loss_summary = tf.summary.scalar(\"loss\", loss)\n    merged = tf.summary.merge_all()\n    saver = tf.train.Saver()\n\n    # training session\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n\n#        if(os.path.exists(path)):\n#            print(\"Restore from {}\".format(os.path.join(path, \"Check_Point\/model.ckpt-2\")))\n#            saver.restore(sess, os.path.join(path, \"Check_Point\/model.ckpt-2\"))  # restore variables from selected ckpt file\n#        else:\n#            os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n#            os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n\n        os.makedirs(os.path.join(path, \"Check_Point\"), exist_ok=True)  # make folder to save model\n        os.makedirs(os.path.join(path, \"logs\"), exist_ok=True)          # make folder to save log\n        \n        writer = tf.summary.FileWriter(os.path.join(path, \"logs\"), sess.graph)\n        epoch = 0\n        lr_factor = 1   # lr decay factor ( 1\/2 per 10000 iteration)\n        loss_acc = 0    # accumulated loss ( for running average of loss)\n\n        for iter in range(config.iteration):\n            # run forward and backward propagation and update parameters\n            _, loss_cur, summary = sess.run([train_op, loss, merged],\n                                  feed_dict={batch: random_batch(), lr: config.lr*lr_factor})\n            loss_acc += loss_cur    # accumulated loss for each 100 iteration\n            if iter % 10 == 0:\n                writer.add_summary(summary, iter)   # write at tensorboard\n            if (iter+1) % 100 == 0:\n                print(\"(iter : %d) loss: %.4f\" % ((iter+1),loss_acc\/100))\n                loss_acc = 0                        # reset accumulated loss\n            if (iter+1) % 2000 == 0:\n                lr_factor \/= 2                      # lr decay\n                print(\"learning rate is decayed! current lr : \", config.lr*lr_factor)\n            if (iter+1) % 5000 == 0:\n                saver.save(sess, os.path.join(path, \".\/Check_Point\/model.ckpt\"), global_step=iter\/\/5000)\n                print(\"model is saved!\")","c7ed1c4c":"def similarity(embedded, w, b, N=4, M=5, P=64, center=None):\n    \"\"\" Calculate similarity matrix from embedded utterance batch (NM x embed_dim) eq. (9)\n        Input center to test enrollment. (embedded for verification)\n    :return: tf similarity matrix (NM x N)\n    \"\"\"\n    embedded_split = tf.reshape(embedded, shape=[N, M, P])\n\n    if center is None:\n        center = normalize(tf.reduce_mean(embedded_split, axis=1))              # [N,P] normalized center vectors eq.(1)\n        center_except = normalize(tf.reshape(tf.reduce_sum(embedded_split, axis=1, keep_dims=True)\n                                             - embedded_split, shape=[N*M,P]))  # [NM,P] center vectors eq.(8)\n        # make similarity matrix eq.(9)\n        S = tf.concat(\n            [tf.concat([tf.reduce_sum(center_except[i*M:(i+1)*M,:]*embedded_split[j,:,:], axis=1, keep_dims=True) if i==j\n                        else tf.reduce_sum(center[i:(i+1),:]*embedded_split[j,:,:], axis=1, keep_dims=True) for i in range(N)],\n                       axis=1) for j in range(N)], axis=0)\n    else :\n        # If center(enrollment) exist, use it.\n        S = tf.concat(\n            [tf.concat([tf.reduce_sum(center[i:(i + 1), :] * embedded_split[j, :, :], axis=1, keep_dims=True) for i\n                        in range(N)],\n                       axis=1) for j in range(N)], axis=0)\n\n    S = tf.abs(w)*S+b   # rescaling\n\n    return S\n\n\ndef normalize(x):\n    \"\"\" normalize the last dimension vector of the input matrix\n    :return: normalized input\n    \"\"\"\n    return x\/tf.sqrt(tf.reduce_sum(x**2, axis=-1, keep_dims=True)+1e-6)\n\n\ndef cossim(x,y, normalized=True):\n    \"\"\" calculate similarity between tensors\n    :return: cos similarity tf op node\n    \"\"\"\n    if normalized:\n        return tf.reduce_sum(x*y)\n    else:\n        x_norm = tf.sqrt(tf.reduce_sum(x**2)+1e-6)\n        y_norm = tf.sqrt(tf.reduce_sum(y**2)+1e-6)\n        return tf.reduce_sum(x*y)\/x_norm\/y_norm\n    \ndef loss_cal(S, type=\"softmax\", N=config.N, M=config.M):\n    \"\"\" calculate loss with similarity matrix(S) eq.(6) (7) \n    :type: \"softmax\" or \"contrast\"\n    :return: loss\n    \"\"\"\n    S_correct = tf.concat([S[i*M:(i+1)*M, i:(i+1)] for i in range(N)], axis=0)  # colored entries in Fig.1\n\n    if type == \"softmax\":\n        total = -tf.reduce_sum(S_correct-tf.log(tf.reduce_sum(tf.exp(S), axis=1, keep_dims=True) + 1e-6))\n    elif type == \"contrast\":\n        S_sig = tf.sigmoid(S)\n        S_sig = tf.concat([tf.concat([0*S_sig[i*M:(i+1)*M, j:(j+1)] if i==j\n                              else S_sig[i*M:(i+1)*M, j:(j+1)] for j in range(N)], axis=1)\n                             for i in range(N)], axis=0)\n        total = tf.reduce_sum(1-tf.sigmoid(S_correct)+tf.reduce_max(S_sig, axis=1, keep_dims=True))\n    else:\n        raise AssertionError(\"loss type should be softmax or contrast !\")\n\n    return total\n\ndef optim(lr):\n    \"\"\" return optimizer determined by configuration\n    :return: tf optimizer\n    \"\"\"\n    if config.optim == \"sgd\":\n        return tf.train.GradientDescentOptimizer(lr)\n    elif config.optim == \"rmsprop\":\n        return tf.train.RMSPropOptimizer(lr)\n    elif config.optim == \"adam\":\n        return tf.train.AdamOptimizer(lr, beta1=config.beta1, beta2=config.beta2)\n    else:\n        raise AssertionError(\"Wrong optimizer type!\")","efa71d34":"# Test Session\ndef test(path):\n    tf.reset_default_graph()\n\n    # draw graph\n    enroll = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32) # enrollment batch (time x batch x n_mel)\n    verif = tf.placeholder(shape=[None, config.N*config.M, 40], dtype=tf.float32)  # verification batch (time x batch x n_mel)\n    batch = tf.concat([enroll, verif], axis=1)\n\n    # embedding lstm (3-layer default)\n    with tf.variable_scope(\"lstm\"):\n        lstm_cells = [tf.contrib.rnn.LSTMCell(num_units=config.hidden, num_proj=config.proj) for i in range(config.num_layer)]\n        lstm = tf.contrib.rnn.MultiRNNCell(lstm_cells)    # make lstm op and variables\n        outputs, _ = tf.nn.dynamic_rnn(cell=lstm, inputs=batch, dtype=tf.float32, time_major=True)   # for TI-VS must use dynamic rnn\n        embedded = outputs[-1]                            # the last ouput is the embedded d-vector\n        embedded = normalize(embedded)                    # normalize\n\n    print(\"embedded size: \", embedded.shape)\n\n    # enrollment embedded vectors (speaker model)\n    enroll_embed = normalize(tf.reduce_mean(tf.reshape(embedded[:config.N*config.M, :], shape= [config.N, config.M, -1]), axis=1))\n    # verification embedded vectors\n    verif_embed = embedded[config.N*config.M:, :]\n\n    similarity_matrix = similarity(embedded=verif_embed, w=1., b=0., center=enroll_embed)\n\n    saver = tf.train.Saver(var_list=tf.global_variables())\n    with tf.Session() as sess:\n        tf.global_variables_initializer().run()\n        \n        # load model\n        print(\"model path :\", path)\n        ckpt = tf.train.get_checkpoint_state(checkpoint_dir=os.path.join(path, \"Check_Point\"))\n        ckpt_list = ckpt.all_model_checkpoint_paths\n        loaded = 0\n        for model in ckpt_list:\n            if config.model_num == int(model[-1]):    # find ckpt file which matches configuration model number\n                print(\"ckpt file is loaded !\", model)\n                loaded = 1\n                saver.restore(sess, model)  # restore variables from selected ckpt file\n                break\n\n        if loaded == 0:\n            raise AssertionError(\"ckpt file does not exist! Check config.model_num or config.model_path.\")\n\n        print(\"test file path : \", config.test_path)\n        '''\n            test speaker:p225--p243\n        '''\n        # return similarity matrix after enrollment and verification\n        time1 = time.time() # for check inference time\n        if config.tdsv:\n            S = sess.run(similarity_matrix, feed_dict={enroll:random_batch(shuffle=False, noise_filenum=1),\n                                                       verif:random_batch(shuffle=False, noise_filenum=2)})\n        else:\n            S = sess.run(similarity_matrix, feed_dict={enroll:random_batch(shuffle=False),\n                                                       verif:random_batch(shuffle=False, utter_start=config.M)})\n        S = S.reshape([config.N, config.M, -1])\n        time2 = time.time()\n\n        np.set_printoptions(precision=2)\n        print(\"inference time for %d utterences : %0.2fs\"%(2*config.M*config.N, time2-time1))\n        print(S)    # print similarity matrix\n\n        # calculating EER\n        diff = 1; EER=0; EER_thres = 0; EER_FAR=0; EER_FRR=0\n\n        # through thresholds calculate false acceptance ratio (FAR) and false reject ratio (FRR)\n        for thres in [0.01*i+0.5 for i in range(50)]:\n            S_thres = S>thres\n\n            # False acceptance ratio = false acceptance \/ mismatched population (enroll speaker != verification speaker)\n            FAR = sum([np.sum(S_thres[i])-np.sum(S_thres[i,:,i]) for i in range(config.N)])\/(config.N-1)\/config.M\/config.N\n\n            # False reject ratio = false reject \/ matched population (enroll speaker = verification speaker)\n            FRR = sum([config.M-np.sum(S_thres[i][:,i]) for i in range(config.N)])\/config.M\/config.N\n\n            # Save threshold when FAR = FRR (=EER)\n            if diff> abs(FAR-FRR):\n                diff = abs(FAR-FRR)\n                EER = (FAR+FRR)\/2\n                EER_thres = thres\n                EER_FAR = FAR\n                EER_FRR = FRR\n\n        print(\"\\nEER : %0.2f (thres:%0.2f, FAR:%0.2f, FRR:%0.2f)\"%(EER,EER_thres,EER_FAR,EER_FRR))","a28a476f":"tf.reset_default_graph()\n\nif __name__ == \"__main__\":\n    save_spectrogram_tisv(audio_path, cutting = True)\n    config.train = True\n    # start training\n    print(\"\\nTraining Session\")\n    with tf.device(\"\/gpu:0\"):\n        train(config.model_path)\n            \n    # start test\n    config.train = False\n    print(\"\\nTest session\")\n    if os.path.isdir(config.model_path):\n        with tf.device(\"\/gpu:0\"):\n            test(config.model_path)\n    else:\n        raise AssertionError(\"model path doesn't exist!\")","73393274":"# Speaker verification with GE2E loss (Text independent version)\n\nMainly based on https:\/\/github.com\/Janghyun1230\/Speaker_Verification","2c756595":"## 3. Define the loss\nand other utilities.","61fa0e92":"## 1. Data preparation\n\nWe use pack [librosa](http:\/\/librosa.github.io\/librosa\/) to preprocess the audio.\n\nThe functions we use:\n* [librosa.effects.trim](https:\/\/librosa.github.io\/librosa\/generated\/librosa.effects.trim.html?highlight=librosa%20effects%20trim#librosa.effects.trim)\n\n    Trim leading and trailing silence from an audio signal.\n\n* [librosa.core.load](https:\/\/librosa.github.io\/librosa\/generated\/librosa.core.load.html?highlight=librosa%20core%20load#librosa.core.load)\n    \n    Load an audio file as a floating point time series.\n    \n    Audio will be automatically resampled to the given rate (default sr=22050).\n\n    To preserve the native sampling rate of the file, use sr=None.\n\n* [librosa.feature.mfcc](https:\/\/librosa.github.io\/librosa\/generated\/librosa.feature.mfcc.html#librosa-feature-mfcc)\n\n    Mel-frequency cepstral coefficients. Return a MFCC sequence with shape [n_mfcc, t].\n    \nFor simplicity, we just use the first 30 files with its first 100 items in the datasets. You can adjust parameter `cutted` and `cut_utter` to change this setting.","7a83a677":"## 5. Train model","78bfcb65":"Organize batches.","e2b0b2df":"## 2. Define model and training session","6d27ad91":"## 4. Test session"}}