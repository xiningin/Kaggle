{"cell_type":{"7343e3f7":"code","23eda49a":"code","01604a49":"code","b8599469":"code","7c4cef81":"code","06d3f828":"code","de817d80":"code","62ef7e19":"code","779db418":"code","fbedd19c":"code","563d4f15":"code","b5ce8581":"code","8ce862e4":"code","413747c4":"code","bb0d5493":"code","6340ec0d":"code","1da60907":"code","79f951e9":"code","6074b988":"code","221bba71":"code","3c5daa51":"code","e6659baa":"code","45326244":"code","b2ec3eee":"code","295540dd":"code","7783a653":"code","025ae516":"code","719f865c":"code","f83a8ffa":"code","78cd2476":"code","4ef53d99":"code","f794a718":"code","e31ec4d6":"code","686d3b3c":"code","bafceb2a":"code","e6757d9e":"code","238ac105":"code","57bdeebb":"code","57c98761":"code","fb84eb08":"code","751ba22e":"code","ed3ffd10":"code","d104c81c":"markdown","dd494d34":"markdown","7c2adc3f":"markdown","8e0dd8f2":"markdown","6d333fc3":"markdown","e1ca9ee5":"markdown","3436a91a":"markdown","d0b5d583":"markdown","6ce52dd5":"markdown","042dd127":"markdown","f3c61671":"markdown","1d19dd4d":"markdown","21f0bd91":"markdown","efb30bc7":"markdown","d1128c7b":"markdown","02a051a4":"markdown","0d5cc43c":"markdown","d06403a0":"markdown"},"source":{"7343e3f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23eda49a":"train = pd.read_csv(\"\/kaggle\/input\/grocery-sales-forecast-weekend-hackathon\/Grocery_Sales_ParticipantsData\/Train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/grocery-sales-forecast-weekend-hackathon\/Grocery_Sales_ParticipantsData\/Test.csv\")\nsample = pd.read_excel(\"\/kaggle\/input\/grocery-sales-forecast-weekend-hackathon\/Grocery_Sales_ParticipantsData\/Sample_Submission.xlsx\")","01604a49":"print(train.head())\ntrain.shape","b8599469":"print(test.head())\ntest.shape","7c4cef81":"print(sample.head())\nsample.shape","06d3f828":"train.isnull().sum()","de817d80":"import matplotlib.cm as cm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.boxplot(train['GrocerySales'])","62ef7e19":"# We are deleting the outliers from quartile 27th percentile and 73th percentile as appropriate for our model.\nQ1 = train.quantile(0.27)\nQ3 = train.quantile(0.73)\nIQR = Q3 - Q1\nprint(IQR)","779db418":"train.shape","fbedd19c":"train1 = train[~((train < (Q1 - 1.5 * IQR)) |(train > (Q3 + 1.5 * IQR))).any(axis=1)]\ntrain1.shape","563d4f15":"plt.boxplot(train1['GrocerySales'])","b5ce8581":"plt.scatter(train['Day'], train['GrocerySales'],  color='black')","8ce862e4":"fig= plt.figure(figsize=(15,5))\nplt.plot(train1['Day'], train1['GrocerySales'], color='green',markersize=1)","413747c4":"#Fetaures \nX = train1['Day'].copy()\nX.head()","bb0d5493":"y = train1['GrocerySales'].copy()\ny.head()","6340ec0d":"X = np.array(train1['Day'])\nX=X.reshape(-1, 1)\nX","1da60907":"from sklearn.model_selection import train_test_split\ntrain_X, val_X, train_y, val_y = train_test_split(X, y,train_size=0.8, test_size=0.2,random_state = 0)","79f951e9":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nregr = LinearRegression()\nregr.fit(train_X,train_y)\n","6074b988":"predict = regr.predict(val_X)\nprint('Mean squared error: %.2f'\n      % mean_squared_error(val_y,predict))","221bba71":"plt.scatter(train['Day'], train['GrocerySales'],  color='black')\nplt.plot(val_X, predict, color='blue', linewidth=3)\n\nplt.xticks(())\nplt.yticks(())\n\nplt.show()","3c5daa51":"from sklearn.neighbors import KNeighborsRegressor\n\nneigh = KNeighborsRegressor()\nneigh.fit(train_X,train_y)\npredict_n = neigh.predict(val_X)\nprint('Mean squared error: %.2f'\n     % mean_squared_error(val_y,predict_n))","e6659baa":"parameters_for_testing = {\n   'algorithm':['auto', 'ball_tree', 'kd_tree', 'brute'],\n   'n_neighbors':[1,4,2],\n   'weights':['uniform','distance'],\n    \n}","45326244":"from sklearn.model_selection import GridSearchCV\ngsearch1 = GridSearchCV(cv=8,estimator = neigh, param_grid = parameters_for_testing, n_jobs=-1,verbose=0,scoring='neg_mean_squared_error')\ngsearch1.fit(train_X,train_y)","b2ec3eee":"print (gsearch1.best_params_)","295540dd":"print (gsearch1.best_score_)","7783a653":"predict_grid = gsearch1.predict(val_X)\nprint('Mean squared error: %.2f'\n     % np.sqrt(mean_squared_error(val_y,predict_grid)))","025ae516":"from sklearn import tree\nclf = tree.DecisionTreeRegressor()\nclf.fit(train_X,train_y)\npredict_clf = clf.predict(val_X)\nprint('Mean squared error: %.2f'\n     % mean_squared_error(val_y,predict_clf))","719f865c":"parameters = {\n   'criterion':['mse', 'friedman_mse', 'mae'],\n   'splitter':['best', 'random'],\n   'min_samples_split':[2,3,4,5]\n}","f83a8ffa":"gsearch2 = GridSearchCV(estimator = clf, param_grid = parameters, n_jobs=-1,verbose=0,scoring='neg_mean_squared_error')\ngsearch2.fit(train_X,train_y)","78cd2476":"print('best params')\nprint (gsearch2.best_params_)\nprint('best score')\nprint (gsearch2.best_score_)","4ef53d99":"predict_grid2 = gsearch2.predict(val_X)\nprint('Mean squared error: %.2f'\n     % (mean_squared_error(val_y,predict_grid2)))","f794a718":"import xgboost as xgb\nxg = xgb.XGBRegressor(eta=0.6599999999999999999999999999)\nxg = xg.fit(train_X,train_y)\npredict_x = xg.predict(val_X)\nprint('Mean squared error is: {}'.format((mean_squared_error(val_y,predict_x))))","e31ec4d6":"from sklearn.ensemble import VotingRegressor\nereg = VotingRegressor(estimators=[('nh', neigh), ('tree', gsearch1)])\nereg = ereg.fit(train_X,train_y)\npredict_clf = ereg.predict(val_X)\nprint('Mean squared error: %.2f'\n     % mean_squared_error(val_y,predict_clf))","686d3b3c":"from sklearn.ensemble import BaggingRegressor\nbag = BaggingRegressor(base_estimator=xg, n_estimators=14, random_state=0).fit(train_X,train_y)\npredict_bag = bag.predict(val_X)\nprint('Mean squared error is {}'.format((mean_squared_error(val_y,predict_bag))))\n","bafceb2a":"plt.scatter(train_X, train_y, color = 'blue') \n  \nplt.plot(train_X, bag.predict(train_X), color = 'red') \nplt.title('Bagging meta-estimator') \nplt.xlabel('Days') \nplt.ylabel('Sale') \n  \nplt.show() ","e6757d9e":"from catboost import CatBoostRegressor\n\nmodel = CatBoostRegressor()\n#train the model\nmodel.fit(train_X,train_y)\n# make the prediction using the resulting model\npreds = model.predict(val_X)\nprint('Mean squared error: %.2f'\n     % (mean_squared_error(val_y,preds)))","238ac105":"import lightgbm as lgb \nlg = lgb.LGBMRegressor()\nlg = lg.fit(train_X,train_y)\npredict_l = lg.predict(val_X)\nprint('Mean squared error: %.2f'\n     % (mean_squared_error(val_y,predict_l)))","57bdeebb":"test = np.array(test)\ntest=test.reshape(-1, 1)\ntest","57c98761":"bag = BaggingRegressor(base_estimator=xg, n_estimators=108, random_state=0).fit(X,y)","fb84eb08":"predict = bag.predict(test)","751ba22e":"df = pd.DataFrame(predict,columns=['GrocerySales'])\ndf.head()","ed3ffd10":"df.to_excel(\"submission2.xlsx\")","d104c81c":"# 1. Voting","dd494d34":"# Univirate Analysis","7c2adc3f":"# KNeighborsRegressor","8e0dd8f2":"# 2. Bagging","6d333fc3":"# Let's try different algorithms and check the mean square error","e1ca9ee5":"## Tuning of KNeighborsRegressor","3436a91a":"# Ensembling Methods","d0b5d583":"# Dataset","6ce52dd5":"# DecisionTreeRegressor","042dd127":"# Select features and labels","f3c61671":"# CatBoostRegressor","1d19dd4d":"# Deleting outliers","21f0bd91":"# lightgbm","efb30bc7":"# XGBOOST","d1128c7b":"# Split the data","02a051a4":"## Tuning of DecisionTreeRegressor","0d5cc43c":"Sales forecasting has always been one of the most predominant applications of machine learning. Big companies like Walmart have been employing this technique to achieve steady and enormous growth over decades now. In this challenge, you as a data scientist must use machine learning to help a small grocery store in predicting its future sales and making better business decisions.","d06403a0":"# LinearRegression"}}