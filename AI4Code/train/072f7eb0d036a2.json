{"cell_type":{"0ed78a5b":"code","700ea46b":"code","3ccbc591":"code","cd304ead":"code","507a21ee":"code","11f5fd82":"code","3647ee3a":"code","f0a30f31":"code","ce52bedf":"code","c0811546":"code","93eb9a4c":"code","1095b466":"code","cf35b5b4":"code","18dbc3a6":"code","064c6ff2":"code","7ca3d041":"code","e9e0d414":"code","868d42f9":"code","5b885410":"code","d52dc7bf":"code","2728de6f":"code","d48f993a":"code","b9e5aeb0":"code","102f6637":"code","57d505f1":"code","609b9a97":"code","a8bda371":"code","355ea9e0":"code","ffe319ad":"code","d6fb1cf7":"code","eb0aa6c7":"code","72e8f81d":"code","4a55b23d":"code","26a678e0":"code","2e2db986":"code","8a9c0ce5":"code","6ced552b":"code","98b58143":"code","6a5b819c":"code","2396dd2d":"code","9c26c7b9":"markdown","643c8810":"markdown","abd2c36e":"markdown","9ba5f29e":"markdown","18fefd70":"markdown","f5fe5db0":"markdown","10efbc03":"markdown","1a2253f0":"markdown","a2583f06":"markdown","4885d193":"markdown","dd435653":"markdown","e1c9ead4":"markdown","68a526f3":"markdown","6e3f4e7e":"markdown","aaee0f71":"markdown","a62c9408":"markdown","5e2f47e7":"markdown","8e71bfb2":"markdown","d3e647c4":"markdown","135dc5fc":"markdown"},"source":{"0ed78a5b":"# The usual imports :)\nimport pandas as pd\nimport numpy as np\nimport missingno as msno\nimport pandas_profiling as pdp\nimport lightgbm as lgb\nimport warnings\nimport datetime\n# No one likes warnings :p\nwarnings.simplefilter('ignore')","700ea46b":"# Some constants\n#\u00a0This is the augmented train produced by the following kernel:\n# https:\/\/www.kaggle.com\/yassinealouini\/start-here-features-engineering-pipeline\nTRAIN_DATA_PATH = \"..\/input\/augmented_train.csv\"\nTEST_DATA_PATH = \"..\/input\/augmented_test.csv\"\nDTYPE = {\"fullVisitorId\": str, \"sessionId\":str}\n#\u00a0The \"campaignCode\" column is only available in the train dataset.\n#\u00a0The \"targetingCriteria\" isn't useful at all (only NaN or {} values). \nTO_DROP_COLS = [\"campaignCode\", \"targetingCriteria\"]\nID_COLS = [\"fullVisitorId\", \"sessionId\", \"visitId\"]\n# Will only need this column for identifying the visitors. \nID_COL = \"fullVisitorId\"\nTARGET_COL = \"transactionRevenue\"\nID_COLS_TO_DROP = list(set(ID_COLS) - set([ID_COL]))\nTODAY = datetime.date.today()\nOUTPUT_DATA_PATH = f\"submission_{TODAY}.csv\"","3ccbc591":"train_df = (pd.read_csv(TRAIN_DATA_PATH, low_memory=False, dtype=DTYPE)\n              .drop(TO_DROP_COLS, axis=1))\n#\u00a0Will only use this to prepare the submission file at the end. \ntest_df = (pd.read_csv(TEST_DATA_PATH, low_memory=False, dtype=DTYPE)\n             .drop(TO_DROP_COLS, axis=1, errors=\"ignore\"))","cd304ead":"train_test_cols_diff = list(set(train_df.columns) - set(test_df.columns))\nprint(f\"The train dataset shape: {train_df.shape}\")\nprint(f\"The train dataset shape: {test_df.shape}\")\nprint(\"The only difference between test and train dasets should be the target.\")\nprint(f\"It is: {train_test_cols_diff}. The target column is: {TARGET_COL}.\")","507a21ee":"train_df.sample(2).T","11f5fd82":"msno.matrix(train_df)","3647ee3a":"# I fill missing values with -1 for now in order to get the profile report. This will change in the \n# upcoming sections.\npdp.ProfileReport(train_df.fillna(-1))","f0a30f31":"for col in ID_COLS:\n    print(train_df[col].sample(5))\n    print(24 * \"*\")","ce52bedf":"cols_diff = (train_df[\"sessionId\"] != \n             (train_df[\"fullVisitorId\"] + \"_\" + train_df[\"visitId\"].astype(str))).sum()\nprint(f\"The are {cols_diff} rows different between 'sessionId' and the concatenation of 'fullVisitorId' and 'visitId' columns\")\n","c0811546":"for col in ID_COLS:\n    print(f\"The {col} column has {train_df[col].nunique()} unique values\")","93eb9a4c":"#\u00a0The maximum number of visits for a user: \ntrain_df.groupby(\"fullVisitorId\")[\"visitId\"].count().max()","1095b466":"def get_cat_cols(df):\n    # I remove the ID_COLS\n    all_cat_cols = df.select_dtypes(include=[\"object\"]).columns\n    return list(set(all_cat_cols) - set(ID_COLS))","cf35b5b4":"CAT_COLS = get_cat_cols(train_df)","18dbc3a6":"print(f\"There are {len(CAT_COLS)} categorical columns. \\nThese are: {'|'.join(CAT_COLS)}\")","064c6ff2":"msno.matrix(train_df.loc[:, CAT_COLS])","7ca3d041":"#\u00a0TODO: What happens when a new category appears during test and wasn't \n#\u00a0available during train?\n\n# The training median number of uniques for categorical columns is 33.5\nCAT_THRESHOLD = 30\n\nclass CategoricalTransformer(object):\n    \n\n\n    def __init__(self, cat_threshold=CAT_THRESHOLD):\n        self.factors_mapping = None\n        self.cat_threshold = cat_threshold\n    \n    def fit_transform(self, s):\n        \"\"\" Fit and transform a categorical Series. \n        This will also transform the test categorical series.\n        \"\"\"\n        cat_total = s.nunique(dropna=False)\n        most_common = s.value_counts(dropna=False).index[0]\n        print(f\"Categorical transformation for: {s.name}\")\n        if cat_total > self.cat_threshold:\n            print(f\"Most common vs rest\")\n            # Most common category vs other categories\n            return pd.Series((s == most_common).astype(int), \n                             name=f\"is_{s.name}_{str(most_common).lower()}\")\n        elif cat_total > 2:\n            print(f\"Label encoding\")\n            #\u00a0Factorization + mapping for test set\n            if self.factors_mapping is None:\n                labels, uniques = pd.factorize(s)\n                self.factors_mapping = {v:k for k, v in enumerate(uniques)}\n                return pd.Series(labels, name=col)\n            else:\n                return s.replace(self.factors_mapping)\n        else:\n            # Binary\n            print(f\"One-hot encoding\")\n            return (s == most_common).astype(int)        \n    ","e9e0d414":"train_cat_data = []\ntest_cat_data = []\nfor col in CAT_COLS:\n    cat_tr = CategoricalTransformer()\n    train_cat_s = cat_tr.fit_transform(train_df[col])\n    train_cat_data.append(train_cat_s)\n    test_cat_s = cat_tr.fit_transform(test_df[col])\n    test_cat_data.append(test_cat_s)","868d42f9":"def list_of_s_to_df(data):\n    return pd.concat(data, axis=1, keys=[s.name for s in data])\n\ntrain_cat_df = list_of_s_to_df(train_cat_data)\ntest_cat_df = list_of_s_to_df(test_cat_data)","5b885410":"train_cat_df.sample(2).T","d52dc7bf":"test_cat_df.sample(2).T","2728de6f":"processed_train_df = pd.concat([train_df.drop(CAT_COLS + ID_COLS_TO_DROP, axis=1), \n                                train_cat_df], axis=1)\nprocessed_test_df = pd.concat([test_df.drop(CAT_COLS + ID_COLS_TO_DROP, axis=1), \n                               test_cat_df], axis=1)","d48f993a":"processed_train_df['bounces'].value_counts(dropna=False)","b9e5aeb0":"processed_train_df['newVisits'].value_counts(dropna=False)","102f6637":"processed_train_df['page'].value_counts(dropna=False)","57d505f1":"FILL_MISSINGS_COLS = [\"page\", \"bounces\", \"newVisits\"]\nprocessed_train_df[FILL_MISSINGS_COLS] = processed_train_df[FILL_MISSINGS_COLS].fillna(0.0)\nprocessed_test_df[FILL_MISSINGS_COLS] = processed_test_df[FILL_MISSINGS_COLS].fillna(0.0)","609b9a97":"processed_train_df.sample(2).T","a8bda371":"msno.matrix(processed_train_df)","355ea9e0":"pdp.ProfileReport(processed_train_df)","ffe319ad":"SEED = 314\nlgb_reg = lgb.LGBMRegressor(random_state=SEED)\nlgb_reg.fit(processed_train_df.drop([TARGET_COL, ID_COL], axis=1), \n            processed_train_df[TARGET_COL])","d6fb1cf7":"from lightgbm.plotting import plot_importance","eb0aa6c7":"plot_importance(lgb_reg)","72e8f81d":"plot_importance(lgb_reg, importance_type=\"gain\")","4a55b23d":"# As mentioned in the previous sections, some categories haven't been seen during\n# training. For now, I replace them with -1. \n# TODO: Improve this process.\nNOT_SEEN_OPERATING_SYSTEM_CAT = [\"Tizen\", \"Playstation Vita\", \"OS\/2\", \"SymbianOS\"]\nprocessed_test_df[\"operatingSystem\"] = processed_test_df[\"operatingSystem\"].replace(NOT_SEEN_OPERATING_SYSTEM_CAT, -1)\nNOT_SEEN_SLOT_CAT = [\"Google Display Network\"]\nprocessed_test_df[\"slot\"] = processed_test_df[\"slot\"].replace(NOT_SEEN_SLOT_CAT, -1)\nNOT_SEEN_AD_CAT = [\"Content\"]\nprocessed_test_df[\"adNetworkType\"] = processed_test_df[\"adNetworkType\"].replace(NOT_SEEN_AD_CAT, -1)\nprocessed_test_df = processed_test_df.fillna(-1)","26a678e0":"test_predictions_a = lgb_reg.predict(processed_test_df.drop(ID_COL, axis=1))\nsubmission_df = pd.DataFrame({TARGET_COL: test_predictions_a, \n                              ID_COL: processed_test_df[ID_COL]})","2e2db986":"neg_predictions_total  = (test_predictions_a < 0).sum()\nprint(f\"There are {neg_predictions_total} predicted values < 0.\")","8a9c0ce5":"submission_df.loc[lambda df: df[TARGET_COL] < 0, TARGET_COL] = 0.0","6ced552b":"TRANSFORMED_TARGET_COL = \"PredictedLogRevenue\"\nprocessed_submission_df = (submission_df.groupby(ID_COL)\n                                        .agg(lambda g: np.log1p(g.sum()))\n                                        .reset_index()\n                                        .rename(columns={TARGET_COL: TRANSFORMED_TARGET_COL}))","98b58143":"processed_submission_df.sample(5)","6a5b819c":"# A sanity check for the number of ID_COL unique values and number of rows \n# in the final test DataFrame.\nassert processed_submission_df.shape[0] == processed_test_df[ID_COL].nunique()","2396dd2d":"processed_submission_df.to_csv(OUTPUT_DATA_PATH, index=False)","9c26c7b9":"In the following notebook, I quickly explore the dataset, transform the categorical\nfeatures, train a (simple) gradient boosting tree model, and finally submit the result \n(after some post-processing). \n\nNotice that the input datasets are the ones produced by the following script: \nhttps:\/\/www.kaggle.com\/yassinealouini\/start-here-features-engineering-pipeline.\n\nEnjoy!","643c8810":"It seems that the `sessionId` is a concatenation of `fullVisitorId` and `visitId`.\nLet's check this claim.","abd2c36e":"Few things to notice: \n   \n- Some columns contain a lot of missing values (replaced with the value -1 for now)\n- A lot of categorical columns (24 out of 38 columns)\n- Among these categoricals, a lot have high cardinality => it would be a mess dummifying them. So we are left with two practical solutions: either factorize (i.e. give a numerical label to each one) or transform into a binary variable based on the most common one. More about this in what follows. \n\nBefore processing the categorical columns, I will first deal with the various ID columns. ","9ba5f29e":"#\u00a0Next steps","18fefd70":"Finally, based on the the below values, I will fill `page`, `bounces`, and `newVisits` missing values with 0 (the action\nis absent). Notice that this is an approximation: this misses all the times when there is a \"true\" missing value (the tracking\ndidn't work for example).","f5fe5db0":"# Data loading and basic EDA","10efbc03":"In what follows, only the `fullVisitorId` column will be used.","1a2253f0":"That's it for today. \n\nSome other things I can work on\/improve:\n\n* Hyperparamter optimisation.\n* Adverserial validation (to mitigate the difference between CV and LB scores). Here is a great [kernel](https:\/\/www.kaggle.com\/konradb\/adversarial-validation-and-other-scary-terms) to get you\nstarted. \n* More features engineering.\n* Stacking.\n\nI hope you have enjoyed this entry. Good luck to all!","a2583f06":"All right. That's enough processing for today. Let's build a (simple) gradient boosting model.\nFor that, I will be using [LightGBM](https:\/\/github.com\/Microsoft\/LightGBM). ","4885d193":"First thing, let's get the categorical columns. ","dd435653":"No cross-validation for now or hyperparamters optimization. Just training.","e1c9ead4":"As you can see, some predicted values are < 0. I will clip these to 0 instead.","68a526f3":"#\u00a0Categorical processing","6e3f4e7e":"# Model training","aaee0f71":"Finally, I group the predicted values using the ID_COL column, sum the grouped values, and\ntransform using the transformation $f:x \\to\\ \\ln(x+1)$ (to avoid having $\\inf$ values). ","a62c9408":"Let's explore the features' importance.","5e2f47e7":"#\u00a0ID olumns EDA","8e71bfb2":"#\u00a0Submission","d3e647c4":"In this section, I will predict using the trained model and do some post-processing as required in the\ncompetition's [evaluation](https:\/\/www.kaggle.com\/c\/ga-customer-revenue-prediction#evaluation) section. ","135dc5fc":"All right, let's start by processing the various categorical columns. As mentioned above, there are two strategies at play here:\n\n- if the cardinality of the column is high (let's say more than 10), then the feature is binned by taking the most common value as the postive category (encoded with 1) and the rest as the negative one (encoded with 0) .\n- if the cadinality is medium, the features are labeled with numerical values. Notice that the mapping cat <-> label will be saved for the test processing. \n- if the cardinality is low, the features are one-hot encoded. \n\nLet's get started!"}}