{"cell_type":{"e1886acf":"code","adf4b688":"code","4936914b":"code","890ea91a":"code","f4e07134":"code","ab0ef071":"code","f7cf3031":"code","4a619a8d":"code","c5bad8f0":"code","44313c12":"code","9496385c":"code","5e8579ff":"code","e0476c74":"code","96df92c5":"code","e6660138":"code","985f714d":"code","1bdfac64":"code","d40b8e83":"code","175c931e":"code","2fa18d6a":"code","64c44069":"code","a0b30630":"code","894dcbd5":"code","91d6702c":"code","0930ceb1":"code","26a1817d":"code","ce230802":"code","2badf5ac":"code","1c7c87a9":"code","3b57456f":"code","12589d29":"code","ff7b1ae1":"code","dae8152d":"code","1a19290f":"code","28e50ffb":"code","88061e4b":"code","bb99df78":"code","e8685e85":"code","29562f4a":"code","e6522661":"code","0af73e31":"code","b3c48097":"code","5501956c":"code","46f1be2f":"code","cc1b37e9":"code","ddf98ce9":"code","3b883081":"code","c53de3e0":"code","22e190c7":"code","1d17dbe6":"code","7bd04f83":"code","11a7fc19":"code","c7b22302":"code","163d9485":"code","2b89ece5":"code","a6afaa29":"code","5a512627":"code","35179886":"code","5ac839f8":"code","d5030bd1":"code","eb07539d":"code","e9b489ed":"code","a723c467":"code","f537e4b8":"code","e8883d33":"code","4bb8f52f":"code","5f6a070b":"code","7ce12310":"code","3e6ec6af":"code","21b614ea":"code","1e0d6d1c":"code","3ff946d3":"markdown","e331972e":"markdown","aed719c6":"markdown","558f89f8":"markdown","4a9d3aa9":"markdown","cdcc9a82":"markdown","77b99968":"markdown","6a334055":"markdown","2abe2fde":"markdown","2f31ba9f":"markdown","e3712e32":"markdown","2af6f3bd":"markdown","9c49930a":"markdown","4c644ea6":"markdown","003f18dd":"markdown","de82a9ba":"markdown","d97541b9":"markdown"},"source":{"e1886acf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nfrom collections import Counter\nimport datetime\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","adf4b688":"train = pd.read_csv('\/kaggle\/input\/demand-forecasting\/train_0irEZ2H.csv') \ntest = pd.read_csv('\/kaggle\/input\/demand-forecasting\/test_nfaJ3J5.csv')\nsample = pd.read_csv('\/kaggle\/input\/demand-forecasting\/sample_submission_pzljTaX.csv')","4936914b":"train['week'] = pd.to_datetime(train['week'])\ntest['week'] = pd.to_datetime(test['week'])","890ea91a":"train.isna().sum()","f4e07134":"test.isna().sum()","ab0ef071":"train.apply(lambda x: len(x.unique()))","f7cf3031":"test.apply(lambda x: len(x.unique()))","4a619a8d":"print(train['is_featured_sku'].value_counts()\/train['is_featured_sku'].shape[0])\n\nprint(test['is_featured_sku'].value_counts()\/test['is_featured_sku'].shape[0])","c5bad8f0":"print(train['is_display_sku'].value_counts()\/train['is_display_sku'].shape[0])\n\nprint(test['is_display_sku'].value_counts()\/test['is_display_sku'].shape[0])","44313c12":"train.info()","9496385c":"test.info()","5e8579ff":"train.columns","e0476c74":"start = []\nend = []\nct = 0\nfor store in list(set(train['store_id'])):\n    temp_of_train = train[train['store_id'] == store]\n    temp_of_test = test[test['store_id'] == store]\n    for sku in list(set(temp_of_train['sku_id'])):\n        \n        train_df = temp_of_train[temp_of_train['sku_id'] == sku]\n        test_df = temp_of_test[temp_of_test['sku_id'] == sku]\n        \n        train_df.sort_values(\"week\")\n        test_df.sort_values(\"week\")\n        print(store,sku)\n        print(train_df['week'].iloc[-1])\n        start.append(train_df['week'].iloc[-1])\n        print(test_df['week'].iloc[0])\n        end.append(test_df['week'].iloc[0])\n        print(test_df)\n        \n        \n        ct+=1","96df92c5":"print(Counter(start))\nprint(Counter(end))","e6660138":"train.groupby('week').sum()['units_sold'].plot(figsize=(12,8))\nplt.xlabel('Week')\nplt.ylabel('units_sold')","985f714d":"train.groupby('week').sum()['total_price'].plot(figsize=(12,8))\nplt.xlabel('Week')\nplt.ylabel('total_price')","1bdfac64":"train.groupby('store_id').sum()['units_sold'].plot(figsize=(15,8),kind='bar')\nplt.xlabel('store_id')\nplt.ylabel('units_sold')","d40b8e83":"train.groupby(['store_id','sku_id']).mean()['units_sold'].sort_values().plot(figsize=(15,8),kind='bar')\n\nplt.xlabel('store_id')\nplt.ylabel('units_sold')","175c931e":"import seaborn as sns\nsns.distplot(train['total_price'])","2fa18d6a":"train.isna().sum()","64c44069":"train[train['total_price'].isna() == True]","a0b30630":"train[(train['store_id']==9436) & (train['sku_id']==245338) ].set_index('week')['total_price'].plot()","894dcbd5":"train[(train['store_id']==9436) & (train['sku_id']==245338) & (train['week'] >= datetime.datetime.strptime('2013-04-23', '%Y-%M-%d'))]","91d6702c":"train = train.fillna(475)","0930ceb1":"train.isna().sum()","26a1817d":"df = train.append(test)","ce230802":"df.shape","2badf5ac":"column=['sku_id','store_id']\nname='count_id_sku_store'\ntemp=df.groupby(column)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\ntemp\ndf = pd.merge(df,temp,how = 'left',on = column)\ndf[name] = df[name].astype(float)\ndf[name].fillna(np.median(temp[name]),inplace=True)","1c7c87a9":"column = ['sku_id']\nname='count_id_sku'\ntemp=df.groupby(column)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\ntemp\ndf = pd.merge(df,temp,how = 'left',on = column)\ndf[name] = df[name].astype(float)\ndf[name].fillna(np.median(temp[name]),inplace=True)","3b57456f":"column = ['store_id']\nname='count_id_store'\ntemp=df.groupby(column)['record_ID'].count().reset_index().rename(columns={'record_ID':name})\ntemp\ndf = pd.merge(df,temp,how = 'left',on = column)\ndf[name] = df[name].astype(float)\ndf[name].fillna(np.median(temp[name]),inplace=True)","12589d29":"df['price_diff_percent'] = (df['base_price'] - df['total_price']) \/ df['base_price']","ff7b1ae1":"df['Month'] = pd.to_datetime(df['week']).dt.month\n\ndf['Day'] = pd.to_datetime(df['week']).dt.day\n\ndf['Dayofweek'] = pd.to_datetime(df['week']).dt.dayofweek\n\ndf['DayOfyear'] = pd.to_datetime(df['week']).dt.dayofyear\n\ndf['Week'] = pd.to_datetime(df['week']).dt.week\n\ndf['Quarter'] = pd.to_datetime(df['week']).dt.quarter \n\ndf['Is_month_start'] = pd.to_datetime(df['week']).dt.is_month_start\n\ndf['Is_month_end'] = pd.to_datetime(df['week']).dt.is_month_end\n\ndf['Is_quarter_start'] = pd.to_datetime(df['week']).dt.is_quarter_start\n\ndf['Is_quarter_end'] = pd.to_datetime(df['week']).dt.is_quarter_end\n\ndf['Is_year_start'] = pd.to_datetime(df['week']).dt.is_year_start\n\ndf['Is_year_end'] = pd.to_datetime(df['week']).dt.is_year_end\n\ndf['Is_weekday'] = np.where(df['week'].isin([0,1,2,3,4]),1,0)","dae8152d":"df","1a19290f":"from sklearn.preprocessing import LabelEncoder","28e50ffb":"df.dtypes","88061e4b":"df.select_dtypes(exclude=[\"int\",\"float\"]).columns","bb99df78":"col=['store_id','sku_id','Is_month_start','Is_month_end','Is_quarter_start','Is_quarter_end','Is_year_start','Is_year_end']","e8685e85":"for i in col:\n    df = pd.get_dummies(df, columns=[i])","29562f4a":"df","e6522661":"df.drop(['record_ID','week'],inplace=True,axis=1)","0af73e31":"df.head()","b3c48097":"import seaborn as sns\nsns.distplot(df['total_price'])","5501956c":"df['total_price']=np.log1p(df['total_price'])\ndf['base_price']=np.log1p(df['base_price'])\ndf['units_sold'] = np.log1p(df['units_sold'])","46f1be2f":"sns.distplot(df['total_price'])","cc1b37e9":"tests = df[df['units_sold'].isna() == True]\ntrains = df[df['units_sold'].isna() == False]","ddf98ce9":"print(trains.shape,tests.shape)","3b883081":"del tests['units_sold']","c53de3e0":"trains.corr()","22e190c7":"X = trains.drop('units_sold',axis = 1)\nY = train['units_sold']","1d17dbe6":"# Perform cross-validation\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold","7bd04f83":"x_train, x_valid, y_train, y_valid = train_test_split(X, Y, test_size = 0.2,random_state=23)","11a7fc19":"from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier","c7b22302":"rf_base = RandomForestRegressor()\nrf_base.fit(x_train,y_train)\n\n\nrf_tuned = RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=30,\n                      max_features='sqrt', max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=1, min_samples_split=10,\n                      min_weight_fraction_leaf=0.0, n_estimators=600,\n                      n_jobs=None, oob_score=True, random_state=None,\n                      verbose=0, warm_start=False)\nrf_tuned.fit(x_train,y_train)","163d9485":"import lightgbm as lgb","2b89ece5":"model_lgb_base=lgb.LGBMRegressor(objective='regression')\nmodel_lgb_base.fit(x_train,y_train)\n\nmodel_lgb_tuned=lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.1, max_depth=30,\n              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n              min_split_gain=0.0001, n_estimators=200, n_jobs=-1,\n              num_leaves=1200, objective=None, random_state=None, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)\n\nmodel_lgb_tuned.fit(x_train,y_train)","a6afaa29":"def rmlse(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true)-np.log1p(y_pred), 2)))","5a512627":"prediction_rfb_valid=rf_base.predict(x_valid)\nprediction_rft_valid=rf_tuned.predict(x_valid)\nprediction_lgbmb_valid=model_lgb_base.predict(x_valid)\nprediction_lgbmt_valid=model_lgb_tuned.predict(x_valid)\n\nrf_base_msle=100*rmlse(y_valid,prediction_rfb_valid)\nrf_tuned_msle=100*rmlse(y_valid,prediction_rft_valid)\nlgbm_base_msle=100*rmlse(y_valid,prediction_lgbmb_valid)\nlgbm_tuned_msle=100*rmlse(y_valid,prediction_lgbmt_valid)\n\nprediction_ensemble_base=(((1-rf_base_msle)*prediction_rfb_valid)+((1-lgbm_base_msle)*prediction_lgbmb_valid))\/(2-rf_base_msle-lgbm_base_msle)\nprediction_ensemble_tuned=(((1-rf_tuned_msle)*prediction_rft_valid)+((1-lgbm_tuned_msle)*prediction_lgbmt_valid))\/(2-rf_tuned_msle-lgbm_tuned_msle)\n\nensemble_base_msle=100*rmlse(y_valid,prediction_ensemble_base)\nensemble_tuned_msle=100*rmlse(y_valid,prediction_ensemble_tuned)\n\n\nprint(\"RF Base: {}; RF Tuned: {}\".format(rf_base_msle,rf_tuned_msle))\nprint(\"LGBM Base: {}; LGBM Tuned: {}\".format(lgbm_base_msle,lgbm_tuned_msle))\nprint(\"Ensemble Base: {}; Ensemble Tuned: {}\".format(ensemble_base_msle,ensemble_tuned_msle))","35179886":"model = lgb.LGBMRegressor(bagging_fraction=0.8, bagging_frequency=4, boosting_type='gbdt',\n              class_weight=None, colsample_bytree=1.0, feature_fraction=0.5,\n              importance_type='split', learning_rate=0.1, max_depth=30,\n              min_child_samples=20, min_child_weight=30, min_data_in_leaf=70,\n              min_split_gain=0.0001, n_estimators=100, n_jobs=-1,\n              num_leaves=1400, objective=None, random_state=None, reg_alpha=0.0,\n              reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000, subsample_freq=0)\n\nmodel.fit(X,Y)","5ac839f8":"prediction=model.predict(tests)","d5030bd1":"final_prediction=np.round(np.expm1(prediction))\nsample['units_sold']=final_prediction\n","eb07539d":"sample['units_sold'] = sample['units_sold'].abs()","e9b489ed":"sample.to_csv('submissions.csv',index=False,encoding='utf-8')","a723c467":"from sklearn.ensemble import RandomForestRegressor\nreg = RandomForestRegressor()\nreg.fit(x_train, y_train)","f537e4b8":"def RMSLE(actual, predicted):\n\n    predicted = np.array([np.log(np.abs(x+1.0)) for x in predicted])  # doing np.abs for handling neg values  \n    actual = np.array([np.log(np.abs(x+1.0)) for x in actual])\n    log_err = actual-predicted\n    \n    return 1000*np.sqrt(np.mean(log_err**2))","e8883d33":"preds = reg.predict(tests)\n#print(f'The validation RMSLE error for baseline model is: {RMSLE(np.exp(x_valid), np.exp(preds))}')","4bb8f52f":"print(preds)","5f6a070b":"final_prediction_rf=np.round((preds))\nsample['units_sold']=final_prediction_rf\nsample['units_sold']\nsample['units_sold'] = sample['units_sold'].abs()\nsample.to_csv('submissions_base.csv',index=False,encoding='utf-8')","7ce12310":"Xtrain, Xval, ytrain, yval = train_test_split(X, Y, test_size = 0.2, random_state = 1)\nprint(Xtrain.shape, ytrain.shape, Xval.shape, yval.shape)","3e6ec6af":"def runLGB(Xtrain, ytrain, Xval, yval, cat_cols, Xtest = None):\n    params = {\n    'boosting_type': 'gbdt',\n    'objective': 'regression',\n    'metric': 'l1',\n    #'max_depth': 9, \n    'learning_rate': 0.1\n    ,'verbose': 1\n    , \"min_data_in_leaf\" : 10\n    }\n\n    n_estimators = 800\n    early_stopping_rounds = 10\n\n    d_train = lgb.Dataset(Xtrain.copy(), label=ytrain.copy(), categorical_feature=cat_cols)\n    d_valid = lgb.Dataset(Xval.copy(), label=yval.copy(), categorical_feature=cat_cols)\n    watchlist = [d_train, d_valid]\n    model = lgb.train(params, d_train, n_estimators\n                      , valid_sets = [d_train, d_valid]\n                      , verbose_eval=n_estimators\n                      , early_stopping_rounds=early_stopping_rounds)\n\n    preds = model.predict(Xval, num_iteration=model.best_iteration)\n    err = RMSLE(yval, np.exp(preds))\n    \n    preds_test = model.predict(Xtest, num_iteration=model.best_iteration)\n    return  preds, err, np.exp(preds_test), model","21b614ea":"pred_val, err, pred_test,model = runLGB(Xtrain, ytrain, Xval, yval, list(X.select_dtypes(exclude=[\"int\",\"float\"]).columns), tests)","1e0d6d1c":"\nfinal_prediction=np.round(np.expm1(pred_test))\nsample['units_sold'] = final_prediction\nsample.to_csv('lgb_v1.csv', index = False)","3ff946d3":"# Handling missing values","e331972e":"Conclusion - Balanced data set","aed719c6":"# Data Visualization","558f89f8":"Week","4a9d3aa9":"Interpolating the value","cdcc9a82":"Label encoding","77b99968":"# Exploratory data analysis","6a334055":"# sku - stock-keeping unit","2abe2fde":"# Data preprocessing ","2f31ba9f":"Rank - 75","e3712e32":"One missing value","2af6f3bd":"Same store and same sku_id in both test and train","9c49930a":"is_featured_sku and is_display_sku -> Units  = 1","4c644ea6":"For each store and sku pair, the start and end is same\n\nend of train - 2013-09-07\n\n\nstart of test - 2013-07-23","003f18dd":"![image.png](attachment:image.png)","de82a9ba":"Change week to date time","d97541b9":"Model - Another approach"}}