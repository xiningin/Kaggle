{"cell_type":{"eed0c6e0":"code","43d86f32":"code","39d46277":"code","a7dfe972":"code","a2f4fd91":"code","b65ced6e":"code","dd64dba0":"code","c221b8c8":"code","c128ed45":"code","40858b16":"code","a2dfde95":"code","bbc2161b":"code","e7230da6":"code","ccedfe38":"code","096927cb":"code","0f1907bc":"code","e69be9e5":"code","c22f8a0c":"markdown","3de42da7":"markdown","c1f33a49":"markdown","cc785a77":"markdown","30260c82":"markdown","e478c12c":"markdown","a5d1710c":"markdown","9910258a":"markdown","9a2b1054":"markdown","d4afd0bd":"markdown","349eb8a9":"markdown","a8aa3a59":"markdown","ca13eb4d":"markdown","e5eb7dd7":"markdown","c9275de2":"markdown","6fb899d2":"markdown"},"source":{"eed0c6e0":"# import required libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib.pyplot as plt\nfrom textblob.classifiers import NaiveBayesClassifier\nfrom sklearn.metrics import confusion_matrix\nimport re\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split","43d86f32":"# load the dataset and take a look at the first few records\ndf = pd.read_csv('..\/input\/London_hotel_reviews.csv', encoding = \"ISO-8859-1\")\nprint(df.shape)\ndf.head()","39d46277":"print(df.isna().sum(), end = '\\n\\n')\ndf[df.isnull().any(axis=1)].head()","a7dfe972":"df[df[\"Date Of Review\"].isnull()]","a2f4fd91":"print(len(df[df['Review Title'].str.contains(\"<U\")]), 'reviews that are probably gibberish.')\ndf[df['Review Title'].str.contains(\"<U\")].head()","b65ced6e":"df = df[df['Review Title'].str.contains(\"<U\") == False]","dd64dba0":"sns.set(rc={'figure.figsize':(15,10)}) # this will set the size of all the following graphs (default is too small)\ngrid = sb.countplot(x = 'Property Name', data = df, order = df['Property Name'].value_counts().index)\ngrid.set_title('Number Of Reviews Per Hotel')\ngrid.set_xticklabels(grid.get_xticklabels(), rotation=90)","c221b8c8":"grid = sb.countplot(x = 'Review Rating', data = df, order = df['Review Rating'].value_counts().index)\ngrid.set_title('Number Of Ratings')","c128ed45":"# get average review rating for each hotel\naverage_rating_df = pd.DataFrame(columns = ['Property Name', 'Average Rating'])\ncount = 0\nfor i in df['Property Name'].unique():\n    average_rating = sum(df['Review Rating'][df['Property Name'] == i]) \/ sum(df['Property Name'] == i)\n    average_rating_df.loc[count] = [i, average_rating]\n    count += 1 \naverage_rating_df = average_rating_df.sort_values('Average Rating', ascending = False)\naverage_rating_df.plot(kind = 'bar', x = 'Property Name')","40858b16":"df = df.fillna('NA') # 3,953 NA's in the \"Location Of The Reviewer\" column, so replace these with the string 'NA' so the following loop doesn't run into an error\nspecific_locations = []\nfor i in df['Location Of The Reviewer']:\n    if \", \" not in i:\n        specific_locations.append(i)\n    else:\n        specific_locations.append(i.rsplit(\", \", 1)[1])\ndf['Specific Location'] = specific_locations\n\ngrid = sb.countplot(x = 'Specific Location', data = df, order = df['Specific Location'].value_counts().iloc[:20].index)\ngrid.set_title('Locations Of Reviewers')\ngrid.set_xticklabels(grid.get_xticklabels(), rotation=90)","a2dfde95":"years = []\nfor i in df['Date Of Review']:\n    years.append(i[-4:])\ndf['Year'] = years\ngrid = sb.countplot(x = 'Year', data = df)\ngrid.set_title('Review Count Per Year')\ngrid.set_xticklabels(grid.get_xticklabels(), rotation=90)","bbc2161b":"df['Complete Review'] = df['Review Title'] + ' ' + df['Review Text']\ndf.loc[df['Review Rating'] > 4, 'Good Review'] = 1\ndf.loc[df['Review Rating'] <= 4, 'Good Review'] = 0\nprint(sum(df['Good Review'] == 0) \/ len(df['Good Review']) * 100, 'percent of reviews are bad (less than 5 star).')","e7230da6":"msk = np.random.rand(len(df)) < 0.8\ntrain = df[msk]\ntest = df[~msk]","ccedfe38":"REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\|)|(\\()|(\\))|(\\[)|(\\])\")\nREPLACE_WITH_SPACE = re.compile(\"(<br\\s*\/><br\\s*\/>)|(\\-)|(\\\/)\")\n\ndef preprocess_reviews(reviews):\n    reviews = [REPLACE_NO_SPACE.sub(\"\", line.lower()) for line in reviews]\n    reviews = [REPLACE_WITH_SPACE.sub(\" \", line) for line in reviews]\n    \n    return reviews\n\nreviews_train_clean = preprocess_reviews(train['Complete Review'])\nreviews_test_clean = preprocess_reviews(test['Complete Review'])","096927cb":"cv = CountVectorizer(binary=True)\ncv.fit(reviews_train_clean)\nX_train = cv.transform(reviews_train_clean)\nX_test = cv.transform(reviews_test_clean)","0f1907bc":"target_train = train['Good Review']\ntarget_test = test['Good Review']\n\nmodel = LogisticRegression()\nmodel.fit(X_train, target_train)\nprint (\"Accuracy: %s\" % accuracy_score(target_test, model.predict(X_test)))","e69be9e5":"feature_to_coef = {\n    word: coef for word, coef in zip(\n        cv.get_feature_names(), model.coef_[0]\n    )\n}\nprint('Good words:', end = \"\\n\\n\")\nfor best_positive in sorted(\n    feature_to_coef.items(), \n    key=lambda x: x[1], \n    reverse=True)[:5]:\n    print (best_positive)\nprint('')\nprint('Bad words:', end = \"\\n\\n\")\nfor best_negative in sorted(\n    feature_to_coef.items(), \n    key=lambda x: x[1])[:5]:\n    print (best_negative)","c22f8a0c":"Now a few graphs to visualize some of the data.","3de42da7":"### Sentiment Analysis\n\nNow that we have a good handle on the data, we can try to make a simple model that can take the text of a review and predict from that what rating was given. To keep it simple for the model I'll divide reviews into good and bad, with \"good\" being a 5 star review and bad being anything below. ","c1f33a49":"Before feeding reviews into a model, they should be cleaned to remove punctuation, as that willl be useless from the computers point of view in trying to work out the sentiment of a text - for this case we're only interested in the words in a review. The code below (from https:\/\/towardsdatascience.com\/sentiment-analysis-with-python-part-1-5ce197074184) gets rid of punctuation so a review is turned into words only.","cc785a77":"What about the average rating for each hotel? To do this I'll make a new dataframe with two columns (\"Property Name\" and \"Average Rating\"), calculate the average rating of each hotel, and then add each hotel name and its average rating to the dataframe. A dataframe can be displayed as a graph very easily.","30260c82":"So the majority of hotels here are very good, with a handful of very bad ones. The next graph I'll make is a count of the locations of reviewers. Looking at the \"Location Of The Reviewer\" column, many entries are of the pattern (city), (country) eg. \"London, United Kingdom\" (though for US locations it'll be (city), (state) like \"Broomfield, Colorado\"). To return countries I'll make a new column that is everything after the last comma (,) if there is a comma in the location of the reviewer. Then I'll display the most common 20 locations.","e478c12c":"The \"Location Of The Reviewer\" column has a large number of NaN values, with the \"Date Of Review\" column containing just one. We can take a look at that.","a5d1710c":"With over 80% accuracy, this is a reasonable model in that it clearly beats the naive baseline that merely guesses the most likely rating (5 stars - good) for each review, though it could be improved as I haven't done any tweaking.\n\nFinally, it's interesting to see what words the model considers good and bad, ","9910258a":"##### Thank you for opening this Kernel! \n\n### I'll be doing some simple data exploration and then make a simple sentiment analysis model that can be used to predict reviews.","9a2b1054":"The next step is *one hot encoding*, where we turn each review into a very large matrix of 0's and 1's. A 0 would represent a certain word isn't included, whereas a 1 means that word is included. A short review of a few words would have a matrix of almost entirely 0's (ie. the vast majority of unique words across all reviews aren't present in the review), with just a small number of 1's. This is necessary for the logistic regression algorithm used below.","d4afd0bd":"This is a pretty straightforward dataset of 27,330 rows, each representing an individual review of some hotel. First I'll check if there are any NaN values and display some of them. ","349eb8a9":"Next to make a train and test set, with 80% of the data being the train set and 20% being the test set. The model will be trained on the training data and evaluated on the test data.","a8aa3a59":"Now we can finally train a model on the train data with model.fit(). After that, model.predict() can be called on the test data to evaluate how good the model does. We know around 67% of reviews are 5 stars, so as a naive benchmark, we'd expect the model to predict the right rating based on the review text over 67% of the time.","ca13eb4d":"The final graph will be looking at the number of reviews given for each year, which will be the last 4 characters of the \"Date Of Review\" column. ","e5eb7dd7":"Something wrong there - probably the review was written in a different language and the characters couldn't be displayed. I'll check if there are others like it.","c9275de2":"Some of the words the model considers good and bad are a bit strange, so it's clear there's room for improvement :)\n\nComments\/feedback welcome.","6fb899d2":"These can be removed."}}