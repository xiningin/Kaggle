{"cell_type":{"e11f84a4":"code","c8f5f5c6":"code","57fb1a58":"code","f71d5f55":"code","07d97866":"code","f51ec944":"code","2e8b7c58":"code","5ba1592c":"code","1f4bbb92":"code","cfd412da":"code","f2c6b258":"code","d1231d88":"code","c5dfc570":"code","2c63d5fc":"code","0032ef3d":"code","539a19dc":"code","148b9917":"code","f2fce4a3":"code","d3849e2f":"code","af636773":"code","881495d6":"code","1f5c6517":"code","be4b6842":"code","a679bc0e":"code","943d7b62":"code","e21506f2":"code","2d3a4016":"code","bf1698d4":"code","e74a7501":"code","1f139898":"code","72c6325a":"code","58c88abe":"code","618f885d":"code","66d39de3":"code","d9f883ac":"code","5577571b":"code","421535fa":"code","2232c48f":"code","5fa8a66d":"code","2c83e645":"code","11c4b36d":"code","202e2ac3":"code","f7eeb595":"code","7fcfcd65":"code","60bc3d0b":"code","eb3f05c6":"code","c8fb2402":"code","e6ec77ea":"code","e09f5017":"code","1e0385a4":"code","7155d272":"code","d4d1f053":"code","a4b24d60":"code","e91e7e6f":"code","157847a4":"code","ba4d4c68":"code","16457181":"code","eed1ab10":"code","dd1baea8":"code","758d971d":"code","a6ca00c4":"code","6f260d2c":"code","791de7d0":"code","0acd7d52":"code","a9fc0c34":"code","6f657a1a":"code","a43f71da":"code","ccd3e2a7":"code","4394abe8":"code","b10ea42c":"code","d8ba1fbb":"code","8c35fc1a":"code","dd3a80ad":"code","8aa5e459":"code","1426e35f":"code","bafbf591":"code","c3efbc23":"code","1d6ca5ab":"code","37c85c4f":"code","ad4848e8":"code","9c89aad9":"code","70144b59":"code","bd94d5e0":"code","f6ecb39c":"code","aa9cb8bc":"code","7fe07bb3":"code","831d59af":"code","ba14873a":"code","c30dacd3":"code","af3b5033":"code","4631fc73":"code","4bfa4304":"code","95ca6d97":"code","3ddfbb89":"code","047c04aa":"markdown","2842a763":"markdown","4365595c":"markdown","d74bab80":"markdown","456a1d8e":"markdown","2ff62d27":"markdown","99227128":"markdown","a52ba062":"markdown","272a8330":"markdown","73f5046a":"markdown","429655df":"markdown","ff9f4d76":"markdown","ade1a6f9":"markdown","040be59c":"markdown","39967ea9":"markdown","fb504b96":"markdown","0bdd042d":"markdown","ff3cdfee":"markdown","803b00ed":"markdown","12ddeba6":"markdown","45b708bf":"markdown","20d98e80":"markdown","433af480":"markdown","d67b0f99":"markdown","906382a1":"markdown","5503a886":"markdown","7a72ae71":"markdown","72aa1a1f":"markdown","60c72148":"markdown"},"source":{"e11f84a4":"# import libraries here; add more as necessary\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os \nimport time\n# magic word for producing visualizations in notebook\n%matplotlib inline\n\n#scikit-learn\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, MinMaxScaler, StandardScaler, Normalizer\nfrom sklearn.metrics import accuracy_score, roc_auc_score, classification_report\nfrom sklearn.model_selection import train_test_split, GridSearchCV\n\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nimport xgboost as xgb","c8f5f5c6":"train = pd.read_csv('..\/input\/titanic\/train.csv', sep=',')","57fb1a58":"train.head()","f71d5f55":"train.shape","07d97866":"stats_by_class = train.groupby('Pclass')['Survived'].value_counts()\nstats_by_class","f51ec944":"stats_by_class.unstack(level=1).plot(kind='bar', subplots=False)","2e8b7c58":"stats_by_sex = train.groupby('Pclass')['Sex'].value_counts()\nstats_by_sex","5ba1592c":"stats_by_sex.unstack(level=0).plot(kind='bar', subplots=False)","1f4bbb92":"survived_by_sex = train.groupby('Sex')['Survived'].value_counts()\nsurvived_by_sex","cfd412da":"survived_by_sex.unstack(level=1).plot(kind='bar', subplots=False)","f2c6b258":"survived_by_embarked = train.groupby('Embarked')['Survived'].mean()\nsurvived_by_embarked","d1231d88":"survived_by_embarked.plot(kind='bar', subplots=False)","c5dfc570":"survived_by_SibSp = train.groupby('SibSp')['Survived'].mean()\nsurvived_by_SibSp","2c63d5fc":"survived_by_SibSp.plot(kind='bar', subplots=False)","0032ef3d":"survived_by_Parch = train.groupby('Parch')['Survived'].mean()\nsurvived_by_Parch","539a19dc":"survived_by_Parch.plot(kind='bar', subplots=False)","148b9917":"#derive new feature \"family size\" by adding number of parents and siblings\ndef count_family_members(df):\n    df['Fml_size'] = df['Parch'] + df['SibSp']","f2fce4a3":"count_family_members(train)\ntrain.head()","d3849e2f":"survived_by_Fml_size = train.groupby('Fml_size')['Survived'].mean()\nsurvived_by_Fml_size","af636773":"survived_by_Fml_size.plot(kind='bar', subplots=False)","881495d6":"missing_df = pd.Series(train.isna().sum(), name='number').to_frame()\nmissing_df = missing_df.assign(percentage=lambda x: round((x.number\/train.shape[0]), 2))\nmissing_df","1f5c6517":"def extract_cabin_type(df):\n    df['Cabin'] = df['Cabin'].str.extract('([A-Za-z]+)')","be4b6842":"extract_cabin_type(train)\ntrain['Cabin'].unique()","a679bc0e":"def extract_titel(df):\n    df['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.')","943d7b62":"extract_titel(train)\ntrain.head()","e21506f2":"train.groupby(['Title'])['Survived'].value_counts()","2d3a4016":"def replace_title(df):\n    df['Title'] = df['Title'].replace('Mlle', 'Miss')\n    df['Title'] = df['Title'].replace('Ms', 'Miss')\n    df['Title'] = df['Title'].replace('Mme', 'Mrs')\n    df['Title'] = df['Title'].replace(['Capt', 'Col', 'Major'], 'Military')\n    df['Title'] = df['Title'].replace(['Countess','Don', 'Jonkheer', 'Dona'], 'Nobility')\n    df['Title'] = df['Title'].replace(['Lady', 'Sir','Rev', 'Dr'], 'Honorific')","bf1698d4":"replace_title(train)\ntrain.groupby(['Title'])['Survived'].value_counts()","e74a7501":"columns = ['Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nfor column in columns:\n    train.plot(x=column, y='PassengerId', kind='scatter')\n    #sns.boxplot(x=train[column])","1f139898":"train.loc[train['Age'] > 70]","72c6325a":"train.loc[train['Fare'] > 300]","58c88abe":"train.loc[train['PassengerId'] == 631, 'Age'] = 48\ntrain.loc[train['PassengerId'] == 117, 'Age'] = 66\n\ntrain.loc[train['PassengerId'] == 69, ['SibSp', 'Parch']] = [0,0]","618f885d":"train.loc[train['Fare'] == 0]","66d39de3":"def complimentary_ticket(df):\n    df['CTicket'] = 0\n    df.loc[df['Fare'] == 0.0, 'CTicket'] = 1","d9f883ac":"complimentary_ticket(train)\ntrain.loc[train['Fare'] == 0]","5577571b":"def fill_nan(df):\n    df['Age'] = df.groupby(['Pclass', 'Sex', 'Title'])['Age'].transform(lambda x: x.fillna(x.median()))\n    df['Embarked'].fillna(df['Embarked'].value_counts().idxmax(), inplace=True)\n    df['Fare'] = df.groupby(['Pclass', 'Title'])['Fare'].transform(lambda x: x.fillna(x.median()))\n    df['Cabin'].fillna('U', inplace=True)","421535fa":"fill_nan(train)\ntrain.head()","2232c48f":"def age_bin(df):\n    df['Age'] = df['Age'].astype(int)\n    \n    df['Age_bin'] = pd.cut(df['Age'], bins=5, labels=[\"Kid\", \"Young\", \"Adult\", 'Middle-Aged', \"Old\"])","5fa8a66d":"age_bin(train)\ntrain","2c83e645":"survived_by_Age = train.groupby(['Age_bin', 'Sex'])['Survived'].mean()\nsurvived_by_Age","11c4b36d":"survived_by_Age.unstack(level=1).plot(kind='bar', subplots=False)","202e2ac3":"def travel_alone(df):\n    df['IsAlone'] = 1\n    df['IsAlone'].loc[df['Fml_size'] > 0] = 0","f7eeb595":"travel_alone(train)\ntrain.head()","7fcfcd65":"#encode string values into number categories\ndef label_encoding(df):\n    labelencoder = LabelEncoder()\n    columns = ['Sex', 'Embarked', 'Cabin', 'Age_bin', 'Title']\n    for column in columns:\n        df[column] = labelencoder.fit_transform(df[column])\n        # get frequency of each category\n        encoding = df.groupby([column]).size()\n        encoding = encoding\/len(df)\n        new_col = column+'_freq'\n        df[new_col] = df[column].map(encoding)","60bc3d0b":"label_encoding(train)\ntrain.head()","eb3f05c6":"def create_new_features(df):\n    df['Sex_Class'] = df['Sex']*df['Pclass']\n    df['Sex_Fare']  = df['Sex']*df['Fare']\n    df['Sex_Age']   = df['Sex']*df['Age_bin']\n    \n    df['Class_Age'] = df['Age_bin']*df['Pclass']\n    df['Age_Fare']  = df['Age_bin']*df['Fare']\n    \n    df['Small_Fml'] = df['Fml_size'].map(lambda s: 1 if  s <= 2  else 0)\n    df['Avg_Fml']   = df['Fml_size'].map(lambda s: 1 if 3 <= s <= 4 else 0)\n    df['Big_Fml']   = df['Fml_size'].map(lambda s: 1 if s >= 5 else 0)","c8fb2402":"create_new_features(train)\ntrain.head()","e6ec77ea":"def corr_to_drop(df):\n    \n    corr_matrix = df.corr().abs().round(2)\n    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n    # Find features with correlation greater than 0.90\n    cols_to_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\n    \n    return cols_to_drop","e09f5017":"cols_to_drop = corr_to_drop(train)\nprint(cols_to_drop)","1e0385a4":"def drop_columns(df):\n    cols_to_drop = ['Name', 'Age', 'Ticket', 'Fml_size', 'SibSp', 'Parch', 'Age_Fare']\n    return df.drop(cols_to_drop, axis=1)","7155d272":"train = drop_columns(train)\ntrain.head()","d4d1f053":"plt.figure(figsize=(12,10))\ncor = train.corr()\nsns.heatmap(cor)","a4b24d60":"passengerId = pd.DataFrame(train['PassengerId'].values,columns=['PassengerId'])\ntrain_Y=pd.DataFrame(train['Survived'].values,columns=['Survived'])\ntrain_X=train.drop(['PassengerId','Survived'],axis=1)\ntrain_X.shape","e91e7e6f":"def perform_log_transform(df):\n    df_log = pd.DataFrame()\n\n    for col in df.columns:\n        df_log[col]=(df[col]-df[col].min()+1).transform(np.log)\n    return df_log","157847a4":"train_X = perform_log_transform(train_X)","ba4d4c68":"def scale_dataset(df):\n    scaler = StandardScaler()\n    scale=scaler.fit(df)\n    return scaler.transform(df)","16457181":"X_scaled = scale_dataset(train_X)","eed1ab10":"def sklearn_pca(data, n_components=None):\n    pca = PCA(n_components)\n    data_transformed = pca.fit_transform(data)\n    return pca, data_transformed","dd1baea8":"def plot_variance_sklearn(pca):\n    exp_variance = np.cumsum(pca.explained_variance_ratio_)\n    plt.plot(exp_variance)\n    plt.xlabel('No of Components')\n    plt.ylabel('Explained Variance')\n    plt.title('Explained Variance Ratio')\n    plt.grid(b=True)\n    plot = plt.show()","758d971d":"pca_model, pca_transformed = sklearn_pca(X_scaled)","a6ca00c4":"plot_variance_sklearn(pca_model)\n","6f260d2c":"pca_model, pca_transformed = sklearn_pca(X_scaled, 9)","791de7d0":"X_train, X_val, y_train, y_val = train_test_split(X_scaled, train_Y, test_size=0.3)","0acd7d52":"print(X_train.shape)\nprint(X_val.shape)","a9fc0c34":"lr = LogisticRegression(random_state=42)\nlr.fit(X_train, y_train.values.ravel())","6f657a1a":"y_pred = lr.predict_proba(X_val)[:,1]\nprint('Logistic regression ROC-AUC: {:.2f}'.format(roc_auc_score(y_val, y_pred)))","a43f71da":"def model_trainer(model, X_train, y_train, X_val, y_val):\n   \n    t = time.time()\n    model = model.fit(X_train, y_train.values.ravel())\n    y_pred = model.predict(X_val)\n    \n    roc_score = round(roc_auc_score(y_val, y_pred), 2)\n\n    train_time = time.time() - t\n    \n    return roc_score, train_time","ccd3e2a7":"def run_multiple(classifiers, X_train, y_train, X_val, y_val):\n    \n    result={\n    'classifier':[],\n    'AUC-ROC score':[],\n    'train_time':[]\n    }\n    for name, classifier in classifiers:\n        score, t = model_trainer(classifier, X_train, y_train, X_val, y_val)\n        result['classifier'].append(name)\n        result['AUC-ROC score'].append(score)\n        result['train_time'].append(t)\n    results_df = pd.DataFrame.from_dict(result, orient='index').transpose()\n    return results_df","4394abe8":"classifiers = [\n    #(\"Logistic Regression\", LogisticRegression(random_state=42)),\n    (\"XGBClassifier\",xgb.XGBClassifier(random_state=42)),\n    (\"SVC\", SVC(gamma=2, C=1)),\n    (\"QD Analysis\", QuadraticDiscriminantAnalysis()),\n    (\"Naive Bayes\", GaussianNB()),\n    (\"Nearest Neighbors\", KNeighborsClassifier(3)),\n    (\"Decision Tree\", DecisionTreeClassifier(random_state=42)),\n    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"GradientBoostingClassifier\", GradientBoostingClassifier(random_state=42))\n]","b10ea42c":"run_multiple(classifiers, X_train, y_train, X_val, y_val)","d8ba1fbb":"best_classifiers = [\n    (\"XGBClassifier\",xgb.XGBClassifier(random_state=42)),\n    (\"Random Forest\", RandomForestClassifier(random_state=42)),\n    (\"AdaBoost\", AdaBoostClassifier(random_state=42)),\n    (\"GradientBoostingClassifier\", GradientBoostingClassifier(random_state=42))\n]","8c35fc1a":"vot_hard = VotingClassifier(estimators = best_classifiers, voting ='hard') \nvot_hard.fit(X_train, y_train.values.ravel()) \ny_pred = vot_hard.predict(X_val) ","dd3a80ad":"print('Accuracy on validation set: {:.2f}'.format(vot_hard.score(X_val, y_val)))\nprint('ROC-AUC: {:.2f}'.format(roc_auc_score(y_val, y_pred)))","8aa5e459":"grid_params = [\n    #xgb\n    [{\n        'n_estimators': range(20, 200, 20),\n        'max_depth': range(5, 15, 1),\n        'eta': [0.05, 0.1, 0.2],\n        'min_child_weight': (1, 12, 2)\n    }],\n    #random forest\n    [{\n        'n_estimators': range(20, 200, 20),\n        'criterion': ['gini', 'entropy'],\n        'max_depth': range(5, 15, 1),\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'min_samples_leaf': (1, 13, 2),\n        'min_samples_split': range(20, 100, 10)\n    }],\n    #adaboost\n    [{\n        'n_estimators': [25, 50, 100, 150, 200],\n        'learning_rate' : [0.01,0.05,0.1,0.3,0.5,1]\n    }],\n    #gb\n    [{\n        'n_estimators': range(20, 200, 20),\n        'max_depth': range(2, 16, 2),\n        'max_features': ['auto', 'sqrt', 'log2'],\n        'min_samples_leaf': (1, 13, 2),\n        'min_samples_split': range(20, 100, 10),\n        'subsample':[0.6,0.7,0.8],\n        'learning_rate':[0.05, 0.1]\n    }]\n]","1426e35f":"for estimator, grid_dict in zip(best_classifiers, grid_params):\n    best_model = GridSearchCV(estimator = estimator[1],\n                         param_grid = grid_dict,\n                         scoring = \"roc_auc\",\n                         cv = 5,\n                         n_jobs=-1,\n                         verbose=1)\n    t = time.time()\n    best_model.fit(X_train, y_train.values.ravel())\n    print(f'{estimator[0]} - done in: {time.time() - t}')\n    best_param = best_model.best_params_\n    estimator[1].set_params(**best_param) \n    ","bafbf591":"vot_hard = VotingClassifier(estimators = best_classifiers, voting ='hard') \nvot_hard.fit(X_train, y_train.values.ravel()) \ny_pred = vot_hard.predict(X_val)","c3efbc23":"print('Accuracy on validation set: {:.2f}'.format(vot_hard.score(X_val, y_val)))\nprint('ROC-AUC: {:.2f}'.format(roc_auc_score(y_val, y_pred)))","1d6ca5ab":"import torch\nimport torch.nn.functional as F\nfrom torch import nn, optim\n\nclass BinaryClassifier(nn.Module):\n\n    def __init__(self, input_features, hidden_dim, output_dim):\n        \n        super(BinaryClassifier, self).__init__()\n        \n        self.fc1 = nn.Linear(input_features, 100)\n        self.fc2 = nn.Linear(100, 100)\n        self.fc3 = nn.Linear(100, output_dim)\n        self.drop = nn.Dropout(0.3)\n\n        self.sig = nn.Sigmoid()\n    \n    def forward(self, x):\n       \n        out = F.relu(self.fc1(x)) # activation on hidden layer\n        out = self.drop(out)\n        out = self.fc2(out)\n        #out = self.drop(out)\n        out = self.fc3(out)\n        out = self.drop(out)\n        \n        return self.sig(out)\n","37c85c4f":"X_train_nn = torch.from_numpy(X_train).float()\ny_train_nn = torch.squeeze(torch.from_numpy(y_train.to_numpy()).float())\n\nX_val_nn = torch.from_numpy(X_val).float()\ny_val_nn = torch.squeeze(torch.from_numpy(y_val.to_numpy()).float())","ad4848e8":"nn_model = BinaryClassifier(X_train_nn.shape[1], 50, 1)","9c89aad9":"optimizer = optim.Adam(nn_model.parameters(), lr=0.001)\ncriterion = torch.nn.BCELoss()","70144b59":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(\"Using device {}.\".format(device))","bd94d5e0":"X_train_nn = X_train_nn.to(device)\ny_train_nn = y_train_nn.to(device)\n\nX_val_nn = X_val_nn.to(device)\ny_val_nn = y_val_nn.to(device)","f6ecb39c":"nn_model = nn_model.to(device)\ncriterion = criterion.to(device)","aa9cb8bc":"for epoch in range(1, 30001):\n    nn_model.train() # Make sure that the model is in training mode.\n\n    total_loss = 0\n\n    optimizer.zero_grad()\n\n        # get predictions from model\n    y_pred_nn = nn_model(X_train_nn)\n            \n        # perform backprop\n    loss = criterion(y_pred_nn, y_train_nn)\n    loss.backward()\n    optimizer.step()\n            \n    total_loss += loss.data.item()\n    \n    if epoch%1000 == 0:\n        print(\"Epoch: {}, Loss: {}\".format(epoch, round(total_loss, 4)))","7fe07bb3":"classes = ['Not survived', 'Survived']\n\ny_pred_nn = nn_model(X_val_nn)\n\ny_pred_nn = y_pred_nn.ge(.5).view(-1).cpu()\ny_val_nn = y_val_nn.cpu()\n\nprint('ROC-AUC: {:.2f}'.format(roc_auc_score(y_val_nn, y_pred_nn)))\nprint(classification_report(y_val_nn, y_pred_nn, target_names=classes))","831d59af":"test = pd.read_csv('..\/input\/titanic\/test.csv', sep=',')\ntest.head()","ba14873a":"passengerId = test['PassengerId']\ntest = test.drop(['PassengerId'],axis=1)\ntest.head()","c30dacd3":"def preprocess(df):\n    count_family_members(df)\n    extract_cabin_type(df)\n    extract_titel(df)\n    replace_title(df)\n    complimentary_ticket(df)\n    fill_nan(df)\n    age_bin(df)\n    travel_alone(df)\n    label_encoding(df)\n    create_new_features(df)\n    print(df.shape)\n    df = drop_columns(df)\n    print(set(df.columns) - set(train_X.columns))\n    df = perform_log_transform(df)\n    return scale_dataset(df)","af3b5033":"test = preprocess(test)\ntest","4631fc73":"solution_pred = vot_hard.predict(test)","4bfa4304":"data = {'Survived': solution_pred}\nsolution_df = pd.DataFrame(index=passengerId, data=data)","95ca6d97":"solution_df.head(15)","3ddfbb89":"solution_df.to_csv(\"vote_hard_K_Final.csv\")","047c04aa":"### 3.3 Benchmark models\n\nHere I will run several different scikit-learn's classifiers, to see what works best.","2842a763":"In general data looks great, except of Age and Cabin columns. Each Cabin value is actually a combination of the Deck name and cabin number. Therefore, we may extract the Deck names because thay might be useful. \n\n### 2.1 Transform Cabin to Deck","4365595c":"### 2.4 Binning\n\nIn this dataset we do have some features to which we are supposed apply binnig.\n\nThe main motivation of binning is to make the model more robust and prevent overfitting, however, it has a cost to the performance. [Medium](https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114)","d74bab80":"## 1 Explore Dataset\n\n### 1.1 Load all necessary data + libraries","456a1d8e":"### 2.3 Outliers\nAt this point I want to check for possible outliers.","2ff62d27":"It turns out that most passenger who were picked up in Cherbourg survived.\n\n### 1.5 Family size","99227128":"So now, we can fill missing values by median value from the result of grouping values by specific features such as Class, Title and Sex.  ","a52ba062":"## 3 Modeling\n\n### 3.1 PCA for data varience ","272a8330":"So i have check the meaning of titles on wikipedia to perform binnig on them. Below are the explanations.\n\n* Capt       - military\n* Col        - military\n* Countess   - English nobility\n* Don        - a Spanish, Portuguese, southern Italian, and Filipino nobility title\n* Dr         - academic title for medical practitioner \/ honorific\n* Jonkheer   - Netherland's nobility\n* Lady       - English nobility\/honorific\n* Major      - military\n* Rev        - a courtesy title for Protestant Christian ministers or pastors \/ honorific\n* Sir        - honorific","73f5046a":"We can actiually cover more than 90% of data varience with only 6 features. And we need 10 for 100%","429655df":"### 2.6 Log tranformation","ff9f4d76":"From the graph above we may conclude, that Sex had an impact on the survival chance. There were more woomen among survivors. \"A disproportionate number of men were left aboard because of a \"women and children first\" protocol for loading lifeboats.\" [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Titanic)\n\n### 1.4 Survived by place of the embarkment\n\nAfter leaving Southampton on 10 April 1912 with 922 passengers on board, Titanic headed at Cherbourg in France and Queenstown (now Cobh) in Ireland, before heading west to New York.","ade1a6f9":"### 1.2 Survived vs. Class\n\nThe first thought that came to my mind was that most probably the survival chance depended on class. So we can group people from dataset by class to which they belonged and chack the survival rate.","040be59c":"### 2.5 Feature engineering\n\nBecause wether person traveled alone or with family had an impact on survival chance, I have decided to create new feature \"IsAlone\".","39967ea9":"It might be useful to derive new features from existing ones.","fb504b96":"### Extra: Try Simple FNN","0bdd042d":"From the graph above we may conclude that those who had families had more chances to survive.\n\n## 2 Preprocess data\n\nFirst, I want to check the quantity of missing values in train dataset.","ff3cdfee":"This NN is not as much efficient as the rest of the ML algorithms used before. However, it is a good idea to experiment more with NN solutions for this problem.\n\n## 5 Submission","803b00ed":"But why choosing only one model for tunning, if we can use several simultaniously. Therefore, I picked out 4 classifiers with best performance and unify them with Voting Classifier.\n\n### 3.4 Voting Classifier","12ddeba6":"Now, I want to use feature correlation across the entire dataset to determine which features are too highly-correlated. Because if such exist they may over-inflate the importance of a single feature.","45b708bf":"I will keep most of columns anyway, to see how model will deal with it.","20d98e80":"The above-mentioned people actually got complimentary tickets. Therefore I will create new feature to handle this case.","433af480":"Indeed, most people from the first class survived and from 3rd parished.\n\n### 1.3 Survived vs. Sex\n\nThe next idea is to verify wether more women survived rather than men. It is actually, very logical, because children and women were first to board the lifeboats.","d67b0f99":"According to the Encyclopedia Titanica, [Mr. Algernon Henry Wilson Barkworth](https:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/algernon-barkworth.html) was 48 y.o. at the moment he stepted on the board of Titanic, not 80. So we will need to re-assign this value later.\n\nThere also were people who payed more than 500 for the ticked. Actually, it is not a mistake and was true.","906382a1":"From data it turned out that there were more man than women on board of Titanic in general.","5503a886":"### 3.2 Split dataset into train\/validation","7a72ae71":"### 2.7 Scaling dataset","72aa1a1f":"### 2.2 Combine Titles\n\nThe next things is Titles from \"Name\" Column. I want to extract thrm and have a look.","60c72148":"## 4 Hyperparameters tunning\n"}}