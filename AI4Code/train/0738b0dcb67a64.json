{"cell_type":{"daa5f84a":"code","05985e4e":"code","087b401c":"code","c6ee417f":"code","c189ef1b":"code","3af50bc3":"code","93483c18":"code","b5dd90b0":"code","9074d943":"code","35d203e4":"code","b5246916":"code","793cb57a":"code","4b2ce32d":"code","7d1963f6":"code","efdb901f":"code","56d582f3":"code","89a13d2e":"code","edcc659b":"code","eb1b3f20":"code","e01e3ac8":"code","aa73e034":"code","8675e615":"code","da9cfe9f":"code","761b32bf":"code","761b1a8c":"code","677c6c98":"code","897d410a":"code","742a1d93":"code","faa24b3d":"code","e992027f":"code","84d22688":"code","1197cd9a":"code","30aa3505":"code","97014fbd":"code","79a78c1d":"code","97dd5bef":"code","fc285a17":"code","272d1543":"code","cb3878b3":"code","db81385f":"code","fb62145d":"code","a1216a56":"code","4b026939":"code","df16fffc":"code","cc74e3f7":"code","1ee64c04":"code","ae386e2a":"code","ebc4b70e":"markdown","0b8a4bb9":"markdown","e9bfc795":"markdown","f92b25d3":"markdown","6d7112ec":"markdown","2ea2b907":"markdown","8bee9761":"markdown","508f2b1a":"markdown","b520e3f4":"markdown","e9bbfde9":"markdown","e0c6fddf":"markdown","571d2d3c":"markdown","983c89ae":"markdown","6676793d":"markdown","3436c897":"markdown","e6889e0a":"markdown","95fc8d91":"markdown","0263eb8b":"markdown","56fc7808":"markdown","f5ee9dbf":"markdown","e75d4288":"markdown","91659045":"markdown","3911fb3d":"markdown","153e3411":"markdown","4104672d":"markdown","746deab4":"markdown","fc96eb32":"markdown","61b9f54e":"markdown","e972afd1":"markdown","67065f0a":"markdown","95c09c7a":"markdown","be288db9":"markdown"},"source":{"daa5f84a":"\nimport numpy as np\nimport pandas as pd\nfrom scipy import stats\nimport math\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n","05985e4e":"#import data from kaggle store\ndata=pd.read_csv('\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv')\n#data.head()","087b401c":"# nice resume table to describe the data\ndef resumetable(df):\n    print(f\"Dataset Shape: {df.shape}\")\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    summary['Name'] = summary['index']\n    summary = summary[['Name','dtypes']]\n    summary['Missing'] = df.isnull().sum().values    \n    summary['Uniques'] = df.nunique().values\n    summary['First Value'] = df.loc[0].values\n    summary['Second Value'] = df.loc[1].values\n    summary['Third Value'] = df.loc[2].values\n    summary['Fourth Value'] = df.loc[3].values\n    summary['Fifth Value'] = df.loc[4].values\n\n    for name in summary['Name'].value_counts().index:\n        summary.loc[summary['Name'] == name, 'Entropy'] = round(stats.entropy(df[name].value_counts(normalize=True), base=10),4) \n\n    return summary","c6ee417f":"# resumetable(data)\n\n#also handy built\/in function for data description\n# data.describe()\n\n# and one more\n# data.info()","c189ef1b":"# retyping TotalCharges to numeric\ndata['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n\n# for the sake of ease while plotting\ndf_y = data[data[\"Churn\"]==\"Yes\"] \ndf_n = data[data[\"Churn\"]==\"No\"]","3af50bc3":"# Calculate differene between Totalcharge and Tenure*MonthlyCharges\ndata['TotalCharge_diff'] = (data['tenure'] * data['MonthlyCharges']) - data['TotalCharges']\ndata['TotalCharge_diff_abs'] = data['TotalCharge_diff'].abs()\n# leaving both as a possible good features, from logic of the thing, I suppose only TotalCharges_diff will be of any use","93483c18":"# plot\nplt.figure(figsize=(14, 4))\nplt.title(\"KDE for {}\".format('TotalCharge_diff'))\nax0 = sns.histplot(data[data['Churn'] == 'No']['TotalCharge_diff'].dropna(), color = \"#22ff57\", label= 'Churn: No')\nax1 = sns.histplot(data[data['Churn'] == 'Yes']['TotalCharge_diff'].dropna(), color= \"#FF5722\", label= 'Churn: Yes')\nplt.legend(prop={'size': 12})\n","b5dd90b0":"#kde_plot('TotalCharge_diff')\n#kde_plot('TotalCharge_diff_abs')","9074d943":"# create copy for thingama-jigging with categorical vars\ndf=data","35d203e4":"# borrowed fcn for plotting nice barplots\ndef barplot_percentages(feature, orient='v', axis_name=\"percentage of customers\"):\n    ratios = pd.DataFrame()\n    g = df.groupby(feature)[\"Churn\"].value_counts().to_frame()\n    g = g.rename({\"Churn\": axis_name}, axis=1).reset_index()\n    g[axis_name] = g[axis_name]\/len(df)\n    if orient == 'v':\n        ax = sns.barplot(x=feature, y= axis_name, hue='Churn', data=g, orient=orient)\n        ax.set_yticklabels(['{:,.0%}'.format(y) for y in ax.get_yticks()])\n    else:\n        ax = sns.barplot(x= axis_name, y=feature, hue='Churn', data=g, orient=orient)\n        ax.set_xticklabels(['{:,.0%}'.format(x) for x in ax.get_xticks()])\n    ax.plot()","b5246916":"# borrowed fcn for plotting pie plots with percentages of each category based on rule\ndef plot_var_percentages (df, var_list):\n\n    n_rows = math.ceil(len(var_list)\/3)\n    mapper = []\n    count_c = 0\n    count_r = 0\n    for n in range(len(var_list)):\n        if count_c <= 2:\n            mapper.append((count_r,count_c))\n            count_c += 1\n        else:\n            count_r += 1\n            count_c = 0\n            \n    #fig, axes = plt.subplots(nrows = n_rows,ncols = 3,figsize = (15,12))\n    for i,var in enumerate(var_list):\n        \n        labels = list(df[var].value_counts().index)\n        counts = list(df[var].value_counts())\n        \n        plt.figure(i)\n        plt.pie(counts, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)\n        plt.title(var)\n    plt.show ","793cb57a":"# Brief look at data distribution across categories\nvar_list = data.columns[1:-5].drop(['tenure'])\n\n#plot_var_percentages(data, var_list)","4b2ce32d":"#print(var_list)\n#print(data.columns)\n\n# Lets start with our seniors\n#barplot_percentages(\"SeniorCitizen\")","7d1963f6":"df['churn_rate'] = df['Churn'].replace(\"No\", 0).replace(\"Yes\", 1)\n#g = sns.FacetGrid(df, col=\"SeniorCitizen\", height=4, aspect=.9)\n#ax = g.map(sns.barplot, \"gender\", \"churn_rate\", palette = \"Blues_d\", order= ['Female', 'Male'])","efdb901f":"#churn rates across customers w\/ partners and dependents\n\n# fig, axis = plt.subplots(1, 2, figsize=(12,4))\n# axis[0].set_title(\"Has partner\")\n# axis[1].set_title(\"Has dependents\")\n# axis_y = \"percentage of customers\"\n# # Plot Partner column\n# gp_partner = df.groupby('Partner')[\"Churn\"].value_counts()\/len(df)\n# gp_partner = gp_partner.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n# ax = sns.barplot(x='Partner', y= axis_y, hue='Churn', data=gp_partner, ax=axis[0])\n# # Plot Dependents column\n# gp_dep = df.groupby('Dependents')[\"Churn\"].value_counts()\/len(df)\n# gp_dep = gp_dep.to_frame().rename({\"Churn\": axis_y}, axis=1).reset_index()\n# ax = sns.barplot(x='Dependents', y= axis_y, hue='Churn', data=gp_dep, ax=axis[1])","56d582f3":"# Categorical vars - MultipleLines\n#plt.figure(figsize=(9, 4.5))\n#barplot_percentages(\"MultipleLines\", orient='h')","89a13d2e":"# What internet service the customer has? \n#plt.figure(figsize=(9, 4.5))\n#barplot_percentages(\"InternetService\", orient=\"h\")","edcc659b":"plt.figure(figsize=(15, 15))\nsome_vars = ['gender','SeniorCitizen','Partner','Dependents','PhoneService','PaperlessBilling']\ni=1\nfor var in some_vars:\n    plt.subplot(3, 2, i)\n    sns.countplot(x=var,data=data, hue='Churn')\n    i+=1","eb1b3f20":"# Show what we get here. Again.\nresumetable(df)","e01e3ac8":"non_dummy_cols = ['customerID','tenure','MonthlyCharges','TotalCharges','Churn','churn_rate','TotalCharge_diff','TotalCharge_diff_abs'] \ndummy_cols = list(set(df.columns) - set(non_dummy_cols))\ndf_test = pd.get_dummies(df, columns=dummy_cols)\n\n# non_dummy_cols = ['A','B','C'] \n# Takes all other columns\n# dummy_cols = list(set(df.columns) - set(non_dummy_cols))\n# df = pd.get_dummies(df, columns=dummy_cols)\n","aa73e034":"# check dummies\n# resumetable(df_test)","8675e615":"# tenure - create two more categories, as the tenure feature does not have linear behaviour\ndf_test['tenure_short'] = np.where(df_test['tenure']<18, 1, 0)\ndf_test['tenure_long'] = np.where(df_test['tenure']>54, 1, 0)\n#df.head()","da9cfe9f":"# create cat var for high monthly charges\n#df.loc[:,'high_payer'] = np.where(df['MonthlyCharges'] > 60, 1,0)","761b32bf":"# frames = [df, df_dummies]\n\n# df_ready = pd.concat(frames,axis=1)\n# print(df_ready.head())","761b1a8c":"#resumetable(df_ready)","677c6c98":"# drop NaNs in TotalCharges\ndf_test = df_test.dropna()\n\n# drop customerID, as would not be of any help\ndf_test.drop(['customerID'],axis=1,inplace=True)\n\nresumetable(df_test)","897d410a":"# Further usage of just \"df\"\ndf = df_test","742a1d93":"# retype to boolean\nnon_int_cols = ['tenure','MonthlyCharges','TotalCharges','Churn','churn_rate','TotalCharge_diff','TotalCharge_diff_abs'] \nint_cols = list(set(df.columns) - set(non_int_cols))\ndf[int_cols] = df[int_cols].astype(bool)\n\n# retype floats\nfloat_cols = ['MonthlyCharges','TotalCharges','TotalCharge_diff','TotalCharge_diff_abs']\ndf[float_cols] = df[float_cols].astype(np.int64)\n","faa24b3d":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\nfrom sklearn.linear_model import LogisticRegression","e992027f":"corrMatrix = df.drop(['Churn','churn_rate'], axis=1).corr()\nfig, ax = plt.subplots(figsize=(30,25))\nsns.heatmap(corrMatrix,annot=True, annot_kws={'size':12},cmap=\"GnBu\")\nplt.show();","84d22688":"# drop \"No internet service\" items and others with high correlation. It was nto clear to me what is the meaning, from the correlation it is clear there is no information added by multiple columns\n# this was actually added after looking at Correrlation matrix, but I left it here for the sake of simplicity\ndf_test.drop(['OnlineBackup_No internet service','TechSupport_No internet service','StreamingTV_No internet service','DeviceProtection_No internet service','OnlineBackup_No internet service',\n              'OnlineSecurity_No internet service', 'StreamingMovies_No internet service', 'MultipleLines_No phone service','PhoneService_No'],axis=1,inplace=True)\n              # ,'MultipleLines_No',\n              # 'OnlineSecurity_No','OnlineBackup_No','DeviceProtection_No','TechSupport_No','StreamingTV_No','StreamingMovies_No'],\n              # axis=1,inplace=True)\n        \n# leaving out all the rest for now","1197cd9a":"#Correlation of \"Churn\" with other variables in 1D:\n\nplt.figure(figsize=(15,8))\ndf.drop(['Churn'], axis=1).corr()['churn_rate'].sort_values(ascending = False).plot(kind='bar')","30aa3505":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import RobustScaler","97014fbd":"target0 = df['churn_rate'] # for y\nfeatures0 = df.drop(['Churn','churn_rate'], axis=1) # for X","79a78c1d":"# To preserve the shape of the dataset (no distortion), data will be min max scaled to values between (0, 1) \n# instead of standard scaled. I tried also StandardScaler, but results were worse since the distribution of data is not gaussian. \n# RobustScaler was similar in performance to MinMaxScaker\nscaler0=MinMaxScaler()\n\nf_scale0 = scaler0.fit_transform(features0)","97dd5bef":"# create train and test split on scaled data\nX_train0, X_test0, y_train0, y_test0 = train_test_split (f_scale0,target0,test_size=0.2, random_state=123)","fc285a17":"from sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn.model_selection import GridSearchCV","272d1543":"# grid search to find optimal value of C (for regularization) and solver\n\nmodel_gs = LogisticRegression(max_iter=500)\nsolvers = ['newton-cg', 'lbfgs', 'liblinear']\npenalty = ['l2']\nc_values = [100, 10, 1.0, 0.1, 0.01]\n# define grid search\ngrid = dict(solver=solvers,penalty=penalty,C=c_values)\ncv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\ngrid_search = GridSearchCV(estimator=model_gs, param_grid=grid, n_jobs=-1, cv=cv, scoring='accuracy',error_score=0)\ngrid_result = grid_search.fit(X_train0, y_train0)","cb3878b3":"# summarize results of grid search\nprint(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\nmeans = grid_result.cv_results_['mean_test_score']\nstds = grid_result.cv_results_['std_test_score']\nparams = grid_result.cv_results_['params']\nfor mean, stdev, param in zip(means, stds, params):\n    print(\"%f (%f) with: %r\" % (mean, stdev, param))","db81385f":"from sklearn.metrics import roc_curve, auc\n\n# Zero gen model with tuned C and solver\n\n# increased the number of iterations here, as the lbfgs solver was not converging in 100 steps\nlogreg0 = LogisticRegression(max_iter=500,C=10, penalty='l2', solver='lbfgs')\n\n#Probability scores for test set\ny_score0 = logreg0.fit(X_train0, y_train0).decision_function(X_test0)\n#False positive Rate and true positive rate\nfpr0, tpr0, thresholds0 = roc_curve(y_test0, y_score0)\n\n#Visualization for ROC curve\nsns.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n\nprint('AUC: {}'.format(auc(fpr0, tpr0)))\nplt.figure(figsize=(10,8))\nlw = 2\nplt.plot(fpr0, tpr0, color='darkorange',\n         lw=lw, label='ROC curve')\nplt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\nplt.xlim([-0.05, 1.0])\nplt.ylim([0.0, 1.05])\nplt.yticks([i\/20.0 for i in range(21)])\nplt.xticks([i\/20.0 for i in range(21)])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic (ROC) Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n#print(y_score0)","fb62145d":"from sklearn.metrics import confusion_matrix\ny_hat_test0 = logreg0.predict(X_test0)\ncm0=confusion_matrix(y_test0,y_hat_test0)\nconf_matrix0=pd.DataFrame(data=cm0,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsns.heatmap(conf_matrix0, annot=True,fmt='d',cmap=\"YlGnBu\")\n#print(y_hat_test0)","a1216a56":"from sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report\n#print(precision_recall_fscore_support(y_hat_test0, y_test0))\nprint(classification_report(y_hat_test0, y_test0))\nprint(\"Accuracy: \")\nprint (accuracy_score(y_test0, y_hat_test0))","4b026939":"# checking what is the accuracy on Train set. lbfgs solver contains L2 regularization by default\nprint(\"Train Accuracy:\",logreg0.score(X_train0, y_train0))","df16fffc":"# check features0\n# features0.columns.values","cc74e3f7":"# To get the weights of all the variables\nweights = pd.Series(logreg0.coef_[0],\n                 index=features0.columns.values)\n\nplt.figure(figsize=(15, 5))\nplt.subplot(1, 2, 1)\n# show features negatively affecting churn - based on LR coefficients\nweights.sort_values(ascending = False)[:10].plot(kind='bar')\nplt.subplot(1, 2, 2)\n# show features positively affecting churn - based on LR coefficients\nweights.sort_values(ascending = False)[-10:].plot(kind='bar')\n","1ee64c04":"from sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import validation_curve\n### 1. Use of validation curves for both datasets.\nC_param_range = [0.001,0.005,0.01,0.05,0.1,0.5,1,5,10,50,100]\n\nplt.figure(figsize=(15, 10))\n\n# Apply logistic regression model to training data\nlr = LogisticRegression(penalty='l2',C = i,random_state = 42,max_iter=500)\n\n# Plot validation curve\ntrain_scores, test_scores = validation_curve(estimator=lr\n                                                            ,X=X_train0\n                                                            ,y=y_train0\n                                                            ,param_name='C'\n                                                            ,param_range=C_param_range\n                                                            )\n\ntrain_mean = np.mean(train_scores,axis=1)\ntrain_std = np.std(train_scores,axis=1)\ntest_mean = np.mean(test_scores,axis=1)\ntest_std = np.std(test_scores,axis=1)\n\nplt.plot(C_param_range\n            ,train_mean\n            ,color='blue'\n            ,marker='o'\n            ,markersize=5\n            ,label='training accuracy')\n\n    \nplt.plot(C_param_range\n            ,test_mean\n            ,color='green'\n            ,marker='x'\n            ,markersize=5\n            ,label='test accuracy') \n    \nplt.xscale('log')\nplt.xlabel('C_parameter')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.725,0.825])\n","ae386e2a":"churn_prob = logreg0.predict_proba(X_test0[:10])\nprint(churn_prob)\nprint(y_test0[:10])","ebc4b70e":"* MultipleLines var seems to be not of much use","0b8a4bb9":"Customers w\/o partners and dependents are more likely tu churn. Feeling free. Interesting.","e9bfc795":"* Now, depending on our needs, we might in the end go with different C value, e.g. 0.1, as this might generalize best, as we have a \n* *     *small difference* of accuracies on train and test set and \n* *     *high both train a test set performance*\n* The lower test set accuracy around C=0 might be because of this specific data split\/e.g. inconsistency in data","f92b25d3":"* The customers are probarly more satisfied with fiber optic connection than with DSL.\n* This mighe be a good feature!","6d7112ec":"* so TotalCharge_diff is not correlated with churn, which makes sense amd is great \n* Gender, as expected, is also not. \n\n* Seems we can leave all features for now, none are so heavily correlated that they should be omitted atm. ","2ea2b907":"# Features correlation","8bee9761":"* Seems we do not have many senior citizen customers (even though we clearly see how are they behaing differently concernign churn)\n* PhoneService - also not many user not using PhoneService. Lets see, if this inbalance will cause trouble further. ","508f2b1a":"* Interestingly, not churning customers seems to be following \"fatter\" distribution ends while looking at difference of payment. \n* It seems that customers that were not exposed to change of payment are more prone to churn, while customers that were exposed to change in payment are less prone to churn. \n* **Interestingly the distribution is kind of symetrical on both sides, but, as expected, the fatter end is on the left side, i.e. customers who got a discount compred to their previous pay were less likely to churn**\n* **BUT** we might be exposed to Law of Small Numbers, as the sample nor the effect are as big.","b520e3f4":"# Weights of model illustrated","e9bbfde9":"# ROC-AUC score","e0c6fddf":"# CustomerID dropping!","571d2d3c":"#  **4. Now lets explore the categorical variables**","983c89ae":"# Next possible steps\n\n* prettify the syntax, add more comments and use more looping for the sake of simplicity. This solutions is just a get-dirty-hands try, and I know it doesnt read as easy as I would liek to... ","6676793d":"* It seems senior citizens like to change operators more often. Might be bored at home and be the only one to answer the cold calls. Or they might just have more time to calculate what services do they need for what price. Or stg else :-)\n* One way or the other, it seems that being a senior citizen goes with significantly higher probability of churn.","3436c897":"# 3. TotalCharge vs Tenure x MonthlyCharges - Discounts effect\n* TotalCharge should equal MonthlyCharges x Tenure. If not, it is a sign of a given discount or price inrease, that the customer got. \n* That might be a big factor for churning, lets see further","e6889e0a":"* Churn rate of women\/men is similar accross ages (but is highr for senior citizens)","95fc8d91":"# ***Telco customer churn predictions*** \nrecommended music for exploring this notebook: \nhttps:\/\/www.youtube.com\/watch?v=t3217H8JppI&ab_channel=AnAmericanComposer\n\nWas used while creating.\n\n****","0263eb8b":"* The correlation matrix is heavy a lot, but nevertheless we see what features we can drop atm.","56fc7808":"# Import libraries, for starters","f5ee9dbf":"# How to apply findings?\n* Accuracy is certainly not high, although giving some hint and maybe better than nothing (in 75\/25 data split having accuracy over 82% is just 7% better.. which is not much). But still increase of roughly 30% compared to not using any model.\n* Since it is easy to get to probabilities outputted by Logistic Regression, we might consider using these probabilities of churn, and maybe combine information about probability of churning with MonthlyCharges and try not to loose most valuable customers\n","e75d4288":"# **2. Lets look at histogram of numerical variables: tenure, MonthlyCharge and TotalCharge**\n\nKDE was omitted intentionally for the first few plots, in order not to pollute visually the histograms","91659045":"# Grid search and model evaluation","3911fb3d":"# Open questions\n\n* Does a circa 75\/25% churn split match the real life situation of a telco cpy? \n* What metrics would fit best the needs of service provider - is recall more important? Or false negatives? Probably FN connected to revenue generated by customer?","153e3411":"* No missing data\n* One line ... one customer\n* Objects should be retyped, TotalCharges checked\n","4104672d":"# Train\/test accuracy check\n* From the results above, it is clear we have different results on train and test set. \n* It will be interesting to look at model performance on the 2 data sets with different C values","746deab4":"# Retype to ints and bools","fc96eb32":"# Import data and explore basic properties","61b9f54e":"# Apply feature scaling and split dataset","e972afd1":"# 5. Preprocessing: Data preps, feature adding based on EDA","67065f0a":"# Credits\n* https:\/\/www.analyticsvidhya.com\/blog\/2016\/02\/complete-guide-parameter-tuning-gradient-boosting-gbm-python\/ \n* https:\/\/towardsdatascience.com\/the-dummys-guide-to-creating-dummy-variables-f21faddb1d40 \n* https:\/\/machinelearningmastery.com\/one-hot-encoding-for-categorical-data\/ \n* https:\/\/www.analyticsvidhya.com\/blog\/2020\/03\/one-hot-encoding-vs-label-encoding-using-scikit-learn\/\n* https:\/\/machinelearningmastery.com\/hyperparameters-for-classification-machine-learning-algorithms\/\n* https:\/\/www.kaggle.com\/joparga3\/2-tuning-parameters-for-logistic-regression\n* ... and various Kaggle kernels\n","95c09c7a":"Looking good now. ","be288db9":"* This might lead us to giving some vouchers, packages to customers with churn probability e.g >40% or stg like that"}}