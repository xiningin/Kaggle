{"cell_type":{"a8c1a57b":"code","d5ddf353":"code","7b052b18":"code","218a552c":"markdown","dca4bac1":"markdown","679de40b":"markdown","67f2a83b":"markdown","8c103df4":"markdown"},"source":{"a8c1a57b":"import numpy as np \nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import StratifiedKFold, RandomizedSearchCV\nfrom sklearn.metrics import auc, roc_auc_score","d5ddf353":"train = pd.read_csv('..\/input\/song-popularity-prediction\/train.csv').iloc[:, 1:]\ntest = pd.read_csv('..\/input\/song-popularity-prediction\/test.csv').iloc[:, 1:]\n\nx = train.iloc[:, :-1]\ny = train.song_popularity ","7b052b18":"cv_inner = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)          #1\ncv_outer = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\nclassifier = RandomForestClassifier(n_estimators=100, n_jobs=-1)               #2\np_grid = {'min_samples_split': [60, 120, 180],\n          'max_depth': [5, 10, 15],\n          'max_features': ['sqrt', 'log2']}\n\nhistory = []\n\npointer = 1\nfor train_index, test_index in cv_outer.split(x, y):                          #3\n    print('\\nNestedCV: {} of outer fold {}'.format(pointer, cv_outer.get_n_splits()))\n    x_train, x_test = x.loc[train_index], x.loc[test_index]\n    y_train, y_test = y.loc[train_index], y.loc[test_index]\n\n    model = RandomizedSearchCV(classifier, param_distributions=p_grid,\n                               scoring='roc_auc', cv=cv_inner, n_jobs=-1)     #4\n    model.fit(x_train.fillna(-99), y_train)\n    \n    pred_test = model.predict_proba(x_test.fillna(-99))\n    pred_training = model.predict_proba(x_train.fillna(-99))\n    \n    auc_train = roc_auc_score(y_train, pred_training[:, 1])                   #5\n    auc_test = roc_auc_score(y_test, pred_test[:, 1])\n    \n    print(\"\"\"\n    Best set of parameters: {}\n    Best AUC              : {:.2f}\n\n    Training\n        AUC: {:.3f}\n    Test\n        AUC: {:.3f}\n    \"\"\".format(\n        model.best_params_,\n        model.best_score_,\n        auc_train,\n        auc_test,\n        )\n    )\n    history.append(auc_test)\n    pointer += 1\n\nprint('Overall test performance: {:.2f}'.format(np.mean(history)))","218a552c":"# Nested CV Framework","dca4bac1":"# Introduction\n## Cross-Validation & Nested Cross-Validation\n\nWhen searching for parameters and estimating the error of a model, one could be tempted to use only one Cross Validation loop.\n\n<img src=\"https:\/\/www.researchgate.net\/profile\/Johar-Ashfaque\/publication\/332370436\/figure\/fig1\/AS:746775958806528@1555056671117\/Diagram-of-k-fold-cross-validation-with-k-10-Image-from-Karl-Rosaen-Log.ppm\" width=500 height=300 \/>\n\nIn the image above, we see an example of a generic **K-fold** cross validation. This procedure can be used both when **optimizing the hyperparameters of a model and when comparing and selecting a model for the dataset**. When the same cross-validation procedure and dataset are used to **both tune and select a model**, it is likely to lead to a **biased evaluation** of the model performance.\n\nOne way to avoid this is to **set aside** a piece of data, aka the **test set**.\n\n<img src=\"https:\/\/scikit-learn.org\/stable\/_images\/grid_search_cross_validation.png\" width=500 height=300 \/>\n\nIn this approach:\n- The dataset is divided in training, validation and test set\n- The model is trained using the training fold\n- Hyperparameters are tested on the validation set\n- The final performance are evaluated on the test set.\n\nWhat about **Nested Cross Validation then**?\n\nTo generalize better, we simply **rotate the test set on the entire dataset**. By doing so, we are sure to have a **better overview** of the performance of our model on the totality of the data.\n\n<img src=\"https:\/\/privefl.github.io\/R-presentation\/nested-crossval.png\" width=500 height=500 \/>\n\n**Let's see it in action!**","679de40b":"Let's see first the code in action, then I'll discuss it below","67f2a83b":"# Conclusions\n\nI hope this tutorial was useful to you, in case leave me a comment if you have any doubts or notice any inaccuracies!","8c103df4":"## Code comment\n\n**#1**\n\n`cv_inner` and `cv_outer` represent the external and internal CV splits of the NestedCV. While `cv_outer` is used to split the dataset in training\/test, `cv_inner` will be used by our `RandomizedSearchCV` to find the best set of hyperparameters, splitting the training set in training\/validation.\n\n**#2**\n\nHere we init our base model and the grid parameters to search.\n\n**#3**\n\nKey moment, as we are generating the training\/test folds\n\n**#4**\n\nAs stated in **#1**, `cv_inner` is being used by `RandomizedSearchCV` to find the best set of hyperparameters, splitting the training set in training\/validation internally with `cv=cv_inner`\n\n**#5**\n\nWith the best set of hyperparameters, we then predict both on the training and test set to check the model performance\n\n## Results\nAs can be seen, the overall performance of the model is around 0.57, stable for all test splits generated.\n\nThe best parameters vary but overall we can say that we have a situation of stability."}}