{"cell_type":{"3d1225a4":"code","d4a7b556":"code","62f284ef":"code","c643d4c3":"code","f9c639e5":"code","b5aec3c4":"code","005fb6f4":"code","ea1cd566":"code","457cba4e":"code","fd91f729":"code","3353ff95":"code","7f844d47":"code","bc8903db":"code","7af525e9":"code","3ca27e85":"code","5dfe87ea":"code","74034643":"code","54dc826b":"code","f6c45b7d":"code","f1b41b58":"code","6a9350cb":"code","866db214":"code","6b3bf9d8":"code","0f14c631":"code","5e3de0e7":"code","95690ab8":"code","d7f8eaa9":"code","586d5367":"code","737acb44":"code","2af2cd24":"code","fe8c4962":"code","aa9fe2b9":"code","37965b8a":"code","bad950f5":"code","9667f8f5":"code","128ef328":"code","b04b4acd":"code","8e5ee495":"code","025567ec":"code","8aa61548":"code","58028a78":"code","c129242c":"code","e15fb0ef":"code","ad351c43":"markdown","1c24faba":"markdown","fc24afb8":"markdown","00b111cb":"markdown","eb0b5fba":"markdown","e2e530d1":"markdown","d27d14a5":"markdown","004ed1d2":"markdown","160004be":"markdown","b06d07e6":"markdown","31619267":"markdown","b7822642":"markdown","0a0021b0":"markdown","525a6003":"markdown","04c3b38d":"markdown","320e7876":"markdown","c28bdd37":"markdown","dd101c1e":"markdown","0c4a960c":"markdown","c9c0736a":"markdown","ad725f88":"markdown","262741fa":"markdown"},"source":{"3d1225a4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport re\nimport nltk\nimport string\nfrom nltk.tokenize import regexp_tokenize, sent_tokenize, word_tokenize, TweetTokenizer\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\nsns.set()\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4a7b556":"train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntrain.head()","62f284ef":"#Understand the dataset\ntrain.shape","c643d4c3":"#summary of the dataset\ntrain.info()","f9c639e5":"# Find the number of positive and negative classes\ntrain.target.value_counts()\n\ntrain.target.value_counts() \/ len(train)","b5aec3c4":"plt.figure(figsize=(8,6))\nsns.countplot(x='target', data=train)\nplt.title('No.of Disaster Vs Non-Disaster Tweets')\nplt.xlabel('Target', fontsize=11)\nplt.show()","005fb6f4":"# importing all necessery modules \nfrom wordcloud import WordCloud, STOPWORDS \nimport matplotlib.pyplot as plt \n\nstopwords = set(STOPWORDS)\n\ntweet_words= ''\n\n# iterate through the csv file \nfor val in train.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    tweet_words += \" \".join(tokens)+\" \"","ea1cd566":"plt.figure(figsize=(10,6))\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(tweet_words) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \n  \nplt.show() ","457cba4e":"#Removal of URLS, HTML link\n\ndef remove_urls(text):\n    url_pattern = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url_pattern.sub(r'', text)\n\ndef remove_html(text):\n    html_pattern = re.compile('<.*?>')\n    return html_pattern.sub(r'', text)","fd91f729":"EMOTICONS = {\n    u\":\u2011\\)\":\"Happy face or smiley\",\n    u\":\\)\":\"Happy face or smiley\",\n    u\":-\\]\":\"Happy face or smiley\",\n    u\":\\]\":\"Happy face or smiley\",\n    u\":-3\":\"Happy face smiley\",\n    u\":3\":\"Happy face smiley\",\n    u\":->\":\"Happy face smiley\",\n    u\":>\":\"Happy face smiley\",\n    u\"8-\\)\":\"Happy face smiley\",\n    u\":o\\)\":\"Happy face smiley\",\n    u\":-\\}\":\"Happy face smiley\",\n    u\":\\}\":\"Happy face smiley\",\n    u\":-\\)\":\"Happy face smiley\",\n    u\":c\\)\":\"Happy face smiley\",\n    u\":\\^\\)\":\"Happy face smiley\",\n    u\"=\\]\":\"Happy face smiley\",\n    u\"=\\)\":\"Happy face smiley\",\n    u\":\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\":D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"8D\":\"Laughing, big grin or laugh with glasses\",\n    u\"X\u2011D\":\"Laughing, big grin or laugh with glasses\",\n    u\"XD\":\"Laughing, big grin or laugh with glasses\",\n    u\"=D\":\"Laughing, big grin or laugh with glasses\",\n    u\"=3\":\"Laughing, big grin or laugh with glasses\",\n    u\"B\\^D\":\"Laughing, big grin or laugh with glasses\",\n    u\":-\\)\\)\":\"Very happy\",\n    u\":\u2011\\(\":\"Frown, sad, andry or pouting\",\n    u\":-\\(\":\"Frown, sad, andry or pouting\",\n    u\":\\(\":\"Frown, sad, andry or pouting\",\n    u\":\u2011c\":\"Frown, sad, andry or pouting\",\n    u\":c\":\"Frown, sad, andry or pouting\",\n    u\":\u2011<\":\"Frown, sad, andry or pouting\",\n    u\":<\":\"Frown, sad, andry or pouting\",\n    u\":\u2011\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\[\":\"Frown, sad, andry or pouting\",\n    u\":-\\|\\|\":\"Frown, sad, andry or pouting\",\n    u\">:\\[\":\"Frown, sad, andry or pouting\",\n    u\":\\{\":\"Frown, sad, andry or pouting\",\n    u\":@\":\"Frown, sad, andry or pouting\",\n    u\">:\\(\":\"Frown, sad, andry or pouting\",\n    u\":'\u2011\\(\":\"Crying\",\n    u\":'\\(\":\"Crying\",\n    u\":'\u2011\\)\":\"Tears of happiness\",\n    u\":'\\)\":\"Tears of happiness\",\n    u\"D\u2011':\":\"Horror\",\n    u\"D:<\":\"Disgust\",\n    u\"D:\":\"Sadness\",\n    u\"D8\":\"Great dismay\",\n    u\"D;\":\"Great dismay\",\n    u\"D=\":\"Great dismay\",\n    u\"DX\":\"Great dismay\",\n    u\":\u2011O\":\"Surprise\",\n    u\":O\":\"Surprise\",\n    u\":\u2011o\":\"Surprise\",\n    u\":o\":\"Surprise\",\n    u\":-0\":\"Shock\",\n    u\"8\u20110\":\"Yawn\",\n    u\">:O\":\"Yawn\",\n    u\":-\\*\":\"Kiss\",\n    u\":\\*\":\"Kiss\",\n    u\":X\":\"Kiss\",\n    u\";\u2011\\)\":\"Wink or smirk\",\n    u\";\\)\":\"Wink or smirk\",\n    u\"\\*-\\)\":\"Wink or smirk\",\n    u\"\\*\\)\":\"Wink or smirk\",\n    u\";\u2011\\]\":\"Wink or smirk\",\n    u\";\\]\":\"Wink or smirk\",\n    u\";\\^\\)\":\"Wink or smirk\",\n    u\":\u2011,\":\"Wink or smirk\",\n    u\";D\":\"Wink or smirk\",\n    u\":\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"X\u2011P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"XP\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u00de\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"d:\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"=p\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\">:P\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\":\u2011\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":-[.]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\">:\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=\/\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=[(\\\\\\)]\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\"=L\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":S\":\"Skeptical, annoyed, undecided, uneasy or hesitant\",\n    u\":\u2011\\|\":\"Straight face\",\n    u\":\\|\":\"Straight face\",\n    u\":$\":\"Embarrassed or blushing\",\n    u\":\u2011x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":x\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":#\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":\u2011&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\":&\":\"Sealed lips or wearing braces or tongue-tied\",\n    u\"O:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"O:\\)\":\"Angel, saint or innocent\",\n    u\"0:\u20113\":\"Angel, saint or innocent\",\n    u\"0:3\":\"Angel, saint or innocent\",\n    u\"0:\u2011\\)\":\"Angel, saint or innocent\",\n    u\"0:\\)\":\"Angel, saint or innocent\",\n    u\":\u2011b\":\"Tongue sticking out, cheeky, playful or blowing a raspberry\",\n    u\"0;\\^\\)\":\"Angel, saint or innocent\",\n    u\">:\u2011\\)\":\"Evil or devilish\",\n    u\">:\\)\":\"Evil or devilish\",\n    u\"\\}:\u2011\\)\":\"Evil or devilish\",\n    u\"\\}:\\)\":\"Evil or devilish\",\n    u\"3:\u2011\\)\":\"Evil or devilish\",\n    u\"3:\\)\":\"Evil or devilish\",\n    u\">;\\)\":\"Evil or devilish\",\n    u\"\\|;\u2011\\)\":\"Cool\",\n    u\"\\|\u2011O\":\"Bored\",\n    u\":\u2011J\":\"Tongue-in-cheek\",\n    u\"#\u2011\\)\":\"Party all night\",\n    u\"%\u2011\\)\":\"Drunk or confused\",\n    u\"%\\)\":\"Drunk or confused\",\n    u\":-###..\":\"Being sick\",\n    u\":###..\":\"Being sick\",\n    u\"<:\u2011\\|\":\"Dump\",\n    u\"\\(>_<\\)\":\"Troubled\",\n    u\"\\(>_<\\)>\":\"Troubled\",\n    u\"\\(';'\\)\":\"Baby\",\n    u\"\\(\\^\\^>``\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(\\^_\\^;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(~_~;\\) \\(\u30fb\\.\u30fb;\\)\":\"Nervous or Embarrassed or Troubled or Shy or Sweat drop\",\n    u\"\\(-_-\\)zzz\":\"Sleeping\",\n    u\"\\(\\^_-\\)\":\"Wink\",\n    u\"\\(\\(\\+_\\+\\)\\)\":\"Confused\",\n    u\"\\(\\+o\\+\\)\":\"Confused\",\n    u\"\\(o\\|o\\)\":\"Ultraman\",\n    u\"\\^_\\^\":\"Joyful\",\n    u\"\\(\\^_\\^\\)\/\":\"Joyful\",\n    u\"\\(\\^O\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(\\^o\\^\\)\uff0f\":\"Joyful\",\n    u\"\\(__\\)\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"_\\(\\._\\.\\)_\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<\\(_ _\\)>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"<m\\(__\\)m>\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(__\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"m\\(_ _\\)m\":\"Kowtow as a sign of respect, or dogeza for apology\",\n    u\"\\('_'\\)\":\"Sad or Crying\",\n    u\"\\(\/_;\\)\":\"Sad or Crying\",\n    u\"\\(T_T\\) \\(;_;\\)\":\"Sad or Crying\",\n    u\"\\(;_;\":\"Sad of Crying\",\n    u\"\\(;_:\\)\":\"Sad or Crying\",\n    u\"\\(;O;\\)\":\"Sad or Crying\",\n    u\"\\(:_;\\)\":\"Sad or Crying\",\n    u\"\\(ToT\\)\":\"Sad or Crying\",\n    u\";_;\":\"Sad or Crying\",\n    u\";-;\":\"Sad or Crying\",\n    u\";n;\":\"Sad or Crying\",\n    u\";;\":\"Sad or Crying\",\n    u\"Q\\.Q\":\"Sad or Crying\",\n    u\"T\\.T\":\"Sad or Crying\",\n    u\"QQ\":\"Sad or Crying\",\n    u\"Q_Q\":\"Sad or Crying\",\n    u\"\\(-\\.-\\)\":\"Shame\",\n    u\"\\(-_-\\)\":\"Shame\",\n    u\"\\(\u4e00\u4e00\\)\":\"Shame\",\n    u\"\\(\uff1b\u4e00_\u4e00\\)\":\"Shame\",\n    u\"\\(=_=\\)\":\"Tired\",\n    u\"\\(=\\^\\\u00b7\\^=\\)\":\"cat\",\n    u\"\\(=\\^\\\u00b7\\\u00b7\\^=\\)\":\"cat\",\n    u\"=_\\^=\t\":\"cat\",\n    u\"\\(\\.\\.\\)\":\"Looking down\",\n    u\"\\(\\._\\.\\)\":\"Looking down\",\n    u\"\\^m\\^\":\"Giggling with hand covering mouth\",\n    u\"\\(\\\u30fb\\\u30fb?\":\"Confusion\",\n    u\"\\(?_?\\)\":\"Confusion\",\n    u\">\\^_\\^<\":\"Normal Laugh\",\n    u\"<\\^!\\^>\":\"Normal Laugh\",\n    u\"\\^\/\\^\":\"Normal Laugh\",\n    u\"\\\uff08\\*\\^_\\^\\*\uff09\" :\"Normal Laugh\",\n    u\"\\(\\^<\\^\\) \\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\.\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\.\\)\":\"Normal Laugh\",\n    u\"\\(\\^_\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\^J\\^\\)\":\"Normal Laugh\",\n    u\"\\(\\*\\^\\.\\^\\*\\)\":\"Normal Laugh\",\n    u\"\\(\\^\u2014\\^\\\uff09\":\"Normal Laugh\",\n    u\"\\(#\\^\\.\\^#\\)\":\"Normal Laugh\",\n    u\"\\\uff08\\^\u2014\\^\\\uff09\":\"Waving\",\n    u\"\\(;_;\\)\/~~~\":\"Waving\",\n    u\"\\(\\^\\.\\^\\)\/~~~\":\"Waving\",\n    u\"\\(-_-\\)\/~~~ \\($\\\u00b7\\\u00b7\\)\/~~~\":\"Waving\",\n    u\"\\(T_T\\)\/~~~\":\"Waving\",\n    u\"\\(ToT\\)\/~~~\":\"Waving\",\n    u\"\\(\\*\\^0\\^\\*\\)\":\"Excited\",\n    u\"\\(\\*_\\*\\)\":\"Amazed\",\n    u\"\\(\\*_\\*;\":\"Amazed\",\n    u\"\\(\\+_\\+\\) \\(@_@\\)\":\"Amazed\",\n    u\"\\(\\*\\^\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\^_\\^\\)v\":\"Laughing,Cheerful\",\n    u\"\\(\\(d[-_-]b\\)\\)\":\"Headphones,Listening to music\",\n    u'\\(-\"-\\)':\"Worried\",\n    u\"\\(\u30fc\u30fc;\\)\":\"Worried\",\n    u\"\\(\\^0_0\\^\\)\":\"Eyeglasses\",\n    u\"\\(\\\uff3e\uff56\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\\uff3e\uff55\\\uff3e\\)\":\"Happy\",\n    u\"\\(\\^\\)o\\(\\^\\)\":\"Happy\",\n    u\"\\(\\^O\\^\\)\":\"Happy\",\n    u\"\\(\\^o\\^\\)\":\"Happy\",\n    u\"\\)\\^o\\^\\(\":\"Happy\",\n    u\":O o_O\":\"Surprised\",\n    u\"o_0\":\"Surprised\",\n    u\"o\\.O\":\"Surpised\",\n    u\"\\(o\\.o\\)\":\"Surprised\",\n    u\"oO\":\"Surprised\",\n    u\"\\(\\*\uffe3m\uffe3\\)\":\"Dissatisfied\",\n    u\"\\(\u2018A`\\)\":\"Snubbed or Deflated\"\n}","3353ff95":"def remove_emoji(string):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', string)\n\ndef remove_emoticons(text):\n    emoticon_pattern = re.compile(u'(' + u'|'.join(k for k in EMOTICONS) + u')')\n    return emoticon_pattern.sub(r'', text)\n\nPUNCT_TO_REMOVE = string.punctuation\ndef remove_punctuation(text):\n    \"\"\"custom function to remove the punctuation\"\"\"\n    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))","7f844d47":"#HTML removal\ntrain['text_clean'] = train['text'].apply(lambda x:remove_html(x))\n\n#removing url tags\ntrain['text_clean'] = train['text'].apply(lambda x:remove_urls(x))\n\n#removing emoticons\ntrain['text_clean'] = train['text'].apply(lambda x:remove_emoticons(x))\n\n#removing emojis\ntrain['text_clean'] = train['text'].apply(lambda x:remove_emoji(x))\n\n#removing punctuation\ntrain['text_clean'] = train['text'].apply(lambda x:remove_punctuation(x))","bc8903db":"def find_hashtags(tweet):\n    return \" \".join([match.group(0)[1:] for match in re.finditer(r\"#\\w+\", tweet)]) or 'no'\n\ndef process_text(df):\n        \n    df['hashtags'] = df['text'].apply(lambda x: find_hashtags(x))\n    \n    return df\n    \ntrain = process_text(train)","7af525e9":"tweet_words= ''\n\n# iterate through the csv file \nfor val in train.text_clean: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    tweet_words += \" \".join(tokens)+\" \"\n\n    \nplt.figure(figsize=(10,6))\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(tweet_words) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \n  \nplt.show()","3ca27e85":"disastertweet_words= ''\n\n# iterate through the csv file \nfor val in train[train[\"target\"]==1].text_clean: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    disastertweet_words += \" \".join(tokens)+\" \"\n    \nplt.figure(figsize=(10,6))\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(disastertweet_words) \nplt.imshow(wordcloud) \nplt.title(\"Word Cloud of tweets if real disaster\")\nplt.axis(\"off\") \n  \nplt.show() ","5dfe87ea":"nodisastertweet_words= ''\n\n# iterate through the csv file \nfor val in train[train[\"target\"]==0].text_clean: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    nodisastertweet_words += \" \".join(tokens)+\" \"\n    \nplt.figure(figsize=(10,6))\nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(nodisastertweet_words) \nplt.imshow(wordcloud) \nplt.title(\"Word Cloud of tweets if no disaster\")\nplt.axis(\"off\") \n  \nplt.show() ","74034643":"# Import the necessary scikit learn modules\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report","54dc826b":"# Initialize a CountVectorizer object: count_vectorizer\n#tfidf_vectorizer_text = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), min_df=5)\ntfidf_vectorizer_text_clean = TfidfVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), min_df=5)\ncount_vectorizer_hashtags = CountVectorizer(stop_words=ENGLISH_STOP_WORDS, ngram_range=(1, 2), min_df=5)\n\n\n# Transform the training data using only the 'text' column values: count_train \n#tfidf_text = tfidf_vectorizer_text.fit_transform(train.text)\ntfidf_text_clean = tfidf_vectorizer_text_clean.fit_transform(train.text_clean)\ncount_hashtags = count_vectorizer_hashtags.fit_transform(train.hashtags)\n\n#train_text = pd.DataFrame(tfidf_text.toarray(), columns=tfidf_vectorizer_text.get_feature_names())\ntrain_text_clean = pd.DataFrame(tfidf_text_clean.toarray(), columns=tfidf_vectorizer_text_clean.get_feature_names())\ntrain_hashtags = pd.DataFrame(count_hashtags.toarray(), columns=count_vectorizer_hashtags.get_feature_names())\n\nprint(train_text_clean.shape, train_hashtags.shape)","f6c45b7d":"# Joining the dataframes together\n\n#train = train.join(train_text, rsuffix='_count_text')\ntrain = train.join(train_text_clean, rsuffix='_count_text_clean')\ntrain = train.join(train_hashtags, rsuffix='_count_hashtags')\n\nprint (train.shape)","f1b41b58":"features_to_drop = ['id', 'keyword','location','text','text_clean', 'hashtags','target_count_text_clean']\n\nfinal_df = train.drop(columns = features_to_drop, axis=1)\nfinal_df.shape","6a9350cb":"y= final_df['target']\nX= final_df.drop('target', axis=1)","866db214":"# Create training and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, stratify=y)\n\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","6b3bf9d8":"logreg= LogisticRegression(solver='liblinear', penalty='l2')\n\nlogreg.fit(X_train, y_train)\n\ny_predicted= logreg.predict(X_test)\n\n# Print accuracy score and confusion matrix on test set\nprint('Accuracy on the test set: ', accuracy_score(y_test, y_predicted))\nprint(confusion_matrix(y_test, y_predicted)\/len(y_test))","0f14c631":"#accuracy of training and test set\nprint(\"Training set accuracy is:\", logreg.score(X_train, y_train))\nprint(\"Test set accuracy is:\", logreg.score(X_test, y_test))","5e3de0e7":"print(classification_report(y_test,y_predicted))","95690ab8":"# Cross validation\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.metrics import make_scorer\n\nscorer= make_scorer(accuracy_score)\n\ncv = ShuffleSplit(n_splits=5, test_size=0.2, random_state=123)\ncv_score = cross_val_score(logreg, X_train, y_train, cv=cv, scoring=scorer)\nprint('Cross validation accuracy score: %.3f' %np.mean(cv_score))","d7f8eaa9":"## Feature selection\nfrom sklearn.feature_selection import RFECV\n\nsteps = 20\nn_features = len(X_train.columns)\nX_range = np.arange(n_features - (int(n_features\/steps)) * steps, n_features+1, steps)\n\nrfecv = RFECV(estimator=logreg, step=steps, cv=cv, scoring=scorer)\n\nrfecv.fit(X_train, y_train)","586d5367":"print ('Optimal no. of features: %d' % np.insert(X_range, 0, 1)[np.argmax(rfecv.grid_scores_)])","737acb44":"selected_features = X_train.columns[rfecv.ranking_ == 1]\nX_train2 = X_train[selected_features]\nX_test2 = X_test[selected_features]","2af2cd24":"print(classification_report(y_test,y_predicted))","fe8c4962":"logreg.fit(X_train2, y_train)\ncv2 = ShuffleSplit(n_splits=5, test_size=0.2, random_state=99)\ncv_score2 = cross_val_score(logreg, X_train2, y_train, cv=cv2, scoring=scorer)\nprint('Cross validation accuracy score: %.3f' %np.mean(cv_score2))","aa9fe2b9":"from sklearn.model_selection import GridSearchCV\n\ngrid={\"C\":np.logspace(-2,2,5), \"penalty\":[\"l1\",\"l2\"]}\nsearcher_cv = GridSearchCV(logreg, grid, cv=cv2, scoring = scorer)\nsearcher_cv.fit(X_train2, y_train)\n\nprint(\"Best parameter: \", searcher_cv.best_params_)\nprint(\"accuracy score: %.3f\" %searcher_cv.best_score_)","37965b8a":"#accuracy of training and test set\nprint(\"Training set accuracy is:\", searcher_cv.score(X_train2, y_train))\nprint(\"Test set accuracy is:\", searcher_cv.score(X_test2, y_test))","bad950f5":"# Import the necessary modules\nfrom sklearn.naive_bayes import MultinomialNB\n\n# Instantiate a Multinomial Naive Bayes classifier: nb_classifier\nnb_classifier = MultinomialNB()\n\n# Fit the classifier to the training data\nnb_classifier.fit(X_train2, y_train)","9667f8f5":"# Create the predicted tags: pred\npred = nb_classifier.predict(X_test2)\n\n# Calculate the accuracy score: score\nscore = accuracy_score(y_test, pred)\nprint(score)","128ef328":"#accuracy of training and test set\nprint(\"Training set accuracy is:\", nb_classifier.score(X_train2, y_train))\nprint(\"Test set accuracy is:\", nb_classifier.score(X_test2, y_test))","b04b4acd":"#import and read test dataset\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\ntest.head()","8e5ee495":"#convert the text column to string\ntest['text']= test['text'].astype('str')\n\ntest['text_clean'] = test['text'].apply(lambda x:remove_html(x))\ntest['text_clean'] = test['text'].apply(lambda x:remove_urls(x))\ntest['text_clean'] = test['text'].apply(lambda x:remove_emoticons(x))\ntest['text_clean'] = test['text'].apply(lambda x:remove_emoji(x))\ntest['text_clean'] = test['text'].apply(lambda x:remove_punctuation(x))\n\ntest = process_text(test)","025567ec":"#tfidf_text2 = tfidf_vectorizer_text.transform(test.text)\ntfidf_text2_clean = tfidf_vectorizer_text_clean.transform(test.text_clean)\ncount_hashtags2 = count_vectorizer_hashtags.transform(test.hashtags)\n\n#test_text = pd.DataFrame(tfidf_text2.toarray(), columns=tfidf_vectorizer_text.get_feature_names())\ntest_text_clean = pd.DataFrame(tfidf_text2_clean.toarray(), columns=tfidf_vectorizer_text_clean.get_feature_names())\ntest_hashtags = pd.DataFrame(count_hashtags2.toarray(), columns=count_vectorizer_hashtags.get_feature_names())\n\nprint(test_text_clean.shape, test_hashtags.shape)","8aa61548":"# Joining the dataframes together\n\n#test = test.join(test_text, rsuffix='_count_text')\ntest = test.join(test_text_clean, rsuffix='_count_text_clean')\ntest = test.join(test_hashtags, rsuffix='_count_hashtags')\n\nprint(test.shape)","58028a78":"features_to_drop = ['id', 'keyword','location','text','text_clean', 'hashtags']\n\ntest_df = test.drop(columns = features_to_drop, axis=1)\n\n#select optimal features \nfinal_df= test_df[selected_features]\nfinal_df.shape","c129242c":"#predict the target label for test set\ntest_predictions = searcher_cv.predict(final_df)\ntest_predictions","e15fb0ef":"submission = pd.DataFrame()\nsubmission['id'] = test['id']\nsubmission['target'] = test_predictions\n\nsubmission.to_csv(\"submission.csv\", index=False)\n\nsubmission.tail()","ad351c43":"### Building a Logistic Regression classifier","1c24faba":"Location columns contains more missing values, filling them will add bias to our model. We will ignore the column\nKeyword will not be informative as we will be extracting tokens from main text column. Hence, keyword column is nothing but duplicate","fc24afb8":"### Predict the test Dataset","00b111cb":"https, co are resultant of hyperlinks displayed bigger and bolder in wordcloud. These words cannot be part of vocabulary tokens","eb0b5fba":"We will Train our training and test set with Naive Bayes model","e2e530d1":"Notice Storm, fire, flood, death are some words from Real disaster tweets","d27d14a5":"### Model Evaluation\n\n        Cross validation with shuffle split\n        Feature selections\n        Grid search for hyperparameters","004ed1d2":"### Text Preprocessing","160004be":"Training Accuracy is greater than Test Accuracy which results in Underfitting","b06d07e6":"Target Label is imbalanced","31619267":"#### WordCloud on Text","b7822642":"Makesure to repeat similar transformation we did in Train Dataset","0a0021b0":"## Naive Bayes Model","525a6003":"Model underfits on Test set","04c3b38d":"Notice we dont have Target Label which we need to predict based on our Trained Logistic Regression model","320e7876":"We then pick up the 2888 selected features to do Grid Search CV to find optimal hyperparameters","c28bdd37":"#### WordCloud for Diaster Tweets","dd101c1e":"Notice words like new, people, time, day are part of non disaster tweets","0c4a960c":"Word Cloud on Cleaned Text","c9c0736a":"#### WordCloud for NO- diaster Tweets","ad725f88":"##### A little improvement from our earlier model","262741fa":"Though Model underfits on Test Set. The difference is less compared to LogisticRegression model"}}