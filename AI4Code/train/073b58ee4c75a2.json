{"cell_type":{"d14ac1be":"code","838dab65":"code","9010e99d":"code","227a2cc6":"markdown"},"source":{"d14ac1be":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import LinearSVC\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n","838dab65":"def postprocess_separate(submission_df, test_df=None, pure_df=None):\n    \"\"\"Update submission_df so that the predictions for the two sides of the hyperplane don't overlap.\n    \n    Parameters\n    ----------\n    submission_df : pandas DataFrame with columns 'id' and 'target'\n    test_df : the competition's test data\n    pure_df : the competition's original training data\n    \n    From https:\/\/www.kaggle.com\/ambrosm\/tpsnov21-007-postprocessing\n    \"\"\"\n    if pure_df is None: pure_df = pd.read_csv('..\/input\/november21\/train.csv')\n    if pure_df.shape != (600000, 102): raise ValueError(\"pure_df has the wrong shape\")\n    if test_df is None: test_df = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\n    if test_df.shape[0] != submission_df.shape[0] or test_df.shape[1] != 101: raise ValueError(\"test_df has the wrong shape\")\n\n    # Find the separating hyperplane for pure_df, step 1\n    # Use an SVM with almost no regularization\n    model1 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model1.fit(pure_df.drop(columns=['id', 'target']), pure_df.target)\n    pure_pred = model1.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 1 599999\n    # model1 is not perfect: it predicts the wrong class for 1 of 600000 samples\n\n    # Find the separating hyperplane for pure_df, step 2\n    # Fit a second SVM to a subset of the points which contains the support vectors\n    pure_pred = model1.decision_function(pure_df.drop(columns=['id', 'target']))\n    subset_df = pure_df[(pure_pred > -5) & (pure_pred < 0.9)]\n    model2 = make_pipeline(StandardScaler(), LinearSVC(C=1e5, tol=1e-7, penalty='l2', dual=False, max_iter=2000, random_state=1))\n    model2.fit(subset_df.drop(columns=['id', 'target']), subset_df.target)\n    pure_pred = model2.predict(pure_df.drop(columns=['id', 'target']))\n    print((pure_pred != pure_df.target).sum(), (pure_pred == pure_df.target).sum()) # 0 600000\n    # model2 is perfect: it predicts the correct class for all 600000 training samples\n    \n    pure_test_pred = model2.predict(test_df.drop(columns=['id', 'target'], errors='ignore'))\n    lmax, rmin = submission_df[pure_test_pred == 0].target.max(), submission_df[pure_test_pred == 1].target.min()\n    if lmax < rmin:\n        print(\"There is no overlap. No postprocessing needed.\")\n        return\n    # There is overlap. Remove this overlap\n    submission_df.loc[pure_test_pred == 0, 'target'] -= lmax + 1\n    submission_df.loc[pure_test_pred == 1, 'target'] -= rmin - 1\n    print(submission_df[pure_test_pred == 0].target.min(), submission_df[pure_test_pred == 0].target.max(),\n          submission_df[pure_test_pred == 1].target.min(), submission_df[pure_test_pred == 1].target.max())\n","9010e99d":"sub = pd.read_csv('..\/input\/simple-nn-tps-nov-21\/submission.csv')\npostprocess_separate(sub)\nsub.to_csv(\"submission_postprocessed.csv\", index=False)\nsub.head()","227a2cc6":"# Postprocessing\n\nA few days ago, the [original train.csv of this competition](https:\/\/www.kaggle.com\/criskiev\/november21) was published. It shows the labels before 25 % of them were flipped. We can use these data to postprocess the output of neural networks and improve the score.\n\nThe original training file has the nice property that the two classes can be separated perfectly (i.e. with 100 % accuracy) by a single hyperplane. This same hyperplane has a meaning in the updated train.csv as well: On one side of the hyperplane, about 75 % of the samples are class 0, one the other side of the hyperplane, 75 % are class 1.\n\nIf we want to optimize a neural network's predictions for the highest possible auc score, we have to ensure that the network outputs low predictions for one side of the hyperplane and high predictions for the other side of the hyperplane, and that there isn't any overlap between the two sides.\n\nThe `postprocess_separate()` function in this notebook reads a submission file, checks if the predictions overlap and removes the overlap. It can be applied to the output of any network; for demonstration purposes we apply it to @adityasharma01's [simple NN](https:\/\/www.kaggle.com\/adityasharma01\/simple-nn-tps-nov-21).\n"}}