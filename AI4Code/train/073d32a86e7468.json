{"cell_type":{"b1451d00":"code","9544c127":"code","300d184d":"code","db3f4383":"code","4bb74dd8":"code","e6063d70":"code","cda44711":"code","dadeaf8f":"code","f147da50":"code","a930731f":"code","580e486e":"code","f780500d":"code","2d26549f":"code","25393dcd":"code","ddf1eb3f":"code","7b6cc3b8":"code","5469db07":"code","2e5c9b65":"code","5e7a1691":"code","c79e8a55":"code","93cef72f":"code","3ab4e082":"code","27664095":"code","7f2ab37e":"code","db4f28fd":"code","794cdca6":"code","31951f9b":"code","badc2812":"code","51229bb6":"code","82b7b898":"code","558b75ad":"code","00467919":"code","0d2f967e":"code","62fce0b0":"code","87cb930e":"code","5c826068":"code","ac50cae8":"code","3c5fd818":"code","975a3fbe":"markdown","c1607f2a":"markdown","6741320c":"markdown","83ecc8e6":"markdown","299214db":"markdown","29363032":"markdown","3effe6ed":"markdown","5fb1c70d":"markdown","e0da5e30":"markdown","bc6545f7":"markdown"},"source":{"b1451d00":"from pathlib import Path\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport random\n\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom math import sqrt\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nimport pickle\n\nimport torch\nfrom torch.utils.data import random_split,DataLoader, Dataset\nfrom torchtext.data import Field, TabularDataset, BucketIterator, Iterator\n\nfrom transformers import (BertTokenizer, BertModel, AdamW,\n                          get_linear_schedule_with_warmup,\n                          RobertaTokenizerFast,RobertaModel,\n                          RobertaConfig,PreTrainedModel,\n                          get_constant_schedule_with_warmup,\n                          AutoModelForSequenceClassification,\n                          AutoModel,AutoConfig,\n                          AutoTokenizer,get_cosine_schedule_with_warmup\n                         )\nimport warnings\nwarnings.simplefilter('ignore')\n","9544c127":"def set_seed(seed = 0):\n    '''Sets the seed of the entire notebook so results are the same every time we run.\n    This is for REPRODUCIBILITY.'''\n    np.random.seed(seed)\n    random_state = np.random.RandomState(seed)\n    random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    return random_state\n\nseed = 82\nrandom_state = set_seed(seed)","300d184d":"data_dir = Path('..\/input\/commonlitreadabilityprize')\ntrain_file = data_dir \/ 'train.csv'\ntest_file = data_dir \/ 'test.csv'\nsample_file = data_dir \/ 'sample_submission.csv'\nsubmission_file = 'submission.csv'\noutput_path = \".\/\"","db3f4383":"os.makedirs('.\/model_preds')\nos.makedirs('.\/models')\nos.makedirs('.\/test')","4bb74dd8":"# Set random seed and set device to GPU.\n#torch.manual_seed(17)\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelse:\n    device = torch.device('cpu')\n\nprint(device)","e6063d70":"df = pd.read_csv(train_file)\ndf.head()","cda44711":"k_folds = 5","dadeaf8f":"from sklearn import model_selection\n\ndf.loc[:,\"kfold\"] = -1\ndf = df.sample(frac=1).reset_index(drop=True)\n\ny = df.target.values\nskf = model_selection.KFold(n_splits=k_folds)\n\nfor f, (t_, v_) in enumerate(skf.split(X=df, y=y)):\n    df.loc[v_, \"kfold\"]=f\n    \ndf.to_csv(\".\/train_folds.csv\", index=False)","f147da50":"from nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\ndef remove_stopwords(text):\n    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])\n\n\", \".join(stopwords.words('english'))\nSTOPWORDS = set(stopwords.words('english'))\n\ndef preprop(df):\n    df[\"excerpt\"]=df.excerpt.str.lower()\n    \n    ps = PorterStemmer()\n    df[\"excerpt\"] = df.excerpt.apply(ps.stem)\n    \n    wnl = WordNetLemmatizer()\n    df[\"excerpt\"] = df.excerpt.apply(wnl.lemmatize)\n\n    df[\"excerpt\"] = df.excerpt.apply(lambda text: remove_stopwords(text))\n    \n    return df","a930731f":"#train\ndf = pd.read_csv(\".\/train_folds.csv\")\ndf=preprop(df)\n\ndf.to_csv(\".\/train_folds_preprop.csv\", index=False)","580e486e":"#linear model with TfidfVectorizer on Excerpt\ndef run_Tfidf(fold):\n    df = pd.read_csv(\".\/train_folds.csv\") #better results\n    df.excerpt = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = linear_model.LinearRegression()\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n        \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = '.\/models\/clf_lr_Tfidf.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"lr_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"lr_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_Tfidf(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/lr_Tfidf_excerpt.csv\", index=False)","f780500d":"#linear model with CountVectorizer on Excerpt\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom math import sqrt\nfrom sklearn.feature_extraction.text import CountVectorizer\n\ndef run_lr_cnt(fold):\n    #df = pd.read_csv(\".\/train_folds.csv\")\n    df = pd.read_csv(\".\/train_folds_preprop.csv\") #Better results\n    df.excerpt = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = CountVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = linear_model.LinearRegression()\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = '.\/models\/clf_lr_cnt.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"lr_cnt_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"lr_cnt_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_lr_cnt(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/lr_cnt_excerpt.csv\", index=False)","2d26549f":"#rf_svd\nfrom sklearn import decomposition\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef run_rf_svd(fold):\n    #df = pd.read_csv(\".\/train_folds.csv\")\n    df = pd.read_csv(\".\/train_folds_preprop.csv\")\n    df.review = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    svd = decomposition.TruncatedSVD(n_components=120)\n    svd.fit(Xtrain)\n    xtrain_svd = svd.transform(Xtrain)\n    xvalid_svd = svd.transform(Xvalid)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=-1)\n    clf.fit(xtrain_svd, ytrain)\n    pred = clf.predict(xvalid_svd)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    if fold == (k_folds-1):\n        filename = '.\/models\/clf_rfr.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"rf_svd_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"rf_svd_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_rf_svd(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/rf_svd_Tfidf_excerpt.csv\", index=False)","25393dcd":"#xgboost\nfrom sklearn import decomposition\nfrom sklearn import ensemble\nfrom sklearn import metrics\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom xgboost import XGBRegressor\n\ndef run_xgboost_Tfidf(fold):\n    df = pd.read_csv(\".\/train_folds.csv\") #better results\n    #df = pd.read_csv(\".\/train_folds_preprop.csv\")\n    df.review = df.excerpt.apply(str)\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    tfv = TfidfVectorizer()\n    tfv.fit(df_train.excerpt.values)\n    \n    Xtrain = tfv.transform(df_train.excerpt.values)\n    Xvalid = tfv.transform(df_valid.excerpt.values)\n    \n    svd = decomposition.TruncatedSVD(n_components=120)\n    svd.fit(Xtrain)\n    xtrain_svd = svd.transform(Xtrain)\n    xvalid_svd = svd.transform(Xvalid)\n    \n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = XGBRegressor(n_estimators=500, n_jobs=-1, learning_rate=0.05)\n    clf.fit(xtrain_svd, ytrain)\n    pred = clf.predict(xvalid_svd)  \n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n        \n    if fold == (k_folds-1):\n        filename = '.\/models\/clf_xgboost.sav'\n        pickle.dump(clf, open(filename, 'wb'))\n        print(\"model saved\")\n    \n    df_valid.loc[:,\"xgboost_Tfidf_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"xgboost_Tfidf_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_xgboost_Tfidf(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/xgboost_Tfidf_excerpt.csv\", index=False)","ddf1eb3f":"EMBEDDING_FILE = '..\/input\/glove6b\/glove.6B.100d.txt'\n\nembeddings_dict = {}\nfor line in open(EMBEDDING_FILE):\n    values = line.split()\n    word = values[0]\n    # print(word)\n    vector = np.asarray(values[1:], \"float32\")\n    # print(vector)\n    embeddings_dict[word] = vector","7b6cc3b8":"def get_feature_vectors(sentence):\n    words = sentence.split()\n    feature_vec = np.zeros((100,),dtype=\"float32\")\n    i=0\n    for word in words:\n        try:\n            feature_vec = np.add(feature_vec, embeddings_dict.get(word))\n        except:\n            i = i + 1\n    if len(words) > 0:\n        feature_vec = np.divide(feature_vec, len(words)- i)\n    return feature_vec","5469db07":"#randomforest with Glove on excerpt\ndef run_rf_glove(fold):\n    #df = pd.read_csv(\".\/train_folds.csv\")\n    df = pd.read_csv(\".\/train_folds_preprop.csv\") #Better results\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    Xtrain_glove = np.array([get_feature_vectors(sentence) for sentence in df_train.excerpt.values])\n    Xvalid_glove = np.array([get_feature_vectors(sentence) for sentence in df_valid.excerpt.values])\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n    clf = ensemble.RandomForestRegressor(n_estimators=500, n_jobs=-1)\n    clf.fit(Xtrain_glove, ytrain)\n    pred = clf.predict(Xvalid_glove)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"rf_glove_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"rf_glove_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_rf_glove(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/rf_glove_excerpt.csv\", index=False)","2e5c9b65":"#xgboost with Glove on excerpt\ndef run_XGBR(fold):\n    #df = pd.read_csv(\".\/train_folds.csv\")\n    df = pd.read_csv(\".\/train_folds_preprop.csv\") #Better results\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    Xtrain_glove = np.array([get_feature_vectors(sentence) for sentence in df_train.excerpt.values])\n    Xvalid_glove = np.array([get_feature_vectors(sentence) for sentence in df_valid.excerpt.values])\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n\n    clf = XGBRegressor(n_estimators=600,\n                       objective = 'reg:squarederror',\n                       eval_metric = 'rmse',\n                       n_jobs=-1,\n                       subsample = 1.0,\n                       learning_rate=0.05,\n                       max_depth = 5,\n                       early_stopping_rounds = 10,\n                       gamma = 1,\n                       colsample_bytree=0.9,\n                       verbosity = 0,\n                       random_state=seed\n                      )\n    clf.fit(Xtrain_glove, ytrain)\n    pred = clf.predict(Xvalid_glove)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"XGBR_glove_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"XGBR_glove_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_XGBR(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/xgboost_glove_excerpt.csv\", index=False)","5e7a1691":"from tqdm import tqdm\n\nclass CLRPDataset(torch.nn.Module):\n    def __init__(self, df, tokenizer, max_len=256):\n        self.excerpt = df['excerpt'].to_numpy()\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n    \n    def __getitem__(self,idx):\n        encode = self.tokenizer(self.excerpt[idx],\n                                return_tensors='pt',\n                                max_length=self.max_len,\n                                padding='max_length',\n                                truncation=True)\n        return encode\n    \n    def __len__(self):\n        return len(self.excerpt)\n    \n\ndef get_embeddings(df, path, plot_losses=True, verbose=True):\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"{device} is used\")\n            \n    MODEL_PATH = path\n    model = AutoModel.from_pretrained(MODEL_PATH,output_hidden_states = True,)\n    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n    model.to(device)\n    model.eval()\n    \n    ds = CLRPDataset(df, tokenizer, 256)\n    dl = DataLoader(ds,\n                    batch_size=128,\n                    shuffle=False,\n                    num_workers = 4,\n                    pin_memory=True,\n                    drop_last=False)\n    \n    embeddings = list()\n    with torch.no_grad():\n        for i, inputs in tqdm(enumerate(dl)):\n            inputs = {key:val.reshape(val.shape[0], -1).to(device) for key, val in inputs.items()}\n            outputs = model(**inputs)\n#https:\/\/mccormickml.com\/2019\/05\/14\/BERT-word-embeddings-tutorial\/#31-running-bert-on-our-text\n            cat = torch.cat(tuple([outputs[2][i] for i in [-4, -3, -2, -1]]), dim=-1)\n            outputs = cat[:, 0, :].detach().cpu().numpy()\n            embeddings.extend(outputs)\n            \n    del model\n    \n    return np.array(embeddings)","c79e8a55":"# source: https:\/\/www.kaggle.com\/maunish\/clrp-roberta-lgbm\n#https:\/\/www.kaggle.com\/c\/commonlitreadabilityprize\/discussion\/237795\n#https:\/\/www.kaggle.com\/abhishek\/modelf1\n\nMODEL_NAME = Path('..\/input\/modelf1')","93cef72f":"df = pd.read_csv(\".\/train_folds.csv\")\ntrain_embeddings =  get_embeddings(df,MODEL_NAME)","3ab4e082":"#import xgboost as xgb\n#xgboost with roberta on excerpt\ndef run_XGBR_Roberta(fold):\n    df = pd.read_csv(\".\/train_folds.csv\")\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    #df = pd.read_csv(\".\/train_folds_preprop.csv\")\n    train_idx = df[df.kfold != fold].index\n    valid_idx = df[df.kfold == fold].index\n    #target = df['target'].to_numpy()\n    \n    Xtrain = train_embeddings[train_idx]\n    Xvalid = train_embeddings[valid_idx]\n    ytrain = df_train.target.values\n    yvalid = df_valid.target.values\n    \n\n    clf = XGBRegressor(n_estimators=500,\n                       objective = 'reg:squarederror',\n                       eval_metric = 'rmse',\n                       n_jobs=-1,\n                       subsample = 0.7,\n                       learning_rate=0.05,\n                       max_depth = 4,\n                       early_stopping_rounds = 15,\n                       gamma = 1,\n                       colsample_bytree=1,\n                       verbosity = 0,\n                       random_state=seed,\n                       tree_method='gpu_hist'\n                      )\n    clf.fit(Xtrain, ytrain)\n    pred = clf.predict(Xvalid)\n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    filename = f\".\/models\/XGBR_roberta_fold_{fold}.sav\"\n    pickle.dump(clf, open(filename, 'wb'))\n    print(\"model saved\")\n    \n    df_valid.loc[:,\"XGBR_roberta_pred\"] = pred\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"XGBR_roberta_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_XGBR_Roberta(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/xgboost_roberta_excerpt.csv\", index=False)","27664095":"# SEARCH BEST PARAMS XGBOOST\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n#import warnings\n\ndef rmse(predict, actual):\n    predict = np.array(predict)\n    actual = np.array(actual)\n    distance = predict - actual\n    square_distance = distance ** 2\n    mean_square_distance = square_distance.mean()\n    score = np.sqrt(mean_square_distance)\n\n    return score\n\nrmse_score = make_scorer(rmse, greater_is_better = False)\n\ny_train = df.target.values\n\n\nx_fit,x_test,y_fit,y_test = train_test_split(train_embeddings, y_train, train_size =0.15, \n                            random_state=seed)\nclf = XGBRegressor(\n        eval_metric = 'rmse',\n        #nthread = 4,\n        eta = 0.1,\n        n_estimators = 500,\n        max_depth = 5,\n        subsample = 0.5,\n        gamma = 1,\n        colsample_bytree = 1.0,\n        #silent = 1,\n        verbosity = 0,\n        random_state=seed,\n        tree_method='gpu_hist'\n        )\nparameters = {\n    'n_estimators': [200, 300, 400, 500],\n    'eta': [0.01, 0.05, 0.1],\n    'early_stopping_rounds':[10,15,20],\n    'gamma':[0, 1, 10],\n    'max_depth': [3, 4, 5],\n    'subsample': [0.7, 0.8, 0.9, 1.0],\n    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n}\n\nclf1 = RandomizedSearchCV(clf, parameters, n_jobs=-1, scoring=rmse_score, cv=5, return_train_score=True)\nclf1.fit(x_fit, y_fit)\nprint('Best Params: \\n', clf1.best_params_ )\n\nResult = pd.DataFrame(clf1.cv_results_)\nResult","7f2ab37e":"MODEL_NAME = Path('..\/input\/modelf1')\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, model_max_length=256) ","db4f28fd":"class Data(Dataset):\n    def __init__(self, data):\n        super().__init__()\n        self.data = data\n\n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, idx):       \n        excerpt = self.data.excerpt[idx]\n        target = self.data.target[idx]\n        return excerpt, target","794cdca6":"class Roberta(PreTrainedModel):\n   def __init__(self, conf):\n       super(Roberta, self).__init__(conf)\n       self.roberta = AutoModel.from_pretrained(MODEL_NAME, config=conf)\n    \n       self.W = torch.nn.Linear(4*self.roberta.config.hidden_size, self.roberta.config.hidden_size) \n       self.drop_out = torch.nn.Dropout(0.4)\n       self.V = torch.nn.Linear(self.roberta.config.hidden_size, 1)\n    \n       self.linear = torch.nn.Linear(4*self.roberta.config.hidden_size, 1)\n\n   def forward(self, input_ids, attention_mask):\n       out = self.roberta(input_ids=input_ids, \n                             attention_mask=attention_mask\n                            )\n       cat = torch.cat(tuple([out[2][i] for i in [-4, -3, -2, -1]]), dim=-1)\n       att = torch.tanh(self.W(cat))\n       score = self.V(att)\n       attention_weights = torch.softmax(score, dim=1)\n       context_vector = attention_weights * cat #* out[2]\n       context_vector = torch.sum(context_vector, dim=1)\n       out = self.drop_out(context_vector)\n       out = self.linear(out)\n        \n       return out","31951f9b":"# Training Function\n\ndef train(model,\n          optimizer,\n          train_iter,\n          valid_iter,\n          loss_fct,\n          fold,\n          valid_period,\n          num_epochs = 5,\n          scheduler = None,\n          output_path = output_path):\n    \n    # Initialize losses and loss histories\n    train_loss = 0.0\n    valid_loss = 0.0\n\n    train_loss_list = []\n    valid_loss_list = []\n\n    best_valid_loss = float('Inf')\n    \n    global_step = 0\n    global_steps_list = []\n    \n    model.train()\n        # Train loop\n    for epoch in range(num_epochs):\n        for (excerpts, target) in train_iter:\n            \n            batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n            input_ids = batch['input_ids']\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask']\n            attention_mask = attention_mask.to(device, dtype=torch.long)\n            \n            target=torch.tensor(target).to(device, dtype=torch.float)\n\n            logits = model(input_ids=input_ids,  \n                           attention_mask=attention_mask)\n            loss = loss_fct(torch.squeeze(logits), target)\n            loss.backward()\n            \n            # Optimizer and scheduler step\n            optimizer.step()    \n            scheduler.step()\n            \n            optimizer.zero_grad()\n\n            # Update train loss and global step\n            train_loss += loss.item()\n            global_step += 1\n                        # Validation loop. Save progress and evaluate model performance.\n            if global_step % valid_period == 0:\n                all_preds=[]\n                model.eval()\n                with torch.no_grad():\n                    for (excerpts, target) in valid_iter:\n                        batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=True)\n                        input_ids = batch['input_ids']\n                        input_ids = input_ids.to(device, dtype=torch.long)\n                        attention_mask = batch['attention_mask']\n                        attention_mask = attention_mask.to(device, dtype=torch.long)\n\n                        target=torch.tensor(target).to(device, dtype=torch.float)\n\n                        logits = model(input_ids=input_ids, \n                                       attention_mask=attention_mask)\n\n                        preds = torch.squeeze(logits)\n                        loss = loss_fct(preds, target)\n                        valid_loss += loss.item()\n                        all_preds.append(preds.detach().cpu())\n\n                # Store train and validation loss history\n                train_loss = train_loss \/ valid_period\n                valid_loss = valid_loss \/ len(valid_iter)\n                train_loss_list.append(train_loss)\n                valid_loss_list.append(valid_loss)\n                global_steps_list.append(global_step)\n\n                # print summary\n                print('Epoch [{}\/{}], global step [{}\/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n                              train_loss, valid_loss))\n                                \n                # checkpoint\n                if best_valid_loss > valid_loss:\n                    best_valid_loss = valid_loss\n                    best_pred = np.array(all_preds)\n                    save_path = f'.\/models\/RobertaBase_model-fold-{fold}.pth'\n                    torch.save(model.state_dict(), save_path)\n                        \n                train_loss = 0.0                \n                valid_loss = 0.0\n\n                model.train()\n    return best_pred\n    print('Training done!')\n\n\n    ","badc2812":"model_config = RobertaConfig.from_pretrained(MODEL_NAME)\nmodel_config.output_hidden_states = True","51229bb6":"def test(model, test_loader):\n    \n    model.eval()\n    all_preds=[]\n    with torch.no_grad():\n        for (excerpts, _) in test_loader:\n            batch = tokenizer(list(excerpts), truncation=True, padding=True, return_tensors='pt', add_special_tokens=False)\n            input_ids = batch['input_ids']\n            input_ids = input_ids.to(device, dtype=torch.long)\n            attention_mask = batch['attention_mask']\n            attention_mask = attention_mask.to(device, dtype=torch.float)\n\n            output = model(input_ids, attention_mask)\n            preds = torch.squeeze(output)\n            all_preds.append(preds.detach().cpu())\n\n    torch.cuda.empty_cache()\n    return np.array(all_preds)","82b7b898":"#https:\/\/www.kaggle.com\/chamecall\/clrp-finetune\n\ndef create_optimizer(model):\n    named_parameters = list(model.named_parameters())    \n    \n    roberta_parameters = named_parameters[:197]    \n    attention_parameters = named_parameters[199:203]\n    regressor_parameters = named_parameters[203:]\n        \n    attention_group = [params for (name, params) in attention_parameters]\n    regressor_group = [params for (name, params) in regressor_parameters]\n\n    parameters = []\n    parameters.append({\"params\": attention_group})\n    parameters.append({\"params\": regressor_group})\n\n    for layer_num, (name, params) in enumerate(roberta_parameters):\n        weight_decay = 0.0 if \"bias\" in name else 0.01\n\n        lr = 2e-5\n\n        if layer_num >= 69:        \n            lr = 5e-5\n\n        if layer_num >= 133:\n            lr = 1e-4\n\n        parameters.append({\"params\": params,\n                           \"weight_decay\": weight_decay,\n                           \"lr\": lr})\n\n    return AdamW(parameters)","558b75ad":"# Main training loop\n# Configuration options\n\ndef run_Roberta(fold):\n    # Configuration options\n    NUM_EPOCHS = 3\n    loss_fct = torch.nn.MSELoss()\n    dfs = pd.read_csv(\".\/train_folds.csv\")\n    \n    \n    print('--------------------------------')\n    print(f'FOLD {fold}')\n    print('--------------------------------')\n    df_train = dfs[dfs.kfold != fold].reset_index(drop=True)\n    df_valid = dfs[dfs.kfold == fold].reset_index(drop=True)\n    train_data = Data(data = df_train)\n    valid_data = Data(data = df_valid)\n    yvalid = df_valid.target.values\n    \n    train_loader = torch.utils.data.DataLoader(\n                      train_data, \n                      batch_size=16,\n                      shuffle=True)\n    \n    val_loader = torch.utils.data.DataLoader(\n                      valid_data)\n    \n    model = Roberta(model_config)\n    model = model.to(device)\n\n    optimizer = create_optimizer(model)\n    \n    print(\"======================= Start training =================================\")\n\n    scheduler = get_cosine_schedule_with_warmup(optimizer,\n                                                num_warmup_steps=50,\n                                                num_training_steps= NUM_EPOCHS * len(train_loader))\n\n    pred = train(model=model, \n                  train_iter=train_loader, \n                  valid_iter=val_loader, \n                  optimizer=optimizer, \n                  scheduler=scheduler,\n                  loss_fct =loss_fct,\n                  num_epochs=NUM_EPOCHS,\n                  fold =fold,\n                  valid_period = len(train_loader))\n    \n    \n    rmse = sqrt(metrics.mean_squared_error(yvalid, pred))\n    print(f\"fold={fold}, RMSE={rmse}\")\n    \n    df_valid.loc[:,\"Roberta_pred\"] = pred\n    #torch.cuda.empty_cache() # PyTorch empty cache\n    \n    return df_valid[[\"id\",\"target\",\"kfold\",\"Roberta_pred\"]]\n\ndfs = []\nfor j in range(k_folds):\n    temp_df = run_Roberta(j)\n    dfs.append(temp_df)  \nfin_valid_df = pd.concat(dfs)\nfin_valid_df.to_csv(\".\/model_preds\/Roberta.csv\", index=False)\n","00467919":"import glob\nimport numpy as np\n\nfiles = glob.glob(\".\/model_preds\/*.csv\")\ndf = None\nfor f in files :\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df  = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n\nprint(list(df.columns))\ntargets = df.iloc[:,1].values\n\npred_cols = [\"XGBR_roberta_pred\", \"Roberta_pred\"]\n\nfor col in pred_cols:\n    rmse = sqrt(metrics.mean_squared_error(targets, df[col].values))\n    print(f\"{col}, overall_rmse={rmse}\")\n\nprint(\"average\")\navg_pred = np.mean(df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values, axis=1)\nprint(sqrt(metrics.mean_squared_error(targets, avg_pred)))\n\nprint(\"weighted average\")\n#rf_glove_pred = df.rf_glove_pred.values\nXGB_roberta_pred = df.XGBR_roberta_pred.values\nRoberta_pred = df.Roberta_pred.values\n\navg_pred = (Roberta_pred + 2*XGB_roberta_pred)\/3\nprint(sqrt(metrics.mean_squared_error(targets, avg_pred)))","0d2f967e":"#optimal weights\nimport glob\nimport numpy as np\nfrom functools import partial\nfrom scipy.optimize import fmin\n\nclass OptimizeRMSE:\n    def __init__(self):\n        self.coef_ =0\n        \n    def _rmse(self, coef, X, y):\n        x_coef = X*coef\n        predictions = np.sum(x_coef, axis=1)\n        rmse_score = sqrt(metrics.mean_squared_error(y, predictions))\n        return 1.0 * rmse_score\n    \n    def fit(self, X, y):\n        partial_loss = partial(self._rmse, X=X, y=y)\n        init_coef = np.random.dirichlet(np.ones(X.shape[1]))\n        self.coef_ = fmin(partial_loss, init_coef, disp=True)\n        \n    def predict(self, X):\n        x_coef = X*self.coef_\n        predictions = np.sum(x_coef, axis=1)\n        return predictions\n    \ndef run_training(pred_df, fold):\n    \n    train_df = pred_df[pred_df.iloc[:,2] != fold].reset_index(drop=True)\n    valid_df = pred_df[pred_df.iloc[:,2] == fold].reset_index(drop=True)\n    \n    xtrain = train_df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values\n    xvalid = valid_df[[\"XGBR_roberta_pred\", \"Roberta_pred\"]].values\n    \n    opt = OptimizeRMSE()\n    opt.fit(xtrain, train_df.iloc[:,1].values)\n    return opt.coef_\n     \nfiles = glob.glob(\".\/model_preds\/*.csv\")\ndf = None\nfor f in files :\n    if df is None:\n        df = pd.read_csv(f)\n    else:\n        temp_df  = pd.read_csv(f)\n        df = df.merge(temp_df, on=\"id\", how=\"left\")\n        \ntargets = df.iloc[:,1].values\npred_cols = [\"XGBR_roberta_pred\", \"Roberta_pred\"]\n\ncoefs = []\nfor j in range(k_folds):\n          coefs.append(run_training(df, j))\ncoefs = np.array(coefs)\nprint(coefs)\n\ncoefs = np.mean(coefs, axis=0)\nprint(coefs)\n\nwt_avg = (coefs[0]*df.XGBR_roberta_pred.values \n          + coefs[1]*df.Roberta_pred.values\n         )\nprint(\"optimal rmse after finding optimal coefs\")\nprint(sqrt(metrics.mean_squared_error(targets, wt_avg)))","62fce0b0":"#Data preprocessing\n\n#Load Data\ntest_df = pd.read_csv(test_file)\n\n#add fake label in the test only in the aim to use the evalution function\ntest_df.insert(1,'target', 0, False)\ntest_df.to_csv(\".\/test\/test_with_col_target.csv\", index=False)\n\ntest_preprop=preprop(test_df)\ntest_preprop.to_csv(\".\/test\/test_preprop.csv\", index=False)","87cb930e":"df_test = pd.read_csv(\".\/test\/test_with_col_target.csv\")\nMODEL_NAME = Path('..\/input\/modelf1')\ntest_embeddings = get_embeddings(df_test,MODEL_NAME)\n\ndf_xgboost_roberta= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\n\n\nfor j in range(k_folds):\n    xgb_roberta_reloaded = pickle.load(open(f'.\/models\/XGBR_roberta_fold_{j}.sav','rb'))\n    df_xgboost_roberta[f\"fold_{j}\"] = xgb_roberta_reloaded.predict(test_embeddings)\n    \ndf_xgboost_roberta['target_xgb_roberta'] = df_xgboost_roberta.iloc[:, 1:].mean(axis=1)\ndf_xgboost_roberta = df_xgboost_roberta[['id','target_xgb_roberta']]","5c826068":"#ROBERTA_Base\ndf_test = pd.read_csv(\".\/test\/test_with_col_target.csv\") #Better results\ntest_data = Data(data = df_test) \ntest_loader = DataLoader(dataset = test_data, shuffle=False)#, batch_size = 64)\n\ndf_Roberta= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\nfor j in range(k_folds):\n    torch.cuda.empty_cache() # PyTorch empty cache\n    \n    model = Roberta(model_config)    \n    load_path = f'.\/models\/RobertaBase_model-fold-{j}.pth'\n    model.load_state_dict(torch.load(load_path))\n    model = model.to(device)\n    \n    # Print about testing\n    print(f'Starting testing - fold_{j}')    \n    model_return = test(model=model,\n        test_loader = test_loader\n        )\n    \n    \n    df_Roberta[f\"fold_{j}\"] = model_return\n    \ndf_Roberta['target_Roberta'] = df_Roberta.iloc[:, 1:].mean(axis=1)\ndf_Roberta = df_Roberta[['id','target_Roberta']]","ac50cae8":"test_avg = (coefs[0]*df_xgboost_roberta.target_xgb_roberta.values\n          + coefs[1]*df_Roberta.target_Roberta.values\n         )","3c5fd818":"df= pd.DataFrame({\n     \"id\" : df_test.id.values\n  })\ndf['target']=test_avg\n\ndf.to_csv(f\"{output_path}\/submission.csv\",index=False)\n","975a3fbe":"# Create models","c1607f2a":"## Transformers model","6741320c":"# Create fold","83ecc8e6":"# Blending","299214db":"## Word embedding with Glove","29363032":"# Load models","3effe6ed":"## Word embedding with Roberta","5fb1c70d":"# text pre-processing for Sklearn models","e0da5e30":"# Test","bc6545f7":"## Word embedding with TfIDF"}}