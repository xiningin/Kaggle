{"cell_type":{"ecfd9307":"code","86166af0":"code","05cec308":"code","00dc7809":"code","c0548e53":"code","16aa46d3":"code","38065c22":"code","d2212ba7":"code","7fddb7ef":"code","466576ee":"code","b82d166a":"code","4cc8c72d":"code","6ce2df3f":"code","bac1c43b":"code","35a8d0e3":"code","7351f744":"code","2194f2f2":"code","caa0ba0e":"code","080e0dd1":"code","464dad01":"code","92f01141":"code","4f673219":"code","299d450b":"code","6809a168":"code","3a7f457d":"code","b825d91a":"code","07edd642":"code","216a7b65":"code","6fd46338":"code","b8898e3b":"code","e2da59bb":"code","13fde7de":"code","004afad7":"code","2de7d993":"code","7c277591":"code","0d977466":"code","9dea396a":"code","f844caff":"code","7e7e0724":"code","1cc54704":"code","da214ff9":"code","0c76773a":"code","a9986d1f":"code","d6c1e4e8":"code","51900fbc":"code","2451e6f8":"code","e337e662":"code","4af06f0a":"code","533f80af":"code","7e6db4f1":"code","07fbf327":"code","0eb80fbf":"code","41629c0a":"code","7f5dca55":"code","a66a9b15":"code","dc50ef93":"code","86e4e089":"code","ec3ec30d":"code","d4959852":"code","6fcf100c":"code","0642134e":"code","1b2534f3":"code","8fa93327":"code","1366c149":"code","f7d958e0":"code","a6ea0af7":"code","fd4c453d":"code","3d702ee2":"code","066374c4":"code","13091ccf":"code","de1cea24":"code","46456a53":"code","d7453ccb":"code","7d971e56":"code","34bd8112":"code","46dd1094":"code","8f580019":"markdown","68f968cc":"markdown","14bb7939":"markdown","6fdd6169":"markdown","cb29a487":"markdown","18a9ccd9":"markdown","9cbb0750":"markdown","f26e926d":"markdown","f3d8c552":"markdown","f0339650":"markdown","76e15e71":"markdown","0eaa50fa":"markdown","84c27c67":"markdown","37bcc020":"markdown","ad41d8e2":"markdown","0df09d5f":"markdown","3fde3fd2":"markdown","6c5cab3c":"markdown","dccf2eac":"markdown","c6f6b762":"markdown","15c61d60":"markdown","2f82e189":"markdown","aca876e8":"markdown","6353142f":"markdown","7aad0458":"markdown","6b63171e":"markdown","93872cef":"markdown","dc4ddfe8":"markdown","42d7ed16":"markdown","7d97cf61":"markdown","53f15769":"markdown","2a0b6ac0":"markdown","e4bdc1e2":"markdown","b83ece95":"markdown","9d3a4b11":"markdown","d6becd8a":"markdown","4527711f":"markdown","c93091c6":"markdown","7e2f5ea4":"markdown","47644e4e":"markdown","f579632b":"markdown","92b86a53":"markdown","81fdd046":"markdown","52a47ac6":"markdown","f6cda5c9":"markdown","b432a883":"markdown","5452c7b8":"markdown","a98b6742":"markdown","089dd8d7":"markdown","ca4260e7":"markdown","fcdeb694":"markdown","690fd114":"markdown","0fdeef1e":"markdown","25903a2c":"markdown","ffa8d407":"markdown","b2f220af":"markdown","8d3dcc0a":"markdown","742edf19":"markdown","2de95475":"markdown","4b880159":"markdown","06d0fc78":"markdown","c2ea4a2b":"markdown","5d5403ce":"markdown","fb763817":"markdown"},"source":{"ecfd9307":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nplt.style.use(['fivethirtyeight'])\nsns.set(font_scale = 1)  \npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\nnp.random.seed(42)\n\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score\n\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom mpl_toolkits.mplot3d import Axes3D\n# from matplotlib import colors\n# from matplotlib.colors import ListedColormap\n\nprint(\"Let's start!\")","86166af0":"data = pd.read_csv(\"..\/input\/customer-personality-analysis\/marketing_campaign.csv\", sep = '\\t')\ndisplay(data.shape, data.head(3))\n","05cec308":"data.columns","00dc7809":"data_info = pd.DataFrame(columns=['Name of Col', 'Num of Null', 'Dtype', 'N_Unique'])\n\nfor i in range(0, len(data.columns)):\n    data_info.loc[i] = [data.columns[i],\n                        data[data.columns[i]].isnull().sum(),\n                        data[data.columns[i]].dtypes,\n                        data[data.columns[i]].nunique()] \n    \ndata_info","c0548e53":"data.dropna(inplace = True)\n\ndata.shape","16aa46d3":"data['Dt_Customer'] = pd.to_datetime(data['Dt_Customer'])\ndates = []\n\nfor i in data['Dt_Customer']:\n    i = i.date()   \n    dates.append(i)\n    \nprint(\"The newest customer's enrolment date in therecords:\",max(dates))\nprint(\"The oldest customer's enrolment date in the records:\",min(dates))   ","38065c22":"days = []\nd1 = max(dates)\n\nfor i in dates:\n    delta = d1 - i\n    days.append(delta)\n    \ndays[0]","d2212ba7":"days_int = []\nfor i in range(0, len(days)):\n    days_int.append(days[i].days)  # with the method .days, we are able to get only the number of days from the timedelta type.\n    \ndays_int[0]","7fddb7ef":"data['Cus_for'] = days_int","466576ee":"print(data[\"Cus_for\"].dtypes) \ndata['Cus_for'].head(1) ","b82d166a":"print(data.Marital_Status.value_counts())\nprint(f'Num of unique category: {data.Marital_Status.nunique()}')\nprint('\\n')\nprint(data.Education.value_counts())\nprint(f'Num of unique category: {data.Education.nunique()}')","4cc8c72d":"data['Age'] = 2021 - data['Year_Birth']\n\n\ndata['Education'].replace({\"2n Cycle\" : \"Post Graduate\", \n                           \"Master\" : \"Post Graduate\", \n                           \"PhD\" : \"Post Graduate\"}, inplace = True)\n\n\ndata['Children'] = data['Kidhome'] + data['Teenhome']\n\n\ndata['Spent_All'] = data['MntWines'] + data['MntFruits'] + data['MntMeatProducts'] + data['MntFishProducts'] + data['MntSweetProducts'] + data['MntGoldProds']\ndata['PurchaseNumAll'] = data['NumDealsPurchases'] + data['NumWebPurchases'] + data['NumCatalogPurchases'] + data['NumStorePurchases']\ndata['PurDeal_PurAll_ratio'] = data['NumDealsPurchases'] \/ data['PurchaseNumAll']\ndata[\"Total_Promos\"] = data[\"AcceptedCmp1\"]+ data[\"AcceptedCmp2\"]+ data[\"AcceptedCmp3\"]+ data[\"AcceptedCmp4\"]+ data[\"AcceptedCmp5\"] + data[\"Response\"]\n\ndata.rename(columns = {'Marital_Status' : 'Living_with'}, inplace = True)\ndata.loc[(data['Living_with'] == 'Married') | (data['Living_with'] == 'Together'), 'Living_with'] = 'Partner'\ndata.loc[data['Living_with'] != 'Partner', 'Living_with'] = 'Alone'\n\n\ndata['Family_Size'] = data['Living_with'].replace({'Partner':2, 'Alone':1}) + data['Children']\n\ndata['Is_Parent'] = np.where(data['Children']>0, 1, 0)\n\ndisplay(data.shape, data.head(2))","6ce2df3f":"data.describe()","bac1c43b":"data.loc[data['Age'] >= 90]","35a8d0e3":"print(data.shape)\n\ndata.drop(index = [192, 239, 339], axis = 0, inplace = True)\n\nprint(data.shape)\ndata.Age.describe()","7351f744":"data['PurDeal_PurAll_ratio'].fillna(0, inplace=True)","2194f2f2":"drop_cols = ['Dt_Customer', 'ID', 'Year_Birth', 'Z_CostContact', 'Z_Revenue', 'Complain']\ndata.drop(drop_cols, axis = 1, inplace = True)\nprint(data.shape)","caa0ba0e":"x = data['Is_Parent'].value_counts().sort_values()\n\nplt.figure(figsize=(5, 5))\nax = plt.pie(x = x, labels=['Not_Parent', 'Is_Parent'], autopct = '%1.1f%%', wedgeprops = {'linewidth': 5})\nplt.title('Propotion of \"Is_Parent\"')\nplt.style.use(['fivethirtyeight'])\nplt.show()","080e0dd1":"to_plot = ['Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',\n           'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos','Is_Parent']\n\nsns.pairplot(data[to_plot], hue = \"Is_Parent\")\n# plt.style.use(['fivethirtyeight'])\nplt.show()","464dad01":"fig, axes = plt.subplots(2, 1, figsize=(10,5))\n\nsns.boxplot(data = data, x = 'Income', color = 'red', ax = axes[0])\nsns.histplot(data = data, x = 'Income', kde=True, color='blue', ax = axes[1])\nplt.show()","92f01141":"# define the function for capping the outliers\n\nprint(f'old shape: {data.shape}')\nq1 = data.Income.quantile(0.25)\nq3 = data.Income.quantile(0.75)\n\niqr = q3 - q1\n\nupper_bound = q3 + 1.5*iqr\nlower_bound = q1 - 1.5*iqr\nprint(f'upper bound value: {upper_bound}')\nprint(f'lower bound value: {lower_bound}')\n\ndata.loc[data['Income'] > upper_bound, 'Income'] = upper_bound\ndata.loc[data['Income'] < lower_bound, 'Income'] = lower_bound\n\nprint(f'new shape: {data.shape}, There is no change of the shape.')","4f673219":"# Recheck\n\nfig, axes = plt.subplots(2, 1, figsize=(10,5))\n\nsns.boxplot(data = data, x = 'Income', color = 'red', ax = axes[0])\nsns.histplot(data = data, x = 'Income', kde=True, color='blue', ax = axes[1])\n\nplt.show()","299d450b":"to_plot = ['Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',\n           'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos','Is_Parent']\n\nsns.pairplot(data[to_plot], hue = \"Is_Parent\")\n\nplt.show()","6809a168":"x = data['Education'].value_counts().sort_values()\nlabels = data['Education'].value_counts().sort_values().index\n\nplt.figure(figsize=(5, 5))\nplt.pie(x = x, labels = labels, autopct = '%1.1f%%', wedgeprops = {'linewidth': 5}, explode = [0.3, 0, 0])\nplt.title('Propotion of \"Education\"')\nplt.show()","3a7f457d":"to_barplot = ['Income', 'Recency', 'Cus_for', 'Spent_All', 'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos']\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize = (10, 15))\naxes = axes.flatten()\n\nfor col, ax in zip(to_barplot, axes):\n    ax = sns.barplot(data = data, x = 'Education', y = col, ax = ax,\n                order = ['Basic', 'Graduation', 'Post Graduate'], ci = None)\n    ax.set_title(f'mean of {col} by Eduction', fontsize = 15)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n    \naxes[-1].axis('off')    \nplt.show()","b825d91a":"# Will unify the Graduation and Post Graduate since there's no meaningful difference btw 2 classes.\n\ndata['Education'].replace({\"Graduation\" : \"Higher\", \n                           \"Post Graduate\" : \"Higher\"}, inplace = True)","07edd642":"plt.figure(figsize=(5, 4))\n\nsns.histplot(data = data, x = 'Age', kde=True)\nplt.title('Age distribution')\n\nplt.show()","216a7b65":"to_lineplot = ['Income', 'Recency', 'Cus_for', 'Spent_All', 'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos']\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize = (15, 10))\naxes = axes.flatten()\n\nfor col, ax in zip(to_lineplot, axes):\n    ax = sns.lineplot(data = data, x = 'Age', y = col, ax = ax, )\n    ax.set_title(f'mean of {col} by Age', fontsize = 15)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n\naxes[-1].axis('off')    \nplt.show()","6fd46338":"x = data['Living_with'].value_counts().sort_values()\nlabels = data['Living_with'].value_counts().sort_values().index\n\nplt.figure(figsize=(5, 5))\nplt.pie(x = x, labels = labels, autopct = '%1.1f%%', wedgeprops = {'linewidth': 5})\nplt.title('Propotion of \"Living_with\"')\nplt.show()","b8898e3b":"to_barplot = ['Income', 'Recency', 'Cus_for', 'Spent_All', 'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos']\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize = (10, 15))\naxes = axes.flatten()\n\nfor col, ax in zip(to_barplot, axes):\n    ax = sns.barplot(data = data, x = 'Living_with', y = col, ax = ax,\n                order = ['Alone', 'Partner'], ci = None)\n    ax.set_title(f'mean of {col} by \"Living_With\"', fontsize = 15)\n    plt.subplots_adjust(hspace = 0.5, wspace = 0.3)\n\naxes[-1].axis('off')    \nplt.show()","e2da59bb":"data.drop(['Living_with'], axis = 1, inplace = True)\n\ndisplay(data.shape, data.head(1))","13fde7de":"corr_cols = ['Income', 'Kidhome', 'Teenhome', 'Recency', 'NumDealsPurchases', 'NumWebPurchases',\n             'NumCatalogPurchases', 'NumStorePurchases', 'NumWebVisitsMonth', 'Cus_for', 'Age',\n             'Children', 'Spent_All', 'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos','Family_Size', 'Is_Parent']\n\ncorr = data[corr_cols].corr()\n\nplt.figure(figsize=(12, 12))\n\nmask = np.zeros_like(corr, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nheat_map = sns.heatmap(corr,\n                       annot = True,\n                       cmap = 'RdYlGn',\n                       mask = mask,\n                       linewidths = 0.1,\n                       cbar_kws = {'shrink':0.5})","004afad7":"pd.pivot_table(data, index=['Is_Parent', 'Education'],\n               values = ['Spent_All', 'PurDeal_PurAll_ratio', 'Total_Promos',\n                         'NumWebVisitsMonth', 'NumCatalogPurchases',\n                         'NumDealsPurchases', 'NumStorePurchases', 'NumWebPurchases'], aggfunc='mean',\n               margins= True)","2de7d993":"# There are 4 specific data types on this data.\n# I'd like to introduce one of the easy ways to seperate the categorical fetures and numerical features\n\ndata.dtypes.unique()","7c277591":"# We are able to extract the numerical data of dataset using \"._get_numeric_data()\" method\n# After that, use the set calculation to get the columns of categorical features.\n\nnum_cols = data._get_numeric_data().columns\ncat_cols = set(data.columns) - set(num_cols)\n\nprint(list(cat_cols))","0d977466":"le = LabelEncoder()\n\nfor i in cat_cols:\n    data[i] = data[[i]].apply(le.fit_transform)\n    \ndata[cat_cols].head(2)\n\n# result: encoded by {'Basic':0, 'Higher':1}","9dea396a":"data.head(1)","f844caff":"# drop_cols: detailed columns against 'Spent_All' and T\/F columns against 'Total_Promos'.\n\ndata_scaled = data.copy()\n\ndrop_cols = ['AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Response',\n             'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds']\ndata_scaled.drop(columns = drop_cols, axis = 1, inplace = True)\n\nprint(data_scaled.shape)","7e7e0724":"scale_cols = ['Income', 'Kidhome', 'Teenhome', 'Recency',\n       'NumDealsPurchases', 'NumWebPurchases', 'NumCatalogPurchases',\n       'NumStorePurchases', 'NumWebVisitsMonth', 'Cus_for', 'Age', 'Children', 'Spent_All', 'PurchaseNumAll',\n       'PurDeal_PurAll_ratio', 'Total_Promos', 'Family_Size']","1cc54704":"ss = StandardScaler()\n\nfor col in scale_cols:\n    data_scaled[col] = ss.fit_transform(data_scaled[[col]])\n    \ndisplay(data_scaled.shape, data_scaled.head(3))","da214ff9":"data_scaled.head(2)","0c76773a":"variance_ratio = {}\n\nfor i in range(1, len(data_scaled.columns)+1):\n    pca = PCA(n_components=i)\n    pca.fit(data_scaled)\n    variance_ratio[f'n_{i}'] = pca.explained_variance_ratio_.sum()","a9986d1f":"# getting the cumulative variance ratio.\n# for instance, if we select number of components 10, the variables fitted by PCA would probably explain the data by 94%\n\nvariance_ratio","d6c1e4e8":"# eigenvector per each PC\ndata_pca5 = pd.DataFrame(pca.components_[0:5],\n                         columns=data_scaled.columns,\n                         index = ['PC1','PC2','PC3', 'PC4', 'PC5']).T\n\ndata_pca5","51900fbc":"# heatmap for each loading(value of the elements of eigenvector against each principal component.)\n\nplt.figure(figsize=(14, 10))\nsns.heatmap(data_pca5,\n            annot=True,\n            cmap='RdYlGn',\n            cbar_kws={'shrink' : 0.5}           \n           )\n\nplt.show()","2451e6f8":"# eigen value\neigen_value = np.sort(pca.explained_variance_)[::-1]\n\nplt.figure(figsize=(12, 5))\n\nplt.plot(variance_ratio.keys(), eigen_value)\nplt.ylim(0, 10, 1)\nplt.axhline(1, color = 'red', ls = '--')\nplt.title('Elbow Point')\n\nplt.show()","e337e662":"# eigen value\neigen_value","4af06f0a":"# example for calculating the explained variance ratio of the first eigen value\neigen_value[0] \/ eigen_value.sum()","533f80af":"print(f'the number of eigenvalue greater than one: {len(eigen_value[eigen_value > 1])}')","7e6db4f1":"plt.figure(figsize = (12, 5))\n\nplt.plot(variance_ratio.keys(), variance_ratio.values())\nplt.axhline(0.7, color = 'red', ls = '--', lw = 1)\nplt.axhline(0.9, color = 'red', ls = '--', lw = 1.5)\nplt.title(\"Variance Ratio\")\n\nplt.show()","07fbf327":"pca = PCA(n_components = 5, random_state = 42)\n\npca.fit(data_scaled)\ndata_pca = pd.DataFrame(pca.transform(data_scaled), \n                        columns = ([\"PC1\", \"PC2\", \"PC3\", \"PC4\", \"PC5\"]))\n\ndata_pca.describe().T","0eb80fbf":"km = KMeans()\nElbow_M = KElbowVisualizer(estimator = km, k = 10)\nElbow_M.fit(data_pca)\nElbow_M.show()\n\nplt.show()","41629c0a":"km = KMeans()\nElbow_M = KElbowVisualizer(estimator = km, k = 10, metric='silhouette')\nElbow_M.fit(data_pca)\nElbow_M.show()\n\nplt.show()","7f5dca55":"def visualize_silhouette_layer(data):\n    clusters_range = range(2,10)\n    results = []\n\n    for i in clusters_range:\n        km = KMeans(n_clusters=i, random_state=42)\n        cluster_labels = km.fit_predict(data)\n        silhouette_avg = silhouette_score(data, cluster_labels)\n        results.append([i, silhouette_avg])\n\n    result = pd.DataFrame(results, columns=[\"n_clusters\", \"silhouette_score\"])\n    pivot_km = pd.pivot_table(result, index=\"n_clusters\", values=\"silhouette_score\")\n\n    plt.figure()\n    sns.heatmap(pivot_km, annot=True, linewidths=1, fmt='.3f', cmap='RdYlGn')\n    plt.tight_layout()\n    plt.show()","a66a9b15":"visualize_silhouette_layer(data_pca)","dc50ef93":"km = KMeans(n_clusters=3, random_state=42)\n\nyhat_AC = km.fit_predict(data_pca)\n\ndata_pca[\"Clusters\"] = yhat_AC   # for evaluating the model\ndata[\"Clusters\"]= yhat_AC        # for customers profiling","86e4e089":"# 3 dimension\n\nx =data_pca[\"PC1\"]\ny =data_pca[\"PC2\"]\nz =data_pca[\"PC3\"]\n\nfig = plt.figure(figsize=(12,10))\nax = plt.subplot(111, projection='3d')\nax.scatter(x, y, z, s=40, c=data_pca[\"Clusters\"], marker='o', alpha = 0.5, cmap = 'Spectral')\nax.set_title(\"The Plot Of The Clusters(3D)\")\n\nplt.show()","ec3ec30d":"# 2 dimension\n\nplt.figure(figsize=(8,6))\n\nsns.scatterplot(data = data_pca, x = 'PC1', y='PC2', hue='Clusters')\nplt.title('The Plot Of The Clusters(2D)')\nplt.show()","d4959852":"data.head(1)","6fcf100c":"# Plotting countplot of clusters\n\nax = sns.countplot(x=data[\"Clusters\"])\nax.bar_label(ax.containers[0])\nax.set_title(\"Distribution Of The Clusters\")\nplt.show()","0642134e":"plt.figure(figsize=(5, 4))\n\nsns.scatterplot(data = data, x = data['Income'], y = data.index, hue = 'Clusters')\n\nplt.show()","1b2534f3":"plt.figure(figsize=(5, 4))\n\nsns.scatterplot(data = data, x = data['PurchaseNumAll'], y = data.index, hue = 'Clusters')\n\nplt.show()","8fa93327":"plt.figure()\n\nsns.scatterplot(data = data, x = data['Spent_All'], y = data.index, hue = 'Clusters')\n\nplt.show()","1366c149":"plt.figure()\npl=sns.swarmplot(x=data[\"Clusters\"], y=data[\"Spent_All\"], alpha=0.5 )\npl=sns.boxenplot(x=data[\"Clusters\"], y=data[\"Spent_All\"])\nplt.show()","f7d958e0":"ax = sns.scatterplot(data = data,x=data[\"Spent_All\"], y=data[\"Income\"],hue=data[\"Clusters\"])\nax.set_title(\"Cluster's Profile Based On Income And Spending\")\nplt.legend()\nplt.show()","a6ea0af7":"Places =[\"NumWebPurchases\", \"NumCatalogPurchases\", \"NumStorePurchases\",  \"NumWebVisitsMonth\"] \n\npd.pivot_table(data = data, index = ['Clusters', 'Is_Parent'], values = Places, margins=True)","fd4c453d":"#for more details on the purchasing style \n\nfor i in Places:\n    plt.figure()\n    sns.jointplot(x=data[i],y = data[\"Spent_All\"],hue=data[\"Clusters\"])\n    plt.show()\n\nplt.show()","3d702ee2":"Promos =['NumDealsPurchases', 'PurDeal_PurAll_ratio', 'Total_Promos'] \n\npd.pivot_table(data = data, index = ['Clusters', 'Is_Parent'], values = Promos, margins=True)","066374c4":"# for more details on the promotion sensitivity\n\nfor i in Promos:\n    plt.figure()\n    sns.jointplot(x=data[i],y = data[\"Spent_All\"],hue=data[\"Clusters\"], kind='scatter')\n    plt.show()\n\nplt.show()","13091ccf":"#Plotting count of total campaign accepted.\n\nplt.figure()\nax = sns.countplot(x=data[\"Total_Promos\"],hue=data[\"Clusters\"])\nax.set_title(\"Count Of Promotion Accepted\")\nax.bar_label(ax.containers[0])\nax.bar_label(ax.containers[1])\nax.bar_label(ax.containers[2])\nax.set_xlabel(\"Number Of Total Accepted Promotions\")\nplt.legend(loc = 'upper right')\nplt.show()","de1cea24":"#Plotting the number of deals purchased\n\nplt.figure()\nax = sns.boxenplot(y=data[\"NumDealsPurchases\"],x=data[\"Clusters\"])\nax.set_title(\"Number of Deals Purchased\")\nplt.show()","46456a53":"plt.figure()\nax = sns.lineplot(data = data, x = \"Clusters\", y = \"PurDeal_PurAll_ratio\", hue = 'Is_Parent', ci=None)\nax.set_xticks([0, 1, 2])\nax.set_title(\"PurchaseDeal \/ PurchaseAll ratio\")\nplt.show()","d7453ccb":"Personal = [\"Income\", \"Education\", \"Cus_for\", \"Age\", \"Kidhome\",\"Teenhome\", \"Children\", \"Family_Size\" ]\n\npd.pivot_table(data = data, index = ['Clusters', 'Is_Parent'], values = Personal, margins=True)","7d971e56":"# only 7% of people in cluster 0 is parents.\n\ndata['Is_Parent'].groupby(data['Clusters']).mean()","34bd8112":"sns.countplot(data = data, x = 'Is_Parent', hue = 'Clusters')\nplt.title('Number of people as per each cluster - Is_Parent')\nplt.show()","46dd1094":"for i in Personal:\n    plt.figure()\n    sns.jointplot(x=data[i], y=data[\"Spent_All\"], hue =data[\"Clusters\"])\n    plt.show()\n","8f580019":"<a id = \"1\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Importing Libraries<\/p>","68f968cc":"<br>\n\n* `.explained_variace_ratio_` : We are able to check the percentage of the explained variacne of each PC.\n\n\n* Consequently, we recognize the cumulative amount of explained variance by `.explained_variace_ratio_.sum()`\n    * Possible to explain the whole data by 37.8% with the 1st PC.\n    * Possilbe to explain the whole data by 75.7% if we select the first 5 PC.","14bb7939":"<a id = \"5\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Dimensionality Reduction<\/p>\n\n\n#### PCA(Principal Component Analysis)\n\nPrincipal Component Analysis (PCA) is an unsupervised technique used in machine learning to reduce the dimensionality of a data. It does so by compressing the feature space by identifying a subspace that captures most of the information in the complete feature matrix.\n\n<br>\n\n**By using PCA, we are able to solve the problem of,**\n1. the curse of dimentionality.\n2. multicollinearity between features.\n3. high-dimensional visualtizaion.\n\n<br>\n\n**Combined method of PCA and K-Means Clustering**\n\nUsing PCA before K-Means Clustering reduces dimensions and decreases computation cost. And the performance of clustering depends on the distribution of a data set and the correlation of features.\nTherefore using PCA before clustering is quite reasonable if you need to cluster data based on many features.\n\n<br>\n\n**Parameter of PCA**\n\nIn the PCA algorithm, we take **N** dimensional features and reduce them to some **K** dimensional feature representation. \n\nThe **K** is a parameter of the PCA algorithm and it is equal to **`n_components`** in scikit-learn API for PCA implementation.\n\nI hope this section is helpful for people who want to practice PCA algorithm for the customer segmentation.","6fdd6169":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Visualization Analysis<\/p>**","cb29a487":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Personal Information of Each Cluster<\/p>**\n\n\n\n\n* Personal Information: \"Income\", \"Education\", \"Cus_for\", \"Age\", \"Is_Parent\", \"Kidhome\",\"Teenhome\", \"Children\", \"Family_Size\" ","18a9ccd9":"<br>\n\n#### Outlier in Income feature\n\n\nThere found the outlier in Income feature. Let me change the value of outliers into upper value or lower value after the calculation of Q1, Q3, and eventually interquatile range.\n\nIt means that we just convert the value of outliers instead of simply dropping them from the data.\n\n\n<img src = \"https:\/\/img1.daumcdn.net\/thumb\/R800x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcAqc6V%2FbtqyQLiddUd%2FiXQVu1nYTo2rx3Q8xZBqy0%2Fimg.png\">\n\n<br>","9cbb0750":"<br>\n\n**3. Total \"number\" of purchasing per each cluster.**\n\n```PurchaseNumAll```: We can be sure that cluster2 shows the lowest figure as almost every values in cluster2 is very close to each other.\n\nWe will take a look at ```Spent_All``` since ,for sure, we couldn't distinguish the difference between C0 and C1.","f26e926d":"<br>\n\n**2. Sensitivity to Promotion**\n\n\n* Cluster 0 - Is_parent: Remember that this cluster is the group of the highest income.\n    * The parents group is slightly more sensitive to the promotion than the no-parents group is.\n    * Cluster 0 shows the highest value of ```Total_Promos``` among all three clusters, but the lowest value of ```PurDeal_PurAll_ratio```.\n    * We can guess that the people in cluster 0 usually accept the promotion for the relatively cheap items like daily necessities and are willing to pay for the relatively expensive items.  \n    \n    \n* Cluster 1 - Is_parent:\n    * The result shows the clear differnece accoring to whether the people of this cluster are parents or not.\n    * The more the shoppers of this cluster is parents, the more they are sensitive to the promotion from the shop.\n\n    \n* Cluster 2 - Is_parent:\n    * Almost the same result as the cluster 1 shows.\n    * As the people in this group usually earn the lower income than the people in other clusters, their ```PurDeal_PurAll_ratio``` is quite high, especially the more they are parents. \n    \n    \n* **Insight**\n    * It will be very important to figure out which kind of promotion was more acceptable to each customer.","f3d8c552":"<br>\n\nThe best **K** for clustering by silhouette method is 2.\n\nBut It is usually recommended to select the number **K** with the second largest silhouette score.\n\nTherefore I'd like to select the optimal **K** as 3.\n\n<br>","f0339650":"<br>\n\n**2. Income comparison per each cluster**\n\nIncome: C2 < C1 < C0\n\nC2 is the cluster of the lowest income and C0 is the one of the highest income.\n","76e15e71":"<br><br>\n\n***Insight***\n\n* **The comparison between Age feature and others**\n    * Income: The higher the age is, the higher the income generally.\n    * Recency: No significant difference but it looks like that the recency is higher according to the age.\n    * Cus_for: No meaningful difference\n    * Spent_All: No clear interpretation before 30 but the amount spent increases when the age is 30~70.\n    * PurchaseNumAll: The higher the age is, the higher the income generally.\n    * PurDeal_PurAll_ratio: The ratio appears high when the age is 30~60 but decreases after 60\n        \n* **Age feature is also the meaningful to judge how much the customers are sensitive to the promotion.**\n\n<br>","0eaa50fa":"<br>\n\nLet's take a look at the selected features using pairplot.\n\n\n* 'Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',  'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos'\n\n<br>","84c27c67":"<br>\n\n2. Silhouette method\n\n* Elbow method makes up for the disadvantage of the above method\n    \n    \n* Returns the Silhouette score according to the specific formula.(the higher the better)\n\n\n* In most cases, cluster with K=2 returns the highest score but it is highly recommended to select the number of K by the second largest score.","37bcc020":"**Process of PCA**\n\n1. Generating the covariance matrix of the data\n    * Covariance matrix has the shape of \"Num of features X Num of features\"\n    * In this case we will use the data frame **`data_scaled`** which was defined above.\n    * Thus the shape of covariance matrix for this data will be 19 X 19.\n    \n    \n2. Generating the correlation matrix from the covariance matrix\n    * The shape will be equally 19 x 19.\n    \n    \n3. Returning the eigenvalue and the eigenvector for each eigenvalue\n    * Totally 19 eigenvalues are returned by the number of features.\n    * Each eigenvector has 19 elements.\n    * Each eigenvalue is the variance of each principal component, and the elements of each eigenvector is the coefficients(a.k.a loading) for the features.\n\n\n4. Original features are converted based on the eigenvector\n    * In this case, totally 19 principal components are generated.\n    * We are able to check how much each principal components explains the variance of the original data.\n    \n    \n5. PCA implementation after the selection of the proper number of PC\n    * Using the 3 methods I've mentioned above.","ad41d8e2":"<br>\n\n**5. The relation between ```Spent_All``` and ```Income``` based on each cluster**","0df09d5f":"<br>\n\nThis is true that the plot forms an elbow on **n_3** but the number of eigenvalue which is greater than one is **5**.\n\nWhat if we move on to **Percent of variation threshold** method? \n\n<br>","3fde3fd2":"<br>\n\n### Data Cleaning ","6c5cab3c":"#### Label Encoding","dccf2eac":"<br><br>\n\n* Simple summary for your reference before the next step ! ","c6f6b762":"# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Customer Segmentation<\/p>\n\n\n<img\n     src=\"https:\/\/www.segmentify.com\/wp-content\/uploads\/2021\/08\/Top-Customer-Segmentation-Examples-every-Marketer-Needs-to-Know.png\">\n\n\n<br>\n<p style=\"background-color:white;font-family:Tahoma;color:dark gray;font-size:100%;text-align:left;border-radius:10px 0px;\">#Kaggle  #Customer Personality Analysis  #K-means clustering  #PCA  #Thanks To Karnika Kapoor, Peterson F<\/p>\n\nReferred to [Here](https:\/\/www.kaggle.com\/karnikakapoor\/customer-segmentation-clustering), [and Here](https:\/\/www.kaggle.com\/imperiopts\/customer-segmentation-and-potential-forecast#2nd-Part---Machine-Learning-Models-to-Predict-the-Potential-of-Recent-Customers)\n\n\nI referred two notebooks above when I started to analyze this data set and they are quite helpful for me to embark upon my analysis.\n\nIn my work I focused on the theory about PCA & K Means Clustering and the methodology about how to implement them based on the theory.\n\nThanks for visiting my work and I hope you upvote if you find it helpful for you to understand this dataset.\n\n<br>\n\n\n**Problem Statement**\n\nCustomer Personality Analysis is a detailed analysis of a company\u2019s ideal customers. It helps a business to better understand its customers and makes it easier for them to modify products according to the specific needs, behaviors and concerns of different types of customers.\n\nCustomer personality analysis helps a business to modify its product based on its target customers from different types of customer segments. For example, instead of spending money to market a new product to every customer in the company\u2019s database, a company can analyze which customer segment is most likely to buy the product and then market the product only on that particular segment.\n\n<br>\n\n**Target**\n\nNeed to perform clustering to summarize customer segments.\n\n\n<br><br>\n   <a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:navy;font-family:tahoma;color:#FFF9ED;font-size:150%;text-align:center;border-radius:10px 10px;\">TABLE OF CONTENTS<\/p>   \n\n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. LOADING DATA](#2)\n    \n* [3. DATA CLEANING](#3)\n    \n* [4. DATA PREPROCESSING](#4)   \n    \n* [5. DIMENSIONALITY REDUCTION](#5) \n      \n* [6. CLUSTERING](#6)\n    \n* [7. EVALUATING MODELS](#7)\n    \n* [8. CUSTOMERS PROFILING](#8)\n    \n* [9. CONCLUSION](#9)\n    \n<br>","15c61d60":"<br>\n\n**1. Purchasing style**\n\n* Cluster 0 - Is_parent: Remember that this cluster is the group of the highest income.\n    * The parents group has the higher probability with which this group generally shops the items via catalog or website compared to the no-parents group \n    \n    \n* Cluster 1 - Is_parent:\n    * The result is opposite. Different from the parent group on cluster 0, The parents group in cluster 0 usually shops their items by visiting store more than the no-parents group does.\n\n    \n* Cluster 2 - Is_parent:\n    * Couldn't verify if there is any difference according to whether the people of this cluster are parents or not.\n   \n<br>   \n    \n***Insight***\n\n* Even though people are the parents, they would have the different purchasing style based on the income level.","2f82e189":"<br>\n\n#### Value counting the categorical variables\n\n\nWe will be exploring the unique category in the object features, \"Marital_Status\" and \"Education\".\n\n* Marital_Status column has 8 categories.\n* Education column has 5 categories.","aca876e8":"<br>\n\n#### Standardization and Creating a subset dataframe\n\n* Copy the original data.\n\n\n* Some of features will be dropped.\n    * 'AcceptedCmp3', 'AcceptedCmp4', 'AcceptedCmp5', 'AcceptedCmp1', 'AcceptedCmp2', 'Response'\n        * Every information about them is included in 'Total_Promos' feature.        \n        \n    * 'MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds'\n        * Every information about them is included in 'Spent_All' feature.\n\n\n* We will use the copied data when we implement dimensionality reduction using PCA.\n\n\n<br>","6353142f":"<br>\n\n\n### Correlation Analysis","7aad0458":"<br><br>\n\n***Insight***\n\n\n* **Comparison within 'Education' feature**\n    * Income: clearly shows the difference between 'Basic' and the others(above Graduation). \n    * Recency: No meaningful difference\n    * Cus_for: is longer when 'Education == Basic'\n    * Spent_All: Total amount spent by 'Gradution' and 'Post Graduate' is higher than the group of 'Education == Basic'.\n    * PurchaseNumAll: Total number of purchased by 'Gradution' and 'Post Graduate' more than double that by 'Education == Basic'\n    * PurDeal_PurAll_ratio: The value is much higher when 'Education == Basic'\n    * Total_Promos: The group of 'Education !=Basic' is more sensitive to the promotion\n    \n\n* **We are able to sure that 'Education' feature is the meaningful feature to separate not only the volume of the amount spent but also the sensitivity to the promotion**\n\n<br>","6b63171e":"<br><br>\n\n***Insight***\n\n* **Comparison within 'Is_Parent' feature**\n    * Income: The average income of 'Is_Parent == 0' is higher than that of 'Is_Parent == 1'\n    * Recency: No meaningful difference\n    * Cus_for: No meaningful difference\n    * Age: No meaningful difference\n    * Spent_All: Total amount spent by 'Is_Parent == 0' is higher\n    * PurchaseNumAll: No significant difference whether the customer is parent or not.\n    * PurDeal_PurAll_ratio: The ratio is quite higher when the customer is parent\n    * Total_Promos: The 'Is_Parent == 0' is more sensitive to the promotion than 'Is_Parent ==1'\n        \n* **Positive correlation appears between 'Income' and 'Spent_All'**\n\n\n* **The value of 'Spent_All' is higher when 'Is_Parent ==0'**\n\n\n\n<br>","93872cef":"### Variation according to \"Is_Parent\" feature","dc4ddfe8":"* **`Income` and `Is_Parent` features is quite important to separate the clusters and explain the profile of each cluster.**\n\n* **It looks like `Promotion` of this shop is not related with the \"Deal\" because the people in C0(High income) is very sensitive to the promotion whereas they have the lowest `PurDeal_PurAll_ratio`. Vice versa on the other clusters.**\n\n* **Limitaion of this work: There would be other meaningful result or insight if we analyze the product category.**\n\n<br><br>\n\n***Thanks for watching my notebook and please give me an upvote if you find it helpful !***","42d7ed16":"<br>\n\n<a id = \"9\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Conclusion<\/p>","7d97cf61":"#### Drop the rows including null data","53f15769":"<br><br>\n\n* **Correlation**\n    * Most of independent features are connected with high correlation.\n    * It will be reasonable to reduce the dimension before clustering to prevent from multicolinearity btw features.\n\n<br>","2a0b6ac0":"<a id = \"2\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Loading Data<\/p>","e4bdc1e2":"<br><br>\n\n**How could we select the proper number of `n_components`?**\n\n[Reference 1](https:\/\/www.datasklr.com\/principal-component-analysis-and-factor-analysis\/principal-component-analysis),\n[Reference 2](https:\/\/www.mikulskibartosz.name\/pca-how-to-choose-the-number-of-components\/),\n[Reference 3(Korean)](https:\/\/tobigs.gitbook.io\/tobigs\/data-analysis\/undefined-2\/python-2-4#2.)\n\n\nThere are three widely used methods to decide the number of components to retain.\n\n\n**1. Selection from the Scree plot(Elbow method)**\n\nA scree plot shows the number of components on the X-axis against the proportion of the explained variance on the Y-axis. It uses WCSS(Within Clusters Sum of Squares) that explains the total variation within the clusters according to the increase of the number of principle components. \nThe suggested number of components to keep is where the plot forms an elbow and the curve flattens out.\nBut it seems a little bit ambiguous since the decision from the scree plot is arbitrary.\n\n\n**2. Kaiser's Rule**\n\nAfter `PCA().fit(your data)` we can check each eigenvalue using `.explained_variance_` attribute.\nKaiser's Rule means the minimum eigenvalue to be retained, and that value is the number greater than 1.\nIn my opinion we can implement it by using scree plot at the same time.\n\n\n**3. Percent of variation threshold**\n\nIn this case, we would keep as many principal components as needed to explain at least ***70%*** (or some other threshold) of the total variation in the data. \n\n<br><br>","b83ece95":"<br><br>\n\n***Insight***\n\n\n* **Comparison within 'Living_with' feature**\n    * No meaningful difference according to the value of this feature\n    * Will remove it.\n\n<br>","9d3a4b11":"<br>\n\n#### Percent of variation threshold method","d6becd8a":"<a id = \"3\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Data Cleaning and EDA<\/p>\n\n\nLet's take a look at the data imformation and implement the data cleaning on this section.","4527711f":"<br>\n\nThere is 1893 in Year_Birth. Though this could be reasonable if that person is truly healthy but...\n\nI think I should check the people whose age is more than 100 before dropping Year_Birth column.\n\n<br>","c93091c6":"<br>\n\nThe best **K** for clustering by Elbow method is 4.\n\nLet me do another method, silhouette method.\n\n<br>","7e2f5ea4":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Attributes Information<\/p>**\n\n* **Customer's Info(10)**\n    + ID\n    + Year_Birth\n    + Education\n    + Marital_Status\n    + Income: Customer's yearly household income\n    + Kidhome: Number of children in customer's household\n    + Teenhome: Number of teenagers in customer's household\n    + Dt_Customer: Date of customer's enrollment with the company\n    + Recency: Number of days since customer's last purchase\n    + Complain: 1 if customer complained in the last 2 years, 0 otherwise\n    \n    \n* **Products(6): amount spent on different products in last 2 years.**\n    + MntWines: Amount spent on wine in last 2 years\n    + MntFruits: Amount spent on fruits in last 2 years\n    + MntMeatProducts: Amount spent on meat in last 2 years\n    + MntFishProducts: Amount spent on fish in last 2 years\n    + MntSweetProducts: Amount spent on sweets in last 2 years\n    + MntGoldProds: Amount spent on gold in last 2 years\n\n\n* **Promotion(7)**\n    + NumDealsPurchases: Number of purchases made with a discount\n    + AcceptedCmp1: 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n    + AcceptedCmp2: 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n    + AcceptedCmp3: 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n    + AcceptedCmp4: 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n    + AcceptedCmp5: 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n    + Response: 1 if customer accepted the offer in the last campaign, 0 otherwise\n\n    \n    \n* **Place(4)**\n    + NumWebPurchases: Number of purchases made through the company\u2019s web site\n    + NumCatalogPurchases: Number of purchases made using a catalogue\n    + NumStorePurchases: Number of purchases made directly in stores\n    + NumWebVisitMonth: Number of visits to company\u2019s web site in the last month\n\n<br><br>","47644e4e":"<br>\n\nCreating the new feature \"Cus_for\" to show the number of days the customers being the member of the shop.","f579632b":"<br>\n\n#### Elbow Method and  Kaiser's Rule at the same time.","92b86a53":"<a id = \"4\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Data Preprocessing<\/p>\n\n\nOn this section we will do the data preprocessing for the next step, PCA and Clustering Analysis.\n\n* Label encoding for the categorical features\n\n\n* Standard scaling for the numerical features\n\n\n* Creating a subset data frame for dimensionality reduction","81fdd046":"<br>\n\n### Variation according to \"Age\" feature\n\n* 'Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',  'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos'","52a47ac6":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Profile of Each Cluster<\/p>**\n\n\n* **Cluster 0:**\n    * Income : The people in C0 earn the highest.\n    * Age: Mean age is about 50 and it seems they are at the height of their career. \n    * Is_Parent: 93% in C0 is not parents.\n    * Education: Almost every people in C0 took the higher education.\n    * Family_Size : Relatively smaller than other clusters.\n    * Purchasing Style: Usually visit the store, but if parent they will use the website somewhat more than the people who is not parents.\n    * Promotion: Usually very sensitive to the promotions but the ratio of purchased by promotion is quite low.\n    \n    \n* **Cluster 1:**\n    * Income : The people in C1 look like earing the median income.\n    * Age: Mean age is about 60 and they look like at the time of the retirement from the company.\n    * Is_Parent: 98% are parents. \n    * Education: Almost every people in C1 took the higher education.\n    * Family_Size : The mean value of Family_size is 2.9. and most of their kids are the teenager.\n    * Purchasing Style: Usually visit the store, but if parents they will use the website somewhat more than the people who is not parents.\n    * Promotion: Usually sensitive to the promotions if parents and the ratio of purchased by promotion is higher than the people in C0.\n    \n   \n* **Cluster 2:**\n    * Income : The people in C2 earn the low income.\n    * Age: Mean age is relatively lower than other clusters.\n    * Is_Parent: 88% are parents. \n    * Education: Education level is relatively lower than other clustes.\n    * Family_Size : The mean value of Family_size is more than 3. and most of their kids are below teenager.\n    * Purchasing Style: Usually visit the store even though they visit the website much more than other clusters.\n    * Promotion: Very sensitive to the promotions if parents and the ratio of purchased by promotion is the highest.\n","f6cda5c9":"<br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Simple Summary of Each Cluster<\/p>**\n\n\n**1. Value counting**","b432a883":"<br>\n\n4. Total \"amount\" spent per each cluster.\n\nSpent_All: C2 < C1 < C0\n\nWe can clearly conclude that cluster 0 is the group that earns the highest income and spends the most on this shop.","5452c7b8":"From the above,\n\n1. \"Income\" has 24 null values.\n\n2. \"Dt_customer\" needs to be parsed as DateTime.\n\n3. \"Education\" and \"Marital_Status\" need to be encoded.\n\n\nBecause the number of null values in \"Income\" is just 24, I'd like to simply drop those rows.","a98b6742":"<br><br>\n\nFrom **`n_4`** the cumulative ratio of the explained variance is almost 70%, but exactly saying less than 70%.\n\nTherefore we are able to **`n_components`** as **5** according to our reasoning above.\n\n<br><br>","089dd8d7":"<a id = \"8\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Customers Profiling<\/p>\n\n\n**Profiling**\n\n* Purchasing behavior by clusters\n    * 'Spent_All', 'PurDeal_PurAll_ratio', 'NumWebVisitsMonth', 'NumCatalogPurchases', 'NumDealsPurchases', 'NumStorePurchases', 'NumWebPurchases'\n    \n    \n* Personal information by clusters\n    * 'Income', 'Age', 'Education', 'Is_Parent', Family_Size', 'Kidhome', 'Teenhome'","ca4260e7":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Planning Further Analysis<\/p>**\n\n\n#### Summary of the next step after EDA\n\n\n* **Data Preprocessing**\n    * label encoding for categorical variable.\n    * standardization for numerical variables.\n\n\n* **Dimensionality reduction using PCA**\n    * Exploring the proper number of principal components for clustering\n\n\n* **Clustering**\n    * Exploring the proper number of **`K`** for clustering\n\n\n* **Profiling Customers**\n    * personal information: 'Income', 'Age', 'Education', 'Is_Parent', Family_Size', 'Kidhome', 'Teenhome'\n    * Purchasing behavior: 'Spent_All', 'PurDeal_PurAll_ratio', 'NumWebVisitsMonth', 'NumCatalogPurchases', 'NumDealsPurchases', 'NumStorePurchases', 'NumWebPurchases'","fcdeb694":"### Checking the data information","690fd114":"#### New feature creation","0fdeef1e":"<br>\n\nI'll drop some more columns that are useless for further analysis.\n\n* 'Dt_Customer': We will use 'Cus_for' column instead of it.\n* 'Year_Birth': We will use 'Age' column instead of it.\n* 'ID': useless\n* 'Z_CostContact', 'Z_Revenue': Every row has the same value for these columns.\n* 'Complain': Only 20 rows among 3213 rows has 1 and otherwise 0 hence it is also useless.\n\n<br>","25903a2c":"<br>\n\n#### Determine the proper number of **`K`**\n\n\n1. Elbow method\n\n* Also known as SSE(Sum Sqaured Error) method.\n    * returns distortion score when using `KElbeowVisualizer`, and it is equal to SSE.\n\n\n* Cons: It reflects the distance within the cluster, but doesn't do that between the clusters.","ffa8d407":"<br>\n\nCan you believe the people more than 120 yr-old can generally earn the yearly income range 36,000 ~ 84,000?\n\nOr\n\nCan you believe the people more than 120 yr-old can actively purchase the items on the website?\n\nI'd like to infer the age values in these 3 rows are, in my opinion, wrongly registered hence they should be dropped.\n\n\n\n<br>","b2f220af":"<br>\n\n### Variation according to \"Living_with\" feature\n\n* 'Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',  'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos'","8d3dcc0a":"<a id = \"6\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Clustering<\/p>\n\n\n**Clustering**\n\n\nClustering is one of the most popular unsupervised technics in identifying groups in marketing area.\n\nHere we practice clustering process following these 3 steps.\n\n\n* **How to determine the best optimal number of clusters**\n    * **`Elbow method`**: Calculating the Sum Sqaured Error according to the number of K\n    * **`Silhouette method`**[Link](https:\/\/towardsdatascience.com\/silhouette-coefficient-validating-clustering-techniques-e976bb81d10c): Calculating the Silhouette score according to the number of K\n\n\n* **Clustering**\n    * **`Agglomerative(hierachical)`**: It is generally useful for the data such as geographical data to group residents by Block, City, or State because it performs operations sequentially.\n    * **`K-Means`**: It is generally useful for the data such as customers' data because it divides the data into the K sets simultaneously.\n    * Therefore we will implement **`K Means Clustering`** for this dataset.\n    \n    \n* **Visualization**\n    * We can visualize the result of clustering and evaluate how it is clustered well.","742edf19":"<br><br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Purchasing Behavior of Each Cluster<\/p>**\n\nOn this section, I'd like to examine the purchasing style and sensitivity to promotion as per each cluster.\n\nI utilized the ```Is_parent``` feature to do so because this feature is one of the most important features to relate with the other features to explain our customers.\n\n\n* Purchasing style: 'NumWebVisitsMonth', 'NumCatalogPurchases', 'NumStorePurchases', 'NumWebPurchases'\n\n\n* Sensitivity to promotion: 'NumDealsPurchases', 'PurDeal_PurAll_ratio', 'Total_Promos'","2de95475":"<a id = \"7\"><\/a>\n# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Evaluating Models<\/p>\n\n\n","4b880159":"<br>\n\n### **<p style=\"background-color:white;font-family:tahoma;color:navy;font-size:120%;text-align:left\">Data Cleaning<\/p>**","06d0fc78":"<br>\n\nUsing 5 PCs as a trial. ","c2ea4a2b":"<br>\n\n### Variation according to \"Education\" feature\n\n* 'Income', 'Recency', 'Cus_for', 'Age', 'Spent_All',  'PurchaseNumAll', 'PurDeal_PurAll_ratio', 'Total_Promos'","5d5403ce":"<br>\n\n#### New features creation\n\n**Ideation about creating new features and preprocessing the current features**\n+ Year_Birth: We can create the age column\n+ Education: {Basic: Basic Education, Graduation: Graduated from Univ., Post graduate : (2n Cycle + Master + PhD)}\n+ Marital_Status: Changed the name of columns as 'Living_with', and mapped by {(Married + Together) : Partner, otherwise : Alone}\n+ Children = Kidhome + Teenhome\n+ Spent_All: adding all features staring Mnt\n+ PurchaseNumAll = all featrues staring Num\n+ PurDeal_PurAll_ratio: propotion of NumDealPurchases out of PurchaseNumAll\n+ Total_Promos = all featrues staring Accepted + Response\n+ Family_Size = ({\"Alone\": 1, \"Partner\":2})+ data[\"Children\"]\n+ Is_Parent = (data.Children> 0, 1, 0)\n+ Drop Dt_Customer, ID, Year_Birth, Z_CostContact, Z_Revenue","fb763817":"<br>\n\n#### Selection of the number of principal components "}}