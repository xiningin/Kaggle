{"cell_type":{"f610f8e2":"code","01c40258":"code","cda6160c":"code","0330ff74":"code","09599707":"code","8d52b166":"code","ff7da317":"code","50443a82":"code","69dee7bb":"code","027aea79":"code","957ce004":"code","ef701219":"code","0eb8678b":"code","116ac623":"code","f408cf90":"code","59673f52":"code","27ff8a66":"code","d4b6088d":"code","7832dc7a":"code","e76965de":"code","b321f151":"code","51a4922f":"code","95999f56":"code","5445c608":"code","0e479ddc":"code","e9c87cdd":"code","63fe1755":"code","49cf432e":"code","72df0ace":"code","5fbf46ba":"code","33be0cc4":"code","5d68fde3":"code","da39d4c3":"code","78017a89":"code","906c7eaa":"code","9bab5f0c":"code","90956a68":"code","8a9a90c9":"code","3c0f3fdb":"code","09723131":"code","c9a35add":"code","54234593":"code","7cf24048":"code","8879322b":"code","56fed8d1":"code","ab39068f":"code","4338f281":"code","7efd1f04":"code","05831041":"code","9af926ef":"code","2b641527":"code","df95aba0":"code","b1a5faed":"code","d65ef95e":"code","fdb79677":"code","2d887709":"code","5cc401ce":"code","dec072fe":"code","75e46472":"code","f79b7757":"code","47cc773d":"code","c0155a90":"code","c60344da":"code","45c32f34":"code","b40b5119":"code","d14bec17":"code","b3f24def":"code","82abf8eb":"markdown","4e8a696a":"markdown","1518524c":"markdown","183fa507":"markdown","7f8cdae3":"markdown","46cac193":"markdown","2a9bf340":"markdown"},"source":{"f610f8e2":"# import data manipulation library\nimport numpy as np\nimport pandas as pd\n\n# import data visualization library\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# import scientific computing library\nimport scipy\n\n# import sklearn data preprocessing\nfrom sklearn.preprocessing import RobustScaler\n\n# import sklearn model class\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n\n# import xgboost model class\nimport xgboost as xgb\n\n# import sklearn model selection\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n\n# import sklearn model evaluation regression metrics\nfrom sklearn.metrics import mean_squared_error","01c40258":"# acquiring training and testing data\ndf_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')","cda6160c":"# visualize head of the training data\ndf_train.head(n=5)","0330ff74":"# visualize tail of the testing data\ndf_test.tail(n=5)","09599707":"# combine training and testing dataframe\ndf_train['DataType'], df_test['DataType'] = 'training', 'testing'\ndf_test.insert(df_test.shape[1] - 1, 'SalePrice', np.nan)\ndf_data = pd.concat([df_train, df_test], ignore_index=True)","8d52b166":"def boxplot(categorical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a box plot applied for categorical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        categorical_x (list or str): The categorical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    categorical_x, numerical_y = [categorical_x] if type(categorical_x) == str else categorical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(categorical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.boxplot(x=vj, y=vi, data=data, ax=axes[i*len(categorical_x) + j]) for i, vi in enumerate(numerical_y) for j, vj in enumerate(categorical_x)]\n    return fig","ff7da317":"def scatterplot(numerical_x: list or str, numerical_y: list or str, data: pd.DataFrame, figsize: tuple = (4, 3), ncols: int = 5, nrows: int = None) -> plt.figure:\n    \"\"\" Return a scatter plot applied for numerical variable in x-axis vs numerical variable in y-axis.\n    \n    Args:\n        numerical_x (list or str): The numerical variable in x-axis.\n        numerical_y (list or str): The numerical variable in y-axis.\n        data (pd.DataFrame): The data to plot.\n        figsize (tuple): The matplotlib figure size width and height in inches. Default to (4, 3).\n        ncols (int): The number of columns for axis in the figure. Default to 5.\n        nrows (int): The number of rows for axis in the figure. Default to None.\n    \n    Returns:\n        plt.figure: The plot figure.\n    \"\"\"\n    \n    numerical_x, numerical_y = [numerical_x] if type(numerical_x) == str else numerical_x, [numerical_y] if type(numerical_y) == str else numerical_y\n    if nrows is None: nrows = (len(numerical_x)*len(numerical_y) - 1) \/\/ ncols + 1\n    \n    fig, axes = plt.subplots(figsize=(figsize[0]*ncols , figsize[1]*nrows), ncols=ncols, nrows=nrows)\n    axes = axes.flatten()\n    _ = [sns.scatterplot(x=vj, y=vi, data=data, ax=axes[i*len(numerical_x) + j], rasterized=True) for i, vi in enumerate(numerical_y) for j, vj in enumerate(numerical_x)]\n    return fig","50443a82":"# describe training and testing data\ndf_data.describe(include='all')","69dee7bb":"# convert dtypes numeric to object\ncol_convert = ['MSSubClass']\ndf_data[col_convert] = df_data[col_convert].astype('object')","027aea79":"# list all features type number\ncol_number = df_data.select_dtypes(include=['number']).columns.tolist()\nprint('features type number:\\n items %s\\n length %d' %(col_number, len(col_number)))\n\n# list all features type object\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\nprint('features type object:\\n items %s\\n length %d' %(col_object, len(col_object)))","957ce004":"# feature exploration: histogram of all numeric features\n_ = df_data.hist(bins=20, figsize=(20, 15))","ef701219":"# feature extraction: sale price\ndf_data['SalePrice'] = np.log1p(df_data['SalePrice'])","0eb8678b":"# feature extraction: value of miscellaneous feature\ndf_data['MiscVal'] = np.log1p(df_data['MiscVal'])","116ac623":"# feature exploration: sale price\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['Id']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\n_ = scatterplot(col_number, 'SalePrice', df_data[df_data['DataType'] == 'training'])\n_ = boxplot(col_object, 'SalePrice', df_data[df_data['DataType'] == 'training'])","f408cf90":"# feature exploration: lot frontage\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['Id']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\n_ = scatterplot(col_number, 'LotFrontage', df_data)\n_ = boxplot(col_object, 'LotFrontage', df_data)","59673f52":"# feature extraction: lot frontage\ndf_data['LotFrontage'] = df_data['LotFrontage'].fillna(df_data.groupby(['Neighborhood'])['LotFrontage'].transform('mean'))\ndf_data.loc[(df_data['LotFrontage'] > 200) & (df_data['DataType'] == 'trainnig'), 'DataType'] = 'excluded'","27ff8a66":"# feature extraction: lot area\ndf_data.loc[(df_data['LotArea'] > 100000) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","d4b6088d":"# feature extraction: basement type 1 finished area square feet\ndf_data.loc[(df_data['BsmtFinSF1'] > 4000) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","7832dc7a":"# feature extraction: basement total area square feet\ndf_data.loc[(df_data['TotalBsmtSF'] > 5000) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","e76965de":"# feature extraction: first floor area square feet\ndf_data.loc[(df_data['1stFlrSF'] > 4000) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","b321f151":"# feature extraction: above grade (ground) living area square feet\ndf_data.loc[(df_data['GrLivArea'] > 4500) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","51a4922f":"# feature extraction: open porch area square feet\ndf_data.loc[(df_data['OpenPorchSF'] > 500) & (df_data['SalePrice'] < 11) & (df_data['DataType'] == 'training'), 'DataType'] = 'excluded'","95999f56":"# feature extraction: all features related to area\ncol_convert = ['LotArea', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF',\n               '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea', 'GarageArea',\n               'WoodDeckSF', 'OpenPorchSF', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea']\ndf_data[col_convert] = df_data[col_convert].fillna(0)\ndf_data['TotalSF'] = df_data['TotalBsmtSF'] + df_data['GrLivArea']\ndf_data['TotalPorch'] = df_data['OpenPorchSF'] + df_data['EnclosedPorch'] + df_data['3SsnPorch'] + df_data['ScreenPorch']\ndf_data['TotalArea'] = df_data['TotalSF'] + df_data['TotalPorch'] + df_data['GarageArea'] + df_data['WoodDeckSF']","5445c608":"# feature extraction: all features related to room\ncol_convert = ['BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n               'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd']\ndf_data[col_convert] = df_data[col_convert].fillna(0)\ndf_data['TotalBathBsmt'] = df_data['BsmtFullBath'] + 0.5 * df_data['BsmtHalfBath']\ndf_data['TotalBathAbvGrd'] = df_data['FullBath'] + 0.5 * df_data['HalfBath']\ndf_data['TotalRmsAbvGrdIncBath'] = df_data['TotRmsAbvGrd'] + df_data['TotalBathAbvGrd']\ndf_data['TotalRms'] = df_data['TotalRmsAbvGrdIncBath'] + df_data['TotalBathBsmt']","0e479ddc":"# feature extraction: total area per rooms\ndf_data['AreaPerRmsBsmt'] = df_data['TotalBsmtSF'] \/ (df_data['TotalBathBsmt'] + 1)\ndf_data['AreaPerRmsGrLivAbvGrd'] = df_data['GrLivArea'] \/ (df_data['TotalRmsAbvGrdIncBath'] + 1)\ndf_data['AreaPerRmsTotal'] = df_data['TotalSF'] \/ (df_data['TotalRms'] + 1)","e9c87cdd":"# feature extraction: all features related to quality and condition\ncol_convert = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'HeatingQC',\n               'KitchenQual', 'FireplaceQu', 'GarageQual', 'GarageCond', 'PoolQC']\ndf_data[col_convert] = df_data[col_convert].replace('Ex', 5).replace('Gd', 4).replace('TA', 3).replace('Fa', 2).replace('Po', 1).replace('NA', 0)\ndf_data[col_convert] = df_data[col_convert].fillna(0).astype(int)\ndf_data['ExterQualCond'] = df_data['ExterQual'] * df_data['ExterCond']\ndf_data['BsmtQualCond'] = df_data['BsmtQual'] * df_data['BsmtCond']\ndf_data['GarageQualCond'] = df_data['GarageQual'] * df_data['GarageCond']\ndf_data['OverallQualCond'] = df_data['OverallQual'] * df_data['OverallCond']","63fe1755":"# feature extraction: all features related to exposure\ncol_convert = ['BsmtExposure']\ndf_data[col_convert] = df_data[col_convert].replace('Gd', 4).replace('Av', 3).replace('Mn', 2).replace('No', 1).replace('NA', 0)\ndf_data[col_convert] = df_data[col_convert].fillna(0).astype(int)","49cf432e":"# feature extraction: all features related to basement finished\ncol_convert = ['BsmtFinType1', 'BsmtFinType2']\ndf_data[col_convert] = df_data[col_convert].replace('GLQ', 6).replace('ALQ', 5).replace('BLQ', 4).replace('Rec', 3).replace('LwQ', 2).replace('Unf', 1).replace('NA', 0)\ndf_data[col_convert] = df_data[col_convert].fillna(0).astype(int)","72df0ace":"# feature extraction: all features related to garage finished\ncol_convert = ['GarageFinish']\ndf_data[col_convert] = df_data[col_convert].replace('Fin', 3).replace('RFn', 2).replace('Unf', 1).replace('NA', 0)\ndf_data[col_convert] = df_data[col_convert].fillna(0).astype(int)","5fbf46ba":"# feature extraction: all features related to fence\ncol_convert = ['Fence']\ndf_data[col_convert] = df_data[col_convert].replace('GdPrv', 4).replace('MnPrv', 3).replace('GdWo', 2).replace('MnWw', 1).replace('NA', 0)\ndf_data[col_convert] = df_data[col_convert].fillna(0).astype(int)","33be0cc4":"# feature extraction: all features related to year\ndf_data['GarageYrBlt'] = df_data['GarageYrBlt'].fillna(df_data['YearBuilt'])\ndf_data['YearBuiltRemod'] = df_data['YearRemodAdd'] - df_data['YearBuilt']\ndf_data['YearBuiltSold'] = df_data['YrSold'] - df_data['YearBuilt']\ndf_data['YearRemodSold'] = df_data['YrSold'] - df_data['YearRemodAdd']\ndf_data['YearGarageSold'] = df_data['YrSold'] - df_data['GarageYrBlt']","5d68fde3":"# feature exploration: season dataframe\ndf_season = df_data.loc[df_data['DataType'] == 'training'].groupby(['YrSold', 'MoSold'], as_index=False).agg({\n    'SalePrice': 'mean'\n})\nfig, axes = plt.subplots(figsize=(20, 3))\n_ = sns.pointplot(x='MoSold', y='SalePrice', data=df_season, join=True, hue='YrSold')","da39d4c3":"# feature extraction: fillna on type of utilities available\ndf_data['Utilities'] = df_data['Utilities'].fillna('ELO')","78017a89":"# feature extraction: fillna on type of sale\ndf_data['SaleType'] = df_data['SaleType'].fillna('Oth')","906c7eaa":"# feature extraction: fillna with repetitive\ncol_fillnas = ['MSZoning', 'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Electrical', 'Functional']\nfor col_fillna in col_fillnas: df_data[col_fillna] = df_data[col_fillna].fillna(df_data[col_fillna].value_counts().idxmax())","9bab5f0c":"# feature extraction: fillna with na\ncol_fillnas = ['Alley', 'GarageType']\nfor col_fillna in col_fillnas: df_data[col_fillna] = df_data[col_fillna].fillna('NA')","90956a68":"# feature extraction: fillna with 0\ncol_fillnas = ['GarageCars', 'MiscFeature']\ndf_data[col_fillnas] = df_data[col_fillnas].fillna(0)","8a9a90c9":"# verify and print columns contain nan\nif df_data.isna().any().any(): print(df_data.loc[:, df_data.columns[df_data.isna().any()].tolist()].describe(include='all'))\nelse: print('no null entry')","3c0f3fdb":"# feature extraction: apply log1p transform for all high skewness numeric features\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['MiscVal', 'SalePrice', 'YearBuiltRemod', 'YearBuiltSold', 'YearRemodSold', 'YearGarageSold']).tolist()\nfor col_transform in col_number:\n    skewness = scipy.stats.skew(df_data[col_transform].dropna())\n    if skewness > 0.75: df_data[col_transform] = np.log1p(df_data[col_transform])","09723131":"# feature exploration: sale price\ncol_number = df_data.select_dtypes(include=['number']).columns.drop(['Id']).tolist()\ncol_object = df_data.select_dtypes(include=['object']).columns.tolist()\n_ = scatterplot(col_number, 'SalePrice', df_data[df_data['DataType'] == 'training'])\n_ = boxplot(col_object, 'SalePrice', df_data[df_data['DataType'] == 'training'])","c9a35add":"# feature extraction: sale price\ndf_data['SalePrice'] = df_data['SalePrice'].fillna(0)","54234593":"# convert category codes for data dataframe\ndf_data = pd.get_dummies(df_data, columns=None, drop_first=True)","7cf24048":"# describe data dataframe\ndf_data.describe(include='all')","8879322b":"# verify dtypes object\ndf_data.info()","56fed8d1":"# compute pairwise correlation of columns, excluding NA\/null values and present through heat map\ncorr = df_data[df_data['DataType_training'] == 1].corr()\nfig, axes = plt.subplots(figsize=(200, 150))\nheatmap = sns.heatmap(corr, annot=True, cmap=plt.cm.RdBu, fmt='.1f', square=True, vmin=-0.8, vmax=0.8)","ab39068f":"# select all features to evaluate the feature importances\nx = df_data[df_data['DataType_training'] == 1].drop(['Id', 'SalePrice', 'DataType_training', 'DataType_testing'], axis=1)\ny = df_data.loc[df_data['DataType_training'] == 1, 'SalePrice']","4338f281":"# set up lasso regression to find the feature importances\nlassoreg = Lasso(alpha=1e-5).fit(x, y)\nfeat = pd.DataFrame(data=lassoreg.coef_, index=x.columns, columns=['FeatureImportances']).sort_values(['FeatureImportances'], ascending=False)","7efd1f04":"# plot the feature importances\nfeat[(feat['FeatureImportances'] < -1e-3) | (feat['FeatureImportances'] > 1e-3)].dropna().plot(y='FeatureImportances', figsize=(20, 5), kind='bar')\nplt.axhline(-0.005, color=\"grey\")\nplt.axhline(0.005, color=\"grey\")","05831041":"# list feature importances\nmodel_feat = feat[(feat['FeatureImportances'] < -0.005) | (feat['FeatureImportances'] > 0.005)].index","9af926ef":"# select the important features\nx = df_data.loc[df_data['DataType_training'] == 1, model_feat]\ny = df_data.loc[df_data['DataType_training'] == 1, 'SalePrice']","2b641527":"# create scaler to the features\nscaler = RobustScaler()\nx = scaler.fit_transform(x)","df95aba0":"# perform train-test (validate) split\nx_train, x_validate, y_train, y_validate = train_test_split(x, y, test_size=0.25, random_state=58)","b1a5faed":"# linear regression model setup\nmodel_linreg = LinearRegression()\n\n# linear regression model fit\nmodel_linreg.fit(x_train, y_train)\n\n# linear regression model prediction\nmodel_linreg_ypredict = model_linreg.predict(x_validate)\n\n# linear regression model metrics\nmodel_linreg_rmse = mean_squared_error(y_validate, model_linreg_ypredict) ** 0.5\nmodel_linreg_cvscores = np.sqrt(np.abs(cross_val_score(model_linreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('linear regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_linreg_rmse, model_linreg_cvscores.mean(), 2 * model_linreg_cvscores.std()))","d65ef95e":"# lasso regression model setup\nmodel_lassoreg = Lasso(alpha=0.001, max_iter=1024)\n\n# lasso regression model fit\nmodel_lassoreg.fit(x_train, y_train)\n\n# lasso regression model prediction\nmodel_lassoreg_ypredict = model_lassoreg.predict(x_validate)\n\n# lasso regression model metrics\nmodel_lassoreg_rmse = mean_squared_error(y_validate, model_lassoreg_ypredict) ** 0.5\nmodel_lassoreg_cvscores = np.sqrt(np.abs(cross_val_score(model_lassoreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('lasso regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_lassoreg_rmse, model_lassoreg_cvscores.mean(), 2 * model_lassoreg_cvscores.std()))","fdb79677":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, 4, base=10, num=9),\n          'max_iter': [1024],\n}\n\n# lasso regression grid search model setup\nmodel_lassoreg_cv = GridSearchCV(model_lassoreg, params, iid=False, cv=5)\n\n# lasso regression grid search model fit\nmodel_lassoreg_cv.fit(x_train, y_train)\n\n# lasso regression grid search model prediction\nmodel_lassoreg_cv_ypredict = model_lassoreg_cv.predict(x_validate)\n\n# lasso regression grid search model metrics\nmodel_lassoreg_cv_rmse = mean_squared_error(y_validate, model_lassoreg_cv_ypredict) ** 0.5\nmodel_lassoreg_cv_cvscores = np.sqrt(np.abs(cross_val_score(model_lassoreg_cv, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('lasso regression grid search\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_lassoreg_cv_rmse, model_lassoreg_cv_cvscores.mean(), 2 * model_lassoreg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_lassoreg_cv.best_params_)","2d887709":"# ridge regression model setup\nmodel_ridgereg = Ridge(alpha=10)\n\n# ridge regression model fit\nmodel_ridgereg.fit(x_train, y_train)\n\n# ridge regression model prediction\nmodel_ridgereg_ypredict = model_ridgereg.predict(x_validate)\n\n# ridge regression model metrics\nmodel_ridgereg_rmse = mean_squared_error(y_validate, model_ridgereg_ypredict) ** 0.5\nmodel_ridgereg_cvscores = np.sqrt(np.abs(cross_val_score(model_ridgereg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('ridge regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_ridgereg_rmse, model_ridgereg_cvscores.mean(), 2 * model_ridgereg_cvscores.std()))","5cc401ce":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, 4, base=10, num=9)}\n\n# ridge regression grid search model setup\nmodel_ridgereg_cv = GridSearchCV(model_ridgereg, params, iid=False, cv=5)\n\n# ridge regression grid search model fit\nmodel_ridgereg_cv.fit(x_train, y_train)\n\n# ridge regression grid search model prediction\nmodel_ridgereg_cv_ypredict = model_ridgereg_cv.predict(x_validate)\n\n# ridge regression grid search model metrics\nmodel_ridgereg_cv_rmse = mean_squared_error(y_validate, model_ridgereg_cv_ypredict) ** 0.5\nmodel_ridgereg_cv_cvscores = np.sqrt(np.abs(cross_val_score(model_ridgereg_cv, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('ridge regression grid search\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_ridgereg_cv_rmse, model_ridgereg_cv_cvscores.mean(), 2 * model_ridgereg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_ridgereg_cv.best_params_)","dec072fe":"# elastic net regression model setup\nmodel_elasticnetreg = ElasticNet(alpha=0.1, l1_ratio=0.1, max_iter=1024)\n\n# elastic net regression model fit\nmodel_elasticnetreg.fit(x_train, y_train)\n\n# elastic net regression model prediction\nmodel_elasticnetreg_ypredict = model_elasticnetreg.predict(x_validate)\n\n# elastic net regression model metrics\nmodel_elasticnetreg_rmse = mean_squared_error(y_validate, model_elasticnetreg_ypredict) ** 0.5\nmodel_elasticnetreg_cvscores = np.sqrt(np.abs(cross_val_score(model_elasticnetreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('elastic net regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_elasticnetreg_rmse, model_elasticnetreg_cvscores.mean(), 2 * model_elasticnetreg_cvscores.std()))","75e46472":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, 4, base=10, num=9),\n          'l1_ratio': np.linspace(0.1, 0.9, num=5),\n          'max_iter': [1024],\n}\n\n# elastic net regression grid search model setup\nmodel_elasticnetreg_cv = GridSearchCV(model_elasticnetreg, params, iid=False, cv=5)\n\n# elastic net regression grid search model fit\nmodel_elasticnetreg_cv.fit(x_train, y_train)\n\n# elastic net regression grid search model prediction\nmodel_elasticnetreg_cv_ypredict = model_elasticnetreg_cv.predict(x_validate)\n\n# elastic net regression grid search model metrics\nmodel_elasticnetreg_cv_rmse = mean_squared_error(y_validate, model_elasticnetreg_cv_ypredict) ** 0.5\nmodel_elasticnetreg_cv_cvscores = np.sqrt(np.abs(cross_val_score(model_elasticnetreg_cv, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('elastic net regression grid search\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_elasticnetreg_cv_rmse, model_elasticnetreg_cv_cvscores.mean(), 2 * model_elasticnetreg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_elasticnetreg_cv.best_params_)","f79b7757":"# kernel ridge regression model setup\nmodel_kernelridgereg = KernelRidge(alpha=0.1, kernel='polynomial', degree=2)\n\n# kernel ridge regression model fit\nmodel_kernelridgereg.fit(x_train, y_train)\n\n# kernel ridge regression model prediction\nmodel_kernelridgereg_ypredict = model_kernelridgereg.predict(x_validate)\n\n# kernel ridge regression model metrics\nmodel_kernelridgereg_rmse = mean_squared_error(y_validate, model_kernelridgereg_ypredict) ** 0.5\nmodel_kernelridgereg_cvscores = np.sqrt(np.abs(cross_val_score(model_kernelridgereg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('kernel ridge regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_kernelridgereg_rmse, model_kernelridgereg_cvscores.mean(), 2 * model_kernelridgereg_cvscores.std()))","47cc773d":"# specify the hyperparameter space\nparams = {'alpha': np.logspace(-4, 4, base=10, num=9),\n          'degree': [1, 2, 3, 4, 5],\n}\n\n# kernel ridge regression grid search model setup\nmodel_kernelridgereg_cv = GridSearchCV(model_kernelridgereg, params, iid=False, cv=5)\n\n# kernel ridge regression grid search model fit\nmodel_kernelridgereg_cv.fit(x_train, y_train)\n\n# kernel ridge regression grid search model prediction\nmodel_kernelridgereg_cv_ypredict = model_kernelridgereg_cv.predict(x_validate)\n\n# kernel ridge regression grid search model metrics\nmodel_kernelridgereg_cv_rmse = mean_squared_error(y_validate, model_kernelridgereg_cv_ypredict) ** 0.5\nmodel_kernelridgereg_cv_cvscores = np.sqrt(np.abs(cross_val_score(model_kernelridgereg_cv, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('kernel ridge regression grid search\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_kernelridgereg_cv_rmse, model_kernelridgereg_cv_cvscores.mean(), 2 * model_kernelridgereg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_kernelridgereg_cv.best_params_)","c0155a90":"# decision tree regression model setup\nmodel_treereg = DecisionTreeRegressor(splitter='best', min_samples_split=5)\n\n# decision tree regression model fit\nmodel_treereg.fit(x_train, y_train)\n\n# decision tree regression model prediction\nmodel_treereg_ypredict = model_treereg.predict(x_validate)\n\n# decision tree regression model metrics\nmodel_treereg_rmse = mean_squared_error(y_validate, model_treereg_ypredict) ** 0.5\nmodel_treereg_cvscores = np.sqrt(np.abs(cross_val_score(model_treereg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('decision tree regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_treereg_rmse, model_treereg_cvscores.mean(), 2 * model_treereg_cvscores.std()))","c60344da":"# random forest regression model setup\nmodel_forestreg = RandomForestRegressor(n_estimators=100, min_samples_split=3, random_state=58)\n\n# random forest regression model fit\nmodel_forestreg.fit(x_train, y_train)\n\n# random forest regression model prediction\nmodel_forestreg_ypredict = model_forestreg.predict(x_validate)\n\n# random forest regression model metrics\nmodel_forestreg_rmse = mean_squared_error(y_validate, model_forestreg_ypredict) ** 0.5\nmodel_forestreg_cvscores = np.sqrt(np.abs(cross_val_score(model_forestreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('random forest regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestreg_rmse, model_forestreg_cvscores.mean(), 2 * model_forestreg_cvscores.std()))","45c32f34":"# specify the hyperparameter space\nparams = {'n_estimators': [100],\n          'max_depth': [10, 20, None],\n          'min_samples_split': [3, 5, 7, 9],\n          'random_state': [58],\n}\n\n# random forest regression grid search model setup\nmodel_forestreg_cv = GridSearchCV(model_forestreg, params, iid=False, cv=5)\n\n# random forest regression grid search model fit\nmodel_forestreg_cv.fit(x_train, y_train)\n\n# random forest regression grid search model prediction\nmodel_forestreg_cv_ypredict = model_forestreg_cv.predict(x_validate)\n\n# random forest regression grid search model metrics\nmodel_forestreg_cv_rmse = mean_squared_error(y_validate, model_forestreg_cv_ypredict) ** 0.5\nmodel_forestreg_cv_cvscores = np.sqrt(np.abs(cross_val_score(model_forestreg_cv, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('random forest regression grid search\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_forestreg_cv_rmse, model_forestreg_cv_cvscores.mean(), 2 * model_forestreg_cv_cvscores.std()))\nprint('  best parameters: %s' %model_forestreg_cv.best_params_)","b40b5119":"# xgboost regression model setup\nmodel_xgbreg = xgb.XGBRegressor(max_depth=5, learning_rate=0.1, n_estimators=1000, objective='reg:linear', booster='gbtree',\n                                gamma=0, subsample=0.9, colsample_bytree=0.9, reg_alpha=0.1, reg_lambda=0.9, random_state=58)\n\n# xgboost regression model fit\nmodel_xgbreg.fit(x_train, y_train, eval_set=[(x_train, y_train), (x_validate, y_validate)], early_stopping_rounds=50, verbose=False, callbacks=[xgb.callback.print_evaluation(period=50)])\n\n# xgboost regression model prediction\nmodel_xgbreg_ypredict = model_xgbreg.predict(x_validate)\n\n# xgboost regression model metrics\nmodel_xgbreg_rmse = mean_squared_error(y_validate, model_xgbreg_ypredict) ** 0.5\nmodel_xgbreg_cvscores = np.sqrt(np.abs(cross_val_score(model_xgbreg, x, y, cv=5, scoring='neg_mean_squared_error')))\nprint('xgboost regression\\n  root mean squared error: %0.4f, cross validation score: %0.4f (+\/- %0.4f)' %(model_xgbreg_rmse, model_xgbreg_cvscores.mean(), 2 * model_xgbreg_cvscores.std()))","d14bec17":"# model selection\nfinal_model = model_kernelridgereg\n\n# prepare testing data and compute the observed value\nx_test = df_data.loc[df_data['DataType_testing'] == 1, model_feat]\nx_test = scaler.transform(x_test)\ny_test = pd.DataFrame(final_model.predict(x_test), columns=['SalePrice'], index=df_data.loc[df_data['DataType_testing'] == 1, 'Id'])\ny_test['SalePrice'] = np.expm1(y_test['SalePrice'])","b3f24def":"# submit the results\nout = pd.DataFrame({'Id': y_test.index, 'SalePrice': y_test['SalePrice']})\nout.to_csv('submission.csv', index=False)","82abf8eb":"> **Supply or submit the results**\n\nOur submission to the competition site Kaggle is ready. Any suggestions to improve our score are welcome.","4e8a696a":"> **Problem overview**\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.","1518524c":"After extracting all features, it is required to convert category features to numerics features, a format suitable to feed into our Machine Learning models.","183fa507":"> **Feature exploration, engineering and cleansing**\n\nHere we generate descriptive statistics that summarize the central tendency, dispersion and shape of a dataset\u2019s distribution together with exploring some data.","7f8cdae3":"> **Acquiring training and testing data**\n\nWe start by acquiring the training and testing datasets into Pandas DataFrames.","46cac193":"> **Analyze and identify patterns by visualizations**\n\nLet us generate some correlation plots of the features to see how related one feature is to the next. To do so, we will utilize the Seaborn plotting package which allows us to plot very conveniently as follows.\n\nThe Pearson Correlation plot can tell us the correlation between features with one another. If there is no strongly correlated between features, this means that there isn't much redundant or superfluous data in our training data. This plot is also useful to determine which features are correlated to the observed value.\n\nThe pairplots is also useful to observe the distribution of the training data from one feature to the other.\n\nThe pivot table is also another useful method to observe the impact between features.","2a9bf340":"> **Model, predict and solve the problem**\n\nNow, it is time to feed the features to Machine Learning models."}}