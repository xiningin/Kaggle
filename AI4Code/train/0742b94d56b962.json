{"cell_type":{"b0f07bbf":"code","de9801ed":"code","c90fe988":"code","5e642d8c":"code","7b1e8263":"code","9f091b0c":"code","a1cf1019":"code","b987abf2":"code","d4f178c0":"code","a1ae5070":"code","880c9895":"code","6f042fc3":"code","bcc97826":"code","2a73e06e":"code","55d9cfbc":"code","f5be133e":"code","ba7a016f":"code","ad67b9bc":"code","7d7549f7":"code","9ce67951":"code","0624039a":"code","19b7b0b2":"code","492496a2":"code","b54fecd5":"code","1f1afa61":"code","954836ca":"code","06653222":"code","ecd82473":"code","45ec3533":"code","b7572420":"code","19a3baa1":"code","100fe1a3":"code","9d0adffd":"code","7734fc5d":"code","3c382642":"code","1073d199":"code","65a13f79":"code","464205a9":"code","84ff509b":"code","5661a9b3":"code","d9c2071e":"code","95e01469":"code","51dd4eea":"code","93b04e51":"code","be2ed808":"code","7313a240":"code","b637661b":"code","0a156c88":"markdown","5f30c5b7":"markdown","bc71e25d":"markdown","d6aa8b47":"markdown","20bfa442":"markdown","df633bcb":"markdown","b87fcaa7":"markdown","e8008af6":"markdown","253a0d7c":"markdown","73602ba3":"markdown","1f0caf10":"markdown","4abb4739":"markdown","a3196393":"markdown","7afc0583":"markdown"},"source":{"b0f07bbf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","de9801ed":"# import the python libraries needed \n# data preparation, analysis & cleaning \nimport numpy as np\nimport pandas as pd\n# visualization libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# machine learning libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC\n# metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score","c90fe988":"# gender submission \ngender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\n# train data set\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n# test data set\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")","5e642d8c":"# import pandas_profiling to geneate profile report of the train dataset\nimport pandas_profiling\n# generating the report\ntrain.profile_report()","7b1e8263":"# the shape and size of the dataset\nprint(\"Train data shape:\", train.shape)\nprint(\"Test data shape:\", test.shape)","9f091b0c":"# brief description of the data sets\ntrain.info()\nprint(\"\\n\")\ntest.info()","a1cf1019":"# dealing with missing values\ntrain['Age'].fillna(train['Age'].mean(), inplace = True)\ntest['Age'].fillna(test['Age'].mean(), inplace = True)\n# column embarked has missing values that will replace with mode after feature engineering","b987abf2":"# Checking for missing values \nprint(train.isnull().sum())\nprint('\\n')\ntest.isnull().sum()","d4f178c0":"# sns.boxplot(train['Age'], color = 'red')\nplt.figure(figsize = (15,11))\nsns.boxplot(train['Embarked'],train['Age'] , hue = train['Sex'], palette = 'Accent')\nplt.ylabel('Age Distribution', fontsize = 20, color = 'blue')\nplt.xticks(fontsize = 15)\nplt.xlabel('Embarked',fontsize = 20, color = 'blue')\nplt.title('Boxplot showing the outliers per Port of Embarkation', fontsize = 22, color = 'blue')\nplt.show()","a1ae5070":"# dropping the unnecessary colums \ntrain.drop(['Name', 'Ticket','Fare','Cabin'], axis = 1, inplace = True)\ntest.drop(['Name', 'Ticket','Fare','Cabin'],axis = 1, inplace = True)","880c9895":"# the distribution of age \nage = train['Age']\n# plotting a histogram \nplt.figure(figsize=(13,7), dpi= 80)\n# plt.hist(age, bins = 10, histtype = 'bar', rwidth = 0.9, color = 'black')\nsns.distplot(train['Age'], color = \"darkgreen\", label = \"Age\" )\n# labeling the axis \nplt.ylabel('Frequency', fontsize = 20)\nplt.xlabel('Passengers Age', fontsize = 20)\n# title\nplt.title('Passenger Age Distribution', fontsize = 22)\nplt.show()","6f042fc3":"# distribution of Age of the different genders in the ship\nplt.figure(figsize=(13,7), dpi= 80)\nsns.kdeplot(train.loc[train['Sex'] == 'male', \"Age\"], shade=True, color=\"red\", label=\"Male\", alpha=.7)\nsns.kdeplot(train.loc[train['Sex'] == 'female', \"Age\"], shade=True, color=\"green\", label=\"Female\", alpha=.7)\nplt.title('Distribution of Age as per respective gender', fontsize = 22, color = 'blue')\nplt.ylabel('Frequency', fontsize = 20, color = 'blue')\nplt.xlabel('Passenger Age', fontsize = 20, color = 'blue')\nplt.show()","bcc97826":"#Creating Visualisation of passengers who survived or died \ndef plot(passenger_info):\n    survived = train[train.Survived == 1][passenger_info].value_counts()\n    not_survived = train[train.Survived == 0][passenger_info].value_counts()\n    df = pd.DataFrame([survived, not_survived])\n    df.index = ['Survived', 'Not_survived']\n    sns.set()\n    df.plot(kind = 'bar', stacked = 'True', figsize = (14, 8))\n    plt.xticks(fontsize = 18)","2a73e06e":"# passengers who survived based on gender \nplot('Sex')","55d9cfbc":"# passengers who survived based on place they embarked \nplot('Embarked')","f5be133e":"# based on the class\nplot('Pclass')","ba7a016f":"plot('SibSp')","ad67b9bc":"plot('Parch')","7d7549f7":"survive = train.groupby(['Survived'])['Survived'].count()\nsurvive\nplt.figure(figsize = (16, 12))\nexplode = (0,0.05)\nlabels = (['Not_Survived' , 'Survived'])\ncolors = ['Red','Green']\nplt.pie(survive.values, labels = labels, explode = explode, autopct = '%1.2f%%', colors = colors, startangle = 135, textprops = {'color':'black', 'style': 'oblique', 'size':18})\nplt.title('Pie Chart Displaying passngers fate', fontsize = 20)\nplt.show()","9ce67951":"train.head()","0624039a":"test.head()","19b7b0b2":"train.isnull().sum()\ntest.isnull().sum()\n# replacing null value in train embarked column\ntrain['Embarked'].value_counts()\ntrain['Embarked'].fillna('S', inplace = True) # replaced using the mode of the stations ","492496a2":"# using label encoder for feature engineering\n# import labelencoder\nfrom sklearn.preprocessing import LabelEncoder\n# instantiate labelencoder \nlabel_encoder = LabelEncoder()","b54fecd5":"# encode labels in column sex and embarked\ncols = ['Sex','Embarked']\n# using a loop \nfor col in cols:\n    train[col] = label_encoder.fit_transform(train[col])\n    test[col] = label_encoder.fit_transform(test[col])\n","1f1afa61":"# data used to train the model\ntrain.head()","954836ca":"# data used for testing\ntest.head()","06653222":"# splitting the train data into attributes and label\ntest_features = ['PassengerId', 'Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Embarked']\nX = train[test_features].values\n\ny = train['Survived'].values","ecd82473":"# splitting into train and test \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)","45ec3533":"# fitting the logistic regression\nregressor = LogisticRegression(random_state = 42)\nregressor.fit(X_train, y_train)","b7572420":"# retrieve the intercept \nprint(regressor.intercept_)\n# retrieve the coeffecient \nprint(regressor.coef_)","19a3baa1":"# use the model to make prediction \ny_pred = regressor.predict(X_test)","100fe1a3":"# create a data frame for comparison\nregressor_df = pd.DataFrame({'Actual':y_test, 'Predicted': y_pred})\nregressor_df.head()","9d0adffd":"# model metrics\nprint('Model performance based on confusion matrix report:')\nconfusion_matrix(y_test, y_pred, labels = [0,1])","7734fc5d":"print('Accuracy score of the model is',np.round((accuracy_score(y_test, y_pred)*100),4), '%')","3c382642":"# fitting the decisiontree classifier \ndesClf = DecisionTreeClassifier(criterion = 'entropy', max_depth = 3,\n                                min_samples_split = 5, random_state = 42)\ndesClf.fit(X_train, y_train)","1073d199":"# use the model to make prediction\ndesClf_pred = desClf.predict(X_test)","65a13f79":"# Create a data frame for comparison\ndesClf_df = pd.DataFrame({'Actual': y_test, 'Predicted': desClf_pred})\ndesClf_df.head()","464205a9":"# model evaluation \nprint('Model evaluation based on the confusion matrix')\nconfusion_matrix(y_test, desClf_pred)","84ff509b":"print('Accuracy score of the model is',np.round((accuracy_score(y_test, desClf_pred)*100),4), '%')","5661a9b3":"# creating a random forest classifier function with 100 number of decision trees\nranf = RandomForestClassifier(n_estimators = 100, random_state = 42, max_depth = 6, min_samples_split = 4)\n# training on train data \nranf.fit(X_train, y_train)","d9c2071e":"# use the model to make prediction\nranf_pred = ranf.predict(X_test)","95e01469":"# making comparison\nranf_df = pd.DataFrame({'Actual': y_test, 'Predicted': ranf_pred})\nranf_df.tail(10)","51dd4eea":"# model evaluation \nprint('Model evaluation based on the confusion matrix')\nconfusion_matrix(y_test, ranf_pred)","93b04e51":"print('Accuracy score of the model is',np.round((accuracy_score(y_test, ranf_pred)*100),4), '%')","be2ed808":"# making predictions on the test data set \npred = ranf.predict(test)","7313a240":"# creating a dataframe \nsubmission = pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': pred})\n# display the last 10 rows\nsubmission.head(10)","b637661b":"# Converting\nantonny_titanic = submission.to_csv('antonny_titanic.csv', index = False)","0a156c88":"## Decision Tree ","5f30c5b7":"This algorithm has an accuracy of 82.12% . This is an improvement compared to the previous models. The accuracy of the model is not that great but it can be used to make predictions","bc71e25d":"**Data Cleaning and Analysis**","d6aa8b47":"## Making prediction on the test Data","20bfa442":"From the above chart , the following conclusions can be made:\n* There are more young females than males in the titanic who are less than 20 years.\n* Between 20 and 40 years, there are more male than female. \n* Passengers aged more than 40 years contain more male than female also.\n\nAge is Most likely to indicate who survived or not during the sinking of the boat.","df633bcb":"## Feature Engineering \n>> The Data set has columns Sex and Embarked containing categorical data . Feature engineering involves convertig the categorical data into numerical data which will be used for machine learning. Embarked column also has 2 missing values which can be replaced with the mode","b87fcaa7":"The model has an accuracy of 79.33% . The accuracy of the model can be improved by fine tunning the regressor or using another model. Decision trees can be used to improve the model performance.","e8008af6":"# **Machine Learning**","253a0d7c":"The model has an accuracy of 79.89%. This is not an improvement compared to logistic regression. will therefore try using Random forest algorithm taking into consideration bias and variance will trying to improve the performance.","73602ba3":"* Columns Embarked and Sex have been converted from categorical.\n* On the embarked column ,C is represented by 0, Q is representded by 1 & S is represented by 2 while on the sex column male is represented by 1 while female is 0.","1f0caf10":"## Random Forest Algorithm","4abb4739":"## Logistic Regression","a3196393":"**LOADING THE DATA SETS**","7afc0583":"the following conclusion can be made from the above chat:\n    * The was alot of old passengers who embarked the ship at port S compared to C and Q \n    * Port Q saw many young males embark the ship compared to S and Q.\n    * At port S and Q more male embarked the ship than female while at port C more female embarked the ship than male.\n    * Few passengers Embarked the ship at Port Q compared to both C and S.\n    "}}