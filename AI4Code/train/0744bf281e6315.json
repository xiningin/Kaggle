{"cell_type":{"ffd1e3ef":"code","c6a84390":"code","bec1e51c":"code","9174b375":"code","c6a06e85":"code","8cdba79e":"code","c997a2d1":"code","1775b127":"code","7fcd9e2b":"code","73f28310":"markdown","b4b5405c":"markdown","a07a4f51":"markdown","8d07909b":"markdown","78f8b96c":"markdown","d6ea536e":"markdown","b5a5b6d6":"markdown","3dfe67f4":"markdown","66d2d432":"markdown","59d4d401":"markdown","2352d57d":"markdown","2077b39c":"markdown"},"source":{"ffd1e3ef":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.options.display.max_rows = 999\npd.options.display.max_columns = 999","c6a84390":"df = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv\")\nsample = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n","bec1e51c":"print(df.shape)\nprint(\"ROWS: 1460 COLUMNS: 81\")","9174b375":"df.head(10)","c6a06e85":"df.tail()","8cdba79e":"df.isnull().sum()","c997a2d1":"df.info()","1775b127":"## This code will seperate the features by their dtype \"object\" however; you can adjust this seperation into whatever dtype you want or need depending on your context or data.\ncategorical_features = df.select_dtypes(include=\"object\")\nnumerical_features = df.select_dtypes(exclude=\"object\")\n\nprint(\"This is the number of Categorical Features:\", len(categorical_features.columns))\nprint(\"This is the number of Numerical Features: \", len(numerical_features.columns))","7fcd9e2b":"##This is the code to convert the missing values for the numerical features to the mean value.\n## You would need to replace the features with their relevant names\ndf['LotFrontage'] = df['LotFrontage'].fillna(df['LotFrontage'].mean())\n\n## This is the code to conver the missing values for the categorical features to the mode value.\ndf['BsmtCond'] = df['BsmtCond'].fillna(df['BsmtCond'].mode())","73f28310":"#  2. What does my dataset look like?# \n\nWhen you first look at your dataset, it can be just one large unorganized pile of information. In this specific case, it is split between test, train, and sample data. The \"train\" dataset will be part of the dataset which will \"train\" the model for a particular variable which the test dataset will be missing. This will help prevent the model from underfitting and overfitting the information.[ To have a better understanding of the significance of splitting the data, overfitting and underfitting the model, you can read about it here,](https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6#:~:text=Train%2FTest%20Split,model's%20prediction%20on%20this%20subset). I will include resources in the reference section at the end of the notebook to explain why this is so important for many models, and the significance to seperate and split the information accordingly,so for this notebook I won't touch on the significance of the test vs train set.\n\nWe will use pandas library to convert each of the dataset csv files to their dataframe counterparts which will use python to transform the data as we may need.","b4b5405c":"This information can show the values missing as well. In some cases, it may be more useful to just use .info() instead of seeing the total NaN values. One reason to go through the isnull().sum() instead of going through the info() first, is because there may be less features to analyze (in this dataset there is 81 but it might not be helpful to go through 10 or 30 features when you already familiar with the data and can already tell that some features will be categorical or not).\n\nThe datatypes from this function can also give us good insight into all the features of the dataset. We know there are 43 features that must be categorical since they are object type (which refers to them being a string or just words). There are 35 integer types which mean they feature numbers so they may or may not be catagorical (we cannot assume, but judging by their feature name and other information we might be able to make a good assumption without a deeper analysis) and there are 3 float types which refer to integers including decimals which may refer to numbers that are not discrete, but are continuous (which may include averages or means). This might strengthen or weaken some assumptions or questions we may have when analyzing how these features may connect or influence the variable we are trying to analyze.\n\nWe know the NaN values now, so the next question:\n\n> How can we proceed with these missing values? \n> \nIt may be helpful at this point to consider looking a bit deeper at the features and how they may or may not influence SalePrice. **It would be important to consider whether you want to drop some of these features**. This may help increase accuracy, but could also potentially decrease accuracy as well. It may be time consuming to apply transformations to ALL the features. For this dataset, it might not be such an issue. At this point, I may ask myself some questions before I even consider going through this tedious step of applying different transformations for each feature. Some of these questions that I may ask myself are:\n\n1. Do I have enough time to run through all of this information and data?\n2. Is it worth it or are some of these features not going to have THAT much of an influence on the variable?\n3. What questions am I trying to answer? Is it necessary to figure out how these missing features may influence the variable?\n\nFor me, I would prefer to fill in all the missing data and THEN perform an analysis to see whether or not those values may have an affect on the variable of SalePrice. When you fill in all the information from the dataset, you can have a more finished idea of how EVERY feature may influence SalePrice. Is this recommended everytime you may approach a new dataset, No. I would not based my decisions on my own intuition, and would always recommend performing the appropriate analysis before continuing on with dropping features or not.\n\nIt would be best to go ahead and seperate the columns into relevant subgroups of numerical vs categorical features.","a07a4f51":"# **References & Additional Material**# \n\n1) [Further information on Data types and their relevant format](https:\/\/datacarpentry.org\/python-ecology-lesson\/04-data-types-and-format\/index.html)\n\n2) [Explanation on the importance of the seperation of train, test, split and cross validation](https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6#:~:text=Train%2FTest%20Split,model's%20prediction%20on%20this%20subset)\n\n3) [Data School's video on an explanation of Machine Learning](https:\/\/www.youtube.com\/watch?v=elojMnjn4kk)\n\n4) [Data School's video walkthrough on analyzing an iris dataset and how to incorporate ML and data analysis tools](https:\/\/www.youtube.com\/watch?v=hd1W4CyPX58)\n\n5) [Krish Naik's video walkthrough on analyzing the Kaggle house prices dataset](https:\/\/www.youtube.com\/watch?v=vtm35gVP8JU)\n\n6) [Further explanations on python libraries and their relvance to your own analysis and future endeavors](https:\/\/data-flair.training\/blogs\/python-libraries\/)\n","8d07909b":"# **House Regression Analysis #1: Beginning Approach**# \n> Christopher Lee \n\nThis notebook is dedicated to how I would first approach this regression analysis. This information should be relevant for most notebooks and will cover a collection of information found through various kaggle notebooks and other online resources. All credit and additional supplementary information will be noted in the reference section. I will try to explain the first few steps of analysis. \n\nThis is my first kaggle notebook, so I will breakdown more of the information here as opposed to future notebooks. In the future, I will not explain all of these methods and will assume all of this information is already known.\n\n# 1. What libraries do I need?# \n\nPython has many different libraries to choose from. A Python library is a reusable chunk of code that describes a collection of core modules. Essentially, then, a library is a collection of modules. [[1]](https:\/\/data-flair.training\/blogs\/python-libraries\/) The most prominent libraries will be pandas, numpy, sklearn, matplotlib, and seaborn. For this notebook, all of these libraries may not be utilized, but it is still quite important to know the function and purpose of each library. \n\n[Pandas](https:\/\/pandas.pydata.org\/)\n> When working with tabular data, such as data stored in spreadsheets or databases, Pandas is the right tool for you.\n> Pandas will help you to explore, clean and process your data\n\n[Matplotlib:](https:\/\/matplotlib.org\/)\n> Matplotlib is a comprehensive library for creating static, animated, and interactive visualizations in Python.\n\n[Seaborn](https:\/\/seaborn.pydata.org\/)\n>Seaborn is a Python data visualization library based on matplotlib. It provides a high-level interface for drawing attractive and informative statistical graphics.\n\n[Numpy](https:\/\/numpy.org\/)\n>The fundamental package for scientific computing with Python\n\n[Scikit-learn](https:\/\/scikit-learn.org\/stable\/)\n>Simple and efficient tools for predictive data analysis\nAccessible to everybody, and reusable in various contexts\nBuilt on NumPy, SciPy, and matplotlib\n\nBefore, I begin, I am going to remove the limiter on pandas display rows & columns. Pandas will have a default limit to rows and columns displayed, so it is important to adjust accordingly since if you do not adjust this limit value when you display rows or columns, some of them may be cut which may prevent you from seeing the entire picture as you analyze the data. For this dataset, I have adjusted this limit value to it's maximum value. However; this number or limit can always be adjusted to whatever number you prefer.\n","78f8b96c":"This code is very slow and will take a lot of manual manipulation and running code. There are possibly faster solutions to convert the data quickly especially since you have already separated the features in the previous code. However, this is possibly the most simple code you can run and may be better for simplicity I would definitely recommend checking out other kaggle notebooks for maybe other ways to manipulate N\/A values using loops\n\nThe next steps will include more analysis, using models provided by sklearn or perhaps other models in other libraries to create predictions from your data. There are so many ways to continue on this data and path. This is only the start of the journey to your own analysis and research. This will be the end of this notebook. I hope this was helpful, and I definitely will continue creating more notebooks for the future. Definitely leave and comment! I hope this was helpful for those beginners who are on their own pathway to Data Science and Machine Learning!","d6ea536e":"\nBefore I approach the next few steps, I want to first explain a few things that may be important:\n\n*1.  I did not COMBINE the training and test dataset together.*\n\nIf your dataset is seperated into these two sets, it may be benefical and recommended to combine both the training dataset and the test dataset in the beginning of your analysis. Having all the data together can help ease cleaning the data, transforming it, and can create better conclusions about how the features may relate and connect to the variable you may be trying to test and figure out. If the dataset is already seperated, you will be forced to perform these actions **TWICE** since both the training and dataset **WILL NOT** have the same information. It is important not to assume that they will both be the same.\n\nAnother key note though is if you combine the dataset together in the beginning of your analysis, you will eventually have to seperate them again into both the training and testing set explained in the previous section when you are fitting the data into your relevant machine models. This notebook will not focus too much on the rationale or reason behind this (more information will be provided in the reference section), I will just go through the following steps assuming either you will combine the data and perform these actions on ONE whole entire dataset, or you will perform these actions on BOTH datasets (if it is split similar to this notebook training \/ testing dataset).\n\n*2. I am still quite a data science novice*\n\nI am currently a data science undergraduate student, so much of this approach is based more on youtube videos, reading kaggle notebooks, and learning from others. As I develop my own experience and knowledge, I believe this approach may be changed or adjusted. If you feel as though there are some things that are more or less important based on your own experiences, definitely comment and suggest below what might be some other approaches and ways you may face this dataset in the beginning. I definitely am open to any criticism and I am always looking forward to learn more from others.","b5a5b6d6":"We can see that there is a various amount of different features that have missing values. The features that have the most missing are Alley, PoolQC, Fence, MiscFeature. However; there are other features that are missing values as well. It would take a lot of time to go through each feature individually, so the faster solution may be to check the datatype of each feature.\n\nWe can use the info() method to check these datatypes and other relevant information in pandas. The datatypes of each feature can quickly give us information on whether the category is quantitative or categorical just by seeing whether the datatype of the features are either a integer or an object. This assumption won't always work because sometimes categorical features may use integers and float values, so there will need to be some very simple analysis involved. However; it is safe to assume ALL object datatype features will represent categorical features (because objects are typically String types, but there are situations this might be different you must make your own conclusions and inferences from observing the head() functions. For Example: numberical values may be inputted as \"Seven\" instead of 7, so you would have to convert these values to the correct datatype and perform the further analysis)","3dfe67f4":"The shape can give me a good idea of the length of the dataset, and can help me consider how much information am I going to have to look through. I can also have a good indication of the number of features that I will have to dig into. As the features increase, I may consider decreasing some of these features to test against the variable I am testing for. Even though this is such a small step, you can already create some conclusions about the data just from these two pieces of information.\n\nNext, I will grab the first 10 columns from the dataset using the head() function from pandas. You can leave the integer value empty {df.head()}, and the default value will be 5. I chose ten columns because I wanted to just compared the values with each other, but you can change this value to however high or low you want, it is up to you. The opposite would be the tail() function, which grabs the last columns from the dataset. This may or may not be quite significant depending on the context, so it will just depend on your usage.\n\nSince I changed the default values from pandas display rows & columns, I will be able to see all the columns below. However; if you do not change the default values, a lot of the features will be cut to display them in the head() function. You can test this yourself by removing this code from above:\n\n> pd.options.display.max_rows = 999\n\n> pd.options.display.max_columns = 999","66d2d432":"The head() function shows there are many different catagories and information recorded. It is useful to group all of these features between categorical vs quantative values. Even in quantitative values, I would also consider thinking about whether the values are discrete or continuous. As I observe these features, some questions I may have when I first look at this data are:\n\n**1. \"What factors may influence sale price? Which ones may have the largest relationship to sale's price.\"**\n\n>  There are approximately 80 features in this dataset. They all cannot have the same influence on it's sale price, so it is important to already consider what features may be more significant or less significant. We can probably assume the roof won't have as much of an influence on it's sale price as it's living space. Perhaps even the street material in front of the house may not give as much weight as whether or not the house has a pool or garage. These are questions and things to consider before diving deep into the data, and can help you create a better analysis of specific categories you may want to test or look deeper into.\n\n**2. \"Are the categorical features relevant to it's price? Which ones are not as important.\"**\n\n> This dataset is looking into the prices of the homes and how all of these features will influence that variable. I can already assume that the integer values such as living space, lot area, basement size, and maybe even year built will have more of a signficance than perhaps categorical features such as alley, roof style, and maybe even it's foundation. We probably want to then consider what are the most important categorical features to consider looking deeper into. There may be more weight to certain features like pools, garages, compared to other categorical features. It might be good to consider dropping some of these features to increase accuracy in the end, but this can also decrease accuracy..\n\n**3. \"What does some of the values mean or refer to? What do they represent for some of the categorical categories.\"**\n\n>There are so many different features that are a bit unclear on what they may represent. What is MSSubClass, what does neighborhood refer to? What about street? It is very important to have a clear idea of every one of the features before continuing on. In this dataset, each of the features are explained clearly in the documentation. If you are approaching other datasets without any documentation, it would be vital to get the exact definition and explanation of the feature prior to any analysis. \n\nSince there are so many NaN values, this is not good for our analysis and must be approached FIRST before going further. It is important to see what categories have the most NaN values.","59d4d401":"# 3. What are my missing values?# \n\nThe dataset is almost always going to have some issues with some of its data. These issues may result from incorrect data entry, some problems with data conversion, or perhaps even issues with pandas or the python notebook. These issues may result with some of the data or information being displayed as NaN, None, Null, etc within it's columns instead of it's correct information value. Each of these individual errors are NOT the same and can appear for different reasons, but they all will note missing data or information from the columns. We can use pandas and python to approach these values in the same manner. [[2]](https:\/\/medium.com\/analytics-vidhya\/dealing-with-missing-values-nan-and-none-in-python-6fc9b8fb4f31)\n\nAfter seeing there are many NaN values or missing values in the head() function from the dataset, I will next see the total of missing values from the entire dataset.","2352d57d":"First, I will look at first few rows of training dataset and find the complete shape of the data.","2077b39c":"# 4. What's next?\n\nThe next following steps with approaching replacing values can be much more complicated. For the purposes of this dataset and this notebook, I will offer a much simpler solution and strategy to replace potential missing values. However; in the future, I may create another notebook which can offer alternative solutions which may involve a bit deeper analysis or manual configurations. For the categorical features, we can replace all the missing values with their mode. For the numerical features, we can replace the missing values with the mean value."}}