{"cell_type":{"2ce81f79":"code","2ece58a1":"code","8141ca34":"code","5c12a0ca":"code","b227e2d7":"code","e7d2f33f":"code","c7b7d49e":"code","301ea5bf":"code","7e0e040f":"code","7c1fcca1":"code","bb2f22cf":"code","4f266bc2":"code","27a1596d":"code","fbe5e3a5":"code","17f47c04":"code","51889676":"code","6bb0311f":"code","8f599925":"code","0d2884d2":"markdown","46c39104":"markdown","dafc7780":"markdown","c7615faa":"markdown","1b32786d":"markdown","e478f44b":"markdown","51ae1d8e":"markdown","1d03cf8a":"markdown","0652842d":"markdown"},"source":{"2ce81f79":"! conda install -y hvplot","2ece58a1":"#importing Libs\nimport os\nimport glob\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport hvplot.pandas","8141ca34":"data_dir = Path('..\/input\/excerpt-from-openimages-2020-train')\nim_list = sorted(data_dir.glob('train_00_part\/*.jpg'))\nmask_list = sorted(data_dir.glob('train-masks-f\/*.png'))\nboxes_df = pd.read_csv(data_dir\/'oidv6-train-annotations-bbox.csv')\n\nnames_ = ['LabelName', 'Label']\nlabels =  pd.read_csv(data_dir\/'class-descriptions-boxable.csv', names=names_)\n\nim_ids = [im.stem for im in im_list]\ncols = ['ImageID', 'LabelName', 'XMin', 'YMin', 'XMax', 'YMax']\nboxes_df = boxes_df.loc[boxes_df.ImageID.isin(im_ids), cols] \\\n                   .merge(labels, how='left', on='LabelName')\nboxes_df","5c12a0ca":"# Annotate and plot\ncols, rows  = 3, 2\nplt.figure(figsize=(20,30))\n\n\nfor i,im_file in enumerate(im_list[9:15], start=1):\n    df = boxes_df.query('ImageID == @im_file.stem').copy()\n    img = cv2.imread(str(im_file))\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    # Add boxes\n    h0, w0 = img.shape[:2]\n    coords = ['XMin', 'YMin', 'XMax', 'YMax']\n    df[coords] = (df[coords].to_numpy() * np.tile([w0, h0], 2)).astype(int)\n\n    for tup in df.itertuples():\n        cv2.rectangle(img, (tup.XMin, tup.YMin), (tup.XMax, tup.YMax),\n                      color=(0,255,0), thickness=2)\n        cv2.putText(img, tup.Label, (tup.XMin+2, tup.YMax-2),\n                    fontFace=cv2.FONT_HERSHEY_DUPLEX,\n                    fontScale=1, color=(0,255,0), thickness=2)\n    \n    # Add segmentation masks\n    mask_files = [m for m in mask_list if im_file.stem in m.stem]    \n    mask_master = np.zeros_like(img)\n    np.random.seed(10)\n    for m in mask_files:\n        mask = cv2.imread(str(m))\n        mask = cv2.resize(mask, (w0,h0), interpolation = cv2.INTER_AREA)\n        color = np.random.choice([0,255], size=3)\n        mask[np.where((mask==[255, 255, 255]).all(axis=2))] = color\n        mask_master = cv2.add(mask_master, mask)\n    img = cv2.addWeighted(img,1, mask_master,0.5, 0)    \n    \n    plt.subplot(cols, rows, i)    \n    plt.axis('off')\n    plt.imshow(img)\n\nplt.show()","b227e2d7":" urls = pd.read_csv(data_dir\/\"image_ids_and_rotation.csv\", \n                   usecols=['ImageID', 'OriginalURL'])","e7d2f33f":"classes = np.loadtxt(data_dir\/\"openimages.names\", dtype=np.str, delimiter=\"\\n\")\nnet = cv2.dnn.readNet(str(data_dir\/\"yolov3-openimages.weights\"), str(data_dir\/\"yolov3-openimages.cfg\"))\n\nlayer_names = net.getLayerNames()\noutputlayers = [layer_names[i[0] - 1] for i in net.getUnconnectedOutLayers()]","c7b7d49e":"%%time\n\nfrom skimage import io\n\nim_url = urls.loc[urls.ImageID==im_list[11].stem, 'OriginalURL'].squeeze()\nimg = io.imread(im_url)\n\nheight,width,channels = img.shape\n\n# Make a blob array and run it through the network\nblob = cv2.dnn.blobFromImage(img,0.00392,(416,416),(0,0,0),True,crop=False)\nnet.setInput(blob)\nouts = net.forward(outputlayers)\n\n# Get confidence scores and objects\nclass_ids=[]\nconfidences=[]\nboxes=[]\nfor out in outs:\n    for detection in out:\n        scores = detection[5:]\n        class_id = np.argmax(scores)\n        confidence = scores[class_id]\n        if confidence > 0.2:   # threshold\n            print(confidence)\n            center_x= int(detection[0]*width)\n            center_y= int(detection[1]*height)\n            w = int(detection[2]*width)\n            h = int(detection[3]*height)\n            x=int(center_x - w\/2)\n            y=int(center_y - h\/2)\n            boxes.append([x,y,w,h]) #put all rectangle areas\n            confidences.append(float(confidence)) #how confidence was that object detected and show that percentage\n            class_ids.append(class_id) #name of the object tha was detected\n            \n# Non-max suppression\nindexes = cv2.dnn.NMSBoxes(boxes,confidences,0.4,0.6)\nprint(indexes, boxes, class_ids)","301ea5bf":"font = cv2.FONT_HERSHEY_DUPLEX\nfor i in range(len(boxes)):\n#     if i in indexes:\n        x,y,w,h = boxes[i]\n        label = str(classes[class_ids[i]])\n        cv2.rectangle(img, (x,y), (x+w,y+h), (255,255,0), 2)\n        cv2.putText(img, label, (x,y+30), font, 2, (255,255,0), 2)\n        \nplt.clf()\nplt.figure(figsize=(10,15))\nplt.imshow(img)","7e0e040f":"annotations = boxes_df.groupby('ImageID').agg(\n                        box_count=('LabelName', 'size'),\n                        box_unique=('LabelName', 'nunique')\n                        )\n\npd.options.display.float_format = '{:,.1f}'.format\nannotations.describe()","7c1fcca1":"all = annotations.hvplot.hist('box_count', width=600, bins=30)\nunique = annotations.hvplot.hist('box_unique', width=600)\n(all + unique).cols(1)","bb2f22cf":"onepct = annotations.box_count.quantile(0.99)\nannotations.query('box_count < @onepct').box_count.value_counts(normalize=True) \\\n    .sort_index().hvplot.bar(xticks=list(range(0,60,10)), width=600,\n                            line_alpha=0, xlabel='objects per image',\n                            ylabel='fraction of images')","4f266bc2":"print(boxes_df.loc[boxes_df.ImageID==\"fe7c6f7d298893da\"] \\\n         .groupby(['ImageID', 'Label'])['LabelName'].size()\n     )\n\nim_file = \"..\/input\/excerpt-from-openimages-2020-train\/train_00_part\/fe7c6f7d298893da.jpg\"\nim = cv2.imread(im_file)\nplt.imshow(im)","27a1596d":"from PIL import Image\nfrom dask import bag, diagnostics\n\n\ndef faster_get_dims(file):\n    dims = Image.open(file).size\n    return dims\n\ndfile_list = glob.glob('..\/input\/open-images-object-detection-rvc-2020\/test\/*.jpg')\nprint(f\"Getting dimensions for {len(dfile_list)} files.\")\n\n# parallelize\ndfile_bag = bag.from_sequence(dfile_list).map(faster_get_dims)\nwith diagnostics.ProgressBar():\n    dims_list = dfile_bag.compute()","fbe5e3a5":"sizes = pd.DataFrame(dims_list, columns=['width', 'height'])\ncounts = sizes.groupby(['width', 'height']).agg(count=('width', 'size')).reset_index()","17f47c04":"plot_opts = dict(xlim=(0,1200), \n                 ylim=(0,1200), \n                 grid=True, \n                 xticks=[250, 682, 768, 1024], \n                 yticks=[250, 682, 768, 1024], \n                 height=500, \n                 width=550\n                 )\n\nstyle_opts = dict(scaling_factor=0.2,\n                  line_alpha=1,\n                  fill_alpha=0.1\n                  )\n\ncounts.hvplot.scatter(x='width', y='height', size='count', **plot_opts) \\\n             .options(**style_opts)","51889676":"train_labels = boxes_df[['ImageID', 'LabelName']].merge(labels, how='left', on='LabelName')\ntrain_labels.Label.value_counts(normalize=True)[:45] \\\n            .hvplot.bar(width=650, height=350, rot=60, line_alpha=0,\n                        title='Label Frequencies',\n                        ylabel='fraction of all objects')","6bb0311f":"relations = pd.read_csv(data_dir\/'oidv6-relationship-triplets.csv')\nrelations = relations.merge(labels, how='left', left_on='LabelName1', right_on='LabelName') \\\n                     .merge(labels, how='left', left_on='LabelName2', right_on='LabelName',\n                            suffixes=['1', '2']) \\\n                     .loc[:, ['Label1', 'RelationshipLabel', 'Label2']] \\\n                     .dropna() \\\n                     .sort_values('RelationshipLabel') \\\n                     .reset_index(drop=True)","8f599925":"import networkx as nx\n\nkids = relations.query('Label1==\"Girl\" or Label1==\"Boy\"')\nG = nx.from_pandas_edgelist(kids, 'Label1', 'Label2', 'RelationshipLabel')\n\n\ngraph_opts = dict(arrows=False,\n                  node_size=5,\n                  width=0.5,\n                  alpha=0.8,\n                  font_size=10,\n                  font_color='darkblue',\n                  edge_color='gray'\n                \n                 )\n\nfig= plt.figure(figsize=(12,10))\nnx.draw_spring(G, with_labels=True, **graph_opts)","0d2884d2":"# Oject Detection Demo\nReading instance-segmentation data","46c39104":"[Copied from OPEN Image EDA](https:\/\/www.kaggle.com\/jpmiller\/open-images-eda\/data)","dafc7780":"Reading Dataset for RVC-2020","c7615faa":"Below we are using opencv to draw rectagle and text on the objects","1b32786d":"# Images and Annotaions\nexcerpt-from-openimages-2020-train Data set is from another dataset, but this dataset has more number of objects per images and more classes of objects","e478f44b":"Reading the data and shwoing few images containing boxes and segments masks along with labels.","51ae1d8e":"Mapping the entire network is quite complex. Here's a map for only two entities, boy and girl, and all the things to which they connect in the images.","1d03cf8a":"Distributions of Object labels","0652842d":"Labels Counts per image"}}