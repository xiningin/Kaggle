{"cell_type":{"b53fa6ef":"code","a2c37db0":"code","a3b43d92":"code","137c2492":"code","4d256413":"code","9f362504":"code","4ccaa697":"code","1c473dea":"code","39784e00":"code","ce263847":"code","b352f943":"code","282f7732":"code","3d81d3b4":"code","00821dc0":"code","fcd29f4b":"code","81107938":"code","f70e863f":"code","78f73714":"code","11975a3d":"code","eb3c3bed":"code","748acce6":"code","c14675b5":"code","2efcbcd2":"code","f47aba93":"code","077756cf":"code","e515a230":"code","327a5ef3":"code","509daca5":"code","8ef508ae":"code","1d712ce5":"code","47f87afc":"code","a39f7358":"code","0de5158c":"code","d378b493":"code","2bbb54fc":"code","8df1be5a":"code","167b2e33":"code","a3454f8c":"code","26293662":"code","ed03ab43":"code","682ed111":"code","12c3b8ff":"code","5d075697":"code","81bd39ae":"markdown","ec864b4b":"markdown","53913fb8":"markdown","5830737a":"markdown","8baa0aa3":"markdown","33b53921":"markdown","ea1f7684":"markdown","68f58944":"markdown","c3846669":"markdown","ccbc039c":"markdown","78f38217":"markdown","1279efcc":"markdown","1fd137e7":"markdown"},"source":{"b53fa6ef":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('..\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a2c37db0":"## look into the dater\nfrom pandas.api.types import is_string_dtype, is_numeric_dtype, is_categorical_dtype\nfrom fastai.tabular.all import *\nfrom sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\nfrom sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n#!pip install dtreeviz \n\nfrom IPython.display import Image, display_svg, SVG","a3b43d92":"from sklearn.metrics import confusion_matrix, roc_auc_score, plot_confusion_matrix, precision_score, recall_score, classification_report, plot_roc_curve, roc_curve\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, GroupKFold","137c2492":"import gc\nimport random as r\nimport joblib","4d256413":"## pandas\npd.options.display.max_rows = None\npd.options.display.max_columns = None","9f362504":"main_path = \"..\/input\/fraud-detection-categorified-and-split\/\"","4ccaa697":"## Choosing the type simulations\nbuildLv = False ## Hold out validation\nbuildSkf = True ## stratified k fold \n\nAV=True ## Adeverserial Validation","1c473dea":"%%time\n## Load pre-processed tabular pandas (fastai) test and train\ndset_str=\"all\"#all#50k#10k\n\nto = load_pickle(main_path+\"to_\"+dset_str+\"c.pkl\") \nto_tst = load_pickle(main_path+\"to_tst_\"+dset_str+\"c.pkl\")\n\n## split as Xs and Ys\nxs,y = to.train.xs,to.train.y\nvalid_xs,valid_y = to.valid.xs,to.valid.y\n\ntest_xs = to_tst.xs.copy()\nxs.shape, test_xs.shape","39784e00":"## delete large unused variables\ndel to, to_tst; x=gc.collect()","ce263847":"## Function to easily get the metrics in the format for saving\ndef m_rep(m,xs,y): return classification_report(y, m.predict(xs), labels=[1,0], digits=4, output_dict=True)\n\ndef metrics(m, xs, y , valid_xs, valid_y): \n    tr_rep = m_rep(m,xs,y)\n    vd_rep = m_rep(m,valid_xs,valid_y)\n    tr_auc = roc_auc_score(y,m.predict_proba(xs)[:,1])\n    oob_auc=0.0000\n    #oob_auc = roc_auc_score(y,m.oob_decision_function_[:,1])\n    vd_auc = roc_auc_score(valid_y,m.predict_proba(valid_xs)[:,1])\n    print('{:.4f} ; {:.4f} ; {:.4f}'\n          .format(tr_auc, oob_auc, vd_auc))","b352f943":"## Generate table of important features from the model with other useful information\ndef xgb_fi(m, df,df_real=None):\n    fi = pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False).reset_index(drop=True)\n    #fi[\"isCont\"] = fi.cols.isin(cont)\n    fi[\"countNA\"] = [df.loc[:,col].isna().sum()\/len(df) if col in df.columns else float(\"NaN\") for col in fi.cols]\n    return fi\n\ndef plot_fi(fi):\n    return fi.plot('cols', 'imp', 'barh', figsize=(12,7), legend=False)","282f7732":"## Format Test Predictions for Submission\ndef format_test_preds(preds,test_xs,comment):\n    #preds = m.predict_proba(test_xs[cols])[:,1]\n    print(pd.DataFrame(preds).describe())\n    \n    ## Make into submission dataframe\n    df_pred = pd.DataFrame({\"TransactionID\":test_xs.TransactionID.to_list(),\n                            \"isFraud\": preds})\n    \n    ## save\n    df_pred.to_csv(comment+\"my_subs.csv\",index=False)\n    print(df_pred.shape)\n    del df_pred; x=gc.collect()","3d81d3b4":"def xgb_lvd_fun(get_feat_imp):\n    ## model\n    clf = xgb.XGBClassifier(n_estimators=2000, \n                        max_depth=8, #12\n                        learning_rate= 0.05,\n                        subsample= 0.6,#0.8 \n                        colsample_bytree= 0.4, \n                        random_state = r.randint(0,9999),\n                        use_label_encoder=False,\n                        # USE GPU\n                        tree_method='gpu_hist')\n\n    m = clf.fit(xs[cols].iloc[idxT], y.iloc[idxT], eval_set=[(xs[cols].iloc[idxV],y.iloc[idxV])],\n                eval_metric= \"auc\", verbose=100, early_stopping_rounds=100)\n    ## score prediction\n    tr_auc = roc_auc_score(y.iloc[idxT], m.predict_proba(xs[cols].iloc[idxT])[:,1])\n    vd_auc = roc_auc_score(y.iloc[idxV],m.predict_proba(xs[cols].iloc[idxV])[:,1])\n    \n    ## test results predictions\n    te_pred = m.predict_proba(test_xs[cols])[:,1]\n    \n    ## Getting feature importances\n    if get_feat_imp:\n        fi = xgb_fi(m, xs[cols])\n        fi.to_csv(\"fi_lv.csv\",index=False)\n        print(fi)\n    \n    ## remove large files    \n    del m, clf; x=gc.collect()\n    return [tr_auc, vd_auc], te_pred\n    ","00821dc0":"def xgb_pred_fun():\n    \n    ## setup kfold\n    skf = KFold(n_splits = 5, shuffle = False)\n    tr_pred = np.zeros(len(xs))\n    oof_pred = np.zeros(len(xs))\n    te_pred = np.zeros(len(test_xs))\n\n    for i, (idxT, idxV) in enumerate(skf.split(xs,y)):\n        ## model\n        print('Fold',i)\n        print(' n_rows of train =',len(idxT),'rows of holdout =',min(idxV),\"to\",max(idxV))#len(idxV))        \n        clf = xgb.XGBClassifier(n_estimators=1000, \n                        max_depth=12, #12\n                        learning_rate= 0.02,\n                        subsample= 0.8,#0.8 \n                        colsample_bytree= 0.4, \n                        random_state = r.randint(0,9999),\n                        use_label_encoder=False,\n                        # USE GPU\n                        tree_method='gpu_hist')\n\n        m = clf.fit(xs[cols].iloc[idxT], y.iloc[idxT], eval_set=[(xs[cols].iloc[idxV],y.iloc[idxV])],\n                eval_metric= \"auc\", verbose=100, early_stopping_rounds=100)\n        \n        ## predicting the probabilities of for Train OOF and Test\n        tr_pred[idxT] += m.predict_proba(xs[cols].iloc[idxT])[:,1]\/(skf.n_splits-1)\n        oof_pred[idxV] += m.predict_proba(xs[cols].iloc[idxV])[:,1]\n        te_pred += m.predict_proba(test_xs[cols])[:,1]\/skf.n_splits\n        \n        ## Getting Feature Importances\n        if i==4:\n            fi = xgb_fi(m, xs[cols])\n            fi.to_csv(\"fi_lv.csv\",index=False)\n            print(fi)\n            \n        ## remove large files    \n        del m; x=gc.collect()\n\n    print('{:.4f} ; {:.4f} ; {:.4f}'\n              .format(roc_auc_score(y,tr_pred),roc_auc_score(y,oof_pred),0.000))\n    \n    return te_pred","fcd29f4b":"## collecting all columns\ncols = xs.columns.to_list()","81107938":"## Vcol buckets (first filter based on correlation) From fraud detection v11's EDA \nv = []\nv += ['V1', 'V3', 'V11', 'V9', 'V5', 'V7']\nv += ['V13', 'V17', 'V24', 'V14', 'V20', 'V27', 'V34', 'V26', 'V30']\nv += ['V36', 'V37', 'V47', 'V40', 'V48', 'V52', 'V41', 'V45']\nv += ['V54', 'V65', 'V60', 'V67', 'V56', 'V68', 'V62', 'V55', 'V70']\nv += ['V76', 'V89', 'V91', 'V81', 'V82', 'V87', 'V78', 'V88']\nv += ['V127', 'V121', 'V99', 'V110', 'V104', 'V130', 'V129', 'V131', 'V109', 'V136', 'V116', 'V120', 'V125', 'V113', 'V118', 'V98', 'V107', 'V117', 'V115']\nv += ['V138', 'V140', 'V142', 'V147', 'V155', 'V162']\nv += ['V165', 'V160', 'V166']\nv += ['V203', 'V207', 'V216', 'V187', 'V176', 'V173', 'V183', 'V215']\nv += ['V169', 'V195', 'V201', 'V171', 'V174', 'V175', 'V209', 'V185', 'V188', 'V210', 'V198', 'V180']\nv += ['V274', 'V264', 'V265', 'V261', 'V235', 'V223', 'V258', 'V260', 'V246', 'V252', 'V241', 'V266', 'V240', 'V277', 'V228', 'V226']\nv += ['V220', 'V239', 'V271', 'V221', 'V234', 'V251']\nv += ['V307', 'V291', 'V285', 'V290', 'V312', 'V297', 'V305', 'V320', 'V303', 'V286', 'V309', 'V284', 'V310']\nv += ['V281', 'V301', 'V282', 'V315', 'V289', 'V296', 'V314', 'V283']\nv += ['V332', 'V338', 'V337', 'V336', 'V325', 'V326', 'V328', 'V335']\nlen(v)\n\ndef set_approach(a,b):\n    return list(set(a)-set(b))\n\n## Remove all v columns using below code\ntf_V = [bool(re.search(\"^V\"+\"[0-9]+\",col)) for col in xs.columns.to_list()]\ncols = xs.columns[list(~np.array(tf_V))].to_list()","f70e863f":"## add specific v columns\ncols +=v\nprint(len(cols))","78f73714":"## remove time and other columns that don't add to score\ncols_rem = ['TransactionID', 'TransactionDT']\n\ncols_rem +=[\"DayNum\",\"HrOfDay\",\"WkDayNum\",'R_emaildomain1', 'R_emaildomain2', 'P_emaildomain1', 'P_emaildomain2','id_31_browser',\"DeviceInfo_make\"]\ncols = set_approach(cols,cols_rem)","11975a3d":"len(cols)","eb3c3bed":"## function to one hot encode\ndef ohe(df,cols):\n    for col in cols:\n        df[col+\"_OHE\"] = df[col].isna().astype(int)","748acce6":"## OHE of cols_ohe\ncols_ohe = [\"D2\",\"D9\"] \n\nohe(xs,cols_ohe)\nohe(test_xs,cols_ohe)\ncols += [\"D2_OHE\",\"D9_OHE\"]","c14675b5":"cols_rem =[\"D2\",\"D9\",\"D12\"]\ncols = set_approach(cols,cols_rem)","2efcbcd2":"len(cols)","f47aba93":"cols += [\"DayNum\",\"HrOfDay\",\"WkDayNum\"]","077756cf":"def Dn(df,cols):\n    for col in cols:\n        df[col+\"n\"] = df[\"DayNum\"] - df[col]","e515a230":"d_cols = ['D1',\"D2\", 'D3', 'D4', 'D5', 'D6', 'D7', \"D8\", \"D9\", 'D10', 'D11', \"D12\",\n          'D13', 'D14', 'D15']\nDn(xs,d_cols)\nDn(test_xs,d_cols)\nd_cols = [string +\"n\" for string in d_cols]\ncols +=d_cols","327a5ef3":"len(cols)","509daca5":"print(cols)","8ef508ae":"def encode_CB2(df1,df2,uid):\n    newcol = \"_\".join(uid)\n    ## make combined column\n    df1[newcol] = df1[uid].astype(str).apply(lambda x: '_'.join(x), axis=1)\n    df2[newcol] = df2[uid].astype(str).apply(lambda x: '_'.join(x), axis=1)\n    \n    ## concat and then factorize\n    temp_df = pd.concat([df1[newcol],df2[newcol]],axis=0)\n    temp_df,_ = temp_df.factorize(sort=True)\n    \n    ## unconcat    \n    if temp_df.max()>32000: \n        df1[newcol+\"_fact\"] = temp_df[:len(df1)].astype('int32')\n        df2[newcol+\"_fact\"] = temp_df[len(df1):].astype('int32')\n    else:\n        df1[newcol+\"_fact\"] = temp_df[:len(df1)].astype('int16')\n        df2[newcol+\"_fact\"] = temp_df[len(df1):].astype('int16')\n    print(newcol+\"_fact\")\n    return [newcol+\"_fact\"]","1d712ce5":"## Aggregations \ndef encode_ag(df,df_te,uid,cols_ag,func_list,ag=True):\n    ## concat test and train\n    new_cols_ret = []\n    ## 1. concat test and train\n    temp_df = pd.concat([df[uid+cols_ag],df_te[uid+cols_ag]],axis=0).reset_index(drop=True)\n    temp_df[uid] = temp_df[uid].fillna(-9999)\n    ## 2. group them by UID\n    grouped = temp_df.groupby(uid)\n    for func in func_list:\n        ## 3. Create new features based on \"func\"\n        temp2_df = grouped[cols_ag].transform(func).reset_index(drop=True)#.iloc[:,0]\n        if ag:\n            new_cols = [col+\"_\"+\"_\".join(uid)+\"_\"+func+\"2\" for col in cols_ag]\n        else:\n            new_cols = [\"_\".join(uid)+\"_FE2\" for col in cols_ag]\n        new_cols_ret +=new_cols\n        ## 4. Save functions\n        df[new_cols] = temp2_df[0:len(df)].fillna(-9999).astype('float32')\n        df_te[new_cols] = temp2_df[len(df):].fillna(-9999).reset_index(drop=True).astype('float32')\n    print(new_cols_ret)\n    return new_cols_ret","47f87afc":"## UID definition\nuid = [\"D1n\",\"card1\",\"addr1\",\"P_emaildomain\"] ## uid1\n#uid = [\"D1n\",\"card1\",\"addr1\",\"P_emaildomain\",\"D3n\",\"V1\",\"M7\"]\nprint(xs[uid].isna().sum())\n","a39f7358":"len(cols)","0de5158c":"# ## Combine\ncols+= encode_CB2(xs,test_xs,[\"card1\",\"addr1\"])\ncols+= encode_CB2(xs,test_xs,uid)\n\n## Frequency Encoding (FE)\ncols += encode_ag(xs,test_xs,[\"addr1\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"card1\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"card2\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"dist1\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"card1\",\"addr1\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,uid,[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"card1\",\"D1n\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"P_emaildomain\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"TransactionAmt\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"cents\"],[\"TransactionID\"],[\"count\"],ag=False)\n## Additional FE\ncols += encode_ag(xs,test_xs,[\"ProductCD\"],[\"TransactionID\"],[\"count\"],ag=False)\ncols += encode_ag(xs,test_xs,[\"HrOfDay\"],[\"TransactionID\"],[\"count\"],ag=False)\n\n## Aggregation\ncols_ag_av = [\"D3n\",\"M7\",\"D9\",\"D10n\",\"D13n\",\"D5n\",\"D15n\",\"D4n\",\"D6n\",\"D11n\",\"HrOfDay\",\"D14n\",\"id_13\"] ## cols from AV importance\ncols_ag_other = [\"TransactionAmt\",\"cents\"]\ncols_C = [\"C\"+str(i) for i in range(1,15)]##v96\ncols_M = ['M'+str(x) for x in range(1,10) if x!= 7]##v96\n\ncols += encode_ag(xs,test_xs,uid,cols_ag_av+cols_ag_other, [\"std\",\"mean\"])\ncols += encode_ag(xs,test_xs,[\"card1\",\"addr1\",\"P_emaildomain\"],cols_ag_av+cols_ag_other, [\"std\",\"mean\"])##v95\ncols += encode_ag(xs,test_xs,uid,cols_C, [\"std\",\"mean\"]) ##v96\n\ncols_ag_v = [\"V9\",\"V314\",\"V3\",\"V171\",\"V81\",\"V289\",\"V88\",\"V89\",\"V271\",\"V326\",\"V7\",\"V52\",\"V1\"]\ncols += encode_ag(xs,test_xs,uid,cols_ag_v, [\"std\",\"mean\"])\n## Agg nunique\n\ncols_ag_numb = [\"addr2\",\"ProductCD\",\"dist1\",\"DeviceType\"]#,\"P_emaildomain\"]\n\ncols += encode_ag(xs,test_xs,uid,cols_ag_numb+cols_M, [\"nunique\"])","d378b493":"cols = set_approach(cols,[\"DayNum\"])","2bbb54fc":"pd.DataFrame(xs[cols].dtypes).reset_index().sort_values(\"index\").reset_index(drop=True)","8df1be5a":"len(cols)","167b2e33":"## Fillna\nxs.fillna(-9999, inplace=True)\ntest_xs.fillna(-9999, inplace=True)","a3454f8c":"## variable to trigget getting FI\ni=4\nif i==4: get_feat_imp=True\nget_feat_imp","26293662":"## Linear split 80% 20%\nmsk = np.arange(len(xs))<0.8*len(xs) #usually 0.8, 0.8851 for the oversampling case\nidxT = list(np.where(msk)[0])\nidxV = list(np.where(~msk)[0])","ed03ab43":"%%time\n## Run LV 5 times\n\nif buildLv:\n    get_feat_imp=False\n    n=5\n    preds = np.zeros(len(test_xs))\n    auc_lst = []\n    for i in range(n):\n        print(\"iteation\",i)\n        if i==4: get_feat_imp=True\n        a,temp = xgb_lvd_fun(get_feat_imp)\n        preds += temp\/n\n        auc_lst.append(a)\n        del temp; x=gc.collect()\n        #print(a)\n    auc_lst = np.array(auc_lst)\n    print(auc_lst.mean(axis=0))\n    print(auc_lst.std(axis=0))\n    format_test_preds(preds,test_xs,\"baselineLV\")\n","682ed111":"%%time\n## Run 6 time kfold along with OOF prediction and fold based prediction of Test\n\nif buildSkf:\n\n    preds = xgb_pred_fun()\n    format_test_preds(preds,test_xs,\"baseline\")\n    #del preds; x=gc.collect()\n    \n#     ## Post_Proc\n#     df_ptr = pd.concat([xs[[\"TransactionID\"]+uid],y],axis=1)\n#     print(df_ptr.sample())\n#     df_pte = pd.concat([test_xs[[\"TransactionID\"]+uid],pd.DataFrame({\"isFraud\":preds})],axis=1)\n#     print(df_pte.sample())\n#     encode_ag(df_ptr,df_pte,uid,[\"isFraud\"],[\"mean\"])\n#     print(df_pte.sample())\n#     format_test_preds(df_pte.isFraud_D1n_card1_addr1_mean2.to_list(),test_xs,\"post_proc_\")\n#     del df_ptr, df_pte; x=gc.collect();","12c3b8ff":"## AV\nif AV:\n    df_dom = pd.concat([xs[cols], test_xs[cols]])\n    is_test = pd.DataFrame([0]*len(xs) + [1]*len(test_xs))\n    idxT,idxV = train_test_split(np.arange(len(df_dom)),test_size=0.2)\n    del xs, test_xs; x=gc.collect()\n\n    clf = xgb.XGBClassifier(n_estimators=1000, \n                            max_depth=8, #12\n                            learning_rate= 0.05,\n                            subsample= 0.6,#0.8 \n                            colsample_bytree= 0.4, \n                            random_state = r.randint(0,9999),\n                            use_label_encoder=False,                            \n    #                         #USE CPU\n    #                         nthread=4)\n                            # USE GPU\n                            tree_method='gpu_hist')\n\n    m = clf.fit(df_dom.iloc[idxT], is_test.iloc[idxT], eval_set=[(df_dom[cols].iloc[idxV],is_test.iloc[idxV])],\n                    eval_metric= \"auc\", verbose=100, early_stopping_rounds=100)\n\n    fi = xgb_fi(m, df_dom,df_dom)\n    fi.to_csv(\"fi_av.csv\",index=False)\n    fi","5d075697":"fi","81bd39ae":"# Import","ec864b4b":"## Local Validation: kfold","53913fb8":"## Functions","5830737a":"## Clean up D columns","8baa0aa3":"## Group aggregation ","33b53921":"## Combine and group aggregation funtions","ea1f7684":"## Local validation: Hold out","68f58944":"## Adverserial Validation","c3846669":"## Adding Dn columns","ccbc039c":"## Load data","78f38217":"## My aggregations","1279efcc":"## Final list of columns","1fd137e7":"## Column selection from EDA."}}