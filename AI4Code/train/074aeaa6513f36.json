{"cell_type":{"bb885b89":"code","ce252a51":"code","625ac5a8":"code","21dc24b3":"code","b0142c4e":"code","4b52de6b":"markdown"},"source":{"bb885b89":"! pip3 install newspaper3k","ce252a51":"import json\nimport newspaper\nfrom newspaper import Article\nfrom time import mktime\nfrom datetime import datetime\nimport csv\nimport os\n\nos.listdir(\"..\/input\/\")","625ac5a8":"# Set the limit for number of articles to download\nLIMIT = 50\n\ndata = {}\ndata['newspapers'] = {}\n\n# Loads the JSON files with news sites\nwith open('..\/input\/newspapers.json') as data_file:\n    companies = json.load(data_file)\n    \ncount = 1\n\ncsv_articles  = []","21dc24b3":"# Iterate through each news company\nfor company, value in companies.items():\n    # It uses the python newspaper library to extract articles\n    print(\"Building site for \", company)\n    paper = newspaper.build(value['link'], memoize_articles=False)\n    newsPaper = {\n           \"link\": value['link'],\n           \"articles\": []\n        }\n    noneTypeCount = 0\n    for content in paper.articles:\n        if count > LIMIT:\n            break\n        try:\n            content.download()\n            content.parse()\n        except Exception as e:\n            print(e)\n            print(\"continuing...\")\n            continue\n            # Again, for consistency, if there is no found publish date the article will be skipped.\n            # After 10 downloaded articles from the same newspaper without publish date, the company will be skipped.\n        if content.publish_date is None:\n            print(count, \" Article has date of type None...\")\n            noneTypeCount = noneTypeCount + 1\n            if noneTypeCount > 10:\n                print(\"Too many noneType dates, aborting...\")\n                noneTypeCount = 0\n                break\n            count = count + 1\n            continue\n        article = {}\n        article['title'] = content.title\n        article['text'] = content.text\n        article['link'] = content.url\n        try:\n            article['published'] = content.publish_date.isoformat()\n        except:\n            print(\"bad date\")\n\n        csv_article  = {}\n        csv_article['headline'] = content.title.encode('utf-8')\n        try:\n            csv_article['date'] = content.publish_date.isoformat()\n        except:\n            print(\"bad date\")\n\n        csv_article['link'] = content.url\n\n        csv_articles.append(csv_article)\n\n\n        newsPaper['articles'].append(article)\n        print(count, \"articles downloaded from\", company, \" using newspaper, url: \", content.url)\n        count = count + 1\n        noneTypeCount = 0\n    count = 1\n    data['newspapers'][company] = newsPaper","b0142c4e":"# Finally, save the articles as a JSON-file\ntry:\n    with open('scraped_articles.json', 'w') as outfile:\n        json.dump(data, outfile)\nexcept Exception as e: print(e)\n\nOUTFILE = \"headlines.csv\"\nwith open(OUTFILE, 'w') as output_file:\n    keys = ['headline', 'date', 'link']\n    dict_writer = csv.DictWriter(output_file, keys)\n    dict_writer.writeheader()\n    for row in csv_articles:\n        dict_writer.writerow(row)","4b52de6b":"Credits : Michael Tauberg for [this](https:\/\/towardsdatascience.com\/how-does-news-coverage-differ-between-media-outlets-20aa7be1c96a) Medium article and his Github [repo](https:\/\/github.com\/taubergm\/NewsHeadlines)\n& the [newspaper](https:\/\/github.com\/codelucas\/newspaper) library"}}