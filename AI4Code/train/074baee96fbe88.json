{"cell_type":{"dd04e249":"code","1aaa3014":"code","3c0e06b8":"code","9a4049c4":"code","9f3b7afd":"code","2aa1ebc3":"code","7d77499a":"code","ddb1935d":"code","00d95fbe":"code","4d8a4115":"code","e2d74e5f":"code","956d2fff":"code","5abf79bb":"code","c73b8e4c":"code","8e96fd21":"code","e22d6c99":"code","051c5987":"code","d0a280cf":"code","c74af09a":"code","18ecf86b":"code","539729ef":"code","e9ce2406":"code","5a34a53b":"code","69cfef24":"code","44880196":"code","8f386cbc":"code","69243a2d":"code","c4d4a9dc":"code","1fff692f":"code","1e5d759d":"code","9908ddee":"code","6e446690":"code","baa8e011":"code","1a235bf4":"code","21de13fe":"code","b08c1317":"code","6ae73594":"code","e3bb6561":"code","e08800da":"code","2eae9d9d":"code","ebb75cd1":"code","7125245e":"code","aa58775f":"code","05e2a349":"code","0bcffe67":"code","2dd328c0":"code","a4b2dbe6":"code","54498d4f":"code","f3702c4e":"code","444a1cfb":"code","b2d28f3b":"code","d690d746":"code","9c2b8c7b":"code","8b7c6303":"code","cc649262":"code","95854770":"code","7cef98c5":"code","e54c643a":"code","c45b03d7":"code","30a570a9":"code","7402e6a3":"code","3929d7f8":"code","1f8cb984":"code","f8c2cf1c":"markdown","109ae503":"markdown","a96d738e":"markdown","86142433":"markdown","866fb6e5":"markdown","9be24c83":"markdown","96b6b9eb":"markdown","3075e61b":"markdown","8e473d04":"markdown","d700f9db":"markdown","e18c3644":"markdown","b98a8af3":"markdown","0910d091":"markdown","50b11256":"markdown","2dc8d056":"markdown","adc6f343":"markdown","0fd1cab3":"markdown","4bf90ee7":"markdown","ff64c353":"markdown","f7661625":"markdown","d07ecee5":"markdown","17a6f559":"markdown","e3339e26":"markdown","aca3f784":"markdown","cd3e7f79":"markdown","942ba111":"markdown","6bb073e7":"markdown","701c166d":"markdown","0b354af1":"markdown","9d376eec":"markdown","158da814":"markdown","c9009439":"markdown","d3d2d4bb":"markdown","58582493":"markdown","89edaa6a":"markdown","a3620193":"markdown","f4de9ff5":"markdown","ba6a7a88":"markdown","9dc4d79a":"markdown","0076f0d3":"markdown","d6d08491":"markdown","0dbfcaa4":"markdown"},"source":{"dd04e249":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\nimport string\nimport nltk\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom collections import Counter\nimport spacy\nfrom tqdm import trange\nimport random\nfrom spacy.util import compounding,minibatch\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nstop = stopwords.words('english')\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1aaa3014":"train = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","3c0e06b8":"print('Train Shape:',train.shape)\nprint('Test Shape:',test.shape)\ntrain.head()","9a4049c4":"print('Sentiment of text : {} \\nOur training text :\\n{}\\nSelected text which we need to predict:\\n{}'.format(train['sentiment'][1],train['text'][1],train['selected_text'][1]))","9f3b7afd":"train.isnull().sum()","2aa1ebc3":"train.dropna(inplace=True)","7d77499a":"train.sentiment.describe()","ddb1935d":"# distribution of tweets by sentiment in the training set\ntemp=train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Blues')","00d95fbe":"fig=make_subplots(1,2,subplot_titles=('Train set','Test set'))\nx=train.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#3368d4','#32ad61','#f24e4e'],name='train'),row=1,col=1)\nx=test.sentiment.value_counts()\nfig.add_trace(go.Bar(x=x.index,y=x.values,marker_color=['#3368d4','#32ad61','#f24e4e'],name='test'),row=1,col=2)","4d8a4115":"def Jaccard_similarity(str1,str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c))\/(len(a)+len(b)-len(c))","e2d74e5f":"str1 = 'My name is Kevin'\nstr2 = 'Myself Kevin'\njaccard_score = Jaccard_similarity(str1,str2)\nprint('Jaccard Score :',jaccard_score)","956d2fff":"def Jaccard_similarity(df):\n    a = set(df['text'].lower().split())\n    b = set(df['selected_text'].lower().split())\n    c = a.intersection(b)\n    return float(len(c))\/(len(a)+len(b)-len(c))","5abf79bb":"train['jaccard_score'] = train.apply(Jaccard_similarity,axis=1)","c73b8e4c":"train['no_words_st'] = train.selected_text.apply(lambda x: len(str(x).split()))\ntrain['no_words_t'] = train.text.apply(lambda x: len(str(x).split()))\ntrain['diff_words']  = train['no_words_t'] - train['no_words_st']                                                ","8e96fd21":"train.head()","e22d6c99":"#Distribution of Length b\/w selected_text and text'\nplt.hist(train['no_words_st'],bins=20,label='selected_text')\nplt.hist(train['no_words_t'],bins=20,label='text')\nplt.title('Distribution of Length b\/w selected_text and text')\nplt.legend()\nplt.show()","051c5987":"plt.figure(figsize=(8,6))\nsns.kdeplot(train['no_words_st'],shade=True,color='b')\nsns.kdeplot(train['no_words_t'],shade=True,color='r')\nplt.title('Distribution of Length')\nplt.show()","d0a280cf":"plt.figure(figsize=(8,6))\nsns.kdeplot(train[train['sentiment']=='positive']['diff_words'],shade=True,color='b',label='diff_words_pos')\nsns.kdeplot(train[train['sentiment']=='negative']['diff_words'],shade=True,color='r',label='diff_words_neg')\nplt.title('Distribution of Differnce in length of Positive words & Negative Words')\nplt.show()","c74af09a":"plt.figure(figsize=(8,6))\nsns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'],shade=True,color='b',label='Jaccard_score_pos')\nsns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'],shade=True,color='r',label='Jaccard_score_neg')\nplt.title('Distribution of Jaccard Score of Positive words , Negative Words & Neutral Words')\nplt.show()","18ecf86b":"train[train['sentiment']=='neutral']['jaccard_score'].describe()","539729ef":"plt.figure(figsize=(12,6))\nsns.boxplot(train[train['sentiment']=='neutral']['jaccard_score'])\nplt.show()","e9ce2406":"plt.plot(train[train['sentiment']=='neutral']['jaccard_score'],'r+')\nplt.show()","5a34a53b":"def clean_text(text):\n\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","69cfef24":"def clean_text1(text):\n\n    # tokenize text and remove puncutation\n    text = [word.strip(string.punctuation) for word in text.split(\" \")]\n    # remove words that contain numbers\n    text = [word for word in text if not any(c.isdigit() for c in word)]\n    # remove stop words\n    text = [x for x in text if x not in stop]\n    # remove empty tokens\n    text = [t for t in text if len(t) > 0]\n    # remove words with only one letter\n    text = [t for t in text if len(t) > 1]\n    # join all\n    text = \" \".join(text)\n    return(text)","44880196":"train['text'] = train['text'].apply(str).apply(lambda x: clean_text(x))\ntrain['selected_text'] = train.selected_text.apply(str).apply(lambda x: clean_text(x))","8f386cbc":"train['cleaned_text'] = train['text'].apply(lambda x: clean_text1(x))\ntrain['cleaned_selected_text'] = train.selected_text.apply(lambda x: clean_text1(x))","69243a2d":"train.head(3)","c4d4a9dc":"word_token = word_tokenize(\"\".join(train['cleaned_selected_text']))\nprint(word_token[:50])","1fff692f":"most_comman_token_15 = Counter(word_token).most_common(15)\nmost_comman_token_15_df = pd.DataFrame(most_comman_token_15)\nmost_comman_token_15_df.columns = ['Word','Count']\nmost_comman_token_15_df.style.background_gradient(cmap='Blues')","1e5d759d":"def plot_wordcloud(text,mask=None,max_words=400,max_font_size=100,figure_size=(24.0,16.0),title=None,title_size=40,image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords={'u',\"im\"}\n    stopwords=stopwords.union(more_stopwords)\n    \n    wordcloud = WordCloud(background_color='white',\n                         stopwords = stopwords,max_words=max_words,\n                         max_font_size=max_font_size,random_state=42,mask=mask)\n    \n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors),interpolation=\"bilinear\");\n        plt.title(title,fontdict={'size':title_size,\n                                  'verticalalignment':'bottom'})\n    else:\n            plt.imshow(wordcloud);\n            plt.title(title,fontdict={'size':title_size,'color':'red',\n                                     'verticalalignment':'bottom'})\n            plt.axis('off');\n    plt.tight_layout()  \n    \nd = '..\/input\/imagetc\/'","9908ddee":"positive_sentiment = train[train['sentiment']=='positive']\nnegative_sentiment = train[train['sentiment']=='negative']\nneutral_sentiment = train[train['sentiment']=='neutral']","6e446690":"plt.figure(figsize=(8,6))\nsns.kdeplot(neutral_sentiment['no_words_st'],shade=True,color='b',label='neu_no_words_st')\nsns.kdeplot(neutral_sentiment['no_words_t'],shade=True,color='r',label='neu_no_words_t')\nplt.title('Distribution of Number of words in selected text & text in neutral dataframe')\nplt.show()","baa8e011":"word_token_pos = word_tokenize(\"\".join(positive_sentiment['cleaned_selected_text']))\nprint(word_token_pos[:50])","1a235bf4":"most_comman_token_15_pos = Counter(word_token_pos).most_common(15)\nmost_comman_token_15_pos_df = pd.DataFrame(most_comman_token_15_pos)\nmost_comman_token_15_pos_df.columns = ['Word','Count']\nmost_comman_token_15_pos_df.style.background_gradient(cmap='Blues')","21de13fe":"twitter_mask=np.array(Image.open(d+'twitter.png'))\nplot_wordcloud(positive_sentiment.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Positive tweets\")","b08c1317":"word_token_neg = word_tokenize(\"\".join(negative_sentiment['cleaned_selected_text']))\nprint(word_token_neg[:50])","6ae73594":"most_comman_token_15_neg = Counter(word_token_neg).most_common(15)\nmost_comman_token_15_neg_df = pd.DataFrame(most_comman_token_15_neg)\nmost_comman_token_15_neg_df.columns = ['Word','Count']\nmost_comman_token_15_neg_df.style.background_gradient(cmap='Reds')","e3bb6561":"twitter_mask=np.array(Image.open(d+'twitter.png'))\nplot_wordcloud(negative_sentiment.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Negative tweets\")","e08800da":"word_token_neu = word_tokenize(\"\".join(neutral_sentiment['cleaned_selected_text']))\nprint(word_token_neu[:50])","2eae9d9d":"most_comman_token_15_neu = Counter(word_token_neu).most_common(15)\nmost_comman_token_15_neu_df = pd.DataFrame(most_comman_token_15_neu)\nmost_comman_token_15_neu_df.columns = ['Word','Count']\nmost_comman_token_15_neu_df.style.background_gradient(cmap='Greens')","ebb75cd1":"twitter_mask=np.array(Image.open(d+'twitter.png'))\nplot_wordcloud(neutral_sentiment.text,mask=twitter_mask,max_font_size=80,title_size=30,title=\"WordCloud for Neutral tweets\")","7125245e":"def get_top_n_words(corpus,n_grams=None):\n    vec = CountVectorizer(ngram_range=(n_grams,n_grams)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    \n    sum_of_words = bag_of_words.sum(axis=0)\n    word_freq = [(word, sum_of_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    word_freq = sorted(word_freq, key = lambda x: x[1], reverse=True)\n    return word_freq[:15]","aa58775f":"top_n_bigrams = get_top_n_words(train['text'].dropna(),2)\nx,y = map(list,zip(*top_n_bigrams))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","05e2a349":"top_n_bigrams = get_top_n_words(train['selected_text'].dropna(),2)\nx,y = map(list,zip(*top_n_bigrams))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","0bcffe67":"top_n_trigrams = get_top_n_words(train['text'].dropna(),3)\nx,y = map(list,zip(*top_n_trigrams))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","2dd328c0":"top_n_trigrams = get_top_n_words(train['selected_text'].dropna(),3)\nx,y = map(list,zip(*top_n_trigrams))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","a4b2dbe6":"top_n_trigrams_pos = get_top_n_words(positive_sentiment['text'].dropna(),3)\nx,y = map(list,zip(*top_n_trigrams_pos))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","54498d4f":"top_n_trigrams_neg = get_top_n_words(negative_sentiment['text'].dropna(),3)\nx,y = map(list,zip(*top_n_trigrams_neg))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","f3702c4e":"top_n_trigrams_neu = get_top_n_words(neutral_sentiment['text'].dropna(),3)\nx,y = map(list,zip(*top_n_trigrams_neu))\nplt.figure(figsize=(9,7))\nsns.barplot(x=y,y=x)\nplt.show()","444a1cfb":"data_copy = train.copy()\ndata_train = data_copy[data_copy['no_words_t']>=3]","b2d28f3b":"data_train.head()","d690d746":"def get_training_data(sentiment):\n    train_data=[]\n    \n    '''\n    Returns Training data in the format needed to train spacy NER\n    '''\n    for index,row in data_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.cleaned_selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start,end,'selected_text']]}))\n    return train_data","9c2b8c7b":"def training(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model,set up the pipeline and train the entity recognizer\"\"\"\n    if model is not None:\n        nlp=sapcy.load(model) #load existing spaCy model\n        print(\"Loaded model '%s'\" %model)\n    else:\n        nlp = spacy.blank(\"en\") #create blank Language class\n        print(\"Created blank 'en' model \")\n        \n        # The pipeline execution\n        # Create the built-in pipeline components and them to the pipeline\n        # nlp.create_pipe works for built-ins that are registered in the spacy\n        \n        if \"ner\" not in nlp.pipe_names:\n            ner = nlp.create_pipe(\"ner\")\n            nlp.add_pipe(ner,last=True)\n            \n        # otherwise, get it so we can add labels\n        \n        else:\n            ner = nlp.get_pipe(\"ner\")\n            \n        # add labels \n        for _, annotations in train_data:\n                for ent in annotations.get(\"entities\"):\n                    ner.add_label(ent[2])\n        \n        # get names of other pipes to disable them during training\n        \n        pipe_exceptions = [\"ner\",\"trf_wordpiecer\",\"trf_tok2vec\"]\n        other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n        \n        with nlp.disable_pipes(*other_pipes): # training of only NER\n            \n            # reset and intialize the weights randoml - but only if we're\n            # training a model\n            \n            if model is None:\n                nlp.begin_training()\n            else:\n                nlp.resume_training()\n            \n            for itn in trange(n_iter):\n                random.shuffle(train_data)\n                losses={}\n                \n                # batch up the example using spaCy's mnibatch\n                batches = minibatch(train_data,size=compounding(4.0,1000.0,1.001))\n                #print(batches)\n                for batch in batches:\n                    texts , annotations = zip(*batch)\n                    nlp.update(\n                        texts, #batch of texts\n                        annotations, # batch of annotations\n                        drop = 0.5,  # dropout - make it harder to memorise data\n                        losses = losses,\n                )\n            print(\"Losses\", losses)\n        save_model(output_dir, nlp, 'st_ner')","8b7c6303":"def get_model_path(sentiment):\n    model_out_path = None \n    if sentiment == 'positive':\n        model_out_path = 'models\/model_pos'\n    elif sentiment == 'negative':\n        model_out_path = 'models\/model_neg'\n    return model_out_path","cc649262":"def save_model(output_dir,nlp,new_model_name):\n    if output_dir is not None:\n        if not os.path.exists(output_dir):\n            os.makedirs(output_dir)\n        nlp.meta[\"name\"] = new_model_name\n        nlp.to_disk(output_dir)\n        print(\"Saved model to\",output_dir)","95854770":"sentiment ='positive'\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_path(sentiment)\ntraining(train_data,model_path,n_iter=3,model=None)","7cef98c5":"sentiment ='negative'\ntrain_data = get_training_data(sentiment)\nmodel_path = get_model_path(sentiment)\ntraining(train_data,model_path,n_iter=3,model=None)","e54c643a":"MODEL_PATH = '\/kaggle\/working\/models\/'\nMODEL_PATH_POS = MODEL_PATH + 'model_pos'\nMODEL_PATH_NEG = MODEL_PATH + 'model_neg'","c45b03d7":"def predict(text,model):\n    docx = model(text)\n    ent_arr=[]\n    for ent in docx.ents:\n        #print(ent.text)\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        entity_arr = [start,end,ent.label_]\n        if entity_arr not in ent_arr:\n            ent_arr.append(entity_arr)\n    selected_text = text[ent_arr[0][0]:ent_arr[0][1]] if len(ent_arr)>0 else text\n    return selected_text","30a570a9":"selected_text=[]\nif MODEL_PATH is not None:\n    print(\"Loading Models  from \", MODEL_PATH)\n    model_pos = spacy.load(MODEL_PATH_POS)\n    model_neg = spacy.load(MODEL_PATH_NEG)\n    for index,row in test.iterrows():\n        text = row.text.lower()\n        if row.sentiment == 'neutral':\n            selected_text.append(text)\n        elif row.sentiment == 'positive':\n            selected_text.append(predict(text,model_pos))\n        else:\n            selected_text.append(predict(text,model_neg))       ","7402e6a3":"assert len(test.text) == len(selected_text)\nsubmission['selected_text'] = selected_text\nsubmission.to_csv('submission.csv',index=False)","3929d7f8":"from IPython.core.display import HTML\n\ndef multi_table(table_list):\n    ''' Acceps a list of IpyTable objects and returns a table which contains each IpyTable in a cell\n    '''\n    return HTML(\n        '<table><tr style=\"background-color:white;\">' + \n        ''.join(['<td>' + table._repr_html_() + '<\/td>' for table in table_list]) +\n        '<\/tr><\/table>'\n    )","1f8cb984":"multi_table([test.head(10),submission.head(10)])","f8c2cf1c":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:32px;'>WHAT WE NEED TO PREDICT ?<\/font>","109ae503":"<font style='font-weight:700;letter-spacing:1.5px;text-transform:uppercase'><font style='color:#5b6c87;font-size:28px;'>N-gram Exploration:<\/font>\n<font style='color:gray;letter-spacing:1px;text-transform:uppercase'>Check for top N-Grams for the <font color='#32ad61'>POSITIVE\/<\/font><font color='#f24e4e'>NEGATIVE\/<\/font><font color='#3368d4'>NEUTRAL<\/font> Texts <\/font>","a96d738e":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline'>LOADING DATA<\/font>","86142433":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Most Comman 15 words in Positive texts<\/font>","866fb6e5":"# It always starts with **IMPORT**,so lets import the necessary helper libraries.","9be24c83":"<font style='color:#4e8df2;font-weight:700;letter-spacing:1.5px;text-decoration:underline'>SAMPLE EXAMPLE<\/font>","96b6b9eb":"<font style='color:#38f295;font-weight:700;letter-spacing:1.5px'>WORK STILL IN PROGRESS !<\/font>","3075e61b":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>2. Training the Spacy NER Model<\/font>","8e473d04":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline;text-transform:uppercase;'>Calculating length of <b>text<\/b> and <b>selected_text<\/b><\/font>","d700f9db":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:26px;text-transform:uppercase;'>Finding Most Comman 15 words from <font color='#32ad61'>POSITIVE\/<\/font><font color='#f24e4e'>NEGATIVE\/<\/font><font color='#3368d4'>NEUTRAL<\/font> texts<\/font>","e18c3644":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Bigrams in selected_text <\/font>","b98a8af3":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline'>READING DATA<\/font>","0910d091":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top TRIgrams in Text Data<\/font>","50b11256":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Negative Trigrams in Training Data<\/font>","2dc8d056":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Neutral Trigrams in Training Data<\/font>","adc6f343":"# ** Introduction**\n\n\"My ridiculous dog is amazing.\" [sentiment: positive]\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds. But, which words actually lead to the sentiment description? In this competition you will need to pick out the part of the tweet (word or phrase) that reflects the sentiment.\n\n![twitter](https:\/\/media.giphy.com\/media\/k4ZItrTKDPnSU\/giphy.gif)","0fd1cab3":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Postive Trigrams in Training Data<\/font>","4bf90ee7":"\n<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:28px;text-transform:uppercase'>Data Modeling<\/font>\n<ul style='color:gray;font-weight:700;letter-spacing:1px;text-transform:uppercase'>\n   <li>Getting the Training Data<\/li>\n     <li>Training the Model<\/li>\n    <li>Getting MODEL PAth<\/li>\n     <li>Saving the Model<\/li>\n     <li>Prediction using Test Data<\/li>\n<\/ul>","ff64c353":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Trigrams in selected_text<\/font>","f7661625":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Most Comman 15 words in Negative texts<\/font>","d07ecee5":"\n<font style='color:#32ad61;font-weight:700;letter-spacing:1.5px;text-decoration:underline;font-size:25px;'>Training for Positive Texts .....<\/font>","17a6f559":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Top Bigrams in Training Data<\/font>","e3339e26":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline'>CHECKING FOR NULL VALUES...<\/font>","aca3f784":"\n<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>4. Saving the Model in the Output Directory<\/font>","cd3e7f79":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:32px;'>* EXPLORATORY DATA ANALYSIS<\/font>","942ba111":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Most Comman 15 words in Neutral texts<\/font>","6bb073e7":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;font-size:28px;'>* Displaying <font color='#3368d9'>Predicted Text<\/font> with <font color='#32ad71'>Testing Text<\/font><\/font>","701c166d":"<font style='color:#901af0;font-weight:700;letter-spacing:1.5px;text-decoration:underline;text-transform:uppercase;'>If you find this kernel useful please consider upvoting it \ud83d\ude0a which keeps me motivated for doing hard work and to produce more content.<\/font>","0b354af1":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>5. Prediction using test data<\/font>","9d376eec":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>* Most Comman 15 words in whole Training set<\/font>","158da814":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:28px;'>GENERATING FEATURES FROM THE DATA<\/font>\n<ul style='color:#5b6c87;font-weight:700;letter-spacing:1px;text-transform:uppercase'>\n*     <li>Jaccard Similarity<\/li>\n*     <li>Length of **text**<\/li>\n*     <li>Length of **selected_text**<\/li>\n*     <li>Difference In Number Of words of **Selected_text** and **Text**<\/li>\n<\/ul>","c9009439":"<font style='color:#4e8df2;font-weight:700;letter-spacing:1.5px;text-transform:uppercase'>By calculatig the jaccard index we infer that for neutral sentiment the text and selected text are almost same.<\/font>","d3d2d4bb":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;font-size:28px;'>WORDCLOUDS<\/font>\n<ul style='color:gray;letter-spacing:1px;text-transform:uppercase'>\n<li>WordCloud for Positive Words <\/li>\n<li>WordCloud for Negative Words <\/li>\n<li>WordCloud for Neutral Words <\/li>\n<\/ul>","58582493":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline'>DROPING NULL VALUES..<\/font>","89edaa6a":"<font style='color:#5b6c87;font-weight:700;letter-spacing:1.5px;font-size:28px;text-transform:uppercase;'>* Basic Text Processing<\/font>","a3620193":"# **Acknowlegements**\n* https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes --> WORDCLOUDS FUNCTION\n* https:\/\/www.kaggle.com\/rohitsingh9990\/ner-training-using-spacy-0-628-lb --> For understanding how to    train spacy NER on custom inputs\n* https:\/\/www.kaggle.com\/ethernext\/experimenting-with-sentiment-analysis & https:\/\/www.kaggle.com\/tanulsingh077\/twitter-sentiment-extaction-analysis-eda-and-model--> how to start with Kaggle compitions.\n\n![thanks](https:\/\/i0.wp.com\/media1.giphy.com\/media\/ip6n2oVNZBHiM\/giphy.gif)","f4de9ff5":"\n<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>1. Getting the Training Data<\/font>","ba6a7a88":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-decoration:underline;text-transform:uppercase;'>Calculating Jaccard Similarity for the <b>text<\/b> and <b>selected_text<\/b> columns<\/font>","9dc4d79a":"<font style='color:#eb0707;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>Feedbacks are always appreciated.<\/font>","0076f0d3":"\n<font style='color:#f24e4e;font-weight:700;letter-spacing:1.5px;text-decoration:underline;font-size:25px;'>Training for Negative texts<\/font>","d6d08491":"<font style='font-weight:700;letter-spacing:1px;text-transform:uppercase'><font style='color:#5b6c87;font-size:28px;'>JACCARD SIMILARITY:<\/font><font style='color:gray;'>Jaccard Similarity (coefficient), a term coined by Paul Jaccard, measures similarities between sets. It is defined as the size of the intersection divided by the size of the union of two sets <\/font><\/font>\n\n![jaccard](https:\/\/bugra.github.io\/static\/images\/work\/notes\/2017\/2\/7\/industry-similarity-for-jaccard-index.png)","0dbfcaa4":"<font style='color:gray;font-weight:700;letter-spacing:1.5px;text-transform:uppercase;'>3. Getting the [POS\/NEG] Model Path<\/font>"}}