{"cell_type":{"3782e347":"code","04a332e5":"code","83d8ca38":"code","76ef51df":"code","d1b22716":"code","9e2d835e":"code","aa90374a":"code","2c0244f1":"code","cebcebb5":"code","2fed34e3":"code","672e0664":"code","2eca8b6f":"code","15544b91":"code","3f0861c8":"code","dcc64339":"code","028fc44d":"code","facc0795":"code","9c83f734":"code","132c1845":"code","11741411":"code","ab697806":"code","daddc710":"code","4b9e6cfc":"code","2e164ff5":"code","dc2e979f":"code","ce955637":"code","980e1096":"code","8a2fd956":"code","ae3d7ff7":"code","39a2bc60":"code","cc6846b9":"code","4e4fe77d":"code","174ccd9c":"code","f4dd02ca":"code","6dbfc600":"code","708cd193":"code","7a1582ed":"code","2a6e36ca":"markdown","9073c4cf":"markdown","85b11108":"markdown","f280c157":"markdown","2ed70d7a":"markdown","bcde80bb":"markdown","5645b99f":"markdown","dac37f8b":"markdown","3dac2002":"markdown","d4b14841":"markdown","fa804c28":"markdown","e173ba6a":"markdown","d3aff7d2":"markdown","957b2372":"markdown","fc828f05":"markdown","17a46406":"markdown","ab1e41c4":"markdown"},"source":{"3782e347":"%load_ext autoreload\n%autoreload 2\n%matplotlib inline\n\n# Ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.filterwarnings(\"ignore\",category=DeprecationWarning)","04a332e5":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport pickle\nimport re\nimport seaborn as sns","83d8ca38":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\ndisplay(train_df.tail()) \ndisplay(test_df.tail())","76ef51df":"#list all columns\ntrain_df.columns","d1b22716":"#just the numeric columns\ntrain_df.select_dtypes(include=[np.number]).columns","9e2d835e":"#missing data \ntotal = train_df.isnull().sum().sort_values(ascending=False)\npercent = (train_df.isnull().sum()\/train_df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[missing_data.Total>0]","aa90374a":"#outlier check\nplt.figure(figsize=(15,5))\nax = sns.scatterplot(x=train_df.GrLivArea, y=train_df.SalePrice, \n                     hue=train_df.Neighborhood, legend=False).set_title(\"SalePrice vs. SqFt by Neighborhood\")","2c0244f1":"#remove the 2 wierdos\ntrain_df = train_df[train_df.GrLivArea<4500]\nplt.figure(figsize=(15,5))\nax = sns.scatterplot(x=train_df.GrLivArea, y=train_df.SalePrice, \n                     hue=train_df.Neighborhood, legend=False).set_title(\"SalePrice vs. SqFt by Neighborhood\")","cebcebb5":"# date sold to category\ntrain_df['sale_date'] = train_df.MoSold.astype('str') + '_' + train_df.YrSold.astype('str')\ntest_df['sale_date'] = test_df.MoSold.astype('str') + '_' + test_df.YrSold.astype('str')\n\n#combine baths\ntrain_df['baths'] = train_df.FullBath + train_df.HalfBath*0.5\ntest_df['baths'] = test_df.FullBath + test_df.HalfBath*0.5\ntrain_df['bsmt_baths'] = train_df.BsmtFullBath + train_df.BsmtHalfBath*0.5\ntest_df['bsmt_baths'] = test_df.BsmtFullBath + test_df.BsmtHalfBath*0.5\n\n#drop columns\ndrop_me = ['MoSold','YrSold','Id','FullBath','HalfBath','BsmtFullBath','BsmtHalfBath']\ntrain_df.drop(columns=drop_me, inplace=True)\ntest_df.drop(columns=drop_me, inplace=True)\n\n#category columns from numeric\nnew_cats = ['MSSubClass','OverallQual','OverallCond', 'YearBuilt', 'YearRemodAdd',\n           'GarageYrBlt', 'GarageCars','EnclosedPorch', '3SsnPorch', 'ScreenPorch',\n           'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd','Fireplaces','baths', 'bsmt_baths']\ntrain_df[new_cats] = train_df[new_cats].astype('object')\ntest_df[new_cats] = test_df[new_cats].astype('object')","2fed34e3":"train_df.select_dtypes(include=[np.number]).columns","672e0664":"from sklearn.model_selection import train_test_split\n\nX = train_df.drop(columns='SalePrice')\nY = np.log(train_df['SalePrice'])  #convert Y to log(y)\nX_train, X_valid, y_train, y_valid = train_test_split(X, Y, test_size=0.2)\nX_test = test_df.copy()","2eca8b6f":"from sklearn import model_selection\ndef cv_mean_enc(col):\n    #X_train mean encodings calculated with Kfold to ensure no data leakage\n    kf = model_selection.KFold(5, shuffle=False)\n    X_train['target']=y_train\n    X_train[col+'_target_enc'] = np.nan\n    for tr_ind, val_ind in kf.split(X_train):\n        X_tr, X_val = X_train.iloc[tr_ind], X_train.iloc[val_ind]\n        X_train.loc[X_train.index[val_ind], col+'_target_enc'] = X_val[col].map(X_tr.groupby(col)['target'].mean())\n    X_train[col+'_target_enc'].fillna(y_train.mean(), inplace=True)\n    \n    #X_valid from calculations on X_train\n    encodings = X_train.groupby(col)[col+'_target_enc'].mean()\n    X_valid[col+'_target_enc'] = X_valid[col].map(encodings)\n    X_valid[col+'_target_enc'].fillna(y_train.mean(), inplace=True)\n    \n    #X_test get calculated from entire data set\n    X_valid['target']=y_valid\n    all_data = X_train.append(X_valid)\n    encodings = all_data.groupby(col)['target'].mean()\n    X_test[col+'_target_enc'] = X_test[col].map(encodings)\n    X_test[col+'_target_enc'].fillna(all_data['target'].mean(), inplace=True)\n    \n    #cleanup\n    X_train.drop(columns=['target',col],inplace=True)\n    X_valid.drop(columns=['target',col],inplace=True)\n    X_test.drop(columns=[col],inplace=True)","15544b91":"cols = X_train.select_dtypes(include='object').columns\nfor c in cols: cv_mean_enc(c)","3f0861c8":"total = X_train.isnull().sum().sort_values(ascending=False)\npercent = (X_train.isnull().sum()\/X_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data[missing_data.Total>0]","dcc64339":"X_train.fillna(0, inplace=True)\nX_valid.fillna(0, inplace=True)\nX_test.fillna(0, inplace=True)","028fc44d":"dataset = (X_train,y_train,X_valid,y_valid,X_test)\npickle.dump(dataset,open(\"dataset.pk\", 'wb'))","facc0795":"X_train,y_train,X_valid,y_valid,X_test = pickle.load(open(\"dataset.pk\", 'rb'))","9c83f734":"import xgboost as xgb\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVR","132c1845":"#this is where we save our predictions for the ensemble later\nvalid_preds = pd.DataFrame()\ntest_preds = pd.DataFrame()","11741411":"params = {'max_depth':3,\n        'n_estimators':100, \n        'learning_rate':0.1,\n        'min_child_weight' : 1,\n        'gamma' : 0,\n        'subsample' : 1,\n        'colsample_bytree':1,\n        'reg_alpha':0}","ab697806":"def tune_param(target_param, param_range, params):\n    best_score = 100\n    best_iteration = 0\n    new_value = params[target_param]\n    \n    for a in param_range:\n        params[target_param]=a\n        gbm = xgb.XGBRegressor(**params,random_state=13)\n        model_xgb = gbm.fit(X_train, y_train,\n            eval_metric=\"rmse\",\n            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n            verbose=False,\n            early_stopping_rounds = 3)\n        if model_xgb.best_score < best_score:\n            new_value = a\n            best_score = model_xgb.best_score\n            best_iteration = model_xgb.best_iteration\n    \n    #return best results as parameter dict\n    print(f'After tuning {target_param} to {new_value}, model score is {best_score:.3f} after {best_iteration + 1} iterations.')\n    params[target_param]=new_value\n    return params","daddc710":"params = tune_param('max_depth',range(3,15), params)\nparams = tune_param('min_child_weight',range(1,11), params)\nparams = tune_param('gamma',[0,.1,.2,.3,.4,.5], params)\nparams = tune_param('subsample',[.6,.7,.8,.9,1], params)\nparams = tune_param('colsample_bytree',[.6,.7,.8,.9,1], params)\nparams = tune_param('reg_alpha',[0,.1,.5,1,5,10,50,100,500,1000,5000], params)\nparams = tune_param('learning_rate',[.1,.2,.3,.4,.5,.6,.7,.8], params)\nparams","4b9e6cfc":"gbm = xgb.XGBRegressor(**params,random_state=13)\nmodel_xgb = gbm.fit(X_train, y_train,\n    eval_metric=\"rmse\",\n    eval_set=[(X_train, y_train), (X_valid, y_valid)],\n    verbose=False,\n    early_stopping_rounds = 3)\nvalid_preds['xgb'] = model_xgb.predict(X_valid)\ntest_preds['xgb'] = model_xgb.predict(X_test)","2e164ff5":"from sklearn.preprocessing import StandardScaler\ns = StandardScaler()\nX_train_s = s.fit_transform(X_train)\nX_valid_s = s.fit_transform(X_valid)\nX_test_s = s.fit_transform(X_test)","dc2e979f":"def knn_hyperopt(n):\n    model_knn = KNeighborsRegressor(n_neighbors=n)\n    model_knn.fit(X_train_s,y_train)\n    preds = model_knn.predict(X_valid_s)\n    rmse = np.sqrt(mean_squared_error(preds,y_valid))\n    print(f\"for {n} neighbors, rmse is {rmse:.5f}\")\n    return rmse","ce955637":"n_neigh = [2,3,5,7,9,11]\nresults = [] \nfor n in n_neigh:\n    results.append(knn_hyperopt(n))\nopt_neigh = n_neigh[np.argmin(results)]","980e1096":"model_knn = KNeighborsRegressor(n_neighbors=opt_neigh)\nmodel_knn.fit(X_train_s,y_train)\nvalid_preds['knn'] = model_knn.predict(X_valid_s)\ntest_preds['knn'] = model_knn.predict(X_test_s)","8a2fd956":"def lasso_hyperopt(alpha):\n    reg = Lasso(alpha=alpha)\n    reg.fit(X_train,y_train)\n    preds = reg.predict(X_valid)\n    rmse = np.sqrt(mean_squared_error(preds,y_valid))\n    print(f\"for alpha:{alpha}, rmse is {rmse:.5f}\")\n    return rmse","ae3d7ff7":"alpha_lasso = [1e-8, 1e-5, 1e-4, 1e-3,1e-2, 0.5, 0.1, 1, 5, 10]\nresults = []\nfor a in alpha_lasso:\n    results.append(lasso_hyperopt(a))\nopt_alpha = alpha_lasso[np.argmin(results)]","39a2bc60":"reg = Lasso(alpha=opt_alpha)\nreg.fit(X_train,y_train)\nvalid_preds['lasso'] = reg.predict(X_valid)\ntest_preds['lasso'] = reg.predict(X_test)","cc6846b9":"def svr_hyperopt(c):\n    model_svr = SVR(C=c, gamma='scale')\n    model_svr.fit(X_train_s,y_train)\n    preds = model_svr.predict(X_valid_s)\n    rmse = np.sqrt(mean_squared_error(preds,y_valid))\n    print(f\"for C:{c}, rmse is {rmse:.5f}\")\n    return rmse","4e4fe77d":"c_svr = [1e-4, 1e-3,1e-2, 0.5, 0.1, 1, 5, 10, 50, 1e2, 5e2, 1e3, 5e3, 1e4]\n#c_svr = [74 + i\/10 for i in range(0,20,2)]\nresults = []\nfor a in c_svr:\n    results.append(svr_hyperopt(a))\nopt_c = c_svr[np.argmin(results)]","174ccd9c":"model_svr = SVR(C=opt_c, gamma='scale')\nmodel_svr.fit(X_train_s,y_train)\nvalid_preds['lasso'] = model_svr.predict(X_valid)\ntest_preds['lasso'] = model_svr.predict(X_test)","f4dd02ca":"def stack_hyperopt(alpha):\n    reg = Lasso(alpha=alpha)\n    reg.fit(valid_preds,y_valid)\n    preds = reg.predict(valid_preds)\n    rmse = np.sqrt(mean_squared_error(preds,y_valid))\n    print(f\"for alpha:{alpha}, rmse is {rmse:.5f}\")\n    return rmse","6dbfc600":"alpha_lasso = [1e-8, 1e-5, 1e-4, 1e-3,1e-2, 0.5, 0.1, 1, 5, 10]\nresults = []\nfor a in alpha_lasso:\n    results.append(stack_hyperopt(a))\nopt_alpha = alpha_lasso[np.argmin(results)]","708cd193":"reg = Lasso(alpha=opt_alpha)\nreg.fit(valid_preds,y_valid)\npreds = reg.predict(test_preds)","7a1582ed":"sub_df = pd.DataFrame()\nsub_df['Id']=pd.read_csv(\"..\/input\/test.csv\")['Id']\nsub_df['SalePrice']=np.exp(preds)\nsub_df.to_csv('submission.csv', index=False)","2a6e36ca":"The goal of feature engineering here is to create as many categories as we can. These will all be target encoded later, so we don't have to worry about normalization, etc.","9073c4cf":"### svm","85b11108":"**commentary**: looks like Nan values means \"doesn't exist\" for garages and basements. It's probably safe to assume the same for pool, alley, fence, massvnr","f280c157":"### knn","2ed70d7a":"### Target Encoding","bcde80bb":"## Data Exploration","5645b99f":"## Models ","dac37f8b":"# Intro","3dac2002":"## Feature Eng","d4b14841":"### Test\/Valid\/Train Split","fa804c28":"**commentary**: This is all a mess. We have dates as numbers, quality metrics both text and numbers, categories as numbers and text.","e173ba6a":"### xgb ","d3aff7d2":"### lasso","957b2372":"This uses the dataset from [Ames House Prices](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques) Kaggle Competition.\n\n**Overview**\n\nWe're given housing data from Ames, Iowa. This is split into training and test sets, each having 80 features. Two of those feature describe the date of the sale. We're asked to predict the sale price of the test set. Submissions are judged on log RMSE. \n\n**Why this is interesting**\n\nThis is an interesting competition because there are a lot of variables, with a lot of Nan values within them. We can also find a lot of opportunities for feature engineering.\n\n**Analysis Strategy**\n\nWe're going to do a very small amount of data cleanup and feature engineering, then target encode the categorical features. Finally, we'll create several types of models and stack the results of the models to create the final prediction.","fc828f05":"## Generate Predictions","17a46406":"## Stack","ab1e41c4":"## Final cleanup & Save"}}