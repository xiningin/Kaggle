{"cell_type":{"9139b37c":"code","96850157":"code","c6a68192":"code","fa95d614":"code","d5f99591":"code","5c04a92a":"code","80203604":"code","5153cf61":"code","1c523494":"code","b8cf6b1d":"code","09f045c8":"code","b30d05d8":"code","9ce040a6":"code","6c39da8f":"code","40da05ee":"code","cc0a1b4f":"code","042edfc4":"code","212abfdd":"code","532801b7":"code","a810fa08":"code","482bd7a1":"code","31558b9d":"code","fb5f90a4":"code","1b0f7f9a":"code","9301b334":"code","c1081878":"code","71d4f033":"code","af8064bf":"code","1672a1dd":"code","97780750":"code","647a205d":"code","e3a65d15":"code","7d998098":"markdown","4e776201":"markdown","88e60dee":"markdown","fda4edba":"markdown","7a0429e1":"markdown","e488fc4f":"markdown","a545f600":"markdown","d2b2ef06":"markdown","e91ed289":"markdown","0745b58d":"markdown","c0a648f5":"markdown","b4f1bf19":"markdown","2c2cbb6b":"markdown","f2c12e72":"markdown","e01ef036":"markdown","b39c77ce":"markdown","c8ec1278":"markdown","1dfa247e":"markdown","f9095a03":"markdown","c1b5893d":"markdown","74cb2aea":"markdown","d0c4bd88":"markdown"},"source":{"9139b37c":"import os \nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport nltk\n#read the data\ndata = pd.read_csv('..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv')","96850157":"#pick at the first 5 rows of the data\ndata.head()","c6a68192":"data.info()","fa95d614":"data.sentiment.value_counts()","d5f99591":"#importing and installing of packages\n!pip install stanza\nimport os,re,nltk,stanza\nfrom nltk.corpus import stopwords\nfrom sklearn.model_selection import train_test_split","5c04a92a":"#for lets prepare all the functions and tools, in the end will apply them all in sequntioal order on the dataframe\ndef data_cleaning(rev:str)->str:\n    \"\"\"\n    lower-casing, decontraction  and cleaning of html tags for each review\n\n    Parameters\n    ----------\n    rev : str\n        the review that needs to be preprocessed.\n\n    Returns\n    -------\n    rev : str\n        the cleaned review .\n\n    \"\"\"\n    rev = re.sub(re.compile('<.*?>'), \"\", rev.lower())\n    rev = re.sub(\"'s\", \" is\", rev)\n    rev = re.sub(\"'ve\", \" have\", rev)\n    rev = re.sub(\"n't\", \" not\", rev)\n    rev = re.sub(\"cannot\", \" can not\", rev)\n    rev = re.sub(\"'re\", \" are\", rev)\n    rev = re.sub(\"'d\", \" would\", rev)\n    rev = re.sub(\"'ll\", \" will\", rev)\n    rev = re.sub(\"won\\'t\", \"will not\", rev)\n    rev = re.sub(\"can\\'t\", \"can not\", rev)\n    rev = re.sub(\"\\'t\", \" not\", rev)\n    rev = re.sub(\"\\'ve\", \" have\", rev)\n    rev = re.sub(\"\\'m\", \" am\", rev)\n    rev = re.sub(\"[^a-z ]+\", '', rev.replace('.',' ').replace(',',' '))\n    return rev","80203604":"nltk.download('stopwords')\nstop_words = set(stopwords.words('english')) - {'not', 'nor','no'}\ndef remove_bad_words(rev:str)->str:\n    \"\"\"\n     stop words(except no,not and nor) and single character word removal\n        \n     Parameters\n     ----------\n     rev : str\n         review to be cleaned.\n\n     Returns\n     -------\n     rev : str\n         review after the cleaning process.\n\n     \"\"\"\n    temp=\"\"\n    for word in rev.split():\n        if word not in stop_words and len(set(word))>1:\n            temp+=word+ \" \"\n    return temp","5153cf61":"stanza.download('en')\nnlp = stanza.Pipeline(lang='en', processors=\"tokenize, mwt,pos,lemma\",tokenize_no_ssplit=True)\ndef lematize(rev:str)->list:\n    \"\"\"\n    lemattization of the review. \n        \n    Parameters\n    ----------\n    rev : str\n        the review that we want to lemmatize.\n\n    Returns\n    -------\n    rev : list\n        the review after tokenization and lematization.\n\n    \"\"\"\n    return [ word.lemma for sent in nlp(rev).sentences for word in sent.words]","1c523494":"def split_into_train_test_dev():\n    global data\n    temp = {}\n    temp['train'], temp['test'] = train_test_split(data, test_size=0.15, random_state=42)\n    temp['train'] , temp['dev'] = train_test_split(temp['train'], test_size=0.15, random_state=42)\n    data = temp","b8cf6b1d":"class Dictionary:\n    \"\"\"\n    a class that represents the dictionary of the text\n    the class gives a unique index to every word\n    \"\"\"\n    def __init__(self):\n        self.word2idx = {\"PAD\":0}\n        self.idx2word = {0:\"PAD\"}\n        self.word2freq = {\"PAD\":1}\n        self.idx = 1\n    \n    def add_words(self,input):\n        \"\"\"\n        Input: word or words \n        InputType: list,tuple or str\n        a method to add a word(s),\n        the method checks if the word(s) is allredy in the dictinoary\n        if not, add it.\n        othewith adds one to its freq\n        \"\"\"\n        def add():\n            if word not in self.word2idx.keys():\n                self.word2idx[word] = self.idx\n                self.idx2word[self.idx] = word\n                self.word2freq[word] = 1\n                self.idx+=1\n            else:\n                self.word2freq[word] += 1\n       \n        def addSeq():\n            nonlocal word\n            for word in input:\n                add()\n        inputType = type(input)\n        word = []\n        if inputType != list and inputType != str and inputType != tuple:\n            raise TypeError (\"dict at add_word :the type of the input is not allowed\")\n        if inputType == \"str\":\n            input = [input]\n        addSeq()\n            \n            \n    def __getitem__(self,words):\n        try:\n            if type(words) == str:\n                return self.word2idx[words]\n            return [self.word2idx[word] for word in words]\n\n        except KeyError:\n            print(\"The word does not exists in the dict\")\n            return None\n    \n    def __setitem__(self,word,id):\n        \"\"\"\n        add (word,id) to the dict iff the word does not appear in the dict\n        \"\"\"\n        if word not in self.word2idx.keys():\n            self.word2idx[word] = id \n            self.word2idx[id] = word \n            self.idx+=1\n\n            \n    def __len__(self):\n        \"\"\"\n        returns the size of the dictionary\n        \"\"\"\n        return len(self.word2idx)","09f045c8":"dictionary =Dictionary()\ndef fit_on_train(rev:list)->list:\n    \"\"\"\n    building dictionary based on the words in the train set\n    the final result of this method is a dictionary that holds \n    the following information:\n    for each word:\n        1. it's uniqe id (for future translation of the words in case of embedding layer)\n        2. the freq of each word in the set (for optional filtering of un-freqent words)\n \n\n    Parameters\n    ----------\n    rev : list\n        list of words.\n\n    Returns\n    -------\n    rev : list\n        list of words.\n\n    \"\"\"\n    dictionary.add_words(rev)\n    return rev","b30d05d8":"def transform(rev:list)->list:\n    \"\"\"\n    function to transform a review into sequence \n\n    Parameters\n    ----------\n    rev : list\n        list of tokens.\n \n    Returns\n    -------\n    list\n        returns the transformed review.\n\n    \"\"\"\n    return [dictionary[word] for word in rev if dictionary[word]]","9ce040a6":"def build_weight_matrix():\n    \"\"\"\n    a function that creates weight_matrix out of FastText pre-trained vectors\n    for future use as a weights init of an embedding layer\n    if there is no vector for the word that we inizlize it with random vector\n        \n    Returns\n    -------\n    weight_matrix : list\n    \"\"\"\n    def init_fast_text():\n        nonlocal fastText,dim\n        print(\"Loading FastText pre-trained vectors\")\n        with open('..\/input\/fasttext-wikinews\/wiki-news-300d-1M.vec', 'r', encoding='utf-8', newline='\\n',errors='ignore') as f:\n            _, dim = map(int, f.readline().split())\n            fastText = {}\n            for line in f:\n                tokens = line.rstrip().split(' ')\n                fastText[tokens[0]] = np.asarray(tokens[1:], \"float32\")\n    \n    def build_matrix():\n        nonlocal weights_matrix\n        print(\"starting to build weight matrix for embedding encoding,  based on FastText pre-trained vectors\")\n        maching_words = 0\n        dataset_size = len(dictionary)+1\n        weights_matrix = np.zeros(shape=(dataset_size,dim))\n        for i,word in enumerate(dictionary.word2idx.keys(),1):\n            try:\n                save = fastText[word]\n                maching_words += 1\n            except KeyError:\n                save = np.random.uniform(size=(dim,))\n            weights_matrix[i] = save\n                     \n        print(\"pre-treind words: {} randomaly initilaized: {}\".format(maching_words,dataset_size))     \n    fastText,dim,weights_matrix=[],0,[]\n    init_fast_text()\n    build_matrix()\n    data['matrix'] = weights_matrix","6c39da8f":"data.review = data.review.apply(data_cleaning)\ndata.review = data.review.apply(remove_bad_words)\ndata.review = data.review.apply(lematize)\ndata.sentiment = data.sentiment.map(lambda sent:{'positive':1,'negative':0}[sent])\nsplit_into_train_test_dev()\ndata[\"train\"].review.apply(fit_on_train)\nmax_len = data[\"train\"].review.apply(len).max()\nfor set in [\"train\",\"test\",\"dev\"]:     \n    data[set].review = data[set].review.apply(transform)\n    data[set].review = data[set].review.apply(lambda rev:rev+[0]\n                                                      *(max_len-len(rev)) \n                                   if len(rev)<max_len else rev[:max_len])\n    data[set] ={\"X\":np.array(data[set].review.to_list()),\"Y\":np.array(data[set].sentiment.to_list())}   ","40da05ee":"build_weight_matrix()","cc0a1b4f":"#for future use of the preprocessed data , so we will not do the preprocess again and agin on the draft , saving!\nimport pickle\ndata['dict'] = dictionary\nwith open(\"\/kaggle\/working\/data.pkl\",\"wb\") as f:\n     pickle.dump(data,f)","042edfc4":"#load the  trained data from the last draft session\nimport pickle\nwith open('..\/input\/pretrained-data-from-previous-draft-session\/data.pkl',\"rb\") as f:\n     data = pickle.load(f)","212abfdd":"#importing\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense,Embedding,Flatten \nfrom tensorflow.keras.callbacks import ModelCheckpoint\n#callback: we will monitor val_acc (because it is a classification problem) and save the best model.\ndef checkpoint(name:str):\n    return ModelCheckpoint(name, monitor='val_binary_accuracy', verbose=1, save_best_only=True,\n                                   save_weights_only=False, mode='auto', save_freq=\"epoch\")","532801b7":"#baseline\nmodel = Sequential([Embedding(len(data['dict'])+1, 300,weights=[data['matrix']],input_length=data['train']['X'].shape[-1],\n                              mask_zero=True,name=\"words_latent_space\"),\n                    Flatten(name=\"flat\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"Baseline\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=8,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/fc_baseline.h5')])","a810fa08":"#configure ploting function for future use\nimport matplotlib.pyplot as plt\ndef plot(hist:dict,y_title:str,plot_title:str):\n    plt.figure(figsize=(6,2),dpi=140,facecolor=\"w\")\n    plt.grid(c='black',linestyle=\"-\",linewidth=2)\n    plt.ylabel(y_title)\n    plt.xlabel(\"epochs\")\n    plt.title(plot_title)\n    plt.plot(hist)","482bd7a1":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","31558b9d":"model = Sequential([Embedding(len(data['dict'])+1,200,input_length=data['train']['X'].shape[-1],\n                              mask_zero=True,name=\"words_latent_space\"),\n                    Flatten(name=\"flat\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"200_emd\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=8,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/fc_baseline_200_emd.h5')])","fb5f90a4":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","1b0f7f9a":"from tensorflow.keras import regularizers\n\nmodel = Sequential([Embedding(len(data['dict'])+1,200,input_length=data['train']['X'].shape[-1],\n                              embeddings_regularizer=regularizers.l2(1e-6),\n                              mask_zero=True,name=\"words_latent_space\"),\n                    Flatten(name=\"flat\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"200_emb_d\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=5,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/fc_baseline_200_emb_r.h5')])","9301b334":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","c1081878":"model = Sequential([Embedding(len(data['dict'])+1,128,input_length=data['train']['X'].shape[-1],\n                              embeddings_regularizer=regularizers.l2(1e-6),\n                              mask_zero=True,name=\"words_latent_space\"),\n                    Flatten(name=\"flat\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"128_emd\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=5,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/128_emd.h5')])","71d4f033":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","af8064bf":"#baseline\nfrom tensorflow.keras.layers import Bidirectional,LSTM\nmodel = Sequential([Embedding(len(data['dict'])+1, 300,weights=[data['matrix']], mask_zero=True),\n                    Bidirectional(LSTM(300),name=\"bi-lstm\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"lstm_base\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=5,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/lstm_base.h5')])","1672a1dd":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","97780750":"model = Sequential([Embedding(len(data['dict'])+1, 128,mask_zero=True),\n                    Bidirectional(LSTM(128),name=\"bi-lstm\"),\n                    Dense(1, activation='sigmoid',name='classifyer')],name=\"lstm_128\")\n\nmodel.compile(loss='binary_crossentropy',\n              optimizer='adam',\n              metrics=['binary_accuracy'])\nmodel.summary()\nhist = model.fit(data['train']['X'], data['train']['Y'], batch_size=64, epochs=5,\n                                  validation_data=(data['dev']['X'],data['dev']['Y']),\n                 callbacks=[checkpoint('\/kaggle\/working\/lstm_128.h5')])","647a205d":"plot(hist.history[\"binary_accuracy\"],\"acc\",\"train\");plot(hist.history[\"val_binary_accuracy\"],\"val_acc\",\"dev\")","e3a65d15":"from tensorflow.keras.models import load_model\nbest_model= load_model('\/kaggle\/working\/lstm_128.h5')\neval=best_model.predict_classes(data['test']['X'])\nfrom sklearn.metrics import classification_report\nprint(classification_report(eval, data['test']['Y']))                        ","7d998098":"### 4. split the data into train-dev and test sets (train: 36,152 , dev: 6,375, test: 7,473) \n    we must do the spliting before we build the numerical representaion of the words,\n    the reason for this is that we want to avoid Data leakage.\n\n**Data leakage**\n\n    we want to  separate our train set from the test and dev sets,\n    so we can check the preformance of our model on outside\\real-world data.\n\n    if our train will not completele separated from the others sets we will have a data leakage:\n    meaning our sets will not show us how our model really preforms on the outside data.\n    the result of data leakage can be: good preformance on all the sets but poor preformance on outside data.\n\n    to avoid this problem  we must build numerical representaion (fit) only \n    of our train set and translate the words of all the sets to numbers based on this representaion.\n","4e776201":"### 8. building an weight-marrix based on FastText vectors for future init of the embedding layer.\n#### Word Embedding\n    This is a very popular set of models that maps words from the vocab into an numerical vectors. \n    These models help the computer \u201cunderstand\u201d the semantic and syntactical similarity between words and\n    the context of the word regarding to the document.\n    The word represented by dense vector; similar words will be represented as similar vectors.\n    \n#### Word2Vec \n    At the year of 2013, Mikolov a researcher at google labs suggested a new model. This model is one\n    of the most popular models up to this day.\n    This model uses unsupervised shallow neural network with one hidden layer.\n    There are two different model architectures which can be leveraged by Word2Vec to create these\n    word embedding representations:\n\n##### Continuous Bag of words (CBoW)\n    The input of this architecture is bunch of words (vectors) and the output is one word (as vector).\n    As a matter of fact, this architecture tries to understand what the word by looking on its surroundings,\n    we can look at this as \u201ctell me who your friends and family and I will tell you who you are\u201d\n##### Skip Gram\n    This is the opposite of CBoW architecture, in this model the input is one word and the output are the\n    surrounding words of this word.\n    According to Mikolov those to architectures behave differently, and each one of them has its own\n    pros and cons.\n    The skip-gram model works better with smaller amount of data and can represent rear words better\n    than CBoW. On the other hand, CBoW is faster and can represent better words that appears more\n    frequently.\n\n##### FastText \n    This model  firstly introduced by Facebook in the year of 2016 as an extension of Word2Vec.\n    The main improvement over Word2Vec is the use of sub- word information meaning, while Word2Vec\n    tries to find a unique vector for each word, the FastText tries to find a vector for sequences of chars\n    within each word. Each word becomes the sum of the sub-word vectors.\nwe will use a pre-trained FastText vectors to init the weight-marrix of the embedding layer of our model.\nwords that does not appear in fastText will be randomally init.","88e60dee":"### 1. data cleaning: get rid of the html tags, punctutions and numbers + start of dim reduction with  decontraction of words and lower-casing.\n    we will do this step with the help of our old freind regex.\n","fda4edba":"## Problem defintion and Modeling\n\n    our dataset contains labeled samples so -> our problem is supervised learning.\n    for each sample we want identify to which  category(distinct value) it belongs so -> we got ourselves a classification problem.\n    and becouse we have only to categorises it is a binary classification problem.\n    \n    we do not want to  use here classic techniques such as (logistic regression, svm etc.), \n    we will  use nerual network to do this task, neural networks good in text classification problems\n    becuse  they can capture non-liniear relationships.\n        ","7a0429e1":"### TEST","e488fc4f":"### Model2: fc+emneddings (less units in latent space)","a545f600":"## LSTMS\ntruth be told, the dataset is to small for this, but let give it a shot.\nlet's add to our baseline a bidirectional lstm with 300 cells\n    1. LSTM good for text and sequentes becuse of its memory cells.\n    2. bidirectional feed the model from the past to the future as long as future to the past.","d2b2ef06":"### first impression\n    1.we can see that our dataset have two columns:\n      the first is the review and the second is the corresponding sentiment.\n\n    2.we can see that the dataset comes unprocessed (in plain languge , with html tags).\n\nnow, let's dive a little deeper!","e91ed289":"## Preprocessing\nthe next step is preprocessing, as we saw in the EDA section the data is in plain text and with tags.\nthe computer can not understand english or any other languge,\nthe only thing that it understands is numbers, so our primary goal is to translate the data into computer-understandable lang\n    our steps in this section (not nececerly in this order):\n    1. data cleaning: \n       get rid of the html tags, punctutions, numbers and one (repeted) letter words (like aaaaa).\n    2. dim reduction: \n       lowercase of the text, decontraction of the words (can't->can not) and stop words removal\n       (words like is,are that does not give to much information) \n       we will not remove: no, not and nor becuse this kind of stop words gives us alot of information\n       and may  change the meaning of the review for example:\n       \"this is a bad\" and \"this is not as bad as it seems\" have abslutly different meaning.\n    3. dim reduction: word normalization. \n    4. spliting of the data into train-dev and test sets with the following ratio (70,15,15).\n    5. numerical-encoding of the words (optional removal of un-freq words) based on the train set only(to avoid data-leakege.\n    6. padding of the reviews to maxlen.\n    7. numerical-encoding of the sentiments.\n    8. building an weight-marrix based on FastText vectors for future init of the embedding layer.\n","0745b58d":"### 90 acc and f1 on the test!  not bad!","c0a648f5":"### Future improvment ideas\nthere are some improvements that we can try:\n    1.  add data, add data, add data, data!!!\n    2.  use FastText without training option-> use the weights as is.\n    3.  try different pre-trained vectors (Glove or Word2Vec).\n    4.  go deeper alongside with better regularization.\n    5.  better data cleaning.\n    6.  for noise reduction trunscate reviews (instaed of max padding), we need to learn the distrabution of the reviews\n        and if there is not so much max_len reviews,we should trunscate them.\n    7.  learn the distrabution of words and trunscate the vocab for noise reduction:\n        remove un-frequent words (I have made a prepration for this in the dictionary class,\n        all we need to do is modify the transform  function.\n    8.  do a grid search do determinate the best hyper-paramters including: num of epochs,batch size and learning rate.\n    9.  try different optimizers.\n    10. for better feture extracion use a conv layer combined with the lstm.\n    11. for better capturing of local context along with global context , \n        we can try and combine multi-channel cnn with the original input as input of the lstm.\n    12. use self-attention models for sentence level embeddings (BERT,RobeRta, etc.).\n ## first steps should be better preprocessing(4-7) and more data  ","b4f1bf19":"## Data Loading and  Exploratory Analysis\n    before we jump into coding, into preprocessing and modeling,\n    let's take a few lines of code to understand our data, what info it holds within, it's form and it's level of preprocessing.\n","2c2cbb6b":"### insights\n   as we can see, our best validation score is  0.8827 (not bad) but: **we have overfiting!**\n\n  \n#### Overfiting!\n    overfiting ocuurs when the model starts to memorize instead of learning or understanding,\n    in this scenario the model will give us a good results on the train set \n    but very bad on dev\/test (on un-seen samples). \n    this situation is similar to a student that learns to an exam by memorizing two past exams,\n    well sure if he gets by some miracle one of those past exams (aka. data leakage) he will succeed,\n    but if he will get a completely new exam, he will fail.\n    \n    overfiting ussally occurs when our model is too complex for the data.\n      \n  we can handle overfiting with the following tequnices:\n    1. early stopping. we do not need this becuase we are saving the best model.\n    2. regularization: intituvly give pennalty to noise\n       (or to something that allmost all the time irelvent and relvant only for some minor scenarios).\n    3. dropout (also one of the regularization technique:\n       randomaly remove a set of connections between the neurons (disable features,make neurons not to fire).\n    4. reducing the nn\u2019s capacity(reduce trainable params):\n       too many params can add noise, to little can miss importent data.\n    5. add data.   \n       \nmost of our network is simple as is, let's try to neglect FastText and let the network try and learn new word embeddings from scratch on smaller latent space.","f2c12e72":"### Model 2: less cells","e01ef036":"### 3.dim reduction: word normalization.\n    the meaning of \"ask\" and \"asked or \"help\" and \"helped\" is the same except the time factor,\n    in NLP ofthen we want our model to recotnize different forms of the same verb\n    as the verb (or root) itself,\n    or in other words: we want to map or reduce word into it's basic form\\unit.\n    \n    it is good for us becuase of main two reasons:\n        1. the meaning is the same!(if we do not need the tense factor).\n        2. dim reduction, instad of having more then on representor to a word\n           with the same meaning we will have exactly one (it may boost the preformance of the model).\n\n    essentially: this is a normalization techniqe.\n\nthere are two main approtches to achive this goal:\n\n** Stemming **\n\n    it is a heuristic rule-based(regex) approch and it is the simpler approach. \n    this approach tries to reduce the word into it's stem, the stem may or maynot be the acctual morphological root.\n\n** Lemmatization **\n\n    becuse the stemming approtch based on heuristics, it is suffering from problems like: overstemming and understemming.\n    the lemmatization taking different approtch, \n    the process of lemmatizing is map a word into it's lemma or in other words: into its dictionary canonical form.\n\n we will do Lemmatization with the help of stanfords stanza.","b39c77ce":"### let's add L2 regularizetion on the embeddings weights","c8ec1278":"### 5. numerical-encoding of the words (optional removal of un-freq words) based on the train set only","1dfa247e":"### 2. dim reduction cont.: stop_words removal and the last part of the data cleaning: removal of words like \"aaaa\".\n","f9095a03":"### model1: baseline\n    for our baseline we will use a simple embeddings model:\n        1. input layer.\n        2. embedding layer (maps each word into dense vector in the latent space, close words wiil be closed at the \n        vector space(see word embedding at the top). \n        for now, we will init this layer with the weights_matrix.\n        3. Flatten -> flat the data for the Dense layer.\n        4. output layer: dense (fc) layer.\n           **with sigmoid activation function which gives us a  probability of the sample being in the positive category.\n         this model is (give or take) a logistic reggresion model.","c1b5893d":"### some new insigths\nok so we made another step in our journey of understanding our dataset!\nthere are two very importent insights:\n\n    1.we have 50,000 reviews and sentiments, there is not any null objects.\n    2.our dataset is balanced, we have 25,000 positive reviews and 25,000 negetive.\n","74cb2aea":"### tie it all together\n","d0c4bd88":"## Introduction\n    Opinnion mining is a task that if done correctly may give alot of useful information to a buissness,\n    Information that may help the buissness grow.\n\n    some examples of the usfuleness of this task:\n    1. what is the public opinion about a political canidadte? what does pepole think about his new plan? \n    2. what does pepole think about the new phone? about it's feutures?\n\n    the first question may help in predicting the outcome of an elections or help the candiadte with future decisions.\n\n    the second question may help the company with future decisions and may help on predicting the next day stock value of the company.\n\nin this notebook we will try to use serval deep learning methods to make opinion mining on movie reviews. \n"}}