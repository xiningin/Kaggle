{"cell_type":{"edb9cc49":"code","34dfd359":"code","e3e7cd4b":"code","c1e483b4":"code","24ed9531":"code","26ac3183":"code","08c11f20":"code","123165b3":"code","58bdf511":"code","3fa92eef":"code","5c88f967":"code","917fdc30":"code","ec413af7":"code","f2ff8efa":"code","69bca279":"code","43e158bf":"code","16768ab8":"code","b9920c68":"code","e49b35df":"code","2d84c60f":"code","4d145f53":"code","336538db":"code","ab81d46a":"code","abf72e45":"code","1c06c371":"code","64c671d8":"code","7999b275":"code","0208233c":"code","d3d90638":"code","23218bb2":"code","f144621e":"code","cffe1edc":"code","dc24808f":"code","2d56878f":"code","9834c95b":"code","a9aca8c4":"code","21f13ad1":"code","39784a1d":"code","3a7fc2ed":"code","f649ed06":"code","e94164d7":"code","c7755412":"code","e189c80c":"code","dc824992":"code","1aafc512":"code","91edb249":"code","a6a52f30":"code","89d60f43":"code","1cbd663d":"code","f4390270":"code","8799b874":"code","21cf28a6":"code","11c291c0":"code","69b82014":"code","7129e574":"code","70541f58":"code","dfca6b2b":"code","ed0e2b6f":"code","48ec8a7d":"code","ad4a4ed5":"code","7fa8326b":"code","8f59ef2d":"code","2ef864ff":"markdown","8a37e261":"markdown","32d956c1":"markdown","06801307":"markdown","c710d4eb":"markdown","d789f743":"markdown","d323854a":"markdown","12c74a52":"markdown","6367d85d":"markdown","dc220fbe":"markdown","59b773b5":"markdown","941c86f8":"markdown","f4bd2302":"markdown","4132ca91":"markdown","594ab4e1":"markdown","397bedf2":"markdown","cbcb165e":"markdown","c1eea2aa":"markdown","0ce7e320":"markdown","e180f237":"markdown","9d5c3bac":"markdown","288e0286":"markdown","8332fef5":"markdown","eca71b0d":"markdown","eaf54f57":"markdown","ee2e93c4":"markdown","02467966":"markdown","47b6ff39":"markdown","aac9753c":"markdown","a1ec210a":"markdown","2bfacdb8":"markdown","abbc007e":"markdown","6dc1475b":"markdown","9314e676":"markdown","e3e3d76e":"markdown","008885f5":"markdown","d3fddba2":"markdown","a8e07c28":"markdown","ef38ece4":"markdown","117bdca7":"markdown"},"source":{"edb9cc49":"#import packages\n\n%matplotlib inline\nimport pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom wordcloud import WordCloud, STOPWORDS\nimport warnings\nwarnings.filterwarnings(\"ignore\")","34dfd359":"#read data file\ndata = pd.read_csv('..\/input\/movie_metadata.csv')","e3e7cd4b":"pd.set_option('display.max_columns',None)\ndata.head()","c1e483b4":"data.tail(3)","24ed9531":"data.shape","26ac3183":"data.describe()","08c11f20":"data.info(verbose=False)  # check what kind of data are","123165b3":"#Check how many values are null in each column\ndata[data.columns[:]].isnull().sum()","58bdf511":"data[data['imdb_score']>7.5].shape[0]","3fa92eef":"plt.rcParams['figure.figsize']=(18,9)\n\ndata_groupby_ratings = data.groupby(['imdb_score'])['movie_title'].count()\ndata_groupby_ratings.plot()","5c88f967":"data_groupby_duration = data.groupby(['duration'])['movie_title'].count()\ndata_groupby_duration.plot()","917fdc30":"data[data['duration'] <= 100].shape[0]","ec413af7":"data[data['duration'] >= 180].shape[0]","f2ff8efa":"# use a visualization to detect whether there is a relationship between duration and star rating\ndata.boxplot(column='duration', by='imdb_score');","69bca279":"# visualize the relationship between content rating and duration\ndata.boxplot(column='duration', by='content_rating')","43e158bf":"data['language'].unique()","16768ab8":"sns.set(style=\"darkgrid\")\nplt.figure(figsize = (12, 6))\nsns.countplot(x=\"language\", data = data)\nax = plt.xticks(rotation=90)","b9920c68":"sns.set(style=\"darkgrid\")\nsns.countplot(x=\"color\", data = data)","e49b35df":"# plot title year vs gross\ndata_groupby_gross = data.groupby(['title_year'])['gross'].count()\ndata_groupby_gross.plot()","2d84c60f":"#ploting buget vs title_year\ndata_groupby_gross = data.groupby(['title_year'])['budget'].count()\ndata_groupby_gross.plot()","4d145f53":"data[data['language'] == 'English'].shape[0] # number of english movies","336538db":"highest_imdb = data.sort_values('imdb_score', ascending = False)\nhigh = highest_imdb.loc[:,['movie_title', 'imdb_score','title_year', 'language', 'country', 'budget', 'director_name', 'duration', 'gross' ]]\nhigh.head(10)","ab81d46a":"#French top 5 rated movies\nfrench = high[high['language']== 'French']\nfrench.head(5)","abf72e45":"#find proportion of missing values\nprop_missing = round((data[data.columns[:]].isnull().sum()\/data.shape[0])*100,2)\nprop_missing","1c06c371":"col_filling = []\nfor s in data.columns:\n    ratio = (len(data[s])-data[s].isnull().sum()) \/ len(data[s])*100\n    number = data[s].notnull().sum()\n    col_filling.append([ratio, s, number])\ncol_filling.sort(key = lambda x:x[0])\n#------------------------------------\nfor ratio, s, number in col_filling:\n    print(\"{:<30} -> {:<6}%\".format(s, round(ratio,2)))","64c671d8":"#Remove the missing data with title year missing\nclean_data = data[data.title_year.notnull() & data.duration.notnull()]\nlen(clean_data)","7999b275":"clean_data.loc[:, 'title_year'] = clean_data['title_year'].astype(int).astype(str)\nclean_data.loc[:, 'year'] = pd.to_datetime(clean_data['title_year'], format='%Y')","0208233c":"#describe the dataset\nclean_data.describe()","d3d90638":"#Get data required for the plot\ndf_1 = clean_data[['title_year', 'movie_title']]\nser = df_1.groupby(df_1.title_year.astype(int) \/\/ 10 * 10).size()\ndf = pd.DataFrame({'decade':ser.index, 'movies':ser.values})","23218bb2":"#Plot using plt.subplots\nfig,ax = plt.subplots()\nax.bar(df.decade, df.movies, width=2.6, color='b')\nax.set_xticks(df.decade+1.3)  # set the x ticks to be at the middle of each bar since the width of each bar is 2.6\nax.set_xticklabels(df.decade)  #replace the name of the x ticks with your Groups name\nax.grid(False) #remove gridlines\nplt.xlabel('Decade', fontsize=16)\nplt.ylabel('No of movies released', fontsize=16)\nplt.title('Movies released by decade', fontsize=24)\nplt.show()","f144621e":"data['decade'] = data['title_year'].apply(lambda x:((x-1900)\/\/10)*10)\n#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}\n#______________________________________________________________\n# Creation of a dataframe with statitical infos on each decade:\ntest = data['title_year'].groupby(data['decade']).apply(get_stats).unstack()['decade'] = data['title_year'].apply(lambda x:((x-1900)\/\/10)*10)\n#__________________________________________________________________\n# function that extract statistical parameters from a grouby objet:\ndef get_stats(group):\n    return {'min': group.min(), 'max': group.max(),\n            'count': group.count(), 'mean': group.mean()}\n#______________________________________________________________\n# Creation of a dataframe with statitical infos on each decade:\ntest = data['title_year'].groupby(data['decade']).apply(get_stats).unstack()","cffe1edc":"sns.set_context(\"poster\", font_scale=0.85)\n#_______________________________\n# funtion used to set the labels\ndef label(s):\n    val = (1900 + s, s)[s < 100]\n    chaine = '' if s < 50 else \"{}'s\".format(val)\n    return chaine\n#    if s < 50:        \n#        return ''\n#    elif s < 100:\n#        return \"{}'s\".format(int(s))\n#    else:\n#        return \"{}'s\".format(int(1900+s))\n#____________________________________\nplt.rc('font', weight='bold')\nf, ax = plt.subplots(figsize=(14, 6))\nlabels = [label(s) for s in  test.index]\nsizes  = test['count'].values\nexplode = [0.2 if sizes[i] < 100 else 0.01 for i in range(11)]\nax.pie(sizes, explode = explode, labels=labels,\n       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n       shadow=False, startangle=0)\nax.axis('equal')\nax.set_title('% of films per decade',\n             bbox={'facecolor':'k', 'pad':5},color='w', fontsize=16);","dc24808f":"genre_labels = set()\nfor s in data['genres'].str.split('|').values:\n    genre_labels = genre_labels.union(set(s))","2d56878f":"def count_word(df, ref_col, liste):\n    keyword_count = dict()\n    for s in liste: keyword_count[s] = 0\n    for liste_keywords in df[ref_col].str.split('|'):\n        if type(liste_keywords) == float and pd.isnull(liste_keywords): continue\n        for s in liste_keywords: \n            if pd.notnull(s): keyword_count[s] += 1\n    #______________________________________________________________________\n    # convert the dictionary in a list to sort the keywords by frequency\n    keyword_occurences = []\n    for k,v in keyword_count.items():\n        keyword_occurences.append([k,v])\n    keyword_occurences.sort(key = lambda x:x[1], reverse = True)\n    return keyword_occurences, keyword_count","9834c95b":"keyword_occurences, dum = count_word(data, 'genres', genre_labels)\nkeyword_occurences[:5]","a9aca8c4":"# Function that control the color of the words\n\ndef random_color_func(word=None, font_size=None, position=None,\n                      orientation=None, font_path=None, random_state=None):\n    h = int(360.0 * tone \/ 255.0)\n    s = int(100.0 * 255.0 \/ 255.0)\n    l = int(100.0 * float(random_state.randint(70, 120)) \/ 255.0)\n    return \"hsl({}, {}%, {}%)\".format(h, s, l)","21f13ad1":"words = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 100 # define the color of the words\nf, ax = plt.subplots(figsize=(14, 6))\nwordcloud = WordCloud(width=550,height=300, background_color='black', \n                      max_words=1628,relative_scaling=0.7,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","39784a1d":"set_keywords = set()\nfor liste_keywords in data['plot_keywords'].str.split('|').values:\n    if type(liste_keywords) == float: continue  # only happen if liste_keywords = NaN\n    set_keywords = set_keywords.union(liste_keywords)","3a7fc2ed":"keyword_occurences, dum = count_word(data, 'plot_keywords', set_keywords)\nkeyword_occurences[:5]","f649ed06":"#_____________________________________________\n# UPPER PANEL: WORDCLOUD\nfig = plt.figure(1, figsize=(18,13))\nax1 = fig.add_subplot(2,1,1)\n#_______________________________________________________\n# I define the dictionary used to produce the wordcloud\nwords = dict()\ntrunc_occurences = keyword_occurences[0:50]\nfor s in trunc_occurences:\n    words[s[0]] = s[1]\ntone = 55.0 # define the color of the words\n#________________________________________________________\nwordcloud = WordCloud(width=1000,height=300, background_color='black', \n                      max_words=1628,relative_scaling=1,\n                      color_func = random_color_func,\n                      normalize_plurals=False)\nwordcloud.generate_from_frequencies(words)\nax1.imshow(wordcloud, interpolation=\"bilinear\")\nax1.axis('off')\n#_____________________________________________\n# LOWER PANEL: HISTOGRAMS\nax2 = fig.add_subplot(2,1,2)\ny_axis = [i[1] for i in trunc_occurences]\nx_axis = [k for k,i in enumerate(trunc_occurences)]\nx_label = [i[0] for i in trunc_occurences]\nplt.xticks(rotation=85, fontsize = 15)\nplt.yticks(fontsize = 15)\nplt.xticks(x_axis, x_label)\nplt.ylabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\nax2.bar(x_axis, y_axis, align = 'center', color='g')\n#_______________________\nplt.title(\"Keywords popularity\",bbox={'facecolor':'k', 'pad':5},color='w',fontsize = 25)\nplt.show()","e94164d7":"#get data\ntemp2 = clean_data[['title_year', 'imdb_score']]\n#plot\ntemp2 = temp2.groupby(temp2.title_year.astype(int)).imdb_score.mean().plot(kind ='line', grid =False, title ='IMDB Average Score Trend', xlim=((1950, 2016)))\ntemp2.xaxis.set_ticks(np.arange(1950, 2016, 7))\nplt.xlabel('Year', fontsize=18)\nplt.ylabel('Average IMDB Score', fontsize=18)\nplt.tight_layout()","c7755412":"#create new table with grouped information\ntemp = clean_data[['title_year', 'imdb_score', 'movie_imdb_link']]\ntemp = temp[temp.title_year.astype(int)>1949]\nres = temp.groupby(temp.title_year.astype(int)).agg({'imdb_score': 'mean', 'movie_imdb_link': 'count'}).reset_index()\nres.columns = ['title_year', 'avg_imdb_score', 'movies_created']\nrows = res.title_year\n\nfig = plt.figure()\nax1 = fig.add_subplot(111)\nax1.plot(res.title_year, res.avg_imdb_score, color = 'blue')\nax1.set_ylabel('Average IMDB Score', color = 'blue')\nax1.set_xlabel('Year')\nax1.grid(False)\n#ax1.legend(loc = 'upper right')\n\n\nax2 = ax1.twinx()\nax2.plot(res.title_year, res.movies_created, color='green')\nax2.set_ylabel('Movies Released', color = 'green')\nax2.grid(False)\nfor tl in ax2.get_yticklabels():\n    tl.set_color('r')\n#ax2.legend(loc = 'upper right')\nplt.title('Avg IMDB Score vs Movies Released ~ Trend')\n\nplt.show()\n#plt.savefig('images\/two-scales-5.png')","e189c80c":"#Get data required for the plot\ntemp = clean_data[['content_rating', 'imdb_score']]\ntemp = temp.groupby(temp.content_rating.astype(str)).imdb_score.mean()\ndf = pd.DataFrame({'Content_Rating':temp.index, 'IMDB_Score':temp.values})\n#sort data by score descending\ndf = df.sort_values(['IMDB_Score'], ascending=[False])\n#plot\ndf.plot('Content_Rating','IMDB_Score', kind='bar')\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.xticks(rotation=0)\nplt.grid(False)\nplt.xlabel('Content Rating', fontsize=14)\nplt.ylabel('Average IMDB Score', fontsize=14)\nplt.title('Average IMDB Score per Content Rating')\nplt.tight_layout()\nplt.show()","dc824992":"#Get data required for the plot\ntemp = clean_data[['director_name', 'imdb_score']]\ntemp = temp.groupby(temp.director_name.astype(str)).imdb_score.mean()\ndf = pd.DataFrame({'Director_Name':temp.index, 'Avg_IMDB_Score':temp.values})\n#sort data by score descending\ndf = df.sort_values(['Avg_IMDB_Score'], ascending=[False])\ndf = df.head(10)\n#plot while sorting plot\ndf.sort_values('Avg_IMDB_Score').plot('Director_Name','Avg_IMDB_Score', kind='barh')\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.xticks(rotation=0)\nplt.grid(False)\nplt.xlabel('Director', fontsize=14)\nplt.ylabel('Average IMDB Score', fontsize=14)\nplt.title('Top 10 high scoring directors')\nplt.tight_layout()\nplt.legend().set_visible(False)\nplt.show()","1aafc512":"##scatterplot average_imdb_score vs movie_facebook_likes\ntemp = clean_data[['movie_facebook_likes', 'imdb_score']]\ntemp = temp[temp.imdb_score > 0]\nx = temp.plot(x='movie_facebook_likes', y = 'imdb_score',kind='scatter', xlim = (0, 100000), title='IMDB Score VS Movie facebook likes', legend=[True])\nplt.tick_params(axis='both', which='major', labelsize=12)\nplt.tick_params(axis='both', which='minor', labelsize=10)\nplt.grid(False)\nplt.xlabel('Movie Facebook Likes', fontsize=14)\nplt.ylabel('Average IMDB Score', fontsize=14)\nplt.title('Movie FB Likes VS IMDB Score')\nplt.tight_layout()\nplt.show()","91edb249":"temp[['imdb_score','movie_facebook_likes']].corr()","a6a52f30":"temp = clean_data[['duration', 'imdb_score']]\ntemp = temp.plot('duration', 'imdb_score', kind ='scatter', title ='Duration VS Mean IMDB Score')\nplt.xlabel('Duration', fontsize=12)\nplt.ylabel('Avg IMDB Score', fontsize=12)\nplt.title('Duration VS Avg IMDB Score')\nplt.grid(False)\nplt.tight_layout()\nplt.show()","89d60f43":"clean_data[['imdb_score','duration']].corr()","1cbd663d":"temp = clean_data[['duration', 'imdb_score']]\nsns.regplot(x=\"duration\", y=\"imdb_score\", data=temp);\nplt.xlabel('Duration', fontsize=12)\nplt.ylabel('Avg IMDB Score', fontsize=12)\nplt.title('Duration VS Avg IMDB Score')\nplt.grid(False)\nplt.tight_layout()\nplt.show()","f4390270":"temp = clean_data[['language', 'duration', 'title_year']]\ntemp1 = temp[temp.title_year.astype(int) >= 2000]\ntemp1 = temp1.loc[temp1['language'].isin(['English','Hindi'])]\n# temp1 = temp1[temp1.language == 'English' | temp1.language == 'Hindi']\ntemp1.groupby(temp1.language).duration.mean().plot(kind='bar')\nplt.xticks(rotation=0)\nplt.xlabel('Language')\nplt.ylabel('Avg Duration')\nplt.title('Duration of movie by Language')\nplt.grid(False)\nplt.show()","8799b874":"hindi = temp1[temp1.language == 'Hindi']\nenglish = temp1[temp1.language == 'English'] ","21cf28a6":"print(\"The dataset has {} Hindi and {} English movies\".format(len(hindi), len(english)) )","11c291c0":"#plot histogram for hindi movie durations\nhindi.duration.plot(kind='hist',color='0.5', bins = 10, title = 'Histogram for duration of Hindi movies').set_xlabel('Duration')\nhindi_mean = round(hindi[\"duration\"].mean(),2)\nhindi_sd = round((hindi[\"duration\"]).std(),2)\nprint(\"The mean duration of the Hindi movies is {} and standard deviation is {}\".format(hindi_mean, hindi_sd))","69b82014":"#plot histogram for english movie durations\nenglish.duration.plot(kind='hist',color='0.5', bins = 10, title = 'Histogram for duration of English movies').set_xlabel('duration')\nenglish_mean = round(english[\"duration\"].mean(),2)\nenglish_sd = round((english[\"duration\"]).std(),2)\nprint(\"The mean duration of the English movies is {} and standard duration is {}\".format(english_mean, english_sd) )","7129e574":"data_use = data.loc[:,['genres','plot_keywords','movie_title','actor_1_name',\n                      'actor_2_name','actor_3_name','director_name','imdb_score']]\n\ndata_use['movie_title'] = [i.replace(\"\\xa0\",\"\") for i in list(data_use['movie_title'])]","70541f58":"print(data_use.shape)\nclean_data = data_use.dropna(axis = 0)\nprint(clean_data.shape)\nclean_data = clean_data.drop_duplicates(['movie_title'])\nclean_data = clean_data.reset_index(drop=True)\nprint(clean_data.shape)","dfca6b2b":"people_list = []\nfor i in range(clean_data.shape[0]):\n    name1 = clean_data.loc[i,'actor_1_name'].replace(\" \",\"_\")\n    name2 = clean_data.loc[i,'actor_2_name'].replace(\" \",\"_\")\n    name3 = clean_data.loc[i,'actor_3_name'].replace(\" \",\"_\")\n    name4 = clean_data.loc[i,'director_name'].replace(\" \",\"_\")\n    people_list.append(\"|\".join([name1,name2,name3,name4]))\nclean_data['people'] = people_list","ed0e2b6f":"from sklearn.feature_extraction.text import CountVectorizer\n\ndef token(text):\n    return(text.split(\"|\"))\n\n\ncv_kw=CountVectorizer(max_features=100,tokenizer=token )\nkeywords = cv_kw.fit_transform(clean_data[\"plot_keywords\"])\nkeywords_list = [\"kw_\" + i for i in cv_kw.get_feature_names()]\n\ncv_ge=CountVectorizer(tokenizer=token )\ngenres = cv_ge.fit_transform(clean_data[\"genres\"])\ngenres_list = [\"genres_\"+ i for i in cv_ge.get_feature_names()]\n\ncv_pp=CountVectorizer(max_features=100,tokenizer=token )\npeople = cv_pp.fit_transform(clean_data[\"people\"])\npeople_list = [\"pp_\"+ i for i in cv_pp.get_feature_names()]\n\ncluster_data = np.hstack([keywords.todense(),genres.todense(),people.todense()*2])\ncriterion_list = keywords_list+genres_list+people_list","48ec8a7d":"from sklearn.cluster import KMeans\n\nmod = KMeans(n_clusters=100)\ncategory = mod.fit_predict(cluster_data)\ncategory_dataframe = pd.DataFrame({\"category\":category},index = clean_data['movie_title'])","ad4a4ed5":"clean_data.loc[list(category_dataframe['category'] == 0),['genres','movie_title','people']]","7fa8326b":"def recommend(movie_name,recommend_number = 5):\n    if movie_name in list(clean_data['movie_title']):\n        movie_cluster = category_dataframe.loc[movie_name,'category']\n        score = clean_data.loc[list(category_dataframe['category'] == movie_cluster),['imdb_score','movie_title']]\n        sort_score = score.sort_values(['imdb_score'],ascending=[0])\n        sort_score = sort_score[sort_score['movie_title'] != movie_name]\n        recommend_number = min(sort_score.shape[0],recommend_number)\n        recommend_movie = list(sort_score.iloc[range(recommend_number),1])\n        print(recommend_movie)\n    else:\n        print(\"Can't find this movie!\")","8f59ef2d":"recommend('Avatar')","2ef864ff":"This shows that there is weak positive correlation between imdb_score and duration, we can use this to find an optimum range of duration that gives the best scores.","8a37e261":"As we can see, the decrease in average movie scores can be attributed to the increase in amount of movies created in recent times, the increase in amount of movies will lead to more outliers affecting the mean for the duration.","32d956c1":"__This shows a growing trend of movies created every decade. The amount of movies created is growing exponentially. The last decade data is only available for 4 years (2010-2014) so it obviously shows a drop in movies created in the last decade__","06801307":"###### 1.6 Director name","c710d4eb":"__68 movies have time duration more than equal to 3 hr.\naverage movie time duration is 107 min__","d789f743":"# II. Movie Recommender Systems","d323854a":"###### 1.4 Recommender System","12c74a52":"###### 1.7 Movie Facebook likes","6367d85d":"###### 1.1 Cleaning","dc220fbe":"At this stage, the list of keywords has been created and we know the number of times each of them appear in the dataset. In fact, this list can be used to have a feeling of the content of the most popular movies. A fancy manner to give that information makes use of the wordcloud package. In this kind of representation, all the words are arranged in a figure with sizes that depend on their respective frequencies. Instead of a wordcloud, we can use histograms to give the same information. This allows to have a figure where the keywords are ordered by occurence and most importantly, this gives the number of times they appear, an information that can not be retrieved from the wordcloud representation. In the following figure, I compare both types of representations:","59b773b5":"Below plot shows comparison of movies created vs average imdb score over time","941c86f8":"__Many movies are in English and in color__","f4bd2302":"# top 10 imdb rating movies ","4132ca91":"# I. Exploration","594ab4e1":"__We can see that most of the variables are well filled since only 2 of them have a filling factor below 93%.__","397bedf2":"__Some key point from this table\nAvg movie duration is 107.2 minuts\navg imdb is 6.64\navg number of users revies is 272__","cbcb165e":"Overall, Approved and Passed content has highest score but there isnt much difference in scores for different content rating types, however TV-14 has lowest IMDB Score","c1eea2aa":"__so we have int, float , string all type of mixtures.__","0ce7e320":"###### 1.3 KMeans","e180f237":"###### 1.2 Number of films per year\n\nThe variable 'title_year' deals with the year the films came out. In order to have a global look at the way films are distributed according to this variable, I group the films by decades:","9d5c3bac":"__we can see more than 200  movies have rating of around 6.5__","288e0286":"###### 1.2 CountVectorizer","8332fef5":"It looks like the average score of movies is decreasing over time, However, this could be due to an increase in no of movies being created over time","eca71b0d":"###### 1.5 IMDB Score ","eaf54f57":"There is weak positive correlation between imdb_score and duration of movie","ee2e93c4":"__As in every analysis, at some point, we will have to deal with the missing values and as a first step, I determine the amount of data which is missing in every variable:__","02467966":"These is weak positive correlation between imdb score and movie facebook likes","47b6ff39":"Finally, the results is shown as a wordcloud:","aac9753c":"As we can see from the above plot, Hindi movies are longer than English movies, however the Hindi movie dataset only has 26 movies while the English movie dataset has 3308 movies (all realeased since year 2000). We need more data for Hindi movies in order to draw a comparison. Below we plot the histogram for these two datasets:","a1ec210a":"and then counting how many times each of them occur:","2bfacdb8":"# Summary\n\n###### I. Exploration\n\n    1.1 Missing values\n    \n    1.2 Number of films per year\n    \n    1.3 Genres\n    \n    1.4 Keywords\n    \n    1.5 IMDB Score\n    \n    1.6 Director name\n    \n    1.7 Movie Facebook likes\n    \n    1.8 Duration, IMDB Score and Language\n\n###### II. Movie Recommender Systems\n\n    1.1 Cleaning\n    \n    1.2 CountVectorizer\n    \n    1.3 KMeans\n    \n    1.4 Recommender System","abbc007e":"###### 1.1. Missing Values","6dc1475b":" - We see that the amount of movies created over time is increasing exponentially while the average IMDB score for the movies is decreasing over time\n - We see a positive correlation between IMDB Score and Movie facebook likes and between Duration of movie and its IMDB Score\n - We see a difference in mean duration of movies created since 2000 between groups of Hindi and English movies, this leads of us formulate a Hypothesis test to test if the durations are similar between the groups or not","9314e676":"###### 1.3 Genres\n\nThe __genres__ variable describes the content of the film (i.e. Drama, Comedy, Action, ...). To see exactly which genres are the most popular, I use the same approach than for the keywords (hence using similar lines of code), first making a census of the genres:","e3e3d76e":"__747 out of 5043 movies are having more than 7.5 imdb rating.\nGenrally people watch this king of rating movies__","008885f5":"__We have 5043 movies described by 28 variables__","d3fddba2":"__4704 movies are in english__","a8e07c28":"###### 1.8 Duration, IMDB Score and Language","ef38ece4":"###### 1.4 Keywords\n\nI think, a basic assumption is that films described by similar keywords should have similar contents. Hence, I plan to have a close look at the way keywords are defined and as a first step, I quickly characterize what's already in there. To do so, I first list the keywords which are in the dataset:","117bdca7":"__I represent the results in a pie chart:__"}}