{"cell_type":{"a8891b33":"code","b0a3ca77":"code","14740fde":"code","36eeb25b":"code","79af78d8":"code","0052db11":"code","5e02be43":"code","a2b8cf7b":"code","537c2b50":"code","a65c292c":"code","a1954b75":"code","638c3ae7":"code","d84267e4":"code","430acf0a":"code","5db835ad":"code","b7f98b47":"markdown","2d533bb4":"markdown","5756e3c9":"markdown","4e973551":"markdown","23339622":"markdown","95b2f93f":"markdown","ea649e59":"markdown","54bfcfb2":"markdown"},"source":{"a8891b33":"# load necessary packages\nimport pandas as pd\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\") # ignoring annoying warnings\n\n# load data\ndf_features = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip', parse_dates=['Date']) # parse_date to ensure Date in 'datetime64' format\ndf_sales = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip', parse_dates=['Date'])\ndf_stores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ndf_sales_answer = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip', parse_dates=['Date'])\nsample_submission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\n","b0a3ca77":"# combine all 4 training dataset into one. from largest to smallest\n\n# merge two sales into one for now\nsales_answer = pd.merge(df_sales ,df_sales_answer, how='outer', on=['Store', 'Dept', 'Date', 'IsHoliday'])\n\n# merge features & stores on 'store' key\nsales_feat = pd.merge(sales_answer ,df_features, how='outer', on=['Store', 'Date', 'IsHoliday'])\n\n# merge sales_feat & sales on 'Store' and 'Date' key\ndf_all = pd.merge(sales_feat, df_stores, how='outer', on='Store')","14740fde":"def multipledummies(df, non_numerical_columns):\n    ''' Input the whole dataframe & name of non-numerical columns, output is clean dataframe that all is in numerical format'''\n\n    for i in non_numerical_columns:\n\n        # convert to numerical using get_dummies\n        one_hot = pd.get_dummies(df[i], prefix=i)\n\n        # append new numerical column to main df\n        df = df.join(one_hot)\n\n        # drop that non-numerical column\n        df.drop(i, axis = 1, inplace=True)\n\n    return df","36eeb25b":"# convert Date to 'Day, Week, Month' to make it numerical\ndf_all['Day'] = df_all.Date.dt.day\ndf_all['Week'] = df_all.Date.dt.week \ndf_all['Year'] = df_all.Date.dt.year\n","79af78d8":"# convert Type columns to numerical using multipledummies\ndf_all = multipledummies(df_all, ['Type'])\ndf_all.sample(3)\n","0052db11":"# separate df into data inclusive(2010-02-05 until 2012-10-26) and answer (2012-11-02 until 2013-07-26)\n\ndata_range = (df_all['Date'] >= '2010-02-05') & (df_all['Date'] <= '2012-10-26')\nanswer_range = (df_all['Date'] >= '2012-11-02') & (df_all['Date'] <= '2013-07-26')\n\n\ndf = df_all.loc[data_range]\ndf_answer = df_all.loc[answer_range]\n\n\n# drop date column now since its been segregated properly already\ndf.drop(['Date'], axis=1, inplace=True)\ndf_answer.drop(['Date'], axis=1, inplace=True)","5e02be43":"# ensure all columns in df is in integer format before done with \"scrub\" section\ndf.info()\ndf_answer.info()\n\n# for multiple null, we check in heatmap first, if weak correlation (between 0.1 & -0.1) with weekly_sales, we drop those column. else we use IterativeImputer() package\n\n# IsHoliday stays boolean object for now. converted in 'Model' section WMAE function","a2b8cf7b":"sns.set(style=\"white\")\n\ncorr = df.corr()\n\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n\nf, ax = plt.subplots(figsize=(20, 15))\n\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\n\nplt.title('Correlation Matrix', fontsize=18)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=1, vmin=-1, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5}, annot=True, fmt='.2f')\n\nplt.show()","537c2b50":"# drop CPI, Unemployment & all markdowns1-5, as it is : weak correlation to weekly_sales AND too much Null data\ndf.drop(['CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1, inplace=True)\ndf_answer.drop(['CPI', 'Unemployment', 'MarkDown1', 'MarkDown2', 'MarkDown3', 'MarkDown4', 'MarkDown5'], axis=1, inplace=True)","a65c292c":"# WMAE function as error measurement (lower the better)\n\ndef WMAE(dataset, real, predicted):\n    ''' Input df, real value , predicted value. Output the error value. lower the value, more accurate our model is '''\n\n    # weight allocation on IsHoliday\n    weights = dataset.IsHoliday.apply(lambda x : 5 if x else 1)\n\n    # WMSE formula\n    return np.round(np.sum(weights * abs(real - predicted)) \/ (np.sum(weights)), 2)","a1954b75":"# prep data\n\nX_train = df.drop(['Weekly_Sales'], axis = 1)\nY_train = df['Weekly_Sales']\n","638c3ae7":"# model building & training\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn.model_selection import train_test_split\n\nRF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=12, min_samples_split=4, min_samples_leaf=1)\nRF.fit(X_train, Y_train)","d84267e4":"# get prediction answer\n\nX_test = df_answer.drop(['Weekly_Sales'], axis = 1)\npredict = RF.predict(X_test)\n\n","430acf0a":"sample_submission['Weekly_Sales'] = predict\nsample_submission","5db835ad":"# export file to csv\nsample_submission = sample_submission.set_index('Id')\nsample_submission.to_csv('walmart_v1.csv', sep=',')","b7f98b47":"## Most beginner-friendly notebook & straight-to-the-point (no fuss & no lengthy charts)\n- Obtain data\n- Scrub data : to make it all numerical & model-friendly\n- Explore data : correlation check with weekly_sales\n- Model : RandomForestClassifier\n- Interpret : predicting the future sales","2d533bb4":"# Model\n- Since competition error measurement is WMAE (given in competition, and not the usual RMSE from GridSearchCV), we cant use GridSearchCV or RandomSearchCV to fine-tune our model\n- for the sake of siimplicity, we'll just jump right to model building.\n- once you grasp the concept, you may tweak & google around to improve this","5756e3c9":"# Scrub\n- combine all datasets into one\n- then separate into train inclusive(2010-02-05 until 2012-10-26) and test (2012-11-02 until 2013-07-26)\n- convert all columns to numerical","4e973551":"## Author: Dwi Hadyan Harsono\n* [Github source](https:\/\/github.com\/dwihdyn\/ds-exploration\/blob\/main\/p2\/retail-simple.ipynb) ","23339622":"# iNterpret\n\n- convert output to csv that to be submitted","95b2f93f":"> Beginner friendly, as this helps you on getting the concept of data science full-cycle (once you understand, youre able to treat this as stepping-stone to improve the model accuracy)","ea649e59":"# Obtain","54bfcfb2":"# Explore\n\n- heatmap. drop if corr with targetVariable is between 0.1 & -0.1 "}}