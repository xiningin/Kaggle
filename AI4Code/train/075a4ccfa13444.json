{"cell_type":{"a6849376":"code","7c27f7da":"code","ddd27a8d":"code","51a8da2a":"code","fb8f2c85":"code","a4fcb4d1":"code","9bd8b290":"code","b75ff7e9":"code","55809766":"code","a9efaa98":"code","bae6cb7c":"code","d773d8dc":"code","a2e5bccf":"code","25da4121":"code","1b5b76b1":"code","74b1f1ec":"code","0c11cbb0":"code","8cd62bf8":"code","c2306089":"code","18715706":"code","b8675b99":"code","64351024":"code","a4d8f675":"code","b6036def":"code","d4ab5cda":"markdown","03f28150":"markdown","3387245c":"markdown","d674fce1":"markdown","e71e6c4b":"markdown","6eca2423":"markdown","1f50c754":"markdown","33813ad1":"markdown","2dd569b2":"markdown","e64bcfcf":"markdown","dcb62b19":"markdown","77eee595":"markdown","b3a4bea8":"markdown","db662884":"markdown","73fce954":"markdown","e671705e":"markdown","7ba3f9b2":"markdown","1b89aca8":"markdown","669634fd":"markdown","a0855784":"markdown","b2f8bc8b":"markdown","961e10b7":"markdown","a55ab1a1":"markdown","1177bf84":"markdown","eb1ba74c":"markdown","f5a23399":"markdown","8e35689c":"markdown"},"source":{"a6849376":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib_venn import venn3\nfrom functools import reduce\nfrom random import sample\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV, train_test_split","7c27f7da":"data_labeled = pd.read_csv(\"..\/input\/uamia-customer-leakage\/customer_leakage_labeled.csv\")\ndata_to_predict  = pd.read_csv(\"..\/input\/uamia-customer-leakage\/customer_leakage_to_predict.csv\")","ddd27a8d":"# Check if all columns are the same in both datasets\nassert(data_labeled.columns.equals(data_to_predict.columns))","51a8da2a":"# Check that Churn Status only takes values of 0 and 1\nset(data_labeled[\"Churn Status\"])","fb8f2c85":"# Check that Churn Status is Nan in \"to_predict\" data\nprint(data_to_predict[\"Churn Status\"].isnull().all())","a4fcb4d1":"features = list(data_labeled.columns)\nfeatures.remove(\"Churn Status\")\nfeatures.remove(\"Customer ID\")  # Remove ID, since it is useless for the model\n","9bd8b290":"import matplotlib.pyplot as plt\n\n\nplt.figure(figsize=(30, 30))\nfor i, name in enumerate(features):\n    plt.subplot(6,4, i + 1)\n    aux = \"Density\" if i % 4 == 0 else ''\n    \n    number_of_values = data_labeled[name].nunique()\n    if number_of_values == 2:\n        kind = \"hist\"\n        xmin = -1\n        xmax = 2\n    else:\n        kind = \"kde\"\n        xmin = data_labeled[name].quantile(0.01)\n        xmax = data_labeled[name].quantile(0.99)\n    data_labeled.groupby(\"Churn Status\")[name].plot(kind=kind, title=f\"Hist. de {name}\", xlim=(xmin, xmax), legend=True)\n    plt.ylabel(aux);","b75ff7e9":"complain_calls = data_labeled[\"Total Call centre complaint calls\"]\ncomplain_calls.plot.kde(xlim=(0, 8), title=\"Total Call Center complaint calls\").figure\nprint(f\"Average: {complain_calls.mean()}\")\nprint(f\"Median: {complain_calls.median()}\")\nprint(f\"Mode: {complain_calls.mode()[0]}\")\nprint(f\"Max: {complain_calls.max()}\")","55809766":"data_labeled.sort_values(by=\"Total Call centre complaint calls\", \n                               ascending=False)[[\"Total Call centre complaint calls\", \"Churn Status\"]].head(10)","a9efaa98":"from math import log2\nfrom numpy import round\n\ndef b(p):\n    if p == 0 or p == 1:\n        return 0\n    \n    q = 1 - p\n    return -(p*log2(p) + q*log2(q))\n\ndef entropy(column):\n    p = column.value_counts()[0] \/ column.count()\n    return b(p)\n\ninitial_entropy = entropy(data_labeled[\"Churn Status\"])\nfor name in list(features):\n    number_of_values = data_labeled[name].nunique()\n    \n    if number_of_values == 2:\n        df_true = data_labeled[data_labeled[name] == 0]\n        df_false = data_labeled[data_labeled[name] == 1]\n        \n        p = df_true.shape[0] \/ (data_labeled.shape[0])\n        q = 1 - p\n        \n        entropy_true = entropy(df_true[\"Churn Status\"])\n        entropy_false = entropy(df_false[\"Churn Status\"])\n        \n        gain = initial_entropy - (p*entropy_true + q*entropy_false)\n        gain = round(gain, 2)\n        \n        print(f\"{name} has an information gain of {round(gain, 2)}\")\n        if gain < 0.01:\n            features.remove(name)","bae6cb7c":"np.round(data_labeled[\"network_age\"] \/ data_labeled[\"Customer tenure in month\"]).unique()","d773d8dc":"features.remove(\"Customer tenure in month\")","a2e5bccf":"X = data_labeled[features].values\ny = data_labeled[\"Churn Status\"].values\n# Scale data\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=0)\n\nX_to_predict = data_to_predict[features].values\nscaler.transform(X_to_predict)","25da4121":"def get_best_ids(model, csv_name: str = None) -> pd.DataFrame:\n    model.fit(X, y)  # Train model\n    y_explotacion = model.predict_proba(X_to_predict)[:, 1]\n    \n    prediction = pd.DataFrame({\"Customer ID\": data_to_predict[\"Customer ID\"].values,\n                               \"Probability\": y_explotacion})\n    prediction.sort_values(by=\"Probability\", ascending=False, inplace=True)\n    \n    if csv_name:\n        prediction.head(100)[\"Customer ID\"].to_csv(csv_name, index=False)\n    \n    return prediction\n\ndef get_error(model):\n    model.fit(X_train, y_train)\n    \n    predicted_y = model.predict(X_test)\n    difference = predicted_y - y_test\n    \n    hits = np.round((difference == 0).sum() \/ predicted_y.size * 100, 2)\n    false_positive = np.round((difference == 1).sum() \/ predicted_y.size * 100, 2)\n    false_negative = np.round((difference == -1).sum() \/ predicted_y.size * 100, 2)\n    \n    print(f\"Correct: {hits}%\")\n    print(f\"False positives: {false_positive}%\")\n    print(f\"False negatives: {false_negative}%\")","1b5b76b1":"grid_params = {\n    'n_neighbors': range(1,50,2),\n    'p': [1,2,10000],  # Choose p for Minkowski metric\n    'weights': ['uniform', 'distance']\n}\n\ngs = GridSearchCV(KNeighborsClassifier(), grid_params, verbose=False, n_jobs=-1)\n\ngs_results = gs.fit(X_train, y_train)\nprint(f\"Best parameters: {gs_results.best_params_} with score {gs_results.best_score_}\")\n","74b1f1ec":"model = KNeighborsClassifier(n_neighbors=gs_results.best_params_['n_neighbors'],\n                             p=gs_results.best_params_['p'],\n                             weights=gs_results.best_params_['weights'])\n\nget_error(model)\nget_best_ids(model, \"predictions_knn.csv\")","0c11cbb0":"grid_params = {\n    \"max_depth\": range(1,20),\n    \"criterion\": [\"gini\", \"entropy\"],\n    \"min_samples_split\": range(2,10,2),\n}\n\ngs = GridSearchCV(DecisionTreeClassifier(), grid_params, verbose=False, n_jobs=-1)\n\ngs_results = gs.fit(X_train, y_train)\nprint(f\"Best parameters: {gs_results.best_params_} with score {gs_results.best_score_}\")","8cd62bf8":"model = DecisionTreeClassifier(criterion=gs_results.best_params_['criterion'],\n                               max_depth=gs_results.best_params_['max_depth'],\n                               min_samples_split=gs_results.best_params_['min_samples_split'])\n\nget_error(model)\nget_best_ids(model, \"predictions_tree.csv\")","c2306089":"plt.figure(figsize=(40, 20))\nplot_tree(model, feature_names=[name[-10:] for name in features], label=\"root\", impurity=False)\nplt.show()\n","18715706":"n_layers = 4\nn_neurons = [[n] for n in (10, 20, 50, 75, 100, 150)] # Array with the posible amount of neurons per layer\nconfigurations = list(n_neurons)\nconfig_prev = list(n_neurons)\n\nfor current_layer in range(1, n_layers):\n    config_new = [config + neuron for config in config_prev for neuron in n_neurons]\n    configurations += config_new\n    config_prev = config_new","b8675b99":"grid_params = {\n    'hidden_layer_sizes': configurations,\n    'activation': ['logistic', 'tanh', 'relu'],\n    'max_iter': (5000,),\n    'solver': ('lbfgs', 'adam')\n}\n\ngs = GridSearchCV(MLPClassifier(), grid_params, verbose=True, n_jobs=-1)\n\n# Uncomment these lines to get best parameters for NN\n# gs_results = gs.fit(X_train, y_train)\n# print(f\"Best parameters: {gs_results.best_params_} with score {gs_results.best_score_}\")","64351024":"# Harcoded best variables\nactivation = 'logistic'\nhidden_layer_sizes = (150, 50, 50)\nlearning_rate = 'constant'\nmax_iter = 5000\nsolver = 'adam'\n\nmodel = MLPClassifier(hidden_layer_sizes=hidden_layer_sizes,\n                      activation=activation,\n                      solver=solver,\n                      learning_rate=learning_rate,\n                      max_iter=max_iter)\n\nget_error(model)\nget_best_ids(model, \"predictions_neuron.csv\")","a4d8f675":"best_ids_knn = set(pd.read_csv(\"predictions_knn.csv\")[\"Customer ID\"].unique())\nbest_ids_tree = set(pd.read_csv(\"predictions_tree.csv\")[\"Customer ID\"].unique())\nbest_ids_neuron = set(pd.read_csv(\"predictions_neuron.csv\")[\"Customer ID\"].unique())\n\nplt.figure(figsize=(10, 10))\nvenn3([best_ids_knn, best_ids_tree, best_ids_neuron], set_labels=(\"KNN\", \"Tree\", \"MLP\"))\nplt.show()","b6036def":"sets = (best_ids_knn, best_ids_tree, best_ids_neuron)\n# Compute the union of the 3 sets\nunion = reduce(lambda x, y: x.union(y), sets)\n# Compute A \u0548 B, A \u0548 C and B \u0548 C\nintersections = [sets[i].intersection(sets[j]) for i in range(3) for j in range(i + 1, 3)]\n# Compute the union of the previous sets (all the elements will have been predicted by more than one model)\nfinal_prediction = reduce(lambda x, y: x.union(y), intersections)\n# Get those elements that have only been predicen by one model\nremaining = union - final_prediction\n# Fill the final prediction so it has 100 elements\nfinal_prediction.update(sample(remaining, k=100 - len(final_prediction)))\n# Write the prediction into a CSV file\ndf = pd.DataFrame({\"Customer ID\": list(final_prediction)})\ndf.to_csv(\"predictions_final.csv\", index=False)\ndf","d4ab5cda":"## Competitors\nAs the attributes related to competitors that customers are interested in are too many, we thought that perhaps this information was being given too much importance. Therefore, we are going to see which of these attributes give us a greater information gain.","03f28150":"As the end point of data preprocessing, we proceed to standardize the attribute values. On the other hand, we perform a random partition that contains 10% of the data to have a Test set with which to analyze the behavior of the model and verify that there is no *overfitting*.","3387245c":"## First model: KNN\n\nThe best value of the hyperparameters for the labeled data is computed:","d674fce1":"This project was developed as part of the Artificial Intelligence course in March 2020 at UAM by [Daniel Gallo](https:\/\/github.com\/daniel-gallo) and myself.\n\nWe were required to predict the 100 clients that were most likely to leave a telephone company, based on the provided data and using Machine Learning techniques. To do this task, we followed the following steps:\n\n1. Preprocessing:\n    \n    - Feature selection based on distribution plots, information gain, ...\n    \n    - Standardization\n2. Models: optimize the hyperparameters and train three main models (KNN, Decision Tree and MLP)\n3. Predict the 100 clients: this predictions are stored in their correspondent csv. In addition to this, for the final prediction, we decided to combine the predictions of the models.\n\nDetailed and step-by-step implementation can be found in the following lines","e71e6c4b":"Taking advantage of the fact that the tree is a visible and understandable model once it is trained, we print it.","6eca2423":"The best value of the hyperparameters for the labeled data is computed. As can be seen, this search was carried out on a very wide space of hyperparameters, to obtain an overview of which were the optimal values. The calculation time is exorbitant (around 4 hours), so it is not recommended to run this cell again.","1f50c754":"# Task and Data\n\nA phone company is interested in developing a model that predicts the **100 current customers** that are most likely to leave the company. For this, they provide us with a database **customer_leakage_labeled.csv** with labeled cases, which will be used to build our prediction model.\n\nThe fields in this database are:\n\n* **Customer ID**\n\n* **network_age:** customer tenure in days\n\n* **Customer tenure in months:** customer tenure in months\n\n* **Total Spend in Months 1 and 2:** total customer spending in months 1 and 2\n\n* **Total SMS Spend:** total SMS spend\n\n* **Total Data Spend:** total data \/ internet spend\n\n* **Total Data Consumption:** total data consumption (in KB) during the study period\n\n* **Total Unique Calls:** total number of unique calls\n\n* **Total Onnet spend:** total expenditure on calls to other users on the same telephone network\n\n* **Total Offnet spend:** total expenditure on calls to other users of different telephone networks\n\n* **Total Call center complaint calls:** number of complaint calls to the call center\n\n* **Network type subscription in Month 1:** Network type subscription in month 1. This indicates a customer's preferred network subscription, which may indicate their device type: 2G or 3G service\n\n* **Network type subscription in Month 2:** same as above but in the following month\n\n* **Churn Status:** the value is 1 if the customer leaves the telephone company, 0 if he stays there\n\n* **Most Loved Competitor network in Month 1:** which other competitor provider the customer prefers in month 1. It's actually a set of columns, each one focused on a particular provider\n\n* **Most Loved Competitor network in Month 2:** which other competing provider the customer prefers in month 2. It's actually a set of columns, each one focused on a particular provider\n\nThe variable to predict is **Churn Status**: the value is 1 if the customer **leaves** the company, 0 if he does not leave it.\n\nThe company also provides us with another database, **customer_leakage_to_predict.csv**, with information on clients for whom it is not known right now whether or not they will remain with the company. Therefore in this second database all the values of the column **Churn Status** are missing values (NaN).\n\nThe company asks us to provide the IDs of the 100 clients in the second database that are most likely to leave the company. To do this, we will provide the company with a csv file with a single column, **Customer ID**, and 100 rows containing the selected customer IDs.","33813ad1":"We see that it is a heave-tailed distribution with, that is, most customers call little to complain, but some of them call many times. To finish convincing ourselves that there was no relationship, we printed a list of the customers who made the most complaint calls along with the *Churn Status*.","2dd569b2":"# Matches between the three models","e64bcfcf":"# Conclusions\nIn view of the scores obtained, it is not clear which model should be used. It seems good idea to choose the elements predicted by more than one model. The following code takes all the elements that belong to more than one prediction, and fills with random elements that have only been predicen by one model until reaching 100. This list would be the one that we would deliver to the company. In addition, we would warn them that the predictor, when it fails, it is usually due to false positives, that is, it predicts that a customer will leave when he really will not.","dcb62b19":"As we have discussed, this search was very expensive. That is why in the next cell we store the values resulting from the analysis. It is too precious to risk losing it and having to run the search again.\n\nThe 100 clients most likely to leave are obtained and written to the csv:","77eee595":"We confirm that the first column (*network_age*) is always 29, 30 or 31 times the second (*Customer tenure in month*). Therefore, we can eliminate the second without losing any information.","b3a4bea8":"## Second model: Decision Tree\n\nThe best value of the hyperparameters for the labeled data is computed:","db662884":"## Total Call Centre Complaint Calls\nAlthough it seems completely counterintuitive, looking at the histogram we realize that there does not seem to be a correlation between the number of complaint calls and the *Churn Status*. Thinking that there may be an error with this, we did some simple checks:","73fce954":"# Load files","e671705e":"Numerically, what we already suspected by looking at the histograms is confirmed, *Most Loved Competitor network in Month 2_Uxaa* gives us more information than others. For this reason, we are going to only keep attributes that give us a gain greater than 0.01 bits.","7ba3f9b2":"The 100 clients most likely to leave are obtained and written to the csv:","1b89aca8":"## Standardization","669634fd":"We see that the histograms for *network_age* and *Customer tenure in month* are almost the same in shape, since one attribute is the customer seniority in days and the other in months. Let's confirm this with a quick calculation","a0855784":"# Auxiliary functions\nFirst, we define an auxiliary function that each of the models will use. This predicts the *Churn Status* of the \"to_predict\" data, orders the clients from highest to lowest probability of leaving and, optionally, writes the results in a CSV in the required format (that is, only the first 100 and only the column Customer ID). Secondly, we define a function that calculates the error of the model trained with the test partition, indicating the percentage of correct answers, false positives and false negatives.","b2f8bc8b":"# Models\n\nFrom now on, we are going to obtain the 100 clients who are most likely to leave the company based on the following scheme:\n\nWe use three different models. For each of them, we optimize their corresponding hyperparameters using `GridSearchCV` and the Train partition as training data.\nThen we load the model with the optimal hyperparameters. With this, we verify that *overfitting* has not occurred by checking the score resulting from applying the trained model to the Test partition. In general, this score should be similar to the one obtained with Train to certify that there is no *overfitting*. After this, we call `get_best_ids` which returns the customers ordered, from highest to lowest, based on the probability of ChurnSatus = 1. In addition, this function writes the first 100 customers with the desired format in the indicated CSV.","961e10b7":"# Context","a55ab1a1":"# Preprocessing\nWe preprocess the data, both \"labeled\" and \"to_predict\" data. Let's first remove *Churn Status*, as it is not an attribute, and *Customer ID*, as it does not help us to model the problem. After that, the distribution of each feature is plotted, so we get a graphical approach on which columns should be more useful","1177bf84":"## Customer tenure","eb1ba74c":"Although with the data at hand there does not seem to be a correlation between this attribute and the *Churn Status*, as it is only an attribute (not like the competitors, which will be commented next) we are not going to eliminate it. The models themselves will not take the attribute into account if it does not provide additional information.","f5a23399":"## Third model: Neural network\n\nFirst, we get a list of lists with the possible configurations for the specified number of layers.\nEach element will be a list of length = current_layer with the number of neurons for each layer as elements.\nFor this, all the configurations corresponding to current_layer are stored in config_new. They are obtained from concatenating the configurations of the previous current_layer with each possible number of neurons.","8e35689c":"The 100 clients most likely to leave are obtained and written to the csv:"}}