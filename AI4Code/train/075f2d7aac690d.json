{"cell_type":{"3e5c4372":"code","a25fd360":"code","5c7f3615":"code","f1ce29c3":"code","69289913":"code","620a644a":"code","c0dd1efe":"code","faca1e13":"code","84d38210":"code","25654c27":"code","35698454":"code","06112003":"code","195781f1":"code","ff4c2b43":"code","1a7f2e06":"code","c4a5463e":"code","77d67834":"code","db2ca744":"code","3c41e157":"code","888eae21":"code","df796948":"code","d16fb05d":"code","e90d4c45":"code","2e278ac5":"code","b59f72ed":"code","93a62b3d":"code","136efddf":"code","f242623b":"code","1c1a4a3b":"code","aec70177":"code","e767acfb":"code","db0ce591":"code","5f730e60":"code","6885e414":"code","d2bdace9":"code","3998ddd9":"code","24ad0c02":"code","b04b3e47":"code","41c1bcf5":"code","2a0afe80":"code","69016ef7":"code","35019e7a":"code","c742412e":"code","a6ce1991":"code","57b64475":"code","c3505908":"code","bcd6d7aa":"code","664a4506":"code","5ef9a0e5":"code","adf3c094":"code","b41df85c":"code","f5ed8428":"code","acc521a5":"code","4c1b3471":"code","72b6aa33":"code","174d4526":"markdown","04451169":"markdown","58d6fc69":"markdown","638c5010":"markdown","0ccba026":"markdown","9e2450e7":"markdown","c8237e48":"markdown","5f14e98f":"markdown","3a6650e2":"markdown","fc1ce121":"markdown","852d5177":"markdown","09ee3d1a":"markdown","0f212d46":"markdown","28c93cf9":"markdown","9e738f6b":"markdown","7058b3da":"markdown","58c2948b":"markdown","d91c2011":"markdown","c555eda8":"markdown","19197a81":"markdown","5db42363":"markdown","e76fa580":"markdown","ea110e7a":"markdown","cff7fb53":"markdown","794c05db":"markdown","82c341d7":"markdown","ae16df13":"markdown","7dee3300":"markdown","ec52d958":"markdown","e9cf5972":"markdown","de8a81cf":"markdown","5c9c7ad8":"markdown","0152cbbd":"markdown","21fe214d":"markdown","1a5f1708":"markdown","dc68e6be":"markdown","6f0cf18f":"markdown","47aa371d":"markdown","1230f348":"markdown"},"source":{"3e5c4372":"# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport pandas as pd\nimport numpy as np\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a25fd360":"from bokeh.io import output_notebook\nfrom bokeh.io import show\nfrom bokeh.plotting import figure\nfrom bokeh.transform import cumsum\nfrom bokeh.palettes import Spectral6\nfrom bokeh.models import ColumnDataSource\nfrom bokeh.layouts import gridplot\nfrom bokeh.io import curdoc\nfrom bokeh.models import HBar, Plot\n# curdoc().theme = 'dark_minimal'\ncurdoc().theme = 'caliber'\n\noutput_notebook()","5c7f3615":"web_time = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/WebSearch_InterestOverTime.csv')\nweb_time.head()","f1ce29c3":"web_time.columns = web_time.iloc[0]\nweb_time = web_time.reset_index()\nweb_time.rename(columns = {'index': 'Month', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (Worldwide)': 'Total'}, inplace=True)\nweb_time.drop(0, axis = 0, inplace = True)\nweb_time.head()","69289913":"web_time['Total'] = pd.to_numeric(web_time['Total'])\nweb_time['Year'] = pd.DatetimeIndex(web_time['Month']).year\nweb_time.head() ","620a644a":"def trend(df):\n    \"\"\"\n    A function that will plot the trend for the given years\n    \"\"\"\n    \n    cols = [int(i) for i in df['Year']]\n    vals = df['Total'].to_list()\n    source = ColumnDataSource(data = dict(cols = cols, counts = vals))\n    p = figure(plot_height = 400, plot_width = 800, \n               x_axis_label = 'Years', y_axis_label = 'Count')\n    \n    # add a line renderer with legend and line thickness\n    p.line('cols', 'counts', line_width = 2, source = source, line_dash = \"dashed\")\n    p.title.align = 'center'\n    p.title.text_font_size = '20pt'\n    p.title.text_font_style = 'bold'\n    p.title.text_font = 'Serif'\n    p.xaxis.axis_label_text_font_size = \"16pt\"\n    p.yaxis.axis_label_text_font_size = \"16pt\"\n\n    # show the results\n    show(p)","c0dd1efe":"trend(web_time)","faca1e13":"web_region = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/WebSearch_InterestByRegion.csv')\nweb_region.head()","84d38210":"web_region.columns = web_region.iloc[0]\nweb_region = web_region.reset_index()\nweb_region.rename(columns = {'index': 'Country', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (1\/1\/10 - 4\/18\/21)': 'Total'}, inplace=True)\nweb_region.drop(0, axis = 0, inplace = True)\nweb_region.head()","25654c27":"web_region[\"Total\"] = pd.to_numeric(web_region[\"Total\"]) # Chaning the data type from string to integer.","35698454":"import folium\nimport json\nworld_geojson = json.load(open('\/kaggle\/input\/world1\/world-countries.json'))\n\n## The json file is neccessary for the choropleth, as it contains coordinate information about each country.","06112003":"def mapping(df, world_geojson, indices_to_drop = False):\n    \"\"\"\n    A function that enables you to view the cases distribution on a world map\n    \"\"\"\n    \n    df1 = df[['Country', 'Total']]\n    df1['Total'] = df1['Total'].fillna(0) # Replacing null values with 0\n    if indices_to_drop:\n        df1 = df1.drop(indices_to_drop, axis = 0) # Removing them because they had values <1 which causes problems while mapping.\n    df1[\"Total\"] = pd.to_numeric(df1[\"Total\"])\n    \n    m = folium.Map(tiles = \"cartodbpositron\")\n    \n    folium.Choropleth(\n        geo_data = world_geojson,\n        data = df1,\n        columns = ['Country', 'Total'],\n        key_on = 'feature.properties.name',\n        fill_color=\"YlOrRd\",\n        fill_opacity = 0.7, \n        line_opacity = 0.2,\n    ).add_to(m)\n\n    m\n    \n    return m","195781f1":"mapping(web_region, world_geojson)","ff4c2b43":"web_topics = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/WebSearch_RelatedTopics.csv', skiprows = 4, names = ['Topic', 'Number of Searches'])","1a7f2e06":"web_topics.head()","c4a5463e":"web_topics[web_topics['Number of Searches'] == 'Breakout']['Topic'].unique()","77d67834":"df = web_topics[web_topics['Number of Searches'] != 'Breakout']\ndf['Number of Searches'] = pd.to_numeric(df['Number of Searches'])\ndf_sort = df.sort_values(by = 'Number of Searches', ascending = False).head(10)\ndf_sort.head()","db2ca744":"topics = df_sort['Topic'].to_list()[:10]\ncounts = df_sort['Number of Searches'].to_list()[:10]\nsource = ColumnDataSource(data = dict(topics = topics, counts = counts, color = ['teal'] * 10))\n\np = figure(x_range = topics, plot_height = 400, plot_width = 800, title = \"Most Popular Topics\", tools = \"hover\", tooltips = \"@topics: @counts\", background_fill_color=\"#f4f0ec\")\np.vbar(x = 'topics', top = 'counts', width = 0.9, source = source, color = 'color')\n\np.xgrid.grid_line_color = None\np.legend.orientation = \"horizontal\"\np.legend.location = \"top_right\"\n\np.title.align = 'center'\np.title.text_font_size = '20pt'\np.title.text_font_style = 'bold'\np.title.text_font = 'Serif'\np.xaxis.axis_label_text_font_size = \"16pt\"\np.yaxis.axis_label_text_font_size = \"16pt\"\n    \nshow(p)","3c41e157":"web_queries = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/WebSearch_RelatedQueries.csv', skiprows = 4, names = ['Query', 'Number of Searches'])\nweb_queries.head()","888eae21":"web_queries[web_queries['Number of Searches'] == 'Breakout']['Query'].unique()","df796948":"def get_top_n_words(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' unigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus) ## Shape: (2045, 46774) -> There are 2045 sentences and 46774 words\n    sum_words = bag_of_words.sum(axis=0) ## Shape: (1, 46774) -> Count of occurance of each word\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()] ## vec.vocabulary_.items returns the dictionary with (word, index)\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse = True)\n    return freq_sorted[:n]\n\ndef get_top_n_bigram(corpus, n = None):\n    \"\"\"\n    A function that returns the top 'n' bigrams used in the corpus\n    \"\"\"\n    vec = CountVectorizer(ngram_range = (2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis = 0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    freq_sorted = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return freq_sorted[:n]","d16fb05d":"def unigram_plot(unigram, color):\n    \"\"\"\n    A function used to plot bar charts for top unigrams\n    \"\"\"\n    words = [i[0] for i in unigram]\n    count = [i[1] for i in unigram]\n    source = ColumnDataSource(data = dict(Word = words, counts = count, color = [color] * 10))\n\n    p = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Unigram\", tools = \"hover\", tooltips = \"@Word: @counts\", background_fill_color=\"#f4f0ec\")\n    p.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n    p.title.align = 'center'\n    p.xaxis.major_label_orientation = \"horizontal\"\n\n    p.xgrid.grid_line_color = None\n    p.legend.orientation = \"horizontal\"\n    p.legend.location = \"top_right\"\n\n    p.title.align = 'center'\n    p.title.text_font_size = '20pt'\n    p.title.text_font_style = 'bold'\n    p.title.text_font = 'Serif'\n    p.xaxis.axis_label_text_font_size = \"16pt\"\n    p.yaxis.axis_label_text_font_size = \"16pt\"\n    \n    return p","e90d4c45":"def bigram_plot(bigram, color, color_length):\n    \"\"\"\n    A function used to plot bar charts for top bigrams\n    \"\"\"\n    words = [i[0] for i in bigram]\n    count = [i[1] for i in bigram]\n    source = ColumnDataSource(data = dict(Word = words, counts = count, color = [color] * color_length))\n\n    p = figure(x_range = words, plot_height = 400, plot_width = 800, title = \"Top Bigrams\", tools = \"hover\", tooltips = \"@Word: @counts\", background_fill_color=\"#f4f0ec\")\n    p.vbar(x = 'Word', top = 'counts', width = 0.8, source = source, color = 'color')\n    p.xgrid.grid_line_color = None\n    p.xaxis.major_label_orientation = \"vertical\"\n    p.legend.orientation = \"horizontal\"\n    p.legend.location = \"top_right\"\n\n    p.title.align = 'center'\n    p.title.text_font_size = '20pt'\n    p.title.text_font_style = 'bold'\n    p.title.text_font = 'Serif'\n    p.xaxis.axis_label_text_font_size = \"16pt\"\n    p.yaxis.axis_label_text_font_size = \"16pt\"\n\n    return p","2e278ac5":"top_unigram = get_top_n_words(web_queries['Query'], 10)\nshow(unigram_plot(top_unigram, '#6baed6'))","b59f72ed":"top_bigram = get_top_n_bigram(web_queries['Query'], 10)\nshow(bigram_plot(top_bigram, '#a1dab4', 10))","93a62b3d":"youtube_time = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/YoutubeSearch_InterestOverTime.csv')\nyoutube_time.head()","136efddf":"youtube_time.columns = youtube_time.iloc[0]\nyoutube_time = youtube_time.reset_index()\nyoutube_time.rename(columns = {'index': 'Month', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (Worldwide)': 'Total'}, inplace=True)\nyoutube_time.drop(0, axis = 0, inplace = True)\nyoutube_time.head()","f242623b":"youtube_time['Total'] = pd.to_numeric(youtube_time['Total'])\nyoutube_time['Year'] = pd.DatetimeIndex(youtube_time['Month']).year\nyoutube_time.head() ","1c1a4a3b":"trend(youtube_time)","aec70177":"youtube_topics = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/YoutubeSearchRelatedTopics.csv', skiprows = 4, names = ['Topic', 'Number of Searches'])\nyoutube_topics.head()","e767acfb":"youtube_topics[youtube_topics['Number of Searches'] == 'Breakout']['Topic'].unique()","db0ce591":"df = youtube_topics[youtube_topics['Number of Searches'] != 'Breakout']\ndf['Number of Searches'] = pd.to_numeric(df['Number of Searches'])\ndf_sort = df.sort_values(by = 'Number of Searches', ascending = False).head(10)\ndf_sort.head()","5f730e60":"topics = df_sort['Topic'].to_list()[:10]\ncounts = df_sort['Number of Searches'].to_list()[:10]\nsource = ColumnDataSource(data = dict(topics = topics, counts = counts, color = ['lightseagreen'] * 10))\n\np = figure(x_range = topics, plot_height = 400, plot_width = 800, title = \"Most Popular Topics\", tools = \"hover\", tooltips = \"@topics: @counts\", background_fill_color=\"#f4f0ec\")\np.vbar(x = 'topics', top = 'counts', width = 0.9, source = source, color = 'color')\n\np.xgrid.grid_line_color = None\np.legend.orientation = \"horizontal\"\np.legend.location = \"top_right\"\np.xaxis.major_label_orientation = \"vertical\"\np.title.align = 'center'\np.title.text_font_size = '20pt'\np.title.text_font_style = 'bold'\np.title.text_font = 'Serif'\np.xaxis.axis_label_text_font_size = \"16pt\"\np.yaxis.axis_label_text_font_size = \"16pt\"\n    \nshow(p)","6885e414":"youtube_queries = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/YoutubeSearchRelatedQueries.csv', skiprows = 4, names = ['Query', 'Number of Searches'])\nyoutube_queries.head()","d2bdace9":"youtube_queries[youtube_queries['Number of Searches'] == 'Breakout']['Query'].to_list()","3998ddd9":"top_unigram = get_top_n_words(youtube_queries['Query'], 10)\nshow(unigram_plot(top_unigram, 'salmon'))","24ad0c02":"top_bigram = get_top_n_bigram(youtube_queries['Query'], 10)\nshow(bigram_plot(top_bigram, 'mediumaquamarine', 10))","b04b3e47":"youtube_region = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/YoutubeSearch_InterestByRegion.csv')\nyoutube_region.head()","41c1bcf5":"youtube_region.columns = youtube_region.iloc[0]\nyoutube_region = youtube_region.reset_index()\nyoutube_region.rename(columns = {'index': 'Country', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (1\/1\/10 - 4\/18\/21)': 'Total'}, inplace=True)\nyoutube_region.drop(0, axis = 0, inplace = True)\nyoutube_region.head()","2a0afe80":"mapping(youtube_region, world_geojson, [142, 143, 144])","69016ef7":"news_time = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/NewsSearchInterestOverTime.csv')\nnews_time.head()","35019e7a":"news_time.columns = news_time.iloc[0]\nnews_time = news_time.reset_index()\nnews_time.rename(columns = {'index': 'Month', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (Worldwide)': 'Total'}, inplace=True)\nnews_time.drop(0, axis = 0, inplace = True)\nnews_time.head()","c742412e":"news_time['Total'] = pd.to_numeric(news_time['Total'])\nnews_time['Year'] = pd.DatetimeIndex(news_time['Month']).year\nnews_time.head() ","a6ce1991":"trend(news_time)","57b64475":"news_topics = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/NewsSearchRelatedTopics.csv', skiprows = 4, names = ['Topic', 'Number of Searches'])\nnews_topics.head()","c3505908":"news_topics[news_topics['Number of Searches'] == 'Breakout']['Topic'].unique()","bcd6d7aa":"df = news_topics[news_topics['Number of Searches'] != 'Breakout']\ndf['Number of Searches'] = pd.to_numeric(df['Number of Searches'])\ndf_sort = df.sort_values(by = 'Number of Searches', ascending = False).head(10)\ndf_sort.head()","664a4506":"topics = df_sort['Topic'].to_list()[:10]\ncounts = df_sort['Number of Searches'].to_list()[:10]\nsource = ColumnDataSource(data = dict(topics = topics, counts = counts, color = ['lightseagreen'] * 10))\n\np = figure(x_range = topics, plot_height = 400, plot_width = 800, title = \"Most Popular Topics\", tools = \"hover\", tooltips = \"@topics: @counts\", background_fill_color=\"#f4f0ec\")\np.vbar(x = 'topics', top = 'counts', width = 0.9, source = source, color = 'color')\n\np.xgrid.grid_line_color = None\np.legend.orientation = \"horizontal\"\np.legend.location = \"top_right\"\np.xaxis.major_label_orientation = \"vertical\"\np.title.align = 'center'\np.title.text_font_size = '20pt'\np.title.text_font_style = 'bold'\np.title.text_font = 'Serif'\np.xaxis.axis_label_text_font_size = \"16pt\"\np.yaxis.axis_label_text_font_size = \"16pt\"\n    \nshow(p)","5ef9a0e5":"web_queries = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/NewsSearchRelatedQueries.csv', skiprows = 4, names = ['Query', 'Number of Searches'])\nweb_queries.head()","adf3c094":"web_queries[web_queries['Number of Searches'] == 'Breakout']['Query'].to_list()","b41df85c":"top_unigram = get_top_n_words(web_queries['Query'], 10)\nshow(unigram_plot(top_unigram, 'salmon'))","f5ed8428":"top_bigram = get_top_n_bigram(web_queries['Query'], 10)\nshow(bigram_plot(top_bigram, 'mediumaquamarine', 7))","acc521a5":"news_region = pd.read_csv('\/kaggle\/input\/data-science-global-trends-2010-2021\/NewsSearchInterestByRegion.csv')\nnews_region.head()","4c1b3471":"news_region.columns = news_region.iloc[0]\nnews_region = news_region.reset_index()\nnews_region.rename(columns = {'index': 'Country', 'Data science + Machine Learning + Computer Vision + Natural Language Processing + Deep Learning: (1\/1\/10 - 4\/18\/21)': 'Total'}, inplace=True)\nnews_region.drop(0, axis = 0, inplace = True)\nnews_region.head()","72b6aa33":"mapping(news_region, world_geojson, [89])","174d4526":"**From the above graph, we can observe that there has been a constant increase in the number of searches over a while. We have often heard 'Data Science' has been the buzzword for a long period, the above graph certainly proves this.**","04451169":"**Woah, I see Andrew Ng's name there :). He is truly the best in this field. Next, we can also see the Massachusetts Institute of Technology, it might be due to its great ML course that is available on Youtube. We can also see words like Neuron, ANN, etc. Oh God, these words are taking me back to my initial days!**","58d6fc69":"Before we move ahead and have a look at the three levels, we will always be having a look at the following parameters:\n\n* Time -> The number of searches spread across the given duration\n\n* Region -> Number of Searches made in different regions\n\n* Topic -> What were the different topics searched by the user.\n\n* Queries -> What different queries did people have?","638c5010":"**Again there are some breakout queries, let's have a look at them!**","0ccba026":"![ds.jpeg](attachment:d68f9155-d961-4547-87cc-caae80d6bf14.jpeg)","9e2450e7":"# News Search","c8237e48":"\n**Data Science, a word that you must have come across many times, refers to deriving meaning out of data by combining statistical and mathematical concepts. It's a buzzword that is going around and everyone tends to try it out at least once when they are starting with their development journey.**\n\n**In this notebook, we have a look at the global data science trends like the most popular words related to data science that are searched, locations where they were searched, what were the most common queries? etc based on three different categories, i.e News Search, YouTube Search, and Web Search.**\n\n**I have used Bokeh to make the bar plots and line charts etc and Folium has been used for plotting out data on the map to get a clearer picture. Throughout this notebook, you will have many instances that will take you back to your initial days in the data science journey.**\n\n**Do Upvote the notebook if you liked it!**","5f14e98f":"## Region","3a6650e2":"# Importing the Libraries","fc1ce121":"## Queries","852d5177":"## Time","09ee3d1a":"## Topics","0f212d46":"## Time","28c93cf9":"**We will have to change the column name to the one present in the first row**","9e738f6b":"**China and India are the countries where the web searches are made the most!**","7058b3da":"**This trend is a little bit different, eventhough there was a constant rise till 2015 but after that there has been a constant dip and rise.**","58c2948b":"**From the above graph, we can observe that words like 'Machine Learning', 'Data', 'Data Science' were searched most often. These are certain words that a lot of people tend to search the most number of times. Imagine you as a beginner and starting to learn AI, first I will go and read about 'Machine Learning', while reading Machine Learning, I will come across 'Deep Learning' and then I will search for that. Then I may across 'Python' as a programming language for doing ML\/AI, then I will search and read about Python. So all these searches make a lot of sense.**","d91c2011":"**These results were quite expected, here we can see that words like 'Machine Learning', 'Data Science', and 'Andrew Ng' are searched the most on Youtube. I would recommend the Andrew Ng course for anyone who is looking to get started with AIML.**","c555eda8":"# Using Data to Analyze Data Science","19197a81":"**From the above graph, we can observe that there has been a constant increase in the number of searches on Youtube from 2010 to 2016 then it dropped for one month, then again it started rising till 2020.**","5db42363":"## Region","e76fa580":"## Time","ea110e7a":"**These are again very similar queries, I can see many queries that I have also searched a lot of times :), like Python for ML, kaggle, r for ML, Azure ML, TensorFlow, etc. I am sure you also have searched queries a lot of times.**","cff7fb53":"# Web Search","794c05db":"## Queries","82c341d7":"**After going through the above visualizations, one thing is very clear that many people are interested in Data Science, which is reflected in the search topics on the Web as well as Youtube. People are constantly looking out for resources to get started with AI\/ML or to enhance their skills. We also got to know the various top instructors or institutions on YouTube for learning AI\/ML.**\n\n**Data Science is here to stay so do not hesitate to get started and embark on this beautiful journey of learning!**\n\n**Any Feedback would be highly appreciated.**\n\n**Do upvote if you liked the notebook!**","ae16df13":"There is a unique kind of value for 'Number of Searches' column, breakout means extremely popular, therefore let's see the topics that broke out.","7dee3300":"Let's look at the words that have some count associated with it.","ec52d958":"## Topics","e9cf5972":"* Most of the people have queried about Machine Learning tutorials, data science tutorials, and machine learning in python.\n\n* People also queried about machine learning projects which I have also searched quite often.\n\n* It's nice that terms like Statistics are being queried to get to know the core concepts.\n\n**It would be fun to analyze the top channels that people follow for learning such topics.**","de8a81cf":"**It's pretty amazing how we can easily differentiate between Youtube Search, News Search, and Web Search by just looking at the topics. For news search, we have sort of more application-based stuff. Like IoT, Cloud Computing, or basically the things in ML and AI can be used.**","5c9c7ad8":"After looking at the above terms, a smile came to my face. It's because these are the words that we as data scientists must have searched at some point of our career, I mean I still do. For example:\n\n* If I want to read about Big Data I would search 'Big Data' a lot of times on Google.\n\n* If I want to learn AWS or read its docs, I would search Amazon Web Services.\n\n* If I want to learn about something new I would go to Coursera or Udemy. \n\nTherefore, these above terms are pretty popular and are regularly searched.","0152cbbd":"## Region","21fe214d":"# Youtube Search","1a5f1708":"# Conclusion","dc68e6be":"## Bokeh Setup","6f0cf18f":"**We will be using Folium for dealing with maps. Folium makes it easy to visualize data that\u2019s been manipulated in Python on an interactive leaflet map. It enables both the binding of data to a map for choropleth visualizations as well as passing rich vector\/raster\/HTML visualizations as markers on the map.**","47aa371d":"## Queries","1230f348":"## Topics"}}