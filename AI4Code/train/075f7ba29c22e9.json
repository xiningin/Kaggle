{"cell_type":{"e7eb52b9":"code","65405433":"code","abba6b40":"code","189c7b30":"code","0295cc95":"code","6e7352b9":"code","984280fe":"code","056e2f3a":"code","01c60258":"code","a3cadbf5":"code","67d3cb73":"code","c2030d4e":"code","075d71ab":"code","ace72ad3":"code","c41ca7c1":"code","c42cae1e":"code","af11ff16":"code","18bd2c8c":"code","ad56a6a9":"code","2dcf6a6f":"code","9ac8e736":"code","3b87f9a8":"code","0a9ff840":"code","6d2cb032":"code","988fed25":"code","77fa0519":"code","7cef9604":"code","cbc6f5f1":"code","b6d152d4":"code","c3f63111":"code","0979e36b":"code","e82c2475":"code","665af043":"code","51d7538b":"code","5a8fc130":"code","190fedd6":"code","7f6e6f79":"code","cad64bbf":"code","01d8f27f":"code","dd902cea":"code","33f4e594":"code","49c0ca28":"code","41da7329":"code","7e5ea1ca":"code","73cb6bc8":"code","012ff2d4":"code","c3f46a56":"code","3f3004c2":"code","15b472a6":"code","af36a385":"code","f3f86bec":"code","b11ede74":"code","575b8b96":"code","544dcbee":"code","576a7ae5":"code","a2457cfb":"code","b2ef894a":"code","c443357a":"code","8d746dea":"code","d879993b":"code","8cfb7d50":"code","2c20781e":"code","a500be3f":"markdown","c9e31dc8":"markdown","8c390399":"markdown","8c4bfcfa":"markdown","37390b02":"markdown","a895eae0":"markdown","54153b26":"markdown","a8e296bb":"markdown","25ff0064":"markdown","0bfadf29":"markdown","334df727":"markdown","564f8189":"markdown","7e1ebfc2":"markdown","aa54770f":"markdown","acaec943":"markdown","8afbf1fb":"markdown","cdfaf2fe":"markdown","466e5498":"markdown","749dc5ed":"markdown","24628cf5":"markdown","280aa22d":"markdown","04405ac5":"markdown","0815fc0c":"markdown","bfd0da92":"markdown","dff38209":"markdown","749a1a85":"markdown","e7c8a0c7":"markdown","56e16eab":"markdown","e067427a":"markdown","176d0022":"markdown"},"source":{"e7eb52b9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport seaborn as sns #for visualisation\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","65405433":"import warnings\nwarnings.filterwarnings('ignore')","abba6b40":"from subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","189c7b30":"df=pd.read_csv(\"..\/input\/telecom-churn\/Churn.csv\")","0295cc95":"df.head()","6e7352b9":"print (\"\\nFeatures : \\n\" ,df.columns.tolist())  #listing all the features","984280fe":"df.describe() ","056e2f3a":"Null_val = [(c, df[c].isna().mean()*100) for c in df]\nNull_val = pd.DataFrame(Null_val, columns=[\"column_name\", \"percentage\"])","01c60258":"Null_val","a3cadbf5":"df.info()   ","67d3cb73":"df.TotalCharges = pd.to_numeric(df.TotalCharges, errors='coerce')   #changing to numeric","c2030d4e":"print (\"\\nUnique values for each column:\\n\",df.nunique())","075d71ab":"df.drop([\"customerID\"],axis=1,inplace = True) #dropping CustomerID column because it has got nothing to do with analysis of Churn","ace72ad3":"df.head()","c41ca7c1":"df.gender = [1 if each == \"Male\" else 0 for each in df.gender] #mapping male to 1 and female to 0","c42cae1e":"df.head()","af11ff16":"df.gender[df.gender == 'male'] = 1\ndf.gender[df.gender == 'female'] = 0\n\n#mapping male to 1 and female to 0","18bd2c8c":"change_to_num = ['Partner', \n                      'Dependents', \n                      'PhoneService', \n                      'MultipleLines',\n                      'OnlineSecurity',\n                      'OnlineBackup',\n                      'DeviceProtection',\n                      'TechSupport',\n                      'StreamingTV',\n                      'StreamingMovies',\n                      'PaperlessBilling', \n                      'Churn']\n\nfor x in change_to_num:\n    df[x] = [1 if each == \"Yes\" else 0 if each == \"No\" else -1 for each in df[x]]\n    \ndf.head()","ad56a6a9":"\nsns.countplot(x=\"Churn\",data=df) #Visualising the distribution of Churn values\n","2dcf6a6f":"sns.pairplot(df,vars = ['tenure','MonthlyCharges','TotalCharges'], hue=\"Churn\") \n#plotting the three numeric features with hue as \"Churn\" \n#Hue is a Variable in data to map plot aspects to different colors.\n","9ac8e736":"v=sns.catplot(x=\"Contract\", y=\"Churn\", data=df,kind=\"bar\")\nv.set_ylabels(\"Probability of Churn to be 1\")\n# All types of contract vs Churning probability","3b87f9a8":"u=sns.catplot(x=\"InternetService\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Probability of churn to be 1\")\n#All types of IS vs CHurn probability","0a9ff840":"u=sns.catplot(x=\"TechSupport\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Probability of churn\")","6d2cb032":"u=sns.catplot(x=\"gender\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Probabilty for churn to be 1\")","988fed25":"u=sns.catplot(x=\"SeniorCitizen\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Churn Probability\")","77fa0519":"u=sns.catplot(x=\"OnlineSecurity\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Churn probability\")","7cef9604":"u=sns.catplot(x=\"DeviceProtection\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Churning Probability\")","cbc6f5f1":"u=sns.catplot(x=\"PaperlessBilling\", y=\"Churn\", data=df,kind=\"bar\")\nu.set_ylabels(\"Churn Probability\")","b6d152d4":"#Now we will map the remaining columns (InternetService, Contract, PaymentMethod)","c3f63111":"df = pd.get_dummies(data=df)\ndf.head()\n","0979e36b":"p=df.corr() #Finding the correlation between the columns so that I can remove one of two highly correlated column \n#Usually the values ranging from +\/-0.5 to +\/-1 are said to be highly correlated, so we will look for it","e82c2475":"p","665af043":"p['Churn'].sort_values() ","51d7538b":"#No value is highly correlated, so we are good to go","5a8fc130":"df = df.reset_index()","190fedd6":"y=df.Churn.values #storing Churn(which is to be predicted) in variable Y","7f6e6f79":"df1=df.drop([\"Churn\"],axis=1) #dropping Churn column, so that we can be left with rest of the features","cad64bbf":"x = (df1-np.min(df1))\/(np.max(df1)-np.min(df1)).values","01d8f27f":"pd.isnull(x).sum() > 0  #finding the column where lies any null value","dd902cea":"x=x.fillna(x.mean()) #replacing the null value with mean","33f4e594":"np.any(np.isnan(x)) #checking whethetr a null value still exists in the dataframe","49c0ca28":"x = x[np.isfinite(x).all(1)]  #Only keeping finite values","41da7329":"np.all(np.isfinite(x)) #Checking whether all values are finite","7e5ea1ca":"print(x.astype(np.float32)) #to avoid any dtype error, finding the value that exceeds the bounds of a float 32 dtype","73cb6bc8":"X = np.nan_to_num(x.astype(np.float32)) #bringing value in the bound of float 32 dtype\nprint(X)","012ff2d4":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2,random_state =1) \n#I've split the data in the ratio 80:20","c3f46a56":"from sklearn.tree import DecisionTreeClassifier         #Decision Tree\ndt_model = DecisionTreeClassifier()\ndt_model.fit(x_train,y_train)\naccuracy_dt = dt_model.score(x_test,y_test)\nprint(\"Decision Tree's accuracy:\",accuracy_dt)\n","3f3004c2":"from sklearn.svm import SVC                             #SVM\nsvc_model = SVC(random_state = 1)\nsvc_model.fit(x_train,y_train)\naccuracy_svc = svc_model.score(x_test,y_test)\nprint(\"Accuracy using SVM :\",accuracy_svc)","15b472a6":"from sklearn.naive_bayes import GaussianNB              #Naive Bayes\nnb_model = GaussianNB()\nnb_model.fit(x_train,y_train)\naccuracy_nb = nb_model.score(x_test,y_test)\nprint(\"Accuracy using Naive Bayes :\",accuracy_nb)","af36a385":"from sklearn.linear_model import LogisticRegression    #Logistic Regression\nlr_model = LogisticRegression()\nlr_model.fit(x_train,y_train)\naccuracy_lr = lr_model.score(x_test,y_test)\nprint(\" Accuracy using Logistic Regression is:\",accuracy_lr)","f3f86bec":"from sklearn.neighbors import KNeighborsClassifier    #K-Nearest Neighbor\nknn = KNeighborsClassifier(n_neighbors = 3) #set K neighbor as 3\nknn.fit(x_train,y_train)\npredicted_y = knn.predict(x_test)\nprint(\"KNN accuracy when k=3:\",knn.score(x_test,y_test))","b11ede74":"arr1 = []\nfor each in range(1,25):\n    knn_loop = KNeighborsClassifier(n_neighbors = each) #set K neighbor as 3\n    knn_loop.fit(x_train,y_train)\n    arr1.append(knn_loop.score(x_test,y_test))\n    \nplt.plot(range(1,25),arr1)\nplt.xlabel(\"Range\")\nplt.ylabel(\"Score\")\nplt.show()\n","575b8b96":"#KNN gives highest accuracy at k=16","544dcbee":"knn_model = KNeighborsClassifier(n_neighbors = 16) #at k=16\nknn_model.fit(x_train,y_train)\npredicted_y = knn_model.predict(x_test)\naccuracy_knn = knn_model.score(x_test,y_test)\nprint(\"KNN accuracy when K=16:\",accuracy_knn)","576a7ae5":"from sklearn.ensemble import RandomForestClassifier     #Random Forest\nrf_model_initial = RandomForestClassifier(n_estimators = 5, random_state = 1)\nrf_model_initial.fit(x_train,y_train)\nprint(\"Random Forest accuracy for 7 trees is:\",rf_model_initial.score(x_test,y_test))\n","a2457cfb":"arr = []   #plotting a graph to find best value of K that would give us maximum accuracy\nfor each in range(1,50):\n    rf_loop = RandomForestClassifier(n_estimators = each, random_state = 1)\n    rf_loop.fit(x_train,y_train)\n    arr.append(rf_loop.score(x_test,y_test))\n    \nplt.plot(range(1,50),arr)\nplt.xlabel(\"Range\")\nplt.ylabel(\"Score\")\nplt.show()","b2ef894a":"#Accuracy of RF is highest at 35 and 42","c443357a":"rf_model = RandomForestClassifier(n_estimators = 35, random_state = 1) \nrf_model.fit(x_train,y_train)\naccuracy_rf = rf_model.score(x_test,y_test)\nprint(\"Random Forest accuracy for 35 trees is:\",accuracy_rf)\n","8d746dea":"from sklearn.metrics import recall_score, confusion_matrix, precision_score, f1_score, accuracy_score, classification_report\ncm_lr = confusion_matrix(y_test,lr_model.predict(x_test)) #for Logistic Regression\nf, ax = plt.subplots(figsize = (5,5))\nsns.heatmap(cm_lr, annot = True, linewidths = 0.5, color = \"blue\", fmt = \".0f\", ax=ax)\nplt.xlabel(\"y_predicted\")\nplt.ylabel(\"y_true\")\nplt.title(\"Confusion Matrix of LR\")\nplt.show()\n","d879993b":"def print_scores(headline, y_true, y_pred):\n    print(headline)\n    acc_score = accuracy_score(y_true, y_pred)\n    print(\"accuracy: \",acc_score)\n    pre_score = precision_score(y_true, y_pred)\n    print(\"precision: \",pre_score)\n    rec_score = recall_score(y_true, y_pred)                            \n    print(\"recall: \",rec_score)\n    f_score = f1_score(y_true, y_pred, average='weighted')\n    print(\"f1_score: \",f_score)\n","8cfb7d50":"print_scores(\"Logistic Regression;\",y_test, lr_model.predict(x_test))\nprint_scores(\"SVC;\",y_test, svc_model.predict(x_test))\nprint_scores(\"KNN;\",y_test, knn_model.predict(x_test))\nprint_scores(\"Naive Bayes;\",y_test, nb_model.predict(x_test))\nprint_scores(\"Decision Tree;\",y_test, dt_model.predict(x_test))\nprint_scores(\"Random Forest;\",y_test, rf_model.predict(x_test))","2c20781e":"report = classification_report(y_test, lr_model.predict(x_test))  #Report of best performing LR model\nprint(report)","a500be3f":"\nWe realise that Columns Partner,Dependents, PhoneService,MultipleLines, OnlineSecurity,OnlineBackup,DeviceProtection, Techsupport, StreamingTV, StreamingMOvies,PaperlessBilling and Churn have a same set of unique values(i,e., \"yes\" or \"no\" plus one other) so we can map them to 0\/1\/-1 together","c9e31dc8":"### Importing the data","8c390399":"OBSERVATIONS:\n1. Maximum probability of losing a customer is when they are using Fiber optic IS\n2. Lowest is when they're using none","8c4bfcfa":"In the output above, it is to be noted that for different columns there exists a set of unique values.\nIt is easy to interpret looking at customerID column that each value must be unique and hence there are 7043 values(same as the total number of rows) and for gender(here considering binary only) there exists 2 unique values; either male or female. And so on","37390b02":"since there are so many values, we will sort them to check ","a895eae0":"The steps below were performed to avoid an error during implementation of models\n\"Input contains NaN, infinity or a value too large for dtype('float32')\"\n","54153b26":"CHANGING NON NUMERIC COLUMNS TO NUMERIC","a8e296bb":"CHANGING NON NUMERIC COLUMNS TO NUMERIC\n","25ff0064":"##### Calculating the columnwise percentage of null values","0bfadf29":"Observation: ONline security is directly proportional to churn probability","334df727":"We observe that there are no NUll values in any column. So we will simply proceed","564f8189":"Observations from the above Plots:\n1. Tenure is indirectly proportional to Churn\n2. Monthly charges are directly proportional to churn","7e1ebfc2":"Observation: Less device protection may lead to more churning","aa54770f":"#### Conclusion: The above charts helped in analysing how these features affect Pobability of Churning.\nGender Feature had little impact as women were very less likely to churn\n","acaec943":"OBSERVATIONS:\n1. Customers are more likely to stop using service when the contract is month to month\n2. Least when a two year contract is made","8afbf1fb":"Observation: In case of no tech support, customers have high probability to churn","cdfaf2fe":"### Data Manipulation (Part 1)","466e5498":"Columns such as InternetService, contract and payement mode are useful in visualisation as mapped values of them to numerals would be confusing to see on graphs\nSo they'll be mapped after some visualisation","749dc5ed":"Observation: Younger people are likely to turn into Churn","24628cf5":"###                                    Understanding the data","280aa22d":"### Implementing Machine Learning Models","04405ac5":"Observation: Gender doesn't play a significant role in analysing whether a customer would churn","0815fc0c":"### Data Visualization","bfd0da92":"### Final Conclusions","dff38209":"### Model Evaluation","749a1a85":"### Data Manipulation (part 2)","e7c8a0c7":"We can see that most of the columns have dtype as \"Objects\". So they needed to be mapped with a numeric values.\n\nIt is to be observed that there are 21 columns and 7042 rows. TotalCharges contains numeric values but here the o\/p comes to be object, we need to change that too.\n","56e16eab":"Splitting into train and test dataframes","e067427a":"Tenure is inversely proportional to Churn\n\nMonthly charges is directly proportional to Churn\n\nThe RF model performed best when number of trees=35\/42\n\nThe KNN model performed the best when k=16\n\nLogistic Regression model gave highest accuracy\n\nDecision Tree Performed the worst\n\n\n\n\n\n##### Some steps to avoid Churn:\n1. Engage with customers\n2. Proving offers\n3. Analysis of risk\n4. Providing genuine and better service\n5. Customer retention is more important than customer acquisition.","176d0022":"The description didn't come out to be of much use because most of the columns are non numeric.\nHowever for tenure and monthly charges median value(50%) may be useful in case of missing values."}}