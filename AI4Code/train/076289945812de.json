{"cell_type":{"5dc3457f":"code","84cc62b9":"code","56cc963f":"code","214187d2":"code","faeec448":"code","98012482":"code","d9d796b5":"code","b35bcd90":"code","432b6877":"code","dba2c18e":"code","38b294cc":"code","17f92d74":"code","24b31f6f":"code","0704f8e8":"code","96f4ee4b":"code","6002f52b":"code","a34291cf":"code","24b03cb1":"code","441a779c":"code","2bcfdf08":"code","81f10cf7":"code","d5684425":"code","327b4169":"code","aefcfdb3":"code","01981afa":"code","6c713f49":"code","28a6280d":"code","d4096f7e":"code","2b6c423a":"code","89ed28a6":"code","937cdbde":"code","bac29fe1":"code","4ae77be7":"code","6e7414be":"code","4056eae4":"code","be461dfa":"code","89d5ddcd":"code","0965f2a1":"code","0396a704":"code","178a126c":"code","99155c3e":"code","7d46cd14":"code","d06e7a24":"code","1871c8eb":"code","ab60aad4":"code","808a1d3a":"code","3e141ad6":"code","4111a745":"code","371f1128":"code","b35f99cb":"code","dbc18a76":"code","45b5f8a4":"code","719dafc2":"code","504f17f5":"code","1bb2504c":"code","859e27d5":"code","1af6084d":"code","8e30cddb":"code","97e0871d":"code","54e382c6":"code","d3b8c363":"code","c5ae4239":"code","3ecf2ef6":"markdown","2c6c15a8":"markdown","098286b1":"markdown","2a6af6f2":"markdown","27cf731e":"markdown","62e4d9a5":"markdown","1dabbe6f":"markdown","0b1e2a8a":"markdown","3bc7555a":"markdown","0f307c9c":"markdown","896634b6":"markdown","12440dbf":"markdown","a2dd19d0":"markdown","e0954fee":"markdown","c58e0461":"markdown","cce35b4e":"markdown"},"source":{"5dc3457f":"import numpy as np\nimport pandas as pd \nimport matplotlib.pyplot as plt\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport seaborn as sns\nsns.set()","84cc62b9":"import sqlite3\n\nconn = sqlite3.connect('..\/input\/database.sqlite')\n\ndata = pd.read_sql_query(\"select * from Reviews where Score !=3 order by Time\", conn)\ndata.head()","56cc963f":"# removing duplicates\ndata = data.drop_duplicates(subset={'ProductId', 'UserId', 'Score', \n                            'Text'}, keep='first')","214187d2":"positiveData = {\n    \"Text\": [],\n    \"Sentiment\": [],\n    \"Time\": []\n}\n\nnegativeData = {\n    \"Text\": [],\n    \"Sentiment\": [],\n    \"Time\": []    \n}\n\nn_posRneg = 50000\nn_negative = 0\nn_positive = 0\n\nfor row in data.itertuples():\n    \n    if row[7] < 3:\n        if n_negative < n_posRneg:\n            positiveData['Text'].append(row[10])\n            positiveData['Sentiment'].append(-1)\n            positiveData['Time'].append(row[8])\n            n_negative += 1\n    else:\n        if n_positive < n_posRneg:\n            negativeData['Text'].append(row[10])\n            negativeData['Sentiment'].append(1)\n            negativeData['Time'].append(row[8])\n            n_positive += 1\n\npositiveData = pd.DataFrame(positiveData).sort_values(['Time'], axis=0)\nnegativeData = pd.DataFrame(negativeData).sort_values(['Time'], axis=0)","faeec448":"te = int(5000 * 7)\npositiveTrain = positiveData[0:te]\npositiveTest = positiveData[te:50000]\nnegativeTrain = negativeData[0:te]\nnegativeTest = negativeData[te:50000]","98012482":"trainData = pd.concat([positiveTrain, negativeTrain], axis=0)\ntrainData = trainData.sort_values(['Time'], axis=0)\nX_train = trainData['Text']\nY_train = trainData['Sentiment']\n\ntestData = pd.concat([positiveTest, negativeTest], axis=0)\ntestData = testData.sort_values(['Time'], axis=0)\nX_test = testData['Text']\nY_test = testData['Sentiment']","d9d796b5":"from nltk.corpus import stopwords\nfrom nltk.stem import SnowballStemmer\nimport re\n\nsnow = SnowballStemmer('english')\npre_processedTrain = []\n\ni = 0\nN = 70000\nfor sentence in X_train:    \n    sentence = str(sentence)\n    sentence = sentence.lower()\n    clnr = re.compile('<.*?>') # for cleaning html tags\n    sentence = re.sub(clnr, ' ', sentence)\n    sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|\/]', r' ', sentence) \n    \n    words = [snow.stem(word) for word in sentence.split() \\\n             if word not in stopwords.words('english')]\n    final_sentence = ''\n    for word in words:\n        final_sentence = final_sentence + word + ' '\n    pre_processedTrain.append(final_sentence)\n    print(\"{0:.2f} %\".format(i\/N*100), end='\\r')\n    i += 1\n    \nprint(pre_processedTrain[0])","b35bcd90":"\nlen(pre_processedTrain)","432b6877":"max_features = 5000\n\nbow_model = CountVectorizer(max_features=max_features)\nbow_dataTrain = bow_model.fit_transform(pre_processedTrain)\nbow_dataTrain = bow_dataTrain.toarray()\nbow_dataTrain[0]","dba2c18e":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nlogistic_ = LogisticRegression(penalty='l1')\n\ngrid_parameters = {'C':[10**-2, 10**-1, 1, 10, 100]}\ngridSearchModel = GridSearchCV(logistic_, grid_parameters, cv=5)\ngridSearchModel.fit(bow_dataTrain, Y_train)\ngridSearchModel.best_params_","38b294cc":"classifier = LogisticRegression(penalty='l1', C=gridSearchModel.best_params_['C'])\nclassifier.fit(bow_dataTrain, Y_train)\ntr_score = classifier.score(bow_dataTrain, Y_train)\nprint(tr_score)","17f92d74":"pre_processedTest = []\n\ni = 0\nN = 30000\nfor sentence in X_test:    \n    sentence = str(sentence)\n    sentence = sentence.lower()\n    clnr = re.compile('<.*?>') # for cleaning html tags\n    sentence = re.sub(clnr, ' ', sentence)\n    sentence = re.sub(r'[?|!|\\'|\"|#]', r'', sentence)\n    sentence = re.sub(r'[.|,|)|(|\\|\/]', r' ', sentence) \n    \n    words = [snow.stem(word) for word in sentence.split() \\\n             if word not in stopwords.words('english')]\n    final_sentence = ''\n    for word in words:\n        final_sentence = final_sentence + word + ' '\n    pre_processedTest.append(final_sentence)\n    print(\"{0:.2f} %\".format(i\/N*100), end='\\r')\n    i += 1\n    \nprint(pre_processedTest[0])\n\nbow_dataTest = bow_model.transform(pre_processedTest).toarray()","24b31f6f":"len(pre_processedTest)","0704f8e8":"pred = classifier.predict(bow_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","96f4ee4b":"models_performence = {\n    'Model':['Logistic Regression'],\n    'Vectorizer': ['BoW'],\n    'Regulizer': ['l1'],\n    'best parameter Search Technique': ['Grid Search'],\n    'Training Accuracy':[tr_score*100],\n    'Test Accuracy':[accuracy*100], \n    'Precision':[pre*100],\n    'Recall':[rec*100],\n    'F1-Score':[f1*100]\n}","6002f52b":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","a34291cf":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm)","24b03cb1":"from sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import uniform\n\nrandom_parameters = {'C':uniform()}\nrandomSearchModel = RandomizedSearchCV(logistic_, random_parameters, cv=5)\nrandomSearchModel.fit(bow_dataTrain, Y_train)\nrandomSearchModel.best_params_","441a779c":"classifier = LogisticRegression(penalty='l1', C=randomSearchModel.best_params_['C'])\nclassifier.fit(bow_dataTrain, Y_train)\ntr_score = classifier.score(bow_dataTrain, Y_train)\nprint(tr_score)","2bcfdf08":"pred = classifier.predict(bow_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","81f10cf7":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('BoW')\nmodels_performence['Regulizer'].append('l1')\nmodels_performence['best parameter Search Technique'].append('Random Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","d5684425":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","327b4169":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","aefcfdb3":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nlogistic_ = LogisticRegression(penalty='l2', max_iter=130)\n\ngrid_parameters = {'C':[10**-2, 10**-1, 1, 10, 100]}\ngridSearchModel = GridSearchCV(logistic_, grid_parameters, cv=5)\ngridSearchModel.fit(bow_dataTrain, Y_train)\ngridSearchModel.best_params_","01981afa":"classifier = LogisticRegression(penalty='l2', C=gridSearchModel.best_params_['C'])\nclassifier.fit(bow_dataTrain, Y_train)\ntr_score = classifier.score(bow_dataTrain, Y_train)\nprint(tr_score)","6c713f49":"pred = classifier.predict(bow_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","28a6280d":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('BoW')\nmodels_performence['Regulizer'].append('l2')\nmodels_performence['best parameter Search Technique'].append('Grid Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","d4096f7e":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","2b6c423a":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","89ed28a6":"random_parameters = {'C':uniform()}\nrandomSearchModel = RandomizedSearchCV(logistic_, random_parameters, cv=5)\nrandomSearchModel.fit(bow_dataTrain, Y_train)\nrandomSearchModel.best_params_","937cdbde":"classifier = LogisticRegression(penalty='l2', C=randomSearchModel.best_params_['C'])\nclassifier.fit(bow_dataTrain, Y_train)\ntr_score = classifier.score(bow_dataTrain, Y_train)\nprint(tr_score)","bac29fe1":"pred = classifier.predict(bow_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","4ae77be7":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('BoW')\nmodels_performence['Regulizer'].append('l2')\nmodels_performence['best parameter Search Technique'].append('Random Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","6e7414be":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","4056eae4":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","be461dfa":"max_features = 5000\n\ntf_idf_model = TfidfVectorizer(max_features=max_features)\ntf_idf_dataTrain = tf_idf_model.fit_transform(pre_processedTrain)\ntf_idf_dataTrain = tf_idf_dataTrain.toarray()\ntf_idf_dataTrain[0]","89d5ddcd":"logistic_ = LogisticRegression(penalty='l1', max_iter=130)\n\ngrid_parameters = {'C':[10**-2, 10**-1, 1, 10, 100]}\ngridSearchModel = GridSearchCV(logistic_, grid_parameters, cv=5)\ngridSearchModel.fit(tf_idf_dataTrain, Y_train)\ngridSearchModel.best_params_","0965f2a1":"classifier = LogisticRegression(penalty='l1', C=gridSearchModel.best_params_['C'])\nclassifier.fit(tf_idf_dataTrain, Y_train)\ntr_score = classifier.score(tf_idf_dataTrain, Y_train)\nprint(tr_score)","0396a704":"tf_idf_dataTest = tf_idf_model.transform(pre_processedTest).toarray()","178a126c":"pred = classifier.predict(tf_idf_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","99155c3e":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('Tf-IDF')\nmodels_performence['Regulizer'].append('l1')\nmodels_performence['best parameter Search Technique'].append('Grid Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","7d46cd14":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","d06e7a24":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","1871c8eb":"random_parameters = {'C':uniform()}\nrandomSearchModel = RandomizedSearchCV(logistic_, random_parameters, cv=5)\nrandomSearchModel.fit(tf_idf_dataTrain, Y_train)\nrandomSearchModel.best_params_","ab60aad4":"classifier = LogisticRegression(penalty='l1', C=randomSearchModel.best_params_['C'])\nclassifier.fit(tf_idf_dataTrain, Y_train)\ntr_score = classifier.score(tf_idf_dataTrain, Y_train)\nprint(tr_score)","808a1d3a":"pred = classifier.predict(tf_idf_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","3e141ad6":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('Tf-IDF')\nmodels_performence['Regulizer'].append('l1')\nmodels_performence['best parameter Search Technique'].append('Random Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","4111a745":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","371f1128":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","b35f99cb":"logistic_ = LogisticRegression(penalty='l2', max_iter=130)\n\ngrid_parameters = {'C':[10**-2, 10**-1, 1, 10, 100]}\ngridSearchModel = GridSearchCV(logistic_, grid_parameters, cv=5)\ngridSearchModel.fit(tf_idf_dataTrain, Y_train)\ngridSearchModel.best_params_","dbc18a76":"classifier = LogisticRegression(penalty='l2', C=gridSearchModel.best_params_['C'])\nclassifier.fit(tf_idf_dataTrain, Y_train)\ntr_score = classifier.score(tf_idf_dataTrain, Y_train)\nprint(tr_score)","45b5f8a4":"pred = classifier.predict(tf_idf_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","719dafc2":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('Tf-IDF')\nmodels_performence['Regulizer'].append('l2')\nmodels_performence['best parameter Search Technique'].append('Grid Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","504f17f5":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","1bb2504c":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","859e27d5":"random_parameters = {'C':uniform()}\nrandomSearchModel = RandomizedSearchCV(logistic_, random_parameters, cv=5)\nrandomSearchModel.fit(tf_idf_dataTrain, Y_train)\nrandomSearchModel.best_params_","1af6084d":"classifier = LogisticRegression(penalty='l2', C=randomSearchModel.best_params_['C'])\nclassifier.fit(tf_idf_dataTrain, Y_train)\ntr_score = classifier.score(tf_idf_dataTrain, Y_train)\nprint(tr_score)","8e30cddb":"pred = classifier.predict(tf_idf_dataTest)\n\naccuracy = metrics.accuracy_score(Y_test, pred)\npre = metrics.precision_score(Y_test, pred)\nrec = metrics.recall_score(Y_test, pred)\nf1 = metrics.f1_score(Y_test, pred)","97e0871d":"models_performence['Model'].append('Logistic Regression')\nmodels_performence['Vectorizer'].append('Tf-IDF')\nmodels_performence['Regulizer'].append('l2')\nmodels_performence['best parameter Search Technique'].append('Random Search')\nmodels_performence['Training Accuracy'].append(tr_score*100)\nmodels_performence['Test Accuracy'].append(accuracy*100)\nmodels_performence['Precision'].append(pre*100)\nmodels_performence['Recall'].append(rec*100)\nmodels_performence['F1-Score'].append(f1*100)","54e382c6":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","d3b8c363":"from sklearn.metrics import confusion_matrix\ncm = confusion_matrix(Y_test, pred)\nprint(cm)\nsns.heatmap(cm, annot=True, fmt=\"d\")","c5ae4239":"columns = [\"Model\", \"Vectorizer\", \"Regulizer\", \"Training Accuracy\", \"best parameter Search Technique\", \"Test Accuracy\",\n          \"Precision\", \"Recall\", \"F1-Score\"]\npd.DataFrame(models_performence, columns=columns)","3ecf2ef6":"# Summary","2c6c15a8":"## Preprocessing","098286b1":"### Training Logistic Regression ( L2 - regularizer )","2a6af6f2":"### Training Logistic Regression ( L1 - Regularizer )","27cf731e":"#### Random Search","62e4d9a5":"### Training Logistic Regression ( L2 - regularization)","1dabbe6f":"#### Random Search","0b1e2a8a":"## Bag of words ","3bc7555a":"#### Random Search","0f307c9c":"#### Grid Search","896634b6":"#### Random Search","12440dbf":"#### Grid Search","a2dd19d0":"#### Grid Search","e0954fee":"### Training logistic Regression ( L1 - Regularization )","c58e0461":"#### Grid Search","cce35b4e":" ## Tf - IDF"}}