{"cell_type":{"550bd720":"code","c113f65e":"code","0a7007ca":"code","1b829463":"code","0d74f64d":"code","a288e917":"code","8161780e":"code","260240fc":"code","63b85b9b":"code","f34ed0ae":"code","165d20d5":"code","d819bf35":"code","f3a1d47b":"code","711f1b7e":"code","1d90ec24":"code","aad9b1ea":"code","e3c83af0":"code","9fc99a64":"code","79ded41c":"code","29c93804":"code","5a39e8ad":"code","4de4bfd7":"code","46f79a01":"code","024984a5":"code","f454d376":"code","86dfd48e":"code","46f6166d":"code","2401b11d":"code","172189c0":"code","ce76fb0b":"code","14961b9f":"code","b640f168":"code","6b9b28f7":"markdown","93d3fabe":"markdown","b8483e15":"markdown","459b0557":"markdown","d7e6776d":"markdown","6b73d545":"markdown","c3f13952":"markdown","1af45a7b":"markdown","2f191dcc":"markdown","70e4f937":"markdown"},"source":{"550bd720":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import confusion_matrix\nfrom  xgboost import XGBClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nimport scipy.stats as st","c113f65e":"df = pd.read_csv('..\/input\/credit-scoring-for-microfinance-dataset\/\u00e7\u00e1\u00f1\u00e1\u00a1\u00bf\u00d1 ML \u00f1\u00bd\u2229 \u00ac\u00e1\u00a1\u00f1\u00bf\u00f1\u00e1\u0393\u00e1\/data.csv', sep=';')\ndf","0a7007ca":"df.shape","1b829463":"df.describe()","0d74f64d":"pd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","a288e917":"df.describe()","8161780e":"df.isnull().sum()","260240fc":"pd.set_option('display.max_rows', 15)","63b85b9b":"df[(df['feature_102'].isna()) & (df['feature_103'].isna()) & (df['feature_104'].isna()) & (df['feature_105'].isna()) & (df['feature_106'].isna())]","f34ed0ae":"df[\"target\"].value_counts()","165d20d5":"df[(df['feature_102'].isna()) & (df['feature_103'].isna()) & (df['feature_104'].isna()) & (df['feature_105'].isna()) & (df['feature_106'].isna())][\"target\"].value_counts()","d819bf35":"df[(df['feature_105'].notna()) & (df['feature_106'].isna())][\"target\"].value_counts()","f3a1d47b":"verify = pd.read_csv('..\/input\/credit-scoring-for-microfinance-dataset\/\u00e7\u00e1\u00f1\u00e1\u00a1\u00bf\u00d1 ML \u00f1\u00bd\u2229 \u00ac\u00e1\u00a1\u00f1\u00bf\u00f1\u00e1\u0393\u00e1\/verify.csv', sep=';')\npd.set_option('display.max_rows', None)\nverify.isnull().sum()","711f1b7e":"import pandas as pd\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n\ndata = pd.DataFrame(df[df['feature_106'].notna()],columns=['feature_102','feature_103','feature_104','feature_105','feature_106','target'])\n\ncorrMatrix = data.corr()\nsn.heatmap(corrMatrix, annot=True)\nplt.show()","1d90ec24":"#\u0421\u0442\u0440\u043e\u0438\u043c heatmap \u0434\u043b\u044f \u043f\u0440\u043e\u0432\u0435\u0440\u043a\u0438 \u043d\u0430 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u044e\nsns.heatmap(round(df.corr(), 1),annot=True,cmap='viridis')\nsns.set(rc={'figure.figsize':(20,10)})\nplt.title('Heatmap of co-relation',fontsize=15)\nplt.show()","aad9b1ea":"def scatter_plot_with_correlation_line(title, x_label, y_label, x, y):\n  #\u0441\u0442\u0440\u043e\u0438\u043c scatter\n  plt.scatter(x, y)\n  plt.title(title)\n  plt.xlabel(x_label)\n  plt.ylabel(y_label)\n  #\u0434\u043e\u0431\u0430\u0432\u043b\u044f\u0435\u043c \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u0443\u044e \u043b\u0438\u043d\u0438\u044e\n  m, b = np.polyfit(x, y, 1)\n  plt.plot(x, m*x + b)\n  #\u0432\u044b\u0432\u043e\u0434\u0438\u043c scatter \u0441 \u043a\u043e\u0440\u0440\u0435\u043b\u044f\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u043b\u0438\u043d\u0438\u0435\u0439\n  plt.show()\n    \ndef correlations(x,y):\n  print('Pearson\u2019s correlation coefficient = ' + str(st.pearsonr(x, y)[0]) + ', p-value = ' + str(st.pearsonr(x, y)[1]))","e3c83af0":"column_list = ['feature_102','feature_103','feature_104','feature_105','feature_106']","9fc99a64":"for column in range(len(column_list)):\n    scatter_plot_with_correlation_line('Correlation scatter for ' + column_list[column], column_list[column], 'target', df[df[column_list[column]].notna()][column_list[column]], df[df[column_list[column]].notna()]['target'])\n    correlations(df[df[column_list[column]].notna()][column_list[column]],df[df[column_list[column]].notna()]['target'])","79ded41c":"df_filled = df.fillna(df.max())","29c93804":"df_filled.isnull().sum()","5a39e8ad":"df_filled.dtypes","4de4bfd7":"pd.set_option('display.max_rows', 15)","46f79a01":"train, test = train_test_split(df_filled, test_size=0.33, random_state=42)\nX_train = train.drop(columns =['ID', 'target'])\ny_train = train['target']\nX_test = test.drop(columns =['ID', 'target'])\ny_test = test['target']","024984a5":"# Feature scaling with StandardScaler \nfrom sklearn.preprocessing import StandardScaler \nsc_X = StandardScaler() \nX_train_scaled = sc_X.fit_transform(X_train)\nX_test_scaled = sc_X.transform(X_test)","f454d376":"#weights = {0:4, 1:1}\nmodel = LogisticRegression(solver='liblinear',class_weight= 'balanced')\nmodel.fit(X_train_scaled,y_train)","86dfd48e":"y_pred_proba = model.predict_proba(X_test_scaled)","46f6166d":"def group_proba(proba):\n    if proba <= 0.1:\n        grouped_proba = '[0.0, 0.1]'\n    elif proba <= 0.2:\n        grouped_proba = '(0.1, 0.2]'\n    elif proba <= 0.3:\n        grouped_proba = '(0.2, 0.3]'\n    elif proba <= 0.4:\n        grouped_proba = '(0.3, 0.4]'\n    elif proba <= 0.5:\n        grouped_proba = '(0.4, 0.5]'\n    elif proba <= 0.6:\n        grouped_proba = '(0.5, 0.6]'\n    elif proba <= 0.7:\n        grouped_proba = '(0.6, 0.7]'\n    elif proba <= 0.8:\n        grouped_proba = '(0.7, 0.8]'\n    elif proba <= 0.9:\n        grouped_proba = '(0.8, 0.9]'\n    elif proba <= 1:\n        grouped_proba = '(0.9, 1.0]'\n    return(grouped_proba)","2401b11d":"test['prediction'] = y_pred_proba[:,1]\ntest['\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c'] = test['prediction'].apply(group_proba)\ncustom_dict = {'[0.0, 0.1]': 0}\ntest[['\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c','target']].groupby('\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c')['target'].agg(['sum','count']).sort_values(by=['\u0412\u0435\u0440\u043e\u044f\u0442\u043d\u043e\u0441\u0442\u044c'], key=lambda x: x.map(custom_dict)).reset_index().rename(columns={'sum':'\u0412\u0441\u0435\u0433\u043e \u0432 \u0433\u0440\u0443\u043f\u043f\u0435', 'count': '\u041a\u043e\u043b-\u0432\u043e tagget 1'})","172189c0":"test","ce76fb0b":"from sklearn.metrics import f1_score\ny_pred = model.predict(X_test_scaled)\nf1_score(y_test, y_pred, average='micro')","14961b9f":"verify = pd.read_csv('..\/input\/credit-scoring-for-microfinance-dataset\/\u00e7\u00e1\u00f1\u00e1\u00a1\u00bf\u00d1 ML \u00f1\u00bd\u2229 \u00ac\u00e1\u00a1\u00f1\u00bf\u00f1\u00e1\u0393\u00e1\/verify.csv', sep=';')\nverify_filled = verify.fillna(verify.max())\nX_verify = verify_filled.drop(columns = 'ID')\nsc_X = StandardScaler() \nX_verify_scaled = sc_X.fit_transform(X_verify)\ny_verify = model.predict_proba(X_verify_scaled)","b640f168":"# Create the dataframe\nverify['score'] = y_verify[:,1]\nverify[['ID', 'score']].to_csv('y_verify.csv',index=False)\nverify[['ID', 'score']]","6b9b28f7":"## Ok, lets think how we can deal with nulls\n## We can not delete rows with missing values, becouse we have missing values in the verify file as well\n## ","93d3fabe":"# \u2022\t\u0442\u0430\u0431\u043b\u0438\u0446\u0430 \u0441 \u0433\u0440\u0443\u043f\u043f\u0438\u0440\u043e\u0432\u043a\u043e\u0439","b8483e15":"## OK, but it would be great to see all columns of the dataframe","459b0557":"## Ok, we can see 5 columns with null values, let's check if nulls are random distributed or not and decide how to deal with it","d7e6776d":"## As we can see distibution values in the tagret column are different in rows with null values and in general, let's check the verify file if it contains nulls","6b73d545":"## Looks better, let's check the count of nulls in each column","c3f13952":"# EDA","1af45a7b":"# \u2022\t\u0444\u0430\u0439\u043b \u0441 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f\u043c\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 \u0444\u0430\u0439\u043b\u0430 verify.csv","2f191dcc":"# \u2022\t\u043c\u0435\u0442\u0440\u0438\u043a\u0438 \u043c\u043e\u0434\u0435\u043b\u0438 \u043d\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438\u0437 data.csv","70e4f937":"## Here we can see that nulls in all 5 columns are on the same rows, it is not look like random distribution\n## Let's check how the data in the target column is distributed in general and how distribution related to the nulls"}}