{"cell_type":{"2da4dfc2":"code","bacd4efc":"code","29c6f492":"code","fda9c122":"code","3cadc058":"code","96ff515d":"code","57eaa6dd":"code","e326b209":"code","cfa745cd":"code","ca8bf70b":"code","5afc3028":"code","610ae6b1":"code","c37dcf99":"code","893a94fb":"code","786d9c5a":"code","6edce49f":"code","2a8d23b5":"code","12447a45":"code","32d3b2f7":"code","9fdac86e":"code","61671f8c":"code","a6c04e6f":"code","0a10458b":"code","a7611f71":"code","cf3ec377":"code","bca8454a":"code","81e3fb6c":"code","efbdd04e":"markdown","31a2766c":"markdown","cd4d638c":"markdown","2884973f":"markdown","8bc54189":"markdown","3dcb157f":"markdown","7b3b3d71":"markdown","2cc22722":"markdown","b692e1f6":"markdown","02bc73b5":"markdown","c8549df0":"markdown","c2b8662a":"markdown","338dab00":"markdown","f91aa81e":"markdown","e17f33db":"markdown","06fa08ab":"markdown","21a8d487":"markdown","a1b3b1f3":"markdown","b76f41cb":"markdown","478154be":"markdown","f7d2748d":"markdown","bc8de307":"markdown","c59b0dc8":"markdown","68ab5e3e":"markdown","6766e754":"markdown","222c1b1d":"markdown","52d457ce":"markdown","f2960b15":"markdown"},"source":{"2da4dfc2":"# General:\nimport numpy as np\nimport warnings\nimport time\n# Graphics:\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\n# Mathematics:\nfrom random import random, randint, uniform\nimport numpy.polynomial.polynomial as poly\nfrom scipy.special import legendre\n# ML:\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\n\n\n# Helper: class for using colors in output\nclass color:\n    PURPLE   = '\\033[95m'\n    DARKCYAN = '\\033[36m'\n    BLUE     = '\\033[94m'\n    BOLD     = '\\033[1m'\n    END      = '\\033[0m'\nPRINT_LINE   = '-'*60 + '\\n'\n\n\n# Helper: short rounding routine\nr = lambda x: np.round(x, decimals=10)\n# NOTE that in this kernel we will round output to 10 decimals so that\n# we won't see things like \"6.162975822039155e-32\" instead of \"0\".\n\n\n# Helper: plot describer\ndef describe_plot(plot, title, xlabel='x', ylabel='y', grid=False):\n    \"\"\"Sets specified title and x,y labels to a plot and shows grid if told so\"\"\"\n    plot.title(title)\n    plot.xlabel(xlabel)\n    plot.ylabel(ylabel)\n    if grid:\n        plt.grid()\n\n\ndef experiment(pop_size, sample_size, target_order, model_order, mu, std, \n               low, hi, metrics='mse', show_result=True):\n    # 1. Generating data from a target polynomial of specified order:\n    target = legendre(target_order)               # We generate Legendre\n    X = np.random.uniform(low, hi, size=pop_size) # polynomials so our targets\n    y = np.array([target(x) for x in X])          # will look \"interesting\"\n    \n    # 2. Adding stochastic noise:\n    y = y + np.random.normal(mu, std, y.shape)\n    \n    # 3. Splitting data to train and test sets:\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=pop_size-sample_size)\n    \n    # 4. Fitting a polynomial of specified order to sampled points:\n    model_coefs = poly.polyfit(X_train, y_train, model_order)\n    fit = poly.Polynomial(model_coefs)\n    \n    # 5. Calculating E_in and E_out:\n    #           In-sample error E_in:\n    y_pred = [fit(x) for x in X_train]\n    E_in = mean_squared_error(y_train, y_pred)\n    #         Out of sample error E_out:\n    y_pred = [fit(x) for x in X_test]\n    E_out  = mean_squared_error(y_test, y_pred)\n    if metrics == 'rmse':\n        E_in, E_out = np.sqrt(E_in), np.sqrt(E_out)\n        \n    # 6. Showing the result graphically:\n    if show_result:\n        # 6a. Printing:\n        print(PRINT_LINE, 'Data generated.\\n\\tPopulation size :', pop_size)\n        print('\\tSample size     :', sample_size, '\\n\\tTarget order    :', target_order)\n        print('\\tModel order     :', model_order)\n        if mu == 0 and std == 0:\n            print(PRINT_LINE, 'Noise is not present.\\n')\n        else:\n            print(PRINT_LINE, 'Noise added.\\n\\tNoise mean :', mu, \n                  '\\n\\tNoise std  :', std, '\\n')\n        print(PRINT_LINE, 'Model fitted.\\n\\tE_in  :', r(E_in))\n        print('\\tE_out :', r(E_out))\n        # 6b. Drawing:\n        plt.clf()\n        describe_plot(plt, 'Training points, target and model', grid=True)\n        # -- Training sample points:\n        plt.scatter(X_train, y_train, c='r', marker='.', label='Points in sample')        \n        # -- Target polynomial:\n        x_coords = np.linspace(min(X), max(X), num=len(X)*10)\n        plt.plot(x_coords, target(x_coords), c='g', ls='--', label='Target polynomial')\n        # -- Fitted polynomial:\n        plt.plot(x_coords, fit(x_coords), label='Fitted polynomial')\n        # -- Area between the two polynomials:\n        plt.fill_between(x_coords, target(x_coords), fit(x_coords), \n                         facecolor = 'lavenderblush', label='Error area')\n        plt.legend(shadow=True)\n        plt.show()\n    return E_in, E_out","bacd4efc":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 2      # training set size\nNOISE_MEAN    = 0      # mean of Gaussian stochastic noise\nNOISE_STD     = 0      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\n# ********************************************************************** #\n\nexperiment(POPULATION_SIZE, SAMPLE_SIZE, TARGET_ORDER, MODEL_ORDER, \n           NOISE_MEAN, NOISE_STD, LOW, HI);","29c6f492":"warnings.filterwarnings('ignore') # Ignoring RankWarnings for now...\n\ndef series_of_experiments(repetitions, pop_size, sample_size, target_order, model_order, \n                          mu, std, low, hi, metrics='mse', show_output=True):\n    # 1. Repeating the experiment and storing the results:\n    in_list, out_list = [], []\n    for _ in range(repetitions):\n        e_in, e_out = experiment(pop_size, sample_size, target_order, model_order,\n                                 mu, std, low, hi, metrics=metrics, show_result=False)\n        in_list.append(e_in)\n        out_list.append(e_out)\n    # 2. Printing the results if asked:\n    if not show_output:\n        return np.mean(in_list), np.mean(out_list)\n    print('Conducted series of', repetitions, 'experiments. \\n',\n          '   \u2022 Sample size =', sample_size)\n    if mu == 0 and std == 0:\n        print('    \u2022 No stochastic noise')\n    else:\n        print('    \u2022 Stochastic noise present, \u03c3 =', std)\n    print('    \u2022 Fitting'+color.BOLD, target_order, color.END+'order with'+color.BOLD,\n          model_order, color.END+'order' )\n    print(PRINT_LINE,'\\tResults in sample')\n    print('\\t\\t'+color.DARKCYAN+'Mean E_in   :', r(np.mean(in_list)), color.END)\n    print('\\t\\tVariance    :', r(np.var(in_list)))\n    print('\\t\\tMin and Max : (', r(min(in_list)), '), (', r(max(in_list)), ')')\n    print('\\n\\tResults out of sample')\n    print('\\t\\t'+color.BLUE+'Mean E_out  :', r(np.mean(out_list)), color.END)\n    print('\\t\\tVariance    :', r(np.var(out_list)))\n    print('\\t\\tMin and Max : (', r(min(out_list)), '), (', r(max(out_list)), ')')\n    return np.mean(in_list), np.mean(out_list)","fda9c122":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 100  # amount of experiments to conduct\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 2      # training set size\nNOISE_MEAN    = 0      # mean of Gaussian stochastic noise\nNOISE_STD     = 0      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\nLOW, HI       = -9, 10 # lower and upper limits for X to generate\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","3cadc058":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 3      # training set size\nNOISE_MEAN    = 0      # mean of Gaussian stochastic noise\nNOISE_STD     = 0      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\n# ********************************************************************** #\n\nexperiment(POPULATION_SIZE, SAMPLE_SIZE, TARGET_ORDER, MODEL_ORDER, \n           NOISE_MEAN, NOISE_STD, LOW, HI);","96ff515d":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 100  # amount of experiments to conduct\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","57eaa6dd":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 3      # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\n# ********************************************************************** #\n\nexperiment(POPULATION_SIZE, SAMPLE_SIZE, TARGET_ORDER, MODEL_ORDER, \n           NOISE_MEAN, NOISE_STD, LOW, HI);","e326b209":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 100  # amount of experiments to conduct\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","cfa745cd":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 1000  # general population size\nSAMPLE_SIZE     = 500   # training set size\nNUM_EXPERIMENTS = 100   # amount of experiments to conduct\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","ca8bf70b":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 1000 # amount of experiments to conduct\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 3      # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","5afc3028":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 1      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","610ae6b1":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 0      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","c37dcf99":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 500  # general population size\nSAMPLE_SIZE   = 100    # training set size\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 2      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","893a94fb":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 1      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","786d9c5a":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 0      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","6edce49f":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 3      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","2a8d23b5":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 5      # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","12447a45":"# ************************* CONTROL PARAMETERS ************************* #\nTARGET_ORDER  = 2      # order of target polynomial\nMODEL_ORDER   = 15     # order of polynomial used for fitting\n# ********************************************************************** #\n\nseries_of_experiments(NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                      TARGET_ORDER, MODEL_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","32d3b2f7":"# And you may try it here: just use  MODEL_ORDER = 20","9fdac86e":"# Try it here!","61671f8c":"# Find it out!","a6c04e6f":"def errors_vs_complexity(delta, repetitions, pop_size, sample_size, \n                         target_order, mu, std, low, hi, metrics='mse'):\n    # 1. Calculating complexity interval:\n    l_order = max(0, target_order - delta)\n    h_order = target_order + delta\n    E_in_list, E_out_list = [], []\n    \n    # 2. Running series_of_experiments for each order of polynomial:\n    start_time = time.time()  # Time in!\n    for current_order in range(l_order, h_order + 1):\n        mean_E_in, mean_E_out = series_of_experiments(\n            repetitions,pop_size,sample_size,target_order, current_order, \n            mu, std, low, hi, metrics, show_output=False)\n        E_in_list.append(mean_E_in)\n        E_out_list.append(mean_E_out)\n    finish_time = time.time()  # Time out!\n    print('Measurement time :', finish_time-start_time, 'sec.')\n    \n    # 3. Drawing:\n    plt.clf()\n    describe_plot(\n        plt, '$E_{\\mathrm{in}}$, $E_{\\mathrm{out}}$ and complexity',\n        'Model complexity', 'Error', grid=True)\n    # -- E_in:\n    plt.plot(list(range(l_order, h_order+1)), E_in_list, \n             label='$E_{\\mathrm{in}}$')\n    # -- E_out:\n    plt.plot(list(range(l_order, h_order+1)), E_out_list, \n             label='$E_{\\mathrm{out}}$')\n    # -- Best point:\n    optimal_Q = list(range(l_order, h_order+1))[E_out_list.index(min(E_out_list))]\n    plt.scatter(optimal_Q, min(E_out_list), marker = '*', c='r',\n                label='Optimum')\n    plt.xticks(np.arange(l_order, h_order+1, 1))\n    plt.legend(shadow=True)\n    plt.show()","0a10458b":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 5000 # amount of experiments to conduct\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 3      # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\nDELTA         = 2      # orders [target_order-delta, target_order+delta]\n# ********************************************************************** #\n\nerrors_vs_complexity(DELTA, NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                     TARGET_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI);","a7611f71":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 5000 # amount of experiments to conduct\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 3      # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\nDELTA         = 2      # orders [target_order-delta, target_order+delta]\n# ********************************************************************** #\n\nerrors_vs_complexity(DELTA, NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                     TARGET_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI, metrics='rmse');","cf3ec377":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 5000 # amount of experiments to conduct\nPOPULATION_SIZE = 500  # general population size\nSAMPLE_SIZE   = 100    # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 2      # order of target polynomial\nLOW, HI       = -5, 5  # lower and upper limits for X to generate\nDELTA         = 6      # orders [target_order-delta, target_order+delta]\n# ********************************************************************** #\n\nerrors_vs_complexity(DELTA, NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n                     TARGET_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI, metrics='rmse');","bca8454a":"# ************************* CONTROL PARAMETERS ************************* #\nNUM_EXPERIMENTS = 100  # amount of experiments to conduct\nPOPULATION_SIZE = 5000 # general population size\nSAMPLE_SIZE   = 500    # training set size\nNOISE_MEAN    = 2      # mean of Gaussian stochastic noise\nNOISE_STD     = 8      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 16     # order of target polynomial\nLOW, HI       = -1, 1  # lower and upper limits for X to generate\nDELTA         = 10     # orders [target_order-delta, target_order+delta]\n# ********************************************************************** #\n\n# errors_vs_complexity(DELTA, NUM_EXPERIMENTS, POPULATION_SIZE, SAMPLE_SIZE, \n#                      TARGET_ORDER, NOISE_MEAN, NOISE_STD, LOW, HI, metrics='rmse');","81e3fb6c":"# ************************* CONTROL PARAMETERS ************************* #\nPOPULATION_SIZE = 100  # general population size\nSAMPLE_SIZE   = 15     # training set size\nNOISE_MEAN    = 0      # mean of Gaussian stochastic noise\nNOISE_STD     = 0.1      # std  of Gaussian stochastic noise\nTARGET_ORDER  = 10      # order of target polynomial\nMODEL_ORDER   = 10      # order of polynomial used for fitting\nLOW, HI       = -1 , 1  # lower and upper limits for X to generate\n# ********************************************************************** #\nexperiment(POPULATION_SIZE, SAMPLE_SIZE, TARGET_ORDER, MODEL_ORDER, \n           NOISE_MEAN, NOISE_STD, LOW, HI);","efbdd04e":"# [Theory]: experiments with noise and overfitting\n<br><br>\nThis kernel is mosty theoretical. It is written in Python, but I've tried to build it in such a way that if you're an R user unfamiliar with Python, you still will be able to use this kernel easily as Python knowledge isn't really required here.\n\nHow to use this kernel: you may fork it and conduct various experiments, changing code and\/or control variables in experiment cells as you will see below. It is highly recommended that you watch [Lecture 11 - Overfitting](https:\/\/youtu.be\/EQWr3GGCdzw) prior to reading this kernel or at the same time with it.  [Lecture 8 - Bias-Variance Tradeoff](https:\/\/youtu.be\/zrEyxfl2-a8) may be helpful too in case you are unfamiliar with bias-variance decomposition and tradeoff.\n\n<font color='brown'>Any questions, suggestions or corrections are highly appreciated!<\/font>\n<br><br>\n\n<a class=\"anchor\" id=\"bullet-intro\"><\/a>\n# Introduction\n\nAs follows from [this CalTech lecture about overfitting by Professor Yaser Abu-Mostafa](https:\/\/youtu.be\/EQWr3GGCdzw?t=13m12s), overfitting is fitting the data more than is warranted, and the main culprit of overfitting is noise. This means that if noise is present, fitting a model which is a 10th order polynomial to data points generated by a 2nd order polynomial target function leads to a model fitting noise rather than the signal - in case of training set size not being suffeciently large. We can define deterministic and stochastic noise.\n\nLet's say we have data which is generated by the following function:\n\n$$y = \\underbrace{f(x)}_{\\text{signal}} + \\underbrace{\\varepsilon}_{\\text{noise}}$$\n\nWhile $f(x)$ is the true target function which generates the response $y$ from any given input data $x$, the $\\varepsilon$ is **stochastic noise**: it may represent various side factors affecting response generation process, like a measurement error. For example, a weighing scale measures one's weight, but its measurement may be affected by magnetic fields, friction of mechanical parts, mis-calibration over time, and a number of other factors. In this case the number it shows us consists of:\n\n$$\\underbrace{y}_{\\text{what we see}} = \\underbrace{f(x)}_{\\text{true weight}} + \\underbrace{\\varepsilon}_{\\text{side factors}}$$\n\nStrictly speaking it also transforms weight in Newtons to mass in kilograms, but that's irrelevant for now, you get the idea anyway.\n\nStochastic noise is random noise, meaning we cannot predict its exact value. It interferes with response variable values, which is a problem for us: when a digital scale displays \"74.19 kg\", which part of this number is your real mass, and which is a measurement error? We don't know. Well, in case of a weighing scale you may assume that measurement error is close to zero and so it's too small to account for unless you're working with extremely small masses, but it's still there anyway. If you want an example of significant error, look at GPS navigators: sometimes when you search your location, the spot they show you on the map differs from the your real location by 50 or 100 meters. It may show you absurd things, like your car being off road or you standing on the water in the middle of the lake.\n\nOne might be led to think that if the minimum needed amount of data points is provided and if stochastic noise is absent, overfitting won't occur too (since the noise is what's responsible for it). However, there is another kind of noise, called **deterministic noise**. That's when there are random fluctuations or measurement errors in the data which are not modeled, or the phenomenon being modeled (or learned) is too complex, and so the data contains this added complexity that is not modeled, this added complexity in the data has been called deterministic noise. Example: a target function is 10th order polynomial, and we use 2nd order polynomial to learn it. Naturally you cannot approximate (perfectly) the 10th order target with the 2nd order polynomial, so your model is physically incapable of modelling the phenomenon which is described by 10th order polynomial perfectly; thus, there will always be some error, no matter how many data points you've got: even if you are given the whole population and not a random sample, still your 2nd order polynomial won't be close to the 10th order one so you'll observe some fitting deviation (say, if you use $\\text{MSE}$ metric, you'll get $\\text{MSE}\\gt 0$).<br>\nAnd if you were to use the 10th order polynomial as a model to fit the data from 10th order target, your model would have the potential to approximate the target perfectly: indeed, you can approximate 10th order polynomial with a 10th order polynomial just perfectly, spot on. However  if you don't have enough data points, fitting such a model will be an extremely bad idea. This is because deterministic noise is closely related to model **bias**, which you may have noticed already. Bias is inability of a model to approximate the target function. From bias-variance tradeoff follows that if bias decreases, variance increases. Thus decreasing deterministic noise will lead to increasing model variance, and if you don't have enough data points, variance will increase far more than the bias have decreased, thus your error (MSE for example) will be even greater than when you were using the 2nd order model, which we will see shortly.\n\nIn fact, we can represent both kinds of noise in bias-variance decomposition:\n\n$$\\text{E}_{\\mathcal{D}}[E_{out}(g^{\\mathcal{D}}(x))] = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{\\text{variance}}+ \\underbrace{\\text{E}_x[(\\bar{g}(x)-f(x))^2]}_{\\text{ deterministic noise}} + \\underbrace{ \\text{E}_{\\varepsilon, x}[(\\varepsilon(x))^2] }_{\\text{stochastic noise}}$$\n\n<a class=\"anchor\" id=\"bullet-sec1\"><\/a>\n## Section 1: Noise\n\nSo from\n<br><br>\n$$\\text{E}_{\\mathcal{D}}[E_{out}(g^{\\mathcal{D}}(x))] = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{\\text{variance}}+ \\underbrace{\\text{E}_x[(\\bar{g}(x)-f(x))^2]}_{\\text{deterministic noise}} + \\underbrace{ \\text{E}_{\\varepsilon, x}[(\\varepsilon(x))^2] }_{\\text{stochastic noise}}$$\n<br>\nfollows that if stochastic and deterministic noise are both absent ($ = 0$), the expected error $E$ will be equal to just variance:\n<br><br>\n$$\\text{E}_{\\mathcal{D}}[E_{out}(g^{\\mathcal{D}}(x))] = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{\\text{variance}}+ \\underbrace{\\text{E}_x[(\\bar{g}(x)-f(x))^2]}_{=\\space0} + \\underbrace{ \\text{E}_{\\varepsilon, x}[(\\varepsilon(x))^2] }_{=\\space0} = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{\\text{variance}}$$\n<br>\nIn terms of polynomial targets and models, this means:\n1. Using polynomial of the same order as the target to eliminate deterministic noise. This would lead to mean hypothesis $\\bar{g}$ (the result of training the model on $K$ datasets with $K \\to \\infty$) being exactly equal to target function $f$: $\\bar{g}(x) \\approx \\frac{1}{K} \\sum_{i=1}^K g^\\mathcal{D_i}(x) = f(x)$\n2. Absense of stochastic noise.\n\nLet's try it. For now we will use a *fixed target function which is 2nd order polynomial* and MSE as error metric.\n<br><br><hr><br>\n<a class=\"anchor\" id=\"bullet-exp1\"><\/a>\n<center> <font color=\"green\"><b>Experiment 1. Zero noise, nonzero variance<\/b><\/font><\/center>\n<br><br>\n\n**Target function:**\n* 2nd order polynomial\n\n**Conditions:**\n1. Stochastic noise is zero: $\\forall{x}, \\varepsilon(x)=0$\n2. Deterministic noise is zero: $\\text{Complexity(g)}=\\text{Complexity(f)}$. In our case it means that we are using parabola to fit data generated from another parabola.\n3. Model variance is nonzero. To achieve this, train set size must be $\\lt 3$ (in case of target = parabola and with the absense of stochastic noise). Why? Well, imagine the train set size is 1, so we have only 1 point to fit our model to. How many parabolas passing through 1 point can one draw? That's right, infinitely many. The same goes for 2 points. However, if you have 3 points, you can only draw one parabola which passes through those points. If we were to generate 3 training points without a noise, than model variance in this setting would be exactly zero. We will try this shortly, but for now let's proceed with nonzero variance.\n\n**Expected results**:\n* We expect exactly zero in-sample error $E_{\\text{in}}(X) = 0$: a parabola can be fitted to any 2 points perfectly, and with no noise present those points lie exactly on target.\n* We expect some nonzero out-of-sample error $E_{\\text{out}}(X) \\neq 0$ which is caused entirely by model variance, since stochastic and deterministic noise is absent. Depending on the exact parabola which is picked by the algorithm, the error can be extremely small or extremely large, and we have no way to predict which one will it be.","31a2766c":"* What happens in this scenario: you have a sample of size 2, the 2nd order target function and no stochastic noise. Which model does better: the 0th order, the 1st order or the 2nd order?","cd4d638c":"This time the 1st order model yielded a *larger* $E_{\\text{out}}$ than the 2nd one did! What about 0th order?","2884973f":"$E_{\\text{in}}\\neq 0$, increased a bit, but $E_{\\text{in}}\\neq 0$ decreased a lot. \n\n**Note**: so in this setting (small sample size + presence of stochastic noise) fitting the provided data with a 2nd or even 1st order target would be an overfit. We should use the 0th order for the best result. Keep in mind that in machine learning what's important for us is $E_{\\text{out}}$. \"How well does the learning algorithm generalize to unseen data?\" is the question of interest. Paying the price of getting a larger $E_{\\text{in}}$ for getting a substantially decreased $E_{\\text{out}}$ is definitely a good deal.\n\n\n### Case B - Ample data resources\n* Sample size is relatively large.\n\nNow let's simulate the same 3 situations, but this time our sample size will be 100 instead of just 3. We begin by trying to fit 2nd order model to data from 2nd order target. Recall that 3 code cells above this resulted in overfitting and a terrible $E_{\\text{out}}$. However, now that the sample is quiet a bit larger, the result is:","8bc54189":"What happened here is the model learned the noise rather than the signal. We again over-fitted: we used a too powerful model (surely 15th order polynomial is capable of approximating any 2nd order polynomial possible) in a situation when we didn't have enough data resources for actually using it.\n\n**Summary:**\n* For a noisy 2nd order target generating just 3 training examples, fitting 2nd order model was an over-fit even though the true target function is 2nd order also, so we should have nailed it perfectly, but noise and insufficient training data were the reasons we got a terrible $E_{\\text{out}}$.\n* For a noisy 2nd order target generating 100 training examples, fitting 0th and 1th order models was an under-fit, using 2nd order was a good fit, and using 5th and 15th order was an over-fit.\n\n**Things to try:**\n* Notice that if we had enough data for fitting the 15th order, we would do way better than this. You may try to check it yourself.","3dcb157f":"Indeed we've got an increased $E_{\\text{in}}$, but in exchange our $E_{\\text{out}}$ decreased by a hundred thouthands times. \n\nSo compare mean error in these two cases: sample size = 500 points and 3 points (the two code cells above). For 3 points we observed low $E_{\\text{in}}$ and high $E_{\\text{out}}$, and for 500 points $E_{\\text{in}}$ significantly increased while $E_{\\text{out}}$ dropped.\n\nNotice that this is an example of <font color='brown'>overfitting<\/font>. Indeed, we had no detereministic noise since model and target complexity were equal: we approximated a parabola with a parabola which sounds logical. However, we had stochastic noise in data and way too little resources (only 3 points) to produce a good fit given presense of $\\varepsilon$. We should have used the 1st order in this case, or maybe just a horizontal line instead of parabola; instead, we *over-fitted*, we used a model that is too strong for this specific scenario, even though this model is theoretically capable of approximating the target perfectly. Mathematically we had a situation of\n\n$$\\text{E}_{\\mathcal{D}}[E_{out}(g^{\\mathcal{D}}(x))] = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{huge}\\space+\\space\\underbrace{\\text{E}_x[(\\bar{g}(x)-f(x))^2]}_{\\text{= 0}}\\space+\\space\\underbrace{ \\text{E}_{\\varepsilon, x}[(\\varepsilon(x))^2] }_{\\text{= 0}} = \\text{huge!}$$\n\nSo far we have seen that:\n* When we have zero noise, zero bias and zero variance, we will be spot on every time we train (Experiment 2)\n* Fit to data resources we have, not target complexity (Experiment 3) <br><br><br><br>\n\n<a class=\"anchor\" id=\"bullet-sec2\"><\/a>\n## Section 2: Overfitting\n<br>\nWe observed influence of stochastic and deterministic noise on a model. Now let us observe various cases of overfitting. Again, overfitting is fitting the data more than it's warranted, which means using a model that is \"too strong\" when stochastic noise is present and\/or we have insufficient data resources. Since we typically have no idea of target function complexity, it's difficul for us to tell whether our chosen model matches target complexity, so we have to match the data resources we have, not target complexity. Actually as it is shown [in the lecture](https:\/\/www.youtube.com\/watch?v=EQWr3GGCdzw), we should match the data resources we have in any case.\n\nRecall that in Experiment 3 we already encountered a case of overfitting. We had 2 data points and tried to fit a parabola to them, which was a bad idea: we overfitted. Let's \"down-fit\" a bit! What if we use a \"weaker\" model? - that is, less complex one. In case of 2nd order target (parabola), we will use the 1st order (general line) and the 0th order (horizontal line) as a model and see which one produces the better fit and the lowest out-of-sample error $E_{out}$.\n\n<br><br><hr><br>\n<a class=\"anchor\" id=\"bullet-exp4\"><\/a>\n<center> <font color=\"green\"><b>Experiment 4. Overfitting with fixed stochastic noise<\/b><\/font><\/center>\n<br><br>\n\n**Conditions:**\n1. Stochastic noise is present and fixed: $\\sigma^2[\\varepsilon] = \\text{const} \\neq 0$\n2. Sample size $n$ will vary.\n3. Target and model complexity will vary.\n\n**Expected results**:\n* For relatively small sample sizes $n$ we expect simple model to perform better out-of-sample and worse in-sample, than a complex model would. Simple model's bias may be bigger than that of a complex model, but its variance is significantly lower. However, too simple model will underfit.\n* For relatively large sample sizes $n$ we expect the more complex model to perform better both in and out of sample. This is because complex model has more \"flexibility\" to fit to data better, and when we have data that is ample enough, it may negate influence of stochastic noise. However, too complex model will overfit.\n\nLet us examine two cases: when data resources available for training are scarce, and the contrary - when they are ample enough.\n\n### Case A - Poor available data resources\n* Sample size is relatively small.\n\nTo remind you, in Experiment 3 we tried to fit parabola to 3 points generated from a parabola with some added noise. The result we got was:","7b3b3d71":"This shows that the best model to use is 0th order. Just as we did in Experiment 3, let's provide more training data to the model - namely 100 points:\n\n*<font color='grey'>Caution: this may take 3-5 minutes or so to run. For faster execution time but also less stable estimates, decrese `NUM_EXPERIMENTS` parameter.<\/font>*","2cc22722":"<br>\n## Plotting errors as functions of model order\n<br>\n\nTo see the big picture let's try to plot $E_{\\text{in}}$ and $E_{\\text{in}}$ as functions of model complexity.","b692e1f6":"You should get \"The fit may be poorly conditioned\" warning here, which is fine: of course it's poorly conditioned, because we have only 2 training points and there's an infinite amount of parabolas which pass through those 2 points so an algorhitm doesn't know which of them to pick as final hypothesis that will be returned to us. We just need it so that there will be nonzero model variance.\n\nIf you run the above cell several times, you may notice that the results tend to vary quite a bit. The reason is, again, the fact that we provided insufficient amount of training points: since there are infinitely many 2nd order polynomials that pass through these given points, our learner has infinitely many hypotheses which are equally good from his point of view, so sometimes he might give us a final hypothesis that is really close to the target polynomial, and sometimes it will be way off.\n\nWe may repeat the experiment several times, keep record of the results and display them in an organized manner. Let's define a new function:","02bc73b5":"* This time let's use the 1st order model which is a general line $ax+b$ to fit those 2 data points:","c8549df0":"Notice how the models goes through all those noisy points. Depending on randomly sampled points, this plot may come out a bit ugly though, need to fix it.\n<br><br>\n\nLater I will add some other things here. If you have suggestions about that, feel free to share!","c2b8662a":"As we can see,\n* 0th and 1st order models led to underfitting: bad $E_{\\text{in}}$, bad $E_{\\text{out}}$\n* 2nd order model produced good fit and generalization: nice $E_{\\text{in}}$, nice $E_{\\text{out}}$\n* 3rd, 4th, 5th, 6th, 7th, 8th order (and so on) models led to overfitting: excellent $E_{\\text{in}}$, bad $E_{\\text{out}}$\n\n<br><br><hr><br>\n<a class=\"anchor\" id=\"bullet-sec3\"><\/a>\n# Section 3: Playground\n<br><br>\nHere you may run the experiments that come to your mind. What would be the result if we were to use 14th order model for 23rd order target provided noisy sample sized 1500? What if we remove the noise, change its mean or energy? Which model complexity would be optimal? Etc...\n\nFrankly I've tried to reproduce a nice smooth curve like the one in the picture below:\n\n![of][3]\n\nBut was unable to do so. To replicate this, we may try to run the following:\n\n[3]: https:\/\/image.ibb.co\/nqFqBw\/Screenshot_from_2018_01_21_01_55_48.png","338dab00":"**Results:**\n* We've got $E_{\\text{in}} = E_{\\text{out}} =0$ as expected.\n\nNotice that in this experiment we've created an ideal situation where given various samples S, the <font color='brown'>learner's hypothesis set $\\mathcal{H}$ will consist of just one hypothesis, which is also a true target function $f(x)$<\/font>. This means that in this setting the learner is guaranteed to guess the true function every time, for every training set we could possibly give him to train on.\n\nThis is the situation of \"zero bias, zero variance, zero noise\". \n\nNow let's take this situation again, and add some stochastic noise.\n<br><br><hr><br>\n<a class=\"anchor\" id=\"bullet-exp3\"><\/a>\n<center> <font color=\"green\"><b>Experiment 3. Nonzero noise, nonzero variance<\/b><\/font><\/center>\n<br><br>\n\n**Target function:**\n* 2nd order polynomial\n\n**Conditions:**\n1. Stochastic noise is nonzero: $\\sigma^2[\\varepsilon] \\neq 0$\n2. Deterministic noise is zero: $\\text{Complexity(g)}=\\text{Complexity(f)}$.\n3. Model variance is nonzero. This time, even if we have a sample of size $\\geq 3$ parabola fitted to those points will most likely to be different from the true $f(x)$ because stochastic noise is present. The stronger the noise is, the more fitted parabola will differ from true $f(x)$. So every time we draw a sample to train on, we may get 3 points that don't lie on a target parabola, meaning that this time the fitted parabola will be different from the true one. The mean hypothesis  $\\bar{g}(x)=\\text{E}_{\\mathcal{D}}[g(x)]$ will no longer be the only one hypothesis present in hypotheses set $\\mathcal{H}$ of our learner.\n\n**Expected results**:\n* We expect $E_{\\text{in}}$ to vary depending on noise energy and size of training set. For example for training set of size 3, $E_{\\text{in}}$ will be exactly 0, because you can fit a parabola to any given 3 points perfectly. However, if we add one more point, we will expect $E_{\\text{in}} \\neq 0$ because due to presense of noise the 4th point won't lie on the same curve as the previous 3 points. We can expect $E_{\\text{in}}$ to grow to a certain limit with growth of training set's size.\n* We expect $E_{\\text{out}}$ to vary depending on noise energy and size of training set, but the relation here is different: we can expect $E_{\\text{in}}$ to decrease to a certain limit with growth of training set's size.","f91aa81e":"We see that in sample error is not zero anymore $E_{\\text{in}}\\neq 0$, but out-of-sample error $E_{\\text{out}}$ went down significantly. \n\nWhat if we do it again, but this time with even simpler model - a 0th order which is just a horizontal line?","e17f33db":"So this time 1st and 0th order models shown approximately the same result - worse than the 2nd order, both in sample and out of sample. This means that this time using 2nd order model for fitting was not an over-fit. Why? It is because this time the 2nd order model had ample train data which negated the noise to a certain degree, thus the fit we got was worlds better. \n\nDoes it mean the 3rd order will do even better, now that we have 100 points to train on? Let's see:","06fa08ab":"No, we see that the 3rd order model did roughly as good as the 2nd one. What about the 5th order then?","21a8d487":"$E_{\\text{out}}$ increases even further. Let's skip a bunch of orders and see what happens if we try to use 15th order model:","a1b3b1f3":"And now for a series of experiments:","b76f41cb":"RankWarning is not shown anymore because it's ignored: we don't want it to print itself `NUM_EXPERIMENTS` times, do we?\n\n**Results:**\n - We observed zero $E_{\\text{in}}$ as expected. If you don't use rounding or any form of output truncation, you might be seeing something like $E_{\\text{in}} = 1.1093356479670479\\text{e}^{-31} \\neq 0$, the reason is simply a calculation error due to hardware\/software limitations. $E_{\\text{in}}$ in this case is exactly $=\\space0$.\n - We observed a large $E_{\\text{out}}$ with huge variance (just look at it!), because one may draw an infinite amount of parabolas and it's not clear which one of them to return as the best guess, since all of them are equally good.\n * We observed $E_{\\text{out}}$ caused by model variance alone.\n \n What happens if we remove variance?\n \n<br><br><hr><br>\n<a class=\"anchor\" id=\"bullet-exp2\"><\/a>\n<center> <font color=\"green\"><b>Experiment 2. Zero noise, zero variance<\/b><\/font><\/center>\n<br><br>\n\n**Target function:**\n* 2nd order polynomial\n\n**Conditions:**\n1. Stochastic noise is zero: $\\forall{x}, \\varepsilon(x)=0$\n2. Deterministic noise is zero: $\\text{Complexity(g)}=\\text{Complexity(f)}$. In our case it means that we are using parabola to fit data generated from another parabola.\n3. Variance is zero. To achieve this, train set size must be $\\geq 3$ (in case of target = parabola and with the absense of stochastic noise). Why? Because if we have 3 points, there is only one parabola that can pass through them (any parabola can be written as $\\text{a}x^2+\\text{b}x+\\text{c}$ so it requires 3 points to uniquely define it). This means that in absense of stochastic and deterministic noise, our target function will generate 3 points lying on its graph every time we ask it to, so for every possible sample of $n\\geq3$ points the model polynomial will be exactly the same as target polynomial, spot on. In other words, for every given sample S of size 3 or greater, our model's returned hypothesis will be equal to its mean hypothesis: $g^S(x) = \\text{E}_\\mathcal{D}\\big[g^\\mathcal{D}(x)\\big]$, and since $\\varepsilon = 0$ and $\\text{bias}=0$, the mean hypothesis will be exactly equal to $f(x)$, and remember it is returned for every given S. <br> So in this setting, our model is expected to return the true function $f(x)$ every time we train it.<br>\nSame goes for training set size $\\gt 3$: if we specify 50, or 100, or $10^6$ training points which all lie on a target parabola, then there's only one parabola that goes through all of them. Thus our model will return the same guess - which also will be identical to target function - every time we train it.\n\n**Expected results**:\n* We expect exactly zero in-sample error $E_{\\text{in}}(X) = 0$\n* We expect exactly zero out-of-sample error $E_{\\text{out}}(X) = 0$ as a result of zero variance and zero noise:\n\n$$\\text{E}_{\\mathcal{D}}[E_{out}(g^{\\mathcal{D}}(x))] = \\underbrace{ \\text{E}_{\\mathcal{D}}[\\text{E}_x[(g^{\\mathcal{D}}(x)-\\bar{g}(x))^2]] }_{=\\space 0}+ \\underbrace{\\text{E}_x[(\\bar{g}(x)-f(x))^2]}_{=\\space 0} + \\underbrace{ \\text{E}_{\\varepsilon, x}[(\\varepsilon(x))^2] }_{=\\space 0} = 0$$","478154be":"We're observing zero $E_{\\text{in}}$ and very large $E_{\\text{out}}$.\n\nNotice that if we increase training set size significantly (from 3 to 500 in this case), $E_{\\text{out}}$ will drop, whereas $E_{\\text{in}}$ will grow as you can see in the cell below:","f7d2748d":"Running this cell took around 3 minutes, so I commented it out and included the screenshot of the result only.\n\n![result_2][4]\n\nWe don't see smooth curves in the picture. I guess they should be smoother if we increase `NUM_EXPERIMENTS` so the estimates of $\\text{E}_{\\text{in}}$ and $\\text{E}_{\\text{out}}$ will be a bit more stable, but it increases execution time as well, which quickly gets to 30 minutes, 1 hour, 5 hours etc. So we're rather limited here! With `NUM_EXPERIMENTS = 1` this runs in 1.5 sec., with 10 it runs in 15 sec.,  with 100 it takes 159 sec., so for  `NUM_EXPERIMENTS = 1000` it's going to take half an hour.\n\nEmpirically I've noticed that `NUM_EXPERIMENTS = 10` produces very unstable estimates (try it yourself), and 100 or more does notably better. I guess it shoud be around 1000 or even greater, but it will take several hours to run. Kaggle Kernel interactive session will expire by that time.\n\n[4]: https:\/\/image.ibb.co\/eYDZnK\/finish_2.png\n\nOr one can see that in situation of overfitting, a model learns noise:","bc8de307":"And now multiple runs:","c59b0dc8":"Now, the cell you see below is the **experiment cell**. If you want to play around, you can fork this kernel and run experiment cells, altering the control parameters at whim to see different results in different situations, without the need to alter the actual code of `experiment()` function. But of course you can alter the `experiment()` if you need to.","68ab5e3e":"And now, running it we will have:","6766e754":"$E_{\\text{out}}$ is just huge! Notice that the 15th order does better in sample ($E_{\\text{in}}$ decreased), but it performs very poorly out of sample. Trying to fit the available data with, let's say, 20th order will result in even greater $E_{\\text{out}}$ and smaller $E_{\\text{in}}$.","222c1b1d":"We saw earlier in Experiment 3 that given 3 noisy training points, the 0th order model led to a better result than 1st or 2nd order models, but given 100 training points, 2nd order was the best out of those three. We can see graphically the behaviour of $E_{\\text{in}}$ and $E_{\\text{in}}$ as model flexibility changes:","52d457ce":"The plot here doesn't look nice because y axis is way too large, just look at its scale. Let's use RMSE instead of MSE so that our errors will have a lesser magnitude:","f2960b15":"We've got a much better result than the last time! $E_{\\text{out}}$ is very small. Now, just as we did before, let's try fitting the data with the 1st order model:"}}