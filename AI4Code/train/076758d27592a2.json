{"cell_type":{"994781c3":"code","ca27cb55":"code","1019dc2e":"code","ad652aad":"code","be27d302":"code","3b057783":"code","e63f9b82":"code","81e35042":"code","3f0d09ac":"code","b81c24f3":"code","86c8706a":"code","94a874de":"code","0b35d58c":"code","fe308b6b":"code","36335987":"code","97a92c85":"code","71fe8666":"code","2b551ec0":"code","2c26a983":"code","94c8f710":"code","a5d87b22":"code","6c100fc4":"code","e1bd5eef":"code","9ddcb80e":"code","a6902d4b":"markdown","8210805c":"markdown","5a5116c3":"markdown","67fc6c57":"markdown","0cf76cc2":"markdown","6ac3b666":"markdown","14b8fb69":"markdown","8d36a4ec":"markdown","e0a7d58b":"markdown"},"source":{"994781c3":"import sys\nsys.path.append('..\/input\/timm-pytorch-image-models\/pytorch-image-models-master')\nimport timm\n","ca27cb55":"import os\nimport sys\nimport re\nimport gc\nimport platform\nimport random\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import autocast, GradScaler\nfrom torch.optim.lr_scheduler import CosineAnnealingWarmRestarts, CosineAnnealingLR, ReduceLROnPlateau\n\n#import einops\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nimport json\n\nimport glob\nimport cv2\n\n#from rich import print as _pprint\n#from rich.progress import track\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n#import wandb\n\nimport warnings\nwarnings.simplefilter('ignore')","1019dc2e":"if os.path.exists(\"..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\"):\n    data_directory = '..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"..\/input\/efficientnetpyttorch3d\/EfficientNet-PyTorch-3D\"\nelse:\n    data_directory = '\/media\/roland\/data\/kaggle\/rsna-miccai-brain-tumor-radiogenomic-classification'\n    pytorch3dpath = \"EfficientNet-PyTorch-3D\"","ad652aad":"mri_types=['FLAIR']\n\ndef get_patient_id(patient_id):\n    res=str(int(patient_id)).zfill(5)\n    return res\n    \n\ndef get_path(row,is_test=False,mri_type=mri_types[0]):\n    patient_id = get_patient_id(row.BraTS21ID)\n    if is_test:\n        path=f'..\/input\/rsna-miccai-png\/test\/{patient_id}\/{mri_type}\/'\n    else:\n        path=f'..\/input\/rsna-miccai-png\/train\/{patient_id}\/{mri_type}\/'\n    return path\n\ndef wandb_log(**kwargs):\n    \"\"\"\n    Logs a key-value pair to W&B\n    \"\"\"\n    for k, v in kwargs.items():\n        wandb.log({k: v})\n        \ndef cprint(string):\n    \"\"\"\n    Utility function for beautiful colored printing.\n    \"\"\"\n    print(f\"[black]{string}[\/black]\")","be27d302":"Config = dict(\n    MAX_FRAMES = 12,\n    EPOCHS = 15,\n    LR = 1.2e-5,\n    IMG_SIZE = (224, 224),\n    FEATURE_EXTRACTOR = 'resnet34',\n    DR_RATE = 0.35,\n    NUM_CLASSES = 1,\n    RNN_HIDDEN_SIZE = 100,\n    RNN_LAYERS = 1,\n    TRAIN_BS = 8,\n    VALID_BS = 4,\n    NUM_WORKERS = 2,\n    infra = \"Kaggle\",\n    competition = 'rsna_miccai',\n    _wandb_kernel = 'tanaym'\n)","3b057783":"class Augments:\n    \"\"\"\n    Contains Train, Validation Augments\n    \"\"\"\n    train_augments = A.Compose([\n        ToTensorV2(p=1.0),\n    ],p=1.)\n    \n    valid_augments = A.Compose([\n        ToTensorV2(p=1.0),\n    ], p=1.)","e63f9b82":"class RSNADataset(Dataset):\n    def __init__(self, df, augments=None, is_test=False):\n        self.df = df\n        self.augments = augments\n        self.is_test = is_test\n        \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        paths = self.getPaths(row)\n        frames = []\n        for path in paths:\n            img = cv2.imread(path)\n            img = cv2.resize(img, Config['IMG_SIZE'])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            frames.append(img)\n\n        frames_tr = np.stack(frames, axis=2)\n        #frames_tr = np.asarray(frames_tr,dtype=int)\n        if self.augments:\n            for frame in frames:\n                frame = self.augments(image=frame)['image']\n                frames_tr.append(frame)\n            \n        if self.is_test:\n            return frames_tr,idx\n        else:\n            label = torch.tensor(row['MGMT_value']).float()\n            return frames_tr, label\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def getPaths(self, row):\n        paths = glob.glob(row['path'] + '*.png')\n        sortedPaths = self.sort(paths)\n        maxWindowStart = len(sortedPaths) - Config['MAX_FRAMES']\n        start = 0 # np.random.randint(1, maxWindowStart)\n        paths = sortedPaths[start:Config['MAX_FRAMES']]\n        \n        return paths\n        \n    def sort(self, entry):\n        # https:\/\/stackoverflow.com\/a\/2669120\/7636462\n        convert = lambda text: int(text) if text.isdigit() else text \n        alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n    \n        return sorted(entry, key = alphanum_key)","81e35042":"with open('..\/input\/timm-pretrained-resnet\/index.json','r') as f:\n    index=json.load(f)\n    \nbackstone=Config['FEATURE_EXTRACTOR']\nwhere=index['resnet'][backstone]","3f0d09ac":"\nclass ResNextModel(nn.Module):\n    def __init__(self):\n        super(ResNextModel, self).__init__()\n        self.backbone = timm.create_model(backstone, pretrained=False, in_chans=1)\n        pretrained=f'..\/input\/timm-pretrained-resnet\/resnet\/{where}'\n        state_dict=torch.load(pretrained)\n        conv1_weight = state_dict['conv1.weight']\n        state_dict['conv1.weight'] = conv1_weight.sum(dim=1, keepdim=True)\n        self.backbone.load_state_dict(state_dict)\n    def forward(self, x):\n        return self.backbone(x)\n\nclass Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n    def forward(self, x):\n        return x\n\nclass RSNAModel(nn.Module):\n    def __init__(self, pretrained=True):\n        super(RSNAModel, self).__init__()\n        self.backbone = ResNextModel()\n        num_features = self.backbone.backbone.fc.in_features\n        \n        self.backbone.backbone.fc = Identity()\n        self.dropout= nn.Dropout(Config['DR_RATE'])\n        self.rnn = nn.LSTM(num_features, Config['RNN_HIDDEN_SIZE'], Config['RNN_LAYERS'])\n        self.fc1 = nn.Linear(Config['RNN_HIDDEN_SIZE'], Config['NUM_CLASSES'])\n        \n    def forward(self, x):\n        b_z, fr, h, w = x.shape\n        ii = 0\n        in_pass = x[:, ii].unsqueeze_(1)\n        y = self.backbone((in_pass))\n        output, (hn, cn) = self.rnn(y.unsqueeze(1))\n        for ii in range(1, fr):\n            y = self.backbone((x[:, ii].unsqueeze_(1)))\n            out, (hn, cn) = self.rnn(y.unsqueeze(1), (hn, cn))\n        out = self.dropout(out[:, -1])\n        out = self.fc1(out)\n        out = torch.sigmoid(out)\n        return out","b81c24f3":"def train_one_epoch(model, train_dataloader, optimizer, loss_fn, epoch, device, log_wandb=True, verbose=False):\n    \"\"\"\n    Trains model for one epoch\n    \"\"\"\n    model.train()\n    running_loss = 0\n    prog_bar = tqdm(enumerate(train_dataloader), total=len(train_dataloader))\n    for batch, (frames, targets) in prog_bar:\n        optimizer.zero_grad()\n        \n        frames = frames.to(device, torch.float)\n        targets = targets.to(device, torch.float)\n        #print(frames.shape)\n        # Re arrange the frames in the format our model wants to recieve\n        #frames = einops.rearrange(frames, 'b h w f -> b f h w')\n        frames = frames.permute(0,3,1,2)\n        preds = model(frames).view(-1)\n        del frames\n        gc.collect()\n        loss = loss_fn(preds, targets)\n        \n        loss.backward()\n        optimizer.step()\n        \n        loss_item = loss.item()\n        running_loss += loss_item\n        \n        prog_bar.set_description(f\"loss: {loss_item:.4f}\")\n        \n        if log_wandb == True:\n            wandb_log(\n                batch_train_loss=loss_item\n            )\n        \n        if verbose == True and batch % 20 == 0:\n            print(f\"Batch: {batch}, Loss: {loss_item}\")\n    \n    avg_loss = running_loss \/ len(train_dataloader)\n    \n    return avg_loss\n\n@torch.no_grad()\ndef valid_one_epoch(model, valid_dataloader, loss_fn, epoch, device, log_wandb=True, verbose=False):\n    \"\"\"\n    Validates the model for one epoch\n    \"\"\"\n    model.eval()\n    running_loss = 0\n    prog_bar = tqdm(enumerate(valid_dataloader), total=len(valid_dataloader))\n    \n    for batch, (frames, targets) in prog_bar:\n        frames = frames.to(device, torch.float)\n        targets = targets.to(device, torch.float)\n\n        # Re arrange the frames in the format our model wants to recieve\n        #frames = einops.rearrange(frames, 'b h w f -> b f h w')\n        frames = frames.permute(0,3,1,2)\n        preds = model(frames).view(-1)\n        del frames\n        gc.collect()\n        loss = loss_fn(preds, targets)\n        loss_item = loss.item()\n        running_loss += loss_item\n\n        prog_bar.set_description(f\"val_loss: {loss_item:.4f}\")\n\n        if log_wandb == True:\n            wandb_log(\n                batch_val_loss=loss_item\n            )\n\n        if verbose == True and batch % 10 == 0:\n            print(f\"Batch: {batch}, Loss: {loss_item}\")\n    \n    avg_val_loss = running_loss \/ len(valid_dataloader)\n    \n    return avg_val_loss","86c8706a":"log_wandb = False\nif torch.cuda.is_available():\n    print(\"Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n    device = torch.device('cuda')\nelse:\n    print(\"\\nGPU not found. Using CPU: {}\\n\".format(platform.processor()))\n    device = torch.device('cpu')\n\n\n# Load training csv file\ndf = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/train_labels.csv')\ndf['path'] = df.apply(lambda row: get_path(row), axis=1)\n#df=df[:20]  #close it for submitting\n\n# Removing two patient ids from the dataframe since there are not FLAIR directories for these ids. \ndf = df.loc[df.BraTS21ID!=109]\ndf = df.loc[df.BraTS21ID!=709]\ndf = df.reset_index(drop=True)\n\ntrain_df, valid_df = train_test_split(df, test_size=0.1, stratify=df.MGMT_value.values)\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\nprint(f'Size of Training Set: {len(train_df)}, Validation Set: {len(valid_df)}')","94a874de":"def prepera_data():\n    train_data = RSNADataset(train_df)\n    valid_data = RSNADataset(valid_df)\n\n    train_loader = DataLoader(\n        train_data,\n        batch_size=Config['TRAIN_BS'], \n        shuffle=True,\n        num_workers=Config['NUM_WORKERS']\n    )\n\n    valid_loader = DataLoader(\n        valid_data, \n        batch_size=Config['VALID_BS'], \n        shuffle=False,\n        num_workers=Config['NUM_WORKERS']\n    )\n    return train_loader,valid_loader\n\ntrain_loader,valid_loader=prepera_data()","0b35d58c":"def hunt_model():\n    model = RSNAModel()\n    model = model.to(device)\n    optimizer = torch.optim.AdamW(model.parameters(), lr=Config['LR'])\n\n    train_loss_fn = nn.BCEWithLogitsLoss()\n    valid_loss_fn = nn.BCEWithLogitsLoss()\n\n    print(f\"\\nUsing Backbone: {Config['FEATURE_EXTRACTOR']}\")\n\n    current_loss = 1000\n    for epoch in range(Config['EPOCHS']):\n        print(f\"\\n{'--'*8} EPOCH: {epoch+1} {'--'*8}\\n\")\n\n        train_loss = train_one_epoch(model, train_loader, optimizer, train_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n\n        valid_loss = valid_one_epoch(model, valid_loader, valid_loss_fn, epoch=epoch, device=device, log_wandb=log_wandb)\n\n        print(f\"val_loss: {valid_loss:.4f}\")\n\n        if log_wandb == True:\n            wandb_log(\n                train_loss=train_loss,\n                valid_loss=valid_loss\n            )\n\n        if valid_loss < current_loss:\n            current_loss = valid_loss\n            torch.save(model.state_dict(), f\"model_{Config['FEATURE_EXTRACTOR']}.pt\")\n    return model\n\nmodel=hunt_model()\nmodelfiles=[model]","fe308b6b":"import pydicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nfrom skimage.transform import resize\ndef load_dicom_image(path, img_size, voi_lut=True, rotate=0):\n    dicom = pydicom.read_file(path)\n    data = dicom.pixel_array\n    if voi_lut:\n        data = apply_voi_lut(dicom.pixel_array, dicom)\n    else:\n        data = dicom.pixel_array\n       \n    #data=crop_center(data,100,100)\n    \n    if np.max(data)==0:\n        data=None\n    else:\n        data = cv2.resize(data, img_size)\n    return data\n\n\ndef load_dicom_images_3d(scan_id,img_size, mri_type=\"FLAIR\", rotate=0,split='test'):\n\n    files = sorted(glob.glob(f\"{data_directory}\/{split}\/{scan_id}\/{mri_type}\/*.dcm\"), \n               key=lambda var:[int(x) if x.isdigit() else x for x in re.findall(r'[^0-9]|[0-9]+', var)])\n    '''\n    middle = len(files)\/\/2\n    num_imgs2 = num_imgs\/\/2\n    p1 = max(0, middle - num_imgs2)\n    p2 = min(len(files), middle + num_imgs2)\n    '''\n    #img3d = np.stack([load_dicom_image(f, rotate=rotate) for f in files[p1:p2]]).T \n    img3d=[]\n    #n=min(len(files),Config['MAX_FRAMES'])\n    frame_count=0\n    for f in files:\n \n        temp= load_dicom_image(f,img_size=Config['IMG_SIZE'])\n        if  temp is None:\n            #print('remove empty array')\n            continue\n        else:\n            img3d.append(temp)\n            frame_count+=1\n            if frame_count>=Config['MAX_FRAMES']:\n                break\n    img3d=np.stack(img3d,axis=2)\n    img3d = img3d - np.min(img3d)\n    img3d = img3d \/ np.max(img3d)\n    #img3d=np.asarray(img3d,dtype=np.uint8)\n    #print(f'before:{img3d.shape}')\n    #img3d=min_cube(img3d)\n    #print(f'after:{img3d.shape}')\n    #img3d=np.stack([load_dicom_image(f) for f in files])\n    #if img3d.shape[-1] < num_imgs:\n    #    n_zero = np.zeros((img_size, img_size, num_imgs - img3d.shape[-1]))\n    #    img3d = np.concatenate((img3d,  n_zero), axis = -1)\n        \n    #if np.min(img3d) < np.max(img3d):\n    \n    \n    if rotate > 0 :\n        #rot_choices = [0, cv2.ROTATE_90_CLOCKWISE, cv2.ROTATE_90_COUNTERCLOCKWISE, cv2.ROTATE_180]\n        #data = cv2.rotate(data, rot_choices[rotate])\n        if rotate==1:\n            img3d=img3d+np.random.random(img3d.shape)*0.012\n        elif rotate==2:\n            img3d=gaussian_filter(img3d,sigma=7)\n        elif rotate==3:\n            img3d=img3d*1.05;\n            img3d[img3d>1]=1.\n        elif rotate==4:\n            img3d=laplace(img3d)\n        elif rotate==5:\n            img3d=crop_center(img3d,(150,150,150))\n        else: \n            img3d=img3d*0.99\n    #img3d.shape\n    if frame_count<Config['MAX_FRAMES']:\n        img3d=resize(img3d,(SIZE,SIZE,Config['MAX_FRAMES']))\n        print(scan_id,frame_count)\n    #print(img3d.shape,scan_id,frame_count)\n    return img3d\n    #return np.expand_dims(img3d,0)\n","36335987":"class TestSet(Dataset):\n    def __init__(self, df, mri_type):\n        #self.paths = paths\n        self.df=df\n        self.mri_type = mri_type\n        self.label_smoothing = 0\n        self.augment =False\n          \n    def __len__(self):\n        return len(self.df)\n    '''\n    def old(self):\n        row = self.df.loc[idx]\n        paths = self.getPaths(row)\n        frames = []\n        for path in paths:\n            img = cv2.imread(path)\n            img = cv2.resize(img, Config['IMG_SIZE'])\n            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n            frames.append(img)\n\n        frames_tr = np.stack(frames, axis=2)\n        #frames_tr = np.asarray(frames_tr,dtype=int)\n        if self.augments:\n            for frame in frames:\n                frame = self.augments(image=frame)['image']\n                frames_tr.append(frame)\n            \n        if self.is_test:\n            return frames_tr,idx\n        else:\n            label = torch.tensor(row['MGMT_value']).float()\n            return frames_tr, label\n    '''  \n    \n    def __getitem__(self, idx):\n        row = self.df.loc[idx]\n        scan_id=row['BraTS21ID']\n        frames = load_dicom_images_3d(str(scan_id).zfill(5), img_size=Config['IMG_SIZE'],mri_type=self.mri_type)\n        #print(scan_id,frames.shape)\n        return frames, idx","97a92c85":"def min_cube(array_3d):\n    brain=np.nonzero(array_3d)\n    up=np.max(brain,axis=1)+1\n    down=np.min(brain,axis=1) \n    solid=array_3d[down[0]:up[0],down[1]:up[1],down[2]:up[2]]    \n    return solid","71fe8666":"def predict(model, df, mri_type, split):\n    print(\"Predict:\",mri_type, df.shape)\n    df.loc[:,\"MRI_Type\"] = mri_type\n    #display(df)\n    #test_data = RSNADataset(df,is_test=True)\n    test_data=TestSet(\n        df, mri_type\n    )\n    test_loader = DataLoader(\n        test_data, \n        batch_size=4,\n        shuffle=False,\n        num_workers=1\n    )\n    model.to(device)\n    #print(test_loader[0])\n    #checkpoint = torch.load(modelfile)\n    #model.load_state_dict(checkpoint[\"model_state_dict\"])\n    model.eval()\n    #model.train()\n    \n    y_pred = []\n    ids = []\n\n    for e, batch in enumerate(test_loader,1):\n        print(f\"{e}\/{len(test_loader)}\", end=\"\\r\")\n        with torch.no_grad():\n            frames,ids_=batch\n            #print(frames.shape)\n            frames = frames.permute(0,3,1,2)\n            frames = frames.to(device, torch.float)\n            tmp_pred = model(frames).view(-1)\n            #print(tmp_pred)\n            if tmp_pred.size == 1:\n                y_pred.append(tmp_pred.item())\n            else:\n                y_pred.extend(tmp_pred.tolist())\n            ids.extend(ids_.numpy().tolist())\n            \n    preddf = pd.DataFrame({\"BraTS21ID\": ids, \"MGMT_value\": y_pred}) \n    preddf = preddf.set_index(\"BraTS21ID\")\n    return preddf","2b551ec0":"df = pd.read_csv('..\/input\/rsna-miccai-brain-tumor-radiogenomic-classification\/sample_submission.csv')\nsubmission=df.copy(deep=True)","2c26a983":"submission['path'] = submission.apply(lambda row: get_path(row,is_test=True), axis=1)\n\nsubmission[\"MGMT_value\"] = 0\nfor m, mtype in zip(modelfiles, mri_types):\n    pred = predict(m, submission, mtype, split=\"test\")\n    submission[\"MGMT_value\"] += pred[\"MGMT_value\"]","94c8f710":"submission[\"MGMT_value\"] \/= len(modelfiles)\n#submission.drop(columns=['path','MRI_Type'],inplace=True)","a5d87b22":"df['MGMT_value']=submission[\"MGMT_value\"]","6c100fc4":"#submission=submission.set_index('BraTS21ID')\ndf.to_csv(\"submission.csv\",index=False)\ndf","e1bd5eef":"submission[\"MGMT_value\"].hist()","9ddcb80e":"submission","a6902d4b":"Copy from [[PyTorch Train] RSNA Video Classification + W&B \ud83d\ude80](https:\/\/www.kaggle.com\/heyytanay\/pytorch-train-rsna-video-classification-w-b) and edit for running without Internet.","8210805c":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83c\udff4\u200d\u2620\ufe0f Training and Validation Functions<\/h2>\n<\/div>","5a5116c3":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83c\udfd7 Training and Validating the Model<\/h2>\n<\/div>","67fc6c57":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcbb Custom Dataset Class<\/h2>\n<\/div>\n\n<div class=\"alert alert-block alert-info\" style=\"font-size:14px; font-family:verdana; line-height: 1.7em;\">\n    \ud83d\udccc In this custom Dataset, I am essentially reading \"MAX_FRAMES\" number of images from a patient's FLAIR folder and making list of those frames and converting it to torch tensor.\n<\/div>","0cf76cc2":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcd4 Imports and Installation<\/h2>\n<\/div>","6ac3b666":"<div class=\"alert alert-success\">\n    <h2 align='center'>\u26fd Utility Functions <\/h2>\n<\/div>","14b8fb69":"[](https:\/\/www.kaggle.com\/heyytanay\/pytorch-train-rsna-video-classification-w-b)","8d36a4ec":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\udcc8 Model Class with ResNext Backbone<\/h2>\n<\/div>","e0a7d58b":"<div class=\"alert alert-success\">\n    <h2 align='center'>\ud83d\ude80 Config Dictionary and W&B Integration <\/h2>\n<\/div>"}}