{"cell_type":{"96c605f8":"code","0e801361":"code","93925226":"code","e761ef46":"code","4bf92c7d":"code","07ff0f05":"code","70a36ad9":"code","90ef2cd3":"code","914c8c00":"code","4e4ccc46":"code","fd4db4c1":"code","3f7abb18":"code","b50b4f40":"code","32459d69":"code","edc4011d":"code","d00d822b":"code","6b0bbda9":"code","de630258":"code","488097d6":"code","5e0e879c":"code","fafe769c":"code","b715dc8b":"code","3792a4ad":"code","976611a4":"code","fcde497c":"code","4ff66911":"code","77b9df92":"code","175edf0f":"code","c315df5a":"code","fd565a4c":"code","f8c5497c":"code","0cc408ca":"code","672f2800":"code","68d9ff81":"code","ce559c1e":"code","5da248dd":"code","b2df3866":"code","3de0bd36":"code","03576ff1":"code","a26c2847":"code","6f6e923a":"code","8d7086a7":"code","36363eaa":"code","709a3d1d":"code","d479f942":"code","e3a42a30":"code","70d65804":"code","fc45518c":"code","db04009e":"code","7dd3baf5":"code","5684f323":"code","d1b2ebb8":"code","2002224a":"code","375c35d1":"code","3f865099":"code","8be11dad":"code","7a29f31b":"code","780c2d4a":"code","85d3e7b5":"code","73e722b6":"code","8998c0d1":"code","ad2cce31":"code","12971352":"code","e50882bb":"code","1c8662f1":"code","d02c3bb4":"code","bf078659":"code","a86d0062":"code","0c62bc6e":"code","341c7646":"code","a7891972":"code","9f6cf82a":"code","ccd83b45":"code","92e07642":"code","d03361d5":"code","c798d7c9":"code","75f38a86":"code","a18b502c":"markdown","4164dde9":"markdown","2573f6ed":"markdown","81bda015":"markdown","c0d81115":"markdown","0a05a1b8":"markdown","17a2852b":"markdown","af7f2ed7":"markdown","5e4ee2cc":"markdown","dc690e4b":"markdown","ee92f371":"markdown","5bac9f93":"markdown","de548c88":"markdown","6835be8e":"markdown","0af678fd":"markdown","79b7a115":"markdown","4a6608f4":"markdown","7fb8af7c":"markdown","9a48ae67":"markdown","319c0f6f":"markdown","edf84763":"markdown","9d7bd5a0":"markdown","c92fb693":"markdown","a56e1129":"markdown","7f093a8b":"markdown","74a57fa0":"markdown","0edcd2b9":"markdown","55ec873c":"markdown","63d3638a":"markdown","37dc50cc":"markdown","c4226a13":"markdown"},"source":{"96c605f8":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport random\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nimport imageio\nfrom IPython.display import Image\nimport matplotlib.image as mpimg\n#MUSIC PROCESS\nimport pydub\nfrom scipy.io.wavfile import read, write\nimport librosa\nimport librosa.display\nimport IPython\nfrom IPython.display import Audio\nimport scipy\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape,\\\nConv2DTranspose, LeakyReLU, Conv1D, AveragePooling1D, MaxPooling1D\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\n#SKLEARN CLASSIFIER\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom lightgbm import LGBMClassifier, LGBMRegressor\nfrom catboost import CatBoostClassifier, CatBoostRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingClassifier, GradientBoostingRegressor\nfrom sklearn.ensemble import BaggingRegressor\nfrom sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\nfrom sklearn.neural_network import MLPClassifier, MLPRegressor\nfrom sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.cross_decomposition import PLSRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LassoCV\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.linear_model import ElasticNetCV\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","0e801361":"Main_WAV_Path = Path(\"..\/input\/toronto-emotional-speech-set-tess\/TESS Toronto emotional speech set data\")","93925226":"Wav_Path = list(Main_WAV_Path.glob(r\"**\/*.wav\"))","e761ef46":"Wav_Labels = list(map(lambda x: os.path.split(os.path.split(x)[0])[1],Wav_Path))","4bf92c7d":"Wav_Path_Series = pd.Series(Wav_Path,name=\"WAV\").astype(str)\nWav_Labels_Series = pd.Series(Wav_Labels,name=\"EMOTION\")","07ff0f05":"Main_Wav_Data = pd.concat([Wav_Path_Series,Wav_Labels_Series],axis=1)","70a36ad9":"print(Main_Wav_Data.head(-1))","90ef2cd3":"print(Main_Wav_Data[\"EMOTION\"].value_counts())","914c8c00":"Main_Wav_Data = Main_Wav_Data.sample(frac=1).reset_index(drop=True)","4e4ccc46":"print(Main_Wav_Data.head(-1))","fd4db4c1":"def add_noise(data):\n    noise_value = 0.015 * np.random.uniform() * np.amax(data)\n    data = data + noise_value * np.random.normal(size=data.shape[0])\n    \n    return data","3f7abb18":"def stretch_process(data,rate=0.8):\n    \n    return librosa.effects.time_stretch(data,rate)","b50b4f40":"def shift_process(data):\n    shift_range = int(np.random.uniform(low=-5,high=5) * 1000)\n    \n    return np.roll(data,shift_range)","32459d69":"def pitch_process(data,sampling_rate,pitch_factor=0.7):\n    \n    return librosa.effects.pitch_shift(data,sampling_rate,pitch_factor)","edc4011d":"def extract_process(data):\n    \n    output_result = np.array([])\n    mean_zero = np.mean(librosa.feature.zero_crossing_rate(y=data).T,axis=0)\n    output_result = np.hstack((output_result,mean_zero))\n    \n    stft_out = np.abs(librosa.stft(data))\n    chroma_stft = np.mean(librosa.feature.chroma_stft(S=stft_out,sr=sample_rate).T,axis=0)\n    output_result = np.hstack((output_result,chroma_stft))\n    \n    mfcc_out = np.mean(librosa.feature.mfcc(y=data,sr=sample_rate).T,axis=0)\n    output_result = np.hstack((output_result,mfcc_out))\n    \n    root_mean_out = np.mean(librosa.feature.rms(y=data).T,axis=0)\n    output_result = np.hstack((output_result,root_mean_out))\n    \n    mel_spectogram = np.mean(librosa.feature.melspectrogram(y=data,sr=sample_rate).T,axis=0)\n    output_result = np.hstack((output_result,mel_spectogram))\n    \n    return output_result","d00d822b":"def export_process(path):\n    \n    data,sample_rate = librosa.load(path,duration=2.5,offset=0.6)\n    \n    output_1 = extract_process(data)\n    result = np.array(output_1)\n    \n    noise_out = add_noise(data)\n    output_2 = extract_process(noise_out)\n    result = np.vstack((result,output_2))\n    \n    new_out = stretch_process(data)\n    strectch_pitch = pitch_process(new_out,sample_rate)\n    output_3 = extract_process(strectch_pitch)\n    result = np.vstack((result,output_3))\n    \n    return result","6b0bbda9":"rate,speech = read(Main_Wav_Data[\"WAV\"][2342])\nprint(Main_Wav_Data[\"EMOTION\"][2342])\n\nAudio(speech,rate=rate,autoplay=False)","de630258":"rate,speech = read(Main_Wav_Data[\"WAV\"][3])\nprint(Main_Wav_Data[\"EMOTION\"][3])\n\nAudio(speech,rate=rate,autoplay=False)","488097d6":"rate,speech = read(Main_Wav_Data[\"WAV\"][2795])\nprint(Main_Wav_Data[\"EMOTION\"][2795])\n\nAudio(speech,rate=rate,autoplay=False)","5e0e879c":"rate,speech = read(Main_Wav_Data[\"WAV\"][2795])\nprint(Main_Wav_Data[\"EMOTION\"][2795])\nprint(speech.shape)\nprint(speech.dtype)\nprint(rate)","fafe769c":"rate,speech = read(Main_Wav_Data[\"WAV\"][314])\nprint(Main_Wav_Data[\"EMOTION\"][314])\nprint(speech.shape)\nprint(speech.dtype)\nprint(rate)","b715dc8b":"rate,speech = read(Main_Wav_Data[\"WAV\"][134])\nprint(Main_Wav_Data[\"EMOTION\"][134])\nprint(speech.shape)\nprint(speech.dtype)\nprint(rate)","3792a4ad":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,rate = librosa.load(Main_Wav_Data[\"WAV\"][134])\nlibrosa.display.waveplot(audio_speech,sr=rate)\nAudio(audio_speech,rate=rate)","976611a4":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,rate = librosa.load(Main_Wav_Data[\"WAV\"][34])\nlibrosa.display.waveplot(audio_speech,sr=rate)\nAudio(audio_speech,rate=rate)","fcde497c":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,rate = librosa.load(Main_Wav_Data[\"WAV\"][4])\nlibrosa.display.waveplot(audio_speech,sr=rate)\nAudio(audio_speech,rate=rate)","4ff66911":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,rate = librosa.load(Main_Wav_Data[\"WAV\"][458])\n\nstft_audio = librosa.stft(audio_speech)\nDb_audio = librosa.amplitude_to_db(abs(stft_audio))\nlibrosa.display.specshow(Db_audio,sr=rate,x_axis=\"time\",y_axis=\"hz\")\nAudio(audio_speech,rate=rate)","77b9df92":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,rate = librosa.load(Main_Wav_Data[\"WAV\"][4])\n\nstft_audio = librosa.stft(audio_speech)\nDb_audio = librosa.amplitude_to_db(abs(stft_audio))\nlibrosa.display.specshow(Db_audio,sr=rate,x_axis=\"time\",y_axis=\"hz\")\nAudio(audio_speech,rate=rate)","175edf0f":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000])\n\nstft_audio = librosa.stft(audio_speech)\nDb_audio = librosa.amplitude_to_db(abs(stft_audio))\nlibrosa.display.specshow(Db_audio,sr=rate,x_axis=\"time\",y_axis=\"hz\")\nAudio(audio_speech,rate=rate)","c315df5a":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000])\n\nnoise_injection = add_noise(audio_speech)\n\nlibrosa.display.waveplot(noise_injection,sr=sample_rate)\nAudio(noise_injection,rate=sample_rate)","fd565a4c":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000])\n\nstretching_audio = stretch_process(audio_speech)\nlibrosa.display.waveplot(stretching_audio,sr=sample_rate)\nAudio(stretching_audio,rate=sample_rate)","f8c5497c":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000])\n\nshifting_audio = shift_process(audio_speech)\nlibrosa.display.waveplot(shifting_audio,sr=sample_rate)\nAudio(shifting_audio,rate=sample_rate)","0cc408ca":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000])\n\npitch_audio = pitch_process(audio_speech,sample_rate)\nlibrosa.display.waveplot(pitch_audio,sr=sample_rate)\nAudio(pitch_audio,rate=sample_rate)","672f2800":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][2000],duration=2.5,offset=0.4)\nlibrosa.display.waveplot(audio_speech,sr=sample_rate)\nprint(audio_speech.shape)\nAudio(audio_speech,rate=sample_rate)","68d9ff81":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][3],duration=2.5,offset=0.6)\nlibrosa.display.waveplot(audio_speech,sr=sample_rate)\nprint(audio_speech.shape)\nAudio(audio_speech,rate=sample_rate)","ce559c1e":"figure = plt.figure(figsize=(14,5))\n\naudio_speech,sample_rate = librosa.load(Main_Wav_Data[\"WAV\"][1398],duration=2.5,offset=0.6)\nlibrosa.display.waveplot(audio_speech,sr=sample_rate)\nprint(audio_speech.shape)\nAudio(audio_speech,rate=sample_rate)","5da248dd":"x_Train, y_Train = [],[]\n\nfor path,emotion in zip(Main_Wav_Data.WAV,Main_Wav_Data.EMOTION):\n    \n    features = export_process(path)\n    \n    for element in features:\n        x_Train.append(element)\n        y_Train.append(emotion)","b2df3866":"print(len(x_Train))\nprint(len(y_Train))\nprint(len(Main_Wav_Data.WAV))","3de0bd36":"print(x_Train[0].shape)","03576ff1":"print(y_Train[0:5])","a26c2847":"New_Features_Wav = pd.DataFrame(x_Train)\nNew_Features_Wav[\"EMOTIONS\"] = y_Train\n\nNew_Features_Wav.to_csv(\"New_Wav_Set.csv\",index=False)","6f6e923a":"New_Features_Wav.head(-1)","8d7086a7":"print(New_Features_Wav[\"EMOTIONS\"].value_counts())","36363eaa":"encoder_label = OneHotEncoder()","709a3d1d":"scaler_data = StandardScaler()","d479f942":"X = New_Features_Wav.iloc[:,:-1].values\nY = New_Features_Wav[\"EMOTIONS\"].values","e3a42a30":"print(X.shape)\nprint(Y.shape)","70d65804":"Y = encoder_label.fit_transform(np.array(Y).reshape(-1,1)).toarray()","fc45518c":"print(Y.shape)","db04009e":"xTrain,xTest,yTrain,yTest = train_test_split(X,Y,train_size=0.9,random_state=42,shuffle=True)","7dd3baf5":"print(xTrain.shape)\nprint(yTrain.shape)\nprint(xTest.shape)\nprint(yTest.shape)","5684f323":"xTrain = scaler_data.fit_transform(xTrain)\nxTest = scaler_data.transform(xTest)","d1b2ebb8":"print(xTrain.shape)\nprint(xTest.shape)","2002224a":"xTrain = np.expand_dims(xTrain,axis=2)\nxTest = np.expand_dims(xTest,axis=2)","375c35d1":"print(xTrain.shape)\nprint(xTest.shape)","3f865099":"Model=Sequential()\nModel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(xTrain.shape[1], 1)))\nModel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nModel.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\nModel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nModel.add(Conv1D(128, kernel_size=5, strides=1, padding='same', activation='relu'))\nModel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\nModel.add(Dropout(0.2))\n\nModel.add(Conv1D(64, kernel_size=5, strides=1, padding='same', activation='relu'))\nModel.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n\nModel.add(Flatten())\nModel.add(Dense(units=32, activation='relu'))\nModel.add(Dropout(0.3))\n\nModel.add(Dense(units=14, activation='softmax'))","8be11dad":"Model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])","7a29f31b":"early_stop = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")","780c2d4a":"Conv1D_Model = Model.fit(xTrain, yTrain, batch_size=64, epochs=50, validation_data=(xTest, yTest), callbacks=[early_stop])","85d3e7b5":"Grap_Data = pd.DataFrame(Conv1D_Model.history)\nfigure = plt.figure(figsize=(10,10))\nGrap_Data.plot()","73e722b6":"plt.plot(Conv1D_Model.history[\"accuracy\"])\nplt.plot(Conv1D_Model.history[\"val_accuracy\"])\nplt.ylabel(\"ACCURACY\")\nplt.legend()\nplt.show()","8998c0d1":"plt.plot(Conv1D_Model.history[\"loss\"])\nplt.plot(Conv1D_Model.history[\"val_loss\"])\nplt.ylabel(\"LOSS\")\nplt.legend()\nplt.show()","ad2cce31":"Model_Results = Model.evaluate(xTest,yTest)\nprint(\"LOSS:  \" + \"%.4f\" % Model_Results[0])\nprint(\"ACCURACY:  \" + \"%.4f\" % Model_Results[1])","12971352":"prediction_test = Model.predict(xTest)\ny_prediction = encoder_label.inverse_transform(prediction_test)\n\nyTest = encoder_label.inverse_transform(yTest)","e50882bb":"print(prediction_test[0:10])","1c8662f1":"print(y_prediction[0:10])","d02c3bb4":"print(yTest[0:10])","bf078659":"conf_matrix = confusion_matrix(yTest, y_prediction)","a86d0062":"sns.heatmap(conf_matrix, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n\nplt.title('Confusion Matrix', size=20)\nplt.xlabel('Predicted Labels', size=14)\nplt.ylabel('Actual Labels', size=14)\nplt.show()","0c62bc6e":"print(classification_report(yTest, y_prediction))","341c7646":"print(accuracy_score(yTest, y_prediction))","a7891972":"new_predict_list = []\nfeat_new = export_process(\"..\/input\/audio-speech-sentiment\/TRAIN\/10.wav\")\n\nfor feat in feat_new:\n    new_predict_list.append(feat)","9f6cf82a":"print(new_predict_list[0].shape)","ccd83b45":"New_Predict_Feat = pd.DataFrame(new_predict_list)","92e07642":"New_Predict_Feat = scaler_data.fit_transform(New_Predict_Feat)\nNew_Predict_Feat = np.expand_dims(New_Predict_Feat,axis=2)","d03361d5":"print(New_Predict_Feat.shape)","c798d7c9":"prediction_nonseen = Model.predict(New_Predict_Feat)\narg_prediction_nonseen = prediction_nonseen.argmax(axis=-1)\ny_prediction_nonseen = encoder_label.inverse_transform(prediction_nonseen)\n","75f38a86":"print(arg_prediction_nonseen)\nprint(y_prediction_nonseen)","a18b502c":"#### NOISE","4164dde9":"#### SPECIAL PREDICTION","2573f6ed":"#### SPECSHOW","81bda015":"#### HEARING","c0d81115":"#### SAME TIMEFRAME PERIOD","0a05a1b8":"> # DATA PROCESS FUNCTIONS","17a2852b":"#### SHIFTING","af7f2ed7":"#### STRETCHING","5e4ee2cc":"#### STRETCH","dc690e4b":"> # ANALYSIS","ee92f371":"#### PITCH","5bac9f93":"> # DATA PROCESS AND ENGINEERING","de548c88":"#### PITCH","6835be8e":"#### TO SERIES","0af678fd":"#### SHAPE - TYPE","79b7a115":"> # PACKAGES AND LIBRARIES","4a6608f4":"#### SPLITTING","7fb8af7c":"#### TRANSFORMATION AND EXPORTATION","9a48ae67":"#### TO SHUFFLE","319c0f6f":"#### TO DATAFRAME","edf84763":"> # PATH, LABEL, TRANSFORMATION","9d7bd5a0":"#### WAV PATH","c92fb693":"#### PREDICTION","a56e1129":"#### EXTRACT FEATURES","7f093a8b":"#### WAVEPLOT","74a57fa0":"#### WAV LABELS","0edcd2b9":"#### EXPORT FEATURES","55ec873c":"#### MAIN","63d3638a":"#### NOISE","37dc50cc":"#### SHIFT","c4226a13":"# MODEL STRUCTURE"}}