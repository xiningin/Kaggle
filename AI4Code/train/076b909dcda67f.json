{"cell_type":{"183ee46e":"code","ab45d605":"code","90f41650":"code","b1ea0b73":"code","4324e234":"code","4e00cbf2":"code","d25ed115":"code","1fa31df5":"code","a8155fa5":"code","e0e50b27":"code","e5974cd0":"code","5b97cffa":"code","7fff25c1":"code","fd4bbc5d":"code","12a9ee62":"code","79401ff6":"code","6f3ccc6b":"code","acb18089":"code","5371d7ea":"code","b64ce94b":"code","865b0234":"code","048a607c":"code","9831d6b3":"code","4f5a9cc7":"code","3f9afd29":"code","e03d1899":"code","2a697631":"code","e3b31971":"code","5554d28b":"markdown","6d801681":"markdown","1d38c65d":"markdown","7789de60":"markdown","1de7d4f2":"markdown","5e319e3e":"markdown","da4dc99d":"markdown","e7c91e89":"markdown","cdab263f":"markdown","f1cfd4f1":"markdown","5feaaf1d":"markdown","c64730bc":"markdown","55045226":"markdown","2d550412":"markdown"},"source":{"183ee46e":"!pip install nltk\n!pip install keras\n!pip install gensim\n!pip install seaborn\n\nimport nltk\nnltk.download('brown')\nnltk.download('treebank')\nnltk.download('conll2000')\nnltk.download('punkt')\nnltk.download('universal_tagset')","ab45d605":"# import libraries\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport re\nimport numpy as np\nimport nltk\nfrom nltk.tokenize import word_tokenize\nimport requests\nfrom nltk.tokenize import word_tokenize\n\nfrom gensim.models import KeyedVectors\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.vis_utils import plot_model\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Embedding","90f41650":"# download ebook\nurl = \"https:\/\/www.gutenberg.org\/files\/24869\/24869-0.txt\"\nbook = requests.get(url)\ndata = book.text","b1ea0b73":"# let's look at the text\nprint(data[:500])","4324e234":"# subset the book from the first chapter, that ism INVOCATION - everything before first chapter is irrelevant data\nstart_index = re.search(\"invocation.\\(1\\)\", data, re.I)\nprint(start_index.start())","4e00cbf2":"# Let's see how does the text look like\ndata = data[start_index.start():]","d25ed115":"# let's look at the text\nprint(data[:500])","1fa31df5":"# define a function to clean text data\ndef clean_document(document, char_filter = r\"[^\\w]\"):\n    '''\n    input:\n    document          :  string\n    char_filter       :  regex pattern - removes those characters from the text that match the pattern\n\n    output: clean document\n    '''\n    \n    # convert words to lower case\n    document = document.lower()\n\n    # tokenise words\n    words = word_tokenize(document)\n\n    # strip whitespace from all words\n    words = [word.strip() for word in words]\n\n    # join back words to get document\n    document = \" \".join(words)\n\n    # remove unwanted characters\n    document = re.sub(char_filter, \" \", document)\n\n    # replace multiple whitespaces with single whitespace\n    document = re.sub(r\"\\s+\", \" \", document)\n\n    # strip whitespace from document\n    document = document.strip()\n\n    return document\n\ndata = clean_document(data)","a8155fa5":"# length of text\nwords = word_tokenize(data)\nprint(\"Number of words in document: {}\".format(len(words)))","e0e50b27":"# use Keras' Tokenizer() function to encode text to integers\nword_tokeniser = Tokenizer()\nword_tokeniser.fit_on_texts([data])\nencoded_words = word_tokeniser.texts_to_sequences([data])[0]","e5974cd0":"# check the size of the vocabulary\nVOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\nprint('Vocabulary Size: {}'.format(VOCABULARY_SIZE))","5b97cffa":"sequences = []\nMAX_SEQ_LENGTH = 5  # X will have five words, y will have the sixth word\n\nfor i in range(MAX_SEQ_LENGTH, len(encoded_words)):\n    sequence = encoded_words[i-MAX_SEQ_LENGTH:i+1]\n    sequences.append(sequence)\nsequences = np.array(sequences)","7fff25c1":"print('Total number of training samples: {}'.format(len(sequences)))\nprint('\\nSample sequences: \\n{}'.format(sequences[0:3]))","fd4bbc5d":"# divide the sequence into X and y\nsequences = np.array(sequences)\n\nX = sequences[:80000,:-1]  # assign all but last words of a sequence to X\ny = sequences[:80000,-1]   # assign last word of each sequence to y","12a9ee62":"# Look at the first training example\nprint(\"Input of the first data point:\", X[0], \"\\n\")\nprint(\"Output of the first data point: [\", y[0], \"]\")","79401ff6":"y.shape","6f3ccc6b":"y = to_categorical(y, num_classes=VOCABULARY_SIZE)","acb18089":"print(X.shape)\nprint(y.shape)","5371d7ea":"X = pad_sequences(X, maxlen=MAX_SEQ_LENGTH, padding='pre')\nprint('Input sequence length: {}'.format(MAX_SEQ_LENGTH))","b64ce94b":"# word2vec download link (Size ~ 1.5GB): https:\/\/drive.google.com\/file\/d\/0B7XkCwpI5KDYNlNUTTlSS21pQmM\/edit\n\npath = '..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin'\n\n# load word2vec using the following function present in the gensim library\nword2vec = KeyedVectors.load_word2vec_format(path, binary=True)","865b0234":"# assign word vectors from word2vec model\n\nEMBEDDING_SIZE  = 300  # each word in word2vec model is represented using a 300 dimensional vector\nVOCABULARY_SIZE = len(word_tokeniser.word_index) + 1\n\n# create an empty embedding matix\nembedding_weights = np.zeros((VOCABULARY_SIZE, EMBEDDING_SIZE))\n\n# create a word to index dictionary mapping\nword2id = word_tokeniser.word_index\n\n# copy vectors from word2vec model to the words present in corpus\nfor word, index in word2id.items():\n    try:\n        embedding_weights[index, :] = word2vec[word]\n    except KeyError:\n        pass","048a607c":"# check embedding dimension\nprint(\"Embeddings shape: {}\".format(embedding_weights.shape))","9831d6b3":"# create model architecture\n\nmodel_wv = Sequential()\n\n# embedding layer\nmodel_wv.add(Embedding(VOCABULARY_SIZE, EMBEDDING_SIZE, input_length = MAX_SEQ_LENGTH,\n                    weights = [embedding_weights], trainable=True))\n\n# lstm layer 1\nmodel_wv.add(LSTM(128, return_sequences=True))\n\n# lstm layer 2\n# when using multiple LSTM layers, set return_sequences to True at the previous layer\n# because the current layer expects a sequential intput rather than a single input\nmodel_wv.add(LSTM(128))\n\n# output layer\nmodel_wv.add(Dense(VOCABULARY_SIZE, activation='softmax'))","4f5a9cc7":"# compile network\nmodel_wv.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n  \n# summarize defined model\nmodel_wv.summary()","3f9afd29":"# fit network\nmodel_wv.fit(X, y, epochs=75, verbose=1, batch_size=256)","e03d1899":"# generate a sequence from a language model\ndef generate_words(model, word_tokeniser, MAX_SEQ_LENGTH, seed, n_words):\n    \n    text = seed\n    \n    # generate n_words\n    for _ in range(n_words):\n        \n        # encode text as integers\n        encoded_words = word_tokeniser.texts_to_sequences([text])[0]\n        \n        # pad sequences\n        padded_words = pad_sequences([encoded_words], maxlen=MAX_SEQ_LENGTH, padding='pre')\n        \n        # predict next word\n        prediction = model.predict_classes(padded_words, verbose=0)\n        \n        # convert predicted index to its word\n        next_word = \"\"\n        for word, i in word_tokeniser.word_index.items():\n            if i == prediction:\n                next_word = word\n                break\n        \n        # append predicted word to text\n        text += \" \" + next_word\n        \n    return text","2a697631":"# text generation using second model - model with word embeddings\nseed_text = \"rama never told anyone about\"\nnum_words = 100\nprint(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))","e3b31971":"# text generation using second model - model with word embeddings\nseed_text = \"how are you doing\"\nnum_words = 100\nprint(generate_words(model_wv, word_tokeniser, MAX_SEQ_LENGTH, seed_text, num_words))","5554d28b":"# Text generation using RNN - Word Level\n\nTo generate text using RNN, we need a to convert raw text to a supervised learning problem format.\n\nTake, for example, the following corpus:\n\n\"Her brother shook his head incredulously. He was not aware of the situation at all.\"\n\nFirst we need to divide the data into tabular format containing input (X) and output (y) sequences. In case of a character level model, the X and y will look like this:\n\n|      X                |  Y      |\n|-----------------------|---------|\n|    < word1 >< word2 > | < word3 > |\n|    Her brother        |  shook  |\n|    brother shook      |  his    |\n|    shook his          |  head   |\n|    his head           | incredulously |\n|    head incredulously |    .    |\n|    ..                 |    .    |\n|    situation at       |  all    |\n|    at all             |    .    |\n\nNote that in the above problem, the sequence length of **X is two words** and that of **y is one word**. Hence, this is a many-to-one architecture. We can, however, change the number of input words to any number depending on the problem.\n\nA model is trained on such data. To generate text, we simply give the model any two words using which it predicts the next word. Then it appends the predicted word to the input sequence (to the extreme right of the sequence) and discards the first word (word on extreme left of the sequence). Then it predicts again using the new sequence and the cycle continues until a fix number of iterations. An example is shown below:\n\nSeed text: \"Did I\"\n\n|      X                                            |  Y                       |\n|---------------------------------------------------|--------------------------|\n|                        Did I                      |    < predicted word 1 >  |\n|               I < predicted word 1 >              |    < predicted word 2 >  |\n|       < predicted word 1 > < predicted word 2 >   |    < predicted word 3 >  |\n|       < predicted word 2 > < predicted word 3 >   |    < predicted word 4 >  |\n|                      ...                          |            ...           | ","6d801681":"### Pad sequences","1d38c65d":"## Clean text","7789de60":"# Preprocess data","1de7d4f2":"### Create sequences\n\nIn each training sample, X will have a sequence of 5 words and y will have the sixth word. In other words, this means that use previous five words of a sequence to predict next word.","5e319e3e":"There are 120000 sequences (data points) in total.\n\nRemember that to use an RNN data has to be of the shape (#samples, #timesteps, #features)\n\nIn X, the third dimension, that is, number of features is missing because we're going to use the Keras' Embedding Layer. Hence we don't need to explicitly reshape the data to incorporate the third dimension. That will be done automatically by Keras.\n\nIn y, the second dimension is missing, that is, the number of timesteps because y is not a sequence, it's just a single word. The number of features are represented by a one-hot encoded vector whose length is the VOCABULARY_SIZE.","da4dc99d":"### One-hot encode y","e7c91e89":"## Divide data in X and y","cdab263f":"### Let's look at some text generations","f1cfd4f1":"### Load word embeddings to represent the input words","5feaaf1d":"# Generate text","c64730bc":"## Convert characters to integers","55045226":"Previous :\n\nPart1 : https:\/\/www.kaggle.com\/gauravduttakiit\/text-generation-using-rnn-part-1","2d550412":"# Notebook Overview\n1. Preprocess data\n2. Build LSTM model\n3. Generate text"}}