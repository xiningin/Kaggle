{"cell_type":{"980a5606":"code","4de5c644":"code","6f809eb9":"code","e674e73a":"code","2014a63d":"code","2195a650":"code","7ce8c50a":"code","ca555b7e":"code","46f11468":"code","9521722f":"code","dab45fed":"code","787dc97b":"code","0b1b7220":"code","cbe6e520":"code","c9055ec1":"code","ef8c71bc":"code","210f1819":"code","a7c90d9d":"code","62cf6aae":"code","430813b9":"code","57ae707c":"code","1a934e14":"code","9f39cd15":"code","af8300a6":"code","6c253a9d":"code","752f5408":"code","f8cfca35":"code","fc7398ab":"code","fd2a520d":"markdown","06e24f85":"markdown","bad542dc":"markdown","8bfff099":"markdown","bfef4862":"markdown","2acb727c":"markdown","655dd16d":"markdown","aec79c35":"markdown","61f9746a":"markdown","5bf6b104":"markdown","d6ba7339":"markdown","e60eca26":"markdown","830b7d27":"markdown","d4aff77b":"markdown"},"source":{"980a5606":"#importing libraries\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","4de5c644":"#reading all the files\ntrain=pd.read_csv(\"\/kaggle\/input\/fake-news\/train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/fake-news\/test.csv\")\nsub=pd.read_csv(\"\/kaggle\/input\/fake-news\/submit.csv\")","6f809eb9":"train.head(5)","e674e73a":"train.shape","2014a63d":"train.isna().sum()","2195a650":"train=train.dropna() #dropping null values","7ce8c50a":"train.reset_index(inplace=True) #resetting the index","ca555b7e":"import matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nplt.figure(figsize=(10,8))\nsns.countplot(x='label',data=train)","46f11468":"import tensorflow as tf\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional\n","9521722f":"x=train.drop('label',axis=1)\ny=train.label\ny.value_counts()","dab45fed":"#vocab size \nvocab_size = 5000\n \n#one hot representation\n\nmessages = x.copy()\n\nmessages.reset_index(inplace=True)","787dc97b":"import nltk\nimport re\nfrom nltk.corpus import stopwords\nnltk.download('stopwords')","0b1b7220":"#data set preprocessing \n\nfrom nltk.stem.porter import PorterStemmer\nps=PorterStemmer()\ncorpus=[]\nfor i in range(0,len(messages)):\n    result = re.sub('[^a-zA-Z]',' ',messages['title'][i])\n    result = result.lower()\n    result = result.split()\n    result = [ps.stem(word) for word in result if not word in stopwords.words('english')]\n    result = ' '.join(result)\n    corpus.append(result)","cbe6e520":"#Convert it into one hot vectors\n\nonehot_rep = [one_hot(words,vocab_size) for words in corpus]\nonehot_rep\n","c9055ec1":"#set a maximum length for sentences\nsmax_length=20\n#embedded representation\nembedd = pad_sequences(onehot_rep,padding='pre',maxlen=smax_length)","ef8c71bc":"dims=40\nbi_model=Sequential()\nbi_model.add(Embedding(vocab_size,dims,input_length=smax_length))\nbi_model.add(Bidirectional(LSTM(100))) #lstm with 100 neurons\nbi_model.add(Dense(1,activation='sigmoid'))\nbi_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(bi_model.summary())\n","210f1819":"len(embedd)","a7c90d9d":"y.shape","62cf6aae":"x_final = np.array(embedd)\ny_final = np.array(y)\nx_final.shape,y_final.shape","430813b9":"x_final","57ae707c":"y_final","1a934e14":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x_final,y_final,test_size=0.3)","9f39cd15":"bi_model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=64)","af8300a6":"#create a model\nfrom tensorflow.keras.layers import Dropout\ndims=40\nbi_model=Sequential()\nbi_model.add(Embedding(vocab_size,dims,input_length=smax_length))\nbi_model.add(Dropout(0.3))\nbi_model.add(Bidirectional(LSTM(100))) #lstm with 100 neurons\nbi_model.add(Dropout(0.3))\nbi_model.add(Dense(1,activation='sigmoid'))\nbi_model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(bi_model.summary())","6c253a9d":"bi_model.fit(x_train,y_train,validation_data=(x_test,y_test),epochs=20,batch_size=64)","752f5408":"y_pred = bi_model.predict_classes(x_test)","f8cfca35":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\ncm=confusion_matrix(y_pred,y_test)\nprint(\"accuracy score :\",accuracy_score(y_pred,y_test))\nprint(\"classification report :\",classification_report(y_pred,y_test))","fc7398ab":"import matplotlib.pyplot as plt\nimport seaborn as sns\nplt.figure(figsize=(12,8))\nsns.heatmap(cm,annot=True)\nplt.title(\"Confusion Matrix\")\nplt.show()","fd2a520d":"# Preprocessing the Data","06e24f85":"<img src=\"https:\/\/www.mdpi.com\/sensors\/sensors-18-04211\/article_deploy\/html\/images\/sensors-18-04211-g005.png\" \/>","bad542dc":"# Model Evaluation","8bfff099":"# Understanding the data","bfef4862":"<p style=\"font-size:15px;\">\nWhen we are dealing with text data, first we need to preprocess the text and then convert it into vectors.\n<ul>\n    <li>Stemming is actually removing the suffix from a word and reducing it to its root word. First use stemming technique on text to convert into its root word.<\/li>\n\n<li>We generally get text mixed up with a lot of special characters,numerical, etc. we need to take care of removing unwanted text from data. Use regular expressions to replace all the unnecessary data with spaces <\/li>\n\n<li>Convert all the text into lowercase to avoid getting different vectors for the same word . Eg: and, And ------------> and <\/li>\n<li>Remove stopWords - \u201cstop words\u201d  typically  refers to the most common words in a language, Eg: he, is, at etc.  We need to filter stopwords\n <ol>\n     <li>Split the sentence into words<\/li>\n     <li>Extract the text except for stopwords<\/li>\n     <li>Again join them into sentences<\/li>\n     <li>Append the cleaned text into a list (corpus)<\/li>\n    <\/ol>\n   <\/ul>\nNow our text is ready , convert the text into vectors using Countvectorizer\n<\/p>","2acb727c":"<center><img src=\"https:\/\/d3t4nwcgmfrp9x.cloudfront.net\/upload\/mayores-fake-news-2017.png\" \/><\/center>","655dd16d":"### Missing Values","aec79c35":"## checking for imbalance classes","61f9746a":"# Model Creation","5bf6b104":"## Adding Dropout","d6ba7339":"<center><h1 style=\"color:green;\">Bidirectional LSTM Architecture<\/h1><\/center>","e60eca26":"<img src=\"https:\/\/thumbs.dreamstime.com\/b\/white-stone-words-thank-you-smile-face-color-glitter-boke-background-117350639.jpg\" \/>","830b7d27":"# What is Fake News??\n\nMany people have been using the term fake news for the last few years, but do they actually know what it looks like? The term has been used so amorphously that it begs a more direct examination. Sensationalist fake news is often used to generate clicks onto a webpage to improve ad revenue. It has also been used to influence public thought.","d4aff77b":"<b>Bidirectional LSTMs<\/b> are an extension of traditional LSTMs that can improve model performance on sequence classification problems.\n<p>\nIn problems where all timesteps of the input sequence are available, Bidirectional LSTMs train two instead of one LSTMs on the input sequence. The first on the input sequence as-is and the second on a reversed copy of the input sequence. This can provide additional context to the network and result in faster and even fuller learning on the problem.\n<\/p>"}}