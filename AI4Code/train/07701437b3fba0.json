{"cell_type":{"7d7511fd":"code","8c133467":"code","47fde3e7":"code","52d5400f":"code","2b003e9b":"code","2e1289c9":"code","a4ce7fd9":"code","9f9fad0a":"code","586ce41d":"code","c4436cea":"code","77938daa":"code","3f088b14":"code","cc30fda0":"code","36041e75":"code","c99c5a3f":"code","0f62bcd4":"code","46dea7ef":"code","598740ed":"code","a9c6ce9c":"code","73f0b0a7":"code","8471cce9":"code","371a2398":"code","13b0dd44":"markdown","e5e5c4a3":"markdown","dd81c9be":"markdown","fa26fa9d":"markdown","101fdfb6":"markdown","22bc6f5b":"markdown","3780ff20":"markdown","f326cd0d":"markdown","fdbedf45":"markdown","b8afd918":"markdown"},"source":{"7d7511fd":"import os \nimport string\nimport re\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom wordcloud import WordCloud, STOPWORDS\nfrom spacy.lang.hi import Hindi\nfrom spacy.lang.ta import Tamil\nfrom spacy.lang.hi import STOP_WORDS as hindi_stopwords\nfrom spacy.lang.ta import STOP_WORDS as tamil_stopwords\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom collections import defaultdict\nfrom collections import  Counter\nplt.style.use('ggplot')\nstop=set(stopwords.words('english'))\nfrom nltk.tokenize import word_tokenize\nfrom tqdm import tqdm\nimport torch\nimport torch.nn as nn \nfrom torch.optim import Adam,AdamW\nfrom torch.utils.data import SequentialSampler\nfrom torch.utils.data import Dataset,DataLoader\nimport transformers\nfrom transformers import XLMRobertaTokenizer,XLMRobertaModel,AutoTokenizer,XLMRobertaModel,XLMRobertaConfig\nfrom transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer, AutoTokenizer, default_data_collator","8c133467":"DATA_DIR='..\/input\/chaii-hindi-and-tamil-question-answering\/'\ntrain_df = pd.read_csv(DATA_DIR+'train.csv')\ntest_df = pd.read_csv(DATA_DIR+'test.csv')\ntest_df.head()","47fde3e7":"print('There are {} rows and {} columns in train'.format(train_df.shape[0],train_df.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test_df.shape[0],test_df.shape[1]))","52d5400f":"x = train_df.language.value_counts()\nsns.barplot(x.index, x)\nplt.gca().set_ylabel('samples')","2b003e9b":"#character level analysis of context\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['context'].str.len()\nax1.hist(trainh_len,color='red')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['context'].str.len()\nax2.hist(traint_len,color='green')\nax2.set_title('Tamil')\nfig.suptitle('Characters')\nplt.show()","2e1289c9":"#character level analysis of questions\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['question'].str.len()\nax1.hist(trainh_len,color='red')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['question'].str.len()\nax2.hist(traint_len,color='green')\nax2.set_title('Tamil')\nfig.suptitle('Characters')\nplt.show()","a4ce7fd9":"#character level analysis of answer\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['answer_text'].str.len()\nax1.hist(trainh_len,color='red')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['answer_text'].str.len()\nax2.hist(traint_len,color='green')\nax2.set_title('Tamil')\nfig.suptitle('Characters')\nplt.show()","9f9fad0a":"#word level analysis of context\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['context'].str.split().map(lambda x: len(x))\nax1.hist(trainh_len,color='blue')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['context'].str.split().map(lambda x: len(x))\nax2.hist(traint_len,color='orange')\nax2.set_title('Tamil')\nfig.suptitle('Word Level')\nplt.show()","586ce41d":"#word level analysis of question\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['question'].str.split().map(lambda x: len(x))\nax1.hist(trainh_len,color='blue')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['question'].str.split().map(lambda x: len(x))\nax2.hist(traint_len,color='orange')\nax2.set_title('Tamil')\nfig.suptitle('Word Level')\nplt.show()","c4436cea":"#word level analysis of answer\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ntrainh_len=train_df[train_df['language']=='hindi']['answer_text'].str.split().map(lambda x: len(x))\nax1.hist(trainh_len,color='blue')\nax1.set_title('Hindi')\ntraint_len=train_df[train_df['language']=='tamil']['answer_text'].str.split().map(lambda x: len(x))\nax2.hist(traint_len,color='orange')\nax2.set_title('Tamil')\nfig.suptitle('Word Level')\nplt.show()","77938daa":"#lets take a sample data from test\ntest_df.loc[1,'context'][:1000]","3f088b14":"def cleaner(text):\n    table=str.maketrans('','',string.punctuation)\n    text = text.translate(table)\n    return ' '.join([w for w in text.split() if not re.match(r'[A-Z]+', w, re.I)])\ncleaner(test_df.loc[1,'context'])[:1000]","cc30fda0":"train_df.loc[:,'context'] = train_df.loc[:,'context'].apply(lambda x:cleaner(x))\ntrain_df.loc[:,'question'] = train_df.loc[:,'question'].apply(lambda x:cleaner(x))\ntrain_df.loc[:,'answer_text'] = train_df.loc[:,'answer_text'].apply(lambda x:cleaner(x))\ntest_df.loc[:,'context'] = test_df.loc[:,'context'].apply(lambda x:cleaner(x))\ntest_df.loc[:,'question'] = test_df.loc[:,'question'].apply(lambda x:cleaner(x))","36041e75":"test_df.head()","c99c5a3f":"#fetch all the tamil and hindi text\ntamil_text = \" \".join(train_df[train_df[\"language\"]==\"tamil\"][\"question\"])\nhindi_text = \" \".join(train_df[train_df[\"language\"]==\"hindi\"][\"question\"])","0f62bcd4":"# Download and extract the fonts\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Devanagari.zip\n!wget -q http:\/\/www.lipikaar.com\/sites\/www.lipikaar.com\/themes\/million\/images\/support\/fonts\/Tamil.zip\n\n!unzip -qq Devanagari.zip\n!unzip -qq Tamil.zip","46dea7ef":"# Get the tokens and frequencies for Hindi language\nhindi_nlp = Hindi()\nhindi_doc = hindi_nlp(hindi_text)\nhindi_tokens = set([token.text for token in hindi_doc])\nhindi_tokens_counter = Counter(hindi_tokens)\n\n\n# Get the tokens and frequencies for Tamil language\ntamil_nlp = Tamil()\ntamil_doc = hindi_nlp(tamil_text)\ntamil_tokens = set([token.text for token in tamil_doc])\ntamil_tokens_counter = Counter(tamil_tokens)","598740ed":"def plot_wordcloud(\n    font_path,\n    frequencies,\n    stopwords,\n    width=500,\n    height=500,\n    background_color=\"white\",\n    collocations=True,\n    min_font_size=8,\n):\n    \n    wordcloud = WordCloud(font_path=font_path,\n                      width=width,\n                      height=height,\n                      background_color=background_color,\n                      stopwords=stopwords,\n                      collocations=collocations,\n                      min_font_size=min_font_size).generate_from_frequencies(frequencies)\n\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.show()","a9c6ce9c":"# Plot the wordcloud for hindi langauge\nplot_wordcloud(font_path=\"Devanagari\/kalimati.ttf\",\n               frequencies=hindi_tokens_counter,\n               stopwords=hindi_stopwords\n              )","73f0b0a7":"# Plot the wordcloud for hindi langauge\nplot_wordcloud(font_path=\"Tamil\/Samyak-Tamil.ttf\",\n               frequencies=tamil_tokens_counter,\n               stopwords=tamil_stopwords,\n              )","8471cce9":"class ChaiiQAModel(nn.Module):\n    def __init__(self):\n        super(ChaiiQAModel, self).__init__()\n        self.model_config = XLMRobertaConfig.from_pretrained('..\/input\/xlm-roberta-base')\n        self.model_config.return_dict=True\n        self.model = XLMRobertaModel.from_pretrained('..\/input\/xlm-roberta-base', config=self.model_config)\n        self.dropout = nn.Dropout(0.2)\n        self.fc = nn.Linear(self.model_config.hidden_size, 2)\n        \n    def forward(self, input_ids, attention_mask):\n        output = self.model(inputs_ids, attention_mask)\n        last_hiddent_state = output['last_hidden_state']\n        x = self.dropout(last_hidden_state)\n        x = self.fc(x)\n        start_logits, end_logits = x.split(1,dim=-1)\n        start_logits= start_logits.squeeze(-1)\n        end_logits=end_logits.squeeze(-1)\n        return start_logits, end_logits","371a2398":"model = ChaiiQAModel()\nmodel","13b0dd44":"# QA Model using XLM-RoberTa","e5e5c4a3":"This clearly indicates that Hindi is double when compared to Tamil. So need to be careful with evaluation metrics","dd81c9be":"## Coming Soon","fa26fa9d":"## Basic EDA\nNow lets do a character\/word level analysis of both context and question to understand the average size of context and questions","101fdfb6":"## Data Cleaning\n\nFrom a quick data investigation  we can see there are punctuations and some English words (Google) within the context and questions, so lets clean that up","22bc6f5b":"This looks much better :) \nNow we can clean up all the relevant contents like context, questions, answer_text ..etc","3780ff20":"Class Distribution\n\nBefore we begin with anything else,let's check the class distribution.There are only two classes Hindi and Tamil.","f326cd0d":"\n## WordCloud\n\nNow lets look at the word cloud\n\nThanks to @aakashnain for reference\n","fdbedf45":"# chaii \u2615 - Hindi and Tamil Question Answering\n\nHindi happens to be the most comman language in India which is the 2nd Most populated country in the world. Comparing Hindi, Tamil is not far behind thanks to Mr Rajinikanth \ud83e\udd29. Building a QA model for Indian languages are not as easy compared to building them for English.So lets give it a try\n\nOur Task is build a QA model which gives answers to all the questions given in either Hindi or Tamil.","b8afd918":"The above context contains a lot of noise like English words, special characters, index ...etc. We can just clean them up"}}