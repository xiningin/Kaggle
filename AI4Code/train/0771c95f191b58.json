{"cell_type":{"ecfaa31d":"code","259ca4b1":"code","85b297cc":"code","4bbf82de":"code","3006b852":"code","07dd882c":"code","4db45663":"code","43294a92":"code","7b442799":"code","2150763d":"code","0a9835ad":"code","45c5b410":"code","a636d3ec":"code","1a733f96":"code","5253ba40":"code","0e560e37":"code","f38c437d":"code","6ce59fc1":"code","131ca482":"code","69c815db":"code","f9d63861":"code","f097b921":"code","9014e1c1":"code","ceb93069":"code","47b241ca":"code","914a927e":"code","c2e3d149":"code","8c63a255":"code","12293f35":"code","99169ee2":"code","baa42a0e":"code","2da2a841":"code","f43571fa":"code","c4286737":"code","1a55aaeb":"code","a39c1526":"code","b718007b":"code","cc3636be":"code","4ff273e7":"code","c153933c":"code","9e9dcde8":"code","6a2131e7":"code","d302207a":"code","762f55e7":"code","8679e5ca":"code","de2be834":"code","d6797b7e":"code","b8d585cc":"code","64b09795":"code","8244f0ec":"code","24d73ea4":"code","74996fbc":"code","7671fcd4":"code","b87f9cd7":"code","5eaf8eb2":"code","dcfeab3e":"code","42900422":"code","a5b69927":"code","69df5954":"code","73d27c7f":"code","e60dbeed":"code","fa0edc8c":"code","1f67b1cd":"code","307ab645":"code","11e3cf0a":"code","3b82435d":"code","5f8bd023":"code","a4a8cba3":"code","685098af":"code","f4d90dd9":"code","f248f71a":"code","79037ef0":"code","cf56849c":"code","0eb8ef4f":"code","dc4f78a5":"code","c9bd562e":"code","89c254a4":"code","855a735a":"code","5e4732dc":"code","d01bb2e2":"code","cd6b2c3f":"code","31eb6899":"code","52c594ce":"code","0b11d94a":"code","08fbbd68":"code","3d4f86d1":"code","46f46472":"code","c00183f7":"code","478d17dc":"code","31ff7260":"code","c8f07360":"code","c3b7b722":"code","5f78dc64":"code","4063bf4b":"code","13842de6":"code","75e3f864":"code","01a82361":"code","9ec18439":"code","4ef73dc1":"code","0a3dd366":"code","cf2baa85":"code","2efe6cc7":"code","1e4deb34":"code","993ce25e":"code","773c34bd":"code","62c0563b":"code","39755d01":"code","cef76df0":"code","e95ac1fd":"code","0c88fad7":"code","f6b1293b":"code","266884df":"code","da1a920a":"code","d748d99b":"code","af732468":"code","75151227":"code","555cda62":"code","78d01e9b":"code","7c6d1ca6":"markdown","1ac30b7f":"markdown","0f0b18da":"markdown","da8a577c":"markdown","c2a88fa9":"markdown","cd6c4bc8":"markdown","9f465c22":"markdown","19ff92c5":"markdown","074fcf8d":"markdown","88762c62":"markdown","6f6808f2":"markdown","7a3aab47":"markdown","4dc539ad":"markdown","20f1549e":"markdown","a10d7ac8":"markdown","a3210ceb":"markdown","2abc300e":"markdown","f87abe90":"markdown","13493988":"markdown","0eb4ff0d":"markdown","1b4b11ad":"markdown","d853f27f":"markdown","d5156a31":"markdown","de42b6d6":"markdown","d5487b71":"markdown","321c58d5":"markdown","a0a79b25":"markdown","23416962":"markdown","6849fa8e":"markdown","020f66f4":"markdown","b624f946":"markdown","65695aec":"markdown","e692c846":"markdown","712e2afb":"markdown","2cc77a7c":"markdown","edb10f65":"markdown","78ae8238":"markdown","d2d8834a":"markdown","f1c8d469":"markdown","4f117f7d":"markdown","f3983b1b":"markdown","b6f5f8cf":"markdown","5b516ae9":"markdown","04c12f99":"markdown","10978870":"markdown","d80ad8e7":"markdown","15cd7c86":"markdown","4c5cae2c":"markdown"},"source":{"ecfaa31d":"# Import the required libraries\n\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import recall_score, accuracy_score, confusion_matrix, f1_score\nfrom sklearn.metrics import precision_score, auc, roc_auc_score, roc_curve, precision_recall_curve\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.decomposition import PCA, IncrementalPCA\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n\nfrom xgboost import XGBClassifier\n\nfrom imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import NearMiss, RandomUnderSampler\nfrom imblearn.combine import SMOTEENN, SMOTETomek\n\n# Ignore Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom lightgbm import LGBMClassifier\n\n# To display all the columns\npd.options.display.max_columns = None\n\n# To display all the rows\npd.options.display.max_rows = None\n\n# To map Empty Strings or numpy.inf as Na Values\npd.options.mode.use_inf_as_na = True\n\npd.options.display.expand_frame_repr =  False\n\n%matplotlib inline\n\n# Set Style\nsns.set(style = \"whitegrid\")","259ca4b1":"telecom_data = pd.read_csv('..\/input\/telecom-churn-data-new\/telecom_churn_data.csv')\n\ntelecom_data.head()","85b297cc":"telecom_data.shape","4bbf82de":"# Check the total missing values in each column.\nprint(\"Total NULL Values in each columns\")\nprint(\"*********************************\")\nprint(telecom_data.isnull().sum())","3006b852":"# Lets check the percentage of missing values column-wise\n\n(telecom_data.isnull().sum()\/ len(telecom_data)) * 100","07dd882c":"total_rech_data_6_index = telecom_data['total_rech_data_6'].isnull()\ndate_of_last_rech_data_6_index = telecom_data['date_of_last_rech_data_6'].isnull()\n\nif total_rech_data_6_index.equals(date_of_last_rech_data_6_index):\n    print('The indexes for NULL values for month 6 are equal')\n","4db45663":"telecom_data['total_rech_data_6'].fillna(0, inplace=True)\ntelecom_data['av_rech_amt_data_6'].fillna(0, inplace=True)","43294a92":"total_rech_data_7_index = telecom_data['total_rech_data_7'].isnull()\ndate_of_last_rech_data_7_index = telecom_data['date_of_last_rech_data_7'].isnull()\n\nif total_rech_data_7_index.equals(date_of_last_rech_data_7_index):\n    print('The indexes for NULL values for month 7 are equal')","7b442799":"total_rech_data_8_index = telecom_data['total_rech_data_8'].isnull()\ndate_of_last_rech_data_8_index = telecom_data['date_of_last_rech_data_8'].isnull()\n\nif total_rech_data_8_index.equals(date_of_last_rech_data_8_index):\n    print('The indexes for NULL values for month 8 are equal')","2150763d":"# Month 7\ntelecom_data['total_rech_data_7'].fillna(0, inplace=True)\ntelecom_data['av_rech_amt_data_7'].fillna(0, inplace=True)\n\n# Month 8\ntelecom_data['total_rech_data_8'].fillna(0, inplace=True)\ntelecom_data['av_rech_amt_data_8'].fillna(0, inplace=True)","0a9835ad":"(telecom_data.isnull().sum(axis = 0)\/ len(telecom_data)) * 100","45c5b410":"# Also lets check how many columns have more than 70% data missing\n\ncols_with_atleast_70_percent_missing_data = ((telecom_data.isnull().sum()\/ len(telecom_data)) * 100) >= 70\ncols_with_atleast_70_percent_missing_data.sum()","a636d3ec":"telecom_data['total_data_rech_amt_6'] = telecom_data['total_rech_data_6'] * telecom_data['av_rech_amt_data_6']\ntelecom_data['total_data_rech_amt_7'] = telecom_data['total_rech_data_7'] * telecom_data['av_rech_amt_data_7']\ntelecom_data['total_data_rech_amt_8'] = telecom_data['total_rech_data_8'] * telecom_data['av_rech_amt_data_8']\ntelecom_data['total_data_rech_amt_9'] = telecom_data['total_rech_data_9'] * telecom_data['av_rech_amt_data_9']","1a733f96":"# Lets compute the average recharge amount for the month 6 & 7. This total amount is equal to the sum of talk time recharge \n# and data recharge amounts for the respective months.\n\navg_recharge_amount_month_6_7 = telecom_data[['total_data_rech_amt_6','total_data_rech_amt_7','total_rech_amt_6',\n                                             'total_rech_amt_7']].mean(axis = 1)\n\namount_70th_percentile = np.percentile(avg_recharge_amount_month_6_7, 70)\n\nprint(\"70th percentile of the average recharge amount in the first two months is - \", amount_70th_percentile)","5253ba40":"# Filtering the high values\n\ntelecom_data = telecom_data[avg_recharge_amount_month_6_7 >= amount_70th_percentile]","0e560e37":"telecom_data.shape","f38c437d":"telecom_data.head()","6ce59fc1":"# Reset the dataframe indexes\n\ntelecom_data = telecom_data.reset_index(drop=True)\ntelecom_data.head()","131ca482":"# Rename month named vbc columns to format 6,7,8 and 9\n\ntelecom_data.rename(columns = {'jun_vbc_3g':'vbc_3g_6',\n                               'jul_vbc_3g':'vbc_3g_7',\n                               'aug_vbc_3g':'vbc_3g_8',\n                               'sep_vbc_3g':'vbc_3g_9'}, inplace=True)","69c815db":"churn_parameters_data = telecom_data[['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9', 'vol_3g_mb_9']]\nchurn_parameters_data.head()","f9d63861":"telecom_data['churn'] = telecom_data.apply(lambda x: 1 if((x.total_ic_mou_9 == 0) & \n                                                          (x.total_og_mou_9 == 0) and \n                                                          (x.vol_2g_mb_9 == 0) and \n                                                          (x.vol_3g_mb_9 == 0)) else 0, axis=1)","f097b921":"telecom_data['churn'].head()","9014e1c1":"# We will try to get all the columns in the 'telecom_data' dataset where the column have '_9' in the name.\n\ncols_with__9 = [col for col in telecom_data.columns if '_9' in col]\ncols_with__9","ceb93069":"print(\"Total number of columns to be dropped at this stage is - \", len(cols_with__9))","47b241ca":"telecom_data.drop(cols_with__9, axis=1, inplace = True)","914a927e":"telecom_data.shape","c2e3d149":"(telecom_data.isnull().sum(axis = 0)\/ len(telecom_data)) * 100","8c63a255":"# Also lets check how many columns have more than 40% data missing.\n\ntotal_cols_with_atleast_40_percent_missing_data = ((telecom_data.isnull().sum()\/ len(telecom_data)) * 100) >= 40\ntotal_cols_with_atleast_40_percent_missing_data.sum()","12293f35":"# Get the column\/feature names which have more than 40% missing data.\n\ncols_names_with_40_percent_missing_value = total_cols_with_atleast_40_percent_missing_data[\n    total_cols_with_atleast_40_percent_missing_data > 0.40].index\n\ncols_names_with_40_percent_missing_value","99169ee2":"telecom_data.drop(['date_of_last_rech_data_6', 'date_of_last_rech_data_7', 'date_of_last_rech_data_8'], axis=1, inplace = True)","baa42a0e":"print(\"Column info for max_rech_data_6\")\nprint(\"-------------------------------\")\nprint(telecom_data.max_rech_data_6.describe())\nprint('-------------------------------------')\nprint(\"Total NULL values for max_rech_data_6 columns is - \", telecom_data.max_rech_data_6.isnull().sum())","2da2a841":"print(\"Column info for max_rech_data_7\")\nprint(\"-------------------------------\")\nprint(telecom_data.max_rech_data_7.describe())\nprint('-------------------------------------')\nprint(\"Total NULL values for max_rech_data_7 columns is - \", telecom_data.max_rech_data_7.isnull().sum())","f43571fa":"print(\"Column info for max_rech_data_8\")\nprint(\"-------------------------------\")\nprint(telecom_data.max_rech_data_8.describe())\nprint('-------------------------------------')\nprint(\"Total NULL values for max_rech_data_8 columns is - \", telecom_data.max_rech_data_8.isnull().sum())","c4286737":"# Replace NA with 0\n\nfor col in ['max_rech_data_6', 'max_rech_data_7', 'max_rech_data_8']:\n    telecom_data[col].fillna(0, inplace=True)","1a55aaeb":"# Month 6\n\narpu_3g_6_index = telecom_data['arpu_3g_6'].isnull()\nnight_pck_user_6_index = telecom_data['night_pck_user_6'].isnull()\ncount_rech_2g_6_index = telecom_data['count_rech_2g_6'].isnull()\ncount_rech_3g_6_index = telecom_data['count_rech_3g_6'].isnull()\nfb_user_6_index = telecom_data['fb_user_6'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_3g_6_index.equals(night_pck_user_6_index) & night_pck_user_6_index.equals(count_rech_2g_6_index) & count_rech_2g_6_index.equals(count_rech_3g_6_index) &  count_rech_3g_6_index.equals(fb_user_6_index):\n    print('The indexes for NULL values for month 6 are equal')\n\n\n# ***************************************************************************************************************************\n\n# Month 7\n\narpu_3g_7_index = telecom_data['arpu_3g_7'].isnull()\nnight_pck_user_7_index = telecom_data['night_pck_user_7'].isnull()\ncount_rech_2g_7_index = telecom_data['count_rech_2g_7'].isnull()\ncount_rech_3g_7_index = telecom_data['count_rech_3g_7'].isnull()\nfb_user_7_index = telecom_data['fb_user_7'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_3g_7_index.equals(night_pck_user_7_index) & night_pck_user_7_index.equals(count_rech_2g_7_index) & count_rech_2g_7_index.equals(count_rech_3g_7_index) &  count_rech_3g_7_index.equals(fb_user_7_index):\n    print('The indexes for NULL values for month 7 are equal')\n\n# ***************************************************************************************************************************\n\n# Month 8\n\narpu_3g_8_index = telecom_data['arpu_3g_8'].isnull()\nnight_pck_user_8_index = telecom_data['night_pck_user_8'].isnull()\ncount_rech_2g_8_index = telecom_data['count_rech_2g_8'].isnull()\ncount_rech_3g_8_index = telecom_data['count_rech_3g_8'].isnull()\nfb_user_8_index = telecom_data['fb_user_8'].isnull()\n\n# If all the above objects\/arrays are equal for all entries, then we can confirm that the missing values are all from the\n# same observations\/index.\n\nif arpu_3g_8_index.equals(night_pck_user_8_index) & night_pck_user_8_index.equals(count_rech_2g_8_index) & count_rech_2g_8_index.equals(count_rech_3g_8_index) &  count_rech_3g_8_index.equals(fb_user_8_index):\n    print('The indexes for NULL values for month 8 are equal')\n","a39c1526":"# From the above inferences let's impute the missing values with 0\n\ncolumns_to_impute = ['count_rech_2g_6', 'count_rech_2g_7',\n       'count_rech_2g_8', 'count_rech_3g_6', 'count_rech_3g_7',\n       'count_rech_3g_8', 'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8', 'arpu_2g_6',\n       'arpu_2g_7', 'arpu_2g_8', 'night_pck_user_6', 'night_pck_user_7',\n       'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\n\ntelecom_data[columns_to_impute] = telecom_data[columns_to_impute].fillna(0)","b718007b":"# Lets check the info() for the above columns again - \n\ntelecom_data[columns_to_impute].info()","cc3636be":"# Variance  - df.var() method\n\ncolumns_with_0_variance = telecom_data.var() == 0\nprint(\"Total columns with ZERO variance are - \", columns_with_0_variance.sum())\n\ncolumn_name_with_0_variance = columns_with_0_variance[columns_with_0_variance == 1].index\nprint(column_name_with_0_variance)\n\nprint(\"\\n------------------------------------------------------------------------------\\n\")\n\n\ncolumns_with_1_unique_value = telecom_data.nunique() == 1\nprint(\"Total columns with only ONE unique value are - \", columns_with_1_unique_value.sum())\n\ncolumn_name_with_1_unique_value = columns_with_1_unique_value[columns_with_1_unique_value == 1].index\nprint(column_name_with_1_unique_value)","4ff273e7":"# Dropping the non - date columns \n\ntelecom_data.drop(column_name_with_0_variance, axis=1, inplace = True)","c153933c":"# Data glimpse\n\ntelecom_data.head()","9e9dcde8":"columns_with_null_values = telecom_data.columns[telecom_data.isna().any()].tolist()\nprint(columns_with_null_values)","6a2131e7":"cols_with_null_values_for_month_6 = [col for col in columns_with_null_values if '_6' in col]\n\ntelecom_data[cols_with_null_values_for_month_6].info()","d302207a":"# But before proceeding let's confirm our inferences for months 7 & 8.\n\n# Month 7\nprint(\"*************************Month 7*************************\\n\")\ncols_with_null_values_for_month_7 = [col for col in columns_with_null_values if '_7' in col]\nprint(telecom_data[cols_with_null_values_for_month_7].info())\n\n# Month 8\nprint(\"\\n*************************Month 8*************************\\n\")\ncols_with_null_values_for_month_8 = [col for col in columns_with_null_values if '_8' in col]\nprint(telecom_data[cols_with_null_values_for_month_8].info())","762f55e7":"# Let's impute the missing values from the above columns with 0. There are 3 column with date values - 'date_of_last_rech'.\n# We do not want to impute date objects with 0. So will exclude it for now.\n\nfor column in columns_with_null_values:\n    if \"date_of_last_rech\" not in column:\n        telecom_data[column].fillna(0, inplace=True)","8679e5ca":"telecom_data.head()","de2be834":"# Let's check on the date values\ndate_columns = ['date_of_last_rech_6', 'date_of_last_rech_7', 'date_of_last_rech_8', \n                'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8']\n\ntelecom_data[date_columns].info()","d6797b7e":"# The date columns will not be of any use to us during the analysis.\n# Let's delete the date columns as we do not need them.\n\ntelecom_data.drop(date_columns, axis=1, inplace = True)","b8d585cc":"# Lets have a look at the data description for missing values again -\n\nprint(telecom_data.isnull().sum())","64b09795":"# Let's drop individual columns whose totals are available as a different attribute\n\nindividual_cols = ['loc_ic_t2t_mou_6', 'loc_ic_t2t_mou_7', 'loc_ic_t2t_mou_8',\n                   'loc_ic_t2m_mou_6', 'loc_ic_t2m_mou_7', 'loc_ic_t2m_mou_8',\n                   'loc_ic_t2f_mou_6', 'loc_ic_t2f_mou_7', 'loc_ic_t2f_mou_8',\n                   'std_ic_t2t_mou_6', 'std_ic_t2t_mou_7', 'std_ic_t2t_mou_8',\n                   'std_ic_t2m_mou_6', 'std_ic_t2m_mou_7', 'std_ic_t2m_mou_8',\n                   'std_ic_t2f_mou_6', 'std_ic_t2f_mou_7', 'std_ic_t2f_mou_8',\n                   'loc_og_t2t_mou_6', 'loc_og_t2t_mou_7', 'loc_og_t2t_mou_8',\n                   'loc_og_t2m_mou_6', 'loc_og_t2m_mou_7', 'loc_og_t2m_mou_8',\n                   'loc_og_t2f_mou_6', 'loc_og_t2f_mou_7', 'loc_og_t2f_mou_8',\n                   'loc_og_t2c_mou_6', 'loc_og_t2c_mou_7', 'loc_og_t2c_mou_8',\n                   'std_og_t2t_mou_6', 'std_og_t2t_mou_7', 'std_og_t2t_mou_8',\n                   'std_og_t2m_mou_6', 'std_og_t2m_mou_7', 'std_og_t2m_mou_8',\n                   'std_og_t2f_mou_6', 'std_og_t2f_mou_7', 'std_og_t2f_mou_8',\n                   'last_day_rch_amt_6', 'last_day_rch_amt_7', 'last_day_rch_amt_8',\n                   'arpu_3g_6', 'arpu_3g_7', 'arpu_3g_8',\n                   'arpu_2g_6', 'arpu_2g_7', 'arpu_2g_8',\n                   'av_rech_amt_data_6', 'av_rech_amt_data_7', 'av_rech_amt_data_8']\n\ntelecom_data.drop(individual_cols, axis = 1, inplace = True)\n\ntelecom_data.shape","8244f0ec":"# Let's check the data description for the arpu columns\n\narpu_columns = ['arpu_6', 'arpu_7', 'arpu_8']\n\ndef describe_ARPU_Columns():\n    for column in arpu_columns:\n        print(\"Column Description for -\",column, end='\\n')\n        print(telecom_data[column].describe(), end='\\n')\n        print(\"----------------------------------------\", end='\\n')\n\ndescribe_ARPU_Columns()","24d73ea4":"# Index where the arpu values for month 6 are less than 0 -\n\narpu_6_index = (telecom_data['arpu_6'] < 0)\n    \n# Total number of such observations for month 6 -\nprint('Total observations with negative arpu values for month 6 -', arpu_6_index.sum())","74996fbc":"# Index where the arpu values for month 7 are less than 0 -\n\narpu_7_index = (telecom_data['arpu_7'] < 0)\n    \n# Total number of such observations for month 7 -\nprint('Total observations with negative arpu values for month 7 -', arpu_7_index.sum())","7671fcd4":"# Index where the arpu values for month 8 are less than 0 -\n\narpu_8_index = (telecom_data['arpu_8'] < 0)\n    \n# Total number of such observations for month 8 -\nprint('Total observations with negative arpu values for month 8 -', arpu_8_index.sum())","b87f9cd7":"# Let's delete the observations with negative arpu values. \n\ntelecom_data = telecom_data[(telecom_data['arpu_6'] >= 0) & \n                            (telecom_data['arpu_7'] >= 0) & \n                            (telecom_data['arpu_8'] >= 0)]","5eaf8eb2":"telecom_data.shape","dcfeab3e":"# Summary Statistics\n\ntelecom_data.describe()","42900422":"category_list = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\n\ntelecom_data[category_list] = telecom_data[category_list].astype(int)\n\ntelecom_data[category_list].info()","a5b69927":"if len(telecom_data['mobile_number'].unique()) == len(telecom_data.index):\n    print(\"No Duplicates. Each Mobile Number is unique.\")\nelse:\n    print(\"Duplicate observations present.\")","69df5954":"# Dropping mobile number\n\ntelecom_data.drop(['mobile_number'], axis=1, inplace = True)\n\n# Data glimpse\ntelecom_data.head()","73d27c7f":"# Custom Function to add data labels in the graph\n\ndef add_data_labels(ax, spacing = 5):\n\n    # For each bar: Place a label\n    for rect in ax.patches:\n        # Get X and Y placement of label from rect.\n        y_value = rect.get_height()\n        x_value = rect.get_x() + rect.get_width() \/ 2\n\n        # Number of points between bar and label. Change to your liking.\n        space = spacing\n        # Vertical alignment for positive values\n        va = 'bottom'\n\n        # If value of bar is negative: Place label below bar\n        if y_value < 0:\n            # Invert space to place label below\n            space *= -1\n            # Vertically align label at top\n            va = 'top'\n\n        # Use Y value as label and format number with one decimal place\n        label = \"{:.2f}%\".format(y_value)\n\n        # Create annotation\n        plt.annotate(\n            label,                        # Use `label` as label\n            (x_value, y_value),           # Place label at end of the bar\n            xytext = (0, space),          # Vertically shift label by `space`\n            textcoords = \"offset points\", # Interpret `xytext` as offset in points\n            ha = 'center',                # Horizontally center label\n            va = va)                      # Vertically align label differently for positive and negative values.","e60dbeed":"# Univariate Plot Analysis of Ordered categorical variables vs Percentage Rate\ncategory_list = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', 'fb_user_6', 'fb_user_7', 'fb_user_8']\ncounter = 1\n\nplt.figure(figsize = (15, 12))\n\nfor col_list in category_list:\n        \n    series = round(((telecom_data[col_list].value_counts(dropna = False))\/(len(telecom_data[col_list])) * 100), 2)\n\n    plt.subplot(2, 3, counter)\n    ax = sns.barplot(x = series.index, y = series.values, order = series.sort_index().index)\n    plt.xlabel(col_list, labelpad = 15)\n    plt.ylabel('Percentage Rate', labelpad = 10)\n\n    # Call Custom Function\n    add_data_labels(ax)\n\n    counter += 1\n\ndel category_list, counter, ax\n\nplt.subplots_adjust(hspace = 0.3)\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","fa0edc8c":"# Get number of positve and negative examples\npos = telecom_data[telecom_data[\"churn\"] == 1].shape[0]\nneg = telecom_data[telecom_data[\"churn\"] == 0].shape[0]\n\nprint(f\"Positive examples = {pos}\")\nprint(f\"Negative examples = {neg}\")\nprint(f\"Proportion of positive to negative examples = {(pos \/ neg) * 100:.2f}%\")\n\nplt.figure(figsize = (8, 8))\n\nseries = round(((telecom_data[\"churn\"].value_counts(dropna = False))\/(len(telecom_data[\"churn\"])) * 100), 2)\n\nax = sns.barplot(x = series.index, y = series.values, order = series.sort_index().index)\nplt.xlabel(\"Churn\", labelpad = 15)\nplt.ylabel('Percentage Rate', labelpad = 10)\n\n# Call Custom Function\nadd_data_labels(ax)\n\nplt.subplots_adjust(hspace = 0.3)\nplt.subplots_adjust(wspace = 0.5)\nplt.show()","1f67b1cd":"# Custom Function for Default Plotting variables\n\n\ndef set_plotting_variable(figure_title, xlabel, ylabel):\n    \n    plt.title(figure_title)\n    plt.xlabel(xlabel, labelpad = 15)\n    plt.ylabel(ylabel, labelpad = 10)","307ab645":"# Custom Function for Univariate Analysis\n\n\ndef plot_univariate(figsize_x, figsize_y, subplot_x, subplot_y, xlabel, ylabel, x_axis, data, wspace):\n    \n    plt.figure(figsize = (figsize_x, figsize_y))\n    \n    title_1 = \"Distribution Plot of \" + xlabel\n    title_2 = \"Box Plot of \" + xlabel\n\n    # Subplot - 1\n    plt.subplot(subplot_x, subplot_y, 1)\n\n    sns.distplot(data[x_axis], hist = True, kde = True, color = 'g')\n    # Call Custom Function\n    set_plotting_variable(title_1, xlabel, ylabel)\n\n    # Subplot - 2\n    plt.subplot(subplot_x, subplot_y, 2)\n\n    sns.boxplot(x = x_axis, data = data, color = 'm')\n    # Call Custom Function\n    set_plotting_variable(title_2, xlabel, ylabel)\n    \n    plt.subplots_adjust(wspace = wspace)\n    plt.show()","11e3cf0a":"# Univariate Plot Analysis of Quantitative Variables\n\ncategory_list = ['night_pck_user_6', 'night_pck_user_7', 'night_pck_user_8', \n                 'fb_user_6', 'fb_user_7', 'fb_user_8', 'churn']\ncounter = 1\n\nfor col_list in telecom_data.columns:\n    \n    if col_list not in category_list:\n        \n        # Call Custom Function\n        plot_univariate(figsize_x = 20,\n                        figsize_y = 8,\n                        subplot_x = 1,\n                        subplot_y = 2,\n                        xlabel = col_list,\n                        ylabel = \"Distribution\",\n                        x_axis = col_list,\n                        data = telecom_data,\n                        wspace = 0.2)\n\n        counter += 1","3b82435d":"# Custom Function for Bivariate Analysis\n\n\ndef plot_bivariate(y_axis):\n    \n    plt.figure(figsize = (15, 5))\n    \n    xlabel = \"Churn\"\n    x_axis = \"churn\"\n    \n    title_1 = \"Month 6 - \" + xlabel\n    title_2 = \"Month 7 - \" + xlabel\n    title_3 = \"Month 8 - \" + xlabel\n    \n    print(\"\\nData Visualization of churn vs \" + y_axis)\n\n    # Subplot - 1\n    plt.subplot(1, 3, 1)\n\n    sns.boxplot(x = x_axis, y = y_axis + \"_6\", hue = \"churn\", data = telecom_data, showfliers = False)\n    #sns.barplot(x = x_axis, y = y_axis + \"_6\", hue = \"churn\", data = telecom_data)\n    # Call Custom Function\n    set_plotting_variable(title_1, xlabel, y_axis + \"_6\")\n\n    # Subplot - 2\n    plt.subplot(1, 3, 2)\n\n    sns.boxplot(x = x_axis, y = y_axis + \"_7\", hue = \"churn\", data = telecom_data, showfliers = False)\n    #sns.barplot(x = x_axis, y = y_axis + \"_7\", hue = \"churn\", data = telecom_data)\n    # Call Custom Function\n    set_plotting_variable(title_2, xlabel, y_axis + \"_7\")\n    \n    # Subplot - 3\n    plt.subplot(1, 3, 3)\n\n    sns.boxplot(x = x_axis, y = y_axis + \"_8\", hue = \"churn\", data = telecom_data, showfliers = False)\n    #sns.barplot(x = x_axis, y = y_axis + \"_8\", hue = \"churn\", data = telecom_data)\n    # Call Custom Function\n    set_plotting_variable(title_3, xlabel, y_axis + \"_8\")\n    \n    plt.subplots_adjust(wspace = 0.4)\n    plt.show()","5f8bd023":"telecom_data.head()","a4a8cba3":"# Bivariate Analysis\n\nplot_bivariate(\"arpu\")\n\nplot_bivariate(\"onnet_mou\")\n\nplot_bivariate(\"offnet_mou\")\n\nplot_bivariate(\"total_og_mou\")\n\nplot_bivariate(\"total_ic_mou\")\n\nplot_bivariate(\"total_rech_num\")\n\nplot_bivariate(\"total_rech_amt\")\n\nplot_bivariate(\"total_rech_data\")\n\nplot_bivariate(\"vol_2g_mb\")\n\nplot_bivariate(\"vol_3g_mb\")\n\nplot_bivariate(\"vbc_3g\")\n\nplot_bivariate(\"total_data_rech_amt\")","685098af":"telecom_data.corr()","f4d90dd9":"plt.figure(figsize = (25, 20))\n\nsns.heatmap(telecom_data.corr())\n\nplt.show()","f248f71a":"#getting corr vars\ncorr_matrix = telecom_data.corr().abs()\n\n#the matrix is symmetric so we need to extract upper triangle matrix without diagonal (k = 1)\nupper_triangle = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)))\n\nhighly_correlated_features = [column for column in upper_triangle.columns if any(upper_triangle[column] > 0.80)]\nprint(\"List of highly correlated features from the above plot - \\n\\n\", highly_correlated_features)\nprint(\"\\n\\nTotal features with high correlation - \", len(highly_correlated_features))","79037ef0":"# Custom Function to derive new good phase columns and drop the original columns \ndef derive_good_action_phase(df, col):\n    \n    col_6 = col + \"_6\"\n    col_7 = col + \"_7\"\n    col_8 = col + \"_8\"\n    good_phase_col = col + \"_good_phase\"\n    action_phase_col = col + \"_action_phase\"\n    \n    df[good_phase_col] = (df[col_6] + df[col_7])\/2\n    df[action_phase_col] = df[col_8] - df[good_phase_col]\n    \n    df.drop([col_6, col_7, col_8], axis = 1, inplace = True)\n    \n    return df","cf56849c":"# Derive Good and Action Phase Variables\n\ntelecom_data = derive_good_action_phase(telecom_data, \"arpu\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"onnet_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"offnet_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"roam_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"roam_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"loc_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"std_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"isd_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"spl_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"og_others\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_og_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"loc_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"std_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"spl_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"isd_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"ic_others\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_ic_mou\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_rech_num\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_rech_amt\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"max_rech_amt\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_rech_data\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"max_rech_data\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"count_rech_2g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"count_rech_3g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"vol_2g_mb\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"vol_3g_mb\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"monthly_2g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"sachet_2g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"monthly_3g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"sachet_3g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"vbc_3g\")\n\ntelecom_data = derive_good_action_phase(telecom_data, \"total_data_rech_amt\")\n\ntelecom_data.head()","0eb8ef4f":"telecom_data.shape","dc4f78a5":"# train test split\nX = telecom_data.drop('churn', axis = 1)\ny = telecom_data[['churn']]\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.7, test_size = 0.3, random_state = 100)\n\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","c9bd562e":"# Normalize the data\nscaler = RobustScaler()\n\nscaled_data = scaler.fit_transform(X_train)\n\nX_train = pd.DataFrame(data = scaled_data, index = X_train.index, columns = X_train.columns)\nX_test = pd.DataFrame(data = scaler.transform(X_test), index = X_test.index, columns = X_test.columns)","89c254a4":"# Implement SMOTEENN\ndef implement_smoteenn(X_train, y_train):\n    \n    sampling = SMOTEENN(random_state = 100)\n    X_train_resampled, y_train_resampled = sampling.fit_resample(X_train.values, y_train.values.ravel())\n\n    # Shuffle the data\n    perms = np.random.permutation(X_train_resampled.shape[0])\n    X_train_resampled = X_train_resampled[perms]\n    y_train_resampled = y_train_resampled[perms]\n\n    X_train_resampled = pd.DataFrame(data = X_train_resampled)\n    y_train_resampled = pd.DataFrame(data = y_train_resampled)\n\n    print(X_train_resampled.shape)\n    print(y_train_resampled.shape)\n    \n    return X_train_resampled, y_train_resampled","855a735a":"# Custom Function for ensembling classifiers using sampling and then Shuffling the data\n\ndef transform(transformer, X, y):\n    \n    print(\"Transforming {}\".format(transformer.__class__.__name__))\n    \n    X_resampled, y_resampled = transformer.fit_resample(X.values, y.values.ravel())\n    \n    # Shuffle the data\n    perms = np.random.permutation(X_resampled.shape[0])\n    X_resampled = X_resampled[perms]\n    y_resampled = y_resampled[perms]\n    \n    # Return the classifier, X_resampled and y_resampled\n    return transformer.__class__.__name__, pd.DataFrame(X_resampled), pd.DataFrame(y_resampled)","5e4732dc":"# Custom Function to Apply transformations to dataset\n\ndef apply_transformations(dataset, X_train, y_train):\n    \n    dataset.append((\"base\", X_train, y_train))\n    dataset.append(transform(SMOTEENN(random_state = 100), X_train, y_train))\n    \n    return dataset","d01bb2e2":"# Custom Function to get Scores and plots\ndef get_scores(scores, reg, X_test):\n    \n\n\n    pred_test = reg.predict(X_test.values)\n\n    pred_test_probs = reg.predict_proba(X_test.values)[:, 1:]\n\n    fpr, tpr, thresholds = roc_curve(y_test.values.ravel(), pred_test)\n    p, r, t = precision_recall_curve(y_test.values.ravel(), pred_test_probs)\n\n    model_f1_score = f1_score(y_test.values.ravel(), pred_test)\n    model_precision_score = precision_score(y_test.values.ravel(), pred_test)\n    model_recall_score = recall_score(y_test.values.ravel(), pred_test)\n    model_accuracy_score = accuracy_score(y_test.values.ravel(), pred_test)\n\n\n    scores.append((model_f1_score,\n                   model_precision_score,\n                   model_recall_score,\n                   model_accuracy_score,\n                   confusion_matrix(y_test.values.ravel(), pred_test)))\n\n\n    return scores","cd6b2c3f":"# Custom Function for Model Building\n\ndef model(sampling_type, X, y, param_grid, model_type, ml = 'None'):\n    \n    model_type_list = ['Logistic', 'SVM', 'Decision Tree', 'Random Forest', 'Adaboost', 'XGBoost', 'LightGBM']\n    \n    if model_type == 'Logistic':\n    \n        ml = LogisticRegression(penalty = 'l2', C = 10, n_jobs = -1, random_state = 100)\n    \n    elif model_type == 'SVM':\n        \n        ml = SVC(C = 1000, kernel = 'rbf', probability = True, random_state = 100)\n    \n    elif model_type == 'Decision Tree':\n        \n        ml = DecisionTreeClassifier(max_depth = 7, min_samples_split = 26, min_samples_leaf = 46, random_state = 100)\n    \n    elif model_type == 'Random Forest':\n        \n        ml = RandomForestClassifier(n_estimators = 900, max_depth = 29, min_samples_split = 2, min_samples_leaf = 1,\n                                    n_jobs = -1, random_state = 100)\n    \n    elif model_type == 'Adaboost':\n        \n        dt = DecisionTreeClassifier(max_depth = 17, min_samples_split = 3, min_samples_leaf = 1, random_state = 100)\n\n        ml = AdaBoostClassifier(base_estimator = dt, n_estimators = 300, learning_rate = 0.3, random_state = 100)\n    \n    elif model_type == 'XGBoost':\n        \n        ml = XGBClassifier(max_depth = 18, learning_rate = 0.3, n_estimators = 900, min_child_weight = 1,\n                           subsample = 0.9, colsample_bytree = 0.4, n_jobs = -1, random_state = 100)\n    \n    elif model_type == 'LightGBM':\n        \n        ml = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600,\n                            min_child_samples = 20, subsample = 0.1, colsample_bytree = 1,\n                            objective = 'binary', n_jobs = -1, random_state = 100)\n    \n    if model_type not in model_type_list:\n        \n        gc = GridSearchCV(estimator = ml, param_grid = param_grid, scoring = 'roc_auc',\n                          n_jobs = 15, cv = 5, verbose = 2, return_train_score=True)\n    \n    else:\n        \n        gc = ml\n    \n    gc = gc.fit(X.values, y.values.ravel())\n    \n    if model_type in model_type_list:\n        \n        return sampling_type, gc\n    \n    else:\n        \n        return gc","31eb6899":"# Custom Function to plot GridSearch Result to get the best value\n\ndef hypertuning_plot(scores, parameter):\n    \n    col = \"param_\" + parameter\n    \n    plt.figure()\n    \n    \n    plt.plot(scores[col], scores[\"mean_train_score\"], label = \"training accuracy\")\n    plt.plot(scores[col], scores[\"mean_test_score\"], label = \"test accuracy\")\n    \n    plt.xlabel(parameter)\n    plt.ylabel(\"Accuracy\")\n    \n    plt.legend()\n    plt.show()","52c594ce":"# Custom Function to Train\/evaluate models for each of tranformed datasets\n\ndef evaluate_train(scores, benchmark_scores, dataset, X_test):\n    \n    # Plot ROC and PR curves using all models and test data\n    fig, axes = plt.subplots(1, 2, figsize = (14, 6))\n\n    # train models based on benchmark params\n    for sampling_type, reg in benchmark_scores:\n\n        print(\"Training on {}\".format(sampling_type))\n\n        for s_type, X, y in dataset:\n\n            if s_type == sampling_type:\n\n                pred_test = reg.predict(X_test.values)\n\n                probs = reg.predict_proba(X_test.values)[:, 1:]\n\n                fpr, tpr, thresholds = roc_curve(y_test.values.ravel(), pred_test)\n                p, r, t = precision_recall_curve(y_test.values.ravel(), probs)\n\n                model_f1_score = f1_score(y_test.values.ravel(), pred_test)\n                model_precision_score = precision_score(y_test.values.ravel(), pred_test)\n                model_recall_score = recall_score(y_test.values.ravel(), pred_test)\n                model_accuracy_score = accuracy_score(y_test.values.ravel(), pred_test)\n\n                scores.append((sampling_type,\n                               model_f1_score,\n                               model_precision_score,\n                               model_recall_score,\n                               model_accuracy_score,\n                               confusion_matrix(y_test.values.ravel(), pred_test)))\n\n\n    axes[0].plot([0, 1], [0, 1], 'k--')\n    axes[0].legend(loc = \"lower right\")\n    axes[0].set_xlabel(\"False Positive Rate\")\n    axes[0].set_ylabel(\"True Positive Rate\")\n\n    axes[1].legend()\n    axes[1].set_xlabel(\"recall\")\n    axes[1].set_ylabel(\"precision\")\n    axes[1].set_title(\"PR curve\")\n\n    plt.tight_layout()\n    plt.show()\n    \n    return scores","0b11d94a":"def get_different_sampling_type_results(regression_type, X_test):\n    \n    # Apply Logistic Regression with on Different Sampling Types\n\n    benchmark_scores = []\n    \n    param_grid = {}\n\n    for sample_type, X, y in dataset:\n\n        print('Performing {}'.format(sample_type))\n\n        benchmark_scores.append(model(sample_type, X, y, param_grid, regression_type))\n    \n    print(' ')\n    \n    # Train\/evaluate models for each of tranformed datasets\n    scores = []\n\n    scores = evaluate_train(scores, benchmark_scores, dataset, X_test)\n\n    # Tabulate results\n    sampling_results = pd.DataFrame(scores, columns = ['Sampling Type', 'f1', 'precision', 'recall', 'accuracy',\n                                                       'confusion_matrix'])\n    \n    return sampling_results","08fbbd68":"def perform_PCA(X):\n    \n    pca = PCA(svd_solver = 'randomized', random_state = 100)\n\n    #Doing the PCA on the train data\n    pca.fit(X)\n    \n    # Making the screeplot - plotting the cumulative variance against the number of components\n    fig = plt.figure(figsize = (12,8))\n\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()","3d4f86d1":"def perform_incremental_PCA(X_train, y_train, components):\n    \n    # Using incremental PCA for efficiency - saves a lot of time on larger datasets\n\n    pca_final = IncrementalPCA(n_components = components)\n\n    X_train_pca = pca_final.fit_transform(X_train)\n    X_test_pca = pca_final.transform(X_test)\n\n    X_train_pca = pd.DataFrame(data = X_train_pca)\n    X_test_pca = pd.DataFrame(data = X_test_pca)\n\n    print(X_train_pca.shape)\n    print(y_train.shape)\n    print(X_test_pca.shape)\n    print(y_test.shape)\n    \n    #creating correlation matrix for the principal components\n    corrmat = np.corrcoef(X_train_pca.transpose())\n\n    #plotting the correlation matrix\n    plt.figure(figsize = (20,10))\n    sns.heatmap(corrmat, annot = True)\n    plt.show()\n    \n    # 1s -> 0s in diagonals\n    corrmat_nodiag = corrmat - np.diagflat(corrmat.diagonal())\n    print(\"max corr:\",corrmat_nodiag.max(), \", min corr: \", corrmat_nodiag.min(),)\n    # we see that correlations are indeed very close to 0\n    \n    return X_train_pca, X_test_pca","46f46472":"perform_PCA(X_train)","c00183f7":"X_train_pca, X_test_pca = perform_incremental_PCA(X_train, y_train, 5)","478d17dc":"# Implement SMOTEENN\nX_train_resampled, y_train_resampled = implement_smoteenn(X_train_pca, y_train)","31ff7260":"# Logistic Regression with Default Parameters\n\nlr = LogisticRegression(n_jobs = -1, random_state = 100)\n\nlr = lr.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lr, X_test_pca)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                    'confusion_matrix'])\nsampling_results","c8f07360":"# GridSearchCV to find best penalty\n\nlr = LogisticRegression(n_jobs = -1, random_state = 100)\n\nparameter = 'penalty'\n\n# parameters to build the model on\nparam_grid = {parameter: ['l1', 'l2']}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lr)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","c3b7b722":"# GridSearchCV to find best C\n\n# Penalty remains the same and hence any penalty method (l1 or l2) can be taken into consideration.\n\nlr = LogisticRegression(penalty = 'l2', n_jobs = -1, random_state = 100)\n\nparameter = 'C'\n\n# parameters to build the model on\nparam_grid = {parameter: [1, 10, 100, 1000]}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lr)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","5f78dc64":"# Logistic with best parameters obtained from grid search\n\nlr = LogisticRegression(penalty = 'l2', C = 10, n_jobs = -1, random_state = 100)\n\nlrf = lr.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lrf, X_test_pca)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                  'confusion_matrix'])\nsampling_results","4063bf4b":"# Apply Class Imbalance transformations to dataset\n\ndataset = []\n\ndataset = apply_transformations(dataset, X_train_resampled, y_train_resampled)","13842de6":"# Get Different Sampling Results\n\nsampling_results = get_different_sampling_type_results('Logistic', X_test_pca)\n\n# Tabulate results\nsampling_results","75e3f864":"# SVM with Default Parameters and probability = True\n\nsvm = SVC(kernel = 'rbf', probability = True, random_state = 100)\n\nsvm = svm.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, svm, X_test_pca)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                    'confusion_matrix'])\nsampling_results","01a82361":"# GridSearchCV to find best min_samples_split\n\nsvm = SVC(kernel = 'rbf', probability = True, random_state = 100)\n\nparameter = 'C'\n\n# parameters to build the model on\nparam_grid = {parameter: [100, 200]}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', svm)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","9ec18439":"# SVM with best parameters obtained from grid search\n\nsvm = SVC(C = 1000, kernel = 'rbf', probability = True, random_state = 100)\n\nsvm = svm.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, svm, X_test_pca)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                  'confusion_matrix'])\nsampling_results","4ef73dc1":"# Get Different Sampling Results\nsampling_results = get_different_sampling_type_results('SVM', X_test_pca)\n\n# Tabulate results\nsampling_results","0a3dd366":"# Implement SMOTEENN on the whole train Dataset\nX_train_resampled, y_train_resampled = implement_smoteenn(X_train, y_train)","cf2baa85":"# Adaboost with Default Parameters\n\ntree = DecisionTreeClassifier(max_depth = 8, min_samples_split = 9, min_samples_leaf = 29, random_state = 100)\n\nabc = AdaBoostClassifier(base_estimator = tree, random_state = 100)\n\nabc = abc.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, abc, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'confusion_matrix'])\nsampling_results\n","2efe6cc7":"# GridSearchCV to find best n_estimators\n\ntree = DecisionTreeClassifier(max_depth = 7, min_samples_split = 9, min_samples_leaf = 29, random_state = 100)\n\nabc = AdaBoostClassifier(base_estimator = tree, random_state = 100)\n\nparameter = 'n_estimators'\n\nparam_grid = {parameter: range(900, 1100, 100)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', abc)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","1e4deb34":"# GridSearchCV to find best learning_rate\n\ntree = DecisionTreeClassifier(max_depth = 8, min_samples_split = 9, min_samples_leaf = 29, random_state = 100)\n\nabc = AdaBoostClassifier(base_estimator = tree, n_estimators = 1000, random_state = 100)\n\nparameter = 'learning_rate'\n\nparam_grid = {parameter: np.arange(0.8, 1, 0.1)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', abc)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","993ce25e":"# AdaBoost with best parameters obtained from grid search\n\ntree = DecisionTreeClassifier(max_depth = 8, min_samples_split = 9, min_samples_leaf = 29, random_state = 100)\n\nabc = AdaBoostClassifier(base_estimator = tree, n_estimators = 1000, learning_rate = 0.9, random_state = 100)\n\nabc = abc.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, abc, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'confusion_matrix'])\nsampling_results","773c34bd":"# LightGBM with Default Parameters\n\nlgb = LGBMClassifier(objective = 'binary', n_jobs = -1, random_state = 100)\n\nlgb = lgb.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'confusion_matrix'])\nsampling_results","62c0563b":"# GridSearchCV to find optimal num_leaves\n\nlgb = LGBMClassifier(objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'num_leaves'\n\nparam_grid = {parameter: range(65, 72)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","39755d01":"# GridSearchCV to find optimal max_depth\n\nlgb = LGBMClassifier(num_leaves = 70, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'max_depth'\n\nparam_grid = {parameter: range(28, 32)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","cef76df0":"# GridSearchCV to find optimal learning_rate\n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'learning_rate'\n\nparam_grid = {parameter: np.arange(0.4, 0.8, 0.1)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","e95ac1fd":"# GridSearchCV to find optimal n_estimators\n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, objective = 'binary',\n                     n_jobs = -1, random_state = 100)\n\nparameter = 'n_estimators'\n\nparam_grid = {parameter: range(500, 800, 100)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","0c88fad7":"# GridSearchCV to find optimal min_child_samples\n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600,\n                     objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'min_child_samples'\n\nparam_grid = {parameter: range(18, 24)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","f6b1293b":"# GridSearchCV to find optimal subsample: \n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600, min_child_samples = 20,\n                     objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'subsample'\n\nparam_grid = {parameter: np.arange(0.1, 1.1, 0.1)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","266884df":"# GridSearchCV to find optimal colsample_bytree: \n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600, min_child_samples = 20,\n                     subsample = 0.1, objective = 'binary', n_jobs = -1, random_state = 100)\n\nparameter = 'colsample_bytree'\n\nparam_grid = {parameter: np.arange(0.8, 1.1, 0.1)}\n\ngc = model('SMOTEENN', X_train_resampled, y_train_resampled, param_grid, 'Individual', lgb)\n    \n# scores of GridSearch CV\nscores = gc.cv_results_\n\n# Plot the scores\nhypertuning_plot(scores, parameter)\n\n# Get the best value\ngc.best_params_","da1a920a":"# LightGBM with best parameters obtained from grid search\n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600, min_child_samples = 20,\n                     subsample = 0.1, colsample_bytree = 1, objective = 'binary', n_jobs = -1, random_state = 100)\n\nlgb = lgb.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'confusion_matrix'])\nsampling_results","d748d99b":"# Sampling through RandomOverSampler\nsampling = RandomOverSampler(random_state = 100)\nX_train_resampled, y_train_resampled = sampling.fit_resample(X_train.values, y_train.values.ravel())\n\n# Shuffle the data\nperms = np.random.permutation(X_train_resampled.shape[0])\nX_train_resampled = X_train_resampled[perms]\ny_train_resampled = y_train_resampled[perms]\n\nX_train_resampled = pd.DataFrame(data = X_train_resampled)\ny_train_resampled = pd.DataFrame(data = y_train_resampled)\n\nprint(X_train_resampled.shape)\nprint(y_train_resampled.shape)","af732468":"# Running LightGBM with RandomOverSampler Technique to get the final model scores.\n\nlgb = LGBMClassifier(num_leaves = 70, max_depth = 29, learning_rate = 0.5, n_estimators = 600, min_child_samples = 20,\n                     subsample = 0.1, colsample_bytree = 1, objective = 'binary', n_jobs = -1, random_state = 100)\n\nlgb = lgb.fit(X_train_resampled.values, y_train_resampled.values.ravel())\n\n# Get the Score Metrics and plots\nscores = []\n\nscores = get_scores(scores, lgb, X_test)\n\n# Tabulate results\nsampling_results = pd.DataFrame(scores, columns = ['f1', 'precision', 'recall', 'accuracy',\n                                                   'confusion_matrix'])\nsampling_results","75151227":"feature_importances = pd.DataFrame(lgb.feature_importances_,\n                                   index = X_train.columns,\n                                    columns=['importance']).sort_values('importance',ascending=False)\n\nfeature_importances","555cda62":"# Plotting the features\nplt.figure(figsize=(20, 19))\n\nsns.barplot(x = \"importance\",\n            y = feature_importances.index,\n            data = feature_importances.sort_values(by = \"importance\", ascending = False))\n\nplt.title('LightGBM Features (avg over folds)')\nplt.tight_layout()\nplt.show()","78d01e9b":"# I have reduced the parameter range for most of the hyper-parameters as it takes a lot of time to run on kaggle\n#based on the findings I had by running the code on my system, generally you should take a wider range of hyper-params \n#e.x. - learning rate = 0.1 - 1.1 \n#e.x. - min-child-samples = 10 - 30","7c6d1ca6":"# Data Preparation\n","1ac30b7f":"There is a significant drop in the columns for data in 8th month for churned customers.","0f0b18da":"#### Running the SVM with best parameters obtained from grid search.","da8a577c":"Dimensionality Reduction","c2a88fa9":"So after filtering out the high-value customers we are left with 30027 observations. The shape of the telecom dataset is now\n(30027, 230)","cd6c4bc8":"### Data Insight\n\nAlso an observation with mobile internet usage having a missing value would mostly mean that the customer is not using that particular service. This would also mean that the person would not be using any add-on services that would require a mobile internet pack.\n\nSo with this inference we can impute the missing values related to columns for mobile data with 0.\n","9f465c22":"# Treating the imbalanced data","19ff92c5":"\n\n# Data Cleaning & Missing Values Treatment","074fcf8d":"We will follow the same logic for **'total_rech_data_7', av_rech_amt_data_7,  'total_rech_data_8' & 'av_rech_amt_data_8'** columns as well.","88762c62":" - RandomUnderSampler leads to high recall but comes at a huge cost to precision (also reduces training time).\n - SMOTE and RandomOverSampler perform good considering auc_roc and auc_pr with accpetables levels of false positives.\n\n**None of the Sampling Type scores can be accepted as they are very low. We will proceed with some different Machine Learning algorithms to increase the scores (f1, precision, recall, accuracy, auc_roc).**","6f6808f2":"## Filtering high-value customers","7a3aab47":"As there are 3 date columns we see a difference in result. We will keep the date columns for now and deal with it at a later point. Lets drop only the non-date columns now.","4dc539ad":"Lets check for the missing values %-age again","20f1549e":"# Problem Statement\n# Business problem overview\nIn the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n\n \n\nFor many incumbent operators, retaining high profitable customers is the number one business goal.\n\n \n\n# To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n\n \n\nIn this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.\n\n \n\nUnderstanding and defining churn\nThere are two main models of payment in the telecom industry - postpaid (customers pay a monthly\/annual bill after using the services) and prepaid (customers pay\/recharge with a certain amount in advance and then use the services).\n\n \n\nIn the postpaid model, when customers want to switch to another operator, they usually inform the existing operator to terminate the services, and you directly know that this is an instance of churn.\n\n \n\nHowever, in the prepaid model, customers who want to switch to another network can simply stop using the services without any notice, and it is hard to know whether someone has actually churned or is simply not using the services temporarily (e.g. someone may be on a trip abroad for a month or two and then intend to resume using the services again).\n\n \n\nThus, churn prediction is usually more critical (and non-trivial) for prepaid customers, and the term \u2018churn\u2019 should be defined carefully.  Also, prepaid is the most common model in India and Southeast Asia, while postpaid is more common in Europe in North America.\n\n \n\nThis project is based on the Indian and Southeast Asian market.\n\n \n\n# Definitions of churn\nThere are various ways to define churn, such as:\n\n1. Revenue-based churn: Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as \u2018customers who have generated less than INR 4 per month in total\/average\/median revenue\u2019.\n\n \n\n2. The main shortcoming of this definition is that there are customers who only receive calls\/SMSes from their wage-earning counterparts, i.e. they don\u2019t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n\n \n\n3. Usage-based churn: Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n\n \n\nA potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a \u2018two-months zero usage\u2019 period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n\n \n\n# In this project, you will use the usage-based definition to define churn.\n\n \n\n## High-value churn\nIn the Indian and the Southeast Asian market, approximately 80% of revenue comes from the top 20% customers (called high-value customers). Thus, if we can reduce churn of the high-value customers, we will be able to reduce significant revenue leakage.\n\n \n\nIn this project, you will define high-value customers based on a certain metric (mentioned later below) and predict churn only on high-value customers.\n\n \n\n## Understanding the business objective and the data\nThe dataset contains customer-level information for a span of four consecutive months - June, July, August and September. The months are encoded as 6, 7, 8 and 9, respectively. \n\n\nThe business objective is to predict the churn in the last (i.e. the ninth) month using the data (features) from the first three months. To do this task well, understanding the typical customer behaviour during churn will be helpful.\n\n \n\n## Understanding customer behaviour during churn\nCustomers usually do not decide to switch to another competitor instantly, but rather over a period of time (this is especially applicable to high-value customers). In churn prediction, we assume that there are three phases of customer lifecycle :\n\n1. The \u2018good\u2019 phase: In this phase, the customer is happy with the service and behaves as usual.\n\n2. The \u2018action\u2019 phase: The customer experience starts to sore in this phase, for e.g. he\/she gets a compelling offer from a  competitor, faces unjust charges, becomes unhappy with service quality etc. In this phase, the customer usually shows different behaviour than the \u2018good\u2019 months. Also, it is crucial to identify high-churn-risk customers in this phase, since some corrective actions can be taken at this point (such as matching the competitor\u2019s offer\/improving the service quality etc.)\n\n3. The \u2018churn\u2019 phase: In this phase, the customer is said to have churned. You define churn based on this phase. Also, it is important to note that at the time of prediction (i.e. the action months), this data is not available to you for prediction. Thus, after tagging churn as 1\/0 based on this phase, you discard all data corresponding to this phase.\n\n \n\nIn this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","a10d7ac8":"We need to drop the above list of last month columns entirely.","a3210ceb":"#### Logistic Regression - HyperTuning C","2abc300e":"### <font color = brown>2. Rows<\/font>\n\n#### Now lets analyze the revenue columns to check if there are any discrepencies in the data.","f87abe90":"### Data :\n\n**From the above bar plot it is evident that the dataset is highly imbalanced. ","13493988":"# <font color = green>Business Actions<\/font>\n\n1. Less number of **high value customer** are churing but for last **6 month** no new high valued cusotmer is onboarded which is concerning and company should concentrate on that aspect.\n\n2. Customers with less than 4 years of tenure are more likely to churn and company should concentrate more on that segment by rolling out new schems to that group.\n\n3. Average revenue per user seems to be most important feature in determining churn prediction.\n\n4. Incoming and Outgoing Calls on romaing for 8th month are strong indicators of churn behaviour\n\n5. Local Outgoing calls made to landline , fixedline , mobile and call center provides a strong indicator of churn behaviour.\n\n6. Better 2G\/3G area coverage where 2G\/3G services are not good, it's strong indicator of churn behaviour.","0eb4ff0d":"So from the stats for the 3 columns above we know that the maximum recharge amount is 1555 and the minimun amount is 1. Thus the missing values can be infered as **No recharge was done** and can be imputed with 0 for all the 3 columns.","1b4b11ad":"**The score is very low. Let's see the effect of other sampling techniques.**","d853f27f":"Variables `night_pck_user_6`, `night_pck_user_7`, `night_pck_user_8`, `fb_user_6`, `fb_user_7` and `fb_user_8` are encoded with number 0 and 1. These variables can be considered as **Ordered Categorical** columns.\n\nAlso, the datatype of these variables can be converted to integer.","d5156a31":"Looks like 5 components are enough to describe 95% of the variance in the dataset. Let's choose 5 components for modeling.","de42b6d6":"#### Logistic Regression - HyperTuning Penalty","d5487b71":"Logistic regression","321c58d5":"### Data Insight\n\n**Looking at the problem statement, attributes `total_ic_mou_9`, `total_og_mou_9`, `vol_2g_mb_9` and `vol_3g_mb_9` are used to tag churners. \n\nLet's now drop all those individual columns whose totals are available.","a0a79b25":"### Data Insight\n\n- There are many features that are highly correlated.\n- Total of 25 features with a correlation > 0.80","23416962":"Now filtering out the values we get -","6849fa8e":"From the `night_pck_user` variables, it can be inferred that almost 99% of users are not using nightly pack in all the 3 months.\n\nHowever users are using facebook(fb) `fb_user` in equal percentage and it is observed that as the month increases, there is a decline in the fb usage.","020f66f4":"## LIGHTGBM","b624f946":"### Data Insight\n\n**Also one more insight that the above figures give us is that towards the 8th month the number of missing values increases and this could possibly mean that the customer might have stopped using these services. This could be an indication that the particular customer is more likely to churn.**\n\nSo not using the services means the minutes of usage can be imputed with 0.","65695aec":"There are lot of Outliers present in the variables. We will remove these outliers by performaing normalization in the latter stage.","e692c846":"Now let's have a look at the 3 date columns we had excluded above","712e2afb":"**There is no correlation between any two components.**\n\n#### Now Let's handle Class Imbalance on PCA derived data","2cc77a7c":"### Duplicate Row Check\n\n#### Let's now check if all the rows are unique or not. This can be done by verifying the Unique Key - `mobile_number`","edb10f65":"## Applying adaboost","78ae8238":"So there are 34 columns where 70% data is missing. Now this 70% data is based out of the complete data which contains about 1 lac observations. But our objective is to find the churn rate for only **high valued** customers. It has been given that after filtering high-valued customers we would be left with about 29.9k rows. So dropping the variables now might not be a good idea. ","d2d8834a":"GRID SEARCH","f1c8d469":"We have removed missing values from the columns which had > 40% missing data.","4f117f7d":"## <font color = brown>4. Model Building<\/font>","f3983b1b":"So any customer who does not use any of the facility (calls or mobile data) during the 9th month, will have the row sum for the attributes equal to zero. This customer can be tagged as **Churn (1)** else the customer will be tagged as **Not Churn (0)**.","b6f5f8cf":"#### EDA\n","5b516ae9":"## SVM","04c12f99":"# Modelling","10978870":"# <font color = green>Final Inference<\/font>\n\nTo predict and analyse if a customer will churn or not we built 2 predictive models as mentioned below -  \n\n1. Logistic Regression Model(With and without PCA).\n2. Support Vector Machine.\n\nThe above models were initially created with default parameters which did not give good results hence we built boosting models which are as follows:\n\n1. AdaBoostClassifier.\n2. Light Gradient Boosting Machine with LightGBM.\n\nThe above boosting models were initially created with default parameters which performed better than any of the logistic regression model and SVM. The classification scores and metrics shoot up very high after we did some tuning.\n\nOut of all the above models, **Light Gradient Boosting Machine with LightGBM**  produce best accuracy and it can be selected to predict churn data for future dataset or production.\n\n\nTop 7 Features affecting churn\n\n- aon\t\n- loc_ic_mou_action_phase\t\n-  arpu_action_phase\t\n- std_ic_mou_action_phase\t\n- max_rech_amt_action_phase\t\n- loc_ic_mou_good_phase\t\n- total_ic_mou_good_phase\t\n\n\n\n\n","d80ad8e7":"# Rename Columns","15cd7c86":"Let's again look at the columns with missing values - ","4c5cae2c":"# SVM - HyperTuning"}}