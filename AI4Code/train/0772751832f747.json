{"cell_type":{"0dacb2bc":"code","dad41289":"code","8203634e":"code","364c2704":"code","060e8d16":"code","061f9db9":"code","d5c5a934":"code","afd5dcf4":"code","95f222f3":"code","ea23e4db":"code","81a8ea3d":"code","623d3a1f":"code","bb29fd43":"code","83832cfd":"code","dfba1884":"code","ea8a49ad":"code","d226b7b9":"code","1737dcd5":"code","67ff7505":"code","b3ed615c":"code","d4d764aa":"code","3fb8d522":"markdown","5959d5e8":"markdown","50cb587c":"markdown","43e2c52f":"markdown","3cccb5a8":"markdown","440fd893":"markdown","f7f55925":"markdown","66bfa98e":"markdown"},"source":{"0dacb2bc":"!pip install wandb torchsummary easydict","dad41289":"import wandb\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient() \npersonal_key_for_api = user_secrets.get_secret(\"wandb-key\")\n\n! wandb login $personal_key_for_api","8203634e":"import os\nimport time\nimport random\nimport wandb\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport errno\nimport torchvision\nimport torch.optim as optim\n\nfrom mpl_toolkits.axes_grid1 import ImageGrid\nfrom torchsummary import summary\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms\nfrom easydict import EasyDict as edict\nfrom torch.utils.data import DataLoader\nfrom matplotlib import pyplot as plt","364c2704":"__C = edict()\n\ncfg = __C\n\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(f\"Run on device: {device}\")\n__C.PROJECT_NAME = \"DCGAN-Anime-Faces\"\n__C.PROJECT_VERSION_NAME = \"Standard DCGAN\"\n# Global\n__C.NUM_EPOCHS = 50\n__C.LEARNING_RATE = 0.0002\n__C.BATCH_SIZE = 128\n__C.DATASET_SIZE = None\n# CHANNELS\n__C.IMG_SIZE = 64\n__C.CHANNELS_IMG = 3\n__C.Z_DIMENSION = 128\n# Models\n__C.FEATURES_DISC = 64\n__C.FEATURES_GEN = 64\n# Paths and saves\n__C.SAVE_EACH_EPOCH = 25\n__C.OUT_DIR = ''\n__C.SAVE_CHECKPOINT_PATH = '\/kaggle\/working'\n\n# Display results\n__C.NUM_SAMPLES = 64    # size grid for display images\n__C.FREQ = 50","060e8d16":"def set_seed(val):\n    \"\"\"\n    Freezes random sequences\n    :param val: ``int`` random value\n    \"\"\"\n    random.seed(val)\n    np.random.seed(val)\n    torch.manual_seed(val)\n    torch.cuda.manual_seed(val)\n    \nset_seed(789)","061f9db9":"data_path = '\/kaggle\/input\/animefacedataset\/images'","d5c5a934":"class AnimeFacesDataset(Dataset):\n    def __init__(self, img_folder):\n        \"\"\"\n        :param img_folder: path to dataset folder\n        \"\"\"\n        self.img_folder = img_folder\n        self.img_names = [n for n in os.listdir(img_folder) if n.endswith(('png', 'jpeg', 'jpg'))]\n\n    def __getitem__(self, idx):\n        img = Image.open(os.path.join(self.img_folder, self.img_names[idx])).convert('RGB')\n        return self.transform(img)\n\n    def __len__(self):\n        return len(self.img_names)\n\n    @property\n    def transform(self):\n        return transforms.Compose([transforms.Resize(cfg.IMG_SIZE),\n                                   transforms.CenterCrop(cfg.IMG_SIZE),\n                                   transforms.RandomHorizontalFlip(),\n                                   transforms.ToTensor(),\n                                   transforms.Normalize(mean=[0.5, 0.5, 0.5],\n                                                        std=[0.5, 0.5, 0.5])\n                                   ])","afd5dcf4":"dataset = AnimeFacesDataset(data_path)\ndataloader = DataLoader(dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=2, drop_last=True, pin_memory=True)","95f222f3":"def show_batch(batch, num_samples=36, figsize=(10, 10), normalize=True):\n    images = batch[:num_samples, ...]\n    nrows = int(np.sqrt(num_samples))\n    grid = torchvision.utils.make_grid(images, nrow=nrows, normalize=normalize, scale_each=True)\n    fig = plt.figure(figsize=figsize)\n    plt.imshow(np.moveaxis(grid.detach().cpu().numpy(), 0, -1), aspect='auto')\n    plt.axis('off')","ea23e4db":"batch = next(iter(dataloader))\nshow_batch(batch)","81a8ea3d":"class Generator(nn.Module):\n    \"\"\"Full convolution generator\"\"\"\n    def __init__(self, channels_noise, channels_img, features_gen):\n        \"\"\"\n        :param channels_noise: ``int``, input latent space dimension\n        :param channels_img: ``int``,  3 for RGB image or 1 for GrayScale\n        :param features_gen: ``int``, num features of generator\n        \"\"\"\n        super().__init__()\n        self.body = nn.Sequential(\n            Generator._default_block(channels_noise, features_gen * 8, 4, 1, 0),    # 4x4\n            Generator._default_block(features_gen * 8, features_gen * 4, 4, 2, 1),  # 8x8\n            Generator._default_block(features_gen * 4, features_gen * 2, 4, 2, 1),   # 16x16\n            Generator._default_block(features_gen * 2, features_gen, 4, 2, 1),   # 32x32\n            nn.ConvTranspose2d(\n                features_gen, channels_img, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n            # out dimension: [N x 3 x 64 x 64] with range [-1, 1]\n        )\n\n    @staticmethod\n    def _default_block(in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU(True)\n        )\n\n    def forward(self, x):\n        return self.body(x)\n    \n\"\"\"\nGenerator\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n   ConvTranspose2d-1            [-1, 512, 4, 4]       1,048,576\n       BatchNorm2d-2            [-1, 512, 4, 4]           1,024\n              ReLU-3            [-1, 512, 4, 4]               0\n   ConvTranspose2d-4            [-1, 256, 8, 8]       2,097,152\n       BatchNorm2d-5            [-1, 256, 8, 8]             512\n              ReLU-6            [-1, 256, 8, 8]               0\n   ConvTranspose2d-7          [-1, 128, 16, 16]         524,288\n       BatchNorm2d-8          [-1, 128, 16, 16]             256\n              ReLU-9          [-1, 128, 16, 16]               0\n  ConvTranspose2d-10           [-1, 64, 32, 32]         131,072\n      BatchNorm2d-11           [-1, 64, 32, 32]             128\n             ReLU-12           [-1, 64, 32, 32]               0\n  ConvTranspose2d-13            [-1, 3, 64, 64]           3,075\n             Tanh-14            [-1, 3, 64, 64]               0\n================================================================\nTotal params: 3,806,083\nTrainable params: 3,806,083\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.00\nForward\/backward pass size (MB): 3.00\nParams size (MB): 14.52\nEstimated Total Size (MB): 17.52\n----------------------------------------------------------------\n\"\"\"\nclass Discriminator(nn.Module):\n    \"\"\"Standard full convolution discriminator\"\"\"\n    def __init__(self, in_channels, features_d):\n        super().__init__()\n        self.body = nn.Sequential(\n            nn.Conv2d(in_channels, features_d, kernel_size=4, stride=2, padding=1, bias=False),\n            nn.LeakyReLU(0.2, inplace=True),\n            Discriminator._default_block(features_d, features_d * 2, 4, 2, 1),\n            Discriminator._default_block(features_d * 2, features_d * 4, 4, 2, 1),\n            Discriminator._default_block(features_d * 4, features_d * 8, 4, 2, 1),\n            nn.Conv2d(features_d * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n            nn.Sigmoid(),\n        )\n\n    @staticmethod\n    def _default_block(in_channels, out_channels, kernel_size, stride, padding):\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.LeakyReLU(0.2, inplace=True)\n        )\n\n    def forward(self, x):\n        return self.body(x)\n    \n\"\"\"\nDiscriminator\n----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1           [-1, 64, 32, 32]           3,136\n         LeakyReLU-2           [-1, 64, 32, 32]               0\n            Conv2d-3          [-1, 128, 16, 16]         131,072\n       BatchNorm2d-4          [-1, 128, 16, 16]             256\n         LeakyReLU-5          [-1, 128, 16, 16]               0\n            Conv2d-6            [-1, 256, 8, 8]         524,288\n       BatchNorm2d-7            [-1, 256, 8, 8]             512\n         LeakyReLU-8            [-1, 256, 8, 8]               0\n            Conv2d-9            [-1, 512, 4, 4]       2,097,152\n      BatchNorm2d-10            [-1, 512, 4, 4]           1,024\n        LeakyReLU-11            [-1, 512, 4, 4]               0\n           Conv2d-12              [-1, 1, 1, 1]           8,193\n          Sigmoid-13              [-1, 1, 1, 1]               0\n================================================================\nTotal params: 2,765,633\nTrainable params: 2,765,633\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 0.05\nForward\/backward pass size (MB): 2.31\nParams size (MB): 10.55\nEstimated Total Size (MB): 12.91\n----------------------------------------------------------------\n\"\"\"\n\ndef init_weights(model):\n    # Initializes weights according to the DCGAN paper\n    for m in model.modules():\n        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n            nn.init.normal_(m.weight.data, 0.0, 0.02)","623d3a1f":"class MetricLogger:\n    \"\"\"Metric class\"\"\"\n    def __init__(self, project_version_name, wab=True, show_accuracy=True, resume_id=False):\n        \"\"\"\n        :param project_version_name: name of current version of project\n        :param wab: good realtime metric, you can register free account in https:\/\/wandb.ai\/\n        :param show_accuracy: if True: show accuracy on real and fake data\n        \"\"\"\n        self.project_version_name = project_version_name\n        self.show_acc = show_accuracy\n        self.data_subdir = f\"{os.path.join(cfg.OUT_DIR, self.project_version_name)}\/imgdata\"\n        \n        self.g_loss = []\n        self.d_loss = []\n\n        if wab:\n            if resume_id:\n                wandb_id = resume_id\n            else:\n                wandb_id = wandb.util.generate_id()\n            wandb.init(id=wandb_id, project='DCGAN-Anime-Faces', name=project_version_name, resume=True)\n            wandb.config.update({\n                'train_images_count': cfg.DATASET_SIZE,\n                'init_lr': cfg.LEARNING_RATE,\n                'noise_z_size': cfg.Z_DIMENSION,\n                'batch_size': cfg.BATCH_SIZE,\n                'initialization_weights': 'Normal distribution',\n                'beta 1': 0.5,\n                'beta 2': 0.999\n            })\n\n    def display_status(self, epoch, num_epochs, batch_idx, num_batches, dis_loss,\n                       gen_loss, acc_real=None, acc_fake=None):\n        \"\"\"\n        Display training progress\n        :param epoch: ``int``, current epoch\n        :param num_epochs: ``int``, numbers epoch\n        :param batch_idx: ``int``, current batch\n        :param num_batches: ``int``, numbers bathes\n        :param dis_loss: ``torch.autograd.Variable``, discriminator loss\n        :param gen_loss: ``torch.autograd.Variable``, generator loss\n        :param acc_real: ``torch.autograd.Variable``, discriminator predicted on real data\n        :param acc_fake: ``torch.autograd.Variable``, discriminator predicted on fake data\n        \"\"\"\n        if isinstance(dis_loss, torch.autograd.Variable):\n            dis_loss = dis_loss.item()\n        if isinstance(gen_loss, torch.autograd.Variable):\n            gen_loss = gen_loss.item()\n        if self.show_acc and isinstance(acc_real, torch.autograd.Variable):\n            acc_real = acc_real.float().mean().item()\n        if self.show_acc and isinstance(acc_fake, torch.autograd.Variable):\n            acc_fake = acc_fake.float().mean().item()\n\n        print('Batch Num: [{}\/{}], Epoch: [{}\/{}]'.format(batch_idx, num_batches, epoch, num_epochs))\n        print('Discriminator Loss: {:.4f}, Generator Loss: {:.4f}'.format(dis_loss, gen_loss))\n        if acc_real and acc_fake:\n            print('D(x): {:.4f}, D(G(z)): {:.4f}'.format(acc_real, acc_fake))\n\n    def log(self, dis_loss, gen_loss, acc_real=None, acc_fake=None):\n        \"\"\"\n        Logging values\n        :param dis_loss: ``torch.autograd.Variable``, critical loss\n        :param gen_loss: ``torch.autograd.Variable``, generator loss\n        :param acc_real: ``torch.autograd.Variable``, D(x) predicted on real data\n        :param acc_fake: ``torch.autograd.Variable``, D(G(z)) paramredicted on fake data\n        \"\"\"\n        if isinstance(dis_loss, torch.autograd.Variable):\n            dis_loss = dis_loss.item()\n        if isinstance(gen_loss, torch.autograd.Variable):\n            gen_loss = gen_loss.item()\n        if self.show_acc and isinstance(acc_real, torch.autograd.Variable):\n            acc_real = acc_real.float().mean().item()\n        if self.show_acc and isinstance(acc_fake, torch.autograd.Variable):\n            acc_fake = acc_fake.float().mean().item()\n\n        wandb.log({'d_loss': dis_loss, 'g_loss': gen_loss, 'D(x)': acc_real, 'D(G(z))': acc_fake})\n        self.g_loss.append(gen_loss)\n        self.d_loss.append(dis_loss)\n\n    def log_image(self, images, num_samples, epoch, batch_idx, num_batches, normalize=True):\n        \"\"\"\n        Create image grid and save it\n        :param images: ``Tor    ch.Tensor(N,C,H,W)``, tensor of images\n        :param num_samples: ``int``, number of samples\n        :param normalize: if True normalize images\n        :param epoch: ``int``, current epoch\n        :param batch_idx: ``int``, current batch\n        :param num_batches: ``int``, numbers bathes\n        \"\"\"\n        images = images[:num_samples, ...]\n        nrows = int(np.sqrt(num_samples))\n        grid = torchvision.utils.make_grid(images, nrow=nrows, normalize=normalize, scale_each=True)\n        self.save_torch_images(grid, epoch, batch_idx, num_batches)\n        wandb.log({'fixed_noise': [wandb.Image(np.moveaxis(grid.detach().cpu().numpy(), 0, -1))]})\n\n    def save_torch_images(self, grid, epoch, batch_idx, num_batches):\n        \"\"\"\n        Display and save image grid\n        :param grid: ``ndarray``, grid image\n        :param epoch: ``int``, current epoch\n        :param batch_idx: ``int``, current batch\n        :param num_batches: ``int``, numbers bathes\n        \"\"\"\n        out_dir = self.data_subdir\n        fig = plt.figure(figsize=(16, 16))\n        plt.imshow(np.moveaxis(grid.detach().cpu().numpy(), 0, -1), aspect='auto')\n        plt.axis('off')\n        MetricLogger._save_images(fig, out_dir, epoch, batch_idx, num_batches)\n        plt.close()\n\n    @staticmethod\n    def _save_images(fig, out_dir, epoch, batch_idx, num_batches):\n        \"\"\"\n        Saves image on drive\n        :param fig: pls.figure object\n        :param out_dir: path to output dir\n        :param epoch: ``int``, current epoch\n        :param batch_idx: ``int``, current batch\n        :param num_batches: ``int``, numbers bathes\n        \"\"\"\n        MetricLogger._make_dir(out_dir)\n        image_name = f\"epoch({str(epoch).zfill(len(str(cfg.NUM_EPOCHS)))})-\" \\\n                     f\"batch({str(batch_idx).zfill(len(str(num_batches)))}).jpg\"\n        fig.savefig('{}\/{}'.format(out_dir, image_name))\n\n    @staticmethod\n    def _make_dir(directory):\n        try:\n            os.makedirs(directory)\n        except OSError as e:\n            if e.errno != errno.EEXIST:\n                raise","bb29fd43":"def get_random_noise(size, dim, device):\n    \"\"\"\n     Get random noise from normal distribution\n    :param size: ``int``, number of samples (batch)\n    :param dim: ``int``, dimension\n    :param device: cuda or cpu device\n    :return: Tensor([size, dim, 1, 1])\n    \"\"\"\n    return torch.randn(size, dim, 1, 1).to(device)\n\n\ndef checkpoint(epoch, end_epoch, gen, disc, opt_gen, opt_disc, fixed_noise):\n    print(\"=> Saving checkpoint\")\n    torch.save({\n        'gen': gen.state_dict(),\n        'disc': disc.state_dict(),\n        'opt_gen': opt_gen.state_dict(),\n        'opt_disc': opt_disc.state_dict(),\n        'start_epoch': epoch,\n        'end_epoch': end_epoch,\n        'fixed_noise': fixed_noise\n    }, f\"{cfg.SAVE_CHECKPOINT_PATH}\/DCGAN_epoch_{epoch}.pth.tar\")\n    print(f\"=> Checkpoint save to {cfg.SAVE_CHECKPOINT_PATH}\")\n\n\ndef load_checkpoint(checkpoint, gen, disc, opt_gen, opt_disc):\n    print(\"=> Load checkpoint...\")\n    gen.load_state_dict(checkpoint['gen'])\n    disc.load_state_dict(checkpoint['disc'])\n    opt_gen.load_state_dict(checkpoint['opt_gen'])\n    opt_disc.load_state_dict(checkpoint['opt_disc'])\n    print(\"=> Checkpoint loaded\")\n    return checkpoint['start_epoch'], checkpoint['end_epoch'], checkpoint['fixed_noise']","83832cfd":"def epoch_time(f):\n    \"\"\"Calculate time of each epoch\"\"\"\n    def timed(*args, **kwargs):\n        ts = time.time()\n        result = f(*args, **kwargs)\n        te = time.time()\n        print(\"epoch time: %2.1f min\" % ((te-ts)\/60))\n        return result\n    return timed\n\n\n@epoch_time\ndef train_one_epoch(epoch, dataloader, gen, disc, criterion, opt_gen, opt_disc,\n                    fixed_noise, device, metric_logger, num_samples, freq=100):\n    \"\"\"\n    Train one epoch\n    :param epoch: ``int`` current epoch\n    :param dataloader: object of dataloader\n    :param gen: Generator model\n    :param disc: Discriminator model\n    :param criterion: Loss function (for this case: binary cross entropy)\n    :param opt_gen: Optimizer for generator\n    :param opt_disc: Optimizer for discriminator\n    :param fixed_noise: ``tensor[[cfg.BATCH_SIZE, latent_space_dimension, 1, 1]]`` fixed noise (latent space) for image metrics\n    :param device: cuda device or cpu\n    :param metric_logger: object of MetricLogger\n    :param num_samples: ``int`` well retrievable sqrt() (for example: 4, 16, 64) for good result,\n    number of samples for grid image metric\n    :param freq: ``int``, freq < len(dataloader)`` freq for display results\n    \"\"\"\n    for batch_idx, img in enumerate(dataloader):\n        real = img.to(device)\n        noise = get_random_noise(cfg.BATCH_SIZE, cfg.Z_DIMENSION, device)\n        fake = gen(noise)\n\n        # Train discriminator: We maximize log(D(x)) + log(1 - D(G(z))\n        disc_real = disc(real).reshape(-1)\n        loss_disc_real = criterion(disc_real, torch.ones_like(disc_real))\n        disc_fake = disc(fake.detach()).reshape(-1)\n        loss_disc_fake = criterion(disc_fake, torch.zeros_like(disc_fake))\n        loss_disc = (loss_disc_real + loss_disc_fake) \/ 2\n        disc.zero_grad()\n        loss_disc.backward()\n        opt_disc.step()\n\n        # Train generator: We minimize log(1 - D(G(z))). This is the same as maximize log(D(G(z))\n        output = disc(fake).reshape(-1)\n        loss_gen = criterion(output, torch.ones_like(output))\n        gen.zero_grad()\n        loss_gen.backward()\n        opt_gen.step()\n        # logs metrics\n        if batch_idx % freq == 0:\n            with torch.no_grad():\n                metric_logger.log(loss_disc, loss_gen, disc_real, disc_fake)\n                fake = gen(fixed_noise)\n                metric_logger.log_image(fake, num_samples, epoch, batch_idx, len(dataloader))\n                metric_logger.display_status(epoch, cfg.NUM_EPOCHS, batch_idx, len(dataloader), loss_disc, loss_gen, disc_real, disc_fake)","dfba1884":"# defininig models\ngen = Generator(cfg.Z_DIMENSION, cfg.CHANNELS_IMG, cfg.FEATURES_GEN).to(device)\ndisc = Discriminator(cfg.CHANNELS_IMG, cfg.FEATURES_DISC).to(device)\n# init weights\n#init_weights(gen)\n#init_weights(disc)\n# defining optimizers\nopt_gen = optim.Adam(gen.parameters(), lr=cfg.LEARNING_RATE, betas=(0.5, 0.999))\nopt_disc = optim.Adam(disc.parameters(), lr=cfg.LEARNING_RATE, betas=(0.5, 0.999))","ea8a49ad":"start_epoch = 1\nend_epoch = cfg.NUM_EPOCHS\nfixed_noise = get_random_noise(cfg.BATCH_SIZE, cfg.Z_DIMENSION, device)","d226b7b9":"criterion = nn.BCELoss()\ngen.train()\ndisc.train()\nmetric_logger = MetricLogger(cfg.PROJECT_VERSION_NAME)","1737dcd5":"start_time = time.time()\nfor epoch in range(start_epoch, end_epoch + 1):\n    train_one_epoch(epoch, dataloader, gen, disc, criterion, opt_gen, opt_disc,\n                    fixed_noise, device, metric_logger, num_samples=cfg.NUM_SAMPLES, freq=cfg.FREQ)\n    if epoch == cfg.NUM_EPOCHS + 1:\n        checkpoint(epoch, end_epoch, gen, disc, opt_gen, opt_disc, fixed_noise)\n    elif epoch % cfg.SAVE_EACH_EPOCH == 0:\n        checkpoint(epoch, end_epoch, gen, disc, opt_gen, opt_disc, fixed_noise)\n\ntotal_time = time.time() - start_time\nprint(f\"=> Training time:{total_time}\")","67ff7505":"def get_interpolations(noise1, noise2, num_interpolates):\n    x = [((1.0 - (i\/num_interpolates)) * noise1) + ((i\/num_interpolates) * noise2) for i in range(num_interpolates + 1)]\n    return torch.cat(x, dim = 0)\n\n\ndef visualize_batch(images_list, size = 14, shape = (6, 6), title = None, save = None, return_images = False):\n    images_list = (images_list - images_list.min())\/(images_list.max() - images_list.min())\n\n    fig = plt.figure(figsize = (size, size))\n    grid = ImageGrid(fig, 111, nrows_ncols = shape, axes_pad = 0.04)\n    for ax, image in zip(grid, images_list):\n        ax.imshow(image.permute(1, 2, 0))\n        ax.axis('off')\n\n    if title:\n        print(title)\n    if save:\n        plt.savefig(save)\n    plt.show()\n    \n    if return_images:\n        return images_list","b3ed615c":"with torch.no_grad():\n    interpolations = []\n    for _ in range(10):\n        noise1 = get_random_noise(1, cfg.Z_DIMENSION, device)\n        noise2 = get_random_noise(1, cfg.Z_DIMENSION, device)\n        interpolations.append(get_interpolations(noise1, noise2, 9).to(device))\n    \n    interpolations = torch.cat(interpolations, dim = 0)\n    false_images = gen(interpolations).cpu()\n    print(f'\\nShowing Generated Samples\\n')\n    visualize_batch(false_images, shape = (10, 10), size = 16, save = None)","d4d764aa":"plt.figure(figsize=(10,5))\nplt.title(\"Generator and Discriminator Loss\")\nplt.plot(metric_logger.g_loss,label=\"G\")\nplt.plot(metric_logger.d_loss,label=\"D\")\nplt.xlabel(\"step\")\nplt.ylabel(\"value\")\nplt.legend()\nplt.show()","3fb8d522":"## Main train loop","5959d5e8":"## Defining models","50cb587c":"## Train function","43e2c52f":"## Defining metric class","3cccb5a8":"## Config values","440fd893":"## Defining dataset and dataloader","f7f55925":"# Utils","66bfa98e":"## Imports"}}