{"cell_type":{"6a3f3ba0":"code","7872c58b":"code","4fa9afab":"code","ba9b5bc9":"code","dba9de8d":"code","46714055":"code","3ff903f8":"code","f65647e3":"code","0d14e74b":"code","d566782d":"code","d84cffaf":"code","8109646c":"code","5a74cf48":"code","77eddabb":"code","54192906":"code","92d085f7":"code","477c8ed0":"code","196baf46":"code","18e82660":"code","737337f3":"code","ffd0b6f8":"code","01f4c48d":"code","7d9855dd":"code","44f1773b":"code","5e2b27af":"code","800945fb":"code","e69144ee":"code","bba5995d":"code","83027aba":"code","5cf23a34":"code","222eec94":"code","b75dcf1f":"code","09182653":"code","285247a2":"code","375db7d9":"code","260b9c08":"code","b8e5710d":"code","a74bfbb6":"markdown","5c6236ac":"markdown","2ec4489f":"markdown","03b1a4f5":"markdown","d88c3325":"markdown","cc273858":"markdown","8fab531d":"markdown","a64ac42c":"markdown","97d812d9":"markdown","b8e68447":"markdown","534707a6":"markdown"},"source":{"6a3f3ba0":"!rm -r \/opt\/conda\/lib\/python3.6\/site-packages\/lightgbm\n!git clone --recursive https:\/\/github.com\/Microsoft\/LightGBM","7872c58b":"!apt-get install -y -qq libboost-all-dev","4fa9afab":"%%bash\ncd LightGBM\nrm -r build\nmkdir build\ncd build\ncmake -DUSE_GPU=1 -DOpenCL_LIBRARY=\/usr\/local\/cuda\/lib64\/libOpenCL.so -DOpenCL_INCLUDE_DIR=\/usr\/local\/cuda\/include\/ ..\nmake -j$(nproc)","ba9b5bc9":"!cd LightGBM\/python-package\/;python3 setup.py install --precompile","dba9de8d":"!mkdir -p \/etc\/OpenCL\/vendors && echo \"libnvidia-opencl.so.1\" > \/etc\/OpenCL\/vendors\/nvidia.icd\n!rm -r LightGBM","46714055":"!nvidia-smi","3ff903f8":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/train.csv', index_col = 'id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-may-2021\/test.csv', index_col = 'id')\ntrain","f65647e3":"test","0d14e74b":"train.shape","d566782d":"test.shape","d84cffaf":"train.info()","8109646c":"test.info()","5a74cf48":"target = train.target.copy()\ntarget","77eddabb":"target.describe()","54192906":"train.drop('target', axis = 1, inplace = True)\ntrain","92d085f7":"(train.columns).equals(test.columns)","477c8ed0":"train.describe().T.style.bar(subset = ['mean'], color = 'royalblue').background_gradient(subset = ['std'], cmap = 'Blues_r')","196baf46":"pd.plotting.register_matplotlib_converters()\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nsns.set_style('whitegrid')","18e82660":"plt.figure(figsize = (16, 6))\ntarget_order = sorted(target.unique())\nsns.barplot(x = target.value_counts().index, y = target.value_counts(), order = target_order, palette = 'Blues_r')","737337f3":"def plot_grid(data, fig_size, grid_size, plot_type, target = ''):\n    \"\"\"\n    Custom function for plotting grid of plots.\n    It takes: DataFrame of data, size of a grid, type of plots, string name of target variable;\n    And it outputs: grid of plots.\n    \"\"\"\n    fig = plt.figure(figsize = fig_size)\n    if plot_type == 'histplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.histplot(data[column_name], kde = True, color = 'royalblue', stat = 'count')\n    if plot_type == 'boxplot':\n        for i, column_name in enumerate(data.select_dtypes(exclude = 'object').columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            plot = sns.boxplot(x = data[column_name], color = 'royalblue')\n    if plot_type == 'barplot':\n        target = data[target]\n        target_order = sorted(target.unique())\n        for i, column_name in enumerate(data.drop(target.name, axis = 1).columns):\n            fig.add_subplot(grid_size[0], grid_size[1], i + 1)\n            new_data = data[[column_name, target.name]].groupby(target.name).mean()\n            plot = sns.barplot(x = new_data.index, y = new_data[column_name], palette = 'Blues_r', order = target_order)\n    plt.tight_layout()","ffd0b6f8":"plot_grid(train, (16, 36), (17, 3), 'histplot')","01f4c48d":"plot_grid(train, fig_size = (16, 36), grid_size = (17, 3), plot_type = 'boxplot')","7d9855dd":"plot_grid(pd.concat([train, target], axis = 1), (16, 36), (17, 3), 'barplot', 'target')","44f1773b":"plt.figure(figsize = (16, 16))\nsns.heatmap(train.corr(),\n#             annot = True,\n#             fmt = '.2f',\n            square = True,\n            cmap = 'Blues_r',\n            cbar = False,\n            mask = np.triu(train.corr()))","5e2b27af":"zeroes = pd.DataFrame()\nfor i, column in enumerate(train.columns):\n    zeroes.loc[i, 'ColumnName'] = column\n    zeroes.loc[i, 'PercentOfZeroes'] = train.loc[train[column] == 0, column].count() \/ train.shape[0]\n#     print(f'{column} = {train.loc[train[column] == 0, column].count() \/ train.shape[0]}')\nzeroes.sort_values(by = 'PercentOfZeroes', ascending = False).style.background_gradient('Blues')","800945fb":"train_test = pd.concat([train, test], keys = ['train', 'test'], axis = 0)\ntrain_test","e69144ee":"train_test = (train_test - train_test.mean()) \/ train_test.std()\ntrain = train_test.xs('train').copy()\ntest = train_test.xs('test').copy()\ntrain","bba5995d":"class_map = {\n    'Class_1': 0,\n    'Class_2': 1,\n    'Class_3': 2,\n    'Class_4': 3,\n}\n\ntarget = target.map(class_map).astype('int')\n\ntarget","83027aba":"from sklearn.model_selection import cross_val_score, cross_validate, train_test_split, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","5cf23a34":"def test_estimators(X, y, estimators, labels, cv):\n    ''' \n    A function for testing multiple estimators.\n    It takes: full train data and target, list of estimators, \n              list of labels or names of estimators,\n              cross validation splitting strategy;\n    And it returns: a DataFrame of table with results of tests\n    '''\n    result_table = pd.DataFrame()\n\n    row_index = 0\n    for est, label in zip(estimators, labels):\n\n        est_name = label\n        result_table.loc[row_index, 'Model Name'] = est_name\n        \n        cv_results = cross_validate(est,\n                                    X,\n                                    y,\n                                    cv = cv,\n                                    scoring = 'neg_log_loss',\n                                    n_jobs = -1)\n\n        result_table.loc[row_index, 'Test log loss'] = -cv_results['test_score'].mean()\n        result_table.loc[row_index, 'Test Std'] = cv_results['test_score'].std()\n        result_table.loc[row_index, 'Fit Time'] = cv_results['fit_time'].mean()\n\n        row_index += 1\n\n    result_table.sort_values(by = ['Test log loss'], ascending = True, inplace = True)\n\n    return result_table","222eec94":"X_train, X_valid, y_train, y_valid = train_test_split(train, \n                                                      target, \n                                                      stratify = target,\n                                                      train_size = 0.1,\n                                                      random_state = 1)\ny_train","b75dcf1f":"logreg = LogisticRegression()\ndt = DecisionTreeClassifier(random_state = 1)\nrf = RandomForestClassifier()\nxgb = XGBClassifier()\nlgbm = LGBMClassifier(device = 'gpu', gpu_platform_id = 0, gpu_device_id = 0)\ncb = CatBoostClassifier(allow_writing_files = False, logging_level = 'Silent')\nsvc = SVC(probability = True)\ngnb = GaussianNB()\n\nestimators = [logreg,\n              dt,\n              rf,\n              lgbm, \n              cb,\n              svc,\n              gnb,]\n#               xgb]\n\nlabels = ['LogRegression',\n          'DecisionTree',\n          'RandomForest',\n          'LGBM',\n          'CatBoost',\n          'SVC',\n          'GNB',]\n#           'XGB']\n\nresults = test_estimators(X_train, y_train, estimators, labels, cv = StratifiedKFold(n_splits = 5))\nresults.style.background_gradient(cmap = 'Blues')","09182653":"import optuna\nfrom optuna.trial import TrialState\n\nfrom lightgbm import Dataset, cv\n\ndef objective(trial, model, X_train_full, y_train_full):\n    if (model == 'lgbm'):\n        train_set = Dataset(X_train_full, label = y_train_full)\n    \n        params = {\n            'objective': 'multiclass',\n            'metric': 'multi_logloss',\n            'num_classes': 4,\n            \"verbosity\": -1,\n            \"boosting_type\": \"gbdt\",\n            \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log = True),\n            \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log = True),\n            \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n            \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.4, 1.0),\n            \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.4, 1.0),\n            \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n            \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 5, 100),\n            'max_depth': trial.suggest_int('max_depth', 2, 12),\n            'learning_rate': trial.suggest_float('learning_rate', 1e-5, 1e-1, log = True),\n            \n            'device': 'gpu',\n            'gpu_platform_id': 0,\n            'gpu_device_id': 0,\n        }\n        \n        k = 5\n        lgbm_cv_results = cv(\n            params = params,\n            train_set = train_set,\n            num_boost_round = 3000,\n            nfold = k,\n            stratified = True,\n            early_stopping_rounds = 100,\n            verbose_eval = False,\n        )\n\n        # Set n_estimators as a trial attribute; Accessible via study.trials_dataframe().\n        trial.set_user_attr(\"n_estimators\", len(lgbm_cv_results['multi_logloss-mean']))\n        # Extract the best score.\n        best_score = lgbm_cv_results['multi_logloss-mean'][-1]\n        return best_score","285247a2":"study_lgbm = optuna.create_study(direction = 'minimize')\nstudy_lgbm.optimize(lambda trial: objective(trial, 'lgbm', train, target), n_trials = 100, timeout = 3600 * 7)","375db7d9":"print(\"Number of finished trials: \", len(study_lgbm.trials))\nprint(\"Best trial:\")\ntrial = study_lgbm.best_trial\n\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\nprint(\"  Number of estimators: {}\".format(trial.user_attrs[\"n_estimators\"]))","260b9c08":"lgbm = LGBMClassifier(n_estimators = trial.user_attrs[\"n_estimators\"], **study_lgbm.best_params)\nlgbm.fit(train, target)\npredictions = lgbm.predict_proba(test)\npredictions","b8e5710d":"submission = pd.DataFrame({'id': test.index,\n                           'Class_1': predictions[:, 0],\n                           'Class_2': predictions[:, 1],\n                           'Class_3': predictions[:, 2],\n                           'Class_4': predictions[:, 3],})\n\nsubmission.to_csv('submission.csv', index = False)","a74bfbb6":"# 5. Parameter tuning with Optuna","5c6236ac":"# 6. Creating a final model and submitting results","2ec4489f":"Taking a sample to save some time.","03b1a4f5":"It seems that there are a few features that consist almost entirely out of zeroes. Let's look into that.","d88c3325":"# Introduction\nGreetings!\ud83d\udc4b\n\nIn this kernel you will find my data science approach to \"Tabular Playground Series - May 2021\" competition. Specifically, I wanted to test dimensionality reduction using Principal Component Analysis as well as to test LGBM model running on Kaggle's GPU.\n\nAs always, any feedback Is very much appreciated! :)","cc273858":"# 3. Doing a bit of preprocessing","8fab531d":"# Table of contents:\n\n0. Setting up a GPU\n\n1. Meeting our data\n\n2. Creating visualizations\n\n3. Doing a bit of preprocessing\n\n4. Creating and evaluating models\n\n5. Parameter tuning with Optuna\n\n6. Creating a final model and submitting results","a64ac42c":"# 4. Creating and evaluating models","97d812d9":"# 1. Meeting our data","b8e68447":"# 2. Creating visualizations","534707a6":"# 0. Setting up a GPU\n\nMake sure you turned on GPU option in the Accelerator menu! Also, LGBMClassifier will require these parameters (**device = 'gpu', gpu_platform_id = 0, gpu_device_id = 0**) to be passed alongside with the others."}}