{"cell_type":{"59d9f3a4":"code","57d8a0e2":"code","1d47a5b3":"code","149d3529":"code","4c432048":"code","99f81d8b":"code","3644c2f9":"code","b5a9a0f8":"code","a386c426":"code","43c98b87":"code","39c902f3":"code","1334366b":"code","b883a424":"code","2a0fb437":"code","25069893":"code","b40d7f60":"code","f16f7212":"code","10007ea7":"code","64422eab":"markdown","a5fa319f":"markdown","147ea85b":"markdown","d916fad3":"markdown","8967416c":"markdown","1d1546d6":"markdown","ec897f45":"markdown","c5dd07a9":"markdown","7eef0226":"markdown","5dc02643":"markdown","03df23ee":"markdown","615dfe69":"markdown","4d2600b0":"markdown","605da829":"markdown","aa81c628":"markdown"},"source":{"59d9f3a4":"from math import sqrt\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.offline as py\nfrom plotly.offline import plot, iplot\nplotly.offline.init_notebook_mode(connected=True)\nfrom yellowbrick.features import FeatureImportances\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","57d8a0e2":"fifa = pd.read_csv(\"..\/input\/fifa.csv\")","1d47a5b3":"columns_to_drop = [\"Unnamed: 0\", \"ID\", \"Name\", \"Photo\", \"Nationality\", \"Flag\",\n                   \"Club\", \"Club Logo\", \"Value\", \"Wage\", \"Special\", \"Preferred Foot\",\n                   \"International Reputation\", \"Weak Foot\", \"Skill Moves\", \"Work Rate\",\n                   \"Body Type\", \"Real Face\", \"Position\", \"Jersey Number\", \"Joined\",\n                   \"Loaned From\", \"Contract Valid Until\", \"Height\", \"Weight\", \"LS\",\n                   \"ST\", \"RS\", \"LW\", \"LF\", \"CF\", \"RF\", \"RW\", \"LAM\", \"CAM\", \"RAM\", \"LM\",\n                   \"LCM\", \"CM\", \"RCM\", \"RM\", \"LWB\", \"LDM\", \"CDM\", \"RDM\", \"RWB\", \"LB\", \"LCB\",\n                   \"CB\", \"RCB\", \"RB\", \"Release Clause\"\n]\n\ntry:\n    fifa.drop(columns_to_drop, axis=1, inplace=True)\nexcept KeyError:\n    logger.warning(f\"Columns already dropped\")","149d3529":"# Sua an\u00e1lise come\u00e7a aqui.\nfifa.head()","4c432048":"# Identificando o tipo de cada vari\u00e1vel:\nfifa.dtypes","99f81d8b":"# Verificando se existem valores nulos no Dataset.\nif fifa.isnull().sum().sort_values(ascending=False).any() != 0:\n    print(f'Existe valores missing no dataset? {True}')\nelse:\n    print(f'Existe valores missing no dataset? {False}')","3644c2f9":"# Removendo os valores missing:\nfifa.dropna(inplace=True)","b5a9a0f8":"# Separando os dados em componentes de input e output:\nX = fifa.drop(['Overall'], axis=1)\ny = fifa['Overall']","a386c426":"# Aplicando o PCA no nosso conjunto de dados:\npca = PCA(n_components=3)\npcamodel = pca.fit_transform(fifa)","43c98b87":"# Gr\u00e1fico do PCA:\nfig = go.Figure()\nfig = go.Figure(data=[go.Scatter3d(x=pcamodel[:, 0],\n                    y=pcamodel[:, 1],\n                    z=pcamodel[:, 2],\n                    marker=dict(opacity=1,\n                    reversescale=True,\n                    colorscale='Blues',\n                    color='#228B22',\n                    size=2.5),\n                    line=dict (width=0.09),\n                    mode='markers')])\n\n# Layout:\nfig.update_layout(scene=dict(xaxis=dict( title=\"PCA1\"),\n                                yaxis=dict( title=\"PCA2\"),\n                                zaxis=dict(title=\"PCA3\")),\n                                template='plotly_dark',\n                                title=\"PCA\",\n                                font=dict(family=\"Courier New, monospace\",\n                                          size=12, \n                                          color=\"#ffffff\"),\n                                          autosize=False,\n                                          width=700,\n                                          height=400)\n\n\n# Plot:\npy.iplot(fig)","39c902f3":"def q1():\n    # PCA:\n    pca = PCA().fit(fifa)\n\n    # Fra\u00e7\u00e3o da vari\u00e2ncia:\n    return float(round(pca.explained_variance_ratio_[0],3))\nq1()","1334366b":"# PCA:\npca = PCA().fit(fifa)\n\n# Gr\u00e1fico:\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=(np.cumsum(pca.explained_variance_ratio_)),\n                         mode='lines', line=dict(color=\"#8B0000\",width = 4)))\n    \n# Layout:  \nfig.update_layout(showlegend=False,\n                  title=\"Vari\u00e2ncia explicada pelo primeiro componente principal\",\n                  xaxis_title=\"N\u00famero de componentes\",\n                  yaxis_title=\"Vari\u00e2ncia explicada (%)\",\n                  template='plotly_dark',\n                  font=dict(family=\"Courier New, monospace\",\n                            size=12, \n                            color=\"#ffffff\"),\n\n                  annotations=[dict(x=0,\n                                    y=0.565,\n                                    xref=\"x\",\n                                    yref=\"y\",\n                                    text=\"fraction of variance\",\n                                    showarrow=True,\n                                    arrowhead=2,\n                                    arrowsize=2,\n                                    arrowcolor=\"#FF8C00\",\n                                    ax=100,\n                                    ay=-1,\n                                    font=dict(\n                                    family=\"Courier New, monospace\",\n                                    size=12,\n                                    color=\"#ffffff\"))])\n\n# Plot:\npy.iplot(fig)","b883a424":"def q2():\n    # PCA:\n    pca = PCA(.95).fit_transform(fifa)\n\n    # N\u00famero de componentes:\n    return pca.shape[1]\nq2()","2a0fb437":"# Gr\u00e1fico:\n\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=(np.cumsum(pca.explained_variance_ratio_)),\n                         mode='lines', line=dict(color=\"#8B0000\",width = 4)))\n    \n\n# Layout:  \nfig.update_layout(showlegend=False,\n                  title=\"N\u00famero de componentes principais que explicam 95% da vari\u00e2ncia\",\n                  xaxis_title=\"N\u00famero de componentes\",\n                  yaxis_title=\"Vari\u00e2ncia explicada (%)\",\n                  template='plotly_dark',\n                  font=dict(family=\"monospace\",\n                            size=10, color=\"#ffffff\"),\n\n                  annotations=[dict(x=15,\n                                    y=0.95,\n                                    xref=\"x\",\n                                    yref=\"y\",\n                                    text=\"95% of the total variance\",\n                                    showarrow=True,\n                                    arrowhead=6,\n                                    arrowsize=2,\n                                    arrowcolor=\"#FF8C00\",\n                                    ax=120,\n                                    ay=-1,\n                                    font=dict(\n                                    family=\"Courier New, monospace\",\n                                    size=12,\n                                    color=\"#ffffff\"))])\n\n# Plot:                                    \npy.iplot(fig)","25069893":"x = [0.87747123,  -1.24990363,  -1.3191255, -36.7341814,\n     -35.55091139, -37.29814417, -28.68671182, -30.90902583,\n     -42.37100061, -32.17082438, -28.86315326, -22.71193348,\n     -38.36945867, -20.61407566, -22.72696734, -25.50360703,\n     2.16339005, -27.96657305, -33.46004736,  -5.08943224,\n     -30.21994603,   3.68803348, -36.10997302, -30.86899058,\n     -22.69827634, -37.95847789, -22.40090313, -30.54859849,\n     -26.64827358, -19.28162344, -34.69783578, -34.6614351,\n     48.38377664,  47.60840355,  45.76793876,  44.61110193,\n     49.28911284\n]","b40d7f60":"def q3():\n    # PCA:\n    pca = PCA(n_components=2).fit(fifa)\n\n    # Coordenadas (primeiro e segundo componentes principais):\n    return tuple([round(x,3) for x in pca.components_.dot(x)])\nq3()","f16f7212":"# Gr\u00e1fico :\nfig = plt.figure(figsize=(12, 8))\nax = fig.add_subplot()\n\n# Layout:\nax.spines['right'].set_visible(False)\nax.spines['top'].set_visible(False)\nax.spines['left'].set_visible(False)\nax.spines['bottom'].set_visible(False)\n\n# Plot:\nviz = FeatureImportances(LinearRegression(), ax=ax)\nviz.fit(X, y)\nviz.poof();","10007ea7":"def q4():\n    # Cria\u00e7\u00e3o do modelo:\n    modelo = LinearRegression()\n    \n    # RFE:\n    rfe = RFE(modelo,5).fit(X,y)\n    return list(X.loc[:, rfe.support_].columns)\nq4()","64422eab":"O **Recursive Feature Elimination (RFE)** \u00e9 uma t\u00e9cnica para sele\u00e7\u00e3o de atributos. Basicamente ele remove recursivamente os atributos e constr\u00f3i o modelo com os atributos remanescentes. Essa t\u00e9cnica utiliza a acur\u00e1cia do modelo para identificar os atributos que mais contribuem para prever a vari\u00e1vel alvo.\n\nPara a solu\u00e7\u00e3o do exerc\u00edcio utilizei a t\u00e9cnica de elimina\u00e7\u00e3o recursiva de atributos com um algortimo de Regress\u00e3o linear para selecionar as 5 melhores vari\u00e1veis preditoras.","a5fa319f":"## _Setup_ geral","147ea85b":"## Quest\u00e3o 3\n\nQual s\u00e3o as coordenadas (primeiro e segundo componentes principais) do ponto `x` abaixo? O vetor abaixo j\u00e1 est\u00e1 centralizado. Cuidado para __n\u00e3o__ centralizar o vetor novamente (por exemplo, invocando `PCA.transform()` nele). Responda como uma tupla de float arredondados para tr\u00eas casas decimais.","d916fad3":"## Quest\u00e3o 4\n\nRealiza RFE com estimador de regress\u00e3o linear para selecionar cinco vari\u00e1veis, eliminando uma a uma. Quais s\u00e3o as vari\u00e1veis selecionadas? Responda como uma lista de nomes de vari\u00e1veis.","8967416c":"## Quest\u00e3o 1\n\nQual fra\u00e7\u00e3o da vari\u00e2ncia consegue ser explicada pelo primeiro componente principal de `fifa`? Responda como um \u00fanico float (entre 0 e 1) arredondado para tr\u00eas casas decimais.","1d1546d6":"## Quest\u00e3o 2\n\nQuantos componentes principais precisamos para explicar 95% da vari\u00e2ncia total? Responda como un \u00fanico escalar inteiro.","ec897f45":"## Inicia sua an\u00e1lise a partir daqui","c5dd07a9":"Antes de resolver o exerc\u00edcio vamos visualizar graficamente quais os atributos mais importantes. \nA lib [Yellowbrick](https:\/\/www.scikit-yb.org\/en\/latest\/api\/model_selection\/importances.html) possui o atributo FeatureImportances para classificar e plotar as import\u00e2ncias relativas.","7eef0226":"A an\u00e1lise de componentes principais **(PCA)** \u00e9 um m\u00e9todo para extra\u00e7\u00e3o das vari\u00e1veis importantes a partir de um grande conjunto de vari\u00e1veis dispon\u00edveis em um conjunto de dados. Esta t\u00e9cnica permite extrair um n\u00famero pequeno de conjuntos dimensionais a partir de um dataset altamente dimensional. Com menos vari\u00e1veis a visualiza\u00e7\u00e3o tamb\u00e9m se torna muito mais significativa.","5dc02643":"Nesse caso, para obter 95% da varia\u00e7\u00e3o explicada, preciso de 15 componentes principais.","03df23ee":"Esse exerc\u00edcio aborda a etapa de Feature Selection, onde selecionamos os atributos(vari\u00e1veis) que ser\u00e3o melhores candidatas a vari\u00e1veis preditoras. Essa t\u00e9cnica nos ajuda a reduzir o overfiting (quando o algoritmo aprende demais), aumenta a acur\u00e1cia do modelo e reduz o tempo de treinamento.","615dfe69":"# Desafio 5\n\nNeste desafio, vamos praticar sobre redu\u00e7\u00e3o de dimensionalidade com PCA e sele\u00e7\u00e3o de vari\u00e1veis com RFE. Utilizaremos o _data set_ [Fifa 2019](https:\/\/www.kaggle.com\/karangadiya\/fifa19), contendo originalmente 89 vari\u00e1veis de mais de 18 mil jogadores do _game_ FIFA 2019.\n\n> Obs.: Por favor, n\u00e3o modifique o nome das fun\u00e7\u00f5es de resposta.","4d2600b0":"A an\u00e1lise de componentes principais \u00e9 associada a ideia de redu\u00e7\u00e3o de massa de dados, com menor perda poss\u00edvel da informa\u00e7\u00e3o. O objetivo \u00e9 sumarizar os dados que cont\u00e9m muitas vari\u00e1veis (p) por um conjunto menor de vari\u00e1veis (k) compostas, derivadas a partir do conjunto original. Os primeiros k componentes cont\u00e9m a maior quantidade de varia\u00e7\u00e3o dos dados.","605da829":"O primeiro componente principal \u00e9 a combina\u00e7\u00e3o linear dos atributos com m\u00e1xima vari\u00e2ncia e determina a dire\u00e7\u00e3o em que h\u00e1 mais alta variabilidade nos dados. Quanto maior a variabilidade capturada no primeiro componente principal, mais informa\u00e7\u00e3o ser\u00e1 capturada pelo componente.","aa81c628":"## **Redu\u00e7\u00e3o de Dimensionalidade: Principal Component Analysis (PCA)**"}}