{"cell_type":{"89b32f3b":"code","15ca4901":"code","ea7ce3cf":"code","b076b2ad":"code","5daaa8b9":"code","1e589ed5":"code","2fad20e4":"code","dcd59886":"code","e6ff56a7":"code","2ff6199d":"code","8ff4fe7a":"code","0cf03cdc":"code","ff507712":"code","7a9e442c":"code","f59b0973":"code","f0ce3aa8":"code","17476f7e":"code","1476bd4b":"code","52839880":"code","08914baf":"code","ac006df8":"code","31f0ba2f":"code","bc1cde3a":"code","cf96e86f":"code","cd14db44":"code","b261eea2":"code","162c19dd":"code","a4e49abb":"code","fdd06995":"code","3ab41da4":"code","e0c920ec":"code","a6eacf4d":"code","7bf94bbc":"code","c30a1dc8":"code","00656fc0":"code","ee0ad070":"code","4b926250":"code","3e355f09":"code","a966eda0":"code","07cddcc1":"code","db10af7f":"code","1ef36c47":"code","d2c65a5b":"code","9a1b333d":"code","e6f6e97f":"markdown","871afc46":"markdown","e32cce7e":"markdown","f877a9a4":"markdown","0fde6043":"markdown","1e9464d5":"markdown","4b3b7191":"markdown","8a14e3c1":"markdown","f1d620b9":"markdown","7f403206":"markdown","83511a3a":"markdown","ead56a1f":"markdown","5778c78a":"markdown","74ff8f59":"markdown","84270662":"markdown","e902113d":"markdown"},"source":{"89b32f3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15ca4901":"import warnings\nwarnings.filterwarnings('ignore')","ea7ce3cf":"import pandas as pd\ndf=pd.read_csv('..\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","b076b2ad":"import numpy as np\ndf['Glucose']=np.where(df['Glucose']==0,df['Glucose'].median(),df['Glucose'])\ndf.head(10)","5daaa8b9":"#### Independent And Dependent features\nX=df.drop('Outcome',axis=1)\ny=df['Outcome']","1e589ed5":"pd.DataFrame(X,columns=df.columns[:-1])","2fad20e4":"#### Train Test Split\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.20,random_state=0)","dcd59886":"from sklearn.ensemble import RandomForestClassifier\nrf_classifier=RandomForestClassifier(n_estimators=10).fit(X_train,y_train)\nprediction=rf_classifier.predict(X_test)","e6ff56a7":"y.value_counts()","2ff6199d":"from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\nprint(confusion_matrix(y_test,prediction))\nprint(accuracy_score(y_test,prediction))\nprint(classification_report(y_test,prediction))","8ff4fe7a":"### Manual Hyperparameter Tuning\nmodel=RandomForestClassifier(n_estimators=300,criterion='entropy',\n                             max_features='sqrt',min_samples_leaf=10,random_state=100).fit(X_train,y_train)\npredictions=model.predict(X_test)\nprint(confusion_matrix(y_test,predictions))\nprint(accuracy_score(y_test,predictions))\nprint(classification_report(y_test,predictions))","0cf03cdc":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10,14]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4,6,8]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features':max_features,\n               'max_depth':max_depth,\n               'min_samples_split':min_samples_split,\n               'min_samples_leaf':min_samples_leaf,\n               'criterion':['gini','entropy']}","ff507712":"print(random_grid)","7a9e442c":"rf=RandomForestClassifier()\nrf_randomcv=RandomizedSearchCV(estimator=rf,param_distributions=random_grid,n_iter=100,cv=3,verbose=2,\n                               random_state=100,n_jobs=-1)\n### fit the randomized model\nrf_randomcv.fit(X_train,y_train)","f59b0973":"rf_randomcv.best_params_","f0ce3aa8":"rf_randomcv","17476f7e":"best_random_grid=rf_randomcv.best_estimator_","1476bd4b":"from sklearn.metrics import accuracy_score\ny_pred=best_random_grid.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score:{}\" .format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","52839880":"\nrf_randomcv.best_params_","08914baf":"from sklearn.model_selection import GridSearchCV\nparam_grid={\n    'criterion':[rf_randomcv.best_params_['criterion']],\n    'max_depth':[rf_randomcv.best_params_['max_depth']],\n    'max_features':[rf_randomcv.best_params_['max_features']],\n    'min_samples_leaf': [rf_randomcv.best_params_['min_samples_leaf'], \n                         rf_randomcv.best_params_['min_samples_leaf']+2, \n                         rf_randomcv.best_params_['min_samples_leaf'] + 4],\n    'min_samples_split':[rf_randomcv.best_params_['min_samples_split']-2,\n                        rf_randomcv.best_params_['min_samples_split']-1,\n                        rf_randomcv.best_params_['min_samples_split'],\n                        rf_randomcv.best_params_['min_samples_split']+1,\n                        rf_randomcv.best_params_['min_samples_split']+2] ,\n    'n_estimators': [rf_randomcv.best_params_['n_estimators'] - 200, rf_randomcv.best_params_['n_estimators'] - 100, \n                     rf_randomcv.best_params_['n_estimators'], \n                     rf_randomcv.best_params_['n_estimators'] + 100, rf_randomcv.best_params_['n_estimators'] + 200]\n    \n}\nparam_grid","ac006df8":"#### Fit the grid_search to the data\nrf=RandomForestClassifier()\ngrid_search=GridSearchCV(estimator=rf,param_grid=param_grid,cv=10,n_jobs=-1,verbose=2)\ngrid_search.fit(X_train,y_train)","31f0ba2f":"grid_search.best_params_","bc1cde3a":"best_grid_search = grid_search.best_estimator_\nbest_grid_search","cf96e86f":"y_pred=best_grid_search.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(\"Accuracy Score {}\".format(accuracy_score(y_test,y_pred)))\nprint(\"Classification report: {}\".format(classification_report(y_test,y_pred)))","cd14db44":"from hyperopt import hp,fmin,tpe,Trials,STATUS_OK","b261eea2":"space = {'criterion': hp.choice('criterion', ['entropy', 'gini']),\n         'max_depth':hp.quniform('max_depth',10,1200,10),\n         'max_features':hp.choice('max_features',['auto','sqrt','log',None]),\n         'min_samples_leaf':hp.uniform('min_samples_leaf',0,0.5),\n         'min_samples_split':hp.uniform('min_samples_split',0,1),\n         'n_estimators':hp.choice('n_estimators',[10, 50, 300, 750, 1200,1300,1500])\n        \n        }","162c19dd":"space","a4e49abb":"def objective(space):\n    model = RandomForestClassifier(criterion = space['criterion'], max_depth = space['max_depth'],\n                                 max_features = space['max_features'],\n                                 min_samples_leaf = space['min_samples_leaf'],\n                                 min_samples_split = space['min_samples_split'],\n                                 n_estimators = space['n_estimators'], \n                                 )\n    \n    accuracy = cross_val_score(model, X_train, y_train, cv = 5).mean()\n\n    # We aim to maximize accuracy, therefore we return it as a negative value\n    return {'loss': -accuracy, 'status': STATUS_OK }\n    ","fdd06995":"from sklearn.model_selection import cross_val_score\ntrials = Trials()\nbest = fmin(fn= objective,\n            space= space,\n            algo= tpe.suggest,\n            max_evals = 80,\n            trials= trials)\nbest","3ab41da4":"crit={0:'entropy',1:'gini'}\nfeat={0: 'auto', 1: 'sqrt', 2: 'log2', 3: None}\nest={0: 10, 1: 50, 2: 300, 3: 750, 4: 1200,5:1300,6:1500}\n\nprint(crit[best['criterion']])\nprint(feat[best['max_features']])\nprint(est[best['n_estimators']])","e0c920ec":"print(best['min_samples_split'])\nprint(best['min_samples_leaf'])","a6eacf4d":"trainedforest=RandomForestClassifier(criterion=crit[best['criterion']],max_depth=best['max_depth'],\n                                    max_features=feat[best['max_features']],\n                                    min_samples_leaf=best['min_samples_leaf'],\n                                    min_samples_split=best['min_samples_split'],\n                                    n_estimators=est[best['n_estimators']]).fit(X_train,y_train)\npredforest=trainedforest.predict(X_test)\nprint(confusion_matrix(y_test,predforest))\nprint(\"Accuracuy Score: {}\" .format(accuracy_score(y_test,predforest)))\nprint(classification_report(y_test,predforest))\n","7bf94bbc":"import numpy as np\nfrom sklearn.model_selection import RandomizedSearchCV\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt','log2']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 1000,10)]\n#min number of samples required to split a node\nmin_samples_split=[2,5,10,14]\n#min number of samples required at each leaf node\nmin_samples_leaf=[1,2,4,6,8]\n# Create the random grid\nparam={'n_estimators':n_estimators,\n            'max_features':max_features,\n            'max_depth':max_depth,\n            'min_samples_split':min_samples_split,\n            'min_samples_leaf':min_samples_leaf,\n            'criterion':['gini','entropy']\n      }\nprint(param)","c30a1dc8":"from tpot import TPOTClassifier\n\n\ntpot_classifier=TPOTClassifier(generations=5,population_size=24,offspring_size=12,\n                              verbosity=2,early_stop=12,\n                              config_dict={'sklearn.ensemble.RandomForestClassifier': param},\n                              cv=4,scoring='accuracy')\ntpot_classifier.fit(X_train,y_train)","00656fc0":"accuracy=tpot_classifier.score(X_test,y_test)\nprint(accuracy)","ee0ad070":"import optuna\nimport sklearn.svm\ndef objective(trial):\n    \n    classifier=trial.suggest_categorical('classifier',['RandomForest','SVC'])\n    \n    if classifier=='RandomForest':\n        n_estimators=trial.suggest_int('n_estimators', 200, 2000,10)\n        max_depth=int(trial.suggest_float('max_depth', 10, 100, log=True))\n        \n        clf=sklearn.ensemble.RandomForestClassifier(\n            n_estimators=n_estimators,max_depth=max_depth)\n    else:\n        c = trial.suggest_float('svc_c', 1e-10, 1e10, log=True)\n        \n        clf = sklearn.svm.SVC(C=c, gamma='auto')\n    \n    return sklearn.model_selection.cross_val_score(\n           clf,X_train,y_train,n_jobs=-1,cv=3).mean()","4b926250":"study=optuna.create_study(direction='maximize')\nstudy.optimize(objective,n_trials=100)\n\n\ntrial=study.best_trial\n\nprint(\"Accuracy: {}\".format(trial.value))\nprint(\"Best Hyperparameters: {}\".format(trial.params))","3e355f09":"trial","a966eda0":"study.best_params","07cddcc1":"rf=RandomForestClassifier(n_estimators=330,max_depth=30)\nrf.fit(X_train,y_train)","db10af7f":"y_pred=rf.predict(X_test)\nprint(confusion_matrix(y_test,y_pred))\nprint(accuracy_score(y_test,y_pred))\nprint(classification_report(y_test,y_pred))","1ef36c47":"optuna.visualization.plot_optimization_history(study)       ## In this graph the best value and Objective value has been displayed","d2c65a5b":"optuna.visualization.plot_slice(study)                       ## the graph showing the accuracies of model which is coming \n                                                             ##to be greater than 70%\n                                         ## Darker the colour  better is ths accuracy","9a1b333d":"optuna.visualization.plot_contour(study,params=['n_estimators','max_depth'])   \n                                                                             ## In this Plot we are plotting the Accuracy surface\n                                                                             ## involved in Random Forest.","e6f6e97f":"### So we are observing that our target Variable i.e Outcome has true result occuring 500 times and 268 persons not having diabetes","871afc46":"## **Automated Hyperparameter Tuning**\n\n**Automated Hyperparameter Tuning can be done by using techniques such as**\n* Bayesian Optimization\n* Gradient Descent\n* Evolutionary Algorithms","e32cce7e":"## **Here in GridSearchCV we dont have an option of number of iterations.Now to determine The number of Iterations we have to take the product of count of each Feature in param_grid**\n### So Total number of iterations = 1* 1* 1* 3* 5* 5=75\n### So We will be fiiting 10 folds for each of 75 candidates, total= 750","f877a9a4":"## ***Optimize hyperparameters of the model using Optuna***\n\n### **The hyperparameters of this algorithm are 'n_estimators' and 'max_depth' for which we can try different values to see if the model accuracy can be improved. The objective function is modified to accept a trial object. This trial has several methods for sampling hyperparameters. We create a study to run the hyperparameter optimization and finally read the best hyperparameters.**","0fde6043":"# **Plotting the accuracies for each hyperparameter for each trial**","1e9464d5":"# **Plotting the study**\n### ***plotting the Optimization history of the study***","4b3b7191":"## **The main parameters used by a Random Forest Classifier are:**\n\n* criterion = the function used to evaluate the quality of a split.\n* max_depth = maximum number of levels allowed in each tree.\n* max_features = maximum number of features considered when splitting a node.\n* min_samples_leaf = minimum number of samples which can be stored in a tree leaf.\n* min_samples_split = minimum number of samples necessary in a node to cause node splitting.\n* n_estimators = number of trees in the ensemble.","8a14e3c1":"### **Genetic Algorithms**\n\n**Genetic Algorithms tries to apply natural selection mechanisms to Machine Learning contexts.**\n\n**Let's imagine we create a population of N Machine Learning models with some predefined Hyperparameters. We can then calculate the accuracy of each model and decide to keep just half of the models (the ones that performs best). We can now generate some offsprings having similar Hyperparameters to the ones of the best models so that go get again a population of N models. At this point we can again caltulate the accuracy of each model and repeate the cycle for a defined number of generations. In this way, just the best models will survive at the end of the process.**","f1d620b9":"## **Bayesian Optimization**\n\n### **Bayesian optimization uses probability to find the minimum of a function. The final aim is to find the input value to a function which can gives us the lowest possible output value.It usually performs better than random,grid and manual search providing better performance in the testing phase and reduced optimization time. In Hyperopt,Bayesian Optimization can be implemented giving 3 three main parameters to the function fmin.\n\n* Domain Space = defines the range of input values to test (in Bayesian Optimization this space creates a probability distribution for each of the used Hyperparameters).\n* Objective Function = defines the loss function to minimize.\n* Optimization Algorithm = defines the search algorithm to use to select the best input values to use in each new iteration.","7f403206":"# **Plotting the accuracy for the hyperparameters involved in the random forest model.**","83511a3a":"### Before learning different optimization techniques you may look into github repo.\n### [here](https:\/\/github.com\/fmfn\/BayesianOptimization) is a link to the github repo of Bayesian Optimization\n### [here](https:\/\/github.com\/hyperopt\/hyperopt) is the github repo of Hyperopt\n### Here providing you the link for [Optuna](https:\/\/optuna.org\/) which is official page of Optuna","ead56a1f":"### **[Hyperparameter Optimization](https:\/\/en.m.wikipedia.org\/wiki\/Hyperparameter_optimization) ia a common problem in machine learning. Machine Learning Algorithms ,from Logistic regression to neural nets,depend on well-tuned hyperparameters to reach maximum effectiveness .Different hyperparameter optimizaton strategies have varied performance and cost. So how do we choose?**\n\n### **Evaluating optimization strategies is non intuitive. stochastic optimization strategies produce a distribution of best-found values. And how should we choose between two similarly performing strategies? This kernel provides solutions for comparing different optimization strategies.**","5778c78a":"# Thankyou very much for visiting the Kernel.An upvote is the sign of Motivation and Encouragement So Please give the Upvote if You like it and found useful..\n\n## I have mentioned everything in the Comments still if you have any doubt or trying to ask please do so in Comment Section.\n","74ff8f59":"## **All Techniques Of Hyper Parameter Optimization**\n\n1. GridSearchCV\n2. RandomizedSearchCV\n3. Bayesian Optimization -Automate Hyperparameter Tuning (Hyperopt)\n4. Sequential Model Based Optimization(Tuning a scikit-learn estimator with skopt)\n5. Optuna- Automate Hyperparameter Tuning\n6. Genetic Algorithms (TPOT Classifier)","84270662":"## GRID SEARCH CV","e902113d":"## RANDOMIZED SEARCH CV"}}