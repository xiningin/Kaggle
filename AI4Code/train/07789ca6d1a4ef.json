{"cell_type":{"160137b6":"code","bded0cc0":"code","b21ca44a":"code","78812f04":"code","bc0456da":"code","b1e2e330":"code","32ac0c13":"code","23f6c262":"code","836dc620":"code","dae7f189":"code","c8901db0":"code","cd554054":"code","3f1e7018":"code","62d3659d":"code","0dd7cf78":"code","f90ffb64":"code","f596a5b8":"code","ee81bac2":"code","e965c8a3":"code","c517b4d2":"code","54526a46":"code","d15e3054":"code","96c39600":"code","20866b19":"code","398d609b":"code","7fd02249":"code","275be7bc":"code","2606a190":"code","8ae224f7":"code","1ccb14f5":"code","3ae1c76b":"code","f1d1c355":"code","d877bef6":"code","581d302e":"code","ab94317d":"code","d072a71c":"code","babe4f40":"code","4720f903":"markdown","2559b204":"markdown","a2a94947":"markdown","7b884a81":"markdown","6b8ff5ce":"markdown","ab116e4a":"markdown","d4af89e8":"markdown","99d2d359":"markdown","23f3602e":"markdown","c94b24d7":"markdown","eb1880e2":"markdown","c59b9897":"markdown","163a9fd7":"markdown","a00d02ff":"markdown","28cfa637":"markdown","fc31daa9":"markdown","a81432a6":"markdown","2a2efde1":"markdown","d3f0587e":"markdown","fa2d1274":"markdown","faf0ee5e":"markdown","b3bc117a":"markdown","f18c04f6":"markdown","74e6e625":"markdown"},"source":{"160137b6":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nimport pandas_profiling as pp\n\n# models\nimport sklearn.model_selection\nfrom sklearn.model_selection import cross_val_predict as cvp\nfrom sklearn import metrics\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb\n\n# model tuning\nfrom hyperopt import STATUS_OK, Trials, fmin, hp, tpe, space_eval\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bded0cc0":"valid_part = 0.3","b21ca44a":"train0 = pd.read_csv('\/kaggle\/input\/prediction-bod-in-river-water\/train.csv')","78812f04":"train0.head(15)","bc0456da":"train0.info()","b1e2e330":"pp.ProfileReport(train0)","32ac0c13":"train0 = train0.drop(['Id','4','5','6'], axis = 1)\ntrain0 = train0.dropna()\ntrain0.info()","23f6c262":"train0.head(3)","836dc620":"target_name = 'target'","dae7f189":"# For boosting model\ntrain0b = train0\ntrain_target0b = train0b[target_name]\ntrain0b = train0b.drop([target_name], axis=1)\n# Synthesis valid as test for selection models\ntrainb, testb, targetb, target_testb = train_test_split(train0b, train_target0b, test_size=valid_part, random_state=0)","c8901db0":"train_target0 = train0[target_name]\ntrain0 = train0.drop([target_name], axis=1)","cd554054":"#For models from Sklearn\nscaler = StandardScaler()\ntrain0 = pd.DataFrame(scaler.fit_transform(train0), columns = train0.columns)","3f1e7018":"train0.head(3)","62d3659d":"len(train0)","0dd7cf78":"# Synthesis valid as test for selection models\ntrain, test, target, target_test = train_test_split(train0, train_target0, test_size=valid_part, random_state=0)","f90ffb64":"train.head(3)","f596a5b8":"test.head(3)","ee81bac2":"train.info()","e965c8a3":"test.info()","c517b4d2":"acc_train_r2 = []\nacc_test_r2 = []\nacc_train_d = []\nacc_test_d = []\nacc_train_rmse = []\nacc_test_rmse = []","54526a46":"def acc_d(y_meas, y_pred):\n    # Relative error between predicted y_pred and measured y_meas values\n    return mean_absolute_error(y_meas, y_pred)*len(y_meas)\/sum(abs(y_meas))\n\ndef acc_rmse(y_meas, y_pred):\n    # RMSE between predicted y_pred and measured y_meas values\n    return (mean_squared_error(y_meas, y_pred))**0.5","d15e3054":"def acc_boosting_model(num,model,train,test,num_iteration=0):\n    # Calculation of accuracy of boosting model by different metrics\n    \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    if num_iteration > 0:\n        ytrain = model.predict(train, num_iteration = num_iteration)  \n        ytest = model.predict(test, num_iteration = num_iteration)\n    else:\n        ytrain = model.predict(train)  \n        ytest = model.predict(test)\n\n    print('target = ', targetb[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(targetb, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(targetb, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(targetb, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_testb[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_testb, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_testb, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_testb, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","96c39600":"def acc_model(num,model,train,test):\n    # Calculation of accuracy of model \u0430\u043a\u0449\u044c Sklearn by different metrics   \n  \n    global acc_train_r2, acc_test_r2, acc_train_d, acc_test_d, acc_train_rmse, acc_test_rmse\n    \n    ytrain = model.predict(train)  \n    ytest = model.predict(test)\n\n    print('target = ', target[:5].values)\n    print('ytrain = ', ytrain[:5])\n\n    acc_train_r2_num = round(r2_score(target, ytrain) * 100, 2)\n    print('acc(r2_score) for train =', acc_train_r2_num)   \n    acc_train_r2.insert(num, acc_train_r2_num)\n\n    acc_train_d_num = round(acc_d(target, ytrain) * 100, 2)\n    print('acc(relative error) for train =', acc_train_d_num)   \n    acc_train_d.insert(num, acc_train_d_num)\n\n    acc_train_rmse_num = round(acc_rmse(target, ytrain) * 100, 2)\n    print('acc(rmse) for train =', acc_train_rmse_num)   \n    acc_train_rmse.insert(num, acc_train_rmse_num)\n\n    print('target_test =', target_test[:5].values)\n    print('ytest =', ytest[:5])\n    \n    acc_test_r2_num = round(r2_score(target_test, ytest) * 100, 2)\n    print('acc(r2_score) for test =', acc_test_r2_num)\n    acc_test_r2.insert(num, acc_test_r2_num)\n    \n    acc_test_d_num = round(acc_d(target_test, ytest) * 100, 2)\n    print('acc(relative error) for test =', acc_test_d_num)\n    acc_test_d.insert(num, acc_test_d_num)\n    \n    acc_test_rmse_num = round(acc_rmse(target_test, ytest) * 100, 2)\n    print('acc(rmse) for test =', acc_test_rmse_num)\n    acc_test_rmse.insert(num, acc_test_rmse_num)","20866b19":"xgb_clf = xgb.XGBRegressor({'objective': 'reg:squarederror'}) \nparameters = {'n_estimators': [60, 70, 80, 90, 95, 100, 105, 110, 120, 130, 140], \n              'learning_rate': [0.005, 0.01, 0.05, 0.075, 0.1],\n              'max_depth': [3, 5, 7, 9],\n              'reg_lambda': [0.1, 0.3, 0.5]}\nxgb_reg = GridSearchCV(estimator=xgb_clf, param_grid=parameters, cv=5, n_jobs=-1).fit(trainb, targetb)\nprint(\"Best score: %0.3f\" % xgb_reg.best_score_)\nprint(\"Best parameters set:\", xgb_reg.best_params_)\nacc_boosting_model(7,xgb_reg,trainb,testb)","398d609b":"#%% split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(trainb, targetb, test_size=0.2, random_state=0)\ntrain_set = lgb.Dataset(Xtrain, Ztrain, silent=False)\nvalid_set = lgb.Dataset(Xval, Zval, silent=False)","7fd02249":"params = {\n        'boosting_type':'gbdt',\n        'objective': 'regression',\n        'num_leaves': 31,\n        'learning_rate': 0.01,\n        'max_depth': -1,\n        'subsample': 0.8,\n        'bagging_fraction' : 1,\n        'max_bin' : 5000 ,\n        'bagging_freq': 20,\n        'colsample_bytree': 0.6,\n        'metric': 'rmse',\n        'min_split_gain': 0.5,\n        'min_child_weight': 1,\n        'min_child_samples': 10,\n        'scale_pos_weight':1,\n        'zero_as_missing': False,\n        'seed':0,        \n    }\nmodelL = lgb.train(params, train_set = train_set, num_boost_round=10000,\n                   early_stopping_rounds=2000,verbose_eval=500, valid_sets=valid_set)","275be7bc":"acc_boosting_model(8,modelL,trainb,testb,modelL.best_iteration)","2606a190":"models = pd.DataFrame({\n    'Model': ['XGB', 'LGBM'],\n    \n    'r2_train': acc_train_r2,\n    'r2_test': acc_test_r2,\n    'd_train': acc_train_d,\n    'd_test': acc_test_d,\n    'rmse_train': acc_train_rmse,\n    'rmse_test': acc_test_rmse\n                     })","8ae224f7":"pd.options.display.float_format = '{:,.2f}'.format","1ccb14f5":"\nmodels.sort_values(by=['r2_test', 'r2_train'], ascending=False)","3ae1c76b":"# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['r2_train'], label = 'r2_train')\nplt.plot(xx, models['r2_test'], label = 'r2_test')\nplt.legend()\nplt.title('graph for XGB and LGBM models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('%')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","f1d1c355":"# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['d_train'], label = 'd_train')\nplt.plot(xx, models['d_test'], label = 'd_test')\nplt.legend()\nplt.title('Relative errors for XGB and LGBM models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('Relative error, %')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","d877bef6":"# Plot\nplt.figure(figsize=[25,6])\nxx = models['Model']\nplt.tick_params(labelsize=14)\nplt.plot(xx, models['rmse_train'], label = 'rmse_train')\nplt.plot(xx, models['rmse_test'], label = 'rmse_test')\nplt.legend()\nplt.title('graph for XGB and LGBM models for train and test datasets')\nplt.xlabel('Models')\nplt.ylabel('%')\nplt.xticks(xx, rotation='vertical')\nplt.savefig('graph.png')\nplt.show()","581d302e":"testn = pd.read_csv('\/kaggle\/input\/prediction-bod-in-river-water\/test.csv')\ntestn.info()","ab94317d":"testn = testn.drop(['Id','5','6','7'], axis = 1)\ntestn.head(3)","d072a71c":"#For models from Sklearn\ntestn = pd.DataFrame(scaler.transform(testn), columns = testn.columns)","babe4f40":"#XGB model for basic train\nxgb_reg.fit(train0, train_target0)\nxgb_reg.predict(train)[:3]","4720f903":"The analysis showed that many values \u200b\u200bare only available in stations 1 and 2, while others have much less data. We propose that at the start code, the BOD5 prediction should be carried out only for data from the first two stations","2559b204":"Thanks for your attention, I hope you find this kernel clear and useful.","a2a94947":"[Go to Top](#0)","7b884a81":"![image.png](attachment:image.png)","6b8ff5ce":"We can now compare our models and to choose the best one for our problem.","ab116e4a":"**XGBoost** is an ensemble tree method that apply the principle of boosting weak learners (CARTs generally) using the gradient descent architecture. XGBoost improves upon the base Gradient Boosting Machines (GBM) framework through systems optimization and algorithmic enhancements. Reference [Towards Data Science.](https:\/\/towardsdatascience.com\/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d)","d4af89e8":"Thanks to:\n* [Vitalii Mokin](https:\/\/www.kaggle.com\/vbmokin)\n\nThis kernel based on [BOD prediction in river - 15 regression models](http:\/\/www.kaggle.com\/vbmokin\/bod-prediction-in-river-15-regression-models).","99d2d359":"## 3. EDA <a class=\"anchor\" id=\"3\"><\/a>\n\n","23f3602e":"## 2. Download datasets <a class=\"anchor\" id=\"2\"><\/a>\n","c94b24d7":"![image.png](attachment:image.png)","eb1880e2":"### 5.1 XGB<a class=\"anchor\" id=\"5.8\"><\/a>\n\n","c59b9897":"<a class=\"anchor\" id=\"0\"><\/a>\n\n# XGB & LGBM","163a9fd7":"![image.png](attachment:image.png)","a00d02ff":"![image.png](attachment:image.png)","28cfa637":"### 5.2 LGBM <a class=\"anchor\" id=\"5.9\"><\/a>\n\n","fc31daa9":"## 5. Tuning models and test for all features <a class=\"anchor\" id=\"5\"><\/a>\n\n","a81432a6":"## 4. Preparing to modeling <a class=\"anchor\" id=\"4\"><\/a>\n\n","2a2efde1":"![image.png](attachment:image.png)","d3f0587e":"**Light GBM** is a fast, distributed, high-performance gradient boosting framework based on decision tree algorithms. It splits the tree leaf wise with the best fit whereas other boosting algorithms split the tree depth wise or level wise rather than leaf-wise. So when growing on the same leaf in Light GBM, the leaf-wise algorithm can reduce more loss than the level-wise algorithm and hence results in much better accuracy which can rarely be achieved by any of the existing boosting algorithms. Also, it is surprisingly very fast, hence the word \u2018Light\u2019. Reference [Analytics Vidhya](https:\/\/www.analyticsvidhya.com\/blog\/2017\/06\/which-algorithm-takes-the-crown-light-gbm-vs-xgboost\/).","fa2d1274":"![image.png](attachment:image.png)","faf0ee5e":"## 6. Models comparison <a class=\"anchor\" id=\"6\"><\/a>\n\n","b3bc117a":"## 7. Prediction <a class=\"anchor\" id=\"7\"><\/a>\n","f18c04f6":"This code is based on kernel \"[FE & EDA with Pandas Profiling](https:\/\/www.kaggle.com\/vbmokin\/fe-eda-with-pandas-profiling)\"","74e6e625":"## 1. Import libraries <a class=\"anchor\" id=\"1\"><\/a>\n\n"}}