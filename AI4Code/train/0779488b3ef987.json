{"cell_type":{"f6abddcd":"code","f8209cc9":"code","6c36cbaf":"code","a54f309b":"code","ef30bd9f":"code","56b1f7d2":"code","61f262c6":"code","26d5bfb6":"code","24a17382":"code","b989e0a4":"code","38ec0689":"code","42aa6e80":"code","c3e986ee":"code","15fed1c9":"code","e9f3d3a7":"code","82fe6376":"code","bb0c53b8":"code","cebe5804":"code","ce72620a":"code","6cec8857":"code","6d93b556":"code","de43cb89":"code","4f9aba25":"code","1000ae90":"code","6d239af9":"code","a2ab1011":"markdown","b19e1681":"markdown","ab0e4cfa":"markdown","3379a35d":"markdown","6df0e790":"markdown","831509db":"markdown"},"source":{"f6abddcd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f8209cc9":"# loading the data using pandas \nimport pandas as pd\n\ntrain = pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\", index_col=0)\ntest = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\", index_col=0)\nsubmission = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\")","6c36cbaf":"train.head()","a54f309b":"# check for columns with missing values and categorical variables\ntrain.info()","ef30bd9f":"# we only have object cols that need to be taken care of\ntrain.describe()","56b1f7d2":"# plottng the data using a histogram to show the number of instances on the y and x gven range\nimport matplotlib.pyplot as plt\ntrain.hist(bins=50, figsize=(20,15))\nplt.show()","61f262c6":"# # dropping the target col\n# # different data types for easy identification\n# features = train.drop(['target'], axis=1)\n\n# from termcolor import colored\n# num_col = list(train.select_dtypes(include='float64').columns)\n# cat_cols = list(train.select_dtypes(include='object').columns)\n# num_col.remove('target')\n# print('Number of numerical columns is:',colored(len(num_col),'green'),\n#       '\\nNumber of categorical columsn is:',colored(len(cat_cols),'green'))","26d5bfb6":"feature_cols = [col for col in train.columns if col != \"target\"]\ncategory_cols = [col for col in train.columns if \"cat\" in col]\ncont_cols = [col for col in train.columns if \"cont\" in col]","24a17382":"X = train[feature_cols]\ny = train[\"target\"]\n\nX_final = test[feature_cols]","b989e0a4":"# # lets first take care of categorical columns \n# from sklearn.preprocessing import LabelEncoder \n# encoder = LabelEncoder()\n# s = (train.dtypes == 'object')\n# object_cols = list(s[s].index)\n# for col in object_cols:\n#     train[col] = encoder.fit_transform(train[col])\n#     test[col] = encoder.transform(test[col])","38ec0689":"# Tranforming category cols \nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import OrdinalEncoder, StandardScaler\n\ntransformers = ColumnTransformer(\n    [(\"ordinary_encoder\", OrdinalEncoder(), category_cols),\n     (\"standardize\", StandardScaler(), cont_cols)],\n    remainder=\"passthrough\"\n)\n\nX = transformers.fit_transform(X)\nX_final = transformers.transform(X_final)","42aa6e80":"# splitting dataset to train and validation data\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, random_state=42, test_size=0.1)","c3e986ee":"X_train.shape, X_valid.shape","15fed1c9":"X_final","e9f3d3a7":"# now data is ready for our XGBoost model \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom xgboost import XGBRegressor\nfrom numpy import absolute\n\nmodel = XGBRegressor()\n# cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n# evaluate model\nscores = cross_val_score(model,X_train, y_train, scoring='neg_mean_absolute_error', n_jobs=-1)\n# force scores to be positive\nscores = absolute(scores)\nprint('Mean MAE: %.3f (%.3f)' % (scores.mean(), scores.std()) )","82fe6376":"# fitting data to the model \nmodel.fit(X_train, y_train)","bb0c53b8":"yhat = model.predict(X_final)\n# summarize prediction\nprint('Predicted: %.3f' % yhat)","cebe5804":"from sklearn.metrics import mean_squared_error\nimport numpy as np\n\ny_base = np.array([y_train.mean()]*y_valid.shape[0])\nmean_squared_error(y_valid, y_base, squared=False)","ce72620a":"from xgboost import XGBRegressor\nmodel2 = XGBRegressor()\nmodel2.fit(X_train, y_train)\ny_hat = model2.predict(X_valid)\n\nmean_squared_error(y_valid, y_hat, squared=False)","6cec8857":"SEED = 42\nXGB_params = {\n    \n    'learning_rate': 0.028752712882542178,\n\n    'n_estimators': 1000,\n\n    'num_leaves': 781,\n\n    'max_depth': 6,\n\n    'reg_alpha': 11.204053023177407,\n\n    'reg_lambda': 18.58812972996122,\n\n    'colsample_bytree': 0.12924180099983043,\n\n    'min_child_samples': 1063,\n\n    'max_bin': 503,\n\n    'min_data_per_group': 214,\n\n    'n_jobs': -1,\n\n    'random_state': SEED,\n    'bagging_seed': SEED,\n    'feature_fraction_seed': SEED,\n    \n    # 'boosting' : 'dart', \n    \n    \"objective\": \"regression\",\n    \n    \"metric\": \"rmse\",\n}","6d93b556":"# model = XGBRegressor(**XGB_params)\n    \n# model.fit(X_train, y_train, early_stopping_rounds=300, eval_set=[(X_valid, y_valid)],\n#         verbose=0)\n# y_hat = model.predict(X_valid)\n\n# mean_squared_error(y_valid, y_hat, squared=False)\n\n","de43cb89":"# using optuna \nimport optuna\nfrom sklearn.model_selection import cross_val_score\n\ndef objective(trial):\n    \n    params = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1000, 12000, step=100),\n        \"max_depth\":trial.suggest_int(\"max_depth\", 1, 5),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-1, log=True),\n        \"gamma\": trial.suggest_float(\"gamma\", 0.1, 1.0, step=0.1),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 7),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.2, 1.0),\n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.1, 1.0),\n        \"reg_alpha\": trial.suggest_float(\"reg_alpha\", 1e-6, 100.),\n        \"reg_lambda\": trial.suggest_float(\"reg_lambda\", 1e-6, 100.),\n    }\n    \n    \n    model = XGBRegressor(\n        **params,\n        n_jobs=-1, \n    )\n    \n    model.fit(X_train, y_train, \n              early_stopping_rounds=300, \n              eval_set=[(X_valid, y_valid)],\n              verbose=0)\n    \n    y_hat = model.predict(X_valid)\n    \n    return mean_squared_error(y_valid, y_hat, squared=False)\n\nstudy = optuna.create_study()\nstudy.optimize(objective, n_trials=100)\n\n","4f9aba25":"study.best_params","1000ae90":"from sklearn.model_selection import KFold\n\nbest_params = study.best_params\nfinal_predictions = []\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nfor i, j in kfold.split(X, y):\n    model3 = XGBRegressor(\n        **best_params,\n        n_jobs=-1\n    )\n    \n    model3.fit(\n        X[i], y[i], \n        early_stopping_rounds=300, \n        eval_set=[(X[j], y[j])],\n        verbose=0\n    )\n    \n    final_predictions.append(model3.predict(X_final))\n\npreds = np.mean(np.column_stack(final_predictions), axis=1)","6d239af9":"submission[\"target\"] = preds\nsubmission.to_csv(\"submissions.csv\", index=False)","a2ab1011":"Defining XGB parameters to be tuned onwards as i evaluate how the model performs. ","b19e1681":"upvote if you find these useful","ab0e4cfa":"second approach ","3379a35d":"Model Evaluation","6df0e790":"using the tuned parameters to see if we are getting an improvement","831509db":"visualization of the data "}}