{"cell_type":{"932509e4":"code","c4f055e3":"code","33915f30":"code","a6ed1dab":"code","1c7b76b1":"code","96b87674":"code","a4563aec":"code","ff80a2ce":"code","3e703219":"code","91f50e75":"code","7c915e77":"code","4aca5d51":"code","a763fa09":"code","7cebdafc":"code","dd02ea64":"code","488b2bef":"code","5cf2a3ce":"code","6834f860":"code","dd484d12":"code","ad3c1f10":"code","47f80bff":"code","6af77895":"code","133e306b":"code","eb8b0f85":"markdown","fcf5a245":"markdown","aecd3ff8":"markdown","47aec389":"markdown","48d660f2":"markdown","4a2f2a7c":"markdown","971ed8a1":"markdown","29a80955":"markdown","afdf302a":"markdown","6265368d":"markdown","0a18cc48":"markdown","e26829dd":"markdown","76ac771d":"markdown","f1ae48c0":"markdown","0446dcb7":"markdown","10a6787c":"markdown","d14825cc":"markdown","095421f6":"markdown","292fcf4b":"markdown","1a02406f":"markdown"},"source":{"932509e4":"#   1. Initial block of the model:\n  #         Input\n  #        \/     \\\n  #       \/       \\\n  #maxpool2d    conv2d-3x3\n  #       \\       \/  \n  #        \\     \/\n  #      concatenate -->\n\n\n\n#   2.Regular Downsampling&Dilation bottlenecks:\n        #class for regular downsampling, dilated convolution \n  #\n  #     Bottleneck Input\n  #        \/        \\\n  #       \/          \\\n  # maxpooling2d   conv2d-1x1\n  #      |             | PReLU\n  #      |         conv2d-3x3\n  #      |             | PReLU\n  #      |         conv2d-1x1\n  #      |             |\n  #  Padding2d     Regularizer\n  #       \\           \/  \n  #        \\         \/\n  #      Summing + PReLU\n  #\n#   3.Asymetric bottleneck:#class for separable convolutions(n.n convolutions are made into n.1 and 1.n)\n  #\n  #     Bottleneck Input\n  #        \/        \\\n  #       \/          \\\n  #      |         conv2d-1x1\n  #      |             | PReLU\n  #      |         conv2d-1x5\n  #      |             |\n  #      |         conv2d-5x1\n  #      |             | PReLU\n  #      |         conv2d-1x1\n  #      |             |\n  #  Padding2d     Regularizer\n  #       \\           \/  \n  #        \\         \/\n  #      Summing + PReLU\n    \n#   4. Upsampling bottleneck: - #class for upsampling bottlenecks\n  #           Input\n  #        \/        \\\n  #       \/          \\\n  # conv2d-1x1     convTrans2d-1x1\n  #      |             | PReLU\n  #      |         convTrans2d-3x3\n  #      |             | PReLU\n  #      |         convTrans2d-1x1\n  #      |             |\n  # maxunpool2d    Regularizer\n  #       \\           \/  \n  #        \\         \/\n  #      Summing + PReLU\n  #","c4f055e3":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.optim.lr_scheduler import StepLR\nimport cv2\nimport os\nfrom tqdm import tqdm\nfrom PIL import Image\n","33915f30":"class Initial_Encoding(nn.Module):\n    \n    def __init__ (self,chanel_in = 3,chanel_out = 13):\n        super().__init__()\n\n\n        self.maxpool = nn.MaxPool2d(kernel_size=2, stride = 2, padding = 0)\n\n        self.conv = nn.Conv2d(in_channels=chanel_in, out_channels=chanel_out, kernel_size = 3,\n                              stride = 2, padding = 1)\n\n        self.prelu = nn.PReLU(16)\n\n        self.batchnorm = nn.BatchNorm2d(chanel_out)\n  \n    def forward(self, x):\n        \n        main = self.conv(x)\n        main = self.batchnorm(main)\n        \n        side = self.maxpool(x)\n        \n        # concatenating on the channels axis\n        x = torch.cat((main, side), dim=1)\n        x = self.prelu(x)\n        \n        return x","a6ed1dab":"   #  dilation (bool) - if True: creating dilation bottleneck\n  #  stride_p ={1,2} - if 2: creating downsampling bottleneck\n  #  projection_ratio - ratio between input and output channels\n  #  a: activation - if a=1: relu used as the activation function else if a=2: Prelu us used\n  #  p - dropout ratio\nclass Downsampling_Dilation(nn.Module):\n    def __init__(self, dilation, chanel_in, chanel_out, stride_p, a=1, projection_ratio=4, p=0.1):\n             \n        super().__init__()\n        \n        # Define class variables\n        self.chanel_in = chanel_in\n        \n        self.chanel_out = chanel_out\n        self.dilation = dilation\n        self.stride_p = stride_p\n        \n        # calculating the number of reduced channels\n        if stride_p:\n            self.stride = 2\n            self.reduced_depth = int(chanel_in \/\/ projection_ratio)\n        else:\n            self.stride = 1\n            self.reduced_depth = int(chanel_out \/\/ projection_ratio)\n        \n        if (a==1):\n            activation = nn.ReLU()\n        elif(a==2):\n            activation = nn.PReLU()\n        \n        self.maxpool = nn.MaxPool2d(kernel_size = 2,\n                                      stride = 2,\n                                      padding = 0, return_indices=True)\n        \n\n        \n        self.dropout = nn.Dropout2d(p=p)\n\n        self.conv1 = nn.Conv2d(in_channels = self.chanel_in, out_channels = self.reduced_depth,\n                               kernel_size = 1,stride = 1,padding = 0, bias = False, dilation = 1)\n        \n        self.prelu1 = activation\n        \n        self.conv2 = nn.Conv2d(in_channels = self.reduced_depth,out_channels = self.reduced_depth,\n                                  kernel_size = 3, stride = self.stride,padding = self.dilation,\n                                  bias = True,dilation = self.dilation)\n                                  \n        self.prelu2 = activation\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,out_channels = self.chanel_out,\n                                  kernel_size = 1,stride = 1,padding = 0,bias = False,dilation = 1)\n        \n        self.prelu3 = activation\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.chanel_out)\n        \n        \n    def forward(self, x):\n        \n        bs = x.size()[0]\n        x_copy = x\n        \n        # Side Branch\n        x = self.conv1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.conv2(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.conv3(x)\n        x = self.batchnorm2(x)\n                \n        x = self.dropout(x)\n        \n        # Main Branch\n        if self.stride_p:\n            x_copy, indices = self.maxpool(x_copy)\n          \n        if self.chanel_in != self.chanel_out:\n            out_shape = self.chanel_out - self.chanel_in\n            \n            #padding and concatenating in order to match the channels axis of the side and main branches\n            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n            if torch.cuda.is_available():\n                extras = extras.cuda()\n            x_copy = torch.cat((x_copy, extras), dim = 1)\n\n        # Summing main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        if self.stride_p:\n            return x, indices\n        else:\n            return x","1c7b76b1":"  \n  #  projection_ratio - ratio between input and output channels\nclass ASNeck(nn.Module):\n    def __init__(self, chanel_in, chanel_out, projection_ratio=4):\n             \n        super().__init__()\n        \n        # Define class variables\n        self.chanel_in = chanel_in\n        self.reduced_depth = int(chanel_in \/ projection_ratio)\n        self.chanel_out= chanel_out\n        \n        self.dropout = nn.Dropout2d(p=0.1)\n        \n        self.conv1 = nn.Conv2d(in_channels= self.chanel_in,out_channels = self.reduced_depth,\n                               kernel_size = 1,stride = 1,padding = 0,bias = False)\n        \n        self.prelu1 = nn.PReLU()\n        \n        self.conv21 = nn.Conv2d(in_channels = self.reduced_depth,out_channels = self.reduced_depth,\n                                  kernel_size = (1, 5), stride = 1,padding = (0, 2), bias = False)\n        \n        self.conv22 = nn.Conv2d(in_channels = self.reduced_depth, out_channels = self.reduced_depth,\n                                  kernel_size = (5, 1),stride = 1,padding = (2, 0),bias = False)\n        \n        self.prelu2 = nn.PReLU()\n        \n        self.conv3 = nn.Conv2d(in_channels = self.reduced_depth,out_channels = self.chanel_out,\n                                  kernel_size = 1, stride = 1,padding = 0,bias = False)\n        \n        self.prelu3 = nn.PReLU()\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.chanel_out)\n        \n    def forward(self, x):\n        bs = x.size()[0]\n        x_copy = x\n        \n        # Side Branch\n        x = self.conv1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.conv21(x)\n        x = self.conv22(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.conv3(x)\n                \n        x = self.dropout(x)\n        x = self.batchnorm2(x)\n        \n        # Main Branch\n        \n        if self.chanel_in != self.chanel_out:\n            out_shape = self.chanel_out - self.chanel_in\n            \n            #padding and concatenating in order to match the channels axis of the side and main branches\n            extras = torch.zeros((bs, out_shape, x.shape[2], x.shape[3]))\n            if torch.cuda.is_available():\n                extras = extras.cuda()\n            x_copy = torch.cat((x_copy, extras), dim = 1)\n        \n        # Summing main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        return x","96b87674":" #  projection_ratio - ratio between input and output channels\na=1 #activation function,if 1: relu used as the activation function else if a=2: Prelu us used\nclass Upsampl_layer(nn.Module):\n    \n    def __init__(self, chanel_in, chanel_out, a, projection_ratio=4):\n        \n        super().__init__()\n        \n        # Define class variables\n        self.chanel_in = chanel_in\n        self.reduced_depth = int(chanel_in \/ projection_ratio)\n        self.chanel_out = chanel_out\n        \n        \n        if (a==1):\n            activation = nn.ReLU()\n        elif(a==2):\n            activation = nn.PReLU()\n        elif(a==3):\n            activation = nn.Softmax()    \n        \n        self.unpool = nn.MaxUnpool2d(kernel_size = 2,\n                                     stride = 2)\n        \n        self.main_conv = nn.Conv2d(in_channels = self.chanel_in,out_channels = self.chanel_out,\n                                   kernel_size = 1)\n        \n        self.dropout = nn.Dropout2d(p=0.1)\n        \n        \n        self.convt1 = nn.ConvTranspose2d(in_channels = self.chanel_in,out_channels = self.reduced_depth,\n                               kernel_size = 1,padding = 0, bias = False)\n        \n        \n        self.prelu1 = activation\n        \n        # This layer used for Upsampling\n        self.convt2 = nn.ConvTranspose2d(in_channels = self.reduced_depth,\n                                  out_channels = self.reduced_depth, kernel_size = 3,\n                                  stride = 2, padding = 1, output_padding = 1,bias = False)\n        \n        self.prelu2 = activation\n        \n        self.convt3 = nn.ConvTranspose2d(in_channels = self.reduced_depth,out_channels = self.chanel_out,\n                                  kernel_size = 1, padding = 0,bias = False)\n        \n        self.prelu3 = activation\n        \n        self.batchnorm = nn.BatchNorm2d(self.reduced_depth)\n        self.batchnorm2 = nn.BatchNorm2d(self.chanel_out)\n        \n    def forward(self, x, indices):\n        x_copy = x\n        \n        # Side Branch\n        x = self.convt1(x)\n        x = self.batchnorm(x)\n        x = self.prelu1(x)\n        \n        x = self.convt2(x)\n        x = self.batchnorm(x)\n        x = self.prelu2(x)\n        \n        x = self.convt3(x)\n        x = self.batchnorm2(x)\n        \n        x = self.dropout(x)\n        \n        # Main Branch\n        \n        x_copy = self.main_conv(x_copy)\n        x_copy = self.unpool(x_copy, indices, output_size=x.size())\n        \n        # summing the main and side branches\n        x = x + x_copy\n        x = self.prelu3(x)\n        \n        return x","a4563aec":"# Class - number of classes\nclass ENet(nn.Module):\n  \n  # Creating Enet model!\n  \n    def __init__(self, n_Class):\n        super().__init__()\n        \n        self.Class = n_Class\n        \n        # The initial block\n        self.init = Initial_Encoding()\n        \n        \n        # The first bottleneck\n        self.b10 = Downsampling_Dilation(dilation=1,chanel_in=16,chanel_out=64,stride_p=True,p=0.01)\n        \n        self.b11 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64,stride_p=False,p=0.01)\n        \n        self.b12 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64,stride_p=False,p=0.01)\n        \n        self.b13 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64,stride_p=False,p=0.01)\n        \n        self.b14 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64,stride_p=False,p=0.01)\n        \n        \n        # The second bottleneck\n        self.b20 = Downsampling_Dilation(dilation=1, chanel_in=64,chanel_out=128, stride_p=True)\n        \n        self.b21 = Downsampling_Dilation(dilation=1, chanel_in=128, chanel_out=128,stride_p=False)\n        \n        self.b22 = Downsampling_Dilation(dilation=2, chanel_in=128, chanel_out=128, stride_p=False)\n        \n        self.b23 = ASNeck(chanel_in=128,chanel_out=128)\n        \n        self.b24 = Downsampling_Dilation(dilation=4, chanel_in=128,chanel_out=128,stride_p=False)\n        \n        self.b25 = Downsampling_Dilation(dilation=1, chanel_in=128, chanel_out=128, stride_p=False)\n        \n        self.b26 = Downsampling_Dilation(dilation=8,chanel_in=128,  chanel_out=128, stride_p=False)\n        \n        self.b27 = ASNeck(chanel_in=128,chanel_out=128)\n        \n        self.b28 = Downsampling_Dilation(dilation=16, chanel_in=128,chanel_out=128, stride_p=False)\n        \n        \n        # The third bottleneck\n        self.b31 = Downsampling_Dilation(dilation=1, chanel_in=128, chanel_out=128,stride_p=False)\n        \n        self.b32 = Downsampling_Dilation(dilation=2,  chanel_in=128, chanel_out=128, stride_p=False)\n        \n        self.b33 = ASNeck(chanel_in=128, chanel_out=128)\n        \n        self.b34 = Downsampling_Dilation(dilation=4,chanel_in=128, chanel_out=128,stride_p=False)\n        \n        self.b35 = Downsampling_Dilation(dilation=1, chanel_in=128, chanel_out=128, stride_p=False)\n        \n        self.b36 = Downsampling_Dilation(dilation=8, chanel_in=128,chanel_out=128,  stride_p=False)\n        \n        self.b37 = ASNeck(chanel_in=128, chanel_out=128)\n        \n        self.b38 = Downsampling_Dilation(dilation=16, chanel_in=128,chanel_out=128,stride_p=False)\n        \n        \n        # The fourth bottleneck\n        self.b40 = Upsampl_layer(chanel_in=128, chanel_out=64,  a=1)\n        \n        self.b41 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64, stride_p=False,a=1)\n        \n        self.b42 = Downsampling_Dilation(dilation=1,chanel_in=64,chanel_out=64, stride_p=False,a=1)\n        \n        \n        # The fifth bottleneck\n        self.b50 = Upsampl_layer(chanel_in=64, chanel_out=16, a=1)\n        \n        self.b51 = Downsampling_Dilation(dilation=1,chanel_in=16,chanel_out=16,stride_p=False,a=1)\n        \n        \n        # Final ConvTranspose Layer\n        self.fullconv = nn.ConvTranspose2d(in_channels=16,out_channels=self.Class,  kernel_size=3, \n                                           stride=2, padding=1, output_padding=1, bias=False)\n        \n        \n    def forward(self, x):\n        \n        # The initial block\n        x = self.init(x)\n        \n        # The first bottleneck\n        x, i1 = self.b10(x)\n        x = self.b11(x)\n        x = self.b12(x)\n        x = self.b13(x)\n        x = self.b14(x)\n        \n        # The second bottleneck\n        x, i2 = self.b20(x)\n        x = self.b21(x)\n        x = self.b22(x)\n        x = self.b23(x)\n        x = self.b24(x)\n        x = self.b25(x)\n        x = self.b26(x)\n        x = self.b27(x)\n        x = self.b28(x)\n        \n        # The third bottleneck\n        x = self.b31(x)\n        x = self.b32(x)\n        x = self.b33(x)\n        x = self.b34(x)\n        x = self.b35(x)\n        x = self.b36(x)\n        x = self.b37(x)\n        x = self.b38(x)\n        \n        # The fourth bottleneck\n        x = self.b40(x, i2)\n        x = self.b41(x)\n        x = self.b42(x)\n        \n        # The fifth bottleneck\n        x = self.b50(x, i1)\n        x = self.b51(x)\n        \n        # Final ConvTranspose Layer\n        x = self.fullconv(x)\n        \n        return x","ff80a2ce":"def loader(training_path, segmented_path, batch_size, h=320, w=1000):\n    filenames_t = os.listdir(training_path)\n    total_files_t = len(filenames_t)\n    \n    filenames_s = os.listdir(segmented_path)\n    segmented_img = len(filenames_s)\n    \n    assert(total_files_t == segmented_img)\n    \n    if str(batch_size).lower() == 'all':\n        batch_size = segmented_img\n    \n    idx = 0\n    while(1):\n      # 1d array containing random indexes of images and labels\n        batch_idxs = np.random.randint(0, segmented_img, batch_size)\n            \n        \n        inputs = []\n        labels = []\n        \n        for jj in batch_idxs:\n          # Reading normalized photo\n            img = plt.imread(training_path + filenames_t[jj])\n          # Resizing using nearest neighbor method to get sharp, jaggy image\n            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n            inputs.append(img)\n          \n          # Reading semantic image\n            img = Image.open(segmented_path + filenames_s[jj])\n            img = np.array(img)\n          # Resizing using nearest neighbor method\n            img = cv2.resize(img, (h, w), cv2.INTER_NEAREST)\n            labels.append(img)\n        \n        #Joining 3d-images along axis=2 --> (h,w,c) to (h,c,w)\n        inputs = np.stack(inputs, axis=2) \n      # Changing image format to C x H x W matrices\n        inputs = torch.tensor(inputs).transpose(0, 2).transpose(1, 3)\n        \n        labels = torch.tensor(labels)\n        print(labels.shape)\n        \n        yield inputs, labels #similar to return ","3e703219":"def get_class_weights(num_classes, c=1.02):\n    pipe = loader('..\/input\/camvid-dataset\/train\/', '..\/input\/camvid-dataset\/trainannot\/', batch_size='all')\n    _, labels = next(pipe)\n    all_labels = labels.flatten()\n    each_class = np.bincount(all_labels, minlength=num_classes)\n    prospensity_score = each_class \/ len(all_labels)\n    class_weights = 1 \/ (np.log(c + prospensity_score))\n    return class_weights\n\nclass IoULoss(nn.Module):\n    def __init__(self, weight=None, size_average=True):\n        super(IoULoss, self).__init__()\n\n    def forward(self, inputs, targets, smooth=1):\n        \n        #comment out if your model contains a sigmoid or equivalent activation layer\n        inputs = F.sigmoid(inputs)       \n        \n        #flatten label and prediction tensors\n        inputs = inputs.view(-1)\n        targets = targets.view(-1)\n        \n        #intersection is equivalent to True Positive count\n        #union is the mutually inclusive area of all labels & predictions \n        intersection = (inputs * targets).sum()\n        total = (inputs + targets).sum()\n        union = total - intersection \n        \n        IoU = (intersection + smooth)\/(union + smooth)\n                \n        return 1 - IoU\n\nenet = ENet(12)\ndevice = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nenet = enet.to(device)\nclass_weights = get_class_weights(12)","91f50e75":"iters_t = []\niters_v=[]\ntrain_losses = []\nval_losses = []\niou_train=[]\niou_val=[]\n\nopt=1\nif(opt==1):\n    lr = 5e-4\n    optimizer = torch.optim.Adam(enet.parameters(),lr=lr,weight_decay=2e-4)\nelif(opt==2):\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.01, momentum=0.99)\n    \nprint_every = 5\neval_every = 5\nbatch_size = 10\nminib_train = 367 \/\/ batch_size # mini_batch train \nminib_val = 101 \/\/ batch_size  # mini_batch validation\nloss_f=1\n\npipe = loader('..\/input\/camvid-dataset\/train\/', '..\/input\/camvid-dataset\/trainannot\/', batch_size)\neval_pipe = loader('..\/input\/camvid-dataset\/val\/', '..\/input\/camvid-dataset\/valannot\/', batch_size)\nepochs = 100\n# 100\n","7c915e77":"# Train loop\nfor e in range(1, epochs+1):\n    train_loss = 0\n    enet.train()\n    for _ in tqdm(range(minib_train)):\n        X_batch, mask_batch = next(pipe)\n        X_batch, mask_batch = X_batch.to(device), mask_batch.to(device) # assign data to cpu\/gpu\n        \n        optimizer.zero_grad()\n        out = enet(X_batch.float())\n        \n        # loss calculation\n        if (loss_f==1):\n            criteria=nn.CrossEntropyLoss(weight=torch.FloatTensor(class_weights).to(device))\n            loss = criteria(out, mask_batch.long())\n        elif(loss_f==2):\n            loss = IoULoss(out,mask_batch.long())\n        \n        loss.backward() # update weights\n        optimizer.step() #update gradients\n\n        train_loss += loss.item()\n#     print ()\n    train_losses.append(train_loss)\n    iters_t.append(e)\n    \n#     if (e+1) % print_every == 0:\n#         print ('Epoch {}\/{}...'.format(e, epochs),\n#                 'Loss {:6f}'.format(train_loss))\n    \n    if e % eval_every == 0:\n        with torch.no_grad():\n            enet.eval()\n            val_loss = 0\n            for _ in tqdm(range(minib_val)):\n                inputs, labels = next(eval_pipe)\n                inputs, labels = inputs.to(device), labels.to(device)\n                out = enet(inputs)\n                out = out.data.max(1)[1]\n                val_loss += (labels.long() - out.long()).sum()  \n            print ()\n            print ('Loss {:6f}'.format(val_loss))\n            val_losses.append(val_loss)\n            iters_v.append(e)\n            \n    if e % print_every == 0:\n        checkpoint = {'epochs' : e, 'state_dict' : enet.state_dict()}\n        torch.save(checkpoint, '\/content\/ckpt-enet-{}-{}.pth'.format(e, train_loss))\n        print ('Model saved!')\n\nif(loss_f==1):\n    print ('Epoch {}\/{}...'.format(e, epochs),\n       'Total Mean Loss: {:6f}'.format(sum(train_losses) \/ epochs))\n\nelif(loss_f==2):\n    print ('Epoch {}\/{}...'.format(e, epochs),\n       'IoU: {:6f}'.format(sum(train_losses) \/ epochs))","4aca5d51":"plt.figure()\nplt.subplot(iters_t, train_losses)\nplt.subplot(iters_v, val_losses)\nplt.show()","a763fa09":"# Load a pretrained model if needed\nenet = ENet(12)\nstate_dict = torch.load('..\/input\/weights\/ckpt-enet.pth',map_location='cpu')['state_dict']\nenet.load_state_dict(state_dict)","7cebdafc":"fname = 'Seq05VD_f05100.png'\nsample_img = plt.imread('..\/input\/camvid-dataset\/test\/' + fname)\nsample_img = cv2.resize(sample_img, (512, 512), cv2.INTER_NEAREST)\nsample_img = torch.tensor(sample_img).unsqueeze(0).float()\nsample_img = sample_img.transpose(2, 3).transpose(1, 2).to(device)\n\nenet.to(device)\nwith torch.no_grad():\n    out1 = enet(sample_img.float()).squeeze(0)","dd02ea64":"sample_img_seg = Image.open('..\/input\/camvid-dataset\/testannot\/' + fname)\nsample_img_seg = cv2.resize(np.array(sample_img_seg), (512, 512), cv2.INTER_NEAREST)","488b2bef":"out2 = out1.cpu().detach().numpy()","5cf2a3ce":"mno = 8 # Should be between 0 - n-1 | where n is the number of classes\n\nfigure = plt.figure(figsize=(20, 10))\nplt.subplot(1, 3, 1)\nplt.title('Input Image')\nplt.axis('off')\nplt.imshow(sample_img)\nplt.subplot(1, 3, 2)\nplt.title('Output Image')\nplt.axis('off')\nplt.imshow(out2[mno, :, :])\nplt.show()","6834f860":"op_label = out1.data.max(0)[1].cpu().numpy()","dd484d12":"def decode_segmap(image):\n    Sky = [128, 128, 128]\n    Building = [128, 0, 0]\n    Pole = [192, 192, 128]\n    Road_marking = [255, 69, 0]\n    Road = [128, 64, 128]\n    Pavement = [60, 40, 222]\n    Tree = [128, 128, 0]\n    SignSymbol = [192, 128, 128]\n    Fence = [64, 64, 128]\n    Car = [64, 0, 128]\n    Pedestrian = [64, 64, 0]\n    Bicyclist = [0, 128, 192]\n\n    label_colours = np.array([Sky, Building, Pole, Road_marking, Road, \n                              Pavement, Tree, SignSymbol, Fence, Car, \n                              Pedestrian, Bicyclist]).astype(np.uint8)\n    r = np.zeros_like(image).astype(np.uint8)\n    g = np.zeros_like(image).astype(np.uint8)\n    b = np.zeros_like(image).astype(np.uint8)\n    for l in range(0, 12):\n        r[image == l] = label_colours[l, 0]\n        g[image == l] = label_colours[l, 1]\n        b[image == l] = label_colours[l, 2]\n\n    rgb = np.zeros((image.shape[0], image.shape[1], 3)).astype(np.uint8)\n    rgb[:, :, 0] = b\n    rgb[:, :, 1] = g\n    rgb[:, :, 2] = r\n    return rgb","ad3c1f10":"true_seg = decode_segmap(sample_img_seg)\npred_seg = decode_segmap(op_label)","47f80bff":"figure = plt.figure(figsize=(20, 10))\nplt.subplot(1, 3, 1)\nplt.title('Input Image')\nplt.axis('off')\nplt.imshow(sample_img)\nplt.subplot(1, 3, 2)\nplt.title('Predicted Segmentation')\nplt.axis('off')\nplt.imshow(pred_seg)\nplt.subplot(1, 3, 3)\nplt.title('Ground Truth')\nplt.axis('off')\nplt.imshow(true_seg)\nplt.show()","6af77895":"# Save the parameters\ncheckpoint = {\n    'epochs' : e,\n    'state_dict' : enet.state_dict()\n}\ntorch.save(checkpoint, 'ckpt-enet-1.pth')","133e306b":"# Save the model\ntorch.save(enet, 'model')\n\n\n# import pickle \n# filename = 'enet1.sav'\n# pickle.dump(enet, open(filename, 'wb'))","eb8b0f85":"\n<a id='step2.3'><\/a>\n\n2.3.Training model and plot of loss function with each epoch\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","fcf5a245":"<a id='step1.1'><\/a>\n\n1.1. Initial Encoding Layer\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","aecd3ff8":"# ENet -  Real Time Semantic Segmentation\n\nIn this notebook, we have reproduced the ENet paper. <br\/>\nLink to the paper: https:\/\/arxiv.org\/pdf\/1606.02147.pdf <br\/>\n\n\n**ALL THE CODE IN THIS NOTEBOOK ASSUMES THE USAGE OF THE <span style=\"color:blue;\">CAMVID<\/span> DATASET**\n\n<a class=\"anchor\" id=\"toc\"><\/a>\n\n<div style=\"background: #f9f9f9 none repeat scroll 0 0;border: 1px solid #aaa;display: table;font-size: 95%;margin-bottom: 1em;padding: 20px;width: 400px;\">\n<h3>Contents<\/h3>\n<\/ul>\n<li style=\"list-style: outside none none !important;\"><a \n    \n>1 ENET implementation<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\"><li style=\"list-style: outside none none !important;\"><a\nhref=\"#step1.1\">1.1 Initial Encoding<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a\nhref=\"#step1.2\">1.2 Regular Downsampling and Dilated Convolution Bottleneck layer <\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step1.3\">1.3 Separable convolution Bottleneck layer<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a                                 href=\"#step1.4\">1.4 Upsampling Bottleneck<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a \n href=\"#step1.5\">1.5 Compiling all layers of ENET<\/a><\/li>\n          \n <\/ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"#step2.1\">2.1 Training <\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.1\">2.1 Image Preprocessing<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.2\">2.2 Initiating model, weights, hyper parameters<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step2.3\">2.3 Model training and plot of loss fuction<\/a><\/li>  \n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step2.4\">2.4 N-gram analysis<\/a><\/li>\n    \n <\/ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"#step3.1\">3 Data cleaning<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">    \n            <li style=\"list-style: outside none none !important;\"><a href=\"#step3.1\">3.1 Cleaning URLs and HTML tags<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a         href=\"#step3.2\">3.2 Cleaning Punctuations and emojis<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a \n                                                                     href=\"#step3.3\">3.3  Cleaning stop words <\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step3.4\">3.4 Using Glo-Ve for word embeddings<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a \nhref=\"#step3.5\">3.5 Train-Test split <\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a><\/li> \n<\/ul>\n<li style=\"list-style: outside none none !important;\"><a href=\"step4.1\">4 Creating Models<\/a><\/li>\n      <ul style=\"font-weight: 700;text-align: left;list-style: outside none none !important;\">\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step4.1\">4.1 LSTM Model with Glove Embeddings<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a href=\"#step4.2\">4.2 Plotting accuracy and loss curves<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a  href=\"#step4.3\">4.3 LSTM with Glove Results<\/a><\/li>\n            <li style=\"list-style: outside none none !important;\"><a \n\n<\/ul>\n<\/div>","47aec389":"## Save the entire model","48d660f2":"<a id='step1.5'><\/a>\n\n1.5. Compiling all layers of ENET model\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","4a2f2a7c":"## Save the model checkpoint","971ed8a1":"Load the ENet model","29a80955":"## Infer using the trained model","afdf302a":"<a id='step2.2'><\/a>\n\n2.2. Initiate ENET model and Initiate weights, Hyper parameters\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","6265368d":"<a id='step1.3'><\/a>\n\n1.3. Separable Convolution Bottleneck Layer\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","0a18cc48":"<a id='step1.4'><\/a>\n\n1.4. Upsampling for deconvolution\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","e26829dd":"## Load the label image","76ac771d":"<a id='step2'><\/a>\n\n2. Image preprocessing\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","f1ae48c0":"Get the PreTrained ENet model","0446dcb7":"Define the function that maps a 2D image with all the class labels to a segmented image with the specified colored maps\n<a id='step1.2'><\/a>\n\n1.2. Contractions in words\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>","10a6787c":"## Use the code to infer on new images","d14825cc":"## Move the output to cpu and convert it to numpy and see how each class looks\n","095421f6":"Decode the images","292fcf4b":"Get the class labels from the output","1a02406f":"<a id='step1.2'><\/a>\n\n1.2. Dilation Convolution Bottleneck layer\n\n<a class=\"anchor\" id=\"dp-re\"><\/a>\n<a href=\"#toc\">Back to the table of contents<\/a>"}}