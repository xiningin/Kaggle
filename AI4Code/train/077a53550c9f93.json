{"cell_type":{"03d58aa9":"code","d77e482e":"code","88b8c98c":"code","abaa19be":"code","6774fc4b":"code","d6bd66ab":"code","2e877830":"code","62408242":"code","dacac105":"code","bed7fa19":"code","090d94ba":"code","24a7567d":"code","90d9580d":"code","6841f649":"code","66889a05":"code","02f8c02e":"code","7a8b6661":"code","da57270d":"code","69e7875b":"code","2b6e3566":"code","940299de":"code","a578bb50":"code","121b3974":"code","5991fc45":"code","a04423d4":"code","eac96d9a":"code","415cefaa":"code","7f6475c8":"code","62f06b97":"code","89f70291":"code","440bdb35":"code","534929da":"code","b34752cd":"code","42476da3":"code","d80096c8":"code","cbeb1e2e":"code","7117173c":"code","7abfbd19":"code","61c934ba":"code","0bc5a72b":"markdown","90109f06":"markdown","35826366":"markdown","dc2c0ce1":"markdown","893e3f51":"markdown","39427822":"markdown","1361cfe2":"markdown","f1de15cd":"markdown","2e58cbe4":"markdown","cff9245b":"markdown","29c34ec0":"markdown","dae53b47":"markdown","b3f066e9":"markdown","668572fd":"markdown","9e26983a":"markdown","185e8fec":"markdown","9a863e22":"markdown"},"source":{"03d58aa9":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport seaborn as sns\nimport warnings\nfrom sklearn.model_selection import train_test_split as tts\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.model_selection import RandomizedSearchCV","d77e482e":"Train_Titanic = pd.read_csv(\"..\/input\/titanic\/train.csv\")\nprint(Train_Titanic.shape)\nTrain_Titanic.head(3)","88b8c98c":"#Checking for null values\nTrain_Titanic.isnull().sum()","abaa19be":"#Cabin feature has 77.10% of null values. So I'm removing that column\nTrain_Titanic = Train_Titanic.drop('Cabin', axis = 1)\nTrain_Titanic.head(2)","6774fc4b":"# we can observed two nalues in Embarked column, to treat it I'm replacing with grouped mode value\nTrain_Titanic['Embarked'] = Train_Titanic.groupby(['Pclass','Sex'], sort = False)['Embarked'].apply(lambda x : x.fillna(x.mode().iloc[0]))","d6bd66ab":"warnings.filterwarnings('ignore')\nAge_Predictor = Train_Titanic[[ 'Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Embarked', 'Fare']]\nAge_Predictor['Sex'] = Age_Predictor.Sex.map({'male':0, 'female':1})\nAge_Predictor = pd.get_dummies(Age_Predictor).drop('Embarked_S', axis = 1)\nAge_Predictor_train = Age_Predictor[Age_Predictor.Age.isnull() != True]\nAge_Predictor_test = Age_Predictor[Age_Predictor.Age.isnull() == True]\nAge_Predictor.head(3)","2e877830":"print(Age_Predictor_test.shape)\nprint(Age_Predictor_train.shape)\n\nX_train = Age_Predictor_train.drop('Age', axis = 1)\nX_test = Age_Predictor_test.drop('Age', axis = 1)\nY_train = Age_Predictor_train['Age']\nY_test = Age_Predictor_test['Age']\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","62408242":"from sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\nRF.fit(X_train,Y_train)\nAge_Pred = RF.predict(X_test)\nAge_Predictor_test['Age'] = Age_Pred\n#Test_Titanic_data['Age'] = Age_Pred\nprint(\"Accuracy on Traing set: \",RF.score(X_train,Y_train))\nAge_Predictor_train['index'] = Age_Predictor_train.index\nAge_Predictor_test['index'] = Age_Predictor_test.index\ntrain_df = Age_Predictor_train.append(Age_Predictor_test)","dacac105":"Train_Titanic['index'] =Train_Titanic.index\nTrain_Titanic = Train_Titanic.drop('Age', axis = 1)\nTrain_Titanic = train_df[['index', 'Age']].merge(Train_Titanic, on = 'index', how = 'inner')\nTrain_Titanic = Train_Titanic.drop('index', axis = 1)","bed7fa19":"Train_Titanic.isnull().sum()","090d94ba":"colors = ['#00CC96','#636EFA']\nl = Train_Titanic.groupby('Sex').agg({'Pclass':'count'}).reset_index()\nl['Sex_Percentage'] = (l['Pclass']\/sum(l['Pclass']) *100).round(2).astype(str) + '%'\nfig = go.Figure(data=[go.Bar(x=l.Sex, y=l.Pclass, \n                             marker_color=colors , text=l.Sex_Percentage)])\nfig.update_layout(title_text='No.of Male and Female Passengers in Titanic - Male > female',autosize=False, width=550,height=450)","24a7567d":"colors = ['#00CC96','#636EFA','#AB63FA']\nl = Train_Titanic.groupby('Pclass').agg({'Sex':'count'}).reset_index()\nl['Pclass'] =  \"Class\"+\" - \"+l.Pclass.astype(str)\n\nl['Pclass_Percentage'] = (l['Sex']\/sum(l['Sex']) *100).round(2).astype(str) + '%'\n\n\nfig = px.pie(l, values='Sex', names='Pclass',\n             title='Most No.of Passengers in Titanic are preffered to travel in 3rd class',\n             hover_data=['Pclass_Percentage'], width=620, height = 450)\nfig.update_traces(textposition='outside', textinfo='percent+label')\nfig.show()","90d9580d":"colos = ['#00CC96','#636EFA']\nl = Train_Titanic.groupby('Survived').agg({'Sex':'count'}).reset_index()\n\nl['Survived'] = l.Survived.map({0: 'Not Survived', 1 : 'Survived'})\n\nl['Survived_Percentage'] = (l['Sex']\/sum(l['Sex']) *100).round(2).astype(str) + '%'\n\n\nfig = px.pie(l, values='Sex', names='Survived',\n             title='Only 38.4% of passengers are survived after titanic crash',\n             hover_data=['Survived_Percentage'], width=620, height = 450  )\nfig.update_traces(textposition='outside', textinfo='percent+label')\nfig.show()","6841f649":"colors = ['#00CC96','#636EFA','#AB63FA']\nl = Train_Titanic.groupby('Embarked').agg({'Sex':'count'}).reset_index()\nl['Pclass_Percentage'] = (l['Sex']\/sum(l['Sex']) *100).round(2).astype(str) + '%'\n\nfig = px.pie(l, values='Sex', names='Embarked',\n             title='Most No.of Passengers were embarked at Southampton',\n             hover_data=['Pclass_Percentage'], width=620, height = 450)\nfig.update_traces(textposition='outside', textinfo='percent+label')\nfig.show()\n","66889a05":"import plotly.figure_factory as ff\nAge = [Train_Titanic.Age]\nm = ['Age']\nfig = ff.create_distplot(Age,m, bin_size = 10)\nfig.update_layout(title_text='Most of the passengers age between 20-30',autosize=False, width=750,height=450)\nfig.show()","02f8c02e":"l = Train_Titanic.groupby(['Sex','Survived']).agg({'Pclass':'count'}).reset_index().sort_values(by = 'Pclass', ascending=True).rename(columns={'Pclass':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Sex', barmode='group',\n             text='Percentage')\nfig.update_layout(title_text='Female Passengers are survived more than male passengers',autosize=False, width=550,height=450)\nfig.show()","7a8b6661":"l = Train_Titanic.groupby(['Pclass','Survived']).agg({'Sex':'count'}).reset_index().sort_values(by = 'Sex', ascending=True).rename(columns={'Sex':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\nl['Pclass'] =  \"Class\"+\" - \"+l.Pclass.astype(str)\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Pclass', barmode='group',\n             text='Percentage')\nfig.update_layout(title_text='Class-1 passengers are survived more than other class passengers',autosize=False, width=605,height=450)\nfig.show()","da57270d":"l = Train_Titanic.groupby(['Embarked','Survived']).agg({'Sex':'count'}).reset_index().sort_values(by = 'Sex', ascending=True).rename(columns={'Sex':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Embarked', barmode='group',\n             text='Percentage')\nfig.update_layout(title_text='Class-1 passengers are survived more than other class passengers',autosize=False, width=605,height=450)\nfig.show()","69e7875b":"l = Train_Titanic.groupby(['Pclass','Survived', 'Sex']).agg({'Name':'count'}).reset_index().sort_values(by = 'Name', ascending=True).rename(columns={'Name':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\nl['Pclass'] =  \"Class\"+\" - \"+l.Pclass.astype(str)\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Pclass', barmode='group',\n             text='Percentage',hover_data=['Sex'])\nfig.update_layout(title_text='In Class-1,2,3 female passengers are survived more than male passengers',autosize=False, width=678,height=450)\nfig.show()","2b6e3566":"l = Train_Titanic.groupby(['Pclass','Survived', 'Embarked']).agg({'Name':'count'}).reset_index().sort_values(by = 'Name', ascending=True).rename(columns={'Name':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\nl['Pclass'] =  \"Class\"+\" - \"+l.Pclass.astype(str)\n\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Pclass', barmode='group',\n             text='Percentage',hover_data=['Embarked'])\nfig.update_layout(title_text='In Class-1,2,3 passengers embarked at Southampton are survived more',autosize=False, width=678,height=450)\nfig.show()","940299de":"l = Train_Titanic.groupby(['Sex','Survived', 'Embarked']).agg({'Name':'count'}).reset_index().sort_values(by = 'Name', ascending=True).rename(columns={'Name':'Count'})\nl['Percentage'] = (l['Count']\/sum(l['Count']) *100).round(2).astype(str) + '%'\nl['Survived'] = l.Survived.map({0 : 'Not Survived',1: 'Survived'})\n\n\nfig = px.bar(l, x=\"Survived\", y=\"Count\",\n             color='Embarked', barmode='group',\n             text='Percentage',hover_data=['Sex'])\nfig.update_layout(title_text='Female passengers survived more Southampton are survived more',autosize=False, width=678,height=450)\nfig.show()","a578bb50":"# Spliting data for training the model. Splitting the data will be done at the begining of feature seletion phase to avoid the overfitting condition\nfrom sklearn.model_selection import train_test_split\nX = Train_Titanic[['Sex','Age','Pclass','SibSp','Parch','Fare','Embarked']]\nY = Train_Titanic['Survived']\nX['Sex'] = X.Sex.map({'male':0, 'female':1})\nX= pd.get_dummies(X)\n\nX_train, X_test, Y_train, Y_test = tts(X, Y, test_size=0.20,random_state=42)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","121b3974":"from sklearn.preprocessing import MinMaxScaler\n#Minmax transformation of train data\ndef minmaxtrain(x):\n    minmax = MinMaxScaler()\n    column = x.columns\n    global train_ads\n    train_ads = pd.DataFrame(minmax.fit_transform(x),columns = column)\n    return train_ads \n\n#Minmax transformation of test data\ndef minmaxtest(x):\n    minmax = MinMaxScaler()\n    column = x.columns\n    global test_ads\n    test_ads = pd.DataFrame(minmax.fit_transform(x),columns = column)\n    return test_ads  ","5991fc45":"minmaxtrain(X_train)\nminmaxtest(X_test)\nprint(train_ads.shape)\nprint(test_ads.shape)\nprint(train_ads.head(2))\nprint(test_ads.head(2))","a04423d4":"from sklearn.feature_selection import mutual_info_classif\n\nmic =mutual_info_classif(train_ads, Y_train)\n\nmutual_info = pd.Series(mic)\nmutual_info.index = train_ads.columns\nmutual_info.sort_values(ascending=False)\n\n#Considering the columns for training the model which are atleast 5% of information shared with dependent variable\/feature\nReq_Cols = list(mutual_info[mutual_info>0.005].index)\nReq_Cols","eac96d9a":"mutual_info","415cefaa":"train_ads_ = train_ads[Req_Cols]\ntest_ads_ = test_ads[Req_Cols]\ntrain_ads_.head(2)","7f6475c8":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nmodel_names = {\n    \"Logistic Regressor\":\n    {\n        \"model\": LogisticRegression()\n    },\n    \"KNN Classfier\":\n    {\n        \"model\": KNeighborsClassifier()\n    }\n}\n\n","62f06b97":"for model_name, mn in model_names.items():\n    model = mn['model']\n    model.fit(train_ads_, Y_train)\n    y_pred= model.predict(test_ads_)\n    print(model_name)\n    print(classification_report(Y_test, y_pred))\n    print(confusion_matrix(Y_test, y_pred))\n    print(accuracy_score(Y_test, y_pred))\n    print('\\n \\n')","89f70291":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\nmodel_names = {\n    \"Gaussian Classifier\":\n    {\n        \"model\": GaussianNB()\n    },\n    \"Decision Tree Classfier\":\n    {\n        \"model\": DecisionTreeClassifier()\n    },\n    \"Random Forest Classfier\":\n    {\n        \"model\": RandomForestClassifier()\n    },\n    \"XgBoost Classfier\":\n    {\n        \"model\": GradientBoostingClassifier()\n    }\n}\n","440bdb35":"for model_name, mn in model_names.items():\n    model = mn['model']\n    model.fit(X_train, Y_train)\n    y_pred= model.predict(X_test)\n    print(model_name)\n    print(classification_report(Y_test, y_pred))\n    print(\"Confusin Matrix: \\n\",confusion_matrix(Y_test, y_pred))\n    print(\"Accuracy: \\t\",accuracy_score(Y_test, y_pred))\n    print('\\n \\n')","534929da":"import xgboost as xgb\nxgb_model = xgb.XGBClassifier()\n\nlearning_rate = [0.01,0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ]\nmax_depth = [int(x) for x in np.linspace(5, 40, num = 6)]\nmin_child_weight = [int(x) for x in np.linspace(1, 20, num = 6)]\nsubsample =  [0.5, 0.7]\ngamma =[ 0.0, 0.1, 0.2 , 0.3, 0.4 ]\ncolsample_bytree = [ 0.3, 0.4, 0.5 , 0.7 ]\nobjective = ['reg:squarederror']\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 40)]\n\n\n\nrandom_grid = {'learning_rate': learning_rate,\n               'max_depth': max_depth,\n               'min_child_weight': min_child_weight,\n               'subsample': subsample,\n               'colsample_bytree': colsample_bytree,\n               'objective': objective,\n               'n_estimators': n_estimators}\n\n\nrf_random = RandomizedSearchCV(estimator = xgb_model, param_distributions = random_grid, n_iter = 10, cv = 5, verbose=2, random_state=42, n_jobs = 1)\n\nrf_random.fit(X_train, Y_train)","b34752cd":"k = rf_random.best_params_\nk","42476da3":"xgb_model = xgb.XGBClassifier(param_distributions = k)\nxgb_model.fit(X_train, Y_train)\ny_pred= xgb_model.predict(X_test)\nprint(classification_report(Y_test, y_pred))\nprint(\"Confusin Matrix: \\n\",confusion_matrix(Y_test, y_pred))\nprint(\"Accuracy: \\t\",accuracy_score(Y_test, y_pred))\n\naccuracy_xgb = accuracy_score(Y_test, y_pred)","d80096c8":"Score = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(train_ads_, Y_train)\n    y_pred= knn.predict(test_ads_)\n    score_1=accuracy_score(Y_test, y_pred)\n    Score.append(score_1)\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),Score,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=1)\nplt.title('Accuracy vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Accuracy')\nprint(\"Max Accuracy error:-\",max(Score),\"at K =\",Score.index(max(Score))+1)","cbeb1e2e":"best_k = Score.index(max(Score))+1\nknn = KNeighborsClassifier(n_neighbors=best_k)\nknn.fit(train_ads_, Y_train)\ny_pred= knn.predict(test_ads_)\naccuracy_KNN = accuracy_score(Y_test, y_pred)\nprint(\"Confusin Matrix: \\n\",confusion_matrix(Y_test, y_pred))","7117173c":"Test_Titanic = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nTest_Titanic.isnull().sum()","7abfbd19":"Titanic_Test_Ads = Test_Titanic.copy()\nTest_Titanic = Test_Titanic.drop('Cabin', axis = 1)\nTest_Titanic['Fare'] = Test_Titanic.groupby(['Pclass','Sex','Embarked'])['Fare'].apply(lambda x : x.fillna(x.mean()))\nwarnings.filterwarnings('ignore')\nAge_Predictor = Test_Titanic[['Pclass', 'Sex', 'Age', 'SibSp','Parch', 'Embarked', 'Fare']]\nAge_Predictor['Sex'] = Age_Predictor.Sex.map({'male':0, 'female':1})\nAge_Predictor = pd.get_dummies(Age_Predictor).drop('Embarked_S', axis = 1)\nAge_Predictor_train = Age_Predictor[Age_Predictor.Age.isnull() != True]\nAge_Predictor_test = Age_Predictor[Age_Predictor.Age.isnull() == True]\nAge_Predictor.head(3)\n\nprint(Age_Predictor_test.shape)\nprint(Age_Predictor_train.shape)\n\nX_train = Age_Predictor_train.drop('Age', axis = 1)\nX_test = Age_Predictor_test.drop('Age', axis = 1)\nY_train = Age_Predictor_train['Age']\nY_test = Age_Predictor_test['Age']\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)\n\nprint(X_train.isnull().sum())\n\nfrom sklearn.ensemble import RandomForestRegressor\nRF = RandomForestRegressor()\nRF.fit(X_train,Y_train)\nAge_Pred = RF.predict(X_test)\nAge_Predictor_test['Age'] = Age_Pred\nAge_Predictor_train['index'] = Age_Predictor_train.index\nAge_Predictor_test['index'] = Age_Predictor_test.index\ntrain_df = Age_Predictor_train.append(Age_Predictor_test)\nTest_Titanic['index'] =Test_Titanic.index\nTest_Titanic = Test_Titanic.drop('Age', axis = 1)\nTest_Titanic = train_df[['index', 'Age']].merge(Test_Titanic, on = 'index', how = 'inner')\nTest_Titanic = Test_Titanic.drop('index', axis = 1)\nX_test = Test_Titanic[['Sex','Age','Pclass','SibSp','Parch','Fare','Embarked']]\nX_test['Sex'] = X_test.Sex.map({'male':0, 'female':1})\nX_test= pd.get_dummies(X_test)","61c934ba":"if accuracy_KNN > accuracy_xgb:\n    minmaxtrain(X)\n    minmaxtest(X_test)\n    X_train_ = train_ads[Req_Cols]\n    X_test_ = test_ads[Req_Cols]\n    Y_train_ = Y\n    knn = KNeighborsClassifier(n_neighbors=best_k)\n    knn.fit(X_train_, Y_train_)\n    y_pred= knn.predict(X_test_)\n    Titanic_Test_Ads['Pred Survived'] = y_pred\n    print(\"KNN Classifier has been choosen\")\n\nelse:\n    X_train_ = X[Req_Cols]\n    X_test_ = X_test[Req_Cols]\n    Y_train_ = Y\n    xgb_model = xgb.XGBClassifier(param_distributions = k)\n    xgb_model.fit(X_train_, Y_train_)\n    y_pred= xgb_model.predict(X_test_)\n    Titanic_Test_Ads['Pred Survived'] = y_pred\n    print(\"XgBoost Classifier has been choosen\")\n\nTitanic_Test_Ads.tail(3)","0bc5a72b":"## Feature Selection-Information gain - mutual information In Classification Problem Statements\n\nMI Estimate mutual information for a discrete target variable.\n\nMutual information (MI) between two random variables is a non-negative value, which measures the dependency between the variables. It is equal to zero if and only if two random variables are independent, and higher values mean higher dependency.\n\nThe function relies on nonparametric methods based on entropy estimation from k-nearest neighbors distances.\n\n### Inshort\n\nA quantity called mutual information measures the amount of information one can obtain from one random variable given another","90109f06":"#### We are getting good accuracy from KNN Classifier, Randomforest Classifier and XgBoost Classifier..... Let's try to perform performing hyperparameter tuning to KNN Classifier, Randomforest Classifier and XgBoost Classifier ","35826366":"### Gathering required data for analysis","dc2c0ce1":"## Treating null values","893e3f51":"## Data Description & Feature Engineering - Data Cleaning and creating an ADS for Analysis\n* Age - Passenger Age\n* Sibsp - Number of Siblings \/ Spouses Aboard\n* Parch - Number of Parents \/ Children Aboard\n* Fare - Fare Amount Paid in British Pounds\n* Female - Binary variable indicating whether passenger is female\n* Male - Binary variable indicating whether passenger is male\n* Embark-C - Binary variable indicating whether passenger embarked at Cherbourg\n* Embark-Q - Binary variable indicating whether passenger embarked at Queenstown\n* Embark-S - Binary variable indicating whether passenger embarked at Southampton\n* PClass-1 - Binary variable indicating whether passenger was in first class\n* PClass-2 - Binary variable indicating whether passenger was in second class\n* PClass-3 - Binary variable indicating whether passenger was in third class","39427822":"\n## Feature Engineering & Feature Selection For Test Dataset\n","1361cfe2":"## Hyperparameter tuning XgBoost Classifier","f1de15cd":"We are getting same good accuracy for above two algorithms. So, we can choose anyone of them","2e58cbe4":"\n\nI've tried with Logestic Regression, KNN Classifier, Randomforest Classifier, Decision Tree Classifier, Gaussians Classifier, XgBoost Classifier and hyperparameter tuning for XGB Classifier & KNN Classifier. \n\n### If you like this notebook .... Please Upvote add up your comments ... Happy coding :)\n","cff9245b":"### Treating Age column null values  \n- In our dataset, We have 177 null values for Age column, if we replace with mean\/median or some random values, It might effects on accuracy or model performance. To make model more efficiency, I'm predicting the value by using egression algorithms\n- Here I'm making Age as a dependent variable and rest all are independent variables. All the null values of LotFrontage, I'm considering as test dataset. So that we can predict the missing values\n","29c34ec0":"#### There are no-null values in above created ADS. We are good to go\n\n## Exploratory Data Analysis","dae53b47":"## Creating a pipeline for Machine Learining models which requires feature scaling","b3f066e9":"## Hyperparameter tuning for KNN Classifier","668572fd":"## Creating a pipeline for Machine Learining models which doesn not required feature scaling","9e26983a":"Scaled the data for each metrics by using feature scaling techniques to reduce the bias, to normalize the data within a range and speeding up the calculation while training the model. After applying the MinMax Scaler, data range is in between 0 to 1\n","185e8fec":"## Model Evaluation\n\n![](https:\/\/miro.medium.com\/max\/3000\/1*uR09zTlPgIj5PvMYJZScVg.png)\n\n### Confusion Matrix: \n* It is a 2*2 matrix where top values are Actual values and leftside are predicted values and it is used to describe the performance of a classification model on set of test data for which the true values are known\n\n### Accuracy:\n*  Number of correct predictions made by the model over all kinds predictions made. Whenever target variable in data set is nearly balanced then we can use accuracy as a key metrics. Above dataset is nearly balanced with the ratio of 61:39 so we can consider Accuracy as primary metrics\n\n### If the dataset is imbalanced, we can use Precision\/Recall\/F-beta score \n\n### Precision (Positive prediction value or Specificity):\n* It is used to measure, Out of total predicted positives, how many were actuall positives\n\n### Recall (True Positive Rate or Sensitivity):\n* Out of total actual positive results, how many positives did we predicted correctly\n\n\n### F-Beta Score:\n![](https:\/\/www.mikulskibartosz.name\/assets\/images\/2019-02-04-f1-score-explained\/f_beta.png)\n* F1 score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account\n\n### Note: \n* If False positive and False neagative both are equally impoortant and equally impacting out output variable, we can consider beta = 1\n* If False positive is more important, we can reduce the beta value i.e., beta value ranges between 0 to 1\n* If False negative is more important then value of beta = 2","9a863e22":"## Feature Selection"}}