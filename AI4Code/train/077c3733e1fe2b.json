{"cell_type":{"3131b570":"code","ec8247d8":"code","c4f3d037":"code","0cb7a9b2":"code","7dc5e1f7":"code","65bf3042":"code","f2cf5024":"code","1667b10e":"code","5fd061f4":"code","89d65646":"code","11d71acb":"code","8ea24ab8":"code","9bb04281":"code","36b3463b":"code","45374227":"code","5b589e5b":"code","cb2cec56":"code","3a400075":"code","27350ae6":"code","6d4b7c41":"code","4bfe391e":"code","5fcf9cae":"code","81d0171b":"code","eab5bcdb":"code","617994fa":"code","37f54c45":"code","4d34adfe":"markdown","e1c75b0b":"markdown","1afd382e":"markdown","baa9f2b3":"markdown","5ec8ee37":"markdown","fbec248f":"markdown","d4473087":"markdown","2efda2b6":"markdown","b7e632c5":"markdown","b16978c6":"markdown","c974b044":"markdown","ee28e63b":"markdown","985c7c11":"markdown","e7f3907b":"markdown","214a405f":"markdown","749e7760":"markdown"},"source":{"3131b570":"!pip install nlpaug","ec8247d8":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.cuda.amp import autocast, GradScaler\n\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport nlpaug.flow as nafc\nfrom nlpaug.util import Action\n\nfrom transformers import (AutoModel, \n                          AutoModelForMaskedLM,\n                          AutoTokenizer,\n                          AutoConfig,\n                          AdamW,\n                          LineByLineTextDataset,\n                          DataCollatorForLanguageModeling,\n                          Trainer,\n                          TrainingArguments)\n\nfrom transformers.optimization import get_cosine_schedule_with_warmup\n\nfrom sklearn.model_selection import StratifiedKFold\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport os, random, gc, warnings\n\ngc.enable()\nwarnings.filterwarnings(\"ignore\")","c4f3d037":"df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/validation_data.csv\")\ntest_df = pd.read_csv(\"..\/input\/jigsaw-toxic-severity-rating\/comments_to_score.csv\")","0cb7a9b2":"pretrain_text = pd.concat([df.less_toxic, df.more_toxic, test_df.text])\npretrain_text.drop_duplicates(inplace = True)\npretrain_text.reset_index(drop = True, inplace = True)\npretrain_text = pretrain_text.apply(lambda x: x.replace('\\n',''))","7dc5e1f7":"text  = '\\n'.join(pretrain_text.tolist())\n\nwith open('text.txt','w', encoding='utf-8') as f:\n    f.write(text)","65bf3042":"class pretrain_cfg:\n    model_name = 'roberta-base'\n    epochs = 1 # adjust\n    learning_rate = 5e-05\n    train_batch_size = 32\n    eval_batch_size = 32\n    eval_steps = 100\n    block_size = 256\n    gradient_accum_steps = 2\n    mlm_prob = 0.15\n    fp16 = True\n    output_dir = 'roberta_pt'","f2cf5024":"model = AutoModelForMaskedLM.from_pretrained(pretrain_cfg.model_name)\ntokenizer = AutoTokenizer.from_pretrained(pretrain_cfg.model_name)\ntokenizer.save_pretrained(pretrain_cfg.output_dir);","1667b10e":"# Sequences are truncated to block size\ntrain_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",\n    block_size=pretrain_cfg.block_size)\n\nvalid_dataset = LineByLineTextDataset(\n    tokenizer=tokenizer,\n    file_path=\"text.txt\",\n    block_size=pretrain_cfg.block_size)\n\ndata_collator = DataCollatorForLanguageModeling(\n    tokenizer=tokenizer, \n    mlm=True, \n    mlm_probability=pretrain_cfg.mlm_prob)","5fd061f4":"training_args = TrainingArguments(\n    output_dir=pretrain_cfg.output_dir+'_chk',\n    overwrite_output_dir=True,\n    num_train_epochs=pretrain_cfg.epochs,\n    per_device_train_batch_size=pretrain_cfg.train_batch_size,\n    per_device_eval_batch_size=pretrain_cfg.eval_batch_size,\n    learning_rate=pretrain_cfg.learning_rate,\n    gradient_accumulation_steps=pretrain_cfg.gradient_accum_steps,\n    fp16=pretrain_cfg.fp16,\n    eval_steps=pretrain_cfg.eval_steps,\n    evaluation_strategy='steps',\n    save_total_limit=2,\n    metric_for_best_model='eval_loss',\n    greater_is_better=False,\n    load_best_model_at_end=True,\n    prediction_loss_only=True,\n    report_to='none')","89d65646":"trainer = Trainer(\n    model=model,\n    args=training_args,\n    data_collator=data_collator,\n    train_dataset=train_dataset,\n    eval_dataset=valid_dataset)","11d71acb":"trainer.train()\ntrainer.save_model(pretrain_cfg.output_dir)","8ea24ab8":"# adjust aug_max as needed, the run time can be very long..\naug = naw.SynonymAug(aug_src='wordnet', aug_min=1, aug_max=2) ","9bb04281":"df_aug = df.copy()\nnew_less_toxic = df.less_toxic.apply(lambda txt: aug.augment(txt))\nnew_more_toxic = df.more_toxic.apply(lambda txt: aug.augment(txt))\n\ndf_aug.less_toxic = new_less_toxic\ndf_aug.more_toxic = new_more_toxic","36b3463b":"df_aug = pd.concat([df, df_aug])\ndf_aug.reset_index(drop=True, inplace=True)","45374227":"class cfg:\n    seed = 123 \n    train_bs = 20\n    valid_bs = 24\n    epochs = 5\n    lr = 5e-5\n    wd = 1e-6\n    fp16 = True\n    grad_accum_steps = 1\n    folds = 5\n    margin = 0.5\n    max_len = 128\n    model_name = pretrain_cfg.output_dir\n    output_hidden_states = False\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    checkpoint = lambda fold: f'model_{fold}.pt'\n\ncfg.tokenizer = AutoTokenizer.from_pretrained(cfg.model_name)","5b589e5b":"skf = StratifiedKFold(n_splits=cfg.folds, shuffle=True, random_state=cfg.seed)\n\nfor fold, ( _, val_) in enumerate(skf.split(X=df_aug, y=df_aug.worker)):\n    df_aug.loc[val_ , \"kfold\"] = int(fold)\n    \ndf_aug[\"kfold\"] = df_aug[\"kfold\"].astype(int)","cb2cec56":"def seed_all(seed_value):\n    random.seed(seed_value) # Python\n    np.random.seed(seed_value) # cpu vars\n    torch.manual_seed(seed_value) # cpu  vars\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value) # gpu vars\n        torch.backends.cudnn.deterministic = True  #needed\n        torch.backends.cudnn.benchmark = False\n        \nseed_all(cfg.seed)","3a400075":"class JigsawDataset(Dataset):\n    def __init__(self, df, tokenizer, max_length):\n        self.df = df\n        self.max_len = max_length\n        self.tokenizer = tokenizer\n        self.more_toxic = df['more_toxic'].values\n        self.less_toxic = df['less_toxic'].values\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, index):\n        more_toxic = self.more_toxic[index]\n        less_toxic = self.less_toxic[index]\n        inputs_more_toxic = self.tokenizer.encode_plus(\n                                more_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        inputs_less_toxic = self.tokenizer.encode_plus(\n                                less_toxic,\n                                truncation=True,\n                                add_special_tokens=True,\n                                max_length=self.max_len,\n                                padding='max_length'\n                            )\n        target = 1\n        \n        more_toxic_ids = inputs_more_toxic['input_ids']\n        more_toxic_mask = inputs_more_toxic['attention_mask']\n        \n        less_toxic_ids = inputs_less_toxic['input_ids']\n        less_toxic_mask = inputs_less_toxic['attention_mask']\n        \n        \n        return {\n            'more_toxic_ids': torch.tensor(more_toxic_ids, dtype=torch.long),\n            'more_toxic_mask': torch.tensor(more_toxic_mask, dtype=torch.long),\n            'less_toxic_ids': torch.tensor(less_toxic_ids, dtype=torch.long),\n            'less_toxic_mask': torch.tensor(less_toxic_mask, dtype=torch.long),\n            'target': torch.tensor(target, dtype=torch.long)\n        }","27350ae6":"class JigsawModel(nn.Module):\n    def __init__(self, model_name):\n        super().__init__()\n        self.model = AutoModel.from_pretrained(\n                     model_name,\n                     output_hidden_states=cfg.output_hidden_states)\n        \n        self.dropout = nn.Dropout(p=0.2)\n        \n        self.fc = nn.Linear(768, 1)\n        self._init_weights(self.fc)\n        \n        \n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=0.02)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n            \n        \n    def forward(self, ids, mask):        \n        out = self.model(input_ids=ids,attention_mask=mask).pooler_output\n        \n#         last_hidden = self.model(input_ids=ids,attention_mask=mask).last_hidden_state\n#         mean_pool = torch.mean(last_hidden, 1)\n#         _, max_pool = torch.max(last_hidden, 1)\n#         mean_max_embeddings = torch.cat((mean_pool, max_pool), 1)\n\n        out = self.dropout(out)\n        outputs = self.fc(out)\n        return outputs","6d4b7c41":"def criterion(outputs1, outputs2, targets):\n    return nn.MarginRankingLoss(margin=cfg.margin)(outputs1, outputs2, targets)","4bfe391e":"class AverageMeter:\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","5fcf9cae":"class EarlyStopping:\n    def __init__(self, patience=7, mode=\"max\", delta=0.001, verbose = None):\n        self.patience = patience\n        self.counter = 0\n        self.mode = mode\n        self.best_score = None\n        self.early_stop = False\n        self.delta = delta\n        self.verbose = verbose\n        if self.mode == \"min\":\n            self.val_score = np.Inf\n        else:\n            self.val_score = -np.Inf\n\n    def __call__(self, epoch_score, model, model_path):\n\n        if self.mode == \"min\":\n            score = -1.0 * epoch_score\n        else:\n            score = np.copy(epoch_score)\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n        elif score < self.best_score: #  + self.delta\n            self.counter += 1\n            if self.verbose:\n                print('EarlyStopping counter: {} out of {}'.format(self.counter, self.patience))\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(epoch_score, model, model_path)\n            self.counter = 0\n\n    def save_checkpoint(self, epoch_score, model, model_path):\n        if epoch_score not in [-np.inf, np.inf, -np.nan, np.nan]:\n            if self.verbose:\n                print('Validation score improved ({:.4f} --> {:.4f}). Saving model!'.format(self.val_score, epoch_score))\n                \n            torch.save(model.state_dict(), model_path)\n        self.val_score = epoch_score","81d0171b":"def make_loaders(df, fold):\n    df_train = df[df.kfold != fold].reset_index(drop=True)\n    df_valid = df[df.kfold == fold].reset_index(drop=True)\n    \n    train_dataset = JigsawDataset(df_train, tokenizer=cfg.tokenizer, max_length=cfg.max_len)\n    valid_dataset = JigsawDataset(df_valid, tokenizer=cfg.tokenizer, max_length=cfg.max_len)\n\n    train_loader = DataLoader(train_dataset, \n                              batch_size=cfg.train_bs, \n                              num_workers=2, \n                              shuffle=True, \n                              pin_memory=True,\n                             )\n    \n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=cfg.valid_bs, \n                              num_workers=2, \n                              shuffle=False, \n                              pin_memory=True\n                             )\n    \n    return train_loader, valid_loader","eab5bcdb":"class Trainer:\n    def __init__(self, model, device, loss_fn, opt, scheduler = None, grad_accum_steps = 1, fp16 = False):\n        self.model = model\n        self.device = device\n        self.loss_fn = loss_fn\n        self.opt = opt\n        self.scheduler = scheduler\n        self.fp16 = fp16\n        self.scaler = GradScaler() if fp16 else None\n        self.grad_accum_steps = grad_accum_steps\n        \n    def train_one_epoch(self, dl):\n        self.model.train()\n        self.model.zero_grad()\n        \n        losses = AverageMeter()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), leave = False)\n        for bi, d in prog_bar:\n            more_toxic_ids = d['more_toxic_ids'].to(self.device, dtype = torch.long)\n            more_toxic_mask = d['more_toxic_mask'].to(self.device, dtype = torch.long)\n            less_toxic_ids = d['less_toxic_ids'].to(self.device, dtype = torch.long)\n            less_toxic_mask = d['less_toxic_mask'].to(self.device, dtype = torch.long)\n            targets = d['target'].to(self.device, dtype=torch.long)\n            \n            with autocast(enabled=self.fp16):\n                more_toxic_outputs = self.model(more_toxic_ids, more_toxic_mask)\n                less_toxic_outputs = self.model(less_toxic_ids, less_toxic_mask)\n                loss = self.loss_fn(more_toxic_outputs, less_toxic_outputs, targets)\n                prog_bar.set_description('loss: {:.2f}'.format(loss.item()))\n                \n            loss = loss \/ self.grad_accum_steps\n            losses.update(loss.item(), more_toxic_ids.size(0))\n            \n            if self.fp16: self.scaler.scale(loss).backward()\n            else: loss.backward()\n                \n            if bi % self.grad_accum_steps == 0 or bi == len(dl) - 1:\n                if self.fp16:\n                    self.scaler.step(self.opt)\n                    self.scaler.update()\n                else:\n                    self.opt.step()\n            \n            if self.scheduler: self.scheduler.step()\n            self.opt.zero_grad()      \n            \n    @torch.no_grad()\n    def eval_one_epoch(self, dl, **kwargs):\n        self.model.eval()\n        losses = AverageMeter()\n        prog_bar = tqdm(enumerate(dl), total = len(dl), leave = False)\n        for bi, d in prog_bar:  \n            more_toxic_ids = d['more_toxic_ids'].to(self.device, dtype = torch.long)\n            more_toxic_mask = d['more_toxic_mask'].to(self.device, dtype = torch.long)\n            less_toxic_ids = d['less_toxic_ids'].to(self.device, dtype = torch.long)\n            less_toxic_mask = d['less_toxic_mask'].to(self.device, dtype = torch.long)\n            targets = d['target'].to(self.device, dtype=torch.long)\n            \n            with autocast(enabled=self.fp16):\n                more_toxic_outputs = self.model(more_toxic_ids, more_toxic_mask)\n                less_toxic_outputs = self.model(less_toxic_ids, less_toxic_mask)\n                loss = self.loss_fn(more_toxic_outputs, less_toxic_outputs, targets)\n                    \n            losses.update(loss.item(), more_toxic_ids.size(0))\n        print(f\"F{kwargs['fold']} E{str(kwargs['epoch']):2s}\"\\\n              f\" Valid Loss: {losses.avg:.4f}\")   \n        return losses.avg          ","617994fa":"def train_fold(fold, epochs = 5):\n    train_dl, valid_dl = make_loaders(df_aug, fold)\n    es = EarlyStopping(patience = 5, mode=\"min\", verbose = False)\n    \n    num_update_steps_per_epoch = len(train_dl)\n    max_train_steps = epochs * num_update_steps_per_epoch\n    warmup_proportion = 0\n    if warmup_proportion != 0:\n        warmup_steps = math.ceil((max_train_steps * 2) \/ 100)\n    else:\n        warmup_steps = 0\n    \n    model = JigsawModel(cfg.model_name).to(cfg.device)\n    \n    opt = AdamW(model.parameters(),\n                lr = cfg.lr,\n                weight_decay= cfg.wd\n               )\n    \n    scheduler = get_cosine_schedule_with_warmup(\n                opt,\n                num_warmup_steps=warmup_steps,\n                num_training_steps=max_train_steps\n                )\n    \n    \n    trainer = Trainer(model, \n                      cfg.device,\n                      opt=opt,\n                      loss_fn = criterion,\n                      scheduler=scheduler,\n                      grad_accum_steps=cfg.grad_accum_steps,\n                      fp16=cfg.fp16\n                     )\n    \n    for epoch in range(epochs):\n        trainer.train_one_epoch(train_dl)\n        valid_loss = trainer.eval_one_epoch(valid_dl, fold = fold, epoch = epoch)\n        \n        es(valid_loss, trainer.model, model_path = cfg.checkpoint(fold))\n        \n        if es.early_stop:\n            break","37f54c45":"for fold in range(cfg.folds):\n    train_fold(fold, cfg.epochs)","4d34adfe":"<h1 style=\"font-size:30px;color:#1371B0\"><strong>About <\/strong><strong style=\"color:black\">this notebook:<\/strong><\/h1>\n\nThis notebook continues the trend of training using `nn.MarginRanking` for toxicity prediction  \nThis notebook experiments with the following:  \n - MLM pretraining on training\/test data\n - Synonym replacement data augmentation with `nlpaug`\n - Adding mean-max instead of pooler head\n - Adding a simpler Trainer API with earlystopping\n  \nThanks to debarshichanda's amazing Jigsaw starter for helping define the problem\/utilities:  \nhttps:\/\/www.kaggle.com\/debarshichanda\/pytorch-w-b-jigsaw-starter\n\n ---","e1c75b0b":"## Reproducibility","1afd382e":"---","baa9f2b3":"## Training","5ec8ee37":"## Training Config","fbec248f":"## Dataset","d4473087":"## Trainer","2efda2b6":" - - -","b7e632c5":"## Training Utils","b16978c6":"# Masked Language Modelling\n![image.png](attachment:image.png)\n\nHere we pretrain our transformer which should help align the model more to competition's data-dist prior to fine-tuning.  \n\nThis code was based on maunish's excellent CommonLit MLM notebook:  \nhttps:\/\/www.kaggle.com\/maunish\/clrp-pytorch-roberta-pretrain","c974b044":"## Model Architecture","ee28e63b":"## Loss Function\n","985c7c11":"---","e7f3907b":"---","214a405f":"# Language Data Augmentation\n\n![image.png](attachment:image.png)\n\n`nlpaug` helps with generating synthetic language data (hopefully in ways which are context\/label preserving)  \n\nThe library also makes it simple to use context-aware augmentations such as MLM for sentence augmentation  \nWe'll go with trying simple augmentation using synonym replacement in this case   \n\nA list of supported augmentation strategies can be found in the documentation:  \nhttps:\/\/github.com\/makcedward\/nlpaug\/blob\/master\/example\/textual_augmenter.ipynb  ","749e7760":"## Folds"}}