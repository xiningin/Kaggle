{"cell_type":{"52235883":"code","d4066d91":"code","1d7535cd":"code","6d8a063d":"code","fb45604c":"code","746812e2":"code","0022406d":"code","b584151c":"code","27d503fe":"code","027714ab":"code","7f7ef184":"code","7e066ec7":"code","5f95b663":"code","9dc8eeda":"code","e4f8613e":"code","a7cc5740":"code","eb42451b":"code","64aa677f":"code","79e14b8e":"code","f9fbe683":"code","c1c71be5":"code","f899fb3a":"code","cd8be846":"code","80c7545e":"code","c556f678":"code","822699b0":"code","29e90887":"code","f38bf034":"code","698ddd86":"code","75101525":"code","d30ac9c5":"code","374a1328":"code","616460d3":"code","44f7bc24":"code","3ef07840":"code","fb9fc3e3":"code","765391e9":"code","00d52d45":"code","dee9e645":"code","8e2a80cf":"code","145b3220":"code","4235c458":"code","1e122ba1":"code","e969c01e":"code","0609632e":"code","04524d0f":"code","728657b3":"code","d207d655":"code","ced02dd4":"code","0693ec2d":"code","7cae1cef":"code","d625db5e":"code","c27781c2":"code","5ea23500":"code","8735f241":"code","dfa24159":"code","a24d4ba4":"code","e63497ec":"code","c3b121df":"code","bc8af9ca":"code","fdce2568":"code","420dfe85":"code","3c166f86":"code","660266ff":"code","f9e3a11d":"code","c9d1dba2":"code","dc5673de":"code","06652ec3":"code","b1876558":"code","bfed2794":"code","ae73bd46":"code","1bc31a05":"markdown","36bcd2aa":"markdown","635cea63":"markdown","18796183":"markdown","6653c9fc":"markdown","ea24b645":"markdown","c9ddfa01":"markdown","f7713a40":"markdown","03f73c00":"markdown","9c413b36":"markdown","8b2e3b65":"markdown","53fbbbcb":"markdown","ca9b494a":"markdown","c57eb3ed":"markdown","dce1c308":"markdown","2b9f7358":"markdown","3b37cf11":"markdown","aef3d05e":"markdown","882455dd":"markdown","63684c8b":"markdown","5295962c":"markdown","3d852ec0":"markdown","0813951c":"markdown","432d3c91":"markdown","45e895e8":"markdown","5380e5f6":"markdown","3eaf64d6":"markdown","94240e09":"markdown","9500999c":"markdown","2b44a0ee":"markdown"},"source":{"52235883":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d4066d91":"df = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf.head()","1d7535cd":"df.describe()","6d8a063d":"df.info()","fb45604c":"df['Sex'].value_counts()","746812e2":"df['Embarked'].value_counts()","0022406d":"dummies = pd.get_dummies(df[['Sex', 'Embarked']], drop_first=True)\ndf = pd.concat([df, dummies], axis=1)","b584151c":"df['Name'].head(10)","27d503fe":"df.drop('Name', axis=1, inplace=True)","027714ab":"df['Ticket'].value_counts()","7f7ef184":"df['Ticket'].apply(lambda x: x.split()[0] if len(x.split())>1 else np.nan).value_counts()","7e066ec7":"df['Ticket'].apply(lambda x: 0 if len(x.split())>1 else 1).value_counts()","5f95b663":"df['Ticket_numeric_only'] = df['Ticket'].apply(lambda x: 0 if len(x.split())>1 else 1)\ndf.drop('Ticket', axis=1, inplace=True)","9dc8eeda":"df['Cabin'].value_counts()","e4f8613e":"df['Cabin'][df['Cabin'].notnull()].apply(lambda x:x[0]).value_counts()","a7cc5740":"df['cabin_known'] = df['Cabin'].notnull().astype(int)\ndf.drop('Cabin', axis=1, inplace=True)","eb42451b":"df.info()","64aa677f":"df.corr()['Age'].sort_values()","79e14b8e":"df['Age'] = df['Age'].fillna(df.groupby(['Pclass', 'SibSp'])['Age'].transform('mean'))","f9fbe683":"df.dropna(inplace=True) # A few empty grops of the groupby may be dropped\ndf.info()","c1c71be5":"import matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","f899fb3a":"sns.countplot(x='Survived', data=df)","cd8be846":"plt.figure(figsize=(12,6))\nsns.heatmap(df.corr(), cmap='RdYlGn', annot=True)","80c7545e":"df.drop('PassengerId', axis=1, inplace=True)","c556f678":"df.corr()['Survived'].sort_values()[:-1].plot.bar()","822699b0":"sns.countplot(x='Sex', hue='Survived', data=df)","29e90887":"sns.countplot(x='Pclass', hue='Survived', data=df)","f38bf034":"sns.countplot(x='Embarked', data=df, hue='Survived')","698ddd86":"sns.histplot(x='Fare', data=df, hue='Survived')","75101525":"len(df[df['Fare']>200].index)","d30ac9c5":"sns.histplot(x='Fare', data=df[df['Fare']<200], hue='Survived')","374a1328":"sns.countplot(x='cabin_known', data=df, hue='Survived')","616460d3":"sns.countplot(x='Ticket_numeric_only', data=df, hue='Survived')","44f7bc24":"df.groupby('Ticket_numeric_only')['Survived'].value_counts(normalize=True)","3ef07840":"df.drop(['Ticket_numeric_only', 'Sex', 'Embarked'], axis=1, inplace=True)","fb9fc3e3":"df.head()","765391e9":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(df.drop('Survived', axis=1), df['Survived'], test_size=0.20)","00d52d45":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","dee9e645":"from sklearn.svm import LinearSVC\nlsvm = LinearSVC(max_iter=5000)\nlsvm.fit(X_train, y_train)\npred_lsvm = lsvm.predict(X_test)","8e2a80cf":"from sklearn.metrics import confusion_matrix, classification_report","145b3220":"print(confusion_matrix(y_test, pred_lsvm))\nprint(classification_report(y_test, pred_lsvm))","4235c458":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\npred_knn = knn.predict(X_test)","1e122ba1":"print(confusion_matrix(y_test, pred_knn))\nprint(classification_report(y_test, pred_knn))","e969c01e":"from sklearn.model_selection import GridSearchCV\nparams= {'n_neighbors':[5,10,15,20,25,50]}\nknn = KNeighborsClassifier()\ngrid_knn = GridSearchCV(knn,params)\ngrid_knn.fit(X_train,y_train)\npred_knn = grid_knn.predict(X_test)\ngrid_knn.best_estimator_","0609632e":"print(confusion_matrix(y_test, pred_knn))\nprint(classification_report(y_test, pred_knn))","04524d0f":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\nrfc.fit(X_train, y_train)\npred_rfc = rfc.predict(X_test)","728657b3":"print(confusion_matrix(y_test, pred_rfc))\nprint(classification_report(y_test, pred_rfc))","d207d655":"rfc = RandomForestClassifier()\nparams= {'n_estimators':[50,100,200,250,300]}\n\ngrid_rfc = GridSearchCV(rfc,params)\ngrid_rfc.fit(X_train,y_train)\npred_rfc = grid_rfc.predict(X_test)\ngrid_rfc.best_estimator_","ced02dd4":"print(confusion_matrix(y_test, pred_rfc))\nprint(classification_report(y_test, pred_rfc))","0693ec2d":"from sklearn.svm import SVC\nsvm = SVC()\nparams= {'C':[0.001, 0.01, 0.1,1,10,100,1000],\n        'gamma':[0.001, 0.01, 0.1,1,10,100,1000]}\n\ngrid_svm = GridSearchCV(svm,params)\ngrid_svm.fit(X_train,y_train)\npred_svm = grid_svm.predict(X_test)\ngrid_svm.best_estimator_","7cae1cef":"print(confusion_matrix(y_test, pred_svm))\nprint(classification_report(y_test, pred_svm))","d625db5e":"from sklearn.ensemble import VotingClassifier\ncl1 = grid_knn.best_estimator_\ncl2 = grid_rfc.best_estimator_\ncl3 = grid_svm.best_estimator_\nvc = VotingClassifier(estimators=[('knn',cl1),('rfc',cl2),('svm',cl3)], voting='hard')\nvc.fit(X_train, y_train)\npred_vote = vc.predict(X_test)","c27781c2":"print(confusion_matrix(y_test, pred_vote))\nprint(classification_report(y_test, pred_vote))","5ea23500":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_test.head()","8735f241":"df_test.info()","dfa24159":"passenger_id = df_test['PassengerId']\ndummies = pd.get_dummies(df_test[['Sex', 'Embarked']],drop_first=True)\ndf_test = pd.concat([df_test,dummies],axis=1).drop(['PassengerId','Name','Sex','Ticket','Embarked'], axis=1)","a24d4ba4":"df_test['cabin_known'] = df_test['Cabin'].notnull().astype(int)\ndf_test.drop('Cabin', axis=1, inplace=True)","e63497ec":"df_test['Age'] = df_test['Age'].fillna(pd.concat([df.drop('Survived',axis=1),df_test],ignore_index=True).groupby(['Pclass', 'SibSp'])['Age'].transform('mean'))","c3b121df":"df_test.info()","bc8af9ca":"df_test['Fare'].fillna(df_test['Fare'].mean(), inplace=True)","fdce2568":"df_test.info()","420dfe85":"test_pred_vc = pd.Series(vc.predict(df_test),name='Survived')\nresult = pd.concat([passenger_id,test_pred_vc],axis=1)\nresult.to_csv('titanic_predictions.csv',index=False)","3c166f86":"from tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import Input","660266ff":"X_train.shape","f9e3a11d":"model = Sequential()\nmodel.add(Input(shape=(9,)))\nmodel.add(Dense(9, activation='relu'))\nmodel.add(Dropout(.33))\nmodel.add(Dense(9, activation='relu'))\nmodel.add(Dropout(.33))\nmodel.add(Dense(3, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])","c9d1dba2":"cb = EarlyStopping(monitor='val_loss', mode='min', patience=25, verbose=1)","dc5673de":"model.fit(x=X_train, y=y_train, batch_size=32, epochs=500, callbacks=[cb], validation_data=(X_test,y_test), verbose=2)","06652ec3":"pd.DataFrame(model.history.history).plot()","b1876558":"pred =(model.predict(df_test.values) > 0.5).astype(\"int32\")","bfed2794":"test_pred_ann = pd.Series(pred.T[0], name='Survived')\nresult_ann = pd.concat([passenger_id, test_pred_ann], axis=1)\nresult_ann.to_csv('titanic_predictions_ann.csv',index=False)","ae73bd46":"result_ann['Survived'].value_counts()","1bc31a05":"We can see that there is no perfect correlation between any of the two columns. So none of the data is duplicate.\nWe can also see that passengerID has no important information so we may drop it. This was obvious from the beginning but we kept it to check its correlation incase. Now we can drop it","36bcd2aa":"# Neural Network","635cea63":"All results are close to each other so we may use a voting classifier","18796183":"We may see that ratios are almost same so this column we created doesn't make any difference. It was only a test that maybe there is a pattern on ticket codes about letters in the beginning of them but it looks like there isn't so we may drop it.","6653c9fc":"## TODO\n* There are 5 columns which are string -> feature engineer them\n* There are null values on Age, Cabin and Embarked -> Get rid of them","ea24b645":"# Data Cleaning","c9ddfa01":"We cleared the data except the missing age values. To fill the age values we may easily fill them with mean of the age but first let's see the correlation of it with other values to make a more educated guess.","f7713a40":"# Model","03f73c00":"# EDA and Data Visualisation","9c413b36":"# Prediction","8b2e3b65":"There are 681 unique values of ticket. Some of the values has letters in the beginning, some of them are only numeric. We may check for the letters in the beginning.","53fbbbcb":"This shows that as the Fare increases survival rate also increases","ca9b494a":"On the names column we may get two information. These are the sex from their title (Mr. , Mrs ...) but it already exists as a column.\nWe could also get some relations from the surnames but we also have siblings and spouses count column.\nSo we may drop this column.","c57eb3ed":"Lastly Pclass is actually a categorical data so we may turn it to dummy but we don't have to because 1 is the highest class 2 is the second and 3 is the last so it is ordered at it would work as it is.","dce1c308":"This clearly shows that the column we created from the cabin column has effect on survival rate","2b9f7358":"For filling the missing age values, training data is also used. It isn't filled only by test data.","3b37cf11":"This looks close. we should check the ratio","aef3d05e":"We got rid of all null values and all string columns except Sex and Embarked which are turned to dummies and they will be dropped after some visualisation.","882455dd":"These 2 are easy to deal with. Dummy variables will be created.","63684c8b":"There isn't enough data about each category because there are too much null. We may create a column which shows if the Cabin is known or null.","5295962c":"This plot shows that the city passanger embarked also changes the survival rate","3d852ec0":"Test data is already split but we will split the data for validation to check our model and tune the parameters if needed or choose from multiple models according to performace.","0813951c":"Our test data doesn't have any more null values and it is in the same format with the training data so we can predict it with the created model.","432d3c91":"This is the corelation of columns with survived. So let's check the highest corelated columns.","45e895e8":"We can see that there are outliers in the fare value but there are only 20 values higher than 200 so it's about 3% of the dataset","5380e5f6":"It has too many unique values and too many null. We may check the letters in the beginning of Cabin value","3eaf64d6":"Surviving chance is highly correlated with sex","94240e09":"There are too many categories to turn this into dummy variables. We may make a boolean variable for the ticketname whether it is numeric only or it has letters. Later on we can check if it has any corelation with survived. If it doesn't we may drop it.","9500999c":"It has the biggest absolute correlation with Pclass and SibSp so we may groupby them and fill with the group's mean","2b44a0ee":"This also shows us that as the Pclass decreases surviving chance increases"}}