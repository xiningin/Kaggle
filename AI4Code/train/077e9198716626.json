{"cell_type":{"73155fb2":"code","2bcda338":"code","64a0bb37":"code","8e2fa185":"code","cd0f23bd":"code","37ca2e6c":"code","186cfad3":"code","5aceec25":"code","2948a9cb":"code","c917183b":"code","b52d08cb":"code","809dcf9e":"code","33bb8d6f":"code","09eb9f7b":"code","7d6b7bef":"code","0874452b":"code","cbec0e67":"code","fcc2cc4e":"code","dee60ac9":"code","722857e6":"code","c48dba3f":"code","8e69e41b":"code","6d11e502":"code","89de3b3a":"code","7bc7b357":"code","731b06c1":"code","424c5e14":"code","1c403b44":"code","c5134f9d":"code","b6d14c35":"code","5973cff4":"code","f158afdb":"code","2738ab01":"code","1b4117f0":"code","74774a28":"code","8f066cd7":"code","9f2d2c82":"code","cf988b2f":"code","227c46ac":"code","6f0d09cc":"code","4e5a1369":"code","e0e4ff4f":"code","f3f45a23":"code","e1100969":"code","0079a789":"code","ef8c6594":"code","88479e91":"code","a4206106":"code","92f155de":"code","56f054dc":"code","8eb89f2a":"code","d41e25c3":"code","7b2c2419":"code","ebc60ceb":"code","9ff2034a":"code","4c7e7ae1":"code","19b39451":"code","77772f4e":"code","83bf4135":"code","af661e3e":"code","f33baad2":"code","b6a5c03a":"code","f4770ec1":"code","29efaa4d":"code","6cad655a":"code","f19c6105":"code","1f5ab669":"code","7e7f9762":"code","61a03e64":"code","5190aeea":"code","87e73ec5":"code","f2cf47ff":"code","c2c92cb5":"code","70f096ba":"code","c19ec663":"markdown","43fad016":"markdown","d9b7784c":"markdown","78239dbc":"markdown","246a40d5":"markdown","e8ccd694":"markdown","9eee8756":"markdown","3d0181b1":"markdown","babac55b":"markdown","d2267d9e":"markdown","985a7859":"markdown","d7a2a8fb":"markdown","8e77809e":"markdown","daa37bc8":"markdown","cc58695d":"markdown","032280dd":"markdown","571932c7":"markdown","a36c5444":"markdown","d55be43a":"markdown","d6d74e7e":"markdown","bfc5f1b4":"markdown","05a169ca":"markdown","5bc5f682":"markdown","a57f5873":"markdown","cc59077f":"markdown","fe08aeec":"markdown","5ac3d5c3":"markdown","48c3bfb7":"markdown"},"source":{"73155fb2":"import pandas as pd\nimport numpy as np \nimport seaborn as sns \nimport matplotlib.pyplot as plt\n\n\npd.options.display.max_rows = 10000\npd.options.display.max_seq_items = 20000\npd.options.display.max_colwidth = 500","2bcda338":"MODE = 'train'\nTARGET_COL = 'diabetes_mellitus'","64a0bb37":"train_df = pd.read_csv ( \"..\/input\/widsdatathon2021\/TrainingWiDS2021.csv\")\ntrain_df['train'] = 1 \ntest_df = pd.read_csv(\"..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv\")\ntest_df['train'] = 0 \ndata_dict = pd.read_csv ( \"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\n# Combine all the data . It helps in preprocessing of data \ndata_df = train_df.append ( test_df )","8e2fa185":"# Check the data for missing values, outliers \nmetric = data_df.describe().transpose()\nmetric","cd0f23bd":"data_df.drop ( columns = ['readmission_status'], inplace=True)","37ca2e6c":"# identify all columns that have less than 40% data \nbad_cols = metric[metric['count']<data_df.shape[0]*0.4].index\nbad_cols","186cfad3":"#\n# a simple utiltity method to check if the column is of type\n# categorical or not \n#\ndef identify_columns_type ( df ):\n    binary_cols = []\n    cat_cols = [] \n    other_cols = []\n    column_list = df.columns \n    for col in column_list:\n        if ( len ( train_df[col].value_counts().index  )  <= 25 ) :\n            cat_cols = cat_cols + [ col ]\n        else :\n            other_cols = other_cols + [col ]\n\n    return cat_cols, other_cols ","5aceec25":"\n#\n# The Chi-Square Test of Independence determines whether there is an association\n# between categorical variables (i.e., whether the variables are independent or related).\n# i.e. is there a greater percentage of postive 'target' for a particular value of category \n#\ndef display_chi2_metric (df):\n    cols = df.columns \n    final_df = pd.DataFrame () \n\n    for col in cols:\n        if col == TARGET_COL:\n            continue\n        data_0 =  pd.DataFrame ( df[df[TARGET_COL] == 0 ] [col].value_counts() ) \n        data_1 =  pd.DataFrame ( df[df[TARGET_COL] == 1 ] [col].value_counts() ) \n        data = pd.concat ( [data_0, data_1] , axis =1  )\n        data.columns = ['non-diabetic', 'diabetic']\n        df_temp = pd.DataFrame ( { 'names':   [list ( data.index ) ] , \n                             'measure':  [list( data['diabetic']\/ (data['diabetic'] +  data['non-diabetic'] ).values )] }, \n                           index = [col])\n        df_temp['diff'] = np.max(data['diabetic']\/ (data['diabetic'] +  data['non-diabetic'] )) - \\\n            np.min( data['diabetic']\/ (data['diabetic'] +  data['non-diabetic'] ))\n        final_df = final_df.append (df_temp)\n\n\n    return final_df ","2948a9cb":"#\n# Step 1: Identify the columns\n#\nid_cols =  list ( data_dict[data_dict['Category']=='identifier']['Variable Name'].values )\nid_cols","c917183b":"print ( \"Count of unique encounter_id :\" , len(data_df['encounter_id'].unique ()) )\nprint ( \"Count of unique hospital_id :\" , len(data_df['hospital_id'].unique ()) )\nprint ( \"Count of unique Unnamed: 0 :\" , len(data_df['Unnamed: 0'].unique ()) )","b52d08cb":"data_df = data_df.drop(columns = [ 'Unnamed: 0', 'hospital_id' ])","809dcf9e":"#\n# Step 1: Identify the columns\n#\ndemographic_cols =  list ( data_dict[data_dict['Category']=='demographic']['Variable Name'].values )\ndemographic_cols.remove( 'icu_admit_type' )\ndemographic_cols.remove( 'readmission_status' )\ndata_df [ demographic_cols ] ","33bb8d6f":"# Some rows have age marked as 0.0. May be incorrect. Mark it as nan\nprint ( \" count of data with incorrect age:\" , data_df[data_df['age'] == 0.0 ]['age'].count() ) \ndata_df.loc[data_df['age'] < 1.0 , 'age'] = np.nan","09eb9f7b":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\ndemographic_cat_col, demographic_other_col = identify_columns_type(data_df[demographic_cols])\n\n# from our desribe output , icu_id seems more like a label data rather than continuous value\ndemographic_other_col.remove ( 'icu_id')\ndemographic_cat_col.append('icu_id' )\n\ndata_df [demographic_other_col + [TARGET_COL] ].corr()","7d6b7bef":"# Convert the bmi into bins \ndata_df['bmi_cat'] = pd.cut(data_df['bmi'], [0, 18.5, 24.9 , 29.9, 50 ], labels=False,  right=False)","0874452b":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\n\ndemographic_cat_col.append('bmi_cat')\nmetric = display_chi2_metric(data_df[demographic_cat_col + [TARGET_COL]])\nmetric \n","cbec0e67":"# check if the bmi has a significant impact on diabetes\n\nbmi_dict= { 0.0:\"Underweight\", 1.0:\"Normal\", 2.0:\"Overweight\", 3.0:\"obese\"}\ndata = data_df[[\"bmi_cat\", \"diabetes_mellitus\"]].dropna()\ndata['bmi_cat'] = data['bmi_cat'].apply ( lambda x : bmi_dict[x])\nax=sns.countplot (data['bmi_cat'], hue=data[\"diabetes_mellitus\"] )\n","fcc2cc4e":"import plotly.express as px\n\n# check if the unit id has a significant impact on diabetes\npx.histogram(data_df[['icu_id',\"diabetes_mellitus\"]].dropna(), x=\"icu_id\", color=\"diabetes_mellitus\") \n","dee60ac9":"#\n# Stpe 4: One hot encoding of the selected categorical columns  and drop other columns \n#\n\nfrom sklearn import preprocessing \n\n# Need to check if there is a better way to handle this ? \n# For now retaining the NANs and replacing the strings of the categorical columns with numerical label \n\nfor col in [ 'ethnicity', 'gender', 'icu_stay_type', 'icu_admit_source', 'icu_type', 'hospital_admit_source' ]:\n    data_df[col] = data_df[col].fillna('XXX')\n    le = preprocessing.LabelEncoder()\n    data_df[col+\"temp\"] = le.fit_transform ( data_df[col]) \n    data_df[col]=data_df[[col, col+\"temp\"]].apply( lambda x: x[col] if x[col] == 'XXX' else x[col+\"temp\"], axis=1)\n    data_df.loc[ data_df[col] == 'XXX', col] = np.nan\n    data_df = data_df.drop(columns=col+\"temp\")\n\ncat_feats = demographic_cat_col\ncont_feats = demographic_other_col\n\n\n","722857e6":"#\n# Step 1: Identify the columns\n#\napache_cov_cols =  list ( data_dict[data_dict['Category']=='APACHE covariate']['Variable Name'].values )\napache_cov_cols","c48dba3f":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\napache_cov_cat_col, apache_cov_other_col = identify_columns_type(train_df[apache_cov_cols])\n\n# from the 'describe output, ' - apache_2_diagnosis and apache_3j_diagnosis seems like a label data rather than coninuous variable\napache_cov_other_col.remove ( 'apache_2_diagnosis')\napache_cov_cat_col.append ( 'apache_2_diagnosis')\napache_cov_other_col.remove ( 'apache_3j_diagnosis')\napache_cov_cat_col.append ('apache_3j_diagnosis')\n\n#apache_cov_other_col.append('diabetes_mellitus')\ndata_df [apache_cov_other_col + [TARGET_COL] ].corr()","8e69e41b":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\nmetric = display_chi2_metric(data_df[apache_cov_cat_col+[TARGET_COL]])\nmetric ","6d11e502":"px.histogram(data_df[[\"apache_2_diagnosis\",\"diabetes_mellitus\" ]].dropna(), x=\"apache_2_diagnosis\", color=\"diabetes_mellitus\") \n\n","89de3b3a":"px.histogram(data_df[[\"apache_3j_diagnosis\", \"diabetes_mellitus\"]].dropna(), x=\"apache_3j_diagnosis\", color=\"diabetes_mellitus\") ","7bc7b357":"# Plot the \"acute renal failure\" and the \"diabetes\"\nax = sns.countplot ( train_df ['arf_apache'], hue = train_df ['diabetes_mellitus'])\n\nprint ( \" Patients with renal failure and with  diabetes:\", \n       train_df [ ( train_df ['arf_apache'] == 1)  &  (train_df ['diabetes_mellitus'] == 1 ) == True ].shape[0] ) \nprint ( \" Patients with renal failure and without  diabetes:\", \ntrain_df [ ( train_df ['arf_apache'] == 1)  &  (train_df ['diabetes_mellitus'] == 0 ) == True ].shape[0] ) ","731b06c1":"#Keep a track of categorical and continuos variables \ncat_feats = cat_feats +  apache_cov_cat_col\ncont_feats = cont_feats + apache_cov_other_col \n\n","424c5e14":"#\n# Step 1: Identify the columns\n#\nvitals_cols =  list ( data_dict[data_dict['Category']=='vitals']['Variable Name'].values )\nvitals_cat_cols, vitals_other_cols = identify_columns_type (train_df[vitals_cols] )\n","1c403b44":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\ncolumns = list( set(vitals_other_cols) - set(bad_cols ) )\ndata_df[columns + [TARGET_COL]].corr()\n","c5134f9d":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\ndisplay_chi2_metric ( data_df[vitals_cat_cols+ [TARGET_COL] ])","b6d14c35":"#cont_feats = cont_feats + vitals_other_cols","5973cff4":"#\n# Step 1: Identify the columns\n#\nlab_cols =  list ( data_dict[data_dict['Category']=='labs']['Variable Name'].values )\nlab_cat_cols, lab_other_cols = identify_columns_type (data_df[lab_cols] )","f158afdb":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\ncolumns = list( set( lab_other_cols )  - set ( bad_cols  ) )\ndata_df[columns + [TARGET_COL]].corr()","2738ab01":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\ndisplay_chi2_metric ( data_df[lab_cat_cols+ [TARGET_COL]])","1b4117f0":"#\n# Plot the data based on the aboce data \n#\nplt.figure ( figsize = ( 20, 15))\nplt.subplot ( 2, 2, 1 )\nax = sns.scatterplot ( train_df [ \"h1_glucose_max\"], train_df[\"d1_glucose_max\"] ,  hue = train_df [ \"diabetes_mellitus\"] )\n\nplt.subplot ( 2, 2, 2 )\nax = sns.scatterplot ( train_df [ \"d1_glucose_max\"], train_df[\"d1_glucose_min\"] ,  hue = train_df [ \"diabetes_mellitus\"] )\n\ntemp_df = train_df [train_df['arf_apache'] == 1 ]\nplt.subplot ( 2, 2, 3 )\nax = sns.scatterplot ( temp_df[\"d1_glucose_max\"] ,temp_df [ \"d1_glucose_min\"],  hue = temp_df [ \"diabetes_mellitus\"] )\n\nplt.subplot ( 2, 2, 4 )\nax = sns.scatterplot ( train_df[\"d1_glucose_max\"] ,train_df [ \"glucose_apache\"],  hue = train_df [ \"diabetes_mellitus\"] )\n\n","74774a28":"import plotly.express as px\nfig = px.scatter_3d(train_df, x='d1_glucose_max', y='glucose_apache', z='h1_glucose_max',\n              color='diabetes_mellitus')\nfig.show()","8f066cd7":"plt.figure ( figsize = ( 20, 15))\n\nplt.subplot ( 2, 2, 1 )\nax = sns.scatterplot ( train_df[\"d1_glucose_min\"] ,train_df [ \"h1_glucose_min\"],  hue = train_df [ \"diabetes_mellitus\"] )\n\nplt.subplot ( 2, 2, 2 )\nax = sns.scatterplot ( train_df[\"d1_glucose_min\"] ,train_df ['glucose_apache'],  hue = train_df [ \"diabetes_mellitus\"] )\n\n","9f2d2c82":"cont_feats = cont_feats + lab_other_cols","cf988b2f":"#\n# Step 1: Identify the columns\n#\nlab_blood_gas_cols =  list ( data_dict[data_dict['Category']=='labs blood gas']['Variable Name'].values )\ncat_cols, other_cols = identify_columns_type (data_df[lab_blood_gas_cols] )","227c46ac":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\n\ndata_df[other_cols+ [TARGET_COL]].corr()","6f0d09cc":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\ndisplay_chi2_metric ( data_df[cat_cols+ [TARGET_COL]])","4e5a1369":"\n#cont_feats = cont_feats + other_cols","e0e4ff4f":"#\n# Step 1: Identify the columns\n#\ncomorb_cols =  list ( data_dict[data_dict['Category']=='APACHE comorbidity']['Variable Name'].values )\ncat_cols, other_cols = identify_columns_type ( train_df[comorb_cols] )","f3f45a23":"#\n# Step 2: Identify the corelation between continuous variables  and correlation between these columns and diabetes.\n#\ntrain_df[other_cols+ [TARGET_COL]].corr()","e1100969":"#\n# Step 3: CHI2 test of independece between categorical columns and 'diabetes'.\n#\ndisplay_chi2_metric ( train_df[cat_cols+ [TARGET_COL]])","0079a789":"cat_feats = cat_feats + cat_cols","ef8c6594":"#\n# Drop the bad columns \n#\ndata_df = data_df.drop (columns = bad_cols, errors='ignore')\ncont_feats = list( set(cont_feats) - set(bad_cols))\n","88479e91":"for col in cat_feats:\n    data_df[col] = data_df[col].astype('object')\ndata_df.dtypes","a4206106":"#\n# Impute the missing data \ndata_df[cont_feats] = data_df[cont_feats].fillna(data_df[cont_feats].median())\ndata_df[cat_feats] = data_df[cat_feats].fillna(data_df[cat_feats].mode().iloc[0])\n\n\n'''from sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.impute import KNNImputer\n\nfrom fancyimpute import KNN\n\nfrom sklearn.ensemble import ExtraTreesRegressor\n\nimpute1 = KNNImputer (n_neighbors=1) \ndf_new=pd.DataFrame(impute1.fit_transform( data_df[cont_feats+cat_feats]) , columns = cat_feats)'''","92f155de":"#\n# Normalize the data continuos data \n#\n\nfrom sklearn.preprocessing import StandardScaler\n\nscalar = StandardScaler()\ndata_df[cont_feats] = scalar.fit_transform( data_df[cont_feats])","56f054dc":"sel_cols = cont_feats+cat_feats+['train','diabetes_mellitus','encounter_id']","8eb89f2a":"data_df= data_df[sel_cols]\nfor col in cat_feats:\n    dummy_var=pd.get_dummies(data_df[col], prefix = col)\n    data_df=pd.concat([data_df,dummy_var],axis=1)\n    \ndata_df = data_df.drop(columns = cat_feats)","d41e25c3":"print ( \"COunt of +ve cases \", data_df[ (data_df['diabetes_mellitus']== 1) & (data_df['train'] == 1) == True ].shape[0] )\nprint ( \"COunt of -ve cases \", data_df[ (data_df['diabetes_mellitus']== 0) & (data_df['train'] == 1) == True ].shape[0] ) ","7b2c2419":"#SMOTE is taking too long to complete. Alternately, we can reduce the negative dm data during training \ntrain_data =  data_df[ (data_df['diabetes_mellitus']== 1) & (data_df['train'] == 1) == True ]\ntrain_data = train_data.append (data_df[(data_df['diabetes_mellitus']== 0) & (data_df['train'] == 1) == True ].sample(50000))\ntrain_data","ebc60ceb":"print ( \"COunt of +ve cases \", train_data[ (train_data['diabetes_mellitus']== 1) & (train_data['train'] == 1) == True ].shape[0] )\nprint ( \"COunt of -ve cases \", train_data[ (train_data['diabetes_mellitus']== 0) & (train_data['train'] == 1) == True ].shape[0] ) ","9ff2034a":"#\n# Split the data \n#\nfrom sklearn.model_selection import train_test_split\n\nX = train_data.drop(columns = ['train','diabetes_mellitus','encounter_id'])\ny=  train_data['diabetes_mellitus']\n","4c7e7ae1":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=.25, random_state=1)\nprint ( \"Postivie dm cases % in train data :\", y_train[y_train == 1].shape[0]\/ y_train.shape[0] ) \nprint ( \"Positive dm cases % in test data :\",y_test[y_test == 1].shape[0]\/ y_test.shape[0] ) ","19b39451":"'''from imblearn.over_sampling import SMOTE \noversample = SMOTE()\nx_train, y_train = oversample.fit_resample(x_train,y_train)'''\n","77772f4e":"import tensorflow.keras as keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense\nfrom tensorflow.keras import optimizers\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Dropout\nfrom tensorflow.keras import regularizers \nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\n\ndef create_model (input_len ) :\n\n    inputs = keras.Input(shape=input_len)\n\n    x = keras.layers.Dense(300, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(inputs)\n    x1 = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x)\n    \n    x = keras.layers.Dense(300, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x1)\n    \n   \n    x = keras.layers.Dense(300, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x)\n    \n    x = keras.layers.Add () ([x, x1])\n    \n    x = keras.layers.Dense(150, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(x)\n    x2 = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x2)\n    \n    x = keras.layers.Dense(100, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x)\n    \n    x = keras.layers.Dense(50, kernel_initializer=keras.initializers.GlorotUniform(seed=None))(x)\n    x = keras.layers.BatchNormalization()(x)\n    x = keras.layers.Dropout(0.5)(x)\n    x = keras.layers.LeakyReLU()(x)\n\n    outputs = keras.layers.Dense(1, activation='sigmoid')(x)\n\n    \n    model = keras.Model(inputs=inputs, outputs=outputs)\n    return model \n\nmodel = create_model (941)\nadam = optimizers.Adam(learning_rate=0.0001)\nmodel.compile(optimizer = adam, loss = 'binary_crossentropy', metrics = ['accuracy'])\n\n#.{epoch:02d}-{loss:.2f}\ncheckpoint = ModelCheckpoint(\"weights.hdf5\", monitor=\"val_loss\", verbose=1, save_best_only=True,\n                         save_weights_only=True, mode=\"min\", period=1)\n\nstop = EarlyStopping(monitor=\"val_loss\", patience=20, mode=\"min\")\n\nreduce_lr = ReduceLROnPlateau(monitor=\"val_loss\", factor=0.5, patience=5, min_lr=.0000001, verbose=1, mode=\"min\")\n\nhistory = model.fit(x_train, y_train,  validation_data = (x_test, y_test) , epochs = 25, verbose = 1, callbacks = [checkpoint, stop, reduce_lr ])\n","83bf4135":"model.load_weights ( \"weights.hdf5\")","af661e3e":"dm = model.predict(x_test[y_test==1])\ndm = dm >=0.5\nprint ( \"Count of originally positive cases predicted correctly:\", len ( dm[dm==True] ) ) \nprint ( \"Count of originally positive cases predicted incorrectly:\", len ( dm[dm==False] ) ) ","f33baad2":"dm = model.predict(x_test[y_test==0])\ndm = dm < 0.5\nprint ( \"Count of originally negative cases predicted correctly:\", len ( dm[dm==True] ) ) \nprint ( \"Count of originally negative cases predicted incorrectly:\", len ( dm[dm==False] ) ) ","b6a5c03a":"from sklearn.decomposition import PCA\n#There are 900+ features. Reduce it so that the reduced features explain about ~90% of the data \npca = PCA(n_components=100) \nX_pca = pca.fit_transform (X) \nprint(pca.explained_variance_ratio_.sum())\nx_pca_train, x_pca_test, y_train, y_test = train_test_split(X_pca, y, test_size=.25, random_state=1)","f4770ec1":"\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV\n\n\n# Create first pipeline for base without reducing features.\npipe = Pipeline([('classifier' , KNeighborsClassifier())])\n\n# Create param grid.\nparam_grid = [\n    \n    {\n         'classifier' : [XGBClassifier(objective= 'binary:logistic', nthread=4, seed=42, scale_pos_weight = 0.8)],\n         'classifier__max_depth' : [8] , \n         'classifier__n_estimators' : [20 ],\n         'classifier__learning_rate' : [ 0.01 ]\n    }\n        \n]\n\n# Create grid search object\nclf = GridSearchCV(pipe, param_grid = param_grid, cv = 3, verbose=True, n_jobs=-1)\n\n# Fit on data\nmodel_xgb = clf.fit(X_pca, y)","29efaa4d":"model_xgb.score ( x_pca_test, y_test)","6cad655a":"dm = model_xgb.predict(x_pca_test[y_test==1] ) \ndm = dm >= 0.5\nprint ( \"Count of originally positive cases predicted correctly:\",len ( dm[dm==True] ) ) \nprint ( \"Count of originally positive cases predicted incorrectly:\", len ( dm[dm==False] ) ) ","f19c6105":"dm = model_xgb.predict(x_pca_test[y_test==0]) \ndm = dm < 0.5\nprint ( \"Count of originally positive cases predicted correctly:\",len ( dm[dm==True] ) ) \nprint ( \"Count of originally positive cases predicted incorrectly:\", len ( dm[dm==False] ) ) ","1f5ab669":"model_xgb.best_params_","7e7f9762":"import lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score, accuracy_score\n\n\ndef train_model_lgb (x_train, y_train, x_test, y_test ) : \n\n    lgb_train = lgb.Dataset(x_train, y_train )\n    lgb_valid = lgb.Dataset(x_test, y_test)\n\n    lgb_model = lgb.train(\n                        {'objective': 'binary', 'learning_rate':'0.01'}, \n                        lgb_train,\n                        valid_sets=[lgb_valid],\n                        verbose_eval=100,\n                        num_boost_round=10000,\n                        early_stopping_rounds=20\n                    )\n    result = lgb_model.predict (x_test )\n    print ( roc_auc_score(y_test, result )) \n    \n    return lgb_model\n\nmodel_lgb = train_model_lgb ( x_train, y_train, x_test, y_test)","61a03e64":"dm = model_lgb.predict(x_test[y_test==1]) \ndm = dm >= 0.5\nprint ( \"Count of originally positive cases predicted correctly:\",len ( dm[dm==True] ) ) \nprint ( \"Count of originally positive cases predicted incorrectly:\", len ( dm[dm==False] ) ) ","5190aeea":"dm = model_lgb.predict( x_test[y_test==0]) \ndm = dm < 0.5\nprint ( \"Count of originally negative cases predicted correctly:\",len ( dm[dm==True] ) ) \nprint ( \"Count of originally negative cases predicted incorrectly:\", len ( dm[dm==False] ) ) \n\n","87e73ec5":"'''from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\nmodel = LogisticRegression(solver='newton-cg',penalty='l2', C=.000575 )\n#model=SVC(kernel='rbf', C=100, gamma=0.1)\nmodel.fit(x_train[cat_feats, y_train)\n\n#predict on test\ny_predict = model.predict(x_test)\n\n#calculate the score\nmodel_score = model.score(x_test, y_test)\ntrain_score = model.score(x_train, y_train)'''","f2cf47ff":"data_df[data_df['train'] == 0 ].shape[0]","c2c92cb5":"test_df = data_df[data_df['train'] == 0 ]\n#test_df ['nn_dm'] =  model.predict (test_df.drop(columns = ['train', 'diabetes_mellitus','encounter_id']) )\ntest_df ['diabetes_mellitus'] =  model_lgb.predict (test_df.drop(columns = ['train', 'diabetes_mellitus','encounter_id']) )\nprint (test_df[test_df['diabetes_mellitus']>=0.5].shape[0])\ntest_df[['encounter_id', 'diabetes_mellitus']]","70f096ba":"test_df[['encounter_id', 'diabetes_mellitus']].to_csv('\/kaggle\/working\/submission.csv', index=False)","c19ec663":"The person who have a renal failure (arf ) has higher number of diabetics than the ones who do not have. This is obvious since diabtetes is know to cause renal failure. ","43fad016":"### For each of the Feature category we follow these steps:\n* Identify the columns \n* Identify the corelation between them  and correlation between these columns and diabetes. \n* CHI2 test of independece between categorical columns and 'diabetes'.\n* Encoding of the selected categorical columns and drop the unnecessary columns \n","d9b7784c":"![image.png](attachment:image.png)","78239dbc":"## Understanding the Problem \n* What is Diabetes mellitus ?\n\nDiabetes mellitus is a disorder in which the body does not produce enough or respond normally to insulin, causing blood sugar (glucose) levels to be abnormally high.\n\n* What is APACHE? \n\nAcute Physiology and Chronic Health Evaluation (APACHE). APACHE (Acute Physiology, Age, Chronic Health Evaluation) methodology helps to more accurately predict hospital mortality risk for critically ill hospitalized adults. These include - major medical and surgical disease categories, acute physiologic abnormalities, age, preexisting functional limitations, major comorbidities, and treatment location immediately prior to ICU admission. (https:\/\/pubmed.ncbi.nlm.nih.gov\/1959406\/ )\n\n* What is the goal ?\n\nAbnormal glucose measurements are common among intensive care unit (ICU) patients for numerous reasons. Drastic variation in this leads to severe complications and mortality in patients.   The American Diabetes Association and the American Association of Clinical Endocrinologists recommend maintaining blood glucose levels close to 110 mg\/dl and generally <140 mg\/dl. \n\nIt is very important to know if the patient has been \"diabetic\" previously in order to maintain these levels. \n\nTypically, a person is identified as a 'diabetic' by conducting tests while fasting and subsequently with a known quantity of glucose to test the tolerance . However, this is not possible for a patient in the ICU. Hence we need a method to  derive this condition from known parameters.","246a40d5":"### labs blood gas - 1st day and 1st hour ","e8ccd694":"### labs 1st day and 1st hour","9eee8756":"## TRAINING","3d0181b1":"### Prepare for training","babac55b":"Patients with renal failure have more chance of being diabetic ","d2267d9e":"The patients with aids have lesser chance of being diabetic compared to others. For the other comorbidities, no relationship can be established with the current data","985a7859":"## WORK IN PROGRESS","d7a2a8fb":"## Fully connected neural network ","8e77809e":"The percentage of diabetic patients among \"Native Americans\" is more than the \"Caucassians\". Similarly the percentage of diabetic among patients with cardiosurgergy (CSICU) is more than the patients with neuro icu .\n\n\nGender or Elective Surgery does not seem to have an impact on diabetes","daa37bc8":"## EDA and Data Cleaning","cc58695d":"### Demographic params\n\n","032280dd":"* This indicates so many missing data. columns like 'albumin_apache' has too many missing values. May have to be dropped \n* Age has 0.0 as min value ( may be an incorrect value ). \n* icu_id is more like a categorical value ( representing the unit within the ICU )\n* pre_icu_los_days  ( The length of stay of the patient between hospital admission and unit admission) has negative values ( may be an incorrect value ). \n* Should 'apache_2_diagnosis', 'apache_3j_diagnosis'  be treated as a categorical columns ( The APACHE score is between 0 and 70 . However the values here are much higher) .\n* 'gcs_eyes_apache' , 'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache' are all categorical columns \n\n* 'readmission_status' has the same value","571932c7":"## Methodology to analyze the data :\n\nAnalyzing all the parameters together is going to be overwhelming. Hence divide the data based on the feature subgroup ( Data Dictionary) will help us to understand the data better. \n\n* demographic\n \n* APACHE covariate\n\n* vitals\n\n* labs\n\n* labs blood gas\n\n* APACHE comorbidity\n\n\nFor each of these feature groups, we can retreive the list of features, understand their correlation with the target ( diabetes mellitus ) and check if any further feature engineering needs to be done. The categorical columns are checked for chi2 value with the target ( i.e is there is an increase percentage of diabetes for a specific value of the category ). This helps us to analyze and (if needed ) eliminate certain categorical values .\n\nNormally, diabetes is associated with kidney related ailments, unregulated glucose levels , heart failure, uncontrolled createnine etc \n\n\n","a36c5444":"### Comorbidity","d55be43a":"## INFERENCE ","d6d74e7e":"### APACHE covariate","bfc5f1b4":"Higher values of glucose implies higher chance of being diabetic ","05a169ca":"bmi\/weight have a small correlation with diabetes ( .17 ). bmi and weight internally have a high correlation. (0.8786 ) ","5bc5f682":"*image courtesy*:https:\/\/cleowilson.com\/free-download-pdf-joslins-diabetes-mellitus-14th-edition\/","a57f5873":"## XGBoost","cc59077f":"### Identifier params ","fe08aeec":"### LGB Model","5ac3d5c3":"### Vitals 1st day and 1st hour\n","48c3bfb7":"Patients with higher glucose values ( hourly and day1 ) are more likely to be diabetic. This is reflected in the corelation coeffi"}}