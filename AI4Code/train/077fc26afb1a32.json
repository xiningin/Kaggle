{"cell_type":{"bc380a0e":"code","16d3edf5":"code","1bfa685c":"code","4b41f7bd":"code","f754e1cb":"code","ca39cf67":"code","8f29b58f":"code","b16bad71":"code","63210afe":"code","81391776":"code","4e93e3e5":"code","aa281eab":"code","ba63546b":"code","22e6cebf":"code","8d096801":"code","4fc143ce":"markdown"},"source":{"bc380a0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","16d3edf5":"!pip install mlfinlab\n\n# Imports\nimport seaborn as sns\nimport mlfinlab as ml\nfrom numba import jit, prange\nfrom mlfinlab.sampling.bootstrapping import get_ind_mat_average_uniqueness, get_ind_matrix, seq_bootstrap\nfrom mlfinlab.sampling.concurrent import get_av_uniqueness_from_triple_barrier\nimport pandas as pd\nimport numpy as np","1bfa685c":"# Get barrier events (you might have to drop duplicate timestamps...)\nbarrier_events = pd.read_csv('..\/input\/triplebarrier\/barrier_events.csv', parse_dates=[0])\nbarrier_events.drop_duplicates(subset=\"t1\", keep=False, inplace=True)\nbarrier_events.set_index('t1', drop=False, inplace=True)","4b41f7bd":"# Get our close prices from csv\nclose_prices = pd.read_csv('..\/input\/mlfinlabdata\/sample.csv', index_col=0, parse_dates=[0,2])\nprint(close_prices)","f754e1cb":"# We can measure average label uniqueness using get_av_uniqueness_from_tripple_barrier function from mlfinlab package\nav_unique = get_av_uniqueness_from_triple_barrier(barrier_events, close_prices.close, num_threads=3)\nav_unique.mean()\nprint(av_unique.mean())","ca39cf67":"# Index of the first unique label\nunique_label_index = av_unique[av_unique.tW==1].index[0] # take the first sample\nprint(unique_label_index)\n\nbarrier_events[barrier_events.index >= unique_label_index].head()  ### Figure out why this does not work","8f29b58f":"### Bagging, Bootstrapping and Random Forrest\n# Ensemble learning technique (bagging with replacement) the goal is to randomly choose data samples\n# that are unique and non-concurrent for each decision tree\n# With sequential bootsrapping our goal is to select samples such that with each iteration we can\n# maximize average unqiueness of subsamples\nind_mat = pd.DataFrame(index=range(0,6), columns=range(0,3))\nind_mat.loc[:, 0] = [1, 1, 1, 0, 0, 0]\nind_mat.loc[:, 1] = [0, 0, 1, 1, 0, 0]\nind_mat.loc[:, 2] = [0, 0, 0, 0, 1, 1]\nind_mat\nprint(ind_mat)","b16bad71":"# Get triple barier method indicator matrix\ntriple_barrier_ind_mat = get_ind_matrix(barrier_events, price_bars=close_prices['close'])\nprint(triple_barrier_ind_mat)\n\nind_mat_uniqueness = get_ind_mat_average_uniqueness(triple_barrier_ind_mat)  ### CHECK BACK AFTER FIXING DUPLICATE T Values\nprint(ind_mat_uniqueness)\n\nfirst_sample = ind_mat_uniqueness\nfirst_sample[first_sample > 0].mean()","63210afe":"# Jupyter notebook output\n# av_unique.loc[0]","81391776":"# Get the values\nind_mat = ind_mat.values","4e93e3e5":"# On the first step all labels will have equal probabilities as average uniquess of matrix with 1 column is 1\nphi = [1]\nuniqueness_array = np.array([None, None, None])\nfor i in range(0,3):\n    ind_mat_reduced = ind_mat[:, phi + [i]]\n    label_uniqueness = get_ind_mat_average_uniqueness(ind_mat_reduced)#[-1] # The last value corresponds to appended i TODO fix this\n    uniqueness_array[i] = (label_uniqueness[label_uniqueness > 0].mean())\nprob_array = uniqueness_array \/ sum(uniqueness_array)\n\nprint(prob_array)","aa281eab":"phi = [1,2]\nunqiueness_array = np.array([None, None, None])\nfor i in range(0,3):\n    ind_mat_reduced = ind_mat[:, phi + [i]]\n    label_uniqueness = get_ind_mat_average_uniqueness(ind_mat_reduced)#[-1] TODO fix this\n    uniqueness_array[i] = (label_uniqueness[label_uniqueness > 0].mean())\nprob_array = uniqueness_array \/ sum(uniqueness_array)\n\nphi = [1, 2, 0]\nuniqueness_array = np.array([None, None, None])\nfor i in range(0, 3):\n    ind_mat_reduced = ind_mat[:, phi + [i]]\n    label_uniqueness = get_ind_mat_average_uniqueness(ind_mat_reduced)#[-1] TODO fix thiss\n    uniqueness_array[i] = (label_uniqueness[label_uniqueness > 0].mean())\nprob_array = uniqueness_array \/ sum(uniqueness_array)\n\nprint(prob_array)","ba63546b":"samples = seq_bootstrap(ind_mat, sample_length=4, warmup_samples=[1], verbose=True)\nprint(samples)","22e6cebf":"### Monte-Carlo experiment (checks to see how sequential bootsrapping will improve average label uniqueness)\n\nstandard_unq_array = np.zeros(10000) * np.nan # Array of random sampling uniqueness\nseq_unq_array = np.zeros(10000) * np.nan # Array of Sequential Bootstapping uniqueness\nfor i in range(0, 10000):\n    bootstrapped_samples = seq_bootstrap(ind_mat, sample_length=3)\n    random_samples = np.random.choice(ind_mat.shape[1], size=3)\n\n    random_unq = get_ind_mat_average_uniqueness(ind_mat[:, random_samples])\n    random_unq_mean = random_unq[random_unq > 0].mean()\n\n    sequential_unq = get_ind_mat_average_uniqueness(ind_mat[:, bootstrapped_samples])\n    sequential_unq_mean = sequential_unq[sequential_unq > 0].mean()\n\n    standard_unq_array[i] = random_unq_mean\n    seq_unq_array[i] = sequential_unq_mean","8d096801":"np.median(standard_unq_array), np.median(seq_unq_array)\nnp.mean(standard_unq_array), np.mean(seq_unq_array)\n\n# KDE plots of label uniqueness support the fact taht sequential bootstrapping gives higher average label uniqueness\nsns.kdeplot(standard_unq_array, shade=True, label='Random Sampling')\nsns.kdeplot(seq_unq_array, shade=True, label='Sequential Sampling')\n\n# Let's apply sequential bootstrapping to our full data set and draw 50 samples.\nbootstrapped_samples = seq_bootstrap(triple_barrier_ind_mat, compare=True, sample_length=50)","4fc143ce":"Code written by Soren and Sean in reference to:\nhttps:\/\/github.com\/hudson-and-thames\/research\/blob\/master\/Advances%20in%20Financial%20Machine%20Learning\/Sample%20Weights\/Sequential_Bootstrapping.ipynb\nand\nhttps:\/\/github.com\/hudson-and-thames\/research\/blob\/master\/Advances%20in%20Financial%20Machine%20Learning\/Labelling\/Trend-Follow-Question.ipynb"}}