{"cell_type":{"6bd77311":"code","6f6cd9c4":"code","22031036":"code","4aff507b":"code","f54bb3e7":"code","cbcc611a":"code","a595620d":"code","787bcbc1":"code","207a953a":"code","3c231ee8":"code","a95a558f":"code","48c603c5":"code","66120828":"code","0e065ce5":"code","39e32498":"code","62360816":"code","d73abfc6":"code","1ddc6a70":"code","60897913":"code","e7b05b2e":"code","7bde9ce7":"code","a191766a":"code","913a5633":"code","e03b09b1":"code","d2643181":"code","7abf7544":"code","826a0ef6":"code","6c138633":"code","a5e3a434":"code","5e31ef80":"code","8b1a1db6":"code","ca159ed5":"code","cfa92a0a":"code","b5d67be1":"code","9a23efd9":"code","fb251726":"code","9127b190":"code","358cad6c":"code","0bf588b8":"code","c8cde0ed":"code","800f330d":"code","637bc247":"code","b305bf0a":"code","c4d72bae":"code","f9104739":"code","339efd9c":"code","aa0aa8b7":"code","8a672dc6":"code","84ea2ba1":"code","7ec25055":"code","52c280f4":"code","bfab7831":"code","308a3a34":"code","a53674c5":"code","98dbe788":"code","4ace4050":"code","d7e90dbc":"code","70fbb656":"markdown","0f262638":"markdown","1f76df09":"markdown","15e48906":"markdown","88baf381":"markdown","d9d636a4":"markdown","d39e969f":"markdown","306f0ac7":"markdown","f78a50ee":"markdown","e98fc0f8":"markdown","725e40c2":"markdown","96481f28":"markdown","782e32af":"markdown","aa4be0d5":"markdown","07906adf":"markdown","f5cbbde0":"markdown","37affad5":"markdown","7ac9df0a":"markdown","7c5dc965":"markdown","2ecdc6f0":"markdown","aee03f85":"markdown","24f5e860":"markdown","b6543097":"markdown","7ffa8903":"markdown","237dac94":"markdown","a8d368b6":"markdown","c53e8e39":"markdown","fc1ccad2":"markdown","edf0c64a":"markdown","3ee48206":"markdown","4e8b7436":"markdown","f10c85c8":"markdown","52a482c8":"markdown","a29a100c":"markdown","663c2336":"markdown","561ad310":"markdown","78dabc62":"markdown","2be475b9":"markdown","113c73b3":"markdown","151157c8":"markdown","24d960d2":"markdown","752d6a2a":"markdown","1caaf09b":"markdown","dd2558a6":"markdown","1969fdbf":"markdown","5048219f":"markdown","4c495325":"markdown","27d64dd0":"markdown","8a2e0eaa":"markdown","76674dbf":"markdown","d224e152":"markdown","c3415f6d":"markdown","c442152a":"markdown","7ce5bc76":"markdown","0e42e60f":"markdown","1bf47186":"markdown","d15d93a2":"markdown","e385857b":"markdown","6010c02b":"markdown","669bc39a":"markdown","e5989a9d":"markdown","796d72e4":"markdown","e58e8dbc":"markdown","56527395":"markdown","6df09811":"markdown","2e0c3890":"markdown","ee39d348":"markdown","bd049db8":"markdown"},"source":{"6bd77311":"# import libraries\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport numpy as np\nimport re\nimport random\nimport nltk\nfrom scipy import sparse\nfrom scipy.sparse import csr_matrix, vstack\nfrom textblob import TextBlob\nfrom langdetect import detect_langs\nimport pickle\nfrom datetime import datetime\n\nfrom sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler","6f6cd9c4":"# import data\nsong_df = pd.read_csv('..\/input\/songlyrics\/songdata.csv')\nsong_df.head()","22031036":"print(song_df['text'].iloc[7000])","4aff507b":"print(song_df['text'].iloc[10000])","f54bb3e7":"text_in_round_brackets = sum(list(song_df['text'].map(lambda s: re.findall(r'\\((.*?)\\)',s))), [])\nprint('Number of round brackets: {}'.format(len(text_in_round_brackets)))","cbcc611a":"random.seed(0)\nrandom.choices(text_in_round_brackets, k=20)","a595620d":"text_in_square_brackets = sum(list(song_df['text'].map(lambda s: re.findall(r'\\[(.*?)\\]',s))), [])\nprint('Number of square brackets: {}'.format(len(text_in_square_brackets)))","787bcbc1":"random.seed(0)\nrandom.choices(text_in_square_brackets, k=20)","207a953a":"text_in_curly_brackets = sum(list(song_df['text'].map(lambda s: re.findall(r'\\{(.*?)\\}',s))), [])\nprint('Number of square brackets: {}'.format(len(text_in_curly_brackets)))","3c231ee8":"# remove round brackets but not text within\nsong_df['text'] = song_df['text'].map(lambda s: re.sub(r'\\(|\\)', '', s))\n\n# remove square brackest and text within\nsong_df['text'] = song_df['text'].map(lambda s: re.sub(r'\\[(.*?)\\] ', '', s))","a95a558f":"# count number of lines\nsong_df['lines'] = song_df['text'].map(lambda t: len(re.findall(r'\\n', t)))\n# remove line breaks\nsong_df['text'] = song_df['text'].map(lambda s: re.sub(r' \\n|\\n', '', s))","48c603c5":"def get_eng_prob(text):\n    detections = detect_langs(text)\n    for detection in detections:\n        if detection.lang == 'en':\n            return detection.prob\n    return 0\n\nsong_df['en_prob'] = song_df['text'].map(get_eng_prob)\n\nprint('Number of english songs: {}'.format(sum(song_df['en_prob'] >= 0.5)))\nprint('Number of non-english songs: {}'.format(sum(song_df['en_prob'] < 0.5)))","66120828":"song_df = song_df.loc[song_df['en_prob'] >= 0.5]","0e065ce5":"tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\nsong_df['tokens'] = song_df['text'].map(tokenizer.tokenize)\n\nprint('Text:')\nprint(song_df['text'].iloc[0])\n\nprint('Tokens:')\nprint(song_df['tokens'].iloc[0])","39e32498":"# initialise stemmer\nstemmer = nltk.stem.porter.PorterStemmer()\n\ntoken = 'make'\nprint('{} -> {}'.format(token, stemmer.stem(token)))\n\ntoken = 'makes'\nprint('{} -> {}'.format(token, stemmer.stem(token)))\n\ntoken = 'making'\nprint('{} -> {}'.format(token, stemmer.stem(token)))\n\ntoken = 'made'\nprint('{} -> {}'.format(token, stemmer.stem(token)))","62360816":"# create dictionary to map tokens their stem\ntoken_to_stem = {}\n# initialise word count\ntoken_count = 0\n# iterate through all songs\nfor lst in song_df['tokens']:\n    # iterate through all tokens of song\n    for token in lst:\n        token_count += 1\n        # check if token is in dictionary\n        if token not in token_to_stem:\n            # add token to dictionary\n            token_to_stem[token] = stemmer.stem(token)\n            \nsong_df['stems'] = song_df['tokens'].map(lambda lst: [token_to_stem[token] for token in lst])\n\nprint('Number of tokens: {}'.format(token_count))\nprint('Number of unique tokens: {}'.format(len(token_to_stem.keys())))\nprint('Number of unique stems: {}'.format(len(set(token_to_stem.values()))))","d73abfc6":"# number of songs\nprint('number of songs: ', str(len(song_df)))\n\n# number of artists\nprint('number of artists: ', str(len(song_df['artist'].unique())))\n\n# distribution songs per artist\nsong_count_df = song_df.groupby('artist')[['song']].count()\nfig = px.histogram(song_count_df, x='song', title='Songs per artist', labels={'song': 'Songs'})\nfig.show()","1ddc6a70":"# words per song\nsong_df['n_stems'] = song_df['stems'].map(len)\n\nfig = px.histogram(song_df, x='n_stems', title='Words per song')\nfig.show()","60897913":"# create dataframe with lists of artists\nsong_df['stems_str'] = song_df['stems'].map(lambda lst: ' '.join(lst))\n\n# map text to artists\nstems_to_artist = {}\nfor tp in song_df[['artist', 'stems_str']].itertuples(index=False):\n    artist = tp[0]\n    stems = tp[1]\n    if stems in stems_to_artist:\n        stems_to_artist[stems].append(artist)\n    else:\n        stems_to_artist[stems] = [artist]\n\n# insert list of artists to dataframe\nsong_df['artists'] = song_df['stems_str'].map(stems_to_artist)\nsong_df['duplicates'] = song_df['artists'].map(len) - 1\n\n# convert list of artists to set of artists\nsong_df['artists'] = song_df['artists'].map(set)\nsong_df['n_artists'] = song_df['artists'].map(len)\n\n# remove duplicate songs\nartist_text_df = song_df.drop_duplicates('stems_str')","e7b05b2e":"# number of unique songs\nprint('Number of unique lyrics: {}'.format(sum(artist_text_df['duplicates'] == 0)))\n# number of duplicate songs\nprint('Number of duplicate lyrics: {}'.format(sum(artist_text_df['duplicates'] > 0) + \\\n                                              sum(artist_text_df['duplicates'])))\n# number of duplicates from same artist\nprint('Number of duplicate lyrics from same artist: {}'.format(sum(artist_text_df['duplicates'] + 1 - \\\n                                                                   artist_text_df['n_artists'])))\n# number of duplicates from different artists\nprint('Number of duplicate lyrics from different artists: {}'.format(sum(artist_text_df['n_artists']\\\n                                                                         .loc[artist_text_df['duplicates'] > 0])))","7bde9ce7":"# randomly select artists\nn_artist = 10\nrandom.seed(0)\n\nartist_select = random.choices(song_df['artist'].unique(), k=n_artist)\n\nsong_filter_df = song_df.loc[song_df['artist'].isin(artist_select)]\nprint('Total number of songs: {}'.format(len(song_filter_df)))\nsong_filter_df.groupby('artist')[['song']].count().reset_index().rename(columns={'song':'songs'})","a191766a":"fig = px.box(song_filter_df, x='artist', y='n_stems', title='Word count per song by artist')\nfig.show()","913a5633":"# number of unique stems\nsong_df['n_unique_stems'] = song_df['stems'].map(lambda lst: len(set(lst)))\n# ratio of unique stems\nsong_df['unique_stems_ratio'] = song_df['n_unique_stems'] \/ song_df['n_stems']\n\n# attach column to selected artists\nsong_filter_df = song_filter_df.join(song_df['unique_stems_ratio'])","e03b09b1":"fig = px.box(song_filter_df, x='artist', y='unique_stems_ratio', title='Ratio of unique words to all words')\nfig.show()","d2643181":"# calculate number of words per line\nsong_df['stems_per_line'] = song_df['n_stems'] \/ song_df['lines'].astype(float)\n\nsong_filter_df = song_filter_df.join(song_df[['stems_per_line']])","7abf7544":"fig = px.box(song_filter_df, x='artist', y='stems_per_line', title='Words per line')\nfig.show()","826a0ef6":"# initialise count vectorizer\ncv = CountVectorizer()\n\n# generate word counts\nstem_count_vector = cv.fit_transform(song_df['stems_str'])\n\n# compute idf\ntfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\ntfidf_transformer.fit(stem_count_vector)","6c138633":"# print idf values\ntfidf_df = pd.DataFrame({'stem': cv.get_feature_names(), 'weight': tfidf_transformer.idf_})\n \n# get lowest weights\ntfidf_df.sort_values('weight').head()","a5e3a434":"# get highest weights\ntfidf_df.sort_values('weight', ascending=False).head()","5e31ef80":"# assign tf idf scores to each song\ntf_idf_vector = tfidf_transformer.transform(stem_count_vector)\n\n# attach count vectors to dataframe\ntf_idf_vector_lst = [-1] * len(song_df)\nfor i in range(len(song_df)):\n    tf_idf_vector_lst[i] = tf_idf_vector[i]\nsong_df['tf_idf_vector'] = tf_idf_vector_lst    \n\nsong_df['tf_idf_score'] = song_df['tf_idf_vector'].map(lambda vec: np.sum(vec.todense()))\n\n# join valus to selected artists\nsong_filter_df = song_filter_df.join(song_df[['tf_idf_vector', 'tf_idf_score']])","8b1a1db6":"fig = px.box(song_filter_df, x='artist', y='tf_idf_score', title='TFIDF scores of songs per artist')\nfig.show()","ca159ed5":"# caclculate mean vector\ndef get_mean_vector(vec_lst):\n    return csr_matrix(vstack(vec_lst).mean(axis=0))","cfa92a0a":"# calculate mean vector over all songs of same artist\nartist_df = song_df.groupby('artist').agg({'tf_idf_vector': get_mean_vector, 'song': len}).reset_index()\\\n                   .rename(columns={'song': 'songs'})\n\n# get selected artists\nartist_filter_df = artist_df.loc[artist_df['artist'].isin(song_filter_df['artist'])]","b5d67be1":"similarity_matrix = cosine_similarity(vstack(artist_filter_df['tf_idf_vector']), \n                                      vstack(artist_filter_df['tf_idf_vector']))\nartist_names = artist_filter_df['artist'].tolist()\nfig = go.Figure(data=go.Heatmap(z=np.flipud(similarity_matrix), x=artist_names, y=list(reversed(artist_names)), \n                                colorscale='balance', zmin=0.5, zmax=1.1))\nfig.show()","9a23efd9":"artist_song_filter_df = pd.merge(artist_filter_df[['artist', 'tf_idf_vector', 'songs']].assign(key = 0), \n                                 song_filter_df[['artist', 'tf_idf_vector', 'song']].assign(key = 0), on='key', \n                                 suffixes=['_artist', '_song']).drop('key', axis=1).reset_index(drop=True)\nartist_song_filter_df['same_artist'] = artist_song_filter_df['artist_artist'] == artist_song_filter_df['artist_song']","fb251726":"# calculate similarity of artist tf idf vector and song vector\ndef tf_idf_vector_similarity(artist_vector, song_vector, songs, same_artist):\n    # check if song is from same artist\n    if same_artist:\n        # deduct song vector from artist vector\n        artist_vector = (songs * artist_vector - song_vector) \/ (songs - 1)\n    # calculate similarity\n    return cosine_similarity(artist_vector, song_vector)[0][0]","9127b190":"artist_song_filter_df['vector_similarity'] = \\\n    artist_song_filter_df.apply(lambda row: tf_idf_vector_similarity(row['tf_idf_vector_artist'], \n                                                                     row['tf_idf_vector_song'], \n                                                                     row['songs'], row['same_artist']), axis=1)","358cad6c":"df = artist_song_filter_df\n\nfig = go.Figure()\n\nfig.add_trace(go.Violin(x=df['artist_artist'][df['same_artist']],\n                        y=df['vector_similarity'][df['same_artist']],\n                        legendgroup='Same Artist', scalegroup='Same Artist', name='Same Artist',\n                        side='negative')\n             )\nfig.add_trace(go.Violin(x=df['artist_artist'][~df['same_artist']],\n                        y=df['vector_similarity'][~df['same_artist']],\n                        legendgroup='Different Artists', scalegroup='Different Artists', name='Different Artists',\n                        side='positive')\n             )\n\nfig.update_traces(meanline_visible=True)\nfig.update_layout(violingap=0, violinmode='overlay')\nfig.update_layout(title='Similarity of Songs')\nfig.update_xaxes(range=[-0.5, 9.5])\nfig.update_yaxes(range=[-0.1, 0.8], title='Similarity')\nfig.show()","0bf588b8":"polarity_lst = [-1] * len(song_df)\nsubjectivity_lst = [-1] * len(song_df)\nfor i, text in enumerate(song_df['text']):\n    sentiment = TextBlob(text)\n    polarity_lst[i] = sentiment.polarity\n    subjectivity_lst[i] = sentiment.subjectivity\n    \nsong_df['polarity'] = polarity_lst\nsong_df['subjectivity'] = subjectivity_lst\n\nsong_filter_df = song_filter_df.join(song_df[['polarity', 'subjectivity']])","c8cde0ed":"fig = px.scatter(song_filter_df, x='polarity', y='subjectivity', color='artist', hover_data=['song'], \n                 title='Polarity and Subjectivity of Songs')\nfig.show()","800f330d":"fig = px.box(song_filter_df, x='artist', y='polarity', title='Polarity by artist')\nfig.show()","637bc247":"# parameter\n# number of sets\nn_set = {'train': 20, 'val': 20}\n# number of artists per set\nn_artist = 3\n# minimum number of songs of one artist\nn_song_min = 5\n# maximum number of song - artist pairs per artist set\nn_song_artist_max = 100","b305bf0a":"def select_artist_song_create_feature(song_df, n_set, n_artist, n_song_min, n_song_artist_max):\n    song_count_df = song_df.groupby('artist')[['artist']].count().rename(columns={'artist': 'count'})\n    artist_lst = list(song_count_df.loc[song_count_df['count'] >= n_song_min].index.values)\n\n    n_set_total = sum(n_set.values())\n\n    artist_set = []\n    while len(artist_set) < n_set_total:\n        new_artist = tuple(np.random.choice(artist_lst, size=n_artist, replace=False))\n        if new_artist not in artist_set:\n            artist_set.append(new_artist)\n\n    # split artist sets\n    artist_select = {}\n    for field, n in n_set.items():\n        i_select = np.random.choice(range(len(artist_set)), size=n, replace=False)\n        artist_list = list(artist_set)\n        artist_select[field] = [artist_list[i] for i in i_select]\n        artist_set = [s for s in artist_set if s not in artist_select[field]]\n\n    # create dataframe with all features\n    feature_dict = {}\n    # dictionary to map artist set id to list of artists\n    set_id_to_artist_tp = {}\n\n    i = 0\n    for field, artist_set in artist_select.items():\n        df_lst = []\n        for artist_tp in artist_set:\n            i += 1\n            df = song_df.loc[song_df['artist'].isin(artist_tp), \n                             ['artist', 'song', 'n_stems', 'unique_stems_ratio', 'stems_per_line', 'tf_idf_vector', \n                              'tf_idf_score', 'polarity']]\n            # check if number of songs is too high\n            if len(df) * n_artist > n_song_artist_max:\n                df = df.sample(int(n_song_artist_max \/ n_artist), random_state=0)\n                \n            df['artist_set_id'] = i\n            set_id_to_artist_tp[i] = artist_tp\n            df_lst.append(df)\n        feature_dict[field] = pd.concat(df_lst)  \n        print('Number of songs in {}: {}'.format(field, len(feature_dict[field])))\n\n    # get all selected artists\n    artist_select_set = set.union(*[set(sum(tp_lst, ())) for tp_lst in artist_select.values()])\n\n    # create artist dataframe from training data\n    df_lst = []\n    for artist, df in song_df.loc[song_df['artist'].isin(artist_select_set)].groupby('artist'):\n        dic = {'artist': artist}\n        # calculate averages and standard diviations\n        for field in ['n_stems', 'unique_stems_ratio', 'stems_per_line', 'tf_idf_score', 'polarity']:\n            dic[field + '_mean'] = df[field].mean()\n            dic[field + '_std'] = df[field].std()\n\n        # number of songs\n        dic['songs'] = len(df)\n\n        # calculate average tf idf vector\n        dic['tf_idf_vector_mean'] = get_mean_vector(df['tf_idf_vector'])\n\n        df_lst.append(pd.DataFrame(dic, index=[0]))\n    artist_feature_df = pd.concat(df_lst)\n\n    def get_features(df):\n        # get artist set id\n        artist_set_id = df['artist_set_id'].iloc[0]\n        \n        # get all artists\n        artist_feature_select_df = artist_feature_df.loc[artist_feature_df['artist']\\\n                                                         .isin(set_id_to_artist_tp[artist_set_id])]\n\n        # merge dataframes\n        artist_song_feature_df = pd.merge(artist_feature_select_df.assign(key=0), df.assign(key=0), on='key', \n                                          suffixes=['_artist', '_song']).drop('key', axis=1)    \n        artist_song_feature_df['same_artist'] = \\\n            artist_song_feature_df['artist_artist'] == artist_song_feature_df['artist_song']\n\n        # calculate features\n        for feature in ['n_stems', 'unique_stems_ratio', 'stems_per_line', 'tf_idf_score', 'polarity']:\n            artist_song_feature_df[feature + '_diff'] = \\\n                artist_song_feature_df[feature] - artist_song_feature_df[feature + '_mean']\n            artist_song_feature_df[feature + '_diff_std'] = \\\n                artist_song_feature_df[feature + '_diff'] \/ artist_song_feature_df[feature + '_std']\n\n        # calculate vector similarity between artist and song\n        artist_song_feature_df['vector_similarity'] = \\\n            artist_song_feature_df.apply(lambda row: tf_idf_vector_similarity(row['tf_idf_vector_mean'], \n                                                      row['tf_idf_vector'], row['songs'], row['same_artist']), \n                                         axis=1)    \n        return artist_song_feature_df\n\n    artist_song_feature = {}\n    for field in feature_dict:\n        artist_song_feature[field] = feature_dict[field].groupby('artist_set_id').apply(get_features)\\\n                                                        .reset_index(drop=True)\n        \n    return artist_song_feature","c4d72bae":"np.random.seed(0)\nartist_song_feature = select_artist_song_create_feature(song_df, n_set, n_artist, n_song_min, n_song_artist_max)","f9104739":"artist_song_feature['train'].iloc[0]","339efd9c":"feature = ['n_stems_diff', 'n_stems_diff_std',\n       'unique_stems_ratio_diff', 'unique_stems_ratio_diff_std',\n       'stems_per_line_diff', 'stems_per_line_diff_std', 'tf_idf_score_diff',\n       'tf_idf_score_diff_std', 'polarity_diff', 'polarity_diff_std',\n       'vector_similarity']\ndf_lst = []\nfor f in feature:\n    df = artist_song_feature['train'][['same_artist']]\n    df['feature'] = f\n    df['value'] = artist_song_feature['train'][f]\n    df_lst.append(df)\nfeature_df = pd.concat(df_lst)\nfeature_df.head()","aa0aa8b7":"def violine_feature_plot(feature_df, feature_select):\n\n    fig = go.Figure()\n    df = feature_df.loc[feature_df['feature'].isin(feature_select)]\n\n    fig.add_trace(go.Violin(x=df['feature'][df['same_artist']],\n                            y=df['value'][df['same_artist']],\n                            legendgroup='Same Artist', scalegroup='Same Artist', name='Same Artist',\n                            side='negative')\n                 )\n    fig.add_trace(go.Violin(x=df['feature'][~df['same_artist']],\n                            y=df['value'][~df['same_artist']],\n                            legendgroup='Different Artists', scalegroup='Different Artists', name='Different Artists',\n                            side='positive')\n                 )\n\n    fig.update_traces(meanline_visible=True)\n    fig.update_layout(violingap=0, violinmode='overlay')\n    fig.update_layout(title='Feature Comparison')\n    fig.update_xaxes(title='Feature')\n    return fig","8a672dc6":"fig = violine_feature_plot(feature_df, ['n_stems_diff_std', 'unique_stems_ratio_diff_std', 'stems_per_line_diff_std', \n                                        'tf_idf_score_diff_std', 'polarity_diff_std'])\nfig.update_xaxes(range=[-0.5, 4.5])\nfig.show()","84ea2ba1":"fig = violine_feature_plot(feature_df, ['vector_similarity'])\nfig.update_xaxes(range=[-1, 1])\nfig.show()","7ec25055":"def prepare_data(df, feature_org, feature_abs):\n    for f in feature_abs:\n        df[f] = df[f].abs()\n    X = df[feature_org + feature_abs].values\n    y = df['same_artist'].values\n    \n    return X, y\n\ndef select_songs_train_pipeline(song_df, n_set, n_artist, n_song_min, n_song_artist_max, feature_org, feature_abs, \n                                pipeline):\n    artist_song_feature = select_artist_song_create_feature(song_df, n_set, n_artist, n_song_min, n_song_artist_max)\n\n    # prepare data\n    X, y = prepare_data(artist_song_feature['train'], feature_org, feature_abs)\n\n    pipeline = pipeline.fit(X, y)\n    \n    return artist_song_feature, pipeline","52c280f4":"# prepare data create and train pipeline\nn_artist = 3\nn_song_min = 5\nn_set = {'train': 100}\nn_song_artist_max = 100\n\nfeature_org = ['n_stems', 'unique_stems_ratio', 'stems_per_line', 'tf_idf_score', 'polarity', 'vector_similarity']\nfeature_abs = ['n_stems_diff', 'n_stems_diff_std', 'unique_stems_ratio_diff', 'unique_stems_ratio_diff_std', \n               'stems_per_line_diff', 'stems_per_line_diff_std', 'tf_idf_score_diff', 'tf_idf_score_diff_std', \n               'polarity_diff', 'polarity_diff_std']\n\npipeline = Pipeline([('scale', StandardScaler()), \n                     ('clf', LogisticRegression(solver='lbfgs', max_iter=3000, \n                                                class_weight={False: 1\/n_artist, True:(n_artist - 1)\/n_artist}))])\n\nnp.random.seed(1)\nartist_song_feature, pipeline = select_songs_train_pipeline(song_df, n_set, n_artist, n_song_min, n_song_artist_max, \n                                                            feature_org, feature_abs, pipeline)","bfab7831":"feature_importance_df = pd.DataFrame({'feature': feature_org+feature_abs, 'coefficient':pipeline['clf'].coef_[0]})\n\npx.bar(feature_importance_df.sort_values('coefficient'), x='feature', y='coefficient')","308a3a34":"def predict_artist(df, feature_org, feature_abs, pipeline, top_n):\n    # prepare data\n    X, y = prepare_data(df, feature_org, feature_abs)\n    \n    # get probability\n    proba = pipeline.predict_proba(X)\n    # attach to dataframe\n    df['probability'] = proba[:, 1]\n    df['correct_prediction'] = df['artist_artist'] == df['artist_song']\n    \n    # get artist song pairs with highest probability\n    predict_select = df.sort_values('probability', ascending=False).groupby(['artist_set_id']).head(top_n)\\\n                       .groupby(['artist_set_id'])['correct_prediction'].max()\n    \n    # get accuracy\n    print('Accuracy: {}'.format(predict_select.mean()))\n    \n    return predict_select","a53674c5":"artist_predict_df = predict_artist(artist_song_feature['train'], feature_org, feature_abs, pipeline, top_n=1)","98dbe788":"artist_predict_df = predict_artist(artist_song_feature['train'], feature_org, feature_abs, pipeline, top_n=2)","4ace4050":"n_artist_lst = [2, 4, 8, 16, 32, 64, 128]\ntop_n_lst = [1, 2, 4, 8, 16, 32, 64]\nn_song_artist_max = 128\nnp.random.seed(2)\n\nn_set = {'train': 100, 'val': 100}\n\nfeature_org = ['n_stems', 'unique_stems_ratio', 'stems_per_line', 'tf_idf_score', 'polarity', 'vector_similarity']\nfeature_abs = ['n_stems_diff', 'n_stems_diff_std', 'unique_stems_ratio_diff', 'unique_stems_ratio_diff_std', \n               'stems_per_line_diff', 'stems_per_line_diff_std', 'tf_idf_score_diff', 'tf_idf_score_diff_std', \n               'polarity_diff', 'polarity_diff_std']\n\npipeline = Pipeline([('scale', StandardScaler()), \n                     ('clf', LogisticRegression(solver='lbfgs', max_iter=3000, \n                                                class_weight={False: 1\/n_artist, True:(n_artist - 1)\/n_artist}))])\n\nresult_lst = []\n\nfor n_artist in n_artist_lst:\n    print(datetime.now())\n    print('n_artist: {}'.format(n_artist))\n    \n    artist_song_feature, pipeline = select_songs_train_pipeline(song_df, n_set, n_artist, n_song_min, \n                                                                n_song_artist_max, feature_org, feature_abs, pipeline)\n    \n    for top_n in [n for n in top_n_lst if n < n_artist]:\n        print('top_n: {}'.format(top_n))\n        \n        predict_select = predict_artist(artist_song_feature['val'], feature_org, feature_abs, pipeline, top_n=top_n)\n        \n        result_dict = {'n_artist': n_artist, 'top_n': top_n, 'accuracy': predict_select.mean()}\n        result_lst.append(result_dict)\n        \n    print('')\n    \nresult_df = pd.DataFrame(result_lst)","d7e90dbc":"fig = px.line(result_df, x='n_artist', y='accuracy', color='top_n', \n              title='Accuracy vs number of artist and number of top selections', \n              labels={'n_artist': 'Number of artists per set', 'top_n': 'Top predictions'})\\\n        .update_traces(mode='lines+markers')\nfig.show()","70fbb656":"<a id=\"Brackets\"><\/a>\n### Brackets","0f262638":"Above I created one function to convert the extracted feature values to a matrix and get a target vector for prediction. The other function combines the creation of the feature dataframe, convertion of the features to a matrix and training a machine learning pipeline.","1f76df09":"The function above makes the matching prediction and validates the prediction accuracy. The variable `top_n` specifies how many top predictions are considered as correct. For example if there are four artists (A, B, C, D) matched to one song which belongs to artist C, the model orders the artist with repspect to the probability that they match to the respective song. The result could be B, C, A, D. \n\nIf `top_n` is set to one, the prediction is only considered as correct if the artist with the highest probability (in this case B) matches to the song. As this is not the case the prediction would be considered as wrong. If `top_n` is set to 2 or higher the prediction would be correct as C is the artist with the second highest probability. \nHence, the higher `top_n` the more likely a prediction is considered as correct. ","15e48906":"A very common way to analyse text is to seperate it into a list of words which makes it much easier to do further analysis. I'm using `nltk.tokenize` to do that. Furthermore, all punctuations are removed as well. Below is an example how the text is converted.","88baf381":"Now I have a look at how many words are there per line. This doesn't say something directly about the content of the songs, however artists may have different styles of structuring their songs. ","d9d636a4":"Songs can be covered by other artists, so it is possible that there are duplicated songs from different artists in the dataset.","d39e969f":"The histogram shows the distribution of number of words per song. Typically songs have 150 to 250 words. The distribution has a long tail - there are songs with far more words (up to 900).","306f0ac7":"I use following variables to create the datasets for training and validating the model:\n- `n_set`: number of sets for training and validation \n- `n_artist`: number of artists per set\n- `n_song_min`: minimum number of songs an artist must have to be selected\n- `n_song_artist_max`: maximum number of song - artist pairs per artist set\n\nThe artsits are randomly assigned to sets. It is possible that the same artist is assigned to several sets, but it is not possible that there are two sets with identical artists.","f78a50ee":"<a id=\"Line_breaks\"><\/a>\n### Line breaks","e98fc0f8":"<a id=\"Tokenization\"><\/a>\n### Tokenization","725e40c2":"Above are all columns with values of the first row of the dataframe. Every artist set has an id which is contained in `artist_set_id`. The column `artist_artist` contains the name of the artist from whom all artist related features were taken. `aritst_song` is the name of the artist whoes song was matched to the artist. In this case the artist features are from \"Little Mix\" and the song features are from the song \"Secret Love\" from \"Little Mix\". Thus, the prediction algorithm is expected to return a high probability that the artist and song match (the target variable is `same_artist`). \n\nFollowing features are added to each artist and song: `n_stems`, `unique_stems_ratio`, `stems_per_line`, `tf_idf_score`, and `polarity`. The artist features contain the mean (`_mean`) and standard deviation (`_std`). The difference of the song and artist features (song feature - artist feature) have the suffix `_diff`. Additionally these features are divided by the standard deviation to get a normalised measure for the difference (`_diff_std`). The dataframe also contains the TFIDF vector of all artist songs and the matched song (`tf_idf_vector_mean` and `tf_idf_vector`), the similarity of the vectors contains the feature `vector_similarity`. ","96481f28":"<a id=\"Sentiment_analysis\"><\/a>\n### Sentiment analysis","782e32af":"There are 57 thousand songs from 638 artists, on average there are 90 songs per artist. \n\nThe histogrm shows the distribution of songs per artist. We can see that there is quite a wide range: 28 artists have less than 10 songs while 22 have even more than 180 songs. \n\nThese differences may cause different accuracies for matching songs to the correct artist - artists with very few songs will be much more difficult to match.","aa4be0d5":"In this kernel I develop a machine learning approach to match artists (singer or band) to their song lyrics. \n\nThis is very challanging as there are so many artist while the number of songs per artist is relatively small. Thus, there is probably not enough data to train a multi-class classifier where each class represnts an artist. Furthermore, this approach would be very inflexible as it would require to train the whole model again when just adding one artist. \n\nInstead, I first match the song lyrics with each artist and predict for each pair separately if this song belongs to this artist. For example if I have one song s1 and a set of three artist a1, a2, a3, I create pairs s1 - a1, s1 - a2, and s1 - a3 and get prediction probabilities for each pair: p1, p2, p3. Then I select the artist - song pair with the highest probability. \n\nTo make these predictions I extract features for each song and each artist. \n\nNext in this kernel I will pre-process the data, do an exploratory data analysis (EDA), engineer features, build and train a machine learning model and finally validate the model's accuracy.","07906adf":"To validate the performance of the model I randomly create 100 training and 100 valdiation sets of 2, 4, 8, 16, 32, 64, and 128 artists per set. The accuracy of the model is calculated for values of `top_n` from 1 until 64. \n\nThe graph above shows how the accrucy decreased with respect to the number of artists for different values of `top_n`. If a match is only considered as correct when the artist of a song is the artist with the highest probability (`top_n = 1`) then the accuracy decreases from 96% for two artists per set to 12% for sets of 128 artists. Although 12% is a low value, it must be kept in mind that the probability of selection the correct artist purly by guessing would be 1\/128 which is about 0.8%. \n\nWhen a match is considered as correct if the selected artist is within the upper half of all artists (e.g. for a set of 32 artist `top_n` is set to 16). Then the accuracy ranges from 90% to 97%.","f5cbbde0":"In this part I do an exploratory data analysis to get more familiar with the dataset.","37affad5":"On the other hand, these are words with the highest values. \n\nNext I calculate an TFIDF vector for each song. Every element of this vector represents one word. Its value is calculated by multiplying its frequency with the corresponding weight which was calculated above. This value is normalised by the total number of words of the song. Words which don't appear in the text get a value of 0.\n\nThe TFIDF score is the sum of the TFIDF vector elements. The higher the score the more unusal the words of this song are compared to other songs.","7ac9df0a":"<a id=\"Number_of_songs_artists_and_words\"><\/a>\n### Number of songs, artists, and words","7c5dc965":"<a id=\"Remove_non-english_songs\"><\/a>\n### Remove non-english songs","2ecdc6f0":"I selected all features which were introduced in the feature engineering part, created a pipeline which normalises all features and uses logistic regression to estimate the probabilities. In the definition of the classifier I use class weights: positive samples (song is from matched artist) are weithted with `(n_artist - 1)\/n_artist` (`n_artist` is the number of artists per set) while negative samples are weithed with `1\/n_artist`. Thus, the more artists are in the set the higher the weight for positive samples. The higher the weight the stronger the impact of a wrong prediction on the loss function. For example in case of 5 artists predicting a true sample as false has a five times higher impact than predicting a false sample as true. This is done to prevent the classifier from predicting every sample as false. Therefore, a classifier which would predict every sample as false would get an accuracy of 80% while the weighted accuracy would be only 50%. \n\nI also tried other classifiers like random forest, support vector machine, or XG boost but always got very similar results. Therefore, I decided to use the simplest classifier. ","aee03f85":"# Artist classification by song lyrics\n\n2 Nov 2019  \nReinhard Sellmair\n\n\n1. [Introduction](#Introduction)\n2. [Pre-processing](#Pre-processing)  \n    2.1 [Brackets](#Brackets)  \n    2.2 [Line breaks](#Line_breaks)   \n    2.3 [Remove non-english songs](#Remove_non-english_songs)  \n    2.4 [Tokenization](#Tokenization)  \n    2.5 [Stemming](#Stemming)  \n3. [EDA](#EDA)  \n    3.1 [Number of songs, artists, and words](#Number_of_songs_artists_and_words)  \n    3.2 [Duplicated songs](#Duplicated_songs)  \n4. [Feature engineering](#Feature_engineering)  \n    4.1 [Number of words](#Number_of_words)  \n    4.2 [Repeated words](#Repeated_words)  \n    4.3 [Words per line](#Words_per_line)  \n    4.4 [Word frequency (TFIDF)](#Word_frequency_TFIDF)  \n    4.5 [Sentiment analysis](#Sentiment_analysis)  \n5. [Prediction](#Prediction)  \n6. [Validation](#Validation)  \n7. [Potential improvements](#Potential_improvements)  ","24f5e860":"<a id=\"Words_per_line\"><\/a>\n### Words per line","b6543097":"Stemming reduced the number of unique tokens from 103 thousand to 57 thousand.","7ffa8903":"The dataset contains four columns:\n- artist: name of artist\n- song: name of song\n- link: song reference (http:\/\/www.lyricsfreak.com needs to be added in front)\n- text: text of song lyric\n\nAll columns except the `link` are relevant for my work.\nThe `artist` and `song` columns are fine already, let's have a look at some `text` entries:","237dac94":"There are 633 duplicate song texts among them 579 are from different artists. This means that even if the matching of lyrics to artists is perfect it can't reach 100% accuracy. There are 54 duplicate lyrics from the same artist. These lyrics may have different instrumental versions.\n\nI only considered songs as duplicates when the song texts were completely identical. Possibly there are even more cover songs in the dataset with only very minor differences to the original song. For these songs it will also be very difficult to match the correct artist. ","a8d368b6":"The diagram above shows the polarity distribution of songs from the same artist. All distribution look very similar. Thus, neither polarity nor objectivity seems to be a good feature to distinguish songs from different artists.","c53e8e39":"<a id=\"Validation\"><\/a>\n## Validation","fc1ccad2":"<a id=\"Feature_engineering\"><\/a>\n## Feature engineering","edf0c64a":"<a id=\"Repeated_words\"><\/a>\n### Repeated words","3ee48206":"<a id=\"Pre-porcessing\"><\/a>\n## Pre-processing","4e8b7436":"There are more than 29 thousand square brackets. Looks like the text in square brackets does mostly not belong to the lyrics of the song and should be removed.\n\nLet's check curly brackets:","f10c85c8":"Stemming is a simple method to convert words to a common base form. For example converting plural to singular or past tense to present tense. This helps to treat words with the same meaning in the same way.\n\nIn contrast to lemmatization it uses a very simple method and does not always find the grammatically correct from. However, as stemming is much faster than lemmatization I'm using it here. \n\nBelow is an example how different forms of the word \"make\" are converted. In this case stemming failes to correctly convert the past tense.","52a482c8":"The plot shows the coefficient values of the logistic regression model. The higher the absolute value of the coefficient, the more important the feature. Thus, the features at the edge of the plot are the most important ones. \n\nThe more positive a coeffiecient the stronger the corresponding feature (all features were normalised and have positive values) causes a positive prediction. Thus, if the features on the left side of the plot have high values a negative prediction (different artists) is more likely, while the higher the features on the right the more likely a positive prediction (same artists) becomes.\n\nThe most negative coefficient is for `tf_idf_score` this means the more unusual the words of the song the more likely the song is from a different artist. Although this affects the probability, this feature only depends on the song and therefore does not affect the selection of artist - song pair.\nThe next feature is `stems_per_line_diff_std`, this feature describes the difference in the number of stems (words) per line between the artist and song. The higher the difference the more unlikely that the artist matches to the song. \n\nOn the other end of the scale is `vector_similarity`. This feature describes how similar the TF IDF vectors of the artist and the song are. The more similar the more likely it is that the artist and song match.","a29a100c":"First, I have a look at word counts - are there artists with typically long or short songs?","663c2336":"Every point of this plot represents one song. The x value is the polarity and the y value the subjectivity. Although there is a high variance in both features, I cannot find any area with predominantly songs of only one artist. ","561ad310":"This ratio typically varies from 0.2 to 0.7 among songs of the same artist. Nevertheless, there are some artists who repeat their words more often than others. In this selection these artist are Justin Timberlake and Olly Murs.","78dabc62":"<a id=\"Potential_improvements\"><\/a>\n## Potential improvements","2be475b9":"<a id=\"Prediction\"><\/a>\n## Prediction","113c73b3":"If we only accept the artist with the highest probability as correct (`top_n = 1`) the training accuracy is 79%. Increasing `top_n` to 2 raises the accuracy to 97%.","151157c8":"Above are the words with the lowest values. These words apear many times in almost each song. Thus, these words are not usefull to characterise a song. ","24d960d2":"In this section I use TFIDF to analyse word frequencies in more detail. TFIDF means term frequency - inverse document frequency. Term frequency means how often a word appears in a specific text (in this case song lyrics). Inverse document frequency is the inverse frequency of the same word in the whole document (in this case all song lyrics). \n\nThe idea of TFIDF is to find out if a specific word appears unusually often in the text. If this is the case, this word gets a high value, if the word appears more often in other texts, its value will be low. \n\nThis method is very common to cluster texts with similar topics.","752d6a2a":"<a id=\"Word_frequency_TFIDF\"><\/a>\n### Word frequency (TFIDF)","1caaf09b":"In total there are 63 thousand round brackets. Above are 20 randomly chosen content within brackets. Most of these seem to be part of the songs' text.\n\nNext I search for square brackets.","dd2558a6":"<a id=\"Number_of_words\"><\/a>\n### Number of words","1969fdbf":"This plot shows the distribution for the similarity of TFIDF vectors. Here we can see a difference in the distributions for songs from the same artist or different artists. So this seems to be the best feature. However, the distributions overlap a lot, therefore it will be difficult in general to distinguish songs from the same or different artist.","5048219f":"The distributions show the similarity of song vectors from the same artist with the artist's vector (blue) and the similarity of song vectors from other artists with the artist vector (red). \n\nIt can be seen that Genesis, Justin Timberlake, Olly Murs, and Supertramp have a much higher similarity with their own songs than with songs from other artists. However other artists like Harry Belafonte, Louis Jordan, and Vengaboys have even a higher similarity with other songs than with their own songs. This is probably a coincidence and its more fair to say that for these artists vector similarity is not a good measure to distinguish their songs from others.","4c495325":"I expect that the most likely way to improve the model's accuracy would be to add more features to the songs\/artits. Probably the most important feature is the vector similarity which is based on the TFIDF vectors of each song lyric. These vectors have more than 50,000 dimensions (one for each unique stems). Therefore, it is not practical to create one feature for each vector element. However, a way to extract more compact information could be to do PCA (princile component analysis) and select the eigenvalues of the most important components. \n\nAnother possiblity to create more features would be to apply `doc2vec` which is a neural networks based approach to convert a text to a vector.","27d64dd0":"In this part I create additional features which can be used by ML algorithm to improve the matching accuracy. \n\nTo show how these features vary from artist to artist I randomly select a set of 10 artists.","8a2e0eaa":"Tom T. Hall is having many more words per line than all other artists and Vengaboys have clearly fewer words than others. The distributions of all other artist look again very similar.","76674dbf":"<a id=\"Introduction\"><\/a>\n## Introduction","d224e152":"<a id=\"EDA\"><\/a>\n## EDA","c3415f6d":"In this part I have the first look at the data, remove unnecessary information and transform it so that I can do the EDA next.\n","c442152a":"There are 470 songs which probability of being english is less than 50%. I remove all those songs from the data.","7ce5bc76":"There are some artist who use way more words than others. For example Justin Timberlake as a median of 480 words per song while Harry Belafonte ony has a medain of 168 words. However, there are also many artists with very similar distributions like Kenny Loggins, Louis Jordan, Michael Buble, Supertramp, Tom T. Hall and Vengaboys.","0e42e60f":"The violine plot above shows the distribution of the normalised difference features `n_stems`, `unique_stems_ratio`, `stems_per_line`, `tf_idf_score`, and `polarity`. There is one distribution created which only contains artist - song pairs of the same artist (blue) and one distribution for the case of different artists (red). \n\nAll distributions look very similar, it is very difficult to find only one feature which indicates a difference between the same and different artists. ","1bf47186":"In this part I create a model match song lyrics to their artist. Therefore, I use all song features which were presented above. The artist features are calculated by averaging the features of all songs from this artist. \n\nFirst, I match all songs with all aritsts. Then I use logistic regression to estimate the probability that a song belongs to an artist for each song artist pair. After that I choose for each song the artist with the highest probability.","d15d93a2":"The texts also contains `\\n` which indicate line breaks. As this is also does not belong to the lyrics I remove all line breaks. However, the number of lines or number of words per line could vary from artist to artist I add the number of lines as new column before removing the breaks.","e385857b":"The distributions are quite different. Vengaboys have the lowest median and a high variance, while for example Supertramp have a high median and a low variance. The artist using the most unusual words is Genesis.\n\nAnother way to quantify differences in word selections is to calculate the similarity of TFIDF vectors. This can be done by calculating the angle between two vectors. For example if `v1` is (1, 0) and `v2` is (0, 1) these vectors point in orthogonal directions and have an angle of 90 degree. The cosine of these vectors would be 0 (this is the lowest possible similarity we can get as all TFIDF vectors cannot have negative values). If we change `v1` to (1, 1) the angle between the vectors would be smaller wich increases the cosine value and therewith their similarity. When two vectors are parallel their angle is 0 which means that the cosine would be 1 which is the maximum similarity. \n\nI will apply this metric to TFIDF vectors to compare their similarity. The only difference is that these vectors have several thousand dimensions (one for each word). \n\nFirst I will calculate an TFIDF vector for each artist which is calculated by taking the average of all TFIDF vectors of the artist's songs.","6010c02b":"Not all songs are in english language. As I will later do some text analysis which is language specific I remove all non-english songs. To do that I use `detect_langs` to get the probability that a text is in english language.","669bc39a":"Next I check the ratio of unique words over all words. This ratio is 1 if all words of a song are different, the more repeated words appearing in a song the lower this ratio becomes.","e5989a9d":"Below I stem all tokens.","796d72e4":"<a id=\"Stemming\"><\/a>\n### Stemming","e58e8dbc":"There are no curly brackets at all.\n\nSince the text in round brackets seems to belong to the song lyrics I only remove the brackets and keep the content. As square brackets usually do not contain text belonging to the lyrics I remove them including their content:","56527395":"The last features I engineer are polarity and subjectivity of songs. The `TextBlob` library has a function to get these values with respect to a given text.","6df09811":"It looks like the texts are formated in different styles: the first example includes text in round brackets which says how often parts of the text shall be repeated e.g. \"(repeat previous line 3 times)\". The second example does not have this but describes the type of lyrics in square brackets like \"[Chorus]\".\n\nFirst I have a look at round brackets:","2e0c3890":"This matrix visualises the similarity between TFIDF artist vectors. Again, a value of 1 (red) means that the vectors are identical which only appears when comparing vectors of the same artist. The lowest similarities are 0.4 between Louis Jordan and Vengaboys.\n\nThe matrix shows that Vengaboys seem to use very different words than any other artists. We can also see that some artists use very similar words like Kenny Loggins and Genesis. \n\nNext, I want to analyse how different TFIDF vectors of songs from the same artist are. Therefore, I calculate the similarity of the artist's TFIDF vector with the TFIDF vector of each song. The problem hereby is that the artist vector was averaged over all song vectors including the one I want to compare against. To avoid any bias from that I calculate the artist vector over all song vectors except the one I'm comparing it against. \n\nFor example if an artist has three songs: A, B, C. In order to compare how similar song A is with all songs of the artist I calculate the artist vector only from song B and C. To compare song B, the artist vector only consists of song A and C, and so on.","ee39d348":"<a id=\"Duplicated_songs\"><\/a>\n### Duplicated songs","bd049db8":"The above function creates a dictionary with the fields specified in `n_set` with the number of defined artist sets. An artist set is a set of songs from `n_artist` number of randomly selected artists. \n\nThe values of this dictionary are dataframes with all artist sets, e.g. if `n_set['train'] = 20` it contains 20 artist sets. Each row of the data set contains a pair of artist - song matches. Thereby, the song is from one of the randomly selected artists of this set. In this example the first artist set contains the artists: Little Mix, Our Lady Peace, and Underoath."}}