{"cell_type":{"cbd782cb":"code","e25f0ace":"code","44d45f4a":"code","a1ca457d":"code","9599d0cf":"code","563d25f7":"code","0eb50090":"code","21f49239":"code","ec4fec5c":"code","20e05b55":"code","b92c2748":"code","1b23cee0":"code","3cc1eb88":"code","11efa303":"code","a4ec52fd":"code","1e35f3d0":"code","912c1a1e":"code","4d1dbbad":"code","dacf2f9b":"code","1911a1a4":"code","79f752dc":"code","7ba3f27e":"code","424f21a3":"code","0d966674":"code","6b91fef9":"code","5a3cbecc":"markdown","0ed67aa3":"markdown","399a24a2":"markdown","4f2d7d34":"markdown","34cf2e13":"markdown","d4a02410":"markdown","60ac5dd6":"markdown","5972638a":"markdown","b709f493":"markdown","355ccdd5":"markdown","a874119f":"markdown","5723b83e":"markdown","2d1308c6":"markdown","2a418eee":"markdown","7ace857d":"markdown"},"source":{"cbd782cb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e25f0ace":"import cv2\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","44d45f4a":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/HappyFish.jpg')\nplt.imshow(image)","a1ca457d":"image2 = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nplt.imshow(image2)","9599d0cf":"ks = np.array([[-1,-1,-1],[-1,40,-1],[-1,-1,-1]])","563d25f7":"sharp = cv2.filter2D(image2, -1, ks)\nplt.imshow(sharp)","0eb50090":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/hand.jpg', 0)\nplt.imshow(image)\n#Thresholding- Image Proccesing Method to creates a binary image on setting a threshold value","21f49239":"#Thresholding- Image Proccesing Method to creates a binary image on setting a threshold value as \n#i did here below 127 is 0 and above 127 is 255 check image array to convince urself. \nret,thresh1 = cv2.threshold(image, 127, 255, cv2.THRESH_BINARY)\nplt.imshow(thresh1)\n#print(thresh1)","ec4fec5c":"image = cv2.GaussianBlur(image, (3, 3), 0)\n#to reduce Noise and details \nplt.imshow(image)","20e05b55":"thresh = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_MEAN_C, cv2.THRESH_BINARY, 3, 5) \nplt.imshow(thresh)\n#in adaptive thresholding local threshold values use as throshold vlaues\n#use to saperate desired foreground from background based on the pixel intensity of each region","b92c2748":"_, th2 = cv2.threshold(image, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\nplt.imshow(th2)\nblur = cv2.GaussianBlur(image, (5,5), 0)\n_, th3 = cv2.threshold(blur, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n","1b23cee0":"plt.imshow(th3)","3cc1eb88":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/orange.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20, 20))\nplt.subplot(3, 2, 1)\nplt.imshow(image)\nkernel = np.ones((5,5), np.uint8)#5*5 array of ones\nerosion = cv2.erode(image, kernel, iterations = 1)\nplt.subplot(3,2,2)\nplt.imshow(erosion)#erosion morphological process, GFG link to understand the difference \ndilation = cv2.dilate(image, kernel, iterations = 1)\nplt.subplot(3, 2, 3)\nplt.imshow(dilation)#https:\/\/www.geeksforgeeks.org\/difference-between-dilation-and-erosion\/\nopening = cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel)\nplt.subplot(3, 2, 4)\nplt.imshow(opening)# Closing - Good for removing noise\n#Opening is generally used to restore or recover the original image to the maximum possible extent\nclosing = cv2.morphologyEx(image, cv2.MORPH_CLOSE, kernel)\nplt.subplot(3, 2, 5)\n#Closing is generally used to smoother the contour of the distorted image and fuse back the narrow breaks and long thin gulfs\nplt.imshow(closing)#https:\/\/www.geeksforgeeks.org\/difference-between-opening-and-closing-in-digital-image-processing\/\n","11efa303":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/fruits.jpg')\nplt.figure(figsize=(20, 20))\nplt.subplot(3,2,1)\nplt.imshow(image)\nheight, width,_ = image.shape\nsobel_x = cv2.Sobel(image, cv2.CV_64F, 0, 1, ksize=5)\nsobel_y = cv2.Sobel(image, cv2.CV_64F, 1, 0, ksize=5)\nplt.subplot(3,2,2)\nplt.imshow(sobel_x)\nplt.subplot(3,2,3)\nplt.imshow(sobel_y)\n#https:\/\/homepages.inf.ed.ac.uk\/rbf\/HIPR2\/sobel.htm\n#sobel filter a gradient based filter that looks for strong changes in the first derivative of image \nsobel_OR = cv2.bitwise_or(sobel_x, sobel_y)\nplt.subplot(3,2,4)\nplt.imshow(sobel_OR)\nlaplacian = cv2.Laplacian(image, cv2.CV_64F)\ncanny = cv2.Canny(image, 50, 120)\n\nplt.subplot(3, 2, 5)\nplt.title(\"Laplacian\")\nplt.imshow(laplacian)\nplt.subplot(3, 2, 6)\nplt.title(\"Canny\")\nplt.imshow(canny)","a4ec52fd":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/scan.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20, 20))\nplt.subplot(3,2,1)\nplt.imshow(image)\npoints_A = np.float32([[320,15], [700,215], [85,610], [530,780]])\npoints_B = np.float32([[0,0], [420,0], [0,594], [420,594]])\nM = cv2.getPerspectiveTransform(points_A, points_B)\nwarped = cv2.warpPerspective(image, M, (420,594))\nplt.subplot(3, 2, 2)\nplt.imshow(warped)\n","1e35f3d0":"#Re-sizing is very easy using the cv2.resize function, \n#it's arguments are: cv2.resize(image, dsize(output image size), x scale, y scale, interpolation)\nimage = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/fruits.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\nplt.subplot(3,2,1)\nplt.imshow(image)\nimage_scaled = cv2.resize(image, None, fx=0.75, fy=0.75)\nimg_scaled = cv2.resize(image, None, fx=2, fy=2, interpolation = cv2.INTER_CUBIC)\nplt.subplot(3,2,2)\nplt.imshow(img_scaled)\n","912c1a1e":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/butterfly.jpg')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\nplt.subplot(2,2,1)\nplt.imshow(image)\nsmall = cv2.pyrDown(image)\nlarg = cv2.pyrUp(small)\nplt.subplot(2,2,2)\nplt.imshow(small)\nplt.subplot(2,2,3)\nplt.imshow(larg)\n","4d1dbbad":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/messi5.jpg')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\nplt.subplot(2,2,1)\nplt.imshow(image)\nheight, width = image.shape[:2]\nstart_row, start_col = int(height * .25), int(width * .25)\nend_row, end_col = int(height * .75), int(width * .75)\ncropped = image[start_row:end_row , start_col:end_col]\nplt.subplot(2, 2, 2)\nplt.title(\"Cropped\")\nplt.imshow(cropped)\n","dacf2f9b":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/home.jpg')\nimage = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\nplt.subplot(2,2,1)\nplt.imshow(image)\nkernel_3x3 = np.ones((3, 3), np.float32) \/ 9\nblurred = cv2.filter2D(image, -1, kernel_3x3)\nplt.subplot(2, 2, 2)\nplt.title(\"3x3 Kernel Blurring\")\nplt.imshow(blurred)\n\nkernel_7x7 = np.ones((7, 7), np.float32) \/ 49\nblurred2 = cv2.filter2D(image, -1, kernel_7x7)\nplt.subplot(2, 2, 3)\nplt.title(\"7x7 Kernel Blurring\")\nplt.imshow(blurred2)\n","1911a1a4":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/sudoku.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20, 20))\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\nedges = cv2.Canny(gray, 100, 170, apertureSize = 3)\n\nplt.subplot(2, 2, 1)\nplt.title(\"edges\")\nplt.imshow(edges)\nlines = cv2.HoughLines(edges, 1, np.pi\/180, 200)\n#Hough Lines first input binary image, 2nd r(rcos(theta)+rsin(theta)),3rd theta, 4th threshold\n#value, number of points on the line, if value>threshold it would be a line otherwise not \nfor line in lines:\n    rho, theta = line[0]\n    a = np.cos(theta)\n    b = np.sin(theta)\n    x0 = a * rho\n    y0 = b * rho\n    x1 = int(x0 + 1000 * (-b))\n    y1 = int(y0 + 1000 * (a))\n    x2 = int(x0 - 1000 * (-b))\n    y2 = int(y0 - 1000 * (a))\n    cv2.line(image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n\n\nplt.subplot(2, 2, 2)\nplt.title(\"Hough Lines\")\nplt.imshow(image)","79f752dc":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/blobs.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\nplt.subplot(3,2,1)\nplt.imshow(image)\ndetector = cv2.SimpleBlobDetector_create()\nkeypoints = detector.detect(image)\nblank = np.zeros((1,1)) \nblobs = cv2.drawKeypoints(image, keypoints, blank, (0,0,255),\n                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\nnumber_of_blobs = len(keypoints)\ntext = \"Total Number of Blobs: \" + str(len(keypoints))\ncv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (100, 0, 255), 2)\nplt.subplot(3, 2, 1)\nplt.title(\"Blobs using default parameters\")\nplt.imshow(blobs)\nparams = cv2.SimpleBlobDetector_Params()\n\n# Set Area filtering parameters\nparams.filterByArea = True\nparams.minArea = 100\n\n# Set Circularity filtering parameters\nparams.filterByCircularity = True \nparams.minCircularity = 0.9\n\n# Set Convexity filtering parameters\nparams.filterByConvexity = False\nparams.minConvexity = 0.2\n    \n# Set inertia filtering parameters\nparams.filterByInertia = True\nparams.minInertiaRatio = 0.01\n\n# Create a detector with the parameters\ndetector = cv2.SimpleBlobDetector_create(params)\n    \n# Detect blobs\nkeypoints = detector.detect(image)\n\n# Draw blobs on our image as red circles\nblank = np.zeros((1,1)) \nblobs = cv2.drawKeypoints(image, keypoints, blank, (0,255,0),\n                                      cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n\nnumber_of_blobs = len(keypoints)\ntext = \"Number of Circular Blobs: \" + str(len(keypoints))\ncv2.putText(blobs, text, (20, 550), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)\n\n# Show blobs\nplt.subplot(2, 2, 2)\nplt.title(\"Filtering Circular Blobs Only\")\nplt.imshow(blobs)","7ba3f27e":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/data\/chessboard.png')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(20,20))\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\ngray = np.float32(gray)\n#harris_corner requires float array\nharris_corners = cv2.cornerHarris(gray, 3, 3, 0.05)\nkernel = np.ones((7,7),np.uint8)\nharris_corners = cv2.dilate(harris_corners, kernel, iterations = 10)\nimage[harris_corners > 0.025 * harris_corners.max() ] = [255, 127, 127]\nplt.subplot(1, 1, 1)\nplt.title(\"Harris Corners\")\nplt.imshow(image)","424f21a3":"image = cv2.imread('\/kaggle\/input\/opencv-samples-images\/WaldoBeach.jpg')\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(30, 30))\n\nplt.subplot(2, 2, 1)\nplt.title(\"Where is Josh?\")\nplt.imshow(image)\n\ngray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n\n# Load Template image\ntemplate = cv2.imread('\/kaggle\/input\/opencv-samples-images\/waldo.jpg',0)\n\nresult = cv2.matchTemplate(gray, template, cv2.TM_CCOEFF)\nmin_val, max_val, min_loc, max_loc = cv2.minMaxLoc(result)\n\n#Create Bounding Box\ntop_left = max_loc\nbottom_right = (top_left[0] + 50, top_left[1] + 50)\ncv2.rectangle(image, top_left, bottom_right, (0,0,255), 5)\n\nplt.subplot(2, 2, 2)\nplt.title(\"Josh\")\nplt.imshow(image)","0d966674":"algo = 'MOG2'\n#i copied the whole code from the opencv tutorials.\nif algo == 'MOG2':\n    backSub = cv2.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv2.createBackgroundSubtractorKNN()\n\nplt.figure(figsize=(20, 20))\n\nframe = cv2.imread('\/kaggle\/input\/opencv-samples-images\/Background_Subtraction_Tutorial_frame.png')\nfgMask = backSub.apply(frame)\n\nplt.subplot(2, 2, 1)\nplt.title(\"Frame\")\nplt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\nplt.subplot(2, 2, 2)\nplt.title(\"FG Mask\")\nplt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n                       \nframe = cv2.imread('\/kaggle\/input\/opencv-samples-images\/Background_Subtraction_Tutorial_frame_1.png')\nfgMask = backSub.apply(frame)\n\nplt.subplot(2, 2, 3)\nplt.title(\"Frame\")\nplt.imshow(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n\nplt.subplot(2, 2, 4)\nplt.title(\"FG Mask\")\nplt.imshow(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))","6b91fef9":"algo = 'MOG2'\ninputt = '\/kaggle\/input\/opencv-samples-images\/video_input\/Background_Subtraction_Tutorial_frame.mp4'\n\ncapture = cv2.VideoCapture(cv2.samples.findFileOrKeep(inputt))\nframe_width = int(capture.get(3))\nframe_height = int(capture.get(4))\n\nout = cv2.VideoWriter('Background_Subtraction_Tutorial_frame_output.mp4',cv2.VideoWriter_fourcc('M','J','P','G'),30, (frame_width,frame_height))\n\nif algo == 'MOG2':\n    backSub = cv2.createBackgroundSubtractorMOG2()\nelse:\n    backSub = cv2.createBackgroundSubtractorKNN()\n\n# If you want to run it on video and locally, you must set it to (While) True. (Do not try on Kaggle you will get the error)\nwhile False:\n    \n    ret, frame = capture.read()\n    \n    if frame is None:\n        break\n    \n    fgMask = backSub.apply(frame)\n    \n    cv2.rectangle(frame, (10, 2), (100,20), (255,255,255), -1)\n\n    cv2.imshow('Frame', frame)\n    cv2.imshow('FG Mask', fgMask)\n    \n    out.write(cv2.cvtColor(fgMask, cv2.COLOR_BGR2RGB))\n    \n    keyboard = cv2.waitKey(1) & 0xFF;\n        \n    if (keyboard == 27 or keyboard == ord('q')):\n        cv2.destroyAllWindows()\n        break;\n        \ncapture.release()\nout.release()\ncv2.destroyAllWindows()","5a3cbecc":"# Edge Detection & Image Gradients","0ed67aa3":"# Cropping","399a24a2":"# Image Pyramids","4f2d7d34":"# Background Subtraction Methods\nBackground subtraction (BS) is a common and widely used technique for generating a foreground mask (namely, a binary image containing the pixels belonging to moving objects in the scene) by using static cameras.\n\nAs the name suggests, BS calculates the foreground mask performing a subtraction between the current frame and a background model, containing the static part of the scene or, more in general, everything that can be considered as background given the characteristics of the observed scene","34cf2e13":"# Finding Corners","d4a02410":"# Blurring","60ac5dd6":"# If you want to run it on video and locally, you must set it to (While) True.","5972638a":"# Counting Circles and Ellipses","b709f493":"# Dilation, Erosion, Opening and Closing","355ccdd5":"# Line Detection - Using Hough Lines","a874119f":"# Finding Josh","5723b83e":"# Scaling, re-sizing and interpolations","2d1308c6":"# Perpsective Transform","2a418eee":"# For Funny Mirrors Using OpenCV We need vcam ","7ace857d":"#  Thresholding, Binarization & Adaptive Thresholding"}}