{"cell_type":{"bc2b926f":"code","5d729d8e":"code","7f3e2e0a":"code","af3e4a1f":"code","6705747d":"code","0ca6028e":"code","3d953d65":"code","c618815a":"code","a99aa984":"code","71a8b079":"code","e9c2f9bf":"code","77dcbdda":"code","93fe747a":"code","d81ad53e":"code","be7eb1bc":"code","be1853e1":"code","996523a5":"code","b99cd435":"code","7bd838ba":"code","0f7290f7":"code","f12cd91a":"code","a7ced247":"code","a3ff6aea":"code","05211037":"code","ad02ae64":"code","887ec05e":"code","a0ec433b":"markdown","6dc62daf":"markdown","90eafd45":"markdown","2b901108":"markdown","926fe89e":"markdown","c33038f2":"markdown","19d6b511":"markdown","016ebfa2":"markdown","967ee8a9":"markdown","7980c246":"markdown","62629bc1":"markdown","5108a11f":"markdown","2c11aeda":"markdown","6a2de830":"markdown","05f5cfe2":"markdown","b9526ab1":"markdown","39b5beed":"markdown","c87c95e3":"markdown","99dca35e":"markdown","0b4966d4":"markdown","539cf8ce":"markdown"},"source":{"bc2b926f":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport IPython\nimport matplotlib.pyplot as plt\nimport librosa\nfrom nltk.corpus import stopwords \nimport random\nfrom scipy.io import wavfile\nfrom tqdm import tqdm_notebook as tqdm\nimport tensorflow as tf\nimport os\nfrom collections import Counter\nfrom sklearn.preprocessing import OneHotEncoder\nfrom tensorflow.python.keras.optimizer_v2.adam import Adam\nfrom nltk.util import ngrams\nfrom keras.callbacks import LearningRateScheduler\nimport nltk\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport glob\nfrom PIL import Image","5d729d8e":"import_df = pd.read_csv('..\/input\/medical-speech-with-spectrograms\/Medical Speech, Transcription, and Intent\/overview-of-recordings.csv')\nimport_df = import_df[['file_name','phrase','prompt','overall_quality_of_the_audio','speaker_id']]\nprint(import_df.shape)\nimport_df.head()","7f3e2e0a":"test_num = random.randrange(0, len(import_df))\ntest_file_name = import_df.loc[test_num, 'file_name']\nprint(import_df.loc[test_num, 'prompt'] + '\\n' + import_df.loc[test_num, 'phrase'])\ndisplay_audio_file = f'..\/input\/medical-speech-with-spectrograms\/Medical Speech, Transcription, and Intent\/recordings\/test\/{test_file_name}'\nIPython.display.Audio(display_audio_file)","af3e4a1f":"grouped_series = import_df.groupby('prompt').agg('count')['speaker_id'].sort_values(ascending=False)\nunique_prompts = len(import_df['prompt'].unique())\nprint(\"Number of unique prompts : \", unique_prompts)\nsns.barplot(grouped_series.values, grouped_series.index)\nplt.title('Prompts to be Used as Classification Targets')\nsns.despine()","6705747d":"preprocess_df = import_df.drop('overall_quality_of_the_audio', axis=1)\n\nfig = plt.figure(figsize=(7,4))\nsns.distplot(import_df['overall_quality_of_the_audio'], hist=False, color='teal')\nsns.despine()","0ca6028e":"stop_words = set(stopwords.words('english')) \nword_dict = {}\npreprocess_df['phrase'] = [i.lower() for i in preprocess_df['phrase']]\npreprocess_df['phrase'] = [i.replace('can\\'t', 'can not') for i in preprocess_df['phrase']]\npreprocess_df['phrase'] = [i.replace('i\\'m', 'i am') for i in preprocess_df['phrase']]\npreprocess_df['phrase'] = [i.replace('i\\'ve', 'i have') for i in preprocess_df['phrase']]\npreprocess_df['phrase'] = [' '.join([j for j in i.split(' ') if j not in stop_words]) for i in preprocess_df['phrase']]\n\nfor phrase in preprocess_df['phrase']:\n    for word in phrase.split(' '):\n        word = word.lower()\n        if word in stop_words or word == '':\n            pass\n        elif word not in word_dict:\n            word_dict[word] = 1\n        else:\n            word_dict[word] += 1\n            \nsorted_word_list = sorted(word_dict.items(), key=lambda kv: kv[1], reverse=True)","3d953d65":"n = 30\nfig = plt.figure(figsize=(7,10))\nplt.style.use('ggplot')\nsns.barplot([i[1] for i in sorted_word_list[:n]], [i[0] for i in sorted_word_list[:n]])","c618815a":"def get_ngrams(text, n):\n    n_grams = ngrams((text), n)\n    return [' '.join(i) for i in n_grams]\n\ndef gramfreq(phrases, n, num):\n    ngram_dict = {}\n    for phrase in phrases:\n        result = get_ngrams(phrase.split(' '),n)\n        result_count = Counter(result)\n        for gram in result_count.keys():\n            if gram not in ngram_dict.keys():\n                ngram_dict[gram] = 1\n            else:\n                ngram_dict[gram] += 1\n    df = pd.DataFrame.from_dict(ngram_dict, orient='index')\n    df = df.rename(columns={'index':'words', 0:'frequency'}) # Renaming index column name\n    return df.sort_values([\"frequency\"],ascending=[0])[:num]\n\ndef gram_table(x, ns, result_length):\n    output = pd.DataFrame(index=None)\n    for n in ns:\n        table = pd.DataFrame(gramfreq(x, n, result_length).reset_index())\n        table.columns = [f\"{n}-Gram\",f\"{n}-Occurence\"]\n        output = pd.concat([output, table], axis=1)\n    return output\n\ngram_df = gram_table(x=preprocess_df['phrase'], ns=[1,2,3,4], result_length=30)\ngram_df.head(20)","a99aa984":"fig = plt.figure(figsize=(10,5))\nplt.title('Frequency of 2-grams, 3-grams and 4-grams', fontsize=15)\n\nsns.distplot(gram_df['4-Occurence'], kde=False, label = '4-grams')\nsns.distplot(gram_df['3-Occurence'], kde=False, label = '3-grams')\nsns.distplot(gram_df['2-Occurence'], kde=False, label = '2-grams')\n\nplt.ylabel('# of N-Grams')\nplt.xlabel('# of Appearences')\nplt.legend(facecolor='white')","71a8b079":"base_dir = '..\/input\/medical-speech-with-spectrograms\/Medical Speech, Transcription, and Intent\/recordings\/'\n\ntrain_files = [base_dir + 'train\/' + i for i in os.listdir(base_dir + 'train')]\nval_files = [base_dir + 'validate\/' + i for i in os.listdir(base_dir + 'validate')]\ntest_files = [base_dir + 'test\/' + i for i in os.listdir(base_dir + 'test')]\n\nall_files = train_files + test_files + val_files\nlen(all_files)","e9c2f9bf":"tokenizer = Tokenizer(oov_token=\"<OOV>\")\ntokenizer.fit_on_texts(preprocess_df['phrase'])\nword_index = tokenizer.word_index\nvocab_size = len(word_index)\nprint(f'vocab_size : {vocab_size}')\n\nphrases_seq = tokenizer.texts_to_sequences(preprocess_df['phrase'])\npadded_phrases_seq = pad_sequences(phrases_seq, padding='post')\npadded_phrases_seq = np.asarray(padded_phrases_seq)\nmax_seq_length = padded_phrases_seq.shape[0]\nprint(\"padded_phrases_seq shape : \", padded_phrases_seq.shape)","77dcbdda":"random_phrase_num = random.randrange(0, len(preprocess_df))\nrandom_import_phrase = import_df.loc[random_phrase_num, 'phrase']\nrandom_phrase = preprocess_df.loc[random_phrase_num, 'phrase']\n\nprint('padded_phrase example : ' + '\\n' + random_import_phrase + '\\n' + random_phrase + '\\n' + str(padded_phrases_seq[random_phrase_num]))","93fe747a":"#os.listdir('..\/input\/x-wav-array\/x_wav_array.npy')#\/x_wav_array.npy')","d81ad53e":"wav_list = []\nimport librosa.display\nfrom pathlib import Path\nspec_dir = base_dir + 'spectrograms\/'\n\n!mkdir \/kaggle\/working\/spectro\n\n\n### Function from https:\/\/www.kaggle.com\/devilsknight\/sound-classification-using-spectrogram-images\n### Originally linked by kaggle user _____\ndef create_spectrogram(filename,name):\n    plt.interactive(False)\n    clip, sample_rate = librosa.load(filename, sr=None)\n    fig = plt.figure(figsize=[2,2])\n    ax = fig.add_subplot(111)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_frame_on(False)\n    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n    filename = Path(\"\/kaggle\/working\/spectro\/\" + name + '.jpg')\n    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n    plt.close()    \n    fig.clf()\n    plt.close(fig)\n    plt.close('all')\n    del filename,name,clip,sample_rate,fig,ax,S\n\n### Not going to run this, I loaded the files from local to minimize run time on Kaggle.\n#for file in tqdm(all_files, total=len(all_files)):\n#    create_spectrogram(file, file.split('\/')[-1])\n","be7eb1bc":"#spec_filelist = [f'..\/input\/medical-speech-with-spectrograms\/Medical Speech, Transcription, and Intent\/recordings\/spectrograms\/{i}.jpg' for i in preprocess_df.file_name]\n#x_wav_array = np.array([np.array(Image.open(fname)) for fname in spec_filelist])\nx_wav_array = np.load('..\/input\/x-wav-array\/x_wav_array.npy')\nprint(x_wav_array.shape)","be1853e1":"enc = OneHotEncoder(handle_unknown='ignore')\nprompt_array = preprocess_df['prompt'].values.reshape(-1,1)\nlabels_onehot = enc.fit_transform(prompt_array).toarray()\n\nlabels_onehot.shape","996523a5":"x_train, x_test, y_train, y_test = train_test_split(preprocess_df.index, labels_onehot, test_size = .2)","b99cd435":"x_phrase_train = padded_phrases_seq[x_train]\nx_phrase_test = padded_phrases_seq[x_test]\n\nx_wav_train = x_wav_array[x_train]\nx_wav_test = x_wav_array[x_test]\n\ntry:\n    del x_wav_array\nexcept:\n    pass\n\nx_wav_train = np.stack(x_wav_train, axis=0)\nx_wav_test = np.stack(x_wav_test, axis=0)\n\nprint(x_phrase_train.shape)\nprint(x_phrase_test.shape)\n\nprint(x_wav_train.shape)\nprint(x_wav_test.shape)","7bd838ba":"def build_phrase_model(vocab_size, embedding_dim, rnn_units, max_seq_length):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Embedding(vocab_size + 1, ### Without +1, layer expects [0,1160) and our onehot encoded values include 1160\n                                        embedding_dim, ### Output layer size\n                                        input_length =  14))\n    model.add(tf.keras.layers.LSTM(rnn_units))\n    model.add(tf.keras.layers.Dense(100, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.2))\n    model.add(tf.keras.layers.Dense(unique_prompts, activation='softmax'))\n    return model\n\nmodel = build_phrase_model(\n    vocab_size = vocab_size,\n    embedding_dim=100,\n    rnn_units=150,\n    max_seq_length=max_seq_length)\n\nadam_opt = Adam(lr=0.01)\n\nmodel.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\nmodel.summary()","0f7290f7":"vocab_size","f12cd91a":"earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=2, min_delta=.005)\n\n\ndef exp_decay(epoch):\n    initial_lrate = 0.01\n    k = 0.1\n    lrate = initial_lrate * np.exp(-k*epoch)\n    return lrate\nlrate = LearningRateScheduler(exp_decay)\n\ncallbacks_list = [earlystop_callback, lrate]\n\nhistory = model.fit(x_phrase_train, y_train,\n                    epochs=15, batch_size=30, validation_split = .2,\n                    callbacks=callbacks_list)","a7ced247":"model_cm = confusion_matrix(np.argmax(y_test,axis=1), np.argmax(model.predict(x_phrase_test),axis=1))\n\nfig = plt.figure(figsize=(15,10))\nsns.heatmap(model_cm, annot=True, xticklabels=enc.categories_[0].tolist(), yticklabels=enc.categories_[0].tolist())","a3ff6aea":"from keras.constraints import max_norm\ndef build_wav_model(filters, input_shape):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Conv2D(filters, 2, 2, activation='relu', padding=\"same\", input_shape=input_shape, kernel_constraint=max_norm(3), bias_constraint=max_norm(3)))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n    model.add(tf.keras.layers.Conv2D(int(filters \/ 2), 2, 2, activation='relu', padding=\"same\"))\n    model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n    model.add(tf.keras.layers.BatchNormalization())\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dropout(.2))\n    #model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.Dense(256, activation='relu'))\n    model.add(tf.keras.layers.Dense(unique_prompts, activation='softmax'))\n    return model\n\nwav_model = build_wav_model(\n    filters = 32,\n    input_shape = x_wav_train[0].shape)\n\nadam_opt = Adam(lr=0.001)\n\nwav_model.compile(optimizer=adam_opt, loss='categorical_crossentropy', metrics=['accuracy'])\nwav_model.summary()","05211037":"earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, min_delta=.005)\n\ncallbacks_list = [earlystop_callback]\n\n#history = wav_model.fit(x_wav_train, y_train,epochs=15, batch_size=20, validation_split = .2,callbacks=callbacks_list)","ad02ae64":"def alexnet(in_shape=x_wav_train[0].shape, n_classes=unique_prompts, opt='sgd'):\n    model = tf.keras.Sequential()\n    model.add(tf.keras.layers.Input(in_shape))\n    model.add(tf.keras.layers.Conv2D(96,11, strides=4, activation='relu'))\n    model.add(tf.keras.layers.MaxPool2D(3, 2))\n    model.add(tf.keras.layers.Conv2D(256,5, strides=1, padding='same', activation='relu'))\n    model.add(tf.keras.layers.MaxPool2D(3, 2))\n    model.add(tf.keras.layers.Conv2D(384, 3, strides=1, padding='same', activation='relu'))\n    model.add(tf.keras.layers.Conv2D(256, 3, strides=1, padding='same', activation='relu'))\n    model.add(tf.keras.layers.MaxPool2D(3, 2))\n    model.add(tf.keras.layers.Flatten())\n    model.add(tf.keras.layers.Dense(4096, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(4096, activation='relu'))\n    model.add(tf.keras.layers.Dropout(0.5))\n    model.add(tf.keras.layers.Dense(n_classes, activation='softmax'))\n\n    return model","887ec05e":"earlystop_callback = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=5, min_delta=.005)\n\ncallbacks_list = [earlystop_callback]\n\nalexnet_model = alexnet()\n\nalexnet_model.compile(loss=\"categorical_crossentropy\", optimizer='adam',\n\t              metrics=[\"accuracy\"])\n\n#history = alexnet_model.fit(x_wav_train, y_train,epochs=15, batch_size=20, validation_split = .2,callbacks=callbacks_list)","a0ec433b":"#### .WAV Features\n\nThe WAV processing pipeline is very laborous and difficult to run on a Kaggle Kernal so I've included the code but commented out the functions to create the Spectrogram files and the array. I've included the processed array as a kernal here. ","6dc62daf":"### Building the WAV Model\n\nThe WAV model wasn't successful. It was VERY difficult to get the data in a format that would fit in the kaggle kernal and even once I thought I did that, the model wasn't able to identify any more accurately than random. I've left the code in but I will not run it in the kernal.","90eafd45":"#### Test Train Split\n\nUsing sklearn, I'll split up the training and test data from the padded, encoded phrases and the one hot encoded labels.","2b901108":"## Import Data\n\nFirst things first, import the data and select the relevant columns. \n- `Prompt` will be used as the classification target.\n\n- `file_name` and `phrase` will be used as features in two neural networks. \n\n- `overall_quality_of_the_audio` may be relevant for filtering bad quality audio later on. ","926fe89e":"#### Target Array\n\nI'll one hot encode the target column to get the labels for the model.","c33038f2":"We can plot the resulting df for a better picture.","19d6b511":"Below I'll show the `n` most common words in the phrases. ","016ebfa2":"## Data Exploration\n\nFirst, I'll visualize the target variable to get an idea what we're working with. ","967ee8a9":"It wasn't a problem with the network :') \n\n### Conclusions\n\n- The text phrases were able to be tokenized and fed into an LSTM which classified them into their corresponding prompts very accurately.\n\n- The .wav files are difficult to work with.\n    - Feeding the data derived directly from the file isn't an option as its nearly 100k lines per file.\n    - Convert the sound data into spectrograms and then feeding the images into a CNN did not allow for classification. \n    \n#### Things to Improve\n\n- Try different methods of working with the .wav files. \n    - Larger images may allow for higher information transcirption density but the file size is a big issue. \n    - Converting them to a different type of image may be necessary.\n    - Working with the raw sound data rather than an image representation would be ideal but would require a \"big data\" approach.","7980c246":"### Sample \n\nI'll randomly sample the dataframe and display the transcription, the classification and the audio file in a player to get an idea of what the neural network will be figuring out. ","62629bc1":"The confusion matrix shows that the RNN was able to accurately classify the phrases into the categories! The accuracy looks a little decieving but that's what you get when you havesqueaky clean, short text samples and plenty of training data!","5108a11f":"When this code was run, it was not able to identify phrases with any more accuracy than random.\n\nI'll try the AlexNet architecture to see if it was a problem with the NN.","2c11aeda":"I'm removing contractions and then creating a word dictionary with all the words from the phrases.","6a2de830":"### Data Pre-processing\n\n#### Create training, test, validation indices\n\nWe'll group the examples by the folder the .wav file is in. ","05f5cfe2":"# Classifying Ailemnts using Medical Text and Speech Phrases\n\nThe dataset contains brief audio recordings and transcripts of issues related to 25 different ailments. In this kernal, I'll do a little bit of pre-processing and then categorize the ailments using the text and then the speech recording files.\n\n## Import Packages","b9526ab1":"This function will create a spectrogram of each recording using the `librosa` library. ","39b5beed":"We can also plot the distribution of the reported audio quality. It looks the values are on the higher end of the 5-point scale so we won't have to filter anything. We can just drop the column now. ","c87c95e3":"### Building the Phrase Model\n\nThe first model we'll build is the phrase model. We'll use TF\/Keras and put together an LSTM to predict the classification of a phrase using the padded sequence values of the phrase. ","99dca35e":"With inspiration from [this](https:\/\/www.kaggle.com\/nicapotato\/explore-the-spooky-n-grams-wordcloud-bayes#N-Grams) kernal, I define some helper functions to create a dataframe of the most common ngrams. ","0b4966d4":"#### Text Features\n\nThe data from Kaggle was squeaky clean so there's very little in terms of real \"pre\" processing. We removed contractions earlier and that's all we're going to do.\n\nFor data processing, I'm going to fit an [NLTK tokenzier](https:\/\/www.nltk.org\/api\/nltk.tokenize.html) object on the phrases, use that tokenzier to convert them to integer sequences of their word index and then pad the sequences so they'll all be the same length.","539cf8ce":"We can randomly sample our data to look at the original phrase, the transformed phrase and the padded sequence version of it."}}