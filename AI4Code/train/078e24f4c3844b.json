{"cell_type":{"ee1ecda3":"code","1ad60e50":"code","bee5a71f":"code","5dd3f278":"code","15dc8f89":"code","b614ae55":"code","f20d334f":"code","3816cade":"code","24e3cf63":"code","1934fb07":"code","7e1bd930":"code","d9f1f1db":"markdown","f26adb1e":"markdown","84953d25":"markdown","07c4043d":"markdown","b6ce1b3a":"markdown","0cd694a7":"markdown","23de56e7":"markdown","8e9d1735":"markdown","636afcde":"markdown","60f7b282":"markdown"},"source":{"ee1ecda3":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom category_encoders.woe import WOEEncoder\nfrom sklearn.compose import ColumnTransformer","1ad60e50":"train_transactions = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_transaction.csv')\ntrain_identity = pd.read_csv('\/kaggle\/input\/ieee-fraud-detection\/train_identity.csv')","bee5a71f":"# merge two datasets\ntrain = pd.merge(train_transactions, train_identity, on='TransactionID', how='left')\ntrain['isFraud'].value_counts(normalize=True)","5dd3f278":"train_transactions.head()","15dc8f89":"# Make an hour feature from datetime stamp (source:https:\/\/www.kaggle.com\/ajaykgp12\/ieee-cis-fraud-detection-lgb-with-fe#Categorical-Columns)\ndef make_hour_feature(f):\n    #Creates an hour of the day feature, encoded as 0-23.  \n    hours = f \/ (3600)        \n    encoded_hours = np.floor(hours) % 24\n    return encoded_hours\n\ntrain['hour'] = make_hour_feature(train['TransactionDT'])","b614ae55":"cat_features = ['ProductCD', 'card1', 'card2', 'card3', 'card4', 'card5', 'card6', \n               'addr1', 'addr2', 'P_emaildomain', 'R_emaildomain', 'M1',\n               'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9', 'DeviceType', 'DeviceInfo',\n               'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20',\n               'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30',\n               'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38']\n\nexclude = ['TransactionID', 'TransactionDT', 'isFraud']\nnum_features = [f for f in train.columns if (f not in cat_features) & (f not in exclude)]\n\n# drop more than 90% NAs\ncol_na = train.isna().sum()\nto_drop = col_na[(col_na \/ train.shape[0]) > 0.9].index\n\nuse_cols = [f for f in train.columns if f not in to_drop]\ncat_features = [f for f in cat_features if f not in to_drop]\nnum_features = [f for f in num_features if f not in to_drop]\n\ntrain[cat_features] = train[cat_features].astype(str)\ntrain[num_features] = train[num_features].astype(np.float)\ntrain = train[use_cols]","f20d334f":"# fill numeric NAs with median\nmedian_values = train[num_features].median() \ntrain[num_features] = train[num_features].fillna(median_values)\n\n# fill categorical NAs with \"missing\"\ntrain[cat_features] = train[cat_features].replace(\"nan\", \"missing\")\n\ntrain.isna().sum().sum()","3816cade":"data = train.drop(columns=['TransactionID', 'TransactionDT'])\n\ntarget = 'isFraud'\nnum_features = data.select_dtypes(include=np.number).columns\ncat_features = data.select_dtypes(exclude=np.number).columns\n\nnum_features = [f for f in num_features if f != target]","24e3cf63":"train_X, test_X, train_y, test_y = train_test_split(data[num_features+list(cat_features)], data['isFraud'], test_size=0.2)\ntrain_X, val_X, train_y, val_y = train_test_split(train_X, train_y, test_size=0.2)\nprint(len(train_X), 'train examples')\nprint(len(val_X), 'validation examples')\nprint(len(test_X), 'test examples')","1934fb07":"to_ohe=[]\nto_emb=[]\nfor c in cat_features:\n    if train_X[c].nunique() < 5:\n        to_ohe.append(c)\n    else:\n        to_emb.append(c)","7e1bd930":"# Numeric columns will be scaled by StandardScaler\nscaler = StandardScaler()\n\n# Categorical with < 5 unique values will be One Hot Encoded\nohe = OneHotEncoder(handle_unknown='ignore')\n\n# Categorical with >= 5 unique values will be encoded using Weight of Evidence\nwoe = WOEEncoder()\n\ncolumn_trans = ColumnTransformer(\n    [ ('scaler',scaler, num_features),\n    ('ohe', ohe, to_ohe),\n    ('woe', woe, to_emb)], remainder='passthrough', n_jobs=-1)\n\ntrain_X_transformed = column_trans.fit_transform(train_X, train_y)\nval_X_transformed = column_trans.transform(val_X)\ntest_X_transformed = column_trans.transform(test_X)\n\nprint(train_X_transformed.shape, val_X_transformed.shape, test_X_transformed.shape)","d9f1f1db":"The goal of this notebook is to provide a minimal working example of how you can pre-process your data for this problem. It's meant to be used as a starting point and was created for the reproducability sake for the blog that I'm writing.","f26adb1e":"### CategoricalEncoding","84953d25":"### Split Train\/Val\/Test","07c4043d":"## Preprocess","b6ce1b3a":"### Fill NAs","0cd694a7":"TODO: Add more","23de56e7":"## Load Data","8e9d1735":"### Data Types","636afcde":"### Feature Engineering","60f7b282":"Now you can use this data for any type of modelling."}}