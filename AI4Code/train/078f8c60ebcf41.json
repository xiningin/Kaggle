{"cell_type":{"e7c87b9f":"code","fda50b57":"code","4417ce4d":"code","7a7e428f":"code","39bc8860":"code","c9dc5142":"code","c69da8a3":"code","07fad6df":"code","7bcbb0c4":"code","5c3df771":"code","1eec114f":"code","ca1aac10":"code","778c5a58":"code","42eb3b30":"code","3c5ce0b8":"code","0c8a3db5":"code","eec1eab9":"code","56fdcb42":"code","5926f147":"code","40a27bea":"code","0e14dcdb":"code","d6d0b5c9":"code","35538d3b":"code","94537c3e":"code","a04e6219":"code","cfcffee9":"code","cde43252":"code","d9574d1e":"code","21cd224e":"code","e2e8cc05":"code","26aef808":"code","32ed8ea9":"code","3e8263f5":"code","111812b8":"code","eac68922":"code","b91e2510":"code","baa8e08a":"markdown","2010ae44":"markdown","3493aab8":"markdown","85b1150c":"markdown","1be80a49":"markdown","41ba8c8c":"markdown","3824edcc":"markdown","a0e8849a":"markdown","6d5902a7":"markdown","8aea9658":"markdown","ca451dc4":"markdown","1f52bc3c":"markdown","fb76c800":"markdown","0e0db54c":"markdown","d7385db4":"markdown"},"source":{"e7c87b9f":"import numpy as np\nimport pandas as pd\n\nfrom random import randint\n\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-white')\nimport seaborn as sns\nsns.set_style(\"white\")\n\nfrom sklearn.model_selection import train_test_split\n\nfrom skimage.transform import resize\n\nfrom keras.preprocessing.image import load_img\nfrom keras import Model\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\nfrom keras.models import load_model\nfrom keras.optimizers import Adam\nfrom keras.utils.vis_utils import plot_model\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\nfrom keras.layers import Conv2D, Concatenate, MaxPooling2D\nfrom keras.layers import UpSampling2D, Dropout, BatchNormalization\nfrom tqdm import tqdm_notebook\nfrom keras.losses import binary_crossentropy\nfrom keras import backend as K\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Activation","fda50b57":"img_size_ori = 101\nimg_size_target = 128\n\ndef upsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_target, img_size_target), mode='constant', preserve_range=True)\n    #res = np.zeros((img_size_target, img_size_target), dtype=img.dtype)\n    #res[:img_size_ori, :img_size_ori] = img\n    #return res\n    \ndef downsample(img):\n    if img_size_ori == img_size_target:\n        return img\n    return resize(img, (img_size_ori, img_size_ori), mode='constant', preserve_range=True)\n    #return img[:img_size_ori, :img_size_ori]","4417ce4d":"def dice_coef(y_true, y_pred):\n    y_true_f = K.flatten(y_true)\n    y_pred = K.cast(y_pred, 'float32')\n    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n    intersection = y_true_f * y_pred_f\n    score = 2. * K.sum(intersection) \/ (K.sum(y_true_f) + K.sum(y_pred_f))\n    return score\n\ndef dice_loss(y_true, y_pred):\n    smooth = 1.\n    y_true_f = K.flatten(y_true)\n    y_pred_f = K.flatten(y_pred)\n    intersection = y_true_f * y_pred_f\n    score = (2. * K.sum(intersection) + smooth) \/ (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n    return 1. - score\n\nfrom keras import backend as K\n\ndef bce_dice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, K.sigmoid(y_pred)) + dice_loss(y_true, K.sigmoid(y_pred))\n\ndef bce_logdice_loss(y_true, y_pred):\n    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n\ndef weighted_bce_loss(y_true, y_pred, weight):\n    epsilon = 1e-7\n    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n    logit_y_pred = K.log(y_pred \/ (1. - y_pred))\n    loss = weight * (logit_y_pred * (1. - y_true) + \n                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n    return K.sum(loss) \/ K.sum(weight)\n\ndef weighted_dice_loss(y_true, y_pred, weight):\n    smooth = 1.\n    w, m1, m2 = weight, y_true, y_pred\n    intersection = (m1 * m2)\n    score = (2. * K.sum(w * intersection) + smooth) \/ (K.sum(w * m1) + K.sum(w * m2) + smooth)\n    loss = 1. - K.sum(score)\n    return loss\n\ndef weighted_bce_dice_loss(y_true, y_pred):\n    y_true = K.cast(y_true, 'float32')\n    y_pred = K.cast(y_pred, 'float32')\n    # if we want to get same size of output, kernel size must be odd\n    averaged_mask = K.pool2d(\n            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n    weight = K.ones_like(averaged_mask)\n    w0 = K.sum(weight)\n    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n    w1 = K.sum(weight)\n    weight *= (w0 \/ w1)\n    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n    return loss\n\n\n\"\"\"\nLovasz-Softmax and Jaccard hinge loss in Tensorflow\nMaxim Berman 2018 ESAT-PSI KU Leuven (MIT License)\n\"\"\"\n\nfrom __future__ import print_function, division\n\nimport tensorflow as tf\nimport numpy as np\n\ndef lovasz_grad(gt_sorted):\n    \"\"\"\n    Computes gradient of the Lovasz extension w.r.t sorted errors\n    See Alg. 1 in paper\n    \"\"\"\n    gts = tf.reduce_sum(gt_sorted)\n    intersection = gts - tf.cumsum(gt_sorted)\n    union = gts + tf.cumsum(1. - gt_sorted)\n    jaccard = 1. - intersection \/ union\n    jaccard = tf.concat((jaccard[0:1], jaccard[1:] - jaccard[:-1]), 0)\n    return jaccard\n\n\n# --------------------------- BINARY LOSSES ---------------------------\n\n\ndef lovasz_hinge(logits, labels, per_image=True, ignore=None):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n      labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n      per_image: compute the loss per image instead of per batch\n      ignore: void class id\n    \"\"\"\n    if per_image:\n        def treat_image(log_lab):\n            log, lab = log_lab\n            log, lab = tf.expand_dims(log, 0), tf.expand_dims(lab, 0)\n            log, lab = flatten_binary_scores(log, lab, ignore)\n            return lovasz_hinge_flat(log, lab)\n        losses = tf.map_fn(treat_image, (logits, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n    return loss\n\n\ndef lovasz_hinge_flat(logits, labels):\n    \"\"\"\n    Binary Lovasz hinge loss\n      logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n      labels: [P] Tensor, binary ground truth labels (0 or 1)\n      ignore: label to ignore\n    \"\"\"\n\n    def compute_loss():\n        labelsf = tf.cast(labels, logits.dtype)\n        signs = 2. * labelsf - 1.\n        errors = 1. - logits * tf.stop_gradient(signs)\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort\")\n        gt_sorted = tf.gather(labelsf, perm)\n        grad = lovasz_grad(gt_sorted)\n        loss = tf.tensordot(tf.nn.relu(errors_sorted), tf.stop_gradient(grad), 1, name=\"loss_non_void\")\n        return loss\n\n    # deal with the void prediction case (only void pixels)\n    loss = tf.cond(tf.equal(tf.shape(logits)[0], 0),\n                   lambda: tf.reduce_sum(logits) * 0.,\n                   compute_loss,\n                   strict=True,\n                   name=\"loss\"\n                   )\n    return loss\n\n\ndef flatten_binary_scores(scores, labels, ignore=None):\n    \"\"\"\n    Flattens predictions in the batch (binary case)\n    Remove labels equal to 'ignore'\n    \"\"\"\n    scores = tf.reshape(scores, (-1,))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return scores, labels\n    valid = tf.not_equal(labels, ignore)\n    vscores = tf.boolean_mask(scores, valid, name='valid_scores')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vscores, vlabels\n\n\n# --------------------------- MULTICLASS LOSSES ---------------------------\n\n\ndef lovasz_softmax(probas, labels, classes='all', per_image=False, ignore=None, order='BHWC'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [B, H, W, C] or [B, C, H, W] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [B, H, W] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n      per_image: compute the loss per image instead of per batch\n      ignore: void class labels\n      order: use BHWC or BCHW\n    \"\"\"\n    if per_image:\n        def treat_image(prob_lab):\n            prob, lab = prob_lab\n            prob, lab = tf.expand_dims(prob, 0), tf.expand_dims(lab, 0)\n            prob, lab = flatten_probas(prob, lab, ignore, order)\n            return lovasz_softmax_flat(prob, lab, classes=classes)\n        losses = tf.map_fn(treat_image, (probas, labels), dtype=tf.float32)\n        loss = tf.reduce_mean(losses)\n    else:\n        loss = lovasz_softmax_flat(*flatten_probas(probas, labels, ignore, order), classes=classes)\n    return loss\n\n\ndef lovasz_softmax_flat(probas, labels, classes='all'):\n    \"\"\"\n    Multi-class Lovasz-Softmax loss\n      probas: [P, C] Variable, class probabilities at each prediction (between 0 and 1)\n      labels: [P] Tensor, ground truth labels (between 0 and C - 1)\n      classes: 'all' for all, 'present' for classes present in labels, or a list of classes to average.\n    \"\"\"\n    C = 1\n    losses = []\n    present = []\n    class_to_sum = list(range(C)) if classes in ['all', 'present'] else classes\n    for c in class_to_sum:\n        fg = tf.cast(tf.equal(labels, c), probas.dtype)  # foreground for class c\n        if classes == 'present':\n            present.append(tf.reduce_sum(fg) > 0)\n        errors = tf.abs(fg - probas[:, c])\n        errors_sorted, perm = tf.nn.top_k(errors, k=tf.shape(errors)[0], name=\"descending_sort_{}\".format(c))\n        fg_sorted = tf.gather(fg, perm)\n        grad = lovasz_grad(fg_sorted)\n        losses.append(\n            tf.tensordot(errors_sorted, tf.stop_gradient(grad), 1, name=\"loss_class_{}\".format(c))\n                      )\n    if len(class_to_sum) == 1:  # short-circuit mean when only one class\n        return losses[0]\n    losses_tensor = tf.stack(losses)\n    if classes == 'present':\n        present = tf.stack(present)\n        losses_tensor = tf.boolean_mask(losses_tensor, present)\n    loss = tf.reduce_mean(losses_tensor)\n    return loss\n\n\ndef flatten_probas(probas, labels, ignore=None, order='BHWC'):\n    \"\"\"\n    Flattens predictions in the batch\n    \"\"\"\n    if order == 'BCHW':\n        probas = tf.transpose(probas, (0, 2, 3, 1), name=\"BCHW_to_BHWC\")\n        order = 'BHWC'\n    if order != 'BHWC':\n        raise NotImplementedError('Order {} unknown'.format(order))\n    C = 1\n    probas = tf.reshape(probas, (-1, C))\n    labels = tf.reshape(labels, (-1,))\n    if ignore is None:\n        return probas, labels\n    valid = tf.not_equal(labels, ignore)\n    vprobas = tf.boolean_mask(probas, valid, name='valid_probas')\n    vlabels = tf.boolean_mask(labels, valid, name='valid_labels')\n    return vprobas, vlabels\n\ndef keras_lovasz_softmax(labels,probas):\n    #return lovasz_softmax(probas, labels)+binary_crossentropy(labels, probas)\n    return lovasz_softmax(probas, labels)\n\ndef keras_lovasz_hinge(labels,logits):\n    return lovasz_hinge(logits, labels, per_image=True, ignore=None)\n\ndef mean_iou(y_true, y_pred):\n    prec = []\n    for t in np.arange(0.5, 1.0, 0.05):\n        y_pred_ = tf.to_int32(y_pred > t)\n        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n        K.get_session().run(tf.local_variables_initializer())\n        with tf.control_dependencies([up_opt]):\n            score = tf.identity(score)\n        prec.append(score)\n    return K.mean(K.stack(prec), axis=0)\n\ndef focal_loss(y_true, y_pred):\n    gamma=0.75\n    alpha=0.25\n    pt_1 = tf.where(tf.equal(y_true, 1), y_pred, tf.ones_like(y_pred))\n    pt_0 = tf.where(tf.equal(y_true, 0), y_pred, tf.zeros_like(y_pred))\n\n    pt_1 = K.clip(pt_1, 1e-3, .999)\n    pt_0 = K.clip(pt_0, 1e-3, .999)\n\n    return -K.sum(alpha * K.pow(1. - pt_1, gamma) * K.log(pt_1))-K.sum((1-alpha) * K.pow( pt_0, gamma) * K.log(1. - pt_0))\n","7a7e428f":"train_df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"id\", usecols=[0])\ndepths_df = pd.read_csv(\"..\/input\/depths.csv\", index_col=\"id\")\ntrain_df = train_df.join(depths_df)\ntest_df = depths_df[~depths_df.index.isin(train_df.index)]","39bc8860":"train_df[\"images\"] = [np.array(load_img(\"..\/input\/train\/images\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","c9dc5142":"train_df[\"masks\"] = [np.array(load_img(\"..\/input\/train\/masks\/{}.png\".format(idx), grayscale=True)) \/ 255 for idx in tqdm_notebook(train_df.index)]","c69da8a3":"train_df[\"coverage\"] = train_df.masks.map(np.sum) \/ pow(img_size_ori, 2)","07fad6df":"def cov_to_class(val):    \n    for i in range(0, 11):\n        if val * 10 <= i :\n            return i\n        \ntrain_df[\"coverage_class\"] = train_df.coverage.map(cov_to_class)","7bcbb0c4":"fig, axs = plt.subplots(1, 2, figsize=(15,5))\nsns.distplot(train_df.coverage, kde=False, ax=axs[0])\nsns.distplot(train_df.coverage_class, bins=10, kde=False, ax=axs[1])\nplt.suptitle(\"Salt coverage\")\naxs[0].set_xlabel(\"Coverage\")\naxs[1].set_xlabel(\"Coverage class\")","5c3df771":"plt.scatter(train_df.coverage, train_df.coverage_class)\nplt.xlabel(\"Coverage\")\nplt.ylabel(\"Coverage class\")","1eec114f":"sns.distplot(train_df.z, label=\"Train\")\nsns.distplot(test_df.z, label=\"Test\")\nplt.legend()\nplt.title(\"Depth distribution\")","ca1aac10":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(train_df.index[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","778c5a58":"ids_train, ids_valid, x_train, x_valid, y_train, y_valid, cov_train, cov_test, depth_train, depth_test = train_test_split(\n    train_df.index.values,\n    np.array(train_df.images.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    np.array(train_df.masks.map(upsample).tolist()).reshape(-1, img_size_target, img_size_target, 1), \n    train_df.coverage.values,\n    train_df.z.values,\n    test_size=0.2, stratify=train_df.coverage_class, random_state=1337)","42eb3b30":"tmp_img = np.zeros((img_size_target, img_size_target), dtype=train_df.images.loc[ids_train[10]].dtype)\ntmp_img[:img_size_ori, :img_size_ori] = train_df.images.loc[ids_train[10]]\nfix, axs = plt.subplots(1, 2, figsize=(15,5))\naxs[0].imshow(tmp_img, cmap=\"Greys\")\naxs[0].set_title(\"Original image\")\naxs[1].imshow(x_train[10].squeeze(), cmap=\"Greys\")\naxs[1].set_title(\"Scaled image\")","3c5ce0b8":"def conv_block(m, dim, acti, bn, res, do=0):\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(m)\n\tn = BatchNormalization()(n) if bn else n\n\tn = Dropout(do)(n) if do else n\n\tn = Conv2D(dim, 3, activation=acti, padding='same')(n)\n\tn = BatchNormalization()(n) if bn else n\n\treturn Concatenate()([m, n]) if res else n\n\ndef level_block(m, dim, depth, inc, acti, do, bn, mp, up, res):\n\tif depth > 0:\n\t\tn = conv_block(m, dim, acti, bn, res)\n\t\tm = MaxPooling2D()(n) if mp else Conv2D(dim, 3, strides=2, padding='same')(n)\n\t\tm = level_block(m, int(inc*dim), depth-1, inc, acti, do, bn, mp, up, res)\n\t\tif up:\n\t\t\tm = UpSampling2D()(m)\n\t\t\tm = Conv2D(dim, 2, activation=acti, padding='same')(m)\n\t\telse:\n\t\t\tm = Conv2DTranspose(dim, 3, strides=2, activation=acti, padding='same')(m)\n\t\tn = Concatenate()([n, m])\n\t\tm = conv_block(n, dim, acti, bn, res)\n\telse:\n\t\tm = conv_block(m, dim, acti, bn, res, do)\n\treturn m\n\ndef UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n\ti = Input(shape=img_shape)\n\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n\to = Conv2D(out_ch, 1)(o)\n\to = Activation(\"sigmoid\")(o)\n\treturn Model(inputs=i, outputs=o)\n\ndef Logit_UNet(img_shape, out_ch=1, start_ch=64, depth=4, inc_rate=2., activation='relu', \n\t\t dropout=0.5, batchnorm=False, maxpool=True, upconv=True, residual=False):\n\ti = Input(shape=img_shape)\n\to = level_block(i, start_ch, depth, inc_rate, activation, dropout, batchnorm, maxpool, upconv, residual)\n\to = Conv2D(out_ch, 1)(o)\n\treturn Model(inputs=i, outputs=o)","0c8a3db5":"model = Logit_UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)\n#model = Logit_UNet((img_size_target,img_size_target,1),start_ch=16,depth=5,batchnorm=True)","eec1eab9":"#model.compile(loss=keras_lovasz_hinge, optimizer=\"sgd\", metrics=[\"accuracy\",mean_iou])\nmodel.compile(loss=bce_dice_loss, optimizer=\"adam\", metrics=[\"accuracy\",mean_iou])\n#model.compile(loss=focal_loss, optimizer=\"sgd\", metrics=[\"accuracy\",mean_iou])","56fdcb42":"model.summary()","5926f147":"#x_train = np.append(x_train, [np.fliplr(x) for x in x_train], axis=0)\n#y_train = np.append(y_train, [np.fliplr(x) for x in y_train], axis=0)\ntrain_gen = ImageDataGenerator(\n    horizontal_flip=True,\n    shear_range=0)\nval_gen = ImageDataGenerator(\n    horizontal_flip=True,\n    shear_range=0)","40a27bea":"fig, axs = plt.subplots(2, 10, figsize=(15,3))\nfor i in range(10):\n    axs[0][i].imshow(x_train[i].squeeze(), cmap=\"Greys\")\n    axs[0][i].imshow(y_train[i].squeeze(), cmap=\"Greens\", alpha=0.3)\n    axs[1][i].imshow(x_train[int(len(x_train)\/2 + i)].squeeze(), cmap=\"Greys\")\n    axs[1][i].imshow(y_train[int(len(y_train)\/2 + i)].squeeze(), cmap=\"Greens\", alpha=0.3)\nfig.suptitle(\"Top row: original images, bottom row: augmented images\")","0e14dcdb":"early_stopping = EarlyStopping(patience=10, verbose=1)\nmodel_checkpoint = ModelCheckpoint(\".\/keras.model\", save_best_only=True, verbose=1)\nreduce_lr = ReduceLROnPlateau(factor=0.5, patience=5, min_lr=0.00001, verbose=1)\n\nepochs = 20\nbatch_size = 128\n\nhistory1 = model.fit_generator(train_gen.flow(x_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=len(x_train) \/ 32, \n                    epochs=epochs,\n                    validation_data=val_gen.flow(x_valid, y_valid, batch_size=batch_size),\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)\n\nmodel.compile(loss=focal_loss, optimizer=\"sgd\", metrics=[\"accuracy\",mean_iou])\n\nepochs = 80\nbatch_size = 128\n\nhistory2 = model.fit_generator(train_gen.flow(x_train, y_train, batch_size=batch_size),\n                    steps_per_epoch=len(x_train) \/ 32, \n                    epochs=epochs,\n                    validation_data=val_gen.flow(x_valid, y_valid, batch_size=batch_size),\n                    callbacks=[early_stopping, model_checkpoint, reduce_lr],shuffle=True)","d6d0b5c9":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history1.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history1.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history1.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history1.history[\"val_acc\"], label=\"Validation accuracy\")","35538d3b":"fig, (ax_loss, ax_acc) = plt.subplots(1, 2, figsize=(15,5))\nax_loss.plot(history.epoch, history2.history[\"loss\"], label=\"Train loss\")\nax_loss.plot(history.epoch, history2.history[\"val_loss\"], label=\"Validation loss\")\nax_acc.plot(history.epoch, history2.history[\"acc\"], label=\"Train accuracy\")\nax_acc.plot(history.epoch, history2.history[\"val_acc\"], label=\"Validation accuracy\")","94537c3e":"model.load_weights(\".\/keras.model\")","a04e6219":"preds_valid = model.predict(x_valid).reshape(-1, img_size_target, img_size_target)\npreds_valid = np.array([downsample(x) for x in preds_valid])\ny_valid_ori = np.array([train_df.loc[idx].masks for idx in ids_valid])","cfcffee9":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(pred, alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","cde43252":"# src: https:\/\/www.kaggle.com\/aglotero\/another-iou-metric\ndef iou_metric(y_true_in, y_pred_in, print_table=False):\n    labels = y_true_in\n    y_pred = y_pred_in\n    \n    true_objects = 2\n    pred_objects = 2\n\n    intersection = np.histogram2d(labels.flatten(), y_pred.flatten(), bins=(true_objects, pred_objects))[0]\n\n    # Compute areas (needed for finding the union between all objects)\n    area_true = np.histogram(labels, bins = true_objects)[0]\n    area_pred = np.histogram(y_pred, bins = pred_objects)[0]\n    area_true = np.expand_dims(area_true, -1)\n    area_pred = np.expand_dims(area_pred, 0)\n\n    # Compute union\n    union = area_true + area_pred - intersection\n\n    # Exclude background from the analysis\n    intersection = intersection[1:,1:]\n    union = union[1:,1:]\n    union[union == 0] = 1e-9\n\n    # Compute the intersection over union\n    iou = intersection \/ union\n\n    # Precision helper function\n    def precision_at(threshold, iou):\n        matches = iou > threshold\n        true_positives = np.sum(matches, axis=1) == 1   # Correct objects\n        false_positives = np.sum(matches, axis=0) == 0  # Missed objects\n        false_negatives = np.sum(matches, axis=1) == 0  # Extra objects\n        tp, fp, fn = np.sum(true_positives), np.sum(false_positives), np.sum(false_negatives)\n        return tp, fp, fn\n\n    # Loop over IoU thresholds\n    prec = []\n    if print_table:\n        print(\"Thresh\\tTP\\tFP\\tFN\\tPrec.\")\n    for t in np.arange(0.5, 1.0, 0.05):\n        tp, fp, fn = precision_at(t, iou)\n        if (tp + fp + fn) > 0:\n            p = tp \/ (tp + fp + fn)\n        else:\n            p = 0\n        if print_table:\n            print(\"{:1.3f}\\t{}\\t{}\\t{}\\t{:1.3f}\".format(t, tp, fp, fn, p))\n        prec.append(p)\n    \n    if print_table:\n        print(\"AP\\t-\\t-\\t-\\t{:1.3f}\".format(np.mean(prec)))\n    return np.mean(prec)\n\ndef iou_metric_batch(y_true_in, y_pred_in):\n    batch_size = y_true_in.shape[0]\n    metric = []\n    for batch in range(batch_size):\n        value = iou_metric(y_true_in[batch], y_pred_in[batch])\n        metric.append(value)\n    return np.mean(metric)","d9574d1e":"thresholds = np.linspace(0, 1, 50)\nious = np.array([iou_metric_batch(y_valid_ori, np.int32(preds_valid > threshold)) for threshold in tqdm_notebook(thresholds)])","21cd224e":"threshold_best_index = np.argmax(ious[9:-10]) + 9\niou_best = ious[threshold_best_index]\nthreshold_best = thresholds[threshold_best_index]","e2e8cc05":"plt.plot(thresholds, ious)\nplt.plot(threshold_best, iou_best, \"xr\", label=\"Best threshold\")\nplt.xlabel(\"Threshold\")\nplt.ylabel(\"IoU\")\nplt.title(\"Threshold vs IoU ({}, {})\".format(threshold_best, iou_best))\nplt.legend()","26aef808":"max_images = 60\ngrid_width = 15\ngrid_height = int(max_images \/ grid_width)\nfig, axs = plt.subplots(grid_height, grid_width, figsize=(grid_width, grid_height))\nfor i, idx in enumerate(ids_valid[:max_images]):\n    img = train_df.loc[idx].images\n    mask = train_df.loc[idx].masks\n    pred = preds_valid[i]\n    ax = axs[int(i \/ grid_width), i % grid_width]\n    ax.imshow(img, cmap=\"Greys\")\n    ax.imshow(mask, alpha=0.3, cmap=\"Greens\")\n    ax.imshow(np.array(np.round(pred > threshold_best), dtype=np.float32), alpha=0.3, cmap=\"OrRd\")\n    ax.text(1, img_size_ori-1, train_df.loc[idx].z, color=\"black\")\n    ax.text(img_size_ori - 1, 1, round(train_df.loc[idx].coverage, 2), color=\"black\", ha=\"right\", va=\"top\")\n    ax.text(1, 1, train_df.loc[idx].coverage_class, color=\"black\", ha=\"left\", va=\"top\")\n    ax.set_yticklabels([])\n    ax.set_xticklabels([])\nplt.suptitle(\"Green: salt, Red: prediction. Top-left: coverage class, top-right: salt coverage, bottom-left: depth\")","32ed8ea9":"# Source https:\/\/www.kaggle.com\/bguberfain\/unet-with-depth\ndef RLenc(img, order='F', format=True):\n    \"\"\"\n    img is binary mask image, shape (r,c)\n    order is down-then-right, i.e. Fortran\n    format determines if the order needs to be preformatted (according to submission rules) or not\n\n    returns run length as an array or string (if format is True)\n    \"\"\"\n    bytes = img.reshape(img.shape[0] * img.shape[1], order=order)\n    runs = []  ## list of run lengths\n    r = 0  ## the current run length\n    pos = 1  ## count starts from 1 per WK\n    for c in bytes:\n        if (c == 0):\n            if r != 0:\n                runs.append((pos, r))\n                pos += r\n                r = 0\n            pos += 1\n        else:\n            r += 1\n\n    # if last run is unsaved (i.e. data ends with 1)\n    if r != 0:\n        runs.append((pos, r))\n        pos += r\n        r = 0\n\n    if format:\n        z = ''\n\n        for rr in runs:\n            z += '{} {} '.format(rr[0], rr[1])\n        return z[:-1]\n    else:\n        return runs","3e8263f5":"x_test = np.array([upsample(np.array(load_img(\"..\/input\/test\/images\/{}.png\".format(idx), grayscale=True))) \/ 255 for idx in tqdm_notebook(test_df.index)]).reshape(-1, img_size_target, img_size_target, 1)","111812b8":"preds_test = model.predict(x_test)","eac68922":"pred_dict = {idx: RLenc(np.round(downsample(preds_test[i]) > threshold_best)) for i, idx in enumerate(tqdm_notebook(test_df.index.values))}","b91e2510":"sub = pd.DataFrame.from_dict(pred_dict,orient='index')\nsub.index.names = ['id']\nsub.columns = ['rle_mask']\nsub.to_csv('submission.csv')","baa8e08a":"# Loading of training\/testing ids and depths\nReading the training data and the depths, store them in a DataFrame. Also create a test DataFrame with entries from depth not in train.","2010ae44":"# Read images and masks\nLoad the images and masks into the DataFrame and divide the pixel values by 255.","3493aab8":"# Training","85b1150c":"# Predict the validation set to do a sanity check\nAgain plot some sample images including the predictions.","1be80a49":"# Plotting the depth distributions\nSeparatelty plotting the depth distributions for the training and the testing data.","41ba8c8c":"# Params and helpers","3824edcc":"Thank you guys for giving this much attention to something I wrote in about two hours ... Lovasz hinge is supposed to improve the score, only if there is a [\"gradient flow.\" ](https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\/discussion\/64645) (Shoutout to Heng for his amazing insights!) Unfortunately, I'm not investigating into better achitectures for trained-from-scratch U-Nets (because pretrained encoders works better), so you may use this script as a baseline for your TTA testing. \n\nSincerely, Alexander Liao\n## Tips\n- All losses are provided below, yet the model is only compiled with one. Change loss at your discretion.\n- Hyperparameters are not tuned\n- Focal Loss is unstable, fine-tuning is definitely needed\n\n## Original Message\n-Add Batch Normalization layer after each conv\n-Add shuffle=True in model.fit() method for a better BN effect (so that we have different batch to normalize in each epoch during the training)\n-You can use crf method (https:\/\/www.kaggle.com\/meaninglesslives\/apply-crf) to improve the result \n## Changelog\n- Changed uncov to uconv, but removed the dropout in the last layer\n- Corrected sanity check of predicted validation data (changed from ids_train to ids_valid)\n- Used correct mask (from original train_df) for threshold tuning (inserted y_valid_ori)\n- Added DICE loss functions\n- Added Lovasz Softmax loss functions and its keras wrappers\n- Added Focal loss\n- Added ImageDataGenerator for TTA","a0e8849a":"# Another sanity check with adjusted threshold\nAgain some sample images with the adjusted threshold.","6d5902a7":"# Show some example images","8aea9658":"# Submission\nLoad, predict and submit the test image predictions.","ca451dc4":"# Calculating the salt coverage and salt coverage classes\nCounting the number of salt pixels in the masks and dividing them by the image size. Also create 11 coverage classes, -0.1 having no salt at all to 1.0 being salt only.\nPlotting the distribution of coverages and coverage classes, and the class against the raw coverage.","1f52bc3c":"# Data augmentation","fb76c800":"# Scoring\nScore the model and do a threshold optimization by the best IoU.","0e0db54c":"# Build model","d7385db4":"# Create train\/validation split stratified by salt coverage\nUsing the salt coverage as a stratification criterion. Also show an image to check for correct upsampling."}}