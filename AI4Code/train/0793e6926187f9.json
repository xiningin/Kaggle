{"cell_type":{"b233b547":"code","83abc908":"code","718d6a79":"code","2a9bc076":"code","981853ae":"code","cf3664ae":"code","98704f84":"code","6ba38058":"code","46e9bc76":"code","6d941703":"code","8885aaff":"code","db6b0f3c":"code","8720ad81":"code","9dfbad6c":"code","47a829c2":"code","a54b3fe5":"code","c96de575":"code","27b3be2f":"code","ce74da58":"code","86692a0f":"code","ba65464c":"code","b5995d08":"code","501687d8":"code","77543ac1":"code","9c6bd272":"code","175bbbf3":"code","52c21ca3":"code","04d09994":"code","a426ef9d":"code","414558d8":"code","7687650e":"markdown","be2aa5ac":"markdown","00e15743":"markdown","08b85315":"markdown","592b09fd":"markdown"},"source":{"b233b547":"import matplotlib.pyplot as plt\nimport IPython.display as ipd\nimport librosa\nimport librosa.display as disp\nimport pandas as pd\nimport os\nimport numpy as np\nfrom tqdm import tqdm\nfrom sklearn.model_selection import train_test_split\nfrom pickle import load","83abc908":"filename = '..\/input\/urbansounds\/UrbanSound8K\/audio\/fold5\/100032-3-0-0.wav'","718d6a79":"plt.figure(figsize=(14,6))\ndata,sample_rate = librosa.load(filename)\ndisp.waveplot(data, sr=sample_rate)","2a9bc076":"ipd.Audio(filename)","981853ae":"sample_rate","cf3664ae":"metadata = pd.read_csv('..\/input\/urbansounds\/UrbanSound8K\/metadata\/UrbanSound8K.csv')","98704f84":"metadata.head()","6ba38058":"metadata[['classID','class']].value_counts()","46e9bc76":"mfccs = librosa.feature.mfcc(y=data, sr=sample_rate, n_mfcc=40)\nprint(mfccs.shape)","6d941703":"mfccs[10]","8885aaff":"# Extract All File\naudio_path = '..\/input\/urbansounds\/UrbanSound8K\/audio'\nmetadata = pd.read_csv('..\/input\/urbansounds\/UrbanSound8K\/metadata\/UrbanSound8K.csv')","db6b0f3c":"def features_extractor(file):\n    audio, sample_rate = librosa.load(file, res_type='kaiser_fast')\n    mfcss_features = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n    mfcss_scaled_features = np.mean(mfcss_features.T, axis=0)\n    \n    return mfcss_scaled_features","8720ad81":"extracted_features = []\nfor index_num,row in tqdm(metadata.iterrows()):\n    file_name = os.path.join(os.path.abspath(audio_path),'fold' + str(row['fold'])+'\/',str(row['slice_file_name']))\n    final_class_labels = row['class']\n    data = features_extractor(file_name)\n    extracted_features.append([data, final_class_labels])","9dfbad6c":"from pickle import dump\ndump(extracted_features, open('extracted_features.pkl', 'wb'))","47a829c2":"extracted_features = load(open('..\/input\/trainpkl\/extracted_features.pkl', 'rb'))","a54b3fe5":"extract_feature_df = pd.DataFrame(extracted_features, columns=['feature','class'])\nextract_feature_df.head()","c96de575":"# Split dataset to X(Independent) and Y(Dependent) dataset\nX = np.array(extract_feature_df['feature'].tolist())\ny = np.array(extract_feature_df['class'].tolist())","27b3be2f":"y = np.array(pd.get_dummies(y))","ce74da58":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=48)","86692a0f":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nfrom sklearn import metrics","ba65464c":"num_labels = y.shape[1]\nnum_labels","b5995d08":"classes = ['air_conditioner', \n           'car_horn', \n           'children_playing', \n           'dog_bark', 'drilling', \n           'engine_idling', \n           'gun_shot', \n           'jackhammer', \n           'siren', \n           'street_music']","501687d8":"model = Sequential()\nmodel.add(Dense(100, input_shape=(40,), activation='relu'))\nmodel.add(Dense(100, activation='relu'))# Input shape from n in n_mfcc\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(200, activation='relu'))\nmodel.add(Dropout(0.2))\n\nmodel.add(Dense(num_labels, activation='softmax'))","77543ac1":"model.summary()","9c6bd272":"model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])","175bbbf3":"num_epochs = 100\nnum_batch_size = 32\n\ncheckpoint = ModelCheckpoint(filepath='.\/model.h5', verbose=1, save_best_only=True)\n\nmodel.fit(X_train,y_train, batch_size = num_batch_size, epochs = num_epochs, validation_split=0.2, callbacks=[checkpoint])","52c21ca3":"model.evaluate(X_test,y_test)","04d09994":"file = metadata['slice_file_name'].sample(1).tolist()[0]\nfold = metadata[metadata['slice_file_name']== file]['fold'].tolist()[0]\nreal_class = metadata[metadata['slice_file_name']== file]['class'].tolist()[0]\ntest_file = f'..\/input\/urbansounds\/UrbanSound8K\/audio\/fold{str(fold)}\/{file}' \ntest_file_extract = features_extractor(test_file)\ntest_file_extract = test_file_extract.reshape(1,-1)\npred = np.argmax(model.predict(test_file_extract), axis=-1)\nresult = classes[pred.tolist()[0]]\nprint(f'Predict Class : {result}\\nReal Class : {real_class}')\nprint('Predict True' if result==real_class else 'Predict False')","a426ef9d":"ipd.Audio(test_file)","414558d8":"test_file_extract.shape","7687650e":"## Extract Features","be2aa5ac":"# Audio EDA","00e15743":"# Predict Data","08b85315":"# Audio Preprocessing","592b09fd":"# Create Model"}}