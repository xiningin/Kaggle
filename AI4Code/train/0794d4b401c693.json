{"cell_type":{"03a894ce":"code","564ce6b1":"code","943f90dd":"code","b3a0d980":"code","70309687":"code","f63dc631":"code","d2fd0eb0":"code","ddbac727":"code","e7360c08":"code","6c99451d":"code","8181684c":"code","2b252d8f":"code","03ef5e09":"code","9c7408ba":"code","19f8ef9e":"markdown","8e8590d7":"markdown","1f718d27":"markdown","5f5a31d9":"markdown","f9eb0edd":"markdown","df412ebd":"markdown","a838aee5":"markdown","a1214f59":"markdown"},"source":{"03a894ce":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport sklearn\nfrom sklearn import svm\nfrom sklearn import model_selection\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","564ce6b1":"#import train and test CSV files\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n#take a look at the training data\ntrain_data.describe(include=\"all\")","943f90dd":"# Just to get an idea of the dataset\nfeatures = train_data.columns\nprint(features)\n\n#printing 10 random passengers\ntrain_data.sample(10)","b3a0d980":"# Let's check how many passengers have some NaN values\n\nprint(pd.isnull(train_data).sum())","70309687":"print(train_data.shape, test_data.shape)\n\ntrain_data = train_data.drop([\"Cabin\"], axis = 1)\ntest_data = test_data.drop([\"Cabin\"], axis = 1)\n\ntrain_data = train_data.drop([\"Ticket\"], axis = 1)\ntest_data = test_data.drop([\"Ticket\"], axis = 1)\n\nprint(train_data.shape, test_data.shape)\n\nfeatures = features.drop([\"Survived\", \"Cabin\", \"Ticket\"])","f63dc631":"used_features = ['Pclass', 'Sex', 'Age', \"SibSp\"]\n\nfinal_train_data = pd.get_dummies(train_data[used_features])\nlabels = train_data[\"Survived\"]\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(final_train_data, labels, test_size = 0.20, random_state = 0)\n\nX_test = pd.get_dummies(test_data[used_features])\n\nprint(X_train.shape, y_train.shape, X_test.shape)\n\nX_train.sample(10)\n\n","d2fd0eb0":"age_mean_train = X_train.mean(axis = 0, skipna = True)[\"Age\"] \nage_mean_val = X_val.mean(axis = 0, skipna = True)[\"Age\"]\nage_mean_test =  X_test.mean(axis = 0, skipna = True)[\"Age\"] \n\nX_train[\"Age\"].fillna(age_mean_train, inplace = True) \nX_val[\"Age\"].fillna(age_mean_val, inplace = True) \nX_test[\"Age\"].fillna(age_mean_test, inplace = True) \n\n# Checking if NaN count of age feature is 0\nprint(pd.isnull(X_train).sum())\nprint(pd.isnull(X_val).sum())\n\ntraining_scores = []\nvalidation_scores = []","ddbac727":"svc = svm.SVC()\nparameters = {'kernel':['rbf'], 'C': [0.1, 0.01, 1, 10, 100], 'gamma':[0.01, 0.1,1.,10, 100]}\nsvm_clf = model_selection.GridSearchCV(svc, parameters, cv = 5, n_jobs = -1, verbose = True)\nsvm_clf.fit(X_train, y_train)\nfinal_svm = svm_clf.best_estimator_\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_train, final_svm.predict(X_train)))\nsvm_training_score = final_svm.score(X_train, y_train)\ntraining_scores.append(svm_training_score)\nprint(\"The score is %f !\" %(svm_training_score))","e7360c08":"from sklearn.linear_model import LogisticRegressionCV\n\nlog_reg = LogisticRegressionCV([0.1, 0.01, 1, 10, 100], max_iter = 10000, n_jobs = -1, cv=5, verbose = True).fit(X_train, y_train)\nprint(classification_report(y_train, log_reg.predict(X_train)))\n\nlog_reg_training_score = log_reg.score(X_train, y_train)\ntraining_scores.append(log_reg_training_score)\nprint(\"The score is %f !\" %(log_reg_training_score))","6c99451d":"from sklearn.neural_network import MLPClassifier\n\nparameters = {'hidden_layer_sizes': [(4,), (10,), (50,), (10,10,), (50,50,), (4,4), (10,10,10,10)]} # \"activation\" : ['identity', 'logistic', 'tanh', 'relu']}\nnn_cv = model_selection.GridSearchCV(MLPClassifier(max_iter=1000, random_state = 0), parameters, n_jobs = -1, cv = 5).fit(X_train, y_train)\n\nprint(nn_cv.cv_results_)\n\nprint(classification_report(y_train, nn_cv.predict(X_train)))\n\nnn_cv_training_score = nn_cv.score(X_train, y_train)\ntraining_scores.append(nn_cv_training_score)\nprint(\"The score is %f !\" %(nn_cv_training_score))","8181684c":"from sklearn.tree import DecisionTreeClassifier\nparameters = {\"criterion\": [\"gini\", \"entropy\"], 'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\ntree_cv = model_selection.GridSearchCV(DecisionTreeClassifier(), parameters,  cv = 5, n_jobs = -1, verbose = True).fit(X_train, y_train)\nprint(classification_report(y_train, tree_cv.predict(X_train)))\n\ntree_cv_training_score = tree_cv.score(X_train, y_train)\ntraining_scores.append(tree_cv_training_score)\nprint(\"The score is %f !\" %(tree_cv_training_score))","2b252d8f":"from sklearn.ensemble import RandomForestClassifier\nparameters = {\"criterion\": [\"gini\", \"entropy\"], 'max_leaf_nodes': list(range(2, 100)), 'min_samples_split': [2, 3, 4]}\nrand_forest_cv = model_selection.GridSearchCV(RandomForestClassifier(), parameters, n_jobs = -1, cv = 5, verbose = True).fit(X_train, y_train)\n\nprint(classification_report(y_train, rand_forest_cv.predict(X_train)))\n\nrand_forest_cv_training_score = rand_forest_cv.score(X_train, y_train)\ntraining_scores.append(rand_forest_cv_training_score)\nprint(\"The score is %f !\" %(rand_forest_cv_training_score))","03ef5e09":"validation_scores = [final_svm.score(X_val, y_val), log_reg.score(X_val, y_val), nn_cv.score(X_val, y_val), tree_cv.score(X_val, y_val), rand_forest_cv.score(X_val, y_val)]\n\nprint(\"Validation score of SVM Classifier: %f\" %(validation_scores[0]))\nprint(\"Validation score of Logistic Regression Classifier: %f\" %(validation_scores[1]))\nprint(\"Validation score of Neural Network Classifier: %f\" %(validation_scores[2]))\nprint(\"Validation score of Tree Classifier: %f\" %(validation_scores[3]))\nprint(\"Validation score of Random Forest Classifier: %f\" %(validation_scores[4]))\n\ndata = []\nfor element in list(zip(training_scores, validation_scores)):\n      data.append({\"Training score\": element[0], \"Validation score\": element[1]})\n      \nresults = pd.DataFrame(data, index = [\"SVM RBF\", \"Logistic Regression\", \"Neural Network\", \"Decision Tree\", \"Random Forest\"])\nprint(results)","9c7408ba":"predictions = rand_forest_cv.predict(X_test)\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","19f8ef9e":"*Cabin feature is almost always NaN, followed by age: let's delete cabin and try fixing age!*","8e8590d7":"*Let's fill the gaps in the age feature by using its average*","1f718d27":"**Let's try some models! Let's go with RBF-Kernel SVM**","5f5a31d9":"*Let's try some Decision trees*","f9eb0edd":"*Let's try with Logistic Regression*","df412ebd":"*Let's try with a Neural Network*","a838aee5":"**Sources**\n\n[Titanic Survival Predictions (Beginner)](https:\/\/www.kaggle.com\/ashish2070\/titanic-survival-predictions-beginner)","a1214f59":"*Now Random Forest: my first ensemlbed ml model*"}}