{"cell_type":{"0a77e931":"code","f25a55c5":"code","ab81f5f7":"code","f2e81f97":"code","eb82b528":"code","04ba6a8a":"code","a5af16fe":"code","6b35dd0b":"code","ea02eafe":"code","6d1b6e77":"code","47ee8500":"code","3c18bd85":"code","cf5f98db":"code","79e34943":"code","974c4564":"code","c2114f04":"code","18558235":"code","0d5fa350":"code","748645c4":"code","42b2328f":"code","b69580ab":"code","f30f59ce":"code","44ad5b7c":"code","56fa0760":"code","9e42d27f":"code","6e828203":"code","348321d3":"code","6fe0df4a":"markdown"},"source":{"0a77e931":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f25a55c5":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nimport pathlib\nimport treelite\nimport treelite_runtime \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nimport xgboost as xgb\nimport optuna   \nimport cudf\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ab81f5f7":"traincudf = cudf.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv', nrows=100000)\ntraincudf.info()","f2e81f97":"train = traincudf.to_pandas()","eb82b528":"del traincudf","04ba6a8a":"train = train[train['weight']!=0]","a5af16fe":"train['action'] = train['resp'].apply(lambda x:x>0).astype(int)","6b35dd0b":"#features = [col for col in list(train.columns) if 'feature' in col]","ea02eafe":"X = train.loc[:, train.columns.str.contains('feature')]\ny = train['action']","6d1b6e77":"# Import the necessary libraries first\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.linear_model import Lasso,LogisticRegression\nfrom sklearn.metrics import roc_auc_score,accuracy_score","47ee8500":"x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.2)","3c18bd85":"test_median = X.median()\n","cf5f98db":"train_median = x_train.median()\n# Impute medians in both training set and the hold-out validation set\nx_train = x_train.fillna(train_median)\nx_test = x_test.fillna(train_median)","79e34943":"# linear models benefit from feature scaling\nscaler=StandardScaler()\nscaler.fit(x_train)","974c4564":"train_trans = scaler.transform(x_train)","c2114f04":"from sklearn.model_selection import GridSearchCV\n","18558235":"from hyperopt import STATUS_OK, Trials, fmin, hp, tpe","0d5fa350":"space={'max_depth': hp.quniform(\"max_depth\", 3, 18, 1),\n        'gamma': hp.uniform ('gamma', 1,9),\n        'reg_alpha' : hp.quniform('reg_alpha', 40,180,1),\n        'reg_lambda' : hp.uniform('reg_lambda', 0,1),\n        'colsample_bytree' : hp.uniform('colsample_bytree', 0.5,1),\n        'min_child_weight' : hp.quniform('min_child_weight', 0, 10, 1),\n        'n_estimators': 180,\n        'seed': 0\n    }","748645c4":"def objective(space):\n    clf=xgb.XGBClassifier(\n                    n_estimators =space['n_estimators'], max_depth = int(space['max_depth']), gamma = space['gamma'],\n                    reg_alpha = int(space['reg_alpha']),min_child_weight=int(space['min_child_weight']),\n                    colsample_bytree=int(space['colsample_bytree']))\n    \n    evaluation = [( train_trans, y_train), ( scaler.transform(x_test), y_test)]\n    \n    clf.fit(train_trans, y_train,\n            eval_set=evaluation, eval_metric=\"auc\",\n            early_stopping_rounds=10,verbose=False)\n    \n\n    pred = clf.predict(scaler.transform(x_test))\n    accuracy = accuracy_score(y_test, pred>0.5)\n    print (\"SCORE:\", accuracy)\n    return {'loss': -accuracy, 'status': STATUS_OK }","42b2328f":"trials = Trials()\n\nbest_hyperparams = fmin(fn = objective,\n                        space = space,\n                        algo = tpe.suggest,\n                        max_evals = 100,\n                        trials = trials)","b69580ab":"print(\"The best hyperparameters are : \",\"\\n\")\nprint(best_hyperparams)","f30f59ce":"# Fit the XGBoost classifier with optimal hyperparameters\nclf = xgb.XGBClassifier(colsample_bytree = 0.871380814805137, gamma= 1.4630522178861056, max_depth = 9, min_child_weight = 4.0, reg_alpha = 50.0, reg_lambda= 0.8077808908136562)","44ad5b7c":"clf.fit(train_trans, y_train)  #Used the whole training data","56fa0760":"y_predict = clf.predict(scaler.transform(x_test))\nfrom sklearn.metrics import accuracy_score\nprint(accuracy_score(y_test,y_predict))","9e42d27f":"import janestreet\nenv = janestreet.make_env() # initialize the environment\niter_test = env.iter_test() # an iterator which loops over the test set","6e828203":"def fillna_npwhere(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array","348321d3":"for (test_df, sample_prediction_df) in iter_test:\n    X_test = test_df.loc[:, test_df.columns.str.contains('feature')]\n    x_test=fillna_npwhere(X_test.values,train_median)\n    y_preds = clf.predict(scaler.transform(x_test))\n    sample_prediction_df.action = y_preds\n    env.predict(sample_prediction_df)","6fe0df4a":"# features"}}