{"cell_type":{"a10582b3":"code","f662112b":"code","21ef612e":"code","a4434fdc":"code","2118b5fb":"code","33d31711":"code","e4858d34":"code","3fe32251":"code","f0e2ad51":"code","35d7ba74":"code","208f1559":"code","029bb7e0":"code","8142c05a":"code","dd524fe1":"code","11dcd359":"code","c5af8c5d":"code","69570b2c":"code","2d094855":"code","ccd93e01":"code","c8bc5d85":"code","981824f6":"code","10c728ab":"code","4bf5f494":"code","1564af8d":"code","ddaed797":"code","d811943f":"code","92f3d3fa":"code","f4952a8c":"code","bdc1cccd":"code","c7f98114":"code","98b0465b":"code","69089281":"code","dfca0168":"code","5039900a":"code","124c4d5c":"code","b899461a":"code","f2955c54":"code","a315a90c":"code","c37030e8":"code","df618006":"code","7da0cfc4":"code","9da48388":"markdown","4f483ce2":"markdown","d98e196a":"markdown","cbf64e00":"markdown","2cae96e1":"markdown","6906f764":"markdown","f52a3d8e":"markdown","30f18710":"markdown","73b54b4c":"markdown","12e83a98":"markdown","bf67b54e":"markdown","5beaf684":"markdown","0a19d59f":"markdown","8e1d4922":"markdown","6b248b23":"markdown","68e40100":"markdown","aa3cab50":"markdown","d6efe570":"markdown","7ebac31b":"markdown","af060032":"markdown","95c85c49":"markdown","77ebb3bf":"markdown","d14009f3":"markdown","c08486c1":"markdown"},"source":{"a10582b3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f662112b":"train_df = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/train.csv\")\ntest_df = pd.read_csv(\"\/kaggle\/input\/santander-customer-transaction-prediction\/test.csv\")","21ef612e":"train_df.head()","a4434fdc":"print(train_df.shape, test_df.shape)","2118b5fb":"print(train_df.isna().sum().sum(), test_df.isna().sum().sum())","33d31711":"train_df.describe()","e4858d34":"test_df.describe()","3fe32251":"sns.set(style=\"darkgrid\")\nsns.countplot(train_df['target'])","f0e2ad51":"train_df[\"target\"].value_counts()","35d7ba74":"sns.distplot(train_df[train_df.columns[2:]].mean(), kde=False)","208f1559":"train_df.info()","029bb7e0":"column_corr = train_df.corr()['target']","8142c05a":"column_corr","dd524fe1":"print(column_corr.sort_values().tail(11))","11dcd359":"print(column_corr.sort_values().head(10))","c5af8c5d":"X = train_df.iloc[:,2:202]\ny = train_df.iloc[:,1]","69570b2c":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.fit_transform(X)","2d094855":"from imblearn.over_sampling import SMOTE","ccd93e01":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)","c8bc5d85":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression(random_state=0,)\nlogreg.fit(X_train, y_train)","981824f6":"from sklearn.metrics import confusion_matrix\n\ny_pred = logreg.predict(X_test)\nconfusion_matrix = confusion_matrix(y_test, y_pred)\nprint(confusion_matrix)","10c728ab":"from sklearn.metrics import roc_auc_score\n\nroc_auc_score(y_test, y_pred)","4bf5f494":"logreg.score(X_test, y_test)","1564af8d":"y_pred = logreg.predict(test_df.drop(columns = ['ID_code']))","ddaed797":"from sklearn.metrics import roc_curve\nimport matplotlib.pyplot as plt\n\nproba = logreg.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","d811943f":"submission_logreg = pd.DataFrame({ \"ID_code\": test_df[\"ID_code\"], \"target\": y_pred })\nsubmission_logreg.to_csv('submission_logreg.csv', index=False)","92f3d3fa":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score","f4952a8c":"model = GaussianNB()\nmodel.fit(X_train, y_train)\npredicted= model.predict(X_test)\nprint(\"NBGaussian Accuracy :\", accuracy_score(y_test, predicted))","bdc1cccd":"roc_auc_score(y_test, predicted)","c7f98114":"proba = model.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","98b0465b":"from sklearn import tree\nTree = tree.DecisionTreeClassifier()\nTree = Tree.fit(X_train,y_train)","69089281":"predicted= Tree.predict(X_test)\nprint(\"Decision Tree Accuracy :\", accuracy_score(y_test, predicted))","dfca0168":"roc_auc_score(y_test, predicted)","5039900a":"proba = Tree.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","124c4d5c":"from sklearn.ensemble import RandomForestClassifier\nForest = RandomForestClassifier(n_estimators = 100)\nForest = Forest.fit(X_train,y_train)","b899461a":"predicted= Forest.predict(X_test)\nprint(\"Random Forest Accuracy :\", accuracy_score(y_test, predicted))","f2955c54":"roc_auc_score(y_test, predicted)","a315a90c":"proba = Forest.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","c37030e8":"import xgboost as xgb\nXGB_model = xgb.XGBClassifier()\n\nXGB_model = XGB_model.fit(X_train, y_train)\n\npredicted= XGB_model.predict(X_test)\n\nprint(\"XGBoost Accuracy :\", accuracy_score(y_test, predicted))\n\n","df618006":"roc_auc_score(y_test, predicted)","7da0cfc4":"proba = XGB_model.predict_proba(X_test)[:, 1]\nscore = roc_auc_score(y_test, proba)\nfpr, tpr, _  = roc_curve(y_test, proba)\n\nplt.figure()\nplt.plot(fpr, tpr, color='c', label=f\"ROC curve (auc = {score})\")\nplt.plot([0, 1], [0, 1], color='m', linestyle='--')\nplt.title(\"Results\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.legend(loc=\"lower right\")\nplt.show()","9da48388":"Unfortunately, I didn't have enough strengths of will to wait until SVC fits data, so I just left it\n\n","4f483ce2":"# MIDTERM","d98e196a":"To sum up, we saw, that the model's auc_roc_score of each model showed not well performance and also I know the reason why.\n\nI did not implement SMOTE on training set, that's why the predicted list perceived incorrect outputs.\n\nAs I mentioned above, I didn't have enough time to wait for SMOTE class cell to run \n\n*(because it takes forever for kaggle notebook even to open )*\n\n\nSo if I would apply training set on SMOTE model, I believe the results would be much greater. But it was not the only reason.\n\nThe second point to mention, a lot of parameters of model do affect the prediction. \n\nAnd as you have noticed, I didn't set any parameter, because (of course slow processing of the notebook) I wanted to see the pure model behavior.","cbf64e00":"Both test_df and train_df haven't got any Nan's assigned","2cae96e1":"**According to two cells above, I can say that: **\n1. standard deviation of train and test columns is large\n2. values of mean, std, min, max for both data look similar\n3. also, mean values are flactuated between a large range of values for each column.","6906f764":"SVC.fit(X_train,y_train)\nSVC_predict = SVC.predict(X_test)\nprint(\"SVC Accuracy :\", accuracy_score(y_test, SVC_predict))","f52a3d8e":"## XGBoost","30f18710":"roc_auc_score(y_test, LSVC_predict)","73b54b4c":"These are the parameters that correlate also well with target, but correlate negatively \n\nI mean, they are also important for prediction","12e83a98":"print(classification_report(y_test, LSVC_predict))","bf67b54e":"### Train set description:","5beaf684":"Overall, we have all float\/int values in column, except one column which is ID_code\n\nSo I do not have to do any datattype changes","0a19d59f":"### Test set description:","8e1d4922":"SVC = SVC()\nLSVC = LinearSVC()","6b248b23":"## LogReg","68e40100":"## DT and RF","aa3cab50":"## NB","d6efe570":"And the values for target column variables, as we see, distinguish in amount\n\nSo we need to implement *data augmentation* on this dataset, in order to avoid imbalanced classification\n\nBy data augmentation I mean: to artificially expand the size of a training dataset, \n\nto which we referred in classes as SMOTE (or Synthetic Minority Oversampling Technique)\n","7ebac31b":"I showed last 11 elements, because it includes target values too (of course, target correlates well with itself)\n\nAnd so, we have 10 positive parameters that correlate relatively well with target parameter ","af060032":"from sklearn.svm import SVC, LinearSVC","95c85c49":"In above you may notice a histogram\n\nI plotted distribution of all column means, so to say, in order to observe visually the dataset\n\nLooking at this plot, I can say, it's pattern is bimodal","77ebb3bf":"## SVM","d14009f3":"I want to evaluate correlation for each column with target values, just to see how much do I need them in the dataset\n\nSo we'll see which ones are irreplaceble (and which one are not)","c08486c1":"## SMOTE"}}