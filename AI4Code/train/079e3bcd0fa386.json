{"cell_type":{"e7f40fdf":"code","3eae55a8":"code","9afdcaae":"code","97ab128f":"code","bf6e784b":"code","ba9704ef":"code","542b7f9c":"code","9f642c50":"code","1688c5e4":"markdown","f6b66bc4":"markdown","2fd21349":"markdown","02bb78ab":"markdown","2fd11835":"markdown","5937ba37":"markdown"},"source":{"e7f40fdf":"import pandas as pd\nimport numpy as np","3eae55a8":"from keras.datasets import reuters\n(x_train, y_train), (x_test, y_test) = reuters.load_data(path=\"reuters.npz\", num_words=None, \n                                                         skip_top=0,maxlen=None, test_split=0.2, \n                                                         seed=113, start_char=1,oov_char=2, \n                                                         index_from=3)","9afdcaae":"from keras import utils\nfrom keras.preprocessing.text import Tokenizer\n\nt = Tokenizer(num_words=10000)\nseq = np.concatenate((x_train, x_test), axis=0)\nt.fit_on_sequences(seq)\n\nxt_train = t.sequences_to_matrix(x_train, mode='tfidf')\nxt_test = t.sequences_to_matrix(x_test, mode='tfidf')\n\nyt_train = utils.to_categorical(y_train, max(y_train) + 1)\nyt_test = utils.to_categorical(y_test, max(y_train) + 1)","97ab128f":"from keras import layers, models, callbacks\nfrom keras.layers.core import Dense, Dropout # Dropout is being used for not to overfit","bf6e784b":"# 3 elements, 1st is for first layer, 2nd for last layer\nactivations = ['relu', 'softmax'] \nlayer_sizes = [512, 46] # for each layer a size is given\noptimizer_method = 'adadelta'\nbatch_size = 128\nepochs = 5\ncallback = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.001, \n                                   patience=2, mode='auto')","ba9704ef":"model = models.Sequential()\nmodel.add(layers.Dense(layer_sizes[0], input_shape=(10000,), activation=activations[0]))\nmodel.add(Dropout(0.5))\nmodel.add(layers.Dense(layer_sizes[1], activation=activations[1]))\nmodel.compile(optimizer=optimizer_method, loss='cosine_proximity', metrics=['accuracy'])","542b7f9c":"model.fit(x=xt_train, y=yt_train, batch_size=batch_size, epochs=epochs, verbose=1, \n          shuffle=True, validation_split=0.15, use_multiprocessing=True,\n          workers=8, steps_per_epoch=None, callbacks=[callback])","9f642c50":"print(model.metrics_names)\nmodel.evaluate(xt_test, yt_test, verbose=0)","1688c5e4":"## Reading the Train and Test Data","f6b66bc4":"### - Parameters:\n","2fd21349":"# REUTERS DATASET PROCESSES USING KERAS NEURAL NETWORKS","02bb78ab":"## Tokenizing the Values\nDifferent from the count vectorization. Words that are common in many\/all documents are being punished. It considers not just the occurrence of a word in a single document but in the entire corpus.  What we would want is to down weight the common words occurring in almost all documents and give more importance to words that appear in a subset of documents (like \u2018the\u2019, \u2018a\u2019 etc.).\n\n- TF = (# of times term t appears in a document) \/ (# of terms in the document)\n- IDF = log(N\/n), where, N is the # of documents and n is the # of documents a term t has appeared in.\n\nValues are found for each word by multiplication of these parameters. ","2fd11835":"## Training Model with Keras","5937ba37":"#### Notes for Activation:\n- **Tanh vs Sigmoidal:** **tanh** is better for optimization since symmetric around 0, all have *vanishing gradient problem*.\n- **ReLU** is being used in hidden layers, it has no *vanishing gradient problem*. But in final layer, if problem is classification, then **softmax**; if problem is regression, then **linear** is being used."}}