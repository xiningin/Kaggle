{"cell_type":{"75dd9962":"code","d2e60112":"code","aa2b4d9c":"code","5b6d573f":"code","678de71c":"code","45cc9e2e":"code","1c3df984":"code","13e004fb":"code","8de58a47":"code","3b707fe0":"code","e9d0da9b":"code","362b6b15":"code","7fa3a43c":"code","408194cb":"markdown","0661f2e8":"markdown","78f52d0c":"markdown","ee4efc13":"markdown","7727b605":"markdown","087e5782":"markdown","ea1f9772":"markdown","21dd439d":"markdown","4018688a":"markdown"},"source":{"75dd9962":"import os\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport glob\nimport random\nfrom PIL import Image\nimport time\nimport datetime, zipfile\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.losses import mean_squared_error\nfrom tensorflow.keras.optimizers import Adam","d2e60112":"# function to load the image in the form of tensors.\n\ndef load_image(img_path):\n    img = tf.io.read_file(img_path) # returns string type tensor.\n    img = tf.io.decode_jpeg(img, channels = 3)\n    img = tf.image.resize(img, size = (384, 384), antialias = True)\n    img = img \/ 255.0\n    return img","aa2b4d9c":"# function to get the path of individual image.\n\ndef data_path(orig_img_path, hazy_img_path):\n\n    train_img = []\n    val_img = []\n\n    orig_img = glob.glob(orig_img_path + '\/*.jpg')\n    n = len(orig_img)\n    random.shuffle(orig_img)\n    train_keys = orig_img[:int(0.9*n)]        #90% data for train, 10% for test\n    val_keys = orig_img[int(0.9*n):]\n    \n    split_dict = {}\n    for key in train_keys:\n        split_dict[key] = 'train'\n    for key in val_keys:\n        split_dict[key] = 'val'\n\n    hazy_img = glob.glob(hazy_img_path + '\/*.jpg')\n    for img in hazy_img:\n        img_name = img.split('\/')[-1]\n        orig_path = orig_img_path + '\/' + img_name.split('_')[0] + '.jpg'\n        if (split_dict[orig_path] == 'train'):\n            train_img.append([img, orig_path])  # img -> hazy image path\n        else:\n            val_img.append([img, orig_path])\n            \n    return train_img, val_img","5b6d573f":"# data_path(orig_img_path = '..\/input\/dehaze\/clear_images', hazy_img_path = '..\/input\/dehaze\/haze')","678de71c":"# function to load tensor image data in batches.\n\ndef dataloader(train_data, val_data, batch_size):\n    \n    train_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in train_data]).map(lambda x: load_image(x))\n    train_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in train_data]).map(lambda x: load_image(x))\n    train = tf.data.Dataset.zip((train_data_haze, train_data_orig)).shuffle(buffer_size=100, reshuffle_each_iteration=True).batch(batch_size)\n    \n    val_data_orig = tf.data.Dataset.from_tensor_slices([img[1] for img in val_data]).map(lambda x: load_image(x))\n    val_data_haze = tf.data.Dataset.from_tensor_slices([img[0] for img in val_data]).map(lambda x: load_image(x))\n    val = tf.data.Dataset.zip((val_data_haze, val_data_orig)).shuffle(buffer_size=100, reshuffle_each_iteration=True).batch(batch_size)\n    \n    return train, val","45cc9e2e":"# dataloader(train_data, val_data, 8)","1c3df984":"# function to display output.\n\ndef display_img(model, hazy_img, orig_img):\n    \n    dehazed_img = model(hazy_img, training = True)\n    plt.figure(figsize = (15,12))\n    \n    display_list = [hazy_img[0], orig_img[0], dehazed_img[0]]\n    title = ['Hazy Image', 'Ground Truth', 'Dehazed Image']\n    \n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        plt.imshow(display_list[i])\n        plt.axis('off')\n        \n    plt.show()","13e004fb":"def gman_net():\n    \n    inputs = tf.keras.Input(shape = [384, 384, 3])     # height, width of input image changed because of error in output\n    \n                                    ######################## GMAN Network ###########################\n        \n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(inputs)\n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    \n    \n                                    #### Encoding Layers #####\n    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    conv_up = Conv2D(filters = 128, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n                                    \n                                    #### Residual Layers #####\n    conv1_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv_up)\n    conv1_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_1)\n    conv1_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                   bias_initializer = b_init, kernel_regularizer = regularizer)(conv1_2)\n    conc1 = tf.add(conv1_3, conv1_1)\n    conv1 = tf.keras.activations.relu(conc1)\n\n    conv2_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv1)\n    conv2_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_1)\n    conv2_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2_2)\n    conc2 = tf.add(conv2_3, conv2_1)\n    conv2 = tf.keras.activations.relu(conc2)\n\n    conv3_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv2)\n    conv3_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_1)\n    conv3_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_2)\n    conv3_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_3)\n    conv3_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3_4)\n    conc3 = tf.add(conv3_5, conv3_1)\n    conv3 = tf.keras.activations.relu(conc3)\n\n    conv4_1 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv3)\n    conv4_2 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_1)\n    conv4_3 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_2)\n    conv4_4 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_3)\n    conv4_5 = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                     bias_initializer = b_init, kernel_regularizer = regularizer)(conv4_4)\n    conc4 = tf.add(conv4_5, conv4_1)\n    conv4 = tf.keras.activations.relu(conc4)\n\n                                            ##### Decoding Layers #####\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                             kernel_regularizer = regularizer)(conv4)\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, strides = 2, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                             kernel_regularizer = regularizer)(deconv)\n\n    conv = Conv2D(filters = 64, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(deconv)\n    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                  bias_initializer = b_init, kernel_regularizer = regularizer)(conv)\n    conc = tf.add(conv, inputs)\n    gman_output = tf.keras.activations.relu(conc)\n    \n                               ######################## Parallel Network ###########################\n    \n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(inputs)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 2, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 64, kernel_size = 3, dilation_rate = 1, padding = 'same', kernel_initializer = k_init, activation = 'relu',\n                 kernel_regularizer = regularizer)(conv)\n    deconv = Conv2DTranspose(filters = 64, kernel_size = 3, dilation_rate = 4, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                           activation = 'relu', kernel_regularizer = regularizer)(conv)\n    conv = Conv2D(filters = 3, kernel_size = 3, strides = 1, padding = 'same', kernel_initializer = tf.keras.initializers.glorot_normal(seed = 101),\n                 kernel_regularizer = regularizer)(deconv)\n    conc = tf.add(conv, inputs)\n    pn_output = tf.keras.activations.relu(conc)\n    \n    output = tf.add(gman_output, pn_output)\n    \n    return Model(inputs = inputs, outputs = output)","8de58a47":"epochs = 15\nbatch_size = 16\nk_init = tf.keras.initializers.random_normal(stddev=0.008, seed = 101)      \nregularizer = tf.keras.regularizers.L2(1e-4)\nb_init = tf.constant_initializer()\n\ntrain_data, val_data = data_path(orig_img_path = '..\/input\/dehaze\/clear_images', hazy_img_path = '..\/input\/dehaze\/haze')\ntrain, val = dataloader(train_data, val_data, batch_size)\n\noptimizer = Adam(learning_rate = 1e-4)\nnet = gman_net()\n\ntrain_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"train loss\")\nval_loss_tracker = tf.keras.metrics.MeanSquaredError(name = \"val loss\")","3b707fe0":"def train_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer):\n    \n    for epoch in range(epochs):\n        \n        print(\"\\nStart of epoch %d\" % (epoch,), end=' ')\n        start_time_epoch = time.time()\n        start_time_step = time.time()\n        \n        # training loop\n        \n        for step, (train_batch_haze, train_batch_orig) in enumerate(train):\n\n            with tf.GradientTape() as tape:\n\n                train_logits = net(train_batch_haze, training = True)\n                loss = mean_squared_error(train_batch_orig, train_logits)\n\n            grads = tape.gradient(loss, net.trainable_weights)\n            optimizer.apply_gradients(zip(grads, net.trainable_weights))\n\n            train_loss_tracker.update_state(train_batch_orig, train_logits)\n            if step == 0:\n                print('[', end='')\n            if step % 64 == 0:\n                print('=', end='')\n        \n        print(']', end='')\n        print('  -  ', end='')\n        print('Training Loss: %.4f' % (train_loss_tracker.result()), end='')\n        \n        # validation loop\n        \n        for step, (val_batch_haze, val_batch_orig) in enumerate(val):\n            val_logits = net(val_batch_haze, training = False)\n            val_loss_tracker.update_state(val_batch_orig, val_logits)\n            \n            if step % 32 ==0:\n                display_img(net, val_batch_haze, val_batch_orig)\n        \n        print('  -  ', end='')\n        print('Validation Loss: %.4f' % (val_loss_tracker.result()), end='')\n        print('  -  ', end=' ')\n        print(\"Time taken: %.2fs\" % (time.time() - start_time_epoch))\n        \n        net.save('trained_model')           # save the model(variables, weights, etc)\n        train_loss_tracker.reset_states()\n        val_loss_tracker.reset_states()","e9d0da9b":"%%time\ntrain_model(epochs, train, val, net, train_loss_tracker, val_loss_tracker, optimizer)","362b6b15":"def evaluate(net, test_img_path):\n    \n    test_img = glob.glob(test_img_path + '\/*.jpg')\n    random.shuffle(test_img)\n    \n    for img in test_img:\n        \n        img = tf.io.read_file(img)\n        img = tf.io.decode_jpeg(img, channels = 3)\n        \n        if img.shape[1] > img.shape[0]:\n            img = tf.image.resize(img, size = (1080, 1920), antialias = True)\n        if img.shape[1] < img.shape[0]:\n            img = tf.image.resize(img, size = (1920, 1080), antialias = True)\n        \n        img = img \/ 255\n        img = tf.expand_dims(img, axis = 0)      # transform input image from 3D to 4D\n        \n        dehaze = net(img, training = False)\n        \n        plt.figure(figsize = (80, 80))\n        \n        display_list = [img[0], dehaze[0]]       # make the first dimension zero\n        title = ['Hazy Image', 'Dehazed Image']\n\n        for i in range(2):\n            plt.subplot(1, 2, i+1)\n            plt.title(title[i], fontsize = 65, y = 1.045)\n            plt.imshow(display_list[i])\n            plt.axis('off')\n        \n        plt.show()","7fa3a43c":"test_net = tf.keras.models.load_model('trained_model', compile = False)\nevaluate(test_net, '..\/input\/hazy-test-images')","408194cb":"# Network Architecture","0661f2e8":"Documentation and explanation available [here](tinyurl.com\/gman-dehaze-net)  \n[Github Repository](https:\/\/github.com\/sanchitvj\/Image-Dehazing-using-GMAN-net)","78f52d0c":"# Hyperparameters","ee4efc13":"# Training and validation function","7727b605":"# Evaluation","087e5782":"## GMAN Network\n![image.png](attachment:4f645fab-fd5e-45ed-bc99-a0fdb3f7dc92.png)\n\n## Parallel Network  \n![image.png](attachment:4acfb038-8cb0-411a-a01c-c710fb9829ef.png)  \n\n## Combined Network  \n![image.png](attachment:4f9f34e4-fbb1-44be-baa3-d51f34b1f4a6.png)  ","ea1f9772":"# Preprocessing and loading of data","21dd439d":"## Normal Convolution with Padding and Stride &emsp; &emsp;&emsp;&emsp;Dilated Convolution\n<img align=\"left\" src=\"https:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/gif\/padding_strides.gif?raw=true\">  \n\n<figure>\n<img align=\"right\" src=\"https:\/\/github.com\/vdumoulin\/conv_arithmetic\/blob\/master\/gif\/dilation.gif?raw=true\">\n<\/figure>","4018688a":"# Network Function"}}