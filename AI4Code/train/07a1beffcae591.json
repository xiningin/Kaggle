{"cell_type":{"5199c6dc":"code","efe6a161":"code","711cd89f":"code","da38ce65":"code","0893d7de":"code","87412a86":"markdown"},"source":{"5199c6dc":"import numpy as np\nimport pandas as pd\nfrom math import ceil, floor\nimport tensorflow as tf\nimport tensorflow.keras.layers as L\nfrom tensorflow.keras.initializers import TruncatedNormal\nfrom sklearn import model_selection\nfrom transformers import BertConfig, TFBertPreTrainedModel, TFBertMainLayer\nfrom tokenizers import BertWordPieceTokenizer\n\nimport logging\ntf.get_logger().setLevel(logging.ERROR)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n    \n\ntf.config.optimizer.set_jit(True)\ntf.config.optimizer.set_experimental_options(\n    {\"auto_mixed_precision\": True})","efe6a161":"train_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/train.csv')\ntrain_df.dropna(inplace=True)\n\ntest_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/test.csv')\ntest_df.loc[:, \"selected_text\"] = test_df.text.values\n\nsubmission_df = pd.read_csv('..\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\nprint(\"train shape =\", train_df.shape)\nprint(\"test shape  =\", test_df.shape)\n\n# set some global variables\nPATH = \"..\/input\/bert-base-uncased\/\"\nMAX_SEQUENCE_LENGTH = 128\nTOKENIZER = BertWordPieceTokenizer(f\"{PATH}\/vocab.txt\", lowercase=True)\n\n# let's take a look at the data\ntrain_df.head(10)","711cd89f":"def preprocess(index, tweet, selected_text, sentiment):\n    \"\"\"\n    Will be used in tf.data.Dataset.from_generator(...)\n    \n    \"\"\"\n    \n    # The original strings have been converted to \n    # byte strings, so we need to decode it\n    tweet = tweet.decode('utf-8')\n    selected_text = selected_text.decode('utf-8')\n    sentiment = sentiment.decode('utf-8')\n    \n    # Clean up the strings a bit\n    tweet = \" \".join(str(tweet).split())\n    selected_text = \" \".join(str(selected_text).split())\n    \n    # find the intersection between text and selected text\n    idx_start, idx_end = None, None\n    for index in (i for i, c in enumerate(tweet) if c == selected_text[0]):\n        if tweet[index:index+len(selected_text)] == selected_text:\n            idx_start = index\n            idx_end = index + len(selected_text)\n            break\n    \n    intersection = [0] * len(tweet)\n    if idx_start != None and idx_end != None:\n        for char_idx in range(idx_start, idx_end):\n            intersection[char_idx] = 1\n    \n    # tokenize with offsets\n    enc = TOKENIZER.encode(tweet)\n    input_ids, offsets = enc.ids, enc.offsets\n\n    # compute targets, one-hot encoding \n    targets = np.zeros(len(input_ids))\n    for i, (o1, o2) in enumerate(offsets):\n        if sum(intersection[o1:o2]) > 0:\n            targets[i] = 1\n    \n    target_start = np.zeros(len(input_ids))\n    target_end = np.zeros(len(input_ids))\n    targets_nonzero = np.nonzero(targets)[0]\n    if len(targets_nonzero) > 0: \n        target_start[targets_nonzero[0]] = 1\n        target_end[targets_nonzero[-1]] = 1\n\n    # trim the data if necessary\n    diff = max(len(input_ids) - (MAX_SEQUENCE_LENGTH - 2), 0)\n    input_ids = input_ids[1:-1][:len(input_ids)-diff-2]\n    offsets = offsets[:len(offsets)-diff]\n    target_start = list(target_start[:len(target_start)-diff])\n    target_end = list(target_end[:len(target_end)-diff])\n    \n    \n    # add and pad data\n    # --> [CLS] sentiment [SEP] input_ids [SEP] [PAD]\n    sentiment_map = {\n        'positive': 3893,\n        'negative': 4997,\n        'neutral': 8699,\n    }\n    \n    input_ids = [101] + [sentiment_map[sentiment]] + [102] + input_ids + [102]\n    \n    attention_mask = [1]*len(input_ids) + [0]*(MAX_SEQUENCE_LENGTH-len(input_ids))\n    input_type_ids = [0] + [0] + [0] + [1]*(MAX_SEQUENCE_LENGTH - 3)\n    input_ids = input_ids + [0]*(MAX_SEQUENCE_LENGTH - len(input_ids))\n    offsets = offsets + [(0, 0)]*(MAX_SEQUENCE_LENGTH - len(offsets))\n    target_start = target_start + [0]*(MAX_SEQUENCE_LENGTH - len(target_start))\n    target_end = target_end + [0]*(MAX_SEQUENCE_LENGTH - len(target_end))\n    \n    return (\n        input_ids, attention_mask, input_type_ids, offsets,\n        target_start, target_end, tweet, selected_text, sentiment, \n    )\n\n\nclass TweetSentimentDataset(tf.data.Dataset):\n    \n    OUTPUT_TYPES = (\n        tf.dtypes.int32,  tf.dtypes.int32,   tf.dtypes.int32, \n        tf.dtypes.int32,  tf.dtypes.float32, tf.dtypes.float32,\n        tf.dtypes.string, tf.dtypes.string,  tf.dtypes.string,\n    )\n    \n    OUTPUT_SHAPES = (\n        (128,),   (128,), (128,), \n        (128, 2), (128,), (128,),\n        (),       (),     (),\n    )\n    \n    # AutoGraph will automatically convert Python code to\n    # Tensorflow graph code. You could also wrap 'preprocess' \n    # in tf.py_function(..) for arbitrary python code\n    def _generator(tweet, selected_text, sentiment):\n        for i, (tw, st, se) in enumerate(\n            zip(tweet, selected_text, sentiment)):\n            yield preprocess(i, tw, st, se)\n    \n    # This dataset object will return a generator\n    def __new__(cls, tweet, selected_text, sentiment):\n        return tf.data.Dataset.from_generator(\n            cls._generator,\n            output_types=cls.OUTPUT_TYPES,\n            output_shapes=cls.OUTPUT_SHAPES,\n            args=(tweet, selected_text, sentiment)\n        )\n    \n    @staticmethod\n    def create(dataframe, batch_size, shuffle_buffer_size=-1):\n        dataset = TweetSentimentDataset(\n            dataframe.text.values, \n            dataframe.selected_text.values, \n            dataframe.sentiment.values\n        )\n\n        dataset = dataset.cache()\n        if shuffle_buffer_size != -1:\n            dataset = dataset.shuffle(shuffle_buffer_size)\n        dataset = dataset.batch(batch_size)\n        dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n\n        return dataset","da38ce65":"class TransformerModel(TFBertPreTrainedModel):\n    \n    def __init__(self, config, *inputs, **kwargs):\n        super().__init__(config, *inputs, **kwargs)\n        \n        self.bert = TFBertMainLayer(config, name=\"bert\")\n        self.concat = L.Concatenate()\n        self.dropout = L.Dropout(config.hidden_dropout_prob)\n        self.qa_outputs = L.Dense(\n            config.num_labels, \n            kernel_initializer=TruncatedNormal(stddev=config.initializer_range),\n            name=\"qa_outputs\")\n        \n    @tf.function\n    def call(self, inputs, **kwargs):\n        # outputs: Tuple[sequence, pooled, hidden_states]\n        _, _, hidden_states = self.bert(inputs, **kwargs)\n        \n        hidden_states = self.concat([\n            hidden_states[-1], hidden_states[-2],\n            hidden_states[-3], hidden_states[-4]\n        ])\n        \n        hidden_states = self.dropout(hidden_states)\n        logits = self.qa_outputs(hidden_states)\n        start_logits, end_logits = tf.split(logits, 2, axis=-1)\n        start_logits = tf.squeeze(start_logits, axis=-1)\n        end_logits = tf.squeeze(end_logits, axis=-1)\n        \n        return start_logits, end_logits\n    \n    \ndef train(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def train_step(model, inputs, y_true, loss_fn, optimizer):\n        with tf.GradientTape() as tape:\n            y_pred = model(inputs, training=True)\n            loss  = loss_fn(y_true[0], y_pred[0])\n            loss += loss_fn(y_true[1], y_pred[1])\n            scaled_loss = optimizer.get_scaled_loss(loss)\n    \n        scaled_gradients = tape.gradient(scaled_loss, model.trainable_variables)\n        gradients = optimizer.get_unscaled_gradients(scaled_gradients)\n        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n        return loss, y_pred\n\n    epoch_loss = 0.\n    for batch_num, sample in enumerate(dataset):\n        loss, y_pred = train_step(\n            model, sample[:3], sample[4:6], loss_fn, optimizer)\n\n        epoch_loss += loss\n\n        print(\n            f\"training ... batch {batch_num+1:03d} : \"\n            f\"train loss {epoch_loss\/(batch_num+1):.3f} \",\n            end='\\r')\n        \n        \ndef predict(model, dataset, loss_fn, optimizer):\n    \n    @tf.function\n    def predict_step(model, inputs):\n        return model(inputs)\n        \n    def to_numpy(*args):\n        out = []\n        for arg in args:\n            if arg.dtype == tf.string:\n                arg = [s.decode('utf-8') for s in arg.numpy()]\n                out.append(arg)\n            else:\n                arg = arg.numpy()\n                out.append(arg)\n        return out\n    \n    # Initialize accumulators\n    offset = tf.zeros([0, 128, 2], dtype=tf.dtypes.int32)\n    text = tf.zeros([0,], dtype=tf.dtypes.string)\n    selected_text = tf.zeros([0,], dtype=tf.dtypes.string)\n    sentiment = tf.zeros([0,], dtype=tf.dtypes.string)\n    pred_start = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n    pred_end = tf.zeros([0, 128], dtype=tf.dtypes.float32)\n    \n    for batch_num, sample in enumerate(dataset):\n        \n        print(f\"predicting ... batch {batch_num+1:03d}\"+\" \"*20, end='\\r')\n        \n        y_pred = predict_step(model, sample[:3])\n        \n        # add batch to accumulators\n        pred_start = tf.concat((pred_start, y_pred[0]), axis=0)\n        pred_end = tf.concat((pred_end, y_pred[1]), axis=0)\n        offset = tf.concat((offset, sample[3]), axis=0)\n        text = tf.concat((text, sample[6]), axis=0)\n        selected_text = tf.concat((selected_text, sample[7]), axis=0)\n        sentiment = tf.concat((sentiment, sample[8]), axis=0)\n\n    pred_start = tf.nn.softmax(pred_start)\n    pred_end = tf.nn.softmax(pred_end)\n    \n    pred_start, pred_end, text, selected_text, sentiment, offset = \\\n        to_numpy(pred_start, pred_end, text, selected_text, sentiment, offset)\n    \n    return pred_start, pred_end, text, selected_text, sentiment, offset\n\n\ndef decode_prediction(pred_start, pred_end, text, offset, sentiment):\n    \n    def decode(pred_start, pred_end, text, offset):\n\n        decoded_text = \"\"\n        for i in range(pred_start, pred_end+1):\n            decoded_text += text[offset[i][0]:offset[i][1]]\n            if i <= pred_end and offset[i][1] < offset[i+1][0]:\n                decoded_text += \" \"\n        return decoded_text\n    \n    decoded_predictions = []\n    for i in range(len(text)):\n        if sentiment[i] == \"neutral\":\n            decoded_text = text[i]\n        else:\n            idx_start = np.argmax(pred_start[i])\n            idx_end = np.argmax(pred_end[i])\n            if idx_start > idx_end:\n                idx_end = idx_start \n            decoded_text = str(decode(idx_start, idx_end, text[i], offset[i]))\n            if len(decoded_text) == 0:\n                decoded_text = text[i]\n        decoded_predictions.append(decoded_text)\n    \n    return decoded_predictions\n\ndef jaccard(str1, str2):\n    a = set(str1.lower().split())\n    b = set(str2.lower().split())\n    c = a.intersection(b)\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","0893d7de":"num_folds = 5\nnum_epochs = 3\nbatch_size = 32\nlearning_rate = 5e-5\n\noptimizer = tf.keras.optimizers.Adam(learning_rate)\noptimizer = tf.keras.mixed_precision.experimental.LossScaleOptimizer(\n    optimizer, 'dynamic')\n\nconfig = BertConfig()\nconfig.output_hidden_states = True\nconfig.num_labels = 2\nmodel = TransformerModel.from_pretrained(PATH, config=config)\n\nloss_fn = tf.keras.losses.CategoricalCrossentropy(from_logits=True)\n\nkfold = model_selection.KFold(\n    n_splits=num_folds, shuffle=True, random_state=42)\n\n# initialize test predictions\ntest_preds_start = np.zeros((len(test_df), 128), dtype=np.float32)\ntest_preds_end = np.zeros((len(test_df), 128), dtype=np.float32)\n\nfor fold_num, (train_idx, valid_idx) in enumerate(kfold.split(train_df.text)):\n    print(\"\\nfold %02d\" % (fold_num+1))\n        \n    train_dataset = TweetSentimentDataset.create(\n        train_df.iloc[train_idx], batch_size, shuffle_buffer_size=2048)\n    valid_dataset = TweetSentimentDataset.create(\n        train_df.iloc[valid_idx], batch_size, shuffle_buffer_size=-1)\n    test_dataset = TweetSentimentDataset.create(\n        test_df,                  batch_size, shuffle_buffer_size=-1)\n    \n    best_score = float('-inf')\n    for epoch_num in range(num_epochs):\n        print(\"\\nepoch %03d\" % (epoch_num+1))\n        \n        # train for an epoch\n        train(model, train_dataset, loss_fn, optimizer)\n        \n        # predict validation set and compute jaccardian distances\n        pred_start, pred_end, text, selected_text, sentiment, offset = \\\n            predict(model, valid_dataset, loss_fn, optimizer)\n        \n        selected_text_pred = decode_prediction(\n            pred_start, pred_end, text, offset, sentiment)\n        jaccards = []\n        for i in range(len(selected_text)):\n            jaccards.append(\n                jaccard(selected_text[i], selected_text_pred[i]))\n        \n        score = np.mean(jaccards)\n        print(f\"valid jaccard epoch {epoch_num+1:03d}: {score}\"+\" \"*15)\n        \n        if score > best_score:\n            best_score = score\n            # requires you to have 'fold-{fold_num}' folder in PATH\n            # model.save_pretrained(PATH+f'fold-{fold_num}')\n            \n            # predict test set\n            test_pred_start, test_pred_end, test_text, _, test_sentiment, test_offset = \\\n                predict(model, test_dataset, loss_fn, optimizer)\n    \n    # add epoch's best test preds to test preds arrays\n    test_preds_start += test_pred_start\n    test_preds_end += test_pred_end\n    \n    # refresh for new fold; to avoid memory leaks, perhaps more needs to be done here\n    del model; model = TransformerModel.from_pretrained(PATH, config=config)\n    \n# decode test set and add to submission file\nselected_text_pred = decode_prediction(\n    test_preds_start, test_preds_end, test_text, test_offset, test_sentiment)\nsubmission_df.loc[:, 'selected_text'] = selected_text_pred\nsubmission_df.to_csv(\"submission.csv\", index=False)","87412a86":"# Load packages"}}