{"cell_type":{"8712eccc":"code","b531383b":"code","974b5f0a":"code","99ee2d83":"code","d799779b":"code","5b012d25":"code","e747aca0":"code","92212fa9":"code","eebb6164":"code","5178b3a7":"code","061dfa2a":"code","e2e3bedf":"code","cd8d148a":"code","c5f25698":"code","bdc8a612":"code","aee8ff6d":"code","e48de624":"code","bc33e14c":"code","a3a4a8fc":"code","59640d53":"code","f0d6146b":"code","769e5ed0":"code","e37db2b6":"code","a4aed77f":"code","9df6289d":"code","d470268e":"code","a20d9636":"code","04128d07":"code","4f43c2fd":"code","ed6cbbf0":"code","86560424":"code","ac880b7b":"code","82937942":"code","ced3584e":"code","0134047e":"code","9ed7bcd8":"code","84f7514f":"code","5b45045a":"code","d24e1995":"code","6f97bc39":"code","617abfc0":"code","0b30bf3d":"code","cb07dec0":"code","573b56cb":"code","1d943538":"code","0178d541":"code","b42efbb0":"code","48e7c581":"code","4e4eded7":"code","dc69c1f1":"code","786987b8":"code","fbf0d3c8":"code","5dc45532":"code","4c39813d":"code","2821f2af":"code","5ad7f3b5":"code","c650ccad":"code","e797dca8":"code","869f5d72":"code","5be626f2":"code","53e06aad":"code","315e7f6a":"code","f92fa948":"code","ad31ec6b":"code","94381142":"code","5364d1fa":"code","a6562a97":"code","add5a5a4":"code","aa947273":"code","d842e1ac":"markdown","8fb8461f":"markdown","22c518b1":"markdown","2dec5127":"markdown","5463215b":"markdown","a25e1850":"markdown","f75a7da6":"markdown","ebaabe94":"markdown","1c679556":"markdown","99bd088d":"markdown","09b542fb":"markdown","64ba046e":"markdown","29cc142c":"markdown","e2a2c3a2":"markdown","07e84a31":"markdown","6a7c69bf":"markdown","2babe567":"markdown","a22b243f":"markdown","18f92739":"markdown","0c7d5ae6":"markdown","7bf4998a":"markdown","8798f5f6":"markdown","b2d713f3":"markdown","21e11bbb":"markdown","3780b868":"markdown","76909529":"markdown","c4686173":"markdown","1bd7c2f2":"markdown","fa2950e8":"markdown","982aaa91":"markdown","880ddc77":"markdown","910a3e67":"markdown","9d70e76a":"markdown","db3de64a":"markdown","c0390832":"markdown","b5c3b9c9":"markdown","e6a654a9":"markdown","023df48c":"markdown","d4bd4820":"markdown","91b4136f":"markdown","6d8854f4":"markdown","8b02825f":"markdown","baedd391":"markdown","7a4c38e8":"markdown","af86bbbf":"markdown","2cce2f74":"markdown","a7669230":"markdown","6d7fd41e":"markdown","1fcff634":"markdown","0038e882":"markdown","83ba110e":"markdown"},"source":{"8712eccc":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport category_encoders as ce\n\n\nfrom sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_curve\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import average_precision_score\nfrom sklearn.preprocessing import label_binarize\n\n\nimport plotly.graph_objects as go\nfrom nltk.corpus import stopwords","b531383b":"df = pd.read_csv(\"..\/input\/us-accidents\/US_Accidents_Dec20_updated.csv\")\ndf.head()","974b5f0a":"state_counts = df[\"State\"].value_counts()\nfig = go.Figure(data=go.Choropleth(locations=state_counts.index, z=state_counts.values.astype(float), locationmode=\"USA-states\", colorscale=\"turbo\"))\nfig.update_layout(title_text=\"Number of US Accidents for each State\", geo_scope=\"usa\")\nfig.show()","99ee2d83":"plt.figure(figsize=(20, 8))\nplt.title(\"Top 10 states with the highest number of accidents\")\nsns.barplot(state_counts[:10].values, state_counts[:10].index, orient=\"h\")\nplt.xlabel(\"Number of accident\")\nplt.ylabel(\"State\")\nplt.show()","d799779b":"stop = stopwords.words(\"english\") + [\"-\"]\n\ndf_s4_desc = df[df[\"Severity\"] == 4][\"Description\"]\n# Split the description\ndf_words = df_s4_desc.str.lower().str.split(expand=True).stack()\n\n# If the word is not in the stopwords list\ncounts = df_words[~df_words.isin(stop)].value_counts()[:10]\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Top 10 words used to describe an accident with severity 4\")\nsns.barplot(counts.values, counts.index, orient=\"h\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Word\")\nplt.show()","5b012d25":"road_features = [\"Amenity\", \"Bump\", \"Crossing\", \"Give_Way\", \"Junction\", \"No_Exit\", \"Railway\", \"Roundabout\", \"Station\", \"Stop\", \"Traffic_Calming\", \"Traffic_Signal\", \"Turning_Loop\"]\ndata = df[road_features].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Most frequent road features\")\nsns.barplot(data.values, data.index, orient=\"h\")\nplt.xlabel(\"Value\")\nplt.ylabel(\"Road feature\")\nplt.show()","e747aca0":"severity_distance = df.groupby(\"Severity\").mean()[\"Distance(mi)\"].sort_values(ascending=False)\n\nplt.figure(figsize=(18, 8))\nplt.title(\"Medium distance by severity\")\nsns.barplot(severity_distance.values, severity_distance.index, orient=\"h\", order=severity_distance.index)\nplt.xlabel(\"Distance (mi)\")\nplt.show()","92212fa9":"counts = df[\"Weather_Condition\"].value_counts()[:15]\nplt.figure(figsize=(20, 8))\nplt.title(\"Histogram distribution of the top 15 weather conditions\")\nsns.barplot(counts.index, counts.values)\nplt.xlabel(\"Weather Condition\")\nplt.ylabel(\"Value\")\nplt.show()","eebb6164":"counts = pd.to_datetime(df['Start_Time']).dt.day_name().value_counts()\nweekdays = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n\nplt.figure(figsize=(20, 8))\nplt.title(\"Number of accidents for each weekday\")\nsns.barplot(counts.index, counts.values, order=weekdays)\nplt.xlabel(\"Weekday\")\nplt.ylabel(\"Value\")\nplt.show()","5178b3a7":"X = df\nX.head()","061dfa2a":"# Cast Start_Time to datetime\nX[\"Start_Time\"] = pd.to_datetime(X[\"Start_Time\"])\n\n# Extract year, month, weekday and day\nX[\"Year\"] = X[\"Start_Time\"].dt.year\nX[\"Month\"] = X[\"Start_Time\"].dt.month\nX[\"Weekday\"] = X[\"Start_Time\"].dt.weekday\nX[\"Day\"] = X[\"Start_Time\"].dt.day\n\n# Extract hour and minute\nX[\"Hour\"] = X[\"Start_Time\"].dt.hour\nX[\"Minute\"] = X[\"Start_Time\"].dt.minute\n\nX.head()","e2e3bedf":"corr_matrix = X.corr()\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(corr_matrix, vmin=-1, vmax=1, cmap=\"seismic\")\nplt.gca().patch.set(hatch=\"X\", edgecolor=\"#666\")\nplt.show()","cd8d148a":"features_to_drop = [\"ID\", \"Start_Time\", \"End_Time\", \"End_Lat\", \"End_Lng\", \"Description\", \"Number\", \"Street\", \"County\", \"State\", \"Zipcode\", \"Country\", \"Timezone\", \"Airport_Code\", \"Weather_Timestamp\", \"Wind_Chill(F)\", \"Turning_Loop\", \"Sunrise_Sunset\", \"Nautical_Twilight\", \"Astronomical_Twilight\"]\nX = X.drop(features_to_drop, axis=1)\nX.head()","c5f25698":"print(\"Number of rows:\", len(X.index))\nX.drop_duplicates(inplace=True)\nprint(\"Number of rows after drop of duplicates:\", len(X.index))","bdc8a612":"X[\"Side\"].value_counts()","aee8ff6d":"X = X[X[\"Side\"] != \" \"]\nX[\"Side\"].value_counts()","e48de624":"X[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)","bc33e14c":"X = X[X[\"Pressure(in)\"] != 0]\nX = X[X[\"Visibility(mi)\"] != 0]\nX[[\"Pressure(in)\", \"Visibility(mi)\"]].describe().round(2)","a3a4a8fc":"unique_weather = X[\"Weather_Condition\"].unique()\n\nprint(len(unique_weather))\nprint(unique_weather)","59640d53":"X.loc[X[\"Weather_Condition\"].str.contains(\"Thunder|T-Storm\", na=False), \"Weather_Condition\"] = \"Thunderstorm\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Snow|Sleet|Wintry\", na=False), \"Weather_Condition\"] = \"Snow\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Rain|Drizzle|Shower\", na=False), \"Weather_Condition\"] = \"Rain\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Wind|Squalls\", na=False), \"Weather_Condition\"] = \"Windy\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Hail|Pellets\", na=False), \"Weather_Condition\"] = \"Hail\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Fair\", na=False), \"Weather_Condition\"] = \"Clear\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Cloud|Overcast\", na=False), \"Weather_Condition\"] = \"Cloudy\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Mist|Haze|Fog\", na=False), \"Weather_Condition\"] = \"Fog\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Sand|Dust\", na=False), \"Weather_Condition\"] = \"Sand\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"Smoke|Volcanic Ash\", na=False), \"Weather_Condition\"] = \"Smoke\"\nX.loc[X[\"Weather_Condition\"].str.contains(\"N\/A Precipitation\", na=False), \"Weather_Condition\"] = np.nan\n\nprint(X[\"Weather_Condition\"].unique())","f0d6146b":"X[\"Wind_Direction\"].unique()","769e5ed0":"X.loc[X[\"Wind_Direction\"] == \"CALM\", \"Wind_Direction\"] = \"Calm\"\nX.loc[X[\"Wind_Direction\"] == \"VAR\", \"Wind_Direction\"] = \"Variable\"\nX.loc[X[\"Wind_Direction\"] == \"East\", \"Wind_Direction\"] = \"E\"\nX.loc[X[\"Wind_Direction\"] == \"North\", \"Wind_Direction\"] = \"N\"\nX.loc[X[\"Wind_Direction\"] == \"South\", \"Wind_Direction\"] = \"S\"\nX.loc[X[\"Wind_Direction\"] == \"West\", \"Wind_Direction\"] = \"W\"\n\nX[\"Wind_Direction\"] = X[\"Wind_Direction\"].map(lambda x : x if len(x) != 3 else x[1:], na_action=\"ignore\")\n\nX[\"Wind_Direction\"].unique()","e37db2b6":"X.isna().sum()","a4aed77f":"features_to_fill = [\"Temperature(F)\", \"Humidity(%)\", \"Pressure(in)\", \"Visibility(mi)\", \"Wind_Speed(mph)\", \"Precipitation(in)\"]\nX[features_to_fill] = X[features_to_fill].fillna(X[features_to_fill].mean())\n\nX.dropna(inplace=True)\n\nX.isna().sum()","9df6289d":"X.describe().round(2)","d470268e":"severity_counts = X[\"Severity\"].value_counts()\n\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(severity_counts.index, severity_counts.values)\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Value\")\nplt.show()","a20d9636":"size = len(X[X[\"Severity\"]==1].index)\ndf = pd.DataFrame()\nfor i in range(1,5):\n    S = X[X[\"Severity\"]==i]\n    df = df.append(S.sample(size, random_state=42))\nX = df","04128d07":"severity_counts = X[\"Severity\"].value_counts()\nplt.figure(figsize=(10, 8))\nplt.title(\"Histogram for the severity\")\nsns.barplot(severity_counts.index, severity_counts.values)\nplt.xlabel(\"Severity\")\nplt.ylabel(\"Value\")\nplt.show()","4f43c2fd":"scaler = MinMaxScaler()\nfeatures = ['Temperature(F)','Distance(mi)','Humidity(%)','Pressure(in)','Visibility(mi)','Wind_Speed(mph)','Precipitation(in)','Start_Lng','Start_Lat','Year', 'Month','Weekday','Day','Hour','Minute']\nX[features] = scaler.fit_transform(X[features])\nX.head()","ed6cbbf0":"categorical_features = set([\"Side\", \"City\", \"Wind_Direction\", \"Weather_Condition\", \"Civil_Twilight\"])\n\nfor cat in categorical_features:\n    X[cat] = X[cat].astype(\"category\")\n\nX.info()","86560424":"print(\"Unique classes for each categorical feature:\")\nfor cat in categorical_features:\n    print(\"{:15s}\".format(cat), \"\\t\", len(X[cat].unique()))","ac880b7b":"X = X.replace([True, False], [1, 0])\n\nX.head()","82937942":"# Remove city because it will be encoded later\nonehot_cols = categorical_features - set([\"City\"])\n\nX = pd.get_dummies(X, columns=onehot_cols, drop_first=True)\n\nX.head()","ced3584e":"binary_encoder = ce.binary.BinaryEncoder()\n\ncity_binary_enc = binary_encoder.fit_transform(X[\"City\"])\ncity_binary_enc","0134047e":"X = pd.concat([X, city_binary_enc], axis=1).drop(\"City\", axis=1)\n\nX.head()","9ed7bcd8":"# Metrics dictionary\naccuracy = dict()\nprecision = dict()\nrecall = dict()\nf1 = dict()\nfpr = dict()\ntpr = dict()","84f7514f":"# Train\/Validation - Test split\nX, X_test = train_test_split(X, test_size=.2, random_state=42)\nprint(X.shape, X_test.shape)","5b45045a":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","d24e1995":"lr = LogisticRegression(random_state=42, n_jobs=-1)\nparams = {\"solver\": [\"newton-cg\", \"sag\", \"saga\"]}\ngrid = GridSearchCV(lr, params, n_jobs=-1, verbose=5)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","6f97bc39":"print(\"Default scores:\")\nlr.fit(X_train, y_train)\nprint(\"Train score:\", lr.score(X_train, y_train))\nprint(\"Validation score:\", lr.score(X_validate, y_validate))","617abfc0":"pd.DataFrame(grid.cv_results_)","0b30bf3d":"y_pred = lr.predict(X_validate)\n\naccuracy[\"Logistic Regression\"] = accuracy_score(y_validate, y_pred)\nf1[\"Logistic Regression\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, lr.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","cb07dec0":"y_pred = lr.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Logistic Regression\")\nplt.show()","573b56cb":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = lr.predict_proba(X_validate)\n\nprecision[\"Logistic Regression\"], recall[\"Logistic Regression\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Logistic Regression\"], precision[\"Logistic Regression\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Logisitc Regression\")\nplt.show()","1d943538":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Logistic Regression\"], tpr[\"Logistic Regression\"], where=\"post\")\n\nplt.title(\"ROC curve - Logistic Regression\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","0178d541":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","b42efbb0":"dtc = DecisionTreeClassifier(random_state=42)\nparameters = [{\"criterion\": [\"gini\", \"entropy\"], \"max_depth\": [5, 10, 15, 30]}]\ngrid = GridSearchCV(dtc, parameters, verbose=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","48e7c581":"print(\"Default scores:\")\ndtc.fit(X_train, y_train)\nprint(\"Train score:\", dtc.score(X_train, y_train))\nprint(\"Validation score:\", dtc.score(X_validate, y_validate))","4e4eded7":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","dc69c1f1":"y_pred = dtc.predict(X_validate)\n\naccuracy[\"Decision Tree\"] = accuracy_score(y_validate, y_pred)\nf1[\"Decision Tree\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, dtc.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","786987b8":"y_pred = dtc.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Decision Tree\")\nplt.show()","fbf0d3c8":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n\nimportances.iloc[:,0] = dtc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()","5dc45532":"fig, ax = plt.subplots(figsize=(20, 10))\nplot_tree(dtc, max_depth=4, fontsize=10, feature_names=X_train.columns.to_list(), class_names = True, filled=True)\nplt.show()","4c39813d":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = dtc.predict_proba(X_validate)\n\nprecision[\"Decision Tree\"], recall[\"Decision Tree\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Decision Tree\"], tpr[\"Decision Tree\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Decision Tree\"], precision[\"Decision Tree\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Decision Tree\")\nplt.show()","2821f2af":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Decision Tree\"], tpr[\"Decision Tree\"], where=\"post\")\n\nplt.title(\"ROC curve - Decision Tree\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","5ad7f3b5":"sample = X\ny_sample = sample[\"Severity\"]\nX_sample = sample.drop(\"Severity\", axis=1)\n\nX_train, X_validate, y_train, y_validate = train_test_split(X_sample, y_sample, random_state=42)\nprint(X_train.shape, y_train.shape)\nprint(X_validate.shape, y_validate.shape)","c650ccad":"rfc = RandomForestClassifier(n_jobs=-1, random_state=42)\nparameters = [{\"n_estimators\": [50, 100, 200, 500], \"max_depth\": [5, 10, 15, 30]}]\ngrid = GridSearchCV(rfc, parameters, verbose=5, n_jobs=-1)\ngrid.fit(X_train, y_train)\n\nprint(\"Best parameters scores:\")\nprint(grid.best_params_)\nprint(\"Train score:\", grid.score(X_train, y_train))\nprint(\"Validation score:\", grid.score(X_validate, y_validate))","e797dca8":"print(\"Default scores:\")\nrfc.fit(X_train, y_train)\nprint(\"Train score:\", rfc.score(X_train, y_train))\nprint(\"Validation score:\", rfc.score(X_validate, y_validate))","869f5d72":"pd.DataFrame(grid.cv_results_).sort_values(by=\"rank_test_score\")","5be626f2":"y_pred = rfc.predict(X_validate)\n\naccuracy[\"Random Forest\"] = accuracy_score(y_validate, y_pred)\nf1[\"Random Forest\"] = f1_score(y_validate, y_pred, average=\"macro\")\n\nprint(classification_report(y_train, rfc.predict(X_train)))\nprint(classification_report(y_validate, y_pred))","53e06aad":"y_pred = rfc.predict(X_validate)\nconfmat = confusion_matrix(y_true=y_validate, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.title(\"Confusion Matrix - Random Forest\")\nplt.show()","315e7f6a":"importances = pd.DataFrame(np.zeros((X_train.shape[1], 1)), columns=[\"importance\"], index=X_train.columns)\n\nimportances.iloc[:,0] = rfc.feature_importances_\n\nimportances = importances.sort_values(by=\"importance\", ascending=False)[:30]\n\nplt.figure(figsize=(15, 10))\nsns.barplot(x=\"importance\", y=importances.index, data=importances)\nplt.show()","f92fa948":"Y = label_binarize(y_validate, classes=[1, 2, 3, 4])\n\ny_score = rfc.predict_proba(X_validate)\n\nprecision[\"Random Forest\"], recall[\"Random Forest\"], _ = precision_recall_curve(Y.ravel(), y_score.ravel())\nfpr[\"Random Forest\"], tpr[\"Random Forest\"], _ = roc_curve(Y.ravel(), y_score.ravel())\n\nplt.figure(figsize=(18, 10))\nplt.step(recall[\"Random Forest\"], precision[\"Random Forest\"], where=\"post\")\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR Curve - Random Forest\")\nplt.show()","ad31ec6b":"plt.figure(figsize=(18, 10))\nplt.step(fpr[\"Random Forest\"], tpr[\"Random Forest\"], where=\"post\")\n\nplt.title(\"ROC curve - Random Forest\")\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.show()","94381142":"plt.figure(figsize=(20, 8))\nplt.title(\"Accuracy on Validation set for each model\")\nsns.barplot(list(range(len(accuracy))), list(accuracy.values()))\nplt.xticks(range(len(accuracy)), labels=accuracy.keys())\nplt.show()","5364d1fa":"plt.figure(figsize=(20, 8))\nplt.title(\"F1 Score on Validation set for each model\")\nsns.barplot(list(range(len(f1))), list(f1.values()))\nplt.xticks(range(len(f1)), labels=f1.keys())\nplt.show()","a6562a97":"plt.figure(figsize=(15, 8))\nfor key in f1.keys():\n    plt.step(recall[key], precision[key], where=\"post\", label=key)\n\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"PR curve\")\nplt.legend()\nplt.show()","add5a5a4":"plt.figure(figsize=(15, 8))\nfor key in f1.keys():\n    plt.step(fpr[key], tpr[key], where=\"post\", label=key)\n\nplt.xlabel(\"False Positive Rate\")\nplt.ylabel(\"True Positive Rate\")\nplt.xlim([0, 1])\nplt.ylim([0, 1.01])\nplt.title(\"ROC curve\")\nplt.legend()\nplt.show()","aa947273":"sample = X_test\ny_test_sample = sample[\"Severity\"]\nX_test_sample = sample.drop(\"Severity\", axis=1)\n\ny_pred = rfc.predict(X_test_sample)\n\nprint(classification_report(y_test_sample, y_pred))\n\nconfmat = confusion_matrix(y_true=y_test_sample, y_pred=y_pred)\n\nindex = [\"Actual Severity 1\", \"Actual Severity 2\", \"Actual Severity 3\", \"Actual Severity 4\"]\ncolumns = [\"Predicted Severity 1\", \"Predicted Severity 2\", \"Predicted Severity 3\", \"Predicted Severity 4\"]\nconf_matrix = pd.DataFrame(data=confmat, columns=columns, index=index)\nplt.figure(figsize=(8, 5))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"YlGnBu\")\nplt.show()","d842e1ac":"In most frequent cases the weather condition is clear.","8fb8461f":"<a id=\"5\"><\/a>\n# Results","22c518b1":"<a id=\"3.8\"><\/a>\n## 3.8 Feature scaling\n\nIn this section we are going to scale and normalize the features.\n\nTo improve the performance of our models, we normalized the values of the continuous features.","2dec5127":"As we can see from the map and the plot above California is the state with the highest number of accidents, then we have Texas and Florida.","5463215b":"<a id=\"2.5\"><\/a>\n## 2.5 Weather condition histogram\n\nWith the following code we can plot the histogram of the weather condition column.","a25e1850":"We can see that there is one record without side, so we can drop it.","f75a7da6":"Since a lot of records do not have informations about Precipitation, we are going to drop the feature.\n\nFor numerical features we are going to fill the missing features with the mean, while for categorical features like City, Wind_Direction, Weather_Condition and Civil_Twilight, we are going to delete the records with missing informations.","ebaabe94":"Let's instead analyze Pressure and Visibility:","1c679556":"<a id=\"3.1\"><\/a>\n## 3.1 Feature addition\n\nWe decided to decompose the Start_Time feature in year, month, day, weekday, hour and minute, in order to feed them to the models.","99bd088d":"<a id=\"3.5\"><\/a>\n## 3.5 Handle erroneous and missing values\n\nHere we are going to clean the dataset from erroneous or missing values.\n\nLet's start looking to the Side column:","09b542fb":"<a id=\"4.1\"><\/a>\n## 4.1 Logistic Regression","64ba046e":"To do so, we are going to replace them with a more generic description:","29cc142c":"<a id=\"2.4\"><\/a>\n## 2.4 Medium distance by severity\n\nLet's now analyze the medium distance of an accident based on its severity.","e2a2c3a2":"<a id=\"3.7\"><\/a>\n## 3.7 Handle unbalanced data","07e84a31":"As we can see from the plot above, the days with the most accidents are working days, while in the weekend we have a frequency of at least 2\/3 less. This may be due to the fact that during the weekend there are fewer cars on the road.","6a7c69bf":"<a id=\"2.6\"><\/a>\n## 2.6 Number of accidents for weekday\n\nHere is a plot of the number of accidents appened in each weekday.","2babe567":"Let's first encode the boolean values in a numerical form.","a22b243f":"From the matrix we can see that the start and end GPS coordinates of the accidents are highly correlated. \n\nIn fact, from the medium distance shown before, the end of the accident is usually close to the start, so we can consider just one of them for the machine learning models.\n\nMoreover, the wind chill (temperature) is directly proportional to the temperature, so we can also drop one of them.\n\nWe can also see that the presence of a traffic signal is slightly correlated to the severity of an accident meaning that maybe traffic lights can help the traffic flow when an accident occurs.\n\nFrom the matrix we can also note that we couldn't compute the covariance with Turning_Loop, and that's because it's always False.","18f92739":"<a id=\"1.2\"><\/a>\n# 1.2 Required libraries","0c7d5ae6":"<a id=\"3.4\"><\/a>\n## 3.4 Drop duplicates\n\nIn this section we are going to check if there are some duplicates in the dataset.","7bf4998a":"### Table of content\n1. [OVERVIEW AND IMPORTING DATA](#1) \n    \n    1.1 [Overview](#1.1) \n    \n    1.2 [Required  Libraries](#1.2)\n    \n    <br>    \n2. [Exploratory Data Analysis (EDA)](#2)\n\n    2.1 [Number of Accidents per State](#2.1)\n    \n    2.2 [Most frequent words in the description](#2.2)\n    \n    2.3 [Most frequent road features](#2.3)\n    \n    2.4 [Medium distance by severity](#2.4)\n    \n    2.5 [Weather condition histogram](#2.5)\n    \n    2.6 [Number of accidents for weekday](#2.6)\n    \n    <br>    \n3. [DATA PREPROCESSING](#3)\n    \n    3.1 [Feature addition](#3.1)\n    \n    3.2 [Check correlation between features](#3.2)\n    \n    3.3 [Feature selection](#3.3)\n    \n    3.4 [Drop duplicates](#3.4)\n    \n    3.5 [Handle erroneous and missing values](#3.5)\n    \n    3.6 [Check features variance](#3.6)\n    \n    3.7 [Handle unbalanced data](#3.7)\n    \n    3.8 [Feature scaling](#3.8)\n    \n    3.9 [Feature encoding](#3.9)\n    \n    <br>\n4. [MODEL](#4)\n    \n    4.1 [Logistic regression](#4.1)\n    \n    4.2 [Decision Tree](#4.2)\n    \n    4.3 [Random Forest](#4.3) \n    \n    <br>\n5. [RESULT](#5)","8798f5f6":"<a id=\"2.1\"><\/a>\n## 2.1 Number of Accidents per State\n\nWith the following code we are going to create a map of each state of the US with a color based on the number of accidents present in the dataset for that state.","b2d713f3":"# USA Car Accidents Severity Prediction","21e11bbb":"As we can see, we can group the values like we did with Weather_Condition:","3780b868":"Now we can encode the categorical features using the method `get_dummies()` which converts the features with the one-hot encoding.","76909529":"<a id=\"4.2\"><\/a>\n## 4.2 Decision Tree","c4686173":"<a id=\"4\"><\/a>\n# 4 Model\n\nIn this section we will test sevaral models to identify the best one for this task.\n\nIn particular, we will use the following models:\n- Logistic Regression;\n- Decision Tree\n- Random Forest\n\nMoreover, we are going to search the best hyperparameters to produce the best results for each model. In the end, we will show the performance of each model using different metrics: precision, recall, accuracy.","1bd7c2f2":"<a id=\"3.2\"><\/a>\n## 3.2 Check correlation between features\n\n\nIn the next block is presented the correlation matrix between all the possible features, in the form of an heatmap. \n\nWith that we can observe the correlation between the different features of the dataset, in order to check if some features are highly correlated and remove one of them.","fa2950e8":"Now, remains only to encode the *City* feature. In order to, reduce the usage of memory and the number of features we used the `BinaryEncoder` included in the library category_encoders.","982aaa91":"<a id=\"3\"><\/a>\n# 3 Data Preprocessing\n\nData preprocessing involves transforming raw data to well-formed data sets so that data mining analytics can be applied. Raw data is often incomplete and has inconsistent formatting. The adequacy or inadequacy of data preparation has a direct correlation with the success of any project that involve data analyics. In this phase we are going to process the dataset in order to make it usable for the machine learning models.","880ddc77":"<a id=\"2.2\"><\/a>\n## 2.2 Most frequent words in the description of an accident with severity 4\n\nWe are going to compute the most frequent words in the description column of the accidents with a value of severity equal to 4, using some stopwords from the english language.","910a3e67":" We can see that the minimum value is 0, meaning that some records are missing them and replaced them by putting zeros.\n For this reason, we are going to drop the records with missing values for these two columns.","9d70e76a":"Let's check also the Wind_Direction field:","db3de64a":"# INTRODUCTION\n\n### Motivation\nThe economic and societal impact of traffic accidents cost U.S. citizens hundreds of billions of dollars every year. And a large part of losses is caused by a small number of serious accidents. Reducing traffic accidents, especially serious accidents, is nevertheless always an important challenge. The proactive approach, one of the two main approaches for dealing with traffic safety problems, focuses on preventing potential unsafe road conditions from occurring in the first place. For the effective implementation of this approach, accident prediction and severity prediciton are critical. If we can identify the patterns of how these serious accidents happen and the key factors, we might be able to implement well-informed actions and better allocate financial and human resources.\n\n### Objectives\nThe first objective of this project is to recognize key factors affecting the accident severity. The second one is to develop a model that can accurately predict accident severity. To be specific, for a given accident, without any detailed information about itself, like driver attributes or vehicle type, this model is supposed to be able to predict the likelihood of this accident being a severe one. The accident could be the one that just happened and still lack of detailed information, or a potential one predicted by other models. Therefore, with the sophisticated real-time traffic accident prediction solution developed by the creators of the same dataset used in this project, this model might be able to further predict severe accidents in real-time.\n\n### Process\nData cleaning was first performed to detect and handle corrupt or missing records. EDA (Exploratory Data Analysis) and feature engineering were then done over most features. Finally, Logistic regression, Decision Tree, and Random Forest Classifier were used to develop the predictive model.\n\nIt is worth noting that the severity in this project is \"**an indication of the effect the accident has on traffic**\", rather than the injury severity that has already been thoroughly studied by many articles. Another thing is that the final model is dependent on only a small range of data attributes that are easily achievable for all regions in the United States and before the accident really happened.\n\n### Key Findings\n* California is the state with the highest number of accidents, then we have Texas and Florida.\n* Most of the accidents occured near a traffic signal, expecially where a junction or a crossing was present.\n* Distance of the accident is more or less proportional to the severity, and accidents with severity 4 have the longest distance.\n* Most frequent cases of the weather condition is clear.\n* Days with the most accidents are working days, while in the weekend we have a frequency of at least 2\/3 less.\n* An accident is much less likely to be severe if it happens near traffic signal while more likely if near junction.\n\n### Dataset Overview\nThis is a countrywide car accident dataset, which covers 49 states of the USA. The accident data are collected from February 2016 to Dec 2020, using multiple APIs that provide streaming traffic incident (or event) data. These APIs broadcast traffic data captured by a variety of entities, such as the US and state departments of transportation, law enforcement agencies, traffic cameras, and traffic sensors within the road-networks. Currently, there are about 1.5 million accident records in this dataset. Check here to learn more about this dataset.\n\nLink for kaggle dataset: https:\/\/www.kaggle.com\/sobhanmoosavi\/us-accidents\n\n","c0390832":"<a id=\"2\"><\/a>\n# 2 Exploratory Data Analysis (EDA)\n\nExploratory Data Analysis refers to the critical process of performing initial investigations on data so as to discover patterns,to spot anomalies,to test hypothesis and to check assumptions with the help of summary statistics and graphical representations.\nIn this first part of this notebook we are going to do an EDA of the dataset.","b5c3b9c9":"Even though Precipitation and Pressure have a small variance, there is no need to drop them since they usually have small increments.","e6a654a9":"<a id=\"2.3\"><\/a>\n## 2.3 Most frequent road features\n\nNow we are going to analyze which are the most frequent road features in accidents.","023df48c":"Next, let's analyze the missing values:","d4bd4820":"We can see that the most used word in the description is *closed*. Subsequent words are *accident*, *due* and *road*.","91b4136f":"First of all, we show the number of unique classes for each categorical feature.","6d8854f4":"Finally, we can merge the two dataframes and obtain the final dataframe X with the categorical features encoded.","8b02825f":"If we analyze the weather conditions, we can see that there are lots of them, so it's better to reduce the number of unique conditions.","baedd391":"<a id=\"3.6\"><\/a>\n## 3.6 Check features variance\n\nIn this section we are going to check the variance for each feature in order to remove features with a very low variance beacuse they can't help to discriminate instances.","7a4c38e8":"In this graph we can see that the distance of the accident is more or less proportional to the severity, and in fact accidents with severity 4 have the longest distance.","af86bbbf":"As we can see, most of the accidents occured near a traffic signal, expecially where a junction or a crossing was present. \n\nThe fourth most common road feature, instead, was the presence of a nearby station, probably because of the high presence of vehicles.","2cce2f74":"<a id=\"3.9\"><\/a>\n## 3.9 Feature encoding\n\nFinally, in this section we are going to encode the categorical features.","a7669230":"<a id=\"1.1\"><\/a>\n## 1.1 Overview the dataset\nDetails about features in the dataset:\n\n**Traffic Attributes (12)**:\n\n* **ID**: This is a unique identifier of the accident record.\n\n* **Source**: Indicates source of the accident report (i.e. the API which reported the accident.).\n\n* **TMC**: A traffic accident may have a Traffic Message Channel (TMC) code which provides more detailed description of the event.\n\n* **Severity**: Shows the severity of the accident, a number between 1 and 4, where 1 indicates the least impact on traffic (i.e., short delay as a result of the accident) and 4 indicates a significant impact on traffic (i.e., long delay).\n\n* **Start_Time**: Shows start time of the accident in local time zone.\n\n* **End_Time**: Shows end time of the accident in local time zone.\n\n* **Start_Lat**: Shows latitude in GPS coordinate of the start point.\n\n* **Start_Lng**: Shows longitude in GPS coordinate of the start point.\n\n* **End_Lat**: Shows latitude in GPS coordinate of the end point.\n\n* **End_Lng**: Shows longitude in GPS coordinate of the end point.\n\n* **Distance(mi)**: The length of the road extent affected by the accident.\n\n* **Description**: Shows natural language description of the accident.\n\n**Address Attributes (9)**:\n\n* **Number**: Shows the street number in address field.\n\n* **Street**: Shows the street name in address field.\n\n* **Side**: Shows the relative side of the street (Right\/Left) in address field.\n\n* **City**: Shows the city in address field.\n\n* **County**: Shows the county in address field.\n\n* **State**: Shows the state in address field.\n\n* **Zipcode**: Shows the zipcode in address field.\n\n* **Country**: Shows the country in address field.\n\n* **Timezone**: Shows timezone based on the location of the accident (eastern, central, etc.).\n\n**Weather Attributes (11)**:\n\n* **Airport_Code**: Denotes an airport-based weather station which is the closest one to location of the accident.\n\n* **Weather_Timestamp**: Shows the time-stamp of weather observation record (in local time).\n\n* **Temperature(F)**: Shows the temperature (in Fahrenheit).\n\n* **Wind_Chill(F)**: Shows the wind chill (in Fahrenheit).\n\n* **Humidity(%)**: Shows the humidity (in percentage).\n\n* **Pressure(in)**: Shows the air pressure (in inches).\n\n* **Visibility(mi)**: Shows visibility (in miles).\n\n* **Wind_Direction**: Shows wind direction.\n\n* **Wind_Speed(mph)**: Shows wind speed (in miles per hour).\n\n* **Precipitation(in)**: Shows precipitation amount in inches, if there is any.\n\n* **Weather_Condition**: Shows the weather condition (rain, snow, thunderstorm, fog, etc.).\n\n**POI Attributes (13)**:\n\n* **Amenity**: A Point-Of-Interest (POI) annotation which indicates presence of amenity in a nearby location.\n\n* **Bump**: A POI annotation which indicates presence of speed bump or hump in a nearby location.\n\n* **Crossing**: A POI annotation which indicates presence of crossing in a nearby location.\n\n* **Give_Way**: A POI annotation which indicates presence of give_way sign in a nearby location.\n\n* **Junction**: A POI annotation which indicates presence of junction in a nearby location.\n\n* **No_Exit**: A POI annotation which indicates presence of no_exit sign in a nearby location.\n\n* **Railway**: A POI annotation which indicates presence of railway in a nearby location.\n\n* **Roundabout**: A POI annotation which indicates presence of roundabout in a nearby location.\n\n* **Station**: A POI annotation which indicates presence of station (bus, train, etc.) in a nearby location.\n\n* **Stop**: A POI annotation which indicates presence of stop sign in a nearby location.\n\n* **Traffic_Calming**: A POI annotation which indicates presence of traffic_calming means in a nearby location.\n\n* **Traffic_Signal**: A POI annotation which indicates presence of traffic_signal in a nearby location.\n\n* **Turning_Loop**: A POI annotation which indicates presence of turning_loop in a nearby location.\n\n**Period-of-Day (4)**:\n\n* **Sunrise_Sunset**: Shows the period of day (i.e. day or night) based on sunrise\/sunset.\n\n* **Civil_Twilight**: Shows the period of day (i.e. day or night) based on civil twilight.\n\n* **Nautical_Twilight**: Shows the period of day (i.e. day or night) based on nautical twilight.\n\n* **Astronomical_Twilight**: Shows the period of day (i.e. day or night) based on astronomical twilight.","6d7fd41e":"# Dataset import\n\nIn the first place we are going to import the dataset using pandas.","1fcff634":"<a id=\"4.3\"><\/a>\n## 4.3 Random Forest","0038e882":"<a id=\"3.3\"><\/a>\n## 3.3 Feature selection\n\nHere is the process of feature selection, in order to select the best features from which our models can learn.\n\nFrom the observations made with the correlation matrix, we are going to drop the following features:\n- End_Lat and End_Lng\n- Wind Chill\n\nMoreover, we are going to drop the following features:\n- ID, Source: since they don't carry any information for the severity\n- TMC: because it could already contains information about the accident severity\n- Start_Time: because it was decomposed by the time features added before (day, month, weekday)\n- End_Time: beause we cannot know in advance when the traffic flow will become regular again\n- Description: most description only report the name of the road of the accident, and so we decided to omit this feature for semplicity\n- Number, Street, County, State, Zipcode, Country: because we just focus on the City where the accident happened\n- Timezone, Airport_Code, Weather_Timestamp: because they are not useful for our task\n- Turning_Loop: since it's always False\n- Sunrise_Sunset, Nautical_Twilight, Astronomical_Twilight: because they are redundant","83ba110e":"The severity attribute as we can see from the previous plot is highly unbalanced, the number of accident with the severity 1 is very small instead the number of accident with severity 2 is much higher.\n\nSo, in order to balance the data we are going to undersample all the categories to the number of records of the minority category, in this case the severity 1.\nWe thought this was a good choice since this leaves us with a good amount of records for each category, which is ~25k records"}}