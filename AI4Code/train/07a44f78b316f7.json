{"cell_type":{"82359e3e":"code","4452f511":"code","8bf1b159":"code","252cdee8":"code","92b7ec21":"code","63896bc4":"code","fab638e0":"code","6f066504":"code","80ba230a":"code","5a7b23ca":"code","b405dd3a":"code","2690eee5":"code","28369484":"code","7a382cde":"code","b60a92f3":"code","e36c17a3":"code","6f06423a":"code","1dc57f4a":"code","6f849c8f":"code","cc616d07":"code","cf28fc1c":"code","6f8a1c32":"code","d9089027":"code","01890e48":"code","a0c4436f":"code","7701d280":"code","27b6c78d":"code","948f5d11":"code","166e65c5":"code","9def273c":"code","55481d35":"code","7c95eeaf":"code","951c4ef1":"code","b23ff978":"code","566b7f0d":"code","890b6bf1":"code","aadd781d":"code","b99a9f88":"code","576d856b":"code","edd2c90d":"code","a6f5f3ba":"code","836aa836":"markdown","69ae92b1":"markdown","3289a1f7":"markdown","3aaec70e":"markdown","1ea092ae":"markdown","3ecae167":"markdown","a9fec2a9":"markdown","cd6ef385":"markdown","371da7ab":"markdown","db25ea93":"markdown","184e76e8":"markdown","63b25454":"markdown","5457b16e":"markdown","c218a356":"markdown","3456e6a5":"markdown","8615f3af":"markdown","cbd0d993":"markdown","d6226d9d":"markdown","673d3b3c":"markdown","00d1c37e":"markdown","6230a873":"markdown","9c5279a3":"markdown","0eba3e73":"markdown","d65e1050":"markdown","1585de8f":"markdown","a2b238e0":"markdown","8276e594":"markdown","23f436aa":"markdown","7a4405f1":"markdown","87e4ec2c":"markdown","61fb4926":"markdown","02e72e9e":"markdown","3de0cb93":"markdown","0e43c2e7":"markdown","dd62a37d":"markdown","e698ccee":"markdown","d3427eb2":"markdown"},"source":{"82359e3e":"from IPython.display import Image\nImage(url = 'https:\/\/atrium.ai\/wp-content\/uploads\/elementor\/thumbs\/real_cost_retention-ooi87zuk2wbz6qkh6nglnfwpm2vkbw4t3idvdyf3bc.jpg')","4452f511":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport matplotlib\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')","8bf1b159":"df = pd.read_csv('..\/input\/predicting-churn-for-bank-customers\/Churn_Modelling.csv')","252cdee8":"df.head()","92b7ec21":"df.shape","63896bc4":"# Dropping the first three columns, as they are not relevant in predictive analysis:\ndf.drop(labels=['RowNumber','CustomerId','Surname'], axis=1, inplace=True)","fab638e0":"df.info()","6f066504":"for col in ['HasCrCard', 'IsActiveMember']: \n    df[col] = df[col].astype('object')\nprint(df.dtypes) ","80ba230a":"plt.figure(figsize=(8,6))\nsns.countplot(df.Exited,palette=['#D7263D','#27FB6B'])\nplt.show()","5a7b23ca":"print(f'Ratio of customers who exited: {(df.Exited==1).sum()\/len(df):.3f}')","b405dd3a":"fig_data=['Geography', 'Gender', 'Tenure','NumOfProducts', 'HasCrCard', \n                  'IsActiveMember']\nq=1\nplt.figure(figsize=(16,12))\n# Plot a grid with count plots of all categorical variables\nfor j in fig_data:\n    plt.subplot(2,3,q)\n    ax=sns.countplot(df[j],hue=df.Exited, palette=['#D7263D','#27FB6B'])\n    plt.xlabel(j)\n    q+=1\nplt.show()","2690eee5":"'''\nA heat map showing the correlation between variables\n'''\n\n# Get dummies of the non-binary categorical variables\ngender_dummies = pd.get_dummies(df.Gender,drop_first=True,dtype='int32')\ngeography_dummies = pd.get_dummies(df.Geography,drop_first=True,dtype='int32')\n\n# Initialize a new data frame\ndf_new=pd.DataFrame()\n# Loop through all columns\nfor col in df.columns:\n    # If data type is not float, add the categorical variable\n    if df[col].dtype!='float64':\n        # If non-binary, add the dummies columns\n        if col == 'Gender':\n            df_new['Male'] = gender_dummies\n        elif col =='Geography':\n            df_new=pd.concat([df_new,geography_dummies],axis=1)\n        # If binary category, add the column as an integer data type  \n        else:\n            df_new[col] = df[col].astype('int32')\n    # If data type is float, simply add it to the new data frame\n    else:\n        df_new[col] = df[col]\n\n# Get the correlation matrix and plot it\ncorr = df_new.corr()\nplt.figure(figsize=(10,8))\nsns.heatmap(corr)\nplt.show()","28369484":"from sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n\n# Column Transformer to scale numerical data, and encode categorical non-binary columns\nct = ColumnTransformer([\n     (\"scaling\", StandardScaler(), ['CreditScore', 'Age','Balance','EstimatedSalary',\n                                   'Tenure','NumOfProducts']),\n     (\"onehot\", OneHotEncoder(sparse=False,drop='if_binary'), ['Gender', 'Geography'])\n])","7a382cde":"# Save a series of the target variable\ndata_features = df.drop('Exited', axis=1)","b60a92f3":"from sklearn.model_selection import train_test_split\n\n# Split data into training and test datasets\nX_train, X_test, y_train, y_test = train_test_split(data_features, df.Exited)","e36c17a3":"'''\nPerform grid search to find best the parameters for\nRandomForestClassifier\n'''\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.pipeline import Pipeline\n\n# Define the parameter grid\nparam_grid = {\n   'clf__max_features': ['sqrt', 'log2'],\n    'clf__n_estimators': [100, 200],\n    'clf__max_depth': [3, 5, None], \n}\n\n# Set up the pipeline\npipe = Pipeline([\n    ('preprocess', ct),\n    ('clf',RandomForestClassifier())\n])\n\n# Grid search, using recall as the score to maximise\ngrid=GridSearchCV(estimator=pipe, param_grid=param_grid,cv=10,\n                  scoring='recall_macro',return_train_score=True,\n                  verbose=0)","6f06423a":"# Fit the grid search on the training data\ngrid.fit(X_train, y_train)","1dc57f4a":"grid.best_params_","6f849c8f":"grid.best_score_","cc616d07":"# Use the model on the test data\ntest_predictions = grid.best_estimator_.predict(X_test)","cf28fc1c":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\ntest_accuracy = accuracy_score(y_test, test_predictions)\ntest_precision = precision_score(y_test, test_predictions)\ntest_recall = recall_score(y_test, test_predictions)\ntest_f1_score = f1_score(y_test, test_predictions)\ntest_roc_auc_score = roc_auc_score(y_test, test_predictions)\n\nprint(\"Accuracy on test data: {:.3f}%\".format(test_accuracy*100))\nprint(\"Precision on test data: {:.3f}%\".format(test_precision*100))\nprint(\"Recall on test data: {:.3f}%\".format(test_recall*100))\nprint(\"F1 Score on test data: {:.3f}%\".format(test_f1_score*100))\nprint(\"AUC Score on test data: {:.3f}\".format(test_roc_auc_score))","6f8a1c32":"from sklearn.metrics import confusion_matrix\n\ndef get_conf_matrix(y_test, y_pred):    \n    # Get confusion matrix\n    data = confusion_matrix(y_test, y_pred) \n    # Build the confusion matrix as a dataframe table\n    cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test)) \n    cm.index.name = 'Observed'\n    cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    # Plot a heatmap\n    sns.heatmap(cm, annot=True, fmt=\"d\", annot_kws={\"size\": 12}) \n    plt.title(\"Confusion Matrix\")\n    plt.show()\nget_conf_matrix(y_test, test_predictions)","d9089027":"from sklearn.inspection import permutation_importance\n# Obtain feature importance\nr = permutation_importance(grid.best_estimator_, df, df.Exited.astype('int32'), \n                           n_repeats=10, random_state=0)","01890e48":"# Print the mean importance and the margin of error, for each variable\nfor i in r.importances_mean.argsort()[::-1]:\n    print(f\"{df.columns[i]:<28}\"\n          f\"{r.importances_mean[i]:.3f}\"\n          f\" +\/- {r.importances_std[i]:.3f}\")","a0c4436f":"# Transform the train data using the previously defined metrics\nct.fit(X_train)\nX_train_trans = ct.transform(X_train)","7701d280":"from scipy.cluster.hierarchy import linkage\n\nlinkage_matrix = linkage(X_train_trans, method='complete', metric='euclidean')","27b6c78d":"from scipy.cluster.hierarchy import dendrogram, set_link_color_palette\n\n'''\n'''\ndef fancy_dendrogram(*args, **kwargs):\n    max_d = kwargs.pop('max_d', None)\n    if max_d and 'color_threshold' not in kwargs:\n        kwargs['color_threshold'] = max_d\n    annotate_above = kwargs.pop('annotate_above', 0)\n\n    ddata = dendrogram(*args, **kwargs)\n\n    if not kwargs.get('no_plot', False):\n        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n        plt.xlabel('sample index or (cluster size)')\n        plt.ylabel('distance')\n        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n            x = 0.5 * sum(i[1:3])\n            y = d[1]\n            if y > annotate_above:\n                plt.plot(x, y, 'o', c=c)\n                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n                             textcoords='offset points',\n                             va='top', ha='center')\n        if max_d:\n            plt.axhline(y=max_d, c='k')\n    return ddata","948f5d11":"plt.figure(figsize=(9,7))\nfancy_dendrogram(\n    linkage_matrix,\n    truncate_mode='lastp',\n    p=8,\n    leaf_rotation=90.,\n    leaf_font_size=12.,\n    show_contracted=True,\n    annotate_above=6,\n)\nplt.show() ","166e65c5":"last = linkage_matrix[-10:, 2]\nlast_rev = last[::-1]\nidxs = np.arange(1, len(last) + 1)\nplt.plot(idxs, last_rev)\n\n# 2nd derivative of the distances\nacceleration = np.diff(last, 2)  \nacceleration_rev = acceleration[::-1]\nplt.plot(idxs[:-2] + 1, acceleration_rev)\nplt.show()\n# If idx 0 is the max of this we want 2 clusters\nk = acceleration_rev.argmax() + 2  \nprint (\"clusters:\", k)","9def273c":"from scipy.cluster.hierarchy import fcluster\nclusters_smaller  = fcluster(linkage_matrix, 4, criterion='maxclust')\n# Show the counts of each cluster\nnp.unique(clusters_smaller, return_counts=True)","55481d35":"df_clusters=pd.concat([X_train,y_train],axis=1)\n\nGroup1 = df_clusters[clusters_smaller == 1]\nGroup2 = df_clusters[clusters_smaller == 2]\nGroup3 = df_clusters[clusters_smaller == 3]\nGroup4 = df_clusters[clusters_smaller == 4]","7c95eeaf":"Group2.head()","951c4ef1":"# Count Plots of customers who exited, for each group\nf, axes = plt.subplots(1, 4, figsize=(9, 6), sharey = True)\nax1=sns.countplot(Group1.Exited,ax=axes[0], palette=['#D7263D','#27FB6B'])\nax2=sns.countplot(Group2.Exited,ax=axes[1], palette=['#D7263D','#27FB6B'])\nax3=sns.countplot(Group3.Exited,ax=axes[2], palette=['#D7263D','#27FB6B'])\nax4=sns.countplot(Group4.Exited,ax=axes[3], palette=['#D7263D','#27FB6B'])","b23ff978":"# Print percentages of exited customers, for each group\nfor i,group in enumerate([Group1, Group2, Group3, Group4]):\n    print(f'Group {i+1} customer attrition rate :',f'{group.Exited.sum()\/len(group)*100:.2f}%')","566b7f0d":"categorical_data=['Geography', 'Gender', 'Tenure','NumOfProducts', 'HasCrCard', \n                  'IsActiveMember']\nq=1\nplt.figure(figsize=(20,20))\nfor j in categorical_data:\n    plt.subplot(3,3,q)\n    ax=sns.countplot(X_train[j],hue=clusters_smaller)\n    plt.xlabel(j)\n    q+=1\nplt.show()","890b6bf1":"numerical_data=['Age','CreditScore','Balance','EstimatedSalary']\n\nq=1\nplt.figure(figsize=(14,14))\n\nfor col in numerical_data:\n    plt.subplot(2,2,q)\n    ax=sns.boxplot(y=X_train[col], x=pd.Series(clusters_smaller), hue = clusters_smaller)\n    plt.xlabel('Group')\n    q+=1\nplt.show()\nplt.show()","aadd781d":"exted = df[df.Exited==1]\nothers = df[df.Exited!=1][:2000]\nnew_df = pd.concat([exted, others],axis=0)\n\nX = new_df.drop('Exited',axis=1)\n\ngrid=GridSearchCV(estimator=pipe, param_grid=param_grid,cv=5,\n                  scoring='f1',return_train_score=True,\n                  verbose=0)\n\nX_train, X_test, y_train, y_test = train_test_split(X, new_df.Exited)\n\ngrid.fit(X_train, y_train)","b99a9f88":"grid.best_score_","576d856b":"test_predictions = grid.best_estimator_.predict(X_test)","edd2c90d":"test_accuracy = accuracy_score(y_test, test_predictions)\ntest_precision = precision_score(y_test, test_predictions)\ntest_recall = recall_score(y_test, test_predictions)\ntest_f1_score = f1_score(y_test, test_predictions)\ntest_roc_auc_score = roc_auc_score(y_test, test_predictions)\n\nprint(\"Accuracy on test data: {:.3f}%\".format(test_accuracy*100))\nprint(\"Precision on test data: {:.3f}%\".format(test_precision*100))\nprint(\"Recall on test data: {:.3f}%\".format(test_recall*100))\nprint(\"F1 Score on test data: {:.3f}%\".format(test_f1_score*100))\nprint(\"AUC Score on test data: {:.3f}\".format(test_roc_auc_score))","a6f5f3ba":"def get_conf_matrix(y_test, y_pred):    \n    # Get confusion matrix\n    data = confusion_matrix(y_test, y_pred) \n    # Build the confusion matrix as a dataframe table\n    cm = pd.DataFrame(data, columns=np.unique(y_test), index = np.unique(y_test)) \n    cm.index.name = 'Observed'\n    cm.columns.name = 'Predicted'\n    plt.figure(figsize = (10,7))\n    # Plot a heatmap\n    sns.heatmap(cm, annot=True, fmt=\"d\", annot_kws={\"size\": 12}) \n    plt.title(\"Confusion Matrix\")\n    plt.show()\nget_conf_matrix(y_test, test_predictions)","836aa836":"Categorical and ordinal data is encoded, and continous data standardised.","69ae92b1":"This model achieves smaller accuracy, but larger recall, f1 and AUC score. While following it may lead to targeting some customers at small risk of exiting, it misses significantly less of the customers at risk of leaving, and can therefore be a better option.","3289a1f7":"The next step is to assess the model's performance on the test set.","3aaec70e":"The model is split into train and test sets, using the default shuffling and 3:1 ratio.","1ea092ae":"Reference: [SciPy Hierarchical Clustering and Dendrogram Tutorial](https:\/\/joernhees.de\/blog\/2015\/08\/26\/scipy-hierarchical-clustering-and-dendrogram-tutorial\/)","3ecae167":"Add back the dependent variable, and separate the three customer profiles","a9fec2a9":"## 2. Exploratory Data Analysis (EDA)","cd6ef385":"Some variables that should be categorical objects are integers: HasCrCard, IsActiveMember and Exited. \nTenure and NumOfProducts are ordinal variables.","371da7ab":"<h3>Summary<\/h3> Around 20% of the bank's customers in this dataset have exited. Methods for predicting customer attrition are devised, achieving a 0.702, 0.77 roc area under curve score respectively. Age is the most important feature, being positively correlated to the probability that a customer will exit. Three customer groups are identified, one of which is 14% more likely to exit than the mean. This document provides a framework for classifying the customers. Further data has the potential to significantly improve the efficiency of the models.","db25ea93":"The best model has no maximum depth, uses sqrt(n_features) when looking for best split, and has a maximum of 200 estimators (Decision Trees). For more info on the RFC: https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html","184e76e8":"## 6. Customer Profiles Analysis","63b25454":"## 5. Customer Clusters","5457b16e":"Getting a linkage matrix of the transformed data","c218a356":"<h2>7. Conclusion<\/h2>\nTwo models are designed to predict whether a customer will exit or not, depending on whether accuracy or recall are the main focus. Customers were agglomeratively clustered into 4 groups, out of which one is of particular interest. While these models could be leveraged to identifiy customers at risk of leaving, additional data could significantly improve the performance of the models.","3456e6a5":"Creating 4 flat clusters","8615f3af":"Because the recall of the first model was so low, an alternative model is plotted, which aims to improve recall by reducing the number of customers who did not exit. Levelling the categories of the target variable should decrease the model's incentive to make negative prediction.","cbd0d993":"The graph above shows hierarchically clustered groups of customers, based on the distance between them. Each black dot is an individual person.","d6226d9d":"Let's look at the feature importance, which variables are the best independent predictors:","673d3b3c":"German citizens have the highest rates of attrition among the 3 countries. Although there are more males than females in the dataset, the latter were responsible for more exits.\n\nPeople who purchased 2 products were less likely to exit than those who bought only one. Interestingly, the rates of churn of people with 3+ products are very high.\n\nNon-active members were much more likely to exit than active ones.","00d1c37e":"Age is the best predictor, followed by Number of Products.","6230a873":"Clustering has identified a particular group of 150 customers who are 90.67% likely to leave. Aggregating this model on top of the supervised learning model may improve its performance and provide additional insights into what makes clients exit, and how churn rates can be decreased. ","9c5279a3":"#### The dataset contains 10,000 bank customers, with 13 independent variables and 1 dependent variable: whether they exited the bank or not. In this document, predictive analysis is leveraged in order to predict future customer behaviour and inform decisions to arrest the attrition of bank clients.","0eba3e73":"Before performing predictive analysis, data must be cleaned and formatted. 'RowNumber','CustomerId','Surname' are not relevant in predictive analysis, and are therefore dropped.","d65e1050":"Hierarchical Clustering Dendogram","1585de8f":"The recall of the best model is 70.2%","a2b238e0":"In this section, unsupervised machine learning is leveraged in order to cluster the customers into different categories.","8276e594":"The four groups are similar in terms of the numerical variables ","23f436aa":"Age is the feature most positively correlated with churn status (~0.25).\n\nGermany customers appear to be significantly positiely correlated with a higher balance (~0.5).","7a4405f1":"## 4. Feature Importance","87e4ec2c":"Reference: [SciPy Hierarchical Clustering and Dendrogram Tutorial](https:\/\/joernhees.de\/blog\/2015\/08\/26\/scipy-hierarchical-clustering-and-dendrogram-tutorial\/)","61fb4926":"Even though the accuracy is relatively high (84.16%), recall\/sensitivity is fairly low, meaning that out of the total number of customers who exited, only 41.602% are identified.","02e72e9e":"Elbow Plot is drawn to determine the optimal number of clusters","3de0cb93":"The number of products seems to be the strongest determinant for categorising customers into the second group. However, for clients with 3 products, there is an unclear distinction being made between the second and third clusters.","0e43c2e7":"## 3. Predictive Analysis on all Customers","dd62a37d":"### Alternative Model","e698ccee":"## 1. Data cleaning","d3427eb2":"Then, GridSearchCV is performed on Random Forest Classifier, in order to find the most optimal hyper parameters."}}