{"cell_type":{"1d42b93e":"code","51071d0c":"code","20da7515":"code","23410930":"code","e36ddce7":"code","46e8dfc5":"code","b86cc1e8":"code","0da75603":"code","742e28c0":"code","7aee1523":"code","c39cc0ff":"code","e279e9a6":"code","295ba872":"code","3f784871":"code","11f08f2a":"code","128ce979":"code","aa422215":"code","852e53c9":"code","c56df200":"code","19f853f2":"code","3f06b62f":"code","402a241a":"code","9da6852b":"code","18c51e9d":"code","cf7594c7":"code","f08103c7":"code","6283ee49":"code","6dca91fe":"code","60fd0d3b":"code","27a592d1":"code","07bcab13":"code","bbc8d952":"code","59f12225":"code","c081eddf":"code","5eb53f3f":"code","9758a337":"code","36af9a5b":"code","c0998d1c":"code","b99db688":"code","fd9bd09e":"code","0c1aa88c":"code","23d1ccc8":"code","4675ca05":"code","b7ea692b":"code","27b91e71":"code","dc4225c8":"code","a33229be":"code","2dfaa334":"code","2cee8a85":"code","475f8e82":"code","3ba7cbe3":"code","296ac847":"code","656e955c":"code","b7023aad":"code","a17963ed":"code","888ff439":"code","35c5967c":"code","9a880b7b":"code","f11f0a63":"code","a6aecf84":"code","1562b89a":"code","c2265130":"code","35cad934":"code","b0a62d55":"code","86b1b938":"code","5106afc2":"code","a45b983b":"code","4fad3aa1":"code","0a2d8b2b":"code","c3b878d4":"code","8a3eb954":"code","70be093d":"code","aa935fcb":"code","3ed42147":"code","f7af3d0b":"code","a21ea680":"code","f04caf73":"code","27622cb4":"code","a13d15e7":"code","ca439d4d":"code","d3509987":"code","0c8abb11":"code","2c612a29":"code","96a7a024":"code","fd524240":"code","ae84d71a":"code","b6373a3c":"code","2f3dcdb5":"code","28baffef":"code","6830e3a9":"code","21593016":"code","5f68f940":"markdown","64f23236":"markdown","93405540":"markdown","f307e79d":"markdown","2698cdc3":"markdown","3f0f3e0a":"markdown","1bf7df58":"markdown","c6474338":"markdown","9d817cad":"markdown","db2320c4":"markdown","81c185ef":"markdown","8d1a21f8":"markdown","d30b5b40":"markdown","b4f0a191":"markdown","0861301c":"markdown","496a625e":"markdown","d135af6d":"markdown","d72bc0a6":"markdown","707ec07a":"markdown","60105e12":"markdown","5702b7f7":"markdown","f020fdd0":"markdown","f8a4db36":"markdown","53bc6873":"markdown","a92fed5c":"markdown","69fa8982":"markdown","07e8c26f":"markdown","0c60911c":"markdown","d1649823":"markdown","ebe3118c":"markdown","83666e25":"markdown","8a591fcd":"markdown","5e9fd6ae":"markdown","0af68fd1":"markdown","272a2937":"markdown","b9d2a666":"markdown","eb0c67d7":"markdown","4f1376b1":"markdown","3d6fa6e6":"markdown","814c3cfc":"markdown","5d319c45":"markdown","bbefc109":"markdown","c7dd9867":"markdown","2006e8d9":"markdown","941cc81c":"markdown","5f4bb244":"markdown","9592f4aa":"markdown","ee12232a":"markdown","31e66e91":"markdown","1d8c1c44":"markdown","b9d7c37c":"markdown","9de44eb7":"markdown","1a1760de":"markdown","0814b8e7":"markdown","bfb066fd":"markdown","6cb1c075":"markdown","fe0c5c83":"markdown","d75f2d18":"markdown","4f97838b":"markdown","58b914b2":"markdown","411e9ac9":"markdown","f8ee341b":"markdown","d3996d3c":"markdown","f77481e8":"markdown","c3dbe041":"markdown"},"source":{"1d42b93e":"# import \n# will use a familiar stack of data science libraries: pandas, numpy, matplotlib, seaborn and eventually sklearn for modeling\n\n# data manipulation\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# set a few plotting defaults\n%matplotlib inline\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.size'] = 18\nplt.rcParams['patch.edgecolor'] = 'k'\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(action='ignore') ","51071d0c":"# read in data and look at summary information\n\npd.options.display.max_columns = 150\n\n# read in DATA\ntrain = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/train.csv')\ntest = pd.read_csv('..\/input\/costa-rican-household-poverty-prediction\/test.csv')\n\ntrain.head()","20da7515":"train.select_dtypes(np.int64).nunique().value_counts().sort_index().plot.bar(color = 'blue',\n                                                                             figsize = (8, 6),\n                                                                             edgecolor = 'k', linewidth = 2);\nplt.xlabel('number of unique values');\nplt.ylabel('count');\nplt.title('count of unique values in integer columns');","23410930":"# collections \ubaa8\ub4c8 OrderedDict \ud074\ub798\uc2a4\ub97c \uc0ac\uc6a9\ud558\uba74 \ub370\uc774\ud130\uc758 \uc21c\uc11c \ubd80\uc5ec \nfrom collections import OrderedDict\n\nplt.figure(figsize = (20, 16))\nplt.style.use('fivethirtyeight')\n\n# color mapping\ncolors = OrderedDict({1: 'red', 2: 'orange', 3: 'blue', 4: 'green'})\npoverty_mapping = OrderedDict({1: 'extreme', 2: 'moderate', 3: 'vulnerable', 4: 'non vulnerable'})\n\n# iterate through the float columns\nfor i, col in enumerate(train.select_dtypes('float')):\n    ax = plt.subplot(4, 2, i + 1)\n    \n    # iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # plot each poverty level as a seperate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(),\n        ax = ax, color = color, label = poverty_mapping[poverty_level])\n    \n    plt.title(f'{col.capitalize()} Distribution');\n    plt.xlabel(f'{col}');\n    plt.ylabel('Density')\n\nplt.subplots_adjust(top = 2)","e36ddce7":"train.select_dtypes('object').head()","46e8dfc5":"mapping = {'yes': 1, 'no': 0}\n\n# apply same operation to both train and test\nfor df in [train, test]:\n    # fill in the values with the correct mapping\n    df['dependency'] = df['dependency'].replace(mapping).astype(np.float64)\n    df['edjefe'] = df['edjefe'].replace(mapping).astype(np.float64)\n    df['edjefa'] = df['edjefa'].replace(mapping).astype(np.float64)\n    \ntrain[['dependency', 'edjefa', 'edjefe']].describe()","b86cc1e8":"plt.figure(figsize = (16, 12))\n\n# iterate through the float columns\nfor i, col in enumerate(['dependency', 'edjefa', 'edjefe']):\n    ax = plt.subplot(3, 1, i+1)\n    # iterate through the poverty levels\n    for poverty_level, color in colors.items():\n        # plot each poverty level as a seperate line\n        sns.kdeplot(train.loc[train['Target'] == poverty_level, col].dropna(),\n                    ax = ax, color = color, label = poverty_mapping[poverty_level])\n        plt.title(f'{col.capitalize()} Distribution');\n        plt.xlabel(f'{col}');\n        plt.legend();\n        plt.ylabel('density')\n\n    plt.subplots_adjust(top = 2)","0da75603":"# add null target column to test\ntest['Target'] = np.nan\ndata = train.append(test, ignore_index = True)","742e28c0":"# heads of household\nheads = data.loc[data['parentesco1'] == 1].copy()\n\n# labels for training\ntrain_labels = data.loc[(data['Target'].notnull()) & (data['parentesco1'] == 1), \n                        ['Target', 'idhogar']]\n\n# value counts of target\nlabel_counts = train_labels['Target'].value_counts().sort_index()\n\n# bar plot of occurrences of each label\nlabel_counts.plot.bar(figsize = (8, 6),\n                      color = colors.values(),\n                      edgecolor = 'k', linewidth = 2)\n\n# formatting\nplt.xlabel('poverty label');\nplt.ylabel('count');\nplt.xticks([x - 1 for x in poverty_mapping.keys()],\n           list(poverty_mapping.values()), rotation = 60)\nplt.title('poverty level breakdown');\n\nlabel_counts","7aee1523":"# groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x:x.nunique() == 1)\n\n# households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('there are {} households where the family members do not all have the same target.'.format(len(not_equal)))","c39cc0ff":"train[train['idhogar'] == not_equal.index[0]][['idhogar', 'parentesco1', 'Target']]","e279e9a6":"# standard by idhogar to plus parentsco1\nhouseholds_leader = train.groupby('idhogar')['parentesco1'].sum()\n\n# fifnd households without a head\nhouseholds_no_head = train.loc[train['idhogar'].isin(households_leader[households_leader == 0].index), :]\n\nprint('there are {} households without a head.'.format(households_no_head['idhogar'].nunique()))","295ba872":"# find households without a head and where labels are different\nhouseholds_no_head_equal = households_no_head.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\nprint('{} households with no head have different labels.'.format(sum(households_no_head_equal == False)))","3f784871":"# iterate through each household\nfor household in not_equal.index:\n    # find the correct label (for the head of household)\n    true_target = int(train[(train['idhogar'] == household) & (train['parentesco1'] == 1.0)]['Target'])\n    \n    # set the correct label for all members in the household\n    train.loc[train['idhogar'] == household, 'Target'] = true_target\n    \n# groupby the household and figure out the number of unique values\nall_equal = train.groupby('idhogar')['Target'].apply(lambda x: x.nunique() == 1)\n\n# households where targets are not all equal\nnot_equal = all_equal[all_equal != True]\nprint('there are {} households where the family members do not all have the same target.'.format(len(not_equal)))","11f08f2a":"# number of missing in each column\nmissing = pd.DataFrame(data.isnull().sum()).rename(columns = {0: 'total'})\n\n# create a percentage missing\nmissing['percent'] = missing['total'] \/ len(data)\n\nmissing.sort_values('percent', ascending = False).head(10).drop('Target')","128ce979":"def plot_value_counts(df, col, heads_only = False):\n    if heads_only:\n        df = df.loc[df['parentesco1'] == 1].copy()\n        \n    plt.figure(figsize = (8, 6))\n    df[col].value_counts().sort_index().plot.bar(color = 'blue',\n                                                edgecolor = 'k',\n                                                linewidth = 2)\n    plt.xlabel(f'{col}');\n    plt.title(f'{col} value counts');\n    plt.ylabel('count');\n    plt.show();","aa422215":"plot_value_counts(heads, 'v18q1')","852e53c9":"heads.groupby('v18q')['v18q1'].apply(lambda x: x.isnull().sum())","c56df200":"data['v18q1'] = data['v18q1'].fillna(0)","19f853f2":"# variables indicating home ownership\nown_variables = [x for x in data if x.startswith('tipo')]\n\n# plot of the home ownership variables for home missing rent payments\ndata.loc[data['v2a1'].isnull(), own_variables].sum().plot.bar(figsize = (10, 8),\n                                                              color = 'green',\n                                                              edgecolor = 'k',\n                                                              linewidth = 2);\n# matplotlib \ub208\uae08 \ud45c\uc2dc\ud558\uae30\nplt.xticks([0, 1, 2, 3, 4],\n           ['owns and paid off', 'owns and paying', 'rented', 'precarious', 'other'],\n           rotation = 60)\n\nplt.title('home ownership status for households missing rent payments', size = 18)","3f06b62f":"# fill in households that own the house with 0 rent payment\ndata.loc[(data['tipovivi1'] == 1), 'v2a1'] = 0\n\n# create missing rent payment column\ndata['v2a1-missing'] = data['v2a1'].isnull()\n\ndata['v2a1-missing'].value_counts()","402a241a":"data.loc[data['rez_esc'].notnull()]['age'].describe()","9da6852b":"data.loc[data['rez_esc'].isnull()]['age'].describe()","18c51e9d":"# if individual is over 19 or younger than 7 and missing years behind, set it to 0\ndata.loc[((data['age'] > 19) | (data['age'] < 7)) & (data['rez_esc'].isnull()), 'rez_esc'] = 0\n\n# add a flag for those between 7 and 19 with a missing value\ndata['rez_esc-missing'] = data['rez_esc'].isnull()","cf7594c7":"def plot_categoricals(x, y, data, annotate = True):\n    # raw counts\n    raw_counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = False))\n    raw_counts = raw_counts.rename(columns = {x: 'raw_count'})\n    \n    # calculate counts for each group of x and y\n    counts = pd.DataFrame(data.groupby(y)[x].value_counts(normalize = True))\n    \n    # calculate the column and reset the index\n    counts = counts.rename(columns = {x: 'normalized_count'}).reset_index()\n    counts['percent'] = 100 * counts['normalized_count']\n    \n    # add the raw count\n    counts['raw_count'] = list(raw_counts['raw_count'])\n    \n    plt.figure(figsize = (14, 10))\n    \n    # scatter plot sized by percent\n    plt.scatter(counts[x], counts[y], edgecolor = 'k', color = 'lightgreen',\n                s = 100 * np.sqrt(counts['raw_count']), marker = 'o',\n                alpha = 0.6, linewidth = 1.5)\n    \n    if annotate:\n        # annotate the plot with text\n        # Iterate over DataFrame rows as (index, Series) pairs.\n        for i, row in counts.iterrows():\n            # put text with appropariate offsets\n            plt.annotate(xy = (row[x] - (1 \/ counts[x].nunique()),\n                               row[y] - (0.15 \/ counts[y].nunique())),\n                         color = 'navy',\n                         s = f\"{round(row['percent'],1 )}\")\n            \n    # set tick marks\n    plt.xticks(counts[y].unique())\n    plt.yticks(counts[x].unique())\n    \n    # transform min and max to evenly space in square root domain\n    sqr_min = int(np.sqrt(raw_counts['raw_count'].min()))\n    sqr_max = int(np.sqrt(raw_counts['raw_count'].max()))\n    \n    # 5 sizes for legend\n    msizes = list(range(sqr_min, sqr_max, int((sqr_max - sqr_min) \/ 5)))\n    \n    markers = []\n    \n    # markers for legend\n    for size in msizes:\n        markers.append(plt.scatter([], [], s = 100 * size,\n                                   label = f'{int(round(np.square(size) \/ 100) * 100)}',\n                                   color = 'lightgreen',\n                                   alpha = 0.6, edgecolor ='k', linewidth = 1.5))\n    # Legend and formatting\n    plt.legend(handles = markers, title = 'Counts',\n               labelspacing = 3, handletextpad = 2,\n               fontsize = 16,\n               loc = (1.10, 0.19))\n        \n    plt.annotate(f'* Size represnts raw count while % is for a given y value.',\n                 xy = (0, 1), xycoords = 'figure points', size = 10)\n    \n    # adjust axes limits\n    plt.xlim((counts[x].min() - (6 \/ counts[x].nunique()),\n              counts[x].max() + (6 \/ counts[x].nunique())))\n    plt.ylim((counts[y].min() - (4 \/ counts[y].nunique()),\n              counts[y].max() + (4 \/ counts[y].nunique())))\n    plt.grid(None)\n    plt.xlabel(f\"{x}\");\n    plt.ylabel(f\"{y}\");\n    plt.title(f\"{y} vs {x}\");","f08103c7":"plot_categoricals('rez_esc', 'Target', data);","6283ee49":"plot_categoricals('escolari', 'Target', data, annotate = False)","6dca91fe":"plot_value_counts(data[(data['rez_esc-missing'] == 1)], 'Target')","60fd0d3b":"plot_value_counts(data[(data['v2a1-missing'] == 1)], 'Target')","27a592d1":"id_ = ['Id', 'idhogar', 'Target']\n\nind_bool = ['v18q', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', \n            'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7', \n            'parentesco1', 'parentesco2',  'parentesco3', 'parentesco4', 'parentesco5', \n            'parentesco6', 'parentesco7', 'parentesco8',  'parentesco9', 'parentesco10', \n            'parentesco11', 'parentesco12', 'instlevel1', 'instlevel2', 'instlevel3', \n            'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', \n            'instlevel9', 'mobilephone', 'rez_esc-missing']\n\nind_ordered = ['rez_esc', 'escolari', 'age']\n\nhh_bool = ['hacdor', 'hacapo', 'v14a', 'refrig', 'paredblolad', 'paredzocalo', \n           'paredpreb','pisocemento', 'pareddes', 'paredmad',\n           'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisoother', \n           'pisonatur', 'pisonotiene', 'pisomadera',\n           'techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', \n           'abastaguadentro', 'abastaguafuera', 'abastaguano',\n            'public', 'planpri', 'noelec', 'coopele', 'sanitario1', \n           'sanitario2', 'sanitario3', 'sanitario5',   'sanitario6',\n           'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', \n           'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', \n           'elimbasu5', 'elimbasu6', 'epared1', 'epared2', 'epared3',\n           'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', \n           'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', \n           'computer', 'television', 'lugar1', 'lugar2', 'lugar3',\n           'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'v2a1-missing']\n\nhh_ordered = [ 'rooms', 'r4h1', 'r4h2', 'r4h3', 'r4m1','r4m2','r4m3', 'r4t1',  'r4t2', \n              'r4t3', 'v18q1', 'tamhog','tamviv','hhsize','hogar_nin',\n              'hogar_adul','hogar_mayor','hogar_total',  'bedrooms', 'qmobilephone']\n\nhh_cont = ['v2a1', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'overcrowding']\n\nsqr_ = ['SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', \n        'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq']","07bcab13":"x = ind_bool + ind_ordered + id_ + hh_bool + hh_ordered + hh_cont + sqr_\n\nfrom collections import Counter\n\nprint('there are no repeats: ', np.all(np.array(list(Counter(x).values())) == 1))\nprint('we covered every variabl')","bbc8d952":"sns.lmplot('age', 'SQBage', data = data, fit_reg = False);\nplt.title('squared age versus age');","59f12225":"# remove squared variables\ndata = data.drop(columns = sqr_)\ndata.shape","c081eddf":"heads = data.loc[data['parentesco1'] == 1, :]\nheads = heads[id_ + hh_bool + hh_cont + hh_ordered]\nheads.shape","5eb53f3f":"# create a correlation matrix\ncorr_matrix = heads.corr()\n\n# select upper triangle of correlation matrix\n# np.triu() \uc0c1\uc0bc\uac01\ud589\ub82c (Upper triangular matrix)\uc744 \ubc18\ud658\n# https:\/\/ko.wikipedia.org\/wiki\/%EC%82%BC%EA%B0%81%ED%96%89%EB%A0%AC\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n\n# find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","9758a337":"corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9]","36af9a5b":"sns.heatmap(corr_matrix.loc[corr_matrix['tamhog'].abs() > 0.9, corr_matrix['tamhog'].abs() > 0.9],\n            annot = True, cmap = plt.cm.autumn_r, fmt = '.3f');","c0998d1c":"heads = heads.drop(columns = ['tamhog', 'hogar_total', 'r4t3'])","b99db688":"sns.lmplot('tamviv', 'hhsize', data, fit_reg = False, size = 8);\nplt.title('hosehold size vs number of persons living in the household');","fd9bd09e":"heads['hhsize-diff'] = heads['tamviv'] - heads['hhsize']\nplot_categoricals('hhsize-diff', 'Target', heads)","0c1aa88c":"corr_matrix.loc[corr_matrix['coopele'].abs() > 0.9, corr_matrix['coopele'].abs() > 0.9]","23d1ccc8":"elec = []\n\n# assign values\nfor i, row in heads.iterrows():\n    if row['noelec'] == 1:\n        elec.append(0)\n    elif row['coopele'] == 1:\n        elec.append(1)\n    elif row['public'] == 1:\n        elec.append(2)\n    elif row['planpri'] == 1:\n        elec.append(3)\n    else:\n        elec.append(np.nan)\n        \n# Record the new variable and missing flag\nheads['elec'] = elec\nheads['elec-missing'] = heads['elec'].isnull()","4675ca05":"plot_categoricals('elec', 'Target', heads)","b7ea692b":"heads = heads.drop(columns = 'area2')\n\nheads.groupby('area1')['Target'].value_counts(normalize = True)","27b91e71":"# Wall ordinal variable\nheads['walls'] = np.argmax(np.array(heads[['epared1', 'epared2', 'epared3']]),\n                           axis = 1)\n\n# heads = heads.drop(columns = ['epared1', 'epared2', 'epared3'])\nplot_categoricals('walls', 'Target', heads)","dc4225c8":"# Roof ordinal variable\nheads['roof'] = np.argmax(np.array(heads[['etecho1', 'etecho2', 'etecho3']]),\n                           axis = 1)\nheads = heads.drop(columns = ['etecho1', 'etecho2', 'etecho3'])\n\n# Floor ordinal variable\nheads['floor'] = np.argmax(np.array(heads[['eviv1', 'eviv2', 'eviv3']]),\n                           axis = 1)\n# heads = heads.drop(columns = ['eviv1', 'eviv2', 'eviv3'])","a33229be":"# Create new feature\nheads['walls+roof+floor'] = heads['walls'] + heads['roof'] + heads['floor']\n\nplot_categoricals('walls+roof+floor', 'Target', heads, annotate=False)","2dfaa334":"counts = pd.DataFrame(heads.groupby(['walls+roof+floor'])['Target'].value_counts(normalize = True)).rename(columns = {'Target' : 'Normalized Count'}).reset_index()\ncounts.head()","2cee8a85":"# No toilet, no electricity, no floor, no water service, no ceiling\nheads['warning'] = 1 * (heads['sanitario1'] + \n                         (heads['elec'] == 0) + \n                         heads['pisonotiene'] + \n                         heads['abastaguano'] + \n                         (heads['cielorazo'] == 0))","475f8e82":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'warning', y = 'Target', data = heads);\nplt.title('Target vs Warning Variable');","3ba7cbe3":"plot_categoricals('warning', 'Target', data = heads)","296ac847":"# Owns a refrigerator, computer, tablet, and television\nheads['bonus'] = 1 * (heads['refrig'] + \n                      heads['computer'] + \n                      (heads['v18q1'] > 0) + \n                      heads['television'])\n\nsns.violinplot('bonus', 'Target', data = heads,\n                figsize = (10, 6));\nplt.title('Target vs Bonus Variable');","656e955c":"heads['phones-per-capita'] = heads['qmobilephone'] \/ heads['tamviv']\nheads['tablets-per-capita'] = heads['v18q1'] \/ heads['tamviv']\nheads['rooms-per-capita'] = heads['rooms'] \/ heads['tamviv']\nheads['rent-per-capita'] = heads['v2a1'] \/ heads['tamviv']","b7023aad":"# use only training data\ntrain_heads = heads.loc[heads['Target'].notnull(), :].copy()\n\npcorrs = pd.DataFrame(train_heads.corr()['Target'].sort_values()).rename(columns = {'Target':'pcorr'}).reset_index()\npcorrs = pcorrs.rename(columns = {'index':'feature'})\n\nprint('most negatively correlated variables: ')\nprint(pcorrs.head(5))\n\nprint('\\n')\n\nprint('most positively correlated variables: ')\nprint(pcorrs.dropna().tail())","a17963ed":"from scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings('ignore', category = RuntimeWarning)\n\nfeats = []\nscorr = []\npvalues = []\n\n# iterate through each column\nfor c in heads:\n    # only valid for numbers\n    if heads[c].dtype != 'object':\n        feats.append(c)\n        \n        # calculate spearman correlation\n        scorr.append(spearmanr(train_heads[c], train_heads['Target']).correlation)\n        pvalues.append(spearmanr(train_heads[c], train_heads['Target']).pvalue)\n        \nscorrs = pd.DataFrame({'feature' : feats, \n                       'scorr' : scorr,\n                       'pvalue' : pvalues}).sort_values('scorr')","888ff439":"print('Most negative Spearman correlations:')\nprint(scorrs.head())\nprint('\\nMost positive Spearman correlations:')\nprint(scorrs.dropna().tail())","35c5967c":"corrs = pcorrs.merge(scorrs, on = 'feature')\ncorrs['diff'] = corrs['pcorr'] - corrs['scorr']\n\ncorrs.sort_values('diff').head()","9a880b7b":"corrs.sort_values('diff').dropna().tail()","f11f0a63":"sns.lmplot('dependency', 'Target', fit_reg = True, data = train_heads, x_jitter = 0.05, y_jitter = 0.05);\nplt.title('target vs dependency');","a6aecf84":"sns.lmplot('rooms-per-capita', 'Target', fit_reg = True, data = train_heads, x_jitter=0.05, y_jitter=0.05);\nplt.title('Target vs Rooms Per Capita');","1562b89a":"variables = ['Target', 'dependency', 'warning', 'walls+roof+floor', 'meaneduc',\n             'floor', 'r4m1', 'overcrowding']\n\n# calculate the correlations\ncorr_mat = train_heads[variables].corr().round(2)\n\n# draw a correlation heatmap\nplt.rcParams['font.size'] = 18\nplt.figure(figsize = (12, 12))\nsns.heatmap(corr_mat, vmin = -0.5, vmax = 0.9, center = 0,\n            cmap = plt.cm.RdYlBu_r, annot = True);","c2265130":"# copy the data for plotting\nplot_data = train_heads[['Target', 'dependency', 'walls+roof+floor',\n                         'meaneduc', 'overcrowding']]\n\n# create the pairgrid object\ngrid = sns.PairGrid(data = plot_data, size = 4, diag_sharey = False,\n                    hue = 'Target', hue_order = [4, 3, 2, 1],\n                    vars = [x for x in list(plot_data.columns) if x != 'Target'])\n\n# upper is a scatter plot\ngrid.map_upper(plt.scatter, alpha = 0.8, s = 20)\n\n# diagonal is a histogram\ngrid.map_diag(sns.kdeplot)\n\n# bottom is density plot\ngrid.map_lower(sns.kdeplot, cmap = plt.cm.Oranges_r);\ngrid = grid.add_legend()\nplt.suptitle('feature plots colored by target', size = 32, y = 1.05);","35cad934":"household_feats = list(heads.columns)","b0a62d55":"ind = data[id_ + ind_bool + ind_ordered]","86b1b938":"# create coreelation matrix\ncorr_matrix = ind.corr()\n\n# select upper triagnle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nto_drop","5106afc2":"ind = ind.drop(columns = 'male')","a45b983b":"ind[[ c for c in ind if c.startswith('instl')]].head()","4fad3aa1":"ind['inst'] = np.argmax(np.array(ind[[c for c in ind if c.startswith('instl')]]), axis = 1)\n\nplot_categoricals('inst', 'Target', ind, annotate = False);","0a2d8b2b":"plt.figure(figsize = (10, 8))\nsns.violinplot(x = 'Target', y = 'inst', data = ind);\nplt.title('Education Distribution by Target');","c3b878d4":"ind['escolari\/age'] = ind['escolari'] \/ ind['age']\n\nplt.figure(figsize = (10, 8))\nsns.violinplot('Target', 'escolari\/age', data = ind);","8a3eb954":"ind['inst\/age'] = ind['inst'] \/ ind['age']\nind['tech'] = ind['v18q'] + ind['mobilephone']\nind['tech'].describe()","70be093d":"# Define custom function\nrange_ = lambda x: x.max() - x.min()\nrange_.__name__ = 'range_'\n\n# Group and aggregate\nind_agg = ind.drop(columns = 'Target').groupby('idhogar').agg(['min', 'max', 'sum', 'count', 'std', range_])\nind_agg.head()","aa935fcb":"# rename\nnew_col = []\n\n# make levels by arregation\nfor c in ind_agg.columns.levels[0]:\n    for stat in ind_agg.columns.levels[1]:\n        new_col.append(f'{c}-{stat}')\n        \nind_agg.columns = new_col\nind_agg.head()","3ed42147":"# create correlation matrix\ncorr_matrix = ind_agg.corr()\n\n# select upper triangle of correlation matrix\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k = 1).astype(np.bool))\n\n# find index of feature columns with correlation greater than 0.95\nto_drop = [column for column in upper.columns if any(abs(upper[column]) > 0.95)]\n\nprint(f'there are {len(to_drop)} correlated columns to remove.')","f7af3d0b":"ind_agg = ind_agg.drop(columns = to_drop)\nind_feats = list(ind_agg.columns)\n\n# merge on the household id\nfinal = heads.merge(ind_agg, on = 'idhogar', how = 'left')\n\nprint(f'final feature shape: ', final.shape)","a21ea680":"corrs = final.corr()['Target']\nplot_categoricals('escolari-max', 'Target', final, annotate=False);","f04caf73":"plt.figure(figsize = (10, 6))\nsns.violinplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","27622cb4":"plt.figure(figsize = (10, 6))\nsns.boxplot(x = 'Target', y = 'escolari-max', data = final);\nplt.title('Max Schooling by Target');","a13d15e7":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import f1_score, make_scorer\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.impute import SimpleImputer as Imputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.pipeline import Pipeline\n\n# custom scorer for cross validation\nscorer = make_scorer(f1_score, greater_is_better = True, average = 'macro' )","ca439d4d":"# labels for training\ntrain_labels = np.array(list(final[final['Target'].notnull()]['Target'].astype(np.uint8)))\n\n# extract the training data\n# isnull() \uba54\uc18c\ub4dc\ub294 \uad00\uce21\uce58\uac00 \uacb0\uce21\uc774\uba74 True, \uacb0\uce21\uc774 \uc544\ub2c8\uba74 False\uc758 boollean \uac12\uc744 \ubc18\ud658\ud569\ub2c8\ub2e4.\n# notnull() \uba54\uc18c\ub4dc\ub294 \uad00\uce21\uce58\uac00 \uacb0\uce21\uc774\uba74 False, \uacb0\uce21\uc774 \uc544\ub2c8\uba74 True\ub97c \ubc18\ud658\ud569\ub2c8\ub2e4.(isnull() \uacfc \uc815\ubc18\ub300)\ntrain_set = final[final['Target'].notnull()].drop(columns = ['Id', 'idhogar', 'Target'])\ntest_set = final[final['Target'].isnull()].drop(columns = ['Id', 'idhogar', 'Target'])\n\n# submission base which is used for making submissions to the competition\nsubmission_base = test[['Id', 'idhogar']].copy()","d3509987":"features = list(train_set.columns)\n\npipeline = Pipeline([('imputer', Imputer(strategy = 'median')), \n                      ('scaler', MinMaxScaler())])\n\n# fit and transform training data\ntrain_set = pipeline.fit_transform(train_set)\ntest_set = pipeline.transform(test_set)","0c8abb11":"model = RandomForestClassifier(n_estimators=100, random_state=10, \n                               n_jobs = -1)\n# 10 fold cross validation\ncv_score = cross_val_score(model, train_set, train_labels, cv = 10, scoring = scorer)\n\nprint(f'10 Fold Cross Validation F1 Score = {round(cv_score.mean(), 4)} with std = {round(cv_score.std(), 4)}')","2c612a29":"model.fit(train_set, train_labels)\n\n# feature importances into a dataframe\nfeature_importances = pd.DataFrame({'feature':features, 'importance':model.feature_importances_})\nfeature_importances.head()","96a7a024":"# short function useful and can apply lots of position\n\ndef plot_feature_importances(df, n = 10, threshold = None):\n    \"\"\"Plots n most important features. Also plots the cumulative importance if\n    threshold is specified and prints the number of features needed to reach threshold cumulative importance.\n    Intended for use with any tree-based feature importances. \n    \n    Args:\n        df (dataframe): Dataframe of feature importances. Columns must be \"feature\" and \"importance\".\n    \n        n (int): Number of most important features to plot. Default is 15.\n    \n        threshold (float): Threshold for cumulative importance plot. If not provided, no plot is made. Default is None.\n        \n    Returns:\n        df (dataframe): Dataframe ordered by feature importances with a normalized column (sums to 1) \n                        and a cumulative importance column\n    \n    Note:\n    \n        * Normalization in this case means sums to 1. \n        * Cumulative importance is calculated by summing features from most to least important\n        * A threshold of 0.9 will show the most important features needed to reach 90% of cumulative importance\n    \n    \"\"\"\n    plt.style.use('fivethirtyeight')\n    \n    # sort features with most important at the head\n    df = df.sort_values('importance', ascending = False).reset_index(drop = True)\n    \n    # normalize the feature importances to add up to one and calculate cumulative importance\n    df['importance_normalized'] = df['importance'] \/ df['importance'].sum()\n    df['cumulative_importance'] = np.cumsum(df['importance_normalized'])\n    \n    plt.rcParams['font.size'] = 12\n    \n    #bar plot of n most importance features\n    df.loc[:n, :].plot.barh(y = 'importance_normalized',\n                            x = 'feature', color = 'darkgreen', edgecolor = 'k',\n                            figsize = (12, 8), legend = False, linewidth = 2)\n    \n    plt.xlabel('normalized importance', size = 18);\n    plt.ylabel('');\n    plt.title(f'{n} Most importance features', size = 18);\n    plt.gca().invert_yaxis()\n    \n    if threshold:\n        # cumulative importance plot\n        plt.figure(figsize = (8, 6))\n        plt.plot(list(range(len(df))), df['cumulative_importance'], 'b-')\n        plt.xlabel('Number of Features', size = 16); plt.ylabel('Cumulative Importance', size = 16); \n        plt.title('Cumulative Feature Importance', size = 18);\n        \n        # Number of features needed for threshold cumulative importance\n        # This is the index (will need to add 1 for the actual number)\n        importance_index = np.min(np.where(df['cumulative_importance'] > threshold))\n        \n        # Add vertical line to plot\n        plt.vlines(importance_index + 1, ymin = 0, ymax = 1.05, linestyles = '--', colors = 'red')\n        plt.show();\n        \n        print('{} features required for {:.0f}% of cumulative importance.'.format(importance_index + 1, \n                                                                                  100 * threshold))\n    \n    return df        ","fd524240":"norm_fi = plot_feature_importances(feature_importances, threshold=0.95)","ae84d71a":"def kde_target(df, variable):\n    \"\"\"Plots the distribution of `variable` in `df` colored by the `Target` column\"\"\"\n    \n    colors = {1: 'red', 2: 'orange', 3: 'blue', 4: 'green'}\n\n    plt.figure(figsize = (12, 8))\n    \n    df = df[df['Target'].notnull()]\n    \n    for level in df['Target'].unique():\n        subset = df[df['Target'] == level].copy()\n        sns.kdeplot(subset[variable].dropna(), \n                    label = f'Poverty Level: {level}', \n                    color = colors[int(subset['Target'].unique())])\n\n    plt.xlabel(variable); plt.ylabel('Density');\n    plt.title('{} Distribution'.format(variable.capitalize()));","b6373a3c":"kde_target(final, 'meaneduc')","2f3dcdb5":"kde_target(final, 'escolari\/age-range_')","28baffef":"test_ids = list(final.loc[final['Target'].isnull(), 'idhogar'])","6830e3a9":"def submit(model, train, train_labels, test, test_ids):\n    # train on the data\n    model.fit(train, train_labels)\n    predictions = model.predict(test)\n    predictions = pd.DataFrame({'idhogar':test_ids,\n                                'Target':predictions})\n    \n    # make a submission dataframe\n    submission = submission_base.merge(predictions, on = 'idhogar', how = 'left').drop(columns = ['idhogar'])\n    \n    # fill in households missing a head\n    submission['Target'] = submission['Target'].fillna(4).astype(np.int8)\n    \n    return submission","21593016":"rf_submission = submit(RandomForestClassifier(n_estimators = 100, \n                                              random_state=10, n_jobs = -1), \n                         train_set, train_labels, test_set, test_ids)\n\nrf_submission.to_csv('rf_submission.csv', index = False)","5f68f940":"##### Correct Errors\nNow we can correct labels for the households that do have a head AND the members have different poverty levels.","64f23236":"### Exploring Label Distribution\n\nNext, we can get an idea of how imbalanced the problem is by looking at the distribution of labels. There are four possible integer levels, indicating four different levels of poverty. To look at the correct labels, we'll subset only to the columns where parentesco1 == 1 because this is the head of household, the correct label for each household.\n\nThe bar plot below shows the distribution of training labels (since there are no testing labels).","93405540":"Since we are only going to use the heads of household for the labels, this step is not completely necessary but it shows a workflow for correcting data errors like you may encounter in real life. Don't consider it extra work, just practice for your career!","f307e79d":"This new feature may be useful because it seems like a Target of 4 (the lowest poverty level) tends to have higher values of the 'house quality' variable. We can also look at this in a table to get the fine-grained details.","2698cdc3":"We can see that for every value of the Target, the most common source of electricity is from one of the listed providers.\n\nThe final redundant column is area2. This means the house is in a rural zone, but it's redundant because we have a column indicating if the house is in a urban zone. Therefore, we can drop this column.","3f0f3e0a":"Because we are going to be comparing different models, we want to scale the features (limit the range of each column to between 0 and 1). For many ensemble models this is not necessary, but when we use models that depend on a distance metric, such as KNearest Neighbors or the Support Vector Machine, feature scaling is an absolute necessity. When comparing different models, it's always safest to scale the features. We also impute the missing values with the median of the feature.\n\nFor imputing missing values and scaling the features in one step, we can make a pipeline. This will be fit on the training data and used to transform the training and testing data.","1bf7df58":"The next variable will be a warning about the quality of the house. It will be a negative value, with -1 point each for no toilet, electricity, floor, water service, and ceiling.","c6474338":"### Feature Construction\nIn addition to mapping variables to ordinal features, we can also create entirely new features from the existing data, known as feature construction. For example, we can add up the previous three features we just created to get an overall measure of the quality of the house's structure.","9d817cad":"### Individual Level Variables\nThere are two types of individual level variables: Boolean (1 or 0 for True or False) and ordinal (discrete values with a meaningful ordering).","db2320c4":"The organizers tell us that the correct label is that for the head of household, where parentesco1 == 1. For this household, the correct label is 3 for all members. We can correct this (as shown later) by reassigning all the individuals in this household the correct poverty level. In the real-world, you might have to make the tough decision of how to address the problem by yourself (or with the help of your team).","81c185ef":"The size of the markers represents the raw count. To read the plot, choose a given y-value and then read across the row. For example, with a poverty level of 1, 93% of individuals have no years behind with a total count of around 800 individuals and about 0.4% of individuals are 5 years behind with about 50 total individuals in this category. This plot attempts to show both the overall counts and the within category proportion; it's not perfect , but I gave it a shot!","8d1a21f8":"rez_esc: years behind in school\n\nThe last column with a high percentage of missing values is rez_esc indicating years behind in school. For the families with a null value, is possible that they have no children currently in school. Let's test this out by finding the ages of those who have a missing value in this column and the ages of those who do not have a missing value.","d30b5b40":"### Feature Construction\nWe can make a few features using the existing data. For example, we can divide the years of schooling by the age.","b4f0a191":"The remaining missing values in each column will be filled in, a process known as Imputation. There are several types of imputation commonly used, and one of the simplest and most effective methods is to fill in the missing values with the median of the column.\n\nAs a final step with the missing values, we can plot the distribution of target for the case where either of these values are missing.","0861301c":"### Feature Engineering through Aggregations\nIn order to incorporate the individual data into the household data, we need to aggregate it for each household. The simplest way to do this is to groupby the family id idhogar and then agg the data. For the aggregations for ordered or continuous variables, we can use six, five of which are built in to pandas, and one of which we define ourselves range_. The boolean aggregations can be the same, but this will create many redundant columns which we will then need to drop. For this case, we'll use the same aggregations and then go back and drop the redundant columns.","496a625e":"It's hard to see the relationship, but it's slightly negative: as the dependency increases, the value of the Target decreases. This makes sense: the dependency is the number of dependent individuals divided by the number of non-dependents. As we increase this value, the poverty severty tends to increase: having more dependent family members (who usually are non-working) leads to higher levels of poverty because they must be supported by the non-dependent family members.","d135af6d":"This kernel used dataset from the Costa Rican Household Poverty Level Prediction and copied from the 'A Complete Introduction and Walkthrough' written by Will Koehrsen.\nIntroduction to 'A Complete Introduction and Walkthrough': [URL](https:\/\/www.kaggle.com\/willkoehrsen\/a-complete-introduction-and-walkthrough\/notebook)\n\nThanks for sharing kernel, Will Koehrsen","d72bc0a6":"## Plot Two Categorical Variables\nTo show how two categorical variables interact with one another, there are a number of plotting options: scatterplots, faceted bar plots, boxplots, etc. I wasn't satisfied with any of these choices so I wrote the function below, which essentially is a scatterplot of two categoricals where the size of the points represent the percentage of a given y-value represented by each x-value.","707ec07a":"Education reigns supreme! The most important variable is the average amount of education in the household, followed by the maximum education of anyone in the household. I have a suspicion these variables are highly correlated (collinear) which means we may want to remove one of them from the data. The other most important features are a combination of variables we created and variables that were already present in the data.\n\nIt's interesting that we only need 106 of the ~180 features to account for 90% of the importance. This tells us that we may be able to remove some of the features. However, feature importances don't tell us which direction of the feature is important (for example, we can't use these to tell whether more or less education leads to more severe poverty) they only tell us which features the model considered relevant.","60105e12":"#### Correlation Heatmap","5702b7f7":"v2a1: Monthly rent payment\n\nThe next missing column is v2a1 which represents the montly rent payment.","f020fdd0":"It seems like households in an urban area (value of 1) are more likely to have lower poverty levels than households in a rural area (value of 0).\n\n#### Creating Ordinal Variables\nFor the walls, roof, and floor of the house, there are three columns each: the first indicating 'bad', the second 'regular', and the third 'good'. We could leave the variables as booleans, but to me it makes more sense to turn them into ordinal variables because there is an inherent order: bad < regular < good. To do this, we can simply find whichever column is non-zero for each household using np.argmax.\n\nOnce we have created the ordinal variable, we are able to drop the original variables.","f8a4db36":"We are dealing with an imbalanced class problem (which makes it intriguing why the contest organizers choose the macro F1 score as the metric instead of weighted F1!). There are many more households that classify as non vulnerable than in any other category. The extreme poverty class is the smallest (I guess this should make us optimistic!).\n\nOne problem with imbalanced classification problems is that the machine learning model can have a difficult time predicting the minority classes because it sees far less examples. Think about this in human terms: if we are classifiying poverty and we see far more cases of no poverty than extreme poverty, it will make it more difficult for us to identify the high poverty households because of less exposure. One potential method to address class imbalanceds is through oversampling (which is covered in more advanced notebooks).","53bc6873":"### Per Capita Features\nAdditional features we can make calculate the number of certain measurements for each person in the household","a92fed5c":"### Redundant Individual Variables\nWe can do the same process we did with the household level variables to identify any redundant individual variables. We'll focus on any variables that have an absolute magnitude of the correlation coefficient greater than 0.95.","69fa8982":"## Feature Importances\nWith a tree-based model, we can look at the feature importances which show a relative ranking of the usefulness of features in the model. These represent the sum of the reduction in impurity at nodes that used the variable for splitting, but we don't have to pay much attention to the absolute value. Instead we'll focus on relative scores.\n\nIf we want to view the feature importances, we'll have to train a model on the whole training set. Cross validation does not return the feature importances.","07e8c26f":"#### Missing Variables\nOne of the most important steps of exploratory data analysis is finding missing values in the data and determining how to handle them. Missing values have to be filled in before we use a machine learning model and we need to think of the best strategy for filling them in based on the feature: this is where we'll have to start digging into the data definitions.\n\nFirst we can look at the percentage of missing values in each column.","0c60911c":"If we read through some of the discussions for this competition, we learn that this variable is only defined for individuals between 7 and 19. Anyone younger or older than this range presumably has no years behind and therefore the value should be set to 0. For this variable, if the individual is over 19 and they have a missing value, or if they are younger than 7 and have a missing value we can set it to zero. For anyone else, we'll leave the value to be imputed and add a boolean flag.","d1649823":"### Creating Ordinal Variables\nMuch as we did with the household level data, we can map existing columns to an ordinal variable. Here we will focus on the instlevel_ variables which indicate the amount of education an individual has from instlevel1: no level of education to instlevel9: postgraduate education.\n\nTo create the ordinal variable, for each individual, we will simply find which column is non-zero. The education has an inherent ordering (higher is better) so this conversion to an ordinal variable makes sense in the problem context.","ebe3118c":"There are several variables here having to do with the size of the house:\n\n* r4t3: Total persons in the household\n* tamhog:  size of the household\n* tamviv: number of persons living in the household\n* hhsize: household size\n* hogar_total: # of total individuals in the household\n\nThese variables are all highly correlated with one another. In fact, hhsize has a perfect correlation with tamhog and hogar_total. We will remove these two variables because the information is redundant. We can also remove r4t3 because it has a near perfect correlation with hhsize.\n\ntamviv is not necessarily the same as hhsize because there might be family members that are not living in the household. Let's visualize this difference in a scatterplot.","83666e25":"### Exploring Household Variables\nAfter going to all the trouble of getting our features in order, now we can take a look at them in relation to the Target. We've already done a little of this, but now we can try to quantify relationships.","8a591fcd":"These variables indicate where the electricity in the home is coming from. There are four options, and the families that don't have one of these two options either have no electricity (noelec) or get it from a private plant (planpri).\n\n### Creating Ordinal Variable\u00b6\nI'm going to compress these four variables into one by creating an ordinal variable. I'm going to choose the mapping myself, based on the data decriptions:\n\n* 0: No electricity\n* 1: Electricity from cooperative\n* 2: Electricity from CNFL, ICA, ESPH\/JASEC\n* 3: Electricity from private plant\n\nAn ordered variable has an inherent ordering, and for this we choose our own based on the domain knowledge. After we create this new ordered variable, we can drop the four others. There are several households that do not have a variable here, so we will use a nan (which will be filled in during imputation) and add a Boolean column indicating there was no measure for this variable.","5e9fd6ae":"### Squared Variables\nFirst, the easiest step: we'll remove all of the squared variables. Sometimes variables are squared or transformed as part of feature engineering because it can help linear models learn relationships that are non-linear. However, since we will be using more complex models, these squared features are redundant. They are highly correlated with the non-squared version, and hence can actually hurt our model by adding irrelevant information and also slowing down training.\n\nFor an example, let's take a look at SQBage vs age.","0af68fd1":"This plot shows us that there are a number of variables that have a weak correlation with the Target. There are also high correlations between some variables (such as floor and walls+roof+floor) which could pose an issue because of collinearity.\n\n#### Features Plot\n\nFor the final exploration of the household level data, we can make a plot of some of the most correlated variables with the Target. This shows scatterplots on the upper triangle, kernel density estimate (kde) plots on the diagonal, and 2D KDE plots on the lower triangle.","272a2937":"What this tells us is that the oldest age with a missing value is 17. For anyone older than this, maybe we can assume that they are simply not in school. Let's look at the ages of those who have a missing value.","b9d2a666":"The violinplot is not great here because it smooths out the categorical variable with the effect that it looks as if the Target can take on lesser and greater values than in reality. Nonetheless, we can see a high concentration of households that have no warning signs and have the lowest level of poverty. It looks as if this may be a useful feature, but we can't know for sure until we get to modeling!\n\nThe final household feature we can make for now is a bonus where a family gets a point for having a refrigerator, computer, tablet, or television.","eb0c67d7":"We see for a number of cases, there are more people living in the household than there are in the family. This gives us a good idea for a new feature: the difference between these two measurements!\n\nLet's make this new feature.","4f1376b1":"these show one out of each pair of correlated variables. To find the other pari, we can subset the corr_matrix","3d6fa6e6":"Later on we'll calculate correlations between the variables and the Target to gauge the relationships between the features, but these plots can already give us a sense of which variables may be most \"relevant\" to a model. For example, the meaneduc, representing the average education of the adults in the household appears to be related to the poverty level: a higher average adult education leads to higher values of the target which are less severe levels of poverty. The theme of the importance of education is one we will come back to again and again in this notebook!","814c3cfc":"#### Float Columns","5d319c45":"Even though most households do not have a difference, there are a few that have more people living in the household than are members of the household.\n\nLet's move on to the other redundant variables. First we can look at coopele","bbefc109":"The Spearman correlation coefficient calculation also comes with a pvalue indicating the significance level of the relationship. Any pvalue less than 0.05 is genearally regarded as significant, although since we are doing multiple comparisons, we want to divide the p-value by the number of comparisons, a process known as the Bonferroni correction.","c7dd9867":"### Redundant Household Variables\nLet's take a look at the correlations between all of the household variables. If there are any that are too highly correlated, then we might want to remove one of the pair of highly correlated variables.\n\nThe following code identifies any variables with a greater than 0.95 absolute magnitude correlation.","2006e8d9":"There is also one outlier in the rez_esc column. Again, if we read through the competition discussions, we learn that the maximum value for this variable is 5. Therefore, any values above 5 should be set to 5.","941cc81c":"The data has no missing values and is scaled between zero and one. This means it can be directly used in any Scikit-Learn model.","5f4bb244":"We can keep using our plot_categoricals function to visualize these relationships, but seaborn also has a number of plotting options that can work with categoricals. One is the violinplot which shows the distribution of a variable on the y axis with the width of each plot showing the number of observations in that category.","9592f4aa":"We don't have to worry about the Target becuase we made that NaN for the test data. However, we do need to address the other 3 columns with a high percentage of missing values.\n\nv18q1: Number of tablets\n\nLet's start with v18q1 which indicates the number of tablets owned by a family. We can look at the value counts of this variable. Since this is a household variable, it only makes sense to look at it on a household level, so we'll only select the rows for the head of household.","ee12232a":"In addition to looking at the missing values of the monthly rent payment, it will be interesting to also look at the distribution of tipovivi_, the columns showing the ownership\/renting status of the home. For this plot, we show the ownership status of those homes with a nan for the monthyl rent payment.","31e66e91":"Well, that solves the issue! Every family that has nan for v18q1 does not own a tablet. Therefore, we can fill in this missing value with zero.","1d8c1c44":"#### Object Columns\nThe last column type is object which we can view as follows.","b9d7c37c":"### Addressing Wrong Labels\nAs with any realistic dataset, the Costa Rican Poverty data has some issues. Typically, 80% of a data science project will be spent cleaning data and fixing anomalies\/errors. These can be either human entry errors, measurement errors, or sometimes just extreme values that are correct but stand out. For this problem, some of the labels are not correct because individuals in the same household have a different poverty level. We're not told why this may be the case, but we are told to use the head of household as the true label.\n\nThat information makes our job much easier, but in a real-world problem, we would have to figure out the reason Why the labels are wrong and how to address the issue on our own. This section fixes the issue with the labels although it is not strictly necessary: I kept it in the notebook just to show how we may deal with this issue.\n\n#### Identify Errors\nFirst we need to find the errors before we can correct them. To find the households with different labels for family members, we can group the data by the household and then check if there is only one unique value of the Target.","9de44eb7":"### Id Variables\nThese are pretty simple: they will be kept as is in the data since we need them for identification.\n\n### Household Level Variables\nFirst let's subset to the heads of household and then to the household level variables.","1a1760de":"Higher levels of education seem to correspond to less extreme levels of poverty. We do need to keep in mind this is on an individual level though and we eventually will have to aggregate this data at the household level.","0814b8e7":"For most of the household level variables, we can simply keep them as is: since we want to make predictions for each household, we use these variables as features. However, we can also remove some redundant variables and also add in some more features derived from existing data.","bfb066fd":"#### Families without Heads of Household\nWe can correct all the label discrepancies by assigning the individuals in the same household the label of the head of household. But wait, you may ask: \"What if there are households without a head of household? And what if the members of those households have differing values of the label?\"","6cb1c075":"## Feature Engineering\nThere is plenty more exploratory data analysis we can do, but first we should work on consolidating our data at a household level. We already have some of the information for each household, but for training, we will need all of the information summarized for each household. This means grouping the individuals in a house (groupby) and performing an aggregation (agg) of the individual variables.\n\nIn another notebook, I show how we can use automated feature engineering to do this, and automated feature engineering should be a standard part of the machine learning workflow. Right now, we'll stick to doing this by hand, but definitely take a look at automated feature engineering in Featuretools.","fe0c5c83":"These variables are now correctly represented as numbers and can be fed into a machine learning model.\n\nTo make operations like that above a little easier, we'll join together the training and testing dataframes. This is important once we start feature engineering because we want to apply the same operations to both dataframes so we end up with the same features. Later we can separate out the sets based on the Target.","d75f2d18":"#### Function to Plot Value Counts\nSince we might want to plot value counts for different columns, we can write a simple function that will do it for us!","4f97838b":"## Making a Submission\nIn order to make a submission, we need the test data. Fortunately, we have the test data formatted in exactly the same manner as the train data.\n\nThe format of a testing submission is shown below. Although we are making predictions for each household, we actually need one row per individual (identified by the Id) but only the prediction for the head of household is scored.","58b914b2":"## Machine Learning Modeling\nOnce feature engineering\/construction is done, we can get started with the machine learning! All of our data (both training and testing) is aggregated for each household and so can be directly used in a model. To first show the process of modeling, we'll use the capable Random Forest Classifier in Scikit-Learn. This probably won't get us to the top of the leaderboard, but it will allow us to establish a baseline. Later we'll try several other models including the powerful Gradient Boosting Machine.\n\nTo assess our model, we'll use 10-fold cross validation on the training data. This will essentially train and test the model 10 times using different splits of the training data. 10-fold cross validation is an effective method for estimating the performance of a model on the test set. We want to look at the average performance in cross validation as well as the standard deviation to see how much scores change between the folds. We use the F1 Macro measure to evaluate performance.","411e9ac9":"Well that's a relief! This means that we don't have to worry about a household both where there is no head AND the members have different values of the label! For this problem, according to the organizers, if a household does not have a head, then there is no true label. Therefore, we actually won't use any of the households without a head for training Nonetheless, it's still a good exercise to go through this process of investigating the data!","f8ee341b":"#### Integer Columns\nLet's look at the distribution of unique values in the integer columns. For each column, we'll count the number of unique values and show the result in a bar plot.","d3996d3c":"The Id and idhogar object types make sense because these are identifying variables. However, the other columns seem to be a mix of strings and numbers which we'll need to address before doing any machine learning. ","f77481e8":"This looks like it could be an indicator of more poverty given the higher prevalence of 2: moderate poverty.\n\nThis represents an important point: sometimes the missing information is just as important as the information you are given.","c3dbe041":"It looks like the most common number of tablets to own is 1 if we go only by the data that is present. However, we also need to think about the data that is missing. In this case, it could be that families with a nan in this category just do not own a tablet! If we look at the data definitions, we see that v18q indicates whether or not a family owns a tablet. We should investigate this column combined with the number of tablets to see if our hypothesis holds.\n\nWe can groupby the value of v18q (which is 1 for owns a tablet and 0 for does not) and then calculate the number of null values for v18q1. This will tell us if the null values represent that the family does not own a tablet."}}