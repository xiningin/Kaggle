{"cell_type":{"b9f0bc49":"code","8b4563dd":"code","4d6f41ab":"code","3ddae3de":"code","62a06d6e":"code","f2ee16b8":"code","26c1fd82":"code","e0d849c6":"code","a3b1425c":"code","15aacf14":"code","0eaefd13":"code","c7b8f53f":"code","faa0ebf9":"code","0a15975a":"code","3bc52836":"code","0d898175":"code","ac501809":"code","13afd20a":"code","c8c3d4f4":"code","6ec872c3":"code","ed323739":"code","2ac187e6":"code","3e20723b":"code","0a7fd9d2":"code","78dea588":"code","180ac90a":"code","522a5e54":"code","d0c66147":"code","703a23ee":"code","d53e0ff7":"code","93f172ed":"code","cc03700b":"code","470f2a10":"code","f2f19fd6":"code","974b47ce":"code","66c6245e":"code","8f30f7a7":"code","0516e016":"code","fb991d2e":"code","f177dd28":"code","1851d261":"code","7917793d":"code","65996ed8":"code","3026c509":"code","c9015550":"code","7f544991":"markdown","eb1e5735":"markdown","02a02e6b":"markdown","4fe857ec":"markdown","3ed1d65a":"markdown","ffd0223d":"markdown","626e4a69":"markdown","02a9da88":"markdown","3cc9d111":"markdown","d1b03c18":"markdown","afb3656d":"markdown","e3668c27":"markdown","17e5dedd":"markdown","849ef342":"markdown","8b0cbe4e":"markdown","db2e526d":"markdown","276eb9f2":"markdown"},"source":{"b9f0bc49":"!pip install --upgrade -q wandb","8b4563dd":"! nvidia-smi","4d6f41ab":"import wandb\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nimport IPython","3ddae3de":"data_dir = '..\/input\/flickr-image-dataset\/flickr30k_images'\nimage_dir = f'{data_dir}\/flickr30k_images'\ncsv_file = f'{data_dir}\/results.csv'","62a06d6e":"df = pd.read_csv(csv_file, delimiter='|')\n\nprint(f'[INFO] The shape of dataframe: {df.shape}')\nprint(f'[INFO] The columns in the dataframe: {df.columns}')\nprint(f'[INFO] Unique image names: {len(pd.unique(df[\"image_name\"]))}')","f2ee16b8":"df.columns = ['image_name', 'comment_number', 'comment']\ndel df['comment_number']\n\n# Under scrutiny I had found that 19999 had a messed up entry\ndf['comment'][19999] = ' A dog runs across the grass .'\n\n# Image names now correspond to the absolute position\ndf['image_name'] = image_dir+'\/'+df['image_name']\n\n# <start> comment <end>\ndf['comment'] = \"<start> \"+df['comment']+\" <end>\"","26c1fd82":"# Shuffle the dataframe\ndf = df.sample(frac=1).reset_index(drop=True)\ndf.head()","e0d849c6":"SIZE = len(df)\n\ntrain_size = int(0.7* SIZE) \nval_size = int(0.1* SIZE)\ntest_size = int(0.2* SIZE)\n\ntrain_size, val_size, test_size","a3b1425c":"train_df = df.iloc[:train_size,:]\nval_df = df.iloc[train_size+1:train_size+val_size,:]\ntest_df = df.iloc[train_size+val_size+1:,:]","15aacf14":"# Enter different indices.\nindex = 200\n\nimage_name = train_df['image_name'][index]\ncomment = train_df['comment'][index]\n\nprint(comment)\n\nIPython.display.Image(filename=image_name)","0eaefd13":"# Choose the top 5000 words from the vocabulary\ntop_k = 10000\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=top_k,\n                                                  oov_token=\"<unk>\",\n                                                  filters='!\"#$%&()*+.,-\/:;=?@[\\]^_`{|}~')","c7b8f53f":"# build the vocabulary\ntokenizer.fit_on_texts(train_df['comment'])","faa0ebf9":"# This is a sanity check function\ndef check_vocab(word):\n    i = tokenizer.word_index[word]\n    print(f\"The index of the word: {i}\")\n    print(f\"Index {i} is word {tokenizer.index_word[i]}\")\n    \ncheck_vocab(\"pajama\")","0a15975a":"tokenizer.word_index['<pad>'] = 0\ntokenizer.index_word[0] = '<pad>'","3bc52836":"# Create the tokenized vectors\ntrain_seqs = tokenizer.texts_to_sequences(train_df['comment'])\nval_seqs = tokenizer.texts_to_sequences(val_df['comment'])\ntest_seqs = tokenizer.texts_to_sequences(test_df['comment'])","0d898175":"# Pad each vector to the max_length of the captions\n# If you do not provide a max_length value, pad_sequences calculates it automatically\ntrain_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(train_seqs, padding='post')\nval_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(val_seqs, padding='post')\ntest_cap_vector = tf.keras.preprocessing.sequence.pad_sequences(test_seqs, padding='post')","ac501809":"# Caption vector\ntrain_cap_vector.shape, val_cap_vector.shape, test_cap_vector.shape","13afd20a":"train_cap_ds = tf.data.Dataset.from_tensor_slices(train_cap_vector)\nval_cap_ds = tf.data.Dataset.from_tensor_slices(val_cap_vector)\ntest_cap_ds = tf.data.Dataset.from_tensor_slices(test_cap_vector)","c8c3d4f4":"@tf.function\ndef load_img(image_path):\n    img = tf.io.read_file(image_path)\n    img = tf.image.decode_jpeg(img)\n    img = tf.image.convert_image_dtype(img, tf.float32)\n    img = tf.image.resize(img, (224, 224))\n    return img","6ec872c3":"train_img_name = train_df['image_name'].values\nval_img_name = val_df['image_name'].values\ntest_img_name = test_df['image_name'].values","ed323739":"train_img_ds = tf.data.Dataset.from_tensor_slices(train_img_name).map(load_img)\nval_img_ds = tf.data.Dataset.from_tensor_slices(val_img_name).map(load_img)\ntest_img_ds = tf.data.Dataset.from_tensor_slices(test_img_name).map(load_img)","2ac187e6":"# prefecth and batch the dataset\nAUTOTUNE = tf.data.experimental.AUTOTUNE\nBATCH_SIZE = 512\n\ntrain_ds = tf.data.Dataset.zip((train_img_ds, train_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\nval_ds = tf.data.Dataset.zip((val_img_ds, val_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)\ntest_ds = tf.data.Dataset.zip((test_img_ds, test_cap_ds)).shuffle(42).batch(BATCH_SIZE).prefetch(buffer_size=AUTOTUNE)","3e20723b":"run = wandb.init(project=\"show-and-tell\",\n                 entity=\"collaborativeml\",\n                 name=\"ariG-devJ-img-cap\",\n                 save_code=False)\n\nimages = []\nfor img, cap in test_ds.take(1):\n    batch_size = img.shape[0]\n    for i in range(batch_size):\n        text = []\n        for c in cap[i]:\n            if c.numpy() == 0:\n                break\n            text.append(tokenizer.index_word[c.numpy()])\n        images.append(wandb.Image(img[i],\n                                  caption=' '.join(text)))\nrun.log({\"img\":images})\nrun.finish()","0a7fd9d2":"# Some global variables\nEMBEDDIN_DIM = 512\nVOCAB_SIZE = 10000\nUNITS_RNN = 256","78dea588":"class CNN_Encoder(tf.keras.Model):\n    \n    def __init__(self, embedding_dim):\n        super(CNN_Encoder, self).__init__()\n        self.embedding_dim = embedding_dim\n        \n    def build(self, input_shape):\n        self.resnet = tf.keras.applications.ResNet50(include_top=False,\n                                                     weights='imagenet')\n        self.resnet.trainable=False\n        self.gap = GlobalAveragePooling2D()\n        self.fc = Dense(units=self.embedding_dim,\n                        activation='sigmoid')\n        \n    def call(self, x):\n        x = self.resnet(x)\n        x = self.gap(x)\n        x = self.fc(x)\n        return x","180ac90a":"# Checking the CNN\nencoder = CNN_Encoder(EMBEDDIN_DIM)\nfor image, caption in train_ds.take(1):\n    print(encoder(image).shape)\n    break","522a5e54":"class RNN_Decoder(tf.keras.Model):\n    def __init__(self, embedding_dim, units, vocab_size):\n        super(RNN_Decoder, self).__init__()\n        self.units = units\n        self.embedding_dim = embedding_dim\n        self.vocab_size = vocab_size\n        self.embedding = Embedding(input_dim=self.vocab_size,\n                                   output_dim=self.embedding_dim)\n    \n    def build(self, input_shape):\n        self.gru1 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru2 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru3 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.gru4 = GRU(units=self.units,\n                       return_sequences=True,\n                       return_state=True)\n        self.fc1 = Dense(self.units)\n        self.fc2 = Dense(self.vocab_size)\n\n    def call(self, x, initial_zero=False):\n        # x, (batch, 512)\n        # hidden, (batch, 256)\n        if initial_zero:\n            initial_state = decoder.reset_state(batch_size=x.shape[0])\n            output, state = self.gru1(inputs=x,\n                                      initial_state=initial_state)\n            output, state = self.gru2(inputs=output,\n                                      initial_state=initial_state)\n            output, state = self.gru3(inputs=output,\n                                      initial_state=initial_state)\n            output, state = self.gru4(inputs=output,\n                                      initial_state=initial_state)\n        else:\n            output, state = self.gru1(inputs=x)\n            output, state = self.gru2(inputs=output)\n            output, state = self.gru3(inputs=output)\n            output, state = self.gru4(inputs=output)\n        # output, (batch, 256)\n        x = self.fc1(output)\n        x = self.fc2(x)\n        \n        return x, state\n    \n    def embed(self, x):\n        return self.embedding(x)\n    \n    def reset_state(self, batch_size):\n        return tf.zeros((batch_size, self.units))","d0c66147":"# Checking the RNN\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1) # (batch, 1, 128)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    print(x.shape)\n    predictions, state = decoder(x, True)\n    print(predictions.shape)\n    print(state.shape)","703a23ee":"encoder = CNN_Encoder(EMBEDDIN_DIM)\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)","d53e0ff7":"optimizer = tf.keras.optimizers.Adam()\nloss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(real, pred):\n    mask = tf.math.logical_not(tf.math.equal(real, 0))\n    loss_ = loss_object(real, pred)\n\n    mask = tf.cast(mask, dtype=loss_.dtype)\n    loss_ *= mask\n\n    return tf.reduce_mean(loss_)\n","93f172ed":"@tf.function\ndef train_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    with tf.GradientTape() as tape:\n        features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n        em_words = decoder.embed(target)\n        x = tf.concat([features,em_words],axis=1)\n        predictions, _ = decoder(x, True)\n\n        loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n\n    trainable_variables = encoder.trainable_variables + decoder.trainable_variables\n\n    gradients = tape.gradient(loss, trainable_variables)\n\n    optimizer.apply_gradients(zip(gradients, trainable_variables))\n\n    return loss","cc03700b":"@tf.function\ndef val_step(img_tensor, target):\n    # img_tensor (batch, 224,224,3)\n    # target     (batch, 80)\n    loss = 0\n    features = tf.expand_dims(encoder(img_tensor),1) # (batch, 1, 128)\n    em_words = decoder.embed(target)\n    x = tf.concat([features,em_words],axis=1)\n    predictions, _ = decoder(x, True)\n    loss = loss_function(target[:,1:], predictions[:,1:-1,:])\n    return loss","470f2a10":"from tqdm import tqdm","f2f19fd6":"\nEPOCHS = 15\nepoch_wise_loss = []\nepoch_wise_val_loss = []\nfor epoch in range(EPOCHS):\n    batch_wise_loss = []\n    for (batch, (img_tensor, target)) in enumerate(train_ds):\n        loss = train_step(img_tensor, target)\n        batch_wise_loss.append(loss.numpy())\n        if batch%100 == 0:\n            print(f'Epoch: {epoch} Batch: {batch} Loss: {batch_wise_loss[-1]:.3f}')\n    epoch_wise_loss.append(np.mean(batch_wise_loss))\n    \n    batch_wise_val_loss = []\n    for (batch, (img_tensor, target)) in enumerate(val_ds):\n        loss = val_step(img_tensor, target)\n        batch_wise_val_loss.append(loss.numpy())\n    epoch_wise_val_loss.append(np.mean(batch_wise_val_loss))\n    print(f'Epoch: {epoch} Total Loss: {epoch_wise_loss[-1]:.3f} Val Loss:{epoch_wise_val_loss[-1]:.3f}')\n    print('-'*40)","974b47ce":"run = wandb.init(project=\"show-and-tell\",\n                 entity=\"collaborativeml\",\n                 name=\"ariG-devJ-model_train\",\n                 save_code=False)\nwith run:\n    for idx,loss in enumerate(epoch_wise_loss):\n        run.log({\"Train_Loss\":loss, \"Epoch_Train\":idx})\n    for idx,loss in enumerate(epoch_wise_val_loss):\n        run.log({\"Val_Loss\":loss, \"Epoch_Val\":idx})","66c6245e":"!mkdir models\nencoder.save_weights('.\/models\/encoder.h5')\ndecoder.save_weights('.\/models\/decoder.h5')","8f30f7a7":"encoder = CNN_Encoder(EMBEDDIN_DIM)\nfor image, caption in train_ds.take(1):\n    encoder(image)\n\ndecoder = RNN_Decoder(embedding_dim=EMBEDDIN_DIM,\n                      units=UNITS_RNN,\n                      vocab_size=VOCAB_SIZE)\nfor image, caption in train_ds.take(1):\n    features = tf.expand_dims(encoder(image),1)\n    em_words = decoder.embed(caption)\n    x = tf.concat([features,em_words],axis=1)\n    predictions, state = decoder(x, True)\n\nencoder.load_weights('.\/models\/encoder.h5')\ndecoder.load_weights('.\/models\/decoder.h5')","0516e016":"batch_loss = []\nfor (batch, (img_tensor, target)) in tqdm(enumerate(test_ds.take(10))):\n    loss = val_step(img_tensor, target)\n    batch_loss.append(loss.numpy())\nprint(f'Test Loss: {np.mean(batch_loss):.3f}')","fb991d2e":"run = wandb.init(project=\"show-and-tell\",\n                 entity=\"collaborativeml\",\n                 name=\"ariG-devJ-models\",\n                 save_code=False)\nwith run:\n    trained_model_artifact = wandb.Artifact(\"encoder_decoder\",\n                                            type=\"model\",\n                                            description=\"vision encoder and text decoder\",)\n    trained_model_artifact.add_dir('.\/models\/')\n    run.log_artifact(trained_model_artifact)","f177dd28":"img, cap = next(iter(test_ds.take(1)))\n\nimg[0].shape, cap[0].shape","1851d261":"img = tf.expand_dims(img[0],0)\ncap = tf.expand_dims(cap[0],0)\n\nimg.shape, cap.shape","7917793d":"feature = tf.expand_dims(encoder(img),1) # (1, 1, 128)\n\nfeature.shape","65996ed8":"# For the image\nprediction, _ = decoder(feature, True)\nprint(prediction.shape)","3026c509":"word = tf.reshape(tokenizer.word_index['<start>'], shape=(1,1))\nem_words = decoder.embed(word)\nprint(em_words.shape)\n\nprediction, _ = decoder(em_words)\nidx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\nword = tokenizer.index_word[idx]\nprint(word)","c9015550":"count = 0\nwhile word != '<end>':\n    print(word, end=\" \")\n    if count > 100:\n        break\n    word_int = tf.reshape(tokenizer.word_index[word], shape=(1,1))  \n    em_words = decoder.embed(word_int)\n    prediction, _ = decoder(em_words)\n    idx = tf.random.categorical(tf.squeeze(prediction,1), 1)[0][0].numpy()\n    word = tokenizer.index_word[idx]\n    count += 1\n\nplt.imshow(img[0])\nplt.show()","7f544991":"## Checking with an image","eb1e5735":"We use `Adam` as the optimizer.\n\nThe loss is `SparseCategoricalCrossentropy`, because here it would be inefficient to use one-hot-encoders are the ground truth. We will also use mask to help mask the `<pad>` so that we do not let the sequence model learn to overfit on the same.","02a02e6b":"Here we are padding the sentences so that each of the sentences are of the same length.","4fe857ec":"Here we fit the `tokenizer` object on the captions. This helps in the updation of the vocab that the `tokenizer` object might have.\n\nIn the first iteration the vocabulary does not start from `0`. Both the dictionaries have 1 as the key or value.","3ed1d65a":"Save the weights","ffd0223d":"# Text Handling\n- Defined the size of the vocab which is `5000`.\n- Initialized the Tokenizer class.\n    - Standardized (all to lower case)\n    - Filters the punctuations\n    - Splits the text\n    - Creates the vocabulary (`<start>, <end> and <unk>` is defined)","626e4a69":"# Imports\nThe following packages are imported:\n- tensorflow\n- matplotlib\n- numpy\n- IPython\n","02a9da88":"Sanity check for the division of datasets","3cc9d111":"Here we read the csv file as a dataframe and make some observations from it.\nFor a quick EDA we are going to \n- check the shape of the dataframe\n- check the names of the columns\n- find out the unique image names there are","d1b03c18":"# Introduction\nThis notebook is a basic implementation of [Show and Tell: A Neural Image Caption Generator](https:\/\/arxiv.org\/abs\/1411.4555) by Oriol Vinyals et. al. In this paper the authors have suggested an end to end solution to an image caption generator. Previous to this paper, all that was proposed for this task involved independent task optimization (vision and natural language) and then a hand engineered stitching of theses independent tasks.\n\nThis paper takes its inspiration from Neural Machine Translation, where an encoder trains on a sequence in a given language and produces a **fixed length representation** for a decoder, that spits a sequence in another language. Stemming from this idea, the authors have used a vision feature extractor as the encoder and a sequence model as the decoder.\n![image.png](attachment:image.png)","afb3656d":"# Joint data","e3668c27":"# Inference\n## Total Loss","17e5dedd":"Splitting the dataframe accordingly","849ef342":"## Model\n### Show (Encoder)\n- InceptionV3: This will act like the feature extractor\n- Use an FC layer to extract the features of the image\n- The features will be used as the initial hidden state for the RNN\n\n### Tell (Decoder)\n- The initial hidden state is used\n- The text is embedded\n- Usage of an LSTM to produce softmax on the vocab\n- Loss with captions","8b0cbe4e":"A quick observation here is to see that the dataframe has `158915` elements but only `31783` image names. This means that there is a duplicacy involved. On further inspection we will see that each image has 5 unique captions attached to it ($31783\\times 5=158915$)\n\nWhile looking into the dataframe I found out that `19999` had some messed up entries. This has led me to manually change the entries in that row.","db2e526d":"# Data\n- data_dir is the path to the images and the `results.csv`\n- image_dir is the path exculsively to the images\n- csv_file is the path to the `results.csv` file","276eb9f2":"# Image Handling\n- Load the image\n- decode jpeg\n- resize\n- standardize"}}