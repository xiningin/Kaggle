{"cell_type":{"64d560b9":"code","78e802ad":"code","10fe0266":"code","99e1a915":"code","3fdee4af":"code","c1ea2957":"code","866b4f39":"code","8b504800":"code","2e55177e":"code","1b419208":"code","4a01689d":"code","71745780":"code","031c559f":"code","ce10f116":"code","349d62a8":"code","61eca9d0":"code","8229aa0e":"code","cd2217c9":"code","3973df3f":"code","904e4a03":"code","4a583a05":"code","afc72f19":"code","5692cbe1":"code","c6e94edb":"code","23dc5e05":"code","4a29c1bb":"code","38290954":"code","bf978167":"code","95ae5bb3":"code","deffef76":"code","44d96eab":"code","5a1bf17b":"code","7647fdd0":"code","9b7702ef":"code","945c0fd9":"code","8a6cc7dc":"code","0a1c0f35":"code","2c9fbee9":"code","c235bd1b":"code","c3e05906":"code","c5a9368e":"code","65dd5e1a":"code","f51717dd":"code","22dd9e47":"code","6120d4e4":"code","c3df37b2":"code","7d32f2a7":"code","818570b8":"code","1a5af70a":"code","e0a78e71":"code","84b99d76":"code","f5edc3a1":"code","ac3b8ba0":"code","df2738bb":"code","bfc6b864":"code","6cf525ec":"code","7a9960da":"code","d1c182ba":"code","bc688045":"code","01218a51":"code","a02dda66":"code","a87b65cd":"code","188e16b2":"code","b775056e":"code","2904f314":"code","d5299ad0":"code","bc753cd5":"code","f93678c9":"code","0a6f5d37":"code","d49ee444":"code","295bfd51":"code","717ae4cb":"markdown","16b4bcf5":"markdown","46e29b45":"markdown","a2117324":"markdown","218cad91":"markdown","3f65f0b8":"markdown","09bc2421":"markdown","532b5731":"markdown","6b3eddfe":"markdown","bbdf8c0c":"markdown","7e418c95":"markdown","4bab9d6c":"markdown","aca3bc62":"markdown","d968456a":"markdown","e314b16e":"markdown","7769d4ec":"markdown","1ca49db7":"markdown","a2cb44ab":"markdown","7fedb2bd":"markdown","41d83a3c":"markdown","99147c99":"markdown","78caed10":"markdown","1e90a88a":"markdown","c7d7bec8":"markdown","87efffc8":"markdown","a6ac8299":"markdown","75ba0487":"markdown","f9448ff4":"markdown","bf719057":"markdown","d7baa2d7":"markdown","b3805fa6":"markdown","809b1f4b":"markdown","710c4317":"markdown","e6783d5f":"markdown","397aaebb":"markdown"},"source":{"64d560b9":"import numpy as np\nimport pandas as pd\nfrom pandas_summary import DataFrameSummary\n\nfrom lightgbm import LGBMClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier, StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.inspection import plot_partial_dependence  # newly learnt this time!\n\nfrom sklearn.metrics import classification_report, recall_score, plot_confusion_matrix, plot_precision_recall_curve, roc_curve\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\n\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom tqdm import tqdm\n\n\nplt.rcParams['axes.unicode_minus'] = False\nplt.style.use('fivethirtyeight')\nsns.set(font_scale = 1)  \npd.set_option('display.max_columns', None)\nwarnings.filterwarnings('ignore')\n\nprint(\"Let's start!\")","78e802ad":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n\ndisplay(df.shape, df.head())","10fe0266":"# When summarize the data you can use DataFrameSummary()\n\ndf_info = DataFrameSummary(df)\ndf_info.summary().T","99e1a915":"# Or customize according to what you'd like to know.\n\ndf_info2 = pd.DataFrame(columns=['Name of Col', 'Num of Null', 'Dtype', 'N_unique'])\n\nfor i in range(0, len(df.columns)):\n    df_info2.loc[i] = [df.columns[i],\n                      df[df.columns[i]].isnull().sum(),\n                      df[df.columns[i]].dtypes,\n                      df[df.columns[i]].nunique()]\n    \ndf_info2","3fdee4af":"x = df['HeartDisease'].value_counts().sort_values()\n\nplt.figure(figsize=(5, 5))\nax = plt.pie(x = x, labels=['0', '1'], autopct = '%1.1f%%', wedgeprops = {'linewidth': 5})\nplt.title('Propotion of \"HeartDisease')\nplt.style.use(['fivethirtyeight'])\nplt.show()","c1ea2957":"ax = sns.countplot(data = df, x = 'HeartDisease')\n\nax.bar_label(ax.containers[0])\nax.set_ylim(0, 550)\n\nplt.show()","866b4f39":"dtype = pd.DataFrame(df_info.summary().loc['types'] == 'numeric')\nnum_cols = dtype[dtype['types'] == True].index.to_list()\nnum_cols","8b504800":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize = (12, 7))\naxes = axes.flatten()\n\nfor col, ax in zip(num_cols, axes):\n    ax = sns.histplot(data = df, x = col, ax = ax, kde = True)\n    ax.set_title(f\"Distribution of {col}\", fontsize=15)\n    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n    \naxes[-1].axis('off')\nplt.show()","2e55177e":"cat_cols = list(set(df.columns) - set(num_cols))\ncat_cols.remove('HeartDisease')\n\ncat_cols","1b419208":"fig, axes = plt.subplots(nrows=2, ncols=3, figsize = (12, 7))\naxes = axes.flatten()\n\nfor col, ax in zip(cat_cols, axes):\n    ax = sns.countplot(data = df, x = col, ax = ax)\n    ax.set_title(f\"Distribution of {col}\", fontsize=15)\n    plt.subplots_adjust(hspace=0.5, wspace=0.3)\n    \nplt.show()","4a01689d":"df['FastingBS'] = df['FastingBS'].astype('O')","71745780":"ss = StandardScaler()\n\ndf[num_cols] = ss.fit_transform(df[num_cols])","031c559f":"df = pd.get_dummies(df, drop_first = True)  # drop_first option\n\ndisplay(df.shape, df.head(3))","ce10f116":"corr = df.corr().round(2)","349d62a8":"plt.figure(figsize = (13, 13))\n\nmask = np.zeros_like(corr, dtype = np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nheat_map = sns.heatmap(corr,\n                       annot = True,\n                       cmap = 'RdYlGn',\n                       mask = mask,\n                       linewidths = 0.01,\n                       cbar_kws = {'shrink' : .5})","61eca9d0":"abs(corr['HeartDisease']).sort_values()[:-1].plot.barh()\nplt.title('absolute value of feature correlation with target')\nplt.show()","8229aa0e":"# First of all, we need to split the data as train and test dataset.\n# We will use train dataset for training and validation.\n\nX = df.drop(['HeartDisease'], axis=1)\ny = df['HeartDisease']\n\nX_trn, X_tst, y_trn, y_tst = train_test_split(X, y, test_size = 0.3, stratify = y, random_state=42)\n\ndisplay(X_trn.shape, y_trn.shape, X_tst.shape, y_tst.shape)","cd2217c9":"X_trn.reset_index(drop=True, inplace=True)\ny_trn.reset_index(drop=True, inplace=True)\nX_tst.reset_index(drop=True, inplace=True)\ny_tst.reset_index(drop=True, inplace=True)","3973df3f":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_lgb = []\nmean_recall = 0\n\nmodel_lgb = LGBMClassifier(objective='binary', random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_lgb.fit(X_train, y_train,\n                  verbose = False,\n                  eval_set = [(X_train, y_train), (X_val, y_val)],\n                  eval_metric = 'logloss',\n                  early_stopping_rounds = 100)\n    \n    y_preds = model_lgb.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_lgb.append(model_lgb.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","904e4a03":"final_preds = np.mean(preds_lgb, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","4a583a05":"cm = plot_confusion_matrix(model_lgb,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_lgb')\nplt.grid()","afc72f19":"# https:\/\/scikit-learn.org\/stable\/modules\/partial_dependence.html#partial-dependence\n# https:\/\/www.kaggle.com\/ohseokkim\/heart-disease-could-our-model-save-lives#Checking-Feature-Importance\n\nall_cols = [c for c in X_trn.columns]\n\nfig,ax = plt.subplots(figsize=(15,30))\nplot_partial_dependence(model_lgb, X_trn, all_cols, ax=ax)\n\nplt.show()","5692cbe1":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_xgb = []\nmean_recall = 0\n\nmodel_xgb = XGBClassifier(objective='binary:logistic', random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_xgb.fit(X_train, y_train,\n                  verbose = False,\n                  eval_set = [(X_train, y_train), (X_val, y_val)],\n                  eval_metric = 'logloss',\n                  early_stopping_rounds = 100)\n    \n    y_preds = model_xgb.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_xgb.append(model_xgb.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","c6e94edb":"final_preds = np.mean(preds_xgb, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","23dc5e05":"cm = plot_confusion_matrix(model_xgb,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_xgb')\nplt.grid()","4a29c1bb":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_rf = []\nmean_recall = 0\n\nmodel_rf = RandomForestClassifier(random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_rf.fit(X_train, y_train)\n    \n    y_preds = model_rf.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_rf.append(model_rf.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","38290954":"final_preds = np.mean(preds_rf, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","bf978167":"cm = plot_confusion_matrix(model_rf,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_xgb')\nplt.grid()","95ae5bb3":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_et = []\nmean_recall = 0\n\nmodel_et = ExtraTreesClassifier(random_state=SEED)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_et.fit(X_train, y_train)\n    \n    y_preds = model_et.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_et.append(model_et.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","deffef76":"final_preds = np.mean(preds_et, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","44d96eab":"cm = plot_confusion_matrix(model_et,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_xgb')\nplt.grid()","5a1bf17b":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_vote = []\nmean_recall = 0\n\nmodel_vote = VotingClassifier(estimators = [('lgb', model_lgb),\n                                            ('xgb', model_xgb),\n                                            ('rf', model_rf),\n                                            ('et', model_et)], voting = 'soft')\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_vote.fit(X_train, y_train)\n    \n    y_preds = model_vote.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_vote.append(model_vote.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","7647fdd0":"final_preds = np.mean(preds_vote, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","9b7702ef":"cm = plot_confusion_matrix(model_vote,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_xgb')\nplt.grid()","945c0fd9":"# Definition of meta model.\n\nmodel_lr = LogisticRegression(random_state=SEED)","8a6cc7dc":"SEED = 42\nn_splits = 5\nskf = StratifiedKFold(n_splits = n_splits, shuffle=True, random_state=SEED)\n\npreds_stack = []\nmean_recall = 0\n\nmodel_stack = StackingClassifier(estimators = [('lgb', model_lgb),\n                                            ('xgb', model_xgb),\n                                            ('rf', model_rf),\n                                            ('et', model_et)], final_estimator= model_lr)\n\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(X_trn, y_trn)):\n    X_train, X_val = X_trn.loc[trn_idx], X_trn.loc[val_idx]\n    y_train, y_val = y_trn.loc[trn_idx], y_trn.loc[val_idx]\n    \n    model_stack.fit(X_train, y_train)\n    \n    y_preds = model_stack.predict(X_val)\n    score = recall_score(y_val, y_preds)\n    mean_recall += score\n    \n    print(f\"Fold {fold}'s score: {score:.4f}\")\n    \n    preds_stack.append(model_stack.predict(X_tst))\n    \nprint(\"========================================\")\nprint(f\"Mean recall of all folds: {mean_recall \/ n_splits:.4f}\")","0a1c0f35":"final_preds = np.mean(preds_stack, axis=0)\nfinal_preds = np.round(final_preds).astype('int64')\n\n\n# classification report comparing the actual y and predicted y after the prediction using actual test data, X_tst.\nprint(classification_report(y_tst, final_preds))","2c9fbee9":"cm = plot_confusion_matrix(model_stack,\n                           X_tst, y_tst,\n                           display_labels=[0, 1],\n                           cmap='Blues')\n\ncm.ax_.set_title('Confusion Matrix_xgb')\nplt.grid()","c235bd1b":"# !pip install pycaret","c3e05906":"from pycaret.classification import *","c5a9368e":"# cat_cols = list(set(df.columns) - set(num_cols))\n# cat_cols.remove('HeartDisease')\n\n# cat_cols","65dd5e1a":"setup(data = df, \n      target = 'HeartDisease',\n      session_id = 42,\n      preprocess = False,\n      numeric_features = all_cols,\n      silent = True\n     )\n#       silent = True","f51717dd":"top5 = compare_models(sort='Recall',n_select = 5,exclude=['ridge','svm','dummy','knn','dt','qda','ada', 'nb', 'lda', 'gbc'])","22dd9e47":"lightgbm = create_model('lightgbm')","6120d4e4":"catboost = create_model('catboost')","c3df37b2":"xgboost = create_model('xgboost')","7d32f2a7":"rf = create_model('rf')","818570b8":"et = create_model('et')","1a5af70a":"lr = create_model('lr')","e0a78e71":"# It seems better not to use tuned_lightgbm\n\ntuned_lightgbm = tune_model(lightgbm, optimize = 'Recall')","84b99d76":"tuned_catboost = tune_model(catboost, optimize = 'Recall')","f5edc3a1":"# xgboost = tune_model(xgboost, optimize = 'Recall')","ac3b8ba0":"tuned_rf = tune_model(rf, optimize = 'Recall')","df2738bb":"tuned_et = tune_model(et, optimize = 'Recall')","bfc6b864":"tuned_lr = tune_model(lr, optimize = 'Recall')","6cf525ec":"evaluate_model(tuned_catboost)","7a9960da":"evaluate_model(tuned_rf)","d1c182ba":"evaluate_model(tuned_et)","bc688045":"evaluate_model(tuned_lr)","01218a51":"blend_soft = blend_models(estimator_list = [xgboost, tuned_rf,lightgbm,tuned_et,tuned_catboost],\n                          optimize = 'Recall',method = 'soft')","a02dda66":"blend_hard = blend_models(estimator_list = [xgboost,tuned_rf,lightgbm,tuned_et,tuned_catboost],\n                          optimize = 'Recall',method = 'hard')","a87b65cd":"plt.figure(figsize=(6, 6))\nplot_model(blend_soft, plot='boundary')","188e16b2":"plt.figure(figsize=(6, 6))\nplot_model(blend_soft, plot = 'auc')","b775056e":"plt.figure(figsize=(6, 6))\nplot_model(blend_soft, plot='confusion_matrix')","2904f314":"stack_model = stack_models(estimator_list = [lightgbm,xgboost, tuned_rf,tuned_catboost], meta_model = tuned_et,optimize = 'Recall')","d5299ad0":"plt.figure(figsize=(6, 6))\nplot_model(stack_model, plot='boundary')","bc753cd5":"plt.figure(figsize=(6, 6))\nplot_model(stack_model, plot = 'auc')","f93678c9":"plot_model(stack_model, plot='confusion_matrix')","0a6f5d37":"final_model = finalize_model(blend_soft)","d49ee444":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='boundary')","295bfd51":"plt.figure(figsize=(6, 6))\nplot_model(final_model, plot='confusion_matrix')","717ae4cb":"**This time I'll use PyCaret which is one of the popular AUTOML tools to optimize the models**\n\n**Using PyCaret you can do all of the above procedure with a few line of code and easy interpretation.**\n\nsource : https:\/\/www.kaggle.com\/ohseokkim\/heart-disease-could-our-model-save-lives\n\nI appreciate the author(Mr.Kim) sharing this great work.","16b4bcf5":"## Extra Trees Classifier","46e29b45":"## LGBM\n\n* First of all, we should notice that it is different from other tree models like XGBoost in terms of the way for the growth of tree. LGBM grows the tree based on a leaf-wise.\n\n* You can check out the detailed explanation from the official documentation **[here](https:\/\/lightgbm.readthedocs.io\/en\/latest\/pythonapi\/lightgbm.LGBMClassifier.html)**\n\n* I learnt a lot about the LGBM HPO using OPTUNA from **[this great work](https:\/\/www.kaggle.com\/bextuychiev\/lgbm-optuna-hyperparameter-tuning-w-understanding)**.\n\n* There are largely 3 kinds of hyperparameters we could get benefits from HPO in case of most tree-based model. But usually these have a lot of overlap, and trade-offs(increasing in one may risk a decrease in another.\n\n<br><br>\n\n**1. Hyperparameters that control the tree structure**\n    \n* **`num_leaves`** : controls the number of decision leaves in a single tree. Theoretically we can set `num_leaves = 2^(max_depth)` to obtain the same number of leaves as depth-wise tree. But with the reason that a leaf-wise tree is typically much deeper than a depth tree for a same `num_leaves`. Therefore you should let it be smaller than `2^(max_depth)` when tuning `num_leaves`. The official documentation recommends that it would be better to set it to be ranged on approx. 60% of the value of `2^(max_depth)`       \n        \n* **`max_depth`** : The higher `max_depth`, the deeper levels the tree has, which makes it more complex and prone to overfit. The empirical recommendation for `max_depth` is a value between 3 and 12.\n        \n* **`min_data_in_leaf`** : Another important parameter to prevent overfitting in a leaf-wise tree. Its optimal value depends on the number of training samples and `num_leaves`. In practice, setting it to hundreds or thousands is enough for a large dataset.\n\n<br>\n\n**2. Hyperparameters for better accuracy**\n\nFor the better accuracy, there are 2 main hyperparameters, `n_estimators` and `learning_rate`. `n_estimators` controls the number of decision trees while `learning_rate` is the step size parameter of the gradient descent to find the global minimum. In other words, we need to find the best combination of `n_estimators` and `learning_rate` in LGBM.\n\n* **`n_estimators`** : use large number with `early_stopping` option.\n\n* **`learning_rate`** : It controls the learning speed. the smaller `learning_rate`, the longer the time for learning. Typically the values lie within 0.01 and 0.1.\n\n<br>\n\n**3. Hyperparameters that control the overfitting**\n\n* **`lambda_l1`, `lambda_l2`** : specify L1 and L2 regularization, respectively. A good search range is (0.0, 1.0) for both. They correspond to `reg_lambda`, `reg_alpha` in XGBoost.\n\n* **`min_gain_to_split`** : This value is the minimal gain to perform split. A conservative search range is (0, 15). It can be used as extra regularization in large parameter grids. It corresponds to `gamma` in XGBoost.\n\n* **`bagging_fraction`(must used with `bagging_freq`)** : It lie within 0.0 and 1.0 as a percentage of training samples to be used to train each tree. you should understand how `bagging_freq` works with bagging fraction when using `bagging_fraction`. 0 means disable bagging `k` means perform bagging at every `k` iteration. Every `k`-th iteration, LightGBM will randomly select `bagging_fraction * 100 %` of the data to use for the next `k` iterations\n\n* **`feature_fraction`** : specifies the percentage of features to sample when training each tree. So, it also takes a value between (0, 1). It corresponds to `colsample_bytree` in XGBoost.\n\n<br><br>\n    ","a2117324":"<br>\n\n## Numerical features","218cad91":"## Stacking Ensembles","3f65f0b8":"roc curve + precision recall curve for each ensemble","09bc2421":"## Stacking Classifier","532b5731":"* Strong correlation: ST, ExerciseAngina, ChestPainType, Oldpeak, MaxHR","6b3eddfe":"<br>\n\n# Conclusion\n\n* This dataset is really good to practice the ensemble technique. Simple, well balanced and no missing values.\n\n* Using HPO via PyCaret API and finalizing the model, we got the highest Recall score 97%(159 \/ (5+159))\n\n* Thanks for Kagglers who willingly shared their knowledge so that it was possible for me to finalize this notebook!","bbdf8c0c":"## Correlation analysis","7e418c95":"## Random Forest Classifier","4bab9d6c":"## Tuning Models","aca3bc62":"# EDA","d968456a":"## Categorical Features","e314b16e":"## Creating Models","7769d4ec":"## Voting Ensembles","1ca49db7":"* **Attribute Information**\n\n\n1. Age: age of the patient [years]\n2. Sex: sex of the patient [M: Male, F: Female]\n3. ChestPainType: chest pain type(\uac00\uc2b4\ud1b5\uc99d\uc720\ud615) [TA: Typical Angina, ATA: Atypical Angina, NAP: Non-Anginal Pain, ASY: Asymptomatic]\n4. RestingBP: resting blood pressure(\uc548\uc815 \ud608\uc555: \ubcd1\uc6d0\uc785\uc6d0\uc2dc) [mm Hg]\n5. Cholesterol: serum cholesterol(\ud608\uccad \ucf5c\ub808\uc2a4\ud14c\ub864) [mm\/dl]\n6. FastingBS: fasting blood sugar(\uacf5\ubcf5\ud608\ub2f9)[1: if FastingBS > 120 mg\/dl, 0: otherwise]\n7. RestingECG: resting electrocardiogram results (\uc548\uc815 \uc2ec\uc804\ub3c4 \uacb0\uacfc)[Normal: Normal, ST: having ST-T wave abnormality (T wave inversions and\/or ST elevation or depression of > 0.05 mV), LVH: showing probable or definite left ventricular hypertrophy by Estes' criteria]\n8. MaxHR: maximum heart rate achieved [Numeric value between 60 and 202]\n9. ExerciseAngina: exercise-induced angina [Y: Yes, N: No]\n10. Oldpeak: oldpeak = ST [Numeric value measured in depression]\n11. ST_Slope: the slope of the peak exercise ST segment [Up: upsloping, Flat: flat, Down: downsloping]\n12. HeartDisease: output class [1: heart disease, 0: Normal]","a2cb44ab":"The data looks well-balanced.","7fedb2bd":"## XGBClassifier","41d83a3c":"All features except FastingBS need to be encoded.","99147c99":"<br>\n\nMean recall of all folds of LGBM model during the K-fold CV is 0.8648, but the recall score of the prediction increases by 0.89.\n\nIt means that training the whold dataset with ensemble technique is quite helpful to prevent the overfitting and imporve the prediction ability of the model.\n\n<br>","78caed10":"## Distribution of target feature","1e90a88a":"<br>\n\n## Voting Classifier","c7d7bec8":"# Model ensembles\n\nThere are 3 kinds of ensembles generally used.\n\n<br>\n\n\n### 1. K-Fold Cross Validation\n\nCross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample.\n\nThe procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into. As such, the procedure is often called k-fold cross-validation. When a specific value for k is chosen, it may be used in place of k in the reference to the model, such as k=10 becoming 10-fold cross-validation.\n\nCross-validation is primarily used in applied machine learning to estimate the skill of a machine learning model on unseen data. That is, to use a limited sample in order to estimate how the model is expected to perform in general when used to make predictions on data not used during the training of the model.\n\nIt is a popular method because it is simple to understand and because it generally results in a less biased or less optimistic estimate of the model skill than other methods, such as a simple train\/test split.\n\nThe general procedure is as follows:\n\n1. Shuffle the dataset randomly.\n2. Split the dataset into k groups\n3. For each unique group:\n    * Take the group as a hold out or test data set\n    * Take the remaining groups as a training data set\n    * Fit a model on the training set and evaluate it on the test set\n    * Retain the evaluation score and discard the model\n4. Summarize the skill of the model using the sample of model evaluation scores\n\n**[Tip]** It is better to use `StratifiedKFold` when K-fold CV because it controls the percentage of each class of the target variable.  \n\n<br>\n\n<img \n     src=\"https:\/\/img1.daumcdn.net\/thumb\/R1280x0\/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FluuGW%2FbtrgjGrj9g6%2FNOsrBEQIr2uTRynM2PZ3h1%2Fimg.png\">\n     \n<br>\n\n\nSource: https:\/\/scikit-learn.org\/stable\/modules\/cross_validation.html, https:\/\/machinelearningmastery.com\/k-fold-cross-validation\/\n\n\n<br><br>\n\n### 2. Voting\n\nVoting is an ensemble method that combines the performances of multiple models to make predictions.\n\nSince voting relies on the performance of many models, they will not be hindered by large errors or misclassifications from one model. A poor performance from one model can be offset by a strong performance from other models.\n\n\n* **Hard Voting vs. Soft Voting**\n\nThere are two ways in voting procedure. Classification by hard-voting is like Winner Take All system. Out of multiple outputs produced by the classifiers, the majority output is chosen to be the final result of the model.\n\nIn contrast, soft-voting is a voting process which every classifiers' outputs are taken into account. Soft-voting sums the predicted probabilities for class lables and returns the final classification with the largest sum probability.\n\n<br>\n\n<img \n     src=\"https:\/\/media.vlpt.us\/images\/jiselectric\/post\/d3eabe8e-b223-4e6d-8a9f-3442f8767d9b\/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202021-01-24%2017.42.36.png\">\n     \n<br>\n\nsource: https:\/\/towardsdatascience.com\/combine-your-machine-learning-models-with-voting-fa1b42790d84, https:\/\/velog.io\/@jiselectric\/Ensemble-Learning-Voting-and-Bagging-at6219ae\n\n<br><br>\n\n### 3. Stacking\n\nWhile bagging and boosting used homogenous weak learners for ensemble, Stacking often considers heterogeneous weak learners, learns them in parallel, and combines them by training a meta-learner to output a prediction based on the different weak learner\u2019s predictions. A meta learner inputs the predictions as the features and the target being the ground truth values in data D(Fig 2.), it attempts to learn how to best combine the input predictions to make a better output prediction. \n\nA general approach for stacking is using meta model as Linear Regression (regression problem) or Logistic Regression (classification problem) to combine the predictions of the sub-models with any learning algorithm.\n\n<br>\n\n<img \n     src=\"https:\/\/editor.analyticsvidhya.com\/uploads\/39725Stacking.png\">\n     \n<br>\n\n\nsource: https:\/\/www.analyticsvidhya.com\/blog\/2021\/08\/ensemble-stacking-for-machine-learning-and-deep-learning\/\n\n<br><br>","87efffc8":"## LGBClassifier","a6ac8299":"# Using PyCaret","75ba0487":"## Set up","f9448ff4":"# Hyper Parameter Optimization\n\n\nJust for my reference about main hyper parameters of LGBM and XGBoost.","bf719057":"## Result of tuning each model","d7baa2d7":"# Data Load and Data Information\n\n\n* **The data is consist of 918 observations and 12 columns(11 independent + 1 target)**","b3805fa6":"# Library Load","809b1f4b":"Numeric features are scaled and others are successfully encoded by one-hot encoding.","710c4317":"## XGBoost\n\n* First of all, we should notice that it is different from other tree models like XGBoost in terms of the way for the growth of tree. LGBM grows the tree based on a leaf-wise.\n\n* You can check out the detailed explanation from the official documentation **[here](https:\/\/xgboost.readthedocs.io\/en\/stable\/)**\n\n* There are largely 3 kinds of hyperparameters we could get benefits from HPO in case of most tree-based model. But usually these have a lot of overlap, and trade-offs(increasing in one may risk a decrease in another.\n\n<br><br>\n\n**1. Hyperparameters that control the tree structure**\n    \n<!-- * **`num_leaves`** : controls the number of decision leaves in a single tree. Theoretically we can set `num_leaves = 2^(max_depth)` to obtain the same number of leaves as depth-wise tree. But with the reason that a leaf-wise tree is typically much deeper than a depth tree for a same `num_leaves`. Therefore you should let it be smaller than `2^(max_depth)` when tuning `num_leaves`. The official documentation recommends that it would be better to set it to be ranged on approx. 60% of the value of `2^(max_depth)`        -->\n        \n* **`max_depth`** : The higher `max_depth`, the deeper levels the tree has, which makes it more complex and prone to overfit. The empirical recommendation for `max_depth` is a value between 3 and 12.\n        \n<!-- * **`min_data_in_leaf`** : Another important parameter to prevent overfitting in a leaf-wise tree. Its optimal value depends on the number of training samples and `num_leaves`. In practice, setting it to hundreds or thousands is enough for a large dataset. -->\n\n<br>\n\n**2. Hyperparameters for better accuracy**\n\nFor the better accuracy, there are 2 main hyperparameters, `n_estimators` and `learning_rate`. `n_estimators` controls the number of decision trees while `learning_rate` is the step size parameter of the gradient descent to find the global minimum. In other words, we need to find the best combination of `n_estimators` and `learning_rate` in LGBM.\n\n* **`n_estimators`** : use large number with `early_stopping` option.\n\n* **`learning_rate`** : It controls the learning speed. the smaller `eta`, the longer the time for learning. Typically the values lie within 0.01 and 0.1. Alias is `eta`\n\n<br>\n\n**3. Hyperparameters that control the overfitting**\n\n* **`reg_lambda`, `reg_alpha`** : specify L1 and L2 regularization, respectively. A good search range is (0.0, 1.0) for both. They correspond to `lambda_l1`, `lambda_l2` in XGBoost.\n\n* **`gamma`** : This value is the minimal gain to perform split. In other words, it is the minimum loss reduction required to make a further partition on a leaf node of the tree.  I will use the same conservative search range (0, 15) as explained in LGBM part above. The larger gamma is, the more conservative the algorithm will be. It corresponds to `min_gain_to_split` in LGBM.\n\n<!-- * **`bagging_fraction`(must used with `bagging_freq`)** : It lie within 0.0 and 1.0 as a percentage of training samples to be used to train each tree. you should understand how `bagging_freq` works with bagging fraction when using `bagging_fraction`. 0 means disable bagging `k` means perform bagging at every `k` iteration. Every `k`-th iteration, LightGBM will randomly select `bagging_fraction * 100 %` of the data to use for the next `k` iterations -->\n\n* **`sub_sample`** : Setting it to 0.5 means that XGBoost would randomly sample half of the training data prior to growing trees. and this will prevent overfitting. It takes a value between (0.0, 1.0).\n\n* **`colsample_bytree`** : specifies the percentage of features to sample when training each tree. It takes a value between (0.0, 1.0). It corresponds to `feature_fraction` in LGBM.\n\n<br><br>\n    ","e6783d5f":"# <p style=\"background-color:navy;font-family:Tahoma;color:ivory;font-size:150%;text-align:center;border-radius:10px 0px;\">Heart Failure Prediction<\/p>\n\n**Problem Statement**\n\nCardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worldwide. Four out of 5CVD deaths are due to heart attacks and strokes, and one-third of these deaths occur prematurely in people under 70 years of age. Heart failure is a common event caused by CVDs and this dataset contains 11 features that can be used to predict a possible heart disease.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n\n\n**Target**\n\nNeed to predict the people who have a high possibility of Cardiovascular diseases.","397aaebb":"# Preprocessing"}}