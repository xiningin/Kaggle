{"cell_type":{"d6b0933c":"code","0d1e920c":"code","f079317f":"code","65571363":"code","1d00d7ab":"code","42693d9f":"code","78e5e489":"code","4c06fdd0":"code","13b3f048":"code","9033f177":"code","e094b289":"code","9283cffc":"code","50780127":"code","1a9d58ba":"code","70638483":"code","9161ea0e":"code","fbc3e14d":"code","55f1553e":"code","5d0407fe":"code","859b32bc":"code","fbfce667":"code","02d2fbfa":"code","a8090d3d":"code","c554aae3":"code","5bbd27a3":"code","ca72d487":"code","8dd62b86":"code","f02221f6":"code","548476b2":"code","8db30f6e":"code","3e2c7f35":"markdown","dba83359":"markdown","72bc9ddf":"markdown","3ae01097":"markdown","ea4456fc":"markdown","e521db1f":"markdown","6e9dcb54":"markdown","b6700534":"markdown","e1ddba32":"markdown","4f39e096":"markdown","76403695":"markdown","da5d6edc":"markdown","44c6ef08":"markdown","8eb44ef7":"markdown","dc81843b":"markdown","f7a501d8":"markdown","4e00560e":"markdown","76cfc8bc":"markdown","dbac63cf":"markdown","9ea0b056":"markdown","5ed26812":"markdown","d516f290":"markdown","e6b59057":"markdown","2fcb65b4":"markdown","831ed982":"markdown","a2baa727":"markdown","9a161a0a":"markdown","a5695439":"markdown","69a6314d":"markdown","716cbd36":"markdown"},"source":{"d6b0933c":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nprint(os.listdir(\"..\/input\"))\n","0d1e920c":"housing = pd.read_csv('..\/input\/housing.csv')\n","f079317f":"housing.head()","65571363":"housing.info()","1d00d7ab":"housing.ocean_proximity.value_counts()","42693d9f":"housing.describe()","78e5e489":"sns.set()\nhousing.isna().sum().sort_values(ascending=True).plot(kind='barh',figsize=(10,7))#Quick peak into the missing columns values\n#Let's deal with that later on the cleaning part with various methods !","4c06fdd0":"housing.hist(bins=50,figsize=(20,15))#The bins parameter is used to custom the number of bins shown on the plots.\nplt.show()","13b3f048":"from sklearn.model_selection import train_test_split\ntrain_, test_ = train_test_split(housing,test_size=0.2,random_state=1)","9033f177":"plotter = housing.copy()","e094b289":"sns.set()\nplt.figure(figsize=(10,8))#Figure size\nplt.scatter('longitude','latitude',data=plotter)\nplt.ylabel('Latitudes')\nplt.xlabel('Longitudes')\nplt.title('Geographical plot of Lats\/Lons')\nplt.show()","9283cffc":"sns.set()\nplt.figure(figsize=(10,8))#Figure size\nplt.scatter('longitude','latitude',data=plotter,alpha=0.1)\nplt.ylabel('Latitudes')\nplt.xlabel('Longitudes')\nplt.title('Geographical plot of Lats\/Lons')\nplt.show()","50780127":"plt.figure(figsize=(10,7))\nplotter.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n        s=plotter[\"population\"]\/100, label=\"population\", figsize=(15,8),\n        c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"),colorbar=True,\n    )\nplt.legend()","1a9d58ba":"corr_matrix=plotter.corr()\ncorr_matrix.median_house_value.sort_values(ascending=False)","70638483":"from pandas import scatter_matrix\nsns.set()\nfeat = ['median_house_value','median_income','total_rooms','housing_median_age']\nscatter_matrix(plotter[feat],figsize=(15,8))","9161ea0e":"plt.figure(figsize=(12,7))\nplt.scatter('median_income','median_house_value',data=plotter,alpha=0.1)\nplt.xlabel('Median income')\nplt.ylabel('Median house value')\nplt.title('Linear correlation Median income\/Median House value')","fbc3e14d":"plotter['rooms_per_household']= plotter.total_rooms\/housing.households","55f1553e":"plotter.head()","5d0407fe":"corr_matrix1=plotter.corr()\ncorr=corr_matrix1.median_house_value.sort_values(ascending=False)\nd= pd.DataFrame({'Column':corr.index,\n                 'Correlation with median_house_value':corr.values})\nd","859b32bc":"#plotter.dropna(subset=[\"total_bedrooms\"]) # option 1 \n#plotter.drop(\"total_bedrooms\", axis=1) # option 2 \n#median = plotter[\"total_bedrooms\"].median() # option 3 \n#plotter[\"total_bedrooms\"].fillna(median, inplace=True)\n","fbfce667":"from sklearn.impute import SimpleImputer\nimputer =SimpleImputer(strategy='median')#In this case its better to use the median to replace missing values","02d2fbfa":"ft_data = plotter.drop('ocean_proximity',axis=1)","a8090d3d":"imputer.fit(ft_data)\n","c554aae3":"imputer.statistics_ #Here's the median of every attribute in our data !","5bbd27a3":"ft_data.total_bedrooms.median()","ca72d487":"X = imputer.transform(ft_data)","8dd62b86":"ft_transformed = pd.DataFrame(X,columns=ft_data.columns)\nft_transformed.tail() #The missing values in total_bedrooms were replaced by the median value","f02221f6":"obj_cols = housing.dtypes\nobj_cols[obj_cols=='object']","548476b2":"sns.set(palette='Set2')\nhousing.ocean_proximity.value_counts().sort_values(ascending=True).plot(kind='barh',figsize=(10,7))\nplt.legend()","8db30f6e":"from sklearn.preprocessing import OneHotEncoder\nlab_encoder = OneHotEncoder()\ncat_house = housing[['ocean_proximity']]\ncat_enc = lab_encoder.fit_transform(cat_house)","3e2c7f35":"> Now you can use this \u201ctrained\u201d imputer to transform the training set by replacing missing values by the learned medians:","dba83359":"> The result is a plain NumPy array containing the transformed features. We want to\nput it back into a Pandas DataFrame, it\u2019s simple:\n","72bc9ddf":"> We have two common ways to get all the attributes to have the same scale\n1. Min-Max Scaling.\n    Many people call it Normalization and its quite simple , values are shifted and rescaled to be in a range of 0 and 1","3ae01097":"> If we run the code ( imputer.fit(data) ) we'll have an error since the imputer doesn't work on objects, and as shown at the very beginning we have a categorical attribute which is **\"Ocean_proximity\"** so we need to drop that.","ea4456fc":"Loading our data , usually done with **Pandas** lib","e521db1f":"> # NOW you're ready to go and start training your model on the train set and test it's accuracy on the test set that we created with the train_test_split function !","6e9dcb54":"> In this case i will one hot encode the labels, we got various encoders for categorical objects, label encoding, ordinal encoder...","b6700534":"> ### Anyway as we showed we need a lot of transformations but thanks to scikit learn that provides a **Pipeline** class to help with such transformations link here : [Pipeline doc'](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html)","e1ddba32":"2. Standardization is a bit different, first it substracts the mean value so standardized values always have a zero mean, then it divides by the standard deviation so that the resulting distribution has unit variance, this is how we calculate standard deviation ( \u00c9cart Type )","4f39e096":"# Data cleaning","76403695":"> The count for each value in ocean_proximity column.","da5d6edc":"> Checking the correlation between the main features with the Pandas function (Scatter_matrix) wich shows linear correlations between the features","44c6ef08":"> Not bad haha ! The number of rooms per household is now more informative than the total number of rooms in a district","8eb44ef7":"> But we don't have a **informative** look on the plot since we need to know the density for each point, let's do a simple modification.\n","dc81843b":"> Scikit learn have a handy class to compute median , mean... strategies.\n We'll use that !","f7a501d8":"> **NB:** \nOne last thing you may want to do before actually preparing the data for Machine Learning algorithms is to try out various attribute combinations. For example, the total number of rooms in a district is not very useful if you don\u2019t know how many households there are. What you really want is the number of rooms per household.","4e00560e":"> I'm commenting those options just to show you how to do them i won't use them in this tutorial","76cfc8bc":"> # EDA Time to have a look on our Data\n    One good practice is to do EDA on the full data and creating a copy of it for not harming our test and training data.","dbac63cf":"> Let's handle our categorical data issue","9ea0b056":"> Now it's much better , and if we're familiar with Californias map we can see clearly that the high-density areas , namely the Bay Area and all around Los Angeles & San diego\nMore generally our brains can spot patterns visually , but we always need to play around with the vizualisations to make the patterns stands out.","5ed26812":"> N is the number of our samples, We sum the Squared difference from mean which means (X(i) - X\u0305) squared then we have our standard deviation, but dont worry we have a lot of ways compute all this, but it's always good to know what your computing.\nTo compute STD ( standard deviation ) we use numpy , exemple : to compute the STD for the median_income we only have to do this --> np.std(data['median_income'])","d516f290":"Since there is geographical information (latitude and longitude), it is a good idea to create a scatterplot of all districts to visualize the data","e6b59057":"> One of the most important transformation step to apply to your data is **Feature Scaling**\n    >Because with some few exceptions, Machine learning algorithm won't perform well since we have different attributes scales, so what we want to do is to scale them , **note that target attribute doesn't have to be scaled**","2fcb65b4":"> Now we can say that the house price is a bit related to the location (e.g close to ocean) and to the density of the population.","831ed982":"![iz](https:\/\/i.imgur.com\/FH9LCE6.png)","a2baa727":"> Most Machine Learning algorithms cannot work with **missing features**, so let\u2019s create a few functions to take care of them. You noticed earlier that the total_bedrooms attribute has some missing values, so let\u2019s fix this. You have three options:\n1. Get rid of the corresponding districts.\n2. Get rid of the whole attribute.\n3. Set the values to some value (zero, the mean, the median, etc.)\n\nSince we don't have a lot of data the first option won't be the best , the second one too because we need that feature , the wisest choice could be the median , we can't affect the mean because we have some outliers this will affect our training model.","9a161a0a":"## Thank you for reading !\n    If you found this helpful an upvote would be very much appreciated ","a5695439":"![img](https:\/\/i.imgur.com\/EFlEx48.png)","69a6314d":"# Hello Fellow kagglers !\n## As the title says this tutorial is for  beginners every part of this kernel summarizes how to get started in ML competitions field and IRL data problems , hope you enjoy this kernel.\n### Let's start !!","716cbd36":"> #### The plot above look like california RIGHT ?![img](https:\/\/california.azureedge.net\/cdt\/CAgovPortal\/images\/Uploads\/menu-living.jpg)"}}