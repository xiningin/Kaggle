{"cell_type":{"15567728":"code","5e0ac841":"code","01ab0da1":"code","903784a8":"code","ddbcc2b4":"code","243bfd7b":"code","5f66bc89":"code","410cbcee":"code","6a79ca8f":"code","5621b80f":"code","e216af40":"code","2afe0901":"code","3759d26e":"code","b5a16504":"code","0e93f447":"code","56cad9cc":"code","36667f8c":"code","ec1cb198":"code","526ec024":"code","cf22b2ae":"code","a7acf71b":"code","0a33a4f8":"code","b56dfa1c":"code","12b9451f":"code","e2b68353":"code","2fd8d8c2":"code","99b1f9f7":"code","c1002ac2":"code","ccccff60":"code","ed9e332b":"markdown","5419fd2b":"markdown","00c199c3":"markdown","b4b0cbba":"markdown","de50fe34":"markdown","df03afe9":"markdown","09828aed":"markdown","6bce6a2f":"markdown","acb7e4d1":"markdown","d2012085":"markdown","f1a37e3b":"markdown","ba29be99":"markdown","1b3c251d":"markdown","814f5496":"markdown","e61c7640":"markdown","777db012":"markdown","3e881131":"markdown","fac97008":"markdown","6d87240d":"markdown"},"source":{"15567728":"!pip install git+https:\/\/github.com\/fastai\/fastai@2e1ccb58121dc648751e2109fc0fbf6925aa8887\n%load_ext autoreload\n%autoreload 2\n%matplotlib inline","5e0ac841":"import numpy as np \nimport pandas as pd \nfrom fastai.imports import*\nfrom fastai.structured import *\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom pandas_summary import DataFrameSummary\nfrom sklearn.ensemble import RandomForestRegressor\nfrom IPython.display import display\nfrom sklearn import metrics\nfrom pprint import pprint\nimport os\nimport shap\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom pdpbox import pdp, get_dataset, info_plots\n\nprint(os.listdir(\"..\/input\"))","01ab0da1":"df_train = pd.read_csv(\"..\/input\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/test.csv\")","903784a8":"df_train['SalePrice'] = np.log(df_train['SalePrice'])","ddbcc2b4":"train_cats(df_train)#Change any columns of strings in a panda's dataframe to a column of categorical values","243bfd7b":"apply_cats(df_test, df_train)","5f66bc89":"train_df, y_trn, nas = proc_df(df_train, 'SalePrice')\ntest_df, _, _ = proc_df(df_test, na_dict=nas)\ntrain_df.head()","410cbcee":"df_test.columns[df_test.isnull().any()]","6a79ca8f":"df_train.columns[df_train.isnull().any()]","5621b80f":"test_df.columns","e216af40":"train_df.columns","2afe0901":"test_df.drop(['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na'], axis =1, inplace = True)\ntrain_df.drop(['LotFrontage_na', 'MasVnrArea_na', 'GarageYrBlt_na'], axis = 1, inplace = True)","3759d26e":"def rmse(x,y): return math.sqrt(((x-y)**2).mean())\n\ndef print_score(m):\n    res = [rmse(m.predict(train_X), train_y), rmse(m.predict(val_X), val_y),     ## RMSE of log of prices\n                m.score(train_X, train_y), m.score(val_X, val_y)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","b5a16504":"train_X, val_X, train_y, val_y = train_test_split(train_df, y_trn, test_size=0.33, random_state=42)","0e93f447":"%time\nm = RandomForestRegressor(n_estimators=1, min_samples_leaf=3, n_jobs=-1, max_depth = 3, oob_score=True) ## Use all CPUs available\nm.fit(train_X, train_y)\n\nprint_score(m)","56cad9cc":"draw_tree(m.estimators_[0], train_X, precision=3)","36667f8c":"# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10, stop = 200, num = 20)]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","ec1cb198":"rf = RandomForestRegressor()\n# Random search of parameters, using 3 fold cross validation, \n# search across 100 different combinations, and use all available cores\nrf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)# Fit the random search model\nrf_random.fit(train_X, train_y)","526ec024":"def evaluate(model, test_features, test_labels):\n    predictions = model.predict(test_features)\n    errors = abs(predictions - test_labels)\n    mape = 100 * np.mean(errors \/ test_labels)\n    accuracy = 100 - mape\n    print('Model Performance')\n    print('Average Error: {:0.4f} degrees.'.format(np.mean(errors)))\n    print('Accuracy = {:0.2f}%.'.format(accuracy))\n    \n    return accuracy\n\nbest_random = rf_random.best_estimator_\nrandom_accuracy = evaluate(best_random, val_X, val_y)","cf22b2ae":"perm = PermutationImportance(rf_random, random_state=1).fit(val_X, val_y)\neli5.show_weights(perm, feature_names = val_X.columns.tolist())","a7acf71b":"preds = np.stack([t.predict(val_X) for t in rf_random.best_estimator_])","0a33a4f8":"preds.shape","b56dfa1c":"preds[:,0], np.mean(preds[:,0]), val_y[0]","12b9451f":"plt.plot([metrics.r2_score(val_y, np.mean(preds[:i+1], axis=0)) for i in range(20)]);","e2b68353":"for feat_name in val_X.columns:\n#for feat_name in base_features:\n    #pdp_dist = pdp.pdp_isolate(model=rf_random.best_estimator_, dataset=val_X, model_features=base_features, feature=feat_name)\n    pdp_dist = pdp.pdp_isolate(model = rf_random.best_estimator_, dataset=val_X, model_features=val_X.columns, feature=feat_name)\n\n    pdp.pdp_plot(pdp_dist, feat_name)\n\n    plt.show()","2fd8d8c2":"explainer = shap.TreeExplainer(rf_random.best_estimator_)\nshap_values = explainer.shap_values(val_X)\n\n# visualize the first prediction's explanation (use matplotlib=True to avoid Javascript)\nshap.force_plot(explainer.expected_value, shap_values[1,:], val_X.iloc[1,:], matplotlib=True) ## change shap and val_X","99b1f9f7":"shap.summary_plot(shap_values, val_X)","c1002ac2":"pred = rf_random.best_estimator_.predict(test_df)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission.head()","ccccff60":"#submission['SalePrice'] = np.exp(pred)   ## Convert log back \nsubmission.to_csv('submission_v2.csv', index=False)","ed9e332b":"We don't want to mess the catagorical values in df_test, values should be same as use in df_raw. There is a function in fast.ai to solve this problem.","5419fd2b":"The actual value is 11.94 all of our predictions comes close to this value. On taking the average of all our predictions we get 11.84, which is a good prediction.","00c199c3":"We have to impute the missing values and store the data as dependent and independent part. This is done by using the fastai function proc_df. The function performs the following tasks:\n\n*     For continuous variables, it checks whether a column has missing values or not\n*     If the column has missing values, it creates another column called columnname_na, which has 1 for missing and 0 for not missing\n*     Simultaneously, the missing values are replaced with the median of the column\n*     For categorical variables, pandas replaces missing values with -1. So proc_df adds 1 to all the values for categorical variables. Thus, we have 0 for missing while all other values are incremented by 1\n","b4b0cbba":"**Table of contents**\n\nImporting Necessary Libraries\n\nPreprocessing\n\nModel Building\n\nCreating a validation set\n\nVisualization\n","de50fe34":"Bulid a single tree","df03afe9":"Credit: \n* https:\/\/www.analyticsvidhya.com\/blog\/2018\/10\/comprehensive-overview-machine-learning-part-1\/\n* https:\/\/www.kaggle.com\/dansbecker\/partial-plots\n","09828aed":"**Importing Necessary Libraries**","6bce6a2f":"As expected, the r^2 becomes better as the number of trees increases","acb7e4d1":"**Fastai implemantation**","d2012085":"The dimensions of the predictions is (130, 482) . This means we have 130 predictions for each row in the validation set.","f1a37e3b":"Use of grid search to find best parameter for Regressor","ba29be99":"Split data","1b3c251d":"**If you like my work Please UPVOTE**","814f5496":"**Preprocessing**","e61c7640":"**Model Building**","777db012":"The evaluation criteria is RMSE of log of Sales Price. So first, let's change the target variable to log","3e881131":"Convert the categorical variables into numbers. We can use the train_cats function from fastai for this purpose","fac97008":"Defining function to calculate the evaluation metric","6d87240d":"**Visualization**"}}