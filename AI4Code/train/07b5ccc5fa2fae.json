{"cell_type":{"d95e56fd":"code","a5cd2bef":"code","0ebbf6c1":"code","dc34541f":"code","979f9e09":"code","4c261eea":"code","2e2fe381":"code","45fe4251":"code","90e75408":"code","fcf292b3":"code","b2fb62d6":"code","1d25e7f5":"code","60e90c21":"code","7c703e58":"code","f1bd1de1":"code","b82fb975":"code","1e7ebcd1":"code","3a45945f":"code","318033ab":"code","1968fa1f":"code","1a35d586":"code","1f38e160":"code","ea360a3b":"code","34f42145":"code","910c54b9":"code","523fd916":"code","35a724ac":"code","c19071b0":"code","1f5ca426":"code","8ef159a4":"code","6e044d56":"code","df66f723":"markdown","e7b10056":"markdown","2422a046":"markdown","7a14d6bd":"markdown","3cacad90":"markdown","77f96525":"markdown","1ee0da37":"markdown","52d53f4f":"markdown","2379cf94":"markdown","567af44d":"markdown","cc10b674":"markdown","9f6acfee":"markdown","913ad13c":"markdown","26b8c310":"markdown","f09954ea":"markdown","dff80284":"markdown","7dd49c51":"markdown","e6571695":"markdown","4438d5a4":"markdown"},"source":{"d95e56fd":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom numpy import mean\nfrom numpy import std\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedStratifiedKFold\nfrom sklearn import metrics","a5cd2bef":"dataset_train = pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/train_csv.csv\")\ndataset_test = pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/test.csv.csv\")\nsubmission = pd.read_csv(\"..\/input\/loan-prediction-practice-av-competition\/sample_submission.csv\")\n\ndataset_train.info()","0ebbf6c1":"dataset_test.info()","dc34541f":"dataset_train.isnull().sum()","979f9e09":"################################################################################################################## Missing Values\ndataset_train = dataset_train.dropna(subset = ['Gender', 'Married', 'LoanAmount', 'Loan_Amount_Term'])\ndataset_train['Dependents'].fillna(dataset_train['Dependents'].mode()[0], inplace = True) \ndataset_train['Self_Employed'].fillna(dataset_train['Self_Employed'].mode()[0], inplace = True) \ndataset_train['Credit_History'].fillna(dataset_train['Credit_History'].mode()[0], inplace = True)\ndataset_test = dataset_test.dropna(subset = ['Gender', 'Married', 'LoanAmount', 'Loan_Amount_Term'])\ndataset_test['Dependents'].fillna(dataset_test['Dependents'].mode()[0], inplace = True) \ndataset_test['Self_Employed'].fillna(dataset_test['Self_Employed'].mode()[0], inplace = True) \ndataset_test['Credit_History'].fillna(dataset_test['Credit_History'].mode()[0], inplace = True)\ndataset_train['Total_Income'] = dataset_train['ApplicantIncome'] + dataset_train['CoapplicantIncome']\ndataset_test['Total_Income'] = dataset_test['ApplicantIncome'] + dataset_test['CoapplicantIncome']","4c261eea":"##################################################################################################################Finding and removing outliers\ndef outlier_remover(dataset, column):\n    q1 = dataset[column].quantile(0.25)\n    q3 = dataset[column].quantile(0.75)\n    iqr = q3 - q1\n    dataset.drop(dataset[(dataset[column] < (q1 - 1.5 * iqr)) | (dataset[column] > (q3 + 1.5 * iqr))].index, inplace = True)\n    return\n\noutlier_remover(dataset_train, 'Total_Income')\noutlier_remover(dataset_test, 'Total_Income')","2e2fe381":"dataset_train['Loan_Status'].value_counts().plot.bar()\nplt.show()","45fe4251":"################################################################################################################## Dealing with categories\ndef replacer_catagory(points, data, items):\n    for i in range (0, points):\n        dataset_train.replace(items[i], i, inplace=True)\n        dataset_test.replace(items[i], i, inplace=True)\n    return","90e75408":"################################################################################################################## Percentage caculator \ndef percentage_calc(data1, data2):\n    percentage = (data1 \/ data2) * 100\n    return percentage","fcf292b3":"################################################################################################################## Difference graph plotter\ndef difference_plotter(points, data, plt_title, plt_diff, items):\n    dataset_train[data].value_counts().plot.bar(title = plt_title)\n    percent_approval = []\n    relevance = 0\n    replacer_catagory(points, data, items)\n    for i in range (0, points):\n        x = dataset_train[data][(dataset_train['Loan_Status'] == 'Y')].value_counts()[i]\n        y = dataset_train[data].value_counts()[i]\n        percent_approval.insert(i, percentage_calc(x, y))\n        print(\"Percentage of loans approved of \", plt_diff[i], \" :-> \", int(percent_approval[i]), '%')\n    for i in range (0, points - 1):\n        relevance = percent_approval[i] - percent_approval[i + 1]\n        if relevance >= 0:\n            temp = plt_diff[i]\n        else:\n            temp = plt_diff[i + 1]\n            relevance = relevance * -1\n        if relevance >= 10:\n            conclusion = \"Which is high and one should be \", temp, \" In order to higher the chances of getting the loan approved\"\n        if (relevance < 10) & (relevance >= 5):\n            conclusion = \"Which is average and is not mandatory \", temp, \" In order to higher the chances of getting the loan approved\"\n        if relevance <= 5:\n            conclusion = \"Which is low and completely optional that one should be \", temp\n        print(\"The difference in loan Approvals is :-> \", int(relevance), \"% \", conclusion)\n    plt.show()\n    return","b2fb62d6":"#########################################################Plot based on Genders\ndifference_plotter(2, 'Gender', 'Type of Gender', ['Male', 'Female'], ['Male', 'Female'])","1d25e7f5":"#########################################################Plot based on Education\ndifference_plotter(2, 'Education', 'Type of Education', ['Graduated', 'Unraduated'], ['Graduate', 'Not Graduate'])","60e90c21":"#########################################################Plot based on self employment\ndifference_plotter(2, 'Self_Employed', 'Self Employment', ['Self_Employed', 'Employed by others'], ['Yes', 'No'])","7c703e58":"#########################################################Plot for Marrital Status\ndifference_plotter(2, 'Married', 'Marrital Status', ['Married', 'Un-Married'], ['Yes', 'No'])","f1bd1de1":"#########################################################Plot for Marrital Status\ndifference_plotter(4, 'Dependents', 'Number of Dependents', ['1 Dependents', '2 Dependents', '3 Dependents', '3+ Dependents'], ['0', '1', '2', '3+'])","b82fb975":"#########################################################Plot for Credit History\ndifference_plotter(2, 'Credit_History', 'Credit History', ['Non Credit History', 'Credit History'], ['1', '0'])","1e7ebcd1":"#########################################################Plot for Property Area\ndifference_plotter(3, 'Property_Area', 'Property Area', ['Rural', 'Semi-Urban', 'Urban'], ['Rural', 'Semiurban', 'Urban'])","3a45945f":"######################################################### Histogram maker\ndef histogram_making(data):\n    dataset_train[data].hist(bins = 50)\n    print('Median value of ' + data +  ' is  :', dataset_train[data].median())\n    plt.show()\n    return","318033ab":"#########################################################Plot for Applicants Income\nhistogram_making('ApplicantIncome')","1968fa1f":"#########################################################Plot for Coapplicant Income\nhistogram_making('CoapplicantIncome')","1a35d586":"#########################################################Plot for Loan Amount\nhistogram_making('LoanAmount')","1f38e160":"######################################################### Group histogram maker\ndef group_histogram_making(bins, groups, data, x_label, conclusion):\n    dataset_train['bins_data'] = pd.cut(dataset_train[data], bins, labels = groups)\n    Income_bin = pd.crosstab(dataset_train['bins_data'], dataset_train['Loan_Status']) \n    Income_bin.div(Income_bin.sum(1).astype(float), axis = 0).plot(kind = \"bar\", stacked = True)\n    plt.xlabel(x_label) \n    plt.ylabel('Percentage')\n    plt.show()\n    print(conclusion)\n    return","ea360a3b":"#########################################################Plot for Applicants Income vs Loan Status\ngroup_histogram_making([0, 1000, 2000, 4000 ,6000, 81000], ['Low', 'Below Average','Average','High', 'Very high'], 'ApplicantIncome', 'Applicant Income', \"The Income should be above 2000 in order to get loan approved with ease\")","34f42145":"#########################################################Plot for CoapplicantStatus Income vs Loan \ngroup_histogram_making([0, 1000, 2000, 3000, 42000], ['Low', 'Average', 'High', 'Very High'], 'CoapplicantIncome', 'Coapplicant Income', \"The Compliant graph shows that lower income are better in getting loans approved, which is a bit not right.\\nThis result can be due to some missing factor\\nLets make a new factor to recognise this behaviour\")","910c54b9":"#########################################################Plot for Total Income vs Loan Status\ngroup_histogram_making([0, 2000, 4000 ,6000, 81000], ['Low','Average','High', 'Very high'], 'Total_Income', 'Total Income', \"The Income should be above 2000 in order to get loan approved with ease\\nThis Behaviour is acceptable and firmly holds the conclusion predicted.\")","523fd916":"#########################################################Plot for Loan Amount vs Loan Status\ngroup_histogram_making([0, 100, 200, 700], ['Low', 'Average', 'High'], 'LoanAmount', 'Loan Amount', \"The low amounts of loans get approved more than the higher amounts but the difference is small to be baising factor\")\ndataset_train = dataset_train.drop(['bins_data'], axis = 1)","35a724ac":"######################################################### Plot for Finding corellations between variables\ndataset_train['Dependents'].replace('3+', 3,inplace=True)\ndataset_train['Loan_Status'].replace('N', 0,inplace=True)\ndataset_train['Loan_Status'].replace('Y', 1,inplace=True)\nmatrix = dataset_train.corr()\nsns.heatmap(matrix, vmax = 1, square = True)\nplt.show()\nprint(\"High Correlation :-> Applicant Income & Loan Amount\\nHigh Correlation :-> Credit History & Loan Amount\\n\")\nprint(\"Average Correlation :-> Compplicant Income & Loan Amount\\nAverage Correlation :-> Dependents & Loan Amount\\n\")","c19071b0":"##################################################################################################################Scaling Total Income and Loan Amount for prediction\ndef approximator(dataset, column, approximation):\n    rounded = []\n    j = 0\n    for i in dataset[column].items():\n        j = approximation * round(i[1] \/ approximation)\n        rounded.append(j)\n    dataset[column] = rounded\n    return\n\napproximator(dataset_train, 'Total_Income', 500)\napproximator(dataset_test, 'Total_Income', 500)","1f5ca426":"################################################################################################################## Model Building for training\ndef get_dataset(dataset_y, dataset_x):\n    y = dataset_y.values\n    X = dataset_x\n    \n    cv = RepeatedStratifiedKFold(n_splits = 10, n_repeats = 3, random_state = 1)\n    scores = cross_val_score(RandomForestClassifier(), X, y, scoring = 'accuracy', cv = cv, n_jobs = -1, error_score = 'raise')\n    print('%.3f (%.3f)' % (mean(scores), std(scores)))\n    \n    model_test = RandomForestClassifier()\n    model_test.fit(X, y)\n    ypred = model_test.predict(dataset_test.drop(['Loan_ID'], 1))\n    return ypred, model_test","8ef159a4":"################################################# get database and plot the result of all models & predicting on new dataset\nypred, model_test = get_dataset(dataset_train['Loan_Status'], pd.get_dummies(dataset_train.drop(['Loan_ID', 'Loan_Status'], 1)))\nsubmission['Loan_ID'] = dataset_test['Loan_ID']\nsubmission['Loan_Status'] = ypred\nsubmission['Loan_Status'].replace({0 : 'N', 1 : 'Y'}, inplace = True)","6e044d56":"approximator(dataset_test, 'LoanAmount', 100)\napproximator(dataset_train, 'LoanAmount', 100)\n\nypred, model_test = get_dataset(dataset_train['LoanAmount'], pd.get_dummies(dataset_train.drop(['Loan_ID', 'LoanAmount'], 1)))\nconfusion_matrix = pd.DataFrame(dataset_test['LoanAmount'])\nconfusion_matrix['predicted'] = ypred\nsns.heatmap(metrics.confusion_matrix(confusion_matrix['LoanAmount'], confusion_matrix['predicted'], labels = confusion_matrix['LoanAmount'].unique()), vmax = 100, annot = True, fmt='g')","df66f723":"The above function calculates the percentages upon calling","e7b10056":"Importing the datasets","2422a046":"Now that all the data visualisation is erformed and our data analytics part is completed. the last thing befor predicting the status and amount is to approximate values which will help reduce randomness of data and provide some beter scores in matchine learner model.","7a14d6bd":"Above function makes histograms for data provided","3cacad90":"Here is the information on dataset we have to test on!\n\nThe test dataset have column ['Loan_Status'] missing which we have to predict!!","77f96525":"The above function perform like category encoding and replacing values in same column with numbers","1ee0da37":"This function approximates the values bu certain amount provided","52d53f4f":"Here is the information on dataset we have to train on!\n","2379cf94":"The above function removes outlier using the IQR score method","567af44d":"Here We can see the ration of loan status aproved vs rejected!!","cc10b674":"Here we are performing a rediction on dataset which wil requir some modifications to the original data and we need to perform feature engineering on it. so let's start the journey!!","9f6acfee":"Above function creates grouped histograms with bins data","913ad13c":"# Model for prediction","26b8c310":"Here we can see that the dataset is having missing values. we can fill the missing values in such a way that the data is not affected by it.","f09954ea":"This is the prediction percentage accuracy of Loan Amount predictions on test dataset which is 71.8% with standard daviation of 4.8%. The predicted loan Amount and the original Loan Amount are shown with the hep of confustion matrix","dff80284":"This above function is used to train the model on dataset_train data","7dd49c51":"Importing the liberaries to work with","e6571695":"This is the prediction percentage accuracy of Loan Status predictions on train dataset which is 79.9% with standard daviation of 3.3%.\nThe predicted loan Status are stored in submission dataset","4438d5a4":"The above function plots a differentiating graph and responds to the amount of differences between variables!!"}}