{"cell_type":{"262ea55f":"code","1554ad74":"code","98cb881e":"code","fc700ed5":"code","9ef6e518":"code","b6bb9463":"code","f66573b0":"code","fdf1427e":"code","5bbddfc1":"code","fdf2921a":"code","5f6ffcb3":"markdown"},"source":{"262ea55f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1554ad74":"import pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy.sparse import hstack, vstack\nimport csv\nimport re\nfrom nltk.corpus import stopwords\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nimport gensim\nfrom gensim.models import KeyedVectors\nfrom gensim.test.utils import datapath\nfrom sklearn.ensemble import RandomForestRegressor\nimport nltk\nimport string\n# !pip install contractions\n# import contractions\n# nltk.download('stopwords')\n# nltk.download('wordnet')","98cb881e":"REPLACE_BY_SPACE_RE = re.compile('[\/(){}\\[\\]\\|@,;]')\nBAD_SYMBOLS_RE = re.compile('[^a-z ]')\nSTOPWORDS = set(stopwords.words('english'))\nstemmer = nltk.stem.WordNetLemmatizer()\n\ndef text_prepare(text):\n    \"\"\"\n        text: a string\n        \n        return: modified initial string\n    \"\"\"\n    text = text.lower()# lowercase text\n#     text = \" \".join([contractions.fix(t) for t in text.split(' ')])\n    text = re.sub(REPLACE_BY_SPACE_RE, ' ', text)# replace REPLACE_BY_SPACE_RE symbols by space in text\n    text = re.sub(BAD_SYMBOLS_RE, '', text)# delete symbols which are in BAD_SYMBOLS_RE from text\n    text = ' '.join([word.strip(string.punctuation) for word in text.split(' ')])\n    text = ' '.join([t for t in text.split(\" \") if len(t) > 2])\n    text = ' '.join([stemmer.lemmatize(x) for x in text.split() if x not in STOPWORDS])# delete stopwords from text\n    return text\n    \n    ","fc700ed5":"df = pd.read_csv(\"\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv\", sep=\"\\t\")\ndf.head()","9ef6e518":"print(df.iloc[0][\"Phrase\"])\nprint(df[\"Sentiment\"].value_counts())","b6bb9463":"# sample run of function\ntext_prepare(df.iloc[0][\"Phrase\"])","f66573b0":"df[\"Phrase\"] = df[\"Phrase\"].apply(text_prepare)","fdf1427e":"# tfidf = TfidfVectorizer(ngram_range=(1,2))\n# features = tfidf.fit_transform(np.array(train_x)) #fit transform to learn vocubulary and transform in matrix\n\ntrainx = df[\"Phrase\"]\ntrainy = df[\"Sentiment\"]\n\ntfidf = TfidfVectorizer(min_df=5, max_df=0.5, ngram_range=(1,3))\nfeatures = tfidf.fit_transform(np.array(trainx)) #fit transform to learn vocubulary and transform in matrix\n\n\nregressor = OneVsRestClassifier(LogisticRegression())\n\nregressor.fit(features, np.array(trainy))\n# regressor.fit(np.array(question2vec_result), np.array(train_y))","5bbddfc1":"df1 = pd.read_csv(\"\/kaggle\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv\", sep=\"\\t\")\ndf1.head()","fdf2921a":"df1[\"Phrase\"] = df1[\"Phrase\"].apply(text_prepare)\ntestx = df1[\"Phrase\"]\n\nfeatures3 = tfidf.transform(np.array(testx))\n\n\nx3 = regressor.predict(features3)\n\nx4 = pd.DataFrame({'PhraseId':np.array(df1[\"PhraseId\"]), 'Sentiment':x3})\nx4[\"Sentiment\"] = x4[\"Sentiment\"].apply(lambda x:int(round(x)))\nx4.to_csv(\"output.csv\", index=False)","5f6ffcb3":"The sentiment labels are:\n\n0 - negative\n1 - somewhat negative\n2 - neutral\n3 - somewhat positive\n4 - positive"}}