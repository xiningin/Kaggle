{"cell_type":{"ca26cb92":"code","96e4d5f8":"code","a48b7cb9":"code","18a96368":"code","5a8ffa3b":"code","08be7ae7":"code","c881600b":"code","9b26dff3":"code","6ece670a":"code","4b0a244c":"code","a62958ca":"code","88ddd1f6":"code","80c49ecc":"code","76557982":"code","7f0da3ff":"code","ac0e75b1":"code","5bdfd8c6":"code","1f0e7c31":"code","b2fa8fa1":"code","e18d2436":"code","47543c04":"code","5ec3e3ad":"code","15a90c9b":"code","19d9959e":"code","4d159e5e":"code","ac5a906e":"code","8b34a6ed":"code","bbb4be69":"code","760ac509":"code","33c27b47":"code","e47861b7":"code","638ea6ae":"code","50d0a149":"code","a0a372f8":"code","ee47d33b":"code","9406cc36":"code","86a69290":"code","e28d8979":"code","da778f89":"code","0fda9d4d":"code","e54181fa":"code","6fa243cc":"code","7298bc12":"code","aa30d6e7":"code","b30809bf":"code","76cc2020":"code","da885400":"code","89b2b94e":"code","6fb03701":"code","6576716c":"code","97c4353d":"code","823ad90f":"markdown","2ab7c6bd":"markdown","28f8c12a":"markdown","550c1da8":"markdown","26f363d2":"markdown","c9855f91":"markdown","05bc1fb1":"markdown","b1d9088e":"markdown","ff227643":"markdown","596fe273":"markdown","c9b35a6d":"markdown","4f5120dc":"markdown"},"source":{"ca26cb92":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom tqdm import tqdm","96e4d5f8":"df=pd.read_csv('\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv')\ndf.head()","a48b7cb9":"df.info()","18a96368":"# Distribution of our data\ndf.hist(bins=25,figsize=(20,8))","5a8ffa3b":"# Correlation \ncorr=df.corr()\nf,ax=plt.subplots(1,1,figsize=(12,8))\nsns.heatmap(corr, annot=True, cmap=\"YlGnBu\", ax=ax)","08be7ae7":"ax=sns.countplot('Outcome', data=df)\nprint('1:- Diabetes....|||||....0:- healthy')","c881600b":"df=df.loc[(df.BMI>10) & (df.BloodPressure>20) & (df.Glucose>25)]","9b26dff3":"# Distribution of our data\ndf.hist(bins=25,figsize=(20,8))","6ece670a":"df.info()","4b0a244c":"df.describe()","a62958ca":"df.SkinThickness.hist(bins=20)","88ddd1f6":"df.loc[(df.SkinThickness<5)& (df.Outcome==0), 'SkinThickness']=int(df[(df.Outcome==0)]['SkinThickness'].median())\ndf.loc[(df.SkinThickness<5)& (df.Outcome==1), 'SkinThickness']=int(df[(df.Outcome==1)]['SkinThickness'].median())","80c49ecc":"df.loc[(df.Insulin==0)& (df.Outcome==0), 'Insulin']=int(df[(df.Outcome==0)]['Insulin'].median())\ndf.loc[(df.Insulin==0)& (df.Outcome==1), 'Insulin']=int(df[(df.Outcome==1)]['Insulin'].median())","76557982":"df.Insulin.hist(bins=20)","7f0da3ff":"df.sample(6)","ac0e75b1":"scaler = StandardScaler()\ndata_x=scaler.fit_transform(df.drop(['Outcome'], axis=1))\n#data_x=df.drop(['Outcome'], axis=1)","5bdfd8c6":"data_y=df.Outcome.values\n#data_y=data_y.reshape((-1,1))","1f0e7c31":"data_x.shape,data_y.shape","b2fa8fa1":"from sklearn.decomposition import PCA","e18d2436":"xtrain,xtest,ytrain,ytest=train_test_split(data_x,data_y,random_state=998)\nxtrain.shape, xtest.shape","47543c04":"pca=PCA(n_components=2)\npca.fit(xtrain)","5ec3e3ad":"pca_xtrain=pca.transform(xtrain)\npca_xtest=pca.transform(xtest)\npca_xtrain.shape, pca_xtest.shape","15a90c9b":"def plot_2d(x_train,y_train,x_test,y_test):\n    plt.figure(figsize=(16,8))\n    sns.scatterplot(x=x_train[:,0], y=x_train[:,1], hue=y_train, marker = 'v', alpha=0.9,)\n    sns.scatterplot(x=x_test[:,0], y=x_test[:,1], hue=y_test, alpha=0.8,  marker = 'o')\n    ","19d9959e":"plot_2d(pca_xtrain,ytrain, pca_xtest, ytest)","4d159e5e":"# imports we need............\nfrom sklearn.model_selection import cross_val_score, ShuffleSplit, train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC","ac5a906e":"def fit(model, cv):\n    return cross_val_score(model,data_x, data_y, cv=cv).mean()","8b34a6ed":"cv = ShuffleSplit(n_splits=10, test_size=0.18)\nacc=[]\nfor i in range(1,21):\n    log_clf=LogisticRegression(C=i)\n    acc.append(fit(log_clf,cv))\nplt.grid(True)\nplt.plot(acc ,marker='o')","bbb4be69":"cv = ShuffleSplit(n_splits=10, test_size=0.18)\nacc=[]\nfor i in tqdm(range(1,76)):\n    log_clf=KNeighborsClassifier(n_neighbors=i)\n    acc.append(fit(log_clf,cv))\nplt.figure(figsize=(12,5))\nplt.grid(True)\nplt.plot(acc ,marker='o')","760ac509":"cv = ShuffleSplit(n_splits=10, test_size=0.18)\nacc=[]\nfor i in tqdm(range(1,60)):\n    log_clf=SVC(C=i)\n    acc.append(fit(log_clf,cv))\n\nplt.figure(figsize=(12,5))\nplt.grid(True)\nplt.plot(acc ,marker='o')","33c27b47":"cv = ShuffleSplit(n_splits=10, test_size=0.18)\nacc=[]\ndict_={}\nfor i in tqdm(range(1,152)):\n    log_clf=RandomForestClassifier(n_estimators=i)\n    Accuracy=fit(log_clf,cv)\n    acc.append(Accuracy)\n    dict_[i]=Accuracy\n\nplt.figure(figsize=(12,5))\nplt.grid(True)\nplt.plot(acc ,marker='o')","e47861b7":"sorted(dict_.items(), key=lambda x: x[1], reverse=True)[:6]","638ea6ae":"from sklearn.metrics import confusion_matrix,accuracy_score,roc_curve, classification_report\nxtrain,xtest,ytrain,ytest=train_test_split(data_x,data_y,random_state=998)","50d0a149":"rand_clf=RandomForestClassifier(n_estimators=101)\nrand_clf.fit(xtrain,ytrain)","a0a372f8":"print(confusion_matrix(ytest, rand_clf.predict(xtest)))\nprint('Accuracy of our model is: ', accuracy_score(ytest, rand_clf.predict(xtest)))","ee47d33b":"print(classification_report(ytest, rand_clf.predict(xtest)))","9406cc36":"rand_clf=RandomForestClassifier(n_estimators=91)\ncross_val_score(rand_clf,data_x, data_y, cv=cv).mean()","86a69290":"%cd \/kaggle\/working","e28d8979":"import pickle\nPkl_Filename = \"Pima_final.pkl\"  ","da778f89":"with open(Pkl_Filename, 'wb') as file:  \n    pickle.dump(rand_clf, file)","0fda9d4d":"xtrain,xtest,ytrain,ytest=train_test_split(data_x,data_y,random_state=998)","e54181fa":"ytrain.sum(),len(ytrain),ytest.sum(),len(ytest)","6fa243cc":"ytest=ytest.reshape(-1,1)\nytrain=ytrain.reshape(-1,1)","7298bc12":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras.regularizers import l1_l2","aa30d6e7":"check_point=tf.keras.callbacks.ModelCheckpoint(\n    filepath='diabetes.h5', monitor='val_loss', verbose=1, save_best_only=True,\n    save_weights_only=False, mode='min')","b30809bf":"model=Sequential([\n    Dense(80,activation='relu',input_shape=(None,8)),\n    Dropout(0.5),\n    Dense(120,activation='relu', kernel_regularizer=l1_l2()),\n    Dropout(0.5),\n    Dense(128,activation='relu'),\n    Dropout(0.5),\n    Dense(60,activation='relu'),\n    Dropout(0.5),\n    Dense(30,activation='relu'),\n    Dense(1,activation='sigmoid')\n])\n\nmodel.compile(loss='BinaryCrossentropy', optimizer=tf.keras.optimizers.Adam(0.001), metrics=['accuracy'])\nmodel.summary()","76cc2020":"history=model.fit(xtrain,ytrain,epochs=300,validation_data=(xtest,ytest), callbacks=[check_point])","da885400":"plt.figure(1, figsize = (25, 12))\nplt.subplot(1,2,1)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot( history.history[\"loss\"], label = \"Training Loss\")\nplt.plot( history.history[\"val_loss\"], label = \"Validation Loss\")\nplt.grid(True)\nplt.legend()\n\nplt.subplot(1,2,2)\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.plot( history.history[\"accuracy\"], label = \"Training Accuracy\")\nplt.plot( history.history[\"val_accuracy\"], label = \"Validation Accuracy\")\nplt.grid(True)\nplt.legend()","89b2b94e":"model_new=keras.models.load_model('diabetes.h5')","6fb03701":"model_new.evaluate(xtest,ytest)","6576716c":"print(confusion_matrix(ytest, model_new.predict_classes(xtest)))\nprint('Accuracy of our model is: ', accuracy_score(ytest, model_new.predict_classes(xtest)))","97c4353d":"print(classification_report(ytest, model_new.predict_classes(xtest)))","823ad90f":"> Plot our 2D-data","2ab7c6bd":"## LogisticRegression","28f8c12a":"# Save our model","550c1da8":"# Removing outliers !\n### In above Distribution graph we can notice outliers in :\n* **BMI**  \n\nA BMI of less than 18.5 means that a person is underweight. A BMI of between 18.5 and 24.9 is ideal. A BMI of between 25 and 29.9 is overweight. A BMI over 30 indicates obesity.\n<hr>\n\n* **Blood Pressure**\n\nAs a general guide: ideal blood pressure is considered to be between 90\/60mmHg and 120\/80mmHg. high blood pressure is considered to be 140\/90mmHg or higher. low blood pressure is considered to be 90\/60mmHg or lower.\n\n<hr>\n\n* **Glucose**\n\nFor the majority of healthy individuals, normal blood sugar levels are as follows: Between 4.0 to 5.4 mmol\/L (72 to 99 mg\/dL) when fasting. Up to 7.8 mmol\/L (140 mg\/dL) 2 hours after eating.\n\n<hr>\n\n* **SkinThickness**\n\nFor adults, the standard normal values for triceps skinfolds are (see TableH): 2.5mm (men) or about 20% fat; 18.0mm (women) or about 30% fat","26f363d2":"# Machine learning Algo...\n","c9855f91":"# Get feel of data","05bc1fb1":"# Ann","b1d9088e":"> **These are missing values replaced with zeros. Take a look at section 3.7 of this paper:**\nhttps:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352914816300016","ff227643":"# PCA ","596fe273":"## Diabetes mellitus, commonly known as diabetes, is a metabolic disease that causes high blood sugar. The hormone insulin moves sugar from the blood into your cells to be stored or used for energy. With diabetes, your body either doesn't make enough insulin or can't effectively use the insulin it does make\n\n![](https:\/\/images.squarespace-cdn.com\/content\/v1\/53e3bacbe4b022bcdbe1f538\/1504716606889-SRWLJ8EH744M4IHRX3LQ\/ke17ZwdGBToddI8pDm48kDrMjE7hBq4fQV3wYHraitJZw-zPPgdn4jUwVcJE1ZvWQUxwkmyExglNqGp0IvTJZUJFbgE-7XRK3dMEBRBhUpzj2bmKhA1a89vhGCTEuFcMrGIAhTIwGn2DOXg1A8iNSPxvh_zK_LmuDa3ZMbEzfBk\/Diabetes-is-a-Drag.gif)","c9b35a6d":"# When Should You Use Normalization And Standardization:\n\n* **Normalization** is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.\n\n* **Standardization** assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis.","4f5120dc":"> **Must fit pca with only training set. For gaining insights how it performs on real data**"}}