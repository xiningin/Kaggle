{"cell_type":{"ab45646c":"code","f824b999":"code","a8f61fde":"code","386cb95c":"code","437e8ed1":"code","8913049a":"code","07a2ebb7":"code","7a684fc2":"code","2fea7db9":"code","aa9fc1ca":"code","37ec3c86":"code","2d4242dc":"code","1a0067cc":"code","befe670c":"code","dbb76391":"code","92109090":"code","1c266bd9":"code","2e5f0a7f":"code","b7654c0c":"code","7821cb3f":"code","8bb2efb3":"code","c88145e5":"code","e84bfa70":"code","f41f499b":"code","64adf621":"code","241603f5":"code","c9e54cc0":"code","0e6d2004":"code","96b4383f":"code","da560a16":"code","c16af944":"code","eb333fdd":"code","2fd7d0d6":"code","be41913a":"code","80f7db32":"code","23090eb9":"code","172e0cac":"code","9e1fe066":"code","24c00ff5":"code","06c34075":"code","d938ab70":"code","e8145409":"code","8e313fda":"code","e3012069":"code","e5401d69":"code","4808da96":"code","bd49dbf2":"code","c585e786":"code","f8a3cf77":"code","e833d4a8":"markdown","b9c5c403":"markdown","abb8b575":"markdown","fe496986":"markdown","d7f27da0":"markdown","b3c5d972":"markdown","f491a6c8":"markdown","d7429800":"markdown","bd42985e":"markdown","ba002eb5":"markdown","653e7fa6":"markdown","3975f402":"markdown","ba7b24f5":"markdown","6e1332bf":"markdown"},"source":{"ab45646c":"#REQUIRED LIBRARIES\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression,RidgeClassifier,Lasso\n# from sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import PCA\nfrom sklearn.metrics import roc_auc_score,accuracy_score\nfrom sklearn.model_selection import KFold,StratifiedKFold\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier\nimport lightgbm as lgb\nfrom sklearn.utils.extmath import softmax\n# import pickle\n# from sklearn.externals import joblib\n\n# import pandas_profiling as pp\n\nwarnings.filterwarnings('ignore')\ngc.enable()\n%matplotlib inline","f824b999":"#CHECKING ALL AVAILABLE FILES\npath='\/kaggle\/input\/tabular-playground-series-nov-2021\/'\ndata_files=list(os.listdir(path))\ndf_files=pd.DataFrame(data_files,columns=['file_name'])\ndf_files['size_in_gb']=df_files.file_name.apply(lambda x: round(os.path.getsize(path+x)\/(1024*1024*1024),4))\ndf_files['type']=df_files.file_name.apply(lambda x:'file' if os.path.isfile(path+x) else 'directory')\ndf_files['file_count']=df_files[['file_name','type']].apply(lambda x: 0 if x['type']=='file' else len(os.listdir(path+x['file_name'])),axis=1)\n\nprint('Following files are available under path:',path)\ndisplay(df_files)","a8f61fde":"#ALL CUSTOM FUNCTIONS\n\n#FUNCTION FOR PROVIDING FEATURE SUMMARY\ndef feature_summary(df_fa):\n    print('DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=df_fa.columns,columns=col_list)\n    df['null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df['unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df['data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.at[col,'mean']=round(df_fa[col].mean(),4)\n            df.at[col,'median']=round(df_fa[col].median(),4)\n            df.at[col,'mode']=round(df_fa[col].mode()[0],4)\n            df.at[col,'std']=round(df_fa[col].std(),4)\n            df.at[col,'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.at[col,'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.at[col,'sample_values']=list(df_fa[col].unique())\n    display(df_fa.head())      \n    return(df.fillna('-'))\n\n\ndef feature_compare(df_fa,df_ft):\n    print('Train DataFrame shape')\n    print('rows:',df_fa.shape[0])\n    print('cols:',df_fa.shape[1])\n    \n    print('Test DataFrame shape')\n    print('rows:',df_ft.shape[0])\n    print('cols:',df_ft.shape[1])\n    \n    col_list=['null','unique_count','data_type','max\/min','mean','median','mode','std','skewness','sample_values']\n    df=pd.DataFrame(index=pd.MultiIndex.from_product([df_train.columns,['train','test']],names=['features','dataset']),columns=col_list)\n   \n    df.loc[(slice(None),['train']),'null']=list([len(df_fa[col][df_fa[col].isnull()]) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'null']=list([len(df_ft[col][df_ft[col].isnull()]) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    \n    #df['%_Null']=list([len(df_fa[col][df_fa[col].isnull()])\/df_fa.shape[0]*100 for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['train']),'unique_count']=list([len(df_fa[col].unique()) for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'unique_count']=list([len(df_ft[col].unique()) for i,col in enumerate(df_ft.columns)])+['-']\n    \n    df.loc[(slice(None),['train']),'data_type']=list([df_fa[col].dtype for i,col in enumerate(df_fa.columns)])\n    df.loc[(slice(None),['test']),'data_type']=list([df_ft[col].dtype for i,col in enumerate(df_ft.columns)])+['-']\n    \n    for i,col in enumerate(df_fa.columns):\n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max\/min']=str(round(df_fa[col].max(),2))+'\/'+str(round(df_fa[col].min(),2))\n            df.loc[([col],['train']),'mean']=round(df_fa[col].mean(),4)\n            df.loc[([col],['train']),'median']=round(df_fa[col].median(),4)\n            df.loc[([col],['train']),'mode']=round(df_fa[col].mode()[0],4)\n            df.loc[([col],['train']),'std']=round(df_fa[col].std(),4)\n            df.loc[([col],['train']),'skewness']=round(df_fa[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['train']),'max\/min']=str(df_fa[col].max())+'\/'+str(df_fa[col].min())\n        df.loc[([col],['train']),'sample_values']=str(list(df_fa[col].unique()))\n        \n        \n    for i,col in enumerate(df_ft.columns):            \n        if 'float' in str(df_fa[col].dtype) or 'int' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max\/min']=str(round(df_ft[col].max(),2))+'\/'+str(round(df_ft[col].min(),2))\n            df.loc[([col],['test']),'mean']=round(df_ft[col].mean(),4)\n            df.loc[([col],['test']),'median']=round(df_ft[col].median(),4)\n            df.loc[([col],['test']),'mode']=round(df_ft[col].mode()[0],4)\n            df.loc[([col],['test']),'std']=round(df_ft[col].std(),4)\n            df.loc[([col],['test']),'skewness']=round(df_ft[col].skew(),4)\n        elif 'datetime64[ns]' in str(df_fa[col].dtype):\n            df.loc[([col],['test']),'max\/min']=str(df_ft[col].max())+'\/'+str(df_ft[col].min())\n        df.loc[([col],['test']),'sample_values']=str(list(df_ft[col].unique()))\n        \n    return(df.fillna('-'))\n\n#EXTENDING RIDGE CLASSIFIER WITH PREDICT PROBABILITY FUNCITON\n\nclass RidgeClassifierwithProba(RidgeClassifier):\n    def predict_proba(self, X):\n        d = self.decision_function(X)\n        d_2d = np.c_[-d, d]\n        return softmax(d_2d)\n\n    \n#PREDICTION FUNCTIONS\n\ndef response_predictor(X,y,test,iterations,model,model_name):  \n\n    df_preds=pd.DataFrame()\n    df_preds_x=pd.DataFrame()\n    k=1\n    splits=iterations\n    avg_score=0\n\n    #CREATING STRATIFIED FOLDS\n    skf = StratifiedKFold(n_splits=splits, shuffle=True, random_state=200)\n    print('\\nStarting KFold iterations...')\n    for train_index,test_index in skf.split(X,y):\n        df_X=X[train_index,:]\n        df_y=y[train_index]\n        val_X=X[test_index,:]\n        val_y=y[test_index]\n       \n\n    #FITTING MODEL\n        model.fit(df_X,df_y)\n\n    #PREDICTING ON VALIDATION DATA\n        col_name=model_name+'xpreds_'+str(k)\n        preds_x=pd.Series(model.predict_proba(val_X)[:,1])\n        df_preds_x[col_name]=pd.Series(model.predict_proba(X)[:,1])\n\n    #CALCULATING ACCURACY\n        acc=roc_auc_score(val_y,preds_x)\n        print('Iteration:',k,'  roc_auc_score:',acc)\n        if k==1:\n            score=acc\n            best_model=model\n            preds=pd.Series(model.predict_proba(test)[:,1])\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds\n        else:\n            preds1=pd.Series(model.predict_proba(test)[:,1])\n            preds=preds+preds1\n            col_name=model_name+'preds_'+str(k)\n            df_preds[col_name]=preds1\n            if score<acc:\n                score=acc\n                best_model=model\n        avg_score=avg_score+acc        \n        k=k+1\n    print('\\n Best score:',score,' Avg Score:',avg_score\/splits)\n    #TAKING AVERAGE OF PREDICTIONS\n    preds=preds\/splits\n    \n    print('Saving test and train predictions per iteration...')\n    df_preds.to_csv(model_name+'.csv',index=False)\n    df_preds_x.to_csv(model_name+'_.csv',index=False)\n    x_preds=df_preds_x.mean(axis=1)\n    del df_preds,df_preds_x\n    gc.collect()\n    return preds,best_model,x_preds ","386cb95c":"%%time\n#READING TRAIN DATASET\n\ndf_train=pd.read_csv(path+'train.csv')\n\n#READING TEST DATASET AND SUBMISSION FILE\ndf_test=pd.read_csv(path+'test.csv')\ndf_submission=pd.read_csv(path+'sample_submission.csv')","437e8ed1":"%%time\n#UNDERSTANDING TRAIN AND TEST DATASET USING FEATURE BY FEATURE COMPRISON\npd.set_option('display.max_rows', len(df_train.columns)*2)\nfeature_compare(df_train,df_test)","8913049a":"gc.collect()","07a2ebb7":"#CREATING A FEATURE LIST EXCLUDING ID AND TARGET\nfeatures=[col for col in df_train.columns if col!='id' and col!='target']","7a684fc2":"%%time\n#VISUALIZING TRAIN AND TEST FEATURE DISTRIBUTION\nplt.figure()\nfig, ax = plt.subplots(20, 5,figsize=(20,70))\n\nfor i,feature in enumerate(features):\n    plt.subplot(20, 5,i+1)\n    sns.kdeplot(data=df_train[feature],x=df_train[feature],color='red', label='train')\n    plt.axvline(x=df_train[feature].mean(),color='yellow',linestyle='--',label='train mean')\n    sns.kdeplot(df_test[feature],x=df_test[feature],color='grey',label='test')\n    plt.axvline(x=df_test[feature].mean(),color='orange',linestyle='--',label='test mean')\n    plt.xlabel(feature,color='blue')\n    if i%5!=0:\n        plt.ylabel('')\n        \n    else:\n        plt.ylabel('Density',color='blue')\n    plt.legend(loc=1,fontsize='x-small')\n    \n    \nplt.show();","2fea7db9":"#CHECKING TRAIN AND TEST DATASET MEMORY USAGE BEFORE DOWNCASTING\nprint('train dataset data usage information\\n')\ndf_train.info(memory_usage='deep')\nprint('\\ntest dataset data usage information\\n')\ndf_test.info(memory_usage='deep')","aa9fc1ca":"%%time\n#DOWNCASTING TRAIN DATASET\nfor column in df_train.columns:\n    if df_train[column].dtype == \"float64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"float\")\n    if df_train[column].dtype == \"int64\":\n        df_train[column]=pd.to_numeric(df_train[column], downcast=\"integer\")\n        \n#DOWNCASTING TEST DATASET\nfor column in df_test.columns:\n    if df_test[column].dtype == \"float64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"float\")\n    if df_test[column].dtype == \"int64\":\n        df_test[column]=pd.to_numeric(df_test[column], downcast=\"integer\")","37ec3c86":"#CHECKING TRAIN AND TEST DATASET MEMORY USAGE AFTER DOWNCASTING\nprint('train dataset data usage information\\n')\ndf_train.info(memory_usage='deep')\nprint('\\ntest dataset data usage information\\n')\ndf_test.info(memory_usage='deep')","2d4242dc":"gc.collect()","1a0067cc":"#Understanding Target (claim) feature distribution\npie_labels=['Spam-'+str(df_train['target'][df_train.target==1].count()),'No Spam-'+\n            str(df_train['target'][df_train.target==0].count())]\npie_share=[df_train['target'][df_train.target==1].count()\/df_train['target'].count(),\n           df_train['target'][df_train.target==0].count()\/df_train['target'].count()]\nfigureObject, axesObject = plt.subplots(figsize=(6,6))\npie_colors=('yellow','lightgrey')\npie_explode=(.01,.01)\naxesObject.pie(pie_share,labels=pie_labels,explode=pie_explode,autopct='%.2f%%',colors=pie_colors,startangle=30,shadow=True)\naxesObject.axis('equal')\nplt.title('Percentage of Response - No Response Observations',color='blue',fontsize=12)\nplt.show()","befe670c":"#CORRELATION CHECK CATEGORICAL FEATURES\ncorr = df_train[features+['target']].corr()\n\n# Generate a mask for the upper triangle\nmask = np.triu(np.ones_like(corr, dtype=bool))\n\n# Plotting correlation heatmap\nfig,ax=plt.subplots(figsize=(20,20))\nax.set_xticklabels(labels=corr.columns,fontsize=12)\nax.set_yticklabels(labels=corr.columns,fontsize=12)\n# plt.rcParams.update({'font.size': 12})\nsns.heatmap(corr,mask=mask,cmap='tab20c',linewidth=0.1)\nplt.title('Correlation Map',color='blue',fontsize=12)\nplt.show()","dbb76391":"del corr\ngc.collect()","92109090":"%%time\nX=df_train.drop(['id','target'],axis=1)\n\npca = PCA(n_components=2,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2'])\nprincipalDf['target']=df_train['target']\n\nfig = plt.figure(figsize=(15,15))\nsc=plt.scatter(x=principalDf['principal_component_1'], y=principalDf['principal_component_2'],c=principalDf['target'],cmap='Accent')\nplt.legend(*sc.legend_elements(),bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('2D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","1c266bd9":"%%time\nX=df_train.drop(['id','target'],axis=1)\n\npca = PCA(n_components=3,random_state=200)\nprincipalComponents = pca.fit_transform(X)\nprincipalDf = pd.DataFrame(data = principalComponents,columns = ['principal_component_1','principal_component_2','principal_component_3'])\nprincipalDf['target']=df_train['target']\n\nfig = plt.figure(figsize=(15,15))\nax = fig.add_subplot(111, projection = '3d')\n\nax.set_xlabel(\"principal_component_1\")\nax.set_ylabel(\"principal_component_2\")\nax.set_zlabel(\"principal_component_3\")\n\nsc=ax.scatter(xs=principalDf['principal_component_1'], ys=principalDf['principal_component_2'],\n              zs=principalDf['principal_component_3'],c=principalDf['target'],cmap='Accent')\nplt.legend(*sc.legend_elements(), bbox_to_anchor=(1.05, 1), loc=2)\nplt.title('3D Visualization of train Dataset',color='blue',fontsize=12)\nplt.show()","2e5f0a7f":"del X\ngc.collect()","b7654c0c":"%%time\n#SIMPLE FEATURE ENGINEERING, CREATING SOME AGGREGATION FEATURES\ndf_train['sum']=df_train[features].sum(axis=1)\ndf_test['sum']=df_test[features].sum(axis=1)\n\ndf_train['mean']=df_train[features].mean(axis=1)\ndf_test['mean']=df_test[features].mean(axis=1)\n\ndf_train['std'] = df_train[features].std(axis=1)\ndf_test['std'] = df_test[features].std(axis=1)\n\ndf_train['max'] = df_train[features].max(axis=1)\ndf_test['max'] = df_test[features].max(axis=1)\n\ndf_train['min'] = df_train[features].min(axis=1)\ndf_test['min'] = df_test[features].min(axis=1)\n\ndf_train['kurt'] = df_train[features].kurtosis(axis=1)\ndf_test['kurt'] = df_test[features].kurtosis(axis=1)\n\nagg_features= ['sum','mean','std','max','min','kurt']","7821cb3f":"gc.collect()","8bb2efb3":"%%time\nscaler = StandardScaler()\nX = scaler.fit_transform(df_train[features+agg_features])\ntest = scaler.transform(df_test[features+agg_features])\ny=df_train['target'].values","c88145e5":"gc.collect()","e84bfa70":"#FINAL DATASET SHAPES\nX.shape,y.shape,test.shape","f41f499b":"%%time\nlr_params={'C': 0.01, 'penalty': 'l2', 'solver': 'lbfgs'}\nmodel=LogisticRegression(**lr_params)\nprint('Logistic Regression parameters:\\n',model.get_params())\n\nlogistic_predictions,best_logistic_model,LRpreds=response_predictor(X,y,test,10,model,'LR')","64adf621":"gc.collect()","241603f5":"df_submission['target']=logistic_predictions\n#SAVING LOGISTIC PREDICTIONS\ndf_submission.to_csv('logistic_submission.csv',index=False)\ndf_submission","c9e54cc0":"gc.collect()","0e6d2004":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_logistic_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Logistic Regression Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","96b4383f":"%%time\nmodel=RidgeClassifierwithProba()\nprint('Ridge Classifier parameters:\\n',model.get_params())\n\nridge_predictions,best_ridge_model,ridge_preds=response_predictor(X,y,test,10,model,'RC')","da560a16":"gc.collect()","c16af944":"df_submission['target']=ridge_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('ridge_submission.csv',index=False)\ndf_submission","eb333fdd":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_ridge_model.coef_[0]\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance Ridge Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","2fd7d0d6":"gc.collect()","be41913a":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n   }\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=response_predictor(X,y,test,10,model,'LGB')","80f7db32":"gc.collect()","23090eb9":"df_submission['target']=lgb_predictions\n#SAVING LGBM PREDICTIONS\ndf_submission.to_csv('lgb_submission.csv',index=False)\ndf_submission","172e0cac":"df_feature_impt=pd.DataFrame()\ndf_feature_impt['features']=features+agg_features\ndf_feature_impt['importance']=best_lgb_model.feature_importances_\n\ndf_feature_impt.sort_values(by=['importance'],inplace=True,ascending=False)\nplt.figure(figsize = (15,50))\nax=sns.barplot(x=df_feature_impt['importance'],y=df_feature_impt['features'],data=df_feature_impt)\nplt.title('Feature importance LGB Model',color='blue',fontsize=12)\nax.bar_label(ax.containers[0]);","9e1fe066":"gc.collect()","24c00ff5":"#BLENDING PREDICTIONS\ndf_submission['target']=logistic_predictions*0.6+ridge_predictions*0.2+lgb_predictions*0.2\n#CREATING SUMBISSION FILE\ndf_submission.to_csv('submission.csv',index=False)","06c34075":"df_submission","d938ab70":"%%time\n#READING PREDICTED VALUES\ndf_LR_=pd.read_csv('.\/LR_.csv')\ndf_LR=pd.read_csv('.\/LR.csv')\ndf_RC_=pd.read_csv('.\/RC_.csv')\ndf_RC=pd.read_csv('.\/RC.csv')\ndf_LGB_=pd.read_csv('.\/LGB_.csv')\ndf_LGB=pd.read_csv('.\/LGB.csv')\n","e8145409":"%%time\nX=pd.concat([df_LR_,df_RC_,df_LGB_],axis=1).to_numpy()\ntest=pd.concat([df_LR,df_RC,df_LGB],axis=1).to_numpy()","8e313fda":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'verbose' : 0,\n     'learning_rate':0.01,\n    'n_estimators':200\n    }\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=response_predictor(X,y,test,10,model,'CBS')","e3012069":"df_submission['target']=catb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('catbstack_mixed_submission.csv',index=False)\ndf_submission","e5401d69":"%%time\n#STACKING OUTCOMES FROM LINEAR MODELS\nX=pd.concat([df_LR_,df_RC_],axis=1).to_numpy()\ntest=pd.concat([df_LR,df_RC],axis=1).to_numpy()","4808da96":"%%time\nlgbm_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n   }\n\nmodel=lgb.LGBMClassifier(**lgbm_params)\nprint('LGBM parameters:\\n',model.get_params())\n\nlgb_predictions,best_lgb_model,LGBpreds=response_predictor(X,y,test,10,model,'LGB')","bd49dbf2":"df_submission['target']=lgb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('lgbstack_linear_submission.csv',index=False)\ndf_submission","c585e786":"%%time\ncatb_params = {\n    'eval_metric' : 'AUC',\n    'verbose' : 0,\n     'learning_rate':0.01,\n    'n_estimators':200\n    }\nmodel=CatBoostClassifier(**catb_params)\nprint('CatBoost paramters:\\n',model.get_params())\n\ncatb_predictions,best_catb_model,CBpreds=response_predictor(X,y,test,10,model,'CBS')","f8a3cf77":"df_submission['target']=catb_predictions\n#SAVING CATBOOST PREDICTIONS\ndf_submission.to_csv('catbstack_linear_submission.csv',index=False)\ndf_submission","e833d4a8":"<h2 id=\"LogisticRegression\">LogisticRegression<\/h2>\nStarting with base Linear Model, with basic hyperparameter tuning.\n\n<h4>Observations<\/h4>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","b9c5c403":"<h2 id=\"Ridge\">Ridge Classifier<\/h2>\nStarting with base Ridge model, without any hyperparameter tuning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","abb8b575":"<h2 id=\"Corr\">Correlation Check<\/h2>\nLets check if there are any correlated features. If two features are highly correlated we can remove one of the feature.\nThis will help in dimentionality reduction.\n\n<h4>Observation<\/h4>\n<ul>\n    <li>No reasonable correlation is observed<\/li>\n   \n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","fe496986":"<h2 id=\"Stacking\">Stacking approach<\/h2>\n\n<ol>\n    <li>Stacking Linear Regression, Ridge and LGBM. Using Catboost on stacked dataset<\/li>\n    <li>Stacking Linear Regression and Ridge. Using LGBM on stacked dataset<\/li>\n    <li>Stacking Linear Regression and Ridge. Using Catboost on stacked dataset.<\/li>\n<\/ol>\n<h3>Observations<\/h3>\n<ul>\n    <li>Getting best <b>Public Score of 0.74635<\/b> using 3rd approach<\/li>\n<\/ul>\n<br><a href=\"#Approach\">back to main menu<\/a>","d7f27da0":"<h2 id=\"TrainVisual\">Visualizating Training dataset<\/h2>\nWe are making use of PCA, dimentionality reduction technique to Visualize Training dataset.<br>\nVisualization is also helpful in understanding any grouping or patterns within dataset.\n<h4>Observation<\/h4>\nNo pattern or grouping observed in training dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>","b3c5d972":"<h2 id=\"LGBM\">LGBM Classification<\/h2>\n\nSimple LGBMClassifier without any hyperparameter tunning.\n\n<br><a href=\"#Approach\">back to main menu<\/a>","f491a6c8":"<h2 id=\"AggFeatures\">Creating Aggregated features<\/h2>\nCreating aggregated features\n<h4>Observation<\/h4>\n\n\n<br><a href=\"#Approach\">back to main menu<\/a>","d7429800":"<h2 id=\"Downcasting\">Down Casting Training and Testing datasets<\/h2>\nChecking possibility for down casting dataset datatypes. This will help in reducing overall dataset size.\n\n<h4>Observations<\/h4>\n<ul>\n    <li>We have only two data types in dataset float64 and int64<\/li>\n    <li>It is always a good idea to reduce overall dataset size by finding correct datatypes<\/li>\n    <li>With downcasting able to reduce training dataset size from 466.9 MB to 231.7 MB<\/li>\n    <li>With downcasting able to reduce training dataset size from 416.1 MB to 208.1 MB<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","bd42985e":"<h2 id=\"Target\">Understanding Target feature distribution<\/h2>\nLets visualize Target feature.\n\n<h4>Observation<\/h4>\nAs observations have almost equal count of response and no response observations, this is a balanced dataset. \n\n<br><a href=\"#Approach\">back to main menu<\/a>","ba002eb5":"<b>Problem Statement:<\/b>  The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting <b>identifying spam emails<\/b> via various extracted features from the email. We have to predict molecule response probability.\n\n<b>Problem type:<\/b> A binary classification problem.\n\n<b>Evaluation matrix:<\/b> Submissions are evaluated on area under the <b>ROC(receiver operating characteristic)<\/b> curve between the predicted probability and the observed target.","653e7fa6":"<h2 id=\"Blending\">Blending<\/h2>\n\nSimple Blending approach, based on observation\n\n<br><a href=\"#Approach\">back to main menu<\/a>","3975f402":"<h2 id=\"Approach\">Approach to the problem<\/h2>\nIdea is to develop a generalized approach for solving any binary classification problem\n<ol>\n    <li>Performing exploratory data analysis (EDA) and Data Preparation (DP).<\/li>\n    <ol>\n        <li><a href=\"#FeatureSummary\">Understanding Train and Test dataset features (EDA)<\/a><\/li>\n        <li><a href=\"#Downcasting\">Down Casting Training and Testing datasets (DP)<\/a><\/li>\n        <li><a href=\"#Target\">Understanding Target feature distribution (EDA)<\/a><\/li>\n        <li><a href=\"#Corr\">Correlation check (EDA)<\/a><\/li>\n        <li><a href=\"#TrainVisual\">Visualizing Training dataset (EDA)<\/a><\/li>\n        <li><a href=\"#Normalizing\">Normalizing dataset (DP)<\/a><\/li>\n    <\/ol>\n    <li>Feature Engineering.<\/li>\n    <ol>\n        <li><a href=\"#AggFeatures\">Creating Aggregated features<\/a><\/li>\n    <\/ol>\n    <li>Training Linear,Gradient Boost and Ensemble models.<\/li>\n    <ol>\n        <li><a href=\"#LogisticRegression\">Logistic Regression<\/a><\/li>\n        <li><a href=\"#Ridge\">Ridge Classifier<\/a><\/li>\n        <li><a href=\"#LGBM\">LGBM Classification<\/a><\/li>\n    <\/ol>\n    <li><a href=\"#Blending\">Blending<\/a><\/li>\n    <li><a href=\"#Stacking\">Stacking<\/a><\/li>\n   <\/ol>\n\n<h4>Observations<\/h4>\n<ul>\n    <li>Linear models performing better than gradient boost models. Best <b>Logistic Regression Public Score of 0.74553<\/b> submitted by this notebook <\/li>\n    <li>Can't find a blending score better than single model Linear Regression score.<\/li>\n    <li>Achieved best <b>Stacking Public Score of 0.74635<\/b> by Stacking Linear Regression and Ridge outputs to build Catboost model.<\/li>\n   \n<\/ul>","ba7b24f5":"<h2 id=\"FeatureSummary\">Understanding Train and Test dataset features<\/h2>\nUnderstanding Train and Test dataset features in comparative view, using basic statistical measures.\n\n<h4>Observations<\/h4>\n<ul>\n    <li>No missing values in train or test dataset<\/li>\n<\/ul>\n\n<br><a href=\"#Approach\">back to main menu<\/a>","6e1332bf":"<h2 id=\"Normalization\">Normalizing dataset<\/h2>\nUsing Standard Scaler to normalize dataset\n\n<br><a href=\"#Approach\">back to main menu<\/a>\n"}}