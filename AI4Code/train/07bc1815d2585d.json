{"cell_type":{"7195badf":"code","b38e9629":"code","a3af9953":"code","c5cc242e":"code","4d17e722":"code","895e73e5":"code","85fc18f7":"code","79a8cbff":"code","c0f6dced":"code","7d7d9379":"code","b2778019":"code","44c40537":"code","98415ba3":"code","380cd86d":"code","34fed7be":"code","592e2d29":"code","76569092":"code","82a03a2a":"code","9271efb6":"code","0897d828":"code","986966c5":"code","480e1d64":"code","49ebe0ca":"markdown","8ac457be":"markdown","9e57c76d":"markdown","1a1f68c5":"markdown","1ec3594f":"markdown","1a7314c0":"markdown","c66a794b":"markdown","548ad7f8":"markdown","3695ca08":"markdown","2f5413a9":"markdown","ab2e5980":"markdown","7911f032":"markdown","5e8ed6a6":"markdown","4ebbe68c":"markdown","8cb7f52f":"markdown","122038df":"markdown","84642e10":"markdown","b7e11b9b":"markdown"},"source":{"7195badf":"import os\nimport gc\nimport pickle\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom joblib import Parallel, delayed\nfrom sklearn.model_selection import KFold\n\nimport xgboost as xgb\n\ntqdm.pandas()\n%matplotlib inline\n\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","b38e9629":"# in order to preprocess parquet files, use the standard pyarrow lib\n!pip install pyarrow\n\nimport pyarrow.parquet as pq","a3af9953":"# files\ntrain = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/train.csv\")\ntest = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/test.csv\")\nss = pd.read_csv(\"..\/input\/optiver-realized-volatility-prediction\/sample_submission.csv\")\n\nprint(\"Train Info: \\n\")\ntrain.info()\n\nprint(\"\\nTest Info: \\n\")\ntest.info()\n\nprint(\"\\nSample Submission Format: \\n\")\nss.info()","c5cc242e":"train.head()","4d17e722":"train[\"stock_id\"].value_counts()","895e73e5":"train[\"time_id\"].value_counts()","85fc18f7":"stock_example = train[train[\"stock_id\"]==0]\n\nplt.figure(figsize=(20, 8))\nplt.plot(stock_example[\"time_id\"].values, stock_example[\"target\"].values, label=\"Target\")\nplt.plot(stock_example[\"time_id\"].values, stock_example[\"target\"].rolling(window=10).mean().values, label=\"Moving Avg WS10\") #WS=Window Size\nplt.title(\"Target values over time IDs for stock 0\")\nplt.legend()\nplt.show()","79a8cbff":"display(train[\"target\"].describe())\n\nfig, axs = plt.subplots(nrows=1, ncols=2, figsize=(20, 6))\ntrain[\"target\"].plot(kind=\"hist\", bins=100, ax=axs[0], title=\"Target Distribution\")\ntrain[\"target\"].progress_apply(np.log).plot(kind=\"hist\", bins=100, ax=axs[1], title=\"log-transformed(for skewness) Target Distribution\")\n\nplt.figure(figsize=(20, 4))\nplt.boxplot(train[\"target\"], vert=False)\nplt.title(\"Boxplot for Target\")\nplt.show()","c0f6dced":"def book_length(stock_id):\n    return len(pq.read_table(f\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id={stock_id}\"))\n    pass\n\ndef trade_length(stock_id):\n    return len(pq.read_table(f\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id={stock_id}\"))\n    pass\n\ntrain_meta = pd.DataFrame(columns=[\"stock_id\", \"len_order_book\", \"len_trade\"])\ntrain_meta[\"stock_id\"] = train[\"stock_id\"].unique()\ntrain_meta[\"len_order_book\"] = train_meta[\"stock_id\"].progress_apply(book_length)\ntrain_meta[\"len_trade\"] = train_meta[\"stock_id\"].progress_apply(trade_length)\n\ntrain_meta","7d7d9379":"# reading book sample\nprint(\"A book sample: \\n\")\nbook_sample = pq.read_table(\"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\/stock_id=0\")\nbook_sample = book_sample.to_pandas()\ndisplay(book_sample)\n\n# reading trade sample\nprint(\"\\nA trade sample: \\n\")\ntrade_sample = pq.read_table(\"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\/stock_id=0\")\ntrade_sample = trade_sample.to_pandas()\ndisplay(trade_sample)","b2778019":"# for book sample\nprint(\"For order book: \\n\", book_sample[\"time_id\"].value_counts())\n# for trade sample\nprint(\"For trade sample: \\n\", trade_sample[\"time_id\"].value_counts())","44c40537":"chunk_ob = book_sample[book_sample[\"time_id\"]==5]                  # chun_ob means a chunk from order book\n\n\ndef plot_series(df, kind='price', plot_all=True, plot_wap=False):\n    plt.figure(figsize=(20, 10))\n    \n    if plot_all:\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"bid_{kind}1\"], label=f\"bid_{kind}1\")\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"ask_{kind}1\"], label=f\"ask_{kind}1\")\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"bid_{kind}2\"], label=f\"bid_{kind}2\")\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"ask_{kind}2\"], label=f\"ask_{kind}2\")\n    \n    if plot_wap:\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"wap1\"], label=f\"wap1\")\n        plt.plot(chunk_ob[\"seconds_in_bucket\"], chunk_ob[f\"wap2\"], label=f\"wap2\")\n        \n    plt.xlabel(\"seconds in bucket\")\n    plt.title(f\"Buying\/selling {kind} plot over seconds_in_bucket\")\n    plt.legend()\n    pass\n\nplot_series(chunk_ob)\nplot_series(chunk_ob, kind=\"size\")","98415ba3":"# utilities for realized volatility calculation\n# ref: https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data\/\ndef calculate_wap(df, rank=\"1\"):\n    df[f\"wap{rank}\"] = (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"ask_price{rank}\"] * df[f\"bid_size{rank}\"])\/(df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n    return df\n    pass\n\ndef calculate_logReturn(df, rank=\"1\"):\n    df.loc[:, f\"log_return{rank}\"] = np.log(df[f\"wap{rank}\"]).diff()\n    return df[~df[f\"log_return{rank}\"].isnull()]\n    pass\n\ndef calculate_volatility(logReturn):\n    return np.sqrt(np.sum(logReturn**2))\n    pass\n\n# calculate wap1\ncalculate_wap(chunk_ob)\ncalculate_logReturn(chunk_ob)\n\n# calculate wap2\ncalculate_wap(chunk_ob, rank=\"2\")\ncalculate_logReturn(chunk_ob, rank=\"2\")\n\nplot_series(chunk_ob, plot_all=False, plot_wap=True)","380cd86d":"print(\"True target label for stock_id==0 and time_id==5: \", train[(train[\"stock_id\"]==0) & (train[\"time_id\"]==5)][\"target\"].values[0])\nprint(\"Rank-1 target label for stock_id==0 and time_id==5: \", calculate_volatility(chunk_ob[\"log_return1\"]))\nprint(\"Rank-2 target label for stock_id==0 and time_id==5: \", calculate_volatility(chunk_ob[\"log_return2\"]))\nprint(\"Avg of target label for stock_id==0 and time_id==5: \", (calculate_volatility(chunk_ob[\"log_return1\"]) + calculate_volatility(chunk_ob[\"log_return2\"]))\/2)","34fed7be":"chunk_trade = trade_sample[trade_sample[\"time_id\"]==5]\nchunk_trade.head()","592e2d29":"# feature utils\ndef calculate_wap(df, rank=\"1\"):\n    \"\"\"\n    Weighted Average Pricing for a stock at a given time ID is given by:\n    (bid_price1 * ask_size1 + bid_size1 * ask_price1)\/(bid_size1 + ask_size1)\n\n    It can further be extended to:\n\n        sum(bid_price_i * ask_size_i + bid_size_i * ask_price_i)\/sum(bid_size_i + ask_size_i)\n\n    :param rank: which wap to calculate\n    :param df: parquet table containing order book\n    :return:\n    \"\"\"\n    return (df[f\"bid_price{rank}\"] * df[f\"ask_size{rank}\"] + df[f\"bid_size{rank}\"] * df[f\"ask_price{rank}\"]) \/ (df[f\"bid_size{rank}\"] + df[f\"ask_size{rank}\"])\n\n\ndef calculate_logreturn(series):\n    return np.log(series).diff()\n\n\ndef calculate_rv(series):\n    return np.sqrt(np.sum(np.square(series)))\n\n\ndef count_unique(series):\n    return len(np.unique(series))\n\n\ndef get_stats_window(df, seconds_in_bucket, features_dict, add_suffix=False):\n    df_feature = df[df[\"seconds_in_bucket\"] >= seconds_in_bucket].groupby([\"time_id\"]).agg(features_dict).reset_index()\n    df_feature.columns = [\"_\".join(col) for col in df_feature.columns]\n    \n    if add_suffix:\n        df_feature = df_feature.add_suffix(\"_\" + str(seconds_in_bucket))\n        \n    return df_feature\n    pass","76569092":"# configs\nclass cfg:\n    \n    paths = {\n        # train path\n        \"train_csv\": \"..\/input\/optiver-realized-volatility-prediction\/train.csv\",\n        \"train_book\": \"..\/input\/optiver-realized-volatility-prediction\/book_train.parquet\",\n        \"train_trade\": \"..\/input\/optiver-realized-volatility-prediction\/trade_train.parquet\",\n\n        # test path\n        \"test_csv\": \"..\/input\/optiver-realized-volatility-prediction\/test.csv\",\n        \"test_book\": \"..\/input\/optiver-realized-volatility-prediction\/book_test.parquet\",\n        \"test_trade\": \"..\/input\/optiver-realized-volatility-prediction\/trade_test.parquet\",\n    }\n\n    feature_dict_book = {\n        \"wap1\": [np.sum, np.mean, np.std],\n        \"wap2\": [np.sum, np.mean, np.std],\n        \"log_return1\": [np.sum, calculate_rv, np.mean, np.std],\n        \"log_return2\": [np.sum, calculate_rv, np.mean, np.std],\n        \"wap_balance\": [np.sum, np.mean, np.std],\n        \"volume_imbalance\": [np.sum, np.mean, np.std],\n        \"total_volume\": [np.sum, np.mean, np.std],\n        \"price_spread1\": [np.sum, np.mean, np.std],\n        \"price_spread2\": [np.sum, np.mean, np.std],\n        \"bid_spread\": [np.sum, np.mean, np.std],\n        \"ask_spread\": [np.sum, np.mean, np.std],\n    }\n\n    feature_dict_trade = {\n        \"log_return\": [calculate_rv],\n        \"seconds_in_bucket\": [np.mean, count_unique],\n        \"size\": [np.sum, np.mean],\n        \"order_count\": [np.mean, np.sum],\n        \"agg_amount\": [np.mean, np.sum, np.std]\n    }\n    \n    model_params = {\n        \"xgb_bl\": {\n            \"objective\": \"reg:squarederror\",\n            \"booster\": \"gbtree\",\n            \"nthread\": -1,\n            \"eta\": 0.3,\n            \"subsample\": 0.2,\n            \"colsample_bytree\": 0.33,\n            \"sampling_method\": \"uniform\",\n#             \"tree_method\": \"gpu_hist\"  # turn it on for GPU\n        }\n    }","82a03a2a":"# order book features\ndef get_book_features(file_path):\n    book_df = pd.read_parquet(file_path)\n\n    # calculate wap\n    book_df['wap1'] = calculate_wap(book_df, rank=\"1\")\n    book_df['wap2'] = calculate_wap(book_df, rank=\"2\")\n\n    # calculate log return\n    book_df[\"log_return1\"] = book_df.groupby([\"time_id\"])[\"wap1\"].apply(calculate_logreturn)\n    book_df[\"log_return2\"] = book_df.groupby([\"time_id\"])[\"wap2\"].apply(calculate_logreturn)\n\n    # calculate balance\n    book_df[\"wap_balance\"] = abs(book_df[\"wap1\"] - book_df[\"wap2\"])\n    book_df[\"volume_imbalance\"] = abs(\n        (book_df[\"ask_size1\"] + book_df[\"ask_size2\"]) - (book_df[\"bid_size1\"] + book_df[\"bid_size2\"]))\n    book_df[\"total_volume\"] = book_df[\"ask_size1\"] + book_df[\"ask_size2\"] + book_df[\"bid_size1\"] + book_df[\n        \"bid_size2\"]\n\n    # calculate spread\n    book_df[\"price_spread1\"] = (book_df[\"ask_price1\"] - book_df[\"bid_price1\"]) \/ (\n            (book_df[\"ask_price1\"] + book_df[\"bid_price1\"]) \/ 2)\n    book_df[\"price_spread2\"] = (book_df[\"ask_price2\"] - book_df[\"bid_price2\"]) \/ (\n            (book_df[\"ask_price2\"] + book_df[\"bid_price2\"]) \/ 2)\n\n    book_df[\"bid_spread\"] = book_df[\"bid_price1\"] - book_df[\"bid_price2\"]\n    book_df[\"ask_spread\"] = book_df[\"ask_price1\"] - book_df[\"ask_price2\"]\n\n    book_df_merged = get_stats_window(book_df, seconds_in_bucket=0, features_dict=cfg.feature_dict_book)\n\n    book_df_450 = get_stats_window(book_df, seconds_in_bucket=450, features_dict=cfg.feature_dict_book, add_suffix=True)\n    book_df_300 = get_stats_window(book_df, seconds_in_bucket=300, features_dict=cfg.feature_dict_book, add_suffix=True)\n    book_df_150 = get_stats_window(book_df, seconds_in_bucket=150, features_dict=cfg.feature_dict_book, add_suffix=True)\n\n    # merge stats\n    book_df_merged = book_df_merged.merge(book_df_450, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__450\")\n    book_df_merged = book_df_merged.merge(book_df_300, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__300\")\n    book_df_merged = book_df_merged.merge(book_df_150, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__150\")\n\n\n    book_df_merged.drop(columns=[\"time_id__450\", \"time_id__300\", \"time_id__150\"], inplace=True)\n\n    book_df_merged[\"row_id\"] = book_df_merged[\"time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n    book_df_merged.drop([\"time_id_\"], axis=1, inplace=True)\n\n    return book_df_merged\n\n# trade features\ndef get_trade_features(file_path):\n    trade_df = pd.read_parquet(file_path)\n    \n    trade_df[\"log_return\"] = trade_df.groupby([\"time_id\"])[\"price\"].apply(calculate_logreturn)\n    trade_df[\"agg_amount\"] = trade_df[\"price\"] * trade_df[\"size\"]\/trade_df[\"order_count\"]\n\n    trade_df_merged = get_stats_window(trade_df, seconds_in_bucket=0, features_dict=cfg.feature_dict_trade)\n\n    trade_df_450 = get_stats_window(trade_df, seconds_in_bucket=450, features_dict=cfg.feature_dict_trade, add_suffix=True)\n    trade_df_300 = get_stats_window(trade_df, seconds_in_bucket=300, features_dict=cfg.feature_dict_trade, add_suffix=True)\n    trade_df_150 = get_stats_window(trade_df, seconds_in_bucket=150, features_dict=cfg.feature_dict_trade, add_suffix=True)\n\n    # merge stats\n    trade_df_merged = trade_df_merged.merge(trade_df_450, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__450\")\n    trade_df_merged = trade_df_merged.merge(trade_df_300, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__300\")\n    trade_df_merged = trade_df_merged.merge(trade_df_150, how=\"left\", left_on=\"time_id_\", right_on=\"time_id__150\")\n    \n    trade_df_merged.drop(columns=[\"time_id__450\", \"time_id__300\", \"time_id__150\"], inplace=True)\n    \n    trade_df_merged = trade_df_merged.add_prefix(\"trade_\")\n\n    trade_df_merged[\"row_id\"] = trade_df_merged[\"trade_time_id_\"].apply(lambda x: f\"{file_path.split('=')[1]}-{x}\")\n    trade_df_merged.drop([\"trade_time_id_\"], axis=1, inplace=True)\n\n    return trade_df_merged","9271efb6":"class GetData:\n    def __init__(self, df, book_path, trade_path):\n        self.df = df.copy(deep=True)\n        self.order_book_path = book_path\n        self.trade_path = trade_path\n\n        self._get_rowid()\n\n    def _get_rowid(self):\n        self.df[\"row_id\"] = self.df[\"stock_id\"].astype(str) + \"-\" + self.df[\"time_id\"].astype(str)\n\n    def get_time_stock(self):\n        vol_cols = ['log_return1_calculate_rv', 'log_return2_calculate_rv',\n                    'log_return1_calculate_rv_450', 'log_return2_calculate_rv_450',\n                    'log_return1_calculate_rv_300', 'log_return2_calculate_rv_300',\n                    'log_return1_calculate_rv_150', 'log_return2_calculate_rv_150',\n                    'trade_log_return_calculate_rv', 'trade_log_return_calculate_rv_450',\n                    'trade_log_return_calculate_rv_300', 'trade_log_return_calculate_rv_150']\n\n        df_stock_id = self.df.groupby(['stock_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n        df_stock_id.columns = ['_'.join(col) for col in df_stock_id.columns]\n        df_stock_id = df_stock_id.add_suffix('_' + 'stock')\n\n        df_time_id = self.df.groupby(['time_id'])[vol_cols].agg(['mean', 'std', 'max', 'min']).reset_index()\n        df_time_id.columns = ['_'.join(col) for col in df_time_id.columns]\n        df_time_id = df_time_id.add_suffix('_' + 'time')\n\n        # Merge with original dataframe\n        self.df = self.df.merge(df_stock_id, how='left', left_on=['stock_id'], right_on=['stock_id__stock'])\n        self.df = self.df.merge(df_time_id, how='left', left_on=['time_id'], right_on=['time_id__time'])\n        self.df.drop(['stock_id__stock', 'time_id__time'], axis=1, inplace=True)\n        return self.df\n\n    def process_features(self, list_stock_ids):\n        def parallel_helper(stock_id):\n            book_sample_path = os.path.join(self.order_book_path, f\"stock_id={stock_id}\")\n            trade_sample_path = os.path.join(self.trade_path, f\"stock_id={stock_id}\")\n\n            return pd.merge(get_book_features(book_sample_path), get_trade_features(trade_sample_path),\n                            on=\"row_id\",\n                            how=\"left\")\n\n        df = Parallel(n_jobs=-1, verbose=1)(delayed(parallel_helper)(stock_id) for stock_id in list_stock_ids)\n        df = pd.concat(df, ignore_index=True)\n\n        return df\n\n    def get_features(self):\n        features_df = self.process_features(self.df[\"stock_id\"].unique())\n        self.df = self.df.merge(features_df, on=[\"row_id\"], how=\"left\")\n\n        return self.get_time_stock()\n        pass","0897d828":"train_data = GetData(train, cfg.paths[\"train_book\"], cfg.paths[\"train_trade\"])\ntrain_df = train_data.get_features()\ntrain_df.head()","986966c5":"def rmspe(y_true, y_pred):\n    return np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true)))\n\ndef feval_rmspe(y_pred, xgb_dtrain):\n    y_true = xgb_dtrain.get_label()\n    return \"RMSPE\", rmspe(y_true, y_pred)\n\nclass TrainFer:\n    def __init__(self, params_dict, n_splits, model_path, random_state=2021):\n        self.params = params_dict\n        self.n_splits = n_splits\n        self.random_state = random_state\n        self.model_path = model_path\n        \n        if not os.path.isdir(model_path):\n            os.makedirs(model_path)\n    \n    def train(self, X, y):\n        oof_predictions = np.zeros(X.shape[0])\n        kfold = KFold(n_splits=self.n_splits, random_state=self.random_state, shuffle=True)\n        \n        for fold, (train_idx, val_idx) in enumerate(kfold.split(X)):\n            print(f\"\\nFold - {fold}\\n\")\n\n            x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n            x_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n            \n            x_train[\"stock_id\"] = x_train[\"stock_id\"].astype(int)\n            x_val[\"stock_id\"] = x_val[\"stock_id\"].astype(int)\n\n            dtrain = xgb.DMatrix(x_train, label=y_train, weight=1\/np.square(y_train), enable_categorical=True)\n            dval = xgb.DMatrix(x_val, label=y_val, weight=1\/np.square(y_val), enable_categorical=True)\n\n            model = xgb.train(self.params,\n                              dtrain=dtrain,\n                              num_boost_round=10000,\n                              evals=[(dtrain, \"dtrain\"), (dval, \"dval\")],\n                              verbose_eval=50,\n                              feval=feval_rmspe,\n                              early_stopping_rounds=200)\n            \n            pickle.dump(model, open(os.path.join(self.model_path, f\"xgb_bl_{fold}.pkl\"), \"wb\"))\n            oof_predictions[val_idx] = model.predict(dval)\n            \n        rmspe_score = rmspe(y, oof_predictions)\n        print(f\"OOF RMSPE: {rmspe_score}\")","480e1d64":"trainer = TrainFer(cfg.model_params[\"xgb_bl\"], n_splits=5, model_path=\".\/xgbBaseline\/\")\ntrainer.train(train_df.drop(columns=[\"row_id\", \"target\", \"time_id\"]), train_df[\"target\"])","49ebe0ca":"We observed that, we have around ~3830 instances for a given `(stock_id, time_id)` pair, and for each pair we can have a seuqential data\/array of WAP. The length may vary from 70-550 (it's for stock 0, results can be different for different stock ids). If we are to treat these as a sequential data, the length will be a matter of concern. Well, will dive into that later. ","8ac457be":"# Introduction\n\nBefore diving into the details of the problem, let's understand what is the goal?\n\n```\n1. We need to predict the short-term volatility (for 10 minutes range) for hundreds of stocks across different sectors. \n2. We have hundreds of millions of highly granular(i.e. detailed) data. \n3. For the first three months, the forecasting model will be over given dataset for 10 minute periods. For next three months, it will predicted on real time data. (Just like Jane street Market Prediction competition).\n4. Evaluation Metrics: RMSPE (Root Mean Square Percentage Error).\n5. This is a code competition and expect to predict 150K target values in RUN TIME.\n6. Public and Private test data will have zero over lap. Hence, just build a generalized model\/models. Forget about the leaderboard during training phase :)\n```","9e57c76d":"# Data Attributes\n\n- Train\n    - stock_id: ID code for the stock. (as categorical in parquet files, can be converted into int8 dtype)\n    - time_id : ID code for time bucket. Not necessarily sequential. Consistent across all stocks.\n    - target  : The realized volatility computed over a `10 minute` window. For each stock-id-time_id pair, we have a target.\n    \n\n- Test\n    - stock_id: same as above\n    - time_id : same as above\n    - row_id  : Unique identifier for the submission row. One row for each stock-time pair.\n\n\n- Sample Submission\n    - row_id  : same as above\n    - target  : same as above","1a1f68c5":"# Meta Data Overview\n\nLet's investigate parquet files for a single stock for Order book and trades.","1ec3594f":"Speakng of `time_id`, let's see the occurances we have for a given time id per stock.","1a7314c0":"Alright, we got a basic idea of what and how of the problem statement. Let's get on to it. There are actually pretty good baselines, so instead of describing more feature-centric approaches, I will build a XGB pipeline here. Let's go.","c66a794b":"# Feature Engineering and Dataset","548ad7f8":"Before diving into details, let's look into a single data point, i.e. one (stock_id, time_id) pair. Let's take `time_id=5` for `stock_id=0`.","3695ca08":"Okay, now let's inspect trade data for the same `time_id`.","2f5413a9":"Well, `ask_price2` and `bid_price1` seem to leading thier counterparts. Read that again.\n\nFor order size, nothing is interpretable from this graph for me. We will look into that later. As of now, the takeaway can be: there is no point in feeding all the buying\/selling prices data for a (stock_id, time_id) pair. Cause it's redudant. What could be the relation with weighted average pricing? Let's see.","ab2e5980":"# Domain Introduction and Resources\n\n- Volatility: https:\/\/www.optiver.com\/insights\/guides\/options-volatility\/\n- How Price is related to Volatility?: https:\/\/www.optiver.com\/insights\/guides\/options-pricing\/\n- Organizer's Notebook on Introduction and Domain Knowledge: https:\/\/www.kaggle.com\/jiashenliu\/introduction-to-financial-concepts-and-data","7911f032":"# Modeling","5e8ed6a6":"## Notes:\n\n- Our data points are (X_i, Y_i), where X_i is the feature vector for every (stock_id, time_id) pair, and Y_i is the corresponding target.\n- Y_i can be calculated from squared aggregation of log of WAP(weighted avg price), (Ref: Organizer's notebook.)\n- If we create a cleaned, and one merged dataset, we can have below features (as of now, pure naive features) for every (X, Y):\n    - Statistical data about `seconds_in_bucket`.\n    - `wap` calculated from most and second most competitive buy\/sell.\n    - `log(wap)`, and probably the calculated squared aggregrate of log(wap) from the log return. Don't know whether we should use this as a feature in our model training, as it is most likely to have a high, very high correlation with the `target` value. Let's see, no sepculations now.\n    - Aggregrate statistical values, and some feature engineering between pirces and sizes.","4ebbe68c":"So, for each stock ID, we have 3830\/3829 time IDs, except stock 80, and stock 38, having 3820 and 3815 time IDs respectively. And we have total `112` stocks. Meta data for each stock are available in corresponding `book` and `trade` folders. We will look into each stock later, let's have a look on the time id, target distributions w.r.t. stock id.","8cb7f52f":"As mentioned, yeah the time IDs are consistent throughout the stocks. P.S: There are 3830 time IDs. Let's pick a particular stock and have a brief overview of what we have.","122038df":"So, what we need to find here is, how price is influenced by order_count and size. And the dependency of seconds_in_bucket. Keeping this simple, and considering the statistical features only, we can proceed for modeling.","84642e10":"### Work In Progress","b7e11b9b":"# Dataset Overview\n\n- Given `Order Book` snapshots and `Executed Trades` in parquet format.\n- What are Order books and Executed Trades, refer to the notebook by organizers to get domain expertise."}}