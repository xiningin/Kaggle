{"cell_type":{"12f8d3a8":"code","a0c6466d":"code","b45e9105":"code","da3afc77":"code","377c2d6a":"code","5778c68a":"code","78117d51":"code","81abd7ec":"code","5742c3d6":"code","debf3879":"code","a365196f":"code","503dd339":"code","db26b8ed":"code","f2eadef5":"code","af95a8c8":"code","38875697":"code","e42da57c":"code","22d99cef":"code","bdcdd9ad":"code","0dae9d68":"code","f37176bb":"code","97a1c740":"code","4fcd0864":"code","53ab6199":"code","fa7993ad":"code","8768844c":"code","4ea804db":"code","97c727de":"code","4932b59f":"code","55bf4147":"code","13a0ca61":"code","d9f47eed":"code","d4dcbebb":"code","740719ff":"code","df14aed4":"code","f8c35c10":"code","9a4b12e0":"code","1d227a7f":"code","f32e2b34":"code","0084e33e":"code","398a1ad1":"code","ad6da326":"code","ba8770f3":"code","c24ebcf9":"markdown","2b087121":"markdown","479e24d5":"markdown"},"source":{"12f8d3a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a0c6466d":"!pip install python-vivid","b45e9105":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom vivid.estimators.base import MetaBlock\nfrom vivid.estimators.boosting.mixins import BoostingEarlyStoppingMixin\n\nfrom catboost import CatBoostRegressor","da3afc77":"from vivid.features.base import BinningCountBlock\nfrom vivid.features.base import CountEncodingBlock\nfrom vivid.features.base import FilterBlock\n\nfrom vivid.estimators.boosting import XGBRegressorBlock\nfrom vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.estimators.boosting.block import create_boosting_seed_blocks\n\nfrom vivid.estimators.linear import TunedRidgeBlock\nfrom vivid.estimators.svm import SVRBlock\nfrom vivid.estimators.ensumble import RFRegressorBlock\nfrom vivid.estimators.base import EnsembleBlock, BaseBlock","377c2d6a":"train_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/test.csv')\nsample_submission_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2021\/sample_submission.csv')\n\nfeature_columns = [\n 'cont1',\n 'cont2',\n 'cont3',\n 'cont4',\n 'cont5',\n 'cont6',\n 'cont7',\n 'cont8',\n 'cont9',\n 'cont10',\n 'cont11',\n 'cont12',\n 'cont13',\n 'cont14',    \n]\n\ny = train_df['target'].values","5778c68a":"!pip install ptitprince","78117d51":"!pip freeze | grep seaborn","81abd7ec":"!pip install -U seaborn","5742c3d6":"from ptitprince import RainCloud","debf3879":"RainCloud(y, orient='h')","a365196f":"sns.distplot(y)","503dd339":"np.percentile(y, [.5, 1, 10, 20])","db26b8ed":"sns.distplot(10 ** y)","f2eadef5":"from sklearn.decomposition import PCA","af95a8c8":"model = PCA(n_components=2)\nmodel.fit(train_df[feature_columns])","38875697":"z = model.transform(train_df[feature_columns])","e42da57c":"fig, ax = plt.subplots(figsize=(10, 10))\nax.scatter(*z.T, c=y, alpha=.1)","22d99cef":"from sklearn.mixture import GaussianMixture","bdcdd9ad":"clf = GaussianMixture(n_components=3)\nx = train_df[feature_columns].values\nclf.fit(x)\nz = clf.predict_proba(x)","0dae9d68":"sns.pairplot(pd.DataFrame(np.vstack([z.T, y]).T).sample(400))","f37176bb":"from vivid.core import BaseBlock","97a1c740":"class PCABlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='pca_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = PCA(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.transform(source_df[self.columns])\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('PCA_')\n    \n\nclass GaussianMixtureBlock(BaseBlock):\n    def __init__(self, n_components=3, columns=None, *args, **kwrgs):\n        self.n_components = n_components\n        \n        if columns is None: columns = feature_columns\n        self.columns = columns\n        super().__init__(name='GMM_n={}'.format(n_components), *args, **kwrgs)\n    \n    def fit(self, source_df, y, experiment=None) -> pd.DataFrame:\n        clf = GaussianMixture(n_components=self.n_components)\n        clf.fit(source_df[self.columns].values)\n        self.clf_ = clf\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        z = self.clf_.predict_proba(source_df[self.columns])\n        z = np.clip(z, 1e-6,1 - 1e-6)\n        out_df = pd.DataFrame(z)\n        return out_df.add_prefix('GMM_')","4fcd0864":"PCABlock().fit(train_df, y)","53ab6199":"GaussianMixtureBlock().fit(train_df, y)","fa7993ad":"from vivid.estimators.boosting import LGBMRegressorBlock\nfrom vivid.runner import create_runner","8768844c":"class CatRegressorBlock(BoostingEarlyStoppingMixin, MetaBlock):\n    \"\"\"cat-boost regressor blocks. \"\"\"\n    \n    # use cat-boost regressor \n    model_class = CatBoostRegressor\n    \n    fit_verbose = 100\n    early_stopping_rounds = 200\n    \n    # pass to __init__ \n    initial_params = {\n        'learning_rate': .05,\n        'verbose': 100,\n        'num_boost_round': 20000\n    }\n    \n    def get_fit_params_on_each_fold(self, *args, **kwrgs):\n        \"\"\"create parameters pass to `model.fit` method.\n        > see: https:\/\/github.com\/nyk510\/vivid\/blob\/master\/vivid\/estimators\/boosting\/mixins.py#L15\n        \"\"\"\n        params = super().get_fit_params_on_each_fold(*args, **kwrgs)\n        params['verbose'] = self.fit_verbose\n        \n        # delete keys for lightGBM \/ XGBoost.\n        remove_keys = [\n            'eval_metric',\n            'callbacks'\n        ]\n        \n        for k in remove_keys:\n            if k in params:\n                del params[k]\n        return params","4ea804db":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nrunner = create_runner(blocks=[\n    CatRegressorBlock(name='cat', parent=feature_blocks)\n])","97c727de":"y_cutted = np.clip(y, 5, np.inf)","4932b59f":"oof_results = runner.fit(train_df[feature_columns], y=y_cutted)","55bf4147":"ls -lat","13a0ca61":"class FillnaBlock(BaseBlock):\n    def fit(self, source_df, y, experiment) -> pd.DataFrame:\n        self.fill_values_ = source_df.dropna().median()\n        return self.transform(source_df)\n    \n    def transform(self, source_df):\n        return source_df.fillna(self.fill_values_)","d9f47eed":"feature_blocks = [\n    BinningCountBlock(name='BINS', column=feature_columns),\n    CountEncodingBlock(name='CE', column=feature_columns),\n    FilterBlock(name='F', column=feature_columns),\n    PCABlock(n_components=3),\n    GaussianMixtureBlock(n_components=3)\n]\n\n\nfilled_feature_block = FillnaBlock(name='FNA', parent=feature_blocks)","d4dcbebb":"single_models = [\n    create_boosting_seed_blocks(feature_class=XGBRegressorBlock, \n                                prefix='xgb_', \n                                parent=feature_blocks),\n    create_boosting_seed_blocks(feature_class=LGBMRegressorBlock, \n                                prefix='lgb_', \n                                parent=feature_blocks),\n    CatRegressorBlock(name='cat', parent=feature_blocks),\n    RFRegressorBlock(name='rf', parent=filled_feature_block),\n    TunedRidgeBlock(name='ridge', \n                    add_init_param={ 'target_scaling': 'standard' },\n                    n_trials=30, \n                    parent=filled_feature_block)\n]\n\nstacking_models = [\n    EnsembleBlock(prefix='ens', parent=single_models),\n    TunedRidgeBlock(name='stacking_ridge', n_trials=30, parent=single_models, \n                    add_init_param={ 'target_scaling': 'standard' }),\n    LGBMRegressorBlock(name='stacked_lgb', parent=[*single_models, *feature_blocks]),\n    CatRegressorBlock(name='stacked_cat', parent=[*single_models, *feature_blocks]),\n    XGBRegressorBlock(name='stacked_xgb', parent=[*single_models, *feature_blocks])\n]\n\ntwo_stage_stacking_models = [\n    TunedRidgeBlock(name='stage-2_ridge', n_trials=30, parent=stacking_models, \n                    add_init_param={ 'target_scaling': 'standard' },)\n]","740719ff":"from vivid.backends.experiments import LocalExperimentBackend\n\nrunner = create_runner(two_stage_stacking_models, \n                       experiment=LocalExperimentBackend(to='\/kaggle\/working\/'))","df14aed4":"# train models\noof_results = runner.fit(train_df, y=y_cutted)","f8c35c10":"# predict\ntest_results = runner.predict(test_df)","9a4b12e0":"# create out-of-fold overview\noof_df = pd.DataFrame()\n\nfor result in oof_results:\n    oof_df[result.block.name] = result.out_df.values[:, 0]","1d227a7f":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.heatmap(oof_df.corr(), cmap='Blues', annot=True, fmt='.2f', ax=ax)\nax.set_title('Out of Fold Correlation')","f32e2b34":"fig, ax = plt.subplots(figsize=(10, 6))\n\nfor result in oof_results:\n    sns.distplot(result.out_df.values[:, 0], ax=ax, label=str(result.block.name))\n\nax.legend()\nfig.tight_layout()","0084e33e":"from vivid.metrics import regression_metrics\n\nscore_df = pd.DataFrame()\n\nfor name, pred in oof_df.T.iterrows():\n    score_i = regression_metrics(y, pred)\n    score_df = score_df.append(pd.Series(score_i, name=name))","398a1ad1":"score_df.sort_values('rmse')","ad6da326":"sample_submission_df","ba8770f3":"for result in test_results:\n    out_df = result.out_df\n    \n    sub_df = sample_submission_df.copy()\n    sub_df['target'] = result.out_df.values[:, 0]\n    to = f'\/kaggle\/working\/{str(result.block.name)}_submission.csv'\n    print('save to {}'.format(to))\n    sub_df.to_csv(to, index=False)","c24ebcf9":"### Visualize Models\n\n* Model Output Correation\n* OOf Distribution\n* sort by RMSE","2b087121":"### Modeling\n\ncreate more complex models","479e24d5":"* cont13 \/ 3 \/ 4 is very importance feature. \n* ground truth vs out-of-fold predict is not similar ;(\n    * y is long tail dist but the objective and metric is `RMSE`, it assumes the noise is normal distribution."}}