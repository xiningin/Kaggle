{"cell_type":{"8f0b0f00":"code","8c1c008b":"code","cc0bc661":"code","4039d3fc":"code","f0c6c8e6":"code","b6692dda":"code","9a1ac393":"code","0e03e33a":"code","81bc1454":"code","bc87686c":"code","00d6b827":"code","4de0bb73":"code","2f8c8311":"code","54da5275":"code","65d7fbf2":"code","ebc1e837":"code","2e3275df":"code","fdfe2841":"code","ac087fd6":"code","7b15d54c":"code","23afd095":"code","6f22a878":"code","3c1494dd":"code","bdc009a9":"code","110af2f1":"code","73c50f57":"code","a1a548bd":"code","18a07881":"code","da5ee83c":"code","458c780d":"code","c7365d69":"code","986eb0b6":"code","26b17ffd":"code","8a72c8a4":"code","469e7c3d":"code","13169b78":"code","a7aca155":"code","606913e5":"code","a1980411":"code","7ba87e44":"code","b828a84b":"code","d60f0c11":"code","2939c026":"code","dafaf1c1":"code","c819f2bc":"code","8c39377c":"code","181b1107":"code","79e50d89":"code","2b3c5091":"code","ef862449":"code","ecf4240c":"code","fe5c33c2":"code","e4d13722":"code","2d648dd6":"code","fbce2544":"code","eefad498":"code","67019d68":"code","272ce04a":"code","abdae6c2":"code","5951935f":"markdown","80b00503":"markdown","24fdcb62":"markdown","11d7d616":"markdown","797f7ee6":"markdown","622bbca6":"markdown"},"source":{"8f0b0f00":"# imports needed\n%matplotlib inline\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier \nfrom urllib.request import urlopen \n\nplt.style.use('ggplot')\npd.set_option('display.max_columns', 500) \n\n# Get Wisconsin Breast Cancer Data\nbreast_cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n\nnames = ['id', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst'] \n\ndx = ['Benign', 'Malignant']\nbreast_cancer.head()","8c1c008b":"# Setting 'id_number' as our index\nbreast_cancer.set_index(['id'], inplace = True) \n\n# Converted to binary to help later on with models and plots\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'M':1, 'B':0})\n\n# check mapping worked \nmapping = breast_cancer[['diagnosis']]\nmapping","cc0bc661":"# check check number of null values in each field\nbreast_cancer.apply(lambda x: x.isnull().sum())\n\n# drop the field we dont need\nbreast_cancer = breast_cancer.drop(columns=['Unnamed: 32'])\n\n# double check the field is gone\nbreast_cancer.apply(lambda x: x.isnull().sum())","4039d3fc":"# some infomation about the data, could also do this with breast_cancer.info()\nprint(\"Shape of our dataframe:\\n\", \n     breast_cancer.shape)\nprint(\"data types of our columns:\\n\",\n     breast_cancer.dtypes)","f0c6c8e6":"# check if dataset is suffers imbalance between classes Benign = 0 and Malignant = 1\ns = pd.value_counts(breast_cancer.diagnosis)\n\n# for class 0\nnum_of_benign = s[0]\n# for class 1\nnum_of_malignant = s[1]\n\ntotal_cases = len(breast_cancer)\n\n# calculate percentages of data that resides in both classes\npercent_b = num_of_benign \/ total_cases\npercent_m = num_of_malignant \/ total_cases\n\nprint(\"Distribution between Benign and Malignant\\nPercent Benign: {0:.3f} \\nPercent Malignant: {1:.3f} \".format(percent_b, percent_m))","b6692dda":"# create dataset\nfeature_space = breast_cancer.iloc[:, breast_cancer.columns != 'diagnosis']\nfeature_class = breast_cancer.iloc[:, breast_cancer.columns == 'diagnosis']\n\n# train_test_split\ntraining_set, test_set, class_set, test_class_set = train_test_split(feature_space,\n                                                                    feature_class,\n                                                                    test_size = 0.20, \n                                                                    random_state = 42)\n# Cleaning test sets to avoid future warning messages\nclass_set = class_set.values.ravel() \ntest_class_set = test_class_set.values.ravel()","9a1ac393":"# instantiate classifier \nrf_classifier = RandomForestClassifier(random_state=42, n_estimators=10)","0e03e33a":"np.random.seed(42)\nstart = time.time()\n\n# give to GridSearchCV\nparam_dist = {'max_depth': [2, 3, 4],\n              'bootstrap': [True, False],\n              'max_features': ['auto', 'sqrt', 'log2', None],\n              'criterion': ['gini', 'entropy']}\n\n# set up the GridSearch\ncv_rf = GridSearchCV(rf_classifier, cv = 5,\n                     param_grid=param_dist, \n                     n_jobs = 3)\n\n# fit the GridSearch\ncv_rf.fit(training_set, class_set)\nprint('Best Parameters using grid search: \\n', cv_rf.best_params_)\n\n\nend = time.time()\nprint('Time taken in grid search: {0: .2f}'.format(end - start))","81bc1454":"# Set best parameters given by grid search \nrf_classifier.set_params(criterion = 'gini',\n                  max_features = 'log2', \n                  max_depth = 3, \n                  )","bc87686c":"# warm_start = True reuse the solution of the previous call to fit \n# and add more estimators to the ensemble, otherwise, just fit a whole new forest.\n\nrf_classifier.set_params(warm_start=True, \n                  oob_score=True)\n\n# found this from sci-kit learn https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_ensemble_oob.html\nmin_estimators = 15\nmax_estimators = 500\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1):\n    rf_classifier.set_params(n_estimators=i)\n    rf_classifier.fit(training_set, class_set)\n\n    oob_error = 1 - rf_classifier.oob_score_\n    error_rate[i] = oob_error","00d6b827":"# Convert dictionary to a pandas series for easy plotting \noob_series = pd.Series(error_rate)","4de0bb73":"# Plotting the OOB_scores line graph: oob_error vs. n_estimators\nfig, ax = plt.subplots(figsize=(10, 10))\n\nax.set_facecolor('#fafafa')\n\noob_series.plot(kind='line',\n                color = 'blue')\nplt.axhline(0.055, \n            color='#875FDB',\n           linestyle='--')\nplt.axhline(0.05, \n            color='#875FDB',\n           linestyle='--')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')","2f8c8311":"# set the estimators so we can proceed to fitting the rf_classifier turning warm start and oob_score to False\nrf_classifier.set_params(n_estimators=420,\n                  bootstrap = True,\n                  warm_start=False, \n                  oob_score=False)","54da5275":"# fit the Random forest to the training data\nrf_classifier.fit(training_set, class_set)","65d7fbf2":"# returns a dict with value pairs {importance: indices} for printing\ndef variable_importance(fit):\n    try:\n        # Checks whether first parameter is a model\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Checks whether model has been trained\n        if not vars(fit)[\"estimators_\"]:\n            return print(\"Model does not appear to be trained.\")\n    except KeyError:\n        print(\"Model entered does not contain 'estimators_' attribute.\")\n\n    importances = fit.feature_importances_\n    # sort from most import to least\n    indices = np.argsort(importances)[::-1]\n    return {'importance': importances,\n            'index': indices}","ebc1e837":"# get variable importance and their indexes\nrf_var_imp = variable_importance(rf_classifier)\n\nrf_importances = rf_var_imp['importance']\n\nrf_indices = rf_var_imp['index']","2e3275df":"# unpacks and prints values in importance dict according to the index \ndef print_var_importance(importance, indices, name_index):\n    print(\"Feature ranking:\")\n    # iterate thru variable indices\n    for f in range(0, indices.shape[0]):\n        i = f\n        # prints the name of the feature and its importance metric \n        print(\"{0}. The feature '{1}' has a Mean Decrease in Impurity of {2:.5f}\"\n              .format(f + 1, names_index[indices[i]], importance[indices[f]]))","fdfe2841":"# get the classes uses to train this model\nnames_index = names[2:]\n\n# print out classes by importance in decending order\nprint_var_importance(rf_importances, rf_indices, names_index)","ac087fd6":"# Make a horizontal bar chart to visualize feature importantance\n\ndef variable_importance_plot(importance, indices, name_index):\n    index = np.arange(len(names_index))\n\n    importance_desc = sorted(importance)\n\n    feature_space = []\n\n    for i in range(indices.shape[0] - 1, -1, -1):\n        feature_space.append(names_index[indices[i]])\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n\n    plt.title('Feature importances for Random Forest Model\\\\nBreast Cancer (Diagnostic)')\n    \n    plt.barh(index,\n              importance_desc,\n              align=\"center\",\n              color = '#FFB6C1')\n    plt.yticks(index,\n                feature_space)\n\n    plt.ylim(-1, 30)\n    plt.xlim(0, max(importance_desc) + 0.01)\n    plt.xlabel('Mean Decrease in Impurity')\n    plt.ylabel('Feature')\n\n    plt.show()\n    plt.close()","7b15d54c":"variable_importance_plot(rf_importances, rf_indices, names_index)","23afd095":"# Perform Cross_validation to see how robust our model is \nimport time\n\ndef cross_val_metrics(fit, training_set, class_set, estimator, print_results = True):\n    start = time.time()\n    \"\"\"\n    Returns Mean Accurancy with standard_dev of model over Kfolds Validation\n    ----------\n    scores.mean(): Float representing cross validation score\n    scores.std() \/ 2: Float representing the standard error (derived\n                from cross validation score's standard deviation)\n    \"\"\"\n    my_estimators = {\n    'rf': 'estimators_',\n    'nn': 'out_activation_',\n    'knn': '_fit_method'\n    }\n    try:\n        # Checks whether first parameter is a model\n        if not hasattr(fit, 'fit'):\n            return print(\"'{0}' is not an instantiated model from scikit-learn\".format(fit)) \n\n        # Checks whether the model has been trained\n        if not vars(fit)[my_estimators[estimator]]:\n            return print(\"Model does not appear to be trained.\")\n\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n              \\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    # create KFolds validation\n    n = KFold(n_splits=10)\n\n    # record score for each split\n    scores = cross_val_score(fit, \n                         training_set, \n                         class_set, \n                         cv = n)\n    end = time.time() \n    # print how much time the Kfolds took\n    print(\"Time elapsed to do Cross Validation: {0:.2f} seconds.\".format(end-start))\n    if print_results:\n        for i in range(0, len(scores)):\n            # print out the scores for each validation split\n            print(\"Cross validation run {0}: {1: 0.3f}\".format(i, scores[i]))\n\n        print(\"Accuracy: {0: 0.3f} (+\/- {1: 0.3f})\".format(scores.mean(), scores.std() \/ 2))     \n    else:\n        return scores.mean(), scores.std() \/ 2","6f22a878":"# call cross_val_metrics to see how our model did\ncross_val_metrics(rf_classifier, \n                  training_set, \n                  class_set, \n                  'rf',\n                  print_results = True)","3c1494dd":"# make a prediction on test set now that the model has been tuned and validated\nrf_predictions = rf_classifier.predict(test_set)","bdc009a9":"\ndef create_conf_mat(test_class_set, predictions):\n    \"\"\"Function returns confusion matrix comparing two arrays\"\"\"\n    if (len(test_class_set.shape) != len(predictions.shape) == 1):\n        return print('Arrays entered are not 1-D.\\nPlease enter the correctly sized sets.')\n    elif (test_class_set.shape != predictions.shape):\n        return print('Number of values inside the Arrays are not equal to each other.\\nPlease make sure the array has the same number of instances.')\n    else:\n        # Set Metrics; Compute a simple cross tabulation of two (or more) factors. \n        # By default computes a frequency table of the factors unless an array of values and an aggregation function are passed.\n        test_crosstb_comp = pd.crosstab(index = test_class_set,\n                                        columns = predictions)\n        # Changed for Future deprecation of as_matrix\n        test_crosstb = test_crosstb_comp.values\n        return test_crosstb","110af2f1":"conf_mat = create_conf_mat(test_class_set, rf_predictions)\n\n# use seaborn heatmap\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","73c50f57":"# use built in score function to get accurancy of this model against the test set\nrf_accuracy = rf_classifier.score(test_set, test_class_set)\n\nprint(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n      .format(rf_accuracy))","a1a548bd":"# Here we calculate the test error rate!\nrf_test_error_rate = 1 - rf_accuracy\nprint(\"The test error rate for our model is:\\n {0: .4f}\"\\\n      .format(rf_test_error_rate))","18a07881":"# predict_proba returns two arrays that represent the predicted_prob of negative class and positive class respectively.\n\n# I just want to take the positive classes in this instance\npredictions_prob = rf_classifier.predict_proba(test_set)[:, 1]\n\n# use roc_curve to produce \nfpr2, tpr2, _ = roc_curve(test_class_set,\n                          predictions_prob,\n                          pos_label = 1)\n\nauc_rf = auc(fpr2, tpr2)\n\nprint(auc_rf)","da5ee83c":"def plot_roc_curve(fpr, tpr, auc, estimator, xlim=None, ylim=None):\n    \"\"\"\n    Purpose\n    ----------\n    Function creates ROC Curve for respective model given selected parameters.\n    Optional x and y limits to zoom into graph\n\n    Parameters\n    ----------\n    * fpr: Array returned from sklearn.metrics.roc_curve for increasing\n            false positive rates\n    * tpr: Array returned from sklearn.metrics.roc_curve for increasing\n            true positive rates\n    * auc: Float returned from sklearn.metrics.auc (Area under Curve)\n    * estimator: String represenation of appropriate model, can only contain the\n    following: ['knn', 'rf', 'nn']\n    * xlim: Set upper and lower x-limits\n    * ylim: Set upper and lower y-limits\n    \"\"\"\n    my_estimators = {'knn': ['Kth Nearest Neighbor', 'deeppink'],\n              'rf': ['Random Forest', 'red'],\n              'nn': ['Neural Network', 'purple']}\n\n    try:\n        plot_title = my_estimators[estimator][0]\n        color_value = my_estimators[estimator][1]\n    except KeyError as e:\n        print(\"'{0}' does not correspond with the appropriate key inside the estimators dictionary. \\\n\\nPlease refer to function to check `my_estimators` dictionary.\".format(estimator))\n        raise\n\n    fig, ax = plt.subplots(figsize=(10, 10))\n    ax.set_facecolor('#fafafa')\n\n    plt.plot(fpr, tpr,\n             color=color_value,\n             linewidth=1)\n    plt.title('ROC Curve For {0} (AUC = {1: 0.3f})'\\\n              .format(plot_title, auc))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=2) # Add Diagonal line\n    plt.plot([0, 0], [1, 0], 'k--', lw=2, color = 'black')\n    plt.plot([1, 0], [1, 1], 'k--', lw=2, color = 'black')\n    if xlim is not None:\n        plt.xlim(*xlim)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.show()\n    plt.close()","458c780d":"plot_roc_curve(fpr2, tpr2, auc_rf, 'rf',\n               xlim=(-0.01, 1.05), \n               ylim=(0.001, 1.05))","c7365d69":"# zoom in\nplot_roc_curve(fpr2, tpr2, auc_rf, 'rf', \n               xlim=(-0.01, 0.2), \n               ylim=(0.85, 1.01))","986eb0b6":"def print_class_report(predictions, alg_name):\n    # print some title\n    print('Classification Report for {0}:'.format(alg_name))\n    # print class report metrics for each target \n    print(classification_report(predictions, \n            test_class_set, \n            target_names = dx))","26b17ffd":"class_report = print_class_report(rf_predictions, 'Random Forest')","8a72c8a4":"# ReImport modules\n# notebook isnt producing the correct results unless I redo this little bit of work.\n\n%matplotlib inline\n\nimport time\nimport random\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import RandomForestClassifier \nfrom urllib.request import urlopen \n\n# read in data and look at it \nplt.style.use('ggplot')\npd.set_option('display.max_columns', 500) \n\nbreast_cancer = pd.read_csv('..\/input\/breast-cancer-wisconsin-data\/data.csv')\n\nnames = ['id', 'diagnosis', 'radius_mean', \n         'texture_mean', 'perimeter_mean', 'area_mean', \n         'smoothness_mean', 'compactness_mean', \n         'concavity_mean','concave_points_mean', \n         'symmetry_mean', 'fractal_dimension_mean',\n         'radius_se', 'texture_se', 'perimeter_se', \n         'area_se', 'smoothness_se', 'compactness_se', \n         'concavity_se', 'concave_points_se', \n         'symmetry_se', 'fractal_dimension_se', \n         'radius_worst', 'texture_worst', \n         'perimeter_worst', 'area_worst', \n         'smoothness_worst', 'compactness_worst', \n         'concavity_worst', 'concave_points_worst', \n         'symmetry_worst', 'fractal_dimension_worst'] \n\ndx = ['Benign', 'Malignant']\nbreast_cancer.head()","469e7c3d":"# Setting 'id_number' as our index\nbreast_cancer.set_index(['id'], inplace = True) \n\n# Converted to binary to help later on with models and plots\nbreast_cancer['diagnosis'] = breast_cancer['diagnosis'].map({'M':1, 'B':0})\n\n# check mapping worked \nmapping = breast_cancer[['diagnosis']]\nmapping","13169b78":"# check check number of null values in each field\nbreast_cancer.apply(lambda x: x.isnull().sum())\n\n# drop the field we dont need\nbreast_cancer = breast_cancer.drop(columns=['Unnamed: 32'])\n\n# double check the field is gone\nbreast_cancer.apply(lambda x: x.isnull().sum())","a7aca155":"breast_cancer.info()","606913e5":"from sklearn.model_selection import train_test_split\n\nfeature_space = breast_cancer.iloc[:, breast_cancer.columns != 'diagnosis']\nfeature_class = breast_cancer.iloc[:, breast_cancer.columns == 'diagnosis']\n\nX_train, X_test, y_train, y_test = train_test_split(feature_space, feature_class, test_size=0.20, random_state = 42, stratify=feature_class)","a1980411":"import numpy as np\nfrom sklearn.preprocessing import StandardScaler\n\n# instantiate scaler\nscaler = StandardScaler()\n\n# scale the train and test data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\ny_train = np.array(y_train).ravel()","7ba87e44":"rf_pca_classifier = RandomForestClassifier(random_state=42, n_estimators=10)","b828a84b":"import matplotlib.pyplot as plt\nimport seaborn as sb\n\nfrom sklearn.decomposition import PCA \n\n# instantiate PCA for testing number of components, I will analyze explained_variance\ncomponent_test = PCA(n_components=30)\ncomponent_test.fit(X_train_scaled)\n\nsb.set(style='whitegrid')\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative Explained Variance')\nplt.plot(np.cumsum(component_test.explained_variance_ratio_))\nplt.axvline(linewidth=4, color='r', linestyle = '--', x=14, ymin=0, ymax=1)\nplt.show()","d60f0c11":"# initialize n_components\npca = PCA(n_components=14)\n\n# fit to our scaled training data\npca.fit(X_train_scaled)\n\n# use PCA fit to transform X_train and X_test data \nX_train_scaled_pca = pca.transform(X_train_scaled)\nX_test_scaled_pca = pca.transform(X_test_scaled)","2939c026":"# just so I can view how much variance features carry\nevr = component_test.explained_variance_ratio_\ncvr = np.cumsum(component_test.explained_variance_ratio_)\npca_df = pd.DataFrame()\npca_df['Cumulative Variance Ratio'] = cvr\npca_df['Explained Variance Ratio'] = evr\npca_df","dafaf1c1":"# get the indices for the pca\npca_dims = []\nfor x in range(0, len(pca_df)):\n    pca_dims.append('PCA Component {}'.format(x))\n\npca_test_df = pd.DataFrame(component_test.components_, columns=names_index, index=pca_dims)\npca_test_df.head()","c819f2bc":"np.random.seed(42)\nstart = time.time()\n\n# give to GridSearchCV\nparam_dist = {'max_depth': [2, 3, 9, 10, 12, 13, 15],\n              'bootstrap': [True, False],\n              'max_features': ['auto', 'sqrt', 'log2', None],\n              'criterion': ['gini', 'entropy'],\n              'min_samples_split': [6,8,10,12,16,18,20],\n              'min_samples_leaf':[3,4,5,6,7,9]}\n\n# set up the GridSearch\ncv_rf = GridSearchCV(rf_pca_classifier, cv = 5,\n                     param_grid=param_dist, \n                     n_jobs = 5)\n\n# fit the GridSearch\ncv_rf.fit(X_train_scaled_pca, y_train)\nprint('Best Parameters using grid search: \\n', cv_rf.best_params_)\n\n\nend = time.time()\nprint('Time taken in grid search: {0: .2f}'.format(end - start))","8c39377c":"# I set bootstrap = true so I can do OOB\nrf_pca_classifier.set_params(bootstrap=True,criterion='gini', max_depth=9,\n                             max_features='auto', min_samples_leaf=3, min_samples_split=6)\n","181b1107":"rf_pca_classifier.set_params(warm_start=True, \n                  oob_score=True)\n\nmin_estimators = 15\nmax_estimators = 200\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1):\n    rf_pca_classifier.set_params(n_estimators=i)\n    rf_pca_classifier.fit(X_train_scaled_pca, y_train)\n\n    oob_error = 1 - rf_pca_classifier.oob_score_\n    error_rate[i] = oob_error","79e50d89":"fig, ax = plt.subplots(figsize=(10, 10))\n\nax.set_facecolor('#fafafa')\n\noob_series.plot(kind='line',\n                color = 'blue')\nplt.axhline(0.055, \n            color='#875FDB',\n           linestyle='--')\nplt.axhline(0.05, \n            color='#875FDB',\n           linestyle='--')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')","2b3c5091":"rf_pca_classifier.set_params(n_estimators=185,\n                  bootstrap = False,\n                  warm_start=False, \n                  oob_score=False)","ef862449":"rf_pca_classifier.fit(X_train_scaled_pca, y_train)","ecf4240c":"cross_val_metrics(rf_pca_classifier, \n                  X_train_scaled_pca, \n                  y_train, \n                  'rf',\n                  print_results = True)","fe5c33c2":"pca_rf_predictions = rf_pca_classifier.predict(X_test_scaled_pca)","e4d13722":"conf_mat = create_conf_mat(test_class_set, pca_rf_predictions)\nsns.heatmap(conf_mat, annot=True, fmt='d', cbar=False)\nplt.xlabel('Predicted Values')\nplt.ylabel('Actual Values')\nplt.title('Actual vs. Predicted Confusion Matrix')\nplt.show()","2d648dd6":"accuracy_rf = rf_pca_classifier.score(X_test_scaled_pca, y_test)\n\nprint(\"Here is our mean accuracy on the test set:\\n {0:.3f}\"\\\n      .format(accuracy_rf))","fbce2544":"# Here we calculate the test error rate!\ntest_error_rate_rf = 1 - accuracy_rf\nprint(\"The test error rate for our model is:\\n {0: .4f}\"\\\n      .format(test_error_rate_rf))","eefad498":"# predict_proba returns two arrays that represent the predicted_prob of negative class and positive class respectively.\n# I just want to take the positive classes in this instance\n\npredictions_prob_pca_rf = rf_pca_classifier.predict_proba(X_test_scaled_pca)[:, 1]\n\n# use roc_curve to produce \nfals_pos_r2, true_pos_r2, _ = roc_curve(y_test,\n                          predictions_prob_pca_rf,\n                          pos_label = 1)\n\nauc_pca_rf = auc(fals_pos_r2, true_pos_r2)\n\nprint(auc_pca_rf)","67019d68":"plot_roc_curve(fals_pos_r2, true_pos_r2, auc_pca_rf, 'rf',\n               xlim=(-0.01, 1.05), \n               ylim=(0.001, 1.05))","272ce04a":"plot_roc_curve(fals_pos_r2, true_pos_r2, auc_pca_rf, 'rf',\n               xlim=(-0.01, 0.2), \n               ylim=(0.85, 1.01))","abdae6c2":"print('Classification Report for PCA reduced Random Forest\\n', classification_report(pca_rf_predictions, y_test, target_names = dx))","5951935f":"I will use the PCA to reduce our feature space into 14 components with a variance over 95%","80b00503":"This is my exploration of Random Forest for Classification.\n\nIn this notebook I use the Wisconsin Breast Cancer dataset from UCI. More information about the dataset and about FNA scans, [here](http:\/\/pages.cs.wisc.edu\/~olvi\/uwmp\/cancer.html). I also took some guidance from Raul's notebook on this topic which can also be found, [here](https:\/\/www.kaggle.com\/raviolli77\/random-forest-in-python). I strongly recommend learning about the dataset and visiting Raul's notebook before exploring Random Forest on your own.\n\nAdditionally, reviewing Principle Component Analysis may serve useful before viewing my work.\n\nThe purpose of this notebook is to be able to classify cells as Malignant or Benign based off information recorded from an FNA scan. \n\nI will be releasing another notebook soon which will compare random forest, Logistic Regression, Nearest-Neighbor, SVM, and Neural Networks for model selection. \n\nFor now lets jump into this Random Forest classification problem. ","24fdcb62":"No class imbalance here so we can proceed.","11d7d616":"Conclusion\n\nMore work is to be done...\n\nAs we can see from our results of these two models, Random Forest does well at classifying given enough information. The use of PCA greatly enhances the performance of the overall model by being able to capture the variance of 30 original features and consolidate into 14 components. This reduction allowed for the model to make faster predictions without losing any precision or accuracy compared to the stand-alone Random Forest model.\n\nThere is a ton of analysis that could be done on the feature space that I did not do. The most analysis I did in the notebook was understanding the importance of each feature but I went on to explore PCA's rather than understand each feature in deep depth. I most likely will explore this another time. ","797f7ee6":"Set up for ROC (receiver operating characteristic) curve which calculates the false positive rates and true positive rates across different thresholds.\n\nAn ideal model will have a false positive rate of 0 and true positive rate of 1. Most the curve will be in the top left corner of the graph.\n\nOn the other hand, a ROC curve that is at 45 degrees is indicative of a model that is essentially randomly guessing. Most of the curve will be in the middle of the graph.","622bbca6":"I ended up testing and choosing values 400-430 for the number of estimators. This is where the error fluctuate last, near 0.0500. "}}