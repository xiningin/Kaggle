{"cell_type":{"a2a35c43":"code","8664260d":"code","27cd4267":"code","6946ec61":"code","1cdccfcb":"code","22fd423d":"code","f97c9b56":"code","1de554c9":"code","44e035c0":"code","b485cdd0":"code","940bdfbb":"code","9e5b7d8c":"code","516e0737":"code","65151d0b":"code","d602285b":"code","8a334f16":"code","26b96294":"code","788c7ce7":"code","d7f28a29":"code","886fe803":"code","9d3dc7ff":"code","9569a6b9":"code","ccba29f4":"code","6c65cbc1":"code","bef53d6b":"code","d28ed8a4":"code","3c81a9ee":"code","e56a6319":"code","c8b68f16":"code","052998c4":"code","014abe63":"code","2cdd9ab5":"code","e70a12f4":"code","6113e64a":"code","f6db8220":"code","4e20b6ce":"code","ada16261":"code","cff40899":"code","34048f43":"code","70d4886f":"code","7550a10b":"code","26ceb565":"code","636a4890":"markdown","2af34215":"markdown","9a6aa1d8":"markdown","54afab67":"markdown","d8113226":"markdown","b4939a2a":"markdown","58b10280":"markdown","5109a275":"markdown","921a78a0":"markdown","673738b6":"markdown","a472ed7e":"markdown","f393e15f":"markdown","8b01a56b":"markdown","aadf8981":"markdown","c77b262d":"markdown","4910d24c":"markdown","1d4a122c":"markdown","3b95161b":"markdown"},"source":{"a2a35c43":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8664260d":"import pandas as pd\nfrom pandas import DataFrame\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n\ndf = pd.read_csv(\"\/kaggle\/input\/telco-customer-churn\/WA_Fn-UseC_-Telco-Customer-Churn.csv\")\n","27cd4267":"df.head()","6946ec61":"df.dtypes","1cdccfcb":"df.isnull().sum()","22fd423d":"df.drop(['customerID'],axis=1,inplace=True)\n","f97c9b56":"df.MultipleLines.unique()","1de554c9":"df.InternetService.unique()","44e035c0":"df.OnlineSecurity.unique()","b485cdd0":"df.OnlineBackup.unique()","940bdfbb":"df.DeviceProtection.unique()","9e5b7d8c":"df.TechSupport.unique()","516e0737":"df.StreamingTV.unique()","65151d0b":"df.StreamingMovies.unique()","d602285b":"df.Contract.unique()","8a334f16":"df.PaymentMethod.unique()","26b96294":"df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')","788c7ce7":"df.isnull().sum()\n","d7f28a29":"is_NaN = df.isnull()\nmask = is_NaN.any(axis=1)\nnan_rows = df[mask]\nprint(nan_rows)","886fe803":"df.dropna(inplace=True)\n\n","9d3dc7ff":"df['Churn'] = df['Churn'].replace('Yes',1)\ndf['Churn'] = df['Churn'].replace('No',0)\n\ndf['gender'] = df['gender'].replace('Male',1)\ndf['gender'] = df['gender'].replace('Female',0)\n\ndf['Partner'] = df['Partner'].replace('No',0)\ndf['Partner'] = df['Partner'].replace('Yes',1)\n\ndf['PhoneService'] = df['PhoneService'].replace('No',0)\ndf['PhoneService'] = df['PhoneService'].replace('Yes',1)\n\ndf['Dependents'] = df['Dependents'].replace('No',0)\ndf['Dependents'] = df['Dependents'].replace('Yes',1)\n\ndf['PaperlessBilling'] = df['PaperlessBilling'].replace('No',0)\ndf['PaperlessBilling'] = df['PaperlessBilling'].replace('Yes',1)\n\n\ndf_dummies = pd.get_dummies(\n    columns = [\n        'MultipleLines',\n        'InternetService',\n        'OnlineSecurity',\n        'OnlineBackup',\n        'DeviceProtection',\n        'TechSupport',\n        'StreamingTV',\n        'StreamingMovies',\n        'Contract',\n        'PaymentMethod'\n    ],\n    data = df)\n","9569a6b9":"df_dummies.head()\n\n","ccba29f4":"sns.countplot(x=\"Churn\",data=df)\n","6c65cbc1":"def correlation_matrix_viz(corr, name):\n\n    corr.style.background_gradient(cmap='coolwarm')\n    # print(corr)\n\n    # plt.matshow(corr)\n    # plt.show()\n\n    f = plt.figure(\n        figsize=(15, 15)\n    )\n\n    plt.matshow(\n        corr,\n        fignum=f.number\n    )\n\n    plt.xticks(range(corr.shape[1]), corr.columns, fontsize=14, rotation=90)\n    plt.yticks(range(corr.shape[1]), corr.columns, fontsize=14)\n    cb = plt.colorbar()\n    cb.ax.tick_params(labelsize=14)\n    plt.title(\"Correlation Matrix \" + name, fontsize=16)\n    plt.plot()\n","bef53d6b":"\ncorr_data = df_dummies.corr()\ncorrelation_matrix_viz(corr_data, 'raw')","d28ed8a4":"print(corr_data[\"tenure\"])","3c81a9ee":"churn_correl = corr_data['Churn']\nchurn_correl = churn_correl.drop(['Churn'])\nplt.figure(figsize=(16,16))\nchurn_correl = churn_correl.sort_values()\nchurn_correl.plot.bar()\nplt.show()","e56a6319":"from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, roc_auc_score, f1_score\n\ndef evaluation(Y_true, Y_pred, phase_name):\n    print(f\"Evaluation of data {phase_name}\")\n\n    print(\"Confusion matrix:\")\n    conf_matrix_test = confusion_matrix(Y_true, Y_pred)\n    print(conf_matrix_test)\n\n    print(\"F1 score:\")\n    f_1_test = f1_score(Y_true, Y_pred)\n    print(f_1_test)\n\n    print(\"Accuracy:\")\n    accuracy_test = accuracy_score(Y_true, Y_pred)\n    print(accuracy_test)\n\n    print(\"Precision:\")\n    precision_test = precision_score(Y_true, Y_pred)\n    print(precision_test)\n\n    print(\"Roc_auc:\")\n    roc_auc_test = roc_auc_score(Y_true, Y_pred)\n    print(roc_auc_test)\n\n\ndef model_evaluation(model_name, Y_train_true, Y_train_pred, Y_test_true, Y_test_pred):\n    print(\"--------------------------------------------------------------------------------\")\n    print(\"--------------------------------------------------------------------------------\")\n    print(f\"Evaluation of model: {model_name}\")\n    evaluation(Y_train_true, Y_train_pred, \"train_phase\")\n    evaluation(Y_test_true, Y_test_pred, \"test_phase\")\n","c8b68f16":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\n\nfrom sklearn.preprocessing import StandardScaler\n\nY = df_dummies['Churn']\n\ndf_final = df_dummies.drop('Churn', axis=1)","052998c4":"X = df_final\n\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, shuffle = True)","014abe63":"scaler = StandardScaler()\ntransformed_train = scaler.fit_transform(X_train)\ntransformed_test = scaler.transform(X_test)\n\nmodel_01 = LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n                   intercept_scaling=1, l1_ratio=None, max_iter=1000, n_jobs=10, penalty='l2',\n                   random_state=None, tol=0.0001, verbose=0,\n                   warm_start=False)\n\nmodel_01.fit(X = transformed_train, y = Y_train)\n\nY_train_predicted = model_01.predict(transformed_train)\nY_test_predicted = model_01.predict(transformed_test)\n\nmodel_evaluation(\"LogisticRegression\", Y_train, Y_train_predicted, Y_test, Y_test_predicted)","2cdd9ab5":"transposed = model_01.coef_.flatten()\n\nprint(transposed.shape)","e70a12f4":"pd.DataFrame(\n    {\n        'Input' : X.columns,\n        'Weights to normalized inputs' : transposed\n    }).sort_values('Weights to normalized inputs', ascending=False)\n","6113e64a":"import xgboost as xgb\nmodel_02 = xgb.XGBClassifier(\n    n_estimators=2000,\n    max_depth=5,\n    num_boost_round = 100,\n    # gamma =,\n    # reg_alpha= 0,\n    subsample= 0.8\n)\n\nX_train_t, X_valid, Y_train_t, Y_valid = train_test_split(X_train, Y_train, test_size = 0.1)\nmodel_02.fit(X_train_t, Y_train_t, eval_set=[(X_valid, Y_valid)], early_stopping_rounds = 5)\n\nY_train_predicted = model_02.predict(X_train)\nY_test_predicted = model_02.predict(X_test)\n\nmodel_evaluation(\"RandomForest\", Y_train, Y_train_predicted, Y_test, Y_test_predicted)","f6db8220":"pd.DataFrame(\n    {\n        'Input' : X.columns,\n        'Importance' : model_02.feature_importances_\n    }).sort_values('Importance', ascending=False)\n","4e20b6ce":"import shap\nexplainer = shap.TreeExplainer(model_02)\nshap_values = explainer.shap_values(X_train)\n\n\n\nnp.abs(shap_values.sum(1) + explainer.expected_value - Y_train_predicted).max()\nshap.summary_plot(shap_values, X_train)","ada16261":"df_dummies.loc[df_dummies[\"Churn\"] == 0, 'Contract_Month-to-month'].mean()","cff40899":"df_dummies.loc[df_dummies[\"Churn\"] == 1, 'Contract_Month-to-month'].mean()\n","34048f43":"df_dummies.loc[df_dummies[\"Churn\"] == 0, 'InternetService_Fiber optic'].mean()","70d4886f":"df_dummies.loc[df_dummies[\"Churn\"] == 1, 'InternetService_Fiber optic'].mean()\n","7550a10b":"df_dummies.loc[df_dummies[\"Churn\"] == 0, 'OnlineSecurity_No'].mean()","26ceb565":"df_dummies.loc[df_dummies[\"Churn\"] == 1, 'OnlineSecurity_No'].mean()","636a4890":"We can clearly see that 88+% of leaving customers have month-to-month contract.\n\nA significant majority (69%) of leaving customers also have Fiber optic cable and no online security (78%).\n\nI would say that the Company should try to persuade clients to sign to a longer contracts or give to the customers something extra for start and once customers get used to a product, switch to standard product package.\n\nAlso, there is some problem with the optic fiber connection. There might be stronger competition on the market or fiber optic connections are not stable\/fast enough for some reason.\n\nI don't know what \"Online security\" means, but I am guessing it is some sort of firewall. People without online security tend to leave company as well. The Company should probably make \"online security\" feature as a part of a standard product.\n\n\n","2af34215":"Dropping useless column.","9a6aa1d8":"Logistics regression is my first choice. The model is easy, robust, can't be overfitted and often provides good baseline.","54afab67":"Coding binary features into numeric values and creating one-hot encoding for non-binary non-numeric ones.","d8113226":"And dropping them as well, since there is relatively small number of them.","b4939a2a":"XGBoost returns slightly worse results than logistic regression. K-fold would give us greater confidence which model is better. XBBoost obviously uses validation set to detect overfitting. I've also tried naive bayes model, but it gave me poor results. It might be indicator that Chur value is based on multiple feature interaction.","58b10280":"Potentially bad methodology, since I am looking at all data. This might be done with seen-dataset only. But since I am not optimizing any parameters here, I am leaving it here for now","5109a275":"Printing general info about dataframe","921a78a0":"Plotting correlations with predicted value.\n\n\nPotentially bad methodology, since I am looking at all data. This might be done with seen-dataset only. But since I am not optimizing any parameters here, I am leaving it here for now.\n\n","673738b6":"Correlations of Tenure data feature with others. I am interested into this because higher tenure values reflects a satisfied customer.","a472ed7e":"Just printing unique values of non-binary and non-numeric columns","f393e15f":"If I would like to be super methodologically precise, K-fold would be nice here.","8b01a56b":"Just overall look on data correlations. Not providing much information since it is quite big.","aadf8981":"I must admit I am using SHAP analysis for the first time. I wanted to try it:","c77b262d":"Checking, how rows containing some nan values look like.","4910d24c":"Some naively-bayesian observations after model explanation. We can see probabilities of customer leaving\/staying based on three picked data features. (I am using means here since selected data features are in boolean values)","1d4a122c":"From correlation matrix, linear regression coefficients and XGBoost analysis we can see that tenure and total charges are highly correlated.\nWe can also see a positive correlation between customers leaving and monthly payments - which is not surprising nor interesting.\n\nThe longer customer uses the product, the lower probability one leaves the Company.","3b95161b":"Importance of features based on XGBoost differs from run to run. K-fold with multiple fits per fold would give us better statistics over what features are important."}}