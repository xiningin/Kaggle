{"cell_type":{"f1e0ce78":"code","82b3ae36":"code","1fd6c9fc":"code","51a9b217":"code","aaf86279":"code","bb7e842b":"code","dc636864":"code","cb3feac6":"code","1df4b49b":"code","80eb0b15":"code","5b64a623":"code","7dd694fe":"code","cc2920b8":"code","4907ba96":"code","ed8a9ffd":"code","f33ee49a":"markdown","165b5205":"markdown","023609f0":"markdown","e2085027":"markdown","fe692767":"markdown","b15e044f":"markdown","e3725699":"markdown","2b1bf881":"markdown","da80ada7":"markdown","feddc555":"markdown","ad21b8c1":"markdown","b5b43579":"markdown","451cbe8c":"markdown","dc5257d7":"markdown","dc505054":"markdown","4b4ff5f8":"markdown","8ea9d70b":"markdown","d403864e":"markdown","7b55bcd2":"markdown","043a2a89":"markdown","fd1fd016":"markdown","fe628e33":"markdown"},"source":{"f1e0ce78":"import sys\nsys.path.append('..\/input\/iterativestratification\/iterative-stratification-master\/')\nfrom iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.metrics import log_loss\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nimport tensorflow_addons as tfa\n\nimport random","82b3ae36":"X_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_features.csv')\nX_test = pd.read_csv('\/kaggle\/input\/lish-moa\/test_features.csv')\ny_train = pd.read_csv('\/kaggle\/input\/lish-moa\/train_targets_scored.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/lish-moa\/sample_submission.csv')","1fd6c9fc":"X_train.set_index('sig_id', inplace=True)\nX_test.set_index('sig_id', inplace=True)\ny_train.set_index('sig_id', inplace=True)","51a9b217":"X_train.cp_time = X_train.cp_time \/\/ 24\nX_train.cp_dose = X_train.cp_dose.map({'D1': 0, 'D2': 1})\nX_test.cp_time = X_test.cp_time \/\/ 24\nX_test.cp_dose = X_test.cp_dose.map({'D1': 0, 'D2': 1})","aaf86279":"X_train_moa = X_train[X_train.cp_type != 'ctl_vehicle'].drop(columns=['cp_type'])\nX_test_moa = X_test[X_test.cp_type != 'ctl_vehicle'].drop(columns=['cp_type'])\n\n# Don't forget to keep only proper rows in y_train\ny_train_moa = y_train.loc[X_train_moa.index]","bb7e842b":"X_train_moa.head()","dc636864":"def mean_predictions(models_dense, X):\n    y_pred_dense = [model.predict(X) for model in models_dense]\n    return np.mean(y_pred_dense, axis=0)\n\ndef macro_log_loss(y_true, y_pred):\n    if len(y_true.shape) == 1:\n        return log_loss(y_true, y_pred, labels=[0, 1]), [log_loss(y_true, y_pred, labels=[0, 1])] \n    y_pred = np.maximum(np.minimum(y_pred, [[1 - 1e-15] * y_pred.shape[1]] * y_pred.shape[0]), [[1e-15] * y_pred.shape[1]] * y_pred.shape[0])\n    losses = [log_loss(y_true[:, i], y_pred[:, i], labels=[0, 1]) for i in range(y_true.shape[1])]\n    return np.mean(losses), losses","cb3feac6":"def create_dense_model(input_shape, output_shape):\n    inputs = keras.Input(shape=(input_shape,), name='drug')\n    x = layers.BatchNormalization()(inputs)\n    x = layers.Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(layers.Dense(units=256, activation='relu', name='dense_1'))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    x = tfa.layers.WeightNormalization(layers.Dense(units=256, activation='relu', name='dense_2'))(x)\n    x = layers.BatchNormalization()(x)\n    x = layers.Dropout(0.2)(x)\n    output = tfa.layers.WeightNormalization(layers.Dense(output_shape, activation='sigmoid', kernel_regularizer=keras.regularizers.l2(l2=1e-5), \n                                                         name='predictions'))(x)\n    model = keras.Model(inputs=inputs, outputs=output)\n    opt = tfa.optimizers.AdamW(weight_decay=1e-5, learning_rate=1e-2)\n    model.compile(optimizer=opt, loss='binary_crossentropy')\n    return model","1df4b49b":"callbacks = [\n    keras.callbacks.EarlyStopping(\n        monitor='val_loss',\n        min_delta=1e-5,\n        patience=3,\n        verbose=0),\n    keras.callbacks.ReduceLROnPlateau(\n        monitor=\"val_loss\",\n        factor=0.1,\n        patience=2)\n]\n","80eb0b15":"x_train_dense = X_train_moa.to_numpy()\ny_train_dense = y_train_moa.to_numpy()\n\nkf = MultilabelStratifiedKFold(n_splits=5, shuffle=True)\nmlls = []\nmodels = []\ni = 1\nfor train, test in kf.split(x_train_dense, y_train_dense):\n    print(f'FOLD {i}.', end=' ')\n    i = i + 1\n    \n    from numpy.random import seed\n    seed(train[0])\n    tf.random.set_seed(train[0])\n\n    dense = create_dense_model(x_train_dense.shape[1], y_train_dense.shape[1])\n    history_dense = dense.fit(x_train_dense[train], y_train_dense[train],\n                     batch_size=64,\n                     epochs=100,\n                     callbacks=callbacks,\n                     validation_data=(x_train_dense[test], y_train_dense[test]),\n                     shuffle=True, verbose=0)\n    mll, _ = macro_log_loss(y_train_dense[test], dense.predict(x_train_dense[test]))\n    models.append(dense)\n    mlls.append(mll)\n    print('Dense macro log loss:', mll)","5b64a623":"print(np.mean(mlls))","7dd694fe":"preds = mean_predictions(models, X_test_moa)","cc2920b8":"indexes = dict(zip(X_test_moa.index, range(X_test_moa.shape[0])))","4907ba96":"for idx, row in submission.iterrows():\n    sig_id = row.sig_id\n    if sig_id in indexes.keys():\n        submission.loc[idx] = [sig_id] + [np.maximum(np.minimum(pred, 1 - 1e-3), 1e-3) for pred in preds[indexes[sig_id]]]\n    else:\n        submission.loc[idx] = [sig_id] + [0] * preds.shape[1]","ed8a9ffd":"submission.to_csv('submission.csv', index=False)","f33ee49a":"# Code Time","165b5205":"Now we have such dataframe","023609f0":"The only one library, that might confuse you, is iterstrat. You can find it on Kaggle or download [here](https:\/\/github.com\/trent-b\/iterative-stratification). It provides multilabel stratified k-fold. You can't use StratifiedKFold from sklearn because of multilabel task (you have 206 targets, and StratifiedKFolds supports only 1d target arrays).","e2085027":"Finally, let's change submission row by row. If $sig\\_id$ is not in test indexes, then it's control group and should have no MoA. And we also clip our predictions. We do that to reduce the cost of mistakes.","fe692767":"# Hi everyone!\n\nThis is my first public work here as well as first big competition on Kaggle :)\n\nWhat do we have here? The competition is about mechanisms of drugs action. First steps, such as basic info and EDA, you can find here:\n* [Insights](http:\/\/www.kaggle.com\/c\/lish-moa\/discussion\/184005)\n* [EDA](https:\/\/www.kaggle.com\/isaienkov\/mechanisms-of-action-moa-prediction-eda)\n\nBoth of those links might be extremely useful for you.","b15e044f":"Also, let's transform $cp\\_time$ and $cp\\_dose$. $Cp\\_time$ can be divided by 24 to get duration in days, and $cp\\_dose$ should be categorical feature with two possible integer values.","e3725699":"Now we can write a function to create a simple dense neural network. It consists of three layer triplets (BatchNorm-Dropoup-Dense with weight norm). Last one is output. I've tried different units number, dropout rates, activation functions and optimizers, and those are the best params I've found so far.","2b1bf881":"Finally, let's predict out test and do submission","da80ada7":"The idea is simple. Let's do a cross-validation, but also let's save every model we've trained. Then we can apply them to our test and average their predictions - that should be something with low variance.\n\nWe need some new functions. First of them will apply given models to given dataframe and get a mean array of their predictions. Second of them will count log loss. It also returns losses by column. They might be useful if you want to know, which column is the best to predict and which one is the worst.","feddc555":"Reading train features, train target, test and submission data.","ad21b8c1":"Let's have some callbacks for fitting our model. First of them will be $EarlyStopping$, so we want to stop, when the learning process doesn't improve validation loss during $patience$ number of epochs. Second is there to reduce learning rate when learning is no more useful.","b5b43579":"Now let's train our models with 5 splits. We can save losses of every single model to average them and see the mean loss.  ","451cbe8c":"## Thanks for reading! \nIf you have any ideas, how to make this solution better, or you just want to tell your opinion - please let me know in the comments :)","dc5257d7":"And the final step is to write our submission to csv file","dc505054":"And let's print our mean loss. It's definitely not so bad!","4b4ff5f8":"## Briefly about my way in MoA competition \n\nI guess, everyone should have started doing his first solution based on some basic ML algorithms. Of course, me too. So, my first submission was based on logistic regression. I was training it for every label alone, and then concatenating predictions to get the submission dataframe. Unfortunately, results were frustrating. \n\nThe next step was to change LR model to something more complicated. I chose XGBClassifier. It was doing much better, but it took a couple of my life hours to tune its hyperparameters, and it's not worth the results.\n\nFinally, I was told that all the best results are based on neural networks. Until that moment I was just living with kinda small NN inside my head, but never did any NN by myself. So, everything I had was Google and motivation.\n\nI decided to share my results here with you to probably help someone else, who is also newbie, and maybe get some useful advices from someone :)\n\nGood luck with MoA competition! If you find this notebook somehow useful for you, please thumbs up :)","8ea9d70b":"First of all let's make an index from $sig\\_id$ column. That might be helpful. Now we always can get proper line from every dataframe","d403864e":"Finally, we should delete control rows from both train and test data, because they have no mechanisms of action and might confuse our neural network. And we can delete $cp\\_type$ column after that, because it's no more informative.","7b55bcd2":"Let's make a dict to connect test dataframe indexes with predictions array indexes to get a prediction for submission row by $sig\\_id$.","043a2a89":"## Test predictions","fd1fd016":"## NN","fe628e33":"### Preparing data"}}