{"cell_type":{"b1d09855":"code","d4c0a591":"code","94051ef5":"code","e7d8de84":"code","2fd395b7":"code","6c2401d2":"code","e3c25ef3":"code","a88de072":"code","1289c7e0":"code","5801a418":"code","c13ac4fe":"code","1ce4541d":"code","c98cb5cf":"code","94e6a5ac":"code","d52471f8":"code","306a0012":"code","00ed779e":"code","32ccb11c":"code","8ee3b420":"code","7549232e":"code","1a571823":"code","d3d52709":"code","5efa4544":"code","e033d134":"code","72d8b84e":"code","99dd3145":"code","acc1835a":"code","9bea97f5":"code","f586c28f":"code","a1492f2a":"code","5a73a7b4":"code","9d3d960c":"code","9851bd07":"code","9d3cd9aa":"code","4dfbb463":"code","88821500":"code","da1129ea":"code","10a67316":"code","7a34b89b":"code","1dc56bca":"code","61865ef3":"code","9bd53994":"code","5623717f":"markdown","4e4fdc5f":"markdown","264427c6":"markdown","e3db05de":"markdown","4250d8de":"markdown","1cfd44d6":"markdown","f43433f8":"markdown","51acbbd7":"markdown","30f7aa60":"markdown","5e57f1b6":"markdown","ec0a015d":"markdown","44c58eaa":"markdown","d3b0aaf1":"markdown","3171c6d2":"markdown","e62cfbbc":"markdown","4a7bba28":"markdown","c85e1248":"markdown","df070f5f":"markdown","224988ea":"markdown","38cb5877":"markdown","094bbefc":"markdown","f45defc5":"markdown","5ce7165d":"markdown","9d31bae7":"markdown","64b8a8d3":"markdown","9575c629":"markdown","94a8f3ef":"markdown","28d0963d":"markdown","90c443c0":"markdown","9463a05c":"markdown","b0e2a246":"markdown","2ca65eaf":"markdown","4b922917":"markdown","496faece":"markdown","27182d8a":"markdown","06d2eef6":"markdown","1e319f05":"markdown","8e23c3af":"markdown"},"source":{"b1d09855":"## install packages \nimport os , glob \nimport pandas as pd \nimport numpy as np \nfrom scipy import stats\nimport math \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nsns.set(style=\"whitegrid\")\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n#machine learning\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, Normalizer, MaxAbsScaler\nfrom sklearn.preprocessing import (StandardScaler, PowerTransformer, QuantileTransformer ,LabelEncoder,OneHotEncoder, OrdinalEncoder)\n#models \nfrom xgboost import XGBRegressor\nprint(\"set up complete\")","d4c0a591":"train_data= pd.read_csv(\"..\/input\/30-days-of-ml\/train.csv\") # train_data \ntest_data = pd.read_csv(\"..\/input\/30-days-of-ml\/test.csv\") # test_data \nsample = pd.read_csv(\"..\/input\/30-days-of-ml\/sample_submission.csv\",index_col=\"id\") # sample_submission ","94051ef5":"#quick look at the train data\ntrain_data.head(3)\ntrain_data.tail(3)","e7d8de84":"print(f'Number of rows: {train_data.shape[0]};  Number of columns: {train_data.shape[1]}; No of missing values: {sum(train_data.isna().sum())}')","2fd395b7":"print('Info about train data: ')\ntrain_data.info()","6c2401d2":"cat_features = [feature for feature in train_data.columns if 'cat' in feature]\ncont_features = [feature for feature in train_data.columns if 'cont' in feature]\nprint(f'categorical features are : {cat_features};  numirical features are  : {cont_features}')","e3c25ef3":"train_data.describe().T.style.bar().background_gradient(cmap='coolwarm')","a88de072":"\nplt.figure(figsize=(20,10))\nax= sns.violinplot(data=train_data[cont_features],inner=None, palette=\"hls\")\nplt.title('Continuous features distribution');\n","1289c7e0":"train_data.var()","5801a418":"train_data.std()","c13ac4fe":"cat= train_data.select_dtypes(include='object').columns.tolist()\nidx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\nfor row in range(5):\n    for col in range(2):\n        data = train_data[cat[idx]].value_counts()\n        sns.barplot(x = data.index, y = data.values, palette='bright', ax=axes[row, col])\n        axes[row,col].set_title(cat[idx])\n        idx += 1","1ce4541d":"# Correlationmatrix\ncorrMatrix =train_data.corr(method='pearson', min_periods=1)\ncorrMatrix \n","c98cb5cf":"#heatmap \nplt.figure(figsize=(25,20))\nax = sns.heatmap(corrMatrix, cmap=\"YlGnBu\", annot=True)","94e6a5ac":"print('target column basic statistics:')\ntarget=train_data['target']\ntarget.describe()","d52471f8":"plt.figure(figsize=(12,5))\nsns.distplot(target,color=\"darkblue\", kde=True,bins=120, label='target')\nplt.title(\"target values Distribution \")\n","306a0012":"plt.figure(figsize=(12,5))\nax = sns.boxplot(train_data[\"target\"] , orient='h')\nax.set_title('Target variable boxplot')","00ed779e":"target =train_data[\"target\"] \nfig,ax = plt.subplots(5,3,figsize=(20,30),sharey=False)\nrow = col = 0\nfor n,i in enumerate(cont_features):\n    if (n % 3 == 0) & (n > 0):\n        row += 1\n        col = 0\n    sns.scatterplot(x=i,y=target,data=train_data,color=\"g\",ax=ax[row,col])\n    ax[row,col].set_title(f\"Target vs {i}\")\n    col += 1\n    \nplt.show();","32ccb11c":"#quick look at the test data\ntest_data.head(3)\ntest_data.tail(3)","8ee3b420":"print(f'Number of rows: {test_data.shape[0]};  Number of columns: {test_data.shape[1]}; No of missing values: {sum(test_data.isna().sum())}')","7549232e":"test_data.describe().T.style.bar().background_gradient(cmap='coolwarm')\n","1a571823":"cont_test=test_data.select_dtypes('float64').columns.tolist()\nplt.figure(figsize=(20,10))\nax= sns.violinplot(data=test_data[cont_features],inner=None, palette=\"hls\")\nplt.title('Continuous test features distribution');\n","d3d52709":"cat_test= test_data.select_dtypes(include='object').columns.tolist()\nidx = 0\nf, axes = plt.subplots(5, 2, sharex=True, figsize=(12,14))\nplt.suptitle('Categorical features distribution', size=16, y=(0.94))\nfor row in range(5):\n    for col in range(2):\n        data = test_data[cat[idx]].value_counts()\n        sns.barplot(x = data.index, y = data.values, palette='colorblind', ax=axes[row, col])\n        axes[row,col].set_title(cat[idx])\n        idx += 1","5efa4544":"# Correlationmatrix\ncorrMatrix =test_data.corr(method='pearson', min_periods=1)\nplt.figure(figsize=(25,20))\nax = sns.heatmap(corrMatrix, cmap=\"Greens\", annot=True)","e033d134":"features = train_data.iloc[:,11:25]\n","72d8b84e":"# we will look into train_test features distribution \ni = 1\nplt.figure()\nfig, ax = plt.subplots(5, 3,figsize=(14, 24))\nfor feature in features:\n    plt.subplot(5, 3,i)\n    sns.distplot(train_data[feature],color=\"red\", kde=True,bins=120, label='train')\n    sns.distplot(test_data[feature],color=\"blue\", kde=True,bins=120, label='test')\n    plt.xlabel(feature, fontsize=9); plt.legend()\n    i += 1\nplt.show()","99dd3145":"train = train_data.copy()\ntest = test_data.copy()\ny= train_data[\"target\"]\n","acc1835a":"#LabelEncoder for categorical features \ncat_features = [feature for feature in train.columns if 'cat' in feature]\nfor col in cat_features:\n    encoder = LabelEncoder()\n    for col in cat_features:\n        train[col] = encoder.fit_transform(train[col])\n        test[col] = encoder.transform(test[col])\n","9bea97f5":"## install package \nimport featuretools as ft","f586c28f":"#generate new features for train_data\nautofeat = train.drop(['target'], axis=1)\ngenerate = ft.EntitySet(id = 'data')\ngenerate.entity_from_dataframe(entity_id = 'ml_challenge', \n                         dataframe = autofeat, \n                         index='id')\n\nfeature_matrix, feature_defs = ft.dfs(entityset = generate,                                          \n                                      target_entity = 'ml_challenge',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'], \n                                      verbose=1)        \n# generate new features for test_data\nautofeat_test= test.copy()\ngenerate.entity_from_dataframe(entity_id = 'ml_challenge', \n                         dataframe = autofeat_test, \n                         index='id')\n\nfeature_test_matrix, feature_defs = ft.dfs(entityset = generate,                                          \n                                      target_entity = 'ml_challenge',                               \n                                      trans_primitives = ['add_numeric', 'multiply_numeric'], \n                                      verbose=1)    ","a1492f2a":"# standarization\nX_train = feature_matrix.copy()\nX_test = feature_test_matrix.copy()\nnum_f = [feature for feature in X_train.columns if 'cont' or 'cat' in feature]\nnum_cols = X_train[num_f]\nfor i in num_cols:\n    scale = StandardScaler().fit(X_train[[i]])\n    X_train[i] = scale.transform(X_train[[i]])\n    X_test[i] = scale.transform(X_test[[i]])\n# adding target and id back  column back \nprint(X_train.shape, X_test.shape)","5a73a7b4":"## for  i will use only the first 100 features\nX_train = X_train.iloc[:,0:100]\nX_test =X_test.iloc[:,0:100]\nprint(X_train.shape, X_test.shape)","9d3d960c":"# data normalization \nX_train_nor = train.copy()\nX_test_nor = test.copy()\nX_train_nor.drop([\"target\",'id'],axis=1,inplace=True)\nX_test_nor.drop(['id'], axis=1, inplace=True)\nnum_f = [feature for feature in train.columns if 'cont' and 'cat' in feature]\nnum_cols = train[num_f]\nfor i in num_cols:\n    norm = Normalizer().fit(X_train[[i]])\n# transform training data\nX_train_norm = norm.transform(X_train[[i]])\n# transform testing data\nX_test_norm = norm.transform(X_test[[i]])\n# converting to df \nX_train_normal = pd.DataFrame(X_train_norm)\nX_test_normal = pd.DataFrame(X_test_norm)\n### adding loss column back \nX_train_normal[\"target\"] = train_data[\"target\"]\nX_train_normal[\"id\"] = train_data[\"id\"]\nX_test_normal[\"id\"] = train_data[\"id\"]","9851bd07":"#install_packages \nimport h2o\nfrom h2o.automl import H2OAutoML \nfrom h2o.grid.grid_search import H2OGridSearch\n","9d3cd9aa":"h2o.init() # h2o initialization \ntrain= h2o.H2OFrame(X_train) # convert to h2o frame\ntest = h2o.H2OFrame(X_test) # convert to h2o frame \ntrain_data= h2o.H2OFrame(train_data)# convert to h2o frame \ntest_data= h2o.H2OFrame(test_data)# convert to h2o frame ","4dfbb463":"train['target']= train_data['target'] # adding target col back ","88821500":"#run model\nfeatures = [x for x in train.columns if x not in ['target']]\nauto_ml = H2OAutoML( \n    nfolds=7, # use 5 folds \n    seed = 1262,\n    max_models = 10,\n    include_algos = [\"DeepLearning\",\"StackedEnsemble\",\"XGBoost\",\"GBM\" ],\n    max_runtime_secs=2*3600,  #time in sec \n    stopping_metric='RMSE'\n    )\nauto_ml.train(x=features, y='target', training_frame=train)","da1129ea":"# check leaderboard\nleader = auto_ml.leaderboard\nleader","10a67316":"model = h2o.get_model(leader[0,\"model_id\"])\nmodel.varimp(use_pandas=True)\n","7a34b89b":"# visualization \nmodel.varimp_plot()","1dc56bca":"preds = auto_ml.leader.predict(test)","61865ef3":"## create submission\nsubmission = pd.DataFrame({\n    'id': test_data['id'].as_data_frame().id,\n    'target': preds.as_data_frame().predict\n})\nsubmission.head()","9bd53994":"# save submission\nsubmission.to_csv('h2o_submission.csv', index=False)","5623717f":"Now we print the distribution of categorical features","4e4fdc5f":" <div class=\"alert alert-info\">\n    <strong> <h1> Note: <\/h1><\/strong>\n   <strong>This is just a baseline submission where a lot of improvement can be made , i recommend you spend your most of time in  features egineering and features selection , meanwhile you can expriment with other models and approches<\/strong>.\n<\/div>","264427c6":"\n<div class=\"alert alert-success\">\n  <h2><center>  I hope you find this useful , Thank you and Good luck  \ud83d\ude4f <\/h2><\/center>\n<\/div>\n","e3db05de":"We can see that train and test features have almost identical distribution.","4250d8de":"<center><h1> 30 Days of ML Competition <h1><center>\n    <center><h5> I hope you find this helpful \ud83d\ude0a <h5><center>\n","1cfd44d6":"**Basic summary statistic**\n<p> basic statistics for numerical  variables in the train data which contain information on count, mean, standard deviation, minimum, median ,1st quartile and 3rd quartile and maximum.","f43433f8":"We can detect that are not much features with a lot of outliers.","51acbbd7":"**Basic summary statistic**\n<p>\nbasic statistics for numerical variables in the train data which contain information on count, mean, standard deviation, minimum, median ,1st quartile and 3rd quartile and maximum.","30f7aa60":"**Features Importance** ","5e57f1b6":"**Numerical features variation and standard deviation**\n","ec0a015d":"**Generate predictions**","44c58eaa":"## Building the Model \nNow after we did basic EDA and get some insghits from our data , it's time to start modeling the data in order to make predictions for the test data and then submit.","d3b0aaf1":"Yay ! featuretool have generated 552 new features for us ( with in seconds ) , now it's up for us to select the best of them. ","3171c6d2":"### Set UP The Notebook \nIn order to be able to explore this data we are going to setup the notebook , we import some common libraries used in data analysis including:\n\n* **Pandas** : to handle and work with dataframe.\n* **numpy** : for mathematical operations.\n* **matplotlib** : for creating static, animated, and interactive visualizations.\n* **seaborn** : to provide a high-level interface for drawing attractive and informative statistical graphics.\n* **Other packages** for machine learning ","e62cfbbc":"**Feature correlations**\n<p>\nlet's now calculate and plot feature correlations map.","4a7bba28":"We can see that columns [cat0, cat2, cat4, cat6, cat7]. are dominated by one category whcih make them non-informative but still we need to go deep.","c85e1248":"**Categorical features**","df070f5f":"![Capture (2).jpg](attachment:c91b6ca9-daae-4e4a-89c2-b42e1537f01b.jpg)","224988ea":"The train dataset has 300000 of rows with 26 of columns and there is 0 missing values. we can continue..","38cb5877":"The test data has  200000 rows and 25 columns with 0 missing values","094bbefc":"## About the competition \nThe 30 ML compettion is 15 day competition open only to people who have signed up for the [30 Days of ML program](https:\/\/www.kaggle.com\/thirty-days-of-ml).\n","f45defc5":"**Target variable**","5ce7165d":"## Exploratory Data Analysis","9d31bae7":"**Data Normalization**","64b8a8d3":"### Model Definition \nWe are going to use an autoMl library H2o Which is automated machine learning by [H2o.ai](https:\/\/www.h2o.ai\/) .and you can check [Documentation here](https:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html)","9575c629":"15 columns are in a float4 type while 10 are in object type and 1 in int64 type.\n\n","94a8f3ef":"## About the data \nwe will be predicting a continuous target based on a number of feature columns given in the data. All of the feature columns, cat0 - cat9 are categorical, and the feature columns cont0 - cont13 are continuous. the data contains three files : \n\n* train.csv - the training data with the target column\n* test.csv - the test set; you will be predicting the target for each row in this file\n* sample_submission.csv - a sample submission file in the correct format\n","28d0963d":"**Automated feature engineering** ","90c443c0":"\n**Feature correlations**\n<p> let's now calculate and plot feature correlations map fort the test data .\n","9463a05c":"### Train dataset ","b0e2a246":"**Categorical features**\n<p> Now we print the distribution of categorical features\n","2ca65eaf":"## Load the data \nNow we can load the data and start our analysis.","4b922917":"**Train\/Test features distribution**","496faece":"**Features Standarization**","27182d8a":"### Test dataset ","06d2eef6":"### Data preparation And Feature engineering ","1e319f05":"looks like XGboost and GBM are dominating the leaderboard.","8e23c3af":"![Capture.jpg](attachment:e32d6338-a34b-4736-8dc5-51a6142b2217.jpg)"}}