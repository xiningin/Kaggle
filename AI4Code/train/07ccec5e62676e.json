{"cell_type":{"952e359e":"code","cb1a92ab":"code","a710cc7f":"code","141a7694":"code","eddc65af":"code","8eca203c":"code","3f94c349":"code","0a3d6756":"code","f9c9518b":"code","a5cf590f":"code","3a77be40":"code","c7b98293":"code","2e930701":"code","77e455a8":"code","0c5f13e6":"code","dad664f6":"code","3e121699":"code","4f85f313":"code","0f046f49":"code","cd8c08b1":"code","9bba5b2e":"code","38ad1515":"code","9847de66":"code","9421ceb7":"code","b1f15c82":"code","5648f1bc":"code","32c561ec":"code","314fb039":"code","7ba3aae1":"code","c7edbfaf":"code","85d46977":"code","790ac90f":"markdown","d005d180":"markdown","ba30bb9c":"markdown","47604cae":"markdown","12115fdb":"markdown"},"source":{"952e359e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","cb1a92ab":"from transformers import AutoTokenizer, AutoModel,TFBertForSequenceClassification\nimport numpy as np\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModel\n\n# Load the BERT tokenizer and the pretrained Arabic base BERT model\nbert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya\/bert-base-arabic\")\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = TFAutoModel.from_pretrained(\"asafaya\/bert-base-arabic\")\n#get the liste of sentences\n\"\"\"\nWhen feeding text data into our model, there are a few things to \nbe aware of. First, we must use tokenizer.encode_plus(...) \nto convert our text into input IDs and attention mask tensors\n\"\"\"\nimport tensorflow as tf\n\ndef tokenize_bert(data):\n    sentences = data['setences'].values\n    seq_len = 512\n    num_samples = len(sentences)\n    #lists of ids\n    input_ids = np.zeros((num_samples, seq_len))\n    #get the lists of masks\n    attention_masks = np.zeros((num_samples, seq_len))\n    for i,sent in enumerate(sentences):\n        #tokenize words in the sentence\n        #return_tensors: return the resultat as tensor\n        #pad_to_max_length :pads the sentences to max length\n        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True\n                                            ,max_length =seq_len,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf')  # Return pytorch tensors.\n        #add encoded \n        input_ids[i, :] = bert_inp['input_ids']\n        #applying attention masks\n        attention_masks[i, :] = bert_inp['attention_mask']\n    return input_ids,attention_masks\n\n\ndef get_model():\n    # Load the BERT tokenizer and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya\/bert-base-arabic\")\n    # Load the BERT model and the pretrained Arabic base BERT model\n    model1 = TFAutoModel.from_pretrained(\"asafaya\/bert-base-arabic\")\n    model1.trainable = False\n    max_len = 512\n    #'input_ids', 'attention_mask'\n    input_ids = layers.Input(shape=(max_len,), dtype=tf.int32, name='input_ids')\n    attention_mask = layers.Input(shape=(max_len,), dtype=tf.int32, name='attention_mask')\n    embeddings = model1.bert(\n            input_ids, attention_mask=attention_mask\n        )[1]\n    x = tf.keras.layers.Dense(519, activation='relu')(embeddings)\n    #output layer\n    y = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\n    model1 = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=y)\n    \n    return model1","a710cc7f":"neg = \"..\/input\/twitter\/Negative\"\npos = \"..\/input\/twitter\/Positive\"\npositiveList=[]\nnegativeList=[]\n\nnegative = os.listdir(neg) \npostive = os.listdir(pos)\n\nfor file in negative:\n    with open('..\/input\/twitter\/Negative\/'+file,encoding=\"UTF-8\") as f:\n        try:\n            negativeList.append(f.read())\n        except:\n            pass\nfor file in postive:\n    with open(pos+\"\/\"+file,encoding=\"UTF-8\") as f:\n        try:\n            positiveList.append(f.read())\n        except:\n            pass\nprint(len(positiveList))\nprint(len(negativeList))\ndata={\"setences\":positiveList,\"positive\":[1 for i in range(len(positiveList))]}\ndf = pd.DataFrame.from_dict(data)\ndata1={\"setences\":negativeList,\"positive\":[0 for i in range(len(negativeList))]}\ndf1 =pd.DataFrame.from_dict(data1)\ndf = df.append(df1)\nfrom sklearn.utils import shuffle\nRANDOM_SEED = 14\n#shuffle the data randomly\ndata = shuffle(df, random_state=RANDOM_SEED)","141a7694":"from sklearn.model_selection import train_test_split\nsplit = int((80*len(data))\/100)\n#train \ntrain = data[:split]\ntest = data[split:]\n#get the inputs tokens and masks\ntrain_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask = tokenize_bert(test)","eddc65af":"optimizer = tf.keras.optimizers.Adam(lr=1e-5, decay=1e-6)\n#categorical_crossentropy\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = get_model()\nmodel.summary()\nmodel.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\nmodel.fit([train_inp,train_mask],train['positive'],epochs=16,verbose=2)","8eca203c":"model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\ntest_lost ,test_acc_model = model.evaluate(\n[test_inp,test_mask],test['positive']\n)\nprint(\"The accuracy of the model model is:\",(test_acc_model*100))","3f94c349":"name=\"BERT sentiment analysis\"\nname = name.replace(' ','-')\nmodel.save(f'{name}.h5')","0a3d6756":"from nltk.tokenize import sent_tokenize,word_tokenize\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nimport string\nimport tensorflow as tf\n#text feature exctraction\ndef sentences_count(sentences):\n    tokens = sent_tokenize(sentences)\n    return len(tokens)\ndef word_count(sentences):\n    tokens = word_tokenize(sentences)\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef distanctWords(sentences):\n    tokens = set(word_tokenize(sentences))\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef avg_word_count(sentences):\n    try:\n        sentences = sent_tokenize(sentences)\n        result = sum(word_count(sentence) for sentence in sentences)\/len(sentences)\n        return result\n    except:\n        return 0\ndef questionMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"\u061f\") )\ndef exclamationMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"!\") )\ndef negation(sentences):\n    negationList=['\u0644\u0646','\u0644\u0645','\u0644\u0627','\u0644\u064a\u0633','\u0644\u0633\u062a\u0645','\u0644\u064a\u0633\u062a','\u0644\u064a\u0633\u0648\u0627']\n    sentences = sent_tokenize(sentences)\n    return sum(1 for token in sentences if token in negationList)\ndef calculValues(sentences):\n    sentCont=sentences_count(sentences)\n    wordCont=word_count(sentences)\n    disWordCont=distanctWords(sentences)\n    agvWordCont=avg_word_count(sentences)\n    quesMark = questionMarks(sentences)\n    exclamationMark = exclamationMarks(sentences)\n    negate = negation(sentences)\n    # Load the BERT model and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya\/bert-base-arabic\")\n    bert_inp=bert_tokenizer.encode_plus(sentences,add_special_tokens = True\n                                            ,max_length =512,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf') \n    sent = tf.keras.models.load_model(f'.\/BERT-sentiment-analysis.h5')\n    sentiment = sent.predict([bert_inp['input_ids'],bert_inp['attention_mask']])\n    sentiment=list(sentiment[0])\n    sentiment_calc = sentiment[0] if sentiment[0] > sentiment[1] else sentiment[1]\n    return [sentCont,wordCont,disWordCont,agvWordCont,quesMark,exclamationMark,negate,sentiment.index(sentiment_calc)]\n\n","f9c9518b":"from nltk.tokenize import sent_tokenize,word_tokenize\nfrom transformers import TFAutoModel\nfrom transformers import AutoTokenizer\nimport string\nimport tensorflow as tf\n#text feature exctraction\ndef sentences_count(sentences):\n    tokens = sent_tokenize(sentences)\n    return len(tokens)\ndef word_count(sentences):\n    tokens = word_tokenize(sentences)\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef distanctWords(sentences):\n    tokens = set(word_tokenize(sentences))\n    punctuations = string.punctuation\n    return sum(1 for token in tokens if token not in punctuations)\ndef avg_word_count(sentences):\n    try:\n        sentences = sent_tokenize(sentences)\n        result = sum(word_count(sentence) for sentence in sentences)\/len(sentences)\n        return result\n    except:\n        return 0\ndef questionMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"\u061f\") )\ndef exclamationMarks(sentences):\n    return sum(1 for token in sentences if (token ==\"!\") )\ndef negation(sentences):\n    negationList=['\u0644\u0646','\u0644\u0645','\u0644\u0627','\u0644\u064a\u0633','\u0644\u0633\u062a\u0645','\u0644\u064a\u0633\u062a','\u0644\u064a\u0633\u0648\u0627']\n    sentences = sent_tokenize(sentences)\n    return sum(1 for token in sentences if token in negationList)\ndef calculValues(sentences):\n    sentCont=sentences_count(sentences)\n    wordCont=word_count(sentences)\n    disWordCont=distanctWords(sentences)\n    agvWordCont=avg_word_count(sentences)\n    quesMark = questionMarks(sentences)\n    exclamationMark = exclamationMarks(sentences)\n    negate = negation(sentences)\n    # Load the BERT model and the pretrained Arabic base BERT model\n    bert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya\/bert-base-arabic\")\n    bert_inp=bert_tokenizer.encode_plus(sentences,add_special_tokens = True\n                                            ,max_length =512,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf') \n    sent = tf.keras.models.load_model(f'.\/BERT-sentiment-analysis.h5')\n    sentiment = sent.predict([bert_inp['input_ids'],bert_inp['attention_mask']])\n    sentiment=list(sentiment[0])\n    sentiment_calc = sentiment[0] if sentiment[0] > sentiment[1] else sentiment[1]\n    return [sentCont,wordCont,disWordCont,agvWordCont,quesMark,exclamationMark,negate,sentiment.index(sentiment_calc)]\n","a5cf590f":"import pandas as pd\nimport warnings\nwarnings.simplefilter(\"ignore\")\n#import data from csv file\ndata = pd.read_csv('..\/input\/covidremors\/rumours_dataset.csv')\ndata.head()","3a77be40":"#drop unnecery rows\ndata = data.drop([\"post url\",\"post page\",\"post date\",\"likes\",\"shares number\",\"comments\"], axis=1)\ndata.head()","c7b98293":"data['post text']=data['post text'].fillna('\u0628\u062f\u0648\u0646 \u0646\u0635')\ndata[data['post text']=='\u0628\u062f\u0648\u0646 \u0646\u0635']","2e930701":"data['type'] = data['type'].apply(lambda x: 1 if x=='VRAI' else 0)\ndata.head()","77e455a8":"error=0\ninfos = []\ncols = ['sentencesCount', 'wordCount','distanctWords','avg_word_count','questionMarks','exclamationMarks','negation','sentiment']\nfor i, row in data.iterrows():\n    print(i)\n    try:\n        features = calculValues(row['post text'])\n        infos.append(features)\n    except Exception as e:\n        print(e)\ndf = pd.DataFrame(infos,columns =cols)\ndata1 = pd.concat([data, df], axis=1, join=\"inner\")\n","0c5f13e6":"data1.head()","dad664f6":"# Load the BERT model and the pretrained Arabic base BERT model\nmodel1 = TFAutoModel.from_pretrained(\"asafaya\/bert-base-arabic\")\n\nmodel1.trainable = False  #freeze the weight\n\ninput_ids = layers.Input(shape=(512,), dtype=tf.int32, name='input_ids')\nattention_mask = layers.Input(shape=(512,), dtype=tf.int32, name='attention_mask')\nfeatures = layers.Input(shape=(8,), dtype=tf.float32, name='features1')\nembeddings = model1.bert(\n            input_ids, attention_mask=attention_mask\n        )[1]\nx = tf.keras.layers.Concatenate()([embeddings, features])\nx = tf.keras.layers.Dense(599, activation='relu')(x)\n#output layer\ny = tf.keras.layers.Dense(2, activation='softmax', name='outputs')(x)\nmodel1 = tf.keras.Model(inputs=[input_ids, attention_mask,features], outputs=y)\nmodel1.summary()\n# get the input and the output shapes of the BERT model\nfrom keras.models import Model\nBERT_model = Model(model1.input, model1.output)","3e121699":"split = int((80*len(data))\/100)\nfrom sklearn.utils import shuffle\n\ndata1 = shuffle(data1)\n#train \ntrain = data1.iloc[:split,:]\ntest = data1.iloc[split:,:]\nimages_train = images=create_dataset(train)\nimages_test = images=create_dataset(test)\ntrain = train.drop(['post image'],axis=1)","4f85f313":"train","0f046f49":"test","cd8c08b1":"import cv2\nimport numpy as np\nfrom matplotlib import pyplot as plt\ndef get_pixel(img, center, x, y):\n      \n    new_value = 0\n      \n    try:\n        # If local neighbourhood pixel \n        # value is greater than or equal\n        # to center pixel values then \n        # set it to 1\n        if img[x][y] >= center:\n            new_value = 1      \n    except:\n        pass\n      \n    return new_value\n   \n# Function for calculating LBP\ndef lbp_calculated_pixel(img, x, y):\n   \n    center = img[x][y]\n   \n    val_ar = []\n      \n    # top_left\n    val_ar.append(get_pixel(img, center, x-1, y-1))\n      \n    # top\n    val_ar.append(get_pixel(img, center, x-1, y))\n      \n    # top_right\n    val_ar.append(get_pixel(img, center, x-1, y + 1))\n      \n    # right\n    val_ar.append(get_pixel(img, center, x, y + 1))\n      \n    # bottom_right\n    val_ar.append(get_pixel(img, center, x + 1, y + 1))\n      \n    # bottom\n    val_ar.append(get_pixel(img, center, x + 1, y))\n      \n    # bottom_left\n    val_ar.append(get_pixel(img, center, x + 1, y-1))\n      \n    # left\n    val_ar.append(get_pixel(img, center, x, y-1))\n       \n    # Now, we need to convert binary\n    # values to decimal\n    power_val = [1, 2, 4, 8, 16, 32, 64, 128]\n    val = 0\n    for i in range(len(val_ar)):\n        val += val_ar[i] * power_val[i]    \n    return val\n\ndef LBP(img_bgr):\n    height, width, _ = img_bgr.shape\n    # We need to convert RGB image \n    # into gray one because gray \n    # image has one channel only.\n    img_gray = cv2.cvtColor(img_bgr,cv2.COLOR_BGR2GRAY)\n\n    # Create a numpy array as \n    # the same height and width \n    # of RGB image\n    img_lbp = np.zeros((height, width),\n                       np.uint8)\n\n    for i in range(0, height):\n        for j in range(0, width):\n            img_lbp[i, j] = lbp_calculated_pixel(img_gray, i, j)\n    return img_lbp\n\ndef divide_image_by_block(image):\n    blocks=[]\n    img_x=image.shape[0]\n    img_y=image.shape[1]\n    for x in range(img_x):\n        for y in range(img_y):\n            blocks.append(image[x:x+16][y:y+16])\n    return blocks","9bba5b2e":"from scipy.fftpack import dct\ndef ddt_for_blocks(image):\n    dct_vectors=[]\n    for x in range(16):\n        dct_vectors.append(dct(dct(image[x]).T).T)\n    return np.array(dct_vectors)\ndef ddt_matrx_std(image):\n    image = LBP(image)\n    image = divide_image_by_block(image)\n    image = ddt_for_blocks(image)\n    ddt_std=[]\n    for x in range(16):\n        ddt_std.append(image[x].std())\n    return ddt_std","38ad1515":"import cv2\nimport numpy as np\nimport os\n#load images from the disk\ndef create_dataset(data):\n    #importing images from the disk\n    path=\"..\/input\/covidremors\/\"\n    img_data_array=[]\n    class_name=[]\n    #new height and width of images\n    IMG_HEIGHT, IMG_WIDTH = 224,224\n    #iterate throw csv file to get the name of images\n    for index,file in data[['post image']].iterrows():\n        #get image path\n        img_path = data.loc[index,['post image']].values[0]\n        image_path= os.path.join(path,img_path)\n        #read image via opencv\n        image= cv2.imread(image_path, cv2.COLOR_BGR2RGB)\n        #resize the image into IMG_HEIGHT*IMG_WIDTH*3\n        image=cv2.resize(image, (IMG_HEIGHT, IMG_WIDTH ),interpolation = cv2.INTER_AREA)\n        #convert image to numpy array\n        image=np.array(image)\n        #applying DDT on images\n        ddt_std = ddt_matrx_std(image)\n        #convert image to float32\n        image = image.astype('float32')\n        #normalise the image via deviding all the values by 255\n        image \/= 255\n        img_data_array.append(image)\n        #append image to class_name\n        class_name.append(data.loc[index,['type']].values[0])\n    return np.array(img_data_array)\n\n","9847de66":"from keras.models import Model\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, Flatten,GlobalAveragePooling2D\n#load pretrained ResNet152V2\nResNet152V2 = tf.keras.applications.ResNet152V2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n#freewe the weights \nResNet152V2.trainable = False\n# add custom output layers for ResNet\n#get the output of the model\nx = ResNet152V2.output\nx = GlobalAveragePooling2D()(x)\nx = Flatten()(x)\nx = Dense(units=117, activation='relu')(x)\nx = Dense(units=1080, activation='relu')(x)\nx = Dropout(0.7)(x)\nResNet_output  = Dense(units=2, activation='softmax')(x)\nResNet_model = Model(ResNet152V2.input, ResNet_output)","9421ceb7":"from keras.layers.merge import concatenate\n#concatenate the outputs of the 2 models\nmerge_layer = concatenate([ResNet_output, BERT_model.output], name='Concatenate')\nout = Dense(1048, activation='relu', name='output_layer')(merge_layer)\nout = Dropout(0.5)(out)\nfinal_model_output = Dense(2, activation='sigmoid',name=\"final_layer\")(out)\nmerged_model = Model([ResNet152V2.input, BERT_model.input], outputs=final_model_output)\nmerged_model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.9999999, epsilon=1e-06, decay=0.1),\n                     loss = \"sparse_categorical_crossentropy\" ,\n                     metrics=[\"accuracy\"])\n","b1f15c82":"from transformers import AutoTokenizer, AutoModel,TFBertForSequenceClassification\nimport numpy as np\nfrom transformers import TFAutoModel\nfrom tensorflow.keras import layers\nimport tensorflow as tf\nfrom transformers import AutoTokenizer, AutoModel\n# Load the BERT tokenizer and the pretrained Arabic base BERT model\nbert_tokenizer = AutoTokenizer.from_pretrained(\"asafaya\/bert-base-arabic\")\n# Load the BERT model and the pretrained Arabic base BERT model\nmodel = TFAutoModel.from_pretrained(\"asafaya\/bert-base-arabic\")\n#get the liste of sentences\n\"\"\"\nWhen feeding text data into our model, there are a few things to \nbe aware of. First, we must use tokenizer.encode_plus(...) \nto convert our text into input IDs and attention mask tensors\n\"\"\"\nimport tensorflow as tf\n\ndef tokenize_bert(data):\n    sentences = data['post text'].values\n    seq_len = 512\n    num_samples = len(sentences)\n    #lists of ids\n    input_ids = np.zeros((num_samples, seq_len))\n    #get the lists of masks\n    attention_masks = np.zeros((num_samples, seq_len))\n    for i,sent in enumerate(sentences):\n        #tokenize words in the sentence\n        #return_tensors: return the resultat as tensor\n        #pad_to_max_length :pads the sentences to max length\n        bert_inp=bert_tokenizer.encode_plus(sent,add_special_tokens = True\n                                            ,max_length =seq_len,pad_to_max_length = True,\n                                             truncation=True\n                                            ,return_attention_mask = True,\n                                             return_tensors='tf')  # Return pytorch tensors.\n        #add encoded \n        input_ids[i, :] = bert_inp['input_ids']\n        #applying attention masks\n        attention_masks[i, :] = bert_inp['attention_mask']\n    return input_ids,attention_masks","5648f1bc":"split = int((80*len(data))\/100)\nfrom sklearn.utils import shuffle\n\ndata1 = shuffle(data1)\n#train \ntrain = data1.iloc[:split,:]\ntest = data1.iloc[split:,:]\nimages_train =create_dataset(train)\nimages_test = images=create_dataset(test)\ntrain = train.drop(['post image'],axis=1)","32c561ec":"train","314fb039":"train_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask = tokenize_bert(test)","7ba3aae1":"train_inp,train_mask = tokenize_bert(train)\ntest_inp,test_mask= tokenize_bert(test)\nlabels_train = train[['type']]\nlabels_test = test[['type']]\n\nmerged_model.compile(optimizer=optimizer,loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\ntrain_x = [np.array(images_train),[train_inp,train_mask, train.iloc[:,2:]  ]]\nmerged_model.fit(train_x,labels_train, epochs=30, steps_per_epoch=50)","c7edbfaf":"test_lost ,test_acc_model = merged_model.evaluate(\n[np.array(images_test), [test_inp,test_mask, test.iloc[:,3:].values ]  ],labels_test\n)\nprint(\"The accuracy of the model model is:\",(test_acc_model*100))","85d46977":"def get_metrics(y_pred,y_list):\n    FP,FN,TP,TN=0,0,0,0\n    for i in range(len(y_list)):\n        if y_pred[i]==y_list[i]:\n            if y_pred[i]==1:\n                TP+=1\n            else:\n                TN+=1\n        else:\n            if y_pred[i]==0:\n                FP+=1 \n            else: \n                FN+=1\n    return FP,FN,TP,TN\ndef accuracy(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return (TP+TN)\/len(y_list)\ndef recall(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return TP\/(TP+FN)\ndef precision(y_pred,y_list):\n    FP,FN,TP,TN=get_metrics(y_pred,y_list)\n    return TP\/(TP+FP)\ndef f1_score(y_pred,y_list):\n    prec,rec=recall(y_pred,y_list),precision(y_pred,y_list)\n    return (2*prec*rec)\/(prec+rec)\n\n\ny_list = list( labels_test['type'] )\n\npredicted = merged_model.predict(  [np.array(images_test), [test_inp,test_mask, test.iloc[:,3:].values ]  ] )\ny_pred = list(predicted)\ny_pred = [ list(i).index(max(i[0],i[1])) for i in predicted ]\nprint(accuracy(y_pred,y_list))\nprint(recall(y_pred,y_list))\nprint(precision(y_pred,y_list))\nprint(f1_score(y_pred,y_list))","790ac90f":"## define the DDT","d005d180":"## calcul the local binary pattern","ba30bb9c":"# Rumors analysis","47604cae":"# Encode output","12115fdb":"# images processing"}}