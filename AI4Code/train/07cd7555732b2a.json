{"cell_type":{"ac2cb08b":"code","95e5d2a8":"code","4362dc5b":"code","c40422bd":"code","279cf15d":"code","e79a9dda":"code","8e0d2a61":"code","54518cf0":"code","03dc3eb5":"code","7bd0e8a4":"code","f02d7f26":"code","25ac8d76":"code","dd67360a":"code","2ed506a4":"code","deb5096b":"code","eb29357d":"code","e4d24a87":"code","14cf0636":"code","341a2f9c":"code","ad7a5528":"code","f6700139":"code","e06dad77":"code","3ce42d94":"code","571e1060":"code","c5e96166":"code","2a8231cc":"code","69f8579e":"code","251638d8":"code","b0d2f662":"code","34c9d9a1":"code","9c0d3558":"code","4b748bc1":"code","330b8525":"code","920d9c55":"code","e812e560":"code","fa2e57c6":"code","97dafea1":"code","951a6446":"code","a2f839f2":"code","ea7b8c71":"code","42949bc9":"code","4ebf1abc":"markdown","0e2e2cb8":"markdown","0feb4af3":"markdown","c8386232":"markdown","9c5cdad7":"markdown","b7f63940":"markdown","3e77c94b":"markdown","8b505116":"markdown","1d7515d9":"markdown","ef5aa019":"markdown","3eaaedc6":"markdown","8775a253":"markdown","3dc2f937":"markdown","c76ccc32":"markdown","71f93b0b":"markdown"},"source":{"ac2cb08b":"# libraries\nimport random\nimport os, math\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom catboost import CatBoostClassifier\n# Finding the weighting coefficients for each of the fold-models\n\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import SimpleImputer, IterativeImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder","95e5d2a8":"# Data: train and test\ndf_train = pd.read_csv(\"\/kaggle\/input\/netflix-appetency\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/netflix-appetency\/test.csv\")","4362dc5b":"df_train.head()","c40422bd":"df_train.feature_19.value_counts()","279cf15d":"N = df_train.shape[0]\n\ndrop_features = []\nfor col in df_train.columns:\n    if df_train[col].dtype == \"object\":\n        vcs = df_train[col].value_counts().to_dict()\n        r1 = sorted(list(vcs.values()), reverse=True)[0]\/N\n        try:\n            r2 = sorted(list(vcs.values()), reverse=True)[1]\/N\n        except IndexError:\n            drop_features.append(col)\n            print(f\"{col} has only one category!\")\n            continue\n        \n        # single category > 70% but second highest category very small\n        if r1 > 0.7 and r2 < 0.001:\n            drop_features.append(col)\n            print(col, round(r1, 2), round(r2, 2))","e79a9dda":"# Dropping features found above\ndf_train = df_train.drop(columns=drop_features)\ndf_test = df_test.drop(columns=drop_features)","8e0d2a61":"drop_features = []\nfor col in df_train.columns:\n    if df_train[col].isnull().sum()\/df_train.shape[0] > 0.20:\n        drop_features.append(col)\nprint(\"Features dropped with more than 20% missing values: \", len(drop_features))","54518cf0":"df_train = df_train.drop(columns=drop_features)\ndf_test = df_test.drop(columns=drop_features)","03dc3eb5":"drop_features = []\nfor f in df_train.columns:\n    if df_train[f].nunique() == 1:\n        drop_features.append(f)","7bd0e8a4":"df_train = df_train.drop(columns=drop_features)\ndf_test = df_test.drop(columns=drop_features)","f02d7f26":"# datetime related features\ndatetime_features = [\"feature_191\", \"feature_192\",  \"feature_199\", \"feature_200\", \"feature_201\"]","25ac8d76":"def return_day(string):\n    string = str(string).strip()\n    if string == 'nan':\n        return np.nan\n    return int(string.split(\"\/\")[0])\n\ndef return_month(string):\n    string = str(string).strip()\n    if string == 'nan':\n        return np.nan\n    return int(string.split(\"\/\")[1])\n\n# datetime --> month and day features\nfor f in datetime_features:\n    df_train[f+\"_month\"] = df_train[f].apply(lambda x: return_month(x))\n    df_train[f+\"_day\"] = df_train[f].apply(lambda x: return_day(x))\n    \n    df_test[f+\"_month\"] = df_test[f].apply(lambda x: return_month(x))\n    df_test[f+\"_day\"] = df_test[f].apply(lambda x: return_day(x))","dd67360a":"df_train = df_train.drop(columns=datetime_features)\ndf_test = df_test.drop(columns=datetime_features)","2ed506a4":"categorical_features = []\nnumerical_features = []\nfor k, v in df_train.dtypes.to_dict().items():\n    if v == 'object' or \"_month\" in k:\n        categorical_features.append(k)\n    else:\n        numerical_features.append(k)\ncategorical_features = np.array(categorical_features)\nnumerical_features = np.array(numerical_features)","deb5096b":"# Number of features\nlen(categorical_features), len(numerical_features)","eb29357d":"# Impute train and test dfs\ndef impute_df(df_train, df_test):\n    for col in tqdm(df_train.columns):\n        if \"feature\" not in col: continue\n\n        if col in numerical_features:\n            impute = SimpleImputer(strategy=\"mean\")\n            df_train[col] =  impute.fit_transform(df_train[col].values.reshape(-1, 1)).flatten()\n            df_test[col] =  impute.transform(df_test[col].values.reshape(-1, 1)).flatten()\n\n        if col in categorical_features:\n            impute = SimpleImputer(strategy=\"most_frequent\")\n            df_train[col] =  impute.fit_transform(df_train[col].values.reshape(-1, 1)).flatten()\n            df_test[col] =  impute.transform(df_test[col].values.reshape(-1, 1)).flatten()\n            \n    return df_train, df_test","e4d24a87":"train, test = impute_df(df_train, df_test)","14cf0636":"train.head()","341a2f9c":"high_cardinality_features = []\nprint(\"Feature\", \"  \",\"Cardinality\")\nfor c in categorical_features:\n    ans = np.concatenate((train[c].values, test[c].values))\n    count = train[c].nunique()\n    if count > 50:\n        print(c, \"==>\", count)\n        high_cardinality_features.append(c)","ad7a5528":"_features = [\"feature_18\", \"feature_20\", \"feature_133\"]\nN = train.shape[0]\n\nfeature_low_cat_dict = {}\nfor f in _features:\n    _dict = train[f].value_counts().to_dict()\n    cat_list = []\n    for k, v in _dict.items():\n        if v < 5:\n            cat_list.append(k)\n    feature_low_cat_dict[f] = cat_list","f6700139":"# Creating \"Other\" category for low freq categories in _features\nfor f in _features:\n    train[f] = train[f].apply(lambda x: \"OTHER\" if x in feature_low_cat_dict[f] else x)","e06dad77":"# frequency of category in train data for high cardinality features\ntrain_cardinality_dict = {}\nfor f in high_cardinality_features:\n    cat_freqs = train[f].value_counts().to_dict()\n    train_cardinality_dict[f] = cat_freqs","3ce42d94":"def return_train_cardinality(x, feature):\n    if x in train_cardinality_dict[feature].keys():\n        return train_cardinality_dict[feature][x]\n    # new category only in test set\n    else:\n        return 0","571e1060":"# Finally, transformation of the high cardinality features\nfor c in high_cardinality_features:\n    train[c] = train[c].apply(lambda x: return_train_cardinality(x, c))\n    test[c] = test[c].apply(lambda x: return_train_cardinality(x, c))","c5e96166":"drop_features = []\nfor col in train.columns:\n    if \"_day\" in col:\n        train[col+\"_sin\"] = train[col].apply(lambda x: math.sin(2*math.pi*x\/30))\n        train[col+\"_cos\"] = train[col].apply(lambda x: math.cos(2*math.pi*x\/30))\n        test[col+\"_sin\"] = test[col].apply(lambda x: math.sin(2*math.pi*x\/30))\n        test[col+\"_cos\"] = test[col].apply(lambda x: math.cos(2*math.pi*x\/30))\n        \n        drop_features.append(col)","2a8231cc":"train = train.drop(columns=drop_features)\ntest = test.drop(columns=drop_features)","69f8579e":"# categorical features left\ncat_features = []\nfor f in train.columns:\n    if train[f].dtype == \"object\":\n        cat_features.append(f)","251638d8":"# Label Encoder\nfor col in cat_features:\n    encoder = LabelEncoder()\n    encoder = encoder.fit(np.concatenate((train[col].values, test[col].values)))\n    train[col] = encoder.transform(train[col].values)\n    test[col] = encoder.transform(test[col].values)","b0d2f662":"# calculating correlation of features with target\nfrom scipy.stats import pearsonr\n\nk = 0\nlow_corr_features = []\nfor col in train.columns:\n    if col == \"id\" or col == \"target\":\n        continue\n    try:\n        cor = abs(pearsonr(train[col].values, train.target.values)[0])\n    except:\n        continue\n    if cor < 0.0002:\n        low_corr_features.append(col)\n        k += 1\nprint(f\"Features with low correlation with target: \\n\", np.array(low_corr_features))","34c9d9a1":"train = train.drop(columns=low_corr_features)\ntest = test.drop(columns=low_corr_features)","9c0d3558":"# final set of numerical and categorical features\nfinal_cat_features = []\nfinal_num_features = []\nfor f in train.columns:\n    if f == \"id\" or f == \"target\":\n        continue\n    elif f in cat_features or \"_month\" in f:\n        if f not in high_cardinality_features:\n            final_cat_features.append(f)\n        else:\n            final_num_features.append(f)\n    else:\n        final_num_features.append(f)","4b748bc1":"# merged features list\nfeatures = []\nfeatures.extend(final_num_features)\nfeatures.extend(final_cat_features)","330b8525":"# categorical features into int type\nfor f in final_cat_features:\n    train[f] = train[f].astype(int)\n    test[f] = test[f].astype(int)","920d9c55":"print(\"Total number of features: \", len(features))","e812e560":"# Test data\nX_test = test[features]","fa2e57c6":"# Training-validation data\nX = train[features]\ny = train.target","97dafea1":"# categorical features indices\ncat_indices = []\nfor f in final_cat_features:\n    if f in X.columns:\n        idx = list(X.columns).index(f)\n        cat_indices.append(idx)\nnp.array(cat_indices)","951a6446":"# scale positive weight\nscale = round(y.value_counts()[0]\/y.value_counts()[1], 2)\nscale","a2f839f2":"# k-fold cross-validation and oof predictions\n\nNFOLDS = 5\nst_kfolds = StratifiedKFold(n_splits=NFOLDS, shuffle=True)\n\n\ncat_params = {\n                'subsample': 0.50,\n                'learning_rate': 0.05,\n                \"max_depth\": 6,\n                \"min_data_in_leaf\": 70,\n                \"colsample_bylevel\": 0.4,\n                \"scale_pos_weight\": scale,\n                'random_state':42,\n                'num_boost_round': 2000,\n                'l2_leaf_reg': 2.5,\n                'eval_metric': \"AUC\",\n                }\n\npredictions = np.zeros(len(X_test))\n\nmean_cv_score = 0\nfor fold, (tr_idx, val_idx) in enumerate(st_kfolds.split(X, y)):\n    if fold > 0:\n        print(\"=\"*65)\n    print(\"FOLD: \", fold)\n    \n    X_train, y_train = X.iloc[tr_idx], y.iloc[tr_idx]\n    X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n    \n    # Model with parameters\n    model = CatBoostClassifier(**cat_params)\n    model.fit(\n        X_train, y_train, \n        cat_features=cat_indices,\n        eval_set=[(X_train, y_train), (X_valid, y_valid)],\n        early_stopping_rounds=200,\n        verbose=200\n    )\n\n    # validation data:\n    valid_preds = model.predict_proba(X_valid)[:, 1]\n    cv_score = roc_auc_score(y_valid, valid_preds)\n    print(\"Validation AUC: \", cv_score)\n    mean_cv_score += cv_score\/NFOLDS\n    \n    predictions += model.predict_proba(X_test)[:, 1]\/NFOLDS\n\n# mean cv score\nprint(\"=\"*40)\nprint(\"Mean CV score: \", mean_cv_score)\nprint(\"=\"*40)","ea7b8c71":"submission = pd.DataFrame(data={\"id\": test.id.values, \"target\": predictions})\nsubmission.head()","42949bc9":"submission.to_csv(\"submission.csv\", index=False)","4ebf1abc":"## **<span style=\"color:#e76f51;\">Final Features<\/span>**","0e2e2cb8":"## **<span style=\"color:#e76f51;\">DataTime Features<\/span>**","0feb4af3":"### Low correation of feature with target\n- Dropping some very low correlated features with target?","c8386232":"## **<span style=\"color:#e76f51;\">Missing Value Imputation<\/span>**","9c5cdad7":"## **<span style=\"color:#e76f51;\">Label Encoder: Categorical features<\/span>**","b7f63940":"## **<span style=\"color:#e76f51;\">MODEL & Training<\/span>**","3e77c94b":"## **<span style=\"color:#e76f51;\">Missing Values in the Features<\/span>**\n\n- If more than 50% of the records have missing value, drop the feature","8b505116":"**Some features have very high cardinality > 5000** \n\n- We can further look into details of such features, and reduce cardinality if possible.\n- For categories with very small frequency - single frequency --> we can merge them as a separate \"OTHER\" category","1d7515d9":"## **<span style=\"color:#e76f51;\">High Cardinality Features<\/span>**\n- **Strategy:** replace very high cardinality categorical features with their frequencies.","ef5aa019":"## **<span style=\"color:#e76f51;\">Drop features with only 1 unique value<\/span>**\n\n- Categorical or numerical features with only 1 unique values, not important for classification model - drop them","3eaaedc6":"- A great dataset to work with.\n- Excellent opportunities to learn many aspects of building a prediction model with real world data.\n- Infinite ideas to test.\n\n\ud83c\udfc3 \ud83d\udd1b","8775a253":"# **<span style=\"color:#e76f51;\">Netflix Appetency<\/span>**\n\n**Objective of this contest:**\n- Classify consumers according to their appetite to subscribe to Netflix.\n- For reasons of confidentiality, the data is anonymized and augmented.\n- **A lot of possibilities: this dataset has almost everything we can see in a real world data - many categorical variables, missing values, datetime features, features with abnormal distributions, features requiring scaling and transformaions,  and many more.** \n\n- **[CV: 0.784, LB: 0.786]**","3dc2f937":"## **<span style=\"color:#e76f51;\">Numerical & Categorical Features<\/span>**","c76ccc32":"## **<span style=\"color:#e76f51;\">Creating DateTime features<\/span>**\n\n- Create sin and cos features with day and month? Ignore year for now.\n- Drop the original datetime feature columns","71f93b0b":"## **<span style=\"color:#e76f51;\">High Cardinality Features<\/span>**\n**(But mostly single category)**\n- Feature with very high number of categories, but only a few categories have high cardinality.\n- Drop such features"}}