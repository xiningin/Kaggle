{"cell_type":{"a5785640":"code","55b6413d":"code","e929cded":"code","a6ad426b":"code","3a2687e9":"code","9eb33836":"code","27e7f63d":"code","cc0629f7":"code","c14f59f4":"code","d688fe75":"code","d47851d9":"code","0f6d288e":"code","5171cc78":"code","8536f820":"code","0e573c9e":"code","07f5603f":"code","e0388908":"code","225c01c9":"code","015327fd":"code","d57cbb15":"code","fb39d210":"code","ce2f1941":"code","ecdd7739":"code","8c14d03f":"code","b1acfc17":"code","a1a30ce3":"code","3d9c5e5b":"code","8ebcb191":"code","8c18e591":"code","358d1082":"code","d23a60aa":"code","e66d98f0":"code","573e14f6":"code","64132754":"code","d24264d9":"code","ff53908a":"code","db55f57a":"code","31f2d3f8":"code","a7dab4f8":"code","dcc2ae9e":"code","d0a268fd":"markdown","2c23b77d":"markdown","91f8f69e":"markdown","a9ef5523":"markdown","0e969e05":"markdown","832989ca":"markdown","bc97e76e":"markdown","710f100c":"markdown","8f955721":"markdown","771578b1":"markdown","d4c9ada4":"markdown","f3974764":"markdown","cc6f7cb6":"markdown","f9e32075":"markdown","d12fc7c1":"markdown","f9342123":"markdown","8fe75cc8":"markdown","403dbe0a":"markdown","d9728690":"markdown","c9e7e6ea":"markdown","7232a291":"markdown","e7763796":"markdown","6ff27bc8":"markdown","ef26e520":"markdown","4dce2904":"markdown","21082662":"markdown","346fc15c":"markdown","d144c1b4":"markdown","d61be284":"markdown","fa7d9d5a":"markdown"},"source":{"a5785640":"import os # accessing directory structure\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","55b6413d":"import numpy as np \nimport pandas as pd \nimport pandas_profiling\nimport pandas_summary as ps\nimport shap\n\n\n# Data processing, metrics and modeling\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.decomposition import PCA\nimport collections\n\n# Lgbm\nimport lightgbm as lgb\n\n# Hyper_opt\nfrom hyperopt import hp\nfrom hyperopt import fmin, tpe, Trials, space_eval, STATUS_OK, STATUS_RUNNING\nimport hyperopt.pyll\nfrom hyperopt.pyll import scope\n\n# Suppr warning\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Plots\nimport matplotlib\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom matplotlib import rcParams\n\n# Others\nimport shap\nimport datetime\nfrom tqdm import tqdm_notebook\nimport sys\nimport pickle\nimport re\nimport json\n\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 1000)\npd.set_option('display.width', 1000)\npd.set_option('use_inf_as_na', True)\n\nwarnings.simplefilter('ignore')\nmatplotlib.rcParams['figure.dpi'] = 100\nsns.set()\n%matplotlib inline","e929cded":"def reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","a6ad426b":"train_df = reduce_mem_usage(pd.read_csv('..\/input\/regression-with-categorical-data\/train.csv'))\ntest_df = reduce_mem_usage(pd.read_csv('..\/input\/regression-with-categorical-data\/test.csv'))","3a2687e9":"train_df.shape, test_df.shape","9eb33836":"train_df.head()","27e7f63d":"train_df.tail()","cc0629f7":"test_df.head()","c14f59f4":"test_df.tail()","d688fe75":"dfs = ps.DataFrameSummary(train_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","d47851d9":"dfs = ps.DataFrameSummary(test_df)\nprint('categoricals: ', dfs.categoricals.tolist())\nprint('numerics: ', dfs.numerics.tolist())\ndfs.summary()","0f6d288e":"train_df['target'].hist();","5171cc78":"train_df[train_df['target']>25000]['target'].hist();","8536f820":"train_df['target'] = np.log1p(train_df['target'])","0e573c9e":"train_df['target'].hist();","07f5603f":"y = train_df['target']\ntrain_df.drop('target', axis=1, inplace=True)","e0388908":"train_df.drop('id', axis=1, inplace=True)\ntest_df.drop('id', axis=1, inplace=True)","225c01c9":"train_df.columns.tolist() == test_df.columns.tolist()","015327fd":"pandas_profiling.ProfileReport(train_df)","d57cbb15":"pandas_profiling.ProfileReport(test_df)","fb39d210":"cat = train_df.select_dtypes(include=['category']).columns.tolist()","ce2f1941":"X_train, X_val, y_train, y_val = train_test_split(train_df, y, test_size=0.25, random_state=777)","ecdd7739":"%%time\n# \u0427\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 P, \u0442\u0435\u043c \u043c\u0435\u043d\u044c\u0448\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0448\u0442\u0440\u0430\u0444\u043e\u0432\u0430\u0442\u044c \u0437\u0430 \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 train  \u0438 test\np = 0.8\n# k - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nk = 30\n\nskf = KFold(n_splits=3, shuffle=True, random_state=7)\n\ndef score(params):\n    print('Training with params:')\n    print(params)\n    w=[]\n    best_iter = []\n    \n\n    for train_index, val_index in skf.split(X_train, y_train):\n        x_train_1, x_valid_1 = X_train.iloc[train_index, :], X_train.iloc[val_index, :]\n        y_train_1, y_valid_1 = y_train.iloc[train_index], y_train.iloc[val_index]\n        train_data = lgb.Dataset(x_train_1, label=y_train_1, categorical_feature=cat)\n        val_data = lgb.Dataset(x_valid_1, label=y_valid_1, categorical_feature=cat, reference=train_data)\n        gbm = lgb.train(params,\n                        train_data,\n                        valid_sets = [train_data, val_data],\n                        valid_names=['train', 'val'],\n                        num_boost_round = 5000,\n                        verbose_eval = False, \n                        categorical_feature=cat\n                       )\n        w.append([gbm.best_score['train']['rmse'], gbm.best_score['val']['rmse']])\n        best_iter.append(gbm.best_iteration)\n    nrounds = np.mean(best_iter)\n    print('best iter:', int(nrounds), 'all iter:', best_iter)\n    res = list(np.mean(w, axis=0))\n    print(\"\\t rmse train {0}, rmse test {1}, dif {2}, \\n\\t final score {3} \\n\\n\".format(res[0], res[1], np.power(np.square(res[0]-res[1]), p), +res[1]+np.power(np.square(res[0]-res[1]), p)))\n    return {'loss': +res[1]+np.power(np.square(res[0]-res[1]), p), 'status': STATUS_OK, \n            'mean_rmse_train': res[0], 'mean_rmse_test': res[1], 'best_iter': int(nrounds)}\n\ndef optimize(trials):\n    space = {\n        #'max_depth': hp.choice('max_depth', [-1, 6, 7]),\n        'max_depth': -1,\n        'max_bin': scope.int(hp.uniform('max_bin', 100, 2500)),\n        'num_leaves': scope.int(hp.uniform('num_leaves', 20, 200)),\n        'lambda_l1': hp.quniform('lambda_l1', 0, 8, 0.25),\n        'learning_rate': hp.quniform('learning_rate', 0.01 , 0.05, 0.005),\n        'bagging_fraction': hp.quniform('bagging_fraction', 0.3, 0.9, 0.1),\n        'metric': ('rmse',),\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'nthread': 8,\n        'early_stopping_rounds': 20,\n        'silent':1,\n    }\n    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=k)\n    print(best)\n\ntrials = Trials()\noptimize(trials)","8c14d03f":"params = trials.best_trial['misc']['vals']\nparams['max_depth'] = -1\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = ('l1', 'l2')\nparams['nthread'] = 8\nparams['early_stopping_rounds'] = 100\nparams['silent'] = 1\nparams['num_leaves'] = int(params['num_leaves'][0])\nparams['max_bin'] = int(params['max_bin'][0])\nparams","b1acfc17":"train_ds = lgb.Dataset(X_train, label=y_train, categorical_feature=cat, )\nval_ds = lgb.Dataset(X_val, label=y_val, categorical_feature=cat, reference=train_ds)","a1a30ce3":"%%time\nbooster = lgb.train(params, train_ds, num_boost_round=10000, valid_sets=[train_ds, val_ds], valid_names=['train', 'valid'], verbose_eval=100, categorical_feature=cat, early_stopping_rounds=100)","3d9c5e5b":"gain = booster.feature_importance(importance_type='gain')\ntotal = sum(gain)\ntmp = pd.DataFrame({'Name': X_train.columns.tolist(), 'Value': gain\/total})\ntmp = tmp.sort_values('Value', ascending=False)\ntmp.index = range(1, tmp.shape[0]+1)\ntmp.head(10)","8ebcb191":"%%time\ni = 0\nf = pd.DataFrame(columns=['Number_of_cols', 'rmse_test', 'rmse_train', 'rmse_diff'])\nk = tmp.shape[0]\nwhile k > 10:\n    columns = list(tmp.loc[:k, 'Name'])\n    w=[]\n    best_iter = []\n    for train_index, val_index in skf.split(X_train, y_train):\n        x_train_2, x_valid_2 = X_train.loc[train_index, columns], X_train.loc[val_index, columns]\n        y_train_2, y_valid_2 = y_train.iloc[train_index], y_train.iloc[val_index]\n        cat_2 = x_train_2.select_dtypes(include=['category']).columns.tolist()\n        train_data_2 = lgb.Dataset(x_train_2, label=y_train_2, categorical_feature=cat_2)\n        val_data_2 = lgb.Dataset(x_valid_2, label=y_valid_2, categorical_feature=cat_2, reference=train_data_2)\n        gbm_2 = lgb.train(params,\n                        train_data_2,\n                        valid_sets = [train_data_2, val_data_2],\n                        valid_names=['train', 'val'],\n                        num_boost_round = 5000,\n                        verbose_eval = False, \n                        categorical_feature=cat_2\n                       )\n        w.append([gbm_2.best_score['train']['l1'], gbm_2.best_score['val']['l1']])\n        best_iter.append(gbm_2.best_iteration)\n    nrounds = np.mean(best_iter)\n    res = list(np.mean(w, axis=0))\n    \n    rmse_test = res[0]\n    rmse_train = res[1]\n    f.loc[i, :] = k, rmse_test, rmse_train, rmse_test - rmse_train\n    print('n columns ', k, 'rmse_test ', rmse_test, 'rmse_train ', rmse_train, 'diff ', rmse_train - rmse_test)\n    i+=1\n    k-=5","8c18e591":"tmp.head(55)","358d1082":"column = tmp.head(55)['Name'].tolist()","d23a60aa":"%%time\n# \u0427\u0435\u043c \u0431\u043e\u043b\u044c\u0448\u0435 P, \u0442\u0435\u043c \u043c\u0435\u043d\u044c\u0448\u0435 \u043c\u044b \u0445\u043e\u0442\u0438\u043c \u0448\u0442\u0440\u0430\u0444\u043e\u0432\u0430\u0442\u044c \u0437\u0430 \u0440\u0430\u0437\u043d\u0438\u0446\u0443 \u043c\u0435\u0436\u0434\u0443 train  \u0438 test\np = 0.8\n# k - \u043a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nk = 250\n\ncat = X_train[column].select_dtypes(include=['category']).columns.tolist()\n\nskf = KFold(n_splits=3, shuffle=True, random_state=7)\n\ndef score(params):\n    print('Training with params:')\n    print(params)\n    w=[]\n    best_iter = []\n    \n\n    for train_index, val_index in skf.split(X_train, y_train):\n        x_train_1, x_valid_1 = X_train[column].iloc[train_index, :], X_train[column].iloc[val_index, :]\n        y_train_1, y_valid_1 = y_train.iloc[train_index], y_train.iloc[val_index]\n        train_data = lgb.Dataset(x_train_1, label=y_train_1, categorical_feature=cat)\n        val_data = lgb.Dataset(x_valid_1, label=y_valid_1, categorical_feature=cat, reference=train_data)\n        gbm = lgb.train(params,\n                        train_data,\n                        valid_sets = [train_data, val_data],\n                        valid_names=['train', 'val'],\n                        num_boost_round = 5000,\n                        verbose_eval = False, \n                        categorical_feature=cat\n                       )\n        w.append([gbm.best_score['train']['rmse'], gbm.best_score['val']['rmse']])\n        best_iter.append(gbm.best_iteration)\n    nrounds = np.mean(best_iter)\n    print('best iter:', int(nrounds), 'all iter:', best_iter)\n    res = list(np.mean(w, axis=0))\n    print(\"\\t rmse train {0}, rmse test {1}, dif {2}, \\n\\t final score {3} \\n\\n\".format(res[0], res[1], np.power(np.square(res[0]-res[1]), p), +res[1]+np.power(np.square(res[0]-res[1]), p)))\n    return {'loss': +res[1]+np.power(np.square(res[0]-res[1]), p), 'status': STATUS_OK, \n            'mean_rmse_train': res[0], 'mean_rmse_test': res[1], 'best_iter': int(nrounds)}\n\ndef optimize(trials):\n    space = {\n        #'max_depth': hp.choice('max_depth', [-1, 6, 7]),\n        'max_depth': -1,\n        'max_bin': scope.int(hp.uniform('max_bin', 1000, 2000)),\n        'num_leaves': scope.int(hp.uniform('num_leaves', 8, 60)),\n        'lambda_l1': hp.quniform('lambda_l1', 0.5, 2.25, 0.125),\n        'learning_rate': hp.quniform('learning_rate', 0.03 , 0.06, 0.005),\n        'bagging_fraction': hp.quniform('bagging_fraction', 0.6, 0.9, 0.01),\n        'metric': ('rmse',),\n        'boosting_type': 'gbdt',\n        'objective': 'regression',\n        'nthread': 8,\n        'early_stopping_rounds': 20,\n        'silent':1,\n    }\n    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=k)\n    print(best)\n\ntrials = Trials()\noptimize(trials)","e66d98f0":"params = trials.best_trial['misc']['vals']\nparams['max_depth'] = -1\nparams['boosting_type'] = 'gbdt'\nparams['objective'] = 'regression'\nparams['metric'] = ('l1', 'l2')\nparams['nthread'] = 8\nparams['early_stopping_rounds'] = 100\nparams['silent'] = 1\nparams['num_leaves'] = int(params['num_leaves'][0])\nparams['max_bin'] = int(params['max_bin'][0])\nparams","573e14f6":"train_ds = lgb.Dataset(X_train[column], label=y_train, categorical_feature=cat, )\nval_ds = lgb.Dataset(X_val[column], label=y_val, categorical_feature=cat, reference=train_ds)","64132754":"%%time\nbooster = lgb.train(params, train_ds, num_boost_round=10000, valid_sets=[train_ds, val_ds], valid_names=['train', 'valid'], verbose_eval=100, categorical_feature=cat, early_stopping_rounds=100)","d24264d9":"shap.initjs()\nexplainer = shap.TreeExplainer(booster)\nshap_values = explainer.shap_values(X_val[column])","ff53908a":"shap.summary_plot(shap_values, X_val[column], plot_type='bar', max_display=30)","db55f57a":"pred_train = np.expm1(booster.predict(X_train[column]))\npred_val = np.expm1(booster.predict(X_val[column]))\npred_test = np.expm1(booster.predict(test_df[column]))","31f2d3f8":"from sklearn import metrics","a7dab4f8":"print(metrics.mean_absolute_error(y_train, pred_train))\nprint(metrics.mean_squared_error(y_train, pred_train))\nprint(metrics.mean_squared_log_error(y_train, pred_train))\nprint(metrics.median_absolute_error(y_train, pred_train))","dcc2ae9e":"print(metrics.mean_absolute_error(y_val, pred_val))\nprint(metrics.mean_squared_error(y_val, pred_val))\nprint(metrics.mean_squared_log_error(y_val, pred_val))\nprint(metrics.median_absolute_error(y_val, pred_val))","d0a268fd":"Good optimization","2c23b77d":"## Much better!","91f8f69e":"## Let's look at the distributions, correlations, and other characteristics of the samples.","a9ef5523":"## With this amount, you can use gradient boosting.","0e969e05":"## EDA","832989ca":"Choose the optimal number of features by the difference and the validation metric. More than half of the features do not carry informational content.","bc97e76e":"### Log it","710f100c":"# Some import","8f955721":"# Metrics","771578b1":"# Here you need to highlight the bucket with the business and calculate the metric that is understandable for the business -% of the bucket getting into the bucket. You can also calculate the% hit in the 20% interval.","d4c9ada4":"## Introduction\nGreetings from the Kaggle bot! This is an automatically-generated kernel with starter code demonstrating how to read in the data and begin exploring. If you're inspired to dig deeper, click the blue \"Fork Notebook\" button at the top of this kernel to begin editing.\n\n## Exploratory Analysis\nTo begin this exploratory analysis, first import libraries and define functions for plotting the data using matplotlib. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","f3974764":"## Choosing the number of features by gain\nFrom each iteration, we will drop from lightgbm training one bundle according to min gain. For us, the difference between the train and the test, as well as the metric on validation, is important","cc6f7cb6":"# Feature importance","f9e32075":"\u0417\u043d\u0430\u0447\u0438\u043c\u043e\u0441\u0442\u044c \u0444\u0438\u0447\u0435\u0439 \u043f\u043e gain lightgbm:","d12fc7c1":"# HOW MUCH NOISE !!!!","f9342123":"### Now we narrow the selection ranges with a hyperparameter and remove insignificant features","8fe75cc8":"# Predict\n\nDo not forget to expose the predict","403dbe0a":"# Regression with categorical data starter pack","d9728690":"# Target:","c9e7e6ea":"## We will use more modern methods. We will choose hyperparameters as a hyperopt, and not a gridscaerch.","7232a291":"In the id column, some insider is unlikely to be sewn up, we will drop. It\u2019s more correct, of course, to index the data frame by id, but there is no task to build a pipeline, so we\u2019ll omit this moment)","e7763796":"### Check out my guide to linear regressions:\nhttps:\/\/www.kaggle.com\/podsyp\/complete-linear-model-guide","6ff27bc8":"# Val","ef26e520":"### Immediately, do the right thing. Reduce the amount of memory under the data frame when reading.","4dce2904":"Using pandas_summary we\u2019ll see describe the columns. By the way, here you can immediately see if reduce_mem_usage worked correctly, in some cases it gives inf ... If something went wrong, then the infs will appear in min and max.","21082662":"##### Let's check the columns in the test and train","346fc15c":"# Let's train a model with selected hyperparameters.","d144c1b4":"# Train","d61be284":"## Feature engineering will not do. Hypothesis - the number of features is excessive. We will look for top features and exclude those that negatively affect quality.","fa7d9d5a":"The error is clearly reduced!"}}