{"cell_type":{"4d6bbf18":"code","b94cbd8a":"code","c43c516a":"code","b66501ea":"code","657e6d89":"code","986b4701":"code","3239921d":"code","9b508c4a":"code","43d86b76":"code","981b3b92":"code","cd1b3fe2":"code","61a49e6d":"code","131d448e":"code","dce7a821":"code","5ef9ecfe":"code","f94ec0b1":"code","20292651":"code","703a0359":"code","07a35135":"code","0cddcdd7":"code","0e6acb2d":"code","b010008d":"code","ffeabfd7":"code","db0e081f":"code","65982565":"code","2fd56669":"code","38b25b62":"code","8003d031":"code","7cb99e12":"code","a4f9e3c8":"code","28f1b350":"code","6559ee77":"code","d9dce8ba":"code","afe46afa":"code","a44d7e2c":"code","e5703595":"code","e491f6cd":"code","ac21d054":"code","cd3d180d":"code","e8d50bda":"code","5c62fe6f":"code","7a7d9469":"code","244e4130":"code","3ba80120":"code","53bcbca1":"code","ae4c3ead":"code","1aea1ffb":"code","6b6c1d13":"code","805573bb":"code","97af4e12":"code","fd7f2370":"code","81740f62":"code","77fad531":"code","a88a2a6e":"code","6a7ca019":"code","1a065126":"code","4b30ca21":"code","5d6f501e":"code","a1a67676":"code","e9d03d2b":"code","7083dd37":"code","ca0bd6f1":"code","91dc6e6e":"code","75a523f9":"code","16cc2cb3":"code","3fae5ef2":"code","44f35382":"code","b4015640":"code","c8a52c72":"code","58410dd9":"code","b90c0c9d":"code","74a89b4c":"code","c48ca845":"code","0fbac088":"code","b814c9dd":"code","74e06613":"code","430b6805":"code","addf62a0":"code","6647e458":"code","61f9829c":"code","232668f3":"code","1c60b3fc":"code","162359fb":"code","2e599273":"code","fca407dc":"code","3a2f50e3":"markdown","5093716a":"markdown","dff6073f":"markdown","a23857d5":"markdown","4f179152":"markdown","343dba04":"markdown","ebfbd14e":"markdown","4635e6b6":"markdown","6e2548a0":"markdown","72d9be82":"markdown","89c34aad":"markdown","152e64d2":"markdown","9b131fbb":"markdown","0f43b460":"markdown","c6c18d01":"markdown","ea9de260":"markdown","f9ae846d":"markdown","7802d8c3":"markdown","235fdd22":"markdown","5805ab9c":"markdown","f8195253":"markdown"},"source":{"4d6bbf18":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b94cbd8a":"%%javascript\n$.getScript('https:\/\/kmahelona.github.io\/ipython_notebook_goodies\/ipython_notebook_toc.js')","c43c516a":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport scipy.stats as stats\nfrom scipy.stats import norm\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nfrom scipy.stats import skew, norm\nfrom sklearn.neighbors import KNeighborsRegressor\n%matplotlib inline","b66501ea":"#Import the files\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","657e6d89":"print('Shape of train data is', df_train.shape)\nprint ('Shape of tests data is', df_test.shape)","986b4701":"#check the train data\ndf_train.head()\n\n# so there are around 81 features here in this dataset","3239921d":"#information about the train data set\ndf_train.info()","9b508c4a":"df_train.describe()","43d86b76":"#check for the missing values in the train data\ndf_train.isnull().sum()","981b3b92":"# Check which variables have missing values\n\nTrain_columns_with_missing_values = df_train.columns[df_train.isnull().any()]\ndf_train[Train_columns_with_missing_values].isnull().sum()","cd1b3fe2":"df_train.isnull().sum().sum() # total missing values in the entire df","61a49e6d":"missing_series = df_train.isnull().sum() \nmissing_series.where(missing_series>1000)\nmissing_series.loc[missing_series>1000]\/df_train.shape[0] # to locate the missing values that are more than 1000","131d448e":"#Lets look at the histogram of all the features first\n\ndf_train.hist(bins=50,figsize=(20,15))","dce7a821":"# Now using heatmaps\nplt.figure(figsize=(15,8))\nsns.heatmap(df_train.isnull(), cbar=False)\n# FRom this heatmap too it is evident that the above 4 mentioned vars have more missing values","5ef9ecfe":"# Next is to use the missingno library to analyze the missing values\nimport missingno as msno \n\nmsno.bar(df_train,labels=True,fontsize=10) ","f94ec0b1":"msno.heatmap(df_train,labels=True) \n# Entries marked <1 or >-1 have a correlation that is close to being exactingly negative or positive, but is still not quite perfectly so\n# Nullity correlation ranges from -1 (if one variable appears the other definitely does not) to 0 (variables appearing or not appearing have no effect on one another) to 1 (if one variable appears the other definitely also does)","20292651":"df_train['Alley'].unique()","703a0359":"# variable - Alley \n# Alley: Type of alley access to property Grvl-Gravel, Pavd-Paved, NA-No alley access\n# so nan in alley is not a missing value it means no access -> replace nan with no access\n# Py took the a\u00b4nan as missing values \ndf_train['Alley'].fillna('No Access', inplace=True)","07a35135":"# variable - LotFrontage \n# it is afloat variable , so missing values can be replaced by using median\/mean - central tendency mneasures\n# which one to use depends on the plot of the data\n# https:\/\/towardsdatascience.com\/understanding-boxplots-5e2df7bcbd51\nsns.boxplot(df_train['LotFrontage'])\n","0cddcdd7":"sns.distplot(df_train['LotFrontage'])","0e6acb2d":"\n# impute the missing values using median\ndf_train['LotFrontage'].fillna(df_train['LotFrontage'].median(),inplace =True)","b010008d":"df_train['BsmtQual'].fillna('No Basement', inplace=True)\ndf_train['BsmtCond'].fillna('No Basement', inplace=True)\ndf_train['BsmtExposure'].fillna('No Basement', inplace=True)\ndf_train['BsmtFinType1'].fillna('No Basement', inplace=True)\ndf_train['BsmtFinType2'].fillna('No Basement', inplace=True)","ffeabfd7":"df_train['GarageType'].fillna('No Garage', inplace=True)\ndf_train['GarageFinish'].fillna('No Garage', inplace=True)\ndf_train['GarageQual'].fillna('No Garage', inplace=True)\ndf_train['GarageCond'].fillna('No Garage', inplace=True)","db0e081f":"df_train['PoolQC'].fillna('No Pool', inplace=True)\ndf_train['Fence'].fillna('No Fence', inplace=True)\ndf_train['MiscFeature'].fillna('None', inplace=True)\ndf_train['MasVnrType'].fillna('None', inplace=True)\ndf_train['FireplaceQu'].fillna('No Fireplace', inplace=True)\n","65982565":"df_train['MasVnrArea'].value_counts()","2fd56669":"df_train['MasVnrArea'].fillna(df_train['MasVnrArea'].mode()[0], inplace=True)\ndf_train['Electrical'].fillna(df_train['Electrical'].mode()[0], inplace=True)","38b25b62":"#drop the ID and year features\ndf_train.drop(['GarageYrBlt'],axis=1,inplace=True)\ndf_train.drop(['Id'],axis=1,inplace=True)","8003d031":"#Finally check the DF after the imputation process\n# Now using heatmaps\nplt.figure(figsize=(15,8))\nsns.heatmap(df_train.isnull(), cbar=False)\n\n# missing values for the train data are succesfully handled","7cb99e12":"# Check which variables have missing values in test data\n\nTestcolumns_with_missing_values = df_test.columns[df_test.isnull().any()]\ndf_test[Testcolumns_with_missing_values].isnull().sum()","a4f9e3c8":"#Finally check the DF after the imputation process\n# Now using heatmaps\nplt.figure(figsize=(15,8))\nsns.heatmap(df_test.isnull(), cbar=False)\n\n# missing values for the train data are succesfully handled","28f1b350":"df_test['LotFrontage'].fillna(df_test['LotFrontage'].median(),inplace =True)\ndf_test['Alley'].fillna('No Access', inplace=True)","6559ee77":"df_test['BsmtQual'].fillna('No Basement', inplace=True)\ndf_test['BsmtCond'].fillna('No Basement', inplace=True)\ndf_test['BsmtExposure'].fillna('No Basement', inplace=True)\ndf_test['BsmtFinType1'].fillna('No Basement', inplace=True)\ndf_test['BsmtFinType2'].fillna('No Basement', inplace=True)","d9dce8ba":"df_test['GarageType'].fillna('No Garage', inplace=True)\ndf_test['GarageFinish'].fillna('No Garage', inplace=True)\ndf_test['GarageQual'].fillna('No Garage', inplace=True)\ndf_test['GarageCond'].fillna('No Garage', inplace=True)","afe46afa":"df_test['PoolQC'].fillna('No Pool', inplace=True)\ndf_test['Fence'].fillna('No Fence', inplace=True)\ndf_test['MiscFeature'].fillna('None', inplace=True)\ndf_test['MasVnrType'].fillna('None', inplace=True)\ndf_test['FireplaceQu'].fillna('No Fireplace', inplace=True)","a44d7e2c":"#Categorical features \ndf_test['MasVnrArea'].fillna(df_test['MasVnrArea'].mode()[0], inplace=True)\ndf_test['Electrical'].fillna(df_test['Electrical'].mode()[0], inplace=True)\ndf_test['MSZoning'].fillna(df_test['MSZoning'].mode()[0], inplace=True)\ndf_test['Utilities'].fillna(df_test['Utilities'].mode()[0], inplace=True)\ndf_test['SaleType'].fillna(df_test['SaleType'].mode()[0], inplace=True)\ndf_test['Functional'].fillna(df_test['Functional'].mode()[0], inplace=True)\ndf_test['KitchenQual'].fillna(df_test['KitchenQual'].mode()[0], inplace=True)\ndf_test['BsmtFinSF2'].fillna(df_test['BsmtFinSF2'].mode()[0], inplace=True)\ndf_test['Exterior1st'].fillna(df_test['Exterior1st'].mode()[0], inplace=True)\ndf_test['Exterior2nd'].fillna(df_test['Exterior2nd'].mode()[0], inplace=True)\ndf_test['BsmtFullBath'].fillna(df_test['BsmtFullBath'].mode()[0], inplace=True)\ndf_test['BsmtHalfBath'].fillna(df_test['BsmtHalfBath'].mode()[0], inplace=True)","e5703595":"#drop the year features\ndf_test.drop(['GarageYrBlt'],axis=1,inplace=True)\n","e491f6cd":"sns.boxplot(df_test['GarageArea'])","ac21d054":"df_test['GarageArea'].fillna(df_test['GarageArea'].median(),inplace =True)","cd3d180d":"sns.boxplot(df_test['GarageCars'])","e8d50bda":"df_test['GarageCars'].fillna(df_test['GarageCars'].mean(),inplace =True)","5c62fe6f":"sns.boxplot(df_test['BsmtFinSF1'])\ndf_test['BsmtFinSF1'].fillna(df_test['BsmtFinSF1'].median(),inplace =True)","7a7d9469":"(df_test['BsmtFullBath'])","244e4130":"sns.distplot(df_test['BsmtUnfSF'])\ndf_test['BsmtUnfSF'].fillna(df_test['BsmtUnfSF'].median(),inplace =True)","3ba80120":"sns.boxplot(df_test['TotalBsmtSF'])\ndf_test['TotalBsmtSF'].fillna(df_test['TotalBsmtSF'].median(),inplace =True)","53bcbca1":"df_test['BsmtFullBath'].value_counts()","ae4c3ead":"#Finally check the DF after the imputation process\n# Now using heatmaps\nplt.figure(figsize=(15,8))\nsns.heatmap(df_test.isnull(), cbar=False)\n\n# missing values for the train data are succesfully handled","1aea1ffb":"#Shape after handling the missing data\nprint('Shape of train data is', df_train.shape)\nprint ('Shape of tests data is', df_test.shape)","6b6c1d13":"df_train.corr()","805573bb":"# Correlation Matrix\n\n# Increase the size of the heatmap.\nplt.figure(figsize=(25, 15))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(df_train.corr(), vmin=-1, vmax=1, annot=True,cmap='BrBG')\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':6}, pad=14);\n\n\n\n\n","97af4e12":"#check the features that are highly correlated to the output variable\nplt.figure(figsize=(8, 12))\nheatmap_outputvar = sns.heatmap(df_train.corr()[['SalePrice']].sort_values(by='SalePrice', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap_outputvar.set_title('Features Correlating with Sales Price', fontdict={'fontsize':18}, pad=16);","fd7f2370":"# most correlated features\n\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice'])>0.5]\nplt.figure(figsize=(16,9))\ng = sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","81740f62":"sns.distplot(df_train['SalePrice']);\n#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())\n# the dist is right skewed, ie it has many outliersspred out to the right","77fad531":"# For parametric models , like regression it is important to have  a belll type distribution of the target variable\n# So the idea here is to log transform the output variable\ndf_train['SalePrice_Log'] = np.log(df_train['SalePrice'])\n\nsns.distplot(df_train['SalePrice_Log']);\n# skewness and kurtosis\nprint(\"Skewness after Transform: %f\" % df_train['SalePrice_Log'].skew())\nprint(\"Kurtosis after Transform: %f\" % df_train['SalePrice_Log'].kurt())\n# dropping old column\n\ndf_train.drop('SalePrice', axis= 1, inplace=True)","a88a2a6e":"# Plot skew value for each numerical value\nfrom scipy.stats import skew \nskew_in_vars = df_train [numerical_feats].apply(lambda x: skew(x))\nskew_in_vars.sort_values(ascending=False)","6a7ca019":"skew_in_vars = skew_in_vars[abs(skewness)>0.5]\nskew_in_vars.index","1a065126":"skew_features = df_train[skew_in_vars.index]\nskew_features.columns","4b30ca21":"#check correlation again\n# Increase the size of the heatmap.\nplt.figure(figsize=(25, 15))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(df_train.corr(), vmin=-1, vmax=1, annot=True,cmap='BrBG')\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':6}, pad=14);","5d6f501e":"# most correlated features\n\ncorrmat = df_train.corr()\ntop_corr_features = corrmat.index[abs(corrmat['SalePrice_Log'])>0.5]\nplt.figure(figsize=(16,9))\ng = sns.heatmap(df_train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","a1a67676":"#First of all we need to get thecolumns where the categorical features are present\n# idea is to extract all the columns with type as object\ndf_train.dtypes.unique()","e9d03d2b":"for types in df_train.dtypes.unique():\n    print(types,len(df_train.select_dtypes(types).columns))\n    print(df_train.select_dtypes(types).columns)","7083dd37":"numerical_feats = df_train.dtypes[df_train.dtypes != \"object\"].index\nprint(\"Number of Numerical features: \", len(numerical_feats))\n\ncategorical_feats = df_train.dtypes[df_train.dtypes == \"object\"].index\nprint(\"Number of Categorical features: \", len(categorical_feats))","ca0bd6f1":"nr_rows = 12\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*3.5,nr_rows*3))\n\nli_plot_num_feats = [c for c in list(numerical_feats) ]\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(li_plot_num_feats):\n            sns.regplot(df_train[li_plot_num_feats[i]], df_train['SalePrice_Log'], ax = axs[r][c])\n            stp = stats.pearsonr(df_train[numerical_feats[i]], df_train['SalePrice_Log'])\n          \n            str_title = \"r = \" + \"{0:.2f}\".format(stp[0]) + \"      \" \"p = \" + \"{0:.2f}\".format(stp[1])\n            axs[r][c].set_title(str_title,fontsize=11)\n            \nplt.tight_layout()    \nplt.show()   ","91dc6e6e":"\nnr_rows = 15\nnr_cols = 3\n\nfig, axs = plt.subplots(nr_rows, nr_cols, figsize=(nr_cols*4,nr_rows*3))\n\nfor r in range(0,nr_rows):\n    for c in range(0,nr_cols):  \n        i = r*nr_cols+c\n        if i < len(categorical_feats):\n            sns.boxplot(x=categorical_feats[i], y=df_train['SalePrice_Log'], data=df_train, ax = axs[r][c])\n    \nplt.tight_layout()    \nplt.show() ","75a523f9":"#https:\/\/towardsdatascience.com\/the-search-for-categorical-correlation-a1cf7f1888c9\n\n# from scipy.stats import chi2_contingency\n# def cramers_v(x, y):\n#     confusion_matrix = pd.crosstab(x,y)\n#     chi2 = chi2_contingency(confusion_matrix)[0]\n#     n = confusion_matrix.sum().sum()\n#     phi2 = chi2\/n\n#     r,k = confusion_matrix.shape\n#     phi2corr = max(0, phi2-((k-1)*(r-1))\/(n-1))\n#     rcorr = r-((r-1)**2)\/(n-1)\n#     kcorr = k-((k-1)**2)\/(n-1)\n#     return np.sqrt(phi2corr\/min((kcorr-1),(rcorr-1)))\n# for cat in categorical_feats:\n#     print('The relation with {} is'.format(cat) ,cramers_v(df_train[cat],df_train['SalePrice']))","16cc2cb3":"# Drop the columns that are not strongly correlated to the target variable\n# numerical features that are less than a threshold corr value\n# categorical feats that are manually selected ","3fae5ef2":"numerical_feats","44f35382":"top_corr_features","b4015640":"numerical_feats_below = [ x for x in numerical_feats if x not in  top_corr_features] # list comphrehension to find the diff\ncategorical_feats_to_keep = ['SaleType', 'PoolQC', 'MSZoning', 'Neighborhood','Condition1','HouseStyle','RoofMatl', 'Condition2', 'MasVnrType', 'ExterQual','BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType']\ncategorical_feats_to_drop = [ x for x in categorical_feats if x not in  categorical_feats_to_keep]\n#item for item in first if item not in second","c8a52c72":"categorical_feats_to_drop","58410dd9":"#Features that we are going to drop\n\nprint('Numerical features to drop are:\\n',numerical_feats_below)\nprint('---'*50)\nprint('Categorical features to drop are:\\n', categorical_feats_to_drop)","b90c0c9d":"df_test.drop('Id', inplace= True, axis = 1)    \n\ndf_test.columns","74a89b4c":"# Drop the unwanted columns\n\nfor df in [df_train, df_test] :\n    df.drop(categorical_feats_to_drop, inplace= True, axis = 1)\n    df.drop(numerical_feats_below, inplace= True, axis = 1)    ","c48ca845":"df_train.columns","0fbac088":"# check correlation after dropping\n\nplt.figure(figsize=(25, 15))\n# Store heatmap object in a variable to easily access it when you want to include more features (such as title).\n# Set the range of values to be displayed on the colormap from -1 to 1, and set the annotation to True to display the correlation values on the heatmap.\nheatmap = sns.heatmap(df_train.corr(), vmin=-1, vmax=1, annot=True,cmap='BrBG')\n# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\nheatmap.set_title('Correlation Heatmap after drops', fontdict={'fontsize':18}, pad=14);","b814c9dd":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","74e06613":"df_train.head()","430b6805":"df_train.shape","addf62a0":"y_train_data  =df_train['SalePrice_Log']\nx_train_data = df_train.loc[:, df_train.columns != 'SalePrice_Log']\nfrom sklearn.model_selection import  train_test_split\n#split the data to train the model \nX_train,X_test,y_train,y_test = train_test_split(x_train_data,y_train_data,test_size = 0.3,random_state= 0)","6647e458":"X_train.shape,X_test.shape,y_train.shape,y_test.shape","61f9829c":"# Classical Models","232668f3":"from sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.model_selection import KFold, cross_val_score\n\nn_folds = 5\n\nscorer = make_scorer(mean_squared_error,greater_is_better = False)\ndef rmse_CV_train(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(x_train_data.values)\n    rmse = np.sqrt(-cross_val_score(model,X_train,y_train,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)\ndef rmse_CV_test(model):\n    kf = KFold(n_folds,shuffle=True,random_state=42).get_n_splits(x_train_data.values)\n    rmse = np.sqrt(-cross_val_score(model,X_test,y_test,scoring =\"neg_mean_squared_error\",cv=kf))\n    return (rmse)","1c60b3fc":"lr = LinearRegression()\nlr.fit(X_train,y_train)\ntest_pre = lr.predict(X_test)\ntrain_pre = lr.predict(X_train)\nprint('rmse on train',rmse_CV_train(lr).mean())\nprint('rmse on test',rmse_CV_test(lr).mean())","162359fb":"#plot between predicted values and residuals\nplt.scatter(train_pre, train_pre - y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre,test_pre - y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n","2e599273":"# Plot predictions - Real values\nplt.scatter(train_pre, y_train, c = \"blue\",  label = \"Training data\")\nplt.scatter(test_pre, y_test, c = \"black\",  label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","fca407dc":"# Boosting models","3a2f50e3":"The issues with the following variable is also due to the misintepretation by python , here na means no garage\n* GarageType        81\n* GarageYrBlt       81\n* GarageFinish      81\n* GarageQual        81\n* GarageCond        81","5093716a":"## Finding missing values","dff6073f":"# TEST Data","a23857d5":"* The number of categorical data = 43\n* Number of numerical features = 36","4f179152":"## Categorical features to dummy ","343dba04":"The issues with the following variable is also due to the misintepretation by python\n* FireplaceQu      690 - No Fireplace\n* PoolQC          1453 - No Pool\n* Fence           1179 - No Fence\n* MasVnrType         8 - None\n* MasVnrArea         8 - Replace with the most common\n* MiscFeature     1406 - None","ebfbd14e":"we can see that there are lot of outliers in this feature and from the distribution plot it can also be seen that the plot is skewed. Outliers data points will have significant impact on the mean and hence, in such cases, it is not recommended to use mean for replacing the missing values. Using mean value for replacing missing values may not create a great model and hence gets ruled out. For symmetric data distribution, one can use mean value for imputing missing values.","4635e6b6":"## Preprocessing\n* Drop the missing values\n* fill the missing values\n    * Fill with meaningful values, after reading data description\n    * fill with stat values after looking at the variable (if it is a  continious variable)","6e2548a0":"## Categorical features","72d9be82":"<h1 id=\"tocheading\">Table of Contents<\/h1>\n<div id=\"toc\"><\/div>","89c34aad":"# Modelling","152e64d2":"The issues with the following variable is also due to the misintepretation by python , here na means no basement\n* BsmtQual          37\n* BsmtCond          37\n* BsmtExposure      38\n* BsmtFinType1      37\n* BsmtFinType2      38","9b131fbb":"## Transforming Target & other numerical variable","0f43b460":"## Correlation Matrix","c6c18d01":"![image.png](attachment:image.png)","ea9de260":"We can see that the above 4 features have a lot of missing values and they won't brinng any value to the further analysis . To strengthen our analysis the other methods that we could use to see the missing values are:\n* Heatmaps\n* Histogram plot of all the features and then visual inspection\n* missingno library","f9ae846d":"# TRAIN Data","7802d8c3":"Apart from visual inspection, how to check the correaltion of categorical variables to Target variable ?? ","235fdd22":"## Target variable vs Numerical features","5805ab9c":"* The categorical variables are corelated to the target variable Saleprice if the mean and the distrubition are different \/ varying for each features of a particular variable\n\n- Varying mean\/distribuition \u2192 strong correlation \u2192 can keep the features\n    * SaleType, PoolQC, 'MSZoning', 'Neighborhood','Condition1','HouseStyle','RoofMati', 'Condition2', 'MasVnrType',   'ExterQual','BsmtQual','CentralAir', 'Electrical', 'KitchenQual', 'SaleType' ","f8195253":"From the above correlation plots it is evident that the features that really matter the most are:\n* OverallQual\n* GrlivingArea\n* GarageCrs\n* GarageArea"}}