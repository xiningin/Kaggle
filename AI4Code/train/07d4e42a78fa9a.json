{"cell_type":{"496a27ff":"code","2ab643e0":"code","6f4ff382":"code","d115c714":"code","4c7b2505":"code","db77872b":"code","e920f748":"code","7f133f62":"code","8a519d94":"code","bb731440":"code","35ec5a6f":"code","cfc8e8eb":"code","de42cb91":"code","143ebe60":"code","613b3196":"code","c574e689":"code","487bdafe":"code","fa032502":"code","4e0757a1":"code","1d10b6b3":"code","b386a77a":"code","c3e180ad":"code","e5f27678":"code","6fb1c4b2":"code","75e1e7ef":"code","14e38f97":"code","e33169b5":"code","e5d688b6":"code","e441e3e7":"code","f737d320":"code","e1914a22":"code","c3c0050c":"code","dde9037f":"code","da228288":"code","2376c112":"code","ab98160a":"code","bb5b8fc8":"code","69a3c695":"code","2f258044":"code","e503ada1":"code","304694cb":"code","18e43196":"code","626ad5ac":"code","67200ccb":"code","14af48bc":"code","eae22507":"code","4fa0108a":"code","ba655a3a":"code","e3715a3e":"code","f4b87e1c":"code","eb1ad96b":"code","37f0c9d1":"code","6d353662":"code","4e195d51":"code","e71f96b1":"code","443fa3d8":"code","297d0387":"code","99e2d19d":"code","277fb0d9":"code","609c4130":"code","c4f46e73":"code","8f8f2f1e":"code","ec7dab3e":"code","e519f620":"code","5b7d8a5c":"code","a9999feb":"code","1841c615":"code","1b471a59":"code","4b01ee6c":"code","ea35d60c":"code","e138c312":"code","528e12fd":"code","ff7858cc":"code","cbfcc963":"code","311bda19":"code","ba342a76":"code","7245d3ce":"code","636f6bc9":"code","381965fb":"code","2095936b":"code","73622686":"code","4c8319d4":"code","37b981ea":"code","322b999d":"code","53b4686a":"code","325a57dd":"code","3317a89f":"code","68dbdf50":"code","f7f56c9e":"markdown","bb143d95":"markdown","2b09522d":"markdown","c6912ccf":"markdown","63c67728":"markdown"},"source":{"496a27ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom lightgbm import LGBMClassifier\nimport numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.feature_selection import SelectFwe, f_regression\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom tpot.builtins import OneHotEncoder, StackingEstimator\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2ab643e0":"train = pd.read_csv('\/kaggle\/input\/janatahack\/train_8wry4cB.csv')\ntest = pd.read_csv('\/kaggle\/input\/janatahack\/test_Yix80N0.csv')\nsample = pd.read_csv('\/kaggle\/input\/janatahack\/sample_submission_opxHi4g.csv')","6f4ff382":"print(train.shape)\nprint(train.info)","d115c714":"train.apply(lambda x:len(x.unique()))","4c7b2505":"train.isna().sum()","db77872b":"sns.countplot(train['gender'])","e920f748":"train['startTime'] = pd.to_datetime(train['startTime'])\ntrain['endTime'] = pd.to_datetime(train['endTime'])","7f133f62":"train","8a519d94":"df = train.append(test)","bb731440":"df = df[['session_id','startTime','endTime','ProductList','gender']]","35ec5a6f":"df['ProductCount'] = df.ProductList.str.count(';')+1","cfc8e8eb":"df['ProductList']","de42cb91":"df['startTime'] = pd.to_datetime(df['startTime'])\ndf['endTime'] = pd.to_datetime(df['endTime'])","143ebe60":"import numpy as np\nfrom itertools import chain\n\n# return list from series of comma-separated strings\ndef chainer(s):\n    return list(chain.from_iterable(s.str.split(';')))\n\n# calculate lengths of splits\nlens = df['ProductList'].str.split(';').map(len)\n\n# create new dataframe, repeating or chaining as appropriate\ndf1 = pd.DataFrame({'session_id': np.repeat(df['session_id'], lens),\n                    'startTime': np.repeat(df['startTime'], lens),\n                    'endTime':np.repeat(df['endTime'],lens),\n                    'ProductCount': np.repeat(df['ProductCount'], lens),\n                    'ProductList': chainer(df['ProductList']),\n                    'gender':np.repeat(df['gender'],lens)})\n\nprint(df1)","613b3196":"df1.head()","c574e689":"df1['TimeTaken'] = abs(df1['endTime'] - df1['startTime']).astype('timedelta64[m]')","487bdafe":"df1[['Date','Time']] = df1['startTime'].astype(str).str.split(\" \",expand=True) ","fa032502":"df1['Date'] = pd.to_datetime(df1['Date'])","4e0757a1":"df1['Day'] = df1['Date'].apply(lambda x: x.weekday())","1d10b6b3":"df1.head()","b386a77a":"df1['TimeTaken'].max()","c3e180ad":"df1[['Category','SubCategory','SubSubCategory','SubSubSubCategory','Extra']] = df1['ProductList'].str.split(\"\/\",expand=True) ","e5f27678":"del df1['Extra']\ndel df1['ProductList']\n","6fb1c4b2":"del df1['Time']\ndel df1['Date']","75e1e7ef":"len(df1['session_id'].unique())","14e38f97":"df1.columns","e33169b5":"df1 = df1[['session_id','TimeTaken','Day','ProductCount','Category','SubCategory','SubSubCategory','SubSubSubCategory','gender']]","e5d688b6":"df1['TimeTaken'] = df1.TimeTaken.apply(lambda x:int(x))","e441e3e7":"df1.head()","f737d320":"from sklearn import preprocessing \ncolumns = ['Category','SubCategory','SubSubCategory','SubSubSubCategory']\nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    df1[i]= label_encoder.fit_transform(df1[i]) \n  ","e1914a22":"df1['session_id'].describe()","c3c0050c":"test1 = df1[df1['gender'].isnull() == True]","dde9037f":"train1 = df1[df1['gender'].isnull() == False]\n","da228288":"train1.head()","2376c112":"test1.head()","ab98160a":"from sklearn import preprocessing \ncolumns = ['gender']\n# label_encoder object knows how to understand word labels. \nlabel_encoder = preprocessing.LabelEncoder() \n  \nfor i in columns:\n    train1[i]= label_encoder.fit_transform(train1[i]) \n  ","bb5b8fc8":"train1.corr()","69a3c695":"sns.countplot(train1['Category'])","2f258044":"sns.countplot(train1['Day'])","e503ada1":"train1.groupby('Day')['gender'].size()","304694cb":"sns.countplot(train1['ProductCount'])","18e43196":"pd.crosstab(train1['Day'],train1['gender'])","626ad5ac":"del test1['gender']","67200ccb":"def extra_tree(Xtrain,Ytrain,Xtest):\n    extra = ExtraTreesClassifier()\n    extra.fit(Xtrain, Ytrain) \n    extra_prediction = extra.predict(Xtest)\n    return extra_prediction\ndef Xg_boost(Xtrain,Ytrain,Xtest):\n    xg = XGBClassifier(loss='exponential', learning_rate=0.05, n_estimators=1000, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=5, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=1, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001)\n    xg.fit(Xtrain, Ytrain) \n    xg_prediction = xg.predict(Xtest)\n    return xg_prediction\ndef LGBM(Xtrain,Ytrain,Xtest):\n    lgbm = LGBMClassifier(boosting_type='gbdt', num_leaves=40,\n                            max_depth=5, learning_rate=0.05, n_estimators=1000, subsample_for_bin=200, objective='binary', \n                            min_split_gain=0.0, min_child_weight=0.001, min_child_samples=10,\n                            subsample=1.0, subsample_freq=0, colsample_bytree=1.0, reg_alpha=0.0,\n                            reg_lambda=0.0, random_state=None, n_jobs=1, silent=True, importance_type='split')\n    #lgbm = LGBMClassifier(n_estimators= 500)\n    lgbm.fit(X_train, Y_train)\n    lgbm_preds = lgbm.predict(X_test)\n    return lgbm_preds","14af48bc":"print(train1.columns)\nprint(test1.columns)","eae22507":"X_train = train1[['TimeTaken','Day','ProductCount','Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]\nY_train = train1['gender']\nX_test = test1[['TimeTaken','Day','ProductCount', 'Category', 'SubCategory', 'SubSubCategory','SubSubSubCategory']]","4fa0108a":"from autoviml.Auto_ViML import Auto_ViML","ba655a3a":"target = 'gender'\nscoring_parameter = 'balanced-accuracy'","e3715a3e":"\nm, feats, trainm, testm = Auto_ViML(train1, target, test1,\n                                    scoring_parameter=scoring_parameter,\n                                    hyper_param='GS',feature_reduction=True,\n                                     Boosting_Flag='Boosting_Flag',Binning_Flag=False)","f4b87e1c":"sam = pd.read_csv('\/kaggle\/input\/sample\/gender_Binary_Classification_submission.csv')","eb1ad96b":"\ntest1['gender'] = sam['gender_predictions']\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Auto.csv',index = False)","37f0c9d1":"X_train.head()\ncate_features_index = np.where(X_train.dtypes != float)[0]","6d353662":"xtrain,xtest,ytrain,ytest = train_test_split(X_train,Y_train,train_size=0.99,random_state=1236)","4e195d51":"from catboost import Pool, CatBoostClassifier, cv, CatBoostRegressor\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score","e71f96b1":"model = CatBoostClassifier(iterations=7000, learning_rate=0.001, l2_leaf_reg=3.5, depth=5, \n                           rsm=0.99, loss_function= 'Logloss', eval_metric='AUC',use_best_model=True,random_seed=50)","443fa3d8":"model.fit(xtrain,ytrain,cat_features=cate_features_index,eval_set=(xtest,ytest))","297d0387":"predss = model.predict(X_test)","99e2d19d":"\ntest1['gender'] = predss\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Catboost+PC.csv',index = False)","277fb0d9":"#pred_xg = Xg_boost(X_train,Y_train,X_test)\n#pred_et = extra_tree(X_train,Y_train,X_test)\npred_l = LGBM(X_train,Y_train,X_test)\n","609c4130":"# 0 - female, 1 male","c4f46e73":"test1['gender'] = pred_xg\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DXG.csv',index = False)","8f8f2f1e":"test1['gender'] = pred_et\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DETC.csv',index = False)","ec7dab3e":"\ntest1['gender'] = pred_l\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DPCLGBM.csv',index = False)\n","e519f620":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=0).fit(X_train, Y_train)\nans = clf.predict(X_test)\n","5b7d8a5c":"print(len(pred_l))\ntest1['gender'] = ans\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DLR.csv',index = False)","a9999feb":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=100).fit(X_train, Y_train)\nprediction_of_ada = ada.predict(X_test)","1841c615":"from sklearn.ensemble import GradientBoostingClassifier\ngbc = GradientBoostingClassifier(loss='exponential', learning_rate=0.1, n_estimators=100, subsample=1.0, criterion='friedman_mse', \n                                  min_samples_split=2, \n                                  min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_depth=10, min_impurity_decrease=0.0, \n                                  min_impurity_split=None, \n                                  init=None, random_state=None, max_features=None, verbose=0, max_leaf_nodes=None, warm_start=False, \n                                  presort='deprecated', \n                                  validation_fraction=0.1, n_iter_no_change=None, tol=0.0001).fit(X_train, Y_train)\nprediction_of_gbc = gbc.predict(X_test)","1b471a59":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier(n_estimators=10).fit(X_train, Y_train)\nprediction_of_rf = rf.predict(X_test)","4b01ee6c":"\ntest1['gender'] = prediction_of_ada\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DADA.csv',index = False)","ea35d60c":"\ntest1['gender'] = prediction_of_gbc\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dgbc.csv',index = False)","e138c312":"\ntest1['gender'] = prediction_of_rf\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DRF.csv',index = False)","528e12fd":"from sklearn.neighbors import KNeighborsClassifier\nneigh = KNeighborsClassifier(n_neighbors=3)\nneigh.fit(X_train,Y_train)\n\n# Predicted class\nnri = neigh.predict(X_test)\n","ff7858cc":"\ntest1['gender'] = nri\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('Dknn.csv',index = False)","cbfcc963":"from sklearn.calibration import CalibratedClassifierCV","311bda19":"model = XGBClassifier()\nmetLearn=CalibratedClassifierCV(model, method='isotonic', cv=2)\nmetLearn.fit(X_train, Y_train)\ntestPredictions = metLearn.predict(X_test)","ba342a76":"def submissions(predictions_by_model,string):\n    test1['gender'] = predictions_by_model\n    testn = test1[['session_id','gender']]\n    print(testn.isna().sum())\n    test_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\n    dic = {1:'male',0:'female'}\n    test_final['gender'] = test_final['gender'].map(dic)\n    test_final.to_csv(string.csv,index = False)\n","7245d3ce":"test1['gender'] = testPredictions\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DCCV.csv',index = False)","636f6bc9":"import pandas as pd\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\n\n# sklearn tools for model training and assesment\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import PredefinedSplit\nfrom sklearn.model_selection import GridSearchCV, ParameterGrid\nfrom sklearn.metrics import (roc_curve, auc, accuracy_score)\n\n# specify your configurations as a dict\nparams = {\n    'task': 'train',\n    'boosting_type': 'gbdt',\n    'objective': 'binary',\n    'metric': {'binary_logloss', 'auc'},\n    'metric_freq': 1,\n    'is_training_metric': True,\n    'max_bin': 255,\n    'learning_rate': 0.1,\n    'num_leaves': 63,\n    'tree_learner': 'serial',\n    'feature_fraction': 0.8,\n    'bagging_fraction': 0.8,\n    'bagging_freq': 5,\n    'min_data_in_leaf': 50,\n    'min_sum_hessian_in_leaf': 5,\n    'is_enable_sparse': True,\n    'use_two_round_loading': False,\n    'is_save_binary_file': False,\n    'output_model': 'LightGBM_model.txt',\n    'num_machines': 1,\n    'local_listen_port': 12400,\n    'machine_list_file': 'mlist.txt',\n    'verbose': 0,\n    'subsample_for_bin': 200000,\n    'min_child_samples': 20,\n    'min_child_weight': 0.001,\n    'min_split_gain': 0.0,\n    'colsample_bytree': 1.0,\n    'reg_alpha': 0.0,\n    'reg_lambda': 0.0\n}\n\n\nlgb_train = lgb.Dataset(X_train, Y_train)\n\n ","381965fb":"lgb_train","2095936b":"lgb_eval = lgb.Dataset(X_test)","73622686":"# train\ngbm = lgb.train(params,\n                lgb_train,\n                valid_sets=lgb_eval)","4c8319d4":"\n\n\ngridParams = {\n    'learning_rate': [ 0.1],\n    'num_leaves': [63],\n    'boosting_type' : ['gbdt'],\n    'objective' : ['binary']\n}\n\nmdl = lgb.LGBMClassifier(\n    task = params['task'],\n    metric = params['metric'],\n    metric_freq = params['metric_freq'],\n    is_training_metric = params['is_training_metric'],\n    max_bin = params['max_bin'],\n    tree_learner = params['tree_learner'],\n    feature_fraction = params['feature_fraction'],\n    bagging_fraction = params['bagging_fraction'],\n    bagging_freq = params['bagging_freq'],\n    min_data_in_leaf = params['min_data_in_leaf'],\n    min_sum_hessian_in_leaf = params['min_sum_hessian_in_leaf'],\n    is_enable_sparse = params['is_enable_sparse'],\n    use_two_round_loading = params['use_two_round_loading'],\n    is_save_binary_file = params['is_save_binary_file'],\n    n_jobs = -1\n)\n\nscoring = {'AUC': 'roc_auc'}\n\n# Create the grid\n#grid = GridSearchCV(mdl, gridParams, verbose=2, cv=5, scoring=scoring, n_jobs=-1, refit='AUC')\n# Run the grid\n\n\n#print('Best parameters found by grid search are:', grid.best_params_)\n#print('Best score found by grid search is:', grid.best_score_)\n","37b981ea":"yes = gbm.predict(X_test)","322b999d":"yess =[]\nfor i in yes:\n    if i>=0.5:\n        yess.append(1)\n    else:\n        yess.append(0)","53b4686a":"test1['gender'] = yess\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DDCCV.csv',index = False)","325a57dd":"import h2o\nh2o.init()\ntrain2 = h2o.H2OFrame(train1)\ntest2 = h2o.H2OFrame(X_test)\ntrain1.columns\ny = 'gender'\nx = train2.col_names\nx.remove(y)\ntrain2['gender'] = train2['gender'].asfactor()\ntrain2['gender'].levels()\nfrom h2o.automl import H2OAutoML\naml1 = H2OAutoML(max_models = 30, max_runtime_secs=200, seed = 1)\naml1.train(x = x, y = y, training_frame = train2)\npreds = aml1.predict(test2)\nprint(sample.columns)\ntest1['gender'] = preds\n#ans=h2o.as_list(preds) \n#sample['gender'] = ans['predict']\n#sample.to_csv('Solution_H2O(Divided).csv',index=False)\n#lb = aml.leaderboard\n#lb.head()\n#lb.head(rows=lb.nrows)","3317a89f":"test1['gender'] = (h2o.as_list(preds['predict']))","68dbdf50":"\ntestn = test1[['session_id','gender']]\nprint(testn.isna().sum())\ntest_final = testn.drop_duplicates(subset='session_id', keep='first', inplace=False)\ndic = {1:'male',0:'female'}\ntest_final['gender'] = test_final['gender'].map(dic)\ntest_final.to_csv('DH2o.csv',index = False)","f7f56c9e":"Changing the orders","bb143d95":"We could also see that there is some inconsitency in start and end time","2b09522d":"No null values","c6912ccf":"More females view then male","63c67728":"Gender - Male and Female\nUnique session id\n9402 unique products"}}