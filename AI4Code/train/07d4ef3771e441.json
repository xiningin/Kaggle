{"cell_type":{"cf1b0190":"code","3dfec997":"code","d391af7c":"code","65b4e84c":"code","fd2aad8c":"code","bac3370e":"code","7eeaec4d":"code","2aa86aec":"code","bcb9db84":"code","7dd99912":"code","97cf4c36":"code","8d83c471":"code","133ea3ff":"code","df0c530b":"code","9714c0d5":"code","62df14a5":"code","204e9e1e":"code","e859dd08":"code","cdcb8247":"code","6d3cdf50":"code","d952b0bf":"markdown","1ed0af2d":"markdown","0ce6f6c5":"markdown","ce37935a":"markdown","c483fbfc":"markdown","bd18e754":"markdown","a267f111":"markdown","fcbc5a61":"markdown"},"source":{"cf1b0190":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","3dfec997":"df = pd.read_csv('..\/input\/customer-segmentation-tutorial-in-python\/Mall_Customers.csv')\ndf","d391af7c":"X = df.drop(['CustomerID','Gender'],axis = 1)\nX","65b4e84c":"X","fd2aad8c":"sns.pairplot(X)","bac3370e":"from sklearn.cluster import KMeans","7eeaec4d":"num_of_clusters = []\nfor i in range(1,11):\n  km = KMeans(n_clusters=i).fit(X)\n  num_of_clusters.append(km.inertia_)\n","2aa86aec":"num_of_clusters","bcb9db84":"plt.figure(figsize=(16,8))\nsns.lineplot(x=np.arange(1,11),y=num_of_clusters)","7dd99912":"km3 = KMeans(n_clusters=3).fit(X)\nplt.figure(figsize=(12,8))\nsns.scatterplot(X['Annual Income (k$)'],X['Spending Score (1-100)'],hue=km3.labels_,palette=sns.color_palette('hls', 3))\n","97cf4c36":"km5 = KMeans(n_clusters=5).fit(X)\nplt.figure(figsize=(12,8))\nsns.scatterplot(X['Annual Income (k$)'],X['Spending Score (1-100)'],hue=km5.labels_,palette=sns.color_palette('hls', 5))\n","8d83c471":"from sklearn.cluster import MiniBatchKMeans","133ea3ff":"mbk = MiniBatchKMeans(n_clusters=5).fit(X)","df0c530b":"plt.figure(figsize=(12,8))\nsns.scatterplot(X['Annual Income (k$)'],X['Spending Score (1-100)'],hue=mbk.labels_,palette=sns.color_palette('hls', 5))\n","9714c0d5":"from sklearn.datasets import make_blobs\nfrom sklearn.cluster import AgglomerativeClustering\nimport scipy.cluster.hierarchy as sch","62df14a5":"df = make_blobs(n_samples=200,centers=4)\n","204e9e1e":"points =  df[0]\npoints","e859dd08":"plt.figure(figsize=(18,20))\ndednogram = sch.dendrogram(sch.linkage(points,method = 'ward'),orientation='right')","cdcb8247":"agg = AgglomerativeClustering(n_clusters=5).fit(X)","6d3cdf50":"plt.figure(figsize=(12,8))\nsns.scatterplot(X['Annual Income (k$)'],X['Spending Score (1-100)'],hue=agg.labels_,palette=sns.color_palette('hls', 5))","d952b0bf":"# Clustering","1ed0af2d":"As k-means has a good time performance. But with the increasing size of the datasets being analyzed, the computation time of K-means increases because of its constraint of needing the whole dataset in main memory. For this reason, several methods have been proposed to reduce the temporal and spatial cost of the algorithm. A different approach is the Mini batch K-means algorithm\n\nMini Batch K-means algorithm\u2018s main idea is to use small random batches of data of a fixed size, so they can be stored in memory. Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence. Each mini batch updates the clusters using a convex combination of the values of the prototypes and the data, applying a learning rate that decreases with the number of iterations. This learning rate is the inverse of the number of data assigned to a cluster during the process. As the number of iterations increases, the effect of new data is reduced, so convergence can be detected when no changes in the clusters occur in several consecutive iterations.\nThe empirical results suggest that it can obtain a substantial saving of computational time at the expense of some loss of cluster quality, but not extensive study of the algorithm has been done to measure how the characteristics of the datasets, such as the number of clusters or its size, affect the partition quality.","0ce6f6c5":"Clustering is a type of unsupervised learning technique and it's applied when there is nothing to predict but when the instances are to be divided into natural groups \"Data mining practical machine learning tools and techniques book\"\n\n","ce37935a":"From the plot we'll use the elpow method to determine the number of clusters and from the plot it can be 3 or 5\n","c483fbfc":"K_means algorithm is probably the most used clustering algorithm \nit starts by initializing the number of clusters  you want (There are methods to determine the number of clusters. ex: the elbow method)\n1) Let's say the number of clusters is 3, we assign 3 random samples to the clusters \n2) We measure the distance between each sample and the 3 clusters and assign each sample to their closest cluster\n3) We calculate the mean of each cluster \n4) then we repeat what we did, if the clusters didn't change then we're done","bd18e754":"# Hierarchical Cluster","a267f111":"The minibatch is faster but gives different results than kmeans","fcbc5a61":"**Hierarchical Cluster** is a clustering algorithm where you group similiar samples together and form a cluster.\n\nHierarchical clustering refers to a collection of clustering algorithms that all build upon the same principles: the algorithm starts by declaring each point its own cluster, and then merges the two most similar clusters until\nthey all form a one cluster\n\nIt works well with heatmaps\n"}}