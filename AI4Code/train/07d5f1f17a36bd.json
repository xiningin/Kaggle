{"cell_type":{"66f4ad6b":"code","5e02f589":"code","cff2b509":"code","19af6fa5":"code","257a2500":"code","5a390043":"code","bcd0d275":"code","d532225d":"code","bbf78f3e":"code","bf6c59d7":"code","978369b1":"code","32ab9caa":"code","08079307":"code","6cbaf4c7":"code","ba22c19b":"code","673a6633":"code","907cd1b6":"code","ee1308de":"code","0676772b":"code","596a95e3":"code","d5898bad":"code","54c234ea":"code","31c03f4a":"code","413af9f2":"code","8005c7ce":"code","111cff3e":"code","dc5033ff":"code","7a98666b":"markdown","0ce1ea89":"markdown","dbd74e9a":"markdown","8f5e4f24":"markdown","5ecf1d5f":"markdown","f68b6308":"markdown","fa82f9d4":"markdown","b703aa43":"markdown"},"source":{"66f4ad6b":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline\nplt.rcParams['figure.figsize'] = (12, 5);\nsns.set_style('whitegrid')","5e02f589":"root_dir = '..\/input\/contradictory-my-dear-watson'\ntrain_path = 'train.csv'\ntest_path = 'test.csv'\nsub_path = 'sample_submission.csv'","cff2b509":"train_df = pd.read_csv(os.path.join(root_dir, train_path))\ntest_df = pd.read_csv(os.path.join(root_dir, test_path))\ntrain_df.head()","19af6fa5":"sorted(train_df.language.unique()) == sorted(test_df.language.unique())","257a2500":"train_df.language.value_counts()","5a390043":"train_df.label.hist(color='orange')","bcd0d275":"train_df.isna().sum()","d532225d":"for each in ['premise', 'hypothesis']:\n    print(f'Mean symbols in {each}:', \n          train_df[each].apply(lambda x: len(x)).mean())\n    print(f'Maximum symbols in {each}:', \n          train_df[each].apply(lambda x: len(x)).max())\n    print(f'Minimum symbols in {each}:', \n          train_df[each].apply(lambda x: len(x)).min())\n    print(f'Median symbols in {each}:', \n          train_df[each].apply(lambda x: len(x)).median())","bbf78f3e":"for each in ['premise', 'hypothesis']:\n    print(f'Mean number of words in {each}:', \n          train_df[each].apply(lambda x: len(x.split(' '))).mean())\n    print(f'Maximum number of words in {each}:', \n          train_df[each].apply(lambda x: len(x.split(' '))).max())\n    print(f'Minimum number of words in {each}:', \n          train_df[each].apply(lambda x: len(x.split(' '))).min())\n    print(f'Median number of words in {each}:', \n          train_df[each].apply(lambda x: len(x.split(' '))).median())","bf6c59d7":"train_df['premise_len'] = train_df['premise'].apply(lambda x: len(x.split(' ')))\ntrain_df['hypothesis_len'] = train_df['hypothesis'].apply(lambda x: len(x.split(' ')))","978369b1":"fig, ax = plt.subplots(1, 3)\ntrain_df[train_df.label==0].premise_len.hist(ax=ax[0], color='gray', label='entailment', bins=10)\nax[0].legend();\ntrain_df[train_df.label==1].premise_len.hist(ax=ax[1], color='gold', label='neutral', bins=10)\nax[1].legend();\ntrain_df[train_df.label==2].premise_len.hist(ax=ax[2], color='olive', label='contradiction', bins=10)\nax[2].legend();","32ab9caa":"fig, ax = plt.subplots(1, 3)\ntrain_df[train_df.label==0].hypothesis_len.hist(ax=ax[0], color='gray', label='entailment', bins=10)\nax[0].legend();\ntrain_df[train_df.label==1].hypothesis_len.hist(ax=ax[1], color='gold', label='neutral', bins=10)\nax[1].legend();\ntrain_df[train_df.label==2].hypothesis_len.hist(ax=ax[2], color='olive', label='contradiction', bins=10)\nax[2].legend();","08079307":"lang_en = train_df[train_df.language=='English']\nlang_en.describe()","6cbaf4c7":"from nltk.probability import FreqDist\nfrom nltk.corpus import stopwords\nsw = stopwords.words('english')\n\nlang_en.loc[:, 'premise'] = lang_en['premise'].apply(lambda x: x.lower())\nlang_en.loc[:, 'hypothesis'] = lang_en['hypothesis'].apply(lambda x: x.lower())\n\np = ' '.join(lang_en['premise'].tolist()).split(' ')\nh = ' '.join(lang_en['hypothesis'].tolist()).split(' ')\nf_dist_p = FreqDist([x for x in p if x.replace('.', '') not in sw and len(x)>1])\nf_dist_h = FreqDist([x for x in h if x.replace('.', '') not in sw and len(x)>1])","ba22c19b":"p_common = f_dist_p.most_common(20)\nplt.bar([x[0] for x in p_common], [x[1] for x in p_common], \n        color='purple', label='most common in premise');\nplt.legend();","673a6633":"p_common = f_dist_h.most_common(20)\nplt.bar([x[0] for x in p_common], [x[1] for x in p_common], \n        color='purple', label='most common in hypothesis');\nplt.legend();","907cd1b6":"import spacy\nnlp = spacy.load('en')","ee1308de":"doc = nlp(lang_en.loc[17, 'premise'])\nspacy.displacy.render(doc, style='dep', options={'distance':80})","0676772b":"doc = nlp(lang_en.loc[17, 'hypothesis'])\nspacy.displacy.render(doc, style='dep', options={'distance':80})","596a95e3":"lang_en.loc[17, 'label']","d5898bad":"doc = nlp(lang_en.loc[321, 'premise'])\nspacy.displacy.render(doc, style='dep', options={'distance':60})","54c234ea":"doc = nlp(lang_en.loc[321, 'hypothesis'])\nspacy.displacy.render(doc, style='dep', options={'distance':60})","31c03f4a":"lang_en.loc[321, 'label']","413af9f2":"def leave_diff(string1, string2):\n    string1 = string1.lower().replace('.', '')\n    string2 = string2.lower().replace('.', '')\n    string1 = string1.replace(',', '')\n    string2 = string2.replace(',', '')\n    tokens1 = string1.split(' ')\n    tokens2 = string2.split(' ')\n    diff = set(tokens1).difference(tokens2)\n    return ' '.join(list(diff))","8005c7ce":"for i in lang_en.index:\n    lang_en.loc[i, 'diff'] = leave_diff(\n        lang_en.loc[i, 'premise'], lang_en.loc[i, 'hypothesis'])\nlang_en.head()","111cff3e":"lang_en.loc[7, ['premise', 'hypothesis', 'diff']]","dc5033ff":"lang_en[lang_en['diff']=='']['label'].value_counts()","7a98666b":"That seems about right. My hypothesis though is that keywords and presence of a negating words will be quite predictive of the label. Before we boil the ocean with compute let's try another example.","0ce1ea89":"## Challange\n\nWe are challanged in this competition to use all available NLP tools to \"teach\" an algorithm the way we do... at least should. This is too idealistic to say we will teach a CPU to understand language humans speak but Natural Language Processing techniques help us to translate the problem into the one machine can understand. \n\nBefore jumping into crunching the numbers though let us take a look at the given data and see what conclusions we can get from it.","dbd74e9a":"It seems hypothesis have much wider distribution despite having less words in the sentence on average. Still these are not really good metrics when dealing with several languages. As such Spanish is usually considered to be more verbose language overall. Let's concentrate on each language individually. That is where we will start with in the next version of this notebook.","8f5e4f24":"To be honest this is not surprising to have English language being dominating dataset.","5ecf1d5f":"There is not much we can draw from regular data so let's jump into texts processing, we start simple.","f68b6308":"Notice here: usual simple bag-of-words approach certainly is will not be a choice. But even though we are encouraged to use TPU here we should exhaust simple approaches before boiling the ocean with compute, right?","fa82f9d4":"Yeah, that could not be that easy. Nevertheless useful insight we should take into account meaning of the words, especialy those which differ between the sentences.","b703aa43":"Good to know: we have the same set of languages in both test and training sets."}}