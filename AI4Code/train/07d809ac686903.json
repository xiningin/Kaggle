{"cell_type":{"cce4e892":"code","564fab92":"code","444ea319":"code","fcae699e":"code","0b6e80b5":"code","4c4699b0":"code","302084f0":"code","489452ca":"code","5583d4a1":"code","e65f43f2":"code","0e098201":"code","f8ecead6":"code","c415956b":"code","13264f77":"code","07f429c6":"code","5c726fcd":"code","4472404b":"code","bd7b17d7":"code","ba4ddb27":"code","c16aabc7":"code","acb2bb4f":"code","b31630e3":"code","8369c685":"code","5d11184f":"code","51b013f0":"code","72204837":"code","b1fff011":"code","455fa006":"code","ee3f0edb":"code","219d854c":"code","8e208c5e":"code","ca565342":"code","7542e701":"code","40d4b61e":"markdown","3fba5636":"markdown","661d7291":"markdown","b6294ece":"markdown","4cf5ba77":"markdown","444e032f":"markdown"},"source":{"cce4e892":"import numpy as np\nimport pandas as pd\nimport warnings\nimport re\nimport nltk\nfrom nltk import word_tokenize\nfrom nltk.tokenize import sent_tokenize\nfrom textblob import TextBlob\nimport string\nfrom string import punctuation\nfrom nltk.corpus import stopwords\nfrom statistics import mean\nfrom heapq import nlargest\nfrom wordcloud import WordCloud\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nstop_words = set(stopwords.words('english'))\npunctuation = punctuation + '\\n' + '\u2014' + '\u201c' + ',' + '\u201d' + '\u2018' + '-' + '\u2019'\nwarnings.filterwarnings('ignore')","564fab92":"# Importing the dataset\ndf_1 = pd.read_csv(\"\/kaggle\/input\/all-the-news\/articles1.csv\")\ndf_2 = pd.read_csv(\"\/kaggle\/input\/all-the-news\/articles2.csv\")\ndf_3 = pd.read_csv(\"\/kaggle\/input\/all-the-news\/articles3.csv\")","444ea319":"# Checking if the columns are same or not\ndf_1.columns == df_2.columns","fcae699e":"# Checking if the columns are same or not\ndf_2.columns == df_3.columns","0b6e80b5":"# Making one Dataframe by appending all of them for the further process\nd = [df_1, df_2, df_3]\ndf = pd.concat(d, keys = ['x', 'y', 'z'])\ndf.rename(columns = {'content' : 'article'}, inplace = True);","4c4699b0":"df.head(10)","302084f0":"# Shape of the dataset\nprint (\"The shape of the dataset : \", df.shape)","489452ca":"# Dropping the unnecessary columns\ndf.drop(columns = ['Unnamed: 0'], inplace = True)\ndf.head()","5583d4a1":"df.head()","e65f43f2":"# Replacing the unnecessary row value of year with it's actual values\ndf['year'] = df['year'].replace(\"https:\/\/www.washingtonpost.com\/outlook\/tale-of-a-woman-who-died-and-a-woman-who-killed-in-the-northern-ireland-conflict\/2019\/03\/08\/59e75dd4-2ecd-11e9-8ad3-9a5b113ecd3c_story.html\", 2019)","0e098201":"# Years\ndf['year'].value_counts()","f8ecead6":"# Countplot shows the distribution of the articles according to the year\nplt.rcParams['figure.figsize'] = [15, 8]\nsns.set(font_scale = 1.2, style = 'whitegrid')\nsns_year = sns.countplot(df['year'], color = 'darkcyan')\nsns_year.set(xlabel = \"Year\", ylabel = \"Count\", title = \"Distribution of the articles according to the year\")","c415956b":"# Authors\ndf['author'].value_counts()","13264f77":"# Changing the value \"The Associated Press\" to \"Associated Press\"\ndf['author'] = df['author'].replace(\"The Associated Press\", \"Associated Press\")","07f429c6":"# Top 100 authors\ndf['author'].value_counts()[0:100]","5c726fcd":"# Barplot showing the top 50 Authors\nplt.rcParams['figure.figsize'] = [20, 10]\nauthor_count = df['author'].value_counts()[0:50]\nauthor_top_50 = sns.barplot(x = list(author_count.index), y = (author_count.values), color = 'orange')\nauthor_top_50.set(xlabel = \"Authors\", ylabel = \"Published Articles\", title = \"Top 50 Authors\")\nplt.setp(author_top_50.get_xticklabels(), rotation = 90);","4472404b":"# Countplot shows the distribution of the Publications\nplt.rcParams['figure.figsize'] = [15, 10]\nsns.set(font_scale = 1.2, style = 'whitegrid')\nsns_pub = sns.countplot(df['publication'], color = 'slategrey')\nsns_pub.set(xlabel = \"Publications\", ylabel = \"Count\", title = \"Distribution of the Publications across the dataset\")\nplt.setp(sns_pub.get_xticklabels(), rotation = 90);","bd7b17d7":"contractions_dict = { \n\"ain't\": \"am not\",\n\"aren't\": \"are not\",\n\"can't\": \"cannot\",\n\"can't've\": \"cannot have\",\n\"'cause\": \"because\",\n\"could've\": \"could have\",\n\"couldn't\": \"could not\",\n\"couldn't've\": \"could not have\",\n\"didn't\": \"did not\",\n\"doesn't\": \"does not\",\n\"doesn\u2019t\": \"does not\",\n\"don't\": \"do not\",\n\"don\u2019t\": \"do not\",\n\"hadn't\": \"had not\",\n\"hadn't've\": \"had not have\",\n\"hasn't\": \"has not\",\n\"haven't\": \"have not\",\n\"he'd\": \"he had\",\n\"he'd've\": \"he would have\",\n\"he'll\": \"he will\",\n\"he'll've\": \"he will have\",\n\"he's\": \"he is\",\n\"how'd\": \"how did\",\n\"how'd'y\": \"how do you\",\n\"how'll\": \"how will\",\n\"how's\": \"how is\",\n\"i'd\": \"i would\",\n\"i'd've\": \"i would have\",\n\"i'll\": \"i will\",\n\"i'll've\": \"i will have\",\n\"i'm\": \"i am\",\n\"i've\": \"i have\",\n\"isn't\": \"is not\",\n\"it'd\": \"it would\",\n\"it'd've\": \"it would have\",\n\"it'll\": \"it will\",\n\"it'll've\": \"it will have\",\n\"it's\": \"it is\",\n\"let's\": \"let us\",\n\"ma'am\": \"madam\",\n\"mayn't\": \"may not\",\n\"might've\": \"might have\",\n\"mightn't\": \"might not\",\n\"mightn't've\": \"might not have\",\n\"must've\": \"must have\",\n\"mustn't\": \"must not\",\n\"mustn't've\": \"must not have\",\n\"needn't\": \"need not\",\n\"needn't've\": \"need not have\",\n\"o'clock\": \"of the clock\",\n\"oughtn't\": \"ought not\",\n\"oughtn't've\": \"ought not have\",\n\"shan't\": \"shall not\",\n\"sha'n't\": \"shall not\",\n\"shan't've\": \"shall not have\",\n\"she'd\": \"she would\",\n\"she'd've\": \"she would have\",\n\"she'll\": \"she will\",\n\"she'll've\": \"she will have\",\n\"she's\": \"she is\",\n\"should've\": \"should have\",\n\"shouldn't\": \"should not\",\n\"shouldn't've\": \"should not have\",\n\"so've\": \"so have\",\n\"so's\": \"so is\",\n\"that'd\": \"that would\",\n\"that'd've\": \"that would have\",\n\"that's\": \"that is\",\n\"there'd\": \"there would\",\n\"there'd've\": \"there would have\",\n\"there's\": \"there is\",\n\"they'd\": \"they would\",\n\"they'd've\": \"they would have\",\n\"they'll\": \"they will\",\n\"they'll've\": \"they will have\",\n\"they're\": \"they are\",\n\"they've\": \"they have\",\n\"to've\": \"to have\",\n\"wasn't\": \"was not\",\n\"we'd\": \"we would\",\n\"we'd've\": \"we would have\",\n\"we'll\": \"we will\",\n\"we'll've\": \"we will have\",\n\"we're\": \"we are\",\n\"we've\": \"we have\",\n\"weren't\": \"were not\",\n\"what'll\": \"what will\",\n\"what'll've\": \"what will have\",\n\"what're\": \"what are\",\n\"what's\": \"what is\",\n\"what've\": \"what have\",\n\"when's\": \"when is\",\n\"when've\": \"when have\",\n\"where'd\": \"where did\",\n\"where's\": \"where is\",\n\"where've\": \"where have\",\n\"who'll\": \"who will\",\n\"who'll've\": \"who will have\",\n\"who's\": \"who is\",\n\"who've\": \"who have\",\n\"why's\": \"why is\",\n\"why've\": \"why have\",\n\"will've\": \"will have\",\n\"won't\": \"will not\",\n\"won't've\": \"will not have\",\n\"would've\": \"would have\",\n\"wouldn't\": \"would not\",\n\"wouldn't've\": \"would not have\",\n\"y'all\": \"you all\",\n\"y\u2019all\": \"you all\",\n\"y'all'd\": \"you all would\",\n\"y'all'd've\": \"you all would have\",\n\"y'all're\": \"you all are\",\n\"y'all've\": \"you all have\",\n\"you'd\": \"you would\",\n\"you'd've\": \"you would have\",\n\"you'll\": \"you will\",\n\"you'll've\": \"you will have\",\n\"you're\": \"you are\",\n\"you've\": \"you have\",\n\"ain\u2019t\": \"am not\",\n\"aren\u2019t\": \"are not\",\n\"can\u2019t\": \"cannot\",\n\"can\u2019t\u2019ve\": \"cannot have\",\n\"\u2019cause\": \"because\",\n\"could\u2019ve\": \"could have\",\n\"couldn\u2019t\": \"could not\",\n\"couldn\u2019t\u2019ve\": \"could not have\",\n\"didn\u2019t\": \"did not\",\n\"doesn\u2019t\": \"does not\",\n\"don\u2019t\": \"do not\",\n\"don\u2019t\": \"do not\",\n\"hadn\u2019t\": \"had not\",\n\"hadn\u2019t\u2019ve\": \"had not have\",\n\"hasn\u2019t\": \"has not\",\n\"haven\u2019t\": \"have not\",\n\"he\u2019d\": \"he had\",\n\"he\u2019d\u2019ve\": \"he would have\",\n\"he\u2019ll\": \"he will\",\n\"he\u2019ll\u2019ve\": \"he will have\",\n\"he\u2019s\": \"he is\",\n\"how\u2019d\": \"how did\",\n\"how\u2019d\u2019y\": \"how do you\",\n\"how\u2019ll\": \"how will\",\n\"how\u2019s\": \"how is\",\n\"i\u2019d\": \"i would\",\n\"i\u2019d\u2019ve\": \"i would have\",\n\"i\u2019ll\": \"i will\",\n\"i\u2019ll\u2019ve\": \"i will have\",\n\"i\u2019m\": \"i am\",\n\"i\u2019ve\": \"i have\",\n\"isn\u2019t\": \"is not\",\n\"it\u2019d\": \"it would\",\n\"it\u2019d\u2019ve\": \"it would have\",\n\"it\u2019ll\": \"it will\",\n\"it\u2019ll\u2019ve\": \"it will have\",\n\"it\u2019s\": \"it is\",\n\"let\u2019s\": \"let us\",\n\"ma\u2019am\": \"madam\",\n\"mayn\u2019t\": \"may not\",\n\"might\u2019ve\": \"might have\",\n\"mightn\u2019t\": \"might not\",\n\"mightn\u2019t\u2019ve\": \"might not have\",\n\"must\u2019ve\": \"must have\",\n\"mustn\u2019t\": \"must not\",\n\"mustn\u2019t\u2019ve\": \"must not have\",\n\"needn\u2019t\": \"need not\",\n\"needn\u2019t\u2019ve\": \"need not have\",\n\"o\u2019clock\": \"of the clock\",\n\"oughtn\u2019t\": \"ought not\",\n\"oughtn\u2019t\u2019ve\": \"ought not have\",\n\"shan\u2019t\": \"shall not\",\n\"sha\u2019n\u2019t\": \"shall not\",\n\"shan\u2019t\u2019ve\": \"shall not have\",\n\"she\u2019d\": \"she would\",\n\"she\u2019d\u2019ve\": \"she would have\",\n\"she\u2019ll\": \"she will\",\n\"she\u2019ll\u2019ve\": \"she will have\",\n\"she\u2019s\": \"she is\",\n\"should\u2019ve\": \"should have\",\n\"shouldn\u2019t\": \"should not\",\n\"shouldn\u2019t\u2019ve\": \"should not have\",\n\"so\u2019ve\": \"so have\",\n\"so\u2019s\": \"so is\",\n\"that\u2019d\": \"that would\",\n\"that\u2019d\u2019ve\": \"that would have\",\n\"that\u2019s\": \"that is\",\n\"there\u2019d\": \"there would\",\n\"there\u2019d\u2019ve\": \"there would have\",\n\"there\u2019s\": \"there is\",\n\"they\u2019d\": \"they would\",\n\"they\u2019d\u2019ve\": \"they would have\",\n\"they\u2019ll\": \"they will\",\n\"they\u2019ll\u2019ve\": \"they will have\",\n\"they\u2019re\": \"they are\",\n\"they\u2019ve\": \"they have\",\n\"to\u2019ve\": \"to have\",\n\"wasn\u2019t\": \"was not\",\n\"we\u2019d\": \"we would\",\n\"we\u2019d\u2019ve\": \"we would have\",\n\"we\u2019ll\": \"we will\",\n\"we\u2019ll\u2019ve\": \"we will have\",\n\"we\u2019re\": \"we are\",\n\"we\u2019ve\": \"we have\",\n\"weren\u2019t\": \"were not\",\n\"what\u2019ll\": \"what will\",\n\"what\u2019ll\u2019ve\": \"what will have\",\n\"what\u2019re\": \"what are\",\n\"what\u2019s\": \"what is\",\n\"what\u2019ve\": \"what have\",\n\"when\u2019s\": \"when is\",\n\"when\u2019ve\": \"when have\",\n\"where\u2019d\": \"where did\",\n\"where\u2019s\": \"where is\",\n\"where\u2019ve\": \"where have\",\n\"who\u2019ll\": \"who will\",\n\"who\u2019ll\u2019ve\": \"who will have\",\n\"who\u2019s\": \"who is\",\n\"who\u2019ve\": \"who have\",\n\"why\u2019s\": \"why is\",\n\"why\u2019ve\": \"why have\",\n\"will\u2019ve\": \"will have\",\n\"won\u2019t\": \"will not\",\n\"won\u2019t\u2019ve\": \"will not have\",\n\"would\u2019ve\": \"would have\",\n\"wouldn\u2019t\": \"would not\",\n\"wouldn\u2019t\u2019ve\": \"would not have\",\n\"y\u2019all\": \"you all\",\n\"y\u2019all\": \"you all\",\n\"y\u2019all\u2019d\": \"you all would\",\n\"y\u2019all\u2019d\u2019ve\": \"you all would have\",\n\"y\u2019all\u2019re\": \"you all are\",\n\"y\u2019all\u2019ve\": \"you all have\",\n\"you\u2019d\": \"you would\",\n\"you\u2019d\u2019ve\": \"you would have\",\n\"you\u2019ll\": \"you will\",\n\"you\u2019ll\u2019ve\": \"you will have\",\n\"you\u2019re\": \"you are\",\n\"you\u2019re\": \"you are\",\n\"you\u2019ve\": \"you have\",\n}\ncontractions_re = re.compile('(%s)' % '|'.join(contractions_dict.keys()))","ba4ddb27":"# Function to clean the html from the article\ndef cleanhtml(raw_html):\n    cleanr = re.compile('<.*?>')\n    cleantext = re.sub(cleanr, '', raw_html)\n    return cleantext\n\n# Function expand the contractions if there's any\ndef expand_contractions(s, contractions_dict=contractions_dict):\n    def replace(match):\n        return contractions_dict[match.group(0)]\n    return contractions_re.sub(replace, s)\n\n# Function to preprocess the articles\ndef preprocessing(article):\n    global article_sent\n    \n    # Converting to lowercase\n    article = article.str.lower()\n    \n    # Removing the HTML\n    article = article.apply(lambda x: cleanhtml(x))\n    \n    # Removing the email ids\n    article = article.apply(lambda x: re.sub('\\S+@\\S+','', x))\n    \n    # Removing The URLS\n    article = article.apply(lambda x: re.sub(\"((http\\:\/\/|https\\:\/\/|ftp\\:\/\/)|(www.))+(([a-zA-Z0-9\\.-]+\\.[a-zA-Z]{2,4})|([0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}\\.[0-9]{1,3}))(\/[a-zA-Z0-9%:\/-_\\?\\.'~]*)?\",'', x))\n    \n    # Removing the '\\xa0'\n    article = article.apply(lambda x: x.replace(\"\\xa0\", \" \"))\n    \n    # Removing the contractions\n    article = article.apply(lambda x: expand_contractions(x))\n    \n    # Stripping the possessives\n    article = article.apply(lambda x: x.replace(\"'s\", ''))\n    article = article.apply(lambda x: x.replace('\u2019s', ''))\n    article = article.apply(lambda x: x.replace(\"\\'s\", ''))\n    article = article.apply(lambda x: x.replace(\"\\\u2019s\", ''))\n    \n    # Removing the Trailing and leading whitespace and double spaces\n    article = article.apply(lambda x: re.sub(' +', ' ',x))\n    \n    # Copying the article for the sentence tokenization\n    article_sent = article.copy()\n    \n    # Removing punctuations from the article\n    article = article.apply(lambda x: ''.join(word for word in x if word not in punctuation))\n    \n    # Removing the Trailing and leading whitespace and double spaces again as removing punctuation might\n    # Lead to a white space\n    article = article.apply(lambda x: re.sub(' +', ' ',x))\n    \n    # Removing the Stopwords\n    article = article.apply(lambda x: ' '.join(word for word in x.split() if word not in stop_words))\n    \n    return article\n\n# Function to normalize the word frequency which is used in the function word_frequency\ndef normalize(li_word):\n    global normalized_freq\n    normalized_freq = []\n    for dictionary in li_word:\n        max_frequency = max(dictionary.values())\n        for word in dictionary.keys():\n            dictionary[word] = dictionary[word]\/max_frequency\n        normalized_freq.append(dictionary)\n    return normalized_freq\n\n# Function to calculate the word frequency\ndef word_frequency(article_word):\n    word_frequency = {}\n    li_word = []\n    for sentence in article_word:\n        for word in word_tokenize(sentence):\n            if word not in word_frequency.keys():\n                word_frequency[word] = 1\n            else:\n                word_frequency[word] += 1\n        li_word.append(word_frequency)\n        word_frequency = {}\n    normalize(li_word)\n    return normalized_freq\n\n# Function to Score the sentence which is called in the function sent_token\ndef sentence_score(li):\n    global sentence_score_list\n    sentence_score = {}\n    sentence_score_list = []\n    for list_, dictionary in zip(li, normalized_freq):\n        for sent in list_:\n            for word in word_tokenize(sent):\n                if word in dictionary.keys():\n                    if sent not in sentence_score.keys():\n                        sentence_score[sent] = dictionary[word]\n                    else:\n                        sentence_score[sent] += dictionary[word]\n        sentence_score_list.append(sentence_score)\n        sentence_score = {}\n    return sentence_score_list\n\n# Function to tokenize the sentence\ndef sent_token(article_sent):\n    sentence_list = []\n    sent_token = []\n    for sent in article_sent:\n        token = sent_tokenize(sent)\n        for sentence in token:\n            token_2 = ''.join(word for word in sentence if word not in punctuation)\n            token_2 = re.sub(' +', ' ',token_2)\n            sent_token.append(token_2)\n        sentence_list.append(sent_token)\n        sent_token = []\n    sentence_score(sentence_list)\n    return sentence_score_list\n\n# Function which generates the summary of the articles (This uses the 20% of the sentences with the highest score)\ndef summary(sentence_score_OwO):\n    summary_list = []\n    for summ in sentence_score_OwO:\n        select_length = int(len(summ)*0.25)\n        summary_ = nlargest(select_length, summ, key = summ.get)\n        summary_list.append(\".\".join(summary_))\n    return summary_list\n\n# This Function can be used to generate the summary which uses the mean sentence score\n#def summary(sentence_score_OwO):\n#    summary_list = []\n#    li_sen = []\n#    for summ in sentence_score_OwO:\n#        for sent, score in summ.items():\n#            threshold_score = mean(list(summ.values()))\n#            if score >= threshold_score:\n#                li_sen.append(sent)\n#            else:\n#                continue\n#        sugoi = ', '.join(li_sen)\n#        summary_list.append(sugoi)\n#        li_sen = []\n#    return summary_list\n\n# Functions to change the article string (if passed) to change it to generate a pandas series\ndef make_series(art):\n    global dataframe\n    data_dict = {'article' : [art]}\n    dataframe = pd.DataFrame(data_dict)['article']\n    return dataframe\n\n# Function which is to be called to generate the summary which in further calls other functions alltogether\ndef article_summarize(artefact):\n    \n    if type(artefact) != pd.Series:\n        artefact = make_series(artefact)\n    \n    df = preprocessing(artefact)\n    \n    word_normalization = word_frequency(df)\n    \n    sentence_score_OwO = sent_token(article_sent)\n    \n    summarized_article = summary(sentence_score_OwO)\n    \n    return summarized_article","c16aabc7":"# Generating the Word Cloud of the article using the preprocessing and make_series function mentioned below\ndef word_cloud(art):\n    art_ = make_series(art)\n    OwO = preprocessing(art_)\n    wordcloud_ = WordCloud(height = 500, width = 1000, background_color = 'white').generate(art)\n    plt.figure(figsize=(15, 10))\n    plt.imshow(wordcloud_, interpolation='bilinear')\n    plt.axis('off');","acb2bb4f":"# Generating the summaries for the first 100 articles\nsummaries = article_summarize(df['article'][0:100])","b31630e3":"print (\"The Actual length of the article is : \", len(df['article'][0]))\ndf['article'][0]","8369c685":"print (\"The length of the summarized article is : \", len(summaries[0]))\nsummaries[0]","5d11184f":"word_cloud(df['article'][0])","51b013f0":"print (\"The Actual length of the article is : \", len(df['article'][50]))\ndf['article'][50]","72204837":"print (\"The length of the summarized article is : \", len(summaries[50]))\nsummaries[50]","b1fff011":"word_cloud(df['article'][50])","455fa006":"print (\"The Actual length of the article is : \", len(df['article'][99]))\ndf['article'][99]","ee3f0edb":"print (\"The length of the summarized article is : \", len(summaries[99]))\nsummaries[99]","219d854c":"word_cloud(df['article'][99])","8e208c5e":"print (\"The Actual length of the article is : \", len(df['article'][75]))\ndf['article'][75]","ca565342":"print (\"The length of the summarized article is : \", len(summaries[75]))\nsummaries[75]","7542e701":"word_cloud(df['article'][75])","40d4b61e":"### Importing the required Libraries","3fba5636":"Since the content of the article are in a pandas series so we need to change them into a single string","661d7291":"### Exploratory Data Analysis","b6294ece":"### Examples","4cf5ba77":"### Making the Article Summarizer","444e032f":"# Article Summarizer using NLP"}}