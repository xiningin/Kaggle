{"cell_type":{"5cbfdcf5":"code","794a8e0c":"code","8baebc02":"code","ea0d2584":"code","35cfd001":"code","5d1097a3":"code","ca4e9e47":"code","9f480dc3":"code","0eb35180":"code","65a760f6":"code","559dab0b":"code","56787bba":"code","3fc25889":"code","7fedb6e8":"code","d550c064":"code","6cf55c6e":"code","f7a954ef":"code","ffba43ec":"code","868111aa":"code","85380812":"code","61c9c1ba":"code","21370924":"code","b2a6060c":"code","861c12bf":"code","2532b5cc":"code","87d69c60":"code","46e6a7c5":"code","25513a59":"code","78c596ff":"code","b2337328":"code","860c31ef":"code","86fe3fc0":"code","e9abd8fb":"code","87b911fb":"code","20a803e7":"code","189949b7":"code","431e5166":"code","a27a37aa":"code","641ff6df":"code","99cbd271":"code","0b5ca29a":"code","0583c280":"code","f3b8e98c":"code","13a18973":"code","10fe7cc5":"code","1b6c2386":"code","8c2f2b23":"markdown","5ab8af9d":"markdown","2b17eede":"markdown","ea7d3f03":"markdown","bf2f9487":"markdown","77b228ad":"markdown","a5cc9bdf":"markdown","bb0a82ce":"markdown","a55b8ce4":"markdown","ebeb3ff3":"markdown","a76f3d08":"markdown","56ac7b3a":"markdown","5b3ae6d6":"markdown","c723a603":"markdown","e4bae3f3":"markdown","1ccf2c58":"markdown","f14fa94f":"markdown","11e05b24":"markdown","931bb45d":"markdown","ecae4a8d":"markdown","a2db4928":"markdown","f0a0e67d":"markdown","604395e1":"markdown","8eb4670c":"markdown","6a4333c3":"markdown","07c9316b":"markdown","ed2c3804":"markdown","50d8c4c1":"markdown","ff5dc544":"markdown","37fd25c6":"markdown","094a77c1":"markdown","e773c6b5":"markdown","2c142f43":"markdown","4c2e619b":"markdown","068aeb86":"markdown","2ee64d3c":"markdown","894b68ac":"markdown","c04ead0a":"markdown","ba9a63f3":"markdown","6cda27f6":"markdown","10971c9b":"markdown"},"source":{"5cbfdcf5":"import spacy\nimport en_core_web_sm\nimport networkx as nx\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom tqdm import tqdm\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, accuracy_score, classification_report\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom bokeh.io import show, output_file\nfrom bokeh.models import Plot, Range1d, MultiLine, Circle, HoverTool, TapTool, BoxSelectTool, BoxZoomTool, ResetTool\nfrom bokeh.models.graphs import from_networkx, NodesAndLinkedEdges, EdgesAndLinkedNodes\nfrom bokeh.io import output_file,show,output_notebook,push_notebook\n\nfrom bokeh.palettes import Spectral4\n\n\n%matplotlib inline","794a8e0c":"path = \"..\/input\/corpus.txt\"\nwith open(path) as f:\n    content = f.readlines()\n# you may also want to remove whitespace characters like `\\n` at the end of each line\ncontent = [x.strip() for x in content] \ncontent[:20]","8baebc02":"speaker_list = []\ntext_list = []\nfor c in content:\n    temp = c.split(':')\n    speaker = temp[0].split()[0]\n#     if (speaker[0] == '(') or (speaker[0] == '['):\n    if not (speaker[0].isalpha()):\n        speaker_list.append(\"SEP\")\n        text_list.append(\"SEP\")\n        continue\n    if 'INT' in speaker:\n        speaker_list.append(\"SEP\")\n        text_list.append(\"SEP\")\n        continue\n    if len(temp) < 2:\n        try:\n            text = temp[0].split(' ', 1)[1]\n        except:\n            continue\n    else:\n        text =temp[1]\n    speaker_list.append(speaker)\n    text_list.append(text)\ndf = pd.DataFrame(columns=['speaker', 'text'])\ndf['speaker'] = speaker_list\ndf['text'] = text_list\ndf.iloc[:20]","ea0d2584":"counter = df.copy()\ndrop_list = counter[counter['speaker'] == 'SEP'].index\ncounter = counter.drop(drop_list)\ncounter = counter.groupby(['speaker'], as_index = False).count()\ncounter = counter.sort_values('text', ascending= False)\ncounter.columns = ['speaker','lines count']\ncounter = counter.reset_index(drop = True)\ncounter.iloc[:20]","35cfd001":"plt.figure(figsize = (16, 16), facecolor = None)\nsns.set_palette(\"Paired\")\nsns.barplot(x=\"speaker\" ,y=\"lines count\", data=counter.iloc[:20])\nplt.show()","5d1097a3":"plt.figure(figsize = (8, 8), facecolor = None)\nsns.barplot(x=\"speaker\" ,y=\"lines count\", data=counter.iloc[:4])\nplt.show()","ca4e9e47":"nlp = en_core_web_sm.load()\ndef extract_name(text):\n    entities = []\n    doc = nlp(text)\n    for entity in doc.ents:\n        label = entity.label_\n        if label != 'PERSON':\n            continue\n        e = entity.text.upper().strip()\n        if len(e) < 3:\n            continue\n        entities.append(e)\n    return entities\ndf['ppl'] = df['text'].apply(extract_name)\ndf.iloc[:-15]","9f480dc3":"df['person2'] = '-'\nfor index , row in df.iterrows():\n    if row['speaker'] == 'SEP':\n        continue\n    if index+1 == len(df):\n        continue\n    next_preson = df.iloc[index + 1]['speaker']\n    if next_preson == 'SEP':\n        continue\n    if next_preson == row['speaker']:\n        continue\n    df.at[index, 'person2'] = next_preson\ndf.iloc[:10]","0eb35180":"drop_list1 = df[df['speaker'] == 'SEP'].index\ndf2 = df.drop(drop_list1)\ndrop_list2 = df2[df2['person2'] == '-'].index\ndf2 = df2.drop(drop_list2)\ndf2 = df2.reset_index()\ndf2 = df2[['speaker','person2']]\ndf2.columns = ['p1', 'p2']\ndf2.iloc[:10]","65a760f6":"df2['count'] = 0\ndf2 = df2.groupby(['p1', 'p2'], as_index = False).count()\ndf2 = df2.sort_values('count', ascending = False)\ndf2.iloc[:10]","559dab0b":"df2 = df2[df2['count'] > 3]\ndf2.iloc[:10]","56787bba":"G = nx.Graph()\n\nfor index , row in df2.iterrows():\n    G.add_edge(row['p1'],row['p2'] , weight=row['count'])\n\nprint(nx.info(G))","3fc25889":"plt.figure(figsize = (20, 20), facecolor = None)\nnx.draw_kamada_kawai(G, with_labels=True)","7fedb6e8":"df2 = df2[df2['count'] > 15]\nG = nx.Graph()\n\nfor index , row in df2.iterrows():\n    G.add_edge(row['p1'],row['p2'] , weight=row['count'])\n    \nplt.figure(figsize = (20, 20), facecolor = None)\npos=nx.spring_layout(G)\nnx.draw(G, pos, node_size=300, with_labels=True)","d550c064":"output_notebook()\n\ncolor_dict = {\"JERRY\": \"red\",\"GEORGE\": \"blue\",\"KRAMER\": \"black\",\"ELAINE\": \"green\"}\nedge_attrs = {}\n\nfor start_node, end_node, _ in tqdm(G.edges(data=True)):\n    if start_node in color_dict:\n        edge_color = color_dict[start_node]\n    else:\n        edge_color = \"orange\"\n    edge_attrs[(start_node, end_node)] = edge_color\n\nnx.set_edge_attributes(G, edge_attrs, \"edge_color\")\n\nplot = Plot(plot_width=800, plot_height=800,\n            x_range=Range1d(-1.1,1.1), y_range=Range1d(-1.1,1.1))\n\nplot.title.text = \"Graph Interaction Seinfeld\"\n\nnode_hover_tool = HoverTool(tooltips=[(\"Name\", \"@index\")])\n\nplot.add_tools(node_hover_tool, BoxZoomTool(), ResetTool())\n\ngraph_renderer = from_networkx(G, nx.spring_layout, scale=1, center=(0, 0))\n\ngraph_renderer.node_renderer.glyph = Circle(size=15, fill_color=Spectral4[2])\ngraph_renderer.edge_renderer.glyph = MultiLine(line_color=\"edge_color\", line_alpha=0.8, line_width=1)\nplot.renderers.append(graph_renderer)\n\noutput_file(\"interactive_graphs.html\")\nshow(plot,notebook_handle=True) \n","6cf55c6e":"df3 = df[['speaker', 'text']]\ndrop_list1 = df3[df3['speaker'] == 'SEP'].index\ndf3 = df3.drop(drop_list1)\ndf3 = df3.reset_index(drop = True)\ndf3 = df3[df3['speaker'].isin({'JERRY' , 'GEORGE', 'KRAMER', 'ELAINE'})]\ndf3.iloc[:10]","f7a954ef":"X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['speaker'], test_size=0.2, random_state=124)\nres_list = pd.DataFrame(columns=['Method', 'Model', 'Acc'])","ffba43ec":"def fit_evaluate(x_train, y_train, x_test, y_test, print_report = False):\n    rf = RandomForestClassifier(n_estimators = 25)\n    ada = AdaBoostClassifier(n_estimators = 25)\n    rf.fit(x_train, y_train)\n    ada.fit(x_train, y_train)\n    pred1 = rf.predict(x_test)\n    pred2 = ada.predict(x_test)\n    score_rf = accuracy_score(y_test, pred1)\n    # auc = roc_auc_score(y_test, pred1)\n    print(\"Random forest \\n Accuracy:\", score_rf)\n    if print_report:\n        print(classification_report(y_test, pred1))\n    score_ada = accuracy_score(y_test, pred2)\n    print(\"Adaboost \\n Accuracy:\", score_ada)\n    if print_report:\n        print(classification_report(y_test, pred2))\n    return score_rf, score_ada","868111aa":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='char', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test, print_report = True)\nres_list = res_list.append({'Method':'tfidf - char, ngram 1', 'Model': 'RF', 'Acc': s1}, ignore_index=True)\nres_list = res_list.append({'Method':'tfidf - char, ngram 1', 'Model': 'Ada', 'Acc': s2}, ignore_index=True)","85380812":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(2, 3), stop_words='english', strip_accents='unicode',analyzer='char', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\nres_list = res_list.append({'Method':'tfidf - char, ngram (2,3)', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'tfidf - char, ngram (2,3)', 'Model': 'Ada', 'Acc': s2},ignore_index=True)\n","61c9c1ba":"tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\nres_list = res_list.append({'Method':'tfidf - word, ngram 1', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'tfidf - word, ngram 1', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","21370924":"count_vectorizer = CountVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',\n                                   analyzer='word')\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\ns1, s2 = fit_evaluate(count_train, y_train, count_test, y_test)\nres_list = res_list.append({'Method':'Count - Word, ngram 1', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'Count - Word, ngram 1', 'Model': 'Ada', 'Acc': s2},ignore_index=True)\n","b2a6060c":"count_vectorizer = CountVectorizer(ngram_range=(3, 4), stop_words='english', strip_accents='unicode',\n                                   analyzer='char')\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\ns1, s2 = fit_evaluate(count_train, y_train, count_test, y_test)\nres_list = res_list.append({'Method':'Count - char, ngram (3,4)', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'Count - char, ngram (3,4)', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","861c12bf":"count_vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words='english', strip_accents='unicode',\n                                   analyzer='char')\ncount_train = count_vectorizer.fit_transform(X_train)\ncount_test = count_vectorizer.transform(X_test)\ns1, s2 = fit_evaluate(count_train, y_train, count_test, y_test)\nres_list = res_list.append({'Method':'Count - char, ngram 2', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'Count - char, ngram 2', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","2532b5cc":"nlp = en_core_web_sm.load()\nvectors = []\nfor t in tqdm(df3['text']):\n    vectors.append(nlp(t).vector)\n\ndf3['vector'] = vectors\nvectors = pd.DataFrame(df3.vector.values.tolist(), index= df3.index)\nvectors.iloc[:20]","87d69c60":"vectors.isnull().sum().sum()","46e6a7c5":"vectors = vectors.replace(np.nan , 0)\nvectors.isnull().sum().sum()","25513a59":"X_train, X_test, y_train, y_test = train_test_split(vectors, df3['speaker'], test_size=0.2, random_state=124)","78c596ff":"s1, s2 = fit_evaluate(X_train, y_train, X_test, y_test)\nres_list = res_list.append({'Method':'Vector', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list = res_list.append({'Method':'Vector', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","b2337328":"res_list.sort_values('Acc', ascending=False)","860c31ef":"df3['is_j'] = df3['speaker'].apply(lambda x: int(x == 'JERRY'))\ndf3['is_e'] = df3['speaker'].apply(lambda x: int(x == 'ELAINE'))\ndf3['is_k'] = df3['speaker'].apply(lambda x: int(x == 'KRAMER'))\ndf3['is_g'] = df3['speaker'].apply(lambda x: int(x == 'GEORGE'))\ndf3.iloc[:10]","86fe3fc0":"res_list_2 = pd.DataFrame(columns=['Method', 'Model', 'Acc'])","e9abd8fb":"X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['is_g'], test_size=0.2, random_state=124)\n\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\n\nres_list_2 = res_list_2.append({'Method':'TFIDF - George model', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list_2 = res_list_2.append({'Method':'TFIDF - George model', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","87b911fb":"X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['is_j'], test_size=0.2, random_state=124)\n\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\n\nres_list_2 = res_list_2.append({'Method':'TFIDF - Jerry model', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list_2 = res_list_2.append({'Method':'TFIDF - Jerry model', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","20a803e7":"X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['is_k'], test_size=0.2, random_state=124)\n\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\n\nres_list_2 = res_list_2.append({'Method':'TFIDF - Kramer model', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list_2 = res_list_2.append({'Method':'TFIDF - Kramer model', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","189949b7":"X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['is_e'], test_size=0.2, random_state=124)\n\ntfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\ntfidf = tfidf_vectorizer.fit_transform(X_train)\ntfidf_test = tfidf_vectorizer.transform(X_test)\n\ns1 , s2 = fit_evaluate(tfidf, y_train, tfidf_test, y_test)\n\nres_list_2 = res_list_2.append({'Method':'TFIDF - Elaine model', 'Model': 'RF', 'Acc': s1},ignore_index=True)\nres_list_2 = res_list_2.append({'Method':'TFIDF - Elaine model', 'Model': 'Ada', 'Acc': s2},ignore_index=True)","431e5166":"res_list_2.sort_values('Acc', ascending=False)","a27a37aa":"def ensemble_fit_evaluate(df3, model, weights = False):\n    role_list = ['is_j', 'is_g', 'is_e', 'is_k']\n    name_dict = {'is_j': 'JERRY', 'is_g': 'GEORGE', 'is_e': 'ELAINE', 'is_k' : 'KRAMER'}\n    wieghts_dict = {'is_j': 0.64, 'is_g': 0.74, 'is_e': 0.8, 'is_k' : 0.83}\n\n    X_train, X_test, y_train, y_test = train_test_split(df3['text'], df3['speaker'], test_size=0.2, random_state=124)\n    preds = pd.DataFrame(y_test)\n    preds = preds.reset_index(drop=True)\n    for r in role_list:\n        X_train, _, y_train, __ = train_test_split(df3['text'], df3[r], test_size=0.2, random_state=124)\n        \n        tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 1), stop_words='english', strip_accents='unicode',analyzer='word', norm='l2')\n        tfidf = tfidf_vectorizer.fit_transform(X_train)\n        tfidf_test = tfidf_vectorizer.transform(X_test)\n        clf = model\n        clf.fit(tfidf, y_train)\n        if weights:\n            pred = clf.predict_proba(tfidf_test) * wieghts_dict[r]\n        else:\n            pred = clf.predict_proba(tfidf_test)\n        preds[name_dict[r]] = pred[:,1]\n        \n    return preds ","641ff6df":"model = RandomForestClassifier(n_estimators = 50)\npreds = ensemble_fit_evaluate(df3, model = model, weights = False)\npreds['pred'] = preds[['JERRY','GEORGE', 'ELAINE','KRAMER']].idxmax(axis=1)\npreds.iloc[:10]","99cbd271":"res_list3 = pd.DataFrame(columns=['Method', 'Model', 'Acc'])","0b5ca29a":"score = accuracy_score(preds['speaker'], preds['pred'])\nprint(classification_report(preds['speaker'], preds['pred']), \"\\n Acc = \", score)\nres_list3 = res_list3.append({'Method':'Ensamble - TFIDF', 'Model': 'RF - no weights', 'Acc': score},ignore_index=True)","0583c280":"model = RandomForestClassifier(n_estimators = 50)\npreds = ensemble_fit_evaluate(df3, model = model, weights = True)    \npreds['pred'] = preds[['JERRY','GEORGE', 'ELAINE','KRAMER']].idxmax(axis=1)\n\nscore = accuracy_score(preds['speaker'], preds['pred'])\nprint(classification_report(preds['speaker'], preds['pred']), \"\\n Acc = \", score)\n\nres_list3 = res_list3.append({'Method':'Ensamble - TFIDF', 'Model': 'RF - With weights', 'Acc': score},ignore_index=True)","f3b8e98c":"model = AdaBoostClassifier(n_estimators = 500)\npreds = ensemble_fit_evaluate(df3, model = model, weights = True) \n\npreds['pred'] = preds[['JERRY','GEORGE', 'ELAINE','KRAMER']].idxmax(axis=1)\nscore = accuracy_score(preds['speaker'], preds['pred'])\n\nprint(classification_report(preds['speaker'], preds['pred']), \"\\n Acc = \", score)\nres_list3 = res_list3.append({'Method':'Ensamble - TFIDF', 'Model': 'Adaboost - With weights', 'Acc': score},ignore_index=True)","13a18973":"model = AdaBoostClassifier(n_estimators = 500)\npreds = ensemble_fit_evaluate(df3, model = model, weights = False) \n\npreds['pred'] = preds[['JERRY','GEORGE', 'ELAINE','KRAMER']].idxmax(axis=1)\nscore = accuracy_score(preds['speaker'], preds['pred'])\n\nprint(classification_report(preds['speaker'], preds['pred']), \"\\n Acc = \", score)\n\nres_list3 = res_list3.append({'Method':'Ensamble - TFIDF', 'Model': 'Adaboost - No weights', 'Acc': score},ignore_index=True)","10fe7cc5":"res_list3.sort_values('Acc', ascending=False)","1b6c2386":"res_list.sort_values('Acc', ascending=False)","8c2f2b23":"In this notebook I will analyze the Seinfeld script, for starters i will show how to parse the data into a dataframe, later I will show some data exploration techniques with some visualization with an interactive graph. afterwards i will try to build a weighted network of the relations between some of the characters. and finally will construct a few models that will try to classify if a given text was said by one of the main roles in the TV Show.\n\n-- this is my first notebook submission, i hope you will like it -- ","5ab8af9d":"Let's try to extract the names of persons from each of the lines in order to create the connection in our graph using a spacy entity extraction.\n\n","2b17eede":"Now let's clean the data and import it into a datafrmae, I will use SEP in order to seperate between scenes in order to use it for later.","ea7d3f03":"I will use the number of appearances of each edge to determine the weight of each edge in the network","bf2f9487":"Now building a model per role:","77b228ad":"OK, so this didn't go as planned, some misclassification of a person's name such as the name extracted: 'OOOOOO' was extracted by spacy. \n\nSo, let's try a different approach... I will connect between all the characters that were on the same scene together and spoke in a sequence manner (that's why I left the SEP earlier:","a5cc9bdf":"### Some Conclusions:\nThe binary classification task had provided better accuracy levels than the merge of the binary classifiers.\n\nI guess that it might be possible to improve the building of the ensamble and that way to improve the performance but... That's it for now... ","bb0a82ce":"# Interactive Plotting","a55b8ce4":"Creating another res list for performance measurement:","ebeb3ff3":"ok so the results looks better for each one of the binary classifiers, lets make an esamble of those in order to see if it works better\n\nI will try 2 versions of the ensamble, one with weights per model (factor by thier performance) and without weights, the ensamble will be decided by the highest predict proba score","a76f3d08":"So it looks that using a model per role there was an improvment in accuracy of ~ 1.5% to 40.9% accuracy on the test-set with the Adaboost model.","56ac7b3a":"Let's see how we did...","5b3ae6d6":"Let's plot a more interactive and modern graph plot.","c723a603":"Ok the best shot so far was 39% accuracy,,, I think there is still room for improvment\n\nlet's try with Word2Vec:\n\n*Tip: TQDM is a very nice runtime bar for your code...*","e4bae3f3":"Now let's remove the SEP between scenes","1ccf2c58":"Zooming in the top 4 roles:","f14fa94f":"We can see that the results are still not that good... \nlet's try to a build model for each main role and see if it improved the accuracy of the model:","11e05b24":"I noticed that there are a few nans on the data, lets check if I was right...","931bb45d":"First, one hot encoding of the target value:","ecae4a8d":"This is how we did in the model per role:","a2db4928":"Now let's create the weighted network:","f0a0e67d":"Zooming in on nodes that have more than 15 connections (in addition to limiting up to 300 nodes):","604395e1":"And that's the performance of a global classifier:","8eb4670c":"# Line Classifier and Text Analysis","6a4333c3":"# Creating a Weighted Graph","07c9316b":"## The imports:","ed2c3804":"# Seinfeld Text Analysis","50d8c4c1":"first let's try to see how much lines did each of the roles had","ff5dc544":"Each edge is colored as follow:\n\nJERRY = red\nGEORGE = blue\nKRAMER = black\nELAINE= green\nOther = orange","37fd25c6":"And now to plot the Graph:","094a77c1":"So there are some Nans apperantly, lets impute it simply by changing the value to 0 (there arent alot of missing data thus probably wont matter that much).","e773c6b5":"We can see that Jerry, Elaine, George and Kramer are somewhere in the center nodes and have alot of connections (not surprising...)","2c142f43":"Parsing the text into a list of all the sentences:","4c2e619b":"# Data Exploration and Visualizations","068aeb86":"There is some clutter with low frequency connections, I will toss all the connections that appeared less than 3 times.","2ee64d3c":"First I will focus on the lines of the main roles and insert them into a df","894b68ac":"# Parsing","c04ead0a":"Hope you enjoyed this notebook and that it will help you in your data endeavours.\nIf you liked it or have any questions feel free to leave a comment :)","ba9a63f3":"Now let's see what model perfomed the best:","6cda27f6":"Not suprising that Jerry is leading with almost 15K lines!\n\nNow to some visualizations","10971c9b":"Now I will try to create some TFIDF and Word count features for the classifier and will try to see which model preduce the best accuracy.\nThe classifiers will be Adaboost and Random forest which are quiet popular classifiers."}}