{"cell_type":{"8ceb31df":"code","c03854f1":"code","db061670":"code","e4b77436":"code","2edf62fe":"code","bccb3fb4":"code","8e0db98e":"code","880b1d49":"code","5416304f":"code","47fef11e":"code","c3f9e011":"code","1c425be4":"code","92721ede":"code","481ee9e5":"code","4d8db0b1":"code","16dd552b":"code","c603fbd7":"code","3bfce94f":"code","6781f046":"code","94b67fe0":"code","fdccff2a":"code","5aa31435":"code","fa564e02":"code","9f867629":"code","fcfd1234":"code","e04a692d":"code","5abb415f":"code","c5c5a275":"code","f1aaaddb":"code","ca0f9667":"code","206c9b74":"code","ba42c537":"code","70283e94":"code","a4d363aa":"code","6841d7e5":"code","9ba13552":"code","7218d742":"code","1a0cf962":"code","e68e4d88":"code","412bedad":"code","b31ec88d":"code","b8617d0e":"code","ddf30b6a":"code","1e9cce71":"code","587c3166":"code","ffbe93d8":"code","92540607":"code","cd952719":"code","c0cdf9c9":"markdown","661486a2":"markdown","a49ba6ca":"markdown","8c0af10b":"markdown","14d56c02":"markdown","d1f81546":"markdown","75a697b7":"markdown","65ef14ae":"markdown","3834f53c":"markdown","b16f107f":"markdown","019084ed":"markdown","05b5769a":"markdown","4f014ad7":"markdown","24b3a0ce":"markdown","243a118b":"markdown","675dfca5":"markdown","5c470396":"markdown","aedfe730":"markdown","f4e10818":"markdown","c5930e73":"markdown","8f334010":"markdown","e3ff4efe":"markdown","854e7845":"markdown","a6410275":"markdown","aebc6ccf":"markdown","22525ce3":"markdown","e222f6c3":"markdown","ef73a6e8":"markdown","1214ef44":"markdown","0ec2c142":"markdown","8f5d63ce":"markdown","4be3360f":"markdown","7aeac7ac":"markdown","d884ba77":"markdown","5b68e761":"markdown","d842ad14":"markdown","c2190280":"markdown","4e96ecbb":"markdown","0234e4a8":"markdown","1723707a":"markdown","06595c3e":"markdown","6e10b9dc":"markdown","9bccb23c":"markdown","cd29ddb6":"markdown"},"source":{"8ceb31df":"# data analysis tools\nimport pandas as pd\nimport numpy as np\n\n# data visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning library\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer \nfrom sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c03854f1":"# load data\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")","db061670":"train_data.head()","e4b77436":"train_data.drop(\"Survived\", axis=1).describe()","2edf62fe":"test_data.describe()","bccb3fb4":"train_data.info()","8e0db98e":"train_data.describe(include=\"O\")","880b1d49":"train_data[\"Pclass\"].value_counts()","5416304f":"train_data[\"Sex\"].value_counts()","47fef11e":"train_data[\"Embarked\"].value_counts()","c3f9e011":"train_data[[\"Pclass\", \"Survived\"]].groupby([\"Pclass\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","1c425be4":"train_data[[\"Sex\", \"Survived\"]].groupby([\"Sex\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","92721ede":"train_data[[\"SibSp\", \"Survived\"]].groupby([\"SibSp\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","481ee9e5":"train_data[[\"Parch\", \"Survived\"]].groupby([\"Parch\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","4d8db0b1":"# set visualization style\nsns.set_theme(context=\"notebook\", style=\"darkgrid\", palette=\"deep\")","16dd552b":"plot = sns.FacetGrid(train_data, col=\"Survived\", height=6)\nplot.map(plt.hist, \"Age\", bins=20)","c603fbd7":"grid = sns.FacetGrid(train_data, row='Sex', col='Pclass', hue='Survived', height=6)\ngrid.map(plt.hist, 'Age', alpha=.75)\ngrid.add_legend()","3bfce94f":"grid = sns.FacetGrid(train_data, row=\"Embarked\", col=\"Survived\", height=4, aspect=1.6)\ngrid.map(sns.barplot, \"Sex\", \"Fare\", ci=None)\ngrid.add_legend()","6781f046":"sns.displot(data=train_data, x=\"Fare\", hue=\"Survived\", binwidth=20, height=7, aspect=2)","94b67fe0":"Titles = train_data.Name.str.extract(\" ([A-Za-z]+)\\.\", expand=False)\npd.crosstab(Titles, train_data[\"Sex\"])","fdccff2a":"def ExtractTitles(data):\n    data = data[\"Name\"].str.extract(\" ([A-Za-z]+)\\.\", expand=False)\n    \n    data = data.replace([\"Lady\", \"Countess\",\"Capt\", \"Col\", \"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Rare\")\n    data = data.replace(\"Mlle\", \"Miss\")\n    data = data.replace(\"Ms\", \"Miss\")\n    data = data.replace(\"Mme\", \"Mrs\")\n    data = data.to_numpy()\n    data = data.reshape(-1, 1)\n    \n    return data\n\nExtractTitles = FunctionTransformer(ExtractTitles)\nExtractTitles.fit_transform(train_data[[\"Name\"]]).shape","5aa31435":"def discretizateAge(data):\n    # infant, kid, teen, adult, senior\n    bins = [0, 1, 13, 20, 60, np.inf]\n    \n    data = np.digitize(data, bins=bins)\n    \n    return data\n\nDiscretizateAge = FunctionTransformer(discretizateAge)\nDiscretizateAge.fit_transform(train_data[[\"Age\"]]).shape","fa564e02":"def SumRelatives(data):\n    data = data.sum(axis=1)\n    data = data.to_numpy()\n    data = data.reshape(-1, 1)\n    return data\n\nSumRelatives = FunctionTransformer(SumRelatives)\nSumRelatives.fit_transform(train_data[[\"SibSp\", \"Parch\"]]).shape","9f867629":"def SplitFare(data):\n    # cheap, lower-intermediate, intermediate, expensive, very-expensive\n    bins = [0, 20, 80, 200, 500, np.inf]\n    data = np.digitize(data, bins=bins)\n    \n    return data\n\nSplitFare = FunctionTransformer(SplitFare)\nSplitFare.fit_transform(train_data[[\"Fare\"]]).shape","fcfd1234":"fare_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"discretizator\", SplitFare)\n])\n\n#fare_pipeline.fit_transform(train_data[[\"Fare\"]])","e04a692d":"categorical_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(sparse=False))\n])\n\n#categorical_pipeline.fit_transform(train_data[[\"Pclass\", \"Sex\", \"Embarked\"]])","5abb415f":"age_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\")),\n    (\"discretizator\", DiscretizateAge)\n])\n\n#age_pipeline.fit_transform(train_data[[\"Age\"]])","c5c5a275":"relatives_pipeline = Pipeline([\n    (\"sum\", SumRelatives)\n])\n\n#relatives_pipeline.fit_transform(train_data[[\"SibSp\", \"Parch\"]])","f1aaaddb":"names_pipeline = Pipeline([\n    (\"title\", ExtractTitles),\n    (\"encoder\", OneHotEncoder(sparse=False))\n])\n\n#names_pipeline.fit_transform(train_data[[\"Name\"]])","ca0f9667":"pclass_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\"))\n])\n\n#pclass_pipeline.fit_transform(train_data[[\"Pclass\"]])","206c9b74":"preprocessing_pipe = ColumnTransformer([\n    (\"fare\", fare_pipeline, [\"Fare\"]),\n    (\"cat\", categorical_pipeline, [\"Sex\", \"Embarked\"]),\n    (\"pclass\", pclass_pipeline, [\"Pclass\"]),\n    (\"age\", age_pipeline, [\"Age\"]),\n    (\"name\", names_pipeline, [\"Name\"]),\n    (\"relatives\", relatives_pipeline, [\"SibSp\", \"Parch\"])\n], verbose=True)\n\n#preprocessing_pipe.fit_transform(train_data).shape","ba42c537":"# get Labels\n\nY_train = train_data[\"Survived\"]\nX_train = preprocessing_pipe.fit_transform(train_data)\n\nX_test = preprocessing_pipe.fit_transform(test_data)","70283e94":"# Logistic Regression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, Y_train)\nacc_log = round(logreg.score(X_train, Y_train) * 100, 2)\nacc_log","a4d363aa":"# Support Vector Machines\n\nsvc = SVC()\nsvc.fit(X_train, Y_train)\nacc_svc = round(svc.score(X_train, Y_train) * 100, 2)\nacc_svc","6841d7e5":"# K-Neigbors Classifier\n\nknn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(X_train, Y_train)\nacc_knn = round(knn.score(X_train, Y_train) * 100, 2)\nacc_knn","9ba13552":"# Decision Tree\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(X_train, Y_train)\nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)\nacc_decision_tree","7218d742":"# Random Forest\n\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)\nacc_random_forest","1a0cf962":"# Gaussian Naive Bayes\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, Y_train)\nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)\nacc_gaussian","e68e4d88":"# Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, Y_train)\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)\nacc_perceptron","412bedad":"# Linear SVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, Y_train)\nacc_linear_svc = round(linear_svc.score(X_train, Y_train) * 100, 2)\nacc_linear_svc","b31ec88d":"# Stochastic Gradient Descent\n\nsgd = SGDClassifier()\nsgd.fit(X_train, Y_train)\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)\nacc_sgd","b8617d0e":"models = pd.DataFrame({\n    \"Model\": [\"Support Vector Machines\", \"KNN\", \"Logistic Regression\", \n              \"Random Forest\", \"Naive Bayes\", \"Perceptron\", \n              \"Stochastic Gradient Decent\", \"Linear SVC\", \n              \"Decision Tree\"],\n    \"Score\": [acc_svc, acc_knn, acc_log, \n              acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_linear_svc, acc_decision_tree]})\nmodels.sort_values(by=\"Score\", ascending=False)","ddf30b6a":"parameters = {'max_depth': [10, 20, None],\n 'min_samples_leaf': [1, 2, 4],\n 'min_samples_split': [2, 5, 10],\n 'n_estimators': [50, 100, 200]}","1e9cce71":"grid_search = GridSearchCV(RandomForestClassifier(), parameters, verbose=1)\ngrid_search.fit(X_train, Y_train)","587c3166":"grid_search.best_score_","ffbe93d8":"grid_search.best_params_","92540607":"# fine-tune the model\n\nrandom_forest_tuned = RandomForestClassifier(max_depth=10, min_samples_leaf=4, min_samples_split=2, n_estimators=100)\nrandom_forest_tuned.fit(X_train, Y_train)\nY_pred = random_forest_tuned.predict(X_test)","cd952719":"submission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_pred\n    })\n\nsubmission.to_csv(\"submission.csv\", index=False)","c0cdf9c9":"We'll begin with extracting titles from names:","661486a2":"<a id=\"ch3\"><\/a>\n## 3. Preprocess data","a49ba6ca":"Combine number of relatives, children, and parents each passenger had aboard into one feature:","8c0af10b":"-----","14d56c02":"We can see that there were no passengers with same names and that there were only three locations where passengers could get into Titanic.","d1f81546":"The attributes of each passenger are: <br>\n+ **PassengerID**: sequential array of integers (probably unique for each passenger, will check later)\n+ **Survived**: our target lable, stating whether a passenger survived or not \n  + 1 - yes \n  + 0 - no\n+ **Pclass**: passenger's travel class\n+ **Name**: passenger's name with titles such as Mr., Mrs. etc.\n+ **Sex**: male or female\n+ **SibSp**: number of passenger's siblings and spouses aboard\n+ **Parch**: number of passenger's parents and children aboard\n+ **Ticket**: ticket number\n+ **Fare**: ticket price (pounds)\n+ **Cabin**: cabin number\n+ **Embarked**: a location, where a passenger got on board of Titanic","75a697b7":"If you were a female, your chances of survival would have been much higher than of men.","65ef14ae":"Some titles are common (Miss, Mr), others appear just once or a few times (Sir, Lady). We may want to combine titles into more representative groups.","3834f53c":"---","b16f107f":"Confirming our previous observation, the higher (in price) the travel class, the more chances of survival. A lot of males traveling the lowest class didn't survive.","019084ed":"# Titanic Survival Prediction","05b5769a":"<a id=\"ch4\"><\/a>\n## 4. Model selection","4f014ad7":"----","24b3a0ce":"----","243a118b":"**Contents**\n1. [Load and explore data](#ch1)\n2. [Visualize data](#ch2)\n3. [Preprocess data](#ch3)\n4. [Model selection](#ch4)","675dfca5":"Lastly, less run a grid seacrh and tune our model by selecting the best hyperparameters:","5c470396":"Let's go trhough the pipeline elements one by one: \n+ It starts by processing **Fare** - imputing missing values with median (we saw huge outliers earlier hence not mean) and dividing fares into categories based on amount paid for the ticket. \n+ Then we process categorical features (**Sex**, **Embarked**, **Pclass**). We impute missing values by the most frequent value and encode **Sex** and **Embarked** categories by OneHotEncoding; we use OneHot because these categories are not in order and we don't want our classifier to think that there is some linear relation between them. We don't have to encode **Pclass**.\n+ Then comes the age pipeline. It imputes missing values with medians and makes discrete age groups. <br>\n+ After age we process names or, more specifically we extract titles and encode them by OneHot encoding. Once again we chose OneHot as there is no relation between titles. <br>\n+ Lastly, we sum all the relatives of passengers. <br>\nNow our data is ready for classifiers :)","aedfe730":"Now, let's try to come up with some assumptions:\n+ **Age** and **Embarked** features may correlate with survival probability and their missing values should be completed. <br>\n+ **Ticket** attribute contains only unique values and may not be significant for survival predictions, this feature will be dropped. <br>\n+ **Cabin** values are missing for more than 50% of the passengers, this feature should be dropped. <br>\n+ **PassengerID** should be dropped from the dataframe as it does not contribute to survival rate. <br>\n+ **Name** feature has only unique values and won't help us identify survivors, however titles contained in names may help us, we should extract titles from names. <br>\n+ We also may want to split **Age** attribute into age groups, as well as split **Fares** into groups.","f4e10818":"Further analysis of the data shows us there are **Age** and **Embarked** info missing for some passengers. We can also notice more than 50% of missing values for **Cabin** location, probably will get rid of this feature.","c5930e73":"Keeping in mind some assumptions that were made earlier let's create a few preprocessing methods.","8f334010":"-------","e3ff4efe":"Explorng categorical features we can see that more than half of the passengers traveled in the lowest - 3rd class. This may be an indicator of how much care a passenger was given when the incident occured. <br>\nMajority of the passengers entered Titanic in a location that is abbreviated as S.","854e7845":"<a id=\"ch2\"><\/a>\n## 2. Visualize data","a6410275":"### Import libraries","aebc6ccf":"From this histogram we can see that lots of infants and children (under 5 y.o.) have survived.","22525ce3":"And indeed, we can see that the higher the travel class was, the higher were the one's chances to survive.","e222f6c3":"Apparently people who paid for the ticket more than 80 pounds had higher chance to stay alive during the disaster. We may want to divide fares into categories based on that.","ef73a6e8":"Inexplicably, embarkment location appears to correlate with survival chances.","1214ef44":"### Submit solution","0ec2c142":"Let's take a first glimpse at the data with the help of Pandas ```head()```, ```describe()```, and ```info()``` methods.","8f5d63ce":"----","4be3360f":"We can compare statistical parameters of the attributes between train and test data to ensure we have a representative training dataset. All the differences are neglectible. We can also notice some outliers: 75% of the tickets costed 31 pound or less, however the most expensive ticket (or tickets) costed as much as 512 pounds! From now let's put test set to the side and not touch it untill we have our model ready.","7aeac7ac":"Now, having all the methods completed let's wrap them into preprocessing pipeline:","d884ba77":"Let's see if any of these features differ for those, who survived or not:","5b68e761":"<a id=\"ch1\"><\/a>\n## 1. Load and explore data","d842ad14":"Now, let's create a method for discretization of age feature, i.e. divide passengers into some representative groups by **Age**:","c2190280":"There seem to be a trend towards smaller survival probability with higher number of siblings and spouses on the board, but passengers with none of them obscure the trend.","4e96ecbb":"----","0234e4a8":"-------","1723707a":"No evident correlation between number of parents\/children on board and the outcome. ","06595c3e":"------","6e10b9dc":"Both Random Forest and Decision Tree perform the best. We will stick with Random Forest as it's less prone to overfitting than Decision Tree.","9bccb23c":"+ Logistic Regression\n+ Support Vector Machines (SVM)\n+ K-Nearest Neighbours (KNN)\n+ Decision Tree\n+ Random Forest\n+ Perceptron\n+ Artificial Neural Network\n+ Relevance Vector Machine (RVM)","cd29ddb6":"Unsurprisingly, Titanic happened to be my first Kaggle competition. Any comments, questions, critique, and compliments are welcome. Also consider upvoting if this notebook was of any use for you! :)"}}