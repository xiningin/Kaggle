{"cell_type":{"f4cf628a":"code","2e5db097":"code","4fbf1624":"code","bcb745a0":"code","f19988bd":"code","c7ea6418":"code","2d849c58":"code","43b33dcd":"code","a5606728":"code","13b4a08d":"code","2882f550":"code","a038db93":"code","be99c83e":"code","b0b1a988":"code","bc3876db":"code","af172baa":"code","92dd0ccf":"code","00c994ee":"code","b7202319":"code","ddec28c1":"code","a8c14d6e":"code","19658086":"code","fdb0faf9":"code","9dabf0fa":"code","b7ba79ad":"code","1916d525":"code","0c70b02a":"code","c0aab246":"code","e45ed06d":"code","aee51608":"code","db29d299":"code","074e0561":"code","4e315a43":"code","ff88de89":"code","2f88f3e2":"code","f82ffcc8":"code","31af523f":"code","90919500":"code","feaf2a3c":"code","d347e5e1":"code","88222822":"code","4440db6e":"code","55f90f17":"code","2dbb12c5":"code","e0e52670":"code","9bfc9912":"code","5e4e549d":"code","c6f00d3b":"code","135d42a0":"code","3df06193":"code","d5a7ddda":"code","8fca2742":"code","cb4e65c3":"code","6bd89810":"code","e214afc6":"code","f57a227a":"code","360e1f32":"code","345a2c25":"code","9872425a":"code","595e531c":"code","df5edca5":"code","cda8a594":"markdown","57bf9076":"markdown","cc01cf1d":"markdown","5b84b06b":"markdown","47c7b6cc":"markdown","ec7ca1c8":"markdown","48af4fd0":"markdown","4087a97b":"markdown","a6395168":"markdown","aa09dced":"markdown","cbfd1c50":"markdown","5226c6c8":"markdown"},"source":{"f4cf628a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\n#invite people for the Kaggle party\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.metrics import mean_squared_error, make_scorer\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.linear_model import Ridge, RidgeCV, ElasticNet, LassoCV, LassoLarsCV\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom scipy import stats\nfrom scipy.stats import norm, skew\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","2e5db097":"# save filepath to variable for easier access\ntrain_data = '..\/input\/train.csv'\ntest_data = '..\/input\/test.csv'\n\n# Input datas\ntrain = pd.read_csv(train_data) \ntest = pd.read_csv(test_data) \n\n#Columns names\ntrain.columns","4fbf1624":"# Check data type\ntrain.dtypes.sample(train.shape[1])","bcb745a0":"y =pd.DataFrame(train['SalePrice']) \ntrain=train.drop(columns=['SalePrice'])\n\n#descriptive statistics summary\ny.describe()","f19988bd":"#histogram\nsns.distplot(y);","c7ea6418":"#skewness and kurtosis\nprint(\"Skewness: %f\" % y.skew())\nprint(\"Kurtosis: %f\" % y.kurt())","2d849c58":"concat = [train, test]\nresult = pd.concat(concat)","43b33dcd":"#missing data\ntotal = result.isnull().sum().sort_values(ascending=False)\npercent = (result.isnull().sum()\/result.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","a5606728":"all_data_na = (result.isnull().sum() \/ len(result)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data1 = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data1.head(all_data_na.shape[0])\nf, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","13b4a08d":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = result.select_dtypes(include = [\"object\"]).columns\nnumerical_features = result.select_dtypes(exclude = [\"object\"]).columns\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\nresult_num = result[numerical_features]\nresult_cat = result[categorical_features]\n\n# Input missing values\nmy_imputer = Imputer(strategy='mean')\nresult[numerical_features] = my_imputer.fit_transform(result[numerical_features])","2882f550":"########## Dealing with missing data\n# Delete the columns that has more than 1 missing values\nresult = result.drop((missing_data[missing_data['Percent'] > 0.00]).index,1)","a038db93":"train = result[:train.shape[0]]\ntest = result[train.shape[0]:]\n\n# Add Column price to the train\ntrain=pd.concat([train, y], axis=1, join='inner')","be99c83e":"# Differentiate numerical features (minus the target) and categorical features\ncategorical_features = train.select_dtypes(include = [\"object\"]).columns\nnumerical_features = train.select_dtypes(exclude = [\"object\"]).columns\nnumerical_features = numerical_features.drop(\"SalePrice\")\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ntrain_num = train[numerical_features]\ntrain_cat = train[categorical_features]","b0b1a988":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[1:5].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","bc3876db":"train = train.drop(train[(train['SalePrice']>300000) & (train['OverallCond']==2)].index)","af172baa":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[5:10].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","92dd0ccf":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[10:15].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","00c994ee":"train = train.drop(train[(train['1stFlrSF']>4000)].index)","b7202319":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[15:20].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","ddec28c1":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[20:25].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","a8c14d6e":"#scatterplot\nsns.set()\ncols = ['SalePrice']\ncols.extend(numerical_features[25:30].tolist())\nsns.pairplot(train[cols], size = 3.5)\nplt.show();","19658086":"corrmat = train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmin=-1,vmax=1, square=True,center=0)","fdb0faf9":"# 'SalePrice' correlation matrix (zoomed heatmap style)\n#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","9dabf0fa":"# 'SalePrice' correlation matrix (zoomed heatmap style)\n#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nsmallest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","b7ba79ad":"#standardizing SalePrice and check the data\n# We will standardize the data. In this context, data standardization means converting data values to have mean of 0 \n# and a standard deviation of 1.\nsaleprice_scaled = StandardScaler().fit_transform(train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","1916d525":"#histogram and normal probability plot\nsns.distplot(train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)","0c70b02a":"y=train['SalePrice']\ntrain=train.drop(columns=['SalePrice'])","c0aab246":"concat = [train, test]\nresult = pd.concat(concat)","e45ed06d":"numeric_feats = result.dtypes[result.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = result[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","aee51608":"skewed_feats = skewed_feats[skewed_feats > 0.75]\nskewed_feats = skewed_feats.index\n\nresult[skewed_feats] = np.log1p(result[skewed_feats])","db29d299":"numeric_feats = result.dtypes[result.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = result[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","074e0561":"print('Before getting dummys',result.shape)\nresult = pd.get_dummies(result)\nprint('after getting dummys',result.shape)","4e315a43":"# Creating new features \n# nonlinear transformation for the top 5 correlation features\nresult[\"OverallQual-s2\"] = result[\"OverallQual\"] ** 2\nresult[\"OverallQual-s3\"] = result[\"OverallQual\"] ** 3\nresult[\"OverallQual-Sq\"] = np.sqrt(result[\"OverallQual\"])\n\nresult[\"GrLivArea-2\"] = result[\"GrLivArea\"] ** 2\nresult[\"GrLivArea-3\"] = result[\"GrLivArea\"] ** 3\nresult[\"GrLivArea-Sq\"] = np.sqrt(result[\"GrLivArea\"])\n\nresult[\"1stFlrSF-2\"] = result[\"1stFlrSF\"] ** 2\nresult[\"1stFlrSF-3\"] = result[\"1stFlrSF\"] ** 3\nresult[\"1stFlrSF-Sq\"] = np.sqrt(result[\"1stFlrSF\"])\n\nresult[\"FullBath-2\"] = result[\"FullBath\"] ** 2\nresult[\"FullBath-3\"] = result[\"FullBath\"] ** 3\nresult[\"FullBath-Sq\"] = np.sqrt(result[\"FullBath\"])\n\nresult[\"TotRmsAbvGrd-s2\"] = result[\"TotRmsAbvGrd\"] ** 2\nresult[\"TotRmsAbvGrd-s3\"] = result[\"TotRmsAbvGrd\"] ** 3\nresult[\"TotRmsAbvGrd-Sq\"] = np.sqrt(result[\"TotRmsAbvGrd\"])","ff88de89":"train = result[:train.shape[0]]\ntest = result[train.shape[0]:]","2f88f3e2":"x=train.drop(columns=['Id'])","f82ffcc8":"#histogram and normal probability plot\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)","31af523f":"# Log in the price\ny = np.log1p(y)\n\n#histogram and normal probability plot\nsns.distplot(y, fit=norm);\nfig = plt.figure()\nres = stats.probplot(y, plot=plt)","90919500":"# Partition the dataset in train + validation sets\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)","feaf2a3c":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","d347e5e1":"lr = LinearRegression()\nlr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(lr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(lr).mean())\ny_train_pred = lr.predict(X_train)\ny_test_pred = lr.predict(X_test)\n","88222822":"# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","4440db6e":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)","55f90f17":"print(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)","2dbb12c5":"print(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)","e0e52670":"# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","9bfc9912":"# Plot important coefficients\ncoefs = pd.Series(ridge.coef_, index = X_train.columns)\nprint(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Ridge Model\")\nplt.show()","5e4e549d":"lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)","c6f00d3b":"print(\"Try again for more precision with alphas centered around \" + str(alpha))\nlasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 10)\nlasso.fit(X_train, y_train)\nalpha = lasso.alpha_\nprint(\"Best alpha :\", alpha)","135d42a0":"print(\"Lasso RMSE on Training set :\", rmse_cv_train(lasso).mean())\nprint(\"Lasso RMSE on Test set :\", rmse_cv_test(lasso).mean())\ny_train_las = lasso.predict(X_train)\ny_test_las = lasso.predict(X_test)","3df06193":"# Plot residuals\nplt.scatter(y_train_las, y_train_las - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_las, y_test_las - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Lasso regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","d5a7ddda":"# Plot important coefficients\ncoefs = pd.Series(lasso.coef_, index = X_train.columns)\nprint(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n      str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the Lasso Model\")\nplt.show()","8fca2742":"elasticNet = ElasticNetCV(l1_ratio = [0.1, 0.3, 0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95, 1],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                    0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )","cb4e65c3":"print(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\nelasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                          alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )","6bd89810":"print(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n      \" and alpha centered around \" + str(alpha))\nelasticNet = ElasticNetCV(l1_ratio = ratio,\n                          alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                    alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                    alpha * 1.35, alpha * 1.4], \n                          max_iter = 50000, cv = 10)\nelasticNet.fit(X_train, y_train)\nif (elasticNet.l1_ratio_ > 1):\n    elasticNet.l1_ratio_ = 1    \nalpha = elasticNet.alpha_\nratio = elasticNet.l1_ratio_\nprint(\"Best l1_ratio :\", ratio)\nprint(\"Best alpha :\", alpha )","e214afc6":"print(\"ElasticNet RMSE on Training set :\", rmse_cv_train(elasticNet).mean())\nprint(\"ElasticNet RMSE on Test set :\", rmse_cv_test(elasticNet).mean())\ny_train_ela = elasticNet.predict(X_train)\ny_test_ela = elasticNet.predict(X_test)","f57a227a":"# Plot residuals\nplt.scatter(y_train_ela, y_train_ela - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_ela, y_test_ela - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with ElasticNet regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()","360e1f32":"# Plot important coefficients\ncoefs = pd.Series(elasticNet.coef_, index = X_train.columns)\nprint(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\")\nplt.title(\"Coefficients in the ElasticNet Model\")\nplt.show()","345a2c25":"test\ntest_ID = test.Id\ntest =test.drop(columns=['Id'])","9872425a":"y_price = elasticNet.predict(test)","595e531c":"sub=pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice']=y_price","df5edca5":"sub.to_csv('submission.csv',index=False)","cda8a594":"Correlation Matrix","57bf9076":"Skewness","cc01cf1d":"Gettin Dummy Categorical Features","5b84b06b":"Linear Regression","47c7b6cc":"Ridge Regression","ec7ca1c8":"Send the prices","48af4fd0":"ElasticNet Model","4087a97b":"Relationship with numerical variables","a6395168":"Models","aa09dced":"Removing Out Liars","cbfd1c50":"Lasso Model","5226c6c8":"Missing Data"}}