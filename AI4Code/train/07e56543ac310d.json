{"cell_type":{"6b33d75d":"code","3c20750e":"code","8cab01a6":"code","472e3609":"code","24f831b0":"code","1447a62e":"code","502d6a98":"code","0aabd6e2":"code","dbcc66d9":"code","4cab7460":"code","6a58ca8f":"code","689d565f":"code","15997591":"code","4cf5cd24":"code","b256b6f3":"code","bac31ed5":"code","ccfe6c4c":"code","752029c8":"markdown","b68c5576":"markdown","5502a661":"markdown","3fc7ce7f":"markdown","a3f19c7b":"markdown","335285f9":"markdown","54dff53f":"markdown","0b263acb":"markdown","2400f46a":"markdown","1c348912":"markdown","d3254393":"markdown","70d7ae86":"markdown","72660c21":"markdown","da29ee77":"markdown","eb1eaca1":"markdown","89d99f81":"markdown","5401d6c5":"markdown","cd6e66b8":"markdown","e97130d5":"markdown","75881a17":"markdown"},"source":{"6b33d75d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3c20750e":"import os\nos.listdir(\"..\/input\/flowers-recognition\/flowers\/flowers\/\")","8cab01a6":"daisy = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/daisy\"\nsunflower = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/sunflower\"\ntulip = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/tulip\"\nrose = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/rose\"\ndandelion = \"\/kaggle\/input\/flowers-recognition\/flowers\/flowers\/dandelion\"","472e3609":"x = []\ny = []\nimg_size=(224,224)\nimport matplotlib.pyplot as plt\nimport cv2\ndef make_whole_dataset(directory,flower):\n    try:\n        for img in os.listdir(directory):\n            path = os.path.join(directory,img)\n            img_array = cv2.imread(path,cv2.IMREAD_COLOR)\n            img_array = cv2.resize(img_array,img_size)\n            x.append(np.array(img_array))\n            y.append(str(flower))\n    except:\n        None\n\n    print(\"Flower {} has been added to the Dataset Successfully\".format(flower))\n        \n","24f831b0":"make_whole_dataset(daisy,\"Daisy\")\nmake_whole_dataset(rose,\"Rose\")\nmake_whole_dataset(dandelion,\"Dandelion\")\nmake_whole_dataset(sunflower,\"Sunflower\")\nmake_whole_dataset(tulip,\"Tulip\")","1447a62e":"import random\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(12,10))\nfig.set_size_inches(10,10)\nfor i in range(6):\n    plt.subplot(3,3,i+1)\n    sample = random.randint(0,len(y))\n    plt.imshow(x[sample])\n    plt.xlabel(\"Flower: {}\".format(y[sample]))\n    \nplt.tight_layout()","502d6a98":"from tensorflow.keras.utils import to_categorical\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)\ny = to_categorical(y,5)","0aabd6e2":"from sklearn.model_selection import train_test_split\nx = np.array(x)\nx = x\/255.0\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)","dbcc66d9":"from tensorflow.keras.applications.vgg16 import VGG16\nvgg = VGG16(weights = 'imagenet',include_top=True)","4cab7460":"vgg.summary()","6a58ca8f":"for layers in vgg.layers[:19]:\n    layers.trainable = False","689d565f":"from tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense\nx = vgg.layers[-2].output\npredictions = Dense(5,activation='softmax')(x)\nmodel = Model(inputs=vgg.input, outputs = predictions)\nmodel.summary()","15997591":"model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['accuracy'])","4cf5cd24":"from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping\n\ncheckpoint = ModelCheckpoint(\"vgg16_1.h5\",monitor = 'val_accuracy',verbose=1,save_best_only = True,save_weights_only = False,\n                             mode='auto',period=1)\nearlystop = EarlyStopping(monitor ='val_acc',patience=20,min_delta = 0,verbose=1,mode='auto')","b256b6f3":"history = model.fit(x_train,y_train,epochs = 30,validation_data=(x_test,y_test),callbacks=[checkpoint,earlystop])","bac31ed5":"from sklearn.metrics import accuracy_score\ny_pred = model.predict(x_test)\ny_pred = np.argmax(y_pred,axis=1)","ccfe6c4c":"import random\nplt.style.use('ggplot')\nfig = plt.figure(figsize=(12,10))\nfig.set_size_inches(13,13)\nfor i in range(8):\n    plt.subplot(4,4,i+1)\n    sample = random.randint(0,len(y_test))\n    plt.imshow(x_test[sample])\n    plt.xlabel(\"Actual Flower: {} \\n Predicted Flower: {}\".format(np.argmax(y_test[sample]),y_pred[sample]))\n    \n    \nplt.tight_layout()","752029c8":"><h3>Let's see our VGG16 models Summary<h3>","b68c5576":">Now let's Compile our model","5502a661":"><h3>Import Necessary libraries & load Dataset<\/h3>","3fc7ce7f":"><h3>Let's look at some information of VGG16 model<\/h3>","a3f19c7b":"><h4>Importing VGG16 Model<\/h4>","335285f9":"><h3>Let's see some of our Images<\/h3>","54dff53f":"---\n\n<h1 style=\"text-align: center;font-size: 20px;color:blue;\">Thanks for reading the Notebook<\/h1>\n\n---","0b263acb":">VGG16 model is trained by 1000 classes,but here in this dataset we have only 5 classes,so for that we have to delete the last layer of vgg16 model which has 1000 classes and there adding a Dense layer for our dataset,where we have only 5 class","2400f46a":"><h3>Configuration:<\/h3>\nThe ConvNet configurations are outlined in figure 02. The nets are referred to their names (A-E). All configurations follow the generic design present in architecture and differ only in the depth: from 11 weight layers in the network A (8 conv. and 3 FC layers) to 19 weight layers in the network E (16 conv. and 3 FC layers). The width of conv. layers (the number of channels) is rather small, starting from 64 in the first layer and then increasing by a factor of 2 after each max-pooling layer, until it reaches 512.\n","1c348912":"---\n\n<h1 style=\"text-align: center;font-size: 40px;\">Flower Recognition Using VGG16<\/h1>\n\n---\n\n<center><img src=https:\/\/newevolutiondesigns.com\/images\/freebies\/flowers-facebook-cover-2.jpg\n\"width=\"800\"><\/center>\n\n---","d3254393":"<h3>Dataset<\/h3>\nImageNet is a dataset of over 15 million labeled high-resolution images belonging to roughly 22,000 categories. The images were collected from the web and labeled by human labelers using Amazon\u2019s Mechanical Turk crowd-sourcing tool. Starting in 2010, as part of the Pascal Visual Object Challenge, an annual competition called the ImageNet Large-Scale Visual Recognition Challenge (ILSVRC) has been held. ILSVRC uses a subset of ImageNet with roughly 1000 images in each of 1000 categories. In all, there are roughly 1.2 million training images, 50,000 validation images, and 150,000 testing images. ImageNet consists of variable-resolution images. Therefore, the images have been down-sampled to a fixed resolution of 256\u00d7256. Given a rectangular image, the image is rescaled and cropped out the central 256\u00d7256 patch from the resulting image.<\/br>\n\n","70d7ae86":"> <h4>Here in this Notebook i'm going to using VGG16 for Flower recognition.VGG16 is a convolutional neural network model proposed by K. Simonyan and A. Zisserman from the University of Oxford in the paper \u201cVery Deep Convolutional Networks for Large-Scale Image Recognition\u201d. The model achieves 92.7% top-5 test accuracy in ImageNet, which is a dataset of over 14 million images belonging to 1000 classes. It was one of the famous model submitted to ILSVRC-2014. It makes the improvement over AlexNet by replacing large kernel-sized filters (11 and 5 in the first and second convolutional layer, respectively) with multiple 3\u00d73 kernel-sized filters one after another. VGG16 was trained for weeks and was using NVIDIA Titan Black GPU\u2019s.<\/h4>","72660c21":"><h3> Let's see Predicted Flower vs Actual Flower<\/h3>","da29ee77":"\n<h3>Architecture:<\/h3>\n\n<center><img src=https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16.png\n\"width=\"800 height = \"200\"><\/center>\n\nThe input to cov1 layer is of fixed size 224 x 224 RGB image. The image is passed through a stack of convolutional (conv.) layers, where the filters were used with a very small receptive field: 3\u00d73 (which is the smallest size to capture the notion of left\/right, up\/down, center). In one of the configurations, it also utilizes 1\u00d71 convolution filters, which can be seen as a linear transformation of the input channels (followed by non-linearity). The convolution stride is fixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved after convolution, i.e. the padding is 1-pixel for 3\u00d73 conv. layers. Spatial pooling is carried out by five max-pooling layers, which follow some of the conv.  layers (not all the conv. layers are followed by max-pooling). Max-pooling is performed over a 2\u00d72 pixel window, with stride 2.\n\nThree Fully-Connected (FC) layers follow a stack of convolutional layers (which has a different depth in different architectures): the first two have 4096 channels each, the third performs 1000-way ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is the soft-max layer. The configuration of the fully connected layers is the same in all networks.\n\nAll hidden layers are equipped with the rectification (ReLU) non-linearity. It is also noted that none of the networks (except for one) contain Local Response Normalisation (LRN), such normalization does not improve the performance on the ILSVRC dataset, but leads to increased memory consumption and computation time.","eb1eaca1":">VGG16 is a trained model so, if we don't mention trainable = False then ,it's going to train again,so we have mention it as False","89d99f81":"><h4>In our Dataset we have 5 different directories containing different flowers,So let's define this directories with the Flower name they contains<\/h4>","5401d6c5":"> <h4>Here In y we have Flower names ,which is in String Format,But CNN doesn't support string,we need to convert those  names into numbers,here i'm using LabelEncoder to convert those strings into number & after this by using to_categorical i'm going to convert the numerical values into Binary Form.It Converts a class vector (integers) to binary class matrix.<\/h4>","cd6e66b8":"><h4>Now let's combine all the images and their names in  2 different list x & y, so that we can separate them together into train & test dataset<\/h4>","e97130d5":">Now let's  change our Pixel values of x between 0 to 1 ,dividing by 255,since the Highest value of pixels  is 255 and  split our dataset into Train & test dataset","75881a17":"\n<center><img src=https:\/\/neurohive.io\/wp-content\/uploads\/2018\/11\/vgg16-1-e1542731207177.png\n\"width=\"800 height = \"200\"><\/center>\n"}}