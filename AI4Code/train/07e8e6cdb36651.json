{"cell_type":{"5d6d87bc":"code","dbc1c9fb":"code","8fbc74a8":"code","5a976fd0":"code","f865be1a":"code","caf469d8":"code","d75874f6":"code","8e725cd8":"code","1edc1d52":"code","e2f19794":"code","0ec6bb75":"code","64bacff3":"code","02f2435e":"code","8d453c0f":"code","de97df7d":"code","c9e35413":"code","92a67fc6":"code","aabc9fb1":"code","3315ce2a":"code","265146ff":"code","35e4f65c":"code","2a57d0a5":"code","61b7fba8":"code","4068d4be":"code","9c337e39":"code","c0ffa36d":"markdown","4c96924f":"markdown","58cf5eb7":"markdown","bd86e84d":"markdown","d1d2a194":"markdown","245ba923":"markdown","dad2345b":"markdown","c27616b2":"markdown","effc7bac":"markdown","e045e2a9":"markdown","e3d39986":"markdown","ce745fdc":"markdown","4afbf143":"markdown","952f0617":"markdown","c2b15a6c":"markdown","65b5c0a5":"markdown","e5e7c03f":"markdown","a9757374":"markdown","5ea200f0":"markdown","578d3084":"markdown","869470fc":"markdown","0c02f31b":"markdown","df69a289":"markdown","eedcd1b9":"markdown","7ff2c4c3":"markdown","bfebd94d":"markdown","80ab0da2":"markdown","5d7e33f4":"markdown","6aada0fc":"markdown","b5c7fdf4":"markdown","1b51621e":"markdown","c8943e9d":"markdown","0dbbb1af":"markdown","70be1363":"markdown"},"source":{"5d6d87bc":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nsns.set(context=\"notebook\", palette=\"Spectral\", style = 'darkgrid' ,font_scale = 1.5, color_codes=True)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport matplotlib.pyplot as plt","dbc1c9fb":"ad_data = pd.read_csv('..\/input\/Advertising.csv',index_col='Unnamed: 0')","8fbc74a8":"ad_data.info()","5a976fd0":"ad_data.describe()","f865be1a":"p = sns.pairplot(ad_data)","caf469d8":"# visualize the relationship between the features and the response using scatterplots\np = sns.pairplot(ad_data, x_vars=['TV','Radio','Newspaper'], y_vars='Sales', size=7, aspect=0.7)","d75874f6":"x = ad_data.drop([\"Sales\"],axis=1)\ny = ad_data.Sales","8e725cd8":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX = sc.fit_transform(x)","1edc1d52":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y,random_state = 0,test_size=0.25)","e2f19794":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import r2_score\nfrom sklearn import linear_model\n\nregr = linear_model.LinearRegression()\nregr.fit(X_train,y_train)\ny_pred = regr.predict(X_train)","0ec6bb75":"print(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=y_pred)))","64bacff3":"residuals = y_train.values-y_pred\nmean_residuals = np.mean(residuals)\nprint(\"Mean of Residuals {}\".format(mean_residuals))","02f2435e":"p = sns.scatterplot(y_pred,residuals)\nplt.xlabel('y_pred\/predicted values')\nplt.ylabel('Residuals')\nplt.ylim(-10,10)\nplt.xlim(0,26)\np = sns.lineplot([0,26],[0,0],color='blue')\np = plt.title('Residuals vs fitted values plot for homoscedasticity check')","8d453c0f":"import statsmodels.stats.api as sms\nfrom statsmodels.compat import lzip\nname = ['F statistic', 'p-value']\ntest = sms.het_goldfeldquandt(residuals, X_train)\nlzip(name, test)","de97df7d":"from scipy.stats import bartlett\ntest = bartlett( X_train,residuals)\nprint(test)","c9e35413":"p = sns.distplot(residuals,kde=True)\np = plt.title('Normality of error terms\/residuals')","92a67fc6":"plt.figure(figsize=(10,5))\np = sns.lineplot(y_pred,residuals,marker='o',color='blue')\nplt.xlabel('y_pred\/predicted values')\nplt.ylabel('Residuals')\nplt.ylim(-10,10)\nplt.xlim(0,26)\np = sns.lineplot([0,26],[0,0],color='red')\np = plt.title('Residuals vs fitted values plot for autocorrelation check')","aabc9fb1":"from statsmodels.stats import diagnostic as diag\nmin(diag.acorr_ljungbox(residuals , lags = 40)[1])","3315ce2a":"import statsmodels.api as sm","265146ff":"# autocorrelation\nsm.graphics.tsa.plot_acf(residuals, lags=40)\nplt.show()","35e4f65c":"# partial autocorrelation\nsm.graphics.tsa.plot_pacf(residuals, lags=40)\nplt.show()","2a57d0a5":"plt.figure(figsize=(20,20))  # on this line I just set the size of figure to 12 by 10.\np=sns.heatmap(ad_data.corr(), annot=True,cmap='RdYlGn',square=True)  # seaborn has very simple solution for heatmap","61b7fba8":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree = DecisionTreeRegressor(random_state=0)\ndec_tree.fit(X_train,y_train)\ndec_tree_y_pred = dec_tree.predict(X_train)\nprint(\"Accuracy: {}\".format(dec_tree.score(X_train,y_train)))\nprint(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=dec_tree_y_pred)))","4068d4be":"from sklearn.ensemble import RandomForestRegressor\n\nrf_tree = RandomForestRegressor(random_state=0)\nrf_tree.fit(X_train,y_train)\nrf_tree_y_pred = rf_tree.predict(X_train)\nprint(\"Accuracy: {}\".format(rf_tree.score(X_train,y_train)))\nprint(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=rf_tree_y_pred)))","9c337e39":"from sklearn.svm import SVR\n\nsvr = SVR()\nsvr.fit(X_train,y_train)\nsvr_y_pred = svr.predict(X_train)\nprint(\"Accuracy: {}\".format(svr.score(X_train,y_train)))\nprint(\"R squared: {}\".format(r2_score(y_true=y_train,y_pred=svr_y_pred)))","c0ffa36d":"Table of Content\n<br><a href=\"#linearity\">1. Linearity<\/a>\n<br><a href=\"#mean\">2. Mean of Residuals<\/a>\n<br><a href=\"#homo\">3. Check for Homoscedasticity<\/a>\n<br><a href=\"#normal\">4. Check for Normality of error terms\/residuals<\/a>\n<br><a href=\"#auto\">5. No autocorrelation of residuals<\/a>\n<br><a href=\"#multico\">6. No perfect multicollinearity<\/a>\n<br><a href=\"#other\">7. Other Models for comparison<\/a>\n","4c96924f":"### Look for correlation of rows where the dependent variable (Sales in this case) is not involved because if a variable is correlated with the dependent variable then this would be a good sign for our model. Correlation within dependent variables is what we need to look for and avoid. This data doesn't contain perfect multicollinearity among independent variables. In case there was any then we would try to remove one of the correlated variables depending on which was more important to our regression model.","58cf5eb7":"### Detecting heteroscedasticity! \nGraphical Method: Firstly do the regression analysis and then plot the error terms against the predicted values( Yi^). If there is a definite pattern (like linear or quadratic or funnel shaped) obtained from the scatter plot then heteroscedasticity is present.","bd86e84d":"Reference:\n* http:\/\/r-statistics.co\/Assumptions-of-Linear-Regression.html\n* https:\/\/www.statisticssolutions.com\/assumptions-of-linear-regression\/","d1d2a194":"### Homoscedasticity means that the residuals have equal or almost equal variance across the regression line. By plotting the error terms with predicted terms we can check that there should not be any pattern in the error terms.","245ba923":"### By looking at the plots we can see that with the Sales variable the none of the independent variables form an accurately linear shape but TV and Radio do still better than Newspaper which seems to hardly have any specific shape. So it shows that a linear regression fitting might not be the best model for it. A linear model might not be able to *efficiently* explain the data in terms of variability, prediction accuracy etc. \n\nA tip is to remember to always see the plots from where the dependent variable is on the y axis. Though it wouldn't vary the shape much but that's how linear regression's intuition is, to put the dependent variable as y and independents as x(s).","dad2345b":"## <a id=\"multico\">6. No perfect multicollinearity<\/a>","c27616b2":"##  Goldfeld Quandt Test\nChecking heteroscedasticity : Using Goldfeld Quandt we test for heteroscedasticity.\n* Null Hypothesis: Error terms are homoscedastic\n* Alternative Hypothesis: Error terms are heteroscedastic.","effc7bac":"### Note that the scores are high because I have used the same data for training and testing. This also shows how significant data splitting, train_test_split() etc. are. This is only for model exploration purposes. Moreover there's almost no hyperparameter tuning done at this point to make this a simple representation but tuning can highly improve the kind of learning that the model can achieve and keep overfitting away.","e045e2a9":"# <a id=\"other\"> 7. Some other model evaluations for fun<\/a>","e3d39986":"### Very close to zero so all good here.","ce745fdc":" ### Linear regression needs the relationship between the independent and dependent variables to be linear.  Let's use a pair plot to check the relation of independent variables with the Sales variable","4afbf143":"### In regression, multicollinearity refers to the extent to which independent variables are correlated. Multicollinearity affects the coefficients and p-values, but it does not influence the predictions, precision of the predictions, and the goodness-of-fit statistics. If your primary goal is to make predictions, and you don\u2019t need to understand the role of each independent variable, you don\u2019t need to reduce severe multicollinearity.","952f0617":"### Now we will be applying tests. \nA tip is to keep in mind that if we want 95% confidence on our findings and tests then the p-value should be less than 0.05 to be able to reject the null hypothesis. Remember, a researcher or data scientist would always aim to reject the null hypothesis.","c2b15a6c":"# So most of the major assumptions of Linear Regression are successfully through. Great! Since this was one of the simplest data sets it demonstrated the steps well. These steps can be applied on other problems to be able to make better decisions about which model to use. I hope this acts as a decent template of sort to be applied to data.","65b5c0a5":"## <a id=\"normal\">4. Check for Normality of error terms\/residuals<\/a>","e5e7c03f":"### Since p value is quite less than 0.05 in Bartlett, it's null hypothesis that error terms are homoscedastic gets rejected, that's not good for a regression.","a9757374":"### The residual terms are pretty much normally distributed for the number of test points we took. Remember the central limit theorem which says that as the sample size increases the distribution tends to be normal. A skew is also visible from the plot. It's very difficult to get perfect curves, distributions in real life data.","5ea200f0":"### Now rest of the assumptions require us to perform the regression before we can even check for them. So let's perform regression on it.","578d3084":"## <a id=\"linearity\">1. Linearity<\/a>\n","869470fc":"### Since p value is more than 0.05 in Goldfeld Quandt Test, we can't reject it's null hypothesis that error terms are homoscedastic. Good.","0c02f31b":"### Residuals as we know are the differences between the true value and the predicted value. One of the assumptions of linear regression is that the mean of the residuals should be zero. So let's find out.\n","df69a289":"#  Assumptions for Linear Regression","eedcd1b9":"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX","7ff2c4c3":"## This notebook explains the assumptions of linear regression in detail. One of the most essential steps to take before applying linear regression and depending solely on accuracy scores is to check for these assumptions.","bfebd94d":"### Bartlett\u2019s test tests the null hypothesis that all input samples are from populations with equal variances. ","80ab0da2":"## <a id=\"homo\">3. Check for Homoscedasticity<\/a>","5d7e33f4":"### The results show signs of autocorelation since there are spikes outside the red confidence interval region. This could be a factor of seasonality in the data.","6aada0fc":"### Fitting the linear model","b5c7fdf4":"## <a id=\"mean\">2. Mean of Residuals<\/a>","1b51621e":"### Checking for autocorrelation To ensure the absence of autocorrelation we use Ljungbox test.\n* Null Hypothesis: Autocorrelation is absent.\n* Alternative Hypothesis: Autocorrelation is present.","c8943e9d":"### Since p value is less than 0.05 we reject the null hypothesis that error terms are not autocorrelated.","0dbbb1af":"### When the residuals are autocorrelated, it means that the current value is dependent of the previous (historic) values and that there is a definite unexplained pattern in the Y variable that shows up in the error terms. Though it is more evident in time series data.\n\n#### In plain terms autocorrelation takes place when there's a pattern in the rows of the data. This is usual in time series data as there is a pattern of time for eg. Week of the day effect which is a very famous pattern seen in stock markets where people tend to buy stocks more towards the beginning of weekends and tend to sell more on Mondays. There's been great study about this phenomenon and it is still a matter of research as to what actual factors cause this trend.\n\n### There should not be autocorrelation in the data so the error terms should not form any pattern.","70be1363":"## <a id=\"auto\">5. No autocorrelation of residuals<\/a>"}}