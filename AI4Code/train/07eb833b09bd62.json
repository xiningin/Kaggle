{"cell_type":{"ca7897e2":"code","5a66b43f":"code","7d83121d":"code","fc94afe1":"code","d570a7ea":"code","371acc21":"code","4d7514ff":"code","60b8d053":"code","fdd3b910":"code","a5c69770":"code","8a7b2f88":"code","900d5c64":"code","3d25abac":"code","722bfbc4":"code","20025220":"code","b2afd529":"code","25d1c5f8":"code","8b9ebd91":"code","2c3c8744":"code","dcc94e1b":"code","a3a1f273":"code","964d66f5":"code","27fd9fca":"code","1b88e278":"code","87241fda":"code","bc0ca4fc":"code","0cd29d03":"code","a52421d8":"code","3d1f9d19":"code","043281d8":"code","95bfadbc":"code","0d397109":"code","0291c552":"code","f91e6df0":"code","fcb91d88":"markdown","bde1d68f":"markdown","5a2c7eed":"markdown","8ab1bb75":"markdown","6a15b70b":"markdown","16afd592":"markdown","1b049377":"markdown","5e308273":"markdown","e94dc598":"markdown","e8cf5269":"markdown","e91959d6":"markdown","797fb082":"markdown","a473a48f":"markdown","5632c499":"markdown","263a2c6c":"markdown","a0cc8268":"markdown","d553bcaf":"markdown","addd4269":"markdown","d9e3490d":"markdown","3c873a0b":"markdown","63709edb":"markdown","9f2296ef":"markdown","b45f68f6":"markdown","8bad4e02":"markdown"},"source":{"ca7897e2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.graph_objs as go","5a66b43f":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression,LogisticRegression\nfrom sklearn.tree import DecisionTreeRegressor,DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score\nfrom plotly.subplots import make_subplots\nfrom sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, ConfusionMatrixDisplay, precision_score, recall_score, f1_score, classification_report, roc_curve, plot_roc_curve, auc, precision_recall_curve, plot_precision_recall_curve, average_precision_score\nfrom imblearn.over_sampling import SMOTE\n\nfrom sklearn.model_selection import GridSearchCV","7d83121d":"df = pd.read_csv(\"..\/input\/stroke-prediction-dataset\/healthcare-dataset-stroke-data.csv\")","fc94afe1":"df.head()","d570a7ea":"df.shape","371acc21":"# All unique values and value counts of every column.\nfor column in df.columns:\n    print(\"\\n\" + column)\n    print(df[column].value_counts())","4d7514ff":"# Is there any null value?\ndf.isnull().sum()","60b8d053":"# 'bmi' column has 201 nan values. \n# This is not too much, so we will drop this rows\n# Drop null rows by 'bmi' column\ndf = df.dropna()","fdd3b910":"# Drop 'id' column\ndf = df.drop('id', axis=1)\n\n# Round 'age' column and convert data type to integer\ndf.age = df.age.round().astype('int')\n\n# Drop 'Other' gendered individuals for simplify the mathematical compuations. Because only one patient has 'other'\ndf.drop(df.loc[df['gender']=='Other'].index, inplace=True)","a5c69770":"# Correlation of values with each other\ncorrelation = df.corr()\nfig, axes = plt.subplots(figsize=(7, 7))\nsns.heatmap(correlation, vmax=.8, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10});","8a7b2f88":"# Visualize stroke distribution\nsns.countplot(x='stroke', data=df, palette='cool')\ndf.stroke.value_counts()","900d5c64":"# ratio of stroke positive and stroke negative patient counts:\nprint(\"Stroke positive patient ratio: \" + str(df.stroke.value_counts()[1]\/len(df)))\nprint(\"Stroke negative patient ratio: \" + str(df.stroke.value_counts()[0]\/len(df)))","3d25abac":"# Visualize age distribution\nplt.figure(figsize=(12,5))\nsns.distplot(df['age'], bins=max(df['age']), kde=False);","722bfbc4":"# Visualize age and stroke correlation.\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['stroke'] == 0][\"age\"], bins=max(df['age']), kde=False)\nsns.distplot(df[df['stroke'] == 1][\"age\"], bins=max(df['age']), color='red', kde=False)\n\nplt.title('Stroke Negative vs Stroke Positinve Patient Counts by Age', fontsize=15)\nplt.xlim([min(df['age']),max(df['age'])])\nplt.show()","20025220":"# Visualize age and stroke correlation\nplt.figure(figsize=(10, 5))\n\nsns.distplot(df[df['stroke'] == 0][\"age\"], bins=max(df['age']), kde=True)\nsns.distplot(df[df['stroke'] == 1][\"age\"], bins=max(df['age']), color='red', kde=True)\n\nplt.title('Age Distributions in Stroke Negative vs Stroke Positinve Patient Groups', fontsize=15)\nplt.xlim([min(df['age']),max(df['age'])])\nplt.show()","b2afd529":"# Data distributions\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\nsns.countplot(x='gender', data=df, hue='stroke', ax=ax1, palette='cool');\nsns.countplot(x='ever_married', data=df, hue='stroke', ax=ax2, palette='cool');\nsns.countplot(x='heart_disease', data=df, hue='stroke', ax=ax3, palette='cool');\nfig.set_figwidth(15)\nfig.set_figheight(5)","25d1c5f8":"fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\nsns.countplot(x='heart_disease', data=df, hue='stroke', ax=ax1, palette='cool')\nsns.countplot(x='stroke', data=df, hue='heart_disease', ax=ax2, palette='cool')\nfig.set_figwidth(10)\nfig.set_figheight(4)","8b9ebd91":"fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\nsns.countplot(x='Residence_type', data=df, hue='stroke', ax=ax1, palette='cool')\nsns.countplot(x='stroke', data=df, hue='Residence_type', ax=ax2, palette='cool')\nfig.set_figwidth(10)\nfig.set_figheight(4)","2c3c8744":"# Work type and stroke relationship\nfig, (ax1, ax2, ax3) = plt.subplots(ncols=3, sharey=True)\nsns.countplot(x='work_type', data=df, hue='stroke', ax=ax1, palette='cool')\nsns.countplot(x='stroke', data=df, hue='work_type', ax=ax2, palette='cool')\nfig.set_figwidth(15)\nfig.set_figheight(5)","dcc94e1b":"# Smoking status and stroke relationship\nfig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True)\nsns.countplot(x='smoking_status', data=df, hue='stroke', ax=ax1, palette='cool')\nsns.countplot(x='stroke', data=df, hue='smoking_status', ax=ax2, palette='cool')\nfig.set_figwidth(15)\nfig.set_figheight(5)","a3a1f273":"stroke_df = df[df[\"stroke\"] == 1]","964d66f5":"#Work type distribution in stroke patients\nplt.figure(figsize=(10,4))\nsns.countplot(data=stroke_df,x='work_type', palette='cool');","27fd9fca":"#Smoking status distribution in stroke patients\nplt.figure(figsize=(10,4))\nsns.countplot(data=stroke_df,x='smoking_status',palette='cool');","1b88e278":"df.head()","87241fda":"# One-hot encode 'work_type','Residence_type', 'smoking_status' column\ndf = pd.get_dummies(df, columns=[\"work_type\", \"smoking_status\"], prefix=[\"work_type\", \"smoking_status\"])\n\n# Convert 'gender' column to boolean\ndf[\"gender\"] = df[\"gender\"].apply(lambda x: 1 if x==\"Male\" else 0)\n\n# Convert 'ever_married' column to boolean\ndf[\"ever_married\"] = df[\"ever_married\"].apply(lambda x: 1 if x==\"Yes\" else 0)\n\n# Encode \"Residence_type\" column to boolean\ndf[\"Residence_type\"] = df[\"Residence_type\"].apply(lambda x: 1 if x==\"Urban\" else 0)","bc0ca4fc":"df.head()","0cd29d03":"# Scaling the variance in features\n\nstd=StandardScaler()\n\ncolumns = ['age','avg_glucose_level','bmi']\nscaled = std.fit_transform(df[['age','avg_glucose_level','bmi']])\nscaled = pd.DataFrame(scaled, columns=columns, index= df.index.values)\n\ndf2=df.drop(columns=columns,axis=1)\ndf2=df2.merge(scaled, left_index=True, right_index=True, how = \"left\")\ndf2","a52421d8":"X = df.loc[:, df.columns != 'stroke']\ny = df['stroke']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n#X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.25, random_state=42)\n\nX_train, y_train = SMOTE().fit_resample(X_train, y_train)","3d1f9d19":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","043281d8":"from sklearn.ensemble import AdaBoostClassifier\n\n#create adaboost classification obj\nab_clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(), n_estimators=100, \n                            learning_rate=0.5, random_state=100)\n\n#training via adaboost classficiation model\nab_clf.fit(X_train, y_train)\n\n#make prediction using the test set\ny_pred= ab_clf.predict(X_test)\n\n#score\nab_clf_score = ab_clf.score(X_test, y_test)\n\n\nscore = cross_val_score(ab_clf, X_train, y_train, cv = 6)\nprecision = precision_score(y_test, y_pred)\nroc = roc_auc_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\ncm = confusion_matrix(y_test, y_pred)\n\n# print ('Train score of AdaBoostClassifier is', score.mean(),'%')\n# print ('--')\n# print ('Precision score is ', precision)\n# print ('--')\n# print ('ROC Score is', roc)\n# print ('--')\n# print ('Recall Score is ', recall)\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Reds\");\n\nprint(classification_report(y_test,y_pred))","95bfadbc":"from sklearn.ensemble import GradientBoostingClassifier\n\n#create xgboost classification obj\nxgboost_clf = GradientBoostingClassifier(random_state=0)\n\n#training via xgboost classficiation model\nxgboost_clf.fit(X_train, y_train)\n\n#make prediction using the test set\ny_pred= xgboost_clf.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Reds\");\n\nprint(classification_report(y_test,y_pred))","0d397109":"#create SVC classification obj\nsvc_obj = SVC()\nparameters = {'kernel':('linear', 'rbf'), 'C':[1, 10, 100]}\nsvc = GridSearchCV(svc_obj, parameters)\n\n#training via SVC classficiation model\nsvc.fit(X_train, y_train)\n\n#make prediction using the test set\ny_pred= svc.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Reds\");\n\nprint(classification_report(y_test,y_pred))","0291c552":"#create RandomForest classification obj\nforest_obj = RandomForestClassifier()\nparameters = {'n_estimators':[10, 20, 50, 100], 'max_depth':[2, 4, 6]}\nforest = GridSearchCV(forest_obj, parameters)\n\n#training via RandomForest classficiation model\nforest.fit(X_train, y_train)\n\n#make prediction using the test set\ny_pred= forest.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Reds\");\n\nprint(classification_report(y_test,y_pred))","f91e6df0":"#create LogisticRegression classification obj\nregression = LogisticRegression()\n\n#training via LogisticRegression classficiation model\nregression.fit(X_train, y_train)\n\n#make prediction using the test set\ny_pred= regression.predict(X_test)\n\ncm = confusion_matrix(y_test, y_pred)\nsns.heatmap(cm, annot=True,fmt='d',cmap=\"Reds\");\n\nprint(classification_report(y_test,y_pred))","fcb91d88":"# Exploratory Data Analysis - EDA","bde1d68f":"**Best model:**  Random Forest Classifier ,  **f1-score:**  0.19, **Accuracy:** 0.79","5a2c7eed":"# My Medium blog post about data analysis of this project","8ab1bb75":"# SVM","6a15b70b":"#### Stroke Prediction Dataset","16afd592":"# Random Forest Classifier","1b049377":"A stroke occurs when the blood supply to part of your brain is interrupted or reduced, preventing brain tissue from getting oxygen and nutrients. Brain cells begin to die in minutes. A stroke is a medical emergency, and prompt treatment is crucial. Early action can reduce brain damage and other complications. The good news is that many fewer Americans die of stroke now than in the past. Effective treatments can also help prevent disability from stroke.\n\nThis project for understand what are the reasons that cause stroke to peoeple and see if we can succefully detect stroke on some features using ML technics.","5e308273":"# XGboost","e94dc598":"According to data of smokers, most common pattern in stroke group is non-smokers. Strange but true. This may be due to other factors affecting stroke and a biased assesment.","e8cf5269":"# Data Preprocessing for Machine Learning","e91959d6":"First plot shows that the numbers and rates of stroke and non-stroke patients are very similar in both localization types.\nThe second plot also supports this.","797fb082":"According to above plot, we have balanced age distribution in stroke negative group. But stroke positive patients are stacked to the left. Let's look age ratio distribution in each groups:","a473a48f":"Age distribution of data is balanced.","5632c499":"The rate of stroke is higher in married people, but this may be a result of bias.","263a2c6c":"# Background and Motivation","a0cc8268":"According to accuracy, XGboost seems as best predictor model. But according to f1-scores, best model is random forest classifier with grad-searched parameters. Accuracy value can be misleading for unbalanced datasets. f1-score gives better insight such this situations.","d553bcaf":"# Model Selection","addd4269":"Context\nAccording to the World Health Organization (WHO) stroke is the 2nd leading cause of death globally, responsible for approximately 11% of total deaths.\nThis dataset is used to predict whether a patient is likely to get stroke based on the input parameters like gender, age, various diseases, and smoking status. Each row in the data provides relavant information about the patient.\n\nAttribute Information:\n\n1) id: unique identifier<br>\n2) gender: \"Male\", \"Female\" or \"Other\"<br>\n3) age: age of the patient<br>\n4) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension<br>\n5) heart_disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease<br>\n6) ever_married: \"No\" or \"Yes\"<br>\n7) work_type: \"children\", \"Govt_jov\", \"Never_worked\", \"Private\" or \"Self-employed\"<br>\n8) Residence_type: \"Rural\" or \"Urban\"<br>\n9) avg_glucose_level: average glucose level in blood<br>\n10) bmi: body mass index<br>\n11) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*<br>\n12) stroke: 1 if the patient had a stroke or 0 if not<br><br>\n*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient","d9e3490d":"https:\/\/www.kaggle.com\/fedesoriano\/stroke-prediction-dataset","3c873a0b":"Stroke and heart disesase surprisingly negative correlated.","63709edb":"### Adaboost Classification","9f2296ef":"Data-Driven Analysis of Mythes About Stroke\u00a0Causes\n\nhttps:\/\/medium.com\/@sahika.betul\/data-driven-analysis-of-mythes-about-stroke-causes-dd347899bba5","b45f68f6":"# Logistic Regression","8bad4e02":"# About Dataset"}}