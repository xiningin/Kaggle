{"cell_type":{"df20cee2":"code","2743a3a9":"code","c4a08625":"code","579f5556":"code","e3d86d80":"code","119a644e":"code","efefca35":"code","f9171d66":"code","5543f6e4":"code","00a5c4fa":"code","5600af8c":"code","442e2d4e":"code","4335c7df":"code","b5e1775f":"code","cf93c9b0":"code","45dad878":"code","7b3477de":"code","9982b1be":"code","a0db0300":"code","a6e9cf8b":"code","f596aaf6":"code","680cdcd3":"code","3e9b448d":"code","ad9bc331":"code","adae3fbe":"code","c081b025":"code","ad52714b":"code","783460b9":"code","b32a2f7a":"code","72b2f1bf":"code","0bc3e433":"code","4656d56a":"code","08c3cb73":"code","67a4c956":"code","6ff14a1e":"code","8bdcf5d0":"code","49b4b6b0":"code","be2aa9db":"code","0554cd70":"code","c55cca73":"code","12e32536":"markdown","437a013d":"markdown","088275c0":"markdown","25364474":"markdown","b9bec6ec":"markdown","03316efe":"markdown","2d9f705e":"markdown","108e366b":"markdown","c7aae30b":"markdown","c441b3fc":"markdown","2cdaf629":"markdown","a75071e9":"markdown","92ef0564":"markdown","46575ce0":"markdown","43a5e0c7":"markdown","e78a54c4":"markdown","db17f8cd":"markdown","203fbe60":"markdown","284c1654":"markdown","73004bcf":"markdown","2654a774":"markdown","400ce56a":"markdown"},"source":{"df20cee2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2743a3a9":"!pip install -q hvplot","c4a08625":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport hvplot.pandas\n\n\n%matplotlib inline\n\npd.pandas.set_option('display.max_columns', None)\npd.pandas.set_option('display.max_rows', 100)","579f5556":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsample_submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","e3d86d80":"train.head()","119a644e":"test.head()","efefca35":"sample_submission.head()","f9171d66":"print(f\"Train data shape {train.shape}\")\nprint(f\"Test data shape {test.shape}\")","5543f6e4":"train.hvplot.hist(\"SalePrice\", title=\"Sales Price Distribution\")","00a5c4fa":"train['SalePrice'].describe()","5600af8c":"train[train['SalePrice']>500000].shape","442e2d4e":"#Checking for missing values-\nmissing = train.isnull().sum()\n\n#Storing only those feature that have missing values-\nmissing = missing[missing > 0]\n\n#Sorting and plotting a bar graph-\nmissing.sort_values(inplace=True)\nmissing.hvplot.barh(title=\"Missing Values (Training Data)\")","4335c7df":"#Checking for missing values-\nmissing = test.isnull().sum()\n\n#Storing only those feature that have missing values-\nmissing = missing[missing > 0]\n\n#Sorting and plotting a bar graph-\nmissing.sort_values(inplace=True)\nmissing.hvplot.barh(title=\"Missing Values (Testing Data)\", height=500)","b5e1775f":"#Detailed overview of missing values: \ntrain_missing = []\nfor column in train.columns:\n    if train[column].isna().sum() != 0:\n        missing = train[column].isna().sum()\n        print(f\"{column:-<{30}}: {missing} ({missing \/ train.shape[0] * 100:.2f}%)\")\n        if missing > train.shape[0] \/ 3:\n            train_missing.append(column)","cf93c9b0":"test_missing = []\nfor column in test.columns:\n    if test[column].isna().sum() != 0:\n        missing = test[column].isna().sum()\n        print(f\"{column:-<{30}}: {missing} ({missing \/ test.shape[0] * 100:.2f}%)\")\n        if missing > test.shape[0] \/ 3:\n            test_missing.append(column)","45dad878":"#Checking why some variables have high % of missing values\n#FOUND - dataset uses NaN for certain features to denote \"absence of feature\" \n#[Eg: Na for GarageType indicates the house has no garage]\n\nprint( train['GarageType'].unique())","7b3477de":"#list of columns\ncols = train.columns\n#print(cols)\n\n#number of columns\nno_of_cols = len(cols)\nprint(\"Number of columns: {}\".format(no_of_cols))\n\n#numeric columns details\nnumeric_cols = [col for col in train.columns if (train[col].dtype in (\"int32\" , \"int64\", 'float64'))]#print (\"Numeric Columns: {}\".format(numeric_cols))\nprint (\" Numeric columns: {}\".format(numeric_cols))\nprint (\" Number of numeric columns: {}\".format(len(numeric_cols)))\n\n#categorical columns details\ncategorical_cols = [col for col in train.columns if (train[col].dtype == \"object\")]\nprint (\"Categorical Columns: {}\".format(categorical_cols))\nprint (\" number of categorical columns: {}\".format(len(categorical_cols)))","9982b1be":"#finding cardinality of data\ncardinality ={col : train[col].nunique() for col in categorical_cols }\nprint(cardinality)","a0db0300":"plt.figure(figsize=(12, 10))\nsns.heatmap(train.corr(), vmax=.8, square=True)","a6e9cf8b":"corr_cols = train.corr().nlargest(15, 'SalePrice')['SalePrice'].index\nplt.figure(figsize=(12, 8))\nsns.heatmap(train[corr_cols].corr(), annot=True, vmax=.8, square=True)","f596aaf6":"for col in train[categorical_cols]:\n    #Checking if there are missing values in the column\n    if train[col].isnull().sum()>0:\n        print(col, train[col].unique())","680cdcd3":"#Creating a list of all the categorical variables that have NaN as a category: \n\ncols_fillna = ['PoolQC','MiscFeature','Alley','Fence','MasVnrType','FireplaceQu',\n               'GarageQual','GarageCond','GarageFinish','GarageType', 'Electrical',\n               'BsmtExposure','BsmtCond','BsmtQual','BsmtFinType1','BsmtFinType2']","3e9b448d":"# Replacing all the NaN values with \"no\":\nfor col in cols_fillna:\n    train[col].fillna('no',inplace=True)\n    test[col].fillna('no',inplace=True)","ad9bc331":"# List of features having null values - Test \n\n#Finding total and percentage of missing values\ntotal = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\n\n#Creating a data frame that contains the total and percentage of missing values:\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n#Printing first 20 elements of the newly created data frame - missing_data\nmissing_data.head(20)","adae3fbe":"# List of features having null values - Test \ntotal_test = test.isnull().sum().sort_values(ascending=False)\n\n#Finding total and percentage of missing values\npercent_test = (test.isnull().sum()\/test.isnull().count()).sort_values(ascending=False)\n\n#Creating a data frame that contains the total and percentage of missing values:\nmissing_data_test = pd.concat([total_test, percent_test], axis=1, keys=['Total', 'Percent'])\n\n#Printing first 20 elements of the newly created data frame - missing_data\nmissing_data_test.head(20)","c081b025":"# fillna with median for the remaining columns: LotFrontage, GarageYrBlt, MasVnrArea\ntrain.fillna(train.median(), inplace=True)\ntest.fillna(test.median(), inplace=True)","ad52714b":"#Checking total number of missing values in the test and train data frames\nprint(train.isnull().sum().sum())\nprint(test.isnull().sum().sum())","783460b9":"#Listing out number of missing values in the test data set\ntest.isnull().sum().sort_values(ascending=False)[:10]","b32a2f7a":"#Filling missing columns of test with Mode\nfor col in test.columns:\n    if test[col].dtypes=='object' and test[col].isnull().sum()>0:\n        test[col].fillna(test[col].mode()[0],inplace=True)","72b2f1bf":"from sklearn.preprocessing import OrdinalEncoder\nordinal_encoder = OrdinalEncoder()\n\ntrain[categorical_cols] = ordinal_encoder.fit_transform(train[categorical_cols])\ntest[categorical_cols] = ordinal_encoder.transform(test[categorical_cols])","0bc3e433":"X = train.drop('SalePrice', axis=1)\ny = train.SalePrice.copy()","4656d56a":"from sklearn.feature_selection import mutual_info_classif\nfrom matplotlib import pyplot as plt\n\nimportances = mutual_info_classif(X, y)\n#Creting a data frame to store importances of each feature\nfeature_importances = pd.Series(importances, train.columns[:len(train.columns)-1])\nfeature_importances","08c3cb73":"# selecting columns with greater than 0.5 threshold importance\nfeature_col = []\n\nfor key, val in feature_importances.items():\n    if np.abs(val)>0.5:\n        feature_col.append(key)\nprint (feature_col)\nprint (len(feature_col))","67a4c956":"X = X[feature_col]\nxtest = test[feature_col]","6ff14a1e":"from sklearn.model_selection import train_test_split\nxtrain, xvalid, ytrain, yvalid = train_test_split(X, y, random_state=2, test_size=0.2)","8bdcf5d0":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_squared_error\n\nrf_model = RandomForestRegressor(random_state=2, n_estimators=200)\nrf_model.fit(xtrain, ytrain)\npreds_y = rf_model.predict(xvalid)\nmean_squared_error(yvalid, preds_y, squared=False)\n\n#500 estimators - 30348.832108819155\n\n\n#200 estimators - 30490.88170116555\n#100 estimators - 30510.838834369253","49b4b6b0":"from sklearn.preprocessing import StandardScaler\n\nsc = StandardScaler()\nxtrain_sc = sc.fit_transform(xtrain)\nxvalid_sc = sc.transform(xvalid)\n\nrf_model_sc = RandomForestRegressor(random_state=2, n_estimators=200)\nrf_model_sc.fit(xtrain_sc, ytrain)\npreds_y_sc = rf_model_sc.predict(xvalid_sc)\n\nmean_squared_error(yvalid, preds_y_sc, squared=False)","be2aa9db":"submission_csv = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","0554cd70":"yPred = rf_model.predict(xtest)\nmean_squared_error(submission_csv['SalePrice'], yPred, squared=False)\n\n#208281.97200317192\n#73941.80663198546","c55cca73":"submission_csv.SalePrice = yPred\nsubmission_csv.to_csv('output.csv', index=False)","12e32536":"**Reducing train set (X) to contain only important features**\n\nX is the reduced dataset that contains only features that are deemed important (>0.5 - threshold) based on mutual information classifier. ","437a013d":" **4 G) Column Data Types**","088275c0":" **4 B) Testing set sample**","25364474":" **4 F) Missing data**","b9bec6ec":"#  Loading the dataset","03316efe":" **4 C)Sample submission**","2d9f705e":"# Exploratory Data Analysis","108e366b":" **4 E) Target Label exploration**","c7aae30b":"**Without Standard Scaler**","c441b3fc":"**5 a) Looking further into nan values - CATEGORICAL FEATURES**","2cdaf629":"# Categorical variables encoding - Ordinal Encoder","a75071e9":"# Installing and importing packages","92ef0564":"**With Standard Scaler**","46575ce0":"# Identifying Important features - Mutual information classifier","43a5e0c7":"# Model Creation - Random Forest regressor","e78a54c4":"# Creating the Train and Validation Split","db17f8cd":"# Seperating target label from test set","203fbe60":"# 5) Feature Engineering","284c1654":" **4 A)Training set sample**","73004bcf":" **4 D) Dimensions of the data**","2654a774":"**Installing hvPlot for visualisation**","400ce56a":"**5 b) Dealing with missing values - NUMERICAL FEATURES**"}}