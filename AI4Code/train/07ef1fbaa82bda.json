{"cell_type":{"35a3d91e":"code","d7378950":"code","99c64098":"code","1337246e":"code","68fda109":"code","e80aa6bd":"code","03d3bede":"code","dcba8b38":"code","66bb7cda":"code","336dfadc":"code","9d2b6794":"code","1191d6b9":"code","f3748711":"code","de0682d0":"code","9af171b6":"code","e143b0d5":"code","3db91ebd":"code","5bc9e93d":"code","d3b8d2a7":"markdown","1cc266fd":"markdown","bc12a89e":"markdown","d7b43d41":"markdown","d4dbd8a1":"markdown","192b15e1":"markdown","ab3def3b":"markdown","fe0d4ba1":"markdown","c5640526":"markdown","4e3d8d9d":"markdown","86c5fe26":"markdown","a33a18ef":"markdown"},"source":{"35a3d91e":"%matplotlib inline\nfrom pathlib import Path\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torchvision.transforms.functional as F\nfrom torchvision.io import read_image","d7378950":"plt.rcParams[\"savefig.bbox\"] = 'tight'\n\ndef show(imgs):\n    if not isinstance(imgs, list):\n        imgs = [imgs]\n    fix, axs = plt.subplots(ncols=len(imgs), squeeze=False)\n    for i, img in enumerate(imgs):\n        img = img.detach()\n        img = F.to_pil_image(img)\n        axs[0, i].imshow(np.asarray(img))\n        axs[0, i].set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])","99c64098":"from torchvision.utils import make_grid\n\ndog1_int0 = cv2.imread('..\/input\/stanford-dogs-dataset\/images\/Images\/n02089973-English_foxhound\/n02089973_1303.jpg')\ndog2_int0 = cv2.imread('..\/input\/stanford-dogs-dataset\/images\/Images\/n02091831-Saluki\/n02091831_10733.jpg')\n\ndog1_int1 = cv2.resize(dog1_int0,dsize=(400,400),interpolation=cv2.INTER_CUBIC)\ndag2_int1 = cv2.resize(dog2_int0,dsize=(400,400),interpolation=cv2.INTER_CUBIC)\n\nprint(dog1_int1.shape)\n\ndog1_int =torch.from_numpy(cv2.cvtColor(dog1_int1, cv2.COLOR_BGR2RGB)).permute(2,0,1)\ndog2_int =torch.from_numpy(cv2.cvtColor(dag2_int1, cv2.COLOR_BGR2RGB)).permute(2,0,1)\n\nprint(dog1_int.shape)\n\ngrid = make_grid([dog1_int, dog2_int])\nshow(grid)","1337246e":"from torchvision.utils import draw_bounding_boxes\n\nboxes = torch.tensor([[50, 50, 100, 200], [210, 150, 350, 400]], dtype=torch.float)\ncolors = [\"blue\", \"yellow\"]\nresult = draw_bounding_boxes(dog1_int, boxes, colors=colors, width=4)\nshow(result)","68fda109":"from torchvision.models.detection import fasterrcnn_resnet50_fpn\nfrom torchvision.transforms.functional import convert_image_dtype\n\nbatch_int = torch.stack([dog1_int, dog2_int])\nbatch = convert_image_dtype(batch_int, dtype=torch.float)\n\nmodel = fasterrcnn_resnet50_fpn(pretrained=True, progress=False)\nmodel = model.eval()\n\noutputs = model(batch)\n#print(outputs)","e80aa6bd":"score_threshold = 0.8\ndogs_with_boxes = [\n    draw_bounding_boxes(dog_int, boxes=output['boxes'][output['scores'] > score_threshold], width=4)\n    for dog_int, output in zip(batch_int, outputs)\n]\nshow(dogs_with_boxes)","03d3bede":"from torchvision.models.segmentation import fcn_resnet50\n\nmodel = fcn_resnet50(pretrained=True, progress=False)\nmodel = model.eval()\n\nnormalized_batch = F.normalize(batch, mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\noutput = model(normalized_batch)['out']\n\nprint(output.shape, output.min().item(), output.max().item())","dcba8b38":"sem_classes = [\n    '__background__', 'aeroplane', 'bicycle', 'bird', 'boat', 'bottle', 'bus',\n    'car', 'cat', 'chair', 'cow', 'diningtable', 'dog', 'horse', 'motorbike',\n    'person', 'pottedplant', 'sheep', 'sofa', 'train', 'tvmonitor'\n]\nsem_class_to_idx = {cls: idx for (idx, cls) in enumerate(sem_classes)}\nnormalized_masks = torch.nn.functional.softmax(output, dim=1)\n\ndog_and_chair_masks = [\n    normalized_masks[img_idx, sem_class_to_idx[cls]]\n    for img_idx in range(batch.shape[0])\n    for cls in ('dog','chair')\n]\n\nshow(dog_and_chair_masks)","66bb7cda":"class_dim = 1\nboolean_dog_masks = (normalized_masks.argmax(class_dim) == sem_class_to_idx['dog'])\nprint(f\"shape = {boolean_dog_masks.shape}, dtype = {boolean_dog_masks.dtype}\")\nshow([m.float() for m in boolean_dog_masks])","336dfadc":"from torchvision.utils import draw_segmentation_masks\n\ndogs_with_masks = [\n    draw_segmentation_masks(img, masks=mask, alpha=0.7)\n    for img, mask in zip(batch_int, boolean_dog_masks)\n]\nshow(dogs_with_masks)","9d2b6794":"num_classes = normalized_masks.shape[1]\ndog1_masks = normalized_masks[0]\nclass_dim = 0\ndog1_all_classes_masks = dog1_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None]\n\nprint(f\"dog1_masks shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}\")\nprint(f\"dog1_all_classes_masks = {dog1_all_classes_masks.shape}, dtype = {dog1_all_classes_masks.dtype}\")\n\ndog_with_all_masks = draw_segmentation_masks(dog1_int, masks=dog1_all_classes_masks, alpha=.6)\nshow(dog_with_all_masks)","1191d6b9":"class_dim = 1\nall_classes_masks = normalized_masks.argmax(class_dim) == torch.arange(num_classes)[:, None, None, None]\nprint(f\"shape = {all_classes_masks.shape}, dtype = {all_classes_masks.dtype}\")\n\nall_classes_masks = all_classes_masks.swapaxes(0, 1)\n\ndogs_with_masks = [\n    draw_segmentation_masks(img, masks=mask, alpha=.6)\n    for img, mask in zip(batch_int, all_classes_masks)\n]\nshow(dogs_with_masks)","f3748711":"from torchvision.models.detection import maskrcnn_resnet50_fpn\n\nmodel = maskrcnn_resnet50_fpn(pretrained=True, progress=False)\nmodel = model.eval()\n\noutput = model(batch)\n#print(output)","de0682d0":"dog1_output = output[0]\ndog1_masks = dog1_output['masks']\nprint(f\"shape = {dog1_masks.shape}, dtype = {dog1_masks.dtype}, \"\n      f\"min = {dog1_masks.min()}, max = {dog1_masks.max()}\")","9af171b6":"inst_classes = [\n    '__background__', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N\/A', 'stop sign',\n    'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',\n    'elephant', 'bear', 'zebra', 'giraffe', 'N\/A', 'backpack', 'umbrella', 'N\/A', 'N\/A',\n    'handbag', 'tie', 'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',\n    'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard', 'tennis racket',\n    'bottle', 'N\/A', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl',\n    'banana', 'apple', 'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',\n    'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed', 'N\/A', 'dining table',\n    'N\/A', 'N\/A', 'toilet', 'N\/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone',\n    'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N\/A', 'book',\n    'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'\n]\n\ninst_class_to_idx = {cls: idx for (idx, cls) in enumerate(inst_classes)}\n\nprint(\"For the first dog, the following instances were detected:\")\nprint([inst_classes[label] for label in dog1_output['labels']])","e143b0d5":"proba_threshold = 0.5\ndog1_bool_masks = dog1_output['masks'] > proba_threshold\nprint(f\"shape = {dog1_bool_masks.shape}, dtype = {dog1_bool_masks.dtype}\")\n\ndog1_bool_masks = dog1_bool_masks.squeeze(1)\n\nshow(draw_segmentation_masks(dog1_int, dog1_bool_masks, alpha=0.9))","3db91ebd":"print(dog1_output['scores'])","5bc9e93d":"score_threshold = 0.7\n\nboolean_masks = [\n    out['masks'][out['scores'] > score_threshold] > proba_threshold\n    for out in output\n]\n\ndogs_with_masks = [\n    draw_segmentation_masks(img, mask.squeeze(1))\n    for img, mask in zip(batch_int, boolean_masks)\n]\nshow(dogs_with_masks)","d3b8d2a7":"## Visualizing bounding boxes\n**torchvision.utils.draw_bounding_boxes** function can draw boxes on an image. The boxes are in (xmin, ymin, xmax, ymax) format.\n\n","1cc266fd":"**torchvision.utils.draw_segmentation_masks** can plot only boolean masks on the original images.","bc12a89e":"It is possible to plot more than one mask per image.\n\n","d7b43d41":"The masks corresponds to probabilities indicating the predicted label. \n\n","d4dbd8a1":"- The output of the segmentation model is a tensor of shape (batch_size, num_classes, H, W). \n- Each value is a non-normalized score, and we can normalize them into [0, 1] by using a softmax.\n- After the softmax, we can interpret each value as a probability indicating a belonging  class.","192b15e1":"## Visualizing segmentation masks\n**torchvision.utils.draw_segmentation_masks** function can draw segmentation masks on images as  **Semantic segmentation** or **Instance segmentation**.\n\n\n## Semantic segmentation models\n\n- torchvision.models.segmentation.fcn_resnet50\n- torchvision.models.segmentation.deeplabv3_resnet50\n- torchvision.models.segmentation.lraspp_mobilenet_v3_large\n\n","ab3def3b":"## Object Detection\n- torchvision.models.detection.fasterrcnn_resnet50_fpn\n- torchvision.models.detection.retinanet_resnet50_fpn\n- torchvision.models.detection.ssdlite320_mobilenet_v3_large\n- torchvision.models.detection.ssd300_vgg16","fe0d4ba1":"**torchvision.utils.draw_segmentation_masks** function can plots those masks on top of the original image. \n\n","c5640526":"## Visualizing a grid of images\n**torchvision.utils.make_grid** function can create a tensor that represents multiple images in a grid. \n","4e3d8d9d":"# Torchvision Object Detection and Segmentation Sample\nThis notebook utilized the scripts of VISUALIZATION UTILITIES from Official Pytorch Vision Example Gallery<br\/>\nhttps:\/\/pytorch.org\/vision\/stable\/auto_examples\/plot_visualization_utils.html#sphx-glr-auto-examples-plot-visualization-utils-py","86c5fe26":"## Instance segmentation models\n\nInstance segmentation models have a significantly different output from the semantic segmentation models. These models don't require the images to be normalized.\n\n- torchvision.models.detection.maskrcnn_resnet50_fpn\n- torchvision.models.detection.keypointrcnn_resnet50_fpn\n- torchvision.models.detection.fasterrcnn_resnet50_fpn","a33a18ef":"For each image in the batch, the model outputs some detections (or instances). Each instance is described by its bounding box, its label, its score and its mask."}}