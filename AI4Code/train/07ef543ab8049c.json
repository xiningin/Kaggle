{"cell_type":{"fe573eaf":"code","b5d5e925":"code","271c4800":"code","68074a44":"code","b25cd22e":"code","515b288f":"code","0a614a18":"code","ead4759a":"code","72bc5aac":"code","e977497d":"code","d2ebae3c":"code","878d71f3":"code","a3342980":"code","fab7bb9b":"code","e2b462e6":"code","399399f9":"code","d06824e2":"code","5ae163c5":"code","7499999b":"code","fab3d04f":"code","15a098fc":"code","8c7a1b88":"code","4ca47c91":"code","4d77730c":"code","2032385b":"code","fedba1e2":"code","4be4c133":"code","14de0dbd":"code","a766fbba":"code","ffd82feb":"code","a16e8618":"code","846afbe8":"code","53f5f8ac":"code","6b52ca9f":"code","c0b3b6f5":"code","7d4fa596":"code","28bc0cb5":"code","6cc41039":"code","c91209ef":"code","5d5a4342":"code","5c5f2fda":"code","3f825731":"code","c329373d":"code","eeef82b3":"code","298c5780":"code","4c4599f6":"code","d1385793":"code","9e7026bc":"code","ea3f644c":"code","06abb5b4":"code","ba33bd4b":"code","11061647":"code","7c762d80":"code","ef01170d":"code","2b6855c0":"code","70818c65":"code","ded25c62":"code","f3e7ad7d":"code","8f8809e1":"code","0713cce5":"code","09073e58":"code","e3544cd2":"code","1826e2b6":"code","482babdf":"markdown","8256afa7":"markdown","b1fe41f3":"markdown","514af0ee":"markdown","19b803dd":"markdown","ab155be5":"markdown","31f13b26":"markdown","7b7e7195":"markdown","13164be6":"markdown","efb4a29a":"markdown"},"source":{"fe573eaf":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport re\nimport string\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV, StratifiedKFold\nfrom xgboost import XGBClassifier,plot_importance\nfrom sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn.feature_selection import SelectFromModel\nfrom itertools import compress\n\n\nfrom pylab import rcParams\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom scipy.misc import imread\nfrom wordcloud import WordCloud, STOPWORDS\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","b5d5e925":"df = pd.read_csv('..\/input\/mbti_1.csv')","271c4800":"df.head()","68074a44":"dist = df['type'].value_counts()\ndist","b25cd22e":"dist.index","515b288f":"plt.hlines(y=list(range(16)), xmin=0, xmax=dist, color='skyblue')\nplt.plot(dist, list(range(16)), \"D\")\n# plt.stem(dist)\nplt.yticks(list(range(16)), dist.index)\nplt.show()","0a614a18":"df['seperated_post'] = df['posts'].apply(lambda x: x.strip().split(\"|||\"))\ndf['num_post'] = df['seperated_post'].apply(lambda x: len(x))","ead4759a":"df.head()","72bc5aac":"num_post_df = df.groupby('type')['num_post'].apply(list).reset_index()","e977497d":"rcParams['figure.figsize'] = 10,5\nsns.violinplot(x='type',y='num_post',data=df)\nplt.xlabel('')\nplt.ylabel('Number of posts')","d2ebae3c":"def count_youtube(posts):\n    count = 0\n    for p in posts:\n        if 'youtube' in p:\n            count += 1\n    return count\n        \ndf['youtube'] = df['seperated_post'].apply(count_youtube)","878d71f3":"sns.violinplot(x='type',y='youtube',data=df)\nplt.xlabel('')\nplt.ylabel('Number of posts which mention Youtube')\nplt.show()","a3342980":"plt.hist(df['youtube'])\nplt.title('Distribution of number of posts containing Youtube across individuals')\nplt.show()","fab7bb9b":"df['seperated_post'][1]","e2b462e6":"# Before expanding the dataframe, give everyone an unique ID?\ndf['id'] = df.index","399399f9":"len(df)","d06824e2":"expanded_df = pd.DataFrame(df['seperated_post'].tolist(), index=df['id']).stack().reset_index(level=1, drop=True).reset_index(name='idposts')","5ae163c5":"expanded_df.head()","7499999b":"expanded_df=expanded_df.join(df.set_index('id'), on='id', how = 'left')","fab3d04f":"expanded_df=expanded_df.drop(columns=['posts','seperated_post','num_post','youtube'])","15a098fc":"def clean_text(text):\n    result = re.sub(r'http[^\\s]*', '',text)\n    result = re.sub('[0-9]+','', result).lower()\n    result = re.sub('@[a-z0-9]+', 'user', result)\n    return re.sub('[%s]*' % string.punctuation, '',result)\n    ","8c7a1b88":"final_df = expanded_df.copy()","4ca47c91":"final_df['idposts'] = final_df['idposts'].apply(clean_text)","4d77730c":"final_df.head()","2032385b":"cleaned_df = final_df.groupby('id')['idposts'].apply(list).reset_index()","fedba1e2":"cleaned_df.head()","4be4c133":"df['clean_post'] = cleaned_df['idposts'].apply(lambda x: ' '.join(x))","14de0dbd":"df.head()","a766fbba":"vectorizer = CountVectorizer(stop_words = ['and','the','to','of',\n                                           'infj','entp','intp','intj',\n                                           'entj','enfj','infp','enfp',\n                                           'isfp','istp','isfj','istj',\n                                           'estp','esfp','estj','esfj',\n                                           'infjs','entps','intps','intjs',\n                                           'entjs','enfjs','infps','enfps',\n                                           'isfps','istps','isfjs','istjs',\n                                           'estps','esfps','estjs','esfjs'],\n                            max_features=1500,\n                            analyzer=\"word\",\n                            max_df=0.8,\n                            min_df=0.1)","ffd82feb":"corpus = df['clean_post'].values.reshape(1,-1).tolist()[0]\nvectorizer.fit(corpus)\nX_cnt = vectorizer.fit_transform(corpus)","a16e8618":"X_cnt","846afbe8":"# Transform the count matrix to a tf-idf representation\ntfizer = TfidfTransformer()\ntfizer.fit(X_cnt)\nX = tfizer.fit_transform(X_cnt).toarray()","53f5f8ac":"X.shape","6b52ca9f":"all_words = vectorizer.get_feature_names()\nn_words = len(all_words)","c0b3b6f5":"df['fav_world'] = df['type'].apply(lambda x: 1 if x[0] == 'E' else 0)\ndf['info'] = df['type'].apply(lambda x: 1 if x[1] == 'S' else 0)\ndf['decision'] = df['type'].apply(lambda x: 1 if x[2] == 'T' else 0)\ndf['structure'] = df['type'].apply(lambda x: 1 if x[3] == 'J' else 0)","7d4fa596":"df.head()","28bc0cb5":"X_df = pd.DataFrame.from_dict({w: X[:, i] for i, w in enumerate(all_words)})","6cc41039":"def sub_classifier(keyword):\n    y_f = df[keyword].values\n    X_f_train, X_f_test, y_f_train, y_f_test = train_test_split(X_df, y_f, stratify=y_f)\n    f_classifier = XGBClassifier()\n    print(\">>> Train classifier ... \")\n    f_classifier.fit(X_f_train, y_f_train, \n                     early_stopping_rounds = 10, \n                     eval_metric=\"logloss\", \n                     eval_set=[(X_f_test, y_f_test)], verbose=False)\n    print(\">>> Finish training\")\n    print(\"%s:\" % keyword, sum(y_f)\/len(y_f))\n    print(\"Accuracy %s\" % keyword, accuracy_score(y_f_test, f_classifier.predict(X_f_test)))\n    print(\"AUC %s\" % keyword, roc_auc_score(y_f_test, f_classifier.predict_proba(X_f_test)[:,1]))\n    return f_classifier","c91209ef":"fav_classifier = sub_classifier('fav_world')","5d5a4342":"info_classifier = sub_classifier('info')","5c5f2fda":"decision_classifier = sub_classifier('decision')","3f825731":"str_classifier = sub_classifier('structure')","c329373d":"rcParams['figure.figsize'] = 20, 10\nplt.subplots_adjust(wspace = 0.5)\nax1 = plt.subplot(1, 4, 1)\nplt.pie([sum(df['fav_world']), \n         len(df['fav_world']) - sum(df['fav_world'])], \n        labels = ['Extrovert', 'Introvert'],\n        explode = (0, 0.1),\n       autopct='%1.1f%%')\n\nax2 = plt.subplot(1, 4, 2)\nplt.pie([sum(df['info']), \n         len(df['info']) - sum(df['info'])], \n        labels = ['Sensing', 'Intuition'],\n        explode = (0, 0.1),\n       autopct='%1.1f%%')\n\nax3 = plt.subplot(1, 4, 3)\nplt.pie([sum(df['decision']), \n         len(df['decision']) - sum(df['decision'])], \n        labels = ['Thinking', 'Feeling'],\n        explode = (0, 0.1),\n       autopct='%1.1f%%')\n\nax4 = plt.subplot(1, 4, 4)\nplt.pie([sum(df['structure']), \n         len(df['structure']) - sum(df['structure'])], \n        labels = ['Judging', 'Perceiving'],\n        explode = (0, 0.1),\n       autopct='%1.1f%%')\n\nplt.show()","eeef82b3":"# Get the default params for the current E\/I classifier\nfav_classifier.get_xgb_params()","298c5780":"# set up parameters grids\nparams = {\n        'min_child_weight': [1, 5],\n        'gamma': [0.5, 1, 1.5, 2],\n        'subsample': [0.6, 0.8, 1.0],\n        'colsample_bytree': [0.6, 0.8, 1.0],\n        'max_depth': [3, 5, 7]\n        }","4c4599f6":"# xgb = XGBClassifier(learning_rate=0.02, n_estimators=600, objective='binary:logistic',\n#                     silent=True, nthread=1)","d1385793":"# keyword = 'fav_world'\n# y = df[keyword].values\n# folds = 3\n# param_comb = 5\n\n# skf = StratifiedKFold(n_splits=folds, shuffle = True, random_state = 1001)\n\n# random_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=param_comb, scoring='roc_auc', n_jobs=1, cv=skf.split(X,y), verbose=3, random_state=1001 )","9e7026bc":"# random_search.fit(X, y)","ea3f644c":"# subsample=0.6, min_child_weight=1, max_depth=5, gamma=0.5, colsample_bytree=1.0 is the best hyperparameters in the search\n","06abb5b4":"plot_importance(fav_classifier, max_num_features = 20)\nplt.title(\"Features associated with Extrovert\")\nplt.show()","ba33bd4b":"plot_importance(info_classifier, max_num_features = 20)\nplt.title(\"Features associated with Sensing\")\nplt.show()","11061647":"plot_importance(decision_classifier, max_num_features=20)\nplt.title(\"Features associated with Thinking\")\nplt.show()","7c762d80":"plot_importance(str_classifier, max_num_features=20)\nplt.title(\"Features associated with Judging\")\nplt.show()","ef01170d":"# Start with one review:\ndef generate_wordcloud(text, title):\n    # Create and generate a word cloud image:\n    wordcloud = WordCloud(background_color=\"white\").generate(text)\n\n    # Display the generated image:\n    plt.imshow(wordcloud, interpolation='bilinear')\n    plt.axis(\"off\")\n    plt.title(title, fontsize = 40)\n    plt.show()","2b6855c0":"df_by_personality = df.groupby(\"type\")['clean_post'].apply(' '.join).reset_index()","70818c65":"df_by_personality.head()","ded25c62":"for i, t in enumerate(df_by_personality['type']):\n    text = df_by_personality.iloc[i,1]\n    generate_wordcloud(text, t)","f3e7ad7d":"test_string = 'I like to observe, think, and analyze to find cons and pros. Based on my analysis, I like to create a solution based on cost effective analysis to maximize the resource to improve the performance. I like talking to my friends. I like to read and learn. I simulate a lot of different situations to see how I would react. I read or watch a lot to improve myself. I love talking to them and seeing what they have been up to. I have a variety of friends, and I appreciate they all experience different things. Listening to their emotion, experience, and life is always great.'.lower()\nfinal_test = tfizer.transform(vectorizer.transform([test_string])).toarray()","8f8809e1":"test_point = pd.DataFrame.from_dict({w: final_test[:, i] for i, w in enumerate(all_words)})","0713cce5":"fav_classifier.predict_proba(test_point) #[I, E]","09073e58":"info_classifier.predict_proba(test_point) #[N,S]","e3544cd2":"decision_classifier.predict_proba(test_point) #[F,T]","1826e2b6":"str_classifier.predict_proba(test_point) #[P,J]","482babdf":"Clean the text and use simple BoW with multiclass classification Logistic Regression","8256afa7":"## Features importance\nWhat words are associated with our personalities?","b1fe41f3":"This search takes a very long time and the auc doesn't improve substantially. One interesting thing about the search was that Kaggle server might run out of time on multiple workers\/ doesn't have enough resources, hence the number of workers is set to 1. ","514af0ee":"How much each type of personality posts?","19b803dd":"### What words are most common among personality types","ab155be5":"Build the vocabulary from 1500 words that are not common words or MBTI personalities, and appear 0.1-0.7 of the time.","31f13b26":"Interesting observation: Although INTP, INFJ and INFP are claimed to be the rarest types, in this dataset, they are pretty prevalent. It is probably because introverts tend to have much time online than extroverts, who might be busy socializing :D","7b7e7195":"We will use random grid search for random hyperparameters tuning. In this simple example, we will tune for <b> bolded features <\/b>:\n- <b>min_child_weight<\/b>: the minimum sum of weights of all observations required in a child. Used to control over-fitting. Higher values prevent a model from learning relations which might be highly specific to the particular sample selected for a tree. Too high values will lead to under-fitting.\n- <b>max_depth<\/b>: maximum depth of the tree. Too high values will lead to over-fitting and vice versa. Typical values are between 3 and 10.\n- max_leaf_nodes: number of maximum leaves in the tree. Can only be tuned interchangably with max_depth\n- <b>gamma<\/b>: minimum loss in reduction required to make a split. \n- <b>subsample<\/b>: the fraction of observations to be randomly samples for each tree. Lower values make the algorithm more conservative and prevents overfitting but too small values might lead to under-fitting. Typical values are between 0.5 and 1.\n- <b>colsample_bytree<\/b>: denotes the number of features randomly selected for a tree. Typical values are between 0.5 and 1.\n- lambda: L2 regularization term on weights (not used very often but might be helpful to reduce overfitting).\n- alpha: L1 regularization term on weights (not used very often but might be helpful in case of high dimensions as it creates more sparse trees)\n","13164be6":"Text are cleaned so that:\n- words are in lowercase\n- urls are removed\n- all numbers are removed\n- all usernames are replaced by the word 'user'\n- all punctuation are removed","efb4a29a":"At this point, we have 4 classifiers for each of the preferences. The accuracy for each of them are:\n- Favorite world (Extrovert\/Introvert) : 0.81\n- Information (Sensing\/ Intuition): 0.86\n- Decision (Thinking, Feeling): 0.77\n- Structure (Judging, Perceiving): 0.73\n\nWe will use RandomSearchCV with 5-fold cross validation to tune hyperparameters for XGBoost."}}