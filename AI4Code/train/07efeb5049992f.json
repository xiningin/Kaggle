{"cell_type":{"d4212b1e":"code","0aa4a3b5":"code","4788c7d0":"code","16a020f0":"code","138c9152":"code","2e50d046":"code","85844668":"code","b47bdd36":"code","68f52e07":"code","db1475ac":"code","651b1a20":"code","00c51360":"code","61289753":"code","36c1dd23":"code","19a28947":"code","4f5822fe":"code","2a03b35c":"code","4e15452b":"code","9207673d":"code","cc649432":"code","b79892ed":"code","a0187895":"code","79893314":"code","ee69a3c1":"code","27c79022":"code","87199315":"code","d4b3c425":"code","a85f2b75":"markdown","5a7608e0":"markdown","f87bf396":"markdown","560f2660":"markdown","76a192fa":"markdown","753bd592":"markdown","02d9b8be":"markdown","2a235d5c":"markdown","7a085c54":"markdown","4c9c0c0e":"markdown","af1b8cd2":"markdown","abd486f9":"markdown","56e41a7f":"markdown","d163dd31":"markdown","e9dc3f1e":"markdown","e6419f8d":"markdown"},"source":{"d4212b1e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image= https=\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only ..\/input\/ directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using Save & Run All \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0aa4a3b5":"# EDA libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n%matplotlib inline","4788c7d0":"# read train data\ndata = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndata.head()","16a020f0":"# edit train data\ndata_edited = data.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1)\ndata_edited.head()","138c9152":"# Check if there are data missing, and how much of them.\ndata_edited.isna().sum()","2e50d046":"# Check ratio of missing data\ndata_edited.isna().sum()\/len(data_edited)","85844668":"# Delete missing Embarked value\ndata_edited.dropna(subset=['Embarked'], inplace = True)\n\n# Fill missing age with median age\ndata_edited['Age'].fillna(data_edited['Age'].median(), inplace = True)","b47bdd36":"# Check if there are still missing values.\ndata_edited.isna().sum()","68f52e07":"# Check the datatypes of features.\ndata_edited.info()","db1475ac":"# Convert String Objects to categories and then convert to numbers.\nfor label, content in data_edited.items():\n    if pd.api.types.is_string_dtype(content):\n        data_edited[label] = content.astype('category').cat.as_ordered()\n        data_edited[label] = pd.Categorical(content).codes","651b1a20":"# Recheck feature dtype.\ndata_edited.info()","00c51360":"# Correlation Matrix Visualization\ncorr_matrix = data_edited.corr()\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    corr_matrix,\n    annot=True,\n    fmt='.2f',\n    cmap='YlGnBu'\n);","61289753":"# Models\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# Model Evaluators\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV","36c1dd23":"# Split data into training and validation\nX = data_edited.drop('Survived', axis=1)\ny = data_edited['Survived']\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)","19a28947":"model = RandomForestClassifier()\nmodel.fit(X_train, y_train)\nmodel.score(X_val, y_val)","4f5822fe":"# Create model dictionary for looping fit\nmodels = {\n    'LinearSVC': LinearSVC(),\n    'SVC': SVC(),\n    'KNeighborsClassifier': KNeighborsClassifier(),\n    'RandomForestClassifier':RandomForestClassifier(),\n    'LogisticRegression':LogisticRegression()\n}","2a03b35c":"# # Fit and evaluate models\n# def fit_and_score(models, X_train, X_test, y_train, y_test):\n#     np.random.seed(1)\n#     model_scores={}\n#     for name, model in models.items():\n#         model.fit(X_train, y_train)\n#         model_scores[name]=model.score(X_test, y_test)\n#     return model_scores\n","4e15452b":"# model_scores = fit_and_score(models, X_train, X_val, y_train, y_val)\n# model_scores","9207673d":"# # Visualize the score\n# model_compare = pd.DataFrame(model_scores, index=['accuracy'])\n# model_compare.T.plot.bar();","cc649432":"# New models and model grid dictionary\nmodels = {\n    'RandomForestClassifier': RandomForestClassifier(),\n    'LogisticRegression': LogisticRegression()\n}\n\nlog_reg_grid = {\n    'penalty':['l2', 'none'],\n    'dual': [True, False],\n    'tol': np.logspace(-4, 4, 10),\n    'C': np.logspace(-4, 4, 10),\n    'solver': ['liblinear'],\n}\n\nrf_grid = {\n    'n_estimators': np.arange(10, 1000, 50),\n    'max_depth': [None, 3, 5, 10],\n    'min_samples_split': np.arange(2, 20, 2),\n    'min_samples_leaf': np.arange(1, 20, 2)\n}","b79892ed":"# RandomizedSearchCV on RandomForest\n# np.random.seed(1)\n\n# rs_rf_reg = RandomizedSearchCV(\n#     RandomForestClassifier(),\n#     param_distributions=rf_grid,\n#     cv=5,\n#     n_iter=20,\n#     verbose=True\n# )\n\n# rs_rf_reg.fit(X_train, y_train);\n# rs_rf_reg.score(X_val, y_val)","a0187895":"# rs_rf_reg.best_params_","79893314":"# Copy down the best params in RF\n# {'n_estimators': 760,\n#  'min_samples_split': 6,\n#  'min_samples_leaf': 3,\n#  'max_depth': None}","ee69a3c1":"# RandomizedSearchCV on LogisticRegression\n# np.random.seed(3)\n\n# rs_log_reg = RandomizedSearchCV(\n#     LogisticRegression(),\n#     param_distributions=log_reg_grid,\n#     cv=5,\n#     n_iter=50,\n#     verbose=True\n# )\n\n# rs_log_reg.fit(X_train, y_train);\n# rs_log_reg.score(X_val, y_val)","27c79022":"# rs_log_reg.best_params_","87199315":"# Comment RandomizedSearchCV about and use the model to predict the test data.\n\npred = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\npred.drop(['PassengerId', 'Name', 'Ticket', 'Cabin'], axis=1, inplace = True)\npred['Age'].fillna(pred['Age'].median(), inplace = True)\npred['Fare'].fillna(pred['Fare'].mean(), inplace = True)\n\nfor label, content in pred.items():\n    if pd.api.types.is_string_dtype(content):\n        pred[label] = content.astype('category').cat.as_ordered()\n        pred[label] = pd.Categorical(content).codes\n\n\nmodel = RandomForestClassifier(n_estimators=760,\n                           min_samples_split=6,\n                           min_samples_leaf=3,\n                           max_depth=None)\n\nmodel.fit(X_train, y_train)\nlabels = model.predict(pred)\n","d4b3c425":"test = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\n\noutput = pd.DataFrame(\n    {'PassengerId': test.PassengerId, 'Survived': labels}\n)\noutput.to_csv('submission.csv', index=False)","a85f2b75":"## 1.2 Preprocess data","5a7608e0":"## 1.1 Tools","f87bf396":"# 0.4 Features\n\n**Data dictionary for knowing some details**\n* survival, Survival, 0 = No, 1 = Yes\n* pclass, Ticket class, 1 = 1st, 2 = 2nd, 3 = 3rd\n* sex, Sex, \n* Age, Age in years, \n* sibsp, # of siblings \/ spouses aboard the Titanic, \n* parch, # of parents \/ children aboard the Titanic, \n* ticket, Ticket number, \n* fare, Passenger fare, \n* cabin, Cabin number, \n* embarked, Port of Embarkation, C = Cherbourg, Q = Queenstown, S = Southampton\n","560f2660":"# 0.3 Evaluation\n\n95% Accuracy on validation data.","76a192fa":"The column `Embarked` has only 2 values missing, we just delete them.\n\nThe column `Age` has about 20% of the missing value, we cannot just delete them, instead, we fill the NaN with median age of the data.","753bd592":"# 1. EDA","02d9b8be":"\n## 2.1 Tools","2a235d5c":"Great, now let's make sure all features are numerical","7a085c54":"According to correlation matrix, sex effects most on survival rate.","4c9c0c0e":"**According to the scores, we will use RandomForestClassifer and LogisticRegression and tune their hyperparameters to get the highest score**","af1b8cd2":"# 0.1 Problem Def\nUse machine learning to create a model that predicts which passengers survived the Titanic shipwreck.","abd486f9":"`0.741 < 0.80`, we will keep the RandomForestClassifer with the best params\n\n`{'n_estimators': 760,\n 'min_samples_split': 6,\n 'min_samples_leaf': 3,\n 'max_depth': None}`","56e41a7f":"According to common sense, PassengerId, Name, Ticket, Cabin has nothing to do with survival rate, so we just delete these columns and put the edited dataframe into a new variable.","d163dd31":"# 0.2 Data\nhttps=\/\/www.kaggle.com\/c\/titanic\/data\n\nThe data has been split into two groups=\n\ntraining set (train.csv)\ntest set (test.csv)\n\nSince test labels are not given, I will split training set into training and validation datas.(ratio = 0.2)","e9dc3f1e":"# 2. Modeling\n\nhttps=\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html\n\nAccording to Scikit-Learn map, we will try=\n* LinearSVC\n* KNeighboursClassifier\n* RandomForestClassifier\n* SVC\n\nWe will also try LinearRegression which is not in the map but is also a common classifier.\n\nSteps=\n* Prepare Tools\n* Split Data\n* Create Model Library\n* Fit model and evaluate accuracy\n* Choose two models with highest score\n* Parameter tuning with RandomizedSearchCV\n* Choose the model and related params with highest score\n* Make predictions and submit","e6419f8d":"## 1.3 Data Visualization"}}