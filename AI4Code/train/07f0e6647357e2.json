{"cell_type":{"1b89b86e":"code","e4bc41ef":"code","e465e997":"code","4a2c3470":"code","552c3f26":"code","e469db9b":"code","3c7394a0":"code","dc2ceacc":"code","b4562788":"code","b1a8add1":"code","1b55bbec":"code","50adf627":"code","e1344e47":"code","1ff3f5d7":"code","48dbb4d5":"code","33670f46":"code","9b92d2db":"code","245488df":"code","b301d0d7":"code","f6e02cab":"code","b007679f":"code","17c5c96e":"code","1e359b3a":"code","510e4673":"code","c37f4abd":"code","b353dee7":"code","5598cc59":"code","9a98f5e3":"code","c0f22197":"code","38f3f0b1":"code","878d3e90":"code","41069795":"code","1e0ffa26":"code","4e5f4138":"code","07699985":"code","531a6da6":"code","cf2933d7":"code","9ba34c96":"code","3eaba744":"code","af3ab350":"code","19a18dbb":"code","830bcd54":"code","72075a94":"code","9e139557":"code","d21ae622":"code","72967b21":"code","7e7e39db":"code","30ff97ff":"code","de1e9e9a":"code","05849845":"code","96430c0a":"code","16990180":"code","d5560215":"code","1eceda9b":"code","e30cb41e":"code","7a1fd2c9":"code","d3718c59":"code","563cd002":"code","fc6706c6":"code","5ba8535e":"code","6befcb6d":"code","e9959cd0":"code","d7e88ee8":"code","29a427b5":"code","d644a8df":"markdown","34926f43":"markdown","30e9095c":"markdown","ef280381":"markdown","983d330b":"markdown","4b70c7de":"markdown"},"source":{"1b89b86e":"\n\n#import os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n #   for filename in filenames:\n #       print(os.path.join(dirname, filename))\n\n# required libraries\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import cross_val_score\nimport seaborn as sns\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import PolynomialFeatures\n","e4bc41ef":"#Import and clean data","e465e997":"# read the dataset\ndf = pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')\ndf.head()","4a2c3470":"print('Column Names')\ndf.columns","552c3f26":"df.info()","e469db9b":"# drop SL_no & salary which is not needed\ndf=df.drop(['sl_no'],axis=1)\n#df=df.drop(['salary'],axis=1)\n","3c7394a0":"#replace the categories under status\ndf=df.replace('Placed',1)   \ndf=df.replace('Not Placed',0) ","dc2ceacc":"df.info()","b4562788":"df.describe()","b1a8add1":"#pairplot distribution\n_ = sns.pairplot(df, hue=\"status\",corner=True,markers=[\"s\", \"D\"])","1b55bbec":"# check distribution of salary \nplt.figure(figsize=(15,5)) \nax= sns.countplot(x='salary', data=df)\nax.set_xticklabels(ax.get_xticklabels(), rotation=90)\nplt.tight_layout()\nplt.show()","50adf627":"# scatter plots for numeric data\nsns.lmplot(x='salary',y=\"ssc_p\", data=df,fit_reg=True,scatter_kws={'color':'black'},line_kws={'color':'grey','linewidth':'1'}) ","e1344e47":"sns.lmplot(x='salary',y=\"mba_p\", data=df,fit_reg=True,scatter_kws={'color':'black'},line_kws={'color':'grey','linewidth':'1'})","1ff3f5d7":"sns.lmplot(x='salary',y=\"degree_p\", data=df,fit_reg=True,scatter_kws={'color':'black'},line_kws={'color':'grey','linewidth':'1'})","48dbb4d5":"sns.lmplot(x='salary',y=\"etest_p\", data=df,fit_reg=True,scatter_kws={'color':'black'},line_kws={'color':'grey','linewidth':'1'})","33670f46":"sns.lmplot(x='salary',y=\"hsc_p\", data=df,fit_reg=True,scatter_kws={'color':'black'},line_kws={'color':'grey','linewidth':'1'})","9b92d2db":"# Distribution of category data \ncategorical_var = [i for i in df.columns if df[i].dtypes =='object']\ncatVars = categorical_var[:] #shallow copy\n\nfig ,ax = plt.subplots(4,2,figsize=(12,12))\nfor axi ,var in zip(ax.flat,catVars):\n    sns.countplot(x=df['status'],hue=df[var],ax=axi)","245488df":"#box plot of numeric data\nfig, ax=plt.subplots(3,2, figsize=(12,8)) # Creates a grid of 3 rows and 6 colums as we have 18 numeric columns.\nnumeric_col=df.select_dtypes('float64').columns # For selecting perticuler datatype\nfor num_col, axis in zip(numeric_col, ax.ravel()): # ax.ravel() kind of flattens the 2d grid we created, for iteration\n    sns.boxplot(x=num_col, data=df, ax=axis) \n\nplt.tight_layout() # makes the layout of the plot tight, i.e. to avoid overlapping of plots","b301d0d7":"# distribution of numeric data\ndf.hist(bins=40, figsize=(20,20), layout=(10,3), color=\"#FA5858\") \nplt.show()","f6e02cab":"# correlation matrix heatmap visualization\nsns.set(style=\"white\")\n\n# Generate a mask for the upper triangle\nmatrix = np.triu(df.corr())\n\n# Set up the matplotlib figure to control size of heatmap\nfig, ax = plt.subplots(figsize=(10,10))\n\n\n_ = sns.heatmap(df.corr(), annot=True, annot_kws={\"size\": 12}, square=True, \ncmap='coolwarm' , vmin=-1, vmax=1, fmt='.2f')\n","b007679f":"df.nunique()","17c5c96e":"#checking the unique values for categorical columns\ncategorical_columns = df.columns[df.dtypes == object]\nfor col in categorical_columns:\n    print(\"Unique values for \", col ,\": \" ,df[col].unique())","1e359b3a":"# to view get dummies columns\nprint(\"Original features:\\n\", list(df.columns), \"\\n\")\ndf_dummies = pd.get_dummies(df)\nprint(\"Features after get_dummies:\\n\", list(df_dummies.columns))","510e4673":"dff=df_dummies","c37f4abd":"# assign data to X and y\nX=dff.drop('status',axis=1)\nX=dff.drop('salary',axis=1)\ny=dff['status']\n","b353dee7":"dff.info()","5598cc59":"# train-test-split \nX_y_train_test = train_test_split(X, y, test_size = 0.2, shuffle=True, random_state = 0)\nX_train, X_test, y_train, y_test =X_y_train_test","9a98f5e3":"X_test.shape","c0f22197":"y_train","38f3f0b1":"#Classifiers List\nclf_names = []\ngboost =GradientBoostingClassifier(random_state=77) \nrf = RandomForestClassifier(random_state=77) \nsvm_clf = SVC(random_state=77) # classification SVC\nlogr = LogisticRegression(max_iter=2000,random_state=77)\n#lr= LinearRegression(random_state=77) # regression only\nknn = KNeighborsClassifier()\nnn=MLPClassifier(alpha=1,random_state=77,max_iter=400000) # Neural Networks\nada =AdaBoostClassifier(random_state=77)\ndt= DecisionTreeClassifier(random_state=77)\ngpc=GaussianProcessClassifier(1.0 * RBF(1.0),random_state=77)\nsvc=SVC(kernel=\"linear\", C=0.025,probability=True,random_state=77) # Linear SVM\nnb= GaussianNB()\nqda= QuadraticDiscriminantAnalysis()\nsvm_rbf= SVC(kernel='rbf',gamma=2, C=1,probability=True,random_state=77) #RBF SVM\nsgd=SGDClassifier(max_iter=1000, tol=10e-3,penalty='elasticnet',random_state=77)\n\n","878d3e90":"# Pipeline\n\ndef fit_predict_score(model, X_y_train_test):\n    X_train, X_test, y_train, y_test = X_y_train_test\n    # extracts and flattens the series y_train which has series and values to values only\n    model.fit(X_train, y_train.values.ravel())\n    y_pred = model.predict(X_test)\n    score = accuracy_score(y_test, y_pred)\n    print('Accuracy Score on test data :',score)\n    return \n","41069795":"fit_predict_score(gboost, X_y_train_test)","1e0ffa26":"fit_predict_score(rf, X_y_train_test)","4e5f4138":"fit_predict_score(svm_clf, X_y_train_test)","07699985":"fit_predict_score(logr, X_y_train_test)","531a6da6":"#fit_predict_score(lr, X_y_train_test)","cf2933d7":"fit_predict_score(knn, X_y_train_test)","9ba34c96":"fit_predict_score(nn, X_y_train_test)","3eaba744":"fit_predict_score(ada, X_y_train_test)\n","af3ab350":"fit_predict_score(dt, X_y_train_test)","19a18dbb":"fit_predict_score(gpc, X_y_train_test)","830bcd54":"fit_predict_score(svc, X_y_train_test)","72075a94":"fit_predict_score(nb, X_y_train_test)","9e139557":"fit_predict_score(qda, X_y_train_test)","d21ae622":"fit_predict_score(svm_rbf, X_y_train_test)","72967b21":"fit_predict_score(sgd, X_y_train_test)","7e7e39db":"# Pipeline with MinMaxScaler()\n\ndef MMScale_fit_predict_score(model, X_y_train_test):\n    X_train, X_test, y_train, y_test = X_y_train_test\n    scaler = MinMaxScaler() \n    X_train_scaled = scaler.fit_transform(X_train) \n    # extracts and flattens the series y_train which has series and values to values only\n    model.fit(X_train_scaled, y_train.values.ravel()) \n    X_test_scaled = scaler.transform(X_test) \n    y_pred = model.predict(X_test_scaled) \n    score = accuracy_score(y_test, y_pred)\n    print('Accuracy Score on test data :',score)\n    return ","30ff97ff":"MMScale_fit_predict_score(gboost, X_y_train_test)","de1e9e9a":"MMScale_fit_predict_score(rf, X_y_train_test)","05849845":"MMScale_fit_predict_score(svm_clf, X_y_train_test)","96430c0a":"MMScale_fit_predict_score(logr, X_y_train_test)","16990180":"#MMScale_fit_predict_score(lr, X_y_train_test)","d5560215":"MMScale_fit_predict_score(knn, X_y_train_test)","1eceda9b":"MMScale_fit_predict_score(nn, X_y_train_test)","e30cb41e":"MMScale_fit_predict_score(ada, X_y_train_test)","7a1fd2c9":"MMScale_fit_predict_score(dt, X_y_train_test)","d3718c59":"MMScale_fit_predict_score(gpc, X_y_train_test)","563cd002":"MMScale_fit_predict_score(svc, X_y_train_test)","fc6706c6":"MMScale_fit_predict_score(nb, X_y_train_test)","5ba8535e":"MMScale_fit_predict_score(qda, X_y_train_test)","6befcb6d":"MMScale_fit_predict_score(svm_rbf, X_y_train_test)","e9959cd0":"MMScale_fit_predict_score(sgd, X_y_train_test)","d7e88ee8":"# Random forest Feature Importance\n# Getting the feature importance\nfeature_importances_=rf.feature_importances_\nfeature_importances=pd.DataFrame({'Feature_name':X.columns, 'Feature_importance':feature_importances_})\n\nfig, ax=plt.subplots(1, figsize=(10,5))\nsns.barplot(y='Feature_name', x='Feature_importance', data=feature_importances, ax=ax)\n\n# For making the graph look good\nplt.xticks(fontsize=12, rotation=0); \nplt.yticks(fontsize=14);\n\nplt.xlabel('Feature Importance',fontsize=18)\nplt.ylabel('Feature Name',fontsize=18)","29a427b5":"#Conclusion\n#Most of the models gave a very high score to the data especially after scaling the data\n#Also the 5 most important factors for a job placement are ssc_p, degree_p, hsc_p, mba_p and etest_p.\n","d644a8df":"# Split_Train_Test data","34926f43":"# Instantiate Classifiers","30e9095c":"# Run Models with Normalization (MinMaxScaler)","ef280381":"# Run Models ","983d330b":"# Normalization (MinMaxScaler)","4b70c7de":"# Fit, Predict,Score"}}