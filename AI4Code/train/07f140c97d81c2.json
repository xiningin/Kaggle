{"cell_type":{"f3250afa":"code","15bf9808":"code","def48cdd":"code","a1d8c830":"code","2871295e":"code","65db14a5":"code","7b82b301":"code","99c11f38":"code","71617a5a":"code","311f94c7":"code","7e4b5741":"code","8334f616":"code","24d95f25":"code","9c82554b":"code","7c96d291":"code","54d5cd8b":"code","69d46f08":"code","5973589c":"code","f19f7252":"code","65bf1f89":"markdown","2a357084":"markdown","e91e6405":"markdown","ca3da186":"markdown","8c599966":"markdown","72a5f503":"markdown","0c01a2fe":"markdown","42be0391":"markdown","1e3a3f2c":"markdown","2749edf2":"markdown","2e848a48":"markdown","911699e5":"markdown"},"source":{"f3250afa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15bf9808":"# Importing needed libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\n\n# defining describer function\n\ndef summarize_features(df):\n    # first column will be data types of each feature\n    summary = pd.DataFrame(df.dtypes,columns=['dtypes'])\n    summary = summary.reset_index()\n    # second column will contain the name of the feature\n    summary['Name'] = summary['index']\n    # switch name and dtypes\n    summary = summary[['Name','dtypes']]\n    # how many missing values in each feature\n    summary['Missing'] = df.isnull().sum().values\n    # how many unique values in each feature (cardinality indicator)\n    summary['Uniques'] = df.nunique().values\n\n    return summary","def48cdd":"df_train = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/cat-in-the-dat\/sample_submission.csv', index_col='id')","a1d8c830":"summarize_features(df_train)","2871295e":"# getting names of all ordinal feature indices, same encoder can be used for binary data\nord_cols = [\"ord_{}\".format(i) for i in range(6)] + [\"bin_{}\".format(i) for i in range(5)]\n# extracting data from dataframe, you can call ord_data.head() alone in a cell to see the output\nord_data = df_train[ord_cols]\n# initializing an encoder object (you should know oop)\nord_encoder = OrdinalEncoder()\n# the encoder learns the categories and replaces them with numeric equivalents\nencoded_ord_data = ord_encoder.fit_transform(ord_data)","65db14a5":"# you can access learned categories from the .categories_ attribute see the docs for more\n\n## also you can pass the categories your self to the encoder object\n## in case you need them to have a specific order which is most of the cases\n## `OrdinalEncoder([\"cat_1\", \"cat_2\"...])`","7b82b301":"encoded_ord_data","99c11f38":"# getting names of all nominal feature indices, same encoder can be used for binary data\nnom_cols = [\"nom_{}\".format(i) for i in range(10)]\n# extracting data from dataframe, you can call nom_data.head() alone in a cell to see the output\nnom_data = df_train[ord_cols]\n# initializing an encoder object (you should know oop)\nnom_encoder = OneHotEncoder()\n# the encoder learns the categories and replaces them with one hot array equivalent\nencoded_nom_data = nom_encoder.fit_transform(ord_data)","71617a5a":"encoded_nom_data.shape\n# the shape is (m, 257) which means we have 257 columns after encoding,\n# even though before encoding we only had 10 nominal columns, which means that\n# each column has been replaced with multiple columns all containing zeros but one of them as explained","311f94c7":"from sklearn.base import BaseEstimator, TransformerMixin\n\nclass CyclicEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        super().__init__()\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        columns = X.columns\n        for col in columns:\n            X[col+'_sin'] = np.sin( (2*np.pi*X[col]) \/ X[col].nunique() )\n            X[col+'_cos'] = np.cos( (2*np.pi*X[col]) \/ X[col].nunique() )\n        \n        onehot_encoder = OneHotEncoder(sparse=False, handle_unknown=\"ignore\")\n        new_data = X.drop(columns, axis=1)\n        return onehot_encoder.fit_transform(new_data)\n    ","7e4b5741":"# getting names of all nominal feature indices, same encoder can be used for binary data\ncyc_cols = [\"day\", \"month\"]\n# extracting data from dataframe, you can call nom_data.head() alone in a cell to see the output\ncyc_data = df_train[cyc_cols]\n# initializing an encoder object (you should know oop)\ncyc_encoder = CyclicEncoder()\n# the encoder learns the categories and replaces them with one hot array equivalent\nencoded_cyc_data = cyc_encoder.fit_transform(cyc_data)","8334f616":"encoded_cyc_data.shape","24d95f25":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\n\nbasic_transformer = ColumnTransformer([\n    (\"ordinal_transformer\", OrdinalEncoder(), ord_cols),\n    (\"nominal_transformer\", OneHotEncoder(handle_unknown=\"ignore\"), nom_cols),\n    (\"cyclic_transformer\", CyclicEncoder(), cyc_cols)\n])\n\nX = df_train.drop(['target', 'id'], axis=1)\ny = df_train['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\n# now we can preprocess the data all at once, now let's make a pipeline with a logistic regression model\n\nbasic_pipeline = Pipeline([\n    (\"preprocessor\", basic_transformer),\n    (\"scaler\", StandardScaler(with_mean=False)),\n    (\"model\", LogisticRegression())\n])\n\nbasic_pipeline.fit(X_train, y_train)\ntrain_acc = basic_pipeline.score(X_train, y_train)\nval_acc = basic_pipeline.score(X_val, y_val)\nprint(\"Training Accuracy = {}, Validation Accuracy = {}\".format(train_acc, val_acc))","9c82554b":"# let's define an encoder that replaces each category with its normalized frequency\n\nclass NormalizedFrequencyEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self, log_scale=False):\n        \"\"\"\n        For high cardinality features we are going to use logarithmic scaling,\n        because we don't want numbers to be very small and close to one another\n        \"\"\"\n        super().__init__()\n        self.log_scale = log_scale\n        \n    def fit(self, X, y=None):\n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        columns = X.columns\n        m = X.shape[0]\n        \n        for col in columns:\n            frequency_map = X[col].value_counts() \/ m\n            X[col] = X[col].map(frequency_map)\n            if self.log_scale == True:\n                X[col] = np.log(X[col])\n        \n        return X\n\nstatistics_encoder = NormalizedFrequencyEncoder()\n# let's test it on a couple of features\nord_3 = df_train.copy()[['ord_3', 'ord_4']]\npreprocessed = statistics_encoder.fit_transform(ord_3)\npreprocessed.head()\n\n# for high cardinality features the numbers are going to be very close to zero.\n# going to the logarithmic scale at this case could be a good idea","7c96d291":"class MeanEncoder(BaseEstimator, TransformerMixin):\n    def __init__(self):\n        super().__init__()\n        \n    def fit(self, X, y=None):\n        self.columns = X.columns\n        X = X.copy()\n        X['target'] = y\n        self.mean_maps = dict()\n        for col in self.columns:\n            self.mean_maps[col] = X.groupby([col])['target'].mean() \n        return self\n    \n    def transform(self, X, y=None):\n        X = X.copy()\n        for col in self.columns:\n            X[col] = X[col].map(self.mean_maps[col])\n        \n        return X","54d5cd8b":"mean_encoder = MeanEncoder()\n# let's test it on a couple of features\nord_3 = df_train.copy()[['ord_3', 'ord_4']] # target is needed for fitting\npreprocessed = mean_encoder.fit_transform(ord_3, df_train['target'])\npreprocessed.head()\n\n# for high cardinality features the numbers are going to be very close to zero.\n# going to the logarithmic scale at this case could be a good idea","69d46f08":"advanced_transformer = ColumnTransformer([\n    (\"ordinal_transformer\", MeanEncoder(), ord_cols),\n    (\"nominal_low_cardinality_transformer\", NormalizedFrequencyEncoder(), nom_cols[:5]),\n    (\"nominal_high_cardinality_transformer\", NormalizedFrequencyEncoder(log_scale=True), nom_cols[5:]),\n    (\"cyclic_transformer\", CyclicEncoder(), cyc_cols),\n])\n\nX = df_train.drop(['target', 'id'], axis=1)\ny = df_train['target']\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n\n# now we can preprocess the data all at once, now let's make a pipeline with a logistic regression model\n\nadvanced_pipeline = Pipeline([\n    (\"preprocessor\", advanced_transformer),\n    (\"scaler\", StandardScaler()),\n    (\"model\", LogisticRegression())\n])\n\nadvanced_pipeline.fit(X_train, y_train)\ntrain_acc = advanced_pipeline.score(X_train, y_train)\nval_acc = advanced_pipeline.score(X_val, y_val)\nprint(\"Training Accuracy = {}, Validation Accuracy = {}\".format(train_acc, val_acc))","5973589c":"# let's submit a solution\nX = df_test.drop('id', axis=1)\npreds = advanced_pipeline.predict(X)\npreds","f19f7252":"submission['target'] = preds\nsubmission.to_csv(\"results.csv\")\n\nread_submission = pd.read_csv(\"results.csv\")\nread_submission.head()","65bf1f89":"#### Encoding cyclic features, and a bit of OOP\nThe following image shows a two dimintional representation of a cyclic feature like month of the year.  \n![cyclic features](https:\/\/miro.medium.com\/max\/343\/1*70cevmU8wNggGJEdLam1lw.png)  \none way of handling this kind of feature is OneHotEncoding, another way is dealing with the month as an angle and calculate its sin and cosin then encode those using OneHotEncoding. both ways are valid the second seems to be more effective however the first is easier.  \nNow we are going to implement our own CyclicEncoder class extending the sklearn BaseEstimator and TranformerMixin classes","2a357084":"#### Ordinal Encoding\nThis type of encoding is used when dealing with ordinal features which are features that have some sort of order to its values, for example the class in an airplane. It is obvious that first class is better than second class which is also better than third class, so this is an example of an ordinal feature.  \nWe use [sklearn.preprocessing.OrdinalEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OrdinalEncoder.html) with it.  \nlet's use this encoder on features from `ord_0` to `ord_5`","e91e6405":"# Conclusion and reading material\nIf you've gone this far, congratulations, you now know a lot of things about categorical feature encoding.  \nIf you use these techniques along with some data analysis and feature selection techniques, it shall give your models a nice boost.  \nWe didn't cover everything in this notebook, and we encourage you to look some keywords up and read the documentation of sklearn carefully or even read more notebooks on this compeition to understand more, and learn more advanced concepts.  \nHappy learning :)  \n## More readng material\n[https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02](https:\/\/towardsdatascience.com\/all-about-categorical-variable-encoding-305f3361fd02)\n\n[https:\/\/towardsdatascience.com\/cyclical-features-encoding-its-about-time-ce23581845ca](https:\/\/towardsdatascience.com\/cyclical-features-encoding-its-about-time-ce23581845ca)\n\n[https:\/\/towardsdatascience.com\/ml-intro-5-one-hot-encoding-cyclic-representations-normalization-6f6e2f4ec001](https:\/\/towardsdatascience.com\/ml-intro-5-one-hot-encoding-cyclic-representations-normalization-6f6e2f4ec001)\n\nand please read [sklearn's documentation](https:\/\/scikit-learn.org\/stable\/)\n\n","ca3da186":"# Introduction\nLet's talk about types of features in machine learning. Basically, there are two main types of features\n- numerical features (usually continous numbers)\n- categorical features (descrete numbers or string categories e.g. male \/ female)  \n\neach of which has its own way of handling and preprocessing.  \nIn this notebook we descuss different types of categorical features and how to encode and preprocess them.  \n### Types of categorical features\n- Binary\n- Nominal ( low and high cardinality )\n- Ordinal ( low and high cardinality )\n- Cyclic\n\n> high cardenality (informally) means a lot of categories","8c599966":"> **NOTE**\n> You might be familiar with pandas.DataFrame.get_dummies which does onehot encoding too, but the sklearn encoder returns a sparce matrix which is more efficient and consumes less memory.","72a5f503":"> **NOTE** The accuracy is not that good in this submission because we didn't do any kind of model selection, perhaps you can experamint on your own with different models and grid search through model parameters to make better predictions, however, our main focus in this notebook is to explain different preprocessing techniques and get you familiar with sklearn encoders and base classes.","0c01a2fe":"After inspecting the output of the function above we see that we don't have any missing value in any column, and it also seems that the target variable has two unique values which means that this is a binary classification problem.  \nThe names `nom`, `ord` and `bin` are to indicate that these columns represent nominal, ordinal and binary features (duh!!), day and month are both cyclic featuers, more about that later.  \nnow let's study how to encode these features.  \n### Types of encoding\n- Ordinal Encoding\n- One Hot Encoding\n- Encoding Cyclic Features\n- Encoding from Statistics\n- Target Encoding","42be0391":"### Target Encoding\nIn statistics encoding we used statistics from the feature itself, but that doesn't tell us anything about the label, how about we use statistics from the training set labels (target) to encode our features?  \nIn this example we are going to use mean encoding which uses the mean of the target to encode a categorical feature.  \nThere are many ways to do mean encoding, we are going to do the most straight forward way, but we encourage you to lookup more interesting and sophisticated ways.\n> **NOTE** You only fit this encoder once on the training set and not on the test set\n> as a general rule of thumb, only use fit on the training set nomatter what encoder you're using and use transform on the validation and test set.","1e3a3f2c":"#### OneHot Encoding\nOneHot encoding is used with nominal data. Unlike ordinal data, nominal data does not have a specific order or quantitative attribute to it, for example, Blood type (A+, B-, etc...) nothing necissarely says that one is better than the other. in this case you would want to use one hot encoding which basically replaces each column with `n` columns where `n` is the number of categories in the column all of which will have the value `0` except one of the columns will have the value `1` the position of this `1` indicates to which category this sample belongs.  \nTo use it check out [sklearn.preprocessing.OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)","2749edf2":"### Pipeline and ColumnTransformer\nBefore we descuss cardinality and what not let's get familiar with sklearn pipelines which are going to make your code much more organized and readable and also let's test a dummy LogisticRegression model, shall we?  \n[sklearn.pipeline.Pipeline](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.pipeline.Pipeline.html) \/ [sklearn.compose.ColumnTransformer](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.compose.ColumnTransformer.html?highlight=columntransformer#sklearn.compose.ColumnTransformer)","2e848a48":"## Advanced encoding techniques\nSo far so good, you can encode all categorical features now, but some features require further preprocessing and visualization, like features with high cardinality (many categories but in fancy terms).  \nAlso sometimes (usually with ordinal features) you might need to use statistics from the target to encode the feature (e.g. mean encoding) or even statistics from other features.","911699e5":"### Statistics Encoding\nIn some cases you might want to choose a certain statistic with which you want to encode some feature, for example the normalized frequency of the categories (frequency \/ number_of_samples).  \nI like oop so I'll do as I did with Cyclyc features, so ... sorry, I guess!"}}