{"cell_type":{"185506c7":"code","7fd80cdd":"code","b39c8a1b":"code","05bf9bfb":"code","4de8b50a":"code","37cab43b":"code","c164676c":"code","a9672250":"code","639ab57d":"code","af2cfe90":"code","e3681df6":"code","dc75d0f7":"code","c0b19a31":"code","73a85dca":"code","05f702bf":"code","d3d0c8cb":"code","36a7a510":"code","966cfaff":"code","75c4cbf3":"code","9ba1eefc":"code","ce29e353":"code","1aae86fa":"code","6f07355e":"code","25f3990e":"code","2e3a90e3":"code","3e84d8b7":"code","1f75a633":"code","7757b588":"code","4971f318":"code","0d04c0fd":"code","2d82b173":"code","1704890f":"code","ebce2f15":"code","474f9b84":"code","8c108a78":"code","9868c730":"code","fda99bcb":"code","5bb58f71":"code","7d25bf6a":"code","3bbfccfa":"code","ae30da49":"code","fa63de00":"code","010c2f75":"code","cf8acea7":"code","509e2734":"code","8d7efcfb":"code","16c61cd0":"code","72877e7b":"code","2b1631b7":"code","8a6fbfa0":"markdown","bcda27b2":"markdown","df96791e":"markdown","9368f0cf":"markdown","742d920e":"markdown","e94f5fe4":"markdown","f8dd50e9":"markdown","b3668059":"markdown","40e8835c":"markdown","915c663c":"markdown","23140cb9":"markdown"},"source":{"185506c7":"#import important liberaries\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nimport matplotlib.pyplot as plt\nimport seaborn as sb\n\n%matplotlib inline\n\n# suppress warnings from final output\nimport warnings\nwarnings.simplefilter(\"ignore\")\n","7fd80cdd":"#load data\ndf_dist=pd.read_csv(r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\ndf_dist.info()","b39c8a1b":"# set up to view all the info of the columns\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)","05bf9bfb":"df_dist.sample(10)","4de8b50a":"def assess_NA(data):\n    \"\"\"\n    Returns a pandas dataframe denoting the total number of NA values and the percentage of NA values in each column.\n    The column names are noted on the index.\n    \n    Parameters\n    ----------\n    data: dataframe\n    \"\"\"\n    # pandas series denoting features and the sum of their null values\n    null_sum = data.isnull().sum()# instantiate columns for missing data\n    total = null_sum.sort_values(ascending=False)\n    percent = ( ((null_sum \/ len(data.index))*100).round(2) ).sort_values(ascending=False)\n    \n    # concatenate along the columns to create the complete dataframe\n    df_NA = pd.concat([total, percent], axis=1, keys=['Number of NA', 'Percent NA'])\n    \n    # drop rows that don't have any missing data; omit if you want to keep all rows\n    #df_NA = df_NA[ (df_NA.T != 0).any() ]\n    \n    return df_NA","37cab43b":"assess_NA(df_dist)","c164676c":"#unique values in districts columns\nprint(df_dist['pct_free\/reduced'].nunique(dropna = True))\nprint(df_dist['pp_total_raw'].nunique(dropna = True))\nprint(df_dist['pct_black\/hispanic'].nunique(dropna = True))\nprint(df_dist['county_connections_ratio'].nunique(dropna = True))\n    ","a9672250":"#average points from county_connections_ratio\n#this column has two unique values [.18,1[ and [1,2[ so I am using numbers closure to 1 and 2 for open interval side\n#using this points I will substitute intervals to single values\nm=(0.18+0.999999)\/2\nm1=(1+1.999999)\/2\nm,m1\n","639ab57d":"#change intervals to single values\nfor i in range(0,233):\n    if df_dist['county_connections_ratio'][i]=='[0.18, 1[':\n        df_dist['county_connections_ratio'][i]=m\nfor i in range(0,232):\n    if df_dist['county_connections_ratio'][i]=='[1, 2[':\n        df_dist['county_connections_ratio'][i]=m1\n    \n        \n    \nprint(df_dist['county_connections_ratio']) \ndf_dist['county_connections_ratio'].nunique()","af2cfe90":"for i in range(0,233):\n    if df_dist['pct_black\/hispanic'][i]=='[0, 0.2[':\n        df_dist['pct_black\/hispanic'][i]=(0+0.1999)\/2\n    if df_dist['pct_black\/hispanic'][i]=='[0.2, 0.4[':\n        df_dist['pct_black\/hispanic'][i]=(0.2+0.3999)\/2\n    if df_dist['pct_black\/hispanic'][i]=='[0.4, 0.6[':\n        df_dist['pct_black\/hispanic'][i]=(0.4+0.5999)\/2\n    if df_dist['pct_black\/hispanic'][i]=='[0.8, 1[':\n        df_dist['pct_black\/hispanic'][i]=(0.8+0.9999)\/2\nfor i in range(0,233):\n    if df_dist['pct_free\/reduced'][i]=='[0, 0.2[':\n        df_dist['pct_free\/reduced'][i]=(0+0.1999)\/2\n    if df_dist['pct_free\/reduced'][i]=='[0.2, 0.4[':\n        df_dist['pct_free\/reduced'][i]=(0.2+0.3999)\/2\n    if df_dist['pct_free\/reduced'][i]=='[0.4, 0.6[':\n        df_dist['pct_free\/reduced'][i]=(0.4+0.5999)\/2\n    if df_dist['pct_free\/reduced'][i]=='[0.8, 1[':\n        df_dist['pct_free\/reduced'][i]=(0.8+0.9999)\/2\n    if df_dist['pct_free\/reduced'][i]=='[0.6, 0.8[':\n        df_dist['pct_free\/reduced'][i]=(0.6+0.7999)\/2","e3681df6":"for i in range(0,233):\n    if df_dist['pp_total_raw'][i]=='[14000, 16000[':\n        df_dist['pp_total_raw'][i]=(14000+15999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[6000, 8000[':\n        df_dist['pp_total_raw'][i]=(6000+7999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[10000, 12000[':\n        df_dist['pp_total_raw'][i]=(10000+11999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[8000, 10000[':\n        df_dist['pp_total_raw'][i]=(8000+9999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[12000, 14000[':\n        df_dist['pp_total_raw'][i]=(12000+13999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[16000, 18000[':\n        df_dist['pp_total_raw'][i]=(16000+17999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[20000, 22000[':\n        df_dist['pp_total_raw'][i]=(20000+21999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[18000, 20000[':\n        df_dist['pp_total_raw'][i]=(18000+19999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[22000, 24000[':\n        df_dist['pp_total_raw'][i]=(22000+23999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[4000, 6000[':\n        df_dist['pp_total_raw'][i]=(4000+5999.999)\/2\n    if df_dist['pp_total_raw'][i]=='[32000, 34000[':\n        df_dist['pp_total_raw'][i]=(32000+33999.999)\/2","dc75d0f7":"print(df_dist.isna().sum())","c0b19a31":"#fill missing using ffill\ndf_dist=df_dist.fillna(method=\"ffill\")\nprint(df_dist.isna().sum())\ndf_dist.head(20)","73a85dca":"df1=df_dist.drop(\"state\",axis=1)\n","05f702bf":"fig, axes = plt.subplots(2, 3, figsize=(10, 6))\n\nfor i, (idx, row) in enumerate(df1.set_index('locale').iterrows()):\n    ax = axes[i \/\/ 3, i % 3]\n    row = row[row.gt(row.sum() * .01)]\n    ax.pie(row, labels=row.index, startangle=30)\n    ax.set_title(idx)\n\nfig.subplots_adjust(wspace=.2)","d3d0c8cb":"for col in df_dist.columns:\n    sb.displot(df_dist[col])\n    plt.show()","36a7a510":"from pandas.plotting import scatter_matrix\nimport pandas.plotting as plt\n\n\nplt.scatter_matrix(df_dist)","966cfaff":"# Product dataset \ndf_prod=pd.read_csv(r'..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')\ndf_prod.info()","75c4cbf3":"df_prod.sample(10)","9ba1eefc":"import pandas as pd\nimport glob\n\npath = r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' # use your path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\ndf_eng = pd.concat(li, axis=0, ignore_index=True)","ce29e353":"df_eng.sample(10)","1aae86fa":"df_eng.info()","6f07355e":"#list all unique values of column\nfor col in df_dist1:\n    print(df_dist1[col].unique())","25f3990e":"df_dist1['pp_total_raw'].nunique()","2e3a90e3":"from pandas.plotting import scatter_matrix\nimport pandas.plotting as plt\n\nplt.scatter_matrix(df_dist)","3e84d8b7":"df_dist1.columns","1f75a633":"df_dist1.plot(x=\"locale\", y=[\"pct_black\/hispanic\", \"pct_free\/reduced\", \"county_connections_ratio\"], kind=\"bar\",figsize=(9,8))\nplt.show()","7757b588":"df_prod.head()","4971f318":"data=pd.concat([df_dist, df_prod, df_eng])\n#fill missing using ffill\ndata=data.fillna(method=\"ffill\")\n#drop duplicated column that is 'lp id'\n#data.dropna\n","0d04c0fd":"data.isna()","2d82b173":"# histograms of the variables\nfor col  in df_dist1.columns:\n    fig = df_dist[col].hist(xlabelsize=6, ylabelsize=6)\n#[x.title.set_size(6) for x in fig.ravel()]\n# show the plot\n    plt.show()","1704890f":"import seaborn as sb\nfor i in range(0,6):\n    sb.histplot(df_dist1[i])\n#sb.histplot(df_dist1['pct_free\/reduced'])\n","ebce2f15":"df_dist1['state'].nunique()","474f9b84":"df_eng.columns","8c108a78":"#","9868c730":"df_dist[df_dist.district_id.duplicated()].shape[0]","fda99bcb":"df_dist.dropna(how=\"all\")","5bb58f71":"#dealing with missing values\n","7d25bf6a":"def save_clean(self):\n    try:\n      self.df.to_csv('..\/data\/clean_data.csv', index=False)\n    except:\n      print('Log: Error while Saving File')","3bbfccfa":"df_eng=pd.read_csv(r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/6345.csv')\ndf_eng.head(4)\ndf_eng.info()","ae30da49":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fa63de00":"df=pd.read_csv(r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data\/5882.csv')\ndf.head()","010c2f75":"import pandas as pd\nimport glob\n\npath = r'\/kaggle\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data' # use your path\nall_files = glob.glob(path + \"\/*.csv\")\n\nli = []\n\nfor filename in all_files:\n    df = pd.read_csv(filename, index_col=None, header=0)\n    li.append(df)\n\nframe = pd.concat(li, axis=0, ignore_index=True)","cf8acea7":"frame.describe()","509e2734":"frame.info()","8d7efcfb":"#load and see sample district data info\ndf_dist=pd.read_csv(r'..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\ndf_dist.head()","16c61cd0":"df_dist.info()","72877e7b":"def with_null_column(df):\n    '''\n    Return List of Columns which contain more than 30% of null\/missing values\n    '''\n    df_size = df.shape[0]\n    \n    columns_list = df.columns\n    columns_null = []\n    \n    for column in columns_list:\n        null_per_column = df[column].isnull().sum()\n        percentage = round( (null_per_column \/ df_size) * 100 , 2)\n        \n        if(percentage > 30):\n            columns_null.append(column)\n    \n    return columns_null","2b1631b7":"with_null_column(df_dist)","8a6fbfa0":"# Product dataset \n\n| Name | Description |\n| :--- | :----------- |\n| LP ID| The unique identifier of the product |\n| URL | Web Link to the specific product |\n| Product Name | Name of the specific product |\n| Provider\/Company Name | Name of the product provider |\n| Sector(s) | Sector of education where the product is used |\n| Primary Essential Function | The basic function of the product. There are two layers of labels here. Products are first labeled as one of these three categories: LC = Learning & Curriculum, CM = Classroom Management, and SDO = School & District Operations. Each of these categories have multiple sub-categories with which the products were labeled |\n","bcda27b2":"From the table above we can see column pp_total_raw,pct_free\/reduced, county_connections_ratio has missing value greater than 30%\n. We can drop this collumns but since data sets contain small features so I want to fill using _fillna() functon.\n","df96791e":"# visualizing district data","9368f0cf":"# Engagment dataset information\n\nThe engagement data are aggregated at school district level, and each file in the folder `engagement_data` represents data from one school district. The 4-digit file name represents `district_id` which can be used to link to district information in `district_info.csv`. The `lp_id` can be used to link to product information in `product_info.csv`.\n\n| Name | Description |\n| :--- | :----------- |\n| time | date in \"YYYY-MM-DD\" |\n| lp_id | The unique identifier of the product |\n| pct_access | Percentage of students in the district have at least one page-load event of a given product and on a given day |\n| engagement_index | Total page-load events per one thousand students of a given product and on a given day |\n","742d920e":"Observe that  the three datasets have relation. That is in district data set column district_id entries are \nthe name of engagment datasets and column lp id is common column for both product and engagment datasets. So I deciede to merge\/concatinate three of them\nin one and will process it.","e94f5fe4":"## Table of Content\n# Preprocss \n    `- District dataset\n    \n     - Product dataset\n     \n     - Engagment dataset\n # Visualize\n \n # Train model\n # Conclusion ","f8dd50e9":"# EDA","b3668059":"# District dataset \n\nDistrict information data\n\n| Name | Description |\n| :--- | :----------- |\n| district_id | The unique identifier of the school district |\n| state | The state where the district resides in |\n| locale | NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural. See [Locale Boundaries User's Manual](https:\/\/eric.ed.gov\/?id=ED577162) for more information. |\n| pct_black\/hispanic | Percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data |\n| pct_free\/reduced | Percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data |\n| county_connections_ratio | `ratio` (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version). See [FCC data](https:\/\/www.fcc.gov\/form-477-county-data-internet-access-services) for more information. |\n| pp_total_raw | Per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project. The expenditure data are school-by-school, and we use the median value to represent the expenditure of a given school district. |\n\n","40e8835c":"# Districts","915c663c":"#  **Introduction**\n\nCurrent research shows educational outcomes are far from equitable. The imbalance was exacerbated by the COVID-19 pandemic. There's an urgent need to better understand and measure the scope and impact of the pandemic on these inequities.\n\nEducation technology company LearnPlatform was founded in 2014 with a mission to expand equitable access to education technology for all students and teachers. LearnPlatform\u2019s comprehensive edtech effectiveness system is used by districts and states to continuously improve the safety, equity, and effectiveness of their educational technology. LearnPlatform does so by generating an evidence basis for what\u2019s working and enacting it to benefit students, teachers, and budgets.\n\nThe data and feature description for this challenge can be found Here in kaggle computation.\n\n**Business Need\n\n- What is the state of digital learning in 2020? \n\n- And how does the engagement of digital learning relate to factors such as district demographics, broadband access, and state\/national level policies and events?\n\n","23140cb9":"Observe that  the interval \"[a, b[\" means that a \u2264 x < b for a,b are real numbers.\nFor the columns pp_total_raw,pct_free\/reduced, county_connections_ratio having interval inputs I understand data as: for instance [0.2,0.4[ in pct_free\/reduced means 20-40 % students in the districts are eligible for free or reduced-price lunch.So for this kind of datasets I am planing to use mean\/avarage of two points a and b and change to single value."}}