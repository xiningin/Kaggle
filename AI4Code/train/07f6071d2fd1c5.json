{"cell_type":{"bd226688":"code","73975b97":"code","cb310184":"code","ac76e170":"code","36ff0053":"code","0de4863d":"code","f94739be":"code","86c9df48":"code","528e2c9d":"code","7fc2d938":"code","849ef4ea":"code","50d12269":"code","18a105da":"code","dc5def37":"code","53f1ecdf":"code","18ca1deb":"code","4c679ca5":"code","0f71c990":"code","441c90df":"code","7d49ca32":"code","0f8490b7":"code","6a3b5855":"code","4ce89844":"code","2cc6614b":"code","3f0cc1c8":"code","4bad496c":"code","5cfd315f":"code","e48a4e1e":"code","bb66e49e":"code","192eb958":"markdown","792f24f3":"markdown","89e36c04":"markdown","19239514":"markdown","0e30fbf4":"markdown","e94d326f":"markdown","217ab820":"markdown","670eebf7":"markdown","7e3a394f":"markdown","57886cac":"markdown","3aa3dad6":"markdown","fc9cb5f0":"markdown","afc178bf":"markdown","a2228328":"markdown","82090ed1":"markdown","8c14980f":"markdown","89dded72":"markdown","145b4799":"markdown","3065bbea":"markdown","e3d8bda3":"markdown","1e0e9110":"markdown","3ff4b7aa":"markdown","d320dec1":"markdown","ae2b99b3":"markdown","9f59230c":"markdown"},"source":{"bd226688":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import models\nimport pdb","73975b97":"models.resnet34()","cb310184":"inp = torch.randn([2,3,224,224])","ac76e170":"inp.shape # bs,rgb channels , width,height","36ff0053":"conv_block = nn.Sequential(nn.Conv2d(3,64,kernel_size=7, stride=2, padding=3, bias=False), #112,112\n                       nn.BatchNorm2d(64),\n                       nn.ReLU(inplace=True),\n                       nn.MaxPool2d(kernel_size=3, stride=2, padding=1)) # 56,56\nconv_block","0de4863d":"#inp=nn.Conv2d(3,64,kernel_size=7, stride=2, padding=3, bias=False)(inp)\n#inp.shape","f94739be":"out=conv_block(inp)\nout.shape","86c9df48":"list(models.resnet34().children())[:4]","528e2c9d":"class BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super().__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=3, stride=stride,\n                     padding=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace=True)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1,\n                     padding=1, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n\n    def forward(self, x):\n        identity = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n\n        if self.downsample is not None:\n            identity = self.downsample(x)\n\n        out += identity\n        out = self.relu(out)\n\n        return out","7fc2d938":"BasicBlock(64,128)","849ef4ea":"t = torch.randn((2,64,56,56))\nt.shape","50d12269":"BasicBlock(64,64)(t).shape","18a105da":"def _make_layer(block, inplanes,planes, blocks, stride=1):\n    downsample = None  \n    if stride != 1 or inplanes != planes:\n        downsample = nn.Sequential(            \n            nn.Conv2d(inplanes, planes, 1, stride, bias=False),\n            nn.BatchNorm2d(planes),\n        )\n    layers = []\n    layers.append(block(inplanes, planes, stride, downsample))\n    inplanes = planes\n    for _ in range(1, blocks):\n        layers.append(block(inplanes, planes))\n    return nn.Sequential(*layers)","dc5def37":"layers=[3, 4, 6, 3]","53f1ecdf":"layer1 =_make_layer(BasicBlock, inplanes=64,planes=64, blocks=layers[0])\nlayer1","18ca1deb":"list(models.resnet34().children())[4]","4c679ca5":"layer2 = _make_layer(BasicBlock, 64, 128, layers[1], stride=2)\nlayer2","0f71c990":"list(models.resnet34().children())[5]","441c90df":"t = torch.rand((2,64,56,56))\nt.shape #batch, RGB channels\/filters,width,height","7d49ca32":"o = nn.Conv2d(64,128,3,2,1)(t)\no.shape","0f8490b7":"#o+t","6a3b5855":"t_d =nn.Conv2d(64,128,1,2,0)(t)\no.shape,t_d.shape","4ce89844":"(o+t_d).shape","2cc6614b":"num_classes=1000\nnn.Sequential(nn.AdaptiveAvgPool2d((1, 1)),\n              nn.Linear(512 , num_classes))","3f0cc1c8":"list(models.resnet34().children())[8:]","4bad496c":"class ResNet(nn.Module):\n\n    def __init__(self, block, layers, num_classes=1000):\n        super().__init__()\n        \n        self.inplanes = 64\n\n        self.conv1 = nn.Conv2d(3, self.inplanes, kernel_size=7, stride=2, padding=3,\n                               bias=False)\n        self.bn1 = nn.BatchNorm2d(self.inplanes)\n        self.relu = nn.ReLU(inplace=True)\n        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n        \n        self.layer1 = self._make_layer(block, 64, layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)\n        \n        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n        self.fc = nn.Linear(512 , num_classes)\n\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None  \n   \n        if stride != 1 or self.inplanes != planes:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes, 1, stride, bias=False),\n                nn.BatchNorm2d(planes),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        \n        self.inplanes = planes\n        \n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    \n    \n    def forward(self, x):\n        x = self.conv1(x)           # 224x224\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)         # 112x112\n\n        x = self.layer1(x)          # 56x56\n        x = self.layer2(x)          # 28x28\n        x = self.layer3(x)          # 14x14\n        x = self.layer4(x)          # 7x7\n\n        x = self.avgpool(x)         # 1x1\n        x = torch.flatten(x, 1)     # remove 1 X 1 grid and make vector of tensor shape \n        x = self.fc(x)\n\n        return x","5cfd315f":"def resnet34():\n    layers=[3, 4, 6, 3]\n    model = ResNet(BasicBlock, layers)\n    return model","e48a4e1e":"model=resnet34()","bb66e49e":"model","192eb958":"\n\nNext 4 layers after initial conv_block are called as layer 1, 2, 3, 4 respectively.\nEach layer consists of multiple convolution blocks.\nConv_block refers to set of operations as in Convolution->BatchNorm->ReLU activation to an input.\nInstead of adding new layers to create a deeper neural network, resnet authors added many conv_block within each layer, thus keeping depth of neural network same - 4 layers.\n\nIn the PyTorch implementation they distinguish between the blocks that includes 2 operations \u2013 Basic Block \u2013 and the blocks that include 3 operations \u2013 Bottleneck Block.\n\nThe make_layer() function takes which type of block to use as an argument, the number of input and output filters,  and number of blocks to be stacked together.\nEvery layer downsamples the input at the start using stride equals to 2 i.e for 1st convolutional layer in 1st block of a layer.\nFor all the rest convolution layers stride is 1.\nAlso, if downsample has to be applied to input stride 2 convolution is used followed by BatchNorm.","792f24f3":" <a id='conv' aria-hidden='true'><\/a>\n\n### Convolution block\n\nFirst we are going to construct a small sequential network for initial convolution block of resnet.\nIt's the very first layer in our resnet architecture.\n\nIt consists of 4 operations,\n\n1. Convolution\n2. Batch Normalization\n3. ReLU activation function\n4. Maxpooling","89e36c04":"<a id='conclusion' aria-hidden='true'><\/a>\n## Conclusion\n\nThe most important concept in resnet in the residual block. Residual blocks enable building neural networks with 1000's of layers deep. Skip connections without adding much of overload on the network preserves information from initial layers till the last. \n\nWe have learned how to build resnet34 architecture from scratch. We can extend it to deeper models like resnet50, 101, and 152 using BottleNeck Block as in PyTorch.  ","19239514":"<a id='implement' aria-hidden='true'><\/a>\n## Building Resnet","0e30fbf4":"<a id='classifier' aria-hidden='true'><\/a>\n### Classifier block\n\n\nThis is a fully connected layer. It's the final layer in our resnet architecture.\nIt consists of 3 operations -\n\n1. Average pooling layer - aggregates all features. It's output grid size is 1X 1 which forms rank 3 tensor of (512, 1,1)\n\n2. Flatten- Our loss function expects a vector of tensor instead of rank 3 tensor. \n\tSo in forward() we call\n\t 'torch.flatten' to remove any unit axis from matrix and make it just a vector of length 512.\n\t```python \n\tx = torch.flatten(x, 1)\n\t```\n\n3. Linear Layer - It's a fully connected layer just before applying softmax, which takes in 512 features and outputs 1000 class probabilities.\n(In case of Imagenet, we have 1000 categories.)","e94d326f":"Conv2d layer takes 3 input channels and generate 64 filters\/channels i.e feature maps. We can choose how many features we want to generate by convolution operation. \n\nHere, kernel size is 7 X 7 , stride is 2 and padding 3. Padding adds border of zeros around your input matrix.\n\nConv2d layer downsamples input when stride is equal to 2, i.e convolution window skips over 1 pixel. \n\nSo after conv2d layer output shape will be (2,64,112,112) tensor of activations.\nBasically height and width of input grid reduces to half.","217ab820":"###  why we need to downsample input","670eebf7":"### Input\n\nGrab random input to understand how feature maps are generated in convolution layer.\n\nPytorch nn.Conv2d layer expects inputs of (batch_size, channels, height, width).\n\nWe are considering batch size of 2, 3 RGB color channels, height and width of 224 each. It is a rank 4 tensor.","7e3a394f":"Ah! it works now!","57886cac":"Uncomment the above code to see what RuntimeError you get.\nPytorch says -\nRuntimeError: The size of tensor a (28) must match the size of tensor b (56) at non-singleton dimension 3\nHeight and width of both the tensors are not matching  56,56 !=28,28","3aa3dad6":"Let's confirm our classifier block matches Pytorch's resnet implementation","fc9cb5f0":"We are going to build this from scratch in next sections.","afc178bf":"To apply convolution we need to have width and height dimentions of 2 tensors same. That is why downsampling is done here.","a2228328":"## Resnet architecture from torchvision \nFirst things first , let's understand what are we trying to build here. \nPull out resnet34 architecture from Pytorch models.","82090ed1":"Please consider upvoting the kernel, if you found something new to learn from it. Thank you for staying with me this long :)","8c14980f":"<a id='residual' aria-hidden='true'><\/a>\n###  Residual block\n\n![res_block](https:\/\/raw.githubusercontent.com\/jarvislabsai\/blog\/master\/build_resnet34_pytorch\/images\/res_block1.png)\nAfter 2 convolution operations input of those 2 convolution is added to their output.","89dded72":"In this kernel, I am going to show you different components of resnet architecture and how to implement each in pytorch.\nRead the accompanying blogpost for detailed explanation [here](https:\/\/www.jarvislabs.ai\/blogs\/resnet).\nHang in there!\n\n\n- [Understanding and Building Resnet from scratch](#building-resnet-from-scratch)\n  * [Why it is important to understand resnets?](#understand)\n  * [Residual Block](#residual)\n  * [Implementing `resnet34` using PyTorch](#implement)\n    + [Convolution block](#conv)\n    + [Residual block](#residual)\n    + [Resnet Layers](#reslayers)\n    + [Classifier block](#classifier)\n    + [ResNet class](#resnet-class)\n    + [Creating Resnet34 model](#create-res34)\n  * [Conclusion](#conclusion)\n  <a id='understand'><\/a>","145b4799":"<a id='reslayers' aria-hidden='true'><\/a>\n\n### make_layer()","3065bbea":"## Why it is important to understand ResNets?\n\nResNets are the backbone behind most of the modern computer vision architectures.  For a lot of common problems in computer vision, the go-to architecture is `resnet34`. Most of the modern CNN architectures like ResNext, DenseNet are different variants to original `resnet` architecture.\n\nUnderstanding the functioning of the `resnet` model helps us while building custom architectures for problems like image classification, segmentation, and object detection. For example, when using the `resnet` model as the backbone for image segmentation using U-net architecture, we create skip connections between different blocks of encoder and decoder. So it becomes a lot easier to understand and build these architectures later on since we already know how resnet is built. Understanding these architecture helps in guessing the output shapes of each `resnet` block which is in turn added to different decoder blocks.\n\nResNets were introduced in the [Deep Residual Learning for Image Recognition](https:\/\/arxiv.org\/abs\/1512.03385) paper by Kaiming He et al.\n","e3d8bda3":"### Basic Block\nEach basic block constitutes of 2 convolution operations.\n\nEach convolutional layer is followed by a batch normalization layer and a ReLU activation function. \n\nexcept if downsample has to be applied the 2nd conv layer's output is added to input before applying relu.","1e0e9110":"<a id='resnet-class' aria-hidden='true'><\/a>\n\n## Resnet class","3ff4b7aa":"In the comments, I have mentioned how output size changes after every layer.","d320dec1":"<a id='create-res34' aria-hidden='true'><\/a>\n###  Creating Resnet34 model","ae2b99b3":"###  imports\nLet's grab some imports","9f59230c":"After maxpooling operation with stride of 2 again output from conv2d->batchNorm->Relu gets downsampled to half.\n\nLet's check what is shape of output after conv_block operations."}}