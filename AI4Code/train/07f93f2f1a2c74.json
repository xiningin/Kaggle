{"cell_type":{"6bb96345":"code","17cdac40":"code","9647f071":"code","15fb6f21":"code","94d22c59":"code","30c7992d":"code","2dc56b2a":"code","961c3675":"code","aa322d75":"code","7c68406c":"code","6b910c7a":"code","fb657eba":"code","14b4dfa8":"code","73ae795a":"code","0ca4861e":"code","7036d771":"code","3c860efa":"code","fee4fc11":"code","f8fd0002":"code","86a55fb8":"code","cc6d2399":"code","b1913f1e":"code","d4a8537b":"code","e7e7c7b2":"code","6e1b2109":"code","66a25782":"code","796cefef":"code","9ff3a4b0":"code","d4795792":"code","4912e956":"code","4961114e":"code","d2ffe563":"code","cec17119":"code","aa252cc5":"code","f7708832":"code","ecd2a645":"code","0e0d87fb":"code","d7f76f91":"code","0120f127":"code","1855a5be":"code","1b74decc":"code","47b420e1":"code","66ef2207":"code","aa18e4e7":"code","5350a464":"code","3c0d46ca":"code","47b08210":"code","5c085e9c":"code","1c47cdb8":"code","c75ea0b0":"code","67b4d440":"code","a78fe605":"code","5b5a6528":"code","aa81d85c":"code","88e0c16f":"code","e11facd2":"code","b4309f83":"code","cdf6c19c":"code","5850440a":"code","a2fb9f6a":"code","27f57ac1":"code","030b4d48":"code","a0cc0214":"code","dd629737":"code","3287d97e":"markdown","32a184fc":"markdown","3a6173e6":"markdown","ce33458c":"markdown","8beffc76":"markdown","9d7f0f4f":"markdown","3e53b92d":"markdown","d3ab805f":"markdown","0ddc61c8":"markdown","4da77cc1":"markdown","6bc4fd69":"markdown","b9bdc8e4":"markdown","372919ad":"markdown","4d087b84":"markdown","7885cb8a":"markdown","9a04495b":"markdown","d273f869":"markdown","a3e2cf23":"markdown","7fa337c7":"markdown","b5727b1f":"markdown"},"source":{"6bb96345":"import numpy as np\nimport pandas as pd \nfrom sklearn.model_selection import KFold\n\nimport numpy as np\nimport pandas as pd \n\nimport os\nimport gc\nimport psutil\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, Normalizer,MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier, XGBRegressor\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import PolynomialFeatures\n\nfrom optuna.integration import LightGBMPruningCallback\n\n# get skewed features to impute median instead of mean\nfrom scipy.stats import skew\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import linear_model\nfrom sklearn.linear_model import Ridge,Lasso\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom xgboost import XGBRegressor, XGBRFRegressor\n\nimport itertools\nimport optuna\nfrom lightgbm import LGBMClassifier,LGBMRegressor\nimport lightgbm as lgb\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostRegressor, CatBoostClassifier\n\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","17cdac40":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","9647f071":"train_data.head(2)","15fb6f21":"test_data.head(2)","94d22c59":"train_data.shape,test_data.shape","30c7992d":"train_data.info()","2dc56b2a":"test_data.info()","961c3675":"train_data.isna().sum()","aa322d75":"test_data.isna().sum()","7c68406c":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)","6b910c7a":"train_data.isna().sum()","fb657eba":"test_data.isna().sum()","14b4dfa8":"train_data['Age'].median(),test_data['Fare'].median(),train_data['Embarked'].mode()[0]","73ae795a":"# 'Age' and 'Embarked' columns of train_data has misssing values\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\n# 'Age' and 'Fare' columns of test_data has missing value\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())","0ca4861e":"train_data.isna().sum()","7036d771":"test_data.isna().sum()","3c860efa":"train_data.drop('Ticket',axis= 1,inplace= True)\ntest_data.drop('Ticket',axis= 1,inplace= True)","fee4fc11":"train_data['Family_size']=train_data.SibSp + train_data.Parch + 1\ntest_data['Family_size']=test_data.SibSp + test_data.Parch + 1","f8fd0002":"train_data['IsAlone'] = 1\ntrain_data[['IsAlone']][train_data.Family_size >1] = 0\n\ntest_data['IsAlone'] = 1\ntest_data[['IsAlone']][test_data.Family_size >1] = 0","86a55fb8":"obj_train=train_data.select_dtypes(['object'])\nobj_test=test_data.select_dtypes(['object'])\nobj_train.head(3)","cc6d2399":"list(zip(obj_train.columns, obj_train.dtypes, obj_train.nunique()))","b1913f1e":"list(zip(obj_test.columns, obj_test.dtypes, obj_test.nunique()))","d4a8537b":"train_data['isTrain'] = 1\ntest_data['isTrain'] = 0","e7e7c7b2":"tt = pd.concat([train_data,test_data])\ntt.head(2)","6e1b2109":"train_data.shape, test_data.shape, tt.shape","66a25782":"obj_tt=  tt.select_dtypes(['object'])\nlist(zip(obj_tt.columns, obj_tt.dtypes, obj_tt.nunique())) ","796cefef":"tt['Title']= tt.Name.apply(lambda x: x.split(',')[1].split('.')[0])","9ff3a4b0":"stat_min = 10\ntitle_names = (tt['Title'].value_counts() >= stat_min)\nt=title_names.reset_index()# most common titles\nmost_freq_titles = list(t[t.Title == True]['index'])\nmost_freq_titles","d4795792":"tt['Title']= tt.Title.apply(lambda x: x if x in most_freq_titles else 'other')","4912e956":"tt= pd.get_dummies(data= tt,columns=['Title'],drop_first= True)","4961114e":"factors = np.random.randn(30)\nfactors","d2ffe563":"sns.histplot(factors,bins=30)","cec17119":"cut1 = pd.cut(factors, 5)\ncut1","aa252cc5":"min(factors),max(factors)","f7708832":"cut1.value_counts() # Here each bin is of equal length and no of elements in each bin are not same.","ecd2a645":"cut2 = pd.qcut(factors, 5)\ncut2","0e0d87fb":"cut2.value_counts() # Here no of elements in each bin are same and bins are of unequal length.","d7f76f91":"tt.head(2)","0120f127":"ax=sns.histplot(tt.Fare)\nax.set_yscale('log')","1855a5be":"tt['Fare_bins']=pd.qcut(tt.Fare, 4)","1b74decc":"ax=sns.histplot(tt.Age)","47b420e1":"tt['Age_bins']= pd.cut(tt.Age.astype(int), 5)","66ef2207":"tt.head(2)","aa18e4e7":"from sklearn.preprocessing import OneHotEncoder, LabelEncoder\nlabel = LabelEncoder()\nfor rows in ['Age_bins','Fare_bins']:\n    tt[rows]=label.fit_transform(tt[rows])","5350a464":"tt.drop(['Name','Fare','Age'],axis=1,inplace=True) # we have created 'Title' , 'Fare_bins', 'Age_bins' ","3c0d46ca":"obj_tt=  tt.select_dtypes(['object'])\nlist(zip(obj_tt.columns, obj_tt.dtypes, obj_tt.nunique())) ","47b08210":"tt.shape","5c085e9c":"tt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)","1c47cdb8":"tt.shape","c75ea0b0":"train_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]","67b4d440":"train_data.head(2)","a78fe605":"test_data.head(2)","5b5a6528":"train_data.info()","aa81d85c":"test_data.info()","88e0c16f":"test_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)","e11facd2":"test_data.info()","b4309f83":"train_data.info()","cdf6c19c":"useful_features = test_data.drop('PassengerId',axis=1).columns.tolist()\nuseful_features  # these are the features on which our model will be trained","5850440a":"train_data[useful_features].shape, train_data.shape, test_data.shape, test_data[useful_features].shape","a2fb9f6a":"test = test_data[useful_features]","27f57ac1":"my_folds = train_data.copy()","030b4d48":"train_data = pd.read_csv('..\/input\/titanic-create-folds\/TITANIC_Folds.csv')\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")","a0cc0214":"train_data.drop('Cabin',axis= 1,inplace= True)\ntest_data.drop('Cabin',axis= 1,inplace= True)\ntrain_data['Age'] = train_data['Age'].fillna(int(train_data['Age'].mean()))\ntest_data['Age'] = test_data['Age'].fillna(int(test_data['Age'].mean()))\ntrain_data['Embarked'] = train_data['Embarked'].fillna(train_data['Embarked'].mode()[0])\ntest_data['Fare'] = test_data['Fare'].fillna(test_data['Fare'].median())\n\ntrain_data.drop('Ticket',axis= 1,inplace= True)\ntest_data.drop('Ticket',axis= 1,inplace= True)\ntrain_data['Family_size']=train_data.SibSp + train_data.Parch + 1\ntest_data['Family_size']=test_data.SibSp + test_data.Parch + 1\ntrain_data['IsAlone'] = 1\ntrain_data[['IsAlone']][train_data.Family_size >1] = 0\ntest_data['IsAlone'] = 1\ntest_data[['IsAlone']][test_data.Family_size >1] = 0\n#-----------------------------------------------\ntrain_data['isTrain'] = 1\ntest_data['isTrain'] = 0\ntt = pd.concat([train_data,test_data])\ntt['Title']= tt.Name.apply(lambda x: x.split(',')[1].split('.')[0])\nstat_min = 10\ntitle_names = (tt['Title'].value_counts() >= stat_min)\nt=title_names.reset_index()# most common titles\nmost_freq_titles = list(t[t.Title == True]['index'])\ntt['Title']= tt.Title.apply(lambda x: x if x in most_freq_titles else 'other')\ntt= pd.get_dummies(data= tt,columns=['Title'],drop_first= True)\ntt['Fare_bins']=pd.qcut(tt.Fare, 4)\ntt['Age_bins']= pd.cut(tt.Age.astype(int), 5)\n\nlabel = LabelEncoder()\nfor rows in ['Age_bins','Fare_bins']:\n    tt[rows]=label.fit_transform(tt[rows])\n\ntt.drop(['Name','Fare','Age'],axis=1,inplace=True) # we have created 'Title' , 'Fare_bins', 'Age_bins' \ntt = pd.get_dummies(tt,columns= ['Sex','Embarked'],drop_first = True)\ntrain_data = tt[tt.isTrain == 1]\ntest_data = tt[tt.isTrain == 0]\ntest_data.drop(['Survived','isTrain','fold'],axis=1,inplace = True)\ntrain_data.drop(['isTrain'],axis=1,inplace= True)\n\n# Note don't drop PassengerId column of train_data and test_data as it is used later. Instead drop it only for test and my_folds\nuseful_features = test_data.drop('PassengerId',axis=1).columns.tolist() #########################################\ntest = test_data[useful_features]\nmy_folds = train_data.copy()","dd629737":"test.shape, my_folds.shape, useful_features","3287d97e":"<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">BINNING<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Binning helps convert continuous values to discrete values by using bins\/intervals.\n<br>Below I will explain how binning works. If you already know then you can skip this part and move to the implementation part.\n<br>There are 2 ways of binning.<br>\n    <b> 1. <code>pd.cut(data, n)<\/code> <br>\n        2. <code>pd.qcut(data, n)<\/code> <\/b><br>\n<code>pd.cut(data, n)<\/code> : it finds minimum and maximum value of our data that is our range and then it cuts this range into n equal parts. Here each bin is of equal length and no of elements in each bin may vary depending on distribution of data. If data is normally distributed that is bell shaped, then we will see more elements in inner bins and less in outer bin.<br>\n<code>pd.qcut(data, n)<\/code> : it cuts data into equal parts such that each bin has same no of elements. So here each bin need not be of equal length. so like if we have 10 data points then bins are created such that each group has 2 data points.<\/p> ","32a184fc":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We have PREPROCESSED our data now we will remove unnecessary columns.<\/p> ","3a6173e6":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Let's first see if there is any missing value<\/p> ","ce33458c":"<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">Implement binning<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will do binning of Fare and Age columns.<\/p>","8beffc76":"\n<a id=\"0\"><\/a>\n# <p style=\"background-color:#FFCC70;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:5px 5px;\">LEVEL1 ROUND2 PREPROCESSING<br><p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">INTRODUCTION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This is a part of the notebook series <i>\"My_Complete_Pipeline_for_any_ML_Competition\"<\/i> where we are building complete pipeline.<br><br> \n\ud83d\udcccLink of first notebook of the series <a href=\"https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition\">https:\/\/www.kaggle.com\/raj401\/my-complete-pipeline-for-any-ml-competition<\/a><br>\n\ud83d\udcccLink of notebook where we have created folds <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-create-folds\">https:\/\/www.kaggle.com\/raj401\/titanic-create-folds<\/a><br>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\n    If you like my effort please do <b><span style=\"color:crimson; font-size:20px\">UPVOTE\ud83d\udc4d<\/span><\/b>, it really keeps me motivated. <\/p>\n\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">In the earlier notebook we have modified our training set by adding new column named 'fold' and then saved it as <i>TITANIC_folds.csv<\/i>. In this notebook we will use this modified training set instead of original training set and do PREPROCESSING for LEVEL1 ROUND2. I am providing the link of <i>TITANIC_folds.csv<\/i> you can just add it to your notebook and you are good to go.<b>\n<br>[Make sure you have added it before moving further. If you have TITANIC_Create_Folds notebook you can also add that notebook instead.]<br><\/b>\n\ud83d\udcccLink of Dataset containing <i>TITANIC_folds.csv<\/i> <a href=\"https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets\">https:\/\/www.kaggle.com\/raj401\/titanic-all-datasets<\/a><br><\/p> \n\n\n<a id='top'><\/a>\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">TABLE OF CONTENTS<\/p>   \n    \n* [1. IMPORTING LIBRARIES](#1)\n    \n* [2. READ DATASETS](#2)\n\n* [3. PREPROCESSING](#3)\n   * [3.1 WITH EXPLANATION](#3)\n   * [3.2 QUICK](#3.2)\n    \n    \n* [4. CONCLUSION](#4)\n    \n* [5. END](#5)\n\n<a id=\"1\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">IMPORTING LIBRARIES<\/p>","9d7f0f4f":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We now split it back into train and test set.<\/p> ","3e53b92d":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Since no of missing value in 'Cabin' column is quite large compared to total no of rows so it is better to drop 'Cabin', Later we may try to do some imputation based on other columns but for now we will simply drop it.<\/p> ","d3ab805f":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will now use pd.get_dummies() method of pandas to create onehot encoding of all the desired categorical columns here 'Sex' and 'Embarked'.\n<br>Notice: here we did drop_first= True to remove redundant columns.\nFor examample 'Sex' column has two categories 'Male' and 'Female' so two new columns are created, but do we really need two column to tell if it is 'Male' or 'Female'. Actually no one column is enough because if 'Male' we will label it as 1 and if 'Female' we will label it as 0. Second column is just the repetition of same information. Hence we drop first. So for 5 categories column is replaced by 4 new columns instead of 5 new column when we do drop_first = True<\/p> ","0ddc61c8":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will drop ticket no as well because ticket no has no impact on outcome variable.<\/p>","4da77cc1":"<a id=\"3\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">PREPROCESSING<\/p>\n\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Here we will go one step at a time expalining everything. From next time we will use single cell to do all the PREPROCESSING<\/p> ","6bc4fd69":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will now do <b>FEATURE ENGINEERING<\/b>.<br>\n[Some of the ideas are inspired from: https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy]<\/p>","b9bdc8e4":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will now aim at converting object type columns to integer or float type because ML model can only interpret numerical values.\n<br> object type generally contains string so we can either convert them into classes by <i>one hot encoding<\/i> or we can do <i>ordinal encoding<\/i>\n<br>For example if we have a column of T-Shirt sizes whoose values are 'S','M','L','XL','XXL', here there is a sense of order like 'S' is smaller than 'M' so here we can do <i>ordinal encoding<\/i>\n<br>While if the column is 'gender' which contains 'Male' and 'Female' then there is no sence of order, so here we can do one hot encoding.<\/p> \n","372919ad":"<a id=\"2\"><\/a>\n# <p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">READ DATASETS<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">\nEither you can add the dataset whoose link I have given above or if you have TITANIC_Create_Folds notebook you can add it's output from Add data option. Both contains TITANIC_Folds.csv (modified train set).\nNow read it as train_data.\n<\/p>","4d087b84":"<a id=\"3.2\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:50%;text-align:center;border-radius:20px 60px;\">\"\"<\/p> \n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Note we did data cleaning and feature engineering step by step. From next time we  will do all the above steps in a single cell.\n<br>Like this:-<\/p> ","7885cb8a":"<a id=\"4\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">CONCLUSION<\/p>\n<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">This was the PREPROCESSING for Level1 Round2. Means we are going to use this PREPROCESSING throughout the Round2 of Level1. So we will now find optimal hyperparameters of all models, train our models and make predictions, all on this PREPROCESSING. \n<br> That is all for now, If you have any doubt feel free to ask me in the comment. If you like my effort please do <b>UPVOTE<\/b>, it really keeps me motivated. <\/p>\n\n**<span style=\"color:#444160;\"> Thanks!<\/span>**\n<a id=\"5\"><\/a>\n<p style=\"background-color:#B721FF;font-family:newtimeroman;color:#444160;font-size:150%;text-align:center;border-radius:20px 60px;\">END<\/p>\n    <a href=\"#top\" role=\"button\" aria-pressed=\"true\" >\u2b06\ufe0fBack to Table of Contents \u2b06\ufe0f<\/a>","9a04495b":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Now for 'Age' and 'Fare' column we fill with median of all the age and 'Fare' values respectively, for 'Embarked' we fill with mode of all the 'Embarked' value since it is a string so we take mode.<br> <b>[Note:- to avoid data leakage we do all filling of missing values before concating train and test set for further preprocessing like one hot encoding.]<\/b><\/p> ","d273f869":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">Let us first use .info() method as it gives us a lot of information about our dataset.<\/p> ","a3e2cf23":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will first concat our train and test set before dealing with categorical featues. The reason is Let say in training set 'col1' has only two types of value 'A' and 'B' while test set has 3 types of categories 'A' , 'C', 'D' in 'col1'. If we separatley create one hot encoding they will give wrong results because they are incompatible. So when we concat them first and then do one hot encoding then both assumes there are 4 types of categories 'A', 'B', 'C', 'D'.\n<br><b>Note:- We can think it as a general rule, \n<br>Fill missing value for train and test set separately. While if it has categorial column do one hot encoding by first concatenating train and test set.<\/b>\n Now after doing one hot encoding we need to split it back in train-test so for this we create a new column 'isTrain' before concatenating and label training rows as 1 and test rows as 0<\/p> ","7fa337c7":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will no to label encoding of Fare_bins adn Age_bins columns to convert them to numerial columns.<br>\n    <b>Lable Encoding: <\/b>It simply transforms categorical data from whatever domain it is, so that it's domain is [0,1,2 ,..., k-1] where k is no of classes in that categorical data. <\/p>","b5727b1f":"<p style=\"font-family:newtimeroman;font-size:120%;color:#444160;\">We will focus on columns with Object data type. There are total 3 columns :- 'Name', 'Sex',  'Embarked'. 'Name' column have many different types of string [total 1307]  which makes sense as People have different names. If we just do one hot encoding of it then our dataset will become very large and very sprse. There are other ways of dealing with it and probably we can extract features from there names like sir-name, last name.<\/p> "}}