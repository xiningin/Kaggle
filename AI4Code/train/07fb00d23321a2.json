{"cell_type":{"3b9384fe":"code","0790cbff":"code","59a1ded5":"code","fcee3c61":"code","5efe8c48":"code","17e377b1":"code","4e8d3406":"code","37359e57":"code","6bb72e92":"code","b7feb51c":"code","e998863c":"code","3252badf":"code","a427bbcb":"code","f4a13b0c":"code","07147cb4":"code","008ccd0c":"code","257ce18a":"code","6a251e72":"code","ba3ce0d3":"code","37edd0d6":"code","822cc2e2":"code","422061a0":"code","bfe3c462":"code","b6ae4a28":"code","f619cb75":"code","527bc25a":"code","bcdaeec1":"code","501ef938":"code","37478925":"code","5699e8b7":"code","abc672c8":"code","60b8abcc":"code","1e0b517d":"code","de12e637":"code","4a76ba10":"code","6c94d711":"markdown","988f0fd4":"markdown","e559a9e4":"markdown","2922a2ac":"markdown","59539dec":"markdown","11854ac5":"markdown","aee9649f":"markdown","e4e341ac":"markdown","891b5e2e":"markdown","b7b8c5c0":"markdown","bcd2d68e":"markdown","cf8c54cd":"markdown","6169afee":"markdown","008832ec":"markdown","865865fe":"markdown","63906fb8":"markdown","c062d255":"markdown","c4864186":"markdown","36ba4c86":"markdown","d5270765":"markdown","1a74ecd5":"markdown","5afa76d1":"markdown","4bccd9c9":"markdown","e6fe8ab9":"markdown","0523e2e7":"markdown","f7dce129":"markdown","c4a2b4d3":"markdown","83cff1da":"markdown","6b9932a0":"markdown","e2b8cdbe":"markdown"},"source":{"3b9384fe":"import numpy as np                  # Mathetimatical Operations\nimport pandas as pd                 # Data manipulation\n\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt     \n%matplotlib inline\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, StratifiedKFold, GridSearchCV\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, auc, roc_curve, roc_auc_score, classification_report, mean_squared_error, confusion_matrix, f1_score, precision_recall_curve, r2_score \nfrom sklearn.ensemble import AdaBoostClassifier, BaggingClassifier, GradientBoostingClassifier, RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostRegressor, BaggingRegressor, GradientBoostingRegressor, RandomForestRegressor\n\n# Scipy\nfrom scipy.stats import stats\nfrom scipy.stats import ttest_ind, ttest_ind_from_stats\n\n# XGBoost\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\n\n# LightGBM\nimport lightgbm as lgb\n\n# Datetime\nimport datetime \nimport time\nfrom datetime import datetime\n\n# Folium\nimport folium \nfrom folium import plugins\nfrom folium.plugins import HeatMap\n\n# Image\nfrom IPython.display import Image\n\n# Bayesian Optimizer\nfrom skopt import BayesSearchCV\n\n# Itertools\nimport itertools\n\n# Remove warnings\nimport warnings\nwarnings.filterwarnings('ignore')","0790cbff":"df = pd.read_csv('..\/input\/Apply_Rate_2019.csv')","59a1ded5":"df.head()","fcee3c61":"# Check the total number of observations in the dataset\n\nprint('Total number of observations in the dataset are:',df.shape[0])","5efe8c48":"df.info()","17e377b1":"df.drop(['apply'],axis=1).describe()","4e8d3406":"# Lets check the distribution for classes who applied and did not apply\n\ncount_classes = pd.value_counts(df['apply'], sort = True)\ncount_classes.plot(kind = 'bar')\n\nplt.title(\"Apply Rate\")\nplt.xticks(range(2))\nplt.xlabel(\"Class\")\nplt.ylabel(\"Frequency\");\n\nprint('Number of customers who didnt apply:',df['apply'].value_counts()[0])\nprint('Number of customers who applied:',df['apply'].value_counts()[1])\nprint('Percentage of apply to non apply',df['apply'].value_counts()[0]\/df['apply'].value_counts()[1],'%')","37359e57":"# Lets check the correlation between the features\n\nsns.heatmap(df.corr())","6bb72e92":"l = ['title_proximity_tfidf', 'description_proximity_tfidf',\n       'main_query_tfidf', 'query_jl_score', 'query_title_score',\n       'city_match', 'job_age_days']\nnumber_of_columns=7\nnumber_of_rows = len(l)-1\/number_of_columns\nplt.figure(figsize=(number_of_columns,5*number_of_rows))\nfor i in range(0,len(l)):\n    plt.subplot(number_of_rows + 1,number_of_columns,i+1)\n    sns.set_style('whitegrid')\n    sns.boxplot(df[l[i]],color='green',orient='v')\n    plt.tight_layout()","b7feb51c":"# Check the distribution\n\n# Now to check the linearity of the variables it is a good practice to plot distribution graph and look for skewness \n# of features. Kernel density estimate (kde) is a quite useful tool for plotting the shape of a distribution.\n\nfor feature in df.columns[:-3]:\n    ax = plt.subplot()\n    sns.distplot(df[df['apply'] == 1][feature], bins=50, label='Anormal')\n    sns.distplot(df[df['apply'] == 0][feature], bins=50, label='Normal')\n    ax.set_xlabel('')\n    ax.set_title('histogram of feature: ' + str(feature))\n    plt.legend(loc='best')\n    plt.show()","e998863c":"print(df.shape)\ndf = df.drop_duplicates(keep = 'first')\ndf.shape","3252badf":"df.isnull().sum()","a427bbcb":"# Lets check the value counts for the three columns\ndf['title_proximity_tfidf'].value_counts().head()","f4a13b0c":"df['description_proximity_tfidf'].value_counts().head()","07147cb4":"df['city_match'].value_counts().head()","008ccd0c":"df['title_proximity_tfidf'].fillna(0,inplace=True)\ndf['description_proximity_tfidf'].fillna(0,inplace=True)\ndf.dropna(subset=['city_match'],inplace=True)","257ce18a":"# From the correlation graph, we observed that title_proximity_tfidf and main_query_tfidf are quite correlated, \n# lets merge them and get a single feature by multiplying both of them\n\ndf['main_title_tfidf'] = df['title_proximity_tfidf']*df['main_query_tfidf']","6a251e72":"df = df.drop(['title_proximity_tfidf','main_query_tfidf'], axis=1)","ba3ce0d3":"# Splitting the dataset by date\ntrain = df.loc[df['search_date_pacific']<'2018-01-27']\ntest = df.loc[df['search_date_pacific'] == '2018-01-27']","37edd0d6":"# Drop the unnecessary columns\ntrain.drop(['search_date_pacific','class_id'],axis=1,inplace = True)\ntest.drop(['search_date_pacific','class_id'],axis=1,inplace = True)","822cc2e2":"# Drop irrelevant features\nX = df.drop(['search_date_pacific','class_id','apply'],axis=1)\ny = df['apply']","422061a0":"# Reset the index\nX = X.reset_index(drop='index')\ny = y.reset_index(drop='index')","bfe3c462":"X_train = train.drop(['apply'],axis=1)\ny_train = train['apply']\nX_test = test.drop(['apply'],axis=1)\ny_test = test['apply']","b6ae4a28":"# Define a function to plot confusion matrix\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`\n    \"\"\"\n    plt.figure()\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.show()","f619cb75":"# Define a function which will be used to get the important parameters like AUC, Classification report\n\ndef report(test_set, predictions,labels,title):\n    print('F1 score is:', f1_score(test_set,predictions))\n    print(\"AUC-ROC is: %3.2f\" % (roc_auc_score(test_set, predictions)))\n    plot_confusion_matrix(confusion_matrix(test_set, predictions),labels,title)\n    \n    #plot the curve\n    fpr, tpr, threshold = roc_curve(test_set,predictions)\n    auc = roc_auc_score(test_set,predictions)\n    fig, ax = plt.subplots(figsize=(6,6))\n    ax.set_title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b',label='Model - AUC = %0.3f'% auc)\n    ax.legend(loc='lower right')\n    plt.plot([0,1],[0,1],'r--', label='Chance')\n    ax.legend()\n    ax.set_xlim([-0.1,1.0])\n    ax.set_ylim([-0.1,1.01])\n    ax.set_ylabel('True Positive Rate')\n    ax.set_xlabel('False Positive Rate')\n    plt.show()","527bc25a":"# Define a function to print the status during bayesian hyperparameter search\n\ndef status_print(optim_result):\n    \"\"\"Status callback durring bayesian hyperparameter search\"\"\"\n    \n    # Get all the models tested so far in DataFrame format\n    all_models = pd.DataFrame(bayes_cv_tuner.cv_results_)    \n    \n    # Get current parameters and the best parameters    \n    best_params = pd.Series(bayes_cv_tuner.best_params_)\n    print('Model #{}\\nBest ROC-AUC: {}\\nBest params: {}\\n'.format(\n        len(all_models),\n        np.round(bayes_cv_tuner.best_score_, 4),\n        bayes_cv_tuner.best_params_\n    ))\n\n","bcdaeec1":"# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\nITERATIONS = 10\nTRAINING_SIZE = 100000 \nTEST_SIZE = 25000\n\n\n# Classifier\nbayes_cv_tuner = BayesSearchCV(\n    estimator = xgb.XGBClassifier(\n        n_jobs = 1,\n        objective = 'binary:logistic',\n        eval_metric = 'auc',\n        silent=1,\n        tree_method='approx'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'max_delta_step': (0, 20),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'colsample_bylevel': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'gamma': (1e-9, 0.5, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100),\n        'scale_pos_weight': (1e-6, 500, 'log-uniform')\n    },    \n    scoring = 'roc_auc',\n    cv = StratifiedKFold(\n        n_splits=3,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 3,\n    n_iter = ITERATIONS,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n","501ef938":"result = bayes_cv_tuner.fit(X, y, callback=status_print)","37478925":"xgb = XGBClassifier(colsample_bylevel= 0.8390144719977516, colsample_bytree= 0.8844821246070537, \n                    gamma= 4.358684608480795e-07, learning_rate= 0.7988179462781242, max_delta_step= 17, \n                    max_depth= 3, min_child_weight= 1, n_estimators= 68, reg_alpha= 0.0005266983003701547, \n                    reg_lambda= 276.5424475574225, scale_pos_weight= 0.3016410771843142, subsample= 0.9923710598637134)\nxgb.fit(X_train, y_train)\npreds_xgb = xgb.predict_proba(X_test)[:, 1]\nlabels = ['No Apply', 'Apply']\n#report(y_test, preds_xgb,labels, 'Confusion Matrix')\nauc = roc_auc_score(y_test, preds_xgb)\n\nprint('The baseline score on the test set is {:.4f}.'.format(auc))","5699e8b7":"# ITERATIONS = 10 # 1000\n# TRAINING_SIZE = 100000 # 20000000\n# TEST_SIZE = 25000\n# # Classifier\n# bayes_cv_tuner = BayesSearchCV(\n#     estimator = RandomForestClassifier(\n#         n_jobs = -1\n#     ),\n#     search_spaces = {\n#     'min_samples_split': [3, 5, 8, 10, 20], \n#     'n_estimators' : [100, 500],\n#     'max_depth': [3, 5, 8, 10, 15],\n#     'max_features': [3, 5, 6]\n# },    \n#     scoring = 'roc_auc',\n#     cv = StratifiedKFold(\n#         n_splits=3,\n#         shuffle=True,\n#         random_state=42\n#     ),\n#     n_jobs = 3,\n#     n_iter = ITERATIONS,   \n#     verbose = 0,\n#     refit = True,\n#     random_state = 42\n# )","abc672c8":"# result = bayes_cv_tuner.fit(X, y, callback=status_print)","60b8abcc":"# rf = RandomForestClassifier(\n#     n_estimators=421, \n#     max_depth=15,\n#     max_features=3,\n#     min_samples_split=8, \n#     class_weight=\"balanced\",\n#     bootstrap=True,\n#     criterion='entropy',\n#     random_state=100\n#     )\n\n# rf.fit(X_train, y_train)\n# preds_rf = rf.predict_proba(X_test)[:,1]\n# #labels = ['No Apply', 'Apply']\n# #report(y_test, preds_rf,labels, 'Confusion Matrix')\n# auc = roc_auc_score(y_test, preds_rf)\n\n# print('The baseline score on the test set is {:.4f}.'.format(auc))","1e0b517d":"# SETTINGS - CHANGE THESE TO GET SOMETHING MEANINGFUL\nITERATIONS = 10\nTRAINING_SIZE = 100000 \nTEST_SIZE = 25000\n\n\n# Classifier\nbayes_cv_tuner = BayesSearchCV(\n    estimator = lgb.LGBMClassifier(\n        n_jobs = 1,\n        objective = 'binary',\n        eval_metric = 'auc',\n        silent=1,\n        tree_method='approx'\n    ),\n    search_spaces = {\n        'learning_rate': (0.01, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 10),\n        'max_depth': (0, 50),\n        'subsample': (0.01, 1.0, 'uniform'),\n        'colsample_bytree': (0.01, 1.0, 'uniform'),\n        'reg_lambda': (1e-9, 1000, 'log-uniform'),\n        'reg_alpha': (1e-9, 1.0, 'log-uniform'),\n        'min_child_weight': (0, 5),\n        'n_estimators': (50, 100)\n    },    \n    scoring = 'roc_auc',\n    cv = StratifiedKFold(\n        n_splits=3,\n        shuffle=True,\n        random_state=42\n    ),\n    n_jobs = 3,\n    n_iter = ITERATIONS,   \n    verbose = 0,\n    refit = True,\n    random_state = 42\n)\n","de12e637":"result = bayes_cv_tuner.fit(X, y, callback=status_print)","4a76ba10":"model = lgb.LGBMClassifier(colsample_bytree=0.8015579071911014, learning_rate=0.07517239253342656, \n                           max_depth=26, min_child_weight=4, n_estimators=95, reg_alpha=0.002839751649223172, \n                           reg_lambda=0.0001230656555713626, subsample=0.653781260730285)\n\nmodel.fit(X_train, y_train)\n\n\npreds_lgb = model.predict_proba(X_test)[:, 1]\nauc = roc_auc_score(y_test, preds_lgb)\n\nprint('The baseline score on the test set is {:.4f}.'.format(auc))","6c94d711":"#### Observation:\n\nThe data is imbalanced and so we might have to use techniques like resmapling (undersampling or oversampling) or use metrics like AUC-ROC curve or AUPRC or SMOTE to handle imbalanced data. Lets explore further which will help us decide what technique should we use. Note: It is already given in the dataset that I have to use AUC as the metric.","988f0fd4":"#### Observation:\n\nAs we can see there are lot of outliers in the data","e559a9e4":"### 1. Data collection","2922a2ac":"### Things TODO:\n- We can use stacking of the above three algorithms which can further improve the AUC\n\n- Include last column (class_id) to improve the results","59539dec":"Search for parameters of machine learning models that result in best cross-validation performance is necessary in almost all practical cases to get a model with best generalization estimate. A standard approach in scikit-learn is using GridSearchCV class, which takes a set of values for every parameter to try, and simply enumerates all combinations of parameter values. The complexity of such search grows exponentially with the addition of new parameters. A more scalable approach is using RandomizedSearchCV, which however does not take advantage of the structure of a search space.\n\nScikit-optimize provides a drop-in replacement for GridSearchCV, which utilizes Bayesian Optimization where a predictive model referred to as \"surrogate\" is used to model the search space and utilized to arrive at good parameter values combination as soon as possible.","11854ac5":"## 3. Data Cleaning","aee9649f":"### 2. Exploratory Data Analysis\n\nThe purpose of the exploratory analysis is to \u201cget to know\u201d the dataset. Doing so up front will make the rest of the project much smoother, in 3 main ways:\n\n- You\u2019ll gain valuable hints for Data Cleaning (which can make or break your models).\n- You\u2019ll think of ideas for Feature Engineering (which can take your models from good to great).\n- You\u2019ll get a \u201cfeel\u201d for the dataset, which will help you communicate results and deliver greater impact.\n\nHowever, exploratory analysis for machine learning should be quick, efficient, and decisive\u2026 not long and drawn out!\n\nDon\u2019t skip this step, but don\u2019t get stuck on it either.\n\nYou see, there are infinite possible plots, charts, and tables, but you only need a handful to \u201cget to know\u201d the data well enough to work with it.\n\nIn this step, we\u2019ll show you the visualizations that provide the biggest bang for your buck.\n\n#### Start with basics\n\nFirst, you\u2019ll want to answer a set of basic questions about the dataset:\n\n- How many observations do I have?\n- How many features?\n- What are the data types of my features? Are they numeric? Categorical?\n- Do I have a target variable?\n","e4e341ac":"## Hyperparameter tuning and CV using Bayesian Optimizer","891b5e2e":"#### Use the tuned parameters to make the predictions\n\n#### Note: For the predictions, because we are measuring ROC AUC and not accuracy, we have the model predict probabilities and not hard binary values.","b7b8c5c0":"### Correlation\n\nCorrelations allow you to look at the relationships between numeric features and other numeric features.\n\nCorrelation is a value between -1 and 1 that represents how closely two features move in unison. You don\u2019t need to remember the math to calculate them. Just know the following intuition:\n\n- Positive correlation means that as one feature increases, the other increases. E.g. a child\u2019s age and her height.\n- Negative correlation means that as one feature increases, the other decreases. E.g. hours spent studying and a number of parties attended.\n- Correlations near -1 or 1 indicate a strong relationship.\n- Those closer to 0 indicate a weak relationship.\n- 0 indicates no relationship\n","bcd2d68e":"### B. Random Forest","cf8c54cd":"### We will be following the process to solve this problem\n\n1. Data Collection\n2. Exploratory Data Analysis\n3. Data Cleaning \n4. Feature Engineering \n5. Model Training (including cross validation and hyperparameter tuning)\n6. Insights","6169afee":"### 4. Feature Engineering","008832ec":"### Insights\n- Although the improvement is not quite significant, the Bayesian optimizer was able to perform the tuning operation with greater speed.\n\n- Such low AUC score of 0.5849 may be attributed to the fact that we don't have many features in the dataset, which makes it difficult for the algorithm to classify the target variable correctly.\n\n- We did not have much domain knowledge because of which we were not able to perform much feature engineering.","865865fe":"### Drop Duplicates","63906fb8":"### Missing Values \n\nMissing data is a deceptively tricky issue in applied machine learning.\n\nFirst, just to be clear, you cannot simply ignore missing values in your dataset. You must handle them in some way for the very practical reason that most algorithms do not accept missing values.\n\n\u201cCommon sense\u201d is not sensible here\nThe following are the most commonly recommended ways of dealing with missing data:\n\n- Dropping observations that have missing values\n- Imputing the missing values based on other observations\n- Interpolation and Extrapolation\n- Using KNN\n- Mean\/ Median Imputation\n- Regression Imputation\n- Stochastic regression imputation\n- Hot-deck imputation","c062d255":"#### Observation:\n\n1. There is notably a large difference between 75th %tile and max values of mostly all the predictors.\n2. Median value of 'title_proximity_tfidf', 'description_proximity_tfidf', 'main_query_tfidf', 'query_jl_score', 'query_title_score', 'job_age_days' is lower than mean\n3. Thus observation 1 and 2 suggest there are lot of outliers in the data","c4864186":"#### Observation: \n\n1. title_proximity_tfidf, description_proximity_tfidf and city_match contains null values\n2. There are 7 float type, 2 integer type and 1 object type features","36ba4c86":"## Introduction\n\nImagine a user visiting a website, and performing a job search. From the set of displayed results, the user clicks on certain ones that he\/she is interested in, and after checking job descriptions, she further clicks on apply button therein to land into an application page. The apply rate is defined as the fraction of applies (after visiting job description pages), and the goal is to predict this metric using the dataset described in the following section.\n\nThis notebook will provide a complete guide to anyone who is new in Machine Learning. My goal is to provide you with an end-to-end blueprint for applied machine learning while keeping it as actionable and succinct as possible.\n\n","d5270765":"### A.  XGBoost ","1a74ecd5":"### C. LightGBM","5afa76d1":"#### Observation: \n\n1. title_proximity_tfidf and main_query_tfidf are correlated with value of arounf 0.7\n2. Other features are not highly correlated","4bccd9c9":"#### Note: I will not be removing outliers since there is possibility of them carrying important information which can help us detect the apply and non apply cases","e6fe8ab9":"In general, you can think of data cleaning as a process of subtraction and feature engineering as a process of addition.\n\nThis is often one of the most valuable tasks a data scientist can do to improve model performance, for 3 big reasons:\n\n- You can isolate and highlight key information, which helps your algorithms \u201cfocus\u201d on what\u2019s important.\n- You can bring in your own domain expertise.\n- Most importantly, once you understand the \u201cvocabulary\u201d of feature engineering, you can bring in other people\u2019s domain expertise!\n\nBelow are some of the ways we can perform feature engineering but please note that this is not an exhaustive compendium of all feature engineering because there are limitless possibilities for this step. The good news is that this skill will naturally improve as you gain more experience.\n\n- Infuse domain knowledge\n- Create interactive features\n- Combine sparse classes\n- Add dummy variables\n- Remove unused features\n\nIn our case, since there is not much domain knowledge about the dataset, we are restricted in our application of feature engineering. The only feature engineering that I have applied is multiplying the two features which were correlated (title_proximity_tfid and main_query_tfidf) to create a new column named main title tfidf.\n\n","0523e2e7":"### Commonly used Machine Learning algorithms for classification\n\n#### Logistic Regression\n\nLogistic Regression models fit a \u201cstraight line\u201d. In practice, they rarely perform well. We actually recommend skipping them for most machine learning problems.\n\nTheir main advantage is that they are easy to interpret and understand. However, our goal is not to study the data and write a research report. Our goal is to build a model that can make accurate predictions.\n\nIn this regard, logistic regression suffers from two major flaws:\n\n* It\u2019s prone to overfit with many input features.\n* It cannot easily express non-linear relationships.\n\n#### Regularization\n\nAs mentioned above, logistic regression suffers from overfitting and difficulty in handling non-linear relationships. Regularization is a technique used to prevent overfitting by artificially penalizing model coefficients.\n\n* It can discourage large coefficients (by dampening them).\n* It can also remove features entirely (by setting their coefficients to 0).\n* The \u201cstrength\u201d of the penalty is tunable. (More on this tomorrow\u2026)\n\nTypes of regularization are Lasso (L1), ridge (L2) and elastic net (compromise between ridge and lasso)\n\n#### Decision Trees\n\nDecision trees model data as a \u201ctree\u201d of hierarchical branches. They make branches until they reach \u201cleaves\u201d that represent predictions. Due to their branching structure, decision trees can easily model nonlinear relationships.\n\nUnfortunately, decision trees suffer from a major flaw as well. If you allow them to grow limitlessly, they can completely \u201cmemorize\u201d the training data, just from creating more and more and more branches. As a result, individual unconstrained decision trees are very prone to overfitting.\u200b\n\nSo, how can we take advantage of the flexibility of decision trees while preventing them from overfitting the training data?\n\n#### Tree Ensembles\n\nEnsembles are machine learning methods for combining predictions from multiple separate models. There are a few different methods for ensembling, but the three most common are:\n\nBagging: attempts to reduce the chance of overfitting complex models.\n\n* It trains a large number of \u201cstrong\u201d learners in parallel.\n* A strong learner is a model that\u2019s relatively unconstrained.\n* Bagging then combines all the strong learners together in order to \u201csmooth out\u201d their predictions.\n* Commonly used technique is Random Forest\n\nBoosting: attempts to improve the predictive flexibility of simple models.\n\n* It trains a large number of \u201cweak\u201d learners in sequence.\n* A weak learner is a constrained model (i.e. you could limit the max depth of each decision tree).\n* Each one in the sequence focuses on learning from the mistakes of the one before it.\n* Boosting then combines all the weak learners into a single strong learner.\n* Commonly used technique is XGBoost and LightGBM\n\nLightGBM: Light GBM is a gradient boosting framework that uses a tree-based learning algorithm. Light GBM grows tree vertically while other algorithm grows trees horizontally meaning that Light GBM grows tree leaf-wise while other algorithm grows level-wise. It will choose the leaf with max delta loss to grow. When growing the same leaf, Leaf-wise algorithm can reduce more loss than a level-wise algorithm.\n\nThere are many other algorithms as well like Support Vector machine, Neural Networks, etc. but we won't be taking it here.\n\nFor our case, I will be using XGBoost, Random Forest and LightGBM.\n\n\n","f7dce129":"#### Observation:\n\nFor all the features, both apply and non apply rates have almost similar distributions","c4a2b4d3":"#### Splitting the dataset into training and testing","83cff1da":"### 5. Modeling\n\n#### Some of the factors affecting the choice of a model are:\n\nWhether the model meets the business goals\n- How much pre-processing the model needs\n- How accurate the model is\n- How explainable the model is\n- How fast the model is: How long does it take to build a model, and how long does the model take to make predictions.\n- How scalable the model is\n\nAn important criterion affecting the choice of algorithm is model complexity. Generally speaking, a model is more complex is:\n\n- It relies on more features to learn and predict (e.g. using two features vs ten features to predict a target)\n- It relies on more complex feature engineering (e.g. using polynomial terms, interactions, or principal components)\n- It has more computational overhead (e.g. a single decision tree vs. a random forest of 100 trees).\n\nBesides this, the same machine learning algorithm can be made more complex based on the number of parameters or the choice of some hyperparameters. For example,\n\n- A regression model can have more features, or polynomial terms and interaction terms.\n- A decision tree can have more or less depth.\n- Making the same algorithm more complex increases the chance of overfitting.","6b9932a0":"### Outliers\n\nOne of the most important steps in Exploratory Data Analysis is outlier detection and treatment. Machine learning algorithms are very sensitive to the range and distribution of data points. Data outliers can deceive the training process resulting in longer training times and less accurate models. Outliers are defined as samples that are significantly different from the remaining data. Those are points that lie outside the overall pattern of the distribution. Statistical measures such as mean, variance and correlation are very susceptible to outliers.\n\n#### Nature of outliers:\n\nOutliers can occur in the dataset due to one of the following reasons,\n\n- Genuine extreme high and low values in the dataset\n- Introduced due to human or mechanical error\n- Introduced by replacing missing values\n\n#### Outlier Detection\n\n- Extreme Value Analysis\n- Z-score method\n- K Means clustering-based approach\n- Visualizing the data\n- Boxplot\n\n#### Outlier Treatment\n\n- Mean\/Median or random Imputation\n- Trimming\n- Top, Bottom and Zero Coding\n- Discretization\n\n**However, in this article, I will be detecting outliers using boxplot method. If you want to learn in-depth on how to detect and handle outliers, please refer to this article. Box plot diagram also termed as Whisker\u2019s plot is a graphical method typically depicted by quartiles and inter quartiles that helps in defining the upper limit and lower limit beyond which any data lying will be considered as outliers.\n\nIn brief, quantiles are points in a distribution that relates to the rank order of values in that distribution. For a given sample, you can find any quantile by sorting the sample. The middle value of the sorted sample is the middle quantile or the 50th percentile (also known as the median of the sample).\n\n","e2b8cdbe":"#### Observation:\n\nThe first 2 columns contains mostly value zero so it would be a safe option to impute a value of '0' to the first two columns. For the 'city-match' column, lets check the percentage of apply and non apply before and after we remove the NaN values. If the percentage is same, we can conclude that it is safe to remove rows that have NaN values in City_match column."}}