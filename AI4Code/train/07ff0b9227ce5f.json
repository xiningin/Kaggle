{"cell_type":{"43cb3cdf":"code","69f5b6c7":"code","4e91f4b8":"code","d1e9f6d8":"code","e31bc2ea":"code","2b2e1182":"code","284a5a6c":"code","5cd55db5":"code","8dc8f3da":"markdown","8b5f1e9b":"markdown","5574577e":"markdown","46189bd8":"markdown","a2879f85":"markdown","30d68e3a":"markdown"},"source":{"43cb3cdf":"import xgboost as xgb\nimport cuml\nimport cupy\n\nfrom cuml.metrics import roc_auc_score\nfrom cuml.datasets.classification import make_classification","69f5b6c7":"def print_version(*x):\n    for i in x:\n        print(i, eval(f'{i}.__version__'))\n        \ndef print_data_info(*x):\n    \n    for i in x:\n        data = eval(i)\n        major = 'row major' if data.flags.c_contiguous else 'column major'\n        print(i, type(data), data.shape, major)","4e91f4b8":"print_version('xgb', 'cuml', 'cupy')","d1e9f6d8":"%%time\n\nX, y = make_classification ( n_classes = 2,\n                             n_features = 10,\n                             n_samples = 10000,\n                             random_state = 0 )\n\nprint_data_info('X', 'y')","e31bc2ea":"def train_xgb(X, y):\n    params = {'eta': 0.1,\n              'max_depth': 3,\n              'objective': 'binary:logistic',\n              'eval_metric': 'auc',\n              'tree_method': 'gpu_hist',\n             }\n\n    dtrain = xgb.DMatrix(data=X, label=y)\n    bst = xgb.train(params, dtrain=dtrain,\n                    num_boost_round=10)\n\n    score = roc_auc_score(y, bst.predict(dtrain))\n    print(f\"training AUC = {score:.3f}\")","2b2e1182":"train_xgb(X, y)","284a5a6c":"print('Befor change:')\nprint_data_info('X')\n\nprint('After change:')\nX = cupy.ascontiguousarray(X)\nprint_data_info('X')","5cd55db5":"train_xgb(X, y)","8dc8f3da":"Train a simple xgboost model and return training AUC. ","8b5f1e9b":"Voila! I was astonished when I first found this.","5574577e":"The performance is quite poor! Let's check out the layout of data and change it.","46189bd8":"### In this notebook, we look at a bug in xgboost (1.2.1 or older) GPU mode. When the input data is `column major`, the performance will be significantly worse. Please change it to `row major` before training.\n\n### Please note that this bug is resolved in the new xgboost 1.3.0 version [#6459](https:\/\/github.com\/dmlc\/xgboost\/pull\/6459)","a2879f85":"Create some synthetic data for binary classification.","30d68e3a":"Let's check out the version of libraries."}}