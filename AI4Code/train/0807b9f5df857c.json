{"cell_type":{"27b3294d":"code","4d9ec011":"code","4580a743":"code","f43f1ddd":"code","ab980b40":"code","726cc680":"code","6906684a":"code","f896e0d4":"code","c36edc3b":"code","1e757d82":"code","889c7cfd":"code","7b4672bb":"code","5b76ad83":"code","48aba199":"markdown","3a6ae3e1":"markdown","ec72dbcc":"markdown","44347dee":"markdown","e23d8119":"markdown","c541e349":"markdown","7485b362":"markdown","5590816b":"markdown"},"source":{"27b3294d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.preprocessing import MinMaxScaler\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4d9ec011":"df = pd.read_csv('\/kaggle\/input\/brain-tumor\/data.csv')","4580a743":"df.isnull().sum()","f43f1ddd":"df['y'] = np.where(df['y'] == 'tumor', 1, 0)","ab980b40":"X = df.drop('y',axis=1).values\ny = df['y'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3,random_state=33)\n\nscaler = MinMaxScaler()\nX_train= scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\n#y_train = F.one_hot(torch.LongTensor(y_train))  # not needed with Cross Entropy Loss\n#y_test = F.one_hot(torch.LongTensor(y_test))\ny_train = torch.LongTensor(y_train)\ny_test = torch.LongTensor(y_test)","726cc680":"class Model(nn.Module):\n    def __init__(self, in_features=7465, h1=20, h2=10, out_features=2):\n        super().__init__()\n        \n        # input layer -> 1 hidden -> 2 hidden -> output \n        self.fc1 = nn.Linear(in_features,h1)    # input layer\n        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n        self.out = nn.Linear(h2, out_features)  # output layer\n        \n    def forward(self, x):\n        # Pass the neuron input through its activation function to obtain the output of the neuron\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.out(x)\n        return x","6906684a":"torch.manual_seed(4)\nmodel = Model()","f896e0d4":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)","c36edc3b":"epochs = 200 # number of runs through the training data\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    y_pred = model.forward(X_train)\n    loss = criterion(y_pred, y_train)\n    losses.append(loss)\n    \n    # a neat trick to save screen space:\n    if i%10 == 1:\n        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n\n    optimizer.zero_grad() #zero-grad to not accomulate the gradient over the epochs\n    loss.backward()\n    optimizer.step()","1e757d82":"plt.plot(range(epochs), losses)\nplt.ylabel('Loss')\nplt.xlabel('epoch');","889c7cfd":"with torch.no_grad():\n    y_test_pred = model.forward(X_test)","7b4672bb":"y_test_pred = torch.max(y_test_pred,1)[1]","5b76ad83":"print(confusion_matrix(y_test, y_test_pred))\nprint('---------------------------------------------------------')\nprint(classification_report(y_test, y_test_pred))","48aba199":"# ****Feature Analysis****","3a6ae3e1":"Lets convert the \"y\" to a numerical value.","ec72dbcc":"In this case I will apply a MLP to classify the data.","44347dee":"Since we have no missing values we are good to go.","e23d8119":"# ****EDA****","c541e349":"Since we have a lot of columns and we are not expertise in this area we can not measure how helpfull each of them is, so we only go to check for missing values.","7485b362":"# ****Deep Neural Network (Pytorch)****","5590816b":"# ****Model Evaluation****"}}