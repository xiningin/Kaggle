{"cell_type":{"ff43da11":"code","5c0b4969":"code","ca564e99":"code","4e9aaa01":"code","9fdff9fa":"code","83a7a149":"code","e434819e":"code","1d70812e":"code","e43ab626":"code","d9b6e285":"code","181e1ce6":"code","5a32138b":"code","573cac5d":"code","943e6625":"code","0ba57f70":"code","0b90a40e":"code","c7f6820f":"code","e9c0a2ad":"code","a4bfbc90":"code","5841ed23":"code","9e291a7f":"code","2c8bb330":"code","cb9a1816":"code","6de161ac":"code","2f1e9db5":"code","5c653aec":"code","9bc66c9f":"code","e3d94d62":"markdown","90d31b16":"markdown","34778136":"markdown","d6b70601":"markdown","ea0202f6":"markdown","12bfc5b8":"markdown","fff570db":"markdown","2bd223f0":"markdown","14f0fddf":"markdown","5f7a000e":"markdown"},"source":{"ff43da11":"import numpy as np\nimport pandas as pd\nimport os\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nimport time\nfrom kaggle_datasets import KaggleDatasets","5c0b4969":"try:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    print('Running on TPU', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() \n\nprint('Replicas:', strategy.num_replicas_in_sync)\n","ca564e99":"def load_image(image):        \n    image = tf.io.read_file(image)\n\n    image = tf.image.decode_jpeg(image)\n\n    w = tf.shape(image)[1]\n\n    w = w \/\/ 2\n    real_image = image[:, w:, :]\n    input_image = image[:, :w, :]\n\n    input_image = tf.cast(input_image, tf.float32)\n    real_image = tf.cast(real_image, tf.float32)\n\n    return input_image, real_image","4e9aaa01":"PATH = KaggleDatasets().get_gcs_path('aivazovsky-pix2pix-dataset')\nDS_PATH = os.path.join(PATH, 'pix2pix')\n\nBUFFER_SIZE = 400\nBATCH_SIZE = 1 * strategy.num_replicas_in_sync\n\nIMG_WIDTH = 256\nIMG_HEIGHT = 256\n\nAUTO = tf.data.experimental.AUTOTUNE","9fdff9fa":"TRAINING_FILENAMES = tf.io.gfile.glob(os.path.join(DS_PATH,'*.jpg'))","83a7a149":"inp, re = load_image(os.path.join(DS_PATH,'12.jpg'))\n\nplt.figure()\nplt.imshow(inp\/255.0)\n\nplt.figure()\nplt.imshow(re\/255.0)","e434819e":"def resize(input_image, real_image, height, width):\n  input_image = tf.image.resize(input_image, [height, width],\n                                method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n  real_image = tf.image.resize(real_image, [height, width],\n                               method=tf.image.ResizeMethod.NEAREST_NEIGHBOR)\n\n  return input_image, real_image\n\ndef random_crop(input_image, real_image):\n  stacked_image = tf.stack([input_image, real_image], axis=0)\n  cropped_image = tf.image.random_crop(\n      stacked_image, size=[2, IMG_HEIGHT, IMG_WIDTH, 3])\n\n  return cropped_image[0], cropped_image[1]\n\n\ndef normalize(input_image, real_image):\n  input_image = (input_image \/ 127.5) - 1\n  real_image = (real_image \/ 127.5) - 1\n\n  return input_image, real_image\n\n@tf.function()\ndef random_jitter(input_image, real_image):\n  # resizing to 286 x 286 x 3\n  input_image, real_image = resize(input_image, real_image, 286, 286)\n\n  # randomly cropping to 256 x 256 x 3\n  input_image, real_image = random_crop(input_image, real_image)\n\n  if tf.random.uniform(()) > 0.5:\n    # random mirroring\n    input_image = tf.image.flip_left_right(input_image)\n    real_image = tf.image.flip_left_right(real_image)\n\n  return input_image, real_image\n\ndef load_image_train(image_file):\n    input_image, real_image = load_image(image_file)\n    input_image, real_image = random_jitter(input_image, real_image)\n    input_image, real_image = normalize(input_image, real_image)\n    return input_image, real_image","1d70812e":"train_dataset = tf.data.Dataset.list_files(TRAINING_FILENAMES)\n\ntrain_dataset = train_dataset.map(load_image_train,\n                                  num_parallel_calls=tf.data.experimental.AUTOTUNE)\n\ntrain_dataset = train_dataset.shuffle(BUFFER_SIZE)\ntrain_dataset = train_dataset.batch(BATCH_SIZE)","e43ab626":"plt.figure(figsize=(6, 6))\nfor i in range(4):\n  rj_inp, rj_re = random_jitter(inp, re)\n  plt.subplot(2, 2, i+1)\n  plt.imshow(rj_inp\/255.0)\n  plt.axis('off')\nplt.show()","d9b6e285":"IMG_WIDTH = 256\nIMG_HEIGHT = 256\nOUTPUT_CHANNELS = 3\n\nIMG_SHAPE = [IMG_WIDTH, IMG_HEIGHT, OUTPUT_CHANNELS]\n\nGF = 64 # generator filters\nDF = 64 # discriminator filters\nS = 4","181e1ce6":"def conv2d(layer_input, filters, f_size=4, bn=True):\n    d = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n    d = tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n    if bn:\n        d = tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n    return d","5a32138b":"def deconv2d(layer_input, skip_input, filters, f_size=4, dropout_rate=0):\n    u =  tf.keras.layers.UpSampling2D(size=2)(layer_input)\n    u = tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=1, padding='same', activation='relu')(u)\n    if dropout_rate:\n        u = tf.keras.layers.Dropout(dropout_rate)(u)\n    u = tf.keras.layers.BatchNormalization(momentum=0.8)(u)\n    u = tf.keras.layers.Concatenate()([u, skip_input])\n    return u","573cac5d":"def Generator():\n    d0 = tf.keras.layers.Input(shape=IMG_SHAPE)\n    \n    d1 = conv2d(d0, GF, bn=False)\n    d2 = conv2d(d1, GF*2)\n    d3 = conv2d(d2, GF*4)\n    d4 = conv2d(d3, GF*8)\n    d5 = conv2d(d4, GF*8)\n    d6 = conv2d(d5, GF*8)\n    d7 = conv2d(d6, GF*8)\n\n    # Upsampling\n    u1 = deconv2d(d7, d6, GF*8)\n    u2 = deconv2d(u1, d5, GF*8)\n    u3 = deconv2d(u2, d4, GF*8)\n    u4 = deconv2d(u3, d3, GF*4)\n    u5 = deconv2d(u4, d2, GF*2)\n    u6 = deconv2d(u5, d1, GF)\n\n    u7 = tf.keras.layers.UpSampling2D(size=2)(u6)\n    output_img = tf.keras.layers.Conv2D(OUTPUT_CHANNELS, kernel_size=4,\n                                        strides=1, padding='same', activation='tanh')(u7)\n    with strategy.scope():\n        return tf.keras.models.Model(d0, output_img)","943e6625":"def d_layer(layer_input, filters, f_size=4, bn=True):\n    \"\"\"Discriminator layer\"\"\"\n    d =  tf.keras.layers.Conv2D(filters, kernel_size=f_size, strides=2, padding='same')(layer_input)\n    d =  tf.keras.layers.LeakyReLU(alpha=0.2)(d)\n    if bn:\n        d =  tf.keras.layers.BatchNormalization(momentum=0.8)(d)\n    return d","0ba57f70":"def Discriminator():\n    img_A = tf.keras.layers.Input(shape=IMG_SHAPE)\n    img_B = tf.keras.layers.Input(shape=IMG_SHAPE)\n\n    combined_imgs = tf.keras.layers.Concatenate(axis=-1)([img_A, img_B])\n\n    d1 = d_layer(combined_imgs, DF, bn=False)\n    d2 = d_layer(d1, DF*2)\n    d3 = d_layer(d2, DF*4)\n    d4 = d_layer(d3, DF*8)\n\n    validity = tf.keras.layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(d4)\n    \n    with strategy.scope():\n        return tf.keras.models.Model([img_A, img_B], validity)","0b90a40e":"generator = Generator()\n\ndiscriminator = Discriminator()\n\n\ngenerator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\ndiscriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)","c7f6820f":"LAMBDA = 100","e9c0a2ad":"loss_object = tf.keras.losses.BinaryCrossentropy(from_logits=True)","a4bfbc90":"def generator_loss(disc_generated_output, gen_output, target):\n    gan_loss = loss_object(tf.ones_like(disc_generated_output), disc_generated_output)\n\n    # mean absolute error\n    l1_loss = tf.reduce_mean(tf.abs(target - gen_output))\n\n    total_gen_loss = gan_loss + (LAMBDA * l1_loss)\n\n    return total_gen_loss, gan_loss, l1_loss","5841ed23":"def discriminator_loss(disc_real_output, disc_generated_output):\n  real_loss = loss_object(tf.ones_like(disc_real_output), disc_real_output)\n\n  generated_loss = loss_object(tf.zeros_like(disc_generated_output), disc_generated_output)\n\n  total_disc_loss = real_loss + generated_loss\n\n  return total_disc_loss","9e291a7f":"def generate_images(model, test_input, tar):\n    prediction = model(test_input, training=True)\n    plt.figure(figsize=(15,15))\n\n    display_list = [test_input[0], tar[0], prediction[0]]\n    title = ['Input Image', 'Ground Truth', 'Predicted Image']\n\n    for i in range(3):\n        plt.subplot(1, 3, i+1)\n        plt.title(title[i])\n        # getting the pixel values between [0, 1] to plot it.\n        plt.imshow(display_list[i] * 0.5 + 0.5)\n        plt.axis('off')\n    plt.show()","2c8bb330":"@tf.function\ndef train_step(input_image, target, epoch):\n    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n        gen_output = generator(input_image, training=True)\n\n        disc_real_output = discriminator([input_image, target], training=True)\n        disc_generated_output = discriminator([input_image, gen_output], training=True)\n\n        gen_total_loss, gen_gan_loss, gen_l1_loss = generator_loss(disc_generated_output, gen_output, target)\n        disc_loss = discriminator_loss(disc_real_output, disc_generated_output)\n\n    generator_gradients = gen_tape.gradient(gen_total_loss,\n                                          generator.trainable_variables)\n    discriminator_gradients = disc_tape.gradient(disc_loss,\n                                               discriminator.trainable_variables)\n\n    generator_optimizer.apply_gradients(zip(generator_gradients,\n                                          generator.trainable_variables))\n    discriminator_optimizer.apply_gradients(zip(discriminator_gradients,\n                                              discriminator.trainable_variables))","cb9a1816":"def fit(train_ds, epochs):\n    for epoch in range(epochs):\n        start = time.time()\n\n        for example_input, example_target in train_ds.take(1):\n            generate_images(generator, example_input, example_target)\n            \n        print(\"Epoch: \", epoch)\n\n        for n, (input_image, target) in train_ds.enumerate():\n            print('.', end='')\n            if (n+1) % 100 == 0:\n                print()\n            train_step(input_image, target, epoch)\n        print()\n        \n        json_config = generator.to_json()\n\n        with open('generator.json', 'w', encoding='utf-8') as f:\n            f.write(json_config)\n\n        json_config = discriminator.to_json()\n\n        with open('discriminator.json', 'w', encoding='utf-8') as f:\n            f.write(json_config)\n\n        generator.save_weights(\"generator.h5\")\n        discriminator.save_weights(\"discriminator.h5\")\n\n        print ('Time taken for epoch {} is {} sec\\n'.format(epoch + 1,\n                                                            time.time()-start))","6de161ac":"generator.load_weights('..\/input\/aivazovsky-pix2pix-dataset\/generator_209.h5')\ndiscriminator.load_weights('..\/input\/aivazovsky-pix2pix-dataset\/discriminator_209.h5')","2f1e9db5":"EPOCHS = 200\n\nfit(train_dataset, EPOCHS)","5c653aec":"inp, re = load_image(os.path.join(DS_PATH, '1.jpg'))\n\npr = generator(((inp.numpy() \/ 127.5) - 1).reshape((1,256,256,3)))\nplt.figure()\nplt.imshow((pr[0]+1)\/2)","9bc66c9f":"json_config = generator.to_json()\n\nwith open('generator.json', 'w', encoding='utf-8') as f:\n    f.write(json_config)\n\njson_config = discriminator.to_json()\n\nwith open('discriminator.json', 'w', encoding='utf-8') as f:\n    f.write(json_config)\n    \ngenerator.save_weights(\"generator.h5\")\ndiscriminator.save_weights(\"discriminator.h5\")","e3d94d62":"train_dataset\n","90d31b16":"# Construct Model","34778136":"# Load Dataset","d6b70601":"Test random_jitter","ea0202f6":"# Save Model","12bfc5b8":"# Training","fff570db":"**Generator**","2bd223f0":"# Prepare Dataset","14f0fddf":"**Discriminator**","5f7a000e":"# Test Model"}}