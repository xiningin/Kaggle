{"cell_type":{"6bf7ab2e":"code","33134cff":"code","d5f58248":"code","da15a5c9":"code","eca7d713":"code","353891b4":"code","d310fa20":"code","3a7d5df9":"code","2bae45f1":"code","018aef09":"code","f3d115d9":"code","768f542a":"code","88c3298a":"code","1f0abc81":"code","ebaa8602":"markdown","9a89646a":"markdown","036763d9":"markdown","ff7ec1a8":"markdown","982fe47a":"markdown","6defebf2":"markdown","ccfa00c2":"markdown","4624b4ea":"markdown","9f563cf0":"markdown","bc25c7e3":"markdown"},"source":{"6bf7ab2e":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nimport numpy as np","33134cff":"%matplotlib inline","d5f58248":"x = torch.linspace(0,799,800)\ny = torch.sin(x*2*3.1416\/40)","da15a5c9":"plt.figure(figsize=(12,4))\nplt.title('The sine wave')\nplt.xlim(-10, 801)\nplt.grid(True)\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(y.numpy(), color=\"#8000ff\")\nplt.show()","eca7d713":"test_size = 40\ntrain_set = y[:-test_size]\ntest_set = y[-test_size:]","353891b4":"plt.figure(figsize=(12,4))\nplt.xlim(-10, 801)\nplt.title('train and test data')\nplt.xlabel('x')\nplt.ylabel('y')\nplt.plot(train_set, color=\"#8000ff\")\nplt.plot(range(760,800),test_set, color=\"#ff8800\")","d310fa20":"def input_data(seq,ws):\n    out = []\n    L = len(seq)\n    \n    for i in range(L-ws):\n        window = seq[i:i+ws]\n        label = seq[i+ws:i+ws+1]\n        out.append((window, label))\n    \n    return out","3a7d5df9":"window_size = 40\ntrain_data = input_data(train_set, window_size)\nlen(train_data)","2bae45f1":"train_data[0]","018aef09":"class LSTM(nn.Module):\n    def __init__(self, input_size=1,hidden_size=50, output_size=1):\n        super(LSTM, self).__init__()\n        self.lstm1 = nn.LSTM(input_size, hidden_size)\n        self.fc1  = nn.Linear(hidden_size, output_size)\n        self.hidden_size = 50\n        self.hidden = (torch.zeros(1,1, hidden_size), torch.zeros(1,1, hidden_size))\n    \n    def forward(self, seq):\n        lstm_out, self.hidden = self.lstm1(seq.view(len(seq),1,-1), self.hidden)\n        pred = self.fc1(lstm_out.view(len(seq),-1))\n        return pred[-1]\n        ","f3d115d9":"# x,y = train_data[0]\n# x","768f542a":"# print(x.view(len(x),1,-1))\n# print(x.view(len(x), -1))","88c3298a":"torch.manual_seed(42)\nmodel = LSTM()\nloss_criterion = nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01)","1f0abc81":"num_epochs = 6\nfuture = 40\n\nfor epoch in range(num_epochs):\n    for seq, y_train in train_data:\n        optimizer.zero_grad()\n        \n        model.hidden = (torch.zeros(1,1,model.hidden_size), torch.zeros(1,1,model.hidden_size))\n        \n        y_pred = model(seq)\n        loss = loss_criterion(y_pred, y_train)\n        \n        loss.backward()\n        \n        optimizer.step()\n    print(f\"Loss for epoch : {epoch} set: {loss}\" )\n    \n    preds = train_set[-window_size:].tolist()\n    \n    for f in range(future):\n        seq = torch.FloatTensor(preds[-window_size:])\n        with torch.no_grad():\n            model.hidden = (torch.zeros(1,1,model.hidden_size), torch.zeros(1,1,model.hidden_size))\n            preds.append(model(seq))\n        \n    loss= loss_criterion(torch.tensor(preds[-window_size:]), y[760:])\n    print(f\"Performance on test range: {loss}\")\n    \n    plt.figure(figsize=(12,4))\n    plt.xlim(700, 801)\n    plt.grid(True)\n    plt.plot(y, color=\"#8000ff\")\n    plt.plot(range(760,800), preds[window_size:], color=\"#ff8000\")\n    plt.show()\n        ","ebaa8602":"## Lets start by creating a sine wave","9a89646a":"## Lets train the model and visualise it","036763d9":"from https:\/\/www.kaggle.com\/namanmanchanda\/rnn-in-pytorch\n\n## Recurrent Neural Networks \n\nThese are a type of neural network designed to work on sequence prediction models.\n\nCan be used for text data, speech data, classification data and generative models.","ff7ec1a8":"checking the first value of the function","982fe47a":"## Creating the model","6defebf2":"calling input_data function on our train set","ccfa00c2":"### Concept of window sizes in RNN\n\nWhile working with LSTM models, we divide the training sequence into series of overlapping windows. The label used for comparison is the next value in the sequence.\n\nFor example if we have series of of 12 records and a window size of 3, we feed [x1, x2, x3] into the model, and compare the prediction to x4. Then we backdrop, update parameters, and feed [x2, x3, x4] into the model and compare the prediction to x5. To ease this process, I'm defining a function input_data(seq,ws) that created a list of (seq,labels) tuples. If ws is the window size, then the total number of (seq,labels) tuples will be len(series)-ws.","4624b4ea":"### splitting the data into training and testing set","9f563cf0":"### Lets plot it","bc25c7e3":"## Model instantiation\n"}}