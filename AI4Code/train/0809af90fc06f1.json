{"cell_type":{"b24b7b55":"code","cd31772e":"code","a457f40c":"code","61d934b4":"code","e1dae7ec":"code","e8800022":"code","f32f5f03":"code","d6923b2a":"code","950b8d77":"code","5fd68565":"code","1ea2805a":"code","6cc0db4e":"code","b4de869d":"code","4d4f643b":"markdown","25b6421a":"markdown","c6826f9f":"markdown","f08053ad":"markdown","81a76903":"markdown","4e998bf0":"markdown","609d82f7":"markdown","f90b7676":"markdown","91efb0e6":"markdown","00251375":"markdown","8409d878":"markdown","3a6dadf8":"markdown","dd7b8b4d":"markdown","fdd0b115":"markdown","87c1e608":"markdown","ee239479":"markdown"},"source":{"b24b7b55":"!pip install mglearn==0.1.9","cd31772e":"%matplotlib inline\nimport sys\nfrom scipy import sparse\nprint(\"Python version: {}\".format(sys.version))\nimport pandas as pd\nprint(\"pandas version: {}\".format(pd.__version__))\nimport matplotlib\nprint(\"matplotlib version: {}\".format(matplotlib.__version__))\nimport numpy as np\nprint(\"NumPy version: {}\".format(np.__version__))\nimport scipy as sp\nprint(\"SciPy version: {}\".format(sp.__version__))\nimport IPython\nprint(\"IPython version: {}\".format(IPython.__version__))\nimport sklearn\nprint(\"scikit-learn version: {}\".format(sklearn.__version__))\nimport mglearn\nimport matplotlib.pyplot as plt","a457f40c":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC\nX, y = mglearn.datasets.make_forge()\nfig, axes = plt.subplots(1, 2, figsize=(20, 6))\nfor model, ax in zip([LinearSVC(), LogisticRegression()], axes):\n    clf = model.fit(X, y)\n    mglearn.plots.plot_2d_separator(clf, X, fill=False, eps=0.5,\n    ax=ax, alpha=.7)\n    mglearn.discrete_scatter(X[:, 0], X[:, 1], y, ax=ax)\n    ax.set_title(\"{}\".format(clf.__class__.__name__))\n    ax.set_xlabel(\"Feature 0\")\n    ax.set_ylabel(\"Feature 1\")\naxes[0].legend()","61d934b4":"mglearn.plots.plot_linear_svc_regularization()","e1dae7ec":"from sklearn.model_selection import train_test_split\nfrom sklearn.datasets import load_breast_cancer\ncancer = load_breast_cancer()\nX_train, X_test, y_train, y_test = train_test_split(\ncancer.data, cancer.target, stratify=cancer.target, random_state=42)\nlogreg = LogisticRegression().fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg.score(X_test, y_test)))","e8800022":"logreg100 = LogisticRegression(C=100).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg100.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg100.score(X_test, y_test)))","f32f5f03":"logreg001 = LogisticRegression(C=0.01).fit(X_train, y_train)\nprint(\"Training set score: {:.3f}\".format(logreg001.score(X_train, y_train)))\nprint(\"Test set score: {:.3f}\".format(logreg001.score(X_test, y_test)))","d6923b2a":"plt.figure(figsize=(20,6))\nplt.plot(logreg.coef_.T, 'o', label=\"C=1\")\nplt.plot(logreg100.coef_.T, '^', label=\"C=100\")\nplt.plot(logreg001.coef_.T, 'v', label=\"C=0.001\")\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.ylim(-5, 5)\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.legend()","950b8d77":"for C, marker in zip([0.001, 1, 100], ['o', '^', 'v']):\n    lr_l2 = LogisticRegression(C=C, penalty=\"l2\").fit(X_train, y_train)\n    print(\"Training accuracy of l2 logreg with C={:.3f}: {:.2f}\".format(C, lr_l2.score(X_train, y_train)))\n    print(\"Test accuracy of l2 logreg with C={:.3f}: {:.2f}\".format(C, lr_l2.score(X_test, y_test)))\n    plt.plot(lr_l2.coef_.T, marker, label=\"C={:.3f}\".format(C))\nplt.xticks(range(cancer.data.shape[1]), cancer.feature_names, rotation=90)\nplt.hlines(0, 0, cancer.data.shape[1])\nplt.xlabel(\"Coefficient index\")\nplt.ylabel(\"Coefficient magnitude\")\nplt.ylim(-5, 5)\nplt.legend(loc=3)","5fd68565":"from sklearn.datasets import make_blobs\nX, y = make_blobs(random_state=42)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend([\"Class 0\", \"Class 1\", \"Class 2\"])","1ea2805a":"linear_svm = LinearSVC().fit(X, y)\nprint(\"Coefficient shape: \", linear_svm.coef_.shape)\nprint(\"Intercept shape: \", linear_svm.intercept_.shape)","6cc0db4e":"mglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) \/ coef[1], c=color)\nplt.ylim(-10, 15)\nplt.xlim(-10, 8)\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n'Line class 2'], loc=(1.01, 0.3))","b4de869d":"mglearn.plots.plot_2d_classification(linear_svm, X, fill=True, alpha=.7)\nmglearn.discrete_scatter(X[:, 0], X[:, 1], y)\nline = np.linspace(-15, 15)\nfor coef, intercept, color in zip(linear_svm.coef_, linear_svm.intercept_,['b', 'r', 'g']):\n    plt.plot(line, -(line * coef[0] + intercept) \/ coef[1], c=color)\nplt.legend(['Class 0', 'Class 1', 'Class 2', 'Line class 0', 'Line class 1',\n'Line class 2'], loc=(1.01, 0.3))\nplt.xlabel(\"Feature 0\")\nplt.ylabel(\"Feature 1\")","4d4f643b":"**You can see that all the points belonging to class 0 in the training data are above the  line corresponding to class 0, which means they are on the \u201cclass 0\u201d side of this binary classifier. The points in class 0 are above the line corresponding to class 2, which means they are classified as \u201crest\u201d by the binary classifier for class 2. The points belonging to class 0 are to the left of the line corresponding to class 1, which means the binary classifier for class 1 also classifies them as \u201crest.\u201d Therefore, any point in this area will be classified as class 0 by the final classifier the result of the classification confidence formula for classifier 0 is greater than zero, while it is smaller than zero for the other two classes.**","25b6421a":"**Now, we train a LinearSVC classifier on the dataset**","c6826f9f":"# Linear Models For Binary Classification","f08053ad":"# Linear Models For Multi-Class Classification","81a76903":"**Using C=100 results in higher training set accuracy, and also a slightly increased test set accuracy, confirming our intuition that a more complex model should perform better.**","4e998bf0":"**As Logistic Regression applies L2 regularization by default, the results look similar to that produced by ridge regularization. Stronger regularization pushes coefficients more and more toward zero. By observing the plot more closely, we can also see an interesting effect in the third coefficient, for \u201cmean perimeter.\u201d For C=100 and C=1, the coefficient\nis negative, while for C=0.001, the coefficient is positive, with a magnitude that is even larger than for C=1. Interpreting a model like this, one might think the coefficient tells us which class a feature is associated with. For example, one might think that a high \u201ctexture error\u201d feature is related to a sample being \u201cmalignant.\u201d However, the change of sign in the coefficient for \u201cmean perimeter\u201d means that depending on which model we look at, a high \u201cmean perimeter\u201d could be taken as being either indicative of \u201cbenign\u201d or indicative of \u201cmalignant.\u201d**","609d82f7":"**In the figure above, we display the first feature of the forge dataset on the x axis and 2nd feature on the y-axis. The decision boundaries found by LinearSVC and LogisticRegression are displayed as straight lines, separating the area classifies as class 1 and 0.**","f90b7676":"**Two-dimensional toy dataset shows dataset containing 3 classes**","91efb0e6":"**Let's analyze LinearLogisticRegression in more detail on the Breast Cancer dataset.**","00251375":"**The default value of C=1 provides quite good performance, with 95% accuracy on both the training and the test set. But as training and test set performance are very close, it is likely that we are underfitting. Let\u2019s try to increase C to fit a more flexible model.**","8409d878":"**On the lefthand figure, small C means a lot of regularization. In the centre plot, C is slightly higher and the very high value of C in the model tilts the decision boundary a lot, now correctly classifying all the points in class 0. One of the points in class 1 is still classified, as it is not possible to correctly classify all points in this dataset using a straight\nline.**","3a6dadf8":"**Let's look at the coefficients learned by the models with the three different settings of the regularization parameter C**","dd7b8b4d":"**Coefficient plot and classification accuracies for L2 regularization**","fdd0b115":"**Here regularization parameter is applied where the strength of the regularization is called 'C'. Higher values of 'C' corresponds to less regulariztion.**","87c1e608":"**The above figure shows Multiclass decision boundaries derived from the three one-vs.-rest classifiers**","ee239479":"**We can also investigate what happens if we use an even more regularized model than the default of C=1, by setting C=0.01:**"}}