{"cell_type":{"8eb9bb5e":"code","7c646c0a":"code","56eca95d":"code","ae77b44e":"code","2b2707e5":"code","926e6aa2":"code","5b35fed5":"code","78495199":"code","2b9fa489":"code","3488bab7":"code","220108da":"code","2fa24573":"code","cb7e92d4":"code","7234b953":"code","ead1cb9e":"code","95e5ac97":"code","8b7ad5e7":"code","6cbcb8fd":"code","d0bf2e9e":"code","e39b1965":"code","5e3903c6":"code","477dfab4":"code","2ff161bd":"code","0fc86db8":"code","6674c531":"code","1ad59292":"code","b89b9631":"code","0fa7d5da":"code","b13d1b29":"code","2d004ed2":"code","ed82a30b":"code","632b1aff":"markdown"},"source":{"8eb9bb5e":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# %tensorflow_version 2.x  # this line is not required unless you are in a notebook\nimport tensorflow as tf\nfrom tensorflow.keras import datasets, layers, models\nimport matplotlib.pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import ImageDataGenerator\nimport tensorflow_datasets as tfds\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","7c646c0a":"#  LOAD AND SPLIT DATASET\n(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\ntrain_images, test_images = train_images \/ 255.0, test_images \/ 255.0\n\nclass_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n               'dog', 'frog', 'horse', 'ship', 'truck']","56eca95d":"# Let's look at a one image\nIMG_INDEX = 7  # change this to look at other images\n\nplt.imshow(train_images[IMG_INDEX] ,cmap=plt.cm.binary)\nplt.xlabel(class_names[train_labels[IMG_INDEX][0]])\nplt.show()","ae77b44e":"model = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))","2b2707e5":"model.summary()  # let's have a look at our model so far","926e6aa2":"model.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10))","5b35fed5":"model.summary()","78495199":"model.compile(optimizer='adam',\n              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n              metrics=['accuracy'])\nhistory = model.fit(train_images, train_labels, epochs=4, \n                    validation_data=(test_images, test_labels))","2b9fa489":"test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\nprint(test_acc)","3488bab7":"# creates a data generator object that transforms images\ndatagen = ImageDataGenerator(\nrotation_range=40,\nwidth_shift_range=0.2,\nheight_shift_range=0.2,\nshear_range=0.2,\nzoom_range=0.2,\nhorizontal_flip=True,\nfill_mode='nearest')\n\n# pick an image to transform\ntest_img = train_images[20]\nimg = image.img_to_array(test_img)  # convert image to numpy arry\nimg = img.reshape((1,) + img.shape)  # reshape image\n\ni = 0\n\nfor batch in datagen.flow(img, save_prefix='test', save_format='jpeg'):  # this loops runs forever until we break, saving images to current directory with specified prefix\n    plt.figure(i)\n    plot = plt.imshow(image.img_to_array(batch[0]))\n    i += 1\n    if i > 4:  # show 4 images\n        break\n\nplt.show()","220108da":"keras = tf.keras","2fa24573":"tfds.disable_progress_bar()\n\n# split the data manually into 80% training, 10% testing, 10% validation\n(raw_train, raw_validation, raw_test), metadata = tfds.load(\n    'cats_vs_dogs',\n    split=['train[:80%]', 'train[80%:90%]', 'train[90%:]'],\n    with_info=True,\n    as_supervised=True,\n)","cb7e92d4":"get_label_name = metadata.features['label'].int2str  \n# creates a function object that we can use to get labels\n\n# display 2 images from the dataset\nfor image, label in raw_train.take(5):\n    plt.figure()\n    plt.imshow(image)\n    plt.title(get_label_name(label))","7234b953":"IMG_SIZE = 160 # All images will be resized to 160x160\n\ndef format_example(image, label):\n    \"\"\"\n    returns an image that is reshaped to IMG_SIZE\n    \"\"\"\n    image = tf.cast(image, tf.float32)\n    image = (image\/127.5) - 1\n    image = tf.image.resize(image, (IMG_SIZE, IMG_SIZE))\n    return image, label","ead1cb9e":"train = raw_train.map(format_example)\nvalidation = raw_validation.map(format_example)\ntest = raw_test.map(format_example)","95e5ac97":"for image, label in train.take(2):\n    plt.figure()\n    plt.imshow(image)\n    plt.title(get_label_name(label))","8b7ad5e7":"BATCH_SIZE = 32\nSHUFFLE_BUFFER_SIZE = 1000\n\ntrain_batches = train.shuffle(SHUFFLE_BUFFER_SIZE).batch(BATCH_SIZE)\nvalidation_batches = validation.batch(BATCH_SIZE)\ntest_batches = test.batch(BATCH_SIZE)","6cbcb8fd":"for img, label in raw_train.take(2):\n    print(\"Original shape:\", img.shape)\n\nfor img, label in train.take(2):\n    print(\"New shape:\", img.shape)","d0bf2e9e":"IMG_SHAPE = (IMG_SIZE, IMG_SIZE, 3)\n\n# Create the base model from the pre-trained model MobileNet V2\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","e39b1965":"base_model.summary()","5e3903c6":"for image, _ in train_batches.take(1):\n    pass\n\nfeature_batch = base_model(image)\nprint(feature_batch.shape)","477dfab4":"base_model.trainable = False","2ff161bd":"base_model.summary()","0fc86db8":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()","6674c531":"prediction_layer = keras.layers.Dense(1)","1ad59292":"model = tf.keras.Sequential([\n  base_model,\n  global_average_layer,\n  prediction_layer\n])","b89b9631":"model.summary()","0fa7d5da":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.RMSprop(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","b13d1b29":"# We can evaluate the model right now to see how it does before training it on our new images\ninitial_epochs = 3\nvalidation_steps=20\n\nloss0,accuracy0 = model.evaluate(validation_batches, steps = validation_steps)","2d004ed2":"# Now we can train it on our images\nhistory = model.fit(train_batches,\n                    epochs=initial_epochs,\n                    validation_data=validation_batches)\n\nacc = history.history['accuracy']\nprint(acc)","ed82a30b":"model.save(\"dogs_vs_cats.h5\")  \n# we can save the model and reload it at anytime in the future\nnew_model = tf.keras.models.load_model('dogs_vs_cats.h5')","632b1aff":"Classifying 10 different everyday objects. \n\nThe dataset we will use is built into tensorflow and called the [**CIFAR Image Dataset.**](https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html) \n\nIt contains 60,000 32x32 color images with 6000 images of each class. \n\nThe labels in this dataset are the following:\n- Airplane\n- Automobile\n- Bird\n- Cat\n- Deer\n- Dog\n- Frog\n- Horse\n- Ship\n- Truck"}}