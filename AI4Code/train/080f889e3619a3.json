{"cell_type":{"929a7bdf":"code","618c1c7b":"code","c58b4f17":"code","175e209b":"code","67a334d0":"code","b0ad45bb":"code","a5aa3f52":"code","2def18c5":"code","65cbc29d":"code","de646563":"code","b62ba231":"code","5260584c":"code","220f01c8":"code","dbf1d2c5":"code","1a0a5d4e":"markdown","35b94970":"markdown","e6380e95":"markdown","2e7d91a0":"markdown","ca99d72c":"markdown","457ff91f":"markdown","bd3c7139":"markdown","6e3d620e":"markdown","eeaa1e5a":"markdown","25bf6b8d":"markdown","a8d290e2":"markdown","f4c6bf99":"markdown","f5c2abab":"markdown"},"source":{"929a7bdf":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.base import BaseEstimator\nfrom sklearn.metrics import mean_squared_error, log_loss, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler","618c1c7b":"class SGDRegressor(BaseEstimator):\n    \n    def __init__(self, eta=1e-3, n_epochs=3):\n        self.eta = eta\n        self.n_epochs = n_epochs\n        self.mse_ = []\n        self.weights_ = []\n        \n    def fit(self, X, y):\n        # add a column of ones to the left from X\n        X = np.hstack([np.ones([X.shape[0], 1]), X])\n        \n        # initialize w with zeros, (d + 1)-dimensional (2-dimensional)\n        w = np.zeros(X.shape[1])\n        \n        for it in tqdm(range(self.n_epochs)):\n            for i in range(X.shape[0]):\n                \n                # new_w is used for simultanious updates of w_0, w_1, ..., w_d\n                new_w = w.copy()\n                # special (simpler) formula for w_0\n                new_w[0] += self.eta * (y[i] - w.dot(X[i, :]))\n                for j in range(1, X.shape[1]):\n                    new_w[j] += self.eta * (y[i] - w.dot(X[i, :])) * X[i, j]  \n                w = new_w.copy()\n                \n                # store the current weight vector\n                self.weights_.append(w)\n                # store current loss function\n                self.mse_.append(mean_squared_error(y, X.dot(w)))\n        # the \"best\" vector of weights        \n        self.w_ = self.weights_[np.argmin(self.mse_)]\n                \n        return self\n                  \n    def predict(self, X):\n        # add a column of ones to the left from X\n        X = np.hstack([np.ones([X.shape[0], 1]), X])\n        # linear prediction\n        return X.dot(self.w_)                  ","c58b4f17":"data_demo = pd.read_csv('..\/input\/weights_heights.csv')","175e209b":"plt.scatter(data_demo['Weight'], data_demo['Height']);\nplt.xlabel('Weight (lbs)')\nplt.ylabel('Height (Inch)')\nplt.grid();","67a334d0":"X, y = data_demo['Weight'].values, data_demo['Height'].values","b0ad45bb":"X_train, X_valid, y_train, y_valid = train_test_split(X, y,\n                                                     test_size=0.3,\n                                                     random_state=17)","a5aa3f52":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train.reshape([-1, 1]))\nX_valid_scaled = scaler.transform(X_valid.reshape([-1, 1]))","2def18c5":"# you code here\nsgd_reg = SGDRegressor()\nsgd_reg.fit(X_train_scaled, y_train)","65cbc29d":"# you code here\nplt.plot(range(len(sgd_reg.mse_)), sgd_reg.mse_)\nplt.xlabel('#updates')\nplt.ylabel('MSE');","de646563":"# you code here\nnp.min(sgd_reg.mse_), sgd_reg.w_","b62ba231":"# you code here\nplt.subplot(121)\nplt.plot(range(len(sgd_reg.weights_)), \n         [w[0] for w in sgd_reg.weights_]);\nplt.subplot(122)\nplt.plot(range(len(sgd_reg.weights_)), \n         [w[1] for w in sgd_reg.weights_]);","5260584c":"# you code here\nsgd_holdout_mse = mean_squared_error(y_valid, \n                                        sgd_reg.predict(X_valid_scaled))\nsgd_holdout_mse","220f01c8":"# you code here\nfrom sklearn.linear_model import LinearRegression\nlm = LinearRegression().fit(X_train_scaled, y_train)\nprint(lm.coef_, lm.intercept_)\nlinreg_holdout_mse = mean_squared_error(y_valid, \n                                        lm.predict(X_valid_scaled))\nlinreg_holdout_mse","dbf1d2c5":"try:\n    assert (sgd_holdout_mse - linreg_holdout_mse) < 1e-4\n    print('Correct!')\nexcept AssertionError:\n    print(\"Something's not good.\\n Linreg's holdout MSE: {}\"\n          \"\\n SGD's holdout MSE: {}\".format(linreg_holdout_mse, \n                                            sgd_holdout_mse))","1a0a5d4e":"Perform train\/test split and scale data.","35b94970":"Train created `SGDRegressor` with `(X_train_scaled, y_train)` data. Leave default parameter values for now.","e6380e95":"Print the minimal value of mean squared error and the best weights vector.","2e7d91a0":"Draw chart of model weights ($w_0$ and $w_1$) behavior during training.","ca99d72c":"Here we'll implement a regressor trained with stochastic gradient descent (SGD). Fill in the missing code. If you do evething right, you'll pass a simple embedded test.","457ff91f":"Draw a chart with training process  \u2013 dependency of mean squared error from the i-th SGD iteration number.","bd3c7139":"## <center>Linear regression and Stochastic Gradient Descent","6e3d620e":"Make a prediction for hold-out  set `(X_valid_scaled, y_valid)` and check MSE value.","eeaa1e5a":"Do the same thing for `LinearRegression` class from `sklearn.linear_model`. Evaluate MSE for hold-out set.","25bf6b8d":"Implement class `SGDRegressor`. Specification:\n- class is inherited from `sklearn.base.BaseEstimator`\n- constructor takes parameters `eta` \u2013 gradient step ($10^{-3}$ by default) and `n_epochs` \u2013 dataset pass count (3 by default)\n- constructor also creates `mse_` and `weights_` lists in order to track mean squared error and weight vector during gradient descent iterations\n- Class has `fit` and `predict` methods\n- The `fit` method takes matrix `X` and vector `y` (`numpy.array` objects) as parameters, appends column of ones to  `X` on the left side, initializes weight vector `w` with **zeros** and then makes `n_epochs` iterations of weight updates (you may refer to this [article](https:\/\/medium.com\/open-machine-learning-course\/open-machine-learning-course-topic-8-vowpal-wabbit-fast-learning-with-gigabytes-of-data-60f750086237) for details), and for every iteration logs mean squared error and weight vector `w` in corresponding lists we created in the constructor. \n- Additionally the `fit` method will create `w_` variable to store weights which produce minimal mean squared error\n- The `fit` method returns current instance of the `SGDRegressor` class, i.e. `self`\n- The `predict` method takes `X` matrix, adds column of ones to the left side and returns prediction vector, using weight vector `w_`, created by the `fit` method.","a8d290e2":"# <center> Assignment #8 (demo). Solution\n\n## <center> Implementation of online regressor","f4c6bf99":"Let's test out the algorithm on height\/weight data. We will predict heights (in inches) based on weights (in lbs).","f5c2abab":"<center>\n<img src=\"https:\/\/habrastorage.org\/files\/fd4\/502\/43d\/fd450243dd604b81b9713213a247aa20.jpg\" \/>\n    \n## [mlcourse.ai](https:\/\/mlcourse.ai) \u2013 Open Machine Learning Course \n\nAuthor: [Yury Kashnitskiy](https:\/\/yorko.github.io). Translated by [Sergey Oreshkov](https:\/\/www.linkedin.com\/in\/sergeoreshkov\/). This material is subject to the terms and conditions of the [Creative Commons CC BY-NC-SA 4.0](https:\/\/creativecommons.org\/licenses\/by-nc-sa\/4.0\/) license. Free use is permitted for any non-commercial purpose."}}