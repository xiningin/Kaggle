{"cell_type":{"b45fec7a":"code","556345a8":"code","09727634":"code","d50333c6":"code","6a2b7d6a":"code","4cc07d60":"code","9870d3ff":"code","0972c6fd":"code","223426fb":"code","7dba8d4d":"code","d178e016":"code","8c05606a":"code","6176a178":"code","e397531c":"code","3bf1b38b":"code","c8984147":"code","eab8cf8b":"code","99cc6e9d":"code","f9a67b45":"code","e16f7c1b":"code","a903d13d":"markdown","83798a25":"markdown","fb330b16":"markdown","df340db5":"markdown","3dd82635":"markdown","06a63106":"markdown"},"source":{"b45fec7a":"%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom torchvision.datasets import ImageFolder\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.transforms import transforms\nfrom torch.autograd import Variable\nfrom tqdm import tqdm_notebook\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom PIL import Image\nfrom glob import glob\nfrom random import choice","556345a8":"transform = transforms.Compose([\n    transforms.Resize([224, 224]),\n    transforms.ToTensor()\n])\n\ntrain_dataset = ImageFolder('..\/input\/fruits-360_dataset\/fruits-360\/Training\/', transform=transform)\ntest_dataset = ImageFolder('..\/input\/fruits-360_dataset\/fruits-360\/Test\/', transform=transform)","09727634":"transforms.ToPILImage()(train_dataset[0][0])","d50333c6":"n_classes = len(train_dataset.classes)","6a2b7d6a":"model = models.resnet18(pretrained=True)","4cc07d60":"n_filters = model.fc.in_features\nmodel.fc = nn.Linear(n_filters, n_classes)","9870d3ff":"criterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())","0972c6fd":"model.cuda()","223426fb":"train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)","7dba8d4d":"hist = {'loss': [], 'val_loss': [], 'val_acc': []}\nnum_epochs = 2\nfor epoch in range(num_epochs):\n    print('Starting epoch {}\/{}'.format(epoch+1, num_epochs))\n    # train\n    model.train()\n    running_loss = 0.0\n    for images, labels in train_loader:\n        images = Variable(images.cuda())\n        labels = Variable(labels.cuda())\n        \n        optimizer.zero_grad()\n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.data[0]\n    \n    train_loss = running_loss \/ len(train_loader)\n    \n    # evalute\n    model.eval()\n    val_running_loss = 0.0\n    correct = 0\n    for images, labels in test_loader:\n        images = Variable(images.cuda())\n        labels = Variable(labels.cuda())\n        \n        outputs = model(images)\n        loss = criterion(outputs, labels)\n        \n        val_running_loss += loss.data[0]\n        _, predicted = torch.max(outputs.data, 1)\n        correct += (predicted == labels.data).sum()\n    \n    \n    val_loss = val_running_loss \/ len(test_loader)\n    val_acc = correct \/ len(test_dataset)\n    \n    hist['loss'].append(train_loss)\n    hist['val_loss'].append(val_loss)\n    hist['val_acc'].append(val_acc)\n    \n    print('loss: {:.4f}  val_loss: {:.4f} val_acc: {:4.4f}\\n'.format(train_loss, val_loss, val_acc))","d178e016":"model.eval()\ny_test = []\ny_pred = []\nfor images, labels in test_loader:\n    images = Variable(images.cuda())\n    labels = Variable(labels.cuda())\n    outputs = model(images)\n    _, predictions = outputs.max(1)\n    \n    y_test.append(labels.data.cpu().numpy())\n    y_pred.append(predictions.data.cpu().numpy())\n    \ny_test = np.concatenate(y_test)\ny_pred = np.concatenate(y_pred)","8c05606a":"accuracy_score(y_test, y_pred)","6176a178":"sns.heatmap(confusion_matrix(y_test, y_pred))","e397531c":"idx_to_class = {idx: key for (key, idx) in train_dataset.class_to_idx.items()}","3bf1b38b":"fruits = glob('..\/input\/fruits-360_dataset\/fruits-360\/Test\/*\/*.jpg', recursive=True)","c8984147":"def what_fruit_is_this(fruit_path):\n    img = Image.open(fruit_path)\n    transform = transforms.Compose([\n        transforms.Resize([224, 224]),\n        transforms.ToTensor()\n    ])\n    x = Variable(transform(img)).cuda().unsqueeze(0)\n    output = model(x)\n    _, prediction = output.max(1)\n    prediction = prediction.data[0]\n    plt.imshow(img)\n    plt.axis('off')\n    plt.title('I think this fruit is a... {}!'.format(idx_to_class[prediction]))","eab8cf8b":"choice(fruits)","99cc6e9d":"what_fruit_is_this(choice(fruits))","f9a67b45":"what_fruit_is_this(choice(fruits))","e16f7c1b":"what_fruit_is_this(choice(fruits))","a903d13d":"From the `train_dataset` we can extract the number of classes:","83798a25":"The Test and and Training folder have the structure:\n\n    Test\/Nectarine\/62_100.jpg\n    Test\/Nectarine\/68_100.jpg\n    ...\n    Test\/Kaki\/254_100.jpg\n    Test\/Kaki\/265_100.jpg\n    ...\n    \n This is the way [ImageFolder](https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#imagefolder) expects, so we can quickly build the dataset:","fb330b16":"Now we can load the pretrained model and change the last layer. Notice that the last layer is linear, without softmax. The softmax operation is computed insided the loss function.","df340db5":"(224, 224) is the input shape used in the pretrained models. Here's a sample image:","3dd82635":"Let's train the model for 2 epochs only:","06a63106":"## Results"}}