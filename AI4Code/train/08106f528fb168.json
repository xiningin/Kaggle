{"cell_type":{"c35afff4":"code","8ecfdb84":"code","8249c98a":"code","2863ca97":"code","64840d83":"code","8d33c414":"code","dec7fa18":"code","f42b5f6f":"code","758fb3f9":"code","1b26013f":"code","21beeaf8":"code","a57e61d8":"code","f20c54bc":"code","d5dfcee3":"markdown","b4fa495d":"markdown","e6a78141":"markdown","5c7ae123":"markdown","1aa9a9db":"markdown","42eaaac3":"markdown","a144e61f":"markdown","abc6691e":"markdown","a560b16f":"markdown","e00facb9":"markdown","6dbad6c5":"markdown","f8dc1444":"markdown"},"source":{"c35afff4":"# Imports\nimport pandas as pd\nimport numpy as np\nfrom sklearn.decomposition import TruncatedSVD\nfrom sklearn.manifold import MDS\nfrom sklearn.manifold import Isomap\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy.stats import gaussian_kde\nfrom scipy import stats\nimport matplotlib.mlab as mlab\nfrom nltk.tokenize import word_tokenize\nimport powerlaw\nimport pylab\npylab.rcParams['xtick.major.pad']='8'\npylab.rcParams['ytick.major.pad']='8'\n#pylab.rcParams['font.sans-serif']='Arial'\n\nfrom matplotlib import rc\nrc('font', family='sans-serif')\nrc('font', size=10.0)\nrc('text', usetex=False)\n\n\nfrom matplotlib.font_manager import FontProperties\n\npanel_label_font = FontProperties().copy()\npanel_label_font.set_weight(\"bold\")\npanel_label_font.set_size(12.0)\npanel_label_font.set_family(\"sans-serif\")","8ecfdb84":"# Functions\n\ndef get_substring(s):\n    # This function takes a sentence as input and returns a tuple with the connective, \n    # the condition and both the connective and condition as a whole.\n    slice_connective = s.sentence_text[s.begin_connective:s.end_connective].strip()\n    slice_condition = s.sentence_text[s.begin_condition:s.end_condition].strip()\n    condition = s.sentence_text[s.begin_connective:s.end_condition].strip()\n    return pd.Series(dict(connective=slice_connective, condition=slice_condition, full_condition=condition))\n\ndef plot_box_whisker(values, labels, threshold, box_width):\n    # This function plots a box and whisker plot.\n    fig = plt.figure(1, figsize=(6,6))\n    ax = fig.add_subplot(111)\n    bp = ax.boxplot(values, patch_artist=True, widths=[box_width for i in range(len(values))])\n    ax.set_xticklabels(labels)\n\n    ## change outline color, fill color and linewidth of the boxes\n    for box in bp['boxes']:\n        # change outline color\n        box.set( color='#7570b3', linewidth=2)\n        # change fill color\n        box.set( facecolor = '#1b9e77')\n\n    ## change color and linewidth of the whiskers\n    for whisker in bp['whiskers']:\n        whisker.set(color='#7570b3', linewidth=2)\n\n    ## change color and linewidth of the caps\n    for cap in bp['caps']:\n        cap.set(color='#7570b3', linewidth=2)\n\n    ## change color and linewidth of the medians\n    for median in bp['medians']:\n        median.set(color='#b2df8a', linewidth=2)\n\n    ## change the style of fliers and their fill\n    for flier in bp['fliers']:\n        flier.set(marker='o', color='#e7298a', alpha=0.5)\n    ## Remove top axes and right axes ticks\n    ax.get_xaxis().tick_bottom()\n    ax.get_yaxis().tick_left()\n    ax.set_ylim(0,threshold * 1.4)\n    ax.axhline(y=threshold, color='r') \n    plt.show()\n    \ndef ccdf_plot(lang, data):\n    plt.figure()\n    fit = powerlaw.Fit(data, discrete=True)\n    ####\n    fit.distribution_compare('power_law', 'lognormal')\n    fig = fit.plot_ccdf(linewidth=3, label='Connectives')\n    fit.power_law.plot_ccdf(ax=fig, color='r', linestyle='--', label='Power law')\n    fit.lognormal.plot_ccdf(ax=fig, color='g', linestyle='--', label='Lognormal')\n    fit.exponential.plot_ccdf(ax=fig, color='b', linestyle='--', label='Exponential')\n    ####\n    fig.set_title('CCDF plot for '+lang)\n    fig.set_ylabel(u\"CCDF\")\n    fig.set_xlabel(\"Frequency\")\n    fig.set_ylim(0.01,1)\n    handles, labels = fig.get_legend_handles_labels()\n    fig.legend(handles, labels, loc=3)\n    \ndef likelihood_ratio_test(lang, results_fit, dist1, dist2):\n    R, p = results_fit.distribution_compare(dist1, dist2)\n    return [lang, dist1, dist2, R, p]\n\ndef scatter_with_gaussian(dim_red, lang, points):\n    x, y = points[:,0], points[:,1]\n    xy = np.vstack([x,y])\n    z = gaussian_kde(xy)(xy)\n    # Sort the points by density, so that the densest points are plotted last\n    idx = z.argsort()\n    x, y, z = x[idx], y[idx], z[idx]\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.set_title(dim_red + ' for '+lang)\n    cax = ax.scatter(x, y, c=z, s=30, edgecolor='')\n    fig.colorbar(cax)\n\n    plt.show()","8249c98a":"# load data\ndf_conds = pd.read_csv('..\/input\/conditions.csv')\ndf_sents = pd.read_csv('..\/input\/sentences.csv')","2863ca97":"# creating combinated dataframes\nconds_sents = pd.merge(df_conds, df_sents, on='sentence_uuid', how='left', suffixes=['','_sent'])\nconditions = conds_sents.apply(get_substring, axis=1)\nconds_sents['connective'] = conditions.connective\nconds_sents['condition'] = conditions.condition\nconds_sents['full_condition'] = conditions.full_condition\n# getting languages\nlanguages = df_sents['language'].unique().tolist()","64840d83":"# create the table of summary\nconds_sents.groupby(['language', 'domain']).connective.describe()","8d33c414":"conds_sents_count = conds_sents.copy()\nconds_sents_count['num_tokens'] = conds_sents_count.full_condition.apply(word_tokenize).str.len()\nnum_tokens_conds = []\nfor lang in languages:\n    num_tokens_conds.append(conds_sents_count[(conds_sents_count['language']==lang)].num_tokens)\nplot_box_whisker(num_tokens_conds, languages, 50, 0.5)","dec7fa18":"df_sents['num_tokens'] = df_sents[df_sents['labelled']].sentence_text.apply(word_tokenize).str.len()\nnum_tokens_per_lang = []\nfor lang in languages:\n    num_tokens_per_lang.append(df_sents[(df_sents['language']==lang) & (df_sents['labelled'])].num_tokens)\nplot_box_whisker(num_tokens_per_lang, languages, 100, 0.5)","f42b5f6f":"list_df_conn = []\nfor lang in languages:\n    df_conn = conds_sents[conds_sents['language'] == lang].copy()\n    df_conn['connective'] = df_conn.connective.str.lower()\n    df_conn = df_conn.groupby('connective').agg('count').reset_index()\n    df_conn = df_conn[['connective', 'condition']].sort_values(by='condition', ascending=False).reset_index(drop=True).copy()\n    df_conn.rename(index=str, columns={\"condition\" : \"freq\"}, inplace=True)\n    list_df_conn.append(df_conn)\n    conn_quarter = round(df_conn.shape[0]\/4)\n    print('Connectives for lang: '+lang)\n    print('Top 5')\n    print(df_conn.iloc[0:6])\n    print('Five around the 75-th percentile')\n    print(df_conn.iloc[conn_quarter-3:conn_quarter+3])","758fb3f9":"# plot of ccdf\nfor (df_conn, lang) in zip(list_df_conn, languages):\n    data = df_conn.freq.values\n    ccdf_plot(lang, data)","1b26013f":"df_powerlaw_rp = pd.DataFrame(columns = ['Lang', 'Dist1', 'Dist2', 'R', 'p-value'])\nfor (df_conn, lang) in zip(list_df_conn,languages):\n    results_fit = powerlaw.Fit(df_conn.freq.values, discrete=True)\n    df_powerlaw_rp.loc[len(df_powerlaw_rp)] = likelihood_ratio_test(lang, results_fit, 'power_law', 'lognormal')\n    df_powerlaw_rp.loc[len(df_powerlaw_rp)] = likelihood_ratio_test(lang, results_fit, 'power_law', 'exponential')\n    df_powerlaw_rp.loc[len(df_powerlaw_rp)] = likelihood_ratio_test(lang, results_fit, 'lognormal', 'exponential')    \n    \nprint(df_powerlaw_rp)","21beeaf8":"# number of words of the vectorisation\nprint('Number of words of the vectorisation')\nwords_per_lang = []\nfor lang in languages:\n    words = conds_sents.loc[conds_sents.language == lang].full_condition.str.lower().values.ravel()\n    vectorizer = TfidfVectorizer()\n    words = vectorizer.fit_transform(words)\n    words_per_lang.append(words)\n    print(lang,':',words.shape[1])","a57e61d8":"# tsvd\nfor (words, lang) in zip(words_per_lang, languages):\n    svd = TruncatedSVD(n_components=2,random_state=0)\n    points = svd.fit_transform(words)\n    scatter_with_gaussian('tsvd', lang, points)","f20c54bc":"# isomap\nfor (words, lang) in zip(words_per_lang, languages):\n    isomap = Isomap(n_neighbors=5, n_components=2)\n    points = isomap.fit_transform(words)\n    scatter_with_gaussian('isomap',lang, points)","d5dfcee3":"Then, we show the Isomap plots.","b4fa495d":"# Connective distribution\nNow, we analyse the connectives, that is,  a piece of text that usually introduces a condition. \n\nWe show the five most frequent connectives for each language and the five around the 75-th percentile. For each language, there are a few usual connectives that have high frequencies, whereas the others have low frequencies.","e6a78141":" # Introduction\nIn this kernel, we first describe our dataset of conditions, then analyse how conditional connectives are distributed, and, finally, analyse how similar conditions are.","5c7ae123":"## Global imports and functions\nFirst, we import all the necessary modules and ancillary functions to perform our dataset analysis.","1aa9a9db":"Next, we present a box and whisker plot that represents the number of words per condition and sentence in our dataset. We also highlighted two length for each plot in a red line. They represents sensible thresholds to conditions and sentences that can be used in a model that takes a fixed length of tokens as input. ","42eaaac3":"# Conclusions\nWe have presented a comprehensive analysis of our dataset that make it easier to understand it.","a144e61f":"We can observe that the conditions are organised as follows: there is one small group with high density, a larger group with average density, and a very large group with low density.","abc6691e":"# Condition similarity\nFinally, we analysed the similarity of the conditions in our dataset.\nWe changed every word into lowercase and then computed a vectorisation of each condition as follows: each component of the vectors corresponds to a different word and represents its tf-idf frequency in the condition being vectorised.","a560b16f":"# Description\nWe provide a summary of our dataset. The columns of the table denote the language (Lang), the domain (Domain), the number of conditions found (#Conds), the number of sentences (#Sents), the number of sentences that we have labelled as of the time of writing this article (#Lab), the number of sentences that contain at least one condition (#SwC), and the corresponding percentage (%SwC).","e00facb9":"From the previous plot we can intuitively think that the connective distribution is quite similar to the Log Normal distribution and the Power-law distribution. To check it statistically, we conducted the next Likelikhood Ratio Test.\nIndependently from the language, the comparison to the Power-Law distribution and the Log-Normal distribution returns p-values that are greater than the standard significance level \u03b1 = 0.05, which indicates that there is not enough empirical evidence to prove that the connective distribution is significantly different from a Power-Law or a Log-Normal distribution; note that the comparisons to the Power-Law and the Exponential distributions or the Log-Normal and the Exponential distribution return a positive log likelihood ratio with a p-value that is smaller than the standard significance level, which indicates that there is enough empirical evidence to prove that the connective distribution is similar to the Power-Law or the Log-Normal distributions, but different from the Exponential distribution.","6dbad6c5":"The previous tables make it intuitively clear that the distribution of connectives might be a long-tail distribution. To confirm it, it is necessary to compare it to the Power-Law and Log-Normal distributions, which are the standard long-tail distributions, and to the Exponential distribution, which is not long-tail by definition.\nNext, we plot the Complementary Cumulative Distribution Functions of the previous distributions. ","f8dc1444":"Now, we performed dimensionality reduction by means of Truncated Single Value Decomposition (TSVD) and Isomap. Furthermore, we computed the Gaussian Kernel Density Estimation to better visualise the density of samples with Scott's Rule to calculate the estimator bandwidth.\n\nFirst, we show the TSVD plots."}}