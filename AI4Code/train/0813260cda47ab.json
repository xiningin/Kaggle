{"cell_type":{"3e06cd15":"code","218e78e3":"code","a7919354":"code","2b2e1148":"code","eb84326a":"code","7016f27a":"code","159d13fc":"code","5bbbbc2a":"code","049fffcf":"code","ce30ab7a":"code","f125bfad":"code","04649926":"code","9574e6c3":"code","29dbbbec":"code","1c3403d6":"code","0fb1880b":"code","9bca7656":"code","78c4daa7":"code","6995e097":"code","e14c9207":"code","e3559673":"code","5d7a6512":"code","7e7fe58e":"code","b02024e8":"code","755810a9":"code","9ee1b92a":"code","2354d74a":"code","4ee4777c":"markdown","7f5aa0c2":"markdown","955fcd8d":"markdown","3aa59e2d":"markdown","91205e43":"markdown","385306e1":"markdown","c04ee461":"markdown","cd600ad6":"markdown","034dabdb":"markdown","1e287631":"markdown","01781dfe":"markdown","678009a5":"markdown","8ca11116":"markdown"},"source":{"3e06cd15":"import numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd \n","218e78e3":"train_data = pd.read_csv('..\/input\/quora-insincere-questions-classification\/train.csv')\ntest_data = pd.read_csv('..\/input\/quora-insincere-questions-classification\/test.csv')","a7919354":"train_data.head(10)","2b2e1148":"#test_data.head(10)","eb84326a":"print(\"Train shape : \", train_data.shape)\nprint(\"Test shape : \", test_data.shape)","7016f27a":"train_data.columns","159d13fc":"train_data= train_data.drop(['qid'], axis=1)\ntest_data= test_data.drop(['qid'], axis=1)","5bbbbc2a":"test_data.head(10)","049fffcf":"train_data.isnull().sum()","ce30ab7a":"test_data.isnull().sum()","f125bfad":"sns.countplot(train_data['target'])","04649926":"train_data['target'].value_counts()","9574e6c3":"sincere_percent= (len(train_data.question_text[train_data['target'] == 0]) \/  len(train_data['question_text']) * 100)\ninsincere_percent= (len(train_data.question_text[train_data['target'] == 1]) \/ len(train_data['question_text']) * 100)","29dbbbec":"print(sincere_percent, insincere_percent)","1c3403d6":"import matplotlib.pyplot as plt\n# Data to plot\nlabels = 'Sincere', 'Insincere'\nsizes = [sincere_percent, insincere_percent]\ncolors = ['lightskyblue', 'lightcoral']\nexplode = (0.1, 0)  # explode 1st slice\n\n# Plot\nplt.pie(sizes, explode=explode, labels=labels, colors=colors,\nautopct='%1.1f%%', shadow=True, startangle=140)\n\nplt.axis('equal')\nplt.show()","0fb1880b":"import nltk\nfrom wordcloud import WordCloud, STOPWORDS","9bca7656":"from collections import defaultdict\ntrain1_data = train_data[train_data[\"target\"]==1]\ntrain0_data = train_data[train_data[\"target\"]==0]","78c4daa7":"def cloud(text, title, size = (10,7)):\n    # Processing Text\n    wordcloud = WordCloud(width=800, height=400, background_color ='white',\n                          collocations=False\n                         ).generate(\" \".join(text))\n    \n    # Output Visualization\n    fig = plt.figure(figsize=size, dpi=80)\n    plt.imshow(wordcloud,interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title, fontsize=25,color='k')\n    plt.tight_layout(pad=0)\n    plt.show()\ncloud(train_data['question_text'], title=\"Word Cloud of Questions\")","6995e097":"cloud(train0_data[\"question_text\"], title=\"Word Cloud of sincere Questions\")","e14c9207":"cloud(train1_data[\"question_text\"], title=\"Word Cloud of insincere Questions\")","e3559673":"## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n","5d7a6512":"## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_data[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_data[\"question_text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]\n","7e7fe58e":"import seaborn as sns\nplt.figure(figsize=(11,10))\nplt.title(\"Frequent words of sincere question\")\nfd_sorted0_head= fd_sorted0.head(40)\nsns.barplot(x=fd_sorted0_head['wordcount'], y=fd_sorted0_head['word'])","b02024e8":"plt.figure(figsize=(11,10))\nplt.title(\"Frequent words of insincere question\")\nfd_sorted1_head= fd_sorted1.head(40)\nsns.barplot(x=fd_sorted1_head['wordcount'], y=fd_sorted1_head['word'])","755810a9":"freq_dict = defaultdict(int)\nfor sent in train0_data[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted0 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted0.columns = [\"word\", \"wordcount\"]\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_data[\"question_text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted1 = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted1.columns = [\"word\", \"wordcount\"]","9ee1b92a":"import seaborn as sns\nplt.figure(figsize=(11,10))\nplt.title(\"Frequent words of sincere question\")\nfd_sorted0_head= fd_sorted0.head(40)\nsns.barplot(x=fd_sorted0_head['wordcount'], y=fd_sorted0_head['word'])","2354d74a":"import seaborn as sns\nplt.figure(figsize=(11,10))\nplt.title(\"Frequent words of insincere question\")\nfd_sorted1_head= fd_sorted1.head(40)\nsns.barplot(x=fd_sorted1_head['wordcount'], y=fd_sorted1_head['word'])","4ee4777c":"## Target Count:","7f5aa0c2":"## Evaluation Metrics :\nMetric is F1 Score between the predicted and the observed targets. There are just two classes, but the positive class makes just over 6% of the total. So the target is highly imbalanced, which is why a metric such as F1 seems appropriate for this kind of problem as it considers both precision and recall of the test to compute the score.\n","955fcd8d":"## Unigram model:","3aa59e2d":"## Build language model:","91205e43":"## Load the data from CSV files into a pandas dataframe","385306e1":"# 2. Data Visualization:","c04ee461":"## Problem statement:\n  Build a model to classify whether a question asked on Quora is sincere or not.\n\nTo help Quora uphold their policy of \u201cBe Nice, Be Respectful\u201d and continue to be a place for sharing and growing the world\u2019s knowledge.","cd600ad6":"## Bigram model:","034dabdb":"# Quora Insincere Questions Classification\n[Quora](https:\/\/www.quora.com\/) is a platform that empowers people to learn from each other. On Quora, people can ask questions and connect with others who contribute unique insights and quality answers. A key challenge is to weed out insincere questions -- those founded upon false premises, or that intend to make a statement rather than look for helpful answers.\n[Competition: Quora Insincere Questions Classification](https:\/\/https:\/\/www.kaggle.com\/c\/quora-insincere-questions-classification)","1e287631":"# 1. Data loading and exploration:","01781dfe":"## Target distribution:\n","678009a5":"## Overview of the data:\n\nQuora provided a good amount of training and test data to identify the insincere questions. Train data consists of 1.3 million rows and 3 features in it. And the Test data consists of 300K rows and 2 features. ","8ca11116":"## Word Frequency plot of sincere & insincere questions:\nLet us look at the frequently occuring words in the data by creating a word cloud on the 'question_text' column."}}