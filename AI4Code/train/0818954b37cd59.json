{"cell_type":{"08de02e6":"code","77045264":"code","8cf512e1":"code","475bcd3d":"code","69a78d04":"code","8aaa2602":"code","14f06189":"code","a7c503ef":"code","5a10db5a":"code","5b289dbc":"code","613c8cb1":"code","307ddccf":"code","54dd0159":"code","38327f4a":"code","162d6649":"code","9eedef39":"code","3315228e":"code","33f13867":"code","75f6c44e":"code","ccdc9fa8":"code","d50f32cb":"code","0ef48182":"code","3bfe0676":"code","9a3a46cf":"code","29029358":"code","1ea5afb0":"code","c976f615":"code","5303a8f1":"code","83aed658":"code","6888846a":"code","8b5e237a":"code","23243ede":"code","311a2c7e":"code","0ad3701e":"code","46b8184e":"code","c55e6241":"code","8e48b5d1":"code","af1881a4":"code","55f2e09e":"code","1a0a5237":"code","6b62881c":"code","22393c4e":"code","2fd288d4":"code","eec90560":"code","779325ea":"code","aabad2b2":"code","898619e5":"code","41955263":"code","5a112863":"code","382d4a8d":"code","9450387b":"code","6f41b4d7":"code","d8e144ae":"code","33cd08dc":"code","337c73c3":"code","f8bb01f8":"code","ef9d89ae":"code","de7edf1e":"code","4391b0cd":"code","063559f3":"code","c3fe33e5":"code","e2bd552e":"code","20550080":"code","ba60263a":"code","0cdfbe9a":"code","cfcc8156":"code","ecc36c70":"code","9e890194":"code","08c7686c":"code","60d9720c":"code","bdb43e35":"code","9c2c790f":"code","1cad49d2":"code","76a7a2dd":"code","3284bac9":"code","12b714a3":"code","7bc4f05a":"code","e27d4450":"code","94ad6ef2":"code","957677a9":"code","60f08910":"code","3f663627":"code","22238602":"code","9a1585d6":"code","8b05824d":"code","ca6e44e7":"code","3012171b":"markdown","9e6cc9ac":"markdown"},"source":{"08de02e6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","77045264":"dataset = pd.read_csv(\"..\/input\/train.csv\")","8cf512e1":"dataset.shape","475bcd3d":"dataset.drop('Id',axis=1,inplace=True)","69a78d04":"# Chi-Square Test for categorical variables. T-Test Z-test Annova","8aaa2602":"import seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.figure(figsize=(15,15))\ncolcorrdataset = dataset.drop('SalePrice',axis=1)\nsns.heatmap(dataset.drop('SalePrice',axis=1).corr(),cmap='coolwarm',annot=True)","14f06189":"c = colcorrdataset.corr().abs()\ns = c.unstack()\nso = s.sort_values(kind=\"quicksort\")","a7c503ef":"crit1 = so > 0.8\ncrit2 = so < 1\ncrit = crit1 & crit2\nso[crit]","5a10db5a":"datasetcorr = dataset.corr()\ncrit1 = datasetcorr['SalePrice'] > 0.5\ncrit2 = datasetcorr['SalePrice'] < -0.5\ncrit = crit1 | crit2\ndatasetcorr[crit]['SalePrice']","5b289dbc":"# Based on the above corr below continous columns are the important \ncolumn_names = ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','FullBath','GarageArea','SalePrice']","613c8cb1":"contdataset = dataset[column_names]","307ddccf":"catdataset = dataset.select_dtypes(include=np.object)","54dd0159":"catdataset.loc[catdataset['MSZoning'] == 'C (all)','MSZoning'] = 'C' # Chaning the C (all) -> C as thats more comfirtable for any processing","38327f4a":"# Prints and displays \ndef chkcatcolsdet(ds):\n    nullcatcols = []\n    for col in ds.columns:\n            print(\"%s count:%d, nullvalues: %s, values: %s \"%(col,len(ds[col].unique()),ds[col].isnull().sum(),ds[col].unique()))\n            if ds[col].isnull().sum(): nullcatcols.append(col) \n    return nullcatcols\n\ndef repmincntvals(ds,nullcatcols):\n    for col in nullcatcols:\n        columnval = pd.DataFrame(ds.groupby([col])[col].count()).transpose().min().index[0]\n        ds.groupby(by=col)[col].count()\n        ds.loc[ds[col].isnull(),col] = columnval\n        ds.groupby(by=col)[col].count()","162d6649":"nullcatcols1 = chkcatcolsdet(catdataset)\nrepmincntvals(catdataset,nullcatcols1)\nchkcatcolsdet(catdataset) ","9eedef39":"catdataset.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","3315228e":"from sklearn.preprocessing import LabelEncoder\nlabelencoder = LabelEncoder()\ndef labelencoderdataset(ds,catcols):\n    labencdataset = pd.DataFrame()\n    for i in range(0,len(catcols)):\n        if (int(ds[str(catcols[i])].isna().sum())):\n            next\n        else:\n            print (\"label encoding column : %s\"%str(catcols[i]))\n            labencdataset[catcols[i]] = labelencoder.fit_transform(ds[catcols[i]])\n    return labencdataset","33f13867":"catlabencdataset = labelencoderdataset(catdataset,catdataset.columns)","75f6c44e":"# Lets Scale the Continous data\ncontdataset1 = contdataset","ccdc9fa8":"# ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','FullBath','GarageArea','SalePrice']\n# - Lets Bucketize these columns later as they are years \n# Discrete columns : OverallQual,FullBath,TotRmsAbvGrd,GarageCars,YearBuilt,YearRemodAdd\n#Continous Variables : 'TotalBsmtSF', '1stFlrSF','GrLivArea','GarageArea'\n\ncontcontdataset = contdataset[['TotalBsmtSF', 'GrLivArea','GarageArea']]\ncontdiscdataset = contdataset[['OverallQual','FullBath']]\ncontbuckdataset = contdataset[['YearBuilt','YearRemodAdd']]\n\nyear_ranges = [\"[{0} - {1})\".format(year, year + 10) for year in range(1870, 2010, 10)]\n\ncontbuckdataset['YearBuiltrange'] = pd.cut(contbuckdataset['YearBuilt'],labels=year_ranges,bins=len(year_ranges))\ncontbuckdataset['YearRemodAddrange'] = pd.cut(contbuckdataset['YearRemodAdd'],labels=year_ranges,bins=len(year_ranges))","d50f32cb":"contbuckdataset.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)","0ef48182":"contbucklabdataset = labelencoderdataset(contbuckdataset,contbuckdataset.columns)","3bfe0676":"nullcatcols1 = chkcatcolsdet(contdiscdataset)","9a3a46cf":"labeldataset = pd.DataFrame(contdataset['SalePrice'])","29029358":"contdataset1.drop(['SalePrice'],axis=1,inplace=True)","1ea5afb0":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ncontcontdataset2 = pd.DataFrame(scaler.fit_transform(contcontdataset),columns=contcontdataset.columns)","c976f615":"finaldataset = pd.concat([contcontdataset2,contdiscdataset,contbucklabdataset,catlabencdataset,labeldataset],axis=1)","5303a8f1":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.model_selection import train_test_split","83aed658":"# Lets see what are the important features based on RadomForestClassifier","6888846a":"X = finaldataset.drop(['SalePrice'],axis=1)\n#Y = finaldataset['SalePrice'] # np.log(finaldataset['SalePrice']) - lets try the skewed data later\nY = np.log(finaldataset['SalePrice']) # Metric to calculate is log of the price","8b5e237a":"x_train, x_test, y_train, y_test = train_test_split(X,Y,test_size=0.33, random_state=101)","23243ede":"from sklearn.decomposition import PCA","311a2c7e":"pca = PCA(.8)","0ad3701e":"X_pca = pca.fit_transform(X)","46b8184e":"X_pca.shape","c55e6241":"pca.explained_variance_ratio_","8e48b5d1":"X_pca.shape,Y.shape","af1881a4":"x_train,x_test,y_train,y_test = train_test_split(X_pca,Y,test_size=0.1)","55f2e09e":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\n\nparam_dist = {\n 'n_estimators': [50,100,150,200,250,350,450,550,650,750,1000],\n 'learning_rate' : [0.001,0.01,0.05,0.1,0.3,1],\n 'loss' : ['linear', 'square', 'exponential']\n }\nAdaBoostRgr = RandomizedSearchCV(AdaBoostRegressor(DecisionTreeRegressor(max_depth=50)),\n param_distributions = param_dist,\n cv=4,\n n_iter = 50,\n n_jobs=-1)\n\nAdaBoostRgr.fit(x_train,y_train)","1a0a5237":"# Create the dataset\nAdaBoostfinalrgr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=50),\n                          n_estimators=AdaBoostRgr.best_params_['n_estimators'], random_state=3,learning_rate=AdaBoostRgr.best_params_['learning_rate'],loss='square')","6b62881c":"AdaBoostfinalrgr.fit(x_train,y_train)","22393c4e":"y_pred_adaboost = AdaBoostfinalrgr.predict(x_test)","2fd288d4":"mean_squared_error(y_test,y_pred_adaboost)**0.5","eec90560":"# Lets Try calculating zscore and figure outliers","779325ea":"X = finaldataset.drop(['SalePrice'],axis=1)\n#Y = finaldataset['SalePrice'] # np.log(finaldataset['SalePrice']) - lets try the skewed data later\nY = np.log(finaldataset['SalePrice']) # Metric to calculate is log of the price","aabad2b2":"# Lets Scale the data and then apply zscore to figure out outliers\nfrom sklearn.preprocessing import StandardScaler\nstdscl = StandardScaler()\nX_scale = stdscl.fit_transform(X)\n\nfrom scipy import stats\nimport numpy as np\nz = np.abs(stats.zscore(X_scale))\nprint(z)","898619e5":"threshold = 10\nprint(np.where(z > threshold))","41955263":"X_scale_ds = pd.DataFrame(X_scale)\nX_scale_ds.drop(np.where(z > threshold)[0],inplace = True)","5a112863":"Y.drop(np.where(z > threshold)[0],inplace = True)","382d4a8d":"#X = finaldataset.drop(['SalePrice'],axis=1)\n#Y = np.log(labeldataset)","9450387b":"x_train,x_test,y_train,y_test = train_test_split(X_scale_ds,Y,test_size=0.2,random_state=7)","6f41b4d7":"from sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import RandomizedSearchCV\nparam_dist = {\n 'n_estimators': [50,100,150,200,250,350,450,550,650,750,1000],\n 'learning_rate' : [0.01,0.02,0.05,0.07,0.1,0.3,0.5,0.7,1],\n 'loss' : ['linear', 'square', 'exponential']\n }\nAdaBoostRgr = RandomizedSearchCV(AdaBoostRegressor(DecisionTreeRegressor(max_depth=100)),\n param_distributions = param_dist,\n cv=4,\n n_iter = 25,\n n_jobs=-1)\nAdaBoostRgr.fit(x_train,y_train)","d8e144ae":"# Create the dataset\nAdaBoostfinalrgr = AdaBoostRegressor(DecisionTreeRegressor(max_depth=100),\n                          n_estimators=AdaBoostRgr.best_params_['n_estimators'], random_state=33,learning_rate=AdaBoostRgr.best_params_['learning_rate'],loss='square')","33cd08dc":"AdaBoostfinalrgr.fit(x_train,y_train)","337c73c3":"y_pred_adaboost_o = AdaBoostfinalrgr.predict(x_test)","f8bb01f8":"mean_squared_error(y_test,y_pred_adaboost_o)**0.5","ef9d89ae":"y_pred_adaboost_o[:6]","de7edf1e":"from xgboost import XGBRegressor","4391b0cd":"param_dist = {\n 'n_estimators': [50,100,150,200,250,350,450,550,650,750,1000],\n 'learning_rate' : [0.5,0.6,0.7,0.8,0.9,1]\n }\nXGBRgrs = RandomizedSearchCV(XGBRegressor(max_depth=100),param_dist)","063559f3":"XGBRgrs.fit(x_train,y_train)","c3fe33e5":"xgb = XGBRegressor(max_depth=100,n_estimators=XGBRgrs.best_params_['n_estimators'],learning_rate=XGBRgrs.best_params_['learning_rate'])","e2bd552e":"xgb.fit(x_train,y_train)","20550080":"xgb_pred = xgb.predict(x_test)","ba60263a":"mean_squared_error(y_test,xgb_pred)**0.5","0cdfbe9a":"testdataset = pd.read_csv('..\/input\/test.csv')","cfcc8156":"column_names = ['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','GrLivArea','FullBath','GarageArea']\nresultid = testdataset['Id']\ntestdataset.drop(['Id'],axis=1,inplace=True)\nconttestdataset = testdataset[column_names]\ncattestdataset = testdataset.select_dtypes(include=np.object)\ncattestdataset.loc[cattestdataset['MSZoning'] == 'C (all)','MSZoning'] = 'C'\nnullcatcols1 = chkcatcolsdet(cattestdataset)\nrepmincntvals(cattestdataset,nullcatcols1)\nnullcatcols1 = chkcatcolsdet(cattestdataset)\ncattestdataset.drop(['Alley','FireplaceQu','PoolQC','Fence','MiscFeature'],axis=1,inplace=True)","ecc36c70":"catlabenctestdataset = labelencoderdataset(cattestdataset,cattestdataset.columns)","9e890194":"contconttestdataset = conttestdataset[['TotalBsmtSF', 'GrLivArea','GarageArea']]\ncontdisctestdataset = conttestdataset[['OverallQual','FullBath']]\ncontbucktestdataset = conttestdataset[['YearBuilt','YearRemodAdd']]","08c7686c":"nullcatcols3 = chkcatcolsdet(contdisctestdataset)\nrepmincntvals(contdisctestdataset,nullcatcols3)","60d9720c":"# Test Dataset has GarageArea\/TotalBsmtSF as NULL values - so Lets replace with mean values.\nTotalBsmtSF_array = contconttestdataset[contconttestdataset[\"TotalBsmtSF\"]!=np.nan][\"TotalBsmtSF\"]\ncontconttestdataset[\"TotalBsmtSF\"].replace(np.nan,TotalBsmtSF_array.mean())\nGarageArea_array = contconttestdataset[contconttestdataset[\"GarageArea\"]!=np.nan][\"GarageArea\"]\ncontconttestdataset[\"GarageArea\"].replace(np.nan,GarageArea_array.mean())","bdb43e35":"# Replace 660 column value with the mean value of TotalBDmtSF\ncontconttestdataset.loc[660,'TotalBsmtSF'] = np.mean(contconttestdataset['TotalBsmtSF'])","9c2c790f":"contbucktestdataset['YearBuiltrange'] = pd.cut(contbucktestdataset['YearBuilt'],labels=year_ranges,bins=len(year_ranges))\ncontbucktestdataset['YearRemodAddrange'] = pd.cut(contbucktestdataset['YearRemodAdd'],labels=year_ranges,bins=len(year_ranges))\ncontbucktestdataset.drop(['YearBuilt','YearRemodAdd'],axis=1,inplace=True)\ncontbucklabtestdataset = labelencoderdataset(contbucktestdataset,contbucktestdataset.columns)","1cad49d2":"contconttestdataset2 = pd.DataFrame(scaler.fit_transform(contconttestdataset),columns=contconttestdataset.columns)","76a7a2dd":"contconttestdataset2[contconttestdataset2['GarageArea'].isnull()]\ncontconttestdataset2.loc[1116,'GarageArea'] = np.mean(contconttestdataset2['GarageArea'])","3284bac9":"finaltestdataset = pd.concat([contconttestdataset2,contdisctestdataset,contbucklabtestdataset,catlabenctestdataset],axis=1)","12b714a3":"testcontdataset = finaltestdataset[['TotalBsmtSF', 'GrLivArea', 'GarageArea', 'OverallQual']]","7bc4f05a":"testcatdataset = finaltestdataset[catdataset.columns]","e27d4450":"finaltestdataset.columns","94ad6ef2":"finaldataset.columns","957677a9":"y_adaboost_csv = AdaBoostRgr.predict(stdscl.fit_transform(finaltestdataset))","60f08910":"Y_test_csv = np.exp(y_adaboost_csv)","3f663627":"finalpredresult = pd.concat([resultid,pd.DataFrame(Y_test_csv,columns=['SalePrice'])],axis=1,names=['Id','SalePrice'])","22238602":"finalpredresult","9a1585d6":"x_train.shape","8b05824d":"finalpredresult.to_csv('adaboost_zscore_standscale.csv', index=False)","ca6e44e7":"''' DNN Can be tried in another Kernel but i couldnt get more accuracy than Adaboost as above, hence commenting it for now\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom keras import regularizers\n# import noise layer\nfrom keras.layers import GaussianNoise\n\nX = finaldataset.drop(['SalePrice'],axis=1)\nY = np.log(labeldataset)\n\nimport seaborn as sns\nvards = pd.DataFrame(X.var())\n\nvards.plot(kind='hist')\n\nfrom sklearn.decomposition import PCA\npca = PCA(0.95)\nx_pca = pca.fit_transform(X)\npca.explained_variance_ratio_\n\nx_train,x_test,y_train,y_test = train_test_split(pd.DataFrame(x_pca),Y,test_size=0.1)\n\nfrom sklearn.preprocessing import StandardScaler\nstdscale = StandardScaler()\nx_train_scale = stdscale.fit_transform(x_train)\nx_test_scale = stdscale.fit_transform(x_test)\n\ndef dnnmodel():\n    model = Sequential()\n    model.add(Dense(256,input_shape=(45,),activation=\"relu\"))\n    model.add(GaussianNoise(0.1))\n    model.add(Dense(256,activation=\"relu\"))\n    model.add(Dense(256,activation=\"relu\"))\n    model.add(Dense(256,activation=\"relu\"))\n    model.add(Dense(256,activation=\"relu\"))\n    model.add(Dense(1))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam',metrics=['acc','mean_squared_error'])\n    return model\n\nmodel = dnnmodel()\nmodel.summary()\n\n# simple early stopping\n#es = EarlyStopping(monitor='val_loss', mode='auto', verbose=1,patience=200)\n# model fit\nhistory = model.fit(x_train,y_train,epochs=100,batch_size=30,validation_split=0.1)\n\nimport matplotlib.pyplot as plt\n# Plot training & validation accuracy values\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Test'], loc='upper left')\nplt.show()\n\ny_test_dnn = model.predict(x_test)\nmean_squared_error(y_test,y_test_dnn)**0.5\n(np.exp(y_test_dnn[:5]),np.exp(y_test[:5]))'''","3012171b":"### Based on above correlation - Either 1 of below is enough in our data\nTotalBsmtSF\/1stFlrSF \nGrLivArea\/TotRmsAbvGrd  \nYearBuilt\/GarageYrBlt \nGarageCars\/GarageArea ","9e6cc9ac":"##### XGBoost"}}