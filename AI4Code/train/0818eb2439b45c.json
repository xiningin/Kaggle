{"cell_type":{"58046390":"code","7e2789bb":"code","95f7dd54":"code","f930adc8":"code","3d17faa9":"code","c6d66b6b":"code","68cf0770":"code","7ca0a3f9":"code","755f06b5":"code","fde3ef2c":"code","e84538c1":"code","f25b335a":"code","94cfd7e3":"code","839d203d":"code","c2c04eaa":"code","ebce47a3":"code","3ad2b8e1":"code","87fb4c3f":"code","65d2743c":"code","fa5bbddd":"code","19dfd38a":"code","c24deb7d":"code","d0256c60":"code","c42f8042":"code","b87009de":"code","2fa0025e":"markdown","b4575bd3":"markdown","2df4c699":"markdown","d959c2e4":"markdown","d332fd70":"markdown","155ab54a":"markdown","9a875985":"markdown","49183128":"markdown","2210da3c":"markdown","280bde81":"markdown","426ff41b":"markdown"},"source":{"58046390":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# NLP\nimport re\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\n# Preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\n# Modeling \nfrom sklearn.svm import SVC\nfrom sklearn.metrics import f1_score\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7e2789bb":"data = pd.read_csv('..\/input\/sms-spam-collection-dataset\/spam.csv', encoding = 'latin')","95f7dd54":"data","f930adc8":"data.drop([data.columns[col] for col in [2, 3, 4]], axis = 1, inplace = True)","3d17faa9":"data","c6d66b6b":"encoder = LabelEncoder()\ndata['v1'] = encoder.fit_transform(data['v1'])\nclass_mapping = {index : label for index, label in enumerate(encoder.classes_)}","68cf0770":"class_mapping","7ca0a3f9":"data","755f06b5":"def process_mail(mail):\n    ps = PorterStemmer()\n    \n    mail = mail.lower()\n    mail = re.sub(r'<[^<>]+>', ' ', mail)\n    mail = re.sub(r'[0-9]+', 'number', mail)\n    mail = re.sub(r'(http|https):\/\/[^\\s]*', 'httpaddr', mail)\n    mail = re.sub(r'[^\\s]+@[^\\s]+', 'emailaddr', mail)\n    mail = re.sub(r'[$]+', 'dollar', mail)\n    \n    words = word_tokenize(mail)\n\n    for i in range(len(words)):\n        words[i] = re.sub(r'[^a-zA-Z0-9]', '', words[i])\n        words[i] = ps.stem(words[i])\n        \n    words = [word for word in words if len(word) >= 1]\n    \n    return words","fde3ef2c":"def getVocabulary(emails, vocab_length):\n    vocabulary = dict()\n    \n    for i in range(len(emails)):\n        emails[i] = process_mail(emails[i])\n        for word in emails[i]:\n            if word in vocabulary.keys():\n                vocabulary[word] += 1\n            else:\n                vocabulary[word] = 1\n                \n    vocabulary = sorted(vocabulary.items(), key = lambda x : x[1], reverse = True)\n    vocabulary = list(map(lambda x : x[0], vocabulary[0: vocab_length]))\n    vocabulary = {index : word for index, word in enumerate(vocabulary)}\n    \n    return vocabulary","e84538c1":"def getKey(dictionary, val):\n    for key, value in dictionary.items():\n        if value == val:\n            return key","f25b335a":"def getIndices(email, vocabulary):\n    word_indices = set()\n    \n    for word in email:\n        if word in vocabulary.values():\n            word_indices.add(getKey(vocabulary, word))\n            \n    return word_indices","94cfd7e3":"def getFeatureVector(word_indices, vocab_length):\n    feature_vec = np.zeros(vocab_length)\n    \n    for i in word_indices:\n        feature_vec[i] = 1\n        \n    return feature_vec","839d203d":"vocab_length = 2000","c2c04eaa":"vocabulary = getVocabulary(data['v2'].to_list(), vocab_length)\n\nemails = data['v2'].to_list()\nemails = list(map(lambda x : process_mail(x), emails))","ebce47a3":"X = list(map(lambda x : getFeatureVector(getIndices(x, vocabulary), vocab_length), emails))\nX = pd.DataFrame(np.array(X).astype(np.int16))","3ad2b8e1":"X","87fb4c3f":"y = data['v1']","65d2743c":"y","fa5bbddd":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.8)","19dfd38a":"model = SVC()\nmodel.fit(X_train, y_train)","c24deb7d":"model.score(X_test, y_test)","d0256c60":"np.sum(y)\/len(y)","c42f8042":"y_pred = model.predict(X_test)","b87009de":"f1_score(y_test, y_pred)","2fa0025e":"# Importing Libraries \ud83d\udcd6","b4575bd3":"# Loading Data set \ud83d\udcbe","2df4c699":"### Accuracy","d959c2e4":"### Storing vocabularies","d332fd70":"# Pre Processing \u2b55","155ab54a":"### F1 score","9a875985":"### Stemming of words","49183128":"### Percentage of +ve results in dataset","2210da3c":"# Model Training \ud83d\udee0\ufe0f","280bde81":"# Scores \ud83d\udcc8","426ff41b":"### Setting length of vocabulary list "}}