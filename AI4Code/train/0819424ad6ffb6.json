{"cell_type":{"00012720":"code","f5e72947":"code","9499eee0":"code","8758aeaa":"code","f0a62e77":"code","d67d6b6f":"code","85b393f6":"code","04cbf375":"code","4f4193ac":"code","73096637":"code","168d555f":"code","386daf3b":"code","af2587c5":"code","cccdeb69":"code","4ad0f997":"code","da147375":"code","3ee88167":"code","80a3dff9":"code","e488965e":"code","3d5120e1":"code","707db3e6":"code","427e15cc":"code","26ba1877":"code","42911e13":"markdown","e30762d0":"markdown","f982e75c":"markdown","79a8f079":"markdown","8fef9845":"markdown","856ce90f":"markdown","c12eddb2":"markdown","00d016bb":"markdown","8cd3f713":"markdown","3e68f5a2":"markdown","341e3e60":"markdown","f9383506":"markdown","f5d054a4":"markdown","d7055aea":"markdown","4740f543":"markdown","527f4dc8":"markdown","3aef20eb":"markdown","47ceb695":"markdown","04236b61":"markdown","79d06d2d":"markdown","d63fbd3a":"markdown","b269c56a":"markdown"},"source":{"00012720":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils.np_utils import to_categorical\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Import the data from BrainStation\nmnist = np.genfromtxt('..\/input\/mnist_data.csv', delimiter=\",\")\nprint('Shape: ', mnist.shape)","f5e72947":"# feature \/\/ target\nX = mnist[:, :-1]\ny = mnist[:, -1]\nprint('X shape: {}\\ny shape: {}'.format(X.shape,y.shape))\n\n\n#########################   Comment this line to run the full dataset   ##########################\nX, X_holdout, y, y_holdout = train_test_split(X, y, test_size=0.96, stratify = y) ################\n##################################################################################################\n\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, stratify = y)\n\n# the images are in square form, so dim*dim = 784\nfrom math import sqrt\ndim = int(sqrt(X_train.shape[1]))\nprint('The images are {}x{} squares.'.format(dim, dim))\nk = sns.countplot(mnist[:, -1], color = 'Cyan')","9499eee0":"plt.figure(figsize=(15,4.5))\nfor i in range(69):  \n    plt.subplot(8, 10, i+1)\n    plt.imshow(X[i].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\nplt.subplots_adjust(wspace=-0.1, hspace=-0.1)\nplt.show()","8758aeaa":"plt.figure(figsize=(5,5))\nplt.imshow(X[9].reshape((28,28)),cmap=plt.cm.binary)","f0a62e77":"m = X.shape[0]\nn = X.shape[1]\nlabels = np.unique(y)\nlabels_count = labels.shape[0]\n\n# Creating and plotting average digits\naverage_digits = np.empty((0, n+1))\n\nplt.figure(figsize=(15,11))\nplt.gray()\n\nfor label in labels:\n    digits = X[y.flatten() == label]\n    average_digit = digits.mean(0)   \n    average_digits = np.vstack((average_digits, np.append(average_digit, label)))\n    image = average_digit.reshape(28, 28)\n    plt.subplot(8,10,label+1)\n    plt.imshow(image)\n    plt.title('Average '+str(label))\nplt.show()\n\naverage_digits_x = average_digits[:,:-1]\naverage_digits_y = average_digits[:,-1]","d67d6b6f":"from sklearn.linear_model import LogisticRegression\n\n# Create an instance of our model\nmy_logreg = LogisticRegression(solver = 'lbfgs')\n\nstart = time.time()\n\n# Fit the model to data\nmy_logreg.fit(X_train,y_train)\nprint(my_logreg.score(X_test, y_test), 'Accuracy Score')\nend = time.time()\nprint(end - start, ' Seconds to fit the logistic regression')\n# X_train.shape","85b393f6":"from sklearn.decomposition import PCA\n\ndef run_pca(n_components):\n    start = time.time()\n    \n    #Build and fit a PCA model to the data\n    my_pca = PCA(n_components=n_components)\n    my_pca.fit(X)\n    \n    #Transform the data\n    X_PCA = my_pca.transform(X)\n\n    # Split into training & test sets\n    PCA_X_train, PCA_X_test, PCA_y_train, PCA_y_test = train_test_split(X_PCA, y, test_size=0.4, stratify = y)\n\n    # Create an instance of our model\n    my_PCA_logreg = LogisticRegression(solver = 'lbfgs')\n    \n    # Fit the model to data\n    my_PCA_logreg.fit(PCA_X_train,PCA_y_train)\n\n    score = my_PCA_logreg.score(PCA_X_test, PCA_y_test)\n    end = time.time()\n    iter_time = end - start\n    return (score, iter_time)","04cbf375":"scores = []\ntimes = []\nk = []\n\nfor i in range(2,34):\n    score, iter_time = run_pca(i)\n    scores.append(score), times.append(iter_time), k.append(i)\n\n# print('times:',times,'\\n','-'*70,'\\n','scores:', scores,'\\n','-'*70,'\\n','k:', k)","4f4193ac":"plt.plot(k, scores, label='Score')\nplt.plot(k, times, label='Iteration Time')\nplt.legend()\nplt.show()","73096637":"from sklearn.neighbors import KNeighborsClassifier\n\n# Instantiate the model & fit it to our data\nstart = time.time()\n\nKNN_model_1 = KNeighborsClassifier(n_neighbors=1)\nKNN_model_1.fit(X_train,y_train)\nprint(KNN_model_1.score(X_train,y_train), ' Train Score KNN(n_neighbors=1)')\nprint(KNN_model_1.score(X_test,y_test), ' Test Score KNN(n_neighbors=1)')\nend = time.time()\nprint(end - start, ' Seconds to fit the KNN(n_neighbors=1)\\n')\n\nstart = time.time()\n\nKNN_model_n = KNeighborsClassifier(n_neighbors=len(X_train))\nKNN_model_n.fit(X_train,y_train)\nprint(KNN_model_n.score(X_train,y_train), ' Train Score KNN(n_neighbors={})'.format(len(X_train)))\nprint(KNN_model_n.score(X_test,y_test), ' Test Score KNN(n_neighbors={})'.format(len(X_train)))\nend = time.time()\nprint(end - start, ' Seconds to fit the KNN(n_neighbors={})'.format(len(X_train)))","168d555f":"test_scores = []\ntrain_scores = []\nK = []\nstart = time.time()\nfor i in range(1,70):\n    KNN_model = KNeighborsClassifier(n_neighbors = i)\n    KNN_model.fit(X_train, y_train)\n    train_scores.append(KNN_model.score(X_train,y_train))\n    test_scores.append(KNN_model.score(X_test,y_test))\n    K.append(i)\n\nplt.plot(K, train_scores, label= 'Train_scores')\nplt.plot(K, test_scores, label = 'Test_scores')\n\nplt.xlabel('n_Neighbors')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.show()\n\nend = time.time()\nprint(end - start, ' Seconds to fit {}x KNNs'.format(len(K)))\nprint('Best n-neighbors was: {}'.format(K[np.argmax(test_scores)]))","386daf3b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.tree import DecisionTreeClassifier\n\nrandom_state = np.random.RandomState(69)\nn_samples, n_features = X.shape\n\nX_train, X_remainder, y_train, y_remainder = train_test_split(X, y, test_size=0.7, random_state=1, stratify = y)\nX_validate, X_test, y_validate, y_test = train_test_split(X_remainder, y_remainder, test_size=0.5, random_state=2, stratify = y_remainder)\n\n#Transform data\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\nX_validate = scaler.transform(X_validate)\n\ntest_scores = []\nvalidation_scores = []\ntrain_scores = []\n\n# C = [.00000001,.0000001,.000001,.00001,.0001,.001,.1,\\\n#                 1,10,100,1000,10000,100000,1000000,10000000,100000000,1000000000]\n\nC= [1,2,3,4,5,6,7,8,9,10,20,30,40,50,60,70,80,90,100,1000]\n\nstart = time.time() # start timer\n\nfor c in np.array(C) :\n#     my_regression = LogisticRegression(penalty='l2',C = c)\n    my_regression = DecisionTreeClassifier(max_depth = c, random_state = 69)\n    my_regression.fit(X_train,y_train);\n    train_scores.append(my_regression.score(X_train,y_train))\n    test_scores.append(my_regression.score(X_test,y_test))\n    validation_scores.append(my_regression.score(X_validate,y_validate))\n\nplt.plot(np.log10(C), train_scores,label=\"Train Score\")\nplt.plot(np.log10(C), test_scores,label=\"Test Score\")\nplt.plot(np.log10(C), validation_scores,label=\"Validation Score\")\n\nplt.legend();\nplt.show();\n\nend = time.time() # stop timer\nprint(end - start, ' Seconds')\nprint('Best Tree depths were: {} and {}'.format(C[np.argmax(validation_scores)], C[np.argmax(train_scores)]))","af2587c5":"# Tune C-value (regularization)\nC = []  # Store result to graph\nvalidation_score_list = []\nsample_range = [10**i for i in np.arange(-7,7,0.25)]\nstart = time.time() # start timer\n\n#Do some cross validation\nfrom sklearn.model_selection import cross_val_score\nfor i in sample_range :\n    LR_model = LogisticRegression(penalty='l1', C=i)\n    validation_score = np.mean(cross_val_score(LR_model, X, y, cv = 5))\n    validation_score_list.append(validation_score)\n\nplt.scatter(np.log10(sample_range), validation_score_list,label=\"Validation Score\")\nplt.legend()\nplt.xlabel('Regularization Parameter: C')\nplt.ylabel('Validation Score')\nplt.show();\n\nC_val = validation_score_list.index(np.max(validation_score_list))+1\nprint('Best C-value was: {}'.format(C_val))\n\nend = time.time() # stop timer\nprint(end - start, ' Seconds')","cccdeb69":"from sklearn.metrics import confusion_matrix\n\nlogistic_model = LogisticRegression(solver = 'lbfgs')\nlogistic_model.fit(X_train, y_train)\n\ny_pred = logistic_model.predict(X_test)\nconfusion = confusion_matrix(y_test, y_pred)\n# print(confusion)\nsns.heatmap(confusion, annot=True)","4ad0f997":"k = sns.countplot(mnist[:, -1], color = 'Cyan')","da147375":"# from sklearn.svm import LinearSVC\n\n# #Fit the model\n# SVM_model = LinearSVC(C=0.99)\n# SVM_model.fit(X_train,y_train)\n\n# # Plotting decision regions\n# PlotBoundaries(SVM_model, X_train,y_train)","3ee88167":"from keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import LearningRateScheduler","80a3dff9":"# Reshape for ImageDataGenerator\nY_train = mnist[:, -1]\nX_train = mnist[:, :-1]\nX_train = X_train \/ 255.0\nX_train = X_train.reshape(-1,28,28,1)\nY_train = to_categorical(Y_train, num_classes = 10)","e488965e":"datagen = ImageDataGenerator(\n        rotation_range=8,  \n        zoom_range = 0.1,  \n        width_shift_range=0.1, \n        height_shift_range=0.1)","3d5120e1":"X_train3 = X_train[9,].reshape((1,28,28,1))\nY_train3 = Y_train[9,].reshape((1,10))\nplt.figure(figsize=(15,4.5))\nfor i in range(69):  \n    plt.subplot(8, 10, i+1)\n    X_train2, Y_train2 = datagen.flow(X_train3,Y_train3).next()\n    plt.imshow(X_train2[0].reshape((28,28)),cmap=plt.cm.binary)\n    plt.axis('off')\n    if i==9: X_train3 = X_train[93,].reshape((1,28,28,1))\n    if i==19: X_train3 = X_train[15,].reshape((1,28,28,1))\n    if i==29: X_train3 = X_train[24,].reshape((1,28,28,1))\n    if i==39: X_train3 = X_train[5,].reshape((1,28,28,1))\n    if i==49: X_train3 = X_train[0,].reshape((1,28,28,1))\n    if i==59: X_train3 = X_train[45,].reshape((1,28,28,1))\n    if i==69: X_train3 = X_train[52,].reshape((1,28,28,1))\nplt.subplots_adjust(wspace=-0.1, hspace=-0.1)\nplt.show()","707db3e6":"# Define Keras ConvNet Model \nnets = 36\nmodel = [0] *nets\nfor j in range(nets):\n    model[j] = Sequential()\n\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu', input_shape = (28, 28, 1)))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(32, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 3, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Conv2D(64, kernel_size = 5, strides=2, padding='same', activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Dropout(0.4))\n\n    model[j].add(Conv2D(128, kernel_size = 4, activation='relu'))\n    model[j].add(BatchNormalization())\n    model[j].add(Flatten())\n    model[j].add(Dropout(0.4))\n    model[j].add(Dense(10, activation='softmax'))\n\n    model[j].compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\nannealer = LearningRateScheduler(lambda x: 1e-3 * 0.95 ** x)","427e15cc":"# Run the model\nepochs = 45\nhistory = [0] * nets\n\nfor j in range(nets):\n    X_train2, X_val2, Y_train2, Y_val2 = train_test_split(X_train, Y_train, test_size = 0.1)\n    history[j] = model[j].fit_generator(datagen.flow(X_train2,Y_train2, batch_size=64),\n        epochs = epochs, steps_per_epoch = X_train2.shape[0]\/\/64,  \n        validation_data = (X_val2,Y_val2), callbacks=[annealer], verbose=1)\n    print(\"CNN {0:d}: Epochs={1:d}, Train accuracy={2:.5f}, Validation accuracy={3:.5f}\".format(\n        j+1,epochs,max(history[j].history['acc']),max(history[j].history['val_acc']) ))","26ba1877":"# Dump the model into a .pkl jar\nimport pickle\n\npickle_out = open(\"keagan_model.pkl\",\"wb\")\npickle.dump(model, pickle_out)\npickle_out.close()","42911e13":"We could also play with the loss functions. (L2 or L1) I found that L1 works better. ","e30762d0":"### Strengths vs. Weaknesses of this model\nJudging by the confusion matrix here our model seems to misclassify a good portion of the Five's. As we can see below the number of fives in the dataset is smaller. Popular misclassifications for the fives were Threes and Eights, which seems likely as these digits are similar in shape. This model is very good at classifying Ones, Zeros and Sevens. I've looked at few of these examples that are INCREDIBLY close, where I thought it was definitely a One but it turned out to be a Seven. So, I mean some of these examples are so close between the two that, they might as well be classified as both!  ","f982e75c":"# Cross Validation","79a8f079":"Here we're rotating, zooming, and shifting height\/width in order to generate a larger train set to feed into the CNN. The more data a neural networks has, in general, the better it performs. ","8fef9845":"## Reducing Dimensionality w\/ PCA","856ce90f":"# Visualize","c12eddb2":"I wish I had more time to classify 4 and 9 and visualize the outputs. It would make sense to do something like a SVC linear. (but the runtime for that would be over 9000) Also, I really don't enjoy using that PlotBoundaries() function from the notebooks... it's very picky and does not work too well.","00d016bb":"# Logistic Regression","8cd3f713":"# Confusion Matrix","3e68f5a2":"Here it seems as we increase tree depth our train set accuracy keeps on rising. As we extend the tree depth our model overfits the training set. This could be a mistake on my part but, it doesn't seem as though increasing the tree depth past the optimal value has a significant effect on test\/validation accruacy. Other than the fact that it is overfitting, the model will be less able to fit to the randomness of more such data. ","341e3e60":"The C-value controls the regularization of the model. Low C-value equates to High regularization. A high C-value equates to Low Regularization. To much, or too little regularization is bad for performance. The sweet spot is somewhere in the middle GoldiLocks-Zone. As calculated above we have the best C-value for regularization","f9383506":"From the figure above it would appear that while increasing the n_neighbors paramater, our accuracy scores for both the training and test set decline. This conforms to our previous experiment with k=1 being pretty accurate, and k=n being very inaccurate. Lower number of neighbors is better for predictive performance.\n\nAny K between 3 and 10 will be good enough for this dataset. We could calculate the K with max accuracy, but it doesn't really matter because this model is not very good. And it will likely be similar to any value between 3 and 10 so might as well just pick one. The gains from picking a different model will be larger than if we try to tune this bad model.","f5d054a4":"## Data Augmentation","d7055aea":"## Average Digits","4740f543":"It would seem that with 1 neighbors our KNN model will overfit the training set, and still has decent score for the test set. A small value of k means that noise will have a higher influence on the result.\n\nConversely, with lots of neighbors the model will underfit, and has very low scores for the training set. A large value makes it computationally expensive. Although the difference between train-score and test-score is low, so it could be argued that this model is  more generalizable than with 1 neighbor.","527f4dc8":"# Decision Tree","3aef20eb":"# KNN","47ceb695":"When reducing the dimensions of our data, it\u2019s important not to lose more information than is necessary. The variation in a data set can be seen as representing the information that we would like to keep. Even though keeping more information increases our compute load. One of the key benefits of PCA is that in addition to the low-dimensional sample representation, it provides a synchronized low-dimensional representation of the variables. The synchronized sample and variable representations provide a way to visually find variables that are characteristic of a group of samples. However, PCA also assumes that the principle components are a linear combination of the original features. If this is not true, PCA will not give sensible results. Further, PCA uses variance as the measure of how important a particular dimension is. So, high variance pixels are treated as principle components, while low variance pixels are treated as noise. ","04236b61":"This model takes about 4-5 Hours to run.","79d06d2d":"Looks like our dataset has somewhat different number of examples of each digit. This could have implications later on...","d63fbd3a":"# Conv Net","b269c56a":"## Example Digits\nHere are some example digits from our dataset. Some of these examples may not be immediately obvious. "}}