{"cell_type":{"a5bd205a":"code","c2e2a5af":"code","aa5d44cf":"code","5db3cb8c":"code","c1313120":"code","748c7191":"code","baef9599":"code","6995690c":"code","3268d068":"code","97713a3b":"code","6db20049":"code","ccbfcd89":"code","fd8f83d6":"code","357879ae":"code","938d6e71":"code","308b8c99":"code","631a6409":"code","2e48f791":"code","c800f2f2":"code","24f16690":"code","36d3f98c":"code","5f7bd021":"code","ab4b2c24":"code","1f2daf8d":"code","eace1488":"code","d36af87f":"code","60c1bee2":"code","b2b3796e":"code","3ce32284":"code","ba1e1548":"code","82c410dc":"code","68a8cbed":"code","f1ba86bb":"code","ecc0249b":"code","62f5aea1":"code","8e7531dd":"code","1727a0e2":"code","6168aaec":"code","29962713":"code","f2568e87":"code","17ebf2f6":"code","82f27b5d":"code","615edb0f":"code","dbcb3c8b":"code","bd2f13a1":"code","897b362c":"code","86f3a4b6":"code","59e2c39b":"code","a3d05d84":"code","04a82a24":"markdown","5463026f":"markdown","473c1ac4":"markdown","ce3a5062":"markdown","c1ee5982":"markdown","55bc02bb":"markdown","75b52475":"markdown","415f65ab":"markdown","77ddf48b":"markdown","e35fd5c2":"markdown","2ec2df18":"markdown","8a9ee44a":"markdown","bceefeac":"markdown","f5fbc853":"markdown","d082cb17":"markdown","deaeb3ca":"markdown","402c743f":"markdown","258f3b6c":"markdown","1f2cc1e0":"markdown","ca13cc6f":"markdown","63f2b9cb":"markdown","ab28f3ff":"markdown","09ce0bfb":"markdown","63ac35d9":"markdown","bf6c2768":"markdown","aebd0f13":"markdown"},"source":{"a5bd205a":"import numpy as np \nimport pandas as pd \nimport os\nimport matplotlib.pyplot as plt\nimport cv2","c2e2a5af":"sample = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")\nsample","aa5d44cf":"test = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")\ntest","5db3cb8c":"train = pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")\ntrain","c1313120":"train_path = \"..\/input\/petfinder-pawpularity-score\/train\"","748c7191":"path = os.path.join(train_path,train[\"Id\"].iloc[0]+\".jpg\")","baef9599":"img = cv2.imread(path)\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)","6995690c":"train.iloc[0]","3268d068":"explain_dict ={  \n\"Id\":\"filename\",\n\"Subject Focus\":\"Pet stands out against uncluttered background, not too close \/ far\",\n\"Eyes\":\"Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear\",\n\"Face\":\"Decently clear face, facing front or near-front\",\n\"Near\":\"Single pet taking up significant portion of photo (roughly over 50% of photo width or height)\",\n\"Action\":\"Pet in the middle of an action (e.g., jumping)\",\n\"Accessory\":\"Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash\",\n\"Group\":\"More than 1 pet in the photo\",\n\"Collage\":\"Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos)\",\n\"Human\":\"Human in the photo\",\n\"Occlusion\":\"Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion\",\n\"Info\":\"Custom-added text or labels (i.e. pet name, description)\",\n\"Blur\":\"Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0\",\n\"Pawpularity\":\"an index of the popularity (attractiveness) of pets\",\n}","97713a3b":"train_eng = train.copy()","6db20049":"train_eng.columns = train.columns.map(explain_dict)","ccbfcd89":"train_eng.head(3)","fd8f83d6":"tmpdf = train_eng[train_eng.index==0].T","357879ae":"tmpdf","938d6e71":"img = cv2.imread(path)\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)","308b8c99":"for a in tmpdf[tmpdf[0]==True].index:\n    print(\"\u30fb\" + a)","631a6409":"for a in tmpdf[tmpdf[0]==False].index:\n    print(\"\u30fb\" + a)","2e48f791":"path = os.path.join(train_path,train[\"Id\"].iloc[0]+\".jpg\")\n\nplt.figure()\n\nimg = cv2.imread(path)\nimg = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\nplt.imshow(img)\n\nplt.show()\n\ntmpdf = train_eng[train_eng.index==0].T\n\nprint(\"--------Pawpularity------------\")\n\n\nprint(tmpdf[0].iloc[-1])\n\nprint(\"--------item Number 1------------\")\n\nfor a in tmpdf[tmpdf[0]==True].index:\n    print(\"\u30fb \" + a)\n\n\nprint(\"\")\n\n\n\nprint(\"--------item Number 0------------\")\n\nfor a in tmpdf[tmpdf[0]==False].index:\n    print(\"\u30fb \" + a)\n\n","c800f2f2":"def showimg(id):\n    \n    plt.figure()\n    path = os.path.join(train_path,train[\"Id\"].iloc[id]+\".jpg\")\n\n    img = cv2.imread(path)\n    img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    \n    plt.show()\n\n    tmpdf = train_eng[train_eng.index==id].T\n    \n    print(\"--------Pawpularity------------\")\n\n    print(tmpdf[id].iloc[-1])\n\n    print(\"--------item Number 1------------\")\n\n    for a in tmpdf[tmpdf[id]==True].index:\n        \n        if a == \"Pawpularity\":\n            continue\n        \n        print(\"\u30fb \" + a)\n\n\n    print(\"\")\n\n\n\n    print(\"--------item Number 0------------\")\n\n    for a in tmpdf[tmpdf[id]==False].index:\n        \n        if a == \"Pawpularity\":\n            continue\n        print(\"\u30fb \" + a)\n\n","24f16690":"showimg(1)","36d3f98c":"showimg(2)","5f7bd021":"train[train[\"Pawpularity\"]==100]","ab4b2c24":"showimg(19)","1f2daf8d":"showimg(50)","eace1488":"train[train[\"Pawpularity\"]==1]","d36af87f":"showimg(2442)","60c1bee2":"showimg(3232)","b2b3796e":"showimg(4235)","3ce32284":"tmpdf3 = train.groupby(\"Pawpularity\").head(1).sort_values(\"Pawpularity\").reset_index()\ntmpdf3","ba1e1548":"tmpdf4 = tmpdf3.iloc[::10,:]\ntmpdf4","82c410dc":"for a in tmpdf4[\"index\"]:\n    showimg(a)\n    print(\"\")\n    print(\"#################################################\")","68a8cbed":"train","f1ba86bb":"from sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","ecc0249b":"folds = train.copy()\nFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[\"Pawpularity\"])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.groupby(['fold', \"Pawpularity\"]).size())","62f5aea1":"import lightgbm as lgb","8e7531dd":"import random\n\ndef fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nSEED = 42\nfix_seed(SEED)","1727a0e2":"features = train.columns.to_list()[1:-1]\nfeatures","6168aaec":"target = \"Pawpularity\"","29962713":"# example of parameters\nlgbm_params = {\n    'objective': 'rmse', # Binary classification : 2\u5024\u5206\u985e\u3067\u306f\u3053\u308c\u3092\u4f7f\u3046\n    'seed': 42, # random seed : \u3053\u308c\u3092\u56fa\u5b9a\u3059\u308b\u3068\u3001\u518d\u73fe\u6027\u304c\u51fa\u308b\n    'metric': 'rmse', \n    'learning_rate': 0.01,\n    'max_bin': 800, # depth\n    'num_leaves': 80, # leaves,\n    \"verbose\":-1\n}","f2568e87":"from sklearn.metrics import mean_squared_error\n\ndef rmsescore(y_true,y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))","17ebf2f6":"test","82f27b5d":"scores = []\nallpreds = []\n\nallvaliddf = pd.DataFrame()\n\n\nfor fold in range(5):\n    \n    fix_seed(SEED) # for repetability\n\n    p_train = folds[folds[\"fold\"] != fold]\n    p_val = folds[folds[\"fold\"] == fold]\n\n    p_train = p_train.reset_index(drop=True)\n    p_val = p_val.reset_index(drop=True)\n\n    lgb_train = lgb.Dataset(p_train[features], p_train[target])\n    lgb_eval = lgb.Dataset(p_val[features], p_val[target])\n\n\n\n    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                     \n                      \n                     )\n\n    import pickle\n\n    model_name = f\"LGBMmodel{fold}.bin\"\n\n    # saving model\n    pickle.dump(model, open(model_name, 'wb'))\n\n    # loading model\n    model = pickle.load(open(model_name, 'rb'))\n\n    # predicting validation value\n    oof_pred = model.predict(p_val[features])\n\n\n    scores.append(rmsescore(p_val[target],oof_pred))\n\n    # predicting for test_X\n    preds = model.predict(test[features])\n\n    #preds2 = np.where(preds>=0.5,1,0)\n    \n    allpreds.append(preds)\n    \n    # out of fold : oof\n    p_val[\"preds\"] = oof_pred\n    \n    allvaliddf = pd.concat([allvaliddf,p_val])","615edb0f":"scores","dbcb3c8b":"np.mean(scores)","bd2f13a1":"rmsescore(allvaliddf[target],allvaliddf[\"preds\"])","897b362c":"allpreds = np.mean(allpreds,axis=0)","86f3a4b6":"sample[\"Pawpularity\"] = allpreds","59e2c39b":"sample.to_csv(\"submission.csv\",index=False)","a3d05d84":"sample","04a82a24":"\n# Thank you for watching so far.\n**upvote** I would be grateful if you could!","5463026f":"first id","473c1ac4":"* Id - filename\n* Focus - Pet stands out against uncluttered background, not too close \/ far.\n* Eyes - Both eyes are facing front or near-front, with at least 1 eye \/ pupil decently clear.\n* Face - Decently clear face, facing front or near-front.\n* Near - Single pet taking up significant portion of photo (roughly over 50% of photo width or height).\n* Action - Pet in the middle of an action (e.g., jumping).\n* Accessory - Accompanying physical or digital accessory \/ prop (i.e. toy, digital sticker), excluding collar and leash.\n* Group - More than 1 pet in the photo.\n* Collage - Digitally-retouched photo (i.e. with digital photo frame, combination of multiple photos).\n* Human - Human in the photo.\n* Occlusion - Specific undesirable objects blocking part of the pet (i.e. human, cage or fence). Note that not all blocking objects are considered occlusion.\n* Info - Custom-added text or labels (i.e. pet name, description).\n* Blur - Noticeably out of focus or noisy, especially for the pet\u2019s eyes and face. For Blur entries, \u201cEyes\u201d column is always set to 0.\n* Pawpularity - an index of the popularity (attractiveness) of pets.","ce3a5062":"## There is no Pawpularity 0, so take a look at 1.","c1ee5982":"### Put the above into a function ###\n\nExpress up to this point in one cell and change id 0 as an argument","55bc02bb":"# 4. Try submit with LGBM","75b52475":"# 2. What data do you predict from? (See test.csv)","415f65ab":"#### mean score","77ddf48b":"# 4.2 Main","e35fd5c2":"**Therefore, We need to pay attention to that the Pawpularity Score is derived from each pet profile's page(maybe including the picture)**","2ec2df18":"# 3.1 Try to display the English explanation","8a9ee44a":"## train.csv item number 1","bceefeac":"Predict Paw pularity from Id photos.\n\nPawpularity is an index of the popularity (attractiveness) of pets.\n\n**How Pawpularity Score Is Derived**\n\n* The Pawpularity Score is derived from each pet profile's page view statistics at the listing pages, using an algorithm that normalizes the traffic data across different pages, platforms (web & mobile) and various metrics.\n* Duplicate clicks, crawler bot accesses and sponsored profiles are excluded from the analysis.","f5fbc853":"So pretty\u3002\u3002\u3002Pawpularity\u3000= 63\n\n","d082cb17":"## train.csv item number 0","deaeb3ca":"## Take a look at the top ID","402c743f":"\n100 even if 2 animals are shown\nJust because a certain item is satisfied does not mean that you will get 100 points.","258f3b6c":"# 3. Take a look at the contents of train.csv","1f2cc1e0":"#### out of fold score","ca13cc6f":"# 1 What to predict? (See from sample_submission.csv)","63f2b9cb":"**\u3010Attention\u3011These labels are not used to derive the Pawpularity score. Details below.**\n\n**Purpose of Photo Metadata**\n* We have included optional Photo Metadata, manually labeling each photo for key visual quality and composition parameters.\n* These labels are not used for deriving our Pawpularity score, but it may be beneficial for better understanding the content and co-relating them to a photo's attractiveness. Our end goal is to deploy AI solutions that can generate intelligent recommendations (i.e. show a closer frontal pet face, add accessories, increase subject focus, etc) and automatic enhancements (i.e. brightness, contrast) on the photos, so we are hoping to have predictions that are more easily interpretable.\n* You may use these labels as you see fit, and optionally build an intermediate \/ supplementary model to predict the labels from the photos. If your supplementary model is good, we may integrate it into our AI tools as well.\n* In our production system, new photos that are dynamically scored will not contain any photo labels. If the Pawpularity prediction model requires photo label scores, we will use an intermediary model to derive such parameters, before feeding them to the final model.","ab28f3ff":"# 4.1 Kfold","09ce0bfb":"# PetFinder.my - Pawpularity Contest\n## Predict the popularity of shelter pet photos\n\nThis is an English version. Japanese version is here.https:\/\/www.kaggle.com\/chumajin\/petfinder-eda-lgbm-for-starter-version\n\n\n--------------------------------------------\nAs the subtitle suggests, this competition predicts the popularity (attractiveness) **Pawpularity** of pet photos.\n\nThis will automatically improve the quality of pet's profile and photos,\n\nStray dogs and cats will be able to find \"enthusiastic\" homes much sooner!\n\n--------------------------------------------\n\nIf you find it useful, I would be grateful if you could **upvote**.\n\nThank you for people who always upvote!\n","63ac35d9":"## Let's look at one for each Pawpularity 10.","bf6c2768":"## Take a look at Pawpularity100","aebd0f13":"## 4.3 for submit"}}