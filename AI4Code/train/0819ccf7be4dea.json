{"cell_type":{"4a57f781":"code","0edf3cf4":"code","05de168b":"code","1d76d3eb":"code","d4331fb5":"code","4e917964":"code","c4cd2bd0":"code","e3890fda":"code","b94eaa66":"code","3715d5d0":"code","11acf434":"code","0dbfc294":"code","2ec4dfa3":"code","b0ef03db":"code","4bec79cc":"code","7871d6fe":"code","aa57d2c1":"code","d96bcf87":"code","ce2e2bd8":"code","665de7c6":"code","cd40047f":"code","f5a07c55":"code","3df96b4d":"code","6689eeb4":"code","c686b18d":"code","85e27aee":"code","42c4806a":"code","0fbc9a9a":"code","d05d3c07":"code","2d0e2828":"code","2782aff6":"code","7c0b1d2a":"code","b6db225f":"code","a402e476":"code","b11dc4ff":"code","e3e392ce":"code","f7e9b523":"code","8677f576":"code","fc2ecb63":"markdown","c0be9bd8":"markdown","cf9c62aa":"markdown","ec8dfe1f":"markdown","485b8da6":"markdown","7edd1010":"markdown","ea8fbb33":"markdown","21023832":"markdown","cb772715":"markdown","10175007":"markdown","2b226b83":"markdown","95daac1e":"markdown","07096cb0":"markdown","dd4f3976":"markdown","d022be95":"markdown","d9447dec":"markdown"},"source":{"4a57f781":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport networkx as nx\nimport math\nimport torch\nimport os\nimport warnings\nimport dateutil\nfrom torch.utils.data import Dataset\nimport torchvision\nfrom sklearn.metrics import accuracy_score , confusion_matrix , r2_score , classification_report\nimport torchvision.transforms as transforms\nfrom tqdm import tqdm\nimport spacy\nnlp = spacy.load('en')\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\nplt.style.use('ggplot')\nwarnings.filterwarnings('ignore')","0edf3cf4":"def create_neural_network(inputs = 4 , activations = 4 , outputs = 3 , node_col = 'g' , edge_col = 'y'):\n    # max number of nodes in Graph are 50\n    dense  = nx.Graph()\n    # input layer  , activation layer , output layer in neural network \n    inputs = {i: (0,i) for i in range(0,inputs)}\n    activations = {i+50 : (1,i) for i in range(0,activations)}\n    outputs = {i+100 : (2,i) for i in range(0,outputs)}\n    all = {**inputs , **activations , **outputs}\n    for input in inputs:\n        for activation in activations:\n            dense.add_edge(input , activation)\n    for activation in activations:\n        for output in outputs:\n            dense.add_edge(activation, output)\n    nx.draw_networkx_nodes(dense , all , nodelist = all.keys() , node_color = node_col)\n    nx.draw_networkx_edges(dense , all  , edge_color = edge_col)\n    axes = plt.axis('off')\n    \n    \ncreate_neural_network()\n# This is Simple View of Neural Network","05de168b":"#Creating Simple Neural Netwok \ninputs = torch.rand(1,1,64,64)\nprint(inputs.shape ,'\\n',inputs)\noutputs = torch.rand(1,2)\nprint(outputs.shape , '\\n' , outputs)\n# Creating Linear Sequential Model \nmodel = torch.nn.Sequential(\n            torch.nn.Linear(64,256),\n            torch.nn.Linear(256,256),\n            torch.nn.Linear(256,2)\n)\nresult = model(inputs)\n# print(result , '\\n' ,result.shape )\nprint('Prediction shape is ' , result.shape )","1d76d3eb":"x = torch.range(-1,1,0.1) # X values for input\ngraph_x = 2 ; graph_y = 3 # Parameter for Graph\nplt.figure(figsize=(16,10)) # Figure Size\nplt.subplot(graph_x,graph_y,1);y = torch.nn.functional.softplus(x);plt.title('Softplus');plt.plot(x.numpy() , y.numpy())\nplt.subplot(graph_x,graph_y,2);y = torch.nn.functional.relu(x);plt.title('Relu');plt.plot(x.numpy() , y.numpy())\nplt.subplot(graph_x,graph_y,3);y = torch.nn.functional.elu(x);plt.title('Elu');plt.plot(x.numpy() , y.numpy())\nplt.subplot(graph_x,graph_y,4);y = torch.nn.functional.tanh(x);plt.title('Tanh');plt.plot(x.numpy() , y.numpy())\nplt.subplot(graph_x,graph_y,5);y = torch.nn.functional.sigmoid(x);plt.title('Sigmoid');plt.plot(x.numpy() , y.numpy())\nplt.subplot(graph_x,graph_y,6);y = torch.nn.functional.gelu(x);plt.title('Gelu');plt.plot(x.numpy() , y.numpy())","d4331fb5":"#Creating More Complex Neural Netwok \ninputs = torch.rand(1,1,64,64)\nprint(inputs.shape ,'\\n',inputs)\noutputs = torch.rand(1,2)\nprint(outputs.shape , '\\n' , outputs)\n# Creating Linear Sequential Model \nmodel = torch.nn.Sequential(\n            torch.nn.Linear(64,256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256,256),\n            torch.nn.ReLU(),\n            torch.nn.Linear(256,2)\n)\nresult = model(inputs)\n# print(result , '\\n' ,result.shape )\nprint('Prediction shape is ' , result.shape )\nloss = torch.nn.MSELoss()(result,outputs)\nprint(\"Mean Square Error is \", loss )\nmodel.zero_grad()\nloss.backward()\nlearning_rate = 0.01\nfor parameter in model.parameters():\n    parameter.data -= parameter.grad.data * learning_rate\nafter_result = model(inputs)\nprint(\"Mean Square Error is \", torch.nn.MSELoss()(after_result,outputs))","4e917964":"class Model(torch.nn.Module):\n    def __init__(self):\n        super().__init__()# for multiple inheritence super is used\n        self.layer_one = torch.nn.Linear(64,256)\n        self.activation_one = torch.nn.ReLU()\n        self.layer_two = torch.nn.Linear(256,256)\n        self.activation_two = torch.nn.ReLU()\n        self.shape_outputs = torch.nn.Linear(64*256 , 2)\n\n    def forward(self, inputs):\n        buffer = self.layer_one(inputs)\n        buffer = self.activation_one(buffer)\n        buffer = self.layer_two(buffer)\n        buffer = self.activation_two(buffer)\n        buffer = buffer.flatten(start_dim = 1)\n        return self.shape_outputs(buffer)\n    ","c4cd2bd0":"model = Model()\ntest_results = model(inputs)\nprint('Test Results are -> ',test_results)\nprint('Outputs are -> ',outputs)","e3890fda":"loss_function = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters() , lr=0.01)\nfor i in range(10000):\n    optimizer.zero_grad()\n    results = model(inputs)\n    loss = loss_function(results , outputs)\n    loss.backward()\n    optimizer.step()\n    gradients = 0.0\n    # Looking for vanishing point\n    for parameter in model.parameters():\n        gradients += parameter.grad.data.sum()\n    if abs(gradients) <= 0.0001:\n        print(gradients)\n        print('Gradient Vanished at iterations {0}'.format(i))\n        break\n        ","b94eaa66":"test_results = model(inputs)\nprint(test_results)\nprint(outputs)","3715d5d0":"class MushRoomDataset(Dataset):\n    def __init__(self):\n        self.data = pd.read_csv('\/kaggle\/input\/mushroom-classification\/mushrooms.csv')\n    def __len__():\n        return len(self.data)\n    def __getitem__(self , idx):\n        if type(idx) is torch.Tensor:\n            idx = idx.item()\n        return self.data.iloc[idx][1:] , self.data.iloc[idx][0:1]\n    def sample(self , samples = 5 , transverse = False):\n        if transverse:\n            display(self.data.sample(samples)).T\n        else:\n            display(self.data.sample(samples)) \n    def test_data(self , per_cen = 0.05 , shuffle = True):\n        number_of_testing = int(len(self.data) * per_cen)\n        number_of_training = len(self.data) - number_of_testing\n        train,test = torch.utils.data.random_split(self.data , [number_of_training , number_of_testing])\n        return train,test\nmrooms = MushRoomDataset()\ntrain , test = mrooms.test_data()","11acf434":"class OneHotEncoder():\n    def __init__(self,series):\n        unique_values = series.unique()\n        self.ordinals = {\n            val: i for i,val in enumerate(unique_values)\n        }\n        self.encoder = torch.eye(len(unique_values) , len(unique_values))\n        \n    def __getitem__(self,value):\n        return self.encoder[self.ordinals[value]]\n    \nclass CategoricalCSV(Dataset):\n    def __init__(self , datafile , output_series_name):\n        self.dataset = pd.read_csv(datafile)\n        self.output_series_name = output_series_name\n        self.encoders = {}\n        for series_name , series in self.dataset.items():\n            self.encoders[series_name] = OneHotEncoder(series)\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self , index):\n        if type(index) is torch.Tensor:\n            index = index.item()\n        sample = self.dataset.iloc[index]\n        output = self.encoders[self.output_series_name][sample[self.output_series_name]]\n        input_components= []\n        for name, value in sample.items():\n            if name != self.output_series_name:\n                input_components.append(self.encoders[name][value])\n        inputs = torch.cat(input_components)\n        return inputs , output\n\ncmrooms = CategoricalCSV('\/kaggle\/input\/mushroom-classification\/mushrooms.csv' , 'class')","0dbfc294":"cmrooms[0]","2ec4dfa3":"# Creating Predictions With Developed Data\nclass Model(torch.nn.Module):\n    def __init__(self , input_dimesions , output_dimesions , size = 128):\n        super().__init__()\n        self.layer_one = torch.nn.Linear(input_dimesions , size)\n        self.activation_one = torch.nn.ReLU()\n        self.layer_two = torch.nn.Linear(size , size)\n        self.activation_two = torch.nn.ReLU()\n        self.shape_outputs = torch.nn.Linear(size , output_dimesions)\n    def forward(self, inputs):\n        buffer = self.layer_one(inputs)\n        buffer = self.activation_one(buffer)\n        buffer = self.layer_two(buffer)\n        buffer = self.activation_two(buffer)\n        buffer = self.shape_outputs(buffer)\n        return torch.nn.functional.softmax(buffer , dim = -1)\n    \nmodel = Model(cmrooms[0][0].shape[0] , cmrooms[0][1].shape[0])\noptimizer = torch.optim.Adam(model.parameters())\nloss_functions = torch.nn.BCELoss()\n# train , test = cmrooms.test_data()\nnumber_of_testing = int(len(cmrooms) * 0.05)\nnumber_of_training = len(cmrooms) - number_of_testing\ntrain,test = torch.utils.data.random_split(cmrooms, [number_of_training , number_of_testing])\n\ntraining = torch.utils.data.DataLoader(train , batch_size = 16 , shuffle = True)\ntesting = torch.utils.data.DataLoader(test , batch_size = len(test) , shuffle = True)\n\nfor epoch in range(3):\n    for inputs,outputs in training:\n        optimizer.zero_grad()\n        results = model(inputs)\n        loss = loss_function(results , outputs)\n        loss.backward()\n        optimizer.step()\n    print(\"Loss : {0} \".format(loss))\n    \nfor inputs , outputs in testing:\n    results = model(inputs).argmax(dim = 1).numpy()\n    actual = outputs.argmax(dim = 1).numpy()\n    accuracy = accuracy_score(actual , results)\n    print(\"Test Accuracy is -> \",accuracy)\n    \nsns.heatmap(confusion_matrix(actual , results), annot=True, annot_kws={\"size\": 16}) # font size\nplt.show()","b0ef03db":"sample_1 = pd.read_csv('\/kaggle\/input\/kc-housesales-data\/kc_house_data.csv')","4bec79cc":"class DateEncoder():\n    def __getitem__(self, datestring):\n        parsed = dateutil.parser.parse(datestring)\n        return torch.Tensor([parsed.year, parsed.month , parsed.day])\n    \nclass MixedCSV(Dataset):\n    def __init__(self , datafile , output_series_name , date_series_name , categorical_series_name , ignore_series_name):\n        self.dataset = pd.read_csv(datafile)\n        self.output_series_name = output_series_name\n        self.encoders = {}\n        for series_name in date_series_name:\n            self.encoders[series_name] = DateEncoder()\n        for series_name in categorical_series_name:\n            self.encoders[series_name] = OneHotEncoder(self.dataset[series_name])\n        self.ignore = ignore_series_name\n        \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self , index):\n        if type(index) is torch.Tensor:\n            index = index.item()\n        sample = self.dataset.iloc[index]\n        output = torch.Tensor([sample[self.output_series_name]])\n        input_components = []\n        for name , value in sample.items():\n            if name in self.ignore:\n                continue\n            elif name in self.encoders:\n                input_components.append(self.encoders[name][value])\n            else:\n                input_components.append(torch.Tensor([value]))\n        \n        input = torch.cat(input_components)\n        return input , output\n","7871d6fe":"date_type = ['date']\ncategorical_data =  ['zipcode' , 'waterfront' , 'condition' , 'grade']\ndiscard = ['id']\noutput_column = 'price'\nhouses = MixedCSV('\/kaggle\/input\/kc-housesales-data\/kc_house_data.csv' , \n                    output_column,\n                    date_type , \n                    categorical_data , \n                    discard)","aa57d2c1":"class Model(torch.nn.Module):\n    def __init__(self , input_dimesions , size = 128):\n        super().__init__()\n        self.layer_one = torch.nn.Linear(input_dimesions , size)\n        self.activation_one = torch.nn.ReLU()\n        self.layer_two = torch.nn.Linear(size , size)\n        self.activation_two = torch.nn.ReLU()\n        self.shape_outputs = torch.nn.Linear(size , 1)\n    \n    def forward(self , inputs):\n        buffer = self.layer_one(inputs)\n        buffer = self.activation_one(buffer)\n        buffer = self.layer_two(buffer)\n        buffer = self.activation_two(buffer)\n        buffer = self.shape_outputs(buffer)\n        return buffer\n    \nmodel = Model(houses[0][0].shape[0])\noptimizer = torch.optim.Adam(model.parameters())\nloss_function = torch.nn.MSELoss()\n\nnumber_for_testing = int(len(houses)*0.05)\nnumber_for_training = len(houses) - number_for_testing\ntrain , test = torch.utils.data.random_split(houses , [number_for_training , number_for_testing])\ntraining = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True)\nfor epoch in range(3):\n    for inputs , outputs in training:\n        optimizer.zero_grad()\n        results = model(inputs)\n        loss = loss_function(results , outputs)\n        loss.backward()\n        optimizer.step()\n    print('Loss {0}'.format(loss))\n    \nprint('Printing R-Square Score ....\\n')\ntesting = torch.utils.data.DataLoader(test , batch_size = len(test) , shuffle= False)\nfor inputs , outputs in testing:\n    predicted = model(inputs).detach().numpy()\n    actual = outputs.numpy()\n    print(r2_score(predicted , actual))","d96bcf87":"# Loading Dataset\nmnist = torchvision.datasets.MNIST('.\/var' , download = True)\n# transform = transforms.Compose([transforms.ToTensor() , transforms.Normalize(mean = (0.5,0.5,0.5) , std = (0.5,0.5,0.5))]) # Normalize take Mean and Standard Deviation for each channel\n# transform = transforms.Compose([transforms.ToTensor() , transforms.Normalize(mean = (0.5,) , std = (0.5,))]) # Normalize take Mean and Standard Deviation for each channel\ntransform = transforms.Compose([transforms.ToTensor()]) # Normalize take Mean and Standard Deviation for each channel\n\ntrain = torchvision.datasets.MNIST('.\/var', train = True , transform = transform)\ntrainloader = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True)\ntest = torchvision.datasets.MNIST('.\/var', train = False , transform = transform)\ntestloader = torch.utils.data.DataLoader(test , batch_size = len(test) , shuffle = True)\n\nfor inputs , outputs  in trainloader:\n    image = inputs[0][0]\n    plt.imshow(image.numpy() , cmap = plt.get_cmap('binary'))\n    break\n","ce2e2bd8":"class Net(torch.nn.Module):\n    def __init__(self):\n        super(Net , self).__init__()\n        self.conv1 = torch.nn.Conv2d(1,3,3) # Input Channel ,Output Channel , Kernel Size\n        self.pool = torch.nn.MaxPool2d(2,2) # Kernel Size , Stride\n        self.conv2 = torch.nn.Conv2d(3,6,3)\n        self.fc1 = torch.nn.Linear(150,128)\n        self.fc2 = torch.nn.Linear(128,128)\n        self.fc3 = torch.nn.Linear(128,10)\n    \n    def forward(self , x):\n        x = self.pool(torch.nn.functional.relu(self.conv1(x)))\n        self.after_con1 = x\n        x = self.pool(torch.nn.functional.relu(self.conv2(x)))\n        self.after_con2 = x\n        x = x.flatten(start_dim = 1)\n        x = torch.nn.functional.relu(self.fc1(x))\n        x = torch.nn.functional.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\nnet_cnn = Net()\nloss_functional = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net_cnn.parameters())\n\nfor epoch in range(3):\n    for inputs, outputs in trainloader:\n        optimizer.zero_grad()\n        results = net_cnn(inputs)\n        loss = loss_functional(results, outputs)\n        loss.backward()\n        optimizer.step()\n    print(\"Loss for epoch {} is {}\".format(epoch , loss))\n    \nfor inputs , actual in testloader:\n    results = net_cnn(inputs).argmax(dim = 1).numpy()\n    accuracy = accuracy_score(actual , results)\n    print(accuracy)\nprint(classification_report(actual , results))\n    ","665de7c6":"for inputs , outputs  in trainloader:\n    figure = plt.figure()\n    image = inputs[0][0]\n    plt.subplot(3,6,1)\n    plt.imshow(image.numpy() , cmap = plt.get_cmap('binary'))\n    output = net_cnn(inputs)\n    \n    filter_one = net_cnn.after_con1[0].detach()\n    for i in range(3):\n        plt.subplot(3,6,6+1+i)\n        plt.imshow(filter_one[i].numpy() , cmap = plt.get_cmap('binary'))\n    filter_two = net_cnn.after_con2[0].detach()\n    # detach remove all gradients and return as simple numpy \n    for i in range(6):\n        plt.subplot(3,6,12+1+i)\n        plt.imshow(filter_two[i].numpy() , cmap = plt.get_cmap('binary'))\n    plt.show()\n    break","cd40047f":"class SlimAlexNet(torch.nn.Module):\n    def __init__(self , num_classes = 10):\n        super().__init__()\n        self.features = torch.nn.Sequential(\n            torch.nn.Conv2d(1 , 32 , kernel_size = 3 , stride = 1),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.MaxPool2d(kernel_size = 3, stride = 2), \n            torch.nn.Conv2d(32 , 64 , kernel_size = 3 ),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.MaxPool2d(kernel_size = 3, stride = 2), \n            torch.nn.Conv2d(64 , 128 , kernel_size = 3 , padding = 1),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.Conv2d(128 , 256 , kernel_size = 3 , padding = 1),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.Conv2d(256 , 128 , kernel_size = 3 , padding = 1),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.MaxPool2d(kernel_size = 3, stride = 2),\n        )\n        self.classifier = torch.nn.Sequential(\n            torch.nn.Dropout(),\n            torch.nn.Linear(128 , 1024),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.Dropout(),\n            torch.nn.Linear(1024 , 1024),\n            torch.nn.ReLU(inplace = True),\n            torch.nn.Linear(1024 , num_classes),\n        )\n        \n    def forward(self, x):\n        x = self.features(x)\n        x = x.flatten(start_dim = 1)\n        x = self.classifier(x)\n        return x\n\nworkers = int(os.cpu_count())\n\ntransform  = transforms.Compose([transforms.ToTensor()])\nmnist = torchvision.datasets.MNIST('.\/var' , download = True)\ntrain = torchvision.datasets.MNIST('.\/var' , train = True , transform = transform)\ntrainloader = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True)\ntest = torchvision.datasets.MNIST('.\/var' , train = False , transform = transform)\ntestloader = torch.utils.data.DataLoader(test , batch_size = len(test) , shuffle = True )\n\nnet_alex = SlimAlexNet(num_classes = 10)\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net_alex.parameters())\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    not_device = torch.device('cpu')\nelse:\n    device = torch.device('cpu')\n    not_device = torch.device('cuda')\n    \nnet_alex.to(device)\nfor epoch in range(3):\n    total_loss = 0\n    for inputs, outputs in trainloader:\n        inputs = inputs.to(device)\n        outputs = outputs.to(device)\n        optimizer.zero_grad()\n        results = net_alex(inputs)\n        loss = loss_functional(results, outputs)\n        total_loss +=loss.item()\n        loss.backward()\n        optimizer.step()\n    print(\"Loss for epoch {} is {}\".format(epoch , total_loss\/len(trainloader)))\n    \nfor inputs , actual in testloader:\n    inputs = inputs.to(device)\n    results = net_alex(inputs).argmax(dim = 1).to(not_device).numpy()\n    accuracy = accuracy_score(actual , results)\n    print(accuracy)\nprint(classification_report(actual , results))","f5a07c55":"vgg = torchvision.models.vgg11_bn(pretrained = True)\ntransform  = transforms.Compose([transforms.Grayscale(3) , transforms.CenterCrop(224) , transforms.ToTensor()])\nmnist = torchvision.datasets.MNIST('.\/var' , download = True)\nworkers = int(os.cpu_count())\n\ntrain = torchvision.datasets.MNIST('.\/var' , train = True , transform = transform)\ntrainloader = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True , num_workers = workers)\n\ntest = torchvision.datasets.MNIST('.\/var' , train = False , transform = transform)\ntestloader = torch.utils.data.DataLoader(test , batch_size = 32 , shuffle = True , num_workers = workers)\n\nfor inputs , outputs in trainloader:\n    image = inputs[0][0]\n    plt.imshow(image.numpy() , cmap=  plt.get_cmap('binary'))\n    break\n    \n# Changing the output from 1000 to 10\nvgg.classifier[-1] = torch.nn.Linear(in_features=4096, out_features=10, bias=True)\n# display(vgg.classifier) # uncomment to Visualize VGG CLassifiers layers\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    not_device = torch.device('cpu')\nelse:\n    device = torch.device('cpu')\n    not_device = torch.device('cuda')\n    \nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(vgg.parameters())\n\nvgg.to(device)\nvgg.train()\n\nfor epoch in range(3):\n    for inputs , outputs in trainloader:\n        inputs = inputs.to(device , non_blocking = True)\n        outputs = outputs.to(device , non_blocking = True)\n        optimizer.zero_grad()\n        results = vgg(inputs)\n        loss = loss_function(results , outputs)\n        loss.backward()\n        optimizer.step()\n    print(\"Last Loss : {0}\".format(loss))","3df96b4d":"results_buffer = []\nactual_buffer = []\nwith torch.no_grad(): # no_grad will set all the Gradients to Zero\n    vgg.eval()\n    for inputs, outputs in testloader:\n        inputs = inputs.to(device , non_blocking = True)\n        results = vgg(inputs).argmax(dim = 1).to('cpu')\n        results_buffer.append(results)\n        actual_buffer.append(outputs)\nprint(classification_report(np.concatenate(actual_buffer) , np.concatenate(results_buffer)))","6689eeb4":"model = torchvision.models.resnet18(pretrained = True)\nprint(model.fc)\n\nmodel.fc = torch.nn.Linear(model.fc.in_features , 10)\nloss_function = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters())\n\nmodel.to(device)\nmodel.train() # To reactivate all the layes to training mode..\nfor epoch in range(3):\n    for inputs , outputs in trainloader:\n        inputs = inputs.to(device , non_blocking = True)\n        outputs = outputs.to(device , non_blocking = True)\n        optimizer.zero_grad()\n        results = model(inputs)\n        loss = loss_function(results , outputs)\n        loss.backward()\n        optimizer.step()\n    print(\"Last Loss : {0}\".format(loss))","c686b18d":"results_buffer = []\nactual_buffer = []\nwith torch.no_grad(): # no_grad will set all the Gradients to Zero\n    model.eval()\n    for inputs, outputs in testloader:\n        inputs = inputs.to(device , non_blocking = True)\n        results = model(inputs).argmax(dim = 1).to('cpu')\n        results_buffer.append(results)\n        actual_buffer.append(outputs)\nprint(classification_report(np.concatenate(actual_buffer) , np.concatenate(results_buffer)))","85e27aee":"class SentimentDataset(Dataset):\n    def __init__(self):\n        self.data = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip' , sep= '\\t' , header = 0).groupby('SentenceId').first()\n        self.ordinals = {}\n        for sample in tqdm(self.data.Phrase):\n            for token in nlp(sample.lower() , disable = ['parser' , 'tagger' , 'ner']): # https:\/\/spacy.io\/usage\/processing-pipelines\n                if token.text not in self.ordinals:\n                    self.ordinals[token.text] = len(self.ordinals) # Tokenizing each word with a number\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self , idx):\n        if type(idx) is torch.Tensor:\n            idx = idx.item()\n        sample = self.data.iloc[idx]\n        bag_of_words = torch.zeros(len(self.ordinals))\n        for token in nlp(sample.Phrase.lower() , disable = ['parser' , 'tagger' , 'ner']):\n            bag_of_words[self.ordinals[token.text]] += 1\n        return bag_of_words , torch.tensor(sample.Sentiment)\n\nclass Model(torch.nn.Module):\n    def __init__(self, input_dimensions , size = 128 , output_shape = 5):\n        super().__init__()\n        self.layer_one = torch.nn.Linear(input_dimensions , size)\n        self.activation_one = torch.nn.ReLU()\n        self.layer_two = torch.nn.Linear(size , size)\n        self.activation_two = torch.nn.ReLU()\n        self.shape_outputs = torch.nn.Linear(size , output_shape)\n    \n    def forward(self, inputs):\n        buffer = self.layer_one(inputs)\n        buffer = self.activation_one(buffer)\n        buffer = self.layer_two(buffer)\n        buffer = self.activation_two(buffer)\n        buffer = self.shape_outputs(buffer)\n        return buffer","42c4806a":"sentiment = SentimentDataset() # Initalizing the Dataset\n\nnumber_of_testing = int(len(sentiment)*0.05) \nnumber_of_training = len(sentiment) - number_of_testing\ntrain , test = torch.utils.data.random_split(sentiment , [number_of_training , number_of_testing])\n\n# Creating the DataLoader\ntrainloader = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True)\ntestloader = torch.utils.data.DataLoader(test , batch_size = 32 , shuffle = True)\n\n# Creating the Model\nmodel = Model(len(sentiment.ordinals)) # 15354 Unique Words for Model\n# Training the Model\noptimizer = torch.optim.Adam(model.parameters())\nloss_functions = torch.nn.CrossEntropyLoss()\n\nmodel.train() # Setting the model to Train\n\nfor epoch in range(15):\n    losses = []\n    for inputs , outputs in tqdm(trainloader):\n        optimizer.zero_grad()\n        results = model(inputs)\n        loss = loss_functions(results , outputs)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    print(\"Loss {0}\".format(torch.tensor(losses).mean()))\n    ","0fbc9a9a":"results_buffer = []\nactual_buffer = []\nwith torch.no_grad():\n    model.eval()\n    for inputs, outputs in testloader:\n        results = model(inputs).argmax(dim = 1).numpy()\n        results_buffer.append(results)\n        actual_buffer.append(outputs)\n\nprint(classification_report(np.concatenate(actual_buffer) , np.concatenate(results_buffer)))","d05d3c07":"nlp_embedding = spacy.load('en_core_web_lg') # 300 Vector of Words","2d0e2828":"class SentimentDataset(Dataset):\n    def __init__(self):\n        self.data = pd.read_csv('\/kaggle\/input\/sentiment-analysis-on-movie-reviews\/train.tsv.zip' , sep= '\\t' , header = 0).groupby('SentenceId').first()\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self , idx):\n        if type(idx) is torch.Tensor:\n            idx = idx.item()\n        sample = self.data.iloc[idx]\n        token_vector = []\n        for token in nlp(sample.Phrase.lower(), disable = ['ner']):\n            token_vector.append(token.vector)\n        return (torch.tensor(token_vector) , torch.tensor(len(token_vector)) , torch.tensor(sample.Sentiment))\nclass Model(torch.nn.Module):\n    def __init__(self,input_dimensions , size = 128 , layer = 1 , output_shape = 5):\n        super().__init__()\n        self.seq = torch.nn.LSTM(input_dimensions , size , layer)\n        self.layer_one = torch.nn.Linear(size*layer , size)\n        self.activation_one = torch.nn.ReLU()\n        self.layer_two = torch.nn.Linear(size , size)\n        self.activation_two = torch.nn.ReLU()\n        self.shape_outputs = torch.nn.Linear(size , output_shape)\n        \n    def forward(self, inputs , lenghts):\n        number_of_batches = lenghts.shape[0]\n        packed_inputs = torch.nn.utils.rnn.pack_padded_sequence(inputs , lenghts , batch_first=True)\n        buffer , (hidden , cell) = self.seq(packed_inputs)\n        buffer = hidden.permute(1,0,2) # Reshaping\n        buffer = buffer.contiguous().view(number_of_batches , -1)\n        buffer = self.layer_one(buffer)\n        buffer = self.activation_one(buffer)\n        buffer = self.layer_two(buffer)\n        buffer = self.activation_two(buffer)\n        buffer = self.shape_outputs(buffer)\n        return buffer\n    \ndef collate(batch):\n    batch.sort(key = lambda x:x[1] , reverse = True) # sort by decresing lenths\n    sequences , lengths , sentiment = zip(*batch)\n    sequences = torch.nn.utils.rnn.pad_sequence(sequences , batch_first = True) # Padding the Sequence with Max Length\n    sentiment = torch.stack(sentiment)\n    lengths = torch.stack(lengths)\n    return sequences , lengths , sentiment\n","2782aff6":"sentiment = SentimentDataset()\n\nnumber_of_testing = int(len(sentiment)*0.05) \nnumber_of_training = len(sentiment) - number_of_testing\ntrain , test = torch.utils.data.random_split(sentiment , [number_of_training , number_of_testing])\n\n# Creating the DataLoader\ntrainloader = torch.utils.data.DataLoader(train , batch_size = 32 , shuffle = True , collate_fn = collate)\ntestloader = torch.utils.data.DataLoader(test , batch_size = 32 , shuffle = True , collate_fn = collate)\nfor batch in trainloader:\n    print(batch[0].shape , batch[1].shape , batch[2].shape)\n    print(batch[1][0])\n    break\n    \nmodel = Model(sentiment[0][0].shape[1])\noptimizer = torch.optim.Adam(model.parameters())\nloss_functions = torch.nn.CrossEntropyLoss()\n\nmodel.train() # Setting the model to Train\n\nfor epoch in range(15):\n    losses = []\n    for sequences , lengths , outputs in tqdm(trainloader):\n        optimizer.zero_grad()\n        results = model(sequences , lengths)\n        loss = loss_functions(results , outputs)\n        losses.append(loss.item())\n        loss.backward()\n        optimizer.step()\n    print(\"Loss {0}\".format(torch.tensor(losses).mean()))","7c0b1d2a":"results_buffer = []\nactual_buffer = []\nwith torch.no_grad():\n    model.eval()\n    for inputs , lenghts, outputs in testloader:\n        results = model(inputs,lenghts).argmax(dim = 1).numpy()\n        results_buffer.append(results)\n        actual_buffer.append(outputs)\n\nprint(classification_report(np.concatenate(actual_buffer) , np.concatenate(results_buffer)))","b6db225f":"class Generator(torch.nn.Module):\n    def __init__(self , context , features , channels):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            torch.nn.ConvTranspose2d(in_channels = context, out_channels=features*8, kernel_size = 4, stride=1, padding=0 ,bias=False), #https:\/\/pytorch.org\/docs\/stable\/nn.html#convtranspose2d\n            torch.nn.BatchNorm2d(features*8),\n            torch.nn.ReLU(True),\n            torch.nn.ConvTranspose2d(in_channels = features *8 , out_channels = features*4 , kernel_size = 4 , stride = 2 , padding = 1 , bias = True),\n            torch.nn.BatchNorm2d(features*4),\n            torch.nn.ReLU(True),\n            torch.nn.ConvTranspose2d(in_channels = features *4 , out_channels = features*2 , kernel_size = 4 , stride = 2 , padding = 1 , bias = True),\n            torch.nn.BatchNorm2d(features*2),\n            torch.nn.ReLU(True),\n            torch.nn.ConvTranspose2d(in_channels = features *2 , out_channels = features , kernel_size = 4 , stride = 2 , padding = 1 , bias = True),\n            torch.nn.BatchNorm2d(features),\n            torch.nn.ReLU(True),\n            torch.nn.ConvTranspose2d(in_channels = features , out_channels = channels , kernel_size = 4 , stride = 2 , padding = 1 , bias = True),\n            torch.nn.Tanh()\n        )\n    def forward(self , input):\n        return self.main(input)\n\nclass Discriminator(torch.nn.Module):\n    def __init__(self , features , channels):\n        super().__init__()\n        self.main = torch.nn.Sequential(\n            torch.nn.Conv2d(channels , features , 4, 2, 1 , bias = False),\n            torch.nn.LeakyReLU(0.2 , inplace = True),\n            torch.nn.Conv2d(features , features*2 , 4, 2, 1 , bias = False),\n            torch.nn.BatchNorm2d(features*2),\n            torch.nn.LeakyReLU(0.2 , inplace = True),\n            torch.nn.Conv2d(features*2 , features*4 , 4, 2, 1 , bias = False),\n            torch.nn.BatchNorm2d(features*4),\n            torch.nn.LeakyReLU(0.2 , inplace = True),\n            torch.nn.Conv2d(features*4 , features*8 , 4, 2, 1 , bias = False),\n            torch.nn.BatchNorm2d(features*8),\n            torch.nn.LeakyReLU(0.2 , inplace = True),\n            torch.nn.Conv2d(features*8 , 1 , 4, 1, 0 , bias = False), # It will give output as 0 or 1\n            torch.nn.Sigmoid()\n        )\n    def forward(self , inputs):\n        return self.main(inputs)\n\ndef weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weights.data , 0.0 , 0.2)\n    elif classname.find('Conv') != -1:\n        torch.nn.init.normal_(m.weights.data , 0.0 , 0.2)\n        torch.nn.init.constant_(m.bias.data , 0)\n\nclass GeneratorCustome(Generator):\n    def __init__(self , context , features , channels):\n        super.__init__(context , features , channels)\n        weights_init(self)\n        \nclass DiscriminatorCustome(Discriminator):\n    def __init__(self , features , channels):\n        super.__init__(features , channels)\n        weights_init(self)","a402e476":"batch_size = 128\ntransform = transforms.Compose([transforms.CenterCrop(64) , transforms.ToTensor(), transforms.Normalize(mean = (0.5,) , std = (0.5,))])\nmnist = torchvision.datasets.MNIST('.\/var' , download = True)\nreal = torchvision.datasets.MNIST('.\/var' , train = True , transform = transform)\nrealloader = torch.utils.data.DataLoader(real , batch_size = batch_size , shuffle = True)\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n    \nepochs = 16\n# Real or Fake\nreal_label = 1\nfake_label = 0\n\ncontext_size = 10\nfeatures = 32\nchannels = 1\n# Learning rate and beat1 for Optimizers\nlr = 0.0002\nbeta1 = 0.5\n\n# Bianry Loss Such as Real or Fake\ncriteria = torch.nn.BCELoss()\n\n# Random Number Generator\nfixed_noise = torch.randn(features , context_size , 1,1 , device = device)\n\n# List to Track Progress\nimg_list = []\nG_losses = []\nD_losses = []\n\nnetD = Discriminator(features , channels).to(device)\nnetG = Generator(context_size , features, channels).to(device)\n# Adam Optmiziers\noptimizerD = torch.optim.Adam(netD.parameters() , lr = lr , betas = (beta1 , 0.999))\noptimizerG = torch.optim.Adam(netG.parameters() , lr = lr , betas = (beta1 , 0.999))\n","b11dc4ff":"fake = netG(fixed_noise).detach().cpu()\nsamples = torchvision.utils.make_grid(fake , padding = 2 , normalize = True)\nplt.axes().imshow(samples.permute(1,2,0))","e3e392ce":"for epoch in range(epochs):\n    with tqdm(realloader , unit = 'batches') as progress:\n        for i , (data , _) in enumerate(realloader):\n            netD.zero_grad()\n            # Preparing Data\n            batch_size = data.shape[0]\n            real_data = data.to(device)\n            real_labels = torch.full((batch_size ,) , real_label , device  = device)\n            output = netD(real_data).view(-1)\n            # Loss Function for Real or Fake\n            errD_real = criteria(output , real_labels)\n            errD_real.backward()\n            # Creating Noise and Fake Labels \n            noise = torch.randn(batch_size , context_size , 1,1,device = device)\n            fake_labels = torch.full((batch_size ,) , fake_label ,device = device)\n            fake_data = netG(noise)\n            output = netD(fake_data).view(-1)\n            errD_fake = criteria(output , fake_labels)\n            errD_fake.backward(retain_graph = True)\n            errD = errD_fake + errD_real\n            optimizerD.step()\n            netG.zero_grad()\n            # How well is Discriminator is working\n            fake_labels.fill_(real_label)\n            output = netD(fake_data).view(-1)\n            errG = criteria(output , real_labels)\n            errG.backward()\n            # Update Optimizer\n            optimizerG.step()\n            # Saving Losses\n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n            progress.set_postfix(G_loss = torch.tensor(G_losses).mean(),\n                                D_loss = torch.tensor(D_losses).mean(),\n                                refresh = False)\n            progress.update()\n            G_losses.append(errG.item())\n            D_losses.append(errD.item())\n        with torch.no_grad():\n            fake= netG(fixed_noise).detach().cpu()\n        samples = torchvision.utils.make_grid(fake , padding = 2 , normalize = True)\n        img_list.append(samples)\n        \n","f7e9b523":"ims = plt.axes().imshow(img_list[0].permute(1,2,0))","8677f576":"ims = plt.axes().imshow(img_list[-1].permute(1,2,0))","fc2ecb63":"# Sentiment Analysis With LSTM and Word Embedding ","c0be9bd8":"# Loading Some Basic Libraries\n","cf9c62aa":"# Regression Problem ( House Pricing Dataset )","ec8dfe1f":"# Neural Network for Classification ","485b8da6":"# Create Neural Network in Torch","7edd1010":"# Creating More Complex Neural Network And Training Them","ea8fbb33":"# Implementing CNN With Deep Neural Network with VGG16 Model (Transfer Leraning)","21023832":"# Implementing CNN With Deep Neural Network with RESNET Model (Transfer Leraning)","cb772715":"# Implementing CNN With Deep Neural Network with ALEXNET Model","10175007":"# Creating Neural Netwok with Module Class in Pytorch","2b226b83":"# Sentiment Analysis with Bag of Words Problem","95daac1e":"# Image Classification with Pytorch on MNIST DataSet","07096cb0":"# Start By Creating Simple Simulation Of How Neural Network Looks.","dd4f3976":"# About Pytorch\n\nPyTorch is an open source machine learning library for Python and is completely based on Torch. It is primarily used for applications such as natural language processing. PyTorch is developed by Facebook's artificial-intelligence research group along with Uber's \"Pyro\" software for the concept of in-built probabilistic programming.\nAdvantages of PyTorch\n\nThe following are the **Advantages of PyTorch** \u2212\n* It is easy to debug and understand the code.\n* It includes many layers as Torch.\n* It includes lot of loss functions.\n* It can be considered as NumPy extension to GPUs.\n* It allows building networks whose structure is dependent on computation itself.\n\n### Topics Covered\n\nThis Kernel Covers following Topics :\n*   [Loading Some Basic Libraries](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Loading-Some-Basic-Libraries)\n*   [Simple Simulation Of How Neural Network Looks](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Start-By-Creating-Simple-Simulation-Of-How-Neural-Network-Looks.)\n*   [Create Neural Network in Torch](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Create-Neural-Network-in-Torch)\n*   [Ploting Some Major Activations Functions Avilable in Pytorch](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Ploting-Some-Major-Activations-Functions-Avilable-in-Pytorch)\n*   [Creating More Complex Neural Network And Training Them](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Creating-More-Complex-Neural-Network-And-Training-Them)\n*   [Creating Neural Netwok with Module Class in Pytorch](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-networ-cnn-rnn-gank#Creating-Neural-Netwok-with-Module-Class-in-Pytorch)\n*   [Regression Problem ( House Pricing Dataset )](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Regression-Problem-(-House-Pricing-Dataset-))\n*   [Image Classification with Pytorch on MNIST DataSet](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Image-Classification-with-Pytorch-on-MNIST-DataSet)\n*   [Implementing CNN With Deep Neural Network with ALEXNET Model](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Implementing-CNN-With-Deep-Neural-Network-with-ALEXNET-Model)\n*   [Implementing CNN With Deep Neural Network with VGG16 Model (Transfer Leraning)](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Implementing-CNN-With-Deep-Neural-Network-with-VGG16-Model-(Transfer-Leraning))\n*   [Implementing CNN With Deep Neural Network with RESNET Model (Transfer Leraning)](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Implementing-CNN-With-Deep-Neural-Network-with-RESNET-Model-(Transfer-Leraning))\n*   [Sentiment Analysis with Bag of Words Problem](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Sentiment-Analysis-with-Bag-of-Words-Problem)\n*   [Sentiment Analysis With LSTM and Word Embedding](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Sentiment-Analysis-With-LSTM-and-Word-Embedding)\n*   [Implementing Deep Convolutional Generative Adversarial Network On MNIST Dataset](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan#Implementing-Deep-Convolutional-Generative-Adversarial-Network-On-MNIST-Dataset)\n\n### Prerequisite\n* Python\n* Data Manipulation(Beginners)\n* Some Idea Of Data Science\n\n### Video Course\nThis Tutorial is based on Series named **PyTorch Deep Learning in 7 Days**....[Click Here.](https:\/\/www.oreilly.com\/library\/view\/pytorch-deep-learning\/9781789135367\/)\n\n### Please Upvote the Kernel If You Like It\n### Other Links\n[Kernel Link](https:\/\/www.kaggle.com\/terminate9298\/pytorch-turorials-for-neural-network-cnn-rnn-gan\/)\n\n[My CV](http:\/\/cv.kaus98.ml\/)","d022be95":"# Implementing Deep Convolutional Generative Adversarial Network On MNIST Dataset","d9447dec":"# Ploting Some Major Activations Functions Avilable in Pytorch"}}