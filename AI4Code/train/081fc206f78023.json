{"cell_type":{"d56ce116":"code","85864ec5":"code","6af3cf4a":"code","81afabc2":"code","0e75c655":"code","79be94cc":"code","0e85c8f5":"code","8c42a6ae":"code","f56ef22e":"code","a9c8f132":"code","8e6e7c21":"code","294b62b5":"code","abd79bc9":"code","6d9d1a84":"code","f86da66d":"code","a65c9063":"code","4d479a07":"code","9ae0928b":"code","112e2aee":"code","04609d3b":"code","093c54ef":"markdown","8fffbcf0":"markdown","0fb9451b":"markdown","ca034cee":"markdown","6e6a0b8e":"markdown","a0b05287":"markdown","233d9ebd":"markdown","fa9aec67":"markdown","c67b7214":"markdown","1400bdc0":"markdown","f07f8524":"markdown","0f06ea6e":"markdown","639bf307":"markdown","fbb926f5":"markdown","1f37f6c0":"markdown","ccaff1a9":"markdown","408c6d68":"markdown","252ad949":"markdown","c6113323":"markdown","fefa051c":"markdown"},"source":{"d56ce116":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","85864ec5":"from os import listdir\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split","6af3cf4a":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Dropout, BatchNormalization, Concatenate, Dense\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.regularizers import l1, l2\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","81afabc2":"!pip install py7zr\n\nfrom py7zr import unpack_7zarchive\nimport shutil\nshutil.register_unpack_format('7zip', ['.7z'], unpack_7zarchive)\nshutil.unpack_archive('\/kaggle\/input\/cifar-10\/train.7z', '\/kaggle\/working')\n","0e75c655":"train_path = '\/kaggle\/working\/train\/'\n\ntrain_list = listdir(train_path)\n\nX_list = [0] * len(train_list)\n\nfor i_image in train_list:\n    num_image = int(i_image[:-4]) -1\n    imagen = Image.open(train_path + i_image)\n    imagen_numpy = np.uint8(imagen)\n    X_list[num_image] = imagen_numpy\n\nX = np.array(X_list)\n\nprint(X.shape)","79be94cc":"train_data = pd.read_csv('\/kaggle\/input\/cifar-10\/trainLabels.csv')\nonehot_encoder = OneHotEncoder()\ny = onehot_encoder.fit_transform(train_data[['label']])\ny = y.toarray()","0e85c8f5":"labels_df = train_data.groupby(['label']).count()\nlabels_matrix = y.sum(axis=0)\nprint(labels_df)\nprint(labels_matrix)","8c42a6ae":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.4)","f56ef22e":"X_train_normalized = X_train\/255.0\nX_train_normalized = X_train_normalized.astype('float32')\n\nneural_network = keras.Sequential([\n    keras.layers.Conv2D(64,(3,3),activation='relu',input_shape=(32,32,3)),\n    keras.layers.Flatten(),\n    keras.layers.Dense(64,activation='relu'),\n    keras.layers.Dense(32,activation='relu'),\n    keras.layers.Dense(10,activation='softmax')\n])\n\nneural_network.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])\n\nneural_network.fit(X_train_normalized,y_train, epochs=50)\n\nnn_loss, nn_score = neural_network.evaluate(X_test,y_test,verbose=2)\n\nprint(\"The accuracy of the neural network on the test data is {0:4.3f}\".format(nn_score,))","a9c8f132":"def create_neural_network(train_array_iterator,test_array_iterator,num_filters_input,num_neurons_input,num_conv_input,num_maxpool_input,overfitting,label=None,verbosity=0):\n\n    # Cleanup inputs\n\n    num_filters = round(num_filters_input)\n    num_neurons = round(num_neurons_input)\n    num_conv = round(num_conv_input)\n    num_maxpool = round(num_maxpool_input)\n    if overfitting == 'l1':\n        regularizer = l1(0.01)\n    elif overfitting == 'l2':\n        regularizer = l2(0.01)\n    else:\n        regularizer = None\n\n    # Build the neural network\n\n    neural_network = Sequential()\n    neural_network.add(Conv2D(num_filters,(3,3),activation='relu',input_shape=(32,32,3)))\n    for i_maxpool in range(num_maxpool):\n        try:\n            for i_conv in range(num_conv):\n                neural_network.add(Conv2D(num_filters, (3, 3), activation='relu', padding='same',\n                                          kernel_regularizer=regularizer, bias_regularizer=regularizer))\n            neural_network.add(MaxPooling2D(pool_size=(2,2)))\n            if overfitting == 'Dropout':\n                neural_network.add(Dropout(0.25))\n            elif overfitting == 'Batch_Normalization':\n                neural_network.add(BatchNormalization(momentum=0.99))\n        except:\n            break\n\n    neural_network.add(keras.layers.Flatten())\n    neural_network.add(keras.layers.Dense(num_neurons,activation='relu'))\n    neural_network.add(keras.layers.Dense(round(num_neurons\/2),activation='relu'))\n    neural_network.add(keras.layers.Dense(10,activation='softmax'))\n    neural_network.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n    print('Created a neural network with the following structure:')\n    print(neural_network.summary())\n    \n    # Callbacks\n\n    rlr = keras.callbacks.ReduceLROnPlateau(patience=15, verbose=1)\n    es = keras.callbacks.EarlyStopping(patience=35, restore_best_weights=True, verbose=1)\n    print('Fitting the neural network')\n    historia = neural_network.fit_generator(train_array_iterator, validation_data=test_array_iterator, epochs=50, verbose=verbosity, callbacks=[rlr,es])\n    \n    # Plot\n    \n    #plt.figure(figsize=[15,15])\n    plt.figure()\n    if label is None:\n        plt.plot(historia.history['accuracy'], label='Train accuracy')\n        plt.plot(historia.history['val_accuracy'], label='Test accuracy')\n    else:\n        plt.plot(historia.history['accuracy'], label='Train accuracy ({0})'.format(label))\n        plt.plot(historia.history['val_accuracy'], label='Test accuracy ({0})'.format(label))\n    plt.legend()\n    \n    return neural_network\n","8e6e7c21":"train_preprocessor = ImageDataGenerator(rescale=1.0\/255.0)\ntest_preprocessor = ImageDataGenerator(rescale=1.0\/255.0)\n\ntrain_iterator = train_preprocessor.flow(X_train,y_train,batch_size=256)\ntest_iterator = test_preprocessor.flow(X_test,y_test,batch_size=256)\n\nred_neuronal = create_neural_network(train_iterator,test_iterator,64,32,2,2,None)\n","294b62b5":"train_preprocessor = ImageDataGenerator(rescale=1.0\/255.0)\ntest_preprocessor = ImageDataGenerator(rescale=1.0\/255.0)\n\ntrain_iterator = train_preprocessor.flow(X_train,y_train,batch_size=256)\ntest_iterator = test_preprocessor.flow(X_test,y_test,batch_size=256)\n\nnormalization = {}\n\nfor i_normalizer in [None,'l1','l2','Dropout','Batch_normalization']:\n#    train_preprocessor = ImageDataGenerator(rescale=1.0 \/ 255.0)\n#    test_preprocessor = ImageDataGenerator(rescale=1.0 \/ 255.0)\n#    train_iterator = train_preprocessor.flow(X_train, y_train, batch_size=256)\n#    test_iterator = test_preprocessor.flow(X_test, y_test, batch_size=256)\n\n    this_neural_network = create_neural_network(train_iterator,test_iterator,64,32,2,2,i_normalizer,i_normalizer)\n\n#    test_preprocessor = ImageDataGenerator(rescale=1.0 \/ 255.0)\n#    test_iterator = test_preprocessor.flow(X_test, y_test, batch_size=256)\n    nn_loss, nn_score = this_neural_network.evaluate_generator(test_iterator,verbose=2)\n    normalization[i_normalizer] = nn_score\n\nprint(normalization)","abd79bc9":"test_nn = {}\ntest_score = {}\n\nfor i_normalizer in [None, 'Dropout']:\n    for i_convolution in range(1,4):\n        for i_maxpool in [1,2]:\n            for i_neurons in [8,32]:\n                for i_filters in [64,128]:\n                    indice = '{0}, {1}, {2}, {3}, {4}'.format(i_normalizer,i_convolution,i_maxpool,i_neurons,i_filters)\n                    texto = 'Creating the neural network for parameters: ' + indice\n                    print(texto)\n                    this_neural_network = create_neural_network(train_iterator, test_iterator, i_filters, i_neurons,\n                                                                i_convolution, i_maxpool, i_normalizer, indice)\n                    nn_loss, nn_score = this_neural_network.evaluate_generator(test_iterator, verbose=2)\n                    print('Test score: {0}'.format(nn_score))\n                    test_score[indice] = nn_score\n                    test_nn[indice] = this_neural_network\n\n","6d9d1a84":"best_nn = sorted(test_score.items(), key=lambda x : x[1], reverse=True)\n\nprint(best_nn)\n\nbest_nn_indices = [ x[0] for x in best_nn ]\n\nbest_5_nn_indices = best_nn_indices[0:5]","f86da66d":"the_input = keras.Input(shape=(32,32,3))\n\nnn_legs = []\nfor i_key in best_5_nn_indices:\n    output_i = test_nn[i_key](the_input)\n    nn_legs.append(output_i)\n\nx = Concatenate()(nn_legs)\nx = Dense(1024,activation='relu')(x)\nthe_output = Dense(10,activation='softmax')(x)\n\nmerged_model = Model(inputs=the_input,outputs=the_output)\nmerged_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\nrlr = keras.callbacks.ReduceLROnPlateau(patience=15, verbose=1)\nes = keras.callbacks.EarlyStopping(patience=35, restore_best_weights=True, verbose=1)\nmerged_model.fit_generator(train_iterator, validation_data=test_iterator, epochs=50, callbacks=[rlr,es])","a65c9063":"shutil.unpack_archive('\/kaggle\/input\/cifar-10\/test.7z', '\/tmp\/testfiles')","4d479a07":"! ls \/tmp\/testfiles\/test | wc -l","9ae0928b":"predict_preprocessing = ImageDataGenerator(rescale=1.0 \/ 255.0)\npredict_iterator = predict_preprocessing.flow_from_directory(directory='\/tmp\/testfiles',target_size=(32,32),color_mode='rgb',class_mode=None,batch_size=256,shuffle=False)\n\nprediccion = merged_model.predict_generator(predict_iterator)\n","112e2aee":"label_prediction = onehot_encoder.inverse_transform(prediccion)","04609d3b":"prediction_df = pd.DataFrame(label_prediction)\n\n# In order to get the indices right, we must take into account that flow_from_directory reads files on a alphabetical order. This means that after '1' comes '10' and not two.\n# For that reason we need to play around a little bit with the indices, to sort them alphabetically.\nlabels_text_sorted = sorted([ str(x) for x in range(1, prediction_df.shape[0] + 1)])\nlabels_as_integers = [int(x) for x in labels_text_sorted]\nprediction_df.insert(0,'Indice',labels_as_integers)\nprediction_df = prediction_df.sort_values(by=['Indice'])\n\nprediction_df.to_csv('\/kaggle\/working\/prediction.csv',\n                     header=['id','label'],index=False)","093c54ef":"What have we done? The very first thing is to normalize the input: we want the data of the numpy array going from 0 to 1, instead of from 0 to 255, because neural networks work better with data between 0 and 1. The second line is more technical: we want to optimize the storage of the numpy array using single precision 32-bit float (`float32`) instead of the default `float64`, so the array size in memory is 50 % less.\n\nThen we started a neural network with `keras.Sequential()`, that has a [CNN](https:\/\/keras.io\/layers\/convolutional\/#conv2d) with 64 $3 \\times 3$ matrix filters (see [this](https:\/\/towardsdatascience.com\/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac#e051) to understand what matrix filters are), then we [flatten](https:\/\/keras.io\/layers\/core\/#flatten) it to have a \"classical\" one-dimensional neural network structure, and finally two layers of neurons (one with 64 and one with 32) and a last one, that will be the output layer, *i. e.* it will tell us how likely is that the image is a horse or a dog, or something else. Following the example in the previous section, an output like (0.8,0.2,0,...,0,0) means that the image has a 80 % probability of being a car and 20 % of being a bird.\n\n\nNow let's build something more sophisticated and generalizable. We will crate a function that generate a deeper neural network, whose architecture will depend on the input parameters.\n\n## Systematic search for the best neural network","8fffbcf0":"# Reading training data\n\nWe finally have all the necessary libraries loaded and the training images in our working directory! We can start doing some neural network stuff.\n\nBut first we have to get the images in a format that a neural network can handle. How can we do that? With the PIL library that we have already loaded.\n\nImages in a computer are nothing but an $a \\times b$ matrix of pixels. Each pixel in the position $(x_a,y_b)$ has three values from 0 to 255 that specify what amount of [red, green and blue](https:\/\/simple.wikipedia.org\/wiki\/Primary_color) each pixel has. With that information, an image can reproduce up to 255$^\\text{3}$ = 16 million colors.\n\nLet's extract this color information and transform it into some mathematical structure that the neural network can handle: a numpy array! As previously mentioned, the PIL library is the way to go.\n\nAt the end we will get an array of the form $(n_{images},a,b,3)$ (3 because we have three color channels: red (R), green (G) and blue (B)).","0fb9451b":"From the results, we see that Dropout gives the best accuracy in the test set. We are going to keep also no regularization, because it can actually be detrimental for simple models that are already biased (since it introduces another source of bias).\n\nNow let's do a systematic search for the best neural network architecture, trying out different number of CNN layers, MaxPool layers, and so on. For that it will be very useful the `create_neural_network` function that we have defined.","ca034cee":"We see that the test accuracy ranges from 0.78 to 0.10. In particular the best five range from 0.78 to 0.74... is it possible to somehow use the results from the five best neural networks to obtain a better result?\n\nIndeed! We could do something as simple as take the average of the output layers of the 5 neural networks. This way errors from each neural network (hopefully) will compensate, and we will get an slightly more accurate neural network. But we can do it better. We can create a neural network!\n\nThis is what we are going to do to further improve the accuracy. Let's add another layer that connects the best 5 neural networks and train it! (see image below).\n\n![Neural_network_draw_low_res.png](attachment:Neural_network_draw_low_res.png)","6e6a0b8e":"Now let's use it to predict the test data! For doing that we are going to try a new tool. Instead of manually creating a numpy array and transform it into an iterator, we are going to use flow_from_directory, that will create the iterator directly for us. It will be quite handy, because now we have 300000 images. If we create a numpy array, it will be 10 times larger and we may have memory problems.","a0b05287":"# Merged neural network with the best architectures","233d9ebd":"The prediction array has now a numpy array shape. We have to invert the one hot encoder to get back which class the neural network is predicting","fa9aec67":"Great! We have improved it by almost 3 %!","c67b7214":"We are going to train our neural network with batches. That means that instead of loading all the 30000 images in memory and use all of them at the same time each epoch, we will load a batch of photos in memory, train the neural network for this batch of photos, load a new batch of photos and train it for this new batch, and so on, until all of the photos are used for training. This has advantages and disadvantages. A great advantage is that we don't need to keep in memory all our photos (imagine if we are training 2 million of 800x600 photos, these are several TB, and no computer will be able to handle so much RAM memory. A disadvantage is that we lose some of the performance of GPUs because communication between GPU and CPU. Another thing is that we may get quite different results if we train on batches ([and this is sometimes good](https:\/\/towardsdatascience.com\/simple-introduction-to-neural-networks-ac1d7c3d7a2c#7b42)).\n\nMoreover, we can do more things while we create our batches. In this case we are normalizing the inputs (i. e. making sure that our inputs go from 0 to 1).","1400bdc0":"OK, now we have the \"independent variables\" X; but we need y, *i. e*. what are the labels associated to each image. For that we will read the trainLabels.csv file. This file contains the labels as text, and this is not what we want. We want a vector whose size is equal to the number of different labels (10 in CIFAR-10): each element of the array correspond to a label, and it will be 1 if this is the actual label of the image and zero otherwise. For example: \"car\" = (1,0,0,...,0,0), \"bird\" = (0,1,0,...,0,0). Sklearn has a tool for doing exactly that: [OneHotEncoder](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html).","f07f8524":"Now we want to check if all labels are more or less equally probable. Why is this important? Because we can have a *class imbalance*, and that can distort our model. For example, imagine that we have a set of dogs and cats, and 90 % of the images are dogs. A neural network that consider that *every* picture is a dog will still have a 90 % of accuracy.","0f06ea6e":"# Creation of a neural network\n\n## Quick and dirty neural network\n\nOK, I guess at this point you are kind of bored. When do we do some cool stuff? OK, I won't bore you anymore. Let's start now! The following code is quick and dirty, but it is very simple to understand.","639bf307":"And the TensorFlow and Keras libraries","fbb926f5":"This is something definitely more complex than our previous neural network, so let me explain it what this function does in more detail.\n\n1) Cleanup inputs. Essentially it makes sure that num_filters and num_neurons are integer numbers, and are passed as such to Conv2D and Dense layers. It also creates a [regularizer object](https:\/\/keras.io\/regularizers\/) that may be l1 or l2. Regularizers prevent overfitting (see below); l1 and l2 do that by penalizing [large weights](https:\/\/www.machinecurve.com\/index.php\/2020\/01\/21\/what-are-l1-l2-and-elastic-net-regularization-in-neural-networks\/). See more details here for [l1](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#lasso) and here for [l2](https:\/\/scikit-learn.org\/stable\/modules\/linear_model.html#ridge-regression-and-classification). If you are a Bayesian fan, as myself, a very intuitive and more mathematically complex way of introducing l2 regularization is explained in this [article](https:\/\/katbailey.github.io\/post\/from-both-sides-now-the-math-of-linear-regression\/).\n\n2) Build the neural network. This essentially consist on a loop that create several multilayers consisting 2 CNN plus a [maxpooling](https:\/\/keras.io\/layers\/pooling\/#maxpooling2d) for decreasing the complexity. If we are trying to prevent overfitting using [dropout](https:\/\/keras.io\/layers\/core\/#dropout) or [batch normalization](https:\/\/keras.io\/layers\/normalization\/#batchnormalization), we include the layers here. We finish it with a flatten layer to return to a 1D neural network, plus two layers, plus the output layer.\n\n3) Finally we add a couple of [callbacks](https:\/\/keras.io\/callbacks\/). The callbacks change the convergence process on the fly, either by [reducing the learning rate](https:\/\/keras.io\/callbacks\/#reducelronplateau) or by [stopping](https:\/\/keras.io\/callbacks\/#earlystopping) it when the accuracy is not increasing anymore (even decreasing).\n\nThen we fit the neural network with batches (see below) and we get a graph of how accuracy improves (for our training and our testing set) as a function of time.","1f37f6c0":"And finally we generate the prediction csv file in order to submit the results.","ccaff1a9":"# Introduction\n\nNeural networks are a great mathematical tool for classification problems (available also for regression *i. e.* for getting a number\/set of numbers as a function of some inputs). They are very hyped at the moment, because they can create very complex non-linear functions that are able to do incredible things like distinguishing cats and dogs using the colors of the pixels of a digital photograph.\n\nTheir drawback is that they have a LOT of internal parameters, that means that you need thousands (if not millions) of input data to build a realistic model.\n\nIf you are not so familiarized with neural networks, read this great introduction from Matthew Stewart: https:\/\/towardsdatascience.com\/simple-introduction-to-neural-networks-ac1d7c3d7a2c\n\nActually, I advise you to read all the articles by [him](https:\/\/towardsdatascience.com\/@matthew_stewart) in Towards Data Science.\n\nFor images, there is a particular type of neural network that is very suitable: the convolutional neural network (CNN). Again, Matthew Stewart has a great [introduction about them](https:\/\/towardsdatascience.com\/simple-introduction-to-convolutional-neural-networks-cdf8d3077bac).\n\nWe will use all of this to try to predict the labels (a car, a cat, a frog...) of a list of images from the [CIFAR-10 dataset](https:\/\/en.wikipedia.org\/wiki\/CIFAR-10).\n\n\n# Preparation\n\nOK, let's prepare our Notebook for doing some neural network stuff. First, let's include the standard header of all Kaggle notebooks.","408c6d68":"Fortunately it is not our case, all our classes are balanced. Good! If not, don't worry, there are ways to deal with that.\n\nFinally, let's divide our labeled photographs in train and test sets. We won't use all the photographs for training, but instead only 60 % of it. The remaining will be used to test how the model performs in data that has not been used for training. We don't want a neural network that fits very good our training data, but it works horrible when we ask it to predict a photo that it hasn't seen before, i. e. we don't want [overfitting](https:\/\/en.wikipedia.org\/wiki\/Overfitting). For that, we will use the train set as a simple way of [cross validation](https:\/\/en.wikipedia.org\/wiki\/Cross-validation_(statistics%29 )","252ad949":"Now we have to unzip the images that we will use to train the neural network (inspired by https:\/\/www.kaggle.com\/faizanurrahmann\/cifar-10-object-classification-cnn-keras)","c6113323":"Great! Another neural network trained. See that in this case, the accuracy for the test set is close to 70 %, instead of 30 % for the quick an dirty one. But we can definitely improve the accuracy.\n\nA way of doing that is using regularization. It is advised to do so in order to prevent two typical problems of neural networks: [overfitting](https:\/\/www.analyticsvidhya.com\/blog\/2018\/11\/neural-networks-hyperparameter-tuning-regularization-deeplearning\/) and [exploding gradients](https:\/\/machinelearningmastery.com\/exploding-gradients-in-neural-networks\/).\n\nThere are several ways of doing normalization. We are going to use l1, l2, dropout and batch normalization.","fefa051c":"Let's start importing some useful libraries"}}