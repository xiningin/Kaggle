{"cell_type":{"1a6c1168":"code","21ce9644":"code","da6811c1":"code","f8a9e7c1":"code","520a7d90":"code","fdf6e611":"code","535b9924":"code","0f50205b":"code","7fb2896d":"code","b83ca554":"code","737c8855":"code","7bc7e87c":"code","051b6e07":"code","1e8bd828":"code","fea1735c":"code","9f784413":"code","70060880":"code","021ddf9a":"code","c2b22585":"code","2e44d4ce":"code","97021004":"code","eff9b1d9":"code","0d93f178":"code","94088ec7":"markdown"},"source":{"1a6c1168":"import pandas as pd\nimport numpy as np\nimport json\nimport re\nimport torch.nn as nn \nimport torch\nfrom transformers import BertTokenizer\nfrom nltk.corpus import stopwords\nimport os","21ce9644":"MAX_SEQ=500","da6811c1":"train_path = '..\/input\/coleridgeinitiative-show-us-the-data\/train.csv'\ntrain = pd.read_csv(train_path)\ntrain\n","f8a9e7c1":"train['cleaned_label'] = train['cleaned_label'].apply(lambda x : x.strip())\nknown_labels = np.unique ( list( train['cleaned_label'].values ))","520a7d90":"'''known_labels_final=[]\nfor label1 in known_labels:\n    ignore=False\n    for label2 in known_labels:\n        \n        if (label1==label2):\n            continue\n        elif ( label1 in label2):\n            ignore=True\n            print( \"ignoring\", label1,\"coz of\", label2 )\n            break\n    if ( ignore == False ) :\n        known_labels_final.append(label1)\n\nknown_labels_final'''\n     ","fdf6e611":"test_path = '..\/input\/coleridgeinitiative-show-us-the-data\/sample_submission.csv'\npaper_test_folder = '..\/input\/coleridgeinitiative-show-us-the-data\/test'\ntest = pd.read_csv(test_path)\ntest=test.set_index(['Id'])\ntest","535b9924":"from tqdm import tqdm\n\nstopwords_set = set(stopwords.words('english')) \ndef clean_text(input_words):\n    #return [   re.sub('[^A-Za-z0-9]+', ' ', str(r).lower()).strip() for r in input_words if not r.lower() in stopwords_set  ]\n    return [   re.sub('[^A-Za-z0-9\\[\\]]+', ' ', str(r).lower()).strip() for r in input_words  ]\n\n\n\ndef process_row_with_bert(words, tokenizer, stopwords_set):  \n        \n    encoded_input_word_list = clean_text(words)\n    encoded_input_word_list = list(map(tokenizer.convert_tokens_to_ids, tokenizer.tokenize ( \" \".join(encoded_input_word_list)) ))\n    \n\n    if ( len(encoded_input_word_list) < MAX_SEQ ):\n        encoded_input_word_list = encoded_input_word_list + list(np.zeros( MAX_SEQ - len(encoded_input_word_list)) )\n    else :\n        encoded_input_word_list = encoded_input_word_list[0:MAX_SEQ]\n    \n     \n    return encoded_input_word_list\n\ndef process_data_with_bert (df, tokenizer) :\n    max_word_count = 0 \n    text_array = [] \n    code_array=[]\n     \n    stopwords_set = set(stopwords.words('english')) \n    \n    df_final = pd.DataFrame(columns = ['Id', 'text'])\n    \n    for index, id in tqdm ( enumerate ( df.index) )  :\n        with open(paper_test_folder+'\/'+ id +'.json', 'r') as f:\n                #Load the json \n                paper = json.load(f)\n                \n                #Convert the relevant sentences into a single paragraph\n                section_sentences =[]\n                for section_index in range ( 0, len(paper)):\n                    section_sentences = section_sentences +  re.split('[.;\\n\u2022]',paper[section_index].get('text'))   \n                section_sentences = \" [sep] \".join(section_sentences) \n                sentence_words = section_sentences.split() + ['[sep]']\n\n                # Chunk the paragraph into size of 500. Todo:Relook at this \n                n = int(0.7*MAX_SEQ) \n                word_chunks = [sentence_words[i:i+n+20] for i in range(0, len(sentence_words), n)] \n                word_chunks[-1] = word_chunks[-1] + list(np.empty( MAX_SEQ - len(word_chunks[-1]), dtype=str) )\n                     \n                # Add the words in the labels to the output wordlist\n                #labels = df.loc[id]\n\n                # Encode\/Embed the words for processing\n                df_new = pd.DataFrame(columns = ['Id', 'text', 'known_labels'])\n                for i in range ( 0, len(word_chunks)):\n                    encoded_input_word_list = process_row_with_bert(word_chunks[i], tokenizer, stopwords_set )\n                    word_chunk_sentence = \" \".join( clean_text( word_chunks[i] ) ) \n                    labels = [known_label for known_label in known_labels  if \" \".join(clean_text( known_label.split() )) in word_chunk_sentence.lower() ] \n                    df_temp = pd.DataFrame ({\"Id\":id, \"org_text\": word_chunk_sentence, \"text\":[encoded_input_word_list], \"known_labels\": \"|\".join(labels) }, index=[i])\n                    df_new =df_new.append(df_temp, ignore_index=True)\n                    #print(df_new)\n\n        df_final = df_final.append(df_new, ignore_index=True)\n\n    return df_final","0f50205b":"import random \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n#tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\ntokenizer = BertTokenizer.from_pretrained(\"..\/input\/pretrained-bert-including-scripts\/uncased_L-12_H-768_A-12\/uncased_L-12_H-768_A-12\/\")\ntokenizer.add_tokens('[cls]')\ntokenizer.add_tokens('[sep]')\n\ndf_test = process_data_with_bert(test, tokenizer)  \n   ","7fb2896d":"df_test","b83ca554":"df_test[ df_test['known_labels'] != \"\"]","737c8855":"import torch.nn as nn \n\nclass FFN(nn.Module):\n    def __init__(self, state_size=200):\n        super(FFN, self).__init__()\n        self.state_size = state_size\n\n        self.lr1 = nn.Linear(state_size, state_size)\n        self.relu = nn.ReLU()\n        self.lr2 = nn.Linear(state_size, state_size)\n        self.dropout = nn.Dropout(0.2)\n    \n    def forward(self, x):\n        x = self.lr1(x)\n        x = self.relu(x)\n        x = self.lr2(x)\n        return self.dropout(x)\n\ndef future_mask(seq_length):\n    future_mask = np.triu(np.ones((seq_length, seq_length)), k=1).astype('bool')\n    return torch.from_numpy(future_mask)\n\nclass ColleridgeTransformerModel(nn.Module):\n\n    def __init__(self, embed_dim = 200, max_seq=MAX_SEQ, input_vocab_len=30523 , output_vocab_len= 30523): #HDKIM 100->MAX_SEQ\n        super(ColleridgeTransformerModel, self).__init__()\n        self.embed_dim = embed_dim\n        self.embedding = nn.Embedding(input_vocab_len+1, embed_dim)\n        self.pos_embedding = nn.Embedding(max_seq, embed_dim)\n        self.e_embedding = nn.Embedding(output_vocab_len+1, embed_dim)\n        self.multi_att = nn.MultiheadAttention(embed_dim=embed_dim, num_heads=8, dropout=0.2)\n        self.dropout = nn.Dropout(0.2)\n        self.layer_normal = nn.LayerNorm(embed_dim) \n        self.ffn = FFN(embed_dim)\n        self.pred = nn.Linear(embed_dim, output_vocab_len+1)\n        self.softmax = nn.LogSoftmax(dim=2)\n    \n    def forward(self, x, generated_words):\n        device = x.device       \n        \n        # Embed the question\n        x = self.embedding(x)\n        \n        # Calculate the position id and embed it\n        pos_id = torch.arange(x.size(1)).unsqueeze(0).to(device)\n        \n        #Add it to the poisition id\n        pos_x = self.pos_embedding(pos_id)\n        x = x + pos_x\n        \n        # This is the target_id\/query ( right shifted input )\n        e = self.e_embedding(generated_words) \n        pos_e_id = torch.arange(e.size(1)).unsqueeze(0).to(device)\n        pos_e = self.pos_embedding(pos_e_id)\n        e = e + pos_e\n         \n        # Send the query key and value to the multi attention network \n        x = x.permute(1, 0, 2) # x: [bs, s_len, embed] => [s_len, bs, embed]\n        att_output, att_weight = self.multi_att(x, x, x)\n        x = self.layer_normal(att_output + x)\n        \n        \n        for i in range ( 0, 1):\n            \n            e = e.permute(1, 0, 2)\n              \n            att_mask = future_mask(e.size(0)).to(device) \n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, e, e, attn_mask=att_mask)\n\n            e = self.layer_normal(att_output + e)\n\n            # Send the query key and value to the multi attention network \n            att_output, att_weight = self.multi_att(e, x, x)\n\n            # Normalization \n            att_output = self.layer_normal(att_output + e)\n\n            # Reshape the output\n            att_output = att_output.permute(1, 0, 2) # att_output: [s_len, bs, embed] => [bs, s_len, embed]\n\n            # Send the output to FFN.\n            e = self.ffn(att_output)\n\n            # \n            e = self.layer_normal(e + att_output)\n        \n        # \n        e = self.pred(e)\n        e = self.softmax(e)\n        e = e.permute(0, 2, 1)\n        return e, att_weight","7bc7e87c":"xfrmer_model = ColleridgeTransformerModel(  )\nxfrmer_model.load_state_dict(torch.load(\"..\/input\/coleridgemodel\/xform-463-fold0.model\", map_location=torch.device(device) ))\nxfrmer_model.eval()","051b6e07":"from torch.utils.data import Dataset, DataLoader\n\n\nclass ColleridgeDatasetEval(Dataset):\n    def __init__(self, df, max_seq=MAX_SEQ):\n        super(ColleridgeDatasetEval, self).__init__()\n        self.max_seq = max_seq\n        self.samples = df\n\n    def __len__(self):\n        return len(self.samples.index)\n    \n    def __getitem__(self, index):\n        x = torch.tensor(self.samples.iloc[index]['text'], dtype=torch.int, device=device).view(-1, 1)\n        y = torch.tensor(np.zeros(MAX_SEQ), dtype=torch.int, device=device).view(-1, 1)\n        y[0] = 30522\n        target_id = y[:-1]\n        label = self.samples.iloc[index]['known_labels']\n        \n        return  x.squeeze(-1), target_id.squeeze(-1), label, self.samples.iloc[index]['org_text']","1e8bd828":"dataset = ColleridgeDatasetEval(df_test)\ndataloader = DataLoader(dataset, batch_size=1, shuffle=False)\n\nnum=-1\nxfrmer_model.to(device)\n\ndf_eval = pd.DataFrame(columns = ['id', 'predicted', 'known_labels'])\nfor item in dataloader:\n    stop = False \n    i = 0\n    num = num+1\n    while ( i < 498 ):\n        output,_ = xfrmer_model( item[0].to(device).long(),item[1].to(device).long()  )\n        y = torch.argmax( output , dim=1)\n        z = y[0][i]\n        item[1][0][i+1]=z\n        i = i+1\n        if ( z == 0.0):\n            break\n    y = [30522] + y.tolist()[0]\n    \n    predicted_text = tokenizer.convert_tokens_to_string (tokenizer.convert_ids_to_tokens( [int(z) for z in y if z not in [30522, 0.0, 0]] ))\n    complete_text= item[3][0]\n\n    for index, label in enumerate ( predicted_text.split('[sep]') )   :  \n            if(label in complete_text and label !=\"\"):\n                    #workaround-tobe fixed later\n                    '''if( label not in item[2] ):\n                        print(label)'''\n                    \n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [label], \"known_labels\": [label]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n            else:\n                    #workaround-tobe fixed later\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [\"\"] , \"known_labels\": [\"\"] }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n            \n    \n    '''if(predicted_text==\"[sep]\"):\n        for known_label1 in item[2][0].split(\"|\"):\n            ignore=False\n            for known_label2 in item[2][0].split(\"|\"):\n                if( known_label1 in known_label2 and known_label1 != known_label2):\n                    #print(known_label1, known_label2)\n                    ignore=True\n                    continue\n\n            if (ignore==False):\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [known_label1], \"known_labels\": [known_label1]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n    else:\n    \n        for index, label in enumerate ( predicted_text.split('[sep]') )   :  \n\n            if(label in complete_text and label !=\"\"):\n                    #workaround-tobe fixed later\n                    df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [label], \"known_labels\": [label]  }, index=[i])\n                    df_eval =df_eval.append(df_temp, ignore_index=True)\n\n            elif label != \"\":\n                for known_label1 in item[2][0].split(\"|\"):\n                    ignore=False\n                    for known_label2 in item[2][0].split(\"|\"):\n                        if( known_label1 in known_label2 and known_label1 != known_label2):\n                            #print(known_label1, known_label2)\n                            ignore=True\n                            continue\n\n                    if (ignore==False):\n                            df_temp = pd.DataFrame ({ \"id\":df_test.iloc[num]['Id'],  \"predicted\": [known_label1], \"known_labels\": [known_label1]  }, index=[i])\n                            df_eval =df_eval.append(df_temp, ignore_index=True) \n            '''\n\ndf_eval","fea1735c":"df_eval[df_eval['known_labels']!=\"\"]","9f784413":"df_temp1=df_eval.groupby('id').apply(lambda row:  \"|\".join( np.unique( row['predicted'].values ) ) ) \ndf_temp2=df_eval.groupby('id').apply(lambda row:  \"|\".join(np.unique((\"|\".join(row['known_labels'].values)).split(\"|\")))  ) \n\n\ndf_final_result = pd.DataFrame(columns = ['Id', 'PredictionString'])\ndf_final_result['Id'] = df_temp1.index\ndf_final_result['PredictionString1'] = list(map(lambda i: \"|\".join( clean_text(df_temp1[i].split(\"|\")) ) , range(0,len(df_temp1))))\ndf_final_result['PredictionString2'] = list(map(lambda i: \"|\".join( clean_text(df_temp2[i].split(\"|\")) ) , range(0,len(df_temp2))))","70060880":"#df_final_result['PredictionString'] = list(map( lambda x: \"|\".join ( np.unique(x.split(\"|\"))[1:] ) , (df_final_result['PredictionString1']+\"|\"+df_final_result['PredictionString2']) ))\ndf_final_result['PredictionString'] = list(map( lambda x: \"|\".join ( np.unique(x.split(\"|\")) ) , (df_final_result['PredictionString1'] )))","021ddf9a":"#df_final_result['PredictionString'] =df_final_result['PredictionString'] + \"|\" + df_final_result['PredictionString'] ","c2b22585":"df_final_result['PredictionString'] .iloc[0]","2e44d4ce":"df_final_result['PredictionString'] =df_final_result['PredictionString'].apply(  lambda x: \"|\".join( string for string in x.split(\"|\")\\\n                                                                                    if string != \"\") )\n\n","97021004":"df_final_result['PredictionString'] =df_final_result['PredictionString']+\"|\"+df_final_result['PredictionString']","eff9b1d9":"df_final_result[['Id','PredictionString']]","0d93f178":"df_final_result[['Id','PredictionString']].to_csv('\/kaggle\/working\/submission.csv', index=False)","94088ec7":"This is the inference using the model created in the following notebook.\nhttps:\/\/www.kaggle.com\/sreejaej\/attention-is-all-you-need-score-of-0-532\n\n\n\n"}}