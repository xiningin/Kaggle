{"cell_type":{"24b025db":"code","8b44bf30":"code","d00d60ae":"code","eb21585b":"code","4b1b23c7":"code","fd0d5b9c":"code","db9f3866":"code","5926c6d9":"code","77bcccd5":"code","10f8e086":"code","17f9c192":"code","f1abfdc1":"code","dad34e56":"code","418a3de7":"code","4c9e9daa":"code","bbc3a397":"code","10140ea4":"code","7025c12f":"code","9ca05dc0":"code","e178a554":"code","d2bd32a9":"code","8d837dd0":"code","a46bf728":"code","255f9cc4":"code","f0fad3d4":"code","a90c1578":"code","e8a4b97b":"code","8a8b0de3":"code","7394ff2b":"code","058e19f1":"code","f8734882":"code","3afc6642":"code","a328b102":"code","7daac834":"code","cc803cfa":"code","222444b9":"code","31389ccf":"code","441e88f2":"code","a3619077":"code","9503f456":"code","5505a256":"code","f44e6008":"code","465a1105":"code","afab9ad2":"code","b05ad884":"code","8f6bf937":"code","0b9a9eb6":"code","e4434653":"code","b539fbc2":"code","0ee0797c":"code","dba9099f":"code","86f7cc95":"code","d12e89b5":"code","c30c4bc2":"code","8abce928":"code","359a6f44":"code","9fe408c6":"code","061eed7b":"code","76348a18":"code","c06afa78":"code","ad0735e5":"code","359dd4e5":"code","fa83537d":"code","51e7a77d":"code","9e4735fc":"code","ca404715":"code","0736d6bd":"code","f6b4f785":"code","01c5b945":"code","fab100aa":"code","f5b56838":"code","e2f2393d":"code","54ef346c":"code","49b998fc":"code","0913c60b":"code","41a34c47":"code","692d74d9":"code","67bae4e6":"code","efab1e41":"code","db2eb7fe":"code","226e02b4":"code","17e5e7e9":"code","2d834fc8":"code","e71541e9":"code","ca716101":"code","64ec4c49":"code","e6f52919":"code","06e21509":"code","a41dd447":"markdown","0fca609c":"markdown","ec4de5ab":"markdown","9a311bea":"markdown","dfa904b6":"markdown","1a28904f":"markdown","38cbd2d4":"markdown","d357c830":"markdown","eed5aa55":"markdown","66016005":"markdown","1a562faa":"markdown","77a2756e":"markdown","4d50fcaa":"markdown","6d235d05":"markdown","a85d2f0f":"markdown","7c7ec37a":"markdown","e3505786":"markdown","cf4e7b32":"markdown","25b807c0":"markdown","4c71c264":"markdown","d7f5350b":"markdown","a337d805":"markdown","e5a41116":"markdown","4a7e0c4b":"markdown","6249ee07":"markdown","831293b4":"markdown","e87542ce":"markdown","78202ec9":"markdown","4ea34e4c":"markdown","a9647a60":"markdown","b7ccacd6":"markdown","76daf573":"markdown"},"source":{"24b025db":"# packages\n\n# standard\nimport numpy as np\nimport pandas as pd\nimport time\n\n# plots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom statsmodels.graphics.mosaicplot import mosaic\n\n# missing values visualization\nimport missingno as msno\n\n# machine learning tools\nimport h2o\nfrom h2o.estimators import H2OGeneralizedLinearEstimator, H2ORandomForestEstimator, H2OGradientBoostingEstimator","8b44bf30":"# load data + first glance\ndf_train = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\ndf_sub = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/sample_submission.csv')\n\n# first glance (training data)\ndf_train.head()","d00d60ae":"# dimensions\nprint('Train Set:', df_train.shape)\nprint('Test Set :', df_test.shape)","eb21585b":"# structure\ndf_train.info()","4b1b23c7":"# show structure of missings\nmsno.matrix(df_train)\nplt.show()","fd0d5b9c":"# fix missings in cabin feature by dummy imputation\ndf_train.Cabin = df_train.Cabin.fillna('0000')\ndf_test.Cabin = df_test.Cabin.fillna('0000')","db9f3866":"# basic stats\nprint(df_train.Survived.value_counts())\ndf_train.Survived.value_counts().plot(kind='bar')\nplt.grid()\nplt.show()","5926c6d9":"features_num = ['Age', 'SibSp', 'Parch', 'Fare']","77bcccd5":"# basic summary stats\ndf_train[features_num].describe(percentiles=[0.01,0.1,0.25,0.5,0.75,0.9,0.99])","10f8e086":"# plot distribution of numerical features\nfor f in features_num:\n    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10,6), sharex=True)\n    ax1.hist(df_train[f], bins=30)\n    ax1.grid()\n    ax1.set_title(f)\n    # for boxplot we need to remove the NaNs first\n    feature_wo_nan = df_train[~np.isnan(df_train[f])][f]\n    ax2.boxplot(feature_wo_nan, vert=False)\n    ax2.grid()\n    ax2.set_title(f + '- boxplot')\n    plt.show()","17f9c192":"corr_pearson = df_train[features_num].corr(method='pearson')\ncorr_spearman = df_train[features_num].corr(method='spearman')\n\nplt.figure(figsize=(15,5))\nax1 = plt.subplot(1,2,1)\nsns.heatmap(corr_pearson, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Pearson Correlation')\n\nax2 = plt.subplot(1,2,2, sharex=ax1)\nsns.heatmap(corr_spearman, annot=True, cmap='RdYlGn', vmin=-1, vmax=+1)\nplt.title('Spearman Correlation')\nplt.show()","f1abfdc1":"# pairwise scatter plot of numerical features\nt1 = time.time()\nsns.pairplot(df_train[features_num],\n             diag_kws = {'alpha': 1.0},\n             plot_kws = {'alpha': 0.1})\nplt.show()\nt2 = time.time()\nprint('Elapsed time:', np.round(t2-t1,2))","dad34e56":"features_cat = ['Pclass', 'Name', 'Sex', 'Ticket', 'Cabin', 'Embarked']","418a3de7":"# explicit conversions\ndf_train.Pclass = df_train.Pclass.astype('object')","4c9e9daa":"# summary stats\ndf_train[features_cat].describe(include='all')","bbc3a397":"features_cat_4plot = ['Pclass', 'Sex', 'Embarked']","10140ea4":"# plot distribution of categorical features\nfor f in features_cat_4plot:\n    plt.figure(figsize=(8,4))\n    df_train[f].value_counts().plot(kind='bar')\n    plt.title(f)\n    plt.grid()\n    plt.show()","7025c12f":"features_cat_too_many = ['Name', 'Ticket', 'Cabin']","9ca05dc0":"# show frequency counts for the features with many levels\nfor f in features_cat_too_many:\n    print('FEATURE', f, ':')\n    print(df_train[f].value_counts())\n    print()","e178a554":"# prefix of cabin could be useful\ndf_train['CabinPrefix'] = df_train.Cabin.apply(lambda x : x[0])\ndf_test['CabinPrefix'] = df_test.Cabin.apply(lambda x : x[0])\n\ndf_train.CabinPrefix.value_counts()","d2bd32a9":"# check for test set as well\ndf_test.CabinPrefix.value_counts()","8d837dd0":"df_train['FirstName'] = df_train.Name.map(lambda x: x.split(', ')[1])\ndf_test['FirstName'] = df_test.Name.map(lambda x: x.split(', ')[1])","a46bf728":"df_train.FirstName.value_counts()","255f9cc4":"df_test.FirstName.value_counts()","f0fad3d4":"df_train['LastName'] = df_train.Name.map(lambda x: x.split(',')[0])\ndf_test['LastName'] = df_test.Name.map(lambda x: x.split(',')[0])","a90c1578":"df_train.LastName.value_counts()","e8a4b97b":"df_test.LastName.value_counts()","8a8b0de3":"# basic stats for numerical features for training\ndf_train[features_num].describe()","7394ff2b":"# and the same for test set\ndf_test[features_num].describe()","058e19f1":"# compare age distributions\nplt.figure(figsize=(10,4))\nplt.hist(df_train.Age, bins=20, alpha=0.5, label='Train')\nplt.hist(df_test.Age, bins=20, alpha=0.5, label='Test')\nplt.title('Age - Train vs Test')\nplt.legend()\nplt.grid()\nplt.show()","f8734882":"# compare gender distribution\nprint('Train Set:')\nprint(df_train.Sex.value_counts(normalize=True))\nprint()\nprint('Test Set:')\nprint(df_test.Sex.value_counts(normalize=True))","3afc6642":"# let's first add a binned version of age\ndf_train['Age_bin10'] = pd.cut(df_train.Age, [0,10,20,30,40,50,60,70,80,90])\ndf_test['Age_bin10'] = pd.cut(df_test.Age, [0,10,20,30,40,50,60,70,80,90])\n\nplt.figure(figsize=(16,4))\nax1 = plt.subplot(1,2,1)\nfoo = df_train.Age_bin10.value_counts().sort_index()\nplt.bar(x=foo.index.astype(str), height=foo.values)\nplt.grid()\nplt.title('Age binned - Train Set')\n\nax2 = plt.subplot(1,2,2, sharex=ax1, sharey=ax1)\nfoo = df_test.Age_bin10.value_counts().sort_index()\nplt.bar(x=foo.index.astype(str), height=foo.values)\nplt.grid()\nplt.title('Age binned - Test Set')\nplt.show()","a328b102":"# calc cross tables Sex\/Age[binned]\ntab_sex_age_train = pd.crosstab(df_train.Sex, df_train.Age_bin10)\ntab_sex_age_test = pd.crosstab(df_test.Sex, df_test.Age_bin10)\n\n# and visualize\nplt.figure(figsize=(14,7))\nax1 = plt.subplot(2,1,1)\nsns.heatmap(tab_sex_age_train, cmap='Blues', \n            annot=True, fmt='d',\n            vmin=0, vmax=30000,\n            linecolor='black',\n            linewidths=0.1)\nplt.title('Age\/Sex - Train Set')\n\nax2 = plt.subplot(2,1,2)\nplt.subplots_adjust(hspace=0.35)\nsns.heatmap(tab_sex_age_test, cmap='Blues',\n            annot=True, fmt='d',\n            vmin=0, vmax=30000,\n            linecolor='black',\n            linewidths=0.1)\nplt.title('Age\/Sex - Test Set')\nplt.show()","7daac834":"plt.figure(figsize=(14,4))\nax1 = plt.subplot(1,2,1)\nfoo = df_train.Pclass.value_counts().sort_index()\nplt.bar(x=foo.index.astype(str), height=foo.values)\nplt.grid()\nplt.title('Pclass - Train Set')\n\nax2 = plt.subplot(1,2,2, sharex=ax1, sharey=ax1)\nfoo = df_test.Pclass.value_counts().sort_index()\nplt.bar(x=foo.index.astype(str), height=foo.values)\nplt.grid()\nplt.title('Pclass - Test Set')\nplt.show()","cc803cfa":"# plot target vs BINNED numerical features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in ['Age', 'Fare']:\n    # add binned version of each numerical feature first\n    new_var = f + '_bin'\n    df_train[new_var] = pd.qcut(df_train[f], 10)\n    # then create mosaic plot\n    plt.rcParams['figure.figsize'] = (16,6) # increase plot size for mosaics\n    mosaic(df_train, [new_var, 'Survived'], title='Target vs ' + f + ' [binned]')\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","222444b9":"# plot target vs (discrete) numerical features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in ['SibSp', 'Parch']:\n    plt.rcParams['figure.figsize'] = (16,6) # increase plot size for mosaics\n    mosaic(df_train, [f, 'Survived'], title='Target vs ' + f)\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","31389ccf":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nfor f in features_cat_4plot:\n    plt.rcParams['figure.figsize'] = (16,6) # increase plot size for mosaics\n    mosaic(df_train, [f, 'Survived'], title='Target vs ' + f)\n    plt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","441e88f2":"# check our new cabin prefix feature as well\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nplt.rcParams['figure.figsize'] = (16,6) # increase plot size for mosaics\nmosaic(df_train, ['CabinPrefix', 'Survived'], title='Target vs CabinPrefix')\nplt.show()\n    \n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","a3619077":"# plot target vs features using mosaic plot\nplt_para_save = plt.rcParams['figure.figsize'] # remember plot settings\n\nplt.rcParams['figure.figsize'] = (16,6) # increase plot size for mosaics\n\nname_list = df_train.FirstName.value_counts()[0:15].index.tolist()\ndf_temp = df_train[df_train.FirstName.isin(name_list)]\nmosaic(df_temp, ['FirstName', 'Survived'], title='Target vs FirstName (Top 15)')\nplt.show()\n\nname_list = df_train.LastName.value_counts()[0:15].index.tolist()\ndf_temp = df_train[df_train.LastName.isin(name_list)]\nmosaic(df_temp, ['LastName', 'Survived'], title='Target vs LastName (Top 15)')\nplt.show()\n\n# reset plot size again\nplt.rcParams['figure.figsize'] = plt_para_save","9503f456":"# select predictors\npredictors = features_num + features_cat_4plot\npredictors = predictors + ['Ticket', 'CabinPrefix', 'FirstName', 'LastName']\nprint('Number of predictors: ', len(predictors))\nprint(predictors)","5505a256":"# start H2O\nh2o.init(max_mem_size='12G', nthreads=4) # Use maximum of 12 GB RAM and 4 cores","f44e6008":"# upload data frames in H2O environment\nt1 = time.time()\ntrain_hex = h2o.H2OFrame(df_train)\ntest_hex = h2o.H2OFrame(df_test)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))\n\n# force categorical target\ntrain_hex['Survived'] = train_hex['Survived'].asfactor()","465a1105":"# fit Gradient Boosting model\nn_cv = 5\n\nfit_GBM = H2OGradientBoostingEstimator(ntrees=100,\n                                       max_depth=7,\n                                       min_rows=15,\n                                       learn_rate=0.1, # default: 0.1\n                                       sample_rate=0.8,\n                                       col_sample_rate=0.4,\n                                       nfolds=n_cv,\n                                       score_each_iteration=True,\n                                       stopping_metric='auc',\n                                       stopping_rounds=5,\n                                       stopping_tolerance=0.0001,\n                                       seed=999)\n# train model\nt1 = time.time()\nfit_GBM.train(x=predictors,\n              y='Survived',\n              training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","afab9ad2":"# show cross validation metrics\nfit_GBM.cross_validation_metrics_summary()","b05ad884":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_GBM.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.8,0.9)\n    plt.legend()\n    plt.grid()\n    plt.show()","8f6bf937":"# variable importance\nfit_GBM.varimp_plot()","0b9a9eb6":"# alternative variable importance using SHAP => see direction as well as severity of feature impact\nt1 = time.time()\nfit_GBM.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","e4434653":"# training performance\nperf_train = fit_GBM.model_performance(train=True)\nperf_train.plot()","b539fbc2":"# cross validation performance\nperf_cv = fit_GBM.model_performance(xval=True)\nperf_cv.plot()","0ee0797c":"# predict on train set (extract probabilities only)\npred_train_GBM = fit_GBM.predict(train_hex)['p1']\npred_train_GBM = pred_train_GBM.as_data_frame().p1\n\n# plot train set predictions (probabilities)\nplt.figure(figsize=(8,4))\nplt.hist(pred_train_GBM, bins=100)\nplt.title('Predictions on Train Set - GBM')\nplt.grid()\nplt.show()","dba9099f":"# calibration\nn_actual = sum(df_train.Survived)\nn_pred_GBM = sum(pred_train_GBM)\n\nprint('Actual Frequency    :', n_actual)\nprint('Predicted Frequency :', n_pred_GBM)\nprint('Calibration Ratio   :', n_pred_GBM \/ n_actual)","86f7cc95":"# convert to 0\/1\nbinary_threshold_GBM = 0.485945 # chose such that actual frequency is (approximately) met\npred_train_GBM_binary = np.where(pred_train_GBM > binary_threshold_GBM, 1, 0)\nprint('Actual Frequency      :', n_actual)\nprint('Calibrated Prediction :', sum(pred_train_GBM_binary))","d12e89b5":"# confusion matrix at selected threshold\npd.crosstab(df_train.Survived, pred_train_GBM_binary)","c30c4bc2":"# Random Forest model\nn_cv = 5\n\nfit_DRF = H2ORandomForestEstimator(nfolds=n_cv,\n                                  distribution='bernoulli',\n                                  ntrees=100,\n                                  mtries=-1, # automatic selection\n                                  max_depth=20,\n                                  score_each_iteration=True,\n                                  stopping_metric='auc',\n                                  stopping_rounds=5,\n                                  stopping_tolerance=0.0001,\n                                  seed=999)\n\n# train model\nt1 = time.time()\nfit_DRF.train(x=predictors,\n            y='Survived',\n            training_frame=train_hex)\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","8abce928":"# show cross validation metrics\nfit_DRF.cross_validation_metrics_summary()","359a6f44":"# show scoring history - training vs cross validations\nfor i in range(n_cv):\n    cv_model_temp = fit_DRF.cross_validation_models()[i]\n    df_cv_score_history = cv_model_temp.score_history()\n    my_title = 'CV ' + str(1+i) + ' - Scoring History [AUC]'\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.training_auc, \n                c='blue', label='training')\n    plt.scatter(df_cv_score_history.number_of_trees,\n                y=df_cv_score_history.validation_auc, \n                c='darkorange', label='validation')\n    plt.title(my_title)\n    plt.xlabel('Number of Trees')\n    plt.ylabel('AUC')\n    plt.ylim(0.7,0.9)\n    plt.legend()\n    plt.grid()\n    plt.show()","9fe408c6":"# variable importance\nfit_DRF.varimp_plot()","061eed7b":"# alternative variable importance using SHAP => see direction as well as severity of feature impact\nt1 = time.time()\nfit_DRF.shap_summary_plot(train_hex);\nt2 = time.time()\nprint('Elapsed time [s]: ', np.round(t2-t1,2))","76348a18":"# training performance\nperf_train = fit_DRF.model_performance(train=True)\nperf_train.plot()","c06afa78":"# cross validation performance\nperf_cv = fit_DRF.model_performance(xval=True)\nperf_cv.plot()","ad0735e5":"# predict on train set (extract probabilities only)\npred_train_DRF = fit_DRF.predict(train_hex)['p1']\npred_train_DRF = pred_train_DRF.as_data_frame().p1\n\n# plot train set predictions (probabilities)\nplt.figure(figsize=(6,4))\nplt.hist(pred_train_DRF, bins=100)\nplt.title('Predictions on Train Set - Random Forest')\nplt.grid()\nplt.show()","359dd4e5":"# calibration\nn_actual = sum(df_train.Survived)\nn_pred_DRF = sum(pred_train_DRF)\n\nprint('Actual Frequency    :', n_actual)\nprint('Predicted Frequency :', n_pred_DRF)\nprint('Calibration Ratio   :', n_pred_DRF \/ n_actual)","fa83537d":"# convert to 0\/1\nbinary_threshold_DRF = 0.4709 # chose such that actual frequency is (approximately) met\npred_train_DRF_binary = np.where(pred_train_DRF > binary_threshold_DRF, 1, 0)\nprint('Actual Frequency      :', n_actual)\nprint('Calibrated Prediction :', sum(pred_train_DRF_binary))","51e7a77d":"# confusion matrix at selected threshold\npd.crosstab(df_train.Survived, pred_train_DRF_binary)","9e4735fc":"# predict on test set (extract probabilities only)\npred_test_GBM = fit_GBM.predict(test_hex)['p1']\npred_test_GBM = pred_test_GBM.as_data_frame().p1\n\n# plot test set predictions (probabilities)\nplt.figure(figsize=(8,4))\nplt.hist(pred_test_GBM, bins=100)\nplt.title('Predictions on Test Set - GBM')\nplt.grid()\nplt.show()","ca404715":"# convert to binary again - aggregate probabilities first\npred_test_GBM_sum = pred_test_GBM.sum()\nprint('GBM - Sum of probs:', np.round(pred_test_GBM_sum,2))","0736d6bd":"# we select threshold such that counts are approximately equal to sum of probs (expected frequency)\nbinary_threshold_GBM_test = 0.44319\npred_test_GBM_binary = np.where(pred_test_GBM > binary_threshold_GBM_test, 1, 0)\npd.Series(pred_test_GBM_binary).value_counts()\nprint('GBM - Number of Survived - Test Set (binary):', sum(pred_test_GBM_binary))","f6b4f785":"# GBM submission\ndf_sub_GBM = df_sub.copy()\ndf_sub_GBM.Survived = pred_test_GBM_binary\ndisplay(df_sub_GBM.head())\n# save to file\ndf_sub_GBM.to_csv('submission_GBM.csv', index=False)","01c5b945":"# save probabilities as well\npred_test_GBM.to_csv('probs_GBM.csv', index=False)","fab100aa":"# predict on test set (extract probabilities only)\npred_test_DRF = fit_DRF.predict(test_hex)['p1']\npred_test_DRF = pred_test_DRF.as_data_frame().p1\n\n# plot test set predictions (probabilities)\nplt.figure(figsize=(8,4))\nplt.hist(pred_test_DRF, bins=100)\nplt.title('Predictions on Test Set - Random Forest')\nplt.grid()\nplt.show()","f5b56838":"# convert to binary again - aggregate probabilities first\npred_test_DRF_sum = pred_test_DRF.sum()\nprint('DRF - Sum of probs:', np.round(pred_test_DRF_sum,2))","e2f2393d":"# we select threshold such that counts are approximately equal to sum of probs (expected frequency)\nbinary_threshold_DRF_test = 0.44357\npred_test_DRF_binary = np.where(pred_test_DRF > binary_threshold_DRF_test, 1, 0)\npd.Series(pred_test_DRF_binary).value_counts()\nprint('DRF - Number of Survived - Test Set (binary):', sum(pred_test_DRF_binary))","54ef346c":"# DRF submission\ndf_sub_DRF = df_sub.copy()\ndf_sub_DRF.Survived = pred_test_DRF_binary\ndisplay(df_sub_DRF.head())\n# save to file\ndf_sub_DRF.to_csv('submission_DRF.csv', index=False)","49b998fc":"# save probabilities as well\npred_test_DRF.to_csv('probs_DRF.csv', index=False)","0913c60b":"# combine predictions in one data frame\ndf_preds_train = pd.DataFrame({'GBM': pred_train_GBM.values, 'DRF': pred_train_DRF.values})\ndf_preds_test = pd.DataFrame({'GBM': pred_test_GBM.values, 'DRF': pred_test_DRF.values})","41a34c47":"# scatter plot of two prediction sets - TRAIN set\nsns.jointplot(data=df_preds_train, x='GBM', y='DRF',\n              joint_kws={'s' : 2},\n              alpha=0.25)\nplt.show()","692d74d9":"# scatter plot of two prediction sets - TEST set\nsns.jointplot(data=df_preds_test, x='GBM', y='DRF',\n              joint_kws={'s' : 2},\n              alpha=0.25)\nplt.show()","67bae4e6":"# correlation (on test set)\ndf_preds_test.corr(method='pearson')","efab1e41":"# blend two model results on probability level\nw_GBM = 0.8\nw_DRF = 1-w_GBM\ndf_preds_train['blend'] = w_GBM*df_preds_train.GBM + w_DRF*df_preds_train.DRF\ndf_preds_test['blend'] = w_GBM*df_preds_test.GBM + w_DRF*df_preds_test.DRF","db2eb7fe":"# plot test set predictions (probabilities)\nplt.figure(figsize=(8,4))\nplt.hist(df_preds_test.blend, bins=100)\nplt.title('Predictions on Test Set - Blend')\nplt.grid()\nplt.show()","226e02b4":"# convert to 0\/1\nprint('Actual Frequency:', n_actual)\n# recalc threshold (for training)\nbinary_threshold_BLEND = 0.488345 # chose such that actual frequency is (approximately) met\npred_train_BLEND_binary = np.where(df_preds_train.blend > binary_threshold_BLEND, 1, 0)\nprint('Number of Survived (binary):', sum(pred_train_BLEND_binary))","17e5e7e9":"# confusion matrix at selected threshold\npd.crosstab(df_train.Survived, pred_train_BLEND_binary)","2d834fc8":"# convert to binary again - aggregate probabilities first\npred_test_BLEND_sum = df_preds_test.blend.sum()\nprint('Blend - Sum of probs:', np.round(pred_test_BLEND_sum,2))","e71541e9":"# we select threshold such that counts are approximately equal to sum of probs (expected frequency)\nbinary_threshold_BLEND_test = 0.44189\npred_test_BLEND_binary = np.where(df_preds_test.blend > binary_threshold_BLEND_test, 1, 0)\npd.Series(pred_test_BLEND_binary).value_counts()\nprint('Blend - Number of Survived - Test Set (binary):', sum(pred_test_BLEND_binary))","ca716101":"# blend submission\ndf_sub_BLEND = df_sub.copy()\ndf_sub_BLEND.Survived = pred_test_BLEND_binary\ndisplay(df_sub_BLEND.head())\n# save to file\ndf_sub_BLEND.to_csv('submission_BLEND.csv', index=False)","64ec4c49":"# pick an example (from training data)\nmy_row = 8\ntrain_hex[my_row,:]","e6f52919":"# what did we predict?\nprint('Prediction (binary):', pred_train_GBM_binary[my_row])\nprint('Prediction (prob.) :', pred_train_GBM[my_row])","06e21509":"# explain prediction by decomposing it into individual contributions\nfit_GBM.shap_explain_row_plot(row_index=my_row, frame=train_hex);","a41dd447":"# Table of Contents\n\n* [Target Exploration](#1)\n* [Numerical Features](#2)\n* [Categorical Features\/Feature Engineering](#3)\n* [Test Set vs Train Set](#4)\n* [Target vs Features](#5)\n* [Build GBM Model](#6)\n* [Build Random Forest Model](#7)\n* [Predict on Test Set & Submission](#8)\n* [Explanations for GBM model](#9)","0fca609c":"### Categorical Features","ec4de5ab":"<a id='6'><\/a>\n# Build GBM Model","9a311bea":"#### => Significantly higher percentages of males in the test set (69.8% vs 56.1%). Males have a much lower probability of survival so we can again (as for feature age) expect to see a different survival situation between test and train set, especially because sex is the most important feature (see below).","dfa904b6":"### GBM","1a28904f":"<a id='1'><\/a>\n# Target Exploration","38cbd2d4":"### Random Forest","d357c830":"#### We observe quite some diffences between train and test set, e. g. lower age and higher SibSp in test set.","eed5aa55":"<a id='9'><\/a>\n# Explanations for GBM model","66016005":"#### Strong impact of sex, we will later see that this is our most important feature.","1a562faa":"#### Ok, also the cabin prefix seems to make a measurable difference!","77a2756e":"### Let's look a little bit behind the scenes of our GBM model predictions:","4d50fcaa":"####  Let's explore the age feature a little bit more:","6d235d05":"#### Interpretation: We have already seen that males had a much lower chance of survival and sex is also here the most important factor. On the positive side we have Parch=1 (# of parents \/ children aboard) and Pclass=1 (1st class ticket). Impact of name is negligible.","a85d2f0f":"<a id='7'><\/a>\n# Build Random Forest Model","7c7ec37a":"<a id='8'><\/a>\n# Predict on Test Set & Submission","e3505786":"#### Also the Pclass feature shows completely different distributions:","cf4e7b32":"#### We have quite a few missings here!","25b807c0":"<a id='4'><\/a>\n# Test Set vs Train Set","4c71c264":"### Let's try some feature engineering","d7f5350b":"<a id='2'><\/a>\n# Numerical Features","a337d805":"### Blend","e5a41116":"### Correlations","4a7e0c4b":"#### => Age distribution is completely different. Especially, the test set includes much more individuals in the age range 20-30, those have a low survival probability as we will see later! Ceteris paribus we would therefore expect lower survival rates on the test set.","6249ee07":"### Check performance on training data \/ cross validations","831293b4":"<a id='5'><\/a>\n# Target vs Features","e87542ce":"### Numerical Features","78202ec9":"#### Let's check the names (most frequent only):","4ea34e4c":"#### Let's also check the **combined** effect of sex and age:","a9647a60":"#### Name, Ticket and Cabin have too many levels. We have to look at them separately...","b7ccacd6":"<a id='3'><\/a>\n# Categorical Features \/ Feature Engineering","76daf573":"#### Nice, there is no balancing issue here."}}