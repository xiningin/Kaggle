{"cell_type":{"a7541c17":"code","6292898a":"code","9ef6cd32":"code","bb67e509":"code","06e502ce":"code","fb501395":"code","afc1caaf":"code","bfbbd99a":"code","ad265900":"code","d85c50e3":"code","e573dbcc":"code","d8c5da4c":"code","1aac62d0":"code","f451ee30":"code","ad59a6e8":"code","240c5510":"code","60bf2fc0":"code","e65cc4de":"code","7a04406d":"code","31912b50":"code","830f053c":"code","93a83e9e":"code","08ab7dc9":"code","6084e544":"code","584811ec":"code","5337dcbf":"code","03a89593":"markdown","bf48f9b6":"markdown","01fe83f2":"markdown","19d0b689":"markdown","9daeb754":"markdown","e0a773df":"markdown","307c8701":"markdown","142cc7cf":"markdown","61bd6280":"markdown","8eeefd74":"markdown","b2bd0c69":"markdown","2329712d":"markdown","56ec7f96":"markdown","485e39dd":"markdown","238fc2e8":"markdown","947d458b":"markdown","87db0353":"markdown","59a44808":"markdown","58790134":"markdown","0b43c7e1":"markdown"},"source":{"a7541c17":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","6292898a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing import StandardScaler,MinMaxScaler\nfrom sklearn.metrics import mean_absolute_error as mae\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","9ef6cd32":"df = pd.read_csv('\/kaggle\/input\/craigslist-carstrucks-data\/vehicles.csv')\ndf.head(10)","bb67e509":"print(df.columns)\nprint(df.shape)","06e502ce":"df.isna().sum()","fb501395":"def remove_col(data):\n    thresh = len(data) * 0.4\n    cols = data.columns\n    remove = []\n    for col in cols:\n        n_nulls = data[col].isna().sum()\n        if n_nulls >= thresh:\n            remove.append(col)\n    return remove\n\nrm_cols = remove_col(df)\ndf = df.drop(rm_cols,axis=1)\ndf.head(5)","afc1caaf":"df.nunique()","bfbbd99a":"rm_cols = [\n    'id',\n    'url',\n    'region',\n    'region_url',\n    'image_url',\n    'description',\n    'model',\n    'state',\n    'paint_color'\n]\ndf = df.drop(rm_cols,axis=1)\ndf.head(10)","ad265900":"print(df.price.describe())\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.boxplot(df.price)","d85c50e3":"descp = interquartile = df.price.describe()\ninterquartile = descp['75%'] - descp['25%']\nthresh = interquartile * 1.5\n\ndf = df[df.price < thresh]\ndf.head(3)","e573dbcc":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.boxplot(df.price)","d8c5da4c":"df[['odometer','year']].describe()","1aac62d0":"df.year.value_counts()\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.countplot(df[df.year.between(1950,2020)].year)\n# plt.xticks([0,15,30,45,60,70])","f451ee30":"df = df[df.year.between(1960,2020)]\ndf.head(3)","ad59a6e8":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.boxplot(df.odometer)","240c5510":"interquartile = df.odometer.quantile(0.75) - df.odometer.quantile(0.25)\nthresh = interquartile * 1.5\ndf = df[df.odometer < thresh]\ndf.head(3)","60bf2fc0":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.boxplot(df.odometer)","e65cc4de":"top_manufacturers = df.manufacturer.value_counts(dropna=False).iloc[:10]\nprint(top_manufacturers)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.barplot(x=top_manufacturers.index,y=top_manufacturers.values)\nplt.xlabel('Manufacturers')\nplt.ylabel('Number of vehicles')\nplt.title('Vehicles from top 10 manufacturers',y=1.02)\nplt.suptitle('Number of vehicles from the top 10 manufacturers listed on Craigslist',y=0.9)","7a04406d":"top_types = df.type.value_counts().iloc[:10]\nprint(top_types)\n\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nsns.barplot(x=top_types.index,y=top_types.values)\nplt.xlabel('Vehicle type')\nplt.ylabel('Number of vehicles')\nplt.title('Generic top 10 vehicle types',y=1.02)\nplt.suptitle('Number of vehicles from the top 10 types of generic vehicle models listed on Craigslist',y=0.9)","31912b50":"df = df.dropna(subset=['lat','long'])\ndf.head(5)","830f053c":"plt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\ncorr = df.corr()\nsns.heatmap(corr,xticklabels=corr.columns,yticklabels=corr.columns, annot=True,cmap='YlGnBu')","93a83e9e":"df_cleaned = pd.get_dummies(df)\nX = df_cleaned.iloc[:,1:]\ny = df_cleaned.price\nX.columns","08ab7dc9":"scaler = MinMaxScaler()\nX = scaler.fit_transform(X)\nX","6084e544":"X_train,X_test,y_train,y_test = train_test_split(X,y,random_state=1)","584811ec":"model = RandomForestRegressor(n_estimators=25,random_state=0)\nmodel.fit(X_train,y_train)\nprint(model.score(X_train,y_train))\npred = model.predict(X_test)\nprint(mae(y_test,pred))\nprint(y.mean())\nmodel.score(X_test,y_test)","5337dcbf":"feature_imp = pd.Series(model.feature_importances_,index=df_cleaned.columns[1:])\nplt.style.use('fivethirtyeight')\nplt.figure(figsize=(12,8))\nfeature_imp.sort_values(ascending=False)[:20].plot.barh()\nplt.ylabel('Features')\nplt.xticks([])","03a89593":"Let us now try to visualize the correlation between variables, to do this we will use a `heatmap` and the `corr` function.","bf48f9b6":"Now that we have our target variable in y and the independent variables in X, we will split the data for training and testing.","01fe83f2":"From above, we see that there are columns having too many unique values and not a variable on interval\/ratio scale, thus we can remove some columns we know will not be any help to us such as :-\n\n* id\n* url\n* region\n* region_url\n* image_url\n* description\n* model\n* state\n* paint_color\n\nThe reason why we removed paint_color is because the color of the vehicle does not add anything to its price, mathematically it might be correlated, but in common sense we know that is not true.","19d0b689":"Let us now look at the nominal or ordinal variables that we have. Lets understand the `manufacturer` and `type` columns. Let us understand our data and see vehicle of which manufacturers and type of vehicle are selling most online.","9daeb754":"Now before we split the data and make it ready for the `MinMaxScaler` to scale the values, since price and odometer can have very high values whereas year cannot, thus it is important we do this. <br>\nWe will convert out categorical variables into dummy binary columns so that it is easier for the model to understand.","e0a773df":"Similarly, the `odometer` column looks like it has outliers, the values seem too high. Thus we will have to trim these values. Let us view its dstribution using a `boxplot` to find outliers.","307c8701":"We can see some absurd values in the `price` column. These values usually skew the data and pull the mean much higher than it actually would be. We will have to remove the outliers. To do so, we will set up a threshold using the `interquartile range` (IQR). Instead of deciding on a threshold myself, this method can easily give me a bound that would be reasonable with respect to the data in general.<br>\ninterquartile range = 75% - 25%","142cc7cf":"We see we no longer have any outliers. The distribution of prices is fairly spread between 0 and 14000 dollars.<br>\nNow let us look at the `odometer` and `year` column, as they are the other discrete interval scale variables.","61bd6280":"So our target variable is going to be the price column. Let us look at the distribution of the price column.","8eeefd74":"We will apply the same `interquartile range` (IQR) method to remove these outliers. As mentioned above, such values will skew the data and our resutls.","b2bd0c69":"The `year` column seems unrealistic at 2021. Let us plot a countplot for the year to see the trend in buying\/selling used cars on craigslist. We can tell that sales actually began increasing after 1960. Hence we will consider data after this point in time as years below that wud have less representative data.","2329712d":"# Predicting used car prices\nWorking on the Craigslist Dataset uploaded on kaggle, the aim on the project is to predict the `price` based on factors decided after analysis. Decription of columns:- \n\n* price - entry price\n* year - entry year\n* model - model of vehicle\n* condition - condition of vehicle\n* cylinders - no. of cylinders\n* fuel type - fuel variant \n* odometer - miles travelled by vehicle\n* vin - vehicle identification number\n* size - size of vehicle\n* type - generic type of vehicle\n\nLet us first explore the dataset","56ec7f96":"There are a number of models to choose from, we will try out the `RandomForestRegressor` here because :-\n* It handles high-dimensionality very well since it takes subsets of data.\n* It is extremely versatile and requires very little preprocessing\n* It is great at avoiding overfitting since each decision tree has low bias","485e39dd":"The `manufacturer` and `type` columns are important when we talk about sale of a car. Hence we will convert these columns into dummy binary columns. Similarly looking at the other variables like `fuel`,`transmission`,`drive`,`year` are important features when someone is buying a car.<br>\nLocation is another influencing factor as prices of vehicles may depend upon locations, thus we have `lat` and `long` for this.<br>\nThe `lat` and `long` columns still have null values, the null values in the nominal variables will be taken care of when we convert into dummy variables. For now, let us ignore these rows.","238fc2e8":"There are a lot of null values for certain columns. We can use a thumb rule that if any column contains atleast 35% - 40% null values, we can remove those columns from consideration in our case. Such columns do not add much to our analysis and are generally irrelevant to the goal.","947d458b":"We have achieved a good spread now, and that will be enough for now.","87db0353":"Let us look at the unique values for each column, especially those that are measured on nominal or ordinal scale. This can give us a sense of which columns can be important to us. Some variables even on a nominal scale have too many unique values, such variables would not add anything to the final model (like url).","59a44808":"All the variables seem to be independent of eachother, there is no single variable that directly influences `price` by alot.","58790134":"As we can see our model did wonderful on train set, and is slightly underfitting on the test set, to better this we will have to do feature engg. Below we can see a `barplot` to identify the most important features for the model.","0b43c7e1":"Now that we have it cleaned, let us confirm this with a `boxplot`."}}