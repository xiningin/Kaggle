{"cell_type":{"87479a4e":"code","014ae9b9":"code","2f8e72b9":"code","8946f455":"code","c302d443":"code","8fbf6586":"code","d5f378a9":"code","28242835":"code","13b7b631":"code","4843909b":"code","b8aab3aa":"code","12a64451":"code","542962be":"code","fc90b392":"code","9d557e7a":"code","f6fa314a":"code","b3221ba1":"code","8caf743f":"code","8a6ce002":"code","6f990a0e":"code","93e759ba":"code","86410f6c":"code","efa7f188":"code","298ab554":"code","b7109976":"code","00b7f2c6":"code","9177fb0c":"code","5ef02f41":"code","cc2810cd":"code","de12eb7b":"code","f5bfe372":"code","375aa445":"markdown","157f1ef0":"markdown","4d729da7":"markdown","3a539b99":"markdown","364e18fd":"markdown","ab2c5546":"markdown","ea1a1400":"markdown","c6ccff4e":"markdown","95273658":"markdown","73f18de3":"markdown","7979c842":"markdown","0fa0ca6e":"markdown","f01d4f98":"markdown","3ecbe69b":"markdown","e5ca1719":"markdown","5fb1dd5f":"markdown","8ce16eb3":"markdown","77f05d61":"markdown","2ac238da":"markdown","19bb644f":"markdown"},"source":{"87479a4e":"# !pip install pycocotools\n\nfrom pycocotools.coco import COCO # COCO python library\nimport numpy as np\nimport skimage.io as io\nimport matplotlib.pyplot as plt\nimport pylab\n\nimport random\nimport string\n\nimport cv2\nimport os\nfrom pickle import dump, load\nimport json\n\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\n\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Bidirectional, Input, Dropout, Attention\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.applications.xception import Xception, preprocess_input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array\nfrom tensorflow.keras.utils import to_categorical\n\nfrom keras.layers.merge import add\n\nfrom tensorflow.keras.models import Model, load_model\n\n# small library for seeing the progress of loops.\nfrom tqdm.notebook import tqdm\n\npylab.rcParams['figure.figsize'] = (8.0, 10.0)","014ae9b9":"coco=COCO(\"..\/input\/cocods\/annotations_trainval2017\/annotations\/instances_train2017.json\")","2f8e72b9":"cats = coco.loadCats(coco.getCatIds())\nmaincategories = list(set([cat['supercategory'] for cat in cats]))\n\nprint(\"Number of main categories: \", len(maincategories))\nprint(\"List of main categories: \", maincategories)","8946f455":"subcategories = [cat['name'] for cat in cats]\n\nprint(\"Number of sub categories: \", len(subcategories))\nprint(\"List of sub categories: \", subcategories)","c302d443":"catIds = coco.getCatIds(catNms=subcategories)\n\nsubcategories_Ids = dict()\nfor i in range(0,len(subcategories)):\n    subcategories_Ids[subcategories[i]] = catIds[i]\n\nprint(\"Sub categories with IDs :\",subcategories_Ids)","8fbf6586":"subcategories_imageIds = dict()\n\nfor i in range(0,len(catIds)):\n    imgIds = coco.getImgIds(catIds=catIds[i])\n    img = []\n    for j in imgIds: \n        img.append(j)\n    subcategories_imageIds[subcategories[i]] = img\n    \nprint(\"Sub categories with Image IDs :\",len(subcategories_imageIds))","d5f378a9":"length_dict = {key: len(value) for key, value in subcategories_imageIds.items()}\nprint(\"Total images in each sub categories: \", length_dict)","28242835":"#subcategories_imageIds['bicycle']","13b7b631":"train_cats = subcategories_imageIds['bicycle'] + subcategories_imageIds['airplane']\nimgIdss = coco.getImgIds(imgIds = train_cats)\nprint(\"Total Images: \", len(imgIdss))","4843909b":"fig = plt.gcf()\nfig.set_size_inches(16, 16)\n\nnext_pix = imgIdss\nrandom.shuffle(next_pix)\n\nfor i, img_path in enumerate(next_pix[0:12]):\n    \n    sp = plt.subplot(4, 4, i + 1)\n    sp.axis('Off')\n\n    img = coco.loadImgs(img_path)[0]\n    I = io.imread(img['coco_url'])\n    plt.imshow(I)\n\nplt.show()","b8aab3aa":"fig = plt.gcf()\nfig.set_size_inches(16, 16)\n\nfor i, img_path in enumerate(next_pix[0:12]):\n    \n    sp = plt.subplot(4, 4, i + 1)\n    sp.axis('Off')\n\n    img = coco.loadImgs(img_path)[0]\n    I = io.imread(img['coco_url'])\n    plt.imshow(I)\n    annIds = coco.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n    anns = coco.loadAnns(annIds)\n    # print(anns)\n    coco.showAnns(anns)\n    \n\nplt.show()","12a64451":"annFile = \"..\/input\/cocods\/annotations_trainval2017\/annotations\/person_keypoints_train2017.json\"\ncoco_kps=COCO(annFile)","542962be":"fig = plt.gcf()\nfig.set_size_inches(16, 16)\n\nfor i, img_path in enumerate(next_pix[0:12]):\n    \n    sp = plt.subplot(4, 4, i + 1)\n    sp.axis('Off')\n\n    img = coco.loadImgs(img_path)[0]\n    I = io.imread(img['coco_url'])\n    plt.imshow(I)\n    annIds = coco_kps.getAnnIds(imgIds=img['id'], catIds=catIds, iscrowd=None)\n    anns = coco_kps.loadAnns(annIds)\n    coco_kps.showAnns(anns)\n    \n\nplt.show()","fc90b392":"annFile = \"..\/input\/cocods\/annotations_trainval2017\/annotations\/captions_train2017.json\"\ncoco_caps=COCO(annFile)","9d557e7a":"img = coco.loadImgs(next_pix[0])[0]\nI = io.imread(img['coco_url'])\nplt.imshow(I)\nannIds = coco_caps.getAnnIds(imgIds=img['id']);\nanns = coco_caps.loadAnns(annIds)\ncoco_caps.showAnns(anns)\nplt.show()","f6fa314a":"img = coco.loadImgs(next_pix[1])[0]\nI = io.imread(img['coco_url'])\nplt.imshow(I)\nannIds = coco_caps.getAnnIds(imgIds=img['id']);\nanns = coco_caps.loadAnns(annIds)\ncoco_caps.showAnns(anns)\nplt.show()","b3221ba1":"img = coco.loadImgs(next_pix[10])[0]\nI = io.imread(img['coco_url'])\nplt.imshow(I)\nannIds = coco_caps.getAnnIds(imgIds=img['id']);\nanns = coco_caps.loadAnns(annIds)\ncoco_caps.showAnns(anns)\nplt.show()","8caf743f":"print(\"Total images for training: \", len(imgIdss))","8a6ce002":"dataset = dict()\nimgcaptions = []\n\nfor imgid in imgIdss:\n    img = coco.loadImgs(imgid)[0]\n    annIds = coco_caps.getAnnIds(imgIds=img['id']);\n    anns = coco_caps.loadAnns(annIds)\n    imgcaptions = []\n    for cap in anns:\n        \n        # Remove punctuation\n        cap = cap['caption'].translate(str.maketrans('', '', string.punctuation))\n        \n        # Replace - to blank\n        cap = cap.replace(\"-\",\" \")\n        \n        # Split string into word list and Convert each word into lower case\n        cap = cap.split()\n        cap = [word.lower() for word in cap]\n        \n        # join word list into sentence and <start> and <end> tag to each sentence which helps \n        # LSTM encoder-decoder model while training.\n        \n        cap = '<start> ' + \" \".join(cap) + ' <end>'\n        imgcaptions.append(cap)\n        \n    dataset[img['coco_url']] = imgcaptions \n    \n    \nprint(\"Length of Dataset: \",len(dataset))\nprint(dataset['http:\/\/images.cocodataset.org\/train2017\/000000047084.jpg'])\n#dataset","6f990a0e":"from itertools import chain\nflatten_list = list(chain.from_iterable(dataset.values())) #[[1,3],[4,8]] = [1,3,4,8]\n\ntokenizer = Tokenizer(oov_token='<oov>') # For those words which are not found in word_index\ntokenizer.fit_on_texts(flatten_list)\ntotal_words = len(tokenizer.word_index) + 1\n\nprint(\"Vocabulary length: \", total_words)\nprint(\"Bicycle ID: \", tokenizer.word_index['bicycle'])\nprint(\"Airplane ID: \", tokenizer.word_index['airplane'])\n\n# print(tokenizer.word_index)","93e759ba":"model = Xception(include_top=False, pooling='avg')\n\nimage_features = {}\n\nfor img in tqdm(dataset.keys()):\n    image = io.imread(img)\n    if image.ndim != 3:\n        image = cv2.cvtColor(image,cv2.COLOR_GRAY2RGB)\n    \n    # Resize each image size 299 x 299\n    image = cv2.resize(image,(299,299))\n    image = np.expand_dims(image, axis=0)\n    \n    # Normalize image pixels\n    image = image\/127.5\n    image = image - 1.0\n\n    # Extract features from image\n    feature = model.predict(image)\n    image_features[img] = feature\n    \nprint(\"Image features length: \", len(image_features))","86410f6c":"image_features['http:\/\/images.cocodataset.org\/train2017\/000000047084.jpg'].shape","efa7f188":"def dict_to_list(descriptions):\n    all_desc = []\n    for key in descriptions.keys():\n        [all_desc.append(d) for d in descriptions[key]]\n    return all_desc\n\ndef max_length(descriptions):\n    desc_list = dict_to_list(descriptions)\n    return max(len(d.split()) for d in desc_list)\n    \nmax_length = max_length(dataset)\nmax_length","298ab554":"#create input-output sequence pairs from the image description.\n\ndef data_generator(descriptions, features, tokenizer, max_length):\n    while 1:\n        for key, description_list in descriptions.items():\n\n            feature = features[key][0]\n            input_image, input_sequence, output_word = create_sequences(tokenizer, max_length, description_list, feature)\n            yield ([input_image, input_sequence], output_word)\n            \n\ndef create_sequences(tokenizer, max_length, desc_list, feature):\n    X1, X2, y = list(), list(), list()\n    \n    # walk through each description for the image\n    for desc in desc_list:\n        \n        # encode the sequence\n        seq = tokenizer.texts_to_sequences([desc])[0]\n        \n        # split one sequence into multiple X,y pairs\n        for i in range(1, len(seq)):\n            \n            # split into input and output pair\n            in_seq, out_seq = seq[:i], seq[i]\n            \n            # pad input sequence\n            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n            \n            # encode output sequence\n            out_seq = to_categorical([out_seq], num_classes=total_words)[0]\n            \n            # store\n            X1.append(feature) # image features\n            X2.append(in_seq)  # Caption input\n            y.append(out_seq)  # Caption output\n            \n    return np.array(X1), np.array(X2), np.array(y)\n","b7109976":"from keras.utils import plot_model\n\n# define the captioning model\ndef define_model(total_words, max_length):\n\n    # features from the CNN model squeezed from 2048 to 256 nodes\n    inputs1 = Input(shape=(2048,))\n    fe1 = Dropout(0.5)(inputs1)\n    fe2 = Dense(256, activation='relu')(fe1)\n\n    # LSTM sequence model\n    inputs2 = Input(shape=(max_length,))\n    se1 = Embedding(total_words, 256, mask_zero=True)(inputs2)\n    se2 = Dropout(0.5)(se1)\n    se3 = LSTM(256)(se2)\n\n    # Merging both models\n    decoder1 = add([fe2, se3])\n    decoder2 = Dense(256, activation='relu')(decoder1)\n    outputs = Dense(total_words, activation='softmax')(decoder2)\n\n    # tie it together [image, seq] [word]\n    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n    model.compile(loss='categorical_crossentropy', optimizer='adam')\n\n    # summarize model\n    print(model.summary())\n    plot_model(model, to_file='model.png', show_shapes=True)\n\n    return model\n","00b7f2c6":"# train our model\nprint('Dataset: ', len(dataset))\nprint('Descriptions: train=', len(dataset))\nprint('Photos: train=', len(image_features))\nprint('Vocabulary Size:', total_words)\nprint('Description Length: ', max_length)\n\nmodel = define_model(total_words, max_length)\nepochs=1\nsteps = len(dataset)\n\n# making a directory models to save our models\nimport os\nos.mkdir(\"models\")\n\nfor i in range(epochs):\n    generator = data_generator(dataset, image_features, tokenizer, max_length)\n    model.fit(generator, epochs=5, steps_per_epoch= steps, verbose=1)\n    model.save(\"models\/model_\" + str(i) + \".h5\")","9177fb0c":"import numpy as np\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\n\nimg_paths = [\"..\/input\/cocods\/val2017\/val2017\/000000001761.jpg\",\n            \"..\/input\/cocods\/val2017\/val2017\/000000022396.jpg\",\n            \"..\/input\/cocods\/val2017\/val2017\/000000098520.jpg\",\n            \"..\/input\/cocods\/val2017\/val2017\/000000101762.jpg\",\n            \"..\/input\/cocods\/val2017\/val2017\/000000224051.jpg\"]\n\ndef extract_features(filename, model):\n        try:\n            image = Image.open(filename)\n\n        except:\n            print(\"ERROR: Couldn't open image! Make sure the image path and extension is correct\")\n        image = image.resize((299,299))\n        image = np.array(image)\n        \n        # for images that has 4 channels, we convert them into 3 channels\n        if image.shape[2] == 4: \n            image = image[..., :3]\n        image = np.expand_dims(image, axis=0)\n        image = image\/127.5\n        image = image - 1.0\n        feature = model.predict(image)\n        return feature\n\ndef word_for_id(integer, tokenizer):\n    for word, index in tokenizer.word_index.items():\n        if index == integer:\n            return word\n    return None\n\n\ndef generate_desc(model, tokenizer, photo, max_length):\n    in_text = 'start'\n    for i in range(max_length):\n        sequence = tokenizer.texts_to_sequences([in_text])[0]\n        sequence = pad_sequences([sequence], maxlen=max_length)\n        pred = model.predict([photo,sequence], verbose=0)\n        pred = np.argmax(pred)\n        word = word_for_id(pred, tokenizer)\n        \n        if word is None:\n            break\n        in_text += ' ' + word\n        \n        if word == 'end':\n            break\n    return in_text\n\n\n#max_length = 46\n\nmodel = load_model('.\/models\/model_0.h5')\nxception_model = Xception(include_top=False, pooling=\"avg\")\n\nphoto = extract_features(img_paths[0], xception_model)\nimg = Image.open(img_paths[0])\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","5ef02f41":"photo = extract_features(img_paths[1], xception_model)\nimg = Image.open(img_paths[1])\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","cc2810cd":"photo = extract_features(img_paths[2], xception_model)\nimg = Image.open(img_paths[2])\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","de12eb7b":"photo = extract_features(img_paths[3], xception_model)\nimg = Image.open(img_paths[3])\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","f5bfe372":"photo = extract_features(img_paths[4], xception_model)\nimg = Image.open(img_paths[4])\ndescription = generate_desc(model, tokenizer, photo, max_length)\nprint(\"\\n\\n\")\nprint(description)\nplt.imshow(img)","375aa445":"#### Find Sub Categories","157f1ef0":"# Define CNN-LSTM Model\n\n#### CNN (Convolution Neural Network)\n\nTechnically, deep learning CNN models to train and test, each input image will pass it through a series of convolution layers with filters (Kernals), Pooling, fully connected layers (FC) and apply Softmax function to classify an object with probabilistic values between 0 and 1.\n\n\n#### [Click here](https:\/\/www.analyticsvidhya.com\/blog\/2018\/12\/guide-convolutional-neural-network-cnn\/) to know more about CNN.\n\n<br>\n\n#### LSTM (Long short-term memory)\n\n\nLong Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence in sequence prediction problems. This is a behavior required in complex problem domains like machine translation, speech recognition, and more. LSTMs are a complex area of deep learning\n\n\n#### [Click here](https:\/\/www.analyticsvidhya.com\/blog\/2017\/12\/fundamentals-of-deep-learning-introduction-to-lstm\/) to know more about LSTM.\n","4d729da7":"#### Find Sub categories with its corresponding IDs","3a539b99":"Here, I have selected two sub categories **Bicycle** and **Airplane** for accomplishing image caption generator project.","364e18fd":"# Model Architechture \n\n### CNN-LSTM model:\n\n- CNN is used for extracting features from the image. We will use the pre-trained model Xception.\n- LSTM will use the information from CNN to help generate a description of the image.\n\n\n### Model structure\n\n![model.png](attachment:5e082228-cfa2-49fe-9526-fd31a45a9868.png)\n\n\n* The feature extracted from the image has a size of 2048, with a dense layer, we will reduce the dimensions to 256 nodes.<br> \n* An embedding layer will handle the text input,followed by the LSTM layer.\n<br>\n* Then By merging the output from the above two layers, we will process by the dense layer to make the final prediction. The final layer will contain the number of nodes equal to our vocabulary size.","ab2c5546":"#### Load images with keypoints objects","ea1a1400":"#### Load images with respective captions","c6ccff4e":"# Introduction\n\n> Image captioning is the task of generating textual descriptions of a given image, requiring  techniques of computer vision and natural language processing.\n\nIt is a popular research area of Artificial Intelligence that deals with image understanding and a language description for that image. Generating well-formed sentences requires both syntactic and semantic understanding of the language. Being able to describe the content of an image using accurately formed sentences is a very challenging task, but it could also have a great impact, by helping visually impaired people better understand the content of images.  [source](https:\/\/www.analyticsvidhya.com\/blog\/2020\/11\/create-your-own-image-caption-generator-using-keras\/) \n\nHere, I have used [COCO Dataset 2017](https:\/\/cocodataset.org\/#download) which contains **12 different types of categories** and among them it has **80 types of sub-categories**. Each sub categories contain list of images and five captions to each image.","95273658":"# Image caption generator\n#### Here I have taken \"Bicycle\" and \"Airplane\" images and captions only.\n","73f18de3":"# Exploring COCO Dataset\n\n#### Initialize COCO class and operate instances_train2017.json","7979c842":"# Prepare Dataset \n\n#### Group all captions together having the same image and store them in dict.","0fa0ca6e":"# Make a Data Generator\n\nWe have to train our model on 6221 images and each image will contain 2048 length feature vector and caption is also represented as numbers. This amount of data for 6221 images is not possible to hold into memory so we will be using a generator method that will yield batches.\n\n**The generator will yield the input and output sequence.**\n\n##### For example:\n\nThe input to our model is [x1, x2] and the output will be y, where x1 is the 2048 feature vector of that image, x2 is the input text sequence and y is the output text sequence that the model has to predict.\n\n![Capture.PNG](attachment:abfdd0f4-ece1-4fe8-a9db-4060a31f10fd.PNG)","f01d4f98":"#### Find Categories","3ecbe69b":"References :\n- https:\/\/www.analyticsvidhya.com\/blog\/2020\/11\/create-your-own-image-caption-generator-using-keras\/\n- https:\/\/github.com\/cocodataset\/cocoapi\/blob\/master\/PythonAPI\/pycocoDemo.ipynb\n- https:\/\/www.tensorflow.org\/tutorials\/text\/image_captioning\n- https:\/\/data-flair.training\/blogs\/python-based-project-image-caption-generator-cnn\/","e5ca1719":"#### Load some of the random images with segmented objects inside them ","5fb1dd5f":"#### Find total images with each sub categories","8ce16eb3":"#### Load some of the random images","77f05d61":"Here, I have found the max_length among the captions which will help to pad each caption with the same length.","2ac238da":"# Preprocess and tokenize the captions","19bb644f":"# Preprocess and caching the features extracted from InceptionV3\n\nI will use InceptionV3 (which is pretrained on Imagenet) to classify each image. I will extract features from the last convolutional layer. This is also called a tranfer learning.\n\nSince the Xception model was originally built for **imagenet**, we will do little changes for integrating with our model. Here, the Xception model takes **100 x 100 x 3** image size as input. We will remove the last classification layer and get the **2048 feature vector**."}}