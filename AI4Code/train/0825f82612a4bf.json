{"cell_type":{"2400de7d":"code","c979038b":"code","f41852c0":"code","d829c41b":"code","3320670c":"code","9fbd8ac3":"code","c9a270e8":"code","b8b87ec1":"code","afb24f8b":"code","0130d98b":"code","052440dc":"code","33563369":"code","d75a188b":"code","6f9150fe":"code","8bb8260f":"code","107a5194":"code","fac7612f":"code","628b2247":"code","e558c834":"code","06f5add0":"code","1d7fb679":"code","dae87d86":"code","874c581a":"code","ef75ec3c":"code","845c58c4":"code","64aa116e":"code","f9b703f4":"code","973c31c5":"code","59ad1f5c":"code","6c9f9456":"code","3fdd4019":"code","78d43b67":"code","4acec46d":"code","2f4a9800":"code","acaa8cee":"code","ad332706":"code","ced04fc6":"code","d2496aa3":"code","f98f89ad":"code","4e59e6d2":"code","5325a672":"code","c2ba3475":"markdown","4787e705":"markdown","b96712be":"markdown","cd8420d9":"markdown","742c2a9c":"markdown","30b8d346":"markdown","5e95aabb":"markdown","99a68906":"markdown","4989b29d":"markdown","6517a128":"markdown","221b2856":"markdown","f0c7b20b":"markdown","38217f11":"markdown","3b065739":"markdown","f2e5b66e":"markdown","cc69dd57":"markdown","0eec6282":"markdown","6c0c7774":"markdown","dfd27d81":"markdown","da5cb6a8":"markdown","294ab5cd":"markdown","94a52681":"markdown","5569bbac":"markdown","72cb448a":"markdown"},"source":{"2400de7d":"# importing all libraries\nimport numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom matplotlib.pyplot import figure\nimport random","c979038b":"# checking the dataset\ndataset = pd.read_csv(\"..\/input\/startup-logistic-regression\/50_Startups.csv\")\ndataset.head()","f41852c0":"# removing the 'State' feature\ndel dataset['State']\ndataset.head()","d829c41b":"# loading all independent features\nx = dataset.iloc[:,:-1] # using for predefined linear_model \n\n# loading the dependent feature\ny = dataset.iloc[:,-1] # using for predefined linear_model","3320670c":"# creating testing and training set to train linear_model\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=1) ","9fbd8ac3":"X = np.matrix(X_train) # using for creating our algorithm\nprint(\"X.shape : \", X.shape)\n\nY = np.matrix(y_train) # using for creating our algorithm\nprint(\"Before Y.shape : \", Y.shape)\n\n# transpoing Y as we need it to get 1 column\nY = Y.T\nprint(\"Afer Y.shape : \", Y.shape)","c9a270e8":"model1 = linear_model.LinearRegression()   \nmodel1.fit(X_train, y_train) \n\nprint('Coefficients (theta) for model 1: \\n', model1.coef_) ","b8b87ec1":"# we are going to use r2_score (R square score) to find the accuracy, you can see its same as model.score\ny_pred_1 = model1.predict(X_test)\nprint(\"Test accuracies for method 1 : \\nR square score : \",r2_score(y_test, y_pred_1), \", model.score : \",model1.score(X_test,y_test))","afb24f8b":"figure(num=None, figsize=(12, 8))\nplt.plot(np.arange(15),y_test, color = \"blue\", label = 'Actual Profit') \nplt.plot(np.arange(15),y_pred_1, color = \"red\", label = 'Linear regressor') \nplt.ylabel(\"Target : Profit\")\nplt.title(\"Comparing actual profit with predicted profit using Linear regression\")\nplt.legend()\nplt.show()","0130d98b":"first = np.matmul(X.transpose(),X) # X.transpose() or X.T\nfirst = np.linalg.inv(first) # for inverse\nfirst.shape","052440dc":"second = np.matmul(X.transpose(),Y) # X transpose * Y \nsecond.shape","33563369":"# computing the theta\ntheta = np.matmul(first,second)\ntheta","d75a188b":"# checking the accuracy\ny_pred_2 = np.matmul(np.matrix(X_test),theta)\nprint(\"Test accuracies for method 2 : \\nR square score : \", r2_score(y_test, y_pred_2))","6f9150fe":"figure(num=None, figsize=(12, 8))\nplt.plot(np.arange(15),y_test, color = \"blue\", label = 'Actual Profit') \nplt.plot(np.arange(15),y_pred_1, color = \"red\", label = 'Linear regressor') \nplt.plot(np.arange(15),np.array(y_pred_2), color = \"green\", label = 'Matrix multiplication') \nplt.ylabel(\"Target : Profit\")\nplt.title(\"Comparing actual profit with predicted profit using Matrix multiplication and Linear Regression\")\nplt.legend()\nplt.show()","8bb8260f":"# function to normalize X and Y\ndef normalize_feature(z):\n    mean = np.mean(z, axis = 0)  # mean of column\n    range_ = np.ptp(z, axis=0) # range of column\n    z_norm = (z - mean)\/range_\n    return z_norm","107a5194":"# function to calculate cost(J) from error\ndef compute_cost(X, y, theta):\n    predictions = X.dot(theta) \n    errors = np.subtract(predictions, y) # predicted y - actual y\n    J = 1\/(2 * m) * errors.T.dot(errors) # squaring eeror so that -ve and +ve error doesn't counter each other\n\n    return J","fac7612f":"def gradient_descent(X, y, theta, alpha, iterations):\n    cost_history = np.zeros(iterations)\n    for i in range(iterations):\n        predictions = X.dot(theta)\n        errors = np.subtract(predictions, y)\n        sum_delta = (alpha \/ m) * X.transpose().dot(errors)\n        theta = theta - sum_delta\n        alph = alpha - alpha\/5 # decreasing alpha after each iteration\n        cost_history[i] = compute_cost(X, y, theta)  \n\n    return theta, cost_history","628b2247":"# printing the first five values of x_norm\nx_norm = normalize_feature(X)\nx_norm[:,0][:5]","e558c834":"# verifying the correctness of normalize_feature()\nfirst_col = X[:,0]\nmean = np.mean(first_col) # computing mean for first col\nrange_ = max(first_col) - min(first_col) # computing range for first col \nresult = (first_col - mean)\/range_ # finding values after normalizing it\nprint(\"First five values of 1st column after normalizing:\\n\", result[:5])","06f5add0":"first = np.linalg.inv(np.matmul(x_norm.T , x_norm)) # calulating inverse of x square\nsecond = np.matmul(x_norm.T , x_norm) # x square\nnp.matmul(first,second)","1d7fb679":"x_norm = normalize_feature(X)\nx_norm = np.c_[np.ones(35), x_norm] # appending 1 at the begnining\nx_norm[:1] # sample row","dae87d86":"y_norm = normalize_feature(Y)\ny_norm[:1] # sample row","874c581a":"theta = np.matrix(np.random.rand(4)) # using random values of theta\ntheta = theta.T \ntheta.shape","ef75ec3c":"alpha = 0.05\nm = Y.size\nprint(\"theta\", theta.shape)\ntheta, cost_history = gradient_descent(x_norm, y_norm, theta, alpha, 1000) # calculating gradient descent","845c58c4":"# updated theta\ntheta","64aa116e":"X_test_upd = np.c_[np.ones(15), X_test] # adding 1 in front of X_test to measure accuracy","f9b703f4":"# checking the accuracy\ny_pred_3 = np.matmul(X_test_upd,theta)\nprint(\"Test accuracies for method 3 : \\nR square score : \", r2_score(y_test, y_pred_3))","973c31c5":"alpha_list = [0.01, 0.02, 0.05, 0.1, 0.25, 0.5]\niterations = 1000\ncolor_list = ['red', 'green', 'yellow', 'purple', 'blue', 'orange', 'black']","59ad1f5c":"cost_history = []\ntheta_list = []\n\n# using zero instead of random\ntheta = np.matrix(np.zeros(4))\ntheta = theta.T\n    \nfor i in range(len(alpha_list)):\n    \n    alpha = alpha_list[i]\n    # computing cost and theta\n    theta_updated, cost_history = gradient_descent(x_norm, y_norm, theta, alpha, iterations)\n    theta_list.append(theta_updated)    # adding theta to theta_list\n    plt.plot(range(1, iterations +1), cost_history, color = color_list[i], label = 'alpha = '+ str(alpha))\n    \n    y_pred_3 = X_test_upd.dot(theta_updated)\n    print(\"For alpha \", alpha, \", score : \", r2_score(y_pred_3,y_test))\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Learning Rate On Convergence of Gradient Descent\")\nplt.legend()\n    ","6c9f9456":"alpha = 0.05\ntheta_updated, cost_history = gradient_descent(x_norm, y_norm, theta, alpha, iterations)\ny_pred_3 = X_test_upd.dot(theta_updated)\nprint(\"For alpha \", alpha, \", score : \", r2_score(y_pred_3,y_test))","3fdd4019":"figure(num=None, figsize=(12, 8))\nfigure(num=None, figsize=(12, 8))\nplt.plot(np.arange(15),y_test, color = \"blue\", label = 'Actual Profit') \nplt.plot(np.arange(15),y_pred_1, color = \"red\", label = 'Linear regressor') \nplt.plot(np.arange(15),np.array(y_pred_2), color = \"green\", label = 'Gradient descent') \nplt.ylabel(\"Target : Profit\")\nplt.title(\"Comparing actual profit with predicted profit using Gradient descent and Linear Regression\")\nplt.legend()\nplt.show()","78d43b67":"# single row will be passed to compute cost\ndef compute_cost(X, y, theta):\n    predictions = X.dot(theta)\n    errors = np.subtract(predictions, y) # y predicte - y actual\n    J = errors.T.dot(errors) #square of errors\n    \n    return J","4acec46d":"def stochastic_gradient_descent(X_norm, Y_norm, theta, alpha, iterations):\n    \n    cost_history = np.zeros(iterations)\n\n    for i in range(iterations):\n        # randomly taking any row\n        random_pos = random.randrange(1, X.shape[0]-1)\n        x = X_norm[random_pos,:]\n        y = Y_norm[random_pos,:]\n\n        predictions = x.dot(theta)\n        errors = np.subtract(predictions, y)\n        # using single row instead of whole dataset\n        sum_delta = (alpha) * x.transpose().dot(errors)\n        theta = theta - sum_delta\n        alph = alpha - alpha\/5\n        # cost based on random row \n        cost_history[i] = compute_cost(x, y, theta)  \n\n    return theta, cost_history","2f4a9800":"# using random value first to see how to run the algo\ntheta = np.matrix(np.random.rand(4))\ntheta = theta.T\niterations = 10000\n","acaa8cee":"alpha = 0.05\nm = Y.size\nprint(\"theta\", theta.shape)\ntheta, cost_history = stochastic_gradient_descent(x_norm, y_norm, theta, alpha, iterations)","ad332706":"theta","ced04fc6":"y_pred_4 = X_test_upd.dot(theta)\nr2_score(y_pred_4,y_test)","d2496aa3":"alpha_list = [0.01, 0.02, 0.05, 0.1, 0.25, 0.5]","f98f89ad":"theta = np.matrix(np.zeros(4))\ntheta = theta.T\ntheta_list = []\ncost_history_list = []\n\nfor i in range(len(alpha_list)):\n    \n    alpha = alpha_list[i]\n    # computinfg theta and cost from stochastic_gradient_descent\n    theta_updated, cost_history = stochastic_gradient_descent(x_norm, y_norm, theta, alpha, iterations)\n    theta_list.append(theta_updated)\n    cost_history_list.append(cost_history)\n    y_pred_4 = X_test_upd.dot(theta_updated)    \n    plt.plot(range(0, iterations), cost_history_list[i], color = color_list[i], label = 'alpha = '+ str(alpha_list[i]))\n\n    print(\"For alpha \", alpha, \", score : \", r2_score(y_pred_4,y_test))\n\nplt.rcParams[\"figure.figsize\"] = (10,6)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Learning Rate On Convergence of Stochastic Gradient Descent\")\nplt.legend()","4e59e6d2":"i = 0 # for alpha 0.01\ntheta_updated, cost_history = stochastic_gradient_descent(x_norm, y_norm, theta, alpha, iterations)\nplt.plot(range(0, iterations), cost_history_list[i], color = color_list[i], label = 'alpha = '+ str(alpha_list[i]))\ny_pred_4 = X_test_upd.dot(theta_list[i])\nprint(\"For alpha \", alpha_list[i], \", score : \", r2_score(y_pred_4,y_test))\nplt.rcParams[\"figure.figsize\"] = (32,8)\nplt.grid()\nplt.xlabel(\"Number of iterations\")\nplt.ylabel(\"cost (J)\")\nplt.title(\"Effect of Learning Rate On Convergence of Stochastic Gradient Descent\")\nplt.legend()","5325a672":"figure(num=None, figsize=(16, 8))\nplt.plot(np.arange(15),y_test, color = \"blue\", label = 'Actual Y') \nplt.plot(np.arange(15),np.array(y_pred_1), color = \"red\", label = 'Linear Regressor') \nplt.plot(np.arange(15),np.array(y_pred_4), color = \"green\", label = 'Stochastic Gradient Descent')\nplt.ylabel(\"Target : Profit\")\nplt.title(\"Comparing actual profit with predicted profit using Stochastic Gradient descent and Linear Regression\")\nplt.legend()\nplt.show()","c2ba3475":"### Verifying correctness of normalize_feature()","4787e705":"Both values are same for 1st five elements. Seems like function is working perfectly","b96712be":"<br><br>\n<hr>\n\n<center><h2> Method 2 : Normal Method of Matrix multiplication <\/h2><\/center>","cd8420d9":"### Getting started with gradient descent","742c2a9c":"<center><h2> Method 3 :  Gradient Descent Method <\/h2><\/center>","30b8d346":"**Derivation:**\n\nLet h<sub>\u03b8<\/sub>(x) = \u03b8<sup>T<\/sup>x\n                     = \u03b8<sub>0<\/sub>x<sub>0<\/sub> + \u03b8<sub>1<\/sub>x<sub>1<\/sub> + \u03b8<sub>2<\/sub>x<sub>2<\/sub> + ... + \u03b8<sub>n<\/sub>x<sub>n<\/sub>\n\nwhere n = number of features = 3,<br> and x<sub>0<\/sub> is taken as 1, thus we have appended 1 at the begining of the matrix \n\nCost function using mean square for m (35 here) rows:\n\nJ(\u03b8) = (1\/2m) \u03a3 <sup>m<\/sup><sub>i=1<\/sub> (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>)<sup>2<\/sup>\n\nand error using mean square = pred - actual y =  (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>)<sup>2<\/sup>\n\nUpdating \u03b8:\n\n\u03b8 = \u03b8 - \u03b1 * \u0394J(\u03b8)\n\nwhere ,\n\n \u0394J(\u03b8) = (1\/m) \u03a3 <sup>m<\/sup><sub>i=1<\/sub> (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>). x<sup>(i)<\/sup>\n\n\nwhere \u03b1 is the learning rate","5e95aabb":"<h3> Finding the correct leanring rate, \u03b1 using different \u03b1 values<\/h3>","99a68906":"<h4>Notes<\/h4>\n\n1. **normalize_feature(z)** :\n\nFor each value in a column or feature, we are subtracting it with the mean value of the feature and then dividing with the range\n\n<b>\nUpdated x<sub>j<\/sub> = x<sub>j<\/sub> - \u03bc(j) \/ range(j)\n<\/b>\n<br><br>\nwhere j is the feature.\n<br><br>\n\nIt is important to normalize the value in the range of -1 to 1 and the reason is that while calulcting J(\u03b8), we are squaring the errors.\nAs values of x are in the range of 1e6, the errors reaches 1e100+ after some iterations. And even though \u03b1 is small (let's say 1e-6), it is not able to make much difference and when we update \u03b8, it becomes eponentially large.\n\n2. <b>Reducing alpha<\/b><br>\n\nLarge value of alpha is only required in the starting to fastly get near the target. As we start moving closer to the target, we don't want to deviate from the goals by using large values of alpha.","4989b29d":"### Note \n\nWe are using np.zeros to initialize theta as using random gives different accuracy in each run and the reason is the head start which random theta gets. If theta is closer to actual coffitient and alpha is low, the model is already having high accuracy for that theta and alpha pair. But if theta is far and alpha is low than the model will struggle to reach the target.","6517a128":"<br><br><hr><br><br>\n\n<center><h2> Method 4: Stochastic Gradient Descent Method <\/h2><\/center>","221b2856":"**Derivation:**\n\nCost function J(\u03b8) = (Y - X\u03b8) <sup>T<\/sup>(Y - X\u03b8)\n\nApplying differntial calculus to find the minimum value for the curve:\n\n\n=> 2X<sup>T<\/sup>X\u03b8  - 2X<sup>T<\/sup>Y = 0\n\n=> X<sup>T<\/sup>X\u03b8 = X<sup>T<\/sup>Y\n\n=> \u03b8 = (X<sup>T<\/sup>X)<sup>-1<\/sup>(X<sup>T<\/sup>Y)\n\n[How we applied differential calculus on matrix : Link](https:\/\/www.stat.cmu.edu\/~cshalizi\/mreg\/15\/lectures\/13\/lecture-13.pdf)\n\n\nLet's solve it into two parts:\n\n**First** :  (X<sup>T<\/sup>X)<sup>-1<\/sup>\n\n**Second** :  (X<sup>T<\/sup>Y)","f0c7b20b":"### Loading the dataset","38217f11":"# Thank you","3b065739":"<center><h1> Linear Regression from Scratch <\/h1><\/center>\n\n<br><br>\n<br>\nWe are trying to make our own linear regressor from scratch using:\n<br><br>\n\n1. Normal Matrix Multiplication\n2. Using Gradient Descent \n3. Using Stochastic Gradient Descent \n\nWe are using <b>r2_score<\/b>, to calculate the test score ([Documentation](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.metrics.r2_score.html)). It can give results from -1 to 1. Anything -ve means model is the worse and 1 means the best.","f2e5b66e":"**alpha = 0.05, seems to give more score**","cc69dd57":"<br><br>\n<hr>\n\n# Using Gradients","0eec6282":"## References:\n\n1. [Multiple Linear Regression with Gradient Descent by Rakend Dubba, Kaggle](https:\/\/www.kaggle.com\/rakend\/multiple-linear-regression-with-gradient-descent)\n2. [Linear Regression with Multiple Variables by Ritchie Ng](https:\/\/www.ritchieng.com\/multi-variable-linear-regression\/)\n3. [Multivariate Linear Regression From Scratch With Python by Satish Gunjal](https:\/\/satishgunjal.com\/multivariate_lr\/)","6c0c7774":"### Defining all the functions","dfd27d81":"alpha = 0.01 seems to give better test score","da5cb6a8":"<center> <h2> Method 1 : Using the linear_model package <\/h2><\/center>\n\n<br><br>\nLet's start with the predefined linear_model","294ab5cd":"Stochastic Gradient Descent is used when you have large number of features and a huge dataset. I am still implementing it here just for the purpose of demonstration. \n\nBasically we are using only one row to compute loss instead of the whole training set. The row is descided randomly for each iteration.\n\nSo previously the cost function was:\n\nJ(\u03b8) = (1\/2m) \u03a3 <sup>m<\/sup><sub>i=1<\/sub> (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>)<sup>2<\/sup>\n\nNo, as we are using only 1 row, m = 1 and no \u03a3\n\nJ(\u03b8) = (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>)<sup>2<\/sup>\n\nUpdating \u03b8:\n\n\u03b8 = \u03b8 - \u03b1 * \u0394J(\u03b8)\n\nwhere ,\n\n\u0394J(\u03b8) = (h<sub>\u03b8<\/sub>(x<sup>(i)<\/sup>) - y <sup>(i)<\/sup>). x<sup>(i)<\/sup>\n","94a52681":"The costs will be fluctuating due to the fact that a random observation is used to calculate cost each time. The main thing is that is should deacrease overall.","5569bbac":"### Further checking\n\nTo varify the normalization is correct, you can find (X<sup>T<\/sup>.X)<sup>-1<\/sup>(X<sup>T<\/sup>.X),  which should return something close to an <b>Identity matrix<\/b>","72cb448a":"It looks like an identity matrix"}}