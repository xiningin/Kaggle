{"cell_type":{"5c26c0a3":"code","21093cab":"code","ad8c03aa":"code","262f8608":"code","3a6f41e1":"code","5b0502e0":"code","fdad042c":"code","27bd4daa":"code","18367099":"code","bbda32f2":"code","360ccb9d":"code","e13ff26b":"code","9af820de":"code","ab1c3dc7":"code","4b62a9e4":"code","a4dd7063":"code","4f43b8bc":"code","9788f9ce":"code","1d7bf212":"code","b5c7a5ed":"code","0edc97a0":"markdown","81d59b59":"markdown","c1ca617b":"markdown","f949ff53":"markdown","06f74142":"markdown","ff30b270":"markdown","60546964":"markdown","92494ffb":"markdown","1e87c999":"markdown","94d41edf":"markdown","28e5950c":"markdown","b51110de":"markdown","30401709":"markdown","57a52173":"markdown","4c06c614":"markdown","2fdaace4":"markdown","c0083162":"markdown","9a34a577":"markdown","14cffb83":"markdown","b2c25803":"markdown"},"source":{"5c26c0a3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","21093cab":"!pip install pycaret","ad8c03aa":"import pandas as pd\nfrom pycaret.nlp import *","262f8608":"data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndata.head()","3a6f41e1":"data = data[['text', 'target']]\ndata.head()","5b0502e0":"setup(data=data, target='text')","fdad042c":"exp = create_model(model='lda', num_topics = 3, multi_core=True)","27bd4daa":"lda_data = assign_model(exp)\nlda_data.head()","18367099":"lda_data.drop(['text', 'Dominant_Topic', 'Perc_Dominant_Topic'], axis=1, inplace=True)\nlda_data.head()","bbda32f2":"from pycaret.classification import * ","360ccb9d":"exp2 = setup(data=lda_data, target='target')","e13ff26b":"compare_models()","9af820de":"xgboost = create_model('xgboost')","ab1c3dc7":"interpret_model(xgboost)","4b62a9e4":"finalize_model(xgboost)","a4dd7063":"save_model('xgboost', 'xgb_basic')","4f43b8bc":"from pycaret.nlp import *\n\ntest_raw = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntest_raw = test_raw[['id','text']]\nsetup(data=test_raw, target='text')\nlda_test_data = assign_model(exp)\nlda_test_data.drop(['text', 'Dominant_Topic', 'Perc_Dominant_Topic'], axis=1, inplace=True)","9788f9ce":"predictions = predict_model(xgboost, data=lda_test_data)\npredictions.head()","1d7bf212":"output = predictions[['id', 'Label']]\noutput = output.rename(columns={'Label': 'target'})\noutput.head()","b5c7a5ed":"output.to_csv('output.csv', index=False)","0edc97a0":"As you can see above, pycaret setup the experiment for us. Now we can start experimenting. Here we use **LDA** to build a few topics.","81d59b59":"Lets see how easy it is to create a model with pycaret. First of all we need to install pycaret","c1ca617b":"Let's read data","f949ff53":"We can use finalize model to finalize and train the model on train and holdout data.","06f74142":"Now we create the submission file","ff30b270":"Most of you will frown upon the **from pycaret.nlp import*** part since this is not a best practice. Importing * like this may lead to namespace confusion but we are doing low code ML and our goal is to use get most out of few code lines. ","60546964":"According to to the above evaluation, **Extreme Gradient Boosting** has the highest Accuracy, AUC, Precision. This is no means a usable model but we just feed the raw dataset and archieved this.\n\nLets create **Extreme Gradient Boosting** and see see how things go.","92494ffb":"We need pycaret's classification modules to build a classification model.","1e87c999":"As you can see above, pycaret outputs a huge list of metadata about our experiment. It automatically split the dataset to test and train too (You can manually change the ratio if you want).\n\nNow the best part of pycaret. Just write **compare_models()** and pycaret will evaluate several different models on the dataset and generate a report.","94d41edf":"We can plot different attributes of the model. This helps to understand the model.\n\nAlso we can use the evaluate_model() to get a plot which will be differernt from model type to type.","28e5950c":"Now we have the dataset consists of processed data, target classification value and topic values. We can use this data in a classification model and try to predict the tweet class.\n\n**Dominant_Topic, Perc_Dominant_Topic** and actual **text** will not be useful in a classification model. We just need the topic data and the target class. So we can create a dataframe with those.","b51110de":"Now we build an experiment as before with our lda_data dataframe. Here the target is the target class which the tweets are belonged to.\n\nPycaret will implicitly detect the data types of each column and prompt to confirm. If the detected data types are correct, you can hit enter. Otherwise press esc and change the datatypes of the columns manulally and retry.","30401709":"# Low code ML\n\nMost of the times what prevent people from do data science experiments is, it involves so many steps. First you need to clean data: treat null values and so on. You can explore different algorithms after you do all these steps. Please don't take me wrong here. I'm not saying that those steps are not important. One need to do all those steps to better understand data but what if I say we can skip most of those steps and evaluate several models quickly and see how your data performs? Yes you can do that with pycaret.\n\n# What is Pycaret\n> PyCaret is an open source, low-code machine learning library in Python that aims to reduce the cycle time from hypothesis to insights. It enables data scientists and analysts to perform iterative end-to-end data science experiments efficiently and allows them to reach conclusions faster due to far less time spent in coding. PyCaret is essentially a Python wrapper around several machine learning libraries and frameworks such as scikit-learn, XGBoost, Microsoft LightGBM, spaCy and many more.\n\nYou can find more information on [PyPi](https:\/\/pypi.org\/project\/pycaret\/) and [Pycaret](https:\/\/pycaret.org\/) web site.\n","57a52173":"Now let's setup our experiment with pycaret. You will now understand why we imported *  from pycaret\n\nwe pass our pandas dataframe which contains tweet data. We need to specify the target parameter also. Since we are setup an nlp experiment, our target would be the name of the column which contain the text we want to classify.","4c06c614":"# Important notes\n\nAlthough this low code approach is easy to follow, I advice not to use it without the understanding of particular algorithms. The purpose of learning is not get higher score somehow. You need to understand why certain things works that way. Pycaret offers model tuning and creatin ensemble models in few lines of code.\n\nIf you are a total beginner, use pycaret to quickly build something out with raw data without tuning. Then start understanding the data, manually engineer the fearures and then try to beat the score you got from raw data. Iteratively you can gamify this process to beat your own scores untill you reach a higher score. \n\n**Use this to break the ice and get to the experiments. Don't use this blindly!**\n\nHappy Learning!!","2fdaace4":"We need to quickly build up something. So let's only consider **text** and **target**","c0083162":"Now we have a model to apply. So we apply the model to data and assign the output topic data to *lda_data* ","9a34a577":"Now we can predict on the test data. We need to do the LDA steps to test data and build topics extract topic and target to do predictions.","14cffb83":"Now the pycaret is installed, we need to import other necessary libraries and start building.","b2c25803":"Saving the model to a file is also possible."}}