{"cell_type":{"0ec43e80":"code","a6e4c8e8":"code","54063515":"code","b518bac6":"code","b9eebafb":"code","105238e5":"code","e3dcbd42":"code","c4d48910":"code","505071a2":"code","07e8a8f2":"code","8e3b00dd":"code","ace27786":"code","dd952cf4":"code","60542a79":"code","07aa4aaf":"code","9530a5af":"code","8cc0ff2d":"code","b0121465":"code","67479d68":"code","2766f0a9":"code","2fd32411":"code","7037fb6b":"code","2caa8722":"code","abd33625":"code","0a32c962":"code","19465da6":"code","641a0b80":"code","813fb76e":"code","8d381869":"code","7c913d2a":"code","3d6588d5":"code","2a38a3cf":"code","2f87d9f3":"code","f99228c0":"code","518ef1de":"code","efec4467":"code","f660ec39":"code","905a6493":"code","d978dbfe":"code","d60be24b":"code","02cdca49":"code","3018c117":"code","42c74714":"code","7921bb86":"code","09be39f2":"code","4a9d0867":"code","c68b9b1b":"markdown","49919941":"markdown","2f827410":"markdown","b22b6294":"markdown","4e805059":"markdown","2415f4d3":"markdown","8b9bbee6":"markdown","46f53636":"markdown","639d489a":"markdown","4b75360c":"markdown","f85f6bcd":"markdown","68d4feae":"markdown","0ee9618a":"markdown","051e830a":"markdown","bdc4ab6a":"markdown","1358dcc6":"markdown","4de4db5f":"markdown","64cf0d27":"markdown","2e5bb664":"markdown","2a1ce58a":"markdown","082677ba":"markdown","53419024":"markdown","03b850e9":"markdown","0d60aaf0":"markdown","f66191bb":"markdown","e528d741":"markdown","4537c30f":"markdown"},"source":{"0ec43e80":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a6e4c8e8":"# Set up IPython to show all outputs from a cell\nimport warnings\nfrom IPython.core.interactiveshell import InteractiveShell\n\nInteractiveShell.ast_node_interactivity = 'all'\n\nwarnings.filterwarnings('ignore', category=RuntimeWarning)\n\nRANDOM_STATE = 50\nEPOCHS = 4\nBATCH_SIZE = 64\nTRAINING_LENGTH = 50\nTRAIN_FRACTION = 0.7\nLSTM_CELLS = 64\nVERBOSE = 0\nSAVE_MODEL = True","54063515":"from tensorflow.python.client import device_lib\nprint(device_lib.list_local_devices())","b518bac6":"import pandas as pd\nimport numpy as np\n\n# Read in data\ndata = pd.read_csv('..\/input\/patent-abstracts\/neural_network_patent_query.csv', parse_dates=['patent_date'])\n\n# Extract abstracts\noriginal_abstracts = list(data['patent_abstract'])\nlen(original_abstracts)\n\ndata.head()","b9eebafb":"data['patent_abstract'][100]","105238e5":"import matplotlib.pyplot as plt\n%matplotlib inline\n\nplt.style.use('fivethirtyeight')\n\ndata['year-month'] = [\n    pd.datetime(year, month, 1) for year, month in zip(\n        data['patent_date'].dt.year, data['patent_date'].dt.month)\n]\n\nmonthly = data.groupby('year-month')['patent_number'].count().reset_index()\n\nmonthly.set_index('year-month')['patent_number'].plot(figsize=(16, 8))\nplt.ylabel('Number of Patents')\nplt.xlabel('Date')\nplt.title('Neural Network Patents over Time')","e3dcbd42":"monthly.groupby(monthly['year-month'].dt.year)['patent_number'].sum().plot.bar(\n    color='red', edgecolor='k', figsize=(12, 6))\nplt.xlabel('Year')\nplt.ylabel('Number of Patents')\nplt.title('Neural Network Patents by Year')","c4d48910":"data.head()","505071a2":"monthly.groupby(monthly['year-month'].dt.month)['patent_number'].sum().plot.bar(\n    color='green', edgecolor='m', figsize=(12, 6))\nplt.xlabel('Months')\nplt.ylabel('Number of Patents')\nplt.title('Neural Network Patents each Month in all these years')","07e8a8f2":"from keras.preprocessing.text import Tokenizer\n\nexample = 'This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas.'\ntokenizer = Tokenizer(filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts([example])\ns = tokenizer.texts_to_sequences([example])[0]\n' '.join(tokenizer.index_word[i] for i in s)","8e3b00dd":"tokenizer = Tokenizer(filters='\"#$%&*+\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\n#Notice that in filters I've removed opening and closing round brackets'()'\ntokenizer.fit_on_texts([example])\ns = tokenizer.texts_to_sequences([example])[0]\n' '.join(tokenizer.index_word[i] for i in s)\ntokenizer.word_index.keys()","ace27786":"import re\n\n\ndef format_patent(patent):\n    \"\"\"Add spaces around punctuation and remove references to images\/citations.\"\"\"\n\n    # Add spaces around punctuation\n    patent = re.sub(r'(?<=[^\\s0-9])(?=[.,;?])', r' ', patent)\n\n    # Remove references to figures\n    patent = re.sub(r'\\((\\d+)\\)', r'', patent)\n\n    # Remove double spaces\n    patent = re.sub(r'\\s\\s', ' ', patent)\n    return patent\n\n\nf = format_patent(example)\nf","dd952cf4":"tokenizer = Tokenizer(filters='\"#$%&*+\/:;<=>?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts([f])\ns = tokenizer.texts_to_sequences([f])[0]\n' '.join(tokenizer.index_word[i] for i in s)\ntokenizer.word_index.keys()","60542a79":"def remove_spaces(patent):\n    \"\"\"Remove spaces around punctuation\"\"\"\n    patent = re.sub(r'\\s+([.,;?])', r'\\1', patent)\n\n    return patent\n\n\nremove_spaces(' '.join(tokenizer.index_word[i] for i in s))","07aa4aaf":"formatted = []\n\n# Iterate through all the original abstracts\nfor a in original_abstracts:\n    formatted.append(format_patent(a))\n\nlen(formatted)","9530a5af":"formatted[5]","8cc0ff2d":"def make_sequences(texts,\n                   training_length=50,\n                   lower=True,\n                   filters='!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n'):\n    \"\"\"Turn a set of texts into sequences of integers\"\"\"\n\n    # Create the tokenizer object and train on texts\n    tokenizer = Tokenizer(lower=lower, filters=filters)\n    tokenizer.fit_on_texts(texts)\n\n    # Create look-up dictionaries and reverse look-ups\n    word_idx = tokenizer.word_index\n    idx_word = tokenizer.index_word\n    num_words = len(word_idx) + 1\n    word_counts = tokenizer.word_counts\n\n    print(f'There are {num_words} unique words.')\n\n    # Convert text to sequences of integers\n    sequences = tokenizer.texts_to_sequences(texts)\n\n    # Limit to sequences with more than training length tokens\n    seq_lengths = [len(x) for x in sequences]\n    over_idx = [\n        i for i, l in enumerate(seq_lengths) if l > (training_length + 20)\n    ]\n\n    new_texts = []\n    new_sequences = []\n\n    # Only keep sequences with more than training length tokens\n    for i in over_idx:\n        new_texts.append(texts[i])\n        new_sequences.append(sequences[i])\n\n    training_seq = []\n    labels = []\n\n    # Iterate through the sequences of tokens\n    for seq in new_sequences:\n\n        # Create multiple training examples from each sequence\n        for i in range(training_length, len(seq)):\n            # Extract the features and label\n            extract = seq[i - training_length:i + 1]\n\n            # Set the features and label\n            training_seq.append(extract[:-1])\n            labels.append(extract[-1])\n\n    print(f'There are {len(training_seq)} training sequences.')\n\n    # Return everything needed for setting up the model\n    return word_idx, idx_word, num_words, word_counts, new_texts, new_sequences, training_seq, labels","b0121465":"TRAINING_LEGNTH = 50\nfilters = '!\"%;[\\\\]^_`{|}~\\t\\n'\nword_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels = make_sequences(\n    formatted, TRAINING_LENGTH, lower=False, filters=filters)","67479d68":"n = 3\nfeatures[n][:10]","2766f0a9":"def find_answer(index):\n    \"\"\"Find label corresponding to features for index in training data\"\"\"\n\n    # Find features and label\n    feats = ' '.join(idx_word[i] for i in features[index])\n    answer = idx_word[labels[index]]\n\n    print('Features:', feats)\n    print('\\nLabel: ', answer)","2fd32411":"find_answer(n)","7037fb6b":"original_abstracts[0]","2caa8722":"find_answer(100)","abd33625":"from sklearn.utils import shuffle\n\n\ndef create_train_valid(features,\n                       labels,\n                       num_words,\n                       train_fraction=TRAIN_FRACTION):\n    \"\"\"Create training and validation features and labels.\"\"\"\n\n    # Randomly shuffle features and labels\n    features, labels = shuffle(features, labels, random_state=RANDOM_STATE)\n\n    # Decide on number of samples for training\n    train_end = int(train_fraction * len(labels))\n\n    train_features = np.array(features[:train_end])\n    valid_features = np.array(features[train_end:])\n\n    train_labels = labels[:train_end]\n    valid_labels = labels[train_end:]\n\n    # Convert to arrays\n    X_train, X_valid = np.array(train_features), np.array(valid_features)\n\n    # Using int8 for memory savings\n    y_train = np.zeros((len(train_labels), num_words), dtype=np.int8)\n    y_valid = np.zeros((len(valid_labels), num_words), dtype=np.int8)\n\n    # One hot encoding of labels\n    for example_index, word_index in enumerate(train_labels):\n        y_train[example_index, word_index] = 1\n\n    for example_index, word_index in enumerate(valid_labels):\n        y_valid[example_index, word_index] = 1\n\n    # Memory management\n    import gc\n    gc.enable()\n    del features, labels, train_features, valid_features, train_labels, valid_labels\n    gc.collect()\n\n    return X_train, X_valid, y_train, y_valid","0a32c962":"X_train, X_valid, y_train, y_valid = create_train_valid(\n    features, labels, num_words)\nX_train.shape\ny_train.shape","19465da6":"import sys\ndef check_sizes(gb_min=1):\n    for x in globals():\n        size = sys.getsizeof(eval(x)) \/ 1e9\n        if size > gb_min:\n            print(f'Object: {x:10}\\tSize: {size} GB.')\n\n\ncheck_sizes(gb_min=1)","641a0b80":"import pickle\n\ndef save_intermediate_results(data):\n    \n    for i in data:\n        with open(f'{i}.pkl','wb') as f:\n            pickle.dump(globals()[i], f)\n            \ndata=['word_idx', 'idx_word', 'num_words', 'word_counts', 'abstracts', 'sequences', 'features', 'labels','X_valid','y_valid']\nsave_intermediate_results(data)","813fb76e":"with open('.\/X_valid.pkl','rb') as f:\n     X_valid = pickle.load(f)\n     print(X_valid.shape)","8d381869":"import gc\ngc.enable()\ndel (word_idx, idx_word, num_words, word_counts, abstracts, sequences, features, labels ,X_valid,y_valid)\ngc.collect()","7c913d2a":"from keras.models import Sequential, load_model\nfrom keras.layers import LSTM, Dense, Dropout, Embedding, Masking, Bidirectional\nfrom keras.optimizers import Adam\n\nfrom keras.utils import plot_model","3d6588d5":"def make_word_level_model(num_words,\n                          lstm_cells=64,\n                          trainable=True,\n                          lstm_layers=1,\n                          bi_direc=False):\n    \"\"\"Make a word level recurrent neural network with option for pretrained embeddings\n       and varying numbers of LSTM cell layers.\"\"\"\n\n    model = Sequential()\n\n    model.add(\n            Embedding(\n                input_dim=num_words,\n                output_dim=100,\n                input_length=50,\n                trainable=True))\n\n    # If want to add multiple LSTM layers\n    if lstm_layers > 1:\n        for i in range(lstm_layers - 1):\n            model.add(\n                LSTM(\n                    lstm_cells,\n                    return_sequences=True,\n                    dropout=0.1,\n                    recurrent_dropout=0.1))\n\n    # Add final LSTM cell layer\n    if bi_direc:\n        model.add(\n            Bidirectional(\n                LSTM(\n                    lstm_cells,\n                    return_sequences=False,\n                    dropout=0.1,\n                    recurrent_dropout=0.1)))\n    else:\n        model.add(\n            LSTM(\n                lstm_cells,\n                return_sequences=False,\n                dropout=0.1,\n                recurrent_dropout=0.1))\n    model.add(Dense(128, activation='relu'))\n    # Dropout for regularization\n    model.add(Dropout(0.5))\n\n    # Output layer\n    model.add(Dense(num_words, activation='softmax'))\n\n    # Compile the model\n    model.compile(\n        optimizer='adam',\n        loss='categorical_crossentropy',\n        metrics=['accuracy'])\n    return model","2a38a3cf":"model = make_word_level_model(\n    16192,\n \n    lstm_cells=LSTM_CELLS,\n    trainable=True,\n    lstm_layers=1)\nmodel.summary()","2f87d9f3":"from IPython.display import Image\nplot_model(model, to_file=f'patent abstract.png', show_shapes=True)\n#Image(f'patent abstract.png')","f99228c0":"from keras.callbacks import EarlyStopping, ModelCheckpoint\n\nBATCH_SIZE = 128\n\n\ndef make_callbacks(model_name, save=SAVE_MODEL):\n    \"\"\"Make list of callbacks for training\"\"\"\n    callbacks = [EarlyStopping(monitor='val_loss', patience=5)]\n    callbacks=[]\n    if save:\n        callbacks.append(\n            ModelCheckpoint(\n                f'patent_abstracts.h5',\n                save_best_only=True,\n                save_weights_only=False))\n    return callbacks\n\n\ncallbacks = make_callbacks(model)","518ef1de":"\nhistory = model.fit(\n    X_train,\n    y_train,\n    epochs=15,\n    batch_size=128,\n    verbose=1,\n    callbacks=callbacks)\n    #validation_data=(X_valid, y_valid)","efec4467":"model.save('patents_abstracts')","f660ec39":"def load_values(data):\n    val=[]\n    for i in data:\n        \n        with open(f'{i}.pkl','rb') as f:\n            globals()[f'{i}'] = pickle.load(f)\n            print(type(globals()[f'{i}']))\n            val.append(globals()[f'{i}'])\n    return val\n\nX_valid,y_valid= load_values(['X_valid','y_valid'])\nX_valid.shape","905a6493":"results = model.evaluate(X_valid,y_valid, batch_size=128)","d978dbfe":"globals()['word_idx', 'idx_word', 'num_words', 'word_counts', 'abstracts', 'sequences', 'features', 'labels','X_valid','y_valid']=load_values(data)","d60be24b":"np.random.seed(40)\n\n# Number of all words\ntotal_words = sum(word_counts.values())\n\n# Compute frequency of each word in vocab\nfrequencies = [word_counts[word] \/ total_words for word in word_idx.keys()]\nfrequencies.insert(0, 0)","02cdca49":"frequencies[1:10], list(word_idx.keys())[0:9]","3018c117":"print(f'The accuracy is {round(100 * np.mean(np.argmax(y_valid, axis = 1) == 1), 4)}%.')","42c74714":"from IPython.display import HTML\n\n\ndef header(text, color='black'):\n    raw_html = f'<h1 style=\"color: {color};\"><center>' + \\\n        str(text) + '<\/center><\/h1>'\n    return raw_html\n\n\ndef box(text):\n    raw_html = '<div style=\"border:1px inset black;padding:1em;font-size: 20px;\">' + \\\n        str(text)+'<\/div>'\n    return raw_html\n\n\ndef addContent(old_html, raw_html):\n    old_html += raw_html\n    return old_html","7921bb86":"import random\n\n\ndef generate_output(model,\n                    sequences,\n                    training_length=50,\n                    new_words=50,\n                    diversity=1,\n                    return_output=False,\n                    n_gen=1):\n    \"\"\"Generate `new_words` words of output from a trained model and format into HTML.\"\"\"\n\n    # Choose a random sequence\n    seq = random.choice(sequences)\n\n    # Choose a random starting point\n    seed_idx = random.randint(0, len(seq) - training_length - 10)\n    # Ending index for seed\n    end_idx = seed_idx + training_length\n\n    gen_list = []\n\n    for n in range(n_gen):\n        # Extract the seed sequence\n        seed = seq[seed_idx:end_idx]\n        original_sequence = [idx_word[i] for i in seed]\n        generated = seed[:] + ['#']\n\n        # Find the actual entire sequence\n        actual = generated[:] + seq[end_idx:end_idx + new_words]\n\n        # Keep adding new words\n        for i in range(new_words):\n\n            # Make a prediction from the seed\n            preds = model.predict(np.array(seed).reshape(1, -1))[0].astype(\n                np.float64)\n\n            # Diversify\n            preds = np.log(preds) \/ diversity\n            exp_preds = np.exp(preds)\n\n            # Softmax\n            preds = exp_preds \/ sum(exp_preds)\n\n            # Choose the next word\n            probas = np.random.multinomial(1, preds, 1)[0]\n\n            next_idx = np.argmax(probas)\n\n            # New seed adds on old word\n            seed = seed[1:] + [next_idx]\n            generated.append(next_idx)\n\n        # Showing generated and actual abstract\n        n = []\n\n        for i in generated:\n            n.append(idx_word.get(i, '< --- >'))\n\n        gen_list.append(n)\n\n    a = []\n\n    for i in actual:\n        a.append(idx_word.get(i, '< --- >'))\n\n    a = a[training_length:]\n\n    gen_list = [\n        gen[training_length:training_length + len(a)] for gen in gen_list\n    ]\n\n    if return_output:\n        return original_sequence, gen_list, a\n\n    # HTML formatting\n    seed_html = ''\n    seed_html = addContent(seed_html, header(\n        'Seed Sequence', color='darkblue'))\n    seed_html = addContent(seed_html,\n                           box(remove_spaces(' '.join(original_sequence))))\n\n    gen_html = ''\n    gen_html = addContent(gen_html, header('RNN Generated', color='darkred'))\n    gen_html = addContent(gen_html, box(remove_spaces(' '.join(gen_list[0]))))\n\n    a_html = ''\n    a_html = addContent(a_html, header('Actual', color='darkgreen'))\n    a_html = addContent(a_html, box(remove_spaces(' '.join(a))))\n\n    return seed_html, gen_html, a_html","09be39f2":"seed_html, gen_html, a_html = generate_output(model, sequences,\n                                              TRAINING_LENGTH)\nHTML(seed_html)\nHTML(gen_html)\nHTML(a_html)","4a9d0867":"seed_html, gen_html, a_html = generate_output(\n    model, sequences, TRAINING_LENGTH, diversity=0.6)\nHTML(seed_html)\nHTML(gen_html)\nHTML(a_html)","c68b9b1b":"# **Introduction: Writing Patent Abstracts with a Recurrent Neural Network**\n\nThe purpose of this notebook is to develop a recurrent neural network using LSTM cells that can generate patent abstracts. We will look at using a word level recurrent neural network and embedding the vocab,  with training our own embeddings. We will train the model by feeding in as the features a long sequence of words (for example 50 words) and then using the next word as the label. Over time, the network will (hopefully) learn to predict the next word in a given sequence and we can use the model predictions to generate entirely novel patent abstracts.","49919941":"**We can apply this operation to all of the original abstracts.**","2f827410":"# END-NOTES\n\nHope you all liked this kernel, please fork it and experiment with it if you are learning about RNN.\n\nThings that can be added:-\n\n* Train for longer period of time using another optimizer\n* Change the length of extracts from 50 to 60 or 40 \n* Change the output dimension of embedding layer and batch size\n* Try to add more dense layers.\n* Use pre-trained embeddings like Glove or word2vec etc.\n* Add many other suitable pre-processing techniques.\n* Make the model bi-directional and see the results.","b22b6294":"# Callbacks\n\n* Early Stopping: Stop training when validation loss no longer decreases\n* Model Checkpoint: Save the best model on disk","4e805059":"We do want to be careful about using up too much memory. One hot encoding the labels creates massive numpy arrays so I took care to delete the un-used objects from the workspace.","2415f4d3":"This removes all the punctuation and now we have a random number in the sentence. If we choose to not remove the punctuation, the sentence looks better, but then we have some interesting words in the vocabulary.","8b9bbee6":"Lets have a look at some features and corresponding labels to ensure everything is correct until now.","46f53636":"Now when we do the tokenization, we get separate entries in the vocab for the punctuation, but not for words with punctuation attached.","639d489a":"Each patent is now represented as a sequence of integers. Let's look at an example of a few features and the corresponding labels. The label is the next word in the sequence after the first 50 words.","4b75360c":"# **Features and Labels**\n\nThis function takes a few parameters including a training length which is the number of words we will feed into the network as features with the next word the label. For example, if we set training_length = 50, then the model will take in 50 words as features and the 51st word as the label.\n\nFor each abstract, we can make multiple training examples by slicing at different points. We can use the first 50 words as features with the 51st as a label, then the 2nd through 51st word as features and the 52nd as the label, then 3rd - 52nd with 53rd as label and so on. This gives us much more data to train on and the performance of the model is proportional to the amount of training data.","f85f6bcd":"# The approach to solving this problem is:\n\n1. Read in training data: thousands of \"neural network\" patents\n2. Convert patents to integer sequences: tokenization\n3. Create training dataset using next word following a sequence as label\n4. Build a recurrent neural network using word embeddings and LSTM cells\n5. Train network to predict next word from sequence\n6. Generate new abstracts by feeding network a seed sequence\n7. Repeat steps 2 - 7 using pre-trained embeddings for experimentation\n8. Try different model architecture to see if performance improves\n9. For fun, create a simple game where we must guess if the output is human or computer!\n\nEach of these steps is relatively simple by itself, so don't be intimidated. We'll walk through the entire process and at the end will be able to have a working application of deep learning!","68d4feae":"The diversity parameter determines how much randomness is added to the predictions. If we just use the most likely word for each prediction, the output sometimes gets stuck in loops. The diversity means the predicted text has a little more variation.","0ee9618a":"# **Data Cleaning**\nOur preprocessing is going to involve using a Tokenizer to convert the patents from sequences of words (strings) into sequences of integers. We'll get to that in a bit, but even with neural networks, having a clean dataset is paramount. The data quality is already high, but there are some idiosyncracies of patents as well as general text improvements to make. For example, let's consider the following two sentences.\n\n> This is a short sentence (1) with one reference to an image. This next sentence, while non-sensical, does not have an image and has two commas\n\nIf we choose to remove all punctuation with the default Tokenizer settings, we get the following.","051e830a":"# Train Model\n\nWe can now train the model on our training examples. We'll make sure to use early stopping with a validation set to stop the training when the loss on the validation set is no longer decreasing. Also, we'll save the best model every time the validation loss decreases so we can then load in the best model to generate predictions.","bdc4ab6a":"We no longer have the image and image. problem but we do have separate symbols for . and ,. This means the network will be forced to learn a representation for these punctuation marks (they are also in the pre-trained embeddings). When we want to get back to the original sentence (without image references) we simply have to remove the spaces.","1358dcc6":"***Brief Data Exploration***\n\nThis data is extremely clean, which means we don't need to do any manual munging. We can still make a few simple plots out of curiousity though!","4de4db5f":"# Saving the results for avoiding extreme memory usage.\n\nSave the intermediate results as it is consuming too much memory!!","64cf0d27":"To check how the model compares to just using the word frequencies to make predictions, we can compute the accuracy if we were to use the most frequent word for every guess. We can also choose from a multinomial distribution using the word frequencies as probabilities.","2e5bb664":"# Training Data\n\nNext we need to take the features and labels and convert them into training and validation data. The following function does this by splitting the data - after random shuffling because the features were made in sequential order - based on the train_fraction specified. All the inputs are converted into numpy arrays which is the correct input to a keras neural network.\n\n# Encoding of Labels\n\nOne important step is to convert the labels to one hot encoded vectors because our network will be trained using categorical_crossentropy and makes a prediction for each word in the vocabulary (we can train with the labels represented as simple integers, but I found performance was better and training faster when using a one-hot representation of the labels). This is done by creating an array of rows of all zeros except for the index of the word which we want to predict - the label - which gets a 1.","2a1ce58a":"# Build Model\n\nThis model is relatively simple and uses an LSTM cell as the heart of the network. After converting the words into embeddings, we pass them through a single LSTM layer, then into a fully connected layer with relu activation before the final output layer with a softmax activation. The final layer produces a probability for every word in the vocab.\n\nWhen training, these predictions are compared to the actual label using the categorical_crossentropy to calculate a loss. The parameters (weights) in the network are then updated using the Adam optimizer (a variant on Stochastic Gradient Descent) with gradients calculated through backpropagation. Fortunately, Keras handles all of this behind the scenes, so we just have to set up the network and then start the training. The most difficult part is figuring out the correct shapes for the inputs and outputs into the model.","082677ba":"I'm creating this notebook to understand working of RNN and LSTM in Keras. This notebook is inspired from [this github repository](https:\/\/github.com\/WillKoehrsen\/recurrent-neural-networks). Please do check it out.I'm just going to write whatever is there in repo and experiment with it. ***This is not my work!! It belongs to [Will Koehrsen](https:\/\/github.com\/WillKoehrsen)*** ","53419024":"**Deciding which pre-processing steps to take in general is the most important aspect of an machine learning project.**","03b850e9":"# **Generating Output**\n\nNow for the fun part: we get to use our model to generate new abstracts. To do this, we feed the network a seed sequence, have it make a prediction, add the predicted word to the sequence, and make another prediction for the next word. We continue this for the number of words that we want. We compare the generated output to the actual abstract to see if we can tell the difference!","0d60aaf0":"# Read in Data\nOur data consists of patent abstracts by searching for the term \"neural networks\" on [patentsview query web](https:\/\/www.patentsview.org\/querydev) interface. The data can be downloaded in a number of formats and can include a number of patent attributes (I only kept 4).","f66191bb":"# **Convert Text to Sequences**\nA neural network cannot process words, so we must convert the patent abstracts into integers. This is done using the Keras utility Tokenizer. By default, this will convert all words to lowercase and remove punctuation but we can avoid that. Then we train the embeddings along with the network at once.","e528d741":"Notice that *image* and *image.* are classified as distinct words. This is because the period is attached to one and not the other and the same with sentence and sentence,. To alleviate this issue, we can add spaces around the punctuation using regular expressions. We will also remove the image references.","4537c30f":"The most common word is 'the'. Let's see the accuracy of guessing this for every validation example."}}