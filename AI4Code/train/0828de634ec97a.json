{"cell_type":{"6e85fa7a":"code","4a952d11":"code","14b7f9ab":"code","cb6d2944":"code","d94702f1":"code","2b3b7fad":"code","63449f64":"code","ada72d30":"code","885be56c":"code","8728ec73":"code","26e85ced":"code","e01984c1":"code","65f3e242":"code","8954a93c":"markdown","db4b8da6":"markdown","5320fc25":"markdown","27bc5f07":"markdown","0b064332":"markdown","1cea181e":"markdown","d039c78d":"markdown","1cf5df38":"markdown","8efdb5cc":"markdown","288b60c3":"markdown","1ac83ec8":"markdown","d30e1e2a":"markdown","10145f04":"markdown","48fa13b5":"markdown","f7ed7a51":"markdown","fd1a1848":"markdown"},"source":{"6e85fa7a":"!pip install wordninja\nimport pandas as pd\nimport numpy as np\nfrom nltk.corpus import stopwords\nimport re\nimport string\nimport emoji\nimport spacy\nimport wordninja #Someone else implemented this ->  https:\/\/github.com\/keredson\/wordninja\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nimport xgboost\nfrom xgboost import XGBClassifier\npd.set_option('display.max_colwidth', 135)\n#!python -m spacy download en\nnlp =spacy.load('en_core_web_sm')\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom catboost import CatBoostClassifier","4a952d11":"tweets_file = '..\/input\/airline-sentiment\/Tweets.csv'\ndata=pd.read_csv(tweets_file)\nprint (\"data_shape: \", data.shape)\ndata.head(2)","14b7f9ab":"def clean_and_tokenize_tweets(input_text):\n    \n    #remove_mentions, urls, hash_sign:\n    mention_words_removed= re.sub(r'@\\w+','',input_text)\n    hash_sign_removed=re.sub(r'#','',mention_words_removed)\n    url_removed=' '.join(word for word in hash_sign_removed.split(\" \") if not word.startswith('http'))\n    \n    #Transform emoji to text\n    demoj=emoji.demojize(url_removed)\n    \n    #Split compound words coming from hashtags\n    splitted=wordninja.split(demoj)\n    splitted=\" \".join(word for word in splitted)\n    \n    # Implement lemmatization & remove punctuation\n    lem = nlp(splitted)\n    punctuations = string.punctuation\n    punctuations=punctuations+'...'\n\n    sentence=[]\n    for word in lem:\n        word = word.lemma_.lower().strip()\n        if ((word != '-pron-') & (word not in punctuations)):\n            sentence.append(word)    \n            \n    #Remove stopwords\n    stop_words=set(stopwords.words('english'))\n    stop_words_removed=[word for word in sentence if word not in stop_words]\n    \n    return stop_words_removed","cb6d2944":"data[\"text_\"]=data[\"text\"].apply(clean_and_tokenize_tweets)\ndata[[\"text\",\"text_\"]].head()","d94702f1":"data[\"text_\"]=[\" \".join(word) for word in data[\"text_\"]]\nX_train, X_test=data[\"text_\"][:10000],data[\"text_\"][10000:]\ntfidf_vector = TfidfVectorizer()\nX_train=tfidf_vector.fit_transform(X_train)\nX_test = tfidf_vector.transform(X_test)\nylabels=data[\"airline_sentiment\"].map({\"negative\":-1,\"neutral\":0,\"positive\":1})\ny_train, y_test=ylabels[:10000],ylabels[10000:] ","2b3b7fad":"base_classifier=LogisticRegression(random_state=0, solver='lbfgs',max_iter=500,multi_class='multinomial').fit(X_train,y_train)\npred_train_base=base_classifier.predict(X_train)\npred_test_base=base_classifier.predict(X_test)","63449f64":"print(\"Logistic Regression Train Accuracy:\",np.round(metrics.accuracy_score(y_train, pred_train_base),4))\nprint(\"Logistic Regression Test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_base),4))\nprint(\"\")\nprint(\"Logistic Regression Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_base,labels=[-1, 0, 1]),sep=\"\\n\")","ada72d30":"gbm = GradientBoostingClassifier(n_estimators=180, max_depth=6, random_state=0,learning_rate=0.1)\ngbm.fit(X_train, y_train)\npred_train_gbm=gbm.predict(X_train)\npred_test_gbm=gbm.predict(X_test)","885be56c":"print(\"LGBM Train Accuracy:\",np.round(metrics.accuracy_score(y_train, pred_train_gbm),4))\nprint(\"LGBM Test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_gbm),4))\nprint(\"\")\nprint(\"LGBM Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_gbm,labels=[-1, 0, 1]),sep=\"\\n\")","8728ec73":"xgb_classifier = XGBClassifier(n_estimators=200,random_state=0,learning_rate=0.7,objective='multi:softprob',num_class=3)\nxgb_classifier.fit(X_train, y_train)\npred_train_xgb=xgb_classifier.predict(X_train)\npred_test_xgb=xgb_classifier.predict(X_test)","26e85ced":"print(\"XGBoost train Accuracy:\",metrics.accuracy_score(y_train, pred_train_xgb))\nprint(\"XGBoost test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_xgb),4))\nprint(\"\")\nprint(\"XGBoost Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_xgb,labels=[-1, 0, 1]),sep=\"\\n\")","e01984c1":"catboost_classifier = CatBoostClassifier(iterations=500, learning_rate=0.5, l2_leaf_reg=3.5, depth=8, rsm=0.98, eval_metric='AUC',use_best_model=True,random_seed=42,loss_function='MultiClass')\ncatboost_classifier.fit(X_train,y_train,eval_set=(X_test,y_test))\npred_train_catb = catboost_classifier.predict(X_train)\npred_test_catb = catboost_classifier.predict(X_test)","65f3e242":"print(\"CatBoost train Accuracy:\",metrics.accuracy_score(y_train, pred_train_catb))\nprint(\"CatBoost test Accuracy:\",np.round(metrics.accuracy_score(y_test, pred_test_catb),4))\nprint(\"\")\nprint(\"CatBoost Confusion Matrix:\",metrics.confusion_matrix(y_test, pred_test_catb,labels=[-1, 0, 1]),sep=\"\\n\")","8954a93c":"-----------------------------------------------------------------------------------------------------------------------------","db4b8da6":"## Clean and tokenize tweet texts","5320fc25":"-----------------------------------------------------------------------------------------------------------------------------","27bc5f07":"## Import Data","0b064332":"### Model metrics","1cea181e":"## Create tfidf vectorizer for text data","d039c78d":"### Model metrics","1cf5df38":"-----------------------------------------------------------------------------------------------------------------------------","8efdb5cc":"### Model metrics","288b60c3":"### Model metrics","1ac83ec8":"## Fourth Model - CatBoost","d30e1e2a":"## First model- Multinomial logistics regression","10145f04":"## Third model- XGBoost","48fa13b5":"# Modelling","f7ed7a51":"## General Imports","fd1a1848":"* ## Second model- GBM"}}