{"cell_type":{"51983676":"code","ae36a2d9":"code","9a4d9d2b":"code","583d97c5":"code","e25150cb":"code","777681dd":"code","4391b185":"code","d808390d":"code","4ff136a8":"code","4741ce46":"code","acd415df":"code","7b96de88":"code","c9e568b6":"code","8dfaaeb3":"code","98fd1d66":"code","20c6e046":"code","2dde8539":"code","ffef5529":"code","f9993dda":"code","0413148a":"code","701f150f":"code","cee75345":"code","290e8d61":"code","0f233a89":"code","19947610":"code","3018101c":"code","ce8c1c6b":"code","03656587":"code","800c9dfc":"code","fad3b6aa":"code","c417175a":"code","0b5c8db5":"code","0975d3b1":"code","6ef908aa":"code","187e2a20":"code","82ca647f":"code","5c0eaa0c":"code","4620104f":"code","eb811ff3":"code","fff0e5a0":"code","567a2197":"code","9126322d":"code","29405a25":"code","5f71b55a":"code","f845b38e":"code","1abfe658":"markdown","a109b844":"markdown","a123def1":"markdown","15f3223b":"markdown","7985bbeb":"markdown","ed180cbe":"markdown","ef6cf3fd":"markdown","aa22fcfe":"markdown","d52c907b":"markdown","b4607fe1":"markdown","031a6ab4":"markdown","47f26024":"markdown","933da560":"markdown","c19bc95c":"markdown","3e1c154a":"markdown","07b546e5":"markdown","44596d22":"markdown","32e76010":"markdown","f1e841be":"markdown","6a521468":"markdown","f2c199fe":"markdown","a07e02d7":"markdown","d1b76499":"markdown","5753ced8":"markdown","24bd7e7c":"markdown","ee5a65b4":"markdown","8e1a3832":"markdown","c36cd5f5":"markdown","baa976c6":"markdown","b56a2318":"markdown","ea428632":"markdown","ba2f81e1":"markdown","1357a280":"markdown","497cdafb":"markdown","f2fce52d":"markdown","f2296b69":"markdown","c5b435f1":"markdown","dcf31ccf":"markdown","00e420b3":"markdown","7be7466d":"markdown"},"source":{"51983676":"from IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n    horizontal-align: middle;\n}\nh1 {\n    text-align: center;\n    background-color: #272343;\n    padding: 20px;\n    margin: 0;\n    font-family: ariel;\n    border-radius: 10px\n}\n\nh2 {\n    text-align: center;\n    background-color: #C84C4B;\n    padding: 20px;\n    margin: 0;\n    color: white;\n    font-family: ariel;\n    border-radius: 10px\n}\n\nh3 {\n    text-align: center;\n    border-style: solid;\n    border-width: 3px;\n    padding: 12px;\n    margin: 0;\n    color: black;\n    font-family: ariel;\n    border-radius: 80px;\n    border-color: #bae8e8;\n}\n\nbody, p {\n    font-family: ariel;\n    font-size: 17px;\n    color: charcoal;\n}\ndiv {\n    font-size: 14px;\n    margin: 0;\n\n}\n\nh4 {\n    padding: 0px;\n    margin: 0;\n    font-family: ariel;\n    color: gold;\n}\n<\/style>\n\"\"\")","ae36a2d9":"import os\nimport re\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import STOPWORDS, WordCloud, ImageColorGenerator\nfrom collections import defaultdict\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom PIL import Image\nimport seaborn as sns\nfrom nltk.tokenize import word_tokenize\nimport string\nimport plotly.figure_factory as ff\nimport random","9a4d9d2b":"df = pd.read_csv('..\/input\/feedback-prize-2021\/train.csv')","583d97c5":"df.head()","e25150cb":"df['discourse_type'].value_counts()","777681dd":"fig = go.Figure()\nfig.add_traces(go.Bar(x=df['discourse_type'].value_counts().index, y=df['discourse_type'].value_counts().values))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2')","4391b185":"fig = go.Figure()\nfig.add_traces(go.Bar(x=df['discourse_type_num'].value_counts().index, y=df['discourse_type_num'].value_counts().values))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2')","d808390d":"all_lens_all = []\nfor s in df['discourse_type'].unique():\n    leads = df[df['discourse_type']==s]\n    all_lens = [len(x) for x in leads['discourse_text'].values]\n    all_lens_all.append(all_lens)\n    \nfig = go.Figure()\nfig = ff.create_distplot(all_lens_all, df['discourse_type'].unique(), show_hist=False, show_rug=False)\nfig.update_xaxes(showgrid=False, zeroline=False, range=[-100, 4000])\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', title='Distribution of no. of characters for each type')\nfig.show()","4ff136a8":"fig = go.Figure()\n\nfig.add_traces(go.Bar(y=df['discourse_type'].unique(), x=[np.mean(x) for x in all_lens_all], orientation='h'))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', title='Avg. len of characters for each type')","4741ce46":"fig = go.Figure()\nlabels = df['discourse_type'].unique()\nfor i in range(len(labels)):\n    fig.add_traces(go.Box(x=all_lens_all[i], name=labels[i], marker_color='#000', opacity=0.9))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', showlegend=False, title='Box plot of no. of characters for each type')","acd415df":"all_tokens_all = []\nfor s in df['discourse_type'].unique():\n    leads = df[df['discourse_type']==s]\n    all_tokens = [len(word_tokenize(x)) for x in leads['discourse_text'].values]\n    all_tokens_all.append(all_tokens)","7b96de88":"fig = go.Figure()\nfig = ff.create_distplot(all_tokens_all, df['discourse_type'].unique(), show_hist=False, show_rug=False)\nfig.update_xaxes(showgrid=False, zeroline=False, range=[-50, 1000])\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', title='Distribution of no. of tokens for each type')\nfig.show()","c9e568b6":"fig = go.Figure()\n\nfig.add_traces(go.Bar(y=df['discourse_type'].unique(), x=[np.mean(x) for x in all_tokens_all], orientation='h'))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', title='Avg. no of tokens for each type')","8dfaaeb3":"fig = go.Figure()\nlabels = df['discourse_type'].unique()\nfor i in range(len(labels)):\n    fig.add_traces(go.Box(x=all_tokens_all[i], name=labels[i], marker_color='purple', opacity=1))\nfig.update_xaxes(showgrid=False, zeroline=False)\nfig.update_yaxes(showgrid=False, zeroline=False)\nfig.update_layout(template='ggplot2', showlegend=False, title='Box plot of no. tokens for each type')","98fd1d66":"def generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","20c6e046":"N = 30","2dde8539":"df['discourse_text'] = [' '.join(df['discourse_text'][i].split()) for i in range(len(df))]","ffef5529":"less_toxic_unigrams = defaultdict(int)\nfor tweet in df['discourse_text']:\n    for word in generate_ngrams(tweet, 1):\n        less_toxic_unigrams[word] += 1\n        \ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nfig, axes = plt.subplots(ncols=1, figsize=(10, N\/\/3), dpi=60)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_less_100[0], x=unigrams_less_100[1], ax=axes, color='purple')\n\naxes.spines['right'].set_visible(False)\naxes.set_xlabel('')\naxes.set_ylabel('')\naxes.tick_params(axis='x', labelsize=13)\naxes.tick_params(axis='y', labelsize=13)\n\naxes.set_title(f'Top {N} most common unigrams in discourse_text', fontsize=15)\n\nplt.show()","f9993dda":"less_toxic_unigrams = defaultdict(int)\nfor tweet in df['discourse_text']:\n    for word in generate_ngrams(tweet, 2):\n        less_toxic_unigrams[word] += 1\n        \ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nfig, axes = plt.subplots(ncols=1, figsize=(10, N\/\/3), dpi=60)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_less_100[0], x=unigrams_less_100[1], ax=axes, color='pink')\n\naxes.spines['right'].set_visible(False)\naxes.set_xlabel('')\naxes.set_ylabel('')\naxes.tick_params(axis='x', labelsize=13)\naxes.tick_params(axis='y', labelsize=13)\n\naxes.set_title(f'Top {N} most common bi-grams in discourse_text', fontsize=15)\n\nplt.show()","0413148a":"less_toxic_unigrams = defaultdict(int)\nfor tweet in df['discourse_text']:\n    for word in generate_ngrams(tweet, 3):\n        less_toxic_unigrams[word] += 1\n        \ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nfig, axes = plt.subplots(ncols=1, figsize=(10, N\/\/3), dpi=60)\n# plt.tight_layout()\n\nsns.barplot(y=unigrams_less_100[0], x=unigrams_less_100[1], ax=axes, color='cyan')\n\naxes.spines['right'].set_visible(False)\naxes.set_xlabel('')\naxes.set_ylabel('')\naxes.tick_params(axis='x', labelsize=13)\naxes.tick_params(axis='y', labelsize=13)\n\naxes.set_title(f'Top {N} most common tri-grams in discourse_text', fontsize=15)\n\nplt.show()","701f150f":"import requests\nfrom io import BytesIO\n\ndef read_img_from_url(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_matrix = np.array(img)\n    return img_matrix\n\ndef read_txt_from_url(url, *size):\n    text = ' '.join(df[df['discourse_type']==labels[0]]['discourse_text'].values)\n    wc = WordCloud(background_color=\"white\", max_words=1000 , max_font_size=100, width=size[1], height=size[0], random_state=42)\n    wc.generate(text)\n    return wc.to_array()\n\nimg_url = \"https:\/\/user-images.githubusercontent.com\/74188336\/146169049-0ee1c10e-050f-4df7-b0de-4017ddddc43d.jpeg\"\nimg_matrix = read_img_from_url(img_url)\ntxt_url = \"https:\/\/en.wikipedia.org\/wiki\/Python_(programming_language)\"\ntxt_matrix = read_txt_from_url(txt_url, *img_matrix.shape)\n\n# print(img_matrix.shape, txt_matrix.shape)\nimg_matrix[txt_matrix == 255] = 0\n# print(img_matrix.shape)\n\nplt.figure(figsize=(10, 10), dpi=300)\nplt.imshow(img_matrix)\nplt.axis('off')\nplt.savefig('foo.png')\nplt.show()\n","cee75345":"\ndef read_img_from_url(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_matrix = np.array(img)\n    return img_matrix\n\ndef read_txt(*size):\n    text = ' '.join(df[df['discourse_type']==labels[1]]['discourse_text'].values)\n    wc = WordCloud(background_color=\"white\", max_words=1000 , max_font_size=50, width=size[1], height=size[0], random_state=42)\n    wc.generate(text)\n    return wc.to_array()\n\nimg_url = \"https:\/\/user-images.githubusercontent.com\/74188336\/146170074-78ca0799-2990-474c-8d27-8f1c11f273dd.jpeg\"\nimg_matrix = read_img_from_url(img_url)\ntxt_matrix = read_txt(*img_matrix.shape)\n\n# print(img_matrix.shape, txt_matrix.shape)\nimg_matrix[txt_matrix == 255] = 0\n# print(img_matrix.shape)\n\nplt.figure(figsize=(10, 10), dpi=300)\nplt.imshow(img_matrix)\nplt.axis('off')\nplt.savefig('foo2.png')\nplt.show()\n","290e8d61":"\ndef read_img_from_url(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_matrix = np.array(img)\n    return img_matrix\n\ndef read_txt(*size):\n    text = ' '.join(df[df['discourse_type']==labels[2]]['discourse_text'].values)\n    wc = WordCloud(background_color=\"white\", max_words=1000 , max_font_size=50, width=size[1], height=size[0], random_state=42)\n    wc.generate(text)\n    return wc.to_array()\n\nimg_url = \"https:\/\/user-images.githubusercontent.com\/74188336\/146171120-5ac64564-ade3-4a48-ad05-7fc04562a61f.jpeg\"\nimg_matrix = read_img_from_url(img_url)\ntxt_matrix = read_txt(*img_matrix.shape)\n\n# print(img_matrix.shape, txt_matrix.shape)\nimg_matrix[txt_matrix == 255] = 0\n# print(img_matrix.shape)\n\nplt.figure(figsize=(10, 10), dpi=1000)\nplt.imshow(img_matrix)\nplt.axis('off')\nplt.savefig('foo3.png')\nplt.show()\n","0f233a89":"newdf = df[['discourse_text', 'discourse_type']]","19947610":"from sklearn.preprocessing import LabelEncoder","3018101c":"le = LabelEncoder()\nnewdf['discourse_type'] = le.fit_transform(newdf['discourse_type'])","ce8c1c6b":"newdf.head()","03656587":"len(newdf)","800c9dfc":"split = int(0.8*len(newdf))\ntraindf = newdf[:split]\ntestdf = newdf[split:]","fad3b6aa":"from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer","c417175a":"tfidf_vec = TfidfVectorizer(stop_words='english', ngram_range=(1,3))\ntfidf_vec.fit_transform(traindf['discourse_text'].values.tolist() + testdf['discourse_text'].values.tolist())\ntrain_tfidf = tfidf_vec.transform(traindf['discourse_text'].values.tolist())\ntest_tfidf = tfidf_vec.transform(testdf['discourse_text'].values.tolist())","0b5c8db5":"trainy = traindf['discourse_type'].values","0975d3b1":"testy = testdf['discourse_type'].values","6ef908aa":"trainy","187e2a20":"from sklearn import linear_model","82ca647f":"lr = linear_model.LogisticRegression(multi_class='multinomial', solver='sag')","5c0eaa0c":"lr.fit(train_tfidf, trainy)","4620104f":"ypred = lr.predict(test_tfidf)","eb811ff3":"from sklearn.metrics import accuracy_score","fff0e5a0":"accuracy_score(testy, ypred)","567a2197":"ids = []\nall_classes=[]\nall_idxs=[]\nfor file in os.listdir('..\/input\/feedback-prize-2021\/test'):\n    f = open('..\/input\/feedback-prize-2021\/test\/'+file, \"r\")\n    essay = f.read()\n    segments = [e for e in essay.split('.') if e!='']\n    idx = 0\n    \n    for segment in segments:\n        idxs = []\n        for word in segment.split():\n            idxs.append(str(idx))\n            idx+=1\n            \n        all_idxs.append(' '.join(idxs))\n    seg_tfidf = tfidf_vec.transform(segments)\n    seg_preds = lr.predict(seg_tfidf)\n    ids.extend([file.split('.')[0] for i in range(len(seg_preds))])\n    all_classes.extend(le.inverse_transform(seg_preds))\n    ","9126322d":"sub=pd.DataFrame()\nsub['id'] = ids\nsub['class'] = all_classes\nsub['predictionstring'] = all_idxs","29405a25":"sub.to_csv('submission.csv', index=False)","5f71b55a":"le_name_mapping = dict(zip(le.classes_, le.transform(le.classes_)))\nprint(le_name_mapping)","f845b38e":"import eli5\neli5.show_weights(lr, vec=tfidf_vec, top=100, feature_filter=lambda x: x != '<BIAS>')","1abfe658":"#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>1. Some distribution plots \ud83d\udcca<\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>2. The N-grams<\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>3. \u2601\ufe0f WordClouds coz why not! For the aesthetics \ud83d\ude43 <\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>4. TF-IDF<\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>5. Vectorizing and Training <\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>6. Submission time \u23f0 <\/span>\n#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>6. Visualizing the TF-IDF Vectorizer <b style='color:black'>(must see)<\/b><\/span>","a109b844":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Here I have plotted different types of distribution plots, box plots, histograms for number of occurances, number of characters, number of tokens etc..","a123def1":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> 6. Submission time \u23f0","15f3223b":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> Hmm.... Seems like there is a lot of writing about evidence","7985bbeb":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>N number of n-grams to visualize","ed180cbe":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> It's really amazing to visualize which words determine which class it belongs to \ud83d\ude03","ef6cf3fd":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Co<b style='color:gold'>nten<\/b>ts","aa22fcfe":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>2. The n-grams","d52c907b":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>There are 7 different `discourse_types`. Those are:\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 1. <b>Lead<\/b> - an introduction that begins with a statistic, a quotation, a description, or some other device to grab the reader\u2019s attention and point toward the thesis<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 2. <b>Position<\/b> - an opinion or conclusion on the main question<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 3. <b>Claim<\/b> - a claim that supports the position<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 4. <b>Counterclaim<\/b> - a claim that refutes another claim or gives an opposing reason to the position<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 5. <b>Rebuttal<\/b> - a claim that refutes a counterclaim<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 6. <b>Evidence<\/b> - ideas or examples that support claims, counterclaims, or rebuttals.<\/ul>\n    <ul style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'> 7. <b>Concluding Statemen<\/b> - a concluding statement that restates the claims<\/ul> <\/span>\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Let's check the distribution of the data for each `discourse_type`","b4607fe1":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> Making the dataframe for training\n    ","031a6ab4":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:272343'> Vectorizing... \u2699\ufe0f","47f26024":"## <span style='font-family:Bradley Hand, cursive;color:#fff'>Number of characters for each<\/span> `discourse_type`\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Firstly, we will be visualising the distribution of character lengths for each `discourse_type`.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>We will aslo visualise the avg number of characters for each `discourse_type`.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>And also, show the box plot to see the number of Inter Quartile Range (IQR) of the number of characters for each `discourse_type`. This will be crucial in helping select a considerable lenght while building our model.","933da560":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>I have done a simple sentence classifier mode. It is a multi-class classification (uses coss-entropy function with LogisticRegression). Used the TF-IDF vectorizer to vectorize the text and feed as the input variables.","c19bc95c":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Importing the libraries","3e1c154a":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:272343'> Label encoding the `discourse_type`","07b546e5":"![COVER](https:\/\/user-images.githubusercontent.com\/74188336\/146253091-252d4222-b242-490d-84c8-d62160016ee6.png)","44596d22":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:272343'> Splitting into train and test","32e76010":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>3. \u2601\ufe0f Word Clouds coz why not! <b style='color:gold'> For the aesthetics \ud83d\ude43<\/b>\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>WordCloud is a great way of visualizing the occurances of the most common words. The font size of the words depend on the occurance of that particular word in the whole corpus.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>**No text visualisation is complete without an WordCloud. So here it is \ud83d\ude03**","f1e841be":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> 1. Some distribution plots \ud83d\udcca<\/span>","6a521468":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Let's get started<\/span>","f2c199fe":"### <span style='font-family:Bradley Hand, cursive;color:#272343'>bi-grams\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Bi-grams are two words zipped together. If we iterate through each word in a sentence, then the pair of that word and the next word is called a bi-gram.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Let's take the previous sentence as example:\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>sentence : `Hello I am the Leader of the Nazis`\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>The bi-igrams are: `Hello I`, `I am`, `am the`, `the Leader`, `Leader of`, `of the`, `the Nazis`","a07e02d7":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>The discourse types are then numbered as 1, 2, 3.... for different parts of the essay\n    \n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Let's check the distribution of the data for each `discourse_type`","d1b76499":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Most Detailed EDA, TF-IDF and Logistic Regression [LB:0.167]","5753ced8":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:272343'> Training and predicting","24bd7e7c":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Please Do <i style='color:gold'>Upvote<\/i> if you like :)","ee5a65b4":"### <span style='font-family:Bradley Hand, cursive;color:#272343'>Uni-grams\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Unigrams are single words in a sentence. It's the smallest unit of word measurement.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>for e.g.\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>sentence : `Hello I am the Leader of the Nazis`\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>The unigrams are: `Hello`, `I`, `am`, `the`, `Leader`, `of`, `the`, `Nazis`","8e1a3832":"#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>Removing extra spaces\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>We need to remove extra spaces as essay have many extra spaces grouped together, which might show-up in top N n-grams","c36cd5f5":"## <span style='font-family:Bradley Hand, cursive;color:#fff'>Number of tokens for each<\/span> `discourse_type`","baa976c6":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> No. of occurances of discourse_type<\/span>","b56a2318":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> 7. Visualising the TF-IDF Vectorizer\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Let's see which words have the most weights for each class. I have printed the class mapping for reference","ea428632":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>5. <b style='color:gold'>Vectorizing<\/b> and <b style='color:gold'>Training<\/b>\n    ","ba2f81e1":"### <span style='font-family:Bradley Hand, cursive;color:#272343'>What are n-grams?\n\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>In the fields of computational linguistics and probability, an n-gram (sometimes also called Q-gram) is a contiguous sequence of n items from a given sample of text or speech. The items can be phonemes, syllables, letters, words or base pairs according to the application.","1357a280":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> If you have enjoyed this notebook, Please Do <i style='color:gold'>Upvote<\/i> :)","497cdafb":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Number of occurances of discourse_type_num<\/span> ","f2fce52d":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>In this notebook I have done some Exploratory Data Analysis of the data given in the `train.csv`. Plotting the distribution of the `discourse_type` and `discourse_type_num`, seeing the distribution of the **number of characters** and **number of tokens** present in each type. Plotting box-plots to see the IQR. \n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Also, visualising the different n-grams (uni-grams, bi-grams and tri-grams). \n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>This notebook also contains a detailed discussion on TF-IDF vectorizer. Then using TF-IDF vectorizer trained a Logistic Regression model and make a submission. (simple sentence classification)","f2296b69":"# <span style='font-family:Bradley Hand, cursive;color:#fff'> 4. TF-IDF\n\n### <span style='font-family:Bradley Hand, cursive;color:#000'>Learning about the tf-idf vectorizer.\n\n<b style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>tf : Term Frequency\n\n\n$$tf(t,d)= \\frac{\\text{count of the word t in d}}{\\text{total number of words in d}}$$\n\n\n<b style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>df : Document Frequency\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>This measures the importance of documents in a whole set of the corpus. This is very similar to TF but the only difference is that TF is the frequency counter for a term t in document d, whereas DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term is present in the document at least once, we do not need to know the number of times the term is present.\n\n$$df(t) = \\text{occurrence of t in N documents}$$\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>To keep this also in a range, we normalize by dividing by the total number of documents. Our main goal is to know the informativeness of a term, and DF is the exact inverse of it. that is why we inverse the DF\n\n\n<b style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>idf : inverse document frequency**\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>IDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because they are present in almost all of the documents, and N\/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n\n$$\\text{df}(t) = \\frac{\\text{N}}{\\text{df}}$$\n\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Now there are few other problems with the IDF, when we have a large corpus size say N=10000, the IDF value explodes. So to dampen the effect we take the log of IDF\n\n$$\\text{idf}(t) = \\log\\frac{N}{df + 1}$$\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Finally, by taking a multiplicative value of TF and IDF, we get the TF-IDF score. \n\n$$\\text{tf-idf}(t, d) = \\text{tf(t, d)}\\times\\log\\frac{N}{df + 1}$$\n\n<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#272343'>Previously thought of using the count vectorizer, but TF-IDF vectorizer automatically drops the weights of the words that doesn't contribute to the classification. That's why we won't need to remove the stopwords.","c5b435f1":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Overview","dcf31ccf":"#### <span style='font-family:Comic Sans MS, Comic Sans, cursive;color:#C84C4B'>n-gram generator","00e420b3":"<span style='font-family:Comic Sans MS, Comic Sans, cursive;color:272343'> The accuracy isn't that bad... \ud83d\ude03","7be7466d":"# <span style='font-family:Bradley Hand, cursive;color:#fff'>Distributions of <b style='color:gold'>number of characters<\/b> and <b style='color:gold'>number of tokens<\/b> "}}