{"cell_type":{"725c692c":"code","ff78c5df":"code","f7a5e392":"code","f80b22bc":"code","a4e06965":"code","958cfc8f":"code","1348f680":"code","a9744ef4":"code","9af54fe9":"code","afa2b226":"code","bdb2515a":"code","40c201f3":"code","4c1c46d7":"code","fa1eb40c":"code","afe66471":"code","24fdf7ff":"code","4fa01870":"code","5252778d":"code","310f2446":"code","879e1699":"code","6fea32ba":"code","d50acb33":"code","88160164":"code","bb6c06e1":"code","a7950f18":"code","f9e37c7e":"code","f5025aa6":"code","8fca4166":"code","8c251b90":"code","2c8ac2e0":"code","4976fb43":"code","2bbaf8cb":"code","48ea512d":"markdown","e7b56c95":"markdown","b44f1c6b":"markdown","3edc7386":"markdown","19fd722b":"markdown","22b24c14":"markdown","af6ff8b7":"markdown","ae8cfe19":"markdown","6b484071":"markdown","15650c6f":"markdown","87be5b44":"markdown","f1554aab":"markdown","27e391d6":"markdown","e5379403":"markdown","0de9339b":"markdown","4e3b0b43":"markdown"},"source":{"725c692c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ff78c5df":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n","f7a5e392":"df1=pd.read_csv('..\/input\/forest-cover-type-prediction\/train.csv')\ndf_test1=pd.read_csv('..\/input\/forest-cover-type-prediction\/test.csv')\ndf_test2=pd.read_csv('..\/input\/forest-cover-type-prediction\/test3.csv')\ndf=df1.copy()\ndf_test=df_test1.copy()","f80b22bc":"df","a4e06965":"pd.set_option('display.max_columns',None)\ndf.drop(columns=['Id','Cover_Type'],inplace=True)\ndf_test.drop(columns=['Id'],inplace=True)","958cfc8f":"df_test","1348f680":"X_train=df\nY_train=df1.iloc[:,-1]","a9744ef4":"X_train","9af54fe9":"df_test","afa2b226":"from collections import Counter\n\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom lightgbm import LGBMClassifier\n\nsns.set(style='white', context='notebook', palette='deep')\nkfold = StratifiedKFold(n_splits=10)\n","bdb2515a":"\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(XGBClassifier(random_state = random_state))\nclassifiers.append(LGBMClassifier(random_state = random_state))\n\ncv_results = []\nfor classifier in classifiers :\n    score=cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=-1)\n    cv_results.append(score)\n    print('{} crossvalidation score:{}\\n'.format(classifier,score.mean()))\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\",'XGboost','LGboost']})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n","40c201f3":"from sklearn.model_selection import train_test_split\nxtrain,xtest,ytrain,ytest=train_test_split(X_train.values,Y_train.values,test_size=0.2)","4c1c46d7":"from sklearn.metrics import accuracy_score\nRFC = RandomForestClassifier(random_state=random_state)\nRFC.fit(xtrain,ytrain)\nypred=RFC.predict(xtest)\nscore=cross_val_score(RFC,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))","fa1eb40c":"RFC.get_params()","afe66471":"from sklearn.metrics import accuracy_score\nRFC2 = RandomForestClassifier(random_state=random_state,\n                             n_estimators=500,\n                             max_depth=32,\n                             min_samples_leaf=1,\n                             criterion='entropy')\nRFC2.fit(xtrain,ytrain)\nypred=RFC2.predict(xtest)\nscore=cross_val_score(RFC2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for random forest: {}'.format(score.mean()))\nprint('Accuracy score for random forest: {}'.format(accuracy_score(ytest,ypred)))","24fdf7ff":"et=ExtraTreesClassifier(random_state=random_state)\net.fit(xtrain,ytrain)\nypred=et.predict(xtest)\nscore=cross_val_score(et,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))\n","4fa01870":"et2=ExtraTreesClassifier()\net2.get_params()","5252778d":"et2=ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,\n                     criterion='entropy', max_depth=38, max_features='auto',\n                     max_leaf_nodes=None, max_samples=None,\n                     min_impurity_decrease=0.0, min_impurity_split=None,\n                     min_samples_leaf=1, min_samples_split=2,\n                     min_weight_fraction_leaf=0.0, n_estimators=500,\n                     n_jobs=None, oob_score=False, random_state=0, verbose=0,\n                     warm_start=False)\net2.fit(xtrain,ytrain)\nypred=et2.predict(xtest)\nscore=cross_val_score(et2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for extra trees classifier: {}'.format(score.mean()))\nprint('Accuracy score for extra trees classifier: {}'.format(accuracy_score(ytest,ypred)))","310f2446":"lgb2=LGBMClassifier(random_state=random_state)\nlgb2.fit(xtrain,ytrain)\nypred=lgb2.predict(xtest)\nscore=cross_val_score(lgb2,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))","879e1699":"lgb=LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n        importance_type='split', learning_rate=0.2, max_depth=-1,\n        min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,\n        n_estimators=200, n_jobs=4, num_leaves=63, objective=None,\n        random_state=None, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n        subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\nlgb.fit(xtrain,ytrain)\nypred=lgb.predict(xtest)\nscore=cross_val_score(lgb,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))","6fea32ba":"vc= VotingClassifier(estimators=[('rfc', RFC2), ('extc', et2),\n('lgb',lgb)], voting='soft', n_jobs=-1)\nvc.fit(xtrain,ytrain)\nypred=vc.predict(xtest)\nscore=cross_val_score(vc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\n","d50acb33":"\"\"\"\nfrom sklearn.ensemble import StackingClassifier\nestimators = [ ('rf', RFC2),\n     ('et', et2)]\n\nsc= StackingClassifier(estimators=estimators, final_estimator=lgb)\nsc.fit(xtrain,ytrain)\nypred=sc.predict(xtest)\nscore=cross_val_score(sc,X_train,Y_train,scoring='accuracy',cv=kfold,n_jobs=-1)\n\n# Best score\nprint('Crossval score for Lightgb classifier: {}'.format(score.mean()))\nprint('Accuracy score for Lightgb classifier: {}'.format(accuracy_score(ytest,ypred)))\"\"\"","88160164":"vc.fit(X_train,Y_train)\nypred=vc.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('.\/submission_ensemblevoting.csv', index=False)","bb6c06e1":"\"\"\"\n#ExtraTrees \net2= ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\n \n \n 'criterion': ['gini','entropy'],\n 'max_depth':[5,10,25],\n 'max_features':[1,3,7],\n 'max_samples': [0.2],\n 'min_samples_leaf': [1,2,5],\n 'min_samples_split': [2,5,7],\n 'n_estimators': [100,200,300],\n }\n\n\ngset = GridSearchCV(et2,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs=-1, verbose = 1)\n\ngset.fit(X_train,Y_train)\ngset_best = gset.best_estimator_\n\n# Best score\nprint(gset.best_score_)\nprint(gset.best_estimator_)\"\"\"\n","a7950f18":"\"\"\"\n# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [80, 90, 100, 110],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4, 5],\n    'min_samples_split': [8, 10, 12],\n    'n_estimators': [100, 200, 300, 1000]\n}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_\"\"\"","f9e37c7e":"\"\"\"\nRFC2 = RandomForestClassifier()\nrf_param_grid = {\n    'bootstrap': [True],\n    'max_depth': [32],\n    'max_features': [2],\n    'min_samples_leaf': [1],\n    'min_samples_split': [6],\n    'n_estimators': [300]\n}\n\n\ngsRFC2 = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC2.fit(X_train,Y_train)\ngsRFC2.best_score_\"\"\"","f5025aa6":"pd.DataFrame(RFC.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]","8fca4166":"pd.DataFrame(et.feature_importances_,\n             index=X_train.columns, columns=['Importance']).sort_values(\n    by='Importance', ascending=False)[:10]","8c251b90":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(RFC,\"Random Forest learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(et,\"Extra trees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsRFC2,\"Random Forest tuned learning curves\",X_train,Y_train,cv=kfold)\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\n\n","2c8ac2e0":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(lgb,\"lgb tuned learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(lgb2,\"Normal lgb learning curves\",X_train,Y_train,cv=kfold)\n\n#g = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)","4976fb43":"#g = plot_learning_curve(vc,\"voting classifier learning curves\",X_train,Y_train,cv=kfold)","2bbaf8cb":"gset_best.fit(X_train,Y_train)\nypred=gset_best.predict(df_test.values)\n\nid=df_test1['Id']\ndict={'ID':id,'Cover_Type':ypred}\ndfsub=pd.DataFrame(dict)\ndfsub.to_csv('.\/submission_gset.csv', index=False)","48ea512d":"### Both models are little overfitting ","e7b56c95":"# ENSEMBLE VOTING CLASSIFIER","b44f1c6b":"## Random Forest Classifier","3edc7386":"## Modelling","19fd722b":"# Hyperparameter tuning of RF And Extratrees","22b24c14":"# Plotting learning curves","af6ff8b7":"## Submisssion File","ae8cfe19":"### Best models are random forst,extra trees, xgboost and lgboost","6b484071":"## ENSEMBLE STACKING CLASSIFIER","15650c6f":"## Comparing all models","87be5b44":"## Extra Trees Classifier","f1554aab":"## LightGb Classifier","27e391d6":"## Tuned LightGB","e5379403":"## Feature Importance","0de9339b":"## Tuned extra trees model","4e3b0b43":"## Tuned Random Forest"}}