{"cell_type":{"c4102261":"code","d44dfa4a":"code","c2b9def0":"code","704dd12c":"code","5100b52f":"code","7916681b":"code","f8502b2a":"code","223a98e2":"code","595438c0":"code","ec66c8e4":"code","28d28cf3":"code","c875ebd5":"code","36e4d559":"code","51ec9c94":"code","c054b2d0":"code","c21bd3fe":"code","891ce90a":"code","6965be49":"code","63c675a9":"code","ebeb1643":"code","c6d5f4bf":"code","74e812ad":"code","5fb1a439":"code","5765365a":"code","4b482abd":"code","fa20512d":"code","34f0dd8b":"code","3ccf215c":"code","afde6d9c":"code","a4751ca0":"code","2deaa629":"code","3382b2ca":"code","f348306f":"code","3867e9e6":"code","a09a5df9":"code","27b6efa6":"code","f931fcac":"code","2aa3fba1":"code","2efed4ab":"code","17e34f63":"code","0707ee3b":"code","5dd0f3e0":"code","a109e222":"code","da650474":"code","1ed630b2":"code","b83ab0a4":"code","ab367941":"code","daeca9db":"code","83daa88b":"code","228eee96":"code","091a1afa":"code","4992d69c":"code","fed4a83c":"code","01880f72":"code","e9704ddb":"markdown","a351dea9":"markdown","c1aebb44":"markdown","dc02c64c":"markdown","b1255fcd":"markdown","0c01c872":"markdown","53402c94":"markdown","5ee2bf04":"markdown","90815b57":"markdown","b89763f4":"markdown","09e5f257":"markdown","8a0e7ae4":"markdown","cf3a6456":"markdown","fe144b9b":"markdown","a5e1ba7c":"markdown","cae30f61":"markdown","31082787":"markdown","0f01d193":"markdown","00b45ca0":"markdown","a996c7b5":"markdown","a0d8ce7f":"markdown","205405e9":"markdown","de72447a":"markdown","b8fae1ac":"markdown","28da3abc":"markdown","63a0696d":"markdown","f3c3b248":"markdown","82bd97bb":"markdown","e6225596":"markdown","521e0038":"markdown","dbced1e1":"markdown","233c1b1e":"markdown","c0c4d5f6":"markdown","2d44bed8":"markdown","efef901e":"markdown","04d25371":"markdown","13a2f5d1":"markdown","d7cdffe1":"markdown","5ce6e43e":"markdown","bccc4555":"markdown","0da8c1d4":"markdown","f8f8e4d2":"markdown","fca0e7cc":"markdown","441488e5":"markdown","40f1ea75":"markdown","e4ef04a8":"markdown","8622dfc6":"markdown","ee1be85c":"markdown","26d88a72":"markdown","c5b65756":"markdown","e71ea714":"markdown","65ab8718":"markdown","94a6be91":"markdown","fb86b317":"markdown","069b67ab":"markdown","6de0c627":"markdown","e1485e82":"markdown","6a771523":"markdown","9c2ff56e":"markdown","1111f76a":"markdown"},"source":{"c4102261":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt \nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.ensemble import RandomForestRegressor\n\n","d44dfa4a":"df = pd.read_csv(\"..\/input\/california-housing-prices\/housing.csv\")","c2b9def0":"df.head()","704dd12c":"df.info()","5100b52f":"df[\"ocean_proximity\"].value_counts()","7916681b":"df.describe()","f8502b2a":"df.hist(bins = 50 , figsize=(30 , 20))","223a98e2":"df[\"income_cat\"] = pd.cut(df[\"median_income\"] , bins = [0 , 1.5 , 3.0 , 4.5 , 6 , np.inf] , \n                          labels=[1 , 2 , 3 , 4 , 5])","595438c0":"df[\"income_cat\"].hist(figsize=(8 , 4))","ec66c8e4":"from sklearn.model_selection import StratifiedShuffleSplit\n\nsplit = StratifiedShuffleSplit(n_splits= 1 , test_size= 0.2 , random_state=42)\nfor train_index, test_index in split.split(df, df[\"income_cat\"]):\n    strat_train_set = df.loc[train_index]\n    strat_test_set = df.loc[test_index]","28d28cf3":"strat_train_set[\"income_cat\"].value_counts()\/len(strat_train_set)","c875ebd5":"strat_test_set.head()","36e4d559":"for set_ in (strat_test_set , strat_train_set):\n    set_.drop(\"income_cat\" , axis = 1 ,  inplace = True)","51ec9c94":"strat_train_set.head()","c054b2d0":"housing = strat_train_set.copy()\n","c21bd3fe":"housing.plot(kind =\"scatter\" , x = \"longitude\" , y = \"latitude\")","891ce90a":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.1, figsize = (10 , 6))\n","6965be49":"housing.plot(kind=\"scatter\", x=\"longitude\", y=\"latitude\", alpha=0.4,\n s=housing[\"population\"]\/100, label=\"population\", figsize=(10,7),\n c=\"median_house_value\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n)\nplt.legend()","63c675a9":"corr_matrix = housing.corr()","ebeb1643":"corr_matrix[\"median_house_value\"].sort_values(ascending= False)","c6d5f4bf":"from pandas.plotting import scatter_matrix\nattributes = [\"median_house_value\", \"median_income\", \"total_rooms\",\n \"housing_median_age\"]\nscatter_matrix(housing[attributes], figsize=(12, 8))","74e812ad":"housing.plot(kind=\"scatter\", x=\"median_income\", y=\"median_house_value\",\n alpha=0.1)","5fb1a439":"housing[\"rooms_per_household\"] = housing[\"total_rooms\"]\/housing[\"households\"]\nhousing[\"bedrooms_per_room\"] = housing[\"total_bedrooms\"]\/housing[\"total_rooms\"]\nhousing[\"population_per_household\"]=housing[\"population\"]\/housing[\"households\"]","5765365a":"corr_matrix = housing.corr()\n>>> corr_matrix[\"median_house_value\"].sort_values(ascending=False)","4b482abd":"housing = strat_train_set.drop(\"median_house_value\" ,axis = 1)\nhousing_labels = strat_train_set[\"median_house_value\"].copy()","fa20512d":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy='median')","34f0dd8b":"housing_num = housing.drop(\"ocean_proximity\", axis=1)","3ccf215c":"imputer.fit(housing_num)","afde6d9c":"imputer.statistics_","a4751ca0":"housing_num.median().values","2deaa629":"X = imputer.transform(housing_num)","3382b2ca":"housing_tr = pd.DataFrame(X , columns=housing_num.columns)","f348306f":"housing_tr.head()","3867e9e6":"housing_tr.isna().sum()","a09a5df9":"housing_cat =  housing[[\"ocean_proximity\"]]\nhousing_cat.head()","27b6efa6":"from sklearn.preprocessing import OrdinalEncoder\nencoder = OrdinalEncoder()","f931fcac":"housing_cat_encoded = encoder.fit_transform(housing_cat)\nhousing_cat_encoded[:5]","2aa3fba1":"from sklearn.preprocessing import OneHotEncoder\nencoder.categories_","2efed4ab":"cat_encoder = OneHotEncoder()\nhousing_cat_1hot = cat_encoder.fit_transform(housing_cat)\nhousing_cat_1hot","17e34f63":"housing_cat_1hot.toarray()","0707ee3b":"cat_encoder.categories_","5dd0f3e0":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n ('imputer', SimpleImputer(strategy=\"median\")),\n ('std_scaler', StandardScaler()),\n ])\nhousing_num_tr = num_pipeline.fit_transform(housing_num)","a109e222":"from sklearn.compose import ColumnTransformer\nnum_attribs = list(housing_num)\ncat_attribs = [\"ocean_proximity\"]\n\nfull_pipeline = ColumnTransformer([\n (\"num\", num_pipeline, num_attribs),\n (\"cat\", OneHotEncoder(), cat_attribs),\n ])\n\nhousing_prepared = full_pipeline.fit_transform(housing)","da650474":"from sklearn.linear_model import LinearRegression\n\nlin_reg = LinearRegression()\nlin_reg.fit(housing_prepared  , housing_labels)","1ed630b2":"some_data = housing.iloc[:5]\nsome_labels = housing_labels.iloc[:5]\nsome_data_prepared = full_pipeline.transform(some_data)\nprint(\"Predictions : \" , lin_reg.predict(some_data_prepared))\nprint(\"\\n Labels : \",list(some_labels))","b83ab0a4":"from sklearn.metrics import mean_squared_error\nhousing_predictions = lin_reg.predict(housing_prepared)\nlin_mse = mean_squared_error(housing_labels, housing_predictions)\nlin_rmse = np.sqrt(lin_mse)\nlin_rmse","ab367941":"from sklearn.tree import DecisionTreeRegressor\n\ntree_reg = DecisionTreeRegressor()\ntree_reg.fit(housing_prepared , housing_labels)","daeca9db":"housing_predictions = tree_reg.predict(housing_prepared)\ntree_mse = mean_squared_error(housing_labels, housing_predictions)\ntree_rmse = np.sqrt(tree_mse)\ntree_rmse","83daa88b":"def display_scores (scores):\n    print(\"Scores:\", scores)\n    print(\"\\n Mean:\", scores.mean())\n    print(\"\\n Standard deviation:\", scores.std(),\"\\n\")\n    \n\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(tree_reg  , housing_prepared  , housing_labels , scoring=\"neg_mean_squared_error\"  , cv = 10 )\ntree_rmse_scores  = np.sqrt(-scores)\n\ndisplay_scores(tree_rmse_scores)","228eee96":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\nparam_grid = [\n    \n    {'n_estimators' : [3 , 10 , 30], 'max_features' : [2 , 4 , 6 , 8]},\n    {'bootstrap':[False] ,'n_estimators' : [3 , 10 ] , \"max_features\" : [2 ,3 ,4 ]},\n]\n\nforest_reg = RandomForestRegressor()\n\ngrid_search = GridSearchCV(forest_reg , param_grid , cv = 5 , scoring = 'neg_mean_squared_error' , return_train_score= False)\n\ngrid_search.fit(housing_prepared , housing_labels)","091a1afa":"grid_search.best_params_","4992d69c":"grid_search.best_estimator_","fed4a83c":"cvres = grid_search.cv_results_\nfor mean_score, params in zip(cvres[\"mean_test_score\"], cvres[\"params\"]):\n    print(np.sqrt(-mean_score), params)","01880f72":"final_model = grid_search.best_estimator_\n\nX_test = strat_test_set.drop(\"median_house_value\" , axis = 1)\nY_test = strat_test_set[\"median_house_value\"].copy()\n\nX_test_prepared = full_pipeline.transform(X_test)\nfinal_predictions= final_model.predict(X_test_prepared)\n\nfinal_mse = mean_squared_error(Y_test , final_predictions)\nfinal_rmse = np.sqrt(final_mse)\n\nprint(final_rmse)\n","e9704ddb":"## Transformation Pipelines\n\nScikit-Learn provides the Pipeline class to help with\nsuch sequences of transformations. Here is a small pipeline for the numerical\nattributes. The Pipeline constructor takes a list of name\/estimator pairs defining a sequence of\nsteps. All but the last estimator must be transformers (i.e., they must have a\nfit_transform() method). The names can be anything you like (as long as they are\nunique and don\u2019t contain double underscores \u201c__\u201d)When you call the pipeline\u2019s fit() method, it calls fit_transform() sequentially on\nall transformers, passing the output of each call as the parameter to the next call, until\nit reaches the final estimator, for which it just calls the fit() method.\n","a351dea9":"#### **NOTE : You can try this for others models and compare the results on your own**","c1aebb44":"1. Notice that the total_bedrooms attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature.\n\n2. All attributes are numerical, except the ocean_proximity field. Its type is object, so it could hold any kind of Python object, but since you loaded this data from a CSV file you know that it must be a text attribute\n\n3. You probably noticed that the values in the ocean_proximity column were repetitive, which means that it is probably a categorical attribute(*To know about categorical values you can check my Notebook for Beginners)*. You can find out what categories exist and how many districts belong to each category by using the **value_counts()** method:","dc02c64c":"### Imagine a Scenario !\n\nSuppose you chatted with experts who told you that the median income is a very important attribute to predict median housing prices.\n\n>You may want to ensure that the test set is representative of the various categories of incomes in the whole dataset.\n\n>Since the median income is a continuous numerical attribute, you first need to create\nan income category attribute.","b1255fcd":"![](https:\/\/miro.medium.com\/max\/1400\/1*ggtP4a5YaRx6l09KQaYOnw.png)","0c01c872":"# Evaluating System on the Test Set","53402c94":"# Taking a Quick Look on Data","5ee2bf04":"## PLotting the Histogram for Visualizing the data from above\n\n**We will plot others graphs and plots later on with detail in Data Visualization part**\n\n A histogram shows the number of instances (on the vertical axis) that have a given value range (on the horizontal axis). You can either plot this one attribute at a time, or you can call the hist() method on the whole dataset, and it will plot a histogram for each numerical attribute","90815b57":"the grid search will explore 12 + 6 = 18 combinations of RandomForestRegressor hyperparameter values, and it will train each model five times (since we are using five-fold cross validation). In other words, all in all, there will be 18 \u00d7 5 = 90 rounds of training! It may take quite a long time, but when it is done you can get the best combination of parameters like this","b89763f4":"## Cross Val Score\nA great alternative is to use Scikit-Learn\u2019s K-fold cross-validation feature. The following code randomly splits the training set into 10 distinct subsets called folds, then it trains and evaluates the Decision Tree model 10 times, picking a different fold for evaluation every time and training on the other 9 folds. The result is an array containing the 10 evaluation scores\n\n![](https:\/\/img-0.journaldunet.com\/MkFZ8DzLWNpTVqNSca_GAIFirY8=\/1080x\/smart\/e86b272c823d4ef8ac15596df4b937aa\/ccmcms-jdn\/11080126.jpg)","09e5f257":"#### This plot reveals a few things. \n\n> First, the correlation is indeed very strong; you can clearly see the upward trend and the points are not too dispersed. \n\n>Second, the price cap that we noticed earlier is clearly visible as a horizontal line at 500,000 dollar \n\n>>But this plot reveals other less obvious straight lines: a horizontal line around 450,000 dollars,\nanother around 350,000 dollars, perhaps one around 280,000 dollars, and a few more below that.\n\n>>**You may want to try removing the corresponding districts to prevent your algorithms\nfrom learning to reproduce these data quirks**","8a0e7ae4":"## Linear Regression\n\nLinear Regression is a machine learning algorithm based on supervised learning. It performs a regression task. Regression models a target prediction value based on independent variables. It is mostly used for finding out the relationship between variables and forecasting. Different regression models differ based on \u2013 the kind of relationship between dependent and independent variables, they are considering and the number of independent variables being used.\n\n![](https:\/\/www.jmp.com\/en_ch\/statistics-knowledge-portal\/what-is-multiple-regression\/fitting-multiple-regression-model\/_jcr_content\/par\/styledcontainer_2069\/par\/lightbox_4130\/lightboxImage.img.png\/1548703926664.png)","cf3a6456":"You can clearly see the high-density areas, namely the Bay Area and around Los Angeles and San Diego, plus a long line of fairly high density in the Central Valley, in particular around Sacramento and Fresno.\n\n[click here to check the places with longitude and latitude as a input](https:\/\/www.latlong.net\/)\n\n**This Information is unavialable to us in description and in the actual data. This info was available in the book**","fe144b9b":"**NOTE: The null values are Ignored**\n\n1. Count is the total number of values in particular column\n\n2. Mean is the mean of the particular column \n\n3. std is standard deviation of the particular column which measures how dispersed the values are.\n\n\n\n> The standard deviation is generally denoted \u03c3 (the Greek letter sigma), and it is the square root of  \n the variance, which is the average of the squared deviation from the mean. When a feature has a bell\n shaped normal distribution (also called a Gaussian distribution), which is very common, the\n \u201c68-95-99.7\u201d  rule applies: about 68% of the values fall within 1\u03c3 of the mean, 95% within 2\u03c3, and \n 99.7% within 3\u03c3`\n\n\n\n4. min is the minimum value and max is the maximum value in the particular column\n\n5. The 25%, 50%, and 75% rows show the corresponding percentiles : a percentile indicates the value below which a given percentage of observations in a group of observations falls. For example, 25% of the districts have a housing_median_age lower than 18, while 50% are lower than 29 and 75% are lower than 37. These are often called the **25th percentile (or 1st quartile), the median, and the 75th percentile (or 3rd quartile)**.","a5e1ba7c":"> The imputer has simply computed the median of each attribute and stored the result in its statistics_ instance variable.\n\n\n> Only the total_bedrooms attribute had missing values, but we cannot be sure that there won\u2019t be any missing values in new data after the system goes live, so it is safer to apply the imputer to all the numerical attributes:","cae30f61":"# Fine Tune Model","31082787":"### Different Method for Checking Correlations\n\nAnother way to check for correlation between attributes is to use Pandas\u2019\nscatter_matrix function, which plots every numerical attribute against every other\nnumerical attribute. Since there are now 11 numerical attributes, you would get 11^2\n=121 plots, which would not fit on a page, so let\u2019s just focus on a few promising\nattributes that seem most correlated with the median housing value","0f01d193":"The most promising attribute to predict the median house value is the median\nincome, so let\u2019s zoom in on their correlation scatterplot","00b45ca0":"Since the dataset is not too large, you can easily compute the standard correlation\ncoeffcient (also called Pearson\u2019s r) between every pair of attributes using the corr()\nmethod","a996c7b5":"#### Removing income_cat column from both strat_train and strat_test set so the data is back to its original state","a0d8ce7f":"**NOTE: Following Information is not available in the description of this Dataset so that's a good reason for you to read this Book**\n\n#### We get this following Information from this Histogram.\n\n1. First, the median income attribute does not look like it is expressed in US dollars(USD). After checking with the team that collected the data, you are told that the data has been scaled and capped at 15 (actually 15.0001) for higher medianincomes, and at 0.5 (actually 0.4999) for lower median incomes. The numbers represent roughly tens of thousands of dollars (e.g., 3 actually means about $30,000).\n\n\n2. The housing median age and the median house value were also capped.The latter may be a serious   problem since it is your target attribute (your labels). Your Machine Learning algorithms may learn that prices never go beyond that limit.You need to check with your client team (the team that will use your system\u2019s output) to see if this is a problem or not. If they tell you that they need precise predictions even beyond $500,000, then you have mainly two options:\n\n*      Collect proper labels for the districts whose labels were capped.\n\n*      Remove those districts from the training set (and also from the test set, sinceyour system should        not be evaluated poorly if it predicts values beyond$500,000)","205405e9":"## Grid Search\nOne way to do that would be to fiddle with the hyperparameters manually, until you find a great combination of hyperparameter values. This would be very tedious work,and you may not have time to explore many combinations.Instead you should get Scikit-Learn\u2019s GridSearchCV to search for you. All you need to do is tell it which hyperparameters you want it to experiment with, and what values to try out, and it will evaluate all the possible combinations of hyperparameter values,using cross-validation.\n","de72447a":"# Importing all Libraries","b8fae1ac":"we have handled the categorical columns and the numerical columns separately. It would be more convenient to have a single transformer able to handle all columns, applying the appropriate transformations to each column","28da3abc":"#### Now you are ready to do stratified sampling based on the income category. For this you can use Scikit-Learn\u2019s StratifiedShuffleSplit class","63a0696d":">The radius of each circle represents the district\u2019s population (option s), and the color represents the price (option c). We will use a predefined color map (option cmap) called jet, which ranges from blue (low values) to red (high prices)\n\n>It will probably be useful to use a clustering algorithm to detect the main clusters, and\nadd new features that measure the proximity to the cluster centers. The ocean prox\u2010\nimity attribute may be useful as well, although in Northern California the housing\nprices in coastal districts are not too high, so it is not a simple rule","f3c3b248":"# Looking for Correlations","82bd97bb":"**IMPORTANT: Since the median can only be computed on numerical attributes, we need to create a\ncopy of the data without the text attribute ocean_proximity**","e6225596":"### What is Stratified sampling ?\n\n![](https:\/\/media.makeameme.org\/created\/stratified-sample.jpg)\n\n#### The population is divided into homogeneous subgroups called strata,and the right number of instances is sampled from each stratum to guarantee that the test set is representative of the overall population\n\n>**NOTE:**  It Simply means that we take that sample from the data which can represent the whole data in the training set \n","521e0038":"## Decision Tree\n\nDecision Tree is a decision-making tool that uses a flowchart-like tree structure or is a model of decisions and all of their possible results, including outcomes, input costs, and utility.Decision-tree algorithm falls under the category of supervised learning algorithms. It works for both continuous as well as categorical output variables.\n\n![](https:\/\/miro.medium.com\/max\/1146\/1*RHN0lpDR96nsklb8b0YNNg.png)","dbced1e1":"#### Removing Null Values\nYou have three options for Removing Null Values:\n\n\u2022 Get rid of the corresponding districts.\n\n      housing.dropna(subset=[\"total_bedrooms\"]) # option 1\n    \n\n\u2022 Get rid of the whole attribute.\n\n     housing.drop(\"total_bedrooms\", axis=1) # option 2\n\n\n\u2022 Set the values to some value (zero, the mean, the median, etc.).\n\n    median = housing[\"total_bedrooms\"].median() # option 3\n    housing[\"total_bedrooms\"].fillna(median, inplace=True)\n\n\n\n>> **Don\u2019t forget to save the median value that you have computed. You will need it later to replace missing values in the test set when you want to evaluate your system, and also once the system goes live to replace missing values in new data.**\n\n\n\n\n\n##### * Scikit-Learn provides a handy class to take care of missing values -- SimpleImputer\n\nand we are going to use Simple Imputer In this case","233c1b1e":"Total number of rooms in a district is not very useful if you don\u2019t know how many households there are. What we really want is the number of rooms per household. Similarly, the total number of bedrooms by itself is not very useful,we probably want to compare it to the number of rooms. And the population per household also seems like an interesting attribute combination to look at.","c0c4d5f6":"Let's Check Correlation Again ","2d44bed8":"# Handling Text and Categorical Values","efef901e":"## df.summary()\nThe describe() method shows a summary of the numerical attributes ","04d25371":"# INTRODUCTION","13a2f5d1":"##### It is hard to see any particular pattern. Setting the alpha option to 0.1 makes it much easier to visualize the places where there is a high density of data points","d7cdffe1":"# Data visualization to gain Insights ","5ce6e43e":"Recently I bought this Book **\"O'Reilly Hands On Machine Learning with Scikit-Learn TensorFlow and Keras**.\n\nWhile going through this book I came came across this same callifornia house dataset so I decided to follow this book Throughly to check How much Accuracy I can achieve.\n\nMost of the part of this article is from Book + I added my observations and inferences in simple language and with some informative images from the web. Hope you will Enjoy reading this Notebook \u2764\n\n\n![](https:\/\/i.ytimg.com\/vi\/ytgp5FaH3kE\/sddefault.jpg)\n\n\nI will Prefer you to buy this book even if you are beginner to this whole machine Learning thing!\nIf you know a better Book for machine learning or deep learning.Please suggest me some in Comments\n\n\nThank You ! \ud83c\udf6a","bccc4555":"#### Here Our Data is quite small so we can work on the whole dataset Instead of working in batches.\n\n > First, make sure you have put the test set aside and you are only exploring the training set. Also, if the training set is very large, you may want to sample an exploration set, to make manipulations easy and fast\n\n\n![](https:\/\/qph.fs.quoracdn.net\/main-qimg-84f36a40492d11fdc8a0ce2b1f7376da)\n\n   **In case we don't mess things up with the original data we will create a copy of the dataframe**","0da8c1d4":"Wait, what!? No error at all? Could this model really be absolutely perfect? Of course,it is much more likely that the model has badly overfit the data. How can you be sure? As we saw earlier, you don\u2019t want to touch the test set until you are ready to launch a model you are confident about, so you need to use part of the training set for training, and part for model validation","f8f8e4d2":"## Data Cleaning","fca0e7cc":"> Most median income values are clustered around 1.5 to 6 (i.e.$15,000\u2013$60,000), but some median incomes go far beyond 6. \n\n>It is important to have a sufficient number of instances in your dataset for each stratum, or else the estimate of the stratum\u2019s importance may be biased.\n\n>This means that you should not have too many strata, and each stratum should be large enough.","441488e5":"The correlation coefficient ranges from \u20131 to 1. When it is close to 1, it means that\nthere is a strong positive correlation; for example, the median house value tends to go\nup when the median income goes up. When the coefficient is close to \u20131, it means\nthat there is a strong negative correlation; you can see a small negative correlation\nbetween the latitude and the median house value (i.e., prices have a slight tendency to\ngo down when you go north). Finally, coefficients close to zero mean that there is no\nlinear correlation.\n\n**Below Figure shows various plots along with the correlation coefficient between their horizontal and vertical axes.**\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/d\/d4\/Correlation_examples2.svg\/600px-Correlation_examples2.svg.png)\n\n### **Warning given in the Book \ud83d\udd3d**\n\n\n>The correlation coefficient only measures linear correlations (\u201cif x goes up, then y generally goes up\/down\u201d). \n\n>It may completely miss out on nonlinear relationships (e.g., \u201cif x is close to zero then y gen\u2010\nerally goes up\u201d). \n\n>**Note how all the plots of the bottom row have a correlation coefficient equal to zero despite the fact that their axes are clearly not independent**\n\n>>These are examples of nonlinear relationships. Also, the second row shows examples where the correlation coefficient is equal to 1 or \u20131\n\n>>**Notice that this has nothing to do with the slope. For example, your height in inches has a correlation coefficient of 1 with your height in feet or in nanometers.**\n","40f1ea75":"Each row represents one district. There are 10 attributes. longitude, latitude, housing_median_age, total_rooms, total_bed\nrooms, population, households, median_income, median_house_value, and\nocean_proximity.","e4ef04a8":"It works, although the predictions are not exactly accurate. Let\u2019s measure this regression model\u2019s RMSE on the whole training set using Scikit-Learn\u2019s mean_squared_error function:\n","8622dfc6":"One issue with this representation is that ML algorithms will assume that two nearby\nvalues are more similar than two distant values. This may be fine in some cases (e.g.,\nfor ordered categories such as \u201cbad\u201d, \u201caverage\u201d, \u201cgood\u201d, \u201cexcellent\u201d), but it is obviously\nnot the case for the ocean_proximity column (for example, categories 0 and 4 are\nclearly more similar than categories 0 and 1). To fix this issue, a common solution is\nto create one binary attribute per category: one attribute equal to 1 when the category\nis \u201c<1H OCEAN\u201d (and 0 otherwise), another attribute equal to 1 when the category is\n\u201cINLAND\u201d (and 0 otherwise), and so on. This is called one-hot encoding, because\nonly one attribute will be equal to 1 (hot), while the others will be 0 (cold)","ee1be85c":"The result is a plain NumPy array containing the transformed features. If you want to\nput it back into a Pandas DataFrame.","26d88a72":"Here , we specify that the numerical columns should be transformed using the num_pipeline that we defined earlier,\nand the categorical columns should be transformed using a OneHotEncoder. Finally,\nwe apply this ColumnTransformer to the housing data: it applies each transformer to\nthe appropriate columns and concatenates the outputs along the second axis (the\ntransformers must return the same number of rows).","c5b65756":"#### You can check the income category proportions in the train set ","e71ea714":"# Create A Test Set ","65ab8718":"# Experimenting with Attribute Combinations","94a6be91":"**Notice that the output is a SciPy sparse matrix, instead of a NumPy array. This is very\nuseful when you have categorical attributes with thousands of categories. After one\u0002hot encoding we get a matrix with thousands of columns, and the matrix is full of zeros except for a single 1 per row. Using up tons of memory mostly to store zeros\nwould be very wasteful, so instead a sparse matrix only stores the location of the non-zero elements. You can use it mostly like a normal 2D array,but if you really want to convert it to a (dense) NumPy array, just call the toarray() method**","fb86b317":"### Visualizing Geographical Data \n\nSince there is geographical information (latitude and longitude), it is a good idea to\ncreate a scatterplot of all districts to visualize the data ","069b67ab":"In this case Creating new features make soo much sense ! \ud83d\ude32\n\n>The new bedrooms_per_room attribute is much more correlated with the median house value than the total number of rooms or bedrooms. \n\n>Apparently houses with a lower bedroom\/room ratio tend to be more expensive. \n\n>The number of rooms per household is also more informative than the total number of rooms in a\ndistrict\u2014obviously the larger the houses, the more expensive they are.\n","6de0c627":"# Prepare Data for Machine Learning Algorithms","e1485e82":"## Feature Scaling \n\n* Machine Learning algorithms don\u2019t perform well when\n  the input numerical attributes have very different scales. This is the case for the hous\u2010\n  ing data: the total number of rooms ranges from about 6 to 39,320, while the median\n  incomes only range from 0 to 15.\n  \n* There are two common ways to get all attributes to have the same scale: min-max\n    scaling and standardization\n    \n     1. Min-max scaling (many people call this normalization) is quite simple: values are\n        shifted and rescaled so that they end up ranging from 0 to 1. We do this by subtract\u2010\n        ing the min value and dividing by the max minus the min   \n        \n     2. Standardization is quite different: first it subtracts the mean value (so standardized\n        values always have a zero mean), and then it divides by the standard deviation so that\n        the resulting distribution has unit variance. Unlike min-max scaling, standardization\n        does not bound values to a specific range, which may be a problem for some algo\u2010\n        rithms (e.g., neural networks often expect an input value ranging from 0 to 1). How\u2010\n        ever, standardization is much less affected by outliers\n        \n  \n  ![](https:\/\/miro.medium.com\/max\/1400\/1*YRWWEdutwJ8i-n7VmgxrKg.png)","6a771523":"# Selecting and Training a Model ","9c2ff56e":"## Random Forest \n\nA random forest is a machine learning technique that\u2019s used to solve regression and classification problems. It utilizes ensemble learning, which is a technique that combines many classifiers to provide solutions to complex problems.A random forest algorithm consists of many decision trees. The \u2018forest\u2019 generated by the random forest algorithm is trained through bagging or bootstrap aggregating. Bagging is an ensemble meta-algorithm that improves the accuracy of machine learning algorithms.The (random forest) algorithm establishes the outcome based on the predictions of the decision trees. It predicts by taking the average or mean of the output from various trees. Increasing the number of trees increases the precision of the outcome.\n\n![](https:\/\/ars.els-cdn.com\/content\/image\/3-s2.0-B9780128177365000090-f09-17-9780128177365.jpg)","1111f76a":"### df.info()\n\nThe info() method is useful to get a quick description of the data, in particular the\ntotal number of rows, and each attribute\u2019s type and number of non-null values"}}