{"cell_type":{"a16dc4c1":"code","12e43a1b":"code","6f75ecd9":"code","566e4c0a":"code","c282a9f3":"code","fcd4c998":"code","f07b92a1":"code","3b40a9b0":"code","0b976e38":"code","504f3ffe":"code","1ce735c1":"code","0fba02a2":"code","e12a571b":"code","b797fbea":"code","fcac0885":"code","43c96bdb":"code","f0d42f15":"code","9ba93066":"code","be03a67a":"code","c0d5077a":"code","d7f257ab":"code","d86a4889":"code","8cb902d8":"code","0c1f38c9":"code","9c190d33":"code","31922d53":"code","b2a2f2bf":"code","7ce89fb3":"code","515c559e":"code","95185203":"code","abd65beb":"code","339596cf":"code","1fc221a1":"code","dc8224b0":"code","3a10d307":"code","df9638d3":"code","7411857a":"code","5b139eab":"code","31276f0e":"code","8ee7732f":"code","3c29e22e":"code","b1317e72":"code","7de53339":"code","2f827045":"code","f08b7167":"code","7da54fa9":"code","fc133699":"code","3a6cc619":"code","9132d50b":"code","72341d99":"code","e8b9673c":"code","bfea558e":"code","43126835":"code","fe9b30fd":"code","47f07280":"code","193ec378":"code","2510237a":"code","d61a6b20":"code","ddda679e":"code","c5de5229":"code","be077d7e":"code","4619cb22":"code","be98e01e":"code","24698106":"code","4592cab0":"code","6b6f50ea":"code","ab1f26cf":"code","6a6c6b75":"code","de2f17fe":"code","3d064467":"code","1de2ce04":"code","76c4e630":"code","b7ea98d5":"code","33b547cc":"code","92fb0d03":"code","8673ed03":"code","dfb651c6":"code","1b49e158":"code","26734da1":"code","e239d829":"code","74dc1758":"code","b04d1394":"code","9edb82bb":"code","bf55e646":"code","d05be95c":"code","02a7fa41":"code","eed95783":"code","dba714f3":"code","484a198f":"code","cc6d1590":"code","f3addbdc":"code","91d6e2ef":"code","a1bdbc82":"code","2c3c1326":"code","f8957a28":"code","55459d6a":"code","fd0e8140":"code","42a70337":"code","a8dabb21":"code","6f634863":"code","853e45bf":"code","61a56bc2":"code","bd845e54":"code","a3f8830c":"code","1e879935":"code","3db294bf":"code","0cdaba73":"code","4bc652d7":"code","7a844d45":"code","793c55f8":"code","b3bb97ee":"code","1640bdaa":"code","db1ef333":"code","3c5a61bb":"code","5dfb90ef":"code","55b1ca45":"code","43735e7c":"code","dee5e61e":"code","b32b490e":"code","ab8d2d6c":"code","04a79223":"code","099e2151":"code","5d4f97de":"code","41853e36":"code","ed6e6323":"code","fd0de34b":"code","e4f7d2b0":"code","73d4fe72":"code","15d9e800":"code","31d89189":"code","ca0307b3":"code","3e6cc10e":"code","42d63c43":"code","9a033558":"code","b85914cf":"code","7b585d8c":"code","692439f4":"code","385515d3":"code","c7583f17":"markdown","216f99e6":"markdown","0d66b719":"markdown","dbe34ae2":"markdown","697a423c":"markdown","a7356717":"markdown","f1776c9c":"markdown","9eb64ab3":"markdown","b6ac77a6":"markdown","63bdbc20":"markdown","566d4240":"markdown","449dd4b4":"markdown","502a1e8f":"markdown","62b71f82":"markdown","fa1106b5":"markdown","3134309c":"markdown","8d79061a":"markdown","14daf061":"markdown","d07fdc6d":"markdown","85942610":"markdown","717fe525":"markdown","35dbc9aa":"markdown","30e937b5":"markdown","d1cf00f2":"markdown","b16575c6":"markdown","d4254100":"markdown","184b5484":"markdown","6493ac9d":"markdown","3a5ab8da":"markdown","284929e5":"markdown","1851d611":"markdown","4913473c":"markdown","c8568c15":"markdown","ac797637":"markdown","1f2e4156":"markdown","26db560c":"markdown","06b7ea93":"markdown","13ddacbe":"markdown","0c037665":"markdown"},"source":{"a16dc4c1":"# Suppressing Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","12e43a1b":"# Importing Pandas and NumPy\nimport pandas as pd, numpy as np","6f75ecd9":"# Importing all datasets\nheart_disease = pd.read_csv(\"\/kaggle\/input\/framingham-heart-study-dataset\/framingham.csv\")\nheart_disease.head()","566e4c0a":"heart_disease.shape","c282a9f3":"# let's look at the statistical aspects of the dataframe\nheart_disease.describe()","fcd4c998":"# Let's see the type of each column\nheart_disease.info()","f07b92a1":"heart_disease.head(3)","3b40a9b0":"heart_disease.columns","0b976e38":"heart_disease.dtypes","504f3ffe":"# import preprocessing from sklearn\nfrom sklearn import preprocessing\n\n# 1. INSTANTIATE\n# encode labels with value between 0 and n_classes-1.\nle = preprocessing.LabelEncoder()\n\n\n# 2\/3. FIT AND TRANSFORM\n# use df.apply() to apply le.fit_transform to all columns\nheart_disease_2 = heart_disease.apply(le.fit_transform)\nheart_disease_2.head(10)","1ce735c1":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","0fba02a2":"# Apply scaler() to all the columns except the 'yes-no' and 'dummy' variables\nnum_vars = [\"male\",\"age\",\"education\",\"currentSmoker\",\"cigsPerDay\",\"BPMeds\",\"prevalentStroke\",\"prevalentHyp\",\"diabetes\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\",\"TenYearCHD\"]\n\nheart_disease_2[num_vars] = scaler.fit_transform(heart_disease_2[num_vars])\n\nheart_disease_2.head()","e12a571b":"heart_disease_2.isnull().sum()","b797fbea":"# Checking for outliers in the continuous variables\nnum_heart_disease = heart_disease[[\"age\",\"education\",\"currentSmoker\",\"cigsPerDay\",\"BPMeds\",\"prevalentStroke\",\"prevalentHyp\",\"diabetes\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\",\"TenYearCHD\"]]","fcac0885":"# Checking outliers at 25%, 50%, 75%, 90%, 95% and 99%\nnum_heart_disease.describe(percentiles=[.25, .5, .75, .90, .95, .99])","43c96bdb":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","f0d42f15":"#Apply matplotlib functionalities\n\n#Change the colour of bins to green\n#Change the number of bins\n\n#Create a distribution plot for rating\n\n#import the necessary libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport seaborn as sns\n\nsns.distplot(num_heart_disease.BMI, bins = 40, color = \"yellow\")\nplt.title(\"Distribution of BMI overs the Graph\", fontsize = 20, fontweight = 10, verticalalignment = 'baseline')\n\nplt.show()","9ba93066":"sns.boxplot(num_heart_disease.age)","be03a67a":"plt.style.use(\"fivethirtyeight\")","c0d5077a":"#Change the number of bins\n\n#Create a distribution plot for rating\n\n#import the necessary libraries\n\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n\nimport seaborn as sns\n\nsns.distplot(num_heart_disease.cigsPerDay, bins = 40, color = \"orange\")\nplt.title(\"Distribution of CigsPerDay overs the Graph\", fontsize = 20, fontweight = 10, verticalalignment = 'baseline', color = \"black\")\nplt.show()","d7f257ab":"num_heart_disease.columns","d86a4889":"import scipy.stats as stats\n\nplt.figure(figsize= [10,5])\n#Change the code to the following\nsns.scatterplot(num_heart_disease.age, num_heart_disease.heartRate)\nsns.set_style(\"whitegrid\")\nsns.set_theme(context='notebook',style='darkgrid',palette='deep',font='sans-serif',font_scale=1,color_codes=True,rc=None)\nplt.title(\"Scatter Plot for Heart Rate And Age\", fontsize = 20, fontweight = 10, verticalalignment = 'baseline', color = \"black\")\nplt.show()","8cb902d8":"# Checking the percentage of missing values\nround(100*(heart_disease.isnull().sum()\/len(heart_disease.index)), 2)","0c1f38c9":"heart_disease['education'] = heart_disease['education'].fillna(heart_disease['education'].mode()[0])","9c190d33":"heart_disease['cigsPerDay'] = heart_disease['cigsPerDay'].fillna(heart_disease['cigsPerDay'].mode()[0])","31922d53":"heart_disease['BPMeds'] = heart_disease['BPMeds'].fillna(heart_disease['BPMeds'].mode()[0])","b2a2f2bf":"heart_disease['totChol'] = heart_disease['totChol'].fillna(heart_disease['totChol'].mode()[0])","7ce89fb3":"heart_disease['BMI'] = heart_disease['BMI'].fillna(heart_disease['BMI'].mode()[0])","515c559e":"heart_disease['glucose'] = heart_disease['glucose'].fillna(heart_disease['glucose'].mode()[0])","95185203":"heart_disease['heartRate'] = heart_disease['heartRate'].fillna(heart_disease['heartRate'].mode()[0])","abd65beb":"# Checking the percentage of missing values\nround(100*(heart_disease.isnull().sum()\/len(heart_disease.index)), 2)","339596cf":"from sklearn.model_selection import train_test_split","1fc221a1":"# Putting feature variable to X\nX = heart_disease.drop(['TenYearCHD'], axis=1)\n\nX.head()","dc8224b0":"# Putting response variable to y\ny = heart_disease['TenYearCHD']\n\ny.head()","3a10d307":"# Splitting the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, test_size=0.3, random_state=100)","df9638d3":"from sklearn.preprocessing import StandardScaler","7411857a":"scaler = StandardScaler()\n\nX_train[[\"age\",\"education\",\"cigsPerDay\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\"]] = scaler.fit_transform(X_train[[\"age\",\"education\",\"cigsPerDay\",\"totChol\",\"sysBP\",\"diaBP\",\"BMI\",\"heartRate\",\"glucose\"]])\n\nX_train.head()","5b139eab":"### Checking the TenYearCHD Rate\ntenYearCHD = (sum(heart_disease['TenYearCHD'])\/len(heart_disease['TenYearCHD'].index))*100\ntenYearCHD","31276f0e":"# Importing matplotlib and seaborn\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","8ee7732f":"# Let's see the correlation matrix \nplt.figure(figsize = (20,10))        # Size of the figure\nsns.heatmap(heart_disease.corr(),annot = True,cmap=\"Greens\")\nplt.show()","3c29e22e":"import statsmodels.api as sm","b1317e72":"# Logistic regression model\nlogm1 = sm.GLM(y_train,(sm.add_constant(X_train)), family = sm.families.Binomial())\nlogm1.fit().summary()","7de53339":"from sklearn.linear_model import LogisticRegression\nlogreg = LogisticRegression()","2f827045":"from sklearn.feature_selection import RFE\nrfe = RFE(logreg, 6)             # running RFE with 13 variables as output\nrfe = rfe.fit(X_train, y_train)","f08b7167":"rfe.support_","7da54fa9":"list(zip(X_train.columns, rfe.support_, rfe.ranking_))","fc133699":"col = X_train.columns[rfe.support_]","3a6cc619":"X_train.columns[~rfe.support_]","9132d50b":"X_train = X_train.drop(['education'], axis=1)","72341d99":"X_train = X_train.drop(['BMI'], axis=1)","e8b9673c":"X_train = X_train.drop(['heartRate'], axis=1)","bfea558e":"X_train = X_train.drop(['currentSmoker'], axis=1)","43126835":"X_train = X_train.drop(['diaBP'], axis=1)","fe9b30fd":"X_train = X_train.drop(['totChol'], axis=1)","47f07280":"X_train = X_train.drop(['BPMeds'], axis=1)","193ec378":"X_train = X_train.drop(['glucose'], axis=1)","2510237a":"X_train = X_train.drop(['cigsPerDay'], axis=1)","d61a6b20":"X_train_sm = sm.add_constant(X_train[col])\nlogm2 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm2.fit()\nres.summary()","ddda679e":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","c5de5229":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train.columns\nvif['VIF'] = [variance_inflation_factor(X_train.values, i) for i in range(X_train.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","be077d7e":"# Getting the predicted values on the train set\ny_train_pred = res.predict(X_train_sm)\ny_train_pred[:10]","4619cb22":"y_train_pred = y_train_pred.values.reshape(-1)\ny_train_pred[:10]","be98e01e":"y_train_pred_final = pd.DataFrame({'TenYearCHD':y_train.values, 'TenYearCHD_Prob':y_train_pred})\ny_train_pred_final.head()","24698106":"y_train_pred_final['predicted'] = y_train_pred_final.TenYearCHD_Prob.map(lambda x: 1 if x > 0.4 else 0)\n\n# Let's see the head\ny_train_pred_final.head()","4592cab0":"y_train_pred_final.predicted.value_counts()","6b6f50ea":"from sklearn import metrics","ab1f26cf":"# Confusion matrix \nconfusion = metrics.confusion_matrix(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted )\nprint(confusion)","6a6c6b75":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted))","de2f17fe":"# Check for the VIF values of the feature variables. \nfrom statsmodels.stats.outliers_influence import variance_inflation_factor","3d064467":"# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\nvif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1de2ce04":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","76c4e630":"col = col.drop('prevalentStroke', 1)\ncol","b7ea98d5":"# Let's re-run the model using the selected variables\nX_train_sm = sm.add_constant(X_train[col])\nlogm3 = sm.GLM(y_train,X_train_sm, family = sm.families.Binomial())\nres = logm3.fit()\nres.summary()","33b547cc":"y_train_pred = res.predict(X_train_sm).values.reshape(-1)","92fb0d03":"y_train_pred[:10]","8673ed03":"y_train_pred_final['TenYearCHD_Prob'] = y_train_pred","dfb651c6":"# Creating new column 'predicted' with 1 if TenYearCHD_Prob > 0.4 else 0\ny_train_pred_final['predicted'] = y_train_pred_final.TenYearCHD_Prob.map(lambda x: 1 if x > 0.4 else 0)\ny_train_pred_final.head()","1b49e158":"# Let's check the overall accuracy.\nprint(metrics.accuracy_score(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted))","26734da1":"vif = pd.DataFrame()\nvif['Features'] = X_train[col].columns\nvif['VIF'] = [variance_inflation_factor(X_train[col].values, i) for i in range(X_train[col].shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","e239d829":"# Let's take a look at the confusion matrix again \nconfusion = metrics.confusion_matrix(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted )\nconfusion","74dc1758":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted)","b04d1394":"TP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives","9edb82bb":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","bf55e646":"# Let us calculate specificity\nTN \/ float(TN+FP)","d05be95c":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","02a7fa41":"# positive predictive value \nprint (TP \/ float(TP+FP))","eed95783":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","dba714f3":"def draw_roc( actual, probs ):\n    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n                                              drop_intermediate = False )\n    auc_score = metrics.roc_auc_score( actual, probs )\n    plt.figure(figsize=(5, 5))\n    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n    return None","484a198f":"fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.TenYearCHD, y_train_pred_final.TenYearCHD_Prob, drop_intermediate = False )","cc6d1590":"draw_roc(y_train_pred_final.TenYearCHD, y_train_pred_final.TenYearCHD_Prob)","f3addbdc":"# Let's create columns with different probability cutoffs \nnumbers = [float(x)\/10 for x in range(10)]\nfor i in numbers:\n    y_train_pred_final[i]= y_train_pred_final.TenYearCHD_Prob.map(lambda x: 1 if x > i else 0)\ny_train_pred_final.head()","91d6e2ef":"# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\ncutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\nfrom sklearn.metrics import confusion_matrix\n\n# TP = confusion[1,1] # true positive \n# TN = confusion[0,0] # true negatives\n# FP = confusion[0,1] # false positives\n# FN = confusion[1,0] # false negatives\n\nnum = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\nfor i in num:\n    cm1 = metrics.confusion_matrix(y_train_pred_final.TenYearCHD, y_train_pred_final[i] )\n    total1=sum(sum(cm1))\n    accuracy = (cm1[0,0]+cm1[1,1])\/total1\n    \n    speci = cm1[0,0]\/(cm1[0,0]+cm1[0,1])\n    sensi = cm1[1,1]\/(cm1[1,0]+cm1[1,1])\n    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\nprint(cutoff_df)","a1bdbc82":"# Let's plot accuracy sensitivity and specificity for various probabilities.\ncutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\nplt.show()","2c3c1326":"y_train_pred_final['final_predicted'] = y_train_pred_final.TenYearCHD_Prob.map( lambda x: 1 if x > 0.1 else 0)\n\ny_train_pred_final.head()","f8957a28":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_train_pred_final.TenYearCHD, y_train_pred_final.final_predicted)","55459d6a":"confusion2 = metrics.confusion_matrix(y_train_pred_final.TenYearCHD, y_train_pred_final.final_predicted )\nconfusion2","fd0e8140":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","42a70337":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","a8dabb21":"# Let us calculate specificity\nTN \/ float(TN+FP)","6f634863":"# Calculate false postive rate - predicting churn when customer does not have churned\nprint(FP\/ float(TN+FP))","853e45bf":"# Positive predictive value \nprint (TP \/ float(TP+FP))","61a56bc2":"# Negative predictive value\nprint (TN \/ float(TN+ FN))","bd845e54":"confusion = metrics.confusion_matrix(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted )\nconfusion","a3f8830c":"confusion[1,1]\/(confusion[0,1]+confusion[1,1])","1e879935":"confusion[1,1]\/(confusion[1,0]+confusion[1,1])","3db294bf":"from sklearn.metrics import precision_score, recall_score","0cdaba73":"precision_score(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted)","4bc652d7":"recall_score(y_train_pred_final.TenYearCHD, y_train_pred_final.predicted)","7a844d45":"from sklearn.metrics import precision_recall_curve","793c55f8":"y_train_pred_final.TenYearCHD, y_train_pred_final.predicted","b3bb97ee":"p, r, thresholds = precision_recall_curve(y_train_pred_final.TenYearCHD, y_train_pred_final.TenYearCHD_Prob)","1640bdaa":"plt.plot(thresholds, p[:-1], \"g-\")\nplt.plot(thresholds, r[:-1], \"r-\")\nplt.show()","db1ef333":"X_test.columns","3c5a61bb":"X_test.head(3)","5dfb90ef":"X_test[['age','education','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']] = scaler.transform(X_test[['age','education','cigsPerDay','totChol','sysBP','diaBP','BMI','heartRate','glucose']])","55b1ca45":"X_test = X_test[col]\nX_test.head()","43735e7c":"X_test_sm = sm.add_constant(X_test)","dee5e61e":"y_test_pred = res.predict(X_test_sm)","b32b490e":"y_test_pred[:10]","ab8d2d6c":"# Converting y_pred to a dataframe which is an array\ny_pred_1 = pd.DataFrame(y_test_pred)","04a79223":"# Let's see the head\ny_pred_1.head()","099e2151":"# Converting y_test to dataframe\ny_test_df = pd.DataFrame(y_test)","5d4f97de":"# Putting CustID to index\ny_test_df['CustID'] = y_test_df.index","41853e36":"# Removing index for both dataframes to append them side by side \ny_pred_1.reset_index(drop=True, inplace=True)\ny_test_df.reset_index(drop=True, inplace=True)","ed6e6323":"# Appending y_test_df and y_pred_1\ny_pred_final = pd.concat([y_test_df, y_pred_1],axis=1)","fd0de34b":"y_pred_final.head()","e4f7d2b0":"# Renaming the column \ny_pred_final= y_pred_final.rename(columns={ 0 : 'TenYearCHD_Prob'})","73d4fe72":"# Rearranging the columns\ny_pred_final = y_pred_final.reindex(['CustID','TenYearCHD','TenYearCHD_Prob'], axis=1)","15d9e800":"# Let's see the head of y_pred_final\ny_pred_final.head()","31d89189":"y_pred_final['final_predicted'] = y_pred_final.TenYearCHD_Prob.map(lambda x: 1 if x > 0.40 else 0)","ca0307b3":"y_pred_final.head()","3e6cc10e":"# Let's check the overall accuracy.\nmetrics.accuracy_score(y_pred_final.TenYearCHD, y_pred_final.final_predicted)","42d63c43":"confusion2 = metrics.confusion_matrix(y_pred_final.TenYearCHD, y_pred_final.final_predicted )\nconfusion2","9a033558":"TP = confusion2[1,1] # true positive \nTN = confusion2[0,0] # true negatives\nFP = confusion2[0,1] # false positives\nFN = confusion2[1,0] # false negatives","b85914cf":"# Let's see the sensitivity of our logistic regression model\nTP \/ float(TP+FN)","7b585d8c":"# Let us calculate specificity\nTN \/ float(TN+FP)","692439f4":"y_pred_final.final_predicted.value_counts()","385515d3":"y_pred_final.head(6)","c7583f17":"#### Checking VIFs","216f99e6":"### Checking the percentage of missing values ","0d66b719":"### Step 8: Feature Selection Using RFE","dbe34ae2":"### Step 7: Model Building\nLet's start by splitting our data into a training set and a test set.","697a423c":"##### Creating a dataframe with the actual churn flag and the predicted probabilities","a7356717":"##### Let's now check the VIFs again","f1776c9c":"#### Running Your First Training Model","9eb64ab3":"### Step 6: Looking at Correlations","b6ac77a6":"## Precision and Recall","63bdbc20":"### Step 3: Data Preparation","566d4240":"#### From the curve above, 0.1 is the optimum point to take it as a cutoff probability.","449dd4b4":"### Final Round Of Check Of The Missing Values ","502a1e8f":"### <u> LOGISTIC REGRESSION - HEART DISEASE PREDICTION <\/u>\n\n### Introduction\n\n\nWorld Health Organization has estimated 12 million deaths occur worldwide, every year due to Heart diseases. \nHalf the deaths in the United States and other developed countries are due to cardio vascular diseases. \nThe early prognosis of cardiovascular diseases can aid in making decisions on lifestyle changes in high risk patients and in turn reduce the complications. \nhis research intends to pinpoint the most relevant\/risk factors of heart disease as well as predict the overall risk using logistic regression\n\n### Data Preparation\n\n### Source\n\nThe dataset is publically available on the Kaggle website, and it is from an ongoing cardiovascular study on residents of the town of \nFramingham, Massachusetts. The classification goal is to predict whether the patient has 10-year risk of future \ncoronary heart disease (CHD).The dataset provides the patients\u2019 information. It includes over 4,000 records and 15 attributes.\n\n### Variables\n\n\nEach attribute is a potential risk factor. There are both demographic, behavioral and medical risk factors.\n\n### Demographic:\n=\n\u2022 Sex: male or female(Nominal)\n\u2022 Age: Age of the patient;(Continuous - Although the recorded ages have been truncated to whole numbers, the concept of age is continuous)\nBehavioral\n\u2022 Current Smoker: whether or not the patient is a current smoker (Nominal)\n\u2022 Cigs Per Day: the number of cigarettes that the person smoked on average in one day.(can be considered continuous as one can have any number of cigarettes, \neven half a cigarette.)\n\n### Medical( history)\n\n\u2022 BP Meds: whether or not the patient was on blood pressure medication (Nominal)\n\u2022 Prevalent Stroke: whether or not the patient had previously had a stroke (Nominal)\n\u2022 Prevalent Hyp: whether or not the patient was hypertensive (Nominal)\n\u2022 Diabetes: whether or not the patient had diabetes (Nominal)\n\n### Medical(current)\n\n\u2022 Tot Chol: total cholesterol level (Continuous)\n\u2022 Sys BP: systolic blood pressure (Continuous)\n\u2022 Dia BP: diastolic blood pressure (Continuous)\n\u2022 BMI: Body Mass Index (Continuous)\n\u2022 Heart Rate: heart rate (Continuous - In medical research, variables such as heart rate though in fact discrete, yet are considered continuous \nbecause of large number of possible values.)\n\u2022 Glucose: glucose level (Continuous)\nPredict variable (desired target)\n\u2022 10 year risk of coronary heart disease CHD (binary: \u201c1\u201d, means \u201cYes\u201d, \u201c0\u201d means \u201cNo\u201d)\n\n### Logistic Regression\n\nLogistic regression is a type of regression analysis in statistics used for prediction of outcome of a categorical dependent variable from a set of predictor \nor independent variables. In logistic regression the dependent variable is always binary. Logistic regression is mainly used to for prediction and also \ncalculating the probability of success.\n\nThe results above show some of the attributes with P value higher than the preferred alpha(5%) and thereby showing low statistically \nsignificant relationship with the probability of heart disease. Backward elimination approach is used here to remove those attributes with highest P-value \none at a time followed by running the regression repeatedly until all attributes have P Values less than 0.05.","62b71f82":" ##### Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0","fa1106b5":"### <b> Distribution of BMI with Heart Disease <\/b>","3134309c":"### Step 4: Test-Train Split","8d79061a":"<b> Heart Disease <\/b>","14daf061":"From , the `Above Dataset`, the max-min scaler is used to put all the values between 0 and 1","d07fdc6d":"### Checking for Outliers","85942610":"Optimal cutoff probability is that prob where we get balanced sensitivity and specificity","717fe525":"##### Precision\nTP \/ TP + FP","35dbc9aa":"### TenYearCHD Final Test Probability :","30e937b5":"## Metrics beyond simply accuracy","d1cf00f2":"Using sklearn utilities for the same","b16575c6":"### Distribution of Age with Heart Disease","d4254100":"### Step 9: Plotting the ROC Curve","184b5484":"### Precision and recall tradeoff","6493ac9d":"##### Recall\nTP \/ TP + FN","3a5ab8da":"##### Assessing the model with StatsModels","284929e5":"Making predictions on the test set","1851d611":"### Step 10: Finding Optimal Cutoff Point","4913473c":"### Checking VIF\n\nVariance Inflation Factor or VIF, gives a basic quantitative idea about how much the feature variables are correlated with each other. It is an extremely important parameter to test our linear model. The formula for calculating `VIF` is:\n\n### $ VIF_i = \\frac{1}{1 - {R_i}^2} $","c8568c15":"##### Looking at the confusion matrix again","ac797637":"### Imputing the missing value with the most common value ","1f2e4156":"### Step 5: Feature Scaling","26db560c":"### Step 11: Making predictions on the test set","06b7ea93":"An ROC curve demonstrates several things:\n\n- It shows the tradeoff between sensitivity and specificity (any increase in sensitivity will be accompanied by a decrease in specificity).\n- The closer the curve follows the left-hand border and then the top border of the ROC space, the more accurate the test.\n- The closer the curve comes to the 45-degree diagonal of the ROC space, the less accurate the test.","13ddacbe":"### Dropping the Variable as Identified by the RFE to reduce complexity","0c037665":"#### Converting some binary variables (Yes\/No) to 0\/1"}}