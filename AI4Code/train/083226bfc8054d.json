{"cell_type":{"0fb91ef5":"code","0b3f1ea3":"code","959bac56":"code","20152efa":"code","8852bebf":"code","1e8531e2":"code","26d7d261":"code","7b46dfa8":"code","2832fa85":"code","502934f2":"code","82a3b41f":"code","0c107bf6":"code","8de2dc81":"code","a154a198":"code","bf2976f5":"code","959da0bc":"code","f6ce58c5":"code","140b8ea2":"code","206fa29d":"code","1b749d25":"code","61eef6b2":"code","272711bb":"code","2890776a":"code","2924f679":"code","8ee9a966":"code","8c4621cd":"code","f6c15f03":"markdown","ac3ce4ce":"markdown","c3188dd1":"markdown","4334b2a3":"markdown","c8497b36":"markdown"},"source":{"0fb91ef5":"!pip install strsimpy","0b3f1ea3":"!cp \/kaggle\/input\/payment-detection\/utils.py \/utils.py","959bac56":"import torch\nimport pandas as pd\n\n\nfrom PIL import Image\nimport cv2\nimport gc\nimport time\n\nfrom torch.autograd import Variable\nfrom torch.optim import Adam, Adagrad, SGD\nimport torch.nn.functional as F\n\nimport random\n\nfrom torchvision import transforms\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.models as models\nimport tqdm\nfrom torch.nn import functional as fnn\nfrom torch.utils import data\nfrom torchvision import transforms\nfrom torch.utils.data import Dataset, DataLoader, Subset\nfrom torch.nn import Module, Sequential, Conv2d, AvgPool2d, GRU, Linear\nfrom torch.nn.functional import ctc_loss, log_softmax\nfrom torchvision import models\n\n\nimport torchvision\nimport pickle\nimport json\n\nfrom torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\nfrom torchvision.transforms import *\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n\n\nfrom itertools import chain\n\nimport torch.distributed as dist\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nimport matplotlib.patches as patches\n\n\nimport os\nimport tqdm\nimport json\nimport numpy as np\n\nfrom string import digits, ascii_uppercase\n\nimport math \nimport utils\n\nfrom strsimpy.levenshtein import Levenshtein","20152efa":"torch.cuda.is_available()","8852bebf":"SEED = 1489\n\n\nrandom.seed(SEED)\nos.environ[\"PYTHONHASHSEED\"] = str(SEED)\nnp.random.seed(SEED)","1e8531e2":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"","26d7d261":"#\u043e\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043e\u043d\u0441\u0442\u0430\u043d\u0442\n\n#\u043f\u0443\u0442\u0438 \u043a \u0444\u0430\u0439\u043b\u0430\u043c\nTEST_PATH = \"\/kaggle\/input\/payment-detection\/test\/test\/\" \nTRAIN_PATH = \"\/kaggle\/input\/payment-detection\/train\/train\/\"\nSUBMISSION_PATH = \"\/kaggle\/input\/payment-detection\/submission.csv\"\nTRAIN_INFO = \"\/kaggle\/input\/payment-detection\/train.csv\"\n\n# \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u0434\u0435\u0442\u0435\u043a\u0442\u0438\u0440\u0443\u0435\u043c\u043e\u043c \u043e\u0431\u044a\u0435\u043a\u0442\u0435\nTHRESHOLD = 0.5\n\n# \u0440\u0430\u0437\u043c\u0435\u0440 \u0434\u043b\u044f \u0440\u0435\u0441\u0430\u0439\u0437\u0438\u043d\u0433\u0430 \u0438\u0437\u043e\u0431\u0440\u0430\u0436\u0435\u043d\u0438\u0439\nIMAGE_WIDTH = 412\nIMAGE_HEIGHT = 412\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u043e\u043d\u043d\u043e\u0439 \u0432\u044b\u0431\u043e\u0440\u043a\u0438\nVAL_SIZE = 0.3\n\n# \u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0438\u0442\u0435\u0440\u0430\u0446\u0438\u0439\nN_ITER = 2\n\n# \u0420\u0430\u0437\u043c\u0435\u0440 \u0431\u0430\u0442\u0447\u0430\nBATCH_SIZE = 3\nBATCH_SIZE_VAL = 1\n\n\nLR = 3e-5\n","7b46dfa8":"train  = pd.read_csv(TRAIN_INFO)","2832fa85":"test = pd.read_csv(SUBMISSION_PATH)","502934f2":"valid_images = np.random.choice(train.image.unique(), size=int(VAL_SIZE * train.image.nunique()), replace=False)\n\nvalid_set = train[train.image.isin(valid_images)]\n\ntrain_set = train[~train.image.isin(valid_images)]","82a3b41f":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndef get_device():\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    else:\n        return torch.device('cpu')","0c107bf6":"class ShapeDataset(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(), \n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               )])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n              \n        def get_boxes(obj):\n            boxes = [[obj[f] for f in ['xmin', 'ymin', 'xmax', 'ymax'] ]]\n            return torch.as_tensor(boxes, dtype=torch.float)\n\n\n        def get_areas(obj):\n            areas = [(obj['xmax'] - obj['xmin']) * (obj['ymax'] - obj['ymin']) ]\n            return torch.as_tensor(areas, dtype=torch.int64)\n        \n        def get_class(obj):\n            \"\"\"\n            \u041a\u043e\u0434\u0438\u0440\u0443\u0435\u043c \u043a\u043b\u0430\u0441\u0441 \u0447\u0438\u0441\u043b\u043e\u043c\n            \"\"\"\n            \n            cards = {\n                'VI':1,\n                'MA':2,\n                'EX':3, \n                'PC':4, \n                'ST':5,\n                'UY':6\n                    }\n            return torch.as_tensor([cards[obj['label']]], dtype=torch.int64)\n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n\n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n        \n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        row = self.data.iloc[idx]\n        obj = {}        \n        \n        obj['xmin'] = int(row['x1'] * (IMAGE_WIDTH\/ row['width']))\n        obj['xmax'] = int((row['x1'] + row['w'])*(IMAGE_WIDTH\/ row['width']))\n        obj['ymin'] = int(row['y1'] * (IMAGE_HEIGHT\/ row['height']))\n        obj['ymax'] = int((row['y1'] + row['h'])*(IMAGE_HEIGHT\/ row['height']))\n\n        \n        obj['label'] = row['label']\n\n        \n        if self.transform:\n            image = self.transform(img)\n            \n\n        target = {}\n        target['boxes'] = get_boxes(obj)\n\n        target['labels'] = get_class(obj)\n        target['image_id'] = torch.as_tensor([idx], dtype=torch.int64)\n        target['area'] = get_areas(obj)\n        target['iscrowd'] = torch.ones((1,), dtype=torch.int64)\n\n        return image, target\n\n    \nclass ShapeDatasetTest(Dataset):\n\n    def __init__(self, IMAGE_DIR, IMAGE_WIDTH, IMAGE_HEIGHT, data,\n                 transform=transforms.Compose([ToTensor(),\n                                               Normalize(\n                                                   mean=[0.485, 0.456, 0.406],\n                                                   std=[0.229, 0.224, 0.225]\n                                               ) ])):\n        self.IMAGE_DIR = IMAGE_DIR\n        self.IMAGE_WIDTH = IMAGE_WIDTH\n        self.IMAGE_HEIGHT = IMAGE_HEIGHT\n        \n        \n        self.data = data\n        self.transform = transform\n\n\n    def num_classes(self):\n        return len(self.class2index)\n\n    \n    def __len__(self, ):\n        return len(self.data)\n    \n    def __getitem__(self, idx):\n        \n        \n        img_name = self.data.iloc[idx]['image']\n        \n        path = os.path.join(self.IMAGE_DIR, img_name)\n        \n        \n        img = cv2.imread(path)\n        \n        shapes  = img.shape\n\n        img = cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT)) \n        \n        \n        if self.transform:\n            image = self.transform(img)\n            \n        return image \n\n","8de2dc81":"train_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, train_set)\nvalid_data = ShapeDataset(TRAIN_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, valid_set)#valid_set)","a154a198":"test_data  = ShapeDatasetTest(TEST_PATH, IMAGE_WIDTH, IMAGE_HEIGHT, test)","bf2976f5":"dataloader_train = DataLoader(\n    train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0, collate_fn=collate_fn)\ndataloader_valid = DataLoader(\n    valid_data, batch_size=BATCH_SIZE_VAL, shuffle=False, num_workers=0, collate_fn=collate_fn)","959da0bc":"device = get_device()\n\nmodel = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\nnum_classes = 7\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n\nmodel = model.to(device)","f6ce58c5":"params = [p for p in model.parameters() if p.requires_grad]","140b8ea2":"optimizer = optim.Adam(model.parameters(), lr=LR, amsgrad=True)\nloss_fn = fnn.mse_loss\n\n\nlr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n                                               step_size=5,\n                                               gamma=0.1)\n","206fa29d":"def lev_dist(preds, df):\n    \n    \n    def c_sort(sub_li):\n        \"\"\"\n        \u0421\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0431\u043e\u043a\u0441\u043e\u0432 \u043f\u043e \u043a\u043e\u043e\u0440\u0434\u0438\u043d\u0430\u0442\u0430\u043c. \u0421\u043b\u0435\u0432\u0430 \u043d\u0430\u043f\u0440\u0430\u0432\u043e \u0441\u0432\u0435\u0440\u0445\u0443 \u0432\u043d\u0438\u0437.\n        \"\"\"\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n        \n        \"\"\"\n        \u0414\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430.\n        \"\"\"\n        cards = {\n                1:'VI',\n                2:'MA',\n                3:'EX', \n                4:'PC', \n                5:'ST',\n                6:'UY'\n                    }\n        return cards[number]\n    \n    levenshtein = Levenshtein()     \n            \n    imgs = {}\n    \n    \n    \n    for index, row in df.iterrows():\n        \n        #\u0438\u0434\u0451\u043c \u043f\u043e \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0443 \u0441\u0447\u0438\u0442\u044b\u0432\u0430\u0435\u043c \u0440\u0430\u0437\u043c\u0435\u0442\u043a\u0443 \u0438 \u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u0432 \u0441\u043b\u043e\u0432\u0430\u0440\u044c\n        \n        if row['image'] in imgs.keys():    \n            \n            \n            \n            imgs[row['image']].append([row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label'] ])\n            imgs[row['image']] = c_sort(imgs[row['image']])\n\n        else:\n            imgs[row['image']] =  [[row['x1'], row['y1'], row['x1'] + row['w'], row['y1'] + row['h'], row['label']]]\n            \n            \n    labels = {}\n    \n    for i in imgs.keys():\n        \n        #\u0437\u0430\u043f\u0438\u0441\u044b\u0432\u0430\u0435\u043c \u043e\u0440\u0438\u0433\u0438\u043d\u0430\u043b\u044c\u043d\u0443\u044e \u0441\u0442\u0440\u043e\u043a\u0443\n        \n        labels[i] = \" \".join([j[4] for j in imgs[i]])\n        \n    preds_list = []\n    \n    cnt = 0\n    \n    \n    labels_pred = {}\n    imgs_name = df['image'].tolist()\n\n    for i in preds:\n        for j in i:\n            #\u0438\u0437\u0432\u043b\u0435\u043a\u0430\u0435\u043c \u0432\u0441\u0435 \u043f\u0440\u0435\u0434\u0438\u043a\u0442\u044b\n            _temp_boxes = i[j]['boxes'].cpu().detach().numpy().tolist()\n            _temp_label = i[j]['labels'].cpu().detach().numpy().tolist()\n            _temp_confidence = i[j]['scores'].cpu().detach().numpy().tolist()\n               \n            for index, _ in enumerate(_temp_boxes):\n                # \u0434\u043b\u044f \u043a\u0430\u0436\u0434\u043e\u0433\u043e \u043d\u0430\u0431\u043b\u044e\u0434\u0435\u043d\u0438\u044f \u0434\u0435\u043a\u043e\u0434\u0438\u043d\u0433 \u043a\u043b\u0430\u0441\u0441\u0430\n                _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n                _temp_boxes[index].append(_temp_confidence[index])\n                \n                \n            _temp_boxes = c_sort(_temp_boxes)\n\n            # \u043e\u0441\u0442\u0430\u0432\u043b\u044f\u0435\u043c \u0442\u043e\u043b\u044c\u043a\u043e \u0442\u0435 \u0441\u0438\u0441\u0442\u0435\u043c\u044b \u0443\u0432\u0435\u0440\u0435\u043d\u043d\u043e\u0441\u0442\u044c \u0432 \u043a\u043e\u0442\u043e\u0440\u044b\u0445 \u0432\u044b\u0448\u0435 THRESHOLD\n            \n            labels_pred[imgs_name[cnt]] = \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD])\n            \n            cnt+=1\n        \n    assert len(imgs) == len(labels_pred)\n    \n    lev_dist = 0\n    \n    for i in imgs.keys():\n        # \u0432\u0430\u043b\u0438\u0434\u0430\u0446\u0438\u044f\n        \n        lev_dist += levenshtein.distance(imgs[i], labels_pred[i])\n    \n    print(\"Levenstein distance {0}\".format(lev_dist\/len(imgs)))\n    \n    return lev_dist\/len(imgs)","1b749d25":"train_losses = []\ntest_losses = []","61eef6b2":"def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    model.train()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    metric_logger.add_meter('lr', utils.SmoothedValue(window_size=1, fmt='{value:.6f}'))\n    header = 'Epoch: [{}]'.format(epoch)\n\n    lr_scheduler = None\n    if epoch == 0:\n        warmup_factor = 1. \/ 1000\n        warmup_iters = min(1000, len(data_loader) - 1)\n\n        lr_scheduler = utils.warmup_lr_scheduler(optimizer, warmup_iters, warmup_factor)\n\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n\n        # reduce losses over all GPUs for logging purposes\n        loss_dict_reduced = utils.reduce_dict(loss_dict)\n        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n\n        loss_value = losses_reduced.item()\n\n        if not math.isfinite(loss_value):\n            print(\"Loss is {}, stopping training\".format(loss_value))\n            print(loss_dict_reduced)\n            #sys.exit(1)\n\n        optimizer.zero_grad()\n        losses.backward()\n        optimizer.step()\n\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n        \n    \n\n    return metric_logger\n\n\ndef _get_iou_types(model):\n    model_without_ddp = model\n    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n        model_without_ddp = model.module\n    iou_types = [\"bbox\"]\n    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n        iou_types.append(\"segm\")\n    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n        iou_types.append(\"keypoints\")\n    return iou_types\n\n\n@torch.no_grad()\ndef evaluate(model, data_loader, device):\n    n_threads = torch.get_num_threads()\n    # FIXME remove this and make paste_masks_in_image run on the GPU\n    torch.set_num_threads(1)\n    cpu_device = torch.device(\"cpu\")\n    model.eval()\n    metric_logger = utils.MetricLogger(delimiter=\"  \")\n    header = 'Val:'\n\n    preds = []\n    for images, targets in metric_logger.log_every(data_loader, 100, header):\n        images = list(img.to(device) for img in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\n\n        res = {target[\"image_id\"].item(): output for target, output in zip(targets, outputs)}\n        \n        preds.append(res)\n        \n\n    # accumulate predictions from all images\n    torch.set_num_threads(n_threads)\n    return preds","272711bb":"\nfor epoch in range(N_ITER):\n    # train for one epoch, printing every 10 iterations\n    train_one_epoch(model, optimizer, dataloader_train, device, epoch, print_freq=1)\n    # update the learning rate\n    lr_scheduler.step()\n\nprint(\"Train error\")\npreds_train = evaluate(model, dataloader_train, device=device)\nlev_dist(preds_train, train_set)\n\nprint(\"Validate error\")\npreds_val = evaluate(model, dataloader_valid, device=device)\nlev_dist(preds_val, valid_set)\n\n     \n# \u0441\u043e\u0445\u0440\u0430\u043d\u044f\u0435\u043c \u043b\u0443\u0447\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c\nwith open(f\"model.pth\", \"wb\") as fp:\n    torch.save(model.state_dict(), fp)\n","2890776a":"def get_prediction(dataset,  model):\n\n    \"\"\"\n    \u0424\u0443\u043d\u043a\u0446\u0438\u044f \u0434\u043b\u044f \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u044f \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0430. \n    \"\"\"\n    \n    model.eval()\n    cpu_device = torch.device(\"cpu\")\n\n    def c_sort(sub_li):\n\n        sub_li.sort(key = lambda x: (x[0], x[1]))\n        return sub_li\n    \n    def get_reversed_class(number):\n\n        cards = {\n            1:'VI',\n            2:'MA',\n            3:'EX', \n            4:'PC', \n            5:'ST',\n            6:'UY'\n                }\n        return cards[number]\n\n\n    preds = []\n    \n     #\u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0435\n    for images in metric_logger.log_every(dataset, 100, header):\n\n        images = torch.stack([images[0][0].to(device), images[1][0].to(device), images[2][0].to(device) ], dim=0).unsqueeze(0)\n\n        torch.cuda.synchronize()\n        model_time = time.time()\n        with torch.no_grad():\n            outputs = model(images)\n\n            outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n        model_time = time.time() - model_time\n\n        preds.append(outputs)\n        \n        labels_pred = []\n        names_pred = []\n        \n    imgs_name = test.image.unique().tolist()\n\n    # \u0438\u0437\u0432\u043b\u0435\u0447\u0435\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u0439, \u0441\u043e\u0440\u0442\u0438\u0440\u043e\u0432\u043a\u0430 \u0438 \u043e\u0437\u0434\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0444\u0440\u0435\u0439\u043c\u0430\n    for ind, i in enumerate(preds):\n\n        _temp_boxes = i[0]['boxes'].cpu().detach().numpy().tolist()\n        _temp_label = i[0]['labels'].cpu().detach().numpy().tolist()\n        _temp_confidence = i[0]['scores'].cpu().detach().numpy().tolist()\n\n        for index, _ in enumerate(_temp_boxes):\n            _temp_boxes[index].append(get_reversed_class(_temp_label[index]))\n            _temp_boxes[index].append(_temp_confidence[index])\n            \n\n        _temp_boxes = c_sort(_temp_boxes)\n        names_pred.append(imgs_name[ind])\n        labels_pred.append( \" \".join([j[4] for j in _temp_boxes if j[5] > THRESHOLD]))\n             \n    d = {'image': names_pred, 'payment': labels_pred}\n\n    df = pd.DataFrame(data=d)\n        \n        \n    return df ","2924f679":"metric_logger = utils.MetricLogger(delimiter=\"  \")\n\nheader = \"Test:\"\n\ndataloader_test = DataLoader(\n    test_data, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)\n\n\npredictions = get_prediction(dataloader_test, model) ","8ee9a966":"predictions.head()","8c4621cd":"predictions.to_csv(\"submission.csv\", index=None)","f6c15f03":"\u0427\u0430\u0441\u0442\u0438\u0447\u043d\u043e \u043e\u0441\u043d\u043e\u0432\u0430\u043d\u043e \u043d\u0430\nhttps:\/\/pytorch.org\/tutorials\/index.html","ac3ce4ce":"## \u0422\u0440\u0435\u043d\u0438\u0440\u043e\u0432\u043a\u0430","c3188dd1":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0438 \u0438 \u0443\u0441\u0442\u0430\u043d\u043e\u0432\u043a\u0430 \u043f\u0430\u0440\u0430\u043c\u0435\u0442\u0440\u043e\u0432","4334b2a3":"## \u0417\u0430\u0433\u0440\u0443\u0437\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445 \u0438 \u0444\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u0434\u0430\u0442\u0430\u0441\u0435\u0442\u0430","c8497b36":"## \u0424\u043e\u0440\u043c\u0438\u0440\u043e\u0432\u0430\u043d\u0438\u0435 \u043f\u0440\u0435\u0434\u0441\u043a\u0430\u0437\u0430\u043d\u0438\u044f"}}