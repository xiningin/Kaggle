{"cell_type":{"fa006133":"code","542d8c83":"code","5c0f45df":"code","43d9eba1":"code","3bcebf3a":"code","8b21e466":"code","b99ad841":"code","cefac0c4":"code","9f16dd27":"code","e7da3371":"code","a4ee7b2c":"code","77eca50b":"code","0d0fdd35":"code","2858988b":"code","87026e71":"code","4b0c2c3a":"code","1f49dbb2":"code","ea2440dc":"code","5058f78e":"code","6af96a52":"code","f0434abb":"code","2f110bde":"code","520f10a3":"code","cf46ec21":"code","3236d5f5":"code","d2b372fa":"code","54d9b813":"code","dc3ab11d":"code","5996744a":"code","33e0223a":"code","fc7142b7":"code","adeed0d6":"code","44f9fa89":"code","9f84eed0":"code","646f49d8":"code","4ef7c008":"code","c5c6708d":"code","326742fc":"markdown","be409601":"markdown","7f011b70":"markdown","ae136289":"markdown","af6d6004":"markdown","146abed8":"markdown","6faad67c":"markdown","83bbd6b6":"markdown","40fac7ec":"markdown","24a8134c":"markdown","5511c1e5":"markdown"},"source":{"fa006133":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","542d8c83":"import os\nimport PIL\nimport cv2\nimport glob\nimport pathlib\nimport zipfile\nimport numpy as np\nfrom fastai import *\nfrom PIL import Image\nimport tensorflow as tf\nfrom fastai.vision import *\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.layers import *\nfrom matplotlib import pyplot as plt\nfrom keras.preprocessing import image\nfrom keras.preprocessing.image import *\nfrom tensorflow.keras.models import Sequential\nfrom fastai.metrics import error_rate, accuracy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom torch.utils.data import Dataset, DataLoader","5c0f45df":"data_dir = '\/kaggle\/input\/emotion-detection-fer\/'\ntrain_path = data_dir + 'train\/'\ntest_path = data_dir + 'test\/'","43d9eba1":"def display_images(emotion):\n    plt.figure(figsize=(10,10))\n    for i in range(1, 10, 1):\n        plt.subplot(3,3,i) #Form a 3x3 grid for each label\n        img = image.load_img(train_path + '\/' + emotion + \"\/\" + os.listdir(train_path + '\/' + emotion)[i], target_size=(48, 48)) #load images\n        plt.title(emotion) #display image label\n        plt.imshow(img) #display image\n        plt.tight_layout() #format images\n    plt.show()\n\nfor i in os.listdir(train_path):\n    display_images(i)","3bcebf3a":"#use if running locally inside git repo\ndef mylistdir(directory):\n    \"\"\"A specialized version of os.listdir() that ignores files that\n    start with a leading period.\"\"\"\n    #Used to remove .DS_Store from our labels\n    filelist = os.listdir(directory)\n    return [x for x in filelist\n            if not (x.startswith('.'))]\n\n#Display class labels\n#labels= mylistdir(train_path)\nlabels = os.listdir(train_path)\nprint(\"Our 7 Emotion Labels:\")\nlabels","8b21e466":"file_count = len(list(pathlib.Path(data_dir).glob('*\/*\/*.png')))\nprint('Total image count:', file_count)\n\nfile_count = len(list(pathlib.Path(train_path).glob('*\/*.png')))\nprint('Total training images:', file_count)\n\nfile_count = len(list(pathlib.Path(test_path).glob('*\/*.png')))\nprint('Total testing images:', file_count)","b99ad841":"#Display our 7 emotion labels and example image from each class\nfig, axes = plt.subplots(1, 7, figsize=(20,20))\n\nfor i in range(7):\n\n    ex_image = train_path + labels[i]+ '\/' + mylistdir(train_path + labels[i]+'\/')[0]\n    axes[i].imshow(plt.imread(ex_image))\n    axes[i].set_title(labels[i])\n\nplt.show()","cefac0c4":"#Print Image Label Distributions\nfor i in labels:\n    print(i, '\\nTrain: ' + str(len(os.listdir(train_path + i +'\/')))+ ' images' +'\\nTest: ' + str(len(os.listdir(test_path+i+'\/')))+' images\\n')","9f16dd27":"train_dist = np.array([len(os.listdir(train_path+i+'\/')) for i in labels])\ntest_dist = np.array([len(os.listdir(test_path+i+'\/')) for i in labels])\nx = labels\n\nplt.figure(figsize=(20,10))\nplt.suptitle('Emotion Distribution', fontsize=25)\n\nax1 = plt.subplot(1,2,1)\nax1.set_title('Training Set')\nplt.xlabel('Emotion Class', fontsize=14)\nplt.ylabel('Number of Images', fontsize=14)\nplt.bar(x, train_dist, color='blue')\n\nax2 = plt.subplot(1, 2, 2)\nax2.set_title('Test Set')\nplt.xlabel('Emotion Class', fontsize=14)\nplt.ylabel('Number of Images', fontsize=14)\nplt.bar(x, test_dist, color='green')\nplt.show()","e7da3371":"#Define our data augmentations\ntrain_datagen = ImageDataGenerator(rescale=1.0\/255,\n                                   zoom_range= 0.2,\n                                   horizontal_flip=True,\n                                   shear_range=0.2,\n                                   validation_split=0.2)\n\ntest_datagen = ImageDataGenerator(rescale=1.0\/255)\n\n#Read our images to the data augmentations\n#Generates batches of augmented data\ntrain = train_datagen.flow_from_directory(train_path,\n                                          subset='training',\n                                          color_mode = 'grayscale',\n                                          target_size = (48, 48),\n                                          batch_size = 64,\n                                          shuffle=True,\n                                          class_mode = 'categorical')\n\nvalid = train_datagen.flow_from_directory(train_path,\n                                          subset='validation',\n                                          color_mode = 'grayscale',\n                                          target_size = (48, 48),\n                                          batch_size = 64,\n                                          shuffle=True,\n                                          class_mode = 'categorical')\n\ntest = test_datagen.flow_from_directory(test_path,\n                                        color_mode = 'grayscale',\n                                        target_size = (48, 48),\n                                        batch_size = 64,\n                                        shuffle=False,\n                                        class_mode = 'categorical')","a4ee7b2c":"train.class_indices","77eca50b":"'''\nDirect Reference: https:\/\/www.tensorflow.org\/tutorials\/images\/cnn\n\nUsing a common pattern: Stacking Conv2D and MaxPooling2D Layers\n'''\n#Initialize our model\nmodel = tf.keras.Sequential()\n\n#Input Layers\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1), padding='same'))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension\nmodel.add(Dropout(0.25)) #test\n\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(Dropout(0.25))\n\n'''model.add(Conv2D(512, (3, 3), activation='relu', padding='same'))\nmodel.add(BatchNormalization())\nmodel.add(MaxPooling2D(pool_size=(2, 2))) #max pooling to decrease dimension\nmodel.add(Dropout(0.2))'''\n\nmodel.add(Flatten()) #Flattens our data into a vector\n\nmodel.add(Dense(1024, activation = 'relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(7, activation = 'softmax')) #Final dense layer always equal to the number of classes (7)","0d0fdd35":"'''from tensorflow.keras.applications.vgg19 import VGG19\nfrom tensorflow.keras.models import Model\n\nvgg = VGG19(weights='imagenet', include_top=False, classes=7)\n\n#freeze\nfor layer in vgg.layers:\n    layer.trainable = False\n    \ninput = Input(shape=(48,48,1)) #Our input image size\n\nconv = Conv2D(3, kernel_size=(3,3), padding='same')(input)\nvgg = vgg(conv)\nx = Flatten()(vgg)\npred = Dense(7, activation='softmax')(x)\n\nmodel = Model(inputs=input, outputs=pred)'''","2858988b":"model.summary()","87026e71":"tf.keras.utils.plot_model(model, to_file='emotion-model.png', show_shapes=True, show_layer_names=True)","4b0c2c3a":"'''\nDirect Reference: https:\/\/keras.io\/api\/callbacks\/\n'''\nfrom keras.callbacks import ModelCheckpoint\nfrom sklearn.utils import class_weight\n\nclass_weights = class_weight.compute_class_weight(class_weight='balanced', \n                                                  classes=np.unique(train.classes), \n                                                  y=train.classes)\nclass_weights = dict(zip(np.unique(train.classes),class_weights))\n\nfilepath = '\/my_best_model.epoch{epoch:02d}-loss{val_loss:.2f}.hdf5' #saves our best model to this location with custom name\ncallbacks = [tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=5, min_delta=0.0001, verbose=1),\n             tf.keras.callbacks.ModelCheckpoint(filepath=filepath, monitor='val_loss',verbose=1, save_best_only=True, mode='min'),\n             tf.keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=3, verbose=1,restore_best_weights=True)]\n\n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])","1f49dbb2":"emotion = model.fit(train, validation_data=valid, epochs=50, callbacks=callbacks, class_weight=class_weights)","ea2440dc":"loss,acc = model.evaluate(test,verbose=2)","5058f78e":"#Saving our model\/weights\nmodel.save('emotion-detection.h5')\nmodel.save_weights('emotion_weight.h5')\nnp.save(\"history\", emotion.history)\nprint('saved model to disk.')","6af96a52":"plt.figure(figsize=(20,10))\nplt.suptitle('Loss & Accuracy Over Time', fontsize=25)\n\nplt.subplot(1, 2, 1)\nplt.xlabel('Epoch', fontsize=18)\nplt.ylabel('Loss', fontsize=18)\nplt.plot(emotion.history['loss'], label='Training Loss')\nplt.plot(emotion.history['val_loss'], label='Validation Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.xlabel('Epoch', fontsize=18)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(emotion.history['accuracy'], label='Training Accuracy')\nplt.plot(emotion.history['val_accuracy'], label='Validation Accuracy')\nplt.legend(loc='upper left')\nplt.show()","f0434abb":"train_loss, train_acc = model.evaluate(train)\ntest_loss, test_acc = model.evaluate(test)","2f110bde":"from keras.models import load_model\nmod = load_model('my_best_model.epoch30-loss1.19.hdf5')","520f10a3":"plt.figure(figsize=(20,10))\nplt.suptitle('Loss & Accuracy Over Time', fontsize=25)\n\nplt.subplot(1, 2, 1)\nplt.xlabel('Epoch', fontsize=18)\nplt.ylabel('Loss', fontsize=18)\nplt.plot(emotion.history['loss'], label='Training Loss')\nplt.plot(test_loss, label='Test Loss')\nplt.legend(loc='upper right')\n\nplt.subplot(1, 2, 2)\nplt.xlabel('Epoch', fontsize=18)\nplt.ylabel('Accuracy', fontsize=16)\nplt.plot(emotion.history['accuracy'], label='Training Accuracy')\nplt.plot(test_acc, label='Test Accuracy')\nplt.legend(loc='upper left')\nplt.show()","cf46ec21":"from sklearn.metrics import classification_report, confusion_matrix\n\ny_pred = model.predict(train)\ny_pred = np.argmax(y_pred, axis=1)\nclass_labels = test.class_indices\nclass_labels = {v:k for k,v in class_labels.items()}\n\n\nprint('Confusion Matrix')\nprint(confusion_matrix)\nprint('Classification Report')\ntarget_names = list(class_labels.values())\nprint(classification_report(train.classes, y_pred, target_names=target_names))\n\nplt.figure(figsize=(8,8))\nplt.imshow(confusion_matrix, interpolation='nearest')\nplt.colorbar()\ntick_mark = np.arange(len(target_names))\n_ = plt.xticks(tick_mark, target_names, rotation=90)\n_ = plt.yticks(tick_mark, target_names)","3236d5f5":"img = image.load_img(test_path+\"\/surprised\/im30.png\",target_size = (48,48),color_mode = \"grayscale\")\nimg = np.array(img)\nplt.imshow(img)","d2b372fa":"labels = sorted(labels)\nimg = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\n#print result - we see that the image slightly detected the fearful emotion as well as surprised\nprint(result)\nprint(train.class_indices)","54d9b813":"img_index = result.index(max(result))\nprint('Prediction:',labels[img_index])","dc3ab11d":"#Everett - He made a surprised face\nimg = image.load_img('..\/input\/emotion2\/everett.jpg', target_size=(48,48),color_mode='grayscale')\nimg = np.array(img)\nplt.imshow(img)","5996744a":"img = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\nimg_index = result.index(max(result))\nprint('Prediction:',labels[img_index])\n\nplt.figure(figsize=(10,5))\nplt.suptitle('Emotion Prediction')\n\nax1 = plt.subplot()\nplt.xlabel('Emotion', fontsize=14)\nplt.ylabel('Certainty', fontsize=14)\n\n\nplt.bar(labels, np.array(result), color='blue')","33e0223a":"#Kameron - He made an angry face\nimg = image.load_img('..\/input\/emotion2\/kameron2.jpg', target_size=(48,48),color_mode='grayscale')\nimg = np.array(img)\nplt.imshow(img)","fc7142b7":"img = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\nimg_index = result.index(max(result))\nprint('Prediction:',labels[img_index])\n\nplt.figure(figsize=(10,5))\nplt.suptitle('Emotion Prediction')\n\nax1 = plt.subplot()\nplt.xlabel('Emotion', fontsize=14)\nplt.ylabel('Certainty', fontsize=14)\n\n\nplt.bar(labels, np.array(result), color='blue')","adeed0d6":"#Andy!\nimg = image.load_img('..\/input\/emotion3\/anderson2.jpg', target_size=(48,48),color_mode='grayscale')\nimg = np.array(img)\nplt.imshow(img)","44f9fa89":"img = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\nimg_index = result.index(max(result))\nprint('Prediction:',labels[img_index])\n\nplt.figure(figsize=(10,5))\nplt.suptitle('Emotion Prediction')\n\nax1 = plt.subplot()\nplt.xlabel('Emotion', fontsize=14)\nplt.ylabel('Certainty', fontsize=14)\n\n\nplt.bar(labels, np.array(result), color='blue')","9f84eed0":"#Jennie!\nimg = image.load_img('..\/input\/emotion4\/jennie.jpg', target_size=(48,48),color_mode='grayscale')\nimg = np.array(img)\nplt.imshow(img)","646f49d8":"img = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\nimg_index = result.index(max(result))\nprint('Prediction:',labels[img_index])\n\nplt.figure(figsize=(10,5))\nplt.suptitle('Emotion Prediction')\n\nax1 = plt.subplot()\nplt.xlabel('Emotion', fontsize=14)\nplt.ylabel('Certainty', fontsize=14)\n\n\nplt.bar(labels, np.array(result), color='pink')","4ef7c008":"#Justin!\nimg = image.load_img('..\/input\/emotion5\/justin.jpg', target_size=(48,48),color_mode='grayscale')\nimg = np.array(img)\nplt.imshow(img)","c5c6708d":"img = np.expand_dims(img,axis = 0) #reshapes to 1,48,48\nimg = img.reshape(1,48,48,1)\nresult = model.predict(img) #using model to predict emotion of image\nresult = list(result[0])\n\nimg_index = result.index(max(result))\nprint('Prediction:',labels[img_index])\n\nplt.figure(figsize=(10,5))\nplt.suptitle('Emotion Prediction')\n\nax1 = plt.subplot()\nplt.xlabel('Emotion', fontsize=14)\nplt.ylabel('Certainty', fontsize=14)\n\n\nplt.bar(labels, np.array(result), color='orange')","326742fc":"Let's also take a look at how our images are distributed accross our 7 emotion labels.","be409601":"Let's take a look at a few samples from each label to see the types of images we are working with.","7f011b70":"## Data Preprocessing\nNow that we have a better understanding of our image data, we can perform data preprocessing on our images to help us expand our dataset and make it more balanced.\n\nKeras ImageDataGenerator allow us to perform image augmentation very easily.","ae136289":"Lets print out the prediction from the model as well as our class labels. The softmax function outputs a list with varying numbers, the index of the highest number is what the model predicts to be the emotion of the given image.","af6d6004":"Now that our data is augmented, we can work on building the model.\n\n## Create CNN Model\nMotivation for using CNN: \n- We used CNN (Convolutional Neural Network) for our image classification because of its high accuracy, hierarchical model, and ability to automatically detect important features. It is also computationally effective because all of the layers are fully connected.\n\n- Since we wanted to play with using many layers, we can use Keras Sequential model to stack many linear layers together.\n- Will be using ReLu as our activation function, and softmax for our final fully connected layer. I decided to use softmax since this is a multi-class classification problem.","146abed8":"## Visualizing our data\nLet's take a deeper look into our data so we can better understand what we are working with.","6faad67c":"## Compiling and Fitting the Model\nWe will implement a callback function that when called during training, will allow the model to stop once the best loss is found, or reduce the learning rate for better performance","83bbd6b6":"## Some Real World Examples\nI asked a few of my friends to send me a selfie of them recreating one of the 7 emotions. Here are the results:","40fac7ec":"## Visualizing the Results (The Fun Part!)\nLet's see our model predict some images!","24a8134c":"## Setting Up Our Data","5511c1e5":"There appears to be an unbalance with a very low amount of images in our Disgusted label. If we do not address these class imbalances, our model may become innacurate and prone to overfitting. Our approach to counteract this is to perform data augmentation & apply weights to our minority and majority classes."}}