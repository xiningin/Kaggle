{"cell_type":{"0609e72c":"code","f5765dea":"code","675b1e02":"code","9c17937a":"code","f99fc040":"code","b8b00a06":"code","55a5eda0":"code","717be762":"code","d356c833":"code","4625230a":"code","92b00d49":"code","598ac673":"code","6946daf6":"code","4ce0fe3d":"code","bb88d013":"code","01d1507a":"code","2bf84495":"code","aad854c3":"code","ac5051be":"code","f0e396f5":"markdown","80a9d15c":"markdown","cec34d05":"markdown","96a5805f":"markdown","b10434ad":"markdown","44aa5138":"markdown","0d46b3db":"markdown","64850373":"markdown","3eb29391":"markdown","de3d3787":"markdown"},"source":{"0609e72c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pylab as plt\nimport json\nimport seaborn as sns\nimport os\nimport random\nfrom tqdm.notebook import tqdm\nimport plotly.express as px\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, Dataset, DataLoader, random_split\nfrom torch.nn import functional as F\nfrom torch.optim import lr_scheduler","f5765dea":"class config:\n    train_file = '..\/input\/stanford-covid-vaccine\/train.json'\n    test_file = '..\/input\/stanford-covid-vaccine\/test.json'\n    pretrain_dir = '.\/'\n    sample_submission = '..\/input\/stanford-covid-vaccine\/sample_submission.csv'\n    learning_rate = 0.001\n    batch_size = 64\n    n_epoch = 100\n    n_split = 5\n    K = 1 # number of aggregation loop (also means number of GCN layers)\n    gcn_agg = 'mean' # aggregator function: mean, conv, lstm, pooling\n    filter_noise = True\n    seed = 1234","675b1e02":"class AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count","9c17937a":"def seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n    \nseed_everything(config.seed)","f99fc040":"class GCN(nn.Module):\n    '''\n    Implementation of one layer of GraphSAGE\n    '''\n    def __init__(self, input_dim, output_dim, aggregator='mean'):\n        super(GCN, self).__init__()\n        self.aggregator = aggregator\n        \n        if aggregator == 'mean':\n            linear_input_dim = input_dim * 2\n        elif aggregator == 'conv':\n            linear_input_dim = input_dim\n        elif aggregator == 'pooling':\n            linear_input_dim = input_dim * 2\n            self.linear_pooling = nn.Linear(input_dim, input_dim)\n        elif aggregator == 'lstm':\n            self.lstm_hidden = 128\n            linear_input_dim = input_dim + self.lstm_hidden\n            self.lstm_agg = nn.LSTM(input_dim, self.lstm_hidden, num_layers=1, batch_first=True)\n        \n        self.linear_gcn = nn.Linear(in_features=linear_input_dim, out_features=output_dim)\n        \n    def forward(self, input_, adj_matrix):\n        if self.aggregator == 'conv':\n            # set elements in diagonal of adj matrix to 1 with conv aggregator\n            idx = torch.arange(0, adj_matrix.shape[-1], out=torch.LongTensor())\n            adj_matrix[:, idx, idx] = 1\n            \n        adj_matrix = adj_matrix.type(torch.float32)\n        sum_adj = torch.sum(adj_matrix, axis=2)\n        sum_adj[sum_adj==0] = 1\n        \n        if self.aggregator == 'mean' or self.aggregator == 'conv':\n            feature_agg = torch.bmm(adj_matrix, input_)\n            feature_agg = feature_agg \/ sum_adj.unsqueeze(dim=2)\n            \n        elif self.aggregator == 'pooling':\n            feature_pooling = self.linear_pooling(input_)\n            feature_agg = torch.sigmoid(feature_pooling)\n            feature_agg = torch.bmm(adj_matrix, feature_agg)\n            feature_agg = feature_agg \/ sum_adj.unsqueeze(dim=2)\n\n        elif self.aggregator == 'lstm':\n            feature_agg = torch.zeros(input_.shape[0], input_.shape[1], self.lstm_hidden).cuda()\n            for i in range(adj_matrix.shape[1]):\n                neighbors = adj_matrix[:, i, :].unsqueeze(2) * input_\n                _, hn = self.lstm_agg(neighbors)\n                feature_agg[:, i, :] = torch.squeeze(hn[0], 0)\n                \n        if self.aggregator != 'conv':\n            feature_cat = torch.cat((input_, feature_agg), axis=2)\n        else:\n            feature_cat = feature_agg\n                \n        feature = torch.sigmoid(self.linear_gcn(feature_cat))\n        feature = feature \/ torch.norm(feature, p=2, dim=2).unsqueeze(dim=2)\n        \n        return feature\n        \n    \nclass Net(nn.Module):\n    def __init__(self, num_embedding=14, seq_len=107, pred_len=68, dropout=0.5, \n                 embed_dim=100, hidden_dim=128, K=1, aggregator='mean'):\n        '''\n        K: number of GCN layers\n        aggregator: type of aggregator function\n        '''\n        super(Net, self).__init__()\n        \n        self.pred_len = pred_len\n        self.embedding_layer = nn.Embedding(num_embeddings=num_embedding, \n                                      embedding_dim=embed_dim)\n        \n        self.gcn = nn.ModuleList([GCN(3 * embed_dim, 3 * embed_dim, aggregator=aggregator) for i in range(K)])\n        \n        self.gru_layer = nn.GRU(input_size=3 * embed_dim, \n                          hidden_size=hidden_dim, \n                          num_layers=3, \n                          batch_first=True, \n                          dropout=dropout, \n                          bidirectional=True)\n        \n        self.linear_layer = nn.Linear(in_features=2 * hidden_dim, \n                                out_features=5)\n        \n    def forward(self, input_, adj_matrix):\n        #embedding\n        embedding = self.embedding_layer(input_)\n        embedding = torch.reshape(embedding, (-1, embedding.shape[1], embedding.shape[2] * embedding.shape[3]))\n        \n        #gcn\n        gcn_feature = embedding\n        for gcn_layer in self.gcn:\n            gcn_feature = gcn_layer(gcn_feature, adj_matrix)\n        \n        #gru\n        gru_output, gru_hidden = self.gru_layer(gcn_feature)\n        truncated = gru_output[:, :self.pred_len]\n        \n        output = self.linear_layer(truncated)\n        \n        return output","b8b00a06":"pred_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']","55a5eda0":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef get_couples(structure):\n    \"\"\"\n    For each closing parenthesis, I find the matching opening one and store their index in the couples list.\n    The assigned list is used to keep track of the assigned opening parenthesis\n    \"\"\"\n    opened = [idx for idx, i in enumerate(structure) if i == '(']\n    closed = [idx for idx, i in enumerate(structure) if i == ')']\n\n    assert len(opened) == len(closed)\n    assigned = []\n    couples = []\n\n    for close_idx in closed:\n        for open_idx in opened:\n            if open_idx < close_idx:\n                if open_idx not in assigned:\n                    candidate = open_idx\n            else:\n                break\n        assigned.append(candidate)\n        couples.append([candidate, close_idx])\n        \n    assert len(couples) == len(opened)\n    \n    return couples\n\ndef build_matrix(couples, size):\n    mat = np.zeros((size, size))\n    \n    for i in range(size):  # neigbouring bases are linked as well\n        if i < size - 1:\n            mat[i, i + 1] = 1\n        if i > 0:\n            mat[i, i - 1] = 1\n    \n    for i, j in couples:\n        mat[i, j] = 1\n        mat[j, i] = 1\n        \n    return mat\n\ndef convert_to_adj(structure):\n    couples = get_couples(structure)\n    mat = build_matrix(couples, len(structure))\n    return mat\n\ndef preprocess_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    inputs = np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )\n    \n    adj_matrix = np.array(df['structure'].apply(convert_to_adj).values.tolist())\n    \n    return inputs, adj_matrix","717be762":"train = pd.read_json(config.train_file, lines=True)\n\nif config.filter_noise:\n    train = train[train.signal_to_noise > 1]\n    \ntest = pd.read_json(config.test_file, lines=True)\nsample_df = pd.read_csv(config.sample_submission)","d356c833":"train_inputs, train_adj = preprocess_inputs(train)\ntrain_labels = np.array(train[pred_cols].values.tolist()).transpose((0, 2, 1))\n\ntrain_inputs = torch.tensor(train_inputs, dtype=torch.long)\ntrain_adj = torch.tensor(train_adj, dtype=torch.long)\ntrain_labels = torch.tensor(train_labels, dtype=torch.float32)","4625230a":"def train_fn(epoch, model, train_loader, criterion, optimizer):\n    model.train()\n    model.zero_grad()\n    train_loss = AverageMeter()\n    \n    for index, (input_, adj, label) in enumerate(train_loader):\n        input_ = input_.cuda()\n        adj = adj.cuda()\n        label = label.cuda()\n        preds = model(input_, adj)\n        \n        loss = criterion(preds, label)\n        \n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        train_loss.update(loss.item())\n    \n    print(f\"Train loss {train_loss.avg}\")\n    return train_loss.avg\n    \ndef eval_fn(epoch, model, valid_loader, criterion):\n    model.eval()\n    eval_loss = AverageMeter()\n    \n    for index, (input_, adj, label) in enumerate(valid_loader):\n        input_ = input_.cuda()\n        adj = adj.cuda()\n        label = label.cuda()\n        preds = model(input_, adj)\n        \n        loss = criterion(preds, label)\n        eval_loss.update(loss.item())\n    \n    print(f\"Valid loss {eval_loss.avg}\")\n    return eval_loss.avg","92b00d49":"def run(fold, train_loader, valid_loader):\n    model = Net(K=config.K, aggregator=config.gcn_agg)\n    model.cuda()\n    criterion = torch.nn.MSELoss()\n    optimizer = torch.optim.Adam(params=model.parameters(), lr=config.learning_rate, weight_decay=0.0)\n    \n    train_losses = []\n    eval_losses = []\n    for epoch in range(config.n_epoch):\n        print('#################')\n        print('###Epoch:', epoch)\n        \n        train_loss = train_fn(epoch, model, train_loader, criterion, optimizer)\n        eval_loss = eval_fn(epoch, model, valid_loader, criterion)\n        train_losses.append(train_loss)\n        eval_losses.append(eval_loss)\n        \n    torch.save(model.state_dict(), f'{config.pretrain_dir}\/gcn_gru_{fold}.pt')\n    return train_losses, eval_losses","598ac673":"splits = KFold(n_splits=config.n_split, shuffle=True, random_state=config.seed).split(train_inputs)\n\nfor fold, (train_idx, val_idx) in enumerate(splits):\n    train_dataset = TensorDataset(train_inputs[train_idx], train_adj[train_idx], train_labels[train_idx])\n    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=8)\n    \n    valid_dataset = TensorDataset(train_inputs[val_idx], train_adj[val_idx], train_labels[val_idx])\n    valid_loader = DataLoader(valid_dataset, batch_size=config.batch_size, shuffle=False, num_workers=8)\n    \n    train_losses, eval_losses = run(fold, train_loader, valid_loader)\n","6946daf6":"fig = px.line(\n    pd.DataFrame([train_losses, eval_losses], index=['loss', 'val_loss']).T, \n    y=['loss', 'val_loss'], \n    labels={'index': 'epoch', 'value': 'Mean Squared Error'}, \n    title='Training History')\nfig.show()","4ce0fe3d":"public_df = test.query(\"seq_length == 107\").copy()\nprivate_df = test.query(\"seq_length == 130\").copy()\n\npublic_inputs, public_adj = preprocess_inputs(public_df)\nprivate_inputs, private_adj = preprocess_inputs(private_df)\n\npublic_inputs = torch.tensor(public_inputs, dtype=torch.long)\nprivate_inputs = torch.tensor(private_inputs, dtype=torch.long)\npublic_adj = torch.tensor(public_adj, dtype=torch.long)\nprivate_adj = torch.tensor(private_adj, dtype=torch.long)","bb88d013":"model_short = Net(seq_len=107, pred_len=107, K=config.K, aggregator=config.gcn_agg)\nmodel_long = Net(seq_len=130, pred_len=130, K=config.K, aggregator=config.gcn_agg)\n\nlist_public_preds = []\nlist_private_preds = []\nfor fold in range(config.n_split):\n    model_short.load_state_dict(torch.load(f'{config.pretrain_dir}\/gcn_gru_{fold}.pt'))\n    model_long.load_state_dict(torch.load(f'{config.pretrain_dir}\/gcn_gru_{fold}.pt'))\n    model_short.cuda()\n    model_long.cuda()\n    model_short.eval()\n    model_long.eval()\n\n    public_preds = model_short(public_inputs.cuda(), public_adj.cuda())\n    private_preds = model_long(private_inputs.cuda(), private_adj.cuda())\n    public_preds = public_preds.cpu().detach().numpy()\n    private_preds = private_preds.cpu().detach().numpy()\n    \n    list_public_preds.append(public_preds)\n    list_private_preds.append(private_preds)","01d1507a":"public_preds = np.mean(list_public_preds, axis=0)\nprivate_preds = np.mean(list_private_preds, axis=0)","2bf84495":"preds_ls = []\n\nfor df, preds in [(public_df, public_preds), (private_df, private_preds)]:\n    for i, uid in enumerate(df.id):\n        single_pred = preds[i]\n\n        single_df = pd.DataFrame(single_pred, columns=pred_cols)\n        single_df['id_seqpos'] = [f'{uid}_{x}' for x in range(single_df.shape[0])]\n\n        preds_ls.append(single_df)\n\npreds_df = pd.concat(preds_ls)","aad854c3":"submission = sample_df[['id_seqpos']].merge(preds_df, on=['id_seqpos'])\nsubmission.to_csv('submission.csv', index=False)","ac5051be":"submission.head()","f0e396f5":"Get predict results by averaging results in 5-folds","80a9d15c":"# Define model","cec34d05":"# Visualize losses","96a5805f":"# Import","b10434ad":"# Train KFold","44aa5138":"# Pytorch model based on GCN (GraphSAGE) and GRU\n\nThanks to:  \n* https:\/\/www.kaggle.com\/xhlulu\/openvaccine-simple-gru-model\/  \n* https:\/\/www.kaggle.com\/theoviel\/generating-graph-matrices-from-the-structures\n\nIn this notebook, we use an inductive version of GCN called [GraphSAGE](https:\/\/arxiv.org\/abs\/1706.02216). GraphSAGE improves traditional GCN by using different aggregator functions instead of only convolutional function.  \nWe implement GraphSAGE with following aggregator function: mean function, pooling function, convolutional function and LSTM. This model can be improved by implementing more efficient aggregator functions.\n\n**Structure of GraphSAGE:**\n![GraphSAGE](http:\/\/snap.stanford.edu\/graphsage\/sample_and_agg.png \"GraphSAGE\")\n  \n  \n  \n  \n**Overall structure of our model is**: Embedding -> GCN -> GRU -> Linear\n","0d46b3db":"# Settings","64850373":"# Utils","3eb29391":"# Predict on test set","de3d3787":"# Load Data"}}