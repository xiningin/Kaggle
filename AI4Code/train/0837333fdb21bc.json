{"cell_type":{"f23a4309":"code","0368c4a7":"code","91e13ed4":"code","2b471fe6":"code","a68190c3":"code","2bfb81eb":"code","37b2d17b":"code","5cf01026":"code","06c7a07d":"code","c3ee6ac9":"code","76101849":"code","f3c5c78e":"code","d9353486":"code","c2ef8dad":"code","0029343d":"code","e63b8042":"code","ddeebcac":"code","66d361cf":"code","64cfa439":"code","e285b5a0":"code","f7f6316d":"code","38187d29":"code","b3bb3795":"code","28707f2b":"code","85485a14":"code","e5cc3057":"code","e2d79422":"code","5215dc41":"code","60d8c67b":"code","d358b6fc":"code","1caa287c":"code","74ee0cf4":"code","1bdd2434":"code","b29ccd68":"code","59d5a376":"code","6de9b5a3":"code","bff02ac9":"code","f319dde2":"code","73f89e82":"code","ac31ab3e":"markdown","ad06daef":"markdown"},"source":{"f23a4309":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0368c4a7":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', 500)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('whitegrid')\nfrom warnings import simplefilter\nsimplefilter(action='ignore', category=FutureWarning)\nsimplefilter(action='ignore', category=DeprecationWarning)","91e13ed4":"data = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')\ndata.head()","2b471fe6":"data.describe()","a68190c3":"def df_stats(df):\n    lines = df.shape[0]\n    d_types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    unique = df.apply(lambda x: x.unique().shape[0])\n    nulls = df.isnull().sum()\n    missing_ratio = (df.isnull().sum()\/lines)*100\n    skew = df.skew()\n    kurt = df.kurt()\n    col_names = ['dtypes', 'counts', 'unique', 'nulls', 'missing_ratio', 'skewness', 'kurtosis']\n    temp = pd.concat([d_types, counts, unique, nulls, missing_ratio, skew, kurt], axis=1)\n    temp.columns = col_names\n    return temp\n    \nst = df_stats(data)\nst","2bfb81eb":"sns.pairplot(data, vars=['age', 'trestbps', 'chol', 'thalach', 'oldpeak'], hue='target')","37b2d17b":"fig = plt.figure(figsize=(18, 10))\nsns.heatmap(data.corr(), annot=True)","5cf01026":"cat_cols = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\nnum_cols = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']\n\nfig, ax = plt.subplots(4, 2, figsize=(15, 25))\nfig.suptitle('Bar Plots of Categorical Data', fontsize=20)\n\ndef plot_cats(df, cols, target, ax):\n    pos = 1\n    sns.countplot(x=target, data=df, ax=ax[0, 0])\n    sns.countplot(x=cols[0], hue=target, data=df, ax=ax[0, 1])\n    for i in range(1, ax.shape[0]):\n        for j in range(ax.shape[1]):\n            sns.countplot(x=cols[pos], hue=target, data=df, ax=ax[i, j])\n            pos += 1\n            \n    plt.show()      \n        \nplot_cats(data, cat_cols, 'target', ax)","06c7a07d":"def plot_numeric(df, cols, target):\n    for col in cols:\n        fig, ax = plt.subplots(1, 2, figsize=(15, 6))\n        sns.distplot(a=df[col], ax=ax[0])\n        ax[0].set_title('distribution of {}, skew={:.4f}'.format(col, df[col].skew()))\n        sns.boxenplot(data=df, x=target, y=col, ax=ax[1])\n        ax[1].set_title('Boxen Plot Split by Target')\n    plt.show()\n        \nplot_numeric(data, num_cols, 'target')","c3ee6ac9":"from sklearn.preprocessing import PowerTransformer, StandardScaler\ndata[['oldpeak']] = PowerTransformer(method='yeo-johnson').fit_transform(data[['oldpeak']])\ndata[['age', 'trestbps', 'chol', 'thalach']] = StandardScaler().fit_transform(data[['age', 'trestbps', 'chol', 'thalach']])","76101849":"data.head()","f3c5c78e":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(data[data.columns[:-1]].values, data['target'].values)\n\nfig = plt.figure(figsize=(18, 8))\nimportance = pd.Series(rf.feature_importances_, index=data.columns[:-1]).sort_values(ascending=False)\nsns.barplot(x=importance, y=importance.index)\nplt.title('Feature Importance')\nplt.xlabel('Score')\nplt.show()","d9353486":"data[['cp','restecg', 'slope', 'ca', 'thal']] = data[['cp','restecg', 'slope', 'ca', 'thal']].astype(str)\ndata = pd.get_dummies(data)\ndata.head()","c2ef8dad":"y = data['target'].values\nX = data.drop(['target'], axis=1).values\nprint('Label shape:    {}'.format(y.shape))\nprint('Features shape: {}'.format(X.shape))","0029343d":"from sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\n\nada_base = AdaBoostClassifier()\nada_deci = AdaBoostClassifier(DecisionTreeClassifier())\nada_extr = AdaBoostClassifier(ExtraTreeClassifier())\nada_logr = AdaBoostClassifier(LogisticRegression())\nada_svml = AdaBoostClassifier(SVC(probability=True , kernel='linear'))","e63b8042":"from sklearn.model_selection import StratifiedKFold, cross_val_score\ndef cv_score_model(mod, X, y, folds, scoring):\n    cv = StratifiedKFold(n_splits=folds, shuffle=True)\n    cv_estimate = cross_val_score(mod, X, y, cv=cv, scoring=scoring, n_jobs=4)\n    return np.mean(cv_estimate), np.std(cv_estimate)","ddeebcac":"models = [ada_base, ada_deci, ada_extr, ada_logr, ada_svml]\nmodel_names = ['Base', 'DecisonTree', 'ExtraTree', 'LogisticRegression', 'SVC']\n\ndef fill_results_df(mod_list, name_list, scoring_list, X, y, folds):\n    \n    results = pd.DataFrame(index=name_list)\n    for score in scoring_list:\n        sc_mean = '{}_mean'.format(score)\n        sc_std = '{}_std'.format(score)\n        for name, model in zip(name_list, mod_list):\n            mean, std = cv_score_model(model, X, y, folds, score)\n            results.loc[name, sc_mean] = mean\n            results.loc[name, sc_std] = std\n    \n    return results\ns = ['roc_auc', 'accuracy', 'precision', 'recall']\nr = fill_results_df(models, model_names, s, X, y, 10)\nprint('Results from untuned classifiers')\nr","66d361cf":"from sklearn.base import clone\nfrom sklearn.model_selection import GridSearchCV","64cfa439":"base = clone(ada_base)\nbase.get_params()","e285b5a0":"cv=StratifiedKFold(n_splits=10, shuffle=True)\n\nparam_grid={'n_estimators' :[50, 100, 250, 500, 750, 1000],\n            'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n\n\nbase_grid = GridSearchCV(estimator=base,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nbase_grid.fit(X, y)\nbase_best_mod = base_grid.best_estimator_","f7f6316d":"print('For Adaboost with default decision stump as base estimator\\n')\nprint('Best GridSearchCV Score roc_auc {}'.format(base_grid.best_score_))\nprint('Hyperparameters                 Values')\nprint('n_estimators:                    {}'.format(base_grid.best_estimator_.n_estimators))\nprint('learning_rate:                   {}'.format(base_grid.best_estimator_.learning_rate))","38187d29":"deci = clone(ada_deci)\ndeci.get_params()","b3bb3795":"cv=StratifiedKFold(n_splits=10, shuffle=True)\n\nparam_grid = {'base_estimator__max_depth' :[1, 2, 5],\n              'base_estimator__min_samples_split' :[2, 3 ,5],\n              'base_estimator__min_samples_leaf' :[2, 3, 5 ,10],\n              'n_estimators' :[10, 50, 100, 250, 500, 750, 1000],\n              'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n\n\ndeci_grid = GridSearchCV(estimator=deci,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\ndeci_grid.fit(X, y)\ndeci_best_mod = deci_grid.best_estimator_","28707f2b":"print('For Adaboost with decision tree as base estimator\\n')\nprint('Best GridSearchCV roc_auc Score {}'.format(deci_grid.best_score_))\nprint('Hyperparameters                   Values')\nprint('base_estimator__max_depth:          {}'.format(deci_grid.best_estimator_.base_estimator.max_depth))\nprint('base_estimator__min_samples_split:  {}'.format(deci_grid.best_estimator_.base_estimator.min_samples_split))\nprint('base_estimator__min_samples_leaf:   {}'.format(deci_grid.best_estimator_.base_estimator.min_samples_leaf))\nprint('n_estimators:                       {}'.format(deci_grid.best_estimator_.n_estimators))\nprint('learning_rate:                      {}'.format(deci_grid.best_estimator_.learning_rate))","85485a14":"extr = clone(ada_extr)\nextr.get_params()","e5cc3057":"cv=StratifiedKFold(n_splits=10, shuffle=True)\n\nparam_grid = {'base_estimator__criterion' :['gini', 'entropy'],\n              'base_estimator__max_depth' :[1, 2, 5],\n              'base_estimator__min_samples_split' :[2, 3 ,5],\n              'base_estimator__min_samples_leaf' :[2, 3, 5 ,10],\n              'n_estimators' :[10, 50, 100, 250, 500, 750, 1000],\n              'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n\n\n\nextr_grid = GridSearchCV(estimator=extr,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nextr_grid.fit(X, y)\nextr_best_mod = extr_grid.best_estimator_","e2d79422":"print('For Adaboost with extra tree as base estimator\\n')\nprint('Best GridSearchCV roc_auc Score {}'.format(extr_grid.best_score_))\nprint('Hyperparameters                   Values')\nprint('base_estimator__criterion:          {}'.format(extr_grid.best_estimator_.base_estimator.criterion))\nprint('base_estimator__max_depth:          {}'.format(extr_grid.best_estimator_.base_estimator.max_depth))\nprint('base_estimator__min_samples_split:  {}'.format(extr_grid.best_estimator_.base_estimator.min_samples_split))\nprint('base_estimator__min_samples_leaf:   {}'.format(extr_grid.best_estimator_.base_estimator.min_samples_leaf))\nprint('n_estimators:                       {}'.format(extr_grid.best_estimator_.n_estimators))\nprint('learning_rate:                      {}'.format(extr_grid.best_estimator_.learning_rate))","5215dc41":"logr = clone(ada_logr)\nlogr.get_params()","60d8c67b":"cv=StratifiedKFold(n_splits=10, shuffle=True)\n\nparam_grid = {'base_estimator__C' :[0.01, 0.1, 1, 10, 50, 100, 500, 1000],\n              'n_estimators' :[10, 50, 100, 250, 500, 750, 1000],\n              'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n\n\n\nlogr_grid = GridSearchCV(estimator=logr,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nlogr_grid.fit(X, y)\nlogr_best_mod = logr_grid.best_estimator_","d358b6fc":"print('For Adaboost with logistic regression as base estimator\\n')\nprint('Best GridSearchCV roc_auc Score {}'.format(logr_grid.best_score_))\nprint('Hyperparameters           Values')\nprint('base_estimator__C:          {}'.format(logr_grid.best_estimator_.base_estimator.C))\nprint('n_estimators:               {}'.format(logr_grid.best_estimator_.n_estimators))\nprint('learning_rate:              {}'.format(logr_grid.best_estimator_.learning_rate))","1caa287c":"svml = clone(ada_svml)\nsvml.get_params()","74ee0cf4":"cv=StratifiedKFold(n_splits=10, shuffle=True)\n\nparam_grid = {'base_estimator__C' :[0.01, 0.1, 1, 10, 50, 100, 500, 1000],\n              'n_estimators' :[10, 50, 100, 250, 500, 750, 1000],\n              'learning_rate' :[0.0001, 0.001, 0.01, 0.1, 1]}\n\n\n\nsvml_grid = GridSearchCV(estimator=svml,\n                        param_grid=param_grid,\n                        cv=cv,\n                        scoring='roc_auc',\n                        return_train_score=True,\n                        n_jobs=4,\n                        verbose=1)\n\nsvml_grid.fit(X, y)\nsvml_best_mod = svml_grid.best_estimator_","1bdd2434":"print('For Adaboost with linear kernal SVM as base estimator\\n')\nprint('Best GridSearchCV roc_auc Score {}'.format(svml_grid.best_score_))\nprint('Hyperparameters           Values')\nprint('base_estimator__C:          {}'.format(svml_grid.best_estimator_.base_estimator.C))\nprint('n_estimators:               {}'.format(svml_grid.best_estimator_.n_estimators))\nprint('learning_rate:              {}'.format(svml_grid.best_estimator_.learning_rate))","b29ccd68":"s = ['roc_auc', 'accuracy', 'precision', 'recall']\nmod_names = ['Base', 'DecisonTree', 'ExtraTree', 'LogisticRegression', 'SVC']\nmods = [clone(base_best_mod), clone(deci_best_mod), clone(extr_best_mod), clone(logr_best_mod), clone(svml_best_mod)]\n\nfinal_results = fill_results_df(mods, mod_names, s, X, y, 10)\nprint('Results from tuned classifiers')\nfinal_results","59d5a376":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.3)\n\ndef grade_model(probs, thresh):\n    return np.array([1 if x>=thresh else 0 for x in probs[:,1]])","6de9b5a3":"best_probs = []\nfor mod in mods:\n    temp_mod = clone(mod)\n    temp_mod.fit(X_train, y_train)\n    best_probs.append(temp_mod.predict_proba(X_test))\n    \nbest_scores = []\nfor pb in best_probs:\n    best_scores.append(grade_model(pb, .5))","bff02ac9":"from sklearn.metrics import confusion_matrix, auc, roc_curve\n\ndef plot_auc(labels, probs, ax, mod_name):\n    fpr, tpr, threshold = roc_curve(labels, probs[:,1])\n    roc = auc(fpr, tpr)\n    sns.lineplot(x=fpr, y=tpr, ax=ax, label = 'AUC = {:.4f}'.format(roc))\n    sns.lineplot(x=[0, 1], y=[0, 1], ax=ax)\n    ax.legend(loc = 'lower right')\n    ax.set_xlim([0, 1])\n    ax.set_ylim([0, 1])\n    ax.set_title('Receiver Operating Characteristic for {}'.format(mod_name))\n    ax.set_ylabel('True Positive Rate')\n    ax.set_xlabel('False Positive Rate')\n    return plt","f319dde2":"from pandas.plotting import table\n\ndef plot_confusion(lab, scor, mod_name, ax):\n    conf = confusion_matrix(lab, scor)\n    tab = pd.DataFrame(conf, columns=['Score positive', 'Score negative'], index=['Actual positive', 'Actual negative'])\n    t = table(ax , tab, loc='center')\n    t.scale(1, 3)\n    ax.set_title('Confusion matrix for {}'.format(mod_name))\n    ax.axis('off')","73f89e82":"fig, ax = plt.subplots(5, 2, figsize=(15, 30))\n\nplot_confusion(y_test, best_scores[0], 'Base Adaboost', ax[0, 0])\nplot_auc(y_test, best_probs[0], ax[0, 1], 'Base Adaboost')\n\nplot_confusion(y_test, best_scores[1], 'DecisionTree Adaboost', ax[1, 0])\nplot_auc(y_test, best_probs[1], ax[1, 1], 'DecisionTree Adaboost')\n\nplot_confusion(y_test, best_scores[2], 'ExtraTree Adaboost', ax[2, 0])\nplot_auc(y_test, best_probs[2], ax[2, 1], 'ExtraTree Adaboost')\n\nplot_confusion(y_test, best_scores[3], 'Logistic Adaboost', ax[3, 0])\nplot_auc(y_test, best_probs[3], ax[3, 1], 'Logistic Adaboost')\n\nplot_confusion(y_test, best_scores[4], 'SVC Adaboost', ax[4, 0])\nplot_auc(y_test, best_probs[4], ax[4, 1], 'SVC Adaboost')\n\nplt.show()","ac31ab3e":"Feature Importance","ad06daef":"The default Adaboost base estimator is a decision stump. it is a decision tree with a max depth of 1."}}