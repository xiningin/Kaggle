{"cell_type":{"0d5f0d29":"code","bb1a17f4":"code","2938975a":"code","60ce4cf5":"code","ac184e2a":"code","6b6df7aa":"code","db3441be":"code","0ff95f1f":"code","073cd726":"code","a6573d69":"code","9d142e42":"code","7c7e68c0":"code","a0e4c7a3":"code","f4d84d6e":"code","8c0a3492":"markdown","a9eedc60":"markdown","c2992d66":"markdown","05f957c5":"markdown","78494db2":"markdown","a6a46518":"markdown","d7b39277":"markdown","cd150694":"markdown","7cc7adab":"markdown","6142743b":"markdown","f14b8229":"markdown","3c58eba4":"markdown","7886db18":"markdown","e762f853":"markdown"},"source":{"0d5f0d29":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\n\n# registering pandas converers to allow plotting with pd conveniently\n#pd.plotting.register_matplotlib_converters()","bb1a17f4":"df_train = pd.read_csv('..\/input\/homeserve-gas-rates\/3ygas.csv', parse_dates=['Date'], index_col=['Date'], sep=';', decimal=\",\")\ndf_train.info()","2938975a":"# display a bunch of data\ndf_train.head()","60ce4cf5":"# plot data\ndf_train.plot(figsize=(20,8))","ac184e2a":"# choose one variable to test and play with, let it be \"base\"\nss = df_train['base'].sort_index(ascending = True)\n\n# the target var must be float type\ndf_train['base'] = df_train['base'].astype('float32')\n\n# plot the total rates, per day\nss = ss.resample('D').sum()\nss.plot(kind='area', figsize=(20,5), legend=True, alpha=.5)","6b6df7aa":"# check for seasonality\nimport statsmodels.api as sm\n        \n# freq=the number of batches per period (year), as we use a weekly granularity ('W'), then freq=52\ndec_a = sm.tsa.seasonal_decompose(ss.resample('W').sum(), model = 'additive', freq = 52)\ndec_a.plot().show()","db3441be":"# use a rolling window and compute mean on the last \"window\" values (varied)\nss_est30d = ss.rolling(window=30).mean()\nss_est60d = ss.rolling(window=60).mean()\nss_est90d = ss.rolling(window=90).mean()","0ff95f1f":"import matplotlib.pyplot as plt\n\n# plot the week by week values: observed vs estimated for different values of window\nplt.figure(figsize=(20, 5))\nplt.plot(ss.resample('W').sum(), color='Black', label=\"Observed\", alpha=1)\nplt.plot(ss_est30d.resample('W').sum(), color='Red', label=\"Prediction: Rolling mean trend, based on 30d\", alpha=.5)\nplt.plot(ss_est60d.resample('W').sum(), color='Blue', label=\"Prediction: Rolling mean trend, based on 60d\", alpha=.5)\nplt.plot(ss_est90d.resample('W').sum(), color='Green', label=\"Prediction: Rolling mean trend, based on 90d\", alpha=.5)\nplt.legend(loc=\"upper left\")\nplt.grid(True)\nplt.show()","073cd726":"# import Prophet \nfrom fbprophet import Prophet\n\n#pd.plotting.deregister_matplotlib_converters()\n\nimport imp  \nimp.reload(pd)\n\n# reformat data\ns = ss.reset_index()\ns.columns = ['ds', 'y']\ns.head()","a6573d69":"# fit a Prohet model. We assume there is no daily seasonality (or not interested in)\nproph = Prophet(yearly_seasonality=True, weekly_seasonality=True, daily_seasonality=False)\nproph.fit(s[['ds','y']])","9d142e42":"# make a void dataframe with future timestamps then compute predictions for it\nfut = proph.make_future_dataframe(include_history=False, periods=12, freq = 'm') # 12 months\nforecast = proph.predict(fut)","7c7e68c0":"# plot all resulting components\nproph.plot_components(forecast).show()","a0e4c7a3":"# another way to plot results\nproph.plot(forecast).show()","f4d84d6e":"#!jupyter nbconvert --execute --to pdf __notebook_source__.ipynb\n#!curl --upload-file __notebook_source__.pdf https:\/\/transfer.sh\/notebook.pdf","8c0a3492":"The trend is then unstable (going up until 218, then down...), in addition to the obvious seasonality (peak in the begginning of the year).","a9eedc60":"![image.png](attachment:image.png)","c2992d66":"# Go Further..\nI'm sure we can build a nice model to predict the best price maximizing revenue for HomeServe. However, it seems that we need some more SQL joins to provide the useful data. When it comes to the mathematical model, a simple optimization model, like linear regression, seems to be a good start. In any case, I'd start by making an overview of the literature and come back with the couple of models that should be considered.\n\nMohamed","05f957c5":"Hi,\n\nThis is a step-by-step tutorial summarizing my first thoughts and tests towards the Static Surge Pricing Model for HomeServe. I usually use this type, called a \"notebook\" when dealing with non-technicals as it allows mixing content (text, figures, and code).\n\nSo, the problem at hand is how to find the optimal price at each time wrt. changing variables (#supply,#demands)?\nTraditional pricing models are both \"static\" or \"dynamic\". The first ones rely either on human knwoledge (manual), or simple mathematical models, like the one I'll explain below.\n\n# How would I take the problem\nWhen I first talked to Linda about the problem of Dynamic Pricing at HomeServe, the first solution that came to my mind is the good old regression model. The problem seems naturally fit into a more-or-less sophisticated regression with a good deal of feature engineering (variable choosing, building, and formatting). **The problem is an optimization problem**.","78494db2":"We observe that the rolling mean (aka moving average) is a good estimator. The 90d model takes more time to capture the trend, whereas the sortest models are the best estimetors. Indeed, a 1-day average model it is generally an excellent baseline, sometimes impossible to beat. ","a6a46518":"# The Rest of this Tutorial\n\n* EDA\n* TS Decomposition\n* Forecast :\n 1. Moving average\n 2. Prophet\n* Go Further.. \n\n ## Load data & display basic statistics","d7b39277":"## Prophet\nProphet is a librry developed by Facebook for time series forecasting and modeling. It has recently shown excellent results and is more and more popular among data scientists (see Kaggle).","cd150694":"There seems to be a significant seasonality while the overall trend is going up..","7cc7adab":"## Check & decompose seasonality","6142743b":"This is a first approach to predicting time series. The assumption is that : tomorrow is likely to look like today. Let's see how it goes..","f14b8229":"## Moving average","3c58eba4":"\n# What I actually did\nWhen going through the data sheets that you sent me by email, I was not sure of my understanding of the problem. For me, we want to predict the best hour rate (*price*) for which some kind of KPI (eg, revenue, number of worked hours, etc..) should be maximized (let's call it Y). The rate depends on those #supply and #demand variables (let's call them X1, X2). Then, the model takes as input X1 and X2 and predicts the price corresponding to the best value of Y (eg, maximum profit or revenue).\n\nHowever, X1 and X2 are lacking from the data. Also, we do not have any feedback on the manually-fixed rates (either with or without weather input); how did they perform? \n\nHere is what an ideal row of data should look like (for me) :\n\n*Date ; X1 ; X2 ; Normal Price ; Manual Surge Coef. ; Y* \n\n    Where : \n    X1, X2 = nb. supply and demands respec.\n    Normal Price = the price before any surge has been applied\n    Manual surge coef. = the applied surge coefficient\n    Y = Revenue amount (how much \u00a3 HomeServe earned from this transaction).","7886db18":"![image.png](attachment:image.png)","e762f853":"This was what Prophet predicted, but the reality is that it is impossible because we can not have negative rates. This happened because Prophet is not aware of that. Please be informed that this kind of external information can be easily injectedinto the model in order to have more precise predictions, along with other information like holidays, max\/min values, etc.."}}