{"cell_type":{"62fcfa68":"code","ea6872fa":"code","38494239":"code","11ec9e40":"code","1719e493":"code","50d5c25a":"code","c412080c":"code","a8c82506":"code","01a95c90":"code","0549264c":"code","dc280ef1":"code","1dfc482e":"code","9a6d805c":"code","faf6f05b":"code","f7e4713d":"markdown","e9d92724":"markdown","1b0c08b8":"markdown","9b8c21a8":"markdown","56f0696d":"markdown"},"source":{"62fcfa68":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n# #         print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea6872fa":"import matplotlib.pyplot as plt","38494239":"train_data_df=pd.read_csv(\"..\/input\/global-wheat-detection\/train.csv\")\ntrain_data_df.head()","11ec9e40":"image_id=[f\"{i}.jpg\" for i in train_data_df.image_id]\nxmins,ymins,xmaxs,ymaxs=[],[],[],[]\nfor bbox in train_data_df.bbox:\n    real_bbox=eval(bbox)\n    \n    xmin, ymin ,w ,h=real_bbox\n    \n    \n    \n    a=int(xmin+w)\n    b=int(ymin+h)\n    xmaxs.append(a)\n    ymaxs.append(b)\n\n    \n    c=int(xmin)\n    d=int(ymin)\n    xmins.append(c)\n    ymins.append(d)","1719e493":"data=pd.DataFrame()\ndata[\"filename\"]=image_id\ndata[\"width\"]=train_data_df.width\ndata[\"width\"]=train_data_df.height\n\ndata[\"class\"]=[\"wheat\"]*len(image_id)\n\ndata[\"xmin\"]=xmins\ndata[\"ymin\"]=ymins\n\ndata[\"xmax\"]=xmaxs\ndata[\"ymax\"]=ymaxs\n","50d5c25a":"data.head()","c412080c":"data.to_csv(\"train_labels.csv\",index=False)","a8c82506":"pd.read_csv(\"\/kaggle\/working\/train_labels.csv\")","01a95c90":"def int64_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n\n\ndef int64_list_feature(value):\n  return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n\n\ndef bytes_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n\n\ndef bytes_list_feature(value):\n  return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n\n\ndef float_list_feature(value):\n  return tf.train.Feature(float_list=tf.train.FloatList(value=value))","0549264c":"from __future__ import division\nfrom __future__ import print_function\nfrom __future__ import absolute_import\n\nimport os\nimport io\nimport pandas as pd\nimport tensorflow as tf\n\nfrom PIL import Image\nfrom collections import namedtuple, OrderedDict\n\n\n# TO-DO replace this with label map\ndef class_text_to_int(row_label):\n    if row_label == 'wheat':\n        return 1\n    else:\n        None\n\n\ndef split(df, group):\n    data = namedtuple('data', ['filename', 'object'])\n    gb = df.groupby(group)\n    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]\n\n\ndef create_tf_example(group, path):\n    with tf.io.gfile.GFile(os.path.join(path, '{}'.format(group.filename)), 'rb') as fid:\n        encoded_jpg = fid.read()\n    encoded_jpg_io = io.BytesIO(encoded_jpg)\n    image = Image.open(encoded_jpg_io)\n    \n    width, height = image.size\n\n    filename = group.filename.encode('utf8')\n    image_format = b'jpg'\n    xmins = []\n    xmaxs = []\n    ymins = []\n    ymaxs = []\n    classes_text = []\n    classes = []\n\n    for index, row in group.object.iterrows():\n        xmins.append(row['xmin'] \/ width)\n        xmaxs.append(row['xmax'] \/ width)\n        ymins.append(row['ymin'] \/ height)\n        ymaxs.append(row['ymax'] \/ height)\n        classes_text.append(row['class'].encode('utf8'))\n        classes.append(class_text_to_int(row['class']))\n\n    tf_example = tf.train.Example(features=tf.train.Features(feature={\n        'image\/height': int64_feature(height),\n        'image\/width': int64_feature(width),\n        'image\/filename': bytes_feature(filename),\n        'image\/source_id':bytes_feature(filename),\n        'image\/encoded':bytes_feature(encoded_jpg),\n        'image\/format': bytes_feature(image_format),\n        'image\/object\/bbox\/xmin': float_list_feature(xmins),\n        'image\/object\/bbox\/xmax': float_list_feature(xmaxs),\n        'image\/object\/bbox\/ymin': float_list_feature(ymins),\n        'image\/object\/bbox\/ymax': float_list_feature(ymaxs),\n        'image\/object\/class\/text':bytes_list_feature(classes_text),\n        'image\/object\/class\/label':int64_list_feature(classes),\n    }))\n    return tf_example\n\n\ndef main(csv_input, output_path, image_dir):\n    writer = tf.io.TFRecordWriter(output_path)\n    path = os.path.join(image_dir)\n    examples = pd.read_csv(csv_input)\n    grouped = split(examples, 'filename')\n    for group in grouped:\n        tf_example = create_tf_example(group, path)\n        writer.write(tf_example.SerializeToString())\n\n    writer.close()\n    output_path = os.path.join(os.getcwd(), output_path)\n    print('Successfully created the TFRecords: {}'.format(output_path))\n\n\nif __name__ == '__main__':\n    csv_input=\"train_labels.csv\"\n    output_path=\"train_label.record\"\n    image_dir=\"\/kaggle\/input\/global-wheat-detection\/train\"\n    main(csv_input, output_path, image_dir)","dc280ef1":"cd \"\/kaggle\/input\/kerasversionefficientdet\"","1dfc482e":"os.mkdir(\"\/kaggle\/working\/model\")","9a6d805c":"!pip install pycocotools","faf6f05b":"# uncomment for training\n# !python main.py --mode=train --training_file_pattern=train.record --model_name=efficientdet-d3 --model_dir=\/kaggle\/working\/model --model_name=efficientdet-d3 --ckpt=\/kaggle\/input\/effiecientdetd3-10k-epoch-checkpoints --train_batch_size=4 --num_epochs=3000 --num_examples_per_epoch=16","f7e4713d":"for Easy inferencing checkout my kernel\n[https:\/\/www.kaggle.com\/ravi02516\/tensorflow-efficientdet-d3-with-default-parameters](http:\/\/)","e9d92724":"**Create tfrecord for training**","1b0c08b8":"After creating tf record files you have to put your .record file in to efficientdet repo else it will throw an error no match pattern which i don't know why if someone knows why this error occurs do let me know in comment","9b8c21a8":"**Now you can train efficientDet d0-d7**\n\nHere is the link of different efficientDet models pretrained weights on coco dataset\n\n\n\nEfficientDet-D0 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d0.tar.gz](http:\/\/)\n\nEfficientDet-D1 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d1.tar.gz](http:\/\/)\n\nEfficientDet-D2 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d2.tar.gz](http:\/\/)\n\nEfficientDet-D3* \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d3_softnms.tar.gz](http:\/\/)\n\nEfficientDet-D4 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d4.tar.gz](http:\/\/)\n\n\nEfficientDet-D5 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d5.tar.gz](http:\/\/)\n\nEfficientDet-D6 \t[https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d6.tar.gz](http:\/\/)\n\nEfficientDet-D7*    [https:\/\/storage.googleapis.com\/cloud-tpu-checkpoints\/efficientdet\/coco\/efficientdet-d7.tar.gz](http:\/\/)\n\n\n\nTo know more about efficientDet checkout the google automl repo on github \n\n[https:\/\/github.com\/google\/automl\/tree\/master\/efficientdet](http:\/\/)","56f0696d":"currently kaggle does not support tensorboard and i having trouble with showing losses on terminal because of tpu estimator.I have made the dataset which i created and repo publically so you can download repo with tfrecord and train it on google colab which supports tensorboard or locally if you have better gpu"}}