{"cell_type":{"bf531056":"code","2c4c30d7":"code","16068e57":"code","740f9f6a":"code","86d6793e":"code","0ecc1276":"code","f06be411":"code","503251d0":"code","a5ac885a":"code","51d39488":"code","1f933ba4":"code","7edfc629":"code","8777892c":"code","5971aa5c":"code","2ae56f3b":"code","f8113de2":"code","bbda985f":"code","9d6bd69c":"code","bfca64b3":"code","60d3eefc":"code","4fbc2cba":"code","29b6007c":"code","94f7d6c7":"code","913c3249":"code","07a2d5cb":"code","b267b3ca":"code","7814d09b":"code","1c737060":"code","fcc6d76f":"code","46718456":"code","51888b43":"code","6f6f5e9a":"code","f3dbab8e":"code","9f08c6da":"code","642133de":"code","b3bb9688":"code","833ab64d":"code","7498400e":"code","c2521588":"code","40a1243f":"code","f411a1ab":"code","0e5cdc3e":"code","9a24296f":"code","dd8adb10":"code","30414be5":"code","7ff26397":"code","6b45bccb":"code","d7a18a66":"code","135f23af":"code","cf728c3d":"code","859d9f73":"code","c1fcdfae":"code","681ac617":"code","7e315def":"code","1d7d7785":"code","4ac84f59":"code","ee8500b0":"code","74c34904":"code","ef163f10":"code","680514dd":"code","86a3aac7":"code","34c01fda":"code","d932066a":"code","5733c9a4":"code","64e25b65":"code","aa519d6d":"code","4b9e69c8":"code","52adc2c9":"code","37e41041":"code","2809a4a2":"markdown","83cd5f2a":"markdown","ff05021c":"markdown","aabd0371":"markdown","007bc572":"markdown","43541e30":"markdown","a0dbc528":"markdown","5bcc7d30":"markdown","670ae73e":"markdown","61b9c44b":"markdown","20f6f75f":"markdown","5fea5ec9":"markdown","bcd07048":"markdown","2bfbf4c9":"markdown","37565640":"markdown","faf637e1":"markdown","6adbe28d":"markdown","2c85db10":"markdown","5456c402":"markdown","96ed15e0":"markdown","8adcdbce":"markdown","39586763":"markdown","17172498":"markdown","224409b1":"markdown","722cec32":"markdown","34e92579":"markdown","512345d9":"markdown","185552f5":"markdown","a8b4593b":"markdown","c7c224d0":"markdown","f7447e20":"markdown","9fea87d0":"markdown","e0bf0788":"markdown","a8916920":"markdown","9ad2e446":"markdown","5a0a6dc4":"markdown","e1f0c351":"markdown","b95c0bf0":"markdown","474ea644":"markdown","1c85de9e":"markdown","c0338771":"markdown","08ba31ca":"markdown","f2b6b138":"markdown","87115fca":"markdown","e1fe9633":"markdown","04a5446e":"markdown","14f7ce0b":"markdown","6924a825":"markdown","d74e266e":"markdown","f047973b":"markdown","2bd55317":"markdown","977099fe":"markdown","93b3b5ac":"markdown","8493d03c":"markdown"},"source":{"bf531056":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\n\n# mlp for regression with mse loss function\nfrom sklearn.datasets import make_regression\nfrom sklearn.datasets import make_circles\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.datasets.samples_generator import make_blobs\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nfrom keras.optimizers import SGD\nfrom keras.utils import to_categorical\nimport matplotlib.pyplot as plt","2c4c30d7":"# prepare multi-class classification dataset\ndef create_dataset():\n  # generate 2d classification dataset\n    X, y = make_blobs(n_samples=1000, centers=20, n_features=100, cluster_std=2,\n      random_state=2)\n    # one hot encode output variable\n    y = to_categorical(y)\n    # split into train and test\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    return trainX, trainy, testX, testy\n\n\n# fit model with given number of nodes, returns test set accuracy\ndef evaluate_model(n_nodes, trainX, trainy, testX, testy):\n    n_input, n_classes = trainX.shape[1], testy.shape[1]\n    \n    model = Sequential()\n    model.add(Dense(n_nodes, input_dim=(n_input), activation='relu',\n                   kernel_initializer = 'he_uniform'))\n    model.add(Dense(n_classes, activation='softmax'))\n    \n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    # fit model on train set\n    history = model.fit(trainX, trainy, epochs=100, verbose=0)\n    # evaluate model on test set\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    return history, test_acc","16068e57":"# prepare dataset\ntrainX, trainy, testX, testy = create_dataset()\n# evaluate model and plot learning curve with given number of nodes\nnum_nodes = [1, 2, 3, 4, 5, 6, 7]\n\nhist_list , nodes_list = list(), list()\nfor n_nodes in num_nodes:\n    history, result = evaluate_model(n_nodes, trainX, trainy, testX, testy)\n    print('nodes=%d: , Accuracy :%.3f' % (n_nodes, result))\n    hist_list.append(history)","740f9f6a":"loss = list()\nrow_index = 0\nfor x in hist_list:\n    loss.append(x.history['loss'])\n    \n# summarizing historical accuracy\nplt.figure(figsize=(10,8))\nplt.plot(loss[0])\nplt.plot(loss[1])\nplt.plot(loss[2])\nplt.plot(loss[3])\nplt.plot(loss[4])\nplt.plot(loss[5])\nplt.plot(loss[6])\nplt.title('Model Loss vs No.of Neurons\/Nodes')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['1', '2','3','4','5','6','7'], loc='upper right')\nplt.show()","86d6793e":"def evaluate_model(n_layers, trainX, trainy, testX, testy):\n    n_input, n_classes = trainX.shape[1], testy.shape[1]\n    model = Sequential()\n    model.add(Dense(10, input_dim=(n_input), activation='relu',\n                   kernel_initializer='he_uniform'))\n    for _ in range(n_layers):\n        model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(n_classes, activation='softmax'))\n    \n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n    \n    # fit model\n    history = model.fit(trainX, trainy, epochs=100, verbose=0)\n    # evaluate model on test set\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    return history, test_acc\n\n# get dataset\ntrainX, trainy, testX, testy = create_dataset()\n# evaluate model and plot learning curve of model with given number of layers\nhist_list = list()\nnum_layers = [1, 2, 3, 4, 5]\nfor n_layers in num_layers:\n  # evaluate model with a given number of layers\n    history, result = evaluate_model(n_layers, trainX, trainy, testX, testy) \n    print('layers=%d: %.3f' % (n_layers, result))\n    hist_list.append(history)","0ecc1276":"loss = list()\nrow_index = 0\nfor x in hist_list:\n    loss.append(x.history['loss'])\n    \n# summarizing historical accuracy\nplt.figure(figsize=(10,8))\nplt.plot(loss[0])\nplt.plot(loss[1])\nplt.plot(loss[2])\nplt.plot(loss[3])\nplt.plot(loss[4])\n\nplt.title('Model Loss vs No.of layers')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['1', '2','3','4','5'], loc='upper right')\nplt.show()","f06be411":"def create_dataset():\n  # generate 2d classification dataset\n    X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n    # one hot encode output variable\n    y = to_categorical(y)\n    # split into train and test\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    return trainX, trainy, testX, testy\n\n\n\n\ndef run_model(n_batch_size, trainX, trainy, testX, testy, opt):\n    n_input, n_classes = trainX.shape[1], testy.shape[1]\n    \n    model = Sequential()\n    model.add(Dense(50, input_dim=(n_input), activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(n_classes, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer=opt)\n    \n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        batch_size=n_batch_size, epochs=200, verbose=0)\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0) \n    _, test_acc = model.evaluate(testX, testy, verbose=0) \n    print('Train Accuracy: %.3f, Test Accuracy: %.3f' % (train_acc, test_acc))\n    \n    return history","503251d0":"# prepare dataset\ntrainX, trainy, testX, testy = create_dataset()\nopt = SGD(lr=0.01, momentum=0.9)\n\nhistory = run_model(n_batch_size=len(trainX), trainX=trainX, \n                    trainy=trainy, testX=testX, testy=testy, opt=opt)","a5ac885a":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","51d39488":"# prepare dataset\ntrainX, trainy, testX, testy = create_dataset()\nopt = SGD(lr=0.01, momentum=0.9)\nhistory = run_model(1, trainX, trainy, testX, testy, opt)","1f933ba4":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","7edfc629":"# prepare dataset\ntrainX, trainy, testX, testy = create_dataset()\n\n# Reducing Learning rate\nopt = SGD(lr=0.001, momentum=0.9)\nhistory = run_model(1, trainX, trainy, testX, testy, opt)","8777892c":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","5971aa5c":"# prepare dataset\ntrainX, trainy, testX, testy = create_dataset()\nopt = SGD(lr=0.01, momentum=0.9)\n\n\nbatch_sizes = [4, 8, 16, 32, 64, 128, 256, 450]\n\n_ = plt.figure(figsize=(12,12))\n\nfor i in range(len(batch_sizes)):\n    # fit model and plot learning curves for a batch size\n    history = run_model(batch_sizes[i], trainX, trainy, testX, testy, opt)\n    \n    plot_no = 420 + (i+1)\n    plt.subplot(plot_no)\n    _ = plt.plot(history.history['accuracy'], label='train')\n    _ = plt.plot(history.history['val_accuracy'], label='test')\n    _ = plt.title('batch '+str(batch_sizes[i]))\n\nplt.show()","2ae56f3b":"def create_dataset():\n    X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n    X = StandardScaler().fit_transform(X)\n    y = StandardScaler().fit_transform(y.reshape(len(y),1))[:,0]\n\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    \n    return trainX, trainy, testX, testy\n\ndef run_model(loss, trainX, trainy, testX, testy):\n    model = Sequential()\n    model.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(1, activation='linear'))\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(loss=loss, optimizer=opt)\n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        epochs=100, verbose=0)\n    train_mse = model.evaluate(trainX, trainy, verbose=0)\n    test_mse = model.evaluate(testX, testy, verbose=0)\n    print('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n    \n    return history    ","f8113de2":"trainX, trainy, testX, testy = create_dataset()\n\nloss = 'mean_squared_error'\nhistory = run_model(loss, trainX, trainy, testX, testy)","bbda985f":"_ = plt.figure(figsize=(10,5))\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","9d6bd69c":"trainX, trainy, testX, testy = create_dataset()\n\nloss = 'mean_squared_logarithmic_error'\nhistory = run_model(loss, trainX, trainy, testX, testy)","bfca64b3":"_ = plt.figure(figsize=(10,5))\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","60d3eefc":"trainX, trainy, testX, testy = create_dataset()\n\nloss = 'mean_absolute_error'\nhistory = run_model(loss, trainX, trainy, testX, testy)","4fbc2cba":"_ = plt.figure(figsize=(10,5))\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","29b6007c":"def create_dataset():\n    X, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n    # split into train and test\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    return trainX, trainy, testX, testy\n\ndef run_model(loss, activation, trainX, trainy, testX, testy):\n    model = Sequential()\n    model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(1, activation=activation))\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        epochs=200, verbose=0)\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0) \n    _, test_acc = model.evaluate(testX, testy, verbose=0) \n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n    \n    return history      ","94f7d6c7":"trainX, trainy, testX, testy = create_dataset()\n\nloss = 'binary_crossentropy'\nactivation = 'sigmoid'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)","913c3249":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Binary Cross Entropy Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","07a2d5cb":"trainX, trainy, testX, testy = create_dataset()\n\ntrainy[np.where(trainy == 0)] = -1\ntesty[np.where(testy == 0)] = -1\n\nloss = 'hinge'\nactivation = 'tanh'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)","b267b3ca":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Hinge Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","7814d09b":"trainX, trainy, testX, testy = create_dataset()\n\ntrainy[np.where(trainy == 0)] = -1\ntesty[np.where(testy == 0)] = -1\n\nloss = 'squared_hinge'\nactivation = 'tanh'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)","1c737060":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Squared Hinge Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","fcc6d76f":"def create_dataset(one_hot_encode='yes'):\n    X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n    # one hot encode output variable\n    if one_hot_encode == 'yes':\n        y = to_categorical(y)\n    # split into train and test\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    return trainX, trainy, testX, testy\n\ndef run_model(loss, activation, trainX, trainy, testX, testy, lrate=0.01):\n    model = Sequential()\n    model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(3, activation=activation))\n    opt = SGD(lr=lrate, momentum=0.9)\n    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        epochs=200, verbose=0)\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n    \n    return history   ","46718456":"trainX, trainy, testX, testy = create_dataset()\n\nloss = 'categorical_crossentropy'\nactivation = 'softmax'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)","51888b43":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Categorical Cross Entropy Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","6f6f5e9a":"trainX, trainy, testX, testy = create_dataset(one_hot_encode='no')\n\nloss = 'sparse_categorical_crossentropy'\nactivation = 'softmax'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)","f3dbab8e":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Sparse Categorical Cross Entropy Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","9f08c6da":"trainX, trainy, testX, testy = create_dataset(one_hot_encode='yes')\n\nloss = 'kullback_leibler_divergence'\nactivation = 'softmax'\nhistory = run_model(loss, activation, trainX, trainy, testX, testy)\n","642133de":"_ = plt.figure(figsize=(10,10))\n_ = plt.subplot(211)\n\n_ = plt.plot(history.history['loss'], label='train')\n_ = plt.plot(history.history['val_loss'], label='test')\n_ = plt.title('Sparse Categorical Cross Entropy Loss : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()\n\n_ = plt.subplot(212)\n_ = plt.plot(history.history['accuracy'], label='train')\n_ = plt.plot(history.history['val_accuracy'], label='test')\n_ = plt.title('Accuracy : ')\n_ = plt.xlabel('Epochs')\n_ = plt.legend()","b3bb9688":"opt = SGD(lr=0.01, momentum=0.9, decay=0.01)\nmodel.compile(..., optimizer=opt)","833ab64d":"from keras.callbacks import ReduceLROnPlateau\n\nrlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=100) \nmodel.fit(..., callbacks=[rlrop])","7498400e":"from keras.callbacks import LearningRateScheduler\n\n# Your own Function\ndef my_learning_rate(epoch, lrate)\n  return lrate\n\nlrs = LearningRateScheduler(my_learning_rate)\nmodel.fit(..., callbacks=[lrs])","c2521588":"trainX, trainy, testX, testy = create_dataset(one_hot_encode='yes')\n\nloss = 'categorical_crossentropy'\nactivation = 'softmax'\nlearning_rates = [1E-0, 1E-1, 1E-2, 1E-3, 1E-4, 1E-5, 1E-6, 1E-7]\n\n_ = plt.figure(figsize=(12,14))\nfor i in range(len(learning_rates)):\n    print(('LR = '+str(learning_rates[i])))\n    history = run_model(loss, activation, trainX, trainy, testX, testy, learning_rates[i])\n    \n    plot_no = 420 + (i+1)\n    plt.subplot(plot_no)\n    _ = plt.plot(history.history['accuracy'], label='train')\n    _ = plt.plot(history.history['val_accuracy'], label='test')\n    _ = plt.title('LR = '+str(learning_rates[i]))\n    \n_ = plt.show()","40a1243f":"def run_model(loss, activation, trainX, trainy, testX, testy, momentum, lrate=0.01):\n    model = Sequential()\n    model.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(3, activation=activation))\n    opt = SGD(lr=lrate, momentum=momentum)\n    model.compile(loss=loss, optimizer=opt, metrics=['accuracy'])\n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        epochs=200, verbose=0)\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n    \n    return history ","f411a1ab":"trainX, trainy, testX, testy = create_dataset(one_hot_encode='yes')\n\nloss = 'categorical_crossentropy'\nactivation = 'softmax'\nmomentums = [0.0, 0.5, 0.9, 0.99]\n\n_ = plt.figure(figsize=(12,10))\n\nfor i in range(len(momentums)):\n    print(('Momentum = '+str(momentums[i])))\n    history = run_model(loss, activation, trainX, trainy, testX, testy, momentums[i])\n    \n    plot_no = 220 + (i+1)\n    plt.subplot(plot_no)\n    _ = plt.plot(history.history['accuracy'], label='train')\n    _ = plt.plot(history.history['val_accuracy'], label='test')\n    _ = plt.title('Momentum = '+str(momentums[i]))\n    \n_ = plt.show()","0e5cdc3e":"from keras.callbacks import Callback\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras import backend\n\n# monitor the learning rate\nclass LearningRateMonitor(Callback):\n    # start of training\n    def on_train_begin(self, logs={}):\n        self.lrates = list()\n    # end of each training epoch\n    def on_epoch_end(self, epoch, logs={}):\n        # get and store the learning rate\n        optimizer = self.model.optimizer\n        lrate = float(backend.get_value(optimizer.lr))\n        self.lrates.append(lrate)\n\ndef run_model(patience, trainX, trainy, testX, testy):\n    model = Sequential()\n    model.add(Dense(50, input_dim=2, activation='relu',kernel_initializer='he_uniform'))\n    model.add(Dense(3, activation='softmax'))\n    opt = SGD(lr=0.01)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy'])\n    \n    rlrop = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=patience, \n                              min_delta=1E-7)\n    lrm = LearningRateMonitor()\n    \n    history = model.fit(trainX, trainy, validation_data=(testX, testy), \n                        epochs=200,verbose=0, callbacks=[rlrop, lrm])\n    \n    return lrm.lrates, history.history['loss'], history.history['accuracy']\n","9a24296f":"# create line plots for a series\ndef line_plots(patiences, series):\n    plt.figure(figsize=(10,6))\n    for i in range(len(patiences)):\n        plt.subplot(220 + (i+1))\n        plt.plot(series[i])\n        plt.title('patience='+str(patiences[i]), pad=-80) \n    plt.show()","dd8adb10":"trainX, trainy, testX, testy = create_dataset()\npatiences = [2, 5, 10, 15]\nlr_list, loss_list, acc_list, = list(), list(), list()\nfor i in range(len(patiences)):\n    # fit model and plot learning curves for a patience\n    lr, loss, acc = run_model(patiences[i], trainX, trainy, testX, testy)\n    lr_list.append(lr)\n    loss_list.append(loss)\n    acc_list.append(acc)\n\n\n# plot learning rates\nprint('Learning Rate : ')\nline_plots(patiences, lr_list)\n\n# plot loss\nprint(\"Loss :\")\nline_plots(patiences, loss_list)\n\n# plot accuracy\nprint(\"Accuracy : \")\nline_plots(patiences, acc_list)","30414be5":"from keras.layers import BatchNormalization\n\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","7ff26397":"model = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform')) \nmodel.add(Dense(1, activation='sigmoid'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","6b45bccb":"history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss learning curves\nplt.figure(figsize=(10,10))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","d7a18a66":"model = Sequential()\nmodel.add(Dense(50, input_dim=2, activation='relu', kernel_initializer='he_uniform')) \nmodel.add(BatchNormalization())\nmodel.add(Dense(1, activation='sigmoid'))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","135f23af":"history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) # evaluate the model\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n# plot loss learning curves\nplt.figure(figsize=(10,10))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","cf728c3d":"from keras.initializers import RandomUniform\nfrom sklearn.preprocessing import MinMaxScaler\n\nX, y = make_circles(n_samples=1000, noise=0.1, random_state=1)\n# scale input data to [-1,1]\nscaler = MinMaxScaler(feature_range=(-1, 1))\nX = scaler.fit_transform(X)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","859d9f73":"init = RandomUniform(minval=0, maxval=1)\nmodel = Sequential()\nmodel.add(Dense(5, input_dim=2, activation='tanh', kernel_initializer=init)) \nmodel.add(Dense(5, activation='tanh', kernel_initializer=init))\nmodel.add(Dense(5, activation='tanh', kernel_initializer=init)) \nmodel.add(Dense(5, activation='tanh', kernel_initializer=init)) \nmodel.add(Dense(5, activation='tanh', kernel_initializer=init)) \nmodel.add(Dense(1, activation='sigmoid', kernel_initializer=init))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)","c1fcdfae":"plt.figure(figsize=(10,10))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","681ac617":"init = 'he_uniform'\nmodel = Sequential()\nmodel.add(Dense(5, input_dim=2, activation='relu', kernel_initializer=init)) \nmodel.add(Dense(5, activation='relu', kernel_initializer=init))\nmodel.add(Dense(5, activation='relu', kernel_initializer=init)) \nmodel.add(Dense(5, activation='relu', kernel_initializer=init)) \nmodel.add(Dense(5, activation='relu', kernel_initializer=init)) \nmodel.add(Dense(1, activation='sigmoid', kernel_initializer=init))\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])\n# fit model\nhistory = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=500, verbose=0)\n_, train_acc = model.evaluate(trainX, trainy, verbose=0)\n_, test_acc = model.evaluate(testX, testy, verbose=0)","7e315def":"plt.figure(figsize=(10,10))\nplt.subplot(211)\nplt.title('Cross-Entropy Loss', pad=-40)\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy learning curves\nplt.subplot(212)\nplt.title('Accuracy', pad=-40)\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","1d7d7785":"X, y = make_regression(n_samples=1000, n_features=20, noise=0.1, random_state=1)\n# split into train and test\nn_train = 500\ntrainX, testX = X[:n_train, :], X[n_train:, :]\ntrainy, testy = y[:n_train], y[n_train:]","4ac84f59":"model = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform')) \nmodel.add(Dense(1, activation='linear'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9)\nmodel.compile(loss='mean_squared_error', optimizer=opt)","ee8500b0":"history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) # evaluate the model\ntrain_mse = model.evaluate(trainX, trainy, verbose=0)\ntest_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\nplt.figure(figsize=(10,5))\nplt.title('Mean Squared Error')\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","74c34904":"model = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform')) \nmodel.add(Dense(1, activation='linear'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9, clipvalue=2.0)\nmodel.compile(loss='mean_squared_error', optimizer=opt)","ef163f10":"history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) # evaluate the model\ntrain_mse = model.evaluate(trainX, trainy, verbose=0)\ntest_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\nplt.figure(figsize=(10,5))\nplt.title('Mean Squared Error')\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","680514dd":"model = Sequential()\nmodel.add(Dense(25, input_dim=20, activation='relu', kernel_initializer='he_uniform')) \nmodel.add(Dense(1, activation='linear'))\n# compile model\nopt = SGD(lr=0.01, momentum=0.9, clipnorm=1.0)\nmodel.compile(loss='mean_squared_error', optimizer=opt)","86a3aac7":"history = model.fit(trainX, trainy, validation_data=(testX, testy), epochs=100, verbose=0) # evaluate the model\ntrain_mse = model.evaluate(trainX, trainy, verbose=0)\ntest_mse = model.evaluate(testX, testy, verbose=0)\nprint('Train: %.3f, Test: %.3f' % (train_mse, test_mse))\n# plot loss during training\nplt.figure(figsize=(10,5))\nplt.title('Mean Squared Error')\nplt.plot(history.history['loss'], label='train') \nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","34c01fda":"def create_dataset():\n    X, y = make_blobs(n_samples=1000, centers=3, n_features=2, cluster_std=2, random_state=2)\n    # one hot encode output variable\n    y = to_categorical(y)\n    # split into train and test\n    n_train = 500\n    trainX, testX = X[:n_train, :], X[n_train:, :]\n    trainy, testy = y[:n_train], y[n_train:]\n    return trainX, testX, trainy, testy","d932066a":"def get_base_model(trainX, trainy):\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(3, activation='softmax'))\n    opt = SGD(lr=0.01, momentum=0.9)\n    model.compile(loss='categorical_crossentropy', optimizer=opt, metrics=['accuracy']) # fit model\n    model.fit(trainX, trainy, epochs=100, verbose=0)\n    return model\n\ndef evaluate_model(model, trainX, testX, trainy, testy):\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    return train_acc, test_acc","5733c9a4":"# add one new layer and re-train only the new layer\ndef add_layer(model, trainX, trainy):\n    # remember the current output layer\n    output_layer = model.layers[-1]\n    # remove the output layer\n    model.pop()\n    # mark all remaining layers as non-trainable\n    for layer in model.layers:\n        layer.trainable = False\n    # add a new hidden layer\n    model.add(Dense(10, activation='relu', kernel_initializer='he_uniform')) \n    # re-add the output layer\n    model.add(output_layer)\n    # fit model\n    model.fit(trainX, trainy, epochs=100, verbose=0)","64e25b65":"trainX, testX, trainy, testy = create_dataset()\n# get the base model\nmodel = get_base_model(trainX, trainy)\n# evaluate the base model\nscores = dict()\ntrain_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\nprint('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc)) \nscores[len(model.layers)] = (train_acc, test_acc)\n# add layers and evaluate the updated model\nn_layers = 10\nfor i in range(n_layers):\n    # add layer\n    add_layer(model, trainX, trainy)\n    # evaluate model\n    train_acc, test_acc = evaluate_model(model, trainX, testX, trainy, testy)\n    print('> layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n    # store scores for plotting\n    scores[len(model.layers)] = (train_acc, test_acc)","aa519d6d":"plt.figure(figsize=(8,5))\nplt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.') \nplt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.') \nplt.legend()\nplt.show()","4b9e69c8":"def base_autoencoder(trainX, testX):\n    model = Sequential()\n    model.add(Dense(10, input_dim=2, activation='relu', kernel_initializer='he_uniform'))\n    model.add(Dense(2, activation='linear'))\n    model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n    model.fit(trainX, trainX, epochs=100, verbose=0)\n    train_mse = model.evaluate(trainX, trainX, verbose=0)\n    test_mse = model.evaluate(testX, testX, verbose=0)\n    print('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))\n    return model\n\ndef evaluate_autoencoder_as_classifier(model, trainX, trainy, testX, testy):\n    output_layer = model.layers[-1]\n    model.pop()\n    for layer in model.layers:\n        layer.trainable = False\n    model.add(Dense(3, activation='softmax'))\n    model.compile(loss='categorical_crossentropy', optimizer=SGD(lr=0.01, momentum=0.9), \n                  metrics=['accuracy'])\n    model.fit(trainX, trainy, epochs=100, verbose=0)\n    # evaluate model\n    _, train_acc = model.evaluate(trainX, trainy, verbose=0)\n    _, test_acc = model.evaluate(testX, testy, verbose=0)\n    model.pop()\n    model.add(output_layer)\n    model.compile(loss='mean_squared_error', optimizer=SGD(lr=0.01, momentum=0.9))\n    return train_acc, test_acc\n\ndef add_layer_to_autoencoder(model, trainX, testX):\n    output_layer = model.layers[-1]\n    model.pop()\n    for layer in model.layers:\n        layer.trainable = False\n    model.add(Dense(10, activation='relu', kernel_initializer='he_uniform'))\n    model.add(output_layer)\n    # fit model\n    model.fit(trainX, trainX, epochs=100, verbose=0)\n    # evaluate reconstruction loss\n    train_mse = model.evaluate(trainX, trainX, verbose=0)\n    test_mse = model.evaluate(testX, testX, verbose=0)\n    print('> reconstruction error train=%.3f, test=%.3f' % (train_mse, test_mse))","52adc2c9":"# prepare data\ntrainX, testX, trainy, testy = create_dataset()\n# get the base autoencoder\nmodel = base_autoencoder(trainX, testX)\n# evaluate the base model\nscores = dict()\ntrain_acc, test_acc = evaluate_autoencoder_as_classifier(model, trainX, trainy, testX,\ntesty)\nprint('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers),\n    train_acc, test_acc))\nscores[len(model.layers)] = (train_acc, test_acc)\n# add layers and evaluate the updated model\nn_layers = 5\nfor _ in range(n_layers):\n    # add layer\n    add_layer_to_autoencoder(model, trainX, testX)\n    # evaluate model\n    train_acc, test_acc = evaluate_autoencoder_as_classifier(model, trainX, trainy, testX,\n      testy)\n    print('> classifier accuracy layers=%d, train=%.3f, test=%.3f' % (len(model.layers), train_acc, test_acc))\n    # store scores for plotting\n    scores[len(model.layers)] = (train_acc, test_acc)","37e41041":"plt.figure(figsize=(8,5))\nplt.plot(list(scores.keys()), [scores[k][0] for k in scores.keys()], label='train', marker='.') \nplt.plot(list(scores.keys()), [scores[k][1] for k in scores.keys()], label='test', marker='.') \nplt.legend()\nplt.show()","2809a4a2":"<h3>3.2.1 Binary CrossEntropy Loss<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">Cross-entropy is the default loss function to use for binary classification problems.<br>\nMathematically, it is the preferred loss function under the inference framework of maximum likelihood. It is the loss function to be evaluated first and only changed if you have a good reason.<\/div>","83cd5f2a":"<h3><center>2. Configure Gradient Precision with Batch Size<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nNeural networks are trained using the stochastic gradient descent optimization algorithm. This involves using the current state of the model to make a prediction, comparing the prediction to the actual values, and using the difference as an estimate of the error gradient. This error gradient is then used to update the model weights and the process is repeated. <br>The error gradient is a statistical estimate. The more training examples used in the estimate, the more accurate this estimate will be and the more likely that the weights of the network will be adjusted in a way that will improve the performance of the model.\n    <ul>\n        <li>Batch Gradient Descent. Batch size is set to the total number of examples in the training dataset.\n            <li>Stochastic Gradient Descent. Batch size is set to one.\n                <li>Minibatch Gradient Descent. Batch size is set to more than one and less than the total number of examples in the training dataset.\n    <\/ul>\n    <\/div>","ff05021c":"<h4>Batch Norm After Activation","aabd0371":"<h3>1.1. Change Model capacity with Nodes(No of Neurons)","007bc572":"<h3>3.3.2. Sparse Multiclass Cross-Entropy Loss<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nA possible cause of frustration when using cross-entropy with classification problems with a large number of labels is the one hot encoding process. For example, predicting words in a vocabulary may have tens or hundreds of thousands of categories, one for each label. This can mean that the target element of each training example may require a one hot encoded vector with tens or hundreds of thousands of zero values, requiring significant memory. Sparse cross-entropy addresses this by performing the same cross-entropy calculation of error, without requiring that the target variable be one hot encoded prior to training.\n    <\/div>\n<br>  \n\n**To avoid One Hot Encoding for saving memory we use SMCEL(NLP problems)**","43541e30":"<h3>2.1. Batch Gradient Descent<\/h3>","a0dbc528":"<h3>Vanishing Gradient Fix","5bcc7d30":"<h3>Momentum Dynamics","670ae73e":"<h4>RNN<\/h4>\n\n![image.png](attachment:image.png)","61b9c44b":"**<blockquote>The plot of loss shows that indeed, the model converged, but the shape of the error surface is not as smooth as other loss functions where small changes to the weights are causing large changes in loss.<\/blockquote>**","20f6f75f":"<h3>3.2.2 Hinge Loss<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nAn alternative to cross-entropy for binary classification problems is the hinge loss function, primarily developed for use with Support Vector Machine (SVM) models. It is intended for use with binary classification where the target values are in the set {-1, 1}. The hinge loss function encourages examples to have the correct sign, assigning more error when there is a difference in the sign between the actual and predicted class values.<br><br>\n    <\/div>\n**<b>Activation function used : tanh<\/b>**","5fea5ec9":"<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<h4 style=\"font-size:1.9vw\">\nA function is required that can add a new hidden layer and retrain the model but only update the weights in the newly added layer and in the output layer. This requires first storing the current output layer including its configuration and current set of weights.<\/h4><\/div>","bcd07048":"<h3><center>3.3. Multiclass Classification Loss Functions<\/center><\/h3>","2bfbf4c9":"<h3>3.1.2. Mean Squared Logarithmic Error Loss <\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThere may be regression problems in which the target value has a spread of values and when predicting a large value, you may not want to punish a model as heavily as mean squared error. Instead, you can first calculate the natural logarithm of each of the predicted values, then calculate the mean squared error. This is called the Mean Squared Logarithmic Error loss<\/div>","37565640":"<h3>3.1.3. Mean Absolute Error Loss <\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nOn some regression problems, the distribution of the target variable may be mostly Gaussian, but may have outliers, e.g. large or small values far from the mean value. The Mean Absolute Error, or MAE, loss is an appropriate loss function in this case as it is more robust to outliers.<\/div>","faf637e1":"<h3>3.3.1. Multiclass Cross-Entropy Loss","6adbe28d":"<h3><center>9. Deeper Models with Greedy Layer-Wise Pretraining<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    \nTraditionally, training deep neural networks with many layers was challenging. As the number of hidden layers is increased, the amount of error information propagated back to earlier layers is dramatically reduced. This means that weights in hidden layers close to the output layer are updated normally, whereas weights in hidden layers close to the input layer are updated minimally or not at all. <br>Generally, this problem prevented the training of very deep neural networks and was referred to as the vanishing gradient problem. An important milestone in the resurgence of neural networks that initially allowed the development of deeper neural network models was the technique of greedy layer-wise pretraining, often simply referred to as pretraining.<br><br>\n    Pretraining involves successively adding a new hidden layer to a model and refitting, allowing the newly added model to learn the inputs from the existing hidden layer, often while keeping the weights for the existing hidden layers fixed. <br><br>\nThere are two main approaches to pretraining; they are :\n    <ul>\n        <li><h4 style=\"font-size:1.9vw\">Supervised greedy layer-wise pretraining :<\/h4>Broadly, supervised pretraining involves successively adding hidden layers to a model trained on a supervised learning task.<\/li>\n        <li><h4 style=\"font-size:1.9vw\">Unsupervised greedy layer-wise pretraining :<\/h4>Unsupervised pretraining involves using the greedy layer-wise process to build up an unsupervised autoencoder model, to which a supervised output layer is later added.Unsupervised pretraining may be appropriate when you have a significantly larger number of unlabeled examples that can be used to initialize a model prior to using a much smaller number of examples to fine tune the model weights for a supervised task.<\/li>\n","2c85db10":"<h3>2.2. Stochastic Gradient Descent<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nStochastic gradient descent requires that the model make a prediction and have the weights updated for each training example. This has the effect of dramatically slowing down the training process as compared to batch gradient descent. The expectation of this change is that the model learns faster (e.g. in terms of the learning curve) and that changes to the model are noisy, resulting, in turn, in noisy performance over training epochs.\n    <\/div>","5456c402":"<h3><center>Unsupervised Greedy Layer-Wise Pretraining<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<ul>\n    <li>We will develop an autoencoder model that will be trained to reconstruct input data.\n<li>We will remove the output layer, add and fit a new output layer for classification.\n<li>We will use the same two-layer base model as we did in the previous section, except modify it to predict the input as the output and use mean squared error to evaluate how good the model is at reconstructing a given input sample.\n    <\/ul>\n    <\/div>","96ed15e0":"<center><h3>Better learning techniques<\/h3><\/center><br><br>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <ol>\n        <li><h4 style=\"font-size:1.9vw\">Configure Capacity :<\/h4>Including including the number of nodes in a layer and the number of layers used to define the scope of functions that can be learned by the model. \udbff\udc00 \n        <li><h4 style=\"font-size:1.9vw\">Configure Batch Size : <\/h4> Including exploring whether variations such as batch, stochastic or minibatch gradient descent are more appropriate.\n         <li><h4 style=\"font-size:1.9vw\">Configure Loss Function :<\/h4> Including understanding the way different loss functions must be interpreted and whether an alternate loss function would be appropriate for your problem.\n        <li><h4 style=\"font-size:1.9vw\">Configure Learning Rate :<\/h4> Including understanding the effect of different learning rates on your problem and whether modern adaptive learning rate methods such as Adam would be appropriate.\n         <li><h4 style=\"font-size:1.9vw\">Data Scaling Techniques :<\/h4> Including the sensitivity that small network weights have to the scale of input variables and the impact of large errors in the target variable have on weight updates.\n        <li><h4 style=\"font-size:1.9vw\">Batch Normalization :<\/h4> Including the sensitivity to changes in the distribution of inputs to layers deep in a network model and the benefits of standardizing layer inputs to add consistency of input and stability to the learning process.\n        <li><h4 style=\"font-size:1.9vw\">Vanishing Gradients :<\/h4> Prevent the training of deep multiple-layered networks causing layers close to the input layer to not have their weights updated; that can be addressed using modern activation functions such as the rectified linear activation function.\n        <li><h4 style=\"font-size:1.9vw\">Exploding Gradients :<\/h4> Large weight updates cause a numerical overflow or underflow making the network weights take on a NaN or Inf value; that can be addressed using gradient scaling or gradient clipping.\n<\/li><br><br><blockquote>\n    The lack of data on some predictive modeling problems can prevent effective learning. Specialized techniques can be used to jump-start the optimization process, providing a useful initial set of weights or even whole models that can be used for feature extraction; for example:<\/blockquote>\n    <li><h4 style=\"font-size:1.9vw\">Greedy Layer-Wise Pretraining :<\/h4> Where layers are added one at a time to a model, learning to interpret the output of prior layers and permitting the development of much deeper models: a milestone technique in the field of deep learning.<\/li>\n    <li><h4 style=\"font-size:1.9vw\">Transfer Learning :<\/h4> Where a model is trained on a different, but somehow related, predictive modeling problem and then used to seed the weights or used wholesale as a feature extraction model to provide input to a model trained on the problem of interest.\n     <\/ol>\n    <\/div>\n         ","8adcdbce":"<h3><center>8. Fix Exploding Gradients with Gradient Clipping<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nLarge updates to weights during training can cause a numerical overflow or underflow often referred to as exploding gradients. The problem of exploding gradients is more common with recurrent neural networks, such as LSTMs given the accumulation of gradients unrolled over hundreds of input time steps. A common and relatively easy solution to the exploding gradients problem is to change the derivative of the error before propagating it backward through the network and using it to update the weights. Two approaches include rescaling the gradients given a chosen vector norm and clipping gradient values that exceed a preferred range. Together, these methods are referred to as gradient clipping.<br>\nThere are two main methods for updating the error derivative; they are:\n    <ul>\n        <li>Gradient Scaling : Gradient scaling involves normalizing the error gradient vector such that vector norm (magnitude) equals a defined value, such as 1.0\n        <li>Gradient Clipping : Gradient clipping involves forcing the gradient values (element-wise) to a specific minimum or maximum value if the gradient exceeded an expected range.\n    <\/ul><\/div>","39586763":"<h4>After Activation Function<\/h4>\n\n![image.png](attachment:image.png)","17172498":"<h3><center>6. Accelerate Learning with Batch Normalization<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nTraining deep neural networks with tens of layers is challenging as they can be sensitive to the initial random weights and configuration of the learning algorithm. One possible reason for this difficulty is the distribution of the inputs to layers deep in the network may change after each minibatch when the weights are updated. This can cause the learning algorithm to forever chase a moving target. This change in the distribution of inputs to layers in the network is referred to by the technical name internal covariate shift. Batch normalization is a technique for training very deep neural networks that standardizes the inputs to a layer for each minibatch. This has the effect of stabilizing the learning process and dramatically reducing the number of training epochs required to train deep networks.<br><br>\n    <h4 style=\"font-size:1.9vw\">Tips for Using Batch Normalization<\/h4>\n    <ul>\n        <li><h4 style=\"font-size:1.9vw\">Probably Use Before the Activation :<\/h4>It may be more appropriate after the activation function for s-shaped functions like the hyperbolic tangent and logistic function.It may be appropriate before the activation function for activations that may result in non-Gaussian distributions like the rectified linear activation function<\/li><blockquote>The goal of Batch Normalization is to achieve a stable distribution of activation values throughout training, and in our experiments we apply it before the nonlinearity since that is where matching the first and second moments is more likely to result in a stable distribution<\/blockquote>\n        <li><h4 style=\"font-size:1.9vw\">Use Large Learning Rates :<\/h4>Using batch normalization makes the network more stable during training. This may require the use of much larger than normal learning rates, that in turn may further speed up the learning process.<\/li>\n        <li><h4 style=\"font-size:1.9vw\">Less Sensitive to Weight Initialization :<\/h4>Deep neural networks can be quite sensitive to the technique used to initialize the weights prior to training. The stability to training brought by batch normalization can make training deep networks less sensitive to the choice of weight initialization method.<\/li>\n        <li><h4 style=\"font-size:1.9vw\">Don\u2019t Use With Dropout :<\/h4>Batch normalization offers some regularization effect, reducing generalization error, perhaps no longer requiring the use of dropout for regularization.\n<\/li>\n    <\/ul>\n<\/div>\n<br><br><h4>Before Activation Function:<\/h4>\n\n![image.png](attachment:image.png)","224409b1":"**<blockquote>\nThe model rapidly learns the problem as compared to batch gradient descent, leaping up to about 80% accuracy in about 25 epochs rather than the 100 epochs seen when using batch gradient descent. We could have stopped training at epoch 50 instead of epoch 200 due to the faster training. This is not surprising. With batch gradient descent, 100 epochs involved 100 estimates of error and 100 weight updates. In stochastic gradient descent, 25 epochs involved (500 \u00d7 25) or 12,500 weight updates, providing more than 10-times more feedback, albeit more noisy feedback, about how to improve the model.<\/blockquote>**","722cec32":"<h3>Vanishing Gradient Problem","34e92579":"<h3>Effect of Learning Rate and Momentum","512345d9":"<h3><center>Supervised greedy layer-wise pretraining :","185552f5":"<h4>Without Batch Norm","a8b4593b":"<h3>4.2. Learning Rate Scehdule<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    Keras supports learning rate schedules via callbacks<br>\n    It is recommended to use the SGD when using a learning rate schedule callback.<br>\n    Keras provides the ReduceLROnPlateau callback that will adjust the learning rate when a plateau in model performance is detected, e.g. no change for a given number of training epochs.","c7c224d0":"<h3><center>7. Vanishing Gradients and ReLU<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\"><h4 style=\"font-size:1.9vw\">Limitations of sigmoid & tanh :<\/h4>\n    A general problem with both the sigmoid and tanh functions is that they saturate. This means that large values snap to 1.0 and small values snap to -1 or 0 for tanh and sigmoid respectively. Further, the functions are only really sensitive to changes around the mid-point of their input, such as 0.5 for sigmoid and 0.0 for tanh. The limited sensitivity and saturation of the function happen regardless of whether the summed activation from the node provided as input contains useful information or not. Once saturated, it becomes challenging for the learning algorithm to continue to adapt the weights to improve the performance of the model.<br><br>\nAs the capability of hardware increased through GPUs, very deep neural networks using sigmoid and tanh activation functions could not easily be trained. Layers deep in large networks using these nonlinear activation functions fail to receive useful gradient information. Error is back propagated through the network and used to update the weights. The amount of error decreases dramatically with each additional layer through which it is propagated, given the derivative of the chosen activation function. This is called the vanishing gradient problem and prevents deep (multilayered) networks from learning effectively.<\/div>","f7447e20":"<h4>CNN<\/h4>\n\n![image.png](attachment:image.png)","9fea87d0":"<h3>Exploding Gradient","e0bf0788":"<h3><center>3.2.  Binary Classification Loss Functions","a8916920":"<h3><center>1. Configure Capacity with Nodes and Layers<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nA model with less capacity may not be able to sufficiently learn the training dataset. A model with more capacity can model more different functions and may be able to learn a function to sufficiently map inputs to outputs in the training dataset. Whereas a model with too much capacity may memorize the training dataset and fail to generalize or get lost or stuck in the search for a suitable mapping function.<\/div>","9ad2e446":"<h3>3.1.1 Mean Squared Error Loss","5a0a6dc4":"<h3>Normalizing","e1f0c351":"<h3>4.1. Stochastic Gradient Descent :","b95c0bf0":"<h3>1.2. Change Model capacity with Layers","474ea644":"<h3>3.3.3. Kullback Leibler Divergence Loss<\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    The KL divergence loss function is more commonly used when using models that learn to approximate a more complex function than simply multiclass classification, such as in the case of an autoencoder used for learning a dense feature representation under a model that must reconstruct the original input.","1c85de9e":"<h3><center>4. Configure Speed of Learning with Learning Rate<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThe amount that the weights are updated during training is referred to as the step size or the learning rate.<br><br>\n    <ul>\n      <li><h4 style=\"font-size:1.9vw\">Momentum :<\/h4>Training a neural network can be made easier with the addition of history to the weight update. Specifically, an exponentially weighted average of the prior updates to the weight can be included when the weights are updated. This change to stochastic gradient descent is called momentum and adds inertia to the update procedure, causing many past updates in one direction to continue in that direction in the future. <\/li>\n        <li><h4 style=\"font-size:1.9vw\">Learning Rate Schedule :<\/h4>\nlearning rate schedule is to decrease the learning rate linearly from a large initial value to a small value. This allows large weight changes in the beginning of the learning process and small changes or fine-tuning towards the end of the learning process.<\/li>\n        <li><h4 style=\"font-size:1.9vw\">Adaptive Learning Rate :<\/h4>\nThe performance of the model on the training dataset can be monitored by the learning algorithm and the learning rate can be adjusted in response.\n        Perhaps the simplest implementation is to make the learning rate smaller once the performance of the model plateaus, such as by decreasing the learning rate by a factor of two or an order of magnitude.\n        Alternately, the learning rate can be increased again if performance does not improve for a fixed number of training epochs.There are three adaptive learning rate methods that have proven to be robust over many types of neural network architectures and problem types. They are AdaGrad, RMSProp, and Adam, and all maintain and adapt learning rates for each of the weights in the model.<\/li>\n        <\/ul><\/div>","c0338771":"<h3>3.2.3 Squared Hinge Loss","08ba31ca":"<h3><center>3. Configure What to Optimize with Loss Functions<\/center><\/h3>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n    <ul>\n        <li>Neural networks are trained using an optimization process that requires a loss function to calculate the model error.\n        <li>Maximum Likelihood provides a framework for choosing a loss function when training neural networks and machine learning models in general.\n            <li>Cross-entropy and mean squared error are the two main types of loss functions to use when training neural network models.\n    <\/ul>\n    <ol>\n        <li>Mean Squared Error Loss(Linear) : <br>Mean Squared Error loss, or MSE for short, is calculated as the average of the squared differences between the predicted and actual values. The result is always positive regardless of the sign of the predicted and actual values and a perfect value is 0.0.<br> It is the default loss to use for regression problems. Mathematically, it is the preferred loss function under the inference framework of maximum likelihood if the distribution of the target variable is Gaussian.\n            <li>Cross-Entropy Loss (or Log Loss) :<br>Cross-entropy loss is often simply referred to as cross-entropy, logarithmic loss, logistic loss, or log loss for short. Each predicted probability is compared to the actual class output value (0 or 1) and a score is calculated that penalizes the probability based on the distance from the expected value. The penalty is logarithmic, offering a small score for small differences (0.1 or 0.2) and enormous score for a large difference (0.9 or 1.0).\n    <\/ol>\n    <\/div>","f2b6b138":"<h2><center>Framework for Systematically Better Deep Learning<\/center><\/h2>\n\n![image.png](attachment:image.png)\n\n<div style=\"font-family:verdana; word-spacing:1.5px;\">\nThere are three types of problems that are straightforward to diagnose with regard to poor performance of a deep learning neural network model; they are:<br>\n    <ul>\n        <li>Problems with Learning : Problems with learning manifest in a model that cannot effectively learn a training dataset or shows slow progress or bad performance when learning the training dataset.\n        <li>Problems with Generalization : Problems with generalization manifest in a model that overfits the training dataset and makes poor predictions on a holdout dataset.\n        <li>Problems with Predictions : Problems with predictions manifest in the stochastic training algorithm having a strong influence on the final model, causing high variance in behavior and performance.\n    <\/ul>\n    <blockquote><span style=\"color:#159364;\">We will be dealing with Problems with Learning in this notebook. Better learning techniques are those changes to a neural network model or learning algorithm that improve or accelerate the adaptation of the model weights in response to a training dataset.\n        <\/span> <\/blockquote>\n<\/div>","87115fca":"We can see that the addition of momentum does accelerate the training of the model. Specifically, momentum values of 0.9 and 0.99 achieve reasonable train and test accuracy within about 50 training epochs as opposed to 200 training epochs when momentum is not used. In all cases where momentum is used, the accuracy of the model on the holdout test dataset appears to be more stable, showing less volatility over the training epochs.","e1fe9633":"**<blockquote>The plot shows that the training process converged well. The plot for loss is smooth, given the continuous nature of the error between the probability distributions, whereas the line plot for accuracy shows bumps, given examples in the train and test set can ultimately only be predicted as correct or incorrect, providing less granular feedback on performance.<\/blockquote>**","04a5446e":"<h3>Drop Learning Rate on Plateau","14f7ce0b":"<h3>2.2. Mini Batch Gradient Descent","6924a825":"<h3>Clipping","d74e266e":"**<blockquote>The plots show that small batch results generally in rapid learning but a volatile learning process with higher variance in the classification accuracy. Larger batch sizes slow down the learning process (in terms of the learning curves) but the final stages result in a convergence to a more stable model exemplified by lower variance in classification accuracy.\n    <\/blockquote>**","f047973b":"<h4 style=\"font-size:1.9vw\">Gradient Norm Scaling<\/h4>\n<div style=\"font-family:verdana; word-spacing:1.5px;\">Gradient norm scaling involves changing the derivatives of the loss function to have a given vector norm when the L2 vector norm (sum of the squared values) of the gradient vector exceeds a threshold value. For example, we could specify a norm of 1.0, meaning that if the vector norm for a gradient exceeds 1.0, then the values in the vector will be rescaled so that the norm of the vector equals 1.0. This can be used in Keras by specifying the clipnorm argument on the optimizer; for example:<br>\n    <blockquote>SGD(lr=0.01, momentum=0.9, clipnorm=1.0)<\/blockquote><br>\n<\/div>  \n<div style=\"font-family:verdana; word-spacing:1.5px;\">\n<h4 style=\"font-size:1.9vw\">Gradient Value Clipping<\/h4>\nGradient value clipping involves clipping the derivatives of the loss function to have a given value if a gradient value is less than a negative threshold or more than the positive threshold. For example, we could specify a norm of 0.5, meaning that if a gradient value was less than -0.5, it is set to -0.5 and if it is more than 0.5, then it will be set to 0.5. This can be used in Keras by specifying the clipvalue argument on the optimizer, for example:<br>\n    <blockquote>SGD(lr=0.01, momentum=0.9, clipvalue=0.5)<\/blockquote>","2bd55317":"**<blockquote>The plot shows the unstable nature of the training process with the chosen configuration. The poor performance and erratic changes to the model suggest that the learning rate used to update weights after each training example may be too large and that a smaller learning rate may make the learning process more stable.<\/blockquote>**","977099fe":"<blockquote>We can see that the model has learned the problem faster than the model in the previous section without batch normalization. Specifically, we can see that classification accuracy on the train and test datasets leaps above 80% within the first 20 epochs, as opposed to 30-to-40 epochs in the model without batch normalization. <\/blockquote>","93b3b5ac":"**<blockquote>The plots show oscillations in behavior for the too-large learning rate of 1.0 and the inability of the model to learn anything with the too-small learning rates of 1E-6 and 1E-7. We can see that the model was able to learn the problem well with the learning rates 1E-1, 1E-2 and 1E-3, although successively slower as the learning rate was decreased. With the chosen model configuration, the results suggest a moderate learning rate of 0.1 results in good model performance on the train and test sets.<\/blockquote>**","8493d03c":"<h3><center>3.1. Regression Loss Functions"}}