{"cell_type":{"db34ba1f":"code","a857eb29":"code","95fa83d6":"code","421fbe44":"code","0ce95834":"code","7d746677":"code","bd8cb579":"code","5087e12d":"code","0b7375fb":"code","2b7e4c72":"code","71fe5e22":"code","690c82e7":"code","28b9a1ee":"code","9db1ba53":"code","3b7d5cdd":"code","8faaa529":"code","b7ceebed":"code","7eeacf6d":"code","543306c4":"code","69178ac7":"code","92b4aa01":"code","6f55c6e5":"code","550eb7ad":"code","d85d0556":"code","763cde2f":"code","fa456d46":"code","0df67a53":"code","5eac9b90":"code","4f33130c":"code","6d860da0":"code","82b99818":"code","8748dd60":"code","b0757561":"code","abaff167":"code","535c1c68":"markdown","65dc20ec":"markdown","d02cb315":"markdown","6298686b":"markdown","5312af7d":"markdown","b4c3d76d":"markdown","7d146795":"markdown","2443a577":"markdown","85cbe971":"markdown","2f977c7a":"markdown","7413dbc3":"markdown","4d770d41":"markdown","b97ccb2f":"markdown","b31e3456":"markdown","172613c7":"markdown","1c13acfd":"markdown","67b67007":"markdown","2eab8ce1":"markdown","04694738":"markdown","5871dac2":"markdown","48b305b5":"markdown","62fd147a":"markdown","c4a1bddb":"markdown","8a31d0a8":"markdown","52876447":"markdown","1cbe695c":"markdown","f194bfcb":"markdown","ce3de32b":"markdown","684af074":"markdown","8a42205a":"markdown","939dced3":"markdown","cd37477d":"markdown","bbe7bb37":"markdown","70ee5a5d":"markdown","209cdb4f":"markdown","e05143f9":"markdown","b2f3f397":"markdown"},"source":{"db34ba1f":"# Array operations and useful analysis functionalities\nimport numpy as np\nimport pandas as pd\n\n# Seaborn library for visualizations in the notebook\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(\"ticks\")\n%matplotlib inline\n\n\n# Interactive widgets for user-selection\n# This functionality can't be executed in every notebook viewer!\nimport ipywidgets as widgets\nfrom ipywidgets import interact, interact_manual\n\n# Scikit-learn for model building\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression","a857eb29":"df_goods = pd.read_excel('\/kaggle\/input\/sales-and-workload-data-from-retail-industry\/salesworkload.xlsx',            # file name\n                         sheet_name = 0,                                                                             # index of sheet in file\n                         header = 1,                                                                                 # row index of column names\n                         na_values = ['#NV', 'Err:520'])                                                             # special treatment of missing-\/error-values\ndf_hours = pd.read_excel('\/kaggle\/input\/sales-and-workload-data-from-retail-industry\/salesworkload.xlsx',\n                         sheet_name = 1,\n                         header = 5,\n                         na_values = ['#NV', 'Err:520'])","95fa83d6":"print(\"Goods:\")\nprint(df_goods.dtypes)\nprint(\" \")\nprint(\"Hours:\")\nprint(df_hours.dtypes)","421fbe44":"goods_notnum = df_goods[['HoursOwn']].applymap(np.isreal).values\ndf_goods[~goods_notnum][['Dept_ID', 'HoursOwn']]","0ce95834":"# Calculate the mean of HoursOwn for the specified department and without the cells containing a question mark\nimp_dept6 = df_goods.loc[(df_goods['Dept_ID'] == 6.0) & (df_goods['HoursOwn'] != '?')]['HoursOwn'].mean()\nimp_dept2 = df_goods.loc[(df_goods['Dept_ID'] == 2.0) & (df_goods['HoursOwn'] != '?')]['HoursOwn'].mean()\n\n# Replace the question marks\ndf_goods.loc[(df_goods['Dept_ID'] == 6.0) & (df_goods['HoursOwn'] == '?'), ['HoursOwn']] = imp_dept6\ndf_goods.loc[(df_goods['Dept_ID'] == 2.0) & (df_goods['HoursOwn'] == '?'), ['HoursOwn']] = imp_dept2\n\n# Give an information about the new values\nprint('Department 6: ' + str(imp_dept6))\nprint('Department 2: ' + str(imp_dept2))","7d746677":"df_goods['HoursOwn'] = pd.to_numeric(df_goods['HoursOwn'])   # convert column now to numeric\ndf_goods.dtypes                                              # check type of column","bd8cb579":"df_goods.describe(include = 'all')","5087e12d":"df_goods[df_goods['Area (m2)'].isnull()]","0b7375fb":"df_goods = df_goods[df_goods['Area (m2)'].notnull()]   # drop all above shown rows with no\/incomplete content\ndf_goods = df_goods.drop(['Customer'], axis = 1)       # drop \"Customer\" column, due to no values","2b7e4c72":"# Subset the DataFrame as of May 2017\ndf_area_buffer = df_goods[df_goods['MonthYear'] == '05.2017']\n# Extracting row indices of the subset\nbuffer_row_indices = df_goods[df_goods['MonthYear'] == '05.2017'].index\n\n# Creating new DataFrame based on the indices\ndf_area = df_goods.loc[buffer_row_indices, :]\n\n# Adding new column with identifier for store and department\ndf_area['ID'] = df_area['StoreID'].map(str) + df_area['Dept_ID'].map(str)\ndf_area = df_area[['ID', 'Area (m2)']]","71fe5e22":"df_goods['ID'] = df_goods['StoreID'].map(str) + df_goods['Dept_ID'].map(str)\n\ndf_cum = df_goods[['ID',\n                   'HoursOwn',\n                   'Sales units',\n                   'Turnover']].groupby(['ID'], as_index = False).sum()\n\ndf_cum = pd.merge(df_cum, df_area,\n                  on = 'ID',\n                  how = 'left')\n\ndf_cum = pd.merge(df_cum, df_goods[['ID', 'StoreID', 'Dept_ID', 'Dept. Name']],\n                  on = 'ID',\n                  how = 'left').drop_duplicates()","690c82e7":"df_hours = df_hours.drop(df_hours.columns[1:24], axis = 1)\ndf_hours = df_hours.drop(df_hours.columns[2:9], axis = 1)","28b9a1ee":"df_cum = df_cum.rename(columns = {'Dept_ID': 'DeptID',\n                                  'Dept. Name': 'DeptName',\n                                  'Sales units': 'SalesUnits',\n                                  'Area (m2)': 'Area'})","9db1ba53":"df_hours = df_hours.rename(columns = {'id': 'StoreID',\n                                      '5.1': 'CumulativeHours'})","3b7d5cdd":"df = pd.merge(df_cum, df_hours, on = 'StoreID', how = 'left')    # Joining both tables\ndf['HoursRatio'] = df['HoursOwn'] \/ df['CumulativeHours']        # Create new ratio feature\ndf.head(3)                                                       # Check first data objects","8faaa529":"df['ID'] = df['ID'].astype('category')\ndf['StoreID'] = df['StoreID'].astype('int').astype('category')\ndf['DeptID'] = df['DeptID'].astype('int').astype('category')","b7ceebed":"df.loc[:, ['DeptID', 'DeptName']].drop_duplicates().sort_values(by = ['DeptID'])","7eeacf6d":"g_hoursratio = sns.FacetGrid(df,\n                             col = 'DeptName',\n                             col_wrap = 4,\n                             sharex = False, sharey = False);\ng_hoursratio.map(sns.distplot, 'HoursRatio', color = '#012363');","543306c4":"df['DeptID'] = df['DeptID'].cat.remove_categories([3])\ndf.dropna(inplace = True)","69178ac7":"g_salesunits = sns.FacetGrid(df,\n                             col = 'DeptName',\n                             col_wrap = 4,\n                             sharex = False, sharey = False);\ng_salesunits.map(sns.distplot, 'SalesUnits', color = '#e3b900');","92b4aa01":"df['DeptID'] = df['DeptID'].cat.remove_categories([12, 15, 16, 17, 18])\ndf.dropna(inplace = True)","6f55c6e5":"g_turnover = sns.FacetGrid(df,\n                  col = 'DeptName',\n                  col_wrap = 4,\n                  sharex = False, sharey = False);\ng_turnover.map(sns.distplot, 'Turnover', color = '#dd3e21');","550eb7ad":"%%capture\nnp.seterr(divide = 'ignore', invalid = 'ignore')\n# The following FacetGrid throws an error (numpy: Invalid value\n# encountered in true_divide), which has no effect. According to\n# https:\/\/github.com\/belltailjp\/selective_search_py\/issues\/20\n# the error is suppressed by this numpy command.","d85d0556":"g_area = sns.FacetGrid(df,\n                       col = 'DeptName',\n                       col_wrap = 4,\n                       sharex = False, sharey = False);\ng_area.map(sns.distplot, 'Area', color = '#01673b');","763cde2f":"df['DeptID'] = df['DeptID'].cat.remove_categories([11])\ndf.dropna(inplace = True)\n\n# Get a sorted list of department IDs in scope for later use\nfinal_dept_list = np.sort(df.loc[:, ['DeptName']]\n                          .drop_duplicates()\n                          .to_numpy()\n                          .ravel())","fa456d46":"cpal = ['#012363', '#e3b900', '#dd3e21', '#01673b', '#a6acaa', '#b01297', '#7EB00B', '#17E8DB', '#332506', '#701B0C']\npp = sns.pairplot(df,\n                  hue = 'DeptName',\n                  vars = ['HoursRatio',\n                          'SalesUnits',\n                          'Turnover',\n                          'Area'],\n                  palette = cpal)\npp.map_upper(sns.scatterplot)\npp.map_lower(sns.kdeplot)\npp.map_diag(sns.distplot)\npp._legend.remove()\nhandles = pp._legend_data.values()\nlabels = pp._legend_data.keys()\npp.fig.legend(handles = handles, labels = labels, loc = 'upper center', ncol = 5, frameon = False)\npp.fig.subplots_adjust(top = 0.9, bottom = 0.1)","0df67a53":"df_hours = df.loc[:, ['StoreID', 'DeptID', 'HoursRatio']]\ndf_hours = df_hours.pivot(index = 'StoreID',\n                          columns = 'DeptID',\n                          values = 'HoursRatio')","5eac9b90":"df_sales = df.loc[:, ['StoreID', 'DeptID', 'SalesUnits']]\ndf_sales = df_sales.pivot(index = 'StoreID',\n                          columns = 'DeptID',\n                          values = 'SalesUnits')","4f33130c":"df_area = df.loc[:, ['StoreID', 'DeptID', 'Area']]\ndf_area = df_area.pivot(index = 'StoreID',\n                        columns = 'DeptID',\n                        values = 'Area')","6d860da0":"cor_ha = df_hours.corrwith(df_area, axis = 0)\ncor_as = df_area.corrwith(df_sales, axis = 0)\ncor_sh = df_sales.corrwith(df_hours, axis = 0)\n\ncor = pd.concat([cor_ha, cor_as, cor_sh], axis = 1)\ncor.columns = ['Hours-Area', 'Area-Sales', 'Sales-Hours']\ncor = (cor.reset_index(drop=True)\n       .unstack()\n       .reset_index()\n       .rename(columns={'level_0': 'Relation', 'level_1': 'DeptID', 0: 'PCC'}))","82b99818":"sns.set_context(\"notebook\", font_scale = 1.5)\nfig_boxplot = sns.boxplot(data = cor,\n                          x = 'PCC', y = 'Relation',\n                          width = 0.6,\n                          palette = ['#99a7c0', '#f3e399', '#99c2b0'],\n                          orient = 'h')\nfig_boxplot.set_ylabel('')\nsns.despine()\nplt.xlabel(r\"$\\rho$\")","8748dd60":"sns.reset_orig()\ng = sns.FacetGrid(df,\n                  col = 'DeptName',\n                  col_wrap = 3,\n                  sharex = False, sharey = False);\ng.map(sns.scatterplot, 'HoursRatio', 'SalesUnits', alpha = .8, color = '#012363')\ng.add_legend()","b0757561":"def plot_lm(deptname):\n    ax = sns.lmplot(data = df[df['DeptName'] == deptname],\n                    x = 'HoursRatio',\n                    y = 'SalesUnits')\n\n@interact\ndef show_plot_lm(DeptName = final_dept_list):\n    return plot_lm(DeptName)","abaff167":"@interact\ndef plot_model(deptname = final_dept_list):\n    # Creating training and test set\n    X_all = df[df['DeptName'] == deptname][['HoursRatio']].values\n    y_all = df[df['DeptName'] == deptname][['SalesUnits']].values\/1000000          # for smaller axis tick labels\n    X_train_all, X_test_all, y_train_all, y_test_all = train_test_split(X_all, y_all,\n                                                                        test_size = 0.25,\n                                                                        random_state = 42)\n    \n    # Initiating and fitting model\n    lrm = LinearRegression()\n    lrm.fit(X_train_all, y_train_all)\n    \n    # Parameters\n    a = lrm.coef_[0]\n    b = lrm.intercept_\n    r2_training = lrm.score(X_train_all, y_train_all)\n    r2_test = lrm.score(X_test_all, y_test_all)\n    \n    # Printing function\n    print('Function: f(w) = %.3f * w + %.3f' % (a, b))\n    print(\"Intercept: {}\".format(a))\n    print(\"Slope: {}\".format(b))\n    print('Training set R\u00b2 score: {:.2f}'.format(r2_training))\n    print('Test set R\u00b2 score: {:.2f}'.format(r2_test))\n    \n    # Plotting\n    df_concat_train = pd.DataFrame(np.hstack((X_train_all, y_train_all)))\n    df_concat_train['Set'] = 'Train'\n    df_concat_test = pd.DataFrame(np.hstack((X_test_all, y_test_all)))\n    df_concat_test['Set'] = 'Test'\n    df_concat_pred = pd.DataFrame(np.hstack((X_train_all, lrm.predict(X_train_all))))\n    \n    df_concat = pd.concat([df_concat_train, df_concat_test])\n    \n    df_concat.columns = ['X', 'y', 'Set']\n    df_concat_pred.columns = ['X', 'y']\n    \n    ax = sns.scatterplot(data = df_concat,\n                     x = 'X',\n                     y = 'y',\n                     hue = 'Set',\n                     palette = ['#012363', '#e3b900'])\n    \n    sns.lineplot(data = df_concat_pred,\n             x = 'X',\n             y = 'y',\n             color = '#dd3e21',\n             ax = ax)\n    \n    sns.despine()\n    plt.xlabel(r'Ratio of working hours $w$')\n    plt.ylabel(r'Sales units $s$ [1e6]')\n    \n    return ax","535c1c68":"Department 3, which is *Other*, shows a very narrow distribution (see x-axis). After a view into the data set it becomes clear why: The recorded hours are all over the same. Only the cumulative hours as divisor provide the small variance in the feature \"HoursRatio\". This case must be passed back to the business, since it is highly doubtful that the values for department 3 are correct. Therefore, it cannot be considered in the further analysis also.","65dc20ec":"In this pairplot different findings can be seen:\n* Especially the distributions of sales units and turnover are dominated by a few departments with very small quantities.\n* The departments appear as (sometimes) overlapping but clearly defined clusters or areas in the relationships between the individual features.\n* The relationship between sales units and hours ratio (second column, first row) has clusters of departments that are somewhat ellipsoidal, i.e. linear. This suggests a further investigation using linear regression.","d02cb315":"Departments 15 (*Admin*), 16 (*Customer Services*) and 17 (*Others*) are not part of the original sales areas. For this reason, their distribution in goods sold  - which is quite the same - is not very meaningful. They are therefore removed from the data set.\n\nDepartment 12 is the *Checkout*, which plays a special role just like department 18, *All*. Here, all units are summed up, which is very similar to the *Food* department 13 - sum of all departments dealing with food items - due to the highest sales figures by far. In the following, therefore, only department 13 will be considered and 12 and 18 removed.","6298686b":"### Introduction<a class=\"anchor\" id=\"introduction\"><\/a>\n\nRaw data of real analytical use cases in a number of industries and companies is frequently provided in an Excel-based form. These files usually cannot be processed directly in machine learning models, but must first be cleaned and preprocessed. In this procedure, many different types of pitfalls may occur. This makes data preprocessing an essential time factor in the daily work of a data scientist.\n\nIn this concise project an Excel spreadsheet will be presented which in this form is closely oriented to a real case but contains only simulated figures for reasons of data and business results protection. The form and structure of the file correspond to a real case and could be encountered by a data scientist in a company in this way. Such a file can be the result of a download from a financial controlling system, e.g. SAP.\n\nThe data includes information about sold goods resp. product units, the associated turnover and hours worked. This information is grouped by month, store and department of the retailer. Moreover, information about the sales area in a specific department as well as about the opening hours of the store is provided.\n\n*You can find this notebook and the associated data also in a GitHub repository and my portfolio via this [link](https:\/\/dgluesen.github.io\/) in my profile. [Here](https:\/\/github.com\/dgluesen\/cheat-sheets) you can also find a small cheat sheet for a data preprocessing workflow.*","5312af7d":"#### Removal of redundant or implausible departments\n\nFirst it must be determined whether it is possible to work with all features in a meaningful way. For this we consider the distributions in the numerical variables across the departments.\n\n##### Feature \"HoursRatio\"","b4c3d76d":"Department 11 is *Delivery*. This makes it comprehensible that no sales area is required here and that the distribution is therefore centered on 0. The department is finally removed from the data set.","7d146795":"We see two things now:\n* there are in total eight rows in which \"MonthYear\" consists of four dashes, but all columns do not have any value \n* the month of June 2017 does not contain any information about the sales area\n\nThus, we have already discovered the answers for the first two issues mentioned above. The eight additional lines (MonthYear vs. Time index) are like visual separations between the months and contain no relevant data. In addition, in order to remain consistent, we can only perform an analysis until May 2017 because the area is not available in its entirety after this month. If we were to look at different months in terms of quantity of goods and sales area, we would not be able to guarantee the correctness of the derived conclusions. True, using the values of May for a cumulative consideration can also be questioned. The ranges can vary in size over the months. Nevertheless, we stick to this simplification because it is the latest version available.","2443a577":"#### Cumulation of monthly figures\n\nIn the following, we sum up from the beginning of the data series to May 2017. It is important to keep the information about the respective market and department, as this is the elementary unit of consideration.\n\nFirst, we are extracting the departmental sales areas as of May 2017. To avoid a \"SettingWithCopyWarning\" while generating a new identifier for store and department combined, a detour via a buffer is necessary (see https:\/\/bit.ly\/2LdPdjo).","85cbe971":"#### Distribution of correlation coefficients\n\nAt this point, we would like to take a look at the correlation coefficients between the numerical attributes, excluding turnover because of its proximity to sales units.\n\nThe pivot function transposes the department ID information to the columns and fills the cells with the different dimensions HoursRatio, SalesUnits, and Area.\n\nThe correlation can be calculated with pandas-internal function \"corrwith\". This is done three times to end up with all possible combinations. Finally, the vectors with the coefficients are concatenated in order to get a single resulting dataframe.","2f977c7a":"#### Pairwise relationship\n\nThe next step is to look at the mutual relationships between the numerical features grouped by department.","7413dbc3":"### Libraries and settings<a class=\"anchor\" id=\"librariessettings\"><\/a>\nThe first step consists of loading the necessary libraries. In addition, a few settings are made for this notebook.","4d770d41":"### Correlations between features<a class=\"anchor\" id=\"correlationsfeatures\"><\/a>\n\nThe relationships between the variables will now be examined in more detail. Since individual departments also have to be managed completely differently and therefore have their own underlying mechanisms, the following considerations are divided according to departments. The following departments exist in the data set.","b97ccb2f":"##### Feature \"SalesUnits\"","b31e3456":"#### Replacement\n\nWe don't want to ignore these question marks or even delete the cells. Rather, we would like to make a meaningful replacement\/imputation here and insert the respective mean value of the department for \"HoursOwn\". Afterwards, the type of the column is changed to numeric.","172613c7":"### Data allocation and preparation<a class=\"anchor\" id=\"allocationpreparation\"><\/a>\nThe Excel file \"salesworkload.xlsx\" is read with pandas. The two sheets are loaded to different DataFrames. The first sheet contains the information concerning the sold goods supplemented by store and working hours. The other sheet covers detailed information about the opening hours in a specific market, which depends on opening scheme model and the location of the store due to different holiday schedules.\n\nAfter completion of the pre-processing, we would like to have a unified, cleansed data set available with which we can continue to work. In order to avoid seasonal effects, we also use a cumulative view over the months. This is reasonable and requested for a first analysis and to maintain a certain degree of simplicity from a business point of view.","1c13acfd":"#### Handling implausible data types\n\nHowever, the data types of the individual columns should still be checked.","67b67007":"It can be seen that some of the departments such as *Frozen*, *Meat* or even general *Food* have a clearly defined linear structure. Others are a bit more indifferent, such as departments *Dry* and *Household*.\n\nIn the following, a linear line can be plotted for a selected department.","2eab8ce1":"#### Column names\nThe current column names are not ideal for further data processing. Above all, you want to prevent later problems with character encoding or inconsistent names. The columns are therefore renamed.","04694738":"### Table of contents\n* [Introduction](#introduction)\n* [Objectives](#objectives)\n* [Libraries and settings](#librariessettings)\n* [Data allocation and preparation](#allocationpreparation)\n* [Correlations between features](#correlationsfeatures)\n* [Linear regression of hours ratio vs. sales units](#linearregression)\n* [Predictive model](#predictivemodel)","5871dac2":"#### Merging and hours ratio\nThe two individual data sets are merged into one, so that the cumulated number of opening hours is also available in the main table and we only have to work with one flat DataFrame. In addition, we do not want to consider the absolute number of hours worked, because the total amount of hours where the store is opened differs. Rather, we are interested in the ratio of hours worked in a department to the total opening hours of the store. This reflects the effort made in a market\/department even better. For this purpose, an additional column \"HoursRatio\" is calculated and included.","48b305b5":"### Linear regression of hours ratio vs. sales units<a class=\"anchor\" id=\"linearregression\"><\/a>\n\nFirst, we consider the dependence of the sales units on the invested working time. As we have seen before, this only makes sense if it is done per department. This leads us to a regression analysis of the goods sold depending on the hours worked.\n\nWe investigate this relation separated by departments and check whether we get a long dot cloud here, which would support a linear dependency.","62fd147a":"There are no noticeable anomalies with regard to turnover.\n\n##### Feature \"Area\"","c4a1bddb":"In this case, it has already been clarified through advance communication with the business unit that there is missing information in the data. For this reason, the values \"#NV\" and \"Err:520\" could already be intercepted here. Collecting as much information as possible about the data before the analysis saves time during the later processing.","8a31d0a8":"#### Loading the data","52876447":"The following boxplot shows the distribution of correlation coefficients across all departments and stores in the different combinations of the mentioned features.","1cbe695c":"### Objectives<a class=\"anchor\" id=\"objectives\"><\/a>\n\nThe following goals of data cleansing are addressed in this notebook:\n* Import an Excel-file\n* Inspect the dataset\n* Check data types and do meaningful modifications\n* Handle missings\/data gaps\n* Find and solve data inconsistencies\n* Rename columns for improved usage\n* Join tables to a single one\n\nFurthermore, the data is investigated at a very high level for illustrative purposes. Here, correlations between different features are examined and a simple regression model is finally created.","f194bfcb":"### Predictive model<a class=\"anchor\" id=\"predictivemodel\"><\/a>\n\nFor the remaining departments in the dataset a linear regression is modeled as a predictive algorithm in the following.\n\nThe model uses the linear regression model of the scikit-learn library (see https:\/\/bit.ly\/2ZzR3UY). The ratio of the test set is 25% and R^2 is used as evaluation score (see https:\/\/bit.ly\/30N9Vxc).","ce3de32b":"In this way we were able to identify that a question mark (\"?\") is placed in two cells of the column \"HoursOwn\", for whatsoever reason. Experience from business has shown that in manually processed Excel files errors - similar to this example - can occur quite easily.","684af074":"Because we had already seen in the table with sold goods that we only want to look through to May. So, for the second table with the opening hours, we can remove all the other columns except the one representing the cumulative figures until May 2017 which is the column \"5.1\". Beside this the ID is of interest only.","8a42205a":"Now the months are cumulated and a new DataFrame is created.","939dced3":"# Wrangling and cleansing business data\n**Approach and pitfalls of data pre-processing based on a real-world recreated retail industry example**","cd37477d":"#### Categorical IDs\n\nFinally, the IDs are converted to categorical features.","bbe7bb37":"#### Missings\n\nIn order to obtain a general overview of the values stored in the data set, some statistical values are retrieved in an overview.","70ee5a5d":"Here, it is noticeable that:\n* in column \"MonthYear\" there are eight records more than time indices  \n* only 6,800 values are stored for the \"Area (m2)\" feature \n* the column \"Customer\" doesn't include anything\n\nTo begin, let's take the second point and look for the rows where \"Area (m2)\" has no value, i.e. *NaN*.","209cdb4f":"##### Feature \"Turnover\"","e05143f9":"Most of the column types have been recognized correctly. The IDs must be converted to a categorical feature without a decimal point. This is done later after the cleansing.\n\nThe column \"HoursOwn\" stands out, because the type is specified here as *object*. However, one would expect a numeric type. For this reason we search for non-numeric records in the column \"HoursOwn\", which is otherwise obviously populated with numbers.","b2f3f397":"It can be seen that the correlation between goods sold and working hours ratio stands out."}}