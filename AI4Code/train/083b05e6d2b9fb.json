{"cell_type":{"416003c0":"code","fb5c8c19":"code","46d6e61b":"code","af3b311e":"code","14753556":"code","a36f2b7f":"code","175411af":"code","f8057b81":"code","9213f0d5":"code","4dd3504e":"code","91f9fa6c":"code","48cd2a5b":"code","7c3736f7":"code","e837c58a":"code","24bdc292":"code","ea7715aa":"code","19773eac":"code","322ef1b1":"code","79bd9558":"code","de2fb3c6":"code","2c263af6":"code","2807a4ae":"markdown","0f77db97":"markdown","4a5f4306":"markdown","700c520d":"markdown","e0fe6ced":"markdown","e9da6f3c":"markdown"},"source":{"416003c0":"#copy and edit from:\n#https:\/\/www.kaggle.com\/davidcoxon\/first-look-at-october-data\n#https:\/\/www.kaggle.com\/mohammadkashifunique\/tsp-single-xgboost-model\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_curve, auc, RocCurveDisplay\n","fb5c8c19":"%%time\ntrain_1 = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/train.csv',index_col='id')\ntest_1 = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/test.csv',index_col='id')\nprint('import data')","46d6e61b":"print(f'train data shape:{train_1.shape},test data shape:{test_1.shape}')","af3b311e":"print(f'train data type:{train_1.dtypes.unique()},test data type{test_1.dtypes.unique()}')","14753556":"def reduce_mem_usage(df, verbose=True):\n    # form byte to MB\n    start_mem = df.memory_usage().sum() \/ 1024 **2\n    \n    for col in df.columns:\n        col_dtype = df[col].dtypes\n        \n        #check type's upperbound and lowerbound\n        col_min = df[col].min()\n        col_max = df[col].max()\n        \n        #try to change type so we can save space\n        if str(col_dtype) == 'int64':\n            if col_min > np.iinfo(np.int8).min and col_max < np.iinfo(np.int8).max:\n                df[col] = df[col].astype(np.int8)\n            elif col_min > np.iinfo(np.int16).min and col_max < np.iinfo(np.int16).max:\n                df[col] = df[col].astype(np.int16)\n            elif col_min > np.iinfo(np.int32).min and col_max < np.iinfo(np.int32).max:\n                df[col] = df[col].astype(np.int32)\n            elif col_min > np.iinfo(np.int64).min and col_max < np.iinfo(np.int64).max:\n                df[col] = df[col].astype(np.int64)\n        else:\n            if col_min > np.finfo(np.float16).min and col_max < np.finfo(np.float16).max:\n                df[col] = df[col].astype(np.float16)\n            elif col_min > np.finfo(np.float32).min and col_max < np.finfo(np.float32).max:\n                df[col] = df[col].astype(np.float32)\n            else:\n                df[col] = df[col].astype(np.float64)\n    end_mem = df.memory_usage().sum() \/ 1024 ** 2\n    if verbose:\n        print(f'Memory usage decreased to {end_mem:.2f} MB{(start_mem - end_mem) \/ start_mem : .2f}% reduction')\n    return df","a36f2b7f":"%%time\n\ntrain_1 = reduce_mem_usage(train_1,verbose=True)\ntest_1 = reduce_mem_usage(test_1,verbose=True)\n\nprint(f'train dtypes are:{train_1.dtypes.unique()},test dtypes are:{test_1.dtypes.unique()}')","175411af":"train_1.head()","f8057b81":"test_1.head()","9213f0d5":"train_missing = train_1.isnull().values.sum()\ntest_missing = test_1.isnull().values.sum()\nprint(f'train missing data: {train_missing}({train_missing\/train_1.shape[0] :.2f}%)')\nprint(f'test missing data: {test_missing}({test_missing\/test_1.shape[0] :.2f}%)')","4dd3504e":"\n\ncategorical_features = []\ncontinuous_features = []\n\nfor col in train_1.columns:\n    if train_1[col].dtypes == 'int8':\n        categorical_features.append(col)\n    else:\n        continuous_features.append(col)\nsns.barplot(x=[0,1],y=[len(categorical_features),len(continuous_features)])\nplt.xticks([0,1],('categorical','continuous'))\nplt.show()","91f9fa6c":"#drop target column\ncategorical_features.pop()","48cd2a5b":"\ntrain_outlier = train_1[categorical_features]\ntest_outlier = test_1[categorical_features]\n\n\nfig = plt.figure(figsize=(25,50))\nsns.set_style(\"darkgrid\")\nfor idx, col_name in enumerate(train_outlier.columns):\n    fig.add_subplot(len(train_outlier.columns)\/\/4 + 1, 4, idx+1)\n    sns.histplot(data=train_outlier.iloc[:,idx],color='r',bins=2)\n    sns.histplot(data=test_outlier.iloc[:,idx],color='b',bins=2)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title(col_name)\nplt.show()\n    ","7c3736f7":"\ncorr=train_1[categorical_features].corr()\nsns.set_style('white')\n\nmask = np.triu(np.ones_like(corr, dtype = bool)) #True position will not show up\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for categorigal features of Training data')\nsns.heatmap(corr,cmap='coolwarm', mask = mask,annot=False, linewidths =0.5,square=True,cbar_kws={\"shrink\": 0.40})\nplt.show()","e837c58a":"train_outlier = train_1[continuous_features]\ntest_outlier = test_1[continuous_features]\n\n\nfig = plt.figure(figsize=(40,140))\nsns.set_style(\"darkgrid\")\nfor idx, col_name in enumerate(train_outlier.columns):\n    fig.add_subplot(len(train_outlier.columns)\/\/4 + 1, 4, idx+1)\n    sns.histplot(data=train_outlier.iloc[:,idx],color='r',bins=20)\n    sns.histplot(data=test_outlier.iloc[:,idx],color='b',bins=20)\n    plt.xlabel('')\n    plt.ylabel('')\n    plt.title(col_name)\nplt.show()\n    ","24bdc292":"corrs = train_1[continuous_features].corr()\n\nsns.set_style('white')\nmask = np.triu(np.ones_like(corrs, dtype = bool)) #True position will not show up\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for categorigal features of Training data')\nsns.heatmap(corrs,cmap='coolwarm', mask = mask,annot=False, linewidths =0.5,square=True,cbar_kws={\"shrink\": 0.40})\nplt.show()","ea7715aa":"corrs = train_1[continuous_features].corr().abs()\nhigh_corr = np.where(corrs>0.03)\nhigh_corr_col = []\nfor x, y in zip(high_corr[0],high_corr[1]):\n    if x != y and x < y:\n        high_corr_col.append(corrs.columns[x])\n        high_corr_col.append(corrs.columns[y])\nsig_corrs = train_1[list(set(high_corr_col))].corr()\n\nsns.set_style('white')\nmask = np.triu(np.ones_like(sig_corrs, dtype = bool)) #True position will not show up\nplt.figure(figsize = (15, 15))\nplt.title('Correlation matrix for categorigal features of Training data')\nsns.heatmap(sig_corrs,cmap='coolwarm', mask = mask,annot=False, linewidths =0.5,square=True,cbar_kws={\"shrink\": 0.40})\nplt.show()","19773eac":"X = train_1.drop(columns='target')\ny = train_1.target\nX_test = test_1.copy()\n\ndel train_1\ndel test_1","322ef1b1":"print(f'train size :{X.shape}, test size {X_test.shape}')","79bd9558":"X['mean'] = X.mean(axis=1)\nX['std'] = X.std(axis=1)\nX['max'] = X.max(axis=1)\n\nX_test['mean'] = X_test.mean(axis=1)\nX_test['std'] = X_test.std(axis=1)\nX_test['max'] = X_test.max(axis=1)","de2fb3c6":"%%time\nk_fold = StratifiedKFold(n_splits=5, shuffle=True,random_state=42)\n\nscores = []\ntest_predictions = []\n\n\nfor fold ,(train_index,valid_index) in enumerate(k_fold.split(X,y)):\n    X_train, y_train = X.iloc[train_index], y.iloc[train_index]\n    X_valid, y_valid = X.iloc[valid_index], y.iloc[valid_index]\n    \n    model = LogisticRegression(solver='liblinear',max_iter=600)\n    \n    model.fit(X_train,y_train)\n    valid_pred = model.predict_proba(X_valid)[:,1]#binary so it's fine.\n    fpr, tpr,_ = roc_curve(y_valid, valid_pred)\n    score = auc(fpr,tpr)\n    scores.append(score)\n    display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=score)\n    display.plot()\n    \n    \n    print(f'\\n Fold:{fold + 1},Score:{score}')\n    print('-'*50)\n    \n    test_preds = model.predict_proba(X_test)[:,1]\n    test_predictions.append(test_preds)\nprint(f'Overall Valid Score:{np.mean(scores)}')","2c263af6":"predic = np.mean(np.column_stack(test_predictions),axis=1)\nss = pd.read_csv('..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv')\nss['target'] = predic\nss.to_csv('submission.csv',index=False)\nss.head()","2807a4ae":"## Add some Features","0f77db97":"## Save and Check","4a5f4306":"#### There are too many features.  \n#### So just pick up features that realtive high corr.","700c520d":"## Some Overview","e0fe6ced":"## Get Data","e9da6f3c":"## Distribution"}}