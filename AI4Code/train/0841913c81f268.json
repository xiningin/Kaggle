{"cell_type":{"07326ec5":"code","bb5c6965":"code","7e8dec4b":"code","73446727":"code","73a66152":"code","452dcb2c":"code","953d680e":"code","a98d82e9":"code","80bac9e8":"code","a55a8ef9":"code","b01bdee8":"code","5189ecd0":"code","4bdb20da":"code","2e857940":"code","c5f8d38c":"code","53fd41f3":"code","55055074":"markdown","2fbf4037":"markdown","4719c77f":"markdown","5fd4c7e7":"markdown","12a06844":"markdown","0a01b819":"markdown","35731168":"markdown","a62622c1":"markdown","6015d11d":"markdown","7a55abfb":"markdown","47825c6c":"markdown","ee2c5cdd":"markdown","7628badb":"markdown","4670dbcf":"markdown"},"source":{"07326ec5":"# Code you have previously used to load data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.preprocessing import MinMaxScaler\nfrom matplotlib.ticker import MaxNLocator\nimport missingno as msno\n\n# Set up code checking\nimport os\nif not os.path.exists(\"..\/input\/train.csv\"):\n    os.symlink(\"..\/input\/home-data-for-ml-course\/train.csv\", \"..\/input\/train.csv\")  \n    os.symlink(\"..\/input\/home-data-for-ml-course\/test.csv\", \"..\/input\/test.csv\") ","bb5c6965":"# Path of the file to read. We changed the directory structure to simplify submitting to a competition\niowa_file_path = '..\/input\/train.csv'\niowa_test_file_path = '..\/input\/test.csv'\n\nhome_data = pd.read_csv(iowa_file_path)\nhome_data_test = pd.read_csv(iowa_test_file_path)\n# Create target object and call it y\nSalePrice = home_data.iloc[:,-1]\nSalePriceTest = home_data_test.iloc[:,-1]","7e8dec4b":"# Create \nhome_data['YearBuilt'] = home_data['YearBuilt'].astype(int)\nhome_data['YrSold'] = home_data['YrSold'].astype(int)\n\nFeatures = home_data.copy()\nFeatures_test = home_data_test.copy()\n\nFeatures.drop(['Id','SalePrice'], axis=1, inplace=True)\nFeatures_test.drop ('Id', axis=1, inplace=True)","73446727":"def basic_EDA(df):\n    size = df.shape\n    sum_duplicates = df.duplicated().sum()\n    sum_null = df.isnull().sum().sum()\n    is_NaN = df. isnull()\n    row_has_NaN = is_NaN. any(axis=1)\n    rows_with_NaN = df[row_has_NaN]\n    count_NaN_rows = rows_with_NaN.shape\n    return print(\"Number of Samples: %d,\\nNumber of Features: %d,\\nDuplicated Entries: %d,\\nNull Entries: %d,\\nNumber of Rows with Null Entries: %d %.1f%%\" %(size[0],size[1], sum_duplicates, sum_null,count_NaN_rows[0],(count_NaN_rows[0] \/ df.shape[0])*100))\n    \ndef bar_plot(x,y,xlabel,ylabel,title):\n    plt.figure(figsize=(20,5))\n    sns.set(style=\"ticks\", font_scale = 1)\n    ax = sns.barplot(x=x, y = y, palette=\"Blues_d\")\n    sns.despine(top=True, right=True, left=True, bottom=False)\n    plt.xticks(rotation=70,fontsize = 12)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    plt.title(title)\n\n    for p in ax.patches:\n                 ax.annotate(\"%.d\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., abs(p.get_height())),\n                     ha='center', va='center', color='black', xytext=(0, 10),\n                     textcoords='offset points')\n\ndef Null_Analysis(df,title):\n    null_columns=df.columns[df.isnull().any()]\n    null_columns_plot = df[null_columns].isnull().sum().sort_values(ascending = False)\n    bar_plot(null_columns_plot.index, null_columns_plot,\"Features\", \"Number of Null Values\", title)","73a66152":"basic_EDA(Features)","452dcb2c":"basic_EDA(Features_test)","953d680e":"Null_Analysis(Features, \"Training Set\")\nNull_Analysis(Features_test, \"Test Set\")","a98d82e9":"msno.heatmap(home_data)","80bac9e8":"#Separate in Numerical and Categorical Variables\nnumeric_data = Features.select_dtypes(include=[np.number])\nnumeric_data_test = Features_test.select_dtypes(include=[np.number])\n\ncategorical_data = Features.select_dtypes(exclude=[np.number])\ncategorical_data_test = Features_test.select_dtypes(exclude=[np.number])\n\nnumeric_features = numeric_data.columns","a55a8ef9":"def plot_feature_distribution(df1, df2, label1, label2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(6,6,figsize=(18,22))\n\n    for feature in features:\n        i += 1\n        plt.subplot(6,6,i)\n        df1[feature].plot.kde()\n        df2[feature].plot.kde()\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad = 0)\n        plt.tick_params(axis='y', which='major', labelsize=6, pad = 0)\n        plt.ylabel(\"Density\",fontsize=8)\n    fig.tight_layout(pad=3.0)\n    plt.show();\n    \nplot_feature_distribution(numeric_data, numeric_data_test, \"Train\", \"Test\", numeric_features)    ","b01bdee8":"def plot_reg(df1, label1, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(6,6,figsize=(22,15))\n\n    for feature in features:\n        i += 1\n        plt.subplot(6,6,i)\n        sns.scatterplot(x = df1[feature], y = \"SalePrice\",data=home_data,linewidth=0,s=10)\n        #df2[feature].plot.kde()\n        plt.xlabel(feature, fontsize=9)\n        locs, labels = plt.xticks()\n        plt.tick_params(axis='x', which='major', labelsize=6, pad = 0)\n        plt.tick_params(axis='y', which='major', labelsize=6, pad = 0)\n        #plt.ylabel(\"Density\",fontsize=8)\n    fig.tight_layout(pad=3.0)\n    plt.show();\n    \nplot_reg(home_data, \"Train\", numeric_features)","5189ecd0":"def boxplot_disc(y, df):\n    fig, axes = plt.subplots(5, 3, figsize=(25, 25))\n    axes = axes.flatten()\n\n    for i, j in zip(df.columns[:-1], axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,y=y,data=df,palette='Blues_d',order=sortd.index,ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()","4bdb20da":"DiscreteNumeric = home_data.loc[:,['MSSubClass','OverallQual','OverallCond','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','GarageCars','MoSold','KitchenAbvGr','TotRmsAbvGrd','Fireplaces','YrSold','YearBuilt','SalePrice']]\nboxplot_disc('SalePrice', DiscreteNumeric)","2e857940":"cm = sns.diverging_palette(220, 20, sep=5, as_cmap=True)\ncorr = numeric_data.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(20,15))\n    ax = sns.heatmap(corr, mask=mask, vmin = -0.5, vmax=0.8, linewidths=.5,annot=True,fmt=\".2f\",cmap = cm,annot_kws={\"size\": 10},cbar=False)","c5f8d38c":"def boxplot(y, df):\n    fig, axes = plt.subplots(14, 3, figsize=(25, 80))\n    axes = axes.flatten()\n\n    for i, j in zip(df.select_dtypes(include=['object']).columns, axes):\n\n        sortd = df.groupby([i])[y].median().sort_values(ascending=False)\n        sns.boxplot(x=i,y=y,data=df,palette='Blues_d',order=sortd.index,ax=j)\n        j.tick_params(labelrotation=45)\n        j.yaxis.set_major_locator(MaxNLocator(nbins=18))\n\n        plt.tight_layout()","53fd41f3":"boxplot('SalePrice', home_data)","55055074":"# Ames Housing Dataset\n\n![Ames Housing dataset image](https:\/\/i.imgur.com\/lTJVG4e.png)\n\nThe dataset contains 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa. It is a complete dataset if compared to the traditional Boston Housing Dataset commonly used for Data Science tutorials. The Ames Housing dataset was compiled by Dean De Cock for use in data science education.\n\nIn here an initial data analysis is performed to guide the next steps. The most relevant points learned from this data analysis are:\n\n* A strategy to handle the null values must be trialled. KNN, mean values replacement and MICE strategies need to be tested\n* There are features that are highly correlated. They could be removed or combined to simplify the dataset\n* Skewed variables are present.  Feature transformation can be used to improve model performance, mainly if ANN or SVM are used\n* From the feature analysis, it was possible to obtain ideas for new features that could improve model performance","2fbf4037":"# Initial Data Import","4719c77f":"## Numerical Features Analysis \n\nFirst, we start by analysing the numerical data. Then we use box plots to visualise the categorical features.\n\nKDE plots are great to understand the distribution of the variables as well as compare the distribution of the training and test sets. Ideally, all sets should have a similar distribution so that improvements obtained on the training and validation set are likely to be replicated on the test set. \n\nThe set of graphs below provided the following insights:\n\n* Several features are skewed, a strategy to apply some feature transformation can be helpful to improve model performance\n* Some features are continuous, however, shapes such as OverallCond indicate discrete variables. A more apparent distinction between discrete and continuous features can be visualised by using Scatter Plots\n* The train and test sets provided have a similar distribution, which will make our life easier when testing different approaches","5fd4c7e7":"## Null Values Analysis\n\nFrom the previous output, it becomes clear that handling the null values is going to be one of the top priorities. More detailed information about the missing values can be obtained by plotting the missing values per features and using the MSNO package to understand the relationship between the missing variables.","12a06844":"Analysing the Categorical and Discrete variables, ideas regarding new features start to pop-up. \n\nFor example, the boxplot analysis has shown that on average the houses with excellent conditions were sold with higher prices. Therefore a new feature could count how many \"Ex\" (as excellent) or \"Po\" (as poor) quality markers a house has scored. A similar thing can be done by grouping the neighbourhood with similar average prices. Features such as \"has a Fireplace\" or \"has a Pool\" can also be derived from the existing features and can potentially help the model predictions. \n\nThis concludes our initial EDA. In Part 2, we analyse how to handle the missing values, perform feature engineering and variable transformation. \n\n**Please, consider to upvote if you found this helpful.**","0a01b819":"# Part 1 - EDA\n\nInitial Analysis to understand dataset size, number of features, null and duplicated entries:","35731168":"From the plots above, it is possible to obtain the following insights from this dataset:\n* The idea to plot the training and test sets KDE on top of each other is to verify their similarity. There is no point on training the data within a widely different variable range from the test set. When possible, make sure the training and test sets have similar distributions, as it is likely that the strategies that improve the training set will also have a positive effect on the test set.\n* The KDE plots shapes support identification of skewed variables, see LotFrontage for example. This analysis is relevant as later we can attempt to transform skewed variables into a more Gaussian distribution to improve the ML models prediction capability. Variable transformation is appropriate, especially when using ANN or SVM, as they are more sensitive to skewed data (if compared to model tree's). As a downside, features and target variables transformation (log, exp, power) may affect the interpretability of the results; however, this is not a concern on this competition. \n* While the KDE plots can provide some intuition regarding which variables are Discrete or Continuous, the scatter plots offer a more precise insight for the task. From the scatter plots, it is straightforward to identify the Discrete (e.g. OverallQual, YrSold, OverallCond) and Continuous (e.g. LotFrontage, LotArea, GrLiveArea) variables. For the House Market dataset is easy to identify such characteristics even by the variable name. However, in real-life sometimes is not that easy, and this analysis comes in handy as discrete and continuous variables can be analysed differently.\n* Regarding the scatter plots for the continuous variables. Scatter shapes such as the GrLivArea, GarageArea or 1stFlrSF are usually helpful as they present an almost linear dependency to the target, SalePrice. As such, these variables will also have a higher correlation value to the SalePrice. This is because correlation is only able to capture linear relationships. \n* Additionally, see how EnclosedPorch and BsmtFinSF2 have almost an exponential shape. Perhaps some kind of transformation can be helpful here as well, as we can also note from their KDE graph which is also highly skewed. For other variables, the dependency is not as obvious, such as for OPenPorchSF or MsVnrArea. It does not mean that they are not relevant to the model, since we will use non-linear models, they will be able to capture such behaviour. \n* For the Discrete variables, we can also use box plots to analyse their effect on the SalePrice better. The Discrete Features we can identify from the scatter plot are: *MsSubClass, OverallQual, OverallCond, BsmtFullBath, BsmtHalfBath, FullBath, HalfBath, BedroomAbvGr, GarageCars, MoSold, KitchenAbvGr, TotRmsAbvGrd, Fireplaces, YrSold*","a62622c1":"As mentioned earlier, the correlation only measures the linear relationship between two variables. For this reason, I do not believe that the correlation between the variables and target is **that** helpful.  As it is a linear relationship measurement, the correlation is not always a valid strategy to discard variables as most applications will contain non-linear dependences between the variables. \n\nWe can also use the correlation plot to better understand the data. Note in the plot above a few clusters. For this dataset, it is easy to understand the \"why\" of these clusters. For example, GarageYrBlt, GarageCars and GarageArea show a higher correlation to OverallQual, YearBuilt and YearRemodAdd. It is relatively straight forward to understand why the year the garage was built is positively correlated to the year the house was built, which probably also impacts the current quality of the house (OverallQual). Such analysis is interesting to understand the dataset, which variables are affected by what and even understand which variables could be made redundant.\n\nWhen dealing with large datasets and an incredibly high number of features, one strategy to remove features is to analyse their correlation value. There is an argument that adding two variables that are highly correlated to each other do not add meaningful information to the model, using only one of them would suffice. For example, we can see how GarageArea and GarageCars are highly correlated (0.88). As more extensive garage area fits more cars, one of them could be removed to make a leaner model. \n\nMain take-away:\n\n* Highly correlated features could be removed ","6015d11d":"## Categorical Features Analysis\n\nNext we analyse the categorical features also using the Box Plot. Insights regarding the house prices and the different categories of each variables can be extracted.","7a55abfb":"Regarding the Categorical features, the following categories caught my attention as they are related to higher house prices: \n* Alley as \"Pave\", \n* Condition2 as \"PosN\"\n* RoofMatl as \"WdShngl\"\n* MasVnrType as \"Stone\"\n* ExterQual as \"Ex\"\n* BsmtQual as \"Ex\"\n* KitchenQual as \"Ex\"\n* FireplaceQu as \"Ex\"\n* PoolQc as \"Ex\".","47825c6c":"## Graphical Analysis\n\nAs we already analysed the null values and predefined a strategy to handle them, it is now time to understand the features of the dataset. Scatter plots, KDE, Correlation and Box Plots can provide useful information and are shown next.","ee2c5cdd":"The above correlation matrix from the MSNO package helps us understand the relationship between the missing variables. In this analysis, features that present strong correlation signifies that when that specific feature is missing, the other is also likely to be missing. Negative values represent the reverse relationship; if one is missing, the other is likely to be present. \n\nWe see strong relationships within the garage features, which makes sense since if one variable related to the garage is missing, it is likely that garage information is lacking for that property. A similar cluster is formed for the features related to the basement.","7628badb":"* It is interesting how the variable OverallQual seems to affect the price much more than OverallCond. For example, on average houses marked with score 7 or higher for OverallQual have been sold for US 225k. The same analysis cannot be performed by OverallCond as they mean values are quite within the same range.\n* Similar to OverallCond, many other variables do not seem to affect the average price as much. In contrast, the variables FullBath, GarageCars and TotRmsAbvGrd do show a clear variability of the average house price according to their value.","4670dbcf":"There are plenty of missing values. At this point, we can conclude that eliminating the rows with null values is not an option as it would include too many samples. Plausible options are the following:\n\n* Understand the impact of eliminating the Features with too many null values for train and test set, e.g. PoolQC, MiscFeature\n\n* Replacement of the null values. This can be done by using the average, median, KNN or MICE strategies. It will be necessary to try and check the best method for each feature"}}