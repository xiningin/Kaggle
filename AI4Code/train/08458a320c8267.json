{"cell_type":{"f7e567e0":"code","1a29de05":"code","f6d89a68":"code","ce668cd1":"code","8d589d79":"code","2aaa9f24":"code","8e235189":"code","54bf4003":"code","7289966e":"code","d10e87f5":"code","d2d4be72":"code","bc2c2744":"code","3bda28c7":"code","89c2aac9":"markdown","7a445217":"markdown","a5489754":"markdown","2e22ec35":"markdown","b1533152":"markdown","1589a723":"markdown","3eb1b221":"markdown"},"source":{"f7e567e0":"import numpy as np; np.random.random(42)\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score\nfrom tqdm import tqdm\nimport warnings; warnings.filterwarnings(\"ignore\")\n\n%matplotlib inline\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set()","1a29de05":"plt.rcParams['figure.figsize'] = [10, 5]\nplt.rcParams['font.size'] = 12","f6d89a68":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nprint(train.shape, test.shape)","ce668cd1":"train.head(3)","8d589d79":"print( train.target.value_counts() \/ train.shape[0] * 100 )","2aaa9f24":"feature_names = train.columns[2:]","8e235189":"train[\"train_test\"] = 1\ntest[\"train_test\"] = 0","54bf4003":"data = pd.concat(( train, test ))\n\nnp.random.seed(42)\ndata = data.iloc[ np.random.permutation(len( data )) ]\ndata.reset_index( drop = True, inplace = True )\n\nx = data.drop( [ 'target', 'ID_code','train_test' ], axis = 1 )\ny = data.train_test","7289966e":"train_examples = len(train)\n\nx_train = x[:train_examples]\nx_test = x[train_examples:]\ny_train = y[:train_examples]\ny_test = y[train_examples:]","d10e87f5":"x_train, x_test, y_train, y_test = train_test_split( x, y, train_size = train_examples, random_state=42 )","d2d4be72":"clf = LogisticRegression(penalty=\"l1\", C=0.1, solver=\"liblinear\", random_state=42)\nclf.fit(x_train, y_train)\ny_pred = clf.predict_proba(x_test)[:, 1]\nroc_auc_score(y_test, y_pred)","bc2c2744":"clf = RandomForestClassifier(n_estimators=10, random_state=42)\nclf.fit(x_train, y_train)\ny_pred = clf.predict_proba(x_test)[:, 1]\nprint(\"AUC:\",round(roc_auc_score(y_test, y_pred)*100,2),\"%\")","3bda28c7":"clf = RandomForestClassifier(n_estimators=100, random_state=42)\nclf.fit(x_train, y_train)\ny_pred = clf.predict_proba(x_test)[:, 1]\nprint(\"AUC:\",round(roc_auc_score(y_test, y_pred)*100,2),\"%\")","89c2aac9":"***[Please upvote this kernel if you like it!]***\n\n## **[Adversarial validation](http:\/\/fastml.com\/adversarial-validation-part-one\/)**\n\nHere, we will cofirm how different training and test datasets are by the adversarial validation.  \n\n***If we attempted to train a classifier to distinguish training datasets from test datasets, it would perform no better than random. This would correspond to ROC AUC of 0.5.***\n\n( I may be not correct, so I'll welcome any comments. )","7a445217":"**Now we\u2019re ready to train and evaluate. Here are the scores:**","a5489754":"**We start by setting the labels according to the task. It\u2019s as easy as:**","2e22ec35":"**Finally we create a new train\/test split:**","b1533152":"***Although There's room for improvement in these models, but, for now, we can't distinguish train and test datasets.***\n\n***So, \"Trust CV\" may be also very true to this competiton!***","1589a723":"**Come to think of it, there\u2019s a shorter way (no need to shuffle examples beforehand, too):**","3eb1b221":"**Then we concatenate both frames and shuffle the examples:**"}}