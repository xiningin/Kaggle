{"cell_type":{"d32251d7":"code","8c2dbfa0":"code","de9a885e":"code","ca8d4123":"code","be83c5b8":"code","a3a61362":"code","7371167a":"code","bf4bd577":"code","8983ffe5":"code","6ff8ddfa":"code","ce0c322e":"code","9cc0c7da":"code","221413c9":"code","def0bafa":"code","e236c31c":"code","43d1b76e":"code","7ecfed5e":"code","334e821e":"code","9a9593f6":"code","14add99a":"code","97b4498f":"code","27448459":"code","0b103cf8":"code","db89c4f0":"markdown","b5941c3d":"markdown","189704a0":"markdown","9ab1ea8e":"markdown","d9848253":"markdown","ceef253c":"markdown","9f88bd25":"markdown","26936979":"markdown","3a0aaca2":"markdown","63fb914f":"markdown","ff5b49eb":"markdown","321ce7f2":"markdown","fbfe92b9":"markdown","3a1c74c1":"markdown","32d0add4":"markdown","a70bbbf1":"markdown","f52b6ab2":"markdown","6224ed16":"markdown","af42e442":"markdown","02185b61":"markdown","f818c576":"markdown","9b4e38d4":"markdown"},"source":{"d32251d7":"import pandas as pd\n\nmaths = pd.read_csv('..\/input\/student-mat.csv') \nportug = pd.read_csv('..\/input\/student-por.csv')\n\nprint(maths.head())","8c2dbfa0":"sample = maths.loc[0,:]\nprint(sample)","de9a885e":"totalDataSet = pd.concat([maths,portug])","ca8d4123":"import matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\nlst = ['school','sex','address','Dalc']\nfig = plt.figure(figsize=(10, 20))\nplt.rcParams.update({'font.size': 15})\n\nfor x,y in enumerate(lst):\n    plt.subplot(len(lst),1,1+x) \n    plt.xlabel(y)\n    plt.ylabel(\"G3\")\n    totalDataSet.groupby(y)['G3'].mean().plot(kind='bar')\n","be83c5b8":"import seaborn as sns\nsns.set()\nsns.heatmap(totalDataSet.corr(),linewidths=.5)","a3a61362":"ave = sum(totalDataSet.G3)\/float(len(totalDataSet))\ntotalDataSet['average'] = ['above average' if i > ave else 'under average' for i in totalDataSet.G3]\nsns.swarmplot(x=totalDataSet.Dalc, y =totalDataSet.G3, hue = totalDataSet.average)\ntotalDataSet.drop('average',axis=1);","7371167a":"ax2 = pd.value_counts(totalDataSet['Dalc']).sort_values(ascending=False).plot.bar()\nax2.set_xlabel('Number of Weekdays spent Drinking')\nax2.set_ylabel('Number of Students')","bf4bd577":"outcomes = totalDataSet['G3']\nfeatures_raw = totalDataSet.drop(['G3','G2','G1'],axis=1)","8983ffe5":"features_raw.hist(alpha=0.5, figsize=(16, 10))","6ff8ddfa":"skewed=['Dalc','Walc','absences','failures','traveltime']\nfeatures_log_transformed = pd.DataFrame(data=features_raw)\nfeatures_log_transformed[skewed] = features_raw[skewed].apply(lambda x:np.log(x+1))\n\nfeatures_log_transformed[skewed].hist(alpha=0.5, figsize=(16, 10))","ce0c322e":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nnumerical = ['age','Medu','Fedu','traveltime','studytime','failures','famrel','freetime','goout','Dalc','Walc','health','absences']\nfeatures_log_minmax_transform = pd.DataFrame(data=features_log_transformed)\nfeatures_log_minmax_transform[numerical] = scaler.fit_transform(features_log_transformed[numerical])\n\ndisplay(features_log_minmax_transform.head(n=5))","9cc0c7da":"features_final = pd.get_dummies(features_log_minmax_transform)\nencoded = list(features_final.columns)\nprint(\"{} total features after one-hot encoding\".format(len(encoded)))\nprint(encoded)","221413c9":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(features_final,outcomes, test_size = 0.2, random_state=42)\n\nprint(\"Training set has {} samples\".format(X_train.shape[0]))\nprint(\"Testing set has {} samples\".format(X_test.shape[0]))","def0bafa":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression()\nmodel.fit(X_train,y_train)","e236c31c":"y_train_pred = model.predict(X_train)\ny_test_pred = model.predict(X_test)\n\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\n\ntrain_rms = sqrt(mean_squared_error(y_train, y_train_pred))\ntest_rms = sqrt(mean_squared_error(y_test, y_test_pred))\n\n\nprint(\"The Root mean Squared Error for the training set is\", train_rms)\nprint(\"The Root mean Squared Error for the testing set is \", test_rms)\n\n\nmae_train = mean_absolute_error(y_train, y_train_pred)\nmae_test = mean_absolute_error(y_test, y_test_pred)\nprint('Mean Absolute Error for Training Set: %f' % mae_train)\nprint('Mean Absolute Error for Testing Set: %f' % mae_test)\n\nprint(\"Cross val score for training set\",cross_val_score(model, X_train, y_train, cv=5).mean())\nprint(\"Cross val score for testing set\",cross_val_score(model, X_test, y_test, cv=5).mean())","43d1b76e":"def model_Creator_Tester(name,model,X_train,X_test,y_train,y_test):\n    print(name)\n    model.fit(X_train,y_train)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import mean_absolute_error\n\n    train_rms = sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rms = sqrt(mean_squared_error(y_test, y_test_pred))\n\n\n    print(\"The Root mean Squared Error for the training set is\", train_rms)\n    print(\"The Root mean Squared Error for the testing set is \", test_rms)\n\n\n    mae_train = mean_absolute_error(y_train, y_train_pred)\n    mae_test = mean_absolute_error(y_test, y_test_pred)\n    print('Mean Absolute Error for Training Set: %f' % mae_train)\n    print('Mean Absolute Error for Testing Set: %f' % mae_test)\n    return train_rms,test_rms,mae_train,mae_test;\n    #print(\"Cross val score for training set\",cross_val_score(model, X_train, y_train, cv=5).mean())\n    #print(\"Cross val score for testing set\",cross_val_score(model, X_test, y_test, cv=5).mean())","7ecfed5e":"from sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb\nnames = [\"Linear_Regression\",\"XGB\",\"SVM\",\"LGB\"]\nmodels = [LinearRegression(),XGBClassifier(),SVC(gamma='auto'),lgb.LGBMRegressor()]\nresults = {}\nfor x,y in zip(names,models):\n    print(\"\\n\",y,\"\\n\")\n    results[x]=model_Creator_Tester(x,y,X_train,X_test,y_train,y_test)","334e821e":"def color_gradient ( val, beg_rgb, end_rgb, val_min = 0, val_max = 1):\n    val_scale = (1.0 * val - val_min) \/ (val_max - val_min)\n    return ( beg_rgb[0] + val_scale * (end_rgb[0] - beg_rgb[0]),\n             beg_rgb[1] + val_scale * (end_rgb[1] - beg_rgb[1]),\n             beg_rgb[2] + val_scale * (end_rgb[2] - beg_rgb[2]))","9a9593f6":"#print(results)\ndef print_results(results):\n    titles = [\"Root Mean Square for Training Set\",\"Root Mean Square for Testing Set\",\"Mean Absolute Error for Training Set\",\"Mean Absolute Error for Testing Set\"]\n    fig = plt.figure(figsize=(10, 10))\n    plt.rcParams.update({'font.size': 10})\n    grad_beg, grad_end = ( 0.1, 0.1, 0.1), (1, 1, 0)\n    for i,k in enumerate(results):\n        tempVals = []\n        for j in results.keys():\n            #print(i,j)\n            #print(results[j][i])\n            tempVals.append(results[j][i])\n        print(tempVals)\n        print(results.keys())\n        plt.subplot(len(titles)\/2.,len(titles)\/2.,1+i)\n        col_list = [ color_gradient( val,\n                                 grad_beg,\n                                 grad_end,\n                                 min( tempVals),\n                                 max(tempVals)) for val in tempVals]\n\n        plt.bar(results.keys(),tempVals,color = col_list)\n        plt.title(titles[i])\n","14add99a":"print_results(results)","97b4498f":"from sklearn.model_selection import GridSearchCV\nparams={'Linear_Regression':{'fit_intercept':[True,False], 'normalize':[True,False], 'copy_X':[True,False]},\n        'XGB':{\n                'boster':['gbtree'],\n                'eta':[0.05,0.1,0.25,0.5,0.8],\n                'gamma':[0.05,0.1,0.25,0.5,0.8],\n                #'reg_alpha': [0.05,0.1,0.25,0.5,0.8],\n                #'reg_lambda': [0.05,0.1,0.25,0.5,0.8],\n                'max_depth':[3,6,10],\n                'subsample':[0.1,0.25,0.5,0.8]\n        },\n        'SVM':{'C': [0.001, 0.01, 0.1, 1, 10], 'gamma' : [0.001, 0.01, 0.1, 1],'kernel':['rbf','linear']},\n       'LGB':{'boosting_type': ['gbdt'],\n                'num_leaves': [20,50,80],\n                'learning_rate': [0.05,0.1,0.25,0.5,0.8],\n                'subsample_for_bin': [10,100,500],\n                'min_child_samples': [20,50,100],\n                'reg_alpha': [0.05,0.1,0.25,0.5,0.8],\n                'reg_lambda': [0.05,0.1,0.25,0.5,0.8]\n             }\n        }\n\nnames = [\"Linear_Regression\",\"SVM\",\"LGB\",\"XGB\"]\n\nmodels = [LinearRegression(),SVC(),lgb.LGBMRegressor(),XGBClassifier()]\n","27448459":"def grid_model_Creator_Tester(name,model,X_train,X_test,y_train,y_test):\n    print(name)\n    model.fit(X_train,y_train)\n    best_model = model.best_params_\n    print(best_model)\n    y_train_pred = model.predict(X_train)\n    y_test_pred = model.predict(X_test)\n\n    from sklearn.metrics import mean_squared_error\n    from math import sqrt\n    from sklearn.model_selection import cross_val_score\n    from sklearn.metrics import mean_absolute_error\n\n    train_rms = sqrt(mean_squared_error(y_train, y_train_pred))\n    test_rms = sqrt(mean_squared_error(y_test, y_test_pred))\n\n\n    print(\"The Root mean Squared Error for the training set is\", train_rms)\n    print(\"The Root mean Squared Error for the testing set is \", test_rms)\n\n\n    mae_train = mean_absolute_error(y_train, y_train_pred)\n    mae_test = mean_absolute_error(y_test, y_test_pred)\n    print('Mean Absolute Error for Training Set: %f' % mae_train)\n    print('Mean Absolute Error for Testing Set: %f' % mae_test)\n    return train_rms,test_rms,mae_train,mae_test;","0b103cf8":"results = {}\nfor x,y in zip(names,models):\n    results[x]=grid_model_Creator_Tester(x,GridSearchCV(y,params[x]),X_train,X_test,y_train,y_test)\nprint_results(results)","db89c4f0":"The results here are quite interesting. The darkest bars have the lowest errors, which seems to be LGB in every scenario except for MAE on the training set. In this case XGB seems to outperform LGB. It could be said that may have overfitted the training set. It seems to be that Support Vector Machine is the worst performing algorithm on this dataset","b5941c3d":"<h1>Udacity Capstone Project<\/h1>\n\n\nThis investigation hopes to use the Kaggle dataset to investigate social and economic aspects of student's lives and see if their final grades can be predicted based on these.","189704a0":"## Base Model\nLinear Regression will be used as a benchmark model and we will use other algorithms in an attempt to outperform this","9ab1ea8e":"These results suggest that a students grade can be predicted with approximately an error of 1.8-1.9 away from the actual grade which gives us a margin of Error of about 10-40% margin of Error. This is quite a large swing for an error but for the larger error it is still quite close to the actualy score, the error is just relative and shows that the algorithm performs admirably when predicting results ","d9848253":"The above code allows us to dynamically look at different attributes and let us have an insight into how they might affect the final grade. For example we can see that there is little difference in final grades between males and females. However we can see that the GP school seems to have a better average score than MS. One other tool which might give us a better insight would be a heat map. ","ceef253c":"## Training the model\nFinally we can start getting to the good stuff\nWe obviously can't train the model on the whole dataset so we will use SkLearns cross validation implementation to split the data into training and testing sets","9f88bd25":"# Data PreProcessing\nWe will look at the dataset and investigate some preprocessing techniques and see which ones will be most appropriate for the data. \n\nFirst we will separate the feature we are trying to predict, G3, with the rest of the features. In order to make this a bit more challenging for the algorithms we will also drop G2 and G1. It will be interesting to see if final grades can be predicted without these intermediate grades.","26936979":"<h2> Investigating Dataset<\/h2>\nWithin this dataset, there are two csv files. Each file represents a different class, one being a maths class, the other being Portuguese. The file names are \n\n1. student-mat.csv\n\n2. student-por.csv\n\nBoth files have the same columns so we can have a sneak peak at this below","3a0aaca2":"The heat map shows the correlation between each feature in the dataset. It can be seen how there are extremely high correlations between the final grades and the grades given in first and second perion. The next features which seem to be most correlated are the level of education which the student's parents have attained.","63fb914f":"<h3> File Schemas <\/h3>\n\n<ul>\n    <li>school: Student's school (binary: 'GP' - Gabriel Pereira or 'MS' - Mousinho da Silveira)<\/li>\n\n<li>sex:Student's sex (binary: 'F' - female or 'M' - male)<\/li>\n\n<li>age: Student's age (numeric: from 15 to 22)<\/li>\n\n<li>address: Student's home address type (binary: 'U' - urban or 'R' - rural)<\/li>\n\n<li>famsize: Family size (binary: 'LE3' - less or equal to 3 or 'GT3' - greater than 3)<\/li>\n\n<li>Pstatus: Parent's cohabitation status (binary: 'T' - living together or 'A' - living apart)<\/li>\n\n<li>Medu: Mother's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, or 4 - higher education)<\/li>\n\n<li>Fedu: Father's education (numeric: 0 - none, 1 - primary education (4th grade), 2 - 5th to 9th grade, 3 - secondary education, or 4 - higher education)<\/li>\n\n<li>Mjob: Mother's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')<\/li>\n\n<li>Fjob: Father's job (nominal: 'teacher', 'health' care related, civil 'services' (e.g. administrative or police), 'at_home' or 'other')<\/li>\n\n<li>reason: Reason to choose this school (nominal: close to 'home', school 'reputation', 'course' preference or 'other')<\/li>\n\n<li>guardian: Student's guardian (nominal: 'mother', 'father' or 'other')<\/li>\n\n<li>traveltime: Home to school travel time (numeric: 1 - <15 min., 2 - 15 to 30 min., 3 - 30 min. to 1 hour, or 4 - >1 hour)<\/li>\n\n<li>studytime: Weekly study time (numeric: 1 - <2 hours, 2 - 2 to 5 hours, 3 - 5 to 10 hours, or 4 - >10 hours)<\/li>\n\n<li>failures: Number of past class failures (numeric: n if 1<=n<3, else 4)<\/li>\n\n<li>schoolsup:Extra educational support (binary: yes or no)<\/li>\n\n<li>famsup: Family educational support (binary: yes or no)<\/li>\n\n<li>paid: Extra paid classes within the course subject (Math or Portuguese) (binary: yes or no)<\/li>\n\n<li>activities: Extra-curricular activities (binary: yes or no)<\/li>\n\n<li>nursery: Attended nursery school (binary: yes or no)<\/li>\n\n<li>higher\" Wants to take higher education (binary: yes or no)<\/li>\n\n<li>internet: Internet access at home (binary: yes or no)<\/li>\n\n<li>romantic: With a romantic relationship (binary: yes or no)<\/li>\n\n<li>famrel: Quality of family relationships (numeric: from 1 - very bad to 5 - excellent)<\/li>\n\n<li>freetime: Free time after school (numeric: from 1 - very low to 5 - very high)<\/li>\n\n<li>goout: Going out with friends (numeric: from 1 - very low to 5 - very high)<\/li>\n\n<li>Dalc: Workday alcohol consumption (numeric: from 1 - very low to 5 - very high)<\/li>\n\n<li>Walc: Weekend alcohol consumption (numeric: from 1 - very low to 5 - very high)<\/li>\n\n<li>health: Current health status (numeric: from 1 - very bad to 5 - very good)<\/li>\n\n<li>absences: Number of school absences (numeric: from 0 to 93)<\/li>\n\n<li>G1: First period grade (numeric: from 0 to 20)<\/li>\n\n<li>G2: Second period grade (numeric: from 0 to 20)<\/li>\n\n<li>G3: Final grade (numeric: from 0 to 20, output target)<\/li>\n<\/ul>","ff5b49eb":"We can see there are a few features which seem to have a skewed distribution. For example \n    1. Dalc\n    2. absences\n    3. failures\n    4. traveltime\nThese are some good choices which could use a logarithimic distribution applied to them","321ce7f2":"## Testing other algorithms\nIn this section we will test out some other algorithms. Once we have found what is believed to be the optimal solution, a grid search will be used on the hyper parameters in the hope that the optimal solution will be found\nThe algorithms to be tested will include\n\n    1.XgBoost\n    \n    2.LightGBM\n    \n    3.SVM\n    \nWe can create a function to help reduce the code needed ","fbfe92b9":"Looking at the errors for the four algorithms, it can be seen that LGB has the lowest error on both testing sets. It could be said that XGB seems to overfit the datasets as the training error for that is very low while the testing error is much higher!","3a1c74c1":"<h3>Joining Datasets together<\/h3>\nFor the rest of this notebook we will combine the datasets for the two classes together and work on one combined dataset and investigate the features in the dataset","32d0add4":"Now to test the model and see how it performed!","a70bbbf1":"### Linear Regression\nTraining and Testing the model","f52b6ab2":"Even though most of the numerical feature were already within a range of 1-5 it was good to scale all features so they can be on the same range","6224ed16":"## Normalizing Numerical Features\nIt is also good practice to perform some scaling on numerical features. This will ensure that each feature is treated equally when performing supervised learning algortihms on the data.","af42e442":"## One Hot Encoding\nMost algorithms expect numerical values to process. However it can be clearly seen that there are non numerical features. Pandas can be used to encode these to numerical values","02185b61":"We can see we have two files with 33 columns however the meanings of the eery columns are not overly clear from looking at one row of the dataset. Below we have a full mapping of the columns with a more clear explanation.","f818c576":"## Optimising Hyper Paramaters\nIn the results we have seen above this is using the most vanilla of algortihms. The next stage of this is to use GridSearch to try and optimise the hyperparamaters for each algorithm and see how much more we can improve performance","9b4e38d4":"We can see the number of students that drink on a weekday drops dramatically after 1. This is a good sign as it can be seen from the scatter plot that the number of below average students seem to outweigh the number of above average as the student drinks more and more during the week"}}