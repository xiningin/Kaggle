{"cell_type":{"7e49f58b":"code","4dc95fb7":"code","cf5c2d37":"code","71fe3297":"code","ddec788f":"code","c5d6882c":"code","99cb032f":"code","2baed34c":"code","269caf8f":"code","7e889c87":"code","d14aa45f":"code","cc43bd64":"code","918c5d5f":"code","1495f41b":"code","79a78a3e":"code","f0216b77":"code","319021f3":"code","ba72b9d8":"code","47a7b268":"code","dd183f93":"code","b4966f14":"code","a220fde2":"code","310c80e8":"code","6f01a61a":"code","2787efb8":"code","f8ce03f8":"code","0fd12803":"code","a34b0cfc":"code","b3c48d07":"code","7731362b":"code","deef4b4e":"code","0bfaf167":"code","05d0689d":"code","29aae79f":"code","a2238bcc":"code","bfa8a51a":"code","14f4db69":"code","de032e62":"code","5e22feba":"code","013d3497":"code","68318c31":"code","05d32a46":"markdown","558c48a0":"markdown","d66e35d3":"markdown","94cad0af":"markdown","91f47dd3":"markdown","78f9f85d":"markdown","0ac9d86c":"markdown","741c7882":"markdown","7db8d990":"markdown","a0c91519":"markdown","4ee6ea58":"markdown","d1b81998":"markdown","5f8169db":"markdown","53db93d9":"markdown","72a918c9":"markdown","fa6bd12c":"markdown","40535756":"markdown"},"source":{"7e49f58b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","4dc95fb7":"df= pd.read_csv('..\/input\/titanicdataset\/titanic.csv')","cf5c2d37":"df.info()","71fe3297":"# Droping useless columns\n\ndf = df.drop(['PassengerId', 'Name', 'Ticket'],axis=1)","ddec788f":"#Calculating the percentage of missing data in each columns (feature) and then sort it\ndef missing_percentage(df):\n    nan_percent= 100*(df.isnull().sum()\/len(df))\n    nan_percent= nan_percent[nan_percent>0].sort_values()\n    return nan_percent\nnan_percent= missing_percentage(df)\nprint(nan_percent)","c5d6882c":"plt.figure(figsize=(12,6))\nsns.barplot(x=nan_percent.index, y=nan_percent, color=(0.2, 0.4, 0.6, 0.6), edgecolor='blue')\nplt.xticks(rotation=90)","99cb032f":"# Droping the Cabin feature beacuse of its high rate of missing values\n\ndf = df.drop(['Cabin'],axis=1)","2baed34c":"# Droping rows with missing values\n\ndf = df.dropna(subset = [\"Embarked\"])\ndf = df.dropna(subset = [\"Fare\"])","269caf8f":"# Filling missing values related to Age with its mean value\n\ndf['Age'] = df.groupby(['Sex'])['Age'].apply(lambda x: x.fillna(x.mean()))","7e889c87":"sns.scatterplot(data=df,x='Fare', y='Survived')\nplt.axvline(x=400,color='r')","d14aa45f":"# Eliminating the outliers\n\nindex_drop=df[(df['Fare']>300) & (df['Survived']==1)].index\ndf = df.drop(index_drop, axis=0)","cc43bd64":"\nsns.scatterplot(data=df,x='Fare', y='Survived')\nplt.axvline(x=400,color='r')","918c5d5f":"# The Correlation overview\n\ndf.corr()['Survived'].sort_values()","1495f41b":"df.corr()['Survived'].sort_values()","79a78a3e":"df = df.drop(['SibSp'],axis=1)","f0216b77":"df.info()","319021f3":"df['Survived'].value_counts()","ba72b9d8":"sns.countplot(data=df, x='Survived')","47a7b268":"sns.boxplot(data=df, x='Survived', y='Pclass')     ","dd183f93":"fig= plt.figure(figsize=(8,8), dpi=800)\nsns.pairplot(df, hue='Survived')","b4966f14":"fig= plt.figure(figsize=(4,4), dpi=300)\nsns.heatmap(df.corr(), annot=True)","a220fde2":"df_num= df.select_dtypes(exclude='object')\ndf_obj= df.select_dtypes(include='object')","310c80e8":"X= df_num.drop('Survived', axis=1)\ny= df_num['Survived']","6f01a61a":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","2787efb8":"from sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X_train)\nscaled_X_train= scaler.transform(X_train)\nscaled_X_test= scaler.transform(X_test)","f8ce03f8":"from sklearn.linear_model import LogisticRegression\nlog_model= LogisticRegression()\nlog_model.fit(scaled_X_train, y_train)","0fd12803":"#Model Coeficient:\nlog_model.coef_","a34b0cfc":"y_pred= log_model.predict(scaled_X_test)","b3c48d07":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","7731362b":"accuracy_score(y_test, y_pred)","deef4b4e":"confusion_matrix(y_test, y_pred)","0bfaf167":"plot_confusion_matrix(log_model, scaled_X_test, y_test)","05d0689d":"print(classification_report(y_test, y_pred))","29aae79f":"# Scaling the Features\n\nfrom sklearn.preprocessing import StandardScaler\nscaler= StandardScaler()\nscaler.fit(X)\nscaled_X= scaler.transform(X)","a2238bcc":"from sklearn.linear_model import LogisticRegressionCV\nlog_model2= LogisticRegressionCV(cv=5, random_state=101).fit(scaled_X, y) \ny_pred= log_model2.predict(scaled_X)","bfa8a51a":"from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, plot_confusion_matrix","14f4db69":"accuracy_score(y, y_pred)","de032e62":"confusion_matrix(y, y_pred)","5e22feba":"plot_confusion_matrix(log_model2, scaled_X, y)","013d3497":"print(classification_report(y, y_pred))","68318c31":"from sklearn.metrics import plot_roc_curve\nfigure1 = plot_roc_curve(log_model2, scaled_X, y)\nfigure2 = plot_roc_curve(log_model, scaled_X_test, y_test, ax=figure1.ax_)\nplt.rcParams['figure.dpi'] = 300\nplt.rcParams['figure.figsize'] = [5, 3]","05d32a46":"### Comparing LogisticRegression function with LogisticRegressionCV function","558c48a0":"# Step 3: Data Preparation","d66e35d3":"Logistic Regression transforms a Linear Regression into classification model using the below equation:\n\n$\\sigma (x) = 1\/(1 + e^{-x})$\n\nHence, the output always lays between 0 and 1.","94cad0af":"# Step 6: Determining the Features & Target Variable","91f47dd3":"# Step 10: Predicting Test Data","78f9f85d":"\n\nAs can be observed, passengers that have survived are virtually younger than those that could not survive.","0ac9d86c":"The below Table provides information about the dataset.\n\nVariable           | Definition                                              | Key\n-------------------|------------------ ------------------ ------------------ | ------------------\nPassengerId        | Passenger Id                                            | \nSurvived           | Survival people                                         | 0 = No, 1 = Yes\nPclass             | Ticket class                                            | 1 = 1st, 2 = 2nd, 3 = 3rd\nName               | Name                                                    |\nSex                | Sex                                                     |\nAge                | Age                                                     |\nSibSp              | # of siblings \/ spouses aboard the Titanic              |\nParch              | # of parents \/ children aboard the Titanic              |\nTicket             | Ticket number                                           |\nFare               | Passenger fare                                          |\nCabin              | Cabin number                                            |\nEmbarked           | Port of Embarkation                                     | C = Cherbourg, Q = Queenstown, S = Southampton\n\n\n### Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","741c7882":"It was witnessed that people with 1st class ticket have survived.","7db8d990":"# Step 4: Exploratory Data Analysis","a0c91519":"# Step 12: Training the Model (Logistic Regression using cross validation)","4ee6ea58":"# Step 2: Importing the titanic dataset\n","d1b81998":"# Step 9: Training the Model (Logistic Regression)","5f8169db":"# Step 7: Spliting the Dataset to Tain & Test set","53db93d9":"# Step 5: Spliting feature into object and number categories","72a918c9":"# Step 11: Evaluating the Model","fa6bd12c":"# Step 1: Importing required libraries","40535756":"# Step 8: Scaling the Features"}}