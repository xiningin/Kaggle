{"cell_type":{"265d51d2":"code","976284d7":"code","0f0c7417":"code","1f801123":"code","2330a5c3":"code","0081cc80":"code","ebc74de7":"code","b65c7612":"code","edd28693":"code","fb08007e":"code","0304d8ec":"code","3ae671e5":"code","cf1ce522":"code","7536917f":"code","efb2fa31":"code","64cfba4f":"code","6a58e523":"code","9739f36b":"code","bf6b3f3c":"code","fca62a56":"code","de743230":"code","f4ca76f3":"code","c1494450":"code","d4b5e704":"code","2cc4b1ab":"code","4c4bf00f":"code","a523063b":"code","0e4e846c":"code","6fce277e":"code","016afab4":"code","6aab49d3":"code","31aa0563":"code","1d1b4b97":"code","f2887eca":"code","3cb5284a":"code","8b92c3e1":"code","ef8f52b8":"code","6952ad2c":"code","83648f76":"code","bcfd79fe":"markdown","5c9f892f":"markdown","86cf76d7":"markdown","4a19e391":"markdown","af9d9441":"markdown","b157c13d":"markdown","559e097e":"markdown","7fb55f92":"markdown","15d04f81":"markdown","b96b811a":"markdown","db001335":"markdown","5a5a05bb":"markdown","46459ed5":"markdown","f5ea1818":"markdown","bf7c548e":"markdown"},"source":{"265d51d2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","976284d7":"! pip install xgboost","0f0c7417":"df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","1f801123":"df.info()","2330a5c3":"df_test.info()","0081cc80":"low_data = ['MSSubClass','MSZoning','LandContour','Street','Alley','Utilities','LotConfig','LandSlope','Condition1','Condition2','BldgType','RoofStyle','RoofMatl','ExterCond','BsmtCond','BsmtExposure','BsmtFinType2','BsmtFinSF2','Heating','CentralAir','Electrical','Functional','GarageQual','GarageCond','PavedDrive','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','FireplaceQu','PoolQC','Fence','MiscVal','MiscFeature','MasVnrType','SaleType','SaleCondition']\ndf = df.drop(columns = low_data)\ndf_test = df_test.drop(columns = low_data)","ebc74de7":"df.info()","b65c7612":"cats = ['LotShape','Neighborhood','HouseStyle','Exterior1st','Exterior2nd','ExterQual' ,'Foundation','BsmtQual','BsmtFinType1','HeatingQC','KitchenQual','GarageType','GarageFinish']","edd28693":"df[cats] = df[cats].astype('string')\ndf[cats] = df[cats].astype('string')","fb08007e":"from sklearn import preprocessing\n\n\nclass CategoricalFeatures:\n    def __init__(self, df, categorical_features, encoding_type, handle_na=False):\n        \"\"\"\n        df: pandas dataframe\n        categorical_features: list of column names, e.g. [\"ord_1\", \"nom_0\"......]\n        encoding_type: label, binary, ohe\n        handle_na: True\/False\n        \"\"\"\n        self.df = df\n        self.cat_feats = categorical_features\n        self.enc_type = encoding_type\n        self.handle_na = handle_na\n        self.label_encoders = dict()\n        self.binary_encoders = dict()\n        self.ohe = None\n\n        if self.handle_na:\n            for c in self.cat_feats:\n                self.df.loc[:, c] = self.df.loc[:, c].astype(str).fillna(\"-9999999\")\n        self.output_df = self.df.copy(deep=True)\n    \n    def _label_encoding(self):\n        for c in self.cat_feats:\n            lbl = preprocessing.LabelEncoder()\n            lbl.fit(self.df[c].values)\n            self.output_df.loc[:, c] = lbl.transform(self.df[c].values)\n            self.label_encoders[c] = lbl\n        return self.output_df\n    \n    def _label_binarization(self):\n        for c in self.cat_feats:\n            lbl = preprocessing.LabelBinarizer()\n            lbl.fit(self.df[c].values)\n            val = lbl.transform(self.df[c].values)\n            self.output_df = self.output_df.drop(c, axis=1)\n            for j in range(val.shape[1]):\n                new_col_name = c + f\"__bin_{j}\"\n                self.output_df[new_col_name] = val[:, j]\n            self.binary_encoders[c] = lbl\n        return self.output_df\n\n    def _one_hot(self):\n        ohe = preprocessing.OneHotEncoder()\n        ohe.fit(self.df[self.cat_feats].values)\n        return ohe.transform(self.df[self.cat_feats].values)\n\n    def fit_transform(self):\n        if self.enc_type == \"label\":\n            return self._label_encoding()\n        elif self.enc_type == \"binary\":\n            return self._label_binarization()\n        elif self.enc_type == \"ohe\":\n            return self._one_hot()\n        else:\n            raise Exception(\"Encoding type not understood\")\n    \n    def transform(self, dataframe):\n        if self.handle_na:\n            for c in self.cat_feats:\n                dataframe.loc[:, c] = dataframe.loc[:, c].astype(str).fillna(\"-9999999\")\n\n        if self.enc_type == \"label\":\n            for c, lbl in self.label_encoders.items():\n                dataframe.loc[:, c] = lbl.transform(dataframe[c].values)\n            return dataframe\n\n        elif self.enc_type == \"binary\":\n            for c, lbl in self.binary_encoders.items():\n                val = lbl.transform(dataframe[c].values)\n                dataframe = dataframe.drop(c, axis=1)\n                \n                for j in range(val.shape[1]):\n                    new_col_name = c + f\"__bin_{j}\"\n                    dataframe[new_col_name] = val[:, j]\n            return dataframe\n\n        elif self.enc_type == \"ohe\":\n            return self.ohe(dataframe[self.cat_feats].values)\n        \n        else:\n            raise Exception(\"Encoding type not understood\")\n\n\n\ndf_test[\"SalePrice\"] = -1\nfull_data = pd.concat([df, df_test])\n\n#cols = [c for c in SaleCondition   if c not in [\"id\", \"target\"]]\n\ncat_feats = CategoricalFeatures(full_data, \n                                categorical_features=cats, \n                                encoding_type=\"label\",\n                                handle_na=True)\nfull_data_transformed = cat_feats.fit_transform()\n\nX = full_data_transformed[:len(df)]\nX_test = full_data_transformed[len(df):]","0304d8ec":"df.columns","3ae671e5":"X.info()","cf1ce522":"##filling remaining NULL\/Nan  values and convert to float16\nX = X.fillna(X.mean())\nX_test = X_test.fillna(X_test.mean())","7536917f":"scaling_cols =['LotFrontage', 'LotArea', 'LotShape', 'Neighborhood',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'Exterior1st', 'Exterior2nd', 'MasVnrArea', 'ExterQual', 'Foundation',\n       'BsmtQual', 'BsmtFinType1', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF',\n       'HeatingQC', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF', 'GrLivArea',\n       'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath', 'BedroomAbvGr',\n       'KitchenAbvGr', 'KitchenQual', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageType', 'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea',\n       'WoodDeckSF', 'OpenPorchSF', 'MoSold', 'YrSold']","efb2fa31":"from sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX[scaling_cols] = rs.fit_transform(X[scaling_cols])\nX_test[scaling_cols] = rs.transform(X_test[scaling_cols])\n# from sklearn.preprocessing import StandardScaler\n# sc = StandardScaler()\n# X[scaling_cols] = sc.fit_transform(X[scaling_cols])\n# X_test[scaling_cols] = sc.transform(X_test[scaling_cols])\n","64cfba4f":"len(X.columns)","6a58e523":"X_test = X_test.drop(columns = ['SalePrice'])","9739f36b":"##################  finding best so temp vars\ntemp_X = X.drop(columns = 'SalePrice')\ntemp_y = X['SalePrice']","bf6b3f3c":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nreg1 = RandomForestRegressor()\nparam_grid = [\n               {'bootstrap': [False],'n_estimators': [3,10,30,90,200, 300, 600],\n                'max_features':['auto'],\n                'max_depth': [20, 30,None],\n                'min_samples_leaf': [1, 2],\n                'min_samples_split': [2, 5]}\n               ]\n\ngrid_search = GridSearchCV(reg1,param_grid,cv = 5,scoring = 'neg_mean_squared_log_error')","fca62a56":"grid_search.fit(temp_X,temp_y)","de743230":"grid_search.best_estimator_","f4ca76f3":"# RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n#                       max_depth=None, max_features='auto', max_leaf_nodes=None,\n#                       max_samples=None, min_impurity_decrease=0.0,\n#                       min_impurity_split=None, min_samples_leaf=2,\n#                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n#                       n_estimators=30, n_jobs=None, oob_score=False,\n#                       random_state=None, verbose=0, warm_start=False)","c1494450":"from sklearn.ensemble import ExtraTreesRegressor\nreg2 = ExtraTreesRegressor()\nparam_grid = [\n               {'bootstrap': [True,False],'n_estimators': [3,10,30,90,200, 300, 600],\n                'max_features':['auto'],\n                'max_depth': [10, 20,None],\n                'min_samples_leaf': [1, 2, 4],\n                'min_samples_split': [2, 5],\n                'oob_score':[False]}\n               ]\ngrid_search = GridSearchCV(reg2,param_grid,cv = 5,scoring = 'neg_mean_squared_log_error')","d4b5e704":"grid_search.fit(temp_X,temp_y)","2cc4b1ab":"grid_search.best_estimator_","4c4bf00f":"# ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n#                     max_depth=20, max_features='auto', max_leaf_nodes=None,\n#                     max_samples=None, min_impurity_decrease=0.0,\n#                     min_impurity_split=None, min_samples_leaf=1,\n#                     min_samples_split=5, min_weight_fraction_leaf=0.0,\n#                     n_estimators=600, n_jobs=None, oob_score=False,\n#                     random_state=None, verbose=0, warm_start=False)\n","a523063b":"from sklearn.ensemble import AdaBoostRegressor\nreg3 = AdaBoostRegressor()\nparam_grid = [\n               {'n_estimators': [3,10,30,90,200, 300, 600, 800, 1000, 1200, 1400],\n                'learning_rate':[0.0001, 0.001, 0.01, 0.1, 1.0]}\n               ]\n\ngrid_search = GridSearchCV(reg3,param_grid,cv = 5,scoring = 'neg_mean_squared_log_error')","0e4e846c":"grid_search.fit(temp_X,temp_y)","6fce277e":"grid_search.best_estimator_","016afab4":"# AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='linear',\n#                   n_estimators=300, random_state=None)","6aab49d3":"import xgboost as xg\nreg4 = xg.XGBRegressor()\nparam_grid = {'min_child_weight':[4,5], 'gamma':[i\/10.0 for i in range(3,6)],  'subsample':[i\/10.0 for i in range(6,11)],\n'colsample_bytree':[i\/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\ngrid_search = GridSearchCV(reg4,param_grid,cv = 5,scoring = 'neg_mean_squared_log_error')","31aa0563":"grid_search.fit(temp_X,temp_y)","1d1b4b97":"grid_search.best_estimator_","f2887eca":"# XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n#              colsample_bynode=1, colsample_bytree=0.8, gamma=0.3,\n#              importance_type='gain', learning_rate=0.1, max_delta_step=0,\n#              max_depth=4, min_child_weight=4, missing=None, n_estimators=100,\n#              n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n#              reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n#              silent=None, subsample=1.0, verbosity=1)","3cb5284a":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nimport xgboost as xg\nreg1 = RandomForestRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                      max_depth=None, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=2,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      n_estimators=30, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=0, warm_start=False)\n\nreg2 = ExtraTreesRegressor(bootstrap=False, ccp_alpha=0.0, criterion='mse',\n                    max_depth=20, max_features='auto', max_leaf_nodes=None,\n                    max_samples=None, min_impurity_decrease=0.0,\n                    min_impurity_split=None, min_samples_leaf=1,\n                    min_samples_split=5, min_weight_fraction_leaf=0.0,\n                    n_estimators=600, n_jobs=None, oob_score=False,\n                    random_state=None, verbose=0, warm_start=False)\n\nreg3 = AdaBoostRegressor(base_estimator=None, learning_rate=0.1, loss='linear',\n                  n_estimators=300, random_state=None)\n\nreg4 = xg.XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.8, gamma=0.3,\n             importance_type='gain', learning_rate=0.1, max_delta_step=0,\n             max_depth=4, min_child_weight=4, missing=None, n_estimators=100,\n             n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1.0, verbosity=1)\n\nreg1.fit(temp_X,temp_y)\nreg2.fit(temp_X,temp_y)\nreg3.fit(temp_X,temp_y)\nreg4.fit(temp_X,temp_y)\n\npred1 = reg1.predict(X_test)\npred2 = reg2.predict(X_test)\npred3 = reg3.predict(X_test)\npred4 = reg4.predict(X_test)\n\npred_avg = (pred1 + pred2 + pred3 + pred4)\/4\n","8b92c3e1":"pred1","ef8f52b8":"submit = pd.read_csv('\/content\/drive\/MyDrive\/house-price\/sample_submission.csv')","6952ad2c":"i = 0\nfor ans in pred_avg:\n    submit.SalePrice[i] = ans\n    i = i + 1","83648f76":"submit.to_csv('submission_avg.csv')","bcfd79fe":"### Scores  Mean Sq log Error\nmodel 1 Random forrests = 0.20992 \n\n\n---\n\n\nmodel 2 Extra trees = 0.14494\n\n\n---\n\n\nmodel 3 Adaboost = 0.20521\n\n\n---\n\nmodel 4 Xgboost = 0.13636\n\n\n---\nmodel avg = 0.15602\n\n---\nmodel 2-4 Xgboost + Extratrees = 0.13573\n","5c9f892f":"## DATA PLAY\n","86cf76d7":"## Trying some new models and exploring through Ensemble and averaging preds directly to only check the differences ...Only an Experiment with Ensembles\n\n# Got inspiration from medium article : https:\/\/towardsdatascience.com\/ensemble-models-5a62d4f4cb0c\n\nMore to do and apply in other notebooks,\n\n\n### Scores  Mean Sq log Error\nmodel 1 Random forrests = 0.20992 \n---\nmodel 2 Extra trees = 0.14494\n---\nmodel 3 Adaboost = 0.20521\n---\nmodel 4 Xgboost = 0.13636\n---\nmodel avg = 0.15602\n---\nmodel 2-4 Xgboost + Extratrees = 0.13573","4a19e391":"### Scores  Mean Sq log Error\nmodel 1 Random forrests = 0.20992 \n\n\n---\n\n\nmodel 2 Extra trees = 0.14494\n\n\n---\n\n\nmodel 3 Adaboost = 0.20521\n\n\n---\n\nmodel 4 Xgboost = 0.13636\n\n\n---\nmodel avg = 0.15602\n\n---\nmodel 2-4 Xgboost + Extratrees = 0.13573","af9d9441":"## 2.Extra Trees Regressor","b157c13d":"## 3. AdaBoost Regressor","559e097e":"### SCaLe Laid Data and Fill laid  the DATASES","7fb55f92":"## 1.   First Random Forests","15d04f81":"## Ur  Robusted Boiz","b96b811a":"## Grid Search and Model creation ....       How you can practice you new moves ... the META-Models secrets are not in books anymore!","db001335":"## 4. XGBoost Regressor","5a5a05bb":"## asdfghjkjhgfqwertyuiopasdfghjkl;zxcvbnm,.\/","46459ed5":"## 'Cats' moving all over the place--- Strings wont help ... need of numbers now!","f5ea1818":"## EDA and Data management","bf7c548e":"import xgboost as xg"}}