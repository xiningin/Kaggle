{"cell_type":{"b0bd8bb0":"code","9b877d5a":"code","dcaa1a5d":"code","cf6c5cb5":"code","4940190a":"code","94e6bbe6":"code","b5059e09":"code","a5edd59c":"code","22c12c86":"code","ce80eaf7":"code","dcaf87f9":"code","b004b407":"code","c68c1332":"code","3275f6d9":"code","db1babf5":"code","d31b1526":"code","1868ffdb":"code","96fb9a76":"code","12c3f5c7":"code","f398830b":"code","2495073b":"code","d1716f7b":"code","b37051a7":"code","6e112f7a":"code","af478884":"code","40a920cb":"code","a4bb205d":"code","a6682713":"code","74fda7ea":"code","8435029e":"code","524d2405":"code","b0e11718":"code","160899f6":"code","56a20d25":"code","bd7e8581":"code","6e1b0e0e":"code","9678ed16":"code","231f7531":"code","f5efaf6a":"code","d00ceed5":"code","800dedbf":"code","e51ce3f2":"code","a008d2a3":"code","9370dcac":"code","5857b66a":"code","cfe7a8b1":"code","e7d7c0fa":"code","0fd0b365":"code","d5416eed":"code","f34a26ad":"code","56c60000":"code","ffe18ef0":"code","8cdd424d":"code","4c898771":"code","47d4b61e":"code","d04680e0":"markdown","fe4de3ae":"markdown","42c7122a":"markdown","91f1855b":"markdown","2daf9d86":"markdown","223b2936":"markdown","d1697492":"markdown","c01e0fa6":"markdown","f5c5a30e":"markdown","45ebcb07":"markdown","947b8ad9":"markdown","d65ec45d":"markdown","420f8ff3":"markdown","8610e46a":"markdown","a02c6d41":"markdown","c1704a67":"markdown","74c1e565":"markdown","b03dc1a3":"markdown","e80c8b47":"markdown","529de31b":"markdown","3df73b8a":"markdown","2f56eee2":"markdown","33c7c033":"markdown","a2ae1e52":"markdown","a06f5811":"markdown","ff3edb3b":"markdown","5dcb8bb2":"markdown","7fc00c8f":"markdown","5f770c3b":"markdown","a5ca8a15":"markdown","d4be6b41":"markdown","806c89dc":"markdown"},"source":{"b0bd8bb0":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.patches as mpatches\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport time\n\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA, TruncatedSVD\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report\nfrom sklearn.model_selection import GridSearchCV, ShuffleSplit, learning_curve, cross_val_predict, RandomizedSearchCV\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, StratifiedShuffleSplit, cross_val_score\n\n# imblearn\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\n\nfrom collections import Counter\nimport collections\nimport os\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\npd.options.display.float_format = '{:,.4f}'.format\n\ndf = pd.read_csv(\"..\/input\/creditcardfraud\/creditcard.csv\")\ndf.head()","9b877d5a":"def eda_categ_feat_desc_plot(series_categorical, title = \"\"):\n    \"\"\"Generate 2 plots: barplot with quantity and pieplot with percentage. \n       @series_categorical: categorical series\n       @title: optional\n    \"\"\"\n    series_name = series_categorical.name\n    val_counts = series_categorical.value_counts()\n    val_counts.name = 'quantity'\n    val_percentage = series_categorical.value_counts(normalize=True)\n    val_percentage.name = \"percentage\"\n    val_concat = pd.concat([val_counts, val_percentage], axis = 1)\n    val_concat.reset_index(level=0, inplace=True)\n    val_concat = val_concat.rename( columns = {'index': series_name} )\n    \n    fig, ax = plt.subplots(figsize = (12,4), ncols=2, nrows=1) # figsize = (width, height)\n    if(title != \"\"):\n        fig.suptitle(title, fontsize=18)\n        fig.subplots_adjust(top=0.8)\n\n    s = sns.barplot(x=series_name, y='quantity', data=val_concat, ax=ax[0])\n    for index, row in val_concat.iterrows():\n        s.text(row.name, row['quantity'], row['quantity'], color='black', ha=\"center\")\n\n    s2 = val_concat.plot.pie(y='percentage', autopct=lambda value: '{:.2f}%'.format(value),\n                             labels=val_concat[series_name].tolist(), legend=None, ax=ax[1],\n                             title=\"Percentage Plot\")\n\n    ax[1].set_ylabel('')\n    ax[0].set_title('Quantity Plot')\n\n    plt.show()","dcaa1a5d":"import itertools\n\n# Create a confusion matrix\ndef plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=14)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","cf6c5cb5":"print(\"Columns List:\")\nprint(list(df.columns))","4940190a":"eda_categ_feat_desc_plot(df['Class'], 'Rows By Class: No Fraud = 0 | Fraud = 1')","94e6bbe6":"df.describe().T","b5059e09":"import missingno as msno\nsns.heatmap(df.isnull(), cbar=False)","a5edd59c":"# Good No Null Values!\ndf.isnull().sum().max()","22c12c86":"fig, ax = plt.subplots(1, 2, figsize=(18,4))\n\namount_val = df['Amount'].values\ntime_val = df['Time'].values\n\nsns.distplot(amount_val, ax=ax[0], color='r')\nax[0].set_title('Distribution of Transaction Amount', fontsize=14)\nax[0].set_xlim([min(amount_val), max(amount_val)])\n\nsns.distplot(time_val, ax=ax[1], color='b')\nax[1].set_title('Distribution of Transaction Time', fontsize=14)\nax[1].set_xlim([min(time_val), max(time_val)])\n\nplt.show()","ce80eaf7":"f, (ax1, ax2) = plt.subplots(ncols=2,  figsize=(16, 5), sharex=False)\n\nmap_feat_ax = {'Amount': ax1, 'Time': ax2}\n\nfor key, value in map_feat_ax.items():\n    sns.boxplot(x=df[key], ax=value)\n    \nf.suptitle(\"Box Plot to 'Amount' and 'Time'\", fontsize=18)\n    \nplt.show()","dcaf87f9":"# Show Distriupution of Time and Amount\n\nlist_columns = ['Amount', 'Time']\n\nlist_describes = []\nfor f in list_columns:\n    list_describes.append(df[f].describe())\n\ndf_describes = pd.concat(list_describes, axis = 1)\ndf_describes  ","b004b407":"from sklearn.preprocessing import StandardScaler, RobustScaler\n\nrob_scaler = RobustScaler() # Reduce influence of outliers in scaling using IQR (Inter Quartile Range)\n\ndf['Amount'] = rob_scaler.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time'] = rob_scaler.fit_transform(df['Time'].values.reshape(-1,1))\n\ndf.head(1)","c68c1332":"f, (ax1, ax2) = plt.subplots(ncols=2,  figsize=(16, 5), sharex=False)\n\nmap_feat_ax = {'Amount': ax1, 'Time': ax2}\n\nfor key, value in map_feat_ax.items():\n    sns.boxplot(x=df[key], ax=value)\n    \nf.suptitle(\"Box Plot to 'Amount' and 'Time' after Scaled\", fontsize=18)\n    \nplt.show()","3275f6d9":"from sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.model_selection import KFold, StratifiedKFold\n\n# Separate dataset in train test\nX = df.drop('Class', axis=1)\ny = df['Class']\n\n# Separa os dados de maneira estratificada (mantendo as propor\u00e7\u00f5es originais)\nsss = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n\n# De forma iterativa, no final teremos separao 20% para teste e 80% para treinamento\nfor train_index, test_index in sss.split(X, y):\n    original_Xtrain, original_Xtest = X.iloc[train_index].values, X.iloc[test_index].values\n    original_ytrain, original_ytest = y.iloc[train_index].values, y.iloc[test_index].values\n    \n# calculate to check if the 2 sub-set (train,test) have the smae proportion of rows with classes 0 (No fraud) and 1 (fraud)\ntrain_unique_label, train_counts_label = np.unique(original_ytrain, return_counts=True)\ntest_unique_label, test_counts_label = np.unique(original_ytest, return_counts=True)\nprop_train = train_counts_label\/ len(original_ytrain)\nprop_test = test_counts_label\/ len(original_ytest)\noriginal_size = len(X)\n\n# Print Restult to cofirm that has the same proportion\nprint(\"Split DataFrame in Train and Test\\n\")\nprint(\"Original Size:\", '{:,d}'.format(original_size))\nprint(\"\\nTrain: must be 80% of dataset:\\n\", \n      \"X_train:\", '{:,d}'.format(len(original_Xtrain)), '{:.2%}'.format(len(original_Xtrain)\/original_size),\n      \"| y_train:\", '{:,d}'.format(len(original_ytrain)), '{:.2%}'.format(len(original_ytrain)\/original_size),\n            \"\\n => Classe 0 (No Fraud):\", train_counts_label[0],  '{:.2%}'.format(prop_train[0]), \n            \"\\n => Classe 1 (Fraud):   \", train_counts_label[1], '{:.2%}'.format(prop_train[1]),\n      \"\\n\\nTest: must be 20% of dataset:\\n\",\n      \"X_test:\", '{:,d}'.format(len(original_Xtest)), '{:.2%}'.format(len(original_Xtest)\/original_size),\n      \"| y_test:\", '{:,d}'.format(len(original_ytest)), '{:.2%}'.format(len(original_ytest)\/original_size),\n              \"\\n => Classe 0 (No Fraud)\", test_counts_label[0], '{:.2%}'.format(prop_test[0]),\n              \"\\n => Classe 1 (Fraud)   \",test_counts_label[1], '{:.2%}'.format(prop_test[1])\n     )","db1babf5":"# frac = 1 means all data\ndf = df.sample(frac=1)\n\n# amount of fraud classes 492 rows.\nfraud_df = df.loc[df['Class'] == 1] # df where is class = 1\nnon_fraud_df = df.loc[df['Class'] == 0][:492] # df where class = 0 (no fraud) limited by coutn masx of fraude 492\n\n# join 2 df to make a datacet balanced\nnormal_distributed_df = pd.concat([fraud_df, non_fraud_df])\n\n# Shuffle dataframe rows to shrunbel y=1 and y=0 (else will bi sorted (thasi is abnormal))\nnew_df = normal_distributed_df.sample(frac=1, random_state=42)\n\nnew_df.shape","d31b1526":"eda_categ_feat_desc_plot(new_df['Class'], 'Random Under-Sampling: to correct unbalanced  (Fraud = 1; No Fraud = 0)')","1868ffdb":"f, (ax1, ax2) = plt.subplots(2, 1, figsize=(24,20))\n\n# Entire DataFrame (Unbalanced)\ncorr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, cmap='coolwarm_r', annot=True, annot_kws={'size':8}, ax=ax1, mask=mask)\nax1.set_title(\"Imbalanced Correlation Matrix \\n (don't use for reference)\", fontsize=14)\n\n# HeatMap to new_df (Balanced)\nsub_sample_corr = new_df.corr()\n\nmask = np.zeros_like(sub_sample_corr)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot=True, annot_kws={'size':8}, ax=ax2, mask=mask)\nax2.set_title('SubSample Correlation Matrix \\n (use for reference)', fontsize=14)\nplt.show()","96fb9a76":"# Generate Ranking of correlations (boht positives, negatives)\n\ncorr = new_df.corr() # Show greater correlations both negative and positive\ndict_to_rename = {0: \"value\", \"level_0\": \"feat1\", \"level_1\": \"feat2\"} # Rename DataFrame\ns = corr.unstack().reset_index().rename(dict_to_rename, axis = 1) # Restructure dataframe\n\ns['+|-'] = s['value']\ns['value'] = s['value'].abs()\n\n# remove rows thas like 'x' | 'x' \ns_to_drop = s[(s['feat1'] == s['feat2'])].index \ns = s.drop(s_to_drop).reset_index()\n\ns = s[ s['feat1'] == 'Class' ].sort_values(by=\"value\", ascending=False).drop(\"index\", axis=1) \n\n# Biggest correlation with class\ntop_int = 10\ns.head(top_int)","12c3f5c7":"f, (axes, axes2) = plt.subplots(ncols=4, nrows=2, figsize=(20,10))\n\ncolors = [\"#0101DF\", \"#DF0101\"]\n\n# Negative Correlations with our Class (The lower our feature value the more likely it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V17\", data=new_df, palette=colors, ax=axes[0])\naxes[0].set_title('V17 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V14\", data=new_df, palette=colors, ax=axes[1])\naxes[1].set_title('V14 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V12\", data=new_df, palette=colors, ax=axes[2])\naxes[2].set_title('V12 vs Class Negative Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V10\", data=new_df, palette=colors, ax=axes[3])\naxes[3].set_title('V10 vs Class Negative Correlation')\n\n# Positive correlations (The higher the feature the probability increases that it will be a fraud transaction)\nsns.boxplot(x=\"Class\", y=\"V11\", data=new_df, palette=colors, ax=axes2[0])\naxes2[0].set_title('V11 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V4\", data=new_df, palette=colors, ax=axes2[1])\naxes2[1].set_title('V4 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V2\", data=new_df, palette=colors, ax=axes2[2])\naxes2[2].set_title('V2 vs Class Positive Correlation')\n\nsns.boxplot(x=\"Class\", y=\"V19\", data=new_df, palette=colors, ax=axes2[3])\naxes2[3].set_title('V19 vs Class Positive Correlation')\n\nplt.show()","f398830b":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\ncolors = ['b', 'r']\n\n# V14\nsns.boxplot(x='Class', y='V14', data = new_df, ax=ax1, palette=colors)\nax1.set_title('Show outliers to V14', fontsize=14)\n\n# V12\nsns.boxplot(x='Class', y='V12', data = new_df, ax=ax2, palette=colors)\nax2.set_title('Show outliers to V12', fontsize=14)\n\n# V10\nsns.boxplot(x='Class', y='V10', data = new_df, ax=ax3, palette=colors)\nax3.set_title('Show outliers to V10', fontsize=14)\n\nplt.show()","2495073b":"# Remover os outliers de V14 (correla\u00e7\u00e3o negativa alta com a classe)\nv14_fraud = new_df['V14'].loc[new_df['Class'] == 1].values\n# Valores do quartil 25 e quartil 75\nq25, q75 = np.percentile(v14_fraud, 25), np.percentile(v14_fraud, 75)\nprint('QUARTIL 25: {} | QUARTIL 75: {}'.format(q25, q75))\n# Interquartile range\nv14_iqr = q75 - q25\nprint('IQR: ', v14_iqr)\n\n# Limiar\nv14_cut_off = v14_iqr * 1.5\n# Limite superior e inferior\nv14_lower, v14_upper = q25 - v14_cut_off, q75 + v14_cut_off\nprint('LIMIAR: ', v14_cut_off)\nprint('V14 LIMITE INFERIOR', v14_lower)\nprint('V14 LIMITE SUPERIOR', v14_upper)\n\n# Ouliers (fora os limites estabelecidos anteriormente)\noutliers = [x for x in v14_fraud if x < v14_lower or x > v14_upper]\nprint('V14 QUANTIDADE DE OUTLIERS EM FRAUDES:', len(outliers))\n\n# Novo dataframe sem os outliers\nnew_df = new_df.drop(new_df[(new_df['V14'] > v14_upper) | (new_df['V14'] < v14_lower)].index)\nprint('----' * 20)\n\n\n# Remover os outliers de V12\nv12_fraud = new_df['V12'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v12_fraud, 25), np.percentile(v12_fraud, 75)\nv12_iqr = q75 - q25\n\nv12_cut_off = v12_iqr * 1.5\nv12_lower, v12_upper = q25 - v12_cut_off, q75 + v12_cut_off\nprint('V12 LIMITE INFERIOR: {}'.format(v12_lower))\nprint('V12 LIMITE SUPERIOR: {}'.format(v12_upper))\n\noutliers = [x for x in v12_fraud if x < v12_lower or x > v12_upper]\n\nprint('V12 OUTLIERS: {}'.format(outliers))\nprint('V12 QUANTIDADE DE OUTLIERS EM FRAUDES: {}'.format(len(outliers)))\n\nnew_df = new_df.drop(new_df[(new_df['V12'] > v12_upper) | (new_df['V12'] < v12_lower)].index)\nprint('N\u00daMERO DE INST\u00c2NCIAS AP\u00d3S A REMO\u00c7\u00c3O DOS OUTLIERS: {}'.format(len(new_df)))\nprint('----' * 20)\n\n\n# Remover os outliers de V10\n\nv10_fraud = new_df['V10'].loc[new_df['Class'] == 1].values\nq25, q75 = np.percentile(v10_fraud, 25), np.percentile(v10_fraud, 75)\nv10_iqr = q75 - q25\n\nv10_cut_off = v10_iqr * 1.5\nv10_lower, v10_upper = q25 - v10_cut_off, q75 + v10_cut_off\nprint('V10 LIMITE INFERIOR: {}'.format(v10_lower))\nprint('V10 SUPERIOR: {}'.format(v10_upper))\n\noutliers = [x for x in v10_fraud if x < v10_lower or x > v10_upper]\n\nprint('V10 OUTLIERS: {}'.format(outliers))\nprint('V10 QUANTIDAADE DE OUTLIERS EM FRAUDES: {}'.format(len(outliers)))\n\nnew_df = new_df.drop(new_df[(new_df['V10'] > v10_upper) | (new_df['V10'] < v10_lower)].index)\n\n\nprint('---' * 20)\nprint('N\u00daMERO DE INST\u00c2NCIAS AP\u00d3S A REMO\u00c7\u00c3O DOS OUTLIERS (Antes era 984): {}'.format(len(new_df)))","d1716f7b":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n\ncolors = ['b', 'r']\n\n# V14\nsns.boxplot(x='Class', y='V14', data = new_df, ax=ax1, palette=colors)\nax1.set_title('Show outliers to V14 after remove', fontsize=14)\n\n# V12\nsns.boxplot(x='Class', y='V12', data = new_df, ax=ax2, palette=colors)\nax2.set_title('Show outliers to V12 after remove', fontsize=14)\n\n# V10\nsns.boxplot(x='Class', y='V10', data = new_df, ax=ax3, palette=colors)\nax3.set_title('Show outliers to V10 after remove', fontsize=14)\n\n\nplt.show()","b37051a7":"# New_df is from the random undersample data (fewer instances)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# T-SNE Implementation: Tae a time in comparision with others techniques\nt0 = time.time()\nX_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"T-SNE took {:.2} s\".format(t1 - t0))\n\n# PCA Implementation\nt0 = time.time()\nX_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"PCA took {:.2} s\".format(t1 - t0))\n\n# TruncatedSVD\nt0 = time.time()\nX_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\nt1 = time.time()\nprint(\"Truncated SVD took {:.2} s\".format(t1 - t0))","6e112f7a":"f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\nf.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n\nblue_patch = mpatches.Patch(color='#0A0AFF', label='No Fraud')\nred_patch = mpatches.Patch(color='#AF0000', label='Fraud')\n\n# t-SNE scatter plot\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax1.set_title('t-SNE', fontsize=14)\nax1.grid(True)\nax1.legend(handles=[blue_patch, red_patch])\n\n# PCA scatter plot\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax2.set_title('PCA', fontsize=14)\nax2.grid(True)\nax2.legend(handles=[blue_patch, red_patch])\n\n# TruncatedSVD scatter plot\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No Fraud', linewidths=2)\nax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='Fraud', linewidths=2)\nax3.set_title('Truncated SVD', fontsize=14)\nax3.grid(True)\nax3.legend(handles=[blue_patch, red_patch])\n\n# In UnderSampling DataSet Balanced\nplt.show()","af478884":"# Undersampling before cross validating (prone to overfit)\nX = new_df.drop('Class', axis=1)\ny = new_df['Class']\n\n# Our data is already scaled we should split our training and test sets\nfrom sklearn.model_selection import train_test_split\n\n# This is explicitly used for undersampling.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Turn the values into an array for feeding the classification algorithms.\nX_train = X_train.values\nX_test = X_test.values\ny_train = y_train.values\ny_test = y_test.values","40a920cb":"# Let's implement simple classifiers\n\nclassifiers = {\n    \"LogisiticRegression\": LogisticRegression(),\n    \"KNearest\": KNeighborsClassifier(),\n    \"Support Vector Classifier\": SVC(),\n    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n}\n\n# Wow our scores are getting even high scores even when applying cross validation.\nfrom sklearn.model_selection import cross_val_score\n\n\nfor key, classifier in classifiers.items():\n    classifier.fit(X_train, y_train)\n    training_score = cross_val_score(classifier, X_train, y_train, cv=5)\n    print(\"Classifiers: \", classifier.__class__.__name__,\n          \"\\n\\tHas a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score on \")","a4bb205d":"# Use GridSearchCV to find the best parameters.\nfrom sklearn.model_selection import GridSearchCV\n\n# Logistic Regression Best Estimators\nlog_reg_params = {\"penalty\": ['l1', 'l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid_log_reg = GridSearchCV(LogisticRegression(), log_reg_params)\ngrid_log_reg.fit(X_train, y_train)\nlog_reg = grid_log_reg.best_estimator_\nprint('Best Loggistic Params:', log_reg)\n\n# KNears best estimator\nknears_params = {\"n_neighbors\": list(range(2,5,1)), \n                 'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n\ngrid_knears = GridSearchCV(KNeighborsClassifier(), knears_params)\ngrid_knears.fit(X_train, y_train)\nknears_neighbors = grid_knears.best_estimator_\nprint('Best KNears Params:', knears_neighbors)\n\n# Support Vector Classifier\nsvc_params = {'C': [0.5, 0.7, 0.9, 1], \n              'kernel': ['rbf', 'poly', 'sigmoid', 'linear']}\ngrid_svc = GridSearchCV(SVC(), svc_params)\ngrid_svc.fit(X_train, y_train)\nsvc = grid_svc.best_estimator_\nprint('Best SVM Params:', svc)\n\n# DecisionTree Classifier\ntree_params = {\"criterion\": [\"gini\", \"entropy\"],\n               \"max_depth\": list(range(2,4,1)), \n               \"min_samples_leaf\": list(range(5,7,1))}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), tree_params)\ngrid_tree.fit(X_train, y_train)\ntree_clf = grid_tree.best_estimator_\nprint('Best Tree Params:', tree_clf)","a6682713":"from sklearn.metrics import roc_curve\nfrom sklearn.model_selection import cross_val_predict\n# Create a DataFrame with all the scores and the classifiers names.\n\nlog_reg_pred = cross_val_predict(log_reg, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\nknears_pred = cross_val_predict(knears_neighbors, X_train, y_train, cv=5)\n\nsvc_pred = cross_val_predict(svc, X_train, y_train, cv=5,\n                             method=\"decision_function\")\n\ntree_pred = cross_val_predict(tree_clf, X_train, y_train, cv=5)\n\nfrom sklearn.metrics import roc_auc_score\n\nprint(\"ROC AUC SCORE\\n\")\n\nprint('Logistic Regression: ', roc_auc_score(y_train, log_reg_pred))\nprint('KNears Neighbors: ', roc_auc_score(y_train, knears_pred))\nprint('Support Vector Classifier: ', roc_auc_score(y_train, svc_pred))\nprint('Decision Tree Classifier: ', roc_auc_score(y_train, tree_pred))\n\nprint(\"\\nCROSS VALIDATION\\n\")\n\nlog_reg_score = cross_val_score(log_reg, X_train, y_train, cv=5)\nprint('Logistic Regression Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')\n\nknears_score = cross_val_score(knears_neighbors, X_train, y_train, cv=5)\nprint('Knears Neighbors Cross Validation Score', round(knears_score.mean() * 100, 2).astype(str) + '%')\n\nsvc_score = cross_val_score(svc, X_train, y_train, cv=5)\nprint('Support Vector Classifier Cross Validation Score', round(svc_score.mean() * 100, 2).astype(str) + '%')\n\ntree_score = cross_val_score(tree_clf, X_train, y_train, cv=5)\nprint('DecisionTree Classifier Cross Validation Score', round(tree_score.mean() * 100, 2).astype(str) + '%')","74fda7ea":"log_fpr, log_tpr, log_thresold = roc_curve(y_train, log_reg_pred)\nknear_fpr, knear_tpr, knear_threshold = roc_curve(y_train, knears_pred)\nsvc_fpr, svc_tpr, svc_threshold = roc_curve(y_train, svc_pred)\ntree_fpr, tree_tpr, tree_threshold = roc_curve(y_train, tree_pred)\n\n\ndef graph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr):\n    plt.figure(figsize=(16,8))\n    plt.title('ROC Curve \\n Top 4 Classifiers', fontsize=18)\n    plt.plot(log_fpr, log_tpr, label='Logistic Regression Classifier Score: {:.4f}'.format(roc_auc_score(y_train, log_reg_pred)))\n    plt.plot(knear_fpr, knear_tpr, label='KNears Neighbors Classifier Score: {:.4f}'.format(roc_auc_score(y_train, knears_pred)))\n    plt.plot(svc_fpr, svc_tpr, label='Support Vector Classifier Score: {:.4f}'.format(roc_auc_score(y_train, svc_pred)))\n    plt.plot(tree_fpr, tree_tpr, label='Decision Tree Classifier Score: {:.4f}'.format(roc_auc_score(y_train, tree_pred)))\n    plt.plot([0, 1], [0, 1], 'k--')\n    plt.axis([-0.01, 1, 0, 1])\n    plt.xlabel('False Positive Rate', fontsize=16)\n    plt.ylabel('True Positive Rate', fontsize=16)\n    plt.annotate('Minimum ROC Score of 50% \\n (This is the minimum score to get)', xy=(0.5, 0.5), xytext=(0.6, 0.3),\n                arrowprops=dict(facecolor='#6E726D', shrink=0.05),\n                )\n    plt.legend()\n    \ngraph_roc_curve_multiple(log_fpr, log_tpr, knear_fpr, knear_tpr, svc_fpr, svc_tpr, tree_fpr, tree_tpr)\nplt.show()","8435029e":"from sklearn.metrics import confusion_matrix\n\ny_pred = log_reg.predict(X_test)\ny_pred\n\nlabels = ['No Fraud', 'Fraud']\n\nconfusion_mtx = confusion_matrix(y_test, y_pred)\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(confusion_mtx, labels, title=\"Random UnderSample - Logistic Regression \\n Confusion Matrix\")\n","524d2405":"from sklearn.metrics import classification_report\n\n# Logistic Regression fitted using SMOTE technique\ny_pred_log_reg = log_reg.predict(X_test)\n\n# Other models fitted with UnderSampling\ny_pred_knear = knears_neighbors.predict(X_test)\ny_pred_svc = svc.predict(X_test)\ny_pred_tree = tree_clf.predict(X_test)\n\nlog_reg_cf = confusion_matrix(y_test, y_pred_log_reg)\nkneighbors_cf = confusion_matrix(y_test, y_pred_knear)\nsvc_cf = confusion_matrix(y_test, y_pred_svc)\ntree_cf = confusion_matrix(y_test, y_pred_tree)\n\nplt.show()\n\nprint('Logistic Regression:')\nprint(classification_report(y_test, y_pred_log_reg))\n\nprint('KNears Neighbors:')\nprint(classification_report(y_test, y_pred_knear))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_svc))\n\nprint('Support Vector Classifier:')\nprint(classification_report(y_test, y_pred_tree))","b0e11718":"# We will undersample during cross validating\nundersample_X = df.drop('Class', axis=1)\nundersample_y = df['Class']\n\n# Divide Original DataSet in Train and Test\nfor train_index, test_index in sss.split(undersample_X, undersample_y):\n    undersample_Xtrain, undersample_Xtest = undersample_X.iloc[train_index], undersample_X.iloc[test_index]\n    undersample_ytrain, undersample_ytest = undersample_y.iloc[train_index], undersample_y.iloc[test_index]\n    \nundersample_Xtrain = undersample_Xtrain.values\nundersample_Xtest = undersample_Xtest.values\nundersample_ytrain = undersample_ytrain.values\nundersample_ytest = undersample_ytest.values \n\nnp.unique(undersample_ytrain, return_counts=True), np.unique(undersample_ytest, return_counts=True)","160899f6":"from sklearn.metrics import roc_curve, roc_auc_score\nfrom sklearn.model_selection import cross_val_predict\n\n# Com o modelo treinado no undersampling dataset balanced, testamos no ytrain\nlog_reg_pred = cross_val_predict(log_reg, undersample_Xtrain, undersample_ytrain, cv=5,\n                             method=\"decision_function\")\n\n# Perceba, mesmo que tivermos scores altos, em tudo, somente analisando precision na \nprint(\"LOGISTIC REGRESSION\\n\")\n\nprint('ROC AUC SCORE: ', roc_auc_score(undersample_ytrain, log_reg_pred))\n\nlog_reg_score = cross_val_score(log_reg, undersample_Xtrain, undersample_ytrain, cv=5)\nprint('Cross Validation Score: ', round(log_reg_score.mean() * 100, 2).astype(str) + '%')","56a20d25":"y_pred_log_reg = log_reg.predict(undersample_Xtest)\n\nlog_reg_cf = confusion_matrix(undersample_ytest, y_pred_log_reg)\n\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\n\nfig.add_subplot(221)\nplot_confusion_matrix(log_reg_cf, labels, title=\"Random UnderSample \\n Confusion Matrix\")","bd7e8581":"print('Logistic Regression - Classification Report\\n')\nprint(classification_report(undersample_ytest, y_pred_log_reg))","6e1b0e0e":"print('Tamanho do X (treino): {} | Tamanho do y (treino): {}'.format(len(original_Xtrain), len(original_ytrain)))\nprint('Tamanho do X (teste): {} | Tamanho do y (teste): {}'.format(len(original_Xtest), len(original_ytest)))\n\n# Lista para armazenar os scores\naccuracy_lst = []\nprecision_lst = []\nrecall_lst = []\nf1_lst = []\nauc_lst = []\n\n# Par\u00e2metros da Logistic Regression\nlog_reg_params = {\n    'penalty': ['l2'],\n    'C': [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n}\n\n# Randomized SearchCV\nrand_log_reg = RandomizedSearchCV(LogisticRegression(), log_reg_params, n_iter=4)\n\n# Implementa\u00e7\u00e3o do SMOTE\n# Cross-validation da maneira correta\nfor train, test in sss.split(original_Xtrain, original_ytrain):\n    # Pipeline\n    pipeline = imbalanced_make_pipeline(SMOTE(sampling_strategy='minority'), rand_log_reg) # SMOTE durante a valida\u00e7\u00e3o cruzada\n    # Treinamento do modelo\n    model = pipeline.fit(original_Xtrain[train], original_ytrain[train])\n    # Melhores par\u00e2metros\n    best_est = rand_log_reg.best_estimator_\n    # Predi\u00e7\u00f5es\n    prediction = best_est.predict(original_Xtrain[test])\n    \nrand_log_reg","9678ed16":"y_pred_best_est_smote = best_est.predict(original_Xtest)\n\nbest_est_smote_cf = confusion_matrix(original_ytest, y_pred_log_reg)\n\nlabels = ['No Fraud', 'Fraud']\n\nfig = plt.figure(figsize=(16,8))\nfig.add_subplot(221)\n\nplot_confusion_matrix(best_est_smote_cf, labels, title=\"SMOTE OverSample \\n Confusion Matrix\")","231f7531":"# Printa a \"classification report\"\nprint(classification_report(original_ytest, y_pred_best_est_smote, target_names=labels))","f5efaf6a":"y_pred = log_reg.predict(original_Xtest)\nundersample_score = accuracy_score(original_ytest, y_pred)\nprint(classification_report(original_ytest, y_pred, target_names=labels))","d00ceed5":"# Logistic Regression com \"undersampling\" sobre seu dado de test\ny_pred = log_reg.predict(X_test)\nundersample_score = accuracy_score(y_test, y_pred)\n\n# Logistic Regression com SMOTE\ny_pred_sm = best_est.predict(original_Xtest)\noversample_score = accuracy_score(original_ytest, y_pred_sm)\n\n# Dicion\u00e1rio com os scores das duas t\u00e9cnicas (undersampling e oversampling)\nd = {\n    'T\u00e9cnica': ['Random undersampling', 'Oversampling (SMOTE)'],\n    'Score': [undersample_score, oversample_score]\n}\n\n# Cria um dataframe com o dicion\u00e1rio\nfinal_df = pd.DataFrame(data=d)\n\n# Armazena o \"Score\" em outra vari\u00e1vel\nscore = final_df['Score']\n# Remove a coluna \"Score\"\nfinal_df.drop('Score', axis=1, inplace=True)\n# Insere os dados armazenados anteriormente na segunda coluna\nfinal_df.insert(1, 'Score', score)\n\nfinal_df","800dedbf":"import keras\nfrom keras import backend as K\nfrom keras.models import Sequential\nfrom keras.layers import Activation\nfrom keras.layers.core import Dense\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\n\n# Tamanho da camada de entrada\nn_inputs = X_train.shape[1]\n\n# Cria\u00e7\u00e3o da rede\nundersample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])","e51ce3f2":"print(\"Training Dataset generated by RANDOM UnderSampling: Size = \", len(X_train))\n\nundersample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\nundersample_model.fit(X_train, y_train, validation_split=0.2, batch_size=25, epochs=20, shuffle=True, verbose = 0)","a008d2a3":"undersample_fraud_predictions = undersample_model.predict_classes(original_Xtest, batch_size=200)\n\nprint(\"Neural Net KERAS with UnderSampling:\\n\")\nprint(classification_report(original_ytest, undersample_fraud_predictions, target_names=labels))","9370dcac":"# SMOTE\nsm = SMOTE('minority', random_state=42)\n\n# Treina os dados originais utilizando SMOTE\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\nprint(\"Training Dataset generated by SMOTE OverSampling: Size = \", len(Xsm_train))","5857b66a":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\noversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\noversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=0)","cfe7a8b1":"oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\nprint(\"Neural Net KERAS with SMOTE:\\n\")\nprint(classification_report(original_ytest, oversample_fraud_predictions, target_names=labels))","e7d7c0fa":"from imblearn.under_sampling import RandomUnderSampler, TomekLinks\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.combine import SMOTEENN, SMOTETomek","0fd0b365":"from collections import Counter\nt0 = time.time()\nsm = SMOTEENN(\"minority\", random_state=42)\nXsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\nt1 = time.time()\nprint(\"SMOTEENN took {:.6} s\".format(t1 - t0)) # 864.109 s = 15minutos","d5416eed":"t0 = time.time()\nprint('Resampled dataset shape BEFORE %s' % Counter(original_ytrain))\n\nprint('Resampled dataset shape AFTER %s' % Counter(ysm_train))\nt1 = time.time()\nprint(\"PRINT took {:.6} s\".format(t1 - t0))","f34a26ad":"n_inputs = Xsm_train.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\noversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\noversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=0)\n\n# Show Result\n\noversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\nprint(\"Neural Net KERAS with SMOTEENN:\\n\")\nprint(classification_report(original_ytest, oversample_fraud_predictions, target_names=labels))","56c60000":"\"\"\"\nNeural Net KERAS with SMOTEENN:\n\n              precision    recall  f1-score   support\n\n    No Fraud       1.00      1.00      1.00     56863\n       Fraud       0.78      0.72      0.75        98\n\n    accuracy                           1.00     56961\n   macro avg       0.89      0.86      0.88     56961\nweighted avg       1.00      1.00      1.00     56961\n\"\"\"","ffe18ef0":"from imblearn.combine import SMOTETomek\nfrom imblearn.under_sampling import TomekLinks\nfrom collections import Counter\n\nt0 = time.time()\nsm = SMOTETomek(tomek=TomekLinks(sampling_strategy='majority'))\nXsm_train2, ysm_train2 = sm.fit_sample(original_Xtrain, original_ytrain)\nt1 = time.time()\nprint(\"SMOTETomek took {:.6} s\".format(t1 - t0)) # 543.303 s = 9 minutos","8cdd424d":"n_inputs = Xsm_train2.shape[1]\n\noversample_model = Sequential([\n    Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n    Dense(32, activation='relu'),\n    Dense(2, activation='softmax')\n])\n\noversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\noversample_model.fit(Xsm_train2, ysm_train2, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=0)\n\n# Show Result\n\noversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\nprint(\"Neural Net KERAS with SMOTETomek:\\n\")\nprint(classification_report(original_ytest, oversample_fraud_predictions, target_names=labels))","4c898771":"\"\"\"\nNeural Net KERAS with SMOTETomek:\n\n              precision    recall  f1-score   support\n\n    No Fraud       1.00      1.00      1.00     56863\n       Fraud       0.90      0.67      0.77        98\n\n    accuracy                           1.00     56961\n   macro avg       0.95      0.84      0.89     56961\nweighted avg       1.00      1.00      1.00     56961\n\"\"\"","47d4b61e":"## SMOTEENN\n# from collections import Counter\n# # Class to perform over-sampling using SMOTE and cleaning using ENN.\n# # Combine over- and under-sampling using SMOTE and Edited Nearest Neighbours.\n# sm = SMOTEENN(\"minority\", random_state=42)\n\n# # Treina os dados originais utilizando SMOTE\n# Xsm_train, ysm_train = sm.fit_sample(original_Xtrain, original_ytrain)\n# print('Resampled dataset shape %s' % Counter(ysm_train))\n# print(\"Training Dataset generated by SMOTEENN OverSampling: Size = \", len(Xsm_train))\n\n# n_inputs = Xsm_train.shape[1]\n\n# oversample_model = Sequential([\n#     Dense(n_inputs, input_shape=(n_inputs, ), activation='relu'),\n#     Dense(32, activation='relu'),\n#     Dense(2, activation='softmax')\n# ])\n\n# oversample_model.compile(Adam(lr=0.001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n\n# oversample_model.fit(Xsm_train, ysm_train, validation_split=0.2, batch_size=300, epochs=20, shuffle=True, verbose=0)\n\n# # Show Result\n\n# oversample_fraud_predictions = oversample_model.predict_classes(original_Xtest, batch_size=200, verbose=0)\n\n# print(\"Neural Net KERAS with SMOTEENN:\\n\")\n# print(classification_report(original_ytest, oversample_fraud_predictions, target_names=labels))","d04680e0":"## Snippets <a id ='index02'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","fe4de3ae":"SMOTEK e SMOTE Obtiveram os melhores resultados","42c7122a":"**Conclusion**\n\n\ud83c\uddfa\ud83c\uddf8\n\nAs we can see from the tsne, it is possible to separate these classes.\n\n\ud83c\udde7\ud83c\uddf7\n\nComo podemos observar pelo tsne, \u00e9 poss\u00edvel separar essas classes.","91f1855b":"## Neural Network <a id='index16'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### UnderSampling - Random <a id='index17'><\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nIn this section we will implement a simple neural network (with only one hidden layer) in order to verify that the network performs well in predicting fraudulent and non-fraudulent transactions, in addition to using both types of resampling (undersample and oversample).\n\nIn this final test phase, it is important to remember that the model will train with the data using both resampling techniques (random undersampling and oversampling (SMOTE)) and then make the predictions using the original test data.\n\nCreation of the neural network with the \"undersampling\" technique\nThis neural network has the following architecture:\n\n+ 1 input layer\n+ 1 hidden layer\n+ 1 output layer\n\n\ud83c\udde7\ud83c\uddf7\n\nNessa se\u00e7\u00e3o implementaremos uma rede neural simples (com apenas uma camada oculta) com intuito de verificar se a rede performa bem na predi\u00e7\u00e3o de transa\u00e7\u00f5es fraudulentas e n\u00e3o fraudulentas, al\u00e9m de utilizar os dois tipos de reamostragem (undersample e oversample).\n\nNessa fase final de teste, \u00e9 importante lembrar que o modelo treinar\u00e1 com os dados utilizando ambas as t\u00e9cnicas de reamostragem (random undersampling e oversampling (SMOTE)) e depois far\u00e1 as predi\u00e7\u00f5es utilizando os dados originais de teste.\n\nCria\u00e7\u00e3o da rede neural com a t\u00e9cnica \"undersampling\"\nEssa rede neural possui a seguinte arquitetura:\n\n+ 1 camada de entrada\n+ 1 camada oculta\n+ 1 camada de sa\u00edda","2daf9d86":"<span style='font-size: 15pt'>Conclusion<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nTesting in the underSampling test set we have excellent results. But this is a small set, we will now test on all data.\n\n\ud83c\udde7\ud83c\uddf7\n\nTestando no conjunto de teste do underSampling tivermos excelentes restultados. Por\u00e9m esse \u00e9 um pequeno conjunto, vamos agora testar em todos os dados.\n","223b2936":"### Show correlation with *Class* in BoxsPlots <a id='index10'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n**\ud83c\uddfa\ud83c\uddf8**\n\nLooking at these 2 boxplots of the same data for different classes on some features, we can see that the patterns differ between classes.\n\n**\ud83c\udde7\ud83c\uddf7**\n\nObservando esses 2 boxplots de um mesmo dado para classes diferente sobre algumas features, podmeos perceber que os padr\u00f5es diferem entre as classes.","d1697492":"## References\n\n\ud83c\uddfa\ud83c\uddf8 : This dataset consists of a thorough understanding of these Kernels ...\n\n\ud83c\udde7\ud83c\uddf7 : Este dataset consiste em entender minucioso esses Kernels ...\n\n+ [Fraude de cr\u00e9dito - PTBR](https:\/\/www.kaggle.com\/olavomendes\/fraude-de-cr-dito-ptbr) from [CSD](https:\/\/www.kaggle.com\/olavomendes)\n\n+ [Credit Fraud  Dealing with Imbalanced Datasets](https:\/\/www.kaggle.com\/janiobachmann\/credit-fraud-dealing-with-imbalanced-datasets) from [Janio Martinez](https:\/\/www.kaggle.com\/janiobachmann)\n\n## Kaggle Description\n\nhttps:\/\/www.kaggle.com\/mlg-ulb\/creditcardfraud\n\n**Context**\n\nIt is important that credit card companies are able to recognize fraudulent credit card transactions so that customers are not charged for items that they did not purchase.\n\n**Content**\n\nThe datasets contains transactions made by credit cards in September 2013 by european cardholders.\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\n\nIt contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, \u2026 V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.\n\n## Goals\n\n\ud83c\uddfa\ud83c\uddf8\n+ Understand data distribution\n+ Create a 50\/50 \"data subsets\" with Fraudulent and Non-fraudulent transactions.\n+ Determine the classifiers to be used and decide which one is most accurate\n+ Create a neural network and compare accuracy with the classifier\n+ Understand common errors present in unbalanced data sets.\n\n\ud83c\udde7\ud83c\uddf7\n\n+ Entender a distribui\u00e7\u00e3o dos dados\n+ Criar um \"sub-conjuntos de dados\" de raz\u00e3o 50\/50 com as transa\u00e7\u00f5es Fraudulentas e N\u00e3o fraudulentas.\n+ Determinar os classificadores a serem usados e decidir qual tem maior acur\u00e1cia\n+ Criar uma rede neural e comparar a acur\u00e1cia com o clasificador\n+ Entender erros comuns presentes em conjuntos de dados desbalanceados.\n\n## Brief Summary of DataSet\n\n**\ud83c\uddfa\ud83c\uddf8**\n\n**Data Understanding**\n\nWith the exception of the *transaction* and *amount* columns, we don't know what the other columns are (for privacy reasons, as stated earlier). The only thing we know about the unknown columns is that they are staggered.\n\n+ Upstream of transactions and relatively small. The average is approximately USD 88\n+ There are no null values\n+ The vast majority of transactions are non-fraudulent (99.83%), whereas fraudulent transactions occur only in 0.17% of the data set, that is, we have an UNBALANCED dataset.\n\n**Information about the features**\n\n+ PCA: the data description tells us that the \"features\" have undergone the PCA (Principal Component Analysis) transformation, in order to reduce the dimensions of the data (except time and amount)\n+ Scaling: before applying the PCA, the data was scaled.\n\n**\ud83c\udde7\ud83c\uddf7**\n\n**Entendendo os dados**\n\nCom exce\u00e7\u00e3o das colunas *transaction* e *amount*, n\u00e3o sabemos o que s\u00e3o as outras colunas (por raz\u00e3o de privacidade, como dito anteriormente). A \u00fanica coisa que sabemos das colunas desconhecidas \u00e9 que elas est\u00e3o escalonadas.\n\n+ A montante de transa\u00e7\u00f5es e relativamente pequena. A m\u00e9dia \u00e9 aproximadamente USD 88\n+ N\u00e3o h\u00e1 valores nulos\n+ A grande maioria das transa\u00e7\u00f5es s\u00e3o n\u00e3o fraudulentas (99.83%), enquanto transa\u00e7\u00f5es fraudulentas ocorrem apenas em 0.17% do conjunto de dados, ou seja, temos um dataset DESBALANCEADO.\n\n**Informa\u00e7\u00f5es sobre as \"features\"**\n\n+ PCA: a descri\u00e7\u00e3o dos dados nos diz que as \"features\" passaram pela transforma\u00e7\u00e3o PCA (Principal Component Analysis), com intuito de reduzir as dimens\u00f5es dos dados (exceto time e amount)\n+ Escalonamento: antes da aplica\u00e7\u00e3o do PCA, os dados foram escalonados.\n\n","c01e0fa6":"We get that Logistic is the best, now, we make a hyper-turing of params","f5c5a30e":"### Others Tests <a id='index31'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","45ebcb07":"<span style='font-size: 15pt'>Conclusion<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nAverage f1-score of classes: 0.56 (!!!)\n\nThe accuracy and f1-score of class 1 (Fraud) is terrible, that is, when the model says it is fraud it makes a lot of mistakes.\n\nIf we analyze other metrics, we realize that the model is excellent when in fact it is not, it happens because it is unbalanced.\n\n**The undersampling approach did not work**, perhaps because we lost a lot of information.\n\n\ud83c\udde7\ud83c\uddf7\n\nM\u00e9dia do f1-score das classes: 0.56 (!!!)\n\nA precis\u00e3o e f1-score da classe 1 (Fraude) \u00e9 terr\u00edvel, ou seja, quando o modelo diz que \u00e9 fraude ele erra muito.\n\nSe analisarmos outras m\u00e9tricas, percebmos que o modelo est\u00e1 excelente quando na verdade n\u00e3o est\u00e1, isso acontece por estar desbalanceado.\n\n**A abordagem undesampling nao deu certo**, talvez, por perdemos muita informa\u00e7\u00e3o.","947b8ad9":"## Split DataSet in Test and Train <a id='index06'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n<span style='font-size: 15pt'>SubSets with same proportion of Classes<\/span>\n\n**\ud83c\uddfa\ud83c\uddf8**\n\nIt is necessary to separate the dataset into test and training sets. As the amount of ** Fraud (1) ** is very small, we have to ensure that the two sets have the same proportionality. For this we will use `StratifiedKFold (n_splits = 5)` and we will separate the data with the same record population by class, with the test data being 20% of the dataset and the training data 80%.\n\nThe training and test dataset will still be unbalanced each, but with the same proportion of records per class between these datasets.\n\n**\ud83c\udde7\ud83c\uddf7**\n\n\u00c9 necess\u00e1rio separar o dataset em conjuntos de teste e treino. Como a a quantidade de **Fraud (1)** \u00e9 muito pequena, temos que garantir que os dois conjuntos tenham a mesma proporcionalidade. Para isso usaremos `StratifiedKFold(n_splits=5)` e iremos separar os dados com mesma popor\u00e7\u00e3o de registros por classe sendo que os dados de teste ser\u00e3o 20% do dataset e os de treinamento 80%.\n\nOs dataset de treino e testse ainda estarao desbalanceado cada um, mas com mesma propor\u00e7\u00e3o de registros por classes entre esses datasets.","d65ec45d":"## Scalling *Time* and *Amount* <a id='index05'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nWe will scale the columns * time * and * amount * because while the other columns range from values between \\[-100, 100 \\] these columns range from 0 to extremely high values. In training or ranking, these high values compared to other columns will give these columns more advantage over others, and it may be that there are other columns or combinations of them that influence more.\n\nThen we will scale it so that the \"competition between the features\" is as fair as possible.\n\n`RobustScaler ()` will be used to reduce the influence of outliers.\n\n\ud83c\udde7\ud83c\uddf7\n\nIremos escalar as colunas *time* e *amount* pois enquanto que as outra colunas v\u00e3o de valores aproximadamente entre \\[-100, 100\\] essas colunas v\u00e3o de 0 a valores extremamente altos. No treinamento ouclassifica\u00e7\u00e3o, esse valores altos em compara\u00e7\u00e2o as outra colunas v\u00e3o dar mais vantagem a essas colunas sobre as outras, e pode ser que haja outra colunas ou combina\u00e7\u00e3o delas que influenciam mais. \n\nEnt\u00e3o iremos escalar para que a \"competi\u00e7\u00e3o entre as features\" seja a mais justa poss\u00edvel.\n\nSer\u00e1 utilizado o `RobustScale()` para reduzir a influencia de outiliers.","420f8ff3":"### View correlation on balanced dataset <a id='index09'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nNow, having the same proportion of data, we can better analyze the correlation between the variables, since they are all numeric. Below are shown two heatmaps, one from the unbalanced dataframe and the other from the balanced with Under-Sampling.\n\nTo make the classification it will be necessary to find a data pattern that characterizes each class. By making the heatmap of balanced data this discrepancy of standards by class will become more evident.\n\nCorrelations found:\n\nAs it is a case of binary correlation, the heatmap of each feature with class will inform its degree of importance to have a value of 1 **Fraud (1)**. That is, it will inform the level of importance to differentiate between classes 0 and 1.\n\n+ Negative correlation: V16, V14, V12 and V10. That is to say, the lower your values, the more likely the transaction is to be fraudulent\n\n+ Positive correlation: V2, V4, and V11. That is to say, the higher your values, the more likely the transaction is to be fraudulent.\n\n\ud83c\udde7\ud83c\uddf7\n\nAgora, tendo a mesma propor\u00e7\u00e3o de dado, podemos analisar melhor a correla\u00e7\u00e2o entre as vari\u00e1veis, j\u00e1 que todas s\u00e3o num\u00e9ricas. A seguir \u00e9 mostrado dois heatmaps, um do dataframe desbalanceado e outro do balanceado com Under-Sampling.\n\nPara fazer a classifica\u00e7\u00e2o ser\u00e1 necess\u00e1rio encontrar um padr\u00e3o dos dados que caracterize cada classe. Fazendo o heatmap do dados balanceados essa discrepancia dos padr\u00f5es por classe ficar\u00e1 mais evidente.\n\nCorrela\u00e7\u00f5es encontradas:\n\nComo \u00e9 um caso de correla\u00e7\u00e2o bin\u00e1ria, o heatmap de cada feature com classe ir\u00e1 informar o seu grau de import\u00e2ncia para ter valor 1 **Fraud (1)**. Ou seja, informar\u00e1 o n\u00edvel de import\u00e2ncia para diferenciar entre as classe 0 e 1.\n\n+ Correla\u00e7\u00e3o negativa: V16, V14, V12 e V10. Isso que dizer que, quanto menor seus valores, mais prov\u00e1vel a transa\u00e7\u00e3o ser fraudulenta\n\n+ Correla\u00e7\u00e3o positiva: V2, V4, e V11. Isso que dizer que, quanto maior seus valores, mais prov\u00e1vel a transa\u00e7\u00e3o ser fraudulenta.","8610e46a":"### OverSampling - SMOTE <a id ='index18'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","a02c6d41":"<span style='font-size: 15pt'>Conclusion<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nIt's already better, improved the FRAUD precision and consequently its f1-score\n\n\ud83c\udde7\ud83c\uddf7\n\nJ\u00e1 est\u00e1 melhor, melhorou a precision de FRAUD e consequentemente seu f1-score","c1704a67":"<span style='font-size: 16pt'>Missing Data<\/span> \n\n**\ud83c\uddfa\ud83c\uddf8**\n\nNo data missing\n\n**\ud83c\udde7\ud83c\uddf7**\n\nSem dados faltando","74c1e565":"## Table of Content (TOC) <a id=\"top\"><\/a>\n\n+ [Import Libs and DataSet](#index01) \n+ [Snippets](#index02)\n+ [Understand DataSet](#index03)\n+ [Distribution of *Time* and *Amount*](#index04)\n+ [Scalling *Time* and *Amount*](#index05)\n+ [Split DataSet in Test and Train](#index06)\n+ [Random Under-Sampling to correct unbalanced](#index07)\n  - [Make Under-Sampling](#index08)\n  - [View correlation on balanced dataset](#index09)\n  - [Show correlation with *Class* in BoxsPlots](#index10)\n  - [Remove Outiliers](#index11)\n+ [Dimensionality Reduction and Clustering](#index12)\n+ [Train on UnderSampling](#index13)\n+ [Test in Original DataFrame Unbalanced](#index14)\n+ [Oversampling with SMOTE](#index15)\n  - [Create DataSet balanced with SMOTE](#index21)\n  - [Test model in UnderSampling DataSet](#index20)\n+ [Neural Network](#index16)\n  - [UnderSampling - Random](#index17)\n  - [OverSampling - SMOTE](#index18)\n  - [Others Tests](#index31)\n+ [Conclusion: Best Model ...](#index30)","b03dc1a3":"## Conclusion: Best Model <a id='index30'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nThe best model was to use a Keras neural network training on a dataSet modified by the SMOTE OverSampling method to correct its imbalance.\n\nWe achieved great precision in both classes and the average f1-score is now close to 90%.\n\nIn this case, the over-sampling approach using SMOTE gave more results than Random under-sampling.\n\n\ud83c\udde7\ud83c\uddf7\n\nO melhor modelo foi usar uma rede neural Keras treinando sobre um dataSet alterado pelo m\u00e9todo SMOTE de OverSampling para corrigir seu desbalanceamento.\n\nConseguimos uma grande Precis\u00e3o nas duas classe e a m\u00e9dia de f1-score agora \u00e9 perto de 90%.\n\nNesse caso a abordagem de over-sampling usando SMOTE deu mais resultados que Random under-sampling. ","e80c8b47":"## Test in Original DataFrame Unbalanced <a id ='index14'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n<span style='font-size: 15pt'>After train a model in balanced dataset with UnderSampling method, now we will test in original dataset<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nWe will use Logistic Regression trained in the balanced dataset by the \"Random UnderSampling\" method to predict the data from the original dataset.\n\nRealize that, even attending high values **WE ONLY REALIZE THAT IT IS A BIG FAILURE TO ANALYZE PRECISION ABOUT CLASS 1 (FRAUD) AND THEREFORE F1-SCORE**.\n\n\ud83c\udde7\ud83c\uddf7\n\nUsaremos a Regress\u00e3o log\u00edstica treinada no dataset balanceado pelo m\u00e9todo \"Random UnderSampling\" para prever os dados do dataset original.\n\nPerceba que, mesmo atendendo altos valores **SOMENTE PERCEBEMOS QUE \u00c9 UMA GRANDE FALHA AO ANALISAR A PRECISION SOBRE A CLASSE 1 (FRAUDE) E CONSEQUENTEMENTE F1-SCORE**. ","529de31b":"## Understand DataSet <a id ='index03'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n**\ud83c\uddfa\ud83c\uddf8**\n\n284,807 lines, 29 * features * and records classified as class 0 (without fraud) and 1 (fraud), where all resources are dimensioned, except `Time` and` Amount`.\n\nThe dataSet is unbalanced. Almost all lines are ** 0 (No Fraud) **.\n\nWe must be careful with the evaluation of the model, because if we assume that any transaction is Fraud-Free we get 99% right.\n+ If the data as it is used for training, it will hit ** No Fraud ** more and ** ** Fraud ** too much. To solve this, we will use techniques to deal with unbalanced datasets: Random UnderSampling and OverSampling SMOTE.\n\n**\ud83c\udde7\ud83c\uddf7**\n\n284.807 linhas, 29 *features* e registros calssificados como a classe 0 (sem fraude) e 1 (fraude), onde todos os recursos s\u00e3o dimensionados, exceto `Time` e` Amount`.\n\nO dataSet est\u00e1 desequilibrado. Quase todas as linhas s\u00e3o **0 (No Fraud)**.\n\nDevemos ter cuidado com a avalia\u00e7\u00e3o do modelo, pois se assumirmos que qualquer transa\u00e7\u00e3o \u00e9 Sem Fraude acertamos 99%.\n+ Se os dados da forma que est\u00e3o forem usados para treinamento, ele acertar\u00e1 mais o **No Fraud** e errar\u00e1 demais o **Fraud**. Para resolver isso, vamos utilizar t\u00e9cnicas para lidar com datasets desbalanceados: Random UnderSampling e OverSampling SMOTE.","3df73b8a":"### Remove Outiliers <a id ='index11'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\n**\ud83c\uddfa\ud83c\uddf8**\n\n\nThe main objective of this section is to remove extreme outliers from the \"features\" that have a high correlation with the class.\n\n\"Interquartile range\" method (interquartile range)\n\nInterquartile range (IQR): The calculation is made by the difference between the 75th percentile and the 25th percentile. With that we can create a threshold between 75\u00ba and 25\u00ba and any instance outside that threshold will be excluded.\n\nRemoving what's outside the IQR is equivalent to keeping 99% of the data\n\n**Removal of \"tradeoff\" outliers**\n\nWe have to be careful about the threshold for removing outliers. We determine this threshold by multiplying a number (1.5, for example) by the IQR. The higher the threshold, the less \"outliers\" will be detected (If we use the number 3 in the multiplication, for example) and the lower the threshold, the more \"outliers\" will be detected. It is best to focus only on extreme \"outliers\", thus decreasing the risk of loss of information, making the model less accurate.\n\n\n**\ud83c\udde7\ud83c\uddf7**\n\nO principal objetivo dessa se\u00e7\u00e3o \u00e9 remover \"outliers\" extremos das \"features\" que possuem correla\u00e7\u00e3o alta com a classe.\n\nM\u00e9todo \"Interquartile range\" (dist\u00e2ncia interquartil)\n\nInterquartile range (IQR): O c\u00e1lculo \u00e9 feito pela diferen\u00e7a entre 75\u00ba percentil e o 25\u00ba percentil. Com isso podemos criar uma limiar entre o 75\u00ba e 25\u00ba e qualquer inst\u00e2ncia fora dessa limiar ser\u00e1 excluida.\n\nRemover o que est\u00e1 fora do IQR Equivale amanter 99% dos dados\n\n**Remo\u00e7\u00e3o de \"outliers\" \"tradeoff\"**\n\nTemos que ser cuidadosos com rela\u00e7\u00e3o \u00e0 limiar para a remo\u00e7\u00e3o de \"outliers\". Determinamos essa limiar multiplicando um n\u00famero (1.5, por exemplo) pelo IQR. Quanto maior a limiar, menos \"outliers\" ser\u00e3o detectados (Se usarmos o n\u00famero 3 na multiplica\u00e7\u00e3o, por exemplo) e quanto menor a limiar, mais \"outliers\" ser\u00e3o detectados. O melhor \u00e9 focarmos apenas nos \"outliers\" extremos, assim diminuindo o risco de ocorrer a perda de informa\u00e7\u00f5es, fazendo com que o modelo tenha acur\u00e1cia menor.","2f56eee2":"## Dimensionality Reduction and Clustering <a id ='index12'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n \n<span style='font-size: 15pt'>To be able to classify<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nSummary:\n\n+ t-SNE algorithm can pretty accurately cluster the cases that were fraud and non-fraud in our dataset.\n+ Although the subsample is pretty small, the t-SNE algorithm is able to detect clusters pretty accurately in every scenario (I shuffle the dataset before running t-SNE)\n+ **This gives us an indication that further predictive models will perform pretty well in separating fraud cases from non-fraud cases.**\n\n\ud83c\udde7\ud83c\uddf7\n\nResumo:\n\n+ O algoritmo t-SNE pode agrupar com precis\u00e3o os casos de fraude e n\u00e3o fraude em nosso conjunto de dados.\n+ Embora a subamostra seja bastante pequena, o algoritmo t-SNE \u00e9 capaz de detectar clusters com muita precis\u00e3o em todos os cen\u00e1rios (embaralhe o conjunto de dados antes de executar o t-SNE)\n+ **Isso nos d\u00e1 uma indica\u00e7\u00e3o de que outros modelos preditivos ter\u00e3o um bom desempenho na separa\u00e7\u00e3o de casos de fraude de casos n\u00e3o fraudulentos**.","33c7c033":"Now we use this best params in each classify","a2ae1e52":"## Oversampling with SMOTE <a id ='index15'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n### Create DataSet balanced with SMOTE <a id ='index21'><\/a>\n\n**\ud83c\uddfa\ud83c\uddf8**\n\nSMOTE stands for \"Synthetic Minority Over-sampling Technique\". Unlike random undersampling, SMOTE creates new synthetic points in order to make classes balanced and is one of the alternative techniques for dealing with unbalanced datasets.\n\n+ Resolve class imbalance: SMOTE creates synthetic points for the minority class to achieve a balance between the minority and majority classes\n+ Location of synthetic points: SMOTE calculates the distance from the closest neighbors of the minority class and creates synthetic points at these distances\n+ Final effect: more information is kept, since we do not need to delete any lines, different from random undersampling.\n+ Accuracy \/ Time tradeoff: although SMOTE is probably more accurate than random undersampling, it will take longer to run.\n\n<img src=\"https:\/\/www.researchgate.net\/publication\/287601878\/figure\/fig1\/AS:316826589384744@1452548753581\/The-schematic-of-NRSBoundary-SMOTE-algorithm.png\" width=\"300\" height=\"200\">\n\n**\ud83c\udde7\ud83c\uddf7**\n\nSMOTE significa \"Synthetic Minority Over-sampling Technique\". Ao contr\u00e1rio do \"undersampling\" aleat\u00f3rio, SMOTE cria novos pontos sint\u00e9ticos com o intuito de tornar as classes balanceadas e \u00e9 uma das t\u00e9cnicas alternativas para lidar com datasets desbalanceados.\n\n+ Resolve o desbalanceamento de classes: SMOTE cria pontos sint\u00e9ticos da classe minorit\u00e1ria para alcan\u00e7ar o balanceamento entre a classe minorit\u00e1ria e majorit\u00e1ria\n+ Localiza\u00e7\u00e3o dos pontos sint\u00e9ticos: SMOTE calcula a dist\u00e2ncia dos vizinhos mais pr\u00f3ximos da classe minorit\u00e1ria e cria pontos sint\u00e9ticos nessas dist\u00e2ncias\n+ Efeito final: mais informa\u00e7\u00f5es s\u00e3o mantidas, j\u00e1 que n\u00e3o precisamos excluir nenhuma linha ,diferente da undersampling aleat\u00f3ria.\n+ Accuracy\/Time tradeoff: embora o SMOTE provavelmente seja mais preciso que a \"undersampling\" aleat\u00f3ria, levar\u00e1 mais tempo para ser executado.","a06f5811":"## Train on UnderSampling Dataset Balanced <a id ='index13'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nAfter having a balanced DataSet with the same amount of records per class, we can train a model.\n\nAbout this small Balanced dataset with about 900 records, we will separate into training and test data.\n\n\ud83c\udde7\ud83c\uddf7\n\nAp\u00f3s ter um DataSet balanceado tendo a mesma quantidade de registros por classes, podemos treinar um modelo.\n\nSobre esse pequeno dataset Balanceado com cerca de 900 registros, vamos separar em dados de treino e teste.\n\n","ff3edb3b":"## Distribution of *Time* and *Amount* <a id ='index04'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","5dcb8bb2":"<span style='font-size: 15pt'>Conclusion<\/span>\n\n\ud83c\uddfa\ud83c\uddf8\n\nThe oversamplig technique (SMOTE) obtained a result far superior to the random undersampling technique. This may have occurred due to the fact that during the balance of the data with undersampling it removed several examples of non-fraudulent data, in order that the two classes had an equal number of examples, and this caused a loss of information that could have been useful to the model. SMOTE, on the other hand, did not remove this useful information, but created synthetic points of fraudulent data, that is, the useful information remained present in the data set.\n\n\ud83c\udde7\ud83c\uddf7\n\nA t\u00e9cnica de oversamplig (SMOTE) obteve um resultado bem superior \u00e0 t\u00e9cnica de random undersampling. Isso pode ter ocorrido pelo fato de que durante o balanceamente dos dados com undersampling removeu diversos exemplos de dados n\u00e3o fraudulentos, afim de que as duas classes ficassem com uma quantidade igual de exemplos, e isso ocasionou uma perda de informa\u00e7\u00f5es que poderiam ter sido \u00fateis ao modelo. J\u00e1 o SMOTE n\u00e3o removeu essas informa\u00e7\u00f5es \u00fateis e sim criou pontos sint\u00e9ticos de dados fraudulentos, ou seja, as informa\u00e7\u00f5es \u00fateis continuaram presentes no conjunto de dados.","7fc00c8f":"### Make Random Under-Sampling <a id='index08'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n\ud83c\uddfa\ud83c\uddf8\n\nPerforming Random UnderSampling: In a random way, we will remove records classified as **No Fraud (0)** so that it has the same amount of class records **Fraud (1)**, that is, until there are 492 records in the training dataset.\n\n<img src=\"https:\/\/miro.medium.com\/max\/335\/1*YH_vPYQEDIW0JoUYMeLz_A.png\" width=\"400\" height=\"200\">\n\n\ud83c\udde7\ud83c\uddf7\n\nRealizando Random UnderSampling:  De forma aleart\u00f3ria, vamos retirar registros calssificados como **No Fraud (0)** para que tenha a mesma quantidade de registros de classe **Fraud (1)**, ou seja, at\u00e9 que tenha 492 registros no dataset de treino.","5f770c3b":"### Test model in UnderSampling DataSet <a id ='index20'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\nA valida\u00e7\u00e3o final do modelos modelos de classifica\u00e7\u00e3o ser\u00e1 feita nos dados que passaram pela random undersampling e n\u00e3o nos dados originais.\n\nVamos comparar o modelo tanto do UnderSampling como OverSampling","a5ca8a15":"<div style=\"text-align: center;\">\n    <h1>Credit Card Fraud: Classify on Unbalanced data<\/h1>\n    <img src=\"https:\/\/zdnet2.cbsistatic.com\/hub\/i\/r\/2014\/11\/28\/be5ca1a7-76b6-11e4-b569-d4ae52e95e57\/resize\/770xauto\/a1fc0cd4944953755096a9b4cd0ab5a4\/credit-card-fraud-can-be-stopped-heres-how.jpg\" width=\"400\" height=\"200\">\n    <h3 align=\"center\">Made by \ud83d\ude80 <a href=\"https:\/\/www.kaggle.com\/rafanthx13\">Rafael Morais de Assis<\/a><\/h3>\n<\/div>\n\n<br><br>\n\n**Language:** English (\ud83c\uddfa\ud83c\uddf8) and Portuguese (\ud83c\udde7\ud83c\uddf7)\n\nCreated: 2020-08-14; (14\/08\/2020)\n\nLast updated: 2020-08-14; (14\/08\/2020)","d4be6b41":"## Import Libs and DataSet <a id ='index01'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>","806c89dc":"## Random Under-Sampling to correct unbalanced <a id='index07'><\/a> <a href=\"#top\" class=\"btn btn-primary btn-sm\" role=\"button\" aria-pressed=\"true\" style=\"color:white; margin-left: 20px;\" data-toggle=\"popover\">Go to TOC<\/a>\n\n**\ud83c\uddfa\ud83c\uddf8**\n\nIn this step, the \"random undersampling\" technique will be implemented, which basically consists of removing data to make the data sets more balanced.\n\nThe idea is ** EQUALING THE ROWS PROPORTION BY CLASSES BY TAKING ROWS FROM THE CLASS THAT HAS MORE**\n\nThat is: Let's take records from the class **No Fraud (0)** which consists of 99% of the original dataset to be the same size as the class **Fraud (1)** which are **492 rows** being less 1% of the data.\n\n!!! Unfortunately there will be a great loss of information but it will have the same proportion of registration per class\n\n\n**\ud83c\udde7\ud83c\uddf7**\n\nNessa fase ser\u00e1 implementada a t\u00e9cnica \"random undersampling\", que basicamente consiste em remover dados para tornar o conjuntos de dados mais balanceado.\n\nA ideia \u00e9 **IGUALAR A PROPOR\u00c7\u00c3O DE ROWS POR CLASSES TIRANDO ROWS DA CLASSE QUE TEM MAIS**\n\nOu seja: Vamos tirar registros da classe **No Fraud (0)** que consiste em 99% do dataset original para ficar com mesmo tamanho da classe **Fraud (1)** que s\u00e3o **492 rows** sendo menos de 1% dos dados.  \n\n!!! Infelizmente haver\u00e1 grande perda de informa\u00e7\u00e3o mas ter\u00e1 a mesma propor\u00e7\u00e3o de registro por classe\n\n"}}