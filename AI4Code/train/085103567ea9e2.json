{"cell_type":{"8495302b":"code","cfdc6962":"code","a4716d72":"code","943d7ac2":"code","93ed2ad8":"code","272035fb":"code","004c2dbc":"code","b1c65107":"code","e6fa5d2a":"code","949cbbcc":"code","18e3e7e4":"code","5d25243c":"code","6798a002":"code","5d4bb131":"code","a4e77bbf":"code","d61f6392":"code","de9040f3":"code","bf5c229e":"code","59c3e810":"code","27869ea6":"code","ede4e031":"code","8704e168":"code","10be8c49":"code","444cd254":"code","1ec7c5de":"code","e1fbff63":"code","9967283b":"code","8d421ad4":"code","ca68d61c":"code","2d878a35":"code","5be97588":"code","62163e6c":"code","5fd958be":"code","80fff0d3":"code","798c4efc":"code","1eeaf5de":"code","89172653":"code","6c0f7700":"code","d770515e":"code","eaabca1f":"code","b7d6f2ea":"code","a7c5ef1e":"code","c401d692":"code","110a94d6":"markdown","c018ff8b":"markdown","b5e86407":"markdown","efeaeff0":"markdown","dfe02db3":"markdown","c0bae311":"markdown","9da4df85":"markdown","f6a9dd3f":"markdown","5941c13c":"markdown","e367b569":"markdown","26077e56":"markdown","66841abe":"markdown","9f1d6989":"markdown","292cbe7d":"markdown","8e9b47c0":"markdown","c6830c1c":"markdown","6f506643":"markdown","edf6bfd8":"markdown"},"source":{"8495302b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # for data visualization\nimport seaborn as sns\nsns.set(style='darkgrid')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cfdc6962":"traindf=pd.read_csv(\"\/kaggle\/input\/novartis-data\/Train.csv\")\ntestdf=pd.read_csv(\"\/kaggle\/input\/novartis-data\/Test.csv\")\nsubmissiondf=pd.read_csv(\"\/kaggle\/input\/novartis-data\/sample_submission.csv\")\nprint(traindf.shape)\ntraindf.head()","a4716d72":"traindf.info()","943d7ac2":"print(testdf.shape)\ntestdf.head()","93ed2ad8":"# Multiple_offense is the target varaible, lets see how it is populated\n## Plotting the bar char to identify the frequnecy of values\nsns.countplot(traindf[\"MULTIPLE_OFFENSE\"],color='black')\n##prinitng number of values for each type\nprint(traindf[\"MULTIPLE_OFFENSE\"].value_counts())\n","272035fb":"# Analysing NULL values in the features\nfor col in traindf.columns:\n    if traindf[col].isnull().values.any():\n        print(f\"Train Dataset Feature - {col} contains {traindf[col].isna().sum()*100\/traindf[col].sum()}% of Null Values\")\n    \nfor col in testdf.columns:\n    if testdf[col].isnull().values.any():\n        print(f\"Test Dataset Feature - {col} contains {testdf[col].isna().sum()*100\/testdf[col].sum()}% of Null Values\")","004c2dbc":"tempdf=traindf.loc[traindf[\"X_12\"].isnull()==True][\"MULTIPLE_OFFENSE\"]\nsns.countplot(tempdf,color='black')","b1c65107":"print(traindf.shape)\ntraindf=traindf.dropna(axis=0, subset=['X_12'])\nprint(traindf.shape)","e6fa5d2a":"traindf[\"DATE\"].loc[0:5]","949cbbcc":"traindf['DATE'] = pd.to_datetime(traindf.DATE)\ntraindf['DATE'].head()","18e3e7e4":"def Get_feature_from_DATE(df):\n    df['MONTH'] = df['DATE'].dt.month\n    df['DAY'] = df['DATE'].dt.day\n    df['YEAR'] = df['DATE'].dt.year\n    df['DAYOFWEEK'] = df['DATE'].dt.dayofweek\n    df['WEEKEND'] = np.where(df['DATE'].dt.day_name().isin(['Sunday','Saturday']),1,0)\n    df=df.drop(['DATE'],axis=1)\n    return df","5d25243c":"traindf=Get_feature_from_DATE(traindf)\ntraindf.head()","6798a002":"traindf.describe()","5d4bb131":"Numerical_features=['X_1','X_2','X_3','X_4','X_5','X_6']\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.boxplot(traindf[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","a4e77bbf":"Numerical_features=['X_7','X_8','X_9','X_10','X_11','X_12']\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.boxplot(traindf[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","d61f6392":"Numerical_features=['X_6','X_7','X_8','X_9','X_11']\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.distplot(traindf[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","de9040f3":"#removing outliers from X_8 variable\nprint(traindf.shape)\ntraindf = traindf[~((traindf['X_8']>6))]\nprint(traindf.shape)","bf5c229e":"#removing outliers from X_10 variable\nprint(traindf.shape)\ntraindf = traindf[~((traindf['X_10']>10))]\nprint(traindf.shape)","59c3e810":"Numerical_features=['MONTH','DAY','YEAR','DAYOFWEEK','WEEKEND']\nfig, ax = plt.subplots(2, 3, figsize=(20, 10))\nfor variable, subplot in zip(Numerical_features, ax.flatten()):\n    sns.distplot(traindf[variable], ax=subplot)\n    for label in subplot.get_xticklabels():\n        label.set_rotation(0)","27869ea6":"traindf['X_1'].plot.hist()\nprint(traindf['X_1'].value_counts())","ede4e031":"traindf[\"X_1_new\"]=traindf[\"X_1\"].apply(lambda x : x if x ==0  else 1)\n\ntraindf[\"X_1_new\"].plot.hist()","8704e168":"traindf['X_10'].plot.hist()\nprint(traindf['X_10'].value_counts())","10be8c49":"traindf[\"X_10_new\"]=traindf[\"X_10\"].apply(lambda x : x if x ==1  else 2)\ntraindf[\"X_10_new\"].plot.hist()","444cd254":"traindf['X_12'].plot.hist()\nprint(traindf['X_12'].value_counts())","1ec7c5de":"traindf[\"X_12_new\"]=traindf[\"X_12\"].apply(lambda x : x if x ==1  else 2)\ntraindf[\"X_12_new\"].plot.hist()","e1fbff63":"# X_2 and X_3 are highly corelated\ntraindf.drop(['X_1','X_10','X_12','INCIDENT_ID'], axis=1, inplace=True)\ntraindf.head()","9967283b":"fig, ax = plt.subplots(figsize=(25,10))         # Sample figsize in inches\nmatrix = np.triu(traindf.corr())\nsns.heatmap(traindf.corr(), annot=True, mask=matrix,linewidths=.5, ax=ax)","8d421ad4":"#Dropping those columns\ntraindf.drop(['X_3','X_1_new','DAYOFWEEK','X_7'], axis=1, inplace=True)\ntraindf.head()","ca68d61c":"fig, ax = plt.subplots(figsize=(25,10))         # Sample figsize in inches\nmatrix = np.triu(traindf.corr())\nsns.heatmap(traindf.corr(), annot=True, mask=matrix,linewidths=.5, ax=ax)","2d878a35":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import classification_report\n\nx = traindf.drop([\"X_5\",\"X_6\",\"X_9\",\"X_13\",\"MONTH\",\"DAY\",\"MULTIPLE_OFFENSE\"], axis=1)\ny = traindf[\"MULTIPLE_OFFENSE\"]","5be97588":"y.plot.hist()\ny.value_counts()","62163e6c":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.3)","5fd958be":"from sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\nLR= LogisticRegression(penalty='none')\nLR= LR.fit(x_train,y_train)\nY_pred = LR.predict(x_test)","80fff0d3":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred,average='binary'))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","798c4efc":"from sklearn import model_selection\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.naive_bayes import BernoulliNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn import tree\nfrom sklearn.ensemble import GradientBoostingClassifier","1eeaf5de":"# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","89172653":"seed = 7\n# prepare models\nmodels = []\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('DecisonTree', DecisionTreeClassifier()))\nmodels.append(('NB', BernoulliNB()))\nmodels.append(('SVM', SVC()))\nmodels.append(('BaggingDecisonTree', BaggingClassifier(tree.DecisionTreeClassifier(random_state=1))))\nmodels.append(('Adaboost',AdaBoostClassifier()))\nmodels.append(('Logistic', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('GradientBoosting', GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,max_depth=1, random_state=0)))\n\n# evaluate each model in turn\nresults = []\nnames = []\nperformance=[]\nscoring = 'recall_macro'\nfor name, model in models:\n\tkfold = model_selection.KFold(n_splits=15, random_state=seed)\n\tcv_results = model_selection.cross_val_score(model, x_train,y_train, cv=kfold, scoring=scoring)\n\tresults.append(cv_results)\n\tnames.append(name)\n\tmsg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n\tperformance.append(msg)\n# boxplot algorithm comparison\nfig = plt.figure(figsize=(20,8))\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results,widths = 0.5)\nax.set_xticklabels(names)\nplt.show()\n\nfor perf in performance:\n    print(perf)","6c0f7700":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier(criterion=\"entropy\", max_depth=3)\n\n# Train Decision Tree Classifer\nclf = clf.fit(x_train,y_train)\n\n#Predict the response for test dataset\ny_pred = clf.predict(x_test)","d770515e":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred,average='binary'))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","eaabca1f":"from sklearn.ensemble import BaggingClassifier\ndtc = DecisionTreeClassifier(criterion=\"entropy\")\nbag_model=BaggingClassifier(base_estimator=dtc, n_estimators=100, bootstrap=True)\nbag_model=bag_model.fit(x_train,y_train)\nY_pred=bag_model.predict(x_test)","b7d6f2ea":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred,average='binary'))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","a7c5ef1e":"from sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=2)\nclf.fit(x_train, y_train)\nY_pred=clf.predict(x_test)","c401d692":"from sklearn import metrics\nprint(metrics.confusion_matrix(y_test, Y_pred), \"\\n\")\nprint(\"accuracy\", metrics.accuracy_score(y_test, Y_pred))\nprint(\"precision\", metrics.precision_score(y_test,Y_pred))\nprint(\"recall\", metrics.recall_score(y_test,Y_pred,average='binary'))\nconfusion=confusion_matrix(y_test,Y_pred)    \nTP = confusion[1,1] # true positive \nTN = confusion[0,0] # true negatives\nFP = confusion[0,1] # false positives\nFN = confusion[1,0] # false negatives\n# Let's see the sensitivity of our logistic regression model\nprint(\"Sensitivity\",TP \/ float(TP+FN))\n# positive predictive value \nprint (\"Positive Predection Rate\",TP \/ float(TP+FP))\n# Negative predictive value\nprint (\"Negative Predection rate\",TN \/ float(TN+ FN))\n# Calculate false postive rate - predicting churn when customer does not have churned\nprint(\"False positive Predection Rate\",FP\/ float(TN+FP))","110a94d6":"### 2.1 Decision Tree","c018ff8b":"### 1.6 Corelation between Features","b5e86407":"## 1. Data Analysis","efeaeff0":"### 1.3 Handling Date Object","dfe02db3":"Decision Tree, with bagging and boosting have better recall score when compared with other models, so we can consider them as challenger models and dig deep","c0bae311":"X_1,X_10,X_12 are categorical data \nX_2, X_3, X_4, X_5  are nominal data and looks good in terms of outliers, remanining have to be analysed more","9da4df85":"Date and Incident ID belongs to Object type, they may have to be handled differntly","f6a9dd3f":"* X_2 and X_3 are highly corelated\n* X_1_new and X_6,X-7 are highly correlated\n* X_6 and X_7 are highly correlated\n* DayOfWeek and Weekend are highly correlated","5941c13c":"**Clearly Dataset is Imbalanced**","e367b569":"## 2.Model Selection","26077e56":"### 1.1 Target Variable","66841abe":"Very less percentage  of values are null, Lets see how it impacts the target variable","9f1d6989":"### 2.3 Boosting","292cbe7d":"### 2.2 Bagging","8e9b47c0":"### 1.2 Handling Null Values","c6830c1c":"### 1.5 Handling Categorical data","6f506643":"All the Null values belong to Multiple_offense category \"1\" which is the highest frequent class, so we can remove the records with null entires","edf6bfd8":"### 1.4 Outlier Handling"}}