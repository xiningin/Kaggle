{"cell_type":{"93006e47":"code","01b14f44":"code","cc8ccce1":"code","0598a53a":"code","a0be2f88":"code","59baeb53":"code","dcfd6008":"code","62c2415a":"code","ac1ca57d":"code","52c46790":"code","e754d58d":"code","6aef80c3":"code","124399c5":"code","43ab4250":"code","4fcd6e5e":"code","16eb0af5":"code","e07c57b8":"code","1d6272ad":"code","25ce329a":"code","72b96534":"code","90742222":"code","543c7235":"code","514cc8e3":"code","eead5876":"code","f4fc3cde":"code","f466cf22":"code","fa987968":"code","efe44464":"code","bf92f5e9":"code","9fc130e2":"code","6a5e2610":"code","9b0b0350":"code","ee6828ec":"code","773eca80":"code","8332bdc1":"code","76324b80":"code","e26df1fa":"code","196e3688":"code","516838ca":"code","42f1dc1c":"code","2374796b":"code","8f5dd375":"code","84d80517":"code","827727d5":"code","fb58df5d":"code","71522a58":"code","8537489e":"code","8a843a86":"code","9a0a4480":"code","4da75840":"code","bca17195":"code","c57fce90":"code","94f6503a":"code","f7a38960":"code","b6223f9f":"code","5a769b0e":"code","0d3984fc":"code","cac5ff78":"code","62a117b8":"code","28a090fe":"code","9e188c98":"code","d0affa26":"code","348685a0":"code","b0b9bd86":"code","e692b282":"code","d32e516b":"code","4353ab1e":"code","2ea4d30d":"code","651f75ca":"code","2fd60ac7":"markdown","51a27ea2":"markdown","f0229f9f":"markdown","388d92cc":"markdown","41f6e8be":"markdown","e2f2a293":"markdown","68862caa":"markdown","19a5a7b9":"markdown","442ffad1":"markdown","c58ccd9e":"markdown","00997434":"markdown","4da1aec8":"markdown","b15599a6":"markdown","977fa0a8":"markdown","c761185b":"markdown","d93d9367":"markdown","a88a5cc3":"markdown","2f74829e":"markdown","5c36d53e":"markdown","fc9c22cb":"markdown","a5ad3c7d":"markdown","37f04fb3":"markdown","fa2ca377":"markdown","03740d4b":"markdown","6f63a7e2":"markdown","f80581b4":"markdown","4689a61e":"markdown","4d8bf840":"markdown","fab53234":"markdown","afe183dd":"markdown"},"source":{"93006e47":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nfrom tqdm import tqdm\nimport os\nprint(os.listdir(\"..\/input\"))\ntqdm.pandas()\n\n# Any results you write to the current directory are saved as output.","01b14f44":"df = pd.read_csv('..\/input\/cvpr-papers-to-csv\/cvpr2019.csv')","cc8ccce1":"df.head()","0598a53a":"# Oh I mistakingly save index as well. I can remove there but let's  delete it.\ndf.drop('Unnamed: 0', axis=1, inplace=True)","a0be2f88":"df.head()","59baeb53":"df.describe()","dcfd6008":"df.info()","62c2415a":"df['content'] = df['content'].apply(str)\ndf['abstract'] = df['abstract'].apply(str)\ndf['authors'] = df['authors'].apply(str)\ndf['title'] = df['title'].apply(str)","ac1ca57d":"df.info()","52c46790":"# Now the first question is what is the distribution of number of authors.  Let's find out\nsns.distplot(df['authors'].str.split(',').apply(len))","e754d58d":"print('The mean of the distribution is', df['authors'].str.split(',').apply(len).mean(), 'and the standard deviation is', df['authors'].str.split(',').apply(len).std())","6aef80c3":"a = pd.Series([item for sublist in df['authors'].str.split(',') for item in sublist])\na = a.str.strip()\na.value_counts()[:10]","124399c5":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator","43ab4250":"def plot_wordcloud(text, mask=None, max_words=400, max_font_size=120, figure_size=(24.0,16.0), title = None, title_size=40, image_color=False):\n    \"\"\"\n    Function Credit: https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes\n    \"\"\"\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='white',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    mask = mask)\n    wordcloud.generate(text)\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'green', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()\n        \ndef plot_the_author(name):\n    author_paper_abstract = df[df['authors'].str.contains(name)]['abstract']\n    plot_wordcloud(str(author_paper_abstract), max_words=600, max_font_size=120,  title = 'What ' + name + ' is upto?', title_size=20, figure_size=(10,12))","4fcd6e5e":"plot_the_author('Wei Liu')","16eb0af5":"plot_the_author('Xiaogang Wang')","e07c57b8":"plot_the_author('Ling Shao')","1d6272ad":"import networkx as nx  \nfrom tqdm import tqdm","25ce329a":"def relation_graph(name):\n    a = df[df['authors'].str.contains(name)]['authors'].str.split(', ')\n    a = a.tolist()\n    edge_list = set()\n    for l in a:\n        n = len(l)\n        for i in range(n):\n            for j in range(i+1, n):\n                edge_list.add((l[i].strip(), l[j].strip()))\n    edge_list = list(edge_list)\n    G = nx.DiGraph()\n    G.add_edges_from(edge_list)\n    f, ax = plt.subplots(figsize=(18, 12))\n    nx.draw(G.to_undirected(),  with_labels=True, font_weight='bold', ax=ax)\n    plt.show()","72b96534":"relation_graph('Wei Liu')","90742222":"relation_graph('Xiaogang Wang')","543c7235":"relation_graph('Ling Shao')","514cc8e3":"sns.distplot(df['abstract'].str.len())","eead5876":"print('The mean of the distribution is', df['abstract'].str.len().mean(), 'and the standard deviation is', df['abstract'].str.len().std())","f4fc3cde":"plot_wordcloud('\\n'.join(df['abstract'].tolist()), title_size=20, figure_size=(10,12), title=\"Abstract Wordcloud\", max_words=800)","f466cf22":"plot_wordcloud('\\n'.join(df['title'].tolist()), title_size=20, figure_size=(10,12), title=\"Title Wordcloud\", max_words=800)","fa987968":"sns.distplot(df['content'].str.len())","efe44464":"print(df['content'].iloc[0][:1000])","bf92f5e9":"df['content'] = df['content'].str.lower()","9fc130e2":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\", 'u.s':'america', 'e.g':'for example'}","6a5e2610":"def clean_contractions(text, mapping):\n    specials = [\"\u2019\", \"\u2018\", \"\u00b4\", \"`\"]\n    for s in specials:\n        text = text.replace(s, \"'\")\n    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n    return text","9b0b0350":"df['content'] = df['content'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))","ee6828ec":"punct = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', '[', ']', '.', ',']","773eca80":"punct_mapping = {\"\u2018\": \"'\", \"\u20b9\": \"e\", \"\u00b4\": \"'\", \"\u00b0\": \"\", \"\u20ac\": \"e\", \"\u2122\": \"tm\", \"\u221a\": \" sqrt \", \"\u00d7\": \"x\", \"\u00b2\": \"2\", \"\u2014\": \"-\", \"\u2013\": \"-\", \"\u2019\": \"'\", \"_\": \"-\", \"`\": \"'\", '\u201c': '\"', '\u201d': '\"', '\u201c': '\"', \"\u00a3\": \"e\", '\u221e': 'infinity', '\u03b8': 'theta', '\u00f7': '\/', '\u03b1': 'alpha', '\u2022': '.', '\u00e0': 'a', '\u2212': '-', '\u03b2': 'beta', '\u2205': '', '\u00b3': '3', '\u03c0': 'pi', '!':' '}","8332bdc1":"def clean_special_chars(text, punct, mapping):\n    for p in mapping:\n        text = text.replace(p, mapping[p])\n    \n    for p in punct:\n        text = text.replace(p, f' ')    \n    return text","76324b80":"df['content'] = df['content'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))","e26df1fa":"print(df['content'].iloc[0][:1000])","196e3688":"import re","516838ca":"def remove_citations(text):\n    \n    return re.sub(\"\\[ [0-9]+[,  [0-9]+]*\\]\", \"\", text)","42f1dc1c":"a = 'asdas[ 4, 6 ]asds'\nre.findall(\"\\[ [0-9]+[,  [0-9]+]*\\]\", a)","2374796b":"print(remove_citations(df['content'].iloc[0])[-1000:])","8f5dd375":"df['content'] = df['content'].progress_apply(remove_citations)","84d80517":"def remove_ref_sec(text):\n    idx = text.rfind('references')\n    return text[:idx].strip()","827727d5":"df['content'] = df['content'].progress_apply(remove_ref_sec)","fb58df5d":"def rem_author_names(text):\n    return text[3] + '\\n\\n' + text[0][text[0].find('abstract'):]","71522a58":"df['content'] = df.progress_apply(rem_author_names, axis=1)","8537489e":"df.shape","8a843a86":"print(df['content'].iloc[0][:1000])","9a0a4480":"import nltk\nfrom tqdm import tqdm\ntqdm.pandas()","4da75840":"df['content'] = df['content'].progress_apply(nltk.word_tokenize)","bca17195":"en_stopwords = set(nltk.corpus.stopwords.words('english'))\ndf['content'] = df['content'].progress_apply(lambda x: [item for item in x if item not in en_stopwords])","c57fce90":"#function to filter for ADJ\/NN bigrams\ndef rightTypes(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    acceptable_types = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    second_type = ('NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in acceptable_types and tags[1][1] in second_type:\n        return True\n    else:\n        return False\n#filter bigrams\n#filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n#function to filter for trigrams\ndef rightTypesTri(ngram):\n    if '-pron-' in ngram or 't' in ngram:\n        return False\n    for word in ngram:\n        if word in en_stopwords or word.isspace():\n            return False\n    first_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    third_type = ('JJ', 'JJR', 'JJS', 'NN', 'NNS', 'NNP', 'NNPS')\n    tags = nltk.pos_tag(ngram)\n    if tags[0][1] in first_type and tags[2][1] in third_type:\n        return True\n    else:\n        return False\n#filter trigrams\n#filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]","94f6503a":"def get_bigram_trigrams(tokens, title, return_finders = False):\n    bigrams = nltk.collocations.BigramAssocMeasures()\n    trigrams = nltk.collocations.TrigramAssocMeasures()\n    bigramFinder = nltk.collocations.BigramCollocationFinder.from_words(tokens)\n    trigramFinder = nltk.collocations.TrigramCollocationFinder.from_words(tokens)\n    #bigrams\n    bigram_freq = bigramFinder.ngram_fd.items()\n    bigramFreqTable = pd.DataFrame(list(bigram_freq), columns=['bigram','freq']).sort_values(by='freq', ascending=False)\n    #trigrams\n    trigram_freq = trigramFinder.ngram_fd.items()\n    trigramFreqTable = pd.DataFrame(list(trigram_freq), columns=['trigram','freq']).sort_values(by='freq', ascending=False)\n    filtered_bi = bigramFreqTable[bigramFreqTable.bigram.map(lambda x: rightTypes(x))]\n    filtered_tri = trigramFreqTable[trigramFreqTable.trigram.map(lambda x: rightTypesTri(x))]\n    if return_finders:\n        return bigramFinder, trigramFinder, bigrams, trigrams\n    print('Exploring bigrams and trigrams of ' + title)\n    print(filtered_bi.head(10))\n    print(filtered_tri.head(10))\n    return filtered_bi, filtered_tri","f7a38960":"id = 45\n_, __ = get_bigram_trigrams(df['content'].iloc[id], df['title'].iloc[id])","b6223f9f":"id = 23\n_, __ = get_bigram_trigrams(df['content'].iloc[id], df['title'].iloc[id])","5a769b0e":"id = 11\n_, __ = get_bigram_trigrams(df['content'].iloc[id], df['title'].iloc[id])","0d3984fc":"def get_pointwise_mi_scores(content, title):\n    bigramFinder, trigramFinder, bigrams, trigrams = get_bigram_trigrams(content, title, True)\n    #filter for only those with more than 20 occurences\n    bigramFinder.apply_freq_filter(20)\n    trigramFinder.apply_freq_filter(20)\n    bigramPMITable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.pmi)), columns=['bigram','PMI']).sort_values(by='PMI', ascending=False)\n    trigramPMITable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.pmi)), columns=['trigram','PMI']).sort_values(by='PMI', ascending=False)\n    print('Exploring Point wise Mututal Information in bigrams and trigrams of ' + title)\n    print(bigramPMITable.head(10))\n    print(trigramPMITable.head(10))\n    return bigramPMITable, trigramPMITable","cac5ff78":"id = 45\n_, __ = get_pointwise_mi_scores(df['content'].iloc[id], df['title'].iloc[id])","62a117b8":"id = 23\n_, __ = get_pointwise_mi_scores(df['content'].iloc[id], df['title'].iloc[id])","28a090fe":"id = 11\n_, __ = get_pointwise_mi_scores(df['content'].iloc[id], df['title'].iloc[id])","9e188c98":"def get_t_scores(content, title):\n    bigramFinder, trigramFinder, bigrams, trigrams = get_bigram_trigrams(content, title, True)\n    bigramTtable = pd.DataFrame(list(bigramFinder.score_ngrams(bigrams.student_t)), columns=['bigram','t']).sort_values(by='t', ascending=False)\n    trigramTtable = pd.DataFrame(list(trigramFinder.score_ngrams(trigrams.student_t)), columns=['trigram','t']).sort_values(by='t', ascending=False)\n    #filters\n    filteredT_bi = bigramTtable[bigramTtable.bigram.map(lambda x: rightTypes(x))]\n    filteredT_tri = trigramTtable[trigramTtable.trigram.map(lambda x: rightTypesTri(x))]\n    print('Exploring t scores between the words in bigrams and trigrams of ' + title)\n    print(filteredT_bi.head(10))\n    print(filteredT_tri.head(10))\n    return filteredT_bi, filteredT_tri","d0affa26":"id = 45\n_, __ = get_t_scores(df['content'].iloc[id], df['title'].iloc[id])","348685a0":"id = 23\n_, __ = get_t_scores(df['content'].iloc[id], df['title'].iloc[id])","b0b9bd86":"id = 11\n_, __ = get_t_scores(df['content'].iloc[id], df['title'].iloc[id])","e692b282":"import heapq\nfrom operator import itemgetter","d32e516b":"def get_n_most_freq(tokens, n=10):\n    allWordDist = nltk.FreqDist(w.lower() for w in tokens)\n    topitems = heapq.nlargest(iterable=allWordDist.items(), key=itemgetter(1), n=n)\n    topitemsasdict = dict(topitems)\n    return list(topitemsasdict.keys())","4353ab1e":"df['ten_most_freq'] = df['content'].progress_apply(get_n_most_freq)","2ea4d30d":"df.head()","651f75ca":"plot_wordcloud('\\n'.join(np.concatenate(df['ten_most_freq']).tolist()), title_size=20, figure_size=(12,18), title=\"Most frequent words in the papers\", max_words=1200)","2fd60ac7":"** Same observation can be infer from the above graphs. Though they are less dense and interconnected, we can say that dense cluster of cliques show a reseach lab.**\n\n** I think we've done enough analysis of authors. If you find anything missing here, please mention in the comments. We can discuss it and I might add it on future versions.**","51a27ea2":"** Oh Guassian like distribution. Who doesn't like that. Let's find out mean & SD of the distribution**","f0229f9f":"### Conclusion\nCVPR  is the best computer vision conference out there. In this kernel you may found out why. The broad domain of topics along with new ideas made the conference a premium one. Other than that, we've explored that multiple research group are working on crucial computer vision problems and prefer this journal over many others. Hence, I think this analysis is necessary. Please upvore the kernel and if you think I missed anything, kindly let me know in the commments. \n### Thank you for reading\n\n","388d92cc":"** Let's now find the bigrams and trigrams with highest pointwise mututal information. The main intuition is that it measures how much more likely the words co-occur than if they were independent. However, it is very sensitive to rare combination of words. **","41f6e8be":"**That's expected, there are only two group of authors which has two papers. All the title are different which is also expected.**","e2f2a293":"** Xianogang is definitely working on 3D image. His work is mostly related to captioning and synthesizing**","68862caa":"** All I can pick is some words which are common in abstract like propose, address etc. But words like face, video, generation gives quite a hint about Wei's work**","19a5a7b9":"**With that graph, we can infer many insights. First is  the different cliques implies different teams who published work together. But we can see, on the left the cliques  are distinguishable but in the bottom and the right, the clique is more interconnected. Researchers like *Wenhen Luo* and *Zhifeng Li* are part of many teams. We can even say that it is the whole research team working on multiple projects and hence so much interconnection.**","442ffad1":"#### Basic cleaning and preprocessing","c58ccd9e":"** Let's try to find the relation graph of the few top authors**","00997434":"** I believe it's a better representation. It seems that \"Image\" and \"model\" are the most frequent words(It's a computer vision conference). Most of the work  has been done graphs, video, attention and segmentation. You can even get glimplse of \"shot\", \"adverserial\" and \"motion\" which are hot topics in CV now, **","4da1aec8":"** Words like image, model, method, features are most frequent which is obvious as CVPR is a computer vision premium conference. The above graph indicated what commonly researchers worked on.The work mostly related to the novel architectures and most of them are learning based on neural networks. Also, 3D images and video processing and prediction dominated the conference.**","b15599a6":"#### Removing author names from the beginning","977fa0a8":"## Data Analysis","c761185b":"#### Removing citations & References","d93d9367":"## Reading the Data\nYeah, our first task is to read the data and get some info about it.","a88a5cc3":"## Introduction\n**CVPR is the premier annual computer vision event comprising the main conference and several co-located workshops and short courses. With its high quality and low cost, it provides an exceptional value for students, academics and industry researchers.**\n\n\n<img src=\"http:\/\/cvpr2019.thecvf.com\/images\/CVPRLogo.png\"\n     alt=\"Markdown Monster icon\"\n     style=\"float: left; margin-right: 10px;\" \/>\n\n\n\n\nCVPR is one of the best conferences in machine learning and deep learning. The **CVPR 2019 Papers** contains all the paper presented in CVPR 2019. Now, I'll process and analyse the data throughly. But first our challenge was to get the data in a desired format since the is in PDF format and hence unsuitable for through analysis. Hence in [CVPR papers to CSV](https:\/\/www.kaggle.com\/hsankesara\/cvpr-papers-to-csv\/), I converted the pdf data into structured csv  format so now  it's time to clean, understand and get some insights from the data.","2f74829e":"### Let's talk about authors","5c36d53e":"#### Removing Stop Words","fc9c22cb":"** Much better now. You can observe that occurance of this words are common given the title of the paper. **","a5ad3c7d":"**Let's start with most frequent bigrams and trigrams in the text.**","37f04fb3":"** Oh Wei Liu is publishing like hell. Well, let's out what some of them are upto?**","fa2ca377":"**You may have seen few tuples which qualitaatively convey no information but have highest correlation\/mutual information**\n\n** To find out significance of each tuple, we'll use t score and score the bi and tri grams**","03740d4b":"** I believe title gave us better idea about the conference. We can easily conclude that neural network and deep learning models are in abundance. We can also find words like detection, video, Semantic, 3D, Graph and object detection which give us idea about the main themes of the conference. **","6f63a7e2":"### Abstract Analysis","f80581b4":"**  Good ol Guassian distribution with mean 1173 characters and SD of 236.25 ** ","4689a61e":"** Well, Ling is working on few shot learning especially Zero shot learning which is a hot topic in computer vision. This is the last author but if you want to know about more authors fork the notebook and try it yourself.**","4d8bf840":"#### Paper Content Analysis","fab53234":"### Content data cleaning and preprocessing","afe183dd":"** It seems usually there are 3-6 authors per article. This makes sense because usual group size for a research project is the same. Now what, Let's find out about the the most frequent researchers. Just an advice: You might wanna follow them. :)**"}}