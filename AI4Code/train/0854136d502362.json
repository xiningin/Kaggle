{"cell_type":{"27b4d799":"code","e0d1776a":"code","9aec1f7f":"code","bd8bfbf6":"code","9d1dcce2":"code","b31846c4":"code","ce45b033":"code","cc024363":"code","86ef506e":"code","b7999571":"code","a167873c":"code","b1edb866":"code","3c210bb4":"code","07c1eeb5":"code","707d4ebe":"code","e04edcce":"code","97f62995":"code","536bbbf8":"code","850cdfa7":"code","40329fa7":"code","5f0667c1":"code","9c9f18d4":"code","f8fe67fa":"code","c2e7d769":"code","b63249d5":"code","870b3f8f":"code","de3629e9":"code","a41c75c7":"code","8eab455d":"code","f04bc1e4":"code","27be621d":"code","081168a9":"code","40fefc29":"code","f65dc3ac":"code","df3a8d3b":"code","4d56a9be":"code","ef41d5c1":"code","bc166a17":"code","037ab2dc":"code","2e6d2378":"code","5ff388d0":"code","0bfc26b0":"code","21a46e9e":"code","fa48006c":"code","90a3301e":"code","a3f42c7f":"code","851c83c4":"code","0bdb1298":"code","63a389de":"code","4756f588":"code","8a8e1051":"code","5ff55766":"code","ca2238cf":"code","422d88c7":"code","a17551a1":"code","c8be8225":"code","7705735d":"code","83a613a1":"code","d32e5ded":"code","8ad9417a":"code","53ed5a1f":"code","cfb5b012":"code","8e011602":"code","88278567":"code","0d676eef":"markdown","5ded012b":"markdown","9ae04ac1":"markdown","d860edea":"markdown","2878d6d1":"markdown","995d7f71":"markdown","d334c58c":"markdown","4ec50b38":"markdown","b625d4da":"markdown","f0a2ba1f":"markdown","f83a4f6b":"markdown","4a477c8d":"markdown","dd22decf":"markdown","8a6e83c5":"markdown","3c4641c3":"markdown","baba714b":"markdown","25cb30b5":"markdown","1697e180":"markdown","e027d1a8":"markdown"},"source":{"27b4d799":"import numpy as np\nimport pandas as pd\nfrom sklearn.preprocessing import RobustScaler\nrb=RobustScaler()\nfrom imblearn.over_sampling import RandomOverSampler\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.style as stl\nstl.use(\"ggplot\")\nimport warnings\nwarnings.filterwarnings(\"ignore\")","e0d1776a":"data=pd.read_csv(\"..\/input\/imputed-stroke-dataset\/impstroke.csv\")\ndata.drop('Unnamed: 0', axis=1, inplace=True)\nx = data.drop(\"stroke\",axis=1)\ny = data[\"stroke\"]","9aec1f7f":"xfe, yfe = RandomOverSampler(sampling_strategy=0.25, random_state=11).fit_resample(x, y)","bd8bfbf6":"xref = xfe.copy(deep=True)\nxtest = x.copy(deep=True)","9d1dcce2":"xfe[\"Blood&Heart\"]=xfe[\"hypertension\"]*xfe[\"heart_disease\"]\nxtest[\"Blood&Heart\"]=xtest[\"hypertension\"]*xtest[\"heart_disease\"]","b31846c4":"xfe[\"Effort&Duration\"] = xfe[\"work_type\"]*(xfe[\"age\"])\nxtest[\"Effort&Duration\"] = xtest[\"work_type\"]*(xtest[\"age\"])","ce45b033":"xfe[\"Obesity\"] = xfe[\"bmi\"]*xfe[\"avg_glucose_level\"]\/1000\nxtest[\"Obesity\"] = xtest[\"bmi\"]*xtest[\"avg_glucose_level\"]\/1000","cc024363":"xfe[\"AwfulCondition\"] = xfe[\"Obesity\"] * xfe[\"Blood&Heart\"] * xfe[\"smoking_status\"]\nxtest[\"AwfulCondition\"] = xtest[\"Obesity\"] * xtest[\"Blood&Heart\"] * xtest[\"smoking_status\"]","86ef506e":"xfe[\"AwfulCondition\"].unique()","b7999571":"#effect of residence type on Effort&Duration","a167873c":"xfe.head()","b1edb866":"from sklearn.decomposition import PCA","3c210bb4":"pca = PCA()","07c1eeb5":"pca_feats = pca.fit_transform(rb.fit_transform(xref))","707d4ebe":"pca.explained_variance_ratio_","e04edcce":"list(range(1,11))","97f62995":"fig=plt.figure(figsize=(20,9))\nsns.barplot(x=list(range(1,11)),y=pca.explained_variance_,palette = 'Reds_r')\nplt.ylabel('Variation',fontsize=15)\nplt.xlabel('PCA Components',fontsize=15)\nplt.title(\"PCA Components\\nRanked by Variation\",fontsize=25)\nplt.show()","536bbbf8":"xfe[\"PC1\"], xfe[\"PC2\"] = pca_feats[:,0], pca_feats[:,1]","850cdfa7":"xtestpca = pca.transform(rb.transform(x))","40329fa7":"xtest[\"PC1\"], xtest[\"PC2\"] = xtestpca[:,0], xtestpca[:,1]","5f0667c1":"xfe.head()","9c9f18d4":"from sklearn.decomposition import FastICA as ICA","f8fe67fa":"ica = ICA(random_state=11)","c2e7d769":"xica = ica.fit_transform(X=rb.fit_transform(xref))","b63249d5":"ncomp = ica.components_.shape[0]","870b3f8f":"fig,axes=plt.subplots(ncols=1,nrows=ncomp,figsize=(20,10*ncomp))\nfig.suptitle(\"Target Distributions\\nAcross ICA Components\",fontsize=40)\nfor i in range(ncomp):\n    sns.boxenplot(y=xica[:,i], x=yfe, palette=\"seismic\",showfliers=True,ax=axes[i])\n    axes[i].set_xlabel(\"Stroke\",fontsize=15)\n    axes[i].set_ylabel(f\"IC{i+1}\",fontsize=25)\nplt.show()","de3629e9":"xfe[\"ICA\"] = xica[:,3]","a41c75c7":"xtest[\"ICA\"] = ica.transform(rb.transform(x))[:,3]","8eab455d":"from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nlda = LinearDiscriminantAnalysis()","f04bc1e4":"xlda = lda.fit_transform(rb.fit_transform(xref),yfe)\nxlda = xlda.reshape((xlda.shape[0],))","27be621d":"plt.figure(figsize=(20,8))\nsns.boxenplot(y=xlda, x=yfe, color='crimson',showfliers=True)\nplt.title(\"Separation of Classes with LDA\",fontsize=30)\nplt.xlabel(\"Stroke\",fontsize=20)\nplt.show()","081168a9":"xfe[\"LDA\"] = xlda","40fefc29":"xtest[\"LDA\"] = lda.transform(rb.transform(x)).reshape((x.shape[0],))","f65dc3ac":"from sklearn.cluster import KMeans","df3a8d3b":"inertias = []\n\nks=list(range(1,10))\n\n#xkm = rb.fit_transform(xref)\n\nxfesc = rb.fit_transform(xfe)\nxtsc = rb.transform(xtest)\n\nfor k in ks:\n    \n    model=KMeans(n_clusters=k)\n    \n    model.fit(xfesc)\n    \n    inertias.append(model.inertia_)","4d56a9be":"plt.figure(figsize=(20,7))\nsns.barplot(x = ks, y = inertias, palette='mako')\nplt.xlabel('Number of Clusters',fontsize=20)\nplt.ylabel('Inertia',fontsize=20)\nplt.xticks(ks)\nplt.title(\"Inertia per Number of KMeans Clusters\",fontsize=30)\nplt.show()","ef41d5c1":"figure, axes = plt.subplots(nrows=2, ncols=2,figsize=(20, 30))\nfigure.suptitle('\\n\\nTarget Proportions per Cluster', fontsize=40)\n\nfor index in range(4):\n    \n    i,j = (index \/\/ 2), (index % 2)\n    \n    model=KMeans(n_clusters=index+2)\n    \n    model.fit(xfesc)\n    \n    cluster_labels=model.predict(xfesc)\n    \n    sns.heatmap(pd.crosstab(cluster_labels,yfe,normalize=\"index\"),\n                ax=axes[i,j],\n                cmap='Blues',\n                square='True',\n                cbar=False,\n                annot=True,\n                annot_kws={'fontsize':30})\n    \n    axes[i,j].set_title(f\"{index+2} Clusters\",fontsize=30)\n    \n    axes[i,j].set_xlabel(\"Stroke\",fontsize=20)\n\n    axes[i,j].set_ylabel(\"Cluster Labels\",fontsize=20)\n    \n    axes[i,j].set_yticklabels(axes[i,j].get_yticklabels(),fontsize=20)\n\nplt.show()","bc166a17":"from sklearn.metrics import adjusted_mutual_info_score\nfrom sklearn.metrics import normalized_mutual_info_score","037ab2dc":"ami = []\nnmi = []\nfor k in ks:\n    model = KMeans(n_clusters = k)\n    cluster_labels=model.fit_predict(xfesc)\n    ami.append(adjusted_mutual_info_score(yfe,cluster_labels))\n    nmi.append(normalized_mutual_info_score(yfe,cluster_labels))","2e6d2378":"plt.figure(figsize=(20,7))\nsns.barplot(x = ks, y = ami, palette='summer_r')\nplt.xlabel('Number of Clusters',fontsize=20)\nplt.ylabel('AMI',fontsize=20)\nplt.xticks(ks)\nplt.title(\"Adjusted Mutual Information per Number of Clusters\",fontsize=30)\nplt.show()","5ff388d0":"plt.figure(figsize=(20,7))\nsns.barplot(x = ks, y = nmi, palette='summer_r')\nplt.xlabel('Number of Clusters',fontsize=20)\nplt.ylabel('AMI',fontsize=20)\nplt.xticks(ks)\nplt.title(\"Normalized Mutual Information per Number of KMeans Clusters\",fontsize=30)\nplt.show()","0bfc26b0":"model=KMeans(n_clusters=4)\n    \nmodel.fit(xfesc)\n    \ncluster_labels=model.predict(xfesc)","21a46e9e":"cluster_labels = pd.Series(cluster_labels).astype(\"object\")","fa48006c":"from category_encoders.target_encoder import TargetEncoder","90a3301e":"kmeans_enc=TargetEncoder()\nenc_clus = kmeans_enc.fit_transform(cluster_labels, y=yfe)","a3f42c7f":"xfe[\"KMeans\"] = enc_clus","851c83c4":"cluster_labels_test = pd.Series(model.predict(xtsc)).astype(\"object\")\nxtest[\"KMeans\"] = kmeans_enc.transform(cluster_labels_test, y=y)","0bdb1298":"from sklearn.cluster import DBSCAN","63a389de":"eps = [1.01,1.02,1.05,1.1,1.15,1.2,1.25,1.3,1.35]\nmin_samples = [7,8,9,10,11,12,13,14,15]","4756f588":"inds = [f\"eps={e}\" for e in eps]\ncols = [f\"min_samples={m}\" for m in min_samples]\ndbdata = pd.DataFrame(np.zeros((9,9)),columns=cols,index=inds)\n\nfor i in range(len(eps)):\n    for j in range(len(min_samples)):\n        dbscan = DBSCAN(eps=eps[i], min_samples=min_samples[j])\n        dbscan.fit(xfesc)\n        dbdata.iloc[i,j] = np.unique(dbscan.labels_).size","8a8e1051":"plt.figure(figsize=(20,9))\nsns.heatmap(dbdata, cmap='Blues', annot=True, annot_kws={'fontsize':18},cbar=False)\nplt.title(\"Number Of DBSCAN Clusters for Different Values\\nof Epsilon and Minimum_Samples\", fontsize=35)\nplt.xlabel(\"Minimum Number of Points per Cluster\",fontsize=20)\nplt.ylabel(\"Epsilon\",fontsize=20)\nplt.show()","5ff55766":"inds = [f\"  eps={e}\" for e in eps]\ncols = [f\"min_samples={m}\" for m in min_samples]\nami = pd.DataFrame(np.zeros((9,9)),columns=cols,index=inds)\nnmi = pd.DataFrame(np.zeros((9,9)),columns=cols,index=inds)\n\nfor i in range(len(eps)):\n    for j in range(len(min_samples)):\n        dbscan = DBSCAN(eps=eps[i], min_samples=min_samples[j])\n        labels=dbscan.fit_predict(xfesc)\n        nmi.iloc[i,j] = normalized_mutual_info_score(labels,yfe)\n        ami.iloc[i,j] = adjusted_mutual_info_score(labels,yfe)","ca2238cf":"plt.figure(figsize=(20,10))\nsns.heatmap(ami, cmap='mako_r', annot=True, annot_kws={'fontsize':20},cbar=False)\nplt.title(\"DBSCAN Clusters:\\nAdjusted Mutual Information Scores\", fontsize=35)\nplt.xlabel(\"Minimum Number of Points per Cluster\",fontsize=20)\nplt.ylabel(\"Epsilon\",fontsize=20)\nplt.show()","422d88c7":"plt.figure(figsize=(20,10))\nsns.heatmap(nmi, cmap='mako_r', annot=True, annot_kws={'fontsize':20},cbar=False)\nplt.title(\"DBSCAN Clusters:\\nNormalized Mutual Information Scores\", fontsize=35)\nplt.xlabel(\"Minimum Number of Points per Cluster\",fontsize=20)\nplt.ylabel(\"Epsilon\",fontsize=20)\nplt.show()","a17551a1":"fig, axes = plt.subplots(nrows=len(eps), ncols=len(min_samples),figsize=(20,15))\nfig.suptitle(\"DBSCAN Cluster Sizes\", fontsize=40)\n\nfor i in range(len(eps)):\n    for j in range(len(min_samples)):\n        dbscan = DBSCAN(eps=eps[i], min_samples=min_samples[j])\n        labels=dbscan.fit_predict(xfesc)\n        sns.countplot(x=labels, palette='Set2',ax=axes[i,j])\n        axes[i,j].set_xticklabels([])\n        axes[i,j].set_ylabel(None)\n        axes[i,j].set_yticklabels([])\nplt.show()","c8be8225":"from sklearn.feature_selection import SequentialFeatureSelector","7705735d":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier","83a613a1":"models = [SVC(kernel='linear'),\n          SVC(kernel='rbf'),\n          SVC(kernel='poly',degree=2),\n          RandomForestClassifier(n_jobs=-1,max_depth=10),\n          RandomForestClassifier(n_jobs=-1,max_depth=30),\n          KNeighborsClassifier(n_neighbors=4),\n          KNeighborsClassifier(n_neighbors=8),\n          LogisticRegression(),\n          GaussianNB()]","d32e5ded":"names = [\"SVM_Linear\",\"SVM_RBF\",\"SVM_Poly2\",\"ShallowForest\",\"DeepForest\",\"4NN\",\"8NN\",\"LogReg\",\"GaussianNB\"]","8ad9417a":"xfesc = rb.fit_transform(xfe)\nxtsc = rb.transform(xtest)","53ed5a1f":"ffs_scores = pd.DataFrame(np.zeros((len(names),len(names))),columns=names,index=names)\nffsdata= dict()","cfb5b012":"from sklearn.metrics import f1_score","8e011602":"for i in range(len(models)):\n    sel_name = names[i]\n    ffs=SequentialFeatureSelector(direction='forward', n_jobs = -1, estimator=models[i])\n    xffs = ffs.fit_transform(xfesc,yfe)\n    xtfs = ffs.transform(xtsc)\n    ffsdata[sel_name] = [xffs,xtfs]\n    print(f\"Finished Selection with {sel_name}\\n\")\n    print(f\"{ffs.n_features_to_select_} Features are Selected:\\n{list(xfe.columns[ffs.support_])}\\n\")\n    for j in range(len(models)):\n        pred_name = names[j]\n        model = models[j]\n        model.fit(xffs,yfe)\n        ypred = model.predict(xtfs)\n        score = f1_score(ypred,y)\n        ffs_scores.loc[sel_name,pred_name] = score\n        print(f\"F1_score with {pred_name}: {score}\")\n    print(\"\\n\\n\\n\")","88278567":"plt.figure(figsize=(20,8))\nsns.heatmap(ffs_scores,cmap=\"BuGn\",annot=True, annot_kws={'fontsize':20},cbar=False)\nplt.title(\"F1 Scores with Forward Feature Selection\\n\",fontsize=35)\nplt.xlabel(\"Predictive Model\",fontsize=20)\nplt.ylabel(\"Model Used for Selection\",fontsize=25)\nplt.show()","0d676eef":"We'll use the randomly oversampled dataset, to experiment with feature engineering.  \nThe reason being is that the original dataset is so imbalanced that perhaps no feature engineering could improve it.  \nI actually did experiment with it a bit, and it didn't seem informative at all; f1_score, recall and precision remained 0.  \nExperimenting with the randomly oversampled data would help us tell which features improve results, and then we can try other oversampling techniques.  \nAlthough one may ask: \"What if the feature engineering results are biased towards random oversampling?\"  \nWell, we're trying to construct features that separate classes better, \nLet's take a look at what we have.","5ded012b":"#### Target Encoding on Cluster Labels","9ae04ac1":"## 6 - DBSCAN Clustering","d860edea":"## 1 - Combining Features","2878d6d1":"## 2 - Principal Component Analysis","995d7f71":"**Hello and welcome**.  \n\n**This is part 3 to a 3-kernel project on Stroke Prediction.**\n\n  \n**Part 1 is Preprocessing: Data Cleaning, Target Encoding and MICE for missing values**  \nLink: **https:\/\/www.kaggle.com\/mahmoudlimam\/stroke-pre-processing-mice-target-encoding**\n\n  \n**Part 2 is EDA (including UMAP and PCA) and Random Oversampling**  \nLink: **https:\/\/www.kaggle.com\/mahmoudlimam\/stroke-eda-umap-resampling**\n\n  \n**Part 3 (which is this one) is Detailed Feature extraction and Selection, and model evaluation**  \nI didn't include a hyperparameter tuning section as Feature Engineering in an F1_Score of 1 with a somewhat deep Random Forest.","d334c58c":"# Feature Engineering","4ec50b38":"## 7 - Forward Feature Selection","b625d4da":"Well, there you go.  \nHonestly, I didn't expect results to be this good.  \nMy plan was to do feature engineering to separate the classes a bit, so that I can then use SMOTE-based Oversampling methods, such as BorderSmote, DBSMOTE, etc...  \nI also planned to use the genetic algorithm for hyperparameter tuning, and wasn't sure that I would eventually get very good results.  \n\u0627\u0644\u062d\u0645\u062f \u0644\u0644\u0647","f0a2ba1f":"# Stroke Prediction Part 3: Detailed Feature Extraction and Selection\n## (and Prediction, eventually)","f83a4f6b":"## 4 - Linear Discriminant Analysis","4a477c8d":"## 3 - Independent Component Analysis","dd22decf":"### Results:\nDBSCAN couldn't find any densely-packed clusters. It tends to lump most of the data points into one (or a few) big cluster, and several tiny clusters.  \nI can't use cluster labels as a variable for the following reasons:\n1. The tiny clusters would cause the predictive model to overfit.  \n2. Most points would be in the bigger lump (I don't think the word \"cluster\" fits it) which isn't informative as it contains pretty much all points.\n3. Sklearn doesn't have a predict method for the DBSCAN class XD They could make one that assigns points to clusters by looking at nearest neighbours but oh well. One could try coding this from scratch (which might be a little tedious to do) but for now there's no need to, considering the 2 points above.","8a6e83c5":"\u0627\u0644\u062d\u0645\u062f \u0644\u0644\u0647 \u0627\u0644\u0630\u064a \u0628\u0646\u0639\u0645\u062a\u0647 \u062a\u062a\u0645 \u0627\u0644\u0635\u0627\u0644\u062d\u0627\u062a","3c4641c3":"Anyways, thank you for reading,  \nI hope you've enjoyed and benefitted.","baba714b":"### **Summary**  :  \nI'll be using a pre-processed version of the original stroke prediction dataset.  \nIt's basically the end result of the first notebook (Part 1 linked above).  \nYou can find it here: https:\/\/www.kaggle.com\/mahmoudlimam\/imputed-stroke-dataset  \nI will use random oversampling with a minority class proportion of 0.25, since the original data is extremely imbalanced.  ","25cb30b5":"### The 2 following sections are an attempt at using cluster labels as features.  \nKMeans did a decent job.  \nDBSCAN didn't.","1697e180":"## 5 - K-Means Clustering","e027d1a8":"\u0628\u0633\u0645 \u0627\u0644\u0644\u0647"}}