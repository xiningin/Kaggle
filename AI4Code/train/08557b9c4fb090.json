{"cell_type":{"cc869984":"code","31cb6e4e":"code","7e37cdef":"code","eced9ea2":"code","209a8176":"code","2f262993":"code","4e8f9586":"code","ecbaea06":"code","075b26de":"code","d7bd9027":"code","f848f21d":"code","d19f90c0":"code","57ea4060":"markdown","1bf4f3f8":"markdown","e02ae3ea":"markdown","be68aa9f":"markdown","de38dfeb":"markdown","fa45ed30":"markdown","619cba74":"markdown","58d425a8":"markdown","18e767e3":"markdown","30f2a11d":"markdown","0c9279f4":"markdown","3fb9804c":"markdown","45e7c080":"markdown","a5967826":"markdown","1b8d1d5c":"markdown","fb524fb5":"markdown","71f9693f":"markdown","b804182a":"markdown","785c49bb":"markdown","00236ec0":"markdown","c0e779fd":"markdown","5a5ec0dc":"markdown"},"source":{"cc869984":"# Your code here, you can add cells if necessary\nimport math\n\ndef UpdateContainer(container, bucketlist):\n    itemlist = []\n    for bucket in bucketlist:\n        itemlist = container[bucket]\n        if len(itemlist) > 2:\n            start = container[bucket][0][0]\n            container[bucket].pop(0)\n            end = container[bucket][0][1]\n            container[bucket].pop(0)\n            if bucket != bucketlist[-1]:\n                container[bucket * 2].append((start, end))\n        else:\n            break\n\n\ndef OutputResult(container, bucket_list, w_size, last_time):\n    count = 0\n    w_start = last_time - w_size + 1\n    for bucket in bucket_list:\n        if len(container[bucket]) == 0:\n            continue\n        if w_start >= container[bucket][0][0] and w_start < container[bucket][-1][1]:\n            if w_start >= container[bucket][-1][0]:\n                count += 0.5 * bucket\n                continue\n            if w_start >= container[bucket][0][0]:\n                count += 1.5 * bucket\n                continue\n        if w_start <= container[bucket][0][0]:\n            count += bucket * len(container[bucket])\n            continue\n\n    return count\n\n\nwith open('..\/input\/stream-data-dgim\/stream_data_dgim.txt','r') as f:\n    content = f.read()\ndata = content.split('\\t')\n\ncontainer = {}\nwindowsize = 1000\ntimestamp = 0\n\nbucket_num = int(math.log(windowsize, 2)) + 1\nbucketlist = list()\n\n# initialize the container\nfor i in range(bucket_num):\n    bucket = int(math.pow(2, i))\n    bucketlist.append(bucket)\n    container[bucket] = list()\n\n\ntimestamp = 0\ncharlist = []\n\nfor char in data:\n    if char == '1':\n        container[1].append((timestamp, timestamp))\n        UpdateContainer(container, bucketlist)\n        \n    timestamp += 1\n\n\nprint(\"Estimation of 1 in the current window: \", int(OutputResult(container, bucketlist, windowsize, len(data) - 1)))","31cb6e4e":"# Your code here, you can add cells if necessary\nprint(\"Number of 1 in last 500 bits:\", int(OutputResult(container,bucketlist,500,len(data)-1)))\nprint(\"Number of 1 in last 500 bits:\", int(OutputResult(container,bucketlist,200,len(data)-1)))","7e37cdef":"import time\n# Your code here, you can add cells if necessary\ndef count_accurately(words, start, window_size):\n    cnt = 0\n    word = words[start: start + window_size]\n    for i in range(window_size):\n        if word[i] == '1':\n            cnt += 1\n    return cnt\n\nwith open('..\/input\/stream-data-dgim\/stream_data_dgim.txt', 'r') as f:\n    data = f.readline().split('\\t')\n\nprint('------ Brute Force ------')\nstart = time.time()\nground_truth = []\ntruth = count_accurately(data, 39000, 1000)\nprint(\"result: \", truth)\nprint(\"running time:\", time.time()-start)\n\nprint('\\n------ DGIM ------')\nstart = time.time()\nestimation = int(OutputResult(container, bucketlist, windowsize, len(data) - 1))\nprint(\"estimation\", estimation)\nprint(\"running time\", time.time()-start)\n\nprint(\"\\nAccuracy:\", 1-abs(truth-estimation)\/truth)","eced9ea2":"import nltk\nfrom nltk.corpus import words\nword_list = words.words()","209a8176":"from nltk.corpus import movie_reviews\n\nneg_reviews = []\npos_reviews = []\n\nfor fileid in movie_reviews.fileids('neg'):\n  neg_reviews.extend(movie_reviews.words(fileid))\nfor fileid in movie_reviews.fileids('pos'):\n  pos_reviews.extend(movie_reviews.words(fileid))","2f262993":"# Your code here, you can add cells if necessary\ndef check_word(word, word_list):\n    return word in word_list\n\n# test\n# start = time.time()\n# i = 0\n# for word in pos_reviews:\n#     check_word(word, word_list)\n#     i += 1\n#     if i % 10000 == 0:\n#         print(i)\n#         print(time.time()-start)\n#         start = time.time()\n# print(time.time()-start)\n\n# total time: about half an hour","4e8f9586":"# Your code here, you can add cells if necessary\nimport mmh3\nimport time\nclass bloom_filter():\n    def __init__(self,bit_length,hash_func_num):\n        self.bit_length = bit_length\n        self.bitvec = [False]*bit_length\n        self.hash_func_num = hash_func_num\n    \n    def add(self,dataset):\n        for data in dataset:\n            for hash_seed in range(self.hash_func_num):\n                index = mmh3.hash(data,hash_seed)%self.bit_length\n                self.bitvec[index] = True\n                \n    def check(self,data):\n        res = True\n        for hash_seed in range(self.hash_func_num):\n            index = mmh3.hash(data,hash_seed)%self.bit_length\n            res = res&self.bitvec[index]\n        return res\n\nbf = bloom_filter(5000000,10)\nbf.add(word_list)\n\nt1 = time.time()\npos_ = []\nneg_ = []\nfor pos in pos_reviews:\n    pos_.append(bf.check(pos))\nfor neg in neg_reviews:\n    neg_.append(bf.check(neg))\nt2 = time.time()\n\n\nprint(\"Bloom filter: \",t2-t1)","ecbaea06":"# Your code here, you can add cells if necessary\nimport numpy as np\n\npos_set = set(pos_reviews)\nneg_set = set(neg_reviews)\nprint(\"----------- Fix M -----------\")\nfor k in range(1,10):\n    falsecount = 0\n    model = bloom_filter(1500000,k)\n    model.add(word_list)\n    for data in pos_set:\n        flag = model.check(data)\n        if flag and (data not in word_list):\n            falsecount += 1\n    for data in neg_set:\n        flag = model.check(data)\n        if flag and (data not in word_list):\n            falsecount += 1\n    print(\"K = %s, M = %s, False positive: %s\"%(k,150000,falsecount\/(len(pos_set)+len(neg_set))))\n\nprint(\"\\n t----------- Fix K -----------\")\n\n# fix K to be 5, change M\nfor m in range(1,10):\n    falsecount = 0\n    model = bloom_filter(m*1000000,10)\n    model.add(word_list)\n    for data in pos_set:\n        flag = model.check(data)\n        if flag and (data not in word_list):\n            falsecount += 1\n    for data in neg_set:\n        flag = model.check(data)\n        if flag and (data not in word_list):\n            falsecount += 1\n    print(\"K = %s, M = %s, False positive: %s\"%(5,m*1000000,falsecount\/(len(pos_set)+len(neg_set))))","075b26de":"# Your code here, you can add cells if necessary\ndef count_occur(dataset,word):\n    return dataset.count(word)\n\n# test\nword = neg_reviews[1]\nres = count_occur(neg_reviews,word)\nprint(res)","d7bd9027":"# Your code here, you can add cells if necessary\nimport numpy as np\nimport mmh3\nclass CM_Sketch():\n    def __init__(self,width,depth):\n        self.depth = depth\n        self.width = width\n        self.count_map = np.zeros((self.depth,self.width),dtype = np.int64)\n        \n    def add(self,dataset):\n        for data in dataset:\n            for hash_seed in range(self.depth):\n                index = mmh3.hash(data,hash_seed)%self.width\n                self.count_map[hash_seed][index] += 1\n                \n    def retrieve(self,data):\n        res = []\n        for hash_seed in range(self.depth):\n            index = mmh3.hash(data,hash_seed)%self.width\n            res.append(self.count_map[hash_seed][index])\n        return min(res)","f848f21d":"print(\"---------- Fix W to be 1000, change D from 5 to 15 ----------\")\nfor d in range(5,16):\n    w = 1000\n    model = CM_Sketch(w,d)\n    model.add(neg_reviews)\n    res = 0\n    truth = 0\n    for data in neg_reviews[:10]:\n        model_predic = model.retrieve(data)\n        gt = count_occur(neg_reviews,data)\n        res += abs(model_predic-gt)\n        truth += gt\n    print(\"W: %s, D: %s, Error: %.4f\"%(w, d, res\/truth))","d19f90c0":"print(\"---------- Fix D to be 10, change W from 100 to 1000 ----------\")\nfor w in range(100, 1100, 100):\n    d = 10\n    model = CM_Sketch(w,d)\n    model.add(neg_reviews)\n    res = 0\n    truth = 0\n    for data in neg_reviews[:10]:\n        model_predic = model.retrieve(data)\n        gt = count_occur(neg_reviews,data)\n        res += abs(model_predic-gt)\n        truth += gt\n    print(\"W: %s, D: %s, Error: %.4f\"%(w, d, res\/truth))","57ea4060":" ### 2. Implement the bloom filter by yourself and add all words in word_list in your bloom filter. Compare the running time difference between linear search on a list and multiple hash computations in a Bloom filter.","1bf4f3f8":"### 2. Implement the Count-Min sketch by yourself. Set different width w and depth d of the internal data structure of CM-Sketch. Compare the influence of different w and d on the error.","e02ae3ea":"### 1. Write a function that accurately counts the occurrence times of each word in neg_reviews or pos_reviews.","be68aa9f":"### 1. Set the window size to 1000, and count the number of 1-bits in the current window.","de38dfeb":"I choose the first 10 words in neg_reviews for evaluating the error rate of Count-Min Sketch. ","fa45ed30":"Here we get a data stream (word_list) and 2 query lists (neg_reviews and pos_reviews).","619cba74":"Here we use the query stream (neg_reviews or pos_reviews) from task 2.","58d425a8":"#### We can see that the larger W and D is, the lower error rate is.","18e767e3":"#### The running time of brute force approach is about half an hour, while the running time of Bloom Filter is about only 10s.","30f2a11d":"### 2. With the window size 1000, count the number of 1-bits in the last 500 and 200 bits of the bitstream.","0c9279f4":"## **Task1\uff1aDGIM**\n\nDGIM is an efficient algorithm in processing large streams. When it's infeasible to store the flowing binary stream, DGIM can estimate the number of 1-bits in the window. In this coding, you're given the stream_data_dgim.txt (binary stream), and you need to implement the DGIM algorithm to count the number of 1-bits. Write code below.","3fb9804c":"Then we load another dataset from the NLTK Corpora collection: movie_reviews.\n\nThe movie reviews are categorized between positive and negative, so we construct a list of words (usually called bag of words) for each category.","45e7c080":"## **Task3: Count-Min sketch**\n\n","a5967826":"### Data loading:\n\nFrom the NLTK (Natural Language ToolKit) library, we import a large list of English dictionary words, commonly used by the very first spell-checking programs in Unix-like operating systems.","1b8d1d5c":"# EE359-Coursework 4  Streaming Algorithm","fb524fb5":"In computing, the count\u2013min sketch (CM sketch) is a probabilistic data structure that serves as a frequency table of events in a stream of data. ","71f9693f":"### 3. Write a function that accurately counts the number of 1-bits in the current window. Caculate the accuracy of your own DGIM algorithm and compare the running time difference.","b804182a":"### 3. Use different bit array length \u2018m\u2019 and number of hash functions \u2018k\u2019 to implement the bloom filter algorithm. Then compare the impact of different m and k on the false positive rate.","785c49bb":"#### The accuracy is 0.86, which is in the range of [0.5, 1]\n\n#### Apparently, the running time of DGIM is larger. This is in line with the expectation, since DGIM uses significantly less space.","00236ec0":"#### We can see that if k is too large or too small compared to M, the false positive rate is relatively large. For M, if it is too small compared to K, then the positive rate is high.\n\n#### In fact, the best parameter relationship that minimizes the false positive rate is\n\n#### $$K = \\frac{M}{N}ln2$$","c0e779fd":"## **Task2: Bloom Filter**\n\nA Bloom filter is a space-efficient probabilistic data structure. Here the task is to implement a bloom filter by yourself. ","5a5ec0dc":"### 1. Write a function that accurately determines whether each word in neg_reviews and pos_reviews belongs to word_list."}}