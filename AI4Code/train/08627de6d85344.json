{"cell_type":{"a050032c":"code","780094c7":"code","59a3545f":"code","86727421":"code","f3aa8609":"code","bf229408":"code","f6b07003":"code","0108b496":"code","41bc10f5":"code","7a5a728d":"code","f156f786":"code","2507b37b":"code","bd30e3c2":"code","72864e26":"code","bfd6ff01":"code","20a4ef54":"code","0ef1cf1f":"code","468f9729":"code","dcab8299":"code","138ff636":"code","f1d1d622":"code","84672c3c":"code","cf1f10f9":"code","9b055d31":"code","a7fa246f":"code","c9e47dac":"code","02eb05ff":"code","633d0a09":"code","1f6e6d4e":"code","d17c3f15":"code","3313b1b6":"code","9ad23984":"code","ddf6ed54":"code","fc713ee6":"code","6222d1cf":"code","50107f27":"code","179f0b1d":"code","ff19cc97":"code","5aec8ed9":"code","c3b4b822":"code","d7c09ec0":"code","2d7132e3":"code","5894670f":"code","915eb6ef":"code","19d7b2aa":"code","e994878f":"code","2274db51":"code","33440eae":"code","66b94869":"code","4e2520b7":"code","f8871715":"markdown","01d06e4d":"markdown","4fca400e":"markdown","cf678176":"markdown","0943de9d":"markdown","20323c01":"markdown","29e9952c":"markdown"},"source":{"a050032c":"import warnings \nwarnings.filterwarnings('ignore')\nfrom tqdm.notebook import tqdm","780094c7":"import pandas as pd\nimport numpy as np\nimport random","59a3545f":"import nltk\nnltk.download('all')\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.sentiment import SentimentIntensityAnalyzer","86727421":"#libraries regarding topic modelling\nfrom gensim import corpora\nfrom gensim.models.ldamulticore import LdaMulticore\n\nfrom gensim.models import CoherenceModel\nfrom gensim import matutils","f3aa8609":"df_car = pd.read_csv('..\/input\/reviews-of-5-car-brands\/5 Car Brand Reviews\/car_5_brands.csv')\ndf_car","bf229408":"sia = SentimentIntensityAnalyzer()\n\ndef sentimenter(x) :\n  result = sia.polarity_scores(x)\n  return result","f6b07003":"''' we have classified sentiments based on compound score & we have decided tweak the limits a bit. '''\ndf_car['sentiment score'] = df_car['review'].apply(sentimenter)\ndef sent_score_string(x):\n  if (x['compound']>= 0.05):\n    return 'positive'\n  elif (x['compound']> -0.05 and x['compound']< 0.05) :\n    return 'neutral'\n  elif (x['compound'] <= -0.05):\n    return 'negative'\n\ndf_car['relative sentiments'] = df_car['sentiment score'].apply(sent_score_string)","0108b496":"df_car","41bc10f5":"#these numbers tell how many reviews of each kind are present\ndf_car['relative sentiments'].value_counts()","7a5a728d":"df_car.brand_name.value_counts()","f156f786":"df_car2 = df_car[df_car['brand_name']=='Mercedes-Benz']\ndf_car2 = df_car2.reset_index()\n\ndf_car = df_car[df_car['brand_name']=='BMW']\ndf_car = df_car.reset_index()","2507b37b":"df_car.drop('Unnamed: 0',axis=1,inplace=True)\n\ndf_car2.drop('Unnamed: 0',axis=1,inplace=True)","bd30e3c2":"df_car = df_car.drop(labels='index',axis=1)\n\ndf_car2 = df_car2.drop(labels='index',axis=1)","72864e26":"df_car.info()","bfd6ff01":"df_car['Rating'].unique()\ndf_car = df_car[df_car['Rating']<=4]\n\ndf_car2['Rating'].unique()\ndf_car2 = df_car2[df_car2['Rating']<=4]","20a4ef54":"df_car = df_car[df_car['relative sentiments']!='positive']\n\ndf_car2 = df_car2[df_car2['relative sentiments']!='positive']","0ef1cf1f":"stop_words = list(stopwords.words('english'))","468f9729":"stop_words = list(stopwords.words('english'))","dcab8299":"stop_words.extend(['porsche,' 'mercede','comfortsport', 'mercedes','mercedes-benz', 'honda','toyota','audi', 'benz','bentley','lexus',\n                  'nissan','volvo','drive','nt','like','vehicle','infiniti','good','miles','corvette','come','edmund','lotus','diego','snake',\n                 'porsche', 'cayman','bought','year','minute','chicago','car','home', 'work','think','suv','people','edmunds',\n                  'cabriolet','lexuss','japan','husband','baby','range', 'rover','cadillac','cadillacs','michelin','texas','second',\n                   'awsome','one','now', 'take', 'give', 'new','levinson','road','love','sedan','wife','sport','bang','tank',\n                   'truck','lemon','imho','pathfinder','infinity','convertible','allroad','conv','bike','ski','grocery','mclass'\n                  ,'hardtop','club','hubby','child','zoom','test','etc','brain','ashamed','carmax','alpina','rocketship','great','germany',\n                  'autobahn','mercedez','bmw'])","138ff636":"df_car.review","f1d1d622":"from nltk.stem import WordNetLemmatizer \n\n# Initialise the Wordnet Lemmatizer\nlemmatizer = WordNetLemmatizer()","84672c3c":"def tokenisation_pos_stopword_lemmatize(x):\n  ''' This function was created to tokenise,POS tagging, Remove Stopwords & then lemmatise the Reviews. '''\n  tokens = nltk.word_tokenize(x)\n  #print (nltk.pos_tag(tokens))\n  #POS-tagging\n  tags = nltk.pos_tag(tokens)\n  pos_tags_words = [t for t in tags if t[1] in[\"JJ\",\"JJR\",\"JJS\",\"NN\",\"NNP\",\"NNS\",\"NNPS\",\"VB\", \"VBD\" ,\"VBG\" ,\"VBN\" ,\"VBP\", \"VBZ\"] ]\n  #stop-word removal\n  filtered_words = [t[0] for t in pos_tags_words]\n  filtered_words2 = [w for w in filtered_words if not w.lower() in stop_words]\n  #lemmatization with lowercase function\n  lemmatized_output = [lemmatizer.lemmatize(w).lower() for w in filtered_words2]\n  return lemmatized_output","cf1f10f9":"df_car['review'] = df_car['review'].apply(tokenisation_pos_stopword_lemmatize)\n\ndf_car2['review'] = df_car2['review'].apply(tokenisation_pos_stopword_lemmatize)","9b055d31":"df_car = df_car.reset_index()\ndf_car.drop('index',axis=1,inplace=True)\ndf_car\n\ndf_car2 = df_car2.reset_index()\ndf_car2.drop('index',axis=1,inplace=True)\ndf_car2","a7fa246f":"big_array = []\nfor i in range(len(df_car['review'])):\n    big_array.extend(df_car['review'][i])","c9e47dac":"big_array2 = []\nfor i in range(len(df_car2['review'])):\n    big_array2.extend(df_car2['review'][i])","02eb05ff":"docs1 = [' '.join(big_array)]\ndocs2 = [' '.join(big_array2)]","633d0a09":"lng_sent1 =[ i for i in docs1 ]\ndocs = lng_sent1\nlng_sent2 =[ i for i in docs2 ] ","1f6e6d4e":"docs.extend(lng_sent2)","d17c3f15":"from nltk.corpus import webtext\n  \n# use to find bigrams, which are pairs of words\nfrom nltk.collocations import BigramCollocationFinder\nfrom nltk.metrics import BigramAssocMeasures\n\nbiagram_collocation = BigramCollocationFinder.from_words(big_array)\nbiagram_collocation.apply_freq_filter(3)\nbigram_list = biagram_collocation.nbest(BigramAssocMeasures.likelihood_ratio, 15)\n\n#bigram_list","3313b1b6":"''' These lines show exact frequencies for bigrams.'''\nbigram_fd = nltk.FreqDist(nltk.bigrams(big_array))\n#bigram_fd.most_common()","9ad23984":"# Loading Libraries\nfrom nltk.collocations import TrigramCollocationFinder\nfrom nltk.metrics import TrigramAssocMeasures\n  \ntrigram_collocation = TrigramCollocationFinder.from_words(big_array)\n# trigram_list = trigram_collocation.apply_freq_filter(3)\n  \ntrigram_list = trigram_collocation.nbest(TrigramAssocMeasures.likelihood_ratio, 15)\n#trigram_list","ddf6ed54":"''' These lines show exact frequencies for trigrams.'''\ntrigram_fd = nltk.FreqDist(nltk.trigrams(big_array))\n#trigram_fd.most_common()","fc713ee6":"#we have extracted tuples inside tuples ( there are two tuples inside this tuple )\nout = [item for t in trigram_fd.most_common() for item in t] \n\nlst_tup=[] #this contains bigrams in a tuple form\nlst_num=[] #this contains the frequency of bigrams\nfor i in tqdm(out):\n    j=out.index(i)\n    if j%2==0:\n        lst_tup.append(out[j])\n    else:\n        lst_num.append(out[j])\n\n# print(lst1)\n# print(lst_num)","6222d1cf":"#we have extracted tuples inside the first tuple using the above method\nlst_final = [] # this will contain the bigram in a 'X_Y' format\nlst_std = [item for t in lst_tup for item in t]\nfor i in tqdm(lst_std):\n  j=lst_std.index(i)\n  if j%2==0:\n    t=lst_std[j]+'_' + lst_std[j+1] +'_'+ lst_std[j+2]\n    lst_final.append(t)\n\n#print(lst_final)","50107f27":"tgm_wth_freq = []\n\nfor i in tqdm(range(len(lst_final))):\n  if lst_num[i] >= 3:\n    l = [lst_final[i]] * lst_num[i]\n    tgm_wth_freq.extend(l)\n  else:\n    break","179f0b1d":"random.shuffle(tgm_wth_freq)\nlng_sent45 =[ i for i in tgm_wth_freq ]\ndocs = lng_sent45","ff19cc97":"from numpy import array\n\na = array(docs)\nprint (a.shape)\n","5aec8ed9":"# Tokenize the documents.\nfrom nltk.tokenize import RegexpTokenizer\n\n# Split the documents into tokens.\ntokenizer = RegexpTokenizer(r'\\w+')\nfor idx in tqdm(range(len(docs))):\n    docs[idx] = docs[idx].lower()  # Convert to lowercase.\n    docs[idx] = tokenizer.tokenize(docs[idx])  # Split into words.\n\n# Remove numbers, but not words that contain numbers.\ndocs = [[token for token in doc if not token.isnumeric()] for doc in docs]\n\n# Remove words that are only one character.\ndocs = [[token for token in doc if len(token) > 1] for doc in docs]","c3b4b822":"# Lemmatize the documents.\nfrom nltk.stem.wordnet import WordNetLemmatizer\n\nlemmatizer = WordNetLemmatizer()\ndocs = [[lemmatizer.lemmatize(token) for token in doc] for doc in docs]","d7c09ec0":"# Compute bigrams.\nfrom gensim.models import Phrases\n\n# Add bigrams and trigrams to docs (only ones that appear 20 times or more).\nbigram = Phrases(docs, min_count=20)\nfor idx in tqdm(range(len(docs))):\n    for token in bigram[docs[idx]]:\n        if '_' in token:\n            # Token is a bigram, add to document.\n            docs[idx].append(token)","2d7132e3":"# Remove rare and common tokens.\nfrom gensim.corpora import Dictionary\n\n# Create a dictionary representation of the documents.\ndictionary = Dictionary(docs)\n\n# Filter out words that occur less than 20 documents, or more than 50% of the documents.\n#dictionary.filter_extremes(no_below=20, no_above=0.5)","5894670f":"# Bag-of-words representation of the documents.\ncorpus = [dictionary.doc2bow(doc) for doc in docs]","915eb6ef":"print('Number of unique tokens: %d' % len(dictionary))\nprint('Number of documents: %d' % len(corpus))","19d7b2aa":"# Train LDA model.\nfrom gensim.models import LdaModel\n\n# Set training parameters.\nnum_topics = 10\nchunksize = 2000\npasses = 20\niterations = 400\neval_every = None  # Don't evaluate model perplexity, takes too much time.\n\n# Make a index to word dictionary.\ntemp = dictionary[0]  # This is only to \"load\" the dictionary.\nid2word = dictionary.id2token\n\nmodel = LdaModel(\n    corpus=corpus,\n    id2word=id2word,\n    chunksize=chunksize,\n    alpha='auto',\n    eta='auto',\n    iterations=iterations,\n    num_topics=num_topics,\n    passes=passes,\n    eval_every=eval_every\n)","e994878f":"top_topics = model.top_topics(corpus) #, num_words=20)\n\n# Average topic coherence is the sum of topic coherences of all topics, divided by the number of topics.\navg_topic_coherence = sum([t[1] for t in top_topics]) \/ num_topics\nprint('Average topic coherence: %.4f.' % avg_topic_coherence)\n\nfrom pprint import pprint\npprint(top_topics)","2274db51":"# Compute Coherence Score using c_v\ncoherence_model_lda = CoherenceModel(model= model, texts=docs, dictionary=dictionary, coherence='c_v')\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","33440eae":"# Compute Coherence Score using UMass\ncoherence_model_lda = CoherenceModel(model= model, texts=docs, dictionary=dictionary, coherence=\"u_mass\")\ncoherence_lda = coherence_model_lda.get_coherence()\nprint('\\nCoherence Score: ', coherence_lda)","66b94869":"def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n    \"\"\"\n    Compute c_v coherence for various number of topics\n\n    Parameters:\n    ----------\n    dictionary : Gensim dictionary\n    corpus : Gensim corpus\n    texts : List of input texts\n    limit : Max num of topics\n\n    Returns:\n    -------\n    model_list : List of LDA topic models\n    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n    \"\"\"\n    coherence_values = []\n    model_list = []\n    for num_topics in range(start, limit, step):\n        model=LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics)\n        model_list.append(model)\n        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n        coherence_values.append(coherencemodel.get_coherence())\n\n    return model_list, coherence_values","4e2520b7":"model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=docs, start=2, limit=40, step=6)\n# Show graph\nimport matplotlib.pyplot as plt\nlimit=40; start=2; step=6;\nx = range(start, limit, step)\nplt.style.use('fivethirtyeight')\nplt.plot(x, coherence_values)\nplt.xlabel(\"Num Topics\")\nplt.ylabel(\"Coherence score\")\nplt.legend((\"coherence_values\"), loc='best')\nplt.show()","f8871715":"#Sentiment Analysis","01d06e4d":"##Trigram ( Collocations With 3 words )","4fca400e":"Trigram Topic modelling won't produce good PyLDAVis graph. But, nevertheless it extracts more useful topics.","cf678176":"#Preprocessing","0943de9d":"#Topic Modelling","20323c01":"##Biigram ( Collocations With 2 words )","29e9952c":"#Feature Extraction\n"}}