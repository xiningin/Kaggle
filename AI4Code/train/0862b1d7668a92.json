{"cell_type":{"c65c0fbb":"code","902c0a18":"code","90004761":"code","5c9603e9":"code","ce12e436":"code","c2595752":"code","7f854fc3":"code","53c943a4":"code","0096dc22":"code","59f2850d":"code","258118b8":"code","0ed35d9f":"code","95af8f22":"code","68fc42fd":"code","06781609":"code","3515da49":"code","52328f35":"code","12a1b01d":"code","81bc4b0c":"code","b571c24c":"code","4f419d3f":"code","f9705a97":"code","5fa969a1":"code","165e7192":"code","4adccdc7":"code","5f7bba6b":"code","68349c14":"code","701cb0e6":"code","5b9eaa30":"markdown","1c00ac8c":"markdown","3a8d8d74":"markdown"},"source":{"c65c0fbb":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","902c0a18":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score, f1_score\n\n\n\nimport numpy as np\nimport pandas as pd\n\n\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import precision_recall_curve, mean_squared_error\nfrom sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, cross_val_score, RandomizedSearchCV\n\nfrom category_encoders import CatBoostEncoder, OneHotEncoder \n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import pyplot\n# import seaborn as sns\n# sns.set()\n\nimport logging\nlogging.getLogger('tensorflow').disabled = True","90004761":"df_train = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ndf_train.head(2)","5c9603e9":"df_test = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')\ndf_test.head(2)","ce12e436":"print(f'train data structure {df_train.info()}')\nprint(f'test data structure {df_test.info()}')","c2595752":"def count_missing_values(data):\n    \"\"\"\n    data : dataframe \\n\n\n    Info: \\n\n\n    Simply returns the columns with\n    respective Null Value count.\n\n    \"\"\"\n    missing_values = data.isnull().sum()\n    print(f\" Data with missing values in : \\n{missing_values[missing_values > 0].sort_values(ascending = False)}\")\n","7f854fc3":"count_missing_values(df_train)\ncount_missing_values(df_test)","53c943a4":"df_train = df_train.drop(['Name','Cabin', 'Ticket'], axis=1)\ndf_test = df_test.drop(['Name','Cabin', 'Ticket'], axis=1)","0096dc22":"def fill_Missingg_vals(data):\n    columns = ['SibSp','Parch','Age','Fare']\n    for col in columns: \n        data[col].fillna(data[col].median(),inplace=True)\n\n    data.Embarked.fillna('U',inplace=True)\n    print(data.head(2))\n    return data","59f2850d":"df_train = fill_Missingg_vals(df_train)","258118b8":"df_test = fill_Missingg_vals(df_test)","0ed35d9f":"\ncount_missing_values(df_train)\ncount_missing_values(df_test)","95af8f22":"df_train.info()\ndf_test.info()","68fc42fd":"def get_unique_values(data):\n    \"\"\"\n    data : datafram \\n\n\n    Info: \\n\n    Function returns the unique \n    values in each column. \n    \"\"\"\n    \n    for i in data:\n        print( f'{i} : {data[i].unique()}')\n\nget_unique_values(df_train)","06781609":"sns.pairplot(df_train, hue='Survived', height= 2)\nplt.show()","3515da49":"y = df_train['Survived']\ntrain = df_train.loc[:, df_train.columns != 'Survived']","52328f35":"def data_transform_1(data, y):\n\n    \"\"\"\n    data : Treadment Variables \\n\n    y : Response Variables \\n\n\n    Info : \\n\n\n    performs Scaling on all numeric vales \\n \n    and encodes all categorical values \\n\n\n    \"\"\"\n    column_T = make_column_transformer((OneHotEncoder(handle_unknown = 'ignore'),['Sex']),\n                                        (MinMaxScaler(),[\"PassengerId\", \"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\"]),\n                                        (CatBoostEncoder(), [\"Embarked\"]))\n  \n\n    y = y.astype('float32')\n\n    column_T.fit(data, y)\n    data = column_T.transform(data)\n    return data, y\n\n\nnew_normal, y_norm = data_transform_1(train, y)","12a1b01d":"# test, train split \ndef data_split(data, target, split_size):\n\n    \"\"\"\n    data : Treatment Variable \\n\n    target : Response Variable \\n\n    split_size : train to test data split ratio \\n\n\n    Info: \\n\n\n    Splits data into train and test data.\n\n    \"\"\"\n    X_train1, X_test1, y_train1, y_test1 = train_test_split(data, target, test_size = split_size, random_state = 42)\n\n    return [X_train1, X_test1, y_train1, y_test1]\n\nX_train, X_test, y_train, y_test = data_split(new_normal, y_norm, 0.2)","81bc4b0c":"rf_classifier = RandomForestClassifier()","b571c24c":"# Number of trees in random forest\ndef find_best_param(X, y, model):\n\n    \"\"\"\n    X : Treatment Variables \\n\n    y : Response Variable  \\n\n    model : random forest classifer \/ regressor \\n \n\n    Info : \n    \n    Returns suitable values for : \\n\n                {n_estimators', \\n\n                'max_features', \\n\n                'max_depth', \\n\n                'min_samples_split', \\n\n                'min_samples_leaf', \\n\n                'bootstrap'}\n\n    Uses the random grid to search for best hyperparameters.\n\n    \n    \"\"\"\n    n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n    # Number of features to consider at every split\n    max_features = ['auto', 'sqrt']\n    # Maximum number of levels in tree\n    max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n    max_depth.append(None)\n    # Minimum number of samples required to split a node\n    min_samples_split = [2, 5, 10]\n    # Minimum number of samples required at each leaf node\n    min_samples_leaf = [1, 2, 4]\n    # select samples for training each tree\n    bootstrap = [True, False]\n    # Create the random grid\n    random_grid = {'n_estimators': n_estimators,\n                'max_features': max_features,\n                'max_depth': max_depth,\n                'min_samples_split': min_samples_split,\n                'min_samples_leaf': min_samples_leaf,\n                'bootstrap': bootstrap}\n    print(random_grid)\n\n    # Using 3 fold cross validation with 50 combinations\n    rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 50, cv = 3, verbose=2, random_state=42)\n    rf_random.fit(X, y)\n\n    return f\"Possible best Parameters : {rf_random.best_params_}\"\n","4f419d3f":"find_best_param(X_train, y_train, rf_classifier)","f9705a97":"def random_forest(model, X, y, X_test):\n    \"\"\"\n    model : defined model \\n\n    X : Treatment train data \\n\n    y : Response train data \\n\n    X_test : Treatment test data \\n\n    y_test : Response test data \\n\n\n    Info : \\n\n\n    takes training data and \n    performs prediction on test data\n\n    \"\"\"\n    model.fit(X, y)\n    y_pred = model.predict(X_test)\n    return y_pred","5fa969a1":"rf_class = RandomForestClassifier(n_estimators = 800, min_samples_leaf=4, min_samples_split=10, max_features='sqrt', max_depth=50, bootstrap= True, random_state=42)\ny_predict = random_forest(rf_class, X_train, y_train, X_test)","165e7192":"from sklearn.metrics import f1_score\ndef get_f1_score(y_true, y_pred):\n\n    \"\"\"\n    y_true : test data \\n\n    y_pred : predicted data \\n\n\n    Info : \\n\n\n    returns weighted f1 score\n    \"\"\"\n    print(f\"Weighted f1 score : {f1_score(y_true, y_pred, average = 'weighted')}\")","4adccdc7":"get_f1_score(y_test, y_predict)","5f7bba6b":"test, __ = data_transform_1(df_test, y[:418])","68349c14":"y_pred = random_forest(rf_class, X_train, y_train, test)","701cb0e6":"submission = pd.DataFrame({'PassengerId': df_test['PassengerId'],'Survived': y_pred})\nprint(submission)\nsubmission.to_csv(\"submission.csv\", index=False)","5b9eaa30":"### passing *y* because *__catboostEncoder()__* requires target reference.\n*can be done in another way, by writing a separate function using a different encoder*","1c00ac8c":"## Data Normalization and categorical features Encoding\n- All numeric values are Scaled between 0 to 1 using *__MinMaxScaler()__*.\n- Out of all available encoders, *__'CatBoostEncoder()'__* is used for categorical features\nto overcome the *target leakage* problems and *__OneHotEncoder()__* for features with only two categories.\n- *Hashing Tricks* could also yeild good results but the approach is extremely slow for the features like in the given data set.","3a8d8d74":"# Find the best hyperparameter\n### - Using *RandomizedSearchCV()* to find the best suitable parameters"}}